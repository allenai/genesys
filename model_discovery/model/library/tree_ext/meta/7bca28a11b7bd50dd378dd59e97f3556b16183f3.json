{"paperId": "7bca28a11b7bd50dd378dd59e97f3556b16183f3", "title": "MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the performance of other open-source larger models on some public benchmarks. We also introduce an innovative instruction tuning framework tailored for smaller models to enhance their capabilities efficiently. Moreover, we explore the application of MindLLM in specific vertical domains such as law and finance, underscoring the agility and adaptability of our lightweight models.", "venue": "arXiv.org", "year": 2023, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "MindLLM is presented, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters."}, "embedding": {"model": "specter_v2", "vector": [-0.06132706627249718, 0.4477330446243286, -0.5130199193954468, -0.03606973588466644, -0.1838049292564392, -0.2951435148715973, 0.6413506269454956, -0.12816239893436432, -0.7157702445983887, -0.23632679879665375, 0.4808539152145386, -0.45891621708869934, 0.48718956112861633, 0.08143729716539383, -0.28498801589012146, 0.4062080383300781, -0.8104910254478455, 0.7196038961410522, -0.31678441166877747, -0.5262226462364197, -0.40192294120788574, -0.5548864603042603, -0.7840991616249084, -0.2084430456161499, 0.587876558303833, 0.24195259809494019, 0.11345130205154419, 0.9182229042053223, -0.20910045504570007, 0.4494357109069824, 0.30733710527420044, -0.4849317669868469, 0.18536162376403809, 0.3161332607269287, -0.385667622089386, -0.14623135328292847, 0.29539158940315247, -0.693122923374176, -0.19277258217334747, 0.5306016802787781, -0.286830872297287, 0.3120686411857605, 0.47703036665916443, -0.43255144357681274, -0.5479657053947449, 0.8042121529579163, 0.4583825469017029, 0.553589940071106, -0.24701540172100067, -0.1758766919374466, 1.3956809043884277, -1.0397534370422363, 0.15355825424194336, 1.4025700092315674, 0.40341076254844666, 0.5224409103393555, -0.4707978665828705, -0.9194179773330688, 0.285438597202301, -0.4072875380516052, -0.7781798243522644, -0.28651565313339233, -0.0947657972574234, -0.1299005150794983, 2.1078383922576904, -0.3803073465824127, -0.18200033903121948, 0.3868907392024994, -0.3184637129306793, 1.2399487495422363, -0.06081954389810562, -0.9694411754608154, -0.4007319211959839, -0.01896895095705986, 0.11456821113824844, 0.847928524017334, -0.13824637234210968, 0.31070205569267273, -0.6956249475479126, -0.27232885360717773, 0.3615575432777405, -0.5154696106910706, 0.1856675148010254, 0.14993606507778168, -0.4167354106903076, 1.0479973554611206, 0.2038906365633011, 0.9894644021987915, 0.08713468909263611, 0.5757285952568054, 0.40307000279426575, 0.7821256518363953, 0.11379025876522064, 0.4883800745010376, -0.6313877701759338, 0.463555246591568, -0.8098745346069336, 0.331356942653656, 0.12423499673604965, 1.007022500038147, -0.023718412965536118, 0.22523732483386993, -0.7389139533042908, 0.18613587319850922, 1.3282277584075928, 0.20845742523670197, 0.728047251701355, -0.56235671043396, 0.6044825911521912, -0.4192769527435303, 0.10516296327114105, -0.076592355966568, -0.4532250165939331, -0.35270342230796814, -0.6991621255874634, -1.3366522789001465, -0.2786616384983063, -0.09242021292448044, -0.6457464694976807, 0.8838950991630554, -0.2784738540649414, -0.23182785511016846, 0.4886234998703003, 0.11990340054035187, 0.6409515738487244, 0.46693697571754456, 0.3701492249965668, 0.1438588947057724, 1.0089136362075806, -0.9724021553993225, -0.358031690120697, -1.5636100769042969, 1.0416079759597778, -0.35293710231781006, 0.3690568506717682, -0.3964323103427887, -1.3393726348876953, -0.8425801992416382, -0.7256013751029968, -0.28031402826309204, -0.6299316883087158, 0.6812095642089844, 1.1573818922042847, 0.4568164348602295, -0.6678551435470581, 0.30855801701545715, 0.10359925031661987, -0.15534351766109467, -0.02414543181657791, 0.21820776164531708, 0.1909981518983841, -0.44908851385116577, -1.6872788667678833, 0.35007619857788086, 0.532633900642395, -0.8564027547836304, -0.11080051213502884, -0.05928950756788254, -1.3324304819107056, -0.30579036474227905, -0.13753150403499603, -0.25536003708839417, 1.4347219467163086, 0.0868847668170929, -1.2081857919692993, 1.0034027099609375, -0.23366284370422363, -0.20718127489089966, 0.08854944258928299, 0.10217989236116409, -0.8657732009887695, -0.8038725256919861, -0.3741668164730072, 0.46332502365112305, 0.4861516058444977, -0.0009259753860533237, 0.0630682036280632, 0.6366813778877258, 0.16357086598873138, -0.19692307710647583, -0.5294130444526672, 1.1820927858352661, -0.5715389251708984, -0.42074671387672424, 0.03024626336991787, 0.445976585149765, -0.2500409781932831, -0.4548776149749756, -0.2689805328845978, -1.0363963842391968, 0.5157463550567627, -0.2958894371986389, 1.2969579696655273, -0.8522496223449707, -0.7107989192008972, -0.030651509761810303, -0.2566673159599304, 0.047437287867069244, -0.7747207283973694, 0.7385583519935608, -0.2536550760269165, 0.3772428333759308, -0.2812330424785614, -1.1658623218536377, 0.16566799581050873, -0.26895496249198914, -0.43059268593788147, -0.17973178625106812, 0.09233807772397995, 1.1296650171279907, -0.5287070274353027, -0.012508356012403965, -0.1424020677804947, 0.08068221062421799, -1.3004822731018066, 1.2444547414779663, -0.6555562019348145, 0.24943692982196808, 0.0855109691619873, -0.11316648870706558, 0.025776615366339684, -0.451691210269928, 0.40294361114501953, -0.17604082822799683, -0.41659730672836304, 0.29396283626556396, -0.3385407626628876, 1.5875825881958008, -0.5322414040565491, 0.18816328048706055, -0.0013453245628625154, -0.3353557884693146, -0.12696947157382965, 0.5888463854789734, -0.38544797897338867, -0.38493478298187256, 0.41273391246795654, 0.7009536623954773, -0.4535980820655823, 0.34333425760269165, 0.6882658004760742, 0.7180850505828857, -0.41366836428642273, 0.4626809358596802, 0.5737061500549316, -0.508993923664093, 0.8699942827224731, 0.44085773825645447, 0.35836154222488403, 0.09979284554719925, 0.5335477590560913, -0.07358095794916153, 0.6985474228858948, -0.7666675448417664, -0.29720407724380493, 0.26045092940330505, 0.5745858550071716, 0.5030288100242615, 0.08916892111301422, -0.6865730881690979, -0.09673610329627991, 0.1999683976173401, 0.6104806065559387, 1.7160595655441284, -0.4174003005027771, -0.0799078643321991, -0.7580662369728088, -0.34641653299331665, -0.0069580720737576485, 0.26426076889038086, -0.1705935150384903, 0.041554708033800125, -1.0271838903427124, -0.9596495628356934, 0.7839297652244568, 0.04356466606259346, 0.7639819383621216, -0.6427723169326782, -0.2885303199291229, -0.3728218376636505, -0.017957843840122223, -0.8494429588317871, -0.8332944512367249, 0.34288668632507324, -0.5370323657989502, 0.02655264548957348, -0.02038547210395336, -0.4359701871871948, 0.08634219318628311, -0.7006077170372009, 1.0343340635299683, -0.35842856764793396, -0.17272904515266418, -0.13237710297107697, 0.9066747426986694, -0.6232857704162598, -1.1811072826385498, 0.11329654604196548, 0.2818036675453186, -0.3226264417171478, 0.4568406939506531, 0.6624149084091187, 0.41449421644210815, 0.08386435359716415, -0.5780446529388428, 0.32089677453041077, 0.34530675411224365, -0.00940692238509655, 0.5119085907936096, -0.1923547089099884, -0.1188560351729393, -1.2781963348388672, 1.09666109085083, 0.296316534280777, -0.7198023200035095, 0.6111986637115479, -0.6718350648880005, 0.007005165796726942, 0.6576011776924133, -0.7981652617454529, -0.3360678255558014, -1.0307817459106445, 0.340017169713974, 0.18562284111976624, -0.19495715200901031, 0.41046005487442017, 0.3332286477088928, 0.06451763212680817, 0.4675600230693817, 0.33809641003608704, 0.03539806231856346, -0.3931545913219452, 0.5722482204437256, -0.5574358701705933, 0.24579891562461853, 0.12380536645650864, 0.5706192851066589, -0.3290635347366333, -0.54300856590271, -0.48384836316108704, -0.4621184766292572, -0.26597830653190613, -0.2986856698989868, -0.1406836360692978, 0.17077162861824036, -0.8987221121788025, -0.5504562854766846, 0.24617233872413635, -1.0409938097000122, -0.35134825110435486, 0.2562752068042755, -0.12147774547338486, 0.007447207812219858, -0.9283994436264038, -1.4800493717193604, -0.22352415323257446, -0.6768906712532043, -1.0030421018600464, 0.5273550152778625, -0.016832806169986725, -0.39166638255119324, -0.7496411204338074, 0.11609666049480438, 0.10277267545461655, 1.0799907445907593, -0.7288077473640442, 1.2397738695144653, -0.09923318028450012, 0.1626451164484024, -0.5497835874557495, 0.13819576799869537, 0.45255374908447266, -0.34193694591522217, 0.3167882263660431, -0.930252730846405, -0.15025195479393005, -0.38785502314567566, -0.6426686644554138, -0.044494882225990295, 0.21222414076328278, 0.3148519694805145, 0.04625364765524864, -0.2978571355342865, 0.3474021553993225, 1.1609340906143188, -0.9009093642234802, -0.24428728222846985, 0.18783973157405853, 0.8083406686782837, 0.21322235465049744, -0.5113489031791687, 0.5718358755111694, 0.4006645083427429, 0.2427215427160263, -0.01817610301077366, -0.3285458981990814, -0.05080251768231392, -0.4615962505340576, 0.6074462532997131, 1.873477816581726, 0.1248176246881485, -0.2113809734582901, -1.1847941875457764, 0.2823890149593353, -0.6482101082801819, -0.3129046857357025, 0.7297490835189819, 0.7579172849655151, 0.7717426419258118, -0.22607854008674622, -0.4536018967628479, -0.10545949637889862, 0.29271724820137024, 0.5104535222053528, -0.4373667240142822, -0.8332273364067078, -0.42634502053260803, 0.27254050970077515, 0.24683032929897308, 0.5374214053153992, -0.5799016356468201, 0.9576402902603149, 14.769946098327637, 1.1185780763626099, 0.14635619521141052, 0.6933782696723938, 0.7004067897796631, 0.42368003726005554, -0.45730969309806824, -0.3237127661705017, -1.1518200635910034, -0.5100764632225037, 1.3879541158676147, -0.006684456020593643, 1.0939425230026245, 0.20140865445137024, -0.01919971965253353, 0.2169668972492218, -0.14164714515209198, 0.5000354647636414, 0.490795761346817, -1.1974316835403442, 0.6576581001281738, 0.23275186121463776, 0.6914376616477966, 0.8365072011947632, 0.7037941813468933, 1.0286755561828613, 0.595883309841156, -0.43480923771858215, 0.6960179805755615, -0.11623790860176086, 1.0662153959274292, -0.06683417409658432, 0.4475652575492859, 0.9787633419036865, -1.0504522323608398, -0.3919294476509094, -0.5083721876144409, -1.258471131324768, -0.01601399853825569, 0.029901064932346344, -0.32964950799942017, -0.49574461579322815, -0.5000818967819214, 0.8061981797218323, -0.0062682144343853, 0.040258511900901794, -0.21554622054100037, 0.6489971876144409, -0.08749712258577347, 0.1532844603061676, 0.20150718092918396, 0.35238099098205566, 0.11990644037723541, -0.04140569642186165, -0.09593891352415085, -0.3374771177768707, 0.051091909408569336, 0.219648540019989, -0.6928107142448425, 0.26112061738967896, -0.4375957250595093, -0.46273109316825867, -0.40447333455085754, 0.7109289765357971, 0.5927612781524658, 0.2874874174594879, -0.847202718257904, 0.2205679565668106, 0.9186550378799438, 0.22174030542373657, -0.4798050820827484, 0.18204869329929352, 0.2532676160335541, -0.5022971630096436, -0.4263494908809662, 0.3156612515449524, -0.017982302233576775, -0.8356810808181763, -0.6616764664649963, -0.6116814613342285, 0.4370250105857849, -0.4701284170150757, -0.5249078273773193, 0.9027793407440186, -0.3421284556388855, -0.5646647214889526, 0.020350448787212372, -0.8784275650978088, 0.010257590562105179, 0.8097679615020752, -1.7077372074127197, -1.0428091287612915, 0.6764369010925293, -0.13573057949543, -0.35453978180885315, -0.2983701527118683, 1.6024984121322632, 0.2728416621685028, -0.5155324935913086, 0.2041417807340622, 0.32854780554771423, 0.24447590112686157, -0.3362768888473511, -0.44488540291786194, 1.1573055982589722, 0.3758832812309265, 0.01456423383206129, 0.27378952503204346, -0.007975936867296696, 0.11345934867858887, -0.7708070278167725, -0.17616239190101624, 1.2427700757980347, -0.8717703819274902, -0.12252510339021683, -1.037975788116455, -0.39897942543029785, 0.30940985679626465, 0.5173367261886597, -0.46299636363983154, 0.3374568223953247, 0.05108698457479477, -0.665145218372345, 0.02097763493657112, -1.0209311246871948, 0.01423636730760336, 0.381442129611969, -0.9320951700210571, -0.008721452206373215, 0.5273257493972778, 0.498283714056015, -0.9348228573799133, -0.6233759522438049, -0.19527237117290497, 0.10525792092084885, -0.012346114031970501, 0.8361843228340149, -0.4491700530052185, 0.5509315133094788, 1.0145305395126343, -0.2516343593597412, -0.7081121206283569, -0.06212956830859184, -0.9100189208984375, -0.2815413773059845, -0.1498013734817505, 0.9980900287628174, -0.7346497178077698, 0.07513849437236786, 1.120682716369629, 0.5225545763969421, -0.46496644616127014, -0.5310805439949036, -0.4522923231124878, 0.24027389287948608, -0.5311158895492554, 0.5775197148323059, -0.06533752381801605, -0.02207101136445999, 0.07970119267702103, 0.2760116755962372, 0.6096332669258118, -0.19725950062274933, -0.44914674758911133, 0.5456844568252563, -0.14539189636707306, -0.4334252178668976, -0.727925181388855, -0.2650867700576782, -1.39408278465271, 0.19206085801124573, -1.088206171989441, -0.28054288029670715, -0.8378625512123108, -0.4903031885623932, 0.10701438039541245, -0.10207526385784149, 0.142599955201149, 0.5663583874702454, -0.2842339873313904, -0.7061711549758911, -0.35710594058036804, -0.07276388257741928, 0.6434853672981262, 0.931395411491394, -0.8568022847175598, 0.25517159700393677, 0.1160261407494545, 0.3778786361217499, 0.3420690596103668, 0.4424659013748169, -0.2288096398115158, -0.8074769377708435, -1.6215649843215942, 0.23133915662765503, -0.1819782257080078, -0.5554543137550354, -0.817099392414093, 0.4842023551464081, 0.18821413815021515, -0.36492210626602173, 0.3900925815105438, 0.20040200650691986, -0.7756085395812988, -0.5528376698493958, 0.6373107433319092, -0.6553694605827332, 0.5502131581306458, 0.44653332233428955, -0.8722844123840332, -0.5184152722358704, 0.6029533743858337, -0.08841325342655182, -0.9531337022781372, -0.49782586097717285, 0.5136749148368835, -0.5969436168670654, 0.37447038292884827, -0.262431263923645, 0.03908267617225647, -0.9211809635162354, -0.3092377781867981, 0.31232765316963196, 0.28523045778274536, -0.3885919749736786, 0.9710012078285217, 0.054033663123846054, -0.7977169156074524, -0.10648497939109802, 0.9241421818733215, 0.12853534519672394, -0.34491151571273804, 0.35591834783554077, 0.17810861766338348, -0.7312085628509521, 1.0948132276535034, 0.6902012228965759, 0.8077380657196045, -0.8459501266479492, 0.07444197684526443, 0.6009394526481628, -0.8810603618621826, 0.11181677877902985, 1.3922317028045654, -0.28764909505844116, -1.466529130935669, 0.10605036467313766, -1.397240400314331, -0.7366166114807129, -0.7246370911598206, 0.5554566979408264, -0.20176725089550018, 0.026625901460647583, -0.5424476265907288, -0.22083835303783417, 0.30180811882019043, 0.08782221376895905, -0.4149927794933319, 0.3653758764266968, -0.12274929881095886, -0.43500617146492004, 0.5538157820701599, 0.7499825954437256, -0.22944790124893188, -0.43131235241889954, -0.6823729276657104, -0.3249402940273285, 0.3093501627445221, 0.2193472981452942, -0.47521156072616577, -0.6147547960281372, 0.4460146129131317, 0.1970076709985733, 0.42447736859321594, 0.26259753108024597, -0.38870230317115784, 0.5082162022590637, 0.9249998331069946, 0.4051387906074524, -0.8853544592857361, -0.8989960551261902, 1.6208773851394653, 0.9851939678192139, -1.255319595336914, 0.18284101784229279, 0.04181747883558273, -0.7931290864944458, 0.6118420362472534, 0.2554238438606262, 0.31428322196006775, 1.1627471446990967, -0.26921403408050537, 0.24755840003490448, 0.26644763350486755, -0.9199031591415405, -0.16623996198177338, 1.1271357536315918, 0.3533155918121338, 1.238399624824524, 0.5825591087341309, -0.05871715396642685, 0.7784826755523682, -0.025141257792711258, 0.06756347417831421, 0.22954872250556946, 0.6655013561248779, -0.12677675485610962, -0.18754515051841736, 0.1063375174999237, 0.6084243655204773, -0.456104576587677, -0.8414780497550964, 0.02408863604068756, 0.6583765149116516, 0.4124991297721863, 0.6034284830093384, 0.4617117643356323, 0.19725263118743896, 0.5364801287651062, 0.2591865360736847, 0.3752599358558655, -0.6400899887084961, -0.34452149271965027, -0.3545739948749542, -0.4842507839202881, 0.19642823934555054, -0.08645043522119522, -0.25467681884765625, -0.45382973551750183, -0.34968051314353943, 0.20523548126220703, 0.06535343080759048, 0.39575594663619995, 1.178500771522522, 0.29068464040756226, 0.04161589965224266, -0.3880484700202942, -0.432819128036499, -0.3257957398891449, -1.0719969272613525, -0.28628939390182495, -0.41247665882110596, -0.18476083874702454, -0.07352416217327118, -0.22363103926181793, -0.43678292632102966]}, "authors": [{"authorId": "2261393344", "name": "Yizhe Yang"}, {"authorId": "2261689091", "name": "Huashan Sun"}, {"authorId": "2261392836", "name": "Jiawei Li"}, {"authorId": "2261476470", "name": "Runheng Liu"}, {"authorId": "2261448070", "name": "Yinghao Li"}, {"authorId": "2261393548", "name": "Yuhang Liu"}, {"authorId": "2261394092", "name": "Heyan Huang"}, {"authorId": "2261669912", "name": "Yang Gao"}], "references": [{"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "afb39ed837db8750dd1c3b2a54ad442372c106b2", "title": "WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models"}, {"paperId": "2ad1d61622bdbb458b192fbead0be76fa45e57c1", "title": "SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark"}, {"paperId": "4b474c1f42eefbf14ca85c951f2a22ce031b6cb7", "title": "Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9", "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"}, {"paperId": "548278897d46a54958909bb23bcaecf63e24fadf", "title": "Secrets of RLHF in Large Language Models Part I: PPO"}, {"paperId": "ab82cd7194fc609d2d29e6b1a4399d6120f97fdd", "title": "Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning"}, {"paperId": "228aee5393e7a11e018bbef940fea1c2816b6ec4", "title": "Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model"}, {"paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b", "title": "Textbooks Are All You Need"}, {"paperId": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6", "title": "CMMLU: Measuring massive multitask language understanding in Chinese"}, {"paperId": "5dea206e2a36e672f197252bdd27d156d058f48c", "title": "FinGPT: Open-Source Financial Large Language Models"}, {"paperId": "fbd2c8089870814449f9254a711041bbae145a82", "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "b17db2508600f498cf36d8ea06716e238bebe3d7", "title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "9b4f7c97c0b83a80c32bc0b93595cbcfb4ecb16d", "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"}, {"paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0", "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "c01e43c65a04d766e429863bdf7cf65b895df20e", "title": "Chinese Open Instruction Generalist: A Preliminary Release"}, {"paperId": "e5adc219685c9941b9a3d029480af4a51c0ea05a", "title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"}, {"paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843", "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"}, {"paperId": "68c834c19cd126bbd6d25a3572d7205cfed76271", "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"}, {"paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea", "title": "Instruction Tuning with GPT-4"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "b626560f19f815808a289ef5c24a17c57320da70", "title": "MathPrompter: Mathematical Reasoning using Large Language Models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "6f4cc536f9ed83d0dbf7e919dc609be12aa0848a", "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "39e40821b7207125e54e6ed7112e55cd38c6f0c3", "title": "Language Models of Code are Few-Shot Commonsense Learners"}, {"paperId": "15ecdf371d691c6ba0378f3c43bc8ead70c2d501", "title": "CSL: A Large-scale Chinese Scientific Literature Dataset"}, {"paperId": "e19b54ad4c1c8af045069e9cac350ffc2ce60e1a", "title": "No Language Left Behind: Scaling Human-Centered Machine Translation"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd", "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "382ba0c4452aab6ecdaf8a62d567bb3c4684e4f0", "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca", "title": "DeepNet: Scaling Transformers to 1, 000 Layers"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "24b471802eda460d4cc17e591804ef04c0cd18ef", "title": "Curriculum Learning: A Survey"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "65906e6027246ae9e4ecd18d6e019a24505c842e", "title": "Aligning AI With Shared Human Values"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc", "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ab59728a60331d37cbdaaeba52f1dbbe72f1187a", "title": "Multi-source knowledge fusion: a survey"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "90e04f3ae23ca7df5f59b11453341e3db943b6f4", "title": "A Continual Learning Survey: Defying Forgetting in Classification Tasks"}, {"paperId": "0c3c4c88c7b07596221ac640c7b7102686e3eae3", "title": "PubMedQA: A Dataset for Biomedical Research Question Answering"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"}, {"paperId": "04b13b2dbc82271859b961556612e2d1451c8a34", "title": "Fine-Tuning Large Language Models for Answering Programming Questions with Code Snippets"}, {"paperId": "450b8dff662a5d41388d04d994e5117020777ce5", "title": "Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": "b9478e237b58160c65acd2c41894493d27e2c277", "title": "WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "1e9441bbad598e181896349757b82af42b6a6902", "title": "Byte Pair Encoding: a Text Compression Scheme That Accelerates Pattern Matching"}, {"paperId": "ac8ab51a86f1a9ae74dd0e4576d1a019f5e654ed", "title": "Some methods for classification and analysis of multivariate observations"}, {"paperId": null, "title": "Moss: Training conversational language models from synthetic data"}, {"paperId": null, "title": "OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models"}, {"paperId": null, "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data"}, {"paperId": null, "title": "Quantifying and"}, {"paperId": null, "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023a"}, {"paperId": null, "title": "Introducing the world\u2019s first truly open instruction-tuned"}, {"paperId": null, "title": "Internlm: A multilingual language model with progressively enhanced capabilities"}, {"paperId": null, "title": "Groeneveld,"}, {"paperId": null, "title": "synthesis"}, {"paperId": null, "title": "Luotuo: An instruction-following chinese language model, lora tuning on llama"}, {"paperId": null, "title": "Openllama: An open reproduction of llama,"}, {"paperId": null, "title": "Codegen: An open large language model for code with multi-turn program 39"}, {"paperId": null, "title": "Introducing chatgpt"}, {"paperId": null, "title": "Code alpaca: An instruction-following llama model for code generation"}]}