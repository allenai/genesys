{"paperId": "212ebf972046016123051028bee8effe1dbf7fe2", "title": "P ARALLEL A TTENTION AND F EED -F ORWARD N ET D ESIGN FOR P RE - TRAINING AND I NFERENCE ON T RANSFORMERS", "abstract": "In this paper, we introduce Parallel Attention and Feed-Forward Net Design (PAF) for transformer models. Transformer models are indisputably the backbone of all Natural Language Processing applications. Therefore, any efforts aimed at improving their ef\ufb01ciency are guaranteed to have an enormous impact. Transformer models consist of many layers and each layer has an attention block followed by a feed-forward network (FFN) that processes the input based on the attention block\u2019s output. We refer to this standard design as Series Attention and Feed-Forward Net Design (SAF). For each layer in our proposed PAF design for transformer models, we make FFN block\u2019s computations independent of the output of the attention block. This decoupling allows FFN block of each layer to run in parallel to the attention block of that layer. We evaluate PAF design by training two large language models (RoBERTa-large and bert-large-uncased) and comparing them to their SAF counterparts on six tasks of the General Language Understanding (GLUE) benchmark which test a multitude of semantic attributes. PAF models achieves nearly identical performance as their SAF counterparts on all the six tasks. We also compare time complexities of attention blocks with FFN blocks and \ufb01nd that running both blocks in parallel can theoretically and in practice achieve upto 1.5x to 2x gains in speed. We leave the development of fast and ef\ufb01cient libraries for implementation of PAF design for future work.", "venue": "", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper introduces Parallel Attention and Feed-Forward Net Design (PAF) for transformer models and demonstrates that running both blocks in parallel can theoretically and in practice achieve upto 1.5x to 2x gains in speed."}, "embedding": {"model": "specter_v2", "vector": [0.4200711250305176, 0.46572747826576233, 0.06087959185242653, -0.14741571247577667, -0.5156071186065674, -0.3229924440383911, 0.5773841738700867, 0.10799214988946915, -0.5072562098503113, -0.44365814328193665, 0.799838662147522, -0.07742082327604294, 0.19750761985778809, -0.17699120938777924, -0.21747395396232605, 0.3123493492603302, -0.7374154925346375, 0.4703785479068756, 0.029233966022729874, -0.38211333751678467, -0.07635720819234848, -0.6669232249259949, -0.7558863162994385, 0.3415203392505646, 0.2534031569957733, 0.5549357533454895, 0.26769500970840454, 0.974255383014679, -0.23673948645591736, 0.47768527269363403, 0.5777129530906677, -0.39731746912002563, 0.23996339738368988, 0.12406758219003677, -0.48535439372062683, -0.006752460729330778, 0.40502557158470154, -0.4263002872467041, -0.5072869658470154, 0.7010639309883118, -0.2465195655822754, 0.3995209038257599, 0.4102199971675873, -0.44935092329978943, -0.14742425084114075, 1.340607762336731, 0.6458667516708374, 0.9332943558692932, -0.30318009853363037, -0.41184139251708984, 1.5030062198638916, -1.554152488708496, -0.10633181035518646, 1.7598943710327148, 0.43021470308303833, 0.2949165999889374, -0.22183538973331451, -0.557267963886261, 0.8758265376091003, 0.11728250235319138, -0.40750232338905334, -0.6358011960983276, -0.05011586844921112, 0.1476181596517563, 2.2248549461364746, -0.3386749029159546, 0.1297016441822052, 0.24525733292102814, -0.23948664963245392, 1.7583377361297607, -0.34124934673309326, -0.7873932719230652, -0.5022361278533936, -0.21876487135887146, 0.6091336607933044, 0.7351652979850769, -0.5282297134399414, 0.2624867856502533, -0.9401326775550842, 0.13547104597091675, 0.5956506133079529, -0.036572232842445374, -0.25230249762535095, 0.07353287935256958, -0.4372180104255676, 0.5544800758361816, 0.4708811342716217, 0.8483814597129822, -0.4014512002468109, 0.5871336460113525, 0.6209043264389038, 0.43376216292381287, -0.16472028195858002, 0.5302750468254089, -0.233483225107193, 0.4915866255760193, -0.7630565166473389, 0.20612208545207977, 0.043078117072582245, 0.9743062257766724, -0.47760719060897827, 0.33853816986083984, -0.6267093420028687, 0.27014726400375366, 1.4429432153701782, 0.15020540356636047, 0.33375757932662964, -0.6371859312057495, 0.25724339485168457, -0.8514164686203003, -0.087852843105793, -0.48328590393066406, -0.029413169249892235, -0.09127619862556458, -0.6510968804359436, -1.38126540184021, -0.057795848697423935, 0.2364380657672882, -1.1236735582351685, 0.7014470100402832, -0.6981371641159058, 0.041236598044633865, 0.21057337522506714, 0.15699416399002075, 0.4377627670764923, 0.5511799454689026, 0.6240236163139343, 0.4806050956249237, 1.205591082572937, -0.7012363076210022, -0.6919770836830139, -1.1673671007156372, 0.5049206614494324, -0.38457098603248596, -0.08553659915924072, -0.05794923007488251, -1.4697213172912598, -0.8944863080978394, -0.5543922781944275, -0.13818751275539398, -0.5914700031280518, 0.09658825397491455, 0.9289394021034241, 0.2083459347486496, -1.3475642204284668, 0.6042202115058899, -0.06008795648813248, -0.5272535681724548, 0.545514702796936, 0.34808337688446045, 0.06736550480127335, -0.31271564960479736, -1.2726463079452515, 0.24267923831939697, 0.4119018018245697, -0.3305221498012543, -0.28425633907318115, -0.6299847960472107, -1.2603936195373535, 0.24423931539058685, 0.0890730619430542, -0.5476508140563965, 1.5168285369873047, -0.06549079716205597, -1.0858490467071533, 0.5348159074783325, -0.5630749464035034, -0.10084469616413116, -0.2161499559879303, -0.3844374418258667, -0.5216749310493469, -0.5604737401008606, -0.20516131818294525, 0.7884697914123535, 0.26322996616363525, -0.022266792133450508, -0.28289809823036194, 0.04269801452755928, -0.13958978652954102, -0.04162179306149483, -0.4586763083934784, 1.1934423446655273, -0.22483742237091064, -0.3582681715488434, 0.528691291809082, 0.9756479859352112, 0.02136765606701374, -0.23775827884674072, -0.4519425630569458, -1.1178475618362427, 0.8614677786827087, -0.08934060484170914, 0.8087750673294067, -0.8951743245124817, -0.5550881624221802, -0.04695168882608414, 0.057866089046001434, -0.20953360199928284, -1.0223987102508545, 0.5205516219139099, -0.6867660880088806, 0.4808526635169983, -0.05276879668235779, -0.999815821647644, -0.026998246088624, -0.18132515251636505, -0.780167281627655, -0.47116953134536743, 0.22567309439182281, 1.243103265762329, -0.9552662372589111, 0.14137034118175507, 0.08519377559423447, 0.16300581395626068, -0.9027748703956604, 1.3914514780044556, -0.38443443179130554, -0.10523567348718643, -0.19333083927631378, -0.26853692531585693, -0.09248059242963791, -0.2198539674282074, 0.23948095738887787, -0.6118516325950623, -0.10298940539360046, 0.773863673210144, 0.02915850467979908, 1.1530519723892212, -0.5314791202545166, 0.37271004915237427, -0.0007731773075647652, -0.6821879148483276, 0.18193237483501434, 0.8018581867218018, -0.5391972064971924, -0.5015187859535217, 0.567543625831604, 0.406076043844223, -0.2251853048801422, 0.48406603932380676, 0.8125172257423401, 0.6120554804801941, -0.2746904790401459, 0.45614495873451233, 0.7953741550445557, -0.41658973693847656, 0.5064282417297363, 0.4223385155200958, 0.7195350527763367, 0.11134883016347885, 0.22250695526599884, -0.2787363827228546, 0.4127429127693176, -0.8056507706642151, -0.3002643585205078, 0.490570992231369, 0.8550428748130798, 0.8463698625564575, 0.37307482957839966, -0.7090052366256714, -0.3215426504611969, -0.040071722120046616, 0.764140784740448, 1.3724448680877686, -0.42875397205352783, -0.26368945837020874, -0.6232182383537292, -0.44264525175094604, -0.46381875872612, 0.2884143888950348, -0.2421373724937439, -0.1425670087337494, -0.6982641816139221, -0.7250462770462036, 0.8195242881774902, 0.889034628868103, 1.2460477352142334, -0.5633777379989624, 0.03246454522013664, -0.22796845436096191, 0.26880326867103577, -0.7959985136985779, -0.743952751159668, 0.5659424066543579, -0.6019812822341919, -0.3407902419567108, 0.31079214811325073, -0.24930539727210999, 0.2427123486995697, -0.6276692152023315, 1.0096980333328247, -0.6400964856147766, 0.09763801842927933, 0.24340203404426575, 0.6027576923370361, -0.8243600726127625, -0.7418293356895447, 0.18415430188179016, -0.09618039429187775, -0.12930771708488464, 0.7361657023429871, 0.5830582976341248, 0.2614690959453583, 0.12703360617160797, -0.2585891783237457, 0.18475419282913208, 0.024157732725143433, 0.10447867214679718, 0.6671269536018372, -0.1863049566745758, -0.23536953330039978, -1.3441230058670044, 0.7442470192909241, 0.03312680870294571, -0.3855020999908447, 0.3131636679172516, -0.1035989373922348, -0.18523630499839783, 0.6767489314079285, -0.3732800781726837, -0.3446374833583832, -0.931316077709198, 0.25747600197792053, -0.3600606918334961, -0.31473448872566223, 0.21289724111557007, -0.02436773292720318, 0.44058099389076233, -0.016173042356967926, 0.32070666551589966, 0.2050338089466095, -0.3098912239074707, 0.637539803981781, -0.963057816028595, 0.5372971296310425, 0.19133193790912628, 0.30197760462760925, -0.36650514602661133, -0.5254555940628052, -0.6233429908752441, -0.29022789001464844, -0.577422559261322, -0.07268869131803513, -0.0920194536447525, 0.3709641396999359, -0.5368484854698181, -0.8225389122962952, 0.3498559296131134, -1.4569145441055298, -0.05708099901676178, 0.1339896023273468, -0.37020307779312134, -0.038211651146411896, -1.3050428628921509, -1.2781891822814941, -0.41181397438049316, -0.7269642949104309, -0.8427044153213501, 0.43497517704963684, 0.09611928462982178, -0.6334758996963501, -0.7219384908676147, -0.27451032400131226, -0.3353705108165741, 1.2410777807235718, -0.9887922406196594, 1.071590781211853, -0.5486080646514893, -0.4822571277618408, -0.1428249031305313, -0.032709505409002304, 0.6805056929588318, -0.27475279569625854, -0.09492363780736923, -1.1033374071121216, 0.35449621081352234, 0.0686487928032875, -0.16436634957790375, 0.33599206805229187, 0.3664073646068573, 0.5898948311805725, -0.013273275457322598, -0.45777997374534607, 0.22311870753765106, 1.190746784210205, -0.47750526666641235, 0.2846430838108063, -0.07270509004592896, 1.1959863901138306, 0.23596502840518951, -0.5086469650268555, 0.15273462235927582, 0.37603726983070374, 0.0004884723457507789, 0.1418260782957077, -0.05482197925448418, -0.2581409811973572, -0.5319698452949524, 0.47036412358283997, 1.6627542972564697, 0.10703050345182419, -0.31152135133743286, -1.1799657344818115, 0.8303965926170349, -1.1947414875030518, -0.6206713318824768, 0.6680179238319397, 0.6107437610626221, 0.30631762742996216, -0.38626599311828613, -0.541803777217865, 0.017557717859745026, 0.5666604042053223, 0.436346173286438, -0.018664978444576263, -0.7425441741943359, 0.07449499517679214, 0.7671971321105957, 0.31809982657432556, 0.7605252265930176, -0.49165597558021545, 0.6256121397018433, 14.770305633544922, 0.7387236952781677, 0.19682632386684418, 0.4868127703666687, 0.6686158776283264, 0.5300635099411011, -0.6017124652862549, -0.13787779211997986, -1.2663640975952148, -0.5099531412124634, 1.2554775476455688, -0.12108541280031204, 0.6415462493896484, 0.0784485787153244, 0.0469251349568367, 0.23335422575473785, -0.6932924389839172, 0.6993005275726318, 0.47360196709632874, -1.017822265625, 0.7334149479866028, -0.05593390390276909, 0.03133445605635643, 0.4750581681728363, 0.481451153755188, 0.5769017338752747, 0.7823643684387207, -0.5928691625595093, 0.3860379755496979, 0.22325770556926727, 0.5485692620277405, -0.002831903053447604, 0.4421132504940033, 0.21822482347488403, -1.233327865600586, -0.45832765102386475, -0.29923000931739807, -1.1105197668075562, 0.21444407105445862, 0.16464611887931824, -0.3159407675266266, -0.7057183980941772, -0.2355249971151352, 1.0184648036956787, 0.2915138006210327, 0.38654080033302307, -0.5615138411521912, 0.5696731805801392, -0.24847431480884552, 0.06724534183740616, 0.09306646883487701, 0.7726435661315918, 0.3767699599266052, -0.15428821742534637, -0.11530031263828278, 0.08642330765724182, 0.25144627690315247, 0.5630931854248047, -0.8058959245681763, -0.21648508310317993, -0.4548891484737396, -0.3093370497226715, -0.22023868560791016, 0.7187430262565613, 0.13397902250289917, 0.38925790786743164, -0.5557512640953064, 0.16986127197742462, 0.48810309171676636, 0.2682458758354187, -0.11310596019029617, -0.1615140289068222, 0.27617478370666504, -0.1915338635444641, 0.2645615339279175, 0.6953538060188293, -0.0845043882727623, -0.4509972333908081, -0.9118230938911438, -0.4144708514213562, 0.5284491181373596, -0.7875713109970093, -0.8696228265762329, 1.0456483364105225, -0.12637026607990265, -0.10367368906736374, 0.266441285610199, -1.0228275060653687, -0.516391932964325, 0.5562055110931396, -1.4111346006393433, -0.822502851486206, 0.29243239760398865, -0.16435827314853668, -0.034501414746046066, 0.06962072104215622, 1.290519118309021, 0.018951181322336197, -0.21565119922161102, -0.1178034171462059, -0.44361379742622375, 0.36800915002822876, -0.3774854242801666, -0.9388978481292725, 1.0957887172698975, 0.6265943050384521, -0.042732302099466324, 0.07217884063720703, -0.19709432125091553, 0.1891358196735382, -0.6527589559555054, -0.03997156769037247, 1.2564576864242554, -0.9389755129814148, -0.13232925534248352, -0.8732746839523315, -0.7514936327934265, 0.630615234375, 0.7485128045082092, -0.4490163326263428, 0.3103486895561218, 0.1599019467830658, -0.7856327295303345, -0.18838024139404297, -0.5198004841804504, -0.04765934869647026, 0.6429550647735596, -0.9935719966888428, -0.4810260534286499, -0.04396762326359749, 0.3106812834739685, -0.8380389213562012, -0.5955906510353088, -0.37688571214675903, 0.10065916925668716, 0.03299984335899353, 0.9916976690292358, -0.45944803953170776, 0.6591827273368835, 0.8464462757110596, -0.1391066461801529, -0.8346548676490784, -0.11990739405155182, -0.91298508644104, 0.28822657465934753, 0.11438145488500595, 0.7615115642547607, -0.4226342737674713, -0.08377257734537125, 0.8762341737747192, 0.1867624670267105, -0.07510389387607574, -0.3057982921600342, -0.06486375629901886, -0.02611367590725422, -0.7702615857124329, 0.3209207355976105, -0.3059076964855194, -0.04815622419118881, 0.18159733712673187, 0.7364239692687988, 0.5526596903800964, -0.252627968788147, -0.7452004551887512, 0.12074451893568039, -0.28107771277427673, -0.05750025436282158, -0.5110188722610474, -0.38156163692474365, -1.4648463726043701, 0.17359420657157898, -1.1268963813781738, 0.2944031059741974, -1.0327023267745972, -0.513821542263031, 0.10187742114067078, -0.18275552988052368, 0.4085615575313568, 0.4416947662830353, -0.39987966418266296, -0.46122729778289795, -0.6877122521400452, -0.5553667545318604, 0.7035843729972839, 0.6374989748001099, -0.7221558094024658, 0.21709811687469482, -0.17043201625347137, -0.12765732407569885, 0.5627403855323792, 0.42259106040000916, -0.2854495048522949, -1.0522252321243286, -1.5972555875778198, 0.08109458535909653, -0.088140107691288, -0.09374228864908218, -0.7299514412879944, 1.0051723718643188, 0.7968024611473083, -0.1784818321466446, 0.16577905416488647, 0.07265916466712952, -1.038948893547058, -0.7115125060081482, 0.21314920485019684, -0.7757614254951477, 0.3541295826435089, 0.5564329624176025, -0.7835608720779419, -0.4754911959171295, 0.8395506739616394, -0.026359010487794876, -1.3997169733047485, -0.943687915802002, 0.3836616575717926, -0.5772753953933716, 0.27618423104286194, -0.4482298791408539, -0.33976036310195923, -0.9786961674690247, -0.25735795497894287, -0.03703339025378227, 0.5941964983940125, -0.5927130579948425, 1.0100706815719604, 0.6678444147109985, -0.6977872252464294, -0.02745014615356922, 0.6895791292190552, -0.1515006721019745, 0.0314469151198864, 0.3683670163154602, 0.26178988814353943, -0.11729405075311661, 0.5159682035446167, 0.16567263007164001, 0.2995365858078003, -1.118679165840149, -0.2803615927696228, 0.9757471084594727, -0.6156627535820007, -0.1501654088497162, 1.2840367555618286, -0.21008671820163727, -0.973680317401886, 0.1009158343076706, -1.2969776391983032, -0.7377502918243408, -0.30206406116485596, 0.9749183058738708, 0.08102084696292877, -0.07992855459451675, -0.09999842196702957, -0.3173593580722809, 0.309293657541275, -0.4803105592727661, -0.48275068402290344, 0.605032742023468, 0.2274787425994873, -0.6132839322090149, 0.4120120406150818, 0.4683431088924408, -0.7205169200897217, -0.30511972308158875, -0.6513983011245728, -0.3170821964740753, 0.00398742500692606, 0.35508906841278076, -0.3301686942577362, -0.9405952095985413, 0.9236740469932556, 0.0803883746266365, 0.30676209926605225, 0.2732781171798706, -0.2061477154493332, 0.4323916435241699, 0.5428208708763123, 0.03169969469308853, -0.3522925078868866, -0.628221869468689, 1.7306584119796753, 1.3085600137710571, -0.6211459040641785, -0.14926424622535706, -0.6523672938346863, -0.44444283843040466, 0.8814609050750732, 0.2897239625453949, -0.22716358304023743, 1.0095373392105103, 0.23331069946289062, 0.062287382781505585, 0.05152083560824394, -0.8717002868652344, -0.27411824464797974, 0.5767307877540588, 1.2179579734802246, 0.7771397233009338, -0.044689469039440155, 0.2929765582084656, 0.8876739740371704, 0.27339839935302734, 0.017602935433387756, 0.18142907321453094, 0.25198468565940857, -0.16219355165958405, -0.17504769563674927, -0.05313470959663391, 0.8147727251052856, -0.4521294832229614, -0.9773064255714417, 0.13353903591632843, 0.6445318460464478, -0.017471887171268463, 0.7884835004806519, 0.8958328366279602, 0.12926393747329712, 0.7650506496429443, 0.38099387288093567, 0.5487870573997498, -0.5353993773460388, -0.5112323760986328, -0.279369592666626, -0.46877655386924744, -0.19294756650924683, -0.27560311555862427, -0.4365464150905609, -0.49875766038894653, -0.2681681215763092, 0.314026802778244, 0.0029600386042147875, 0.01540362648665905, 1.1767221689224243, 0.346133291721344, 0.6327518820762634, -0.7973835468292236, -0.3752523362636566, -0.19959774613380432, -1.0233824253082275, -0.09778418391942978, -0.5022211074829102, -0.2084813117980957, 0.11893589794635773, -0.20461350679397583, -0.2686900198459625]}, "authors": [{"authorId": "1720691070", "name": "Shashank Sonkar"}, {"authorId": "144908066", "name": "Richard Baraniuk"}], "references": [{"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "b5271f4522fd72e335535c5f65d3afc01d1cb2bd", "title": "Very Deep Transformers for Neural Machine Translation"}, {"paperId": "198b42dcc3a6dbd254fa25ab6dd23f3e32592950", "title": "Multi-node Bert-pretraining: Cost-efficient Approach"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "851de6751aef4128d7feb7c6ca36b180a0e0835e", "title": "DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "2115ea50eb12845d6bf9193b969ea144415cd0f1", "title": "Pretrained Transformers for Simple Question Answering over Knowledge Graphs"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "4d8a4509753cc91832f80ec35795064e79630ef3", "title": "Structured Pruning of a BERT-based Question Answering Model"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "55e960535f643637161b2e99a8c21a92c0d13757", "title": "Representation Degeneration Problem in Training Natural Language Generation Models"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "a022bda79947d1f656a1164003c1b3ae9a843df9", "title": "How to Fine-Tune BERT for Text Classification?"}, {"paperId": "c626d5e3d73ef772b4d5934e7d4cc85b6cb22ed6", "title": "Taming Pretrained Transformers for Extreme Multi-label Text Classification"}, {"paperId": "3b9efab2114a3456dccb9d625ca732100d18ba74", "title": "X-BERT: eXtreme Multi-label Text Classification with using Bidirectional Encoder Representations from Transformers"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "ad76c236fe641aa52d1d6c28bf362ae9ffac91e7", "title": "Fine-tuned Language Models for Text Classification"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": null, "title": "Pytorch distributed: Experiences on accelerating data parallel training"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "94238dead40b12735d79ed63e29ead70730261a2", "title": "An Analysis of Encoder Representations in Transformer-Based Machine Translation"}, {"paperId": null, "title": "Understanding back-translation at scale"}, {"paperId": null, "title": "Parallel Attention and Feed-Forward Net Design for Transformer Models"}]}