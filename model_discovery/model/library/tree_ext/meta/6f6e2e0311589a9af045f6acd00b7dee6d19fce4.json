{"paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4", "title": "The Impact of Positional Encoding on Length Generalization in Transformers", "abstract": "Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model's performance. Overall, our work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 86, "influentialCitationCount": 9, "openAccessPdf": {"url": "http://arxiv.org/pdf/2305.19466", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences, and NoPE outperforms other explicit positional encoding methods while requiring no additional computation."}, "embedding": {"model": "specter_v2", "vector": [0.24541738629341125, 0.4848734140396118, 0.1277320832014084, -0.04954977333545685, -0.19264176487922668, -0.3666367530822754, 0.3915160596370697, -0.01138388179242611, -0.5627025365829468, -0.03630314767360687, 0.4711739718914032, -0.454709529876709, 0.06454488635063171, -0.19352911412715912, -0.1846136748790741, 0.07669200003147125, -0.7123732566833496, -0.02242066152393818, -0.020683523267507553, -0.3992261588573456, 0.08253664523363113, -0.35940492153167725, -0.7751777768135071, 0.15702328085899353, 0.41046032309532166, 0.6378281116485596, 0.46169257164001465, 0.8107386827468872, -0.2965078055858612, 0.5702052712440491, 0.16137632727622986, -0.6593012809753418, 0.25775012373924255, 0.0070507703348994255, -0.16176344454288483, -0.5995995998382568, 0.38203421235084534, -0.4587427079677582, -0.6411277651786804, 0.6558762192726135, -0.240502268075943, 0.3020669221878052, 0.2373567521572113, -0.6911769509315491, -0.7452750205993652, 1.230232834815979, 0.9473965167999268, 0.7284170985221863, -0.174686461687088, -0.45245644450187683, 1.6027870178222656, -1.3795511722564697, 0.10840735584497452, 1.1246142387390137, 0.834108293056488, 0.48431915044784546, 0.007328061852604151, -0.5574384331703186, 0.5092264413833618, 0.04018831253051758, -1.063452124595642, -0.2152891904115677, 0.14893224835395813, 0.30492642521858215, 1.7669540643692017, -0.20458200573921204, 0.11176849156618118, 0.18419966101646423, -0.2387869954109192, 1.2653534412384033, 0.05358372628688812, -0.6125592589378357, -0.28288036584854126, -0.07902013510465622, 0.20339831709861755, 1.0032479763031006, -0.43936067819595337, 0.3085961937904358, -0.7687374949455261, 0.17084765434265137, 0.6960248947143555, -0.33582034707069397, 0.0787409320473671, -0.06678766757249832, -0.3579292595386505, 0.337947279214859, 0.25132545828819275, 0.6640986800193787, -0.0031592685263603926, 0.7439653873443604, 0.6612828373908997, 0.47932058572769165, 0.015568725764751434, 0.3189350664615631, -0.3383747935295105, 0.41874292492866516, -0.890272319316864, 0.3241785764694214, -0.3358364403247833, 1.078061819076538, -0.21630938351154327, 0.2952263653278351, -0.7699152827262878, -0.09029563516378403, 1.0591461658477783, -0.02955961786210537, 0.3728191554546356, -0.9720330238342285, 0.5207371711730957, -0.5361149311065674, 0.06667173653841019, -0.31298279762268066, -0.16837283968925476, -0.49223026633262634, -0.4178662896156311, -1.3024276494979858, 0.02851601503789425, 0.21167238056659698, -0.466975599527359, 0.9661709666252136, -0.5837452411651611, 0.09318629652261734, 0.25708070397377014, 0.05140053480863571, 0.7011734247207642, 0.7560999393463135, 0.31095004081726074, 0.07865474373102188, 0.728800892829895, -0.6095277667045593, -0.42127877473831177, -1.0839382410049438, 1.2833693027496338, -0.24739916622638702, 0.5236400365829468, -0.11231690645217896, -1.6420104503631592, -1.1114187240600586, -1.0048598051071167, -0.18879571557044983, -0.5698087811470032, 0.4052695035934448, 0.6951029300689697, 0.5062185525894165, -1.22533118724823, 1.0857338905334473, 0.014815114438533783, -0.15877552330493927, 0.19118230044841766, 0.12779831886291504, 0.3458459675312042, -0.6402849555015564, -1.6084457635879517, 0.30017367005348206, 0.5685219168663025, -0.717273473739624, 0.10518556833267212, -0.9287766814231873, -1.5544073581695557, 0.11010666191577911, 0.24418893456459045, -0.46956002712249756, 1.119731068611145, 0.4744865596294403, -1.2104990482330322, 0.36308857798576355, -0.1688961684703827, -0.08648912608623505, 0.0946982353925705, 0.014049682766199112, -0.2236178070306778, -0.6919834613800049, -0.4043557345867157, 0.5288562178611755, 0.1398053765296936, -0.1748163253068924, -0.3644513189792633, 0.3442325294017792, -0.06625206023454666, 0.1079062968492508, -0.021141016855835915, 1.1085824966430664, 0.13591815531253815, -0.3380972146987915, 0.3625156283378601, 0.4869939088821411, 0.04825130105018616, -0.18745549023151398, 0.009151427075266838, -1.2398501634597778, 0.6339214444160461, -0.16120994091033936, 1.4494571685791016, -0.8760225772857666, -0.5786586999893188, -0.6924307942390442, -0.22189708054065704, -0.17463497817516327, -0.5574263334274292, 0.69496750831604, -0.29897722601890564, 0.4460912048816681, -0.2065761536359787, -1.0584403276443481, 0.3301333487033844, -0.21700792014598846, -0.6094850301742554, -0.13967770338058472, -0.20733477175235748, 1.1613080501556396, -0.9231300354003906, 0.057018402963876724, -0.011739443987607956, 0.13146556913852692, -0.8785529732704163, 1.1701329946517944, -0.47961002588272095, -0.015539542771875858, 0.32813090085983276, -0.5618664026260376, -0.1301557868719101, -0.3673352003097534, 0.03219238296151161, -0.19631193578243256, -0.3811940550804138, 0.9307417869567871, -0.08292455971240997, 1.6336283683776855, -0.347726434469223, 0.27625206112861633, -0.433577299118042, -0.41660669445991516, 0.20559841394424438, 0.41458916664123535, -0.3592904210090637, -0.22631344199180603, 0.23249872028827667, 0.32927051186561584, -0.292158842086792, 0.44503477215766907, 0.7816140651702881, 0.6911789178848267, -0.8764042258262634, 0.2423398494720459, 0.2606477737426758, -0.2587818503379822, 0.5649259686470032, 0.33891788125038147, 0.592247724533081, 0.5785945057868958, 0.45882532000541687, -0.14067023992538452, 0.5408969521522522, -0.9569458961486816, -0.1433350145816803, 0.44850754737854004, 0.8424735069274902, 0.8415317535400391, 0.26597756147384644, -0.6928897500038147, -0.7988824248313904, -0.114198699593544, 0.6804317831993103, 1.7908499240875244, -0.20851504802703857, -0.6299213767051697, -0.7223905920982361, -0.2558356821537018, -0.5405293703079224, 0.5636650323867798, -0.32795023918151855, -0.41485005617141724, -0.8770624399185181, -0.6451697945594788, 0.8488118052482605, 0.7092734575271606, 0.8126786351203918, -0.57258141040802, -0.40162891149520874, -0.13837583363056183, 0.3329288959503174, -0.6554529666900635, -0.701978862285614, 0.2142704725265503, -0.6899971961975098, 0.23768797516822815, -0.06632232666015625, -0.36758914589881897, 0.13548441231250763, -0.8681506514549255, 0.5876879096031189, -0.499407559633255, -0.1339641660451889, 0.028250092640519142, 0.6164695620536804, -0.44026991724967957, -0.9811038970947266, 0.3660563826560974, -0.12041343748569489, -0.6215479969978333, 0.44123202562332153, 0.5795477628707886, -0.20374462008476257, 0.035737961530685425, -0.40381765365600586, 0.24261680245399475, 0.3613643944263458, -0.2376805990934372, 0.3230148255825043, 0.06823979318141937, -0.25439000129699707, -0.942442774772644, 1.1297215223312378, 0.5809172987937927, -0.604177713394165, 0.4184466600418091, -0.8926172256469727, -0.19995257258415222, 0.6996040344238281, -0.5848100781440735, -0.10549327731132507, -0.99278724193573, 0.3072411119937897, -0.13240288197994232, -0.15840063989162445, 0.327696830034256, 0.18024134635925293, 0.3467145562171936, -0.020315047353506088, 0.787174642086029, 0.49982330203056335, -0.1442984938621521, 0.7102689146995544, -0.9766644835472107, 0.2367534190416336, 0.08101721107959747, -0.0016035452717915177, -0.6956307888031006, -0.04661518335342407, -0.766352653503418, -0.4431106150150299, -0.000387646839953959, -0.14916421473026276, 0.08773767948150635, 0.15810561180114746, -0.2814425528049469, -0.6388617753982544, 0.34325331449508667, -1.1225093603134155, -0.36334747076034546, -0.004568895325064659, -0.6260671019554138, -0.16988208889961243, -0.8816871047019958, -1.7082711458206177, -0.2739095091819763, -0.5136469006538391, -1.119621753692627, 0.42393797636032104, -0.27919796109199524, -0.5912627577781677, -0.6145078539848328, -0.4150192141532898, -0.4765845537185669, 0.9428820610046387, -0.8941042423248291, 1.173258900642395, -0.038499582558870316, -0.5193829536437988, 0.012186345644295216, 0.16640661656856537, 0.6720997095108032, -0.25366368889808655, 0.09946886450052261, -0.4256219267845154, 0.11240006983280182, -0.13432013988494873, -0.12391945719718933, -0.09341738373041153, 0.24078042805194855, 0.7867324352264404, -0.60502028465271, -0.3346892297267914, 0.15678216516971588, 1.110961675643921, -0.33127322793006897, 0.5043070912361145, 0.2543131411075592, 0.91959148645401, 0.20452402532100677, -0.201566681265831, 0.5667044520378113, 0.3540346622467041, 0.3152894079685211, -0.1866319477558136, 0.31008681654930115, -0.23733587563037872, -0.6132772564888, 0.7126838564872742, 1.6853384971618652, 0.2064281851053238, -0.08865675330162048, -0.9115048050880432, 0.3458968698978424, -0.9500762224197388, -0.8644784688949585, 0.9393624067306519, 0.893480122089386, 0.6321386694908142, -0.2515183985233307, -0.299897700548172, 0.2913947105407715, 0.20082899928092957, 0.4563583731651306, 0.10672427713871002, -0.5836976766586304, 0.20372167229652405, 0.5573483109474182, 0.36176618933677673, 0.7761261463165283, -0.09831781685352325, 0.8176867961883545, 14.894142150878906, 0.8298177719116211, -0.1611858755350113, 0.26534298062324524, 0.6363716721534729, 0.5141302347183228, -0.33434537053108215, -0.3132220208644867, -1.3059648275375366, 0.06307904422283173, 1.0006390810012817, -0.1842324435710907, 0.48562100529670715, -0.0062696076929569244, -0.1211741715669632, 0.5897777676582336, -0.8461513519287109, 0.5972179174423218, 0.5257617235183716, -1.2029527425765991, 0.4877896308898926, -0.08728993684053421, 0.0181812085211277, -0.1662130355834961, 1.064430832862854, 0.5912615060806274, 0.018995581194758415, -0.5942646265029907, 0.7969107627868652, -0.11397609859704971, 1.0369867086410522, -0.17106372117996216, 0.2616730034351349, 0.4063103199005127, -1.0405768156051636, -0.20894406735897064, -0.7080415487289429, -1.5350255966186523, 0.09127320349216461, -0.23858614265918732, -0.91681307554245, -0.678214430809021, -0.4920692443847656, 1.0286809206008911, 0.1970154494047165, 0.33183959126472473, -0.5332356691360474, 0.6809238791465759, 0.13196125626564026, -0.2244434952735901, 0.23416098952293396, 0.19691142439842224, 0.12039783596992493, -0.28712379932403564, 0.6764745712280273, -0.05734480172395706, -0.06404145807027817, 0.4358653426170349, -0.22554829716682434, -0.05638762563467026, -0.31193017959594727, -0.18919304013252258, 0.19910480082035065, 0.18490396440029144, 0.41862422227859497, 0.2123207151889801, -0.43885502219200134, 0.2898721396923065, 0.6271275281906128, 0.1897440105676651, -0.3798682689666748, -0.0924798846244812, 0.4013406038284302, -0.1921813040971756, 0.23973847925662994, 0.3276229202747345, -0.5817065834999084, -0.03860955312848091, -0.8024291396141052, -0.4096062481403351, 0.3494313657283783, -0.8080301284790039, -0.3090306222438812, 0.7095991373062134, -0.0835740715265274, -0.4489159882068634, 0.5789685249328613, -1.0582842826843262, -0.08208923041820526, 0.3413134515285492, -1.135873556137085, -0.7213897109031677, 0.46912550926208496, -0.44845837354660034, -0.1334911584854126, 0.20654608309268951, 1.2554917335510254, 0.07472028583288193, -0.2696898579597473, 0.2882513999938965, -0.11751630902290344, 0.11070474237203598, -0.3399927318096161, -0.9490689039230347, 0.7137253284454346, 0.3207908570766449, -0.20568709075450897, 0.6482039093971252, 0.16344639658927917, 0.27879077196121216, -0.5976438522338867, -0.2929111123085022, 1.208078384399414, -1.0331144332885742, -0.20253367722034454, -0.8295466303825378, -0.8641266822814941, 0.7177338600158691, 0.370545357465744, -0.595970869064331, 0.24198973178863525, -0.07858630269765854, -0.5032272934913635, -0.33682093024253845, -0.34650909900665283, 0.1449768990278244, 0.9570847153663635, -0.7382985353469849, -0.6222056746482849, -0.37108084559440613, 0.5716885328292847, -1.1830856800079346, -0.5796258449554443, -0.11545707285404205, 0.07933387905359268, -0.1476868987083435, 1.2552810907363892, -0.41883185505867004, 1.0795493125915527, 0.7684841752052307, -0.06132848560810089, -0.7002098560333252, -0.5119609236717224, -0.7726019024848938, 0.25660210847854614, 0.07980003207921982, 1.068233609199524, -0.5743535161018372, 0.33061841130256653, 0.7791640758514404, -0.03259254992008209, -0.481731653213501, -0.45081841945648193, -0.35516881942749023, 0.5762088894844055, -0.6105123162269592, 0.5832911729812622, -0.473768025636673, 0.04805827513337135, 0.4067005217075348, 0.4221930503845215, 0.7429723143577576, -0.32274776697158813, -0.5195656418800354, -0.14687739312648773, 0.15272557735443115, 0.06341303139925003, -0.9761741161346436, -0.5489513874053955, -1.7210018634796143, -0.07733029127120972, -1.2167633771896362, 0.10135094821453094, -1.1999579668045044, -0.5956211686134338, -0.22762098908424377, -0.5056584477424622, -0.0983266681432724, 0.4524001181125641, -0.3068290054798126, -0.8717508316040039, -0.7183236479759216, -0.4059607982635498, 0.7580581307411194, 0.7918376326560974, -0.7098520398139954, 0.557327389717102, 0.12980321049690247, 0.18521781265735626, 0.045975204557180405, 0.5609036087989807, -0.4662852883338928, -1.1098023653030396, -1.4741718769073486, 0.7552089095115662, -0.057614561170339584, -0.3806273937225342, -0.7019618153572083, 0.6304668188095093, 0.4191480576992035, -0.20033472776412964, 0.028517210856080055, 0.3472767770290375, -0.7961317896842957, -0.47303956747055054, 0.4520229697227478, -1.078363299369812, 0.513347864151001, 0.32653164863586426, -0.6971584558486938, 0.04820176586508751, 0.40834009647369385, -0.15620024502277374, -1.1338776350021362, -0.7383236289024353, 0.5859426856040955, -0.7637566328048706, 0.1308136284351349, -0.2191992849111557, -0.3720574676990509, -1.1165672540664673, -0.25133416056632996, 0.03776732459664345, -0.00720533961430192, 0.04474052041769028, 0.8427861928939819, 0.7020319700241089, -1.0745545625686646, 0.09174242615699768, 0.3947441279888153, 0.029539864510297775, -0.016888024285435677, 0.1366322785615921, 0.2396535724401474, -0.5036637187004089, 0.7028123736381531, 0.3966543972492218, 0.6277657151222229, -1.3656426668167114, 0.31161630153656006, 0.33435630798339844, -0.2039225846529007, -0.30096927285194397, 1.4995958805084229, -0.13579079508781433, -0.9884917140007019, 0.1562025249004364, -1.635380744934082, -0.3353678584098816, -0.730909526348114, 0.6237168908119202, 0.46777892112731934, -0.28180834650993347, 0.17135946452617645, -0.22347824275493622, 0.23808975517749786, 0.09705480188131332, -0.3074830174446106, 0.308313250541687, -0.20383235812187195, -0.6093438863754272, 0.6081487536430359, 0.7238610982894897, -0.5130440592765808, -0.7181767225265503, -0.5452141165733337, -0.26051923632621765, -0.04817716032266617, 0.01781480945646763, -0.03468603268265724, -0.29605767130851746, 0.9291272759437561, -0.15390096604824066, 0.21633978188037872, 0.08998846262693405, -0.04225046932697296, 0.45904821157455444, 0.5345456600189209, 0.32695528864860535, -0.3345834016799927, -0.4354422688484192, 1.4720169305801392, 1.2679791450500488, -0.5758525133132935, 0.09792877733707428, -0.3810541331768036, -0.3705865144729614, 0.850719153881073, 0.2156667858362198, 0.2165493667125702, 0.6865888237953186, -0.029181335121393204, 0.0019253178033977747, 0.19434799253940582, -0.90059894323349, 0.17116931080818176, 0.9143326878547668, 0.7063645720481873, 0.9799356460571289, 0.1477642059326172, 0.018930966034531593, 0.8559107780456543, -0.40412434935569763, 0.14209482073783875, 0.3927408456802368, 0.6639431118965149, -0.4017770290374756, -0.19210129976272583, 0.2651645541191101, 0.3924153447151184, -0.6181924939155579, -1.1517056226730347, 0.25371265411376953, 0.512689471244812, 0.10924289375543594, 0.22924356162548065, 0.6096579432487488, 0.1814657747745514, 0.29792943596839905, 0.6025017499923706, 0.9572523832321167, -0.5980408787727356, -0.06559557467699051, -0.6183179616928101, -0.49231570959091187, 0.18878108263015747, 0.15719227492809296, -0.4978172779083252, -0.4249245524406433, -0.40284234285354614, 0.565229594707489, 0.0836729109287262, -0.07543967664241791, 1.05901300907135, 0.36062511801719666, 0.44363150000572205, -0.6267932653427124, 0.0395682267844677, -0.49775931239128113, -0.8384339213371277, 0.1164192408323288, -0.33214449882507324, -0.3509703576564789, 0.2035161852836609, -0.09188119322061539, -0.22627981007099152]}, "authors": [{"authorId": "1754452702", "name": "Amirhossein Kazemnejad"}, {"authorId": "8350409", "name": "Inkit Padhi"}, {"authorId": "1704263", "name": "K. Ramamurthy"}, {"authorId": "1730372", "name": "Payel Das"}, {"authorId": "145732771", "name": "Siva Reddy"}], "references": [{"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "a5cc5edcabba4c9c62cfbc3379daa140084a2a24", "title": "Tracr: Compiled Transformers as a Laboratory for Interpretability"}, {"paperId": "5735e49e501c8e51e9be4079592e46e047747b03", "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "97833e2aa0da5240e62436373b58af988a4ab6ab", "title": "The Curious Case of Absolute Position Embeddings"}, {"paperId": "108c25905be36b2a7a0fc7256ac314985ecd9699", "title": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"}, {"paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "title": "Exploring Length Generalization in Large Language Models"}, {"paperId": "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617", "title": "Neural Networks and the Chomsky Hierarchy"}, {"paperId": "d253beffd28d88cc3150c9e80511a6187ea6613b", "title": "Unveiling Transformers with LEGO: a synthetic reasoning task"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "a2fc77f075f666b462d9350e7576f0ba9845c61b", "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "92173d081b15824d22a9ef070e118744ceee8052", "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"}, {"paperId": "64522a5b3476e9f201f6a5b3e312ef0005c562f1", "title": "SHAPE: Shifted Absolute Position Embedding for Transformers"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "ed535e93d5b5a8b689e861e9c6083a806d1535c2", "title": "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"}, {"paperId": "49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2", "title": "Making Transformers Solve Compositional Tasks"}, {"paperId": "0735fb79bf34698c1df4461a05ed51c232c412e4", "title": "Thinking Like Transformers"}, {"paperId": "7509c66a666e2e3f14bc8676b969b945ee6e136f", "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "829580d6fc73fa601c4982e2b1b6832f2796270b", "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings"}, {"paperId": "f20d66665b65e565f15bf0f0b6442cc1ef04ba18", "title": "From Local Structures to Size Generalization in Graph Neural Networks"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "89cb62dc83c1b1895267bd28639fbf5bb7ed21a4", "title": "Measuring Systematic Generalization in Neural Proof Generation with Transformers"}, {"paperId": "21f74e2617d8d8f5fc117ff2ad6e58a540541f6d", "title": "Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"}, {"paperId": "f63405f53db3b1016f565f555fc8fa409f02fdbd", "title": "Minimum Width for Universal Approximation"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "f3fbcfbb396f6c0391674c8637a373b729e5e531", "title": "Compositionality Decomposed: How do Neural Networks Generalise?"}, {"paperId": "5e35895fc4731858f0b286cb5a1613a819cc2367", "title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text"}, {"paperId": "f89d2da991935549b109d780be3351e0dda92a8f", "title": "Assessing the Ability of Self-Attention Networks to Learn Word Order"}, {"paperId": "3928b2177086532775fbf607ae3e05a0375a5061", "title": "Language Modeling with Deep Transformers"}, {"paperId": "572c3d99e60974aba172ea2871f0af739a44fd8e", "title": "How Efficiency Shapes Human Language"}, {"paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6", "title": "Analysing Mathematical Reasoning Abilities of Neural Models"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "856fe866bcce5e7a540655bea6ecc7406bdcfcba", "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"}, {"paperId": "adc276e6eae7051a027a4c269fb21dae43cadfed", "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "784ee73d5363c711118f784428d1ab89f019daa5", "title": "Hybrid computing using a neural network with dynamic external memory"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "title": "Neural GPUs Learn Algorithms"}, {"paperId": "d0d3f4d1003db0fb637519ef5d8bb140e7df8355", "title": "May the source be with you."}, {"paperId": "2a41acd4c4512aac101466334aedbad4bad10dd5", "title": "Linguistic complexity: locality of syntactic dependencies"}, {"paperId": null, "title": "2023) and relies on the causal attention mask"}, {"paperId": null, "title": "Scaling instruction-finetuned language models. ArXiv, abs/2210.11416"}, {"paperId": "070ff40f38675cfa42a104a545a47584ad823e70", "title": "Scale Efficiently: Insights from Pretraining and Finetuning Transformers"}, {"paperId": null, "title": "A mathematical framework for transformer circuits. Transformer Circuits Thread"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2022b. Chain of thought prompting elicits reasoning in large language models"}, {"paperId": null, "title": "2022. Notes on teaching gpt-3 adding numbers"}, {"paperId": null, "title": "2022. Holistic evaluation of language models"}]}