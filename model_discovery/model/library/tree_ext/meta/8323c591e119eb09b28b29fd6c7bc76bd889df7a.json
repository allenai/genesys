{"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).", "venue": "arXiv.org", "year": 2019, "citationCount": 1364, "influentialCitationCount": 202, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters and shows that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows."}, "embedding": {"model": "specter_v2", "vector": [0.35427701473236084, 0.3383682668209076, -0.26888594031333923, -0.0683087632060051, -0.3279913067817688, -0.10223355889320374, 0.7173462510108948, -0.3950910270214081, -0.5914145112037659, -0.7195682525634766, 0.7043262124061584, -0.34777671098709106, 0.5987153649330139, 0.1147991418838501, 0.03698471188545227, 0.2860884666442871, -0.6543420553207397, 0.6152085661888123, -0.19100983440876007, -0.5012415647506714, -0.35753878951072693, -0.5060403347015381, -1.1729446649551392, 0.39427193999290466, 0.17147505283355713, 0.35831382870674133, 0.1628485471010208, 0.8320444226264954, -0.680343747138977, 0.2831493616104126, 0.6132134199142456, 0.060918666422367096, 0.17775921523571014, 0.27950602769851685, 0.025533724576234818, 0.08360464125871658, 0.30816584825515747, -0.13396088778972626, -0.2900090515613556, 0.5954298973083496, -0.3948393762111664, 0.16556984186172485, 0.2169695496559143, -0.8614858984947205, 0.07592323422431946, 0.9857229590415955, 0.37388747930526733, 0.7281721234321594, -0.7408311367034912, -0.3564355671405792, 0.8866233825683594, -1.2809728384017944, 0.09918826073408127, 1.4809846878051758, 0.5750990509986877, 0.4380840063095093, -0.2773505449295044, -0.7655599117279053, 0.5500097274780273, -0.4288751482963562, -0.8012663722038269, -0.4773064851760864, -0.15547990798950195, -0.1176358237862587, 2.391033887863159, -0.371497243642807, 0.2861208915710449, 0.3028121292591095, -0.16598892211914062, 1.319242238998413, -0.15130256116390228, -0.7364526391029358, -0.7187803387641907, -0.07883619517087936, 0.32315436005592346, 1.026494026184082, -0.5201380848884583, 0.2855684757232666, -1.0547345876693726, -0.14041732251644135, 0.3702074885368347, -0.24847948551177979, 0.21238625049591064, 0.07615730911493301, -0.5746991038322449, 0.8252202868461609, 0.1616455316543579, 0.6857563853263855, -0.12651197612285614, 0.8121955394744873, 0.7902229428291321, 0.4320354163646698, 0.18315310776233673, 0.26336097717285156, -0.11487649381160736, 0.2624591588973999, -1.058197021484375, 0.030902709811925888, 0.11644896119832993, 0.7754764556884766, -0.22290337085723877, 0.43415015935897827, -0.7709804177284241, 0.1845797449350357, 1.3572956323623657, 0.28727617859840393, 0.46601545810699463, -0.646832287311554, 0.5480035543441772, -0.7365634441375732, -0.14649079740047455, -0.0774366557598114, -0.4281546473503113, -0.4742690622806549, -0.6076778173446655, -1.3238604068756104, -0.3823462724685669, 0.038513485342264175, -1.1188547611236572, 0.7378826141357422, -0.07221979647874832, 0.07879746705293655, 0.2592543065547943, 0.2219189554452896, 0.6022413372993469, 0.8370571136474609, 0.7350051403045654, 0.3761812150478363, 1.194223165512085, -1.0641659498214722, -0.3380161225795746, -1.1803237199783325, 0.9485292434692383, -0.4287070035934448, 0.2824333906173706, -0.06344075500965118, -1.274361491203308, -0.5381538271903992, -0.5414746999740601, -0.09104911237955093, -0.5130205750465393, 0.31412193179130554, 1.260990023612976, 0.5572136044502258, -1.0049601793289185, 0.6632046103477478, -0.22944045066833496, -0.19170011579990387, 0.23647435009479523, 0.27664506435394287, 0.21069689095020294, -0.27228885889053345, -1.2122282981872559, 0.22130189836025238, 0.4940089285373688, -0.4879196286201477, -0.03712910786271095, -1.1366662979125977, -0.945152223110199, 0.11282835155725479, -0.03119509667158127, -0.4948045611381531, 1.3522664308547974, -0.10101489722728729, -1.3981363773345947, 0.8996277451515198, -0.4262336194515228, 0.10600784420967102, -0.01174925360828638, 0.05873323231935501, -0.6129741668701172, -0.6782333254814148, -0.5472068786621094, 0.7212610244750977, 0.40825188159942627, 0.21337682008743286, -0.2846775949001312, 0.18525148928165436, -0.39533910155296326, 0.34049704670906067, -0.4768395721912384, 1.4274601936340332, -0.5782590508460999, 0.024726131930947304, 0.3390423059463501, 0.5589078664779663, -0.10315877199172974, -0.32787057757377625, -0.4342358708381653, -0.8726616501808167, 0.689357340335846, -0.043264202773571014, 0.6990731954574585, -1.0185720920562744, -0.7085235118865967, 0.08773191273212433, 0.08068808913230896, 0.09568700939416885, -0.8044399619102478, 0.7384442090988159, -0.5305456519126892, 0.3503648340702057, -0.15290184319019318, -1.3301897048950195, 0.13218149542808533, -0.06635447591543198, -0.593093752861023, -0.4569052755832672, -0.02175625041127205, 1.1124777793884277, -0.6205505728721619, 0.09593205153942108, -0.014549913816154003, 0.559360682964325, -0.8088220357894897, 1.0066028833389282, -0.4800468385219574, 0.06476689130067825, 0.1732974499464035, -0.44469332695007324, 0.103069007396698, -0.3959497809410095, 0.4890681207180023, -0.5818383693695068, -0.3439634442329407, 0.5065030455589294, -0.1945960819721222, 1.289946436882019, -0.5101586580276489, 0.2109922468662262, 0.090237557888031, -0.5668478012084961, 0.1034044399857521, 0.3607240915298462, -0.327206552028656, -0.6904250979423523, 0.41895487904548645, 0.7253860831260681, -0.3104702830314636, 0.5560482144355774, 0.8157411813735962, 0.47803688049316406, -0.28470587730407715, 0.265570729970932, 0.5766095519065857, -0.38905343413352966, 0.7688086628913879, 0.41972053050994873, 0.43060773611068726, 0.2065461128950119, -0.03646720200777054, -0.21433749794960022, 0.24574175477027893, -0.6351849436759949, -0.15440194308757782, 0.09414064139127731, 0.8658335208892822, 0.4532902240753174, 0.5744432210922241, -0.6047763824462891, -0.6900599598884583, -0.00578317791223526, 0.7320092916488647, 1.650999903678894, -0.7074332237243652, -0.0334373340010643, -0.619461715221405, 0.051593929529190063, -0.2570232152938843, 0.06508230417966843, -0.06694748997688293, 0.03729036822915077, -0.9134561419487, -1.2333343029022217, 0.9866088628768921, 0.08178754895925522, 0.8016021847724915, -0.37529417872428894, -0.4801200330257416, -0.5430224537849426, 0.37020549178123474, -1.0707275867462158, -0.5364129543304443, 0.6551562547683716, -0.9093459248542786, 0.10904531925916672, 0.002690883819013834, -0.13252775371074677, 0.19437552988529205, -0.5221508145332336, 1.1145740747451782, -0.3993840217590332, -0.2956060469150543, -0.04835532233119011, 0.8678223490715027, -0.4804899990558624, -1.1030635833740234, 0.1686689257621765, 0.06396237015724182, -0.44153866171836853, 0.43166935443878174, 0.3848441541194916, 0.381844162940979, -0.16269926726818085, -0.3134821951389313, 0.7426426410675049, -0.01823909766972065, -0.13410761952400208, 0.5183518528938293, -0.44618839025497437, -0.4931829571723938, -1.468247890472412, 1.090160608291626, 0.2734586298465729, -0.6891512274742126, 0.4078129827976227, -0.48772385716438293, -0.023623455315828323, 0.6936816573143005, -0.5804604291915894, -0.2545146644115448, -0.9785305261611938, 0.28918588161468506, -0.4201091527938843, -0.025694793090224266, 0.23576048016548157, 0.48086920380592346, 0.2117982804775238, 0.16774795949459076, 0.377440482378006, 0.5408148765563965, -0.07594627887010574, 0.6803593635559082, -0.8680720925331116, 0.3347374498844147, 0.24654559791088104, 0.3452691435813904, -0.2646365165710449, -0.15451577305793762, -0.7480051517486572, -0.4227527379989624, -0.6472305655479431, -0.04172073304653168, -0.10122395306825638, 0.12937210500240326, -0.7225927710533142, -0.826824963092804, 0.21169820427894592, -1.1991815567016602, -0.38813820481300354, 0.4350484311580658, -0.2852862477302551, 0.04285481572151184, -1.0993677377700806, -1.4249637126922607, -0.2584214210510254, -1.0180745124816895, -1.1325067281723022, 0.615816056728363, 0.12303099036216736, -0.16098587214946747, -0.7384243607521057, 0.01682380400598049, -0.36502325534820557, 1.0935732126235962, -0.804932177066803, 0.925507664680481, 0.058881331235170364, 0.07902856171131134, 0.04835968092083931, -0.10803840309381485, 0.28823524713516235, -0.7395965456962585, 0.43253397941589355, -0.8572806715965271, 0.35862234234809875, -0.8997010588645935, -0.44941920042037964, 0.31016701459884644, 0.21210211515426636, 0.485866904258728, 0.335328072309494, -0.4734441041946411, 0.5684670209884644, 1.4597420692443848, -1.0680601596832275, 0.21640589833259583, -0.10032135248184204, 1.1497745513916016, 0.05037965625524521, -0.7387957572937012, 0.36059287190437317, 0.38556337356567383, 0.4189108610153198, 0.10923371464014053, -0.356437623500824, -0.16919320821762085, -0.49517932534217834, 0.5710216760635376, 1.6151171922683716, 0.6143653392791748, -0.17150475084781647, -1.2799628973007202, 0.32828834652900696, -0.9376789927482605, -0.5034760236740112, 0.20894844830036163, 0.7132207751274109, 0.4018007218837738, -0.43674078583717346, -0.14314019680023193, -0.31434983015060425, 0.4257707893848419, 0.29106494784355164, -0.28440359234809875, -1.2036830186843872, -0.014188298024237156, 0.48271051049232483, 0.6166955828666687, 0.611139714717865, -0.11920401453971863, 1.0420509576797485, 14.646129608154297, 0.8948859572410583, -0.3045573830604553, 0.616915225982666, 0.5863707661628723, 0.11654487252235413, -0.7124865651130676, -0.3458293378353119, -1.545060634613037, -0.3611741065979004, 1.5380332469940186, 0.25077366828918457, 0.9464357495307922, -0.09216148406267166, 0.17108950018882751, 0.10774972289800644, -0.20924942195415497, 0.7046096920967102, 0.30024364590644836, -1.2954176664352417, 0.29703468084335327, 0.1976693719625473, 0.2741055488586426, 1.088417410850525, 0.7298990488052368, 0.8624451160430908, 0.7585443258285522, -0.6987427473068237, 0.34647923707962036, 0.04658563807606697, 0.8383723497390747, -0.16522331535816193, -0.002423984929919243, 0.6827653646469116, -0.8423326015472412, -0.11190537363290787, -0.4783436357975006, -1.1410404443740845, 0.426414430141449, 0.3401232361793518, -0.8750612139701843, -0.5438944101333618, -0.43120551109313965, 0.6229350566864014, 0.44717174768447876, 0.2588631808757782, -0.1290522813796997, 0.6437890529632568, -0.8446040153503418, 0.1044759750366211, 0.5225254893302917, 0.35357728600502014, -0.01111513376235962, 0.07466937601566315, 0.14545133709907532, -0.2044813334941864, 0.12140772491693497, 0.422067254781723, -0.5692142844200134, -0.09768764674663544, -0.17553265392780304, -0.5027690529823303, 0.02328534796833992, 1.1068661212921143, 0.3177051544189453, 0.22665166854858398, -0.47262346744537354, 0.14452213048934937, 0.9938960075378418, -0.015483873896300793, -0.3170892000198364, 0.5378461480140686, 0.4422684609889984, -0.5648648738861084, 0.020207460969686508, 0.24363815784454346, 0.004549289122223854, -0.46315744519233704, -0.9363418817520142, -0.630681037902832, 0.34176144003868103, -0.5680114030838013, -0.45926934480667114, 0.6126300096511841, -0.18177196383476257, -0.1317327469587326, 0.36117154359817505, -1.0663409233093262, -0.312287837266922, 0.734153687953949, -1.6681852340698242, -0.9498422145843506, 0.4974506199359894, -0.24885886907577515, -0.5374311804771423, 0.022896461188793182, 1.4631613492965698, 0.1654980480670929, -0.4494064152240753, 0.10987935215234756, 0.14597220718860626, -0.09516943991184235, -0.680031955242157, -0.34322744607925415, 1.191135287284851, 0.22391606867313385, -0.00640750490128994, 0.11941035091876984, -0.05874917283654213, 0.17055559158325195, -1.0962857007980347, -0.03725133836269379, 1.0606766939163208, -0.49848365783691406, -0.02990785799920559, -0.9482234716415405, -0.487425297498703, 0.22856362164020538, 0.4260929524898529, -0.30737248063087463, 0.47071629762649536, 0.2980891466140747, -0.9841005206108093, 0.15817922353744507, -0.543257474899292, 0.19050124287605286, 0.5609750151634216, -0.7559311985969543, 0.24542176723480225, 0.057169344276189804, 0.4895979166030884, -1.242444396018982, -0.7641943693161011, -0.32554152607917786, 0.21389570832252502, 0.03452380374073982, 0.853834331035614, -0.3147025406360626, 0.7346180081367493, 1.0333852767944336, -0.16158483922481537, -0.6479331254959106, 0.3525581657886505, -0.7457794547080994, 0.14990030229091644, -0.17624321579933167, 0.8766076564788818, -0.334622323513031, 0.23117142915725708, 0.8770356178283691, 0.0907340943813324, -0.39550840854644775, -0.4588634669780731, -0.25962918996810913, 0.2515268623828888, -0.7549730539321899, 0.4099363088607788, -0.0730980783700943, -0.09762558341026306, 0.21678461134433746, 0.3868274390697479, 0.4351062774658203, -0.13961748778820038, -0.7961915135383606, 0.4021100103855133, -0.039291225373744965, -0.26044929027557373, -0.6654914021492004, -0.31356292963027954, -1.165343165397644, 0.13974401354789734, -1.385805606842041, -0.09113237261772156, -0.8305516839027405, -0.2584914565086365, -0.21283943951129913, 0.11541237682104111, 0.33312222361564636, 0.46437570452690125, -0.241696298122406, -0.429269939661026, -0.6087601184844971, -0.2687414884567261, 0.7715269327163696, 0.9115483164787292, -0.4888288676738739, 0.20620225369930267, -0.43148595094680786, 0.2778775691986084, 0.24971705675125122, 0.5691389441490173, -0.188977912068367, -0.8970520496368408, -1.6429755687713623, 0.6432410478591919, -0.032985489815473557, -0.2576445937156677, -0.5856040120124817, 0.5418189167976379, 0.44752737879753113, -0.5689424276351929, 0.3188633918762207, 0.4225368797779083, -0.5358346104621887, -0.44228774309158325, 0.37574782967567444, -0.5736334919929504, 0.43812689185142517, 0.8388626575469971, -0.8561819195747375, -0.020963456481695175, 0.7546700835227966, -0.032253362238407135, -1.14728581905365, -1.0056066513061523, 0.37843215465545654, -0.656524121761322, 0.22339026629924774, -0.5275322198867798, -0.04027209058403969, -1.0447627305984497, -0.0034709570463746786, 0.25255000591278076, 0.5356488227844238, -0.3381134271621704, 0.8791964054107666, 0.3664286136627197, -0.7060503959655762, -0.00794010329991579, 0.31192758679389954, -0.15261311829090118, -0.22673669457435608, 0.5336219072341919, 0.5300812721252441, -0.4899018108844757, 0.6582540273666382, 0.1866743415594101, 0.12716907262802124, -1.1378443241119385, -0.3176884651184082, 0.8850691318511963, -0.9330503940582275, -0.37982380390167236, 1.3750206232070923, -0.33555689454078674, -1.5159127712249756, -0.1379162222146988, -1.1180161237716675, -0.5041230916976929, -0.5201693177223206, 0.5749227404594421, -0.15446719527244568, 0.2631378769874573, -0.09783481061458588, -0.3733085095882416, 0.09184182435274124, -0.06507639586925507, -0.6428709030151367, 0.45926401019096375, 0.2722785174846649, -0.4017690122127533, 0.6050876975059509, 0.8606728911399841, -0.8455160856246948, -0.2840212881565094, -0.7206065654754639, -0.4739510715007782, 0.4212963879108429, 0.6895721554756165, -0.32651692628860474, -0.6728211641311646, 0.90227210521698, 0.22025921940803528, -0.11347734928131104, 0.023179804906249046, -0.5451788902282715, 0.4580402672290802, 0.507013738155365, 0.04453553259372711, -0.7081581354141235, -0.8860923051834106, 1.8106757402420044, 0.7187445163726807, -0.7313136458396912, 0.044539809226989746, -0.49926692247390747, -0.8400810360908508, 0.8666077256202698, 0.08247659355401993, 0.06826823204755783, 1.000423789024353, 0.18343797326087952, 0.23160861432552338, 0.025145333260297775, -1.0399394035339355, -0.31400537490844727, 0.7927683591842651, 0.6033819317817688, 1.0984009504318237, 0.39266905188560486, 0.11651889979839325, 0.4102124869823456, 0.08611655980348587, 0.15197664499282837, 0.2455083280801773, 0.3452979028224945, -0.14883953332901, -0.23893311619758606, 0.07366332411766052, 0.7386096119880676, -0.365349143743515, -1.2118713855743408, 0.42661502957344055, 0.40602076053619385, -0.014788735657930374, 0.4342072010040283, 1.1012392044067383, 0.1832273006439209, 0.32575875520706177, 0.3693876564502716, 0.7777035236358643, -0.6310902833938599, -0.6291420459747314, -0.11260546743869781, -0.13863864541053772, -0.017526842653751373, 0.0011352276196703315, -0.4040340483188629, -0.7749187350273132, -0.5835050344467163, 0.5457988977432251, -0.14123211801052094, 0.49988579750061035, 1.3618171215057373, 0.39424002170562744, 0.4768655300140381, -0.543212354183197, -0.4284513294696808, -0.37761610746383667, -0.8268943428993225, -0.13459506630897522, -0.7178326845169067, -0.6684137582778931, 0.06769385933876038, 0.14267298579216003, -0.5988962650299072]}, "authors": [{"authorId": "1911755", "name": "M. Shoeybi"}, {"authorId": "66870756", "name": "M. Patwary"}, {"authorId": "41158993", "name": "Raul Puri"}, {"authorId": "3081566", "name": "P. LeGresley"}, {"authorId": "48991386", "name": "J. Casper"}, {"authorId": "2301680", "name": "Bryan Catanzaro"}], "references": [{"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "cd63025532a62fa245a02ec05e32ac4d23089631", "title": "Dynamic Evaluation of Transformer Language Models"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "3c6dca9041f54583aeab60587c9e6e9272104dc1", "title": "Reducing BERT Pre-Training Time from 3 Days to 76 Minutes"}, {"paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "d3b0d4b2ea111f9b560815487e65c2619d9cf15d", "title": "Efficient and Robust Parallel DNN Training through Model Parallelism on Multi-GPU Platform"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "ad76c236fe641aa52d1d6c28bf362ae9ffac91e7", "title": "Fine-tuned Language Models for Text Classification"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "1e3d18beaf3921f561e1b999780f29f2b23f3b7d", "title": "Large Batch Training of Convolutional Networks"}, {"paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "title": "Learned in Translation: Contextualized Word Vectors"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "664ec878de4b7170712baae4a7821fc2602bba25", "title": "Learning to Generate Reviews and Discovering Sentiment"}, {"paperId": "85f94d8098322f8130512b4c6c4627548ce4a6cc", "title": "Unsupervised Pretraining for Sequence to Sequence Learning"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "8ec5896b4490c6e127d1718ffc36a3439d84cb81", "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"paperId": "59761abc736397539bdd01ad7f9d91c8607c0457", "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM"}, {"paperId": "e2dba792360873aef125572812f3673b1a85d850", "title": "Enriching Word Vectors with Subword Information"}, {"paperId": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "title": "Communication Efficient Distributed Machine Learning with the Parameter Server"}, {"paperId": "50684b147b752a07c313cb73d864f7b21bd8b703", "title": "Scaling Distributed Machine Learning with the Parameter Server"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "77dfe038a9bdab27c4505444931eaa976e9ec667", "title": "Empirical Evaluation and Combination of Advanced Language Modeling Techniques"}, {"paperId": "8492070dc4031ed825e95e4803781752bb5e909f", "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "8665c9b459e4161825baf1f25b5141f41a5085ff", "title": "A bridging model for parallel computation"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Layernorm"}, {"paperId": null, "title": "Evaluating Language Models Using WikiText103 and LAMBADA In this section we detail our evaluation methodology for the WikiText103 dataset (Merity et al., 2016) and cloze-style"}, {"paperId": null, "title": ": Large-scale machine learning on heterogeneous systems"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Task Model Batch Learning Training size rate epochs 336M MNLI 1.3B 128 1e-5 10"}, {"paperId": null, "title": "instance of the model distributed across these GPUs. The remaining GPUs, which could be within the same server but more typically are located in other servers, run additional model parallel groups."}, {"paperId": null, "title": "of the model parallel groups (for example GPUs 1 505 in Figure 8) form data parallel groups so that all within a data parallel group hold the same model"}, {"paperId": null, "title": "Code 1. Implementation of f operator. g is similar to f with identity in the backward and all-reduce in the forward functions"}, {"paperId": null, "title": "Table 6. Hyperparameters for \ufb01netuning BERT down-stream tasks"}, {"paperId": null, "title": "Training Multi-Billion Parameter Language Models Using Model Parallelism"}]}