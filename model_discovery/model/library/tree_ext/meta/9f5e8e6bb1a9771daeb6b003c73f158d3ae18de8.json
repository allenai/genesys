{"paperId": "9f5e8e6bb1a9771daeb6b003c73f158d3ae18de8", "title": "Save It All: Enabling Full Parameter Tuning for Federated Large Language Models via Cycle Black Gradient Descent", "abstract": "The advent of large language models (LLMs) has revolutionized the deep learning paradigm, yielding impressive results across a wide array of tasks. However, the pre-training or fine-tuning of LLMs within a federated learning (FL) framework poses substantial challenges, including considerable computational and memory resource demands, as well as communication bottlenecks between servers and clients. Existing solutions either make the unrealistic assumption that the entire model is exchanged for training, or apply parameter-effective fine-tuning methods from centralized learning to train LLMs in FL which tend to underperform during training or fine-tuning stages due to the limited search subspace of parameter updating. In this paper, we introduce a novel method for the efficient training and fine-tuning of LLMs in FL, with minimal resource consumption. Our approach, termed FedCyBGD, utilizes Cycle Block Gradient Descent to periodically update the model. In particular, we design a compression scheme for FedCyBGD, aiming to further decrease the model download cost. It enables full parameter training in FL with only selected block updates and uploads, thereby reducing communication, computation, and memory costs. Our method achieves state-of-the-art performance for FL LLM training, while significantly reducing associated costs. Codes are provided here.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A novel method for the efficient training and fine-tuning of LLMs in FL, with minimal resource consumption, is introduced, termed FedCyBGD, that enables full parameter training in FL with only selected block updates and uploads, thereby reducing communication, computation, and memory costs."}, "embedding": {"model": "specter_v2", "vector": [-0.16438478231430054, 0.029644353315234184, -0.7191237807273865, 0.12680543959140778, -0.16642655432224274, -0.13766583800315857, 0.7959707379341125, -0.6141880750656128, -0.4430035948753357, 8.457231160718948e-06, 0.17542482912540436, -0.32986631989479065, 0.436931848526001, 0.15544873476028442, -0.3383081257343292, 0.06304788589477539, -0.842884361743927, 0.5614428520202637, -0.14071553945541382, -0.15220041573047638, -0.46648716926574707, -0.30571961402893066, -1.132568120956421, -0.1599699705839157, 0.27701494097709656, 0.7442943453788757, 0.031174587085843086, 0.9716095328330994, -0.6885026097297668, 0.1483389139175415, 0.6350138187408447, -0.23493312299251556, 0.43366461992263794, -0.08780866116285324, -0.47340118885040283, 0.17958901822566986, 0.009036935865879059, -0.6846697926521301, -0.44416484236717224, 0.7448468208312988, -0.11894288659095764, 0.39789333939552307, 0.018824758008122444, -0.8630061745643616, 0.12557126581668854, 0.2749900817871094, 0.5204627513885498, 0.3934749960899353, -0.7052313089370728, -0.20980603992938995, 0.8608967065811157, -1.3770103454589844, 0.20931889116764069, 1.4544090032577515, 0.43215346336364746, 0.8812955021858215, -0.434649795293808, -0.9724100232124329, 0.6426864266395569, -0.1014733612537384, -0.5060061812400818, -0.4849037528038025, -0.3979574739933014, -0.3089963495731354, 2.0190680027008057, -0.3908439576625824, -0.06354963779449463, 0.21029295027256012, -0.22032740712165833, 1.082940936088562, -0.38790363073349, -0.7792714238166809, -0.1786339432001114, -0.023749511688947678, 0.018733138218522072, 0.8360716104507446, 0.004975549411028624, 0.3817282021045685, -0.9401032328605652, -0.5232530236244202, 0.06431123614311218, -0.022119564935564995, 0.20170684158802032, -0.5197657346725464, 0.29876092076301575, 1.0082906484603882, 0.09830870479345322, 0.37428420782089233, 0.1556556224822998, 1.1608332395553589, 0.7340250611305237, 0.3064921498298645, 0.4585992693901062, 0.08214964717626572, -0.33474159240722656, 0.11207553744316101, -0.6547948122024536, 0.23688678443431854, 0.46078434586524963, 0.5859672427177429, 0.08526966720819473, -0.026539789512753487, -0.5833053588867188, 0.4946129024028778, 1.3886882066726685, -0.18241475522518158, 0.4777313768863678, -0.5785690546035767, 0.7499685883522034, -0.5982993841171265, 0.12666307389736176, -0.2932060658931732, -0.08806679397821426, -0.12361839413642883, -0.9022746682167053, -1.2393244504928589, -0.7844908833503723, -0.07694824039936066, -0.6336614489555359, 0.7574384212493896, -0.05530501902103424, 0.3027994632720947, -0.17214559018611908, 0.36488786339759827, 0.4695086181163788, 0.5069214105606079, 0.3243909478187561, 0.21431124210357666, 0.7007805109024048, -1.4711755514144897, -0.26056811213493347, -1.3485283851623535, 0.6070349216461182, -0.15712477266788483, 0.23955100774765015, -0.21161502599716187, -1.1855583190917969, -0.7681896090507507, -0.9015149474143982, 0.299020916223526, -0.7337625026702881, 0.13986444473266602, 1.1510573625564575, 0.6503495573997498, -0.8433423638343811, 0.9252737164497375, -0.4652601182460785, -0.22637876868247986, 0.4080617427825928, 0.37135615944862366, 0.20553946495056152, -0.5566717386245728, -1.4527462720870972, 0.07159900665283203, 0.20342303812503815, -0.6900410056114197, -0.4760981798171997, -0.4840017855167389, -0.5381423234939575, -0.14558279514312744, 0.08810418099164963, -0.6529814600944519, 1.4525662660598755, -0.1647898405790329, -1.2343380451202393, 0.745783805847168, -0.06647257506847382, -0.013411853462457657, 0.9055832028388977, -0.02407514490187168, -0.6142735481262207, -0.3054496645927429, -0.6092492341995239, 0.5062779188156128, 0.9669994711875916, 0.19258928298950195, 0.11456912010908127, 0.3941851556301117, -0.1757688671350479, -0.03525685891509056, -0.768293559551239, 0.7296273708343506, -1.263843297958374, -0.2883399724960327, 0.42107272148132324, 0.49705609679222107, -0.1057494655251503, 0.05742621421813965, -0.20631076395511627, -0.7600889801979065, 1.0128889083862305, -0.045568257570266724, 1.0164774656295776, -0.6674075722694397, -1.042617917060852, 0.1294354349374771, -0.21619433164596558, 0.19162064790725708, -0.8659432530403137, 0.7872536778450012, -0.36189138889312744, 0.5443741679191589, 0.10675475746393204, -1.2418466806411743, 0.25546011328697205, -0.22712430357933044, -0.6509442329406738, 0.16288720071315765, -0.033599335700273514, 0.9634301662445068, -0.513131320476532, 0.3621513843536377, -0.352378249168396, 0.45013636350631714, -1.3168315887451172, 1.28443443775177, -0.6515486836433411, 0.2637840509414673, -0.16404999792575836, -0.3704448938369751, 0.4416508078575134, -0.55605548620224, 0.680863082408905, -0.31745877861976624, 0.21250446140766144, 0.6401627063751221, -0.3787100613117218, 1.3642971515655518, -0.533538281917572, 0.18287347257137299, 0.38470691442489624, -0.45089730620384216, 0.1610577255487442, 0.4963892102241516, 0.021923692896962166, -0.3129652142524719, 0.24596314132213593, 0.5513961911201477, -0.8826414346694946, 0.6243427991867065, 0.6705428957939148, 0.7344211935997009, -0.06018500030040741, 0.7818810343742371, 0.39556562900543213, -0.3838678002357483, 0.7987781763076782, 0.35406211018562317, 0.4840069115161896, 0.42760610580444336, 0.11424803733825684, 0.2820698320865631, 0.805719792842865, -1.0302141904830933, -0.28061145544052124, 0.5243571400642395, 0.3892059028148651, 0.33919402956962585, 0.38428422808647156, -0.7466290593147278, -0.26510775089263916, 0.3644501566886902, 0.6957401633262634, 1.5448411703109741, -0.4211123287677765, -0.06428363919258118, -0.9637998938560486, -0.5721829533576965, 0.2098657488822937, -0.42507606744766235, -0.16372327506542206, -0.2500790059566498, -0.30434152483940125, -1.4390016794204712, 0.6300694942474365, -0.2373128980398178, 1.1957224607467651, -0.3591017425060272, -0.04228391870856285, -0.22785551846027374, 0.2710455656051636, -0.6719232797622681, -0.7749655246734619, 0.5420206189155579, -0.6935169696807861, -0.29298800230026245, 0.13553357124328613, 0.050638530403375626, 0.1813613921403885, -0.47670358419418335, 1.0981569290161133, -0.2385229468345642, -0.3314858078956604, 0.11514941602945328, 0.6730079054832458, -0.18458978831768036, -1.1940653324127197, 0.41954508423805237, 0.27155622839927673, -0.08392104506492615, 0.27977803349494934, 0.6387309432029724, 0.038634829223155975, 0.2545734941959381, -0.24963009357452393, 0.3138912320137024, -0.19433294236660004, 0.04140472039580345, 0.7000425457954407, -0.5600460767745972, 0.13385704159736633, -1.4661388397216797, 1.0369288921356201, 0.0010635450016707182, -0.6894444227218628, 0.10057953000068665, -1.0765950679779053, -0.19771237671375275, 0.5698843598365784, -0.3925338387489319, -0.3647059500217438, -0.8690745830535889, 0.20524120330810547, -0.6485287547111511, 0.05039457231760025, -0.20575068891048431, 0.4476342797279358, -0.23514869809150696, 0.12441422790288925, 0.31867390871047974, 0.6426182985305786, -0.6906588673591614, 0.706323504447937, -0.7835758328437805, 0.3508562743663788, 0.07496410608291626, 0.14285534620285034, -0.041715990751981735, -0.3337382376194, 0.04183882847428322, 0.09523769468069077, -0.5119531154632568, -0.20733894407749176, -0.191054105758667, -0.033998601138591766, -1.0041333436965942, -0.6457555890083313, -0.06133886054158211, -0.9051125049591064, -0.383438378572464, 0.4428876042366028, -0.12942004203796387, -0.4482372999191284, -0.9974308609962463, -1.3412647247314453, -0.18676510453224182, -1.2973014116287231, -1.0110427141189575, -0.03994256258010864, -0.1333511620759964, -0.1264391988515854, -0.4311521053314209, 0.12129847705364227, -0.3194405734539032, 1.0885941982269287, -1.052085518836975, 1.13197660446167, 0.047882985323667526, -0.18963322043418884, -0.4686696231365204, -0.14550086855888367, 0.5131111741065979, -0.40886494517326355, -0.11161120980978012, -1.0489813089370728, -0.4671006500720978, -0.6087070107460022, -0.4579521715641022, 0.3402441143989563, 0.04727854207158089, 0.9744482636451721, -0.12667591869831085, -0.6559427976608276, 1.194980502128601, 1.6414663791656494, -1.0247693061828613, -0.36430221796035767, 0.21323202550411224, 0.9097988605499268, -0.16902881860733032, -0.5920748114585876, 0.822971761226654, -0.08649390935897827, 0.4038006067276001, 0.18353547155857086, -0.18243660032749176, -0.5005378127098083, -0.6345220804214478, 0.3114326596260071, 1.627267837524414, 0.8050928115844727, 0.056192100048065186, -0.8574486374855042, 0.46025338768959045, -1.083847999572754, -0.3632234036922455, 0.8733798861503601, 0.8276889324188232, 0.34920355677604675, -0.44299396872520447, -0.3304237425327301, -0.49915260076522827, 0.526317834854126, 0.623721718788147, -0.8975167870521545, -1.1403459310531616, 0.23700839281082153, 0.17743338644504547, 0.5207018852233887, 0.11071594059467316, -0.5453393459320068, 0.43515899777412415, 14.610267639160156, 1.089616060256958, 0.14875271916389465, 0.9988497495651245, 0.8471141457557678, -0.14681848883628845, -0.3772505819797516, -0.33230361342430115, -1.3716609477996826, -0.07663564383983612, 1.476965308189392, 0.06117591634392738, 1.0474354028701782, 0.3669116199016571, 0.34287160634994507, 0.25772133469581604, -0.12573325634002686, 0.8346037268638611, 0.2985832393169403, -1.4404741525650024, 0.37241360545158386, 0.08464431762695312, 0.7768410444259644, 1.1912097930908203, 0.5461763739585876, 0.9245534539222717, 0.7241417765617371, -0.4552339017391205, 0.3741094470024109, 0.07049304246902466, 1.214510440826416, -0.11539808660745621, -0.022929366677999496, 0.7251832485198975, -0.8686329126358032, -0.24079878628253937, -0.6664826273918152, -0.8116207122802734, -0.07000257819890976, 0.102986179292202, -0.5522529482841492, -0.54251629114151, 0.13750620186328888, 0.8800584673881531, 0.05337344855070114, 0.21146871149539948, 0.15458129346370697, 0.4904610812664032, -0.1125718504190445, 0.2818036675453186, 0.24712514877319336, 0.11875537037849426, -0.2393340915441513, -0.10540533065795898, -0.13931547105312347, -0.5872557163238525, -0.13677473366260529, 0.46237751841545105, -0.9749395251274109, -0.05274294689297676, -0.04155655577778816, -0.218118354678154, 0.04040102660655975, 0.6616389751434326, 0.8429726958274841, 0.2561577260494232, -0.8005605340003967, 0.3880663514137268, 0.8681679368019104, 0.3132244646549225, -0.06009742617607117, 0.2918825149536133, 0.5066087245941162, -0.6124123334884644, 0.1407432109117508, 0.672938346862793, -0.11298549920320511, -0.4559256136417389, -0.9964184165000916, -0.44866693019866943, 0.37793439626693726, -0.5930569171905518, -0.707107424736023, 0.7021781206130981, -0.43533700704574585, -0.4012041985988617, -0.020700521767139435, -0.4962862432003021, -0.18941830098628998, 0.9433227777481079, -1.3965624570846558, -0.25464269518852234, 0.400721937417984, -0.2230692058801651, -0.5614901185035706, -0.34342220425605774, 1.2812328338623047, 0.6175879836082458, -0.7015247344970703, 0.320352166891098, 0.7566900849342346, -0.023209411650896072, -0.15014469623565674, -0.10974038392305374, 1.4067577123641968, 0.29389557242393494, -0.30383893847465515, 0.0919741615653038, -0.4457697570323944, 0.175077885389328, -0.8104706406593323, -0.5428411364555359, 0.6445207595825195, -0.35627055168151855, -0.17584025859832764, -0.8050397038459778, -0.525153398513794, 0.033971019089221954, 0.04294267296791077, 0.0964222103357315, 0.21571169793605804, -0.06992017477750778, -0.8260791301727295, -0.3451499938964844, -0.9558084607124329, -0.15919479727745056, 0.3648362457752228, -1.0467296838760376, 0.3822490870952606, 0.23722830414772034, 0.36931630969047546, -0.7425399422645569, -0.328536719083786, -0.45052605867385864, -0.1522950828075409, -0.06446424871683121, 1.1464197635650635, -0.41901329159736633, 0.38905149698257446, 1.1684881448745728, -0.18364307284355164, -0.6950799226760864, 0.3246319591999054, -1.1306970119476318, -0.2974656820297241, -0.056598927825689316, 0.4463858902454376, -0.6096096038818359, 0.6082951426506042, 1.052832841873169, 0.6729326248168945, -0.29580485820770264, -0.6757699251174927, -0.3248527944087982, 0.31392136216163635, -0.7770073413848877, 0.9374716281890869, -0.3280036449432373, -0.4533190429210663, -0.44201377034187317, 0.2801399827003479, 0.2465967983007431, -0.06795301288366318, -0.5584219694137573, 0.6908329725265503, 0.056426867842674255, -0.33192646503448486, -0.48849862813949585, 0.07134812325239182, -1.353265643119812, 0.02599630504846573, -1.338774561882019, -0.337505578994751, -0.6618546843528748, -0.25423532724380493, -0.1461011916399002, -0.13267041742801666, 0.08624212443828583, 0.638782262802124, 0.08616266399621964, -0.15346847474575043, -0.10305767506361008, -0.3607615828514099, 0.8923370242118835, 0.8112104535102844, -0.7115490436553955, 0.15525247156620026, 0.0902520939707756, 0.44351789355278015, 0.019182726740837097, 0.5376296639442444, -0.1791716367006302, -0.7249923348426819, -1.247485876083374, 0.3312690258026123, -0.304891973733902, 0.05256638675928116, -0.6511595249176025, 0.5930171012878418, 0.3742714524269104, -0.2620548903942108, 0.08225012570619583, 0.10428715497255325, -0.9651092886924744, -0.2076517790555954, 0.6579309105873108, -0.2750280797481537, 0.14410798251628876, 0.5218945741653442, -0.3249742090702057, -0.19735708832740784, 0.6487336754798889, 0.08423923701047897, -0.9313157200813293, -0.9500759243965149, 0.7746837735176086, -0.35959556698799133, 0.08449795842170715, -0.4937381148338318, 0.10777731984853745, -1.1509321928024292, -0.47727203369140625, 0.005610623862594366, 0.4008273482322693, -0.4180748760700226, 0.849744439125061, -0.051153071224689484, -1.254069209098816, 0.27033305168151855, 0.8237121105194092, -0.1765173226594925, -0.21855269372463226, 0.5800207257270813, 0.4584031105041504, -0.6930593252182007, 0.9127883315086365, 0.40966278314590454, 0.5528371334075928, -0.5738934278488159, 0.08019798994064331, 1.3097695112228394, -0.7988888621330261, -0.06658421456813812, 1.2379149198532104, -0.37104737758636475, -1.5386394262313843, 0.1920144110918045, -0.5782479643821716, -0.25583574175834656, -0.35927826166152954, 0.4678799510002136, 0.2434912919998169, 0.7054013013839722, -0.0487179271876812, -0.5186036825180054, 0.07540185004472733, -0.07947242259979248, -0.39200595021247864, 0.5902169942855835, -0.520196259021759, -0.6748069524765015, 0.22116044163703918, 1.3219406604766846, -0.8992264866828918, -1.1790035963058472, -0.9062844514846802, -0.700771689414978, -0.18562094867229462, 0.41682833433151245, -0.7624990344047546, -0.9660281538963318, 0.6764230132102966, 0.7502832412719727, -0.14526021480560303, 0.18397915363311768, -0.472805917263031, 0.2983993887901306, 0.49665960669517517, 0.12343479692935944, -0.8859007358551025, -0.8843894004821777, 1.0756008625030518, 1.0026153326034546, -0.8463597297668457, 0.3879200220108032, 0.1457577645778656, -0.3774636685848236, 0.8301657438278198, 0.33701664209365845, 0.36591053009033203, 1.2355519533157349, -0.3452818691730499, 0.013955728150904179, 0.3137640058994293, -1.4854803085327148, -0.032920029014348984, 0.9509732723236084, 0.5612877607345581, 0.4242188632488251, 0.26372072100639343, 0.007697469089180231, 0.8849895000457764, 0.3655187785625458, 0.22951598465442657, 0.18642657995224, 0.2541762888431549, -0.1707834005355835, -0.31373536586761475, -0.1698247641324997, 0.8454350829124451, -0.6913452744483948, -0.6406585574150085, 0.24658936262130737, 0.37754273414611816, 0.3179314434528351, 0.5809536576271057, 0.9184355139732361, 0.09181930869817734, 0.367601603269577, 0.3388415277004242, 0.17520469427108765, -0.47594964504241943, -0.4782591760158539, -0.01107778400182724, -0.6384202837944031, -0.2714521586894989, 0.09623902291059494, -0.11499994993209839, -0.3636440634727478, -0.739765465259552, 0.6271199584007263, -0.005893259309232235, 0.5868068337440491, 0.8665827512741089, 0.602741539478302, 0.35000404715538025, -0.1075885072350502, -0.48345059156417847, -0.7544572949409485, -0.8109285831451416, -0.2064768224954605, -0.39467838406562805, -0.5300493836402893, 0.10605525970458984, -0.2931574583053589, -0.5556166768074036]}, "authors": [{"authorId": "2263904514", "name": "Lin Wang"}, {"authorId": "2264501244", "name": "Zhichao Wang"}, {"authorId": "2166492136", "name": "Xiaoying Tang"}], "references": [{"paperId": "3bce09c68236ec61f1bd3cbfacabe45bba8c304f", "title": "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models"}, {"paperId": "3c6f2e0c5ff5dff6151c3e6489378a53318a75b4", "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect"}, {"paperId": "c1fa6255cc9fc3128f74befc7855e255bc7a2c6e", "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"}, {"paperId": "5e71d0e85f65a1c0fb2af7bff281209122c58932", "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method"}, {"paperId": "7ae48b24cbf955bf9b9498fb287bf4c5cd3b73d4", "title": "OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning"}, {"paperId": "67ffe6037cf058b8c5b39f59693c4c349cc1e456", "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization"}, {"paperId": "955ffeebf843f15334c5902fc1f74af512e5e8b6", "title": "Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking"}, {"paperId": "529ff7d6441d244212cf2becafd12a7e67ac56d9", "title": "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "ecf3556506a68d230a4de169465220379891e0ab", "title": "Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective"}, {"paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"}, {"paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843", "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "3599a236f285af48782fc30b1341d13ec7320735", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"}, {"paperId": "0e3d1457a66e442fae46c8f96886dc76aef3b085", "title": "Offsite-Tuning: Transfer Learning without Full Model"}, {"paperId": "f3b14521331d947a548d06eaefdecc1a79920ded", "title": "On the Convergence of Federated Averaging with Cyclic Client Participation"}, {"paperId": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "title": "Robust Speech Recognition via Large-Scale Weak Supervision"}, {"paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "50d909b5c6704668f648545bd62a86a27f3506a1", "title": "FedTP: Federated Learning by Transformer Personalization"}, {"paperId": "15abd9759bc65f560abf74eb5bf14ce40a0c7526", "title": "FedPrompt: Communication-Efficient and Privacy-Preserving Prompt Tuning in Federated Learning"}, {"paperId": "91dde9f9f4c48d59e2c216059994f9b742ab6024", "title": "Training Vision Transformers in Federated Learning with Limited Edge-Device Resources"}, {"paperId": "d96aa291c0c56b9522cd7c901f1acd43818f1db3", "title": "FedAdapter: Efficient Federated Learning for Modern NLP"}, {"paperId": "02be347cd94a5c3596003d98ccaf66831b048df5", "title": "Scaling Language Model Size in Cross-Device Federated Learning"}, {"paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d", "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"}, {"paperId": "b0bd66188b0b32acf3f64fe3bb20cad592f5a468", "title": "A Comprehensive Survey on Training Acceleration for Large Machine Learning Models in IoT"}, {"paperId": "195b88ba2ab7bc56ed24ea5b5d3f28622d2c6166", "title": "A Survey on Federated Learning for Resource-Constrained IoT Devices"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "a208331a249f082c6b19076754dad9cb129e12aa", "title": "A Blockchained Federated Learning Framework for Cognitive Computing in Industry 4.0 Networks"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "478df2ddc7159853d0da9c9d6fc2211077edbe80", "title": "A survey on security and privacy of federated learning"}, {"paperId": "c6c023c2209ce2b8f593dfea0b5b88493c0c00e3", "title": "Group Knowledge Transfer: Federated Learning of Large CNNs at the Edge"}, {"paperId": "5123717445799d8137327f4041e8d5a5a2c91379", "title": "Secure, privacy-preserving and federated machine learning in medical imaging"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "39f8cc684f09ea2b43767f5b9590896774802759", "title": "On the effect of dropping layers of pre-trained transformer models"}, {"paperId": "07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b", "title": "Advances and Open Problems in Federated Learning"}, {"paperId": "56f3293f10ee019eb5ae0bd8be6a342e2c4a20e6", "title": "Energy Demand Prediction with Federated Learning for Electric Vehicle Networks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "93d6752f11d5db3687cc9f895f219b1bed7e1023", "title": "A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection"}, {"paperId": "b4a632a7097e7d0631250884dfc6e1f76b376996", "title": "PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "d1dbf643447405984eeef098b1b320dee0b3b8a7", "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "88f4ccafd2aabe9aeee9274951fffddf9bb94306", "title": "Advances"}, {"paperId": "acbc16961cb2664f74e30abcc0b8d078d802ea85", "title": "FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "Feder-ated learning: Privacy and incentive , volume 12500"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "2023. Instruction tuning with gpt-4"}, {"paperId": "64c3e7711912677e7e3956c1456d196f354b919b", "title": "Communication-Efficient LLM Training for Federated Learning"}, {"paperId": null, "title": "2023. Fate-llm: A industrial grade federated learning framework for large language models"}, {"paperId": null, "title": "consumer-level GPU. Extensive empirical results on various LLMs and NLP tasks demonstrate the framework\u2019s effectiveness in terms of accuracy (up to 58.3% higher) and efficiency (reduced to 13.8%)"}, {"paperId": null, "title": "2023. Fedpetun-ing: When federated learning meets the parameter-efficient tuning methods of pre-trained language models"}, {"paperId": null, "title": "2023b Smoothquant: Accurate and efficient post-training quantization for large language models"}, {"paperId": null, "title": "2024. Introducing meta llama 3: The most capable openly available llm to date"}]}