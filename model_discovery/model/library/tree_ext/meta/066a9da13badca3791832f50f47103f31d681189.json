{"paperId": "066a9da13badca3791832f50f47103f31d681189", "title": "On \u201cScientific Debt\u201d in NLP: A Case for More Rigour in Language Model Pre-Training Research", "abstract": "This evidence-based position paper critiques current research practices within the language model pre-training literature. Despite rapid recent progress afforded by increasingly better pre-trained language models (PLMs), current PLM research practices often conflate different possible sources of model improvement, without conducting proper ablation studies and principled comparisons between different models under comparable conditions. These practices (i) leave us ill-equipped to understand which pre-training approaches should be used under what circumstances; (ii) impede reproducibility and credit assignment; and (iii) render it difficult to understand: \u201cHow exactly does each factor contribute to the progress that we have today?\u201d We provide a case in point by revisiting the success of BERT over its baselines, ELMo and GPT-1, and demonstrate how \u2014 under comparable conditions where the baselines are tuned to a similar extent \u2014 these baselines (and even-simpler variants thereof) can, in fact, achieve competitive or better performance than BERT. These findings demonstrate how disentangling different factors of model improvements can lead to valuable new insights. We conclude with recommendations for how to encourage and incentivize this line of work, and accelerate progress towards a better and more systematic understanding of what factors drive the progress of our foundation models today.", "venue": "Annual Meeting of the Association for Computational Linguistics", "year": 2023, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2306.02870", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "It is demonstrated how disentangling different factors of model improvements can lead to valuable new insights by revisiting the success of BERT over its baselines, ELMo and GPT-1, and how these baselines can, in fact, achieve competitive or better performance than BERT."}, "embedding": {"model": "specter_v2", "vector": [0.2919495403766632, 0.8195663690567017, -0.6615232825279236, -0.08906871825456619, -0.567250669002533, -0.2931157648563385, 0.4164729714393616, -0.2540014684200287, -0.8563703298568726, 0.12696723639965057, 0.4595273435115814, -0.7162028551101685, 0.10504067689180374, 0.3219885528087616, -0.37925663590431213, 0.07904534786939621, -0.6757853627204895, 0.625534176826477, -0.7407500147819519, -0.19631709158420563, -0.5695836544036865, -0.5292975306510925, -0.5308562517166138, -0.10497227311134338, 0.6064268350601196, -0.17150680720806122, 0.09132557362318039, 0.8160171508789062, -0.3401901125907898, 0.16577574610710144, 0.7437816262245178, -0.4959237575531006, 0.35267406702041626, 0.043381962925195694, -0.2046259045600891, -0.327045202255249, 0.28252601623535156, -0.3956966996192932, -0.5887122750282288, 0.9001993536949158, -0.18703140318393707, 0.10818402469158173, 0.7254249453544617, -0.8280357122421265, -0.5123551487922668, 1.2278984785079956, 0.627931535243988, 0.7793174982070923, -0.09589174389839172, -0.2934940457344055, 1.568479299545288, -1.3045158386230469, 0.5075826644897461, 1.4177213907241821, 0.8255606293678284, 0.2970221936702728, -0.42674070596694946, -0.6610522270202637, 0.24261358380317688, -0.26340964436531067, -0.7137928009033203, -0.416975200176239, -0.3043006658554077, -0.2786065936088562, 1.7483789920806885, -0.4215323328971863, -0.29802119731903076, 0.27662643790245056, -0.273062139749527, 1.393314242362976, -0.0018400180852040648, -0.9555739760398865, -0.4784858226776123, 0.5094078183174133, 0.18431930243968964, 0.6329090595245361, -0.5536547899246216, 0.5472121238708496, -0.7706222534179688, -0.3951887786388397, 0.03501858562231064, -0.1814241111278534, -0.422004371881485, 0.27155548334121704, -0.2002520114183426, 0.9654176235198975, 0.30664145946502686, 0.8246989846229553, -0.39935216307640076, 0.5251231789588928, 0.22569791972637177, 0.7004267573356628, 0.3414944112300873, 0.33643221855163574, -0.5701569318771362, 0.33519628643989563, -1.3098558187484741, 0.09979008138179779, 0.12653936445713043, 0.654096245765686, -0.05120319500565529, 0.05133911594748497, -0.9488650560379028, 0.07027046382427216, 1.6353322267532349, 0.05681928992271423, 0.25721099972724915, -0.6271336674690247, 0.4600409269332886, -0.5821159482002258, 0.441125750541687, -0.5338572263717651, -0.40943312644958496, -0.2559809982776642, -0.6416547298431396, -1.49263334274292, -0.398971825838089, -0.0015608855755999684, -0.8490095734596252, 1.0831845998764038, -0.4167352020740509, 0.13220389187335968, 0.3065530061721802, 0.700032114982605, 0.5210963487625122, 0.7465599775314331, 0.47425293922424316, -0.17500777542591095, 0.8527636528015137, -0.6534013748168945, -0.6262713074684143, -1.1598424911499023, 1.023895025253296, -0.1818743646144867, 0.4984731078147888, -0.49130406975746155, -1.1087465286254883, -0.7636778354644775, -0.6807047724723816, -0.06942461431026459, -0.09148912876844406, 0.6901435256004333, 1.1102567911148071, 0.3485374450683594, -1.1945075988769531, 0.7941193580627441, -0.042898233979940414, -0.24361975491046906, 0.24138541519641876, 0.1625732034444809, 0.015032137744128704, -0.5914705991744995, -1.3557422161102295, 0.4189012944698334, 0.6417443752288818, -0.24788784980773926, -0.30080097913742065, -0.6018471717834473, -0.8399701118469238, 0.18716630339622498, 0.2767801284790039, -0.6216252446174622, 1.466545820236206, -0.13711635768413544, -1.169136643409729, 0.7162771821022034, -0.5133291482925415, -0.05021268129348755, 0.5352535843849182, -0.13629385828971863, -0.843466579914093, -0.47110283374786377, -0.08793806284666061, 0.6172321438789368, 0.19361326098442078, -0.0889546200633049, -0.1230965182185173, 0.36837586760520935, -0.25699588656425476, -0.12338478118181229, -0.2795439660549164, 0.8907081484794617, -0.13161249458789825, -0.0372476764023304, 0.3428284227848053, 0.5557459592819214, -0.35447072982788086, -0.11076880991458893, -0.13446658849716187, -1.0715150833129883, 0.3777819573879242, -0.3647906184196472, 0.9698985815048218, -0.8607733249664307, -0.739223837852478, -0.028021402657032013, -0.05976835638284683, -0.11482755839824677, -0.7558852434158325, 0.6125314235687256, -0.16994494199752808, 0.603001058101654, -0.4284496009349823, -1.2049221992492676, 0.24606552720069885, -0.22660969197750092, -0.5603886842727661, -0.20901018381118774, 0.08285456150770187, 1.2555508613586426, -0.8962880373001099, 0.031979601830244064, -0.20403452217578888, 0.21647661924362183, -1.0161527395248413, 0.8961901664733887, -0.47464659810066223, 0.2760009169578552, 0.11641859263181686, -0.3132975697517395, 0.31339526176452637, -0.29165565967559814, 0.39496105909347534, -0.3478299081325531, -0.3530745804309845, 0.437660813331604, -0.4349069893360138, 1.3669705390930176, -0.1933748871088028, 0.5758377909660339, 0.3863150477409363, -0.5462399125099182, -0.1404668390750885, 0.7459858655929565, -0.14044024050235748, -0.10117364674806595, 0.47769811749458313, 0.7010456323623657, -0.2793408930301666, 0.48244938254356384, 0.6611121892929077, 0.334333211183548, 0.1082061380147934, 0.5809764266014099, 0.5347070693969727, -0.08362849801778793, 0.6551616191864014, 0.39659205079078674, 0.458418607711792, 0.25192469358444214, 0.23051917552947998, -0.3050076961517334, 0.47009366750717163, -0.42530184984207153, 0.1057838574051857, 0.5527538657188416, 0.5201660394668579, 0.36247894167900085, 0.10852395743131638, -0.4139656722545624, -0.15781471133232117, 0.03802910074591637, 0.3546028435230255, 1.6776677370071411, -0.709929883480072, -0.08948560804128647, -0.4990389049053192, -0.47379961609840393, -0.042180135846138, 0.32530677318573, -0.21654418110847473, 0.02221088483929634, -0.6206559538841248, -1.3033747673034668, 0.8328162431716919, 0.16467274725437164, 0.44077497720718384, -0.5599831938743591, 0.158070370554924, -0.008981273509562016, 0.3461745083332062, -0.6314424276351929, -0.45964619517326355, 0.47816601395606995, -0.7225521206855774, -0.14373263716697693, -0.15728309750556946, -0.1150028333067894, 0.20959383249282837, -0.5784755945205688, 0.842653214931488, -0.21229803562164307, 0.227467343211174, 0.18447624146938324, 0.6189785003662109, -0.6596948504447937, -1.2353721857070923, 0.22968724370002747, 0.2270343005657196, -0.43543171882629395, 0.36813321709632874, 0.4384942948818207, 0.33408087491989136, 0.05030369013547897, -0.45754727721214294, 0.19592829048633575, 0.40886884927749634, 0.22659160196781158, 0.6137538552284241, -0.15497079491615295, 0.2003457099199295, -1.432934284210205, 1.0995090007781982, 0.09211186319589615, -0.4657348096370697, 0.4874531328678131, -1.0491644144058228, -0.33897972106933594, 0.5204711556434631, -0.43451690673828125, -0.3789874315261841, -1.1437602043151855, 0.06586083769798279, 0.38207364082336426, -0.10077598690986633, 0.8887937664985657, 0.366869181394577, 0.3168489634990692, 0.41267430782318115, 0.2595711052417755, 0.3177626430988312, -0.43229278922080994, 0.793388843536377, -0.5626370906829834, 0.4156268835067749, 0.3727588653564453, 0.3581809401512146, -0.42004725337028503, -0.4973502457141876, -0.5168983340263367, -0.35009121894836426, -0.2262580841779709, -0.2748042643070221, 0.018078824505209923, -0.044060319662094116, -0.6642435193061829, -0.30341091752052307, -0.3062693178653717, -0.8922868967056274, -0.27617406845092773, 0.0787273645401001, 0.1708281934261322, 0.22528062760829926, -1.0341722965240479, -1.3187114000320435, -0.6409479379653931, -0.7268415093421936, -0.9049001336097717, 0.4361194670200348, 0.00018816634838003665, -0.2120385617017746, -0.6314606070518494, 0.2876148521900177, -0.17248967289924622, 0.9904823899269104, -0.9519974589347839, 1.1374138593673706, 0.0874926820397377, 0.24544085562229156, -0.1457109898328781, 0.14588093757629395, 0.45914265513420105, -0.5897020101547241, 0.5629608631134033, -0.9608458876609802, 0.02893744967877865, -0.4661855399608612, -0.2149115800857544, 0.16174115240573883, 0.6204583048820496, -0.04915962740778923, 0.03617697209119797, -0.6438755393028259, 0.43668365478515625, 1.092630386352539, -0.8172996640205383, -0.11803735047578812, 0.16643548011779785, 0.8129276037216187, 0.1933949589729309, -0.18753723800182343, 0.23880521953105927, 0.13121819496154785, 0.18193385004997253, -0.22613558173179626, -0.22676075994968414, 0.0272970013320446, -0.6757223010063171, 0.47565770149230957, 1.4951605796813965, 0.22529174387454987, -0.5682173371315002, -1.237534761428833, 0.20453046262264252, -1.2160016298294067, -0.4832344353199005, 0.15573139488697052, 0.5460656881332397, 0.5991495847702026, -0.4123845398426056, -0.4375123381614685, -0.11537765711545944, 0.224601611495018, 0.1951511800289154, -0.2344648241996765, -0.4422183930873871, -0.21880625188350677, 0.2217501848936081, 0.2648695111274719, 0.7382239699363708, -0.2954888641834259, 0.9115682244300842, 15.075336456298828, 0.691205620765686, 0.13569016754627228, 0.9052008390426636, 0.5171031951904297, 0.49845394492149353, -0.6059240102767944, -0.07037994265556335, -1.1308645009994507, -0.16608472168445587, 1.1885477304458618, 0.004678838886320591, 0.6911068558692932, 0.06828940659761429, 0.3090299963951111, 0.016681436449289322, -0.469725102186203, 0.4926818609237671, 0.16542601585388184, -1.1949001550674438, 0.5000745058059692, 0.32458293437957764, 0.6053398251533508, 1.0084338188171387, 0.49657779932022095, 0.9717403054237366, 0.4378410279750824, -0.5402970314025879, 0.5074784159660339, 0.16760817170143127, 0.6812077760696411, 0.22137568891048431, 0.8693622350692749, 0.8970319032669067, -0.520552396774292, -0.16232936084270477, -0.6709704399108887, -1.242578148841858, 0.4522673785686493, 0.4865972101688385, -0.7981748580932617, -0.3908194899559021, -0.4183582067489624, 0.6274440884590149, 0.071581169962883, 0.28363358974456787, -0.41372978687286377, 1.1158297061920166, -0.3987502455711365, 0.4666242003440857, 0.2518923282623291, 0.43971407413482666, 0.3881904184818268, 0.11846528202295303, 0.04356251284480095, -0.27776437997817993, 0.16628821194171906, 0.5270483493804932, -0.9585830569267273, 0.22724182903766632, -0.008441945537924767, -0.6054313778877258, -0.0789308249950409, 0.6556561589241028, 0.4623476564884186, 0.22697772085666656, -0.2912609875202179, 0.14404918253421783, 0.6885412931442261, 0.34575262665748596, -0.03983646258711815, 0.28029316663742065, 0.31615370512008667, -0.43218502402305603, -0.5995118618011475, 0.4987347424030304, -0.18495643138885498, -0.44964802265167236, -0.7791169285774231, -0.25532665848731995, 0.1838834285736084, -0.6783003211021423, -0.9065935611724854, 0.6825089454650879, -0.4269249141216278, 0.029405150562524796, 0.17481888830661774, -0.7421020865440369, -0.050854723900556564, 0.7331817150115967, -1.5298047065734863, -0.7387107014656067, 0.4912751317024231, -0.3502228260040283, -0.92081218957901, -0.089722640812397, 1.1787983179092407, 0.3951036334037781, -0.5085431933403015, 0.19001567363739014, 0.6819348931312561, 0.010310288518667221, -0.0940404161810875, -0.48978543281555176, 0.8595617413520813, 0.27108100056648254, 0.14678210020065308, 0.2849579453468323, 0.12556631863117218, 0.020263373851776123, -0.5901106595993042, -0.036422569304704666, 1.1456397771835327, -1.0022203922271729, -0.36154934763908386, -0.5280646681785583, -0.7758648991584778, 0.39252719283103943, 0.1922105848789215, -0.6500293016433716, 0.4706898331642151, 0.18772538006305695, -0.15660181641578674, 0.2825404703617096, -0.9530014991760254, 0.08724122494459152, 0.7627228498458862, -0.7529600858688354, -0.41570356488227844, 0.29474303126335144, 0.41312453150749207, -0.7739066481590271, -0.330092191696167, -0.008495029993355274, -0.05570383742451668, 0.280519038438797, 0.6607025861740112, -0.7120291590690613, 0.20956094563007355, 0.7922127842903137, -0.1317465603351593, -0.8608731031417847, -0.2740791440010071, -0.734503448009491, 0.11624690890312195, 0.18240809440612793, 1.2376062870025635, -0.5151957273483276, 0.38643068075180054, 0.9842078685760498, 0.11487075686454773, -0.040037017315626144, -0.7874761819839478, -0.265889972448349, 0.11128265410661697, -0.6318990588188171, 0.3622356653213501, -0.001859452691860497, -0.20626111328601837, 0.08841320872306824, 0.3233703672885895, 0.7078003883361816, -0.48963025212287903, -0.6904247999191284, 0.5195602178573608, -0.6289568543434143, -0.28635072708129883, -0.5295830965042114, -0.0780436173081398, -1.36407470703125, 0.4306265115737915, -1.5547637939453125, 0.2193567007780075, -1.239650845527649, -0.32502415776252747, -0.11904049664735794, -0.12193973362445831, 0.1643017828464508, 0.2276122123003006, -0.40343746542930603, -0.480305552482605, -0.04872245714068413, 0.0758381113409996, 0.7196552157402039, 0.9565595388412476, -0.7338924407958984, -0.17463862895965576, 0.04640495777130127, 0.03265389800071716, 0.2949378788471222, 0.5273833274841309, -0.8834713101387024, -0.7218064665794373, -1.6410651206970215, 0.4318632185459137, -0.3048120141029358, -0.08616253733634949, -0.25472790002822876, 0.5484164953231812, 0.3249788284301758, -0.5312452912330627, 0.1810423731803894, 0.018733955919742584, -1.1459667682647705, -0.15855813026428223, 0.29744988679885864, -0.6442808508872986, 0.36534371972084045, 0.16059915721416473, -0.5383550524711609, -0.11670703440904617, 0.1874227374792099, -0.3754820227622986, -0.7704764008522034, -0.44404688477516174, 0.47370490431785583, -0.5936790704727173, 0.29373031854629517, -0.5251337885856628, -0.012279527261853218, -0.9114251136779785, -0.18415436148643494, -0.11055953055620193, 0.45913809537887573, -0.4675534963607788, 0.4882229268550873, -0.10323800891637802, -0.7861825227737427, -0.44543948769569397, 0.32578691840171814, -0.37191733717918396, -0.2828230559825897, 0.4290405511856079, 0.5993977189064026, -0.37984558939933777, 0.7826729416847229, 0.5607820749282837, 0.33153408765792847, -0.711453914642334, -0.5075908303260803, 0.5791210532188416, -0.6479788422584534, -0.24874934554100037, 1.261831521987915, -0.3899462819099426, -1.4305065870285034, 0.1755179911851883, -1.1358706951141357, -0.3515273928642273, -0.5286075472831726, 0.29796743392944336, 0.2396385222673416, -0.12145145237445831, -0.22795353829860687, -0.32701361179351807, -0.08927832543849945, 0.03536797687411308, -0.8090731501579285, 0.7062371373176575, -0.18182478845119476, -0.27369385957717896, 0.23591850697994232, 0.6324570178985596, -0.5011375546455383, -0.6497232913970947, -0.5404199361801147, -0.011705850251019001, -0.017531413584947586, 0.6756739020347595, -0.7542335987091064, -0.39907070994377136, 0.8349093198776245, 0.3977292478084564, -0.21421000361442566, -0.049635548144578934, -0.2707875669002533, 0.0896335020661354, 0.47756415605545044, 0.31811049580574036, -0.5049493908882141, -1.2841883897781372, 1.4251245260238647, 0.9992953538894653, -1.1466145515441895, 0.3532339334487915, -0.08080904185771942, -0.5541500449180603, 0.651614248752594, 0.10996003448963165, 0.41673386096954346, 1.1002552509307861, -0.33817097544670105, 0.15310390293598175, 0.024034230038523674, -0.5091220736503601, 0.0018593063578009605, 0.8709794282913208, 0.6885824799537659, 0.9237276911735535, 0.3899909555912018, -0.27600327134132385, 1.1915384531021118, -0.21016055345535278, 0.6487081050872803, 0.25507450103759766, 0.39543449878692627, -0.3190421462059021, -0.12944389879703522, 0.23389028012752533, 0.7246921062469482, -0.5404318571090698, -1.0427935123443604, -0.13677161931991577, 0.7866808176040649, 0.3261255919933319, 0.41822880506515503, 0.2832496762275696, 0.2134159654378891, 0.37871021032333374, 0.6383969783782959, 0.21681703627109528, -0.5606172680854797, -0.4483296275138855, -0.21041785180568695, -0.5314459204673767, 0.3218657076358795, -0.19452540576457977, -0.44137707352638245, -0.358458936214447, -0.4491896331310272, 0.16309501230716705, 0.3735436797142029, 0.2459503412246704, 1.1238253116607666, 0.7316062450408936, 0.11278323829174042, -0.5803865194320679, -0.434821754693985, -0.3729476034641266, -1.361341118812561, -0.26943734288215637, -0.9391065239906311, -0.45217546820640564, -0.15502238273620605, -0.6120590567588806, -0.3747764527797699]}, "authors": [{"authorId": "66436856", "name": "Made Nindyatama Nityasya"}, {"authorId": "49918371", "name": "Haryo Akbarianto Wibowo"}, {"authorId": "8129718", "name": "Alham Fikri Aji"}, {"authorId": "9162688", "name": "Genta Indra Winata"}, {"authorId": "2368148", "name": "Radityo Eko Prasojo"}, {"authorId": "1685771", "name": "Phil Blunsom"}, {"authorId": "3376845", "name": "A. Kuncoro"}], "references": [{"paperId": "574beee702be3856d60aa482ec725168fe64fc99", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "fd7e88a2313e176315d99fc299277e752d7703b7", "title": "Efficient Methods for Natural Language Processing: A Survey"}, {"paperId": "8ce9b1e527c4d9d15239621ec4e3ef3fbbe32202", "title": "On the Role of Bidirectionality in Language Model Pre-Training"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "2ef4ab54d00203f9ac610213ac3abc8e1fe541b4", "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling"}, {"paperId": "f71d0c4cae964a71c8a2f2c0919075d3bb18ca63", "title": "Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis-Hastings"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "981dbdf6f87f13f3f3047a925c519fc39a35202b", "title": "Revisiting Simple Neural Probabilistic Language Models"}, {"paperId": "49f905eb03958c7cfae52ac759ea8978b8b2a6ea", "title": "Alignment of Language Agents"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d", "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute"}, {"paperId": "ba233d75aa403092bda0bffc026be7913673ad69", "title": "Mind the Gap: Assessing Temporal Generalization in Neural Language Models"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d0cda85c030711aaa5383c80d5928a4d22f8d3bf", "title": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?"}, {"paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "b9ed2fd3237539b0ad539dad8bdee15efbe0a26e", "title": "Single Headed Attention RNN: Stop Thinking With Your Head"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "d9f1eed347959149f27002485a7fe339604fe45d", "title": "Revisiting Low-Resource Neural Machine Translation: A Case Study"}, {"paperId": "7ace6fd1577d51779260463f376225339402f455", "title": "Unreproducible Research is Reproducible"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "5c30436931e87f1d64d752ad4b89ce43d3dae154", "title": "Child-Directed Speech Is Infrequent in a Forager-Farmer Population: A Time Allocation Study."}, {"paperId": "d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18", "title": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "2397ce306e5d7f3d0492276e357fb1833536b5d8", "title": "On the State of the Art of Evaluation in Neural Language Models"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c50105953aba395a0beb78ae39346d28487ace95", "title": "Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": null, "title": "Vicuna: An open\u2013 source chatbot impressing gpt-4 with 90%* chatgpt"}, {"paperId": "f40aeae3e522ada1f6a9f326841b01ef5c8657b6", "title": "Unifying Language Learning Paradigms"}, {"paperId": null, "title": "2020) introduced a unified, textto-text format (i.e., encoder-decoder) framework, and conducted a systematic study over the impact of different pre-training objectives, architectures, and training"}, {"paperId": null, "title": "ELECTRA: Pre\u2013 training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2019), emphasizing the importance of conducting thorough ablations and apple-to-apple comparisons"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Universal language model \ufb01ne-tuning for text classi\ufb01cation"}, {"paperId": "61a03de23c36ca308c47e2253e3efba10201faa7", "title": "Meaningful differences in the everyday experience of young American children"}, {"paperId": null, "title": "2023. Stanford alpaca: An instruction-following llama model"}]}