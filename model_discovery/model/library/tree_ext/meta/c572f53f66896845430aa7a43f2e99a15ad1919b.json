{"paperId": "c572f53f66896845430aa7a43f2e99a15ad1919b", "title": "Randomness Regularization With Simple Consistency Training for Neural Networks", "abstract": "Randomness is widely introduced in neural network training to simplify model optimization or avoid the over-fitting problem. Among them, dropout and its variations in different aspects (e.g., data, model structure) are prevalent in regularizing the training of deep neural networks. Though effective and performing well, the randomness introduced by these dropout-based methods causes nonnegligible inconsistency between training and inference. In this paper, we introduce a simple consistency training strategy to regularize such randomness, namely R-Drop, which forces two output distributions sampled by each type of randomness to be consistent. Specifically, R-Drop minimizes the bidirectional KL-divergence between two output distributions produced by dropout-based randomness for each training sample. Theoretical analysis reveals that R-Drop can reduce the above inconsistency by reducing the inconsistency among the sampled sub structures and bridging the gap between the loss calculated by the full model and sub structures. Experiments on <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf{7}$</tex-math><alternatives><mml:math><mml:mn mathvariant=\"bold\">7</mml:mn></mml:math><inline-graphic xlink:href=\"li-ieq1-3370716.gif\"/></alternatives></inline-formula> widely-used deep learning tasks (<inline-formula><tex-math notation=\"LaTeX\">$\\mathbf{23}$</tex-math><alternatives><mml:math><mml:mn mathvariant=\"bold\">23</mml:mn></mml:math><inline-graphic xlink:href=\"li-ieq2-3370716.gif\"/></alternatives></inline-formula> datasets in total) demonstrate that R-Drop is universally effective for different types of neural networks (i.e., feed-forward, recurrent, and graph neural networks) and different learning paradigms (supervised, parameter-efficient, and semi-supervised). In particular, it achieves state-of-the-art performances with the vanilla Transformer model on WMT14 English <inline-formula><tex-math notation=\"LaTeX\">$\\to$</tex-math><alternatives><mml:math><mml:mo>\u2192</mml:mo></mml:math><inline-graphic xlink:href=\"li-ieq3-3370716.gif\"/></alternatives></inline-formula> German translation (<inline-formula><tex-math notation=\"LaTeX\">$\\mathbf{30.91}$</tex-math><alternatives><mml:math><mml:mrow><mml:mn mathvariant=\"bold\">30</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant=\"bold\">91</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq4-3370716.gif\"/></alternatives></inline-formula> BLEU) and WMT14 English <inline-formula><tex-math notation=\"LaTeX\">$\\to$</tex-math><alternatives><mml:math><mml:mo>\u2192</mml:mo></mml:math><inline-graphic xlink:href=\"li-ieq5-3370716.gif\"/></alternatives></inline-formula> French translation (<inline-formula><tex-math notation=\"LaTeX\">$\\mathbf{43.95}$</tex-math><alternatives><mml:math><mml:mrow><mml:mn mathvariant=\"bold\">43</mml:mn><mml:mo>.</mml:mo><mml:mn mathvariant=\"bold\">95</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq6-3370716.gif\"/></alternatives></inline-formula> BLEU), even surpassing models trained with extra large-scale data and expert-designed advanced variants of Transformer models.", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "R-Drop is universally effective for different types of neural networks and different learning paradigms (supervised, parameter-efficient, and semi-supervised) and achieves state-of-the-art performances with the vanilla Transformer model."}, "embedding": {"model": "specter_v2", "vector": [0.054111264646053314, 1.0206921100616455, -0.7627542614936829, 0.004002458415925503, -0.014085261151194572, 0.28653356432914734, 0.4056798219680786, -0.5137892961502075, -0.3104195296764374, 0.022022554650902748, 0.3480655550956726, 0.2511972486972809, 0.40868377685546875, 0.073825903236866, -0.4614603817462921, -0.2094530314207077, -0.7095848917961121, -0.07524925470352173, -0.007968924939632416, -0.009005866013467312, -0.2693311274051666, -0.5143093466758728, -0.9594594240188599, -0.1221986934542656, 0.0014423199463635683, 0.9724268913269043, -0.11987733840942383, 0.5387686491012573, -0.387942910194397, 0.959629237651825, 0.6255995631217957, -0.29568129777908325, 0.4964136481285095, -0.3217058777809143, -0.21736417710781097, 0.039426375180482864, 0.1451060026884079, -0.32268208265304565, -0.8271461725234985, 0.9181435108184814, -0.2713717222213745, 0.4286489486694336, 0.23870013654232025, -0.6720383763313293, -0.14548370242118835, 0.5829445123672485, 0.2980169951915741, 0.659241795539856, -0.6570142507553101, -0.2882736623287201, 1.166574239730835, -0.9886086583137512, 0.3882228136062622, 1.2019681930541992, 0.6455211043357849, 0.4516255855560303, -0.4557867646217346, -0.5635806918144226, 0.6368141174316406, -0.08340616524219513, -0.630558431148529, 0.0975986048579216, 0.3831791579723358, -0.1149434819817543, 1.333699345588684, -0.4970458149909973, -0.11935516446828842, 1.1409621238708496, -0.007058356422930956, 1.046946406364441, 0.37960362434387207, -0.4596438705921173, 0.014516743831336498, 0.25441867113113403, 0.5999948382377625, 0.7833214402198792, 0.17202270030975342, 0.5345528721809387, -0.9450221061706543, 0.011154258623719215, 0.6848417520523071, 0.2739104926586151, 0.14845065772533417, -0.05438202619552612, 0.5056089758872986, 0.8253061771392822, 0.6552267670631409, 0.5604711771011353, -0.2073298692703247, 0.8675469756126404, 0.5445323586463928, 0.20342938601970673, 0.09900009632110596, -0.13908898830413818, 0.028377825394272804, 0.6746617555618286, -0.3993479907512665, -0.058026354759931564, -0.18860049545764923, 0.9104343056678772, -0.10639366507530212, 0.6282666325569153, -0.5746150612831116, 0.20178617537021637, 1.4631165266036987, -0.40378957986831665, 0.5064378976821899, -0.5180898308753967, 0.16218093037605286, -0.459124892950058, -0.29822850227355957, -0.7541494369506836, -0.3970412611961365, -0.414935439825058, -1.6521449089050293, -0.49558785557746887, -0.42888522148132324, 0.2550612986087799, -1.0227618217468262, 1.0502867698669434, -0.5139197707176208, 0.0945187658071518, -0.05268022045493126, 0.7411549091339111, 0.7864649295806885, 0.43255648016929626, -0.05620170757174492, 0.15823449194431305, 0.5555325150489807, -0.8497147560119629, -0.8985738754272461, -0.6987535357475281, 0.24043428897857666, 0.08878971636295319, 0.21892845630645752, -0.08266434073448181, -0.968239426612854, -1.0136940479278564, -0.9186505079269409, 0.14325793087482452, -0.39339885115623474, 0.04102714732289314, 0.9275760650634766, 0.42367416620254517, -0.6392346024513245, 1.0464246273040771, -0.49036404490470886, 0.13387013971805573, 0.9058142304420471, 0.39108365774154663, -0.021001502871513367, -0.2064802348613739, -0.9364519119262695, 0.2682453393936157, 0.5006721615791321, -0.1853487342596054, -0.0400274395942688, -0.5463918447494507, -0.8561141490936279, -0.1472378373146057, 0.29037928581237793, -0.5375527739524841, 0.7487035989761353, -0.3361063599586487, -1.3320204019546509, 0.529996931552887, -0.057852987200021744, 0.0749557763338089, 0.8525049686431885, -0.10035261511802673, -0.03381219878792763, -0.3299739360809326, -0.2634020149707794, 0.034614741802215576, 0.3357093334197998, -0.8177714943885803, 0.4352312982082367, 0.1507970541715622, -0.7677937150001526, -0.758194625377655, 0.3389979302883148, 0.5756711959838867, -0.5079331994056702, -0.47422099113464355, 0.5058008432388306, 0.6553405523300171, -0.020727621391415596, -0.07837499678134918, -0.8278591632843018, -0.8648043870925903, 0.5932952761650085, 0.11630763113498688, 0.6745821833610535, -0.7045499682426453, -0.9893487095832825, 0.2956373989582062, 0.12565502524375916, -0.16774389147758484, -0.6170387864112854, -0.06927396357059479, -0.5836430788040161, 0.9609687328338623, -0.32159486413002014, -1.2094500064849854, -0.17283539474010468, 0.18901468813419342, -0.5123559236526489, 0.14409606158733368, 0.5776501893997192, 1.0503029823303223, -0.7691628932952881, 0.4183087646961212, 0.18612554669380188, 0.37617242336273193, -1.0536003112792969, 1.5098623037338257, -0.19989731907844543, 0.4653066396713257, -0.0652366355061531, -0.24825234711170197, 0.5286657810211182, -0.5425481796264648, 0.29403871297836304, -0.33699649572372437, 0.5132644176483154, 0.7117304801940918, -0.932420551776886, 0.9484660029411316, -0.2724364995956421, 0.45530226826667786, 0.025274571031332016, -1.0802221298217773, 0.3678820729255676, 0.2365022599697113, -0.14640115201473236, -0.5154947638511658, 0.436308890581131, 0.1072087213397026, -0.8524253368377686, 0.2702762186527252, 0.29822051525115967, 0.5116645097732544, -0.2077808827161789, 0.15077906847000122, 0.9417330622673035, -0.19793350994586945, 0.12355460226535797, 0.19351805746555328, 0.7289648056030273, 0.34269851446151733, 0.31140413880348206, 0.053306400775909424, -0.33790266513824463, -1.2688665390014648, 0.33628377318382263, 1.1400128602981567, 0.5920324921607971, 1.417162537574768, 0.7840235233306885, -0.7624683976173401, -0.39830154180526733, -0.40399423241615295, 0.5735538005828857, 0.8034553527832031, -0.3082202076911926, 0.06245237588882446, -0.21126414835453033, -0.49397385120391846, -0.4630255401134491, -0.17040613293647766, -0.6881256103515625, -0.39666855335235596, -0.4253964424133301, -1.843225359916687, 0.6848740577697754, 0.07554416358470917, 1.3682481050491333, -0.39784008264541626, 0.3268095850944519, -0.34278133511543274, 0.5384769439697266, -0.999803900718689, -0.01611262373626232, 0.8100761771202087, -0.7672345042228699, -0.26878541707992554, 0.4311222434043884, 0.2196010947227478, 0.27189013361930847, -0.6886985898017883, 1.1901484727859497, -0.324137419462204, -0.26175573468208313, 0.20085684955120087, 0.6421035528182983, -0.4276468753814697, -0.41440191864967346, 0.12217697501182556, -0.11464223265647888, -0.23882515728473663, 0.1287483423948288, 0.053295932710170746, 0.2674884498119354, 0.04514250531792641, -0.8066721558570862, -0.3620903789997101, -0.19586531817913055, 0.45535725355148315, 0.8623221516609192, 0.047906454652547836, 0.21290341019630432, -1.2425764799118042, 0.7942712903022766, -0.413891077041626, -0.42668017745018005, -0.10362396389245987, -0.797639787197113, 0.14542518556118011, 0.6801571249961853, -0.7382081151008606, -0.10898900777101517, -1.1595826148986816, 0.3966068923473358, -0.6035627126693726, -0.1734732985496521, -0.2199893742799759, 0.27203136682510376, 0.09240178763866425, 0.23534360527992249, -0.16818895936012268, 0.35677844285964966, -0.11330604553222656, 0.5993868112564087, -1.0837799310684204, 0.3060445487499237, 0.28881537914276123, 0.17992033064365387, 0.2106350064277649, 0.1821441501379013, -0.859631359577179, -0.7375392317771912, -0.018243618309497833, 0.2568569779396057, -0.056330762803554535, -0.08361734449863434, -0.7028788328170776, -0.9459735155105591, 0.12874513864517212, -0.6087489128112793, -0.2317046821117401, -0.28029191493988037, -0.0013612302718684077, -0.42080992460250854, -1.3401600122451782, -1.1858514547348022, -0.6889148950576782, -0.27729684114456177, -0.9305477142333984, -0.015875883400440216, 0.3526872396469116, -0.0027713023591786623, -0.7819679975509644, -0.3482247292995453, -0.48761633038520813, 0.9395341277122498, -0.02033744566142559, 0.6537950038909912, -0.12411324679851532, -0.7205402851104736, -0.08340748399496078, 0.07545129954814911, 0.8643149733543396, -0.09669490903615952, 0.2396949678659439, -1.0657713413238525, 0.5029507875442505, 0.17859864234924316, -0.7964929938316345, 0.45438718795776367, 0.6688065528869629, 0.8208572864532471, -0.11771684885025024, 0.1069759652018547, 0.8291280269622803, 1.6504778861999512, -1.1524560451507568, 0.35192641615867615, 0.2273547649383545, 1.0479815006256104, 0.10305405408143997, -0.8471782803535461, 0.4883623421192169, -0.13791212439537048, 0.004223310388624668, 0.5057995915412903, -0.23333707451820374, -0.2880466878414154, -0.580954909324646, -0.015082483179867268, 1.3941028118133545, 0.33904963731765747, 0.08563747256994247, -0.33937981724739075, 0.4988688826560974, -1.2031878232955933, -0.9264990091323853, 0.8343650102615356, 0.8207179307937622, 0.2586159408092499, 0.020209049805998802, -0.23703747987747192, 0.018808143213391304, 0.7319895625114441, 0.567156970500946, -0.4334607720375061, -0.5412122011184692, -0.32833173871040344, 0.5374695658683777, 0.5521488189697266, 0.5808819532394409, 0.05143184959888458, 0.2851731777191162, 14.96533203125, 0.7592513561248779, -0.06351965665817261, 0.46767744421958923, 1.0932033061981201, -0.1770486831665039, -0.30331656336784363, -0.13043709099292755, -1.243758201599121, 0.28117790818214417, 0.7774667143821716, 0.7320324778556824, 0.6649671196937561, 0.25751447677612305, 0.20067884027957916, 0.09646811336278915, -0.16863130033016205, 0.840695858001709, 0.2682705521583557, -1.3425220251083374, 0.2800738215446472, -0.21092766523361206, 0.6596742272377014, 0.9158987998962402, 0.8915207982063293, 0.732900083065033, 0.5699849128723145, -0.699653148651123, 0.1993187665939331, 0.6145724058151245, 0.6858981847763062, -0.15532855689525604, 0.6134230494499207, 0.07658711075782776, -0.8339542150497437, 0.014540206640958786, -0.7529123425483704, -1.122922420501709, -0.012977694161236286, 0.675636887550354, 0.018724819645285606, -0.27758172154426575, 0.14720278978347778, 0.8511278629302979, 0.20038577914237976, 0.19769024848937988, -0.42680999636650085, 0.770555317401886, -0.6222831606864929, 0.1234336718916893, 0.12057588249444962, -0.1157028004527092, 0.2479838877916336, -0.059394415467977524, -0.39274728298187256, -0.2087472379207611, 0.17640504240989685, 1.014499306678772, -1.0011487007141113, -0.530157208442688, -0.4172893464565277, 0.02323015220463276, -0.22086124122142792, 0.6454764008522034, 0.3118233382701874, 0.4103284776210785, -0.44165733456611633, 0.002421265933662653, 0.9259406328201294, 0.26232507824897766, -0.2670733630657196, -0.21926885843276978, 0.5030607581138611, -0.3725447356700897, -0.12845207750797272, 0.5404233336448669, -0.7174683213233948, -0.6132932901382446, -0.0549037903547287, -0.10016478598117828, 0.09678974747657776, -0.8303202986717224, -1.2551136016845703, 0.8928530216217041, -0.3071453869342804, -0.21417325735092163, 0.47087326645851135, -0.6947755217552185, -0.4521578848361969, 0.48846155405044556, -1.6226953268051147, -0.39473727345466614, 0.20622967183589935, -0.35227274894714355, -0.6673309803009033, -0.1895652711391449, 0.47545140981674194, -0.3379543721675873, -0.7877878546714783, 0.26433423161506653, 0.12125831842422485, 0.18679623305797577, -0.5834735035896301, -0.8050192594528198, 0.6762558817863464, 0.2923739552497864, -0.42358213663101196, 0.40904539823532104, -0.13240334391593933, 0.3080603778362274, -0.2881843149662018, -0.4012084901332855, 0.009638034738600254, -0.5116661190986633, -0.10702935606241226, -0.6168521046638489, -1.183882236480713, 0.370739221572876, 0.47676706314086914, 0.4491198658943176, 0.2924788296222687, 0.04596114903688431, -0.7483407855033875, -0.13342659175395966, -0.8340678215026855, -0.07032313942909241, 0.49009180068969727, -0.7815387845039368, -0.4020024538040161, -0.05263814330101013, 0.1465599089860916, -0.7479029297828674, -0.5547122359275818, 0.1918545663356781, -0.22622080147266388, -0.6730029582977295, 1.0127017498016357, -0.5661128163337708, 0.5185866951942444, 0.8608464598655701, -0.05453990772366524, -0.42094048857688904, -0.026282601058483124, -1.4267845153808594, 0.19974203407764435, 0.4614405333995819, 0.14719471335411072, -0.7263023257255554, 0.5528143048286438, 0.7515959739685059, 0.4206081032752991, -0.54925537109375, -1.0706602334976196, -0.4509066045284271, 0.10626416653394699, -0.7164168357849121, -0.017848391085863113, -0.06880279630422592, -0.502236008644104, -0.008230014704167843, 0.20144759118556976, 0.4326326251029968, 0.2466568946838379, -1.1934126615524292, 0.10149369388818741, -0.1450219303369522, 0.03257569298148155, -0.7675774693489075, -0.7137765288352966, -1.4275425672531128, 0.20605672895908356, -1.3374662399291992, -0.17585285007953644, -0.7155237793922424, -0.584753155708313, 0.21182741224765778, -0.36599332094192505, -0.13608630001544952, 0.1788061261177063, -0.20119225978851318, -0.6247308254241943, -0.3831613063812256, -0.25414466857910156, 1.1715580224990845, 0.5955997705459595, -0.49183139204978943, -0.22255510091781616, 0.1505204141139984, -0.07798653841018677, 0.43638184666633606, 0.7762048244476318, -0.973131537437439, -0.5992887020111084, -0.9241600036621094, 0.3074531555175781, -0.4771478772163391, 0.11636429280042648, -0.8652644753456116, 0.28198692202568054, 0.46435847878456116, 0.20697727799415588, 0.18334020674228668, 0.18378376960754395, -1.1602383852005005, -0.32740649580955505, 0.3199901282787323, -0.5169208645820618, 0.08518880605697632, 0.11307957768440247, -0.15985138714313507, -0.2496551126241684, 0.4429154098033905, 0.49269726872444153, -0.9827055335044861, 0.17300257086753845, 0.4785403907299042, -0.7468665242195129, 0.630574643611908, -0.5643277764320374, -0.042574964463710785, -1.4347126483917236, -0.1674160659313202, -0.2160159945487976, 0.22265779972076416, -0.20588093996047974, 0.7574130296707153, 0.3272344768047333, -1.4388095140457153, 0.3538011610507965, 0.3686491847038269, -0.3895748257637024, 0.5438137054443359, 0.1716577559709549, 0.059403009712696075, -0.3189798593521118, -0.03570051118731499, 0.42907577753067017, 0.038889188319444656, -0.20117467641830444, 0.007014899980276823, 1.085296392440796, -0.5525327324867249, -0.5496203899383545, 0.7964094877243042, -0.5896494388580322, -1.117006778717041, 0.5596849918365479, -1.234816312789917, -0.18476316332817078, -0.555313766002655, 0.4556260108947754, 0.26895496249198914, -0.11686329543590546, 0.2678981125354767, -0.2946806848049164, 0.372802197933197, 0.06222127750515938, -0.19827046990394592, 0.7237778306007385, -0.13713540136814117, -0.1011984795331955, 0.9953527450561523, 0.9921483993530273, -0.9806734323501587, -1.1600301265716553, -0.8196030259132385, -0.11251134425401688, -0.1279992312192917, 0.25125008821487427, -0.026448186486959457, -1.1387397050857544, 0.46080446243286133, 0.576586902141571, 0.22201231122016907, 0.27715083956718445, -0.2095072716474533, -0.4100596308708191, 0.625953197479248, -0.15346543490886688, -0.7725035548210144, -0.2997289299964905, 1.0823668241500854, 1.1393245458602905, -1.0274518728256226, 0.6037184000015259, -0.3382468521595001, -0.7670880556106567, 0.8152521848678589, 0.18606944382190704, -0.6032039523124695, 1.0307576656341553, -0.3095674514770508, -0.15666578710079193, 0.08207601308822632, -0.8918408751487732, -0.45245882868766785, 0.49629512429237366, 1.0318729877471924, 0.44782906770706177, -0.349883496761322, 0.403615266084671, 0.893534779548645, 0.5491403937339783, -0.03529292345046997, 0.9366820454597473, 0.5819618105888367, 0.022969599813222885, 0.2790224850177765, 0.1250656694173813, 0.695037841796875, -1.1037958860397339, -0.12042255699634552, 0.22243180871009827, 0.8436869382858276, 0.03581225126981735, 0.805065393447876, 0.702585756778717, 0.16257542371749878, 0.5956361293792725, -0.17923282086849213, 0.3683788776397705, -0.3590511083602905, -0.5525513887405396, -0.2439974546432495, -0.9466626644134521, -0.5999559164047241, -0.48275935649871826, -0.10679778456687927, -0.08697384595870972, -0.1325339376926422, 0.2451457530260086, -0.18348555266857147, 0.3157927095890045, 0.9756390452384949, 0.43131786584854126, 0.8416024446487427, 0.3213890790939331, -0.5356292724609375, -0.343770831823349, -0.946713387966156, -0.25435104966163635, -0.1275137960910797, 0.21106599271297455, -0.3052198886871338, -0.844351053237915, -0.5103254914283752]}, "authors": [{"authorId": "2257093356", "name": "Juntao Li"}, {"authorId": "48083523", "name": "Xiaobo Liang"}, {"authorId": "2257371765", "name": "Lijun Wu"}, {"authorId": "2118462606", "name": "Yue Wang"}, {"authorId": "2289607262", "name": "Qi Meng"}, {"authorId": "2267250090", "name": "Tao Qin"}, {"authorId": "2258690233", "name": "Min Zhang"}, {"authorId": "2256760771", "name": "Tie-Yan Liu"}], "references": [{"paperId": "f9fc45bebc6568fbd40d9c0e016b8cf5a0ccd229", "title": "Adaptive Data-Free Quantization"}, {"paperId": "0d4184cff17f093e0487b27180be515c385feff6", "title": "DropGNN: Random Dropouts Increase the Expressiveness of Graph Neural Networks"}, {"paperId": "520bd2331cca8d5a9c032c186a2a0f7704ead6ff", "title": "R-Drop: Regularized Dropout for Neural Networks"}, {"paperId": "24388c7d65f74239ecb3e7985f7e91a39e356287", "title": "Randomness In Neural Network Training: Characterizing The Impact of Tooling"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "0343875960a81feb75293a747d29ccb013285d23", "title": "UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost"}, {"paperId": "dbcff24e72e8360f2026018a5cde646f369767cb", "title": "Not All Attention Is All You Need"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "be51e9141ae2af4daf3a1ba745ad3ff66a5990f3", "title": "Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective"}, {"paperId": "2ff7d8d79c1ab50c1826d965475a1eb32db0c133", "title": "SEED: Self-supervised Distillation For Visual Representation"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "ef8854a62e05c8e741894166689a9cd8352a1df0", "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks"}, {"paperId": "255e6239bcc51047d020d41ce0179c1270f3c22f", "title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning"}, {"paperId": "1013750582c20bbdf1164127b5f26b1e06e817e3", "title": "MixKD: Towards Efficient Distillation of Large-scale Language Models"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "5a11bd4e678fcb05cb8f5d30c45877fb58bdd3b3", "title": "A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation"}, {"paperId": "b5271f4522fd72e335535c5f65d3afc01d1cb2bd", "title": "Very Deep Transformers for Neural Machine Translation"}, {"paperId": "b88c11922cac84e5ea902f82d27ae21c3dda2e04", "title": "Better Fine-Tuning by Reducing Representational Collapse"}, {"paperId": "597bd2e45427563cdf025e53a3239006aa364cfc", "title": "Open Graph Benchmark: Datasets for Machine Learning on Graphs"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "5c27f7106d2f0c95fc51fb98ece672e00185ffa1", "title": "Scheduled DropHead: A Regularization Method for Transformer Models"}, {"paperId": "f74cb4f88f0023bbc350786808ba84c1b7833cec", "title": "Self-Distillation Amplifies Regularization in Hilbert Space"}, {"paperId": "25db56fc85fe15625c3375064a35e908ba6dfd2a", "title": "ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training"}, {"paperId": "bc51622358d8eea83248ef29402fe10640d07ba6", "title": "Big Transfer (BiT): General Visual Representation Learning"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "0c8b40ba2e5202872475120b22da1148c427fb70", "title": "A survey of regularization strategies for deep models"}, {"paperId": "3bc53c49ae68adacf2d5be2fa795bcb879e2717a", "title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "8271311ceeabe333d4555deedcd3926b2145314a", "title": "Self-Knowledge Distillation in Natural Language Processing"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "910aea4a020c329afb8b8a948abaafdf9ebcab3f", "title": "DropAttention: A Regularization Method for Fully-Connected Self-Attention Networks"}, {"paperId": "e146050bfe5e00063e55d467125a86daa45d7e1e", "title": "Depth Growing for Neural Machine Translation"}, {"paperId": "36e30516683032634975c53e60f3737b6e35ff80", "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "789a7069d1a2d02d784e4821685b216cc63e6ec8", "title": "Strategies for Pre-training Graph Neural Networks"}, {"paperId": "a8cab29d2230924dffe89d6dda15ba42790c5ebf", "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation"}, {"paperId": "941745ff77fa2cce8c1ed9692b4559492cd5d3ce", "title": "Survey of Dropout Methods for Deep Neural Networks"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed", "title": "Certified Adversarial Robustness via Randomized Smoothing"}, {"paperId": "cb2722b202b2b0b1b9b87eb4a984dee9040f34c3", "title": "Graph Warp Module: an Auxiliary Module for Boosting the Power of Graph Neural Networks"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9", "title": "How Powerful are Graph Neural Networks?"}, {"paperId": "c4ab32dc966bff2de35723374f7410eeab85053f", "title": "A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "2444be7584d1f5a7e2aa9f65078de09154f14ea1", "title": "Born Again Neural Networks"}, {"paperId": "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb", "title": "A Call for Clarity in Reporting BLEU Scores"}, {"paperId": "d08b35243edc5be07387a9ed218070b31e502901", "title": "Group Normalization"}, {"paperId": "c20148703a706d5883d323c3386978714fe1508d", "title": "Adversarial Dropout Regularization"}, {"paperId": "415f18130edbe06e3e4806dfb0a1edcab6c241eb", "title": "Fraternal Dropout"}, {"paperId": "6cc8fe49b353c77846af7437d8f98d3d7e3b56c3", "title": "Orthogonal Weight Normalization: Solution to Optimization over Multiple Dependent Stiefel Manifolds in Deep Neural Networks"}, {"paperId": "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f", "title": "Improved Regularization of Convolutional Neural Networks with Cutout"}, {"paperId": "58c6f890a1ae372958b7decf56132fe258152722", "title": "Regularizing and Optimizing LSTM Language Models"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f06a12928307e17b1aff2b9f4a6c11791f19b6a7", "title": "Deep Mutual Learning"}, {"paperId": "d773718f36ee1cc5bb9bc5b01afa8f76d09f452f", "title": "Structured Bayesian Pruning via Log-Normal Multiplicative Noise"}, {"paperId": "437da3d1024f7312fde8a5287c99c36b559ed8f3", "title": "Dropout with Expectation-linear Regularization"}, {"paperId": "7601b995303f953955004db7b9b8b206c0e02ff8", "title": "Learning Structured Sparsity in Deep Neural Networks"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0", "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"}, {"paperId": "310ec7796eeca484d734399d9979e8f74d7d8ed2", "title": "Shakeout: A New Regularized Deep Neural Network Training Scheme"}, {"paperId": "0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652", "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "80d2e35888a5f072aae0c6f367c52f33dc874f8d", "title": "Towards dropout training for convolutional neural networks"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "80a624b9327d9050244dfebac96f7f6cf806880f", "title": "Deep Unordered Composition Rivals Syntactic Methods for Text Classification"}, {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend"}, {"paperId": "f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "dc8a5ff6551c6b3419a7843b01910221c964105f", "title": "Analyzing noise in autoencoders and deep networks"}, {"paperId": "f9f19bee621faf46f90b023f8de8248b57becbc4", "title": "Adaptive dropout for training deep neural networks"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "ec92efde21707ddf4b81f301cd58e2051c1a2443", "title": "Fast dropout training"}, {"paperId": "0060745e006c5f14ec326904119dca19c6545e51", "title": "Improving neural networks by preventing co-adaptation of feature detectors"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "ccf415df5a83b343dae261286d29a40e8b80e6c6", "title": "The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training"}, {"paperId": "48e1de7d085808004d5f0493d486669a3d2930b5", "title": "A Simple Weight Decay Can Improve Generalization"}, {"paperId": "7b7110bfc90a4a5eb8dd7ef93d3060ad3754a274", "title": "Bayesian feature interaction selection for factorization machines"}, {"paperId": null, "title": "\u201cSwitchable online knowledgedistillation,\u201din"}, {"paperId": null, "title": "\u201cMulti-teacher distillationwithsinglemodelforneuralmachinetranslation,\u201d"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "\u201cInformer:Beyondef\ufb01cienttransformerforlongsequence time-seriesforecasting,\u201din"}, {"paperId": null, "title": "\u201cThe implicit and explicit regularization effectsofdropout,\u201din"}, {"paperId": null, "title": "\u201cLanguagemodelsarefew-shotlearners,\u201d2020"}, {"paperId": null, "title": "\u201cReformer: The ef\ufb01cient trans-former,"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "\u201cELECTRA: Pre-training text encoders as discriminators rather than generators,\u201d"}, {"paperId": null, "title": "\u201cReducingtransformerdepthondemandwithstructureddropout,\u201din"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "\u201cModelinglong-andshort-termtemporalpatternswithdeepneuralnetworks,\u201din"}, {"paperId": null, "title": "\u201cGLUE: Amulti-taskbenchmarkandanalysisplatformfornaturallanguageunderstanding,\u201din"}, {"paperId": null, "title": "\u201cRecurrent dropout without memoryloss,\u201din"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "\u201cRegularization ofneuralnetworksusingDropConnect,\u201din"}, {"paperId": null, "title": "\u201cRecti\ufb01ed linear units improve restricted Boltzmann machines,\u201d"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": null, "title": "\u201cManualandautomaticevaluationofsummaries,\u201din"}, {"paperId": "5c64fbdc93e21ebebccd1d6d3418f1749491cdea", "title": "Simplifying Neural Nets by Discovering Flat Minima"}, {"paperId": null, "title": "article has supplementary downloadable material"}, {"paperId": null, "title": "She is a senior researcher with Machine Learning Group, Microsoft"}, {"paperId": null, "title": "working toward the PhD degree with Soochow University, supervised by Prof. MinZhang.Hiscurrentinterestslieinneuralmachine translationandconsistencylearning"}, {"paperId": null, "title": "large-scale text processing, intelligent computing, and ma-chinelearning"}, {"paperId": null, "title": "license agreement with IEEE. Restrictions apply"}]}