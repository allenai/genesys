{"paperId": "c570475cab4c8d0662144c4d414c17e776d39409", "title": "A Light Recipe to Train Robust Vision Transformers", "abstract": "In this paper, we ask whether Vision Transformers (ViTs) can serve as an underlying architecture for improving the adversarial robustness of machine learning models against evasion attacks. While earlier works have focused on improving Convolutional Neural Networks, we show that also ViTs are highly suitable for adversarial training to achieve competitive performance. We achieve this objective using a custom adversarial training recipe, discovered using rigorous ablation studies on a subset of the ImageNet dataset. The canonical training recipe for ViTs recommends strong data augmentation, in part to compensate for the lack of vision inductive bias of attention modules, when compared to convolutions. We show that this recipe achieves suboptimal performance when used for adversarial training. In contrast, we find that omitting all heavy data augmentation, and adding some additional bag-of-tricks $(\\varepsilon-\\mathbf{warmup}$ and larger weight decay), significantly boosts the performance of robust ViTs. We show that our recipe generalizes to different classes of ViT architectures and large-scale models on full ImageNet-1k. Additionally, investigating the reasons for the robustness of our models, we show that it is easier to generate strong attacks during training when using our recipe and that this leads to better robustness at test time. Finally, we further study one consequence of adversarial training by proposing a way to quantify the semantic nature of adversarial perturbations and highlight its correlation with the robustness of the model. Overall, we recommend that the community should avoid translating the canonical training recipes in ViTs to robust training and rethink common training choices in the context of adversarial training. We share the code for your experiments at the following URL: https://github.com/dedeswim/vits-robustness-torch.", "venue": "2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)", "year": 2022, "citationCount": 43, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/2209.07399", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper shows that ViTs are highly suitable for adversarial training to achieve competitive performance, and proposes a way to quantify the semantic nature of adversarial perturbations and highlight its correlation with the robustness of the model."}, "embedding": {"model": "specter_v2", "vector": [0.4324866235256195, 0.6900045275688171, -0.047607965767383575, 0.6837188005447388, -0.3326980769634247, -0.667030930519104, 1.3012304306030273, -0.5979872345924377, -0.4155065715312958, -0.741034984588623, -0.05185578018426895, -0.3075578510761261, 0.6267373561859131, -0.036358542740345, -0.7632237076759338, -0.13556812703609467, -0.2660400867462158, -0.32626873254776, 0.30516937375068665, -0.6601716876029968, -0.006526143755763769, -0.5059881806373596, -0.6873860359191895, -0.12206639349460602, 0.1621520072221756, 1.1874595880508423, -0.5509031414985657, 1.2505384683609009, 0.3548405170440674, 0.6660798788070679, 0.44326236844062805, -0.7111814022064209, 0.9963718056678772, 0.3064064681529999, -0.3807069659233093, -0.07929045706987381, 0.7791272401809692, -0.6016584634780884, -0.8712221384048462, 1.0918024778366089, -0.09950665384531021, 0.01965724676847458, 0.3316566050052643, -0.9322832822799683, -0.45410341024398804, -0.027708159759640694, 0.15595388412475586, 0.9721724390983582, -0.6708471775054932, 0.2002258002758026, 0.9945651888847351, -0.8290149569511414, 0.11094976216554642, 1.157004475593567, 0.6544891595840454, 0.6152905225753784, -0.24400930106639862, -0.6274464726448059, 0.5285191535949707, 0.17689862847328186, -0.49617674946784973, -0.2665075659751892, 0.3148854076862335, -0.1450207680463791, 1.2090599536895752, -0.5960632562637329, -0.13705116510391235, 0.917579710483551, 0.32025352120399475, 1.152124285697937, 0.13094012439250946, -0.8679911494255066, -0.11310470104217529, 0.3681277334690094, 0.02212732471525669, 0.7385426163673401, -0.04083241894841194, 0.6336793303489685, -0.5883868932723999, -0.08430846780538559, 0.5530305504798889, 0.12598681449890137, 0.20247414708137512, -0.35351794958114624, 0.18340986967086792, 0.9399312734603882, 0.9113744497299194, 0.5284253358840942, -0.27315157651901245, 1.4508469104766846, 0.5934164524078369, 0.3028278350830078, -0.22456751763820648, 0.5875024199485779, 0.052065663039684296, 1.164726734161377, -0.3732984960079193, -0.08606624603271484, -0.34429335594177246, 0.6844559907913208, -0.14108043909072876, 0.4378206431865692, -0.5512071847915649, -0.5355461239814758, 0.9717779159545898, 0.09674801677465439, 0.0868910476565361, -0.6977795958518982, 0.17607775330543518, -0.7438361644744873, -0.0190736074000597, -0.8060966730117798, 0.32188722491264343, -0.12846316397190094, -1.0932480096817017, -0.18283258378505707, -0.17000578343868256, 0.31128358840942383, -1.279703140258789, 0.9311692118644714, -0.4833391606807709, 0.12268894910812378, -0.08181396871805191, 0.47321411967277527, 0.4283413290977478, 0.42900532484054565, 0.10617227107286453, 0.5524157881736755, 0.8096064925193787, -0.5168521404266357, 0.02144533395767212, -0.9370314478874207, 0.07472772896289825, -0.3918294608592987, 0.5896965861320496, 0.0855790302157402, -1.2582036256790161, -1.0429505109786987, -1.4379488229751587, 0.2769899070262909, -0.6487488150596619, -0.12185908854007721, 0.8482820987701416, 0.5066112279891968, -0.8870125412940979, 0.5688205361366272, -0.4785495102405548, -0.22654417157173157, 0.8971306085586548, 0.2503855228424072, 0.052244532853364944, -0.387538880109787, -1.1047720909118652, 0.6409321427345276, 0.17334452271461487, -0.6593602299690247, -1.02690589427948, -0.4133230447769165, -0.9035745859146118, -0.11569272726774216, 0.2561955749988556, -0.9440851807594299, 1.062434196472168, -0.47282493114471436, -0.5256715416908264, 0.8022937178611755, 0.1772395670413971, -0.07662378996610641, 0.5558456778526306, 0.1344727724790573, 0.049658313393592834, -0.21395382285118103, -0.18201476335525513, 0.6093481183052063, 1.2393121719360352, -0.6231476068496704, 0.46762165427207947, 0.37294378876686096, 0.20704010128974915, -0.7983297109603882, -0.5633190274238586, 0.4706498086452484, -0.22969472408294678, -0.2050788849592209, 0.08234914392232895, 0.653821587562561, -0.1728437840938568, -0.04324186220765114, -0.7144080400466919, -0.7781320810317993, 0.976531445980072, 0.24815137684345245, 0.3245368003845215, -0.8176535964012146, -0.8281136155128479, 0.3412349820137024, 0.327621728181839, 0.00880468636751175, -0.6741003394126892, 0.25257906317710876, -0.7847145199775696, 0.9077011942863464, 0.030214795842766762, -1.2032915353775024, 0.11898033320903778, -0.5215383172035217, -1.011639952659607, 0.12960660457611084, 0.42970725893974304, 1.409909725189209, -0.2782592177391052, 0.25883880257606506, 0.2253628671169281, 0.32790201902389526, -1.1782852411270142, 1.1369105577468872, -0.3812340199947357, 0.4003886878490448, 0.10472659021615982, 0.21147330105304718, 0.008054983802139759, -0.38646435737609863, -0.1213497519493103, -0.671190619468689, 0.39283469319343567, 0.3594394028186798, -0.09122540801763535, 1.1156654357910156, 0.058250315487384796, 0.4515306055545807, 0.07022944837808609, -1.3526813983917236, -0.028590407222509384, 0.2436438947916031, -0.32313433289527893, -0.4732038378715515, 0.60104900598526, 0.08890555799007416, -0.8417612910270691, 0.43837258219718933, 0.39074137806892395, 0.6075964570045471, -0.3980603516101837, 0.15061858296394348, 0.7129105925559998, -0.7670938372612, -0.20708702504634857, 0.8088609576225281, 1.0441076755523682, 0.2763502597808838, 0.12228153645992279, 0.09589558094739914, 0.04493587091565132, -0.5909574627876282, -0.3112685978412628, 0.729459285736084, 0.3925761580467224, 0.8993717432022095, 0.6497132182121277, -1.2102504968643188, -0.6339805126190186, -0.3621830642223358, 0.8767442107200623, 1.0725160837173462, -0.037814997136592865, -0.21443544328212738, -0.7005128264427185, -0.6968380212783813, -0.5176964402198792, -0.3165723383426666, -1.0094190835952759, -0.8189201951026917, -0.25688424706459045, -1.1220170259475708, 1.0680638551712036, -0.18429116904735565, 1.457696795463562, -0.4063240885734558, -0.1906440556049347, -0.9200258255004883, 0.38469624519348145, -0.9541645050048828, -0.6101535558700562, 0.10320478677749634, 0.135227233171463, -0.49896711111068726, 0.28660327196121216, 0.05214875191450119, 0.25059717893600464, -0.43539947271347046, 0.7614054679870605, 0.18794269859790802, -0.49163612723350525, 0.6214051842689514, 0.6736847758293152, -1.0059847831726074, -0.8395659327507019, -0.0558636374771595, 0.03541097417473793, -0.261504590511322, -0.19369320571422577, 0.18808481097221375, -0.16460828483104706, 0.23014642298221588, -1.0405336618423462, -0.6415388584136963, -0.06965264678001404, 0.30987900495529175, 0.7285959124565125, -0.24075621366500854, -0.21087224781513214, -0.8379846811294556, 0.5453731417655945, -0.008713076822459698, -0.6831039786338806, 0.19141864776611328, -0.7221706509590149, -0.19002951681613922, 0.682099461555481, -0.8851961493492126, 0.11335524171590805, -1.000477910041809, 0.27052441239356995, -0.8977528214454651, -0.061902664601802826, -0.37013792991638184, 0.3855656683444977, -0.5765520334243774, 1.0450164079666138, 0.23541957139968872, 0.16727416217327118, 0.3363243341445923, 0.4872201681137085, -1.5700637102127075, 0.8161884546279907, 0.2509993612766266, 0.8005861639976501, 0.28997960686683655, 0.2707609236240387, -0.530214250087738, -0.5661674737930298, 0.23024322092533112, 0.05814886838197708, -0.731356143951416, 0.3141472041606903, -0.4249677062034607, -0.9465197324752808, 0.2358197569847107, -0.8244742155075073, -0.3208461105823517, -0.14087358117103577, -0.6137708425521851, -0.42905324697494507, -0.8837951421737671, -0.809678852558136, -0.2804127037525177, -0.6615657806396484, -1.1795907020568848, 0.12727108597755432, 0.3784029185771942, -0.09110654145479202, -0.44154998660087585, -0.07453206181526184, -0.11957050114870071, 1.1143503189086914, 0.14363205432891846, 0.47194427251815796, -0.020550871267914772, -0.6838919520378113, -0.4768124222755432, 0.1281319111585617, 0.6819798946380615, -0.722602367401123, 0.5510777235031128, -1.4366744756698608, -0.13187944889068604, -0.21923084557056427, -1.008061408996582, 0.850950300693512, 0.2920204997062683, 0.4571460783481598, -0.18680335581302643, -0.12908418476581573, 0.9259858131408691, 1.6761388778686523, -0.7916289567947388, 0.8091371059417725, 0.5553528070449829, 1.0490354299545288, 0.10579809546470642, -0.48367583751678467, 0.36374345421791077, 0.11702616512775421, -0.05588918551802635, 1.0782437324523926, -0.47594183683395386, -0.18907971680164337, -0.8111575245857239, 0.5470834970474243, 0.2530352473258972, 0.3795941472053528, -0.003669650061056018, -0.8275398015975952, 0.49367865920066833, -0.9055324792861938, -1.0287967920303345, 1.0203644037246704, 0.8405627608299255, -0.10072851926088333, -0.07724877446889877, -0.2531839609146118, 0.08340968191623688, 0.4034639596939087, 0.5212903618812561, -0.5852823257446289, -0.6389881372451782, 0.012725683860480785, 0.8704742789268494, 0.5833229422569275, 0.4793640375137329, -0.542922854423523, 0.5964810848236084, 14.618385314941406, 0.878140389919281, -0.32410120964050293, 0.7334766387939453, 0.7553379535675049, 0.28508883714675903, -0.40581369400024414, 0.17004451155662537, -0.5905242562294006, -0.20840902626514435, 0.3763505816459656, 0.6882994771003723, 0.729459285736084, 0.13753949105739594, -0.660957396030426, 0.18712247908115387, -0.019259925931692123, 0.6692211627960205, 0.674656331539154, -1.6792155504226685, 0.10794412344694138, 0.13738158345222473, 0.7487040162086487, 0.832004189491272, 1.2254468202590942, 0.549095630645752, 0.8107094764709473, -0.4933011531829834, 0.4385921359062195, 0.2718954086303711, 1.2559443712234497, 0.043090589344501495, 0.22314485907554626, 0.22320905327796936, -0.8074501156806946, 0.03330308943986893, -0.5026519298553467, -0.9007514715194702, -0.17894715070724487, -0.2670420706272125, -0.3060981035232544, -0.5318019986152649, 0.15790216624736786, 0.47771987318992615, -0.35676971077919006, 0.060872822999954224, -0.5268021821975708, 0.5728294253349304, -0.44799014925956726, 0.46839120984077454, 0.2273530215024948, 0.8210846781730652, 0.2724863290786743, -0.5414628982543945, -0.2271551638841629, -0.11955387890338898, 0.009418182075023651, 0.8193838596343994, -0.7296149134635925, -0.7361926436424255, -0.68937748670578, -0.20116055011749268, -0.28019124269485474, 0.9243767261505127, 0.3563624322414398, 0.3773873746395111, 0.001698739011771977, 0.09315098822116852, 0.30835288763046265, 0.3481217622756958, -0.3505222499370575, -0.078675776720047, 0.29567858576774597, -0.32166987657546997, -0.19705334305763245, 0.6326929926872253, -0.6005793809890747, -0.7127060890197754, -0.5156517028808594, -0.3858588933944702, 0.4576598107814789, -0.9624931216239929, -0.7062060832977295, 0.7410637736320496, -0.4837871789932251, -0.2562059760093689, 0.6347886323928833, -0.6665375828742981, -1.112385869026184, 0.17420823872089386, -1.5818623304367065, -1.4041869640350342, 0.04830462485551834, 0.24618907272815704, -0.5292681455612183, -0.4904731512069702, 0.9331770539283752, -0.42597150802612305, -0.3709544539451599, 0.38374078273773193, -0.5336410403251648, 0.13824376463890076, 0.16635513305664062, -0.6919383406639099, 1.314370036125183, 0.930854856967926, -0.10159558802843094, -0.12527839839458466, 0.2561030089855194, 0.3137419819831848, -0.6049544215202332, 0.3265632092952728, -0.17762817442417145, -1.302291989326477, -0.1860993355512619, -0.2269224226474762, -0.4900873005390167, 0.5793752670288086, 0.7890896797180176, 0.04625815153121948, -0.21465623378753662, -0.19439950585365295, -1.2248066663742065, -0.1883828341960907, -1.197499394416809, 0.021782202646136284, -0.062417786568403244, -1.025335669517517, -0.35194677114486694, -0.12255033850669861, 0.1377595067024231, -0.6524194478988647, -0.0627543032169342, -0.021122615784406662, 0.2064543068408966, -0.5130876898765564, 1.489701509475708, -0.4016132354736328, 0.6764528155326843, 0.759783923625946, -0.21318742632865906, -0.5964491963386536, 0.13311053812503815, -1.1480212211608887, 0.4361504018306732, 0.18950879573822021, 0.23742233216762543, -0.8715728521347046, 0.18469420075416565, 0.8486033082008362, 0.377187579870224, 0.08886317908763885, -0.617350161075592, 0.05992679297924042, 0.3444965183734894, -0.7623341083526611, -0.08908683061599731, -0.12343353778123856, -0.27129805088043213, -0.15783800184726715, -0.0665174201130867, 0.8635653853416443, 0.22601063549518585, -1.0241039991378784, 0.8846820592880249, -0.0009627814288251102, -0.3430536985397339, -0.39475998282432556, -0.9023186564445496, -1.1829349994659424, -0.14479416608810425, -0.8678246140480042, -0.40491983294487, -0.5629262924194336, -0.672562837600708, 0.3221743702888489, -0.18126939237117767, 0.24619603157043457, 0.3453141152858734, 0.4093998372554779, 0.12036320567131042, -0.45507320761680603, -0.25438547134399414, 0.6566293239593506, 0.6021888852119446, -1.040371060371399, 0.3322792947292328, 0.14081157743930817, -0.4972158968448639, 0.6418917179107666, 0.5362454652786255, -0.5367389917373657, -0.5141122937202454, -1.1051517724990845, 0.45335572957992554, -0.726590633392334, 0.5781981945037842, -0.9475244879722595, 0.7664549946784973, 0.494948148727417, 0.31512802839279175, 0.43514305353164673, 0.577498733997345, -1.065492868423462, -0.6745584607124329, 0.5338152647018433, -0.17702679336071014, 0.0514838732779026, 0.48080697655677795, -0.4575040638446808, 0.11115514487028122, 0.8553923964500427, 0.6302341818809509, -0.7889718413352966, -0.6386622190475464, 0.5590910315513611, -0.39490431547164917, 0.11994828283786774, -0.14822989702224731, -0.0690079778432846, -1.7504961490631104, -0.5516445636749268, -0.05426761880517006, 0.24920783936977386, -0.0583629235625267, 0.5504011511802673, 0.40799590945243835, -1.1198887825012207, 0.4442606270313263, 0.6492097973823547, -0.05670281872153282, 0.09028192609548569, 0.38151854276657104, 0.31251370906829834, -0.43013015389442444, -0.1448775976896286, -0.21761645376682281, 0.28051066398620605, -0.6078145503997803, 0.4176490306854248, 0.9723331928253174, -0.5568839311599731, 0.18848241865634918, 1.2215341329574585, 0.27648845314979553, -0.6044530868530273, 0.072001151740551, -0.5905128717422485, -0.33957016468048096, -0.4444006085395813, 0.5135900378227234, 0.005134396255016327, -0.15599209070205688, 0.26338279247283936, -0.43784254789352417, 0.7804052233695984, 0.04845304787158966, -0.5609421133995056, 0.10340482741594315, -0.11419122666120529, 0.006891378667205572, -0.18062034249305725, 0.6217368245124817, -0.5952141284942627, -0.9643964767456055, -0.9772051572799683, -0.823641836643219, -0.13011088967323303, 0.29555824398994446, -0.25914183259010315, -0.8503244519233704, 1.0448004007339478, 0.6011528968811035, 0.4774143099784851, 0.6391919255256653, 0.10216905176639557, 0.03393141180276871, 0.5968175530433655, -0.06745126843452454, -0.1306072473526001, 0.09765196591615677, 1.0606361627578735, 1.1185024976730347, -0.9487144947052002, 0.4673445522785187, 0.13437409698963165, -0.6677753925323486, 0.5975416898727417, 0.476909339427948, -0.7045608758926392, 0.8102314472198486, -0.46766677498817444, -0.07585170865058899, -0.03914567083120346, -0.7953314185142517, -0.35447049140930176, 0.945874810218811, 1.2023507356643677, -0.3972303867340088, -0.4215407073497772, 0.6192954182624817, 0.5442770719528198, 0.2714424431324005, -0.4247901141643524, 0.6320149898529053, 0.44668352603912354, -0.09026891738176346, 0.10529061406850815, -0.6350265145301819, 0.2898302972316742, -1.2330396175384521, -0.23266924917697906, -0.48928093910217285, 0.827145516872406, 0.46677684783935547, 0.6770371794700623, 0.4177949130535126, -0.48092520236968994, 0.8837224841117859, 0.004630120471119881, 0.5824822187423706, -0.012475243769586086, -0.17870570719242096, -0.11363351345062256, -1.1836453676223755, -0.2614765167236328, -0.16969159245491028, -0.3045898675918579, 0.20485439896583557, -0.7261548638343811, 0.14310230314731598, -0.6212725043296814, 0.31002622842788696, 0.9774065613746643, -0.029194751754403114, 0.5565869808197021, 0.2190268486738205, -1.1532652378082275, -0.30483120679855347, -0.632162868976593, 0.591839611530304, -0.4433363676071167, 0.12747283279895782, -0.35381075739860535, -0.7238547205924988, -0.18303795158863068]}, "authors": [{"authorId": "2175939276", "name": "Edoardo Debenedetti"}, {"authorId": "3482535", "name": "Vikash Sehwag"}, {"authorId": "143615345", "name": "Prateek Mittal"}], "references": [{"paperId": "6f48988fd4237f599bf158a5210c70b3c15f1a16", "title": "An Impartial Take to the CNN vs Transformer Robustness Contest"}, {"paperId": "e0deb98643893fe711392d7e9e7ef324421b4235", "title": "Towards Efficient Adversarial Training on Vision Transformers"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "6e83334c9d7eaedd3c1e0992fc76ec01e4c85a2c", "title": "Pyramid Adversarial Training Improves ViT Performance"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "35c0800e657faa18cf3fc3629bdbeafbb976b006", "title": "Are Transformers More Robust Than CNNs?"}, {"paperId": "e4f7fb7a00fdc6dbc7de1deb4cf2d558a3f4d443", "title": "Data Augmentation Can Improve Robustness"}, {"paperId": "9c8d46b59e871e18d8d2e1ec1aa9b96d2f3d7342", "title": "Improving Robustness using Generated Data"}, {"paperId": "ba451a3f88b0f1e08bd5cda55c99d96a88a39c71", "title": "Parameterizing Activation Functions for Adversarial Robustness"}, {"paperId": "2d36eaa618da03d28a48a03e562a9fbc314609c4", "title": "Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks"}, {"paperId": "4bf4cfe3a691b2dc5685b64c340e3f26531c98f6", "title": "Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs"}, {"paperId": "f454f6b5f2ca9749ddf442eb5134612ef7f758c1", "title": "ResNet strikes back: An improved training procedure in timm"}, {"paperId": "cf5e6e3c50a798d87033e0e108e88b3647738bbe", "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers"}, {"paperId": "7fff8018bf625447df837c2fda5c58a705fbc038", "title": "XCiT: Cross-Covariance Image Transformers"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "922e5be564dc51dc645bf312fced4e97198942f8", "title": "Reveal of Vision Transformers Robustness against Adversarial Attacks"}, {"paperId": "5e4f03f68c6867d850f457dc5cc36738e5dff6c1", "title": "Vision Transformers are Robust Learners"}, {"paperId": "b8cee43a51c44f8f4448e78e41ecf081987707cf", "title": "Towards Robust Vision Transformer"}, {"paperId": "8bcb5534227214b83255f5b9dedbc0d46a44794a", "title": "Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "43e51c1bfd69df518e2907f7a955e485985ba423", "title": "On the Robustness of Vision Transformers to Adversarial Examples"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "d2a3bb6356d439146cd8d8e72dc728a1e3d93e7f", "title": "Understanding Robustness of Transformers for Image Classification"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "532035e132e27ddc2a1469e771fa658cbcaed4b3", "title": "DNR: A Tunable Robust Pruning Framework Through Dynamic Network Rewiring of DNNs"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "2aab97e35c43d961d645e650808d5b052ec180ab", "title": "RobustBench: a standardized adversarial robustness benchmark"}, {"paperId": "99931b7b794f7d98c7b3f3eee862129133d123d7", "title": "Do Wider Neural Networks Really Help Adversarial Robustness?"}, {"paperId": "67f74fe9d46f88661573003f8f1f12967ae49fa3", "title": "Bag of Tricks for Adversarial Training"}, {"paperId": "17293cd36ee5e7ec37dcec1d5ab85f9b77ad65d5", "title": "Do Adversarially Robust ImageNet Models Transfer Better?"}, {"paperId": "a2a349218b7889425c005880cc4b16b0a9e54dd8", "title": "Smooth Adversarial Training"}, {"paperId": "0b40141779fafcedc28d83bd678807ddb5980df3", "title": "The Pitfalls of Simplicity Bias in Neural Networks"}, {"paperId": "13da774fe604027bff2951ba82f4c3d9be7e415e", "title": "Augment Your Batch: Improving Generalization Through Instance Repetition"}, {"paperId": "764eff31d9596033859895d9513b838d2c57a6fb", "title": "Improving Adversarial Robustness Requires Revisiting Misclassified Examples"}, {"paperId": "9739f7030feb8cdc9ab479ffcf742ab1dd24eaa5", "title": "Adversarial Weight Perturbation Helps Robust Generalization"}, {"paperId": "365fb36b15f13c0c69596a9fc98ddcaed3fe739c", "title": "Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization"}, {"paperId": "18939eadc9c4460c8385e0591cde214a1ead067b", "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks"}, {"paperId": "2eda2921a8da4b325f9d05f556594a5884c398a7", "title": "Overfitting in adversarially robust deep learning"}, {"paperId": "3805147a98dab8f0c7667fed25490adbd2300fbd", "title": "HYDRA: Pruning Adversarially Robust Neural Networks"}, {"paperId": "6d4a87759917132913319960389f17fa1fe8b630", "title": "Fast is better than free: Revisiting adversarial training"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "8733fe2371b615609b04e2e910b1ecfa8e77cbc2", "title": "Square Attack: a query-efficient black-box adversarial attack via random search"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "5d28bdfa02f0766febff32b7a6b287611d6f2995", "title": "Adversarial Robustness as a Prior for Learned Representations"}, {"paperId": "91a05cb84f1c7dbb0354da2ff11ae92549152435", "title": "Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack"}, {"paperId": "e794be8c290de840281bc6d3ce4738998765050a", "title": "Bio-inspired synthesis of xishacorenes A, B, and C, and a new congener from fuscol"}, {"paperId": "b3f1aa12dde233aaf543bb9ccb27213c494e0fd5", "title": "Unlabeled Data Improves Adversarial Robustness"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "c92be891c5f8f0f60b6de206364f9a744612d1e8", "title": "Adversarial Training for Free!"}, {"paperId": "aa5741c74b7fac10680c1cfbdd49d9ffb5751a68", "title": "Using Pre-Training Can Improve Model Robustness and Uncertainty"}, {"paperId": "6c405d4b5dc41a86be05acd59c06ed19daf01d14", "title": "Theoretically Principled Trade-off between Robustness and Accuracy"}, {"paperId": "1b9c6022598085dd892f360122c0fa4c630b3f18", "title": "Robustness May Be at Odds with Accuracy"}, {"paperId": "804fb9542f4f56e264dd2df57c255a9a2011c00f", "title": "Adversarially Robust Generalization Requires More Data"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}, {"paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "b587ee7c802a5bd222a69090f59285e0dfdb29f1", "title": "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad", "title": "Intriguing properties of neural networks"}, {"paperId": "033c08ca48aaed2d5ab0a17d668d410538678ed8", "title": "Evasion Attacks against Machine Learning at Test Time"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "02b28f3b71138a06e40dbd614abf8568420ae183", "title": "Automated Flower Classification over a Large Number of Classes"}, {"paperId": "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"}, {"paperId": "666010dc3621e1a08fb6830f03a1f7857c7377cc", "title": "I and i"}, {"paperId": "0def290ae38abb4a04e35e0bcdc86b71d237f494", "title": "On the Adversarial Robustness of Vision Transformers"}, {"paperId": "8bb8f9ee86a5fde354497c0148ccde07c01ad811", "title": "Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off"}, {"paperId": null, "title": "EasyRobust: A large-scale robust training toolkit, https"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "RobustBench website"}, {"paperId": "741aa85611475bc534731fa2b04fe0e6695d9c2f", "title": "Robustness"}, {"paperId": null, "title": "Pytorch image models, https : / /github. com / rwightman / pytorch - image - models, 2019"}, {"paperId": "7cd316505f52aa337ef8a2aff10bc6bf1df561d0", "title": "and s"}, {"paperId": "3d2218b17e7898a222e5fc2079a3f1531990708f", "title": "I and J"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "5a554c8d22d47ac499aeb7fb0532ca9be65e5a2e", "title": "A. and Q"}, {"paperId": "ca5cb4e0826b424adb81cbb4f2e3c88c391a4075", "title": "Influence of cultivation temperature on the ligninolytic activity of selected fungal strains"}, {"paperId": "cba63f718781c9490ac10cf3cabbed68866ea7aa", "title": "Using Pre-Training Can Improve Model Robustness and Uncertainty"}, {"paperId": "12b03af504d0960334c77567dab38791bf0f739a", "title": "AND T"}, {"paperId": null, "title": "Image classification on imagenet (papers with code)"}, {"paperId": null, "title": "Q , K , and V represent respectively keys, and values grouped in matrices"}, {"paperId": null, "title": "Authorized licensed use limited to the terms of the applicable license agreement with IEEE"}, {"paperId": null, "title": "or parts"}, {"paperId": null, "title": "for the fact that the dot products can magnitude when d k is large. Large softmax and make its gradients very context of NLP, attention is computed"}]}