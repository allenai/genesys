{"paperId": "6b6ffb94626e672caffafc77097491d9ee7a8682", "title": "On the Connection between Local Attention and Dynamic Depth-wise Convolution", "abstract": "Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as weight computation. Sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. Weight sharing: the connection weights for one position are shared across channels or within each group of channels. Dynamic weight: the connection weights are dynamically predicted according to each image instance. We point out that local attention resembles depth-wise convolution and its dynamic version in sparse connectivity. The main difference lies in weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions. We empirically observe that the models based on depth-wise convolution and the dynamic variant with lower computation complexity perform on-par with or sometimes slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. These observations suggest that Local Vision Transformer takes advantage of two regularization forms and dynamic weight to increase the network capacity. Code is available at https://github.com/Atten4Vis/DemystifyLocalViT.", "venue": "International Conference on Learning Representations", "year": 2021, "citationCount": 82, "influentialCitationCount": 13, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work rephrase local attention as a channel-wise locally-connected layer and analyzes it from two network regularization manners, sparse connectivity and weight sharing, as well as weight computation to suggest that Local Vision Transformer takes advantage of two regularization forms and dynamic weight to increase the network capacity."}, "embedding": {"model": "specter_v2", "vector": [0.2994738221168518, 0.31530600786209106, -0.2753137946128845, 0.1578007936477661, -0.05802566185593605, 0.23766157031059265, 0.6624403595924377, -0.24424926936626434, -0.5018027424812317, -0.8391627669334412, 0.4525301456451416, 0.4260658323764801, 0.31604403257369995, -0.34343063831329346, -0.1483064740896225, 0.055897146463394165, -0.7371025085449219, 0.014056002721190453, 0.7013240456581116, -0.39987853169441223, 0.0742722898721695, -0.670364499092102, -1.2637122869491577, 0.5668550729751587, -0.020406365394592285, 1.425370693206787, 0.5569226741790771, 1.2245776653289795, -0.26558324694633484, 0.5217958092689514, 0.6351097226142883, -0.22659730911254883, 0.3363182246685028, 0.05496521294116974, -0.49326229095458984, 0.051824603229761124, 0.9539385437965393, -0.21695345640182495, -0.5135508179664612, 0.9507326483726501, -0.22265109419822693, 0.19977031648159027, 0.5162654519081116, -0.6808619499206543, 0.07938350737094879, 0.4816373586654663, 0.45201921463012695, 1.195605993270874, -0.5461339354515076, -0.5800928473472595, 1.5067152976989746, -1.320181131362915, 0.14301973581314087, 1.8387318849563599, 0.6707353591918945, 0.19188004732131958, -0.1926431506872177, -0.12260168790817261, 1.0380340814590454, 0.40980401635169983, -0.5459125638008118, -0.1443912833929062, 0.04460270702838898, -0.2621442675590515, 1.7744274139404297, -0.8282504081726074, 0.3425140976905823, 0.6638343334197998, 0.002643840154632926, 1.291530728340149, -0.30780965089797974, -0.7040123343467712, -0.2120427042245865, -0.15635253489017487, 0.38381606340408325, 0.7928349375724792, -0.4153275489807129, 0.1273963898420334, -0.7118353247642517, 0.2396620661020279, 0.7575850486755371, 0.16664093732833862, 0.3670724928379059, -0.30004629492759705, -0.26053962111473083, 0.8257821798324585, 1.0916731357574463, 0.5983568429946899, -0.5766383409500122, 0.9967859387397766, 0.48382967710494995, 0.07163571566343307, -0.22729548811912537, 0.1676078587770462, 0.005617855116724968, 0.8298479318618774, -0.5992690920829773, -0.2582797408103943, -0.11686939001083374, 0.7703800797462463, -0.0793965682387352, 0.1663046032190323, -0.4177621603012085, 0.25590571761131287, 1.3922327756881714, -0.032592643052339554, 0.3909302353858948, -0.8001031875610352, 0.04820488393306732, -0.5986592173576355, -0.4289185404777527, -1.0969221591949463, 0.42612481117248535, -0.4591541588306427, -0.42309141159057617, -0.5299489498138428, -0.18819302320480347, 0.3565637171268463, -1.0367519855499268, 0.19533400237560272, -0.3480752408504486, 0.42878711223602295, -0.31030842661857605, 0.5165438652038574, 0.44418013095855713, 0.43510839343070984, 0.6792608499526978, 0.785335898399353, 1.2116063833236694, -1.0874584913253784, -0.17547470331192017, -1.0240094661712646, -0.27914097905158997, -0.3249269723892212, 0.012583405710756779, -0.22791332006454468, -1.0656076669692993, -1.3411463499069214, -0.8362244367599487, -0.11038781702518463, -0.8546501398086548, -0.16322338581085205, 1.0937104225158691, 0.07666013389825821, -1.1794195175170898, 0.7941663861274719, -0.5142135620117188, -0.7746407389640808, 0.8395146727561951, 0.08255989849567413, 0.4871120750904083, -0.0864841416478157, -0.9416885375976562, 0.2997608482837677, 0.15278683602809906, -0.3976885974407196, -0.7120852470397949, -0.22324007749557495, -1.10795259475708, 0.2667355537414551, 0.3616190552711487, -0.6351773142814636, 1.0416208505630493, -0.6876949071884155, -0.7424203753471375, 0.6074536442756653, -0.3924976885318756, -0.3075963258743286, 0.1329195648431778, -0.17964017391204834, -0.23519913852214813, -0.0036004686262458563, -0.13831870257854462, 0.8563344478607178, 0.7920960783958435, -0.05254615843296051, -0.4719824194908142, -9.000692080007866e-05, -0.28101837635040283, -0.049245256930589676, -0.5795938372612, 0.9815266728401184, -0.5447368025779724, -0.23441645503044128, 0.5726915001869202, 0.9781588912010193, 0.06394615769386292, 0.30685991048812866, 0.137105792760849, -1.0809203386306763, 1.2832775115966797, 0.31460437178611755, 0.37835055589675903, -0.9242517352104187, -0.6580146551132202, -0.11465448141098022, -0.15428771078586578, -0.20796597003936768, -1.1832756996154785, 0.10515345633029938, -0.2381799817085266, 0.14884737133979797, 0.27316978573799133, -1.0039904117584229, -0.4323415756225586, -0.15532821416854858, -0.47545126080513, -0.10666962713003159, 0.32347822189331055, 1.2173908948898315, -0.6931600570678711, -0.24865925312042236, -0.10615607351064682, 0.45586591958999634, -0.9628990292549133, 1.025132656097412, -0.5699388980865479, -0.537102997303009, -0.3694390058517456, 0.2946729362010956, -0.2770327627658844, -0.271198034286499, 0.31813114881515503, -0.6998072862625122, 0.09835029393434525, 0.7136479020118713, -0.5214999914169312, 1.2801419496536255, -0.14515632390975952, 0.7745071649551392, 0.1355278491973877, -1.104771614074707, 0.24705234169960022, 0.3115420639514923, -0.3023812174797058, -0.7221221327781677, 0.6433627009391785, -0.04366106539964676, -0.6295700073242188, 0.48570752143859863, 0.5240573287010193, 1.4209482669830322, -0.05214661732316017, 0.007124888710677624, 1.0246556997299194, -0.21425111591815948, -0.14653043448925018, 0.527144193649292, 0.4847835898399353, 0.3800900876522064, 0.10659565031528473, -0.19950707256793976, -0.24455682933330536, -1.0162827968597412, 0.10779281705617905, 0.8038800954818726, 0.23126175999641418, 1.3289705514907837, 0.6698758006095886, -0.7995200157165527, -0.520527184009552, -0.011076726950705051, 0.5033857822418213, 1.4658485651016235, 0.009794097393751144, 0.023778939619660378, -0.830016016960144, -0.502208948135376, -0.2196023315191269, -0.6781444549560547, -0.6981915831565857, -0.1277334988117218, -0.21990713477134705, -0.7927218675613403, 0.5628240704536438, 0.5750173926353455, 1.517041802406311, -0.8459190130233765, -0.5575442314147949, -0.22907625138759613, 0.44178229570388794, -0.9737213253974915, -0.5366808176040649, 0.3462909162044525, -0.5763628482818604, -0.20223186910152435, 0.2227332442998886, -0.24446271359920502, -0.01512898225337267, -0.29258862137794495, 0.8054158687591553, -0.33940309286117554, -0.7159241437911987, 0.5825759172439575, 0.43755003809928894, -0.6848392486572266, 0.019766714423894882, 0.4018411934375763, -0.20299212634563446, 0.18416260182857513, 0.27240681648254395, 0.41757065057754517, -0.3357081711292267, 0.2632382810115814, -0.1765969693660736, -0.2815311849117279, -0.026584595441818237, -0.09186377376317978, 0.7959190011024475, -0.3695310354232788, -0.018292605876922607, -0.9657028317451477, 0.3548043966293335, -0.020901072770357132, -0.3818134069442749, 0.14159850776195526, -0.4027673900127411, -0.4414241909980774, -0.1622174233198166, -0.6634849905967712, -0.22033821046352386, -0.5991069078445435, 0.38927629590034485, -0.7410001158714294, -0.4873085021972656, 0.037833843380212784, 0.31088078022003174, -0.27286937832832336, 0.5163413286209106, -0.10605636239051819, 0.3438795804977417, -0.15704578161239624, 0.3048555850982666, -1.1056638956069946, 0.9140287637710571, 0.15629461407661438, -0.17473891377449036, 0.09438084810972214, -0.14103829860687256, -0.6114857196807861, -0.6334672570228577, -0.725590705871582, -0.4345161020755768, -0.2968231439590454, 0.6396086812019348, -0.5596041083335876, -0.7932345867156982, 0.32358530163764954, -0.9041284918785095, -0.5032201409339905, 0.04173032566905022, -0.31734973192214966, -0.22025775909423828, -1.2744816541671753, -1.033251166343689, -0.2974114716053009, -0.32738637924194336, -0.961249589920044, 0.04517160728573799, 0.30436182022094727, -0.10878632962703705, -0.6746994256973267, -0.5513805150985718, -0.5966311693191528, 1.3949177265167236, -0.21848216652870178, 0.25133511424064636, 0.016175871714949608, -0.4941740334033966, -0.24534107744693756, -0.2976662218570709, 0.608228862285614, -0.3317417502403259, 0.25194454193115234, -1.0395598411560059, 0.3479016125202179, -0.1195242628455162, -0.38778117299079895, 0.904728889465332, 0.5947993993759155, 0.9265222549438477, 0.4415411651134491, -0.5261244177818298, 0.5594685673713684, 1.6072484254837036, -0.6798318028450012, 0.5508992075920105, 0.22195298969745636, 1.0248310565948486, 0.19618970155715942, -0.24102821946144104, 0.36159470677375793, 0.3279506266117096, 0.13407160341739655, 0.9102550745010376, -0.27851107716560364, -1.006289005279541, -0.6568161845207214, 0.23265793919563293, 0.7445741295814514, -0.010741418227553368, 0.1052422896027565, -1.0407575368881226, 0.9627256393432617, -1.1625088453292847, -0.9449364542961121, 0.8375657200813293, 0.6461957097053528, -0.005670597776770592, -0.27037230134010315, -0.29240572452545166, -0.5746044516563416, 0.8085896372795105, 0.5269718766212463, -0.22267678380012512, -0.5863952040672302, 0.03577933833003044, 0.6969810128211975, 0.3796135187149048, 0.8465871810913086, -0.7136839032173157, 0.9241412878036499, 14.594380378723145, 0.5617532134056091, -0.3036186397075653, 0.5371227264404297, 0.7863626480102539, 0.42573702335357666, 0.08518078178167343, 0.055932577699422836, -1.1953377723693848, -0.23724599182605743, 0.7692373394966125, 0.7445363998413086, 0.490947961807251, 0.4056079387664795, -0.4491770267486572, 0.0976378545165062, -0.5990453362464905, 0.5687496662139893, 0.81023770570755, -1.3209420442581177, 0.2877052128314972, 0.25043007731437683, 0.43434441089630127, 0.8931528329849243, 0.8829965591430664, 0.33584854006767273, 0.3488597869873047, -0.580630362033844, 0.4840508699417114, 0.5496982932090759, 0.7982640266418457, 0.20996719598770142, 0.35572192072868347, -0.042089592665433884, -1.342319369316101, -0.4533248543739319, -0.6551128029823303, -1.0524646043777466, -0.08601805567741394, -0.206004336476326, -0.16154184937477112, -0.8925231695175171, 0.44413644075393677, 0.9675561189651489, -0.15428891777992249, 0.7361832857131958, -0.3596542775630951, 0.2225700169801712, -0.1335298866033554, -0.022716691717505455, 0.5036290884017944, 0.9019139409065247, 0.29235976934432983, 0.1827211081981659, -0.2826608121395111, 0.21944528818130493, 0.38180840015411377, 0.5482047200202942, -0.4955471456050873, -0.606393575668335, -0.05854256451129913, -0.005984722636640072, -0.2057483196258545, 0.8773747086524963, 0.03411167487502098, 0.21751496195793152, -0.3557646870613098, 0.4457661807537079, 0.36555737257003784, 0.35877618193626404, -0.24797698855400085, -0.34322619438171387, 0.37653419375419617, -0.20051616430282593, 0.6227068305015564, 0.44396689534187317, -0.4684619605541229, -0.5029007196426392, -0.967876136302948, 0.002148508094251156, 0.8043599724769592, -0.8039447665214539, -0.707785427570343, 0.8611329197883606, -0.18524305522441864, 0.14476364850997925, 0.6711916327476501, -1.0641140937805176, -0.738501787185669, 0.31514599919319153, -1.4492342472076416, -0.7385417819023132, -0.4908471703529358, 0.18576110899448395, -0.09422793984413147, -0.06926025450229645, 0.9380986094474792, 0.10416871309280396, -0.28936779499053955, 0.1199226826429367, -0.7720993161201477, 0.4300374984741211, -0.09285563975572586, -0.7512123584747314, 0.7290615439414978, 0.5086464881896973, -0.01081455685198307, -0.25482749938964844, -0.40004169940948486, 0.33736056089401245, -0.21157531440258026, 0.008379753679037094, 0.5495847463607788, -0.5637063980102539, -0.20512209832668304, -0.6653839945793152, -0.9786697626113892, 0.49730125069618225, 1.005945086479187, 0.39250731468200684, -0.3919113576412201, 0.15365122258663177, -0.8569115996360779, -0.3793056607246399, -0.7343667149543762, -0.12648633122444153, 0.3024144172668457, -0.7625894546508789, -0.6501815915107727, -0.46734824776649475, 0.3213396966457367, -0.7092698216438293, -0.2949540913105011, -0.25110864639282227, 0.31501612067222595, -0.09750381112098694, 1.4463515281677246, -0.4322299063205719, 0.46732771396636963, 0.8714803457260132, -0.33940741419792175, -0.416156530380249, -0.1269616037607193, -0.4630051553249359, 0.4302107095718384, 0.37086114287376404, 0.485767662525177, -0.6905601620674133, 0.25407785177230835, 0.4540409445762634, 0.3274976909160614, -0.38289204239845276, -0.34339073300361633, 0.20966331660747528, -0.1704498529434204, -0.6819965839385986, -0.0439067967236042, -0.0754372775554657, -0.3571929931640625, -0.2524598240852356, 0.8199960589408875, 0.5043207406997681, -0.0558515228331089, -0.76971435546875, 0.02931426838040352, -0.3579755425453186, -0.2136305421590805, -0.5481650829315186, -1.0882930755615234, -1.3128352165222168, -0.38664811849594116, -0.8760154247283936, -0.08388000726699829, -0.8989628553390503, -0.5036280155181885, 0.18107114732265472, -0.8308364748954773, 0.409187376499176, 0.518349289894104, 0.14298172295093536, -0.2536523938179016, -0.6400538086891174, -0.7617841958999634, 0.5192611217498779, 0.7805156111717224, -0.7446125745773315, 0.0623086541891098, 0.06024995446205139, -0.014214487746357918, 0.7650083303451538, 0.3132604658603668, -0.5112536549568176, -0.618895947933197, -1.229452133178711, 0.3054181933403015, -0.44080600142478943, 0.37431320548057556, -1.1888459920883179, 1.2536120414733887, 0.6346942782402039, 0.11918351799249649, 0.14251208305358887, 0.4363565742969513, -0.8617554903030396, -1.1395102739334106, 0.19385507702827454, -0.7318422794342041, -0.20437480509281158, 0.17512404918670654, -0.3198813796043396, -0.43091079592704773, 0.906594455242157, 0.4182312488555908, -1.1930180788040161, -1.502382516860962, 0.48150932788848877, -0.29332098364830017, 0.11513583362102509, -0.26505476236343384, -0.2734317183494568, -1.3289518356323242, -0.22887101769447327, -0.48596593737602234, 0.24841159582138062, -0.6097936034202576, 0.7709338068962097, 0.7782894968986511, -0.9895904660224915, 0.07015123963356018, 0.5379458069801331, -0.14954893290996552, 0.10055164247751236, 0.35088440775871277, 0.5365991592407227, -0.435222864151001, -0.11448647826910019, -0.15425771474838257, -0.016830850392580032, -0.6891763210296631, -0.029612062498927116, 1.2426453828811646, 0.2513434588909149, -0.24975009262561798, 1.1354721784591675, -0.23932045698165894, -0.44644948840141296, 0.5569310188293457, -1.0707859992980957, -0.804084837436676, 0.1282559335231781, 0.6909189224243164, 0.17892105877399445, -0.23801495134830475, 0.04652572423219681, -0.396100252866745, 0.7426231503486633, -0.18689961731433868, -0.6047449111938477, 0.23872128129005432, -0.08959294110536575, -0.4789663851261139, 0.37941139936447144, 0.7621751427650452, -1.1441913843154907, -1.3372324705123901, -1.2071285247802734, -0.6155198216438293, -0.26559507846832275, 0.43221884965896606, -0.38944846391677856, -1.1579481363296509, 1.0525743961334229, 0.9895660281181335, 0.4630591571331024, 0.168153315782547, 0.3103915750980377, 0.0005661515169776976, 0.5994399189949036, -0.23917458951473236, -0.47185468673706055, 0.14451435208320618, 1.3753679990768433, 1.1597033739089966, -0.7865901589393616, 0.052527401596307755, -0.39099791646003723, -0.5148420929908752, 0.8148627877235413, 0.5861585736274719, -0.8007215857505798, 1.031342625617981, -0.28689202666282654, 0.03097219206392765, 0.10866297036409378, -0.8340223431587219, -0.523375391960144, 0.8291778564453125, 1.2551060914993286, 0.2714150547981262, 0.08518636971712112, 0.5451160073280334, 0.4378584921360016, 0.47200366854667664, -0.5102996826171875, 0.2346969097852707, 0.03989644721150398, -0.6009659171104431, 0.3949137032032013, -0.04958181455731392, 0.47646087408065796, -0.7683965563774109, -0.324704647064209, -0.022862281650304794, 0.7499108910560608, 0.4624871611595154, 0.6906507015228271, 1.3324791193008423, 0.2711678445339203, 0.49859341979026794, 0.07599306106567383, 0.4516565203666687, -0.5777369737625122, -0.35206103324890137, 0.10723274201154709, -0.9388601183891296, -0.5400744080543518, -0.335341215133667, -0.9612541198730469, -0.1614343672990799, 0.028750943019986153, 0.4433939456939697, -0.5679043531417847, 0.32996878027915955, 0.8759680390357971, 0.47436293959617615, 0.8786053657531738, -0.41583603620529175, -0.861193060874939, 0.002275120699778199, -0.7037100195884705, 0.18988321721553802, -0.6272165179252625, 0.0012067202478647232, -0.32143014669418335, -0.340496301651001, -0.0723128467798233]}, "authors": [{"authorId": "1430768604", "name": "Qi Han"}, {"authorId": "2090420110", "name": "Zejia Fan"}, {"authorId": "152464732", "name": "Qi Dai"}, {"authorId": "2110833051", "name": "Lei Sun"}, {"authorId": "2149615771", "name": "Ming-Ming Cheng"}, {"authorId": "2168547810", "name": "Jiaying Liu"}, {"authorId": "2109534192", "name": "Jingdong Wang"}], "references": [{"paperId": "e15fdbde1d56a80743b7d7eafb3409a0a5870094", "title": "HRFormer: High-Resolution Transformer for Dense Prediction"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "1fb10189c500e4902cd1b5afd406f57323d21be8", "title": "VOLO: Vision Outlooker for Visual Recognition"}, {"paperId": "f43b98fcc2d56c60fc71bce96374c1e6b8e12c66", "title": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer"}, {"paperId": "498b323fc8d2eaf9e5a29a8b33d18971c3ed1408", "title": "Representative Batch Normalization with Feature Calibration"}, {"paperId": "adb4302eb7c420a46d770afe2448d4508c65fe58", "title": "ResT: An Efficient Transformer for Visual Recognition"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "fc92009ab34045f9e6d490684c7761f768e88c54", "title": "Beyond Self-Attention: External Attention Using Two Linear Layers for Visual Tasks"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "db848dda8e07d76b5fe006a79d909026584c44b1", "title": "Lite-HRNet: A Lightweight High-Resolution Network"}, {"paperId": "5b68522f58b61e7235b852677337ef3725075fd9", "title": "Co-Scale Conv-Attentional Image Transformers"}, {"paperId": "8f8f73f0f208302546c825ed474432389ed63be4", "title": "EfficientNetV2: Smaller Models and Faster Training"}, {"paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880", "title": "Rethinking Spatial Dimensions of Vision Transformers"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4", "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones"}, {"paperId": "2984ab83ade26639c3a82d29628d0d9e4abbebb0", "title": "Incorporating Convolution Designs into Visual Transformers"}, {"paperId": "96da196d6f8c947db03d13759f030642f8234abf", "title": "DeepViT: Towards Deeper Vision Transformer"}, {"paperId": "610b302950a19acef1c45456111dcd495f638c18", "title": "ConViT: improving vision transformers with soft convolutional inductive biases"}, {"paperId": "81fcd9309e1168fe0664d8df4213771e4ebfa343", "title": "Scalable Visual Transformers with Hierarchical Pooling"}, {"paperId": "fbd730a948a06cd4918c1d632ffdb4572b52d99b", "title": "Involution: Inverting the Inherence of Convolution for Visual Recognition"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "6f6f73e69ee0d9d5d7d088bb882db1851d98175a", "title": "Pre-Trained Image Processing Transformer"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5", "title": "Hopfield Networks is All You Need"}, {"paperId": "f5c8464032a936451b222be1984cabf42d6adfa8", "title": "Are we done with ImageNet?"}, {"paperId": "54c7445f319823c7dcc948c830e75e2fa7460b33", "title": "Exploring Self-Attention for Image Recognition"}, {"paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614", "title": "On the Relationship between Self-Attention and Convolutional Layers"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "441555b5cd09703e55c03e70bd2c9f82c0ffcf9b", "title": "Deep High-Resolution Representation Learning for Visual Recognition"}, {"paperId": "bae9c127f631f6fd321950c4612ca5c7d0eaea49", "title": "Interlaced Sparse Self-Attention for Semantic Segmentation"}, {"paperId": "bc626a52664e948a0ffb2b95d0e1e6377a01171a", "title": "Cascade R-CNN: High Quality Object Detection and Instance Segmentation"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "09b8c3ca3f625b222dea87c1cf904a4632c71a71", "title": "Convolution, attention and structure embedding."}, {"paperId": "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "title": "Local Relation Networks for Image Recognition"}, {"paperId": "6303bac53abd725c3b458190a6abe389a4a1e72d", "title": "Deep High-Resolution Representation Learning for Human Pose Estimation"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "5132500b23d2da47129b3f4f68dd30947a29e502", "title": "CCNet: Criss-Cross Attention for Semantic Segmentation"}, {"paperId": "cd8ddaaf56e38dddafdeac3f9643b9b5e9d35d54", "title": "Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks"}, {"paperId": "c02b909a514af6b9255315e2d50112845ca5ed0e", "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"}, {"paperId": "aaab0bd4d79d4f19109bab0fbcdb05070fb0edd1", "title": "Unified Perceptual Parsing for Scene Understanding"}, {"paperId": "054db727adc1a3a877aae6ad4ac835a3a9b872ba", "title": "IGCV3: Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "b9fd6c8ae5c3dce4a7a40989d6dbf62f0093dc6e", "title": "Interleaved Group Convolutions"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "9da734397acd7ff7c557960c62fb1b400b27bd89", "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "edf9fcdc814cfd4c548d87bfe3d2fc82a2a8522c", "title": "Deep Convolutional Neural Networks with Merge-and-Run Mappings"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "5b6ec746d309b165f9f9def873a2375b6fb40f3d", "title": "Xception: Deep Learning with Depthwise Separable Convolutions"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "aba48504f4f9563eafa44e0cfb22e1345d767c80", "title": "Dynamic Filter Networks"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "654247d5b184495fca18c6aa7e840e4f4559fef0", "title": "Do We Really Need Explicit Position Encodings for Vision Transformers?"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "03ce51e5e854faa614e79afe4dab8baeb5f73980", "title": "Twins: Revisiting Spatial Attention Design in Vision Transformers"}, {"paperId": null, "title": "2021a) 224 2 73M 15.8G -81.6 -Global attention: dynamic channel separable MLP + spatial separable MLP ViT-B/16"}, {"paperId": null, "title": "2020) 224 2 78M 16.1G -79.3 -Channel and spatial separable MLP, spatial separable MLP = point-wise 1 \u00d7 1 convolution Mixer-B/16"}, {"paperId": null, "title": "Pyramid attention: perform attention with spatial low rank"}, {"paperId": null, "title": "MMSegmentation Contributors"}, {"paperId": null, "title": "We use the identical training setting with Swin Transformer in ImageNet pretraining for fair comparison. The default input size is 224 \u00d7 224. The AdamW optimizer"}, {"paperId": null, "title": "Deep learning , volume 1"}, {"paperId": null, "title": "Pyramid: convolution with pyramid (spatial low rank) features"}, {"paperId": null, "title": "MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark"}]}