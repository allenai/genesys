{"paperId": "af365d54e237fb213d980b2dc0c2ef1a4280bbd7", "title": "In-Context Learning Dynamics with Random Binary Sequences", "abstract": "Large language models (LLMs) trained on huge corpora of text datasets demonstrate intriguing capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning. We propose a framework that enables us to analyze in-context learning dynamics to understand latent concepts underlying LLMs' behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate seemingly random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from seemingly random behaviors to deterministic repetition.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A framework is proposed that enables us to analyze in-context learning dynamics to understand latent concepts underlying LLMs' behavioral patterns, providing a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would."}, "embedding": {"model": "specter_v2", "vector": [0.33510473370552063, 0.3234364986419678, -0.2933805286884308, -0.05032442882657051, -0.3048516809940338, -0.41433313488960266, 0.923589289188385, -0.2982753813266754, -0.4266650676727295, 0.19399495422840118, -0.026182634755969048, -0.540846049785614, 0.25115010142326355, 0.2995239198207855, -0.4744454324245453, -0.18150383234024048, -1.2255845069885254, -0.06595837324857712, -0.25148850679397583, -0.10854196548461914, 0.2390519380569458, -0.5747424364089966, -0.9218180179595947, 0.10206831246614456, 0.2557169497013092, -0.026968659833073616, 0.26720884442329407, 1.2124332189559937, -0.3695996105670929, 0.7731718420982361, 0.4055584669113159, -0.09610827267169952, 0.3012050688266754, 0.18138976395130157, -0.26661986112594604, -0.38093286752700806, 0.19009152054786682, -0.2351732850074768, -0.7565023899078369, 0.5821137428283691, -0.07501035183668137, -0.014254072681069374, 0.39922067523002625, -0.3223305940628052, -0.4353930950164795, 1.1061921119689941, 0.411298930644989, 0.7112564444541931, 0.028820306062698364, -0.04886918514966965, 1.6082159280776978, -0.9480701088905334, 0.5676388740539551, 1.202237606048584, 0.38853830099105835, 0.83613520860672, -0.5362187027931213, -0.7678059339523315, 0.7415301203727722, -0.09433677047491074, -0.7652184963226318, 0.16440588235855103, -0.06740488111972809, -0.13474658131599426, 1.6900067329406738, -0.3012830317020416, 0.16733236610889435, 0.8464327454566956, 0.25763195753097534, 1.3796099424362183, 0.2601248323917389, -1.231271743774414, -0.14120328426361084, 0.14751671254634857, 0.26543253660202026, 0.5445265173912048, -0.07377054542303085, 0.7583051323890686, -0.7336524724960327, -0.15663860738277435, 0.5396454930305481, -0.0492236390709877, -0.0857347697019577, 0.0626094788312912, -0.3775603473186493, 0.5092156529426575, 0.08897858113050461, 1.2375726699829102, -0.0056105442345142365, 1.0777170658111572, 0.19530163705348969, 0.37641745805740356, -0.4019893407821655, 0.42820680141448975, -0.30473440885543823, 0.25380101799964905, -0.5889321565628052, 0.39961618185043335, 0.04406566172838211, 0.6352142095565796, -0.11789245903491974, 0.16255110502243042, -0.6607666611671448, 0.2187197357416153, 1.123451590538025, -0.05819638445973396, 0.9616501331329346, -0.23003093898296356, 0.567309558391571, -0.7391040921211243, 0.4061353802680969, -0.4249206781387329, -0.3563084602355957, -0.448643296957016, -0.4092763364315033, -1.2680662870407104, -0.3226456046104431, 0.045470088720321655, -0.4026726186275482, 1.1791220903396606, -0.4199998080730438, -0.06584670394659042, 0.016083544120192528, 0.36419421434402466, 0.06228416785597801, 0.6674110293388367, 0.41452503204345703, 0.04261542484164238, 0.39528539776802063, -0.1507764309644699, -0.26629823446273804, -0.9705491065979004, 1.1240919828414917, 0.05970797315239906, 0.4984639585018158, -0.19715453684329987, -0.9584406614303589, -0.8311861753463745, -0.9468250870704651, 0.4532940685749054, -0.22551675140857697, 0.08436095714569092, 1.206954836845398, 0.6423398852348328, -1.0804513692855835, 0.8624876141548157, -0.1910625547170639, -0.10238483548164368, 0.2906489074230194, 0.2893962562084198, 0.22450733184814453, -0.6137300133705139, -1.181551456451416, 0.1231270506978035, 0.38723984360694885, -0.7165802121162415, -0.3994765281677246, -0.15555350482463837, -1.1026829481124878, -0.0941416546702385, 0.33710289001464844, -0.538402795791626, 1.454508900642395, -0.08875112235546112, -0.8561024069786072, 0.6190058588981628, 0.1081685721874237, 0.08514692634344101, 0.2678462266921997, 0.03533535450696945, -0.646425724029541, -0.4751230478286743, -0.10996010154485703, 0.5038981437683105, 0.039414215832948685, -0.5476341247558594, -0.035050053149461746, 0.17341375350952148, -0.39528635144233704, -0.5401471257209778, 0.14637234807014465, 0.49410268664360046, 0.2223491221666336, -0.427359014749527, -0.01438880991190672, 0.5791357755661011, -0.26772236824035645, -0.6718094348907471, -0.3606724441051483, -1.3229594230651855, 0.5035645961761475, 0.09171875566244125, 1.1371707916259766, -0.8860893249511719, -1.2029101848602295, -0.1262783408164978, -0.08136153966188431, -0.08670245110988617, -0.9342998266220093, 0.9133068323135376, -0.531173825263977, 1.2555445432662964, -0.21957717835903168, -0.7065410614013672, -0.1642703413963318, -0.1322445124387741, -0.6272481679916382, -0.3858039081096649, 0.3303469121456146, 1.0919538736343384, -0.9466953873634338, 0.1500781625509262, 0.09603279083967209, 0.07637733221054077, -1.0443509817123413, 0.9985661506652832, -0.7889580130577087, 0.28473156690597534, -0.2155369222164154, -0.46308770775794983, -0.21133099496364594, -0.5108042359352112, 0.6638643741607666, 0.13971732556819916, -0.19436004757881165, 0.6498642563819885, -0.2693089544773102, 1.1760950088500977, -0.33289748430252075, 0.5816460847854614, -0.37860107421875, -0.8412188291549683, -0.15206606686115265, 0.4522554576396942, -0.3617008626461029, -0.3455542325973511, 0.28996190428733826, 0.1021093875169754, -0.21926982700824738, -0.12333837151527405, 0.4952041506767273, 0.7676200270652771, -0.38265591859817505, 0.6299574375152588, 0.30055612325668335, -0.5121926069259644, 0.4590126872062683, 0.3408726751804352, 0.8233423233032227, 0.4203636646270752, 0.7984244227409363, -0.042522069066762924, 0.0966278538107872, -1.230055332183838, -0.04009304195642471, 1.0087662935256958, 0.6553540229797363, 0.3312240242958069, 0.5074124932289124, -0.8213682770729065, -0.3193916380405426, 0.02278825454413891, 0.43888673186302185, 1.2127488851547241, -0.3698565661907196, -0.5782659649848938, -0.5864219665527344, -0.055637553334236145, -0.5786778926849365, 0.3738989531993866, -0.45969870686531067, -0.04330560564994812, -0.55217045545578, -0.7690222263336182, 0.6249392628669739, -0.12572264671325684, 0.8443971872329712, -0.9726530909538269, -0.2886158525943756, -0.12761376798152924, 0.47536152601242065, -0.7334363460540771, -0.4614524245262146, 0.2708743214607239, -0.5658692717552185, 0.009344246238470078, 0.23640598356723785, -0.048513319343328476, -0.08367524296045303, -0.6341753005981445, 0.8725476264953613, 0.07838839292526245, -0.8202810287475586, 0.6036369800567627, 0.8874892592430115, -0.5856778025627136, -0.7353978157043457, 0.06789732724428177, 0.016067449003458023, -0.46423882246017456, 0.033006951212882996, 0.331983745098114, -0.1781403124332428, -0.02558244578540325, -0.8744410872459412, -0.048983000218868256, 0.2393532395362854, 0.3888630270957947, 0.18140581250190735, -0.1358627825975418, 0.29600629210472107, -1.2331866025924683, 0.8440277576446533, 0.07937677949666977, -0.562615692615509, 0.56103515625, -0.7150607705116272, -0.1867874264717102, 0.5681853890419006, -0.9841715097427368, 0.03425547853112221, -1.0150916576385498, 0.6679213047027588, 0.34721657633781433, -0.5110566020011902, 0.18212784826755524, 0.49385812878608704, 0.27379173040390015, 0.4325469136238098, 0.41481900215148926, -0.15505628287792206, 0.4872315526008606, 0.47438210248947144, -0.8171247839927673, 0.020562486723065376, -0.1190820038318634, 0.20461560785770416, -0.6262229084968567, -0.14514264464378357, -0.6779239773750305, -0.2349545657634735, 0.18087607622146606, 0.24708600342273712, -0.29297614097595215, -0.13973642885684967, -0.5400782227516174, -1.1353318691253662, -0.060923319309949875, -0.788522481918335, -0.95487380027771, 0.045809078961610794, -0.3747512400150299, -0.18613167107105255, -0.9348101615905762, -1.414297103881836, -1.0241450071334839, -0.041866008192300797, -0.8530880808830261, 0.3144625723361969, 0.19423732161521912, -0.7201425433158875, -0.6706191301345825, 0.21128419041633606, -0.427617609500885, 0.6386575102806091, -0.7551566362380981, 1.063736081123352, 0.10534364730119705, -0.8443702459335327, -0.15350010991096497, 0.5280412435531616, 0.20353980362415314, -0.18542173504829407, 0.4072422385215759, -0.9385335445404053, 0.2847949266433716, -0.38252243399620056, -0.7259071469306946, -0.04704703390598297, 0.24655139446258545, 1.1134555339813232, -0.20281480252742767, -0.6506864428520203, -0.028512250632047653, 1.405412197113037, -0.23097258806228638, -0.039921313524246216, 0.04790515452623367, 0.6678796410560608, 0.4581185281276703, -0.45332467555999756, 0.5239856243133545, 0.18988946080207825, 0.15039202570915222, -0.2962076961994171, 0.4006383419036865, 0.562178909778595, -0.8471989631652832, 0.5300329923629761, 0.9078813791275024, 0.41418153047561646, -0.0834476575255394, -1.1976723670959473, 0.2604825794696808, -1.0115797519683838, -0.8172625303268433, 1.0117204189300537, 1.1359155178070068, 0.6559072732925415, -0.08924967795610428, -0.41466444730758667, 0.2331080585718155, 0.4644075334072113, 0.19636061787605286, -0.40186363458633423, -0.510094165802002, -0.12362946569919586, 0.6189504861831665, 0.11695876717567444, 0.5409605503082275, -0.0887664332985878, 0.7595534324645996, 15.115988731384277, 0.7712934017181396, -0.23474827408790588, 0.23300299048423767, 0.84270840883255, 0.2312028855085373, -0.7240486741065979, -0.04440690949559212, -0.9693549871444702, -0.08080167323350906, 1.1746715307235718, 0.5111291408538818, 0.9667720198631287, -0.11875919997692108, -0.18970435857772827, -0.19226489961147308, -0.6265742182731628, 0.6495587825775146, 0.14204968512058258, -1.2865113019943237, 0.46506235003471375, 0.11782265454530716, 0.5490031838417053, 0.595508873462677, 1.0895055532455444, 0.714177131652832, 0.8708651661872864, -0.4439682960510254, 0.5632221698760986, 0.1588325947523117, 1.1815669536590576, 0.0522470586001873, -0.12395038455724716, 0.8831889033317566, -0.4881467819213867, -0.43691328167915344, -0.16490817070007324, -1.248582124710083, -0.2880397439002991, 0.2623603641986847, -0.9076405763626099, -0.5700526237487793, -0.1616506576538086, 0.6807920932769775, 0.28605005145072937, -0.17044804990291595, -0.5628454685211182, 0.8511126041412354, 0.10240557044744492, -0.2987019717693329, -0.048033326864242554, 0.700937032699585, 0.10314013808965683, -0.0673995390534401, -0.07837715744972229, 0.09475935250520706, -0.05907263979315758, 0.7377240657806396, -0.4461546540260315, 0.10233890265226364, -0.8633837103843689, -0.6894134879112244, 0.09326347708702087, 0.6092187762260437, 0.49538835883140564, 0.4119814336299896, -0.10629580914974213, -0.13917696475982666, 0.7347561120986938, 0.5373660326004028, 0.24488720297813416, 0.2818906605243683, 0.08207602053880692, -0.6846120953559875, -0.21099476516246796, 0.0233859121799469, -0.2780044972896576, -0.484162837266922, -0.5790523290634155, -0.40365010499954224, -0.13760897517204285, -1.0385642051696777, -1.0722216367721558, 0.5048485994338989, -0.4211985766887665, -0.1909620463848114, 0.17273327708244324, -0.8130808472633362, -0.3538190424442291, 0.34015315771102905, -1.1310479640960693, -0.5789257287979126, 0.6080328822135925, -0.21903103590011597, -0.5300575494766235, -0.020717879757285118, 1.0792765617370605, -0.533592164516449, -0.5193915367126465, 0.07680650800466537, 0.013941582292318344, -0.14188355207443237, 0.02464769035577774, -1.0590916872024536, 0.8888865113258362, 0.018092038109898567, 0.3719152808189392, 0.8177289962768555, 0.14756329357624054, 0.13013599812984467, -0.6031701564788818, 0.28193390369415283, 0.7013007402420044, -1.263588547706604, -0.17481206357479095, -0.8851465582847595, -0.6137112975120544, 0.5035474300384521, 0.3042299449443817, -0.40078431367874146, 0.5555533766746521, -0.013849309645593166, -0.6738571524620056, -0.14982132613658905, -0.7333444356918335, 0.8783923983573914, 0.705101490020752, -0.8121575117111206, -0.26862987875938416, -0.13234177231788635, 0.4886062741279602, -0.5743052959442139, -0.1254311203956604, -0.03509989380836487, 0.3234401345252991, -0.2989228367805481, 0.7033024430274963, -0.48468202352523804, 1.0221636295318604, 0.7963394522666931, -0.07544936239719391, -0.5440229773521423, -0.0984484925866127, -1.173518180847168, 0.39865994453430176, 0.260311484336853, 0.6490774154663086, -0.9904541969299316, -0.14577926695346832, 0.7997422814369202, 0.6197685599327087, -0.5044787526130676, -0.6698391437530518, 0.005726274568587542, 0.32397258281707764, -0.7974483966827393, 0.5046676993370056, 0.11503594368696213, 0.08482726663351059, -0.040896911174058914, 0.44444018602371216, 0.5585793852806091, -0.5343653559684753, -0.775818407535553, 0.4963289201259613, -0.017838023602962494, -0.3246387243270874, -0.8636878132820129, -0.4449150860309601, -1.047530174255371, -0.06271419674158096, -1.1894617080688477, 0.02878127619624138, -0.4825691878795624, -0.49679410457611084, -0.2720978260040283, -0.45507803559303284, -0.08944448828697205, 0.14546969532966614, -0.8834385275840759, -0.703973650932312, -0.46366363763809204, -0.373233437538147, 0.5111879706382751, 0.7168979644775391, -0.6070626378059387, 0.108649842441082, -0.13300572335720062, -0.03948765993118286, -0.06465938687324524, 0.6527965664863586, -0.3820476531982422, -0.7829836010932922, -0.922467052936554, 0.6154757142066956, 0.18786092102527618, 0.0310609582811594, -0.8299656510353088, 0.4049762189388275, 0.26754868030548096, 0.16162341833114624, 0.26131483912467957, -0.10081148147583008, -0.8374358415603638, -0.48629409074783325, 0.29101040959358215, -1.1818443536758423, 0.33424001932144165, 0.30908268690109253, -0.23457004129886627, 0.1283324807882309, 0.5947545766830444, -0.4886336326599121, -1.1777546405792236, -0.14882612228393555, 0.3968009054660797, -1.0612667798995972, 0.111612468957901, -0.14415784180164337, 0.06822218745946884, -1.056768536567688, -0.13737910985946655, 0.10939787328243256, 0.40867528319358826, -0.22037452459335327, 0.8316020965576172, 0.06747917830944061, -0.7563098073005676, 0.36294999718666077, 0.2706444263458252, 0.11144670099020004, 0.16838674247264862, 0.6769833564758301, -0.24699170887470245, -0.14805302023887634, 0.621797502040863, 0.5896649360656738, -0.09340956062078476, -0.7866420745849609, -0.05442962422966957, 0.6891151070594788, -0.8349242806434631, 0.028919875621795654, 0.7954060435295105, 0.1604163646697998, -1.2844679355621338, 0.0026379567570984364, -1.2152577638626099, -0.5951740145683289, -1.0727688074111938, 0.47016188502311707, -0.3959178328514099, -0.6039038300514221, 0.7333798408508301, 0.023318329825997353, 0.560792088508606, -0.1625451296567917, -0.738364577293396, 0.2134588658809662, -0.11221516132354736, -0.13818250596523285, 1.0635496377944946, 0.27515265345573425, -0.39453500509262085, -0.6583153009414673, -0.727232038974762, 0.060329437255859375, 0.005291401874274015, -0.13619638979434967, -0.6025665402412415, -0.19164325296878815, 0.6656762957572937, 0.547855019569397, 0.39508700370788574, -0.19156388938426971, -0.12106568366289139, -0.339811235666275, 0.5701711177825928, 0.60292649269104, -0.28782784938812256, -0.2375188171863556, 1.0393697023391724, 1.2705073356628418, -0.8553423285484314, 0.2372688204050064, 0.03315145522356033, -0.45366421341896057, 0.864962100982666, 0.5237913131713867, -0.14864623546600342, 0.7183161377906799, -0.23456081748008728, 0.017184268683195114, 0.237141951918602, -1.5135639905929565, 0.3339052200317383, 0.45433759689331055, 0.9011614918708801, 1.1556048393249512, 0.7688905000686646, -0.0823574811220169, 0.9764986038208008, -0.1709662228822708, 0.4257715046405792, 0.7550764083862305, 0.7325019240379333, 0.004594592377543449, -0.46937820315361023, 0.12950320541858673, 0.45621946454048157, -0.8683047294616699, -0.7032420039176941, 0.2262580394744873, 0.6704124808311462, 0.3724534809589386, 0.5562081933021545, 0.4751540422439575, -0.13390137255191803, 0.00789350550621748, 0.4216710031032562, 0.7303956747055054, -0.6929956078529358, -0.31819722056388855, -0.34664371609687805, -0.7650025486946106, 0.030040346086025238, -0.16537676751613617, -0.6519622206687927, -0.30832478404045105, 0.07308515906333923, 0.06089479848742485, 0.07949470728635788, 0.15901577472686768, 1.2307332754135132, 0.44237178564071655, 0.16207385063171387, -0.09591790288686752, -0.4146900773048401, -0.31906917691230774, -0.846194863319397, 0.30656057596206665, -0.7541742920875549, -0.017811566591262817, -0.04058479517698288, -0.46557214856147766, -0.5054256319999695]}, "authors": [{"authorId": "2190821333", "name": "Eric J. Bigelow"}, {"authorId": "35573359", "name": "Ekdeep Singh Lubana"}, {"authorId": "2258717260", "name": "Robert P. Dick"}, {"authorId": "2258898381", "name": "Hidenori Tanaka"}, {"authorId": "37774552", "name": "T. Ullman"}], "references": [{"paperId": "22dc601ca9c70a676052ec36326ecc32c6c35761", "title": "Borges and AI"}, {"paperId": "21091f8133ab034baacb92fdb958e14989eb427f", "title": "Language Modeling Is Compression"}, {"paperId": "3e4afde5a9de2c1801da99b8aff5ae05923f256b", "title": "Are Emergent Abilities in Large Language Models just In-Context Learning?"}, {"paperId": "135ae2ea7a2c966815e85a232469a0a14b4d8d67", "title": "Taken out of context: On measuring situational awareness in LLMs"}, {"paperId": "77f02ff24909896856fec410968aef7999c29440", "title": "Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla"}, {"paperId": "827afa7dd36e4afbb1a49c735bfbb2c69749756e", "title": "Measuring Faithfulness in Chain-of-Thought Reasoning"}, {"paperId": "a72ba8dc49a6a842f69c312ac9a037a0f33b74f5", "title": "The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks"}, {"paperId": "70c3d5ab03a54281be91709b19e3f50a2e4be0e3", "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection"}, {"paperId": "f5e9337477d7a9eb6267d0310549fdefafbb7fe2", "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"}, {"paperId": "d40dbe668d5b68419e934dfa4c5851ffa1c24aa2", "title": "Exposing Attention Glitches with Flip-Flop Language Modeling"}, {"paperId": "11ae58636a5daf0ea1297f1c4ee94042fcebefa8", "title": "Birth of a Transformer: A Memory Viewpoint"}, {"paperId": "61039ce0c0f079b90ecca7d8c659340aee9ee932", "title": "Neuron to Graph: Interpreting Language Model Neurons at Scale"}, {"paperId": "4487bdcf1eb42bdec83709ba0df5b32dcf388976", "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization"}, {"paperId": "d61ca71f64133f12d0534f1be5d2b7a0af3d2803", "title": "Understanding the Capabilities of Large Language Models for Automated Planning"}, {"paperId": "6825ba09383bc758f9a2feaebabe35a6cd4adc4c", "title": "How Language Model Hallucinations Can Snowball"}, {"paperId": "dd889342b0de45f7434cdfa7543e3bd46ec824cb", "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations"}, {"paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56", "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting"}, {"paperId": "12910786da7a34c9ee26798fd81b0ed7b0e38789", "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing"}, {"paperId": "aec826ff336ca442697d5f908ab1668f1ea18987", "title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"}, {"paperId": "0bde1c92e1786f95a1de5ddf77a34e68ec9b9414", "title": "The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning"}, {"paperId": "9a3edb5c6b0e8c84c94ea99a9ab647b1209f650f", "title": "Why think step-by-step? Reasoning emerges from the locality of experience"}, {"paperId": "5da2d404d789aeff266b63a760d07fe8bc31ba23", "title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark"}, {"paperId": "9e4980cb927b803d375c5796f4a2eb3a7fe0555d", "title": "The Quantization Model of Neural Scaling"}, {"paperId": "1a6af6214f33187bc2c9d78bb1dc28cf5b038e16", "title": "A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks"}, {"paperId": "0ea7fc93d4947d9024ccaa202987a2070683bc1f", "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction"}, {"paperId": "154493f69d7db3d49da0e51df0192c6ad5f1724a", "title": "Larger language models do in-context learning differently"}, {"paperId": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552", "title": "The Capacity for Moral Self-Correction in Large Language Models"}, {"paperId": "5969eff0e72e4a5bc0c7392c700be74a01ac2822", "title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations"}, {"paperId": "a7fa71dc6856ebef79f354597128d1c68b19b6e4", "title": "Transformers as Algorithms: Generalization and Stability in In-context Learning"}, {"paperId": "f680d47a51a0e470fcb228bf0110c026535ead1b", "title": "Progress measures for grokking via mechanistic interpretability"}, {"paperId": "69c85405cc1986a41f6387d869aa1648a5668d6f", "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": "cef330bacf014d60daabbd489647b2006af130ca", "title": "Discovering Language Model Behaviors with Model-Written Evaluations"}, {"paperId": "fddff6eabac1607d8cdf4f576254eaf9612e6e15", "title": "Less is More: Parameter-Free Text Classification with Gzip"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "6472bda2c0c5b72d5ba563e4b0d5bba0c91eccca", "title": "Mechanistic Mode Connectivity"}, {"paperId": "99ca5162211a895a5dfbff9d7e36e21e09ca646e", "title": "Measuring Progress on Scalable Oversight for Large Language Models"}, {"paperId": "6edd112383ad494f5f2eba72b6f4ffae122ce61f", "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"}, {"paperId": "d5295f7ddcf281f3d30a7579d5ce482036a8e27c", "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task"}, {"paperId": "e82e3f4347674b75c432cb80604d38ee630d4bf6", "title": "Transformers Learn Shortcuts to Automata"}, {"paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a", "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought"}, {"paperId": "60240322dd39ad4c22fed2dc884e65a20e9e61f6", "title": "Symbols and mental programs: a hypothesis about human singularity"}, {"paperId": "f5f5616f39493566a9d502f611adcc8f1ceb394e", "title": "Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit"}, {"paperId": "142ebbf4760145f591166bde2564ac70c001e927", "title": "Language Models (Mostly) Know What They Know"}, {"paperId": "c6d38add1b7bbc10f0da37a90e3f1b51ee5fb617", "title": "Neural Networks and the Chomsky Hierarchy"}, {"paperId": "955191363c3676f71766af3d14d1e6bbc0f040d6", "title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers"}, {"paperId": "fa3609e00f9f422a309c621a35394c4a38f88687", "title": "Using cognitive psychology to understand GPT-3"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "5437e8adab596d7294124c0e798708e050e25321", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"}, {"paperId": "20de79ec4fe682b68930eb4dcd91b1801b8d4731", "title": "Towards Understanding Grokking: An Effective Theory of Representation Learning"}, {"paperId": "1fafaccebc4a74898a74c606f846318c4c2c7536", "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "c3e7f95e7d1e37003bbef8dca1aee6dcf05a5c16", "title": "Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and Explanations"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "0735fb79bf34698c1df4461a05ed51c232c412e4", "title": "Thinking Like Transformers"}, {"paperId": "e98d6f765af7a0d23b0b823a5f7a19dfe0b51840", "title": "Regular and random judgements are not two sides of the same coin: Both representativeness and encoding play a role in randomness perception"}, {"paperId": "0adec918885dff698acf359988ed79a543157f80", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"}, {"paperId": "b5ba72aaaef1ae5dccb313c64a5cfb5de3e2b442", "title": "Multimodal Neurons in Artificial Neural Networks"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "3dcfa05a1c162e6cab927c5b08d0444f7b6691f4", "title": "Probing Classifiers: Promises, Shortcomings, and Advances"}, {"paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"paperId": "73279a84026b40eadd20ca751c72e4ec1b7f4d64", "title": "A theory of memory for binary sequences: Evidence for a mental compression algorithm in humans"}, {"paperId": "7317dccaf8023b2719a2d0fe787a31b20a3232e1", "title": "A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters"}, {"paperId": "4a54d58a4b20e4f3af25cea3c188a12082a95e02", "title": "Transformer Feed-Forward Layers Are Key-Value Memories"}, {"paperId": "5eef5ed6d574ef5e75d22e774333c896ac90c958", "title": "The Computational Origin of Representation"}, {"paperId": "10c86505de83647c7b4157595ab10f64e97c94ef", "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "56ab6ccf2c0cda02660d79c104f5d29bce4312c0", "title": "Bayesian Models of Conceptual Development: Learning as Building Models of the World"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0ada4f23141e774dcac63cb8569730ddacda75c4", "title": "Does learning require memorization? a short tale about a long tail"}, {"paperId": "aa55677fa86bb05e3cb8bf403a0317f4bb1d36c1", "title": "Meta-learning of Sequential Strategies"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "8b7157531c17d91a4ade28ebc5185e418faee7a7", "title": "Volume: 3D reconstruction of history for immersive platforms"}, {"paperId": "9f1623d474ebd4b9f483f559e96e647934ed8158", "title": "Sequence"}, {"paperId": "5551c5a9f7756836fc9152eac6c04cf51affa363", "title": "Subjective randomness as statistical inference"}, {"paperId": "c44528cd1ca2c34221b35ea7e791132e8cc0995d", "title": "One model for the learning of language"}, {"paperId": "7ffb7e8c1c1ae26f55838fdd308d57ddd0856777", "title": "Human Inferences about Sequences: A Minimal Transition Probability Model"}, {"paperId": "824705cc6b405754d8e17cf24bf3ee9ccc1ed1c0", "title": "Theory learning as stochastic search in the language of thought"}, {"paperId": "20bf7a83c6b90059431fd997ee5e78b963357355", "title": "Bootstrapping in a language of thought: A formal model of numerical concept learning"}, {"paperId": "2a3842f6070b4554ff21fe62b2a486657d9a304a", "title": "How to Grow a Mind: Statistics, Structure, and Abstraction"}, {"paperId": "c08ab58e5cdeaa040ac209ac6d66cd802d9c7492", "title": "Perceptions of randomness: why three heads are better than four."}, {"paperId": "b515a5af7b85a5ad7282df8164092e52a2456f79", "title": "THE PHASE TRANSITION IN HUMAN COGNITION"}, {"paperId": "ffc42d8783d5a8d5cda8622312c7173a19999a05", "title": "What\u2019s Next? Judging Sequences of Binary Events"}, {"paperId": "56683a5ed8df93ba9366c74f7ac2bb474901a742", "title": "When in context"}, {"paperId": "3f0eefbeb6bf5c48476f2cf8d30d66464073e810", "title": "A Rational Analysis of Rule-Based Concept Learning"}, {"paperId": "ab5c79e7ccc16dc7e08824c859734d2e75c29eb9", "title": "Algorithmic information theory"}, {"paperId": "f5843c632333ab2289373dff39070b32f4beb91f", "title": "From Algorithmic to Subjective Randomness"}, {"paperId": "7851c51ee09329e0a023349602f4143331aa1705", "title": "Simplicity: a unifying principle in cognitive science?"}, {"paperId": "1cecf15069b7ee8192a8eb2ad10ac09ada791202", "title": "The production and perception of randomness."}, {"paperId": "75ac7e9fe69de3a368d1c618121100de06f9f457", "title": "The Origin of Concepts"}, {"paperId": "1050141af39d7c64303e60b608db1cec23db528d", "title": "Bayesian Modeling of Human Concept Learning"}, {"paperId": "8988dfc0758fc14e4fbc2d216247aba0ed4d1da7", "title": "GZIP file format specification version 4.3"}, {"paperId": "f4eb171a0c046554d4425bb467b0139eee9b1973", "title": "The \u201cHot Hand\u201d: Statistical Reality or Cognitive Illusion?"}, {"paperId": "75a58819a109b85f4124d137b5c4238f36057904", "title": "The mind's new science: a history of the cognitive revolution"}, {"paperId": "b85ae9f4367548a7dac7eae1a2a8d5866d27813a", "title": "On the Length of Programs for Computing Finite Binary Sequences"}, {"paperId": "6e785a402a60353e6e22d6883d3998940dcaea96", "title": "Three models for the description of language"}, {"paperId": "a131c44951b7ace0892dd830dd0a040b99ed0803", "title": "Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning"}, {"paperId": "38a4f14f8c4933a1aa8781da7d5c746879bbbc85", "title": "Physics of Language Models: Part 1, Context-Free Grammar"}, {"paperId": "ab4467bd55ddfe7b575aad37df11720ec93965d6", "title": "Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning"}, {"paperId": "a2ef68e7d559107baf1d870b85cb7dd3473da002", "title": "Transformers Implement First-Order Logic with Majority Quantifiers"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607", "title": "A Survey on Context Learning"}, {"paperId": "c32723048fad14679e0631890a166f40a5550af1", "title": "Inferring priors in compositional cognitive models"}, {"paperId": "015ca32bca81dbda1e2e432445eef798582236e1", "title": "Conference Paper"}, {"paperId": "94eb9df454da49f0af5c2fcd6779f753a0b93cdd", "title": "Creating Scientific Concepts"}, {"paperId": "f7f15848cd0fbb3d08f351595da833b1627de9c3", "title": "Information Theory, Inference, and Learning Algorithms"}, {"paperId": "068f7f90ea185f64e7467b6f4aab0e0058ecf5f3", "title": "Probability, algorithmic complexity, and subjective randomness"}, {"paperId": "b23a28cf1032aa06db2e6bf326e2005e9d4e9cb5", "title": "Making Sense of Randomness \" Implicit Encoding as a Basis for Judgment"}, {"paperId": "93ebcd2d2fcb9de3566d62df708c35858474581e", "title": "An Introduction to Kolmogorov Complexity and Its Applications"}, {"paperId": null, "title": "Information, randomness & incompleteness: papers on algorithmic information theory , volume 8"}, {"paperId": "989560956282b8545a7ea433851a0cb425dc7040", "title": "Introduction to the theory of computation"}, {"paperId": "b2f8876482c97e804bb50a5e2433881ae31d0cdd", "title": "Binary codes capable of correcting deletions, insertions, and reversals"}, {"paperId": "cb15518b8a848dc68d1c5fab02414d205ccdcb67", "title": "Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers"}, {"paperId": null, "title": "other flip to make important trends in learning dynamics more visible"}, {"paperId": null, "title": "interpretability of transformers: a case study with dyck grammars"}, {"paperId": null, "title": "5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 Sequence length | x | 0.0 0.2 p"}, {"paperId": null, "title": "Towards Developmental Interpretability"}, {"paperId": null, "title": "Can llms generate random numbers? evaluating llm sampling in controlled domains"}, {"paperId": null, "title": "A baby GPT with two tokens 0/1 and context length of 3, view"}, {"paperId": null, "title": "science to understand LLMs, we do not assume that what the LLM does should be human-like. While we propose a new term, Cognitive Interpretability, we opt"}, {"paperId": null, "title": "Mysteries of mode collapse"}, {"paperId": null, "title": "A CKNOWLEDGEMENTS ESL\u2019s time at University of Michigan was partially supported via NSF under award CNS-2008151"}]}