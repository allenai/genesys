{"paperId": "83c19732286e0716e051d1e22d426b335dacd237", "title": "Preparing Lessons for Progressive Training on Language Models", "abstract": "The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prepares lessons for expanding operations by learning high-layer functionality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even rivaling methods using pretrained models, making it a universal and efficient solution for training deep models while reducing time, financial, and environmental costs.", "venue": "AAAI Conference on Artificial Intelligence", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Apollo is proposed, which prepares lessons for expanding operations by learning high-layer functionality during training of low layers during training of low layers, and involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion."}, "embedding": {"model": "specter_v2", "vector": [0.19251996278762817, 0.6002447605133057, -0.32408690452575684, -0.19512216746807098, 0.03333941847085953, -0.060396794229745865, 0.5524659156799316, -0.11900780349969864, -0.6819555759429932, -0.49329397082328796, 0.4813750088214874, -0.010738769546151161, 0.3762170672416687, -0.009492316283285618, -0.4029009938240051, 0.28344598412513733, -1.0593974590301514, 0.4892616868019104, 0.21849682927131653, -0.2626577317714691, -0.22089160978794098, -0.2602294683456421, -1.069158911705017, 0.17602552473545074, 0.446769654750824, 0.8892636299133301, 0.1295670121908188, 0.9497959017753601, -0.47237104177474976, 0.4782453775405884, 0.24094897508621216, -0.2239735722541809, 0.6012795567512512, 0.11678797751665115, -0.381496787071228, -0.22463129460811615, 0.41136255860328674, -0.5955871939659119, -0.3864315152168274, 0.7184414863586426, -0.24500492215156555, 0.17876382172107697, -0.07913640141487122, -0.6886764764785767, 0.06196916103363037, 0.6758216619491577, 0.696003794670105, 0.5752047300338745, -0.3547767996788025, -0.6547187566757202, 1.1384022235870361, -1.2649577856063843, -0.04225938022136688, 1.2761571407318115, 0.7332990765571594, 0.5307438373565674, -0.3230742812156677, -0.6238694190979004, 0.9649224281311035, -0.042416561394929886, -0.5157018899917603, -0.1460319310426712, 0.49075019359588623, 0.1316833198070526, 1.928431510925293, -0.3453589081764221, 0.5843861699104309, 0.2338476926088333, -0.2805618345737457, 1.6679292917251587, -0.3627946674823761, -0.783501386642456, 0.1689428836107254, 0.2360813319683075, 0.1760200560092926, 1.011803150177002, -0.2893735468387604, 0.09086810052394867, -1.154047966003418, -0.07301879674196243, 0.2545940577983856, 0.003322603413835168, 0.32307183742523193, -0.35825490951538086, -0.048453450202941895, 0.7042641043663025, 0.690881073474884, 0.4315599501132965, -0.5866404175758362, 0.7759971022605896, 0.5531108975410461, 0.47194138169288635, -0.026455873623490334, 0.6772838830947876, -0.5011700391769409, 0.6377965211868286, -0.8539508581161499, -0.016174327582120895, -0.3107739984989166, 0.864095151424408, -0.281910240650177, 0.2531694173812866, -0.48390230536460876, 0.26825135946273804, 1.218958854675293, -0.39209461212158203, 0.5290827751159668, -0.7776508331298828, 0.44870272278785706, -0.34993451833724976, -0.08069959282875061, -0.1769772469997406, -0.1407129019498825, -0.6782143712043762, -0.8723231554031372, -0.903383195400238, -0.7001115679740906, 0.12911733984947205, -0.7462836503982544, 0.7595064043998718, -0.3069729804992676, 0.4677468240261078, 0.03498494252562523, -0.037826262414455414, 0.0397714264690876, 0.7166351079940796, -0.0021980912424623966, 0.1642690747976303, 0.6583440899848938, -1.2076884508132935, -0.12561723589897156, -1.3194669485092163, 0.5631601214408875, 0.12388806790113449, 0.5876343846321106, 0.24019767343997955, -1.5803790092468262, -1.1026338338851929, -0.7587104439735413, -0.11209049075841904, -0.4870266020298004, 0.010319925844669342, 1.2581567764282227, 0.1589430272579193, -1.1283302307128906, 1.1215429306030273, -0.45112866163253784, 0.185487762093544, 0.5860449075698853, 0.3607456386089325, 0.4255167245864868, -0.46238839626312256, -0.8793367743492126, 0.001240141922608018, 0.36046358942985535, -0.44681352376937866, -0.2736816108226776, -0.8945869207382202, -0.9544485807418823, 0.05234648659825325, -0.17992688715457916, -0.7722309231758118, 1.2185676097869873, -0.19058778882026672, -1.3732984066009521, 0.40332305431365967, -0.20307931303977966, -0.11917079240083694, 0.30792978405952454, -0.3349282145500183, -0.2355705201625824, -0.2504388391971588, -0.6046958565711975, 0.9185351133346558, 0.31595903635025024, -0.007670217659324408, 0.011641467921435833, 0.19269022345542908, 0.13422717154026031, -0.4389975070953369, -0.8961710929870605, 0.6739414930343628, -0.37751391530036926, -0.35745713114738464, 0.5391801595687866, 0.7039095759391785, -0.41876670718193054, 0.0016963465604931116, -0.03782431408762932, -1.1073436737060547, 0.5405495762825012, -0.22633704543113708, 0.9697659611701965, -0.845720648765564, -0.5339163541793823, -0.2723895013332367, -0.3077314794063568, -0.10702615231275558, -0.9735334515571594, 0.28701677918434143, -0.17459054291248322, 0.21986354887485504, -0.14838537573814392, -1.3519865274429321, -0.1500573456287384, -0.21750646829605103, -0.4547727704048157, -0.4113360047340393, 0.368594765663147, 0.9692136645317078, -1.0006595849990845, 0.20276154577732086, -0.06812664866447449, 0.5436639785766602, -1.7047311067581177, 1.1303741931915283, -0.6420480012893677, 0.26092609763145447, 0.28476136922836304, -0.03838729485869408, 0.12661658227443695, -0.42833271622657776, 0.30169203877449036, -0.32965636253356934, 0.04503801092505455, 0.8949705362319946, -0.2448398470878601, 1.3790570497512817, -0.7532107830047607, 0.2425873726606369, -0.10541177541017532, -0.7552870512008667, 0.1453286111354828, 0.10238654911518097, -0.35291334986686707, -0.204896479845047, 0.35713592171669006, 0.5245723724365234, -0.4488069713115692, 0.4901939332485199, 0.5458248853683472, 0.7445484399795532, -0.3624957203865051, -0.26303690671920776, 0.7662053108215332, -0.4216184914112091, 0.5973409414291382, 0.414265900850296, 0.3914414048194885, 0.17422649264335632, 0.12838928401470184, -0.20193174481391907, 0.3887241780757904, -0.9997808933258057, 0.1753210574388504, 0.32603564858436584, 0.6819295287132263, 0.7047863602638245, 0.23951247334480286, -0.9560787081718445, -0.6930639743804932, 0.08512040972709656, 0.6820072531700134, 1.3843814134597778, -0.5325928330421448, -0.07944481819868088, -0.7259110808372498, -0.278634250164032, -0.037917360663414, 0.11154425144195557, -0.33624082803726196, -0.20050226151943207, -0.5356311798095703, -0.6407948136329651, 0.5388890504837036, 0.49642157554626465, 1.2831628322601318, -0.3891286849975586, -0.26192712783813477, -0.47165530920028687, 0.6928521990776062, -0.7150876522064209, -0.45311298966407776, 0.4256947338581085, -1.1183154582977295, 0.21106339991092682, -0.09211394190788269, -0.45855289697647095, 0.5225297808647156, -0.7478206157684326, 0.9857644438743591, -0.3854368031024933, -0.07836398482322693, -0.11271286755800247, 0.9356138706207275, -0.8132165670394897, -0.6947389245033264, 0.2718508839607239, 0.3369555175304413, -0.20490016043186188, 0.11269155889749527, 0.3752564489841461, -0.02032194845378399, -0.2680368721485138, -0.2950388789176941, 0.39556169509887695, 0.12824273109436035, -0.09800734370946884, 0.7448633909225464, -0.3761410415172577, 0.2752928137779236, -1.1725168228149414, 0.7185390591621399, 0.30182188749313354, -0.6479709148406982, 0.41845065355300903, -0.5424844622612, -0.021701732650399208, 1.0150561332702637, -0.5711576342582703, -0.05200156942009926, -1.1072936058044434, 0.05090947449207306, -0.6195952892303467, 0.1139787957072258, 0.07768510282039642, 0.5402390360832214, 0.021582908928394318, 0.3049251437187195, 0.28515973687171936, 0.46722209453582764, -0.25160011649131775, -0.05714552104473114, -1.112908124923706, 0.4252304434776306, 0.36477959156036377, 0.2582452893257141, -0.15769405663013458, -0.16211017966270447, -0.5233417749404907, -0.6068046689033508, -0.21244141459465027, -0.12737923860549927, -0.17728367447853088, -0.20612899959087372, -0.6143432855606079, -0.3651442229747772, 0.11766460537910461, -1.0008455514907837, -0.5748451352119446, -0.23970669507980347, -0.1879327893257141, 0.2493228018283844, -1.1412099599838257, -1.4451520442962646, -0.42137423157691956, -0.8088130950927734, -1.054805874824524, 0.1734474152326584, 0.2384396344423294, -0.32483163475990295, -0.7368695735931396, -0.1687801033258438, -0.400583416223526, 1.2361811399459839, -0.6969144940376282, 0.6774458885192871, 0.057021476328372955, -0.23638023436069489, -0.518233060836792, 0.3560495972633362, 0.8823545575141907, -0.5394600033760071, 0.11849825084209442, -1.1406179666519165, -0.17108400166034698, -0.42256292700767517, -0.5355273485183716, 0.21802254021167755, 0.3646377921104431, 0.9886005520820618, -0.2008470892906189, -0.006301005836576223, 0.6689296960830688, 1.2778054475784302, -0.8006650805473328, 0.15776492655277252, 0.21000240743160248, 0.8197884559631348, 0.18085865676403046, -0.3330804109573364, 0.7121105194091797, 0.08225944638252258, 0.009700336493551731, 0.22372393310070038, -0.09862199425697327, -0.3500833809375763, -0.7608091235160828, 0.4464360177516937, 1.9106037616729736, 0.37105002999305725, 0.18232785165309906, -0.7428093552589417, 0.5074471235275269, -0.5733774900436401, -0.33798402547836304, 1.1004185676574707, 0.8000373840332031, 0.41131141781806946, -0.2654789090156555, -0.33149605989456177, -0.10309071093797684, 0.3242672085762024, 0.4241185188293457, -0.7338616251945496, -0.7691812515258789, 0.16422703862190247, 0.5100457072257996, 0.19543087482452393, 0.949557900428772, 0.1720990240573883, 0.46334701776504517, 14.862481117248535, 0.5645463466644287, -0.17098744213581085, 0.4147608280181885, 0.7412005066871643, 0.06551788002252579, -0.7413795590400696, -0.22585974633693695, -1.139556884765625, -0.15536366403102875, 1.1890333890914917, 0.20871049165725708, 1.1022131443023682, 0.32539138197898865, -0.1300404816865921, 0.3975808322429657, -0.4837687313556671, 0.4605788588523865, 0.5329073071479797, -1.504455327987671, 0.6743460893630981, 0.021756533533334732, 0.7722905278205872, 0.8242846131324768, 0.5219355225563049, 0.8709102869033813, 0.2932724356651306, -0.09608826786279678, 0.64223313331604, 0.1451079100370407, 0.7645799517631531, -0.08607785403728485, 0.49839329719543457, 0.4999362826347351, -0.817679762840271, -0.27709120512008667, -0.6791946887969971, -1.3928450345993042, 0.07056423276662827, 0.08971099555492401, -0.4815642833709717, -0.5594440698623657, -0.12596410512924194, 0.7545360326766968, -0.31411904096603394, 0.3873797655105591, -0.23879872262477875, 0.4345705807209015, -0.48082271218299866, 0.06267199665307999, 0.31443309783935547, 0.3465551435947418, -0.19534233212471008, -0.22953549027442932, 0.05512408912181854, -0.4796770513057709, 0.13590660691261292, 0.4057808816432953, -0.7622645497322083, -0.3267423212528229, -0.4311421811580658, -0.32427847385406494, 0.31900909543037415, 0.4976655840873718, 0.5273393392562866, -0.06563074886798859, -0.3371760845184326, 0.1278287172317505, 0.6763539910316467, 0.1428956389427185, -0.3448803126811981, -0.20443181693553925, 0.12459884583950043, -0.5090726613998413, -0.34476593136787415, 0.5680456757545471, -0.5965024828910828, -0.5242908596992493, -0.48562464118003845, -0.3000999093055725, 0.43441346287727356, -0.6252346038818359, -0.25745171308517456, 0.7510789632797241, -0.07769162207841873, -0.3857939541339874, 0.12152505666017532, -0.7651292085647583, -0.47912856936454773, 0.3472484350204468, -1.4494714736938477, -0.4425865113735199, 0.1359771341085434, -0.4676803946495056, -0.053379565477371216, -0.38374269008636475, 1.2546062469482422, 0.30617186427116394, -0.49263256788253784, 0.3009817898273468, -0.2086513489484787, -0.3428540825843811, -0.7317035794258118, -0.7642642259597778, 1.2517205476760864, 0.4233270287513733, -0.3300931751728058, 0.27038806676864624, -0.03699403256177902, 0.5132686495780945, -0.8149663209915161, -0.041266050189733505, 0.9782873392105103, -0.3274685740470886, -0.3228814899921417, -0.7559975981712341, -0.7555662393569946, 0.6601620316505432, 0.41253429651260376, -0.20716822147369385, 0.26744699478149414, 0.16521795094013214, -0.5678583383560181, -0.23409025371074677, -0.9738883972167969, 0.02327830158174038, 0.6008760333061218, -0.8049638867378235, -0.038637083023786545, 0.07882987707853317, 0.5755646824836731, -0.7194871306419373, -0.5476183891296387, 0.005803586449474096, 0.06625322252511978, -0.2615586817264557, 1.1303225755691528, -0.27015551924705505, 0.5665296912193298, 0.7783750891685486, 0.18975070118904114, -0.5983142256736755, 0.03997582197189331, -0.8077671527862549, 0.0910889208316803, -0.08903774619102478, 0.7764068245887756, -0.7147353887557983, 0.6148701310157776, 0.8963497281074524, 0.004794088192284107, -0.5195245146751404, -0.42057323455810547, -0.34495922923088074, 0.031017180532217026, -0.5252527594566345, 0.5146366357803345, 0.055014487355947495, -0.17534160614013672, 0.2880936563014984, 0.6988789439201355, 0.6030616760253906, -0.11646727472543716, -0.8809602856636047, 0.4107343554496765, -0.18770183622837067, -0.5281141400337219, -0.7206660509109497, -0.2977674603462219, -1.5097382068634033, -0.3049200475215912, -1.2645866870880127, -0.2547731399536133, -1.0037602186203003, -0.5167219042778015, -0.24807988107204437, -0.2088129073381424, -0.029672274366021156, 0.42492353916168213, -0.3825468420982361, -0.5337408185005188, -0.5967289805412292, -0.41727298498153687, 0.8867324590682983, 0.8494217991828918, -0.614244282245636, -0.034359853714704514, -0.2832948863506317, 0.36345815658569336, 0.7085201144218445, 0.6724202632904053, -0.0037549252156168222, -1.0549343824386597, -1.6559826135635376, 0.547910749912262, -0.6251091361045837, -0.1907825618982315, -0.9044634103775024, 0.7421768307685852, 0.526220440864563, -0.13668175041675568, -0.03768356889486313, 0.4368017315864563, -0.8913609385490417, -0.46851468086242676, 0.7780327200889587, -0.8602561354637146, 0.6030717492103577, 0.591820240020752, -0.7108840942382812, -0.33711403608322144, 0.3054512143135071, 0.19117596745491028, -0.9826363921165466, -0.9064763784408569, 0.2985130846500397, -0.06868782639503479, 0.061333268880844116, -0.5247856378555298, -0.03824622929096222, -0.9064638018608093, -0.09220220893621445, 0.18118825554847717, 0.6223266124725342, -0.29924502968788147, 0.5531101226806641, 0.4820568859577179, -1.143975019454956, 0.24459269642829895, 0.8608872294425964, -0.3028585910797119, 0.5502578616142273, 0.5276354551315308, 0.4802427589893341, -0.8897818326950073, 0.2186460942029953, 0.16787292063236237, 0.41088515520095825, -0.6580433249473572, -0.07985390722751617, 0.7426573038101196, -0.8861430287361145, -0.13765713572502136, 1.773983359336853, -0.38883262872695923, -1.4167442321777344, 0.6378266215324402, -1.2697573900222778, -0.34810781478881836, -0.4007266163825989, 0.7087846994400024, 0.07601788640022278, -0.08298184722661972, 0.7558348774909973, -0.4130968451499939, -0.09558691084384918, -0.2524782121181488, -0.3104816675186157, 0.8527787327766418, 0.24029788374900818, -0.31456589698791504, 0.8072555661201477, 0.9976236820220947, -0.6964988708496094, -1.1433711051940918, -0.7391379475593567, -0.6018242835998535, -0.2196078896522522, 0.32339686155319214, -0.2273428738117218, -0.8528009653091431, 0.9876129031181335, 0.18238256871700287, 0.241655170917511, 0.10392685234546661, -0.12877888977527618, 0.2953275442123413, 0.7366400957107544, 0.4879135191440582, -0.7806206941604614, -0.17210085690021515, 1.1890032291412354, 1.3632875680923462, -0.6436477303504944, 0.5282139778137207, 0.05406726151704788, -0.6004441380500793, 0.6493964791297913, 0.3804687261581421, -0.3530970513820648, 0.8296608328819275, -0.2502247393131256, -0.21878954768180847, 0.3633032739162445, -1.0006849765777588, 0.1945939064025879, 0.747558057308197, 0.7609585523605347, 0.7658824920654297, 0.047527559101581573, 0.5621607303619385, 0.7613584399223328, -0.14342910051345825, 0.08114068955183029, 0.09268609434366226, 0.611626386642456, -0.04650219529867172, 0.048496369272470474, 0.1185714527964592, 0.5876464247703552, -0.5347649455070496, -0.6716200113296509, 0.14484567940235138, 0.5036070346832275, 0.10264218598604202, 0.2790026068687439, 0.8893495798110962, 0.0650387555360794, 0.6443973779678345, 0.13115714490413666, 0.5231283903121948, -0.3930864930152893, -0.33268290758132935, -0.2589975595474243, -0.5512372851371765, 0.10529018193483353, 0.05887782201170921, -0.6564337611198425, -0.30303797125816345, -0.3267501890659332, 0.3020508885383606, -0.02831246331334114, 0.410462349653244, 0.7135921120643616, 0.7104654312133789, 0.9690372347831726, -0.0447518415749073, -0.8697214126586914, -0.520832896232605, -1.002588152885437, 0.3748146891593933, -0.5354123711585999, -0.23225407302379608, 0.0055718799121677876, -0.33357277512550354, -0.5559633374214172]}, "authors": [{"authorId": "2253844324", "name": "Yu Pan"}, {"authorId": "2221302624", "name": "Ye Yuan"}, {"authorId": "1384668226", "name": "Yichun Yin"}, {"authorId": "2279871729", "name": "Jiaxin Shi"}, {"authorId": "2238898625", "name": "Zenglin Xu"}, {"authorId": "2275295604", "name": "Ming Zhang"}, {"authorId": "50812138", "name": "Lifeng Shang"}, {"authorId": "2257942536", "name": "Xin Jiang"}, {"authorId": "2238911873", "name": "Qun Liu"}], "references": [{"paperId": "317bad11d7207183c1538db511c3592297e364e5", "title": "Reusing Pretrained Models by Multi-linear Operators for Efficient Training"}, {"paperId": "159be298e25b7210ae577d7962cceb5e73aee687", "title": "Automated Progressive Learning for Efficient Training of Vision Transformers"}, {"paperId": "1098ca3dbda5778c2bf6c9e8cbb9bc7a02249e10", "title": "Staged Training for Transformer Language Models"}, {"paperId": "54a14855b0417e5ce44c9842ae5790a260147caa", "title": "Speeding up Deep Model Training by Sharing Weights and Then Unsharing"}, {"paperId": "ea7cfe7f2340584cbe653da6077ee7c213e49b92", "title": "Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation"}, {"paperId": "3e1b060ebacfc7a966ec735c940e2ee48f2a7a99", "title": "Firefly Neural Architecture Descent: a General Approach for Growing Neural Networks"}, {"paperId": "0b98f8ec299de3358c5dfc0d842529b5aee0e97c", "title": "Progressively Stacking 2.0: A Multi-stage Layerwise Training Method for BERT Training Speedup"}, {"paperId": "2310d893abf4ec900cb9e0c5da58284a37329780", "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"}, {"paperId": "a5d6b9ed787b558e20d61bd8f5816317ef1b9a39", "title": "On the Transformer Growth for Progressive BERT Training"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "0be032a81fda05ebb7b6da552f5efa6147563ebf", "title": "Heuristic rank selection with progressively searching tensor ring network"}, {"paperId": "66aee4c300357bac539541a1df192ef908c80846", "title": "Steepest Descent Neural Architecture Optimization: Escaping Local Optimum with Signed Neural Splitting"}, {"paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0", "title": "A Primer in BERTology: What We Know About How BERT Works"}, {"paperId": "a29c3bb07d478a354fd5bc5635f98560ede8f8bb", "title": "Splitting Steepest Descent for Growing Neural Architectures"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "01bbcdf35d095d9f6ebefb135d97dd62a538fafc", "title": "Energy-Aware Neural Architecture Optimization with Fast Splitting Steepest Descent"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "3c5f1ab37f70db503636075e15b3173f86eea00b", "title": "Green AI"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "23b7d6a9fce5732ca5c5e11a3f42e17860ef05ad", "title": "Compressing Recurrent Neural Networks with Tensor Ring for Action Recognition"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "cd958525291ee1ab856d23aa93cb95c86d87ccbe", "title": "Multi-level Residual Networks from Dynamical Systems View"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "16cb6876666f3a7b56a636c1d85ad00bd0d98bf3", "title": "Net2Net: Accelerating Learning via Knowledge Transfer"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "8988742cb5658634e2a173d0d4baaaab17304229", "title": "Taking Notes on the Fly Helps Language Pre-Training"}, {"paperId": "12729c57a7cb52aab6c0eb9f8391e9014f05e256", "title": "Concatenated Tensor Networks for Deep Multi-Task Learning"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "2022. Exploring Low Rank Training of Deep Neural Networks"}, {"paperId": null, "title": "2023a. Tensor Networks Meet Neural Networks: A Survey and Future Perspectives"}, {"paperId": null, "title": "Towards Reusable Pretrained Language Models"}]}