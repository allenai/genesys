{"paperId": "f4a25d45bb381b3f6ab08e84c9a65bff90e3a104", "title": "Improving Transformers with Dynamically Composable Multi-Head Attention", "abstract": "Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy. We propose Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads. At the core of DCMHA is a $\\it{Compose}$ function that transforms the attention score and weight matrices in an input-dependent way. DCMHA can be used as a drop-in replacement of MHA in any transformer architecture to obtain the corresponding DCFormer. DCFormer significantly outperforms Transformer on different architectures and model scales in language modeling, matching the performance of models with ~1.7x-2.0x compute. For example, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining perplexity and downstream task evaluation. The code and models are available at https://github.com/Caiyun-AI/DCFormer.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "D Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads."}, "embedding": {"model": "specter_v2", "vector": [0.1643291562795639, 0.5881808996200562, -0.03911806270480156, -0.21051883697509766, -0.434407502412796, 0.15184728801250458, 0.6012228727340698, -0.4617830216884613, -0.18821802735328674, -0.5160816311836243, 1.0151240825653076, 0.3012714087963104, 0.18823090195655823, 0.22019407153129578, 0.2883385121822357, 0.017263155430555344, -1.0236201286315918, 0.7095063328742981, -0.09161423146724701, -0.43358442187309265, -0.3092702627182007, -0.6303164958953857, -1.2172932624816895, 0.39869579672813416, 0.5286098718643188, 0.6087979674339294, 0.6395954489707947, 0.7794172167778015, -0.08603736758232117, 0.220574289560318, 0.5488433837890625, -0.4613167643547058, 0.11945848166942596, -0.16513946652412415, -0.08612348884344101, -0.13363130390644073, 0.5544071197509766, -0.39855286478996277, -0.07233651727437973, 0.759648323059082, 0.16456566751003265, 0.20862792432308197, 0.526894211769104, -0.6360703706741333, -0.1093003898859024, 1.131706714630127, 0.6342764496803284, 0.9097456932067871, -0.3775172829627991, -0.8519826531410217, 1.5563141107559204, -1.9811311960220337, -0.04743322357535362, 1.7817649841308594, 0.17448639869689941, 0.491394579410553, -0.1600758135318756, -0.7531331777572632, 0.9310719966888428, 0.41915151476860046, -0.6448947191238403, -0.6598462462425232, 0.13680045306682587, -0.0962890088558197, 1.848605990409851, 0.0027123792096972466, 0.049685824662446976, 0.5064425468444824, 0.16503138840198517, 1.537322759628296, -0.2237653285264969, -0.5481273531913757, -0.44251272082328796, -0.19208049774169922, 0.40537557005882263, 0.5903300046920776, -0.5432456731796265, 0.14329594373703003, -1.0916600227355957, -0.056707341223955154, 0.3519003987312317, -0.09232287108898163, 0.0034661372192204, -0.07474260032176971, -0.5689895153045654, 0.8767220973968506, 0.20141974091529846, 0.9253947734832764, -0.4009012281894684, 0.31267863512039185, 0.4511513113975525, -0.09342740476131439, -0.025989728048443794, 0.31429678201675415, 0.014223414473235607, 0.7772780060768127, -0.9130148887634277, 0.23463281989097595, 0.023243587464094162, 0.9986244440078735, -0.28971847891807556, 0.2946755886077881, -0.9762511849403381, -0.023495793342590332, 1.4796490669250488, 0.28849339485168457, 0.5282080173492432, -0.46120768785476685, 0.17039956152439117, -0.8536128401756287, -0.34828317165374756, -0.9081876277923584, -0.5021907091140747, -0.38794228434562683, -0.9531179666519165, -1.0709786415100098, -0.5815507769584656, 0.3710736334323883, -0.7961156964302063, 0.6840403079986572, -0.24598540365695953, -0.16715267300605774, -0.6321206092834473, 0.5331827402114868, 0.33653318881988525, 0.3268269896507263, 0.6671727895736694, 0.08049999177455902, 1.0240164995193481, -1.1033697128295898, -0.957087516784668, -0.9450522661209106, 0.0013879809994250536, -0.17567360401153564, 0.09405477344989777, 0.06063435971736908, -1.3874683380126953, -0.8031923174858093, -0.7954288125038147, -0.3718627393245697, -0.16412115097045898, 0.16593271493911743, 1.0699414014816284, -0.1528226137161255, -1.2708245515823364, 0.36105233430862427, -0.4710507094860077, 0.03350096195936203, 0.1173514649271965, 0.5242332816123962, 0.20665153861045837, 0.022076085209846497, -1.230565071105957, 0.2560665011405945, 0.18444783985614777, -0.3961425721645355, -0.37896737456321716, -0.7100493907928467, -0.7689287662506104, 0.0818246603012085, -0.0798850879073143, -0.6486454606056213, 1.2244354486465454, -0.35778725147247314, -1.4513425827026367, 0.4243398904800415, -0.6072649955749512, 0.36035892367362976, -0.020826833322644234, -0.3785993158817291, -0.5710426568984985, -0.5019680857658386, -0.02451292984187603, 0.48252439498901367, 0.796074390411377, 0.19131766259670258, -0.47899121046066284, -0.14921078085899353, -0.7522900104522705, 0.18776437640190125, -0.2961994409561157, 1.3181811571121216, -0.651529848575592, -0.2676681876182556, 0.3370493948459625, 0.7989487648010254, 0.3692437410354614, -0.46345362067222595, -0.6767581105232239, -1.0520401000976562, 0.8886076807975769, -0.0003804768202826381, 1.0907455682754517, -0.8011212944984436, -0.19626151025295258, -0.19016733765602112, 0.3602008819580078, -0.387594610452652, -0.7957510352134705, 0.4211084842681885, -0.4434551000595093, -0.0261712446808815, -0.05626622587442398, -1.2929729223251343, 0.22961464524269104, -0.39481258392333984, -0.46670225262641907, -0.41564685106277466, 0.07335150986909866, 1.0097109079360962, -0.6849644780158997, -0.11616650968790054, -0.27507928013801575, 0.4572566747665405, -1.2785096168518066, 1.386857271194458, -0.0885714739561081, 0.35310617089271545, -0.1631706953048706, 0.058096062391996384, -0.11210029572248459, -0.3212814927101135, 0.5396537184715271, -0.6080149412155151, -0.09602822363376617, 0.3826356530189514, -0.4161643385887146, 1.1470685005187988, -0.20954002439975739, 0.4397970139980316, 0.280913382768631, -0.35072919726371765, 0.05096165090799332, 0.42577797174453735, -0.4372926652431488, -0.80379319190979, 0.2869260609149933, 0.50278240442276, -0.2271602302789688, 0.45089292526245117, 0.6853781342506409, 0.8162574172019958, -0.3937235176563263, -0.08772784471511841, 0.8779038190841675, -0.047699667513370514, 0.15189644694328308, 0.17550891637802124, 0.47379741072654724, -0.06973333656787872, 0.5368765592575073, -0.5402684807777405, 0.35450220108032227, -1.2265974283218384, 0.03192632645368576, 0.2321069836616516, 0.44914010167121887, 1.0828628540039062, 0.1701514720916748, -0.7402621507644653, -0.10340024530887604, 0.20572349429130554, 0.623228907585144, 2.269479990005493, -0.5141957998275757, 0.06042744219303131, -0.7088800072669983, 0.15573176741600037, -0.29972437024116516, 0.04749979451298714, -0.3585472106933594, -0.10684213042259216, -0.662541389465332, -1.2097878456115723, 0.5351174473762512, 0.1762172430753708, 0.8062770962715149, -0.6165647506713867, -0.39550790190696716, -0.06804709136486053, 0.1928078979253769, -0.6876522302627563, -0.8779375553131104, 0.6220275163650513, -0.29114630818367004, 0.030201183632016182, 0.0042749750427901745, -0.22533899545669556, -0.25129544734954834, -0.7530873417854309, 1.09303879737854, -0.7388965487480164, -0.067909836769104, -0.010796652175486088, 0.6557624340057373, -0.7918539047241211, -0.3681129515171051, 0.41195958852767944, 0.4034893214702606, 0.19948017597198486, 0.16259050369262695, 0.1964203268289566, -0.055856671184301376, -0.030621616169810295, -0.2695639431476593, 0.45529189705848694, 0.03872252255678177, -0.006036404054611921, 0.6141819953918457, -0.7311497330665588, -0.15892110764980316, -1.3498660326004028, 0.8116620182991028, 0.019543884322047234, -0.3655136525630951, 0.054160039871931076, -0.4631167948246002, -0.35391658544540405, 0.36933109164237976, -0.4699402451515198, -0.4237515926361084, -0.7384210228919983, 0.4307973086833954, -0.28169646859169006, -0.1460094451904297, 0.09678135067224503, -0.2013045698404312, 0.4072292149066925, 0.08581747114658356, 0.22788569331169128, -0.17393288016319275, -0.3067922592163086, 0.7306329011917114, -0.958687424659729, 0.8155888915061951, 0.2769835591316223, 0.15367044508457184, -0.2884875237941742, -0.33648836612701416, -0.5949866771697998, -0.3103439211845398, -0.4324292540550232, -0.36670270562171936, -0.13466861844062805, 0.27221226692199707, -0.662067711353302, -0.7352741956710815, 0.1320733278989792, -1.3375071287155151, 0.026801403611898422, 0.5676645636558533, -0.23634672164916992, 0.0968070700764656, -1.4720438718795776, -1.2501596212387085, -0.4215453267097473, -1.150396466255188, -1.1547213792800903, 0.5330821871757507, 0.07812121510505676, -0.45970970392227173, -0.4521428942680359, -0.10164724290370941, -0.5489293336868286, 1.3020741939544678, -0.6631075739860535, 0.9797945022583008, -0.46537888050079346, -0.16213254630565643, -0.02577543631196022, 0.18828527629375458, 0.1786784827709198, -0.28775137662887573, 0.03970758244395256, -1.0145080089569092, 0.39128512144088745, -0.16587960720062256, 0.07152616232633591, 0.09504818916320801, 0.22848249971866608, 0.6532292366027832, -0.2728862762451172, -0.401742160320282, 0.6124370098114014, 0.891400933265686, -0.3882012665271759, 0.02090330608189106, 0.026079652830958366, 1.2623251676559448, 0.3405638337135315, -0.6528292298316956, 0.8074443936347961, 0.8701044321060181, 0.49968957901000977, 0.4813977777957916, -0.17887268960475922, -0.3713246285915375, -0.321865439414978, 0.536736249923706, 2.0518624782562256, 0.05103721097111702, 0.0601554699242115, -1.204804539680481, 0.6345401406288147, -1.1899548768997192, -0.6237208247184753, 0.41970279812812805, 0.5594189167022705, 0.43917131423950195, -0.9265883564949036, -0.3050188720226288, -0.4428626596927643, 0.44581055641174316, 0.046730756759643555, -0.2674415111541748, -0.7005018591880798, 0.03466468304395676, 0.6535237431526184, -0.27105897665023804, 0.7834226489067078, -0.3717142939567566, 0.5031163096427917, 14.714645385742188, 0.3095923960208893, -0.09503567218780518, 0.8159171938896179, 0.6741943955421448, 0.09827583283185959, -0.497454971075058, -0.21944154798984528, -1.3036704063415527, 0.03484807163476944, 1.0740717649459839, 0.11572210490703583, 0.6346738338470459, 0.1753595918416977, -0.08456504344940186, 0.5539215803146362, -0.8329076170921326, 0.7197828888893127, 0.7798610329627991, -0.7391402721405029, 0.5951961874961853, 0.05750614032149315, 0.10937980562448502, 0.6161209940910339, 0.7414498329162598, 0.767641544342041, 0.4440152943134308, -0.3805204927921295, 0.5900366902351379, 0.5914334654808044, 0.7012270092964172, -0.09611545503139496, 0.13212959468364716, 0.3932252526283264, -1.0396721363067627, -0.22371947765350342, -0.2128998339176178, -1.219524621963501, 0.03338101506233215, 0.17233812808990479, -0.11924758553504944, -0.8465885519981384, 0.17578433454036713, 0.3818642795085907, -0.19108334183692932, 0.4329882860183716, -0.2830950617790222, 0.4130168855190277, 0.07481905072927475, 0.13833750784397125, -0.09871365875005722, 0.6345839500427246, 0.3775102198123932, 0.24484719336032867, 0.11172047257423401, 0.21482521295547485, -0.08412326872348785, 0.6360282897949219, -0.38278529047966003, -0.23654991388320923, 0.07869264483451843, 0.16802847385406494, 0.17451640963554382, 1.1845301389694214, 0.8249605894088745, 0.3801567852497101, -0.2650704085826874, 0.31366419792175293, 0.7566798329353333, 0.02079564519226551, -0.12898501753807068, -0.11096557229757309, 0.294105589389801, -0.27271026372909546, -0.08874952793121338, 0.4772800803184509, -0.1264752596616745, -0.4538605511188507, -1.0282678604125977, -0.6828364133834839, 0.4761754274368286, -0.827562689781189, -0.8399851322174072, 0.9702653288841248, 0.030098915100097656, -0.02963664010167122, 0.07487346231937408, -0.65739506483078, -0.418218731880188, 0.6358392834663391, -1.077702522277832, -0.9264655113220215, 0.31615254282951355, -0.052617259323596954, -0.14084787666797638, 0.061398863792419434, 1.3808225393295288, 0.3658975660800934, -0.56125408411026, 0.2718481719493866, -0.4464323818683624, -0.13405926525592804, -0.10924742370843887, -0.7425732612609863, 0.7201998233795166, 0.4344020485877991, -0.2005327343940735, 0.2439255267381668, 0.007008480839431286, 0.3840068280696869, -1.0258375406265259, 0.1458432525396347, 1.0882478952407837, -0.8045793771743774, -0.08263307809829712, -0.9024413824081421, -0.9341221451759338, 0.24974724650382996, 0.8986243009567261, -0.18667510151863098, 0.46465086936950684, 0.3465196490287781, -0.6279195547103882, -0.16761520504951477, -0.25501853227615356, 0.023950817063450813, 0.36919674277305603, -0.8166690468788147, -0.3958079218864441, -0.3935031592845917, 0.49391281604766846, -1.085250735282898, -0.2430357187986374, -0.39474207162857056, 0.16085469722747803, -0.09924503415822983, 0.8778145909309387, -0.20942386984825134, 0.24829219281673431, 0.9577199816703796, -0.22713632881641388, -0.8214987516403198, -0.6074510812759399, -0.8578600287437439, -0.08023366332054138, 0.12385746836662292, 0.7123141884803772, -0.11919444799423218, -0.3717559576034546, 1.1166976690292358, 0.08319652080535889, -0.1697094440460205, -0.6827651262283325, 0.2984493374824524, -0.025756411254405975, -0.5868664979934692, 0.3528994023799896, -0.09147034585475922, 0.053107570856809616, 0.3414665758609772, 0.2884354591369629, 0.4649510383605957, 0.10052067786455154, -0.8433729410171509, 0.011552383191883564, -0.3814956843852997, -0.09484855830669403, -0.6007239818572998, -0.36542969942092896, -0.8822394609451294, -0.14578667283058167, -1.243106722831726, 0.22224293649196625, -0.6678873300552368, 0.08654388785362244, 0.3288193941116333, -0.41271424293518066, 0.5579568147659302, -0.05274512618780136, -0.3252132833003998, -0.07342608273029327, -0.23230089247226715, -0.6188600063323975, 0.8815725445747375, 1.0296361446380615, -0.5640226602554321, 0.09539158642292023, -0.24445202946662903, -0.15715306997299194, 0.3432937562465668, 0.26720020174980164, -0.3209768533706665, -0.46955063939094543, -1.5682761669158936, 0.3014512360095978, -0.04419402778148651, 0.011787083931267262, -0.8784824013710022, 1.0547504425048828, 0.5348383188247681, -0.08109569549560547, 0.031059611588716507, 0.18498040735721588, -0.7895670533180237, -0.4267021119594574, 0.1030961275100708, -0.5836326479911804, 0.50888592004776, 0.1890026330947876, -0.8888482451438904, -0.3071722090244293, 1.2241132259368896, -0.20774130523204803, -0.9393178224563599, -0.758637547492981, 0.4887394309043884, -0.7033483982086182, 0.19680577516555786, -0.820704460144043, 0.1474042534828186, -1.0972527265548706, -0.2951359450817108, 0.20472995936870575, 0.11704999953508377, -0.7028899192810059, 0.8923631906509399, 0.5122238397598267, -1.2258464097976685, -0.1337154358625412, 0.6752617955207825, -0.13151371479034424, -0.039833370596170425, 0.2664375603199005, 0.6483055949211121, 0.09579423815011978, 0.48478835821151733, 0.07835471630096436, 0.28212040662765503, -0.7604870200157166, -0.25996366143226624, 0.7857462167739868, -0.4515015184879303, -0.004473844077438116, 1.2613139152526855, -0.031186768785119057, -0.9977905750274658, 0.2248515486717224, -1.0320801734924316, -0.6403611898422241, 0.019772974774241447, 1.0023105144500732, 0.21017104387283325, 0.1272548884153366, -0.44334518909454346, -0.8208749294281006, 0.09627196937799454, -0.2017943412065506, -0.5893651247024536, 0.5446519255638123, 0.04239747300744057, -0.721895694732666, 0.4239003360271454, 0.9019210338592529, -0.6512800455093384, -0.442463755607605, -0.5895638465881348, -0.2500341832637787, 0.5654792189598083, 0.48757192492485046, -0.44142356514930725, -0.7892967462539673, 0.852335512638092, 0.6519489288330078, -0.01235132198780775, 0.4954819083213806, -0.23267902433872223, 0.09585677832365036, 0.4384423792362213, 0.25191426277160645, -0.557957112789154, -0.7140142917633057, 1.6220930814743042, 1.5791257619857788, -0.7583588361740112, 0.06792693585157394, -0.11043226718902588, -0.8748889565467834, 0.8728785514831543, 0.27646133303642273, 0.05280409753322601, 0.9663806557655334, 0.1449383646249771, -0.07420206069946289, 0.09181898087263107, -1.1977572441101074, -0.5823573470115662, 1.373559832572937, 1.1977304220199585, 1.0654884576797485, 0.6277316212654114, 0.13331253826618195, 1.253928780555725, 0.14199021458625793, -0.21170224249362946, 0.07603573799133301, -0.04665035754442215, -0.25241246819496155, -0.12270034104585648, -0.21591316163539886, 0.7211071252822876, -0.6570534110069275, -0.9279397130012512, 0.09746827185153961, 0.4738408029079437, 0.16588568687438965, 0.726115345954895, 0.7271738052368164, 0.016397273167967796, 0.6434881687164307, 0.19708289206027985, 0.11777599900960922, -0.7333659529685974, -0.40892142057418823, 0.1761338710784912, -0.6428507566452026, -0.3922809660434723, -0.16079743206501007, -0.4991188943386078, -0.35869359970092773, 0.0557786226272583, 0.3188513219356537, -0.46648481488227844, 0.26857906579971313, 0.9627276659011841, 0.7758487462997437, 0.6290351748466492, -0.3287973999977112, -0.9368371367454529, -0.23464947938919067, -1.1052802801132202, -0.03120586834847927, -0.6949399709701538, -0.09083425253629684, -0.1340406835079193, -0.2646360993385315, -0.33409056067466736]}, "authors": [{"authorId": "2301203832", "name": "Da Xiao"}, {"authorId": "2301433926", "name": "Qingye Meng"}, {"authorId": "2301254852", "name": "Shengping Li"}, {"authorId": "50242841", "name": "Xingyuan Yuan"}], "references": [{"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "3820231d31540ecb05d94c74d959a2f61d3136ea", "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token"}, {"paperId": "c7fa5c2172a4624d6baa91e66344e4520d3028ad", "title": "Analyzing Transformers in Embedding Space"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "7d67b5cfd19927b30f07de25145f20bf95766e3c", "title": "Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc", "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "14216c91c7d02e58717204f04131107778a84e7b", "title": "Multi-Head Attention: Collaborate Instead of Concatenate"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "26080498fb851b6239114f0871a4957bea3d3684", "title": "Talking-Heads Attention"}, {"paperId": "b1c39d042fdf8f00a407b0df734764beb6c3b062", "title": "Low-Rank Bottleneck in Multi-head Attention Models"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "6c9bfe765f076422256fef909bdca186b5880c52", "title": "Information Aggregation for Multi-Head Attention with Routing-by-Agreement"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "3861ae2a6bdd2a759c2d901a6583e63a216bc2fc", "title": "Weighted Transformer Network for Machine Translation"}, {"paperId": "932a5de79d8a8ebb75ea0c43493450fd9922e738", "title": "Crowdsourcing Multiple Choice Science Questions"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "f302e136c41db5de1d624412f68c9174cf7ae8be", "title": "Axiomatic Attribution for Deep Networks"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "33aa588c89401cbc22a2f4d2fb121d10909a6f18", "title": "Low-rank plus diagonal adaptation for deep neural networks"}, {"paperId": "88695b5bb6462872ce1dd946cff00dd6ebabf2d9", "title": "Scaling TransNormer to 175 Billion Parameters"}, {"paperId": "05b22d6ec2cff81bcfbac2a6cf67bc1e9ef0f60a", "title": "Improving Transformer with an Admixture of Attention Heads"}, {"paperId": "363b476587afe486c0301ec1034e04a210fac552", "title": "Improved Transformer With Multi-Head Dense Collaboration"}, {"paperId": "338d0501947b5fc7d92d09eed9a3e299f7b48ec1", "title": "Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": "f40aeae3e522ada1f6a9f326841b01ef5c8657b6", "title": "Unifying Language Learning Paradigms"}, {"paperId": null, "title": "A mathematical framework for transformer circuits"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Dynamically Composable Multi-Head Attention"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}]}