{"paperId": "ca31b8584b6c022ef15ddfe994fe361e002b7729", "title": "A Comprehensive Overview of Large Language Models", "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to a large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations, better training strategies, context length improvements, fine-tuning, multi-modal LLMs, robotics, datasets, benchmarking, efficiency, and more. With the rapid development of techniques and regular breakthroughs in LLM research, it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs, it is imperative that the research community is able to benefit from a concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the existing literature on a broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to not only provide a systematic survey but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research.", "venue": "arXiv.org", "year": 2023, "citationCount": 157, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2307.06435", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A self-contained comprehensive overview of the existing literature on a broad range of LLM-related concepts discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs."}, "embedding": {"model": "specter_v2", "vector": [0.19307975471019745, 0.41138455271720886, -0.3026278614997864, -0.2822451889514923, -0.3398992121219635, -0.12082798779010773, 0.5341765880584717, -0.11739051342010498, -0.7429533004760742, -0.21704652905464172, 0.46382439136505127, 0.053516045212745667, 0.45794373750686646, 0.4166746735572815, -0.10763460397720337, 0.2659454643726349, -0.9734986424446106, 0.5343756675720215, -0.16170014441013336, -0.23136332631111145, -0.44871923327445984, -0.49819427728652954, -0.8713619709014893, 0.1505458801984787, 0.06617815047502518, 0.013023033738136292, 0.8263753056526184, 0.8050899505615234, -0.42954573035240173, 0.496943861246109, 0.7373543381690979, -0.11424150317907333, 0.0034267816226929426, 0.11049165576696396, -0.14652760326862335, -0.07098932564258575, 0.37865132093429565, -0.49927666783332825, -0.6291102766990662, 0.4414004981517792, -0.3111019432544708, 0.5596779584884644, 0.2722788453102112, -0.5610531568527222, -0.14940860867500305, 1.0451321601867676, 0.7549803256988525, 0.476455420255661, -0.18049338459968567, -0.5792802572250366, 1.4678094387054443, -1.4355417490005493, 0.3194504678249359, 1.8908132314682007, 0.47227054834365845, 0.39625149965286255, -0.48972606658935547, -0.796825647354126, 0.783310055732727, -0.35287925601005554, -1.0441062450408936, -0.5519501566886902, -0.08718525618314743, -0.1422688364982605, 2.1013705730438232, -0.05379698798060417, -0.16380351781845093, 0.7465212941169739, 0.04928458482027054, 1.5201270580291748, 0.0393233485519886, -1.2203705310821533, -0.36197513341903687, 0.022326096892356873, 0.19445978105068207, 0.9915938973426819, -0.6172756552696228, 0.6015698313713074, -0.8813484907150269, -0.1838444173336029, 0.35671916604042053, -0.1458393633365631, 0.15322867035865784, -0.0629572719335556, -0.5368314385414124, 1.0166419744491577, 0.1472032517194748, 0.9382493495941162, 0.10490749031305313, 0.5355815887451172, 0.39375707507133484, 0.3645123243331909, -0.07723766565322876, 0.3970373868942261, -0.32314634323120117, 0.5233790874481201, -0.8512454032897949, 0.2826021909713745, 0.18670648336410522, 1.0391175746917725, -0.43704134225845337, 0.24873003363609314, -0.7968766093254089, 0.5962085127830505, 1.4773457050323486, 0.17394499480724335, 0.5895640850067139, -0.6900529265403748, 0.5044538974761963, -0.5884515047073364, 0.15760600566864014, -0.5717911720275879, -0.5371163487434387, -0.1979496031999588, -0.5935191512107849, -1.3327926397323608, -0.4220772981643677, 0.07322388142347336, -0.5650409460067749, 1.0575419664382935, -0.5042564868927002, -0.3035924434661865, 0.23794753849506378, 0.15845252573490143, 0.334214985370636, 0.8122380375862122, 0.5357809066772461, -0.06464053690433502, 1.0694254636764526, -0.7655196189880371, -0.7271853685379028, -1.7056492567062378, 0.9763902425765991, -0.2618257999420166, 0.34034809470176697, -0.5787079334259033, -0.8798022866249084, -0.8753988742828369, -0.6354823112487793, -0.21739977598190308, -0.6563084721565247, 0.8467980623245239, 0.9464194178581238, 0.12757037580013275, -1.2357146739959717, 0.23395854234695435, -0.055055323988199234, -0.24832472205162048, -0.0981745794415474, 0.27895841002464294, 0.07617942243814468, -0.5146803259849548, -1.4841965436935425, 0.49695661664009094, 0.5274541974067688, -0.41490983963012695, -0.3486292064189911, 0.1992674618959427, -1.4396634101867676, -0.3669518530368805, 0.05082975700497627, -0.45043647289276123, 1.20120108127594, -0.05020483210682869, -1.2824605703353882, 0.7291423678398132, -0.6476790904998779, 0.12177299708127975, 0.10755357146263123, -0.23272553086280823, -0.8043409585952759, -0.8101581335067749, -0.40395593643188477, 0.7900094985961914, 0.27580997347831726, -0.21578329801559448, -0.25141775608062744, 0.0547451488673687, -0.16171897947788239, 0.058154601603746414, -0.23267769813537598, 1.285752296447754, -0.3713817596435547, -0.3857828378677368, 0.5984153151512146, 0.3551212549209595, -0.44425734877586365, -0.20074677467346191, -0.33495762944221497, -0.8078874349594116, 0.714309573173523, -0.1060865968465805, 1.330331563949585, -0.8037599325180054, -0.7616255283355713, -0.30445802211761475, -0.23286600410938263, -0.06624945253133774, -1.2235685586929321, 1.080643892288208, -0.29822853207588196, 0.2719634771347046, -0.20833644270896912, -1.5117019414901733, -0.18444134294986725, 0.1402023285627365, -0.5445704460144043, -0.14069075882434845, -0.10233558714389801, 1.032788872718811, -0.9639769792556763, -0.0851275846362114, 0.26758381724357605, 0.09477211534976959, -0.6637160181999207, 1.3384151458740234, -0.4823860228061676, -0.011903307400643826, 0.013201246038079262, -0.29446250200271606, -0.09324643760919571, -0.18792645633220673, 0.5606974363327026, -0.09883545339107513, -0.3103247582912445, 0.2932155728340149, -0.37657630443573, 1.527748465538025, -0.49447014927864075, 0.6192659139633179, -0.09208129346370697, -0.17467497289180756, -0.29466158151626587, 0.6863952279090881, -0.4408991038799286, -0.1878185123205185, 0.4054549038410187, 0.3842463493347168, -0.6354554295539856, 0.3402755558490753, 0.9847615957260132, 0.6549603939056396, -0.25492748618125916, 0.24711357057094574, 0.2944529950618744, -0.3692317008972168, 0.560038685798645, 0.43762195110321045, 0.32501810789108276, 0.158827543258667, 0.538769543170929, -0.045779481530189514, 0.6166883707046509, -0.9042968153953552, -0.3437610864639282, 0.4953559339046478, 0.7060085535049438, 0.6460952162742615, -0.018928926438093185, -0.3822455406188965, -0.0740211009979248, 0.22237151861190796, 0.7672450542449951, 1.800355076789856, -0.25464698672294617, -0.23723027110099792, -0.6585874557495117, -0.08567162603139877, -0.35340508818626404, 0.32446426153182983, -0.3026321232318878, 0.2632235884666443, -0.7337536811828613, -0.6860410571098328, 0.892768383026123, 0.39949923753738403, 0.2768931984901428, -0.7598217129707336, -0.22180363535881042, -0.058176781982183456, 0.12892228364944458, -1.1245472431182861, -0.6544741988182068, 0.3819129467010498, -1.0193653106689453, -0.21612805128097534, 0.26557406783103943, -0.40399232506752014, 0.3076219856739044, -0.46371039748191833, 0.9461458325386047, -0.7680398225784302, -0.006735936738550663, -0.11244774609804153, 0.673083484172821, -0.598928689956665, -1.1787973642349243, -0.039270151406526566, 0.3276153802871704, -0.28014394640922546, 0.269400954246521, 0.8023255467414856, 0.7600855827331543, -0.03671140968799591, -0.5680552124977112, 0.47625336050987244, 0.23824530839920044, -0.033796221017837524, 0.9608181715011597, -0.47614631056785583, -0.12859268486499786, -1.3880505561828613, 1.1925945281982422, 0.31756553053855896, -0.5477655529975891, 0.4667421579360962, -0.45948365330696106, -0.1521911770105362, 0.5192065238952637, -0.7303822040557861, -0.40358486771583557, -0.4012375473976135, 0.43322649598121643, -0.29250410199165344, -0.5091404914855957, 0.5026166439056396, 0.22462745010852814, 0.45022645592689514, 0.11927127093076706, 0.5073180198669434, 0.33150362968444824, -0.36307358741760254, 0.7060638666152954, -0.36292025446891785, 0.20662079751491547, 0.4802107810974121, 0.06912821531295776, -0.21513521671295166, -0.4642764627933502, -0.7733104228973389, -0.31048867106437683, -0.6287112832069397, -0.31165972352027893, -0.15105363726615906, -0.008108540438115597, -0.7399121522903442, -0.29903295636177063, 0.10781247913837433, -1.1910699605941772, 0.10595380514860153, 0.5015871524810791, -0.08214804530143738, 0.08984890580177307, -0.8754443526268005, -1.347740650177002, -0.7796210646629333, -0.3800504505634308, -0.8820502161979675, 0.40698572993278503, -0.13876189291477203, -0.5248170495033264, -0.7065397500991821, 0.10271728783845901, -0.06608329713344574, 1.0193766355514526, -0.6339213848114014, 1.132135033607483, -0.21349242329597473, 0.009506001137197018, -0.5722112655639648, 0.39286553859710693, 0.28248223662376404, -0.1421881765127182, 0.339650958776474, -0.8673311471939087, -0.08040160685777664, -0.35406494140625, -0.15328389406204224, -0.10603386908769608, 0.37227481603622437, 0.5553160905838013, 0.15824729204177856, -0.6570742130279541, 0.01766893081367016, 1.294485330581665, -0.5501601099967957, -0.5595491528511047, -0.1761326938867569, 0.9594964981079102, 0.299044132232666, -0.47550371289253235, 0.3766094744205475, 0.36646705865859985, 0.48240262269973755, 0.28300178050994873, -0.12551431357860565, 0.0493023507297039, -0.4837213456630707, 0.724576473236084, 1.7973535060882568, 0.49497124552726746, -0.2196129709482193, -0.968886137008667, 0.43570512533187866, -1.229277491569519, -0.5641260147094727, 0.5714446902275085, 0.7408730387687683, 0.5538277626037598, -0.5185443162918091, -0.5099108815193176, -0.38231033086776733, 0.5970785617828369, 0.7447124719619751, -0.16899631917476654, -0.6450129151344299, -0.3941177427768707, -0.03604013845324516, -0.18603461980819702, 0.7717061638832092, -0.7526338696479797, 0.8089585304260254, 14.540865898132324, 0.870880663394928, -0.026464808732271194, 0.6379189491271973, 0.5115625858306885, 0.23390713334083557, -0.37841689586639404, -0.24981163442134857, -1.3637099266052246, -0.33007141947746277, 1.3645728826522827, 0.3550856113433838, 0.8883959054946899, 0.21989576518535614, 0.3963826596736908, 0.013590214774012566, -0.7760359048843384, 0.8464934229850769, 0.47441092133522034, -0.9466813206672668, 0.5424856543540955, -0.04340915009379387, 0.3595828115940094, 0.8405846357345581, 0.4011687934398651, 0.8429200649261475, 0.51258784532547, -0.5283756256103516, 0.745526134967804, 0.1442776322364807, 0.6954125165939331, 0.12932375073432922, 0.3445958197116852, 1.1609340906143188, -1.2377899885177612, -0.4725070893764496, -0.5702300667762756, -1.3088929653167725, 0.3286696970462799, 0.022728508338332176, -0.3508602976799011, -0.5904165506362915, -0.6812588572502136, 0.7435416579246521, -0.03974774852395058, 0.2594568729400635, -0.13705043494701385, 0.5465919375419617, -0.21850177645683289, 0.06233688071370125, 0.3855099081993103, 0.19404880702495575, 0.5224853157997131, 0.12031421810388565, -0.1268201619386673, -0.06905883550643921, 0.3039579689502716, 0.19468019902706146, -0.5656614303588867, 0.37126046419143677, -0.5518076419830322, -0.6650144457817078, -0.0955793485045433, 0.6405708193778992, 0.44619712233543396, 0.37292739748954773, -0.5102385878562927, 0.15731115639209747, 0.8827791810035706, 0.282909631729126, -0.07783046364784241, 0.37758925557136536, 0.27437692880630493, -0.5234858989715576, -0.23741289973258972, 0.38857361674308777, -0.025405706837773323, -0.6771450638771057, -0.6254309415817261, -0.24317190051078796, 0.3499518632888794, -0.6775382161140442, -1.0645591020584106, 1.075377106666565, -0.10316731035709381, -0.346523255109787, -0.03597632423043251, -0.8265529870986938, 0.08536012470722198, 0.6783285737037659, -1.1892262697219849, -0.9112591743469238, 0.5818057060241699, -0.2827746272087097, -0.05111606419086456, -0.042087383568286896, 1.537018895149231, -0.06829235702753067, -0.7589625716209412, -0.060912713408470154, 0.5004864931106567, 0.07499390095472336, -0.5993533730506897, -0.21540144085884094, 0.5696452260017395, 0.4827217161655426, 0.24339498579502106, 0.4953019618988037, -0.1024719774723053, 0.11412742733955383, -0.6796848773956299, -0.19881971180438995, 1.3627026081085205, -0.8348305225372314, -0.4519617259502411, -0.9566576480865479, -0.8311778903007507, 0.44448238611221313, 0.4341897666454315, -0.3512769043445587, 0.5003142356872559, 0.12704700231552124, -0.5389013290405273, 0.1307869255542755, -0.6407725811004639, 0.08032888174057007, 0.46892887353897095, -1.0559309720993042, -0.4018918573856354, 0.29926416277885437, 0.2900110185146332, -0.9652692079544067, -0.13231202960014343, -0.3288615643978119, -0.02284226194024086, 0.4101341664791107, 0.7661803364753723, -0.7921863794326782, 0.4235445261001587, 0.8645675778388977, -0.5446443557739258, -0.8140802383422852, -0.063574880361557, -0.6700406670570374, -0.25441262125968933, -0.26383596658706665, 0.9553232192993164, -0.41998282074928284, -0.16395461559295654, 0.9785901308059692, 0.4184604585170746, -0.6285433173179626, -0.7893179655075073, -0.4497512876987457, -0.06970342248678207, -0.7208496928215027, 0.19046160578727722, -0.2414550930261612, -0.21001069247722626, 0.2716447710990906, 0.2619002163410187, 0.744697630405426, -0.40329161286354065, -0.43848466873168945, 0.20669220387935638, -0.2106418013572693, -0.0631152093410492, -0.48981818556785583, -0.08764171600341797, -1.510489583015442, 0.33319538831710815, -1.1955424547195435, 0.20079456269741058, -1.1327648162841797, -0.3504278361797333, 0.04297299683094025, -0.1573338359594345, 0.1332651674747467, 0.5637401938438416, -0.6563507914543152, -0.3026324510574341, -0.19832812249660492, -0.29753679037094116, 0.7800569534301758, 0.7451801896095276, -0.46272382140159607, -0.0006896738195791841, 0.11830240488052368, 0.6185990571975708, 0.2779428958892822, 0.42306792736053467, -0.6099264025688171, -1.0140312910079956, -1.6255855560302734, 0.37161386013031006, 0.046245425939559937, -0.4526160657405853, -0.710938036441803, 0.5612332820892334, -0.01773981750011444, -0.35010796785354614, 0.15754686295986176, 0.6576664447784424, -1.1090686321258545, -0.24808667600154877, 0.46048039197921753, -1.0639209747314453, 0.43650394678115845, 0.21687482297420502, -0.4245798885822296, -0.5828380584716797, 0.584842324256897, -0.009344588033854961, -1.0635639429092407, -0.6392731666564941, 0.5930796265602112, -0.749722957611084, -0.005843930412083864, 0.09452218562364578, -0.09464573860168457, -0.6067016124725342, -0.4111880362033844, -0.1426970362663269, 0.37833356857299805, -0.4191281199455261, 1.29867684841156, 0.5780234336853027, -0.931792676448822, -0.07368052750825882, 0.5461772084236145, 0.20871368050575256, -0.26600122451782227, 0.2529548704624176, 0.23176610469818115, -0.2795545756816864, 0.9809688329696655, 0.5400134921073914, 0.47505438327789307, -1.3629531860351562, -0.19027258455753326, 0.9324057698249817, -0.5011771321296692, -0.06132281944155693, 1.424952507019043, -0.22739674150943756, -1.3160258531570435, 0.3134545385837555, -1.2669483423233032, -0.7875068783760071, -0.493486613035202, 0.6027461886405945, -0.08347582817077637, -0.08967830240726471, -0.7794600129127502, -0.45808902382850647, 0.0028837642166763544, 0.1219099760055542, -0.5904852747917175, 0.5155338644981384, -0.41561636328697205, -0.450814813375473, 0.925592303276062, 0.8859108090400696, -0.5161129832267761, -0.6046051383018494, -0.7649650573730469, -0.2774389386177063, -0.07559870183467865, 0.3660360276699066, -0.4853975176811218, -0.6085492372512817, 0.5919896960258484, 0.5175467729568481, 0.07175726443529129, -0.06066666170954704, -0.17131972312927246, 0.30182084441185, 0.8172891736030579, 0.27851414680480957, -0.7690698504447937, -0.967494785785675, 1.7222576141357422, 1.4753330945968628, -1.070374846458435, -0.13492563366889954, -0.15209530293941498, -0.6651881337165833, 0.8087379336357117, 0.1261388659477234, 0.29341962933540344, 1.1589789390563965, -0.2988649308681488, 0.25581610202789307, 0.20165057480335236, -1.0462205410003662, 0.12239906936883926, 0.8395900130271912, 0.5777982473373413, 1.100796103477478, 0.7938275337219238, -0.14294178783893585, 0.6401325464248657, 0.14410173892974854, 0.2633489668369293, 0.2861877381801605, 0.7867724299430847, -0.27113643288612366, -0.30631133913993835, 0.28704771399497986, 0.5282133221626282, -0.36598095297813416, -0.8694689869880676, 0.13395966589450836, 0.5959468483924866, 0.23226295411586761, 0.7468173503875732, 0.5061818957328796, 0.1587601900100708, 0.45400989055633545, 0.5394679307937622, 0.30564364790916443, -0.8615853786468506, -0.12605330348014832, -0.47608405351638794, -0.4925377070903778, -0.14149653911590576, -0.2893263101577759, -0.33410918712615967, -0.33361074328422546, -0.017156284302473068, 0.24904286861419678, -0.2163158357143402, 0.5011361241340637, 1.172700047492981, 0.3878099322319031, 0.00713807437568903, -0.5452151298522949, -0.030703242868185043, -0.47564294934272766, -1.2574717998504639, -0.3188178241252899, -0.6726371049880981, -0.2006930708885193, 0.07957901060581207, 0.011411105282604694, -0.2950953245162964]}, "authors": [{"authorId": "32749940", "name": "Humza Naveed"}, {"authorId": "2201619569", "name": "Asad Ullah Khan"}, {"authorId": "2055117257", "name": "Shi Qiu"}, {"authorId": "2153464760", "name": "Muhammad Saqib"}, {"authorId": "49053414", "name": "Saeed Anwar"}, {"authorId": "2223436301", "name": "Muhammad Usman"}, {"authorId": "1712576", "name": "Nick Barnes"}, {"authorId": "1747500", "name": "A. Mian"}], "references": [{"paperId": "44d16a076c00ecada3d425203377e4ec951c4ed0", "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning"}, {"paperId": "575a215f1080dfc0e9f8d055775c294e765aac84", "title": "Leveraging Large Language Models for Decision Support in Personalized Oncology"}, {"paperId": "f1731ac89acecc0f3f6a531df49e79d3e50003c5", "title": "Using Large Language Models to Support Thematic Analysis in Empirical Legal Studies"}, {"paperId": "df2ed9f2d994cc91a710261398ff04b01d1a9f7c", "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack"}, {"paperId": "0f92d5a01baa449edc5592716dd639ec7868c44f", "title": "Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations"}, {"paperId": "4f63c5a89c7299a864c6c48aa1844fb0fe8c9437", "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"}, {"paperId": "9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632", "title": "Large Language Models for Scientific Synthesis, Inference and Explanation"}, {"paperId": "dc976bca01f879f18c9302c9119d7f1ba41332b7", "title": "Contribution and performance of ChatGPT and other Large Language Models (LLM) for scientific and research advancements: a double-edged sword"}, {"paperId": "78a48c296267ae59b4be60ebbbf96b5d7f6c693b", "title": "Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations"}, {"paperId": "faab24bc6cd4a4dea6e82420d145f08445c05fc7", "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"}, {"paperId": "5f5b57728562e43cb564eb6ebd19a81a39120387", "title": "Using Generative AI for Literature Searches and Scholarly Writing: Is the Integrity of the Scientific Discourse in Jeopardy?"}, {"paperId": "a8aef5a15dc2b486b3bf01205d2687a1140a41bb", "title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework"}, {"paperId": "5432b77bfb1dced97c5b1fc684b0fa7d0d84c424", "title": "Large Language Models in Finance: A Survey"}, {"paperId": "3041dc1dc5db0c332b8ee55b11db47e817f4527a", "title": "Large language model (LLM)-driven chatbots for neuro-ophthalmic medical education."}, {"paperId": "b6346f9fa093b8e85df712485a2b851b9f680dac", "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"}, {"paperId": "8b37d13e9ba87792d949e8af5f13ef8f83ee0bf9", "title": "Does ChatGPT Help Us Understand the Medical Literature?"}, {"paperId": "755853c6b30f5a186131e23a63c68a3f2737068e", "title": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models"}, {"paperId": "0c72450890a54b68d63baa99376131fda8f06cf9", "title": "The Rise and Potential of Large Language Model Based Agents: A Survey"}, {"paperId": "5a9d4bcffa9989cac4139b2844358884ae023e8d", "title": "SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments"}, {"paperId": "10fa85cdb6c64f2e9081c3422de628686ae87e6f", "title": "Data Decentralisation of LLM-Based Chatbot Systems in Chronic Disease Self-Management"}, {"paperId": "d00735241af700d21762d2f3ca00d920241a15a4", "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models"}, {"paperId": "26089bdfdbca1e6eaaceca71e3116b715bec6d47", "title": "Explainability for Large Language Models: A Survey"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "24d836bbc35413d76c3c69cb30bfc0f1449f5207", "title": "ProgPrompt: program generation for situated robot task planning using large language models"}, {"paperId": "28c6ac721f54544162865f41c5692e70d61bccab", "title": "A Survey on Large Language Model based Autonomous Agents"}, {"paperId": "2dfb9171e180dcb0af23d305e024d43d311708ab", "title": "Giraffe: Adventures in Expanding Context Lengths in LLMs"}, {"paperId": "338d8f3b199abcebc85f34016b0162ab3a9d5310", "title": "A Survey on Model Compression for Large Language Models"}, {"paperId": "02d4096c030d052e1866d52fbc3b83480e1ed9f5", "title": "Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques"}, {"paperId": "81b10e64133e775dab53153cc82277d276efe1f7", "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization"}, {"paperId": "91206346edbe28abb606d7b3425cd455d4019d4f", "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"}, {"paperId": "e1ea0e1614baf4e247ae4e38e9dd033c8af8070a", "title": "Ethical Considerations and Policy Implications for Large Language Models: Guiding Responsible Development and Deployment"}, {"paperId": "446fb5dead075a1a08862662738f462e9a0e91c8", "title": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"}, {"paperId": "703035b483c181953de1b55b5fd59cd4cd4cf211", "title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework"}, {"paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628", "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"}, {"paperId": "af6d0ba799213cbbcbfceb1fb9b78d2858486308", "title": "Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition"}, {"paperId": "01d31fb9fc6ab36df6627b8555b64789113eb7a5", "title": "RLCD: Reinforcement Learning from Contrastive Distillation for Language Model Alignment"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "ae22f7c57916562e2729a1a7f34298e4220b77a7", "title": "Learning to Retrieve In-Context Examples for Large Language Models"}, {"paperId": "c5d18dbb92d0cd5393baa1e69de33d6922ac3e57", "title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models"}, {"paperId": "467c49fd4f69638ddeda1ff3241fe4495151cf0e", "title": "A Domain-Specific Next-Generation Large Language Model (LLM) or ChatGPT is Required for Biomedical Engineering and Research"}, {"paperId": "0893549771094fac547432cb4f84e9605c911a86", "title": "The imperative for regulatory oversight of large language models (or generative AI) in healthcare"}, {"paperId": "929305892d4ddae575a0fc23227a8139f7681632", "title": "Jailbroken: How Does LLM Safety Training Fail?"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "ae2d35aeb5dd7043ac0056d60002e2e3c240bd1a", "title": "Exploring the potential of Chat-GPT as a supportive tool for sialendoscopy clinical decision making and patient information support."}, {"paperId": "065970427ce71b73a99e520e868625fe05664295", "title": "Artificial Intelligence and Public Health: Evaluating ChatGPT Responses to Vaccination Myths and Misconceptions"}, {"paperId": "3c996ace0fc6c54dbd9035d92d731bd9a57f8c6d", "title": "Can Large Language Models Provide Feedback to Students? A Case Study on ChatGPT"}, {"paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e", "title": "Preference Ranking Optimization for Human Alignment"}, {"paperId": "228aee5393e7a11e018bbef940fea1c2816b6ec4", "title": "Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "87875a07976c26f82705de1fc70041169e5d652b", "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models"}, {"paperId": "ebedc4d7a2356090904baba4104ef0832bc236df", "title": "A Survey on Multimodal Large Language Models"}, {"paperId": "cbbc2cc774c50b0b19922185b80e9ce90b7cd2f6", "title": "Retrieval-Pretrained Transformer: Long-range Language Modeling with Self-retrieval"}, {"paperId": "1db819afb3604c4bfd1e5a0cb2ee9ab9dec52642", "title": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch"}, {"paperId": "0983883619a0ca597d055d0e58da2f514052913d", "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"}, {"paperId": "2a68cfffde314b717ca3fc4bd3ffab597f1b6ea9", "title": "Explaining Legal Concepts with Augmented Large Language Models (GPT-4)"}, {"paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"}, {"paperId": "966852963a88a28786b798c91b6662d6e501e590", "title": "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"}, {"paperId": "94bcf0390d5acb1b92323bd15cc1dc311314122c", "title": "Language to Rewards for Robotic Skill Synthesis"}, {"paperId": "80980cd10d19f021c14a6b7eee871b6a5d328024", "title": "Augmenting Language Models with Long-Term Memory"}, {"paperId": "5dea206e2a36e672f197252bdd27d156d058f48c", "title": "FinGPT: Open-Source Financial Large Language Models"}, {"paperId": "eebb4a3162c1251b51e50ccd83797babc5b776c0", "title": "Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures"}, {"paperId": "bf7025a2e5dbb3c09deae02a1aa98a256ca559e2", "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"}, {"paperId": "6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8", "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning"}, {"paperId": "50f44ef10335d59cec145b15effae20ff22c1fdb", "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory"}, {"paperId": "5d321194696f1f75cf9da045e6022b2f20ba5b9c", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"}, {"paperId": "88b9a3e5882e5dc6dc56d3476948d1c5be67d798", "title": "Evaluating Language Models for Mathematics through Interactions"}, {"paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4", "title": "The Impact of Positional Encoding on Length Generalization in Transformers"}, {"paperId": "73b2dee720ebc9014dfe57d9b73da60ca7645c86", "title": "LLM-BRAIn: AI-driven Fast Generation of Robot Behaviour Tree based on Large Language Model"}, {"paperId": "b458fc5261595f44b36325e5eaea1f874d65138f", "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"}, {"paperId": "80be1426825288ff876acb8cc0babcc6629fa644", "title": "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2", "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"}, {"paperId": "ce913026f693101e54d3ab9152e107034d81fce1", "title": "Holistic Evaluation of Language Models"}, {"paperId": "c6ac708b65b24c20f80831d518c1795ce8133ad5", "title": "ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"}, {"paperId": "f0888b9c0ef63e68c7758e6aec2370961c0eede9", "title": "On the Tool Manipulation Capability of Open-source Large Language Models"}, {"paperId": "7cf64070fd3d7e53d80f260c10e6bd7018d580e1", "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"}, {"paperId": "9c3a9b4821daa03cb5369041d59d2714329a3811", "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"}, {"paperId": "7d8905a1fd288068f12c8347caeabefd36d0dd6c", "title": "Gorilla: Large Language Model Connected with Massive APIs"}, {"paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f", "title": "Reasoning with Language Model is Planning with World Model"}, {"paperId": "5193003d574eff310742e6ce94612fc82851fee0", "title": "Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning"}, {"paperId": "0744580e75a74357e466a57082c85cb42f548aa9", "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning"}, {"paperId": "7742233d33da13910d0303e4ec8814a4e26e96e9", "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation"}, {"paperId": "8c7846c9805834dbe2fb0c8f48253b8d65b79d6a", "title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"}, {"paperId": "5b8f0460d408a8688d9ee0cba127c779d3291d99", "title": "Aligning Large Language Models through Synthetic Feedback"}, {"paperId": "2ad8183c72a90511383a32ccaeea313eb85f4085", "title": "DetGPT: Detect What You Need via Reasoning"}, {"paperId": "a22f3398ea865426c89ee66f4824ec626e56a864", "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "a10843d1349fff8d2a7d9722f800802187fef67f", "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"}, {"paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa", "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"}, {"paperId": "6783b17fe4328f48403f57009a73f784de09f645", "title": "XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters"}, {"paperId": "c7a3f9cc61cfafdc307f8ae24430b6b1121f9b2c", "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings"}, {"paperId": "017010b941d902a467f6d329ae5e74fd67e67912", "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"}, {"paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445", "title": "LIMA: Less Is More for Alignment"}, {"paperId": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"}, {"paperId": "c3a59e1e405e7c28319e5a1c5b5241f9b340cf63", "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "256d20b96fa0ec65a373bfe64f128eb56b4ea508", "title": "Instruction Tuned Models are Quick Learners"}, {"paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"}, {"paperId": "5c7aaee5651221893ea0e67c363cab4c4be53b83", "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning"}, {"paperId": "9ada8fa11b1cdece31f253acae50b62df8d5f823", "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation"}, {"paperId": "ac47bd3b512301371fc87c68416befce6589912e", "title": "Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning"}, {"paperId": "88884b8806262a4095036041e3567d450dba39f7", "title": "Active Retrieval Augmented Generation"}, {"paperId": "d48cb91b9e555194f7494c4d4bb9815021d3ee45", "title": "VideoChat: Chat-Centric Video Understanding"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a", "title": "TidyBot: Personalized Robot Assistance with Large Language Models"}, {"paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156", "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"}, {"paperId": "6f8b9192b1f215254ee7625d752710182c05d2f9", "title": "Caption Anything: Interactive Image Description with Diverse Multimodal Controls"}, {"paperId": "8f831f341e959955a495730d81996e62c57cc0bd", "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs"}, {"paperId": "c79852e9c9cc6734c9150847deb5449e489354ea", "title": "Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner"}, {"paperId": "570079bbdd8758dfe865097e05719313c9c1301a", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"}, {"paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"}, {"paperId": "9c963e11a0a48f946f00095100dd3a303ba65949", "title": "Improved Trust in Human-Robot Collaboration With ChatGPT"}, {"paperId": "12594b6afe01461384d2856d2bf44f1cf8533e3e", "title": "ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health"}, {"paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions"}, {"paperId": "4c8ef2db0c77aba453783f5211ebafc6695d3835", "title": "ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "dbac86036cb5ed4dd6bbdda4a8613b163e20ec90", "title": "Fundamental Limitations of Alignment in Large Language Models"}, {"paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"}, {"paperId": "5be9a64df5f8d7e5a33fcc2c7bdfcde1fbbd085a", "title": "Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions"}, {"paperId": "90dd829f3d64dda19092b6e26909803bea5c37c1", "title": "From Zero to Hero: Examining the Power of Symbolic Tasks in Instruction Tuning"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "a8680b3419f3cbe6650f72b1023aed0ad0becb9e", "title": "Chain of Thought Prompt Tuning in Vision Language Models"}, {"paperId": "e92a5332390f0ba94615935541da4da9bed56512", "title": "OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization"}, {"paperId": "302ee27524a717ddc21f332ca634b9211c6ec6aa", "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge"}, {"paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a", "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"}, {"paperId": "b63e97330154acece935ffa6901e3f36518e5703", "title": "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study"}, {"paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae", "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"}, {"paperId": "ae6a4cd221684be6ca3082b6f526a7901281490b", "title": "Emergent autonomous scientific research capabilities of large language models"}, {"paperId": "38179848e2d6a3ad373b1793848816111428ac36", "title": "OpenAGI: When LLM Meets Domain Experts"}, {"paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea", "title": "Instruction Tuning with GPT-4"}, {"paperId": "bdb68c5e2369633b20e733774ac66eb4600c34d1", "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"}, {"paperId": "51a0bba0c5fb4257e843040615bb23f712fed4e6", "title": "Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models"}, {"paperId": "8f4773974bd2c27919e2a36d9aa6c2331dc34ca8", "title": "Artificial intelligence in scientific writing: a friend or a foe?"}, {"paperId": "a98862ffe4c18634a67a3df8a965a35e5e0d7ec8", "title": "ChatGPT for good? On opportunities and challenges of large language models for education"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "3dfed62c61f650eb114f0f0aa26b4e7d37b963a6", "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research"}, {"paperId": "83edcfbb206ddad38a971d605da09390604248ea", "title": "BloombergGPT: A Large Language Model for Finance"}, {"paperId": "ac7771c332da42b29a913b116bd6ef622cbf89cf", "title": "TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"}, {"paperId": "a757999ed260d7bc45484dc6b4456bf33fe6f679", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"}, {"paperId": "af5c7848417882012203ac21399977ebda695a2b", "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"}, {"paperId": "362cbfd0d05e139cd6cf049754098a6e1520b910", "title": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"}, {"paperId": "0671fd553dd670a4e820553a974bc48040ba0819", "title": "Reflexion: language agents with verbal reinforcement learning"}, {"paperId": "c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4", "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"}, {"paperId": "27d391d65ab42c30dc35595213ba6585633afa5d", "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"}, {"paperId": "0d42221038c05cee8443c5b5af838505ee137dc3", "title": "ART: Automatic multi-step reasoning and tool-use for large language models"}, {"paperId": "6e754273d54a91371efbc928cd6b156364d517da", "title": "ViperGPT: Visual Inference via Python Execution for Reasoning"}, {"paperId": "6c767695b841c52e2d953db893c4140a2b4da429", "title": "Language Models for Human-Robot Interaction"}, {"paperId": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0", "title": "Task and Motion Planning with Large Language Models for Object Rearrangement"}, {"paperId": "af997821231898a5f8d0fd78dad4eec526acabe5", "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "32524aa3ae8522542753ed7e6f4cca3970e4acab", "title": "Can an Embodied Agent Find Your \u201cCat-shaped Mug\u201d? LLM-Based Zero-Shot Object Navigation"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "fdb03aa9c310fa61df0be724705fb6f4ab20d37e", "title": "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction"}, {"paperId": "b626560f19f815808a289ef5c24a17c57320da70", "title": "MathPrompter: Mathematical Reasoning using Large Language Models"}, {"paperId": "be7b764fe1c9c32cbe349bde1fbb19321fd1d71c", "title": "Prompt, Generate, Then Cache: Cascade of Foundation Models Makes Strong Few-Shot Learners"}, {"paperId": "4b3d5da6da9c0b6b61d9672a0374da89b0da1ad3", "title": "The impending impacts of large language models on medical education"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "3599a236f285af48782fc30b1341d13ec7320735", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"}, {"paperId": "22e2f488ecd88bd2adf79092d0d390d8f7b06a0f", "title": "Auditing large language models: a three-layered approach"}, {"paperId": "2029349c55c1dba3493c5b3bd25152f18ba21ae2", "title": "Augmented Language Models: a Survey"}, {"paperId": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552", "title": "The Capacity for Moral Self-Correction in Large Language Models"}, {"paperId": "873a581320d928249609d3c07229d5af182a379c", "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"}, {"paperId": "d05ba0c40f3408aab7bb594628f24d9f04bf2831", "title": "Evaluating ChatGPT as an Adjunct for Radiologic Decision-Making"}, {"paperId": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050", "title": "Multimodal Chain-of-Thought Reasoning in Language Models"}, {"paperId": "0b1c6e2f2a04496dac42bd562f518a0f7415b31f", "title": "ChatGPT passing USMLE shines a spotlight on the flaws of medical education"}, {"paperId": "465471bb5bf1a945549d6291c2d23367966b4957", "title": "In-Context Retrieval-Augmented Language Models"}, {"paperId": "07b14c24833400b79978b0a5f084803337e30a15", "title": "REPLUG: Retrieval-Augmented Black-Box Language Models"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "86478f285356b5c8d27423e6b939634d9e010fba", "title": "Progressive Prompts: Continual Learning for Language Models"}, {"paperId": "7ec58d26c4dddb4bc3b6829fa0654a22cc26fdfe", "title": "Memory Augmented Large Language Models are Computationally Universal"}, {"paperId": "b0f615d4d300778d87122ce10621db2bbc196cad", "title": "Linguistic Data Consortium"}, {"paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d", "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"}, {"paperId": "0c0300f53c01ae609c97395c98de4c9d85d92876", "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning"}, {"paperId": "980e55d9226cac302d0fae7732da4e67b8bc952c", "title": "Parallel Context Windows for Large Language Models"}, {"paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45", "title": "Towards Reasoning in Large Language Models: A Survey"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "6f4cc536f9ed83d0dbf7e919dc609be12aa0848a", "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"}, {"paperId": "3cbffab9d7981da6662d474aaa056dcbd3c1701e", "title": "Emergent analogical reasoning in large language models"}, {"paperId": "b1b8c3e47f44158d22fb70bb453d2494ed013b70", "title": "On Second Thought, Let\u2019s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning"}, {"paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654", "title": "Constitutional AI: Harmlessness from AI Feedback"}, {"paperId": "8ee45aeb7c97e3346cc62f216f673b91277ac718", "title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"}, {"paperId": "a02fbaf22237a1aedacb1320b6007cd70c1fe6ec", "title": "Robust Speech Recognition via Large-Scale Weak Supervision"}, {"paperId": "f3a6115e5fb2237df938976e005468f0b18da797", "title": "The Stack: 3 TB of permissively licensed source code"}, {"paperId": "8a4fc5f00cd4aca61e148e46a2125c3a406719f1", "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "af1c871282ec122869d03f5420ef5d9143358a91", "title": "Visual Programming: Compositional visual reasoning without training"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "1bb6d5761903c7ac978188ae36e2648905e95dc5", "title": "Transcending Scaling Laws with 0.1% Extra Compute"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "c8d594f09413b1555970f43e68847c211235d60f", "title": "Prompting GPT-3 To Be Reliable"}, {"paperId": "c305ab1bdba79442bec72ec7f5c5ee7c49c2a566", "title": "Visual Language Maps for Robot Navigation"}, {"paperId": "dcff38de0e5fb47bdb31d472c21b0c2d88cbc4fc", "title": "AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models"}, {"paperId": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57", "title": "Language Models are Multilingual Chain-of-Thought Reasoners"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "fc74fe92fb9e34d3ae1237bf9a6e718c723f4f3e", "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation"}, {"paperId": "74eae12620bd1c1393e268bddcb6f129a5025166", "title": "Improving alignment of dialogue agents via targeted human judgements"}, {"paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867", "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"}, {"paperId": "30a7390ec0103684eba9fb6bde1983d706fb57b3", "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"}, {"paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7", "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"}, {"paperId": "398e4061dde8f5c80606869cebfa2031de7b5b74", "title": "Few-shot Learning with Retrieval Augmented Language Models"}, {"paperId": "914254fac74a2da051cccf6ca16afcaad416a079", "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"}, {"paperId": "f3cf71c51b882fe3111d71c4bf104297d38197f8", "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"}, {"paperId": "b17cc18e4130505b939f7d527082eb6be2a7fd5b", "title": "Rationale-Augmented Ensembles in Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3", "title": "Fine-tuned Language Models are Continual Learners"}, {"paperId": "354bf043179e3e9f05df73e3f04517e53c326d1f", "title": "TALM: Tool Augmented Language Models"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "39a45eba627199ee12c168dffcead45e138e9a01", "title": "Do Large Language Models Understand Us?"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "597cad6c7b9de94eecc153c7cdcaf824905fe915", "title": "You Are What You Write: Preserving Privacy in the Era of Large Language Models"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "15190e8b459bd85d546286f7d7da61b4f4f3f58a", "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "8666f9f379389a5dff31e72fb0f992a37763ba41", "title": "Teaching language models to support answers with verified quotes"}, {"paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af", "title": "Compression of Generative Pre-trained Language Models via Quantization"}, {"paperId": "081edae651e709e448bdd8a1f1b5760c7c7e1f53", "title": "Long Time No See! Open-Domain Conversation with Long-Term Persona Memory"}, {"paperId": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd", "title": "Internet-augmented language models through few-shot prompting for open-domain question answering"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca", "title": "DeepNet: Scaling Transformers to 1, 000 Layers"}, {"paperId": "4c09ac7b09628aa2aad12aea8dd6c2aef6c83aa0", "title": "Revisiting Parameter-Efficient Tuning: Are We Really There Yet?"}, {"paperId": "62d17b6f6ad77fd71ef9954c7784700d5e316f1f", "title": "What Does it Mean for a Language Model to Preserve Privacy?"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64", "title": "Red Teaming Language Models with Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"}, {"paperId": "79950179d60ba39a74d5fe2aedc47a57c0bf4c03", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"}, {"paperId": "b587204402ceb03dd85d00b0e8cc3286408b7cf2", "title": "CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark"}, {"paperId": "a3184d40d390793232c99c89b57b8f65c16320b2", "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "d617f51833860dc50d202af7f80be71304b2e994", "title": "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "f9838a3be5c94bb2674a0e224de349b50e18f3c4", "title": "Learning To Retrieve Prompts for In-Context Learning"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e", "title": "A General Language Assistant as a Laboratory for Alignment"}, {"paperId": "cbf98ebe967e0f3f3236e7932f37013b98244e94", "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"}, {"paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d", "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "2582a04918f6fe62dc142f2fca9ca0bb0b1d7895", "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "7d5c661fa9a4255ee087e861f820564ea2e2bd6b", "title": "BBQ: A hand-built bias benchmark for question answering"}, {"paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79", "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "e1227daa4877599e13de41a5207a222e1b197456", "title": "RAFT: A Real-World Few-Shot Text Classification Benchmark"}, {"paperId": "a6d8d04962f84ae6225e72723869a002b9fc8036", "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "9ba50f992ccd92f428503ea6246157260a26cd77", "title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?"}, {"paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"}, {"paperId": "7668b23aadf43bebe5e2d3abf37938b44bd16200", "title": "WebQA: Multihop and Multimodal QA"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "de549c1592a62c129b8d49c8c0137aa6859b103f", "title": "Internet-Augmented Dialogue Generation"}, {"paperId": "4237cbebe788a97174f48dc398082739bbffe95b", "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "64902a5077ee68011cd467398dbb66511e8e891a", "title": "It\u2019s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning"}, {"paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d", "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "5aab57cc0530560d82c74c055f664280619d7e81", "title": "PROST: Physical Reasoning about Objects through Space and Time"}, {"paperId": "789b8487da7188442085983caba3ffaae05531e9", "title": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"}, {"paperId": "a870f4abeef3d245641479b9d4c9f626a7178167", "title": "CCPM: A Chinese Classical Poetry Matching Dataset"}, {"paperId": "7547680408358916e66917d03436fca7540a7528", "title": "CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"}, {"paperId": "1ccd031f28dccfb226f6c0c588c93a97a50bf95f", "title": "Measuring Coding Challenge Competence With APPS"}, {"paperId": "28459083ba624020c8f1c1ed7c3a075f48b4e709", "title": "KLUE: Korean Language Understanding Evaluation"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "7fa273f450251523e6b7fcc2eb3fdbdfd4a30493", "title": "CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"}, {"paperId": "eec41f0659b11d39fd606ac6cb8721c512a2ea6f", "title": "Memorisation versus Generalisation in Pre-trained Language Models"}, {"paperId": "1e5b05838e16244310db554b04ff6541f05acb0b", "title": "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge"}, {"paperId": "238eb420c472bfdb1b4d34f9f53abec51f307a6b", "title": "FastMoE: A Fast Mixture-of-Expert Training System"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "13c4e5a6122f3fa2663f63e49537091da6532f35", "title": "Are NLP Models really able to Solve Simple Math Word Problems?"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "2f65c6ac06bfcd992d4dd75f0099a072f5c3cc8c", "title": "Understanding deep learning (still) requires rethinking generalization"}, {"paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74", "title": "What Makes Good In-Context Examples for GPT-3?"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "346081161bdc8f18e2a4c4af7f51d35452b5cb01", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "470735385073e6b378717a886dcc8f1014e69e8a", "title": "A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "6048cbb88d9a6691bfade0d46b41650533ac42bd", "title": "Real-Time Execution of Large-scale Language Models on Mobile."}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc", "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"}, {"paperId": "f13e41d24e5d0a68ca662c1b49de398a6fb68251", "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"}, {"paperId": "c014f8bc3b521453a93a13bb2c90700fcf462738", "title": "Limits to Depth Efficiencies of Self-Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d97e7561fa7710213ccd4f8128044ea6849be377", "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning"}, {"paperId": "24e4d3370dc366d6b353d1d6818a0df266bb31b9", "title": "MLSUM: The Multilingual Summarization Corpus"}, {"paperId": "414f232eda907f7fe7eb5b56f0efcde6c78b0d0b", "title": "DuReaderrobust: A Chinese Dataset Towards Evaluating the Robustness of Machine Reading Comprehension Models"}, {"paperId": "babeda48b10a4d638252118f2238d05a06f4ec55", "title": "StereoSet: Measuring stereotypical bias in pretrained language models"}, {"paperId": "2ffcf8352223c95ae8cef4daaec995525ecc926b", "title": "Adversarial Training for Large Neural Language Models"}, {"paperId": "71017cc6d270d28d9edcd47550450dc05edd65f4", "title": "Can You Put it All Together: Evaluating Conversational Agents\u2019 Ability to Blend Skills"}, {"paperId": "18318b10e7c2dd4ad292208f4399eb1d4dca5768", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"}, {"paperId": "2081ac22151c1075fcc6533f0935c29d486bfa6f", "title": "A Sentence Cloze Dataset for Chinese Machine Reading Comprehension"}, {"paperId": "4980093337f6b4a3a960ba95a54689ee491bc8ca", "title": "KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation"}, {"paperId": "725d5acdbdf0a11677f785a16e1722b92c55a47f", "title": "MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization"}, {"paperId": "83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "3fa61dbb52424e7de78cb193b2460b8d38fac351", "title": "Constitutional"}, {"paperId": "d08463bd665589d04619f04dbde84183ffcf2e63", "title": "Towards a Human-like Open-Domain Chatbot"}, {"paperId": "1a6f4495474f75ae1e8bbf407f70d9a874e5b4d6", "title": "The Pushshift Reddit Dataset"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2aef70dc36ce8c2aba1cc5e823e20a59db3f7326", "title": "Microsoft Research Asia\u2019s Systems for WMT19"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "2e347a977f14eca7cc5bbbb4c71145b75637340c", "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "703685e969fed715e13937c11d7ecc5cc7c4dfd0", "title": "Transformers without Tears: Improving the Normalization of Self-Attention"}, {"paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "title": "Structured Pruning of Large Language Models"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "fbe25e4f069a19dc63daca27b7c98cff338663b9", "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "6ffb1cc32ddde6ae01e2fc0286eafa116ade0ffb", "title": "KorQuAD1.0: Korean QA Dataset for Machine Reading Comprehension"}, {"paperId": "d8cb11d4be955f9869387a18967dee366eb851d9", "title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "04a7021fe6be6bddcfae476493fcc7571e7c613c", "title": "PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification"}, {"paperId": "9ec95c1130a6ac4238ac2e5c7b2b66047511ea92", "title": "Long and Diverse Text Generation with Planning-based Hierarchical Variational Model"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "39e801ca0dbc69c3697f118e24dac964abb63d4a", "title": "The CommitmentBank: Investigating projection in naturally occurring discourse"}, {"paperId": "ebf59587f8f170ff4241c42263bbfb9da5bd2135", "title": "ELI5: Long Form Question Answering"}, {"paperId": "7334f45c06555d4b6bf7e6b4437574c11369697e", "title": "Chinese Relation Extraction with Multi-Grained Information and External Linguistic Knowledge"}, {"paperId": "04234cd1cad396f76b96042227041abc9e525b0a", "title": "HEAD-QA: A Healthcare Dataset for Complex Reasoning"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "e278e072774f23675266881750e20bca74804cb9", "title": "ChID: A Large-scale Chinese IDiom Dataset for Cloze Test"}, {"paperId": "8a1744da011375d711ed75fc2d160c6fdca2cf89", "title": "Deep Modular Co-Attention Networks for Visual Question Answering"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "1b07a24b81834116f6ad1d0232485ba81b9445f3", "title": "Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension"}, {"paperId": "fc09d6486be1c9bbfbef4165ce3c1ab664e5d084", "title": "PAWS: Paraphrase Adversaries from Word Scrambling"}, {"paperId": "b611a8095630557229dc5fb6b07c272f1cd614da", "title": "Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"}, {"paperId": "dda6fb309f62e2557a071522354d8c2c897a2805", "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"}, {"paperId": "42ed4a9994e6121a9f325f5b901c5b3d7ce104f5", "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "9ae17b09c59f06f02ef824b856a440de663471d0", "title": "The Second Conversational Intelligence Challenge (ConvAI2)"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "eefa0df7c5678fa6004f8b48dbbc1c2696702fee", "title": "An Empirical Model of Large-Batch Training"}, {"paperId": "e9b13731027418ed38103d1dfc8a70f6881bc684", "title": "Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering"}, {"paperId": "889ad3c713bd7f1b3a8e9b07e136ec4a88651893", "title": "Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection"}, {"paperId": "be2e66b8b28bfad2cbfa3087176b79ec5ab1ec04", "title": "Character-based BiLSTM-CRF Incorporating POS and Dictionaries for Chinese Opinion Target Extraction"}, {"paperId": "a33a06ddc762fb855b6954c08d5aca603080b011", "title": "Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset"}, {"paperId": "a5b66ee341cb990f7f70a124b5fab3316d3b7e27", "title": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"paperId": "227458886343b86bd15adf58c769be326b4b058a", "title": "Wizard of Wikipedia: Knowledge-Powered Conversational agents"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5", "title": "XNLI: Evaluating Cross-lingual Sentence Representations"}, {"paperId": "84a6d47676c2d2c1414d3893d09e47d33906fb1c", "title": "WiC: 10, 000 Example Pairs for Evaluating Context-Sensitive Representations"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094", "title": "CoQA: A Conversational Question Answering Challenge"}, {"paperId": "39e734da43eb8c72e9549b42e96760545036f8e5", "title": "QuAC: Question Answering in Context"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "549c1a581b61f9ea47afc6f6871845392eaebbc4", "title": "LCQMC:A Large-scale Chinese Question Matching Corpus"}, {"paperId": "2e29be79de2bb255784b65f4ecd59824b8cc21fe", "title": "CAIL2018: A Large-Scale Legal Dataset for Judgment Prediction"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "c997d481606f0346164511cabe74c6d1ef3f6be5", "title": "DRCD: a Chinese Machine Reading Comprehension Dataset"}, {"paperId": "99ad0533f84c110da2d0713d5798e6e14080b159", "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"}, {"paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e", "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"}, {"paperId": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7", "title": "Gender Bias in Coreference Resolution"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "0be19fd9896e5d40222c690cc3ff553adc7c0e27", "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "b1d24e8e08435b7c52335485a0d635abf9bc604c", "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification"}, {"paperId": "c0fdddc750f58373ad6b1e30660812ef9903b7fe", "title": "Matching Article Pairs with Graphical Decomposition and Convolutions"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "9589244bbff8c5b5e57f52f99776cda332e6ba48", "title": "A Discourse-Level Named Entity Recognition and Relation Extraction Dataset for Chinese Literature Text"}, {"paperId": "995b7affd684b910d5a1c520c3af00fd20cc39b0", "title": "DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "678fd7c48efe21434148b4b3482c2b8b3ee618fc", "title": "Deep Neural Solver for Math Word Problems"}, {"paperId": "96bc7e517759afa2972278ef206796154a295c98", "title": "Chinese Medical Question Answer Matching Using End-to-End Character-Level Multi-Scale CNNs"}, {"paperId": "932a5de79d8a8ebb75ea0c43493450fd9922e738", "title": "Crowdsourcing Multiple Choice Science Questions"}, {"paperId": "531a7f2c659787165df4fd5b4580590b953448e4", "title": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"paperId": "ea738439b880ad033ff01602ea52d04b366d0d37", "title": "End-to-End Neural Ad-hoc Ranking with Kernel Pooling"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "40fbb2a926e46f59341b8aa7c4359a9602a9f5b5", "title": "Network Sketching: Exploiting Binary Structure in Deep CNNs"}, {"paperId": "b123a0d46ad917b79c43c5ae981e03ed2458ed11", "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "1a327709cc53ff9e52454e50a643abf4a0ac92af", "title": "Findings of the 2016 Conference on Machine Translation"}, {"paperId": "4c6fe6179c408e1fbb3871af13d1a8e64f766e54", "title": "Solving General Arithmetic Word Problems"}, {"paperId": "7a4f3a0cfc0cc2aafa4ed1a2924380e82d5e3e4c", "title": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English"}, {"paperId": "0199888fe2945829b004d7253c3876b7909b3808", "title": "Advanced User Assistance Systems"}, {"paperId": "bdf28e3cadbabda3261bd904c37edea66ab84766", "title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9", "title": "MAWPS: A Math Word Problem Repository"}, {"paperId": "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105", "title": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"}, {"paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd", "title": "Asynchronous Methods for Deep Reinforcement Learning"}, {"paperId": "62df84d6a4d26f95e4714796c2337c9848cc13b5", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems"}, {"paperId": "d64561879a2fbd3d39a5e876a667ffa4561eed80", "title": "Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "b122a828f5fee3c6afc54e70f41b00184d6383fc", "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs"}, {"paperId": "00beaaa3cc933ecafd815727eb082c28f1338a2e", "title": "QA4MRE 2011-2013: Overview of Question Answering for Machine Reading Evaluation"}, {"paperId": "a9cd386c4bb21d232e94518a0118f9c2caffba19", "title": "Know what you don't know"}, {"paperId": "296094909b3a3524b8265410b6f9c4c63ebc9de8", "title": "Besting the Quiz Master: Crowdsourcing Incremental Classification Games"}, {"paperId": "ed6262b569c0a62c51d941228c54f34e563af022", "title": "Japanese and Korean voice search"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, {"paperId": "47ced790a563344efae66588b5fb7fe6cca29ed3", "title": "The Probabilistic Relevance Framework: BM25 and Beyond"}, {"paperId": "3e4bc1aa55c752918ae99b1a125f6adef61afad2", "title": "Deep Blue"}, {"paperId": "db6ae486a695efc02910b1dc08eeba13b50d5ca8", "title": "Tokenization as the Initial Phase in NLP"}, {"paperId": "f22f6972e66bdd2e769fa64b0df0a13063c0c101", "title": "Multilayer feedforward networks are universal approximators"}, {"paperId": "c38b4734b0c2da90b3053b79619455ef2ad91145", "title": "Some characteristics of selective attention in visual perception determined by vocal reaction time"}, {"paperId": "8aa98fbfb6f1e979dead13ce24075503fe47658e", "title": "A Survey for In-context Learning"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b", "title": "Crosslingual Generalization through Multitask Finetuning"}, {"paperId": "4ebf49a7c053bf1d22fcce17bc8c80db827e8f99", "title": "Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning"}, {"paperId": "e3aa232577bb427b1f3a34acbdef84bd85734042", "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"}, {"paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"}, {"paperId": "ed9943d73eb42116fe33564b5065c78b5ca0b16e", "title": "RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs"}, {"paperId": "ac771182d1780c863954243809d1e144433919f9", "title": "Aligning Large Language Models with Human: A Survey"}, {"paperId": "3221d2373e2f501a623d72eb65a91694e9e162ac", "title": "Investigating OpenAI\u2019s ChatGPT Potentials in Generating Chatbot's Dialogue for English as a Foreign Language Learning"}, {"paperId": "343d24c4dcfaff2132373d218561a23fbd53e934", "title": "OWQ: Lessons learned from activation outliers for weight quantization in large language models"}, {"paperId": "81051b830a4f5606106765902a51ba281c9230f9", "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"}, {"paperId": null, "title": "Redpajama: An open source recipe to reproduce llama training dataset"}, {"paperId": null, "title": "Together Computer, Redpajama"}, {"paperId": null, "title": "Koala: A dialogue model for academic research"}, {"paperId": null, "title": "Languages are rewards: Hindsight finetuning using human feedback"}, {"paperId": "ec64e324ce1210fe5245dfd0fb5a92058732e5b9", "title": "Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks"}, {"paperId": "dccd764ec820c13369e91c53890dfc8cd0334355", "title": "PointCLIP V2: Adapting CLIP for Powerful 3D Open-world Learning"}, {"paperId": "7e2530784eeae241e997627795819cf42ba8562f", "title": "AdaMix: Mixture-of-Adapter for Parameter-efficient Tuning of Large Language Models"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": null, "title": "\u201cHuawei mindspore ai development framework,\u201d"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "b9478e237b58160c65acd2c41894493d27e2c277", "title": "WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"}, {"paperId": "0639baa6dfb35d962e46eb0c38763d769ffb0946", "title": "Models"}, {"paperId": "3ede1108e6cbad247b875aa46b4541967a83b980", "title": "Limits to Depth Ef\ufb01ciencies of Self-Attention Supplementary Material"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation, White Paper"}, {"paperId": null, "title": "\u201cMesh-transformer-jax: Model-parallel implementation of transformer language model with jax,\u201d"}, {"paperId": null, "title": "\u201cEthos: an online hate speech detection dataset,\u201d"}, {"paperId": "e8f297e161f57e461ede2d4e0c26573981cad077", "title": "Findings of the 2020 Conference on Machine Translation (WMT20)"}, {"paperId": "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671", "title": "Text Generation by Learning from Demonstrations"}, {"paperId": "310b8117ae5ce3df8aa6304ad382525b9b46937e", "title": "The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task: Overview and Evaluation Results (WebNLG+ 2020)"}, {"paperId": "f73a99c817a22c9aa0c4ffd38d0a1f08d541b2ef", "title": "The NeurIPS '18 Competition: From Machine Learning to Intelligent Conversations"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "4ae2960d3c6ac489b3b072666fb0b91d0480a170", "title": "A Span-Extraction Dataset for Chinese Machine Reading Comprehension"}, {"paperId": null, "title": "\u201cSocialiqa: Commonsense reasoning about social interactions,\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "\u201cDiscriminating systems,\u201d"}, {"paperId": null, "title": "Iflytek: a multiple categories chinese text classifier"}, {"paperId": "8ce1512e77fa6a646513a60d78e0081afe870c07", "title": "Dataset for the First Evaluation on Chinese Machine Reading Comprehension"}, {"paperId": "7afb83134d5b7914131e10b229d30dc2593266f6", "title": "The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification"}, {"paperId": null, "title": "\u201cJax: composable transformations of python+ numpy programs,"}, {"paperId": "616253f6b1e83ede361457de2f51b0bf70555b13", "title": "Cross-lingual Name Tagging and Linking for 282 Languages"}, {"paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607", "title": "A Survey on Context Learning"}, {"paperId": null, "title": "\u201cA corpus and evaluation framework for deeper understanding of commonsense stories,\u201d"}, {"paperId": null, "title": "\u201cDescription2Code Dataset,\u201d"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": "2d757edc96f1933e5a49ae6a251271fb9b7571bf", "title": "Wikipedia"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": null, "title": "\u201cCodeforces: Results of 2020,\u201d"}, {"paperId": null, "title": "\u201cPrompt Engineering Guide,\u201d"}, {"paperId": null, "title": "\u201cFairscale: A general purpose modular pytorch library for high performance and large scale training,\u201d"}, {"paperId": "4954fa180728932959997a4768411ff9136aac81", "title": "This Paper Is Included in the Proceedings of the 12th Usenix Symposium on Operating Systems Design and Implementation (osdi '16). Tensorflow: a System for Large-scale Machine Learning Tensorflow: a System for Large-scale Machine Learning"}, {"paperId": null, "title": "\u201cCommon crawl.\u201d"}, {"paperId": null, "title": "Multi-turn dialogues, Information-seeking dialogues, Chinchilla-generated [107] conversational questions, GopherCite [240] human evaluation interface"}, {"paperId": null, "title": "Sparrow Human data for rule violations and per-turn response preferences, Self-play data accumulated through training"}, {"paperId": null, "title": "QA: TriviaQA [153], Natural Questions"}, {"paperId": null, "title": "\u201cBigquery dataset.\u201d"}, {"paperId": null, "title": "\u201cOpenwebtext corpus.\u201d"}, {"paperId": null, "title": "Self-collected dialogs with turns by asking crowdworkers to interact with LaMDA"}, {"paperId": "24de1048791bac4972ecc16d1c3c1de23691407d", "title": "Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects"}, {"paperId": null, "title": "Video-chatgpt: Towards de-40"}, {"paperId": null, "title": ". int8 (): 8-bit matrix multiplication for transformers at scale, arXiv preprint arXiv:2208.07339 (2022)"}, {"paperId": null, "title": "guage model compression"}, {"paperId": null, "title": "Gira ff"}, {"paperId": null, "title": "Transparency guidance for chatgpt usage in scientific writing"}, {"paperId": null, "title": "Findings of the 2020 conference on machine translation ( wmt 20 ) , \u201d in Proceedings of the Fifth Conference on Machine Translation"}, {"paperId": null, "title": "From dense to sparse: Contrastive pruning for better pre-trained lan-35"}, {"paperId": null, "title": "\u201cLarge language models still can\u2019t plan (a benchmark for llms on planning and reasoning about change),\u201d"}, {"paperId": null, "title": "The calla dataset: Probing llms\u2019 interactive knowledge acquisition from chi-nese medical literature"}, {"paperId": null, "title": "Exploring the impacts of chatgpt on future scientific work"}, {"paperId": null, "title": "Fine-tuning a llm using reinforcement learning from human feedback for a therapy chatbot application (2023). 31"}, {"paperId": null, "title": "\u201cOntonotes release 4.0,\u201d"}, {"paperId": null, "title": "\u201cFirst quora dataset release: Question pairs,\u201d"}, {"paperId": null, "title": "The pile : An 800 gb dataset of diverse text for language modeling Korquad 1 . 0 : Korean qa dataset for machine reading comprehension"}, {"paperId": null, "title": "\u201cMot: Pre-thinking and recalling enable chatgpt to self-improve with memory-of-thoughts,\u201d"}]}