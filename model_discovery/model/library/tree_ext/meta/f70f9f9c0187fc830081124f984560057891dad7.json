{"paperId": "f70f9f9c0187fc830081124f984560057891dad7", "title": "Nonlinear Initialization Methods for Low-Rank Neural Networks", "abstract": "We propose a novel low-rank initialization framework for training low-rank deep neural networks -- networks where the weight parameters are re-parameterized by products of two low-rank matrices. The most successful prior existing approach, spectral initialization, draws a sample from the initialization distribution for the full-rank setting and then optimally approximates the full-rank initialization parameters in the Frobenius norm with a pair of low-rank initialization matrices via singular value decomposition. Our method is inspired by the insight that approximating the function corresponding to each layer is more important than approximating the parameter values. We provably demonstrate that there is a significant gap between these two approaches for ReLU networks, particularly as the desired rank of the approximating weights decreases, or as the dimension of the inputs to the layer increases (the latter point holds when the network width is super-linear in dimension). Along the way, we provide the first provably efficient algorithm for solving the ReLU low-rank approximation problem for fixed parameter rank $r$ -- previously, it was unknown that the problem was computationally tractable to solve even for rank $1$. We also provide a practical algorithm to solve this problem which is no more expensive than the existing spectral initialization approach, and validate our theory by training ResNet and EfficientNet models (He et al., 2016; Tan&Le, 2019) on ImageNet (Russakovsky et al., 2015).", "venue": "", "year": 2022, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Along the way, this work provides the first provably efficient algorithm for solving the ReLU low-rank approximation problem for fixed parameter rank $r$ -- previously, it was unknown that the problem was computationally tractable to solve even for rank $1."}, "embedding": {"model": "specter_v2", "vector": [-0.4193497896194458, 0.8704302906990051, -0.4304664433002472, 0.265435129404068, -0.14892713725566864, 0.03189631551504135, 0.6446888446807861, -0.6884521842002869, -0.6460373401641846, -0.3808463513851166, 0.8313232660293579, 0.05235090106725693, 0.2638286054134369, 0.11146990209817886, -0.4165758490562439, -0.2919950783252716, -1.0506751537322998, 0.14854401350021362, -0.21379029750823975, -0.4223814606666565, -0.38533222675323486, -0.3647921085357666, -0.8913261294364929, 0.3183444142341614, -0.17741675674915314, 0.6900684833526611, -0.5603079795837402, 0.5052196383476257, -0.05069766938686371, 0.3506563901901245, 0.8527265787124634, -0.3722928464412689, 1.3424293994903564, 0.04872456192970276, 0.05778367817401886, -0.2769279479980469, 0.07864320278167725, -1.0676640272140503, -0.755605936050415, 0.8027305006980896, -0.2993151843547821, 0.5184420347213745, 0.5346472859382629, -0.6239383816719055, 0.19618944823741913, 0.5782050490379333, 0.27207857370376587, 0.5698941946029663, -0.42947113513946533, -0.6344593167304993, 1.1591331958770752, -1.553409457206726, 0.23640571534633636, 1.358712077140808, 0.9180605411529541, 0.3046031594276428, -0.41573601961135864, -0.7960785627365112, 0.22714856266975403, -0.17907708883285522, -0.9888619184494019, -0.1822626292705536, -0.0979461669921875, -0.531152069568634, 0.5176425576210022, -0.7386071681976318, -0.6255003213882446, 0.797774612903595, -0.47468939423561096, 0.6539676189422607, 0.31922316551208496, -0.2525259554386139, -0.10016746073961258, 0.29466772079467773, 0.26474729180336, 0.7255224585533142, -0.025001248344779015, 0.5478422045707703, -1.0724583864212036, -0.351778507232666, 0.6944737434387207, 0.16359135508537292, -0.2467178851366043, -0.4339214265346527, -0.06260814517736435, 0.764794111251831, 0.49234870076179504, 0.1349043995141983, -0.22897735238075256, 0.9298298954963684, 0.7268234491348267, 0.45053961873054504, 0.27710044384002686, 0.2624727785587311, 0.22979091107845306, 0.5967565178871155, -0.6157123446464539, -0.2846493124961853, 0.17502067983150482, 0.4973774552345276, 0.07180402427911758, 0.40424150228500366, -0.1107037141919136, 0.4187470078468323, 1.2342019081115723, -0.7033777832984924, 0.88310706615448, -0.3660644590854645, -0.052530687302351, -0.8954808712005615, 0.40274012088775635, -0.7729758024215698, -0.4431861937046051, -0.8587860465049744, -1.6979870796203613, -0.7058406472206116, -0.20704390108585358, 0.1715138703584671, -0.686524510383606, 0.6143704652786255, -0.49497172236442566, 0.4258511960506439, 0.2986719608306885, 1.2351595163345337, 0.36625033617019653, 1.038374900817871, -0.31115901470184326, 0.18812023103237152, 0.932130753993988, -0.659192681312561, -0.6822725534439087, -0.4253857135772705, 0.30372801423072815, 0.13048313558101654, 0.18334105610847473, 0.21165354549884796, -0.915901243686676, -0.819457471370697, -1.1335830688476562, 0.26812413334846497, -0.5020767450332642, 1.0838505029678345, 1.0767276287078857, 0.39444759488105774, -0.6652734279632568, 0.814156174659729, -0.1197248250246048, -0.10585128515958786, 0.07662654668092728, 0.5962567925453186, 0.13488557934761047, 0.13285678625106812, -1.0729414224624634, 0.4667074382305145, 0.8095310926437378, -0.050710249692201614, -0.330754816532135, -0.42393848299980164, -0.48411789536476135, -0.2298162430524826, 0.005253354087471962, -0.5861856341362, 0.937504231929779, 0.1849476397037506, -1.380934238433838, 0.4296828806400299, 0.4969310164451599, -0.32923153042793274, 0.5114167332649231, -0.27113181352615356, -0.4044989347457886, -0.27886101603507996, -0.27316731214523315, 0.21413834393024445, 0.44348618388175964, -1.191385269165039, 0.39062219858169556, 0.4596553146839142, -0.584296703338623, -0.3072153329849243, -0.2922062575817108, 0.3842989206314087, -0.7075352668762207, -0.5146888494491577, 1.0203955173492432, 0.7821506261825562, -0.4273064136505127, -0.5067015290260315, -0.3886498808860779, -0.2258678674697876, 0.009233491495251656, 0.11662499606609344, 0.7339994311332703, -0.9609590768814087, -0.9917981028556824, 0.7550049424171448, -0.0006838480476289988, -0.39453479647636414, -0.43553441762924194, 0.03632702678442001, -1.049028754234314, 0.5959973335266113, 0.4713950753211975, -0.7928271293640137, -0.33007147908210754, -0.029954232275485992, -1.1611939668655396, 0.5170525312423706, -0.28419187664985657, 0.9504516124725342, -0.8872512578964233, 0.8994803428649902, 0.01901409402489662, 0.12917745113372803, -0.9402819275856018, 1.273895263671875, 0.32076317071914673, 0.008452913723886013, 0.30430057644844055, -0.5422313213348389, 0.09568242728710175, -0.6155344247817993, 0.44728243350982666, -1.0839446783065796, 0.4134874641895294, 0.3303219676017761, -1.552942156791687, 0.9873388409614563, -0.2852729260921478, 0.4011155664920807, 0.4132571816444397, -0.9732561111450195, -0.020019054412841797, 0.19805580377578735, 0.4183421730995178, -0.006469945888966322, 0.8061439394950867, 0.44148021936416626, -0.9329418540000916, 0.7009763121604919, 0.2365163266658783, 0.7800268530845642, -0.23566778004169464, 0.2768801152706146, 0.8240357041358948, 0.295890748500824, 0.09105493873357773, 0.24246211349964142, 0.12485554069280624, 0.23683929443359375, 0.4913038909435272, 0.04755833372473717, 0.04602077230811119, -1.224561333656311, -0.19258929789066315, 0.7747742533683777, 1.3136568069458008, 1.130898356437683, 0.5110901594161987, -0.7976226806640625, -0.6889594197273254, -0.5274586081504822, 0.42815741896629333, 1.125627875328064, -0.06245382875204086, -0.2534586191177368, -0.39873364567756653, 0.19268062710762024, -0.2915993332862854, -0.4959932267665863, -0.21952708065509796, -0.032655440270900726, -0.7381829023361206, -1.6172059774398804, 0.4988533556461334, -0.09190608561038971, 0.8274387717247009, 0.7990792989730835, 0.4635140001773834, -0.37426432967185974, 0.4259384572505951, -0.8671147227287292, -0.42228859663009644, 0.6142498254776001, -0.6522960066795349, 0.0008072922355495393, -0.33348381519317627, -0.030161114409565926, 0.8222718834877014, -0.8831683993339539, 0.40682655572891235, -0.4420662820339203, 0.02524401806294918, 0.053445350378751755, 0.3820902407169342, -0.024623245000839233, -0.2394130975008011, -0.18407250940799713, 0.9277851581573486, 0.3915226459503174, -0.47885867953300476, 0.06423813104629517, 0.09363697469234467, 0.14601363241672516, -0.6757131218910217, 0.12886719405651093, 0.20757336914539337, 0.603857159614563, 0.43229901790618896, -0.025966232642531395, 0.24974551796913147, -1.333065390586853, 1.3211143016815186, -0.1195533275604248, -0.7769672870635986, -0.21243909001350403, -0.8257817625999451, 0.37070170044898987, 0.6900324821472168, -0.9502465724945068, 0.31901589035987854, -0.8005040884017944, -0.06864999234676361, -0.9247035980224609, 0.3015490770339966, 0.41112175583839417, 0.718742311000824, -0.2765355706214905, 0.9012270569801331, -0.37203750014305115, 0.49696096777915955, -0.49720123410224915, 0.8951839208602905, -0.7428803443908691, 1.3164165019989014, -0.18190528452396393, 1.079329013824463, 0.2003190517425537, 0.037886228412389755, -0.4847913682460785, -0.9398839473724365, -0.6873148083686829, -0.042385805398225784, -0.4178982377052307, -0.35004445910453796, -1.1531590223312378, -0.7506021857261658, -0.08456842601299286, -0.5481942296028137, -0.006548839155584574, -0.07908134162425995, 0.044559236615896225, -0.29732751846313477, -0.7122725248336792, -1.5890198945999146, -0.6945480108261108, -0.6070168018341064, -1.0662256479263306, -0.32109320163726807, -0.018212897703051567, 0.16615518927574158, -0.08982497453689575, -0.8510953783988953, -0.582101047039032, 0.6696428656578064, -0.4559932351112366, 0.4306091070175171, -0.32581979036331177, -0.3899441957473755, -0.004857200663536787, -0.15639400482177734, 0.762607753276825, -0.7827004790306091, -0.2648402452468872, -0.5314432978630066, 0.14196766912937164, -0.3509264886379242, -0.464102178812027, 0.18096114695072174, 0.41895386576652527, 0.4018175005912781, -0.40544554591178894, -0.1536291539669037, 1.14487624168396, 1.5405783653259277, -1.0524293184280396, 0.07236448675394058, 0.3430066406726837, 0.8840613961219788, 0.2018379420042038, -0.47728073596954346, 1.0065734386444092, -0.37738603353500366, 0.11670781672000885, 0.3063337206840515, -0.566229522228241, -0.16368220746517181, -0.49477964639663696, 0.16776686906814575, 1.2940670251846313, 0.553968071937561, 0.6176411509513855, -0.6127785444259644, 0.3958907723426819, -1.48423171043396, -0.5871066451072693, 0.6352964639663696, 1.1869992017745972, 0.3954544961452484, -0.06986679136753082, -0.18526339530944824, -0.7253161668777466, 0.4827374815940857, 0.157826766371727, -0.03301909938454628, -0.08958645910024643, -0.0768527239561081, 0.18664291501045227, 1.1422878503799438, 0.5560356378555298, -0.2571558356285095, 0.4505941569805145, 14.766769409179688, 0.382743239402771, 0.16624730825424194, 1.005513310432434, 0.7881916165351868, 0.03148062899708748, -0.19287267327308655, -0.11330368369817734, -1.0562220811843872, -0.10291970521211624, 0.9373770952224731, 0.4919619560241699, 1.01857590675354, 0.4558314085006714, -0.0357854850590229, 0.24824608862400055, -0.22390960156917572, 1.608953595161438, 0.49878785014152527, -1.581091046333313, 0.022499721497297287, 0.15614907443523407, 0.7735092639923096, 0.9050066471099854, 0.5733429789543152, 0.41108447313308716, 0.5650537610054016, -0.17561784386634827, 0.1790446788072586, 0.4674818515777588, 1.133926272392273, -0.5003699660301208, 0.8020879030227661, 0.29757240414619446, -0.5511552691459656, -0.4195810556411743, -0.7242381572723389, -0.5933371186256409, 0.12029827386140823, 0.7127596735954285, -0.4409337341785431, -0.3755396008491516, 0.2675396502017975, 0.230716735124588, 0.2475956380367279, 0.5259299874305725, -0.022043099626898766, 0.3370259404182434, -0.5779454708099365, 0.6046614646911621, -0.10734809190034866, -0.2634284794330597, -0.054024599492549896, -0.33823397755622864, 0.08759060502052307, 0.16910165548324585, 0.4504680037498474, 0.6793960928916931, -0.7065591216087341, -0.16781838238239288, -0.13922318816184998, 0.03062315471470356, -0.38906246423721313, 1.2211828231811523, 0.5522350668907166, 0.11877835541963577, 0.10114944726228714, 0.212063729763031, 0.43821612000465393, 0.5033266544342041, -0.034479159861803055, -0.4101676344871521, 0.6369494199752808, 0.04889272525906563, -1.029572606086731, -0.2314489334821701, -0.6233819127082825, -0.9832626581192017, -1.0972964763641357, -0.49027878046035767, 0.08007188141345978, -1.1973705291748047, -1.1707087755203247, 0.8385595679283142, -0.1806952804327011, -0.7349576950073242, 0.14800092577934265, -1.0218000411987305, 0.31528955698013306, 0.5319492816925049, -1.025248646736145, 0.04375676065683365, 0.3011378347873688, -0.3950924873352051, -0.6877182722091675, -0.6312856674194336, 0.1581878811120987, -0.02612772397696972, -0.5325105786323547, 0.15804541110992432, 0.7561506628990173, 0.32432079315185547, 0.030392399057745934, -0.27140307426452637, 0.46167516708374023, 0.6678957939147949, -0.3209473490715027, 1.0291146039962769, 0.236964613199234, 0.28595417737960815, -0.6853553056716919, 0.3825206756591797, -0.3426417112350464, -0.32073894143104553, 0.26722651720046997, -0.9170565605163574, -0.5349379181861877, 0.1532251387834549, 0.3671902120113373, -0.0007014000439085066, 0.11326133459806442, 0.2433250993490219, -0.597010612487793, -0.41153615713119507, -0.481364369392395, 0.32286199927330017, 0.15258900821208954, -0.9064379334449768, -0.27459001541137695, 0.2592095136642456, -0.18803302943706512, -1.1579890251159668, -0.7766482830047607, 0.1905466467142105, -0.20550593733787537, -0.15304452180862427, 1.59574294090271, -0.2516709864139557, 0.10085799545049667, 0.24674083292484283, 0.23900213837623596, -0.38126349449157715, -0.022440863773226738, -1.0537663698196411, 0.03395086154341698, 0.02903447113931179, -0.005246561486274004, -0.5246033072471619, 0.6223073601722717, 0.48363450169563293, 0.0676373764872551, -0.37520715594291687, -0.6852893829345703, -0.38984358310699463, -0.7489765286445618, -0.3549586534500122, -0.5660337209701538, 0.0364505909383297, -0.26278311014175415, 0.04408026486635208, 0.08847425878047943, 1.1427515745162964, 0.003083507064729929, -1.131978988647461, 0.48915842175483704, -0.070061594247818, 0.1409810185432434, -0.3761540651321411, -0.4104814827442169, -1.6872689723968506, 0.30039674043655396, -1.2589693069458008, 0.05226563662290573, -0.9413759708404541, -0.6574112176895142, 0.37916985154151917, -0.22875243425369263, 0.284879595041275, 1.0480012893676758, 0.19012495875358582, -0.603283703327179, 0.09418664872646332, -0.11183980107307434, 0.5928008556365967, 0.00738612562417984, -0.26171883940696716, -0.40527838468551636, -0.025940289720892906, 0.27314338088035583, 0.520037055015564, 0.2909905016422272, -0.22710011899471283, -0.8737714290618896, -0.44767698645591736, 0.8397316336631775, -0.20646899938583374, -0.09723668545484543, -1.2275665998458862, 0.9599573016166687, 0.5628899931907654, 0.2864033877849579, -0.30136433243751526, 0.2995666265487671, -1.166359305381775, -0.10906526446342468, 0.07259073853492737, -0.4418589770793915, 0.15741658210754395, -0.05290345847606659, -0.5112452507019043, 0.023573588579893112, 0.19086842238903046, 0.2884267270565033, -0.825604259967804, -0.18823806941509247, 0.27383506298065186, -0.8745517730712891, 0.08079732954502106, -0.6300731897354126, -0.3367898166179657, -0.9612604975700378, -0.4221110939979553, -0.1383158564567566, 0.2343406230211258, -0.25137680768966675, 0.7168089747428894, 0.26584184169769287, -1.4445446729660034, 0.37803444266319275, 0.42023858428001404, -0.10254909098148346, -0.361149400472641, 0.19396694004535675, 0.49095988273620605, -0.5528581738471985, 0.09352076798677444, 0.1895759552717209, 0.5304567813873291, -0.08567342162132263, -0.036612618714571, 1.0137392282485962, -0.10170987248420715, -0.30409473180770874, 1.037056565284729, -0.10789678990840912, -1.0355182886123657, 0.35077759623527527, -0.8227428793907166, -0.2578456997871399, 0.11041340231895447, 0.2034725546836853, 0.09139584749937057, 0.154859721660614, -0.04061511904001236, -0.02987266518175602, 0.10348951071500778, 0.017037415876984596, -0.6093802452087402, 0.49839282035827637, -0.22041259706020355, -0.29378876090049744, 0.49558407068252563, 1.1061129570007324, -0.4607779383659363, -0.7403596639633179, -0.820539653301239, -0.20870639383792877, -0.3253485858440399, 0.40015313029289246, -0.0776611939072609, -0.7708989977836609, 0.1970839947462082, 0.8652170300483704, -0.2727951407432556, 0.4061833620071411, -0.28827500343322754, -0.30985352396965027, 0.8568136692047119, 0.10300774872303009, -1.1149225234985352, -0.19463211297988892, 1.409813404083252, 1.2279952764511108, -0.43489745259284973, 0.9414356350898743, -0.3348597288131714, -0.45788657665252686, 1.0929964780807495, 0.23309101164340973, -0.33031293749809265, 0.7954965829849243, -0.12351890653371811, -0.10540866106748581, 0.345199853181839, -0.49557948112487793, -0.6845062971115112, 1.081229329109192, 0.6900529265403748, 0.28627461194992065, -0.4650692343711853, 0.03986513614654541, 1.2916921377182007, 0.00011660010204650462, -0.43217527866363525, 0.8923490047454834, -0.19645318388938904, -0.08153179287910461, -0.14775916934013367, -0.38560211658477783, 0.9538620710372925, -0.6825127005577087, -0.5447608828544617, 0.3305386006832123, 0.5885650515556335, -0.1472199559211731, 0.3361561596393585, 0.5316603779792786, -0.6527470350265503, 0.6024595499038696, -0.07631929963827133, -0.1582014262676239, -0.22393901646137238, -0.6484575271606445, 0.6561733484268188, -0.4598548114299774, -0.4620940387248993, 0.35814136266708374, -0.4357435405254364, -0.6362670660018921, -0.06454335153102875, 0.31706470251083374, -0.17567439377307892, 0.4992525279521942, 0.57596755027771, 0.3920789361000061, 0.3226146101951599, 0.08423438668251038, -1.0330458879470825, -1.0609593391418457, -0.3504144251346588, -0.09098541736602783, -0.2774297595024109, -0.07287091016769409, -0.2024882733821869, -0.5590176582336426, -0.2215505987405777]}, "authors": [{"authorId": "4529644", "name": "Kiran Vodrahalli"}, {"authorId": "2934334", "name": "Rakesh Shivanna"}, {"authorId": "3221924", "name": "M. Sathiamoorthy"}, {"authorId": "2116998330", "name": "Sagar Jain"}, {"authorId": "2226805", "name": "Ed H. Chi"}], "references": [{"paperId": "d151ced8700d84a2efe411a234a4cb2c595e8ca9", "title": "Language model compression with weighted low-rank factorization"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "67618071e2e63921dde7471bc3c835f0cebe5a41", "title": "Plant 'n' Seek: Can You Find the Winning Ticket?"}, {"paperId": "7567744a0e23174166575e8d98590967684696b4", "title": "Scaling Law for Recommendation Models: Towards General-purpose User Representations"}, {"paperId": "f6942ff9837bcb21c305f8d7fd5c444625364ec9", "title": "Learning Pruned Structure and Weights Simultaneously from Scratch: an Attention based Approach"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "309037d7fa4bf15768b23ba77c959ceb8cd18c87", "title": "Rapid training of deep neural networks without skip connections or normalization layers using Deep Kernel Shaping"}, {"paperId": "270893518e80a7f97dc7deaca3829dc4194681e5", "title": "No Fine-Tuning, No Cry: Robust SVD for Compressing Deep Networks"}, {"paperId": "d119383b24653e528e12ad6c4df450253c92956e", "title": "Initialization Matters: Regularizing Manifold-informed Initialization for Neural Recommendation Systems"}, {"paperId": "385d055ca18509d36f8601fca9d832a11d490e30", "title": "A Universal Law of Robustness via Isoperimetry"}, {"paperId": "f93f2476972228de142fde13913bccbec76859b8", "title": "Initialization and Regularization of Factorized Neural Layers"}, {"paperId": "90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb", "title": "Accelerating Sparse Deep Neural Networks"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "8c518d1859d892640247fc6b4beb75545864ebc9", "title": "Pufferfish: Communication-efficient Models At No Extra Cost"}, {"paperId": "0d3a429f7755ba9d89c8a430b32ccf768eb81801", "title": "Simple Heuristics Yield Provable Algorithms for Masked Low-Rank Approximation"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "fa5853fdef7d2f6bb68203d187ddacbbddc63a8b", "title": "High-Dimensional Probability: An Introduction with Applications in Data Science"}, {"paperId": "d8bc0d94f8db59078923f099083805c4a55c4a50", "title": "Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot"}, {"paperId": "0932abfd0fb90e8a28f7bd195633c9891bfd7ecb", "title": "Pruning Neural Networks at Initialization: Why are We Missing the Mark?"}, {"paperId": "e00484961fb2f30d2d48a5f9853fa3ebab140cac", "title": "Improving Transformer Optimization Through Better Initialization"}, {"paperId": "4de08637d620ae781783a91b46f82ee8d9be405f", "title": "Low-Rank Compression of Neural Nets: Learning the Rank of Each Layer"}, {"paperId": "175e3e14b7c8872f7c99638e90ca978d69b41297", "title": "Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors"}, {"paperId": "a68c3412e60560290400d2707596f82a914b7c00", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps"}, {"paperId": "2c5a8950cf0a13e229ad19093ba064495fda8de7", "title": "A Neural Scaling Law from the Dimension of the Data Manifold"}, {"paperId": "e52051204cb1179584f3b008c9d38848b52c1f28", "title": "ReZero is All You Need: Fast Convergence at Large Depth"}, {"paperId": "de66ada65cd9d36e46f1f8dd2c8be480180038ec", "title": "What is the State of Neural Network Pruning?"}, {"paperId": "c114ce10c4a315d92c3815f54bc9893e7e6ef182", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "6cce1d4f64f0fdc4a69e4a108e596c555879083b", "title": "Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "1fc16dfee1c5957f52d913807068d6703be417d4", "title": "Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals"}, {"paperId": "8d42c1e2bf782e81b991e7251fed8134b330b04a", "title": "DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures"}, {"paperId": "56f66951145444036cb6ec748d90a04ffc487cc1", "title": "Data-Independent Neural Pruning via Coresets"}, {"paperId": "b1ac64438608aac1a8dfd0adf8fec8c6220f6bfd", "title": "Butterfly Transform: An Efficient FFT Based Neural Architecture Design"}, {"paperId": "8e2c65ff58b28a076883c99b96840e19b5e0b916", "title": "Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization"}, {"paperId": "8cb3000e8959d1065532d54a07cf8fe97ef6b9c6", "title": "Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons"}, {"paperId": "ad6309d1ea001098189425f54d069ef12abcb583", "title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10, 000-Layer Vanilla Convolutional Neural Networks"}, {"paperId": "597b1dedb58ddb5ee5c35938c1a2574c7a11e0f6", "title": "Initialization matters: Orthogonal Predictive State Recurrent Neural Networks"}, {"paperId": "f91248a4f587f89f1d1d8e557cee08b8114686d9", "title": "Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima"}, {"paperId": "9edf1a25ebc182355c5584fb7f5d234e75ccf3d1", "title": "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice"}, {"paperId": "773d5ddc414424a8948446ddaa5275b944f50891", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon"}, {"paperId": "c568ecbd28e1fe24f58d6566cc37d45e32fb25b9", "title": "Weighted low rank approximations with provable guarantees"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "97dc8df45972e4ed7423fc992a5092ba25b33411", "title": "All you need is a good init"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "b89d7f7439cab841934a1ede06bf6b1f593c754f", "title": "Accelerating Very Deep Convolutional Networks for Classification and Detection"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "8604f376633af8b347e31d84c6150a93b11e34c2", "title": "FitNets: Hints for Thin Deep Nets"}, {"paperId": "ecbea3b74deb06657a2d0100a717501f7d1a252a", "title": "Sketching as a Tool for Numerical Linear Algebra"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "9d3582ae92b9e42db13dbb56d30f782e60068aa9", "title": "Probability in High Dimension"}, {"paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649", "title": "Understanding the difficulty of training deep feedforward neural networks"}, {"paperId": "77e379fd57ea44638fc628623e383eccada82689", "title": "Kernel Methods for Deep Learning"}, {"paperId": "0d82c68980943718a306df67c3ed95f782e9f93a", "title": "Pruning algorithms-a survey"}, {"paperId": "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "title": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon"}, {"paperId": "b745b5512ad3b1f652dc0cbb5ddf5a940f397f7d", "title": "MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training"}, {"paperId": "d918d3193981bd2c2b21e68ef076b0d98c829dc7", "title": "Sketch based Memory for Neural Networks"}, {"paperId": "eec7ad0270eda7298c139af6e2676599f1fd53f6", "title": "Data and Parameter Scaling Laws for Neural Machine Translation"}, {"paperId": null, "title": "Convexity and Monotonicity of C-Maps). It is worth noting that the analysis"}, {"paperId": null, "title": "Nvidia ampere ga102 gpu architecture"}, {"paperId": null, "title": "Rethinking model scaling for convolutional neural networks"}, {"paperId": "11d36cf5fc2fc35c52bd5a735cdd29c685bae95e", "title": "MetaInit: Initializing learning by learning to initialize"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": null, "title": "Large-scale machine learning on heterogeneous systems"}, {"paperId": null, "title": "Analysis of Boolean Functions, chapter 11 sec. 2, 8"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": "a87953825b0bea2a5d52bfccf09d2518295c5053", "title": "Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment"}, {"paperId": "a26eb90a8766449b942547b02b4f82ecfab0875c", "title": "Stat260/cs294: Randomized Algorithms for Matrices and Data"}, {"paperId": null, "title": "Weight decay: This method is the standard Frobenius norm regularization on the weights of the layers In the low-rank setting, instead of penalizing (cid:107) W (cid:107) 2 F , we penalize"}, {"paperId": null, "title": "Empirical observations that taking nonlinearities of layers into account for the initialization scheme improves accuracy for lower-rank layers and larger input dimension and width"}, {"paperId": null, "title": "Theoretical proof"}]}