{"paperId": "4ea5ca620122e6a9a2b000444d36491cebf49c7c", "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey", "abstract": "Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at https://github.com/Strivin0311/long-llms-learning.", "venue": "arXiv.org", "year": 2023, "citationCount": 23, "influentialCitationCount": 4, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference is offered."}, "embedding": {"model": "specter_v2", "vector": [0.41506102681159973, 0.5609207153320312, -0.25019973516464233, -0.025603922083973885, -0.15870194137096405, -0.40029624104499817, 0.8970947265625, 0.06123090162873268, -0.5381554961204529, -0.3136403262615204, 0.418526828289032, -0.25686225295066833, 0.35876336693763733, 0.18281853199005127, -0.2420097440481186, 0.2790471315383911, -0.9029436707496643, 0.49845486879348755, -0.24096836149692535, -0.477896124124527, -0.42652809619903564, -0.6975900530815125, -0.6819682717323303, 0.24322542548179626, 0.4228765666484833, 0.32154950499534607, 0.29644376039505005, 0.9005540013313293, -0.38337790966033936, 0.7166735529899597, 0.5706749558448792, -0.49749407172203064, 0.1540965586900711, 0.3231908082962036, -0.2571282386779785, -0.2538580596446991, 0.3498542606830597, -0.5430707931518555, -0.24714834988117218, 0.6429848074913025, -0.28261488676071167, 0.28585800528526306, 0.15067845582962036, -0.5587596893310547, -0.3566121459007263, 1.2972949743270874, 0.9445024728775024, 0.5700141787528992, -0.009676164016127586, -0.3040100932121277, 1.5301141738891602, -1.369989037513733, 0.2649100422859192, 1.4200894832611084, 0.7566689848899841, 0.41308045387268066, -0.271757036447525, -0.6920315623283386, 1.0537042617797852, -0.2247454822063446, -0.700672447681427, -0.606621503829956, 0.16392531991004944, -0.0782431960105896, 2.1712028980255127, -0.20813405513763428, -0.0395837277173996, 0.838964581489563, -0.15514019131660461, 1.6251537799835205, -0.2313234508037567, -1.0727074146270752, -0.2766750454902649, 0.030741732567548752, 0.27448856830596924, 0.9129464030265808, -0.6045688986778259, 0.4650849997997284, -0.7854856848716736, -0.08034469932317734, 0.5410355925559998, -0.15493592619895935, -0.06977344304323196, 0.21441222727298737, -0.4265853762626648, 0.5870710611343384, 0.23277153074741364, 0.9986094236373901, -0.37289488315582275, 0.36629000306129456, 0.35696858167648315, 0.6249129772186279, -0.14131008088588715, 0.4208810329437256, -0.4079382121562958, 0.2950857877731323, -0.8285512328147888, 0.5237112641334534, 0.08953405171632767, 1.0962932109832764, -0.4773922562599182, 0.140051931142807, -0.8035932183265686, 0.35411587357521057, 1.1433255672454834, 0.08421129733324051, 0.6149975657463074, -0.42204394936561584, 0.4654952883720398, -0.4156670868396759, 0.3165524899959564, -0.46378418803215027, -0.36062559485435486, -0.4188852906227112, -0.3860168755054474, -1.4155904054641724, -0.018066413700580597, -0.12346430122852325, -0.6064348220825195, 0.9018833637237549, -0.5490863919258118, -0.17991405725479126, 0.403612345457077, 0.13782380521297455, 0.5651883482933044, 0.5692116618156433, 0.5374205708503723, 0.04721200838685036, 0.8908267021179199, -0.8562434315681458, -0.6170749068260193, -1.8090178966522217, 0.8992294073104858, -0.06015324220061302, 0.6327460408210754, -0.2836076617240906, -1.4430664777755737, -0.7279808521270752, -0.7853372693061829, -0.09442519396543503, -0.8254954218864441, 0.30010858178138733, 0.846527099609375, 0.18721365928649902, -1.137282371520996, 0.4159119129180908, 0.0132061205804348, -0.23962441086769104, -0.05838601291179657, 0.10902026295661926, 0.44136542081832886, -0.4132554829120636, -1.5913978815078735, 0.42561104893684387, 0.2507196366786957, -0.4938881993293762, -0.239490807056427, -0.1580297201871872, -1.3640248775482178, -0.2442559152841568, 0.30416375398635864, -0.449756383895874, 1.662951111793518, -0.31984809041023254, -1.274190068244934, 0.4404658377170563, -0.350859671831131, 0.05581140145659447, 0.012992608360946178, -0.031417254358530045, -0.885906994342804, -0.6537895202636719, -0.3995177745819092, 0.5323156118392944, 0.210346058011055, -0.08262407034635544, -0.4160008728504181, 0.004646180663257837, 0.1266883909702301, 0.05570617690682411, -0.24428297579288483, 1.0689548254013062, -0.24678653478622437, -0.28483524918556213, 0.06138773635029793, 0.7486425638198853, -0.2583387792110443, -0.5868736505508423, -0.2462349385023117, -1.1373507976531982, 0.6616896390914917, -0.21096932888031006, 1.3930134773254395, -0.7493737936019897, -0.9031117558479309, -0.2686934769153595, -0.30234861373901367, -0.1180906742811203, -0.961524486541748, 0.7245067358016968, -0.3379721939563751, 0.25035491585731506, -0.07599939405918121, -0.848068118095398, 0.20847183465957642, -0.3021640181541443, -0.6622886061668396, -0.48875418305397034, 0.05438033118844032, 1.3676677942276, -1.1555970907211304, 0.1961577832698822, -0.13298676908016205, 0.14937755465507507, -0.7535709738731384, 1.322842001914978, -0.7482074499130249, -0.16058500111103058, -0.00023192183289211243, -0.059651412069797516, -0.2932957410812378, -0.4496302306652069, 0.3858080506324768, -0.5406802892684937, -0.35821855068206787, 0.5025205612182617, -0.05488359555602074, 1.5115910768508911, -0.6469802260398865, 0.20475628972053528, -0.13650725781917572, -0.30146798491477966, -0.198280930519104, 0.7707459330558777, -0.5020415186882019, -0.42497503757476807, 0.44427165389060974, 0.5628580451011658, -0.640013575553894, 0.2590683102607727, 0.8545945882797241, 0.6092068552970886, -0.3988898694515228, 0.06705187261104584, 0.5583047866821289, 0.00044230069033801556, 0.791756272315979, 0.2869938313961029, 0.6862931847572327, 0.34073761105537415, 0.44205373525619507, 0.057640545070171356, 0.5335621237754822, -0.8268270492553711, -0.27694204449653625, 0.42224863171577454, 0.8865649700164795, 0.48753494024276733, 0.16386033594608307, -0.5396437644958496, 0.07731670886278152, 0.3177614212036133, 0.7667974233627319, 1.7495442628860474, -0.372496098279953, -0.2872520983219147, -0.9428046941757202, -0.44278815388679504, -0.47320181131362915, 0.7101432085037231, -0.22867214679718018, 0.06828778982162476, -0.5173406004905701, -0.7904120087623596, 0.9319159984588623, 0.36264997720718384, 0.6149327158927917, -0.8085806369781494, -0.43912646174430847, 0.2581739127635956, -0.0026748867239803076, -0.9466592073440552, -0.6884336471557617, 0.5074365139007568, -0.7223787903785706, -0.3097420930862427, 0.21195295453071594, -0.3928412199020386, 0.16795815527439117, -0.9798545241355896, 1.0663937330245972, -0.7224523425102234, 0.03171877562999725, 0.28829315304756165, 0.5880225300788879, -0.5932661890983582, -0.8820367455482483, -0.15235504508018494, -0.1264769583940506, -0.25560134649276733, 0.6210306882858276, 0.5740645527839661, 0.29737457633018494, -0.139675110578537, -0.4541243314743042, 0.25852930545806885, 0.38444405794143677, 0.03452906385064125, 0.5928986668586731, -0.3115100562572479, 0.15193670988082886, -1.3887274265289307, 1.286210298538208, 0.2552465498447418, -0.47049418091773987, 0.728852391242981, -0.8130337595939636, -0.19452601671218872, 0.8685745596885681, -0.6301146149635315, -0.5362463593482971, -0.8259111642837524, 0.4296465218067169, 0.018143873661756516, -0.3869855999946594, 0.6044777631759644, -0.009377758018672466, 0.39844411611557007, 0.3171769678592682, 0.545538604259491, 0.27526727318763733, -0.2663199007511139, 0.7072039842605591, -0.3743416666984558, 0.4735068082809448, 0.31725940108299255, -0.005677125416696072, -0.5817826390266418, -0.6317793130874634, -0.5740862488746643, -0.39099568128585815, -0.26405152678489685, 0.1205178052186966, -0.22098730504512787, -0.11907356977462769, -0.5469138026237488, -0.4239081144332886, 0.05705329030752182, -1.2054338455200195, -0.4950961172580719, 0.10353970527648926, -0.7070677876472473, -0.06016755849123001, -1.2110539674758911, -1.2741966247558594, -0.48122042417526245, -0.7888338565826416, -0.9364402890205383, 0.21328166127204895, -0.11919184774160385, -0.538283109664917, -1.0422710180282593, -0.009014849551022053, -0.3854628801345825, 0.9863033294677734, -0.7976857423782349, 1.161112904548645, -0.2793002724647522, 0.13380615413188934, -0.4555966556072235, 0.411130428314209, 0.2680497467517853, -0.22453351318836212, 0.02422153577208519, -1.0005818605422974, 0.18318721652030945, -0.0920640230178833, -0.18738655745983124, -0.4758826494216919, 0.26377418637275696, 0.275169312953949, -0.26421472430229187, -0.5507421493530273, 0.15282098948955536, 1.235912561416626, -0.2958934009075165, -0.18959511816501617, 0.27130505442619324, 0.8503719568252563, 0.2959830164909363, -0.2730174660682678, 0.41519734263420105, 0.455441415309906, 0.2148841917514801, 0.1424197554588318, -0.1636284738779068, 0.04837533459067345, -0.7636547684669495, 0.8832382559776306, 1.5130928754806519, 0.363134503364563, -0.18286463618278503, -1.056294322013855, 0.46423155069351196, -1.1923667192459106, -0.3892200291156769, 0.7883480787277222, 0.6786585450172424, 0.7804301381111145, -0.40750735998153687, -0.5360464453697205, -0.42509546875953674, 0.2607783377170563, 0.3528725504875183, -0.3301326632499695, -0.5527738332748413, -0.13337819278240204, 0.2701929211616516, 0.042060352861881256, 0.7229382395744324, -0.5227414965629578, 0.827978253364563, 14.554104804992676, 0.6327427625656128, -0.012783272191882133, 0.5550340414047241, 0.5086258053779602, 0.395553857088089, -0.4941163659095764, -0.21768800914287567, -1.1415870189666748, -0.4410456120967865, 1.4695059061050415, -0.07682531327009201, 0.9469006061553955, 0.059735141694545746, 0.06687942147254944, 0.19564931094646454, -0.9787573218345642, 0.8452523946762085, 0.6329013109207153, -1.0209541320800781, 0.7356725931167603, -0.1799067109823227, 0.11148755997419357, 0.5911591053009033, 0.742430567741394, 0.9368831515312195, 0.6656808853149414, -0.4436670243740082, 0.7254452705383301, 0.10617304593324661, 0.997055172920227, -0.07628677040338516, 0.4894554913043976, 0.9057857394218445, -0.9349655508995056, -0.6445338129997253, -0.3017352521419525, -1.1282083988189697, 0.09138400107622147, -0.046532995998859406, -0.5574353933334351, -0.4514877200126648, -0.5715343356132507, 0.9387781023979187, -0.3444499671459198, 0.04513614624738693, -0.4238623380661011, 0.4744437038898468, -0.04208739474415779, 0.23658323287963867, 0.3931122422218323, 0.3145035207271576, 0.4711581766605377, -0.01473531685769558, 0.1287602037191391, -0.014206658117473125, 0.2702194154262543, 0.4195464849472046, -0.3946129381656647, 0.18536213040351868, -0.6360308527946472, -0.577797532081604, -0.019765574485063553, 0.6239928603172302, 0.6627215147018433, 0.177706778049469, -0.6433941721916199, 0.25625231862068176, 0.7748690843582153, 0.5534751415252686, 0.10022827982902527, 0.04475057125091553, -0.0777348056435585, -0.2940256893634796, -0.3173312544822693, 0.814118504524231, -0.047770630568265915, -0.4500337243080139, -0.5830040574073792, -0.7422269582748413, 0.5913490056991577, -0.6507294774055481, -0.7520937919616699, 1.1072617769241333, -0.31647780537605286, -0.37273263931274414, -0.24870221316814423, -0.9069933891296387, -0.1370263248682022, 0.48124176263809204, -1.281018853187561, -1.1188461780548096, 0.4430965483188629, -0.13919468224048615, -0.1600807011127472, -0.1860916167497635, 1.6271932125091553, 0.09745429456233978, -0.6662806868553162, 0.23975160717964172, 0.12905550003051758, 0.21026362478733063, -0.4841180741786957, -0.7842263579368591, 0.8268022537231445, 0.3885604739189148, -0.13358956575393677, 0.4910479784011841, -0.002113977912813425, -0.061601873487234116, -0.8635629415512085, -0.25798463821411133, 1.18400239944458, -0.9350897669792175, -0.30768463015556335, -0.7096909880638123, -0.6803717017173767, 0.7445359230041504, 0.6308725476264954, -0.5406160950660706, 0.2584126591682434, 0.2271856963634491, -0.45561718940734863, -0.10459966212511063, -0.8496414422988892, 0.3209467828273773, 0.5620885491371155, -0.8727518320083618, -0.3551380932331085, -0.044606659561395645, 0.3616211712360382, -0.9375547766685486, -0.19695010781288147, -0.22025908529758453, 0.1438041627407074, 0.2100399136543274, 0.7354864478111267, -0.2626151740550995, 0.36885499954223633, 0.8136477470397949, -0.1345374882221222, -0.9330185651779175, -0.11737100780010223, -0.8985366225242615, -0.04987025633454323, -0.07198581844568253, 1.106546401977539, -0.4222835600376129, -0.16835370659828186, 1.2965354919433594, 0.3229690492153168, -0.41737690567970276, -0.4649684727191925, -0.36162397265434265, 0.016167446970939636, -0.5465591549873352, 0.5214043259620667, -0.18485651910305023, 0.3103635311126709, 0.4520970582962036, 0.682235598564148, 0.6069077849388123, -0.21609044075012207, -0.71323162317276, 0.2365017533302307, -0.10437566041946411, 0.13171392679214478, -0.6586964726448059, -0.15158003568649292, -1.4536324739456177, -0.046642400324344635, -1.240846037864685, 0.1704486459493637, -0.9623761177062988, -0.5866363048553467, -0.08074941486120224, -0.36537110805511475, 0.2841894030570984, 0.38755229115486145, -0.9333451986312866, -0.6905127167701721, -0.6364929676055908, -0.39332446455955505, 0.68439120054245, 0.5534188151359558, -0.8501829504966736, -0.051183730363845825, -0.20554377138614655, 0.15093407034873962, 0.3334076702594757, 0.26321694254875183, -0.1489400416612625, -1.0197733640670776, -1.6444542407989502, 0.4776734709739685, -0.03462468832731247, -0.19309145212173462, -0.6673656702041626, 0.6980054378509521, 0.16429680585861206, -0.41065695881843567, 0.38146260380744934, 0.11012698709964752, -1.1131099462509155, -0.3824223279953003, 0.14973366260528564, -0.9681667685508728, 0.3761642575263977, 0.3893314003944397, -0.7414618730545044, -0.5273264646530151, 0.4593032896518707, -0.27415499091148376, -1.147060513496399, -0.7487190365791321, 0.400576651096344, -0.5639196634292603, -0.0171208493411541, -0.2823165953159332, -0.03388065844774246, -0.8417530655860901, -0.24483154714107513, 0.17513039708137512, 0.3218518793582916, -0.23025351762771606, 1.1114147901535034, 0.5578064322471619, -0.9192254543304443, 0.04612308740615845, 0.48483046889305115, 0.1671999990940094, 0.10039228945970535, 0.2804695665836334, 0.34308645129203796, -0.14580686390399933, 0.6247256994247437, 0.4214359223842621, 0.48931506276130676, -0.9553635120391846, -0.08760528266429901, 0.9811952710151672, -0.7020695805549622, -0.0953487977385521, 1.279829740524292, -0.29249629378318787, -1.2556543350219727, 0.3712534010410309, -1.375186800956726, -0.8901910781860352, -0.6437656879425049, 0.7813200950622559, -0.06681124120950699, 0.10225578397512436, -0.16266126930713654, -0.3876629173755646, -0.07363167405128479, -0.23669491708278656, -0.5463482737541199, 0.531768262386322, -0.054528288543224335, -0.4187963306903839, 0.7388796806335449, 0.4826568365097046, -0.6057697534561157, -0.6110127568244934, -0.6496208310127258, -0.21464207768440247, 0.1878942996263504, 0.573955774307251, -0.9198057055473328, -0.529735267162323, 0.6072781085968018, 0.46752822399139404, 0.12475761026144028, 0.10861431062221527, -0.2189251333475113, 0.2970673441886902, 0.887423574924469, 0.2559763491153717, -0.9305706024169922, -0.83716881275177, 1.5471137762069702, 1.6288543939590454, -0.9935061931610107, -0.23956432938575745, -0.11758876591920853, -0.47667616605758667, 0.8634504079818726, 0.2852839231491089, 0.4448525309562683, 0.9154028296470642, -0.026156827807426453, 0.36913564801216125, -0.07077569514513016, -1.1939159631729126, 0.09855926036834717, 0.5981918573379517, 0.9207878112792969, 1.0962700843811035, 0.5168699026107788, 0.23264452815055847, 1.3478331565856934, 0.06239266321063042, 0.4116799533367157, 0.29365867376327515, 0.7231797575950623, 0.0030806581489741802, -0.2543979287147522, 0.16424629092216492, 0.6402469873428345, -0.4329957067966461, -1.113098382949829, -0.0010088043054565787, 0.31834694743156433, 0.20217394828796387, 0.8518864512443542, 0.5072064399719238, 0.10445650666952133, 0.802204430103302, 0.5533772110939026, 0.4656009078025818, -0.6586164236068726, -0.4975600838661194, -0.3249492943286896, -0.29363107681274414, -0.009830242022871971, 0.02750302664935589, -0.4296276867389679, -0.3493005633354187, 0.26410743594169617, 0.3407732844352722, 0.04618890583515167, 0.27148133516311646, 1.1229190826416016, 0.37032821774482727, 0.2628130316734314, -0.6627882719039917, -0.008683034218847752, -0.49633070826530457, -1.2727373838424683, 0.12590430676937103, -0.3940330743789673, -0.11486645042896271, 0.15083472430706024, -0.08989936858415604, -0.03612334653735161]}, "authors": [{"authorId": "2267505808", "name": "Yunpeng Huang"}, {"authorId": "2158383105", "name": "Jingwei Xu"}, {"authorId": "2267505325", "name": "Zixu Jiang"}, {"authorId": "2267491104", "name": "Junyu Lai"}, {"authorId": "15401196", "name": "Zenan Li"}, {"authorId": "2254287436", "name": "Yuan Yao"}, {"authorId": "2267520498", "name": "Taolue Chen"}, {"authorId": "2267505839", "name": "Lijuan Yang"}, {"authorId": "2267485767", "name": "Zhou Xin"}, {"authorId": "2267813458", "name": "Xiaoxing Ma"}], "references": [{"paperId": "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b", "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning"}, {"paperId": "a3dd3adc37cdb8991936f3feb109c20b6f892f3d", "title": "Extending Context Window of Large Language Models via Semantic Compression"}, {"paperId": "f56fd8eee26d28111ba0e8dd812ee5fc813f666f", "title": "SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention"}, {"paperId": "c4ae033f14a89db8b6b6b4da598375f177a1fac4", "title": "Memory Augmented Language Models through Mixture of Word Experts"}, {"paperId": "a54761081c2b001c057fb6e1ea9a48058d5aa5e0", "title": "CLEX: Continuous Length Extrapolation for Large Language Models"}, {"paperId": "908dad62c0e43d80e3e3cb3c0402f7c71c70499c", "title": "MemGPT: Towards LLMs as Operating Systems"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "539fadfb615ef84c240f4741061c44eeda540091", "title": "Scaling Laws of RoPE-based Extrapolation"}, {"paperId": "a4382a9b1edba07e0af4df2ff6a4bce22f4e55b5", "title": "AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0", "title": "Effective Long-Context Scaling of Foundation Models"}, {"paperId": "b6346f9fa093b8e85df712485a2b851b9f680dac", "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"}, {"paperId": "73290ecbec2f38d1d647ddef1ada69cee41725b3", "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "2dfb9171e180dcb0af23d305e024d43d311708ab", "title": "Giraffe: Adventures in Expanding Context Lengths in LLMs"}, {"paperId": "2edccb8fa562ed52cd49ea6fc67ed32db6218247", "title": "From Sparse to Soft Mixtures of Experts"}, {"paperId": "c0aec04ee86c0724d61c976f19590fbe9c615723", "title": "Creating Large Language Model Applications Utilizing LangChain: A Primer on Developing LLM Apps Fast"}, {"paperId": "b0db25e317cf856f1ec1ca3df0e573d850ed4696", "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "1ca5618423c64f0656d13c2bc0d387cc2006f7b2", "title": "ZeRO++: Extremely Efficient Collective Communication for Giant Model Training"}, {"paperId": "80980cd10d19f021c14a6b7eee871b6a5d328024", "title": "Augmenting Language Models with Long-Term Memory"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "c8355e22cadd2988ad517b77656fd1212ee9bbb5", "title": "ChatGPT and large language model (LLM) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine."}, {"paperId": "d203c764fb5dec2b053be667c8b06e516ea6ef10", "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention"}, {"paperId": "2a037afd47d22e8ff4733059d5c64aac446ada13", "title": "Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation"}, {"paperId": "af385c0fdd0eda2bbf429bea6fedffc327c8a180", "title": "Randomized Positional Encodings Boost Length Generalization of Transformers"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf", "title": "Adapting Language Models to Compress Contexts"}, {"paperId": "a22f3398ea865426c89ee66f4824ec626e56a864", "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "a10843d1349fff8d2a7d9722f800802187fef67f", "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "d9964ab436eefd21f923a4bc833c6b66692c7f00", "title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text"}, {"paperId": "c3a59e1e405e7c28319e5a1c5b5241f9b340cf63", "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "c8dd0fe00f9a71ea68b6856b36590b8daa316139", "title": "A Frustratingly Easy Improvement for Position Embeddings via Random Padding"}, {"paperId": "700da3f3758e053c379f905bebee261ba69f1073", "title": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation"}, {"paperId": "dbc368bc8b49347dd27679894524fa62f88492c9", "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input"}, {"paperId": "266d671d5d6bac3c88d7bfb18c5210b46f06e6db", "title": "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT"}, {"paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da", "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"}, {"paperId": "594d8e1696619f3cebb7c6bffdad8e0a5592f006", "title": "Scaling Transformer to 1M tokens and beyond with RMT"}, {"paperId": "5be9a64df5f8d7e5a33fcc2c7bdfcde1fbbd085a", "title": "Large Language Models in Medical Education: Opportunities, Challenges, and Future Directions"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "dbbc5003af690799fa4fe6330fb795311cde106f", "title": "FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement"}, {"paperId": "34d28a5d698d6b93f891d70b5008eb899bc08ee5", "title": "Large language models challenge the future of higher education"}, {"paperId": "574beee702be3856d60aa482ec725168fe64fc99", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"}, {"paperId": "a8e0ba16346b72c3a04dd0b1da84bc5f28900174", "title": "Using GitHub Copilot to Solve Simple Programming Problems"}, {"paperId": "68adb03744692247fb834406798894db9fe77010", "title": "A Survey on Long Text Modeling with Transformers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "fdd7d5b0f6b8641c356e170fd264cd11f70ba657", "title": "Planning-oriented Autonomous Driving"}, {"paperId": "5735e49e501c8e51e9be4079592e46e047747b03", "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "661e8d555c4424b5953f17434f2ba910bfcf3afe", "title": "Efficient Long Sequence Modeling via State Space Augmented Transformer"}, {"paperId": "eecb45aa040064cbc0b37fd100706c02e7dc880e", "title": "Structured Prompting: Scaling In-Context Learning to 1, 000 Examples"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "6d088e8d785a57b50c2b0e465e2460e09ced48d7", "title": "Accelerating Distributed MoE Training and Inference with Lina"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "1df0d9c553aee087fe3a7dd1c5f9e03556eb1fe4", "title": "HEGEL: Hypergraph Transformer for Long Document Summarization"}, {"paperId": "4afda39036206dcb3f97829dccb897f1fc80f459", "title": "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "003c08471fe579d98e82cf5c0cac03897403fb55", "title": "FP8 Quantization: The Power of the Exponent"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "732e3faec4e5be4d144256f2c379b9dc49f0b227", "title": "Efficient Long-Text Understanding with Short-Text Models"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "title": "Exploring Length Generalization in Large Language Models"}, {"paperId": "f8d44802ac8190864c61c9aaf4a8b450261873ab", "title": "An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b", "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "51b950bfcaba4bad321e7342b32833d42f42c914", "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "b7ddf2d28bcf0ffc656a592e129ba28a4b82d681", "title": "Survey of Machine Reading Comprehension Models and its Evaluation Metrics"}, {"paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0", "title": "Memorizing Transformers"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "92173d081b15824d22a9ef070e118744ceee8052", "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"}, {"paperId": "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed", "title": "Sparse is Enough in Scaling Transformers"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "274f903041b1a830b37f57929d837c1706e94ec7", "title": "PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "64522a5b3476e9f201f6a5b3e312ef0005c562f1", "title": "SHAPE: Shifted Absolute Position Embedding for Transformers"}, {"paperId": "35ce0b6373c0bc585d6a0232caf7924ea8ad8a1c", "title": "Long-context Transformers: A survey"}, {"paperId": "bb363c8c5bc1c473f0801c647c88d0c071792858", "title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6", "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "c281a2403dba7511d89c8c5bc846f6a35481b22b", "title": "Context-aware Self-Attention Networks for Natural Language Processing"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "aee7fd72f33bc8060ff32eb88f88b904802a9243", "title": "Lower Perplexity is Not Always Human-Like"}, {"paperId": "1fa487376af5d4293ec482d193ca1790176c4dbc", "title": "Language Model Evaluation Beyond Perplexity"}, {"paperId": "16e623059ffccab60f4c35be028a2d4f10933515", "title": "Sequence Parallelism: Long Sequence Training from System Perspective"}, {"paperId": "d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5", "title": "Pretrained Language Models for Text Generation: A Survey"}, {"paperId": "64a29bee2e1ad29547d590a3cc26274f4c537145", "title": "Not All Memories are Created Equal: Learning to Forget by Expiring"}, {"paperId": "6a3e13d7926a4aaa0ddcd3acc7c08e8d24c330e5", "title": "ReadTwice: Reading Very Large Documents with Memories"}, {"paperId": "f4566761fe39c4b5273d696d9bc3f4195c9325bb", "title": "Long-Span Summarization via Local Attention and Content Selection"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "db46b0de44c5113c47f0ec5392eb91d0726497bf", "title": "A Simple and Effective Positional Encoding for Transformers"}, {"paperId": "836e9619f779f2d9642561f2b697ba471b866b27", "title": "Hierarchical Learning for Generation with Long Source Sequences"}, {"paperId": "9dc624d7258d1a56117ca720aea953ce46b66b21", "title": "Efficient Attentions for Long Document Summarization"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "cbb8faf1051d858b4b0426742fdbbd0f104833ea", "title": "Automatic text summarization: A comprehensive survey"}, {"paperId": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8", "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention"}, {"paperId": "c90c24f5487f71a43857371b9427f43cad8cecf8", "title": "A Survey on Locality Sensitive Hashing Algorithms and their Applications"}, {"paperId": "69a72ff5b30642d11c96635e99aadad3140d33a7", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "afad10da0a3b83a4f2a94e8c16c84ac64338e9fe", "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer"}, {"paperId": "320efa53dea3e8f836790682fbd4196132c49749", "title": "Segatron: Segment-Aware Transformer for Language Modeling and Understanding"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "67ee20536c30a225b86902af2f091e28e5e19b40", "title": "Memformer: A Memory-Augmented Transformer for Sequence Modeling"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "ea8c46e193d5121e440daf96edfd15a47151c293", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "09e69bf0926e55cd277a3ef5b1450ba083719cb9", "title": "Sparse and Continuous Attention Mechanisms"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "16c666f7d90ac04a0d0c171a508fae98bdcb5fde", "title": "Na\u00efve Bayes"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "01b15017ac59b8d6f2ce3598c4a7d6358c211426", "title": "A Divide-and-Conquer Approach to the Summarization of Long Documents"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "36e30516683032634975c53e60f3737b6e35ff80", "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "d58fd97d7a8308a20b159bbaf02011e26c323852", "title": "A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures"}, {"paperId": "7edacd94dc1509803d9bbcc1d92fea780d71cb3e", "title": "MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "203b543bfa1e564bb80ff4229b43174d7c71b0c0", "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"}, {"paperId": "0b5d7a79205b44952e24025ce5d46e9f3aa401a1", "title": "Low-Memory Neural Network Training: A Technical Report"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "2a31319e73d4486716168b65cdf7559baeda18ce", "title": "Star-Transformer"}, {"paperId": "d51a9fc27f6836e551094c5c261eddd30f472691", "title": "Hierarchical Attentional Hybrid Neural Networks for Document Classification"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "22655979df781d222eaf812b0d325fa9adf11594", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"}, {"paperId": "e20ff55e87e2b3ef02ae0529880bb705f5efbcae", "title": "Document-Level Neural Machine Translation with Hierarchical Attention Networks"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "7a84a692327534fd227fa1e07fcb3816b633c591", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"}, {"paperId": "ec1f582446aa24f3f0920123ee6f05feea0b5e0a", "title": "Online normalizer calculation for softmax"}, {"paperId": "fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context"}, {"paperId": "0e5d529befc3ca2e3e3371a0c39dc05731c1d5b7", "title": "Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking"}, {"paperId": "0ef460c47377c3b9482d8177cbcafad1730a91a5", "title": "Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling"}, {"paperId": "853d4d94651c6d9f8ed4d114e1eb21f15f786daa", "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "724f6a675455e133172c5447d681f283ac411a85", "title": "Entity mention aware document representation"}, {"paperId": "2b91a2cbcd9cce902cbc8da78fec5f18f4bffc98", "title": "Deep learning for sentiment analysis: A survey"}, {"paperId": "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a", "title": "The NarrativeQA Reading Comprehension Challenge"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5394da74498e00597295d18cd0557bd47e3fc341", "title": "The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "515a21e90117941150923e559729c59f5fdade1c", "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "455afd748e8834ef521e4b67c7c056d3c33429e2", "title": "Hierarchical Attention Networks for Document Classification"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "f63e917638553414526a0cc8550de4ad2d83fe7a", "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"}, {"paperId": "7f5fc84819c0cf94b771fe15141f65b123f7b8ec", "title": "Multi-Scale Context Aggregation by Dilated Convolutions"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "30feade758d15844a5746fd0de7983d5a0e4af02", "title": "ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "d1275b2a2ab53013310e759e5c6878b96df643d4", "title": "Context dependent recurrent neural network language model"}, {"paperId": "bf5243429b1a8a566760c6d38ac5eddb808764c0", "title": "Tiling for Performance Tuning on Different Models of GPUs"}, {"paperId": "c8831d7d318b8d59f9b958d250a58f253f08bd8a", "title": "Robust principal component analysis?"}, {"paperId": "1160c2e988fab7592af1e35698d62a5cb4a20e76", "title": "Meteor, M-BLEU and M-TER: Evaluation Metrics for High-Correlation with Human Rankings of Machine Translation Output"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "ea8647a1b570743c49de6d6a71b3a786d48b8c2e", "title": "The Logical Form of Action Sentences"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": "025a21e5914071b0ade80a5972d137c3ee69b75a", "title": "Virtual Memory"}, {"paperId": "9d7bbf669ce96e022379bc99f8b9aa6083400829", "title": "One-Level Storage System"}, {"paperId": null, "title": ". pipegoose: Large-scale 4D parallelism pre-training for \u2018transformers\u2018"}, {"paperId": "de11a7c546210147536ba977d14e731cda125a56", "title": "Journal of Applied Learning & Teaching"}, {"paperId": "e3aa232577bb427b1f3a34acbdef84bd85734042", "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"}, {"paperId": "8b5e200df664176a8b07cedff84c40aa3929166d", "title": "RecallM: An Architecture for Temporal Context Understanding and Question Answering"}, {"paperId": null, "title": ". Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"}, {"paperId": null, "title": "bloc97. Add ntk-aware interpolation \u201dby parts\u201d"}, {"paperId": null, "title": "Dynamically scaled rope further increases performance of long context llama with zero fine-tuning"}, {"paperId": null, "title": "Introducing qwen-7b: Open foundation and human-aligned models (of the state-of-the-arts)"}, {"paperId": null, "title": "Transformer upgrade roadmap:7. length extrapolation and local attention"}, {"paperId": null, "title": "InfiniteBench: 128k Long-Context Benchmark for Language Models"}, {"paperId": "955f90930d48750e7239478b4eed440eb84131cd", "title": "FourierFormer: Transformer Meets Generalized Fourier Integral Theorem"}, {"paperId": "d0e7e283f8ac89f809e679cd64747be3d24b0c6b", "title": "Distribution-Based Measures of Surprise for Creative Language: Experiments with Humor and Metaphor"}, {"paperId": "d668f12be54174141e6197fad737006b7b0c0571", "title": "PyTorch"}, {"paperId": "a5fa6e7565dca654eab9372ace4b1ba7f63655f7", "title": "CogLTX: Applying BERT to Long Texts"}, {"paperId": null, "title": "How to generate text: using different decoding methods for language generation with transformers"}, {"paperId": null, "title": "Addressing some limitations of transformers with feedback memory"}, {"paperId": null, "title": "Funnel-transformer:Filteringoutsequentialredundancyforefficientlanguageprocessing"}, {"paperId": null, "title": "DeepSpeed Sparse Attention: Powering 10x longer sequences with 6x faster execution"}, {"paperId": null, "title": "Deepspeed: Extreme-scale model training for everyone"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82", "title": "Deep Learning"}, {"paperId": null, "title": "2023. Model Card and Evaluations for Claude Models"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4", "title": "Gradient-based learning applied to document recognition"}, {"paperId": "2cfbd05d82ed222495f7a8f4c335732ee0c8d780", "title": "Adaptive Multivariate Ridge Regression"}, {"paperId": null, "title": "Llama/gptneox: add rope scaling"}, {"paperId": null, "title": "Harrison Chase"}, {"paperId": null, "title": "places different experts on different GPUs and executes them in parallel"}, {"paperId": null, "title": "Chatchat-Space"}, {"paperId": null, "title": "2022. FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models"}, {"paperId": null, "title": "2022. SCROLLS:StandardizedCompaRisonOverLongLanguageSequences"}, {"paperId": null, "title": "2023. How Long Can Open-Source LLMs Truly Promise on Context Length?"}, {"paperId": null, "title": "2023. Mixtral of Experts: A High-Quality Sparse Mixture-of-Experts"}, {"paperId": null, "title": "Manuscript submitted to ACM"}, {"paperId": null, "title": "2023. Vanna: an open-source Python RAG framework for SQL generation and related functionality"}, {"paperId": null, "title": "2023. Llmlingua: Compressing prompts for accelerated inference of large language models"}, {"paperId": null, "title": "Advancing Transformer Architecture in Long-Context"}, {"paperId": null, "title": "OpenAI"}, {"paperId": null, "title": "2023. Transformer Upgrade Roadmap: 10. RoPE is a beta-base Encoding"}, {"paperId": null, "title": "(b) Tensor Parallelism (TP) [200] introduces tensor splitting, where individual layers of the model are horizontally partitioned over multiple devices"}, {"paperId": null, "title": "c) Pipeline Parallelism"}, {"paperId": null, "title": "Alpaca: A strong, replicable instruction-following model"}]}