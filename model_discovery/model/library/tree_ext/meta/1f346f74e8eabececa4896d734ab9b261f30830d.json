{"paperId": "1f346f74e8eabececa4896d734ab9b261f30830d", "title": "Modular Deep Learning", "abstract": "Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.", "venue": "arXiv.org", "year": 2023, "citationCount": 46, "influentialCitationCount": 3, "openAccessPdf": {"url": "http://arxiv.org/pdf/2302.11529", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "A survey of modular architectures is offered, providing a unified view over several threads of research that evolved independently in the scientific literature, and various additional purposes of modularity are explored, including scaling language models, causal inference, programme induction, and planning in reinforcement learning."}, "embedding": {"model": "specter_v2", "vector": [0.4137153923511505, 0.7182419896125793, -0.2521457076072693, -0.02485724538564682, -0.37726157903671265, -0.09382344782352448, 0.9503557682037354, -0.3643551170825958, -0.2508053183555603, 0.162429079413414, 0.1929192692041397, -0.07897121459245682, 0.2503839135169983, 0.006527840159833431, -0.512814462184906, 0.05627084895968437, -0.9682894945144653, 0.22224140167236328, 0.4330034852027893, -0.1867581605911255, -0.36995038390159607, -0.7144003510475159, -0.9515628218650818, 0.26895368099212646, 0.19048136472702026, 0.40562984347343445, 0.22471646964550018, 0.8678684830665588, -0.012462923303246498, 0.5891299247741699, 0.16527818143367767, -0.3754151165485382, 0.3335976004600525, -0.2176990658044815, -0.7445633411407471, 0.03502007946372032, 0.1514146327972412, -0.25888895988464355, -0.27621835470199585, 0.7502039670944214, -0.29325932264328003, 0.0139017878100276, 0.48318248987197876, -0.699040949344635, 0.12500561773777008, 0.8045473098754883, 0.6899716854095459, 0.6542932987213135, -0.3866756856441498, -0.5912865996360779, 1.2887142896652222, -0.7852134108543396, 0.037388771772384644, 1.6245085000991821, 0.2974119186401367, 0.6544877290725708, -0.8643425107002258, -0.570661187171936, 0.6046322584152222, -0.17391346395015717, -0.5345797538757324, -0.09283708035945892, -0.08805549889802933, -0.13742326200008392, 1.754577398300171, -0.5804723501205444, -0.0834527239203453, 0.36944273114204407, 0.2616126239299774, 1.514506459236145, 0.2421959638595581, -0.7043251395225525, -0.26376935839653015, 0.21043860912322998, -0.1965683102607727, 0.9508755803108215, -0.4004005789756775, 0.45604780316352844, -1.069237470626831, 0.03334010764956474, 0.5901715159416199, -0.12288802862167358, 0.28113025426864624, -0.36322200298309326, -0.2575795352458954, 0.7562109231948853, 0.4250848591327667, 0.5565195679664612, -0.32777225971221924, 1.1055324077606201, 0.676679790019989, 0.8716698288917542, -0.07673151046037674, 0.18333154916763306, -0.4117083251476288, 0.6721803545951843, -0.34248194098472595, -0.014238831587135792, 0.10218667984008789, 0.6615695357322693, 0.442891001701355, 0.10162529349327087, -0.4353007674217224, 0.18553154170513153, 1.455878496170044, -0.1792309284210205, 0.5789092779159546, -0.7842799425125122, 0.520596981048584, -0.4007868468761444, -0.23393380641937256, -0.2480091005563736, -0.43579065799713135, -0.14635036885738373, -0.4667893350124359, -0.8836449980735779, -0.8831871151924133, -0.05567260831594467, -0.26344096660614014, 1.0872315168380737, -0.3435691297054291, -0.20949921011924744, 0.10806872695684433, 0.28741371631622314, -0.07118119299411774, 0.25215211510658264, 0.6431386470794678, 0.3840264678001404, 0.895991861820221, -0.4265919625759125, -0.21529754996299744, -1.1230406761169434, 0.5990563035011292, -0.1356050819158554, -0.28028079867362976, -0.37464380264282227, -1.4793322086334229, -0.9785000085830688, -1.024773120880127, 0.4653824269771576, -0.4291536509990692, -0.014452639035880566, 1.3371481895446777, 0.47432225942611694, -0.9959420561790466, 1.1412526369094849, -0.32820987701416016, -0.15956760942935944, 0.6671927571296692, 0.6266175508499146, 0.05667492747306824, -0.26592743396759033, -1.226593017578125, 0.500853955745697, 0.723231315612793, -0.8810566067695618, -0.4955301880836487, -0.49379441142082214, -0.9189519882202148, -0.3225913345813751, 0.026456397026777267, -1.144853949546814, 1.4976279735565186, -0.8262418508529663, -1.455725073814392, 0.7307356595993042, -0.07643020153045654, -0.10347858816385269, 0.25607970356941223, -0.1285899430513382, -0.20010945200920105, -0.11422758549451828, 0.06308545172214508, 0.6848635077476501, 0.688481867313385, -0.026412267237901688, -0.5245575308799744, 0.044719018042087555, -0.12397430837154388, 0.0028222219552844763, -0.18951165676116943, 0.7888036966323853, -0.07257159799337387, -0.0339646078646183, 0.012096332386136055, 0.8510322570800781, 0.10340432077646255, -0.491731733083725, -0.13166360557079315, -1.3454642295837402, 0.3979760408401489, 0.046727847307920456, 0.5367942452430725, -0.9234316945075989, -0.27044129371643066, -0.26211056113243103, -0.1918077915906906, -0.3027177155017853, -0.7156064510345459, 0.8611071705818176, -0.5504651665687561, 0.15486864745616913, -0.528283953666687, -1.1122877597808838, 0.16224858164787292, 0.15071681141853333, -0.35593703389167786, -0.11153066158294678, 0.8022246956825256, 1.2371835708618164, -0.7950975298881531, 0.06419797241687775, -0.06273145228624344, 0.07945503294467926, -0.8983684778213501, 1.3691447973251343, -0.4440532922744751, 0.30579012632369995, 0.0867856815457344, -0.46341192722320557, 0.09364428371191025, -0.20995834469795227, 0.8541668057441711, -0.20431189239025116, 0.23320820927619934, 0.77300626039505, -0.5072019100189209, 1.5814050436019897, -0.5268881916999817, 0.40694352984428406, 0.0073827113956213, -1.208022117614746, -0.11599898338317871, 0.5703476667404175, -0.2315700799226761, -0.6817560195922852, 0.2240908443927765, 0.06162112206220627, -0.5678400993347168, 0.12784011662006378, 0.6026845574378967, 0.15017224848270416, -0.4284098744392395, 0.4982552230358124, 1.2180999517440796, -0.4223152697086334, 0.37395337224006653, 0.36198756098747253, 0.77432781457901, 0.6127396821975708, 0.5121393799781799, -0.21786266565322876, 0.284836083650589, -1.1996253728866577, -0.17720386385917664, 0.09059804677963257, 0.6041930913925171, 0.3740870952606201, 0.578352689743042, -0.42771774530410767, -0.2902165651321411, -0.4236602783203125, 0.593279242515564, 1.9375160932540894, -0.23494946956634521, 0.22784385085105896, -0.6785703897476196, -0.618873655796051, -0.14237064123153687, 0.04558651149272919, -0.6818236708641052, -0.5602890253067017, -0.6586768627166748, -1.1695003509521484, 0.5967151522636414, 0.5096291899681091, 1.054122805595398, -0.4833775758743286, -0.3357369601726532, 7.150724559323862e-05, 0.3842153549194336, -0.37711232900619507, -0.305174320936203, 0.8966057300567627, -0.6545047163963318, -0.2799120843410492, 0.42879530787467957, -0.24622978270053864, 0.24458041787147522, -0.5297656059265137, 1.2298715114593506, -0.5986618399620056, -0.3042449355125427, 0.4107467234134674, 0.8703709840774536, -0.7616971135139465, -0.8398646712303162, 0.2960055470466614, -0.26879003643989563, -0.01818944327533245, -0.00739976717159152, 0.2445523589849472, -0.29176947474479675, 0.4717353582382202, -0.8430548310279846, 0.08825240284204483, 0.20726296305656433, 0.28424525260925293, 0.22714462876319885, -0.17577195167541504, 0.4663549065589905, -1.3097947835922241, 0.7920138239860535, -0.26272690296173096, -0.25754252076148987, 0.03426365926861763, -0.35279497504234314, -0.1555284708738327, 0.29127421975135803, -0.45748868584632874, -0.574830174446106, -0.9684221148490906, 0.4169968366622925, 0.24501092731952667, -0.33998775482177734, -0.11743856966495514, 0.35186147689819336, 0.015048163011670113, 0.3829464316368103, 0.25738027691841125, 0.3942771553993225, 0.14971022307872772, 0.8426551818847656, -1.0805976390838623, 0.19077733159065247, 0.10865725576877594, 0.32201921939849854, 0.02990373782813549, -0.009267573244869709, -0.06424791365861893, -0.4450426399707794, -0.3568366467952728, -0.39309385418891907, -0.2591480016708374, 0.19707296788692474, -0.7215855121612549, -0.963538408279419, -0.0906972736120224, -0.9453147649765015, -0.6457456946372986, 0.14297452569007874, -0.4886223077774048, -0.5050726532936096, -1.1256511211395264, -1.244229793548584, -0.3297220766544342, -0.3030281662940979, -0.6628463268280029, 0.38746142387390137, 0.11406176537275314, -0.6044763922691345, -1.1338231563568115, 0.16459885239601135, -0.45974719524383545, 0.9097276329994202, -0.6967651844024658, 0.8911949992179871, 0.22107499837875366, -0.44963130354881287, -0.09382563829421997, 0.08940551429986954, 0.8187817335128784, -0.3310858905315399, -0.2675538957118988, -1.2288519144058228, 0.12728402018547058, -0.6101081371307373, -0.80581134557724, 0.16658712923526764, -0.034319084137678146, 0.6082652807235718, 0.20294171571731567, -0.3916546702384949, 0.06676676869392395, 1.4529852867126465, -0.7998930215835571, -0.13737773895263672, 0.3760915994644165, 0.5887068510055542, 0.5173913836479187, -0.6212571859359741, 0.23551680147647858, 0.6752238869667053, 0.44359999895095825, 0.36456000804901123, -0.1172851175069809, -0.3609752058982849, -0.679829478263855, 0.6239585876464844, 1.1016706228256226, 0.33299756050109863, 0.4829074740409851, -0.49906307458877563, 0.5458945035934448, -1.2914944887161255, -0.6242833137512207, 1.04396653175354, 0.5996623039245605, 0.7465651631355286, -0.4120498597621918, -0.304452508687973, -0.1306459903717041, 0.3554145395755768, 0.11919813603162766, -0.3182004392147064, -0.5930823087692261, 0.05681174620985985, 0.7025749683380127, 0.26438844203948975, 0.5204732418060303, -0.3735817074775696, 0.46694594621658325, 14.917677879333496, 0.8293426036834717, 0.24258223176002502, 1.0194615125656128, 0.29444369673728943, 0.44766101241111755, -0.33214858174324036, 0.09246610105037689, -1.1733369827270508, -0.19485342502593994, 1.237715482711792, 0.3033618927001953, 0.724567711353302, -0.21391835808753967, -0.4387102723121643, -0.018812507390975952, -0.8267788290977478, 0.11596222221851349, 0.4482758343219757, -0.9209941029548645, 0.21962524950504303, -0.02142130769789219, 0.6964501142501831, 0.6897214651107788, 0.622631311416626, 0.9365163445472717, 0.9386000633239746, -0.30713677406311035, 0.49345868825912476, 0.255896657705307, 0.5508080124855042, 0.08342357724905014, 0.1642119288444519, 0.693148136138916, -0.9108101725578308, -0.2682594060897827, -0.460558146238327, -1.0178667306900024, 0.17777471244335175, -0.10455498844385147, -0.46195659041404724, -0.13391058146953583, -0.328181654214859, 0.7118146419525146, 0.20335142314434052, 0.2386341094970703, -0.6468650102615356, 0.5166099667549133, -0.15817323327064514, 0.28518974781036377, -0.2036077380180359, 0.7207384705543518, -0.13252560794353485, 0.14567016065120697, -0.4515708386898041, -0.5806215405464172, -0.23708757758140564, 0.4208720326423645, -0.778946578502655, -0.36465364694595337, -0.3472488820552826, 0.19235685467720032, -0.191388800740242, 0.7100432515144348, 0.7645282745361328, 0.26861101388931274, -0.505271315574646, 0.5156699419021606, 0.9874412417411804, 0.1210118979215622, -0.3180868923664093, 0.1329159140586853, 0.2046644687652588, -0.7555661797523499, 0.2844434976577759, 0.6355814933776855, -0.1476384550333023, -0.2838360369205475, -0.7884280681610107, 0.013290474191308022, 0.601685106754303, -0.6478327512741089, -0.9341822862625122, 0.4300437569618225, -0.3292873501777649, -0.3794269561767578, 0.09936806559562683, -0.9425459504127502, -0.38160115480422974, 0.5150336623191833, -1.7179648876190186, -0.5296304821968079, 0.5703331232070923, -0.022605037316679955, -0.6309545636177063, -0.15043339133262634, 1.3530148267745972, 0.21447528898715973, -0.46454423666000366, 0.02026453986763954, -0.3310934901237488, -0.24315793812274933, -0.005210231523960829, -1.0214474201202393, 0.5565400123596191, 0.036322228610515594, -0.47849205136299133, 0.2393391877412796, -0.32827699184417725, 0.4243892729282379, -0.5143696069717407, -0.28703421354293823, 0.6502909660339355, -0.9131550192832947, -0.13736049830913544, -0.3821099102497101, -0.5168439149856567, 0.562981903553009, 0.9427986145019531, -0.5554496645927429, 0.5704503655433655, 0.09096070379018784, -0.9165600538253784, -0.3308402895927429, -0.675702691078186, -0.08693353086709976, 0.23679989576339722, -0.8251289129257202, -0.5180429816246033, -0.0892205685377121, 0.39919307827949524, -0.6027047038078308, -0.6707943677902222, -0.4114260971546173, -0.1402999311685562, -0.2713940739631653, 0.9199259281158447, -0.5810359716415405, 0.4755847156047821, 0.7178857326507568, 0.024404309689998627, -0.9746159315109253, -0.05871594697237015, -1.0462982654571533, 0.24443931877613068, 0.19642747938632965, 0.5848968029022217, -0.8764979839324951, -0.1841866821050644, 0.5712979435920715, 0.2320130318403244, -0.14221283793449402, -0.7943810820579529, 0.12389100342988968, 0.32017838954925537, -0.8229501843452454, 0.2838137745857239, -0.1493857502937317, -0.04105287790298462, 0.35379061102867126, 0.5840991139411926, 0.6359672546386719, 0.14217033982276917, -0.5656754374504089, 0.2948072552680969, -0.15272767841815948, -0.3777252435684204, -0.9095569849014282, -0.7109496593475342, -1.44953191280365, 0.18494537472724915, -1.5009983777999878, -0.10158824175596237, -0.9498704671859741, -0.36547961831092834, 0.24367493391036987, -0.2581678032875061, 0.2342740148305893, 0.22576849162578583, -0.1914471685886383, -0.5901491641998291, -0.5195485949516296, -0.625386655330658, 0.8634738326072693, 0.7488753199577332, -0.6174266934394836, 0.4152050316333771, 0.19166946411132812, -0.13332247734069824, 0.24791117012500763, 0.8088268041610718, -0.519225537776947, -0.716964066028595, -1.4502097368240356, 0.6694300174713135, -0.21793945133686066, 0.2864876985549927, -0.5388820171356201, 0.6137294173240662, 0.6584721803665161, -0.07916577160358429, 0.45943447947502136, 0.2721293270587921, -0.9811862111091614, -0.6216297745704651, 0.4611690640449524, -0.5766674280166626, -0.04226680472493172, 0.6546404361724854, -0.3128946125507355, -0.2751408815383911, 0.7190340757369995, 0.09421664476394653, -1.0739943981170654, -0.5854063630104065, 0.08442245423793793, -0.9028025269508362, 0.016282854601740837, 0.018609672784805298, -0.04514182358980179, -1.2951829433441162, 0.018960468471050262, 0.04396475851535797, 0.41323918104171753, -0.6224769949913025, 0.7215188145637512, 0.07653217762708664, -1.1808876991271973, 0.11495373398065567, 0.4577564597129822, 0.002919446676969528, 0.15341930091381073, 0.6811392903327942, 0.4337155520915985, -0.20283369719982147, 0.5984244346618652, -0.10219775140285492, 0.20977479219436646, -0.5157822370529175, -0.3354906439781189, 1.1359586715698242, -0.4269436299800873, 0.18816831707954407, 1.0596593618392944, 0.013713440857827663, -1.5045032501220703, -0.056016165763139725, -0.6322264075279236, -0.39148518443107605, -0.2505797743797302, 0.32472798228263855, 0.05804084986448288, -0.14270909130573273, 0.5262713432312012, -0.14930957555770874, 0.4032299518585205, 0.237538143992424, -0.49512919783592224, 0.5104169845581055, -0.33944323658943176, -0.6223427653312683, 0.7251073718070984, 0.6561540365219116, -1.110232949256897, -0.9720950722694397, -1.0573192834854126, -0.2944290339946747, 0.2814713716506958, 0.15866635739803314, -0.34194710850715637, -0.6351193189620972, 0.9047960638999939, 0.3705731928348541, 0.4122828245162964, 0.15892791748046875, 0.24658596515655518, -0.479137122631073, 0.4242987632751465, 0.26111727952957153, -0.5363276600837708, -0.22148175537586212, 1.1049060821533203, 1.1213874816894531, -0.8678579330444336, -0.07889815419912338, -0.06478353589773178, -0.5481216311454773, 1.0119047164916992, 0.20195314288139343, -0.07566450536251068, 0.856196939945221, -0.29376357793807983, -0.052513036876916885, 0.2318420261144638, -1.3710603713989258, -0.4391000270843506, 0.6003013849258423, 1.2026809453964233, 0.7679168581962585, 0.7527909278869629, 0.1829477846622467, 0.6258020997047424, 0.15228582918643951, 0.041156310588121414, 0.27500051259994507, 0.5914228558540344, -0.218968003988266, -0.2822432816028595, 0.1543043553829193, 0.2468757927417755, -0.7376639246940613, -0.2682206630706787, 0.24302923679351807, 0.43293625116348267, 0.3367116451263428, 0.5950870513916016, 0.8002484440803528, 0.18584606051445007, 0.5494498014450073, 0.31584060192108154, 0.8228311538696289, -0.6838836669921875, -0.34200194478034973, -0.2263997495174408, -0.3641500771045685, -0.2618650794029236, -0.7775235176086426, -0.6192178130149841, -0.3687407374382019, -0.18721430003643036, 0.3796869218349457, -0.37559670209884644, 0.5539772510528564, 1.155709147453308, 0.59303879737854, 0.9684887528419495, -0.039338357746601105, -0.5933242440223694, -0.4796786308288574, -1.150632619857788, 0.15926170349121094, -0.4430863857269287, -0.3882921636104584, -0.28991857171058655, -0.028361186385154724, -0.1260215789079666]}, "authors": [{"authorId": "153733568", "name": "Jonas Pfeiffer"}, {"authorId": "2884561", "name": "Sebastian Ruder"}, {"authorId": "1747849", "name": "Ivan Vulic"}, {"authorId": "3381663", "name": "E. Ponti"}], "references": [{"paperId": "024a25b2445ecb3a181c5e2f39fbf8b73a4c1a6f", "title": "Emergent Modularity in Pre-trained Transformers"}, {"paperId": "b9d77cd9be54a228f811b1ac6212a7041792f217", "title": "AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning"}, {"paperId": "71ba5f845bd22d42003675b7cea970ca9e590bcc", "title": "Editing Models with Task Arithmetic"}, {"paperId": "656b5cb17d28c80fefb8c2f21e8f1f79b6dcd0dc", "title": "ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning"}, {"paperId": "7d42d1a27129c51e618e5127132c32f35260b4c4", "title": "Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for New Speakers"}, {"paperId": "5382f5b97153225a5051fdc1cf0d69476ed8aeec", "title": "Data-Efficient Cross-Lingual Transfer with Language-Specific Subnetworks"}, {"paperId": "f6709982f826718f335f92f7d50f53c759963d80", "title": "Residual Adapters for Few-Shot Text-to-Speech Speaker Adaptation"}, {"paperId": "9953b1c969399026fa5050522752ff7d70710ccd", "title": "Hierarchical3D Adapters for Long Video-to-text Summarization"}, {"paperId": "f4032272986e2a686019ef468bcba2fc33d3aba8", "title": "Exploring Efficient-Tuning Methods in Self-Supervised Speech Models"}, {"paperId": "35119bcbdc37f3c049f50851e2d1e760a2a52703", "title": "Meta-DMoE: Adapting to Domain Shift by Meta-Distillation from Mixture-of-Experts"}, {"paperId": "8f61a198bf9f97e445267f8b48d459e4bac9a14c", "title": "A Multi-Agent Framework for the Asynchronous and Collaborative Extension of Multitask ML Systems"}, {"paperId": "9f7914c2577626868e6a69f2c9605b38f941055b", "title": "StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation"}, {"paperId": "a9e20180153f6c139a4b6f2791b535fa6ffc3959", "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries"}, {"paperId": "ca086f4c09cf8de705830ac2b70951737fab93ca", "title": "A Review of Sparse Expert Models in Deep Learning"}, {"paperId": "8b3a67c7e5289eed160d2acfd04d71cfb552c67d", "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models"}, {"paperId": "ecc6d11de4afa98687fb99bdd04b2fcda72805bd", "title": "UFO: Unified Feature Optimization"}, {"paperId": "e19b54ad4c1c8af045069e9cac350ffc2ce60e1a", "title": "No Language Left Behind: Scaling Human-Centered Machine Translation"}, {"paperId": "26c309dd934331a024419625af46df07065b0ef7", "title": "Bottleneck Low-rank Transformers for Low-resource Spoken Language Understanding"}, {"paperId": "1b407ec19693bdd8fe68453cd1ecdf7e9dfdc6a9", "title": "ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition"}, {"paperId": "ac165b9651e5ea83a880f3d6169f2b9f3527114d", "title": "DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised Learning and Its Application to Children's ASR"}, {"paperId": "9b4d0b3e1af3c6dc3f99b27dab89bf884c7a5997", "title": "Is a Modular Architecture Enough?"}, {"paperId": "ffd8f81ed69ddfef6cddc3e8d0eae78f7b13435a", "title": "Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation"}, {"paperId": "d66dcad5d9f0564a96c035e0f6c64fe046738885", "title": "Discovering Language-neutral Sub-networks in Multilingual Language Models"}, {"paperId": "c9866e126cb70229e2017087f84b5db446f25a20", "title": "An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems"}, {"paperId": "6aeb1469188354fbf572f6039822dde06b8e65bc", "title": "Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer"}, {"paperId": "55a250868627de2d202d06e7cb3f6cbcd3a66f88", "title": "ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts"}, {"paperId": "eb4d54651c4f610749caf2bf401af3ce28ddc439", "title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"}, {"paperId": "c5b71d43165551170b01f74f6f9acc9ee923f3fe", "title": "Contextual Adapters for Personalized Speech Recognition in Neural Transducers"}, {"paperId": "7ae40d924c7224757e860632530ec5ccb02d209c", "title": "muNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-tuning Multitask Systems"}, {"paperId": "a97a7c6040b030157efc239db9cd79ac738badac", "title": "Multilingual Machine Translation with Hyper-Adapters"}, {"paperId": "9e147db7571494a2a3f9492444aeb8f005379416", "title": "Phylogeny-Inspired Adaptation of Multilingual Models to New Languages"}, {"paperId": "c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6", "title": "Lifting the Curse of Multilinguality by Pre-training Modular Transformers"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "4387ae27cadc05aa708dc293cfadd1ab41079ea2", "title": "Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "51b950bfcaba4bad321e7342b32833d42f42c914", "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners"}, {"paperId": "ffd8486787d4f838dbe5601f0cfe67fccf83fefc", "title": "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning"}, {"paperId": "b742ec2459f3984e2ea2f9a1eebf6129aa4b1cfc", "title": "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval"}, {"paperId": "52241619d157dd925160d35b75b6cb917fcffc27", "title": "Using Adapters to Overcome Catastrophic Forgetting in End-to-End Automatic Speech Recognition"}, {"paperId": "16d983ecf01908539dca18f46aa481c38d2a7a23", "title": "A Scalable Model Specialization Framework for Training and Inference using Submodels and its Application to Speech Model Personalization"}, {"paperId": "ebdd22411e27a05191d736f7f04be2d56fd77baa", "title": "Clustering units in neural networks: upstream vs downstream information"}, {"paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84", "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"}, {"paperId": "b879450f50a6113f44a5baf0bcd5b4331eeb7bbc", "title": "Conditional Prompt Learning for Vision-Language Models"}, {"paperId": "4e5b03a8345bf82a7e0b907e40a6d5eb7b02f74a", "title": "SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural Language Understanding"}, {"paperId": "318e22ccebf0e45816a8397ace680b1c9542f46e", "title": "HyperPrompt: Prompt-based Task-Conditioning of Transformers"}, {"paperId": "0d56e3d69c9b3112d77187f96fabcfbdf5303971", "title": "Combining Modular Skills in Multitask Learning"}, {"paperId": "5fca4d8d4893dcaeb99d874ec4b2bc5171b358f4", "title": "Towards Better Meta-Initialization with Task Augmentation for Kindergarten-Aged Speech Recognition"}, {"paperId": "bbc57e1b3cf90e09b64377f13de455793bc81ad5", "title": "Mixture-of-Experts with Expert Choice Routing"}, {"paperId": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3", "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models"}, {"paperId": "79d400814b67bc5a1749a02b2e2916bcc43ceab6", "title": "Delving Deeper into Cross-lingual Visual Question Answering"}, {"paperId": "f6919b54a4f06367947f0cf58cda54cdd08cd5f2", "title": "Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition"}, {"paperId": "c2536182c010c41941e8a031071a1880c34cec60", "title": "Unified Scaling Laws for Routed Language Models"}, {"paperId": "d0336e5be72e97e0493b1ba77ef8ec3c349d496a", "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages"}, {"paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170", "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"}, {"paperId": "5b0cfef3ebb8f709a4cf2e4718e8440cc5890bab", "title": "Efficient Hierarchical Domain Adaptation for Pretrained Language Models"}, {"paperId": "55a19318cc93714802c7ac59e07651789749b20c", "title": "VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "4ecb8bc1d3da10745a47895e109009c5eac81ce3", "title": "Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition"}, {"paperId": "40b4d98588719407fb72a014ab79e4145695654b", "title": "Quantifying Adaptability in Pre-trained Language Models with 500 Tasks"}, {"paperId": "b7f73c9675ae426244fe21b04bd933882635786d", "title": "Speechmoe2: Mixture-of-Experts Model with Improved Routing"}, {"paperId": "06b20a1c6883464fcb2855adc146874fe7937c41", "title": "Merging Models with Fisher-Weighted Averaging"}, {"paperId": "d9cdf21e73519edc593bdf1a00fcd778764b13f6", "title": "Training Neural Networks with Fixed Sparse Masks"}, {"paperId": "19dbb57ad106137553bff4282149ac2800b5c176", "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"}, {"paperId": "fcb226955cec6ae161f955742fa5298b5e8e12d2", "title": "Continual Learning via Local Module Composition"}, {"paperId": "007088458d6d508e8a1ffc4dce24750461453d61", "title": "Unsupervised Domain Adaptation with Adapter"}, {"paperId": "99b12d0df2b93e800207a5e4618a353912f3dff8", "title": "Multilingual Unsupervised Neural Machine Translation with Denoising Adapters"}, {"paperId": "51d62830c1112ea7443398990b850a988ed7c86c", "title": "Multilingual Domain Adaptation for NMT: Decoupling Language and Domain Information with Adapters"}, {"paperId": "547284c6a9c074d7b5cee4049b59fb52ad93df41", "title": "Tricks for Training Sparse Translation Models"}, {"paperId": "c28b7dfe341f1e13a5a98efbce7946ef795cf9b8", "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "fc58779940abb92166b73f47867763a07368c739", "title": "Composable Sparse Fine-Tuning for Cross-Lingual Transfer"}, {"paperId": "8ccaf0c0fbd5e4079f36fa720cf23890be10dd66", "title": "Dynamic Inference with Neural Interpreters"}, {"paperId": "c04067f03fba2df0c14ea51a170f213eb2983708", "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "0e23cc159fd2fb34550600d60dd9148c93636183", "title": "Taming Sparsely Activated Transformer with Stochastic Experts"}, {"paperId": "eea16dfc29f0521dd547e67a84af4ff95a9c5529", "title": "Visually Grounded Reasoning across Languages and Cultures"}, {"paperId": "8ae292cbd9144acbf4b42b7ead82b079faf33192", "title": "Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference"}, {"paperId": "dac5711aea72aba9c607f049e295da5f8abb3c93", "title": "Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and Accented Speech"}, {"paperId": "071440ccd1084d20d345fafd0bcaa5993f71fb04", "title": "xGQA: Cross-Lingual Visual Question Answering"}, {"paperId": "130ab5c480860e330b65280a3410f17bb2d50fe1", "title": "Sustainable Modular Debiasing of Language Models"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "9ba50f992ccd92f428503ea6246157260a26cd77", "title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?"}, {"paperId": "917c63f2186119166b3379f5d2816bb1a2f39b09", "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling"}, {"paperId": "cdcffb2f1678d7252bfd9b902d3cd676a5217005", "title": "Robust Transfer Learning with Pretrained Language Models through Adapters"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "20abd40a0014a1469b11d7f71debd2d9fa3dd481", "title": "Brain-like functional specialization emerges spontaneously in deep neural networks"}, {"paperId": "b2c70c4d23c98dd4e77234fe0720595d3d565a12", "title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "title": "Scaling Vision with Sparse Mixture of Experts"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "bb3425318de7eed5641cda147d61c9a057b9d054", "title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks"}, {"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "title": "Hash Layers For Large Sparse Models"}, {"paperId": "efbe9f591090018f78b42c84613c8afda9292fdb", "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration"}, {"paperId": "36ffa5b1f643f59ccf8396cff9865e5474c8dae7", "title": "DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning"}, {"paperId": "448af0627240e46df757e7b9c640ee30507c18e9", "title": "On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation"}, {"paperId": "b21c1d86360eb97eed57e82273f9c8f1f12acd85", "title": "Mixture of Informed Experts for Multilingual Speech Recognition"}, {"paperId": "92f2000f29f1aed7f2cc4e3cb5f783e079ef553f", "title": "MergeDistill: Merging Language Models using Pre-trained Distillation"}, {"paperId": "eacb5dc57a167aeda3b23c28abfc2b51095f1b7c", "title": "Lightweight Adapter Tuning for Multilingual Speech Translation"}, {"paperId": "916f8ae4dad6ee8a23e50c3fb0729a31aed17f2d", "title": "Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks"}, {"paperId": "7777a31341fa4bfbd25b96a5320681af8dccf3af", "title": "Exploring Sparse Expert Models and Beyond"}, {"paperId": "6ff5ea40e0d1be8c71e5b675db64fe730018db03", "title": "Learning Language Specific Sub-network for Multilingual Machine Translation"}, {"paperId": "48f5d744dfe96c64891a4b3cb396f8b957c93fc3", "title": "Exploiting Adapters for Cross-Lingual Low-Resource Speech Recognition"}, {"paperId": "e8bf6e0f1a1ea7f7d145cbe8782066a2d977d7e2", "title": "Finding Sparse Structures for Domain Specific Neural Machine Translation"}, {"paperId": "0adec918885dff698acf359988ed79a543157f80", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"}, {"paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c", "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "7fa273f450251523e6b7fcc2eb3fdbdfd4a30493", "title": "CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"}, {"paperId": "2b9762e91305986ac8a2d624d0a69521304405f3", "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"}, {"paperId": "8a1c54f6e2c5f1453fddb9f15e769a099286f677", "title": "Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: A Survey"}, {"paperId": "f2885c6a25756cf81aa23b41bc62696a5be5c94d", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "a8f97a65f1cb6106b83d3c21a1dad5fa005aee43", "title": "RotoGrad: Gradient Homogenization in Multitask Learning"}, {"paperId": "57fb3190887d837fca47a0ca176abc782b1f42d3", "title": "Neural Production Systems"}, {"paperId": "ce9ca56036307217ea565644d3d3bd74b879e045", "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP"}, {"paperId": "8f566001453bc6be0a935bf69ffd90d9db3af32b", "title": "Towards Causal Representation Learning"}, {"paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"paperId": "6f7d740699b55a44e9d57d85906d576315e45a5f", "title": "Revisiting Multi-Domain Machine Translation"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e", "title": "WARP: Word-level Adversarial ReProgramming"}, {"paperId": "13bcfb944779165983aaef22cec8a3bbd3e98e62", "title": "UNKs Everywhere: Adapting Multilingual Language Models to New Scripts"}, {"paperId": "ce2e6a287e2b1898665035b1a734d3a559c0933c", "title": "Verb Knowledge Injection for Multilingual Event Processing"}, {"paperId": "e54ffc76d805c48660bb0fd20019ca82ac94ba0d", "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "d642868ce4325ebf3026c0aa0c497a079f112a8d", "title": "On the Binding Problem in Artificial Neural Networks"}, {"paperId": "8b31fef217004560b8c2517c0f6fdc1c3cf55112", "title": "Language Adapters for Zero Shot Neural Machine Translation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "bdeec55f95fd6b73e3e4635459b14c7248543efb", "title": "AdapterDrop: On the Efficiency of Adapters in Transformers"}, {"paperId": "687b13c44f849d23c2496996b5da83e706094db9", "title": "Beyond English-Centric Multilingual Machine Translation"}, {"paperId": "78d9f4a6bc0f53d0594a752f0934a3689b4460d8", "title": "Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models"}, {"paperId": "553028f7f7c850371379c621e40d7d00e75303a6", "title": "On Negative Interference in Multilingual Language Models"}, {"paperId": "649c758b0e59ddedaae37a3757e8eabdba664e5a", "title": "Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks"}, {"paperId": "55c4a747855c74210919c45f7899e1f79e4c97f5", "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data"}, {"paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9", "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"paperId": "74f23063ca77f5b1caa3770a5957ae5fc565843e", "title": "Multi-Task Learning with Deep Neural Networks: A Survey"}, {"paperId": "5baa3e00d66bc42db7e3908f0b70875cff9d0193", "title": "What is being transferred in transfer learning?"}, {"paperId": "389036b1366b64579725457993c1f63a4f3370ba", "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"}, {"paperId": "27cc280dfb808dfae9a9f28691d118f4df837222", "title": "TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning"}, {"paperId": "063f8b1ecf2394ca776ac61869734de9c1953808", "title": "AdapterHub: A Framework for Adapting Transformers"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "74e85e72392208b9a1d135dcb00fe28c61ddb9c7", "title": "Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules"}, {"paperId": "3ecfe6f3daafc1b843879ba89e4a06260b120312", "title": "Supermasks in Superposition"}, {"paperId": "5156381d63bb3e873533b08f203cb56c2d79b6c9", "title": "Object-Centric Learning with Slot Attention"}, {"paperId": "49a049dc85e2380dde80501a984878341dd8efdf", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"}, {"paperId": "2e1107db4731714230b25efa9f7a8787e6e115d8", "title": "Fastpitch: Parallel Text-to-Speech with Pitch Prediction"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8b8c29c0cbb6cbae26b930840396596dd5806f33", "title": "Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "d97e7561fa7710213ccd4f8128044ea6849be377", "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning"}, {"paperId": "91ac65431b2dc46919e1673fde67671c29446812", "title": "When BERT Plays the Lottery, All Tickets Are Winning"}, {"paperId": "26299d5fdc5137291dc6a091573b3d18aba1d1c2", "title": "MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"}, {"paperId": "3b233bdb697cc43effa1eb6d2868ff14efbbab7a", "title": "UDapter: Language Adaptation for Truly Universal Dependency Parsing"}, {"paperId": "7fb301ea25f02dc7f4f7ee1360137503ee942c8c", "title": "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models"}, {"paperId": "01448019e6807b3d11a68ef281a568cafb09ef05", "title": "Under the Hood of Neural Networks: Characterizing Learned Representations by Functional Neuron Populations and Network Ablations"}, {"paperId": "ba4a34680e09e77984624c95f5245d91b54373f6", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"}, {"paperId": "4f03e69963b9649950ba29ae864a0de8c14f1f86", "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"}, {"paperId": "1849955aa9e439d99082bcf038550363e9622e35", "title": "Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages"}, {"paperId": "495da6f19baa09c6db3697d839e10432cdc25934", "title": "Multilingual Denoising Pre-training for Neural Machine Translation"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "3f06d02513a2763e472d2b5d5db08e9061081b9e", "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis"}, {"paperId": "43a3caf942684aa7a354a12c0dc4bfbba50c563d", "title": "AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning"}, {"paperId": "3e4b4d1c39a6a92d014398a8c54bf3cce58fa4de", "title": "Learning Sparse Sharing Architectures for Multiple Tasks"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "49e5b09480189fc9b2316a54f9d1e55cf0097c8b", "title": "Lightweight and Efficient End-To-End Speech Recognition Using Low-Rank Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "b6b8e979f2bf0ea805a62d517db878fc573a8941", "title": "Towards modular and programmable architecture search"}, {"paperId": "207033829813aadc2f2dca8f93279352d39de759", "title": "Learning Neural Causal Models from Unknown Interventions"}, {"paperId": "67a9dde04f367efc903b6d06097df9bdd9887ae7", "title": "Recurrent Independent Mechanisms"}, {"paperId": "48530f3d6425f2f150f07ccdd61ba951951a0a7d", "title": "Simple, Scalable Adaptation for Neural Machine Translation"}, {"paperId": "9e1463599a4cd290f563dcd0328f3aa79e188f2a", "title": "Deep Elastic Networks With Model Selection for Multi-Task Learning"}, {"paperId": "5cf3e46e6d427a87726c18f22def612519176938", "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model"}, {"paperId": "914e4c95e3fbaa9d40a35efc5a4740a8cf10dc59", "title": "Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels"}, {"paperId": "f3fbcfbb396f6c0391674c8637a373b729e5e531", "title": "Compositionality Decomposed: How do Neural Networks Generalise?"}, {"paperId": "2d1316eb8935785f7fa317db14d4b89ba7c54fd1", "title": "Feature Partitioning for Efficient Multi-Task Architectures"}, {"paperId": "c2c8482c713b94073f3d59895b373db4398ddfbb", "title": "Language as an Abstraction for Hierarchical Deep Reinforcement Learning"}, {"paperId": "387e0b95d56e9ecec60a1037ddf7cc57b2851835", "title": "Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "191461705e9961a3b07273c19d89eb214819002c", "title": "Continual learning with hypernetworks"}, {"paperId": "41dd3dd3b9e39c91eb975d91d2d019cb9024015b", "title": "Recursive Routing Networks: Learning to Compose Modules for Language Understanding"}, {"paperId": "157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3", "title": "Transfer Learning in Natural Language Processing"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "443494580bfe0531ba91acf500ac95668e7a9502", "title": "Sparse Transfer Learning via Winning Lottery Tickets"}, {"paperId": "9c10d96b7f31cc488d3f89c2f5eb5f0d41eb0b43", "title": "Budget-Aware Adapters for Multi-Domain Learning"}, {"paperId": "f7c410ab241bc972cda2f47993124ea8483003b6", "title": "Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "726320cdbd04804ffa8f3a78c095bd1b55a2a695", "title": "Similarity of Neural Network Representations Revisited"}, {"paperId": "433fcb584902e716ee25a44175e380267616b54e", "title": "Learning Compositional Neural Programs with Recursive Tree Search and Planning"}, {"paperId": "be928f91385999fa90d1e2fe06058f9dbcfd7186", "title": "Routing Networks and the Challenges of Modular and Compositional Computation"}, {"paperId": "23f425d6cb57938ceaa98fce8133a6924c2f953b", "title": "Three scenarios for continual learning"}, {"paperId": "271ee5a35a1f36e9f7ce64ac6966318e4cb63b7e", "title": "Branched Multi-Task Networks: Deciding what layers to share"}, {"paperId": "575fffa523c150e4e1f899a0d4db322a099afea9", "title": "Many Task Learning With Task Routing"}, {"paperId": "2b627185499791048681e8d24190c31dea928f16", "title": "Uniform convergence may be unable to explain generalization in deep learning"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"paperId": "492ba3ad3f0cb85f0636bc275fecd7e7960709da", "title": "A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms"}, {"paperId": "648c33300ae7597f24e73600353aef31df745086", "title": "Task representations in neural networks trained to perform many cognitive tasks"}, {"paperId": "c3d8d98847bd33fa48bc6448316f78ed7f131afe", "title": "A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks"}, {"paperId": "0b50b9e103e19d87c2d30ed0d157d8379320ce6f", "title": "Modular Networks: Learning to Decompose Neural Computation"}, {"paperId": "24389a037dd045be2fe517925c575b765fbbde62", "title": "Neural Modular Control for Embodied Question Answering"}, {"paperId": "619adbb72b6e6b938851db119cb757751ffcaf76", "title": "Interpreting Layered Neural Networks via Hierarchical Modular Representation"}, {"paperId": "f91ff6c0aeba78f94f02b761446d5d911e6ab390", "title": "On Self Modulation for Generative Adversarial Networks"}, {"paperId": "60b373c65ad0032dda6c200f77147993ddca6a73", "title": "Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation"}, {"paperId": "cd51e3d117c446af06098e05f5bb8b156961323f", "title": "Multi-Source Domain Adaptation with Mixture of Experts"}, {"paperId": "a8a863e85a95919773868204d672f1260e0058ce", "title": "Contextual Parameter Generation for Universal Neural Machine Translation"}, {"paperId": "af8a8dcb74561d52d904f7bc4afcc747e079b702", "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts"}, {"paperId": "1766648967f6206a944a4bd18bbbd92a74c164bd", "title": "Automatically Composing Representation Transformations as a Means for Generalization"}, {"paperId": "0e93ee6176197f758dd5e8e9cff8b9610274c402", "title": "Modular meta-learning"}, {"paperId": "477c4f873379fd551182d05549f1c8e27d5c5bcf", "title": "Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models"}, {"paperId": "39b7007e6f3dd0744833f292f07ed77973503bfd", "title": "Data-Efficient Hierarchical Reinforcement Learning"}, {"paperId": "4081de7e0f94e7e0d7b645c298d7768698d05774", "title": "Efficient Parametrization of Multi-domain Deep Neural Networks"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "2c90d366126a3ccd3c43e47891730650003059da", "title": "Essentially No Barriers in Neural Network Energy Landscape"}, {"paperId": "f6195d8dc6aad8231e97b563246f2585842bc68b", "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs"}, {"paperId": "d55d1d035e91220335edff0fe8f5d249d8c4a00b", "title": "Measuring the Intrinsic Dimension of Objective Landscapes"}, {"paperId": "700db9f849e9a0a6fec8a28a1e0ea00b4ed4558f", "title": "Multinomial Adversarial Networks for Multi-Domain Text Classification"}, {"paperId": "8c8d0031b24937d8a8ec7a4c5ab5fda4f4797803", "title": "NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural Discriminative Dimensionality Reduction"}, {"paperId": "d5bb3faa48b83469da1a01ef267886e71f4a931a", "title": "Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "d93b0b37ee0e87a0e096ef803667b0798f465528", "title": "Learning Independent Causal Mechanisms"}, {"paperId": "47bc048efb90e7b8bae5c1fcc979a78b65763fe9", "title": "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning"}, {"paperId": "fc9d5be5e3f14b4c4b145b6c4bd96a9182f39fd2", "title": "Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning"}, {"paperId": "856fe866bcce5e7a540655bea6ecc7406bdcfcba", "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"}, {"paperId": "c4f3375dab1886f37f542d998e61d8c30a927682", "title": "Beyond Shared Hierarchies: Deep Multitask Learning through Soft Layer Ordering"}, {"paperId": "7cfa5c97164129ce3630511f639040d28db1d4b7", "title": "FiLM: Visual Reasoning with a General Conditioning Layer"}, {"paperId": "feeb3a2aa35a02e06546d05d94bac9a2123fc0c8", "title": "Modulating early visual processing by language"}, {"paperId": "09da8d56351dff51389d764cbc94bc733ee3129c", "title": "Adversarial Adaptation of Synthetic or Stale Data"}, {"paperId": "6d431f835c06afdea45dff6b24486bf301ebdef0", "title": "An Overview of Multi-Task Learning in Deep Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "ebf7b916cf84f9e43d5948395a48e18688c5464d", "title": "Latent Multi-Task Architecture Learning"}, {"paperId": "d89ee98810039d2061ed42ee8026da49c503d16b", "title": "Learning multiple visual domains with residual adapters"}, {"paperId": "a396a6febdacb84340d139096455e67049ac1e22", "title": "Learning to Reason: End-to-End Module Networks for Visual Question Answering"}, {"paperId": "321f1877bc570ff9b318e909cefb7c27138458df", "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks"}, {"paperId": "1423037dd56f85453cd4257861821aeeb7478bc1", "title": "Universal representations: The missing link between faces, text, planktons, and cat breeds"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810", "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"}, {"paperId": "66b8d34477cf1736f91fd22b27e37ce0b703c86e", "title": "Expert Gate: Lifelong Learning with a Network of Experts"}, {"paperId": "00a967cb2d18e1394226ad37930524a31351f6cf", "title": "Fully-Adaptive Feature Sharing in Multi-Task Networks with Applications in Person Attribute Classification"}, {"paperId": "3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7", "title": "Modular Multitask Reinforcement Learning with Policy Sketches"}, {"paperId": "3deecaee4ec1a37de3cb10420eaabff067669e17", "title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning"}, {"paperId": "0f94591cc05e6f75c21749d507ef58d204f63b7d", "title": "Topology and Geometry of Half-Rectified Network Optimization"}, {"paperId": "3db8730c203f88d7f08a6a99e8c02a077dc9b011", "title": "Pruning Convolutional Neural Networks for Resource Efficient Inference"}, {"paperId": "29e944711a354c396fad71936f536e83025b6ce0", "title": "Categorical Reparameterization with Gumbel-Softmax"}, {"paperId": "515a21e90117941150923e559729c59f5fdade1c", "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"}, {"paperId": "e2bd18c1039f27675bd64014117db648d969452e", "title": "Learning and Transfer of Modulated Locomotor Controllers"}, {"paperId": "784ee73d5363c711118f784428d1ab89f019daa5", "title": "Hybrid computing using a neural network with dynamic external memory"}, {"paperId": "8fab7d7dfd233fd5d19bc2641b4c1ca74fc7bc6a", "title": "Learning modular neural network policies for multi-task and multi-robot transfer"}, {"paperId": "15b26d8cb35d7e795c8832fe08794224ee1e9f84", "title": "The Option-Critic Architecture"}, {"paperId": "01cb4071a0a43aeef63e5d568ad5afe1fb8b2411", "title": "Domain Separation Networks"}, {"paperId": "03ad06583c9721855ccd82c3d969a01360218d86", "title": "Deep multi-task learning with low level tasks supervised at lower layers"}, {"paperId": "950619635df80e87c6f25b486cc5eaad4d71d0b0", "title": "DSD: Dense-Sparse-Dense Training for Deep Neural Networks"}, {"paperId": "4423357dd21cc59662c6fabaf9839b15ef0fb8a8", "title": "Learning feed-forward one-shot learners"}, {"paperId": "468a80bcd4ff9b3f47beb9145ff81140777bb3f3", "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach"}, {"paperId": "d37620e6f8fe678a43e12930743281cd8cca6a66", "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation"}, {"paperId": "2976605dc3b73377696537291d45f09f1ab1fbf5", "title": "Cross-Stitch Networks for Multi-task Learning"}, {"paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80", "title": "Identity Mappings in Deep Residual Networks"}, {"paperId": "edac2b92ce6ad61eda93e09650cf3f34ccc49785", "title": "Learning Hidden Unit Contributions for Unsupervised Acoustic Model Adaptation"}, {"paperId": "98ea4abc9bf0e30eb020db2075c9c8a039a848a3", "title": "Learning to Compose Neural Networks for Question Answering"}, {"paperId": "7d39283a0fce1c96f57eb20046d09bd95ccc56d7", "title": "Structured Pruning of Deep Convolutional Neural Networks"}, {"paperId": "b59d91e0699d4e1896a15bae13fd180bdaf77ea5", "title": "Neural Programmer-Interpreters"}, {"paperId": "fba71eefd060e30f3516fdd46df9a191cd0aaaf7", "title": "Conditional Computation in Neural Networks for faster models"}, {"paperId": "21c99706bb26e9012bfb4d8d48009a3d45af59b2", "title": "Neural Module Networks"}, {"paperId": "8bada64d0b04a66165b464a51c29f61e1f474f2a", "title": "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "1d5972b32a9b5a455a6eef389de5b7fca25771ad", "title": "Domain-Adversarial Training of Neural Networks"}, {"paperId": "f500b1a7df00f67c417673e0538d86abb8a333fa", "title": "Facial Landmark Detection by Deep Multi-task Learning"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "18c7fb55ff796db5c5a604e0ca44b6baaeb12239", "title": "Fastfood: Approximate Kernel Expansions in Loglinear Time"}, {"paperId": "44ddac48353ead135eef4096859956eaa31be2a5", "title": "Learning Factored Representations in a Deep Mixture of Experts"}, {"paperId": "a3cbb2a295dba31d9c9af77cc177fe538de94d5e", "title": "Unsupervised Domain Adaptation by Domain Invariant Projection"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "b8de958fead0d8a9619b55c7299df3257c624a96", "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "f1bb50162b731dfd41bdd3648ba5239579420ac0", "title": "On causal and anticausal learning"}, {"paperId": "5bd80cff92ca1cbab27caf3d92d96e5c39cc7cc8", "title": "The role of product architecture in the manufacturing firm"}, {"paperId": "808073f5802d627439ec981c1e7f361e67e624b9", "title": "The Indian Buffet Process: An Introduction and Review"}, {"paperId": "9e96d9172b7cb9c6ae02dc2bbfd55f44a4cf0b4d", "title": "Object-oriented analysis and design with applications, third edition"}, {"paperId": "261a056f8b21918e8616a429b2df6e1d5d33be41", "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks"}, {"paperId": "93a420b4c39909b5ffd78fc156737012099f8aa0", "title": "Spontaneous evolution of modularity and network motifs."}, {"paperId": "1b7d9505b760ab74adee9f37721bb6d127a92d0a", "title": "Design Rules: The Power of Modularity"}, {"paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d", "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"}, {"paperId": "4c96ca25d889251e20e33d01f24eec175301ab94", "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition"}, {"paperId": "2722b9e5ab8da95f03e578bb65879c452c105385", "title": "Catastrophic forgetting in connectionist networks"}, {"paperId": "161ffb54a3fdf0715b198bb57bd22f910242eb49", "title": "Multitask Learning"}, {"paperId": "5f66b5fe5dcb38858fc5cabe23f8681ec3941bbf", "title": "First draft of a report on the EDVAC"}, {"paperId": "f6d8a7fc2e2d53923832f9404376512068ca2a57", "title": "Hierarchical Mixtures of Experts and the EM Algorithm"}, {"paperId": "1678bd32846b1aded5b1e80a617170812e80f562", "title": "Feudal Reinforcement Learning"}, {"paperId": "af06d65387266a3fe35973cbf77593229049bb10", "title": "The Meta-Pi Network: Building Distributed Knowledge Representations for Robust Multisource Pattern Recognition"}, {"paperId": "17594df98c222217a11510dd454ba52a5a737378", "title": "On the computational power of neural nets"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": "c3972cbda528a8ae76386aa317f2ee686d10eb65", "title": "Task Decomposition Through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks"}, {"paperId": "902a9f6fd1b17f0d0bc9de54cccd4baa179a7845", "title": "Cortical connections and parallel processing: Structure and function"}, {"paperId": "92ebd4d0e7b8e039400bd01dd81daf07e13a613f", "title": "The Modularity of mind. An essay on faculty psychology"}, {"paperId": "dd5061631a4d11fa394f4421700ebf7e78dcbc59", "title": "Optimization by Simulated Annealing"}, {"paperId": "e2b9464a590ee3a9731c8fd498874ae4f37e92ed", "title": "BAD-X: Bilingual Adapters Improve Zero-Shot Cross-Lingual Transfer"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": "206d1f2f68614b9f97e6fa176d08af8b18930562", "title": "Language-Family Adapters for Multilingual Neural Machine Translation"}, {"paperId": "9201e46b2fe90c636d57b076020051953456473c", "title": "Multi-Head Adapter Routing for Data-Efficient Fine-Tuning"}, {"paperId": "2bfa15ba6099f62ed8f4067eb5dd62f707fbb412", "title": "BBTv2: Pure Black-Box Optimization Can Be Comparable to Gradient Descent for Few-Shot Learning"}, {"paperId": "38338207e9ee2591e67c926b7da2294318a0dec2", "title": "Models with Conditional Computation Learn Suboptimal Solutions"}, {"paperId": null, "title": "Deep end-toend causal inference"}, {"paperId": null, "title": "Parameterefficient conformers via sharing sparsely-gated experts for end-to-end speech recognition"}, {"paperId": null, "title": "Graphical clusterability and local specialization in deep neural networks"}, {"paperId": "f5dfed82b0c8747e41a1206f52a6d0ea3dce4a5c", "title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"}, {"paperId": "6adc9c231d874ea358554b8680a6aaba4bd6c963", "title": "MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer"}, {"paperId": "fb3dc58cb17c54b997e6301cbde7773f77427833", "title": "Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "Inductive Bias and Modular Design for Sample-Efficient Neural Language Learning"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Neural turing machines. CoRR, abs/1410"}, {"paperId": "bb6898d6041e97c4946661b3a3df0f82286a43b5", "title": "Verbnet: a broad-coverage, comprehensive verb lexicon"}, {"paperId": "ebf673d896e84d69a42d88f109870d6a674e0ac4", "title": "Global workspace theory of consciousness: toward a cognitive neuroscience of human experience."}, {"paperId": null, "title": "Natural selection and the origin of modules. In Werner Callebaut and Diego Rasskin-Gutman (eds.), Modularity: Understanding the Development and Evolution of Complex"}, {"paperId": null, "title": "Modularity: Understanding the Development and Evolution of Complex Natural Systems"}, {"paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614", "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"}, {"paperId": "1f1eaf19e38b541eec8a02f099e3090536a4c936", "title": "The Unified Medical Language System (UMLS): integrating biomedical terminology"}, {"paperId": "6f6beecacdf52771607f6cf8f05f15a4b58ff275", "title": "Temporal abstraction in reinforcement learning"}, {"paperId": "485a9be44200d4cd377aaec69db0a616e3be7be4", "title": "Object-oriented analysis and design with applications (2. ed.)"}, {"paperId": "c213af6582c0d518a6e8e14217611c733eeb1ef1", "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"}, {"paperId": null, "title": "Toward a theory of reinforcement-learning connectionist systems"}, {"paperId": null, "title": "Two problems with back propagation and other steepest descent learning procedures for networks"}, {"paperId": null, "title": "GPT understands, too. CoRR, abs/2103.10385, 2021b"}, {"paperId": null, "title": "Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}]}