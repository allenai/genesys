{"paperId": "3cc0f0b40589f4e0f971280cad48919048d22a41", "title": "GSVA: Generalized Segmentation via Multimodal Large Language Models", "abstract": "Generalized Referring Expression Segmentation (GRES) extends the scope of classic RES to refer to multiple objects in one expression or identify the empty targets absent in the image. GRES poses challenges in modeling the complex spatial relationships of the instances in the image and identifying non-existing referents. Multimodal Large Language Models (MLLMs) have recently shown tremendous progress in these complicated vision-language tasks. Connecting Large Language Models (LLMs) and vision models, MLLMs are proficient in understanding contexts with visual inputs. Among them, LISA, as a representative, adopts a special [SEG] token to prompt a segmentation mask decoder, e.g., SAM, to enable MLLMs in the RES task. However, existing solutions to GRES remain unsatisfactory since current segmentation MLLMs cannot correctly handle the cases where users might reference multiple subjects in a singular prompt or provide descriptions incongruent with any image target. In this paper, we propose Generalized Segmentation Vision Assistant (GSVA) to address this gap. Specifically, GSVA reuses the [SEG] token to prompt the segmentation model towards supporting multiple mask references simultaneously and innovatively learns to generate a [REJ] token to reject the null targets explicitly. Experiments validate GSVA's efficacy in resolving the GRES issue, marking a notable enhancement and setting a new record on the GRES benchmark gRefCOCO dataset. GSVA also proves effective across various classic referring segmentation and comprehension tasks.", "venue": "arXiv.org", "year": 2023, "citationCount": 9, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Generalized Segmentation Vision Assistant is proposed, which reuses the [SEG] token to prompt the segmentation model towards supporting multiple mask references simultaneously and innovatively learns to generate a [REJ] token to reject the null targets explicitly."}, "embedding": {"model": "specter_v2", "vector": [0.2653200924396515, 0.7756348848342896, -0.26866060495376587, -0.335091233253479, -0.8204756379127502, -0.0830482617020607, 0.3815915584564209, -0.07841282337903976, -0.5737940073013306, -0.7675272226333618, 0.5335860848426819, 0.18841488659381866, 0.44156384468078613, 0.31945985555648804, -0.21376582980155945, 0.2779686450958252, -0.8006806969642639, 0.19791993498802185, 0.22019100189208984, -0.3893757164478302, -0.14637552201747894, -0.4486686587333679, -0.9486657977104187, 0.38578808307647705, 0.18423721194267273, 0.1246972307562828, 0.5842597484588623, 0.9347019791603088, -0.12949277460575104, 0.286651611328125, 0.09236334264278412, -0.09098191559314728, 0.2295342981815338, -0.042125750333070755, -0.2705531716346741, 0.37998971343040466, 0.6755883097648621, -0.7224355340003967, -0.6230815052986145, 0.6756432056427002, -0.1473662108182907, 0.4343014061450958, 0.2471209019422531, -0.468330979347229, -0.423814982175827, 0.9280778765678406, 0.443026065826416, 0.5820347666740417, 0.2248390018939972, -0.4448442757129669, 1.2846698760986328, -1.5000497102737427, 0.48547863960266113, 1.8702360391616821, 0.1217462494969368, 0.43426379561424255, 0.10354402661323547, -0.1516602784395218, 0.945458173751831, -0.3014979660511017, -1.2370387315750122, -0.7151434421539307, -0.3399665057659149, 0.03574041277170181, 1.36809241771698, -0.05715671554207802, -0.4659787714481354, 0.46005967259407043, -0.2823495864868164, 1.7994463443756104, -0.0059317597188055515, -0.8389654159545898, -0.22661590576171875, 0.15944410860538483, 0.5754281282424927, 0.8796452879905701, -0.46121329069137573, 0.1357140988111496, -0.6776071190834045, 0.2118586003780365, 0.7446012496948242, -0.48177120089530945, -0.5051769614219666, -0.2507133185863495, -0.26436692476272583, 0.36765357851982117, 0.6528323888778687, 0.623590886592865, -0.035303544253110886, 0.2438889890909195, 0.329449325799942, -0.0057704756036400795, -0.11317601054906845, -0.0016759679419919848, 0.011694296263158321, 0.7749571800231934, -0.9441584348678589, 0.3094329833984375, 0.023817846551537514, 1.1992906332015991, -0.5649639368057251, 0.1317102611064911, -1.0236634016036987, 0.3800581395626068, 1.8680729866027832, -0.30316564440727234, 0.5695191621780396, -0.7669998407363892, 0.1940375715494156, -0.676060676574707, 0.7051845192909241, -0.6950241923332214, -0.48020631074905396, -0.4157518446445465, -0.20709261298179626, -1.2244683504104614, 0.12350917607545853, 0.39647915959358215, -0.9880686402320862, 0.9312863349914551, -0.3121556341648102, 0.09084223955869675, 0.14081208407878876, 0.6534623503684998, 0.9800567030906677, 0.39932024478912354, 0.9561538100242615, 0.07205778360366821, 1.0697461366653442, -0.6507292985916138, -0.48049238324165344, -1.0657328367233276, 0.6944484710693359, -0.030888648703694344, 0.466812402009964, -0.5056018233299255, -1.2462623119354248, -1.3879358768463135, -0.8544925451278687, -0.3759765028953552, -0.6009721755981445, 0.8577916026115417, 0.5996266007423401, 0.3651265799999237, -1.5809249877929688, 0.3854265809059143, -0.2673628628253937, -0.952830970287323, 0.5650153160095215, 0.2415466457605362, 0.44411706924438477, -0.2556656301021576, -0.8449612855911255, 0.017837202176451683, 0.10217923671007156, -0.18276065587997437, 0.36261770129203796, 0.08361508697271347, -1.6827614307403564, -0.08406306803226471, 0.6359362602233887, -0.32453808188438416, 1.1972957849502563, -0.07565589994192123, -0.7110686302185059, 1.19720458984375, -0.5239130854606628, 0.22094503045082092, 0.3722846210002899, -0.4393139183521271, -0.5611312389373779, 0.08217758685350418, 0.11044961959123611, 1.022078275680542, 0.29203200340270996, -0.8782716989517212, -0.05516256392002106, -0.07399161905050278, -0.36446160078048706, 0.08911857008934021, 0.3080156445503235, 1.0777912139892578, -0.4785209000110626, -0.3508598804473877, 0.7244178652763367, 0.9342010617256165, -0.17970269918441772, -0.08827980607748032, -0.608276903629303, -0.760925829410553, 0.678946852684021, -0.3286059498786926, 0.9390256404876709, -1.058162808418274, -0.6751766800880432, -0.25846996903419495, -0.12578018009662628, -0.19093824923038483, -1.2814960479736328, 0.13651253283023834, -0.1780908852815628, 0.6650683879852295, -0.21333980560302734, -1.301291823387146, 0.05509558320045471, -0.3927159309387207, -0.581089437007904, -0.17601194977760315, -0.14357373118400574, 1.6339811086654663, -0.8977421522140503, -0.1769588142633438, -0.08979149162769318, -0.11186695843935013, -0.21187537908554077, 1.2711775302886963, -0.309463232755661, 0.5126534104347229, -0.41247570514678955, 0.11182666569948196, -0.10457438975572586, -0.5038806796073914, 0.09366726130247116, -0.6432625651359558, 0.019490906968712807, -0.038080453872680664, -0.22972793877124786, 1.304011583328247, 0.24368366599082947, 0.4306904673576355, -0.18680989742279053, -0.5583982467651367, 0.2694915533065796, 0.32177603244781494, -0.490652859210968, -0.5597871541976929, 0.694574773311615, 0.3004285991191864, -0.6548471450805664, -0.11902281641960144, 1.0009536743164062, 1.1589014530181885, -0.71278977394104, 0.04521157965064049, 0.20414388179779053, -0.17109215259552002, 0.36606597900390625, 0.6929073929786682, 0.23791390657424927, 0.693385124206543, 0.7819673418998718, 0.09471262991428375, 0.5017111897468567, -0.7652595639228821, -0.3263070285320282, 1.0884954929351807, 0.5003340840339661, 1.2293965816497803, 0.11597011983394623, -0.4359916150569916, -0.23409171402454376, -0.1347176730632782, 0.6457911133766174, 1.6002442836761475, 0.5231397151947021, -0.05626778304576874, -1.0436885356903076, -0.25479888916015625, -0.8076183795928955, 0.11969512701034546, -0.22140677273273468, 0.01176403183490038, -0.6584827899932861, -0.5483464598655701, 0.5660588145256042, 0.7735952138900757, 1.081872820854187, -0.8810656666755676, -0.480864018201828, -0.19006916880607605, 0.13626030087471008, -1.1273515224456787, -0.8625926971435547, -0.415886789560318, -0.4096071422100067, -0.20748953521251678, -0.3132919371128082, -0.5837451219558716, 0.5604651570320129, -0.504980206489563, 0.7515681385993958, -0.4029746651649475, -0.6588506698608398, 1.0084333419799805, 0.46451279520988464, -0.3759639263153076, -0.9670436978340149, -0.03933079168200493, -0.357601135969162, -0.16867655515670776, 0.5424031615257263, 1.063973307609558, -0.013791050761938095, 0.38154593110084534, -0.4735651910305023, 0.3917906880378723, 0.31156960129737854, -0.33502402901649475, 0.9279506802558899, -1.1190307140350342, 0.4786683917045593, -0.9625334143638611, 1.0841529369354248, 0.2554873526096344, -0.4779081344604492, 0.6194297075271606, -0.03590739890933037, -0.9566754698753357, 0.045367009937763214, -0.5673239827156067, -0.2398572564125061, -0.6120169758796692, 0.6834878921508789, -0.18324817717075348, -0.9133279323577881, 0.2896983027458191, 0.14011316001415253, 0.05208227038383484, 0.10547290742397308, 0.5853623747825623, 0.2998920679092407, -0.09806616604328156, 0.701797604560852, -0.4296985864639282, 1.0024811029434204, 0.3188467025756836, -0.05385121330618858, -0.2853306531906128, -0.37962478399276733, -0.9289253354072571, -0.20793193578720093, -0.6663495302200317, -0.8758028745651245, -0.37204575538635254, 0.31295421719551086, -0.5987547039985657, -0.7038994431495667, -0.04299555718898773, -1.1691354513168335, -0.3318046033382416, -0.079953134059906, -0.604907214641571, -0.48611587285995483, -0.7338650822639465, -1.003497838973999, -0.45479393005371094, -0.12030550837516785, -0.9691436290740967, 0.6384817361831665, 0.36441367864608765, -0.8724057078361511, -0.028482791036367416, -0.5705084204673767, -0.258341521024704, 0.7842739224433899, -0.31933942437171936, 0.5116321444511414, -0.19500474631786346, -0.6330265998840332, -0.4298824071884155, -0.006066683679819107, 0.23684720695018768, -0.040795110166072845, -0.14941821992397308, -0.5665484666824341, 0.1449977159500122, -0.11816972494125366, 0.11081209033727646, 0.2370947301387787, 0.7149614691734314, 0.6979320645332336, 0.19311387836933136, -0.660443127155304, -0.13795660436153412, 1.109780192375183, -0.37935978174209595, -0.03670306131243706, -0.1107979416847229, 0.9977458119392395, 0.8337981104850769, -0.06447912007570267, 0.2896536588668823, 0.5879575610160828, 0.14190834760665894, 0.038601942360401154, -0.39151817560195923, -0.3657509684562683, -0.20563136041164398, 0.4839143753051758, 1.0318456888198853, 0.31619536876678467, -0.11720781773328781, -0.8394915461540222, 0.7094241380691528, -1.3022388219833374, -0.1806221306324005, 0.7487106323242188, 0.7211306691169739, -0.5007168650627136, -0.8539748787879944, -0.18537704646587372, -0.9821399450302124, 1.0759392976760864, 0.4784584045410156, -0.2781277000904083, -0.27931270003318787, -0.14368346333503723, 0.18698693811893463, -0.10410935431718826, 0.5631579756736755, -0.8001495599746704, 1.0007246732711792, 14.62401008605957, 0.5220422744750977, -0.007842432707548141, 0.454852432012558, 1.0880868434906006, 0.6450179815292358, -0.14122194051742554, -0.4954959750175476, -1.0825951099395752, -0.5358587503433228, 0.8182517290115356, 0.3669264614582062, 0.2394653707742691, -0.040972739458084106, 0.3581153154373169, -0.016127821058034897, -0.7915018796920776, 0.9079088568687439, 0.9061578512191772, -0.7323410511016846, 0.6167270541191101, -0.10071931034326553, -0.09914281219244003, 0.3889247477054596, 0.9616685509681702, 0.5446248054504395, -0.04831789433956146, -0.7298752069473267, 0.676766037940979, 0.20173302292823792, 0.7280301451683044, 0.22339585423469543, 0.2044755071401596, 0.3379334807395935, -0.9373918175697327, -0.28021782636642456, -0.42440491914749146, -0.40542691946029663, 0.6052981019020081, -0.9685611724853516, -0.20373474061489105, -0.3881128132343292, -0.45292192697525024, 0.3839939832687378, -0.2653484642505646, 0.7288882732391357, -0.13814452290534973, 0.26438477635383606, 0.2476811408996582, 0.12449933588504791, 0.35054948925971985, 0.21646974980831146, 0.6863881349563599, -0.07362335920333862, 0.02944626659154892, 0.5428254008293152, 0.5450584292411804, 1.0582534074783325, -0.683902382850647, 0.22558985650539398, -0.2647944986820221, -0.12303169071674347, -0.157907634973526, 0.7887586951255798, -0.2249165177345276, 0.15855661034584045, -0.5026543140411377, 0.5304322242736816, 0.1611449271440506, 0.6173127293586731, 0.11059063673019409, 0.008286447264254093, -0.2424912303686142, -0.0798872709274292, 0.13465063273906708, 0.30386385321617126, 0.04059707373380661, -0.4556366205215454, -0.6575286984443665, 0.12066091597080231, 0.40122726559638977, -1.0860927104949951, -0.8399841785430908, 1.1512712240219116, 0.07917138189077377, -0.6001977920532227, 0.1793016642332077, -0.7527562379837036, -0.1996537446975708, 0.2606922686100006, -0.8716955184936523, -1.305362582206726, -0.19230492413043976, -0.35965368151664734, 0.2183752954006195, -0.04163110628724098, 0.8780896663665771, -0.4398807883262634, -0.3351832926273346, 0.12213025242090225, -0.5532746315002441, 0.058769598603248596, -0.004186596255749464, -0.6072900891304016, 0.44986212253570557, 0.3985911011695862, -0.05601748451590538, 0.3704136908054352, -0.03594474494457245, -0.07868248969316483, -0.3786110579967499, 0.12189030647277832, 0.4829871952533722, -0.6608700156211853, -0.7528018951416016, -0.6907007694244385, -1.1595247983932495, 0.6191166043281555, 0.32656607031822205, 0.03528880700469017, 0.10569535195827484, -0.207879900932312, -0.06066093593835831, 0.35249248147010803, -1.1992287635803223, 0.3286457359790802, 0.4940773546695709, -0.6882655620574951, -0.4736887216567993, 0.12289690226316452, 0.6072970032691956, -1.4033757448196411, -0.1809164583683014, -0.3797374367713928, -0.16212132573127747, 0.2529568076133728, 0.9016492366790771, -0.49272680282592773, 0.2999822497367859, 0.6938156485557556, -0.40980589389801025, -0.46812233328819275, 0.1749136596918106, -0.8263976573944092, 0.20209231972694397, 0.334276407957077, 0.4717791676521301, -0.34101900458335876, -0.15138918161392212, 1.2045111656188965, 0.15850293636322021, -0.23676033318042755, -0.3446255326271057, -0.1975051313638687, -0.09242793172597885, -0.46544069051742554, 0.23590262234210968, -0.22481940686702728, -0.19691714644432068, -0.05138729512691498, 0.8872646689414978, 1.1500020027160645, -0.07160290330648422, -0.593038022518158, 0.37799814343452454, -0.08361335843801498, 0.2123764455318451, -0.696949303150177, -0.7746192216873169, -1.9212455749511719, 0.2218344509601593, -1.0330140590667725, 0.35979634523391724, -0.9242314100265503, -0.38318899273872375, 0.5192930698394775, -0.25264257192611694, 0.1995791494846344, 0.5402742028236389, -0.16968727111816406, -0.09740754216909409, -0.2516358196735382, -0.9987121820449829, 0.33335474133491516, 0.9729257822036743, -0.7092773914337158, -0.31604376435279846, 0.04329480975866318, -0.44114425778388977, 0.4960745573043823, 0.17742320895195007, -0.09444357454776764, -0.4462161064147949, -1.0360854864120483, -0.23182928562164307, 0.1333637237548828, 0.40459221601486206, -0.6552442312240601, 0.8173894882202148, 0.7643513679504395, -0.11151616275310516, -0.23314222693443298, 0.2160978466272354, -0.5529028177261353, -1.0140000581741333, -0.12973268330097198, -1.0416120290756226, -0.08869074285030365, 0.3598455488681793, -0.5199345946311951, -0.43672043085098267, 0.6680390238761902, -0.13169802725315094, -1.3327027559280396, -1.3139790296554565, 0.2124088555574417, -0.7230889797210693, -0.22642317414283752, -0.022761408239603043, -0.4326052665710449, -0.981199324131012, -0.39245298504829407, -0.7919679284095764, 0.4877494275569916, -0.35255661606788635, 1.2016950845718384, 0.8601410984992981, -0.7427034974098206, -0.0777207538485527, 0.23295509815216064, -0.09906510263681412, 0.471039354801178, 0.7210124731063843, 0.3621780574321747, -0.27787819504737854, 0.3292991816997528, 0.4508460462093353, 0.07929776608943939, -1.2818243503570557, -0.26443877816200256, 0.7551883459091187, 0.3162669539451599, -0.23273730278015137, 0.8827710747718811, 0.15872864425182343, -0.9053203463554382, 0.18843252956867218, -0.9917685389518738, -0.7607654333114624, -0.35638195276260376, 0.9403868317604065, 0.14234161376953125, -0.3620413541793823, -0.9989349842071533, -0.1780334711074829, 0.6582847237586975, -0.41016241908073425, -0.4647403657436371, 0.23714132606983185, -0.8232324123382568, -0.32762816548347473, 0.45760923624038696, 0.6098570823669434, -0.8631335496902466, -0.6529458165168762, -0.5237050652503967, -0.25109678506851196, -0.13077588379383087, 0.0021825097501277924, -0.4701886475086212, -0.3388283848762512, 0.4607527256011963, 0.7077808380126953, 0.05096741393208504, 0.03179645165801048, 0.5815706253051758, 0.063142791390419, 0.4336996078491211, 0.08523029834032059, -0.4874773919582367, -0.45607712864875793, 0.6431474685668945, 1.5951387882232666, -1.0646427869796753, 0.15224941074848175, -0.7332348823547363, -0.6275051236152649, 0.9774187207221985, 0.5805971622467041, 0.05596248060464859, 0.14714880287647247, -0.3282053768634796, 0.4443519711494446, -0.0016047003446146846, 0.15986424684524536, -0.3448645770549774, 0.9359880089759827, 1.044446587562561, 0.729996383190155, 0.5365190505981445, -0.18577203154563904, 0.9626352787017822, 0.2993933856487274, 0.3448459804058075, 0.7630046606063843, 0.391530841588974, -0.36287635564804077, -0.09033861756324768, -0.05638861283659935, 0.7011340260505676, -0.595140814781189, -0.3377780318260193, 0.10172443836927414, 0.7696312069892883, -0.2841896712779999, 0.5447520017623901, 1.0812819004058838, 0.6000239849090576, 0.6920322179794312, 0.3093447983264923, 0.571823000907898, -0.9642831683158875, -0.09545416384935379, 0.1684713512659073, -0.9359359741210938, -0.29409295320510864, -0.5132468342781067, -0.8620807528495789, -0.7759671807289124, 0.6109679937362671, 0.5396676063537598, -0.7298923134803772, -0.007002817001193762, 1.2535507678985596, 0.6066150665283203, 0.43293169140815735, -0.8464426398277283, -0.48783281445503235, -0.2245250642299652, -0.8058043122291565, -0.22268137335777283, -0.38117489218711853, -0.037663932889699936, -0.06774921715259552, 0.10492129623889923, -0.27925291657447815]}, "authors": [{"authorId": "2039921875", "name": "Zhuofan Xia"}, {"authorId": "2159087148", "name": "Dongchen Han"}, {"authorId": "40961502", "name": "Yizeng Han"}, {"authorId": "51170295", "name": "Xuran Pan"}, {"authorId": "2235964292", "name": "Shiji Song"}, {"authorId": "2249906537", "name": "Gao Huang"}], "references": [{"paperId": "bea8541268e34fbd550a390d2bce242f768d96b7", "title": "Revisiting Non-Autoregressive Transformers for Efficient Image Synthesis"}, {"paperId": "c672ec79f55cef8f7a32cd8dddfa981b893f1567", "title": "V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"}, {"paperId": "dc6a7257c19be0b48d200bcb66c493c4ac07a632", "title": "Mask Grounding for Referring Image Segmentation"}, {"paperId": "5b437e0f5353ce5ce5c599bfc778bc099185accd", "title": "Agent Attention: On the Integration of Softmax and Linear Attention"}, {"paperId": "d31bb7d2e09c46c4d26230046ba50ae53bb58004", "title": "See, Say, and Segment: Teaching LMMs to Overcome False Premises"}, {"paperId": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada", "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"}, {"paperId": "441bada9aa6dfd1f94d45d20e0f7eb060d59dd30", "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks"}, {"paperId": "07f9e1d3288b22cda2c980cfb969dcf410e5bd9e", "title": "u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model"}, {"paperId": "abb312b7cf508b91ae7a88b1890c13eb8b96ad1a", "title": "NExT-Chat: An LMM for Chat, Detection and Segmentation"}, {"paperId": "2313afae52d98e569da2dedbf14daf9efc74e7cf", "title": "CogVLM: Visual Expert for Pretrained Language Models"}, {"paperId": "6ae4705139494fcb6b790b6dd6c4225b40ee40f8", "title": "GLaMM: Pixel Grounding Large Multimodal Model"}, {"paperId": "f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a", "title": "Kosmos-2.5: A Multimodal Literate Model"}, {"paperId": "3803d1f291e162bdaa4678a2c5a2bbcf63c050f4", "title": "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning"}, {"paperId": "981970d0f586761e7cdd978670c6a8f46990f514", "title": "DAT++: Spatially Dynamic Vision Transformer with Deformable Attention"}, {"paperId": "787498b02cd5d85f39074a14b0ba3e5034dfd6b5", "title": "Latency-aware Unified Dynamic Networks for Efficient Image Recognition"}, {"paperId": "4200ad698a5c778328d399723eb4cd3f414460ee", "title": "Beyond One-to-One: Rethinking the Referring Image Segmentation"}, {"paperId": "1fd31b74f5e1eeb67341982fd35a613c6fad10e0", "title": "Link-Context Learning for Multimodal LLMs"}, {"paperId": "ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7", "title": "LISA: Reasoning Segmentation via Large Language Model"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "962ccf1fc49c83817fb031e5b24b81b19cdfb89d", "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"}, {"paperId": "094883e42bb9a41f602c0715c1059bc431e33fb2", "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"}, {"paperId": "3b6179c293df29e31d31cea46476f104ab6950f2", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World"}, {"paperId": "e12561022ff0945afd8ea764965f7a6e106ef26a", "title": "Dynamic Perceiver for Efficient Visual Recognition"}, {"paperId": "d47524cd5c3c4b57af2e5a29f6f91c420310f236", "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning"}, {"paperId": "1ec4bc98fafa8d338f676f7a1b1b1131e8ca978e", "title": "GRES: Generalized Referring Expression Segmentation"}, {"paperId": "ed8ac4ff13d32a291bbe74f3e5a138800bba47fd", "title": "Image as a Foreign Language: BEIT Pretraining for Vision and Vision-Language Tasks"}, {"paperId": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "0819c1e60c13b9797f937282d06b54d252d9d6ec", "title": "Segment Everything Everywhere All at Once"}, {"paperId": "53e5db85e2a7442f20670be2ae25019fcf9d27a2", "title": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "a757999ed260d7bc45484dc6b4456bf33fe6f679", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"}, {"paperId": "bc04606ad7fb32dbb06daaecc66c61304bb11ec5", "title": "Adaptive Rotated Convolution for Rotated Object Detection"}, {"paperId": "c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0", "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "55cd2d0a8f26c4dc458303f937af2b6fb8f8b693", "title": "PolyFormer: Referring Image Segmentation as Sequential Polygon Generation"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "006c720992e157f407bb9465992290edf7cf4b8e", "title": "Joint Representation Learning for Text and 3D Point Cloud"}, {"paperId": "17b88fdba24e494134e5b33dc8aa8eb56bd2294e", "title": "PACO: Parts and Attributes of Common Objects"}, {"paperId": "967907503b24423b9b74621051811fcf684e3957", "title": "Generalized Decoding for Pixel, Image, and Language"}, {"paperId": "d2edf22af2239f754ea7fa0e044be254161eee70", "title": "Deep Incubation: Training Large Models by Divide-and-Conquering"}, {"paperId": "19e01fe0194dfff19f113ce8cec07b808e945a08", "title": "CoupAlign: Coupling Word-Pixel with Sentence-Mask Alignments for Referring Image Segmentation"}, {"paperId": "c480a4735b0e3511192333e3a4da6b233bfe1790", "title": "EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones"}, {"paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "3d6c270f61e9f2980a67e51fa95d82aea7ebe793", "title": "Latency-aware Spatial-wise Dynamic Networks"}, {"paperId": "be2af1fc9c07326c5cd5d7ed057083695f188fa2", "title": "Embodied Referring Expression for Manipulation Question Answering in Interactive Environment"}, {"paperId": "281184f7e63c60b3dc30dbf1e62e27c55ba9a9db", "title": "AdaFocusV3: On Unified Spatial-temporal Dynamic Video Recognition"}, {"paperId": "d49d6a4a1cc7cd5cffe980efde65d643da2d4f6c", "title": "Learning to Weight Samples for Dynamic Early-exiting Networks"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "7fafe86c53404cdc1c2c61adc413f2309a097ed8", "title": "Glance and Focus Networks for Dynamic Visual Recognition"}, {"paperId": "e5cb26148791b57bfd36aa26ce2401e231d01b57", "title": "Vision Transformer with Deformable Attention"}, {"paperId": "6c19c299cb291c498ae7076079c9ae0e15743e9c", "title": "AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition"}, {"paperId": "ca1d5aa8f63707931692bf6a62642becf928c1fa", "title": "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation"}, {"paperId": "76a2b197b5427ffd1d3470c6d3ea026588eb5d0a", "title": "CRIS: CLIP-Driven Referring Image Segmentation"}, {"paperId": "a0023d03985f94dddef12f762bda45948f144460", "title": "On the Integration of Self-Attention and Convolution"}, {"paperId": "ecb85de81dd77dd6d7466a9839023000ddc3ce17", "title": "Spatially Adaptive Feature Refinement for Efficient Inference"}, {"paperId": "5818a77da6a23e9d4b914c24efe7e8e91016c493", "title": "YouRefIt: Embodied Reference Understanding with Language and Gesture"}, {"paperId": "367a5169efb790a15bcc1afcb0164b32740b751a", "title": "Vision-Language Transformer and Query Generation for Referring Segmentation"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "1c9cea1cfc9183f51e617b3cdfe6b1d045c23286", "title": "Room-and-Object Aware Knowledge Reasoning for Remote Embodied Referring Expression"}, {"paperId": "14b97585f136671742f6ce4151081e487b1fc1fe", "title": "Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition"}, {"paperId": "c65c5e3ae9afae69c31da59e722a346ec2b10b5b", "title": "Adaptive Focus for Efficient Video Recognition"}, {"paperId": "102cbf1eb78b2c0bdb998ac514dcbae27143c85a", "title": "Encoder Fusion Network with Co-Attention Embedding for Referring Image Segmentation"}, {"paperId": "caf146b244da72523d0ebdd94a8a220b1aa7d946", "title": "Locate then Segment: A Strong Pipeline for Referring Image Segmentation"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "837ac4ed6825502f0460caec45e12e734c85b113", "title": "Dynamic Neural Networks: A Survey"}, {"paperId": "2b8088253e2378fce001a090fe923b81e8dedf25", "title": "RepVGG: Making VGG-style ConvNets Great Again"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "66566337664bee69915d3a46e0c5b66b15a8f5b5", "title": "Referring Image Segmentation via Cross-Modal Progressive Comprehension"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8d1bc83cc65d30e4619c49f53115012a209fd8c9", "title": "Multi-Task Collaborative Network for Joint Referring Expression Comprehension and Segmentation"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "661d82c26fad099f28a8da02d2c9d1cfee32cff4", "title": "See-Through-Text Grouping for Referring Image Segmentation"}, {"paperId": "1bdd8f6d3900e6e7fe492ea21fcdd87f3d8c857f", "title": "REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments"}, {"paperId": "69455376f5ad52cac5b72d5e8c6cf03fb466b55c", "title": "Cross-Modal Self-Attention Network for Referring Image Segmentation"}, {"paperId": "03fdf3abf8d6bb3ff35dc87742ad66722997caeb", "title": "Vision-Based Navigation With Language-Based Assistance via Imitation Learning With Indirect Intervention"}, {"paperId": "c66b8e508718f4b7f14829e5c2cde0add31d2693", "title": "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation"}, {"paperId": "59e5a8e47e9408013f84cdf80b4ac49e9d82fa84", "title": "Key-Word-Aware Network for Referring Expression Image Segmentation"}, {"paperId": "8f44799620bf22f6efa01c35497de7dda3e5d4ab", "title": "Referring Image Segmentation via Recurrent Refinement Networks"}, {"paperId": "fdce9cbe5c726201575b3c8a8c1af0752f1af53f", "title": "MAttNet: Modular Attention Network for Referring Expression Comprehension"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "79828e6e9f137a583082b8b5a9dfce0c301989b8", "title": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes"}, {"paperId": "3d69edb02e935b782b90175cb691f6ab5f4bd64f", "title": "Recurrent Multimodal Interaction for Referring Image Segmentation"}, {"paperId": "0095b9f73c000f2609fc81ffb7769df7cd77bda1", "title": "COCO-Stuff: Thing and Stuff Classes in Context"}, {"paperId": "88512be44744615f4baa8e14f600f036db4c2433", "title": "Semantic Understanding of Scenes Through the ADE20K Dataset"}, {"paperId": "29efbe391950ae438c63d86ad5c82b2942efb0b4", "title": "Modeling Context in Referring Expressions"}, {"paperId": "b133e361e2f8af22b823d25060b2e7c47f690985", "title": "Segmentation from Natural Language Expressions"}, {"paperId": "e65142010431ffc089b272a1174214e00693e503", "title": "Generation and Comprehension of Unambiguous Object Descriptions"}, {"paperId": "92c141447f51b6732242376164ff961e464731c8", "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes"}, {"paperId": "caf202fd5833b1ef635923e79608e1a48d7539f9", "title": "Detect What You Can: Detecting and Representing Objects Using Holistic Models and Body Parts"}, {"paperId": "c30c9009a5fee2974666a89ca536951c373952db", "title": "Learning Aligned Cross-modal Representations for Referring Image Segmentation"}, {"paperId": "a40f5b9acd81fad048a97562336b46d04cde4023", "title": "Budgeted Training for Vision Transformer"}, {"paperId": "63f1f2dad0a2e84d37a97258008c5609195487f0", "title": "Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs"}, {"paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607", "title": "A Survey on Context Learning"}, {"paperId": null, "title": "Imagespirit: Verbal guided image parsing"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt"}, {"paperId": null, "title": "alpaca: An instruction-following llama"}]}