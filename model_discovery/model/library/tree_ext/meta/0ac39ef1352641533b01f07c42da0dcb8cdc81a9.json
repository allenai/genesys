{"paperId": "0ac39ef1352641533b01f07c42da0dcb8cdc81a9", "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference", "abstract": "This paper presents the Block Transformer architecture which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks of self-attention. To apply self-attention, the key-value (KV) cache of all previous sequences must be retrieved from memory at every decoding step. Thereby, this KV cache IO becomes a significant bottleneck in batch inference. We notice that these costs stem from applying self-attention on the global context, therefore we isolate the expensive bottlenecks of global modeling to lower layers and apply fast local modeling in upper layers. To mitigate the remaining costs in the lower layers, we aggregate input tokens into fixed size blocks and then apply self-attention at this coarse level. Context information is aggregated into a single embedding to enable upper layers to decode the next block of tokens, without global attention. Free of global attention bottlenecks, the upper layers can fully utilize the compute hardware to maximize inference throughput. By leveraging global and local modules, the Block Transformer architecture demonstrates 10-20x gains in inference throughput compared to vanilla transformers with equivalent perplexity. Our work introduces a new approach to optimize language model inference through novel application of global-to-local modeling. Code is available at https://github.com/itsnamgyu/block-transformer.", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The Block Transformer architecture is presented, which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks of self-attention to optimize language model inference through novel application of global-to-local modeling."}, "embedding": {"model": "specter_v2", "vector": [0.079288549721241, 0.3402390778064728, -0.6352559328079224, -0.31559932231903076, -0.10920126736164093, 0.00017380750796291977, 0.707582414150238, -0.15847498178482056, -0.3634001910686493, -0.5500503778457642, 0.7611757516860962, 0.04332641884684563, 0.9506117105484009, -0.12020783871412277, -0.12670977413654327, 0.13271330296993256, -0.8669896125793457, -0.05171138793230057, 0.09212261438369751, -0.4638771712779999, -0.22719600796699524, -0.5915172100067139, -1.073269248008728, 0.3174881935119629, 0.3134843111038208, 0.7481836676597595, 0.2983951270580292, 0.8822570443153381, -0.5832534432411194, 0.5864862203598022, 0.43240028619766235, -0.23853786289691925, 0.09592176228761673, -0.052968829870224, -0.11714563518762589, -0.5282497406005859, 0.3240804672241211, -0.5830091238021851, -0.4897697865962982, 1.0136116743087769, -0.2066384255886078, 0.11917906999588013, 0.3732512295246124, -0.6758089661598206, 0.10346383601427078, 1.07639479637146, 0.7380214929580688, 0.8737843036651611, -0.6570565104484558, -0.7843936681747437, 1.4650970697402954, -1.3608049154281616, -0.17540258169174194, 1.5667355060577393, 0.3410705327987671, 0.38942334055900574, -0.13823014497756958, -1.0776515007019043, 1.0531727075576782, 0.2202237844467163, -0.7516116499900818, -0.8267835974693298, 0.2036241739988327, -0.11042043566703796, 1.9830505847930908, -0.42730963230133057, 0.3603687882423401, 0.5289489030838013, 0.23866750299930573, 1.4874050617218018, -0.024669764563441277, -0.4429488182067871, -0.08021099865436554, -0.07003959268331528, 0.4623396694660187, 0.8973302245140076, -0.5196669101715088, 0.07026126980781555, -1.154383659362793, -0.05908111855387688, 0.5973756313323975, -0.2592833340167999, 0.4492585062980652, -0.05623823404312134, -0.10932550579309464, 0.766765832901001, 0.17952728271484375, 0.8356499671936035, -0.14849521219730377, 1.031466007232666, 0.7937769889831543, 0.08679283410310745, 0.3102792799472809, -0.01500542089343071, -0.05880941450595856, 0.04968864470720291, -1.1117743253707886, 0.0733230859041214, -0.04312238097190857, 0.8717456459999084, -0.19612865149974823, 1.004751205444336, -0.7526698112487793, 0.24724851548671722, 1.58689546585083, 0.4448758065700531, 0.6126990914344788, -0.4354916214942932, 0.06335736811161041, -1.1117256879806519, -0.15961676836013794, -0.7697522640228271, 0.037706408649683, -0.6124977469444275, -0.8442507982254028, -1.2628036737442017, -0.6797654032707214, 0.3924710750579834, -0.9597417712211609, 0.7935659885406494, -0.2150927037000656, 0.3742216229438782, -0.16399620473384857, -0.11411584913730621, 0.5194909572601318, 0.5676351189613342, 0.4275851845741272, 0.12539443373680115, 1.3252537250518799, -1.2500944137573242, -0.7299448251724243, -1.080051302909851, 0.5241519808769226, 0.07857310026884079, -0.15547999739646912, -0.23790758848190308, -1.1898053884506226, -1.2435250282287598, -0.9414376616477966, 0.025696247816085815, -0.646886944770813, 0.13776785135269165, 0.7069979310035706, 0.6338162422180176, -1.1983975172042847, 0.627787172794342, -0.539152979850769, 0.09812822937965393, 0.3827957212924957, 0.12676768004894257, 0.40108516812324524, 0.05826638638973236, -1.5781651735305786, 0.28386348485946655, 0.20814955234527588, -0.1953093707561493, -0.38632380962371826, -0.736955463886261, -1.1444096565246582, 0.2109559178352356, -0.018069254234433174, -0.24088959395885468, 1.1978402137756348, -0.21276597678661346, -1.6190402507781982, 0.4506390392780304, -0.7863967418670654, -0.23394039273262024, -0.061489757150411606, -0.1837332844734192, -0.604102611541748, -0.35124486684799194, -0.01911740005016327, 0.3260512351989746, 0.8135605454444885, 0.08188167214393616, -0.4612070918083191, -0.09690478444099426, -0.6555895209312439, -0.15351124107837677, -0.1837611049413681, 1.09368097782135, -0.6569747924804688, -0.5149056315422058, 0.30448660254478455, 0.8453658819198608, 0.04357077553868294, -0.6371655464172363, -0.12798501551151276, -1.2317365407943726, 0.5324728488922119, -0.35198891162872314, 0.9983574151992798, -0.8721678853034973, -0.9900959730148315, 0.15862011909484863, -0.03497082367539406, 0.05746287479996681, -0.7638099193572998, 0.5538281202316284, -0.40177956223487854, 0.2725757360458374, 0.4594584107398987, -0.9298222661018372, 0.07276339828968048, -0.49727773666381836, -1.054854393005371, -0.4149860441684723, 0.01868307963013649, 1.1587517261505127, -1.0344620943069458, -0.1753089874982834, 0.02333514764904976, 0.5858959555625916, -1.0554358959197998, 1.3227617740631104, -0.18946129083633423, -0.01422111690044403, -0.36353594064712524, -0.33783596754074097, 0.09326968342065811, -0.10861577093601227, 0.5949477553367615, -0.6253365874290466, 0.005938939284533262, 0.8212612867355347, -0.10362306237220764, 0.9849842190742493, -0.549992561340332, 0.6487020254135132, -0.10898909717798233, -0.7333639860153198, 0.32792893052101135, 0.33073583245277405, -0.09179159998893738, -0.7595517039299011, 0.18526732921600342, 0.1901894509792328, -0.6337495446205139, 0.6979408860206604, 0.4996175467967987, 1.1012980937957764, -0.15750980377197266, 0.03665142506361008, 0.6359362006187439, -0.04649951308965683, 0.3923462927341461, 0.5184569358825684, 0.7833530306816101, 0.35895416140556335, 0.5063886642456055, -0.1775900423526764, 0.3391169011592865, -0.9203388690948486, -0.0379890538752079, 0.7257071733474731, 0.7139682769775391, 0.398181676864624, 0.45796048641204834, -0.6513561606407166, -0.4145110249519348, 0.07586703449487686, 0.7079429626464844, 1.4937406778335571, -0.7607859969139099, -0.529822051525116, -0.6719472408294678, -0.19910526275634766, -0.23873424530029297, 0.06987844407558441, -0.22212524712085724, -0.28172066807746887, -0.6208533644676208, -0.7137553095817566, 0.7370798587799072, 0.5165618658065796, 1.176712989807129, -0.5237442255020142, -0.09738902747631073, 0.0767621174454689, 0.2966383993625641, -1.1779874563217163, -0.4509487748146057, 0.24962766468524933, -0.6150145530700684, 0.4341168999671936, 0.36139798164367676, 0.16741235554218292, 0.14830337464809418, -0.6447509527206421, 0.9264367818832397, -0.5581021308898926, -0.06355179846286774, 0.15826734900474548, 0.41762685775756836, -0.5051808953285217, -0.8630278706550598, 0.1359204649925232, 0.2150212824344635, 0.030030421912670135, 0.4234520494937897, 0.2908812165260315, -0.1554960161447525, -0.2582765221595764, 0.20398494601249695, 0.3595854640007019, 0.16162109375, -0.1130576878786087, 0.3174011707305908, -0.022670382633805275, -0.15879829227924347, -1.1692253351211548, 0.6272811889648438, 0.2374282330274582, -0.4683058559894562, -0.0611628033220768, -0.6138056516647339, -0.17110508680343628, 0.5712488293647766, -0.3540152609348297, -0.23865251243114471, -0.8576146364212036, 0.09570084512233734, -0.4265812039375305, -0.01343750860542059, 0.007023459766060114, 0.10808204114437103, 0.3745098412036896, -0.3884449303150177, 0.9395880699157715, 0.25996461510658264, -0.06997516006231308, 0.4604099690914154, -0.7716821432113647, 0.45793387293815613, 0.34100669622421265, 0.10043428838253021, 0.09876177459955215, -0.28256404399871826, -0.8565048575401306, -0.4260694980621338, -0.6439506411552429, -0.12477639317512512, -0.33500707149505615, 0.37616822123527527, -1.0266841650009155, -1.1885173320770264, 0.05288926139473915, -0.8654032349586487, -0.483034610748291, 0.5129198431968689, -0.4321482181549072, -0.23024316132068634, -1.1409718990325928, -1.2386422157287598, -0.8422824144363403, -0.8669180870056152, -0.8406604528427124, 0.2789604067802429, -0.21442046761512756, -0.305225133895874, -0.6136958003044128, -0.18396246433258057, -0.3530231714248657, 1.2583093643188477, -1.0090004205703735, 0.4826778173446655, -0.178358793258667, -0.3416793644428253, -0.2878139913082123, 0.14795273542404175, 0.810005784034729, -0.013643540441989899, 0.11969228088855743, -1.0371211767196655, 0.06129908189177513, -0.40267109870910645, 0.22858501970767975, 0.37798500061035156, 0.4002424478530884, 0.777749240398407, 0.057849567383527756, -0.6861132979393005, 0.3837985694408417, 1.4603136777877808, -0.4072266221046448, 0.240520641207695, -0.04874108359217644, 1.2754830121994019, -0.23925715684890747, -0.2122468501329422, 0.4290899634361267, 0.6314800381660461, 0.6210034489631653, 0.02085563912987709, -0.2047152817249298, -0.13802441954612732, -0.599883496761322, 0.8873535990715027, 1.8540204763412476, 0.3306424021720886, 0.16518810391426086, -0.8571205139160156, 0.775320291519165, -1.127874732017517, -0.986410915851593, 0.8795272707939148, 0.8238866329193115, 0.3957052528858185, -0.3687634766101837, -0.28739896416664124, -0.34850504994392395, 0.26367080211639404, 0.5637797117233276, -0.12899795174598694, -1.0155689716339111, 0.3481413722038269, 0.45394110679626465, 0.42879927158355713, 0.7404645681381226, -0.5146563649177551, 1.0657085180282593, 14.659967422485352, 0.8598329424858093, -0.06762123107910156, 0.6792678833007812, 0.7521786689758301, 0.4326518177986145, -0.3030778467655182, 0.014655180275440216, -1.7765156030654907, 0.13200704753398895, 1.4602850675582886, 0.40536126494407654, 0.5525538921356201, 0.2573204040527344, 0.33170661330223083, 0.4881865084171295, -0.25346148014068604, 0.3853656053543091, 0.6870426535606384, -1.1659656763076782, 0.43485602736473083, 0.020418059080839157, -0.05981478467583656, 0.6122318506240845, 0.3623266816139221, 0.9509455561637878, 0.8662596940994263, -0.21504096686840057, 0.2721765637397766, 0.27319714426994324, 0.636786162853241, -0.1263212114572525, -0.009448878467082977, 0.33763787150382996, -1.2510098218917847, -0.1317487359046936, -0.6081108450889587, -0.9745367169380188, -0.03695499151945114, 0.382444828748703, -0.6763577461242676, -0.7159250378608704, -0.15239395201206207, 0.6566165089607239, 0.1338181048631668, 0.10183669626712799, -0.16482171416282654, 1.0029128789901733, -0.08207343518733978, -0.29293930530548096, 0.15128089487552643, 0.4640757143497467, 0.07452885806560516, 0.26344621181488037, 0.013350844383239746, 0.058331623673439026, -0.0711679607629776, 0.25939035415649414, -0.2971823215484619, -0.05755835026502609, -0.10505757480859756, -0.3560332953929901, 0.022833887487649918, 0.8590639233589172, 0.4773728847503662, -0.23812265694141388, -0.3519909381866455, 0.3849848210811615, 0.8780555725097656, 0.33001744747161865, -0.41206488013267517, 0.10338333994150162, 0.643851637840271, -0.5165819525718689, 0.5338151454925537, 0.3435506224632263, 0.125055193901062, -0.758030354976654, -0.9407142400741577, -0.5214128494262695, 0.2714482545852661, -0.8920080661773682, -0.40370064973831177, 0.8417825698852539, -0.26114118099212646, -0.20560118556022644, -0.20160545408725739, -0.7976447343826294, -0.31714338064193726, 0.24485859274864197, -1.2849950790405273, -0.698773980140686, 0.3751130700111389, -0.3359270393848419, -0.18347349762916565, -0.07879282534122467, 1.0847527980804443, 0.00045501330168917775, -0.15841689705848694, 0.1518906205892563, 0.15368321537971497, 0.4687597155570984, -0.29753294587135315, -0.6461150050163269, 1.2864266633987427, 0.4772624373435974, 0.03109300136566162, 0.47023919224739075, 0.0018202282954007387, 0.00986027903854847, -1.0258668661117554, -0.03127739951014519, 0.8043276071548462, -0.6340608596801758, 0.0018554737325757742, -1.2416309118270874, -0.4650813639163971, 0.35229170322418213, 0.5581791400909424, -0.17967598140239716, 0.09551677107810974, 0.1747540831565857, -0.8780854940414429, -0.5656613111495972, -0.24589018523693085, 0.264058381319046, 0.38999754190444946, -0.702791690826416, 0.2633684277534485, -0.305891752243042, 0.37289920449256897, -0.7165583372116089, -0.7547240853309631, -0.3258834183216095, 0.5234851837158203, -0.042094212025403976, 1.0980722904205322, -0.34652194380760193, 0.5424755215644836, 0.9366121888160706, -0.08393149822950363, -0.7409994006156921, -0.26420658826828003, -0.7806925177574158, -0.0732225850224495, 0.4272010028362274, 0.5347869992256165, -0.6042962670326233, 0.27194055914878845, 0.8206973671913147, 0.469918429851532, -0.3135168254375458, -0.7748047709465027, -0.2315818816423416, -0.11603640019893646, -0.6747180819511414, 0.34576237201690674, -0.03209403529763222, -0.08331393450498581, 0.13769109547138214, 0.3533077836036682, 0.5237497091293335, 0.029596364125609398, -0.7485294938087463, 0.6310168504714966, 0.0837617963552475, 0.036240074783563614, -0.5168622732162476, -0.7683556079864502, -1.3702797889709473, 0.2680758535861969, -1.0373109579086304, -0.004063263535499573, -1.0744258165359497, -0.4901416003704071, 0.15476880967617035, -0.585536777973175, 0.16102634370326996, 0.16266073286533356, -0.1672660857439041, -0.3805827796459198, -0.6651746034622192, -0.33817872405052185, 0.6768078804016113, 0.7113792896270752, -0.8531880974769592, 0.25214847922325134, -0.01838577352464199, 0.15672697126865387, 0.08475462347269058, 0.5078641772270203, -0.22325104475021362, -0.6851194500923157, -1.1499348878860474, 0.5189058184623718, -0.34648534655570984, -0.18242932856082916, -0.7359337210655212, 0.9332736730575562, 0.4674100875854492, 0.028463173657655716, -0.06773406267166138, 0.4309530258178711, -0.7748211026191711, -0.7743393778800964, 0.43526431918144226, -0.6341021060943604, 0.2939402759075165, 0.297182559967041, -0.5305530428886414, -0.33627206087112427, 0.937206506729126, -0.14267995953559875, -1.0833817720413208, -1.0345299243927002, 0.4351537525653839, -0.7723440527915955, 0.08521366119384766, -0.679379940032959, -0.028599925339221954, -1.061453104019165, -0.7101542353630066, 0.17506636679172516, 0.16606728732585907, -0.7688917517662048, 1.1213566064834595, 0.5223585963249207, -1.1691267490386963, -0.08213604986667633, 0.7708476781845093, -0.22308015823364258, -0.32265356183052063, 0.581362247467041, 0.2925805151462555, -0.21524734795093536, 0.6176807880401611, 0.2553420960903168, 0.1232987642288208, -0.9154470562934875, -0.11684733629226685, 0.5935380458831787, -0.4691987931728363, -0.1148587316274643, 0.9568977952003479, -0.22406062483787537, -0.7923904061317444, -0.18914708495140076, -1.2678064107894897, -0.5476686954498291, -0.1851942539215088, 0.5961458683013916, -0.01763797365128994, -0.25116562843322754, -0.18199291825294495, -0.5459568500518799, 0.042293429374694824, -0.39534154534339905, -0.896561861038208, 0.28989577293395996, 0.06795576959848404, -0.7413650155067444, 0.5600638389587402, 0.8995836973190308, -0.47025471925735474, -0.5150042176246643, -0.7451875805854797, -0.658115029335022, 0.021786443889141083, 0.7869062423706055, -0.013754736632108688, -0.6886050701141357, 0.7781811356544495, 0.4838322103023529, 0.3134109079837799, 0.05341298133134842, -0.5470948219299316, 0.07183045148849487, 0.4592340588569641, 0.11878465861082077, -0.7692920565605164, -0.7348192930221558, 1.5429637432098389, 0.9933417439460754, -0.501393735408783, 0.2278822362422943, -0.1566324084997177, -0.45290830731391907, 0.5944746732711792, 0.18848785758018494, -0.2924681305885315, 1.2802672386169434, 0.0949784442782402, 0.4038483798503876, 0.4830878674983978, -1.3117339611053467, -0.41078150272369385, 0.8840908408164978, 0.8068834543228149, 0.7730323076248169, 0.26308634877204895, 0.4511764347553253, 0.5972412824630737, 0.0536288321018219, -0.013567556627094746, 0.031360119581222534, 0.039780113846063614, -0.23137632012367249, 0.10063382983207703, 0.18897290527820587, 0.7358445525169373, -0.6551274061203003, -1.1456345319747925, 0.5795169472694397, 0.45763492584228516, -0.3827817142009735, 0.576714277267456, 1.2934871912002563, 0.03662364184856415, 0.16248710453510284, 0.3272026479244232, 0.4402807354927063, -0.3606913089752197, -0.20393604040145874, 0.12402347475290298, -0.45858466625213623, -0.2261616736650467, -0.08122935146093369, -0.3277257978916168, -0.42799150943756104, -0.050063591450452805, 0.4508496820926666, -0.06023326888680458, 0.7832438945770264, 1.0445208549499512, 0.549624502658844, 0.5285229086875916, -0.046774908900260925, -0.28449827432632446, -0.30301210284233093, -1.0358439683914185, 0.043846145272254944, -0.7324643135070801, -0.10355968773365021, 0.09729889780282974, -0.0016053150175139308, -0.4103367328643799]}, "authors": [{"authorId": "2047104015", "name": "Namgyu Ho"}, {"authorId": "2104224550", "name": "Sangmin Bae"}, {"authorId": "1840081210", "name": "Taehyeon Kim"}, {"authorId": "2304748875", "name": "Hyunjik Jo"}, {"authorId": "2304808746", "name": "Yireun Kim"}, {"authorId": "32303439", "name": "Tal Schuster"}, {"authorId": "2064150446", "name": "Adam Fisch"}, {"authorId": "2263749072", "name": "James Thorne"}, {"authorId": "2269051253", "name": "SeYoung Yun"}], "references": [{"paperId": "2f96229c404a5cbd0c0d06492f5ea3d6bfcf50d2", "title": "PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference"}, {"paperId": "8b8a7f1ac390a2394802234d3c539da86c56de66", "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"}, {"paperId": "59f5ae64b4533ef62a883ad5b6b667a4157a1005", "title": "Layer-Condensed KV Cache for Efficient Inference of Large Language Models"}, {"paperId": "b1f5087ab3e782f718a1393bed242b4b412e648b", "title": "Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis"}, {"paperId": "1784c987e681d60c634765fe64c8d9c26f73d5ff", "title": "SnapKV: LLM Knows What You are Looking for Before Generation"}, {"paperId": "3fd5bc3077d04965eaa3498372c39bbdd09d55e4", "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"}, {"paperId": "a1f23f04421cdc62e80fe9f04c1ce60f4a6af9f0", "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"}, {"paperId": "f9c4c6c804930f751debe29780e9741315595aa8", "title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models"}, {"paperId": "41b47f33a24feefd6728bdc1339d0d4ff5fec7be", "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"}, {"paperId": "20f090e35ad598fba2404e550c2462dc9da03a10", "title": "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve"}, {"paperId": "f5c83f5156904bdf92ff2f169871e320394f7c0a", "title": "Tandem Transformers for Inference Efficient LLMs"}, {"paperId": "1b5db3170c195508ff24fee8eda0d4987e806f0b", "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"}, {"paperId": "57e7af0b69325fafb371ef5d502e39ef9c90ef7e", "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"}, {"paperId": "00e18c603e60d861c4e99c541e4d65ef442d5945", "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory"}, {"paperId": "564855d475ed9197dd7516594557ff886ff623e5", "title": "Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding"}, {"paperId": "abe90a291e7cf567ce5c9012a692beeae153068d", "title": "Think before you speak: Training Language Models With Pause Tokens"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "43e624ddeed82df944a6cae0dedec3372438e243", "title": "Accelerating LLM Inference with Staged Speculative Decoding"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "0244aeb7c6927e2fb0c2e668687e160a00737dbe", "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4"}, {"paperId": "e4a95f595b5d60a0858725996b9355f7275492cf", "title": "Hierarchical Attention Encoder Decoder"}, {"paperId": "83e49a574951789b4cb08d0897fe275ddb8c1553", "title": "LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "412e266cddfd87c79087a88ba1e4d11b89a45a13", "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers"}, {"paperId": "594d8e1696619f3cebb7c6bffdad8e0a5592f006", "title": "Scaling Transformer to 1M tokens and beyond with RMT"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c", "title": "Accelerating Large Language Model Decoding with Speculative Sampling"}, {"paperId": "5a3c1afe73d8bcc8288d17cb17be2baec8a98464", "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training"}, {"paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644", "title": "Fast Inference from Transformers via Speculative Decoding"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "2ef60a4ea4ea53056be811ff55679eb59fb4b586", "title": "Confident Adaptive Language Modeling"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "bd44f34b47c8a4b6947695853fc2814ac69664a6", "title": "Datasheet for the Pile"}, {"paperId": "66d735987a31d666a6459566ae026c40ab9a1c3a", "title": "The Efficiency Misnomer"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "42b0d32a7e644b657e34ce056c84d215d9f62187", "title": "Universal"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "46b3ba0f3cb8340bc94f26e0fdf6dc4e38f68948", "title": "Hierarchical Transformers for Long Document Classification"}, {"paperId": "00c957711b12468cb38424caccdf5291bb354033", "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "5e04881e91bff952d102d967c4ffb498ec30d4af", "title": "Blockwise Parallel Decoding for Deep Autoregressive Models"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": null, "title": "OpenLLM: Operating LLMs in production"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Towards fast inference: Exploring and improving blockwise parallel drafts"}, {"paperId": null, "title": ": Open models based on gemini research and technology"}, {"paperId": null, "title": "Decoder-decoder"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}]}