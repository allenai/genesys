{"paperId": "87a108853c92a969fda26d0fd91bbd1d70706a98", "title": "The Evolution of Multimodal Model Architectures", "abstract": "This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape. Systematically categorizing models by architecture type facilitates monitoring of developments in the multimodal domain. Distinct from recent survey papers that present general information on multimodal architectures, this research conducts a comprehensive exploration of architectural details and identifies four specific architectural types. The types are distinguished by their respective methodologies for integrating multimodal inputs into the deep neural network model. The first two types (Type A and B) deeply fuses multimodal inputs within the internal layers of the model, whereas the following two types (Type C and D) facilitate early fusion at the input stage. Type-A employs standard cross-attention, whereas Type-B utilizes custom-designed layers for modality fusion within the internal layers. On the other hand, Type-C utilizes modality-specific encoders, while Type-D leverages tokenizers to process the modalities at the model's input stage. The identified architecture types aid the monitoring of any-to-any multimodal model development. Notably, Type-C and Type-D are currently favored in the construction of any-to-any multimodal models. Type-C, distinguished by its non-tokenizing multimodal model architecture, is emerging as a viable alternative to Type-D, which utilizes input-tokenizing techniques. To assist in model selection, this work highlights the advantages and disadvantages of each architecture type based on data and compute requirements, architecture complexity, scalability, simplification of adding modalities, training objectives, and any-to-any multimodal generation capability.", "venue": "arXiv.org", "year": 2024, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work uniquely identifies and characterizes four prevalent multimodal model architectural patterns in the contemporary multimodal landscape and highlights the advantages and disadvantages of each architecture type based on data and compute requirements, architecture complexity, scalability, simplification of adding modalities, training objectives, and any-to-any multimodal generation capability."}, "embedding": {"model": "specter_v2", "vector": [0.23435458540916443, 0.51791912317276, -0.39445772767066956, -0.14111948013305664, -0.804841935634613, 0.016227373853325844, 0.533605694770813, -0.3799111545085907, -0.26873543858528137, -0.5894498229026794, 0.7950969934463501, 0.4110732078552246, 0.22555673122406006, 0.2888926863670349, -0.031224466860294342, 0.15529710054397583, -0.7604490518569946, -0.21168406307697296, -0.5105591416358948, -0.4588470458984375, -0.21262887120246887, -0.6313941478729248, -1.4384833574295044, 0.5629891157150269, 0.04263640195131302, 0.9889618158340454, 0.31236889958381653, 1.2770981788635254, -0.19478215277194977, 0.2543888986110687, 0.32489141821861267, -0.3142017126083374, 0.13338932394981384, -0.16488182544708252, -0.3242039978504181, 0.14363588392734528, 0.44525083899497986, -0.49595028162002563, -0.6111418604850769, 0.6859086751937866, -0.050158336758613586, -0.06839098781347275, 0.22448869049549103, -0.8532525300979614, 0.11058739572763443, 1.2345292568206787, 0.7332434058189392, 0.20848090946674347, -0.13373011350631714, -0.6238826513290405, 1.1243282556533813, -1.2630112171173096, -0.12493911385536194, 1.8444784879684448, 0.5006621479988098, 0.8080225586891174, -0.40419086813926697, -0.6314845681190491, 0.8817604184150696, -0.24113819003105164, -0.5698386430740356, -0.5316970944404602, 0.3281930685043335, -0.1665126234292984, 1.0433480739593506, -0.40085524320602417, 0.07243108004331589, 0.8722893595695496, -0.247977614402771, 1.5700666904449463, -0.3702675700187683, -0.9350367784500122, -0.0992186889052391, -0.05420401319861412, 0.5254204869270325, 0.7761963605880737, -0.7311214208602905, 0.31548556685447693, -1.2186557054519653, 0.03995944559574127, 0.4288710951805115, -0.09624770283699036, 0.29297760128974915, 0.32866746187210083, -0.5213654041290283, 0.6336265802383423, 0.6423673033714294, 0.7867226600646973, -0.4208586812019348, 0.8867594599723816, 0.692992091178894, 0.1747571974992752, -0.5348169803619385, 0.16563363373279572, 0.15076684951782227, 0.3799481987953186, -1.119802713394165, -0.04255553334951401, -0.19804613292217255, 0.8912469148635864, -0.19603881239891052, 0.61780846118927, -0.5001487135887146, 0.3378015458583832, 1.8022087812423706, -0.025472844019532204, 0.44219040870666504, -0.7483107447624207, 0.5281795859336853, -0.7736750841140747, 0.16899694502353668, -0.3266085684299469, -0.3138040602207184, -0.01200360432267189, -0.43926069140434265, -0.8328705430030823, -0.29341593384742737, -0.2848171591758728, -1.1760454177856445, 0.7797352075576782, -0.7088810205459595, -0.06264153122901917, 0.4282494783401489, 0.1234438568353653, 0.5486806035041809, 0.452231228351593, 0.7472533583641052, 0.577733039855957, 0.9813136458396912, -0.8771888613700867, -0.6223292946815491, -0.4274059236049652, 0.4040028154850006, -0.3930343985557556, -0.0004976798663847148, -0.428697407245636, -1.5295764207839966, -1.3340331315994263, -0.8187105655670166, 0.05306050926446915, -0.3007258176803589, 0.41721978783607483, 1.1065447330474854, 0.5482460260391235, -1.308065414428711, 0.679697573184967, -0.42464637756347656, -0.4020645320415497, 0.23082777857780457, 0.4788561165332794, 0.04047756642103195, -0.06714964658021927, -0.693260133266449, 0.14049483835697174, 0.4119897782802582, -0.1119793951511383, -0.4000275433063507, -0.27651017904281616, -1.3588597774505615, -0.2254336178302765, -0.5717706084251404, -0.6304299831390381, 1.486256718635559, -0.5663065910339355, -1.2852104902267456, -0.04356396198272705, -0.656892716884613, 0.35782748460769653, -0.08791659027338028, -0.07323194295167923, -0.9896509051322937, -0.1632469743490219, -0.5204458236694336, 1.2725690603256226, 0.4750674366950989, -0.6379407048225403, -0.24955101311206818, 0.17755091190338135, -0.21664617955684662, 0.20854473114013672, -0.47949618101119995, 1.014809012413025, -0.24794338643550873, -0.10794657468795776, 0.4942496120929718, 0.8056129217147827, -0.22946861386299133, -0.3421931862831116, -0.9011284112930298, -0.4851202070713043, 0.738142728805542, 0.12197339534759521, 0.8731534481048584, -0.9729474782943726, -0.6509557366371155, 0.004406146705150604, -0.28511935472488403, -0.5666045546531677, -1.3171051740646362, 0.7571426033973694, -0.5213451385498047, 0.22214311361312866, 0.22896422445774078, -1.4819297790527344, 0.23963512480258942, -0.4350139796733856, -0.4275361895561218, -0.6491779088973999, 0.004922252148389816, 0.9298874139785767, -0.4958319067955017, -0.018607359379529953, -0.04308204725384712, 0.3305196464061737, -0.5851507186889648, 1.016229510307312, -0.1503513753414154, 0.20613129436969757, 0.07925057411193848, 0.36180394887924194, 0.2327350378036499, 0.030745400115847588, 0.13511501252651215, -0.8882603049278259, -0.03603455424308777, 0.22929051518440247, -0.12460985034704208, 2.03971266746521, -0.2198948711156845, 0.3734665811061859, 0.06332247704267502, -0.21035020053386688, 0.10212419927120209, 0.7591577768325806, -0.08221714198589325, -0.2379782795906067, 0.6255212426185608, 0.33714497089385986, -0.2802460789680481, 0.19189636409282684, 1.2941993474960327, 0.8135101199150085, -0.4892691969871521, -0.08782272040843964, 0.8817046284675598, 0.10057439655065536, 0.6168738603591919, 0.3572341501712799, 0.4633157253265381, 0.17390118539333344, -0.581309974193573, 0.18977893888950348, -0.02501116879284382, -1.009303331375122, -0.32515859603881836, 0.3121641278266907, 0.551212728023529, 1.0443930625915527, 0.2212221473455429, -0.23839639127254486, -0.2403298169374466, -0.346108078956604, 0.8900985717773438, 1.415786623954773, 0.042969778180122375, -0.022937385365366936, -0.7867392301559448, -0.39010292291641235, -0.3029000461101532, -0.07971984148025513, -0.5457751154899597, 0.14139166474342346, 0.0027519080322235823, -0.542553186416626, 1.1585191488265991, 0.5006674528121948, 0.7603089213371277, -0.7138484716415405, -0.18320366740226746, -0.23728442192077637, 0.2860167920589447, -0.6912142038345337, -0.5392333269119263, 0.21644511818885803, -0.6807396411895752, -0.22221271693706512, -0.4085761606693268, -0.07961615920066833, 0.3231123983860016, -1.012007713317871, 0.4719778001308441, -0.8678512573242188, -0.1342257857322693, 0.5729656219482422, 0.555966854095459, -0.4947821795940399, -0.7377949357032776, 0.21894735097885132, -0.12209451198577881, -0.13898196816444397, 0.0012362744892016053, 0.8048824667930603, -0.051998499780893326, -0.042120303958654404, -0.4339476227760315, 0.17359548807144165, 0.3928647041320801, 0.2017597258090973, 0.5270694494247437, -0.40884116291999817, 0.2728607952594757, -0.5385230779647827, 1.1238888502120972, 0.062406279146671295, -0.01301101129502058, 0.24450406432151794, -0.09863999485969543, -0.4607540965080261, 0.06758347898721695, -0.30517327785491943, -0.4723023474216461, -0.502178430557251, 0.0504661500453949, -0.47020262479782104, -0.6232914924621582, 0.3186376094818115, 0.15260834991931915, 0.16675540804862976, -0.08267024159431458, 0.5024474263191223, 0.5995974540710449, 0.031255051493644714, 0.610657811164856, -0.8172488212585449, 0.6087612509727478, 0.22033430635929108, 0.0031708881724625826, -0.12124272435903549, -0.3213925063610077, -0.6419966220855713, -0.11336822062730789, -0.0820181742310524, -0.16449286043643951, -0.7490828633308411, 0.32503241300582886, -0.40945711731910706, -1.090566873550415, 0.19495970010757446, -1.280056118965149, 0.07965049892663956, -0.046068742871284485, 0.062425266951322556, -0.054239191114902496, -1.2072092294692993, -1.194311499595642, -0.7744118571281433, -0.5971602201461792, -1.5167236328125, 0.26923897862434387, 0.6438825130462646, -0.44870391488075256, -0.49753010272979736, -0.18453337252140045, -0.41333895921707153, 0.8596566319465637, -0.6389881372451782, 0.6151334643363953, 0.22801518440246582, 0.07898228615522385, -0.5014401078224182, 0.07500871270895004, 0.6586414575576782, -0.17890486121177673, 0.4517163038253784, -1.291201114654541, 0.38893359899520874, -0.7421914935112, -0.36431342363357544, 0.07328237593173981, 0.5826113820075989, 0.3923419415950775, 0.33929643034935, -0.3563072085380554, 0.1036662757396698, 1.1949150562286377, -0.12327823787927628, 0.07876448333263397, -0.31921181082725525, 0.9422005414962769, 0.5097162127494812, -0.7813721895217896, 0.5335935950279236, 0.4245454967021942, 0.4019109606742859, 0.48113709688186646, -0.4346066415309906, -0.6546303033828735, -0.8853060007095337, 0.5005928874015808, 1.4960578680038452, 0.2831728160381317, -0.01407101470977068, -0.6271501779556274, 0.571616530418396, -1.2650501728057861, -0.6483228206634521, 0.8825114369392395, 0.5424334406852722, -0.017278071492910385, -0.732721209526062, -0.04852499067783356, -0.04551495239138603, 0.7271292805671692, 0.16305002570152283, -0.25276979804039, -0.8895031213760376, 0.14270439743995667, 0.3122291564941406, -0.022360896691679955, 0.5178122520446777, -0.47267037630081177, 0.27812322974205017, 14.62765121459961, 0.5551709532737732, -0.08965881168842316, 0.24195022881031036, 0.5538661479949951, 0.2365155816078186, -0.5742164850234985, -0.12295020371675491, -1.277229905128479, 0.0772576853632927, 1.0623588562011719, 0.5481082201004028, 0.7024030089378357, 0.004255880136042833, -0.02076529525220394, 0.09783456474542618, -1.0606412887573242, 0.8081337213516235, 0.4701926112174988, -1.192707896232605, 0.48232829570770264, 0.1353243738412857, 0.3708965480327606, 0.4854063391685486, 1.0864404439926147, 0.7620001435279846, 0.21771599352359772, -0.5130165815353394, 0.8742282390594482, 0.6042160391807556, 0.8862885236740112, -0.2370724081993103, 0.3055422008037567, 0.012380837462842464, -1.0911921262741089, -0.5076016187667847, 0.0018801394617184997, -0.7610670328140259, 0.5155633091926575, -0.5320460796356201, -0.37489762902259827, -0.313245952129364, -0.32472938299179077, 0.5769090056419373, 0.035473864525556564, 0.5282060503959656, -0.10597996413707733, 0.3289172053337097, -0.34722471237182617, 0.11277230829000473, 0.31702640652656555, 0.9264435172080994, 0.18848444521427155, -0.016306230798363686, 0.08964040130376816, -0.4416670501232147, 0.03101189061999321, 0.3650786876678467, -0.6192004680633545, -0.38447305560112, -0.47190430760383606, -0.33018770813941956, -0.009148486889898777, 0.8974584341049194, 0.5621109008789062, 0.5319699048995972, -0.3830240070819855, 0.11335773020982742, 0.039665818214416504, 0.6005455851554871, -0.2582092881202698, -0.21819432079792023, 0.32804161310195923, -0.7096728682518005, 0.024927277117967606, 0.6497657299041748, -0.31034255027770996, -0.2659706175327301, -0.9528080821037292, 0.07767896354198456, 0.5415939688682556, -0.9773598909378052, -0.9412062168121338, 1.349832534790039, -0.21182715892791748, -0.5103469491004944, 0.05533119663596153, -0.8065462708473206, -0.5813555717468262, 0.5674668550491333, -1.2283910512924194, -1.218671441078186, 0.0446903333067894, 0.0061963023617863655, -0.5558049082756042, -0.5398221611976624, 1.365515112876892, 0.28757962584495544, -0.6022995114326477, 0.4385676980018616, -0.271017849445343, 0.19524259865283966, -0.4642111361026764, -0.4603935182094574, 0.30750566720962524, 0.2502397894859314, -0.2609402537345886, -0.0331295020878315, -0.08538588136434555, 0.48639488220214844, -0.6486833691596985, 0.0497756190598011, 0.37722596526145935, -0.1864253431558609, -0.3110933303833008, -0.4506462812423706, -0.3634997606277466, 0.3628705143928528, 1.1633968353271484, -0.42096421122550964, 0.4780767858028412, 0.299011766910553, -0.5505786538124084, -0.29752200841903687, -0.9168194532394409, 0.25246861577033997, -0.05869040638208389, -0.8911930918693542, 0.07616575807332993, -0.4080227017402649, 0.15890908241271973, -0.7069113850593567, -0.5023173093795776, -0.3799944519996643, 0.3138750195503235, -0.07271517068147659, 0.6229169964790344, -0.327551007270813, 0.5902618169784546, 0.7760714292526245, -0.4239550232887268, -0.7847831845283508, 0.5017852783203125, -0.864322304725647, 0.278392493724823, 0.38273921608924866, 1.0070438385009766, -0.19831892848014832, 0.6595681309700012, 1.174376130104065, -0.1898941993713379, -0.3543183207511902, -0.588317334651947, 0.19077706336975098, -0.2322034388780594, -0.7908604145050049, 0.54698246717453, -0.6014508008956909, 0.35372743010520935, -0.12275426089763641, 0.5022463798522949, 0.46844032406806946, -0.07242881506681442, -0.9285340309143066, 0.30729013681411743, 0.11704592406749725, 0.07183486968278885, -0.9106422662734985, -0.5904778242111206, -1.2940751314163208, 0.028962716460227966, -1.269150972366333, 0.21076159179210663, -0.9981757402420044, -0.3718203902244568, -0.006634526886045933, -0.5073729753494263, 0.4242512881755829, 0.8353109955787659, -0.4082717001438141, 0.044911857694387436, -0.5024182796478271, -0.4853552281856537, 0.9069482088088989, 1.0034517049789429, -0.8736327886581421, -0.15697774291038513, -0.19355295598506927, -0.05008203163743019, 0.5177220106124878, 0.35452762246131897, -0.246444433927536, -0.8414885401725769, -1.4297146797180176, -0.14026705920696259, 0.21099068224430084, 0.09481962025165558, -1.2130348682403564, 0.8825187087059021, 0.990589439868927, 0.14495410025119781, -0.27285799384117126, 0.8436857461929321, -1.0852874517440796, -0.5108675956726074, 0.09747930616140366, -0.9953465461730957, 0.4533923864364624, 0.5699931383132935, -0.822440505027771, -0.7293215990066528, 0.7119335532188416, -0.20753946900367737, -0.6390385627746582, -1.0963329076766968, 0.3727533221244812, -0.4913860261440277, -0.5615711212158203, -0.06387854367494583, -0.2790924310684204, -1.1406307220458984, -0.2971087396144867, -0.5337575078010559, 0.24094876646995544, -0.7339938879013062, 0.7866135835647583, 1.017393946647644, -1.2344285249710083, -0.050938837230205536, 0.5382356643676758, 0.12959028780460358, -0.37534308433532715, 0.8873770833015442, 0.6123140454292297, -0.12192822992801666, 0.043763477355241776, -0.19968979060649872, 0.08175241947174072, -0.7763808965682983, -0.6140203475952148, 0.8184487223625183, -0.13154064118862152, 0.2976559102535248, 1.4250099658966064, -0.544661283493042, -1.111913800239563, 0.36701714992523193, -0.9876289963722229, -0.4626031517982483, -0.048030830919742584, 0.6237046122550964, 0.08288918435573578, -0.4294112026691437, -0.2857206165790558, -0.1703394204378128, 0.150884211063385, -0.38244810700416565, -1.0835824012756348, -0.014433599077165127, -0.09076042473316193, -0.02582828514277935, 0.46070432662963867, 0.8753305077552795, -1.0551737546920776, -1.0951937437057495, -0.14219607412815094, -0.4524623155593872, 0.25689923763275146, 0.14706701040267944, -0.6431082487106323, -1.0699641704559326, 0.9393369555473328, 0.837568998336792, 0.2761892080307007, 0.22122597694396973, 0.08996109664440155, 0.2381340116262436, 0.3999485373497009, -0.13019241392612457, -0.6383986473083496, -0.45176661014556885, 1.1287516355514526, 1.3312792778015137, -0.7180688381195068, -0.19046731293201447, 0.13579918444156647, -0.836005687713623, 1.1269630193710327, 0.33907878398895264, 0.3087037205696106, 0.9823125600814819, 0.031110920011997223, 0.38234585523605347, 0.38210609555244446, -0.7989611029624939, -0.06508350372314453, 0.6928105354309082, 1.4002212285995483, 0.8177414536476135, 0.45899105072021484, 0.5715641975402832, 0.7821223139762878, 0.11804968118667603, 0.12061949074268341, 0.16070051491260529, 0.5927635431289673, 0.301363468170166, -0.30685311555862427, -0.0540156289935112, 0.47469833493232727, -0.21545147895812988, -0.6278080344200134, 0.5003552436828613, 0.7248218655586243, 0.39553242921829224, 0.9347176551818848, 0.7967220544815063, -0.5834560990333557, 0.6217842698097229, 0.16396436095237732, 0.8668010234832764, -0.25540605187416077, -0.4043991267681122, 0.2543005347251892, -0.5808299779891968, -0.18985798954963684, -0.3363753855228424, -0.5797860622406006, -0.5546436905860901, 0.07201530039310455, 0.49459365010261536, -0.24263498187065125, 0.8682846426963806, 0.9661943316459656, 0.6674534678459167, 0.6492851376533508, -0.22582873702049255, -1.1718909740447998, -0.015954094007611275, -1.2231755256652832, -0.1165701150894165, -0.5710421204566956, -0.18151220679283142, -0.12792804837226868, 0.08395891636610031, -0.12308141589164734]}, "authors": [{"authorId": "1581807199", "name": "S. Wadekar"}, {"authorId": "3422735", "name": "Abhishek Chaurasia"}, {"authorId": "2258718937", "name": "Aman Chadha"}, {"authorId": "2303466814", "name": "Eugenio Culurciello"}], "references": [{"paperId": "ce68430823b79dd3d478c505cc2761f03cf72b30", "title": "What matters when building vision-language models?"}, {"paperId": "bb6afe666ffd07d9059ec94cac551c2b1f33f096", "title": "SpikeMba: Multi-Modal Spiking Saliency Mamba for Temporal Video Grounding"}, {"paperId": "40e996a7c3e914a67c708704fa9b4c54ea70f36e", "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference"}, {"paperId": "6d49ed0ea24b9c218f5ec6731cd261ce618df2ac", "title": "VL-Mamba: Exploring State Space Models for Multimodal Learning"}, {"paperId": "6675bcf6dc97c87da7afda223938ec7e51ecc3b2", "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"}, {"paperId": "835424063bf3c4035f96374a60147d821d474c2a", "title": "UniCode: Learning a Unified Codebook for Multimodal Large Language Models"}, {"paperId": "97388c71be60282e149c2c3d00db7c0eb2c946e4", "title": "MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models"}, {"paperId": "b14e5138d4d1f3577b2390541dc7b730a41bb651", "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding"}, {"paperId": "e291850b23d1c1ec49bc68e9e9266880898216b2", "title": "The (R)Evolution of Multimodal Large Language Models: A Survey"}, {"paperId": "9cf6e252b8a5566910e6fceda7d6a25c5b54be33", "title": "VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language Models with Autonomous Instruction Optimization"}, {"paperId": "1cfb7fba7194860e8b8818eb5e87e0a8e14e518a", "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling"}, {"paperId": "ec8e2b45c4601730015608a58e33409224a81228", "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models"}, {"paperId": "c7a66961ee07b0e9e792d3625c1b20d510f29429", "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations"}, {"paperId": "a091bf215c716a146140f81c751712db628c8e20", "title": "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model"}, {"paperId": "cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3", "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"}, {"paperId": "bce43cb9af37a0c8d90f8cadaebd6bb002685edd", "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model"}, {"paperId": "a050c9b0c321839e4427ab9defa3463be7825ac4", "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models"}, {"paperId": "a3ae8705daa21f4be6970d8c7685112e1e95b843", "title": "KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning"}, {"paperId": "4f2a56102bcbf0fe79379c4c27daecbccfb35a26", "title": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"}, {"paperId": "616e98ba9e60f36c6ee226cc66c787610f0bbb62", "title": "MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer"}, {"paperId": "5502d769595981009e43344f8914e287acca2359", "title": "ModaVerse: Efficiently Transforming Modalities with LLMs"}, {"paperId": "ece33ee67d74c29cd2a83c505e5bf0b818f9c2a1", "title": "LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model"}, {"paperId": "6c64ddd2190909de2c680dd18abc9b92e80c39f9", "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action"}, {"paperId": "98ab627dd147db88b5e5cfa9a74f1bd8da110021", "title": "TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones"}, {"paperId": "6a33e58ef961a3a0a5657518b2be86395eb7c8d0", "title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"}, {"paperId": "c672ec79f55cef8f7a32cd8dddfa981b893f1567", "title": "V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"}, {"paperId": "b5503967d39557a77c70076c308183e92d6d775a", "title": "Osprey: Pixel Understanding with Visual Instruction Tuning"}, {"paperId": "ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7", "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation"}, {"paperId": "1608e505c3e749a00ce0c56e0c2d53e0e9ae7fe4", "title": "CogAgent: A Visual Language Model for GUI Agents"}, {"paperId": "2141ed804636a1cf339d606cd03fd3b3e9582133", "title": "VILA: On Pre-training for Visual Language Models"}, {"paperId": "13925d7d5952b1ba5960dfe2c44d977be109b636", "title": "4M: Massively Multimodal Masked Modeling"}, {"paperId": "c95c4fb96868d6512c32988632a7b101a42c455d", "title": "Dolphins: Multimodal Language Model for Driving"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "e263e08a20080a2543d0ca29d3d63c4717a8beb6", "title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning"}, {"paperId": "ed4e20bcd73b1138d3bb2ed4dbbf5e8b224ef5c7", "title": "mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model"}, {"paperId": "78582ad19779a69d97b797a3c6eb2397f99398b6", "title": "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation"}, {"paperId": "52941cadbd340344f3e0a6f50719fe55b3de5088", "title": "Multimodal Large Language Models: A Survey"}, {"paperId": "f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560", "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"}, {"paperId": "dfa7120276a0a5d36c40de13278c9884305b7c7d", "title": "DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding"}, {"paperId": "98b69e478d2d4e4cf1a0befcdb27c4f220fc0a4b", "title": "LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge"}, {"paperId": "0f993809c1fe00403ecea66d8f572832f075cfe4", "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning"}, {"paperId": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada", "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"}, {"paperId": "619184447595337a9fe3dca72c4e951e7ab7467c", "title": "To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning"}, {"paperId": "bf14244669d5505f63343d4365d99d24aa6c5e82", "title": "Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models"}, {"paperId": "ad13b213681b6f634bc83a264df246e83dd9a9d9", "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"}, {"paperId": "2313afae52d98e569da2dedbf14daf9efc74e7cf", "title": "CogVLM: Visual Expert for Pretrained Language Models"}, {"paperId": "f72be31de9f9a09d4410fd38bc717efe43444827", "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models"}, {"paperId": "1ddbd08ad8cf22a5c66c4242194c4286328533bf", "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "5ba1525dc6d382ee0a4a1ca3c64fc5907ca64c67", "title": "Making LLaMA SEE and Draw with SEED Tokenizer"}, {"paperId": "09565c0f4311f06c264086f2036a744dd2c92b62", "title": "Toloka Visual Question Answering Benchmark"}, {"paperId": "f2f9c02a7eb484dd7b7ac46892856e3f278eed77", "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model"}, {"paperId": "c1e450284e7d6cac1855330a1197df8537df653f", "title": "InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition"}, {"paperId": "f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a", "title": "Kosmos-2.5: A Multimodal Literate Model"}, {"paperId": "fa75a55760e6ea49b39b83cb85c99a22e1088254", "title": "NExT-GPT: Any-to-Any Multimodal LLM"}, {"paperId": "1a735015a1f7ef4f2ba2273ce5fcaaacfa9d1ea2", "title": "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning"}, {"paperId": "afb39ed837db8750dd1c3b2a54ad442372c106b2", "title": "WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models"}, {"paperId": "30cc95639cffca4ffa8c0eafbc502636c0c88fa5", "title": "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"}, {"paperId": "7fbc502441d66daf1f53765d5d86a8dfba9ab0ce", "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"}, {"paperId": "060e0df71620f1844839f6a993dfa5fb8e4c3bf6", "title": "Android in the Wild: A Large-Scale Dataset for Android Device Control"}, {"paperId": "2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f", "title": "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"}, {"paperId": "962ccf1fc49c83817fb031e5b24b81b19cdfb89d", "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"}, {"paperId": "94053805cd59f2e9a47fe3f080c7e7afefb337cc", "title": "Generative Pretraining in Multimodality"}, {"paperId": "451a3f03aca4aa87b93981364842137417549e58", "title": "SVIT: Scaling up Visual Instruction Tuning"}, {"paperId": "094883e42bb9a41f602c0715c1059bc431e33fb2", "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"}, {"paperId": "e2a58fd18961c3941102989e3a3d0d27c615e015", "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"}, {"paperId": "948e8cfae92c2004f2dd5c9316f5972f8baaea21", "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"}, {"paperId": "ecf38d702bea560c1f9d372645e2be7661a71f37", "title": "MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing"}, {"paperId": "58f8925a8b87054ad0635a6398a7fe24935b1604", "title": "Mind2Web: Towards a Generalist Agent for the Web"}, {"paperId": "bf7025a2e5dbb3c09deae02a1aa98a256ca559e2", "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"}, {"paperId": "5d321194696f1f75cf9da045e6022b2f20ba5b9c", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"}, {"paperId": "f22d71c7ce9720ba1f717a4f1181488200e78198", "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"}, {"paperId": "3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a", "title": "PaLI-X: On Scaling up a Multilingual Vision and Language Model"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "6fb5c0eff3696ef252aca9638e10176ecce7cecb", "title": "Generating Images with Multimodal Language Models"}, {"paperId": "d3f79210b54e168c76b8c311488f42d7d1048b81", "title": "PandaGPT: One Model To Instruction-Follow Them All"}, {"paperId": "00cb69a9f280317d1c59ac5827551ee9b10642b8", "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"}, {"paperId": "2ad8183c72a90511383a32ccaeea313eb85f4085", "title": "DetGPT: Detect What You Need via Reasoning"}, {"paperId": "9f411fda2ad5b141a3115f707bcf5ee865b3fb94", "title": "Any-to-Any Generation via Composable Diffusion"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "81e7e82245c2f230eeb8aaaa1a2b2604c143754a", "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"}, {"paperId": "43e6e8d6663d83f1b74cf5a2be7b040b0928f867", "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"}, {"paperId": "d6d3604f369bb0415cbe814e43ca3131323b03e2", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning"}, {"paperId": "570079bbdd8758dfe865097e05719313c9c1301a", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"}, {"paperId": "f9570989919338079088270a9cf1a7afc8db8093", "title": "DataComp: In search of the next generation of multimodal datasets"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "df958800014d310b6df34ad83d771314d68fbb2d", "title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text"}, {"paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea", "title": "Instruction Tuning with GPT-4"}, {"paperId": "a757999ed260d7bc45484dc6b4456bf33fe6f679", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"}, {"paperId": "9ff9ea4a504874a04b674002f090a650a1efe9a0", "title": "On the De-duplication of LAION-2B"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "746bb45433f6b24d3ae64d6cd51c4e9d00a0ffa7", "title": "Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey"}, {"paperId": "6173520a1eb2814d067e8c5fd16212b7cbf6ee78", "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "a2d2bbe4c542173662a444b33b76c66992697830", "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions"}, {"paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"}, {"paperId": "d3135733aa39dec20ce72aa138589dda27c8406d", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c", "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"}, {"paperId": "622428f5122ad12a40229e1768ecb929fd747ee7", "title": "Multimodal Learning With Transformers: A Survey"}, {"paperId": "47a67e76ed84260ff19f7a948d764005d1edf1c9", "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "b1fc7d96d732f99510658c73a8d9da3fd7b25923", "title": "MultiMAE: Multi-modal Multi-task Masked Autoencoders"}, {"paperId": "b611c501269224702d1a9942c8600a31ec66ab28", "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "fe0e647ac5bbe9b127caddfcb52b9f723d6f158c", "title": "Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark"}, {"paperId": "2fd6f77540c1cc8e70b96208ccf9971b4251fc02", "title": "FLAVA: A Foundational Language And Vision Alignment Model"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "9bcf3b43f2323a194036cc52c6878a9b1dc7e058", "title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "1b937ff4b05e2b56c2c2fcdfa5baa3085cd5a08c", "title": "NExT-QA: Next Phase of Question-Answering to Explaining Temporal Actions"}, {"paperId": "fb1c90806fc5ec72987f58110aa255edbce6620d", "title": "Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning"}, {"paperId": "5fda8a8bc80ee26cbf618fc555ea41b9950a5725", "title": "Spoken Moments: Learning Joint Audio-Visual Representations from Video Descriptions"}, {"paperId": "d5611a92619548e7f2af5adb04070574c0dacac1", "title": "InfographicVQA"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "394be105b87e9bfe72c20efe6338de10604e1a11", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "cb596bffc5c5042c254058b62317a57fa156fea4", "title": "Unifying Vision-and-Language Tasks via Text Generation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1", "title": "Learning to summarize from human feedback"}, {"paperId": "3fb85a349adc758a0c72a96fb0065765ac3b0945", "title": "CoVoST 2 and Massively Multilingual Speech-to-Text Translation"}, {"paperId": "b40bfcf339de3f0dba08fabb2b58b9368ff4c51a", "title": "DocVQA: A Dataset for VQA on Document Images"}, {"paperId": "33eadd4e666a894306a22ba0839c5e0cef77280e", "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"}, {"paperId": "29c3242cce0c78f96d7d90e0123b95cb0840f21a", "title": "On the General Value of Evidence, and Bilingual Scene-Text Visual Question Answering"}, {"paperId": "8b2c80788f789d4ce7849c13943fa920d9e3c95f", "title": "Captioning Images Taken by People Who Are Blind"}, {"paperId": "439369de9514e41e0f03fed552d8f6e5aebf51b2", "title": "Connecting Vision and Language with Localized Narratives"}, {"paperId": "c5ff974a69fd0c760b4855b819e61e89f31cfffe", "title": "Objects365: A Large-Scale, High-Quality Dataset for Object Detection"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "1097cf8cf5961589ff693b069002e7181e24e631", "title": "OCR-VQA: Visual Question Answering by Reading Text in Images"}, {"paperId": "d0818dac77eee5b970736e57a478bcedfb1b15fe", "title": "KVQA: Knowledge-Aware Visual Question Answering"}, {"paperId": "0033346700dc450ac22c9b704eab0e906d868662", "title": "Scene Text Visual Question Answering"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907", "title": "Towards VQA Models That Can Read"}, {"paperId": "28b74bb7c8b08cceb2430ec2d54dfa0f3225d796", "title": "VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "cf336d272a30d6ad6141db67faa64deb8791cd61", "title": "A Corpus for Reasoning about Natural Language Grounded in Photographs"}, {"paperId": "634161e4759616dbe06f0b1465999d3df122f366", "title": "TallyQA: Answering Complex Counting Questions"}, {"paperId": "9d0907770cd4619aa6a36139a859e8f09bc9f0ef", "title": "Textual Explanations for Self-Driving Vehicles"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"}, {"paperId": "057b80e235b10799d03876ad25465208a4c64caf", "title": "Video Question Answering via Gradually Refined Attention over Appearance and Motion"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "title": "Multimodal Machine Learning: A Survey and Taxonomy"}, {"paperId": "96dd1fc39a368d23291816d57763bc6eb4f7b8d6", "title": "Dense-Captioning Events in Videos"}, {"paperId": "e10a5e0baf2aa87d804795af071808a9377cc80a", "title": "Towards Automatic Learning of Procedures From Web Instructional Videos"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "1b80416cc2b05954941ac7e7dcbcc358c10e5ace", "title": "Visual Dialog"}, {"paperId": "3a7011346ce939e3251915e92ae2f252e4c7f777", "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs"}, {"paperId": "bc9db6117d0026bc5b11eeba2303d2bddc96c306", "title": "Multi30K: Multilingual English-German Image Descriptions"}, {"paperId": "e18ec2c9f0b4a817b8cf0435822bbc879d7db698", "title": "A Diagram is Worth a Dozen Images"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "b8e2e9f3ba008e28257195ec69a00e07f260131d", "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"}, {"paperId": "def584565d05d6a8ba94de6621adab9e301d375d", "title": "Visual7W: Grounded Question Answering in Images"}, {"paperId": "e65142010431ffc089b272a1174214e00693e503", "title": "Generation and Comprehension of Unambiguous Object Descriptions"}, {"paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, {"paperId": "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "title": "Deep visual-semantic alignments for generating image descriptions"}, {"paperId": "92c141447f51b6732242376164ff961e464731c8", "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "44ddac48353ead135eef4096859956eaa31be2a5", "title": "Learning Factored Representations in a Deep Mixture of Experts"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": "883d1d06d857a85a0e64bb19f0b17d56f2cc9d7b", "title": "KenLM: Faster and Smaller Language Model Queries"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": null, "title": "Llava-next: Improved reasoning, ocr, and world knowledge"}, {"paperId": "5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a", "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"}, {"paperId": "772724892819d7e6f15ce536753fdc32d022c0e0", "title": "A Survey on Image-text Multimodal Models"}, {"paperId": "84dc889beff9d51fe429cff8c92735e7410ee3c2", "title": "Aligning Large Multi-Modal Model with Robust Instruction Tuning"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": "b5620c33b571b6f49a4eaecb4cd790076461591d", "title": "How to Multimodal"}, {"paperId": "327ca68ef78290f784bdc2d405d0d4f5cb96d33b", "title": "State Space"}, {"paperId": "8b55402ffee2734bfc7d5d7595500916e1ef04e8", "title": "nocaps: novel object captioning at scale"}, {"paperId": null, "title": "Enhanced visual instruction tuning for text-rich image"}, {"paperId": null, "title": "language enhanced multi-modal"}, {"paperId": null, "title": "Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices"}, {"paperId": null, "title": "InternLM Team"}, {"paperId": null, "title": "Minigpt-4: En-hancing vision-language understanding with advanced large language models"}, {"paperId": null, "title": "Teams ShareGPT"}, {"paperId": null, "title": "language-vision pretraining in"}]}