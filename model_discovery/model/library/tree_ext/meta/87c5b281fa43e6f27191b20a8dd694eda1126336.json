{"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "abstract": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 1048, "influentialCitationCount": 98, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes."}, "embedding": {"model": "specter_v2", "vector": [0.6172780990600586, 0.7520630359649658, -0.3539433181285858, -0.03575265035033226, -0.06178342550992966, -0.0149446502327919, 0.6638431549072266, -0.06037955731153488, -0.5026991367340088, -0.38021355867385864, 0.7089957594871521, 0.015173307619988918, 0.4369392693042755, 0.19875843822956085, -0.1600653976202011, -0.005179234314709902, -0.7192690372467041, 0.12227800488471985, 0.2676020562648773, -0.21631646156311035, 0.434359073638916, -0.7405332326889038, -1.5109362602233887, 0.29943111538887024, 0.33615827560424805, 0.7661659717559814, 0.445627361536026, 1.1059441566467285, -0.4496179521083832, 0.5902834534645081, 0.32083791494369507, -0.08864044398069382, -0.09987935423851013, -0.012880275957286358, -0.6530030369758606, -0.5279712080955505, 0.7900270819664001, -0.05501015484333038, -0.4923452138900757, 0.7809520363807678, -0.21163493394851685, 0.26020240783691406, 0.5075092315673828, -0.5748135447502136, -0.25180843472480774, 0.6299575567245483, 0.49043816328048706, 0.9510285258293152, -0.3443111777305603, -0.266437292098999, 1.5602593421936035, -1.0907536745071411, 0.3529655933380127, 1.2963576316833496, 0.5279422402381897, 0.3890465795993805, -0.019472826272249222, -0.2204226404428482, 0.9913663268089294, 0.39015892148017883, -0.4853484332561493, -0.3560275733470917, -0.3521076738834381, 0.03858219459652901, 2.407686948776245, -0.16492553055286407, 0.1779009848833084, 0.5108150243759155, 0.3807927966117859, 1.4049333333969116, -0.18885190784931183, -0.7660995721817017, -0.21785491704940796, -0.1961984485387802, 1.0428205728530884, 0.6975463628768921, -0.5300227999687195, 0.19560958445072174, -0.8104425668716431, -0.05261872708797455, 0.5489365458488464, 0.009228752925992012, -0.02861608751118183, -0.19422298669815063, -0.5418476462364197, 0.2798840403556824, 0.443897545337677, 0.8131591081619263, -0.3451565206050873, 0.8408750295639038, 0.38823971152305603, -0.04486478120088577, -0.25838014483451843, 0.5382736325263977, -0.008921320550143719, -0.09847889840602875, -0.8672237992286682, 0.1321786344051361, -0.0874284878373146, 1.2693431377410889, -0.266208291053772, 0.18489329516887665, -0.7977134585380554, -0.05166897177696228, 0.9528168439865112, 0.16081713140010834, 0.5589205622673035, -0.7195149660110474, 0.001868709921836853, -0.708102285861969, 0.13559040427207947, -0.8964430093765259, -0.38957005739212036, -0.41118645668029785, -0.8112202882766724, -0.8525813817977905, -0.29906079173088074, 0.39449000358581543, -0.6229498386383057, 0.5260091423988342, -0.13239648938179016, 0.24330106377601624, -0.1809510886669159, 0.5599737763404846, 0.7521975040435791, 0.6116263270378113, 0.4803915321826935, 0.3401203751564026, 1.1957569122314453, -1.245941400527954, -0.5124338269233704, -1.248795747756958, 0.5762201547622681, 0.030274026095867157, 0.4485224187374115, 0.04294978827238083, -1.129233479499817, -1.1836581230163574, -0.8640741109848022, -0.2867141664028168, -0.522300124168396, 0.02271018549799919, 0.9060413837432861, -0.022918255999684334, -1.118521809577942, 0.8719274997711182, -0.18688750267028809, -0.44281378388404846, 0.4044521450996399, 0.02832799032330513, 0.4030953049659729, -0.23866407573223114, -1.1742057800292969, 0.12471745163202286, 0.20536203682422638, -0.6279309391975403, -0.13512276113033295, -0.8123887181282043, -1.0146186351776123, 0.29209426045417786, 0.3271637558937073, -0.4299037456512451, 1.2772082090377808, -0.5518581867218018, -0.9816003441810608, 0.7689164876937866, -0.7157558798789978, -0.015248667448759079, -0.05931247025728226, -0.2404860109090805, -0.61525958776474, -0.27499836683273315, -0.44949084520339966, 0.8619251847267151, 0.610939621925354, -0.02519110217690468, -0.4637793302536011, 0.1308867484331131, -0.42318499088287354, 0.0006028989446349442, -0.39267247915267944, 0.833828330039978, -0.5933490991592407, -0.17075850069522858, 0.09078912436962128, 0.5572072267532349, 0.23410634696483612, -0.25652071833610535, -0.2512041926383972, -1.087240219116211, 0.6598801016807556, 0.2471473067998886, 1.1386220455169678, -1.0152466297149658, -0.7787086963653564, -0.4791594445705414, 0.1145944818854332, 0.019159995019435883, -0.8638513088226318, 0.4101576805114746, -0.4836386740207672, 0.2297268956899643, -0.09765437245368958, -0.7218037843704224, 0.07300924509763718, -0.3967016935348511, -0.6802500486373901, -0.4041568636894226, 0.09285125136375427, 1.2620717287063599, -1.0536768436431885, -0.16506879031658173, -0.29695212841033936, 0.10128863155841827, -0.9148344993591309, 1.035834550857544, -0.6867108345031738, 0.022133970633149147, -0.15375575423240662, -0.4136488139629364, -0.08258858323097229, -0.6778101325035095, 0.3476293087005615, -0.5411383509635925, -0.09972300380468369, 0.5283427834510803, -0.12598517537117004, 1.163114309310913, -0.12493478506803513, 0.6096486449241638, -0.23799872398376465, -0.4244752526283264, 0.2525562345981598, -0.008312730118632317, -0.5167480111122131, -0.7042014598846436, 0.13931655883789062, -0.13537253439426422, -0.4166155457496643, 0.15778806805610657, 0.9892204999923706, 1.3011236190795898, -0.27525144815444946, 0.16787482798099518, 0.40318724513053894, -0.14630098640918732, 0.14675742387771606, 0.42057284712791443, 1.0845811367034912, 0.6438641548156738, 0.4745132327079773, -0.3596021831035614, 0.05708838254213333, -0.3409242331981659, -0.16078688204288483, 0.813315749168396, 0.5703228712081909, 0.9113394021987915, 0.40061071515083313, -1.13152277469635, -0.6857319474220276, 0.5815854072570801, 0.6099802851676941, 1.5719597339630127, -0.1484687477350235, -0.08876613527536392, -0.9548100829124451, -0.4018418490886688, -0.40223848819732666, 0.3193213939666748, -0.6647273302078247, -0.36402252316474915, -0.49031463265419006, -0.9202783703804016, 0.6344155669212341, 0.6115838885307312, 1.0122466087341309, -1.0407015085220337, -0.8201585412025452, -0.2510150074958801, 0.40327391028404236, -0.834062933921814, -0.7239972949028015, 0.641912043094635, -0.4879594147205353, -0.06904322654008865, 0.18086932599544525, -0.07170737534761429, 0.17338187992572784, -0.34782856702804565, 1.3178623914718628, -0.3922521471977234, -0.3667062819004059, 0.3387857973575592, 0.5183494687080383, -0.5104405879974365, -0.25866562128067017, 0.5884982347488403, -0.06995424628257751, -0.15106883645057678, 0.6637875437736511, 0.3404952585697174, -0.18700750172138214, 0.10731044411659241, -0.13613614439964294, 0.23185515403747559, 0.17496368288993835, 0.07386667281389236, 0.7765219807624817, -0.4434679448604584, -0.15164300799369812, -1.2050234079360962, 0.330888569355011, 0.17000223696231842, -0.20175829529762268, 0.21454761922359467, -0.6100414991378784, -0.3123096227645874, 0.5475654602050781, -0.6768979430198669, -0.33948463201522827, -0.8869591355323792, 0.37536463141441345, -0.36131104826927185, -0.6233625411987305, -0.11097564548254013, 0.05020355060696602, 0.1906851828098297, 0.5124228596687317, 0.5706291794776917, 0.04883963242173195, 0.10557380318641663, 0.3893340229988098, -0.7177426218986511, 0.766459047794342, 0.3654486835002899, -0.4161159098148346, -0.27712079882621765, 0.032678913325071335, -0.9401581883430481, -0.6885688304901123, -0.38392090797424316, -0.47378888726234436, -0.20868024230003357, 0.40202438831329346, -0.3038114309310913, -1.1136589050292969, 0.0003821348655037582, -1.1380478143692017, -0.7577602863311768, 0.10923178493976593, -0.5957343578338623, -0.2240869402885437, -1.0467054843902588, -0.8304648399353027, -0.41115429997444153, -0.7927471995353699, -0.6683166027069092, 0.4841407239437103, 0.09896272420883179, -0.8250455260276794, -0.5348761677742004, 0.002851971657946706, -0.6656050682067871, 0.9673510193824768, -0.5333172082901001, 0.6941323280334473, -0.10236439853906631, -0.2877742648124695, -0.0744774267077446, -0.1064453199505806, 0.07518704235553741, -0.3544892966747284, 0.106593556702137, -0.9091957211494446, 0.4065103232860565, -0.412911593914032, -0.24561594426631927, 0.3552417755126953, 0.3050817549228668, 1.0092273950576782, -0.04126899316906929, -0.8003825545310974, 0.3231475055217743, 1.3648617267608643, -0.48170018196105957, 0.41535520553588867, 0.0852934792637825, 1.1035041809082031, 0.15127457678318024, 0.2551427185535431, 0.6147639751434326, 0.28700026869773865, 0.3884134590625763, 0.3857250511646271, 0.05050555616617203, -0.3793364465236664, -0.4508550763130188, 0.22325561940670013, 1.1362782716751099, 0.3356510102748871, -0.1291557401418686, -1.2695797681808472, 0.7841653823852539, -1.1737126111984253, -0.8368094563484192, 0.21703818440437317, 0.7313897013664246, 0.3139403164386749, -0.4652198851108551, -0.2383582442998886, -0.4869445562362671, 0.3502572476863861, 0.30979666113853455, -0.660436749458313, -0.7652201056480408, -0.09734435379505157, 0.4839588403701782, 0.08789698779582977, 0.6593204140663147, -0.3724498152732849, 0.718393862247467, 14.95496654510498, 0.8196767568588257, -0.0650206059217453, 0.42861607670783997, 1.0690635442733765, 0.2103746235370636, -0.28645336627960205, -0.22390557825565338, -1.477713704109192, -0.05628925561904907, 1.345420479774475, 0.5269689559936523, 0.34963786602020264, 0.26142579317092896, -0.14691685140132904, 0.02014574222266674, -0.6814020276069641, 0.6331366896629333, 0.7187029123306274, -1.0644627809524536, 0.164342001080513, 0.2843676507472992, 0.07962795346975327, 0.3012599050998688, 0.7776878476142883, 0.6748208403587341, 0.6470866203308105, -0.5025091767311096, 0.3808961510658264, 0.7213877439498901, 1.0734039545059204, -0.11470398306846619, 0.24649734795093536, 0.3495519757270813, -0.663539707660675, -0.24010618031024933, -0.7145435810089111, -1.1511048078536987, 0.04516589641571045, -0.024240173399448395, -0.4069506824016571, -0.6710843443870544, 0.07162773609161377, 0.5824494957923889, 0.06534576416015625, 0.32547828555107117, -0.13254794478416443, 0.5314828753471375, 0.17320673167705536, -0.45570749044418335, 0.43227821588516235, 0.8409883975982666, 0.1635635793209076, 0.3481162488460541, 0.0893411934375763, 0.4228368103504181, -0.12651345133781433, 0.40235137939453125, -0.0568862110376358, -0.1930747926235199, -0.22387360036373138, 0.012150870636105537, -0.07598147541284561, 1.001942753791809, 0.6463080048561096, 0.2589254379272461, -0.2626872956752777, 0.4709463119506836, 0.6903507113456726, -0.07342659682035446, -0.26760485768318176, -0.24139203131198883, 0.3187265992164612, -0.40164703130722046, 0.3470536172389984, 0.6414241790771484, -0.2575155198574066, -0.21997250616550446, -1.032713770866394, -0.6447099447250366, 0.4363956153392792, -1.068786382675171, -0.5382899641990662, 0.986747682094574, -0.39576196670532227, -0.30838897824287415, 0.31628015637397766, -1.003682017326355, -0.5317608714103699, 0.3672581613063812, -1.0624213218688965, -0.6366735100746155, -0.20358975231647491, -0.5880830883979797, -0.31631365418434143, 0.2773054838180542, 1.300573706626892, -0.3603355586528778, -0.11618516594171524, 0.032571520656347275, -0.10042810440063477, -0.24213449656963348, -0.2797200381755829, -0.9374951720237732, 1.1585581302642822, 0.06300462037324905, -0.20025977492332458, 0.011522025801241398, -0.021672504022717476, 0.06708060950040817, -0.875782310962677, -0.1504029631614685, 0.7873048782348633, -1.2026301622390747, -0.3519105911254883, -0.755520761013031, -1.0962406396865845, 0.34137487411499023, 0.7210783362388611, -0.033255092799663544, 0.13883008062839508, 0.30477479100227356, -0.6173369884490967, -0.10323640704154968, -0.3648661673069, 0.2548592984676361, 0.8253781795501709, -0.2641550898551941, -0.4247322678565979, -0.4941866099834442, 0.45284005999565125, -0.9041918516159058, -0.5866795182228088, -0.4892764091491699, 0.49195513129234314, -0.24660472571849823, 0.7593587636947632, -0.5117127895355225, 0.8374724984169006, 1.007871150970459, 0.19377483427524567, -0.6875730156898499, -0.4443127512931824, -0.7202193737030029, -0.18654392659664154, 0.30662423372268677, 0.5835911631584167, -0.383003830909729, 0.4013247787952423, 0.8100597858428955, 0.20033800601959229, -0.4797888696193695, -0.32594069838523865, -0.2001926153898239, 0.04606594890356064, -0.35841938853263855, 0.48038944602012634, 0.07311460375785828, 0.3046860992908478, 0.2722366452217102, 0.5935335755348206, 0.3434978425502777, -0.22834523022174835, -0.49424082040786743, 0.48898813128471375, -0.23990145325660706, -0.24785050749778748, -0.6772170066833496, -0.414033979177475, -1.6791287660598755, -0.2271021008491516, -0.8536238074302673, 0.2401125431060791, -0.7480528354644775, -0.41570547223091125, 0.11620927602052689, -0.19574210047721863, 0.3343134820461273, 0.036527328193187714, -0.5127909183502197, -0.5600181221961975, -0.6506931185722351, -0.9526785612106323, 0.44314295053482056, 0.9208828210830688, -0.5160717368125916, 0.2825995683670044, -0.3562465310096741, -0.17558307945728302, 0.19833822548389435, 0.5173282027244568, -0.34389305114746094, -0.5575456023216248, -1.2141379117965698, 0.4019835591316223, -0.05160631984472275, -0.02904180809855461, -0.9902181029319763, 0.8307195901870728, 0.5290020108222961, -0.0796465128660202, -0.24269786477088928, 0.30106082558631897, -0.7814949154853821, -0.6231415271759033, 0.24843347072601318, -0.8207021355628967, 0.21934404969215393, 0.6448614001274109, -0.5066605806350708, -0.07275815308094025, 0.42938104271888733, -0.08523745834827423, -1.1330676078796387, -1.040745735168457, 0.28917571902275085, -0.5680419206619263, 0.37094753980636597, -0.5065025687217712, -0.2518850862979889, -1.403469204902649, -0.30266284942626953, -0.0183948315680027, 0.33961376547813416, -0.24668799340724945, 0.8471115827560425, 0.3128385543823242, -0.8536791205406189, 0.26571542024612427, 0.06928962469100952, 0.03683670982718468, 0.07178403437137604, 0.8434232473373413, 0.44993096590042114, -0.06134829670190811, 0.6819931268692017, 0.013653590343892574, 0.07887732237577438, -1.0142781734466553, 0.4667305648326874, 0.6473658680915833, -0.5032445788383484, -0.1630721241235733, 0.9211786389350891, -0.10306500643491745, -0.8655965328216553, 0.1875579059123993, -1.192398190498352, -0.636188268661499, -0.30322790145874023, 0.95923912525177, 0.09702949225902557, -0.1257900893688202, 0.019596317782998085, -0.5926141738891602, 0.53086256980896, -0.1923156976699829, -0.48798978328704834, 0.4631219506263733, 0.09047095477581024, -0.34545955061912537, 0.538740873336792, 0.6475187540054321, -0.8517404794692993, -0.8968875408172607, -0.6789811253547668, -0.24179504811763763, 0.16778366267681122, 0.3874889612197876, -0.5642650723457336, -0.28898072242736816, 1.1846648454666138, 0.4540920555591583, 0.3539382219314575, 0.21370847523212433, -0.22546924650669098, 0.0493561215698719, 0.7215137481689453, -0.08170245587825775, -0.4820420742034912, -0.5052752494812012, 1.4994566440582275, 1.229573369026184, -0.5003595948219299, 0.13670146465301514, -0.23466263711452484, -0.7877634763717651, 0.7863280773162842, 0.6770415306091309, -0.048778653144836426, 0.6140307188034058, -0.06964900344610214, -0.1672462821006775, 0.0716765820980072, -1.2902765274047852, -0.2120848298072815, 1.0041321516036987, 1.1858620643615723, 0.5138110518455505, -0.15745243430137634, 0.4394550621509552, 0.9291280508041382, 0.2092822939157486, 0.007161751855164766, 0.15893585979938507, 0.30476757884025574, -0.4331859052181244, 0.29439079761505127, 0.05723932012915611, 0.6990754008293152, -0.4995536506175995, -0.9154739379882812, 0.2596915662288666, 0.6068315505981445, 0.015959354117512703, 0.3570529520511627, 1.288241982460022, 0.2539198696613312, 0.42212504148483276, 0.34169840812683105, 0.6061272025108337, -0.44141721725463867, -0.37611672282218933, -0.4092569947242737, -0.5870455503463745, -0.26847758889198303, 0.09415657073259354, -0.8476954102516174, -0.3543427586555481, 0.008211115375161171, 0.49109548330307007, -0.13490302860736847, -0.035129986703395844, 0.8486400842666626, 1.0751482248306274, 0.8293227553367615, -0.6163644194602966, -0.5551052093505859, -0.15854990482330322, -1.0856178998947144, 0.3384876251220703, -0.8100658059120178, -0.2706819772720337, -0.06708082556724548, -0.07059845328330994, -0.15252555906772614]}, "authors": [{"authorId": "24593911", "name": "Tri Dao"}, {"authorId": "49577833", "name": "Daniel Y. Fu"}, {"authorId": "2490652", "name": "Stefano Ermon"}, {"authorId": "1755572", "name": "A. Rudra"}, {"authorId": "2061444681", "name": "Christopher R'e"}], "references": [{"paperId": "4247f45a5730e3bda5836e2bc7941e30f5b91cb7", "title": "Board"}, {"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca", "title": "DeepNet: Scaling Transformers to 1, 000 Layers"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "b55ee75940d24934a54d7f1acfde06e9cb45ac44", "title": "It's Raw! Audio Generation with State-Space Models"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "29584ed6d68a06fdf91440a018f6bc83a44fd177", "title": "Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases"}, {"paperId": "cec7872b194aadf54140578b9be52939eb1112e9", "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "77706ee4cbdbb23345da22af37bc1b9f5ec8f110", "title": "Sub-Linear Memory: How to Make Performers SLiM"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "c0f709acf38eb27702b0fbce1215db0ebaa2de2b", "title": "SMYRF: Efficient Attention using Asymmetric Clustering"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "3df83a60f55c64b40e6dbcd99cf9f67894a0736e", "title": "Do Transformers Need Deep Long-Range Memory?"}, {"paperId": "3836ccb33191799e748e8e96f85a813eaf650ff8", "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "a68c3412e60560290400d2707596f82a914b7c00", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "3f172dffae897113062fc0198f6b05c0195bf5fc", "title": "Kernel Operations on the GPU, with Autodiff, without Memory Overflows"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "09dad6c1d72e53d2da1cee9a60c111e084472847", "title": "The Deep Learning Compiler: A Comprehensive Survey"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3f06d02513a2763e472d2b5d5db08e9061081b9e", "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis"}, {"paperId": "d808dbdd6a53259bea908266397b3983f246aabc", "title": "Dissecting the Graphcore IPU Architecture via Microbenchmarking"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "d8e73f9034e16a380903282353e762d7f9517983", "title": "MLPerf Training Benchmark"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "aca16f64ddbf187f8944118c8f72777c3d682521", "title": "Neural Legal Judgment Prediction in English"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations"}, {"paperId": "075da5ebbb890924267b4b163292ad21d0b100a0", "title": "Stabilizing the Lottery Ticket Hypothesis"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "ec1f582446aa24f3f0920123ee6f05feea0b5e0a", "title": "Online normalizer calculation for softmax"}, {"paperId": "0e5d529befc3ca2e3e3371a0c39dc05731c1d5b7", "title": "Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "bce22675d77e1ef28e92f3793c02f8f5ccdb0ddd", "title": "A Two-pronged Progress in Structured Dense Matrix Vector Multiplication"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "773d5ddc414424a8948446ddaa5275b944f50891", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "95cd83603a0d2b6918a8e34a5637a8f382da96f5", "title": "MIMIC-III, a freely accessible critical care database"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "bf76be8df2f2bc56edac98a5d0dfc19c85882eaa", "title": "Structured Transforms for Small-Footprint Deep Learning"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "d133ef683d10af9102dad8a2a974b388fa42526a", "title": "Scalability! But at what COST?"}, {"paperId": "4d23db55e6671a82c95dacec33b2967a4b8b677d", "title": "Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines"}, {"paperId": "d3986480be64dce5ae55c1f64df660d7d698f8fb", "title": "Parallel stochastic gradient algorithms for large-scale matrix completion"}, {"paperId": "f82d09363e9a657749d14e97e9d6d1410331b436", "title": "Database Management Systems"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "42cdd8bbd91464461f796b565c9beff50186fc20", "title": "Optimal space lower bounds for all frequency moments"}, {"paperId": "8df26a339b6cbb1089e02f2039ac6c30a6ec89d9", "title": "On a new class of structured matrices"}, {"paperId": "8033c9c25c5658982e032ad670a0c525a5d6bbfa", "title": "Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals"}, {"paperId": "f4dff66ba8f2338d118f379f2eff1410feb57ce6", "title": "A data locality optimizing algorithm"}, {"paperId": "bbf58df1856fd582817fe93f816f1e03bfe3b17b", "title": "The input/output complexity of sorting and related problems"}, {"paperId": "7306af13eb2052fe9b9652c7d8b669655d307635", "title": "Displacement ranks of matrices and linear equations"}, {"paperId": "7fc3a6862f746c9988d82c023d2b7aaf0e089168", "title": "The working set model for program behavior"}, {"paperId": "cc2d1a66c701e9be9124eb95431b6c3dd79a61d3", "title": "Revisiting Transformer-based Models for Long Document Classification"}, {"paperId": "82120293d715c3119801361e83e4d2969161f81e", "title": "Fast geometric learning with symbolic matrices"}, {"paperId": null, "title": "XLA: Compiling machine learning for peak performance"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Openwebtext"}, {"paperId": "ec3071fb918ad69ec80df1ca9cf1fdeb386a9603", "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"}, {"paperId": "88cd4209db62a34d9cba0b9cbe9d45d1e57d21e5", "title": "Runtime Neural Pruning"}, {"paperId": null, "title": "V100 GPU"}, {"paperId": null, "title": "Nvidia Tesla V100 GPU architecture"}, {"paperId": "f8b1b43f284f1246ca015cc002ac949bb67c5645", "title": "Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures"}, {"paperId": "94899246a7c0b3592c6bf8f7aa0ad3c9203f61f9", "title": "Parameterized Complexity Theory"}, {"paperId": null, "title": "Memory hierarchy design"}, {"paperId": "e6074b731f7d42fecd3b83450c9674353a9e8c3a", "title": "An Updated Set of Basic Linear Algebra Subprograms Blas"}, {"paperId": "cb11771cbfd45aa09e00f1f3b0fc156a087fa544", "title": "Evaluating derivatives - principles and techniques of algorithmic differentiation, Second Edition"}, {"paperId": null, "title": "Random Butterfly Transformations with Applications in Computational Linear Algebra"}, {"paperId": null, "title": "tensor core"}, {"paperId": null, "title": "When computing the softmax gradient"}, {"paperId": null, "title": "the Annual Conference of the North American Chapter of the Association for Computational Linguistics , Mexico City, Mexico, 2021"}, {"paperId": null, "title": "Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation"}, {"paperId": null, "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating?"}, {"paperId": null, "title": "We do not need to store the dropout mask of size \ud835\udc42 ( \ud835\udc41 2 ) from the forward pass"}, {"paperId": null, "title": "you used crowdsourcing or conducted research with human subjects"}, {"paperId": null, "title": "Did you discuss any potential negative societal impacts of your work? [Yes] See Section 5"}, {"paperId": null, "title": "Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?"}, {"paperId": null, "title": "Have you read the ethics review guidelines and ensured that your paper conforms to them"}, {"paperId": null, "title": "code, data, models) or curating/releasing new assets... (a) If your work uses existing assets"}, {"paperId": null, "title": "If you are including theoretical results"}, {"paperId": null, "title": "written to HBM (Algorithm 0 line 3"}, {"paperId": null, "title": "Results Table 8 summarizes all the experimental con\ufb01gurations and contains pointers to the results tables"}, {"paperId": null, "title": "b) Did you describe any potential participant risks, with links to Institutional Review"}]}