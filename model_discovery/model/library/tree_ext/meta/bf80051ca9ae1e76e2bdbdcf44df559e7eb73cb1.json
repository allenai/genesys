{"paperId": "bf80051ca9ae1e76e2bdbdcf44df559e7eb73cb1", "title": "A Practical Survey on Faster and Lighter Transformers", "abstract": "Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models\u2019 efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer\u2019s limitation by designing lower-complexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide range of solutions, it has become challenging for researchers and practitioners to determine which methods to apply in practice to meet the desired tradeoff between capacity, computation, and memory. This survey addresses this issue by investigating popular approaches to make Transformers faster and lighter and by providing a comprehensive explanation of the methods\u2019 strengths, limitations, and underlying assumptions.", "venue": "ACM Computing Surveys", "year": 2021, "citationCount": 58, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2103.14636", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This survey investigates popular approaches to make Transformers faster and lighter and provides a comprehensive explanation of the methods\u2019 strengths, limitations, and underlying assumptions to meet the desired tradeoff between capacity, computation, and memory."}, "embedding": {"model": "specter_v2", "vector": [0.2674080729484558, 0.36911818385124207, -0.2641482651233673, -0.13470962643623352, 0.21670055389404297, -0.2242058962583542, 0.5465641021728516, -0.24883119761943817, -0.5409049391746521, -0.2848992645740509, 0.5200836658477783, -0.04010125622153282, 0.4114328622817993, 0.09960290044546127, -0.10907014459371567, -0.12108096480369568, -0.8448615670204163, 0.28114160895347595, 0.10017957538366318, -0.23169058561325073, 0.17378613352775574, -0.37775319814682007, -1.120431661605835, -0.11412476748228073, 0.18364858627319336, 0.7200227379798889, 0.36584222316741943, 0.6585330963134766, -0.6304693818092346, 0.9651481509208679, 0.7251365780830383, -0.45783543586730957, 0.48107942938804626, -0.006113084498792887, -0.38551732897758484, -0.3984624445438385, 0.2612198293209076, -0.41237229108810425, -0.5909895300865173, 0.5142650008201599, -0.07607635110616684, 0.36713188886642456, 0.0036350192967802286, -0.5993523001670837, 0.19299566745758057, 0.8993913531303406, 0.7319032549858093, 0.8600229620933533, -0.5100479125976562, -0.539838433265686, 1.471793293952942, -1.2157002687454224, -0.09475980699062347, 1.2270784378051758, 0.7868371605873108, 0.06888844072818756, -0.12151841819286346, -0.7069461345672607, 0.9434794187545776, 0.2623881995677948, -0.5768885612487793, -0.28669193387031555, 0.095110222697258, -0.056622881442308426, 2.0417113304138184, -0.3145701289176941, 0.1686905324459076, 0.6109851002693176, -0.0021350302267819643, 1.2653573751449585, -0.155187726020813, -0.4243713915348053, -0.14132177829742432, -0.2981054484844208, 0.6047574877738953, 0.8442190885543823, -0.45747724175453186, 0.33909159898757935, -1.1968988180160522, -0.11995510011911392, 0.8127932548522949, 0.44493770599365234, 0.10103433579206467, -0.2260959893465042, -0.022506119683384895, 0.8278496861457825, 0.3580794632434845, 0.7808157205581665, -0.508648693561554, 0.6125744581222534, 0.6306238770484924, 0.30723997950553894, -0.3257591426372528, 0.569999098777771, -0.18449126183986664, 0.07120193541049957, -0.7696729302406311, 0.07402627915143967, 0.0787939578294754, 0.888731062412262, -0.36349111795425415, 0.5348633527755737, -0.4089583158493042, 0.33214259147644043, 1.2412525415420532, -0.47293201088905334, 0.515121579170227, -0.7658751606941223, 0.16698278486728668, -0.7357065677642822, -0.08861331641674042, -0.502823531627655, -0.14249823987483978, -0.4983213543891907, -0.86797034740448, -1.4423305988311768, -0.6774817705154419, 0.2617805302143097, -0.7049590349197388, 0.46014511585235596, -0.8302552103996277, 0.5776026844978333, -0.22869010269641876, 0.416496604681015, 0.2966934144496918, 0.8203491568565369, 0.3489704132080078, -0.09242735803127289, 0.5277389287948608, -1.1421502828598022, -0.7041407227516174, -0.9400545954704285, 0.5633925199508667, -0.1319943368434906, -0.18942664563655853, 0.05943511426448822, -1.1930991411209106, -1.158663272857666, -0.7707064151763916, 0.05859091132879257, -0.371513694524765, -0.2635214626789093, 1.0954841375350952, 0.12837354838848114, -0.9969813823699951, 1.124889612197876, -0.28344443440437317, 0.03324706852436066, 0.21457096934318542, 0.14025598764419556, 0.6369162201881409, -0.17316192388534546, -1.385459542274475, 0.5352208614349365, 0.4220738708972931, -0.24802802503108978, -0.1827572137117386, -0.9203484654426575, -1.0276530981063843, 0.16402609646320343, 0.10657728463411331, -0.4120721220970154, 1.3223532438278198, -0.03231346607208252, -1.4065927267074585, 0.3030860126018524, -0.2004251778125763, -0.1134391725063324, 0.2609652876853943, -0.42244914174079895, -0.27906399965286255, -0.4635617136955261, -0.5727958679199219, 0.2018924504518509, 0.34402167797088623, 0.08435467630624771, -0.3338525593280792, 0.2348407655954361, -0.350870817899704, -0.11653896421194077, -0.17542202770709991, 0.8362192511558533, -0.42864206433296204, -0.27879098057746887, 0.4153076708316803, 0.7107546925544739, -0.3742309808731079, -0.3142881691455841, -0.12128029763698578, -0.9759958982467651, 0.6090182065963745, -0.021513616666197777, 1.1677724123001099, -0.7688088417053223, -0.680225670337677, -0.15227098762989044, -0.17199520766735077, 0.011702277697622776, -1.0104706287384033, 0.5591521859169006, -0.6078650951385498, 0.11263465881347656, 0.059783581644296646, -0.9806902408599854, -0.17579655349254608, 0.22512128949165344, -0.9854586720466614, 0.052507758140563965, 0.0017572575015947223, 0.8612828850746155, -1.1096338033676147, 0.21904239058494568, 0.17179735004901886, 0.6028094291687012, -0.7886093258857727, 1.4893879890441895, -0.09858009964227676, -0.2682746350765228, 0.18645726144313812, -0.32570934295654297, 0.020524978637695312, -0.43392413854599, 0.7231723666191101, -0.414808988571167, -0.17936845123767853, 0.7566789388656616, -0.5344869494438171, 1.4864822626113892, -0.5069378018379211, 0.613555371761322, -0.4311559796333313, -0.7959149479866028, 0.6095271706581116, 0.5454891324043274, 0.10584316402673721, -0.6227489113807678, 0.22969704866409302, 0.26708921790122986, -0.708279550075531, 0.44599997997283936, 0.7975516319274902, 0.7699101567268372, 0.04700504615902901, 0.10714933276176453, 0.6659591197967529, -0.07292058318853378, 0.5450140237808228, 0.34106242656707764, 0.5880603194236755, 0.5525261163711548, 0.22551418840885162, 0.11470986157655716, 0.2519792318344116, -1.3336586952209473, 0.30987095832824707, 0.5115411877632141, 0.8636930584907532, 0.8492348194122314, 0.13759717345237732, -0.7440447211265564, -0.6366655826568604, 0.17314906418323517, 0.7844778299331665, 1.5401414632797241, 0.009596300311386585, -0.29941147565841675, -0.14780130982398987, -0.04237397387623787, -0.4333513081073761, 0.1913539469242096, -0.1234968826174736, -0.12113837152719498, -0.7297639846801758, -0.9544559717178345, 0.6847221255302429, 0.6809724569320679, 0.9549311995506287, -0.3792891800403595, -0.6130349636077881, 0.03566686436533928, 0.11141955107450485, -0.581582248210907, -0.3976500630378723, 0.6647630333900452, -1.1624422073364258, -0.25050273537635803, 0.10863763093948364, 0.13792584836483002, -0.020249174907803535, -0.6146349310874939, 1.0356898307800293, -0.5543935894966125, -0.14982113242149353, -0.12371203303337097, 0.46503013372421265, -0.4997486174106598, -0.22696208953857422, 0.2616099417209625, -0.06045613810420036, -0.21375727653503418, 0.34858399629592896, 0.07157948613166809, -0.04568439722061157, 0.05634891986846924, -0.2797856628894806, 0.010664187371730804, 0.335039883852005, -0.05023392289876938, 0.734897792339325, -0.42889079451560974, -0.022697167471051216, -1.057360291481018, 0.7735744714736938, -0.013708270154893398, -0.3862999379634857, 0.10131723433732986, -1.0012457370758057, 0.08849980682134628, 0.4212717115879059, -0.4824811816215515, -0.23428139090538025, -0.9189164042472839, 0.2455447018146515, -0.8481889963150024, 0.16808836162090302, 0.18458950519561768, 0.5344781875610352, 0.3801686465740204, -0.1620398610830307, 0.506044328212738, 0.2312176674604416, -0.31431475281715393, 0.38698214292526245, -0.9670912623405457, 0.4714479148387909, 0.5491020679473877, -0.13001422584056854, -0.48384180665016174, -0.2165556102991104, -0.39613330364227295, -0.6552388668060303, -0.7465696930885315, -0.22807185351848602, -0.06610310822725296, -0.21477892994880676, -0.5718284249305725, -0.7795487642288208, -0.01623835600912571, -1.1393531560897827, -0.03859554976224899, 0.06279470026493073, -0.33221256732940674, -0.3521735966205597, -1.1252425909042358, -1.161663293838501, -0.7613969445228577, -0.7077264785766602, -0.7727037668228149, -0.19657796621322632, 0.14035533368587494, -0.2882691025733948, -0.6875590682029724, 0.0663672611117363, -0.5794660449028015, 1.1522636413574219, -0.6943990588188171, 0.6933237314224243, -0.23998622596263885, -0.1642313301563263, -0.032715797424316406, 0.18294309079647064, 0.6194257736206055, -0.12228591740131378, 0.18512177467346191, -1.0509459972381592, 0.23818159103393555, 0.09568263590335846, 0.006858502048999071, 0.2816702425479889, 0.13440102338790894, 0.8464604616165161, -0.2076212465763092, -0.29093673825263977, 0.3762061893939972, 1.4215561151504517, -0.2057449072599411, 0.30055728554725647, 0.1307099461555481, 1.0075688362121582, 0.12329933792352676, -0.22741831839084625, 0.7416607737541199, 0.004929602611809969, 0.11726219207048416, 0.2327021062374115, 0.18433146178722382, 0.1647159308195114, -0.5235828161239624, 0.3410424292087555, 1.8332916498184204, -0.1519492268562317, 0.20572201907634735, -0.6183952689170837, 0.5401766896247864, -1.4244762659072876, -0.9366883635520935, 0.7929906249046326, 0.5571120977401733, 0.7325847744941711, -0.3420444130897522, -0.23693276941776276, 0.05809739604592323, 0.3666938245296478, 0.6561718583106995, -0.5408205389976501, -0.7697232961654663, -0.03456311300396919, 0.5932880640029907, 0.44338470697402954, 0.5098469853401184, -0.08438578248023987, 0.04826036095619202, 15.167896270751953, 0.49924156069755554, -0.08264767378568649, 0.4940668046474457, 0.5791261792182922, -0.12529149651527405, -0.22987912595272064, 0.000985705410130322, -1.2827361822128296, 0.025106733664870262, 1.362070083618164, 0.23325589299201965, 0.39066365361213684, 0.25048670172691345, 0.006714817136526108, 0.22471338510513306, -0.964165985584259, 1.0785080194473267, 0.4131457805633545, -1.4359437227249146, 0.2663339376449585, -0.20897619426250458, 0.10698050260543823, 0.41930317878723145, 0.788669228553772, 0.7850309014320374, 0.5193089246749878, -0.6209652423858643, 0.5704508423805237, 0.6057246327400208, 0.5902887582778931, -0.3432254195213318, 0.8003537058830261, 0.2940579056739807, -1.0752285718917847, -0.33737727999687195, -0.4194853603839874, -1.24940025806427, 0.16716662049293518, 0.19817812740802765, -0.3510710597038269, -0.11401164531707764, -0.1531461924314499, 1.260162353515625, 0.10119028389453888, 0.6192660927772522, -0.32444795966148376, 0.6153469085693359, -0.12592990696430206, 0.2273636758327484, 0.3830707371234894, 0.11543454974889755, -0.18360593914985657, 0.18842384219169617, -0.08408205211162567, 0.025568241253495216, 0.13349950313568115, 0.02418665960431099, -0.3690983057022095, -0.2434011846780777, -0.22952912747859955, -0.5410950779914856, 0.21865545213222504, 0.5475698113441467, 0.5960105061531067, 0.4515749216079712, -0.23760193586349487, 0.1565822958946228, 0.5212258100509644, 0.16686083376407623, -0.5455408096313477, -0.2828127145767212, 0.31807956099510193, -0.4711087644100189, 0.20110458135604858, 0.5522383451461792, -0.4030369818210602, -0.30835965275764465, -0.8135498762130737, -0.4044649600982666, 0.6455467939376831, -0.8185015916824341, -0.6969471573829651, 0.8045901656150818, -0.25295495986938477, -0.40184739232063293, 0.1413036435842514, -0.5323671698570251, -0.05028558149933815, 0.5510761141777039, -1.4403635263442993, -0.07669967412948608, -0.010416644625365734, -0.1811254769563675, -0.23010870814323425, 0.0031664634589105844, 1.0981829166412354, 0.2580397427082062, -0.46719738841056824, -0.001169830677099526, -0.0634489506483078, -0.060864318162202835, -0.4689326584339142, -0.6119086742401123, 0.565531849861145, 0.20830462872982025, -0.1544150710105896, 0.5789944529533386, 0.05258532613515854, 0.42687496542930603, -0.6355449557304382, -0.2852630019187927, 0.5218274593353271, -0.43607988953590393, -0.2151823490858078, -0.4744454026222229, -1.0070016384124756, 0.6531180143356323, 0.3771229386329651, -0.41910091042518616, 0.25787362456321716, 0.052590858191251755, -0.8750408887863159, -0.3851577639579773, -0.352614164352417, -0.06662361323833466, 0.7708632349967957, -0.9159798622131348, -0.5821343660354614, -0.45909878611564636, 0.3863341808319092, -0.6765419244766235, -0.5963844656944275, -0.12064653635025024, -0.036363568156957626, -0.22653932869434357, 1.0056636333465576, -0.2558106780052185, 0.4812096953392029, 0.9175383448600769, 0.039123110473155975, -0.4634573757648468, -0.20061448216438293, -1.0316028594970703, -0.23024915158748627, -0.04092510789632797, 0.4246288239955902, -0.6493337750434875, 0.22362948954105377, 0.724898099899292, 0.1866026073694229, -0.5625825524330139, -0.5599241256713867, -0.45319873094558716, -0.3188002407550812, -0.22693496942520142, 0.19128364324569702, 0.08441339433193207, -0.19696037471294403, 0.3928132653236389, 0.4424542784690857, 0.12077202647924423, -0.2723636329174042, -0.5507776737213135, -0.2665455937385559, -0.3661551773548126, 0.02766946144402027, -0.5774909853935242, -0.40376728773117065, -1.5144258737564087, -0.08632305264472961, -0.9501704573631287, 0.012661464512348175, -1.0290091037750244, -0.3913329541683197, -0.13391165435314178, -0.6268590688705444, 0.21561141312122345, 0.4005774259567261, -0.7723842859268188, -0.3742738366127014, -0.3310215473175049, -0.5426563024520874, 0.8038638830184937, 0.5841213464736938, -0.5633465051651001, -0.10010307282209396, -0.15329201519489288, 0.2851387560367584, 0.19896861910820007, 0.679219663143158, -0.41520071029663086, -1.0877892971038818, -1.1434705257415771, 0.5096157789230347, -0.04612269252538681, -0.251382976770401, -0.46587058901786804, 0.7782478332519531, 0.2965209484100342, -0.25721362233161926, -0.258036345243454, 0.43301627039909363, -0.9510791897773743, -0.0644887238740921, 0.5192751884460449, -0.9018735289573669, 0.5246402025222778, 0.23614391684532166, -0.625290036201477, -0.33364608883857727, 0.4230928421020508, 0.1646783947944641, -0.9649139642715454, -0.8508588075637817, 0.48782679438591003, -0.539107620716095, -0.0703975185751915, -0.11041071265935898, -0.2650078237056732, -1.0285600423812866, -0.1605774313211441, 0.06352170556783676, 0.48679396510124207, -0.6064067482948303, 0.884357213973999, 0.6896446943283081, -0.951452374458313, 0.17548201978206635, 0.18279948830604553, -0.31934425234794617, 0.1988126039505005, 0.16660697758197784, 0.5209313631057739, -0.33453142642974854, 0.5651174783706665, -0.020006930455565453, 0.38699010014533997, -0.6855092644691467, -0.013573301956057549, 1.2033216953277588, -0.6456995010375977, -0.16702093183994293, 1.1758580207824707, -0.48941463232040405, -0.7217522859573364, 0.520648717880249, -1.0563349723815918, -0.7275629639625549, -0.03729115426540375, 0.6783386468887329, 0.5639327168464661, -0.08227774500846863, 0.3751532733440399, -0.52683025598526, -0.029647836461663246, -0.2635766267776489, -0.23624171316623688, 0.8792281150817871, 0.06816554069519043, -0.4252663850784302, 0.8796340823173523, 0.8053023815155029, -1.2526249885559082, -0.9370855689048767, -0.8686477541923523, -0.07721340656280518, -0.25210148096084595, 0.2194400429725647, -0.17475439608097076, -1.0041450262069702, 0.7211883664131165, 0.4695144295692444, 0.09403569251298904, 0.21139733493328094, -0.3599332869052887, -0.06873667240142822, 0.801957905292511, 0.03426508232951164, -0.615558922290802, -0.3357123136520386, 1.4570753574371338, 1.335218071937561, -0.3536432981491089, 0.45470550656318665, -0.20811133086681366, -0.6405521035194397, 0.960002601146698, 0.24897213280200958, -0.11016666889190674, 0.8008769750595093, -0.09515159577131271, -0.11618467420339584, 0.21155443787574768, -1.1500762701034546, -0.04021167382597923, 0.27538031339645386, 1.0090794563293457, 0.812342643737793, -0.247755765914917, 0.26362141966819763, 0.8046502470970154, 0.07693984359502792, 0.11294356733560562, 0.4524829387664795, 0.7482878565788269, 0.15310169756412506, 0.13902421295642853, 0.27290916442871094, 0.5355992317199707, -0.6130970120429993, -0.7804125547409058, 0.4041399657726288, 0.3494510352611542, 0.022942593321204185, 0.5863602757453918, 1.1349599361419678, 0.2104417383670807, 0.603312611579895, 0.22029659152030945, 0.5046371817588806, -0.4256068170070648, -0.5578139424324036, -0.29613983631134033, -0.6023405194282532, -0.5242494940757751, -0.5787964463233948, -0.5354059934616089, -0.48723888397216797, -0.235614612698555, 0.4035280644893646, 0.13038954138755798, 0.2728431820869446, 0.8673871159553528, 0.550150990486145, 0.6706207990646362, -0.09619279205799103, -0.4543619453907013, -0.44379156827926636, -0.9202145934104919, 0.08621706068515778, -0.42149168252944946, -0.2710777521133423, -0.1702059805393219, -0.10005293786525726, -0.007512990850955248]}, "authors": [{"authorId": "151497735", "name": "Quentin Fournier"}, {"authorId": "39768738", "name": "G. Caron"}, {"authorId": "2663120", "name": "Daniel Aloise"}], "references": [{"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "773c31a64ebbc22cc7b61faae4811ef8f046043b", "title": "On Improving Deep Learning Trace Analysis with System Call Arguments"}, {"paperId": "a194297f3703bae8040ddfd0c08a60cdcaea2aeb", "title": "Transformers with Competitive Ensembles of Independent Mechanisms"}, {"paperId": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8", "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "cec7872b194aadf54140578b9be52939eb1112e9", "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "cb4c1f930d4f97a64d5a041992733c0b54c13331", "title": "Transformer-Based Online Speech Recognition with Decoder-end Adaptive Computation Steps"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "a5bd0d192393372fc9ef2e67564d101fd3f0795e", "title": "Improving RNN Transducer Based ASR with Auxiliary Tasks"}, {"paperId": "32999b20f890bcc5effe80197045d6c147226fe4", "title": "Findings of the Association for Computational Linguistics: EMNLP 2020"}, {"paperId": "51c9d4d2f50ac5707c1f889aa97f08350d549132", "title": "Attention Is All You Need In Speech Separation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "7dfac3bf747fefc62f5bdaa124a609992a18b974", "title": "Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "ffb0bbe26f1cbc0ab22ef34784248f2dcd3a5e5c", "title": "Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "e00484961fb2f30d2d48a5f9853fa3ebab140cac", "title": "Improving Transformer Optimization Through Better Initialization"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "3b76429b4bb5dc8a308febbdb06e45b9678ccb8c", "title": "Estimating Carbon Emissions of Artificial Intelligence [Opinion]"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "91ac65431b2dc46919e1673fde67671c29446812", "title": "When BERT Plays the Lottery, All Tickets Are Winning"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "8d908042f139575d6688c745e94156c9df6eae07", "title": "Understanding the Difficulty of Training Transformers"}, {"paperId": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39", "title": "Training with Quantization Noise for Extreme Model Compression"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "39f8cc684f09ea2b43767f5b9590896774802759", "title": "On the effect of dropping layers of pre-trained transformer models"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "5f2c083f80073c56c41c4ea66ae48312513c55aa", "title": "Energy and Policy Considerations for Modern Deep Learning Research"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "4ee1754f89b250753242c3baf58773dda5d9bf39", "title": "Transformer on a Diet"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "09e2c7adbed37440d4a339852cfa34e5b660f768", "title": "Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "bc51622358d8eea83248ef29402fe10640d07ba6", "title": "Big Transfer (BiT): General Visual Representation Learning"}, {"paperId": "4097148c147f06b54802000a8476d28e525c63cf", "title": "Specaugment on Large Scale Datasets"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "20ba55ee3229db5cb190a00e788c59f08d2a767d", "title": "Self-Training With Noisy Student Improves ImageNet Classification"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "4585611042d2be0d997ee135e3fe219d668db9ec", "title": "Depth-Adaptive Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "67a9dde04f367efc903b6d06097df9bdd9887ae7", "title": "Recurrent Independent Mechanisms"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "81517c02204e8fe1f6c78e928031ace82a70f877", "title": "Benchmarking the Performance and Energy Efficiency of AI Accelerators for AI Training"}, {"paperId": "0ce184bd55a4736ec64e5d82a85421298e0373ea", "title": "A Comparative Study on Transformer vs RNN in Speech Applications"}, {"paperId": "2f9d4887d0022400fc40c774c4c78350c3bc5390", "title": "Small and Practical BERT Models for Sequence Labeling"}, {"paperId": "ce177672b00ddf46e4906157a7e997ca9338b8b9", "title": "Attention is not not Explanation"}, {"paperId": "2bf7c350a8280e7c593d46a60127f99b21517121", "title": "On the Variance of the Adaptive Learning Rate and Beyond"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f5dcdf616c3af51210631d39981de7c131682887", "title": "Benchmarking TPU, GPU, and CPU Platforms for Deep Learning"}, {"paperId": "36e30516683032634975c53e60f3737b6e35ff80", "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "135112c7ba1762d65f39b1a61777f26ae4dfd8ad", "title": "Is Attention Interpretable?"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f", "title": "Attention is not Explanation"}, {"paperId": "2a31319e73d4486716168b65cdf7559baeda18ce", "title": "Star-Transformer"}, {"paperId": "ab456c1ed181c5c48a34adb61395d4806a0ba949", "title": "Attention in Natural Language Processing"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "94be567c32ae76bdaadabd4975807a94181e39b3", "title": "How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)"}, {"paperId": "fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "title": "Efficient Neural Architecture Search via Parameter Sharing"}, {"paperId": "50bdda28de3dcf82a0e10f9ec13eea248b19edb5", "title": "Regularized Evolution for Image Classifier Architecture Search"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "d0611891b9e8a7c5731146097b6f201578f47b2f", "title": "Learning Transferable Architectures for Scalable Image Recognition"}, {"paperId": "3a6d4cd0768ae8768e733280d362bdb4d25924e7", "title": "The Reversible Residual Network: Backpropagation Without Storing Activations"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "67d968c7450878190e45ac7886746de867bf673d", "title": "Neural Architecture Search with Reinforcement Learning"}, {"paperId": "515a21e90117941150923e559729c59f5fdade1c", "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "09879f7956dddc2a9328f5c1472feeb8402bcbcf", "title": "Density estimation using Real NVP"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "13fe71da009484f240c46f14d9330e932f8de210", "title": "Long Short-Term Memory-Networks for Machine Reading"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "3056add22b20e3361c38c0472d294a79d4031cb4", "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "dc8301b67f98accbb331190dd7bd987952a692af", "title": "NICE: Non-linear Independent Components Estimation"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "d770060812fb646b3846a7d398a3066145b5e3c8", "title": "Do Deep Nets Really Need to be Deep?"}, {"paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs"}, {"paperId": "7de66a09cd23f05859a95fa55616b515acab71e9", "title": "Findings of the 2013 Workshop on Statistical Machine Translation"}, {"paperId": "3832057ac487f43e885cdb485a6ca1462834bb8d", "title": "Estimating or Propagating Gradients Through Stochastic Neurons"}, {"paperId": "72d32c986b47d6b880dad0c3f155fe23d2939038", "title": "Deep Learning of Representations: Looking Forward"}, {"paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649", "title": "Understanding the difficulty of training deep feedforward neural networks"}, {"paperId": "9299c07cfc563abdff2f1b728e013d1404ba17ad", "title": "Challenges and Advances in Parallel Sparse Matrix-Matrix Multiplication"}, {"paperId": "a709be3501fec6d9d4c3250e1771f6fd9276993e", "title": "Attention, Attention!"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": "bd2fbbd4228f4e6c79a182876afa7fd972f79dde", "title": "Schemata and Sequential Thought Processes in PDP Models"}, {"paperId": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "title": "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability"}, {"paperId": null, "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "World gross electricity production, by source, 2018"}, {"paperId": "a07609c2ed39d049d3e59b61408fb600c6ab0950", "title": "GPU Kernels for Block-Sparse Weights"}, {"paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82", "title": "Deep Learning"}, {"paperId": null, "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Hinton , Alex Krizhevsky , Ilya Sutskever , and Ruslan Salakhutdinov"}, {"paperId": null, "title": "Saving memory using gradient-checkpointing"}, {"paperId": null, "title": "Large Text Compression Benchmark. Retrieved from http://mattmahoney.net/dc/text.html"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "and A . Krause , Eds . , vol . 80 . PMLR , 2018 , pp . 4092 \u2013 4101 . [ Online ]"}, {"paperId": null, "title": "Received 27 January 2022; revised 5 October 2022; accepted 23 February 2023"}, {"paperId": "6038d62f22be3162324d3cb5214512966fc6ddb0", "title": "Music Transformer \uae30\ubc18 \uc74c\uc545"}, {"paperId": null, "title": "40 Q. Fournier"}, {"paperId": null, "title": "A Practical Survey on Faster and Lighter Transformers 35"}]}