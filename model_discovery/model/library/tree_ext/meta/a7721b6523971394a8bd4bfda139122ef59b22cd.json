{"paperId": "a7721b6523971394a8bd4bfda139122ef59b22cd", "title": "Sparse Attention with Linear Units", "abstract": "Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. \u2018switch off\u2019) for some queries, which is not possible with sparsified softmax alternatives.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "citationCount": 31, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://aclanthology.org/2021.emnlp-main.523.pdf", "status": "HYBRID"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work introduces a novel, simple method for achieving sparsity in attention: it replaces the softmax activation with a ReLU, and shows that sparsity naturally emerges from such a formulation."}, "embedding": {"model": "specter_v2", "vector": [0.33119437098503113, 0.9883905053138733, -0.7895349264144897, -0.002791851293295622, -0.45850253105163574, 0.004131102003157139, 0.6463057994842529, -0.3037228286266327, 0.041446030139923096, -0.020435377955436707, 0.721193253993988, 0.19230668246746063, 0.4263077676296234, 0.1863025277853012, -0.3739800751209259, 0.11090347170829773, -0.5272906422615051, 0.38217097520828247, 0.06936254352331161, -0.6541804075241089, -0.24287010729312897, -1.1917672157287598, -0.4447379410266876, 0.20750990509986877, 0.2712423801422119, 0.7125052213668823, 0.5308547019958496, 0.368131548166275, -0.453569233417511, 0.5266863107681274, 0.47125759720802307, -0.40220916271209717, 0.2735823094844818, -0.44849732518196106, -0.15679742395877838, -0.19721126556396484, 0.3055643141269684, 0.059892356395721436, -0.2967785894870758, 0.7564554214477539, -0.3540535867214203, 0.039975445717573166, 0.4207446575164795, -0.34720689058303833, -0.8024179339408875, 1.0867550373077393, 0.20958855748176575, 0.8378866314888, 0.024924224242568016, -0.8288329839706421, 1.4167711734771729, -1.3093369007110596, 0.1783546656370163, 1.6332364082336426, 0.2806534469127655, 0.13126951456069946, 0.036984413862228394, -0.4985189139842987, 0.8849000930786133, 0.055281370878219604, -0.8444848656654358, -0.4582822322845459, -0.2346259206533432, 0.2217940390110016, 1.9111400842666626, -0.503386378288269, -0.28733372688293457, 0.5225026607513428, 0.11885247379541397, 1.2624881267547607, -0.2304457277059555, -0.7571321725845337, -0.6358333230018616, 0.19157129526138306, 0.07121732085943222, 0.7577651143074036, -0.2541232109069824, -0.1025606095790863, -0.8378479480743408, 0.1633828580379486, 0.5316746830940247, -0.08971486240625381, -0.19969959557056427, -0.2607794404029846, -0.40770527720451355, 0.777829110622406, 0.7392956018447876, 0.8558014035224915, -0.42848098278045654, 0.6753556132316589, 0.41313838958740234, 0.5095264315605164, 0.08538631349802017, 0.4000135064125061, -0.10301238298416138, 0.7264325618743896, -0.9270613789558411, -0.45414599776268005, -0.18249507248401642, 1.1350629329681396, -0.207866832613945, 0.5722118616104126, -0.6049257516860962, 0.3366163372993469, 1.3217235803604126, 0.3410985469818115, 0.6967741847038269, -0.3416078984737396, 0.27239736914634705, -0.7248425483703613, -0.3973548710346222, -0.963513970375061, 0.06331098079681396, -0.5088832974433899, -0.8422542214393616, -1.212618350982666, -0.7042786478996277, 0.2612224221229553, -0.5823103189468384, 0.8885149359703064, -0.39473631978034973, -0.1429659128189087, -0.1581498384475708, 0.49657997488975525, 0.5211780071258545, 0.8488550782203674, 0.20223577320575714, 0.06146003305912018, 1.0425752401351929, -0.9171552658081055, -0.7236682176589966, -1.1373658180236816, 0.5814536213874817, -0.6315872669219971, 0.036511242389678955, -0.24364377558231354, -1.1545065641403198, -0.7862927913665771, -0.7098740339279175, -0.10741405189037323, -0.21974775195121765, 0.24683427810668945, 0.814262866973877, 0.10280639678239822, -0.9733133912086487, 0.45342499017715454, -0.3049461841583252, -0.36120015382766724, 0.5075297951698303, 0.3315817415714264, -0.06417117267847061, -0.5196660161018372, -1.1471960544586182, 0.7076330780982971, 0.24276232719421387, -0.6083537936210632, 0.038036834448575974, -0.5517751574516296, -1.3666985034942627, -0.007943950593471527, 0.24745172262191772, -0.5858764052391052, 1.0672188997268677, -0.6413658857345581, -1.3998298645019531, 0.6324136257171631, -0.4483042061328888, 0.059784017503261566, -0.19210205972194672, -0.46902981400489807, -0.28239724040031433, -0.5723286271095276, -0.04676435887813568, 0.38730186223983765, 0.7472712397575378, 0.05866023153066635, 0.019917940720915794, 0.2074902057647705, -0.4711849093437195, -0.17397011816501617, -0.14013315737247467, 1.193009614944458, -0.6267087459564209, -0.34142184257507324, 0.2991725504398346, 0.6858999729156494, 0.3146006464958191, -0.49830830097198486, -0.32240167260169983, -1.0176316499710083, 0.7560042142868042, -0.11949386447668076, 0.9273937344551086, -0.8229479193687439, -0.08736034482717514, -0.15858833491802216, -0.3083178997039795, -0.05208715423941612, -0.8121885061264038, 0.43880754709243774, -0.6230722665786743, 0.06476730853319168, -0.1203358992934227, -1.1120797395706177, -0.14030739665031433, 0.0025998412165790796, -0.8228621482849121, 0.07401876151561737, 0.30970627069473267, 1.3196563720703125, -0.6847290396690369, -0.02482740394771099, 0.01226299349218607, 0.5364289879798889, -1.0309982299804688, 1.2624796628952026, -0.308262437582016, 0.3163897693157196, -0.2900100648403168, 0.033350080251693726, -0.14148540794849396, -0.5448858737945557, 0.34416982531547546, -0.4709012806415558, 0.2312576323747635, 0.6896554231643677, -0.1888236403465271, 1.3627084493637085, -0.6105071306228638, 0.7925543189048767, 0.08362430334091187, -1.1969190835952759, 0.2659103274345398, 0.46745628118515015, -0.2076791673898697, -0.8167768120765686, 0.6344903707504272, 0.23889222741127014, -0.770156741142273, 0.2591588497161865, 0.6936144828796387, 0.7472838759422302, -0.4372817873954773, 0.012026955373585224, 0.9603421092033386, -0.24918536841869354, 0.03846330940723419, 0.433201402425766, 0.44818249344825745, 0.49337679147720337, 0.8287431597709656, -0.081522636115551, 0.07150488346815109, -0.7601293325424194, -0.0260628554970026, 0.38966822624206543, 1.166365146636963, 0.9905520677566528, 0.25003817677497864, -0.5996002554893494, -0.48425132036209106, 0.09884577989578247, 0.714762270450592, 1.5681586265563965, -0.17600257694721222, -0.12164303660392761, -0.5240740776062012, -0.015290538780391216, -0.5464776158332825, 0.050816211849451065, -0.4204029440879822, -0.2825765311717987, -0.7992638349533081, -0.9432309865951538, 0.451956182718277, 0.20222842693328857, 0.7939613461494446, -0.3115134835243225, -0.07214372605085373, -0.38235780596733093, 0.07768659293651581, -1.0818322896957397, -0.8316359519958496, 0.3877934515476227, -0.3859785795211792, 0.1208949089050293, -0.09132387489080429, -0.3582603931427002, 0.16619054973125458, -0.5202787518501282, 1.1181946992874146, -0.724277913570404, 0.012081176042556763, 0.1688949316740036, 0.19856175780296326, -0.49582406878471375, -0.32002511620521545, 0.4710816442966461, 0.37837091088294983, 0.0480484776198864, 0.24759776890277863, 0.32678648829460144, 0.11399474740028381, 0.23124684393405914, -0.44526901841163635, 0.010488715954124928, 0.2482610046863556, 0.08423289656639099, 0.7300844788551331, -0.6511890292167664, 0.027254600077867508, -1.1595826148986816, 0.5269650220870972, -0.36544492840766907, -0.435984343290329, 0.21157139539718628, -0.3453741669654846, -0.3294818699359894, 0.2731193006038666, -0.6021170616149902, -0.24752399325370789, -0.9975932836532593, 0.43853917717933655, -0.3942055106163025, 0.011051704175770283, 0.41171422600746155, 0.29512259364128113, 0.12593846023082733, 0.24811121821403503, 0.36897706985473633, 0.4201354384422302, -0.2048230916261673, 0.5494570732116699, -0.8177977800369263, 0.6243776082992554, 0.4116635024547577, 0.22417289018630981, -0.18818064033985138, -0.39117154479026794, -0.6663734316825867, -0.6272698044776917, -0.16304853558540344, 0.007179168052971363, 0.16529691219329834, 0.4809323847293854, -0.7536618113517761, -0.49927446246147156, -0.4313618540763855, -1.129647135734558, -0.2021249681711197, -0.013954396359622478, 0.0577596053481102, -0.20021779835224152, -0.9557815194129944, -1.0922659635543823, -0.24728767573833466, -0.6167992353439331, -0.9556071162223816, 0.5726010799407959, 0.26331597566604614, -0.6181966662406921, -0.5644024610519409, -0.09323770552873611, -0.3336726427078247, 1.3545304536819458, -0.6830763220787048, 0.6026349067687988, -0.12482301145792007, -0.5059195160865784, -0.3719962239265442, 0.21808110177516937, 0.4509179890155792, -0.09866062551736832, -0.09032724797725677, -0.4418182373046875, 0.14873617887496948, -0.2787497043609619, -0.37674057483673096, -0.005005170125514269, 0.6673786640167236, 0.28155389428138733, 0.039025019854307175, -0.4868621826171875, 0.5262314081192017, 1.241153359413147, -0.7794528007507324, 0.049775343388319016, 0.277895987033844, 0.9451313614845276, 0.07158834487199783, -0.4796485900878906, 0.40070125460624695, 0.42233774065971375, 0.8401417136192322, 0.38067391514778137, -0.3394676744937897, -0.47848403453826904, -0.631001889705658, 0.6351540684700012, 1.6417231559753418, 0.08087483793497086, -0.11134682595729828, -0.5732030272483826, 0.6122413277626038, -1.206290364265442, -0.852119505405426, 0.6907429099082947, 0.607111930847168, 0.19341647624969482, -0.5804574489593506, -0.5913217663764954, -0.603213906288147, 0.7068842053413391, 0.19434507191181183, -0.11122305691242218, -0.7210355401039124, -0.2514669895172119, 0.679924726486206, 0.397610604763031, 0.9903013110160828, 0.049392763525247574, 0.9478867650032043, 15.083598136901855, 0.6753745675086975, -0.29289737343788147, 1.0194534063339233, 0.33803775906562805, -0.28304323554039, -0.07876811176538467, -0.26488977670669556, -1.179556965827942, -0.08769810944795609, 0.654690146446228, 0.3304212987422943, 0.7358582615852356, 0.11469152569770813, -0.10343841463327408, 0.3201621472835541, -0.5713843703269958, 0.8271518349647522, 0.9268198609352112, -1.069142460823059, 0.4386122226715088, 0.07113754749298096, 0.5233215093612671, 0.7842346429824829, 0.6908261775970459, 0.7233679890632629, 0.4971200227737427, -0.6061098575592041, 0.22997774183750153, 0.33721601963043213, 0.7980985641479492, 0.27027881145477295, 0.33415645360946655, 0.13845494389533997, -0.7245849967002869, -0.05494382232427597, -0.5116539597511292, -1.0804963111877441, 0.6306616067886353, 0.2374424934387207, -0.04338790848851204, -0.2444746196269989, -0.18675656616687775, 0.7928972840309143, 0.39090603590011597, 0.35615167021751404, -0.49076443910598755, 0.621073842048645, -0.40063297748565674, 0.1427856683731079, 0.41939660906791687, 0.476949542760849, 0.3206844627857208, 0.22133441269397736, 0.19506040215492249, -0.1925395429134369, 0.06224483251571655, 0.3469278812408447, -0.5949051380157471, 0.008321261964738369, -0.39594176411628723, -0.1721985638141632, -0.1766374260187149, 0.5635980367660522, 0.7755979895591736, 0.15495160222053528, -0.5183452367782593, 0.5656062364578247, 0.5546718239784241, 0.13013561069965363, -0.383084237575531, -0.09256576001644135, 0.4427547752857208, -0.1827508956193924, 0.16431832313537598, 0.43792882561683655, -0.22480595111846924, -0.6342406868934631, -0.6853885650634766, -0.3813958466053009, 0.5362353324890137, -1.0626990795135498, -0.7433670163154602, 0.9788382053375244, -0.17002379894256592, -0.23679830133914948, 0.31384751200675964, -0.6484988331794739, -0.11493480205535889, 0.6085243821144104, -1.4868049621582031, -0.7178815603256226, 0.29193150997161865, -0.19140411913394928, -0.08060074597597122, -0.37562501430511475, 0.9621314406394958, 0.28165894746780396, -0.3719494938850403, 0.29437169432640076, -0.20978912711143494, -0.16744542121887207, -0.018260732293128967, -0.24505223333835602, 0.3852713406085968, 0.24997016787528992, -0.18138426542282104, 0.3511285185813904, 0.09868886321783066, 0.3476579487323761, -0.8855233788490295, 0.15316228568553925, 1.1161130666732788, -0.8695054054260254, -0.22112083435058594, -0.4815669655799866, -0.9216368198394775, 0.7952556014060974, 0.9785196185112, -0.15731936693191528, 0.47324034571647644, 0.1773047298192978, -0.7324480414390564, -0.015439111739397049, -0.5623233318328857, -0.325635701417923, 0.16544415056705475, -0.7159106731414795, -0.7297574877738953, 0.024405118077993393, 0.4784887135028839, -1.003880262374878, -0.35182297229766846, -0.2747357487678528, -0.1591508835554123, 0.48366308212280273, 1.020034670829773, -0.6441044211387634, 0.7196500301361084, 0.7187329530715942, -0.26905784010887146, -0.8547898530960083, -0.43450337648391724, -0.9664159417152405, 0.00926533155143261, 0.20752251148223877, 0.5103392601013184, -0.4268535375595093, -0.27817365527153015, 0.3164941966533661, 0.007984495721757412, -0.2846413850784302, -0.867493212223053, -0.15133589506149292, -0.22532682120800018, -0.406236857175827, -0.03232830390334129, 0.2915707528591156, 0.18177784979343414, 0.4023014307022095, 0.07095330953598022, 0.5593698024749756, -0.3674066960811615, -0.8118096590042114, 0.1437557339668274, 0.12266527116298676, -0.20437666773796082, -0.7153922915458679, -0.7484301924705505, -1.7840344905853271, 0.09337364882230759, -1.2330156564712524, -0.1754036247730255, -1.1408369541168213, -0.4428057372570038, 0.2947455644607544, -0.19969290494918823, 0.4078749716281891, 0.3261922001838684, 0.2302035391330719, -0.397077739238739, -0.2996790111064911, -0.6210428476333618, 0.8596575260162354, 0.7012909650802612, -0.7233240008354187, 0.26064127683639526, -0.11379747092723846, -0.3351250886917114, 0.3691403269767761, 0.457061767578125, -0.5290735960006714, -0.333257794380188, -1.7405240535736084, 0.5627956986427307, -0.14099349081516266, -0.18508674204349518, -0.4560517966747284, 0.665046751499176, 0.46173179149627686, -0.3213459849357605, 0.34263548254966736, 0.4395827054977417, -0.9960708618164062, -0.5941776633262634, 0.17983144521713257, -0.697772741317749, 0.3563869893550873, 0.05202752724289894, -0.6977618336677551, -0.4406503438949585, 0.9546850919723511, 0.23537582159042358, -1.0088143348693848, -0.47568777203559875, 0.3034401834011078, -0.6130068302154541, 0.19381698966026306, -0.041011612862348557, -0.17005859315395355, -1.0810421705245972, -0.6121253371238708, 0.049363281577825546, 0.3274793326854706, -0.5482252836227417, 1.1650086641311646, 0.006400241516530514, -1.2531068325042725, -0.08381053805351257, 0.3129885494709015, -0.05590863898396492, -0.07249657064676285, 0.12613412737846375, 0.5152937173843384, -0.2730035185813904, 0.6250526905059814, 0.2855749726295471, 0.21938256919384003, -0.684757649898529, 0.08153245598077774, 0.866881787776947, -0.2539888620376587, -0.1490699052810669, 0.9928659796714783, -0.2285105586051941, -0.9281479716300964, 0.14771202206611633, -1.076999306678772, -0.29905974864959717, -0.1876073032617569, 0.68317049741745, -0.2067977488040924, -0.03546690195798874, -0.3409470319747925, -0.4621923565864563, 0.4693712890148163, 0.17029328644275665, -0.1546677201986313, 0.7288457155227661, -0.3183523416519165, -0.6108481287956238, 0.49185892939567566, 0.8129335641860962, -0.6932275891304016, -0.10128194838762283, -1.219154715538025, -0.7002761960029602, 0.013943327590823174, 0.38676726818084717, -0.23459094762802124, -1.0228326320648193, 0.6888831257820129, 0.39205870032310486, 0.19439898431301117, 0.1856008768081665, 0.45293211936950684, -0.09277521818876266, 0.4997282028198242, -0.012711933813989162, -0.42425182461738586, -0.5557796955108643, 1.6353833675384521, 1.4056408405303955, -1.0212103128433228, 0.052199587225914, -0.4300632178783417, -0.6994759440422058, 0.6977195739746094, 0.009416250512003899, -0.04903239384293556, 0.6748903393745422, -0.12237974256277084, 0.007753198966383934, 0.164866641163826, -0.8448708057403564, -0.33840447664260864, 0.7649004459381104, 0.9934200644493103, 1.0495611429214478, 0.3181247413158417, -0.11045337468385696, 0.451456755399704, -0.059464067220687866, 0.15355710685253143, 0.1694599986076355, 0.23772209882736206, -0.3368905484676361, -0.13777616620063782, -0.1485830545425415, 0.34281226992607117, -0.7747529745101929, -0.7722616791725159, 0.17852796614170074, 0.35799384117126465, 0.044103920459747314, 0.6080744862556458, 1.003796935081482, 0.2997927665710449, 0.6508396863937378, 0.12605883181095123, 0.5496488809585571, -0.7031815648078918, -0.2886141240596771, -0.2653235197067261, -0.8242950439453125, -0.23518481850624084, -0.09811221808195114, -0.6049561500549316, 0.04201686009764671, -0.16712859272956848, 0.278924822807312, -0.19189172983169556, 0.3884889781475067, 1.1277517080307007, 0.6397806406021118, 0.6876214146614075, -0.26190635561943054, -0.5608038306236267, -0.12568728625774384, -1.1044450998306274, 0.1882326751947403, -0.5810094475746155, -0.21770375967025757, -0.17186103761196136, 0.17626266181468964, -0.2541080117225647]}, "authors": [{"authorId": "48335426", "name": "Biao Zhang"}, {"authorId": "144889265", "name": "Ivan Titov"}, {"authorId": "2082372", "name": "Rico Sennrich"}], "references": [{"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "b761d0afcf976606543007a528ae87034f663d8e", "title": "Accurate Word Alignment Induction from Neural Machine Translation"}, {"paperId": "9a21740d87976bf76f4a9668a9da631035302fb2", "title": "Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "57f123c95ecf9d901be3a53291f53302740451e2", "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "b03cf6324ecf7a295a4aeae5970c88d1a1c3f336", "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection"}, {"paperId": "ba8215e77f35b0d947c7cec39c45df4516e93421", "title": "Do Attention Heads in BERT Track Syntactic Dependencies?"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "ce177672b00ddf46e4906157a7e997ca9338b8b9", "title": "Attention is not not Explanation"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb", "title": "Text Summarization with Pretrained Encoders"}, {"paperId": "242fd8725ec6d30a3c8648a4f9ffe9f2cf67ae3f", "title": "Saliency-driven Word Alignment Interpretation for Neural Machine Translation"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "3cee801d10f410f0feb1a2390776a01ba2765001", "title": "Sparse Sequence-to-Sequence Models"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f", "title": "Attention is not Explanation"}, {"paperId": "8b3b5d1138fa9ff20e9b12c0f53c22838039efff", "title": "Adding Interpretable Attention to Neural Translation Models Improves Word Alignment"}, {"paperId": "d098bdca6d0335a31fd164c1fac68028784a0737", "title": "Findings of the 2018 Conference on Machine Translation (WMT18)"}, {"paperId": "ad796bf779c8617d1e0d8111913ac3f8eaaf6532", "title": "Context-Aware Neural Machine Translation Learns Anaphora Resolution"}, {"paperId": "520ddb38b59b8fae2209ddc7c6640462cf153eec", "title": "Sparse and Constrained Attention for Neural Machine Translation"}, {"paperId": "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb", "title": "A Call for Clarity in Reporting BLEU Scores"}, {"paperId": "c6b61535f1544835cca3851ceb34222ebc5b4377", "title": "State-of-the-Art Speech Recognition with Sequence-to-Sequence Models"}, {"paperId": "e81c458fb5ea3b1f1597e725d97965d662495518", "title": "Confidence through Attention"}, {"paperId": "b5799525379e505234d727b01f8005f69a4d252d", "title": "What does Attention in Neural Machine Translation Pay Attention to?"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "ba6b48ef52e2432a0d6342381e0863fd82a8687b", "title": "A Regularized Framework for Sparse and Structured Neural Attention"}, {"paperId": "1a327709cc53ff9e52454e50a643abf4a0ac92af", "title": "Findings of the 2016 Conference on Machine Translation"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "c1e3a26fb88c6720f4e84b7118e6f2df7dc8efa3", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation"}, {"paperId": "adf3b591281688b7e71b254ab931b2aa39b4b59f", "title": "Empirical Evaluation of Rectified Activations in Convolutional Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "2f5102ec3f70d0dea98c957cc2cab4d15d83a2da", "title": "The Stanford CoreNLP Natural Language Processing Toolkit"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "c9214ebe91454e6369720136ab7dd990d52a07d4", "title": "Improved Statistical Alignment Models"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "51db583f89e0191c7ebf3fe212c5271c98b33c6b", "title": "Conference on Empirical Methods in Natural Language Processing Proceedings of the Fourth International Workshop on Natural Language Processing for Social Media Socialnlp@emnlp2016 Chairs' Welcome Identifying and Categorizing Disaster-related Tweets Why Do They Leave: Modeling Participation in Online"}, {"paperId": null, "title": "Examples for Null Attention Null Attention for Diverse-quality Examples"}, {"paperId": null, "title": "Technologies, Volume 1 (Long and Short Papers) pages"}]}