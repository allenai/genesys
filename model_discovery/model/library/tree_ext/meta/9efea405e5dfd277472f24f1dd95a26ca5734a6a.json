{"paperId": "9efea405e5dfd277472f24f1dd95a26ca5734a6a", "title": "ViTamin: Designing Scalable Vision Models in the Vision-Language Era", "abstract": "Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models, thanks to the training on the large-scale Internet image-text pairs. However, despite the amazing achievement from the VLMs, vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area, it remains questionable whether it is also the case for image encoding, especially considering that various types of networks are proposed on the ImageNet benchmark, which, unfortunately, are rarely studied in VLMs. Due to small data/model scale, the original conclusions of model design on ImageNet can be limited and biased. In this paper, we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models, covering their zero-shot performance and scalability in both model and training data sizes. To this end, we introduce ViTamin, a new vision models tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy, when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks, including classification, retrieval, open-vocabulary detection and segmentation, and large multi-modal models. When further scaling up the model size, our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy, surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).", "venue": "arXiv.org", "year": 2024, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "An evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework is built, and ViTamin, a new vision models tailored for VLMs is introduced, with promising results on 60 diverse benchmarks."}, "embedding": {"model": "specter_v2", "vector": [0.3492848873138428, 0.13660132884979248, -0.5528495907783508, 0.2971411645412445, -0.1461213380098343, -0.001024453667923808, 0.8020107746124268, -0.3170226514339447, -1.0388721227645874, -0.7515933513641357, 0.2436825931072235, 0.2707993686199188, 0.636280357837677, 0.1617816537618637, -0.0839243233203888, 0.37067240476608276, -0.5549368262290955, 0.1356048285961151, 0.4315209686756134, -0.6480004191398621, 0.002854817546904087, -0.8315467834472656, -1.1040164232254028, -0.2218625247478485, -0.12149713933467865, 0.787652850151062, 0.34272316098213196, 1.338721752166748, -0.16585096716880798, 0.5882002115249634, 0.7830369472503662, -0.5227853059768677, 0.248010516166687, 0.04410354793071747, -0.33724477887153625, 0.18909955024719238, 0.923837423324585, -0.6824550032615662, -0.7974948287010193, 1.0879392623901367, -0.18372948467731476, 0.25979772210121155, 0.7399939894676208, -0.7668628692626953, -0.722435712814331, 0.08013440668582916, 0.4857577681541443, 0.6148957014083862, -0.4826115667819977, 0.04328498616814613, 1.508124828338623, -1.5323021411895752, 0.45407533645629883, 1.677264928817749, 0.40256357192993164, 0.6507030725479126, 0.0700056329369545, -0.2637147307395935, 0.8913029432296753, 0.14480043947696686, -0.6440508365631104, -0.08909390866756439, -0.19446930289268494, -0.41698625683784485, 1.8298943042755127, -0.9080196022987366, 0.24728837609291077, 0.8193535208702087, 0.2147221863269806, 1.1316035985946655, -0.3357873558998108, -0.8415803909301758, -0.3142106235027313, -0.0761159285902977, 0.4455474019050598, 1.1002858877182007, -0.23934251070022583, 0.18793651461601257, -0.3636664152145386, 0.38368111848831177, 0.4824560880661011, 0.26418331265449524, 0.21044912934303284, -0.20308144390583038, -0.4109609127044678, 0.8558254241943359, 0.7838801145553589, 0.6346780061721802, -0.1369527131319046, 0.9962831139564514, 0.7366053462028503, 0.4134015440940857, -0.1784753054380417, 0.21623703837394714, 0.05235190689563751, 1.0933594703674316, -0.6974626183509827, 0.3112017810344696, -0.17789453268051147, 0.9146714806556702, -0.2252150923013687, 0.024742169305682182, -0.7631776332855225, -0.07708761841058731, 1.3819533586502075, 0.19808153808116913, 0.3999880850315094, -0.737648606300354, -0.07267653197050095, -0.8899394273757935, 0.1666329950094223, -1.1180403232574463, -0.0025267775636166334, -0.028255319222807884, -1.00303316116333, -1.0526903867721558, -0.28069815039634705, 0.7653605341911316, -1.3771623373031616, 0.8479092121124268, -0.4965208172798157, 0.24337121844291687, 0.3054272532463074, 0.48267725110054016, 0.9036159515380859, 0.6972367763519287, 0.672005832195282, 0.3723207116127014, 1.447084665298462, -1.2808459997177124, -0.3885386884212494, -1.133341908454895, 0.2471068948507309, -0.19880858063697815, 0.6214514374732971, -0.28521835803985596, -0.8529689908027649, -1.4516247510910034, -1.0127243995666504, -0.2596352994441986, -0.8403589725494385, 0.28407058119773865, 0.7658615112304688, 0.2970753312110901, -1.3458062410354614, 0.19695287942886353, 0.23502607643604279, -0.747200071811676, 0.4796520471572876, -0.1779368817806244, 0.1966652125120163, -0.6217204332351685, -1.1287956237792969, 0.6598735451698303, -0.027477363124489784, -0.47775503993034363, -0.810344398021698, 0.1021355465054512, -1.2893956899642944, -0.10122105479240417, 0.41770321130752563, -0.7252519130706787, 1.0468097925186157, -0.4455152451992035, -0.6598995327949524, 1.0142779350280762, -0.4808445870876312, -0.2806905508041382, 0.3596121072769165, -0.32555583119392395, -0.14171169698238373, 0.029318755492568016, -0.2686159908771515, 1.1630918979644775, 0.9861641526222229, -0.0930984765291214, 0.181997612118721, 0.4773849546909332, -0.16542990505695343, -0.25827756524086, -0.47813495993614197, 1.0110764503479004, -0.8861702084541321, 0.08761842548847198, 0.22403286397457123, 0.6753250360488892, -0.3701741397380829, 0.2709871828556061, -0.03639950230717659, -0.8725115060806274, 0.8766176104545593, 0.22577813267707825, 0.1385081559419632, -0.9854788184165955, -0.8096402883529663, -0.4529803395271301, 0.3342590034008026, 0.09944027662277222, -1.3311549425125122, 0.3373500406742096, 0.056877318769693375, 0.4331749975681305, 0.31523042917251587, -1.4069327116012573, 0.10379364341497421, -0.27817702293395996, -0.7429328560829163, 0.06790265440940857, 0.505003809928894, 1.5650180578231812, -0.8704948425292969, -0.22473318874835968, 0.1930350661277771, 0.2784334123134613, -1.0873159170150757, 0.7179137468338013, -0.8279677629470825, -0.0764617994427681, -0.30637770891189575, -0.0520477369427681, -0.06283900886774063, -0.6362174153327942, 0.15626095235347748, -0.4828435182571411, 0.017215874046087265, -0.21074415743350983, -0.02401548996567726, 1.3044768571853638, -0.012096625752747059, 0.9174470901489258, -0.19733501970767975, -0.8658546209335327, 0.5169780850410461, 0.2791735529899597, -0.37910449504852295, -0.7801900506019592, 0.6245126128196716, -0.06461553275585175, -0.9275959134101868, 0.1227799728512764, 0.9479081034660339, 1.0669552087783813, -0.5045801401138306, -0.18461090326309204, 0.8137937188148499, -0.35725635290145874, 0.4546431303024292, 0.6473885774612427, 0.5401420593261719, 0.04112060368061066, 0.1782667636871338, 0.07886295020580292, 0.5004290342330933, -0.713991105556488, -0.4071924090385437, 1.0868065357208252, -0.10586465150117874, 1.103974461555481, 0.33683788776397705, -0.8595690131187439, -0.6761094331741333, -0.01716303452849388, 0.9466818571090698, 1.3511885404586792, 0.4135827124118805, -0.10941322147846222, -0.896245539188385, -0.09688085317611694, -0.6840466856956482, -0.6199546456336975, -0.47542044520378113, -0.26027151942253113, 0.08995415270328522, -0.7456068396568298, 0.6930503249168396, -0.061315570026636124, 1.3918393850326538, -0.7895939350128174, -0.444099485874176, -0.5168129801750183, 0.09151741117238998, -1.2342032194137573, -0.6753137111663818, -0.1687498241662979, 0.06535446643829346, -0.2539079487323761, -0.30859553813934326, -0.6404505968093872, 0.14065954089164734, -0.07728525251150131, 0.7759361267089844, -0.33975911140441895, -0.7750199437141418, 0.8175951838493347, 0.43328791856765747, -0.600212037563324, -0.6181015968322754, -0.02789442613720894, 0.04522252082824707, 0.024681171402335167, 0.5188735127449036, 1.0186538696289062, -0.18128061294555664, -0.000168348997249268, -0.4640430808067322, -0.038200125098228455, 0.03825036436319351, 0.09375349432229996, 0.9723654985427856, -0.46669456362724304, 0.04438246786594391, -0.42437514662742615, 0.7750630378723145, 0.40974023938179016, -0.5510788559913635, 0.6100172996520996, -0.47505730390548706, -0.6460058093070984, 0.3139507472515106, -0.9225241541862488, -0.03871355950832367, -0.5727279186248779, 0.5312806367874146, -0.8475970029830933, -0.3262949585914612, -0.06358107924461365, 0.5506818890571594, 0.12367776036262512, 0.4960932731628418, 0.5489853620529175, 0.36675047874450684, 0.286582350730896, 0.8356741666793823, -0.9279008507728577, 0.9105152487754822, 0.47253888845443726, 0.047246016561985016, 0.08140536397695541, -0.11413944512605667, -0.8937042355537415, -0.5010666251182556, -0.732328474521637, -0.25519394874572754, -0.914525032043457, 0.914924144744873, -0.8295693397521973, -0.6236526966094971, 0.3825750946998596, -1.1934748888015747, -0.04879593104124069, 0.08919382840394974, -0.3554620146751404, -0.3223017156124115, -0.7561902403831482, -0.8791700601577759, -0.3932923674583435, -0.7978192567825317, -1.1692274808883667, 0.3223150074481964, 0.37541118264198303, -0.16725410521030426, -0.4110831022262573, -0.020957674831151962, -0.2468061000108719, 0.9907844662666321, -0.41204148530960083, 0.3605254590511322, -0.19622604548931122, -0.2906557023525238, -0.5370575785636902, -0.25603964924812317, 0.9554144144058228, -0.5473461747169495, 0.45006656646728516, -0.9104101061820984, 0.05300578102469444, -0.4843093156814575, -0.8038811087608337, 1.0079514980316162, 0.5064671039581299, 0.3801179528236389, 0.42004120349884033, -0.2699187397956848, 0.11037106066942215, 1.7868146896362305, -0.9513707756996155, 0.20585045218467712, -0.1018652617931366, 0.8194289207458496, -0.06333182007074356, -0.33583593368530273, 0.28209006786346436, 0.5601968765258789, -0.2326136827468872, 0.4240707755088806, -0.40994927287101746, -0.5510794520378113, -0.8957057595252991, 0.5019643902778625, 1.0749856233596802, 0.4942764937877655, -0.19473004341125488, -0.9804823994636536, 0.9550250172615051, -1.0080257654190063, -0.9213888645172119, 0.6335539817810059, 0.420632541179657, -0.08346963673830032, -0.30022934079170227, -0.3061712384223938, -0.6075227856636047, 0.7103666067123413, 0.5761712789535522, -0.3485136330127716, -0.6279417872428894, -0.184574693441391, 0.3604138493537903, 0.20704343914985657, 0.5033158659934998, -0.819133460521698, 1.0563912391662598, 14.242197036743164, 0.7598277926445007, -0.40429040789604187, 0.640951931476593, 0.7897434830665588, 0.5972328782081604, -0.13706445693969727, 0.0007818166632205248, -1.1547834873199463, -0.670699417591095, 0.9458118677139282, 0.33115100860595703, 0.30095672607421875, 0.50565105676651, -0.18173053860664368, 0.17294193804264069, -0.3695503771305084, 0.9529497027397156, 0.9564758539199829, -1.3001587390899658, 0.624081015586853, 0.08191938698291779, 0.4310152232646942, 0.8956697583198547, 1.2506020069122314, 0.867277979850769, 0.2127305269241333, -0.36094769835472107, 0.47659116983413696, 0.33804622292518616, 1.0905855894088745, 0.459870845079422, 0.11526142060756683, 0.41544991731643677, -1.1126433610916138, -0.36349737644195557, -0.982195258140564, -1.0035128593444824, 0.15402893722057343, -0.4207012355327606, -0.2477717399597168, -0.5881509184837341, -0.14437280595302582, 0.9589026570320129, -0.2766819894313812, 0.39268437027931213, -0.04806094244122505, 0.348882257938385, -0.06983271986246109, -0.33924147486686707, 0.7170149683952332, 0.5248576998710632, 0.2670271396636963, 0.06103432551026344, -0.2067210078239441, -0.21628324687480927, 0.5001735687255859, 0.5138315558433533, -0.7010566592216492, -0.07250620424747467, -0.5960116982460022, -0.2699960470199585, -0.596887469291687, 0.7707089185714722, -0.033836424350738525, 0.024377986788749695, -0.5091390609741211, 0.29330581426620483, 0.22730550169944763, 0.4637223184108734, -0.5333617329597473, 0.09660457819700241, 0.09997526556253433, -0.690727710723877, 0.2650706171989441, 0.3623391389846802, 0.21932829916477203, -0.8932754397392273, -0.8074557185173035, -0.14383457601070404, 0.19430620968341827, -0.9256619215011597, -0.6694129705429077, 0.9071857333183289, -0.309718519449234, -0.08277124911546707, 0.4277127981185913, -0.9909551739692688, -0.36397087574005127, 0.3296368420124054, -1.644790530204773, -1.0589137077331543, -0.03046608716249466, -0.06585487723350525, 0.019561974331736565, -0.24824638664722443, 0.9982022643089294, 0.028043484315276146, -0.17813463509082794, -0.02780040167272091, -0.2859884202480316, 0.3489360213279724, -0.33775800466537476, -0.13647259771823883, 0.9543403387069702, 0.4818280339241028, -0.04270881041884422, -0.3477167785167694, -0.09710363298654556, 0.4215486943721771, -0.6617565751075745, -0.2064623236656189, 0.6773746013641357, -0.6657783389091492, -0.4361341893672943, -0.6896039843559265, -0.5997944474220276, 0.14838580787181854, 0.6336134672164917, 0.06250282377004623, -0.3174722194671631, -0.319995254278183, -1.0231536626815796, 0.0579838901758194, -0.8097440004348755, 0.3215034604072571, 0.24415414035320282, -0.9673454165458679, -0.29297566413879395, 0.1429750621318817, 0.3515402674674988, -0.43314167857170105, 0.03344939649105072, -0.44598132371902466, 0.35866716504096985, 0.027731657028198242, 1.425513744354248, -0.5351898670196533, 0.837756872177124, 0.9761335253715515, -0.4095900356769562, -0.2980508804321289, 0.03327881172299385, -0.6721063256263733, 0.5345048308372498, 0.05887548252940178, 0.23800525069236755, -0.4916178584098816, -0.18349166214466095, 0.6179177761077881, 0.7381191253662109, -0.4226388931274414, -0.5142697691917419, -0.3404949903488159, 0.15902823209762573, -0.6241475343704224, -0.16510826349258423, -0.4161311388015747, -0.3394136130809784, 0.3142872750759125, 0.15697716176509857, 0.35643619298934937, -0.12183639407157898, -1.0064499378204346, 0.6432496309280396, -0.09906119853258133, -0.07315509021282196, -0.5652971267700195, -0.8921811580657959, -1.66293203830719, 0.07225014269351959, -1.075844407081604, 0.14176397025585175, -0.9818522334098816, -0.15386223793029785, 0.6534570455551147, -0.35932210087776184, 0.17287133634090424, 0.5081270337104797, 0.0903649777173996, 0.17247502505779266, -0.5349522233009338, -1.0512298345565796, 0.644839882850647, 1.279647946357727, -1.1228677034378052, 0.19977381825447083, -0.42573413252830505, 0.06645039469003677, 0.5002957582473755, 0.2490389347076416, -0.19432663917541504, -1.0783520936965942, -1.034806728363037, 0.262864351272583, -0.39667874574661255, 0.5354987382888794, -0.9238322377204895, 0.9497970938682556, 0.6149677038192749, -0.2571328282356262, 0.027367081493139267, 0.6777125000953674, -0.5908979773521423, -0.8660421967506409, 0.6740487217903137, -0.8889123201370239, -0.039394523948431015, 0.24102389812469482, -0.2015501856803894, -0.39758577942848206, 0.7130681276321411, 0.20175525546073914, -1.2589671611785889, -1.3874064683914185, 0.6251598000526428, -0.48605504631996155, 0.3127760589122772, -0.18380104005336761, 0.018662720918655396, -1.1010003089904785, -0.5394978523254395, -0.16530422866344452, 0.4340803623199463, -0.5445672273635864, 1.144697904586792, 1.1159099340438843, -0.643643856048584, -0.1864040046930313, 0.33046966791152954, 0.18209631741046906, -0.18524661660194397, 0.6852990388870239, 0.3605620563030243, -0.3135530650615692, 0.2798297107219696, 0.18978136777877808, 0.010905340313911438, -0.8896592855453491, 0.4477544128894806, 1.0701102018356323, -0.48442965745925903, -0.19615551829338074, 1.1629223823547363, 0.03545801714062691, -0.8322797417640686, 0.3381977677345276, -1.0847162008285522, -0.7343963384628296, -0.1265169233083725, 0.6823623180389404, -0.42043158411979675, -0.4294435381889343, -0.39243337512016296, -0.34963271021842957, 0.7344809174537659, -0.24679677188396454, -0.6033281087875366, 0.15998663008213043, -0.05977483093738556, -0.22964678704738617, 0.48262327909469604, 1.1092634201049805, -1.4366894960403442, -0.8635137677192688, -0.9808794260025024, -0.8282471895217896, 0.05167311057448387, 0.1776111125946045, -0.10495283454656601, -0.5454446077346802, 0.8276650309562683, 0.8709030747413635, 0.21929362416267395, 0.22458885610103607, 0.008983470499515533, 0.17514340579509735, 0.8650467991828918, -0.1834075152873993, -0.5508782267570496, 0.11322759836912155, 1.5502147674560547, 1.1914759874343872, -0.9567079544067383, 0.1659279465675354, -0.12283815443515778, -0.7690954804420471, 0.62872713804245, 0.3005245625972748, -0.5291697978973389, 1.1124225854873657, -0.5772965550422668, 0.13538461923599243, 0.19443608820438385, -0.9095361828804016, -0.643808126449585, 0.9789429306983948, 1.1399095058441162, 0.1270313262939453, 0.02987861819565296, 0.09382850676774979, 0.4395902454853058, 0.44432106614112854, 0.24062897264957428, 0.6508587002754211, 0.1346009373664856, -0.46011853218078613, 0.4294036030769348, 0.10526882857084274, 0.3919263482093811, -0.46476513147354126, -0.5050620436668396, -0.003549700602889061, 0.5929795503616333, 0.17881377041339874, 0.7079815864562988, 1.1053310632705688, 5.2775139920413494e-05, 0.5247400999069214, 0.168587327003479, 0.5860212445259094, -0.29544663429260254, -0.06277298927307129, -0.19449537992477417, -1.1498602628707886, 0.16289007663726807, -0.4969598352909088, -0.4788702726364136, -0.029414361342787743, -0.10403764992952347, 0.420499712228775, -0.7346028089523315, 0.2331274449825287, 1.157497525215149, 0.3307870030403137, 0.3251645267009735, -0.67811119556427, -0.5848388671875, -0.42187508940696716, -0.6980338096618652, 0.20396897196769714, -0.3720989227294922, 0.11365227401256561, -0.4786164462566376, -0.12607398629188538, -0.06485484540462494]}, "authors": [{"authorId": "2294723453", "name": "Jienneg Chen"}, {"authorId": "2156559", "name": "Qihang Yu"}, {"authorId": "2266472250", "name": "Xiaohui Shen"}, {"authorId": "2253485882", "name": "Alan L. Yuille"}, {"authorId": "2266697544", "name": "Liang-Chieh Chen"}], "references": [{"paperId": "dd720f798bf0d16750c442e8caa0bf7a59890fac", "title": "COCONut: Modernizing COCO Segmentation"}, {"paperId": "16513bc0dc13902334a9cb3657056763efdcec6f", "title": "Towards Open-Ended Visual Recognition with Large Language Model"}, {"paperId": "21a001954cc3b620a7db2a0f29c85d99732bc608", "title": "PolyMaX: General Dense Prediction with Mask Transformer"}, {"paperId": "68e0e789b5147b1e7d028c7a825650075f4e26bf", "title": "PaLI-3 Vision Language Models: Smaller, Faster, Stronger"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "d081501a74ef2934a2c30755b17fb5c339399b88", "title": "SIEVE: Multimodal Dataset Pruning Using Image Captioning Models"}, {"paperId": "b6bb3b09d6b77d008adc4788c9212a90040f2376", "title": "CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction"}, {"paperId": "f349e5e8f0d18c948c1ffd92d3791db2b0ba2e55", "title": "Data Filtering Networks"}, {"paperId": "eeb713300eb55b954a5d86acf8628726b99e2da6", "title": "The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering"}, {"paperId": "4bfedad39c1cdd7cf2cc2beaa09d4fe8c5166f18", "title": "TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance"}, {"paperId": "2b26b17fe3a909bc0f5408b3328308153b31f22e", "title": "Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP"}, {"paperId": "94972e30504017156ef5b5debc419bf6edc67384", "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"}, {"paperId": "4309d572a37d655779f9dce6a2c98c66334132de", "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "b37b1dc72b1882858f5120f2cd6883134089a6ed", "title": "MMBench: Is Your Multi-modal Model an All-around Player?"}, {"paperId": "bc82e97888aa8b98d9f4ef1397c09d85dd8a0144", "title": "ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation"}, {"paperId": "81ee57dfc7a549888b4746e4ec260454479e4d1f", "title": "CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a $10, 000 Budget; An Extra $4, 000 Unlocks 81.8% Accuracy"}, {"paperId": "697e0add95e880bd42e00bef838181e105f91981", "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"}, {"paperId": "16b4620b59bfef414d702214a717856c943db7fb", "title": "Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design"}, {"paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773", "title": "Evaluating Object Hallucination in Large Vision-Language Models"}, {"paperId": "a9fbf441bd074b6daeae0903a040aefaab61e757", "title": "An Inverse Scaling Law for CLIP Training"}, {"paperId": "5faee4af70f65e609eafe1f23f26593423f03750", "title": "Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers"}, {"paperId": "f9570989919338079088270a9cf1a7afc8db8093", "title": "DataComp: In search of the next generation of multimodal datasets"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "38a71cff020621b2435ae769b0667d7a4595d0e9", "title": "RECLIP: Resource-efficient CLIP by Training with Small Images"}, {"paperId": "362983f6f6b3e0335f1267b7b9d0288cc4d2619f", "title": "FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation"}, {"paperId": "a08b7123a7158f1a7fbbc18e8b5aaebd47980ecf", "title": "EVA-CLIP: Improved Training Techniques for CLIP at Scale"}, {"paperId": "35aba190f28b5c39df333c06ca21f46bd4845eba", "title": "Sigmoid Loss for Language Image Pre-Training"}, {"paperId": "5a47fef193484287e442d6d82af1969b5b269015", "title": "CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "9856e8752b8ae09e2bb7711b9668e086d5ad9feb", "title": "Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection"}, {"paperId": "323400245885e08ad498cd108e30e18020662278", "title": "Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "bce29cc829fab288c41ae5678e1bb5b95bf218d4", "title": "Aligning Bag of Regions for Open-Vocabulary Object Detection"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "37209c83482d6cbf14492cd9e79455c0d35eaf87", "title": "Learning Customized Visual Models with Retrieval-Augmented Knowledge"}, {"paperId": "cfca7eedc6ede9d363d1662280a74d78dcdc9d4a", "title": "Scaling Language-Image Pre-Training via Masking"}, {"paperId": "b2eb28dd5e2340a5e9de8ae82feaca7b6a265b4e", "title": "Learning Object-Language Alignments for Open-Vocabulary Object Detection"}, {"paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"}, {"paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"}, {"paperId": "a8a2a8229f99c291bf71ec92b801a073854c52e2", "title": "MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models"}, {"paperId": "83aee45f8afc470f5dbaabc05ccca9304599baf2", "title": "F-VLM: Open-Vocabulary Object Detection upon Frozen Vision and Language Models"}, {"paperId": "d3135733aa39dec20ce72aa138589dda27c8406d", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "1114863be2a713a14771ccacb5c9436fb4a375e2", "title": "MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining"}, {"paperId": "930e17321a3ad3721339c6dc4bd0b624d90ee695", "title": "Open-Vocabulary Universal Image Segmentation with MaskCLIP"}, {"paperId": "ece7aa5352dc5bc0463e185ac9eefe4a122f6fad", "title": "WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models"}, {"paperId": "d3461268e1153b1abec8f999f6375378a33e0061", "title": "Bridging the Gap between Object and Image-level Representations for Open-Vocabulary Detection"}, {"paperId": "dd1139cfc609c2f3263d02e97176d5275caebc0a", "title": "EfficientFormer: Vision Transformers at MobileNet Speed"}, {"paperId": "31a9744bd5421b3fbbad2ab38ce33bb2f352c77a", "title": "CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation"}, {"paperId": "a26a7a74f1e5fd562be95c3611a0680759fbdf84", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc", "title": "MaxViT: Multi-Axis Vision Transformer"}, {"paperId": "d6c73f758b05f38529c1a96cab7e908a2047dabd", "title": "Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language Model"}, {"paperId": "403ad5d6e78fcf29f1ac526fbc9ff6cbfea555eb", "title": "Open-Vocabulary DETR with Conditional Matching"}, {"paperId": "0b5f27a5766c5d1394a6282ad94fec21d620bd6b", "title": "GroupViT: Semantic Segmentation Emerges from Text Supervision"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1", "title": "UniFormer: Unifying Convolution and Self-Attention for Visual Recognition"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "86b42cac364985919987789795be7c3a577ee3de", "title": "Detecting Twenty-thousand Classes using Image-level Supervision"}, {"paperId": "e9581d9758062f76e029bd19a58c4ae976cfb414", "title": "SLIP: Self-supervision meets Language-Image Pre-training"}, {"paperId": "8d737dc6a91a7bfde20aed7bb13d100476de5ae3", "title": "Scaling Open-Vocabulary Image Segmentation with Image-Level Labels"}, {"paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "title": "High-Resolution Image Synthesis with Latent Diffusion Models"}, {"paperId": "837173ef1f260adc0d50b76675915776e1cc8ade", "title": "RegionCLIP: Region-based Language-Image Pretraining"}, {"paperId": "ab2a8ca21309859ed027928dc38e6915be0e6776", "title": "Extending the WILDS Benchmark for Unsupervised Adaptation"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "0a7e7347e16bf13d710f6f3d30748baabdbb96ad", "title": "Extract Free Dense Labels from CLIP"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "197d5867a45a2988f4dd159063cdfbfe90164962", "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "a66686e60a3eda0c606e036403cf0a07a5962595", "title": "Mobile-Former: Bridging MobileNet and Transformer"}, {"paperId": "7b664a306b7d2f68dd816ea1d6586cf3472d75c1", "title": "Early Convolutions Help Transformers See Better"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "ac74a160e0ca53d3ffb15f79f0b9d3911df2fc28", "title": "Glance-and-Gaze Vision Transformer"}, {"paperId": "2ec93b332b61d3915dd667d45b4900d336985b8a", "title": "The iWildCam 2021 Competition Dataset"}, {"paperId": "cf9b8da26d9b92e75ba49616ed2a1033f59fce14", "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation"}, {"paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "title": "Multiscale Vision Transformers"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "8f8f73f0f208302546c825ed474432389ed63be4", "title": "EfficientNetV2: Smaller Models and Faster Training"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880", "title": "Rethinking Spatial Dimensions of Vision Transformers"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "96da196d6f8c947db03d13759f030642f8234abf", "title": "DeepViT: Towards Deeper Vision Transformer"}, {"paperId": "610b302950a19acef1c45456111dcd495f638c18", "title": "ConViT: improving vision transformers with soft convolutional inductive biases"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "title": "Bottleneck Transformers for Visual Recognition"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "40848b41ed8c9c255ecd8a920006877691b52d03", "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts"}, {"paperId": "787119e3c3f819244c82b7d97779473773e60696", "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"}, {"paperId": "497d765b1eccbc9e99a37592a1860744559695db", "title": "Open-Vocabulary Object Detection Using Captions"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "022622e024890d6e044ac50e2da6b44c59bdf418", "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "e065392ba6160ac27906e5a79398c9a71e4a4bb7", "title": "Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation"}, {"paperId": "45557cc70cd6989ab6b03e5aeb787e34299099f7", "title": "Natural Adversarial Examples"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "f902a64f7d08aaa6bfca7463e8729952ddc6134e", "title": "LVIS: A Dataset for Large Vocabulary Instance Segmentation"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "4ae0c4a511697e960c477ea3e37b3e11bf3e0e02", "title": "Learning Robust Global Representations by Penalizing Local Predictive Power"}, {"paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907", "title": "Towards VQA Models That Can Read"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "title": "Do ImageNet Classifiers Generalize to ImageNet?"}, {"paperId": "1b27b9cfe0ce17950b6ea72f9ef8cf5a7459bccd", "title": "From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The CAMELYON17 Challenge"}, {"paperId": "2a96afaf3261a87f0daa51699b4b3cf169e092c4", "title": "Rotation Equivariant CNNs for Digital Pathology"}, {"paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"}, {"paperId": "9217e28b2273eb3b26e4e9b7b498b4661e6e09f5", "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "dce916351ef589afa7a63452648dd8acba931e92", "title": "Panoptic Segmentation"}, {"paperId": "a588d38ec81c0337b445931eadf6f443aea13380", "title": "Functional Map of the World"}, {"paperId": "79828e6e9f137a583082b8b5a9dfce0c301989b8", "title": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "9c88c2357abcd58cc330179c1965fe0a8c067ebc", "title": "EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "03eb382e04cca8cca743f7799070869954f1402a", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c", "title": "Densely Connected Convolutional Networks"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "cab372bc3824780cce20d9dd1c22d4df39ed081a", "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"}, {"paperId": "c8c494ee5488fe20e0aa01bddf3fc4632086d654", "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "7ffdbc358b63378f07311e883dddacc9faeeaf4b", "title": "Fast R-CNN"}, {"paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "39ad6c911f3351a3b390130a6e4265355b4d593b", "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs"}, {"paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0", "title": "Fully convolutional networks for semantic segmentation"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "8e3f12804882b60ad5f59aad92755c5edb34860e", "title": "Food-101 - Mining Discriminative Components with Random Forests"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "c9a6c7bfe831f2b154deac4409c35633c63ef326", "title": "SUN Database: Exploring a Large Collection of Scene Categories"}, {"paperId": "3419ccd5c94d301ee08d716d037f0c3c6a62e78e", "title": "The Role of Context for Object Detection and Semantic Segmentation in the Wild"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "44040913380206991b1991daf1192942e038fe31", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}, {"paperId": "a83cec6a91701bd8500f8c43ad731d4353c71d55", "title": "3D Object Representations for Fine-Grained Categorization"}, {"paperId": "18c125ce0f64e85577f7d30132cf0e92ec664bf4", "title": "Describing Textures in the Wild"}, {"paperId": "522d65a3db7431015aeaa201a7fc4450a57e40c3", "title": "Fine-Grained Visual Classification of Aircraft"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "84b50ebe85f7a1721800125e7882fce8c45b5c5a", "title": "Cats and dogs"}, {"paperId": "de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42", "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite"}, {"paperId": "be9a17321537d9289875fe475b71f4821457b435", "title": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning"}, {"paperId": "22fe619996b59c09cb73be40103a123d2e328111", "title": "The German Traffic Sign Recognition Benchmark: A multi-class classification competition"}, {"paperId": "3a9b175324ba11bc0e16c0633912d897b2fac4e2", "title": "The Pascal Visual Object Classes (VOC) Challenge"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "02b28f3b71138a06e40dbd614abf8568420ae183", "title": "Automated Flower Classification over a Large Number of Classes"}, {"paperId": "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories"}, {"paperId": "7a29f47f6509011fe5b19462abf6607867b68373", "title": "GPT-4V(ision) System Card"}, {"paperId": "43817ce40b39be702973e1f88d9e04b579ae6b78", "title": "Exploring the Influence of Information Entropy Change in Learning Systems"}, {"paperId": "90e25fe6534b3d6ec10eb65f9614dffe2d5a1d7c", "title": "Beyond web-scraping: Crowd-sourcing a geographically diverse image dataset"}, {"paperId": "f3b1dd33a2a8b533a0c08382b2a2bbf721beac21", "title": "k-means Mask Transformer"}, {"paperId": "4a761322413c40d482bce9f3477137ca08956861", "title": "Simple Open-Vocabulary Object Detection"}, {"paperId": "d0fec1f23ca591478f210be401e64120022a9a1c", "title": "The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World"}, {"paperId": "99c7644524260838bff7ce946942875c96aae328", "title": "A Simple Baseline for Zero-shot Semantic Segmentation with Pre-trained Vision-language Model"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "639174f32a71ecfe9041ad05ff30eb39bd4977bf", "title": "ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models"}, {"paperId": null, "title": "Searching for mo-bilenetv3"}, {"paperId": "02227c94dd41fe0b439e050d377b0beb5d427cda", "title": "Reading Digits in Natural Images with Unsupervised Feature Learning"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2", "title": "The mnist database of handwritten digits"}, {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4", "title": "Gradient-based learning applied to document recognition"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt"}]}