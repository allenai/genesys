{"paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334", "title": "Gated Linear Attention Transformers with Hardware-Efficient Training", "abstract": "Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.", "venue": "arXiv.org", "year": 2023, "citationCount": 44, "influentialCitationCount": 9, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments."}, "embedding": {"model": "specter_v2", "vector": [0.3334692418575287, 1.1727718114852905, -0.23695746064186096, 0.13709458708763123, -0.20865200459957123, -0.013578424230217934, 0.6103434562683105, -0.39028504490852356, -0.40361344814300537, -0.3707295060157776, 0.42176347970962524, -0.4777998626232147, 0.7962778210639954, 0.01388603262603283, -0.11106367409229279, 0.12636582553386688, -0.9044141173362732, 0.28379344940185547, -0.03497621417045593, -0.09744955599308014, 0.013713192194700241, -0.33672988414764404, -0.8337613344192505, 0.12727797031402588, -0.06657108664512634, 0.9646679162979126, 0.3062630891799927, 0.9468788504600525, -0.3280864953994751, 0.693647027015686, 0.4841459095478058, -0.09805353730916977, 0.19930985569953918, -0.14537520706653595, -0.4606384336948395, -0.02095581591129303, 0.6937510967254639, -0.23382626473903656, -0.5479835271835327, 0.8105374574661255, -0.12398093193769455, 0.0826350525021553, -0.17245544493198395, -0.6651649475097656, -0.3527149558067322, 0.8215200304985046, 0.652796745300293, 0.8579342365264893, -0.5218628644943237, -0.215559184551239, 1.1266586780548096, -1.1403570175170898, -0.32278403639793396, 1.5773067474365234, 0.3666798174381256, 0.39105701446533203, 0.04029468819499016, -0.5418315529823303, 1.0663326978683472, 0.23075029253959656, -0.7630271315574646, -0.3485575020313263, -0.12007598578929901, 0.09933635592460632, 2.145334005355835, -0.4722667634487152, 0.3002155125141144, 0.3973108232021332, 0.3079122304916382, 1.1447073221206665, -0.006249686703085899, -0.7136476635932922, -0.36272987723350525, 0.01759043149650097, 0.574169397354126, 0.7994564175605774, -0.2551235258579254, 0.18328845500946045, -0.7245205044746399, 0.2523835003376007, 0.33891040086746216, 0.22725699841976166, 0.34941208362579346, -0.01857367530465126, -0.1739797592163086, 0.5807363986968994, 0.6070756316184998, 0.7607516050338745, -0.33996814489364624, 0.9839878082275391, 0.528165876865387, 0.13520434498786926, -0.13742811977863312, 0.4090583026409149, -0.07518564164638519, 0.5652808547019958, -0.8054438233375549, -0.11898700147867203, -0.4685056805610657, 1.177019715309143, -0.1550358533859253, 0.7433556318283081, -0.6129297018051147, -0.24447019398212433, 1.2325685024261475, 0.32369422912597656, 0.3999793827533722, -0.2896655201911926, 0.15390068292617798, -0.5420366525650024, -0.3856324255466461, -0.5561652779579163, -0.5820069909095764, -0.39526140689849854, -1.053446888923645, -1.020525336265564, -0.7067672610282898, 0.29620927572250366, -0.895354688167572, 0.6981909275054932, -0.7493777871131897, -0.03666507452726364, -0.11040821671485901, 0.16559703648090363, 0.5409231781959534, 0.7417862415313721, 0.21777039766311646, 0.18180088698863983, 1.1733930110931396, -0.992954671382904, -0.6254595518112183, -1.153181791305542, 0.51162189245224, 0.2544267773628235, 0.5195429921150208, 0.004527820274233818, -1.2208678722381592, -0.9858506321907043, -0.7604401111602783, -0.20573005080223083, -0.5227575302124023, 0.01288171112537384, 0.9098945260047913, 0.4356507956981659, -1.2463972568511963, 0.16452361643314362, -0.6011837720870972, -0.2675463855266571, 0.4896174371242523, 0.33710670471191406, 0.4909154772758484, -0.3054456412792206, -1.1582260131835938, 0.48725616931915283, 0.3337770998477936, -0.28532177209854126, -0.5645623803138733, -0.6775120496749878, -1.106616735458374, 0.15645895898342133, 0.22943052649497986, -0.36561277508735657, 1.2885595560073853, -0.12685678899288177, -1.1733633279800415, 0.6160080432891846, -0.09372634440660477, -0.18099260330200195, -0.031679391860961914, -0.24991099536418915, -0.24248729646205902, -0.48586127161979675, -0.4962904453277588, 0.6488611102104187, 0.43658143281936646, 0.21496367454528809, -0.3144185245037079, 0.19299688935279846, -0.16626568138599396, -0.385669082403183, -0.6787316203117371, 0.7621450424194336, -0.6376730799674988, 0.22158116102218628, 0.12905766069889069, 0.45195361971855164, -0.2858683466911316, -0.41578957438468933, -0.5923579931259155, -0.8976728916168213, 0.6013128161430359, -0.05089987441897392, 1.1519737243652344, -0.9501996636390686, -0.8818266987800598, -0.18681785464286804, 0.1894255429506302, -0.07817686349153519, -0.4681255519390106, 0.2551056742668152, -0.6391911506652832, 0.2753845453262329, 0.3439365327358246, -0.883178174495697, 0.1898856908082962, -0.35064807534217834, -0.5739927887916565, -0.1511865258216858, -0.08352495729923248, 1.32550847530365, -0.7205301523208618, -0.13510902225971222, -0.08228842169046402, 0.2632574439048767, -1.1347159147262573, 1.3813725709915161, -0.21432408690452576, 0.14430643618106842, 0.03548187389969826, -0.10245896130800247, -0.07578819990158081, -0.5661755204200745, 0.6205109357833862, -0.6534209847450256, -0.17847299575805664, 0.7095249891281128, 0.034812361001968384, 1.0419034957885742, -0.5920557975769043, 0.9558120965957642, -0.11073610186576843, -0.8282120823860168, 0.35789817571640015, 0.20671913027763367, -0.5335844159126282, -0.7723537087440491, 0.560614287853241, 0.04979366809129715, -0.6310755014419556, 0.1806250363588333, 0.5989317297935486, 0.9155864715576172, -0.6201501488685608, -0.26202303171157837, 0.7576821446418762, -0.19415849447250366, -0.21125538647174835, 0.42552468180656433, 0.8534983396530151, 0.20603972673416138, 0.6451383233070374, -0.2415122538805008, 0.1693452149629593, -0.9901106357574463, 0.007887378334999084, 0.6960753202438354, 0.5273374319076538, 0.5543239712715149, 0.6093393564224243, -1.0597305297851562, -0.6561285257339478, 0.36369335651397705, 0.7937710285186768, 1.44563889503479, -0.4012008011341095, 0.09198857098817825, -0.7380535006523132, -0.25014612078666687, -0.721387505531311, 0.2078791707754135, -0.19558914005756378, -0.3581627309322357, -0.7739272117614746, -0.8794072270393372, 0.858832061290741, 0.5084630846977234, 1.072074055671692, -0.690915584564209, -0.5418453812599182, -0.4611814022064209, 0.6085858941078186, -1.2532333135604858, -0.545995831489563, 0.6904047727584839, -0.4181683659553528, 0.2176993042230606, 0.49081578850746155, -0.2986292243003845, 0.2634565234184265, -0.7095116972923279, 1.0020782947540283, -0.8765321969985962, -0.033800188452005386, 0.1907953917980194, 0.7035514116287231, -0.6256265044212341, -0.4933437705039978, 0.5955902338027954, 0.21234779059886932, -0.05188308656215668, 0.3283890187740326, 0.3200698494911194, 0.1774224042892456, -0.19137315452098846, -0.20287108421325684, 0.1487821489572525, 0.3328271210193634, -0.08756484091281891, 0.6260998845100403, -0.3321332037448883, -0.06635452061891556, -1.2820903062820435, 0.5687714219093323, 0.4765956997871399, -0.8320043683052063, 0.19668728113174438, -0.5003176927566528, -0.17498086392879486, 0.6636892557144165, -0.47065767645835876, -0.07166603207588196, -0.6334983706474304, 0.1142059788107872, -0.6814133524894714, -0.2683061957359314, 0.013530431315302849, -0.004424723330885172, -0.05748652294278145, 0.13258972764015198, 0.48421111702919006, -0.008154066279530525, 0.1569753736257553, 0.5042716264724731, -0.7836015820503235, 0.5990622043609619, 0.5481305718421936, 0.07147621363401413, -0.28909385204315186, 0.1139497384428978, -0.8883634209632874, -0.6796280741691589, 0.1192455068230629, -0.18628671765327454, -0.30148136615753174, 0.40773969888687134, -0.5591298341751099, -1.0896482467651367, 0.023281218484044075, -0.9877790808677673, -0.5748178362846375, 0.040574491024017334, -0.19029483199119568, 0.08317358791828156, -1.2857835292816162, -1.0234750509262085, -0.5940411686897278, -0.8580559492111206, -1.0423184633255005, 0.054718151688575745, 0.26336804032325745, -0.19630271196365356, -0.8088651299476624, -0.3531285524368286, -0.4523547887802124, 1.055249571800232, -0.5539407134056091, 0.7717870473861694, -0.21473197638988495, -0.39295467734336853, -0.37562718987464905, 0.3232581317424774, 0.47527366876602173, -0.4607391059398651, 0.029840875416994095, -1.0443347692489624, 0.3279605507850647, -0.607883632183075, -0.5251349210739136, 0.07970942556858063, 0.5435822606086731, 0.7945665717124939, 0.014679539948701859, -0.3739511966705322, 0.478424072265625, 1.1872085332870483, -0.5530749559402466, 0.26035618782043457, 0.044256266206502914, 1.4274914264678955, -0.3016166687011719, -0.32354798913002014, 0.4913695752620697, 0.4772253632545471, 0.5830546021461487, 0.5498443841934204, -0.3921957015991211, -0.16213180124759674, -0.4183031916618347, 0.5052076578140259, 1.2868304252624512, 0.4353770315647125, 0.02410605549812317, -1.049371361732483, 0.6045369505882263, -0.811629593372345, -0.5584479570388794, 0.6659945249557495, 0.6993409395217896, 0.10360962152481079, -0.04274532571434975, -0.43300309777259827, -0.20608964562416077, 0.5066405534744263, 0.30240604281425476, -0.38822072744369507, -1.0956158638000488, 0.05421823635697365, 0.4545433223247528, 0.43849000334739685, 0.7621506452560425, -0.07292110472917557, 0.8065296411514282, 14.942459106445312, 0.6385288834571838, -0.4160338342189789, 0.7492769956588745, 0.8509883284568787, 0.12104958295822144, -0.18058441579341888, -0.17963746190071106, -1.3994388580322266, -0.3377736210823059, 1.338060736656189, 0.40552443265914917, 0.6428156495094299, 0.24943208694458008, 0.010679071769118309, 0.5988609790802002, -0.6585415005683899, 0.7854071259498596, 0.5357654094696045, -1.116326093673706, 0.20007070899009705, 0.29691022634506226, 0.21060973405838013, 0.5454679131507874, 0.873302161693573, 0.9282895922660828, 0.5292361974716187, -0.2549927830696106, 0.5752018690109253, 0.29304301738739014, 1.1337401866912842, 0.10711265355348587, 0.20344215631484985, 0.3708370625972748, -0.9920852780342102, -0.41569045186042786, -0.3027086853981018, -1.2330106496810913, 0.29831743240356445, 0.16036058962345123, -0.48125916719436646, -0.7587646245956421, -0.10217329859733582, 0.4868914484977722, 0.20092082023620605, 0.1294674426317215, -0.6258346438407898, 0.7482653856277466, -0.18891766667366028, -0.04751099646091461, 0.486387699842453, 0.7468863725662231, -0.2670350968837738, 0.11558946967124939, -0.1014631986618042, 0.1286250650882721, 0.19528840482234955, 0.4807681441307068, -0.5574196577072144, -0.5379037261009216, -0.10851172357797623, -0.34595805406570435, 0.043838296085596085, 0.9367582201957703, 0.46262818574905396, 0.1166730746626854, -0.5847465395927429, 0.019876299425959587, 0.5315613150596619, -0.009047331288456917, -0.5456658601760864, -0.03766221925616264, 0.19520331919193268, -0.5599744915962219, -0.047626860439777374, 0.4585414528846741, -0.3188817501068115, -0.5208854079246521, -1.0426321029663086, -0.5366608500480652, 0.38638195395469666, -0.8430814146995544, -0.32927757501602173, 1.0315583944320679, -0.4117176830768585, 0.038063645362854004, 0.4884541928768158, -0.7594828605651855, -0.06275723874568939, 0.4135342240333557, -1.2599461078643799, -0.8463646173477173, -0.04071708023548126, -0.336213082075119, -0.3511268198490143, -0.13514664769172668, 0.9687694311141968, 0.29988893866539, -0.7032448053359985, 0.1914912909269333, -0.4543646275997162, 0.019641436636447906, -0.4920594096183777, -0.5143817663192749, 1.1289260387420654, 0.5602321028709412, -0.23958753049373627, -0.29464423656463623, -0.010125132277607918, 0.6483951807022095, -0.7780621647834778, -0.173873171210289, 0.8259110450744629, -0.7371190786361694, -0.23154403269290924, -1.172473669052124, -0.6744025349617004, 0.9838782548904419, 0.7274020314216614, 0.1440945714712143, -0.01844056136906147, 0.0632317066192627, -0.49928462505340576, -0.19494932889938354, -0.3636573255062103, -0.121455617249012, 0.2520545423030853, -0.8434216380119324, -0.10158243030309677, -0.19630548357963562, 0.7160468697547913, -1.0873712301254272, -0.4504469931125641, -0.40952587127685547, -0.06787841022014618, 0.020265113562345505, 0.7967643141746521, -0.4848448634147644, 0.3886162042617798, 1.0275287628173828, 0.027002641931176186, -0.40662524104118347, -0.5333302617073059, -0.7468641400337219, -0.136705219745636, 0.10711029917001724, 0.474117249250412, -0.45674243569374084, 0.2816527187824249, 0.6002584099769592, -0.28883957862854004, -0.5595240592956543, -0.3971785008907318, -0.04481783136725426, -0.29055532813072205, -0.6686204671859741, 0.13607937097549438, -0.04190250113606453, 0.06896257400512695, 0.37439918518066406, 0.16796496510505676, 0.8633295297622681, -0.28270232677459717, -0.7170670628547668, 0.01761040650308132, 0.13389529287815094, -0.05623360350728035, -0.6122310161590576, -1.0183639526367188, -1.6987178325653076, -0.12610220909118652, -0.9333518743515015, 0.06723185628652573, -0.7677445411682129, -0.33309462666511536, 0.12388691306114197, -0.35122060775756836, 0.6852548718452454, 0.22956760227680206, -0.11115021258592606, -0.27108049392700195, -0.6651259064674377, -0.8277310729026794, 0.8006948232650757, 0.47243407368659973, -0.5569489598274231, 0.07178409397602081, -0.29205483198165894, -0.005894626025110483, 0.31106770038604736, 0.2336542308330536, -0.38559490442276, -0.5930066108703613, -1.2611613273620605, 0.3661404550075531, -0.1595482975244522, -0.2502884864807129, -1.0356571674346924, 0.7495266795158386, 0.46432188153266907, -0.39345425367355347, 0.04945041984319687, 0.25329843163490295, -0.7698665261268616, -0.6862821578979492, 0.5258166790008545, -0.8375329375267029, 0.2056569904088974, 0.5705581307411194, -0.9342222213745117, -0.0838872417807579, 0.8374665379524231, 0.09154251217842102, -0.8420336246490479, -0.8265270590782166, 0.460915207862854, -0.9282122850418091, 0.11784476041793823, -0.38149264454841614, -0.0823846310377121, -0.9213801026344299, -0.4652904272079468, -0.14766117930412292, 0.3191213011741638, -0.7131189703941345, 0.7074236869812012, 0.1254754215478897, -1.0977472066879272, 0.20306682586669922, 0.3372712731361389, -0.2653985619544983, 0.0796530544757843, 0.32429295778274536, 0.7147474884986877, -0.37621182203292847, 0.9321136474609375, 0.10155634582042694, 0.320850133895874, -0.8660964369773865, 0.4676531255245209, 1.0195308923721313, -0.4900698959827423, -0.19516217708587646, 1.1774497032165527, -0.04359639063477516, -0.9998821020126343, 0.37490180134773254, -1.6500684022903442, -0.40606996417045593, -0.30337411165237427, 0.5828666687011719, -0.1644650399684906, 0.19468720257282257, 0.3510062098503113, -0.6689218878746033, 0.0798700749874115, 0.1993393450975418, -0.2188243567943573, 0.5095658302307129, -0.016162807121872902, -0.2394930124282837, 0.7718340754508972, 0.7171922922134399, -0.9117807745933533, -0.6853031516075134, -0.99828040599823, -0.5020340085029602, 0.39325693249702454, 0.6541587114334106, -0.08957816660404205, -1.129656195640564, 1.0774487257003784, 0.6393879652023315, 0.42629238963127136, 0.49890682101249695, -0.05033792927861214, -0.16386188566684723, 0.7283273935317993, 0.31197473406791687, -0.06620925664901733, -0.48437434434890747, 1.3749721050262451, 0.9787259101867676, -0.5781328082084656, 0.2590118646621704, -0.2203444540500641, -0.5876933336257935, 0.7583420276641846, 0.39292818307876587, 0.023697495460510254, 0.8081200122833252, 0.11525391787290573, -0.13602371513843536, 0.08756357431411743, -1.074439525604248, -0.27797871828079224, 0.5241171717643738, 0.8145958781242371, 0.8230492472648621, 0.2204936146736145, 0.4405686855316162, 0.822831928730011, 0.14488756656646729, 0.018894800916314125, 0.16621100902557373, 0.4996264576911926, -0.2404116690158844, 0.35084062814712524, -0.2033625841140747, 0.3283686637878418, -0.4769558012485504, -0.8925892114639282, 0.2848845422267914, 0.40129682421684265, 0.2546767592430115, 0.44170883297920227, 1.1812480688095093, -0.14142251014709473, 0.7594234347343445, 0.031598083674907684, 0.5208846926689148, -0.4779173731803894, -0.27382892370224, -0.16022934019565582, -0.6508455276489258, -0.2906379699707031, 0.11996481567621231, -0.33221665024757385, -0.31599926948547363, -0.2890568971633911, 0.22765591740608215, -0.26376649737358093, 0.15400829911231995, 0.9855934977531433, 0.6680202484130859, 1.0610283613204956, -0.20330509543418884, -0.8054165840148926, -0.045180171728134155, -0.9306460618972778, 0.14809714257717133, -0.8635451197624207, -0.16513773798942566, 0.13085797429084778, 0.026061149314045906, -0.232854962348938]}, "authors": [{"authorId": "50591392", "name": "Songlin Yang"}, {"authorId": "2257409822", "name": "Bailin Wang"}, {"authorId": "2273540596", "name": "Yikang Shen"}, {"authorId": "1819152", "name": "Rameswar Panda"}, {"authorId": "2261394948", "name": "Yoon Kim"}], "references": [{"paperId": "1d4c48335d841014d0145256c3c4e7f6c426b8fb", "title": "You Only Cache Once: Decoder-Decoder Architectures for Language Models"}, {"paperId": "98372f2e164a4ae44c390a72a39bd6d7675cae89", "title": "xLSTM: Extended Long Short-Term Memory"}, {"paperId": "46732358e98ce6be0c564ae11f71d556a64b4c35", "title": "HGRN2: Gated Linear RNNs with State Expansion"}, {"paperId": "157ed5647da39a7f5d33a84a90414b2a9e97e301", "title": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence"}, {"paperId": "cde66097f4123a62bf3e28d48c764648e8c69f72", "title": "Simple linear attention language models balance the recall-throughput tradeoff"}, {"paperId": "f4a0c4154203808f362e4678f3741b3d317fdc82", "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry"}, {"paperId": "1df04f33a8ef313cc2067147dbb79c3ca7c5c99f", "title": "Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces"}, {"paperId": "3719ad19da30771aba5d5c48491a21d6c393832d", "title": "Vivim: a Video Vision Mamba for Medical Video Object Segmentation"}, {"paperId": "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27", "title": "MambaByte: Token-free Selective State Space Model"}, {"paperId": "5358b0e98934f1bbe8f6123a529bbb91dd36d662", "title": "SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation"}, {"paperId": "b24e899ec0f77eef2fc87a9b8e50516367aa1f97", "title": "VMamba: Visual State Space Model"}, {"paperId": "38c48a1cd296d16dc9c56717495d6e44cc354444", "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model"}, {"paperId": "3e8d4062ec4353ff2701c7769336dbdb97f8814c", "title": "Transformers are Multi-State RNNs"}, {"paperId": "c1a04730c83967d0bb904b02263b17893cb50bad", "title": "U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation"}, {"paperId": "7294c426b8a95975ca932eaf8f700acdd3d950b2", "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models"}, {"paperId": "1be73fa3e856c33d0aed1d9e46693523e7fa3c60", "title": "Zoology: Measuring and Improving Recall in Efficient Language Models"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "31245344a6eb6cd897a71928dc4b174ab75e4070", "title": "Diffusion Models Without Attention"}, {"paperId": "e10ee483325f590b1e139dfafdb03edeb2e1766a", "title": "Linear Log-Normal Attention with Unbiased Concentration"}, {"paperId": "ade22704be8a0fc3730d320cc7934b2ccbcd97e4", "title": "Striped Attention: Faster Ring Attention for Causal Transformers"}, {"paperId": "5c104f905fcacf390270f619f232a2ba4eb873f2", "title": "FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores"}, {"paperId": "434d751d355d7a7c20efa570e785c76286245e77", "title": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling"}, {"paperId": "813987d484a9eea03e95e677707fd011947a4154", "title": "First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models"}, {"paperId": "d7f64f2bdd80ea15f21ef7d867e102ac9ecdc797", "title": "GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling"}, {"paperId": "cb0ac335adda4ceef9987cbcbca9129e71c37f0a", "title": "Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions"}, {"paperId": "c67052a650aa5c8d9138f90972372cdd858a87eb", "title": "Efficient Parallelization of a Ubiquitous Sequential Computation"}, {"paperId": "09dbfd54e5c8bdcd99b72ed7f8c45428f1e69541", "title": "Recurrent Linear Transformers"}, {"paperId": "c85268696fe1435605ae66a18653cfdcf8153753", "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "02ad9f3fefe33cb9ca546591bec65dbdf7766c80", "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context"}, {"paperId": "9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b", "title": "Transformer-VQ: Linear-Time Transformers via Vector Quantization"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "d2d0371158803df93a249c9f7237ffd79b875816", "title": "Sparse Modular Activation for Efficient Sequence Modeling"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "f35f5aedc30e2c5ded210d9c91ba6e84bd029425", "title": "Toeplitz Neural Network for Sequence Modeling"}, {"paperId": "fc7f626f37f3cf7751523a99bc3f0fac10ec89cf", "title": "The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge"}, {"paperId": "2ef1c2438c3a4552db9e7080e15d8c51bc071f58", "title": "Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "54155c2977a977bf129849455dcae3a2b79b3f41", "title": "Simple Hardware-Efficient Long Convolutions for Sequence Modeling"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "a128b1c47e6842605fb95bceae930d2135fc38fc", "title": "Pretraining Without Attention"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "240300b1da360f22bf0b82c6817eacebba6deed4", "title": "What Makes Convolutional Models Great on Long Sequence Modeling?"}, {"paperId": "f6d8beb02771791d628f7e0773d8906261ce707c", "title": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights"}, {"paperId": "b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc", "title": "Liquid Structural State-Space Models"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "e2ee883fca5f8f32a1dfa2dc06c742d57f2c38b9", "title": "Linearizing Transformer with Key-Value Memory"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "e0cbbca02b332f398c6639b3bea0613f79166220", "title": "ABC: Attention with Bounded-memory Control"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61", "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"}, {"paperId": "16e623059ffccab60f4c35be028a2d4f10933515", "title": "Sequence Parallelism: Long Sequence Training from System Perspective"}, {"paperId": "379c9fe562c47a65a2c9864c48d75fc03149c8d0", "title": "Accelerating non-power-of-2 size Fourier transforms with GPU Tensor Cores"}, {"paperId": "880a9714271da2af2bff4463d06c37f04f39cae7", "title": "tcFFT: Accelerating Half-Precision FFT through Tensor Cores"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "7a485356e538cf5d259912f41a7e54d2370397ca", "title": "3.2 The A100 Datacenter GPU and Ampere Architecture"}, {"paperId": "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a", "title": "CKConv: Continuous Kernel Convolution For Sequential Data"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "b31eef8d9263b02f7d0c1ab55b26012550a2e95a", "title": "OpenCeres: When Open Information Extraction Meets the Semi-Structured Web"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "9fde884f635c2c6a8d8586a027e5e637bfad78ba", "title": "Accelerating reduction and scan using tensor core units"}, {"paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094", "title": "CoQA: A Conversational Question Answering Challenge"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "10428cdda9387933c64f518cefe6ca0a685f579e", "title": "The unreasonable effectiveness of the forget gate"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a", "title": "Fixing Weight Decay Regularization in Adam"}, {"paperId": "4f57f486adea0bf95c252620a4e8af39232ef8bc", "title": "Swish: a Self-Gated Activation Function"}, {"paperId": "fdfa7dc73dc1fc6772d26f88c72e98b68d1f8498", "title": "Parallelizing Linear Recurrent Neural Nets Over Sequence Length"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c91ae35dbcb6d479580ecd235eabf98374acdb55", "title": "Using Fast Weights to Attend to the Recent Past"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "82ef2354ca03cb3ad69e75a07d2a5163f82c4dbd", "title": "Compiling high performance recursive filters"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "11540131eae85b2e11d53df7f1360eeb6476e7f4", "title": "Learning to Forget: Continual Prediction with LSTM"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "bc22e87a26d020215afe91c751e5bdaddd8e4922", "title": "Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"}, {"paperId": "88695b5bb6462872ce1dd946cff00dd6ebabf2d9", "title": "Scaling TransNormer to 175 Billion Parameters"}, {"paperId": "dc48bc1a4d81e0f37603013fd2a95644dc233bd0", "title": "Functional Interpolation for Relative Positions Improves Long Context Transformers"}, {"paperId": "5e424004958853f4e366e7a86a1c3a56a76cb2a4", "title": "LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "6f8c6d0df740eee17395c29a6a2b7410dee644cd", "title": "Partitioning"}, {"paperId": "ec0bc920662693375a881d8beb8c97bf08281dba", "title": "Prefix sums and their applications"}, {"paperId": "7257eacd80458e70c74494eb1b6759b52ff21399", "title": "Using fast weights to deblur old memories"}, {"paperId": null, "title": ": Fast transformers via sketching"}, {"paperId": null, "title": "SlimPajama: A 627B token cleaned and deduplicated version of"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "with Hardware-Ef\ufb01cient Training"}, {"paperId": null, "title": "tokenizer"}]}