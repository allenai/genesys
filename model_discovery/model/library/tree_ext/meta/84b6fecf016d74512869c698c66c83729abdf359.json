{"paperId": "84b6fecf016d74512869c698c66c83729abdf359", "title": "Self-Supervised Multimodal Learning: A Survey", "abstract": "Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) objectives for learning from multimodal unlabeled data via self-supervision, (2) model architectures from the perspective of different multimodal fusion strategies, and (3) pair-free learning strategies for coarse-grained and fine-grained alignment. We also review real-world applications of SSML algorithms in diverse fields such as healthcare, remote sensing, and machine translation. Finally, we discuss challenges and future directions for SSML. A collection of related resources can be found at: https://github.com/ys-zong/awesome-self-supervised-multimodal-learning.", "venue": "arXiv.org", "year": 2023, "citationCount": 18, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2304.01008", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A comprehensive review of the state-of-the-art in SSML, in which three major challenges intrinsic to self-supervised learning with multimodal data are elucidated and existing solutions to these challenges are detailed."}, "embedding": {"model": "specter_v2", "vector": [0.4949338138103485, 0.613926351070404, -0.5808393955230713, -0.6003509759902954, -0.831272304058075, 0.13105659186840057, 0.5214987993240356, -0.12431906163692474, -0.275676965713501, -0.20028507709503174, 0.8303435444831848, 0.8979582190513611, 0.4321143925189972, 0.17950469255447388, -0.12344390153884888, -0.21241769194602966, -0.9625950455665588, 0.17651964724063873, -0.3071362376213074, -0.5178601145744324, -0.7957501411437988, -0.7783849835395813, -0.9895636439323425, 0.13695912063121796, 0.37714260816574097, 0.41033971309661865, 0.41901302337646484, 0.9694491624832153, -0.5048239231109619, 0.1311447024345398, 0.47510194778442383, 0.09989683330059052, 0.24397407472133636, -0.5431336760520935, -0.2891461253166199, 0.34949684143066406, 0.4023136794567108, -0.5392234921455383, -0.5519847273826599, 0.684730589389801, -0.04623367637395859, -0.13864575326442719, 0.5961570143699646, -0.6471271514892578, 0.03191675618290901, 0.6522417664527893, 0.3647467792034149, 0.10130700469017029, 0.11753566563129425, -0.8564465045928955, 1.481339931488037, -0.9821497797966003, 0.08553441613912582, 1.489281177520752, 0.4930714964866638, 0.5969820022583008, -0.2600708603858948, -0.7005860209465027, 0.21107998490333557, -0.3975207209587097, -0.4884909987449646, -0.28991585969924927, -0.0851258710026741, -0.35887831449508667, 0.9476388096809387, -0.3132217228412628, -0.21959327161312103, 0.7290540337562561, -0.059131234884262085, 1.4679772853851318, 0.1473127156496048, -0.6488933563232422, -0.23868627846240997, -0.026433883234858513, 0.19051338732242584, 1.2214137315750122, -0.9950773119926453, 0.6722506284713745, -1.4819849729537964, -0.016623422503471375, 0.20638060569763184, 0.17317146062850952, -0.11450565606355667, -0.4562491178512573, -0.6574714183807373, 0.6576087474822998, 0.7907520532608032, 0.524086594581604, 0.017363635823130608, 0.34503617882728577, 0.6410069465637207, 0.260947585105896, -0.15130750834941864, -0.15366841852664948, -0.025884082540869713, 0.5472701191902161, -0.5407714247703552, -0.30967453122138977, -0.1108265221118927, 0.8939668536186218, 0.19567926228046417, 0.2245040088891983, -0.25140801072120667, 0.6964747309684753, 1.674756407737732, 0.33062708377838135, 0.5928403735160828, -0.49485787749290466, 0.5667533278465271, -0.5810045003890991, 0.004886204842478037, -0.3768984377384186, -0.6648703217506409, -0.0780431479215622, -0.6100633144378662, -0.9533454775810242, -0.4025748074054718, -0.10541896522045135, -1.0009573698043823, 0.856143057346344, -0.42550015449523926, -0.2923406660556793, 0.709136962890625, 0.933659017086029, 0.8284104466438293, 0.500097930431366, 0.21610818803310394, 0.2185995876789093, 0.8095976114273071, -1.0322327613830566, -0.8750271201133728, -0.9305749535560608, 0.5554244518280029, -0.12579692900180817, 0.06016603857278824, -0.6941397786140442, -0.670304536819458, -0.8776429295539856, -1.0010385513305664, 0.3463688790798187, -0.40035420656204224, 0.6005224585533142, 0.9283361434936523, 0.17617212235927582, -0.6216405630111694, 0.7148292064666748, -0.3320399522781372, -0.782211184501648, 0.389811247587204, 0.2336067408323288, -0.29661327600479126, -0.4035738706588745, -1.071441888809204, 0.2586851418018341, 0.40975019335746765, -0.5375338792800903, -0.489141583442688, 0.08041568845510483, -1.6304445266723633, -0.5891432762145996, 0.12377530336380005, -0.10234716534614563, 0.9086182713508606, -0.39254993200302124, -1.0668766498565674, 0.7189728021621704, -0.2886572778224945, 0.2756999731063843, 0.5146954655647278, -0.14716099202632904, -1.0239078998565674, -0.21171332895755768, 0.12216372042894363, 0.8427492380142212, 0.32396724820137024, -0.8948604464530945, -0.24542197585105896, -0.1695830225944519, -0.6965871453285217, 0.31658878922462463, -0.4711330235004425, 0.7906967401504517, -0.35787060856819153, -0.38011741638183594, 0.7447159886360168, 0.6397607326507568, -0.008329466916620731, -0.06240614131093025, -0.7871472835540771, -0.447126567363739, 1.0021110773086548, 0.04077661409974098, -0.09826876223087311, -1.083012580871582, -0.6014483571052551, -0.19130150973796844, -0.22863060235977173, -0.377168744802475, -1.1168224811553955, 0.6784197092056274, -0.40820351243019104, -0.16275128722190857, -0.5908361673355103, -1.3177335262298584, -0.09501594305038452, 0.39132770895957947, -0.5988954901695251, 0.10541731864213943, 0.2813950181007385, 0.9937444925308228, -1.092490792274475, -0.1372166872024536, -0.028154609724879265, 0.11801012605428696, -0.09750235825777054, 1.5933444499969482, -0.14406342804431915, 0.5191377401351929, 0.08838339149951935, 0.27307769656181335, 0.20852334797382355, -0.14981037378311157, 0.6061943769454956, -0.8984936475753784, 0.16459724307060242, 0.20217052102088928, -0.34380003809928894, 2.033236265182495, 0.08613542467355728, 0.6376360654830933, 0.15606263279914856, -0.7660349607467651, 0.22624197602272034, 0.5663620829582214, 0.44159001111984253, -0.4005576968193054, 0.5219841599464417, 0.10413867980241776, -0.7732656002044678, -0.06013108417391777, 0.35236871242523193, 0.45928987860679626, -0.08850274980068207, 0.27462926506996155, 0.9664466977119446, -0.2878151535987854, 0.21044926345348358, 0.34005534648895264, 0.6076228022575378, 0.7887040972709656, 0.19868144392967224, 0.21816985309123993, -0.014810341410338879, -0.6133977770805359, -0.058462437242269516, 0.09229959547519684, 0.5073960423469543, 1.0762052536010742, -0.12261399626731873, -0.39769601821899414, -0.13912104070186615, -0.3904075026512146, 0.6908820271492004, 1.411026954650879, 0.0006262746173888445, -0.01952081359922886, -0.7216602563858032, -0.8328315615653992, -0.4286591410636902, -0.2957625389099121, -0.5269527435302734, 0.07981712371110916, -0.16118182241916656, -1.2524465322494507, 0.6764686703681946, 0.6690220236778259, 1.0831201076507568, -0.20961810648441315, -0.08586232364177704, -0.3125079870223999, 0.0013022583443671465, -0.7445180416107178, -0.6562889218330383, 0.8640772104263306, -0.323436975479126, -0.4675530195236206, -0.3760293126106262, -0.25489869713783264, 0.4731503427028656, -0.7680004835128784, 0.6490192413330078, -0.9315154552459717, -0.24426975846290588, 0.5947117209434509, 0.18493326008319855, -0.7796504497528076, -0.8017308712005615, -0.004698694683611393, 0.124448262155056, 0.10816484689712524, 0.21667355298995972, 0.6141802668571472, 0.48600882291793823, 0.48052436113357544, -0.2527528405189514, 0.1449236422777176, 0.17281846702098846, 0.488437294960022, 0.8037487864494324, -0.3714474141597748, 0.48707905411720276, -0.9511731266975403, 0.5958351492881775, -0.18057505786418915, 0.1276562660932541, 0.12132249772548676, -0.4383540153503418, -0.4275154173374176, 0.1807485967874527, -0.38737043738365173, -0.4127667248249054, 0.04409533739089966, 0.3426651060581207, -0.1223427876830101, -0.847504198551178, 0.18691202998161316, 0.16851459443569183, 0.014032394625246525, 0.5397630929946899, 0.3485206067562103, 0.9353828430175781, -0.044230878353118896, 0.6110445261001587, -0.4465596079826355, 0.966244637966156, 0.44922366738319397, 0.45507165789604187, 0.2561196982860565, -0.13775499165058136, -0.5731939077377319, -0.7302148938179016, -0.575370192527771, -0.8213773369789124, -0.47508174180984497, 0.5237187743186951, -0.7034842371940613, -0.3106817305088043, -0.1389496624469757, -0.6857887506484985, -0.21588337421417236, -0.13844506442546844, 0.14290983974933624, -0.5062637329101562, -0.6993319392204285, -0.9692294597625732, -0.6155933141708374, -0.38650935888290405, -0.8500032424926758, 0.5552511215209961, 0.2051292061805725, -0.2552114725112915, -0.4403533935546875, -0.07729554176330566, -0.14962594211101532, 0.4623596966266632, -0.47636181116104126, 0.18416693806648254, 0.03473910316824913, 0.14927653968334198, -0.49191397428512573, -0.14911630749702454, 0.6577006578445435, 0.4613516330718994, -0.26306796073913574, -1.126830816268921, 0.36056965589523315, -0.4528879225254059, -0.6062325835227966, 0.1978985071182251, 0.10826480388641357, -0.03350437432527542, 0.6775157451629639, -0.32668232917785645, 0.2727343440055847, 1.335301399230957, -0.5826525092124939, -0.4434816241264343, -0.24811497330665588, 1.1184834241867065, 0.9088878035545349, -0.3373856544494629, 0.23857513070106506, 0.5770114064216614, 0.5195289850234985, 0.4814620912075043, -0.623637855052948, -0.3975527286529541, -0.44190192222595215, 0.56573086977005, 1.223212718963623, 0.3508417010307312, 0.17902034521102905, -0.9194413423538208, 0.8365288376808167, -1.4558589458465576, -0.2907032072544098, 0.48001426458358765, 0.46320217847824097, 0.046500444412231445, -0.9750707745552063, -0.03903145343065262, -0.2700016498565674, 0.44111594557762146, 0.43621277809143066, -0.3929135799407959, 0.1507604867219925, -0.5165653228759766, -0.02251661941409111, 0.34097740054130554, 0.30605411529541016, -0.8071125149726868, 0.06959041953086853, 14.975533485412598, 0.42788997292518616, 0.08582957834005356, 0.7140383720397949, 0.2265803962945938, 0.6934384703636169, -0.13911879062652588, -0.24950087070465088, -0.5910733938217163, -0.21157841384410858, 0.7279961109161377, 0.9827908873558044, 0.6959900856018066, -0.07118795812129974, -0.014327325858175755, -0.0398411862552166, -0.8312930464744568, 0.9697130918502808, 0.775501549243927, -0.7942456603050232, -0.11884113401174545, -0.08236958086490631, 0.42041629552841187, 0.954410195350647, 0.44212204217910767, 0.24273768067359924, 0.46922066807746887, -0.3349089026451111, 0.3068256080150604, 0.5257276296615601, 0.9359356760978699, 0.06593387573957443, 0.574361264705658, 0.41458240151405334, -0.9395192265510559, -0.5072989463806152, -0.03121008723974228, -0.4786232113838196, 0.6918451189994812, -0.15431749820709229, -0.4062316417694092, 0.19493061304092407, -0.017659233883023262, 0.9768239259719849, 0.17836758494377136, 0.081544429063797, 0.004893664736300707, 0.6056712865829468, -0.0971372127532959, 0.5573673248291016, 0.15458104014396667, 0.38972750306129456, 0.2986699044704437, 0.1193728819489479, -0.15307113528251648, -0.17734302580356598, 0.47923943400382996, 0.21727652847766876, -0.34001225233078003, 0.05158529803156853, -0.5431358814239502, -0.2516920864582062, -0.9029242992401123, 0.8520132303237915, 0.8447895050048828, 0.3069700300693512, -0.6464507579803467, 0.5604704022407532, 0.22152027487754822, 0.31184321641921997, 0.026986345648765564, -0.038707345724105835, 0.04466217756271362, -0.5958711504936218, -0.3932473063468933, 0.5438982844352722, -0.182587131857872, -0.3343672752380371, -0.9404152631759644, 0.10683852434158325, 0.6314731240272522, -1.0998111963272095, -1.581727147102356, 0.9442770481109619, -0.4143012762069702, -0.8738774657249451, -0.0030000116676092148, -0.698806643486023, -0.06701543927192688, 0.6788774132728577, -1.203857421875, -0.9164972901344299, -0.4542008936405182, -0.20366285741329193, -0.4606379568576813, -0.6221761703491211, 1.4814302921295166, 0.2836272716522217, -0.3555453419685364, -0.1876496970653534, -0.03866823390126228, 0.15766382217407227, 0.3531298339366913, -0.8908789753913879, 0.006941216066479683, -0.017341328784823418, -0.061242036521434784, -0.4250253140926361, -0.110602006316185, -0.014637934044003487, -0.6572370529174805, -0.07447829842567444, 0.23767174780368805, -0.47401922941207886, -0.40937355160713196, -0.40380409359931946, -0.7501234412193298, 0.026598641648888588, 0.8700072169303894, 0.015209739096462727, 0.7563881278038025, -0.1698247343301773, -0.5143813490867615, -0.24898120760917664, -1.0169917345046997, -0.15598851442337036, 0.3222152292728424, -0.9037820100784302, -0.15798264741897583, 0.27505555748939514, 0.1545388549566269, -0.9576348662376404, -0.6849390864372253, 0.035870250314474106, 0.04667489230632782, 0.18461942672729492, 0.6459871530532837, -0.4528407156467438, 0.2734915316104889, 0.37235528230667114, -0.6723417043685913, -1.0518934726715088, 0.5354536771774292, -0.48327094316482544, 0.10325337201356888, 0.28763487935066223, 0.472634881734848, -0.27893751859664917, 0.6087163686752319, 0.4930177628993988, 0.21836711466312408, -0.22834333777427673, -0.30740609765052795, -0.12565670907497406, -0.5948737263679504, -0.38114291429519653, 0.1934700906276703, 0.12236389517784119, -0.10024866461753845, 0.28412678837776184, 0.11832776665687561, 0.723272442817688, 0.2623572051525116, -0.7444157004356384, 0.5427713990211487, -0.23757845163345337, -0.014102432876825333, -0.07982461899518967, -0.36022594571113586, -1.394311547279358, 0.166933074593544, -1.3555160760879517, 0.48125702142715454, -1.0289686918258667, -0.3090648651123047, 0.30255940556526184, -0.14786982536315918, 0.4013916552066803, 0.5824068784713745, -0.16767066717147827, -0.38595086336135864, -0.32465651631355286, -0.27603599429130554, 0.8607603311538696, 0.8126370906829834, -0.9378615021705627, -0.12105944752693176, -0.1179639920592308, 0.13331688940525055, 0.3235290050506592, 0.40272340178489685, -0.3100651800632477, -0.714029848575592, -0.9357244968414307, -0.15271322429180145, 0.1003555878996849, 0.07775446772575378, -0.6553495526313782, 0.4068516790866852, 0.2804098427295685, 0.07548007369041443, -0.3738116919994354, 0.5886139273643494, -1.3599990606307983, -0.4166465401649475, 0.031670816242694855, -0.9001742601394653, -0.3748490512371063, -0.13435347378253937, -0.13542012870311737, -0.7222992777824402, 0.6271694898605347, 0.0025784240569919348, -0.7751482725143433, -0.691961944103241, 0.36870259046554565, -0.32809776067733765, -0.2483220249414444, -0.22233453392982483, 0.06031950190663338, -1.0775988101959229, -0.6236157417297363, -0.5979866981506348, 0.6591762900352478, -0.622766375541687, 0.9796013832092285, 0.4992809593677521, -1.4671881198883057, -0.11671427637338638, 0.35535016655921936, 0.589375913143158, 0.10408846288919449, 0.74537193775177, 0.5198874473571777, -0.06432332843542099, 0.07684189081192017, -0.03993768244981766, -0.09118352830410004, -0.7843919992446899, -0.43382781744003296, 1.1026006937026978, 0.0793071985244751, 0.19801749289035797, 0.7563936710357666, -0.2669270634651184, -1.686041235923767, 0.1982751190662384, -0.4734460115432739, -0.6825659871101379, -0.008025534451007843, 0.4995386600494385, 0.33363232016563416, -0.3247665464878082, -0.8399195075035095, -0.24189619719982147, 0.390493780374527, 0.24325400590896606, -0.7008830308914185, 0.6990357041358948, -0.3318880796432495, -0.17456330358982086, 0.8056976795196533, 0.9316984415054321, -0.8012070655822754, -1.285239815711975, -0.7624641060829163, -0.5730112791061401, -0.0015185419470071793, -0.05655985698103905, -0.7341888546943665, -1.0755690336227417, 0.6061313152313232, 1.2917828559875488, 0.2222544550895691, 0.34612563252449036, 0.4754182696342468, 0.30373236536979675, 0.6031562089920044, -0.30299925804138184, -1.1367015838623047, -0.4643159508705139, 0.8878839612007141, 1.251943826675415, -1.4648773670196533, 0.031619660556316376, -0.4875762164592743, -0.8643319606781006, 0.741048276424408, 0.2509004473686218, 0.4485447108745575, 1.1339313983917236, -0.22250574827194214, 0.6588612198829651, -0.0534280501306057, -0.2698401212692261, -0.7786768078804016, 1.0350592136383057, 1.5509475469589233, 0.5817801356315613, 0.6580676436424255, 0.2814701497554779, 0.5582952499389648, 0.18556833267211914, 0.061086513102054596, 0.09052946418523788, 0.5735925436019897, -0.09478434920310974, -0.3402642607688904, 0.0075769065879285336, 0.8756707310676575, -0.16405783593654633, 0.10825344920158386, 0.23045045137405396, 0.1504203826189041, 0.4929358661174774, 0.8203420042991638, 0.6747289896011353, -0.40296417474746704, 0.5497309565544128, 0.032164640724658966, 0.25648295879364014, -0.6448817253112793, 0.1645764261484146, 0.21318531036376953, -0.4384206533432007, -0.17420823872089386, -0.9167323112487793, -0.782517671585083, -1.0217303037643433, 0.6623764634132385, 0.4608256220817566, -0.40473946928977966, 0.8245498538017273, 1.3383451700210571, 0.40910032391548157, 0.24809788167476654, 0.05850465968251228, -0.4571050703525543, -0.31500062346458435, -1.1099679470062256, -0.32393044233322144, -0.34246549010276794, 0.11360838264226913, -0.3720698058605194, -0.09324470162391663, -0.04914527013897896]}, "authors": [{"authorId": "152218610", "name": "Yongshuo Zong"}, {"authorId": "2918822", "name": "Oisin Mac Aodha"}, {"authorId": "1697755", "name": "Timothy M. Hospedales"}], "references": [{"paperId": "bee79110f7b89292955984c7110ed0de8ae719a1", "title": "Fusing Pre-Trained Language Models with Multimodal Prompts through Reinforcement Learning"}, {"paperId": "7dc6da87eaa6f830354feb2db14023cab8678c91", "title": "ImageBind One Embedding Space to Bind Them All"}, {"paperId": "6bfafb32b423c3f0456a10984814f89046def489", "title": "A Cookbook of Self-Supervised Learning"}, {"paperId": "0388fd88770d60c6ddddb5117863e2b24b1f725e", "title": "Towards Democratizing Joint-Embedding Self-Supervised Learning"}, {"paperId": "6173520a1eb2814d067e8c5fd16212b7cbf6ee78", "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs"}, {"paperId": "2e965b5d97c2d6fb4af284307735be39283792ba", "title": "Extracting Training Data from Diffusion Models"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "835c305e52769a8433f8383e91d33ba6c66ad55b", "title": "Large language models generate functional protein sequences across diverse families"}, {"paperId": "ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b", "title": "CLIPPO: Image-and-Language Understanding from Pixels Only"}, {"paperId": "de46871d33c5ceab19bcf05db31e6a16b3d5ba03", "title": "MAViL: Masked Audio-Video Learners"}, {"paperId": "cfca7eedc6ede9d363d1662280a74d78dcdc9d4a", "title": "Scaling Language-Image Pre-Training via Masking"}, {"paperId": "00b3421e147da7a0c1b60c15c878532cfc93ece6", "title": "VatLM: Visual-Audio-Text Pre-Training With Unified Masked Prediction for Speech Representation Learning"}, {"paperId": "cdd9c1d23f9e89d5113f3e31821bb174c6a6afed", "title": "MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"}, {"paperId": "b287a2765e5bceb732de39dafdf70594dc9cd664", "title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"}, {"paperId": "ad3dfb2514cb0c899fcb9a14d229ff2a6018892f", "title": "Deep Bidirectional Language-Knowledge Graph Pretraining"}, {"paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"}, {"paperId": "e1484706c0fab932fc9804df328044b3cb2f110d", "title": "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"}, {"paperId": "10667c1ae4b49808772b5a377c5b52196701267f", "title": "When and why vision-language models behave like bags-of-words, and what to do about it?"}, {"paperId": "3fa1505ec327b82416987a7db3dadab8b12601ea", "title": "Contrastive Audio-Visual Masked Autoencoder"}, {"paperId": "c4cb3f7056f1216c1ddfbe4b9e55cbc07a1e43b9", "title": "Linearly Mapping from Image to Text Space"}, {"paperId": "76120de60a9e59c23a372457a056da3c220c64b6", "title": "Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning"}, {"paperId": "e342165a614588878ad0f4bc9bacf3905df34d08", "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications"}, {"paperId": "dda118e8154765f73cb8f5e2b1b8daa75faf726f", "title": "Multimodal biomedical AI"}, {"paperId": "f54fd65a4a8fa0b03290a5de4b0be79f545aced4", "title": "Unsupervised Representation Learning in Deep Reinforcement Learning: A Review"}, {"paperId": "02251886950770e82b3d68564d60cdfe15e73199", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"}, {"paperId": "9efba2e9929960ebd986c5ba43deef3c27c67fe0", "title": "UAVM: Towards Unifying Audio and Visual Models"}, {"paperId": "23f4b6432b74e5db05da04e354341807f5044f7e", "title": "Language Modelling with Pixels"}, {"paperId": "8abc6b3246ea821efa862735359313bd220f39e8", "title": "u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality"}, {"paperId": "cb78447bf7d9ef8f10381fa22823e6424f148ba5", "title": "Robustness Analysis of Video-Language Models Against Visual and Language Perturbations"}, {"paperId": "45122c8f76a4e2fd0163d1f0522db37e97ea4721", "title": "Beyond neural scaling laws: beating power law scaling via data pruning"}, {"paperId": "cfd94ae8dd48c695cd0d0d63cd67573bd5310f87", "title": "Self-Supervised Learning in Remote Sensing: A review"}, {"paperId": "76d40153acfbb35a7eb8272a4215854cafa10e78", "title": "PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance"}, {"paperId": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c", "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"}, {"paperId": "d09dc1f8b0cc5c3a43bf63bf9c7df3e63e0da313", "title": "VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "622428f5122ad12a40229e1768ecb929fd747ee7", "title": "Multimodal Learning With Transformers: A Survey"}, {"paperId": "55e31baa3ae5f32fb5e695761892319e26dbc639", "title": "Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data"}, {"paperId": "0aa3cd5ab502b3dd7f23cf4781cd44305a642bea", "title": "VL-BEiT: Generative Vision-Language Pretraining"}, {"paperId": "c837dc61603623c712f2903216a3bbbb18245832", "title": "One Model, Multiple Modalities: A Sparsely Activated Approach for Text, Sound, Image, Video and Code"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "916cf5435f90eba8d27ea4056656db0b8a1ba410", "title": "Sound Localization by Self-Supervised Time Delay Estimation"}, {"paperId": "f5526c23bda899eb38fb3589507a9a1db28389ed", "title": "A Study of Gender Impact in Self-supervised Models for Speech-to-Text Systems"}, {"paperId": "6550076df01275d6e9efcdbbc970e04cb6b761da", "title": "Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data"}, {"paperId": "32e89d4d1225f3e02ac38ec93efc39fb61ccdde8", "title": "Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning"}, {"paperId": "eb03a61ba8e17211b4f247f330a13903b73200de", "title": "Unsupervised Vision-and-Language Pretraining via Retrieval-based Multi-Granular Alignment"}, {"paperId": "0fd08c1237f80d96a6618d93cb1292b45b9f09fc", "title": "CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding"}, {"paperId": "3c6ec875050ad4d2319725697d74154f66ced597", "title": "Hierarchical Perceiver"}, {"paperId": "4df8db87bce83ebdb093f39698a6b661d7960256", "title": "Multimodal audio-visual information fusion using canonical-correlated Graph Neural Network for energy-efficient speech enhancement"}, {"paperId": "8f2bca9d684005675e294b33c26481e36f528cdb", "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language"}, {"paperId": "52f91c8f24ea1559d21fe55ab86f3825980bc3bb", "title": "Multi-modal Sensor Fusion for Auto Driving Perception: A Survey"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "e8a72d29771d1a33b4a0e43c74adcee6c73d74c7", "title": "End-to-end Generative Pretraining for Multimodal Video Captioning"}, {"paperId": "400d619cbabeb669115bb7281a889ab869829ef5", "title": "MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound"}, {"paperId": "dc9a76b7cb690e6a95f0f07bb3d4fabb7181b68d", "title": "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction"}, {"paperId": "e9581d9758062f76e029bd19a58c4ae976cfb414", "title": "SLIP: Self-supervision meets Language-Image Pre-training"}, {"paperId": "265a3039eccc7525234b996f59d5aef9fcfbb7c4", "title": "Fine-grained Multi-Modal Self-Supervised Learning"}, {"paperId": "d4d74b7902cc81b186ad80ba98e28ac38d1662d0", "title": "Contrastive Vision-Language Pre-training with Limited Resources"}, {"paperId": "bdea16e93fc70f316002e5f6aac8ce17388c6ee9", "title": "MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning"}, {"paperId": "2fd6f77540c1cc8e70b96208ccf9971b4251fc02", "title": "FLAVA: A Foundational Language And Vision Alignment Model"}, {"paperId": "c5c3ad98547202f120aaae4007cc665bdff0f447", "title": "Everything at Once \u2013 Multi-modal Fusion Transformer for Video Retrieval"}, {"paperId": "f3ce9ba3fcec362b70263a7ed63d9404975496a0", "title": "PointCLIP: Point Cloud Understanding by CLIP"}, {"paperId": "69d90d8be26ff78d5c071ab3e48c2ce1ffb90eac", "title": "ContIG: Self-supervised Multimodal Contrastive Learning for Medical Imaging with Genetics"}, {"paperId": "a7aa150b55d64d339b1c154d6d88455fc3cbc44f", "title": "ClipCap: CLIP Prefix for Image Captioning"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"}, {"paperId": "aa62d5e43cb151cd574e4df058b4c6a509d62644", "title": "Self-Supervised Representation Learning: Introduction, advances, and challenges"}, {"paperId": "767923635f2fd4467d848dba9655866e4f9b55c8", "title": "Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm"}, {"paperId": "267f1de5ff863ab03f8c48c7ac3df1d422de7c3b", "title": "Multimodal datasets: misogyny, pornography, and malignant stereotypes"}, {"paperId": "0b500aa5fcc175f07aecf26c0e8ddc4f0c6a931d", "title": "GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition"}, {"paperId": "5abf2ddd97bb4428ff88983081edfd13af6fcf24", "title": "COOKIE: Contrastive Cross-Modal Knowledge Sharing Pre-training for Vision-Language Representation"}, {"paperId": "f6b1b06dc881443fca1c34f0890973fb2a282311", "title": "CrossCLR: Cross-modal Contrastive Learning For Multi-modal Video Representations"}, {"paperId": "821ad6c9f0fecb5fabb486a5a87a93b7ea65bcc0", "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding"}, {"paperId": "23bb171f143ec8799f530068ecff717f8346ef11", "title": "Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "23056aeb10e267f4cc41b7830bdd3dd197b3335a", "title": "Semi-Supervised Learning for Joint SAR and Multispectral Land Cover Classification"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "1c5d0dad5f4f4db55c7a16162e9ce0ce09150356", "title": "Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications"}, {"paperId": "e1a0542891ef950b00ad89d526001c61dc998761", "title": "Self-supervised Audiovisual Representation Learning for Remote Sensing Data"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "f4d68b5b81aa12cb5b6db0b48fc2a4709abcdbb8", "title": "OPT: Omni-Perception Pre-Trainer for Cross-Modal Understanding and Generation"}, {"paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d", "title": "Multimodal Few-Shot Learning with Frozen Language Models"}, {"paperId": "fda4530df9eec0e3f714dba3459ac50dab17d89c", "title": "Audioclip: Extending Clip to Image, Text and Audio"}, {"paperId": "0a02607f877ac95256bc4f5e857d085b9ea99e7d", "title": "Multi-level Feature Learning for Contrastive Multi-view Clustering"}, {"paperId": "eafa8e48ebeeebf01e20eba98c6d67bed3587dc0", "title": "Multimodal Video Sentiment Analysis Using Deep Learning Approaches, a Survey"}, {"paperId": "fc123daef8ab12025c2e5e6162a0c94370faa4c5", "title": "Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions"}, {"paperId": "32fb3dd5601977cb8d6de6ffa12f223ad3097d71", "title": "SelfDoc: Self-Supervised Document Representation Learning"}, {"paperId": "07fa32eb364f966fb4b44dcf9435adf165295c96", "title": "Multimodal remote sensing benchmark datasets for land cover classification with a shared and specific feature learning model"}, {"paperId": "18f37f62d2bf3c2e34e2bde78545b47e92d7b72d", "title": "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding"}, {"paperId": "d9b1bb8053f32c6da9bbbec564d750d55b486f00", "title": "Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos"}, {"paperId": "f0524b3005720bcff886bcb0227f7f0dd924ff07", "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"}, {"paperId": "1e83a4a3cc65229403a5f90229007af957c12602", "title": "Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models"}, {"paperId": "281ad83e06d731d5d686acf07cd701576f1188c4", "title": "CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval"}, {"paperId": "42b60f6aa28accd1cd7a0324ab78ad5b86f965ae", "title": "Self-supervised object detection from audio-visual correspondence"}, {"paperId": "9411a49666960f537939a897d00f4f122d102e08", "title": "Self-Supervised Discriminative Feature Learning for Deep Multi-View Clustering"}, {"paperId": "617b00eed4091736f3d2c00823938dad4861de59", "title": "Self-Supervised Change Detection in Multiview Remote Sensing Images"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "1614645a0fa9689a187dedefa36bdf5be43734f6", "title": "There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "e79a20f1079b759ef91c741cc77bfbba4f6fdd0d", "title": "Self-Supervised Multisensor Change Detection"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "81002fbb777f860f9aac2bbc24467a62345af279", "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers"}, {"paperId": "abac50aa18e037f8149b2b212f1691d738f7b056", "title": "Adversarial Text-to-Image Synthesis: A Review"}, {"paperId": "5e5fbc41106db9acaaf3a365801051e477f0e984", "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning"}, {"paperId": "3a5ac09e759f3223ee78b995ae2b519efc0f9292", "title": "Introduction to Reinforcement Learning"}, {"paperId": "eeb33ad2ede9944918724978bcbfb08b4c8c50a8", "title": "Learning Representations from Audio-Visual Spatial Alignment"}, {"paperId": "80089ad641bae28b0e57771afef181b60011069e", "title": "COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning"}, {"paperId": "e585f6e752fb2668b33f7d4b18c8af9bd5abc1a4", "title": "The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research"}, {"paperId": "7097137596f6755675f6aafcdd80969a747322ae", "title": "Contrastive Learning with Hard Negative Samples"}, {"paperId": "6dd9f99cecd38504b667d320eb2a6267a9fee35d", "title": "Contrastive Learning of Medical Visual Representations from Paired Images and Text"}, {"paperId": "87b008a6289fa22c72e1726a8929e815dfbbc65f", "title": "Hard Negative Mixing for Contrastive Learning"}, {"paperId": "17b28643a5aaed62fbccd98781d4ebc0f0477afa", "title": "Look, Listen, and Attend: Co-Attention Network for Self-Supervised Audio-Visual Representation Learning"}, {"paperId": "5ffe9b1d8219438f0343995ad3ea1a888e3d9f8e", "title": "Learning From Noisy Labels With Deep Neural Networks: A Survey"}, {"paperId": "6ff1eb9cdf64a464bf43b54d852456e9ddf55b28", "title": "Debiased Contrastive Learning"}, {"paperId": "33172567ab1dff9ca32c8d995d88bbff466f3236", "title": "Integrating Multimodal Information in Large Pretrained Transformers"}, {"paperId": "10d11f0045dc7f217c7f01bc6cbb47929e9b8808", "title": "Self-Supervised MultiModal Versatile Networks"}, {"paperId": "2a81f6bf76bcb70244aa40217ff316025971bd0f", "title": "Graph Optimal Transport for Cross-Domain Alignment"}, {"paperId": "008ba3c574f8e5085dfb1eef0342e8b408565e45", "title": "Labelling unlabelled videos from scratch with multi-modal self-supervision"}, {"paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a", "title": "Denoising Diffusion Probabilistic Models"}, {"paperId": "706f756b71f0bf51fc78d98f52c358b1a3aeef8e", "title": "Self-Supervised Learning: Generative or Contrastive"}, {"paperId": "3d22bbc6cd373d8a3e4688fa0a4ec24590ecff05", "title": "CoMIR: Contrastive Multimodal Image Representation for Registration"}, {"paperId": "8cda672bd5487ec2c67d5c217dc84ed8fb786640", "title": "ActBERT: Learning Global-Local Video-Text Representations"}, {"paperId": "1f3c381eedfe8914b81e93070bfdb00cf86ac943", "title": "Contrastive Multi-View Representation Learning on Graphs"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "7f768fa192a76ab097ccfda0a68523bc36425423", "title": "What makes for good views for contrastive learning"}, {"paperId": "5546e6073f3b82967b12c87d6b90ba722c4b85c6", "title": "Hero: Hierarchical Encoder for Video+Language Omni-representation Pre-training"}, {"paperId": "4f9a4afc0ba500d839f7ee245513af9b87add8be", "title": "Audio-Visual Instance Discrimination with Cross-Modal Agreement"}, {"paperId": "b643a7186c08db9f13d7204f6e5e739f97902e71", "title": "Music Gesture for Visual Sound Separation"}, {"paperId": "3578e297f24fcff76641973ed6d597e5bde4d0a2", "title": "Self-supervised Feature Learning by Cross-modality and Cross-view Correspondences"}, {"paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"}, {"paperId": "598a2ee223e2949c3b28389e922c1892b4717d2a", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"}, {"paperId": "33eadd4e666a894306a22ba0839c5e0cef77280e", "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"}, {"paperId": "c3d0c717dba8f4c49c820a8ebc7d7df199ff88d2", "title": "Learning Predictive Representations for Deformable Objects Using Contrastive Estimation"}, {"paperId": "a1f1d17a350d30d55de52b15c9fe7fea4ba6ff13", "title": "Visual Grounding in Video for Unsupervised Word Translation"}, {"paperId": "8b2c80788f789d4ce7849c13943fa920d9e3c95f", "title": "Captioning Images Taken by People Who Are Blind"}, {"paperId": "4243555758433880a67b15b50f752b1e2a8c4609", "title": "UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "9de403a58395a1b56bfceee6e009788c43db6d08", "title": "End-to-End Learning of Visual Representations From Uncurated Instructional Videos"}, {"paperId": "4fed2133f467b8ef43d15eb3dc3916605beacdca", "title": "Self-Supervised Learning by Cross-Modal Audio-Video Clustering"}, {"paperId": "87045bfc6f8036d032ab6ad1ebeb0377db05da9a", "title": "Self-labelling via simultaneous clustering and representation learning"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "c937b4499c259835e4e0b2f6b983ca435005fea3", "title": "Large-Scale Representation Learning from Visually Grounded Untranscribed Speech"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "2bc1c8bd00bbf7401afcb5460277840fd8bab029", "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f", "title": "Deep Learning for Video Captioning: A Review"}, {"paperId": "90bbefeddc0bd160e6ad3523c57820d93d5ac52f", "title": "Multilingual Unsupervised NMT using Shared Encoder and Language-Specific Decoders"}, {"paperId": "97f4d09175705be4677d675fa27e55defac44800", "title": "Contrastive Multiview Coding"}, {"paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "title": "ERNIE: Enhanced Language Representation with Informative Entities"}, {"paperId": "b4693a93b033d6ec6c5c98a22797e43928ac7470", "title": "A Survey of Multilingual Neural Machine Translation"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "c880de441a41c351955ad0bf8f712eeee500ac67", "title": "The Sound of Motions"}, {"paperId": "08a302f0bb8dc360ae3a0a20fa7b7555920380d4", "title": "Unified Visual-Semantic Embeddings: Bridging Vision and Language With Structured Meaning Representations"}, {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"}, {"paperId": "f4c60c3ee4904d61ff84c9d4c15c9aecdcf04cdc", "title": "Unpaired Image Captioning via Scene Graph Alignments"}, {"paperId": "0c677d362b8668ebdd5c10b07657752b07947b13", "title": "The Missing Ingredient in Zero-Shot Neural Machine Translation"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "403227333329b36183004f04db72362b604adef3", "title": "A Theoretical Analysis of Contrastive Unsupervised Representation Learning"}, {"paperId": "4c94ee7df6bc2bfcac76703be4f059a79010f7e5", "title": "Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey"}, {"paperId": "6dfc2ff03534a4325d06c6f88c3144831996629b", "title": "From Recognition to Cognition: Visual Commonsense Reasoning"}, {"paperId": "fea3e63c97c7292dc6fbcb3ffe7131eb54053986", "title": "Learning Latent Dynamics for Planning from Pixels"}, {"paperId": "e8e7b0b23e70624fbc3bc5ab74ae01e39c6750a5", "title": "Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks"}, {"paperId": "e7e1313061b0d56364bd2c41f017deb954bb05db", "title": "TVQA: Localized, Compositional Video Question Answering"}, {"paperId": "41cca0b0a27ba363ca56e7033569aeb1922b0ac9", "title": "Recurrent World Models Facilitate Policy Evolution"}, {"paperId": "1d033b30f38642e4b6dd146bb8b464bfb58aad96", "title": "Deep Clustering for Unsupervised Learning of Visual Features"}, {"paperId": "b227f3e4c0dc96e5ac5426b85485a70f2175a205", "title": "Representation Learning with Contrastive Predictive Coding"}, {"paperId": "8abc9fc312fc6916725ec94816ab26c582cf1a90", "title": "Deep Multimodal Clustering for Unsupervised Audiovisual Learning"}, {"paperId": "8dac02f61e12560607f857cee3c1d5abaf40ecd0", "title": "Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization"}, {"paperId": "a2344004f0e1409c0c9473d071a5cfd74bff0a5d", "title": "Learnable PINs: Cross-Modal Embeddings for Person Identity"}, {"paperId": "171f8f1090ef0533ff470ed5a4d31ecfefcc74be", "title": "Audio-Visual Scene Analysis with Self-Supervised Multisensory Features"}, {"paperId": "fe018f22600d07cbd0452a070e03708886470015", "title": "The Sound of Pixels"}, {"paperId": "0c3c6197037ec92de044b3068c57e26815dd6c76", "title": "Scene Graph Parsing as Dependency Parsing"}, {"paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"}, {"paperId": "2fa693d586f09edba19512f3e11ed999a1bdef35", "title": "A Fast Proximal Point Method for Computing Exact Wasserstein Distance"}, {"paperId": "eed8cae46eb28311e88f6fc41f788528ca2d0f00", "title": "State Representation Learning for Control: An Overview"}, {"paperId": "dfc504536e8434eb008680343abb77010965169e", "title": "Objects that Sound"}, {"paperId": "90873a97aa9a43775e5aeea01b03aea54b28bfbd", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering"}, {"paperId": "e9495c6726ec315f2a69b77d573f40e86c0da587", "title": "Deep Metric Learning for Visual Understanding: An Overview of Recent Advances"}, {"paperId": "0197f278e2dedd67ec5067f47037b8cdd3ae8509", "title": "Deep Multimodal Learning: A Survey on Recent Advances and Trends"}, {"paperId": "e3d772986d176057aca2f5e3eb783da53b559134", "title": "Unsupervised Machine Translation Using Monolingual Corpora Only"}, {"paperId": "ee909ad489244016cf301bb7d7d8eeea423dbf35", "title": "Localizing Moments in Video with Natural Language"}, {"paperId": "39153bcd060957a5e79e511cd4d39ec8ada4ed6b", "title": "Scalable and Effective Deep CCA via Soft Decorrelation"}, {"paperId": "a9e28863c7fb963b40a379c5a4e0da00eb031933", "title": "A Corpus of Natural Language for Visual Reasoning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "title": "Multimodal Machine Learning: A Survey and Taxonomy"}, {"paperId": "9b5f696f73c1264ccb8e97d3b738a2342ecd6bee", "title": "Look, Listen and Learn"}, {"paperId": "96dd1fc39a368d23291816d57763bc6eb4f7b8d6", "title": "Dense-Captioning Events in Videos"}, {"paperId": "b2f521c02c6ed3080c5fe123e938cdf4555e6fd2", "title": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"}, {"paperId": "c43d954cf8133e6254499f3d68e45218067e4941", "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks"}, {"paperId": "e10a5e0baf2aa87d804795af071808a9377cc80a", "title": "Towards Automatic Learning of Procedures From Web Instructional Videos"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "8cf83c619423a1504f26495d5f6a495054c46462", "title": "Learning to Poke by Poking: Experiential Learning of Intuitive Physics"}, {"paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51", "title": "Conditional Image Generation with PixelCNN Decoders"}, {"paperId": "2ec8f7e0257a07d3914322b36072d1bbcd58a1e0", "title": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "b8e2e9f3ba008e28257195ec69a00e07f260131d", "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "83a89f5c778a2e7ed7e6327a5b8fcd9558533928", "title": "Multimodal Deep Autoencoder for Human Pose Recovery"}, {"paperId": "e4257bc131c36504a04382290cbc27ca8bb27813", "title": "Action-Conditional Video Prediction using Deep Networks in Atari Games"}, {"paperId": "754dbc09783980f383e16b8728fb9c21a39bfff0", "title": "On Deep Multi-View Representation Learning"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "371400e61632592146f40b621fb3dbb6971721be", "title": "Understanding Image Representations by Measuring Their Equivariance and Equivalence"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "1ab5c006caf3bf8c128fdfad80e58277cb8b1455", "title": "Learning with Noisy Labels"}, {"paperId": "e2257e3f56ccb12875a57bc0a8cca1d9d7e93ec6", "title": "Deep Canonical Correlation Analysis"}, {"paperId": "adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1", "title": "Multimodal learning with deep Boltzmann machines"}, {"paperId": "72729882f8fa3d9084eaece513f6bf9630be5901", "title": "Collecting Highly Parallel Data for Paraphrase Evaluation"}, {"paperId": "cfaae9b6857b834043606df3342d8dc97524aa9d", "title": "Learning a similarity metric discriminatively, with application to face verification"}, {"paperId": "a6b5b20151c752beb74508f813699fa5216dedfa", "title": "Canonical Correlation Analysis: An Overview with Application to Learning Methods"}, {"paperId": "1c7d38f68fe1150895a186e30b60c02dd89a676a", "title": "Solving the Multiple Instance Problem with Axis-Parallel Rectangles"}, {"paperId": "2cb9d167e9ddc9d48b931520248bdc1b7e818703", "title": "Robustness in Multimodal Learning under Train-Test Modality Mismatch"}, {"paperId": null, "title": "Introducing chatgpt"}, {"paperId": "63f93a6d9c38d656933706acfc720684470bc108", "title": "Foundations and Recent Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions"}, {"paperId": "24bd5b7455a07631cbc4023c07eb2f0eef85ea85", "title": "Multimodal research in vision and language: A review of current and emerging trends"}, {"paperId": "1da8da6eb4a7c2322dfa3345cfbc59853de0b042", "title": "MACK: Multimodal Aligned Conceptual Knowledge for Unpaired Image-text Matching"}, {"paperId": "b452d5dabbc97685abb7391d23d519cb72e53ce6", "title": "Self-Supervised SAR-Optical Data Fusion of Sentinel-1/-2 Images"}, {"paperId": null, "title": "\u201cMultimodal image synthesis and editing: A survey,\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "0e22d407b6b818bde02ae4dd03692d984f7806cd", "title": "DM2C: Deep Mixed-Modal Clustering"}, {"paperId": null, "title": "Computational optimal transport: With applications to data science"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6", "title": "GENERATIVE ADVERSARIAL NETS"}, {"paperId": null, "title": "Understanding back-translation at scale"}, {"paperId": "263a6ecdd00daf7c4c5cc37e271b15256c8494a0", "title": "Unsupervised Metric Fusion Over Multiview Data by Graph Random Walk-Based Cross-View Diffusion"}, {"paperId": "516dc7097bc5c8a58214d320f381d79d9222836b", "title": "Grounded cognition."}]}