{"paperId": "c62321199fb07ce317921ff8832e9da1cee778af", "title": "Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency", "abstract": "Recent research has focused on weight sparsity in deep neural network training to reduce FLOPs, aiming for improved efficiency (test accuracy w.r.t training FLOPs). However, sparse weight training often compromises accuracy, requiring extended training schedules to attain the accuracy of dense models. In contrast, our approach, Sparse Iso-FLOP Transformations (Sparse-IFT), uses sparsity to improve accuracy while maintaining dense model FLOPs. Using a single hyperparameter (i.e., the sparsity level), Sparse-IFTs efficiently replace dense layers, expanding the search space for optimal sparse masks. In addition, dynamic sparse training (DST) with Sparse-IFT models effectively navigate this larger sparse mask-weight space, which is evidenced by a spectral analysis using Ramanujan graph properties. Our study reveals a robust correlation among mask topology, weights, and final performance. Notably, without adjusting any training hyperparameters, replacing dense layers with Sparse-IFT yields significant improvements, such as a +3.5% boost for ResNet-18 on ImageNet and +0.9% for GPT-3 Small on the Open LLM leaderboard. To the best of our knowledge, this is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a set of simple-to-use sparse transformations. Code is available at: https://github.com/CerebrasResearch/Sparse-IFT.", "venue": "", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This is the first work to demonstrate the use of sparsity for improving the accuracy of dense models through a set of simple-to-use sparse transformations, and reveals a robust correlation among mask topology, weights, and final performance."}, "embedding": {"model": "specter_v2", "vector": [0.3947007358074188, 0.6938614845275879, -0.3983319401741028, -0.004775497131049633, 0.1563473343849182, 0.2678947150707245, 0.426482617855072, -0.7370054721832275, -0.09711512923240662, -0.20271120965480804, 0.12239465117454529, 0.045697033405303955, 0.2040516883134842, 0.053849197924137115, -0.33291518688201904, 0.08574455231428146, -1.216131567955017, 0.12167224287986755, 0.24678194522857666, -0.2449483722448349, -0.07644745707511902, -0.34586894512176514, -1.2297499179840088, 0.19504162669181824, 0.3535973131656647, 1.3076694011688232, 0.1950049251317978, 0.3285459280014038, -0.26486706733703613, 0.4342860281467438, 0.5633161664009094, -0.41005560755729675, 1.0140132904052734, 0.006595063954591751, -0.047534599900245667, 0.1602770835161209, 0.5352668166160583, -0.07451697438955307, -0.8615714311599731, 1.0295225381851196, -0.4427962899208069, 0.5689125657081604, 0.33315351605415344, -0.7673054337501526, -0.12374783307313919, 0.23517994582653046, 0.0682617649435997, 1.0346217155456543, -0.7913780212402344, -0.38114121556282043, 1.060498595237732, -1.457943081855774, -0.013611013069748878, 1.0981990098953247, 0.3713791072368622, 0.3051827549934387, -0.42441079020500183, -0.7258129119873047, 0.5307720899581909, 0.1262614130973816, -0.6053533554077148, -0.2478267252445221, -0.2335844784975052, -0.08114125579595566, 1.6849894523620605, -0.5419886708259583, 0.3862520456314087, 0.6512476205825806, 0.06924445927143097, 0.9301051497459412, 0.223815456032753, -0.5812035799026489, 0.0463753379881382, -0.011009319685399532, 0.2146146148443222, 1.1080213785171509, 0.07214123010635376, 0.3837922215461731, -1.2969138622283936, -0.10063963383436203, 0.5451072454452515, 0.2783253788948059, 0.08301105350255966, -0.27085524797439575, -0.11555030941963196, 0.9882135987281799, 0.8143762946128845, 0.6420325040817261, -0.10035781562328339, 0.9142294526100159, 0.4391007125377655, 0.5460929870605469, 0.16641607880592346, 0.32851967215538025, -0.12036336213350296, 0.5051354765892029, -0.8102483153343201, -0.17789699137210846, -0.09296403080224991, 0.5454460978507996, 0.19620636105537415, 0.7171275019645691, -0.18066996335983276, 0.3384895622730255, 1.2896186113357544, -0.23749014735221863, 0.648624062538147, -0.4828977882862091, 0.20894068479537964, -0.5739240050315857, -0.2764280438423157, -0.8156525492668152, -0.17296279966831207, -0.6406139135360718, -0.8599419593811035, -0.716864824295044, -0.5287248492240906, 0.13535863161087036, -0.9372133016586304, 0.6408016085624695, -0.32681140303611755, 0.8342476487159729, -0.45522698760032654, 0.6272710561752319, 0.16408748924732208, 0.5434747338294983, -0.2254950851202011, 0.1923784613609314, 0.9240562915802002, -0.8940854072570801, -0.6632063984870911, -1.283734917640686, 0.47796085476875305, -0.20080532133579254, 0.23899903893470764, -0.11454997211694717, -1.0968562364578247, -0.8035588264465332, -0.8990981578826904, 0.34578144550323486, -0.4305965304374695, 0.2320663034915924, 1.3708513975143433, 0.534293532371521, -0.9955818057060242, 0.9243541359901428, -0.43627288937568665, 0.23430046439170837, 0.905213475227356, 0.5861197113990784, 0.21909458935260773, 0.026679396629333496, -1.1183948516845703, 0.40639573335647583, 0.36942219734191895, -0.39509958028793335, -0.3166290819644928, -0.9771167635917664, -1.0386097431182861, 0.18892084062099457, 0.20312361419200897, -0.7882832884788513, 0.7356157898902893, -0.6556365489959717, -1.0479636192321777, 0.8552196025848389, -0.24468190968036652, -0.36538684368133545, 0.3351227939128876, -0.01841343753039837, -0.4082219898700714, 0.10163453221321106, -0.5097518563270569, 0.9495888352394104, 0.8765206336975098, -0.22921842336654663, 0.24436639249324799, 0.25826728343963623, -0.6649449467658997, -0.1874660700559616, -0.44060999155044556, 0.6080487966537476, 0.048607803881168365, -0.35474443435668945, 0.36427736282348633, 0.5701377987861633, -0.0007735987310297787, 0.11045243591070175, 0.02446250058710575, -0.4717378318309784, 0.9233521223068237, 0.2683662176132202, 0.6935093402862549, -1.0163244009017944, -0.9841666221618652, 0.4472794234752655, 0.44610100984573364, -0.13730472326278687, -0.5894445180892944, -0.19473031163215637, -0.886605978012085, 0.3674706816673279, -0.1838613897562027, -0.8957125544548035, -0.15038682520389557, 0.029989775270223618, -0.6696665287017822, 0.3144165277481079, 0.37563949823379517, 0.9789619445800781, -0.4244292080402374, 0.4272749722003937, 0.18710854649543762, 0.3620811700820923, -1.1921614408493042, 0.7355897426605225, -0.10240945219993591, -0.09334705024957657, 0.3227892518043518, -0.31257933378219604, -0.11977440863847733, -0.8400385975837708, 0.20365093648433685, -0.5688472390174866, 0.3516210615634918, 0.42451512813568115, -0.6145731806755066, 1.306355357170105, -0.3173266053199768, 0.6479952335357666, -0.18190233409404755, -0.9200276136398315, 0.39670899510383606, 0.1950034648180008, 0.3127467930316925, -0.6263052225112915, 0.6975459456443787, 0.36497262120246887, -0.22851599752902985, 0.3737339377403259, 0.43788644671440125, 0.8027635812759399, 0.1920287162065506, 0.10974329710006714, 0.7094816565513611, -0.12366505712270737, 0.38244250416755676, 0.35948896408081055, 0.5508031249046326, -0.0830937922000885, 0.23668640851974487, -0.2679787576198578, -0.16263525187969208, -1.3638310432434082, -0.15980753302574158, 0.35125523805618286, 0.4674984812736511, 0.9682229161262512, 0.4250684976577759, -0.8619323372840881, -0.588659942150116, -0.17125725746154785, 0.44129785895347595, 1.0533639192581177, -0.16528013348579407, 0.18015781044960022, -0.7004899382591248, -0.13699571788311005, -0.2891642451286316, -0.4783231317996979, -0.6359440684318542, -0.3798978626728058, -0.4178483486175537, -1.2819058895111084, 0.4493248164653778, -0.3098752796649933, 1.143614649772644, -0.26153314113616943, -0.3483640253543854, -0.5338544249534607, 0.7543033361434937, -0.9694445133209229, -0.7684617042541504, 0.5008577108383179, -0.7521450519561768, -0.27352815866470337, 0.1117641031742096, -0.10083655267953873, 0.4079153537750244, -0.06761234998703003, 0.765616238117218, -0.281430184841156, -0.45921891927719116, -0.12885046005249023, 0.5588998198509216, -0.3613077402114868, 0.06879082322120667, 0.16195254027843475, 0.12603148818016052, 0.6428757309913635, -0.043885864317417145, -0.3368132710456848, -0.439604252576828, 0.25198283791542053, -0.4958944618701935, 0.17183709144592285, -0.09715531021356583, 0.04491698741912842, 0.5521990060806274, -0.6865703463554382, 0.5874212980270386, -1.1685142517089844, 0.5462460517883301, -0.29576969146728516, -0.39391207695007324, -0.20485429465770721, -0.5265545845031738, -0.11754230409860611, 0.20835460722446442, -0.8703534603118896, -0.05662228539586067, -0.8333489894866943, 0.4574115574359894, -0.9666855931282043, 0.04702639579772949, -0.07181254774332047, 0.8031795620918274, -0.7374569773674011, 0.783728301525116, -0.03404834121465683, 0.3890005648136139, 0.38579893112182617, 0.29442480206489563, -1.3460767269134521, 0.24202677607536316, 0.2981284558773041, 0.42624959349632263, 0.24090856313705444, -0.1433417946100235, -0.25177282094955444, -0.9146255254745483, -0.14959228038787842, 0.2624499797821045, -0.5141467452049255, -0.0865788385272026, -0.6059016585350037, -1.0055861473083496, -0.38053229451179504, -0.3515303432941437, -0.41180357336997986, -0.19271592795848846, 0.10564389824867249, -0.44023600220680237, -1.0188729763031006, -1.1263294219970703, -0.3829934000968933, -0.6302977204322815, -1.2443913221359253, 0.05226118862628937, 0.2747505307197571, -0.3889071047306061, -0.7140838503837585, -0.7841934561729431, -0.49827098846435547, 1.021203637123108, -0.35258373618125916, 0.4878396689891815, -0.04476480931043625, -0.021139347925782204, 0.06131162866950035, -0.13561707735061646, 0.18765099346637726, -0.36741095781326294, 0.15070420503616333, -1.0143455266952515, 0.10301954299211502, -0.28462523221969604, -0.6262762546539307, 0.4071352481842041, 0.45867443084716797, 1.1166136264801025, -0.02521589957177639, -0.04338395968079567, 1.013106107711792, 1.3264309167861938, -0.8618030548095703, 0.3469367027282715, -0.006498802453279495, 0.7415262460708618, 0.0013772125821560621, -0.8162379264831543, 0.7616325616836548, -0.008125647902488708, -0.02446126379072666, 0.5430829524993896, -0.259565144777298, -0.5364941358566284, -0.5310832262039185, 0.3836507499217987, 1.4384524822235107, 0.5937287211418152, 0.3392427861690521, -0.5467143654823303, 0.43333253264427185, -1.0933856964111328, -0.6430428624153137, 0.8101394176483154, 0.732001781463623, 0.44539615511894226, -0.010940679349005222, -0.3877193033695221, 0.09924625605344772, 0.6438475847244263, 0.7396953105926514, -0.4299545884132385, -0.4061952233314514, -0.2427273690700531, 0.661033034324646, 0.6151448488235474, 0.4515918493270874, -0.03596288710832596, 0.35900914669036865, 14.968847274780273, 0.9583446383476257, -0.26521795988082886, 0.94389408826828, 1.104238510131836, -0.10090809315443039, -0.4324348568916321, -0.2129213660955429, -0.865578830242157, 0.26241955161094666, 0.9948574304580688, 0.27917665243148804, 0.9747048020362854, 0.0392054058611393, 0.05514093115925789, 0.003364665200933814, -0.27100831270217896, 1.3457403182983398, 0.4794052839279175, -1.6770204305648804, 0.12546420097351074, 0.02679404616355896, 0.9961957931518555, 0.9089692831039429, 1.0461719036102295, 0.2758117616176605, 0.4646928608417511, -0.6774546504020691, 0.6249251365661621, 0.24721920490264893, 1.1924402713775635, 0.04626606032252312, -0.011972609907388687, 0.34598395228385925, -1.0026445388793945, -0.08116968721151352, -0.6283825635910034, -1.234466552734375, -0.12289685010910034, 0.5142898559570312, -0.06932574510574341, -0.5885857343673706, 0.011565420776605606, 1.129501223564148, 0.2760172188282013, 0.7200219631195068, -0.057151246815919876, 0.501695990562439, -0.2802795469760895, 0.32914915680885315, 0.2574765682220459, 0.18423773348331451, -0.21624481678009033, -0.22356180846691132, 0.2997885048389435, -0.3483079969882965, 0.43952158093452454, 0.5848216414451599, -1.0047932863235474, -0.317724347114563, -0.3434716463088989, 0.14667508006095886, -0.014701923355460167, 1.08768892288208, 0.18546321988105774, 0.2550258934497833, -0.2910313010215759, -0.0804939717054367, 0.6400994658470154, 0.2627977728843689, -0.37179818749427795, 0.17610278725624084, 0.6308994293212891, -0.43322739005088806, -0.21897824108600616, 0.2784333825111389, -0.8710542321205139, -0.9216965436935425, -0.5989883542060852, -0.5963929891586304, 0.35203731060028076, -1.0170046091079712, -0.7214577198028564, 0.6750671863555908, -0.3159205913543701, -0.29344844818115234, 0.9111247658729553, -0.7508546710014343, -0.20851829648017883, 0.3263942301273346, -1.706145167350769, -0.01628044992685318, 0.15841713547706604, 0.03600085899233818, -0.19719523191452026, -0.40611329674720764, 0.5958830714225769, 0.40070831775665283, -0.4645438492298126, 0.22578957676887512, -0.4254899322986603, -0.3639470338821411, -0.3839090168476105, -0.29148948192596436, 0.7493406534194946, 0.39456042647361755, 0.1346062272787094, -0.039270687848329544, -0.22683292627334595, 0.46677616238594055, -0.5853103399276733, -0.10537271946668625, -0.09131257981061935, -0.2176397144794464, 0.484511137008667, -0.6015319228172302, -0.9310973882675171, 0.39762720465660095, 0.3483758866786957, 0.5091171264648438, 0.16139298677444458, -0.21459965407848358, -0.7894203662872314, -0.24780717492103577, -0.8270202279090881, 0.013686349615454674, 0.6248857378959656, -0.7197039723396301, -0.3458274304866791, 0.037440165877342224, 0.12463925033807755, -1.1452670097351074, -0.5508148670196533, 0.003439913270995021, -0.2281377613544464, -0.3901382088661194, 0.9498522877693176, -0.1740514636039734, 0.3696378469467163, 0.7113458514213562, 0.13228994607925415, -0.9411901235580444, 0.15611842274665833, -1.1159993410110474, -0.022398358210921288, -0.6299677491188049, 0.5233674645423889, -0.9682104587554932, 0.7194728255271912, 0.30541718006134033, -0.10290643572807312, -0.43955039978027344, -0.9912791848182678, -0.028565291315317154, -0.6155867576599121, -0.609344482421875, -0.07996169477701187, 0.18436415493488312, -0.3097587823867798, -0.26064518094062805, 0.40582266449928284, 0.21307474374771118, -0.2565189301967621, -1.0146050453186035, -0.22849354147911072, -0.04805571958422661, -0.27026495337486267, -0.6655009984970093, -0.771881103515625, -1.2861301898956299, 0.1080663874745369, -1.6151330471038818, -0.6065012216567993, -0.34513258934020996, -0.6241797804832458, 0.08588670939207077, -0.0999666377902031, 0.35290929675102234, 0.4539790153503418, 0.23871608078479767, -0.246040940284729, -0.27399155497550964, -0.28491073846817017, 1.0043630599975586, 0.8921468257904053, -0.523075520992279, 0.2752014100551605, -0.26504772901535034, 0.1546819508075714, 0.6417155861854553, 0.7548940181732178, -0.6261016726493835, -0.9882623553276062, -1.1652750968933105, 0.7669771909713745, -0.39670073986053467, 0.10419654846191406, -1.058782935142517, 0.5377632975578308, 0.48705366253852844, 0.04088730365037918, 0.5055225491523743, 0.4476746916770935, -1.0251737833023071, -0.35405030846595764, 0.48820558190345764, -0.4891408383846283, 0.12460722029209137, 0.4299829602241516, -0.4472164809703827, -0.3970092535018921, 0.41698065400123596, 0.6378668546676636, -0.7744216918945312, -0.372156023979187, 0.7752072215080261, -0.3234116733074188, 0.47700992226600647, 0.054117441177368164, -0.4813831150531769, -1.5273149013519287, -0.24937768280506134, -0.39453253149986267, 0.28447458148002625, -0.6348398327827454, 0.6184000372886658, 0.6284552812576294, -1.2671732902526855, 0.4741349518299103, 0.627994179725647, -0.4479554295539856, -0.21847014129161835, 0.4212583601474762, 0.7130466103553772, -0.49470266699790955, -0.18986591696739197, -0.06814970821142197, 0.3190782070159912, -0.18928228318691254, -0.12878212332725525, 1.3889061212539673, -0.34717944264411926, -0.18526425957679749, 0.8903502225875854, 0.024748781695961952, -0.7395053505897522, 0.29602617025375366, -0.9521148204803467, -0.20734281837940216, -0.08034448325634003, 0.5439735651016235, 0.049226313829422, 0.26207560300827026, 0.3856438994407654, -0.17080293595790863, 0.2177783101797104, 0.0317421592772007, -0.2552047371864319, 0.35324355959892273, -0.016361435875296593, -0.17865638434886932, 0.5757808685302734, 0.9615656137466431, -1.019277811050415, -1.2498149871826172, -0.868465781211853, -0.3309001922607422, -0.422457754611969, 0.4140250086784363, -0.01766970381140709, -1.576358437538147, 0.7985750436782837, 0.6098178625106812, 0.29052460193634033, 0.19331727921962738, -0.20841844379901886, 0.14339910447597504, 0.5252329707145691, -0.03030419908463955, -0.7112916707992554, 0.06761053204536438, 1.36748206615448, 1.048776626586914, -0.6681029200553894, 0.24054241180419922, -0.5399811267852783, -0.6202211380004883, 0.7794749140739441, 0.3265582025051117, -0.6811373829841614, 0.9469367861747742, 0.04401465132832527, -0.5687006115913391, -0.06342703849077225, -1.0190162658691406, -0.46128642559051514, 0.7035046815872192, 0.7396818995475769, 0.3710903823375702, -0.05700694024562836, 0.13804581761360168, 0.4430859684944153, -0.06550959497690201, -0.05472556874155998, 0.5200188755989075, 0.13470348715782166, -0.5426386594772339, 0.606370210647583, 0.035131897777318954, 1.1988396644592285, -0.7815996408462524, -0.5162626504898071, 0.4116232097148895, 0.49677348136901855, 0.20488230884075165, 0.2979659140110016, 0.9773384928703308, -0.008471440523862839, 0.39278605580329895, -0.04803277552127838, 0.7246542572975159, -0.5311154723167419, -0.5745716094970703, -0.19900234043598175, -0.7758283615112305, -0.21040920913219452, -0.21431808173656464, -0.2722228169441223, -0.4836457073688507, -0.497947633266449, 0.21871119737625122, -0.26443973183631897, 0.2905462384223938, 0.9295423626899719, 0.45216336846351624, 0.7583423256874084, 0.2274821698665619, -0.7079840302467346, -0.054423850029706955, -0.7286421656608582, 0.25933611392974854, -0.36917805671691895, -0.4266444444656372, -0.41010838747024536, -0.31897103786468506, -0.1867464929819107]}, "authors": [{"authorId": "51153332", "name": "Vithursan Thangarasa"}, {"authorId": "46708564", "name": "S. Saxena"}, {"authorId": "1922581610", "name": "Abhay Gupta"}, {"authorId": "2212029838", "name": "Sean Lie"}], "references": [{"paperId": "16b4620b59bfef414d702214a717856c943db7fb", "title": "Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design"}, {"paperId": "0f322ac4f9f12b11fe5d042508497148a84fe8a5", "title": "Dynamic Sparse Training with Structured Sparsity"}, {"paperId": "e20b2f83f7993446382f72aacc500319465bca2b", "title": "Cerebras Architecture Deep Dive: First Look Inside the Hardware/Software Co-Design for Deep Learning"}, {"paperId": "7d46073cce9f5ed8d931b804cfaf327201dd4e26", "title": "SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models"}, {"paperId": "94d8e29580b6fc29c9ffbee88d49401e0e2da1b5", "title": "Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off"}, {"paperId": "d1869155960e4b1b882b39171dbecd25a7eda3cd", "title": "More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity"}, {"paperId": "210c47fc0c16bf1cfc9beeb01faf70fcdbd3b978", "title": "Spartan: Differentiable Sparsity via Regularized Transportation"}, {"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "01594f00b0deed32cba4fc4ea8c74b60be31db4a", "title": "Sparsity Winning Twice: Better Robust Generalization from More Efficient Training"}, {"paperId": "07c6a2ccb65913f2478234e96dd50c9593989c5c", "title": "Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision"}, {"paperId": "821b08d595b6482e3d1f5bab6835b72d67ebd894", "title": "The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "6988237a7d8ffc226d21d897724543c915a159ee", "title": "How Well Do Sparse ImageNet Models Transfer?"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "d18d01be0a0a40327e13c1c89aa547a5c73fe3e8", "title": "MEST: Accurate and Fast Memory-Economic Sparse Training Framework on the Edge"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "a85ba5bb3e97c999f5f6dbc78f277b107af1dba2", "title": "Sparse Training via Boosting Pruning Plasticity with Neuroregeneration"}, {"paperId": "279b5affd1a3aec43e1f9d9c21ae69b0d1aa7928", "title": "Effective Model Sparsification by Scheduled Grow-and-Prune Methods"}, {"paperId": "9c4dd36ad206ca8be96ae4000568e899f4acfa91", "title": "Top-KAST: Top-K Always Sparse Training"}, {"paperId": "4862141c0283502fe30d0c3b2f01c87b30fd15dd", "title": "Search Spaces for Neural Model Training"}, {"paperId": "f93f2476972228de142fde13913bccbec76859b8", "title": "Initialization and Regularization of Factorized Neural Layers"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "1f34aee634327c6d4f13f9018c34df4d3239ecb6", "title": "Diverse Branch Block: Building a Convolution as an Inception-like Unit"}, {"paperId": "ac591dbf261777e05d89c27f9a7bcb06f88aab5a", "title": "Scalable Vision Transformers with Hierarchical Pooling"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "b4d207a2096aee4a3764933373eef6edb574c952", "title": "Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks"}, {"paperId": "6e67f5fb8b4169f3f80d2c691910d412b244ba1d", "title": "Doping: A technique for efficient compression of LSTM models using sparse structured additive matrices"}, {"paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "title": "Bottleneck Transformers for Visual Recognition"}, {"paperId": "4df2175c0daadf630623a505f623fe41a386853d", "title": "Selfish Sparse RNN Training"}, {"paperId": "2b8088253e2378fce001a090fe923b81e8dedf25", "title": "RepVGG: Making VGG-style ConvNets Great Again"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "9e104d440540d2ffc9caaa0952a9e5f7f9344ba9", "title": "Are wider nets better given the same number of parameters?"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "389036b1366b64579725457993c1f63a4f3370ba", "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"}, {"paperId": "deadba68c1c190be3974b6a66091e4182fae0f27", "title": "Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks"}, {"paperId": "3c981f8f53734ea17815b2371f3a10e04f5b292f", "title": "DO-Conv: Depthwise Over-Parameterized Convolutional Layer"}, {"paperId": "0b5c5571ab9dd88ebf010e8dc6898d8078d8e877", "title": "Progressive Skeletonization: Trimming more fat from a network at initialization"}, {"paperId": "a0edfe482c702c62147f60fdfb474e153de8d70e", "title": "Collegial Ensembles"}, {"paperId": "3b0fb765716ef6861a84abffcbe40643857c613b", "title": "Pruning neural networks without any data by iteratively conserving synaptic flow"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "c114ce10c4a315d92c3815f54bc9893e7e6ef182", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "52184d7a541eff0b9537e75da7327dd41daba207", "title": "Sparse Weight Activation Training"}, {"paperId": "3f06d02513a2763e472d2b5d5db08e9061081b9e", "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis"}, {"paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97", "title": "Deep double descent: where bigger models and more data hurt"}, {"paperId": "7a87ab984ca45aae2c5768d22cd6df3b5fd509f9", "title": "What\u2019s Hidden in a Randomly Weighted Neural Network?"}, {"paperId": "2e3002f131e1815bda7a10303eff97f79dea01ec", "title": "Rigging the Lottery: Making All Tickets Winners"}, {"paperId": "441555b5cd09703e55c03e70bd2c9f82c0ffcf9b", "title": "Deep High-Resolution Representation Learning for Visual Recognition"}, {"paperId": "f14469790532b5136d283a1b46c6c47a50dbbc79", "title": "ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks"}, {"paperId": "c2c083df88e88223e1a411e61040b94c233b1b63", "title": "MMDetection: Open MMLab Detection Toolbox and Benchmark"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "f7c410ab241bc972cda2f47993124ea8483003b6", "title": "Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217", "title": "The State of Sparsity in Deep Neural Networks"}, {"paperId": "29309743870c825f9645a4803af727402462e513", "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks"}, {"paperId": "37fb9a6589927efd38e6edf56299965435c5128f", "title": "ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks"}, {"paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d", "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "6ea8cbf0cc4cda3d981348a279b464524a8485cc", "title": "On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization"}, {"paperId": "9217e28b2273eb3b26e4e9b7b498b4661e6e09f5", "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "c27db32efa8137cbf654902f8f728f338e55cd1c", "title": "Mastering the game of Go without human knowledge"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"paperId": "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f", "title": "Improved Regularization of Convolutional Neural Networks with Cutout"}, {"paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "6dbb9e4b2e3b67dc4e1634989511f67d41373dd0", "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec", "title": "Why Are Big Data Matrices Approximately Low Rank?"}, {"paperId": "2a94c84383ee3de5e6211d43d16e7de387f68878", "title": "Feature Pyramid Networks for Object Detection"}, {"paperId": "1031a69923b80ad01cf3fbb703d10757a80e699b", "title": "Pyramid Scene Parsing Network"}, {"paperId": "1c4e9156ca07705531e45960b7a919dc473abb51", "title": "Wide Residual Networks"}, {"paperId": "c8c494ee5488fe20e0aa01bddf3fc4632086d654", "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding"}, {"paperId": "77f0a39b8e02686fd85b01971f8feb7f60971f80", "title": "Identity Mappings in Deep Residual Networks"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "d5b4721c8188269b120d3d06149a04435753e755", "title": "Convolutional neural networks with low-rank regularization"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "title": "Network In Network"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, {"paperId": "c8831d7d318b8d59f9b958d250a58f253f08bd8a", "title": "Robust principal component analysis?"}, {"paperId": "fda9a8f0664456dc4accb4018cfad2e6fde2d460", "title": "Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph"}, {"paperId": "dc2d76b5c123a9feedda200f3ebcd6719f4d3939", "title": "Don't just prune by magnitude! Your mask topology is a secret weapon"}, {"paperId": null, "title": "Train a model with weight sparsity"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": "86ee946179119b65c57171d8a2ddaa1eebc0e7ed", "title": "Exposing and Exploiting Fine-Grained Block Structures for Fast and Accurate Sparse Training"}, {"paperId": "68b1fd4f1a4d56e6a6db3ae1c98e1cd6bddf39dd", "title": "An Improved One millisecond Mobile Backbone"}, {"paperId": "d79b9042fe9c30027d49fdd6fe24e737b720af6b", "title": "DRONE: Data-aware Low-rank Compression for Large NLP Models"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Thinking outside the die: Architecting the ml accelerator of the future"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Resnet v1.5 for pytorch"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "f7ad35a51c59ff24fcd2d7f438a96bbfb922bde5", "title": "Measuring Models"}, {"paperId": "32b41612d39834c1983b7779ac2fd8463193cfcc", "title": "To prune or not to prune."}, {"paperId": null, "title": "Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency"}, {"paperId": null, "title": "bring-up of sparsity support on Cerebras CS-2 which was crucial for benchmarking and training of GPT models, and provided feedback to improve the structuring and presentation of the 22"}, {"paperId": null, "title": "We show consistent benefits of Sparse-IFT across computer vision and natural language processing domains"}, {"paperId": null, "title": "Harnessing the Power of Sparsity for Large GPT AI Models"}]}