{"paperId": "efc5e94635a850ede9c1f8dbce65d5dc536f3bfb", "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "abstract": "The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.", "venue": "arXiv.org", "year": 2024, "citationCount": 13, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend and explores LLMs' utilization and provides insights into their future development."}, "embedding": {"model": "specter_v2", "vector": [-0.005979263223707676, 0.6366269588470459, -0.45347872376441956, -0.10223232209682465, -0.44615063071250916, -0.07656153291463852, 0.46076247096061707, -0.22191222012043, -0.328726589679718, -0.1739322543144226, 0.42578041553497314, -0.38627177476882935, 0.5421250462532043, 0.38571521639823914, -0.27669191360473633, 0.15982535481452942, -1.0048307180404663, 0.6946742534637451, -0.26545092463493347, -0.15652352571487427, -0.42830130457878113, -0.758611261844635, -1.0055136680603027, 0.19272641837596893, 0.41851648688316345, 0.4188328981399536, 0.46349775791168213, 0.948465883731842, -0.6117115616798401, 0.2543727457523346, 0.31909048557281494, -0.2947289049625397, -0.03487579897046089, 0.010059980675578117, -0.37116649746894836, -0.22695986926555634, 0.4202987849712372, -0.4226011633872986, -0.15361645817756653, 0.6584486961364746, -0.19938014447689056, 0.4955858588218689, 0.3385867178440094, -0.502128541469574, -0.1754108965396881, 0.9229089021682739, 0.3786415159702301, 0.9019956588745117, -0.4106581211090088, -0.6437320709228516, 1.148200511932373, -1.5924556255340576, 0.14538542926311493, 1.6060060262680054, 0.6825239658355713, 0.40103232860565186, -0.4921089708805084, -0.43542972207069397, 0.48582273721694946, -0.16581428050994873, -0.7954034209251404, -0.623928427696228, -0.2666948139667511, -0.03448260575532913, 2.0932655334472656, 0.1265956610441208, -0.16402669250965118, 0.5614108443260193, -0.41191747784614563, 1.3918743133544922, -0.28471726179122925, -0.8989137411117554, -0.32377567887306213, 0.09268375486135483, -0.018830040469765663, 0.9040594100952148, -0.6871989369392395, 0.20067845284938812, -0.7681977152824402, -0.2103278487920761, 0.03428470715880394, -0.38183116912841797, 0.062963105738163, 0.25213176012039185, 0.028227735310792923, 1.0588856935501099, 0.11227969080209732, 0.8239040970802307, 0.1943359673023224, 0.6071465015411377, 0.5447506308555603, 0.389210969209671, 0.4117775559425354, 0.10059141367673874, -0.3176021873950958, 0.18887721002101898, -1.2211898565292358, 0.14896135032176971, 0.22812525928020477, 0.8466850519180298, -0.42054373025894165, 0.5470844507217407, -0.8288541436195374, 0.7032090425491333, 1.2694413661956787, 0.10091694444417953, 0.5713747143745422, -0.6437097191810608, 0.8204764127731323, -0.9258632063865662, -0.18624885380268097, -0.3906499445438385, -0.03690407797694206, -0.1696588695049286, -0.7283462285995483, -1.393976092338562, -0.5346567630767822, -0.3787146210670471, -0.7492526769638062, 0.6687880158424377, -0.3242056667804718, 0.12605223059654236, 0.3096093535423279, 0.4594723582267761, 0.5412918329238892, 1.0036083459854126, 0.2667885422706604, -0.17440494894981384, 1.151298999786377, -1.0852433443069458, -0.6317726969718933, -1.3592101335525513, 0.7215968370437622, -0.2427007555961609, 0.1190991997718811, -0.6123704314231873, -1.371429204940796, -0.8698760271072388, -0.5928452610969543, -0.08616331964731216, -0.2737121880054474, 0.6068971753120422, 1.1610815525054932, 0.3511812686920166, -1.0079693794250488, 0.6621441841125488, -0.09926401823759079, 0.020119057968258858, 0.09988690167665482, 0.30483725666999817, 0.3882110118865967, -0.21850964426994324, -1.3988896608352661, 0.09977900236845016, 0.22945986688137054, -0.5914272665977478, -0.0025470363907516003, -0.7455158829689026, -1.039285659790039, 0.05338957905769348, 0.13544811308383942, -0.22147530317306519, 1.5960859060287476, -0.187058687210083, -1.6311438083648682, 0.565662682056427, -0.49211621284484863, -0.09826859831809998, 0.29600122570991516, -0.038426682353019714, -0.5659722089767456, -0.5552164316177368, -0.4063566327095032, 0.5278235673904419, 0.6610355973243713, 0.06993448734283447, -0.3102726638317108, 0.2612802982330322, -0.3554658591747284, 0.12717698514461517, 0.013834595680236816, 1.1264210939407349, -0.9655206203460693, -0.22617501020431519, 0.37611234188079834, 0.5396018624305725, -0.30809286236763, -0.2494184821844101, -0.3888118863105774, -0.7527549266815186, 1.0531582832336426, -0.577125072479248, 1.1252330541610718, -0.8661184310913086, -0.5958899259567261, -0.09098675101995468, -0.20452618598937988, 0.20671238005161285, -0.5481594204902649, 0.917204737663269, -0.23438997566699982, 0.3409430980682373, -0.13144657015800476, -1.3458715677261353, 0.04185514524579048, 0.05240333825349808, -0.6401912569999695, -0.5338452458381653, -0.07766913622617722, 0.8729165196418762, -0.7242671251296997, -0.1522340029478073, -0.013450408354401588, 0.350892037153244, -1.211456537246704, 1.4121589660644531, -0.7864999771118164, 0.3102370500564575, 0.08555188775062561, -0.255356103181839, 0.17872650921344757, -0.0871477872133255, 0.9432869553565979, -0.2950911819934845, -0.40538087487220764, 0.3650487959384918, -0.30534031987190247, 1.290101408958435, -0.5116553902626038, 0.5671808123588562, 0.12956412136554718, -0.7063601016998291, -0.20294469594955444, 0.7372332811355591, -0.38243797421455383, -0.4881296157836914, 0.32105061411857605, 0.573789656162262, -0.4304294288158417, 0.28209930658340454, 0.8471291065216064, 0.8404080271720886, -0.14537812769412994, 0.22777047753334045, 0.7246491312980652, -0.09103134274482727, 0.5749757289886475, 0.43569785356521606, 0.2957834005355835, 0.04040130600333214, 0.06482362002134323, -0.0772307887673378, 0.4691961109638214, -0.850945770740509, -0.2560291290283203, 0.49465611577033997, 1.0038175582885742, 0.542654812335968, 0.32792067527770996, -0.30853167176246643, -0.2509804964065552, 0.29059910774230957, 0.7417367100715637, 1.7328168153762817, -0.4932520091533661, -0.18634597957134247, -0.8936801552772522, -0.42352744936943054, -0.11316024512052536, -0.07483475655317307, -0.06792280822992325, 0.1553625613451004, -0.7418287396430969, -0.8158283829689026, 0.7652756571769714, 0.04805075377225876, 0.8547844886779785, -0.4602148234844208, 0.0881345123052597, -0.39301908016204834, 0.2726289629936218, -0.8187785148620605, -0.7309690713882446, 0.39676806330680847, -0.8895196318626404, -0.07905508577823639, 0.06102043390274048, -0.308462530374527, 0.301498144865036, -0.7055909633636475, 0.9525414109230042, -0.06634870916604996, -0.11796843260526657, -0.3579375147819519, 0.5293526649475098, -0.4443216025829315, -1.0885597467422485, 0.3513904809951782, 0.3241938352584839, -0.28486308455467224, 0.08150430023670197, 0.4175340533256531, 0.47639715671539307, -0.23763804137706757, -0.006229331251233816, 0.34607869386672974, 0.1346425712108612, -0.022585349157452583, 0.6892088651657104, -0.19757980108261108, -0.4599478542804718, -1.455161213874817, 1.0324498414993286, 0.09809219837188721, -0.730158805847168, 0.30904608964920044, -0.5094937682151794, -0.04048581048846245, 0.8790743350982666, -0.5565766096115112, -0.5736380815505981, -1.0961284637451172, -0.21119685471057892, -0.3376055061817169, -0.11078973114490509, 0.5530683994293213, 0.39604055881500244, 0.14127255976200104, -0.04986942186951637, 0.4617595672607422, 0.6456040740013123, -0.7549800872802734, 0.5906835198402405, -0.557146430015564, 0.27645355463027954, 0.0462588295340538, 0.2858803868293762, -0.6332643032073975, -0.6026150584220886, -0.804608166217804, -0.5291969180107117, -0.12682558596134186, -0.023874588310718536, -0.043077971786260605, -0.022465888410806656, -0.48354020714759827, -0.617201030254364, -0.29784882068634033, -0.9004344940185547, -0.243870347738266, 0.495767742395401, -0.12681016325950623, 0.16914884746074677, -0.9664767384529114, -1.5668655633926392, -0.7753385305404663, -0.9936632513999939, -1.114585041999817, 0.48997318744659424, -0.20563140511512756, -0.21286635100841522, -0.8176586627960205, 0.031262654811143875, -0.13380134105682373, 1.0757492780685425, -1.1311031579971313, 1.121384620666504, 0.10127518326044083, 0.08451636880636215, 0.07419930398464203, 0.12520024180412292, 0.4813346266746521, -0.6260228157043457, 0.40400901436805725, -0.8536754846572876, -0.054151251912117004, -0.49360039830207825, -0.2806577682495117, -0.17520566284656525, 0.29824283719062805, 0.9115353226661682, -0.13674555718898773, -0.5012298226356506, 0.520183801651001, 1.2100220918655396, -0.8212887048721313, -0.11516407877206802, -0.4825262725353241, 0.7215264439582825, -0.14635148644447327, -0.4094570577144623, 0.3742190897464752, 0.02961471490561962, 0.9173495173454285, 0.03283480182290077, -0.21508583426475525, 0.028661636635661125, -0.5091444253921509, 0.5473324656486511, 2.199275493621826, 0.38641542196273804, 0.008787340484559536, -0.8596880435943604, 0.1829758882522583, -1.1963344812393188, -0.6729405522346497, 0.5472778677940369, 0.6261150240898132, 0.7839068174362183, -0.4011315703392029, -0.32581862807273865, -0.21633599698543549, 0.28207021951675415, 0.49465611577033997, -0.08733099699020386, -1.0815660953521729, 0.5358810424804688, 0.5792685747146606, 0.1500578224658966, 0.8023505806922913, -0.23750954866409302, 0.6911153197288513, 14.605180740356445, 1.1813400983810425, 0.017928333953022957, 0.711607813835144, 0.5889095664024353, 0.4843572676181793, -0.03815231844782829, -0.3483794927597046, -1.301723599433899, 0.01277660671621561, 1.5287598371505737, 0.21823300421237946, 0.8273618817329407, 0.41232818365097046, 0.29403358697891235, 0.08523919433355331, -0.3070257306098938, 0.6478026509284973, 0.31615588068962097, -1.5937694311141968, 0.5409993529319763, 0.12760116159915924, 0.11155251413583755, 0.726455569267273, 0.4872686266899109, 1.1951886415481567, 0.499754935503006, -0.42959460616111755, 0.5510358810424805, 0.004467153921723366, 0.9642761945724487, -0.15578316152095795, 0.5500497221946716, 1.0541943311691284, -0.96412193775177, -0.09263432770967484, -0.4901246428489685, -1.2532353401184082, 0.009478439576923847, -0.03313184157013893, -0.782005250453949, -0.29751163721084595, -0.42527681589126587, 0.40366634726524353, 0.14782683551311493, 0.22739547491073608, -0.033572182059288025, 1.203303337097168, -0.2052432745695114, 0.04399174451828003, 0.11680151522159576, -0.007592438720166683, 0.018244510516524315, 0.10345498472452164, 0.03955894708633423, -0.17536455392837524, 0.2444397211074829, 0.409883052110672, -0.6338292360305786, -0.0822218805551529, -0.08467037975788116, -0.4357735812664032, -0.07805875688791275, 0.6415002942085266, 0.7241616249084473, 0.3586312234401703, -0.5623077154159546, 0.38753199577331543, 1.108452320098877, 0.16010913252830505, -0.3880503177642822, 0.3946022391319275, 0.6112268567085266, -0.5537342429161072, 0.09641917049884796, 0.521958589553833, 0.2218169867992401, -0.48530077934265137, -0.8525339365005493, -0.5317635536193848, 0.17626260221004486, -0.598564863204956, -0.6261695623397827, 0.6416435241699219, -0.12133723497390747, -0.2605149447917938, -0.014307511039078236, -0.6542632579803467, -0.12322687357664108, 0.7866346836090088, -1.1435738801956177, -0.4411361813545227, 0.7526589035987854, -0.39183470606803894, -0.20418128371238708, 0.13052308559417725, 1.7138859033584595, 0.4580537974834442, -0.717839241027832, 0.22044914960861206, 0.4339846968650818, 0.14614859223365784, -0.7223026156425476, -0.5026249289512634, 0.8893684148788452, 0.4331998825073242, 0.15355412662029266, 0.2385113537311554, -0.019615469500422478, 0.2615410089492798, -0.9745169281959534, -0.41509824991226196, 1.019000768661499, -0.5746999382972717, -0.41197308897972107, -1.186043620109558, -0.7619848251342773, 0.37102729082107544, 0.3582848906517029, -0.44826292991638184, 0.4433964788913727, 0.33563604950904846, -0.41590699553489685, -0.0010242500575259328, -0.6405026912689209, 0.10255942493677139, 0.5255410075187683, -0.7819283604621887, -0.17189189791679382, 0.3965943455696106, 0.45556819438934326, -1.4200812578201294, -0.4499657154083252, -0.454531729221344, -0.36006566882133484, 0.22474677860736847, 0.9405696988105774, -0.4610460698604584, 0.4453209936618805, 1.0571844577789307, -0.20363187789916992, -0.8848689794540405, 0.23169784247875214, -0.7659604549407959, -0.46643394231796265, -0.13209384679794312, 1.0296392440795898, -0.09277088940143585, 0.4184744358062744, 0.7555409073829651, 0.195356085896492, -0.48131170868873596, -0.6286196112632751, -0.31974050402641296, -0.09952927380800247, -0.4750095009803772, 0.0931551456451416, -0.08199905604124069, -0.13802990317344666, 0.3134826421737671, 0.19235411286354065, 0.687829315662384, -0.30305662751197815, -0.445890337228775, 0.5357716083526611, -0.019927220419049263, -0.4872977137565613, -0.22095642983913422, 0.20640240609645844, -1.3185491561889648, 0.16711072623729706, -1.2414734363555908, 0.04309142380952835, -0.7214858531951904, -0.36414024233818054, -0.16359832882881165, 0.3290940523147583, 0.006358984392136335, 0.4623629152774811, -0.2962236702442169, -0.5054065585136414, -0.4177963435649872, -0.3540586233139038, 0.6633914709091187, 0.6855249404907227, -0.4370458722114563, 0.16318292915821075, 0.09211704879999161, 0.5541552305221558, 0.11771375685930252, 0.5225209593772888, -0.41719850897789, -1.0059984922409058, -1.7217230796813965, 0.04154505953192711, 0.1518811732530594, -0.4923307001590729, -0.618230402469635, 0.3419460952281952, 0.29739850759506226, -0.508189857006073, 0.29792577028274536, 0.41204404830932617, -0.8102110624313354, -0.07832705974578857, 0.3597472608089447, -0.6162502765655518, 0.16459186375141144, 0.2684493064880371, -0.667052149772644, -0.33962082862854004, 0.3750864565372467, 0.03788800910115242, -0.8987530469894409, -0.7717989087104797, 0.5803431868553162, -0.6550496816635132, 0.3331735134124756, -0.5001245737075806, -0.06773366779088974, -0.7881916761398315, -0.2970190644264221, -0.127861887216568, 0.22890257835388184, -0.7088287472724915, 0.7612314224243164, 0.20476467907428741, -0.953373372554779, 0.16259679198265076, 0.669748842716217, -0.3026435077190399, -0.2781675457954407, 0.386333167552948, 0.6090768575668335, -0.39748960733413696, 0.7747694253921509, 0.6297872066497803, 0.3380202054977417, -1.3663690090179443, -0.3905911147594452, 0.9249106049537659, -0.6211879849433899, -0.27901244163513184, 1.4606714248657227, -0.898323118686676, -1.3814582824707031, 0.23064233362674713, -1.2883473634719849, -0.3976445198059082, -0.676493763923645, 0.505813479423523, -0.212273970246315, 0.26050102710723877, -0.18383873999118805, -0.39992600679397583, -0.22159329056739807, 0.05435971915721893, -0.44916045665740967, 0.3788115382194519, -0.19678863883018494, -0.3668678402900696, 0.6343250870704651, 1.0961673259735107, -0.43132683634757996, -0.6140869855880737, -0.5639727115631104, -0.19815167784690857, -0.1471734493970871, 0.36191534996032715, -0.29752349853515625, -0.7694702744483948, 0.6733267903327942, 0.2633303701877594, 0.14454680681228638, -0.18992279469966888, -0.3902026116847992, 0.3950919210910797, 0.6426342725753784, 0.1872597187757492, -0.9661161303520203, -0.8087962865829468, 1.6893894672393799, 1.1252586841583252, -1.1621776819229126, 0.16051146388053894, -0.38550230860710144, -0.7790141701698303, 0.5863029956817627, 0.13171659409999847, 0.23011760413646698, 1.1837453842163086, -0.17319878935813904, 0.12315361201763153, 0.14819875359535217, -1.0732059478759766, -0.2669004499912262, 0.7192836403846741, 0.8148730397224426, 1.0231822729110718, 0.3161637485027313, 0.09695243835449219, 0.7016836404800415, -0.11271020770072937, 0.09117885679006577, 0.05811811611056328, 0.6240172386169434, -0.23116526007652283, -0.2281591296195984, 0.13995753228664398, 0.8728532791137695, -0.6864044070243835, -1.0561188459396362, 0.2945488691329956, 0.7531343698501587, 0.26103565096855164, 0.5052167177200317, 0.828648567199707, 0.022480329498648643, 0.35744771361351013, 0.38311150670051575, 0.4085009694099426, -0.7897316813468933, -0.3187910318374634, -0.24363714456558228, -0.20549964904785156, 0.010104809887707233, -0.06056934595108032, -0.20994214713573456, -0.3827979862689972, -0.5678932666778564, 0.4308348298072815, 0.47252118587493896, 0.3513268828392029, 1.1777806282043457, 0.5614162087440491, 0.18710599839687347, -0.3803407847881317, -0.5190492868423462, -0.4893602728843689, -1.1540725231170654, -0.21970029175281525, -0.6961167454719543, -0.3596438467502594, 0.12363985925912857, -0.07374496012926102, -0.3771682381629944]}, "authors": [{"authorId": "2116426849", "name": "Yi-Hsueh Liu"}, {"authorId": "2155082967", "name": "Haoyang He"}, {"authorId": "2184719751", "name": "Tianle Han"}, {"authorId": "2273584640", "name": "Xu Zhang"}, {"authorId": "2210636248", "name": "Mengyuan Liu"}, {"authorId": "2257433902", "name": "Jiaming Tian"}, {"authorId": "2257095790", "name": "Yutong Zhang"}, {"authorId": "2136025369", "name": "Jiaqi Wang"}, {"authorId": "2277869261", "name": "Xiaohui Gao"}, {"authorId": "2215167446", "name": "Tianyang Zhong"}, {"authorId": "2221032216", "name": "Yi Pan"}, {"authorId": "2211904452", "name": "Shaochen Xu"}, {"authorId": "2263593041", "name": "Zihao Wu"}, {"authorId": "2145977326", "name": "Zheng Liu"}, {"authorId": "2257586495", "name": "Xin Zhang"}, {"authorId": "2277750447", "name": "Shu Zhang"}, {"authorId": "1742535", "name": "Xintao Hu"}, {"authorId": "49104946", "name": "Tuo Zhang"}, {"authorId": "2251076040", "name": "Ning Qiang"}, {"authorId": "2254792886", "name": "Tianming Liu"}, {"authorId": "2257302793", "name": "Bao Ge"}], "references": [{"paperId": "fc7ee1828030a818f52518022a39f6a3ada60222", "title": "Scalable Extraction of Training Data from (Production) Language Models"}, {"paperId": "3004ffec00059d9c268aa396fff1ac93ec17157d", "title": "Holistic Evaluation of GPT-4V for Biomedical Imaging"}, {"paperId": "41a3c41ba1912e1384849e6898c241af89cc4a11", "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models"}, {"paperId": "a8fc3745ff459e938c3204c78ac09674ab743fc8", "title": "Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities"}, {"paperId": "833677f4b048e116920ad041248b9df5223951f9", "title": "ChatRadio-Valuer: A Chat Large Language Model for Generalizable Radiology Report Generation Based on Multi-institution and Multi-system Data"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "576a3159d6c3f646d6fda6d047dfece4ea941fdd", "title": "Towards Artificial General Intelligence (AGI) in the Internet of Things (IoT): Opportunities and Challenges"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "dd61de46eb2d5681004f8219ec39a0c65eb9170c", "title": "Chat2Brain: A Method for Mapping Open-Ended Semantic Queries to Brain Activation Maps"}, {"paperId": "6770061933096bc52b4e2f817923c285be68204f", "title": "Artificial general intelligence for radiation oncology"}, {"paperId": "ac6c263f9342a019afd1d135135c8acc4f0eed45", "title": "Functional brain network identification and fMRI augmentation using a VAE-GAN framework"}, {"paperId": "420d6754315ac5db8a040386245cd15b9fe5b459", "title": "Radiology-Llama2: Best-in-Class Large Language Model for Radiology"}, {"paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a", "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"}, {"paperId": "089f6328085066263fedc083952624ca121ebbf3", "title": "CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study"}, {"paperId": "8ef83aec0d772ebb9fb22e49d15aa979d38d3ef4", "title": "PharmacyGPT: The AI Pharmacist"}, {"paperId": "e01ab53663e5df5961a021506a9cb09f4efc3788", "title": "Challenges and Applications of Large Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "0030346329d1fa98b5c9b98d5a6e5c2c14cc1655", "title": "Prediction of cognitive scores by joint use of movie-watching fMRI connectivity and eye tracking via Attention-CensNet"}, {"paperId": "888728745dbb769e29ed475d4f7661eebe1a71cf", "title": "A Survey on Evaluation of Large Language Models"}, {"paperId": "d1a6e20eaedcac507064162b698050e37fba8f0b", "title": "SAMAug: Point Prompt Augmentation for Segment Anything Model"}, {"paperId": "7619a98ef077c8f75e0bfb98953457649209e07e", "title": "Review of Large Vision Models and Visual Prompt Engineering"}, {"paperId": "c7288139fa83a54c6bbc3680535256371678ff1e", "title": "Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications"}, {"paperId": "74363a0fae7c2187c75ecedb0408d627c9485eeb", "title": "Segment Anything Model (SAM) for Radiation Oncology"}, {"paperId": "7d5657c78f3fee9756061c6a82db44db9d413e0b", "title": "AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology"}, {"paperId": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6", "title": "CMMLU: Measuring massive multitask language understanding in Chinese"}, {"paperId": "a4f16dda8d25bdc0f93d2deb3b0876d278de9284", "title": "Radiology-GPT: A Large Language Model for Radiology"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "d818f40ea693a335e02f32dab520351d271c58bf", "title": "Artificial General Intelligence for Medical Imaging"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "839cc546b58968e2a8cb968337fb2e3a279e2b00", "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models"}, {"paperId": "6bef46eccb4c7f521e4f255a01595ebf9994ae22", "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models"}, {"paperId": "7c296190ea6f85b534f8859a699e7330df8e75e0", "title": "Multi-head attention-based masked sequence model for mapping functional brain networks"}, {"paperId": "a677938545f63ad44c87d09f85dd231980a8476f", "title": "Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT"}, {"paperId": "385376b8aa48c25403f17d6206db7c09b67e1314", "title": "Prompt Engineering for Healthcare: Methodologies and Applications"}, {"paperId": "286756b2b02d6a7bc49a7ad66686f30831f26c25", "title": "Differentiating ChatGPT-Generated and Human-Written Medical Texts: Quantitative Study"}, {"paperId": "4c8ef2db0c77aba453783f5211ebafc6695d3835", "title": "ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT"}, {"paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da", "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "258605dc5b00fe66b72091f947642a554e472aee", "title": "Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task"}, {"paperId": "848909fbae167f21589bfc7a54fbf27e306b883c", "title": "An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT"}, {"paperId": "16d83e930a4dab2d49f5d276838ddce79df3f787", "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models"}, {"paperId": "51a0bba0c5fb4257e843040615bb23f712fed4e6", "title": "Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models"}, {"paperId": "9ec42d155e2014e86ab49adcf76fd40a41a867ea", "title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "bafe023fb072045dc0cd50316382a61c8dcb9fae", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"}, {"paperId": "3cf78486fcee84f3f6898f0aaf194b7c28b92459", "title": "When Brain-inspired AI Meets AGI"}, {"paperId": "cff26bda86237d113ed01c812ad8bedd0afbe070", "title": "DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "8df67942e29cba92bb5913b62d1d2df7371842d9", "title": "AugGPT: Leveraging ChatGPT for Text Data Augmentation"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "5c61dc694400c7d4c46c93be79420d2a9c601594", "title": "Coarse-to-fine Knowledge Graph Domain Adaptation based on Distantly-supervised Iterative Training"}, {"paperId": "d9c5a59ad32c057edfb8b8410f2410212063d4b4", "title": "Spatial-Temporal Convolutional Attention for Mapping Functional Brain Networks"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "74eae12620bd1c1393e268bddcb6f129a5025166", "title": "Improving alignment of dialogue agents via targeted human judgements"}, {"paperId": "6c673f92ac808d9ccbaf3fc35e8da9cd66caf847", "title": "Petals: Collaborative Inference and Fine-tuning of Large Models"}, {"paperId": "2979b4a59d3aa847b2e2b3254d351060dff50e55", "title": "Survey on natural language processing in medical image analysis."}, {"paperId": "f4fcafa3b415ff023f3ab45fbb986bd33dac4f18", "title": "AgriBERT: Knowledge-Infused Agricultural Language Models for Matching Food and Nutrition"}, {"paperId": "5b0855b9b9361839844ae86277e8189a957d9d23", "title": "MVP: Multi-task Supervised Pre-training for Natural Language Generation"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "aa4d9972af3264d032dbee58501ed4ac49477103", "title": "Scaling Laws and Interpretability of Learning from Repeated Data"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "2c7e63744986053456f5639a45257d552de37b4c", "title": "Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI Brain Activity?"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "75cd0023bd8b45a8b38828629462a43e9ef324c6", "title": "ReMoS: Reducing Defect Inheritance in Transfer Learning via Relevant Model Slicing"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "741776172685b9717159a9fcd21841461bb33b14", "title": "MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca", "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e", "title": "A General Language Assistant as a Laboratory for Alignment"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "3186b9dd1331b647cf3304d185c248ea7ec9ad1b", "title": "OneFlow: Redesign the Distributed Deep Learning Framework from Scratch"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "f3a332ff1b73acda482e5d83696b2c701f487819", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "64a1dbdd7653eaca25c78e87335ee156b6f6959e", "title": "Constrained Language Models Yield Few-Shot Semantic Parsers"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "2b9762e91305986ac8a2d624d0a69521304405f3", "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "49f905eb03958c7cfae52ac759ea8978b8b2a6ea", "title": "Alignment of Language Agents"}, {"paperId": "238eb420c472bfdb1b4d34f9f53abec51f307a6b", "title": "FastMoE: A Fast Mixture-of-Expert Training System"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba", "title": "Extracting Training Data from Large Language Models"}, {"paperId": "33422275fbb9958f55419620697faf531482699b", "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering"}, {"paperId": "f257e1c1d10a4d8388cc132a51351e1b5e594576", "title": "On the Sub-Layer Functionalities of Transformer Decoder"}, {"paperId": "fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7", "title": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9", "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "1728cb805a9573b59330890ba9723e73d6c3c974", "title": "Knowledge Distillation: A Survey"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ba4a34680e09e77984624c95f5245d91b54373f6", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "1a6f4495474f75ae1e8bbf407f70d9a874e5b4d6", "title": "The Pushshift Reddit Dataset"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "a75649771901a4881b44c0ceafa469fcc6e6f968", "title": "How Can We Know What Language Models Know?"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "0accb5cb9d06b4cdc4ceda316bc51c8ba95e6838", "title": "PaddlePaddle: An Open-Source Deep Learning Platform from Industrial Practice"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3", "title": "Language Models as Knowledge Bases?"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "295065d942abca0711300b2b4c39829551060578", "title": "BERTScore: Evaluating Text Generation with BERT"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "0d441ab58a1027cb64084ad065cfea5e15b8e74c", "title": "Why We Need New Evaluation Metrics for NLG"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"}, {"paperId": "62df84d6a4d26f95e4714796c2337c9848cc13b5", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "88789ee88311acef28475ad33dbcd6b3c4be8358", "title": "Wikipedia"}, {"paperId": "184ac0766262312ba76bbdece4e7ffad0aa8180b", "title": "Representation Learning: A Review and New Perspectives"}, {"paperId": "d4c7ff106598422fce7851376f8de8904725b983", "title": "Generalized Minimum Bayes Risk System Combination"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "f1b962fb4070fedd46758e334db3ba4f00ddc3ec", "title": "Supervised Machine Learning: A Review of Classification Techniques"}, {"paperId": "ed8f6a72bd8903078cfaef866ca7d71e12c5b8f1", "title": "Dryad: distributed data-parallel programs from sequential building blocks"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "7a2d852aeb960ad6aebe0f9b562f64234bc5da05", "title": "Feed-forward neural networks"}, {"paperId": "5ef82a8c8aa50f99285f2143b57ca4e82da1af80", "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"}, {"paperId": "d7055e8b386cd2249f6a751c5c450b9743158c10", "title": "A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding"}, {"paperId": "73025a7866764e009483c07f30dd2954b06ab0d6", "title": "Evaluating Large Language Models for Radiology Natural Language Processing"}, {"paperId": "d1a6b3a5efde3783b53f822dc8dd00aaac934b95", "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification"}, {"paperId": "05aa63683f7d027c18f560d4472ac87c1ea754fe", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only"}, {"paperId": "636933272f831f7c30ba4e51911138e4e9509310", "title": "ClinicalRadioBERT: Knowledge-Infused Few Shot Learning for Clinical Notes Named Entity Recognition"}, {"paperId": "03c74f424a2cfa0270bf657309da179ce34502b2", "title": "BMInf: An Efficient Toolkit for Big Model Inference and Tuning"}, {"paperId": "3aa443baa063a4fb69df0deebdf24920ad1eb94a", "title": "BMCook: A Task-agnostic Compression Toolkit for Big Models"}, {"paperId": null, "title": "\u201cHuawei mindspore ai development framework,\u201d"}, {"paperId": null, "title": "\u201cFastermoe:modelingandoptimizingtrainingoflarge-scaledynamicpre-trainedmodels,\u201din"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "d79b9042fe9c30027d49fdd6fe24e737b720af6b", "title": "DRONE: Data-aware Low-rank Compression for Large NLP Models"}, {"paperId": null, "title": "\u201cPytorch distributed Experiences on accelerating data parallel training,\u201d"}, {"paperId": null, "title": "\u201cThe open imagesdatasetv4:Unifiedimageclassification,objectdetection,andvisualrelationshipdetectionatscale,\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "2795be658d617b651865d15c2f151b0eecf340f0", "title": "States"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "\u201cOpenwebtext corpus,\u201d"}, {"paperId": null, "title": "\u201cMegatron-lm:Trainingmulti-billionparameterlanguagemodelsusingmodelparallelism,\u201d"}, {"paperId": null, "title": "\u201cSocialiqa: Commonsense reasoning about social interactions,\u201d"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "\u201cJax: composable transformations of python+ numpy programs,\u201d"}, {"paperId": null, "title": "\u201cGlue:Amulti-taskbenchmarkandanalysisplatformfornaturallanguage understanding,\u201d2018"}, {"paperId": "646d4888871aca2a25111eb2520e4c47e253b014", "title": "The TREC-8 Question Answering Track Report"}, {"paperId": "c213af6582c0d518a6e8e14217611c733eeb1ef1", "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"}, {"paperId": "4954fa180728932959997a4768411ff9136aac81", "title": "This Paper Is Included in the Proceedings of the 12th Usenix Symposium on Operating Systems Design and Implementation (osdi '16). Tensorflow: a System for Large-scale Machine Learning Tensorflow: a System for Large-scale Machine Learning"}, {"paperId": null, "title": "\u201cProject gutenberg.\u201d"}, {"paperId": null, "title": "Wehave organized datasets utilized by distinct LLMs. During the training process, LLMs are typically trained on multiple datasets, as specified in Table 2 for reference"}, {"paperId": null, "title": "Preprint submitted"}, {"paperId": null, "title": "\u201cBigquery dataset.\u201d"}, {"paperId": null, "title": "\u201cCommon crawl.\u201d"}, {"paperId": null, "title": "\u201cMask-guidedbertforfewshottextclassification,\u201d"}, {"paperId": null, "title": "\u201cMoss: Training conversational language models from synthetic data,\u201d"}]}