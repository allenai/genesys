{"paperId": "8264257f573696fc0a1ef7531c825041832197f8", "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases", "abstract": "Improving the deployment e\ufb03ciency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be e\ufb00ective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this work, we fully investigate the feasibility of using INT4 quantization for language models, and show that using INT4 introduces no or negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a signi\ufb01cant accuracy drop for decoder-only models. To materialize the performance gain using INT4, we develop a highly-optimized end-to-end INT4 encoder inference pipeline supporting di\ufb00erent quantization strategies. Our INT4 pipeline is 8 . 5 \u00d7 faster for latency-oriented scenarios and up to 3 \u00d7 for throughput-oriented scenarios compared to the inference of FP16, and improves the SOTA BERT INT8 performance from FasterTransformer by up to 1 . 7 \u00d7 . We also provide insights into the failure cases when applying INT4 to decoder-only models, and further explore the compatibility of INT4 quantization with other compression techniques, like pruning and layer reduction.", "venue": "arXiv.org", "year": 2023, "citationCount": 22, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2301.12017", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work fully investigate the feasibility of usingINT4 quantization for language models, and shows that using INT4 introduces no or negligible accuracy degradation for encoder-only and encoder -decoder models, but causes a signi\ufb01cant accuracy drop for decoder- only models."}, "embedding": {"model": "specter_v2", "vector": [0.37697428464889526, 0.31118351221084595, -1.0147061347961426, 0.07773131132125854, -0.28651052713394165, 0.1553778201341629, 0.30549362301826477, 0.11169429123401642, -0.39343830943107605, -0.6231247186660767, 0.6380245685577393, -0.44938480854034424, 0.49464258551597595, 0.09012066572904587, -0.12902677059173584, 0.7690029740333557, -0.7973443865776062, 0.11166808009147644, 0.16614758968353271, -0.001242449856363237, -0.03084985353052616, -0.4653920531272888, -1.2201193571090698, 0.4603379964828491, 0.324565589427948, 1.0990265607833862, -0.17514027655124664, 0.7585508823394775, -0.764566957950592, 0.43959370255470276, 0.3847198486328125, -0.5735418796539307, 0.13674847781658173, 0.36266154050827026, -0.17223642766475677, -0.18829494714736938, 0.15899914503097534, -0.784406840801239, -0.44987431168556213, 0.8885769248008728, -0.4024026691913605, 0.01446487195789814, 0.22620093822479248, -0.5674282312393188, -0.420371413230896, 1.065750002861023, 0.39834240078926086, 0.6314696669578552, -0.6137080788612366, -0.6980081796646118, 1.2360895872116089, -1.3885666131973267, -0.13889364898204803, 1.514054536819458, 0.46783941984176636, 0.13392019271850586, -0.4156106412410736, -0.782947838306427, 0.00735366391018033, -0.031733132898807526, -1.1905827522277832, -0.833810567855835, -0.5644986629486084, 0.07654350250959396, 1.558000922203064, -0.11554883420467377, 0.04855801910161972, 0.13654069602489471, 0.16590528190135956, 1.1859019994735718, -0.012388218194246292, -0.7149680256843567, 0.10978079587221146, -0.31704771518707275, 0.45123180747032166, 0.8846197724342346, -0.14658978581428528, 0.05237538740038872, -1.055671215057373, -0.22067072987556458, 0.2988440990447998, -0.08586224168539047, 0.5514391660690308, 0.042060486972332, -0.19951440393924713, 0.3668442964553833, 0.1419580578804016, 0.6109240055084229, 0.1515459567308426, 1.2816654443740845, 1.0012038946151733, 0.14289264380931854, 0.25458472967147827, -0.06564027816057205, -0.03116433136165142, 0.11147315800189972, -1.4756635427474976, 0.026807565242052078, -0.2761897146701813, 0.9954072833061218, -0.13620677590370178, 0.41816529631614685, -0.8959107995033264, -0.09227646887302399, 1.183449387550354, 0.5174772143363953, 0.1160954087972641, -0.9560262560844421, 0.31094062328338623, -0.7503383159637451, -0.06711838394403458, -0.22030997276306152, 0.18006736040115356, -0.3330322802066803, -0.8394172191619873, -1.3358598947525024, -0.9138513207435608, 0.26303601264953613, -1.2028484344482422, 0.38137152791023254, -0.6526501774787903, 0.28640198707580566, -0.07521276921033859, -0.12488383054733276, 0.46389660239219666, 0.37144342064857483, 0.15355515480041504, 0.022091560065746307, 1.2754578590393066, -0.9945858716964722, -0.7620940804481506, -0.9990825653076172, 0.8311856389045715, -0.5974316000938416, -0.0725877657532692, -0.2904285192489624, -1.524564504623413, -1.0436164140701294, -0.8052157759666443, -0.2621477246284485, -0.18338538706302643, 0.1768263280391693, 0.8012048006057739, 0.2908659279346466, -1.5311185121536255, 0.5598121285438538, -0.5150989294052124, 0.09141052514314651, 0.40190163254737854, 0.37342938780784607, 0.49198493361473083, 0.103387251496315, -1.0645478963851929, 0.038952238857746124, 0.29543912410736084, -0.8823639154434204, -0.06285662949085236, -0.8755850791931152, -1.147721767425537, 0.19605903327465057, 0.041858382523059845, -0.3910893499851227, 1.5078476667404175, 0.18411988019943237, -1.1764636039733887, 0.46469631791114807, -0.8749454021453857, -0.1905384659767151, -0.12672941386699677, -0.22249098122119904, -0.6648087501525879, -0.2943739891052246, -0.09548311680555344, 0.5043240785598755, 0.5828579664230347, 0.042535919696092606, -0.221199169754982, 0.24816328287124634, -0.23064759373664856, -0.0700274407863617, -0.14582572877407074, 1.4504387378692627, -0.6617940664291382, -0.3804599344730377, 0.39685145020484924, 0.6860376596450806, -0.22274349629878998, 0.19096039235591888, -0.7557827234268188, -0.6262912750244141, 0.9934537410736084, -0.0322389118373394, 1.3880068063735962, -1.226091742515564, -0.9599526524543762, 0.17449571192264557, 0.11836397647857666, 0.17613647878170013, -0.1747044324874878, 0.536871612071991, -0.24580620229244232, 0.8877391815185547, 0.019799161702394485, -1.035528302192688, 0.29463979601860046, -0.4249995946884155, -0.9652749300003052, -0.6380489468574524, -0.0025730314664542675, 1.0606484413146973, -0.3520236313343048, -0.13993775844573975, -0.03118543140590191, 0.4287989139556885, -1.090394377708435, 1.0767695903778076, -0.5524436831474304, 0.1480610966682434, 0.02009279653429985, -0.15508577227592468, 0.2324087768793106, -0.39804190397262573, 0.238272562623024, -0.5451965928077698, -0.3548658788204193, 0.6836951375007629, -0.1981588900089264, 1.3102492094039917, -0.47290509939193726, 0.21663302183151245, -0.07426271587610245, -0.07077254354953766, 0.3097204566001892, 0.36064252257347107, -0.1423114389181137, -0.4475068151950836, 0.39610618352890015, 0.7107181549072266, -0.48123878240585327, 0.5866654515266418, 1.0806727409362793, 0.8776823282241821, -0.2734854221343994, 0.2855309247970581, 0.3520667552947998, -0.14459200203418732, 0.6605746150016785, 0.33435171842575073, 0.8867698311805725, 0.16901148855686188, 0.19757120311260223, -0.16236236691474915, 0.42202484607696533, -1.1736407279968262, -0.3581773042678833, 0.4719674289226532, 0.5025456547737122, 0.7998180389404297, 0.6613442897796631, -0.43608927726745605, -0.5631492137908936, -0.20556774735450745, 0.3352924585342407, 1.3395692110061646, -0.18074917793273926, -0.7490705251693726, -0.5879806280136108, -0.17665739357471466, -0.3436453640460968, 0.21792711317539215, -0.15768437087535858, -0.1874874383211136, -0.5494528412818909, -0.4815959334373474, 1.1637393236160278, 0.19545526802539825, 1.0556174516677856, -0.09634871780872345, -0.29783865809440613, -0.46030116081237793, 0.335633784532547, -1.1238877773284912, -0.46042490005493164, 0.6611025929450989, -0.6359865069389343, 0.3418234586715698, 0.3025633692741394, 0.020221378654241562, 0.0571921169757843, -0.721836268901825, 0.7240241765975952, -0.2562013566493988, 0.011918059550225735, -0.20450736582279205, 0.4484739899635315, -0.22141410410404205, -1.0665556192398071, 0.10300423204898834, -0.1470457911491394, -0.4446970820426941, 0.49254298210144043, 0.30929386615753174, 0.006141116376966238, -0.24226704239845276, -0.47595834732055664, 0.26799625158309937, 0.06273294985294342, -0.32506611943244934, 0.5742353200912476, -0.07885869592428207, -0.7389347553253174, -0.8835030198097229, 0.6190350651741028, 0.1721746176481247, -0.2063968926668167, 0.09076975286006927, -0.712364912033081, 0.06020943820476532, 0.5624955892562866, -0.01269462238997221, -0.20066596567630768, -1.3207396268844604, 0.006723805796355009, -0.6358203887939453, 0.058470144867897034, 0.07681814581155777, 0.649074137210846, 0.37401244044303894, -0.13912953436374664, 0.5636174082756042, 0.31951454281806946, -0.24890774488449097, 0.6187727451324463, -0.7316994071006775, 0.18374322354793549, 0.059055354446172714, 0.24620527029037476, -0.07329046726226807, 0.014753138646483421, -0.6325085163116455, -0.2661168873310089, -0.16081731021404266, -0.07208288460969925, 0.0872315987944603, 0.3843490779399872, -0.6457439064979553, -0.36394429206848145, -0.3530747890472412, -1.417812466621399, 0.009047554805874825, 0.07044675201177597, -0.48129311203956604, -0.06846173852682114, -0.9691566228866577, -1.5009188652038574, -0.46044495701789856, -0.9312185049057007, -1.5773167610168457, 1.0273579359054565, -0.28954043984413147, -0.47659987211227417, -0.23928196728229523, -0.58234041929245, -0.4731943905353546, 0.8188892602920532, -0.9731804132461548, 0.794002890586853, -0.00015429328777827322, -0.23643553256988525, 0.25398337841033936, -0.47150540351867676, 0.5332480072975159, -0.3553175628185272, 0.648557186126709, -0.9576848745346069, 0.5011157989501953, -0.5197567939758301, -0.13865892589092255, 0.2534049153327942, -0.04630357399582863, 1.320902705192566, -0.12830819189548492, -0.4465780556201935, 0.6545609831809998, 1.282639503479004, -0.7062318921089172, 0.24444028735160828, -0.052365005016326904, 1.0055276155471802, -0.34642165899276733, -0.4873666763305664, 0.794870913028717, 0.07837307453155518, 0.669671356678009, 0.033080339431762695, 0.05225726217031479, -0.21885620057582855, -0.430834025144577, 0.8141206502914429, 1.9190934896469116, 0.59489506483078, -0.07500805705785751, -0.9338130354881287, 0.27409833669662476, -0.852368950843811, -0.4745233952999115, 0.44621017575263977, 0.9396532773971558, 0.46589359641075134, -0.10268980264663696, -0.41229957342147827, 0.2922930419445038, 0.2896230220794678, 0.6360442042350769, -0.08625654131174088, -1.4411746263504028, 0.28074219822883606, 0.8956285119056702, 0.7633699774742126, 0.5380315780639648, -0.22941060364246368, 0.3967897593975067, 14.600956916809082, 1.0361093282699585, -0.3159383535385132, 0.6482799649238586, 0.727114200592041, 0.5628743767738342, -0.6476924419403076, -0.015825077891349792, -1.515071153640747, 0.1100243404507637, 1.7082475423812866, -0.18638916313648224, 0.2321280837059021, -0.09336578100919724, 0.051833249628543854, 0.380632221698761, -0.3257198929786682, 0.3996722102165222, 0.28238579630851746, -1.5506001710891724, 0.650623619556427, 0.09749452769756317, -0.03683125972747803, 0.5340914726257324, 0.934908926486969, 0.5830456018447876, 0.4072170555591583, -0.6931268572807312, 0.7421786785125732, -0.09204855561256409, 1.6458687782287598, -0.2736596167087555, 0.3323644995689392, 0.4294562041759491, -1.1951308250427246, 0.06860116124153137, -0.3478856682777405, -1.2602001428604126, 0.05613578483462334, 0.3487413823604584, -1.249309778213501, -0.5089279413223267, -0.17119576036930084, 0.6977142691612244, 0.21169179677963257, 0.16610512137413025, -0.06821342557668686, 0.9774206876754761, -0.38734614849090576, 0.17177662253379822, 0.05503789335489273, 0.7387663722038269, -0.019120078533887863, 0.1344335675239563, 0.28252068161964417, -0.46976929903030396, 0.1884252279996872, 0.3426889479160309, -0.40140458941459656, -0.0962117612361908, -0.2490226775407791, -0.09442713856697083, -0.10629726946353912, 0.37492960691452026, 0.30912238359451294, 0.2028149515390396, -0.5054050087928772, 0.4915817379951477, 0.48202332854270935, 0.11513650417327881, -0.5851251482963562, 0.3673035800457001, 0.4692949652671814, -0.2546136975288391, 0.15899141132831573, 0.3932013511657715, -0.25631260871887207, -0.6019426584243774, -0.8429368138313293, -0.46561571955680847, 0.10644383728504181, -0.5010530948638916, -0.14131256937980652, 0.6727450489997864, -0.01711464487016201, -0.33741095662117004, 0.17634283006191254, -0.559837281703949, 0.1608595848083496, 0.34896114468574524, -1.3554075956344604, -0.6001408696174622, 0.6236833930015564, -0.4148429334163666, -0.3816145658493042, 0.41627824306488037, 1.4821789264678955, 0.5086131691932678, 8.630717638880014e-05, 0.31729909777641296, 0.35792654752731323, 0.03311959281563759, -0.40905362367630005, -0.7388993501663208, 1.2814576625823975, 0.46896135807037354, -0.13974304497241974, 0.5265485048294067, -0.09192997962236404, 0.142780601978302, -1.0404311418533325, -0.6672272682189941, 0.9551172256469727, -0.4344516396522522, 0.05768316611647606, -1.0772082805633545, -0.3676609694957733, 0.5870975852012634, 0.3688376545906067, 0.07151412963867188, 0.2995590269565582, -0.0968150943517685, -0.631672739982605, -0.24295136332511902, -0.3861953020095825, 0.4227321743965149, 0.612403154373169, -1.0267572402954102, 0.35790616273880005, -0.21688047051429749, 0.6325761675834656, -1.1517144441604614, -0.8369737267494202, 0.23035019636154175, 0.16196441650390625, -0.4351082444190979, 0.8645245432853699, 0.2314719706773758, 1.0915136337280273, 0.6792400479316711, -0.49279022216796875, -0.2850998640060425, 0.10769863426685333, -1.0465048551559448, -0.39472293853759766, -0.13735081255435944, 0.7652791738510132, -0.2040073275566101, 0.5391440391540527, 0.6205600500106812, 0.21306097507476807, -0.5231711864471436, -0.7893858551979065, -0.3930347263813019, 0.023087577894330025, -0.7436021566390991, 0.4683023989200592, -0.7744576930999756, -0.28500768542289734, 0.10159427672624588, 0.28517818450927734, 0.3015363812446594, -0.07936594635248184, -0.8955410718917847, 0.47010338306427, 0.3143446147441864, -0.27895045280456543, -0.413191556930542, -0.8195478916168213, -1.5291154384613037, 0.06785210222005844, -1.225436806678772, -0.016303477808833122, -0.4700762927532196, -0.250082403421402, -0.2969760000705719, -0.22188133001327515, -0.1125989481806755, 0.5677215456962585, 0.2529590427875519, -0.41061338782310486, -0.34560391306877136, -0.6613322496414185, 0.8716839551925659, 0.9355617165565491, -0.6417180299758911, 0.7115422487258911, -0.31000006198883057, 0.3025739789009094, 0.3760935664176941, 0.23872828483581543, -0.22773241996765137, -1.2026478052139282, -1.5728493928909302, 0.30557969212532043, -0.19926638901233673, -0.22507107257843018, -0.9335161447525024, 0.5354046821594238, 0.5110193490982056, -0.21766281127929688, 0.08788292855024338, 0.29430022835731506, -0.7360535860061646, -0.324093759059906, 0.572045624256134, -0.760308027267456, 0.33338725566864014, 0.5377017259597778, -0.7149092555046082, -0.20374490320682526, 0.7227197289466858, -0.21471506357192993, -0.5827774405479431, -0.9988420605659485, 0.5393828749656677, -0.3850298225879669, 0.2330102026462555, -0.4741362929344177, -0.1102108284831047, -0.9376780390739441, -0.2561672031879425, 0.19657264649868011, -0.27743634581565857, -0.041364070028066635, 0.8700162172317505, 0.5513703227043152, -0.8318085074424744, 0.025064313784241676, 0.732902467250824, -0.09055155515670776, -0.26458612084388733, 0.0943320244550705, 0.42029041051864624, -0.7626743912696838, 0.5610610246658325, 0.5257138609886169, 0.08251001685857773, -1.0306432247161865, -0.12138473242521286, 0.2971701920032501, -0.5963582992553711, -0.24956896901130676, 1.1841784715652466, -0.33664724230766296, -0.6794930696487427, -0.18486419320106506, -1.6698707342147827, -0.2370263785123825, -0.8293474316596985, 0.7082850337028503, -0.0018577796872705221, 0.37123993039131165, -0.1395900696516037, -0.7162263989448547, -0.12845127284526825, -0.22324614226818085, -0.6747156977653503, -0.13640807569026947, -0.003391000209376216, -0.878603458404541, 0.5248446464538574, 1.1991205215454102, -0.2852419316768646, -0.1346757709980011, -0.46788644790649414, -0.2385108917951584, 0.08095590025186539, 0.30930784344673157, -0.039908893406391144, -0.4898252487182617, 0.8532727360725403, 0.26182466745376587, 0.35952192544937134, 0.28084275126457214, -0.5017350912094116, 0.8348644375801086, 0.2594626545906067, 0.5415507555007935, -0.20790894329547882, -0.8556257486343384, 1.486268162727356, 0.7470035552978516, -0.50038081407547, 0.4983457922935486, -0.3930075764656067, -0.21618469059467316, 0.8324742317199707, -0.010395936667919159, 0.0015265026595443487, 1.2086209058761597, 0.6130010485649109, -0.007036550901830196, 0.5226242542266846, -1.0751221179962158, -0.19591917097568512, 0.9176957011222839, 0.6011654138565063, 1.1413905620574951, 0.36062222719192505, 0.13921792805194855, 0.5862752795219421, -0.09206202626228333, 0.26650747656822205, 0.18601904809474945, 0.5067482590675354, -0.2521972358226776, -0.25037500262260437, 0.11898168176412582, 0.9821978211402893, -0.9042983651161194, -1.3427929878234863, 0.60544753074646, 0.3806006908416748, 0.5286075472831726, 0.5257235765457153, 1.1368625164031982, -0.5012326836585999, 0.15779785811901093, 0.012314701452851295, 0.5313202142715454, -0.7983682751655579, -0.4279143810272217, -0.21844804286956787, -0.39663904905319214, -0.15083767473697662, 0.07404538989067078, -0.01296886708587408, -0.7860331535339355, -0.6840178966522217, 0.4782949388027191, 0.14071567356586456, 0.31087154150009155, 0.9331798553466797, 0.8378756642341614, 0.5489053130149841, -0.3603220283985138, -0.12699128687381744, -0.3605983853340149, -0.6267424821853638, 0.050289884209632874, -0.7139211893081665, -0.13944187760353088, 0.24388161301612854, -0.013556532561779022, -0.2480122447013855]}, "authors": [{"authorId": "2129511744", "name": "Xiaoxia Wu"}, {"authorId": "2133092553", "name": "Cheng Li"}, {"authorId": "3394222", "name": "Reza Yazdani Aminabadi"}, {"authorId": "9088433", "name": "Z. Yao"}, {"authorId": "2145020341", "name": "Yuxiong He"}], "references": [{"paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1", "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "7dd1557758ca670431cf7486790fe7332e581400", "title": "Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "64a8a7eb8360f4003af85c2c8a3bf245fb2f94ae", "title": "Extreme Compression for Pre-trained Transformers Made Simple and Efficient"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "73c722148ed4a5301dc75ae291b647a1915b8ecd", "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"}, {"paperId": "0bffff14430c6fa212c3924cb68190734bd61e2d", "title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "5e8a892cc33a9fb9e701d5e10333e7b8545bf4a5", "title": "LEAP: Learnable Pruning for Transformer-based Models"}, {"paperId": "4267d94ca28170f8bab97888757691f84da09356", "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit"}, {"paperId": "90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb", "title": "Accelerating Sparse Deep Neural Networks"}, {"paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e", "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "755f16fdbbfb0b128adb3cda9c8c2799aea34290", "title": "Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "159dc82a5ee901716b0154051988b5408acfc861", "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "57f123c95ecf9d901be3a53291f53302740451e2", "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "bf151de1c9d4eb214d7c5808085deb6ab5d9127c", "title": "Lambda"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d", "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6a821cb17b30c26218e3eb5c20d609dc04a47bcb", "title": "Exploring the Regularity of Sparse Structure in Convolutional Neural Networks"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "10a2482088e469dd40f49bdc9978b292b3f7bb1f", "title": "Ternary Weight Networks"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "71b4ca7440a06094f58a4dace1a455ad6430bead", "title": "Recognizing Textual Entailment: Models and Applications Ido Dagan1, Dan Roth2, Mark Sammons2, and Fabio Massimo Zanzotto3 (1Bar-Ilan University, Israel, 2University of Illinois, Urbana, IL, and 3University of Rome \u201cTor Vergata,\u201d Italy) Morgan & Claypool (Synthesis Lectures on Human Language Technolo"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": null, "title": "NVIDIA"}, {"paperId": null, "title": "GPU workstation for deep learning"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": null, "title": "Employing CUDA Graphs in a Dynamic Environment"}, {"paperId": "3c61e6b55597cf37b19d2e4b38fc66b9c85c97b9", "title": "Ultra-Low Precision 4-bit Training of Deep Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Among the large body of litterateurs, we mainly cover the recent related works on INT4 quantization and system inference"}, {"paperId": null, "title": "First quora dataset release: Question pairs.(2017)"}, {"paperId": null, "title": "CUTLASS: Fast Linear Algebra in CUDA C++"}, {"paperId": "bf5415ffc086cc8746a4b82898f4e05b63b224e6", "title": "Convolutional Neural Network with INT4 Optimization on Xilinx Devices"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}]}