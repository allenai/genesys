{"paperId": "a4715887bd5a4c328ebf1d6ebb18ab94e71ee8d8", "title": "MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining", "abstract": "Although BERT-style encoder models are heavily used in NLP research, many researchers do not pretrain their own BERTs from scratch due to the high cost of training. In the past half-decade since BERT first rose to prominence, many advances have been made with other transformer architectures and training configurations that have yet to be systematically incorporated into BERT. Here, we introduce MosaicBERT, a BERT-style encoder architecture and training recipe that is empirically optimized for fast pretraining. This efficient architecture incorporates FlashAttention, Attention with Linear Biases (ALiBi), Gated Linear Units (GLU), a module to dynamically remove padded tokens, and low precision LayerNorm into the classic transformer encoder block. The training recipe includes a 30% masking ratio for the Masked Language Modeling (MLM) objective, bfloat16 precision, and vocabulary size optimized for GPU throughput, in addition to best-practices from RoBERTa and other encoder models. When pretrained from scratch on the C4 dataset, this base model achieves a downstream average GLUE (dev) score of 79.6 in 1.13 hours on 8 A100 80 GB GPUs at a cost of roughly $20. We plot extensive accuracy vs. pretraining speed Pareto curves and show that MosaicBERT base and large are consistently Pareto optimal when compared to a competitive BERT base and large. This empirical speed up in pretraining enables researchers and engineers to pretrain custom BERT-style models at low cost instead of finetune on existing generic models. We open source our model weights and code.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 6, "influentialCitationCount": 2, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is shown that MosaicBERT base and large are consistently Pareto optimal when compared to a competitive BERT base and large, enabling researchers and engineers to pretrain custom BERT-style models at low cost instead of finetune on existing generic models."}, "embedding": {"model": "specter_v2", "vector": [0.4036957323551178, 0.8540250062942505, -0.6193214058876038, -0.24665239453315735, -0.1806245595216751, -0.5202733874320984, 0.6311295032501221, -0.07680311799049377, -0.6010392308235168, -0.1927330046892166, 0.5473549962043762, -0.43556419014930725, 0.6629679799079895, 0.3611946403980255, -0.3528391718864441, 0.21919138729572296, -0.5825984477996826, 0.32309576869010925, -0.2481250911951065, -0.13704532384872437, -0.3290666341781616, -0.6566998958587646, -0.6378746628761292, 0.3477974236011505, 0.3238793611526489, 0.27507898211479187, 0.5820333361625671, 0.9174591302871704, -0.5295541286468506, 0.4215812087059021, 0.19354316592216492, -0.6598982214927673, 0.06160733848810196, 0.22785790264606476, -0.33305373787879944, -0.38975876569747925, 0.19419845938682556, -0.3537597954273224, -0.3089401423931122, 0.8156294822692871, -0.12044142186641693, 0.1270865648984909, 0.6225705742835999, -0.6076081395149231, -0.5826085209846497, 1.4702377319335938, 0.7329410314559937, 1.0363045930862427, -0.46043694019317627, -0.4863327741622925, 1.3681402206420898, -1.5361539125442505, 0.3517034947872162, 1.5206555128097534, 0.6113011240959167, 0.43214306235313416, -0.30017921328544617, -0.43439939618110657, 0.6957560777664185, 0.049907583743333817, -0.8633161187171936, -0.4544084668159485, -0.31866908073425293, -0.021916953846812248, 2.181899070739746, -0.4463048577308655, 0.04003385826945305, 0.6873879432678223, -0.3721843957901001, 1.4763221740722656, -0.31455719470977783, -0.8723699450492859, -0.6693776249885559, 0.2899741530418396, 0.16341473162174225, 0.7863091230392456, -0.5200597643852234, 0.4201197624206543, -0.7024415731430054, 0.22092941403388977, 0.33689385652542114, -0.2599695026874542, 0.07239728420972824, 0.25215959548950195, -0.5158719420433044, 0.5406519770622253, 0.6099876761436462, 0.8226256370544434, -0.055785827338695526, 0.570849597454071, 0.44782426953315735, 0.2135402113199234, -0.17168471217155457, 0.33359113335609436, -0.309993177652359, 0.3011595904827118, -0.9759544730186462, 0.14050324261188507, -0.055334556847810745, 1.0774469375610352, -0.2324700951576233, 0.26416921615600586, -1.0206851959228516, -0.08664467930793762, 1.4292738437652588, 0.41618987917900085, 0.33062824606895447, -0.6456072330474854, 0.4071071445941925, -0.8319880962371826, -0.1139148399233818, -0.44713008403778076, -0.04499644786119461, -0.3432295322418213, -0.5967941284179688, -1.4788105487823486, -0.41615262627601624, -0.23424938321113586, -1.2089887857437134, 0.8271219730377197, -0.3727893829345703, 0.08301063627004623, 0.37872079014778137, 0.6048184037208557, 0.7664057612419128, 0.821115255355835, 0.3108919560909271, 0.3197362720966339, 0.9194856882095337, -1.0848726034164429, -0.8019702434539795, -1.1575288772583008, 0.6779541373252869, -0.24584345519542694, 0.0933094471693039, -0.3760625422000885, -1.2313708066940308, -0.8712655901908875, -0.7864567041397095, -0.4952702224254608, -0.650474488735199, 0.368392676115036, 0.9962236881256104, 0.635308563709259, -1.0531326532363892, 0.5451188683509827, -0.020425856113433838, -0.1261133998632431, 0.43010276556015015, 0.03161764517426491, 0.3181026875972748, -0.25703540444374084, -1.3796401023864746, 0.4372212588787079, 0.5372015833854675, -0.3094483017921448, -0.327841192483902, -0.8416338562965393, -1.0731858015060425, 0.1564895212650299, 0.22884665429592133, -0.2359275221824646, 1.5679376125335693, -0.4537504017353058, -1.351823329925537, 0.8509566187858582, -0.8417154550552368, -0.22894999384880066, 0.14149202406406403, -0.06530146300792694, -0.579301655292511, -0.5635469555854797, -0.29522305727005005, 1.0081127882003784, 0.5471887588500977, -0.10658300668001175, -0.2594343423843384, 0.5242655873298645, -0.06927616149187088, 0.07622304558753967, -0.14408476650714874, 1.0648353099822998, -0.2672170400619507, -0.04936914145946503, 0.0038688266649842262, 0.9573736190795898, 0.16661830246448517, -0.5250670909881592, -0.43668776750564575, -1.0607211589813232, 0.6944581866264343, 0.005213149357587099, 0.6987781524658203, -1.0971873998641968, -0.6001811623573303, -0.1262422502040863, -0.1798427700996399, 0.2857562303543091, -0.7612794637680054, 0.5194781422615051, -0.41880351305007935, 0.5134478211402893, -0.010926843620836735, -1.094216227531433, 0.19405332207679749, -0.14411859214305878, -0.693756103515625, -0.44700801372528076, 0.4428863525390625, 1.4219398498535156, -0.7478823661804199, -0.17301912605762482, 0.06733608990907669, 0.4523470401763916, -1.2482833862304688, 1.1984179019927979, -0.5789134502410889, -0.0027686976827681065, 0.3191925287246704, -0.01706550642848015, 0.04184143617749214, -0.29643017053604126, 0.4872642457485199, -0.2685735523700714, -0.219419464468956, 0.5719821453094482, -0.17019778490066528, 1.3682444095611572, -0.3884313106536865, 0.4974954128265381, 0.2609672546386719, -0.6949846744537354, -0.0319657102227211, 0.5365493297576904, -0.48663756251335144, -0.24536117911338806, 0.5140511393547058, 0.666298508644104, -0.4354144036769867, 0.2530912160873413, 0.8541279435157776, 0.6756212115287781, 0.030808033421635628, -0.20435266196727753, 1.0230036973953247, -0.19513024389743805, 0.622941255569458, 0.4881921112537384, 0.6219804286956787, 0.3009209632873535, 0.30295202136039734, -0.1901642084121704, 0.32022204995155334, -0.6244859099388123, -0.18602904677391052, 0.3951631486415863, 0.740306556224823, 0.7716939449310303, 0.6650955080986023, -0.440755158662796, -0.4288233518600464, 0.10773785412311554, 0.5084779262542725, 1.4094748497009277, -0.29796865582466125, -0.011274506337940693, -0.7563356757164001, -0.48661983013153076, -0.04502485692501068, 0.15984921157360077, -0.16683641076087952, 0.20812948048114777, -0.6888623237609863, -0.7772955298423767, 0.7949537038803101, 0.2868397831916809, 0.642154335975647, -0.8315997123718262, -0.08261493593454361, -0.4993821084499359, 0.18726691603660583, -1.0819538831710815, -0.5851691365242004, 0.7410144209861755, -0.3048677444458008, -0.19812822341918945, 0.09603700041770935, -0.463407039642334, 0.4040968418121338, -0.7870635390281677, 0.9587429761886597, -0.4460391104221344, -0.15921014547348022, -0.22702470421791077, 0.437143474817276, -0.5477294921875, -0.7547773718833923, 0.5934348106384277, -0.20059308409690857, -0.2808782160282135, 0.6369003057479858, 0.5378607511520386, 0.2677777111530304, -0.16152672469615936, -0.39443904161453247, 0.2847975790500641, 0.2495783120393753, -0.013203094713389874, 0.6308355331420898, -0.37742459774017334, -0.1736709475517273, -1.4515674114227295, 0.41655606031417847, 0.0745973140001297, -0.6272031664848328, -0.07461762428283691, -0.3827604651451111, -0.40654245018959045, 0.558671236038208, -0.5606423020362854, -0.38865774869918823, -0.7089715003967285, -0.10206609219312668, -0.3790699541568756, -0.2716965675354004, 0.50951087474823, 0.12710921466350555, 0.5182233452796936, 0.32554373145103455, 0.10987509042024612, 0.20935089886188507, -0.11425445228815079, 0.8693056702613831, -0.6001148223876953, 0.5130497217178345, 0.4703872501850128, 0.21921122074127197, -0.3996472656726837, -0.4501553475856781, -0.7340465188026428, -0.5419195890426636, -0.5568639039993286, -0.15408620238304138, -0.07443521916866302, 0.4878661334514618, -0.5899998545646667, -0.9626381993293762, -0.34314483404159546, -1.2247124910354614, -0.3928086757659912, 0.0894751027226448, -0.33190107345581055, 0.1601470410823822, -1.3502733707427979, -1.160944938659668, -0.5650383234024048, -0.5172135829925537, -0.7561206817626953, 0.5962390899658203, 0.20560528337955475, -0.17368438839912415, -0.6520048379898071, -0.01181317400187254, -0.37152573466300964, 0.9208282232284546, -0.9312931895256042, 0.8402290940284729, 0.015110752545297146, -0.14419396221637726, -0.10443270951509476, 0.023944994434714317, 0.88961261510849, -0.6729522943496704, 0.48456305265426636, -0.9241527318954468, 0.5166113972663879, -0.688151478767395, -0.42347028851509094, 0.34785473346710205, 0.34782347083091736, 0.4422350525856018, -0.031351473182439804, -0.5746138691902161, 0.7567448019981384, 1.3606560230255127, -0.7327465415000916, 0.3484058380126953, -0.026370897889137268, 0.9862727522850037, 0.03356275334954262, -0.46319466829299927, 0.1298466920852661, 0.2435554563999176, 0.07160330563783646, 0.333903431892395, 0.031228432431817055, -0.016251754015684128, -0.6281635165214539, 0.7557045817375183, 1.3277947902679443, 0.46111705899238586, -0.33253324031829834, -1.1611813306808472, 0.7359492182731628, -1.0062276124954224, -0.6146130561828613, 0.44688066840171814, 0.23924657702445984, 0.6276106834411621, -0.45922723412513733, -0.5797085165977478, -0.2862056493759155, 0.551444947719574, 0.4658984839916229, -0.15270140767097473, -0.8397911190986633, -0.08326592296361923, 0.3045028746128082, 0.3352947235107422, 0.8203921914100647, -0.5752401947975159, 1.0418263673782349, 14.70217227935791, 0.7493486404418945, -0.12346374988555908, 0.6296159625053406, 0.6166049242019653, 0.19490328431129456, -0.47667044401168823, -0.2861674427986145, -1.6880768537521362, -0.28523436188697815, 1.1676934957504272, 0.3159985840320587, 0.6799843907356262, 0.3474012315273285, -0.17671070992946625, 0.21182747185230255, -0.5978308320045471, 0.6766912937164307, 0.53507399559021, -1.2651933431625366, 0.13566915690898895, 0.00621076999232173, 0.31687819957733154, 0.7410682439804077, 0.7712768316268921, 1.0192173719406128, 0.5037552714347839, -0.6612489223480225, 0.6516080498695374, 0.22905531525611877, 0.9803759455680847, 0.16478629410266876, 0.6986030340194702, 0.5523279309272766, -0.6229656934738159, -0.27291178703308105, -0.616744339466095, -1.133459210395813, 0.34623804688453674, 0.2818778157234192, -0.8945400714874268, -0.5978784561157227, -0.35521021485328674, 0.8850019574165344, 0.3886595666408539, 0.13368146121501923, -0.44917747378349304, 0.7173682451248169, -0.2971191704273224, -0.1696528047323227, 0.2607060670852661, 0.5430518984794617, 0.28985780477523804, 0.23696807026863098, -0.01860344223678112, -0.07251231372356415, 0.06848787516355515, 0.6360280513763428, -0.8572458028793335, -0.17366065084934235, -0.06769829243421555, -0.6010755896568298, -0.3214898109436035, 1.1103644371032715, 0.40802663564682007, 0.30948203802108765, -0.4704709053039551, 0.23050345480442047, 0.8677988052368164, 0.21064384281635284, -0.30093079805374146, 0.06909363716840744, 0.24754177033901215, -0.2730521261692047, 0.36453816294670105, 0.5419313311576843, -0.013750518672168255, -0.33861207962036133, -0.8067393898963928, -0.2772194445133209, 0.15152393281459808, -0.6778750419616699, -0.541485607624054, 0.6798945069313049, -0.4579300284385681, -0.08283527940511703, 0.034873977303504944, -1.120998501777649, 0.18739397823810577, 0.6021008491516113, -1.7184327840805054, -0.7124414443969727, 0.5201694369316101, -0.37625786662101746, -0.6751676201820374, 0.0887271985411644, 1.311684250831604, 0.35277894139289856, -0.20612964034080505, -0.09900490194559097, 0.05039408430457115, 0.3553776443004608, -0.43300381302833557, -0.8610109090805054, 0.9596283435821533, 0.3876858353614807, -0.05489235743880272, -0.02368905022740364, -0.1916799545288086, 0.0052106850780546665, -0.6166753172874451, -0.17066092789173126, 1.1450002193450928, -0.9103036522865295, -0.45615577697753906, -0.9730172157287598, -0.49234136939048767, 0.6803396940231323, 0.6226468086242676, -0.15635739266872406, 0.20105226337909698, 0.41819143295288086, -0.6253209114074707, -0.17680113017559052, -0.7088603377342224, -0.047952648252248764, 0.4831504225730896, -0.6317035555839539, -0.368360698223114, 0.07378613948822021, 0.5161985158920288, -1.2773958444595337, -0.28078481554985046, -0.23025193810462952, 0.17438547313213348, 0.039803605526685715, 0.8798709511756897, -0.29499420523643494, 0.8584796786308289, 0.9669221043586731, -0.10268938541412354, -0.8432512879371643, 0.013445657677948475, -0.7608726620674133, -0.1649867445230484, 0.2419455647468567, 1.0673073530197144, -0.29455164074897766, 0.39061298966407776, 0.4889618754386902, 0.062312062829732895, -0.2544967532157898, -0.521271288394928, -0.2859252393245697, 0.081943079829216, -0.32718926668167114, 0.24855630099773407, -0.20193752646446228, 0.04080972447991371, 0.17422661185264587, 0.5996373295783997, 0.5511776804924011, -0.6261680722236633, -0.8882340788841248, 0.2973273992538452, 0.13038016855716705, -0.1326795220375061, -0.4014791250228882, -0.5665735602378845, -1.3636651039123535, 0.15965497493743896, -1.4235117435455322, 0.3338412940502167, -0.8895633220672607, -0.44652751088142395, -0.058693088591098785, -0.21932774782180786, 0.30207931995391846, 0.14703315496444702, -0.1383277028799057, -0.403739333152771, -0.727898895740509, -0.29625922441482544, 0.6370834112167358, 0.7550016045570374, -0.6800485849380493, 0.17260192334651947, -0.19380693137645721, 0.05943959951400757, 0.25748559832572937, 0.3445335328578949, -0.4340731203556061, -0.6367387771606445, -1.6239662170410156, 0.6016645431518555, -0.14218516647815704, -0.1498485654592514, -0.6380223035812378, 0.42459186911582947, 0.7015830278396606, -0.11010634154081345, 0.022198062390089035, 0.16251888871192932, -0.9115421175956726, -0.7488858103752136, 0.006295396946370602, -0.5090198516845703, 0.10427790135145187, 0.2835453152656555, -0.9141011834144592, -0.20479264855384827, 0.4196270704269409, -0.034108348190784454, -1.0615825653076172, -0.7244985103607178, 0.3345626890659332, -0.4845769703388214, 0.2776271104812622, -0.399747371673584, -0.00950244814157486, -1.115091323852539, -0.1747359186410904, 0.07743970304727554, 0.21764904260635376, -0.4941624104976654, 0.7986820340156555, 0.2943486273288727, -0.7028498649597168, 0.047178007662296295, 0.6097787022590637, -0.39382556080818176, 0.16137973964214325, 0.3517420291900635, 0.4802890419960022, -0.7517500519752502, 0.666712760925293, 0.2984536588191986, 0.023596370592713356, -0.8890460133552551, 0.06421244144439697, 0.9188300967216492, -0.4471808671951294, -0.20278766751289368, 1.4968912601470947, -0.5922167301177979, -1.2989485263824463, 0.2833211123943329, -1.3771427869796753, -0.4882313311100006, -0.25292760133743286, 0.5452659726142883, -0.07672177255153656, 0.2185557633638382, -0.2528832256793976, -0.6502816677093506, -0.0967961773276329, 0.03460372984409332, -0.7205759286880493, 0.39700859785079956, 0.022506780922412872, -0.3346438407897949, 0.37782996892929077, 0.8488782048225403, -0.5581233501434326, -0.6610391736030579, -0.5998241901397705, -0.44601333141326904, 0.1805802583694458, 0.3816388249397278, -0.5413252711296082, -0.767838180065155, 1.0884217023849487, 0.07535328716039658, 0.30680423974990845, -0.08650044351816177, -0.27697232365608215, 0.14431555569171906, 0.8520839214324951, -0.021938156336545944, -0.7043989896774292, -0.8425784707069397, 1.6208186149597168, 0.925773024559021, -0.9904015064239502, -0.03213302418589592, -0.5440722703933716, -0.8039960861206055, 0.6557536125183105, 0.1374165266752243, -0.06348898261785507, 1.055546760559082, 0.07251119613647461, 0.3054566979408264, 0.4013797640800476, -0.7376506924629211, -0.4309316873550415, 0.7360795140266418, 1.0137653350830078, 0.884367048740387, 0.22930750250816345, 0.39205917716026306, 0.7955774068832397, -0.0982782170176506, -0.05659783259034157, 0.020207373425364494, 0.4274253249168396, -0.031340472400188446, -0.08743801712989807, 0.13762864470481873, 0.6774809956550598, -0.90742427110672, -1.0076009035110474, 0.20685407519340515, 0.5186271071434021, 0.4937528967857361, 0.6791735291481018, 0.7503963112831116, 0.3710276782512665, 0.39297938346862793, 0.36052027344703674, 0.6173012256622314, -0.7361709475517273, -0.46687355637550354, -0.2680993974208832, -0.648250937461853, -0.14220686256885529, -0.15408462285995483, -0.41907352209091187, -0.5780046582221985, -0.16288965940475464, 0.4165651798248291, 0.015983179211616516, 0.6843767166137695, 1.2540861368179321, 0.4310621917247772, 0.6050894856452942, -0.7124881148338318, -0.5104775428771973, -0.08315501362085342, -1.267551064491272, -0.09469074755907059, -0.6688342690467834, -0.09758887439966202, 0.13423673808574677, -0.06344150751829147, -0.1300274282693863]}, "authors": [{"authorId": "2267728082", "name": "Jacob Portes"}, {"authorId": "2267726546", "name": "Alex Trott"}, {"authorId": "2267726736", "name": "Sam Havens"}, {"authorId": "145104486", "name": "Daniel King"}, {"authorId": "1594028118", "name": "Abhinav Venigalla"}, {"authorId": "50411022", "name": "Moin Nadeem"}, {"authorId": "2277217297", "name": "Nikhil Sardana"}, {"authorId": "1724652", "name": "D. Khudia"}, {"authorId": "2277215716", "name": "Jonathan Frankle"}], "references": [{"paperId": "44b7adbd196e69c8771734aa8c9af5fd69c04370", "title": "BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model"}, {"paperId": "881883842c2661b41bbfc999d56c763b1ceef0bd", "title": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models"}, {"paperId": "cbff530b0792ae41bec0bab8e8884a00bd32851d", "title": "Dynamic Masking Rate Schedules for MLM Pretraining"}, {"paperId": "83edcfbb206ddad38a971d605da09390604248ea", "title": "BloombergGPT: A Large Language Model for Finance"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "07b14c24833400b79978b0a5f084803337e30a15", "title": "REPLUG: Retrieval-Augmented Black-Box Language Models"}, {"paperId": "373105fdc720c4e8f3fba7f7b7de7335a3cc4340", "title": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference"}, {"paperId": "4b308ba40e67b0b4b25c6fde17195d5a456a2f41", "title": "Cramming: Training a Language Model on a Single GPU in One Day"}, {"paperId": "5a3c1afe73d8bcc8288d17cb17be2baec8a98464", "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training"}, {"paperId": "5f6a5f319aca3f3d3f7a3c7c1ef0b8fc97ee1458", "title": "Reduce, Reuse, Recycle: Improving Training Efficiency with Distillation"}, {"paperId": "0882a2b2787b35dbcc6e341c953d964b77abd4df", "title": "When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain"}, {"paperId": "417f61a186936e07942755b1c2e6ff45f8b01129", "title": "Boosting Distributed Training Performance of the Unpadded BERT Model"}, {"paperId": "b294dc2703d25caa8988ecfd96b1d02bfff82f7e", "title": "Fast Benchmarking of Accuracy vs. Training Time with Cyclic Learning Rates"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "9b3fa3a80afdb6d34984307e457656420e60e7e7", "title": "Should You Mask 15% in Masked Language Modeling?"}, {"paperId": "4f4a409f701f7552d45c46a5b0fea69dca6f8e84", "title": "Unsupervised Dense Information Retrieval with Contrastive Learning"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "972706306f85b1bfb40c7d35c796ad5174eb0c9c", "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"}, {"paperId": "48055cf6bf89b89f87e904a45e084d03a97e8170", "title": "Pre-train or Annotate? Domain Adaptation with a Constrained Budget"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "f829674dceb91b76c9350b90d9432530eb1f7ca1", "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "7694aae9766d5f1fe74d900cd82aee898cb6e8e9", "title": "How to Train BERT with an Academic Budget"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "806adbb35ed4a95f51518f5962fd59685ad4706b", "title": "Query-Key Normalization for Transformers"}, {"paperId": "a2f38d03fd363e920494ad65a5f0ad8bd18cd60b", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "3a2b241ef190d433e24ea2e643de2a9e8336863f", "title": "Code and Named Entity Recognition in StackOverflow"}, {"paperId": "b26f2037f769d5ffc5f7bdcec2de8da28ec14bee", "title": "Dense Passage Retrieval for Open-Domain Question Answering"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028", "title": "SciBERT: A Pretrained Language Model for Scientific Text"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "afcee14fa931094b157bbdf616352fd248dccbc2", "title": "Improvements in multiprocessor system design"}, {"paperId": "70f8e3c72e9178b408667e3619a87a153fd853e6", "title": "Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned"}, {"paperId": "4745b1f7cbf3de31ed3f4436d943bd281b5b0b7a", "title": "Re-train or Train from Scratch? Comparing Pre-training Strategies of BERT in the Medical Domain"}, {"paperId": null, "title": "Electra: Pre-training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": "db8885a0037fe47d973ade79d696586453710233", "title": "The Sixth PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "title": "The Third PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": null, "title": "Mosaicml delivers leading nlp performance"}, {"paperId": null, "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms"}]}