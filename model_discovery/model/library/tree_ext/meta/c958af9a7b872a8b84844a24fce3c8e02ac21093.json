{"paperId": "c958af9a7b872a8b84844a24fce3c8e02ac21093", "title": "Blockwise Parallel Transformer for Long Context Large Models", "abstract": "Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.", "venue": "arXiv.org", "year": 2023, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2305.19370", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times larger than previous memory-efficient methods."}, "embedding": {"model": "specter_v2", "vector": [0.37446027994155884, 0.5046391487121582, 0.15231983363628387, -0.07785260677337646, -0.1461421102285385, -0.2647852599620819, 0.9428890943527222, -0.055082082748413086, -0.6219620704650879, -0.056704964488744736, 0.6137621402740479, -0.03082645870745182, 0.47329190373420715, 0.1851797103881836, -0.328042209148407, 0.04130809009075165, -0.804990828037262, 0.37195292115211487, 0.21496830880641937, -0.5836560130119324, -0.03650200739502907, -0.7063665390014648, -0.824874758720398, -0.10891097784042358, 0.13402415812015533, 0.43397077918052673, 0.34240609407424927, 0.6060000061988831, -0.45010676980018616, 0.6847106218338013, 0.6822986602783203, -0.41354623436927795, 0.5090205073356628, 0.05003023520112038, -0.485852986574173, -0.4123968780040741, 0.5863849520683289, -0.3734022378921509, -0.6045270562171936, 0.656771719455719, -0.25036385655403137, 0.1456025242805481, 0.22698098421096802, -0.5054550766944885, -0.10767737776041031, 1.411319613456726, 0.7209901809692383, 0.8963745832443237, -0.34425193071365356, -0.45609667897224426, 1.674272894859314, -1.1547099351882935, 0.08990377187728882, 1.5040439367294312, 0.5660339593887329, 0.5557546019554138, -0.22821421921253204, -1.1446202993392944, 1.0608372688293457, 0.20197293162345886, -0.4688791036605835, -0.4472082257270813, 0.34644004702568054, -0.01019138004630804, 2.0695884227752686, -0.4640062153339386, 0.42826974391937256, 0.603091835975647, 0.325487345457077, 1.3208719491958618, -0.11156769841909409, -0.7437651753425598, -0.43668311834335327, -0.3867790400981903, 0.4284205734729767, 0.8596783876419067, -0.6493728160858154, 0.7767328023910522, -0.9802765250205994, -0.0653766319155693, 0.6390559077262878, 0.10229640454053879, 0.10441190004348755, -0.08848248422145844, -0.36494454741477966, 0.628206729888916, 0.4698273837566376, 1.1378754377365112, -0.26274213194847107, 0.932676374912262, 0.48896268010139465, 0.3025936782360077, -0.18082506954669952, 0.5026116967201233, 0.06632525473833084, 0.48353874683380127, -0.639190137386322, 0.5187423825263977, -0.28967365622520447, 0.8521676063537598, -0.00021715507318731397, 0.4982761740684509, -0.7621418833732605, 0.36020147800445557, 1.47772216796875, 0.09253165870904922, 0.7920882105827332, -0.7715802788734436, 0.27155447006225586, -0.5372687578201294, -0.01138831116259098, -0.6889813542366028, -0.2700479328632355, -0.3273358643054962, -0.6162063479423523, -1.327527403831482, -0.3577739894390106, 0.30752861499786377, -0.6742681860923767, 1.1290085315704346, -0.3056800961494446, -0.1724228411912918, -0.1256188154220581, 0.10845077782869339, 0.26544389128685, 0.5177437663078308, 0.7753925919532776, -0.01836027018725872, 0.8474130630493164, -0.8659010529518127, -0.7325242161750793, -1.49012291431427, 0.578539252281189, 0.2617816925048828, -0.0053613088093698025, -0.16920103132724762, -1.429713487625122, -1.0367575883865356, -0.830331563949585, -0.1205434501171112, -0.3310464322566986, -0.27011221647262573, 0.8630974888801575, -0.02863749861717224, -0.891176164150238, 0.9612528681755066, -0.12495249509811401, -0.1430879384279251, 0.41099977493286133, 0.21347647905349731, 0.24136999249458313, -0.28540271520614624, -1.4629712104797363, 0.4907379448413849, 0.5605567097663879, -0.20049254596233368, -0.49395501613616943, -0.623274564743042, -1.3543318510055542, 0.13119015097618103, 0.4321697950363159, -0.4391447603702545, 1.4502010345458984, -0.31750771403312683, -1.1363110542297363, 0.6229683756828308, -0.4436722695827484, -0.33806735277175903, 0.14407680928707123, -0.28526145219802856, -0.4414200782775879, -0.6478140950202942, -0.2540621757507324, 0.7624472975730896, 0.44251251220703125, -0.18446999788284302, -0.42895016074180603, -0.2416808307170868, -0.1968352049589157, -0.16078631579875946, -0.3615754544734955, 0.89003986120224, 0.002750743180513382, -0.28274601697921753, 0.1947694569826126, 0.6246991157531738, -0.006029221694916487, -0.5724067687988281, -0.2672578990459442, -1.4788028001785278, 0.4621717929840088, 0.06509958952665329, 0.8675146698951721, -0.9017993807792664, -0.8023081421852112, -0.34785643219947815, 0.04395541921257973, -0.14548277854919434, -1.010769009590149, 0.835655152797699, -0.9046252965927124, 0.28386250138282776, -0.3800232708454132, -0.7594407796859741, -0.028485406190156937, 0.16471144556999207, -0.7961958646774292, -0.26477494835853577, 0.0886748731136322, 1.3427300453186035, -1.3342735767364502, -0.3160078525543213, 0.1555722951889038, -0.09791818261146545, -0.7741582989692688, 1.3402513265609741, -0.29932668805122375, 0.0786333680152893, 0.2924977242946625, -0.6237465143203735, -0.11492525041103363, -0.0812612995505333, 0.570939838886261, -0.6143417358398438, -0.20312072336673737, 0.7711300253868103, -0.008473150432109833, 1.3781883716583252, -0.2190338522195816, 0.5019237995147705, -0.31984424591064453, -0.773947536945343, 0.07304323464632034, 0.44675394892692566, 0.021485066041350365, -0.8409407734870911, 0.3977281451225281, 0.02444344013929367, -0.37744247913360596, 0.2574489414691925, 0.45542609691619873, 0.30291199684143066, -0.2050609588623047, 0.1299872249364853, 0.8119569420814514, -0.4417651295661926, 0.8196744322776794, 0.5175121426582336, 0.666071355342865, 0.5530443787574768, 0.6660025119781494, -0.2267017513513565, 0.4536449611186981, -0.8139221668243408, 0.2649215757846832, 0.29881590604782104, 0.4851670563220978, 0.5261610746383667, 0.29043370485305786, -0.8395785093307495, -0.5680698156356812, 0.357806921005249, 0.8186330795288086, 1.5528792142868042, -0.4317220449447632, -0.3474874198436737, -0.6117658019065857, -0.35839134454727173, -0.44254270195961, 0.387001097202301, -0.44425779581069946, -0.20408262312412262, -0.8734596967697144, -0.4588003158569336, 0.7154451608657837, 0.6826736330986023, 0.9237290024757385, -0.7868431806564331, -0.3566543757915497, 0.12943360209465027, 0.12397897243499756, -0.6944618225097656, -0.6742705702781677, 0.5112574696540833, -0.7022066116333008, -0.21711276471614838, 0.13294222950935364, -0.12217166274785995, -0.06527368724346161, -0.5786964893341064, 1.1795852184295654, -0.8285548686981201, 0.03064732439815998, 0.2890544831752777, 0.8973315358161926, -0.7083401083946228, -0.8028907775878906, -0.16629861295223236, 0.241641566157341, -0.3229968547821045, 0.31682685017585754, 0.3627561330795288, 0.12894019484519958, -0.038621947169303894, -0.30362725257873535, 0.2787022888660431, 0.14051936566829681, 0.33835047483444214, 0.584398090839386, -0.2641565501689911, 0.06893076747655869, -1.4710570573806763, 0.8042053580284119, 0.2670283615589142, -0.6589961647987366, 0.4719430208206177, -0.7321512699127197, -0.22914358973503113, 0.49555325508117676, -0.5629028081893921, -0.5292654633522034, -0.5912092924118042, 0.3218277096748352, -0.4803638756275177, -0.34179961681365967, 0.21125255525112152, 0.34101536870002747, 0.42659735679626465, 0.32486629486083984, 0.6432527303695679, 0.17533443868160248, 0.24643445014953613, 0.5632098913192749, -1.062395691871643, 0.6760949492454529, 0.3645991384983063, 0.13079702854156494, -0.3531261086463928, -0.17106840014457703, -0.7375220656394958, -0.4429633617401123, -0.4773618280887604, -0.3640797734260559, -0.24003097414970398, 0.197910875082016, -0.5326699018478394, -1.079644799232483, 0.053004276007413864, -1.055930256843567, -0.3699818551540375, -0.027433786541223526, -0.41211843490600586, -0.15814530849456787, -0.8254616260528564, -1.1442664861679077, -0.7614325284957886, -0.6855022311210632, -0.4782923758029938, -0.020386433228850365, 0.07498765736818314, -0.43126222491264343, -0.5969969034194946, 0.2706434726715088, -0.3403545618057251, 0.8929187059402466, -0.9431509375572205, 0.6888461112976074, -0.11406512558460236, -0.054812267422676086, -0.055569592863321304, 0.31772381067276, 0.5659944415092468, -0.11449041217565536, 0.07502423226833344, -0.9795190095901489, 0.15762345492839813, -0.22240819036960602, -0.30680230259895325, 0.16357526183128357, 0.1719791740179062, 0.5558213591575623, -0.07673206925392151, -0.5866638422012329, -0.05024353787302971, 1.4015895128250122, -0.20705923438072205, -0.07779201120138168, -0.1110261008143425, 1.1661279201507568, 0.3645707070827484, -0.2952093780040741, 0.2115001082420349, 0.5046877861022949, 0.08377323299646378, 0.3296678364276886, 0.1389177292585373, 0.1452603042125702, -0.7790769338607788, 0.4153890311717987, 1.3434115648269653, 0.2634321451187134, 0.20861171185970306, -1.2670037746429443, 0.9576002359390259, -1.1820917129516602, -0.8904868960380554, 0.7526722550392151, 0.6268032193183899, 0.4896198511123657, -0.39908701181411743, -0.3307746648788452, -0.07202618569135666, 0.2317475825548172, 0.25451144576072693, -0.580007016658783, -0.4255746304988861, -0.10843228548765182, 0.38062652945518494, 0.2798788845539093, 0.5096580982208252, -0.31694045662879944, 0.7460913062095642, 14.798914909362793, 0.4886564612388611, -0.09388412535190582, 0.5648393034934998, 0.5106861591339111, 0.3818322420120239, -0.6070466041564941, 0.03380706161260605, -1.0961014032363892, -0.48369014263153076, 1.1567329168319702, 0.1552993506193161, 0.7408061623573303, -0.05917754024267197, -0.13995927572250366, 0.26411184668540955, -0.8986340165138245, 0.8171273469924927, 0.47976553440093994, -1.0698134899139404, 0.34647321701049805, -0.06566307693719864, 0.06727532297372818, 0.7365354299545288, 0.41166380047798157, 0.9568889141082764, 0.9821481704711914, -0.12622328102588654, 0.3904988467693329, 0.21856561303138733, 0.6309378743171692, 0.265932559967041, -0.0367547944188118, 0.6490069031715393, -0.8040030598640442, -0.6615151762962341, -0.4724755883216858, -1.1753535270690918, 0.20923958718776703, -0.15228915214538574, -0.4203912913799286, -0.5161226391792297, -0.01806659810245037, 0.9769012928009033, 0.18746903538703918, -0.0672655925154686, -0.36381274461746216, 0.6968801021575928, -0.09954290837049484, 0.17477281391620636, 0.3498380780220032, 0.43700113892555237, 0.06975656747817993, -0.11941853165626526, -0.07502104341983795, 0.38724812865257263, -0.13730917870998383, 0.5556020140647888, -0.3477264940738678, -0.12475131452083588, -0.4828302264213562, -0.5166134238243103, -0.10670827329158783, 0.857568085193634, 0.7739697694778442, 0.07144743949174881, -0.25438061356544495, 0.05519514158368111, 0.8519002795219421, 0.10690528899431229, -0.25348538160324097, 0.23692038655281067, -0.046152882277965546, -0.7223751544952393, -0.09406521916389465, 0.4483184814453125, -0.15336404740810394, -0.4857501685619354, -1.0272020101547241, -0.29981446266174316, 0.5548449754714966, -0.920942485332489, -0.8619185090065002, 1.0034584999084473, -0.15399907529354095, -0.2500474452972412, -0.06997932493686676, -0.753106951713562, -0.29475706815719604, 0.36849668622016907, -1.2243081331253052, -0.7543683648109436, 0.1105571985244751, -0.04175259917974472, -0.15635567903518677, 0.03372234106063843, 1.1484116315841675, -0.023961057886481285, -0.4578191936016083, -0.12232179939746857, -0.24781659245491028, -0.017869435250759125, -0.2740558683872223, -0.7775393128395081, 0.653644323348999, 0.24436278641223907, 0.26727747917175293, 0.3685639500617981, 0.10125472396612167, 0.09047188609838486, -0.5398104786872864, 0.2376389503479004, 1.0015292167663574, -0.8144786357879639, -0.49239853024482727, -0.8384979963302612, -0.9951827526092529, 0.6659820079803467, 0.9111881256103516, -0.28273460268974304, 0.3014141619205475, 0.028066957369446754, -0.6943495869636536, -0.37197059392929077, -0.5507411360740662, 0.34751972556114197, 0.6837378740310669, -0.8017622828483582, -0.7555344104766846, -0.2109164297580719, 0.2590881884098053, -0.6620258092880249, -0.4785028100013733, -0.2706303000450134, 0.2527616024017334, -0.18082821369171143, 0.8685919046401978, -0.581236720085144, 0.7074117064476013, 0.9200109243392944, 0.15931260585784912, -0.7938693761825562, -0.5088004469871521, -0.7495484352111816, 0.2281510978937149, 0.3667258322238922, 0.3894808292388916, -0.4757717549800873, -0.06603526324033737, 1.1029162406921387, 0.11028503626585007, -0.187220960855484, -0.8000046014785767, -0.21926423907279968, 0.18620967864990234, -0.5610713958740234, 0.27461010217666626, 0.004348236136138439, 0.13969849050045013, 0.6426180005073547, 0.28313082456588745, 0.17965811491012573, -0.11402947455644608, -0.6145970821380615, 0.12297821044921875, -0.13116666674613953, 0.26374971866607666, -0.6518071293830872, -0.053449712693691254, -1.3116611242294312, 0.0557129792869091, -1.1768274307250977, 0.06547745317220688, -0.9970759749412537, -0.5610793828964233, 0.06606802344322205, -0.421080619096756, 0.4712525010108948, 0.2949284017086029, -0.7002764940261841, -0.192500501871109, -0.3579747676849365, -0.3454306125640869, 0.9424577951431274, 0.6180310845375061, -1.0680924654006958, 0.20788909494876862, -0.4263380169868469, 0.09959917515516281, 0.037737537175416946, 0.3806092441082001, -0.426545649766922, -0.7789097428321838, -1.237552523612976, 0.4652618169784546, 0.058397531509399414, -0.2269393503665924, -1.0352870225906372, 0.7825369834899902, 0.19740748405456543, -0.1673869639635086, -0.0183742493391037, 0.5477544069290161, -1.2316168546676636, -0.5360006093978882, 0.549908459186554, -1.0260076522827148, 0.3674650490283966, 0.3386405110359192, -0.38372623920440674, -0.47343745827674866, 0.9169261455535889, -0.13043911755084991, -1.4551573991775513, -0.6604737043380737, 0.5011812448501587, -0.7349880337715149, 0.02654578536748886, -0.15942132472991943, -0.05669316649436951, -0.9072666764259338, -0.1850220412015915, 0.29772862792015076, 0.8402532339096069, -0.8399707078933716, 0.9505664110183716, 0.6218669414520264, -1.0130095481872559, 0.05917897820472717, 0.53452467918396, 0.1725786179304123, 0.20370061695575714, 0.7728724479675293, 0.23211237788200378, 0.07486069947481155, 0.8280791640281677, -0.09804210066795349, 0.3851088583469391, -0.9828611016273499, 0.402655690908432, 0.9962960481643677, -0.7576788067817688, -0.052001241594552994, 1.0630184412002563, 0.06049611046910286, -1.2894635200500488, -0.01216104719787836, -1.1751242876052856, -1.086206316947937, -0.30021294951438904, 0.5151675343513489, 0.06627842038869858, -0.43816938996315, -0.09948516637086868, -0.3710609972476959, 0.16477538645267487, -0.1950153112411499, -0.6050090789794922, 0.7114701867103577, -0.046167150139808655, -0.3576565384864807, 0.9366783499717712, 0.4114997088909149, -0.8394359350204468, -0.873386025428772, -0.9905246496200562, -0.47855544090270996, 0.07742074131965637, 0.2646702229976654, -0.5287409424781799, -0.6228413581848145, 1.015956997871399, 0.5225152373313904, 0.21459843218326569, -0.06877361983060837, -0.35819071531295776, 0.12195634841918945, 0.6817992925643921, -0.010139228776097298, -0.5031996369361877, -0.6446146368980408, 1.6363930702209473, 1.6777173280715942, -0.7545114159584045, -0.22354766726493835, -0.01861409842967987, -0.677143394947052, 0.8443436622619629, 0.6100361943244934, -0.23476554453372955, 0.884623646736145, -0.39479178190231323, 0.23875443637371063, 0.05376135930418968, -1.2761056423187256, -0.13258813321590424, 0.6436619162559509, 0.9138286709785461, 0.8399350643157959, 0.08296028524637222, 0.1356809288263321, 0.7453579306602478, 0.1319395899772644, 0.24716323614120483, 0.09811066836118698, 0.4194665849208832, 0.10931918025016785, -0.021889932453632355, 0.4016936421394348, 0.5280554294586182, -0.5299323797225952, -0.75575190782547, 0.1776033639907837, 0.2911634147167206, 0.05789969861507416, 0.5922420024871826, 0.8455968499183655, 0.07651872932910919, 0.5908049941062927, 0.3989503085613251, 0.6132721900939941, -0.20905214548110962, -0.5143727660179138, -0.12739324569702148, -0.6402806639671326, -0.18728673458099365, -0.37337446212768555, -0.4743480086326599, -0.3705097734928131, 0.22063389420509338, 0.11683785170316696, -0.10832696408033371, 0.16649103164672852, 1.2670799493789673, 0.31125590205192566, 0.5018237233161926, -0.4333554208278656, -0.23077115416526794, -0.5226610898971558, -1.31944739818573, 0.15620148181915283, -0.29619789123535156, -0.2444416731595993, -0.3005114793777466, 0.17269107699394226, -0.3054981529712677]}, "authors": [{"authorId": "2143855835", "name": "Hao Liu"}, {"authorId": "1689992", "name": "P. Abbeel"}], "references": [{"paperId": "f11044596cf2eaf59f83d82b8167b16ba6a08617", "title": "Emergent Agentic Transformer from Chain of Hindsight Experience"}, {"paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340", "title": "Teaching Large Language Models to Self-Debug"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "cb3125e4f63f3d058a2a39270ecb585e86c3d1ff", "title": "Chain of Hindsight Aligns Language Models with Feedback"}, {"paperId": "a128b1c47e6842605fb95bceae930d2135fc38fc", "title": "Pretraining Without Attention"}, {"paperId": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e", "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"}, {"paperId": "860bc4f071f35d6d8529a52c2c1858d030779a6a", "title": "In-context Reinforcement Learning with Algorithm Distillation"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "6edccbd83a9aae204785d4821f97855677c33866", "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "df434c1289f3c7243b585cb9982afac3c5bf0439", "title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "56a35ffb3ca0d820155e5655b527a74bf8e7b13a", "title": "Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "9317736e9bb1b25c9d8e7325b7b364b7fbae0f3f", "title": "URLB: Unsupervised Reinforcement Learning Benchmark"}, {"paperId": "561f9f5abb2c0960a886ab6221c821295f0461a1", "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"}, {"paperId": "960aaaa85d1de9f8e2490d872165b720e3d4b565", "title": "AlphaFold and implications for intrinsically disordered proteins."}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "509b16378deec0fb6bbec1d7aeb32a4bdeedddb1", "title": "GSPMD: General and Scalable Parallelization for ML Computation Graphs"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "cec7872b194aadf54140578b9be52939eb1112e9", "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention"}, {"paperId": "deee48c5e0ac0407a1e002905caaf2b174bdb0e6", "title": "MSA Transformer"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "ec1f582446aa24f3f0920123ee6f05feea0b5e0a", "title": "Online normalizer calculation for softmax"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": null, "title": "GitHub - karpathy/nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": "1d27a56a8133f947a5a0217b00241d26f585f834", "title": "This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."}, {"paperId": null, "title": "total number of tokens) can be computationally slow"}, {"paperId": null, "title": "Fully Sharded Data Parallel: faster AI training with fewer GPUs \u2014 engineering"}]}