{"paperId": "8671464ff3ce6563939679c30a10679ea93c0f50", "title": "What Improves the Generalization of Graph Transformers? A Theoretical Dive into the Self-attention and Positional Encoding", "abstract": "Graph Transformers, which incorporate self-attention and positional encoding, have recently emerged as a powerful architecture for various graph learning tasks. Despite their impressive performance, the complex non-convex interactions across layers and the recursive graph structure have made it challenging to establish a theoretical foundation for learning and generalization. This study introduces the first theoretical investigation of a shallow Graph Transformer for semi-supervised node classification, comprising a self-attention layer with relative positional encoding and a two-layer perceptron. Focusing on a graph data model with discriminative nodes that determine node labels and non-discriminative nodes that are class-irrelevant, we characterize the sample complexity required to achieve a desirable generalization error by training with stochastic gradient descent (SGD). This paper provides the quantitative characterization of the sample complexity and number of iterations for convergence dependent on the fraction of discriminative nodes, the dominant patterns, and the initial model errors. Furthermore, we demonstrate that self-attention and positional encoding enhance generalization by making the attention map sparse and promoting the core neighborhood during training, which explains the superior feature representation of Graph Transformers. Our theoretical results are supported by empirical experiments on synthetic and real-world benchmarks.", "venue": "arXiv.org", "year": 2024, "citationCount": 6, "influentialCitationCount": 2, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is demonstrated that self-attention and positional encoding enhance generalization by making the attention map sparse and promoting the core neighborhood during training, which explains the superior feature representation of Graph Transformers."}, "embedding": {"model": "specter_v2", "vector": [0.4985745847225189, 1.02903151512146, -0.679628312587738, -0.2790524363517761, 0.27120867371559143, 0.04632498696446419, 0.24104726314544678, -0.06066536530852318, -0.08045987039804459, -0.028927907347679138, 0.22757713496685028, 0.49114781618118286, 0.07203323394060135, -0.38638463616371155, -0.15330544114112854, -0.05964279547333717, -1.0897618532180786, 0.09116539359092712, 0.5060887336730957, -0.2141828089952469, 0.08441318571567535, -0.7414799928665161, -1.1523525714874268, -0.12172140926122665, 0.7170179486274719, 0.7675251364707947, 0.21551714837551117, 0.707161545753479, -0.3597347140312195, 0.4086538553237915, 0.424558162689209, -0.7821710109710693, 0.2644624412059784, -0.11265964806079865, -0.9207592606544495, -0.08699554949998856, 0.48333626985549927, 0.11792894452810287, -0.840229332447052, 1.1750633716583252, -0.3651188313961029, 0.2950744926929474, 0.6759268045425415, -1.2540918588638306, -0.3160078823566437, 0.9983705282211304, 0.3947117030620575, 0.5723201632499695, -0.22608759999275208, -0.916797935962677, 2.1565048694610596, -0.8380900621414185, 0.41869035363197327, 1.2058440446853638, 0.6615576148033142, 0.4611849784851074, -0.34200724959373474, -0.5689923167228699, 0.6189669370651245, 0.29715099930763245, -0.8983258008956909, 0.03811362013220787, 0.32296136021614075, -0.11430346220731735, 1.3870588541030884, -0.4760918617248535, -0.2529025375843048, 0.331596314907074, -0.08166429400444031, 1.6094926595687866, 0.0555826798081398, -0.42010074853897095, 0.022799573838710785, -0.16106343269348145, 0.572891116142273, 1.329239845275879, -0.0721575915813446, 0.35074976086616516, -1.1661319732666016, 0.01936112344264984, 0.4723886549472809, 0.22558893263339996, 0.2223132997751236, -0.5433132648468018, 0.590302586555481, 0.49817362427711487, 1.0637742280960083, 0.4956080913543701, -0.5141675472259521, 1.031911849975586, 0.5237789750099182, 0.23707124590873718, 0.06883262097835541, 0.13840371370315552, 0.41957223415374756, 0.5811872482299805, -0.5774189233779907, -0.12391719222068787, 0.026133943349123, 1.0812982320785522, 0.45345306396484375, 0.1382565200328827, -0.23445598781108856, 0.16833756864070892, 1.2587478160858154, -0.5145126581192017, 0.12882108986377716, -0.7909649014472961, -0.05391612648963928, 0.2679554224014282, -0.4151378571987152, -1.0198293924331665, -0.4263969361782074, -0.7568785548210144, -1.3612172603607178, -0.8840467929840088, -0.47595226764678955, 0.2700653076171875, -0.7006673216819763, 0.5244724750518799, -0.11750297248363495, 0.2074444741010666, -0.3182965815067291, 0.4665924608707428, 0.5266256332397461, 0.3849939703941345, 0.21240659058094025, 0.45450225472450256, 0.48230209946632385, -1.08584725856781, -0.3065890371799469, -0.8662422895431519, 1.0957380533218384, 0.4669349193572998, 0.01116676814854145, -0.22762031853199005, -1.1456514596939087, -1.077646017074585, -0.9757725596427917, 0.37196463346481323, -0.5217686295509338, -0.10468082129955292, 1.1355336904525757, 0.45767390727996826, -1.0536338090896606, 1.4339773654937744, -0.3317388594150543, -0.3919135332107544, 0.7920679450035095, 0.378734827041626, -0.022146783769130707, -0.8004160523414612, -1.205612063407898, 0.0012905754847452044, 0.3783493936061859, -0.5815052390098572, -0.36963140964508057, -0.5858821272850037, -1.326780915260315, 0.3223748207092285, 0.8448233008384705, -0.6945188045501709, 1.0961021184921265, -0.2677391767501831, -0.7162914872169495, 0.8619215488433838, -0.059007201343774796, 0.11917361617088318, 0.3914025127887726, 0.3966362774372101, -0.37568169832229614, -0.23886579275131226, 0.0671960636973381, 0.10243823379278183, 0.1421494334936142, -0.7883810997009277, -0.5545157194137573, -0.36723029613494873, -0.2999069094657898, -0.19691479206085205, -0.36982491612434387, 0.426455020904541, -0.5566779971122742, -0.08238998055458069, 0.33172696828842163, 0.6157625913619995, -0.13597054779529572, -0.13091957569122314, -0.48182278871536255, -1.8962152004241943, 0.3291540741920471, 0.5403452515602112, 0.9447945356369019, -0.8611970543861389, -0.4633409380912781, 0.16314633190631866, 0.5661838054656982, -0.1085108295083046, -0.6610471606254578, 0.8861292600631714, -0.23217956721782684, 0.4329174757003784, -0.08138606697320938, -0.9004044532775879, 0.07738073915243149, 0.2866414487361908, 0.18730874359607697, -0.07659859955310822, -0.11893106997013092, 0.7589401006698608, -1.5675227642059326, -0.22341379523277283, -0.16993363201618195, -0.11476677656173706, -0.7977631092071533, 0.9610724449157715, -0.234482079744339, -0.6168853640556335, -0.4135516583919525, -0.3887993097305298, 0.18772293627262115, -0.7237707376480103, 0.529548168182373, -0.29899129271507263, 0.5355668663978577, 0.521913468837738, -0.740941047668457, 1.1127376556396484, -0.0013934409944340587, 0.2337399423122406, 0.20252133905887604, -0.9210053086280823, 0.21457691490650177, 0.04762844368815422, 0.1843385249376297, -0.4524266719818115, -0.15200500190258026, -0.13335870206356049, -0.7011198997497559, 0.34680041670799255, 0.41519662737846375, 1.1812063455581665, -0.060005418956279755, 0.3312356770038605, 0.7410162091255188, 0.3255796730518341, 0.7367501258850098, 0.7289453744888306, 1.1163448095321655, 0.43143725395202637, 0.40155065059661865, -0.48469623923301697, 0.07969258725643158, -0.5577235221862793, 0.14420899748802185, 0.72336345911026, 0.5494304299354553, 0.42531120777130127, 0.5899044275283813, -0.8075239062309265, -0.03077269159257412, 0.2072213739156723, 0.8258889317512512, 2.0251071453094482, 0.3178942799568176, -0.9793211221694946, -0.8071866035461426, -1.0378013849258423, -0.13017229735851288, 0.15617583692073822, -0.7837300300598145, -0.45801106095314026, -0.10426180064678192, -1.0328162908554077, 0.526271402835846, 0.5688363313674927, 1.1148645877838135, -0.24363945424556732, -0.18460865318775177, -0.462476521730423, 0.4988589882850647, -0.5169627070426941, -0.5732122659683228, 0.8871442079544067, -0.12595686316490173, -0.6346999406814575, -0.020371779799461365, -0.38927724957466125, 0.47018906474113464, -0.4444202184677124, 0.9684335589408875, 0.15460574626922607, -0.2122042030096054, 0.36065512895584106, 0.37062036991119385, -0.45136329531669617, -0.3974917232990265, 0.6446237564086914, 0.04778650403022766, -0.23094059526920319, 0.3126557469367981, 0.19095982611179352, -0.19899454712867737, 0.5600103139877319, -0.2864510715007782, 0.006763983517885208, 0.07706696540117264, 0.20001858472824097, 0.28073495626449585, 0.5145272016525269, -0.1558740735054016, -1.6371512413024902, 0.8281373977661133, -0.09929622709751129, -0.20740661025047302, 0.1932319700717926, -1.3026167154312134, -0.19591549038887024, 0.7489251494407654, -0.3649430274963379, -0.05170777067542076, -0.8611220717430115, 0.8009264469146729, -0.025626178830862045, -0.25106683373451233, 0.24044905602931976, 0.3442988395690918, -0.3878236711025238, 0.7497423887252808, 0.42069777846336365, 0.44067466259002686, 0.12842737138271332, 0.4934142827987671, -0.9963089227676392, 0.7628806829452515, 0.09203028678894043, 0.3813568949699402, -0.36210182309150696, 0.5170527100563049, -0.9217619895935059, -0.6325832605361938, -0.7206822037696838, -0.256803035736084, -0.08865830302238464, 0.014470611698925495, -0.2340368926525116, -1.06375253200531, -0.08780395239591599, -0.39655837416648865, -0.8010082840919495, 0.2520884573459625, -0.7642786502838135, 0.017323438078165054, -0.6679121851921082, -1.010938286781311, -0.5609147548675537, -0.2791282534599304, -0.7230862379074097, 0.20355789363384247, 0.20508290827274323, -0.10045245289802551, -0.7822491526603699, -0.3650484085083008, -0.5326101779937744, 0.7574546337127686, -0.40900301933288574, 0.7808865904808044, 0.002769982209429145, -0.42999619245529175, -0.16162370145320892, -0.29615774750709534, 0.30375048518180847, -0.08173046261072159, -0.12538446485996246, -0.32288625836372375, 0.1254364252090454, -0.9609324932098389, -0.33442407846450806, 0.16821986436843872, 0.307735800743103, 1.021422266960144, -0.2590416371822357, -1.0213892459869385, 0.473157674074173, 1.5518261194229126, -0.8744729161262512, -0.008958647027611732, 0.07403229922056198, 1.1384941339492798, 0.41975781321525574, -0.37939661741256714, 0.21210910379886627, 0.8615724444389343, 0.392595112323761, 0.7417662739753723, -0.49843427538871765, -0.422321081161499, -0.8927903771400452, 0.18595421314239502, 1.0776134729385376, 0.238079234957695, -0.3961873948574066, -1.1057966947555542, 0.769367516040802, -1.2997877597808838, -0.8624273538589478, 0.3588375151157379, 0.4803001880645752, -0.33027759194374084, -0.3878616690635681, 0.20502057671546936, -0.08281319588422775, 0.595630943775177, 0.3656342625617981, -0.46123206615448, 0.11178692430257797, 0.245437353849411, 0.3540022075176239, 0.7701318860054016, 0.5355719923973083, -0.12634754180908203, 0.8862080574035645, 14.557944297790527, 0.8354756832122803, 0.4210648536682129, -0.008692354895174503, 0.6751622557640076, 0.8379001021385193, -0.2936773896217346, 0.012182512320578098, -0.8321171402931213, 0.16418498754501343, 0.6375625133514404, -0.04233020171523094, 0.686438262462616, 0.37438592314720154, -0.338765412569046, 0.3691520392894745, -0.6885577440261841, 0.5489099621772766, 0.37446385622024536, -1.3143587112426758, 0.38905781507492065, 0.39896631240844727, 0.3632711172103882, 0.5794545412063599, 0.4383505880832672, 0.6962054967880249, 1.0124051570892334, -0.47310033440589905, -0.186371847987175, 0.0669441893696785, 1.0537439584732056, 0.04367600753903389, 0.41359201073646545, 0.732789158821106, -1.0920217037200928, -0.12913639843463898, -0.9884353280067444, -1.0927869081497192, -0.2196684032678604, 0.11788041889667511, -0.6631863117218018, 0.1629444658756256, 0.14433825016021729, 1.2859355211257935, -0.08011952042579651, 0.39424511790275574, -0.5610653758049011, 0.5862106680870056, -0.10233411937952042, -0.3604467213153839, 0.1841132640838623, 0.3205499053001404, -0.014097480103373528, 0.09242416173219681, 0.06759044528007507, 0.10642531514167786, 0.46764642000198364, 0.36019366979599, -0.5119863152503967, -0.08785323053598404, -0.5134067535400391, -0.023056451231241226, -0.006650394760072231, 0.9575823545455933, 0.7776961326599121, 0.2886260747909546, -0.08509344607591629, 0.4797283113002777, 0.9518733024597168, 0.2137499451637268, 0.28560176491737366, -0.36892467737197876, 0.10365280508995056, -0.1952437162399292, -0.360288143157959, 0.8282546997070312, -0.4906272888183594, -0.015490526333451271, -1.084792137145996, -0.07020507007837296, 1.0730233192443848, -0.9953895807266235, -1.0386762619018555, 0.9352914690971375, -0.5764944553375244, 0.024441244080662727, 0.12633857131004333, -1.0599662065505981, -0.6316803693771362, 0.2364860326051712, -1.2413103580474854, -0.5595579743385315, -0.4689326286315918, -0.2528383135795593, -0.6498687267303467, -0.06628049165010452, 1.1762553453445435, -0.062280043959617615, -0.48375189304351807, 0.4352572560310364, -0.12237567454576492, 0.06770437955856323, -0.48003822565078735, -1.6710748672485352, 0.6838061213493347, 0.42730575799942017, 0.006547498982399702, 0.20045025646686554, 0.036679916083812714, 0.21255293488502502, -0.5784069895744324, -0.2595396041870117, 0.356886088848114, -0.5223658084869385, -0.37083882093429565, -0.6346567273139954, -1.031626582145691, 0.4578963816165924, 0.49235448241233826, 0.0557398796081543, 0.3159424066543579, 0.39348384737968445, -0.4008418917655945, -0.49642041325569153, -0.822329044342041, 0.20359407365322113, 1.4549682140350342, -0.4323968291282654, -0.13023898005485535, -0.06477584689855576, -0.2022804617881775, -0.43301472067832947, -0.6512380838394165, -0.1907418817281723, -0.08549339324235916, -0.23332761228084564, 0.8716670274734497, -0.7277958393096924, 0.8249965310096741, 0.6636843085289001, 0.3321417272090912, -1.1031956672668457, -0.8847413659095764, -1.3092864751815796, 0.20159368216991425, 0.3366940915584564, 0.5694506764411926, -0.5685153603553772, 1.2975215911865234, 0.6423472166061401, 0.15487797558307648, -0.559929370880127, -0.506727397441864, 0.03948540985584259, -0.4793546497821808, 0.0060873632319271564, 0.16567182540893555, 0.511131763458252, -0.00629759207367897, 0.43230849504470825, 0.4885987639427185, 0.6857413053512573, 0.5355914831161499, -0.6199094653129578, 0.46821650862693787, -0.68129563331604, 0.25671499967575073, -0.7991565465927124, -0.7161203026771545, -1.5920509099960327, 0.1124848946928978, -1.8356484174728394, -0.1507979929447174, -1.5710899829864502, -0.38243332505226135, 0.04212979972362518, -0.40318572521209717, 0.0846429169178009, 0.05023523420095444, -0.4902125298976898, -0.9303136467933655, -0.7986680865287781, -0.7936092615127563, 0.4919869899749756, 0.7821083664894104, -0.5851356983184814, 0.4152224361896515, 0.3501923382282257, -0.4444795846939087, 0.16626200079917908, 0.484062135219574, -0.728026270866394, -0.8127079010009766, -0.987494945526123, 0.5232580304145813, -0.31212761998176575, 0.14530915021896362, -0.5699215531349182, 1.258036494255066, 0.24050486087799072, -0.018891112878918648, 0.08469025790691376, 0.010047737509012222, -1.0213009119033813, -0.4705119729042053, 0.3942549228668213, -1.0017597675323486, 0.23445270955562592, -0.7385810613632202, 0.0838845744729042, 0.028891291469335556, 0.6062586307525635, -0.011413197964429855, -1.1212183237075806, -0.6990135312080383, 0.7348582148551941, -0.2641057074069977, 0.2748000919818878, -0.1867722123861313, -0.4284302294254303, -1.3760770559310913, -0.44151991605758667, 0.11002875864505768, 0.9074285626411438, -0.07596003264188766, 0.4987489879131317, 0.3090747892856598, -1.283935546875, 0.04844177886843681, 0.10385967046022415, 0.1660906821489334, 0.4320613145828247, 0.5709059238433838, 0.14324133098125458, -0.28070786595344543, -0.1516093611717224, 0.07914943248033524, 0.32038578391075134, -0.6409019827842712, 0.38712799549102783, 0.7161992192268372, -0.5331251621246338, -0.42755457758903503, 0.9526671171188354, -0.04761636629700661, -1.056896686553955, 0.3205530643463135, -0.8233810663223267, -0.492482990026474, -0.5180938839912415, 0.03793744370341301, 0.7159590125083923, -0.7492697238922119, 0.15820954740047455, -0.4929856061935425, 0.18793141841888428, -0.025171659886837006, 0.40682852268218994, 0.8481928110122681, 0.04669193550944328, -0.42743006348609924, 0.20696310698986053, 0.6624281406402588, -0.9141944050788879, -0.9936742782592773, -1.0764611959457397, 0.03154117241501808, -0.28477808833122253, 0.38034069538116455, -0.229030042886734, -0.706910252571106, 0.6185784935951233, 0.022951312363147736, 1.1955902576446533, -0.05958840996026993, -0.006674287375062704, -0.22247974574565887, 0.5719135403633118, -0.03768506273627281, -0.8372110724449158, -0.30104637145996094, 0.5671737194061279, 1.0368951559066772, -0.586707353591919, 0.25659048557281494, -0.933232307434082, -0.4769761264324188, 0.8442449569702148, 0.5784971117973328, -0.2827748954296112, 0.78659588098526, -0.45687437057495117, -0.1683657020330429, -0.4552139341831207, -1.010222315788269, -0.4139901399612427, 1.11261785030365, 1.4678760766983032, 0.40810200572013855, 0.045747365802526474, 0.4233413338661194, 0.6227741241455078, -0.05370091646909714, -0.16282954812049866, 0.5592660307884216, -0.09348291903734207, -0.28666263818740845, 0.31435561180114746, 0.8125455975532532, 0.49401018023490906, -0.6329480409622192, -0.11145806312561035, -0.4747704267501831, 0.6754637360572815, -0.06140975281596184, 0.5151804685592651, 0.369192898273468, -0.18716466426849365, 0.4063563644886017, -0.17044854164123535, 0.2983627915382385, -0.5212065577507019, -0.16362518072128296, -0.662539005279541, -0.5046460628509521, -0.26697540283203125, -0.2739728093147278, -0.29263389110565186, -0.3210609555244446, -0.17383193969726562, 0.08699170500040054, 0.07500976324081421, 0.24978779256343842, 0.7619429230690002, 0.6591540575027466, 0.5869559645652771, 0.2819802463054657, -0.1082666739821434, -0.6290009021759033, -0.7649479508399963, -0.2739676535129547, -0.12902680039405823, -0.5145652890205383, -0.35538628697395325, -0.4309122860431671, -0.6633754372596741]}, "authors": [{"authorId": "2162633775", "name": "Hongkang Li"}, {"authorId": "2261906420", "name": "Meng Wang"}, {"authorId": "2304883171", "name": "Tengfei Ma"}, {"authorId": "2261649341", "name": "Sijia Liu"}, {"authorId": "2304609998", "name": "Zaixi Zhang"}, {"authorId": "2257357506", "name": "Pin-Yu Chen"}], "references": [{"paperId": "acb8311ae531198d0e0d479cb26a91e64ce6a1fc", "title": "How Does Promoting the Minority Fraction Affect Generalization? A Theoretical Study of One-Hidden-Layer Neural Network on Group Imbalance"}, {"paperId": "adc09237bd89ed9d1bae26a019414bf5a1cbf5a1", "title": "How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?"}, {"paperId": "154a0dc6aa4fd7c0d3160cdce42735d65e58084d", "title": "On the Convergence and Sample Complexity Analysis of Deep Q-Networks with \u03b5-Greedy Exploration"}, {"paperId": "c4c8bcedf9abfa8ebc7c9ad8eed6204e7a2d1481", "title": "Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes"}, {"paperId": "0d088de6e9358c707a17b1140a4fefb8f973c00b", "title": "Enhancing Graph Transformers with Hierarchical Distance Structural Encoding"}, {"paperId": "a87f40a49da377c0d00bebe711e417fc3b1d8969", "title": "Max-Margin Token Selection in Attention Mechanism"}, {"paperId": "01de6d0c00e7e77050a90945246b2b4acde497a2", "title": "NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification"}, {"paperId": "7e37a2c2575c62f374d7578b4a05e10197c17c7c", "title": "Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks"}, {"paperId": "3f16d91bdfca925df761d27fd3b11af7d68c63d8", "title": "On the Role of Attention in Prompt-tuning"}, {"paperId": "50eb97f832ffcd2114f79957c977215176384e3d", "title": "Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer"}, {"paperId": "206ee73f4e3187b5a71c3569bb1d443b8cce9351", "title": "Towards Understanding the Generalization of Graph Neural Networks"}, {"paperId": "f3fde8a09b757ab356da1314d7a938504edf8314", "title": "How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding"}, {"paperId": "91166a75f0e32b782a57028f1501aba6335ac550", "title": "A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity"}, {"paperId": "377e5e0e5f4253442ad1e476c076a3928b031811", "title": "Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks"}, {"paperId": "e3d1175f5b522220c31f96c5c6753a0757aae471", "title": "Rethinking the Expressive Power of GNNs via Graph Biconnectivity"}, {"paperId": "13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0", "title": "Vision Transformers provably learn spatial structure"}, {"paperId": "5152b9391762aefc532f85a801093bd38a6688c6", "title": "Hierarchical Graph Transformer with Adaptive Node Sampling"}, {"paperId": "ad855e85e324f9d82347537e0ff79d4c0ddfff7f", "title": "Generalization Guarantee of Training Graph Convolutional Networks with Graph Topology Sampling"}, {"paperId": "f7a3d9bcf052f2b4ef7d59dcca4013ea11081d0f", "title": "Long Range Graph Benchmark"}, {"paperId": "295526bc7ee119423af707e649cb55ab98854a47", "title": "NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs"}, {"paperId": "d2b297c553b5820ec114bfb1d037a537f2f66aad", "title": "A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features"}, {"paperId": "277dd73bfeb5c46513ce305136b0e71fcd2a311c", "title": "Recipe for a General, Powerful, Scalable Graph Transformer"}, {"paperId": "d8b1263524740e4af6acba9b1a8a4955e969dc0e", "title": "The Mechanism of Prediction Head in Non-contrastive Self-supervised Learning"}, {"paperId": "6df7ca762a5347a125295db395797edcfca43645", "title": "Learning and generalization of one-hidden-layer neural networks, going beyond standard Gaussian data"}, {"paperId": "ea0e4a9778e33b7f8e7b3246d63071330950995a", "title": "Structure-Aware Transformer for Graph Representation Learning"}, {"paperId": "c7dd7a1a55fd7862b3baae2e4a51ba0cd09fd816", "title": "Generalization Analysis of Message Passing Neural Networks on Large Random Graphs"}, {"paperId": "2b8a207189bc02d73d1dce850bcde24dbd984483", "title": "Representing Long-Range Context for Graph Neural Networks with Global Attention"}, {"paperId": "1948c13b4842b2abdcc3dd0cf9a95cd721d8a01d", "title": "Learning Theory Can (Sometimes) Explain Generalisation in Graph Neural Networks"}, {"paperId": "3b2c6b2184f28e0aeebeb8a1ca884979e2b53415", "title": "On Provable Benefits of Depth in Training Graph Convolutional Networks"}, {"paperId": "07df27f2c7ecacca99a581efbf4be7a0b3b3ba8b", "title": "Gophormer: Ego-Graph Transformer for Node Classification"}, {"paperId": "e52d0d7bf383f0209a07bfe178df185f1d809e9f", "title": "Pre-training Graph Transformer with Multimodal Side Information for Recommendation"}, {"paperId": "fd753314bfa805a0c831cc0a693a1de2defbc387", "title": "Global Self-Attention as a Replacement for Graph Convolution"}, {"paperId": "5863d7b35ea317c19f707376978ef1cc53e3534c", "title": "Rethinking Graph Transformers with Spectral Attention"}, {"paperId": "ac78850445bdeb4d3f8a273297916d6e6a90b2fc", "title": "Toward Understanding the Feature Learning Process of Self-supervised Contrastive Learning"}, {"paperId": "255e6239bcc51047d020d41ce0179c1270f3c22f", "title": "Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning"}, {"paperId": "bb681868f002199ac29fef0102173e61fc56825d", "title": "A PAC-Bayesian Approach to Generalization Bounds for Graph Neural Networks"}, {"paperId": "5c9a07c9d970d7287a02d1176be0c49754c560c0", "title": "The generalization error of graph convolutional networks may enlarge with more layers"}, {"paperId": "146adb863cf47182080e0c7f0ef6499ef568122b", "title": "Text Graph Transformer for Document Classification"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "04faf433934486c41d082e8d75ccfe5dc2f69fef", "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks"}, {"paperId": "e3b2f31bbd33e1f59fa2eb83101d288e1159ad2e", "title": "Fast Learning of Graph Neural Networks with Guaranteed Generalizability: One-hidden-layer Case"}, {"paperId": "a9a4e8e631890a14257539948e1813b5214c60dd", "title": "Self-Supervised Graph Transformer on Large-Scale Molecular Data"}, {"paperId": "13df9302b974633911382231b50d8cca2d189d95", "title": "Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks"}, {"paperId": "0ee0801ba010a441403f9ed666ef9bf006b3aa07", "title": "Adaptive Universal Generalized PageRank Graph Neural Network"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "23c84238f9b15d7fd87a95d3b16882a21b953d8a", "title": "Feature Purification: How Adversarial Training Performs Robust Deep Learning"}, {"paperId": "597bd2e45427563cdf025e53a3239006aa364cfc", "title": "Open Graph Benchmark: Datasets for Machine Learning on Graphs"}, {"paperId": "0ca7d8c3250d43d14fdde46bf6fc299654d861ef", "title": "Heterogeneous Graph Transformer"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "3a5af4545ee3ac3f413841c10c7605a1cefeb9e5", "title": "Generalization and Representational Limits of Graph Neural Networks"}, {"paperId": "04f3203f1214063436d81ce0c2ad7623204da488", "title": "Geom-GCN: Geometric Graph Convolutional Networks"}, {"paperId": "a6294f64b06a15bc6c853bf6e4e50209f25f1a48", "title": "A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks"}, {"paperId": "78542c2be9bb853a4e04642f2d315cfb0c6d94b3", "title": "Graph-Bert: Only Attention is Needed for Learning Graph Representations"}, {"paperId": "0feb7c19f98ac44703126dc46e60d166da1f118c", "title": "An Improved Analysis of Training Over-parameterized Deep Neural Networks"}, {"paperId": "4b5744dd44a0026c6f386d5cb21b795499d5efb7", "title": "Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks"}, {"paperId": "f338528c156f275fcff3e310e157b23980e1bd19", "title": "Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels"}, {"paperId": "44842bba66366522de782f537d9bc61d8868bf08", "title": "Revisiting Graph Neural Networks: All We Have is Low-Pass Filters"}, {"paperId": "5eda6d680dc1360e6d1b835c236a91f9dd1918fc", "title": "Behavior sequence transformer for e-commerce recommendation in Alibaba"}, {"paperId": "eb70684b4878c2669e43609a01e66fd731948d4e", "title": "Stability and Generalization of Graph Convolutional Neural Networks"}, {"paperId": "14558cb69319eed0d5bfc5648aafcd09d882f443", "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"paperId": "611fe6e34df07ea1b2104899e49642b4531b53e9", "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"paperId": "42ec3db12a2e4628885451b13035c2e975220a25", "title": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"paperId": "7a84a692327534fd227fa1e07fcb3816b633c591", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"paperId": "f60244af6cd197c5686fafb179e74709d5577266", "title": "Guaranteed Recovery of One-Hidden-Layer Neural Networks via Cross Entropy"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "99ed21b585f4d6cbc4e20002bedca8d6c08169c6", "title": "Recovery Guarantees for One-hidden-layer Neural Networks"}, {"paperId": "36eff562f65125511b5dfab68ce7f7a943c27478", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"paperId": "4971c3c2d8e3eeae93a6174e222d3c34a5933dcd", "title": "Localized Complexities for Transductive Learning"}, {"paperId": "4406c54f40e0f73db2180704d454951649df32f2", "title": "Introduction to the non-asymptotic analysis of random matrices"}, {"paperId": "eda90bd43f4256986688e525b45b833a3addab97", "title": "A tutorial on spectral clustering"}, {"paperId": "5d00e8f305d20ad937938fa4db054a33186626f7", "title": "AutoGT: Automated Graph Transformer Architecture Search"}, {"paperId": "acf87283fa8ae426f1a4987b345b401bf2913f61", "title": "Do Transformers Really Perform Badly for Graph Representation?"}, {"paperId": null, "title": "A generalization of trans-former networks to graphs"}, {"paperId": "d85f1aa35b308a37223c88a6a6f6202af8dfc751", "title": "Local Signal Adaptivity: Provable Feature Learning in Neural Networks Beyond Kernels"}, {"paperId": "2b323b6746f3dc052543ad891a179406acfeb5ee", "title": "An optimization and generalization analysis for max-pooling networks"}, {"paperId": "6d1d91a413af1212fea8791e266282019b62c37d", "title": "THE REDUCTION OF A GRAPH TO CANONICAL FORM AND THE ALGEBRA WHICH APPEARS THEREIN"}, {"paperId": null, "title": "Rewiring with positional encodings for GNNs, 2022"}, {"paperId": "1e25ca5ad8af2f604a57f504803ffa6dcfa11984", "title": "Transformers as Multi-Task Feature Selectors: Generalization Analysis of In-Context Learning"}, {"paperId": null, "title": "Visual"}, {"paperId": null, "title": "How do skip connections affect graph convolutional networks with graph sampling? a theoretical analysis on generalization"}, {"paperId": null, "title": "What Improves the Generalization of Graph Transformers? A Theoretical Dive into the Self-attention and Positional Encoding"}]}