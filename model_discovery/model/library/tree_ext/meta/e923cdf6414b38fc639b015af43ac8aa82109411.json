{"paperId": "e923cdf6414b38fc639b015af43ac8aa82109411", "title": "IAPT: Instruction-Aware Prompt Tuning for Large Language Models", "abstract": "Soft prompt tuning is a widely studied parameter-efficient fine-tuning method. However, it has a clear drawback: many soft tokens must be inserted into the input sequences to guarantee downstream performance. As a result, soft prompt tuning is less considered than Low-rank adaptation (LoRA) in the large language modeling (LLM) era. In this work, we propose a novel prompt tuning method, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft tokens. First, we install a parameter-efficient soft prompt generator at each Transformer layer to generate idiosyncratic soft prompts for each input instruction. The generated soft prompts can be seen as a semantic summary of the input instructions and can effectively guide the output generation. Second, the soft prompt generators are modules with a bottleneck architecture consisting of a self-attention pooling operation, two linear projections, and an activation function. Pilot experiments show that prompt generators at different Transformer layers require different activation functions. Thus, we propose to learn the idiosyncratic activation functions for prompt generators automatically with the help of rational functions. We have conducted experiments on various tasks, and the experimental results demonstrate that (a) our IAPT method can outperform the recent baselines with comparable tunable parameters. (b) Our IAPT method is more efficient than LoRA under the single-backbone multi-tenant setting.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a novel prompt tuning method, Instruction-Aware Prompt Tuning (IAPT), that requires only four soft tokens to learn the idiosyncratic activation functions for prompt generators automatically with the help of rational functions."}, "embedding": {"model": "specter_v2", "vector": [0.18134519457817078, 0.4779815673828125, -0.36012470722198486, 0.01893078163266182, -0.4257437288761139, -0.3286097049713135, 0.8516632318496704, -0.4722519814968109, -0.24670444428920746, -0.45304280519485474, 0.5734981298446655, 0.0330846831202507, 0.46345171332359314, 0.44159746170043945, -0.32764673233032227, -0.048790134489536285, -0.6951719522476196, 0.4402897357940674, -0.2388005554676056, -0.3482530415058136, -0.018743887543678284, -0.917230486869812, -0.6972826719284058, -0.11470737308263779, 0.44114765524864197, 0.46888965368270874, 0.3708767294883728, 0.8114303350448608, -0.643558144569397, 0.045911408960819244, 0.24641503393650055, -0.05900950729846954, 0.4760720431804657, -0.03661874309182167, -0.24331261217594147, -0.0047811889089643955, 0.3300524353981018, -0.6875144839286804, -0.24881796538829803, 0.6258903741836548, 0.08084307610988617, 0.23558872938156128, 0.5574259161949158, -0.4595990478992462, -0.3490215241909027, 0.6728364825248718, 0.4708527624607086, 0.6553187966346741, -0.0913151353597641, -0.15760967135429382, 1.380208134651184, -1.6754549741744995, -0.005788420792669058, 1.3770710229873657, 0.12770844995975494, 0.32956820726394653, -0.15963135659694672, -0.5328158736228943, 1.228859782218933, 0.003147783223539591, -0.6217979192733765, -0.35497909784317017, 0.03600193187594414, -0.18107475340366364, 1.8243014812469482, -0.18474826216697693, -0.5537149310112, 1.0015469789505005, -0.04407188296318054, 1.3424381017684937, 0.036097221076488495, -0.6669992208480835, -0.13633571565151215, -0.06130479276180267, 0.44889602065086365, 0.6550458669662476, -0.5443915724754333, 0.8045084476470947, -1.0431956052780151, -0.23620377480983734, 0.7270303964614868, -0.34344446659088135, 0.059907276183366776, 0.31622710824012756, -0.5319571495056152, 0.5183765888214111, 0.2309144288301468, 0.6006041169166565, -0.506057620048523, 0.48486068844795227, 0.7957279086112976, 0.1559222787618637, 0.14207112789154053, 0.4825303852558136, -0.4604760408401489, 0.14078232645988464, -0.509244441986084, 0.2745214104652405, 0.30249831080436707, 0.8311696648597717, -0.21427898108959198, 0.3615523874759674, -1.1591904163360596, 0.19019927084445953, 1.404091238975525, 0.18650764226913452, 0.5310076475143433, -0.8171820640563965, 0.24955302476882935, -1.063181757926941, 0.09300295263528824, -0.8140764236450195, -0.2897111475467682, -0.2456919103860855, -0.7744879722595215, -1.2230504751205444, -0.48528409004211426, -0.029648836702108383, -0.6418685913085938, 1.136765718460083, -0.39569610357284546, 0.1366041749715805, -0.04811248555779457, 0.5959891080856323, 0.6933149099349976, 0.9764476418495178, 0.5899736285209656, 0.01818634569644928, 0.708236575126648, -1.2036044597625732, -0.4732109308242798, -1.1507501602172852, 0.7590014338493347, -0.459053099155426, 0.5521582365036011, 0.005236523225903511, -0.980789840221405, -1.0875260829925537, -0.7556284070014954, 0.3102964758872986, -0.23298373818397522, 0.5361890196800232, 1.172960638999939, 0.17070455849170685, -0.8992916345596313, 0.2462320476770401, -0.19791048765182495, -0.2893398404121399, -0.0616212859749794, 0.045542605221271515, 0.3363935053348541, -0.35248881578445435, -1.3897392749786377, 0.016238432377576828, 0.17464008927345276, -0.6046876311302185, -0.43312910199165344, -0.7060656547546387, -0.8175155520439148, -0.0661160945892334, 0.6024026274681091, -0.4784654378890991, 1.7056941986083984, -0.42284321784973145, -1.8420507907867432, 0.4672934114933014, 0.0809689462184906, 0.7062804698944092, 0.20218443870544434, -0.3080205023288727, -0.33216592669487, -0.5779231190681458, 0.19654370844364166, 0.9666422605514526, 0.6081187129020691, -0.05258285626769066, -0.17858529090881348, 0.6067959666252136, -0.13764126598834991, 0.22734376788139343, -0.11910505592823029, 1.0199466943740845, -0.1764717549085617, -0.07693497091531754, 0.2821722626686096, 0.9105959534645081, -0.23833155632019043, -0.710286557674408, -0.552126944065094, -1.3383114337921143, 0.4248986840248108, 0.043799348175525665, 0.6800543069839478, -0.9753217101097107, -0.9940969347953796, -0.3864876627922058, -0.1667104810476303, 0.020431865006685257, -1.1590423583984375, 0.6010498404502869, -0.5083845853805542, 0.24033644795417786, 0.34415626525878906, -0.9942125678062439, 0.4196541905403137, -0.40978291630744934, -0.872255265712738, -0.33068862557411194, 0.3437688946723938, 1.1143847703933716, -0.923870861530304, 0.1402612179517746, -0.21456629037857056, 0.0028763371519744396, -1.2609537839889526, 1.6805425882339478, -0.5526491403579712, 0.4188601076602936, -0.21319687366485596, -0.32784774899482727, 0.0067704785615205765, -0.504166841506958, 0.08891361206769943, -0.3568907380104065, -0.05461036041378975, 0.5774996876716614, -0.5428093671798706, 1.4696111679077148, -0.31624725461006165, 0.18391162157058716, -0.2141154557466507, -0.5569100379943848, 0.440192848443985, 0.6699267625808716, -0.26217371225357056, -0.7702265977859497, 0.18054451048374176, 0.7674798369407654, -0.8251357078552246, 0.02448344975709915, 0.9029783606529236, 0.620728075504303, -0.6090356707572937, -0.0060614910908043385, 0.4146301746368408, 0.08745907992124557, 0.6925654411315918, 0.4914265275001526, 0.6461828351020813, 0.5029915571212769, 0.5376710891723633, -0.19234232604503632, 0.3658369183540344, -0.858401358127594, -0.00877649337053299, 0.5186602473258972, 0.7569567561149597, 0.8056073784828186, 0.4056403636932373, -0.8276533484458923, -0.055459097027778625, -0.17581351101398468, 0.9024757742881775, 2.034409284591675, -0.22034983336925507, -0.06002684682607651, -0.782325029373169, -0.2451993227005005, -0.6013133525848389, 0.1423974633216858, 0.0020931062754243612, -0.33553123474121094, -0.6530136466026306, -1.1186060905456543, 0.4278286397457123, 0.3532719314098358, 0.8811970949172974, -0.8029102087020874, -0.26541680097579956, 0.04660100117325783, 0.03646639734506607, -0.9242813587188721, -1.110849380493164, 0.8184195756912231, -0.16530176997184753, -0.0023981716949492693, -0.17427273094654083, -0.12914326786994934, 0.18857361376285553, -0.74189692735672, 1.112486481666565, -0.9359790682792664, 0.025236371904611588, 0.31636738777160645, 0.33053305745124817, -0.29330095648765564, -0.8492104411125183, 0.47232431173324585, 0.2955017685890198, -0.17672887444496155, 0.3023778796195984, 0.6267427802085876, 0.3071567118167877, 0.08724980056285858, -0.5410367250442505, 0.40490904450416565, 0.16979198157787323, 0.1314254105091095, 0.7850548028945923, -0.8696004748344421, 0.2571636736392975, -1.2655998468399048, 1.2465835809707642, 0.16547685861587524, -0.48997166752815247, 0.14756135642528534, -0.47575828433036804, -0.4048140048980713, 0.7291988134384155, -0.7202162742614746, -0.42066290974617004, -0.8405458331108093, 0.29730117321014404, 0.13733425736427307, -0.7417947053909302, 0.324822336435318, 0.02334653027355671, 0.40643855929374695, 0.5936000347137451, 0.38444775342941284, -0.09103897213935852, -0.30474990606307983, 0.769641101360321, -0.7978017926216125, 0.7986617088317871, -0.1736917346715927, 0.21901005506515503, -0.6221573352813721, -0.5015904307365417, -0.5639134645462036, -0.44207140803337097, -0.5246520042419434, -0.2598712146282196, -0.26921889185905457, 0.1135142594575882, -0.5523407459259033, -0.9434454441070557, 0.07354870438575745, -1.56361985206604, -0.40158769488334656, -0.12146302312612534, -0.42700883746147156, -0.4614785313606262, -1.14643394947052, -1.2751262187957764, 0.007989206351339817, -0.645166277885437, -1.0423897504806519, 0.38675668835639954, 0.31871888041496277, -0.5209842920303345, -0.41473934054374695, 0.411315381526947, -0.6277682185173035, 1.2604992389678955, -0.890785813331604, 0.7838740944862366, -0.2694529891014099, -0.5620548129081726, -0.15017007291316986, 0.07576219737529755, 0.5402820706367493, 0.025117071345448494, 0.07536228001117706, -1.097210168838501, 0.29726970195770264, -0.3297414779663086, -0.32668715715408325, -0.18707671761512756, 0.3609387278556824, 0.28651872277259827, -0.2153855562210083, -0.27278590202331543, 0.6556861996650696, 1.1382240056991577, -0.5317225456237793, -0.2825416922569275, 0.29564642906188965, 1.0710444450378418, 0.1928868442773819, -0.13388431072235107, 0.7299020886421204, 0.38040637969970703, 0.5846297740936279, 0.10101544111967087, -0.4524184465408325, -0.07903940230607986, -0.6407496929168701, 1.012014389038086, 2.0973434448242188, 0.5705646872520447, 0.2852453589439392, -0.9676285982131958, 0.2640296518802643, -1.1147242784500122, -0.25573787093162537, 0.5978666543960571, 0.5380570888519287, 0.4406124949455261, -0.5549967288970947, -0.30754393339157104, -0.5827493667602539, 0.19999800622463226, 0.4913175106048584, -0.30769219994544983, -1.0570653676986694, 0.06370314210653305, 0.011943615972995758, -0.19025151431560516, 0.6551374197006226, -0.36702635884284973, 0.9418298602104187, 14.539175987243652, 0.8099746704101562, -0.24489825963974, 0.6638221740722656, 0.9551637172698975, 0.5396837592124939, -0.28916436433792114, -0.3066087067127228, -1.2385540008544922, -0.21427872776985168, 1.426393747329712, -0.005716138519346714, 0.6099388003349304, 0.2386762797832489, 0.27463462948799133, 0.3354564905166626, -0.6894282698631287, 0.8620124459266663, 0.4026870131492615, -0.9530404806137085, 0.26518216729164124, -0.19393445551395416, 0.8084167838096619, 0.40423640608787537, 0.8484782576560974, 1.0281552076339722, 0.3396448493003845, -0.09908629208803177, 0.6184489130973816, 0.07074404507875443, 0.8067342042922974, 0.15251825749874115, 0.22352787852287292, 0.45074838399887085, -0.6792004108428955, 0.012887456454336643, -0.5458887815475464, -1.2887651920318604, 0.4516507089138031, 0.12431670725345612, -0.5032100677490234, -0.5187429189682007, -0.5742308497428894, 0.7019379138946533, 0.19782894849777222, 0.5466905832290649, -0.41694676876068115, 0.8231733441352844, -0.08667820692062378, 0.32049816846847534, 0.5580869317054749, 0.4904564321041107, 0.25709640979766846, 0.08860961347818375, -0.14333975315093994, 0.022129451856017113, 0.6881725192070007, 0.8319154381752014, -0.28199177980422974, 0.2515062391757965, 0.02535269968211651, -0.05928156152367592, -0.09042718261480331, 0.941852331161499, 0.5955384969711304, 0.18526044487953186, -0.44390180706977844, 0.4038143455982208, 0.4043288826942444, 0.6098811626434326, -0.1537538468837738, 0.358062207698822, 0.19054299592971802, -0.6029878854751587, -0.6211858987808228, 0.6163634657859802, -0.3575645387172699, -0.2899046838283539, -0.7015905380249023, -0.6403000354766846, -0.1357029527425766, -0.5873816609382629, -0.8857484459877014, 0.8965386748313904, -0.0831114649772644, -0.3096730709075928, -0.13830433785915375, -0.6630720496177673, -0.6040236949920654, 0.4864669740200043, -1.162001609802246, -0.30641108751296997, 0.2296682596206665, -0.6624202728271484, -0.22579079866409302, -0.14553162455558777, 1.3690319061279297, 0.33167681097984314, -0.7658922076225281, 0.2619943618774414, 0.040262721478939056, -0.41270121932029724, 0.20131583511829376, -0.7834131121635437, 1.1618610620498657, 0.2882419526576996, -0.3005538582801819, 0.06295949965715408, 0.16527928411960602, 0.2848126292228699, -0.5587276816368103, 0.08716631680727005, 0.818260669708252, -0.6691513657569885, -0.46984609961509705, -0.58319091796875, -0.7617077827453613, -0.0035828067921102047, 0.5322457551956177, -0.35133853554725647, 0.3316114544868469, 0.25463393330574036, -0.7617199420928955, -0.23191775381565094, -0.27556249499320984, -0.08170204609632492, 0.17399060726165771, -0.5153549909591675, 0.02560121938586235, 0.1978132724761963, 0.7565255761146545, -1.2036319971084595, -0.13354991376399994, -0.48901793360710144, -0.5185750722885132, 0.1339089721441269, 0.9145160913467407, -0.44462066888809204, 0.6109299063682556, 0.9287558197975159, -0.20536039769649506, -0.9238339066505432, -0.20409232378005981, -1.225856065750122, 0.4807090759277344, 0.41403070092201233, 0.6918708682060242, -0.5668858289718628, 0.11800676584243774, 1.0645861625671387, -0.05216580629348755, -0.3651996850967407, -0.4816746413707733, -0.4705263674259186, 0.24645167589187622, -0.493233323097229, 0.4136490225791931, -0.2410268783569336, 0.14809487760066986, 0.47481879591941833, 0.4098800718784332, 0.48799750208854675, -0.25297680497169495, -0.8229344487190247, 0.45560285449028015, 0.15854106843471527, -0.21513168513774872, -0.5040984153747559, -0.10914912074804306, -1.9754096269607544, -0.10814415663480759, -1.1364941596984863, 0.5314764976501465, -0.6765110492706299, 0.034935805946588516, 0.1060069128870964, -0.47563380002975464, 0.18429945409297943, -0.08500409126281738, -0.677699863910675, -0.2610068917274475, -0.3768291771411896, -0.8323732614517212, 1.0315169095993042, 0.827301561832428, -0.6086794137954712, -0.1851678192615509, -0.03175721317529678, -0.1595289260149002, 0.18782512843608856, 0.43499550223350525, -0.3646901547908783, -0.3601236641407013, -1.150174617767334, 0.3633311986923218, -0.010326888412237167, -0.0015022201696410775, -0.6473650932312012, 0.6212366819381714, 0.17106592655181885, -0.5380828976631165, 0.18217624723911285, -0.15529148280620575, -0.872613251209259, -0.6913324594497681, 0.14389009773731232, -0.7300564050674438, 0.2983589470386505, 0.28518369793891907, -0.8422407507896423, -0.2563512921333313, 0.8900665640830994, -0.37147215008735657, -0.9143961071968079, -0.8771328330039978, 0.5174895524978638, -0.6915194988250732, 0.1967647224664688, -0.6409247517585754, 0.16374370455741882, -1.2371716499328613, -0.21620704233646393, 0.4479592740535736, 0.13926060497760773, -0.44148704409599304, 0.9974647760391235, 0.27498024702072144, -1.3249539136886597, -0.07376928627490997, 0.6562169194221497, -0.08368498086929321, 0.05660718306899071, 0.44980037212371826, 0.6303366422653198, -0.1472853720188141, 0.9806974530220032, 0.21089814603328705, 0.2281506508588791, -0.4076731503009796, -0.04740462824702263, 0.7499188184738159, -0.7391432523727417, -0.016111720353364944, 1.2152422666549683, 0.3581730127334595, -1.3069645166397095, 0.23861120641231537, -1.0179449319839478, -0.33966439962387085, -0.6530094146728516, 0.6440357565879822, 0.2484244555234909, -0.06902632117271423, -0.20132432878017426, -0.44602084159851074, 0.14670105278491974, -0.38898640871047974, -0.4365711808204651, 0.3065100908279419, -0.06240770220756531, -0.27689650654792786, 0.45042628049850464, 0.938037633895874, -1.1445329189300537, -1.0385016202926636, -0.6389239430427551, -0.16600680351257324, 0.12325817346572876, 0.08235929161310196, -0.49207133054733276, -0.5350321531295776, 0.6509734392166138, 0.5092710256576538, -0.18454530835151672, 0.23833037912845612, -0.058866556733846664, -0.06101072579622269, 0.4714837372303009, 0.1984899491071701, -0.6782031655311584, -0.029731491580605507, 1.4183768033981323, 1.3370089530944824, -1.0796411037445068, -0.4086098372936249, -0.15976056456565857, -0.7022738456726074, 0.7404507398605347, 0.5459915995597839, 0.20281030237674713, 0.4889484643936157, -0.24201199412345886, 0.4687395691871643, 0.014645751565694809, -1.251402735710144, 0.04952574893832207, 0.8463586568832397, 1.0097732543945312, 0.941714882850647, 0.7240976691246033, -0.03476777672767639, 1.1419240236282349, 0.23289094865322113, 0.07532022148370743, 0.5998970866203308, 0.0909011960029602, -0.36261802911758423, -0.5206488370895386, -0.3439912796020508, 0.5772685408592224, -0.498982310295105, -0.830161988735199, 0.30436840653419495, -0.05101998895406723, -0.15772688388824463, 0.6174894571304321, 0.4489201009273529, 0.125007763504982, 0.808927059173584, 0.22092574834823608, -0.032135143876075745, -0.8201186060905457, -0.30603650212287903, 0.08561558276414871, -0.789104700088501, -0.5437667965888977, 0.3129250407218933, -0.5482323169708252, -0.32173070311546326, 0.011326175183057785, 0.3622024655342102, -0.36527660489082336, 0.33375442028045654, 1.1804258823394775, 0.43924573063850403, 0.6418537497520447, -0.0977536290884018, -1.0278534889221191, -0.867813766002655, -0.9815517663955688, -0.02399604208767414, -0.4538690447807312, -0.5414840579032898, -0.03039221279323101, -0.043150581419467926, -0.021642936393618584]}, "authors": [{"authorId": "2239666474", "name": "Wei Zhu"}, {"authorId": "2303472590", "name": "Aaron Xuxiang Tian"}, {"authorId": "2303643642", "name": "Congrui Yin"}, {"authorId": "2072724069", "name": "Yuan Ni"}, {"authorId": "2261365693", "name": "Xiaoling Wang"}, {"authorId": "2239568222", "name": "Guotong Xie"}], "references": [{"paperId": "989254f702db9bdc6f2424803ce1ba5a99949fab", "title": "NAT4AT: Using Non-Autoregressive Translation Makes Autoregressive Translation Faster and Better"}, {"paperId": "0f2fbed561e37da842c98633faf8fe1de9e2e174", "title": "Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey"}, {"paperId": "91e48f3d6836a8d3e842bc691e9759ad7fc9e395", "title": "Text2MDT: Extracting Medical Decision Trees from Medical Texts"}, {"paperId": "acd7adcdb914fb2e18a19792af47e59e78cab3f0", "title": "Overview of the PromptCBLUE Shared Task in CHIP2023"}, {"paperId": "ce2272a439ba89f1ea0a822c8d19078732d75e5d", "title": "PromptCBLUE: A Chinese Prompt Tuning Benchmark for the Medical Domain"}, {"paperId": "0d7f24578340aae6df610ed95aaa276b9c3ddcd3", "title": "VeRA: Vector-based Random Matrix Adaptation"}, {"paperId": "8ce219059d777c2333ee21cb2af2aad71275c98f", "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"}, {"paperId": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6", "title": "CMMLU: Measuring massive multitask language understanding in Chinese"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "623415074702068fdf0b6a4883fa8561e0c35c36", "title": "Multi-task entity linking with supervision from a taxonomy"}, {"paperId": "e6e5c6468466fd2ece4e7653f1b4cb4f965b458c", "title": "F-PABEE: Flexible-Patience-Based Early Exiting For Single-Label and Multi-Label Text Classification Tasks"}, {"paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0", "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"}, {"paperId": "6a2d96d2a7adde6349f15c1e680b67d114e7b67c", "title": "Unified Demonstration Retriever for In-Context Learning"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "873a581320d928249609d3c07229d5af182a379c", "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"}, {"paperId": "45fe20ec9a351e6f6d24316cdfef30a84062ce5b", "title": "Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation"}, {"paperId": "a981a57848e19adb80c4a29471fbb798ac050a8f", "title": "Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts"}, {"paperId": "50a260631a28bfed18eccf8ebfc75ff34917518f", "title": "Convolutional Bypasses Are Better Vision Transformer Adapters"}, {"paperId": "41129978a894dfc9726664444d6d0f7f468416cd", "title": "Sparse Structure Search for Parameter-Efficient Tuning"}, {"paperId": "960d40497717ad22a7ebb84db238fa2415fc89cc", "title": "LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "a7b92f9c62cdf35c37d9ccdb20e43807eb86900a", "title": "IDPG: An Instance-Dependent Prompt Generation Method"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "d05141dc0900140f7146bb71e1f7402cf896ea87", "title": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "f3a332ff1b73acda482e5d83696b2c701f487819", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "84310f76cf8909f87c6a7f2ed30ae28214cc9eab", "title": "LeeBERT: Learned Early Exit for BERT with cross-level optimization"}, {"paperId": "4690eb050572a279f94560b6bbdccaae577b45f5", "title": "MVP-BERT: Multi-Vocab Pre-training for Chinese BERT"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c", "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "87f759eb8c2a8d718b5d1e27ea28bf73c15a922c", "title": "Automatic Student Network Search for Knowledge Distillation"}, {"paperId": "e54ffc76d805c48660bb0fd20019ca82ac94ba0d", "title": "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "bdeec55f95fd6b73e3e4635459b14c7248543efb", "title": "AdapterDrop: On the Efficiency of Adapters in Transformers"}, {"paperId": "7a5d40ba5e1beec91bf45067102dbb0d7e6a420a", "title": "Mining Infrequent High-Quality Phrases from Domain-Specific Corpora"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "44928ee327335883eacce30d3baeb7dfdaa1fed2", "title": "AutoRC: Improving BERT Based Relation Classification Models via Architecture Search"}, {"paperId": "9b39de93ab9b5c74ed726d1ee8e31a927e4f6292", "title": "AutoTrans: Automating Transformer Design via Reinforced Architecture Search"}, {"paperId": "15e5f85bf12c901efea51dee91dcea5d96572c96", "title": "Medical Knowledge Graph to Enhance Fraud, Waste, and Abuse Detection on Claim Data: Model Development and Performance Evaluation"}, {"paperId": "056935031bc5cf0aeeaa0946320de26e14a1817e", "title": "Revisiting Few-sample BERT Fine-tuning"}, {"paperId": "98ef0db84e62aef969629264c9de1f4d0013f3b9", "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"}, {"paperId": "7fb301ea25f02dc7f4f7ee1360137503ee942c8c", "title": "Masking as an Efficient Alternative to Finetuning for Pretrained Language Models"}, {"paperId": "1120e7da9122f3158e5ff6e706b3e0b5d29dbc2e", "title": "Rational neural networks"}, {"paperId": "6fdd641ed7bb01f648ba7d56eefaf90e6d5a4ab9", "title": "Analysis of the Health Information Needs of Diabetics in China"}, {"paperId": "2bb4b6b876581c0c21ef58d3c38809715173ca7a", "title": "PANLP at MEDIQA 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation"}, {"paperId": "a613a3dc78bcd2b52826cab0646c74fd87f1a7d1", "title": "Pad\u00e9 Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks"}, {"paperId": "bc61e3b5612ebbf7363ecfb70f6dce433d6893ed", "title": "The Dr-KGQA System for Automatically Answering Medication Related Questions in Chinese"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "cbd569036fc72ae7ff747350b91816440282596b", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"}, {"paperId": "531a7f2c659787165df4fd5b4580590b953448e4", "title": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification"}, {"paperId": "5ef82a8c8aa50f99285f2143b57ca4e82da1af80", "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"}, {"paperId": "c136f3882f1d466a494e9c52d424b4197b16be04", "title": "Learned Adapters Are Better Than Manually Designed Adapters"}, {"paperId": "22effc238e65136003690f9a0cdcfb73b72b6f0f", "title": "SPT: Learning to Selectively Insert Prompts for Better Prompt Tuning"}, {"paperId": "4e6881ed38b637d377bfa54350a48f98db469b04", "title": "FastNER: Speeding up Inferences for Named Entity Recognition Tasks"}, {"paperId": "4c8cc2383cec93bd9ea0758692f01b98a035215b", "title": "UltraFeedback: Boosting Language Models with High-quality Feedback"}, {"paperId": "4243ba22834129c254543198b7f36d364a1a0eb2", "title": "BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting"}, {"paperId": "8bd3198940c205bad825c63b1950dfc2963a38e8", "title": "NAG-NER: a Unified Non-Autoregressive Generation Framework for Various NER Tasks"}, {"paperId": "4d63a23a299350d6e17d86c1c1b83687469194af", "title": "LECO: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": "ab461008fe8540054c7aa5f42ff55a9a39367704", "title": "PCEE-BERT: Accelerating BERT Inference via Patient and Confident Early Exiting"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "69d0224968a23f45230640f18c58719ac420ce01", "title": "Recurrent Rational Networks"}, {"paperId": "5d1132b0f6eee0cdc1d6e2539520c5c0c49bfa8a", "title": "AutoNLU: Architecture Search for Sentence and Cross-sentence Attention Modeling with Re-designed Search Space"}, {"paperId": "74512e17ba6e9617260bf9a65503ffa22bc66e3a", "title": "Global Attention Decoder for Chinese Spelling Error Correction"}, {"paperId": "fc5e25efe0a30230b553c817e38bc8308e127a08", "title": "paht_nlp @ MEDIQA 2021: Multi-grained Query Focused Multi-Answer Summarization"}, {"paperId": "6fbd2bcf27c3df92c824406888cd9b25ceff31e3", "title": "GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning"}, {"paperId": null, "title": "only inserts sequential adapters after the feed-forward module"}, {"paperId": null, "title": "Parameterized hypercomplex graph neural networks for graph classification"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "ac14aa5c8ba41037446ae04ace3fdcabf1ecd710", "title": "Pingan Smart Health and SJTU at COIN - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks"}, {"paperId": null, "title": "2023. GPT-4 Technical Report"}, {"paperId": null, "title": "2023. Stanford al-paca: An instruction-following llama model"}, {"paperId": null, "title": "2024. Tcmbench: A comprehensive benchmark for evaluating large language models in traditional chinese medicine"}, {"paperId": null, "title": "and achieve better performances"}, {"paperId": null, "title": "medical query understanding"}, {"paperId": null, "title": "2023. Adaptive rational activations to boost deep reinforcement learning"}, {"paperId": null, "title": "2023. QLoRA: Efficient Fine-tuning of Quantized LLMs"}, {"paperId": null, "title": "2024. A machine learning model for predicting acute exacerbation of in-home chronic obstructive pulmonary disease patients"}, {"paperId": null, "title": "2021b. Discovering better model architectures"}, {"paperId": null, "title": "2023. Punica: Multi-tenant lora serving"}, {"paperId": null, "title": "2023d. Acf: Aligned contrastive fine-tuning for language and vision tasks"}, {"paperId": null, "title": "2022. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models"}, {"paperId": null, "title": "2022. Adaptable adapters"}, {"paperId": null, "title": "2023a. Extracting decision trees from medical texts: An overview of the text2dt track in chip2022"}]}