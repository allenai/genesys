{"paperId": "e97480a8de7da6f0516080ba214c67542ab8189f", "title": "Deep Generative Models for Decision-Making and Control", "abstract": "Deep model-based reinforcement learning methods offer a conceptually simple approach to the decision-making and control problem: use learning for the purpose of estimating an approximate dynamics model, and offload the rest of the work to classical trajectory optimization. However, this combination has a number of empirical shortcomings, limiting the usefulness of model-based methods in practice. The dual purpose of this thesis is to study the reasons for these shortcomings and to propose solutions for the uncovered problems. Along the way, we highlight how inference techniques from the contemporary generative modeling toolbox, including beam search, classifier-guided sampling, and image inpainting, can be reinterpreted as viable planning strategies for reinforcement learning problems.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2306.08810", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "It is highlighted how inference techniques from the contemporary generative modeling toolbox, including beam search, classifier-guided sampling, and image inpainting, can be reinterpreted as viable planning strategies for reinforcement learning problems."}, "embedding": {"model": "specter_v2", "vector": [0.04438205808401108, 0.6351574659347534, -0.5487104654312134, 0.35181528329849243, -0.11797532439231873, 0.021659886464476585, 0.8366428017616272, -0.6222845315933228, 0.08753394335508347, -0.10569673776626587, 0.26386475563049316, 0.14451424777507782, -0.09691698104143143, -0.23733729124069214, -0.24075230956077576, -0.5730194449424744, -0.9439369440078735, 0.3721979856491089, -0.1020875796675682, -0.18751269578933716, 0.054341062903404236, -0.43892237544059753, -1.0501552820205688, -0.17374293506145477, -0.00512876408174634, 0.7373512387275696, 0.09969895333051682, 1.3991241455078125, 0.11691535264253616, 0.6715167164802551, 0.4681469798088074, -0.12193012982606888, 0.3680937886238098, -0.5147526860237122, -0.26635506749153137, 0.37913864850997925, -0.09098926186561584, -0.4796946048736572, -0.965718686580658, 0.5925773978233337, -0.10688896477222443, 0.12904943525791168, 0.708566427230835, -0.6075689196586609, 0.09950218349695206, 0.17072056233882904, 0.4172128140926361, 0.8481117486953735, 0.03211890533566475, -0.35163357853889465, 1.0221455097198486, -0.3098568618297577, 0.6308427453041077, 1.7478034496307373, 0.1942051202058792, 0.4379165768623352, -0.6461628079414368, -0.38635650277137756, 0.8894039988517761, 0.26441043615341187, -0.5031572580337524, 0.2144770622253418, -0.04975989833474159, -0.5642927885055542, 0.9641229510307312, -0.5532057881355286, 0.6996088624000549, 1.1596187353134155, 0.62294602394104, 1.706218957901001, 0.1629413515329361, -0.9539018273353577, 0.3245598077774048, 0.228509321808815, -0.24738435447216034, 0.9182339906692505, -0.7520599961280823, 1.1795815229415894, -0.9029750823974609, 0.10088767856359482, 0.5226380228996277, -0.34785711765289307, 0.11744958907365799, -0.6065441966056824, 0.10206222534179688, 1.127651333808899, 0.23921363055706024, 0.2696903347969055, -0.007754408288747072, 1.2163931131362915, 0.06622777879238129, 0.042620301246643066, -0.3459659218788147, 0.04368605092167854, 0.6498083472251892, 0.3107564151287079, -0.16290490329265594, 0.4601915180683136, -0.0345127135515213, 0.8387861847877502, -0.2531203031539917, 0.3841571807861328, -0.9037618637084961, 0.010347665287554264, 1.562282919883728, 0.06193177029490471, 0.4368017613887787, -1.031612515449524, 0.3075063228607178, -1.1348726749420166, 0.6319038271903992, -0.47263601422309875, -0.020317211747169495, -0.6388141512870789, -0.539150595664978, -0.3726758062839508, -0.025490596890449524, -0.03266436606645584, -0.9341380596160889, 0.49465951323509216, -0.02009965106844902, -0.1755485236644745, -0.15854206681251526, 0.7434158325195312, -0.7854504585266113, 0.6156609654426575, -0.03981783986091614, 0.18816635012626648, 0.427061527967453, -0.47049012780189514, -0.7160351872444153, -0.7720285654067993, -0.06378165632486343, 0.1995983123779297, 0.2943210303783417, 0.21327857673168182, -1.2627956867218018, -1.3106706142425537, -1.080444574356079, 0.5059805512428284, -0.0006590320845134556, 0.3401622772216797, 1.3183740377426147, 0.3191840350627899, -0.504288911819458, 1.0353047847747803, -0.5789428949356079, 0.014943274669349194, 0.7122184634208679, -0.0970071330666542, 0.7036190032958984, -0.3237074315547943, -0.5724058747291565, 0.35570886731147766, 0.06932875514030457, -0.6220742464065552, -0.8968257904052734, -0.29445356130599976, -0.7092629671096802, -0.37421631813049316, 0.19195424020290375, -0.7574768662452698, 1.3494895696640015, 0.2850942313671112, -1.515108585357666, -0.10373947024345398, 0.16690640151500702, -0.1968420147895813, 0.6992455124855042, -0.15853050351142883, -0.006551831960678101, -0.15311582386493683, -0.3007623851299286, 0.6753585338592529, 0.734398365020752, -0.5396263003349304, -0.551950991153717, -0.31301724910736084, -0.25756406784057617, -0.23026320338249207, 0.29996803402900696, 0.3421795070171356, -0.41303902864456177, -0.4085385203361511, 0.1591811180114746, 0.2929038405418396, -0.6429377794265747, -0.3416052460670471, -0.5112724900245667, -0.9467398524284363, 0.6662694811820984, 0.12650485336780548, 0.6700806617736816, -0.170596644282341, -0.4343385696411133, -0.01935719884932041, -0.532224178314209, -0.5905100703239441, -0.4662034213542938, 0.6352188587188721, -0.02663183957338333, -0.12927553057670593, -0.07407989352941513, -0.840893566608429, 0.20957069098949432, 0.12706606090068817, -0.9803629517555237, 0.1298508495092392, 0.2518303692340851, 0.7815895080566406, -0.9631825089454651, 0.007538286969065666, 0.06850278377532959, 0.3476928472518921, -0.7266780138015747, 1.0839530229568481, -0.7528565526008606, 0.8023860454559326, -0.6727641820907593, -0.4711296856403351, -0.13607364892959595, 0.000691244553308934, 0.39552879333496094, 0.15469594299793243, 0.3543628454208374, 0.04838879033923149, -0.4824201166629791, 1.640176773071289, -0.3040336072444916, 0.693799614906311, -0.19487303495407104, -0.8823971152305603, 0.14980417490005493, 0.03354911506175995, -0.2661122679710388, -0.41152599453926086, 0.23692135512828827, -0.3563072383403778, -0.5214844346046448, -0.09221428632736206, 0.06921373307704926, 0.5260782241821289, -0.19960013031959534, 0.3866933584213257, 0.7091788649559021, -0.7550716996192932, 0.6724472641944885, 0.18629539012908936, 0.7328687310218811, 1.2525209188461304, -0.16877365112304688, 0.14820218086242676, -0.25053152441978455, -0.8478882312774658, -0.09804381430149078, 0.7694722414016724, 0.7309662103652954, 1.0433779954910278, 0.5381596684455872, -1.0471078157424927, -0.356607049703598, -0.39638376235961914, 0.8348764777183533, 0.7093965411186218, -0.0912623181939125, -0.1826094537973404, -0.9482815861701965, -0.3187103867530823, -0.14747264981269836, 0.6048434972763062, -1.0364223718643188, -0.008208250626921654, -0.5508893728256226, -1.0277196168899536, 0.3229029178619385, 0.4781496524810791, 0.8556198477745056, -0.44779956340789795, -0.3649916648864746, 0.28243356943130493, 0.7135435938835144, -0.34821635484695435, -0.41762980818748474, 0.467285692691803, -0.2936009168624878, -0.24558579921722412, -0.06410518288612366, 0.47254979610443115, 0.09758183360099792, -1.0840609073638916, 0.7239095568656921, -0.5576404333114624, -0.6528987288475037, 0.3939853608608246, 0.670407235622406, -1.1101250648498535, -0.6315107345581055, 0.07674040645360947, -0.3041989207267761, -0.14047349989414215, -0.6508000493049622, 0.28667908906936646, -0.03268685191869736, -0.052356310188770294, -0.14473232626914978, -0.06374909728765488, 0.044927697628736496, 0.19486074149608612, 0.6709910035133362, -0.01456320658326149, -0.1670788675546646, -0.6215738654136658, 1.0676801204681396, 0.280947208404541, -0.880414605140686, -0.05060366913676262, -0.8377124667167664, -0.10473807901144028, 0.26225367188453674, -0.6652964949607849, -0.6274933218955994, -0.42693138122558594, 0.14580215513706207, -0.5846202373504639, -0.4798884391784668, -0.30589792132377625, 0.7158225774765015, -0.029139291495084763, 0.374137818813324, 0.33202946186065674, 0.5507403612136841, 0.18939393758773804, 0.476825475692749, -1.505170226097107, 0.40466514229774475, 0.02734932117164135, 0.14591479301452637, 0.13365663588047028, 0.27899637818336487, -0.36251601576805115, -0.7068595290184021, 0.34438300132751465, -0.3112350404262543, -1.0119906663894653, 0.5487760901451111, -0.2521545886993408, -0.9313957691192627, -0.24950538575649261, -0.619693398475647, -0.6461089849472046, -0.17901542782783508, -0.2546755075454712, -1.082639217376709, -1.3494936227798462, -1.0063014030456543, -1.0728485584259033, 0.07689623534679413, -1.2485878467559814, 0.2678108811378479, 0.3416691720485687, -0.3536475896835327, -0.47502008080482483, 0.39991408586502075, -0.13512027263641357, 0.5526769161224365, -0.4204283654689789, 0.7456861734390259, 0.8186416029930115, -0.8804174661636353, -0.0437130481004715, 0.998593270778656, -0.022198719903826714, -0.6021493077278137, 0.33457890152931213, -0.7128852009773254, 0.14715461432933807, -0.6520407795906067, -0.7308759093284607, 0.17376314103603363, 0.47673678398132324, 0.9401238560676575, -0.04726936295628548, -0.5315378904342651, 0.05180935934185982, 1.3384284973144531, -0.08771123737096786, 0.14308395981788635, 0.16394129395484924, 0.5832011699676514, 0.45903801918029785, 0.2710146903991699, 0.7986404299736023, 0.13157576322555542, 0.8722782731056213, 0.36601266264915466, 0.265122652053833, -0.21694839000701904, -1.2777786254882812, 0.20469294488430023, 0.27242058515548706, 0.33511656522750854, -0.13733194768428802, -0.7077297568321228, 0.24450914561748505, -1.5030275583267212, -0.8893585205078125, 0.9549293518066406, 0.5387970209121704, -0.07709591090679169, 0.0834551602602005, 0.17686359584331512, 0.015774928033351898, 0.3262128531932831, 0.15876488387584686, -0.4944884777069092, -0.2716573178768158, 0.48689496517181396, 0.47238415479660034, -0.10534602403640747, 0.9066155552864075, -0.3573099374771118, 0.16641013324260712, 14.961322784423828, 1.006626844406128, 0.15109319984912872, 0.16147971153259277, 0.7747213840484619, 0.19139167666435242, -0.3551555275917053, -0.024329673498868942, -1.2345225811004639, -0.018427828326821327, 0.7736069560050964, 0.6545354127883911, 0.9449756145477295, 0.04675494134426117, 0.32467952370643616, -0.4598656892776489, -0.3502627909183502, 0.4711259603500366, 0.1530788391828537, -1.690162181854248, 0.10959217697381973, 0.2978750467300415, 0.32094746828079224, 0.20858483016490936, 0.8552175760269165, 0.8030805587768555, 0.8341240286827087, -0.03333814814686775, 1.1653670072555542, 0.6226024627685547, 0.3259172737598419, -0.43208029866218567, -0.056888215243816376, 0.8287585377693176, -0.5866970419883728, -0.4885675609111786, -0.39327123761177063, -0.911992609500885, 0.3192594349384308, -0.24591751396656036, -0.730729877948761, -0.5517236590385437, 0.07768604159355164, 0.10032287240028381, 0.19489674270153046, 0.15672685205936432, -0.6136266589164734, 0.716705858707428, -0.19745923578739166, -0.13786648213863373, 0.2685016095638275, 0.04493900015950203, 0.15798664093017578, -0.3118566870689392, -0.11300681531429291, 0.12900158762931824, 0.3631937503814697, 0.719118595123291, 0.049145616590976715, -0.8898959755897522, -0.4024045169353485, 0.06305709481239319, -0.15464802086353302, 0.5849189758300781, 0.24143339693546295, 0.633463978767395, 0.4983154535293579, -0.05365162342786789, 0.5239244699478149, 0.2408076524734497, -0.17995363473892212, 0.2289574146270752, 0.42916250228881836, -0.524735152721405, 0.43786725401878357, 0.3099762201309204, -0.13631832599639893, -0.12944652140140533, -1.0608881711959839, -0.23297959566116333, 0.19653595983982086, -1.067421793937683, -0.5039606690406799, 0.6300950646400452, 0.013764225877821445, -0.13455446064472198, -0.20767897367477417, -0.23548266291618347, -0.5002985596656799, 0.07135281711816788, -0.8355367183685303, -0.5563645958900452, 0.4527360498905182, -0.3268882632255554, 0.03359832987189293, 0.08310247212648392, 0.6999306082725525, -0.583401620388031, -0.7702848315238953, -0.24782535433769226, 0.2123115360736847, -0.40119636058807373, -0.6262291073799133, -1.1857576370239258, 0.7493200302124023, -0.014113996177911758, 0.31353577971458435, -0.3470260202884674, 0.4647219181060791, 0.37209150195121765, -0.7452651858329773, 0.15217192471027374, -0.4533563256263733, -1.0102870464324951, -0.5265586376190186, -0.7469238042831421, -0.34812653064727783, -0.017041612416505814, 0.5916239619255066, -0.14967958629131317, -0.07983464002609253, -0.40962958335876465, 0.03064046800136566, -0.3229096531867981, -0.6823951005935669, -0.031240390613675117, 0.0855097770690918, -0.18707452714443207, 0.12787368893623352, 0.014791379682719707, -0.23751987516880035, -1.1504706144332886, -0.14930152893066406, -0.5352770686149597, 0.42169517278671265, -0.5465061068534851, 0.9268626570701599, -0.5616204738616943, 0.43187570571899414, 0.544078528881073, 0.09691350162029266, -1.1615957021713257, -0.46376875042915344, -1.2098069190979004, 0.07227975875139236, 0.3081313371658325, 0.1830340176820755, -0.11210174858570099, 0.9591561555862427, 0.39745381474494934, 0.5342472791671753, -0.5967713594436646, -0.5450959801673889, -0.027755113318562508, 0.10830394178628922, -0.6882787346839905, 0.018551237881183624, -0.8171373605728149, -0.5905494689941406, 0.05191029980778694, 0.42960816621780396, 0.7514374852180481, 0.24526283144950867, -0.3711336851119995, 0.5619068741798401, 0.35601988434791565, -0.39129123091697693, -0.37166619300842285, -0.40499022603034973, -1.4348585605621338, -0.2196734994649887, -0.5857896208763123, 0.09305893629789352, -1.2065726518630981, -0.6439034938812256, -0.23766224086284637, 0.22711633145809174, -0.46731388568878174, 0.7162493467330933, -0.5907018184661865, -0.13755986094474792, -0.5397459268569946, -0.3317907452583313, 1.067492961883545, 1.3120290040969849, -1.054990530014038, 0.42256247997283936, 0.4372131824493408, 0.2648945450782776, 0.6273312568664551, 0.5577569603919983, -0.28758683800697327, -1.1638041734695435, -0.886690616607666, 0.1809939593076706, 0.18928462266921997, -0.031476739794015884, -1.3209905624389648, 0.31324365735054016, -0.03370697423815727, 0.730988085269928, -0.20925526320934296, 0.5806467533111572, -0.632184624671936, -0.1492847204208374, 0.33043530583381653, -1.0498069524765015, -0.44009634852409363, -0.10321547836065292, 0.4011470079421997, 0.04857510328292847, 0.5465501546859741, 0.3866850733757019, -1.0858744382858276, -0.3544602394104004, 0.37648382782936096, -0.9309491515159607, 0.1436585783958435, 0.047299906611442566, -0.1850174218416214, -0.47348183393478394, -0.7734197974205017, -0.3754739761352539, 0.17191234230995178, -0.42066189646720886, 0.9102857708930969, 0.3761149048805237, -1.1673831939697266, 0.3241806924343109, 0.20061437785625458, -0.29726526141166687, 0.28642401099205017, 0.4677693247795105, 0.031306248158216476, -0.14997608959674835, 0.5346077680587769, -0.06529034674167633, 0.08108032494783401, -0.3021644353866577, -0.07704101502895355, 0.9980380535125732, -0.5853059887886047, -0.08861502259969711, 1.0046499967575073, -0.222663015127182, -0.728901743888855, 0.5394265651702881, -0.9153748154640198, -0.5645374059677124, -0.2934226989746094, 0.41747739911079407, 0.44902774691581726, -0.35654014348983765, 0.5164453983306885, 0.021311653777956963, 0.4578346312046051, -0.0825478658080101, -0.9205402731895447, 0.3737633228302002, -0.5291532278060913, 0.5688514113426208, 0.6083852052688599, 0.3437187969684601, -0.859586238861084, -1.7858105897903442, 0.046237509697675705, -0.6331562995910645, -0.5238189697265625, 0.3512631356716156, -0.6752442121505737, -0.8099856972694397, 0.9941977858543396, 1.1779147386550903, 0.028338773176074028, -0.013469004072248936, 0.20856697857379913, -0.14613988995552063, 0.36348673701286316, 0.4887200891971588, -0.7555038332939148, 0.0885029062628746, 0.6877714395523071, 1.6517592668533325, -0.42234310507774353, 0.5059302449226379, -0.05980423092842102, -0.831440806388855, 1.031174898147583, 0.5888151526451111, -0.35330480337142944, 0.79358971118927, -0.1750580221414566, -0.04540788009762764, 0.12786072492599487, -1.0502090454101562, -0.33865660429000854, 0.5381128787994385, 1.6119188070297241, 0.018009282648563385, 0.2045663595199585, 0.7906502485275269, 0.5749159455299377, 0.1614546924829483, 0.20448555052280426, 0.5399360656738281, 0.7927899360656738, -0.04762127250432968, -0.24629683792591095, -0.3091072142124176, 0.5001523494720459, -0.46260523796081543, 0.5233001708984375, 0.3175736665725708, 0.9042597413063049, 0.5102328658103943, 0.40788429975509644, 0.7935824394226074, -0.24115262925624847, 0.3937608301639557, -0.4769022464752197, 1.4490283727645874, -0.10898275673389435, 0.04995696246623993, 0.1741078495979309, -0.5855504870414734, -0.3311972916126251, 0.03365331143140793, -0.9658929109573364, 0.003051907056942582, 0.37156346440315247, 0.35458073019981384, -0.15275472402572632, 0.07815097272396088, 0.7987520694732666, 0.5271755456924438, 0.445664644241333, 0.008637395687401295, -1.02643620967865, -0.6201701760292053, -0.8393316864967346, 0.16036356985569, -0.9449036717414856, -0.17337490618228912, -0.8410167098045349, -0.06818995624780655, -0.1643534004688263]}, "authors": [{"authorId": "35163402", "name": "Michael Janner"}], "references": [{"paperId": "2fec20377bc947ec1df003b4aedcb4d7f25ac934", "title": "TransDreamer: Reinforcement Learning with Transformer World Models"}, {"paperId": "c271b4d25bc184bc94622cef6c9aba80e8e2cce3", "title": "DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization"}, {"paperId": "348a855fe01f3f4273bf0ecf851ca688686dbfcc", "title": "Offline Reinforcement Learning with Implicit Q-Learning"}, {"paperId": "5a8b4d43cdf73ef7218dfa5cb305064a4f55b8b5", "title": "Mismatched No More: Joint Model-Policy Optimization for Model-Based RL"}, {"paperId": "f8b94b373c8e402a07d848c76eaee6e5842a862f", "title": "3D Neural Scene Representations for Visuomotor Control"}, {"paperId": "91b32fc0a23f0af53229fceaae9cce43a0406d2e", "title": "Structured Denoising Diffusion Models in Discrete State-Spaces"}, {"paperId": "685ccd3d1ce5afc0dbf06d3f34624dc069f3e044", "title": "Model-Based Reinforcement Learning via Latent-Space Collocation"}, {"paperId": "db6965ab7ae91e7c81d5dc606861159f83bfd3a4", "title": "Vector Quantized Models for Planning"}, {"paperId": "f864d4d2267abba15eb43db54f58286aef78292b", "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794", "title": "Diffusion Models Beat GANs on Image Synthesis"}, {"paperId": "7e687699b6c2e075091cebe4b7b8dbd4dc3a7406", "title": "Autoregressive Dynamics Models for Offline Policy Evaluation and Optimization"}, {"paperId": "8762a085024dbc3de27999b5a75b8b9f82cafca0", "title": "3D Shape Generation and Completion through Point-Voxel Diffusion"}, {"paperId": "cd37fee4da0d4483322d6fa3cc67af9ed8c07be6", "title": "Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation"}, {"paperId": "169fc06bbdadf008cb72a80762b5050bc5756ee4", "title": "Offline Reinforcement Learning with Pseudometric Learning"}, {"paperId": "de18baa4964804cf471d85a5a090498242d2e79f", "title": "Improved Denoising Diffusion Probabilistic Models"}, {"paperId": "0f492be75b051168d96450ad3b7d21bb23ab0883", "title": "Near-Optimal Offline Reinforcement Learning via Double Variance Reduction"}, {"paperId": "9f8a70e9188a913317e97ba1874c8f763fd13c04", "title": "Is Pessimism Provably Efficient for Offline RL?"}, {"paperId": "0bf1a78aeefa158a80b23fd5b57a5586a32eb7c1", "title": "Planning from Pixels using Inverse Dynamics Models"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "5d5805b54cc933f7cef3c9c5e7c3d287234ad601", "title": "The Value Equivalence Principle for Model-Based Reinforcement Learning"}, {"paperId": "b85a70c16a1f961b42356469618faf279cdc98f1", "title": "\u03b3-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction"}, {"paperId": "014576b866078524286802b1d0e18628520aa886", "title": "Denoising Diffusion Implicit Models"}, {"paperId": "05cfd96eed27ceb2aa35285991a745a5cd119abc", "title": "If Beam Search Is the Answer, What Was the Question?"}, {"paperId": "b44bb1762640ed72091fd5f5fdc20719a6dc24af", "title": "Mastering Atari with Discrete World Models"}, {"paperId": "685af6d2bcdff7170574643b2c5ab4fbcc36f597", "title": "WaveGrad: Estimating Gradients for Waveform Generation"}, {"paperId": "5e06f64e91f4f48665c7407ea6eac74f93d5d0d0", "title": "On the model-based stochastic value gradient for continuous reinforcement learning"}, {"paperId": "3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d", "title": "Model-Based Offline Planning"}, {"paperId": "fa7f88f77de02ae9389e514a1cd13083a624ec78", "title": "EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL"}, {"paperId": "e48ee7f99031c42a8ada0a3252eda97e0dd697f1", "title": "Model-based Reinforcement Learning for Semi-Markov Decision Processes with Neural ODEs"}, {"paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a", "title": "Denoising Diffusion Probabilistic Models"}, {"paperId": "024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb", "title": "Array programming with NumPy"}, {"paperId": "0272b14dd471fe7b81df703af1b71d7600b77215", "title": "Accelerating Online Reinforcement Learning with Offline Datasets"}, {"paperId": "28db20a81eec74a50204686c3cf796c42a020d2e", "title": "Conservative Q-Learning for Offline Reinforcement Learning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "dea0f1c5949f8d898b9b6ff68226a781558e413c", "title": "MOPO: Model-based Offline Policy Optimization"}, {"paperId": "309c2c5ee60e725244da09180f913cd8d4b8d4e9", "title": "MOReL : Model-Based Offline Reinforcement Learning"}, {"paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4", "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"}, {"paperId": "561408d38f4d16a3156a06c3c83e1fcabc2ae07c", "title": "Adaptive Transformers in RL"}, {"paperId": "87fe27076c1dd1ef264f03e4fd27ed850c0b537f", "title": "Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "831e7cbafed2dca05db1e7f5ef16d1a7614f44ec", "title": "Learning to Reach Goals via Iterated Supervised Learning"}, {"paperId": "7a7a7847041e7b25febb1491d65d842a6c65927e", "title": "Training Agents using Upside-Down Reinforcement Learning"}, {"paperId": "72973e49f453f678eb0b79b5fa5311b158f3909d", "title": "Reinforcement Learning Upside Down: Don't Predict Rewards - Just Map Them to Actions"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "3507bd62a14bd0e8ead28cdedb1c33ba83c39c6b", "title": "Mastering Atari, Go, chess and shogi by planning with a learned model"}, {"paperId": "9eba601720dc5d9070875468bfd287a646d191f2", "title": "On the Expressivity of Neural Networks for Deep Reinforcement Learning"}, {"paperId": "59a916cdc943f0282908e6f3fa0360f4c5fb78d0", "title": "Stabilizing Transformers for Reinforcement Learning"}, {"paperId": "9be492858863c8c7c24be1ecb75724de5086bd8e", "title": "Behavior Regularized Offline Reinforcement Learning"}, {"paperId": "7a450675968d31b8363e21fb5d5b72474c128076", "title": "Deep Dynamics Models for Learning Dexterous Manipulation"}, {"paperId": "39c7ae0e161742a1184e7e8ebc2e65020fd97904", "title": "Model Based Planning with Energy Based Models"}, {"paperId": "3a5f83be8f7c1ba567d0ca4ed4b1c1e6838f8be5", "title": "Mish: A Self Regularized Non-Monotonic Neural Activation Function"}, {"paperId": "965359b3008ab50dd04e171551220ec0e7f83aba", "title": "Generative Modeling by Estimating Gradients of the Data Distribution"}, {"paperId": "4ee70fb32981f84f9dddc57bd59a69e677c91759", "title": "Benchmarking Model-Based Reinforcement Learning"}, {"paperId": "db1614b97aec1e0ead9554a038aa0f8dd9a26e30", "title": "Exploring Model-based Planning with Policy Networks"}, {"paperId": "9001698e033524864d4d45f051a5ba362d4afd9e", "title": "When to Trust Your Model: Model-Based Policy Optimization"}, {"paperId": "c2c8482c713b94073f3d59895b373db4398ddfbb", "title": "Language as an Abstraction for Hierarchical Deep Reinforcement Learning"}, {"paperId": "55203cd25f9c03d7c6b691ba84e95bb82df0bc6f", "title": "Fast Task Inference with Variational Intrinsic Successor Features"}, {"paperId": "22ade45a75c1ce8ae63feae8d5381316493cefe8", "title": "When to use parametric models in reinforcement learning?"}, {"paperId": "1eeb265595e250cf66751ef9032524386d7a9b32", "title": "Neural Spline Flows"}, {"paperId": "82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd", "title": "Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction"}, {"paperId": "069d18aefd5cc0eef1b3f0403ebcec188b1d2c1f", "title": "Combating the Compounding-Error Problem with a Multi-step Model"}, {"paperId": "6faa9f1359cdcfcb8b502a0288d65d98aa229980", "title": "Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model"}, {"paperId": "417edb826b9dbf8e142e9358b78da70b0bbc7177", "title": "Structured agents for physical construction"}, {"paperId": "1fd4694e7c2d9c872a427d50e81b5475056de6bc", "title": "Model-Based Reinforcement Learning for Atari"}, {"paperId": "132b07740db20df2c36d6f939d296a7e941feac7", "title": "Insertion-based Decoding with Automatically Inferred Generation Order"}, {"paperId": "5285cb8faada5de8a92a47622950f6cfd476ac1d", "title": "Off-Policy Deep Reinforcement Learning without Exploration"}, {"paperId": "fea3e63c97c7292dc6fbcb3ffe7131eb54053986", "title": "Learning Latent Dynamics for Planning from Pixels"}, {"paperId": "68df76350dffba2e5f5f965df57c3747c66bb4d0", "title": "Differentiable MPC for End-to-end Planning and Control"}, {"paperId": "9b114351bf97a66ab8cfef4c3b04a0f70503295e", "title": "Deep Imitative Models for Flexible Inference, Planning, and Control"}, {"paperId": "d82829c9fd9cfba8a44efe5ba048d3332a1671fc", "title": "Dynamic Programming"}, {"paperId": "c4e02b75e525c4e8c4616d2e9dfeb4a638142c51", "title": "Modeling the Long Term Future in Model-Based Reinforcement Learning"}, {"paperId": "41cca0b0a27ba363ca56e7033569aeb1922b0ac9", "title": "Recurrent World Models Facilitate Policy Evolution"}, {"paperId": "2b0a37ee3671524c1956f697466522f7d69e04fe", "title": "The Successor Representation: Its Computational Logic and Neural Substrates"}, {"paperId": "d333f99881b09426283a9c7a1d25f7ac30d63062", "title": "Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees"}, {"paperId": "08555bf7d6a483f783b7508ed2df5b1a4d29661c", "title": "Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion"}, {"paperId": "f650f1fd44ab0778d30577f8c2077b2ff58830da", "title": "Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement"}, {"paperId": "0f710daa7bbba3350169f0bbb5d24f8db3e5199e", "title": "Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings"}, {"paperId": "43879cf527f4918955fd55128baa6745174d8555", "title": "Graph networks as learnable physics engines for inference and control"}, {"paperId": "56136aa0b2c347cbcf3d50821f310c4253155026", "title": "Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"}, {"paperId": "6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba", "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review"}, {"paperId": "d08b35243edc5be07387a9ed218070b31e502901", "title": "Group Normalization"}, {"paperId": "2cdf553c4d3e68b9189217cc1f2d6b90e3f6eb7e", "title": "Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning"}, {"paperId": "394e3995d2faf542104cb7e6ea43cec204253273", "title": "PDDLStream: Integrating Symbolic Planners and Blackbox Samplers via Optimistic Adaptive Planning"}, {"paperId": "852c931b5d9f9d4256befd725ee4185945c4964c", "title": "Temporal Difference Models: Model-Free Deep RL for Model-Based Control"}, {"paperId": "27dfecb6bb0308c7484e13dcaefd5eeebba677d3", "title": "Model-Ensemble Trust-Region Policy Optimization"}, {"paperId": "ec252c3e65d4e0e5bd871a7ee78338ec5e91eca3", "title": "Universal Successor Representations for Transfer Reinforcement Learning"}, {"paperId": "811df72e210e20de99719539505da54762a11c6d", "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"}, {"paperId": "ba1f38d6bbbf7227cda93f3915bc3fa7fc37b58e", "title": "An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation"}, {"paperId": "1fa1f04b80f057e477549e6b9798fab7c7e57db5", "title": "Hindsight policy gradients"}, {"paperId": "6e745266a5c85980e75f9d637d4d23cfc030cfaf", "title": "Uncertainty-driven Imagination for Continuous Deep Reinforcement Learning"}, {"paperId": "c9141c5a1cb0a0f457fb7b34d21e154f2c336baf", "title": "Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation"}, {"paperId": "cce22bf6405042a965a86557684c46a441f2a736", "title": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "9917363277c783a01bff32af1c27fc9b373ad55d", "title": "DeepLoco"}, {"paperId": "cf020b27d06efb28f3e5db264aceeec1f397817b", "title": "Value Prediction Network"}, {"paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2", "title": "Hindsight Experience Replay"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1a995c3bce73629a14fdeb827d7bc4be9b11c0a0", "title": "Value-Aware Loss Function for Model-based Reinforcement Learning"}, {"paperId": "bb430ec2f25e4a1513073a2a4098cbb942c2e3e0", "title": "Recurrent Environment Simulators"}, {"paperId": "39b19ea254b0952f2abd23ad899420749816bb1d", "title": "The Predictron: End-To-End Learning and Planning"}, {"paperId": "74ff6d48f9c62e937023106629d27ef2d2ddf8bc", "title": "Least Squares Generative Adversarial Networks"}, {"paperId": "54ddb00fa691728944fd8becea90a373d21597cf", "title": "Understanding deep learning requires rethinking generalization"}, {"paperId": "e73336a361e853c145d99745ddebcf63038fcd09", "title": "The successor representation in human reinforcement learning"}, {"paperId": "d8686b657b61a37da351af2952aabd8b281de408", "title": "Successor Features for Transfer in Reinforcement Learning"}, {"paperId": "10a4992ece5baea79326a8878a6244eeacbc6af5", "title": "Deep Successor Reinforcement Learning"}, {"paperId": "ffdcad14d2f6a12f607b59f88da4a939f4821691", "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization"}, {"paperId": "5129a9cbb6de3c6579f6a7d974394d392ac29829", "title": "Control of Memory, Active Perception, and Action in Minecraft"}, {"paperId": "84680b30a20775e5d319419a7f3f2a93e57c2a61", "title": "Value Iteration Networks"}, {"paperId": "bcfe915d5983dffbfe95801e9e6757205b3a4723", "title": "Memory-based control with recurrent neural networks"}, {"paperId": "6640f4e4beae786f301928d82a9f8eb037aa6935", "title": "Learning Continuous Control Policies by Stochastic Value Gradients"}, {"paperId": "3177334d5ef8e0ece30913b4692b86801f0845c5", "title": "Model Predictive Path Integral Control using Covariance Variable Importance Sampling"}, {"paperId": "5dc2a215bd7cd5bdd3a0baa8c967575632696fac", "title": "Universal Value Function Approximators"}, {"paperId": "0f899b92b7fb03b609fee887e4b6f3b633eaf30d", "title": "Variational Inference with Normalizing Flows"}, {"paperId": "2dcef55a07f8607a819c21fe84131ea269cc2e3c", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"}, {"paperId": "340f48901f72278f6bf78a04ee5b01df208cc508", "title": "Human-level control through deep reinforcement learning"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "bee044c8e8903fb67523c1f8c105ab4718600cdb", "title": "Explaining and Harnessing Adversarial Examples"}, {"paperId": "dcc81dcd6faed4597409c2fe086bdbb687fe82d5", "title": "Learning of closed-loop motion control"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "e48e36bd90ba237bda05ac2f5959990c2f69a0c6", "title": "Approximate model-assisted Neural Fitted Q-Iteration"}, {"paperId": "48aaaa1e59c58856315d814363f431b93f76e668", "title": "Model Regularization for Stable Sample Rollouts"}, {"paperId": "13bc4e683075bdd6a3f0155241c276a772d4aa06", "title": "Generative adversarial networks"}, {"paperId": "740a11cdea33bd3d00e64895a15ca4e73d60afb0", "title": "A direct method for trajectory optimization of rigid bodies through contact"}, {"paperId": "6b6b078ee9aabf3c17220a2da1e3f0dd822956b7", "title": "Chapter 3 \u2013 The Cross-Entropy Method for Optimization"}, {"paperId": "244539f454800697ed663326b7cfba337ca0c2ec", "title": "Guided Policy Search"}, {"paperId": "71b552b2e058d5a6a760ba203f10f13be759edd3", "title": "Synthesis and stabilization of complex behaviors through online trajectory optimization"}, {"paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1", "title": "MuJoCo: A physics engine for model-based control"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a", "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search"}, {"paperId": "9b7ae896675c71ac50fa1fbc555cb19f80863f0e", "title": "Hierarchical task and motion planning in the now"}, {"paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e", "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"}, {"paperId": "70e10a5459c6f1aaf346ee4f2dcc837151fbe75c", "title": "Efficient Reductions for Imitation Learning"}, {"paperId": "4dcb5efefcff6ea05a041771a8f13d643b5ca8d2", "title": "Sample-based learning and search with permanent and transient memories"}, {"paperId": "69db71aee7561de291e622c30e9b2b46e6989ad3", "title": "Reinforcement Learning by Value Gradients"}, {"paperId": "e635d81a617d1239232a9c9a11a196c53dab8240", "title": "Bandit Based Monte-Carlo Planning"}, {"paperId": "9966e890f2eedb4577e11b9d5a66380a4d9341fe", "title": "Estimation of Non-Normalized Statistical Models by Score Matching"}, {"paperId": "af5e25f7e42ace6c6c04b2d0ccfbd2b15085b10a", "title": "Structure in the Space of Value Functions"}, {"paperId": "3b90b73fa0f904a2dc84bca4b3f80cbb51d7025f", "title": "Reinforcement Learning with Long Short-Term Memory"}, {"paperId": "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d", "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "7ca8ac34767d6e6cb389eeebcdabc4225b39edfe", "title": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding"}, {"paperId": "3b5db92ce2f86b2136fe7cf6a415fe1c0632a881", "title": "TD Models: Modeling the World at a Mixture of Time Scales"}, {"paperId": "c2e8806f0bd1d504bcb395ef1f6fe509a023a048", "title": "Improving Generalization for Temporal Difference Learning: The Successor Representation"}, {"paperId": "5c3ff6424d564e004ccf1440a7d18fa93509132e", "title": "Forward Models: Supervised Learning with a Distal Teacher"}, {"paperId": "f2e4c67cf0a1485b28e8a029054d0257ddce6b56", "title": "Neural networks for self-learning control systems"}, {"paperId": "ce9a21b93ba29d4145a8ef6bf401e77f261848de", "title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"}, {"paperId": "a91635f8d0e7fb804efd1c38d9c24ee952ba7076", "title": "Learning to predict by the methods of temporal differences"}, {"paperId": null, "title": "Flax: A neural network library and ecosystem for JAX"}, {"paperId": "8a85ef6a7ebcd8735b868bf9c4a77e6a3c195caa", "title": "Einops: Clear and Reliable Tensor Manipulations with Einstein-like Notation"}, {"paperId": null, "title": "Diffusion models are autoencoders, 2022"}, {"paperId": "0fc5a4f52a53f7d7809b7782a2aeb96da5ec6fd1", "title": "Compositional Visual Generation with Energy Based Models"}, {"paperId": null, "title": "2020a). B.4 Datasets The D4RL dataset (Fu et al., 2020) used in our experiments is under the Creative Commons Attribution 4.0 License (CC BY)"}, {"paperId": null, "title": "minGPT: A minimal pytorch re-implementation of the openai gpt training, 2020"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "a0d540e15e4377e20e8490758eace078e92a6a1b", "title": "Implicit Generation and Generalization with Energy Based Models"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": "6bf3879103ff55081c462e4f1c8e612c7ab9a75a", "title": "Supplementary Materials for : \u201c f-GAN : Training Generative Neural Samplers using Variational Divergence Minimization \u201d"}, {"paperId": "c37937b6ac966b9f056036a06c8c872397190be0", "title": "Efficient Numerical Methods for Nonlinear MPC and Moving Horizon Estimation"}, {"paperId": "7fc604e1a3e45cd2d2742f96d62741930a363efa", "title": "A Tutorial on Energy-Based Learning"}, {"paperId": "6df43f70f383007a946448122b75918e3a9d6682", "title": "Learning to Achieve Goals"}, {"paperId": "bc6014884d291555d92b8dbef4635a1a9e192962", "title": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming"}, {"paperId": "874b3a63422eeaf24c14435ee6091ed48247bff3", "title": "Efficient memory-based learning for robot control"}, {"paperId": null, "title": "Speech understanding systems: Summary of results of the five-year research effort at Carnegie Mellon University"}, {"paperId": null, "title": "2014a) and the least-squares GAN (Mao et al., 2016) formulation to be equally effective for training \u03b3-models as GANs"}, {"paperId": null, "title": "We include the hyperparameters used for training the GAN \u03b3-model in Table A.1 and the flow \u03b3-model"}]}