{"paperId": "5b89f716ee24b0408c5cb854b49dbcbf89b951d5", "title": "HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models", "abstract": "The increasing size of language models necessitates a thorough analysis across multiple dimensions to assess trade-offs among crucial hardware metrics such as latency, energy consumption, GPU memory usage, and performance. Identifying optimal model configurations under specific hardware constraints is becoming essential but remains challenging due to the computational load of exhaustive training and evaluation on multiple devices. To address this, we introduce HW-GPT-Bench, a hardware-aware benchmark that utilizes surrogate predictions to approximate various hardware metrics across 13 devices of architectures in the GPT-2 family, with architectures containing up to 774M parameters. Our surrogates, via calibrated predictions and reliable uncertainty estimates, faithfully model the heteroscedastic noise inherent in the energy and latency measurements. To estimate perplexity, we employ weight-sharing techniques from Neural Architecture Search (NAS), inheriting pretrained weights from the largest GPT-2 model. Finally, we demonstrate the utility of HW-GPT-Bench by simulating optimization trajectories of various multi-objective optimization algorithms in just a few seconds.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "HW-GPT-Bench is introduced, a hardware-aware benchmark that utilizes surrogate predictions to approximate various hardware metrics across 13 devices of architectures in the GPT-2 family, with architectures containing up to 774M parameters."}, "embedding": {"model": "specter_v2", "vector": [0.3071805238723755, 0.15048623085021973, -0.4612938463687897, 0.10081285238265991, -0.5593630075454712, 0.2419586181640625, 0.20945794880390167, -0.32203206419944763, -0.5380718111991882, -0.16807901859283447, 0.35922950506210327, -0.28165653347969055, 0.5767183303833008, 0.004125230014324188, -0.08481491357088089, 0.24723894894123077, -0.833165168762207, 0.2860364019870758, -0.07648300379514694, -0.15872830152511597, -0.15712611377239227, -0.5301633477210999, -1.224277138710022, -0.23409052193164825, 0.07092519104480743, 1.0894603729248047, 0.16525834798812866, 0.9016394019126892, -0.32391130924224854, -0.09661722928285599, 0.4552730321884155, 0.03441948816180229, 0.08279630541801453, 0.13788558542728424, 0.043551068753004074, -0.17838333547115326, 0.2455543726682663, -0.21738161146640778, -0.5040399432182312, 1.0095640420913696, 0.13442152738571167, 0.08903654664754868, 0.30837059020996094, -0.4191650450229645, 0.03473200276494026, 0.3022937476634979, 0.29591837525367737, 0.4704156816005707, -0.4890490770339966, -0.32959234714508057, 1.1693257093429565, -1.1324578523635864, -0.08663922548294067, 1.2772899866104126, 0.4578913450241089, 0.45545223355293274, -0.529999852180481, -0.5469285249710083, 0.18558676540851593, -0.5586250424385071, -1.0246994495391846, -0.4563200771808624, -0.20713263750076294, 0.000552624580450356, 1.938794732093811, -0.1201539933681488, -0.04412379115819931, 0.9376832842826843, 0.5378151535987854, 1.068177580833435, 0.1789160817861557, -0.802284836769104, 0.3658357262611389, -0.21119152009487152, 0.7124115824699402, 0.7843892574310303, 0.11227364838123322, 0.23582711815834045, -0.7347871661186218, -0.6087995767593384, -0.17881785333156586, -0.3882824182510376, 0.5027377605438232, 0.01443756278604269, -0.4153120815753937, 0.6263240575790405, 0.15868066251277924, 0.5488125085830688, -0.18204082548618317, 1.177942156791687, 0.5983786582946777, 0.06590677797794342, 0.16101528704166412, 0.45190778374671936, -0.1784328818321228, 0.29326215386390686, -1.3804603815078735, -0.0057815685868263245, -0.0014884709380567074, 1.1122610569000244, -0.41309311985969543, 0.5658933520317078, -0.808760941028595, -0.05625147745013237, 1.3305531740188599, 0.3798813223838806, 0.6123737692832947, -0.35484451055526733, 0.24401482939720154, -0.8353531360626221, -0.010745451785624027, -0.4710783064365387, -0.3907173275947571, -0.35801398754119873, -0.7617223262786865, -1.1916464567184448, -0.9915462136268616, 0.006887027993798256, -0.7445188164710999, 0.5439515113830566, -0.6920236349105835, 0.17384320497512817, 0.04229661077260971, 0.17269425094127655, 0.5700528621673584, 0.5957822799682617, 0.013812514021992683, 0.17528872191905975, 1.0791568756103516, -1.3475315570831299, -0.41696104407310486, -1.0797038078308105, 0.5155081748962402, -0.33572766184806824, 0.2225428968667984, -0.09225159138441086, -1.2602919340133667, -0.8683217763900757, -1.0832107067108154, -0.22004856169223785, -0.13624757528305054, 0.5809426307678223, 0.9756350517272949, 0.7392451167106628, -1.4272446632385254, 0.6809455752372742, -0.5651506781578064, -0.15866108238697052, -0.01412980817258358, 0.5664399862289429, 0.8172991871833801, -0.09141464531421661, -0.9056670069694519, 0.05761845037341118, 0.555533766746521, -0.75550776720047, -0.3188552260398865, -0.4263153672218323, -0.7149518728256226, 0.005616731010377407, -0.007134089712053537, -0.8761534690856934, 1.1331987380981445, 0.08692426234483719, -1.4447741508483887, 0.5325225591659546, -0.2733631134033203, -0.05788106843829155, 0.045044608414173126, 0.1751439869403839, -0.9382472634315491, -0.5286445617675781, -0.8726945519447327, 0.8232296705245972, 0.49200239777565, 0.02845223806798458, 0.33969053626060486, 0.34759458899497986, -0.5290908813476562, -0.11501986533403397, -0.42168688774108887, 1.0328004360198975, -0.8775224089622498, -0.3060505986213684, 0.29064470529556274, 0.0722055584192276, -0.3817325532436371, -0.22332878410816193, -0.878532350063324, -0.3151400685310364, 0.28294774889945984, -0.09827171266078949, 1.3218094110488892, -0.9001877307891846, -0.8939619064331055, 0.35102877020835876, -0.0322144478559494, -0.3707793354988098, -0.8541984558105469, 0.37792691588401794, -0.17130903899669647, 0.47491204738616943, -0.029420997947454453, -1.3228302001953125, 0.1918908953666687, -0.510410487651825, -0.4586440324783325, -0.4624394178390503, -0.023616082966327667, 0.9156157374382019, -0.3452290892601013, 0.20519982278347015, -0.4322090148925781, 0.41726815700531006, -1.0739662647247314, 0.8147852420806885, -0.5289021134376526, 0.03213149309158325, -0.3860204517841339, -0.1592659205198288, 0.09419634938240051, -0.5308240652084351, 0.5472077131271362, -0.34699392318725586, 0.08927451074123383, 0.5205065608024597, 0.21232886612415314, 1.848512053489685, -0.44636058807373047, 0.2673608064651489, -0.08244101703166962, 0.09757256507873535, 0.6009064316749573, 0.12796834111213684, -0.425366073846817, -0.8506000638008118, 0.9030884504318237, 0.7758458852767944, -0.257865309715271, 0.46922767162323, 0.9894895553588867, 1.35794198513031, -0.539039671421051, 0.2610083520412445, 0.29174819588661194, -0.44276630878448486, 0.44386473298072815, -0.023741820827126503, 0.8590637445449829, 0.044475555419921875, 0.18145328760147095, -0.8037611246109009, 0.3266400098800659, -0.8093255162239075, -0.4098600149154663, 0.445587158203125, 0.4970836937427521, 0.9968293905258179, 0.27568382024765015, -0.7680317759513855, -0.4848216772079468, -0.15262195467948914, 0.8017061948776245, 1.0751193761825562, -0.38315966725349426, 0.15290871262550354, -1.0043787956237793, 0.1353600025177002, -0.4593491554260254, 0.30742180347442627, -0.08975940197706223, -0.025923989713191986, -0.46148571372032166, -1.4328672885894775, 0.6719794869422913, -0.009831922128796577, 0.7483498454093933, -0.33854055404663086, -0.823257565498352, -0.70497727394104, 0.6747208833694458, -1.021535038948059, -0.9199861288070679, 0.27248305082321167, -0.7001885175704956, 0.3023274540901184, 0.20957133173942566, 0.1099427118897438, 0.2075347602367401, -0.45372048020362854, 1.2010445594787598, -0.20820848643779755, -0.39845290780067444, 0.053829215466976166, 1.0242393016815186, -0.30894166231155396, -0.6586865782737732, 0.27775004506111145, 0.06725133955478668, -0.3361881971359253, 0.18728166818618774, 0.15702123939990997, -0.05530613660812378, -0.2559056878089905, -0.017781740054488182, 0.3524603247642517, 0.22412380576133728, 0.07096671313047409, 0.8786473870277405, -0.32830891013145447, -0.4489213824272156, -0.6247189044952393, 1.300244927406311, 0.16611510515213013, -0.7416746020317078, 0.5249341130256653, -0.45338743925094604, 0.17125043272972107, 0.37922585010528564, -0.5715879797935486, -0.04110252857208252, -1.1335723400115967, 0.3526221215724945, -0.4835735261440277, -0.012706847861409187, 0.05228521302342415, 0.38158878684043884, -0.14698342978954315, 0.346760094165802, 0.5397757887840271, 0.2892778515815735, 0.1502055674791336, 0.21665380895137787, -1.071276307106018, 0.28438207507133484, -0.04826293885707855, -0.09348957240581512, -0.29665884375572205, 0.09956139326095581, -0.7910969257354736, -0.27288195490837097, 0.20338258147239685, 0.33672910928726196, -0.18802621960639954, 0.3089892268180847, -0.6749332547187805, -0.6388886570930481, -0.3742724359035492, -1.051342248916626, 0.10397815704345703, 0.6111942529678345, -0.08909988403320312, -0.027268296107649803, -1.2975043058395386, -1.091723084449768, -0.4469851553440094, -1.4502519369125366, -1.7135233879089355, 0.7378818392753601, -0.0015910618240013719, -0.5706258416175842, -0.5154742002487183, -0.16086095571517944, -0.48816999793052673, 1.1326810121536255, -0.34829720854759216, 0.8872912526130676, -0.17999951541423798, -0.16050271689891815, -0.16807542741298676, 0.01788538694381714, 0.25956466794013977, -0.8692658543586731, 0.5423946380615234, -0.9537221789360046, -0.11732444912195206, -0.4113728404045105, -0.3029327690601349, 0.11910280585289001, 0.46965155005455017, 1.1317497491836548, 0.39625802636146545, -0.4748915433883667, 0.40802669525146484, 1.4082615375518799, -0.8114469051361084, -0.16523906588554382, -0.19694019854068756, 0.8052821755409241, -0.22064633667469025, -0.8207899332046509, 0.6737238168716431, -0.00478842668235302, 0.8215284943580627, -0.04851141944527626, 0.16766291856765747, -0.07190237939357758, -0.2648555040359497, 0.4102848172187805, 2.017524480819702, 0.7579188346862793, -0.42392897605895996, -0.904492199420929, 0.09594639390707016, -0.6881172060966492, 0.11331897974014282, 0.34013262391090393, 1.0062602758407593, 0.018719330430030823, 0.23240430653095245, -0.4318355619907379, -0.2751530408859253, 0.5031192302703857, 0.43099141120910645, -0.4971892833709717, -1.1372654438018799, -0.11707185953855515, 0.6979333162307739, 0.472768098115921, 0.32098516821861267, -0.08296297490596771, 0.22028429806232452, 14.653003692626953, 1.2115657329559326, -0.2636975646018982, 0.6447392106056213, 1.0300657749176025, -0.07552894204854965, -0.6875216960906982, -0.3752450942993164, -0.997462272644043, 0.363085001707077, 1.9655466079711914, 0.24387112259864807, 0.6771862506866455, 0.2867571711540222, 0.14774809777736664, 0.08989685028791428, -0.5120190978050232, 0.6841316819190979, 0.5085448622703552, -1.34498929977417, 0.2377454787492752, 0.3266802430152893, 0.6996659636497498, 0.9114288091659546, 0.9871162176132202, 0.5878483057022095, 0.17026574909687042, -0.4766726493835449, 0.6607124209403992, 0.16948428750038147, 1.0741817951202393, -0.19004657864570618, 0.18684081733226776, 0.39099547266960144, -0.6924537420272827, 0.3425765037536621, -0.5506348609924316, -1.1220319271087646, 0.12351422011852264, 0.3575606048107147, -0.5681748986244202, -0.27835384011268616, -0.46250900626182556, 0.09521862119436264, 0.24706672132015228, 0.25147685408592224, 0.028966400772333145, 0.49407774209976196, -0.3837035894393921, -0.4443278908729553, 0.30183202028274536, 0.3887929320335388, -0.06848873943090439, -0.06719552725553513, 0.5364972949028015, -0.3454192280769348, 0.25477656722068787, 0.372546523809433, -0.630495011806488, -0.12161199748516083, 0.09117419272661209, -0.4185427129268646, 0.0493815578520298, 0.7656898498535156, 0.046809401363134384, 0.5940868854522705, -0.48248645663261414, 0.21484610438346863, 0.40785184502601624, 0.10923918336629868, -0.5961819291114807, 0.5083326697349548, 0.5943650603294373, -0.9942013025283813, -0.41092681884765625, 0.09574516117572784, -0.43452006578445435, -0.5104992985725403, -0.6577719449996948, -0.5408772230148315, -0.06534932553768158, -0.7419831156730652, -0.4945150315761566, 1.074787974357605, -0.5379141569137573, 0.17375724017620087, 0.7131118178367615, -1.0680218935012817, -0.5007678866386414, 0.6144094467163086, -1.2915282249450684, -0.46072450280189514, 0.30995607376098633, -0.7062962055206299, -0.31119483709335327, -0.24534906446933746, 1.5889320373535156, 0.12060368061065674, -0.7782935500144958, 0.34001678228378296, 0.4246145188808441, -0.6053363680839539, -0.5051257014274597, -0.2539355754852295, 1.533884882926941, 0.11708856374025345, -0.4218445122241974, 0.00754409609362483, 0.2352343499660492, 0.09961537271738052, -1.316367745399475, -0.07809070497751236, 0.510124146938324, -0.5544216632843018, 0.2171744853258133, -0.9515077471733093, -0.41239050030708313, -0.0728919729590416, 0.39793917536735535, -0.26007914543151855, 0.4868847131729126, -0.16206763684749603, -0.6099334359169006, 0.39458155632019043, -0.6039063334465027, 0.22293786704540253, 0.5321022868156433, -0.816081166267395, 0.19399331510066986, 0.2442362755537033, 0.4767567813396454, -1.2299244403839111, -0.3844793438911438, 0.023544352501630783, 0.35527780652046204, -0.40699630975723267, 0.8192270398139954, -0.09074393659830093, 0.9530528783798218, 0.9146375060081482, -0.47084110975265503, -0.5359551310539246, 0.3687857985496521, -0.8844262361526489, -0.04918937385082245, -0.608627200126648, 1.0162359476089478, -0.38601285219192505, 0.5555164813995361, 0.9522588849067688, 0.03906003758311272, -0.7138789892196655, -0.793485701084137, -0.2864472270011902, -0.35406947135925293, -0.5610066652297974, 0.4820285737514496, -0.5145733952522278, -0.46161314845085144, -0.046095315366983414, 0.284548282623291, 0.5872101187705994, -0.3735162317752838, -0.7435183525085449, 0.011840706691145897, 0.07931287586688995, -0.6209142208099365, -0.9900189638137817, -0.46193623542785645, -1.1966716051101685, 0.20758488774299622, -0.9734952449798584, -0.0032206941395998, -0.16456057131290436, -0.05948473885655403, 0.06295451521873474, -0.24826209247112274, -0.27014604210853577, 0.6563530564308167, -0.20181624591350555, -0.6159055233001709, -0.516884446144104, -0.518936038017273, 0.9819002747535706, 0.5944178700447083, -0.7374157905578613, 0.1465820074081421, -0.37159231305122375, 0.48084908723831177, 0.3223986327648163, 0.4911485016345978, -0.1310047209262848, -1.0829193592071533, -1.847829818725586, 0.3452354967594147, -0.09618094563484192, -0.3990010619163513, -1.2636456489562988, 0.670996904373169, 0.6272068023681641, -0.5618784427642822, 0.2289111167192459, 0.3774504065513611, -0.712751567363739, -0.020448461174964905, 0.6181349754333496, -0.5993542075157166, 0.8038738369941711, 0.7427643537521362, -0.4494036138057709, 0.014894211664795876, 0.6098975539207458, -0.33918485045433044, -0.4581301808357239, -0.6333514451980591, 0.3892432153224945, -0.2560558617115021, -0.037789486348629, -0.08636919409036636, -0.0914805606007576, -1.1857913732528687, 0.021629570052027702, 0.020885532721877098, 0.49159494042396545, -0.1938188672065735, 0.9086815118789673, 0.5621523261070251, -0.8195143342018127, 0.10097083449363708, 0.5363042950630188, -0.07629439234733582, -0.5262158513069153, 0.6794610023498535, 0.6635451912879944, -0.9828957915306091, 0.8095919489860535, 0.0706808865070343, 0.3367021977901459, -0.7590818405151367, -0.09496705234050751, 0.4658963084220886, -0.6190159916877747, 0.06676601618528366, 1.3149508237838745, -0.2037690132856369, -0.800748348236084, -0.06729845702648163, -1.1826114654541016, -0.04145319387316704, -0.7430514693260193, 0.4779311716556549, 0.012342976406216621, 0.5213797092437744, 0.11377347260713577, -0.4738592803478241, 0.11649113148450851, 0.10391198098659515, -0.6377311944961548, -0.20628021657466888, 0.08384936302900314, -0.39839062094688416, 0.41678327322006226, 0.9870427846908569, -0.8638086318969727, -0.48543062806129456, -0.4516175091266632, -0.1018996611237526, 0.4101161062717438, 0.7289664149284363, -0.04488632082939148, -0.9220540523529053, 0.8607693910598755, 0.6719279289245605, -0.11794459074735641, 0.47905632853507996, -0.7176006436347961, 0.35112085938453674, 0.44999268651008606, 0.9086742997169495, -0.3211795687675476, -0.5933755040168762, 1.314700961112976, 0.887997031211853, -0.7953343987464905, 0.5127792358398438, -0.3143857419490814, -0.3661803603172302, 0.8524549603462219, 0.28088346123695374, 0.0444367490708828, 0.8606985211372375, 0.19641995429992676, -0.3928327262401581, 0.1697426587343216, -1.030062198638916, 0.09356778115034103, 1.196087121963501, 0.2116282433271408, 0.9022135138511658, 0.830829918384552, -0.19084782898426056, 0.6198808550834656, -0.009174554608762264, -0.09847573190927505, 0.49942585825920105, 0.5572225451469421, 0.031014783307909966, 0.003978380933403969, 0.0001166465735877864, 0.5378963351249695, -0.3550112545490265, -0.9842678904533386, 0.21345454454421997, 0.46658653020858765, -0.10943390429019928, 0.4764805734157562, 0.9847602248191833, -0.42816174030303955, 0.26237761974334717, -0.010446207597851753, 0.621228814125061, -0.5526673793792725, -0.23479725420475006, -0.0743003860116005, -0.24201878905296326, -0.20610542595386505, 0.1987018883228302, -0.08451533317565918, -0.289686918258667, -0.4129742681980133, 0.47004303336143494, -0.10167797654867172, 0.6852046251296997, 0.7121714353561401, 0.9466255903244019, 0.8857372999191284, -0.36978599429130554, -0.7360655069351196, -0.37141308188438416, -0.6381988525390625, 0.05275540426373482, -1.140351414680481, -0.6092543601989746, -0.04160866141319275, 0.17304398119449615, -0.9548776149749756]}, "authors": [{"authorId": "46219704", "name": "R. Sukthanker"}, {"authorId": "51109984", "name": "Arber Zela"}, {"authorId": "40539032", "name": "B. Staffler"}, {"authorId": "2240526179", "name": "Jorg K. H. Franke"}, {"authorId": "2269049641", "name": "Frank Hutter"}], "references": [{"paperId": "8e3b5beeeb07df8f15e8c0e2f93ce01faa204d3f", "title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search"}, {"paperId": "35c07f14c5edd6dc87d0ad9ab9849bb6ab6055aa", "title": "Unsupervised Graph Neural Architecture Search with Disentangled Self-Supervision"}, {"paperId": "874f1242b0ad04fb8757064dc0a5dd8de38913ff", "title": "Not all Layers of LLMs are Necessary during Inference"}, {"paperId": "63167c30b06aa6c3d76e09065ced0412090d6c3b", "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"}, {"paperId": "87667fd50710183951fd2206a929a25b4b970276", "title": "Shortened LLaMA: A Simple Depth Pruning for Large Language Models"}, {"paperId": "78971568f764f3479b26c42844845ec99664298c", "title": "The LLM Surgeon"}, {"paperId": "9ea8001d6eb52d14134ceede7f88b1d8fa8db41f", "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models"}, {"paperId": "5851121df5ce46be5faea265c868ec0beabfce96", "title": "Efficient Large Language Models: A Survey"}, {"paperId": "5919f23dcd45bd72973f25f9f807de87f8422e0d", "title": "TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications"}, {"paperId": "56b828717f32251a5e0f0be9c0113077f23c8429", "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees"}, {"paperId": "5fef1fe63687bdaa565d5b0c38a9b32fb13470d4", "title": "Python Tool for Visualizing Variability of Pareto Fronts over Multiple Runs"}, {"paperId": "5e4125b3a2ec91e866d970498f8a138c5a5cc89b", "title": "When Do Neural Nets Outperform Boosted Trees on Tabular Data?"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "0c99f867588c2dc79471613adfc1b0970c1952c2", "title": "EEG-Based Sleep Stage Classification via Neural Architecture Search"}, {"paperId": "8973ad5fc1264594a1fda3bd9e04258074cea9cc", "title": "Neural Architecture Search: Insights from 1000 Papers"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "0bc414d4682076e6f22e143c829b6b13feb37323", "title": "Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition"}, {"paperId": "193aaa44b1b4387d99e592caf96a57a88c02148d", "title": "AMLB: an AutoML Benchmark"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af", "title": "Compression of Generative Pre-trained Language Models via Quantization"}, {"paperId": "4857d4de584ed6f15e7ef12a96823ec8b4a17ec1", "title": "NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy"}, {"paperId": "9202a718ce05395b6e17d5301e3a2e8b1021f31b", "title": "Prune Once for All: Sparse Pre-Trained Language Models"}, {"paperId": "e57d9aa9cf3e292e9eecd2f24b21677fa090d988", "title": "NAS-Bench-x11 and the Power of Learning Curves"}, {"paperId": "afe6957186142f8c5c3d78483a59143100e66da4", "title": "NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks"}, {"paperId": "e9a09f8e474b4c74c700ebbe84d5b0696395a521", "title": "Towards Efficient Post-training Quantization of Pre-trained Language Models"}, {"paperId": "ff69d758764157e612f92f97a987838312c568a9", "title": "Compute and Energy Consumption Trends in Deep Learning Inference"}, {"paperId": "3bc361f9fa99366e1b8e851e508c12dcefb3b3b9", "title": "AdvRush: Searching for Adversarially Robust Neural Architectures"}, {"paperId": "d645bd08fc19d52164695f9cd5ae863345459a06", "title": "AutoFormer: Searching Transformers for Visual Recognition"}, {"paperId": "316a40ad51725c41524f7e8ef78b91a7886167ad", "title": "Multi-objective Asynchronous Successive Halving"}, {"paperId": "3f39e8a869f6e170df708d9ed9da86c10d364665", "title": "A multi-objective perspective on jointly tuning hardware and hyperparameters"}, {"paperId": "973ee6745dc8308171b690b73a26e31fef651c32", "title": "EVSRNet: Efficient Video Super-Resolution with Neural Architecture Search"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "c13f5dbf6fd48ae339dd3f3394e0cbe9661756d3", "title": "How Powerful are Performance Predictors in Neural Architecture Search?"}, {"paperId": "b65f3c9ba243c4ebdec015549cea74c0d1f70a2e", "title": "End-to-End Constrained Optimization Learning: A Survey"}, {"paperId": "a10daed04b387cdb6b9c71a623994bc083599c84", "title": "HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark"}, {"paperId": "949c0941d4c57482318afa28f2c8eb82569fb401", "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey"}, {"paperId": "a34a77bd53e752c4ecfe8c5ddbf1c218e2e8ac31", "title": "AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling"}, {"paperId": "0f5c15ae08601e749d4aecccf562321a842e61b5", "title": "Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty Quantification"}, {"paperId": "d01ca0b8b6d02e2833b8d1a71b28de153cdbc397", "title": "TransNAS-Bench-101: Improving transferability and Generalizability of Cross-Task Neural Architecture Search"}, {"paperId": "9ed4321e552d069ff6aa6f88480809e23927131d", "title": "Surrogate NAS Benchmarks: Going Beyond the Limited Search Spaces of Tabular NAS Benchmarks"}, {"paperId": "8054513b37839878c5ee9d541d228b0148bce306", "title": "NSGANetV2: Evolutionary Multi-Objective Surrogate-Assisted Neural Architecture Search"}, {"paperId": "3bddc8f2dff60d9da7d7c5f315f3467966f61ff1", "title": "Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning"}, {"paperId": "48167fbde4ef8d5caa69e1c0c9d6e2ac7fd48c3b", "title": "NAS-Bench-NLP: Neural Architecture Search Benchmark for Natural Language Processing"}, {"paperId": "1c55f470a8273788d82f05500d507b408a5722b8", "title": "Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization"}, {"paperId": "4eb1f213f5e55946503d88d1f79861907a36a531", "title": "Can Weight Sharing Outperform Random Architecture Search? An Investigation With TuNAS"}, {"paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "39f8cc684f09ea2b43767f5b9590896774802759", "title": "On the effect of dropping layers of pre-trained transformer models"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "9407daf4aaee929f6b193b97b13a1411c3dd803d", "title": "AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data"}, {"paperId": "40744235481391b7d6baf8f9d590dfa32da1f4d4", "title": "NAS-Bench-1Shot1: Benchmarking and Dissecting One-shot Neural Architecture Search"}, {"paperId": "69599593f93023e2f91ef6673ee9860f85777d98", "title": "NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search"}, {"paperId": "645a24296f96f325f4a6fd324cef85661a8987da", "title": "NAS evaluation is frustratingly hard"}, {"paperId": "e68a76145e5054685de5da028dc50a8ab94fd7ae", "title": "Methods for comparing uncertainty quantifications for material property predictions"}, {"paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "title": "Structured Pruning of Large Language Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "5c2d8867055de9df951c29492b5cbffd68ec95be", "title": "Reinforcement learning for neural architecture search: A review"}, {"paperId": "7823292e5c4b05c47af91ab6ddf671a0da709e82", "title": "Once for All: Train One Network and Specialize it for Efficient Deployment"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "6f9dc6f8519e927d948a13aa7ae0df336f443eb9", "title": "Conformalized Quantile Regression"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "79e523beb1e1411a241edde0464b07c2ebc231d1", "title": "Single Path One-Shot Neural Architecture Search with Uniform Sampling"}, {"paperId": "6e4fd9b4b2b673c981cda528d8039a221ad35225", "title": "NAS-Bench-101: Towards Reproducible Neural Architecture Search"}, {"paperId": "35a59bd09974c7fc78cf681f77f7301e180fd23c", "title": "Random Search and Reproducibility for Neural Architecture Search"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "120ffccea4787b88f78b55b9302891ff96cb4228", "title": "Slimmable Neural Networks"}, {"paperId": "a2403c1ce02120f7bd383e395b561ff7c64d52ec", "title": "A System for Massively Parallel Hyperparameter Tuning"}, {"paperId": "430a03b3c83a0426717cb34601cfcc14c8b1b1b9", "title": "NSGA-Net: neural architecture search using multi-objective genetic algorithm"}, {"paperId": "794ca7a3a856683221797c6e03cdc6ef798d1f5e", "title": "Searching for Efficient Multi-Scale Architectures for Dense Image Prediction"}, {"paperId": "45b7b5514a65126d39a51d5a68da53e7aa244c1f", "title": "Understanding and Simplifying One-Shot Architecture Search"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "d9ede71b944460c1293294c9f64bd88a8cc400de", "title": "A Flexible Framework for Multi-Objective Bayesian Optimization using Random Scalarizations"}, {"paperId": "50bdda28de3dcf82a0e10f9ec13eea248b19edb5", "title": "Regularized Evolution for Image Classifier Architecture Search"}, {"paperId": "497e4b08279d69513e4d2313a7fd9a55dfb73273", "title": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "67d968c7450878190e45ac7886746de867bf673d", "title": "Neural Architecture Search with Reinforcement Learning"}, {"paperId": "26bc9195c6343e4d7f434dd65b4ad67efe2be27a", "title": "XGBoost: A Scalable Tree Boosting System"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "06e0f8f409ce76dbc88e1f811dda5058b3fa7e29", "title": "KV-Cache: A Scalable High-Performance Web-Object Cache for Manycore"}, {"paperId": "168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "title": "Scikit-learn: Machine Learning in Python"}, {"paperId": "fb1ab5f9e287bd8ee91670a82d0f7d7cceb3a0c9", "title": "Enhanced recursive feature elimination"}, {"paperId": "6eddc19efa13f7e70301908d98e85a19d6f32a02", "title": "A fast and elitist multiobjective genetic algorithm: NSGA-II"}, {"paperId": "d6cf4508e0393e7a7c62cd3c6cfe13643732bcd1", "title": "On the Performance Assessment and Comparison of Stochastic Multiobjective Optimizers"}, {"paperId": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "title": "Optimal Brain Surgeon and general network pruning"}, {"paperId": "4863d3546ee8cd591d2e18f4b1bfaf492de4ea3e", "title": "EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark"}, {"paperId": "182c6d1d30859f227dca3606c743e178e8ae6780", "title": "Structural Pruning of Large Language Models via Neural Architecture Search"}, {"paperId": "4ad9b03fae793bd314ec06dae735c2988ee37ccc", "title": "Energy Consumption-Aware Tabular Benchmarks for Neural Architecture Search"}, {"paperId": "08f14bc8734253ba886b91531c68ff88393a5a84", "title": "Syne Tune: A Library for Large Scale Hyperparameter Tuning and Reproducible Research"}, {"paperId": "5a00b32876f7d4869bce980500d4ccc978389315", "title": "Why do tree-based models still outperform deep learning on typical tabular data?"}, {"paperId": "d0cf9923b9f9f58522b1407df60e6f96d4588f29", "title": "NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition"}, {"paperId": null, "title": "Gpt-j-6b: A 6 billion parameter autoregressive language model"}, {"paperId": null, "title": "Hardware-adaptive efficient latency prediction for nas via meta-learning"}, {"paperId": null, "title": "Proceedings of the International Conference on Computer Vision and Pattern Recognition (CVPR\u201920)"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "dc39536089a5fead271ae6e44617407e7148536b", "title": "Multi-objective Optimization"}, {"paperId": "4b942bb433be5a57f3cfd269f094a292a32933ad", "title": "Stacked Generalization"}, {"paperId": "7c828e3492a1db8996c147d8f7a70476645bc5cf", "title": "Exploratory Analysis of Stochastic Local Search Algorithms in Biobjective Optimization"}, {"paperId": null, "title": "Quantile regression , volume 38"}, {"paperId": "8e0be569ea77b8cb29bb0e8b031887630fe7a96c", "title": "Random Forests"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": "9153c974abd1682833472824ed1b7925f6b2cba6", "title": "Multi-objective differentiable neural architecture search"}, {"paperId": null, "title": "Forbes ai sustainability havoc"}]}