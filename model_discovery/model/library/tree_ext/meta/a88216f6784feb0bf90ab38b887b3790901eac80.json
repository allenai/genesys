{"paperId": "a88216f6784feb0bf90ab38b887b3790901eac80", "title": "From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers", "abstract": "Pretrained Language Models (PLMs) have become the de facto starting point for fine-tuning on downstream tasks. However, as model sizes continue to increase, traditional fine-tuning of all the parameters becomes challenging. To address this, parameter-efficient fine-tuning (PEFT) methods have gained popularity as a means to adapt PLMs effectively. In parallel, recent studies have revealed the presence of activation sparsity within the intermediate outputs of the multilayer perceptron (MLP) blocks in transformers. Low activation density enables efficient model inference on sparsity-aware hardware. Building upon this insight, in this work, we propose a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models. We demonstrate the effectiveness of our approach by utilizing mainstream PEFT techniques, including QLoRA, LoRA, Adapter, and Prompt/Prefix Tuning, to facilitate efficient model adaptation across diverse downstream tasks. Experiments show that our proposed method, \\textbf{DEFT} (Density-Efficient Fine-Tuning), can consistently reduce activation density by up to \\textbf{44.94\\%} on RoBERTa$_\\mathrm{Large}$ and by \\textbf{53.19\\%} (encoder density) and \\textbf{90.60\\%} (decoder density) on Flan-T5$_\\mathrm{XXL}$ (\\textbf{11B}) compared to PEFT, using GLUE and QA (SQuAD) benchmarks respectively. We also introduce \\textbf{ADA-DEFT}, an adaptive variant of our DEFT approach, which achieves significant memory and runtime savings during inference. For instance, ADA-DEFT reduces runtime by \\textbf{8.79\\%}and memory usage by \\textbf{17.46\\%} in Flan-T5$_\\mathrm{XL}$, and by \\textbf{2.79\\%} and \\textbf{2.54\\%} respectively in Flan-T5$_\\mathrm{XXL}$. Additionally, we showcase that DEFT works complementarily with quantized and pruned models.", "venue": "arXiv.org", "year": 2024, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models and demonstrates that DEFT works complementarily with quantized and pruned models."}, "embedding": {"model": "specter_v2", "vector": [0.49781548976898193, 0.6318855285644531, -0.5750772953033447, -0.09078601747751236, -0.10315509140491486, -0.15892639756202698, 0.47778889536857605, -0.3044580817222595, -0.37489286065101624, -0.32446959614753723, 0.681372344493866, -0.040091391652822495, 0.4233967065811157, 0.05468454957008362, -0.1334034502506256, 0.3014143109321594, -0.8026960492134094, 0.4437163770198822, -0.15664483606815338, -0.5173355937004089, -0.3879620134830475, -0.5512155294418335, -0.9370924830436707, 0.26037168502807617, 0.1440148651599884, 0.8908795714378357, 0.08811044692993164, 0.5961238145828247, -0.3649309277534485, -0.2189321368932724, 0.5274487137794495, -0.22760507464408875, 0.1377650499343872, 0.21159006655216217, 0.010025360621511936, 0.3222077786922455, 0.32868099212646484, -0.24500536918640137, -0.20062175393104553, 1.0347040891647339, -0.27881407737731934, 0.1794973611831665, 0.3246989846229553, -0.35454508662223816, -0.1605147421360016, 0.9490878582000732, 0.5792441964149475, 1.0348726511001587, -0.8270344734191895, -0.38367071747779846, 1.1452091932296753, -1.4790185689926147, -0.4143819510936737, 1.2495609521865845, 0.2526968717575073, 0.374453604221344, -0.6943279504776001, -0.6499264240264893, 0.8069759011268616, -0.2278222292661667, -0.9575725793838501, -0.5458422303199768, -0.029909251257777214, -0.14405183494091034, 2.1543610095977783, -0.527949869632721, 0.11409951746463776, 0.6964997053146362, -0.2902829945087433, 0.7918437123298645, -0.21833553910255432, -0.8537014722824097, -0.26312270760536194, 0.2254875898361206, -0.004869109485298395, 0.9566552639007568, -0.4623927175998688, 0.23543955385684967, -0.971356987953186, -0.18818669021129608, 0.7006430625915527, -0.2864575684070587, 0.4126830995082855, -0.0324234738945961, -0.09556666761636734, 0.8545188903808594, 0.12522637844085693, 0.765394926071167, -0.08745159208774567, 0.735734760761261, 0.5166611671447754, 0.38576263189315796, 0.11333972960710526, 0.5622064471244812, -0.38234561681747437, 0.22275449335575104, -0.911641538143158, -0.31194382905960083, -0.02703457698225975, 0.7920604348182678, -0.05953732505440712, 0.5781315565109253, -0.8423907160758972, 0.31903019547462463, 1.4264885187149048, -0.03818655386567116, 0.3535287380218506, -0.6540606021881104, 0.4534335434436798, -0.6100447177886963, 0.03481757640838623, -0.3566029369831085, -0.1880345344543457, -0.4774336516857147, -0.8998860120773315, -1.2921910285949707, -0.5084882378578186, 0.0655694454908371, -0.8424935340881348, 0.9793176651000977, -0.3876552879810333, 0.20159921050071716, -0.17856435477733612, 0.4697622060775757, 0.033959824591875076, 0.5867504477500916, 0.3488111197948456, 0.09708071500062943, 1.2372627258300781, -1.0012662410736084, -0.44595518708229065, -1.3510726690292358, 0.4805675745010376, -0.06992890685796738, 0.5464648008346558, 0.05759473890066147, -1.406201720237732, -0.7351371645927429, -1.0383422374725342, 0.24266313016414642, -0.3967684805393219, 0.3055325746536255, 0.9868265986442566, 0.5654420852661133, -1.0289266109466553, 0.53671795129776, -0.38742178678512573, -0.06029446795582771, 0.4552857577800751, 0.5571779012680054, 0.5081965923309326, -0.059642013162374496, -1.6314430236816406, 0.08500102907419205, 0.5863909125328064, -0.4022841453552246, -0.3739387094974518, -0.6785275340080261, -0.7461612820625305, 0.3351677656173706, 0.33983755111694336, -0.8649446368217468, 1.262149453163147, -0.3161965310573578, -1.7293914556503296, 0.5455789566040039, -0.0887131467461586, -0.16112054884433746, -0.052550867199897766, -0.18495236337184906, -0.7254521250724792, -0.5356244444847107, -0.7450191378593445, 0.6209145188331604, 1.06981360912323, 0.4102306663990021, -0.1146484985947609, 0.24125328660011292, -0.17574916779994965, 0.14593489468097687, -0.5775849223136902, 0.8150072693824768, -0.7021353840827942, -0.07488084584474564, 0.25845974683761597, 0.9402820467948914, -0.16132837533950806, -0.27683648467063904, -0.3010629117488861, -0.8158243298530579, 0.5803613066673279, 0.29809901118278503, 1.381532907485962, -1.038112759590149, -0.7421224117279053, 0.32085365056991577, 0.034851230680942535, -0.008917274884879589, -0.6556791663169861, 0.2578100264072418, -0.41205984354019165, 0.17841045558452606, 0.12021858245134354, -0.9378689527511597, 0.23976761102676392, -0.256182461977005, -0.5682794451713562, -0.12957078218460083, 0.2698504328727722, 0.9217666387557983, -0.7747642993927002, 0.19646970927715302, 0.05367356166243553, 0.6758416891098022, -1.4799816608428955, 1.2618385553359985, -0.5286111831665039, 0.2560330033302307, -0.1776619255542755, -0.10448634624481201, -0.1802166849374771, -0.7576104402542114, 0.5469017624855042, -0.6804696321487427, 0.07626145333051682, 0.5561529994010925, -0.45833757519721985, 1.4400596618652344, -0.6500966548919678, 0.09986447542905807, 0.12979759275913239, -0.7453627586364746, 0.4696965515613556, 0.37768375873565674, -0.13874876499176025, -0.5604097247123718, 0.3070881962776184, 0.5239621996879578, -0.5449255704879761, 0.5130513906478882, 0.8188268542289734, 0.8097352981567383, -0.708875298500061, -0.2823392450809479, 0.8258982300758362, -0.2409939020872116, 0.4927748441696167, 0.2164909988641739, 0.6403939723968506, 0.10026960074901581, 0.708447277545929, -0.2409728616476059, 0.3610287308692932, -1.2766777276992798, -0.15494194626808167, 0.41040515899658203, 0.7333840131759644, 0.5436780452728271, 0.4380960762500763, -0.6217287182807922, -0.5748682022094727, -0.31830576062202454, 0.4464344382286072, 1.811611294746399, -0.7178534269332886, 0.00695576798170805, -0.7284384369850159, -0.24883082509040833, -0.3833378553390503, -0.0693613737821579, -0.21072173118591309, -0.21607854962348938, -0.6634246110916138, -1.3378747701644897, 0.7094818353652954, 0.1231050118803978, 0.8981205821037292, -0.4175975024700165, -0.1430094987154007, 0.002398890908807516, 0.5957455039024353, -0.8671764731407166, -0.883966326713562, 0.7912037372589111, -0.7021709084510803, 0.27065521478652954, 0.2907535135746002, -0.03804364427924156, 0.23994235694408417, -1.291996717453003, 1.240341305732727, -0.7890579700469971, 0.12381140142679214, -0.12314476817846298, 0.7283146977424622, 0.045528050512075424, -0.7100968956947327, 0.8202201128005981, 0.04379568248987198, 0.12479651719331741, 0.34988975524902344, -0.01272724661976099, -0.045775510370731354, 0.07071559876203537, -0.30889347195625305, 0.028767313808202744, 0.17750829458236694, 0.04240354523062706, 0.8548239469528198, -0.4622003734111786, 0.1307544857263565, -1.6919156312942505, 0.8523069024085999, -0.10225626081228256, -0.7807745933532715, -0.027277309447526932, -0.8032010793685913, -0.00038922642124816775, 0.6790475845336914, -0.5302053689956665, -0.3904378414154053, -0.7616651654243469, 0.04730985686182976, -0.45097196102142334, -0.13375261425971985, 0.10152468085289001, 0.20943571627140045, 0.2044520527124405, 0.37817126512527466, 0.12965629994869232, -0.18145853281021118, -0.16044285893440247, 0.7425889372825623, -0.8264968991279602, 0.7505766153335571, 0.0729302167892456, 0.042714107781648636, -0.29761987924575806, -0.42226824164390564, -0.4006551206111908, -0.6472030282020569, -0.43111300468444824, 0.02652359940111637, 0.18403421342372894, -0.057747259736061096, -0.7756701111793518, -0.6973104476928711, -0.25882720947265625, -0.8368145227432251, -0.3157062530517578, -0.12398071587085724, -0.38271549344062805, -0.1317320168018341, -1.281650185585022, -1.3285419940948486, -0.37952321767807007, -0.7906947731971741, -1.1794813871383667, 0.3969908654689789, 0.09585520625114441, -0.4080295264720917, -0.8966004848480225, -0.12186133116483688, -0.5493319630622864, 1.1514043807983398, -0.9169167876243591, 1.0328742265701294, 0.0865960493683815, -0.035264648497104645, 0.3952626883983612, 0.072238028049469, 0.4651108384132385, -0.6427628397941589, 0.015350652858614922, -1.1593886613845825, 0.0998094379901886, -0.5102981925010681, -0.3146941661834717, 0.2183651179075241, 0.437701553106308, 1.0767543315887451, -0.25517189502716064, -0.31509143114089966, 0.9657148122787476, 1.392899513244629, -0.994548499584198, 0.19424621760845184, 0.07689502090215683, 0.730158269405365, -0.31835466623306274, -0.5881359577178955, 0.9229212999343872, 0.1427871286869049, 0.6480104327201843, 0.16060103476047516, -0.0352548323571682, -0.20285461843013763, -0.8035975098609924, 0.6658773422241211, 1.9061084985733032, 0.7793169021606445, -0.03451443836092949, -0.6591953039169312, 0.45900049805641174, -1.1229808330535889, -0.10685078054666519, 0.7861328125, 0.6864241361618042, 0.6569914817810059, -0.23272810876369476, -0.35583585500717163, -0.27199968695640564, 0.2799895703792572, 0.44371509552001953, -0.37521135807037354, -1.20379638671875, 0.05174822360277176, 0.6213387846946716, 0.4724762439727783, 0.7100986838340759, -0.2743792235851288, 0.587232232093811, 14.57807731628418, 0.9880326986312866, 0.11995483934879303, 0.7908292412757874, 0.7379514575004578, 0.16408944129943848, -0.39516767859458923, -0.23483887314796448, -1.5624868869781494, 0.048490237444639206, 1.5303683280944824, 0.5861805081367493, 0.9728301167488098, 0.011952570639550686, -0.0767250657081604, 0.43493005633354187, -0.4712356626987457, 0.6584812998771667, 0.288711279630661, -1.3519412279129028, 0.25259989500045776, -0.14400196075439453, 0.6264597177505493, 0.5700499415397644, 0.9326816201210022, 0.9946983456611633, 0.6710228323936462, -0.5725172758102417, 0.3957323133945465, 0.22105424106121063, 1.158758282661438, 0.09083335101604462, 0.08643629401922226, 0.3474119007587433, -0.8933429718017578, -0.3502620458602905, -0.4711552858352661, -1.1129822731018066, 0.36723577976226807, 0.500768780708313, -0.7626702189445496, -1.0118802785873413, -0.11430677026510239, 0.5325644612312317, 0.24388442933559418, 0.2187245488166809, -0.4125259518623352, 0.7428860068321228, -0.39613744616508484, 0.4212915301322937, 0.23796483874320984, 0.4355454742908478, 0.02369638904929161, 0.10768496990203857, 0.14945578575134277, -0.47447481751441956, 0.17558029294013977, 0.7796605229377747, -0.7251678109169006, -0.5243397951126099, 0.2075359970331192, -0.23613323271274567, 0.37305334210395813, 0.9610849022865295, 0.7992070913314819, 0.27949902415275574, -0.4004724323749542, 0.1414327621459961, 0.9708524346351624, 0.3205638825893402, -0.33628320693969727, 0.43319010734558105, 0.16710855066776276, -0.38365012407302856, 0.10837302356958389, 0.6878200173377991, -0.07509815692901611, -0.4829803705215454, -1.0404868125915527, -0.5748668313026428, 0.6104949712753296, -0.7400315403938293, -0.6389765739440918, 0.5631300210952759, -0.10179731249809265, 0.06322421133518219, 0.2547335922718048, -0.6187191605567932, 0.056395452469587326, 0.4636474549770355, -1.4364064931869507, -0.4747273027896881, 0.5805044174194336, -0.24219711124897003, -0.4285835921764374, -0.23401951789855957, 1.5208566188812256, 0.40915626287460327, -0.6107231378555298, 0.3484761714935303, -0.10170888155698776, -0.29022714495658875, -0.3101033866405487, -0.6551487445831299, 0.8269609212875366, 0.3047139346599579, -0.15592822432518005, 0.19678477942943573, -0.1824401617050171, 0.6625692844390869, -0.6929618716239929, -0.22216710448265076, 0.6420812606811523, -0.5571817755699158, -0.011421998031437397, -0.82112717628479, -0.7112269997596741, 0.5663698315620422, 0.3732963800430298, -0.061646558344364166, 0.15691308677196503, 0.33459264039993286, -0.7500210404396057, -0.21780744194984436, -0.5987302660942078, -0.3062916696071625, 0.3562317192554474, -0.8098478317260742, -0.1371799111366272, -0.31345832347869873, 0.2055608332157135, -1.3209995031356812, -0.9306504130363464, -0.09071609377861023, -0.21432730555534363, 0.00712292455136776, 1.017591953277588, -0.11196047067642212, 0.198038712143898, 0.7672514915466309, -0.18668493628501892, -0.9091936945915222, -0.11267492920160294, -1.0391745567321777, -0.5277464389801025, 8.982912549981847e-05, 0.9381276965141296, -0.41088059544563293, 0.5598781108856201, 0.5201065540313721, -0.06401441991329193, -0.30115950107574463, -0.6478286385536194, 0.05723831430077553, -0.07485535740852356, -0.48149868845939636, 0.46110326051712036, -0.15000611543655396, 0.06270002573728561, 0.49778711795806885, 0.5685925483703613, 0.6452527642250061, -0.4514409303665161, -0.8298855423927307, -0.06132396683096886, 0.17284271121025085, -0.7608974575996399, -0.49353915452957153, -0.23901380598545074, -1.6475186347961426, -0.23671270906925201, -1.0986305475234985, -0.05330253764986992, -0.2780780494213104, -0.4301181435585022, -0.08036720752716064, -0.41647768020629883, 0.2219984084367752, 0.5346078872680664, -0.19008828699588776, -0.7552153468132019, -0.5423440933227539, -0.46895888447761536, 0.7800412178039551, 0.756423830986023, -0.7197491526603699, 0.18130095303058624, 0.05233590677380562, 0.00310707394964993, 0.4397284984588623, 0.5326079726219177, -0.34143751859664917, -0.8294464945793152, -1.5495973825454712, 0.8757246732711792, -0.21360017359256744, -0.23022188246250153, -0.5553345680236816, 0.5566827058792114, 0.3961631655693054, -0.34419164061546326, 0.4911685585975647, 0.4911094009876251, -0.9429892897605896, -0.4829539358615875, 0.13636791706085205, -0.6255690455436707, 0.2731035649776459, 0.5861674547195435, -0.5416356325149536, -0.2796320617198944, 0.6173116564750671, 0.18064741790294647, -0.8201389312744141, -1.0406503677368164, 0.3779882490634918, -0.42702630162239075, 0.4717119038105011, -0.7470957636833191, 0.13143150508403778, -1.226112723350525, -0.24086961150169373, -0.12867949903011322, 0.25358274579048157, -0.5263318419456482, 0.5408118963241577, 0.24827495217323303, -1.0222103595733643, 0.17453697323799133, 0.7305744886398315, -0.0999622792005539, 0.29562869668006897, 0.2879388928413391, 0.6381646990776062, -0.12435416877269745, 0.590634286403656, 0.23037442564964294, 0.19650177657604218, -0.25099441409111023, -0.2704438865184784, 1.0337727069854736, -0.9354912042617798, 0.0036712440196424723, 1.2679001092910767, -0.3170503079891205, -0.993674635887146, 0.16795048117637634, -1.345666527748108, -0.36042144894599915, -0.49926409125328064, 0.2818024754524231, -0.32217010855674744, 0.2455727607011795, 0.2771363854408264, -0.4639011025428772, -0.058629781007766724, -0.11323606222867966, -0.35172784328460693, 0.3705156743526459, -0.17303144931793213, -0.13615962862968445, 0.328894704580307, 0.7449579834938049, -0.681553304195404, -1.028748631477356, -0.8372192978858948, -0.38225090503692627, 0.16961081326007843, 0.6945748329162598, -0.47507554292678833, -0.8946226835250854, 0.9138758182525635, 0.4531659185886383, 0.026250788941979408, 0.4071502089500427, -0.13387329876422882, -0.16152501106262207, 0.470895379781723, 0.04643310606479645, -0.7418287992477417, -0.4005970060825348, 1.403637170791626, 0.8894801139831543, -0.5178073644638062, 0.0018069366924464703, -0.21842268109321594, -0.5011922121047974, 0.7007744312286377, 0.5063955783843994, -0.036967962980270386, 0.9819689393043518, 0.36898136138916016, -0.28436556458473206, 0.009337730705738068, -1.1081440448760986, -0.2645561695098877, 0.7358641028404236, 0.7996155619621277, 0.7555250525474548, 0.2965753376483917, 0.5084152221679688, 0.8897866606712341, 0.1064455583691597, 0.06759921461343765, 0.2560776174068451, -0.08349736034870148, -0.44864797592163086, -0.1983250230550766, -0.0643034428358078, 0.9301289916038513, -0.6371783018112183, -0.9831773042678833, 0.456869512796402, 0.4619355797767639, 0.3357481360435486, 0.2580961287021637, 1.125207781791687, 0.2355106621980667, 0.6605275273323059, 0.07737985253334045, 0.8158804178237915, -0.8831923007965088, -0.4368038773536682, 0.11865528672933578, -0.35891395807266235, -0.0835011750459671, -0.0727219209074974, -0.28066352009773254, -0.1917746216058731, -0.21685540676116943, 0.3376149833202362, -0.12003758549690247, 0.1813868135213852, 1.0307809114456177, 0.5064854025840759, 0.8819949626922607, -0.3037862479686737, -0.39850282669067383, -0.6905360221862793, -0.9445866942405701, -0.08318859338760376, -0.5270110368728638, -0.37903887033462524, -0.06524574756622314, 0.08398967236280441, -0.3009728193283081]}, "authors": [{"authorId": "2180166734", "name": "Bharat Runwal"}, {"authorId": "22218771", "name": "Tejaswini Pedapati"}, {"authorId": "2282582379", "name": "Pin-Yu Chen"}], "references": [{"paperId": "b47a576d7e6208314791cc20e1b65894096a45b0", "title": "Minimizing Energy Consumption of Deep Learning Models by Energy-Aware Training"}, {"paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6", "title": "A Simple and Effective Pruning Approach for Large Language Models"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "7a91b4f00b76f7908363406c0ba424a71c2f86ab", "title": "DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models"}, {"paperId": "561f9f5abb2c0960a886ab6221c821295f0461a1", "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"}, {"paperId": "8ae292cbd9144acbf4b42b7ead82b079faf33192", "title": "Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference"}, {"paperId": "7a27cc0cc37931e85315ed41333f01cb6de18c02", "title": "Differentiable Subset Pruning of Transformer Heads"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "6346222f4d308dad8e716e0fd33be470f6e94cbb", "title": "Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "063f8b1ecf2394ca776ac61869734de9c1953808", "title": "AdapterHub: A Framework for Adapting Transformers"}, {"paperId": "deadba68c1c190be3974b6a66091e4182fae0f27", "title": "Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks"}, {"paperId": "7b43eb2e0c4b7605649a26c645d7a6f56a44ec9f", "title": "Adversarial Sparsity Attacks on Deep Neural Networks"}, {"paperId": "3b0fb765716ef6861a84abffcbe40643857c613b", "title": "Pruning neural networks without any data by iteratively conserving synaptic flow"}, {"paperId": "49325641f9fba5c5ee7814b239630980262e1bcf", "title": "Sponge Examples: Energy-Latency Attacks on Neural Networks"}, {"paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"}, {"paperId": "26299d5fdc5137291dc6a091573b3d18aba1d1c2", "title": "MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer"}, {"paperId": "c114ce10c4a315d92c3815f54bc9893e7e6ef182", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d", "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a", "title": "Fixing Weight Decay Regularization in Adam"}, {"paperId": "6dbb9e4b2e3b67dc4e1634989511f67d41373dd0", "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "3850ea898d4b31695f9900761e998d1cef838427", "title": "Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers"}, {"paperId": "aedd331f0ec4a0008089d4822df46bedca2aecb0", "title": "Exploring the"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation"}, {"paperId": null, "title": "Efficient Finetuning for Reducing Activation Density in Transformers on Neural Information"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}]}