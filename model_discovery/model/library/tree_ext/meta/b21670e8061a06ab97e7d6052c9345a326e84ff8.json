{"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms", "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 222, "influentialCitationCount": 31, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A unified framework for pre-training models that are universally effective across datasets and setups is presented and Mixture-of-Denoisers (MoD) is proposed, a pre- Training objective that combines diverse pre- training paradigms together."}, "embedding": {"model": "specter_v2", "vector": [0.4236829876899719, 0.7672580480575562, -0.6758772134780884, -0.15982013940811157, -0.3325020670890808, -0.515397310256958, 0.6462684869766235, 0.08021074533462524, -0.6036242246627808, 0.11075608432292938, 0.7323621511459351, -0.37465497851371765, 0.23209035396575928, 0.16345126926898956, -0.02888640947639942, 0.40649551153182983, -0.6360837817192078, 0.5521546602249146, -0.40999922156333923, -0.5554503202438354, -0.16109147667884827, -1.0543875694274902, -0.26113253831863403, 0.3601960837841034, 0.5679421424865723, -0.22989247739315033, 0.22529925405979156, 1.1395188570022583, -0.4825665354728699, 0.4829080104827881, 0.23236365616321564, -0.17071282863616943, 0.22441919147968292, -0.2024828940629959, -0.47643861174583435, 0.07759754359722137, 0.4692123830318451, -0.2775963842868805, -0.3755803406238556, 0.6747986674308777, 0.06370243430137634, 0.1343832165002823, 0.7415680289268494, -0.35814809799194336, -0.8325048685073853, 1.394448161125183, 0.804966151714325, 0.6667641997337341, 0.054089855402708054, -0.564536988735199, 1.6347159147262573, -1.1934064626693726, 0.6926260590553284, 1.48158860206604, 0.6559199690818787, 0.8133857846260071, -0.3570781946182251, -0.23131564259529114, 0.44899317622184753, 0.26645180583000183, -0.5373741388320923, -0.600391149520874, -0.5295867919921875, -0.004425245337188244, 1.8734796047210693, -0.35565003752708435, -0.5108976364135742, 0.37833306193351746, -0.300038605928421, 1.6622610092163086, -0.47586432099342346, -1.0009043216705322, -0.3853757679462433, 0.03891313076019287, 0.2411818504333496, 0.5290229320526123, -0.771348774433136, 0.03226294368505478, -0.6610965132713318, 0.2525375187397003, 0.3011949360370636, -0.01716127246618271, -0.6850261092185974, 0.1661231964826584, -0.39667654037475586, 0.6228291392326355, 0.4157556891441345, 1.1439260244369507, -0.1798599660396576, 0.4869241416454315, 0.6626587510108948, 0.7612523436546326, 0.14674142003059387, 0.6576220989227295, -0.4784739017486572, 0.27795934677124023, -1.1184446811676025, 0.2778938412666321, 0.15383823215961456, 0.6857340931892395, -0.19005779922008514, -0.1580652892589569, -1.0997090339660645, 0.3749028742313385, 1.3485108613967896, -0.17657878994941711, 0.5285847783088684, -1.091063141822815, 0.3527987003326416, -0.622258722782135, 0.4446661174297333, -0.2912435531616211, -0.3548903167247772, -0.165514275431633, -0.6802464127540588, -1.3585491180419922, -0.31059619784355164, -0.11435144394636154, -0.7864358425140381, 0.9831492900848389, -0.3991590142250061, -0.11005344241857529, 0.7142305970191956, 0.6047152280807495, 0.9523129463195801, 0.8444816470146179, 0.16153237223625183, -0.6431161165237427, 0.8534980416297913, -0.8096655011177063, -1.086403489112854, -0.995261013507843, 1.0648149251937866, -0.21978923678398132, 0.30571985244750977, -0.1830630749464035, -1.2584654092788696, -0.7276240587234497, -0.770829439163208, -0.49698635935783386, -0.3994714021682739, 0.6071866750717163, 0.6725932359695435, 0.38710179924964905, -0.8994504809379578, 0.9558123350143433, 0.21744565665721893, -0.5758978724479675, 0.11012754589319229, -0.4557301998138428, 0.1453809291124344, -0.6709833145141602, -1.307492971420288, 0.21478718519210815, 0.7199123501777649, -0.4012230634689331, -0.0941687673330307, -0.7969251871109009, -1.0801596641540527, -0.30849525332450867, 0.6347739100456238, -0.5056832432746887, 1.5539582967758179, 0.07798228412866592, -1.4950908422470093, 0.6969918608665466, -0.5941616296768188, -0.02963315322995186, 0.5724591016769409, -0.3785800337791443, -0.6019909381866455, -0.4539047181606293, 0.15337365865707397, 0.6614416241645813, 0.1540645807981491, -0.11373893171548843, -0.5163679122924805, 0.08670264482498169, -0.02957950532436371, 0.21438367664813995, 0.09511861950159073, 0.6534258127212524, -0.5488937497138977, -0.006819325499236584, 0.3291454613208771, 0.9682866334915161, 0.15031039714813232, -0.4021657109260559, -0.4161558449268341, -1.5167545080184937, 0.8001379370689392, 0.06238440051674843, 0.9768663048744202, -0.7581477165222168, -0.30461397767066956, -0.7474919557571411, -0.042313188314437866, 0.09518030285835266, -0.7055161595344543, 0.7661516070365906, -0.06885336339473724, 0.45223504304885864, -0.2908051013946533, -1.4017406702041626, 0.21458666026592255, 0.06286275386810303, -0.6775546073913574, -0.43721842765808105, 0.28983089327812195, 1.2193307876586914, -1.1524394750595093, 0.06504889577627182, 0.07482578605413437, -0.07044950872659683, -1.051534652709961, 1.0580573081970215, -0.9153594374656677, 0.14238552749156952, 0.16687476634979248, -0.173096165060997, -0.08760073781013489, -0.0879831463098526, 0.6585307121276855, -0.6181061863899231, -0.30178767442703247, 0.41992491483688354, -0.6285778880119324, 1.6007180213928223, -0.26341480016708374, 0.4346103072166443, 0.22105525434017181, -0.5782477259635925, -0.2871859669685364, 0.8501187562942505, -0.3808480203151703, -0.19979897141456604, 0.3286703824996948, 0.5046992897987366, -0.6757583022117615, 0.0019007549853995442, 0.9012582302093506, 0.5692060589790344, -0.18152888119220734, 0.23096399009227753, 0.8780716061592102, -0.4621773362159729, 1.1523867845535278, 0.5955896377563477, 0.894483208656311, 0.4248228967189789, 0.5635480284690857, -0.12137485295534134, 0.3595365285873413, -0.4220661222934723, 0.1513141542673111, 0.5135511159896851, 0.5031212568283081, 0.9448621273040771, 0.31637126207351685, -0.4165845513343811, -0.2898336946964264, 0.13012057542800903, 0.6681006550788879, 1.993664264678955, -0.2269306480884552, -0.39367350935935974, -0.6277855634689331, -0.3634582757949829, -0.3897159993648529, 0.618100106716156, -0.5280835032463074, 0.2554790675640106, -0.6010327339172363, -0.8163731694221497, 0.6872134208679199, 0.205035001039505, 0.8520026206970215, -0.7042542099952698, -0.05805259570479393, 0.043644510209560394, -0.0103466110303998, -0.7358322739601135, -0.5924427509307861, 0.5181496143341064, -0.3845309019088745, -0.5424123406410217, 0.1342163234949112, -0.27386367321014404, 0.03130543977022171, -0.5663294792175293, 1.188595175743103, -0.5140724778175354, 0.03916702792048454, 0.4654472768306732, 0.13684378564357758, -0.5256835222244263, -1.1058343648910522, 0.2917613983154297, 0.007884332910180092, -0.5473782420158386, 0.3577829599380493, 0.454725980758667, 0.40858694911003113, -0.09044316411018372, -0.6626603007316589, -0.06328589469194412, 0.30456024408340454, -0.1210939958691597, 0.6676474809646606, -0.15021660923957825, 0.2426920384168625, -1.6630300283432007, 0.9150274395942688, -0.08116684854030609, 0.07605918496847153, 0.4242003262042999, -0.6287950873374939, -0.6276305317878723, 0.5527402758598328, -0.6021426916122437, -0.5063985586166382, -1.1800837516784668, 0.5715901255607605, -0.10839974135160446, -0.39956244826316833, 0.4301939606666565, -0.02450660802423954, 0.9323631525039673, 0.6399403810501099, 0.28793197870254517, 0.2856634855270386, -0.2570326030254364, 1.187744379043579, -0.5050225257873535, 0.9004561901092529, 0.37384235858917236, 0.18880711495876312, -0.30658990144729614, -0.5533534288406372, -0.8282264471054077, -0.8005976676940918, -0.5810267925262451, -0.07527697831392288, -0.05949049070477486, 0.5224602818489075, -0.4350461959838867, -0.5346936583518982, -0.3026658594608307, -1.3398871421813965, -0.29953089356422424, -0.013832137919962406, -0.6316660046577454, -0.10640117526054382, -0.9752400517463684, -1.4249534606933594, -0.31694045662879944, -0.7064631581306458, -0.47074753046035767, 0.4649280607700348, 0.0817427858710289, -0.5531662702560425, -0.6481589078903198, 0.5262904763221741, -0.1399693340063095, 0.7316053509712219, -0.7187161445617676, 0.7810212969779968, 0.01851820759475231, 0.23537838459014893, 0.01481980737298727, 0.11848066002130508, 0.6895080804824829, -0.049002163112163544, 0.4015562832355499, -0.6075915694236755, 0.3004557192325592, -0.18059173226356506, -0.7242680191993713, 0.2503773868083954, 0.35107728838920593, -0.18502363562583923, -0.1248759999871254, -0.3977905213832855, 0.3216499984264374, 1.2533172369003296, -0.6406895518302917, 0.16211572289466858, 0.11313775181770325, 0.9429062008857727, 0.49559447169303894, -0.20690672099590302, 0.14949940145015717, 0.3359067440032959, -0.01261930912733078, 0.06203456595540047, 0.3974579870700836, -0.07856684178113937, -0.47208452224731445, 0.7331223487854004, 1.3917323350906372, 0.3009378910064697, -0.4307853877544403, -1.0078057050704956, 0.6962742209434509, -1.5971057415008545, -0.8261726498603821, 0.44365569949150085, 0.13900800049304962, 0.8708716630935669, -0.6478374600410461, -0.5572667121887207, -0.10394859313964844, 0.29959920048713684, 0.24790914356708527, -0.4091431796550751, -0.4101301431655884, -0.12332471460103989, 0.006675058044493198, -0.08350857347249985, 0.6863276362419128, -0.5285000801086426, 0.7060316801071167, 14.588868141174316, 1.003300428390503, 0.17834614217281342, 0.6244192719459534, 0.3350710868835449, 0.14922364056110382, -0.44142577052116394, -0.27574700117111206, -1.2307605743408203, -0.4720701277256012, 0.7899224758148193, 0.19808590412139893, 0.4961322546005249, -0.06268249452114105, 0.2348432093858719, 0.10159610211849213, -0.917282223701477, 0.48741403222084045, 0.49975958466529846, -1.40458083152771, 0.38557353615760803, -0.08970598131418228, 0.6820321679115295, 0.5112838745117188, 0.50396728515625, 1.194309949874878, 0.4650224447250366, -0.43588870763778687, 0.4587216079235077, 0.044080283492803574, 0.9522008895874023, 0.18088886141777039, 0.6407670974731445, 0.981921374797821, -0.40471985936164856, -0.35578230023384094, -0.548301637172699, -1.0999722480773926, 0.40492862462997437, 0.10588683187961578, -0.6458941698074341, -0.32256776094436646, -0.281019926071167, 0.905729353427887, 0.24871915578842163, -0.15062972903251648, -0.49895593523979187, 0.6027848124504089, -0.07321213185787201, 0.32335132360458374, 0.3287954032421112, 0.44944632053375244, 0.504332959651947, 0.27647557854652405, -0.22199653089046478, 0.2582904100418091, 0.2799023687839508, 0.4886605441570282, -0.8391966819763184, 0.17076820135116577, -0.18914629518985748, -0.3631954491138458, -0.09316302835941315, 0.706084132194519, 0.6735695600509644, 0.18801003694534302, -0.20898960530757904, 0.30322739481925964, 0.7815316915512085, -0.10634969919919968, 0.1642521768808365, -0.009302331134676933, -0.045155469328165054, -0.4556995630264282, 0.06473521888256073, 0.7052884101867676, 0.01904846914112568, -0.6708231568336487, -0.7276529669761658, -0.45218056440353394, 0.28802093863487244, -0.6747680902481079, -0.9532850980758667, 0.5805208086967468, -0.4393344223499298, -0.499755859375, -0.1570015698671341, -0.5784545540809631, -0.5401530265808105, -0.02493789978325367, -1.4930610656738281, -0.68645179271698, 0.4373649060726166, -0.4504728317260742, -0.23058633506298065, 0.18175777792930603, 1.1952083110809326, -0.002872732002288103, -0.5530195236206055, -0.1978919804096222, 0.5651269555091858, -0.008549773134291172, 0.22041703760623932, -1.2453234195709229, 0.5061359405517578, 0.32614409923553467, -0.2111186981201172, 0.17035436630249023, 0.3271254003047943, 0.02941654436290264, -0.790501058101654, -0.29704800248146057, 0.8497521281242371, -1.0403850078582764, -0.5486742854118347, -0.29895442724227905, -0.9681390523910522, 0.33683744072914124, 0.8337736129760742, -0.38363850116729736, 0.27119964361190796, 0.21836115419864655, -0.3897496163845062, 0.26138055324554443, -1.0760687589645386, 0.08760301023721695, 0.7535086870193481, -0.47325995564460754, -0.9442867040634155, 0.21517343819141388, 0.6423539519309998, -1.0685644149780273, -0.593433141708374, -0.310376912355423, -0.20564959943294525, 0.06208033859729767, 0.5368430018424988, -0.3883824944496155, 0.7160322070121765, 0.6655338406562805, -0.2300422340631485, -0.8869031667709351, 0.013734379783272743, -1.1417957544326782, -0.1200946494936943, 0.7154004573822021, 0.8570535182952881, -0.22850476205348969, 0.25558531284332275, 0.6436383128166199, 0.22497425973415375, -0.16885319352149963, -0.54750657081604, -0.28541192412376404, 0.2724289894104004, -0.6587085723876953, -0.008971557021141052, 0.05497284606099129, 0.16709642112255096, 0.606301486492157, 0.6246698498725891, 0.6307945251464844, -0.19266480207443237, -0.9241676926612854, 0.439124196767807, -0.1552610844373703, -0.20090292394161224, -0.44516050815582275, -0.0691334530711174, -1.2674810886383057, -0.04527634382247925, -1.0346959829330444, 0.27128124237060547, -1.3621538877487183, -0.6110048294067383, 0.3826275169849396, 0.17559237778186798, 0.08671596646308899, 0.04925953596830368, -0.33570969104766846, -0.27822020649909973, -0.3514515161514282, -0.7983304262161255, 0.6894835829734802, 0.7858014106750488, -0.7084002494812012, -0.22950918972492218, 0.012409045360982418, -0.10824550688266754, 0.41914355754852295, 0.5334135293960571, -0.17204737663269043, -0.7874746322631836, -1.5436272621154785, 0.4789068400859833, 0.003622901625931263, -0.307094007730484, -0.5177517533302307, 0.6235131621360779, 0.5446199774742126, -0.4877258539199829, -0.0870945155620575, -0.10339022427797318, -0.8575157523155212, -0.7463855743408203, -0.1277000606060028, -0.8875718712806702, -0.2356758415699005, -0.020797831937670708, -0.5561556816101074, -0.30180981755256653, 0.24070857465267181, -0.07153662294149399, -1.2245104312896729, -0.7162871360778809, 0.0008767982362769544, -0.38461756706237793, 0.6277426481246948, -0.42698153853416443, 0.0864742323756218, -1.0384364128112793, -0.2514577805995941, -0.08719239383935928, 0.7387568950653076, -0.07483220845460892, 0.7143077254295349, 0.039031967520713806, -0.9218957424163818, -0.3820386826992035, -0.10476434975862503, -0.059606291353702545, 0.42445433139801025, 0.6088806986808777, 0.13615287840366364, -0.216318279504776, 0.40537774562835693, 0.4954839050769806, 0.05216512084007263, -0.7497302889823914, -0.21197344362735748, 0.8107805252075195, -0.6955242156982422, 0.0367460697889328, 1.0815726518630981, -0.8144084215164185, -1.5253722667694092, 0.15575431287288666, -1.1337581872940063, -0.818465530872345, 0.061665333807468414, 0.8170633912086487, 0.5465954542160034, -0.15074121952056885, -0.040876615792512894, -0.3631465435028076, 0.2395879626274109, -0.2334516942501068, -0.6075080633163452, 0.7969818115234375, -0.4817711412906647, -0.5667604207992554, 0.5742716789245605, 0.7340874075889587, -0.5657855868339539, -0.6033077239990234, -0.8606696128845215, -0.31075748801231384, 0.3894292712211609, 0.5147457718849182, -0.625069797039032, -0.15420477092266083, 0.5313935875892639, 0.06685777008533478, 0.6059103608131409, 0.0773809477686882, -0.17049084603786469, 0.32553282380104065, 0.9438303112983704, -0.2595899999141693, -0.7793210744857788, -0.7798371315002441, 1.358538269996643, 1.2206552028656006, -1.1246800422668457, 0.2308836579322815, 0.13448330760002136, -0.7042233943939209, 1.0310914516448975, 0.31185269355773926, 0.022326333448290825, 0.5313388109207153, -0.4130982756614685, 0.3472253680229187, 0.1505585014820099, -1.094185471534729, -0.28635066747665405, 0.6805627346038818, 1.0084993839263916, 0.8284898400306702, 0.17730893194675446, -0.006872940342873335, 0.7958306670188904, 0.1867460310459137, 0.4641815423965454, 0.7097327709197998, 0.5648617148399353, -0.2566881477832794, -0.039848510175943375, 0.1836201548576355, 0.4551173448562622, -0.5885311365127563, -0.49182063341140747, 0.19808150827884674, 0.44964030385017395, 0.6292206048965454, 0.8586969971656799, 0.5222405195236206, 0.5136942863464355, 0.511885404586792, 0.342272013425827, 0.454588383436203, -0.931264340877533, -0.1501701921224594, -0.32704558968544006, -0.6091623902320862, -0.040122274309396744, -0.3681477904319763, -0.6245474219322205, -0.4852747321128845, -0.15507648885250092, 0.28227469325065613, 0.16142882406711578, 0.10531751066446304, 1.4587156772613525, 0.7009643912315369, 0.3490790128707886, -0.5175685882568359, -0.5650574564933777, -0.5166112780570984, -1.5061097145080566, 0.054375648498535156, -0.4789994955062866, -0.27846819162368774, 0.17075712978839874, -0.38044488430023193, -0.1128125712275505]}, "authors": [{"authorId": "144447820", "name": "Yi Tay"}, {"authorId": "3226635", "name": "Mostafa Dehghani"}, {"authorId": "2057663102", "name": "Vinh Q. Tran"}, {"authorId": "143936294", "name": "Xavier Garc\u00eda"}, {"authorId": "119640649", "name": "Jason Wei"}, {"authorId": "1524732527", "name": "Xuezhi Wang"}, {"authorId": "3351938", "name": "Hyung Won Chung"}, {"authorId": "2119725651", "name": "Dara Bahri"}, {"authorId": "32303439", "name": "Tal Schuster"}, {"authorId": "2115689465", "name": "H. Zheng"}, {"authorId": "65855107", "name": "Denny Zhou"}, {"authorId": "2815290", "name": "N. Houlsby"}, {"authorId": "1680617", "name": "Donald Metzler"}], "references": [{"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "663a41c866d49ce052801fbc88947d39764cad29", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "e47da75675b9a3fe02ef1efadca39bc8cdfcdc17", "title": "Designing Effective Sparse Expert Models"}, {"paperId": "15190e8b459bd85d546286f7d7da61b4f4f3f58a", "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "1ed66e048bb025e75aa5ea660545285212e5341f", "title": "Scaling Up Models and Data with t5x and seqio"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "318e22ccebf0e45816a8397ace680b1c9542f46e", "title": "HyperPrompt: Prompt-based Task-Conditioning of Transformers"}, {"paperId": "5b44101b2372a33ec06e15ce4d20ad9a15518325", "title": "UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training"}, {"paperId": "7016eb4f34611f97fe8c99176246e314678e03f4", "title": "A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"}, {"paperId": "9b3fa3a80afdb6d34984307e457656420e60e7e7", "title": "Should You Mask 15% in Masked Language Modeling?"}, {"paperId": "8db711adf1beb3e0c2ec492f3936841d827404e9", "title": "The NLP Task Effectiveness of Long-Range Transformers"}, {"paperId": "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb", "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text"}, {"paperId": "9d40837175577bb0009b138269b422f6d5820d00", "title": "Transformer Memory as a Differentiable Search Index"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "79950179d60ba39a74d5fe2aedc47a57c0bf4c03", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"}, {"paperId": "6281c40c66febca1d8003bcc6fdfd2189b30c38f", "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences"}, {"paperId": "3c209e0703ffff26231b1145268c935df494631a", "title": "QuALITY: Question Answering with Long Input Texts, Yes!"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "cbf98ebe967e0f3f3236e7932f37013b98244e94", "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "66d735987a31d666a6459566ae026c40ab9a1c3a", "title": "The Efficiency Misnomer"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "0a1ff1d4102d94a50f8862f60bc2ac21f36ad592", "title": "ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts"}, {"paperId": "2d4f66046bb436864cd6bf589e3a931c405f9f44", "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"}, {"paperId": "395aae6e7a79e5760457ca38e868acc970016230", "title": "MATE: Multi-view Attention for Table Transformer Efficiency"}, {"paperId": "e596b8adbffa546dbc163e817fb3de72744ec4f6", "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models"}, {"paperId": "e7735be4f0fc427515fd7206ebc714356b97e71a", "title": "The Benchmark Lottery"}, {"paperId": "e79d1206292bc5e67ba19737d87d4b2ea4a37105", "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"}, {"paperId": "42f8a3da7021bc725fa14fdb63fa9c7c9fc934f6", "title": "DocNLI: A Large-scale Dataset for Document-level Natural Language Inference"}, {"paperId": "f7664102a451332ed7e1286561b2f621eaff128d", "title": "Programming Puzzles"}, {"paperId": "d65a064eb837f838faf6ff67781b62450b92b159", "title": "CommonsenseQA 2.0: Exposing the Limits of AI through Gamification"}, {"paperId": "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"}, {"paperId": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"}, {"paperId": "2365410a710b421b2295cdca0074946cb50bb1d4", "title": "Are Pretrained Convolutions Better than Pretrained Transformers?"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "2d81e83b8039bd144c61be15201d0ddb6a88d86e", "title": "Planning with Learned Entity Prompts for Abstractive Summarization"}, {"paperId": "43eee0596facea9faf72e3ab3a734d32a6aa82f1", "title": "SummScreen: A Dataset for Abstractive Screenplay Summarization"}, {"paperId": "9dc624d7258d1a56117ca720aea953ce46b66b21", "title": "Efficient Attentions for Long Document Summarization"}, {"paperId": "388513e8e09ad60f619054361f4d2cdf5a146bc8", "title": "FeTaQA: Free-form Table Question Answering"}, {"paperId": "21ec9c0f869bdb33b06c7dbc8880169db0397d08", "title": "UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark"}, {"paperId": "aa28873534c24e4a8c5deb7bff723cd5fc69a6f0", "title": "QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization"}, {"paperId": "eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a", "title": "Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence"}, {"paperId": "13c4e5a6122f3fa2663f63e49537091da6532f35", "title": "Are NLP Models really able to Solve Simple Math Word Problems?"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "824cd8db8a68732db04f4d8b7139eb4475e59ff2", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "346081161bdc8f18e2a4c4af7f51d35452b5cb01", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "a79c6fd3da1c3eafc228a0e429846ff048027689", "title": "InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "a8a168d53e01b0c35d626cfced103656e22b8343", "title": "MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark"}, {"paperId": "b88c11922cac84e5ea902f82d27ae21c3dda2e04", "title": "Better Fine-Tuning by Reducing Representational Collapse"}, {"paperId": "03d8cd1d4bbdc0f6f07bdd0412997606b3648e78", "title": "HyperGrid: Efficient Multi-Task Transformers with Grid-wise Decomposable Hyper Projections"}, {"paperId": "6e3f8187f8fef3e11578a73f32da07d33dbf8235", "title": "DART: Open-Domain Structured Data Record to Text Generation"}, {"paperId": "f13e41d24e5d0a68ca662c1b49de398a6fb68251", "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "ab5f0004c5f3317689e8457e1c8d8390ccbee522", "title": "A domain-specific supercomputer for training deep neural networks"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ad5970584754cc7a1d91c95ab84a1e210258183a", "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System"}, {"paperId": "47fd2e73a9be04ed2186c03d8f88d5c87f64e4e4", "title": "ToTTo: A Controlled Table-To-Text Generation Dataset"}, {"paperId": "27db72a2f643f9dfebc0cc2e8b98a9db307f0f07", "title": "HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data"}, {"paperId": "a87f0bac2ed58e8a79d33d0c1cf81c6407cd645f", "title": "Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks"}, {"paperId": "ad88c2e5da6c911a4a0ac44d1058c82253d07aef", "title": "Multi-task Learning with Multi-head Attention for Multi-choice Reading Comprehension"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "0119a57cf88ef16e6dc291252fae340bb6b3953c", "title": "CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "f9700e31a1d0ae34d4571ab056dfb268c1543349", "title": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "9e1241f017a627beca2542e378a88c642c32098b", "title": "Semantic Noise Matters for Neural Natural Language Generation"}, {"paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"paperId": "c9b051b29feda7b62a4b683b1dfc37408724d8f5", "title": "QASC: A Dataset for Question Answering via Sentence Composition"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "674b6321ae1d12c83f28ade1850a27256c20f0d4", "title": "Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset"}, {"paperId": "ee4e24bdedd4d2e4be977bd0ca9f68a06ebb4d96", "title": "TabFact: A Large-scale Dataset for Table-based Fact Verification"}, {"paperId": "66117f82def0c69a3b9cc77eb3e2694b0245ca86", "title": "Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning"}, {"paperId": "a550f576ff20b8cce98f3ddad0043d3783fbc9b4", "title": "Abductive Commonsense Reasoning"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b", "title": "WinoGrande"}, {"paperId": "b00cda1bf34cc674676bcde38a46e8fe86d8b825", "title": "TWEETQA: A Social Media Focused Question Answering Dataset"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "cc27ec53160d88c25fc5096c0df65536eb780de4", "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "b611a8095630557229dc5fb6b07c272f1cd614da", "title": "Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "c8725f13be7434b69738491c66b45c9225258253", "title": "The Web as a Knowledge-Base for Answering Complex Questions"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a", "title": "The NarrativeQA Reading Comprehension Challenge"}, {"paperId": "cbd569036fc72ae7ff747350b91816440282596b", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"}, {"paperId": "b123a0d46ad917b79c43c5ae981e03ed2458ed11", "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"}, {"paperId": "8ff54aa8045b1e30c348cf2ca42259c946cd7a9e", "title": "Search-based Neural Structured Learning for Sequential Question Answering"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9", "title": "MAWPS: A Math Word Problem Repository"}, {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "b41e95c8c97846d5ca4c11ef79d7814499cc9663", "title": "Compositional Semantic Parsing on Semi-Structured Tables"}, {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "fa1d573a466ea6ab189a21036bdcb9bb1e923bba", "title": "Cryptosporidium Parvum Genome Project"}, {"paperId": "48151071d50240df76df400910dbcb5c65dabc07", "title": "The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) Shared Task"}, {"paperId": "acf2dd4e2853f90832c01c556a2e716e7c720bc2", "title": "G ENIE A Leaderboard for Human-in-the-Loop Evaluation of Text Generation"}, {"paperId": "f83618f13fce0e71e9127784f6ecc261dbdbf089", "title": "Structure-to-Text Generation with Self-Training, Acceptability Classifiers and Context-Conditioning for the GEM Shared Task"}, {"paperId": "4574d7867ec346b17cba4a9c6cd6738776710bf2", "title": "Control Prefixes for Text Generation"}, {"paperId": "2a7023e7d1dbd6ea0d98efd09a1f18d8599fe78f", "title": "PRIMER: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization"}, {"paperId": "310b8117ae5ce3df8aa6304ad382525b9b46937e", "title": "The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task: Overview and Evaluation Results (WebNLG+ 2020)"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": null, "title": "Socialiqa: Commonsense reasoning about social interactions"}, {"paperId": null, "title": "Acl 2019 fourth conference on machine translation (wmt19), shared task: Machine translation of news. URL http://www.statmt.org/wmt19/translation-task.html"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": null, "title": "sequence_length: dict mapping of feature key to int length for that feature"}, {"paperId": null, "title": "Neil and Donald served as technical advisors and sponsors to the project and helped brainstorm, provide feedback and writing of the paper"}, {"paperId": null, "title": "ds = t5.data.preprocessors.select_random_chunk( ds,output_features=output_features, feature_key=\"targets\", max_length=65536"}, {"paperId": null, "title": "inputs_length = sequence_length[input_feature_key"}, {"paperId": null, "title": "input_feature_key: which feature to use from the dataset as the input text"}, {"paperId": null, "title": "times with different values of \u2018noise_density\u2018 and \u2018mean_noise_span_length\u2018"}, {"paperId": null, "title": "for mean_noise_span_length, noise_density in hyperparams: input_length, targets_length = t5.data.preprocessors.random_spans_helper"}, {"paperId": null, "title": "We either shard or copy the dataset"}, {"paperId": null, "title": "This preprocessor amounts to calling the \u2018span_corruption\u2018 function several"}, {"paperId": null, "title": "reserved_for_packing: if specified, reduces the desired inputs length by the specified amount to enable multiple examples to be packed together downstream"}, {"paperId": null, "title": "comparison over long"}, {"paperId": null, "title": "merge_examples_to_reduce_padding: if True, combines multiple input examples"}]}