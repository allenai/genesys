{"paperId": "05d70085d1b580b2369942410ae77c48d1eeacca", "title": "Exploring Extreme Parameter Compression for Pre-trained Language Models", "abstract": "Recent work explored the potential of large-scale Transformer-based pre-trained models, especially Pre-trained Language Models (PLMs) in natural language processing. This raises many concerns from various perspectives, e.g., financial costs and carbon emissions. Compressing PLMs like BERT with negligible performance loss for faster inference and cheaper deployment has attracted much attention. In this work, we aim to explore larger compression ratios for PLMs, among which tensor decomposition is a potential but under-investigated one. Two decomposition and reconstruction protocols are further proposed to improve the effectiveness and efficiency during compression. Our compressed BERT with ${1}/{7}$ parameters in Transformer layers performs on-par with, sometimes slightly better than the original BERT in GLUE benchmark. A tiny version achieves $96.7\\%$ performance of BERT-base with $ {1}/{48} $ encoder parameters (i.e., less than 2M parameters excluding the embedding layer) and $2.7 \\times$ faster on inference. To show that the proposed method is orthogonal to existing compression methods like knowledge distillation, we also explore the benefit of the proposed method on a distilled BERT.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 16, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2205.10036", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work aims to explore larger compression ratios for PLMs, among which tensor decomposition is a potential but under-investigated one, and shows that the proposed method is orthogonal to existing compression methods like knowledge distillation."}, "embedding": {"model": "specter_v2", "vector": [0.17953185737133026, 0.5043827295303345, -0.4678403437137604, -0.08791021257638931, -0.28354302048683167, -0.40758171677589417, 0.6485949158668518, -0.3190188705921173, -0.6531680822372437, -0.09815702587366104, 0.8183318376541138, -0.4276416599750519, 0.35732606053352356, 0.1226952075958252, -0.2823478579521179, 0.0037373611703515053, -0.8761216998100281, 0.36504533886909485, -0.4962993264198303, -0.1989303082227707, -0.20770587027072906, -0.51358562707901, -0.6413916349411011, -0.041560541838407516, 0.36822718381881714, 0.45948663353919983, 0.10294956713914871, 0.8757663369178772, -0.6194416284561157, 0.29740679264068604, 0.8103668093681335, -0.8380407094955444, 0.30059364438056946, 0.4285997450351715, -0.26214686036109924, -0.07436327636241913, 0.030405312776565552, -0.7301331162452698, -0.45621755719184875, 0.7768758535385132, -0.5072935223579407, 0.10834792256355286, 0.4442656934261322, -0.7818637490272522, -0.26731833815574646, 1.0629222393035889, 0.8231686353683472, 0.7945325374603271, -0.45639410614967346, -0.3187496066093445, 1.5535869598388672, -1.2269132137298584, 0.07795485854148865, 1.146389365196228, 0.4128083884716034, 0.18366535007953644, -0.40246325731277466, -0.7585550546646118, 0.27725347876548767, 0.10728872567415237, -0.647118866443634, -0.22126078605651855, 0.08290780335664749, -0.13777582347393036, 1.7707860469818115, -0.3768192231655121, 0.3014237582683563, 0.78087317943573, -0.4506904184818268, 1.2658278942108154, 0.1633840948343277, -0.6789853572845459, -0.31723034381866455, 0.3148800730705261, 0.08493517339229584, 0.9003075957298279, -0.23797063529491425, 0.4661504626274109, -0.8004748821258545, -0.03538011386990547, 0.21127839386463165, 0.10857291519641876, -0.01993323303759098, 0.08277294039726257, -0.4985227584838867, 0.891341507434845, 0.22181740403175354, 0.8113669753074646, -0.24084900319576263, 0.9983948469161987, 0.975629985332489, 0.5123564004898071, 0.08300372213125229, 0.4229199290275574, -0.31397679448127747, 0.05460699647665024, -1.1309202909469604, 0.13270580768585205, 0.3179163336753845, 0.5134232640266418, 0.04837297648191452, 0.3833411633968353, -0.5617377758026123, -0.2437504380941391, 1.2345141172409058, 0.16938000917434692, 0.45295917987823486, -0.8709899187088013, 0.40755945444107056, -0.8895859122276306, 0.011401964351534843, -0.8363837599754333, 0.094204843044281, -0.26462212204933167, -1.095106840133667, -1.9894767999649048, -0.7298687696456909, 0.11593742668628693, -0.8592417240142822, 0.6518774032592773, -0.6860123872756958, 0.6191962361335754, 0.26893705129623413, 0.32330572605133057, 0.2775247097015381, 1.016389012336731, 0.11858370900154114, 0.0691802129149437, 1.1283351182937622, -0.8876575231552124, -0.8275888562202454, -1.0681519508361816, 0.8515690565109253, -0.02899487502872944, 0.32948195934295654, -0.11874005198478699, -1.000601887702942, -0.5549958348274231, -0.6906518340110779, 0.01930290274322033, -0.6299344897270203, 0.41223806142807007, 1.2251079082489014, 0.6932880878448486, -0.9874423742294312, 0.6752482652664185, -0.5392017364501953, -0.015200609341263771, 0.27291980385780334, 0.013398358598351479, 0.19080416858196259, -0.3558421730995178, -1.4847092628479004, 0.44520142674446106, 0.6278120279312134, -0.3741518259048462, -0.3225339353084564, -0.6900750994682312, -1.1717454195022583, 0.23468945920467377, 0.2787395715713501, -0.5036420226097107, 1.1614364385604858, 0.4472799003124237, -1.4079430103302002, 0.6963111162185669, -0.2930135726928711, -0.113929383456707, 0.056441981345415115, -0.26873359084129333, -0.5194246768951416, -0.07720401138067245, -0.2465536743402481, 0.21206939220428467, 0.43442967534065247, -0.19553211331367493, 0.1569584310054779, 0.45641160011291504, -0.1391412615776062, -0.17901384830474854, -0.27361539006233215, 0.9194552898406982, -0.20745080709457397, -0.20988191664218903, 0.011899256147444248, 0.5561882257461548, -0.27059856057167053, -0.263512521982193, -0.7363092303276062, -0.717712938785553, 0.6197583079338074, -0.3536418676376343, 0.939826250076294, -1.1181100606918335, -0.5261273384094238, 0.07485488057136536, 0.055571962147951126, 0.014931348152458668, -0.7679309248924255, 0.7622266411781311, -0.4510953426361084, 0.8378316164016724, -0.24459902942180634, -1.4108587503433228, 0.4617708921432495, 0.1354673206806183, -1.2511529922485352, -0.4701480567455292, 0.07045292854309082, 1.0840108394622803, -0.6587866544723511, 0.2739642560482025, 0.10170349478721619, 0.49531710147857666, -1.3706047534942627, 1.2054327726364136, -0.4932146370410919, -0.01067349873483181, 0.13372506201267242, -0.21869580447673798, 0.02622777596116066, -0.2334039956331253, 0.2186787724494934, -0.624282956123352, -0.023565074428915977, 0.5474674105644226, -0.33312472701072693, 1.2665104866027832, -0.5386634469032288, 0.5539414882659912, 0.03532759100198746, -1.0142241716384888, 0.18921786546707153, 0.5813665390014648, -0.20228706300258636, 0.17465029656887054, 0.2282339632511139, 0.702014148235321, -0.6263724565505981, 0.07937870174646378, 0.8205808401107788, 0.2193886786699295, -0.40584075450897217, 0.25491538643836975, 0.6625429391860962, -0.6352561116218567, 0.7170751094818115, 0.5922289490699768, 0.34614434838294983, 0.06719234585762024, 0.1989990621805191, 0.23320689797401428, 0.2784992754459381, -0.9834656119346619, -0.08159969747066498, 0.1144467294216156, 0.6045485138893127, 0.4283868372440338, 0.6320237517356873, -0.5189362168312073, -0.7096920609474182, 0.1717313528060913, 0.7386550307273865, 1.2137750387191772, -0.6467255353927612, -0.8759868741035461, -0.462834894657135, -0.09206809848546982, -0.2640247642993927, 0.3745940327644348, -0.11522296071052551, -0.1998593658208847, -0.4079091250896454, -1.3009523153305054, 1.0840526819229126, 0.20431062579154968, 1.2499312162399292, -0.10059168189764023, -0.00625181756913662, -0.37629663944244385, -0.032566849142313004, -1.1454696655273438, -0.3420407772064209, 0.44582515954971313, -0.5253625512123108, 0.03461340814828873, 0.14386436343193054, 0.26324090361595154, -0.1271020770072937, -0.8253437280654907, 0.7048947811126709, -0.5419443845748901, -0.09285086393356323, 0.21609017252922058, 0.6044426560401917, -0.3104819655418396, -0.9292998909950256, 0.1269727498292923, 0.15667852759361267, -0.29228565096855164, 0.3700098693370819, 0.15190213918685913, 0.1484542340040207, -0.36469587683677673, -0.6511921286582947, 0.21041364967823029, 0.007260145153850317, -0.09009906649589539, 0.3861250877380371, -0.04657362774014473, -0.41505616903305054, -0.9913933873176575, 0.8106396198272705, 0.09619288891553879, -0.25156170129776, 0.09914904832839966, -0.80056232213974, 0.012177216820418835, 0.6657811999320984, -0.45628228783607483, 0.11726096272468567, -1.1941465139389038, 0.22495052218437195, -0.34344521164894104, -0.10233400017023087, 0.6923659443855286, 0.3690266013145447, 0.5303986072540283, -0.0863010436296463, 0.6485157608985901, 0.1819838285446167, -0.34560298919677734, 0.779886782169342, -0.8102163076400757, 0.2974581718444824, 0.19962483644485474, 0.6257423162460327, -0.3670538365840912, -0.1591894030570984, -0.5133113861083984, -0.5957292914390564, -0.2229260802268982, -0.23257240653038025, 0.06824344396591187, 0.30935540795326233, -0.7058992981910706, -0.5149505138397217, -0.09575055539608002, -1.0137372016906738, -0.09971308708190918, -0.016241006553173065, -0.23690974712371826, -0.2067374587059021, -0.8830261826515198, -1.4840768575668335, -0.4362960159778595, -0.5501135587692261, -1.132684350013733, 0.6444243788719177, -0.2937369644641876, 0.13433226943016052, -0.9002657532691956, -0.06644000858068466, -0.15924087166786194, 1.3268086910247803, -0.525558352470398, 1.0767097473144531, -0.16789016127586365, -0.194328173995018, -0.08275462687015533, 0.011326957494020462, 0.9648488759994507, -0.4775174558162689, 0.5962806940078735, -0.7061654925346375, 0.07931698858737946, -0.4506498873233795, -0.31746718287467957, 0.3613324463367462, -0.026419470086693764, 0.6448668837547302, -0.4508673846721649, -0.3569474518299103, 0.7911656498908997, 1.4139214754104614, -1.0641294717788696, 0.40580904483795166, 0.06812207400798798, 1.0021625757217407, -0.21574334800243378, -0.7352950572967529, 0.5806359052658081, 0.37239980697631836, 0.08708945661783218, -0.034447673708200455, -0.10493043810129166, 0.18386578559875488, -0.6694974899291992, 0.8186567425727844, 2.3060801029205322, 0.09320498257875443, 0.02564598247408867, -0.9912256598472595, 0.3228478729724884, -0.6591737866401672, -0.8051694631576538, 0.7475972175598145, 0.40988683700561523, 0.6816968321800232, -0.3514004349708557, -0.5043113827705383, 0.2985967695713043, 0.19586153328418732, 0.6148542165756226, -0.19903403520584106, -0.9408721327781677, 0.054362669587135315, 0.4541704058647156, 0.45531561970710754, 0.5957441329956055, -0.32477372884750366, 0.6140207648277283, 14.732833862304688, 0.8133839964866638, -0.2239813506603241, 0.4957520365715027, 0.7607736587524414, 0.4880548417568207, -0.5147322416305542, 0.22082458436489105, -1.0541038513183594, 0.025522153824567795, 1.6262059211730957, -0.04287378117442131, 0.8441289663314819, -0.04678080230951309, -0.04278910160064697, 0.37197157740592957, -0.3514774441719055, 0.6343836188316345, 0.478461354970932, -1.4183311462402344, 0.3931367099285126, 0.406735360622406, 0.18817952275276184, 0.5157114267349243, 0.8851115703582764, 1.1007994413375854, 0.40886661410331726, -0.584388792514801, 0.3785107135772705, 0.1132977083325386, 1.326663613319397, -0.03622949495911598, 0.5336663126945496, 0.740321934223175, -0.898735523223877, -0.29548197984695435, -0.4154045581817627, -1.1839606761932373, 0.3546713888645172, 0.5315191149711609, -0.7308838963508606, -0.3068847954273224, -0.3909718096256256, 1.1281366348266602, 0.1471809446811676, 0.05733757093548775, -0.3157723844051361, 1.1511123180389404, -0.49200931191444397, 0.25749319791793823, 0.45660457015037537, 0.13086774945259094, 0.31147754192352295, 0.025225738063454628, 0.15635189414024353, -0.2972571551799774, 0.3008858263492584, 0.43750008940696716, -0.4960658848285675, -0.20620480179786682, -0.35645052790641785, -0.8815746903419495, -0.05044831335544586, 0.6718756556510925, 0.6772159934043884, 0.11156642436981201, -0.306976318359375, 0.294424831867218, 0.5108076333999634, 0.3934856057167053, -0.3027920722961426, 0.09464431554079056, 0.19492730498313904, -0.4158179461956024, 0.03054301254451275, 0.2002849131822586, 0.09084654599428177, -0.8387376666069031, -0.690044105052948, -0.38635727763175964, 0.15426072478294373, -0.7825419902801514, -0.6613363027572632, 0.6206743717193604, -0.047234710305929184, -0.12826058268547058, -0.017167281359434128, -0.9003345370292664, 0.007144346367567778, 0.245217427611351, -1.7642178535461426, -0.5522313714027405, 0.604560375213623, -0.30148622393608093, -0.8698031902313232, 0.0019582773093134165, 1.6675578355789185, 0.27347931265830994, -0.6162605285644531, -0.08120749890804291, 0.40591663122177124, 0.294127494096756, -0.5506271719932556, -0.9895336627960205, 0.6819117069244385, 0.29188233613967896, -0.024601243436336517, 0.29595208168029785, 0.06628526002168655, 0.34544429183006287, -0.8267955780029297, -0.1801263839006424, 0.9297402501106262, -0.73928302526474, 0.006400310434401035, -1.0117576122283936, -0.4208165109157562, 0.5263853073120117, 0.45847296714782715, -0.3664224147796631, 0.6639383435249329, -0.15532605350017548, -0.8380998373031616, -0.12809190154075623, -0.8800567388534546, 0.04946053773164749, 0.32664385437965393, -1.1568716764450073, -0.15884290635585785, 0.24461773037910461, 0.5183273553848267, -0.643629789352417, -0.6880111694335938, 0.05415418744087219, -0.04547157138586044, 0.0006142039783298969, 0.894692599773407, -0.2560499608516693, 1.0091849565505981, 1.0025159120559692, -0.5610101819038391, -0.9763996601104736, 0.13926884531974792, -1.0383927822113037, -0.1372796893119812, 0.03134123980998993, 0.6197736859321594, -0.4347444474697113, 0.11612296849489212, 0.892662525177002, 0.28541287779808044, -0.48218539357185364, -0.6892902851104736, -0.2862156629562378, 0.04876590520143509, -0.54546058177948, 0.2891766428947449, 0.02672656998038292, -0.03141103684902191, 0.2711118459701538, 0.277362585067749, 0.3202119767665863, -0.3257473409175873, -0.9644172787666321, 0.1809694766998291, 0.12012527883052826, -0.18298861384391785, -0.32683661580085754, -0.6365486979484558, -1.2878715991973877, 0.26617422699928284, -1.516256332397461, -0.2330504059791565, -0.9159221053123474, -0.23850636184215546, 0.04722227901220322, -0.09827431291341782, 0.21519924700260162, 0.456417977809906, -0.10225115716457367, -0.31550443172454834, -0.48687466979026794, -0.004948326852172613, 1.0403814315795898, 0.8085078597068787, -0.6661974787712097, 0.3884393870830536, -0.23197567462921143, 0.14785617589950562, 0.2396419197320938, 0.40327274799346924, -0.23512603342533112, -0.8314386010169983, -1.347840428352356, 0.6225210428237915, -0.29601985216140747, 0.04525597020983696, -0.34754714369773865, 0.2944367229938507, 0.3609667718410492, -0.6874097585678101, 0.4296848475933075, 0.4719891846179962, -1.0561836957931519, -0.34324753284454346, 0.45758765935897827, -0.7647651433944702, 0.4158857762813568, 0.02074199542403221, -0.4276585876941681, -0.37362468242645264, 0.6675115823745728, -0.22917531430721283, -1.018445611000061, -0.4730299711227417, 0.5094444751739502, -0.4706196188926697, -0.03036990948021412, -0.4519687294960022, -0.015536717139184475, -1.1162854433059692, -0.2702803611755371, -0.11692596971988678, 0.027294404804706573, -0.45437923073768616, 0.465634822845459, 0.31234011054039, -1.05753493309021, -0.06822554767131805, 0.41005048155784607, -0.29901137948036194, -0.2521663308143616, 0.17057418823242188, 0.41239356994628906, -0.42173323035240173, 0.6419325470924377, 0.6622908711433411, 0.39082786440849304, -0.4327471852302551, -0.06359442323446274, 0.7678356170654297, -0.8341573476791382, -0.42351534962654114, 1.0541824102401733, -0.7999213933944702, -1.0315901041030884, 0.2697617709636688, -1.6867859363555908, -0.43056437373161316, -0.4866649806499481, 0.35052424669265747, -0.05075550451874733, -0.19784684479236603, 0.07421751320362091, -0.4268665313720703, 0.14021462202072144, 0.06559013575315475, -0.6980333924293518, 0.7989377379417419, -0.12013360112905502, -0.6879839897155762, 0.7234618067741394, 1.1409720182418823, -0.6350768208503723, -0.3288921117782593, -0.7585107684135437, -0.4607897698879242, -0.30296939611434937, 0.6181266903877258, -0.4965532720088959, -0.7239058613777161, 0.9096322059631348, 0.20595866441726685, 0.3988035023212433, 0.25035127997398376, -0.3508838415145874, 0.6775817275047302, 0.9063485264778137, -0.01672389730811119, -0.46465057134628296, -0.43214109539985657, 1.5370546579360962, 1.1051208972930908, -0.6612814664840698, 0.3880347013473511, -0.42037490010261536, -0.8217895030975342, 0.6302911043167114, -0.2792309820652008, -0.04079241305589676, 1.1764614582061768, 0.17770378291606903, -0.2272404432296753, 0.1771586835384369, -1.178458333015442, -0.15361271798610687, 0.5837860703468323, 0.6923143863677979, 0.6756852865219116, 0.49856889247894287, 0.278798371553421, 0.9999825358390808, -0.31343942880630493, 0.18053624033927917, 0.7000526785850525, 0.3820611536502838, -0.1114111989736557, 0.06205596774816513, 0.12526072561740875, 0.7201454639434814, -0.8317877650260925, -1.1168732643127441, 0.46774357557296753, 0.5423344969749451, 0.25964483618736267, 0.47720813751220703, 0.6693838238716125, -0.15385089814662933, 0.3779244124889374, 0.5722910761833191, 0.323781818151474, -0.6088205575942993, -0.03730373457074165, -0.4156601130962372, -0.40896499156951904, -0.003018508665263653, -0.43134820461273193, -0.186187744140625, -0.2585989832878113, -0.5736161470413208, 0.47457095980644226, 0.06642287969589233, 0.5336682796478271, 1.1460597515106201, 0.47981253266334534, 0.08604401350021362, -0.13345001637935638, -0.3870951235294342, -0.5754295587539673, -0.9250446557998657, -0.3605336546897888, -0.4214331805706024, -0.11916735023260117, -0.12883177399635315, -0.6477831602096558, -0.07969727367162704]}, "authors": [{"authorId": "2149471533", "name": "Yuxin Ren"}, {"authorId": "2894465", "name": "Benyou Wang"}, {"authorId": "50812138", "name": "Lifeng Shang"}, {"authorId": "2110310493", "name": "Xin Jiang"}, {"authorId": "30738758", "name": "Qun Liu"}], "references": [{"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "ef18db2a18ac61e72783a613328842ce86ef00bf", "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models"}, {"paperId": "e89804a4d2611450893a001c5ab5dcf8b5ad2b3a", "title": "Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators"}, {"paperId": "dd0a27aa2285bc64798fa76944400ab6d9ce3025", "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "4a54d58a4b20e4f3af25cea3c188a12082a95e02", "title": "Transformer Feed-Forward Layers Are Key-Value Memories"}, {"paperId": "7e38476342ce1fcc8ef0dcd23686539395961769", "title": "Inductive biases for deep learning of higher-level cognition"}, {"paperId": "24c5242f71af8795021270f52030534e587dfc1e", "title": "Tensorized Embedding Layers"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "14216c91c7d02e58717204f04131107778a84e7b", "title": "Multi-Head Attention: Collaborate Instead of Concatenate"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "851de6751aef4128d7feb7c6ca36b180a0e0835e", "title": "DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering"}, {"paperId": "159dc82a5ee901716b0154051988b5408acfc861", "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression"}, {"paperId": "1c332cfa211400fc6f56983fb01a6692046116dd", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"}, {"paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0", "title": "A Primer in BERTology: What We Know About How BERT Works"}, {"paperId": "2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7", "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "48988bd2d17ff4fa00654e3e983acf390bb0f110", "title": "word2ket: Space-efficient Word Embeddings inspired by Quantum Entanglement"}, {"paperId": "4d8a4509753cc91832f80ec35795064e79630ef3", "title": "Structured Pruning of a BERT-based Question Answering Model"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "259377d035e5651f7d79486828a59e85c3e77938", "title": "On the Effectiveness of Low-Rank Matrix Factorization for LSTM Model Compression"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "62dc8ddb4907db4b889c5e93673d9b3c189d1f25", "title": "A Tensorized Transformer for Language Modeling"}, {"paperId": "0de0a44b859a3719d11834479112314b4caba669", "title": "A Multiscale Visualization of Attention in the Transformer Model"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "c5cb94f7705bee8c913beb774d902b205ac762f1", "title": "Compressing deep neural networks by matrix product operators"}, {"paperId": "1ae89db458cb0dae764f42c74d97b262beff4e2a", "title": "Tensorized Embedding Layers for Efficient Model Compression"}, {"paperId": "449310e3538b08b43227d660227dfd2875c3c3c1", "title": "Neural Ordinary Differential Equations"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "b79e5e4622a95417deec313cd543617b19611bea", "title": "Deep Learning using Rectified Linear Units (ReLU)"}, {"paperId": "d18cc16563b9e2cef58bbc2a9d212b0ca72de36c", "title": "Loss-aware Weight Quantization of Deep Networks"}, {"paperId": "d76900b0a0b0ad381db20efd0f0cfff54d338d08", "title": "Learning Compact Recurrent Neural Networks with Block-Term Tensor Decomposition"}, {"paperId": "2db62ac8bc735133f746cc10439f419abf3b3a2c", "title": "Tensor-Train Recurrent Neural Networks for Video Classification"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "e6f2f3a5cc7c7213835b9aede15715b5830520e1", "title": "Tensorizing Neural Networks"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "62e348e26976c3ef77909b9af9788ebc2509009a", "title": "Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation"}, {"paperId": "6ff0ab1e9064dba97bb8e5ae0b0f1110b5565e06", "title": "Tensor-Train Decomposition"}, {"paperId": "274bc40268671fa3fff54d2ef89454b13fb026da", "title": "A Multilinear Singular Value Decomposition"}, {"paperId": "757cb62e3d1c0643c9f83bf57d45e427bd76e235", "title": "Analysis of individual differences in multidimensional scaling via an n-way generalization of \u201cEckart-Young\u201d decomposition"}, {"paperId": null, "title": "2021) test their model on offline dev dataset through average results for three"}, {"paperId": "77e73174e606c0820a52a940088832b32d9a033e", "title": "Efficient Large-Scale Language Model Training on GPU Clusters"}, {"paperId": "b60a16d2978b10ead102a6a6cd03dc940b1194cc", "title": "Compressing Pre-trained Language Models by Matrix Decomposition"}, {"paperId": null, "title": "2019) could be considered as a special case of IV, see App. G"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2020) used nonstandard evaluation metrics in GLUE"}, {"paperId": "8213dbed4db44e113af3ed17d6dad57471a0c048", "title": "The Nature of Statistical Learning Theory"}, {"paperId": "4625c313c8d68d107470641f65803975dc03d824", "title": "Determination and Proof of Minimum Uniqueness Conditions for PARAFAC1"}, {"paperId": null, "title": "Implications of factor analysis of three-way matrices for measurement of change"}, {"paperId": null, "title": "Table 12: Parameter compression ratios in various models"}]}