{"paperId": "89d786457591d39091cf6ef4831f2bbd72698caf", "title": "TransformerFAM: Feedback attention is working memory", "abstract": "While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.", "venue": "arXiv.org", "year": 2024, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Feedback Attention Memory is proposed, a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations, allowing it to process indefinitely long sequences."}, "embedding": {"model": "specter_v2", "vector": [0.34950652718544006, 0.7222303748130798, -0.13407300412654877, 0.07413091510534286, 0.06063428148627281, -0.23056109249591827, 0.8312989473342896, -0.16389909386634827, -0.541511058807373, -0.25252658128738403, 0.6149227619171143, -0.4794503450393677, 0.5478665232658386, 0.03774100914597511, -0.008667492307722569, 0.09385483711957932, -0.8420751690864563, 0.15611803531646729, 0.08485489338636398, -0.18949635326862335, 0.1220405176281929, -0.48624375462532043, -0.786527156829834, 0.01240311749279499, -0.12801997363567352, 0.7492762804031372, 0.48129260540008545, 0.8363022804260254, -0.44550570845603943, 0.5605292320251465, 0.7072395086288452, -0.43046772480010986, 0.2710288166999817, 0.1279180347919464, -0.7139826416969299, -0.2663520276546478, 0.6040507555007935, -0.5224859118461609, -0.6626960635185242, 0.6762045621871948, -0.39565762877464294, 0.35597050189971924, 0.28712454438209534, -0.5557069778442383, -0.31783437728881836, 0.7528162002563477, 0.663305401802063, 1.0312756299972534, -0.32138481736183167, -0.14924411475658417, 1.3159499168395996, -1.5287083387374878, 0.17213264107704163, 1.5498323440551758, 0.5822234749794006, 0.3084351122379303, -0.3647134304046631, -0.8014111518859863, 1.3202403783798218, 0.10998053103685379, -0.34846508502960205, -0.05390676110982895, -0.06647693365812302, -0.19782136380672455, 1.9081974029541016, -0.4899885952472687, 0.22703766822814941, 0.7791408896446228, 0.31322452425956726, 1.372159481048584, -0.15874558687210083, -0.8813161253929138, -0.7272534370422363, -0.16858528554439545, 0.7665173411369324, 0.9226000308990479, -0.708691418170929, 0.6280617713928223, -0.8499987125396729, 0.05181203782558441, 0.5897302627563477, 0.08514570444822311, -0.254791259765625, 0.09996549040079117, -0.4757509231567383, 0.6649004817008972, 0.6088544726371765, 0.6617832779884338, -0.25248226523399353, 0.830418586730957, 0.4148847460746765, 0.48220738768577576, 0.051646068692207336, 0.29411178827285767, 0.011553375981748104, 0.5644511580467224, -0.6005276441574097, 0.09288955479860306, -0.17950159311294556, 0.9630251526832581, 0.039901383221149445, 0.20950953662395477, -0.7214143872261047, 0.0435466542840004, 1.4332575798034668, -0.1342139095067978, 0.4329127371311188, -0.6882097125053406, 0.38100966811180115, -0.6868118643760681, 0.059273701161146164, -0.9955940246582031, -0.13326454162597656, -0.4950730502605438, -0.729557454586029, -1.060577630996704, -0.23507234454154968, 0.5013382434844971, -0.6152975559234619, 1.10832679271698, -0.5130082368850708, -0.06983162462711334, -0.4802856743335724, 0.10242913663387299, 0.14517349004745483, 0.6092787384986877, 0.2500787079334259, -0.14702185988426208, 0.8204588294029236, -1.1871795654296875, -0.7178496718406677, -1.4085520505905151, 0.5050048828125, -0.1417945772409439, 0.5468421578407288, 0.22777438163757324, -0.9587638974189758, -1.126550316810608, -0.9003044962882996, -0.15731889009475708, -0.46855276823043823, -0.020858801901340485, 1.100039005279541, 0.23739950358867645, -1.1129159927368164, 0.9810176491737366, -0.1677909940481186, -0.011202173307538033, 0.5392712354660034, 0.16117478907108307, 0.3303433954715729, -0.526167631149292, -1.3168472051620483, 0.48092105984687805, 0.3180793523788452, -0.282122403383255, -0.5053186416625977, -0.8809308409690857, -0.7703197598457336, 0.06104130670428276, 0.05214713141322136, -0.5923694372177124, 1.3605765104293823, -0.4032585322856903, -0.8854166865348816, 0.6928831934928894, -0.3200877010822296, 0.08118665963411331, 0.37229764461517334, -0.510834813117981, -0.4083687365055084, -0.4814753234386444, -0.39023005962371826, 0.6841496229171753, 0.5218836069107056, 0.37877219915390015, -0.06208869814872742, -0.09333211928606033, -0.37630870938301086, -0.42498964071273804, -0.7262070178985596, 0.8404504656791687, -0.267616331577301, -0.40651917457580566, 0.16313013434410095, 0.594695508480072, 0.16560660302639008, -0.030937667936086655, -0.05038870498538017, -1.301998496055603, 0.7079007029533386, -0.09185754507780075, 1.284289836883545, -0.9433077573776245, -0.8344758749008179, -0.37081998586654663, 0.017364906147122383, -0.1056760847568512, -0.8332114815711975, 0.7453937530517578, -0.6564648747444153, 0.3875722289085388, 0.05405118688941002, -0.871142566204071, -0.002892859047278762, -0.34715017676353455, -1.1083405017852783, -0.1568693220615387, 0.3214039206504822, 1.0604089498519897, -1.2420088052749634, 0.0322420634329319, -0.25520792603492737, 0.1069650650024414, -0.7978904843330383, 1.5402292013168335, -0.3615412414073944, 0.11961577087640762, 0.24371306598186493, -0.6218037009239197, 0.10005291551351547, -0.5641281008720398, 0.3267783224582672, -0.5370570421218872, -0.13631181418895721, 0.7331684827804565, 0.0346246175467968, 1.3938785791397095, -0.3786107301712036, 1.0859543085098267, -0.29381775856018066, -0.39001911878585815, 0.39745306968688965, 0.2915056645870209, -0.45558905601501465, -0.4857316017150879, 0.15275156497955322, 0.1777496337890625, -0.6744863390922546, 0.39238086342811584, 0.6307348012924194, 0.693093478679657, -0.11033060401678085, 0.4253341853618622, 0.8475727438926697, 0.13494554162025452, 0.3561054468154907, 0.6873816251754761, 0.7563009262084961, 0.6127722263336182, 0.301946222782135, -0.1472989171743393, 0.21336309611797333, -1.1531926393508911, -0.18029005825519562, 0.42677098512649536, 0.5734999179840088, 0.5930063724517822, 0.4254212975502014, -0.8532800078392029, -0.4822094738483429, -0.009586404077708721, 0.7296326160430908, 1.5867912769317627, -0.377113401889801, -0.13751475512981415, -0.31604987382888794, -0.1720075011253357, -0.32434770464897156, 0.08306622505187988, -0.7365358471870422, -0.5295177102088928, -0.6623587012290955, -0.5188173651695251, 0.7025943398475647, 0.5597389936447144, 0.7379186749458313, -0.8461315631866455, -0.43802329897880554, -0.17118661105632782, 0.434902548789978, -0.3272017538547516, -1.021744728088379, 0.7118226289749146, -0.7596478462219238, -0.14782510697841644, 0.006166066508740187, 0.13915306329727173, -0.02781757153570652, -0.6342824101448059, 1.2805726528167725, -0.6089837551116943, -0.10216012597084045, 0.22655175626277924, 0.9106069803237915, -0.4044126868247986, -0.580659806728363, 0.3246326446533203, 0.1387074887752533, -0.07221017777919769, 0.1439317911863327, 0.4208591878414154, -0.29192665219306946, 0.06494005024433136, -0.23316623270511627, 0.15425561368465424, 0.193035289645195, 0.30484506487846375, 0.42594215273857117, -0.624468982219696, 0.22582973539829254, -1.2701947689056396, 0.8036522269248962, 0.20687706768512726, -0.34820258617401123, 0.2992420196533203, -0.9397748112678528, -0.16008436679840088, 0.2787661552429199, -0.7203103303909302, -0.2777622938156128, -1.106351375579834, 0.3811740577220917, -0.7219708561897278, 0.2038343995809555, 0.2697284519672394, 0.3379071354866028, 0.38096728920936584, 0.20142750442028046, 0.20839974284172058, 0.22183899581432343, -0.054338350892066956, 0.4090496003627777, -1.0821893215179443, 0.5905743837356567, 0.4575780928134918, -0.056229811161756516, -0.4327414035797119, -0.08857905864715576, -0.4102588891983032, -0.7145493626594543, -0.3297683298587799, -0.3535853922367096, -0.4787214696407318, -0.02879803255200386, -0.38047558069229126, -1.1678599119186401, 0.04385101795196533, -1.3001089096069336, -0.3977905809879303, 0.14223018288612366, -0.4118194878101349, -0.19613683223724365, -1.2813535928726196, -1.209154725074768, -0.7071727514266968, -0.6459386944770813, -0.5842943787574768, -0.28797316551208496, -0.04172299802303314, -0.6347306370735168, -0.9433538913726807, -0.26825058460235596, -0.6413875818252563, 1.0357508659362793, -0.8166773915290833, 0.7029680609703064, 0.2606835961341858, -0.4489615261554718, 0.00925463531166315, -0.019776741042733192, 0.5933681726455688, -0.29564985632896423, 0.06139576807618141, -1.297961950302124, -0.013497825711965561, -0.20440644025802612, -0.48497599363327026, 0.6804010272026062, -0.05465006083250046, 0.7844159007072449, -0.20863743126392365, -0.5535734295845032, 0.2312910109758377, 1.1764864921569824, -0.5294302105903625, 0.018827242776751518, 0.14332613348960876, 0.9313941597938538, 0.24059122800827026, -0.13695724308490753, 0.3338792026042938, 0.1714535504579544, 0.07065955549478531, 0.27445176243782043, 0.16521047055721283, -0.20404693484306335, -1.0291743278503418, 0.3721008598804474, 1.3665108680725098, 0.5355473160743713, 0.016776207834482193, -1.2462786436080933, 0.7004215121269226, -1.032297134399414, -0.7325356602668762, 0.6578624248504639, 0.8565077781677246, 0.5048192143440247, -0.26114001870155334, -0.3530414402484894, -0.028064141049981117, 0.21887774765491486, 0.43924564123153687, -0.682853102684021, -0.8204337358474731, 0.09376990050077438, 0.3338412940502167, 0.19401603937149048, 0.6983819603919983, -0.14896921813488007, 0.22416047751903534, 14.763815879821777, 0.5033457279205322, -0.29515352845191956, 0.8114684224128723, 0.7726754546165466, 0.22583279013633728, -0.5477883815765381, -0.005676889792084694, -1.3137359619140625, -0.33587050437927246, 1.1525665521621704, 0.070920430123806, 0.43383511900901794, -0.20053324103355408, -0.19415196776390076, 0.26722991466522217, -0.8014289736747742, 0.695820152759552, 0.2005879282951355, -1.2688982486724854, 0.4650971591472626, 0.16917459666728973, -0.10130579024553299, 0.5172476172447205, 1.0138124227523804, 0.6907210350036621, 0.5132231116294861, -0.3983963131904602, 0.8178612589836121, 0.7136368751525879, 0.937969982624054, 0.0236606877297163, -0.157941997051239, 0.6379832029342651, -0.7965332865715027, -0.5221243500709534, -0.45999613404273987, -1.146332859992981, 0.11346269398927689, -0.0016911291750147939, -0.5397794246673584, -0.8101200461387634, -0.19527599215507507, 0.917569100856781, -0.1400505155324936, 0.05051993578672409, -0.241990327835083, 0.7733230590820312, 0.015975501388311386, 0.2804320156574249, 0.459501713514328, 0.6704753041267395, -0.11973769217729568, 0.07788975536823273, 0.04351811110973358, -0.19893883168697357, 0.10055410116910934, 0.6710214614868164, -0.46836361289024353, -0.3275119960308075, -0.26303717494010925, -0.5383406281471252, 0.023374710232019424, 0.45722153782844543, 0.4432551860809326, 0.5767133831977844, -0.22810371220111847, 0.2001488357782364, 0.8301962018013, 0.31622883677482605, -0.081061452627182, 0.09067144989967346, 0.05470414087176323, -0.37528273463249207, -0.053748808801174164, 0.467000812292099, -0.2749619483947754, -0.20689015090465546, -1.0192776918411255, -0.035532284528017044, 0.5944191217422485, -1.018181562423706, -0.5577014684677124, 0.9330967664718628, -0.46406736969947815, -0.14850261807441711, 0.26511597633361816, -0.9535520672798157, -0.2448718100786209, 0.7478523254394531, -1.1074895858764648, -0.6950293183326721, 0.07243657857179642, -0.2609465420246124, -0.2809933125972748, 0.0067680771462619305, 1.156698226928711, 0.15554217994213104, -0.19020171463489532, 0.26276540756225586, -0.39505916833877563, -0.14557845890522003, -0.21204113960266113, -0.7586053609848022, 0.7727972269058228, 0.04511508718132973, -0.03864465281367302, 0.7350168228149414, -0.0794650986790657, 0.3529747426509857, -0.656282365322113, -0.07363102585077286, 0.9088519215583801, -0.9767847061157227, -0.14731624722480774, -0.7350463271141052, -0.8427340388298035, 0.7339950203895569, 0.6056503057479858, -0.07780876010656357, 0.3796592652797699, 0.3573767840862274, -0.9705784916877747, -0.15050730109214783, -0.1793898642063141, -0.20635639131069183, 0.5687375068664551, -0.8853954076766968, -0.41975876688957214, -0.49086716771125793, 0.6173741817474365, -0.6832514405250549, -0.40396037697792053, -0.4624634087085724, 0.29333293437957764, -0.2984514534473419, 0.9179304838180542, -0.41619744896888733, 0.6039414405822754, 0.9548256397247314, 0.06360569596290588, -0.7673739790916443, -0.3797856569290161, -0.9454479217529297, -0.01322468463331461, 0.3373921513557434, 0.41574323177337646, -0.5983585715293884, -0.041674673557281494, 0.5196243524551392, 0.10543650388717651, -0.5103988647460938, -0.4347778260707855, -0.25100845098495483, 0.22087807953357697, -0.566132128238678, -0.0752861350774765, -0.017716845497488976, -0.18808020651340485, 0.5124111771583557, 0.5144758820533752, 0.0704168900847435, -0.22884030640125275, -0.5944775342941284, -0.04243176057934761, -0.3182918429374695, 0.03927353769540787, -0.6772340536117554, -0.24849678575992584, -1.805765986442566, -0.1874740719795227, -1.2090873718261719, -0.3911861181259155, -0.8579828143119812, -0.5274972915649414, -0.12021864205598831, -0.5422378778457642, 0.10522472858428955, 0.2343222051858902, -0.4880019426345825, -0.25654762983322144, -0.40356191992759705, -0.3534986078739166, 0.7795621156692505, 1.115700364112854, -0.7959126234054565, 0.45330798625946045, -0.1353648155927658, 0.15032897889614105, 0.1791108101606369, 0.17912903428077698, -0.20253105461597443, -0.9954441785812378, -1.3704954385757446, 0.819019079208374, -0.16225405037403107, -0.1421707719564438, -0.4293302297592163, 0.7754338383674622, 0.3346567153930664, -0.1079140454530716, -0.26660072803497314, 0.32291337847709656, -0.8908161520957947, -0.42594006657600403, 0.5168035626411438, -1.0265896320343018, 0.49865710735321045, 0.5318658947944641, -0.2129727602005005, -0.25511983036994934, 0.7265282869338989, 0.08918829262256622, -1.1186299324035645, -0.6736918687820435, 0.5696801543235779, -0.5887499451637268, 0.0940539762377739, -0.4324176609516144, -0.1409841775894165, -1.043154001235962, -0.24736227095127106, -0.021271664649248123, 0.6017264723777771, -0.5894673466682434, 0.9487568736076355, 0.5535852313041687, -1.3278255462646484, 0.4165440499782562, 0.6041449308395386, 0.05920375511050224, -0.021040521562099457, 0.3509272336959839, 0.5866053700447083, 0.07378232479095459, 0.7288323044776917, 0.17743058502674103, 0.4173141419887543, -0.8341108560562134, 0.31018760800361633, 0.832879900932312, -0.5799822211265564, -0.1905335634946823, 1.371281623840332, 0.010560960508883, -1.135892629623413, 0.477705180644989, -1.179147720336914, -0.6184419393539429, 0.3061975836753845, 0.6878082752227783, 0.06610840559005737, -0.0945013165473938, 0.30997732281684875, -0.3216818571090698, -0.15719608962535858, -0.12224259227514267, -0.44320356845855713, 0.4417208731174469, -0.16302138566970825, -0.3926773965358734, 1.3199371099472046, 0.9918351769447327, -0.9985795617103577, -0.8262047171592712, -0.9551891088485718, -0.19210919737815857, -0.21099860966205597, 0.40230849385261536, -0.44279903173446655, -0.5101876258850098, 1.315220832824707, 0.5868441462516785, 0.31506431102752686, 0.2295278161764145, -0.3998654782772064, -0.17368929088115692, 0.5440240502357483, 0.08200615644454956, -0.19730082154273987, -0.012833530083298683, 1.7657268047332764, 1.558495044708252, -0.3449176847934723, -0.11330752819776535, -0.04887120798230171, -0.20572520792484283, 1.0761044025421143, 0.46593955159187317, -0.06375551223754883, 1.0298914909362793, -0.29283544421195984, 0.08350630849599838, 0.254103422164917, -1.4564449787139893, 0.2279512733221054, 0.6642242074012756, 0.761403501033783, 1.0061637163162231, -0.04448452219367027, 0.3346998989582062, 0.6836652159690857, 0.046899039298295975, 0.2609823942184448, 0.16442134976387024, 0.260497123003006, -0.1443968266248703, 0.07879739999771118, 0.12226811796426773, 0.5709447860717773, -0.5052683353424072, -0.7797156572341919, 0.30125436186790466, 0.18265025317668915, 0.18028624355793, 0.287117600440979, 1.006431221961975, 0.1466168612241745, 0.4225720167160034, 0.3934401571750641, 0.8430273532867432, -0.4294978678226471, -0.5355626344680786, -0.4316168427467346, -0.6948829293251038, -0.10379697382450104, -0.05286082997918129, -0.5292133688926697, 0.021532248705625534, -0.08014865964651108, 0.02841402031481266, -0.18045829236507416, -0.1437755525112152, 0.8829535245895386, 0.12250149250030518, 0.7640102505683899, -0.27576586604118347, -0.28488561511039734, -0.6823575496673584, -1.3290361166000366, 0.3336939811706543, -0.721966028213501, 0.07894475013017654, -0.3367805480957031, 0.03417130187153816, -0.20053730905056]}, "authors": [{"authorId": "2241835900", "name": "Dongseong Hwang"}, {"authorId": "2243099428", "name": "Weiran Wang"}, {"authorId": "2296717007", "name": "Zhuoyuan Huo"}, {"authorId": "1693612", "name": "K. Sim"}, {"authorId": "2103777583", "name": "P. M. Mengibar"}], "references": [{"paperId": "3fd5bc3077d04965eaa3498372c39bbdd09d55e4", "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"}, {"paperId": "3e8d4062ec4353ff2701c7769336dbdb97f8814c", "title": "Transformers are Multi-State RNNs"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0", "title": "Effective Long-Context Scaling of Foundation Models"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf", "title": "Adapting Language Models to Compress Contexts"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "7c25adf2ddb35df05a61c697da97efb8583d77df", "title": "TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "6cf65eb8aa66116e14a97bb8f71552359ff814ba", "title": "Will we run out of data? Limits of LLM scaling based on human-generated data"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "8c870bef01a4fbb20f60722ffc2f6bee3870b18b", "title": "AudioLM: A Language Modeling Approach to Audio Generation"}, {"paperId": "6edccbd83a9aae204785d4821f97855677c33866", "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "a26a7a74f1e5fd562be95c3611a0680759fbdf84", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "c57293882b2561e1ba03017902df9fc2f289dea2", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0", "title": "Memorizing Transformers"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "6281c40c66febca1d8003bcc6fdfd2189b30c38f", "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "a7d61ab4a3442fd2382f6c11f991421c0d98674a", "title": "Layer-Wise Analysis of a Self-Supervised Speech Representation Model"}, {"paperId": "ecf5618b513aa5c4d5bf62ca251923a188251117", "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "b50815251c948f00baedccaf5f56c281ffa7650f", "title": "Staircase Attention for Recurrent Processing of Sequences"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "afad10da0a3b83a4f2a94e8c16c84ac64338e9fe", "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "49a049dc85e2380dde80501a984878341dd8efdf", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "63857190aaf5aab1d94b54bb257b7b03b8cb5a50", "title": "GMAT: Global Memory Augmentation for Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "678cba6df672a9160085b75d4e4294165e4bbed8", "title": "Recognizing Long-Form Speech Using Streaming End-to-End Models"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "2a31319e73d4486716168b65cdf7559baeda18ce", "title": "Star-Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "47a7429cc8948dd935513f881949b0521674a7f8", "title": "Can a Compact Neuronal Circuit Policy be Re-purposed to Learn Simple Robotic Control?"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a", "title": "The NarrativeQA Reading Comprehension Challenge"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "7250889c52660a4a77e02c76236b2443f33b9eae", "title": "The Distributed Nature of Working Memory"}, {"paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9", "title": "Overcoming catastrophic forgetting in neural networks"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "79e1ec2411808089389cf52eb601c41952af1b98", "title": "The interactions of multisensory integration with endogenous and exogenous attention"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "56f1827c49b8aef71b7ed16a55b5f466ac2b95f1", "title": "FROST: A Distributed Neurocomputational Model of Working Memory Maintenance"}, {"paperId": "563e821bb5ea825efb56b77484f5287f08cf3753", "title": "Convolutional networks for images, speech, and time series"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "62d36f23580ae0c822ebc7de69ae603d85441bfc", "title": "The structure of the nervous system of the nematode Caenorhabditis elegans."}, {"paperId": "9fa5877347237a3ecfa39948f88df89b0d3d0faf", "title": "Plans and the Structure of Behavior"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "The FLAN Instruction Tuning Repository"}, {"paperId": null, "title": "Addressing some limitations of transformers with feed-back memory"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "5b4022027fa6d467f503c13b3bee3505cc4fc820", "title": "Short Term"}, {"paperId": "ae4df460a413f3b1d9a0dfa47917751af9db2597", "title": "Deep State Space Models for Time Series Forecasting"}, {"paperId": "ebf673d896e84d69a42d88f109870d6a674e0ac4", "title": "Global workspace theory of consciousness: toward a cognitive neuroscience of human experience."}, {"paperId": "898ddbc6d747e4583a3e870d33cb228691efb87c", "title": "Unit activity in prefrontal cortex during delayed-response performance: neuronal correlates of transient memory."}, {"paperId": "a15174ed603bae1b101c4655111bb511787b95b4", "title": "The magical number seven plus or minus two: some limits on our capacity for processing information."}, {"paperId": null, "title": "Gemini: a family of highly capable multimodal models"}, {"paperId": null, "title": "We applied both the TransformerBSWA and TransformerFAM architectures to Flan-PaLM and fine-tuned it for an additional 50k steps"}, {"paperId": null, "title": "Lmsys chatbot arena leaderboard"}, {"paperId": null, "title": "Vision trans-formers need registers"}]}