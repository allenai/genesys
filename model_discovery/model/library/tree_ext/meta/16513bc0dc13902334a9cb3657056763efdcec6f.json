{"paperId": "16513bc0dc13902334a9cb3657056763efdcec6f", "title": "Towards Open-Ended Visual Recognition with Large Language Model", "abstract": "Localizing and recognizing objects in the open-ended physical world poses a long-standing challenge within the domain of machine perception. Recent methods have endeavored to address the issue by employing a class-agnostic mask (or box) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP) using pre-extracted text embeddings. However, it is worth noting that these open-vocabulary recognition models still exhibit limitations in practical applications. On one hand, they rely on the provision of class names during testing, where the recognition performance heavily depends on this predefined set of semantic classes by users. On the other hand, when training with multiple datasets, human intervention is required to alleviate the label definition conflict between them. In this paper, we introduce the OmniScient Model (OSM), a novel Large Language Model (LLM) based mask classifier, as a straightforward and effective solution to the aforementioned challenges. Specifically, OSM predicts class labels in a generative manner, thus removing the supply of class names during both training and testing. It also enables cross-dataset training without any human interference, exhibiting robust generalization capabilities due to the world knowledge acquired from the LLM. By combining OSM with an off-the-shelf mask proposal model, we present promising results on various benchmarks, and demonstrate its effectiveness in handling novel concepts. Code/model are available at https://github.com/bytedance/OmniScient-Model.", "venue": "arXiv.org", "year": 2023, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The OmniScient Model is introduced, a novel Large Language Model (LLM) based mask classifier that predicts class labels in a generative manner, thus removing the supply of class names during both training and testing, and enables cross-dataset training without any human interference."}, "embedding": {"model": "specter_v2", "vector": [-0.04180675372481346, 0.3817681670188904, -0.3458285331726074, -0.0847456306219101, -0.041625916957855225, -0.10385621339082718, 0.6911076307296753, -0.39972296357154846, -0.39470118284225464, -0.6906090378761292, 0.39726734161376953, 0.4587494134902954, 0.6706224083900452, 0.12945489585399628, -0.08809405565261841, 0.26704341173171997, -0.8007797598838806, 0.2891952693462372, 0.1929921954870224, -0.42778944969177246, -0.002955034142360091, -0.493917316198349, -1.0427603721618652, -0.04717845469713211, -0.1145576536655426, 0.8372437953948975, 0.3093850910663605, 0.9344438910484314, -0.5283576846122742, 0.4034445881843567, 0.3013327717781067, -0.34784266352653503, 0.2283669114112854, 0.3925192058086395, -0.11460734903812408, 0.3769032955169678, 0.7042176127433777, -0.4916000962257385, -0.8062381148338318, 0.5535444617271423, 0.03075348399579525, 0.6591541767120361, 0.5599544644355774, -0.8159065246582031, -0.8891276717185974, 0.31969937682151794, 0.6831725835800171, 0.22752097249031067, -0.3602522909641266, -0.38720133900642395, 1.3757166862487793, -2.074676513671875, 0.617871880531311, 1.502225637435913, 0.11743069440126419, 0.43549203872680664, -0.11013100296258926, -0.7522290945053101, 0.9039562940597534, -0.01824493333697319, -1.432350993156433, -0.23347680270671844, -0.059823840856552124, -0.21481084823608398, 1.3294926881790161, -0.7779776453971863, -0.12990020215511322, 1.0888017416000366, -0.11363992094993591, 1.078226923942566, -0.2068992555141449, -1.01346755027771, -0.13401147723197937, 0.35671114921569824, 0.07193023711442947, 0.8693500757217407, -0.2128351330757141, 0.4527803957462311, -0.7256666421890259, 0.24549229443073273, 0.470061331987381, -0.3844655156135559, 0.25847768783569336, -0.8811668157577515, -0.4609108865261078, 0.5914524793624878, 0.5511292219161987, 0.3655685484409332, 0.3450176417827606, 0.6310189962387085, 0.48291364312171936, -0.15437468886375427, -0.08407853543758392, 0.08203618228435516, -0.04236803948879242, 0.5690658688545227, -0.49739646911621094, 0.23273998498916626, 0.20593972504138947, 1.1306519508361816, -0.25432053208351135, 0.41936445236206055, -0.14227372407913208, 0.4475085437297821, 1.5800994634628296, 0.27752408385276794, 0.7282272577285767, -0.39344900846481323, 0.0059913345612585545, -0.8565906286239624, 0.4185655117034912, -0.9787918329238892, -0.0938866138458252, -0.40362823009490967, -0.2047896385192871, -0.8010761737823486, -0.47425317764282227, 0.445710152387619, -1.1399099826812744, 0.6426315307617188, -0.22441530227661133, 0.016933105885982513, 0.21271087229251862, 0.28714272379875183, 0.9740337133407593, 0.6478013396263123, 0.6934029459953308, 0.698605477809906, 1.0204812288284302, -1.0860499143600464, -0.22781242430210114, -1.0636088848114014, 0.7453266382217407, -0.07284887880086899, 0.49045655131340027, -0.3049760162830353, -0.47733524441719055, -1.6221990585327148, -0.8661126494407654, -0.08803702890872955, -1.2006632089614868, 0.7134451866149902, 0.5440357327461243, 0.6519260406494141, -0.9290505051612854, 0.10067778825759888, 0.059416837990283966, -0.7537690997123718, 0.6232884526252747, 0.2835613191127777, -0.1452288031578064, -0.4984624981880188, -0.6980879902839661, 0.7003693580627441, 0.3857220709323883, -0.7018035650253296, -0.520423173904419, 0.351752907037735, -1.683418869972229, -0.5099610686302185, 0.5383480191230774, -0.04039180651307106, 0.7925996780395508, -0.31384292244911194, -0.9528385400772095, 0.8370387554168701, -0.5839609503746033, 0.3513738214969635, -0.03554847836494446, -0.22894561290740967, -1.0045615434646606, -0.4975213408470154, -0.43005451560020447, 1.4480082988739014, 0.8507186770439148, -0.6583631634712219, 0.20123419165611267, 0.41286882758140564, -0.14889127016067505, 0.028103629127144814, -0.6759639978408813, 1.1346060037612915, -0.03913817182183266, -0.6030775308609009, 0.38829874992370605, 0.5345132350921631, 0.13872000575065613, 0.4420572817325592, -0.27240508794784546, -1.148471713066101, 1.1554627418518066, 0.3641759157180786, 0.10320106148719788, -1.1572937965393066, -0.8891653418540955, -0.5828404426574707, -0.07171133160591125, -0.08183160424232483, -1.233577847480774, 0.5828931331634521, -0.34036940336227417, 0.6198799014091492, 0.3401476740837097, -1.2526775598526, 0.3011115789413452, 0.003187961643561721, -0.6248282790184021, 0.031758058816194534, 0.5587528347969055, 1.2756658792495728, -0.7707179188728333, -0.06745138019323349, -0.01981423795223236, 0.18241307139396667, -0.5904704928398132, 1.2633652687072754, -0.6905965805053711, -0.23851612210273743, -0.04081854596734047, 0.12025395035743713, 0.09654734283685684, -0.2674989104270935, 0.04883028566837311, -0.3260769844055176, -0.07836170494556427, 0.23903104662895203, -0.7618764042854309, 1.2435979843139648, -0.1742018610239029, 0.797301709651947, -0.767331063747406, -0.361784964799881, 0.5487716197967529, 0.4986015558242798, -0.5341187119483948, -0.06352170556783676, 0.5729173421859741, 0.1778203547000885, -0.6389684677124023, -0.1712832748889923, 0.9669592380523682, 0.76529461145401, -0.3044447600841522, 0.15701007843017578, 0.646767258644104, 0.01342258881777525, 0.29565712809562683, 0.209104984998703, 0.18278008699417114, 0.8042498230934143, 0.469262033700943, 0.1993478536605835, 0.22307513654232025, -1.0717378854751587, -0.34295767545700073, 0.5518321394920349, 0.21816571056842804, 1.5380747318267822, -0.0861089751124382, -0.8708943724632263, -0.31827884912490845, -0.2612661123275757, 0.5503857731819153, 1.5368843078613281, 0.43670952320098877, -0.2285500317811966, -0.26357147097587585, -0.33257779479026794, -0.4967041015625, -0.2631729245185852, -0.5627391934394836, 0.07680603861808777, -0.2154279351234436, -0.43719929456710815, 0.7274352312088013, 0.6080843806266785, 0.935031533241272, -0.8845917582511902, -0.2898242473602295, -0.5349765419960022, 0.12078990787267685, -1.0673565864562988, -0.8022553324699402, 0.10402300208806992, 0.0011460704263299704, 0.05334562435746193, -0.18204915523529053, -0.5544021725654602, 0.5798369646072388, -0.5329917073249817, 0.650263786315918, -0.8355075716972351, -0.7486066818237305, 0.5123159289360046, 0.05401621013879776, -0.7866037487983704, -0.5609416961669922, 0.1291441023349762, 0.08030377328395844, 0.279157817363739, 0.6452376246452332, 1.015428900718689, 0.18522648513317108, 0.37056902050971985, -0.4985928535461426, 0.17686344683170319, 0.06173110380768776, -0.2474544793367386, 0.6160115003585815, -0.6542445421218872, 0.24371926486492157, -0.3314549922943115, 0.21388091146945953, 0.1002163216471672, 0.0441882461309433, 0.18115107715129852, -0.35207992792129517, -0.5891004800796509, 0.18715043365955353, -1.5031832456588745, -0.13936184346675873, -0.4703834056854248, 0.4760465621948242, -0.2714861035346985, -0.9832783937454224, -0.2111508846282959, -0.018089357763528824, -0.17720486223697662, 0.4150135815143585, 0.6519713997840881, 0.37445926666259766, -0.04806375876069069, 0.8130481839179993, -0.5546954274177551, 0.6067596077919006, 0.079286590218544, 0.34015730023384094, 0.3841681480407715, -0.5303378701210022, -0.8744708299636841, -0.2606692612171173, -0.6577814221382141, -0.8159012198448181, -0.6422538757324219, 0.3015460669994354, -0.8706691265106201, -0.2755611836910248, 0.0892246812582016, -1.187735915184021, 0.02107882685959339, -0.20975542068481445, -0.21704760193824768, -0.36782437562942505, -1.0323107242584229, -0.5869565606117249, -0.58871990442276, 0.20245392620563507, -0.6993371248245239, 0.26825061440467834, 0.23017360270023346, -0.7503584623336792, -0.42174360156059265, 0.13574759662151337, -0.41872072219848633, 0.8307875990867615, -0.22296825051307678, 0.4320916533470154, -0.39379188418388367, -0.41907989978790283, -0.6376703977584839, -0.1656782329082489, 0.707621157169342, -0.1888677477836609, 0.41513553261756897, -0.9106135368347168, 0.41080713272094727, -0.1501808911561966, -0.5150980949401855, -0.1017681360244751, -0.18231309950351715, 0.5660596489906311, 0.3156774342060089, -0.34882527589797974, 0.14649033546447754, 1.6144944429397583, -0.506171703338623, 0.03240080177783966, -0.2576357424259186, 1.0806413888931274, 0.5607415437698364, -0.514314591884613, 0.5290727019309998, 0.5352935194969177, 0.277605265378952, 0.3175893723964691, -0.30767911672592163, -0.6498569250106812, -0.7798746228218079, 0.47089114785194397, 1.1649253368377686, 0.4617818295955658, -0.25526267290115356, -1.14656400680542, 1.0942978858947754, -0.7335294485092163, -0.46204495429992676, 0.698052704334259, 0.6093798279762268, -0.15099568665027618, -0.5218443274497986, -0.2743494510650635, -0.7827534079551697, 0.8446188569068909, 0.5340099930763245, -0.18605907261371613, -0.4233850836753845, -0.22791071236133575, 0.4332045018672943, -0.1911597102880478, 0.19847296178340912, -0.7608247995376587, 0.4670911729335785, 14.365199089050293, 0.5955190062522888, -0.31611478328704834, 0.6234299540519714, 0.6938633918762207, 0.5564688444137573, -0.009173366241157055, 0.1653093844652176, -1.3374513387680054, -0.47135064005851746, 1.1412793397903442, 0.514100968837738, 0.07003199309110641, 0.36373430490493774, -0.22948774695396423, 0.4189000427722931, -0.6788776516914368, 0.9329736232757568, 1.463573932647705, -0.7539072632789612, 0.5151640772819519, 0.27196037769317627, 0.446890652179718, 0.5761833190917969, 1.0390853881835938, 0.7696192264556885, 0.028173523023724556, -0.6482348442077637, 0.9099597930908203, 0.06024058535695076, 0.9565605521202087, 0.3669952154159546, 0.08807753026485443, 0.50537109375, -1.192561388015747, -0.31254616379737854, -1.0654312372207642, -1.2314122915267944, 0.04221998155117035, -0.4568469822406769, -0.32102739810943604, -0.47557157278060913, -0.42231035232543945, 0.7365656495094299, -0.2151443511247635, 0.6126289963722229, -0.02748594805598259, 0.25387078523635864, -0.014769325032830238, -0.3443373739719391, 0.31517094373703003, 0.8841553926467896, 0.5667473673820496, -0.20456041395664215, 0.0363764688372612, -0.049219973385334015, 0.5241426229476929, 0.7226619720458984, -0.6193312406539917, 0.29561033844947815, -0.6446842551231384, -0.19637303054332733, -0.6248841285705566, 0.7277875542640686, -0.06057482585310936, 0.23640505969524384, -0.4621905982494354, 0.24603471159934998, 0.5557557940483093, 0.45975515246391296, -0.5099551677703857, 0.08049307763576508, 0.23162387311458588, -0.46020016074180603, 0.3500337600708008, 0.16649192571640015, -0.0072413040325045586, -0.9045348167419434, -0.37591925263404846, 0.21864193677902222, 0.021317362785339355, -1.0303823947906494, -1.0743099451065063, 1.1634896993637085, -0.09925049543380737, -0.9388124942779541, 0.5909984111785889, -1.2710175514221191, -0.28435438871383667, 0.1031399741768837, -1.105278730392456, -1.241343379020691, -0.06419623643159866, -0.5456943511962891, 0.19735285639762878, -0.10760512202978134, 1.3108717203140259, -0.16997328400611877, -0.23408642411231995, -0.03828452154994011, -0.1317385882139206, 0.3860195279121399, -0.10524305701255798, -0.350283682346344, 0.6165819764137268, 0.026300108060240746, 0.4526589810848236, 0.06193933263421059, -0.26265382766723633, 0.34836119413375854, -0.18882790207862854, -0.17976266145706177, 0.6010205745697021, -1.3945655822753906, -0.4953092336654663, -1.1708544492721558, -0.6580682396888733, 0.0637601912021637, 0.553687572479248, 0.5652331113815308, 0.19848288595676422, 0.1876368224620819, -1.2446942329406738, 0.025003012269735336, -0.48092252016067505, 0.27163392305374146, 0.2560117840766907, -0.7897756099700928, -0.5645117163658142, 0.003046594327315688, -0.04506419599056244, -0.8010876774787903, -0.13330864906311035, -0.05652345344424248, 0.47205469012260437, 0.20170345902442932, 1.0663576126098633, -0.65834641456604, 0.7454460263252258, 0.5610184669494629, -0.42798566818237305, -0.696408212184906, 0.07304813712835312, -0.49688607454299927, -0.0837116613984108, 0.24746869504451752, 0.4107838273048401, -0.5266401767730713, 0.12113775312900543, 0.7702799439430237, 0.8008741736412048, -0.544733464717865, -0.3463374674320221, -0.498462975025177, 0.3568987548351288, -0.7711958885192871, -0.2284269630908966, -0.3195871412754059, -0.22354009747505188, 0.06859982758760452, 0.41806259751319885, 0.5346850156784058, -0.4429462254047394, -0.893968403339386, 0.7345301508903503, -0.09880819916725159, -0.03884394094347954, -0.3407939374446869, -1.0959910154342651, -1.9145876169204712, 0.2094358652830124, -1.372689962387085, 0.2591157853603363, -0.43391889333724976, -0.3069884181022644, 0.1655229926109314, -0.3378090262413025, -0.02606896311044693, 0.8857012987136841, 0.22399885952472687, 0.16768653690814972, -0.4260706901550293, -0.8029675483703613, 0.9214892983436584, 0.6531391739845276, -0.8770056366920471, 0.3381001651287079, -0.3064756989479065, 0.09723912924528122, 0.5214558243751526, 0.18963998556137085, -0.6047411561012268, -0.8523892164230347, -1.0482369661331177, 0.2259150594472885, -0.7845010757446289, 0.39424553513526917, -1.2458668947219849, 0.4021511375904083, 0.4252384901046753, 0.08088945597410202, 0.022314250469207764, 0.7543164491653442, -0.7959755063056946, -0.7949110269546509, 0.08421745896339417, -1.0844159126281738, 0.0527472160756588, 0.100635826587677, -0.14233268797397614, -0.45376160740852356, 0.4166737198829651, 0.1159726083278656, -1.0549557209014893, -1.1539053916931152, 0.4682794511318207, -0.38220667839050293, 0.1777079999446869, 0.24146515130996704, -0.43458887934684753, -1.1563191413879395, -0.4989837408065796, -0.5607048869132996, 0.047969501465559006, -0.3682272732257843, 1.638249397277832, 1.1777559518814087, -1.1412056684494019, -0.09482868015766144, 0.5431473255157471, 0.7729669213294983, -0.16235104203224182, 0.6815655827522278, -0.05008295178413391, -0.32168132066726685, 0.27974733710289, 0.1739683896303177, 0.22722908854484558, -0.7910971641540527, 0.32957133650779724, 0.7542836666107178, 0.04164663702249527, -0.180874302983284, 1.153245449066162, 0.29998812079429626, -1.2296570539474487, 0.33967211842536926, -0.851044774055481, -0.6594706177711487, -0.4160279929637909, 0.7745723724365234, -0.2675822973251343, -0.44810476899147034, -0.8297730684280396, -0.35784217715263367, 0.7323827147483826, -0.08913059532642365, -0.5184020400047302, -0.02528853341937065, -0.4922235310077667, -0.20489998161792755, 0.9318476915359497, 1.4490119218826294, -0.8999744057655334, -0.7148362398147583, -0.4319083094596863, -0.7381458878517151, -0.32677459716796875, -0.513526976108551, -0.26572152972221375, -0.12592683732509613, 0.7815248966217041, 1.0132402181625366, 0.20077332854270935, -0.3072473704814911, 0.23761169612407684, 0.3486219644546509, 0.7502105236053467, 0.05135602504014969, -0.746543824672699, 0.15626558661460876, 1.2045711278915405, 1.2037309408187866, -1.1226751804351807, -0.1673794537782669, -0.6377511620521545, -0.4959498643875122, 1.0103204250335693, 0.7301625609397888, -0.2132146805524826, 1.1857277154922485, -0.44911566376686096, 0.34174755215644836, 0.3960956037044525, -1.057082176208496, -0.2900697588920593, 1.065049409866333, 1.5047051906585693, 0.4309537410736084, 0.47035926580429077, 0.2151566594839096, 0.18524886667728424, 0.3366345763206482, -0.22317825257778168, 0.8742001056671143, 0.3154126703739166, -0.2926250696182251, -0.10321357101202011, 0.009683665819466114, 0.5972715616226196, -0.16461241245269775, -0.21768662333488464, 0.09603697806596756, 0.554692268371582, 0.7170462608337402, 0.6036580204963684, 0.9229522943496704, 0.15824303030967712, 0.24422821402549744, 0.77210932970047, 0.32196202874183655, -0.9640743136405945, 0.26938721537590027, -0.5783572196960449, -0.9525206089019775, 0.3332817256450653, -0.45234090089797974, -0.3631035387516022, -0.28088775277137756, 0.4829518496990204, 0.27785035967826843, -0.8029725551605225, 0.38464754819869995, 1.159999966621399, 0.5732160210609436, 0.19026249647140503, -0.49121949076652527, -0.38519659638404846, -0.3237493336200714, -0.806199312210083, -0.1232905313372612, -0.1912558227777481, -0.1608576476573944, -0.8946096301078796, 0.09849987179040909, 0.07862554490566254]}, "authors": [{"authorId": "2156559", "name": "Qihang Yu"}, {"authorId": "2266472250", "name": "Xiaohui Shen"}, {"authorId": "2266697544", "name": "Liang-Chieh Chen"}], "references": [{"paperId": "1ddbd08ad8cf22a5c66c4242194c4286328533bf", "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "2b26b17fe3a909bc0f5408b3328308153b31f22e", "title": "Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP"}, {"paperId": "8b1ac02ae662c06de72daeac171a0a4461f1e692", "title": "Improving Multimodal Datasets with Image Captioning"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "962ccf1fc49c83817fb031e5b24b81b19cdfb89d", "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"}, {"paperId": "094883e42bb9a41f602c0715c1059bc431e33fb2", "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"}, {"paperId": "e2a58fd18961c3941102989e3a3d0d27c615e015", "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"}, {"paperId": "3b6179c293df29e31d31cea46476f104ab6950f2", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World"}, {"paperId": "5d3b032270276a0c17ef5fb76679291186dccdca", "title": "DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model"}, {"paperId": "77424562deba33e94ea5ca3c662ccfdc2b95fb5c", "title": "Vocabulary-free Image Classification"}, {"paperId": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "971a8b1eebed7b73cdac896e3872dd8cbee04662", "title": "V3Det: Vast Vocabulary Visual Detection Dataset"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "362983f6f6b3e0335f1267b7b9d0288cc4d2619f", "title": "FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation"}, {"paperId": "af6db7ae134ebad3fc12c34ff3a3c2139aa97bd8", "title": "Detecting Everything in the Open World: Towards Universal Object Detection"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "323400245885e08ad498cd108e30e18020662278", "title": "Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models"}, {"paperId": "9a6ce0b13dbc307ee4ea4d6170a6baace88ae6ed", "title": "LMSeg: Language-guided Multi-dataset Segmentation"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "641d7866db6691e22aa36de5c8ba05804233c016", "title": "Side Adapter Network for Open-Vocabulary Semantic Segmentation"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "e45a9242a977352ab1d7379e9e2a01dde82663ed", "title": "NMS Strikes Back"}, {"paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"}, {"paperId": "26c80bd65baa90f5b18157de4951f4eb0b62ab69", "title": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"}, {"paperId": "29c2d3d77b6d6f24f4356d5ba20c1a6ab4229c76", "title": "Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP"}, {"paperId": "31a9744bd5421b3fbbad2ab38ce33bb2f352c77a", "title": "CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation"}, {"paperId": "a26a7a74f1e5fd562be95c3611a0680759fbdf84", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "a09cbcaac305884f043810afc4fa4053099b5970", "title": "Exploring Plain Vision Transformer Backbones for Object Detection"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "86b42cac364985919987789795be7c3a577ee3de", "title": "Detecting Twenty-thousand Classes using Image-level Supervision"}, {"paperId": "8d737dc6a91a7bfde20aed7bb13d100476de5ae3", "title": "Scaling Open-Vocabulary Image Segmentation with Image-Level Labels"}, {"paperId": "658a017302d29e4acf4ca789cb5d9f27983717ff", "title": "Masked-attention Mask Transformer for Universal Image Segmentation"}, {"paperId": "5c1dd63a45dc56009d1d499c8c2f4d7b9953a507", "title": "PartImageNet: A Large, High-Quality Dataset of Parts"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "8cbd24c1fea7a73c23a25365b17e9a9a65b28b15", "title": "Learning Open-World Object Proposals without Learning to Classify"}, {"paperId": "260ad39a1dac4b451019e2bf17925f4df8e3b69a", "title": "Per-Pixel Classification is Not All You Need for Semantic Segmentation"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "1200ce0e0f604f742db6d7fb05c97a8efbc8a8d4", "title": "Simple Multi-dataset Detection"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "787119e3c3f819244c82b7d97779473773e60696", "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "1a09b8d7946bb61deffe21cdd453b2c53cdd1634", "title": "MSeg: A Composite Dataset for Multi-Domain Semantic Segmentation"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "5732c402d3e30902975c5258017f3044570ec727", "title": "Context Prior for Scene Segmentation"}, {"paperId": "80376bdec5f534be78ba82821f540590ebce5559", "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "f902a64f7d08aaa6bfca7463e8729952ddc6134e", "title": "LVIS: A Dataset for Large Vocabulary Instance Segmentation"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "9217e28b2273eb3b26e4e9b7b498b4661e6e09f5", "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"}, {"paperId": "dce916351ef589afa7a63452648dd8acba931e92", "title": "Panoptic Segmentation"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "ee4a012a4b12d11d7ab8c0e79c61e807927a163c", "title": "Rethinking Atrous Convolution for Semantic Image Segmentation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "cab372bc3824780cce20d9dd1c22d4df39ed081a", "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"}, {"paperId": "c8c494ee5488fe20e0aa01bddf3fc4632086d654", "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}, {"paperId": "7ffdbc358b63378f07311e883dddacc9faeeaf4b", "title": "Fast R-CNN"}, {"paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, {"paperId": "39ad6c911f3351a3b390130a6e4265355b4d593b", "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs"}, {"paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0", "title": "Fully convolutional networks for semantic segmentation"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "caf202fd5833b1ef635923e79608e1a48d7539f9", "title": "Detect What You Can: Detecting and Representing Objects Using Holistic Models and Body Parts"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "96a6df2b4aa50cfbd8984933e9c66b0763fc08a6", "title": "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"}, {"paperId": "f3b1dd33a2a8b533a0c08382b2a2bbf721beac21", "title": "k-means Mask Transformer"}, {"paperId": "42254a2361f8a1380f3593fbc56ece21da806103", "title": "Open-Vocabulary Panoptic Segmentation with MaskCLIP"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": null, "title": "Edinburgh Research Explorer The PASCAL Visual Object Classes (VOC) Challenge"}, {"paperId": null, "title": "An open-source chatbot impressing gpt-4 with 90%* chatgpt"}]}