{"paperId": "b6fabf7f4a8549ff22fdac29874c417c44e0dcbd", "title": "Teacher-student knowledge distillation from BERT", "abstract": "Since 2017, natural language processing (NLP) has seen a revolution due to new neural language models \u2013 Transformers (Vaswani et al., 2017). Pre-trained on large text corpora and widely applicable even for NLP tasks with little data, Transformer models like BERT (Devlin et al., 2019) became widely used. While powerful, these large models are too computationally expensive and slow for many practical applications. This inspired a lot of recent effort in compressing BERT to make it smaller and faster. One particularly promising approach is knowledge distillation, where the large BERT is used as a \u201cteacher\u201d from which much smaller \u201cstudent\u201d models \u201clearn\u201d. Today, there is a lot of work on understanding the linguistic skills possessed by BERT, and on compressing the model using knowledge distillation. However, little is known about the learning process itself and about the skills learnt by the student models. I aim to explore both via practical means: By distilling BERT into two architecturally diverse students on diverse NLP tasks, and by subsequently analysing what the students learnt. For analysis, all models are probed for different linguistic capabilities (as proposed by Conneau et al. (2018)), and the models\u2019 behaviour is inspected in terms of concrete decisions and the confidences with which they are made. Both students \u2013 a down-scaled BERT and a bidirectional LSTM model \u2013 are found to learn well, resulting in models up to 14,000x smaller and 1,100x faster than the teacher. However, each NLP task is shown to rely on different linguistic skills and be of different difficulty, thus requiring a different student size and embedding type (word-level embeddings vs sub-word embeddings). On a difficult linguistic acceptability task, both students\u2019 learning is hindered by their inability to match the teacher\u2019s understanding of semantics. Even where students perform on par with their teacher, they are found to rely on easier cues such as characteristic keywords. Analysing the models\u2019 correctness and confidence patterns shows how all models behave similarly on certain tasks and differ on others, with the shallower BiLSTM student better mimicking the teacher\u2019s behaviour. Finally, by probing all models, I measure and localise diverse linguistic capabilities. Some possessed language knowledge is found to be merely residual (not necessary), and I demonstrate a novel use of probing for tracing such knowledge back to its origins.", "venue": "", "year": 2020, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work distilling BERT into two architecturally diverse students on diverse NLP tasks, and subsequently analysing what the students learnt, demonstrates a novel use of probing for tracing such knowledge back to its origins."}, "embedding": {"model": "specter_v2", "vector": [0.07363377511501312, 0.989652693271637, -0.26286208629608154, 0.032226357609033585, -0.48369166254997253, -0.6233189702033997, 0.5328658819198608, 0.26368096470832825, -0.6001221537590027, -0.09465881437063217, 0.3491423726081848, -0.6685299277305603, 0.17181381583213806, -0.0188238937407732, -0.13866394758224487, 0.07112424075603485, -0.44912731647491455, 0.605167031288147, 0.041061315685510635, -0.2647167444229126, -0.2018509954214096, -0.5572623014450073, -0.7981927394866943, -0.030432183295488358, 0.7595774531364441, 0.2712899446487427, 0.17948059737682343, 0.8515597581863403, -0.43807336688041687, 0.5519316792488098, 0.42479628324508667, -0.29654166102409363, 0.23662233352661133, 0.3930905759334564, -0.35415223240852356, -0.7180805206298828, 0.5491862893104553, -0.41322600841522217, -0.3826219141483307, 0.5911546945571899, -0.1436612755060196, 0.24948053061962128, 0.3747808635234833, -0.5017459988594055, -0.5643362998962402, 1.0086506605148315, 0.8324917554855347, 0.9613432288169861, -0.036859892308712006, -0.20119403302669525, 1.4159355163574219, -1.242154598236084, -0.048718687146902084, 1.1607271432876587, 0.609644889831543, 0.40611013770103455, -0.6033777594566345, -0.695440948009491, 0.3072899281978607, 0.2878812253475189, -0.7966570258140564, 0.0219334214925766, -0.17555467784404755, -0.1403210312128067, 1.6904958486557007, -0.43944987654685974, 0.388156920671463, 0.26984676718711853, -0.5233976244926453, 1.7414662837982178, 0.7213518619537354, -0.8301962018013, -0.4723161458969116, 0.8158748149871826, 0.6617813110351562, 0.9197324514389038, -0.1825532615184784, 0.43923118710517883, -0.6960111856460571, -0.1370134949684143, 0.31110939383506775, -0.18905550241470337, -0.202911838889122, -0.40637925267219543, -0.4580771028995514, 0.644920289516449, 0.17272186279296875, 0.7075968980789185, -0.11815512180328369, 0.45230457186698914, 0.5104527473449707, 0.6326308250427246, -0.36896419525146484, 0.723201334476471, -0.935322642326355, 0.2074628323316574, -1.0194512605667114, 0.0642886757850647, 0.5449294447898865, 0.4035644233226776, 0.02647124044597149, 0.3818369209766388, -0.36639171838760376, -0.3003847002983093, 1.3428537845611572, 0.23695848882198334, 0.8292456865310669, -0.7358496189117432, 0.0022999148350208998, -0.5718811750411987, 0.20096392929553986, -0.8196675777435303, 0.08172409236431122, -0.27576935291290283, -0.13278387486934662, -1.5130473375320435, -0.3219141662120819, -0.1651322990655899, -0.8633996248245239, 0.7802835702896118, -0.2882983386516571, 0.6903641819953918, 0.5478163361549377, 0.36109811067581177, 0.9138070344924927, 0.666369616985321, 0.25782084465026855, 0.368723601102829, 0.9508424401283264, -0.4854664206504822, -0.42573752999305725, -1.0074924230575562, 0.9822953343391418, 0.1808985322713852, -0.022040974348783493, 0.14626972377300262, -1.4067453145980835, -1.1592354774475098, -0.7930222749710083, 0.16871124505996704, -0.9020287990570068, 0.11481079459190369, 0.8261739015579224, 0.8644065856933594, -1.1818246841430664, 0.6347349882125854, -0.18390905857086182, -0.0588127039372921, 0.02653813362121582, 0.6683677434921265, 0.1270982176065445, -0.48826953768730164, -1.4497591257095337, 0.7125045657157898, 0.9265854358673096, -0.4963495433330536, -0.27934518456459045, -0.7539722919464111, -1.236294150352478, 0.14449812471866608, 0.2998819053173065, -0.33604124188423157, 1.1891095638275146, 0.010635674931108952, -1.2419087886810303, 0.7106111645698547, -0.19735226035118103, 0.019336821511387825, 0.26034772396087646, -0.03120407834649086, -0.4657846689224243, -0.3563535511493683, -0.04579629376530647, 0.4803686738014221, 0.09223026782274246, -0.28565675020217896, -0.011464554816484451, 0.39304178953170776, -0.2568940818309784, -0.3493703007698059, -0.3108513057231903, 0.5521735548973083, 0.3207964301109314, -0.11508821696043015, 0.29842138290405273, 0.8394569754600525, 0.15732631087303162, -0.16692255437374115, -0.36709028482437134, -1.0625656843185425, 0.35504794120788574, -0.17397910356521606, 0.7530643939971924, -0.7585099935531616, -0.8105670213699341, 0.10132601857185364, 0.0383085273206234, 0.39187443256378174, -0.9614615440368652, 0.5684667825698853, -0.5302074551582336, 0.737751841545105, -0.10763218998908997, -0.7882387638092041, 0.15574046969413757, 0.040632136166095734, -0.6085717082023621, -0.4009820818901062, 0.1166820153594017, 1.0915586948394775, -0.8788024187088013, 0.09756840020418167, 0.2618100941181183, 0.19379831850528717, -1.3250523805618286, 1.1387735605239868, -0.32139700651168823, 0.27291908860206604, 0.024171294644474983, -0.44355136156082153, 0.3685101270675659, 0.058009520173072815, 0.2030661404132843, -0.1976618766784668, 0.053712643682956696, 0.7681313753128052, -0.9059079885482788, 1.1919788122177124, -0.4315771460533142, 0.4445324242115021, 0.22619245946407318, -1.4550228118896484, 0.4056762754917145, 0.640920102596283, -0.5186079740524292, -0.08685676008462906, 0.1785799264907837, 0.960353434085846, 0.0073753646574914455, 0.013694572262465954, 0.7554559111595154, 0.4597235321998596, -0.11177314817905426, 0.35812923312187195, 0.6006704568862915, -0.8101701736450195, 0.17303027212619781, 0.2915864884853363, 0.3805312514305115, 0.321522057056427, 0.3225403130054474, -0.22286203503608704, 0.5813061594963074, -0.7327163219451904, -0.11153817921876907, 0.3265167474746704, 0.4127954840660095, 0.36945992708206177, 0.2628753185272217, -0.6282727122306824, 0.05404329672455788, -0.3311856985092163, 0.5543522834777832, 1.2303327322006226, -0.3962442874908447, -0.1951974779367447, -0.2716860771179199, -0.28725215792655945, -0.21683114767074585, 0.47064393758773804, -0.3566770553588867, -0.19070470333099365, -0.9554775953292847, -0.45190054178237915, 0.6697530150413513, 0.3838343322277069, 0.883553147315979, -0.584324836730957, -0.30861595273017883, -0.3282667100429535, 0.29002615809440613, -0.7105792164802551, -0.0414385125041008, 0.37778860330581665, -0.48101165890693665, -0.26410460472106934, 0.10001839697360992, -0.6817871332168579, 0.1904996633529663, -0.8547971844673157, 0.9380006790161133, -0.3443267047405243, -0.2455988973379135, 0.3836742341518402, 0.920354962348938, -0.6175565719604492, -0.6035308837890625, 0.05712110921740532, 0.2373388707637787, -0.614012598991394, 0.4974294602870941, 0.2847694754600525, 0.1030738353729248, -0.12435264140367508, -0.5154473185539246, 0.034514136612415314, -0.09859144687652588, 0.033730946481227875, 0.4765084385871887, -0.011550157330930233, 0.07326190918684006, -1.1351830959320068, 0.7185869216918945, -0.08314184844493866, -0.38187041878700256, 0.20448240637779236, -0.6861512660980225, -0.2570251226425171, 0.7949504256248474, -0.6303332448005676, 0.11823264509439468, -1.1749321222305298, 0.23027020692825317, -0.08029325306415558, -0.3652682602405548, 0.6506147384643555, 0.48391348123550415, 0.35416379570961, 0.1340414434671402, 0.6619025468826294, 0.4159722924232483, -0.1367407739162445, 0.9290956258773804, -0.907545804977417, 0.38727447390556335, -0.03860648721456528, 0.25007152557373047, -0.49255773425102234, -0.48856350779533386, -0.5070704817771912, -0.580077588558197, -0.18000902235507965, -0.2795338034629822, 0.21167556941509247, -0.059553343802690506, -0.7408371567726135, -1.1051020622253418, 0.13807064294815063, -1.006914496421814, -0.5372536182403564, -0.4863583743572235, -0.19987697899341583, -0.06801711767911911, -1.089587688446045, -1.1988608837127686, -0.27178120613098145, -0.07419969141483307, -1.0669238567352295, -0.1255425363779068, 0.16637293994426727, -0.41959285736083984, -1.141397476196289, 0.2774748206138611, -0.019580770283937454, 1.2372945547103882, -0.7675364017486572, 1.2151483297348022, -0.7317043542861938, -0.04247622564435005, -0.19160543382167816, 0.11061917990446091, 0.6180735230445862, -0.39537540078163147, 0.0304972343146801, -0.7229048609733582, 0.14471590518951416, -0.22542577981948853, -0.9541616439819336, 0.02063921093940735, -0.1268504559993744, 0.4185809791088104, -0.4683160185813904, -0.30216366052627563, 0.0825071781873703, 1.356144666671753, -0.8265146613121033, 0.05694495141506195, 0.21350236237049103, 1.0896939039230347, 0.48125308752059937, -0.5723959803581238, -0.0996202602982521, 0.6247432231903076, -0.004021653905510902, -0.05702618882060051, 0.40168869495391846, 0.048401981592178345, -0.7467066645622253, 0.7608440518379211, 1.7778044939041138, 0.5164054036140442, 0.23759034276008606, -1.3099740743637085, 0.6468979716300964, -0.8413787484169006, -0.14020776748657227, 0.7722200155258179, 0.6326835751533508, 0.9371275305747986, -0.5050758123397827, -0.7899150252342224, 0.07880350202322006, 0.24602864682674408, 0.20753206312656403, -0.34224364161491394, -0.9316214323043823, 0.3633408844470978, 1.012261152267456, -0.09984363615512848, 0.8942621946334839, 0.14370720088481903, 0.6980624794960022, 14.762901306152344, 0.9779767394065857, 0.03719518333673477, 0.3649809658527374, 0.3331325948238373, 0.5270171165466309, -0.7496758103370667, -0.09115388989448547, -1.2265392541885376, -0.24940825998783112, 1.4714341163635254, 0.5066892504692078, 0.31061550974845886, 0.06718820333480835, -0.37152692675590515, -0.09585043787956238, -0.6540020108222961, 0.47774454951286316, 0.6964753270149231, -1.0847145318984985, 0.49025771021842957, 0.2398069053888321, 0.4525269567966461, 0.3531820774078369, 0.8527952432632446, 1.1430025100708008, 0.541926383972168, -0.63083815574646, 0.7627931833267212, 0.21566615998744965, 0.8742038011550903, 0.07556234300136566, 0.6613621711730957, 0.9492720365524292, -0.7357252240180969, -0.3674723505973816, -0.18981623649597168, -1.2534481287002563, -0.18521516025066376, -0.06853967905044556, -0.4207998812198639, -0.9357854723930359, -0.5676285624504089, 0.5097755789756775, -0.04457448050379753, 0.07486571371555328, -0.46854308247566223, 0.7115767598152161, -0.24720065295696259, -0.20739015936851501, 0.20924250781536102, 0.45223087072372437, 0.4494790732860565, -0.19409291446208954, 0.1309075653553009, -0.035649511963129044, 0.3452034890651703, 0.9219306707382202, -0.19392479956150055, -0.17753861844539642, -0.1513659805059433, -0.5379863381385803, -0.0059631443582475185, 0.6309002637863159, 0.5884107947349548, 0.2474164217710495, -0.16510562598705292, 0.2505275309085846, 0.7748350501060486, 0.05374395102262497, 0.2852421998977661, -0.20727118849754333, 0.2317686229944229, -0.18161894381046295, 0.20349246263504028, 0.24471381306648254, -0.1262793242931366, -0.5672122240066528, -0.8183749914169312, -0.333360493183136, 0.13690422475337982, -1.0198769569396973, -0.6371567845344543, 0.39787381887435913, -0.38400402665138245, -0.23373787105083466, -0.004769027233123779, -1.0544358491897583, -0.3912624418735504, 0.4602217972278595, -1.8102961778640747, -0.6151407957077026, 0.5545452237129211, -0.05815185606479645, -0.6075699329376221, -0.14980974793434143, 1.51395845413208, -0.2350715845823288, -0.4224901795387268, -0.037867479026317596, 0.1008998453617096, 0.35304179787635803, -0.28584086894989014, -1.2836017608642578, 0.8442869782447815, -0.01859317347407341, 0.07188554108142853, 0.5584476590156555, -0.13286161422729492, 0.27197739481925964, -0.7005261778831482, 0.3088332712650299, 0.9511862397193909, -1.061317801475525, 0.05085042864084244, -0.5489305257797241, -0.7280333042144775, 0.48339393734931946, 0.650080144405365, -0.2813585698604584, 0.6723135709762573, -0.22348646819591522, -0.8324089646339417, 0.24536573886871338, -1.173618197441101, 0.2296864092350006, -0.015065759420394897, -0.9443289041519165, -1.0051789283752441, -0.17583541572093964, 0.37673628330230713, -0.8290844559669495, -0.5802543759346008, -0.2987537682056427, -0.05420655384659767, -0.05069258436560631, 1.0060386657714844, -0.47573837637901306, 0.4533050060272217, 1.0351530313491821, -0.07291154563426971, -1.022493600845337, 0.24192824959754944, -0.7116769552230835, -0.3476339876651764, 0.010896733030676842, 0.8474467396736145, -0.823152482509613, 0.4036068022251129, 1.3117660284042358, 0.30324751138687134, -0.6055935025215149, -0.2885265052318573, -0.11310544610023499, 0.49176743626594543, -0.650906503200531, 0.20931555330753326, 0.31162795424461365, 0.01029080431908369, 0.257973849773407, 1.0495600700378418, 0.3460058867931366, -0.42634502053260803, -0.8607485294342041, 0.21104834973812103, -0.4760030508041382, -0.20720158517360687, -0.3394341468811035, -0.5460705757141113, -1.5787891149520874, 0.3549163043498993, -1.678216814994812, 0.11755463480949402, -1.1231443881988525, -0.5793898701667786, -0.39042651653289795, -0.39689570665359497, 0.15221834182739258, 0.2765451669692993, -0.3357452154159546, -0.480991929769516, -0.6364917755126953, -0.057915084064006805, 0.4713366627693176, 0.7917726635932922, -0.361025869846344, 0.10813016444444656, -0.13211731612682343, -0.3215504586696625, 0.45958849787712097, 0.5217609405517578, -0.2812044024467468, -0.8245680928230286, -1.342968225479126, 0.6101908087730408, -0.2969839870929718, -0.17175793647766113, -0.5232335329055786, 0.9001843333244324, 0.6716800332069397, -0.31353524327278137, 0.12706175446510315, 0.4044852554798126, -1.0388388633728027, -0.9707440137863159, 0.43087637424468994, -0.7156238555908203, -0.07482835650444031, 0.4471842050552368, -0.5166071653366089, -0.13485537469387054, 0.2110481709241867, -0.16747304797172546, -1.1993622779846191, -0.7383518815040588, 0.12523597478866577, -0.8600829243659973, 0.4085594415664673, -0.3172021210193634, -0.44095879793167114, -1.0398627519607544, -0.12350286543369293, -0.25994640588760376, 0.5499611496925354, -0.6067737936973572, 0.41662055253982544, 0.32497745752334595, -0.8228012323379517, 0.1536044478416443, 0.6576147079467773, -0.2165595442056656, 0.1325380951166153, 0.17421270906925201, 0.38217902183532715, -0.6745416522026062, 0.5260551571846008, 0.23786510527133942, 0.23905350267887115, -0.375485360622406, -0.16220921277999878, 0.5590653419494629, -0.633800745010376, -0.28638842701911926, 1.1050872802734375, -0.8176113367080688, -1.8842103481292725, 0.22638747096061707, -1.2597122192382812, -0.578082799911499, -0.10808822512626648, 0.5305950045585632, 0.025963574647903442, -0.4597213864326477, 0.14385057985782623, -0.03627632185816765, 0.4877518117427826, 0.08083951473236084, -0.661981999874115, 0.4629565179347992, -0.11459284275770187, -0.3721494674682617, 0.6786403656005859, 0.886597216129303, -0.36942169070243835, -0.2500494718551636, -0.8027710914611816, -0.12708845734596252, 0.19054415822029114, 0.18680334091186523, -0.6122125387191772, -0.47777655720710754, 1.0176881551742554, 0.4034264385700226, 0.37817996740341187, -0.2158583402633667, -0.3158367872238159, 0.48395267128944397, 0.990196704864502, 0.1456262767314911, -0.22360166907310486, -0.3564932942390442, 1.759193778038025, 0.5952338576316833, -0.9749794006347656, 0.02950154058635235, -0.19359247386455536, -0.4838860034942627, 0.8776092529296875, 0.3484465777873993, -0.23495444655418396, 1.042926549911499, -0.36884331703186035, 0.1566864401102066, 0.04628441110253334, -0.5174509882926941, -0.12199223041534424, 0.5141851305961609, 1.2057212591171265, 0.6119292378425598, 0.4889802932739258, 0.4266302287578583, 1.0907269716262817, -0.369885116815567, 0.2172725349664688, 0.8202464580535889, 0.649568498134613, -0.6410508155822754, 0.024060923606157303, 0.11875349283218384, 0.6182769536972046, -0.47879984974861145, -1.0438430309295654, 0.04108668491244316, 0.6577152013778687, 0.3244328498840332, 0.40368640422821045, 0.5130772590637207, 0.2488139420747757, 0.3990836441516876, 0.5542020201683044, 0.36038121581077576, -0.7603783011436462, -0.45694437623023987, -0.8627524375915527, -0.5307337641716003, 0.05801309645175934, -0.3703910708427429, -0.10121823102235794, -0.5647494196891785, -0.19796940684318542, 0.10087080299854279, 0.3144944906234741, 0.12721022963523865, 1.0907632112503052, 0.34606295824050903, 0.2687810957431793, -0.5857312679290771, -0.22107794880867004, -0.5605055689811707, -0.7288677096366882, -0.17089751362800598, -0.8903844952583313, 0.040885526686906815, 0.0822509378194809, -0.6692851781845093, -0.5526584982872009]}, "authors": [{"authorId": "2097509550", "name": "Sam Su\u010d\u0301\u0131k"}], "references": [{"paperId": "159dc82a5ee901716b0154051988b5408acfc861", "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "c79a8fd667f59e6f1ca9d54afc34f792e9079c7e", "title": "TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding"}, {"paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0", "title": "A Primer in BERTology: What We Know About How BERT Works"}, {"paperId": "7d767f64e88fdec81a24190c629dcfe23c940793", "title": "Natural Language Generation for Effective Knowledge Distillation"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "c1957e25155d713e7599b9d7e1e318c03cd2631a", "title": "Distilling Transformers into Simple Neural Networks with Unlabeled Transfer Data"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "ddd27dba038d0ed14c48cd027812df58a902ece2", "title": "AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models"}, {"paperId": "2f9d4887d0022400fc40c774c4c78350c3bc5390", "title": "Small and Practical BERT Models for Sequence Labeling"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "335613303ebc5eac98de757ed02a56377d99e03a", "title": "What Does BERT Learn about the Structure of Language?"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "165d51a547cd920e6ac55660ad5c404dcb9562ed", "title": "Open Sesame: Getting inside BERT\u2019s Linguistic Knowledge"}, {"paperId": "809cc93921e4698bde891475254ad6dfba33d03b", "title": "How Multilingual is Multilingual BERT?"}, {"paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee", "title": "What do you learn from context? Probing for sentence structure in contextualized word representations"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "1a9954d86466a7e4de6f98ddee452ceb50e15d86", "title": "DocBERT: BERT for Document Classification"}, {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"}, {"paperId": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89", "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"}, {"paperId": "425482094edea7deef11287bc27d73b84ee7c32e", "title": "Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "668f42a4d4094f0a66d402a16087e14269b31a1f", "title": "Analysis Methods in Neural Language Processing: A Survey"}, {"paperId": "c586ee4c556e8e7f24c156e1fb982ed2170e2ff1", "title": "On-Device Neural Language Model Based Word Prediction"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "d7701e78e0bfc92b03a89582e80cfb751ac03f26", "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning"}, {"paperId": "c41516420ddbd0f29e010ca259a74c1fc2da0466", "title": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties"}, {"paperId": "1abec8d03b8d1435ae920edea9b437be913a30ca", "title": "Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "7113bd87c3e6f727efae24ee52f20c81358da761", "title": "SentEval: An Evaluation Toolkit for Universal Sentence Representations"}, {"paperId": "682b9d2212258fd5edbfca589c86390c31a956b0", "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "d821ce08da6c0084d5eacbdf65e25556bc1b9bc3", "title": "Does String-Based Neural MT Learn Source Syntax?"}, {"paperId": "2d5e4b61cac1a509ad5f174863130c5bbd6d1a95", "title": "Deep Model Compression: Distilling Knowledge from Noisy Teachers"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "e44da7d8c71edcc6e575fa7faadd5e75785a7901", "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks"}, {"paperId": "57a10537978600fd33dcdd48922c791609a4851a", "title": "Sequence-Level Knowledge Distillation"}, {"paperId": "f5a7da72496e2ca8edcd9f9123773012c010cfc6", "title": "Neural Architectures for Named Entity Recognition"}, {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning"}, {"paperId": "6721229d6b6c4abf323f1540fc8d727d2381848d", "title": "Distilling Model Knowledge"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation"}, {"paperId": "66021a920001bc3e6258bffe7076d647614147b7", "title": "From Word Embeddings To Document Distances"}, {"paperId": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "title": "Skip-Thought Vectors"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "8604f376633af8b347e31d84c6150a93b11e34c2", "title": "FitNets: Hints for Thin Deep Nets"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification"}, {"paperId": "b0aca3e7877c3c20958b0fae5cbf2dd602104859", "title": "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts"}, {"paperId": "dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71", "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps"}, {"paperId": "d770060812fb646b3846a7d398a3066145b5e3c8", "title": "Do Deep Nets Really Need to be Deep?"}, {"paperId": "1a2a770d23b4a171fa81de62a78a3deb0588f238", "title": "Visualizing and Understanding Convolutional Networks"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "944a1cfd79dbfb6fef460360a0765ba790f4027a", "title": "Recurrent Continuous Translation Models"}, {"paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17", "title": "Generating Sequences With Recurrent Neural Networks"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "8729441d734782c3ed532a7d2d9611b438c0a09a", "title": "ADADELTA: An Adaptive Learning Rate Method"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "bc1022b031dc6c7019696492e8116598097a8c12", "title": "Natural Language Processing (Almost) from Scratch"}, {"paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, {"paperId": "57458bc1cffe5caa45a885af986d70f723f406b4", "title": "A unified architecture for natural language processing: deep neural networks with multitask learning"}, {"paperId": "30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9", "title": "Model compression"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f", "title": "Autoencoders, Minimum Description Length and Helmholtz Free Energy"}, {"paperId": "319f22bd5abfd67ac15988aa5c7f705f018c3ccd", "title": "Learning internal representations by error propagation"}, {"paperId": "155345976aa505a10a45e9119f2853df4d7999d7", "title": "Comparison of the predicted and observed secondary structure of T4 phage lysozyme."}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Pruning BERT to accelerate inference"}, {"paperId": null, "title": "2019b) use no learning rate scheduling, but they report small batch sizes"}, {"paperId": null, "title": "2019a) report not tuning their BiLSTM hyperparameters, I"}, {"paperId": "fe82735fe8ae2163a37aa2787eee0db8efc745b6", "title": "transformers . zip : Compressing Transformers with Pruning and Quantization"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Like what you like: Knowledge distill via neuron selectivity transfer"}, {"paperId": "eeb748158f662b9e233108c69fe536800e6a3c9f", "title": "Inflecting Verbs with Word Embeddings: A Systematic Investigation of Morphological Information Captured by German Verb Embeddings"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": null, "title": "B.1.1 Choosing learning algorithm and learning rate"}]}