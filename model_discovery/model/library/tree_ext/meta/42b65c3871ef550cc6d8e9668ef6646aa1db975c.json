{"paperId": "42b65c3871ef550cc6d8e9668ef6646aa1db975c", "title": "An Optimized Data\ufb02ow for Mitigating Attention Performance Bottlenecks", "abstract": "Attention mechanisms form the backbone of state-of-the-art machine learning models for a variety of tasks. Deploying them on deep neural network (DNN) accelerators, however, is prohibitively challenging especially under long sequences, as this work identi\ufb01es. This is due to operators in attention layers exhibiting limited reuse opportunities and quadratic growth in memory footprint, leading to severe memory-boundedness. To address this, we introduce a new attention-tailored data\ufb02ow, termed FLAT , which identi\ufb01es fusion opportunities within the attention layer, and implements an on-chip memory-aware interleaved execution and tiling mechanism. FLAT increases the effective memory bandwidth by ef\ufb01ciently utilizing the high-bandwidth, low-capacity on-chip buffer and thus achieves better run time and compute resource utilization. In our evaluation, FLAT achieves 1.94 x and 1.76 x speedup and 49% and 42% of energy reduction comparing to baseline execution over state-of-the-art edge and cloud accelerators.", "venue": "", "year": 2021, "citationCount": 11, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A new attention-tailored data architecture, termed FLAT, is introduced, which identifies fusion opportunities within the attention layer, and implements an on-chip memory-aware interleaved execution and tiling mechanism, and increases the effective memory bandwidth by utilizing the high-bandwidth, low-capacity on-chip buffer."}, "embedding": {"model": "specter_v2", "vector": [0.29704976081848145, 0.3650568425655365, -0.7959074378013611, 0.28811150789260864, 0.11736203730106354, 0.36819809675216675, 0.22315719723701477, -0.10909729450941086, -0.6504263281822205, -0.5394477844238281, 0.45145556330680847, -0.1928081214427948, 0.8511085510253906, 0.019764216616749763, 0.1348964124917984, 0.15374349057674408, -1.0643467903137207, -0.11182152479887009, 0.2528170943260193, -0.05758589878678322, 0.2591564953327179, -0.1799929141998291, -1.6378560066223145, 0.3601660132408142, 0.09943339973688126, 1.3272347450256348, 0.18829227983951569, 1.1090641021728516, -0.2687150239944458, 0.6546708941459656, 0.5857059955596924, 0.1731855720281601, 0.12666183710098267, 0.47398483753204346, -0.18999706208705902, -0.3052143454551697, 0.3898639380931854, -0.7360377311706543, -0.3312833905220032, 0.7194696068763733, 0.02224997989833355, 0.12468183785676956, -0.1046348512172699, -0.9581934213638306, 0.025077389553189278, 0.15599240362644196, 0.20902830362319946, 1.1361802816390991, -0.8715685606002808, -0.10010436177253723, 0.9675800800323486, -1.3859542608261108, -0.0587039552628994, 1.0223944187164307, 0.4688185155391693, 0.1474246233701706, 0.043212756514549255, -0.2506682276725769, 0.4959419369697571, 0.14080440998077393, -0.43634527921676636, -0.632936954498291, 0.11662273854017258, -0.057777032256126404, 2.0379815101623535, -0.24926349520683289, 0.003992332145571709, 0.40632444620132446, 0.6054925918579102, 1.26827073097229, -0.40409034490585327, -0.8961338400840759, 0.3379971385002136, -0.5858845114707947, 0.984719455242157, 0.5304449200630188, 0.10108894109725952, 0.0843174085021019, -1.1839426755905151, -0.06766291707754135, 0.478507936000824, 0.6822591423988342, 0.6960263252258301, -0.24034078419208527, -0.19576312601566315, 0.5559369325637817, 0.7638015747070312, 0.40662750601768494, -0.5860342383384705, 1.0145183801651, 1.0025038719177246, -0.36752527952194214, -0.03530581295490265, -0.009631192311644554, 0.2874988913536072, 0.22849464416503906, -0.8706307411193848, -0.33487042784690857, -0.36113178730010986, 1.2130539417266846, -0.296286940574646, 0.529331624507904, -0.9265398979187012, -0.266198068857193, 0.69270920753479, 0.019144175574183464, 0.20610448718070984, -0.48612090945243835, 0.09613849222660065, -0.46363720297813416, 0.03626294434070587, -0.3161492943763733, -0.3025704026222229, -0.49439823627471924, -1.298147439956665, -0.5538383722305298, -0.9652464985847473, 0.05336610600352287, -0.8345262408256531, 0.036783184856176376, -0.5916798710823059, 0.16011236608028412, -0.07717125117778778, 0.28490906953811646, 0.5882377624511719, 0.31964054703712463, 0.17148087918758392, 0.2718801200389862, 1.270797848701477, -1.824444055557251, -0.6795760989189148, -1.3533692359924316, 0.09066890925168991, -0.44170719385147095, 0.03191130980849266, -0.28900012373924255, -1.2916349172592163, -1.184225082397461, -1.098565697669983, -0.1977359503507614, -0.37430405616760254, 0.03594134747982025, 1.3204412460327148, 0.12601013481616974, -1.4580368995666504, 0.6516532897949219, -0.7366659045219421, 0.06607285141944885, 0.4217697083950043, 0.44721153378486633, 0.9552170634269714, -0.0974288284778595, -0.5674327611923218, -0.03865579888224602, 0.18238019943237305, -0.41324159502983093, -0.21369464695453644, -0.6247822046279907, -0.6892133951187134, 0.2682720720767975, 0.18090039491653442, -0.7028431296348572, 1.4478211402893066, -0.2418661117553711, -0.8953542709350586, 0.3893599212169647, -0.2682040333747864, 0.044981084764003754, -0.13250216841697693, -0.3521733582019806, -0.6859392523765564, -0.08101263642311096, -0.3665102422237396, 0.7449122071266174, 0.8944562077522278, 0.21437478065490723, -0.3119908273220062, -0.0710507482290268, -0.20745597779750824, -0.03382512181997299, -0.6770849823951721, 1.246148705482483, -0.6746626496315002, -0.07974221557378769, 0.34225770831108093, 0.33262938261032104, -0.26441124081611633, -0.3305581510066986, -0.28893911838531494, -0.5610684752464294, 0.9808645844459534, 0.5675001740455627, 1.420042634010315, -1.1199105978012085, -1.2612993717193604, -0.21926455199718475, -0.018990233540534973, 0.06599867343902588, -0.23898713290691376, 0.07642070204019547, -0.4054253101348877, -0.04361655190587044, 0.473101943731308, -0.7505937218666077, -0.1392228752374649, -0.6380816698074341, -0.8032323122024536, -0.6780580878257751, 0.17438118159770966, 1.0588805675506592, -0.5867443680763245, 0.22639673948287964, -0.46354901790618896, 0.47858160734176636, -1.1375985145568848, 1.241485357284546, -0.21656601130962372, -0.26022493839263916, 0.002261953428387642, 0.18122781813144684, 0.19616654515266418, -0.6821023225784302, 0.6344528794288635, -0.8460895419120789, -0.24976855516433716, 0.19654496014118195, 0.02121286652982235, 1.0831477642059326, -0.46094122529029846, 0.7453532814979553, 0.37569138407707214, -0.33657971024513245, 0.2220514565706253, 0.12930519878864288, -0.3838554322719574, -0.6283919215202332, 0.815467357635498, 0.33941733837127686, -0.27992385625839233, 0.3351348638534546, 1.5692068338394165, 1.4806383848190308, -0.6080662608146667, 0.11632326990365982, 0.2772220969200134, 0.16654206812381744, 0.02984851598739624, 0.013156418688595295, 0.9115793704986572, 0.09357930719852448, 0.17684879899024963, -0.7075858116149902, 0.212470144033432, -0.8146653771400452, -0.1446806788444519, 0.4732671082019806, 0.008547690697014332, 0.4278119206428528, 0.6564866900444031, -0.9262036085128784, -0.39895331859588623, 0.12139680236577988, 0.6573340892791748, 1.8476777076721191, -0.05818614736199379, 0.43008822202682495, -0.8095030784606934, -0.285119891166687, -0.6088936924934387, -0.017417289316654205, 0.020493309944868088, -0.21602682769298553, -0.8922581076622009, -0.8524302840232849, 0.5303248167037964, 0.8874654173851013, 1.323227047920227, -1.1072579622268677, -1.4158052206039429, -0.5577632188796997, 0.6765944957733154, -0.8900904655456543, -0.8312236666679382, 0.7470435500144958, -0.9614855647087097, 0.36386966705322266, 0.3967532813549042, -0.3099062442779541, 0.40746021270751953, -0.10371408611536026, 1.3823100328445435, -0.3746814429759979, -0.40897101163864136, -0.06041789427399635, 0.7799188494682312, -0.7010611891746521, -0.054626744240522385, 0.36542558670043945, -0.07711800187826157, -0.27644988894462585, 0.5446969866752625, 0.15304677188396454, -0.4608080983161926, -0.17900574207305908, -0.4594269096851349, 0.03733160346746445, 0.5895614624023438, 0.0733371302485466, 0.7750005125999451, -0.6577048897743225, 0.08447129279375076, -0.8306260704994202, 0.7403863668441772, 0.18435673415660858, -0.3469708263874054, -0.22693660855293274, -0.5795323848724365, 0.17370368540287018, 0.4561990797519684, -0.640633225440979, -0.25842684507369995, -0.9270606637001038, 0.4253615438938141, -1.057508945465088, -0.1264285445213318, -0.4084325134754181, 0.3974810838699341, -0.01843344420194626, -0.021457796916365623, 0.2281462401151657, 0.18746134638786316, 0.4378039240837097, 0.1147560328245163, -0.5356674790382385, 0.8136343359947205, 0.08935341238975525, -0.6834407448768616, 0.09510169178247452, 0.060401514172554016, -0.6504135131835938, -0.2710919678211212, -0.06473691761493683, -0.2661380171775818, -0.6306923031806946, 0.417290061712265, -0.697592556476593, -0.9342598915100098, -0.22211216390132904, -1.3883953094482422, -0.19004900753498077, 0.40178772807121277, -0.24417614936828613, -0.09524654597043991, -1.4980028867721558, -1.0517278909683228, -0.5072120428085327, -1.6166791915893555, -1.562385082244873, 0.5265873670578003, 0.43368643522262573, -0.6194484233856201, -0.2933708131313324, -0.5591309070587158, -0.6891419887542725, 1.3924061059951782, -0.2941500246524811, 0.5913969278335571, -0.0529022291302681, -0.7142899632453918, 0.027027850970625877, -0.2333950251340866, 0.18974757194519043, -0.8039860129356384, 0.17336729168891907, -1.0225424766540527, 0.2375514954328537, -0.09142762422561646, -0.02620423212647438, 0.3262660503387451, 0.263619065284729, 1.1328747272491455, 0.20217527449131012, -0.5390990376472473, 0.34633663296699524, 1.4868896007537842, -0.416313111782074, 0.20167745649814606, -0.06561978161334991, 1.0159841775894165, -0.39220765233039856, -0.101033054292202, 0.5690228343009949, -0.47955048084259033, 0.7631610035896301, 0.4804602861404419, -0.2323303073644638, -0.11627786606550217, 0.18308456242084503, 0.6087115406990051, 1.6233688592910767, 0.4575106203556061, 0.1821661740541458, -0.7906965613365173, 0.5990820527076721, -1.2956677675247192, -0.323525071144104, 0.6116842031478882, 0.8943214416503906, -0.2455829530954361, 0.33711081743240356, -0.26467710733413696, -0.2933521568775177, 0.8477712273597717, 0.6188172101974487, -0.9336617588996887, -1.5241948366165161, 0.47375696897506714, 0.6930312514305115, 0.7145677208900452, 0.4344788193702698, -0.20347960293293, 0.1737055629491806, 14.379523277282715, 0.7981471419334412, -0.19306063652038574, 0.547209620475769, 0.7220617532730103, -0.15915273129940033, -0.012587756849825382, -0.07659405469894409, -1.3441150188446045, 0.2225555032491684, 1.6266798973083496, 0.27549993991851807, 0.07349526882171631, 0.624758780002594, -0.18758800625801086, -0.03112507238984108, -0.8321214914321899, 0.7424864768981934, 0.8627292513847351, -1.4258143901824951, -0.01052098348736763, 0.2928272485733032, 0.3706966042518616, 0.5321334004402161, 0.8251240849494934, 0.819179117679596, 0.3673591911792755, -0.1704878956079483, 0.3803114891052246, 0.3038938045501709, 1.0734983682632446, -0.7921372056007385, 0.6486982107162476, 0.1670919954776764, -1.0581684112548828, -0.006532151717692614, -0.10196829587221146, -1.4471468925476074, 0.1100085899233818, 0.35386064648628235, -0.2736365795135498, -0.49519166350364685, -0.08910858631134033, 0.15214723348617554, 0.18952089548110962, 0.22224479913711548, -0.3175814747810364, 0.03170696273446083, 0.26132479310035706, -0.30575913190841675, 0.2564299702644348, 0.626314640045166, -0.09805774688720703, 0.25958743691444397, -0.1947634071111679, -0.059551749378442764, 0.23087483644485474, 0.27982011437416077, -0.6027495861053467, -0.9343658089637756, -0.20707818865776062, 0.06688125431537628, -0.08228451013565063, 1.1027253866195679, -0.03533123806118965, 0.28287583589553833, -0.6660404205322266, 0.42304322123527527, 0.4488978683948517, -0.2531786561012268, -0.7492478489875793, -0.34513363242149353, 0.5277607440948486, -0.716252326965332, 0.11086372286081314, 0.3150504529476166, -0.8589600324630737, -0.2943325340747833, -0.6515382528305054, -0.318037748336792, 0.22460269927978516, -0.9052703380584717, -0.16608290374279022, 1.1652380228042603, -0.515936553478241, -0.07856712490320206, 0.4758659899234772, -1.000268816947937, -0.7125625610351562, 0.36433905363082886, -1.1623632907867432, -0.24579253792762756, -0.3870697319507599, -0.6262089014053345, -0.24145646393299103, 0.2558281123638153, 1.1408767700195312, 0.2598513662815094, -0.7859801650047302, 0.2095535695552826, -0.2474496215581894, -0.1911800503730774, -0.3816882073879242, -0.3109486401081085, 1.052976131439209, 0.4112379550933838, -0.7517117261886597, -0.2182331085205078, -0.018648045137524605, 0.1438681036233902, -1.0068978071212769, -0.3455066680908203, 0.2941160202026367, -0.34283173084259033, -0.28323209285736084, -0.9129676222801208, -0.7567211985588074, 0.3839365839958191, 0.8241367936134338, 0.3934459090232849, 0.33610445261001587, 0.08617546409368515, -0.28081274032592773, -0.25749707221984863, -0.26926201581954956, 0.3575807213783264, 0.6253812909126282, -0.848720133304596, -0.01971685141324997, -0.21667584776878357, 0.5978886485099792, -1.0846343040466309, -0.3630999028682709, -0.06326830387115479, 0.053509119898080826, -0.5925615429878235, 0.9821313619613647, -0.032640326768159866, 1.0320475101470947, 1.0262309312820435, 0.12176522612571716, -0.1948811113834381, -0.08956386893987656, -0.7336999773979187, -0.39285728335380554, 0.13477736711502075, 0.1637970209121704, -0.18154381215572357, 0.6176840662956238, 0.9098246097564697, -0.1390848010778427, -0.5758370757102966, -0.43778300285339355, -0.014505583792924881, -0.6008971929550171, -0.6362229585647583, 0.22165140509605408, -0.32405829429626465, 0.24835039675235748, 0.3625662922859192, 0.6872493624687195, 0.46846476197242737, 0.017728008329868317, -0.24165946245193481, 0.1418428272008896, 0.12492725998163223, -0.05004904419183731, -0.806253969669342, -0.5835756659507751, -1.1626023054122925, -0.07590124011039734, -0.8837307095527649, 0.012228650972247124, -0.18268056213855743, -0.26482564210891724, 0.24770355224609375, -0.40314486622810364, 0.04026538506150246, 0.019517939537763596, -0.08189507573843002, -0.44508662819862366, -0.23955602943897247, -0.9610927700996399, 0.7018458843231201, 0.4657454788684845, -0.21053281426429749, 0.0746689885854721, -0.3728506863117218, 0.07417568564414978, 0.5691120624542236, 0.49210330843925476, 0.039378371089696884, -0.4553280472755432, -1.429937481880188, -0.011729196645319462, -0.06649613380432129, -0.2162414938211441, -1.2712948322296143, 1.2098138332366943, 0.48085781931877136, -0.14297252893447876, -0.32413873076438904, 0.0026472455356270075, -0.9119442105293274, -0.531761646270752, 0.5298022627830505, -0.36332565546035767, 0.7546351552009583, 1.0787500143051147, -0.6241900324821472, -0.09802500903606415, 0.9105243682861328, 0.014456425793468952, -0.2264610230922699, -1.3760524988174438, 0.35636770725250244, -0.4776759147644043, 0.16261959075927734, 0.21355731785297394, -0.07956133037805557, -1.4126317501068115, 0.08978909999132156, 0.19207768142223358, 0.1896442323923111, -0.2944599986076355, 0.7907872796058655, 0.3921489417552948, -1.2452688217163086, 0.17807579040527344, 0.6768702268600464, -0.4652079939842224, 0.37318697571754456, 0.655191957950592, 0.7864776253700256, -0.6919076442718506, 0.6329559683799744, -0.43994054198265076, 0.19821898639202118, -0.9555025100708008, 0.5725700259208679, 0.31031712889671326, -0.5486602187156677, -0.022368885576725006, 0.7333241701126099, -0.36657071113586426, -0.15929563343524933, 0.26968491077423096, -1.264675498008728, -0.421065092086792, -0.31404203176498413, 0.6641761064529419, 0.4064517319202423, 0.5845924615859985, 0.17186960577964783, -0.8683920502662659, -0.23423218727111816, -0.16591118276119232, -0.2226087898015976, 0.0011867359280586243, 0.19633620977401733, -0.29707470536231995, 0.28314146399497986, 0.6387293338775635, -0.5648603439331055, -0.8233973383903503, -0.7557735443115234, -0.205540731549263, 0.13679228723049164, 0.7551249861717224, 0.1494239717721939, -0.8830090761184692, 0.7856777906417847, 0.5301910042762756, 0.33556464314460754, 0.7843897342681885, -0.45825299620628357, 0.31434762477874756, 0.1482028067111969, 0.15119695663452148, -0.5767952799797058, -0.4363545775413513, 1.4969415664672852, 1.0609101057052612, -0.5912542939186096, 0.5510507822036743, -0.46539729833602905, -0.3227784037590027, 0.90642911195755, 0.8550548553466797, -0.1953021138906479, 0.8488317728042603, 0.7094561457633972, -0.34523236751556396, 0.16593816876411438, -1.0790480375289917, -0.17742176353931427, 0.4678972661495209, 0.5472216606140137, 0.6585637927055359, 0.4681587219238281, 0.24537190794944763, 0.8319846987724304, 0.6119663715362549, 0.2532643973827362, 0.3747522532939911, 0.7171119451522827, -0.08134622126817703, 0.25094008445739746, -0.2671818435192108, 0.409501314163208, -0.5488372445106506, -0.9456207156181335, 0.5483839511871338, 0.8273851275444031, 0.07014719396829605, 0.2765381336212158, 1.6104875802993774, -0.22259370982646942, 0.6109169721603394, -0.14159783720970154, 0.09443462640047073, -0.3841635584831238, -0.42365556955337524, -0.10605499893426895, -0.600628674030304, -0.367220014333725, 0.13107994198799133, -0.3239584267139435, -0.4267204999923706, -0.7042586207389832, 0.531785786151886, -0.3570117652416229, 0.4274429976940155, 0.38676828145980835, 1.349697232246399, 1.6703481674194336, -0.2162291556596756, -1.3681716918945312, -0.23432397842407227, -0.7042010426521301, 0.24718430638313293, -0.6931121349334717, -0.23916511237621307, -0.11074899882078171, -0.08971738070249557, -0.6347214579582214]}, "authors": [{"authorId": "27057088", "name": "Sheng-Chun Kao"}, {"authorId": "1929462", "name": "Suvinay Subramanian"}, {"authorId": "1839673849", "name": "Gaurav Agrawal"}, {"authorId": "145984583", "name": "T. Krishna"}], "references": [{"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "3aef285d724db47d1218c09a0cb7c25573828c4b", "title": "CoSA: Scheduling by Constrained Optimization for Spatial Accelerators"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "d051b4e5f7e400b6c3fdd05945e6aabf6b33cae1", "title": "Mind mappings: enabling efficient algorithm-accelerator mapping space search"}, {"paperId": "765ae4aee20af83b39c35cf81626bb85948552f1", "title": "An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks"}, {"paperId": "eb0931c39904a40c6cb4aa35c9b21d5e3b7dc856", "title": "Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "1020da59ab3db2b3051fb558559d7fdcd2c7e57b", "title": "TransTrack: Multiple-Object Tracking with Transformer"}, {"paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "title": "Taming Transformers for High-Resolution Image Synthesis"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "78211429633a99c5ca1b66ba0a7bc6def79c7f40", "title": "GAMMA: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "2b2056bf5763e32811a69769fa8c223160125f9e", "title": "DNNFusion: accelerating deep neural networks execution with advanced operator fusion"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "1cc730bb43da33f910a90ff1d9da9890f114baa7", "title": "Data Orchestration in Deep Learning Accelerators"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "3836ccb33191799e748e8e96f85a813eaf650ff8", "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "ab5f0004c5f3317689e8457e1c8d8390ccbee522", "title": "A domain-specific supercomputer for training deep neural networks"}, {"paperId": "97bac618fc866ae7656660f3965e9aae37993232", "title": "Efficient Processing of Deep Neural Networks"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "ce10bcf613c30c2184fd53d9ed2149ecb9cddfee", "title": "Centaur: A Chiplet-based, Hybrid Sparse-Dense Accelerator for Personalized Recommendations"}, {"paperId": "ecdddb58d2d98e2814b6b7a9cc5ded2b547236d6", "title": "A Multi-Neural Network Acceleration Architecture"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "fcf1b4473a0af1f3ebc0fd556ee30c9309ff6345", "title": "SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "069e0d896da7c79faeee4cf057548d5da7ce885e", "title": "FlauBERT: Unsupervised Language Model Pre-training for French"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "8051e77dd37ba276b10a53da5b130d6b755c68c3", "title": "Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "8efd76fafff43a1b420f8cda092a8bab8d545732", "title": "Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture"}, {"paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "title": "Structured Pruning of Large Language Models"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "540f074cb6f16563a357741837e41c44c0a38234", "title": "Reweighted Proximal Pruning for Large-Scale Language Representation"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "85ad7241d619ff924e61037cd3a769614641184e", "title": "DeepTools: Compiler and Execution Runtime Extensions for RaPiD AI Accelerator"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "b1051e81e527d841f0936c604aa6966c719e876d", "title": "TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators"}, {"paperId": "9ded246119861dd325e7004b2050bba310e08797", "title": "Timeloop: A Systematic Approach to DNN Accelerator Evaluation"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "cc14e2e99ef12b01ecb1e869b46b9eb50e2179bd", "title": "HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array"}, {"paperId": "b4e1264d08d976fe03ede65f34027107e9191c6e", "title": "TGPA: Tile-Grained Pipeline Architecture for Low Latency CNN Inference"}, {"paperId": "fb507ada871d1e8c29e376dbf7b7879689aa89f9", "title": "Music Transformer: Generating Music with Long-Term Structure"}, {"paperId": "117fd3a77f887f827e7f3521964b51eb788d33c5", "title": "Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "0682bfa5cca15726aab6c00ecfac91eb44379626", "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices"}, {"paperId": "cb91c2f8d3cac0b655a39be318b603334eb18987", "title": "Learning to Optimize Tensor Programs"}, {"paperId": "1f0bbcbcea15b60b39012e9aedf4dac42dff9411", "title": "Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach"}, {"paperId": "e52ee298889fb135101e5ad219692f03d5f01857", "title": "MAESTRO: An Open-source Infrastructure for Modeling Dataflows within Deep Learning Accelerators"}, {"paperId": "8c7310477fd027193cd040288f0aa9824c80b91f", "title": "Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code"}, {"paperId": "0ef460c47377c3b9482d8177cbcafad1730a91a5", "title": "Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling"}, {"paperId": "5f0da3cedda449b72fe36fa78798651a038f515c", "title": "MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "c2e1139691c3a337831e36ee7afeab8817ab5d48", "title": "The tensor algebra compiler"}, {"paperId": "dbef80d8c7784618159a2e281c6eacdb16716dca", "title": "SCALEDEEP: A scalable compute architecture for learning and evaluating deep networks"}, {"paperId": "b02b8096567addbe78ab3e39d8dd8f105d428eb0", "title": "Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "733a765e1f54eb86dbaabe03d7ba66d72363f665", "title": "TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory"}, {"paperId": "ce6403e99465e5e8a48d5c2017fc23976e29fe59", "title": "FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks"}, {"paperId": "72ed74f00d0f7312f7ed96d93ed43f0052d526bc", "title": "Fused-layer CNN accelerators"}, {"paperId": "ffdaa12ef011de9dbf43be46d45a3abcc8288965", "title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "654a3e53fb41d8168798ee0ee61dfab73739b1ed", "title": "Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks"}, {"paperId": "3468a27c2e3019e1216ee9fe8bbf1ed3a0155ff4", "title": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "4d23db55e6671a82c95dacec33b2967a4b8b677d", "title": "Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines"}, {"paperId": "e8294122747a16df4f566c39c2ed6d629bd8af74", "title": "Improving effective bandwidth through compiler enhancement of global cache reuse"}, {"paperId": "4f39adb2021bc54d7a15fc5a00056545b85a217d", "title": "Maximizing Loop Parallelism and Improving Data Locality via Loop Fusion and Distribution"}, {"paperId": "d766378b3fc29efa46931cc8411a0c21faa14d91", "title": "Vector Register Allocation"}, {"paperId": "623d38e327221c52579b906adc21b2a3a706b651", "title": "Collective Loop Fusion for Array Contraction"}, {"paperId": "c1605344b3ecc3243046662aa46a2251a1b0867f", "title": "Optimizing supercompilers for supercomputers"}, {"paperId": null, "title": "architecture tuning guide"}, {"paperId": null, "title": "Self-attention based context-aware 3d object detection"}, {"paperId": "ec2de959c0dfaa4e65fd5baed88bbc5b0e99d28f", "title": "Mind Mappings: Enabling Ef\ufb01cient Algorithm-Accelerator Mapping Space Search Extended"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Dmazerunner: Executing perfectly nested loops on data\ufb02ow accelerators"}, {"paperId": null, "title": "V100 gpu architecture"}, {"paperId": "ec3071fb918ad69ec80df1ca9cf1fdeb386a9603", "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"}, {"paperId": null, "title": "Scale-sim: Systolic cnn accelerator simulator"}, {"paperId": null, "title": "Online verf\u00fcgbar unter"}, {"paperId": null, "title": "): for n=[0, N): for k=[0, K): o[b,h,m,n]+= i"}, {"paperId": null, "title": "Tensorflow xla"}, {"paperId": null, "title": "Google Open-Sources Trillion-Parameter AI Language Model Switch Transformer"}]}