{"paperId": "6001dce1c8f63350263e013e0e6ff69816f0a9af", "title": "Text Classification via Large Language Models", "abstract": "Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification. This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning. In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for $k$NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generalization ability and the task-specific evidence provided by the full labeled dataset. Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-used text-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on AGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance comparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP delivers impressive abilities on low-resource and domain-adaptation setups. Specifically, using 16 examples per class, CARP achieves comparable performances to supervised models with 1,024 examples per class.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "citationCount": 50, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://arxiv.org/pdf/2305.08377", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "Clue And Reasoning Prompting (CARP) adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification, and achieves comparable performances to supervised models with 1,024 examples per class."}, "embedding": {"model": "specter_v2", "vector": [0.2889190912246704, 0.4955524206161499, -0.42017391324043274, -0.017499428242444992, -0.5284243226051331, -0.2985691726207733, 0.7765450477600098, 0.24433010816574097, -0.4485541582107544, -0.10405504703521729, 0.3044951856136322, -0.32482481002807617, 0.34880122542381287, 0.26068219542503357, 0.6158756613731384, 0.03392352908849716, -0.80290287733078, 0.6088730692863464, -0.11207903921604156, -0.6117952466011047, 0.21664340794086456, -0.8737943768501282, -0.3509831428527832, 0.43800652027130127, 0.5549576878547668, 0.5284782648086548, 0.12523043155670166, 0.7702892422676086, -0.7996850609779358, 0.6384148597717285, 0.003914054948836565, -0.5022543668746948, 0.0317440927028656, 0.1749882996082306, -0.6065059900283813, 0.14478804171085358, 0.22123004496097565, -0.10241905599832535, -0.19153696298599243, 0.30612078309059143, -0.04711448401212692, 0.2034459114074707, 0.6481173634529114, -0.4471672773361206, -0.7550042271614075, 1.1005799770355225, 0.4030073881149292, 0.6615490317344666, -0.15209563076496124, -0.4689134657382965, 1.7768443822860718, -1.4129559993743896, 0.22005248069763184, 1.129588007926941, 0.6047530770301819, 0.7514118552207947, -0.16309013962745667, -0.7406229972839355, 0.6823793649673462, 0.5012492537498474, -0.8665907382965088, -0.0719045028090477, 0.1425740122795105, 0.07270026206970215, 2.1547460556030273, -0.3741599917411804, -0.3426436483860016, 0.4537019431591034, -0.3580499291419983, 1.5182256698608398, -0.044692639261484146, -1.1626887321472168, -0.2961132526397705, 0.10238000005483627, 0.32217735052108765, 0.547123908996582, -0.6738372445106506, 0.033984266221523285, -1.0504772663116455, -0.14850328862667084, 0.39826756715774536, -0.20496366918087006, -0.09799784421920776, 0.40801772475242615, -0.6594574451446533, 0.6472644805908203, 0.4723915457725525, 0.9259881973266602, -0.29794085025787354, 0.4230799078941345, 0.4629589915275574, 0.4980263113975525, -0.12889140844345093, 0.7013664245605469, -0.5336709022521973, 0.31406471133232117, -0.5654614567756653, 0.3607570230960846, -0.11858615279197693, 0.6410064697265625, -0.40809428691864014, 0.1503327339887619, -0.9092196226119995, 0.7653040885925293, 0.9005095362663269, 0.02759982831776142, 0.6330068111419678, -0.6388768553733826, 0.611208975315094, -0.21672798693180084, 0.2974177896976471, -0.12239360809326172, -0.3729904890060425, -0.5147668719291687, -0.5949274301528931, -1.2327669858932495, -0.22220724821090698, -0.29912203550338745, -0.32565826177597046, 0.5762456059455872, -0.18767321109771729, -0.20924845337867737, 0.5180156230926514, 0.23113733530044556, 0.8177416324615479, 0.5225032567977905, 0.5422461032867432, 0.10181175917387009, 1.1461303234100342, -0.8474236130714417, -0.5440806746482849, -1.039259672164917, 1.1287585496902466, -0.16293829679489136, 0.3748810291290283, -0.005688668694347143, -0.9350687861442566, -0.9519177079200745, -0.5517387986183167, -0.13275447487831116, -0.6680969595909119, 0.5918437242507935, 0.8050131797790527, 0.15492598712444305, -0.9748395085334778, 0.5472432971000671, 0.34364140033721924, -0.3830946981906891, 0.3269512951374054, -0.1983727067708969, 0.6772506237030029, -0.6153915524482727, -1.5452767610549927, 0.4035963714122772, 0.33311522006988525, -0.43767282366752625, 0.12932273745536804, -0.47443848848342896, -1.0841485261917114, -0.1982330083847046, 0.6105222702026367, -0.47506141662597656, 1.6762609481811523, -0.006718245800584555, -1.0399740934371948, 0.7712398171424866, -0.3051629960536957, 0.23426556587219238, 0.3080691695213318, 0.0755453035235405, -0.7805818319320679, -0.3360298275947571, -0.06840728968381882, 0.9251242876052856, 0.29612424969673157, 0.016942378133535385, -0.6419990658760071, 0.338347852230072, -0.06643080711364746, 0.1488492637872696, -0.40979939699172974, 0.9541617035865784, 0.1220199465751648, -0.6181639432907104, 0.3145581781864166, 0.925481915473938, -0.06708414852619171, -0.018025251105427742, -0.24243873357772827, -1.6424407958984375, 0.8418653607368469, 0.24583254754543304, 0.8577643036842346, -0.8036103844642639, -0.9650827050209045, -0.708836555480957, -0.2524292469024658, 0.021759960800409317, -0.8054742217063904, 0.7866274118423462, -0.2726375162601471, 0.5344514846801758, -0.19270093739032745, -0.8388784527778625, 0.2950740158557892, -0.10907638072967529, -0.7412943243980408, -0.7644643187522888, 0.5298535227775574, 1.2076795101165771, -0.9375327825546265, 0.045004017651081085, -0.1820058673620224, -0.015002495609223843, -0.7577908635139465, 0.7488343119621277, -0.6480926871299744, -0.06236731633543968, -0.295841246843338, -0.036449868232011795, -0.07055109739303589, -0.39109688997268677, 0.26631587743759155, -0.263560026884079, -0.33396977186203003, 0.5164492726325989, -0.40427905321121216, 1.293513536453247, -0.5071840286254883, 0.5390203595161438, -0.412932813167572, -0.5711286067962646, 0.016057027503848076, 0.8103921413421631, -0.6100868582725525, -0.37049663066864014, 0.08910681307315826, 0.6022226810455322, -0.5300664305686951, 0.11761263757944107, 0.8505823016166687, 0.5406004190444946, -0.4519447386264801, 0.3252149522304535, 0.31760668754577637, -0.3457830250263214, 0.6997054815292358, 0.3520336449146271, 0.658649206161499, 0.40040674805641174, 0.5418805480003357, -0.14562514424324036, 0.4916413128376007, -0.18736107647418976, -0.18859852850437164, 0.47293180227279663, 0.7696136832237244, 0.523140549659729, 0.2910939157009125, -0.6522067785263062, 0.0051036071963608265, 0.4796655774116516, 0.29080429673194885, 1.8286278247833252, -0.04967266321182251, -0.4156189560890198, -0.42928677797317505, -0.5258511900901794, -0.3346133530139923, 0.4090387523174286, -0.46803316473960876, 0.1916695535182953, -0.3145000636577606, -1.0208288431167603, 0.8492581844329834, 0.6999025940895081, 1.194248080253601, -0.7886923551559448, -0.36351853609085083, -0.09172903001308441, -0.10483913123607635, -0.7961584329605103, -0.4304940104484558, 0.9890660047531128, -0.6061388850212097, -0.21183690428733826, 0.02929716557264328, -0.5106141567230225, 0.3024504780769348, -0.4537625014781952, 0.8369868993759155, -0.20216688513755798, -0.31010565161705017, -0.06772328168153763, 0.2571772336959839, -0.5102006793022156, -0.5973038673400879, 0.048689041286706924, -0.46451619267463684, -0.41971904039382935, 0.6083693504333496, 0.8155221939086914, 0.21905925869941711, 0.37365594506263733, -0.3422664999961853, 0.3641042709350586, 0.19251655042171478, 0.5856687426567078, 0.41707852482795715, 0.16891489923000336, 0.3750748634338379, -1.2969671487808228, 0.9368268251419067, 0.3187728822231293, -0.07066969573497772, 0.486877977848053, -0.4823315441608429, -0.6593374609947205, 0.7583898305892944, -0.687049388885498, -0.6601890325546265, -1.2457869052886963, 0.4810965359210968, 0.3089977502822876, -0.582111120223999, 0.5729654431343079, 0.3680097460746765, 0.32405418157577515, 0.838207483291626, 0.48452049493789673, -0.2547083795070648, -0.05913248658180237, 0.4662643074989319, -0.3832676410675049, 0.2025231122970581, 0.027676934376358986, 0.2910482883453369, -0.507984459400177, -0.16912491619586945, -0.6040055155754089, -0.7895998954772949, -0.5214085578918457, -0.5467240810394287, 0.07611199468374252, -0.051350921392440796, -0.4565046727657318, -0.5925244688987732, 0.07148497551679611, -1.1196743249893188, -0.7316709756851196, -0.14821121096611023, -0.621620237827301, 0.09991901367902756, -1.121934413909912, -1.287628173828125, -0.48092806339263916, -0.2783416509628296, -0.38095763325691223, 0.6849278807640076, -0.06283152103424072, -0.5281955003738403, -1.014041543006897, -0.2212596982717514, 0.07088290899991989, 0.8620036244392395, -0.5772828459739685, 1.238036036491394, -0.0279898289591074, 0.1847541183233261, -0.17877298593521118, 0.040770273655653, 0.07621218264102936, 0.07386526465415955, 0.021495211869478226, -0.8594313263893127, 0.5603470206260681, 0.09668450802564621, -0.4819890856742859, 0.09972239285707474, -0.10851477086544037, 0.6766089797019958, 0.21448734402656555, -0.46123120188713074, -0.21675200760364532, 1.2850133180618286, -0.38010337948799133, -0.4418054521083832, 0.1326489895582199, 0.9385805130004883, 0.5002356171607971, 0.13568998873233795, 0.44222164154052734, 0.3571726679801941, 0.2588972747325897, -0.18458609282970428, -0.2858959436416626, -0.14113378524780273, -0.27190643548965454, 0.3446892201900482, 1.1374552249908447, 0.08757141977548599, -0.1944979578256607, -1.385231375694275, 0.469526469707489, -1.145246148109436, -0.25213363766670227, 0.3287808895111084, 0.3408832252025604, 0.6564187407493591, -0.457820326089859, -0.6672285199165344, 0.27179524302482605, 0.382454514503479, -0.006724733393639326, -0.42116624116897583, -0.3607494533061981, -0.19838935136795044, 0.3743759095668793, -0.24226014316082, 0.3945218026638031, -0.3159554600715637, 0.6731976866722107, 14.662978172302246, 1.0393080711364746, -0.11610560864210129, 0.32152873277664185, 0.7404438853263855, 0.5548223853111267, -0.2403813898563385, -0.3208444118499756, -1.1186381578445435, -0.8249788880348206, 0.7276460528373718, 0.17440691590309143, 0.556250810623169, -0.18154416978359222, -0.019305765628814697, 0.11046084016561508, -0.7770164608955383, 0.7055320143699646, 0.4700535833835602, -0.9671604037284851, 0.7686730623245239, 0.17574365437030792, 0.32562923431396484, 0.19324375689029694, 0.8580791354179382, 1.0677423477172852, 0.37203243374824524, -0.42697980999946594, 0.5808316469192505, -0.6645780205726624, 0.9687002897262573, 0.07577355206012726, 0.6354426741600037, 0.6999672055244446, -0.9999998211860657, -0.6949355602264404, -0.5592453479766846, -1.2556555271148682, -0.1151588037610054, -0.19222621619701385, -1.0452988147735596, -0.22935158014297485, -0.5417512059211731, 0.8378229737281799, -0.10396198183298111, 0.12272107601165771, -0.8524722456932068, 0.6775614023208618, 0.4639755189418793, -0.3647959232330322, 0.4115851819515228, 0.9583247900009155, 0.40475302934646606, 0.1411035656929016, 0.0757201686501503, 0.22024346888065338, 0.14776144921779633, 0.4411245882511139, -0.502042293548584, 0.26057684421539307, -0.14406298100948334, -0.061573632061481476, -0.09469597786664963, 1.0531665086746216, 0.2660011351108551, -0.020922215655446053, -0.44462016224861145, 0.11634550988674164, 0.6784574389457703, 0.20274701714515686, -0.10585007071495056, -0.025273092091083527, -0.08161795139312744, -0.4769028127193451, 0.1052788719534874, 0.7309702038764954, -0.3153045177459717, -0.7713992595672607, -0.7759798765182495, -0.19912724196910858, 0.7357286810874939, -1.0103806257247925, -1.2036981582641602, 1.0705809593200684, -0.39109012484550476, -0.7095730304718018, 0.14617229998111725, -1.1580506563186646, -0.4703843891620636, 0.4319940209388733, -1.9608426094055176, -0.7829906940460205, 0.05934242904186249, -0.11071902513504028, 0.14995761215686798, 0.18802285194396973, 1.632012963294983, -0.19903460144996643, -0.35354459285736084, -0.01814950443804264, -0.09358702600002289, 0.11638054996728897, -0.008672889322042465, -1.3758599758148193, 0.6368906497955322, 0.029160622507333755, 0.07235725969076157, 0.11306042969226837, -0.2316809892654419, 0.30582407116889954, -0.3881325423717499, -0.27286893129348755, 1.0391427278518677, -0.8839415907859802, -0.1600853055715561, -0.9751811623573303, -1.0225385427474976, -0.05632401630282402, 0.6368839740753174, -0.27615222334861755, 0.539306104183197, 0.6692288517951965, -0.6229326725006104, 0.08978740870952606, -1.0464178323745728, 0.6445870995521545, 0.599759578704834, -0.6198204755783081, -0.7388274073600769, 0.24279442429542542, 0.12796904146671295, -0.773830235004425, -0.4874880313873291, -0.38481923937797546, -0.33564504981040955, 0.4027489125728607, 0.5922224521636963, -0.10417871922254562, 0.6908753514289856, 0.7436001896858215, -0.27197331190109253, -0.9536303281784058, -0.12796254456043243, -0.9269418716430664, 0.32388976216316223, 0.30401620268821716, 1.1726007461547852, -0.6415398120880127, 0.02586217038333416, 0.8790671825408936, 0.21399220824241638, -0.5387274622917175, -0.40644028782844543, -0.017261261120438576, 0.26913002133369446, -0.5497071146965027, 0.40875208377838135, 0.4359451234340668, 0.6698967218399048, 0.5327165126800537, 0.953644335269928, 0.26858165860176086, -0.42344871163368225, -0.2878005802631378, 0.15687520802021027, -0.44957923889160156, 0.044989537447690964, -0.7254180908203125, -0.12203267216682434, -1.603912353515625, -0.022818483412265778, -1.2282323837280273, 0.1877412348985672, -0.8695712089538574, -0.413486123085022, -0.0670265480875969, -0.368640661239624, 0.6480729579925537, 0.03474071994423866, -0.29972854256629944, -0.7164942026138306, -0.648364782333374, -0.49495401978492737, 0.5323437452316284, 1.0365391969680786, -0.7215492129325867, 0.17463542520999908, -0.32447734475135803, -0.35365718603134155, 0.19414831697940826, 0.3803214430809021, -0.37825536727905273, -0.7098739743232727, -1.3920927047729492, 0.2580932378768921, 0.04226762428879738, -0.08449321240186691, -0.529082179069519, 0.6601297855377197, 0.3555631935596466, -0.21421462297439575, 0.12958118319511414, 0.15684819221496582, -0.7039662599563599, -1.0028204917907715, 0.09119461476802826, -1.2930233478546143, 0.014362144283950329, 0.37525248527526855, -0.7376096844673157, -0.4072520136833191, 0.2203868180513382, -0.3328392803668976, -1.0382826328277588, -0.8741949200630188, 0.11626595258712769, -0.6909366846084595, 0.2677948772907257, -0.4810892641544342, -0.19314588606357574, -1.4450302124023438, 0.13195832073688507, 0.20262028276920319, 0.7410069704055786, -0.4958426058292389, 0.9409129023551941, 0.09641613066196442, -1.0537680387496948, -0.22128862142562866, 0.09618153423070908, 0.26642224192619324, 0.2817821204662323, 0.45242470502853394, 0.33941033482551575, -0.02678196132183075, 0.5107462406158447, 0.3815106451511383, 0.08480588346719742, -0.9813134074211121, -0.18885314464569092, 0.836823582649231, -0.6076480150222778, -0.4103316068649292, 1.079651951789856, -0.10564357042312622, -1.100170373916626, 0.23021531105041504, -1.4485373497009277, -0.9597082734107971, -0.48405322432518005, 0.7203078866004944, -0.1276368349790573, 0.12832364439964294, 0.15397968888282776, -0.24698926508426666, 0.351816326379776, -0.1377938687801361, -0.508677065372467, 0.4795519709587097, -0.1969684511423111, -0.13166776299476624, 0.8110946416854858, 0.3020647168159485, -0.7191823124885559, -0.6625072956085205, -0.7578432559967041, -0.05808906629681587, 0.11255393922328949, 0.2731296420097351, -1.0869500637054443, -0.1798916459083557, 0.9635393023490906, -0.03222592920064926, 0.727432131767273, 0.16447241604328156, -0.33585691452026367, 0.17806114256381989, 1.040122151374817, 0.06617818772792816, -0.7088934779167175, -0.3833172023296356, 1.4002606868743896, 1.2914677858352661, -1.1332553625106812, -0.21566450595855713, -0.30763769149780273, -0.9352700114250183, 1.034569263458252, 0.4455125629901886, 0.3924979567527771, 0.5737131834030151, -0.4763517677783966, 0.562959611415863, -0.15137070417404175, -1.5102310180664062, 0.1003936231136322, 0.673986554145813, 1.1886152029037476, 0.9723365902900696, 0.3031553030014038, 0.3126567304134369, 0.9996588230133057, -0.05530526489019394, 0.10527163743972778, 0.5002599954605103, 0.5485821962356567, -0.0741545781493187, -0.1733812689781189, 0.13795749843120575, 0.442006379365921, -0.5860669016838074, -0.8139857649803162, -0.1136268898844719, 0.5996237993240356, 0.3833634853363037, 0.9368301033973694, 0.40995994210243225, 0.4685717225074768, 0.5021998882293701, 0.4386303722858429, 0.4863678216934204, -0.9130967259407043, -0.5899688005447388, -0.2955274283885956, -0.4802792966365814, 0.4268152117729187, -0.3375560939311981, -0.501576840877533, -0.4227658808231354, 0.25822609663009644, 0.014899833127856255, 0.3008890151977539, 0.13768509030342102, 1.1940914392471313, 0.2787577509880066, 0.5195221304893494, -0.32190561294555664, -0.2098763883113861, -0.5439070463180542, -1.1493762731552124, -0.15782584249973297, -0.6143379211425781, -0.300476610660553, -0.33455827832221985, -0.09767340868711472, -0.24069096148014069]}, "authors": [{"authorId": "2109329406", "name": "Xiaofei Sun"}, {"authorId": "2845020", "name": "Xiaoya Li"}, {"authorId": "2172372802", "name": "Jiwei Li"}, {"authorId": "144894837", "name": "Fei Wu"}, {"authorId": "16042895", "name": "Shangwei Guo"}, {"authorId": "2146331573", "name": "Tianwei Zhang"}, {"authorId": "2107926840", "name": "Guoyin Wang"}], "references": [{"paperId": "f2cd02c03d0169374442d9bc227c9aed178f4b20", "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models"}, {"paperId": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4", "title": "GPT-NER: Named Entity Recognition via Large Language Models"}, {"paperId": "f208ea909fa7f54fea82def9a92fd81dfc758c39", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"}, {"paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "title": "Automatic Chain of Thought Prompting in Large Language Models"}, {"paperId": "9d83461d6acbeac94e41b88ef57781a00bc7ae39", "title": "Ranking-Enhanced Unsupervised Sentence Representation Learning"}, {"paperId": "86d0d3855f94105e25d81cab9f3d269c6062a9c4", "title": "Selective Annotation Makes Language Models Better Few-Shot Learners"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "563a851106623b9f112d0e2a290d3950a871079c", "title": "Nearest Neighbor Zero-Shot Inference"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "01a664b2dbb799259bf4a39bf3cabb1b4f252673", "title": "Simplified-Boosting Ensemble Convolutional Network for Text Classification"}, {"paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be", "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "341bdbcfc3febef7691a97c216ad394653211095", "title": "Can language models learn from explanations in context?"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "4081aeb7ff148cc4678efca4e44a72dece4542e3", "title": "Reframing Human-AI Collaboration for Generating Free-Text Explanations"}, {"paperId": "f9838a3be5c94bb2674a0e224de349b50e18f3c4", "title": "Learning To Retrieve Prompts for In-Context Learning"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "9a258f42e333ed5ff79037724eb01747ede0bb49", "title": "Few-Shot Self-Rationalization with Natural Language Prompts"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "654ea3c14dc3dd9dc2c1ff19bf4f9e71dda78d10", "title": "ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information"}, {"paperId": "da454295392cf4caaa39cc465734237ffe55392f", "title": "PTR: Prompt Tuning with Rules for Text Classification"}, {"paperId": "0e3176c920d958925819919fba3d9b587a9c02ac", "title": "Sentence Similarity Based on Contexts"}, {"paperId": "d22b109eb5089179f8bd48ef47513533890f6bf9", "title": "BertGCN: Transductive Text Classification by Combining GNN and BERT"}, {"paperId": "c26759e6c701201af2f62f7ee4eb68742b5bf085", "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"}, {"paperId": "a07a94168608322600fd3cab54df1410b96852b6", "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases"}, {"paperId": "209f9bde2dee7cf1677801586562ffe56d435d38", "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"}, {"paperId": "f2885c6a25756cf81aa23b41bc62696a5be5c94d", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall"}, {"paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74", "title": "What Makes Good In-Context Examples for GPT-3?"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "20d51f8e449b59c7e140f7a7eec9ab4d4d6f80ea", "title": "Nearest Neighbor Machine Translation"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ae2c03cbe6162dadf65edd2ff7dfc5333524dca5", "title": "MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification"}, {"paperId": "0fe2636446cd686830da3d971b31a004d6094b3c", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"}, {"paperId": "6b5359de6572217ea71a1310abffd419f8a2d165", "title": "Description Based Text Classification with Reinforcement Learning"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "75acc731bdd2b626edc74672a30da3bc51010ae8", "title": "CTRL: A Conditional Transformer Language Model for Controllable Generation"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "80f9f109d1564cb8f82aa440a5f6f3fbe220c9ef", "title": "ERNIE 2.0: A Continual Pre-training Framework for Language Understanding"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "a022bda79947d1f656a1164003c1b3ae9a843df9", "title": "How to Fine-Tune BERT for Text Classification?"}, {"paperId": "0feea94f89d395436bf41bd10c797447eecbc128", "title": "Unsupervised Data Augmentation for Consistency Training"}, {"paperId": "603a448c7c17967e90d11bb6774f1e81e54e0789", "title": "Vector of Locally-Aggregated Word Embeddings (VLAWE): A Novel Document-level Representation"}, {"paperId": "162cad5df347bdac469331df540440b320b5aa21", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "6017e81c5ede6c38b306a3df9738aeb04baa7619", "title": "Graph Convolutional Networks for Text Classification"}, {"paperId": "4cfd53606ee76aedc2cf1ae39a64223adc25e4c8", "title": "Joint Embedding of Words and Labels for Text Classification"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "892e53fe5cd39f037cb2a961499f42f3002595dd", "title": "Bag of Tricks for Efficient Text Classification"}, {"paperId": "455afd748e8834ef521e4b67c7c056d3c33429e2", "title": "Hierarchical Attention Networks for Document Classification"}, {"paperId": "f797fd44b9ddd5845611eb7a705ca9464a8819d1", "title": "Very Deep Convolutional Networks for Text Classification"}, {"paperId": "e28ab7c3b994dd4e30baac1eb67c7f87e40c2b7b", "title": "Recurrent Neural Network for Text Classification with Multi-Task Learning"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "title": "A large annotated corpus for learning natural language inference"}, {"paperId": "97e3bb4af723f43927317e9b9f2d794a9e398e8e", "title": "PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks"}, {"paperId": "eba36ac75bf22edf9a1bfd33244d459c75b98305", "title": "Recurrent Convolutional Neural Networks for Text Classification"}, {"paperId": "60dda7f5efd67758bde1ee7f45e6d3ef86445495", "title": "Deep Recursive Neural Networks for Compositionality in Language"}, {"paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "6af58c061f2e4f130c3b795c21ff0c7e3903278f", "title": "Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "Electra: Pre-training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "The original data in SST-2 are sampled from snippets of Rotten Tomatoes HTML files. We use the same train/dev/test splits with Socher"}, {"paperId": null, "title": "hyper-parameters are tuned on the validation set, where learning rate { 2e-5, 3e-5, 4e-5 } , batch size { 16 , 32 , 32 } , a dropout rate of 0.3, a weight decay of 0.01, a warmup proportion of 0.01"}, {"paperId": null, "title": "NVIDIA 3090 GPUs with FP16. Model hyper-parameters are tuned on the validation set, where learning rate { 2e-5, 3e-5, 4e-5 } , batch size"}, {"paperId": null, "title": "The AG News consists of news articles from the AG's corpus. The dataset contains 30,000 training and 1,900 testing examples for each class"}, {"paperId": null, "title": "R8 topic 8 news 4,941 544 875 R52 topic 52 news 5,905 627 1,027 MR sentiment"}, {"paperId": null, "title": "Dataset Task # Label Source # Train # Dev # SST-2 sentiment 2 review 6,920 872 1 AGNews topic 4 news 96,000 24,000 7 R8 topic 8 news 4,941 544 2 R52 topic 52 news 5,905 627 2 MR"}, {"paperId": null, "title": "Hyper-parameters B.1 Fine-tuning Hyper-parameters We fine-tune"}]}