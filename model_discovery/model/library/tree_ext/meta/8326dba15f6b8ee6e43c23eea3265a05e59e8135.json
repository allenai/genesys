{"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training", "abstract": "Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called\"reverse sparsification,\"Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.", "venue": "International Conference on Machine Learning", "year": 2022, "citationCount": 59, "influentialCitationCount": 6, "openAccessPdf": {"url": "http://arxiv.org/pdf/2204.00595", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution and can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications."}, "embedding": {"model": "specter_v2", "vector": [0.2871862053871155, 0.6477941870689392, -0.1354980170726776, 0.1134621724486351, -0.19082674384117126, 0.35073381662368774, 0.2286376804113388, -0.5148500204086304, -0.4377569556236267, -0.3529498875141144, 0.6384281516075134, 0.1264771968126297, 0.38879838585853577, 0.1332385092973709, -0.41952478885650635, -0.15145812928676605, -1.2012994289398193, 0.16290900111198425, 0.2543136775493622, -0.2477414757013321, -0.3370945453643799, -0.2971118986606598, -1.0925073623657227, 0.32807424664497375, 0.05880818888545036, 1.0382846593856812, 0.17307360470294952, 0.7303791046142578, -0.337920218706131, 0.6810922026634216, 0.7653644680976868, -0.2505524158477783, 0.5600966215133667, 0.0640789121389389, -0.16364265978336334, -0.07417356222867966, 0.3236292600631714, -0.33604899048805237, -0.5817917585372925, 0.7159472703933716, -0.09152080118656158, 0.4186473786830902, 0.5108051896095276, -0.37841150164604187, 0.052077822387218475, 0.3249339461326599, 0.37302857637405396, 1.0401854515075684, -0.6822453737258911, -0.4463314414024353, 1.0423113107681274, -1.564277172088623, 0.32929399609565735, 1.21329927444458, 1.0907336473464966, 0.3325037360191345, -0.4646676182746887, -0.565214991569519, 0.36167532205581665, 0.01079545821994543, -0.7060432434082031, -0.5502171516418457, -0.031789857894182205, -0.4453715980052948, 1.6353193521499634, -0.16292698681354523, 0.1328037828207016, 0.6360936164855957, 0.15715408325195312, 0.8851327896118164, 0.15644732117652893, -0.3893928825855255, -0.19522197544574738, -0.02367333136498928, 0.15739752352237701, 0.9051598906517029, -0.402435302734375, 0.5933305025100708, -1.0849610567092896, -0.019956592470407486, 0.4714784026145935, 0.06892947852611542, 0.17434343695640564, -0.5281974077224731, -0.2112104445695877, 0.8089959025382996, 0.4868198037147522, 0.6431481838226318, -0.3287496566772461, 0.9453629851341248, 0.7367944121360779, 0.5566386580467224, 0.07396387308835983, 0.19242295622825623, 0.1793069988489151, 0.2736440598964691, -1.0864335298538208, -0.22112508118152618, 0.2598208487033844, 0.5534837245941162, -0.17464077472686768, 0.811062753200531, -0.3683905303478241, 0.42861059308052063, 1.1481508016586304, -0.021956289187073708, 0.49243345856666565, -0.46941855549812317, 0.15867507457733154, -0.8863148093223572, -0.3430432379245758, -1.026840329170227, -0.10435992479324341, -0.9181678891181946, -1.2962850332260132, -0.9966732263565063, -0.7767741084098816, 0.12482526898384094, -0.6596671342849731, 0.35659846663475037, -0.19739603996276855, 0.7211958169937134, 0.032971594482660294, 0.801952064037323, 0.7840669751167297, 0.9384096264839172, -0.028401972725987434, 0.5687329769134521, 1.0004950761795044, -0.8918927907943726, -0.4307243525981903, -1.0620938539505005, 0.3599448502063751, -0.1549370437860489, -0.050662823021411896, -0.3455595076084137, -1.3920890092849731, -0.8849640488624573, -0.6605753302574158, 0.25269678235054016, -0.45691144466400146, 0.25055602192878723, 1.202654480934143, 0.43382367491722107, -0.7419751882553101, 0.7228456139564514, -0.6801010966300964, -0.04479695111513138, 0.6242199540138245, 0.5775322318077087, 0.22972990572452545, -0.07950890064239502, -1.0010615587234497, 0.3914110064506531, 0.13275521993637085, -0.33984971046447754, -0.497245192527771, -0.7219838500022888, -0.922663688659668, 0.2519291937351227, 0.006495525129139423, -0.7150139808654785, 0.6643063426017761, 0.03050585463643074, -1.0374648571014404, 0.7950639724731445, -0.0055364272557199, -0.3577185273170471, 0.2831893265247345, 0.11415152996778488, -0.5374811887741089, 0.09684496372938156, -0.4823085367679596, 0.5565730333328247, 0.812265932559967, -0.21970514953136444, 0.3467562198638916, 0.4677349328994751, -0.5550520420074463, -0.25614991784095764, -0.4348021447658539, 0.8864061236381531, -0.3927616477012634, -0.5416678786277771, 0.2556205689907074, 0.6812964081764221, -0.31029069423675537, -0.001911301165819168, -0.1634567677974701, -0.12201640009880066, 0.546642005443573, 0.08636855334043503, 0.6108375191688538, -1.2086317539215088, -0.634516179561615, 0.33033397793769836, 0.3576516807079315, -0.35861584544181824, -0.7805294394493103, 0.4827300012111664, -0.6268524527549744, -0.02816111221909523, 0.04145319387316704, -0.8741862177848816, 0.004872114863246679, 0.10329470038414001, -0.7390004992485046, 0.15431423485279083, 0.2936173975467682, 0.9812135696411133, -0.773078441619873, -0.20676934719085693, -0.11184823513031006, 0.5929043889045715, -1.4454599618911743, 0.8560526371002197, 0.1294068545103073, -0.05808459222316742, 0.04000575467944145, -0.2739838659763336, -0.10290899127721786, -0.7179877161979675, 0.5479035973548889, -0.8434541821479797, 0.23673509061336517, 0.30993303656578064, -0.8509664535522461, 1.024477481842041, -0.44634878635406494, 0.629230260848999, 0.41618022322654724, -1.077994704246521, 0.23828125, 0.3805338442325592, 0.2213682383298874, -0.25395670533180237, 0.5870825052261353, 0.42998191714286804, -0.5144309997558594, 0.3440398573875427, 0.446591854095459, 0.5302704572677612, 0.20328356325626373, -0.0007544403779320419, 0.7370113134384155, -0.27414920926094055, 0.6205008029937744, 0.34853073954582214, 0.7094103693962097, 0.1110285148024559, 0.05554940924048424, -0.2598142623901367, -0.03455549478530884, -1.0780519247055054, 0.020524902269244194, 0.5565319061279297, 0.8899840116500854, 0.9119691848754883, 0.2720733880996704, -0.6103412508964539, -0.5938567519187927, -0.09007817506790161, 0.524429202079773, 1.2513781785964966, -0.26789626479148865, 0.004183921031653881, -0.49943384528160095, 0.1178302988409996, -0.43356096744537354, -0.6425355076789856, -0.11324360966682434, -0.2746175527572632, -0.5476588010787964, -1.3548846244812012, 0.5165025591850281, 0.23104065656661987, 0.8846212029457092, -0.28919732570648193, -0.4568359851837158, -0.40042683482170105, 0.4987393319606781, -0.9550713300704956, -0.6392801403999329, 0.5307935476303101, -0.995627224445343, -0.035801175981760025, -0.08742016553878784, -0.2863737642765045, 0.3254167437553406, -0.6245937943458557, 1.0229054689407349, -0.45860958099365234, -0.24243761599063873, 0.10536924004554749, 0.6777625679969788, -0.266964852809906, -0.4299618601799011, 0.3650835454463959, 0.15614300966262817, 0.2954830527305603, -0.32042595744132996, -0.45305484533309937, -0.0610651969909668, 0.09324965626001358, -0.4310070276260376, 0.3517036736011505, 0.3781989514827728, 0.2760719358921051, 0.8542790412902832, -0.4003670811653137, -0.09604983776807785, -1.200013279914856, 0.5320120453834534, -0.14798210561275482, -0.3307698369026184, -0.16030460596084595, -0.2568760812282562, -0.05856838449835777, 0.45985326170921326, -0.8706992864608765, 0.1827789694070816, -0.6482211947441101, -0.13411621749401093, -0.7926107048988342, 0.15044176578521729, 0.2914527356624603, 0.7104474306106567, -0.5598304271697998, 0.5248176455497742, 0.13013936579227448, 0.36120280623435974, -0.16465254127979279, 0.30917295813560486, -1.2504719495773315, 0.7460019588470459, -0.10884374380111694, 0.8115626573562622, 0.321992963552475, 0.08331526815891266, -0.6247913837432861, -0.7292793989181519, -0.5040614008903503, -0.09492935985326767, -0.4475102126598358, -0.052216026932001114, -0.8123234510421753, -0.7805370688438416, -0.4961744546890259, -0.5190194249153137, -0.35373640060424805, -0.2823505401611328, 0.0518169067800045, -0.19283922016620636, -1.0629030466079712, -1.4714447259902954, -0.47824713587760925, -0.9371086955070496, -1.1757699251174927, 0.4116935431957245, -0.17136381566524506, -0.23981483280658722, -0.6968295574188232, -0.4781142473220825, -0.6257023811340332, 1.0574978590011597, -0.293702632188797, 0.5731122493743896, 0.154096320271492, -0.26927679777145386, -0.009976050816476345, -0.21926653385162354, 0.574791669845581, -0.9535447955131531, -0.0773676186800003, -0.8021005392074585, 0.2177830934524536, -0.3152134120464325, -0.41814616322517395, 0.19583018124103546, 0.32167667150497437, 0.6885805726051331, -0.076078861951828, -0.224020853638649, 1.1338565349578857, 1.2295207977294922, -0.8488166928291321, 0.1705152541399002, -0.14227476716041565, 0.9008244276046753, -0.16715937852859497, -0.8236703276634216, 0.6491335034370422, -0.20879533886909485, 0.10811729729175568, 0.23665253818035126, -0.26454684138298035, -0.4843932092189789, -0.41481074690818787, 0.36767035722732544, 2.1108009815216064, 0.41659852862358093, 0.5014767646789551, -0.8081191778182983, 0.5692830681800842, -0.8928751349449158, -0.7718353271484375, 0.7287513613700867, 0.24063190817832947, 0.3798917829990387, -0.18207454681396484, -0.5565727353096008, -0.2961237132549286, 0.3139016330242157, 0.6826744675636292, -0.2882358133792877, -0.31654882431030273, -0.14223620295524597, 0.835007905960083, 0.5080808401107788, 0.3594204783439636, -0.17432746291160583, 0.06440707296133041, 14.967575073242188, 1.0534582138061523, -0.27882423996925354, 0.9648566842079163, 0.6645056009292603, -0.01707138493657112, -0.3199595808982849, -0.10937973856925964, -0.8936594724655151, -0.14364513754844666, 1.1320794820785522, 0.6195564866065979, 0.736017644405365, 0.3022591471672058, -0.23829030990600586, 0.49121811985969543, -0.47871726751327515, 1.2590428590774536, 0.3614169657230377, -1.8808873891830444, 0.04460613429546356, 0.0993095338344574, 0.7397118210792542, 0.7561467289924622, 0.8360671401023865, 0.6725582480430603, 0.2720043361186981, -0.4292760491371155, 0.6919665932655334, 0.44132882356643677, 1.218679666519165, 0.21259769797325134, 0.27831578254699707, 0.33923423290252686, -1.114198923110962, -0.008089656941592693, -0.5755096673965454, -0.9538241624832153, -0.01707843318581581, 0.4970518946647644, -0.2521173357963562, -0.5495147109031677, 0.1042945384979248, 1.1090584993362427, 0.3454931676387787, 0.5983093976974487, 0.34175267815589905, 0.6138439774513245, -0.41459619998931885, 0.3408384621143341, 0.3936504125595093, 0.1643562614917755, 0.21098199486732483, 0.02598819136619568, 0.1337909996509552, -0.01988919824361801, 0.39750564098358154, 0.6465873718261719, -0.6306107640266418, -0.2600286304950714, -0.16771413385868073, -0.21366991102695465, -0.407480388879776, 0.9633128643035889, 0.1926306039094925, 0.3056618273258209, -0.46887409687042236, 0.12153850495815277, 0.4839421808719635, 0.294801265001297, -0.06393390148878098, -0.05344482138752937, 0.2794819176197052, -0.4243537485599518, 0.04429016634821892, 0.34327998757362366, -0.38934192061424255, -0.7702326774597168, -0.7052310705184937, -0.4450707733631134, 0.3675355613231659, -0.931106448173523, -0.8747811317443848, 0.7553077936172485, -0.3843696713447571, -0.4100421667098999, 0.46969103813171387, -0.8249512314796448, -0.040923118591308594, 0.33715564012527466, -1.2062076330184937, -0.20489300787448883, 0.33769848942756653, -0.2926134169101715, -0.6138660907745361, -0.3096431493759155, 0.9892808198928833, 0.5517456531524658, -0.20549403131008148, -0.046417560428380966, 0.2539446949958801, 0.1753969043493271, -0.5961490869522095, -0.20894138514995575, 0.7886260747909546, 0.438560426235199, 0.06095816567540169, 0.34828779101371765, 0.04471611976623535, 0.4302324652671814, -0.8295356035232544, -0.16004711389541626, 0.2060154676437378, -0.42783603072166443, 0.392221063375473, -0.9302947521209717, -0.7470151782035828, 0.30826592445373535, 0.006782073527574539, 0.4183933436870575, 0.27017834782600403, 0.04311979562044144, -0.6225579380989075, -0.45734792947769165, -0.5986872911453247, 0.01797163300216198, 0.5659832954406738, -1.053692102432251, -0.3989243805408478, 0.035844579339027405, 0.0842655673623085, -1.247141718864441, -0.9116438627243042, -0.07068326324224472, 0.22646856307983398, -0.31279775500297546, 1.463484287261963, -0.1959376484155655, 0.8258570432662964, 1.1174918413162231, -0.1562960147857666, -0.6911898851394653, 0.3702056407928467, -1.0468491315841675, -0.5437048673629761, -0.4393002390861511, 0.21811726689338684, -0.42820921540260315, 0.7413556575775146, 0.20842553675174713, -0.06479871273040771, -0.6933788061141968, -0.670059859752655, -0.2901655435562134, -0.6073283553123474, -0.5382786989212036, 0.03965476155281067, -0.1088540181517601, -0.48526301980018616, 0.05622466281056404, 0.3538060486316681, 0.4641701281070709, -0.21379578113555908, -0.4938671290874481, 0.028745422139763832, -0.054090384393930435, -0.4364379942417145, -0.4866451025009155, -0.7850854992866516, -1.416572093963623, 0.031137743964791298, -1.5780904293060303, -0.387795090675354, -0.6773641705513, -0.3886905312538147, -0.010207828134298325, -0.2721422612667084, 0.026655998080968857, 0.7140344381332397, 0.30295634269714355, -0.4144552946090698, -0.4289029538631439, -0.4399395287036896, 0.9985886216163635, 1.0053279399871826, -0.8372496962547302, 0.22540372610092163, -0.12597967684268951, 0.273349404335022, 0.5447316765785217, 0.2618342638015747, -0.5680322647094727, -0.7530790567398071, -1.3825660943984985, 0.4100579023361206, 0.20722906291484833, 0.327431857585907, -1.347131371498108, 0.7144823670387268, 0.49591490626335144, -0.23648546636104584, 0.14499512314796448, 0.69844651222229, -1.241783857345581, -0.03937194123864174, 0.5734351873397827, -0.5909885168075562, 0.10070713609457016, 0.17550286650657654, -0.6479523181915283, -0.39515891671180725, 0.46482256054878235, 0.37087810039520264, -0.7874767780303955, -0.5747220516204834, 0.6251074075698853, -0.4097895920276642, 0.40496358275413513, -0.6238780617713928, -0.4606858193874359, -0.9059193730354309, -0.2744928002357483, -0.5086997151374817, 0.17100675404071808, -0.8815100789070129, 0.31772658228874207, 0.5486178994178772, -1.192643165588379, 0.3834552764892578, 0.3847987949848175, -0.3491031527519226, -0.24225950241088867, 0.7351296544075012, 0.6371278166770935, -0.5878745317459106, 0.1780451387166977, -0.013476844877004623, 0.37137895822525024, -0.5205190181732178, -0.20898360013961792, 1.1470625400543213, -0.5840640068054199, -0.6303340792655945, 1.23601496219635, -0.14227652549743652, -0.9746245741844177, 0.34657201170921326, -1.071983814239502, -0.2760300636291504, -0.45808592438697815, 0.32934144139289856, 0.06986754387617111, 0.1891242265701294, -0.06856521219015121, -0.4058767855167389, 0.18375472724437714, 0.2699570953845978, -0.3050745129585266, 0.8914539217948914, 0.10188943892717361, -0.40309202671051025, 0.5133822560310364, 1.1173462867736816, -0.6913177371025085, -0.8902662992477417, -0.8141872882843018, -0.4286153018474579, -0.14378716051578522, 0.43975940346717834, 0.07287124544382095, -1.4162840843200684, 0.9650457501411438, 0.751598596572876, -0.019436065107584, 0.3158124089241028, -0.38632023334503174, 0.4376961588859558, 0.8525708913803101, -0.05756267532706261, -0.68571937084198, -0.3050033450126648, 1.3254799842834473, 0.9731130599975586, -0.34054288268089294, 0.40615329146385193, -0.47901782393455505, -0.47674837708473206, 0.44518619775772095, 0.048984676599502563, -0.3540329337120056, 1.1025493144989014, 0.17321112751960754, -0.4202890694141388, -0.16547568142414093, -0.8211960196495056, -0.3030306398868561, 1.0732989311218262, 0.9489982724189758, 0.41684186458587646, -0.14054091274738312, 0.5152800679206848, 0.6859930157661438, -0.09921404719352722, -0.18938854336738586, 0.13075721263885498, 0.27142202854156494, 0.006429050117731094, 0.40621671080589294, -0.27071744203567505, 1.1894252300262451, -0.8631591200828552, -0.6401022672653198, 0.7117321491241455, 0.48757171630859375, 0.13695234060287476, 0.2405211329460144, 0.8394425511360168, 0.16656656563282013, 0.5969859957695007, -0.1368986964225769, 0.42469343543052673, -0.5744650959968567, -0.5035830140113831, 0.09415680915117264, -0.5324151515960693, -0.26409998536109924, -0.10308095812797546, -0.28338247537612915, -0.3494825065135956, -0.5093358755111694, 0.6437932252883911, 0.07789059728384018, 0.2432728409767151, 0.799264132976532, 0.5109372735023499, 0.9229543209075928, -0.46530377864837646, -0.7514474987983704, -0.38807210326194763, -0.48416030406951904, -0.06318872421979904, -0.2646891176700592, -0.1372089684009552, -0.282341331243515, 0.05177614092826843, -0.06958238780498505]}, "authors": [{"authorId": "24593911", "name": "Tri Dao"}, {"authorId": "4319427", "name": "Beidi Chen"}, {"authorId": "145193121", "name": "N. Sohoni"}, {"authorId": "150898486", "name": "Arjun D Desai"}, {"authorId": "40585370", "name": "Michael Poli"}, {"authorId": "2161243147", "name": "Jessica Grogan"}, {"authorId": "2161308739", "name": "Alexander Liu"}, {"authorId": null, "name": "Aniruddh Rao"}, {"authorId": "1755572", "name": "A. Rudra"}, {"authorId": "1803218", "name": "Christopher R\u00e9"}], "references": [{"paperId": "fbaa944e73644ce12ea4a0ac8ffb64c3280f3aff", "title": "Deformable Butterfly: A Highly Structured and Sparse Linear Transform"}, {"paperId": "54d8418a03a908aa40c40f4b0615a813204fc5fd", "title": "SKM-TEA: A Dataset for Accelerated MRI Reconstruction with Dense Image Labels for Quantitative Clinical Evaluation"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "e08da6bfbe6e821e47d30d0960bcf341850252fc", "title": "VORTEX: Physics-Driven Data Augmentations Using Consistency Training for Robust Accelerated MRI Reconstruction"}, {"paperId": "232734a65f0d903cdd77b9dfdddb1eded79eff18", "title": "Sparse Factorization of Large Square Matrices"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "3451010e8fa6a3032c8dd3be1daadb4a08375c64", "title": "AC/DC: Alternating Compressed/DeCompressed Training of Deep Neural Networks"}, {"paperId": "ad4f7a9e1c76b1d2fa3fd36198dcf8e514e08f31", "title": "Differentiable Multiple Shooting Layers"}, {"paperId": "9c4dd36ad206ca8be96ae4000568e899f4acfa91", "title": "Top-KAST: Top-K Always Sparse Training"}, {"paperId": "dfb90bbc13949bf332683773562eff5c73b2a87b", "title": "Gotta Go Fast When Generating Data with Score-Based Models"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "f93f2476972228de142fde13913bccbec76859b8", "title": "Initialization and Regularization of Factorized Neural Layers"}, {"paperId": "32ac6ad834487f693a11f554556a54a0c6fe7052", "title": "Blind Primed Supervised (BLIPS) Learning for MR Image Reconstruction"}, {"paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"paperId": "823deee03bc49c306170bee265446b5b7f95666f", "title": "Measuring Robustness in Deep Learning Based Compressive Sensing"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "2afa490dde7a8c582d889530c7f8b042fef6a8b7", "title": "Machine learning\u2013accelerated computational fluid dynamics"}, {"paperId": "0c9d97d2ba489256d4f1760598dc2c7be6d90d96", "title": "EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets"}, {"paperId": "4a54d58a4b20e4f3af25cea3c188a12082a95e02", "title": "Transformer Feed-Forward Layers Are Key-Value Memories"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "2f7dc1ee85e9f6a97810c66016e09ffeed684f03", "title": "Fourier Neural Operator for Parametric Partial Differential Equations"}, {"paperId": "2d11f0a389935a3f29fb4e83921226144ef02ce0", "title": "Approximate Simultaneous Diagonalization of Matrices via Structured Low-Rank Approximation"}, {"paperId": "8f04be7e53f5d66779c3ad3a76e1ee77177406f1", "title": "Solving Inverse Problems in Steady State Navier-Stokes Equations using Deep Neural Networks"}, {"paperId": "53439309acd147a51555dfcbe797beab652b25c5", "title": "Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise Sparsity"}, {"paperId": "06b8ece785ac521ac93bc17c4e9312573d66e0d1", "title": "Unsupervised MRI Reconstruction with Generative Adversarial Networks"}, {"paperId": "caf56a5780d8aff16557af72964aa1fbcea1f8f9", "title": "Prospective Deployment of Deep Learning in MRI: A Framework for Important Considerations, Challenges, and Recommendations for Best Practices"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "9af2b207750268981d20b4c491c7eb96a80a170c", "title": "Hypersolvers: Toward Fast Continuous-Depth Models"}, {"paperId": "384c60c73c0f3dd71774bba3b905cc12a6f8f2a5", "title": "Sparse Linear Networks with a Fixed Butterfly Structure: Theory and Practice"}, {"paperId": "46b93834a4bc3b15fefe77559c346c83e7c88f45", "title": "Accelerated MRI With Un-Trained Neural Networks"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "021739c140c4eefaf2bbb65658e119e5251576a3", "title": "Finding trainable sparse networks through Neural Tangent Transfer"}, {"paperId": "5aacc6e32563c290a32e629a8bfb641d8892c314", "title": "Optimal Lottery Tickets via SubsetSum: Logarithmic Over-Parameterization is Sufficient"}, {"paperId": "3b0fb765716ef6861a84abffcbe40643857c613b", "title": "Pruning neural networks without any data by iteratively conserving synaptic flow"}, {"paperId": "b672afc19364cc801b54543f4f87408ac2c41078", "title": "Logarithmic Pruning is All You Need"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "31603b3339f4da5bdc6b7de4231bd1ddfb32a50a", "title": "Neural Controlled Differential Equations for Irregular Time Series"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "a68c3412e60560290400d2707596f82a914b7c00", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps"}, {"paperId": "c114ce10c4a315d92c3815f54bc9893e7e6ef182", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "696b388ee6221c6dbcfd647a06883b2bfee773d9", "title": "Universal Differential Equations for Scientific Machine Learning"}, {"paperId": "90e79c0b7d7162a21da6285503dd69f341154fcc", "title": "Compressed Sensing: From Research to Clinical Practice With Deep Neural Networks: Shortening Scan Times for Magnetic Resonance Imaging"}, {"paperId": "3f06d02513a2763e472d2b5d5db08e9061081b9e", "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis"}, {"paperId": "989e681731eb27ac1e12b2d60c8612c6982fe8c7", "title": "Towards Physics-informed Deep Learning for Turbulent Flow Prediction"}, {"paperId": "46843b97bfabf0e8e9d4cdbf46f399419dfadba1", "title": "Self-Supervised Physics-Based Deep Learning MRI Reconstruction Without Fully-Sampled Data"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "d8e73f9034e16a380903282353e762d7f9517983", "title": "MLPerf Training Benchmark"}, {"paperId": "336868be817536e7c7fc88c391a2860cd869ea2b", "title": "Drawing early-bird tickets: Towards more efficient training of deep networks"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "60ed82ca3ec8fbfef4d52e98e49ab687ce501a0c", "title": "Sparse Networks from Scratch: Faster Training without Losing Performance"}, {"paperId": "c53ae5c2601de32c87dab796ab686c70e48c356f", "title": "One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers"}, {"paperId": "b1ac64438608aac1a8dfd0adf8fec8c6220f6bfd", "title": "Butterfly Transform: An Efficient FFT Based Neural Architecture Design"}, {"paperId": "26e44b8106a5145126c59b6d0c3af326337447f9", "title": "Unifying Orthogonal Monte Carlo Methods"}, {"paperId": "dd7bae431e0e4d94f24d54b0ac3a422703d38ed3", "title": "The Difficulty of Training Sparse Neural Networks"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "fdb55cde6ea08e1c78e0625e569bd7d61aa5987e", "title": "Deep-Learning Methods for Parallel Magnetic Resonance Imaging Reconstruction: A Survey of the Current Approaches, Trends, and Issues"}, {"paperId": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations"}, {"paperId": "075da5ebbb890924267b4b163292ad21d0b100a0", "title": "Stabilizing the Lottery Ticket Hypothesis"}, {"paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217", "title": "The State of Sparsity in Deep Neural Networks"}, {"paperId": "d86084808994ac54ef4840ae65295f3c0ec4decd", "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations"}, {"paperId": "d4b86f40bd1b4baf99e76d95ee5912c13d4c52fc", "title": "Learning Compressed Transforms with Low Displacement Rank"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "d54f3f8fe8b982e68231f82903fb2e7119746a7f", "title": "Quadrature-based features for kernel approximation"}, {"paperId": "bce22675d77e1ef28e92f3793c02f8f5ccdb0ddd", "title": "A Two-pronged Progress in Structured Dense Matrix Vector Multiplication"}, {"paperId": "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"paperId": "773d5ddc414424a8948446ddaa5275b944f50891", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon"}, {"paperId": "ee661a5510ab1f035aa5dbf3d06109c388e6358d", "title": "Learning a variational network for reconstruction of accelerated MRI data"}, {"paperId": "649ddc5c410788d76d979bce4dc8be8dc1656d44", "title": "Low-Rank and Adaptive Sparse Signal (LASSI) Models for Highly Accelerated Dynamic Imaging"}, {"paperId": "32e934094c4d17fe4d734b2e169ba5e3cd0ee05e", "title": "Orthogonal Random Features"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets"}, {"paperId": "950619635df80e87c6f25b486cc5eaad4d71d0b0", "title": "DSD: Dense-Sparse-Dense Training for Deep Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "54c3e878bf0ff2fdde16e439b5579ee99ee0d0d8", "title": "ACDC: A Structured Efficient Linear Layer"}, {"paperId": "bf76be8df2f2bc56edac98a5d0dfc19c85882eaa", "title": "Structured Transforms for Small-Footprint Deep Learning"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "b6258b32a3838af9efb30e9b323f88b8ff989c86", "title": "Beyond Low Rank + Sparse: Multiscale Low Rank Matrix Decomposition"}, {"paperId": "c9f199fbd06922d297a7332de01accf04d8dbaec", "title": "Flexible Multilayer Sparse Approximations of Matrices and Applications"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "6364fdaa0a0eccd823a779fcdd489173f938e91a", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "071732c06943df639108c162a020e7f81708a0e5", "title": "Low-Rank Modeling of Local  $k$-Space Neighborhoods (LORAKS) for Constrained MRI"}, {"paperId": "8ce9de8009d7186a16804e55ad7d2d8ce595d350", "title": "Fastfood - Computing Hilbert Space Expansions in loglinear time"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "ec6271dd0df8e9d22aaa96a0f6ec7482836786f2", "title": "Sparse MRI: The application of compressed sensing for rapid MR imaging"}, {"paperId": "79d1b330f0ef51f63ecb9b291dd5a05de5a858c0", "title": "Toeplitz and Circulant Matrices: A Review"}, {"paperId": "a79e019871750d3102b2b00e647c012fc0f39d19", "title": "Generalized autocalibrating partially parallel acquisitions (GRAPPA)"}, {"paperId": "21676bb8111e74f05732de20a3bcc1284793417b", "title": "SENSE: Sensitivity encoding for fast MRI"}, {"paperId": "8df26a339b6cbb1089e02f2039ac6c30a6ec89d9", "title": "On a new class of structured matrices"}, {"paperId": "ab3976cc49b61845c54b5babbb0e0e011c85edc9", "title": "Fast Discrete Polynomial Transforms with Applications to Data Analysis for Distance Transitive Graphs"}, {"paperId": "16861fe9ca585d936ecbf07b6697c6a360a01a75", "title": "Numerical Methods for Simultaneous Diagonalization"}, {"paperId": "30e4c372514c568955826abd84676c9392dd7e71", "title": "FFTs in external or hierarchical memory"}, {"paperId": "7306af13eb2052fe9b9652c7d8b669655d307635", "title": "Displacement ranks of matrices and linear equations"}, {"paperId": "956081123f4a530ccf93aa7bfe196412118fc41a", "title": "A generalized solution of the orthogonal procrustes problem"}, {"paperId": "0e6beb95b5150ce99b108acdefabf70ccd3fee30", "title": "An algorithm for the machine calculation of complex Fourier series"}, {"paperId": "8112c4305b88d85199267e9e03d3a0aca4432059", "title": "The approximation of one matrix by another of lower rank"}, {"paperId": "5a5c05f7d4f4bdd374268f9365362cd66089d21c", "title": "Noise2Recon: A Semi-Supervised Framework for Joint MRI Reconstruction and Denoising"}, {"paperId": null, "title": "2021), as well as acceleration of numerical schemes (Poli et al., 2020"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "2020) says that if n is a power of 2 and A is an n\u00d7nmatrix such that multiplying any vector v by A can be represented as a linear arithmetic circuit with depth\u2264 d and\u2264 s total gates, then A\u2208 (BB\u2217(n)"}, {"paperId": null, "title": "2020, Appendix J) show, the matrix class BB\u2217 can represent convolution, Hadamard transform, Toeplitz matrices, and AFDF. Since the Monarch classMM\u2217 contains the butterfly class BB\u2217"}, {"paperId": null, "title": "Note that the Hadamard transform is actually in B (Dao et al., 2020)"}, {"paperId": "712ba0bf4c2208b6446bd03190df31a514855e09", "title": "Deep Generative Adversarial Neural Networks for Compressive Sensing MRI"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "a07609c2ed39d049d3e59b61408fb600c6ab0950", "title": "GPU Kernels for Block-Sparse Weights"}, {"paperId": "88cd4209db62a34d9cba0b9cbe9d45d1e57d21e5", "title": "Runtime Neural Pruning"}, {"paperId": null, "title": "2014)) and kernel approximation (Le et"}, {"paperId": null, "title": "We use the default implementation and hyperparameters used by Desai et al. (2021c) to benchmark the SKM-TEA dataset"}, {"paperId": null, "title": "Speech and language processing , volume 3"}, {"paperId": "c47561c62010c3d7cddcf26b093549558809d102", "title": "THE MINIMAL POLYNOMIAL AND SOME APPLICATIONS"}, {"paperId": "09b028e77aa5f3d3175c4059e4ba57326c317e48", "title": "Toeplitz And Circulant Matrices: A Review (Foundations and Trends(R) in Communications and Information Theory)"}, {"paperId": "181aea672710f1cadbb1cc7e5e75662337b2ce41", "title": "Computed Tomography: Principles, Design, Artifacts, and Recent Advances"}, {"paperId": "7c9411bc50b6a33947575f37ae16931578beae23", "title": "Structured Matrices and Polynomials: Unified Superfast Algorithms"}, {"paperId": "d343e37da5fcb61db8c65e8ba3bb91a98f1befe5", "title": "Spectral Methods in MATLAB"}, {"paperId": null, "title": "Random Butterfly Transformations with Applications in Computational Linear Algebra"}, {"paperId": null, "title": "Sparse matrices, volume 69"}, {"paperId": null, "title": "We compare our method to two baselines, SENSE and U-Net. Parameter count and hyperparameters are available in Table 16"}, {"paperId": null, "title": "We evaluate reconstruction performance using peak signal-to-noise ratio (pSNR) and structural similarity (SSIM) on both echoes (echo1 -E1, echo2 -E2) separately"}, {"paperId": null, "title": "Fuse matrix multiplication and adding bias into one CUDA kernel in the feed-forward network (FFN) layers. The gradient of the bias is also fused with the matrix multiplication the backward pass"}, {"paperId": null, "title": "Use a fused CUDA kernel (FMHA) that combines 4 steps into one kernel: computes QK T , take softmax, apply dropout, multiply by V , where Q, K, V are the query, key, and value respectively"}, {"paperId": null, "title": "Remove padding tokens and only compute the attention for non-padding tokens"}]}