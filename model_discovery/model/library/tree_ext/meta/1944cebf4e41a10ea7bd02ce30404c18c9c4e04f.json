{"paperId": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f", "title": "Linear Complexity Randomized Self-attention Mechanism", "abstract": "Recently, random feature attentions (RFAs) are proposed to approximate the softmax attention in linear time and space complexity by linearizing the exponential kernel. In this paper, we first propose a novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers. This perspective further sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA). RA constructs positive random features via query-specific distributions and enjoys greatly improved approximation fidelity, albeit exhibiting quadratic complexity. By combining the expressiveness in RA and the efficiency in RFA, we develop a novel linear complexity self-attention mechanism called linear randomized attention (LARA). Extensive experiments across various domains demonstrate that RA and LARA significantly improve the performance of RFAs by a substantial margin.", "venue": "International Conference on Machine Learning", "year": 2022, "citationCount": 23, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2204.04667", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers is proposed and sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA)."}, "embedding": {"model": "specter_v2", "vector": [0.30166107416152954, 1.3540313243865967, -1.0082298517227173, -0.03219800442457199, -0.872793972492218, 0.3849753141403198, 0.329700231552124, -0.3169271647930145, -0.373805969953537, -0.31732144951820374, 0.4216805696487427, 0.6369857788085938, 0.4560633599758148, -0.06280753016471863, -0.43314260244369507, -0.07240046560764313, -0.8945564031600952, 0.023589475080370903, 0.29935407638549805, -0.16094805300235748, -0.031767066568136215, -1.3276581764221191, -1.2421854734420776, 0.04432826489210129, 0.2859358489513397, 0.5925917625427246, 0.17723506689071655, 0.8686845302581787, -0.32580357789993286, 0.3137790560722351, 0.3514276444911957, -0.24604514241218567, 0.3506065011024475, -0.3716903328895569, -0.5032152533531189, -0.2543834447860718, 0.00035466928966343403, 0.017534438520669937, -0.8113532066345215, 1.0753099918365479, 0.09763085842132568, 0.2351612150669098, 0.5226266384124756, -0.5408366322517395, -0.850590705871582, 1.0650960206985474, -0.03736253082752228, 0.9381845593452454, -0.29031556844711304, -0.7277665734291077, 1.777612566947937, -1.0592808723449707, 0.2728561758995056, 1.6062387228012085, -0.20871812105178833, 0.4325808584690094, -0.24307046830654144, -0.7079917192459106, 1.1365329027175903, 0.5393052101135254, -0.9373109936714172, -0.34104201197624207, 0.06494800001382828, 0.10747207701206207, 1.5484766960144043, -0.4385138154029846, -0.45374876260757446, 0.373220294713974, 0.11464880406856537, 1.6034998893737793, -0.37846899032592773, -0.8449809551239014, -0.037971947342157364, 0.5316590666770935, 0.3737318515777588, 0.419876366853714, -0.4443131983280182, 0.42294231057167053, -0.9273537397384644, -0.18197336792945862, 0.09746671468019485, -0.3478206992149353, 0.078547403216362, -0.49428120255470276, 0.38620319962501526, 0.9130383133888245, 0.4675005376338959, 0.6505644917488098, -0.23167163133621216, 1.1413614749908447, 0.5052664279937744, 0.11637267470359802, 0.3657573163509369, 0.13370098173618317, -0.1499999761581421, 0.08332768827676773, -0.43916502594947815, 0.0389765202999115, -0.34030216932296753, 0.9878187775611877, 0.039020836353302, 0.6353369355201721, -0.8252923488616943, 0.3726840913295746, 1.047306776046753, 0.4784189760684967, 0.7773600220680237, -0.1647825688123703, 0.06443045288324356, -0.7239426970481873, -0.11672549694776535, -0.6953978538513184, -0.0035086884163320065, -0.25886017084121704, -0.9272836446762085, -0.8292116522789001, -0.7030395269393921, 0.3206133246421814, -0.6884769201278687, 0.9360545873641968, -0.2010338455438614, -0.5361320376396179, -0.22890785336494446, 0.5383408665657043, 0.4202360212802887, 0.3362561762332916, 0.24180984497070312, 0.4138723909854889, 1.1712255477905273, -0.6631767749786377, -0.5546602010726929, -0.9085735082626343, 0.2880804240703583, 0.08366495370864868, 0.2608659565448761, -0.3043438196182251, -1.0532039403915405, -1.0644254684448242, -0.641812264919281, 0.25397786498069763, -0.2955392003059387, 0.2772124409675598, 0.7714287042617798, 0.49141165614128113, -0.6722935438156128, 0.6660282611846924, -0.27293938398361206, -0.24658611416816711, 0.7724579572677612, 0.38879862427711487, 0.2927197813987732, -0.05356500670313835, -1.2284579277038574, 0.08829156309366226, 0.1691041737794876, -0.8573849201202393, 0.022451825439929962, -0.2570779621601105, -1.156730055809021, 0.2221640795469284, 0.8610047698020935, -0.5407726168632507, 1.463202953338623, -0.7137837409973145, -1.4790098667144775, 0.6902817487716675, -0.5832576155662537, -0.09948095679283142, 0.3990377187728882, -0.3929923474788666, -0.5285789966583252, -0.4616374373435974, -0.1387755125761032, 0.21082179248332977, 0.8009845018386841, -0.08414354175329208, -0.35007259249687195, -0.05830894038081169, -0.3644019365310669, -0.41941890120506287, -0.3898371160030365, 0.6047276854515076, -0.6825188398361206, -0.3796432316303253, 0.31037425994873047, 0.47956860065460205, -0.08021264523267746, -0.4214238226413727, -0.6658897995948792, -1.4626531600952148, 0.15511341392993927, 0.5046194195747375, 0.9002331495285034, -0.977741539478302, -0.9298717379570007, 0.2255653440952301, 0.02746731787919998, -0.21018370985984802, -0.8036521673202515, 0.18670876324176788, -1.1079710721969604, 0.38443589210510254, 0.02187602035701275, -0.5329486131668091, 0.07547342032194138, -0.6184473037719727, -0.2940179407596588, 0.01408055517822504, 0.5186684131622314, 0.9866176843643188, -1.1711201667785645, 0.14185504615306854, -0.2477155327796936, 0.07061585783958435, -1.0694224834442139, 1.191031575202942, -0.1335541009902954, -0.2255510687828064, -0.47882741689682007, -0.2845354974269867, 0.3734475374221802, -0.3833402991294861, 0.0767517238855362, -0.42845648527145386, 0.20110061764717102, 0.38737645745277405, -0.585175633430481, 0.7163832187652588, -0.2696371376514435, 0.6426138281822205, -0.1641603261232376, -0.8776580095291138, -0.3312644064426422, 0.35938766598701477, 0.2276608645915985, -0.8349226117134094, 0.36097821593284607, -0.31966274976730347, -0.7730068564414978, -0.151300311088562, 0.5124846696853638, 1.1245722770690918, -0.24246706068515778, 0.28892919421195984, 0.5198536515235901, 0.26264405250549316, -0.0018354718340560794, 0.17384444177150726, 0.8192271590232849, 0.4268738329410553, 1.2983245849609375, -0.1973288208246231, 0.1633904129266739, -1.0257418155670166, 0.09981340169906616, 1.073213815689087, 0.49021202325820923, 0.7112957835197449, 0.28010791540145874, -0.4817117154598236, -0.12291835993528366, -0.14625966548919678, 0.30730196833610535, 1.9004473686218262, -0.43326127529144287, 0.012288683094084263, -0.5062753558158875, -0.644199788570404, -0.17391207814216614, 0.18794511258602142, -0.8835728168487549, -0.5714591145515442, -0.2662663459777832, -1.6142148971557617, 0.21358518302440643, 0.37461552023887634, 0.5872364640235901, -0.5245488882064819, 0.2369271218776703, -0.10697026550769806, 0.45560842752456665, -0.6946966648101807, -0.8284730911254883, 0.7025070190429688, 0.2038213014602661, -0.02495403401553631, -0.07843931764364243, 0.3188253939151764, 0.36761513352394104, -0.38782787322998047, 0.8948147296905518, -1.1902278661727905, -0.4711519777774811, 0.4878663718700409, 0.08947143703699112, -1.0095733404159546, -0.3423095643520355, 0.5037933588027954, 0.27298659086227417, 0.03233588859438896, 0.42130419611930847, 0.4810788631439209, -0.1573454886674881, 0.20878177881240845, -0.31655600666999817, -0.364385187625885, 0.4082310199737549, 0.32585039734840393, 0.3449753224849701, -0.072385273873806, 0.41365477442741394, -1.5568865537643433, 0.6723038554191589, -0.29429617524147034, -0.2748335897922516, -0.09464897960424423, -0.8731803297996521, 0.2542873024940491, 0.43954434990882874, -0.8720937371253967, -0.5142576694488525, -0.5853078961372375, 0.2723362147808075, 0.010082109831273556, -0.041917745023965836, -0.06970816850662231, 0.1821366250514984, -0.04167274013161659, 0.6948972344398499, 0.5130180716514587, 0.15753869712352753, 0.31887397170066833, 0.76837557554245, -0.7252898216247559, 0.45717164874076843, 0.10709942877292633, 0.681062638759613, 0.08825047314167023, -0.537603497505188, -0.8110679984092712, -0.5850334167480469, -0.5410929322242737, 0.23204433917999268, 0.016958288848400116, 0.1380058079957962, -0.685505747795105, -1.2952347993850708, -0.48640820384025574, -0.4184965193271637, -0.06442854553461075, -0.45391345024108887, -0.05541393533349037, -0.3351843059062958, -1.0494216680526733, -0.6833280920982361, -1.1114929914474487, -0.8413038849830627, -0.7201622724533081, 0.3284517824649811, 0.3333127796649933, -0.4810882806777954, -0.24050059914588928, -0.24366934597492218, -0.41896960139274597, 1.2104249000549316, -1.1258808374404907, 0.3736320734024048, 0.018978578969836235, -0.5678166747093201, -0.6422386169433594, -0.06475209444761276, 0.2970442771911621, 0.11856935173273087, -0.08430977165699005, -1.1283276081085205, 0.18745744228363037, -0.5846317410469055, -0.11643572151660919, 0.2753812372684479, 0.7940879464149475, 1.2381943464279175, 0.21042898297309875, -0.6421442031860352, 0.06741929054260254, 1.3749841451644897, -0.6187168955802917, -0.12010329961776733, 0.2514776289463043, 0.8137854337692261, 0.02398831397294998, -0.07949937134981155, 0.7347030639648438, 0.7523962259292603, 0.65628582239151, 0.20450004935264587, -0.41943714022636414, 0.22062097489833832, -0.21545322239398956, 0.5452730059623718, 0.7021000385284424, 0.4189470410346985, 0.050934989005327225, -0.33798182010650635, 0.8379251956939697, -1.2878813743591309, -1.1007628440856934, 0.5437102317810059, 1.0368374586105347, -0.055983252823352814, -0.23545506596565247, -0.07542368024587631, -0.36181163787841797, 0.3712078928947449, 0.12018511444330215, -0.4913825988769531, -0.28608086705207825, 0.06254152953624725, 0.6211756467819214, 0.8180944919586182, 0.47137126326560974, -0.5374215245246887, 0.9956976175308228, 14.853021621704102, 0.7625241875648499, -0.0026966705918312073, 0.6683477759361267, 0.7195303440093994, 0.16433870792388916, -0.019872836768627167, -0.023268837481737137, -1.1031112670898438, -0.18737433850765228, 0.8097816705703735, -0.2915342450141907, 0.5144070386886597, 0.6354787945747375, 0.13443110883235931, -0.02292332798242569, -0.32388439774513245, 0.5601707100868225, 0.4445650279521942, -0.7381949424743652, 0.14478182792663574, 0.19362960755825043, 0.13755469024181366, 0.8734925389289856, 0.8448429703712463, 0.9096730351448059, 1.0699104070663452, 0.08979683369398117, 0.21805799007415771, 0.4833507835865021, 1.0062928199768066, 0.08309388160705566, 0.16101700067520142, 0.08536483347415924, -1.0469062328338623, -0.41251954436302185, -0.48926472663879395, -0.5934834480285645, -0.09753938764333725, 0.3987792730331421, -0.2919897735118866, -0.3090057373046875, 0.2167816460132599, 0.19112201035022736, 0.2582239508628845, 0.33801424503326416, -0.028700055554509163, 0.6136972308158875, 0.20783425867557526, -0.1706760674715042, -0.3941754996776581, 1.011006474494934, 0.13442008197307587, 0.05558457598090172, -0.14104805886745453, 0.04151341691613197, -0.07315807044506073, 0.387742817401886, -0.24166305363178253, 0.18354535102844238, 0.08185996115207672, -0.03665560483932495, -0.35300368070602417, 0.7921253442764282, 1.1223795413970947, -0.06750046461820602, -0.2640202045440674, 0.4688670337200165, 0.6811521053314209, 0.2043333649635315, -0.3899075388908386, -0.2976675033569336, 0.580819308757782, -0.43936458230018616, 0.0805022269487381, 1.0364902019500732, -0.020154815167188644, -0.5909632444381714, -0.6542730927467346, -0.1661745309829712, 0.4840221405029297, -0.9069470763206482, -1.3122509717941284, 0.7842894196510315, -0.44027432799339294, -0.5146571397781372, 0.31508931517601013, -0.437846302986145, -0.2454598993062973, 0.4837658703327179, -1.2666518688201904, -0.07839957624673843, 0.2593107223510742, -0.6934861540794373, -0.43741312623023987, -0.0791747123003006, 1.19037663936615, -0.14519019424915314, -0.5780914425849915, 0.666766881942749, 0.2532442808151245, -0.2012949287891388, 0.37947604060173035, -0.8263393640518188, 0.8092893958091736, -0.11478482186794281, -0.5382450819015503, 0.2221175730228424, 0.1753435581922531, 0.5120301246643066, -0.6327149868011475, -0.04009504243731499, 0.46414896845817566, -0.933510422706604, -0.29382604360580444, -0.6598874926567078, -0.8908195495605469, -0.1047271117568016, 0.456064373254776, 0.20784886181354523, 0.42460426688194275, 0.28816527128219604, -0.4430544972419739, -0.5580119490623474, -0.5596168637275696, -0.10450879484415054, 0.28963688015937805, -0.5904482007026672, -0.029155300930142403, -0.052682965993881226, 0.1276409775018692, -0.83003830909729, -0.23381349444389343, -0.30479779839515686, -0.034614548087120056, 0.22680199146270752, 1.1468063592910767, -0.5532295107841492, 0.4633588194847107, 0.677947461605072, 0.33223119378089905, -0.6485623121261597, -0.7454386353492737, -0.9064504504203796, -0.025650212541222572, 0.5250614881515503, 0.401496946811676, -0.5839202404022217, 0.7632898688316345, 0.7208483219146729, 0.589082658290863, -0.33289775252342224, -0.8219187259674072, -0.0447976291179657, -0.3948531448841095, -0.9031244516372681, 0.0933302491903305, 0.0844678208231926, 0.15133509039878845, -0.07701335102319717, 0.24593757092952728, 0.6752209663391113, 0.18234841525554657, -0.8589997291564941, 0.6431382894515991, -0.06860829889774323, -0.12725135684013367, -0.7412117719650269, -0.581032395362854, -1.4397375583648682, -0.2448679655790329, -1.2763863801956177, 0.16310524940490723, -0.5242027044296265, -0.30430394411087036, 0.30645105242729187, -0.6350087523460388, -0.062084831297397614, -0.1974407434463501, -0.5738144516944885, -0.770893394947052, -0.9256183505058289, -0.6078715324401855, 1.0710994005203247, 0.46256023645401, -0.742760956287384, -0.14126037061214447, 0.6121952533721924, -0.7537917494773865, 0.0595133975148201, 0.44363972544670105, -0.9492371082305908, -0.2633914053440094, -0.6531919836997986, 0.16487833857536316, 0.060541898012161255, -0.14547765254974365, -0.8526079058647156, 0.7081446647644043, 0.005723939277231693, 0.004583210684359074, 0.05370035767555237, 0.04981810972094536, -0.989235520362854, -0.9189290404319763, 0.22223949432373047, -0.909142255783081, -0.06318087875843048, -0.15328697860240936, -0.08310830593109131, -0.2418556958436966, 0.7245963215827942, 0.1688755303621292, -1.1973836421966553, 0.02632885053753853, 0.6758601665496826, -0.959126353263855, 0.442207396030426, -0.3189869821071625, -0.1336723119020462, -0.8590932488441467, -0.4502125084400177, -0.03334539011120796, 0.39040690660476685, -0.7759164571762085, 1.2465951442718506, -0.07730016857385635, -1.1754456758499146, 0.14251260459423065, 0.3086133599281311, 0.6389472484588623, 0.03971448540687561, 1.0095717906951904, 0.24665793776512146, 0.33921968936920166, 0.5049542188644409, 0.7581552863121033, 0.41736119985580444, -0.7597187161445618, 0.3407939374446869, 0.8309271931648254, -0.3961678445339203, -0.1424303650856018, 1.0349875688552856, 0.27080434560775757, -1.004672884941101, 0.2043657898902893, -1.0429737567901611, -0.5232962965965271, -0.21261270344257355, 0.6340827941894531, 0.056596022099256516, -0.34781473875045776, 0.006973444949835539, -0.5765326023101807, 0.1722985953092575, -0.24216587841510773, -0.38178786635398865, 0.6942144632339478, -0.2698204517364502, -0.19533759355545044, 0.4152367115020752, 0.7671023607254028, -0.34880614280700684, -0.4772101938724518, -1.2830384969711304, -0.2674466371536255, -0.12997224926948547, 0.4389066994190216, 0.03334229812026024, -0.923406183719635, 0.2530384063720703, 1.076220154762268, 0.38984575867652893, 0.12334362417459488, 0.164004385471344, -0.5111894607543945, 0.48316773772239685, -0.25383278727531433, -0.879370391368866, -0.5634596943855286, 0.9608474373817444, 0.9285905361175537, -1.0429273843765259, 0.15281429886817932, -0.46648338437080383, -0.6289598345756531, 0.7391465306282043, 0.623897910118103, -0.1411694586277008, 0.9348717331886292, -0.30121558904647827, -0.027162987738847733, 0.17527423799037933, -1.030383586883545, -0.8936566114425659, 1.2217392921447754, 0.8566169142723083, 0.8073258996009827, 0.32958322763442993, 0.15728804469108582, 0.9727702140808105, 0.1452028602361679, 0.09657811373472214, 0.5476394891738892, 0.23959019780158997, -0.3372780978679657, -0.015877248719334602, 0.06559041142463684, 0.8034999966621399, -0.8722859621047974, -0.3581610321998596, 0.3501212000846863, 0.42468106746673584, 0.11197183281183243, 0.36442095041275024, 0.810215950012207, -0.12478432804346085, 0.6466690897941589, -0.1142205074429512, 0.2047600895166397, -0.5836958289146423, -0.03189587965607643, -0.1904110163450241, -0.8061090707778931, -0.2162945568561554, 0.1434590369462967, -0.5499837398529053, -0.3390580117702484, 0.4267776906490326, -0.22563032805919647, -0.15776368975639343, 0.30840662121772766, 1.027367353439331, 0.7545086741447449, 1.0646625757217407, 0.31070610880851746, -0.45357996225357056, -0.374612420797348, -0.9984481334686279, 0.029918836429715157, -0.5460265278816223, -0.09018898755311966, -0.10972996801137924, -0.24417142570018768, -0.5535717606544495]}, "authors": [{"authorId": "1633166807", "name": "Lin Zheng"}, {"authorId": "2146309022", "name": "Chong Wang"}, {"authorId": "47648549", "name": "Lingpeng Kong"}], "references": [{"paperId": "4b0541eccd8f98852d6807a14fbac17f775c7b40", "title": "Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\u00f6m Method"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "12640af46eaf4c16125557b517a2d37fca70a82d", "title": "Ripple Attention for Visual Perception with Sub-quadratic Complexity"}, {"paperId": "e0cbbca02b332f398c6639b3bea0613f79166220", "title": "ABC: Attention with Bounded-memory Control"}, {"paperId": "37abe53ed31caa23ae833b2e67bb4aa1892e8d25", "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention"}, {"paperId": "9933a5af7895354087baf6c96b64dc8a8973eaed", "title": "Perceiver IO: A General Architecture for Structured Inputs & Outputs"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "7b664a306b7d2f68dd816ea1d6586cf3472d75c1", "title": "Early Convolutions Help Transformers See Better"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "7fff8018bf625447df837c2fda5c58a705fbc038", "title": "XCiT: Cross-Covariance Image Transformers"}, {"paperId": "78a0fb70b79116eb8d42c5951ced4f9efba513f0", "title": "Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "2e8149dafb864ec3675087c99bf5572fcf4eb170", "title": "RegionViT: Regional-to-Local Attention for Vision Transformers"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "c0f709acf38eb27702b0fbce1215db0ebaa2de2b", "title": "SMYRF: Efficient Attention using Asymmetric Clustering"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "29168348f4729d418df5acc8a5fce4f1c428a7e3", "title": "Sparsifying Transformer Models with Trainable Representation Pooling"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "ac6535d096fc79dde2d9ce0329e0626b79ede7f0", "title": "Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "13da774fe604027bff2951ba82f4c3d9be7e415e", "title": "Augment Your Batch: Improving Generalization Through Instance Repetition"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "a3c5cdf7819086368629c4d08334288cbdc31a98", "title": "Optimal multiple importance sampling"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145", "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "1e7678467b1807777dcd9be557b79328ce9419a8", "title": "MultiGrain: a unified image embedding for classes and instances"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "0d3c46a3cbfe06cec259fec954b6ff6df6c1a566", "title": "Learning long-range spatial dependencies with horizontal gated-recurrent units"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "b68811a9b5cafe4795a11c1048541750068b7ad0", "title": "The \u201cSomething Something\u201d Video Database for Learning and Evaluating Visual Common Sense"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6", "title": "The Kinetics Human Action Video Dataset"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "3a29aa4eff48624752c07059a44d3288a678c8ab", "title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "2538378f9b2aee6d2cfe106138070274f8edaa1f", "title": "Doubly Stochastic Variational Bayes for non-Conjugate Inference"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "484ad17c926292fbe0d5211540832a8c8a8e958b", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "d64a1588bc3dacfeacf39ff1fa543c6832e051e2", "title": "Optimally combining sampling techniques for Monte Carlo rendering"}, {"paperId": "54c922b956f8d07a81799b8f94d241b3ec0d40fa", "title": "Harmonic Analysis and the Theory of Probability"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "5a9bc55f6332e38f62eb509b684147a1d4f10fd9", "title": "Focal Attention for Long-Range Interactions in Vision Transformers"}, {"paperId": null, "title": "2020), which is widely adopted in the context of NLP"}, {"paperId": null, "title": "LARA use much fewer model parameters than vanilla PVTv2"}, {"paperId": null, "title": "2021b); \u2022 l = 2 and \u03be(x"}, {"paperId": null, "title": "Linear unified nested attention"}, {"paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f", "title": "AUTO-ENCODING VARIATIONAL BAYES"}, {"paperId": null, "title": "Pyslowfast"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Set transformer: A framework for attentionbased permutation-invariant neural networks"}, {"paperId": null, "title": "Listops output prediction (Nangia"}, {"paperId": null, "title": "2.64\u00d7) proposed unified framework of most low-rank attention approximations and (3) Nystr\u00f6mformer (Xiong et al., 2021), which we find is a strong low-rank baseline across our experiments"}, {"paperId": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting"}, {"paperId": null, "title": "Monte Carlo theory, methods and examples"}, {"paperId": "3bf2e6941dbb87ac0d2c771c159e1e27366a26e3", "title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics"}, {"paperId": null, "title": "byte-level document retrieval on AAN (Radev et al., 2013), pixel-level image recognition on CIFAR-10 (Krizhevsky et al., 2009) and Pathfinder (Linsley et al., 2018)"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}]}