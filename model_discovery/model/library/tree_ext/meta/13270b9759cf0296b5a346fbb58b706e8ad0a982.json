{"paperId": "13270b9759cf0296b5a346fbb58b706e8ad0a982", "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design", "abstract": "Attention-based neural networks have become pervasive in many AI tasks. Despite their excellent algorithmic performance, the use of the attention mechanism and feedforward network (FFN) demands excessive computational and memory resources, which often compromises their hardware performance. Although various sparse variants have been introduced, most approaches only focus on mitigating the quadratic scaling of attention on the algorithm level, without explicitly considering the efficiency of mapping their methods on real hardware designs. Furthermore, most efforts only focus on either the attention mechanism or the FFNs but without jointly optimizing both parts, causing most of the current designs to lack scalability when dealing with different input lengths. This paper systematically considers the sparsity patterns in different variants from a hardware perspective. On the algorithmic level, we propose FABNet, a hardware-friendly variant that adopts a unified butterfly sparsity pattern to approximate both the attention mechanism and the FFNs. On the hardware level, a novel adaptable butterfly accelerator is proposed that can be configured at runtime via dedicated hardware control to accelerate different butterfly layers using a single unified hardware engine. On the Long-Range-Arena dataset, FABNet achieves the same accuracy as the vanilla Transformer while reducing the amount of computation by 10$\\sim66\\times$ and the number of parameters 2$\\sim22\\times$. By jointly optimizing the algorithm and hardware, our FPGA-based butterfly accelerator achieves 14.2$\\sim23.2\\times$ speedup over state-of-the-art accelerators normalized to the same computational budget. Compared with optimized CPU and GPU designs on Raspberry Pi 4 and Jetson Nano, our system is up to $273.8\\times$ and $15.1\\times$ faster under the same power budget", "venue": "Micro", "year": 2022, "citationCount": 36, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2209.09570", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "FABNet is proposed, a hardware-friendly variant that adopts a unified butterfly sparsity pattern to approximate both the attention mechanism and the FFNs and a novel adaptable butterfly accelerator is proposed that can be configured at runtime via dedicated hardware control to accelerate different butterfly layers using a single unified hardware engine."}, "embedding": {"model": "specter_v2", "vector": [0.4937419295310974, 0.41499045491218567, -0.18200767040252686, 0.056955743581056595, 0.27837175130844116, 0.17055627703666687, 0.14878994226455688, -0.29109448194503784, -0.5555089116096497, -0.127711683511734, 0.42743486166000366, 0.18786190450191498, 0.2697732746601105, -0.14694912731647491, -0.29210492968559265, 0.0972517654299736, -0.8545039296150208, -0.07241250574588776, 0.2616346478462219, -0.16971170902252197, 0.29551181197166443, -0.21653738617897034, -1.3401044607162476, 0.14055781066417694, -0.01734915003180504, 1.2364013195037842, 0.18453717231750488, 1.0506596565246582, -0.15692166984081268, 0.4109590947628021, 0.6397242546081543, -0.16078847646713257, 0.7210449576377869, 0.12205496430397034, -0.29707249999046326, -0.6680667996406555, 0.8606200814247131, -0.2564237117767334, -0.6263967752456665, 0.8610292077064514, -0.18956418335437775, 0.18794164061546326, 0.39388322830200195, -0.6020137071609497, -0.11996058374643326, 0.49880653619766235, 0.4624846875667572, 1.3623664379119873, -0.612212061882019, 0.1638200730085373, 0.9361271262168884, -0.8825269341468811, -0.027052128687500954, 1.1518089771270752, 0.8383232951164246, 0.2094004899263382, 0.11621156334877014, -0.6766539216041565, 0.4827575981616974, 0.20279566943645477, -0.4012207090854645, -0.18418502807617188, 0.33788561820983887, -0.08854485303163528, 1.9536230564117432, -0.470923513174057, 0.42684024572372437, 0.6524200439453125, 0.45610857009887695, 1.2352858781814575, -0.1264568418264389, -0.938209056854248, 0.19973613321781158, -0.2755267322063446, 0.49641650915145874, 0.4863758385181427, -0.0250296201556921, 0.2765648663043976, -1.1399427652359009, -0.06541194766759872, 1.0091633796691895, 0.3033813238143921, 0.5537622570991516, -0.006650934461504221, -0.2508808374404907, 0.8975690603256226, 1.071978211402893, 0.7521105408668518, -0.7077348232269287, 0.926803708076477, 0.5970050096511841, 0.06395149976015091, -0.5701364874839783, 0.9311949014663696, 0.3624335825443268, 0.6122362613677979, -0.7095143795013428, 0.18356122076511383, -0.2482166886329651, 1.0001378059387207, -0.21703395247459412, 0.8580018877983093, -0.3829188942909241, -0.17270207405090332, 1.026827335357666, 0.11423516273498535, 0.451338529586792, -0.6956048011779785, 0.050047386437654495, -0.4690791070461273, -0.20612843334674835, -1.0574228763580322, -0.22889098525047302, -0.6106122136116028, -1.295684814453125, -0.6581284403800964, -0.2515202760696411, 0.3989299237728119, -0.7366549372673035, 0.21022501587867737, -0.5044294595718384, 0.03282957524061203, -0.11642033606767654, 0.6024073362350464, 0.39736464619636536, 0.4872158169746399, 0.18161024153232574, 0.4698390066623688, 1.0903050899505615, -1.5299627780914307, -0.43158403038978577, -1.347413420677185, -0.216115340590477, -0.07042834162712097, 0.08738687634468079, -0.02571987174451351, -1.6154675483703613, -1.1943572759628296, -1.1276379823684692, -0.1190899908542633, -0.47388485074043274, -0.11593303084373474, 1.3646118640899658, -0.1520838588476181, -0.7096453905105591, 0.9211037755012512, -0.29289281368255615, 0.1327483057975769, 0.7074868679046631, 0.6705394387245178, 0.6395013332366943, 0.16201196610927582, -1.1696016788482666, 0.054194603115320206, 0.26849034428596497, -0.3028510808944702, -0.05994851887226105, -0.44606447219848633, -0.6606911420822144, 0.45962411165237427, -0.08443527668714523, -1.0025490522384644, 1.0011533498764038, -0.5891305208206177, -1.1376591920852661, 0.33187779784202576, 0.05954956263303757, -0.4780554473400116, -0.1975834220647812, 0.05032877251505852, -0.48716220259666443, -0.0023173149675130844, -0.3550839424133301, 1.005671739578247, 0.9003611207008362, 0.37890201807022095, -0.03970460593700409, 0.12538503110408783, 0.0030320172663778067, -0.27270153164863586, -0.6889317631721497, 0.6597334742546082, -0.4014241099357605, -0.42444154620170593, 0.2170201539993286, 0.3688095510005951, -0.2975273132324219, -0.2812255918979645, -0.3015204668045044, -0.8129141330718994, 0.3918485641479492, 0.6525499224662781, 1.2087786197662354, -1.0366392135620117, -0.7613921761512756, 0.04103461652994156, 0.17587393522262573, -0.007920385338366032, -0.5954208374023438, 0.02516683004796505, -0.4058797061443329, -0.044106125831604004, 0.0998290404677391, -0.2175774723291397, -0.427360475063324, -0.5739501118659973, -0.6548808217048645, -0.16787280142307281, 0.1429915875196457, 0.9513137340545654, -0.7303452491760254, 0.062133386731147766, -0.21934258937835693, 0.4069104492664337, -1.4230486154556274, 0.9546704888343811, -0.29112178087234497, -0.08413544297218323, -0.1502847820520401, -0.09040282666683197, -0.2355644255876541, -0.5577049851417542, 0.36547476053237915, -0.8018213510513306, -0.09035926312208176, 0.31056439876556396, -0.2029966562986374, 1.5516173839569092, -0.4304622709751129, 0.7195309400558472, 0.15736477077007294, -0.45697128772735596, 0.3625074326992035, -0.02611725963652134, -0.6537248492240906, -0.7090745568275452, 0.7284613847732544, 0.5493241548538208, -0.06058885157108307, 0.36370304226875305, 0.8257584571838379, 1.4511855840682983, -0.44376325607299805, -0.29818975925445557, 0.7206355333328247, -0.20795777440071106, 0.3697306215763092, 0.08415331691503525, 0.6227112412452698, -0.004560740198940039, 0.5809455513954163, -0.5620466470718384, 0.46700552105903625, -0.8195211887359619, -0.27474477887153625, 0.6929484605789185, 0.5812397003173828, 0.6130353212356567, 0.005221609957516193, -1.505308985710144, -0.38352513313293457, 0.4294297695159912, 0.46627944707870483, 1.35891854763031, -0.16583503782749176, 0.22893664240837097, -0.5438093543052673, -0.11966092884540558, -0.264707088470459, -0.2450682669878006, -0.2972809970378876, 0.006592520512640476, -0.7252907156944275, -0.6950126886367798, 0.8920403122901917, 0.4567471146583557, 1.2345582246780396, -1.2403525114059448, -1.3035874366760254, -0.45959562063217163, 0.9487274885177612, -0.528497576713562, -0.6502199172973633, 0.6604883670806885, -0.5903617739677429, -0.06103852391242981, 0.2671249210834503, -0.14894051849842072, 0.15086759626865387, -0.4546792507171631, 0.781003475189209, -0.43769606947898865, -0.4554804563522339, -0.1371372640132904, 1.1996955871582031, -0.6853719353675842, -0.058444079011678696, 0.6250465512275696, 0.00897387694567442, -0.19630850851535797, 0.14298762381076813, -0.12201489508152008, -0.508298397064209, -0.3331051766872406, -0.3668749928474426, -0.3295348584651947, 0.4038034975528717, 0.22961948812007904, 1.078481912612915, -0.6961386203765869, 0.1268441379070282, -0.9425459504127502, 0.8460271954536438, 0.2182580679655075, -0.5610721707344055, -0.2794744372367859, -0.4944409728050232, 0.09627877920866013, 0.9472451210021973, -0.7804493308067322, 0.030975598841905594, -0.5600832104682922, 0.3081219792366028, -0.8770965337753296, -0.3816344738006592, -0.0755128264427185, 0.754088819026947, -0.6126152276992798, 0.78807532787323, 0.1380579024553299, -0.030544839799404144, 0.33384835720062256, 0.2269514799118042, -0.8390225768089294, 0.9334061741828918, 0.3287014067173004, -0.18552911281585693, 0.010048896074295044, 0.351270854473114, -0.8936309814453125, -0.5011672973632812, -0.12034359574317932, -0.10498394817113876, -0.41621237993240356, 0.13615386188030243, -0.4111124873161316, -1.1612496376037598, -0.35278940200805664, -1.0742814540863037, -0.10557067394256592, 0.06797458231449127, -0.18398906290531158, -0.24127480387687683, -1.3482438325881958, -0.9782115817070007, -0.6394723653793335, -1.3490265607833862, -1.2814980745315552, 0.1515997350215912, 0.5153526067733765, -0.5875114798545837, -0.4568115174770355, -0.2571078836917877, -0.7757205367088318, 1.5268617868423462, -0.2284453958272934, 0.5019434094429016, -0.05057768151164055, -0.13230909407138824, -0.22753296792507172, 0.07437809556722641, 0.12004617601633072, -0.9048776030540466, 0.11612091958522797, -0.9960210919380188, 0.5184708833694458, -0.0820264145731926, -0.6730964779853821, 0.44401291012763977, 0.5528984069824219, 1.1052367687225342, -0.4335907995700836, -0.22440199553966522, 0.6419867277145386, 1.34830641746521, -0.4188913404941559, 0.18694882094860077, 0.5615243911743164, 0.8763915300369263, -0.33360356092453003, -0.43359822034835815, 0.4875876307487488, -0.17352433502674103, 0.48048871755599976, 0.48245686292648315, 0.0946832075715065, -0.45072755217552185, -0.30605778098106384, -0.00043018211727030575, 1.271544337272644, 0.21077485382556915, 0.2390957921743393, -0.737850546836853, 0.183166041970253, -0.9730916619300842, -0.7873759269714355, 0.717138409614563, 0.7319205403327942, 0.33211642503738403, 0.15893138945102692, -0.0721193328499794, -0.05319420248270035, 0.4055716097354889, 0.2928732633590698, -0.7426634430885315, -1.2711507081985474, 0.01947905495762825, 0.5181371569633484, 0.3614421784877777, 0.47643929719924927, -0.4369730055332184, 0.32529500126838684, 14.666910171508789, 0.826298713684082, -0.4733685851097107, 0.18618512153625488, 0.6860297918319702, 0.2693285048007965, -0.0429329052567482, -0.23409561812877655, -1.3355519771575928, -0.04627517983317375, 1.2405598163604736, 0.4360591769218445, 0.8445408940315247, 0.7346908450126648, -0.8125109076499939, 0.35663238167762756, -0.34437325596809387, 0.8498744964599609, 0.47977107763290405, -1.7268178462982178, 0.22939692437648773, 0.04852239415049553, 0.42453402280807495, 0.550676703453064, 0.9189231991767883, 0.6600615978240967, 0.1121862530708313, -0.2749654948711395, 0.46508869528770447, 0.425778329372406, 0.9688440561294556, -0.09123159199953079, 0.2698008418083191, 0.21243086457252502, -1.0378724336624146, -0.3312467336654663, -0.5543006658554077, -1.5811113119125366, -0.2837788760662079, -0.07080085575580597, -0.17061544954776764, -0.8135799169540405, 0.12242788821458817, 0.6848005056381226, 0.09018021821975708, 0.6791965365409851, -0.2177833914756775, -0.03519956395030022, -0.3292404115200043, -0.3022727966308594, 0.31806543469429016, 0.8401244878768921, 0.13159304857254028, 0.021845199167728424, -0.13055379688739777, -0.001184628577902913, 0.13437911868095398, 0.7072457671165466, -0.5436738133430481, -0.3816484808921814, -0.37925225496292114, -0.30507662892341614, 0.19751937687397003, 1.1157467365264893, 0.27024441957473755, 0.46558427810668945, 0.0029236457776278257, -0.09721488505601883, 0.7507525086402893, -0.13722412288188934, -0.26877135038375854, -0.7746472358703613, -0.10271000117063522, -0.7923870086669922, 0.1723610758781433, 0.5042874813079834, -0.818381667137146, -0.3305468261241913, -0.836977481842041, -0.7181148529052734, 0.28271666169166565, -0.8531799912452698, -0.568737804889679, 1.259352684020996, -0.5417801141738892, -0.08178684115409851, 0.5592635273933411, -0.8897890448570251, -0.3927372694015503, 0.18711423873901367, -1.0612350702285767, -0.4045029580593109, 0.33108338713645935, -0.2032119184732437, -0.28703102469444275, -0.22032365202903748, 1.1502784490585327, -0.03237157315015793, -0.30431678891181946, 0.22619853913784027, -0.6168662905693054, -0.4624389111995697, -0.38529282808303833, -0.32227447628974915, 0.8553805947303772, 0.2729308605194092, -0.26297423243522644, 0.15750035643577576, 0.13948307931423187, 0.2965547740459442, -1.1945747137069702, -0.034048836678266525, 0.21196773648262024, -0.2916814386844635, 0.035100825130939484, -0.637421190738678, -0.9160389304161072, 0.24424798786640167, 0.6237956881523132, -0.02758220210671425, -0.08918681740760803, 0.10054561495780945, -0.8553352355957031, -0.3508091866970062, -0.6045640707015991, 0.09129834175109863, 0.2564311623573303, -0.646723210811615, -0.430595338344574, -0.3752954602241516, 0.24346482753753662, -1.2965883016586304, -0.3306582570075989, -0.14094573259353638, 0.46337223052978516, -0.5118146538734436, 1.361402988433838, -0.21533383429050446, 0.7653247117996216, 0.779105544090271, 0.1190614178776741, -0.07813373953104019, -0.28925254940986633, -0.6015247702598572, -0.13838334381580353, -0.40625184774398804, 0.33804935216903687, -0.48889675736427307, 0.5445931553840637, 0.6358867883682251, -0.12068268656730652, -0.5045328736305237, -0.8011350631713867, -0.10271696746349335, -0.40456128120422363, -0.3753418028354645, 0.2983739972114563, -0.37268227338790894, 0.05023721233010292, 0.4092935621738434, 0.2858031094074249, 0.35814520716667175, -0.2818877696990967, -0.20648731291294098, 0.1116679385304451, -0.101150281727314, -0.36164215207099915, -0.9758719801902771, -0.6273455023765564, -1.3586972951889038, -0.3346945345401764, -1.140573501586914, -0.23107309639453888, -0.10947583615779877, -0.7741731405258179, 0.04237876459956169, -0.25173643231391907, 0.37814903259277344, 0.5740524530410767, -0.0835253894329071, -0.33554548025131226, -0.22482502460479736, -0.5849417448043823, 0.6970353722572327, 0.6011975407600403, -0.780733585357666, -0.2679578959941864, -0.13336913287639618, 0.07303769141435623, 0.8021072745323181, 0.4943232834339142, -0.3669664263725281, -0.27301546931266785, -1.3507134914398193, 0.49460333585739136, 0.05953052639961243, -0.12987327575683594, -1.6673088073730469, 1.2742106914520264, 0.43924078345298767, 0.1340816169977188, 0.23658473789691925, 0.2204626500606537, -1.1402400732040405, -0.6600795388221741, 0.725043773651123, -0.8603515625, 0.6185190081596375, 0.9011335968971252, -0.6038699150085449, -0.039754271507263184, 0.712136447429657, -0.041245236992836, -0.5916492342948914, -0.7631125450134277, 0.320155531167984, -0.49193060398101807, 0.23773843050003052, -0.44235336780548096, -0.07970837503671646, -1.3000719547271729, 0.1925678551197052, 0.13686616718769073, 0.142428919672966, -0.570824384689331, 0.5270339846611023, 0.46148622035980225, -1.0641815662384033, 0.5815671682357788, 0.7771090865135193, -0.5187146067619324, 0.09882798045873642, 0.5711861848831177, 0.5354964137077332, -0.3384614884853363, 0.6492213010787964, -0.4955183267593384, 0.24813029170036316, -0.46788203716278076, 0.3689381778240204, 0.646719753742218, -0.34270188212394714, -0.2303769588470459, 1.0349279642105103, -0.2243049442768097, -0.4650347828865051, 0.5178305506706238, -1.1561306715011597, -0.6977161765098572, -0.5284360647201538, 0.6990150213241577, 0.18463920056819916, -0.1336304098367691, 0.1330454796552658, -0.5386255979537964, 0.28359395265579224, -0.04330874979496002, 0.11729475110769272, 0.2757691740989685, 0.4346652925014496, -0.12001506984233856, 0.1806558221578598, 0.3773213028907776, -0.555103063583374, -0.6086326241493225, -0.5487054586410522, -0.5747746229171753, 0.059087157249450684, 0.5077598690986633, -0.14666569232940674, -1.0910890102386475, 1.080533742904663, 0.572320818901062, 0.27896377444267273, 0.4381793737411499, -0.39152467250823975, -0.06588265299797058, 0.8192606568336487, 0.06891781091690063, -0.1998981088399887, -0.19914168119430542, 1.5347812175750732, 1.4701210260391235, -0.3890014886856079, 0.5788943767547607, -0.37608397006988525, -0.5481792688369751, 1.3547513484954834, 0.715469241142273, -0.8231921792030334, 0.5276634097099304, -0.029982563108205795, -0.5665391683578491, -0.09799729287624359, -0.6426670551300049, -0.1238192617893219, 0.7444604635238647, 0.5993749499320984, 0.389527827501297, -0.043281346559524536, 0.13171683251857758, 0.9376093149185181, -0.02666088566184044, 0.16566264629364014, 0.3098349869251251, 0.44343799352645874, 0.030250277370214462, 0.5318861603736877, -0.05728771165013313, 0.7256300449371338, -0.6517238020896912, -0.8029661178588867, 0.5099592208862305, 0.6799095273017883, 0.20783860981464386, 0.11810231953859329, 0.9243434071540833, -0.09101369231939316, 0.6371080875396729, -0.3635716438293457, 0.6149285435676575, -0.04128660634160042, -0.778782069683075, 0.017151184380054474, -0.9067375659942627, -0.3065088987350464, -0.16980168223381042, -0.10615722090005875, -0.45797038078308105, 0.0054486701264977455, 0.255177766084671, -0.0773255005478859, 0.27552491426467896, 0.6460976600646973, 1.0225967168807983, 0.9436949491500854, -0.6059737205505371, -1.4070155620574951, -0.05834461748600006, -0.7308046817779541, 0.3713049590587616, -0.873978316783905, -0.29972249269485474, -0.32322829961776733, -0.26028069853782654, -0.5826017260551453]}, "authors": [{"authorId": "10001427", "name": "Hongxiang Fan"}, {"authorId": "2055625574", "name": "Thomas C. P. Chau"}, {"authorId": "2115955596", "name": "Stylianos I. Venieris"}, {"authorId": "153591987", "name": "Royson Lee"}, {"authorId": "3399274", "name": "Alexandros Kouris"}, {"authorId": "144708627", "name": "W. Luk"}, {"authorId": "2059229468", "name": "N. Lane"}, {"authorId": "2426713", "name": "M. Abdelfattah"}], "references": [{"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "5a71bf38cf409b55b14b2d5159c0b06bef9ad603", "title": "A General Survey on Attention Mechanisms in Deep Learning"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "c67b1a62b868a758791c88d5465c7b6d53510fc3", "title": "Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "bd2d1ec40a366294b1fd969dc3abe323511984d5", "title": "Sparsity-Aware and Re-configurable NPU Architecture for Samsung Flagship Mobile SoC"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "3af8a493cf756f9fe72623204a11e378a9cd71a5", "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "7c6c31412c5dad22543bb71e31620e8868d644a3", "title": "FTRANS: energy-efficient acceleration of transformers using FPGA"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "a68c3412e60560290400d2707596f82a914b7c00", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "ee19253f28d66d2764fb91ad74bd81733bf25257", "title": "AWB-GCN: A Graph Convolutional Network Accelerator with Runtime Workload Rebalancing"}, {"paperId": "62dc8ddb4907db4b889c5e93673d9b3c189d1f25", "title": "A Tensorized Transformer for Language Modeling"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "cc14e2e99ef12b01ecb1e869b46b9eb50e2179bd", "title": "HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array"}, {"paperId": "8f441ee1b62c915c689aca43ec48011fdd05daec", "title": "E-RNN: Design Optimization for Efficient Recurrent Neural Networks in FPGAs"}, {"paperId": "50a770f13a1ea66689c950c5a5a7ebb030ad098c", "title": "Accelerating Convolutional Neural Network With FFT on Embedded Hardware"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "b93dbfbaae539ba3874329ac2b91759259489283", "title": "C-LSTM: Enabling Efficient LSTM using Structured Compression Techniques on FPGAs"}, {"paperId": "bce22675d77e1ef28e92f3793c02f8f5ccdb0ddd", "title": "A Two-pronged Progress in Structured Dense Matrix Vector Multiplication"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "52196168be083c56eb1ef762309dffa5ebd95e4b", "title": "Frequency Domain Acceleration of Convolutional Neural Networks on CPU-FPGA Shared Memory System"}, {"paperId": "72ed74f00d0f7312f7ed96d93ed43f0052d526bc", "title": "Fused-layer CNN accelerators"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "24251f02c34f32b1dd96572a1d984c4463a26a10", "title": "Validity of the single processor approach to achieving large scale computing capabilities"}, {"paperId": "0e6beb95b5150ce99b108acdefabf70ccd3fee30", "title": "An algorithm for the machine calculation of complex Fourier series"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Random Butterfly Transformations with Applications in Computational Linear Algebra"}, {"paperId": null, "title": "As the vanilla FNet on Retrieval task suffers significant accuracy loss, we increase its hidden size to"}]}