{"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers", "abstract": "Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1", "venue": "Transactions of the Association for Computational Linguistics", "year": 2020, "citationCount": 480, "influentialCitationCount": 45, "openAccessPdf": {"url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00353/1923932/tacl_a_00353.pdf", "status": "GOLD"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers."}, "embedding": {"model": "specter_v2", "vector": [0.5507606267929077, 0.5445908308029175, -0.40265101194381714, 0.1968849003314972, -0.2857408821582794, 0.030358627438545227, 0.48780718445777893, -0.041087280958890915, -0.008917147293686867, -0.3796646296977997, 0.89711993932724, 0.4927480220794678, 0.4987184405326843, -0.07648574560880661, -0.1970953792333603, 0.3435724973678589, -0.8784429430961609, 0.09756806492805481, 0.3815440237522125, -0.4605315029621124, 0.39190638065338135, -1.0130906105041504, -1.3917351961135864, 0.3994722068309784, 0.3611564636230469, 0.8094075918197632, 0.5017131567001343, 0.7313938140869141, -0.7203786373138428, 0.6328244805335999, 0.43664222955703735, -0.03071058727800846, 0.20784904062747955, -0.34551265835762024, -0.524687647819519, 0.04619275778532028, 0.6467020511627197, 0.17241902649402618, -0.5787194967269897, 0.7428022027015686, -0.16051806509494781, 0.37641844153404236, 0.38099101185798645, -0.42709389328956604, -0.3149288296699524, 0.8915676474571228, 0.545165479183197, 1.0007030963897705, 0.18560341000556946, -0.7543581128120422, 1.574519395828247, -1.274832844734192, 0.5593546032905579, 1.389847993850708, 0.18265993893146515, 0.5051809549331665, 0.08109816908836365, -0.596471905708313, 1.0926681756973267, 0.6140177249908447, -0.7438513040542603, -0.46529871225357056, -0.03644494712352753, -0.35217711329460144, 1.8023262023925781, -0.36647507548332214, 0.2386605441570282, 0.5762843489646912, -0.18147467076778412, 1.3432742357254028, -0.3800369203090668, -0.6098964214324951, -0.36124634742736816, 0.014918258413672447, -0.08656585216522217, 0.9773994088172913, -0.6950038075447083, 0.14114561676979065, -1.0058040618896484, 0.08318083733320236, 0.5950102806091309, -0.08546093851327896, 0.2096835970878601, -0.5463394522666931, -0.2065829336643219, 0.7353653311729431, 0.24437154829502106, 1.1078753471374512, -0.3046726584434509, 0.7603083252906799, 0.5586809515953064, 0.07620731741189957, 0.052860673516988754, 0.402026504278183, 0.1733129322528839, 0.1955566257238388, -1.0159484148025513, 0.23267658054828644, -0.07515780627727509, 1.3223572969436646, -0.01759777031838894, 0.5783281326293945, -0.6445291042327881, 0.2551570534706116, 1.192075490951538, -0.008312384597957134, 0.9524133205413818, -0.6374139785766602, -0.07369133085012436, -0.7819275856018066, -0.12534639239311218, -1.3920090198516846, 0.059448666870594025, -0.26980286836624146, -0.7648148536682129, -1.2692842483520508, -0.6234318614006042, 0.4399896562099457, -0.5646164417266846, 0.6646856665611267, -0.2672407925128937, 0.3551267981529236, -0.4000081717967987, 0.27399277687072754, 0.623863697052002, 0.5620384812355042, 0.25969552993774414, 0.2534351646900177, 1.1217139959335327, -1.017613172531128, -0.6192331910133362, -0.9813008904457092, 0.446207195520401, -0.31347131729125977, 0.13420386612415314, -0.23314690589904785, -1.2651331424713135, -1.0083833932876587, -0.4290948510169983, 0.18650801479816437, -0.4273514747619629, 0.07730389386415482, 0.7533296346664429, 0.16998697817325592, -1.0506240129470825, 0.9547985196113586, -0.10973566770553589, -0.6432551741600037, 0.8009791970252991, -0.3651641309261322, 0.13824819028377533, -0.15561577677726746, -1.3651008605957031, 0.3542760908603668, -0.22879725694656372, -0.6929173469543457, -0.4996914565563202, -0.8960778117179871, -1.4930119514465332, 0.027432821691036224, 0.48646241426467896, -0.26048004627227783, 0.9884456992149353, -0.6130159497261047, -1.1619007587432861, 0.7480086088180542, -0.7854584455490112, -0.3318987786769867, 0.18745140731334686, -0.29612448811531067, -0.39623063802719116, -0.16217195987701416, 0.3157971203327179, 0.3937256336212158, 0.782646119594574, -0.10103404521942139, -0.27952903509140015, 0.23584523797035217, -0.5026764273643494, -0.10122287273406982, -0.2603365182876587, 0.9300673007965088, -0.5040234327316284, -0.46213048696517944, 0.1260751187801361, 0.7493276000022888, -0.1285354197025299, -0.3641694486141205, -0.23205652832984924, -1.026458740234375, 0.8556089401245117, 0.13622543215751648, 0.8432457447052002, -0.8529019951820374, -0.7093310356140137, -0.1629541665315628, -0.186690092086792, -0.22650481760501862, -1.059394121170044, 0.6891354322433472, -0.6413833498954773, 0.3061095178127289, -0.2599852681159973, -0.8670667409896851, -0.10531903803348541, -0.4624227285385132, -0.7484399080276489, -0.17481043934822083, 0.17137649655342102, 0.9900423288345337, -1.1484572887420654, -0.2321082502603531, -0.10780857503414154, 0.32639601826667786, -0.9986346364021301, 1.1688709259033203, -0.636342465877533, -0.035643789917230606, -0.18082024157047272, -0.2739468216896057, -0.25791293382644653, -0.5348902344703674, 0.2848984897136688, -0.45635414123535156, -0.15551620721817017, 0.4861934185028076, -0.15772107243537903, 1.5108994245529175, -0.2806585133075714, 0.4462748169898987, -0.34929531812667847, -0.9458122849464417, 0.5055562257766724, 0.26405128836631775, 0.20017924904823303, -0.7053086161613464, -0.004219862632453442, -0.19194522500038147, -0.8369054198265076, -0.03332536295056343, 0.6372595429420471, 1.0551761388778687, -0.1036323830485344, 0.1735343188047409, 0.6376151442527771, -0.11129786819219589, 0.6170845627784729, 0.8880637288093567, 0.9655529260635376, 0.5807106494903564, 0.4825461804866791, -0.20123744010925293, 0.2624276876449585, -0.8371869921684265, 0.035649437457323074, 0.6692769527435303, 0.7515740990638733, 1.0817630290985107, 0.3024913966655731, -0.8435770869255066, -0.5966079831123352, 0.4488071799278259, 0.9434567093849182, 1.479079246520996, -0.10069778561592102, -0.3980914056301117, -0.5862559080123901, -0.293540358543396, -0.30291926860809326, -0.05015679821372032, -0.859161376953125, -0.2205711156129837, -0.40508803725242615, -0.648608386516571, 0.23559744656085968, 0.22861304879188538, 0.747845470905304, -1.0089138746261597, -0.5581488609313965, 0.05948001146316528, 0.05626766383647919, -0.8661416172981262, -0.8144124746322632, 0.07189610600471497, -0.17956550419330597, -0.16360537707805634, -0.12114685028791428, -0.11307800561189651, 0.1799478828907013, -0.37018314003944397, 1.1386687755584717, -0.4160873591899872, -0.5228747129440308, 0.35371237993240356, -0.022497985512018204, -0.6983675956726074, -0.264498770236969, 0.34913989901542664, 0.0474107563495636, 0.14617930352687836, 0.3330128788948059, 0.22857490181922913, -0.2561780512332916, 0.04069703444838524, -0.26021242141723633, -0.10624130070209503, -0.03881124034523964, -0.17018644511699677, 0.4687052369117737, -0.19394826889038086, -0.0014706889633089304, -1.2316381931304932, 0.18533147871494293, -0.3477391004562378, -0.3969828486442566, -0.2849273085594177, -0.413710355758667, -0.6726624965667725, 0.3398784101009369, -0.557842493057251, -0.2976216673851013, -0.5318601131439209, 0.3963979184627533, -0.4363211393356323, -0.38143399357795715, 0.14096395671367645, 0.2906741499900818, 0.1174505427479744, 0.24962396919727325, 0.7824701070785522, 0.4597433805465698, 0.08113094419240952, 0.44093644618988037, -1.2963626384735107, 0.5298956632614136, 0.16993355751037598, 0.26001009345054626, 0.31517520546913147, -0.2953766882419586, -0.96742844581604, -0.7205291390419006, -0.5837863087654114, -0.16539612412452698, -0.30304235219955444, 0.26036182045936584, -0.5901588201522827, -1.0572359561920166, -0.23728355765342712, -0.9916790127754211, -0.43283775448799133, -0.1932251900434494, -0.6963071227073669, -0.5567077994346619, -0.6156153678894043, -0.7053245902061462, -0.688860297203064, -0.6313316822052002, -0.673270046710968, 0.13292494416236877, 0.020839210599660873, -0.6263608932495117, -0.5188509821891785, 0.047390416264534, -0.5610507726669312, 1.390990138053894, -0.5135207176208496, 0.32314127683639526, -0.15387895703315735, -0.5595961809158325, -0.25835204124450684, 0.12080265581607819, 0.12725847959518433, 0.0895431637763977, -0.027247074991464615, -0.5033794641494751, 0.3191618025302887, -0.4533228874206543, -0.03046853095293045, 0.260517418384552, 0.5632681250572205, 0.961947500705719, -0.007907014340162277, -0.634370744228363, 0.32391321659088135, 1.5509395599365234, -0.4882696568965912, 0.20524735748767853, 0.30047374963760376, 1.081601619720459, 0.40625324845314026, -0.18131275475025177, 0.8189131617546082, 0.8701836466789246, 0.40655285120010376, 0.4035249948501587, -0.1821393370628357, -0.19566230475902557, -0.5779770016670227, 0.40899041295051575, 1.457898736000061, 0.42524415254592896, 0.1776648610830307, -0.8058703541755676, 1.1204721927642822, -1.2885106801986694, -1.3432174921035767, 0.8915538191795349, 0.4195343255996704, 0.34112119674682617, -0.7825710773468018, 0.02284768410027027, -0.5294668078422546, 0.3687390983104706, 0.4368542432785034, -0.44348064064979553, -0.4264894127845764, -0.10219164937734604, 0.44277632236480713, 0.13365259766578674, 0.576126217842102, -0.47246259450912476, 0.773335874080658, 14.906332015991211, 0.6847999691963196, 0.012148812413215637, 0.4618571102619171, 0.6477315425872803, 0.026598304510116577, -0.3213721215724945, -0.08634433895349503, -1.3365511894226074, 0.06774354726076126, 0.8802924752235413, 0.025542842224240303, 0.4355068802833557, 0.14232122898101807, 0.25511643290519714, 0.17509140074253082, -0.418586403131485, 0.5515604019165039, 0.9687265157699585, -1.497501254081726, 0.42677822709083557, -0.06631788611412048, 0.24512244760990143, 0.2004147469997406, 0.8325138092041016, 0.4813358187675476, 0.6414840817451477, -0.3995286822319031, 0.5318371653556824, 0.5998696684837341, 1.100192904472351, 0.3197123408317566, -0.033385343849658966, 0.1781434416770935, -1.0713834762573242, -0.1907106339931488, -0.7305229902267456, -0.908674418926239, 0.4556553065776825, 0.15624676644802094, -0.2123422920703888, -0.3678268790245056, 0.20414364337921143, 0.9875025153160095, 0.25842586159706116, 0.539151132106781, -0.05264090374112129, 0.4479699730873108, 0.3449530303478241, -0.04594450443983078, 0.3637966811656952, 0.6514028310775757, 0.2544642686843872, 0.4690461754798889, 0.34529033303260803, 0.29160115122795105, 0.09868554770946503, 0.42605507373809814, -0.1606092005968094, 0.1722942739725113, -0.4769797921180725, -0.05240059271454811, -0.01823314279317856, 0.8454462289810181, 0.5432896614074707, -0.044430047273635864, -0.4238460958003998, 0.34616217017173767, 0.40831926465034485, -0.02979123219847679, -0.15681706368923187, -0.12737859785556793, 0.3029731214046478, 0.008971098810434341, 0.6000800132751465, 0.6733853220939636, -0.0017696653958410025, -0.5490108132362366, -1.2125266790390015, -0.6418912410736084, 0.5759047269821167, -0.8978297710418701, -0.8384155631065369, 1.066168189048767, -0.5253497362136841, -0.33401206135749817, 0.23560437560081482, -0.3649939298629761, -0.3120236396789551, 0.2874751091003418, -1.3103516101837158, -0.28827184438705444, 0.06084100902080536, -0.3320644199848175, 0.00951619166880846, -0.37497708201408386, 1.0557304620742798, 0.07811697572469711, -0.004292852710932493, 0.05824483558535576, -0.33524635434150696, -0.15479640662670135, -0.25859585404396057, -0.8797329068183899, 0.8881546258926392, 0.2525678277015686, -0.11873259395360947, 0.2508198916912079, 0.14220337569713593, 0.03112279623746872, -0.6788719892501831, -0.33110880851745605, 0.7357138991355896, -0.7304244637489319, -0.2876236140727997, -0.6547120213508606, -1.13724684715271, 0.19885723292827606, 0.6442723870277405, -0.06681514531373978, 0.3587232530117035, -0.06835287809371948, -0.5956345200538635, -0.045683711767196655, -0.6548798680305481, 0.00956566259264946, 0.5838674306869507, -0.508292555809021, -0.13402120769023895, -0.06128934398293495, 0.5526259541511536, -0.8420099020004272, -0.25412872433662415, -0.4177289307117462, 0.23835599422454834, -0.1416991502046585, 1.0654656887054443, -0.33997151255607605, 0.9513252377510071, 0.8757190108299255, 0.16657865047454834, -0.7483837008476257, -0.5909543633460999, -1.172579050064087, -0.04577005282044411, 0.25055673718452454, 0.41288456320762634, -0.06361702084541321, 0.6671925783157349, 0.40965211391448975, 0.4379599392414093, -0.3812253773212433, -0.3481934666633606, -0.20863227546215057, -0.2523971199989319, -0.3988581895828247, 0.19957460463047028, 0.28191107511520386, 0.02892490103840828, 0.010287568904459476, 0.2373519092798233, 0.24403667449951172, 0.03020307794213295, -0.6213550567626953, 0.31553593277931213, -0.03962273523211479, 0.2422276884317398, -0.5889941453933716, -0.3734418749809265, -1.2524874210357666, 0.10115794837474823, -1.0071470737457275, 0.11204636842012405, -1.0434808731079102, -0.43615520000457764, 0.24037256836891174, -0.16214238107204437, -0.023972079157829285, 0.41570430994033813, -0.26277726888656616, -0.2852937579154968, -0.8993595838546753, -0.8907420635223389, 0.7168195247650146, 0.889937698841095, -0.7209874987602234, 0.32291364669799805, -0.295214980840683, -0.3680998384952545, 0.23823519051074982, 0.5211819410324097, -0.4863685071468353, -0.536880612373352, -1.0454683303833008, 0.3822803199291229, -0.19429007172584534, -0.17062786221504211, -0.6741712689399719, 0.7813996076583862, 0.4126970171928406, -0.17203287780284882, -0.20508719980716705, 0.26802459359169006, -0.885079026222229, -0.5280241966247559, 0.23744991421699524, -1.0090312957763672, 0.2028956115245819, 0.14916078746318817, -0.38094714283943176, -0.36216041445732117, 0.6942926645278931, 0.08015947788953781, -1.688559651374817, -0.8905720114707947, 0.8704620599746704, -0.7632054090499878, 0.43024900555610657, -0.2974473536014557, -0.2848438024520874, -0.8796312808990479, -0.8373275399208069, -0.2275419682264328, 0.32704365253448486, -0.5442054867744446, 1.1942241191864014, 0.6203208565711975, -1.3136366605758667, -0.01609920710325241, 0.385746031999588, 0.07435008883476257, 0.011854022741317749, 0.8946906328201294, 0.09984847903251648, 0.015185587108135223, 0.5011777877807617, 0.23180800676345825, 0.3911595344543457, -0.7450079917907715, 0.3378261923789978, 0.7193677425384521, -0.23814652860164642, -0.19561193883419037, 1.0342895984649658, -0.10634580254554749, -0.6120279431343079, 0.1281554102897644, -0.6651784777641296, -0.9157565832138062, 0.11087453365325928, 0.7919107675552368, 0.1554538607597351, -0.40855106711387634, -0.34893497824668884, -0.60106360912323, 0.47322016954421997, -0.18772144615650177, -0.3568539321422577, 0.761498749256134, -0.1623244285583496, -0.5061242580413818, 0.5693597793579102, 0.8653448820114136, -0.803843080997467, -0.5151264071464539, -1.0919691324234009, -0.34633708000183105, -0.291868656873703, -0.0095622343942523, -0.15111252665519714, -0.6982017755508423, 0.767465353012085, 0.42525002360343933, 0.6919538378715515, -0.11139349639415741, 0.00323040084913373, 0.2363756150007248, 0.27834004163742065, -0.20543581247329712, -0.5672850012779236, -0.12172404676675797, 1.502701759338379, 1.5668070316314697, -0.5232395529747009, 0.026658594608306885, -0.3625607490539551, -0.7382373809814453, 0.5914698839187622, 0.3499600887298584, -0.31614258885383606, 0.850854754447937, -0.22694946825504303, 0.002712469082325697, -0.015066749416291714, -1.254064679145813, -0.5647960305213928, 0.9598304033279419, 1.3311398029327393, 0.7103850841522217, -0.18195445835590363, 0.46918898820877075, 0.4938046634197235, 0.18010003864765167, 0.2062302976846695, 0.41232722997665405, 0.05072302743792534, -0.3503342270851135, 0.20206275582313538, 0.25528624653816223, 0.634693443775177, -0.7915320992469788, -0.7098908424377441, 0.46260854601860046, 0.10830041766166687, 0.038583915680646896, 0.3724856376647949, 1.249398946762085, 0.19990389049053192, 0.4923904836177826, -0.10048277676105499, 0.27361929416656494, -0.8774104714393616, -0.05496452748775482, -0.14407628774642944, -0.9508798718452454, -0.5018798112869263, -0.3779527246952057, -1.019640564918518, -0.12122317403554916, 0.16946430504322052, 0.46321651339530945, 0.06304798275232315, 0.07711835950613022, 0.8958501219749451, 0.9572736620903015, 0.5922946929931641, -0.1543474942445755, -0.4133073091506958, 0.01681823842227459, -0.9009197950363159, 0.1373203694820404, -0.4079780578613281, 0.0005145787145011127, -0.24638602137565613, -0.060079917311668396, -0.0008781647775322199]}, "authors": [{"authorId": "39788470", "name": "Aurko Roy"}, {"authorId": "2814161", "name": "M. Saffar"}, {"authorId": "40348417", "name": "Ashish Vaswani"}, {"authorId": "2529182", "name": "David Grangier"}], "references": [{"paperId": "3df83a60f55c64b40e6dbcd99cf9f67894a0736e", "title": "Do Transformers Need Deep Long-Range Memory?"}, {"paperId": "70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328", "title": "Sparse GPU Kernels for Deep Learning"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "bf442ab269074665a68e4dbbe19e4efc97862541", "title": "Large Memory Layers with Product Keys"}, {"paperId": "7d67237398986a6088c696df0bf57646c714508f", "title": "Look Harder: A Neural Machine Translation Model with Hard Attention"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "36d3a18a1519e27f7c9c8479b19fc00d4d805a00", "title": "Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling"}, {"paperId": "fb507ada871d1e8c29e376dbf7b7879689aa89f9", "title": "Music Transformer: Generating Music with Long-Term Structure"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "21b786b3f870fc7fa247c143aa41de88b1fc6141", "title": "Glow: Generative Flow with Invertible 1x1 Convolutions"}, {"paperId": "c9552f9e2a7a7656c4c9ef9569a824dffa1fd181", "title": "Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms"}, {"paperId": "520ddb38b59b8fae2209ddc7c6640462cf153eec", "title": "Sparse and Constrained Attention for Neural Machine Translation"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "680aafd3d51e666b297e27b93d9554cc2caf1c4d", "title": "An Analysis of Neural Language Modeling at Multiple Scales"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "d1c424c261c577958917055f72fb9e2ad0348865", "title": "PixelSNAIL: An Improved Autoregressive Generative Model"}, {"paperId": "f0afdccf2903039d202085a771953a171dfd57b1", "title": "Monotonic Chunkwise Attention"}, {"paperId": "252571243aa4c0b533aa7fc63f88d07fd844e7bb", "title": "Learning What\u2019s Easy: Fully Differentiable Neural Easy-First Taggers"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73", "title": "Improving Neural Language Models with a Continuous Cache"}, {"paperId": "be8c6c69f3e357bfad2987e45b62cff7e7474378", "title": "Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "0936352b78a52bc5d2b5e3f04233efc56664af51", "title": "Conditional Image Generation with PixelCNN Decoders"}, {"paperId": "7fe83e1a713ccb5ba19bce9ca933f958843916bb", "title": "A Neural Transducer"}, {"paperId": "a418524a3576afff4dc2178ed169e692915bd46b", "title": "An Online Sequence-to-Sequence Model Using Partial Conditioning"}, {"paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation"}, {"paperId": "a2e2970dd99b86fd6198c0b756c6cd4d52e34c3b", "title": "Clustering is Efficient for Approximate Maximum Inner Product Search"}, {"paperId": "b624504240fa52ab76167acfe3156150ca01cf3b", "title": "Attention-Based Models for Speech Recognition"}, {"paperId": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "title": "DRAW: A Recurrent Neural Network For Image Generation"}, {"paperId": "4d8f2d14af5991d4f0d050d22216825cac3157bd", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "8829e3873846c6bbad5aca111e64f9d2c1b24299", "title": "Deep Sequential Neural Network"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "d862bda57dd0a51a1bff42b0e5fbeaaaa0c0d14f", "title": "Balanced K-Means for Clustering"}, {"paperId": "c6f002b73ffe647ef0effd17c9bfbd5cd7adac7d", "title": "Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "44ddac48353ead135eef4096859956eaa31be2a5", "title": "Learning Factored Representations in a Deep Mixture of Experts"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "1b443db0552343e4d02ad24112dc0667a12335a7", "title": "Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Large text compression benchmark"}, {"paperId": "4c0d2d4269895eb367d7b0d38d9de3e99bf3f3ae", "title": "Sparse Nonnegative Matrix Factorization for Clustering"}, {"paperId": "4fc1bf60275c419eeb85e28793305a789dca4717", "title": "On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering"}, {"paperId": null, "title": "Content-Based Sparse Attention with Routing"}, {"paperId": "6fb07b90b7fd2785ffec0da1069e75c53f7313c2", "title": "Algorithms for Non-negative Matrix Factorization"}, {"paperId": "2352d9105de31032538900dfb2ce7c95f6402963", "title": "Convergence Properties of the K-Means Algorithms"}]}