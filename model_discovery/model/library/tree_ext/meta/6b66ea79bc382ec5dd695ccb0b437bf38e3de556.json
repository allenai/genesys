{"paperId": "6b66ea79bc382ec5dd695ccb0b437bf38e3de556", "title": "Do we need Label Regularization to Fine-tune Pre-trained Language Models?", "abstract": "Knowledge Distillation (KD) is a prominent neural model compression technique that heavily relies on teacher network predictions to guide the training of a student model. Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is evident that in KD, deploying the teacher network during training adds to the memory and computational requirements of training. In the computer vision literature, the necessity of the teacher network is put under scrutiny by showing that KD is a label regularization technique that can be replaced with lighter teacher-free variants such as the label-smoothing technique. However, to the best of our knowledge, this issue is not investigated in NLP. Therefore, this work concerns studying different label regularization techniques and whether we actually need them to improve the fine-tuning of smaller PLM networks on downstream tasks. In this regard, we did a comprehensive set of experiments on different PLMs such as BERT, RoBERTa, and GPT with more than 600 distinct trials and ran each configuration five times. This investigation led to a surprising observation that KD and other label regularization techniques do not play any meaningful role over regular fine-tuning when the student model is pre-trained. We further explore this phenomenon in different settings of NLP and computer vision tasks and demonstrate that pre-training itself acts as a kind of regularization, and additional label regularization is unnecessary.", "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "year": 2022, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://aclanthology.org/2023.eacl-main.13.pdf", "status": "HYBRID"}, "tldr": {"model": "tldr@v2.0.0", "text": "This investigation led to a surprising observation that KD and other label regularization techniques do not play any meaningful role over regular fine-tuning when the student model is pre-trained, and additional labelRegularization is unnecessary."}, "embedding": {"model": "specter_v2", "vector": [0.018847927451133728, 0.8243587613105774, -0.36868247389793396, -0.015728754922747612, -0.2332470864057541, -0.25752708315849304, 0.7467840909957886, -0.013102950528264046, -0.7599802613258362, -0.01711047999560833, 0.17265400290489197, -0.39887166023254395, 0.2729758322238922, 0.19417938590049744, -0.18440526723861694, -0.09054339677095413, -0.35674312710762024, 0.603473424911499, -0.10979138314723969, -0.30476346611976624, -0.35932862758636475, -0.6483706831932068, -0.817135214805603, -0.3468383252620697, 0.6028839349746704, 0.37247687578201294, 0.1165456771850586, 0.8004132509231567, -0.5324928164482117, 0.16805092990398407, 0.5804733037948608, -0.5829447507858276, 0.2632104158401489, 0.08238484710454941, -0.3096216022968292, -0.17337718605995178, 0.4200083315372467, -0.35781988501548767, -0.5613524913787842, 0.8479301333427429, -0.010385382920503616, 0.3706257939338684, 0.7822983860969543, -0.4865279495716095, -0.7788898348808289, 0.6922324299812317, 0.45981886982917786, 0.8509756922721863, -0.392833411693573, -0.42476245760917664, 1.1174501180648804, -1.3207969665527344, 0.39427095651626587, 1.3368946313858032, 0.5950413942337036, 0.531335711479187, -0.6080319881439209, -0.4247376620769501, 0.5013406872749329, 0.23103342950344086, -1.053921103477478, -0.11703106760978699, -0.10996489226818085, -0.26784032583236694, 1.7013477087020874, -0.5277966856956482, -0.4453025460243225, 0.33292484283447266, -0.637679398059845, 1.2273775339126587, 0.0721212849020958, -0.9585302472114563, -0.4099899232387543, 0.5352441668510437, 0.209588423371315, 1.1388014554977417, -0.5562273263931274, 0.7690305709838867, -0.6634309887886047, -0.31880173087120056, 0.33359086513519287, -0.48186883330345154, -0.15643443167209625, 0.0031434164848178625, 0.27942097187042236, 1.1278393268585205, 0.2180507332086563, 0.5903980135917664, 0.0049172681756317616, 0.7391717433929443, 0.4976380467414856, 0.4122236669063568, 0.3277861177921295, 0.5947225093841553, -0.42425185441970825, 0.30232641100883484, -0.8693270087242126, 0.08903072029352188, 0.24829375743865967, 0.6555866003036499, 0.21166031062602997, 0.3341885209083557, -0.9889510273933411, 0.49873486161231995, 1.3233180046081543, -0.27837327122688293, 0.5926957726478577, -0.9780780076980591, 0.20896875858306885, -0.7979686856269836, 0.05994970351457596, -0.6827341914176941, -0.10065994411706924, -0.16917970776557922, -0.8723291754722595, -1.1816810369491577, -0.17267882823944092, -0.3050447702407837, -1.0266766548156738, 0.7168933749198914, -0.3005703091621399, 0.5841118097305298, 0.20780105888843536, 0.600002110004425, 0.6483516693115234, 0.7223957777023315, 0.5444707274436951, -0.1942065805196762, 0.9350049495697021, -0.7966446876525879, -0.5232625007629395, -0.9076944589614868, 0.8318623304367065, -0.16078820824623108, 0.4762442708015442, -0.3017041087150574, -1.064964771270752, -1.0057876110076904, -0.635910153388977, -0.16231785714626312, -0.418995201587677, 0.6862917542457581, 0.9736087918281555, 0.3828241527080536, -0.7898074388504028, 0.99451744556427, 0.02610781230032444, -0.08037332445383072, 0.590627908706665, 0.05638612061738968, 0.27405601739883423, -0.45718157291412354, -1.2476344108581543, 0.6747866272926331, 0.5246441960334778, -0.6643391847610474, -0.38943514227867126, -0.6179075837135315, -0.799083411693573, 0.07530257105827332, 0.6228486895561218, -0.5984322428703308, 1.4398916959762573, 0.02098945714533329, -1.3296161890029907, 0.8402402400970459, -0.2869834899902344, -0.2869252562522888, 0.5573123693466187, -0.19425562024116516, -0.40266796946525574, -0.6017329096794128, -0.4523068368434906, 0.8588432669639587, 0.8545876741409302, -0.384988009929657, -0.041702061891555786, 0.6391281485557556, -0.11184379458427429, 0.03809170797467232, -0.02104889042675495, 0.3550359010696411, -0.37643277645111084, -0.29793405532836914, 0.24925793707370758, 0.43036556243896484, -0.01898929663002491, -0.15960891544818878, -0.2571289837360382, -1.327149748802185, 0.680077314376831, 0.08502736687660217, 0.8619351983070374, -0.9376955628395081, -0.6490758657455444, 0.005484115332365036, 0.04914232715964317, 0.3723781406879425, -0.9949004650115967, 0.3826747238636017, -0.1657724231481552, 0.5301671624183655, -0.20691119134426117, -1.1744084358215332, 0.32233601808547974, 0.05960768088698387, -0.5995727181434631, -0.27674031257629395, 0.12420886754989624, 0.9853919148445129, -0.7316830158233643, 0.04067070037126541, -0.12152819335460663, 0.5933569073677063, -1.0458217859268188, 1.042588472366333, -0.9309673309326172, 0.3136443495750427, 0.27100804448127747, -0.41610008478164673, 0.25683191418647766, -0.3570065200328827, 0.17969928681850433, -0.26989373564720154, 0.16510088741779327, 0.36377212405204773, -0.4852500557899475, 1.3737545013427734, -0.2584182620048523, 0.6274158954620361, -0.09288400411605835, -0.8190520405769348, 0.27326613664627075, 0.5176507830619812, -0.3915228843688965, -0.17565451562404633, 0.15726207196712494, 0.5873881578445435, -0.6789509057998657, 0.33530765771865845, 0.8387953639030457, 0.5156168341636658, 0.264091819524765, 0.3672386705875397, 0.48681604862213135, -0.6768259406089783, 0.7241765260696411, 0.2693842947483063, 0.4902598559856415, 0.19873884320259094, 0.19686198234558105, 0.2751889228820801, 0.5476903319358826, -0.6700004935264587, -0.29129427671432495, 0.44789162278175354, 0.8002492189407349, 0.6745720505714417, 0.33963507413864136, -0.48223623633384705, -0.1856352686882019, 0.059585243463516235, 0.5565388798713684, 1.904526710510254, -0.4510723948478699, -0.3731120228767395, -0.6096329689025879, -0.5975605845451355, -0.08559225499629974, 0.4238780736923218, -0.3430829346179962, -0.1440640389919281, -0.5205221772193909, -1.2511100769042969, 0.9416258335113525, 0.11604040861129761, 0.8448469638824463, -0.30547139048576355, 0.04762081801891327, -0.3162020444869995, 0.3157549202442169, -0.5360814332962036, -0.5665060877799988, 0.6787814497947693, -0.5085277557373047, -0.3623959422111511, -0.1429816484451294, -0.2153751403093338, 0.2851085662841797, -0.7054240107536316, 0.914412260055542, -0.1834624856710434, -0.28964585065841675, 0.2869483530521393, 0.4677600562572479, -0.73170405626297, -0.8397496938705444, 0.4419066309928894, 0.09310556948184967, -0.6194784641265869, 0.4160865843296051, 0.44488590955734253, 0.22365911304950714, -0.15048860013484955, -0.42797258496284485, -0.03866037353873253, -0.01510531548410654, 0.08912395685911179, 1.0378615856170654, 0.20022086799144745, 0.19794604182243347, -1.407509207725525, 1.2835888862609863, 0.10131648927927017, -0.6615789532661438, 0.18241339921951294, -1.199659824371338, -0.13915026187896729, 0.18125976622104645, -0.8810200691223145, -0.3574898838996887, -0.8323391079902649, 0.047283586114645004, -0.22159123420715332, -0.21439410746097565, 0.5288242101669312, 0.8619158864021301, 0.04182714596390724, 0.6089159250259399, 0.07797247916460037, 0.4276624321937561, -0.5033175349235535, 1.256661057472229, -1.282968521118164, 0.6569587588310242, 0.2016281634569168, 0.4127758741378784, -0.5706062912940979, -0.2585718333721161, -0.47542649507522583, -0.5347658395767212, -0.16496331989765167, -0.25849100947380066, 0.29750287532806396, 0.011073373258113861, -0.6389077305793762, -0.5822938680648804, -0.3327755928039551, -1.028067708015442, -0.2826003432273865, -0.33872658014297485, -0.14979681372642517, -0.32305482029914856, -0.9710865020751953, -1.3231970071792603, -0.29708194732666016, -0.3429862856864929, -0.7957693934440613, 0.10767194628715515, 0.16763629019260406, -0.18952925503253937, -0.7602856755256653, 0.09901463240385056, -0.47064459323883057, 1.267659306526184, -1.0048203468322754, 1.3538235425949097, -0.02717919461429119, 0.004857135470956564, 0.10591979324817657, -0.2248980700969696, 0.7416735291481018, -0.5223356485366821, 0.1766403317451477, -1.1385674476623535, 0.30634012818336487, -0.31079578399658203, -0.6784991025924683, 0.5460270047187805, 0.44681963324546814, 0.7227679491043091, -0.17545872926712036, -0.49413174390792847, 0.40870633721351624, 1.3314872980117798, -0.9062430262565613, -0.05515260994434357, -0.008336642757058144, 1.0289918184280396, 0.006757674273103476, -0.4965910315513611, 0.2937413454055786, 0.3150859475135803, 0.19443809986114502, 0.009935067035257816, 0.11568578332662582, -0.24487176537513733, -0.6530283093452454, 0.18790443241596222, 1.4899479150772095, 0.16253408789634705, -0.008056518621742725, -1.1111892461776733, 0.15092787146568298, -1.2059223651885986, -0.376565545797348, 0.6138956546783447, 0.3273819088935852, 0.9046710729598999, -0.581834614276886, -0.5224572420120239, -0.1434938907623291, 0.13849052786827087, 0.41949141025543213, -0.4608803391456604, -0.5989383459091187, 0.18493111431598663, 0.3050400912761688, 0.2689567506313324, 0.7134785056114197, -0.49990314245224, 0.7509409785270691, 14.640446662902832, 1.2805286645889282, 0.06533730775117874, 0.7279676198959351, 0.4850757420063019, 0.4994870722293854, -0.24339523911476135, -0.3804459869861603, -1.4160912036895752, -0.47073060274124146, 1.0007177591323853, 0.09814479947090149, 0.7923269867897034, 0.40132829546928406, -0.2386692464351654, -0.1916624903678894, -0.4007358253002167, 0.8511531352996826, 0.3743419647216797, -1.4998608827590942, 0.2129882127046585, 0.14382325112819672, 0.6322447657585144, 0.451772004365921, 0.8905273079872131, 1.186343789100647, 0.46718281507492065, -0.573481559753418, 0.1512286216020584, 0.20343844592571259, 0.9899551868438721, 0.16755130887031555, 0.4170142412185669, 0.8646053075790405, -0.5256428718566895, -0.08523042500019073, -0.8037704229354858, -0.8818545937538147, 0.4109516143798828, 0.3983967900276184, -0.6610434651374817, -0.8034201860427856, -0.5454690456390381, 0.6480117440223694, -0.3416968584060669, 0.4066089391708374, -0.3215457499027252, 0.7645100355148315, -0.2226995974779129, 0.395993173122406, 0.3483874201774597, 0.32644742727279663, 0.2787795066833496, 0.13045541942119598, 0.2126915454864502, 0.06497473269701004, 0.2201320230960846, 0.47219228744506836, -0.821160078048706, -0.04117587208747864, -0.15874984860420227, -0.4251021444797516, -0.16920582950115204, 0.5649193525314331, 0.6215548515319824, 0.2105870246887207, -0.21442148089408875, 0.3552187383174896, 1.0008597373962402, 0.35998421907424927, -0.25756850838661194, 0.3033823072910309, 0.18328535556793213, -0.26572221517562866, -0.0920763611793518, 0.29975074529647827, -0.34756991267204285, -0.32902470231056213, -0.9273884296417236, -0.30083662271499634, 0.20339389145374298, -0.7399200797080994, -1.1706182956695557, 0.8125320672988892, -0.4022519588470459, -0.161516934633255, 0.04529735445976257, -0.6482911705970764, 0.011406374163925648, 0.6263039112091064, -1.7529829740524292, -0.20185162127017975, 0.3845745325088501, -0.35802221298217773, -0.6481904983520508, -0.3254767060279846, 1.649639368057251, 0.26705488562583923, -0.2510174810886383, -0.1060890480875969, 0.39430004358291626, -0.016772141680121422, -0.005735133774578571, -0.9483364224433899, 0.6378191113471985, -0.08504179865121841, 0.1762339025735855, 0.07947796583175659, -0.09559784829616547, 0.1653234362602234, -0.39315471053123474, -0.19982260465621948, 0.761040985584259, -0.7974534630775452, -0.633260190486908, -0.09097203612327576, -0.9745631217956543, 0.12909772992134094, 0.5261510610580444, -0.3170558512210846, 0.4230181574821472, 0.331949383020401, -0.8882327079772949, 0.25915777683258057, -0.8026381731033325, -0.09609472006559372, 0.3302731215953827, -0.8850095868110657, -0.43208011984825134, 0.2918092906475067, 0.4535728693008423, -1.293619155883789, -0.37657052278518677, -0.16449971497058868, -0.45357391238212585, 0.13714194297790527, 1.0796701908111572, -0.44649311900138855, 0.5845867395401001, 0.9086748957633972, -0.09661364555358887, -1.2320911884307861, -0.08676149696111679, -0.6656050682067871, -0.22534073889255524, 0.38787057995796204, 0.8884587287902832, -0.23694051802158356, 0.5739907026290894, 1.1686517000198364, 0.23058615624904633, -0.458898663520813, -0.4918750822544098, -0.3868860900402069, 0.2165517657995224, -0.7036511898040771, 0.05869954824447632, 0.13644884526729584, -0.29093408584594727, 0.4379534423351288, 0.5042687058448792, 0.16448229551315308, -0.6309046149253845, -0.9829795956611633, 0.19936935603618622, -0.35146668553352356, -0.1741647571325302, -0.3303896188735962, 0.062460508197546005, -1.5433481931686401, 0.41479724645614624, -1.6093699932098389, 0.021381156519055367, -1.058748483657837, -0.8190468549728394, -0.4727407991886139, 0.10713686794042587, 0.10382784157991409, 0.19630543887615204, -0.38526609539985657, -0.289897620677948, -0.39138707518577576, -0.05415567010641098, 0.9424800276756287, 0.8961101174354553, -0.5402225255966187, -0.230525940656662, 0.054820142686367035, -0.1386679857969284, 0.5755906701087952, 0.6082887649536133, -0.6779533624649048, -1.033644676208496, -1.4995510578155518, 0.39208632707595825, -0.22608208656311035, -0.05004763603210449, -0.39591243863105774, 0.8444908857345581, 0.43681666254997253, -0.33503207564353943, 0.22334599494934082, 0.5424613356590271, -1.2682839632034302, -0.6992976665496826, 0.15882696211338043, -0.734632670879364, -0.10445398092269897, 0.19236288964748383, -0.21631887555122375, -0.2061624675989151, 0.13810625672340393, 0.1658007949590683, -0.8077484369277954, -0.6735720634460449, 0.29833468794822693, -0.4524155855178833, 0.48220935463905334, -0.5827470421791077, 0.18664613366127014, -1.4333038330078125, -0.1400684416294098, -0.18661010265350342, 0.28891873359680176, -0.6987174153327942, 0.5148196816444397, 0.1205727607011795, -1.2582757472991943, 0.07572071999311447, 0.40733450651168823, -0.12887053191661835, 0.18677200376987457, 0.4822234511375427, 0.4203293025493622, -0.31663569808006287, 0.5555092096328735, 0.49199503660202026, 0.35342973470687866, -0.7635643482208252, -0.0626041442155838, 0.6838185787200928, -0.5881714820861816, -0.42891719937324524, 1.322177767753601, -0.8678508400917053, -1.2879858016967773, 0.6110630035400391, -1.1097432374954224, -0.6393383145332336, 0.023097584024071693, 0.4375222623348236, 0.31865766644477844, -0.14285364747047424, 0.005881588906049728, -0.11440073698759079, 0.21681194007396698, -0.027986809611320496, -0.5962125062942505, 0.5051782131195068, -0.42200571298599243, -0.2032233476638794, 0.3098706305027008, 0.8745477795600891, -0.8235507607460022, -0.8155741095542908, -0.9677268862724304, -0.019837144762277603, -0.3037567734718323, 0.2813848555088043, -0.7137513160705566, -0.6979880928993225, 0.9734309315681458, 0.41156527400016785, 0.1631939858198166, -0.024495741352438927, -0.2311677485704422, 0.13349312543869019, 1.003183364868164, -0.15159322321414948, -0.8003193140029907, -0.5342589616775513, 1.7722643613815308, 1.1370803117752075, -1.0576167106628418, 0.23755039274692535, -0.376384973526001, -1.0119723081588745, 0.7142321467399597, 0.4409920573234558, 0.1712462306022644, 1.1434450149536133, -0.647978663444519, 0.15022161602973938, -0.11314171552658081, -0.7774155735969543, -0.57964688539505, 0.9400380253791809, 1.1045725345611572, 0.7723157405853271, 0.11076994985342026, 0.20116394758224487, 0.963616132736206, -0.09838754683732986, 0.3851146697998047, 0.4798678755760193, 0.27838146686553955, -0.5076977014541626, -0.2806839942932129, 0.28698939085006714, 0.941012442111969, -0.5877628922462463, -0.7394574880599976, 0.22183094918727875, 0.6118866801261902, 0.6458641290664673, 0.29059818387031555, 0.3925354480743408, 0.05707153305411339, 0.45960110425949097, 0.8325965404510498, 0.48135656118392944, -0.7026864886283875, -0.22523288428783417, -0.31348270177841187, -0.5006608366966248, 0.20949333906173706, -0.48801934719085693, -0.18082498013973236, -0.45546048879623413, -0.420355886220932, 0.31173673272132874, 0.14134155213832855, 0.46547409892082214, 1.0690301656723022, 0.5159288644790649, 0.4521678388118744, -0.8508285284042358, -0.2816334664821625, -0.4508149027824402, -0.9692395329475403, -0.26638004183769226, -0.7999463677406311, -0.5645933151245117, -0.28146108984947205, -0.348053514957428, 0.12033770233392715]}, "authors": [{"authorId": "66895761", "name": "I. Kobyzev"}, {"authorId": "31036999", "name": "A. Jafari"}, {"authorId": "2066076226", "name": "Mehdi Rezagholizadeh"}, {"authorId": "6574899", "name": "Tianda Li"}, {"authorId": "1403206217", "name": "Alan Do-Omri"}, {"authorId": "144313479", "name": "Peng Lu"}, {"authorId": "1807041", "name": "P. Poupart"}, {"authorId": "38565890", "name": "A. Ghodsi"}], "references": [{"paperId": "38f683ec0b9fda2069c0b2cb7ee1c71035915723", "title": "KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language Models via Knowledge Distillation"}, {"paperId": "be4bc115836e0a1cdef82e26a586c9fe26a0d8a5", "title": "Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data Augmentation via MiniMax"}, {"paperId": "0789d0f6f3459b7689486ebc80ce39373dd8cc17", "title": "MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "7599401233a40cc8c51b77d0419d2732c35439b2", "title": "Annealing Knowledge Distillation"}, {"paperId": "9ceae85a0bd4231cd2efe14884c40b7bc04d3dac", "title": "Accounting for Variance in Machine Learning Benchmarks"}, {"paperId": "e339c5d31ffc7029c1f72d567ac07b4606701c72", "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "0d5a3fd61911590e887927c39e3cedd36c9c3c8c", "title": "Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers"}, {"paperId": "0abb08c4ec5feab4cdd82c471866dd4395c573ce", "title": "Contrastive Distillation on Intermediate Representations for Language Model Compression"}, {"paperId": "21f74e2617d8d8f5fc117ff2ad6e58a540541f6d", "title": "Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"}, {"paperId": "04422085a52050516b9741e0fd1fda964b73dd53", "title": "An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models"}, {"paperId": "1728cb805a9573b59330890ba9723e73d6c3c974", "title": "Knowledge Distillation: A Survey"}, {"paperId": "05104e44493ce83f182827e4085db7caae413ff6", "title": "The Effects of Mild Over-parameterization on the Optimization Landscape of Shallow ReLU Neural Networks"}, {"paperId": "7ea2a78a8d8a6327bd13aa4f2d9ace9231bd9662", "title": "Revisiting Knowledge Distillation via Label Smoothing Regularization"}, {"paperId": "5335fe1bf347f7ad1dce1611ea4b60bd8391a090", "title": "Transferring Inductive Biases through Knowledge Distillation"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97", "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f8de25118af2abc4c48afb947d6ec298e05ef1e5", "title": "When Does Label Smoothing Help?"}, {"paperId": "274b4ad4840b0a8a70c5bac3fe4b4861ce5fbb95", "title": "FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation"}, {"paperId": "09e49c88eefc9ecfd9e2e8dff5141ff6bfeb2747", "title": "Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem"}, {"paperId": "4afa7896ad35c5fc65112dc586c7a3e90f0404f3", "title": "Tune: A Research Platform for Distributed Model Selection and Training"}, {"paperId": "2444be7584d1f5a7e2aa9f65078de09154f14ea1", "title": "Born Again Neural Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "8dd85e38445a5ddb5dd71cabc3c4246de30c014f", "title": "A Survey of Model Compression and Acceleration for Deep Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "59d823fc9877f8d448ac4c2d90e38051026e3201", "title": "Individual Comparisons by Ranking Methods"}, {"paperId": "24a099c7ac954f5795456235a245b1377f66244a", "title": "Universal-KD: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation"}, {"paperId": null, "title": "Kro-neckerbert: Learning kronecker decomposition for"}, {"paperId": "6d828bd874fddf10bbe059503aa8a38a7b56ead2", "title": "Towards a Better Understanding of Label Smoothing in Neural Machine Translation"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "fd0611608567d9a278e2f030c9b19544f8fae035", "title": "Neural Networks for Machine Learning"}, {"paperId": "20f63033e8775cbab0692aed92d38da7e725d64e", "title": "Understanding Machine Learning - From Theory to Algorithms"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "We studied the gap between base \ufb01ne-tuning and \ufb01ne-tuning with KD/TF and observed that this gap is negligible for NLU tasks"}]}