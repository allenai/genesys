{"paperId": "c57fc2f10a7b9b1cdcbcbba66eaae924ea2717ad", "title": "A Single Transformer for Scalable Vision-Language Modeling", "abstract": "We present SOLO, a single transformer for Scalable visiOn-Language mOdeling. Current large vision-language models (LVLMs) such as LLaVA mostly employ heterogeneous architectures that connect pre-trained visual encoders with large language models (LLMs) to facilitate visual recognition and complex reasoning. Although achieving remarkable performance with relatively lightweight training, we identify four primary scalability limitations: (1) The visual capacity is constrained by pre-trained visual encoders, which are typically an order of magnitude smaller than LLMs. (2) The heterogeneous architecture complicates the use of established hardware and software infrastructure. (3) Study of scaling laws on such architecture must consider three separate components - visual encoder, connector, and LLMs, which complicates the analysis. (4) The use of existing visual encoders typically requires following a pre-defined specification of image inputs pre-processing, for example, by reshaping inputs to fixed-resolution square images, which presents difficulties in processing and training on high-resolution images or those with unusual aspect ratio. A unified single Transformer architecture, like SOLO, effectively addresses these scalability concerns in LVLMs; however, its limited adoption in the modern context likely stems from the absence of reliable training recipes that balance both modalities and ensure stable training for billion-scale models. In this paper, we introduce the first open-source training recipe for developing SOLO, an open-source 7B LVLM using moderate academic resources. The training recipe involves initializing from LLMs, sequential pre-training on ImageNet and web-scale data, and instruction fine-tuning on our curated high-quality datasets. On extensive evaluation, SOLO demonstrates performance comparable to LLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.", "venue": "", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper introduces the first open-source training recipe for developing SOLO, an open-source 7B LVLM using moderate academic resources, and demonstrates performance comparable to LLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning."}, "embedding": {"model": "specter_v2", "vector": [0.3734003007411957, 0.2472461760044098, -0.0922517329454422, 0.0626504123210907, 0.07022132724523544, 0.0005922940908931196, 0.7024157047271729, -0.12033258378505707, -0.67521733045578, -0.7778878211975098, -0.05230836942791939, -0.11549080163240433, 0.6304759383201599, -0.0296851247549057, -0.30341431498527527, 0.2621481716632843, -0.740859866142273, 0.053115326911211014, 0.3750474452972412, -0.336318701505661, -0.40240687131881714, -0.04598071426153183, -1.0276732444763184, 0.16338704526424408, 0.0778108686208725, 1.07405424118042, 0.6482215523719788, 1.3346253633499146, -0.44633936882019043, 0.8829702734947205, 0.7107669711112976, -0.07117259502410889, 0.3232950270175934, 0.13163012266159058, -0.11126275360584259, 0.12240107357501984, 0.7070050239562988, -0.4521094560623169, -0.6955300569534302, 0.8339715003967285, -0.26715534925460815, 0.11367806792259216, 0.5667873620986938, -0.9356812238693237, -0.5626832842826843, 0.04787944257259369, 0.5679931044578552, 0.6268242597579956, -0.6270805597305298, -0.09359114617109299, 1.2706115245819092, -1.344738483428955, 0.27986547350883484, 1.6604756116867065, 0.5665712356567383, 0.5219978094100952, -0.20029963552951813, -0.6294984817504883, 0.72991943359375, -0.32646307349205017, -0.8409794569015503, -0.1722629964351654, -0.2661600112915039, -0.14699214696884155, 1.9245244264602661, -0.646526575088501, 0.22895564138889313, 0.5853666067123413, 0.24028128385543823, 1.1713619232177734, 0.13306774199008942, -0.66986483335495, -0.23276923596858978, 0.055407144129276276, -0.10130194574594498, 1.1183795928955078, -0.08146461099386215, -0.030917178839445114, -0.6901582479476929, 0.45220375061035156, 0.9163381457328796, 0.032646045088768005, 0.4900408685207367, -0.3903283476829529, -0.16821923851966858, 0.5791614055633545, 0.9953627586364746, 0.6148943901062012, -0.02354036457836628, 0.838985800743103, 0.4884026348590851, 0.3001890480518341, -0.12578490376472473, -0.018961336463689804, 0.034149862825870514, 1.2384635210037231, -0.6720089912414551, 0.21183545887470245, -0.30677807331085205, 1.1153762340545654, -0.31439340114593506, 0.3631196916103363, -0.5801339745521545, -0.018361732363700867, 1.629499077796936, 0.5109670162200928, 0.5176581144332886, -0.7012127637863159, 0.36228808760643005, -0.5883849263191223, 0.01710338331758976, -0.8089587092399597, -0.2108556628227234, -0.2348799854516983, -0.972507894039154, -0.5839395523071289, -0.4300987422466278, 0.6670381426811218, -1.2354953289031982, 0.631804883480072, -0.529742956161499, -0.13830454647541046, 0.208256334066391, 0.4136969745159149, 1.0672932863235474, 0.790960431098938, 0.7551863193511963, 0.33205485343933105, 1.1240077018737793, -1.0630875825881958, -0.03399992734193802, -1.2618728876113892, 0.08943232893943787, -0.16701187193393707, 0.5196664929389954, -0.4201781749725342, -1.0802545547485352, -1.3247178792953491, -0.8604328036308289, -0.39970526099205017, -0.7287687063217163, 0.3017978370189667, 1.1717827320098877, -0.019447091966867447, -1.3476907014846802, 0.2568644881248474, 0.024313749745488167, -0.4807409346103668, 0.7000190615653992, 0.14143767952919006, 0.14548799395561218, -0.3268921971321106, -0.774902880191803, 0.5303191542625427, -0.00878908857703209, -0.6584023237228394, -0.614812970161438, -0.21393704414367676, -1.505177617073059, 0.024630386382341385, 0.08925964683294296, -0.9414486885070801, 1.1392407417297363, -0.299370139837265, -0.7534987926483154, 0.6584270000457764, -0.43644556403160095, -0.09159339964389801, 0.28078126907348633, 0.07799311727285385, -0.22591456770896912, -0.07617481797933578, -0.2577444911003113, 1.2023530006408691, 0.6346880793571472, -0.49194395542144775, 0.018008708953857422, 0.4376937747001648, -0.2453291416168213, -0.16124653816223145, -0.2896900773048401, 1.3027608394622803, -0.4595472514629364, -0.14958444237709045, 0.18693041801452637, 0.7393736243247986, -0.5452579259872437, 0.24952444434165955, -0.04024932160973549, -0.7287096977233887, 0.6318047046661377, 0.013573463074862957, 0.35456857085227966, -1.0395010709762573, -0.6794043183326721, -0.5318960547447205, 0.2546694576740265, -0.07521526515483856, -1.099908709526062, 0.2219514399766922, -0.14277391135692596, 0.4453830122947693, 0.11499910056591034, -1.4756571054458618, -0.1802263855934143, -0.2742963433265686, -0.7705051302909851, -0.20334957540035248, 0.4240330159664154, 1.4311796426773071, -0.7932533025741577, -0.4521035850048065, 0.14514575898647308, 0.45723268389701843, -0.8844012022018433, 1.1013425588607788, -0.6330645680427551, -0.016972340643405914, -0.1677313596010208, -0.11139952391386032, -0.05991467088460922, -0.957662045955658, 0.3092805743217468, -0.5505549907684326, -0.0795641615986824, 0.05428431183099747, 0.07775717228651047, 1.211403250694275, -0.40130171179771423, 0.7930843830108643, -0.24808217585086823, -0.45787638425827026, 0.2078569531440735, 0.10550203919410706, -0.3010096549987793, -0.4318597912788391, 0.4632602035999298, 0.15584108233451843, -0.8817718029022217, 0.4091280400753021, 0.4303193986415863, 0.8276239037513733, -0.20530052483081818, -0.3510660231113434, 0.8053388595581055, -0.4627874195575714, 0.3835422396659851, 0.5723974108695984, 0.6591335535049438, 0.3280937373638153, 0.4075128138065338, 0.18585370481014252, 0.36054912209510803, -0.9180631637573242, -0.40266135334968567, 0.9061723351478577, 0.4588642716407776, 1.2096748352050781, 0.14305710792541504, -0.934824526309967, -0.7931150794029236, -0.08254547417163849, 0.8714271783828735, 1.1751759052276611, -0.08882051706314087, -0.2698366940021515, -0.6881676912307739, 0.034219153225421906, -0.46989625692367554, -0.26210349798202515, -0.09899647533893585, 0.15581756830215454, -0.16819438338279724, -0.6833906173706055, 0.6134636402130127, 0.4383111596107483, 1.0780351161956787, -0.8352883458137512, -0.617773711681366, -0.716403603553772, 0.5769648551940918, -1.2254959344863892, -0.45857664942741394, 0.18391689658164978, -0.4766947031021118, 0.09199579805135727, -0.026722082868218422, -0.47974249720573425, 0.4262823462486267, -0.1579168438911438, 0.5389313101768494, -0.6150944232940674, -0.7051398158073425, 0.5566200017929077, 1.0012431144714355, -0.8664459586143494, -0.8184856176376343, -0.21982651948928833, -0.048521555960178375, -0.17902706563472748, 0.06891170889139175, 0.46432390809059143, 0.12061566114425659, -0.16394051909446716, -0.44333693385124207, 0.43128523230552673, 0.3867673873901367, -0.2078947126865387, 0.925532877445221, -0.3198191225528717, -0.4938695728778839, -0.4950909912586212, 0.5955463647842407, 0.5788949728012085, -0.678652286529541, 0.5467607378959656, -0.5980668067932129, -0.47549664974212646, 0.6182657480239868, -0.8094179630279541, -0.08063315600156784, -0.4278440773487091, 0.7543376088142395, -0.8684434294700623, -0.5078740119934082, -0.15572544932365417, 0.4505811333656311, -0.3013569712638855, 0.10555718094110489, 0.6250720620155334, 0.396767795085907, -0.002431080210953951, 0.5658544898033142, -0.9064211249351501, 0.4624563455581665, 0.47060826420783997, 0.3578715920448303, 0.18830594420433044, 0.23669780790805817, -0.9050095677375793, -0.64072585105896, -0.5088829398155212, -0.2447793334722519, -0.58787602186203, 0.7176995277404785, -0.9297838807106018, -0.789543867111206, 0.2517755329608917, -1.32803213596344, -0.250262051820755, 0.27818354964256287, -0.43662363290786743, -0.1837615668773651, -1.0268731117248535, -1.1059315204620361, -0.49678167700767517, -0.6742734313011169, -1.1687394380569458, 0.4796540439128876, 0.3975250720977783, -0.1315288245677948, -0.43544360995292664, -0.44109535217285156, -0.3661714792251587, 0.9569185972213745, -0.40455955266952515, 0.516930341720581, 0.021204421296715736, -0.558279812335968, -0.24524812400341034, -0.29581117630004883, 0.6800275444984436, -0.4071491062641144, 0.4562916159629822, -0.9310666918754578, 0.12202686816453934, -0.3362334966659546, -0.5287067294120789, 0.5477489233016968, 0.39743226766586304, 0.5232123732566833, 0.2702634930610657, -0.20863127708435059, 0.45526808500289917, 1.6771140098571777, -0.8553789854049683, -0.19166217744350433, 0.06605228036642075, 1.176982045173645, -0.18711069226264954, -0.7413634061813354, 0.1820584535598755, 0.5687980055809021, 0.06412957608699799, 0.37003016471862793, -0.3742799758911133, -0.6739177107810974, -0.771803617477417, 0.7298201322555542, 1.1624271869659424, 0.2644289433956146, -0.025836756452918053, -1.0663493871688843, 0.4644494354724884, -0.9529585242271423, -0.8979726433753967, 0.9133570790290833, 0.529613733291626, -0.06649239361286163, -0.12873782217502594, -0.3185203969478607, -0.8177452683448792, 0.554168701171875, 0.5207690596580505, -0.36140814423561096, -0.5255584716796875, -0.3565948009490967, 0.34820160269737244, 0.31360915303230286, 0.6184201836585999, -0.5433747172355652, 0.7136512398719788, 14.5267333984375, 0.709083080291748, -0.37160244584083557, 0.6214068531990051, 0.9453428983688354, 0.2597043216228485, -0.3721899092197418, -0.06487429141998291, -1.3615297079086304, -0.7171960473060608, 1.0215024948120117, 0.49001333117485046, 0.6792073845863342, 0.4025002419948578, -0.14911364018917084, 0.4492312967777252, -0.2031346708536148, 0.9955472946166992, 0.8274797201156616, -1.5097285509109497, 0.7486588358879089, 0.066014364361763, 0.25167712569236755, 0.7442611455917358, 1.0415526628494263, 0.7014820575714111, 0.41841059923171997, -0.7481564283370972, 0.7254494428634644, 0.30238842964172363, 1.1280980110168457, 0.4170536398887634, -0.07320692390203476, 0.3338993191719055, -1.3604658842086792, -0.2551216781139374, -0.7251805067062378, -1.075007438659668, 0.005076251924037933, -0.28458505868911743, -0.4857834577560425, -0.5791676640510559, -0.0276643093675375, 0.8463267087936401, -0.28842079639434814, 0.26734668016433716, -0.11897802352905273, 0.36036956310272217, -0.3613406717777252, -0.07168228179216385, 0.3779006898403168, 0.5402262210845947, 0.021327868103981018, -0.08869277685880661, 0.08223360031843185, -0.155912846326828, 0.41148197650909424, 0.5732483267784119, -0.49374011158943176, -0.14932364225387573, -0.5809022188186646, -0.4079601764678955, -0.47144415974617004, 0.9397851824760437, -0.21258194744586945, 0.3299768567085266, -0.7549299001693726, 0.022176304832100868, 0.7407763600349426, 0.3863687515258789, -0.3881992995738983, 0.37198176980018616, 0.010678277350962162, -0.5401899814605713, 0.5321199297904968, 0.20859819650650024, -0.16156966984272003, -0.764462411403656, -0.6386570930480957, -0.3022399842739105, 0.19436608254909515, -1.0649340152740479, -0.6262974143028259, 0.8655621409416199, -0.16358280181884766, -0.11058466881513596, 0.6512631177902222, -1.18381929397583, -0.16017381846904755, 0.3897722363471985, -1.4359935522079468, -1.463856816291809, 0.056320901960134506, -0.2762346565723419, -0.04719207063317299, -0.38834190368652344, 0.9738306999206543, -0.26408323645591736, -0.052821286022663116, -0.15423396229743958, -0.4290083050727844, 0.22390520572662354, -0.5364260673522949, -0.3026151657104492, 0.850593090057373, 0.4760647416114807, 0.26177144050598145, 0.08292906731367111, 0.09237372875213623, 0.16194996237754822, -0.7999638319015503, -0.013600610196590424, 0.9049623012542725, -0.8014671206474304, -0.07040075212717056, -1.0706535577774048, -0.42733773589134216, 0.4050871729850769, 0.5538228154182434, 0.296699583530426, -0.13517391681671143, -0.3675517439842224, -1.0842686891555786, -0.2632790803909302, -0.6684117317199707, 0.3448823392391205, 0.3675583004951477, -0.9860644340515137, -0.02701757289469242, 0.012891552411019802, 0.33171626925468445, -0.7906252145767212, -0.26349931955337524, -0.2196803092956543, 0.17964014410972595, -0.18815851211547852, 1.401671290397644, -0.6107949614524841, 0.7721565961837769, 0.6286051273345947, -0.28980326652526855, -0.30153438448905945, -0.09717735648155212, -0.6717607975006104, 0.18663614988327026, -0.14265698194503784, 0.2329421490430832, -0.4147804081439972, -0.5203952789306641, 0.28815412521362305, 0.6781140565872192, -0.5298886299133301, -0.5023406147956848, -0.32325291633605957, -0.04001510888338089, -0.594559907913208, -0.0859813392162323, -0.6889587640762329, -0.4089210629463196, 0.4784318208694458, 0.3098796308040619, 0.767181932926178, -0.1417289525270462, -0.5163212418556213, 0.44133949279785156, 0.0884743258357048, -0.18206366896629333, -0.7446147799491882, -0.8413615822792053, -1.544458031654358, 0.03164742514491081, -1.1235347986221313, -0.03561779856681824, -1.2161815166473389, -0.08972188830375671, 0.48079368472099304, -0.28301358222961426, 0.38151150941848755, 0.6766788959503174, 0.01218786183744669, 0.07269628345966339, -0.5180903077125549, -0.8201284408569336, 0.5493096709251404, 1.146371603012085, -1.0373213291168213, 0.5493206977844238, -0.34579089283943176, 0.12684592604637146, 0.45942422747612, 0.12177629768848419, -0.2781132161617279, -1.0508873462677002, -1.0982565879821777, 0.4067533612251282, -0.27868586778640747, 0.47558367252349854, -1.0594255924224854, 0.9163126945495605, 0.2612982392311096, 0.10071250051259995, 0.29105138778686523, 0.47710832953453064, -0.8416066765785217, -0.7473845481872559, 0.6099753379821777, -0.9073666930198669, 0.20053540170192719, 0.6040648221969604, -0.49941393733024597, -0.2238059937953949, 0.8346712589263916, 0.1810198575258255, -1.1656728982925415, -0.9810177087783813, 0.5132779479026794, -0.4679946303367615, -0.017646051943302155, -0.1802772581577301, 0.24681226909160614, -0.9414932131767273, -0.4356032907962799, -0.08162280172109604, 0.2782555818557739, -0.3363623023033142, 1.0184752941131592, 1.0049985647201538, -0.9912438988685608, 0.08912578225135803, 0.3007582128047943, -0.18720848858356476, 0.20023372769355774, 0.41032490134239197, 0.13928234577178955, -0.5302785634994507, 0.5661624073982239, 0.0625201016664505, 0.32864299416542053, -0.9613494873046875, 0.41177353262901306, 0.9477413892745972, -0.4641193151473999, -0.42123275995254517, 1.0683979988098145, 0.19639486074447632, -0.9396318197250366, 0.13609598577022552, -1.3108694553375244, -0.6280910968780518, -0.6122497916221619, 0.5690006017684937, -0.4440041780471802, -0.5876767039299011, -0.2888094484806061, -0.4439953863620758, 0.5413804054260254, 0.18623113632202148, -0.37218812108039856, 0.43586573004722595, 0.006841993425041437, -0.3691035807132721, 0.8115248084068298, 0.7315438985824585, -0.798783004283905, -0.8953198194503784, -0.6057212948799133, -0.7745972871780396, 0.21031686663627625, 0.24934257566928864, -0.23817159235477448, -0.6142797470092773, 0.8546233773231506, 0.5318590998649597, 0.14582937955856323, 0.29821115732192993, -0.019945785403251648, -0.030359504744410515, 0.7916418313980103, 0.0728016346693039, -0.29429635405540466, -0.24483421444892883, 1.149225115776062, 1.405897617340088, -0.9584370255470276, 0.4789676070213318, -0.4521649479866028, -0.7911645174026489, 0.9329400658607483, 0.4052683115005493, -0.19452235102653503, 0.9697045087814331, -0.1046367734670639, 0.004774440545588732, 0.1864195168018341, -0.9855908155441284, -0.1713678389787674, 1.31307053565979, 0.859362781047821, 0.46888309717178345, 0.15655691921710968, 0.5214710235595703, 0.41683197021484375, 0.27547594904899597, 0.24117344617843628, 0.1924394816160202, 0.3469538688659668, -0.23936940729618073, 0.5050191283226013, 0.05978318676352501, 0.30728137493133545, -0.182810977101326, -0.6710566878318787, 0.2847972810268402, 0.3993665874004364, 0.30500516295433044, 0.48126891255378723, 0.9338675737380981, 0.24804271757602692, 0.44126570224761963, 0.04972309246659279, 0.9740357398986816, -0.4453834295272827, 0.059611961245536804, 0.12061706185340881, -1.0523467063903809, 0.12306497991085052, -0.5079812407493591, -0.4248755872249603, -0.5982170104980469, 0.024245547130703926, 0.7447155714035034, -0.6716620922088623, 0.371534138917923, 1.2374212741851807, 0.34997475147247314, 0.5691742300987244, -0.6154208779335022, -0.5098863840103149, -0.5136909484863281, -0.5060765147209167, 0.49103260040283203, -0.5588564276695251, 0.034635670483112335, -0.2929605543613434, -0.010720708407461643, -0.07500631362199783]}, "authors": [{"authorId": "123331686", "name": "Yangyi Chen"}, {"authorId": "2144803999", "name": "Xingyao Wang"}, {"authorId": "2254026935", "name": "Hao Peng"}, {"authorId": "2243197103", "name": "Heng Ji"}], "references": [{"paperId": "e07ff6d26b2e85e5bc28727717a6bcd40998b593", "title": "MACAROON: Training Vision-Language Models To Be Your Engaged Partners"}, {"paperId": "50e69faefa9a78afb3068f7ca15df07e7ac7b319", "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model"}, {"paperId": "32112b798f70faab00e14806f51d46058cf5e597", "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models"}, {"paperId": "ce68430823b79dd3d478c505cc2761f03cf72b30", "title": "What matters when building vision-language models?"}, {"paperId": "01ae1c181dcb5117491affae728065e5e62bf074", "title": "InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD"}, {"paperId": "49873ee415619efd9e1e4c16f73ee066ff008c1f", "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies"}, {"paperId": "ac2848656e68b60665e6bc3e28eb6c7d5bebb4b0", "title": "Advancing LLM Reasoning Generalists with Preference Trees"}, {"paperId": "8a9e11addba791860a9dbf15de75cc28d2cf844c", "title": "Are We on the Right Way for Evaluating Large Vision-Language Models?"}, {"paperId": "e61fde1309a9f5aab2060ace6f709711823c9ca5", "title": "Understanding Emergent Abilities of Language Models from the Loss Perspective"}, {"paperId": "b6648235f437c5be722db822f1f29a6a05984cd2", "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images"}, {"paperId": "29af66d6eefbb0117c0bdfbe1eef84727b70d3b6", "title": "Unlocking the conversion of Web Screenshots into HTML Code with the WebSight Dataset"}, {"paperId": "b14e5138d4d1f3577b2390541dc7b730a41bb651", "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding"}, {"paperId": "e96c86407b3acf09e5d7729bec201824b717b476", "title": "Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset"}, {"paperId": "f6b9ccd7533b58e14d284191f1a576b0c764b3d5", "title": "ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models"}, {"paperId": "f4da79c8b9e708ff46ead6e35c7c6ca125218acb", "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning"}, {"paperId": "78fbb6e7a1c568a04e8c935aa9909d0c942ea5f6", "title": "Executable Code Actions Elicit Better LLM Agents"}, {"paperId": "a050c9b0c321839e4427ab9defa3463be7825ac4", "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models"}, {"paperId": "2b14d9e190022e388476ebb24eb1a84349ca0de4", "title": "Silkie: Preference Distillation for Large Visual Language Models"}, {"paperId": "2141ed804636a1cf339d606cd03fd3b3e9582133", "title": "VILA: On Pre-training for Visual Language Models"}, {"paperId": "154cc4e8a9e8ad24d4f9c9440b187d06b9ba57bd", "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models"}, {"paperId": "b50d19c5c298f6562c3b3c6c3822a351bdc89260", "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"}, {"paperId": "033607afc2e08a58383ad78deb98c844017109c1", "title": "ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation"}, {"paperId": "f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560", "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"}, {"paperId": "391eaeb1092c2b145ff0e5a2fa61637a42921fce", "title": "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"}, {"paperId": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada", "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"}, {"paperId": "619184447595337a9fe3dca72c4e951e7ab7467c", "title": "To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning"}, {"paperId": "bf14244669d5505f63343d4365d99d24aa6c5e82", "title": "Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models"}, {"paperId": "ad13b213681b6f634bc83a264df246e83dd9a9d9", "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"}, {"paperId": "56cb3506586c540d4bd2c573fad66d22dff69826", "title": "CapsFusion: Rethinking Image-Text Data at Scale"}, {"paperId": "1ddbd08ad8cf22a5c66c4242194c4286328533bf", "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "8946891e94831adc8cddb0d32311cce2445c96d2", "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"}, {"paperId": "c1e450284e7d6cac1855330a1197df8537df653f", "title": "InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition"}, {"paperId": "844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5", "title": "Aligning Large Multimodal Models with Factually Augmented RLHF"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "280353fd7a7a3e49c415c443e1b7ccf7de9c2b4e", "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models"}, {"paperId": "fc6a2f7478f68adefd69e2071f27e38aa1647f2f", "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"}, {"paperId": "d7e92d03dfa5427c0c5ef2b59de54733e0589606", "title": "InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"}, {"paperId": "1245ef1926416d649b62323975c6fa22dfb885ee", "title": "Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages"}, {"paperId": "94972e30504017156ef5b5debc419bf6edc67384", "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"}, {"paperId": "7fbc502441d66daf1f53765d5d86a8dfba9ab0ce", "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "40298b8d50109c52fc10763eddc64a07cf8acb31", "title": "Planting a SEED of Vision in Large Language Model"}, {"paperId": "94053805cd59f2e9a47fe3f080c7e7afefb337cc", "title": "Generative Pretraining in Multimodality"}, {"paperId": "ebddfdc5d845a788e8062eddbbf7a335737cb99b", "title": "What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"}, {"paperId": "a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3", "title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"}, {"paperId": "d5d936b142eb81826002888230649c805532630e", "title": "VisText: A Benchmark for Semantically Rich Chart Captioning"}, {"paperId": "d3f79210b54e168c76b8c311488f42d7d1048b81", "title": "PandaGPT: One Model To Instruction-Follow Them All"}, {"paperId": "a122863d239643453195424c04067e89406246e1", "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"}, {"paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773", "title": "Evaluating Object Hallucination in Large Vision-Language Models"}, {"paperId": "848e690a62c327e1210532d58a6b914097cac763", "title": "On the Hidden Mystery of OCR in Large Multimodal Models"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "81e7e82245c2f230eeb8aaaa1a2b2604c143754a", "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"}, {"paperId": "d6d3604f369bb0415cbe814e43ca3131323b03e2", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning"}, {"paperId": "570079bbdd8758dfe865097e05719313c9c1301a", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "574beee702be3856d60aa482ec725168fe64fc99", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "b287a2765e5bceb732de39dafdf70594dc9cd664", "title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"}, {"paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"}, {"paperId": "3e565c544a8639cc9c7568833e484d7610f5e5d4", "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"}, {"paperId": "d3135733aa39dec20ce72aa138589dda27c8406d", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"}, {"paperId": "02251886950770e82b3d68564d60cdfe15e73199", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"}, {"paperId": "599be9043ef3571f65758cf36e184c9dc1781baf", "title": "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers"}, {"paperId": "ef57279f080e6e305c5cd216fd8f22ade2c903da", "title": "Write and Paint: Generative Vision-Language Models are Unified Modal Learners"}, {"paperId": "47a67e76ed84260ff19f7a948d764005d1edf1c9", "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"}, {"paperId": "354b48677e314ef2f47512c5a81723cfd17dd05d", "title": "Visual Spatial Reasoning"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "b611c501269224702d1a9942c8600a31ec66ab28", "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"}, {"paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df", "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "9bcf3b43f2323a194036cc52c6878a9b1dc7e058", "title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "0d5406775fab3e71848908327fb5504df5f60f92", "title": "ImageNet-21K Pretraining for the Masses"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b", "title": "Explaining neural scaling laws"}, {"paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "title": "Taming Transformers for High-Resolution Image Synthesis"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "9a2a5bb18d6252711e124c0029fcf002d62f1795", "title": "Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "3f6570fd55dc5855f93a56150e6d99c7944a1c1e", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes"}, {"paperId": "33eadd4e666a894306a22ba0839c5e0cef77280e", "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "439369de9514e41e0f03fed552d8f6e5aebf51b2", "title": "Connecting Vision and Language with Localized Narratives"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "1097cf8cf5961589ff693b069002e7181e24e631", "title": "OCR-VQA: Visual Question Answering by Reading Text in Images"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "0033346700dc450ac22c9b704eab0e906d868662", "title": "Scene Text Visual Question Answering"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "e3bb5773205477ae4711524a9d4ae739bee40349", "title": "Challenges and Prospects in Vision and Language Research"}, {"paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907", "title": "Towards VQA Models That Can Read"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "634161e4759616dbe06f0b1465999d3df122f366", "title": "TallyQA: Answering Complex Counting Questions"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "7289a240c9425bc7cad87b3b835e5f0cac22f488", "title": "DVQA: Understanding Data Visualizations via Question Answering"}, {"paperId": "cb6be69c67b0b15ebbda89a126f4dd62a4d32958", "title": "FigureQA: An Annotated Figure Dataset for Visual Reasoning"}, {"paperId": "c071a1ad68310fed7f0876b6f01cb7b135043bc3", "title": "Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "e18ec2c9f0b4a817b8cf0435822bbc879d7db698", "title": "A Diagram is Worth a Dozen Images"}, {"paperId": "def584565d05d6a8ba94de6621adab9e301d375d", "title": "Visual7W: Grounded Question Answering in Images"}, {"paperId": "4a98a8b3b28250cb77d99748bff15ddbd9351433", "title": "A Survey of Current Datasets for Vision and Language Research"}, {"paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}, {"paperId": "62a956d7600b10ca455076cd56e604dfd106072a", "title": "Exploring Models and Data for Image Question Answering"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": null, "title": "Llava-next: Improved reasoning, ocr, and world knowledge"}, {"paperId": "84dc889beff9d51fe429cff8c92735e7410ee3c2", "title": "Aligning Large Multi-Modal Model with Robust Instruction Tuning"}, {"paperId": "24bd5b7455a07631cbc4023c07eb2f0eef85ea85", "title": "Multimodal research in vision and language: A review of current and emerging trends"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Introducing our multimodal models"}, {"paperId": null, "title": ": A comprehensive evaluation benchmark for multimodal large language"}, {"paperId": null, "title": "Detailed caption"}, {"paperId": null, "title": "Etched is building an AI chip that only runs one type of model"}, {"paperId": null, "title": "SlimPajama: A 627B token cleaned and dedu-plicated version of RedPajama"}, {"paperId": null, "title": "In-fographicvqa"}, {"paperId": null, "title": "OpenCompass"}, {"paperId": null, "title": "Diagram image to text"}, {"paperId": null, "title": "Accelerate: Training and inference at scale made simple, efficient and adaptable"}, {"paperId": null, "title": "pretraining for the"}]}