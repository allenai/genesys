{"paperId": "3ff76a208441184c48838aca8ff07d82878328cf", "title": "Training-Free Acceleration of ViTs with Delayed Spatial Merging", "abstract": "Token merging has emerged as a new paradigm that can accelerate the inference of Vision Transformers (ViTs) without any retraining or fine-tuning. To push the frontier of training-free acceleration in ViTs, we improve token merging by adding the perspectives of 1) activation outliers and 2) hierarchical representations. Through a careful analysis of the attention behavior in ViTs, we characterize a delayed onset of the convergent attention phenomenon, which makes token merging undesirable in the bottom blocks of ViTs. Moreover, we augment token merging with a hierarchical processing scheme to capture multi-scale redundancy between visual tokens. Combining these two insights, we build a unified inference framework called DSM: Delayed Spatial Merging. We extensively evaluate DSM on various ViT model scales (Tiny to Huge) and tasks (ImageNet-1k and transfer learning), achieving up to 1.8$\\times$ FLOP reduction and 1.6$\\times$ throughput speedup at a negligible loss while being two orders of magnitude faster than existing methods.", "venue": "", "year": 2023, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A careful analysis of the attention behavior in ViTs characterize a delayed onset of the convergent attention phenomenon, which makes token merging undesirable in the bottom blocks of ViTs, and builds a unified inference framework called DSM: Delayed Spatial Merging."}, "embedding": {"model": "specter_v2", "vector": [0.23551131784915924, 0.6403974890708923, -0.3384086489677429, 0.16182376444339752, -0.43539461493492126, 0.22241710126399994, 1.0817404985427856, 0.07421189546585083, -0.8373303413391113, -0.9150992631912231, 0.3020339608192444, 0.40305960178375244, 0.5838838815689087, 0.2587999999523163, -0.3945693373680115, 0.16761651635169983, -0.7187075018882751, 0.1055009737610817, 0.56900954246521, -0.32281216979026794, -0.03851732984185219, -0.4549744725227356, -1.6697914600372314, 0.36832717061042786, 0.7254602909088135, 1.0068295001983643, 0.2698211371898651, 0.8812330961227417, -0.3762916028499603, 0.3332192301750183, 0.13281841576099396, -0.14152327179908752, 0.6082468628883362, 0.055054839700460434, -0.2871285676956177, -0.18191571533679962, 0.8270841240882874, -0.6541681885719299, -0.6581640839576721, 0.9485018253326416, -0.062376201152801514, 0.43706294894218445, 0.39034584164619446, -0.8957332372665405, -0.17844799160957336, 0.5728334188461304, 0.22345589101314545, 0.6622152924537659, -0.6759878993034363, -0.5934818387031555, 0.7189638614654541, -1.488978624343872, 0.2736797332763672, 1.5031776428222656, 0.42573362588882446, 0.015949297696352005, 0.14802387356758118, -0.30583256483078003, 1.185152292251587, 0.5084508061408997, -0.7796716094017029, -0.510295569896698, -0.02891312725841999, -0.009396609850227833, 1.6927410364151, -0.7375180125236511, 0.2701851725578308, 0.3703903555870056, -0.2908765971660614, 1.6203758716583252, -0.18979787826538086, -0.6699842214584351, 0.07732950896024704, 0.05512664467096329, 0.11996936053037643, 0.6940224170684814, -0.28827327489852905, 0.5276358127593994, -1.1136738061904907, 0.11439880728721619, 0.4781610667705536, 0.09223219752311707, 0.5989415049552917, -0.2089574933052063, -0.2209337055683136, 0.4888894557952881, 1.0182712078094482, 0.6203804016113281, -0.46417537331581116, 1.1333578824996948, 0.660470724105835, 0.1082400530576706, 0.23168933391571045, -0.018672393634915352, 0.31949031352996826, 0.5176782608032227, -1.1045421361923218, 0.22240515053272247, -0.6333223581314087, 0.954617440700531, -0.12517669796943665, 0.43363696336746216, -0.9728074073791504, 0.3000902235507965, 1.1501743793487549, 0.2626878619194031, 0.30856403708457947, -0.8497545123100281, -0.007326721679419279, -0.9232810735702515, 0.06939581036567688, -0.3790420591831207, 0.35430142283439636, -0.3332442045211792, -0.812537431716919, -0.6625527739524841, -0.49550914764404297, 0.5700807571411133, -1.4452252388000488, 0.5683565139770508, -0.24512091279029846, 0.6689052581787109, -0.1397775113582611, 0.6137886643409729, 0.5266826152801514, 0.4790787398815155, 0.4661175310611725, 0.27560850977897644, 1.2325934171676636, -0.9307712912559509, -0.3068712651729584, -0.9417542815208435, 0.1565011888742447, -0.09059025347232819, -0.2071884423494339, -0.08498277515172958, -1.8225452899932861, -1.687668800354004, -1.055377721786499, -0.279414564371109, -0.6643317937850952, -0.039468914270401, 0.78980553150177, -0.05457644537091255, -1.5772124528884888, 0.8594478964805603, -0.4624560475349426, -0.3359988331794739, 0.7816470861434937, 0.30585506558418274, 0.5739682912826538, -0.25723469257354736, -0.8326627016067505, 0.31819745898246765, -0.09025795757770538, -0.3244871497154236, -0.47999563813209534, -0.6424803733825684, -1.189864158630371, 0.09300658851861954, 0.25125935673713684, -0.8987183570861816, 1.4264947175979614, -0.13783235847949982, -0.8849261999130249, 0.44461122155189514, -0.7062429189682007, -0.05886053666472435, 0.4804033637046814, -0.2775561213493347, -0.1270376294851303, -0.34880298376083374, -0.043542150408029556, 1.1748535633087158, 1.0645726919174194, -0.5482310652732849, -0.412091463804245, -0.0005444709677249193, -0.6845784187316895, 0.028568778187036514, -0.30720534920692444, 0.5801253914833069, -0.6574382781982422, -0.1967466026544571, 0.7326533198356628, 0.9893699884414673, -0.07667575031518936, -0.32777148485183716, -0.08141791075468063, -1.068191647529602, 0.9052870273590088, 0.3887036442756653, -0.0013713020598515868, -1.0469363927841187, -0.6898071765899658, -0.007292572874575853, 0.3948706388473511, -0.04839036613702774, -0.9442800283432007, 0.2560649514198303, -0.07517439872026443, 0.16869011521339417, 0.07392572611570358, -1.4178193807601929, 0.07784362882375717, -0.33007922768592834, -0.9765773415565491, -0.06884139031171799, 0.33454152941703796, 1.7044209241867065, -1.0277198553085327, -0.21168696880340576, 0.2570072412490845, 0.16774606704711914, -0.8402566313743591, 1.2057619094848633, -0.35860592126846313, 0.08699702471494675, 0.16220982372760773, 0.1154913678765297, 0.3273811936378479, -0.19999384880065918, 0.5009811520576477, -0.9611915349960327, -0.22287316620349884, 0.5157646536827087, -0.5018216371536255, 1.3666857481002808, 0.1240871474146843, 0.6347520351409912, -0.28147435188293457, -1.054335355758667, 0.20604586601257324, 0.2186959981918335, -0.1269579380750656, -1.089890718460083, 0.6678922176361084, 0.02524646371603012, -0.808212399482727, 0.45288407802581787, 1.2348530292510986, 1.0843600034713745, -0.3434005379676819, -0.2935514450073242, 0.9448215365409851, -0.33118125796318054, -0.016114480793476105, 0.7837806940078735, 0.47957414388656616, 0.44826245307922363, 0.06575320661067963, -0.00041647511534392834, -0.32236120104789734, -0.8278186321258545, 0.11981179565191269, 1.1188958883285522, 0.38742372393608093, 0.9144738912582397, 0.15832187235355377, -0.6066421866416931, -0.6641058325767517, -0.3528628945350647, 0.5059530138969421, 1.4442389011383057, 0.4672054052352905, 0.1578460931777954, -0.7510378956794739, -0.1856754720211029, -0.1686869114637375, -0.5197002291679382, -0.43154478073120117, -0.3064386248588562, -0.5524235963821411, -0.4588812291622162, 1.1250296831130981, 0.705269992351532, 1.275541067123413, -0.7278429269790649, -0.17894649505615234, -0.1925593912601471, 0.19248639047145844, -0.9954096674919128, -0.2626577913761139, 0.36079421639442444, -0.4146261215209961, 0.12381627410650253, -0.13397595286369324, -0.30171242356300354, 0.25574904680252075, -0.21163469552993774, 0.8031690716743469, -0.40761879086494446, -0.6235796809196472, -0.012948100455105305, 0.5338044166564941, -0.9489497542381287, -0.5024884939193726, 0.16780954599380493, 0.2195781022310257, -0.32001206278800964, 0.07896006852388382, 0.5800970792770386, -0.35580888390541077, 0.08259037882089615, -0.40944454073905945, 0.16098357737064362, 0.235322967171669, -0.19967104494571686, 0.7063203454017639, -0.8553211688995361, 0.4759107530117035, -0.5252233743667603, 0.45372098684310913, 0.19018103182315826, -0.5091096758842468, 0.09451613575220108, -0.6962480545043945, -0.3807844817638397, 0.021010635420680046, -0.5572789907455444, -0.4132702350616455, -0.9280472993850708, 0.38359376788139343, -0.895671546459198, -0.36000943183898926, -0.5070473551750183, 0.9004497528076172, 0.016625698655843735, 0.3739089071750641, 0.32922059297561646, 0.14522625505924225, 0.0020783613435924053, 0.8808577060699463, -0.9221025109291077, 1.2260161638259888, 0.20722919702529907, 0.11065661907196045, 0.36116376519203186, -0.24213925004005432, -0.8767692446708679, -0.1821015179157257, -0.8660080432891846, -0.505387008190155, -0.6967774629592896, 0.5703416466712952, -0.7822763919830322, -0.9777061939239502, 0.2793223559856415, -1.371187686920166, -0.12138556689023972, -0.11305444687604904, -0.05703221634030342, -0.20112071931362152, -0.92439204454422, -1.2895225286483765, -0.4919317364692688, -0.39759206771850586, -0.9326494336128235, 0.3974516689777374, 0.22135095298290253, -0.34307053685188293, 0.007766853552311659, -0.13216057419776917, -0.4832225441932678, 1.0487409830093384, -0.5131849646568298, 0.33546242117881775, 0.14806851744651794, -0.653019905090332, 0.09717253595590591, -0.23879234492778778, 0.4307972490787506, -0.6866066455841064, 0.24359719455242157, -1.5659750699996948, 0.04344705492258072, -0.5531964898109436, -0.33138570189476013, 0.9871068000793457, 0.6973063945770264, 0.4432799220085144, 0.33396992087364197, -0.7747157216072083, 0.47573235630989075, 1.3424162864685059, -0.5360775589942932, 0.37382420897483826, -0.06456366926431656, 1.1106252670288086, 0.1045999675989151, -0.03906668350100517, 0.5067228674888611, 0.05641778185963631, 0.31727248430252075, 0.651340663433075, -1.028537631034851, -0.5067130327224731, -0.471414178609848, 0.30224019289016724, 1.2673265933990479, 0.33218592405319214, 0.04669566825032234, -0.63582444190979, 1.0762969255447388, -1.6168893575668335, -0.6392961740493774, 0.5644274950027466, 0.786593496799469, -0.28935641050338745, -0.47022032737731934, -0.12159191817045212, -0.6902696490287781, 0.7948012948036194, 0.4703494906425476, -0.6780827641487122, -0.5524025559425354, -0.010780347511172295, 0.7698501944541931, 0.406209260225296, 0.5437899827957153, 0.019547702744603157, 1.2162104845046997, 14.446831703186035, 0.7470149397850037, -0.10040178149938583, 0.5860657095909119, 0.5349313020706177, 0.24945606291294098, -0.22012771666049957, -0.20831412076950073, -0.8992673754692078, -0.5226421356201172, 0.6918392777442932, 0.39181435108184814, 0.47001418471336365, 0.1031893864274025, 0.05476094037294388, 0.044842950999736786, -0.6178024411201477, 0.6740768551826477, 0.3558656871318817, -1.5781904458999634, 0.1975465565919876, 0.024727413430809975, 0.6224594712257385, 1.2564617395401, 0.8884012699127197, 0.8671191930770874, 0.46660465002059937, -0.3402828276157379, 0.7540301084518433, 0.01979498378932476, 1.0911962985992432, 0.2583174407482147, 0.47102025151252747, 0.03442937880754471, -1.261642575263977, 0.14638791978359222, -0.680287778377533, -0.9808167815208435, 0.2791268825531006, -0.05265883728861809, -0.5118052959442139, -0.10099295526742935, 0.05293950438499451, 0.7824259996414185, -0.06242353096604347, 0.4470938742160797, 0.09273161739110947, 0.347705602645874, -0.12005159258842468, 0.3862624764442444, 0.445127934217453, 0.4647788107395172, -0.09803346544504166, 0.06404843926429749, -0.0377172976732254, -0.0894799456000328, -0.044803187251091, 0.6680682897567749, -0.6879331469535828, -0.6101982593536377, -0.35975462198257446, 0.40826472640037537, 0.13601566851139069, 0.7072219848632812, 0.1723850965499878, -0.4375191628932953, 0.014963410794734955, 0.4807836413383484, 0.5299180150032043, 0.2887561619281769, -0.6744455099105835, 0.08783469349145889, 0.40799248218536377, -0.654418408870697, 0.3763608932495117, 0.7320687770843506, -0.3150782883167267, -0.4566195011138916, -0.593625545501709, 0.07283776998519897, 0.21306009590625763, -0.8322398066520691, -0.4338138699531555, 0.9290268421173096, 0.09609068930149078, -0.39783406257629395, 0.3033280074596405, -0.31550082564353943, -0.5127968788146973, 0.30425629019737244, -1.478634238243103, -0.716907799243927, -0.24627837538719177, -0.07750297337770462, -0.3745645582675934, 0.28736114501953125, 0.6523889303207397, 0.16150841116905212, 0.12335497885942459, 0.03891785070300102, -0.15641887485980988, -0.27760618925094604, 0.16143979132175446, -0.6900948286056519, 1.1664825677871704, 0.38193479180336, 0.15835116803646088, -0.1538998931646347, -0.061904992908239365, 0.3033755421638489, -0.4492788016796112, 0.020366597920656204, 0.5065209269523621, -0.44249606132507324, -1.0365509986877441, -0.790923535823822, -1.175783395767212, 0.352451890707016, 0.8510146141052246, -0.05919871851801872, -0.37145641446113586, 0.27715417742729187, -0.6792259812355042, -0.2552843987941742, -0.7663969993591309, 0.1563967615365982, 0.3920530676841736, -0.7391097545623779, -0.351138174533844, -0.2724915146827698, 0.37333622574806213, -0.9985725283622742, -0.24022908508777618, -0.09800318628549576, 0.06329882144927979, -0.2238413691520691, 1.3583418130874634, -0.029852107167243958, 0.14394336938858032, 0.5083930492401123, 0.2425808310508728, -0.5148745775222778, -0.011177828535437584, -0.6146522760391235, 0.22726593911647797, 0.2771815061569214, 0.018913883715867996, -0.22967882454395294, 0.49082672595977783, 0.22903598845005035, -0.06945392489433289, -0.15495309233665466, -0.6783140301704407, -0.2650645077228546, -0.04087185487151146, -0.8783929944038391, 0.24925358593463898, -0.1836760938167572, 0.17779973149299622, -0.18596410751342773, 0.4845578074455261, 0.6819354295730591, 0.21309001743793488, -0.8342270255088806, 0.45694050192832947, -0.09042312204837799, 0.16122806072235107, -0.6777490377426147, -0.6742008328437805, -1.7455394268035889, -0.33743593096733093, -1.04363214969635, 0.26759469509124756, -1.1151258945465088, -0.5716690421104431, 0.09038982540369034, -0.5572599768638611, 0.07098530977964401, 0.3570362329483032, 0.07976847887039185, -0.08253046125173569, -0.4372236728668213, -0.843787670135498, 0.9188457727432251, 0.6833727359771729, -0.8313421010971069, -0.04100579768419266, -0.6988893151283264, 0.24816614389419556, 0.4917490482330322, 0.28858229517936707, -0.4499501883983612, -0.6011186838150024, -1.2010999917984009, 0.15381436049938202, 0.06431008875370026, 0.3304204046726227, -1.013528823852539, 1.1664257049560547, 0.714691162109375, 0.2538808584213257, -0.4833993911743164, 0.5807575583457947, -0.4927029311656952, -0.9097265601158142, 0.45418626070022583, -0.6257983446121216, -0.1609799563884735, 0.5653912425041199, -0.5290442705154419, -0.36081424355506897, 1.1974221467971802, 0.10692110657691956, -0.7815202474594116, -1.3545844554901123, 0.35119450092315674, -0.5714973211288452, 0.12237708270549774, -0.22621910274028778, -0.4482089579105377, -1.4797475337982178, -0.23473285138607025, -0.025147467851638794, 0.4959546625614166, -0.49984610080718994, 1.015074610710144, 0.8310736417770386, -1.0801163911819458, -0.03284858539700508, 0.3957401514053345, -0.03585322946310043, 0.46874645352363586, 0.7237299084663391, 0.6457898020744324, 0.1618371158838272, 0.15709318220615387, 0.021133899688720703, -0.0472395122051239, -0.8319436311721802, 0.32459938526153564, 0.7018917202949524, -0.009421132504940033, -0.15003065764904022, 1.1675925254821777, 0.00740563590079546, -0.7189704775810242, 0.16312864422798157, -1.3484681844711304, -0.20165564119815826, -0.023341026157140732, 0.31605637073516846, 0.43561485409736633, 0.2619912028312683, 0.26311376690864563, -0.7263205647468567, 0.2665311396121979, -0.48786306381225586, -0.9561401605606079, -0.03809155896306038, -0.06278014928102493, -0.0443703830242157, 0.2619450092315674, 0.7677488923072815, -1.0602612495422363, -1.1229498386383057, -1.1340429782867432, -0.6816934943199158, -0.2318623661994934, 0.4258538782596588, -0.022998223081231117, -0.5420075058937073, 0.7574403285980225, 0.7293514609336853, 0.33919861912727356, -0.1393180936574936, 0.49683988094329834, 0.23091793060302734, 0.4794541001319885, -0.11179114878177643, -0.8960592746734619, -0.4106515347957611, 0.8780975937843323, 0.8422858119010925, -0.9105415344238281, 0.172107994556427, -0.06633877754211426, -0.7065982818603516, 0.2602371275424957, 0.5099097490310669, -0.48096829652786255, 0.3942062258720398, -0.2059096395969391, 0.19982436299324036, -0.057516470551490784, -0.8944074511528015, -0.6487210988998413, 0.9151177406311035, 1.1986026763916016, 0.2830265164375305, -0.05039551854133606, 0.5342501401901245, 0.22643376886844635, 0.6405934691429138, 0.12368562817573547, 0.043394364416599274, 0.30382007360458374, -0.24666696786880493, 0.15538091957569122, -0.09022676944732666, 0.5279741287231445, -0.4192033112049103, -0.637937605381012, 0.17677336931228638, 0.5017764568328857, 0.43575674295425415, 0.7930529713630676, 1.4046765565872192, 0.17289738357067108, 0.6781091690063477, -0.1538638174533844, 0.7939928770065308, -0.14017340540885925, -0.3629688024520874, 0.22101202607154846, -1.0698814392089844, -0.2194874882698059, -0.3295914828777313, -0.7372142672538757, -0.7231577038764954, -0.5313540101051331, 0.3164256811141968, -0.10542583465576172, 0.39412927627563477, 1.1356124877929688, 0.6941801309585571, 1.066927194595337, -0.3938983380794525, -0.9386930465698242, -0.2024068385362625, -0.7740549445152283, 0.3713168501853943, -0.2104274481534958, 0.06392261385917664, -0.12862488627433777, -0.028715286403894424, -0.06104600802063942]}, "authors": [{"authorId": "2058258448", "name": "J. Heo"}, {"authorId": "2269734156", "name": "Seyedarmin Azizi"}, {"authorId": "144423756", "name": "A. Fayyazi"}, {"authorId": "69467609", "name": "M. Pedram"}], "references": [{"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "10bd38673951f5d7729568284093cbd80482ab16", "title": "Vision Transformers Need Registers"}, {"paperId": "d193675b92fbfbf22ed82fda35cd2e73587e33bd", "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f", "title": "Token Merging: Your ViT But Faster"}, {"paperId": "eaf8c94d8579be7c356ae161250c59f452d9b557", "title": "Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"}, {"paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84", "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"}, {"paperId": "722d71a19e4049b30a03d1028158881560432135", "title": "SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning"}, {"paperId": "c2a0c18e810535db52e5ebaf180c64ce70356748", "title": "A-ViT: Adaptive Tokens for Efficient Vision Transformer"}, {"paperId": "8144ca1f78c045cb001815090bcf8a726e37e0ad", "title": "Adaptive Token Sampling for Efficient Vision Transformers"}, {"paperId": "5a060dc5a5bcac0879f17841a5cf70bb4302cc47", "title": "Token Pooling in Vision Transformers"}, {"paperId": "cf5e6e3c50a798d87033e0e108e88b3647738bbe", "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers"}, {"paperId": "97d8823ca3c9bd932cec8ad6f3b194168e7cec92", "title": "Knowledge distillation: A good teacher is patient and consistent"}, {"paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"}, {"paperId": "21336e57dc2ab9ae2171a0f6c35f7d1aba584796", "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "a84689ef8eaf38344eb3de24850ec0720c815605", "title": "Synchronous Transformers for end-to-end Speech Recognition"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "295065d942abca0711300b2b4c39829551060578", "title": "BERTScore: Evaluating Text Generation with BERT"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2a94c84383ee3de5e6211d43d16e7de387f68878", "title": "Feature Pyramid Networks for Object Detection"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0", "title": "Fully convolutional networks for semantic segmentation"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "1f88427d7aa8225e47f946ac41a0667d7b69ac52", "title": "What is the best multi-stage architecture for object recognition?"}, {"paperId": "1e80f755bcbf10479afd2338cec05211fdbd325c", "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations"}, {"paperId": "3efcb97c1de1c87832a7a1d99e91801992a938ec", "title": "Crafting Papers on Machine Learning"}, {"paperId": null, "title": "importance predictor using the Gumbel-Softmax distribution"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "eaa09c607780373cc809bce89b6b28b17e301f27", "title": "TokenLearner: Adaptive Space-Time Tokenization for Videos"}, {"paperId": "0d8be19e00af83388523baf86f8cdf682302a0d1", "title": "SPViT: Enabling Faster Vision Transformers via Soft Token Pruning"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "Rethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models"}, {"paperId": null, "title": "Although these methods are effective post-deployment"}]}