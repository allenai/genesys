{"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models", "abstract": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.", "venue": "arXiv.org", "year": 2023, "citationCount": 1374, "influentialCitationCount": 84, "openAccessPdf": {"url": "https://arxiv.org/pdf/2303.18223", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation."}, "embedding": {"model": "specter_v2", "vector": [0.011075285263359547, 0.4682743549346924, -0.14375187456607819, -0.09644269198179245, -0.4314817488193512, -0.21331848204135895, 0.4680566191673279, -0.4812714457511902, -0.6433122158050537, -0.07233286648988724, 0.33013927936553955, 0.07495489716529846, 0.37303251028060913, 0.024351319298148155, -0.5764523148536682, 0.2021285593509674, -1.093629240989685, 0.7826423048973083, 0.06591412425041199, -0.44862374663352966, -0.5197598338127136, -0.776313066482544, -0.7351605296134949, 0.08255929499864578, 0.5648384094238281, 0.038000158965587616, 0.3126695156097412, 0.7740775346755981, -0.3736640214920044, -0.03249579295516014, 0.21541938185691833, -0.5281193256378174, 0.28702232241630554, -0.14068396389484406, -0.1162463054060936, -0.19859690964221954, 0.3029990494251251, -0.5522757172584534, -0.4918782114982605, 0.780604362487793, -0.0896744355559349, 0.19407106935977936, 0.2731083333492279, -0.5744141936302185, -0.5318149328231812, 1.0801687240600586, 0.8312017917633057, 1.015162706375122, -0.44082310795783997, -0.4728263020515442, 1.190566897392273, -1.1334898471832275, 0.18884369730949402, 1.7497506141662598, 0.28691259026527405, 0.6020140051841736, -0.25577643513679504, -0.9387679696083069, 0.3678303062915802, -0.1906324177980423, -1.0062644481658936, -0.383821576833725, -0.026869431138038635, 0.09726177155971527, 2.0609800815582275, -0.5039105415344238, 0.12722228467464447, 0.5032103061676025, -0.22395934164524078, 1.080454707145691, -0.5380345582962036, -1.147679090499878, -0.3244803845882416, 0.049867939203977585, -0.19348035752773285, 1.001677393913269, -0.2841091454029083, 0.07927917689085007, -0.8118108510971069, -0.2714247405529022, 0.8607791066169739, -0.4968551993370056, 0.050264377146959305, -0.07729490846395493, -0.6102489233016968, 1.1928746700286865, 0.2510795593261719, 0.8329409956932068, -0.12325350940227509, 0.4441048502922058, 0.38267645239830017, 0.6852718591690063, -0.0713232085108757, 0.5639535188674927, -0.33352282643318176, 0.41895395517349243, -0.5685581564903259, 0.09616123884916306, -0.012957884930074215, 0.9079297780990601, -0.3064434230327606, 0.43095535039901733, -0.7397435903549194, 0.6736921668052673, 1.22330641746521, 0.26021212339401245, 0.44475290179252625, -0.8138720989227295, 0.489288330078125, -0.4042867422103882, 0.21180453896522522, -0.0562862902879715, -0.31580469012260437, -0.21489940583705902, -0.8869951963424683, -1.4196490049362183, -0.1694604456424713, 0.05598880350589752, -0.810984194278717, 1.0042307376861572, -0.4415079653263092, -0.29790669679641724, 0.19424179196357727, 0.43605348467826843, -0.0033358426298946142, 0.8794978260993958, 0.44842568039894104, -0.2591366469860077, 0.7996833324432373, -0.7166272401809692, -0.6731716990470886, -1.2134729623794556, 0.7650331854820251, -0.12126795947551727, 0.45150089263916016, -0.2873833477497101, -1.3318047523498535, -0.7404804825782776, -0.5784223675727844, 0.11122273653745651, -0.3100956678390503, 0.6183293461799622, 1.2117290496826172, 0.12996678054332733, -0.9593279361724854, 0.29605942964553833, -0.1465834081172943, -0.37287452816963196, -0.007878699339926243, 0.36546334624290466, -0.002259053522720933, -0.5125023722648621, -1.6655619144439697, 0.28246912360191345, 0.3490699827671051, -0.5092036128044128, -0.20038002729415894, 0.026015233248472214, -1.066900372505188, -0.18161195516586304, 0.04928778484463692, -0.5015262961387634, 1.3268299102783203, -0.09016155451536179, -1.3032753467559814, 0.5803859829902649, -0.3682488799095154, -0.21581801772117615, 0.14571674168109894, -0.0718468651175499, -0.6849194169044495, -0.6724995374679565, -0.4293447434902191, 0.4178260266780853, 0.5025374293327332, 0.09988238662481308, -0.08689876645803452, 0.4501098096370697, 0.11006199568510056, -0.009035468101501465, -0.3451346158981323, 1.1389870643615723, -0.8332303166389465, -0.2414277046918869, 0.34257858991622925, 0.5950304269790649, -0.2678748369216919, -0.30972304940223694, -0.10391516983509064, -0.9585721492767334, 0.2024717777967453, -0.11462902277708054, 1.4286906719207764, -0.7766080498695374, -0.31982421875, -0.2177361398935318, -0.43444889783859253, 0.0635501891374588, -0.7860002517700195, 0.988715410232544, 0.04353828728199005, 0.3342789113521576, -0.37918469309806824, -1.0168647766113281, -0.08222820609807968, -0.13390618562698364, -0.5968323945999146, -0.15721958875656128, 0.0589447021484375, 0.9930580854415894, -0.5980339646339417, 0.10994988679885864, 0.06062139943242073, 0.05660701170563698, -1.1857348680496216, 1.3787130117416382, -0.6263594031333923, 0.19767595827579498, 0.001463881111703813, -0.15065991878509521, 0.005809437483549118, -0.2650178074836731, 0.4376438558101654, -0.07539359480142593, -0.22287726402282715, 0.2369752675294876, -0.4188763499259949, 1.5403602123260498, -0.1605405956506729, 0.2623557448387146, 0.017358947545289993, -0.5309150218963623, -0.10471236705780029, 0.6928317546844482, -0.4576352834701538, -0.3992011845111847, 0.11631885915994644, 0.44484856724739075, -0.2245817333459854, 0.3806564509868622, 0.5298667550086975, 0.3480241596698761, -0.5440547466278076, 0.19229446351528168, 0.5755584836006165, -0.2917727530002594, 0.7827938795089722, 0.39113980531692505, 0.4415128231048584, -0.16316451132297516, 0.6282908320426941, 0.13622236251831055, 0.5286457538604736, -0.7513765692710876, -0.13746942579746246, 0.428473562002182, 0.6884008646011353, 0.6474592685699463, 0.10049663484096527, -0.6605606079101562, -0.5147186517715454, 0.003785056062042713, 0.7381062507629395, 1.8053240776062012, -0.17812347412109375, -0.29272568225860596, -0.7368217706680298, 0.07990550249814987, -0.3955685496330261, 0.24972181022167206, -0.2611521780490875, -0.08777205646038055, -1.160264015197754, -0.9803534150123596, 1.0399036407470703, 0.3445236384868622, 0.7181931734085083, -0.6731549501419067, 0.013959401287138462, -0.24939057230949402, 0.11594758182764053, -0.8516088724136353, -0.8518472909927368, -0.03466198220849037, -0.8241946697235107, -0.004888305906206369, -0.02942437306046486, -0.5562033653259277, 0.15208472311496735, -0.748347818851471, 0.7657328248023987, -0.39613014459609985, 0.08602957427501678, -0.05808838829398155, 0.9207644462585449, -0.6913859248161316, -1.1543009281158447, 0.10332310944795609, 0.3348742127418518, -0.2602243423461914, 0.2509315311908722, 0.5560958385467529, 0.5511404871940613, 0.2593143582344055, -0.5736588835716248, 0.3439238965511322, 0.4396313428878784, 0.10214152187108994, 0.7421171069145203, -0.5263103246688843, -0.008370932191610336, -1.2743510007858276, 1.0921984910964966, 0.5440437197685242, -0.7917841076850891, 0.6787882447242737, -0.3541659712791443, -0.2353600412607193, 0.6028098464012146, -0.938675582408905, -0.3113701641559601, -0.9292489886283875, 0.24658893048763275, -0.14169232547283173, -0.08118631690740585, 0.3598179519176483, 0.198264941573143, 0.19504274427890778, 0.3515315055847168, 0.6326956152915955, 0.39793190360069275, -0.3052275776863098, 0.5033168196678162, -0.7844771146774292, 0.2761191129684448, 0.36176222562789917, 0.44586795568466187, -0.32693198323249817, -0.7683282494544983, -0.46413224935531616, -0.1395834982395172, -0.35012656450271606, -0.3395623564720154, -0.02417105622589588, 0.004533803090453148, -0.7596885561943054, -0.36948394775390625, 0.23773954808712006, -1.0908138751983643, -0.39364737272262573, 0.43133997917175293, 0.0839989185333252, -0.05079246684908867, -1.0081651210784912, -1.392622709274292, -0.41615021228790283, -0.5725477933883667, -0.7567470073699951, 0.44994670152664185, 0.08410069346427917, -0.0061684781685471535, -0.5472486615180969, 0.33495840430259705, -0.25999510288238525, 1.298401951789856, -1.0911235809326172, 1.269271969795227, 0.050634175539016724, -0.20563451945781708, -0.4150817096233368, 0.4283691346645355, 0.4205670654773712, -0.4011078476905823, 0.1308148354291916, -0.6950485110282898, 0.005262440070509911, -0.5778268575668335, -0.2208845466375351, -0.01002955436706543, 0.5574063658714294, 0.3087911009788513, 0.09597863256931305, -0.41431087255477905, 0.30669793486595154, 1.1659859418869019, -0.5768760442733765, -0.07739865034818649, 0.03640588000416756, 0.9123402237892151, -0.12756939232349396, -0.6264311671257019, 0.44831767678260803, 0.3513853847980499, 0.4104675352573395, -0.04177010804414749, -0.1376122087240219, 0.16815298795700073, -0.488100528717041, 0.6662122011184692, 1.9319778680801392, -0.10136217623949051, -0.4501359760761261, -0.911771297454834, 0.4462542235851288, -1.0969953536987305, -0.6322632431983948, 1.0641974210739136, 0.877461850643158, 0.5365135669708252, -0.4087076783180237, -0.27426719665527344, -0.344778448343277, 0.753600537776947, 0.15807121992111206, -0.34449058771133423, -0.6769562363624573, -0.2601260542869568, 0.017110517248511314, -0.03655662387609482, 0.6304567456245422, -0.43217208981513977, 0.9626472592353821, 14.744586944580078, 0.9142225980758667, -0.0005929016042500734, 0.7667856812477112, 0.17946772277355194, 0.3833903968334198, -0.2598246932029724, -0.34807202219963074, -1.1341933012008667, -0.4701101779937744, 1.2021182775497437, -0.144572913646698, 1.3644365072250366, 0.0646735206246376, -0.23760627210140228, 0.3483254909515381, -0.32307422161102295, 0.7362027764320374, 0.5936415791511536, -1.110878348350525, 0.8772581219673157, 0.16403567790985107, 0.25285667181015015, 0.8605613112449646, 0.39786362648010254, 1.2358615398406982, 0.3362911343574524, -0.31850555539131165, 0.5330748558044434, 0.1035441979765892, 0.7820660471916199, 0.17682026326656342, 0.5686100721359253, 0.9309632182121277, -0.9359795451164246, -0.3595878779888153, -0.8999860882759094, -1.2472333908081055, 0.47577977180480957, -0.13708847761154175, -0.47588372230529785, -0.6576653718948364, -0.404471218585968, 0.34392261505126953, 0.0907694399356842, 0.5788223743438721, -0.2565477192401886, 0.9106485247612, -0.68910151720047, 0.1998569816350937, 0.03086058422923088, 0.13995157182216644, 0.4031037390232086, -0.16442185640335083, 0.13813799619674683, -0.055498283356428146, 0.2604595422744751, 0.33936408162117004, -0.8512618541717529, 0.1826624870300293, -0.390712708234787, -0.6486379504203796, 0.11362676322460175, 0.5173979997634888, 0.6892416477203369, 0.1263333261013031, -0.4021921157836914, 0.29138487577438354, 0.8929083943367004, 0.3082130253314972, -0.4318782687187195, 0.15060476958751678, 0.2873891294002533, -0.6091792583465576, -0.05944692716002464, 0.4789372980594635, 0.1327325403690338, -0.7865287065505981, -0.42514297366142273, -0.19537800550460815, 0.5249229073524475, -0.6031917333602905, -0.7750674486160278, 1.1676725149154663, 0.017452063038945198, -0.4554252028465271, 0.06619299203157425, -0.6760486960411072, -0.05624347925186157, 0.5230230689048767, -1.2947255373001099, -0.875399112701416, 0.5408148765563965, -0.0674835667014122, -0.2311154007911682, -0.327444851398468, 1.7535808086395264, 0.19767193496227264, -0.6427587270736694, 0.17641694843769073, 0.30214354395866394, 0.2029065638780594, -0.2680937945842743, -0.32241329550743103, 0.861862063407898, 0.47104179859161377, 0.0211009718477726, 0.4460393190383911, -0.050592996180057526, 0.2654713988304138, -0.8305442333221436, 0.0898452177643776, 1.0240048170089722, -0.8368780612945557, -0.41866207122802734, -0.9810311794281006, -0.9386614561080933, 0.43870627880096436, 0.45667344331741333, -0.4768420159816742, 0.15286464989185333, 0.12521415948867798, -0.48678067326545715, 0.01387323159724474, -0.6816413402557373, -0.07162586599588394, 0.6841806769371033, -0.787992000579834, -0.3118959665298462, 0.1462460458278656, 0.5463678240776062, -1.0884140729904175, -0.48647773265838623, -0.3373342752456665, -0.2007097452878952, 0.33140477538108826, 0.9597758054733276, -0.5219324827194214, -0.028081310912966728, 0.8595697283744812, -0.136524498462677, -0.7706778645515442, -0.26056361198425293, -1.0136545896530151, 0.17482328414916992, -0.08131973445415497, 0.9285281896591187, -0.5780541896820068, -0.3189476728439331, 0.9133244156837463, 0.2443062961101532, -0.05064794421195984, -0.9348101615905762, -0.42389488220214844, 0.3057411015033722, -0.6351516246795654, 0.3539596498012543, -0.28419315814971924, -0.12158538401126862, 0.4482538402080536, 0.16080832481384277, 0.9024466872215271, -0.5149461030960083, -0.5815714001655579, 0.5135582685470581, -0.06444258242845535, -0.01992535963654518, -0.40291792154312134, -0.12115897238254547, -1.3440643548965454, 0.07919065654277802, -1.1006863117218018, 0.1736743301153183, -1.1538035869598389, -0.4843987822532654, 0.14543814957141876, 0.09225346893072128, -0.0020261637400835752, 0.6260274052619934, -0.3315810263156891, -0.5111531019210815, -0.0012791769113391638, -0.5023759007453918, 0.8163864016532898, 0.8680481910705566, -0.6553872227668762, -0.17136012017726898, -0.008850464597344398, 0.47922664880752563, 0.3467806875705719, 0.5426459908485413, -0.4429250657558441, -0.7112759947776794, -1.8019497394561768, 0.3013584315776825, -0.16098226606845856, -0.3972889482975006, -0.733515202999115, 0.6707097291946411, 0.24535302817821503, -0.5416503548622131, 0.5083751082420349, 0.6338306069374084, -0.7514358758926392, -0.49993595480918884, 0.2030838429927826, -0.7415011525154114, 0.5596064925193787, 0.3134423792362213, -0.6070800423622131, -0.4175949990749359, 0.8139671087265015, -0.1341058760881424, -1.0893243551254272, -0.5922499895095825, 0.3970469832420349, -0.8380796313285828, 0.1912064105272293, -0.26471954584121704, 0.12898074090480804, -0.8829212784767151, -0.3219253420829773, 0.10911648720502853, 0.31927379965782166, -0.36929574608802795, 0.699611246585846, 0.4911840260028839, -0.8156895041465759, -0.13777413964271545, 0.8673056364059448, 0.09606698155403137, -0.09635157138109207, 0.33787617087364197, 0.4532058835029602, -0.4522359371185303, 1.0156937837600708, 0.4032808840274811, 0.5337663888931274, -0.8432081937789917, -0.11131294071674347, 0.76834636926651, -0.4403116703033447, -0.18080094456672668, 1.4294136762619019, -0.2989756464958191, -1.3809139728546143, 0.2694265842437744, -1.3379875421524048, -0.7028782963752747, -0.9046340584754944, 0.694092333316803, -0.30114373564720154, -0.27212244272232056, -0.3693470358848572, -0.327490895986557, 0.17477209866046906, 0.19132886826992035, -0.49331387877464294, 0.5693483948707581, -0.2501696050167084, -0.49155279994010925, 0.8571754693984985, 0.44724971055984497, -0.5119802355766296, -0.3453879952430725, -0.6713904738426208, -0.4466758370399475, 0.08396761119365692, -0.031221190467476845, -0.5083678364753723, -0.4876396954059601, 0.8834985494613647, 0.381827712059021, 0.15520401298999786, 0.05087799206376076, 0.00511420052498579, 0.27633360028266907, 0.9045957922935486, 0.4158441126346588, -0.7329436540603638, -0.9538148641586304, 1.557221531867981, 1.3914891481399536, -1.0713402032852173, -0.12346766144037247, -0.29761433601379395, -1.0881232023239136, 0.8205353021621704, 0.41263121366500854, 0.16491781175136566, 0.8213180899620056, -0.3827509582042694, 0.20810391008853912, 0.19075992703437805, -1.1057276725769043, -0.16169220209121704, 0.8040664792060852, 0.548022985458374, 0.7996257543563843, 0.5554778575897217, 0.0434085838496685, 0.8747616410255432, -0.011876906268298626, 0.32179200649261475, 0.3081510663032532, 0.3539848029613495, -0.30719614028930664, -0.1072421446442604, 0.0006465347833000124, 0.5523611307144165, -0.2545062005519867, -0.7099237442016602, 0.020342735573649406, 0.7867780327796936, 0.05547241494059563, 0.5133785605430603, 0.7490586638450623, 0.22993925213813782, 0.6005494594573975, 0.4303867816925049, 0.5324640274047852, -0.6946584582328796, -0.19270581007003784, 0.06940406560897827, -0.4870009422302246, -0.02714744210243225, -0.2296205759048462, -0.36418306827545166, -0.21182267367839813, -0.10877491533756256, -0.07342787832021713, 0.21767441928386688, 0.28864067792892456, 1.1279910802841187, 0.5194246172904968, -0.04352352395653725, -0.756209135055542, -0.3906365931034088, -0.5151905417442322, -1.4927791357040405, -0.10524146258831024, -0.5403741002082825, -0.4463554620742798, -0.18953391909599304, -0.046394847333431244, -0.34588807821273804]}, "authors": [{"authorId": "2542603", "name": "Wayne Xin Zhao"}, {"authorId": "1423651904", "name": "Kun Zhou"}, {"authorId": "2018027", "name": "Junyi Li"}, {"authorId": "1997234792", "name": "Tianyi Tang"}, {"authorId": "72541556", "name": "Xiaolei Wang"}, {"authorId": "151472453", "name": "Yupeng Hou"}, {"authorId": "2007666579", "name": "Yingqian Min"}, {"authorId": "2107926615", "name": "Beichen Zhang"}, {"authorId": "2155570461", "name": "Junjie Zhang"}, {"authorId": "2198280871", "name": "Zican Dong"}, {"authorId": "2111895473", "name": "Yifan Du"}, {"authorId": "2181967397", "name": "Chen Yang"}, {"authorId": "2109315001", "name": "Yushuo Chen"}, {"authorId": "46842323", "name": "Z. Chen"}, {"authorId": "2118240359", "name": "Jinhao Jiang"}, {"authorId": "1708171825", "name": "Ruiyang Ren"}, {"authorId": "2209136299", "name": "Yifan Li"}, {"authorId": "2109887979", "name": "Xinyu Tang"}, {"authorId": "2119618242", "name": "Zikang Liu"}, {"authorId": "2108129670", "name": "Peiyu Liu"}, {"authorId": "50204644", "name": "J. Nie"}, {"authorId": "153693432", "name": "Ji-rong Wen"}], "references": [{"paperId": "ce913026f693101e54d3ab9152e107034d81fce1", "title": "Holistic Evaluation of Language Models"}, {"paperId": "114cc72d93c73b97ba03ed8c4e4a8d937b344607", "title": "An Efficient 2D Method for Training Super-Large Deep Learning Models"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "4d7571441f507f39133209e8afa7ad088da2199c", "title": "ChatGPT Is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models"}, {"paperId": "a9e155fda1d97baa2b8712f580cc61887cc64e9b", "title": "ChatGPT outperforms crowd workers for text-annotation tasks"}, {"paperId": "060ab95c38eebe71c28d9bab81de32b934a54f70", "title": "unarXive 2022: All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network"}, {"paperId": "763d953e671e2b6c6d0df2f5bc5472fb6ce074de", "title": "The utility of ChatGPT for cancer treatment information"}, {"paperId": "574beee702be3856d60aa482ec725168fe64fc99", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"}, {"paperId": "7d40189f3fa56728b8d210628e98fc204961778f", "title": "Can we trust the evaluation on ChatGPT?"}, {"paperId": "6d3f29545fa059f7d24e27aaa9351750636d12ce", "title": "On the Educational Impact of ChatGPT: Is Artificial Intelligence Ready to Obtain a University Degree?"}, {"paperId": "348a1efa54376fa39053e5e25d52bd0eb6a0ba68", "title": "Capabilities of GPT-4 on Medical Challenge Problems"}, {"paperId": "362cbfd0d05e139cd6cf049754098a6e1520b910", "title": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"}, {"paperId": "fc3fe217de5d8de1a5814daf94de52e0d941cf21", "title": "Mind meets machine: Unravelling GPT-4's cognitive psychology"}, {"paperId": "a70339b84db86a974335499eb824ebdab3a422b3", "title": "Large Language Model Instruction Following: A Survey of Progresses and Challenges"}, {"paperId": "98b40eb3ce79c24d9726556947e2e2094737fe46", "title": "A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models"}, {"paperId": "9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5", "title": "A Short Survey of Viewing Large Language Models in Legal Aspect"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "0100785773b8217c44606ab260e3212f93b0a4fd", "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!"}, {"paperId": "f93d5d62a227d0c4ae85c08d7de07d7c2ce28a28", "title": "Consistency Analysis of ChatGPT"}, {"paperId": "407b9e9478ba6bff43ce4b20e8b6cb2b303477d2", "title": "Planning with Large Language Models for Code Generation"}, {"paperId": "af997821231898a5f8d0fd78dad4eec526acabe5", "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"}, {"paperId": "b92b8a32f89f45cd771359e3351f6ddcad61450c", "title": "ChatGPT Participates in a Computer Science Exam"}, {"paperId": "bdf7bf9e81a6c12e22323d0402885b2ba62f623e", "title": "Does Synthetic Data Generation of LLMs Help Clinical Text Mining?"}, {"paperId": "a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5", "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "68b560859078171978f2c040b1522f4e7668c38e", "title": "On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "b626560f19f815808a289ef5c24a17c57320da70", "title": "MathPrompter: Mathematical Reasoning using Large Language Models"}, {"paperId": "da872dfc0934f554311d5f91fa7e5ada44fb8155", "title": "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT"}, {"paperId": "72f0acc688f3d7968c5783a39bc3469c6c6282fc", "title": "In AI, is bigger always better?"}, {"paperId": "1f040c3a8d49f8e54169a0e07013692c7d58de4b", "title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks"}, {"paperId": "72d0ad6fca3a679bf9c5011ebe15239fed7672d8", "title": "ChatGPT: potential, prospects, and limitations"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "dc385646887a3669ae0ee506a263d592f4f7c7a6", "title": "Finding Support Examples for In-Context Learning"}, {"paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c", "title": "Language Is Not All You Need: Aligning Perception with Language Models"}, {"paperId": "272afc28d03890160b1f2808cc551c962ea9138c", "title": "ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics"}, {"paperId": "e5c72b92c48d68594b290c84a8904da7c8335554", "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback"}, {"paperId": "5848737f78397f72ceae2ba6f3419a6a8502b8ba", "title": "ChatGPT: Jack of all trades, master of none"}, {"paperId": "6839816cd3ab194dfa3ffe3066cc35d099820e00", "title": "Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT"}, {"paperId": "3599a236f285af48782fc30b1341d13ec7320735", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"}, {"paperId": "5f5253fb15ac382e96ade0335baf1cfaa240fb1d", "title": "Can GPT-3 Perform Statutory Reasoning?"}, {"paperId": "027ec9a2aaa81b01d190e8607b2250779e5834dd", "title": "Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information"}, {"paperId": "53d128ea815bcc0526856eb5a9c42cc977cb36a7", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools"}, {"paperId": "5c125a2d0ed00185a08c67ee64b9d29250fd5274", "title": "ChatGPT-3.5 as writing assistance in students\u2019 essays"}, {"paperId": "873a581320d928249609d3c07229d5af182a379c", "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"}, {"paperId": "bf8491bef353df126e2306ad2fe4b898697b906a", "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity"}, {"paperId": "cd0988714ea326642d2b1bb18753e187fec71e42", "title": "A Categorical Archive of ChatGPT Failures"}, {"paperId": "0b58f4ec8cbf6f63fb65b7e3c368cf511eadecd3", "title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning"}, {"paperId": "3d48f1b4f69f7d22a47fa1599a1fc76277ba0c65", "title": "Evaluating Large Language Models in Theory of Mind Tasks"}, {"paperId": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050", "title": "Multimodal Chain-of-Thought Reasoning in Language Models"}, {"paperId": "69619a2a47faee7a29ec596db13172e2a42ff921", "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models"}, {"paperId": "b115c1e1e9e51f8ad7d47b745bc04e29a654b84d", "title": "Faithful Chain-of-Thought Reasoning"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "fbd49b25bdab98c171af49962a41139c73dacbde", "title": "Specializing Smaller Language Models towards Multi-Step Reasoning"}, {"paperId": "1bfe1e6626067e9075570c575153e2b366f9d6cf", "title": "Could an artificial-intelligence agent pass an introductory physics course?"}, {"paperId": "fdbe8d1896b754125c6255f07bd9fecc2ea59127", "title": "Putting ChatGPT's Medical Advice to the (Turing) Test"}, {"paperId": "cb29cf52f0f7d2e4324c68690a55b22890f2212d", "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection"}, {"paperId": "23cae400cfd1a7c455c721256b838e98a307d5e6", "title": "ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports"}, {"paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d", "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"}, {"paperId": "06edda0310b4ec7c5012d012349252a3a77521b6", "title": "Prompt-Augmented Linear Probing: Scaling Beyond The Limit of Few-shot In-Context Learners"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45", "title": "Towards Reasoning in Large Language Models: A Survey"}, {"paperId": "2dbec38fe353ab0e495ad09263389dbc9260824d", "title": "A Survey of Deep Learning for Mathematical Reasoning"}, {"paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44", "title": "Large Language Models Are Reasoning Teachers"}, {"paperId": "1bed34f2c23b97fd18de359cf62cd92b3ba612c3", "title": "Execution-Based Evaluation for Open-Domain Code Generation"}, {"paperId": "961b82ee828ca70dc391caa2b2053a70e5653b15", "title": "The End of Programming"}, {"paperId": "6845bea94b2fb17d4377b3bb2bd10f73a959f9cc", "title": "Reasoning with Language Model Prompting: A Survey"}, {"paperId": "6f4cc536f9ed83d0dbf7e919dc609be12aa0848a", "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor"}, {"paperId": "8822357efe500caded16e603d21239be3a39547c", "title": "ChatGPT: The End of Online Exam Integrity?"}, {"paperId": "34bc28087e1d6f047e2736791f79d769293f447c", "title": "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale"}, {"paperId": "126a4776ff8315fd506766cb8f3c722cf746ad9e", "title": "Teaching Small Language Models to Reason"}, {"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent"}, {"paperId": "34ff1da13770908ef0bf389365cdde743d3c9db1", "title": "Diverse Demonstrations Improve In-context Compositional Generalization"}, {"paperId": "3eed4de25636ac90f39f6e1ef70e3507ed61a2a6", "title": "Talking about Large Language Models"}, {"paperId": "ec27f85979899a4193a8ec3b932ddb677c59be62", "title": "Legal Prompt Engineering for Multilingual Legal Judgement Prediction"}, {"paperId": "cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3", "title": "Legal Prompting: Teaching a Language Model to Think Like a Lawyer"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "3a07a87090a061ca41dd30ac8398a9a5d9d39826", "title": "Dense Text Retrieval Based on Pretrained Language Models: A Survey"}, {"paperId": "097dc73d5d422b3c09286e72d16b2561ae5fb395", "title": "Complementary Explanations for Effective In-Context Learning"}, {"paperId": "6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7", "title": "PAL: Program-aided Language Models"}, {"paperId": "8a4fc5f00cd4aca61e148e46a2125c3a406719f1", "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "b8bd29a6104d26a16687400049a4e7e026ae6258", "title": "Active Example Selection for In-Context Learning"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "7de36d6b14aadc8cdb6ad1340b9ca64b15375bca", "title": "Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"}, {"paperId": "1bb6d5761903c7ac978188ae36e2648905e95dc5", "title": "Transcending Scaling Laws with 0.1% Extra Compute"}, {"paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd", "title": "Large Language Models Can Self-Improve"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "663a41c866d49ce052801fbc88947d39764cad29", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"}, {"paperId": "82cd40e926300b6b18c34ced2edeb07e84d9d6c7", "title": "Learning Instructions with Unlabeled Data for Zero-Shot Cross-Task Generalization"}, {"paperId": "39e40821b7207125e54e6ed7112e55cd38c6f0c3", "title": "Language Models of Code are Few-Shot Commonsense Learners"}, {"paperId": "2aab6ca1a8dae3f3db6d248231ac3fa4e222b30a", "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision"}, {"paperId": "e070ff286709db28312e08b52b05539debe88146", "title": "Measuring and Narrowing the Compositionality Gap in Language Models"}, {"paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "title": "Automatic Chain of Thought Prompting in Large Language Models"}, {"paperId": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57", "title": "Language Models are Multilingual Chain-of-Thought Reasoners"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a", "title": "Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought"}, {"paperId": "c88cafa3e980765a64febe369ceb7c2aa7261d2a", "title": "Complexity-Based Prompting for Multi-Step Reasoning"}, {"paperId": "74eae12620bd1c1393e268bddcb6f129a5025166", "title": "Improving alignment of dialogue agents via targeted human judgements"}, {"paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f", "title": "In-context Learning and Induction Heads"}, {"paperId": "e86009d9f9b1cdf083a48d087552bc4153784451", "title": "Promptagator: Few-shot Dense Retrieval From 8 Examples"}, {"paperId": "c03fa01fbb9c77fe3d10609ba5f1dee33a723867", "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"}, {"paperId": "d219d971b16c708b8debc731e0c541ae218c7caf", "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese"}, {"paperId": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c", "title": "Code as Policies: Language Model Programs for Embodied Control"}, {"paperId": "4988b3d378b79eb8669112620baf1ff4e3e536fd", "title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango"}, {"paperId": "557a5147d88ad361b807d4c93decac6d57d2d5e9", "title": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans"}, {"paperId": "86d0d3855f94105e25d81cab9f3d269c6062a9c4", "title": "Selective Annotation Makes Language Models Better Few-Shot Learners"}, {"paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7", "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "2a7ae3e98357569c41424dacd60c62d3df78a0db", "title": "Limitations of Language Models in Arithmetic and Symbolic Induction"}, {"paperId": "398e4061dde8f5c80606869cebfa2031de7b5b74", "title": "Few-shot Learning with Retrieval Augmented Language Models"}, {"paperId": "914254fac74a2da051cccf6ca16afcaad416a079", "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"}, {"paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"}, {"paperId": "20b919412b474fdaafc9cc57a4b759798fda21cc", "title": "ScienceQA: a novel resource for question answering on scholarly articles"}, {"paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "title": "Can large language models reason about medical questions?"}, {"paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "title": "Exploring Length Generalization in Large Language Models"}, {"paperId": "e19b54ad4c1c8af045069e9cac350ffc2ce60e1a", "title": "No Language Left Behind: Scaling Human-Centered Machine Translation"}, {"paperId": "b17cc18e4130505b939f7d527082eb6be2a7fd5b", "title": "Rationale-Augmented Ensembles in Language Models"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "dcbf62f17dad0f4554f91c822d141fb92f78429a", "title": "Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "cd31ce657fba5dcdd74c557170bbd9ef0636cb13", "title": "JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "c28e95a06dfcf13fc65a1cac83722f53e34f12a5", "title": "Autoformalization with Large Language Models"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "354bf043179e3e9f05df73e3f04517e53c326d1f", "title": "TALM: Tool Augmented Language Models"}, {"paperId": "c2d574f7c6a9e3bafe396ecb4ab639179d6fd92c", "title": "Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers"}, {"paperId": "5437e8adab596d7294124c0e798708e050e25321", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"}, {"paperId": "aa4d9972af3264d032dbee58501ed4ac49477103", "title": "Scaling Laws and Interpretability of Learning from Repeated Data"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "1fafaccebc4a74898a74c606f846318c4c2c7536", "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"}, {"paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd", "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da", "title": "InCoder: A Generative Model for Code Infilling and Synthesis"}, {"paperId": "15190e8b459bd85d546286f7d7da61b4f4f3f58a", "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "8666f9f379389a5dff31e72fb0f992a37763ba41", "title": "Teaching language models to support answers with verified quotes"}, {"paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af", "title": "Compression of Generative Pre-trained Language Models via Quantization"}, {"paperId": "3f4d11971f2c64be9125a7fe99c019588bbebf16", "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought"}, {"paperId": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd", "title": "Internet-augmented language models through few-shot prompting for open-domain question answering"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca", "title": "DeepNet: Scaling Transformers to 1, 000 Layers"}, {"paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "title": "A systematic evaluation of large language models of code"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d", "title": "Quantifying Memorization Across Neural Language Models"}, {"paperId": "55c36748f2a7c060c3313349c730b053ed03fbf7", "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models"}, {"paperId": "996445d847f06e99b0bd259345408a0cf1bce87e", "title": "Locating and Editing Factual Associations in GPT"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64", "title": "Red Teaming Language Models with Language Models"}, {"paperId": "916a06a6d51aa93de27aac2f3e14faed08dd6706", "title": "Formal Mathematics Statement Curriculum Learning"}, {"paperId": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca", "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts"}, {"paperId": "c2536182c010c41941e8a031071a1880c34cec60", "title": "Unified Scaling Laws for Routed Language Models"}, {"paperId": "1415479215dcfec5e2ee0d33f1e1565ae2c65bb9", "title": "Examining Scaling and Transfer of Language Model Architectures for Machine Translation"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "03488f1a193066b5ea8b9b800e119f07df5c1d9e", "title": "Reasoning Like Program Executors"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"}, {"paperId": "79950179d60ba39a74d5fe2aedc47a57c0bf4c03", "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"}, {"paperId": "a3184d40d390793232c99c89b57b8f65c16320b2", "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "f9838a3be5c94bb2674a0e224de349b50e18f3c4", "title": "Learning To Retrieve Prompts for In-Context Learning"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e", "title": "A General Language Assistant as a Laboratory for Alignment"}, {"paperId": "92173d081b15824d22a9ef070e118744ceee8052", "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"}, {"paperId": "cbf98ebe967e0f3f3236e7932f37013b98244e94", "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7", "title": "MetaICL: Learning to Learn In Context"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "3186b9dd1331b647cf3304d185c248ea7ec9ad1b", "title": "OneFlow: Redesign the Distributed Deep Learning Framework from Scratch"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79", "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"}, {"paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156", "title": "Recursively Summarizing Books with Human Feedback"}, {"paperId": "a6d8d04962f84ae6225e72723869a002b9fc8036", "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "7ba98b00a224094c09676090f5d6d69498f5b299", "title": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "7088e2b21e88b698ae5ed38008c46fa170c6e987", "title": "Complex Knowledge Base Question Answering: A Survey"}, {"paperId": "c0c9f77cb097f2ce53feb91802bcfbae57fcc42f", "title": "BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d", "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models"}, {"paperId": "789b8487da7188442085983caba3ffaae05531e9", "title": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"}, {"paperId": "659b95b28a9f3b6cfa3bd06fa7bd004165db5663", "title": "Tesseract: Parallelize the Tensor Parallelism Efficiently"}, {"paperId": "d8df456f790381f4ddb388be24a546625bd75ee2", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks"}, {"paperId": "16e623059ffccab60f4c35be028a2d4f10933515", "title": "Sequence Parallelism: Long Sequence Training from System Perspective"}, {"paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa", "title": "CogView: Mastering Text-to-Image Generation via Transformers"}, {"paperId": "d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5", "title": "Pretrained Language Models for Text Generation: A Survey"}, {"paperId": "1ccd031f28dccfb226f6c0c588c93a97a50bf95f", "title": "Measuring Coding Challenge Competence With APPS"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "2c871df72c52b58f05447fcb3afc838168d94505", "title": "Knowledge Neurons in Pretrained Transformers"}, {"paperId": "0adec918885dff698acf359988ed79a543157f80", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"}, {"paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c", "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"}, {"paperId": "7fa273f450251523e6b7fcc2eb3fdbdfd4a30493", "title": "CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"}, {"paperId": "4a56f72b9c529810ba4ecfe9eac522d87f6db81d", "title": "Explaining Answers with Entailment Trees"}, {"paperId": "53e161d4434576355fc5f63fe56afd8e135174b2", "title": "proScript: Partially Ordered Scripts Generation via Pre-trained Language Models"}, {"paperId": "bb8d6327497eff7cfc0d2e911698eff2a729a387", "title": "ExplaGraphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "49f905eb03958c7cfae52ac759ea8978b8b2a6ea", "title": "Alignment of Language Agents"}, {"paperId": "4cc1fb128fa3abf6f90d567744767e8fd6315e1d", "title": "NaturalProofs: Mathematical Theorem Proving in Natural Language"}, {"paperId": "238eb420c472bfdb1b4d34f9f53abec51f307a6b", "title": "FastMoE: A Fast Mixture-of-Expert Training System"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "13c4e5a6122f3fa2663f63e49537091da6532f35", "title": "Are NLP Models really able to Solve Simple Math Word Problems?"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"paperId": "f577654d9dd29d88c6db9ee39a4fd831573b8770", "title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models"}, {"paperId": "789b5441743c2e38cf4c38749ed820c0671d81b1", "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning"}, {"paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74", "title": "What Makes Good In-Context Examples for GPT-3?"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "346081161bdc8f18e2a4c4af7f51d35452b5cb01", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1", "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language"}, {"paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba", "title": "Extracting Training Data from Large Language Models"}, {"paperId": "d21c5a9e34a6290212c68f6ec64f65ad5c5e46f4", "title": "Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "f00f2d4b8ddd55aa2cc202f44053e5f97a254175", "title": "WikiLingua: A New Benchmark Dataset for Multilingual Abstractive Summarization"}, {"paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "5fe0a4af3bd1479d5e39fbda2215c86bce54722b", "title": "Generative Language Modeling for Automated Theorem Proving"}, {"paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1", "title": "Learning to summarize from human feedback"}, {"paperId": "750274c61a0936c9c9db32da659cddbcb227cf32", "title": "MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "2aa1d4350e80613feed88d5a6337e79693f7aa57", "title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "f13e41d24e5d0a68ca662c1b49de398a6fb68251", "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "dbeeca8466e0c177ec67c60d529899232415ca87", "title": "On Faithfulness and Factuality in Abstractive Summarization"}, {"paperId": "9b539d413393047b28bb7be9b195f142aaf7a80e", "title": "Recipes for Building an Open-Domain Chatbot"}, {"paperId": "18318b10e7c2dd4ad292208f4399eb1d4dca5768", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"}, {"paperId": "0fe2636446cd686830da3d971b31a004d6094b3c", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "80376bdec5f534be78ba82821f540590ebce5559", "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "1a6f4495474f75ae1e8bbf407f70d9a874e5b4d6", "title": "The Pushshift Reddit Dataset"}, {"paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97", "title": "Deep double descent: where bigger models and more data hurt"}, {"paperId": "f4cf4246f3882aa6337e9c05d5675a3b8463a32e", "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "8c7bf2ed2e4d8e43dce662c67518b8a9a2cdef84", "title": "LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and DBpedia"}, {"paperId": "c9b051b29feda7b62a4b683b1dfc37408724d8f5", "title": "QASC: A Dataset for Question Answering via Sentence Composition"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "0accb5cb9d06b4cdc4ceda316bc51c8ba95e6838", "title": "PaddlePaddle: An Open-Source Deep Learning Platform from Industrial Practice"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3", "title": "Language Models as Knowledge Bases?"}, {"paperId": "ea3e18c7b10a137d495054682c055a80b5be768c", "title": "Findings of the 2019 Conference on Machine Translation (WMT19)"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b", "title": "WinoGrande"}, {"paperId": "0d3c68c207fc83fb402b7217811af22066300fc9", "title": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs"}, {"paperId": "d483e97eb56230b1803a8825ebd4c3e0159ef512", "title": "DiaBLa: a corpus of bilingual spontaneous written dialogues for machine translation"}, {"paperId": "8d89f85b5f8a1d65b4e93a7ebb793618641c3ece", "title": "Assessing The Factual Accuracy of Generated Text"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "dda6fb309f62e2557a071522354d8c2c897a2805", "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"}, {"paperId": "658721bc13b0fa97366d38c05a96bf0a9f4bb0ac", "title": "Multi-Task Deep Neural Networks for Natural Language Understanding"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "7139a5f730652abbeabf9e140009907d2c7da3e5", "title": "VirtualHome: Simulating Household Activities Via Programs"}, {"paperId": "5e9c9d0164ae041786f8fdc5726da12403e91a6c", "title": "Tracking State Changes in Procedural Text: a Challenge Dataset and Models for Process Paragraph Comprehension"}, {"paperId": "d92ee995a2ba3de900ff6ffeb1949220be45c480", "title": "First Experiments with Neural Translation of Informal to Formal Mathematics"}, {"paperId": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7", "title": "Gender Bias in Coreference Resolution"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "45dfef0cc1ed96558c1c650432ce39d6a1050b6a", "title": "Fixing Weight Decay Regularization in Adam"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "4f03dcc7cc3aadae16655c87c6c882407617c725", "title": "Measuring Catastrophic Forgetting in Neural Networks"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "f0a79cb16c3e53e9c536b65ec7b2157db1d352f1", "title": "Program Synthesis"}, {"paperId": "9697d32ed0a16da167f2bdba05ef96d0da066eb5", "title": "Convolutional 2D Knowledge Graph Embeddings"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "b123a0d46ad917b79c43c5ae981e03ed2458ed11", "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "104715e1097b7ebee436058bfd9f45540f269845", "title": "Reading Wikipedia to Answer Open-Domain Questions"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "dd95f96e3322dcaee9b1e3f7871ecc3ebcd51bfe", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "1a327709cc53ff9e52454e50a643abf4a0ac92af", "title": "Findings of the 2016 Conference on Machine Translation"}, {"paperId": "4c6fe6179c408e1fbb3871af13d1a8e64f766e54", "title": "Solving General Arithmetic Word Problems"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9", "title": "MAWPS: A Math Word Problem Repository"}, {"paperId": "bba5f2852b1db8a18004eb7328efa5e1d57cc62a", "title": "Key-Value Memory Networks for Directly Reading Documents"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"}, {"paperId": "62df84d6a4d26f95e4714796c2337c9848cc13b5", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems"}, {"paperId": "1ac30af5522c7a50ec4d1ee43fd2bd8652a9bd52", "title": "A Neural Attention Model for Abstractive Sentence Summarization"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "b5c29457a90ee9af7c3b2985e9f665ce4b5b97d6", "title": "Observed versus latent features for knowledge base and text inference"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "5ec85a0d88adcc4344bb5cc81b0d1aef9bcd8dcc", "title": "Findings of the 2014 Workshop on Statistical Machine Translation"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "bc1022b031dc6c7019696492e8116598097a8c12", "title": "Natural Language Processing (Almost) from Scratch"}, {"paperId": "3104e601b2ca570da4a5b3cf6e0393408dad85da", "title": "Book Review: Statistical Language Models for Information Retrieval by ChengXiang Zhai"}, {"paperId": "1976c9eeccc7115d18a04f1e7fb5145db6b96002", "title": "Freebase: a collaboratively created graph database for structuring human knowledge"}, {"paperId": "ba786c46373892554b98df42df7af6f5da343c9d", "title": "Large Language Models in Machine Translation"}, {"paperId": "5740f80fb61c4489674c9a0beb40c4f5e0ed19ff", "title": "YAGO: A Core of Semantic Knowledge Unifying WordNet and Wikipedia"}, {"paperId": "57444ae44a52df1745d54214168e8d31e6002350", "title": "Statistical language modeling for information retrieval"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "9dc1748099dd4321d42fb84bc7ee1f71e7814459", "title": "Introduction to the special issue on statistical language modeling"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": "beaca3493aed271bdfc42490fd22dd11cb40ce0e", "title": "The faculty of language: what is it, who has it, and how did it evolve?"}, {"paperId": "399da68d3b97218b6c80262df7963baa89dcc71b", "title": "SRILM - an extensible language modeling toolkit"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "c6586e7c73cc1c9e9a251947425c54c5051be626", "title": "Two decades of statistical language modeling: where do we go from here?"}, {"paperId": "f0b5342b00268bd5125afc9670622e19b015d21b", "title": "A Second-Order Hidden Markov Model for Part-of-Speech Tagging"}, {"paperId": "68c03788224000794d5491ab459be0b2a2c38677", "title": "WordNet: A Lexical Database for English"}, {"paperId": "8a99ddac28e5695589789609b1258958c2a6c70a", "title": "The language instinct : how the mind creates language"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": "af386a4e0f2615ed929fdc64a86df8e383bd6121", "title": "A tree-based statistical language model for natural language speech recognition"}, {"paperId": "0c4ef6b97566c1ee8ecb00462c547fe071407ab3", "title": "Phase Transitions in Artificial Intelligence Systems"}, {"paperId": "b0130277677e5b915d5cd86b3afafd77fd08eb2e", "title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer"}, {"paperId": "94d9cbfc474cb6ce961de45a06233b2853ca6724", "title": "Toward automatic program synthesis"}, {"paperId": "eb8d64c0f34610597cc02d31ec10d5d6bcd6d39c", "title": "Experiments with a Heuristic Compiler"}, {"paperId": "2d5673caa9e6af3a7b82a43f19ee920992db07ad", "title": "Computing Machinery and Intelligence"}, {"paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b", "title": "Crosslingual Generalization through Multitask Finetuning"}, {"paperId": null, "title": "Chatgpt plugins"}, {"paperId": null, "title": "Planning for agi and beyond"}, {"paperId": "a131c44951b7ace0892dd830dd0a040b99ed0803", "title": "Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning"}, {"paperId": "372fe9428dff788274081b00634d484db0c2fda4", "title": "ChatGPT Goes to Law School"}, {"paperId": "c9dff8253b2e776abf363d0a4836abcaf64ee327", "title": "ChatDoctor: A Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge"}, {"paperId": "b4829f2e7968be42bead06b1710abee6f5a0afc2", "title": "A Comprehensive Study on Post-Training Quantization for Large Language Models"}, {"paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725", "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "Knife: Distilling meta-reasoning knowledge with free-text rationales"}, {"paperId": null, "title": "How does gpt obtain its ability? tracing emergent abilities of language models to their sources"}, {"paperId": "448e1493034dafe35699ae054ff4480b31dcf64a", "title": "Memory-assisted prompt editing to improve GPT-3 after deployment"}, {"paperId": "f3e54291e235df8ca91d3c83697c392155ed584d", "title": "Does GPT-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation"}, {"paperId": "760561c57f68044e2f1d089088df1da6c627b09a", "title": "On the Advance of Making Language Models Better Reasoners"}, {"paperId": "f02e8f1c9b5ab12ddfb1977570f9f5445a99a973", "title": "Large Language Models are reasoners with Self-Verification"}, {"paperId": "9ec3166f4a9f00c37ce080c4dc1fff04b719dccf", "title": "Logical Form Generation via Multi-task Learning for Complex Question Answering over Knowledge Bases"}, {"paperId": "8d188daf721fde8de4877718e96f89ae9d7a1925", "title": "Findings of the 2022 Conference on Machine Translation (WMT22)"}, {"paperId": "72123a86eae2cb5c4eae8650f43524039d48875d", "title": "Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions"}, {"paperId": null, "title": "Huawei mindspore ai development framework"}, {"paperId": null, "title": "Star: Self-taught reasoner bootstrapping reasoning with reasoning"}, {"paperId": null, "title": "Jurassic- 1: Technical details and evaluation"}, {"paperId": null, "title": "Fairscale: A general purpose modular pytorch library for high performance and large scale training"}, {"paperId": "ec4490055de38e9fba38f6ac9ef50d9205df5067", "title": "Findings of the 2021 Conference on Machine Translation (WMT21)"}, {"paperId": "053c168f94acd75880acdacb846bc3f416e5c07c", "title": "PatrickStar: Parallel Training of Pre-trained Models via a Chunk-based Memory Management"}, {"paperId": null, "title": "Online Event"}, {"paperId": null, "title": "Pretrained models: Past, present and future"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": "e8f297e161f57e461ede2d4e0c26573981cad077", "title": "Findings of the 2020 Conference on Machine Translation (WMT20)"}, {"paperId": null, "title": "Transformers: State-ofthe-art natural language processing"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607", "title": "A Survey on Context Learning"}, {"paperId": "6c5b5adc3830ac45bf1d764603b1b71e5f729616", "title": "YAGO3: A Knowledge Base from Multilingual Wikipedias"}, {"paperId": "2453d7c408f17cd477584fad9a8c864276b24782", "title": "Synthesis Lectures on Human Language Technologies"}, {"paperId": "b4fc91e543ec868658cde6170f1e59c33292e595", "title": "Recurrent Neural Network Based Language Modeling in Meeting Recognition"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "231f6de83cfa4d641da1681e97a11b689a48e3aa", "title": "Statistical methods for speech recognition"}, {"paperId": "cddeb5149f4de157d4daeb609a8b1432a8126e7b", "title": "Good-Turing Frequency Estimation Without Tears"}, {"paperId": "63458be2006a89c4b1469e49f1bed09e45929a94", "title": "The Language Instinct: How the Mind Creates Language"}, {"paperId": "c213af6582c0d518a6e8e14217611c733eeb1ef1", "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"}, {"paperId": null, "title": "Socialiqa: Commonsense reasoning about social interactions"}, {"paperId": "1d27a56a8133f947a5a0217b00241d26f585f834", "title": "This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."}, {"paperId": "4954fa180728932959997a4768411ff9136aac81", "title": "This Paper Is Included in the Proceedings of the 12th Usenix Symposium on Operating Systems Design and Implementation (osdi '16). Tensorflow: a System for Large-scale Machine Learning Tensorflow: a System for Large-scale Machine Learning"}, {"paperId": null, "title": "A reproduction version of cc-stories on hugging face"}, {"paperId": null, "title": "Common crawl"}, {"paperId": null, "title": "Project gutenberg"}, {"paperId": null, "title": "Bmtrain: Effient training for big models"}, {"paperId": null, "title": "Bigquery dataset"}]}