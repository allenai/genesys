{"paperId": "2b05686607991a39aead43f371fd7ea2b08195f5", "title": "Distributed Inference and Fine-tuning of Large Language Models Over The Internet", "abstract": "Large language models (LLMs) are useful in many NLP tasks and become more capable with size, with the best open-source models having over 50 billion parameters. However, using these 50B+ models requires high-end hardware, making them inaccessible to most researchers. In this work, we investigate methods for cost-efficient inference and fine-tuning of LLMs, comparing local and distributed strategies. We observe that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network. This could allow running LLM efficiently by pooling together idle compute resources of multiple research groups and volunteers. We address two open problems: (1) how to perform inference and fine-tuning reliably if any device can disconnect abruptly and (2) how to partition LLMs between devices with uneven hardware, joining and leaving at will. In order to do that, we develop special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput. We showcase these algorithms in Petals - a decentralized system that runs Llama 2 (70B) and BLOOM (176B) over the Internet up to 10x faster than offloading for interactive generation. We evaluate the performance of our system in simulated conditions and a real-world setup spanning two continents.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 14, "influentialCitationCount": 2, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is observed that a large enough model (50B+) can run efficiently even on geodistributed devices in a consumer-grade network, and special fault-tolerant inference algorithms and load-balancing protocols that automatically assign devices to maximize the total system throughput are developed."}, "embedding": {"model": "specter_v2", "vector": [-0.17342868447303772, 0.4237585961818695, -0.7303908467292786, 0.0734405592083931, -0.7351035475730896, -0.46898165345191956, 0.5873615741729736, -0.07572820037603378, -0.39627382159233093, 0.0680876150727272, 0.31840142607688904, -0.3811933398246765, 0.13010485470294952, -0.13097353279590607, -0.5793001055717468, 0.25753605365753174, -1.037448763847351, 1.1962472200393677, -0.0616537444293499, 0.14993347227573395, -0.4892696440219879, -0.16339120268821716, -1.2926033735275269, 0.3689572215080261, 0.4781898558139801, 0.4437048137187958, 0.1206033006310463, 0.987690806388855, -0.30913519859313965, -0.2170431762933731, 0.10240327566862106, 0.015729637816548347, -0.04446341469883919, -0.05256816744804382, -0.29996275901794434, -0.40589383244514465, 0.2730312943458557, -0.294418603181839, -0.05421106144785881, 0.8491175174713135, 0.0080860685557127, -0.0008012331090867519, 0.09249766170978546, -0.9541288018226624, -0.09401044249534607, 1.0591011047363281, 0.37179529666900635, 0.574999988079071, 0.19928522408008575, -0.724751889705658, 1.0638346672058105, -0.8083295822143555, 0.5580611824989319, 1.727540135383606, 0.6190599203109741, 0.2347438782453537, -0.4818043112754822, -0.570875346660614, 0.1773986965417862, 0.1371193528175354, -1.18739652633667, -0.6507036685943604, -0.6721277236938477, -0.1139409989118576, 1.592605710029602, -0.1894066482782364, -0.18403275310993195, 0.3237953186035156, -0.3869035542011261, 1.2660508155822754, -0.2032473385334015, -0.9000813961029053, 0.009876065887510777, 0.01344215590506792, -0.07874902337789536, 0.6340680718421936, 0.069590263068676, -0.20766128599643707, -0.9167320132255554, -1.1873881816864014, 0.06956542283296585, -0.5819665193557739, -0.08999323844909668, -0.021145189180970192, -0.18819063901901245, 0.7414420247077942, 0.024463357403874397, 0.7125855684280396, -0.3712039589881897, 0.5924126505851746, 0.6139176487922668, 0.23197896778583527, 0.26882392168045044, 0.2938987612724304, -0.2805651128292084, 0.019115008413791656, -1.1336179971694946, 0.4151662588119507, 0.5573704838752747, 0.5992981195449829, -0.17529550194740295, -0.12952223420143127, -0.34852027893066406, 0.249022975564003, 1.3574750423431396, 0.17690134048461914, 0.6854269504547119, -0.49509933590888977, 0.347945898771286, -0.7870835661888123, 0.6355199813842773, -0.3296077847480774, -0.36044198274612427, 0.5335243940353394, -0.8536200523376465, -1.4158680438995361, -0.1517707109451294, -0.3526361584663391, -0.5380052924156189, 0.5046961307525635, 0.06284312158823013, 0.5918800830841064, -0.0367916002869606, 0.49872833490371704, 0.2589750587940216, 0.9174508452415466, -0.04600049555301666, 0.04960665479302406, 1.241777777671814, -0.15660421550273895, -0.5971730351448059, -1.004650592803955, 0.9310064315795898, -0.17483185231685638, -0.10704713314771652, -0.35247787833213806, -1.6093175411224365, -0.4176948368549347, -0.4678002595901489, 0.034044887870550156, -0.676511824131012, 0.5425735116004944, 1.1111100912094116, 0.30598536133766174, -1.06650710105896, 0.553443193435669, -0.42595452070236206, -0.6191264390945435, 0.23453901708126068, 0.2531261444091797, 0.2357751876115799, -0.4428384602069855, -1.2598458528518677, 0.03244108334183693, 0.38363829255104065, -0.8302322030067444, 0.13933230936527252, -0.4315449893474579, -0.6434164643287659, 0.10011900961399078, 0.3408932685852051, -0.48734137415885925, 1.2100791931152344, 0.461848646402359, -1.2026689052581787, 0.6865614652633667, -0.31243589520454407, -0.09494110941886902, 0.3746725916862488, 0.16882839798927307, -0.37944257259368896, -0.45517322421073914, -0.2155340164899826, 0.4508104920387268, 0.26867589354515076, 0.014455538243055344, -0.27523350715637207, 0.0313565731048584, -0.4295909106731415, -0.15787722170352936, 0.09904056042432785, 0.9997290968894958, -0.6555291414260864, -0.1440833956003189, 0.401663601398468, 0.4555714726448059, -0.3505629599094391, -0.03393382579088211, -0.5398240089416504, -0.9765841364860535, 0.5865955352783203, -0.2538456618785858, 0.8729845881462097, -0.48127689957618713, -0.6374425292015076, 0.00856567732989788, -0.3208574950695038, 0.03228066489100456, -0.7481736540794373, 0.7080538868904114, -0.21858707070350647, 0.8224948048591614, 0.01623522862792015, -0.6860062479972839, 0.21661117672920227, -0.18107716739177704, -0.44530269503593445, 0.11680658161640167, -0.38503459095954895, 0.7833388447761536, -0.49889519810676575, 0.29041725397109985, -0.2940918803215027, 0.034089431166648865, -1.2437703609466553, 1.1198840141296387, -1.0487240552902222, -0.1487404853105545, -0.22124461829662323, -0.19316142797470093, 0.5862737894058228, -0.3446321487426758, 0.9493855834007263, -0.12548692524433136, -0.22465388476848602, 0.7066295742988586, -0.8372933864593506, 1.2732579708099365, -0.14684520661830902, 0.08311231434345245, 0.5674093961715698, -0.43756982684135437, -0.3593306839466095, 0.6435050964355469, -0.16567839682102203, -0.2412107288837433, -0.05940668284893036, 0.5808144211769104, -0.5245766639709473, 0.14935822784900665, 0.5950796008110046, 1.0450867414474487, -0.4949731230735779, 0.7939295172691345, 0.1735735982656479, -0.6570438146591187, 0.5118624567985535, 0.3472461402416229, 0.5798996686935425, 0.16070331633090973, 0.4027823805809021, -0.12191558629274368, 0.22288401424884796, -0.7767557501792908, -0.14264917373657227, 0.8367816209793091, 0.6390864253044128, 0.38531580567359924, 0.5987140536308289, -0.8806572556495667, -0.4560100734233856, 0.5171571969985962, 0.4147701561450958, 1.5836563110351562, -0.3527102768421173, -0.37065187096595764, -0.9016735553741455, -0.48713308572769165, 0.05193673446774483, 0.08564549684524536, 0.07381224632263184, 0.38191962242126465, -0.6320046782493591, -1.3760552406311035, 1.038365125656128, -0.0651354268193245, 0.5548018217086792, -0.5705471634864807, -0.12605051696300507, -0.3389720618724823, 0.07347166538238525, -0.7511292099952698, -0.349517285823822, -0.015147500671446323, -0.05197305232286453, 0.0645173117518425, -0.014959045685827732, -0.01779765449464321, -0.09969787299633026, -0.7627174258232117, 0.8995973467826843, -0.35528117418289185, -0.4586958587169647, -0.24194595217704773, 0.6974636912345886, -0.7934867739677429, -1.1182688474655151, 0.4212389588356018, 0.2243361920118332, -0.4496532082557678, 0.07863131165504456, 0.599309504032135, 0.1572362184524536, 0.017721813172101974, -0.3096931278705597, -0.09690305590629578, -0.10624697804450989, -0.24086102843284607, 0.5382522344589233, -0.15655329823493958, 0.1696140319108963, -1.3435903787612915, 1.0274752378463745, -0.13270269334316254, -1.0301278829574585, 0.43496450781822205, -0.22719427943229675, -0.25131019949913025, 0.414013147354126, -0.5506304502487183, -0.16888028383255005, -1.0646394491195679, -0.06688879430294037, 0.011782124638557434, -0.3617945611476898, 0.29251232743263245, 0.2659873366355896, 0.1710015833377838, 0.3503468334674835, 0.39709794521331787, 0.34662461280822754, -0.6443493962287903, 0.7696406841278076, -0.3103635907173157, 0.005044074729084969, 0.12303149700164795, -0.10138778388500214, 0.012919194996356964, -0.449618935585022, -0.9373332858085632, -0.10381541401147842, -0.3292137086391449, -0.12523044645786285, 0.3815278112888336, -0.06050746887922287, -0.6717777252197266, -0.8686513304710388, -0.09691059589385986, -1.5292710065841675, -0.3864530920982361, 0.4937777519226074, 0.03834779933094978, -0.03560561686754227, -0.9182246923446655, -1.4719372987747192, -0.6603490114212036, -0.46431034803390503, -0.8576198816299438, 0.8464421629905701, -0.31575801968574524, -0.6120477318763733, -0.17231497168540955, 0.21573077142238617, -0.4174748361110687, 0.6675853729248047, -0.8808715343475342, 0.9384985566139221, 0.07625631988048553, -0.2048608362674713, -0.2843315303325653, -0.1153499037027359, -0.3262234926223755, -0.6508435606956482, 0.35406234860420227, -0.6649854183197021, 0.12416070699691772, -0.7564716935157776, -0.5193018913269043, 0.04518228396773338, 0.5850248336791992, 1.0503497123718262, -0.32061490416526794, -1.0129908323287964, 0.3857893943786621, 1.350297212600708, -0.9719749093055725, 0.11336088180541992, -0.17739813029766083, 0.9266350865364075, 0.1415717452764511, -0.2913869619369507, 0.8602956533432007, 0.24482591450214386, 0.6219475269317627, -0.05269007384777069, -0.2032078206539154, 0.13099299371242523, -0.19539250433444977, 0.24693900346755981, 1.3064346313476562, 0.2866537868976593, -0.5799511671066284, -0.8901829123497009, 0.4169712960720062, -1.4559580087661743, -0.6603326201438904, 0.43108898401260376, 0.8353242874145508, 0.5475386381149292, -0.6069188714027405, 0.010224062949419022, -0.41671934723854065, 0.6197869181632996, 0.35185521841049194, -0.24674856662750244, -0.7054862380027771, 0.2559967637062073, 0.07979654520750046, 0.4298233687877655, 0.4538494348526001, -0.0667772963643074, 0.6419563889503479, 15.126774787902832, 1.3165260553359985, 0.16891270875930786, 0.7579390406608582, 0.537312924861908, 0.4422396719455719, -0.412860631942749, -0.3671603202819824, -1.2696006298065186, 0.16638143360614777, 1.562692642211914, -0.06914079189300537, 0.762634813785553, 0.11804432421922684, 0.33758530020713806, -0.031683873385190964, -0.21375751495361328, 0.565912663936615, 0.43624621629714966, -1.5910191535949707, 0.47428345680236816, 0.5210781693458557, 0.38948512077331543, 1.0674575567245483, 0.07410387694835663, 0.3968944251537323, 0.9420017004013062, -0.4936951994895935, 0.5550968647003174, 0.06949818134307861, 0.9406681656837463, -0.23241254687309265, 0.4904559850692749, 0.9171901941299438, -0.7473915815353394, -0.05415457487106323, -0.8285767436027527, -0.8667383790016174, 0.4300048053264618, 0.5987140536308289, -1.0410863161087036, -0.5008336901664734, 0.012133236974477768, 0.14699038863182068, 0.42207491397857666, 0.7682400345802307, -0.07835935056209564, 0.44068774580955505, -0.598155677318573, 0.06865531951189041, -0.1181180328130722, 0.42866086959838867, 0.045198954641819, -0.1506144106388092, 0.22235122323036194, -0.2934685945510864, 0.422002375125885, 0.2761552035808563, -1.114439845085144, -0.054903071373701096, -0.5096459984779358, -0.24724891781806946, 0.6224848628044128, 0.5004778504371643, 0.2409195899963379, 0.10909156501293182, 0.04635274410247803, 0.36859768629074097, 0.7927510738372803, -0.20379342138767242, -0.033729128539562225, 0.3005126714706421, 0.43754953145980835, -0.3395356237888336, -0.055975936353206635, 0.7148990035057068, 0.18693770468235016, -0.4997742772102356, -0.7404764294624329, -0.6397194862365723, 0.6806773543357849, -0.49939820170402527, -0.790630042552948, 0.6932267546653748, -0.2270764708518982, 0.10517000406980515, -0.024538351222872734, -0.5435681939125061, -0.14648036658763885, 0.6068931818008423, -1.1036689281463623, -0.7757392525672913, 0.8670520186424255, -0.5592015385627747, -0.652018666267395, 0.5351594090461731, 1.7511924505233765, 0.22225239872932434, -0.6358000636100769, -0.11151368170976639, 0.6826028227806091, -0.3839035928249359, 0.09851530194282532, -0.5748035311698914, 1.3304845094680786, 0.27969738841056824, -0.04977467656135559, -0.017882488667964935, -0.13412101566791534, -0.09548729658126831, -0.5453924536705017, 0.024165812879800797, 1.0738990306854248, -1.1313519477844238, -0.48473066091537476, -1.1551053524017334, -0.8592367172241211, 0.4459988474845886, 0.3636692464351654, -0.3004930317401886, 0.09831281006336212, 0.1359122395515442, -0.33643701672554016, 0.060932401567697525, -1.0421327352523804, -0.010755620896816254, 0.45096176862716675, -0.7503714561462402, 0.1271389126777649, 0.5827422738075256, 0.2772604823112488, -1.1396657228469849, -0.6651167273521423, -0.29288941621780396, 0.1904558390378952, -0.0016307643381878734, 0.8171399831771851, -0.3075532913208008, -0.06258153915405273, 0.8333874940872192, 0.23164710402488708, -0.7714202404022217, 0.11564628779888153, -1.0891722440719604, 0.025333184748888016, 0.11470663547515869, 0.8216729760169983, -0.51954185962677, 0.34502071142196655, 0.6351262331008911, 0.664948582649231, 0.049757059663534164, -0.6032974720001221, -0.13662242889404297, -0.22249983251094818, -0.5070468783378601, 0.6979398727416992, 0.08425470441579819, -0.06920990347862244, -0.23355497419834137, 0.3123403787612915, 1.2051197290420532, -0.3342096209526062, -0.48590877652168274, 0.6861055493354797, -0.34317025542259216, -0.1454133540391922, -0.41842442750930786, -0.08752360194921494, -1.2506732940673828, 0.12653020024299622, -0.871523916721344, 0.3320443630218506, -0.9518356919288635, -0.6628733277320862, -0.3364115357398987, 0.5453037619590759, 0.05271894112229347, 0.7397730946540833, -0.298432856798172, -0.8772640228271484, -0.4752817749977112, -0.4145044684410095, 0.3147539794445038, 0.4610649347305298, -0.4935857951641083, 0.06786028295755386, 0.16738036274909973, 0.5066455602645874, 0.7022730112075806, 0.46004152297973633, -0.6105438470840454, -0.6173890829086304, -1.2164815664291382, 0.531421422958374, 0.023927373811602592, -0.2244730293750763, -0.3676166534423828, 0.7208677530288696, 0.2804136276245117, -0.38676851987838745, -0.048368148505687714, 0.130543515086174, -0.7739585638046265, -0.2651720941066742, 0.3419007956981659, -0.5555109977722168, 0.07068069279193878, 0.2651880979537964, -0.5795205235481262, 0.09881668537855148, 0.4069242477416992, -0.21396644413471222, -1.3063223361968994, -0.3823765814304352, 0.1871284693479538, -0.6257182955741882, 0.20928524434566498, -0.34063708782196045, 0.2860732972621918, -0.9362594485282898, -0.22109510004520416, 0.06309664994478226, 0.1521410048007965, -0.20415961742401123, 0.6434997320175171, -0.2920331656932831, -0.5884733200073242, -0.11302049458026886, 0.41443297266960144, -0.2520550787448883, 0.10366366803646088, 0.6255059242248535, 0.31248196959495544, -0.7599035501480103, 0.5821148753166199, 0.6513296365737915, 0.20367754995822906, -0.9547227025032043, -0.10838928818702698, 0.47565844655036926, -0.30736014246940613, -0.2435450702905655, 1.0768269300460815, -0.530938982963562, -1.1556872129440308, 0.08024332672357559, -1.244633436203003, -0.18669261038303375, -0.25786253809928894, 0.7529569864273071, 0.22059188783168793, 0.29486924409866333, 0.08171826601028442, -0.6470995545387268, -0.036202702671289444, 0.13521213829517365, -0.5285499095916748, 0.4405597746372223, -0.48910218477249146, -0.8193135261535645, 0.22575116157531738, 0.2024330049753189, 0.0536988265812397, -0.08310786634683609, -0.359019011259079, -0.299164742231369, 0.19074754416942596, 0.46764498949050903, -0.418491393327713, -0.09483080357313156, 0.5823935270309448, 0.1253843903541565, 0.7232944965362549, -0.4885781407356262, 0.19291093945503235, 0.3077072501182556, 0.7295461893081665, 0.36812716722488403, -0.42132768034935, -1.027546763420105, 0.8731078505516052, 0.9115411639213562, -0.6132076978683472, 0.3980487287044525, -0.25587111711502075, -0.3825635612010956, 0.600604236125946, 0.22052189707756042, 0.3638439476490021, 0.8009426593780518, 0.15287838876247406, 0.03925805538892746, -0.03014400415122509, -1.0943247079849243, -0.28045204281806946, 0.6532522439956665, 0.7535176277160645, 0.29189354181289673, 0.5722808241844177, -0.036113012582063675, 0.8036731481552124, 0.1750517338514328, 0.3464142084121704, 0.6933429837226868, 0.44362759590148926, -0.14895939826965332, -0.2855096757411957, -0.36626696586608887, 0.717678427696228, -0.6379268765449524, -0.4749683737754822, -0.28269609808921814, 0.5792953372001648, 0.446159690618515, 0.3887726366519928, 0.8747119307518005, 0.6477224826812744, 0.20238438248634338, 0.25958889722824097, 0.32900044322013855, -0.5977839231491089, 0.08949781954288483, 0.03643939271569252, -0.1307237595319748, -0.10003384947776794, -0.06893309950828552, -0.2449098527431488, -0.4203554689884186, -0.7294892072677612, -0.08630203455686569, 0.44539397954940796, 0.5037634372711182, 0.9771177768707275, 1.1958268880844116, -0.03620263934135437, -0.5480074882507324, -0.8631643652915955, -0.4312593340873718, -0.7898097038269043, -0.24587704241275787, -0.6851511001586914, -0.5833420157432556, 0.30106908082962036, -0.17864516377449036, -0.5172941088676453]}, "authors": [{"authorId": "2113838061", "name": "Alexander Borzunov"}, {"authorId": "1491753352", "name": "Max Ryabinin"}, {"authorId": "2064702046", "name": "Artem Chumachenko"}, {"authorId": "2273927050", "name": "Dmitry Baranchuk"}, {"authorId": "2288469507", "name": "Tim Dettmers"}, {"authorId": "2037496520", "name": "Younes Belkada"}, {"authorId": "104084918", "name": "Pavel Samygin"}, {"authorId": "2269733851", "name": "Colin Raffel"}], "references": [{"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "5278b81db686b4d36143941bff1c683bea963a63", "title": "SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "6b117a8dcaa161562b0a69afbb9811e11afb5b3e", "title": "Decentralized Training of Foundation Models in Heterogeneous Environments"}, {"paperId": "b7a4c84f699d85716b7b26de29e832ec5f928ded", "title": "Fine-tuning Language Models over Slow Networks using Activation Compression with Guarantees"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "082eaf09599baab29f7bc4359d72ac67cd26cd9a", "title": "Adapting BigScience Multilingual Model to Unseen Languages"}, {"paperId": "9efdd8772ece12b67537b3498952a7ce785f882d", "title": "Bamboo Trimming Revisited: Simple Algorithms Can Do Well Too"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "d9cdf21e73519edc593bdf1a00fcd778764b13f6", "title": "Training Neural Networks with Fixed Sparse Masks"}, {"paperId": "521ccc898395a2818fced22b4cf371b0e5121f94", "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models"}, {"paperId": "f3a332ff1b73acda482e5d83696b2c701f487819", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "a6d8d04962f84ae6225e72723869a002b9fc8036", "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "b769b629c8de35b16735214251d6b4e99cb55762", "title": "Generating Datasets with Pretrained Language Models"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "d22e4cc3a501c17881b9478621f29760e429e76e", "title": "Parameter-Efficient Transfer Learning with Diff Pruning"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "69170faf00279bdcdfd91290a3756d539f9fb6e6", "title": "Communication-Efficient Distributed Deep Learning: A Comprehensive Survey"}, {"paperId": "6a5c0fc737b6fbd6672fc4265b5e0ca38de17416", "title": "Training Large Neural Networks with Constant Memory using a New Execution Algorithm"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "1340978c92e7cbcf9abe87888150c60984e2964b", "title": "PipeMare: Asynchronous Pipeline Parallel DNN Training"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "ae3af798af4e74096fa1637e221343d596bdb68d", "title": "A Pragmatic Introduction to Secure Multi-Party Computation"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "d8c09661b1bebfb690f0566167c87d64c5628d73", "title": "Demystifying Parallel and Distributed Deep Learning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "80d800dfadbe2e6c7b2367d9229cc82912d55889", "title": "One weird trick for parallelizing convolutional neural networks"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "e7823d7c5fdf7145e241fc70b13958815231eee8", "title": "Fast replanning for navigation in unknown terrain"}, {"paperId": "eb51cb223fb17995085af86ac70f765077720504", "title": "Kademlia: A Peer-to-Peer Information System Based on the XOR Metric"}, {"paperId": "766cd91c0d8650495529cab7d4eeed482729cf89", "title": "Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation"}, {"paperId": "77e73174e606c0820a52a940088832b32d9a033e", "title": "Efficient Large-Scale Language Model Training on GPU Clusters"}, {"paperId": null, "title": "Accelerate: Run your raw pytorch training script on any kind of device"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "scalable, low-cost training of massive deep learning models"}, {"paperId": null, "title": "A version of BLOOM with 7.1 billion parameters"}, {"paperId": null, "title": "huggingface"}, {"paperId": null, "title": "The fork of Megatron-LM and Megatron-DeepSpeed by BigScience"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "An open-source autoregressive language"}, {"paperId": null, "title": "Broader impact"}]}