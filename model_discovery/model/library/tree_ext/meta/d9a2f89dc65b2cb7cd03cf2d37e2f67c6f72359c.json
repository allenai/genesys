{"paperId": "d9a2f89dc65b2cb7cd03cf2d37e2f67c6f72359c", "title": "PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels", "abstract": "The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models. Recent theoretical results indicate the intractability of sub-quadratic softmax attention approximation under reasonable complexity assumptions. This paper addresses this challenge by first demonstrating that polynomial attention with high degree can effectively replace softmax without sacrificing model quality. Next, we develop polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees. Crucially, our approach achieves this speedup without requiring the sparsification of attention matrices. We also present a block-based algorithm to apply causal masking efficiently. Combining these techniques, we provide \\emph{PolySketchFormer}, a practical linear-time Transformer architecture for language modeling that offers provable guarantees. We validate PolySketchFormer empirically by training language models capable of handling long contexts. These experiments utilize both synthetic and real-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32k and GPT-2 style models, our model achieves a 2.5-4x speedup in training compared to FlashAttention, with no observed degradation in quality across our experiments.", "venue": "", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper demonstrates that polynomial attention with high degree can effectively replace softmax without sacrificing model quality, and develops polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees."}, "embedding": {"model": "specter_v2", "vector": [0.42738255858421326, 0.8782254457473755, -0.4646988809108734, 0.02688838541507721, -0.2901608645915985, -0.3458322584629059, 0.8293993473052979, -0.23620252311229706, -0.09926610440015793, -0.5975831747055054, 0.2848932147026062, -0.25267574191093445, 0.5416799783706665, 0.0817517638206482, -0.375839501619339, 0.21876922249794006, -0.7612149715423584, 0.014496278949081898, 0.03902096301317215, -0.3902631998062134, -0.1102951392531395, -0.7121670246124268, -0.8964166045188904, 0.033689938485622406, 0.32792022824287415, 0.4335936903953552, 0.16623267531394958, 0.7674782872200012, -0.4000685513019562, 0.300630122423172, 0.5284671783447266, -0.6210317015647888, 0.3877345621585846, 0.44574034214019775, 0.020277054980397224, -0.23103876411914825, 0.13780532777309418, -0.027508433908224106, -1.0467958450317383, 0.8892326951026917, -0.4290240406990051, 0.10054267942905426, 0.16472332179546356, -0.8749398589134216, -0.6621273756027222, 0.9514783620834351, 0.688951313495636, 0.6748098731040955, -0.5553107857704163, -0.5161054730415344, 1.6591764688491821, -1.6570936441421509, -0.0891227200627327, 1.3435022830963135, 0.36462557315826416, 0.3305416703224182, -0.4477235972881317, -0.6293262839317322, 0.8161278367042542, 0.31057167053222656, -0.9107201099395752, -0.5940874218940735, -0.03652194142341614, 0.16902725398540497, 1.9441064596176147, -0.20043224096298218, 0.08914253860712051, 0.29503193497657776, -0.0268710870295763, 1.4186506271362305, 0.04112497344613075, -0.7322810292243958, -0.22007060050964355, 0.20227544009685516, 0.2422797828912735, 0.8030240535736084, -0.4696912467479706, 0.3785952031612396, -0.8329696655273438, -0.3107742369174957, 0.19968591630458832, 0.1486123502254486, 0.14967115223407745, -0.2927304804325104, 0.020435065031051636, 0.5469897389411926, 0.17850692570209503, 0.6459885835647583, 0.20090080797672272, 1.1158126592636108, 0.7024353742599487, 0.2795684337615967, -0.13623090088367462, 0.21899668872356415, -0.16683483123779297, 0.38327756524086, -1.073089838027954, 0.27452272176742554, 0.10829317569732666, 1.2871109247207642, -0.14803414046764374, 0.4876261353492737, -0.540290355682373, -0.06656137853860855, 1.1187211275100708, 0.6153157353401184, 0.22703170776367188, -0.42869463562965393, 0.05820722505450249, -0.7787907719612122, -0.04827524349093437, -0.7869411706924438, 0.018293628469109535, -0.2872922122478485, -0.7155506610870361, -1.3915815353393555, -0.6038883328437805, 0.4374264180660248, -0.7555435299873352, 0.8131994605064392, -0.4180559515953064, 0.22302517294883728, -0.21422724425792694, 0.24042832851409912, 0.40371590852737427, 0.8042735457420349, 0.3870064914226532, 0.11890202015638351, 1.132691502571106, -0.7963167428970337, -0.5848469138145447, -1.2381383180618286, 0.524591863155365, -0.09147858619689941, 0.31873273849487305, 0.057976458221673965, -1.3714444637298584, -0.8468881249427795, -0.7056128978729248, -0.11640375107526779, -0.49928420782089233, 0.5011491179466248, 0.9827080368995667, 0.6617482900619507, -1.0700119733810425, 0.3560080826282501, -0.4504610002040863, 0.25948089361190796, 0.4922855496406555, 0.20120111107826233, 0.2925246059894562, -0.4195927381515503, -1.4739421606063843, 0.415487676858902, 0.15414433181285858, -0.6999602317810059, -0.27159613370895386, -1.1224803924560547, -1.3468272686004639, 0.4694499671459198, 0.6662477254867554, -0.306611567735672, 1.3537468910217285, -0.21589627861976624, -1.1442168951034546, 0.7522913813591003, -0.6772043704986572, -0.13167303800582886, 0.04829808697104454, -0.3105108439922333, -0.6026264429092407, -0.5096597075462341, -0.3720994293689728, 0.3131275177001953, 0.8632693886756897, -0.15149962902069092, -0.17213526368141174, 0.4848199784755707, -0.19133837521076202, -0.24232853949069977, -0.11511716991662979, 1.0647855997085571, -0.5255756378173828, -0.32789936661720276, -0.113437220454216, 0.4572296440601349, -0.36935582756996155, -0.33567073941230774, -0.3863880932331085, -1.1324983835220337, 0.6036688685417175, -0.07993878424167633, 1.159683346748352, -1.253211259841919, -0.6558215022087097, -0.004030909389257431, 0.24994292855262756, 0.10570279508829117, -0.4488956034183502, 0.8527141213417053, -0.7965889573097229, 0.08893562108278275, -0.2520673871040344, -1.0335886478424072, 0.5665135979652405, -0.19607555866241455, -1.0514062643051147, -0.17831012606620789, 0.2924618124961853, 1.3524744510650635, -0.9735551476478577, 0.03852260485291481, 0.1460350751876831, 0.25157076120376587, -1.0325387716293335, 1.0729717016220093, -0.3613537549972534, -0.36772170662879944, 0.43620193004608154, -0.2577403485774994, 0.01174231618642807, -0.4513314962387085, 0.4969526529312134, -0.3969050645828247, 0.0884799137711525, 0.3361756205558777, -0.3449343144893646, 1.1693319082260132, -0.5675045251846313, 0.7026978731155396, -0.06412176042795181, -0.9692191481590271, 0.06917084753513336, 0.4779236316680908, -0.20124870538711548, -0.18099704384803772, -0.09370504319667816, 0.21779845654964447, -0.7412518262863159, 0.14101438224315643, 0.6640779972076416, 0.6805003881454468, -0.2947660982608795, -0.2889467775821686, 0.5669220089912415, -0.2701587677001953, 0.26270705461502075, 0.6451452970504761, 0.9665432572364807, 0.2726385295391083, 0.578670084476471, -0.26399505138397217, 0.39354780316352844, -1.0351132154464722, -0.21263816952705383, 0.4103788435459137, 0.719477653503418, 0.35297009348869324, 0.6010010242462158, -0.5916257500648499, -0.7539020776748657, -0.0573631227016449, 0.6744484901428223, 1.7651885747909546, -0.25887182354927063, -0.5829184055328369, -0.5735929012298584, -0.02496849186718464, -0.2996286451816559, 0.2946808338165283, -0.22202962636947632, -0.26379236578941345, -0.6402924656867981, -0.8038985729217529, 0.8269573450088501, 0.6187424063682556, 0.5021421313285828, -0.14860354363918304, -0.15923351049423218, -0.2637255787849426, 0.49100667238235474, -1.2257944345474243, -0.7610456347465515, 0.5607414841651917, 0.006255055777728558, 0.1375887095928192, 0.16216345131397247, -0.1426110416650772, 0.35511302947998047, -0.5319708585739136, 0.852996826171875, -0.7279428243637085, -0.4122217893600464, 0.46102410554885864, 0.662764310836792, -0.65941321849823, -0.6175248026847839, 0.1167118027806282, 0.03256978839635849, -0.1506403237581253, 0.4624880254268646, 0.15390048921108246, 0.023204945027828217, -0.07920151948928833, -0.3760022521018982, 0.2594175934791565, 0.23288163542747498, 0.17432044446468353, 0.18951743841171265, 0.03964473307132721, -0.12626460194587708, -1.2693512439727783, 0.43922853469848633, 0.0959729254245758, -0.5859954953193665, 0.16857419908046722, -0.7499870657920837, -0.35245105624198914, 0.7338266968727112, -0.8933985233306885, -0.1670958250761032, -0.5160572528839111, 0.339312344789505, -0.5367143154144287, -0.010683774016797543, 0.14307455718517303, 0.08454950153827667, 0.20365244150161743, 0.12572503089904785, 0.8339767456054688, 0.08244875073432922, 0.5454052090644836, 0.9620805978775024, -1.0792639255523682, 0.5111968517303467, 0.11884168535470963, 0.7930557727813721, -0.010829572565853596, -0.006993511226028204, -0.9316486716270447, -0.349833607673645, -0.4083206057548523, 0.011993365362286568, -0.12458904832601547, 0.12756875157356262, -0.708492636680603, -1.0950984954833984, -0.17185358703136444, -0.9862768650054932, -0.276561439037323, 0.1541774719953537, -0.8359169363975525, -0.157785102725029, -0.9706620573997498, -1.164211392402649, -1.0082577466964722, -0.5638329386711121, -0.8074797987937927, 0.6568429470062256, 0.07172054797410965, -0.25614628195762634, -0.685900092124939, -0.4292168617248535, -0.5432150363922119, 1.04067063331604, -0.940947949886322, 0.7181957960128784, 0.0016504075611010194, -0.5390145182609558, -0.18676452338695526, -0.058833424001932144, 0.39905667304992676, -0.7362064123153687, 0.2660581171512604, -1.0372856855392456, 0.1279190480709076, -0.7501125335693359, 0.09078018367290497, -0.009308290667831898, 0.43794479966163635, 0.9719651341438293, -0.28855636715888977, -0.6734104752540588, 0.40002214908599854, 1.4880391359329224, -0.5671250224113464, 0.26354384422302246, -0.3420369625091553, 1.182032823562622, -0.0598682276904583, -0.4637327790260315, 0.6523019671440125, 0.6506149768829346, 0.42749467492103577, 0.08276037871837616, -0.15131710469722748, 0.4602178633213043, -0.6152233481407166, 0.6290314793586731, 1.244351863861084, 0.4443112015724182, 0.14787551760673523, -0.9587060213088989, 0.8936869502067566, -1.2467882633209229, -0.9250269532203674, 0.5898264646530151, 0.42057573795318604, 0.389285147190094, -0.19207391142845154, -0.3878096044063568, -0.07547412812709808, 0.289137065410614, 0.3915097713470459, -0.27498677372932434, -0.6476101875305176, 0.06261350214481354, 0.8815749287605286, 0.6833170652389526, 0.6943115592002869, -0.45942553877830505, 0.8382927179336548, 14.83753776550293, 1.0792864561080933, 0.010889120399951935, 0.7795737981796265, 0.4981335401535034, 0.4480312168598175, -0.5972042679786682, 0.42989885807037354, -1.1276038885116577, -0.16034051775932312, 0.9968061447143555, -0.11424281448125839, 0.7622972726821899, 0.2632612884044647, -0.09489908814430237, 0.35886305570602417, -0.5392885804176331, 0.809708297252655, 0.5176295042037964, -1.221456527709961, -0.207048237323761, -0.1272701621055603, 0.21392051875591278, 0.17527323961257935, 0.8768671751022339, 0.995910108089447, 0.6312118172645569, -0.6514037251472473, 0.5269054174423218, 0.28296959400177, 1.1497321128845215, 0.1534235030412674, -0.20980723202228546, 0.6147000193595886, -1.2269556522369385, -0.3319456875324249, -0.5202953219413757, -0.8755975365638733, 0.2422248274087906, 0.3538661003112793, -0.5682389140129089, -0.35153084993362427, -0.34089595079421997, 0.6501797437667847, 0.26253122091293335, 0.10720552504062653, -0.20970484614372253, 0.8849902749061584, -0.16768987476825714, -0.007138744927942753, -0.1322760432958603, 0.655093252658844, 0.09322303533554077, 0.22900213301181793, 0.12576782703399658, 0.26505622267723083, 0.15413835644721985, 0.7219566702842712, -0.06360328942537308, -0.18480822443962097, -0.19567133486270905, -0.426069051027298, -0.03048202209174633, 0.9978460669517517, 0.8280520439147949, 0.03217379376292229, -0.6332058310508728, 0.2671816349029541, 0.5254462361335754, 0.42820313572883606, -0.5521551370620728, -0.02055385708808899, 0.2153552621603012, -0.34826400876045227, 0.1590915322303772, 0.27304530143737793, -0.25733625888824463, -0.5842798948287964, -0.8295358419418335, -0.6696854829788208, 0.1337515413761139, -0.7821377515792847, -0.660281240940094, 0.31033241748809814, -0.2882901430130005, -0.08760663121938705, 0.6169261932373047, -0.9105883240699768, -0.11062721163034439, 0.49276238679885864, -1.0504941940307617, -0.7626352906227112, 0.36488497257232666, -0.5606799721717834, -0.16824603080749512, 0.12706786394119263, 1.23854398727417, 0.25904080271720886, -0.0469646081328392, 0.2701491117477417, -0.17324474453926086, -0.1776554435491562, -0.3041313886642456, -1.1738897562026978, 1.2075130939483643, 0.2270997017621994, -0.17539305984973907, 0.5618641376495361, 0.48678532242774963, 0.25199654698371887, -1.0645802021026611, 0.12960907816886902, 1.0028702020645142, -1.2325352430343628, 0.14053308963775635, -1.0249987840652466, -0.5702268481254578, 0.37830814719200134, 0.39880114793777466, -0.1618906557559967, 0.13203127682209015, 0.025862326845526695, -0.7631604075431824, -0.2791039049625397, -0.1847148984670639, 0.21317407488822937, 0.4937509298324585, -1.1645487546920776, -0.20287740230560303, -0.262603223323822, 0.48005616664886475, -1.2670423984527588, -0.4664791226387024, -0.19988010823726654, 0.1909271627664566, 0.1316586285829544, 1.1350557804107666, -0.26571768522262573, 0.8508152365684509, 0.8153166770935059, 0.00017468280566390604, -0.6039074659347534, -0.19659224152565002, -1.287495732307434, -0.3476269841194153, -0.0732051357626915, 0.39934197068214417, -0.24226002395153046, 0.23291310667991638, 0.8709146976470947, 0.1557963788509369, -0.5498663783073425, -0.5225697755813599, -0.19677551090717316, -0.0748620554804802, -0.6810083985328674, 0.20825770497322083, 0.1301579475402832, 0.22528235614299774, 0.21853071451187134, -0.01304529421031475, 0.7702609896659851, -0.23559880256652832, -0.7650734186172485, 0.49689722061157227, 0.3007039725780487, -0.3027091920375824, -0.5725365281105042, -0.6359454989433289, -1.4833050966262817, 0.2790358066558838, -1.1791187524795532, -0.0238217543810606, -0.5653918385505676, -0.3825457692146301, 0.04821017012000084, -0.06422055512666702, 0.03988698497414589, -0.07416024804115295, -0.6166785955429077, -0.5640560388565063, -1.1090474128723145, -0.14927102625370026, 0.7018788456916809, 0.5465161800384521, -0.8758086562156677, 0.429737389087677, 0.06606332957744598, 0.007719659712165594, 0.06380906701087952, 0.172866091132164, -0.17393304407596588, -0.9312788248062134, -0.9531760811805725, 0.6469683647155762, 0.19451472163200378, 0.2314096987247467, -0.5432261228561401, 0.6679746508598328, -0.027139173820614815, -0.4697149693965912, 0.29447317123413086, 0.3593308627605438, -1.0967780351638794, -0.3617362082004547, 0.37852123379707336, -0.9200457334518433, 0.20716659724712372, 0.2292887568473816, -0.3794631361961365, 0.08615648001432419, 0.7867470383644104, -0.17006254196166992, -1.136048674583435, -0.4676138460636139, 0.5219882726669312, -0.5114190578460693, 0.18241509795188904, -0.37910985946655273, 0.23441652953624725, -1.2110100984573364, -0.5364966988563538, -0.07120605558156967, 0.3119746148586273, -0.4610622823238373, 0.9276423454284668, 0.2500206232070923, -1.0787274837493896, 0.3268737196922302, 0.31011685729026794, 0.011784127913415432, -0.061871085315942764, 0.6442711353302002, 0.3985576927661896, -0.0024690027348697186, 0.8135512471199036, 0.5765789151191711, 0.35775360465049744, -1.0019978284835815, 0.5766627192497253, 0.7107895612716675, -0.4437321722507477, -0.28981518745422363, 1.0319684743881226, 0.21385356783866882, -0.7619359493255615, 0.13113537430763245, -1.3873893022537231, -0.13431935012340546, -0.7237449288368225, 0.32526546716690063, -0.05755968019366264, -0.2864570617675781, 0.2067793309688568, -0.5328658223152161, -0.028167465701699257, 0.32797741889953613, -0.5163384675979614, 0.3776295483112335, 0.018641533330082893, -0.36754298210144043, 0.5028695464134216, 0.5742223858833313, -0.4370479881763458, -0.42424800992012024, -0.9646439552307129, -0.45171216130256653, -0.04426422715187073, 0.17458736896514893, -0.06801346689462662, -0.2541274428367615, 1.03695547580719, 0.1292581558227539, 0.5470929741859436, 0.0829467922449112, -0.18863297998905182, 0.15417231619358063, 0.48168182373046875, 0.13524547219276428, -0.5934175252914429, -0.6881700158119202, 1.257461667060852, 1.0297300815582275, -0.5436202883720398, -0.03038203716278076, -0.7488679885864258, -0.7343501448631287, 0.597158670425415, 0.08583126217126846, -0.21751433610916138, 1.2024279832839966, 0.09803453832864761, -0.18351218104362488, 0.1584375500679016, -1.385169267654419, -0.39583954215049744, 1.219422459602356, 0.7984340190887451, 0.5520413517951965, 0.4488942325115204, 0.19618603587150574, 1.0122743844985962, -0.0922771617770195, -0.006337784696370363, 0.16419288516044617, 0.34210455417633057, -0.0860561802983284, -0.04031238332390785, 0.02327853813767433, 0.5327397584915161, -0.7428973317146301, -1.0475565195083618, 0.3821096122264862, 0.12640653550624847, 0.07377398759126663, 0.4420364797115326, 0.6418649554252625, 0.10391630232334137, 0.5654386281967163, 0.3526266813278198, 0.7100722193717957, -0.490262895822525, 0.16538886725902557, -0.1637178212404251, -0.6203678250312805, -0.08928342163562775, 0.10660883784294128, -0.3858768343925476, -0.38388171792030334, -0.20152844488620758, 0.03839518129825592, -0.2628576457500458, 0.32079967856407166, 1.3350039720535278, 0.3541632890701294, 0.6398580074310303, -0.013435347937047482, -0.3235923647880554, -0.6120879054069519, -0.5713161826133728, 0.12761615216732025, -0.6049991250038147, -0.16487230360507965, -0.042122162878513336, 0.10649114102125168, 0.09022410213947296]}, "authors": [{"authorId": "1471876925", "name": "Praneeth Kacham"}, {"authorId": "1728881", "name": "V. Mirrokni"}, {"authorId": "2249561001", "name": "Peilin Zhong"}], "references": [{"paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334", "title": "Gated Linear Attention Transformers with Hardware-Efficient Training"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "93e58491830abe1eb965ab37ec64fa97263f6048", "title": "HyperAttention: Long-context Attention in Near-Linear Time"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "39ed1c33af6f0a5fbc16354afcb223a03c9c139b", "title": "Fast Attention Requires Bounded Entries"}, {"paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f", "title": "In-context Learning and Induction Heads"}, {"paperId": "ec139916edd6feb9b3cb3a0325ca96e21dbb0147", "title": "Hydra Attention: Efficient Attention with Many Heads"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "11820ce093d137d0b35c914a40465962120af439", "title": "Fast Sketching of Polynomial Kernels of Polynomial Degree"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "01203341a8b5b7df21dec5359afe8cc388786ebf", "title": "Wiki-40B: Multilingual Language Model Dataset"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "c29a7919c85a606b6da82f24cb3818ae5f6b4fd6", "title": "(Learned) Frequency Estimation Algorithms under Zipfian Distribution"}, {"paperId": "8906f67725f60041ea6200c8b1fc36d0818a4ace", "title": "Polynomial Tensor Sketch for Element-wise Function of Low-Rank Matrix"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "72c969a5dc5b236d511fbdaae88c443d14145ae8", "title": "Learning-Based Frequency Estimation Algorithms"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "503347a28184ec6d92137cfcab11a2f862318fcb", "title": "Subspace Embeddings for the Polynomial Kernel"}, {"paperId": "ecbea3b74deb06657a2d0100a717501f7d1a252a", "title": "Sketching as a Tool for Numerical Linear Algebra"}, {"paperId": "a3651caf1265328e3231fe2d261679b4d926d5b7", "title": "Which problems have strongly exponential complexity?"}, {"paperId": "c49c292e1fb1d215c88828a52134b7ccfa52be44", "title": "Sparse Attention with Learning to Hash"}, {"paperId": null, "title": "Linformer"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "c06eaf6483b63dab04320d9ffd9c1246e9e3aca3", "title": "Tight Dimensionality Reduction for Sketching Low Degree Polynomial Kernels"}, {"paperId": "ec0bc920662693375a881d8beb8c97bf08281dba", "title": "Prefix sums and their applications"}, {"paperId": null, "title": "Implementation of FlashAttention in Pallas"}, {"paperId": null, "title": "Physics Multiple Choice"}, {"paperId": null, "title": "Linear complexity self-attention with 3rd order polynomials"}]}