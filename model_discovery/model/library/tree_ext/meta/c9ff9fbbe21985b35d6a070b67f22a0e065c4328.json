{"paperId": "c9ff9fbbe21985b35d6a070b67f22a0e065c4328", "title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars", "abstract": "Large Language Models (LLMs) have achieved remarkable results, but their increasing resource demand has become a major obstacle to the development of powerful and accessible super-human intelligence. This report introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results suggest that LLM training can be much more cost-effective than generally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE) architecture, composed of attention and feedforward experts. Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B. Moreover, JetMoE-8B is highly open and academia-friendly, using only public datasets and training code. All training parameters and data mixtures have been detailed in this report to facilitate future efforts in the development of open foundation models. This transparency aims to encourage collaboration and further advancements in the field of accessible and efficient LLMs. The model weights are publicly available at https://github.com/myshell-ai/JetMoE.", "venue": "arXiv.org", "year": 2024, "citationCount": 9, "influentialCitationCount": 2, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "JetMoE-8B demonstrates impressive performance, with JetMoE-8B outperforming the Llama2-7B model and JetMoE-8B-Chat surpassing the Llama2-13B-Chat model, suggesting that LLM training can be much more cost-effective than generally thought."}, "embedding": {"model": "specter_v2", "vector": [-0.045738860964775085, 0.8710736036300659, -0.6285268068313599, 0.1338471621274948, -0.09713569283485413, 0.022051677107810974, 0.7082967162132263, -0.543457567691803, -0.6354100108146667, -0.005718223750591278, 0.6125146150588989, -0.09460169821977615, 0.6749188303947449, 0.457236111164093, -0.24856820702552795, 0.10196365416049957, -0.9870498180389404, 0.46775659918785095, -0.34041455388069153, -0.22279958426952362, -0.1678069531917572, -0.8429660201072693, -0.8055904507637024, 0.1676769107580185, 0.39842379093170166, 0.5732076168060303, -0.017205586656928062, 1.004086971282959, -0.2746921181678772, 0.6143178939819336, 0.4200980067253113, -0.6117302775382996, 0.06110364943742752, 0.07489469647407532, -0.6820976734161377, -0.06351488083600998, 0.31159457564353943, -0.5493960380554199, -0.2513115704059601, 0.8384393453598022, -0.23413562774658203, 0.29318487644195557, 0.34895583987236023, -0.7794367671012878, -0.33154651522636414, 1.0112963914871216, 0.7886211276054382, 0.7408283352851868, -0.32377734780311584, -0.31697285175323486, 1.3337230682373047, -1.3134993314743042, 0.28236687183380127, 1.557600498199463, 0.4726856052875519, 0.4173824191093445, -0.2805721163749695, -0.4604402184486389, 0.5719541907310486, 0.13254395127296448, -0.4163663387298584, -0.4162977635860443, -0.13405142724514008, 0.07596953213214874, 2.186389207839966, -0.37555593252182007, 0.18500733375549316, 0.725575864315033, -0.24451018869876862, 1.6195365190505981, 0.016082705929875374, -1.046502709388733, -0.3029809594154358, 0.5416884422302246, 0.26622912287712097, 0.7863023281097412, -0.36282461881637573, 0.1434064358472824, -1.0461359024047852, -0.09891660511493683, 0.35701990127563477, -0.16912312805652618, 0.3592306971549988, 0.17901350557804108, -0.5815874934196472, 0.7726694941520691, 0.2725122570991516, 0.9009212851524353, -0.3057258129119873, 0.728463351726532, 0.5400241017341614, 0.30953124165534973, 0.1466292291879654, 0.39505648612976074, -0.2246505469083786, 0.14526964724063873, -0.6767557859420776, 0.2332077920436859, 0.18606553971767426, 0.7527545690536499, -0.09521468728780746, 0.272142618894577, -0.6844719052314758, -0.06356516480445862, 1.3786489963531494, 0.08178314566612244, 0.43642252683639526, -0.9105621576309204, 0.2336062788963318, -0.6818274855613708, 0.30257710814476013, -0.5620485544204712, -0.18854525685310364, -0.20911753177642822, -0.7698304057121277, -1.201683521270752, -0.5322850346565247, 0.14844027161598206, -0.6270582675933838, 0.4888882339000702, -0.17999181151390076, 0.0966605618596077, 0.2268904745578766, 0.7767605185508728, 0.7036970257759094, 0.5818201899528503, 0.30195724964141846, 0.40026041865348816, 1.0070291757583618, -1.4631805419921875, -0.8982937335968018, -1.2873194217681885, 0.6877743005752563, -0.19080643355846405, 0.0808623731136322, -0.13046513497829437, -1.2361841201782227, -0.7251304388046265, -0.6319460272789001, -0.10417500138282776, -0.4587872326374054, 0.7029931545257568, 1.53163480758667, 0.4337540566921234, -0.5547720789909363, 0.4181268811225891, -0.3060462176799774, 0.20046351850032806, 0.36648306250572205, 0.26290321350097656, 0.16899317502975464, -0.32525673508644104, -1.4081684350967407, 0.542787492275238, 0.5012403130531311, -0.3042607307434082, -0.17876365780830383, -0.4140958786010742, -0.9435078501701355, -0.15031522512435913, 0.346872478723526, -0.4359626770019531, 1.5244473218917847, -0.2172112911939621, -1.3557556867599487, 0.6715263724327087, -0.3831389546394348, -0.14152434468269348, 0.4541080594062805, -0.337062805891037, -0.6079031825065613, -0.22319787740707397, -0.11832523345947266, 0.6787742376327515, 0.4867464303970337, 0.2349333018064499, -0.12550370395183563, 0.4185543358325958, -0.21152512729167938, -0.21943189203739166, -0.4215685725212097, 1.0286204814910889, -0.4531651735305786, -0.13207904994487762, -0.22664767503738403, 0.4856263995170593, -0.08668801188468933, -0.32097992300987244, -0.597470760345459, -1.06559157371521, 0.5662802457809448, -0.10612831264734268, 0.88697350025177, -1.0119452476501465, -0.6162809133529663, -0.3837294578552246, -0.022460589185357094, 0.07158888876438141, -0.3884357213973999, 0.27644628286361694, -0.12808877229690552, 0.30963990092277527, -0.1555355042219162, -1.381295084953308, 0.376231849193573, -0.30259838700294495, -0.48243382573127747, -0.17008671164512634, 0.33741238713264465, 1.1489818096160889, -0.9654862880706787, -0.17524665594100952, -0.06874920427799225, 0.35570356249809265, -1.411803960800171, 1.2740827798843384, -0.4932483434677124, 0.2391427457332611, -0.12879328429698944, -0.20386828482151031, -0.10052642226219177, -0.4161910116672516, 0.3777856230735779, -0.1876562088727951, -0.17435522377490997, 0.5617220997810364, -0.6852913498878479, 1.4655267000198364, -0.32067593932151794, 0.6336750388145447, 0.1040930524468422, -0.5232996344566345, -0.06344320625066757, 0.6870061159133911, -0.4759022891521454, -0.3039649724960327, 0.4939969480037689, 0.6890944242477417, -0.49911510944366455, 0.09248066693544388, 0.7707057595252991, 0.7498705983161926, -0.3916839063167572, 0.30576881766319275, 0.8417370915412903, 0.06530392169952393, 0.38510870933532715, 0.6636863350868225, 0.28141912817955017, 0.25841453671455383, 0.21757757663726807, 0.006685041822493076, 0.11576705425977707, -0.6981924176216125, -0.14331325888633728, 0.4956628680229187, 0.3778603971004486, 0.4958953857421875, 0.3398934006690979, -0.9162776470184326, -0.2775286138057709, 0.3345910310745239, 0.7288455367088318, 1.5062533617019653, -0.0710698664188385, -0.19055606424808502, -0.6225293278694153, -0.3008498251438141, -0.2799278199672699, 0.13452231884002686, -0.1547173708677292, -0.1028386652469635, -0.447847455739975, -1.11207115650177, 0.7661488652229309, 0.12381286919116974, 1.1012325286865234, -1.0190932750701904, -0.600183367729187, -0.3480589687824249, 0.13715755939483643, -0.9695870280265808, -0.5533795952796936, 0.30481791496276855, -0.44241297245025635, 0.36331531405448914, -0.051904913038015366, -0.14232143759727478, 0.06197921931743622, -0.969024121761322, 0.9628555178642273, -0.49046942591667175, -0.3606873154640198, 0.058162763714790344, 0.6789641380310059, -0.5435171127319336, -0.5406582355499268, 0.13265641033649445, 0.28701287508010864, 0.06553740799427032, 0.2754836082458496, 0.45454761385917664, 0.23938652873039246, -0.12012466788291931, -0.17675349116325378, 0.38208580017089844, 0.29836684465408325, 0.1369643360376358, 0.36890384554862976, -0.33052194118499756, 0.04021168127655983, -1.205743670463562, 0.4731912612915039, 0.04000407084822655, -0.21198014914989471, -0.09691812843084335, -0.5021312236785889, -0.3457717001438141, 0.3517347276210785, -0.8094027042388916, -0.5889157056808472, -0.7441195845603943, 0.4755328595638275, -0.24348850548267365, -0.48830410838127136, 0.08931094408035278, 0.3435455858707428, 0.2783057987689972, 0.10163725167512894, 0.39485982060432434, -0.04914463683962822, -0.45328983664512634, 0.7300867438316345, -0.6340312361717224, 0.6904886364936829, 0.21289843320846558, -0.038362056016922, -0.055035628378391266, -0.48198750615119934, -0.5875657200813293, -0.5275577902793884, -0.5320684909820557, -0.30917006731033325, -0.40446922183036804, 0.27401021122932434, -0.7140347957611084, -0.9278486967086792, 0.2072363793849945, -1.3186969757080078, -0.3695286512374878, 0.2496812641620636, 0.33202919363975525, -0.1449960619211197, -1.0242136716842651, -1.28662109375, -0.9038819670677185, -0.6075406670570374, -0.8375043869018555, 0.41171950101852417, 0.24205145239830017, -0.520147979259491, -0.6686288118362427, 0.08290601521730423, -0.013271688483655453, 0.8564577102661133, -0.7833545207977295, 0.42328080534935, -0.04260137677192688, -0.29132279753685, -0.4387945830821991, 0.2675170600414276, 0.5427005290985107, -0.7368330359458923, 0.18956561386585236, -0.9013156890869141, 0.32043027877807617, -0.537589430809021, -0.7913201451301575, 0.2876197099685669, 0.15453976392745972, 0.21706970036029816, -0.07561208307743073, -0.3485909402370453, 0.34366360306739807, 0.9457699656486511, -1.0735288858413696, -0.03445792943239212, 0.2849798798561096, 1.0097869634628296, 0.010894774459302425, -0.43015122413635254, 0.6342217922210693, 0.3678574562072754, 0.3299829363822937, 0.14380255341529846, -0.20993587374687195, -0.06948059797286987, -0.49191078543663025, 0.6660527586936951, 1.4576478004455566, 0.016039106994867325, 0.09334827959537506, -0.7825427055358887, 0.8010854721069336, -1.2389906644821167, -0.8124711513519287, 0.4756053686141968, 0.42013999819755554, 0.34460484981536865, -0.30303695797920227, -0.12287316471338272, -0.2532886564731598, 0.5928789377212524, 0.20417986810207367, -0.3141929805278778, -1.0217384099960327, -0.03114437498152256, 0.12067786604166031, 0.01011505164206028, 0.4702581465244293, -0.41176360845565796, 0.7148486971855164, 15.118253707885742, 0.8278995752334595, 0.1391529142856598, 0.7596278786659241, 0.7421401739120483, 0.05304616689682007, -0.25718942284584045, -0.05413299426436424, -1.212361216545105, -0.09602145850658417, 1.2975908517837524, 0.30492815375328064, 0.7734289765357971, 0.26777586340904236, -0.19015242159366608, 0.09019607305526733, -0.3119868338108063, 0.5508965253829956, 0.5339734554290771, -1.1487975120544434, 0.1470237672328949, 0.250345915555954, 0.6271972060203552, 0.6543572545051575, 0.7977818250656128, 1.0746450424194336, 0.5070555806159973, -0.4915754795074463, 0.8879780769348145, 0.29464903473854065, 0.8075505495071411, -0.09925567358732224, 0.4899848699569702, 0.7858766913414001, -0.7069131135940552, -0.1536560356616974, -0.38639143109321594, -1.5555753707885742, 0.3442443907260895, 0.28510424494743347, -0.46479174494743347, -0.5631824135780334, 0.06281809508800507, 0.6709895730018616, 0.20182760059833527, 0.29576629400253296, -0.2354791909456253, 0.7093400359153748, -0.2264191210269928, 0.023980051279067993, 0.46691539883613586, 0.45092353224754333, 0.3692101538181305, 0.2409469187259674, -0.203255295753479, -0.14998112618923187, 0.1672835499048233, 0.8800188899040222, -0.7269838452339172, -0.19313360750675201, -0.08980841934680939, -0.6313665509223938, -0.3348678946495056, 1.1302719116210938, 0.4188989996910095, -0.03762703016400337, -0.594749927520752, 0.44192296266555786, 0.7006489038467407, 0.016440045088529587, -0.3683425784111023, -0.12006409466266632, 0.11464744806289673, -0.9479796886444092, 0.058129649609327316, 0.5865883827209473, 0.04573862999677658, -0.7746689319610596, -0.6537045240402222, -0.35642150044441223, 0.2686138153076172, -0.5697326064109802, -0.6436514854431152, 0.8390830755233765, -0.44097352027893066, 0.013132991269230843, 0.1501789093017578, -0.7834784984588623, -0.3160577714443207, 0.5929135084152222, -1.275094747543335, -0.7232669591903687, 0.6642869114875793, -0.34822168946266174, -0.5952960252761841, -0.0708346739411354, 1.389905333518982, 0.3018399775028229, -0.5383272767066956, -0.06877408921718597, -0.017927570268511772, 0.029324546456336975, -0.2683365046977997, -0.7799131274223328, 0.7940331697463989, 0.20477013289928436, 0.01667366549372673, 0.1285492330789566, 0.07099870592355728, 0.2852315604686737, -0.750823974609375, 0.18305303156375885, 1.0744529962539673, -1.1321908235549927, -0.30353885889053345, -0.8305904269218445, -0.47686198353767395, 0.5231227278709412, 0.5070464611053467, 0.16542497277259827, 0.17654956877231598, 0.2639691233634949, -0.6668497323989868, 0.26613637804985046, -0.5296340584754944, -0.16872146725654602, 0.23051978647708893, -0.8092990517616272, -0.5994125008583069, -0.054264601320028305, 0.2781967222690582, -0.7845924496650696, -0.3959050178527832, -0.3900826871395111, 0.2894822657108307, -0.43064558506011963, 0.8860881924629211, -0.47443193197250366, 0.5087850093841553, 1.020033359527588, -0.06648921966552734, -0.9050296545028687, 0.04153499752283096, -1.1748117208480835, -0.33971506357192993, 0.008918206207454205, 0.6884793043136597, -0.33806946873664856, -0.0010981725063174963, 0.9702589511871338, 0.3565998375415802, -0.3217802345752716, -0.6470106244087219, -0.3373623788356781, -0.27214351296424866, -0.5253730416297913, 0.10475899279117584, 0.05100874975323677, 0.1355096399784088, 0.31404030323028564, -0.0883193090558052, 0.4637491703033447, -0.15779487788677216, -0.5166183114051819, 0.3870450556278229, 0.022038545459508896, -0.4461449682712555, -0.434139609336853, -0.41047346591949463, -1.2089084386825562, 0.13919764757156372, -1.2050591707229614, 0.06378054618835449, -0.6893143057823181, -0.1113240048289299, 0.1957104355096817, -0.19957928359508514, 0.26723262667655945, 0.1923699527978897, 0.15177570283412933, -0.41217803955078125, -0.5480518341064453, -0.8081561326980591, 0.9340363144874573, 1.0692681074142456, -0.6932507753372192, 0.20493771135807037, -0.12726880609989166, 0.29941681027412415, 0.43347933888435364, 0.2582954168319702, -0.4778376519680023, -0.21986274421215057, -1.2892787456512451, 0.35294169187545776, -0.3215329051017761, 0.2784248888492584, -0.7361811399459839, 0.4434358775615692, 0.747700572013855, -0.22649146616458893, 0.3145236670970917, 0.3879432678222656, -0.6562078595161438, -0.4246968626976013, 0.4355265498161316, -0.7626274228096008, -0.11725743114948273, 0.19348908960819244, -0.78230881690979, -0.2765328586101532, 0.6150826811790466, -0.38874608278274536, -1.0515073537826538, -0.704114556312561, 0.2778778374195099, -0.8092871904373169, 0.12731488049030304, -0.5734434127807617, 0.07761776447296143, -0.8190878629684448, -0.20203661918640137, -0.12650294601917267, 0.12235900014638901, -0.6433663368225098, 0.7323508262634277, -0.10209991037845612, -0.97736656665802, 0.005419680383056402, 0.35904037952423096, -0.15248803794384003, -0.12209231406450272, 0.41535723209381104, 0.5803941488265991, -0.02719973959028721, 0.8475028872489929, 0.12617111206054688, 0.4663437306880951, -0.6551904082298279, -0.07817510515451431, 0.5687544941902161, -0.5021559000015259, -0.4070781171321869, 1.1757612228393555, -0.5663224458694458, -1.3232479095458984, 0.2911631762981415, -1.0035921335220337, -0.392774760723114, -0.567282497882843, 0.6523135304450989, 0.31056156754493713, -0.2011987417936325, -0.35222452878952026, -0.6147183775901794, -0.00032373491558246315, 0.20800797641277313, -0.5538722276687622, 0.5454711318016052, 0.02442535199224949, -0.4376224875450134, 0.855318009853363, 0.8778332471847534, -0.49668020009994507, -0.5879560112953186, -0.5627390742301941, -0.5718179941177368, 0.15147803723812103, 0.6290244460105896, -0.6665594577789307, -0.3987279534339905, 1.003476858139038, 0.28141355514526367, 0.32121169567108154, -0.21095623075962067, -0.11846794933080673, 0.32584288716316223, 0.9279769062995911, 0.11742448806762695, -0.6592816710472107, -0.6063278317451477, 1.303960919380188, 0.758022129535675, -1.0810110569000244, 0.019913922995328903, -0.12463229894638062, -0.8380132913589478, 0.6875355243682861, 0.33745506405830383, 0.04500710219144821, 0.5150170922279358, -0.3556431531906128, -0.060171592980623245, 0.11273699998855591, -1.06905198097229, -0.3044887185096741, 0.7540642619132996, 0.5703595876693726, 0.7869448065757751, 0.6856658458709717, 0.09265639632940292, 1.0355706214904785, 0.10499417781829834, 0.22762224078178406, 0.24663594365119934, 0.45616674423217773, -0.03280487656593323, 0.24093826115131378, -0.05198277533054352, 0.5054596662521362, -0.6824360489845276, -0.5417344570159912, -0.005360995419323444, 0.40257710218429565, 0.47482427954673767, 0.4605849087238312, 0.7238750457763672, 0.5270563960075378, 0.2245907485485077, 0.21886368095874786, 0.575442373752594, -0.45012331008911133, -0.3110581040382385, -0.03595152124762535, -0.44915202260017395, -0.02284257858991623, -0.38129329681396484, -0.18832072615623474, -0.5070652961730957, -0.41043293476104736, 0.035202447324991226, 0.24437984824180603, 0.26940006017684937, 1.3695801496505737, 0.6710845828056335, 0.4163747727870941, -0.48895543813705444, -0.8600380420684814, -0.1626979112625122, -1.1484979391098022, -0.2852952778339386, -0.651959240436554, -0.21902939677238464, -0.25326108932495117, -0.35151082277297974, -0.28994545340538025]}, "authors": [{"authorId": "2296151561", "name": "Yikang Shen"}, {"authorId": "2282899341", "name": "Zhen Guo"}, {"authorId": "2295986722", "name": "Tianle Cai"}, {"authorId": "2296026250", "name": "Zengyi Qin"}], "references": [{"paperId": "49873ee415619efd9e1e4c16f73ee066ff008c1f", "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies"}, {"paperId": "c67c4c81beed122d7f94580d8816a6dc68867ec4", "title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models"}, {"paperId": "18e7ab056c16928d8f9539509a4b366889106d97", "title": "StarCoder 2 and The Stack v2: The Next Generation"}, {"paperId": "b46d05bcf42295b872f3cebf875643d2e66496a4", "title": "Direct Language Model Alignment from Online AI Feedback"}, {"paperId": "ad1bb59e3e18a0dd8503c3961d6074f162baf710", "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research"}, {"paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32", "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"}, {"paperId": "16d6e1ed1cf72212f6154644f3aa59d18bc95fda", "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "860c8de4fdac38695ff6860dd15312f1079c6117", "title": "Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"}, {"paperId": "40e0b9361d88b1879891eb6d16de110b30bf6c62", "title": "OctoPack: Instruction Tuning Code Large Language Models"}, {"paperId": "4993258852711c4e3d0011325ac3db680eae84f4", "title": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "be8db99310602d66bba64bcf41a572c45816fbfc", "title": "Let's Verify Step by Step"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "a122863d239643453195424c04067e89406246e1", "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"}, {"paperId": "87e26319485793e9cef3c5461b914b7f646a6162", "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea", "title": "Instruction Tuning with GPT-4"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "7bf72a3b5fbac8bc0f461780810fbc781c28ef53", "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "43014fc85c4860487336579ec98f509fec1803f7", "title": "MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "0bd2602df71e89c8961562175c8759e625e99389", "title": "ModuleFormer: Learning Modular Large Language Models From Uncurated Data"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": "0dd4436c48f584ba92443256d185419981bcb9fe", "title": "Transcending Runtime-Memory Tradeoffs in Checkpointing by being Fusion Aware"}, {"paperId": "6fe1587be4ff23e45169f129cfd4d5575b767da0", "title": "Databricks"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Socialiqa: Commonsense reasoning about social interactions"}, {"paperId": "70d6dfdc40c4681ba5d51d60116db0311b5126ce", "title": "Language Models"}, {"paperId": null, "title": "airoboros: Customizable implementation of the self-instruct paper"}, {"paperId": null, "title": "glaiveai"}, {"paperId": null, "title": ": Unlocking the potential of slms"}, {"paperId": null, "title": "Progressive learning from complex explanation"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "Open-Assistant: A chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically"}, {"paperId": null, "title": "FastChat: An open platform for training, serving, and evaluating large language model based chatbots"}, {"paperId": null, "title": "Amplify-instruct: Synthetically generated diverse multi-turn conversations for effecient llm training"}, {"paperId": null, "title": "A framework for the evaluation of code generation models"}, {"paperId": null, "title": "Openllama: An open reproduction of llama,"}, {"paperId": null, "title": "Quick, cheap, and powerful refinement"}, {"paperId": null, "title": "CollectiveCognition"}, {"paperId": null, "title": "Locutusque"}, {"paperId": null, "title": ": Open models based on gemini research and technology"}, {"paperId": null, "title": "OpenGPT: A framework for creating grounded instruction based datasets and training conversational domain expert Large Language Models (LLMs)"}, {"paperId": null, "title": "\u201dTeknium\u201d"}]}