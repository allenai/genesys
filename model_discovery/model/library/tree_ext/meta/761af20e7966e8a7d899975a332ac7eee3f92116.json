{"paperId": "761af20e7966e8a7d899975a332ac7eee3f92116", "title": "How to Protect Copyright Data in Optimization of Large Language Models?", "abstract": "Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.\n\nIn this paper, we observe that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.", "venue": "AAAI Conference on Artificial Intelligence", "year": 2023, "citationCount": 17, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.12247", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "It is observed that large language model training and optimization can be seen as a softmax regression problem, and a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data is established."}, "embedding": {"model": "specter_v2", "vector": [0.06881225854158401, 0.9322050213813782, -0.05934574082493782, 0.3597416281700134, -0.5462805032730103, -0.27027642726898193, 0.6380321383476257, -0.5144069790840149, -0.19732920825481415, -0.20733581483364105, 0.47707927227020264, -0.03334380313754082, 0.3231339156627655, 0.01612333208322525, -0.5678690075874329, -0.06303151696920395, -0.6493140459060669, 0.14769940078258514, -0.039621032774448395, -0.23777475953102112, -0.2518426775932312, -0.49597686529159546, -0.8900611400604248, -0.10311625897884369, 0.054091885685920715, 0.7896773815155029, -0.19311513006687164, 0.8484161496162415, -0.0134139908477664, 0.5123000144958496, 0.03551071882247925, -0.8215333223342896, 0.5472306609153748, 0.1345626264810562, -0.3624352812767029, 0.12373538315296173, 0.2932942807674408, -0.35861653089523315, -0.7584735751152039, 0.5474820733070374, -0.4275963604450226, 0.14984114468097687, 0.3536430895328522, -0.7702732682228088, -0.6716312170028687, 0.8270750641822815, 0.5212717056274414, 0.9489056468009949, -0.27630874514579773, -0.6924002170562744, 1.3158960342407227, -0.8396953344345093, 0.2954380512237549, 1.4210177659988403, -0.16555263102054596, 0.1658303290605545, -0.1383655071258545, -0.8379448652267456, 0.2926090657711029, -0.2986445128917694, -1.013968586921692, -0.41871774196624756, 0.1387084722518921, -0.2124987542629242, 1.2777718305587769, -0.24830685555934906, -0.2430141717195511, 0.3436416685581207, 0.05809193477034569, 1.5237456560134888, -0.0871940553188324, -1.059067726135254, -0.4586244225502014, 0.4710306227207184, 0.08409759402275085, 0.9514595866203308, -0.5296241044998169, 0.5024092197418213, -0.762376070022583, -0.39475488662719727, 0.5093129277229309, -0.38877955079078674, 0.0068503753282129765, -0.268795907497406, 0.213242769241333, 1.1366757154464722, -0.09631223231554031, 0.5738586783409119, 0.1990334391593933, 0.6737101078033447, 0.10914093255996704, 0.42362430691719055, 0.23445342481136322, 0.3872675597667694, 0.10665760934352875, 0.6272386908531189, -0.7361332178115845, -0.34430596232414246, 0.011079041287302971, 0.7164559364318848, -0.5230712890625, 0.6572335958480835, -0.8061713576316833, 0.5194767713546753, 1.5852516889572144, 0.08601486682891846, 0.3665914237499237, -0.6190680265426636, 0.3862329125404358, -0.9020425081253052, 0.003976200707256794, -0.4317317306995392, 0.21777112782001495, -0.3142830729484558, -0.4737483263015747, -0.9656546115875244, -0.3332379460334778, -0.03105478733778, -1.042331337928772, 0.7544825077056885, -0.6563302874565125, -0.22117552161216736, 0.10615018010139465, 0.29981914162635803, -0.1093979999423027, 1.107601523399353, 0.3472162187099457, 0.017977062612771988, 0.8505057096481323, -0.27156683802604675, -0.46962228417396545, -0.845856249332428, 0.2555745840072632, -0.3461727499961853, 0.35511115193367004, -0.04272829368710518, -1.3163869380950928, -0.5260778665542603, -1.024969220161438, 0.0690743625164032, -0.25869783759117126, 0.38768523931503296, 0.9213802814483643, 0.7468452453613281, -0.89449542760849, 0.5887632369995117, -0.09700051695108414, -0.13350172340869904, 0.8223530054092407, 0.6951045393943787, 0.36642885208129883, -0.04319172352552414, -1.2218117713928223, 0.3258148729801178, 0.27173396944999695, -0.7508016228675842, 0.0433080829679966, -0.18630068004131317, -1.1149518489837646, 0.3399216830730438, 0.5919104218482971, 0.019970392808318138, 1.1054202318191528, -0.1733400672674179, -1.1131937503814697, 0.7888780236244202, -0.029213596135377884, 0.2731036841869354, -0.11763456463813782, -0.2598399221897125, -0.6205443143844604, -0.6001118421554565, -0.586466372013092, 0.08247454464435577, 0.8182072639465332, -0.48804450035095215, 0.13160206377506256, 0.30773812532424927, -0.28414395451545715, -0.5864086151123047, -0.6917340159416199, 1.349076509475708, -0.43548399209976196, -0.6901710033416748, -0.14792633056640625, 0.39998331665992737, -0.11918392777442932, -0.14100632071495056, -0.6841802000999451, -1.0453556776046753, 0.44488945603370667, -0.2513541877269745, 0.9619319438934326, -0.849177360534668, -0.6222807765007019, 0.3205258250236511, -0.39256736636161804, 0.05588376894593239, -0.7171597480773926, 0.20350809395313263, -0.45598044991493225, 0.6842642426490784, -0.21259240806102753, -0.9196699857711792, 0.17941688001155853, -0.23380844295024872, -0.762812077999115, -0.04870542511343956, 0.09285587072372437, 0.9885504841804504, -0.09329139441251755, -0.1026262640953064, -0.20802932977676392, 0.23530785739421844, -0.5603589415550232, 1.4109100103378296, -0.26112547516822815, -0.010356795974075794, -0.41145583987236023, -0.07413793355226517, 0.06173913925886154, -0.2760965824127197, -0.08559569716453552, 0.06761341542005539, 0.11621438711881638, -0.05920245498418808, -0.08148855715990067, 1.0329445600509644, 0.18774881958961487, 0.7471474409103394, -0.26790285110473633, -0.8627645373344421, 0.07138650864362717, 0.6416316032409668, -0.29453960061073303, -0.3394276797771454, 0.6228788495063782, 0.10377012938261032, -0.8226503133773804, 0.2664840519428253, 0.6191781163215637, 0.3729607164859772, -0.4499763548374176, 0.6948589086532593, 0.4378731846809387, -0.3083571791648865, 0.3190314471721649, 0.377851277589798, 0.23365673422813416, 0.20768019556999207, 0.4545058310031891, 0.2374914437532425, 0.14640015363693237, -0.7885991930961609, -0.18009012937545776, 0.1557193249464035, 0.868831992149353, 0.970751166343689, 0.5323562622070312, -0.6027042865753174, -0.28189703822135925, -0.4186294376850128, 0.6412314772605896, 1.3842554092407227, -0.31139102578163147, -0.21791884303092957, -0.6020028591156006, -0.20882301032543182, -0.389212965965271, 0.31993740797042847, -0.2492886483669281, -0.45905205607414246, -0.7231713533401489, -1.1585904359817505, 0.9119683504104614, -0.11487610638141632, 0.25536444783210754, -0.7181571125984192, 0.0827668085694313, -0.5978302955627441, 0.4554264545440674, -0.5646676421165466, -1.041581153869629, 0.21564029157161713, -0.3598511517047882, 0.10164069384336472, -0.3142220079898834, -0.21931248903274536, 0.15477289259433746, -1.039277195930481, 0.5648795366287231, -0.29380863904953003, -0.2881627082824707, 0.3389072120189667, 0.6544515490531921, -1.1938859224319458, -1.3235924243927002, 0.14813105762004852, -0.08897068351507187, -0.13176213204860687, 0.007698729634284973, 0.6405194997787476, 0.7623427510261536, 0.03868625685572624, -0.7189544439315796, -0.20534735918045044, 0.3740239143371582, -0.19142474234104156, 0.33455267548561096, -0.5192503333091736, -0.03735888749361038, -1.0573110580444336, 1.2347749471664429, 0.15009097754955292, -1.044309139251709, 1.0034562349319458, -0.7896489500999451, 0.39056238532066345, 0.4392737150192261, -0.6654220223426819, -0.1904342770576477, -0.8426964282989502, 0.3404199481010437, -0.4164615273475647, 0.018296638503670692, 0.36110565066337585, 0.39980348944664, -0.023710211738944054, -0.012826294638216496, 0.4513876438140869, 0.3825989365577698, -0.2830234467983246, 0.21673709154129028, -0.568939208984375, 0.4136470556259155, 0.1173386350274086, 0.519294261932373, -0.5071355104446411, -0.2756080627441406, -0.6039122939109802, -0.10587970912456512, 0.13669438660144806, -0.12825943529605865, -0.23594045639038086, 0.05602090433239937, -0.32321587204933167, -0.6527582406997681, -0.13771064579486847, -0.8560248613357544, -0.028715355321764946, -0.40560483932495117, -0.17600251734256744, -0.21732695400714874, -0.9339766502380371, -1.2435805797576904, -0.7959340214729309, -0.397639662027359, -0.8805965781211853, 0.1911965161561966, 0.38899996876716614, -0.19906452298164368, -0.21611757576465607, 0.009663035161793232, 0.11171361058950424, 0.9692642092704773, -0.8404030203819275, 1.06760573387146, -0.04612521082162857, -0.6149291396141052, -0.5411468148231506, 0.524431049823761, 0.4168320596218109, -0.5170750617980957, 0.22484000027179718, -0.7424172163009644, 0.20217275619506836, -0.4411046802997589, -0.5068724751472473, -0.014256633818149567, 0.5956797003746033, 0.7021809220314026, -0.07619769126176834, -0.361244797706604, 0.1729862242937088, 1.031291127204895, -0.3120017349720001, 0.3596670925617218, -0.1221461072564125, 0.9137831926345825, 0.16268126666545868, -0.7499594688415527, 0.3386176526546478, -0.13062769174575806, 0.5313935875892639, -0.07430792599916458, -0.32845842838287354, -0.003199538681656122, -0.4416106939315796, 0.5061519742012024, 1.2909811735153198, 0.17996886372566223, -0.5545802712440491, -0.7539411187171936, 0.29808759689331055, -0.8444308638572693, -0.7323406338691711, 0.9311793446540833, 0.8697616457939148, 0.056848928332328796, -0.20790207386016846, -0.42898038029670715, -0.15720708668231964, 0.4864675998687744, 0.36353033781051636, 0.06874437630176544, -0.8735803961753845, -0.1203068420290947, 0.8858048915863037, 0.3916095197200775, 0.7464176416397095, -0.43431052565574646, 0.591941773891449, 15.395888328552246, 0.9195976257324219, -0.0021956691052764654, 0.7728328108787537, 0.6024399995803833, -0.04590759053826332, -0.4581126570701599, -0.12111576646566391, -0.8055555820465088, -0.22459277510643005, 1.3398749828338623, 0.13236643373966217, 0.8731690645217896, -0.09104806929826736, -0.23753997683525085, 0.2363056242465973, -0.0416097566485405, 0.7962186336517334, 0.7923091650009155, -1.2053989171981812, 0.33178335428237915, 0.8011067509651184, 0.14446674287319183, 0.5521964430809021, 0.5939867496490479, 0.637391209602356, 0.6259050965309143, -0.5628633499145508, 0.8782842755317688, 0.2767203450202942, 0.4409755766391754, -0.29980382323265076, -0.0284026637673378, 0.504778265953064, -0.5750020146369934, -0.1867036372423172, -0.5236443281173706, -0.46080249547958374, 0.4375620186328888, -0.06575316190719604, -0.1864580661058426, -0.530920147895813, -0.5049380660057068, 0.060591064393520355, 0.024197857826948166, 0.2558654248714447, -0.1656317561864853, 0.8258869647979736, -0.43998098373413086, 0.18151390552520752, -0.2775959372520447, 0.43983080983161926, 0.4738318920135498, -0.553705632686615, 0.15770365297794342, 0.14571870863437653, 0.15705989301204681, 0.6439787745475769, -0.6666280031204224, -0.11392863094806671, -0.3200201094150543, -0.6527150869369507, -0.5362944006919861, 0.13030800223350525, 0.5590714812278748, 0.4587417542934418, -0.3665567636489868, 0.3318566381931305, 0.5087478756904602, 0.34904229640960693, 0.16398948431015015, 0.20922665297985077, 0.17128431797027588, -0.17266537249088287, 0.1338118463754654, 0.4672114849090576, -0.11881737411022186, -0.7082617878913879, -0.4458775818347931, 0.01375139132142067, 0.7615538835525513, -1.1601715087890625, -0.8404291272163391, 0.9075113534927368, -0.11126361787319183, -0.4896983206272125, 0.4653546214103699, -0.6138949394226074, 0.15236617624759674, 0.6546174883842468, -1.158986210823059, -0.8186432123184204, 0.9438082575798035, -0.16297492384910583, -0.36961090564727783, -0.3103733956813812, 1.2390353679656982, -0.21564559638500214, -0.5911473035812378, 0.32329896092414856, 0.14299246668815613, 0.2658635973930359, -0.2198047637939453, -0.20354166626930237, 0.483429878950119, 0.575282096862793, 0.34658360481262207, 0.2813987731933594, -0.08151457458734512, -0.1631433069705963, -0.3258543908596039, 0.7988875508308411, 0.8940456509590149, -1.1657861471176147, -0.49978166818618774, -0.7887482643127441, -0.6502381563186646, 0.33678939938545227, 0.6846790313720703, -0.3466397225856781, 0.31872662901878357, 0.21549589931964874, -0.38269907236099243, -0.22433000802993774, -0.6457561254501343, -0.06179368495941162, -0.0007283332524821162, -1.0007872581481934, -0.2748318016529083, 0.01570824906229973, 0.2957907021045685, -0.7085316777229309, -0.5679683089256287, -0.6842354536056519, 0.10800468921661377, 0.46619632840156555, 0.938202440738678, -0.8291056752204895, 0.5668327212333679, 0.9726885557174683, -0.1195380687713623, -0.5361654162406921, -0.33934763073921204, -1.064481496810913, 0.16660043597221375, 0.2341468185186386, 0.6088032722473145, -0.38486984372138977, 0.43313926458358765, 1.48078191280365, 0.2882148325443268, 0.221818745136261, -0.7404964566230774, -0.20543275773525238, 0.16252264380455017, -0.6138601303100586, -0.02515091933310032, -0.13862930238246918, -0.18510638177394867, -0.1344417780637741, -0.28198114037513733, 0.6587392687797546, -0.15060146152973175, -0.6901503801345825, 0.12081462144851685, -0.4630627930164337, -0.0335783027112484, -0.525728702545166, -0.49886637926101685, -1.122049331665039, 0.1843702346086502, -1.1960868835449219, -0.06180783361196518, -0.5947394967079163, -0.3657546639442444, 0.15388254821300507, -0.2685088813304901, 0.2462819367647171, 0.48197758197784424, -0.4857178032398224, -0.030192319303750992, -0.05611518397927284, -0.023201094940304756, 0.850855827331543, 0.5764292478561401, -0.757908821105957, 0.05857646465301514, 0.2881868779659271, -0.3433155417442322, -0.04909772798418999, 0.24715211987495422, -1.0864049196243286, -0.8507968783378601, -1.2064175605773926, 0.15170474350452423, -0.07188252359628677, -0.11285609751939774, -0.49338117241859436, 0.527164876461029, 0.03673411160707474, -0.013200312852859497, 0.5636230707168579, 0.41814684867858887, -1.099586844444275, -0.2806365191936493, 0.09045230597257614, -0.9530986547470093, -0.13806289434432983, 0.1473860889673233, -0.5522513389587402, -0.13724367320537567, 0.4705813527107239, -0.09442248940467834, -0.8722538948059082, -0.15575788915157318, 0.4200253486633301, -0.8873940706253052, -0.11069054901599884, -0.16583015024662018, -0.22013910114765167, -1.0088368654251099, -0.33049172163009644, -0.06694639474153519, 0.04136146605014801, -0.3839823007583618, 1.2249096632003784, 0.1441672146320343, -0.9550191760063171, 0.09565739333629608, 0.5341037511825562, -0.20689824223518372, -0.3548593521118164, 0.2528266906738281, -0.022012118250131607, -0.23398369550704956, 0.3644295930862427, 0.3181106746196747, 0.6252663731575012, -1.1083415746688843, -0.1283140480518341, 0.5347786545753479, -0.6222221851348877, 0.059196021407842636, 1.0618336200714111, -0.2717887759208679, -0.7874647974967957, 0.09608856588602066, -0.9819003343582153, -0.6284427046775818, -0.6606276035308838, 0.35175126791000366, -0.49113184213638306, -0.296908438205719, -0.2889851927757263, -0.2482885867357254, 0.2538689076900482, 0.2082781344652176, -0.6795494556427002, 0.628821849822998, -0.261034220457077, -0.2157607078552246, 0.43197202682495117, 0.7323492169380188, -0.34365716576576233, -0.48442378640174866, -0.5268487334251404, -0.43505269289016724, -0.3377344310283661, 0.3436416685581207, -0.2654312252998352, -0.9167696237564087, 0.7473570704460144, 0.8497057557106018, 0.2766357958316803, 0.17637920379638672, 0.4259529709815979, 0.3441705107688904, 0.1740187108516693, 0.006525793578475714, -0.6311495900154114, -0.5700575709342957, 1.2281149625778198, 1.2613087892532349, -0.7038341164588928, 0.5532047152519226, -0.3906894028186798, -0.8320900201797485, 0.9160355925559998, -0.01295444741845131, 0.3362412452697754, 1.041811466217041, 0.10467935353517532, 0.2314780354499817, -0.12932319939136505, -0.44937270879745483, -0.0020426465198397636, 1.344029188156128, 0.772509753704071, 0.6559239625930786, 0.3998172879219055, 0.4107593894004822, 1.1411278247833252, -0.025141507387161255, 0.5520148277282715, 0.6059141159057617, 0.9903116822242737, -0.1564512401819229, -0.49411675333976746, -0.4361981451511383, 0.8370798230171204, -0.9538553953170776, -0.4592377841472626, -0.31801825761795044, 0.6595855355262756, 0.16891568899154663, 0.3829788565635681, 0.4944656491279602, -0.1353723406791687, 0.7338001132011414, 0.45422959327697754, 0.4057806730270386, -0.4492274224758148, -0.38504698872566223, -0.05543322116136551, -0.3560760021209717, 0.329071581363678, 0.06379184126853943, -0.48680537939071655, -0.08853534609079361, -0.11837682873010635, 0.0925372838973999, -0.2016943246126175, 0.11576169729232788, 1.2440965175628662, 0.45514482259750366, 0.036734625697135925, -0.2651776075363159, -0.08292331546545029, -0.2889423072338104, -0.615757942199707, -0.4610680937767029, -0.6298879981040955, -0.1692602038383484, -0.10747044533491135, -0.15646763145923615, 0.06629122793674469]}, "authors": [{"authorId": "39145583", "name": "T. Chu"}, {"authorId": "2214956470", "name": "Zhao Song"}, {"authorId": "2233128774", "name": "Chiwun Yang"}], "references": [{"paperId": "95240dda409e28acccdc5cf619ad0c036cf4292d", "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"}, {"paperId": "0a9030dd6cc2d438509f7568c1081d58c2523d5c", "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning"}, {"paperId": "b169cbff7d5a11afac18f929d5c69ea0933a4da1", "title": "GradientCoin: A Peer-to-Peer Decentralized Large Language Models"}, {"paperId": "86f95df970183931add1216a32db8c3f21c5d488", "title": "Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?"}, {"paperId": "ff6f2b9e56ee0f3f26bcbdc5079678c059fe24e3", "title": "A Theory for Emergence of Complex Skills in Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "13c1a09ed284602d7c21e76c1c63460bb77a4568", "title": "Zero-th Order Algorithm for Softmax Attention Optimization"}, {"paperId": "afa9f128435501d0f2c3ae524a0a7698d0bc3d21", "title": "Fast Quantum Algorithm for Attention Computation"}, {"paperId": "e0cef31283a8b1292f95765242967098c321853b", "title": "Efficient SGD Neural Network Training via Sublinear Activated Neuron Identification"}, {"paperId": "aa62931579f0a03f9a05fc16506a22b166ef0875", "title": "Trainable Transformer in Transformer"}, {"paperId": "ed3101df8bd7b0addfb9ea8713ddb590a15461a2", "title": "Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness"}, {"paperId": "acf1dfa02eefcab63124869f152fd36d2aad172a", "title": "InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding"}, {"paperId": "993df7df129f8d18816877d69923d7df7b347d85", "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion"}, {"paperId": "42cf52baff90952944da0409ec52ff7611ed55dc", "title": "Representational Strengths and Limitations of Transformers"}, {"paperId": "1d2967d96b5e2daa172cb052b22c094beeec3068", "title": "Federated Learning of Gboard Language Models with Differential Privacy"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "ad4b365630f1c13d74d78f0f5d8cee87ef356d41", "title": "Fine-Tuning Language Models with Just Forward Passes"}, {"paperId": "6cb35dd6e1338faa0c3d6a6b0020bbcbcc18653d", "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training"}, {"paperId": "30a60f2e382129bece174536aa3410d45c1bbf1a", "title": "Differentially Private Attention Computation"}, {"paperId": "a569b9daa3606952dbcfdaa310ddfe6ad4eb95f3", "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression"}, {"paperId": "a5125c0eab822af20f55c969696aeaff6f4fdcf9", "title": "Attention Scheme Inspired Softmax Regression"}, {"paperId": "2dd27bed0b030941c25a4ed119b6fe6362d4186b", "title": "Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models"}, {"paperId": "59c9b38f53c5fb91db9a1a5d0cbe6df5dea44a93", "title": "An Over-parameterized Exponential Regression"}, {"paperId": "8625290c1208297b56fbd723cff12b4bb0c7538c", "title": "Solving Regularized Exp, Cosh and Sinh Regression Problems"}, {"paperId": "39ed1c33af6f0a5fbc16354afcb223a03c9c139b", "title": "Fast Attention Requires Bounded Entries"}, {"paperId": "36e21d8093361027088c1977ebaa2acf105c2b28", "title": "On Provable Copyright Protection for Generative Models"}, {"paperId": "cb5b71a622aff47014d4f28a958679629a8b6363", "title": "A Watermark for Large Language Models"}, {"paperId": "3f77a62ae888c3b816eabd354a6dd0fc6b9528ea", "title": "MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer"}, {"paperId": "1a64454d50795e143d19ec45790eecfd28ef6169", "title": "Exploring Vision Transformers as Diffusion Learners"}, {"paperId": "b8543d539ff849fdbb8847d65d19b3bdf6f358ed", "title": "Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing"}, {"paperId": "897f3bb5eacaa80359e81ff33378e1110e20ae95", "title": "All are Worth Words: A ViT Backbone for Diffusion Models"}, {"paperId": "823cacd5255f3897a8d29f29a7c7cb8f978bd928", "title": "CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks"}, {"paperId": "6740d0bdb43a5cfdbf94d0b978b8a529ffc4912b", "title": "Bounding the Width of Neural Networks via Coupled Initialization - A Worst Case Analysis"}, {"paperId": "5eeb80dc67590422db64ca95ec0aded24799cfb6", "title": "Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "8e72e9d4a5008c8afd805cf9a367ecb06fa7eea3", "title": "Training Multi-Layer Over-Parametrized Neural Network in Subquadratic Time"}, {"paperId": "2569a7309142e40815cf556b6417059df9abbda8", "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "d40c77c010c8dbef6142903a02f2a73a85012d5d", "title": "A Survey on Vision Transformer"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "901b546ae60d1e3b6cfe80f19f0786321e701bf4", "title": "Why are Adaptive Methods Good for Attention Models?"}, {"paperId": "bdeec55f95fd6b73e3e4635459b14c7248543efb", "title": "AdapterDrop: On the Efficiency of Adapters in Transformers"}, {"paperId": "082ac328c76bf823d03869da689f28e1c3c03028", "title": "Generalized Leverage Score Sampling for Neural Networks"}, {"paperId": "99ae1c1ac5f06574bafc6fb7acb25fc29eb0b754", "title": "Training (Overparametrized) Neural Networks in Near-Linear Time"}, {"paperId": "7e68da26e18673f2dcf6038cdbdcc8f8f7397505", "title": "Copyright Infringement in AI-Generated Artworks"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8ee2351221b72fca5eef4c42147ed67071903d93", "title": "IntelliCode compose: code generation using transformer"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "e09c5bafc369baae8089a1d49db173b7c6a1273f", "title": "The New Legal Landscape for Text Mining and Machine Learning"}, {"paperId": "724f81cbc1197c83f546568b2aee2f0b9c4e4fd9", "title": "Over-parameterized Adversarial Training: An Analysis Overcoming the Curse of Dimensionality"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "0fdf1a213ed08012d5d21067544b860f40c08e8f", "title": "Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks"}, {"paperId": "0feb7c19f98ac44703126dc46e60d166da1f118c", "title": "An Improved Analysis of Training Over-parameterized Deep Neural Networks"}, {"paperId": "c437640d0ecad7efad695adca55b75e6a8a97410", "title": "Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound"}, {"paperId": "4b5744dd44a0026c6f386d5cb21b795499d5efb7", "title": "Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks"}, {"paperId": "892bc60e9a8ee06bf481044702e44ffba4adee09", "title": "A Gram-Gauss-Newton Method Learning Overparameterized Deep Neural Networks for Regression Problems"}, {"paperId": "8c683510a80158b3c43205805d812230d8fe8165", "title": "Fast Convergence of Natural Gradient Descent for Overparameterized Neural Networks"}, {"paperId": "1029daa28aa772e441470e61bdd610c222e92932", "title": "On Exact Computation with an Infinitely Wide Neural Net"}, {"paperId": "9b81a4df6fbc2702f335ff984381a1634d1be23d", "title": "Toward Moderate Overparameterization: Global Convergence Guarantees for Training Shallow Neural Networks"}, {"paperId": "14558cb69319eed0d5bfc5648aafcd09d882f443", "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"paperId": "1228a5f81dd6d169858cc3378a59065166583126", "title": "On the Convergence Rate of Training Recurrent Neural Networks"}, {"paperId": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0", "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"paperId": "42ec3db12a2e4628885451b13035c2e975220a25", "title": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"paperId": "ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6", "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "ff1e1ac083bfb0160359609c8394005c247cfcc7", "title": "Artificial Intelligence and the Copyright Dilemma"}, {"paperId": "291b4dd78ea80be3e5700a2ddde64dd757c023e7", "title": "PATENTS IN AN ERA OF INFINITE MONKEYS AND ARTIFICIAL INTELLIGENCE"}, {"paperId": "bc6dff14a130c57a91d5a21339c23471faf1d46f", "title": "Et al"}, {"paperId": null, "title": "and Vaikkunth Mugunthan"}, {"paperId": null, "title": "and Sanjeev Arora"}, {"paperId": null, "title": "A phd student's perspective on research in nlp in the era of very large language models"}, {"paperId": null, "title": "and Tianyi Zhou"}, {"paperId": null, "title": "OpenAI Blog"}, {"paperId": null, "title": "pages 16083\u201316122"}, {"paperId": null, "title": "volume 36"}, {"paperId": "850d753aed56ed1a0facc3e87734aa1dfd197e82", "title": "DALL-E: CREATING IMAGES FROM TEXT"}, {"paperId": "da334dd5de7882b1abdebe8a4b6217a423326db5", "title": "FL-NTK: A Neural Tangent Kernel-based Framework for Federated Learning Analysis"}, {"paperId": "b745b5512ad3b1f652dc0cbb5ddf5a940f397f7d", "title": "MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "pages 322\u2013332"}, {"paperId": null, "title": "UC Davis L"}, {"paperId": null, "title": "Copyright Soc\u2019y USA"}, {"paperId": "4d98ce60f4f8ed822503b8d13b0605f8c5d74ca7", "title": "In Advances in Neural Information Processing Systems"}, {"paperId": null, "title": "2023. Try BARD, an AI experiment by Google"}, {"paperId": null, "title": "2022. Speeding up optimizations via data structures: Faster search, sample and maintenance"}, {"paperId": null, "title": "for the 9th circuits, U. S. C. 2022. Copying\u2014Access and Sub-stantial Similarity"}, {"paperId": null, "title": "United State Courts for the 9th circuits. Copying-access and substantial similarity. Model Civil Jury instructions"}, {"paperId": null, "title": "Dall\u00b7e 2 pre-training mitigations"}, {"paperId": null, "title": "2023. Sparks of arti\ufb01cial general intelligence: Early experiments with gpt-4"}, {"paperId": null, "title": "2023. Do Trans-formers Parse while Predicting the Masked Word?"}, {"paperId": null, "title": "2023b. How to Protect Copyright Data in Optimization of Large Language Models?"}]}