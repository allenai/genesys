{"paperId": "c747ba541fa06715543bb6b9b335ce22e5aa1b86", "title": "A Primal-Dual Framework for Transformers and Neural Networks", "abstract": "Self-attention is key to the remarkable success of transformers in sequence modeling tasks including many applications in natural language processing and computer vision. Like neural network layers, these attention mechanisms are often developed by heuristics and experience. To provide a principled framework for constructing attention layers in transformers, we show that the self-attention corresponds to the support vector expansion derived from a support vector regression problem, whose primal formulation has the form of a neural network layer. Using our framework, we derive popular attention layers used in practice and propose two new attentions: 1) the Batch Normalized Attention (Attention-BN) derived from the batch normalization layer and 2) the Attention with Scaled Head (Attention-SH) derived from using less training data to fit the SVR model. We empirically demonstrate the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model's accuracy, and improving the model's efficiency in a variety of practical applications including image and time-series classification.", "venue": "International Conference on Learning Representations", "year": 2024, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work demonstrates the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model's accuracy, and improving the model's efficiency in a variety of practical applications including image and time-series classification."}, "embedding": {"model": "specter_v2", "vector": [0.32559818029403687, 0.6754878163337708, -0.32427021861076355, -0.03253305330872536, 0.20786508917808533, 0.25548267364501953, 0.5451909899711609, -0.3086923658847809, -0.22665594518184662, 0.055543992668390274, 0.5083651542663574, 0.18244227766990662, 0.7932160496711731, 0.02618224546313286, -0.11929922550916672, 0.000561836757697165, -0.6980382204055786, -0.025175465270876884, 0.06935195624828339, -0.7046445608139038, 0.16698381304740906, -0.8898683190345764, -1.1831550598144531, -0.1298357993364334, 0.19188141822814941, 1.0246105194091797, 0.2754995822906494, 0.8131162524223328, -0.6383510828018188, 0.7670649290084839, 0.39082199335098267, -0.3065028786659241, 0.30748215317726135, -0.03956383094191551, -0.45727306604385376, -0.17816372215747833, 0.6087610721588135, -0.24919559061527252, -0.7902651429176331, 0.7203119397163391, -0.31943729519844055, 0.277976393699646, 0.4037191569805145, -0.8042470812797546, -0.2231699377298355, 0.5191442370414734, 0.5444825887680054, 1.359940767288208, -0.371953547000885, -0.6345962285995483, 1.701384425163269, -1.0617084503173828, -0.014458526857197285, 1.2756277322769165, 0.43595999479293823, 0.3394756019115448, -0.24233901500701904, -0.5779116153717041, 0.9944942593574524, 0.3381676971912384, -0.2783154845237732, -0.22068043053150177, 0.4037044644355774, -0.3382810950279236, 1.7603996992111206, -0.3501068651676178, -0.0713958740234375, 0.8162482976913452, 0.10324649512767792, 1.2971715927124023, -0.20950153470039368, -0.47075921297073364, -0.20698551833629608, 0.10301375389099121, 0.5879042148590088, 0.8641876578330994, -0.5470437407493591, 0.29107359051704407, -1.0412871837615967, -0.08909834176301956, 0.6858543157577515, 0.06199080869555473, 0.042813245207071304, -0.32058948278427124, -0.13071846961975098, 0.971933901309967, 0.40922626852989197, 0.6610048413276672, -0.5058361291885376, 0.6531670093536377, 1.0134273767471313, 0.08931946009397507, -0.0703544169664383, 0.2869073748588562, 0.021983299404382706, 0.44962799549102783, -0.47657519578933716, -0.01848355121910572, -0.34147369861602783, 1.0054432153701782, -0.3994590640068054, 0.8188154101371765, -0.6967592239379883, 0.44044220447540283, 0.9866047501564026, -0.11057041585445404, 0.6561179757118225, -0.5622607469558716, 0.23937676846981049, -0.5118312835693359, -0.16462406516075134, -1.3056459426879883, -0.10055064409971237, -0.3892229199409485, -1.163100004196167, -1.1111645698547363, -0.627701461315155, 0.3760718107223511, -0.6623230576515198, 0.8374980688095093, -0.2902226150035858, 0.10250386595726013, -0.14270378649234772, 0.3855423033237457, 0.26694315671920776, 0.7410325407981873, 0.35974377393722534, -0.1568693071603775, 1.0372718572616577, -0.875548243522644, -0.8366536498069763, -0.9148799180984497, 0.026554029434919357, 0.15171858668327332, 0.17441511154174805, -0.17840442061424255, -0.9356251955032349, -1.1948860883712769, -0.7155666351318359, 0.17981968820095062, -0.09451176226139069, -0.014517363160848618, 0.8685729503631592, -0.02126118168234825, -0.78078693151474, 1.1733466386795044, -0.008233374916017056, -0.3037571907043457, 0.6444541215896606, 0.0934973731637001, 0.33498615026474, 0.059608377516269684, -1.4032739400863647, 0.34536927938461304, 0.11271879076957703, -0.5582931637763977, -0.14881828427314758, -0.5168594121932983, -1.3179961442947388, -0.00795146357268095, 0.27972859144210815, -0.3990229666233063, 1.3184071779251099, -0.6436313390731812, -1.0666755437850952, 0.5453094244003296, -0.3756622076034546, -0.1736629605293274, 0.17988169193267822, -0.003776208497583866, -0.33369696140289307, -0.467091828584671, 0.11268225312232971, -0.026243368163704872, 0.799384355545044, -0.06940409541130066, -0.5174042582511902, 0.036911703646183014, -0.6808382868766785, -0.4539271593093872, -0.5255861282348633, 0.7503234148025513, -0.2328825742006302, -0.5226646661758423, 0.25601184368133545, 0.9107242822647095, 0.22256870567798615, -0.39823269844055176, -0.06840100884437561, -1.3873426914215088, 0.7138022184371948, -0.02652026154100895, 0.8380858302116394, -0.8226199746131897, -0.7735686898231506, -0.2738431692123413, -0.15908852219581604, -0.0728849545121193, -0.7915691137313843, 0.471518874168396, -0.9323803782463074, 0.15464939177036285, -0.012525055557489395, -1.068302869796753, -0.32296085357666016, 0.06656994670629501, -1.1122007369995117, -0.004682078026235104, 0.3211914896965027, 0.8359274864196777, -1.2412216663360596, -0.17043647170066833, -0.19287486374378204, 0.06367149204015732, -0.5696975588798523, 1.450124979019165, -0.18357929587364197, -0.17106445133686066, 0.05240856483578682, -0.18374887108802795, -0.1755480319261551, -0.4404999911785126, 0.27067020535469055, -0.6396079063415527, -0.1312151849269867, 0.4260522127151489, -0.3724724054336548, 0.7295802235603333, 0.033089637756347656, 0.930863618850708, -0.22987142205238342, -1.1960760354995728, 0.3567778766155243, 0.40394335985183716, 0.17272335290908813, -0.7653635740280151, 0.28180167078971863, -0.17506515979766846, -1.192466139793396, -0.021066317334771156, 0.6927742958068848, 0.8118470311164856, -0.1958264410495758, 0.010884035378694534, 0.7804564833641052, 0.007121935952454805, 0.4271441400051117, 0.33688589930534363, 0.4124469459056854, 0.6183731555938721, 0.5184542536735535, 0.01882180944085121, 0.19264838099479675, -0.6756536960601807, 0.5018647313117981, 0.5359290242195129, 0.5070578455924988, 0.9466445446014404, 0.6410085558891296, -0.8820395469665527, -0.722454309463501, 0.29473721981048584, 0.6994217038154602, 1.676135540008545, -0.4014489948749542, -0.1635339856147766, -0.4050430953502655, -0.31834810972213745, -0.10186562687158585, -0.021079905331134796, -0.540197491645813, -0.11211292445659637, -0.45286914706230164, -1.009981632232666, 0.48422113060951233, 0.681185245513916, 0.7743766903877258, -0.5154911279678345, -0.28453195095062256, -0.09985309094190598, 0.29175034165382385, -0.7436683177947998, -0.7696645259857178, 0.47770506143569946, -0.6476659774780273, -0.20626509189605713, 0.013824514113366604, -0.22020278871059418, 0.006633025594055653, -0.3982958197593689, 0.9173290133476257, -0.5650815963745117, -0.1760670691728592, 0.3707168698310852, 0.3981330096721649, -1.0250003337860107, -0.40952613949775696, 0.014860584400594234, 0.6197174787521362, 0.019604505971074104, 0.16756446659564972, 0.22993846237659454, 0.23508073389530182, -0.03792724758386612, -0.1651805341243744, -0.05291777104139328, -0.04407088831067085, 0.4078295826911926, 0.40434935688972473, -0.29451265931129456, 0.0342596136033535, -1.0441110134124756, 0.7169362902641296, -0.003111284226179123, -0.7319069504737854, 0.10407638549804688, -0.9219157695770264, -0.13492779433727264, 0.37936729192733765, -0.7491711974143982, -0.0017071808688342571, -0.8436651825904846, 0.6686578989028931, -0.830512285232544, -0.009539604187011719, 0.24304312467575073, 0.6305590867996216, 0.230539470911026, 0.35272669792175293, 0.3973851501941681, 0.6237354874610901, 0.39939039945602417, 0.07004722207784653, -0.9922254085540771, 0.7588277459144592, 0.15267808735370636, -0.06591232866048813, -0.27036336064338684, -0.14339818060398102, -1.1104793548583984, -0.644315242767334, -0.4900493025779724, -0.021498611196875572, -0.03916342183947563, 0.13945487141609192, -0.5328429341316223, -1.213942527770996, -0.15406452119350433, -1.0532495975494385, -0.11825720965862274, -0.09265926480293274, -0.45903071761131287, -0.19784922897815704, -0.7914584279060364, -0.9434990286827087, -0.8876286149024963, -0.7442569136619568, -0.4776816666126251, -0.020454389974474907, 0.487566739320755, -0.17479562759399414, -0.43993231654167175, -0.04612544924020767, -0.5143176317214966, 1.0872375965118408, -0.461629182100296, 0.7761876583099365, -0.133747398853302, 0.034983858466148376, -0.037445127964019775, 0.10276374220848083, 0.8006661534309387, 0.12130308896303177, 0.2576567232608795, -0.8163499236106873, 0.25119397044181824, -0.04149618744850159, -0.11835166811943054, 0.15744197368621826, 0.446220338344574, 0.7503880262374878, -0.1040220707654953, -0.24138008058071136, 0.3110859990119934, 1.396804928779602, -0.41095903515815735, 0.0705462098121643, 0.26610422134399414, 0.9726428389549255, 0.16418448090553284, -0.3831256926059723, 0.6609929800033569, 0.5153299570083618, 0.2831757068634033, 0.5604674816131592, -0.36340150237083435, 0.27818238735198975, -0.4328584372997284, 0.2562990188598633, 1.2981736660003662, -0.06150388345122337, 0.33783188462257385, -1.0541969537734985, 0.9535067677497864, -1.2969974279403687, -1.1523874998092651, 0.5014521479606628, 0.43077000975608826, 0.10188446938991547, -0.3458399772644043, -0.024989329278469086, -0.1647602915763855, 0.5877523422241211, 0.3414318263530731, -0.6030810475349426, -0.5648700594902039, -0.1284438669681549, 0.31443947553634644, 0.3898741900920868, 0.5536943674087524, -0.6119362711906433, 0.4402177631855011, 15.121844291687012, 0.1082235723733902, -0.37722763419151306, 0.5654685497283936, 0.7310418486595154, 0.19348154962062836, 0.12043487280607224, -0.2552429437637329, -0.9688762426376343, 0.006240496411919594, 0.8177234530448914, 0.08397360891103745, 0.566531240940094, 0.5474464297294617, -0.07281991839408875, 0.3680781126022339, -0.5659091472625732, 1.123998761177063, 0.7466524839401245, -1.1559250354766846, 0.12798884510993958, 0.040697287768125534, 0.1658901870250702, 0.42968493700027466, 0.5238762497901917, 0.88374263048172, 0.2960223853588104, -0.11810556054115295, 0.4944249093532562, 0.3224308490753174, 0.7645847797393799, 0.14122818410396576, 0.5302515029907227, 0.24540796875953674, -0.8099859952926636, -0.4112634062767029, -0.6442489624023438, -1.0981093645095825, 0.09714166074991226, -0.029694123193621635, -0.6109960675239563, -0.23045504093170166, 0.28071120381355286, 0.6738495230674744, 0.3692162334918976, -0.17140822112560272, -0.16526247560977936, 0.5330883860588074, 0.3673768937587738, -0.09159526228904724, 0.028553182259202003, 0.46738266944885254, 0.12463421374559402, 0.1267683357000351, -0.25761035084724426, 0.25659510493278503, 0.09699275344610214, 0.3354911208152771, -0.2619973123073578, -0.17522810399532318, -0.25616079568862915, -0.40497639775276184, -0.47719985246658325, 0.8242443203926086, 0.9245998859405518, -0.009214871563017368, -0.24970975518226624, 0.4256404936313629, 0.4670921266078949, 0.20902083814144135, -0.3244878053665161, -0.25567448139190674, 0.4952681362628937, -0.2870800495147705, 0.21268309652805328, 0.4530981183052063, -0.41523316502571106, -0.5578127503395081, -0.9329395890235901, -0.4223906397819519, 0.5633310079574585, -1.07268488407135, -1.123565435409546, 0.9760376811027527, -0.32903966307640076, -0.1556667536497116, 0.31431153416633606, -1.1205252408981323, -0.40172404050827026, 0.7201505303382874, -1.2874794006347656, -0.1518276035785675, -0.31156134605407715, -0.1501932442188263, -0.12324180454015732, -0.3883318603038788, 0.9859235882759094, -0.19467005133628845, -0.7763814330101013, -0.17260797321796417, -0.2921409606933594, 0.18643121421337128, -0.36150652170181274, -0.8604552745819092, 0.755714476108551, 0.32557055354118347, -0.48820680379867554, 0.3039841949939728, 0.24871739745140076, 0.24985826015472412, -0.5420363545417786, 0.054282285273075104, 0.6338641047477722, -0.5215209722518921, -0.19334149360656738, -0.6740800738334656, -1.1889523267745972, 0.2114342600107193, 0.4512052536010742, -0.076791912317276, 0.19157196581363678, 0.1533598154783249, -0.43695908784866333, -0.35999077558517456, -0.5069289207458496, -0.0987623855471611, 0.25721344351768494, -0.8559220433235168, -0.7500203251838684, -0.10392799973487854, 0.27478376030921936, -0.7704217433929443, -0.24785837531089783, -0.3697223365306854, -0.06344355642795563, 0.006151174195110798, 0.9715056419372559, -0.9312712550163269, 0.7597085237503052, 0.7966741919517517, -0.17648567259311676, -0.577630341053009, -0.6669087409973145, -0.7875096201896667, -0.08159078657627106, 0.49319031834602356, 0.024483876302838326, -0.5069264769554138, 0.34500986337661743, 0.8314818143844604, 0.33288976550102234, -0.2932855486869812, -0.5488724708557129, -0.1364193856716156, -0.520362913608551, -0.49319908022880554, 0.1607089340686798, 0.41586750745773315, 0.038458164781332016, 0.4451547861099243, 0.38045936822891235, 0.3748597204685211, -0.08006588369607925, -0.7707737684249878, 0.10430344939231873, -0.3790030777454376, 0.38682064414024353, -0.6430637836456299, -0.41862770915031433, -1.1966890096664429, 0.07177092880010605, -1.015722393989563, -0.155600443482399, -1.0297774076461792, -0.5023107528686523, 0.3475024104118347, -0.4844905734062195, 0.33551597595214844, 0.17939670383930206, -0.658769428730011, -0.4261421263217926, -0.4832471013069153, -0.13085772097110748, 1.0654785633087158, 0.4360900819301605, -0.9098454713821411, -0.09429945796728134, -0.08044105023145676, -0.24478118121623993, 0.12578028440475464, 0.6927969455718994, -0.5753566026687622, -0.5556859970092773, -0.8640769124031067, 0.3497864305973053, 0.10195378959178925, -0.10583508759737015, -0.5387722253799438, 0.7437098622322083, -0.036811307072639465, 0.11418316513299942, -0.03576838970184326, 0.2913397550582886, -0.8387421369552612, -0.6891966462135315, 0.29310309886932373, -0.6181910634040833, 0.38671332597732544, 0.20288121700286865, -0.3940889537334442, -0.18169233202934265, 0.8097448348999023, 0.02892850711941719, -1.2094926834106445, -0.660559356212616, 0.7652180194854736, -0.48702332377433777, 0.14780764281749725, -0.10205835849046707, -0.1858575940132141, -1.1006512641906738, -0.1925664246082306, 0.29369500279426575, 0.585904061794281, -0.6047175526618958, 1.009137749671936, 0.5423234701156616, -1.2384662628173828, 0.03629299998283386, 0.7501953840255737, 0.030141441151499748, 0.07017239183187485, 0.5409083962440491, 0.25798168778419495, 0.033724285662174225, 0.2836625277996063, 0.014973779208958149, 0.0924660935997963, -0.8057951331138611, 0.44074124097824097, 0.8968799114227295, -0.1821785569190979, -0.2721737325191498, 1.0577661991119385, -0.1815246194601059, -0.6709337830543518, 0.264317125082016, -1.1222202777862549, -0.9651809930801392, 0.10932577401399612, 0.6912888884544373, 0.3166173994541168, -0.27506521344184875, -0.18655993044376373, -0.5685933232307434, 0.09211966395378113, -0.005496311001479626, -0.23460465669631958, 0.9608333110809326, -0.05709494277834892, -0.3206348419189453, 1.0369771718978882, 0.6306646466255188, -1.0654191970825195, -0.9538870453834534, -1.0423543453216553, -0.42528924345970154, -0.27588552236557007, 0.26706933975219727, 0.191453218460083, -0.7277045249938965, 0.6458994746208191, 0.6779504418373108, 0.6566947102546692, 0.13127776980400085, -0.31365010142326355, -0.210616797208786, 0.3388720154762268, -0.20861631631851196, -0.8296147584915161, -0.1761867254972458, 1.5712791681289673, 1.624199628829956, -0.7818684577941895, 0.2293938547372818, -0.06685483455657959, -1.0694420337677002, 0.9402125477790833, 0.4829707145690918, -0.2942335307598114, 0.5747946500778198, -0.600678026676178, -0.140243798494339, 0.03471101075410843, -1.1598747968673706, -0.28511476516723633, 0.8734386563301086, 1.2232705354690552, 0.5836562514305115, -0.1359846293926239, 0.6003981232643127, 0.8537055253982544, 0.23298196494579315, 0.15527833998203278, 0.5520659685134888, 0.33894842863082886, -0.10153377801179886, 0.3036651611328125, -0.07601545751094818, 0.5884016156196594, -0.6604394912719727, -0.40996405482292175, 0.4550306499004364, 0.31589168310165405, 0.010361081920564175, 0.3664039075374603, 0.9499338269233704, -0.15001440048217773, 1.052927851676941, 0.17082303762435913, 0.23741324245929718, -0.48364579677581787, -0.24219143390655518, -0.3646223545074463, -0.8367292284965515, -0.24138109385967255, -0.1692546010017395, -0.8842591047286987, -0.10933563113212585, 0.02960371971130371, 0.48849546909332275, -0.1772632896900177, 0.4886287450790405, 0.8714942932128906, 0.5022228360176086, 0.8896169066429138, 0.2405589371919632, -0.7300440073013306, -0.5150288343429565, -0.9396158456802368, -0.05686300992965698, -0.39540421962738037, 0.407284677028656, -0.41622060537338257, -0.23529818654060364, -0.14622816443443298]}, "authors": [{"authorId": "150322732", "name": "T. Nguyen"}, {"authorId": "2116488139", "name": "Tam Nguyen"}, {"authorId": "3526349", "name": "Nhat Ho"}, {"authorId": "144722242", "name": "A. Bertozzi"}, {"authorId": "144908066", "name": "Richard Baraniuk"}, {"authorId": "103583159", "name": "S. Osher"}], "references": [{"paperId": "39f0f28848990f74eeb9019f579c6ebcc8ef3ea1", "title": "TransTab: Learning Transferable Tabular Transformers Across Tables"}, {"paperId": "d163cca5cfea5d967873d34023554e3d1771716b", "title": "Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers"}, {"paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a", "title": "Flowformer: Linearizing Transformers with Conservation Flows"}, {"paperId": "fe9d978f7718474e9613bac114c398614f09be71", "title": "Sinkformers: Transformers with Doubly Stochastic Attention"}, {"paperId": "48af9b314181b04edcc0b7224ffe4689036b755f", "title": "Improving Transformers with Probabilistic Attention Keys"}, {"paperId": "23d11338be48471b3979b13eb172ec67fc22244b", "title": "Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "94eae578e6af3382f6449506965639f18aab3fa0", "title": "Video Swin Transformer"}, {"paperId": "0d46cbaf914da31a06ef2753e00b7f47e055e70d", "title": "Probabilistic Attention for Interactive Segmentation"}, {"paperId": "5863d7b35ea317c19f707376978ef1cc53e3534c", "title": "Rethinking Graph Transformers with Spectral Attention"}, {"paperId": "f864d4d2267abba15eb43db54f58286aef78292b", "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "72f207c777e4a17180cc54ccc6a743d5f43227af", "title": "Choose a Transformer: Fourier or Galerkin"}, {"paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "title": "ViViT: A Video Vision Transformer"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "ff50b46b4e1cc0fd9beb832fc3468785b635a824", "title": "PCT: Point cloud transformer"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "e1d082562981a9f51649c60663aa484ee623dbb0", "title": "Point Transformer"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "372cce47fa16c538946972e6a7ac8420e64000b0", "title": "Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval"}, {"paperId": "2051548f7681c96d603de932ee23406c525276f9", "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "8e711d80805a18d75965aba1d423f995f2121dac", "title": "Monash University, UEA, UCR Time Series Regression Archive"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "42b0d32a7e644b657e34ce056c84d215d9f62187", "title": "Universal"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "199ff73d2f728e997f860b62a2322823d3e3d9e8", "title": "Designing and Interpreting Probes with Control Tasks"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "830995ef17cc291c13f42dfd9f462137de1d2179", "title": "Augmenting Self-attention with Persistent Memory"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "81e1d123a85562555befb0243256b1a0d9fca014", "title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145", "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "d8abb8206b913d185b4bd406880131c13759a6ff", "title": "The UEA multivariate time series classification archive, 2018"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "de95601d9e3b20ec51aa33e1f27b1880d2c44ef2", "title": "CBAM: Convolutional Block Attention Module"}, {"paperId": "0d3c46a3cbfe06cec259fec954b6ff6df6c1a566", "title": "Learning long-range spatial dependencies with horizontal gated-recurrent units"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "8899094797e82c5c185a0893896320ef77f60e64", "title": "Non-local Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks"}, {"paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "06bb5771e6b8a9356c5f4ae28c98b4397c043349", "title": "A tutorial on support vector regression"}, {"paperId": "fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4", "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond"}, {"paperId": "52b7bf3ba59b31f362aa07f957f1543a29a4279e", "title": "Support-Vector Networks"}, {"paperId": "ba612bafda906c9e26b8e81d2548a7dde6434183", "title": "Probabilistic Transformer For Time Series Analysis"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Deep learning based recommender system: A survey and new perspectives"}, {"paperId": null, "title": "Set trans-former: A framework for attention-based permutation-invariant neural networks"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "2015) consists of 1.28M training images and 50K validation images. The task is to classify 1000 categories"}, {"paperId": null, "title": "An introduction to the finite element method , volume 1221"}, {"paperId": null, "title": "of the LRA benchmark, we set the downsampling factors s of Attention-SH/BN+SH, Linear/Sparse Attention-SH/BN+SH is [1, 2] and kernel size of Attention-Conv1D models is 5"}, {"paperId": null, "title": "Models and baselines All our models and softmax/linear baselines follow the same architecture and configuration as in"}, {"paperId": null, "title": "A.3 IMAGE CLASSIFICATION ON IMAGENET"}, {"paperId": null, "title": "In all models, the number of heads is 8, whereas the model dimension and number of transformer layers are varied. For Attention-SH/SH+BN, we downsample keys and values by the factor of 2"}, {"paperId": null, "title": "LASSIFICATION task\u2019s \u03b2 of Attention-BN/BN+SH is 1. Attention-SH/BN+SH has the downsampling factor of 2 , 4] , and the kernel size of Attention-Conv2D is (2 , 2) . 26"}]}