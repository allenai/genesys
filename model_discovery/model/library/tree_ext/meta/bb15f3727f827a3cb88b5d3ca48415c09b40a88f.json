{"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?", "abstract": "The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale alone. In the process of building BLOOM--the Big Science Large Open-science Open-access Multilingual language model--our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization. In addition, we study the impact of various popular pre-training corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behaviour of Transformers to choose the target model size, shape, and training setup. All our models and code are open-sourced at https://huggingface.co/bigscience .", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 80, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/2210.15424", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "An ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization is performed, and the scaling behaviour of Transformers is considered to choose the target model size, shape, and training setup."}, "embedding": {"model": "specter_v2", "vector": [0.2456095963716507, 0.45434242486953735, -0.21338124573230743, -0.0998106449842453, -0.42817482352256775, -0.2158311903476715, 0.8801563382148743, -0.5245076417922974, -0.7536901831626892, -0.48205700516700745, 0.5358279347419739, -0.34340977668762207, 0.27379587292671204, -0.08181776851415634, 0.05731102079153061, 0.04826442524790764, -0.7026761770248413, 0.5707283616065979, -0.3797209858894348, -0.6644720435142517, -0.5488947033882141, -0.5568998456001282, -1.1626962423324585, -0.11525148898363113, 0.26626521348953247, 0.3005656898021698, 0.25858810544013977, 0.842962920665741, -0.2635830342769623, 0.21609492599964142, 0.6926148533821106, -0.3130797743797302, 0.3180231750011444, 0.07472183555364609, -0.21649284660816193, -0.13719546794891357, 0.5757510662078857, -0.5397565364837646, -0.47161877155303955, 0.7680947184562683, -0.13303114473819733, 0.42603883147239685, 0.3597751259803772, -0.9123843908309937, -0.31451985239982605, 0.6827819347381592, 0.884401261806488, 0.689694881439209, -0.4225342571735382, -0.2042965590953827, 1.0612833499908447, -1.1989778280258179, 0.3781260550022125, 1.5096355676651, 0.5002848505973816, 0.4358542859554291, -0.30510711669921875, -0.5161514282226562, 0.4808696508407593, -0.22744120657444, -0.7980547547340393, -0.4772448241710663, -0.4411763846874237, -0.16320553421974182, 1.9052047729492188, -0.49400749802589417, 0.25992438197135925, 0.5269548296928406, 0.07954925298690796, 0.9975549578666687, -0.03764968365430832, -0.9201724529266357, -0.5328903198242188, 0.019027533009648323, 0.5193353891372681, 0.8927645087242126, -0.4296211302280426, 0.5253660082817078, -0.7250730395317078, -0.14325974881649017, 0.4632803499698639, -0.2849275469779968, 0.1939748078584671, -0.07772617787122726, -0.3876536190509796, 0.9348573088645935, 0.08562833815813065, 0.8415672779083252, 0.1070438027381897, 0.7595399022102356, 0.7029119729995728, 0.7916390895843506, 0.3722902238368988, 0.3019697964191437, -0.09232305735349655, 0.4457778334617615, -1.1032830476760864, 0.23887506127357483, 0.24635735154151917, 1.0031046867370605, 0.03953970596194267, 0.38401666283607483, -0.594413161277771, 0.19155600666999817, 1.2496308088302612, -0.06648625433444977, 0.5438723564147949, -0.734078049659729, -0.0466611422598362, -0.9472768306732178, -0.15578871965408325, -0.2028142511844635, -0.25976619124412537, -0.3519349992275238, -0.808402419090271, -1.4284321069717407, -0.1371229588985443, 0.28587764501571655, -1.0060100555419922, 0.7913461327552795, -0.2007276713848114, -0.010035173036158085, 0.2035847306251526, 0.5630531311035156, 0.7948804497718811, 0.6434592604637146, 0.4448874294757843, 0.35940590500831604, 0.8894029855728149, -0.778314471244812, -0.26460781693458557, -0.9332470893859863, 0.9752256274223328, -0.66596519947052, 0.6749414801597595, 0.014334779232740402, -1.0463720560073853, -0.9844728112220764, -0.6882314682006836, -0.19525344669818878, -0.764177143573761, 0.43960538506507874, 1.2934225797653198, 0.8039958477020264, -1.024807095527649, 0.4774620831012726, -0.12619873881340027, -0.5630882382392883, -0.11635682731866837, -0.03929387778043747, 0.017816511914134026, -0.7690368890762329, -1.6288586854934692, 0.4554457664489746, 0.4689622223377228, -0.6918492913246155, -0.23668433725833893, -0.7347692251205444, -1.087436318397522, -0.10012470185756683, 0.14230044186115265, -0.481973797082901, 1.0281919240951538, -0.04202430695295334, -1.2176737785339355, 0.9901356101036072, -0.34704071283340454, 0.07395703345537186, 0.357079416513443, -0.005409265868365765, -0.638095498085022, -0.7630318403244019, -0.2657102346420288, 0.519233763217926, 0.3621101677417755, -0.06990866363048553, 0.03855839744210243, 0.16283373534679413, -0.06870222091674805, 0.15298736095428467, -0.11936958134174347, 1.065443992614746, -0.4624817669391632, -0.207444429397583, 0.4960339069366455, 0.32698652148246765, -0.11570487916469574, -0.46211376786231995, -0.34240731596946716, -1.1161037683486938, 0.33764731884002686, -0.1446746289730072, 0.5383673906326294, -0.9113540649414062, -0.7700381875038147, -0.04412643611431122, -0.18995468318462372, -0.5323526263237, -0.6082990169525146, 0.8792265057563782, -0.26605460047721863, 0.24329277873039246, 6.271100573940203e-05, -0.976112425327301, 0.4151472747325897, -0.42284536361694336, -0.7336077690124512, 0.1713070422410965, -0.3009200394153595, 0.7833881974220276, -0.6551579833030701, 0.45425689220428467, 0.11741991341114044, 0.4733639657497406, -1.1404095888137817, 1.011847972869873, -0.23070114850997925, 0.03373965993523598, 0.32346975803375244, -0.29903680086135864, 0.28986936807632446, -0.3621204197406769, 0.31625500321388245, -0.35640037059783936, -0.22317726910114288, 0.20899946987628937, -0.26729363203048706, 1.7819316387176514, -0.544428825378418, 0.235398530960083, -0.14482714235782623, -0.5336422324180603, 0.15439455211162567, 0.38523411750793457, -0.33393386006355286, -0.46407583355903625, 0.34970352053642273, 0.9393192529678345, -0.2641201615333557, 0.8034164309501648, 0.7530655860900879, 0.55341637134552, -0.4653396010398865, 0.33204519748687744, 0.8158094882965088, -0.5378693342208862, 0.6126651763916016, 0.6237660646438599, 0.25963911414146423, 0.4390367865562439, 0.11706535518169403, -0.547504723072052, 0.25183334946632385, -0.7932634949684143, -0.476097047328949, 0.8517066240310669, 0.7198488712310791, 0.8128848075866699, -0.08987779915332794, -0.9102973937988281, -0.6931380033493042, -0.23757973313331604, 0.6124112606048584, 1.8172730207443237, -0.2480204850435257, -0.2731248140335083, -0.6792997717857361, 0.25052887201309204, -0.5450382828712463, 0.2920755445957184, -0.26575011014938354, -0.08215567469596863, -0.6878946423530579, -1.178465485572815, 0.503308892250061, -0.27956610918045044, 0.5567954182624817, -0.26406973600387573, -0.2985885739326477, -0.2493404895067215, 0.3932953178882599, -0.7963703274726868, -0.5834370851516724, 0.17966924607753754, -0.2632153332233429, 0.28616562485694885, -0.44560506939888, -0.19373735785484314, -0.20769858360290527, -0.446665495634079, 0.9751294255256653, -0.36664000153541565, -0.49066630005836487, 0.20199216902256012, 0.7293537855148315, -0.4585270881652832, -1.0432621240615845, 0.4079923629760742, 0.2880929410457611, -0.35717612504959106, 0.22202658653259277, 0.44718196988105774, 0.0925031378865242, -0.08382755517959595, -0.12258826941251755, 0.38663750886917114, -0.16882692277431488, -0.21662096679210663, 0.4115835428237915, -0.3291033208370209, -0.25802063941955566, -1.2442480325698853, 1.0615780353546143, 0.22844849526882172, -0.47883012890815735, 0.6534262895584106, -0.6435605883598328, -0.42728903889656067, 0.7369029521942139, -0.7582583427429199, -0.08965927362442017, -0.9864060878753662, 0.429157555103302, -0.280416339635849, -0.08907774090766907, 0.1262877881526947, 0.2332896888256073, 0.31689369678497314, 0.26572489738464355, 0.7315574288368225, 0.13905970752239227, -0.2473505586385727, 0.8849999308586121, -1.1097959280014038, 0.20933164656162262, 0.00516272708773613, -0.0293137114495039, 0.15363934636116028, -0.46872514486312866, -0.8278868198394775, -0.3855413794517517, -0.2866338789463043, -0.10969185829162598, -0.3601175844669342, 0.07121569663286209, -0.790419340133667, -0.5826084613800049, 0.410844087600708, -0.9638034701347351, -0.2327396124601364, 0.3523836135864258, -0.20579905807971954, -0.42881420254707336, -1.2255669832229614, -1.3239753246307373, -0.25801387429237366, -0.856548011302948, -1.2674837112426758, 0.4372568726539612, -0.1994844228029251, -0.2305954098701477, -0.4403553903102875, -0.031165219843387604, -0.39217516779899597, 1.129666805267334, -0.8984062671661377, 0.9325853586196899, -0.13125933706760406, -0.017381099984049797, -0.11624382436275482, 0.030582446604967117, 0.3329618275165558, -0.43358728289604187, 0.2979307472705841, -0.741047203540802, -0.33749306201934814, -0.43862804770469666, -0.6566340923309326, 0.16987083852291107, 0.3030775487422943, 0.4658142924308777, 0.08693970739841461, -0.2561751902103424, 0.7153875231742859, 1.554209589958191, -0.7850099205970764, 0.15160030126571655, -0.0603497140109539, 1.1602356433868408, 0.28063705563545227, -0.5382394790649414, 0.3006425201892853, 0.526469349861145, 0.26018026471138, -0.4011356234550476, -0.02619563788175583, -0.08298952132463455, -0.6584949493408203, 0.5491766929626465, 2.019981861114502, 0.37236571311950684, -0.24365365505218506, -1.0984162092208862, 0.41025257110595703, -0.9638422727584839, -0.4883608818054199, 0.6744482517242432, 0.8461791276931763, 0.16119946539402008, -0.45448392629623413, -0.2689964771270752, -0.4282776117324829, 0.1898019015789032, 0.44971224665641785, -0.027508307248353958, -0.8818316459655762, -0.24333228170871735, 0.3851318955421448, 0.1779131144285202, 0.580670952796936, -0.2792735993862152, 1.0411595106124878, 14.735575675964355, 1.5658338069915771, -0.2673529386520386, 0.8330894112586975, 0.9321467876434326, 0.2109772115945816, -0.6310166716575623, 0.1389082968235016, -1.368050456047058, -0.2226172238588333, 1.5003306865692139, -0.08664852380752563, 1.0759356021881104, 0.04466976970434189, 0.07266340404748917, 0.0692167803645134, -0.19338780641555786, 0.6600468754768372, 0.585788369178772, -1.376162052154541, 0.7408283352851868, 0.15700212121009827, 0.42166781425476074, 0.8907017707824707, 0.6493297219276428, 0.8591096997261047, 0.6652384400367737, -0.8597278594970703, 0.74217289686203, 0.33734965324401855, 0.979859471321106, -0.2624030113220215, -0.016875868663191795, 0.6410924792289734, -0.9327292442321777, 0.033902425318956375, -0.8079394102096558, -1.0732308626174927, 0.20496636629104614, 0.054933179169893265, -0.4364018142223358, -0.4106055498123169, -0.3535170555114746, 0.38381636142730713, 0.21022973954677582, 0.15983571112155914, 0.3550395965576172, 0.6999031901359558, -0.5887354612350464, 0.14142051339149475, 0.539146363735199, 0.01783982664346695, 0.18569320440292358, 0.05753608047962189, 0.298797070980072, -0.17286370694637299, 0.40990468859672546, 0.49137526750564575, -0.3494251072406769, 0.25937145948410034, -0.35001859068870544, -0.27013954520225525, -0.1460016965866089, 0.824255108833313, 0.09875136613845825, 0.19005747139453888, -0.41509273648262024, 0.19301873445510864, 0.9122862219810486, 0.20099323987960815, -0.02956528030335903, 0.089043527841568, 0.3105895519256592, -0.5372885465621948, -0.2274853140115738, 0.3306843638420105, 0.11404845118522644, -0.84748774766922, -0.9234494566917419, -0.672884464263916, 0.3077944219112396, -0.6097705364227295, -0.736652672290802, 0.8439428806304932, -0.19693927466869354, -0.07169235497713089, 0.415383905172348, -0.6136061549186707, -0.08564215898513794, 0.7121642231941223, -1.151652216911316, -1.3677644729614258, 0.6253725290298462, -0.3917820453643799, -0.20612555742263794, -0.14925052225589752, 1.4632505178451538, 0.17016884684562683, -0.1415565460920334, 0.1990795135498047, 0.45264318585395813, -0.17148002982139587, -0.45129674673080444, -0.41507840156555176, 1.344294548034668, 0.004809151869267225, 0.07466374337673187, 0.5093145370483398, 0.19208887219429016, 0.30775630474090576, -0.9135980606079102, 0.21682165563106537, 0.9541794657707214, -0.7908725142478943, -0.25438392162323, -1.301418662071228, -0.6834648847579956, 0.162155881524086, 0.43823787569999695, -0.28607845306396484, 0.1358717978000641, 0.23310694098472595, -0.7772413492202759, 0.14538346230983734, -0.46584945917129517, 0.04256141930818558, 0.8631873726844788, -1.0298305749893188, 0.11681050807237625, 0.16385987401008606, 0.20671485364437103, -0.9603595733642578, -0.5621904730796814, -0.18001388013362885, 0.4818457365036011, -0.15155130624771118, 0.9111735820770264, -0.5100000500679016, 0.3711375296115875, 1.2183151245117188, -0.3893405795097351, -0.9274451732635498, 0.08624313771724701, -0.829980731010437, 0.45172274112701416, -0.12797535955905914, 0.6097021102905273, -0.6469656229019165, 0.10628491640090942, 0.8977355360984802, 0.4321577250957489, -0.566035270690918, -0.5672052502632141, -0.5830889344215393, 0.6073529124259949, -0.741790235042572, 0.5671280026435852, -0.09736252576112747, -0.3623051941394806, 0.08300549536943436, 0.27657338976860046, 0.3798880875110626, -0.10588093101978302, -0.7234799861907959, 0.5905234217643738, -0.001182988635264337, -0.09463762491941452, -0.5169724822044373, -0.31107816100120544, -1.409550666809082, 0.18264135718345642, -1.292708158493042, -0.10053257644176483, -0.8756600022315979, -0.2609245479106903, -0.28836843371391296, -0.13132362067699432, -0.3180992305278778, 0.7276041507720947, -0.18259288370609283, -0.30641990900039673, -0.46300336718559265, -0.22177229821681976, 0.8286318778991699, 0.8768527507781982, -0.6438698768615723, 0.469684362411499, -0.31528177857398987, 0.4256103038787842, 0.3949088156223297, 0.3348185420036316, -0.3452393412590027, -1.1217031478881836, -1.4937007427215576, 0.4215998947620392, -0.17903102934360504, 0.00305676250718534, -0.7096667289733887, 0.41898661851882935, 0.2684246599674225, -0.5360147356987, 0.3578442633152008, 0.4283527731895447, -0.37516409158706665, -0.09769504517316818, 0.14846056699752808, -0.6104527711868286, 0.29959797859191895, 0.2571471035480499, -0.9235876798629761, -0.01696876995265484, 0.614010751247406, -0.10562551021575928, -1.3134993314743042, -0.6850630640983582, 0.46527764201164246, -0.49100083112716675, 0.477615624666214, -0.5929259061813354, -0.0009935538982972503, -0.9484578371047974, -0.40282657742500305, -0.13965719938278198, -0.045302458107471466, 0.016624566167593002, 1.13776695728302, 0.21038983762264252, -0.9145850539207458, -0.25368794798851013, 0.3681591749191284, 0.03713341802358627, -0.5948674082756042, 0.7277459502220154, 0.18007168173789978, -0.4195210337638855, 0.5609692335128784, 0.3678797781467438, 0.353750616312027, -0.7131268382072449, -0.29525792598724365, 0.83296799659729, -0.6021546125411987, -0.42775821685791016, 1.3745718002319336, -0.12696108222007751, -1.5914276838302612, -0.3086003363132477, -0.7315015196800232, -0.14784735441207886, -0.5104401707649231, 0.5039615035057068, 0.19193895161151886, 0.22315539419651031, -0.40112021565437317, -0.3405342996120453, 0.09800317883491516, -0.22529630362987518, -0.8573547005653381, 0.3621773421764374, -0.12776587903499603, -0.15484769642353058, 0.8362645506858826, 1.070604681968689, -0.7725053429603577, -0.4655787944793701, -0.401641309261322, -0.7243307828903198, 0.0868760496377945, 0.4129260778427124, -0.37964749336242676, -0.46820056438446045, 0.8401787281036377, 0.4699053168296814, 0.3832590878009796, -0.2787467837333679, -0.31899192929267883, 0.18375486135482788, 0.5944169759750366, 0.5985221862792969, -0.7114980220794678, -1.2416342496871948, 1.6027979850769043, 0.8846762180328369, -0.915970504283905, 0.11057911813259125, -0.36822861433029175, -0.5315559506416321, 0.6632179021835327, 0.3537243902683258, -0.2863672375679016, 1.0028530359268188, -0.2881748676300049, 0.329989492893219, 0.05558455362915993, -1.0069544315338135, -0.22809328138828278, 0.9858739376068115, 0.5368372797966003, 0.9407046437263489, 0.5253679752349854, -0.12029360979795456, 0.5284976959228516, -0.37021514773368835, 0.1478072702884674, 0.3712363839149475, 0.35781317949295044, -0.04893305525183678, 0.21935854852199554, 0.33032137155532837, 0.9049939513206482, -0.23800808191299438, -1.0863158702850342, 0.29032447934150696, 0.5995317697525024, 0.14495910704135895, 0.34098538756370544, 0.8828965425491333, 0.3441210687160492, 0.2586229741573334, 0.33000391721725464, 1.0617915391921997, -0.37600287795066833, -0.2956896722316742, -0.01272554136812687, -0.515028178691864, 0.18430566787719727, 0.003268798813223839, -0.3760409355163574, -0.442997545003891, -0.5284513831138611, 0.2296828329563141, -0.08220642060041428, 0.22411327064037323, 1.4138925075531006, 0.3948083221912384, -0.05748135969042778, -0.56502765417099, -0.5787747502326965, -0.572926938533783, -0.7459526062011719, -0.16409149765968323, -0.40419867634773254, -0.4160602390766144, -0.1323172003030777, -0.22902221977710724, -0.5276598930358887]}, "authors": [{"authorId": "1379806208", "name": "Teven Le Scao"}, {"authorId": "2135734748", "name": "Thomas Wang"}, {"authorId": "80424302", "name": "Daniel Hesslow"}, {"authorId": "2113836860", "name": "Lucile Saulnier"}, {"authorId": "32136590", "name": "Stas Bekman"}, {"authorId": "2054179756", "name": "Saiful Bari"}, {"authorId": "103476203", "name": "Stella Biderman"}, {"authorId": "2218938", "name": "Hady ElSahar"}, {"authorId": "2037383772", "name": "Niklas Muennighoff"}, {"authorId": "80842917", "name": "Jason Phang"}, {"authorId": "40170001", "name": "Ofir Press"}, {"authorId": "2402716", "name": "Colin Raffel"}, {"authorId": "2285868436", "name": "Victor Sanh"}, {"authorId": "2191455", "name": "Sheng Shen"}, {"authorId": "35566806", "name": "Lintang Sutawika"}, {"authorId": "2112211652", "name": "Jaesung Tae"}, {"authorId": "1725420331", "name": "Zheng-Xin Yong"}, {"authorId": "143945447", "name": "Julien Launay"}, {"authorId": "46181066", "name": "Iz Beltagy"}], "references": [{"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "15190e8b459bd85d546286f7d7da61b4f4f3f58a", "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "a6f5b8f114b3eabbcd7f3f62091a481ca6f7f243", "title": "Predictability and Surprise in Large Generative Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79", "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "2d4f66046bb436864cd6bf589e3a931c405f9f44", "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"}, {"paperId": "a6d8d04962f84ae6225e72723869a002b9fc8036", "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "5aab57cc0530560d82c74c055f664280619d7e81", "title": "PROST: Physical Reasoning about Objects through Space and Time"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc", "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"}, {"paperId": "c014f8bc3b521453a93a13bb2c90700fcf462738", "title": "Limits to Depth Efficiencies of Self-Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ba4a34680e09e77984624c95f5245d91b54373f6", "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "81b4920ad488affaee27389ff9540b7fea90a4ce", "title": "\u201cGoing on a vacation\u201d takes longer than \u201cGoing for a walk\u201d: A Study of Temporal Commonsense Understanding"}, {"paperId": "0c3c4c88c7b07596221ac640c7b7102686e3eae3", "title": "PubMedQA: A Dataset for Biomedical Research Question Answering"}, {"paperId": "92343cecdc990380de362b969eec60081959f507", "title": "Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5", "title": "XNLI: Evaluating Cross-lingual Sentence Representations"}, {"paperId": "84a6d47676c2d2c1414d3893d09e47d33906fb1c", "title": "WiC: 10, 000 Example Pairs for Evaluating Context-Sensitive Representations"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "99ad0533f84c110da2d0713d5798e6e14080b159", "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "932a5de79d8a8ebb75ea0c43493450fd9922e738", "title": "Crowdsourcing Multiple Choice Science Questions"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "9aadff525783996ffb1a9a8240f2152f55bc63d8", "title": "Using the Output"}, {"paperId": "717f8e03b76ff3655f2aed407a30dd69dd80d2bf", "title": "Scale\u2020"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs"}, {"paperId": "0a87926fe36a329f9568a1542401e846b9027273", "title": "Warp size impact in GPUs: large or small?"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "56010a55d49ac1f42355538f494427fd22402be1", "title": "Exploring the Limits"}, {"paperId": null, "title": "The carbon footprint of machine"}, {"paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1", "title": "Few-shot Learning with Multilingual Language Models"}, {"paperId": null, "title": "On the opportuni"}, {"paperId": "3ede1108e6cbad247b875aa46b4541967a83b980", "title": "Limits to Depth Ef\ufb01ciencies of Self-Attention Supplementary Material"}, {"paperId": null, "title": "You: Gee stella, #eleutherai sure hypes rotary embeddings a lot. are you sure that they\u2019re that good? me"}, {"paperId": null, "title": "A framework for few-shot language model"}, {"paperId": null, "title": "On the sizes of openai api models"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation Technical report"}, {"paperId": null, "title": "Turing-nlg: A 17-billionparameter language model by microsoft"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": null, "title": "First quora dataset release: Question pairs"}, {"paperId": "f54777f2881557b79443022cc1d01e0b51da38cc", "title": "Going on Vacation"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": null, "title": "Thir-teenth International Conference on the Principles of Knowledge Representation and Reasoning"}, {"paperId": null, "title": "2022. Bloom (revision 4ab0472)"}]}