{"paperId": "f841f3d912be52a621aab1a979632e9daeab6599", "title": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation", "abstract": "As its core computation, a self-attention mechanism gauges pairwise correlations across the entire input sequence. Despite favorable performance, calculating pairwise correlations is prohibitively costly. While recent work has shown the benefits of runtime pruning of elements with low attention scores, the quadratic complexity of self-attention mechanisms and their on-chip memory capacity demands are overlooked. This work addresses these constraints by architecting an accelerator, called SPRINT1, which leverages the inherent parallelism of ReRAM crossbar arrays to compute attention scores in an approximate manner. Our design prunes the low attention scores using a lightweight analog thresholding circuitry within ReRAM, enabling SPRINT to fetch only a small subset of relevant data to on-chip memory. To mitigate potential negative repercussions for model accuracy, SPRINT re-computes the attention scores for the few fetched data in digital. The combined in-memory pruning and on-chip recompute of the relevant attention scores enables SPRINT to transform quadratic complexity to a merely linear one. In addition, we identify and leverage a dynamic spatial locality between the adjacent attention operations even after pruning, which eliminates costly yet redundant data fetches. We evaluate our proposed technique on a wide range of state-of-the-art transformer models. On average, SPRINT yields 7.5\u00d7 speedup and 19.6\u00d7 energy reduction when total l6KB on-chip memory is used, while virtually on par with iso-accuracy of the baseline models (on average 0.36% degradation).", "venue": "Micro", "year": 2022, "citationCount": 18, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2209.00606", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "The design prunes the low attention scores using a lightweight analog thresholding circuitry within ReRAM, enabling SPRINT to fetch only a small subset of relevant data to on-chip memory, and identifies and leverage a dynamic spatial locality between the adjacent attention operations even after pruning, which eliminates costly yet redundant data fetches."}, "embedding": {"model": "specter_v2", "vector": [0.5597195625305176, 0.21169301867485046, -0.5985089540481567, 0.18348625302314758, -0.4179844856262207, 0.21790508925914764, -0.014978554099798203, 0.08644942194223404, -0.11092197149991989, -0.42360296845436096, 0.5715316534042358, -0.08583211898803711, 0.3561706244945526, -0.4064830541610718, -0.11856213212013245, 0.13693024218082428, -0.7262323498725891, -0.17206980288028717, 0.8044536113739014, -0.19598239660263062, 0.6956155896186829, -0.615672767162323, -1.4008145332336426, 0.3975638151168823, 0.42597267031669617, 1.26775324344635, 0.07179879397153854, 0.6783187389373779, -0.4208087921142578, 0.5419801473617554, 0.4573182165622711, -0.23185019195079803, 0.1804981380701065, 0.042830366641283035, -0.13764749467372894, -0.4769906997680664, 0.5469375848770142, -0.21057641506195068, -0.46142297983169556, 0.6355432271957397, -0.1588001847267151, 0.07422467321157455, 0.21362119913101196, -0.3824172019958496, 0.04370585456490517, 0.6934266090393066, 0.20243850350379944, 1.0424778461456299, -0.5410029888153076, -0.5912374258041382, 1.2983818054199219, -1.1985691785812378, -0.29683423042297363, 0.7052353620529175, 0.5475271940231323, -0.10550972074270248, -0.22064997255802155, -0.36927998065948486, 0.3484598696231842, 0.1989336609840393, -0.4287269711494446, -0.6383177042007446, 0.27256420254707336, 0.1317252367734909, 1.9024462699890137, 0.134231835603714, 0.23025107383728027, 0.225651815533638, 0.48710617423057556, 1.2284610271453857, 0.0040954360738396645, -1.0225615501403809, 0.2574552893638611, -0.4239974319934845, 1.1163636445999146, 0.6989221572875977, 0.05511043593287468, 0.17439590394496918, -1.194280743598938, -0.14731599390506744, 0.6504834294319153, 0.47604456543922424, 0.7602090239524841, -0.4252876341342926, 0.06711656600236893, 0.31379881501197815, 0.36972692608833313, 0.7887790203094482, -0.3027297854423523, 0.93000328540802, 0.6551828384399414, 0.1695159673690796, -0.4093403220176697, 0.20680508017539978, 0.5487700700759888, 0.16073910892009735, -1.3055931329727173, -0.4589017629623413, -0.19112959504127502, 1.0460550785064697, -0.23988670110702515, 0.8773300647735596, -0.5759427547454834, 0.09477448463439941, 0.7178253531455994, -0.01003342680633068, 0.4542921781539917, -0.3020531237125397, -0.2107287496328354, -0.34270766377449036, 0.00923178531229496, -0.45953747630119324, 0.14989235997200012, -0.6407938599586487, -1.197595477104187, -0.7568681240081787, -0.8545912504196167, 0.7885790467262268, -0.9242900013923645, 0.04684038460254669, -0.728500247001648, 0.1956852227449417, -0.4959212839603424, -0.11830651015043259, 0.5901069641113281, 0.318912535905838, 0.26781269907951355, -0.015942292287945747, 1.0648969411849976, -1.4221385717391968, -0.3786188066005707, -1.0188919305801392, -0.04675409942865372, -0.4067109227180481, -0.3405563235282898, -0.15229298174381256, -1.5905684232711792, -1.3596265316009521, -1.029434084892273, 0.08366004377603531, -0.2655683755874634, -0.12204302102327347, 0.9354506134986877, 0.21271930634975433, -1.3305859565734863, 0.5858532190322876, -0.8750796318054199, -0.005756096914410591, 0.4367959797382355, 0.3578363358974457, 0.922477662563324, 0.021301424130797386, -0.6773220896720886, -0.07890749722719193, -0.339760959148407, -0.8840503096580505, -0.25986093282699585, -0.9269249439239502, -0.7586640119552612, 0.42462798953056335, 0.007695589680224657, -0.3102104365825653, 1.1136149168014526, -0.2040952891111374, -0.7074571251869202, 0.5374216437339783, -0.462950199842453, -0.16734392940998077, -0.3566468358039856, 0.07751961797475815, -0.6789587736129761, -0.22439132630825043, -0.3055208921432495, 0.39600613713264465, 0.6751830577850342, -0.1268841177225113, -0.08090485632419586, 0.1044117733836174, -0.5534113645553589, -0.41232267022132874, -0.5695688724517822, 0.9979406595230103, -0.4736142158508301, -0.45337486267089844, 0.5695235133171082, 0.8053930401802063, -0.37412822246551514, -0.33486905694007874, -0.4157589077949524, -0.38913804292678833, 0.6370226144790649, 0.2930198609828949, 1.6854301691055298, -0.8062805533409119, -1.178591012954712, 0.046363525092601776, -0.11333541572093964, 0.07800431549549103, -0.21256054937839508, 0.3138968050479889, -0.6645121574401855, -0.030515626072883606, 0.16778983175754547, -0.18269900977611542, -0.5333919525146484, -1.0274596214294434, -1.0116803646087646, -0.662239670753479, -0.009356302209198475, 1.0915699005126953, -0.5905725359916687, 0.14658471941947937, -0.2506107985973358, 0.1713908463716507, -0.9847163558006287, 1.024563193321228, -0.009730556048452854, -0.30927935242652893, -0.11324409395456314, 0.22612078487873077, 0.06568802893161774, -0.7289001941680908, 0.7234138250350952, -1.1312865018844604, -0.42395827174186707, 0.6029291152954102, 0.43242883682250977, 1.0074740648269653, -0.1828429400920868, 0.33216264843940735, -0.19226209819316864, -0.6104509234428406, 0.5759516954421997, -0.10634259879589081, 0.26713037490844727, -1.0635708570480347, 0.7240502834320068, 0.12489189952611923, -0.25934842228889465, 0.6696693897247314, 1.2547258138656616, 1.3682359457015991, -0.5268284678459167, 0.20514903962612152, 0.41783711314201355, 0.028928270563483238, 0.25401851534843445, 0.22083444893360138, 1.2390917539596558, 0.17552010715007782, 0.6640273928642273, -0.6543302536010742, 0.43590229749679565, -0.657988965511322, -0.09830886870622635, 0.43784868717193604, 0.3807007074356079, 0.5170561075210571, 0.19850045442581177, -0.593634307384491, -0.6925954222679138, 0.1803957223892212, 0.5635399222373962, 1.4266650676727295, 0.006035072263330221, 0.012912946753203869, -0.6325955986976624, -0.16172336041927338, -0.5844253897666931, 0.1243867576122284, 0.06780008971691132, -0.5117045640945435, -0.6756580471992493, -0.8843822479248047, 0.48306339979171753, 0.6871778964996338, 1.0898326635360718, -0.43040701746940613, -1.1443785429000854, -0.1390274316072464, 0.4517478942871094, -0.4425424039363861, -0.6249427795410156, 1.1116446256637573, -0.6957904696464539, 0.267315149307251, 0.38659483194351196, -0.5144789814949036, 0.2639789581298828, -0.16379718482494354, 1.379590630531311, -0.37984493374824524, -0.7335712909698486, 0.08835619688034058, 0.6634894013404846, -0.19165438413619995, -0.5434491634368896, 0.2525166869163513, -0.45530182123184204, -0.5430921912193298, 0.9198669791221619, -0.37243637442588806, -0.33537599444389343, 0.23478913307189941, -0.10617886483669281, -0.1346166431903839, 0.30250683426856995, 0.22787034511566162, 0.8637714982032776, -0.5617195963859558, 0.01084809098392725, -0.73338383436203, 0.2290448397397995, 0.2221239060163498, -0.19193024933338165, -0.16310124099254608, -0.49521905183792114, -0.41628241539001465, 0.6020107269287109, -0.5791856050491333, -0.09141701459884644, -1.1075271368026733, 0.24787360429763794, -0.37197956442832947, 0.20368513464927673, -0.5597357153892517, 0.20776234567165375, -0.4109387695789337, 0.2267719954252243, 0.7115961313247681, 0.02198358066380024, 0.45013168454170227, -0.137480229139328, -0.9132778644561768, 0.5991334915161133, 0.1299028992652893, -0.30585914850234985, 0.06587594002485275, -0.0028015444986522198, -0.5712375044822693, -0.14203131198883057, -0.11468438059091568, -0.18761421740055084, -0.5127272605895996, 0.0797201618552208, -0.5170450806617737, -0.8154835104942322, -0.23544147610664368, -0.850666344165802, -0.29200172424316406, 0.034139372408390045, -0.6341874599456787, -0.3469168245792389, -1.0860395431518555, -1.2452480792999268, -0.5916926264762878, -1.3888235092163086, -1.1422119140625, 0.21149101853370667, 0.20604351162910461, -0.9094645380973816, 0.057732902467250824, -0.4836514890193939, -0.734583854675293, 1.4280184507369995, -0.49852609634399414, 0.6305351853370667, -0.28148123621940613, -0.9377936720848083, 0.10175881534814835, 0.14531369507312775, -0.03529607132077217, -0.4215000867843628, 0.20500676333904266, -0.5937981009483337, 0.452502578496933, 0.0003451818192843348, -0.08247502148151398, 0.44474902749061584, 0.26051250100135803, 1.132904291152954, 0.11756214499473572, -0.9310927987098694, -0.02806301787495613, 1.5442014932632446, 0.15186379849910736, 0.7060912251472473, -0.05095566436648369, 0.7238968014717102, -0.36352741718292236, 0.09708218276500702, 0.6008327007293701, -0.21172906458377838, 0.5962532758712769, 0.2412702590227127, -0.22213780879974365, -0.2717602252960205, -0.04311555624008179, 0.5265548229217529, 1.777492880821228, 0.6318442225456238, 0.16044914722442627, -0.601550817489624, 0.7051026821136475, -0.8964053988456726, -0.7622926831245422, 0.5484694838523865, 1.1109814643859863, 0.41883501410484314, 0.04320536181330681, -0.32687583565711975, 0.17773965001106262, 0.5890732407569885, 0.3560360074043274, -0.6874873042106628, -1.0715968608856201, 0.132525235414505, 1.1098583936691284, 0.744733452796936, 0.43061599135398865, 0.1466834843158722, 0.5217748880386353, 14.926368713378906, 0.9504631161689758, -0.04577657952904701, 0.6882314682006836, 0.974929928779602, 0.22092650830745697, -0.4251869320869446, 0.1602502316236496, -1.3328368663787842, 0.2334527224302292, 1.5925382375717163, 0.18694204092025757, 0.030139757320284843, 0.33051568269729614, -0.32247912883758545, -0.07290179282426834, -0.5722566843032837, 0.35908132791519165, 0.7509773373603821, -1.3943537473678589, 0.2730579972267151, 0.158638134598732, 0.2582896947860718, 0.08712739497423172, 0.8669310212135315, 0.5043085217475891, 0.7468485832214355, -0.03215189650654793, 0.21934574842453003, 0.3772388696670532, 1.1940385103225708, -0.3881606161594391, 0.010702662169933319, -0.23403313755989075, -0.9568750858306885, -0.08258011192083359, -0.3681257963180542, -0.9326751828193665, 0.09604194760322571, 0.47817471623420715, -0.2563343942165375, -0.7069288492202759, -0.031817398965358734, 0.28146374225616455, 0.34555861353874207, 0.5326254367828369, 0.002395505318418145, 0.44863423705101013, 0.038721658289432526, -0.15353918075561523, -0.2192334085702896, 0.5254122018814087, -0.058336250483989716, 0.05618032440543175, 0.13480255007743835, 0.07880423218011856, -0.08109017461538315, 0.46254318952560425, -0.160195991396904, -0.6083042025566101, -0.1365266591310501, -0.05783434584736824, 0.04137058183550835, 0.9576612710952759, 0.40921851992607117, 0.2098105400800705, -0.43694570660591125, 0.25604379177093506, 0.29799649119377136, -0.29214856028556824, -0.7998207807540894, -0.47551292181015015, 0.2404809296131134, -0.2633776366710663, 0.1601947546005249, 0.6315653324127197, -0.8726943731307983, -0.2092006653547287, -1.0597963333129883, -0.6210943460464478, 0.11516577750444412, -0.8970006704330444, -0.32770854234695435, 0.8716107606887817, -0.5663134455680847, -0.30675387382507324, 0.49992793798446655, -0.7199409604072571, -0.5284735560417175, -0.011980458162724972, -0.8855172991752625, -0.21644963324069977, -0.24354968965053558, -0.6394896507263184, -0.31391680240631104, 0.1750900000333786, 0.8965893983840942, 0.20146502554416656, -0.25017181038856506, 0.5456562638282776, -0.20041124522686005, -0.4229172468185425, -0.022526051849126816, -0.24690239131450653, 0.9899021983146667, 0.3636707067489624, -0.5896451473236084, 0.24452047049999237, 0.16220137476921082, -0.07974136620759964, -1.3100714683532715, -0.11283242702484131, 0.40631574392318726, -0.42927637696266174, 0.07416129112243652, -0.7519806027412415, -1.0647469758987427, 0.3780941069126129, 0.6890860795974731, -0.2203979641199112, 0.3541359305381775, -0.23589536547660828, -0.42190203070640564, -0.7046802639961243, -0.6025476455688477, 0.38112905621528625, 0.8452776074409485, -0.7719637751579285, 0.16044388711452484, -0.7074626088142395, 0.2824274003505707, -0.9226601123809814, -0.7046232223510742, 0.1718599945306778, 0.02749348245561123, -0.7010244131088257, 1.129548192024231, 0.18252307176589966, 1.0766828060150146, 0.9743812680244446, -0.0990038514137268, -0.17357388138771057, -0.18170282244682312, -0.7867562174797058, -0.33991310000419617, 0.39477357268333435, 0.328279584646225, -0.2902066707611084, 0.8340519070625305, 0.7096850872039795, 0.11140425503253937, -0.6119863986968994, -0.5437261462211609, 0.09579123556613922, -0.3978419005870819, -0.2843421697616577, 0.11620020121335983, -0.20553041994571686, 0.47087904810905457, 0.19039316475391388, 0.5335012078285217, 0.1869564801454544, -0.06086692586541176, -0.25612473487854004, 0.18097449839115143, -0.09067823737859726, -0.13448262214660645, -0.6437357068061829, -0.8777583241462708, -1.40884268283844, -0.34859833121299744, -0.8482027649879456, 0.11452747136354446, -0.28903642296791077, -0.058879707008600235, -0.21455231308937073, -0.5464833974838257, 0.06585390120744705, 0.32247525453567505, -0.13571342825889587, -0.8976418972015381, -0.5481268763542175, -0.7736727595329285, 0.6531250476837158, 0.3233640193939209, -0.6455166339874268, 0.27913427352905273, -0.470876008272171, -0.31267186999320984, 0.16724345088005066, 0.48113054037094116, -0.18366865813732147, -0.2967764437198639, -1.0557324886322021, 0.0017342781648039818, 0.017475247383117676, -0.17542259395122528, -1.3097566366195679, 1.3380993604660034, 0.595007598400116, -0.09052053093910217, -0.5270193219184875, 0.09401301294565201, -1.0777225494384766, -0.19750434160232544, 0.6873020529747009, -0.5418006777763367, 0.6549428701400757, 0.8945490717887878, -0.6314574480056763, -0.07456033676862717, 0.722612738609314, 0.13157516717910767, -0.6287947297096252, -0.9897425770759583, 0.11741561442613602, -0.70293128490448, 0.09352082014083862, -0.06779321283102036, -0.23729349672794342, -1.3049393892288208, -0.1657838076353073, 0.06929274648427963, 0.02146991528570652, -0.24977152049541473, 0.6950374841690063, 0.3638882637023926, -1.1321135759353638, 0.38084787130355835, 0.605966329574585, -0.3914249837398529, -0.23834626376628876, 0.6475908756256104, 0.5488998889923096, -0.12088445574045181, 0.5731920599937439, -0.2299504578113556, 0.19637355208396912, -0.9860692024230957, 0.2635215222835541, 0.4097529351711273, -0.713688850402832, 0.3123052716255188, 0.7070995569229126, -0.3147121071815491, -0.08635514974594116, -0.011929117143154144, -0.9698631167411804, -0.2914547324180603, -0.3445216417312622, 0.5398572087287903, 0.4619521498680115, 0.2231699526309967, 0.11594465374946594, -0.6359479427337646, 0.1082531064748764, -0.49507057666778564, -0.1401996612548828, 0.24001862108707428, 0.13524474203586578, -0.5413731932640076, 0.22305887937545776, 0.49407631158828735, -0.33543744683265686, -0.3824661076068878, -0.6424466967582703, -0.26744526624679565, 0.19010524451732635, 0.49565795063972473, -0.14996258914470673, -0.9122765064239502, 0.7951176762580872, 0.603850245475769, 0.3140488266944885, 0.508993923664093, -0.4130097031593323, 0.2550221383571625, 0.07823643088340759, 0.5512691736221313, -0.24277502298355103, -0.1980229765176773, 1.3604060411453247, 0.7648525238037109, -0.2705802321434021, 0.4146071672439575, -0.6321284174919128, -0.2503376603126526, 0.9164848327636719, 0.91061931848526, -0.19358207285404205, 0.7168208360671997, 0.9254195690155029, -0.3820141553878784, 0.3385034501552582, -0.9861643314361572, -0.10959326475858688, 0.3181316554546356, 0.9882416129112244, 0.8970833420753479, -0.061211422085762024, 0.07823692262172699, 0.7690650224685669, 0.4354911744594574, 0.4376363158226013, 0.4285334050655365, 0.6073084473609924, -0.5808843374252319, -0.1447061449289322, -0.0679769441485405, 0.8777294754981995, -0.6398483514785767, -1.1698431968688965, 0.43961474299430847, 0.5274090766906738, -0.28301745653152466, 0.48682719469070435, 1.4697694778442383, -0.4174547493457794, 0.41581374406814575, -0.07030156254768372, 0.38337790966033936, -0.17163093388080597, -0.5410397052764893, -0.10843385756015778, -0.8037183880805969, -0.13872377574443817, 0.1983029544353485, -0.23387877643108368, -0.2827284634113312, -0.14172989130020142, 0.16796980798244476, -0.11821798235177994, 0.14468279480934143, 0.16862857341766357, 1.148850440979004, 1.460552453994751, -0.25302553176879883, -0.5909904837608337, -0.27776095271110535, -0.49966153502464294, 0.6532313823699951, -1.0331016778945923, -0.3180747628211975, 0.09025321155786514, 0.02266102284193039, -0.5338233113288879]}, "authors": [{"authorId": "2112229", "name": "A. Yazdanbakhsh"}, {"authorId": "2183596463", "name": "Ashkan Moradifirouzabadi"}, {"authorId": "2146249050", "name": "Zheng Li"}, {"authorId": "34875156", "name": "Mingu Kang"}], "references": [{"paperId": "87bc51ec656a97d6cfbb60f48667f53ccc1aed9d", "title": "Parallel Computing of Graph-based Functions in ReRAM"}, {"paperId": "f47ca12a86cf0910816b4fbed1bcc0a1894ecac7", "title": "PIMGCN: A ReRAM-Based PIM Design for Graph Convolutional Network Acceleration"}, {"paperId": "0540b786daa34789cd97c9cc3d3a42e1ae508c42", "title": "Rerec: In-ReRAM Acceleration with Access-Aware Mapping for Personalized Recommendation"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "377d44f890140e0dcd36fa32290b3814d666f2b6", "title": "Multi-Level Control of Resistive RAM (RRAM) Using a Write Termination to Achieve 4 Bits/Cell in High Resistance State"}, {"paperId": "12ca0dd2ce2c0c0a282bf583200d99a046999bb5", "title": "Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory"}, {"paperId": "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2", "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks"}, {"paperId": "c156b1b30e3dd9284615e5304f2fb2826c09d0ff", "title": "Learned Token Pruning for Transformers"}, {"paperId": "efbe9f591090018f78b42c84613c8afda9292fdb", "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "27cc581928fdd504d60c041671df5c185f966491", "title": "FORMS: Fine-grained Polarized ReRAM-based In-situ Computation for Mixed-signal DNN Accelerator"}, {"paperId": "461d182e4cd4ae775f8ae22750eede197be31f2c", "title": "Hardware Architecture and Software Stack for PIM Based on Commercial DRAM Technology : Industrial Product"}, {"paperId": "6da4b231148bf26677233d1f778d08a5d26f4313", "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference"}, {"paperId": "b080ba53a471348e7e76234decdf14e730fea7db", "title": "Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers"}, {"paperId": "765ae4aee20af83b39c35cf81626bb85948552f1", "title": "An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks"}, {"paperId": "06cf46de57bf82559ba67ad4f6e8e1096f7fdef3", "title": "GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent"}, {"paperId": "9f9f8113afd4c1e449293a46d99018df6c34e0e0", "title": "FAFNIR: Accelerating Sparse Gathering by Using Efficient Near-Memory Intelligent Reduction"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "3af8a493cf756f9fe72623204a11e378a9cd71a5", "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3e22b10630a09d217be7012ad3672498dec3b3b3", "title": "Planaria: Dynamic Architecture Fission for Spatial Multi-Tenant Acceleration of Deep Neural Networks"}, {"paperId": "01ab2e17a4955a5d17f447a085a4aa412ef58f63", "title": "Newton: A DRAM-maker\u2019s Accelerator-in-Memory (AiM) Architecture for Machine Learning"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "776830e99ac509ef2c5955ddc3caba21291a8307", "title": "AccuReD: High Accuracy Training of CNNs on ReRAM/GPU Heterogeneous 3-D Architecture"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "07a79ff9168da5b54636f3887469daba9c60c3ad", "title": "MViD: Sparse Matrix-Vector Multiplication in Mobile DRAM for Accelerating Recurrent Neural Networks"}, {"paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"}, {"paperId": "239e9b4ed0e2aed3e6379ab48a2ba3db1b266815", "title": "Timely: Pushing Data Movements And Interfaces In Pim Accelerators Towards Local And In Time Domain"}, {"paperId": "81c53e8237a87499a2342164f9251097ce08241b", "title": "Bit-Parallel Vector Composability for Neural Acceleration"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "661a9b0b62c7327ebea00384402b66eefda2653c", "title": "OPTIMUS: OPTImized matrix MUltiplication Structure for Transformer neural network accelerator"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "fcf1b4473a0af1f3ebc0fd556ee30c9309ff6345", "title": "SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "b03cf6324ecf7a295a4aeae5970c88d1a1c3f336", "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "3a3d04f71c6fcf16834d255d1dfd4b7f3fd3a72b", "title": "FastWave: Accelerating Autoregressive Convolutional Neural Networks on FPGA"}, {"paperId": "cd37372c4a4366f0943b396bf825d3f482f05ff8", "title": "ReDRAM: A Reconfigurable Processing-in-DRAM Platform for Accelerating Bulk Bit-Wise Operations"}, {"paperId": "a3ef6ee560e93e6f58be2b28f27aed0eb86dc463", "title": "Fine-tune BERT with Sparse Self-Attention Mechanism"}, {"paperId": "8efd76fafff43a1b420f8cda092a8bab8d545732", "title": "Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture"}, {"paperId": "6408b5935d3c6a1a405c944c70def8b2e0e2ac0f", "title": "GraphQ: Scalable PIM-Based Graph Processing"}, {"paperId": "997d473e7aee42ffa04f7ca4a02fa0843158d296", "title": "ComputeDRAM: In-Memory Compute Using Off-the-Shelf DRAMs"}, {"paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "title": "Structured Pruning of Large Language Models"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "97cbcb427c0a530a8854b7038508684e01e4c515", "title": "Thanks for Nothing: Predicting Zero-Valued Activations with Lightweight Convolutional Neural Networks"}, {"paperId": "989fa897fdb002a9a7cc4175f3b011ef5f9480cb", "title": "TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "93a3b041a6120cf2d599b512b4c5dbb3053143b9", "title": "A fully integrated reprogrammable memristor\u2013CMOS system for efficient multiply\u2013accumulate operations"}, {"paperId": "3ecf77ac7b3c11a3f841b6df938b1e32755b3c20", "title": "Mixed-Signal Charge-Domain Acceleration of Deep Neural Networks through Interleaved Bit-Partitioned Arithmetic"}, {"paperId": "a971cec5d7369433e35399f145f80ff106f379db", "title": "BitBlade: Area and Energy-Efficient Precision-Scalable Neural Network Accelerator with Bitwise Summation"}, {"paperId": "a31d8d755192cd1cea18ebe87dc6ae277c93ff4c", "title": "Noise Injection Adaption: End-to-End ReRAM Crossbar Non-ideal Effect Adaption for Neural Network Mapping"}, {"paperId": "5a2b2f48b31f5ecf809d2fd15d0a18511fca902d", "title": "FloatPIM: In-Memory Acceleration of Deep Neural Network Training with High Precision"}, {"paperId": "d62a401a01d81d23bd9d594317d15aea3e0dfa6c", "title": "Laconic Deep Learning Inference Acceleration"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "b1051e81e527d841f0936c604aa6966c719e876d", "title": "TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators"}, {"paperId": "9b71a03af3845f803e0f78587ddfab270c204c35", "title": "ReRAM-Based In-Memory Computing for Search Engine and Neural Network Applications"}, {"paperId": "9f9c4dd9a761a708cfcec6951ff67ce8953978c0", "title": "Bit-Tactical: A Software/Hardware Approach to Exploiting Value and Bit Sparsity in Neural Networks"}, {"paperId": "1e5bdca5fdd1464da54b9cdbaa59d53884c02a3b", "title": "Resistive RAM Endurance: Array-Level Characterization and Correction Techniques Targeting Deep Learning Applications"}, {"paperId": "bd8d9e1b3a192fcd045c7a3389920ac98097e774", "title": "ParaPIM: A Parallel Processing-in-Memory Accelerator for Binary-Weight Deep Neural Networks"}, {"paperId": "40e8b193fc5d8056a05735d760c30caefdbd8064", "title": "3DICT: A Reliable and QoS Capable Mobile Process-In-Memory Architecture for Lookup-based CNNs in 3D XPoint ReRAMs"}, {"paperId": "d85ed474f0b0bb825f84f6596d990c66bff13e32", "title": "DIMA: A Depthwise CNN In-Memory Accelerator"}, {"paperId": "5bcd22c2207785e56b65c4e2bf27f8514d6f0fba", "title": "In-DRAM near-data approximate acceleration for GPUs"}, {"paperId": "e1ad134082937f450d1e14ced1465f3147243a61", "title": "McDRAM: Low Latency and Energy-Efficient Matrix Computations in DRAM"}, {"paperId": "3bc7d98be0dfedeedfd449ed5d95bc7a2600d244", "title": "Processing-in-Memory for Energy-Efficient Neural Network Training: A Heterogeneous Approach"}, {"paperId": "f15e04d2966805cde90913f1ec71b6051198223b", "title": "LerGAN: A Zero-Free, Low Data Movement and PIM-Based GAN Architecture"}, {"paperId": "d67eab96b9dadd8f1ad46174aaef378c89f27d56", "title": "FlexiGAN: An End-to-End Solution for FPGA Acceleration of Generative Adversarial Networks"}, {"paperId": "0682bfa5cca15726aab6c00ecfac91eb44379626", "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices"}, {"paperId": "704bf21bb25e5d460fa5a1101568a764b1bd9ed5", "title": "Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays"}, {"paperId": "79c113913fcbbcab659314d766e21ac4e682fa2e", "title": "ComPEND: Computation Pruning through Early Negative Detection for ReLU in a Deep Neural Network Accelerator"}, {"paperId": "f2b70dd0312393c53d840796df004d3c3c940b49", "title": "SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks"}, {"paperId": "03056f2c785b9073d1ef4487f24c24938d9cbf13", "title": "XNOR-SRAM: In-Memory Computing SRAM Macro for Binary/Ternary Deep Neural Networks"}, {"paperId": "5a5b5fde708d47bb276eeb91f8482e6f522bec21", "title": "DrAcc: a DRAM based Accelerator for Accurate CNN Inference"}, {"paperId": "1324c98f35256b3fe64fae9f777e2f7d76f194c0", "title": "A Configurable Cloud-Scale DNN Processor for Real-Time AI"}, {"paperId": "d0758b9368845a4618dcb4463705b5b5dd085e34", "title": "Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks"}, {"paperId": "1f0bbcbcea15b60b39012e9aedf4dac42dff9411", "title": "Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach"}, {"paperId": "bb56bc6676fb88d67a071fd482a349286e20b16d", "title": "A Survey of ReRAM-Based Architectures for Processing-In-Memory and Neural Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "5f0da3cedda449b72fe36fa78798651a038f515c", "title": "MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects"}, {"paperId": "767ad42da2e1dcd79dba2e5822eed65ac2fe1dea", "title": "GraphP: Reducing Communication for PIM-Based Graph Processing with Efficient Data Partition"}, {"paperId": "fea6a2c5b5c0c8193b1d98254830ab9f45f45df2", "title": "UNPU: A 50.6TOPS/W unified deep neural network accelerator with 1b-to-16b fully-variable weight bit-precision"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "386335e5853b6fe8b552c42bd64b0c75d1287fb4", "title": "A Multi-Functional In-Memory Inference Processor Using a Standard 6T SRAM Array"}, {"paperId": "69e220145b5a7886f9c92da8a253b6cc97181f57", "title": "Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Network"}, {"paperId": "4d61c115fc70ee8abfec80214c11c8a54e72e98a", "title": "DRISA: A DRAM-based Reconfigurable In-Situ Accelerator"}, {"paperId": "35334a700a26ca41464ede7f27763a51c21e1b46", "title": "Ambit: In-Memory Accelerator for Bulk Bitwise Operations Using Commodity DRAM Technology"}, {"paperId": "cb14f2138d6de2f459841430aa52a2b6f2d3fc90", "title": "Exploring the impact of memory block permutation on performance of a crossbar ReRAM main memory"}, {"paperId": "ca1060c50642f9f05735d3007873439347b3bea5", "title": "Learning Intrinsic Sparse Structures within Long Short-term Memory"}, {"paperId": "0fa6868bcb7d1bd38c8975f4b1aac765b4c6d81d", "title": "DeepFense: Online Accelerated Defense Against Adversarial Deep Learning"}, {"paperId": "cc95830a57010a960fda72531e1f811cafc14b77", "title": "GraphR: Accelerating Graph Processing Using ReRAM"}, {"paperId": "04259d8b1a54a2627b984da4b9afe67469a777fb", "title": "XNOR-POP: A processing-in-memory architecture for binary Convolutional Neural Networks in Wide-IO2 DRAMs"}, {"paperId": "2154d8fc2caa334d5fa405a8d81fe6894fe9759e", "title": "Loom: Exploiting Weight and Activation Precisions to Accelerate Convolutional Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "402f850dff86fb601d34b2841e6083ac0f928edd", "title": "SCNN: An accelerator for compressed-sparse convolutional neural networks"}, {"paperId": "d8c44edc0bf50df29c6f1335157ff1bcdfdc3e31", "title": "Face classification using electronic synapses"}, {"paperId": "bd627f5d04666c149efd6a9ff7bfee4109ea6497", "title": "PredictiveNet: An energy-efficient convolutional neural network via zero prediction"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "733a765e1f54eb86dbaabe03d7ba66d72363f665", "title": "TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory"}, {"paperId": "4d579e1628ff4c3bf24e1bdde5f98b8d6d569c64", "title": "ReVAMP: ReRAM based VLIW architecture for in-memory computing"}, {"paperId": "dea9245816968a955918a55b7088a4224c3c6646", "title": "Compute Caches"}, {"paperId": "6b3c06f148deba3926eff3c22ddf4dfd1195ac8a", "title": "PipeLayer: A Pipelined ReRAM-Based Accelerator for Deep Learning"}, {"paperId": "d2dfdbb7a5d64176968f51342a2eff0ff45db4b5", "title": "Bit-Pragmatic Deep Neural Network Computing"}, {"paperId": "bc20f523a6e97800340e57a94d79926fce05572c", "title": "Cambricon-X: An accelerator for sparse neural networks"}, {"paperId": "924d6c44fda59dc9ac1f25d7cc12d669c5f9e557", "title": "From high-level deep neural models to FPGAs"}, {"paperId": "91d03e4bf98a03c827983457e6de43cbd4c6ccd7", "title": "Minerva: Enabling Low-Power, Highly-Accurate Deep Neural Network Accelerators"}, {"paperId": "da6e8bcf1c92167553a5b383fe6fe5109a4c0321", "title": "Cambricon: An Instruction Set Architecture for Neural Networks"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "571acc0feaffe3342845ff80e554ea7d1ad741c5", "title": "Pinatubo: A processing-in-memory architecture for bulk bitwise operations in emerging non-volatile memories"}, {"paperId": "2fdaa46ed54c593f8558dd90857cc5cadbabfa30", "title": "Dot-product engine for neuromorphic computing: Programming 1T1M crossbar to accelerate matrix-vector multiplication"}, {"paperId": "9071775ebcfebddd54d879fe7e6c627673e4d305", "title": "ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars"}, {"paperId": "5ec594e9f5ca4b629be28625cd78c882514ea3be", "title": "Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks"}, {"paperId": "91ab42a71ff42ae07c7f6a8220727af4a6c56369", "title": "Buffered compares: Excavating the hidden parallelism inside DRAM architectures with lightweight logic"}, {"paperId": "2e2b189f668cf2c06ebc44dc9b166648256cf457", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network"}, {"paperId": "24303ead9d87b2a6d3d19106cd13e070d28f334c", "title": "Neural acceleration for GPU throughput processors"}, {"paperId": "5dfbdcedb7bcb8644b816bab2cc3d3fadd36775b", "title": "Gather-Scatter DRAM: In-DRAM address translation to improve the spatial locality of non-unit strided accesses"}, {"paperId": "04400a01703ff92e8730c7164bf9a6fa2dabd37d", "title": "DReAM: Dynamic Re-arrangement of Address Mapping to Improve the Performance of DRAMs"}, {"paperId": "4bd669b70ec1d1fa2d7825bdf7a17ae7c80f4480", "title": "A scalable processing-in-memory accelerator for parallel graph processing"}, {"paperId": "ba8dcc218ed86364208623c2d280089a7acefa4a", "title": "PIM-enabled instructions: A low-overhead, locality-aware processing-in-memory architecture"}, {"paperId": "f72fe87b47bf219689e3975727241bdf919675d3", "title": "NDA: Near-DRAM acceleration architecture leveraging commodity DRAM devices and standard memory modules"}, {"paperId": "4157ed3db4c656854e69931cb6089b64b08784b9", "title": "DaDianNao: A Machine-Learning Supercomputer"}, {"paperId": "a87688b13333ebd95ac56d0a8a83a054baaa3aa2", "title": "General-purpose code acceleration with limited-precision analog computation"}, {"paperId": "ecec038b2b5db43e294dbbedf90bf7ecbd37d296", "title": "SpongeDirectory: Flexible sparse directories utilizing multi-level memristors"}, {"paperId": "d78532d5add9d9751da025f05603c6fca04c8e07", "title": "TOP-PIM: throughput-oriented programmable processing in memory"}, {"paperId": "fdba4f015570634db4dda6162f6737c39c53f12a", "title": "An energy-efficient VLSI architecture for pattern recognition via deep embedding of computation in SRAM"}, {"paperId": "0c0fef1d6f9fa729f6f85ab14b4a9ed4191bb274", "title": "Memristor Crossbar Architecture for Synchronous Neural Networks"}, {"paperId": "670ad55de17548bb61eb1523184c6a40b357a1a3", "title": "NDC: Analyzing the impact of 3D-stacked memory+logic devices on MapReduce workloads"}, {"paperId": "8ba5074d7e199f496b10d86f04f71dc42345490e", "title": "Simulating DRAM controllers for future system architecture exploration"}, {"paperId": "22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd", "title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning"}, {"paperId": "8f8f2b1beaa33be4f7ed14a5884c2333bcccf640", "title": "RowClone: Fast and energy-efficient in-DRAM bulk data copy and initialization"}, {"paperId": "cbacfa4735cbd57fcd9fc996565c62dcd1fa7c85", "title": "AC-DIMM: associative computing with STT-MRAM"}, {"paperId": "8fb5617b45a239f8e4a38ebeb314fd8edb0ed67e", "title": "Design trade-offs for high density cross-point resistive memory"}, {"paperId": "7cd29ed1da71593bfb79b553ba6c5ee39ccf7a7b", "title": "NVSim: A Circuit-Level Performance, Energy, and Area Model for Emerging Nonvolatile Memory"}, {"paperId": "8ee200faea2666d064b927c7b6cafb560a1e4979", "title": "A study on low-power, nanosecond operation and multilevel bipolar resistance switching in Ti/ZrO2/Pt nonvolatile memory with 1T1R architecture"}, {"paperId": "0d89743bbb517a2534b3a4a7a8e9e4f04610c7fa", "title": "PARDIS: A programmable memory controller for the DDRx interfacing standards"}, {"paperId": "3f6100caf10fcaa9825ae2e94b76ac8353c48988", "title": "Investigating the switching dynamics and multilevel capability of bipolar metal oxide resistive switching memory"}, {"paperId": "a0033521d869262a99dfbc975c296c30a7013a05", "title": "Circuit design challenges in embedded memory and resistive RAM (RRAM) for mobile SoC and 3D-IC"}, {"paperId": "697f8d67e3c54640935f99f9fb944d24a14caf65", "title": "Predator: A predictable SDRAM memory controller"}, {"paperId": "69add94258663a0b6664c2c353cf7f7db30b0bb5", "title": "Flash ADC architecture"}, {"paperId": "28ce7bf8e2e62151038f20237722a9613d95f0ad", "title": "Smart Memories: a modular reconfigurable architecture"}, {"paperId": "2af55a02481efb321ce3140a6f7ede471c07c7b1", "title": "FlexRAM: Toward an advanced Intelligent Memory system"}, {"paperId": "47348cba3b5d859cb62321a5ed7dfcf3d0d6f8d3", "title": "Impulse: building a smarter memory controller"}, {"paperId": "e23080194a78d970bd3cafc48611826d3045484a", "title": "A case for intelligent RAM"}, {"paperId": "3c3d74a83bbe6b69f31c912b6725e35145d2690e", "title": "KNOWLEDGE, ATTITUDE AND PRACTICE (KAP) OF PRIMARY HEALTH CARE PHYSICIANS AND NURSES TOWARDS HYPERTENSION: A STUDY FROM DAMMAM, SAUDI ARABIA"}, {"paperId": "f2fc9780327b195fce23511cae31e0449db104ec", "title": "Missing the Memory Wall: The Case for Processor/Memory Integration"}, {"paperId": "ce65f83ca6cbe47f96194ad0e992ec8b7f500bbc", "title": "Design Of Ion-implanted MOSFET's with Very Small Physical Dimensions"}, {"paperId": "35a8c483ed0641ebbb56da1bd2d0f188b000a8ca", "title": "RETROSPECTIVE:PRIME: A Novel Processing-in-memory Architecture for Neural Network Computation in ReRAM-based Main Memory"}, {"paperId": "e6da45c79cf4c0b051b17ec9354927c61ed38776", "title": "RETROSPECTIVE: Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing"}, {"paperId": "2327537fa150b322c2546a98292808d6d010691c", "title": "RETROSPECTIVE: Neurocube: a programmable digital neuromorphic architecture with high-density 3D memory"}, {"paperId": "bc4bf86b9bd3bc4311ca64485a02323024f81ad4", "title": "Accelerating Attention through Gradient-Based Learned Runtime Pruning"}, {"paperId": "18e58bad15992625ab3fcb2f653a7c09db279a43", "title": "Genus Synthesis Solution"}, {"paperId": null, "title": "Transformers: Opening New Age of Arti\ufb01cal Intelligence Ahead"}, {"paperId": null, "title": "The WikiText Long Term Dependency Language Modeling Dataset"}, {"paperId": null, "title": "\u201cArtisan Memory Compilers,\u201d"}, {"paperId": null, "title": "\u201cInnovus Implementation System,\u201d"}, {"paperId": null, "title": "A 74 TMACS/W CMOS-RRAM Neurosynaptic Core with Dynamically Reconfigurable Dataflow and In-situ Transposable Weights for Probabilistic Graphical Models"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "EncoDeep: Realizing Bit-Flexible Encoding for Deep Neural Networks"}, {"paperId": "5bfecd14937da569eabec0afea710db846d3899b", "title": "Stripes: Bit-serial deep neural network computing"}, {"paperId": null, "title": "Multi-Stream Write SSD"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "ADC performance survey"}, {"paperId": "5906fc1d9cc56d31b9373cdb868cb90aa613d90d", "title": "994 International Conference on Parallel Processing Execube -a New Architecture for Scaleable Mpps"}]}