{"paperId": "fb9dfabf5e91ed4e903da01f9c34dcbfecb11efc", "title": "MediSwift: Efficient Sparse Pre-trained Biomedical Language Models", "abstract": "Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense fine-tuning and strategic soft prompting, MediSwift models outperform existing LLMs up to 7B parameters on biomedical tasks, setting new benchmarks w.r.t efficiency-accuracy on tasks such as PubMedQA. Our results show that sparse pre-training, along with dense fine-tuning and soft prompting, offers an effective method for creating high-performing, computationally efficient models in specialized domains.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The results show that sparse pre-training, along with dense fine-tuning and soft prompting, offers an effective method for creating high-performing, computationally efficient models in specialized domains."}, "embedding": {"model": "specter_v2", "vector": [0.5465693473815918, 0.6715453863143921, -0.41600900888442993, -0.17961600422859192, -0.44402143359184265, 0.0890934094786644, 0.5099034309387207, -0.0343039445579052, -0.7866619825363159, -0.027358785271644592, 0.5272271633148193, -0.2302619367837906, 0.39462900161743164, 0.7535378932952881, 0.06627711653709412, 0.14346738159656525, -0.8015099167823792, 0.5662482380867004, -0.33416464924812317, -0.275107204914093, -0.20955288410186768, -0.7265605330467224, -0.3771970868110657, 0.11859113723039627, 0.47318437695503235, 0.15878377854824066, 0.43179067969322205, 0.6749345660209656, -0.5244757533073425, 0.454521119594574, 0.33220186829566956, 0.0857357531785965, -0.013492872007191181, -0.1638277769088745, -0.32457390427589417, -0.10589538514614105, 0.20632751286029816, 0.09670530259609222, -0.3133190870285034, 0.5379536747932434, 0.06218458712100983, 0.24695099890232086, 0.6385224461555481, -0.461474746465683, -0.40772324800491333, 0.7208623290061951, 0.6668470501899719, 0.7088723182678223, -0.08499854803085327, -0.3807007074356079, 1.1212494373321533, -1.533411979675293, 0.7316250801086426, 1.0916215181350708, 0.49894213676452637, 1.0131343603134155, 0.01147574745118618, -0.7881786823272705, 0.39497682452201843, -0.13115784525871277, -0.7237735390663147, -0.5676122307777405, -0.3296314477920532, -0.38383084535598755, 1.8767491579055786, -0.3142566680908203, -0.13277161121368408, 0.5766571164131165, 0.13533899188041687, 0.8967782258987427, -0.06249823421239853, -0.5844915509223938, -0.15272539854049683, 0.05025915056467056, -0.124969482421875, 0.8529402017593384, -0.7248177528381348, 0.02151177078485489, -1.0125559568405151, -0.45971670746803284, 0.0958612933754921, -0.06629731506109238, -0.07258092612028122, 0.27214258909225464, -0.7598487734794617, 0.3938142955303192, 0.3593355417251587, 1.1132169961929321, -0.07215805351734161, 0.17491762340068817, 0.31552770733833313, 0.33022332191467285, 0.31332096457481384, 0.6858291625976562, -0.5898663997650146, 0.677456259727478, -1.473488450050354, 0.14887115359306335, 0.11427215486764908, 0.9490842223167419, -0.6143614649772644, 0.12767794728279114, -0.9458032846450806, 0.22895905375480652, 1.4008651971817017, 0.1522286832332611, 0.8453372716903687, -0.4680350124835968, 0.24806050956249237, -0.6271468997001648, 0.10334625095129013, -0.5544410943984985, -0.7780060768127441, -0.38219404220581055, -1.0765587091445923, -1.7087832689285278, -0.5460541248321533, -0.09025079011917114, -0.8523294925689697, 1.193239688873291, -0.1836138218641281, 0.562477171421051, 0.32392463088035583, 0.5630194544792175, 1.185782551765442, 0.6855484843254089, 0.646578848361969, -0.018421720713377, 1.067652940750122, -0.7267910838127136, -0.7692639827728271, -1.3229068517684937, 0.8172729015350342, -0.4737910032272339, -0.0017414005706086755, -0.2690349519252777, -1.1895395517349243, -0.6040662527084351, -0.31807073950767517, -0.2224137932062149, -0.41429999470710754, 0.5586905479431152, 0.8042510747909546, -0.004896760452538729, -0.7988181114196777, 0.5122115015983582, 0.006257018074393272, -0.3149326741695404, 0.1528928279876709, 0.03543446213006973, 0.1617847979068756, -0.5037338733673096, -1.4759900569915771, 0.34791824221611023, -0.04297523945569992, -0.6481809020042419, -0.374188095331192, -0.8658903241157532, -1.0155447721481323, -0.07026580721139908, 0.27358338236808777, -0.7723855972290039, 1.2920626401901245, -0.02876460552215576, -1.0058143138885498, 1.2684334516525269, -0.4264780282974243, -0.049130626022815704, 0.3371753692626953, -0.004815801978111267, -0.8801538944244385, -0.10531865060329437, -0.14295987784862518, 0.7560654878616333, 0.5237334370613098, 0.3248499631881714, 0.28667333722114563, 0.5583757162094116, -0.5658541321754456, -0.26406970620155334, -0.1999669373035431, 0.7531625628471375, -0.42147156596183777, -0.4956870675086975, 0.16120681166648865, 0.5100880861282349, -0.6673671007156372, -0.08159942179918289, -0.3729478418827057, -0.6899162530899048, 0.28279098868370056, -0.0578894317150116, 0.7803066968917847, -1.091200351715088, -0.6656761169433594, -0.10344995558261871, 0.23975564539432526, -0.08392488211393356, -0.8696213364601135, 0.5204985737800598, -0.5902188420295715, 0.0871855691075325, -0.5901536345481873, -1.2056984901428223, -0.14095722138881683, -0.23109164834022522, -0.49546465277671814, 0.0836382508277893, 0.32958677411079407, 1.0926014184951782, -0.9244306087493896, -0.1872982531785965, -0.2115134745836258, 0.4861789643764496, -1.340553641319275, 1.120399832725525, -0.5186890959739685, 0.4361482560634613, 0.12239573895931244, -0.1925363689661026, -0.12279090285301208, -0.9634270668029785, 0.4638179540634155, -0.5481516718864441, -0.16883604228496552, 0.24557878077030182, -0.41347864270210266, 1.7776858806610107, -0.3269631564617157, 0.09034673124551773, 0.2634631097316742, -0.8991889357566833, 0.09581072628498077, 0.7005239129066467, 0.39631208777427673, -0.3464862108230591, 0.20171454548835754, 0.5617178082466125, -0.5604739189147949, 0.033064354211091995, 0.3942861557006836, 0.4539080858230591, 0.031877484172582626, 0.0087394705042243, 0.657800555229187, -0.2669578492641449, 0.4029134511947632, 0.8568887114524841, 0.6365962624549866, -0.09673956781625748, 0.5453222393989563, -0.34868428111076355, 0.7421067357063293, -0.7649307250976562, 0.029269978404045105, 0.33729633688926697, 0.704065203666687, 0.7097045183181763, 0.09278269112110138, -0.44933706521987915, -0.04025029391050339, 0.28723791241645813, 0.6061350107192993, 1.4419106245040894, -0.2799343168735504, -0.30135005712509155, -0.5458549857139587, 0.0046125054359436035, -0.3633171617984772, -0.10020297765731812, -0.22753162682056427, -0.33193325996398926, -0.39815977215766907, -1.1917071342468262, 0.6699348092079163, 0.005891326814889908, 0.3567047417163849, -0.5484906435012817, -0.3426226079463959, -0.26624026894569397, 0.0015389419859275222, -1.1178308725357056, -0.8976088166236877, 0.3581528961658478, -0.8614161014556885, -0.24910284578800201, -0.6211573481559753, -0.40144047141075134, 0.5248059630393982, -0.7844370603561401, 1.0921857357025146, -0.4195946156978607, -0.7329651117324829, 0.35394734144210815, 0.7157720327377319, -0.4310310482978821, -0.6845114231109619, 0.0189658310264349, 0.14693856239318848, -0.24876978993415833, -0.06194305047392845, 0.5570350289344788, -0.30205395817756653, -0.15554766356945038, -0.5068719983100891, 0.29934772849082947, 0.2770558297634125, 0.2091761976480484, 0.6060672998428345, -0.5623830556869507, 0.33159443736076355, -1.500468134880066, 0.7153599858283997, -0.24190574884414673, -0.27548909187316895, 0.18397915363311768, -0.7599584460258484, -0.3806592524051666, 0.5335201621055603, -0.808684766292572, -0.2585710287094116, -0.8061707019805908, -0.14313887059688568, 0.12764817476272583, -0.006689909845590591, 0.7518429160118103, 0.6787438988685608, 0.33834055066108704, 0.6258437633514404, 0.44167226552963257, 0.23277556896209717, -0.16340984404087067, 0.5841102004051208, -0.6957095861434937, 0.47804200649261475, 0.22428980469703674, 0.2857514023780823, -0.15947921574115753, -0.13450270891189575, -0.8887170553207397, -0.5939729809761047, -0.672956109046936, 0.044286809861660004, 0.08192262053489685, 0.044297955930233, -1.0272531509399414, -0.5181463360786438, -0.37279051542282104, -0.7199872732162476, -0.39615634083747864, 0.22298885881900787, -0.18948917090892792, 0.056256044656038284, -0.7892892360687256, -1.2982559204101562, -0.26528242230415344, -0.9757298827171326, -0.9732357859611511, 0.8962257504463196, -0.21322853863239288, -0.6897689700126648, -0.7345003485679626, -0.23648612201213837, -0.29749342799186707, 1.0213229656219482, -0.6580986976623535, 0.8864960074424744, -0.23744520545005798, 0.4024827182292938, -0.11992793530225754, 0.19530053436756134, 0.3011142313480377, -0.25704526901245117, 0.14849689602851868, -0.7066294550895691, 0.09467129409313202, -0.4511997103691101, -0.2504417598247528, 0.0578220970928669, 0.6134344935417175, 0.8066771030426025, 0.19446520507335663, -0.5802086591720581, 0.4357737898826599, 0.9272205233573914, -0.9077759385108948, -0.4791097342967987, -0.39852261543273926, 0.5693870186805725, -0.06895969063043594, -0.19712688028812408, 0.9456787705421448, 0.15529732406139374, 0.004901284351944923, -0.17713774740695953, -0.5239861011505127, 0.0649639368057251, -0.21903054416179657, 0.352353572845459, 1.7101486921310425, 0.6264639496803284, 0.08572910726070404, -1.2428849935531616, 0.5260266065597534, -0.8060399293899536, -0.3403833508491516, 0.4526316523551941, 0.4123588800430298, 1.0669662952423096, -0.22180864214897156, -0.5168660879135132, -0.5197997689247131, -0.0428653284907341, 0.08004710078239441, -0.279317706823349, -0.06870009750127792, -0.012898280285298824, 0.9260959029197693, 0.15840236842632294, 0.4531378149986267, -0.2211126834154129, 0.4830305874347687, 14.815991401672363, 0.5875459909439087, -0.21390587091445923, 0.8204824328422546, 0.6409125924110413, 0.13638657331466675, -0.20327848196029663, -0.47771206498146057, -1.0807358026504517, -0.16012738645076752, 1.197500228881836, 0.29632046818733215, 0.3876267969608307, 0.2814764380455017, 0.17449043691158295, 0.19415168464183807, -0.6140096187591553, 0.7828488945960999, 0.484245240688324, -1.5016899108886719, -0.02538946084678173, -0.13888049125671387, 0.5775696635246277, 0.5693670511245728, 0.9474896192550659, 0.7228888273239136, -0.002946617780253291, -0.6034047603607178, 0.2597993314266205, 0.5016133785247803, 1.187491774559021, 0.4128396213054657, 0.45030516386032104, 0.8148649334907532, -0.6197538375854492, -0.1436518132686615, -0.4980379641056061, -0.791146993637085, 0.13082262873649597, 0.7003946304321289, -0.8642093539237976, -0.34776630997657776, -0.430708646774292, 1.0904324054718018, 0.3293256461620331, 0.29686909914016724, 0.2656387388706207, 0.8017049431800842, 0.09635302424430847, 0.22163553535938263, 0.21701109409332275, 0.6647244095802307, 0.04930964857339859, 0.37617728114128113, 0.2490859478712082, 0.03645865246653557, 0.37861698865890503, 0.5145665407180786, -0.6249443888664246, 0.15049895644187927, -0.09062612801790237, -0.39917078614234924, -0.24124376475811005, 0.9330964684486389, 0.3809864819049835, -0.032554399222135544, -0.5849377512931824, 0.13107240200042725, 0.2798745632171631, 0.23167608678340912, -0.15775999426841736, 0.26600950956344604, 0.20330749452114105, -0.3645421266555786, -0.6068822741508484, 0.35652628540992737, -0.1467382162809372, -0.7303740978240967, -0.9728420972824097, -0.7245739102363586, 0.48845410346984863, -0.5945631265640259, -0.9678011536598206, 0.5854883790016174, -0.4610061049461365, -0.5208181142807007, 0.46230578422546387, -0.9156079292297363, 0.43744921684265137, 0.8904153108596802, -1.5843878984451294, -0.2312844693660736, 0.4065420925617218, -0.12358292192220688, -0.3059871196746826, -0.37888336181640625, 1.5242282152175903, 0.4555809795856476, -0.2377775013446808, -0.06300808489322662, 0.09846974909305573, -0.23469668626785278, 0.5392300486564636, -0.4843829572200775, 0.5220734477043152, -0.1297425776720047, -0.24697495996952057, 0.8814390301704407, 0.08201942592859268, 0.0001536218769615516, -0.9651098251342773, -0.209538534283638, 1.1777770519256592, -0.9149242043495178, -0.2531008720397949, -0.8150543570518494, -0.691565990447998, 0.07073646038770676, 0.38007181882858276, -0.2146281599998474, 0.900417149066925, 0.06890370696783066, -0.3112008273601532, 0.1685274988412857, -0.8874129056930542, 0.05853357911109924, 0.674667477607727, -0.7514186501502991, -0.4132477641105652, 0.7882382869720459, 0.2516939043998718, -1.15921950340271, -0.7389891147613525, -0.023640984669327736, -0.18738257884979248, 0.6294319033622742, 1.22004234790802, -0.6113677620887756, 0.8468855619430542, 1.0655306577682495, 0.11087016016244888, -0.7997714281082153, 0.1485632061958313, -0.800149142742157, -0.26177340745925903, -0.6107600331306458, 0.7577401995658875, -0.3283348083496094, 0.042505934834480286, 0.8181599974632263, 0.16577376425266266, -0.5466837286949158, -0.5011107921600342, -0.23810167610645294, 0.008999894373118877, -0.15631894767284393, -0.04742981493473053, 0.4734169542789459, 0.09786709398031235, 0.15291088819503784, 0.44414955377578735, 0.4559122323989868, -0.47481268644332886, -0.5224055051803589, 0.16995613276958466, -0.16434794664382935, -0.28961458802223206, -0.40832948684692383, 0.19632598757743835, -1.1724112033843994, 0.09023067355155945, -1.3871556520462036, 0.11070402711629868, -0.6632287502288818, -0.31361857056617737, 0.06028681620955467, -0.4040089249610901, 0.3947685658931732, -0.054856639355421066, -0.4034285545349121, -0.8830185532569885, -0.6139572262763977, -0.45311710238456726, 0.5545927286148071, 1.1676002740859985, -0.8865620493888855, 0.18232543766498566, -0.21518728137016296, 0.05438624322414398, 0.17856784164905548, 0.32118523120880127, -0.4397374093532562, -0.9034923911094666, -1.0848184823989868, 0.4425012767314911, 0.1430852711200714, 0.0029966102447360754, -0.6386579871177673, 0.6730175614356995, 0.1535370647907257, -0.30809280276298523, 0.25434163212776184, 0.2626083791255951, -0.9473741054534912, -0.271149605512619, 0.7747817635536194, -1.0436222553253174, 0.14438898861408234, 0.13404826819896698, -0.8159856796264648, -0.2537756860256195, 0.6396026611328125, 0.16571097075939178, -1.2228124141693115, -0.46207931637763977, 0.5708087086677551, -0.7501417398452759, 0.4846705198287964, -0.6001191139221191, 0.40359941124916077, -0.7594911456108093, -0.19026771187782288, -0.45887887477874756, 0.5829579830169678, -0.9088569283485413, 0.5501606464385986, 0.5146584510803223, -0.8069342374801636, -0.029235094785690308, 0.29209157824516296, 0.00767964543774724, -0.24406154453754425, 0.7501904368400574, 0.6198722124099731, -0.21866442263126373, 0.794548749923706, 0.37454932928085327, 0.3732111155986786, -0.925464928150177, -0.14022688567638397, 0.7053692936897278, -0.5792590975761414, -0.3247103989124298, 1.4842017889022827, -0.2739342153072357, -1.1321113109588623, 0.23075872659683228, -1.2249194383621216, -0.4851779043674469, -0.5152835845947266, 0.4904281497001648, -0.2697579264640808, 0.09432846307754517, 0.009312908165156841, -0.9403674602508545, 0.018761353567242622, 0.05821850895881653, -0.4097517728805542, 0.8322402834892273, 0.1514381617307663, -0.2645145654678345, 0.4083523154258728, 0.687677264213562, -0.5809306502342224, -0.11236854642629623, -0.865099310874939, -0.12239246815443039, 0.49650001525878906, 0.2413257360458374, -0.6437069177627563, -0.7190608382225037, 0.7915773987770081, 0.3852298855781555, -0.22297707200050354, 0.3275580406188965, -0.35228970646858215, 0.20570549368858337, 0.7141523361206055, -0.1337091326713562, -0.8876144886016846, -0.49961259961128235, 1.3129371404647827, 1.0312519073486328, -1.2065502405166626, 0.35203954577445984, -0.2436290681362152, -0.6801735162734985, 0.6350437998771667, 0.11140009760856628, 0.15599356591701508, 1.1715829372406006, -0.19935518503189087, 0.13725118339061737, -0.16056959331035614, -1.0508413314819336, 0.02914717048406601, 1.3688045740127563, 0.7349931001663208, 1.3862709999084473, 0.12790317833423615, -0.15674716234207153, 1.0982533693313599, 0.15364477038383484, 0.36449772119522095, 0.13227726519107819, 0.5633725523948669, 0.008946163579821587, -0.13246922194957733, 0.05970285087823868, 0.7868703603744507, -0.7869805693626404, -0.974934995174408, 0.021207287907600403, 0.11330164223909378, 0.400833398103714, 0.44010281562805176, 0.22643151879310608, 0.42883041501045227, 0.437643438577652, 0.3977978527545929, 0.22142340242862701, -1.0933500528335571, -0.16923291981220245, 0.0073297033086419106, -0.7306479811668396, 0.005964422132819891, -0.32119715213775635, -0.33003395795822144, -0.6313356161117554, 0.20540426671504974, 0.6537875533103943, 0.046458445489406586, 0.2679094672203064, 1.3672348260879517, 0.5898377895355225, 0.8018214106559753, -0.6532360911369324, -0.10134600102901459, -0.27072247862815857, -0.9135262370109558, 0.11448322236537933, -0.44443613290786743, -0.06509494036436081, -0.1596330851316452, 0.05475712940096855, 0.17480702698230743]}, "authors": [{"authorId": "51153332", "name": "Vithursan Thangarasa"}, {"authorId": "2289846001", "name": "Mahmoud Salem"}, {"authorId": "2289848171", "name": "Shreyas Saxena"}, {"authorId": "2212028628", "name": "Kevin Leong"}, {"authorId": "3130228", "name": "Joel Hestness"}, {"authorId": "2212029838", "name": "Sean Lie"}], "references": [{"paperId": "bde9da9a39a065588d7f4573936731510d6f4f29", "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine"}, {"paperId": "2455cd8dc31ab08163100b87702b1e5b8a9982aa", "title": "TorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs"}, {"paperId": "0d2adcddccd72de47c263b6e4e0aab3dd0582a52", "title": "ReLoRA: High-Rank Training Through Low-Rank Updates"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "e154dd91de91558f9d671370754eace62a54c911", "title": "A study of generative large language model for medical research and healthcare"}, {"paperId": "04ee9597be4d6d2457214334e495e591000b5542", "title": "PMC-LLaMA: Towards Building Open-source Language Models for Medicine"}, {"paperId": "348a1efa54376fa39053e5e25d52bd0eb6a0ba68", "title": "Capabilities of GPT-4 on Medical Challenge Problems"}, {"paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "title": "Large language models encode clinical knowledge"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "ad3dfb2514cb0c899fcb9a14d229ff2a6018892f", "title": "Deep Bidirectional Language-Knowledge Graph Pretraining"}, {"paperId": "44279244407a64431810f982be6d0c7da4429dd7", "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"}, {"paperId": "d697b440dd0e65a05fe027e4c0ea85f62dcba033", "title": "Can large language models reason about medical questions?"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "8326dba15f6b8ee6e43c23eea3265a05e59e8135", "title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training"}, {"paperId": "a83cdcc0135c58fddf89fc72f1b92b7a9d1e170f", "title": "LinkBERT: Pretraining Language Models with Document Links"}, {"paperId": "821b08d595b6482e3d1f5bab6835b72d67ebd894", "title": "The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training"}, {"paperId": "d617f51833860dc50d202af7f80be71304b2e994", "title": "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}, {"paperId": "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed", "title": "Sparse is Enough in Scaling Transformers"}, {"paperId": "6d178f85c8ca9698b02d0f4f1f4a8f2fc4895411", "title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models"}, {"paperId": "4183d028c7b7e54f55e63698232e4d0a6df535bc", "title": "Towards Structured Dynamic Sparse Pre-Training of BERT"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a85ba5bb3e97c999f5f6dbc78f277b107af1dba2", "title": "Sparse Training via Boosting Pruning Plasticity with Neuroregeneration"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"}, {"paperId": "4066d78b637c2b8e57de5ffd53950134a551de85", "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed"}, {"paperId": "bb3367b067ef2cca2f7954bb0326e8966cc8895d", "title": "SparseDNN: Fast Sparse Deep Learning Inference on CPUs"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "9e104d440540d2ffc9caaa0952a9e5f7f9344ba9", "title": "Are wider nets better given the same number of parameters?"}, {"paperId": "a2f38d03fd363e920494ad65a5f0ad8bd18cd60b", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"}, {"paperId": "389036b1366b64579725457993c1f63a4f3370ba", "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "7a87ab984ca45aae2c5768d22cd6df3b5fd509f9", "title": "What\u2019s Hidden in a Randomly Weighted Neural Network?"}, {"paperId": "2e3002f131e1815bda7a10303eff97f79dea01ec", "title": "Rigging the Lottery: Making All Tickets Winners"}, {"paperId": "db2d3dc613169b519f1a2dd35e0473dc2e848025", "title": "Fast Sparse ConvNets"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "0c3c4c88c7b07596221ac640c7b7102686e3eae3", "title": "PubMedQA: A Dataset for Biomedical Research Question Answering"}, {"paperId": "dd7bae431e0e4d94f24d54b0ac3a422703d38ed3", "title": "The Difficulty of Training Sparse Neural Networks"}, {"paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217", "title": "The State of Sparsity in Deep Neural Networks"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "6dbb9e4b2e3b67dc4e1634989511f67d41373dd0", "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "7787d3c03ab96337d21f8c9afc223fdcd2489a2f", "title": "Automatic semantic classification of scientific literature according to the hallmarks of cancer"}, {"paperId": "4ee2eab4c298c1824a9fb8799ad8eed21be38d21", "title": "Moses: Open Source Toolkit for Statistical Machine Translation"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": "ee6564e2e0f47c6ac131b093f057ca75907958c9", "title": "BioELECTRA:Pretrained Biomedical text Encoder using Discriminators"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "Thinking outside the die: Architecting the ml accelerator of the future"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "7c5c2ef6f0913dd538bd57022dd21987be170e56", "title": "Exploiting Unstructured Sparsity on Next-Generation Datacenter Hardware"}, {"paperId": null, "title": "Deep learning scaling is predictable"}, {"paperId": null, "title": "2023. Train a model with weight sparsity"}, {"paperId": null, "title": "2023. Clinical camel: An open expert-level medical language model with dialogue-based knowledge"}, {"paperId": null, "title": "2024. ReLU strikes back: Exploiting activation sparsity in large language models"}, {"paperId": null, "title": "2022b. Harnessing the Power of Sparsity for Large GPT AI Models"}, {"paperId": null, "title": "2024. Medlm: Exploring language models for medical question answering systems"}, {"paperId": null, "title": "2022a. Cerebras architecture deep dive: First look inside the hw/sw co-design for deep learning"}, {"paperId": null, "title": "2023. Towards expert-level medical question answering with large language models"}, {"paperId": null, "title": "2022. Pixelated butterfly: Simple and efficient sparse training for neural network models"}, {"paperId": null, "title": "2022. Effective model sparsifi-cation by scheduled grow-and-prune methods"}, {"paperId": null, "title": "Christo-pher"}, {"paperId": null, "title": "2024. Sparsity made easy - introducing the cerebras pytorch sparsity library"}, {"paperId": null, "title": "Together Computer"}, {"paperId": null, "title": "2023b. Sparse iso-FLOP transformations for maximizing training efficiency"}]}