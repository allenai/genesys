{"paperId": "8c8376d3c56a964ed076c0085e775ffd731dbf39", "title": "Thorough Characterization and Analysis of Large Transformer Model Training At-Scale", "abstract": "Large transformer models have recently achieved great success across various domains. With a growing number of model parameters, a large transformer model training today typically involves model sharding, data parallelism, and model parallelism. Thus, the throughput of large-scale model training depends heavily on the network bandwidth since a combination of model sharding and multiple parallelism strategies incurs various costs. However, prior characterizations of transformer models on high-bandwidth DGX machines that use TFLOPS as a metric may not reflect the performance of a system with lower bandwidth. Furthermore, data and model parallelism reveal significantly distinct training profiles on different system bandwidths at scale and, thus, need a thorough study. In this paper, we provide a bottom-up breakdown of training throughput into compute and communication time, and quantitatively analyze their respective influences on overall end-to-end training scaling. Our evaluation involves an in-depth exploration of data parallelism, scaling up to 512 GPUs with limited bandwidth, and examines three model sharding strategies among six model sizes. We also evaluate three combinations of model parallelism on both high and low bandwidth supercomputing systems. Overall, our work provides a broader perspective on large-scale transformer model training, and our analysis and evaluation yield practical insights for predicting training scaling, shaping the future development of supercomputing system design.", "venue": "Proceedings of the ACM on Measurement and Analysis of Computing Systems", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A bottom-up breakdown of training throughput into compute and communication time, and quantitatively analyze their respective influences on overall end-to-end training scaling are provided, providing a broader perspective on large-scale transformer model training."}, "embedding": {"model": "specter_v2", "vector": [0.05415641888976097, 0.034295640885829926, -0.48737552762031555, 0.41806378960609436, -0.28287142515182495, 0.3571314215660095, 0.5268349647521973, -0.1429893672466278, -0.0917302668094635, -0.4632042944431305, 0.09455117583274841, -0.7884998321533203, 0.09156414866447449, -0.22651955485343933, 0.004562937654554844, -0.18745043873786926, -1.2519012689590454, 0.6451708674430847, 0.2615630030632019, -0.11518026888370514, -0.24309800565242767, 0.33754661679267883, -1.3957781791687012, 0.302650511264801, 0.6154826879501343, 1.3423396348953247, -0.45515376329421997, 1.0895839929580688, -0.13084669411182404, 0.2386595755815506, 0.40896472334861755, 0.4364219605922699, 0.4903455078601837, 0.03603862598538399, -0.3145899474620819, 0.17586848139762878, 0.33342689275741577, -0.4885823428630829, -0.8197470307350159, 0.13742764294147491, 0.16821017861366272, 0.37309232354164124, 0.13652005791664124, -1.0980687141418457, 0.5077905058860779, -0.18376649916172028, 0.22288668155670166, 1.3534481525421143, -0.8726969957351685, -0.3474886119365692, 0.5786527991294861, -1.0619354248046875, -0.006772524677217007, 0.8794804215431213, 0.7682846784591675, -0.24452905356884003, -0.7290805578231812, -0.2816630005836487, -0.16546061635017395, -0.23387111723423004, -0.622285008430481, -0.4783286452293396, -0.2700018286705017, -0.4494590759277344, 1.7324581146240234, -0.35076162219047546, 0.20212006568908691, 0.019101886078715324, 0.1424342691898346, 0.9162585735321045, 0.33482685685157776, -0.7968577742576599, 0.2413480132818222, 0.07268623262643814, 0.41236743330955505, 0.7241219282150269, -0.11733878403902054, 0.4757533371448517, -1.5138429403305054, -0.7171990275382996, 0.7221378087997437, -0.31716111302375793, 0.424407422542572, -0.4199484884738922, 0.05626494809985161, 0.725730836391449, 0.20900540053844452, 0.05785257741808891, -0.336270272731781, 0.6349678039550781, 1.0353103876113892, 0.29646262526512146, 0.6078615188598633, -0.10851532220840454, 0.05581241101026535, 0.22627678513526917, -1.1960642337799072, -0.023949753493070602, 0.3021584153175354, 0.8190749883651733, -0.0038937886711210012, 0.527457058429718, -0.5331427454948425, 0.21389031410217285, 0.7608910799026489, -0.09117196500301361, 0.3565049171447754, -0.6088406443595886, 0.056996677070856094, -0.567841112613678, 0.17910230159759521, -0.32480940222740173, -0.42438170313835144, -0.7146733999252319, -1.1678361892700195, -0.6472982168197632, -0.6276084184646606, -0.07758626341819763, -0.6314259767532349, -0.2819131314754486, 0.34729793667793274, 0.81723552942276, -0.12079735845327377, 0.6756817102432251, 0.5844915509223938, 0.41744986176490784, 0.3274032473564148, 0.1796014904975891, 0.9444124698638916, -1.2528605461120605, 0.34408801794052124, -0.8297650218009949, 0.41187408566474915, -0.5589799284934998, -0.06490810215473175, -0.33560991287231445, -1.5829226970672607, -1.1754345893859863, -0.5520138740539551, 0.38033971190452576, -0.030189670622348785, -0.034503985196352005, 1.8220131397247314, 0.07657850533723831, -1.0672948360443115, 0.9095650315284729, -0.6331816911697388, -0.33160242438316345, 0.31696516275405884, 0.0646962821483612, 0.45481061935424805, -0.2411717176437378, -0.7109214067459106, -0.1683560609817505, -0.04512052610516548, -0.8609601855278015, -0.4695819318294525, -1.198184609413147, -0.32662275433540344, 0.31714341044425964, -0.04882077872753143, -0.952751636505127, 1.0427712202072144, 0.13225135207176208, -0.49328431487083435, 0.5915111303329468, 0.07870066910982132, -0.3274339735507965, 0.8441087603569031, 0.23230567574501038, -0.6347586512565613, 0.10055729746818542, -0.22332917153835297, 0.2266857922077179, 0.5328042507171631, -0.3422169089317322, -0.006012717727571726, 0.11882130801677704, -0.5010973215103149, 0.2027989774942398, -0.1667102575302124, 0.8840725421905518, -0.20402544736862183, -0.026588397100567818, 0.275256872177124, 0.445884644985199, -0.5178427696228027, -0.054427389055490494, 0.05966203287243843, -0.22635826468467712, 0.7611653208732605, 0.1766689121723175, 0.8745259642601013, -0.6908417344093323, -1.1034529209136963, 0.2871694266796112, 0.1401529610157013, -0.07355352491140366, -0.17545004189014435, 1.0676473379135132, -0.3701845109462738, 0.042719706892967224, 0.16306759417057037, -0.5779708623886108, -0.07145257294178009, -0.15410949289798737, -0.8730960488319397, -0.3565634489059448, -0.3316431939601898, 0.6197509169578552, -0.3762853741645813, 0.011060871183872223, -0.17109067738056183, 0.28045108914375305, -1.1048564910888672, 1.2869867086410522, -0.2894029915332794, 0.14462202787399292, 0.10094740241765976, -0.2036363035440445, 0.3411012291908264, -1.2966992855072021, 0.7814899682998657, -0.5121338963508606, -0.06288199871778488, 0.29598838090896606, -0.3153851628303528, 0.9382120966911316, -0.3312512934207916, -0.053251639008522034, 0.3760155737400055, -0.984235405921936, -0.07310419529676437, 0.28240111470222473, 0.622816801071167, -0.5811631679534912, 0.39284852147102356, 0.36943569779396057, -0.5197341442108154, 0.6172797083854675, 1.0865252017974854, 0.7550218105316162, 0.10309148579835892, 0.6380244493484497, 0.4244455397129059, 0.004885889124125242, 0.6868056654930115, 0.6372223496437073, 0.6053336262702942, 0.1926804482936859, -0.354147344827652, -0.6948904991149902, -0.3957655131816864, -0.4849317669868469, -0.5098108649253845, 0.8042718172073364, 0.6936952471733093, 0.2278032749891281, 0.7433646321296692, -0.8046955466270447, -0.9762486219406128, -0.0589180551469326, 0.2076476365327835, 1.6248687505722046, -0.1743893027305603, -0.15952125191688538, -0.6262609958648682, -0.24622966349124908, 0.40632128715515137, -0.4570394456386566, 0.03981674835085869, -0.09532853960990906, -0.48725515604019165, -1.5686883926391602, 0.5298270583152771, 0.37530580163002014, 0.9101325273513794, -0.12770305573940277, -0.7176478505134583, -0.5547985434532166, 0.7383301854133606, -0.7955705523490906, -0.05202621594071388, 0.6283116936683655, -1.0482878684997559, -0.19439290463924408, 0.22612877190113068, -0.029808763414621353, 0.029105348512530327, 0.23782794177532196, 1.3263967037200928, 0.20580221712589264, -0.7504041194915771, 0.16134779155254364, 0.6809484958648682, -0.3402125835418701, -0.5724944472312927, 0.01578528620302677, 0.27445581555366516, -0.43904393911361694, -0.0014028261648491025, -0.05399323254823685, -0.3691071569919586, 0.24964703619480133, -0.2747209966182709, 0.420884907245636, -0.028665194287896156, 0.19358232617378235, 0.6311893463134766, -0.03891170397400856, 0.007973882369697094, -0.9984204173088074, 1.0872365236282349, -0.24101728200912476, -0.6291466951370239, 0.1489032357931137, -0.9295254349708557, -0.5408444404602051, 0.6224021315574646, -0.833119809627533, 0.02713140659034252, -1.632097601890564, 0.31299179792404175, -0.6120051741600037, -0.21145275235176086, -0.1164456158876419, 0.8926033973693848, -0.7830846905708313, 0.44962194561958313, 0.5861206650733948, 0.8001568913459778, -0.13767778873443604, 0.14245668053627014, -0.9388132691383362, -0.020648906007409096, -0.47530797123908997, -0.061917372047901154, -0.0015690228901803493, 0.40861523151397705, -0.8427439332008362, -0.3295350670814514, -0.24908970296382904, -0.1803804636001587, -0.44428202509880066, -0.14272639155387878, -0.4334803521633148, -1.152974009513855, -0.2156236469745636, -0.48790356516838074, -0.7501192688941956, 0.47384047508239746, -0.23530234396457672, -0.2292717695236206, -0.9153592586517334, -1.8454402685165405, -0.14919309318065643, -1.163294792175293, -1.1006789207458496, 0.5434731245040894, 0.11434710025787354, -0.19710037112236023, -0.620198667049408, -0.5860382318496704, -0.3760177195072174, 1.2597848176956177, -0.5235989689826965, 0.8000059723854065, 0.04788560792803764, -0.4397903084754944, 0.1379517912864685, -0.600226640701294, -0.058951906859874725, -1.0374096632003784, 0.1810118705034256, -0.5400086641311646, -0.36718255281448364, -0.6825193166732788, -0.3816497325897217, -0.028406159952282906, 0.2503141462802887, 1.10442054271698, 0.14123129844665527, -0.6063963174819946, 0.8009655475616455, 1.3201687335968018, -0.9440887570381165, -0.32242849469184875, -0.027682337909936905, 0.7772130370140076, -0.3095640242099762, -0.1763029545545578, 1.0452913045883179, -0.486061155796051, 0.5992953777313232, -0.16599543392658234, -0.8416656255722046, -0.3831171989440918, -0.33312761783599854, -0.11773180216550827, 1.4889171123504639, 0.6454511284828186, -0.04682834446430206, -1.1220223903656006, 0.08200173079967499, -1.1711891889572144, -0.37400028109550476, 0.5945475697517395, 1.0206266641616821, -0.18415328860282898, -0.09672471880912781, 0.19768860936164856, -0.2978927493095398, 0.07445841282606125, 0.23593370616436005, -0.586268424987793, -0.5682960152626038, 0.43090030550956726, 0.7541244626045227, 0.6612395644187927, 0.21610508859157562, 0.021635325625538826, 0.4062860310077667, 14.830860137939453, 1.215793251991272, -0.043665073812007904, 0.7334727048873901, 0.9135898947715759, 0.296257346868515, -0.09625016897916794, -0.11618390679359436, -1.152483344078064, 0.1755923628807068, 1.9584918022155762, -0.3376903235912323, 0.669070839881897, 0.6535953879356384, 0.07463400810956955, -0.3371773362159729, 0.07323610037565231, 0.5801416635513306, 0.25229841470718384, -1.874495029449463, 0.4815843403339386, 0.435869038105011, 0.4123528301715851, 0.8352149128913879, 0.3288164436817169, 0.6033973097801208, 0.5680625438690186, -0.37131398916244507, -0.07853514701128006, -0.03815360367298126, 1.1262308359146118, -0.6374191641807556, 0.45059093832969666, 0.5877076983451843, -1.0186982154846191, 0.5065805315971375, -0.6453492641448975, -1.156083106994629, 0.4214208722114563, 0.4081706404685974, -0.9481494426727295, 0.06928515434265137, -0.2118099331855774, 0.28256115317344666, 0.14304529130458832, 0.4709753692150116, 0.42949721217155457, 0.7635459303855896, -0.26740580797195435, 0.3785630166530609, 0.022959930822253227, 0.0815875381231308, -0.5532823204994202, -0.10681725293397903, 0.09992383420467377, -0.09745800495147705, 0.9388513565063477, -0.04442782700061798, -0.34651219844818115, -0.2738727629184723, -0.212738499045372, -0.1557110846042633, 0.016019143164157867, 1.085253119468689, -0.08651376515626907, -0.13635870814323425, -0.3868662416934967, 0.23234401643276215, 0.7852630615234375, 0.15208564698696136, -0.5962510108947754, 0.39255502820014954, 0.8828758001327515, -0.14630164206027985, -0.4994775950908661, 0.3248541057109833, -0.5814332962036133, -0.3195621073246002, -1.2079435586929321, -0.37217673659324646, 0.3885975480079651, -0.7203550338745117, -0.8843699097633362, 0.7434882521629333, -0.2449144870042801, -0.2644106447696686, 0.6162998080253601, -0.9575145244598389, -0.18552343547344208, 0.8529276251792908, -1.2065439224243164, -0.530546247959137, -0.06556442379951477, -0.5050492882728577, -0.5334038734436035, -0.05776539444923401, 1.2450244426727295, 0.5982523560523987, -0.30172625184059143, -0.0462568998336792, 0.34032902121543884, -0.2632930278778076, -0.31174784898757935, -0.21906782686710358, 1.2162904739379883, 0.28824466466903687, -0.37112224102020264, 0.09466775506734848, 0.07532903552055359, 0.05351611599326134, -1.0534865856170654, -0.19576679170131683, 0.13041037321090698, -0.4949232041835785, 0.385623037815094, -0.6765127778053284, -1.1411981582641602, 0.1908494085073471, 0.24707280099391937, 0.36204418540000916, 0.7245091795921326, 0.10303155332803726, -0.2249724566936493, -0.1841208040714264, -0.5838562250137329, -0.15022554993629456, 0.6373105645179749, -0.8510529398918152, 0.4877740442752838, 0.4573914408683777, 0.1933528333902359, -1.3043209314346313, -1.2113678455352783, -0.04241417720913887, -0.1438058316707611, -0.41936033964157104, 1.1397137641906738, -0.7197483777999878, 0.7906519770622253, 1.1653423309326172, -0.004958320874720812, -0.5456331372261047, 0.0330355241894722, -0.7283839583396912, -0.14106528460979462, -0.2785145938396454, 0.03519265726208687, -0.39092323184013367, 0.727957010269165, 0.6934493780136108, 0.08381014317274094, -0.7128658294677734, -0.262511670589447, -0.17697012424468994, -0.32078757882118225, -0.34210917353630066, 0.1533869057893753, 0.04791741818189621, -0.07355577498674393, 0.04713204503059387, 0.2184576392173767, 0.7891566157341003, 0.14430153369903564, -0.09769734740257263, 0.7268774509429932, 0.0011635096743702888, -0.3673740327358246, -0.5059899091720581, -0.04829476401209831, -1.1989524364471436, -0.13251808285713196, -1.4293509721755981, -0.2251589447259903, -0.5574270486831665, -0.385544091463089, -0.5724678635597229, 0.16339313983917236, -0.26705074310302734, 0.34858012199401855, 0.10040169209241867, -0.8504168391227722, -0.4508560001850128, -0.5477935075759888, 0.6326422095298767, 0.7579385042190552, 0.2049545794725418, 0.5030597448348999, -0.443419873714447, 0.5863569378852844, 0.2426690012216568, 0.5679174661636353, -0.10903838276863098, -0.6781346201896667, -0.9708728790283203, -0.24504932761192322, 0.30287566781044006, 0.02872498892247677, -1.0681124925613403, 0.5711224675178528, 0.1560003161430359, 0.04847598820924759, 0.2619193494319916, 0.24144265055656433, -0.7474855184555054, -0.07660172134637833, 0.3468629717826843, -0.015370147302746773, 0.37842559814453125, 0.6219866871833801, -0.5951677560806274, 0.09066903591156006, 1.2307356595993042, 0.21212086081504822, -0.8130577206611633, -0.7198492288589478, 0.6520745754241943, -0.3569613993167877, 0.1680915355682373, -0.11691583693027496, 0.15060611069202423, -1.3148624897003174, 0.22848671674728394, -0.05816156044602394, 0.5576566457748413, 0.12694570422172546, 0.5128339529037476, -0.047540318220853806, -1.167587399482727, 0.5127103924751282, 0.6307954788208008, -0.4549999535083771, -0.2120494395494461, 0.9841277003288269, 0.7987690567970276, -0.8393380045890808, 0.22944147884845734, -0.24778205156326294, 0.41217291355133057, -1.021291971206665, 0.1998983472585678, 0.49825891852378845, -0.4956991672515869, -0.1757408082485199, 0.8489667773246765, -0.5530545711517334, -0.8365384936332703, -0.17210185527801514, -0.43635088205337524, -0.02092374488711357, -0.2663854956626892, 0.5050257444381714, 0.7215471863746643, 0.16721555590629578, 0.3724502623081207, -0.2574082314968109, 0.0013718653935939074, 0.15592679381370544, -0.10464451462030411, 0.2812644839286804, -0.2407161146402359, -0.43238624930381775, 0.4809081256389618, 0.2986852526664734, -0.7795674800872803, -0.8150827884674072, -0.6106516122817993, -0.35904693603515625, -0.47698453068733215, 0.45339202880859375, -0.20693452656269073, -0.5809723138809204, 0.9107142090797424, 0.3188171684741974, 0.6109253168106079, 0.14092499017715454, -0.23941132426261902, 0.46956318616867065, 0.11257242411375046, 0.7186093330383301, -0.5747859477996826, -0.7339447736740112, 0.8530638813972473, 0.5477608442306519, -0.9110275506973267, 0.5425077676773071, -0.37923625111579895, -0.6024835705757141, 0.9198622107505798, 0.7570949196815491, 0.009000995196402073, 0.8853834867477417, 0.17919176816940308, -0.05224602296948433, -0.19159874320030212, -1.131298542022705, -0.06905721873044968, 0.5891531109809875, 0.3496726155281067, 0.5365704298019409, 0.4280833601951599, 0.012149032205343246, 0.3179800510406494, 0.061562199145555496, 0.4716440439224243, 0.1533476561307907, 0.7105562090873718, -0.5147367715835571, 0.17580261826515198, 0.06249076500535011, 1.0812671184539795, -0.24919934570789337, -0.6512802839279175, 0.5060978531837463, 0.6631104350090027, -0.39744943380355835, 0.408128559589386, 1.3277831077575684, -0.09199365973472595, 0.3608565032482147, -0.09470829367637634, 0.4211817979812622, -0.3396237790584564, -0.34501129388809204, 0.08875812590122223, -0.17123669385910034, -0.20528756082057953, 0.3601641356945038, 0.16438640654087067, -0.490948349237442, -1.4147617816925049, 0.7889354228973389, 0.7012735605239868, 0.12903736531734467, 0.7403736710548401, 1.1771963834762573, 1.0078439712524414, -0.16562430560588837, -1.1109936237335205, -0.517493486404419, -0.7410134673118591, -0.20905740559101105, -0.4976564049720764, -0.6841485500335693, -0.04396405071020126, -0.05479099974036217, -0.568133533000946]}, "authors": [{"authorId": "2220695947", "name": "Scott Cheng"}, {"authorId": "2262012835", "name": "Jun-Liang Lin"}, {"authorId": "2157261", "name": "M. Emani"}, {"authorId": "151462530", "name": "Siddhisanket Raskar"}, {"authorId": "2256988999", "name": "Sam Foreman"}, {"authorId": "2257449427", "name": "Zhen Xie"}, {"authorId": "2267938602", "name": "Venkat Vishwanath"}, {"authorId": "2258702068", "name": "M. Kandemir"}], "references": [{"paperId": "aeb9454987c3f85563cf7a5d2cb7f3d502d3398d", "title": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"}, {"paperId": "1ca5618423c64f0656d13c2bc0d387cc2006f7b2", "title": "ZeRO++: Extremely Efficient Collective Communication for Giant Model Training"}, {"paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da", "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"}, {"paperId": "8f48c75e1354c88a84a67abb60789083c12e5037", "title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation"}, {"paperId": "ff5eb1bd55d61ae919865f6b4dab84e6ae1974f3", "title": "Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression"}, {"paperId": "62a45ab7b676f3877d41f66f6c9ddf1ec44a1c5f", "title": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics"}, {"paperId": "a306401ef3fedb1eb671e367ecf9ff67349873d1", "title": "On Optimizing the Communication of Model Parallelism"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "91b3f376f063dd59060ea5b95e8cb828adec36d7", "title": "GC3: An Optimizing Compiler for GPU Collective Communication"}, {"paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "title": "High-Resolution Image Synthesis with Latent Diffusion Models"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "659b95b28a9f3b6cfa3bd06fa7bd004165db5663", "title": "Tesseract: Parallelize the Tensor Parallelism Efficiently"}, {"paperId": "91b29761840442005da39bc258e2298b528f31aa", "title": "Breaking the computation and communication abstraction barrier in distributed machine learning workloads"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "3623a39bfbbefff942a2f370d76dd18fbc1d9139", "title": "Demystifying BERT: Implications for Accelerator Design"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "b0d692d8f9b5245b8fa35a6993f77a083e8a5067", "title": "Automatic Graph Partitioning for Very Large-scale Deep Learning"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "461a7bf0c14df0fb08d3c59bdec491778a861270", "title": "Synthesizing optimal collective algorithms"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "3836ccb33191799e748e8e96f85a813eaf650ff8", "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "00c957711b12468cb38424caccdf5291bb354033", "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "b6c096f5dba050868cae25669c0745e86221218f", "title": "Top500 Supercomputer Sites"}, {"paperId": "793f7284ccaa6c2cd530e6d405f5fa75bfd283e8", "title": "Scaling Infrastructure to Support Multi-Trillion Parameter LLM Training"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Tools Extension Library (NVTX)"}, {"paperId": null, "title": "NVIDIA Scalable Hierarchical Aggregation and Reduction Protocol (SHARP)"}, {"paperId": null, "title": "2023. CUDA Samples"}, {"paperId": null, "title": "2023. An efficient 2d method for training super-large deep learning models"}, {"paperId": null, "title": "2022. Alpa: Automating inter-and { Intra-Operator } parallelism for distributed deep learning"}, {"paperId": null, "title": "2023. GPT-4 Technical Report"}, {"paperId": null, "title": "Received October 2023; revised January 2024; accepted January 2024"}, {"paperId": null, "title": "OpenAI"}, {"paperId": null, "title": "2023. Gpts are gpts: An early look at the labor market impact potential of large language models"}, {"paperId": null, "title": "2022. Emergent abilities of large language models"}]}