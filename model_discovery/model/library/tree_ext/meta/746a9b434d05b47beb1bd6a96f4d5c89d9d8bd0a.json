{"paperId": "746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a", "title": "Your Transformer May Not be as Powerful as You Expect", "abstract": "Relative Positional Encoding (RPE), which encodes the relative distance between any pair of tokens, is one of the most successful modifications to the original Transformer. As far as we know, theoretical understanding of the RPE-based Transformers is largely unexplored. In this work, we mathematically analyze the power of RPE-based Transformers regarding whether the model is capable of approximating any continuous sequence-to-sequence functions. One may naturally assume the answer is in the affirmative -- RPE-based Transformers are universal function approximators. However, we present a negative result by showing there exist continuous sequence-to-sequence functions that RPE-based Transformers cannot approximate no matter how deep and wide the neural network is. One key reason lies in that most RPEs are placed in the softmax attention that always generates a right stochastic matrix. This restricts the network from capturing positional information in the RPEs and limits its capacity. To overcome the problem and make the model more powerful, we first present sufficient conditions for RPE-based Transformers to achieve universal function approximation. With the theoretical guidance, we develop a novel attention module, called Universal RPE-based (URPE) Attention, which satisfies the conditions. Therefore, the corresponding URPE-based Transformers become universal function approximators. Extensive experiments covering typical architectures and tasks demonstrate that our model is parameter-efficient and can achieve superior performance to strong baselines in a wide range of applications. The code will be made publicly available at https://github.com/lsj2408/URPE.", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 35, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2205.13401", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work mathematically analyzes the power of RPE-based Transformers regarding whether the model is capable of approximating any continuous sequence-to-sequence functions and develops a novel attention module, called Universal R PE-based (URPE) Attention, which satisfies the conditions and the corresponding URPE- based Transformers become universal function approximators."}, "embedding": {"model": "specter_v2", "vector": [0.39973369240760803, 0.8669711947441101, -0.4626918435096741, -0.3413155674934387, -0.49494874477386475, -0.12763914465904236, 0.32125625014305115, -0.278794527053833, -0.48579034209251404, -0.19865737855434418, 0.4457685053348541, 0.2541036009788513, 0.21127910912036896, 0.05430733412504196, -0.39865466952323914, -0.3722008466720581, -0.7138807773590088, 0.13925598561763763, 0.15668073296546936, -0.5998654365539551, 0.14059507846832275, -0.8322525024414062, -1.0564345121383667, 0.2766571640968323, 0.1027374342083931, 0.756560742855072, 0.5963305830955505, 0.9066382050514221, -0.44233259558677673, 0.9265047907829285, 0.3295305669307709, -0.4965197741985321, 0.22258934378623962, 0.17945513129234314, -0.6506038904190063, -0.6297909021377563, 0.16204969584941864, -0.19003666937351227, -0.8075990676879883, 0.9314712882041931, -0.24072961509227753, 0.48729923367500305, 0.41895005106925964, -0.5493056774139404, -0.6899299621582031, 1.4335371255874634, 0.7085872888565063, 0.7796934247016907, -0.42582088708877563, -0.6327941417694092, 1.7414606809616089, -1.07899808883667, -0.355357825756073, 1.4467182159423828, 0.47536271810531616, 0.36757487058639526, 0.2759765386581421, -0.33743593096733093, 0.770424485206604, 0.18542520701885223, -0.8452182412147522, -0.38676369190216064, 0.2201465219259262, 0.3283465802669525, 1.659072756767273, -0.0890200212597847, 0.00863049365580082, 0.5131424069404602, -0.4143056273460388, 1.7070626020431519, -0.1892714500427246, -0.8297340869903564, -0.34496816992759705, 0.02511126734316349, 0.4766938388347626, 0.6512517333030701, -0.7219838500022888, 0.19645603001117706, -0.9303370714187622, 0.34922564029693604, 0.5871320962905884, -0.044262003153562546, 0.2516113519668579, 0.0428786501288414, -0.173549622297287, 0.42163652181625366, 0.5307782292366028, 0.9760453104972839, -0.3531540334224701, 1.10482919216156, 0.4062063694000244, 0.1873243749141693, -0.2880336344242096, 0.5248679518699646, 0.021712183952331543, -0.17904062569141388, -0.6583627462387085, -0.013895191252231598, -0.3338873088359833, 0.7767864465713501, -0.40390387177467346, 0.5468275547027588, -0.46906229853630066, 0.201236754655838, 1.453665852546692, -0.10303013026714325, 0.5873781442642212, -1.1630818843841553, 0.2374693900346756, -0.9360567927360535, 0.06039176136255264, -0.8631148934364319, 0.393466979265213, -0.3132249712944031, -0.49781373143196106, -1.3130828142166138, -0.2639192044734955, 0.6024778485298157, -0.5151658058166504, 0.8334024548530579, -0.5711320042610168, 0.19005157053470612, -0.10524275153875351, 0.46079540252685547, 0.44058695435523987, 0.7919161915779114, 0.11132223159074783, 0.49467551708221436, 0.6105626225471497, -0.7728509306907654, -0.9856231212615967, -1.0936275720596313, 0.44906777143478394, -0.0722523182630539, 0.5214404463768005, -0.11541508138179779, -1.2922031879425049, -1.157967209815979, -1.0118507146835327, 0.08877256512641907, -0.7934137582778931, 0.10192660987377167, 1.1109169721603394, 0.25105252861976624, -1.1929243803024292, 1.0928328037261963, -0.38861575722694397, 0.046613939106464386, 0.49456802010536194, 0.4978724420070648, 0.20899130403995514, -0.13768433034420013, -1.5718638896942139, 0.6542918086051941, 0.4858173429965973, -0.08543065935373306, 0.07759924978017807, -0.6984750628471375, -1.3318471908569336, 0.39707931876182556, 0.26341789960861206, -0.3671919107437134, 1.3906021118164062, -0.02261163666844368, -1.3847049474716187, 0.4704734981060028, -0.4047597348690033, 0.14410319924354553, -0.05858604982495308, -0.1964159607887268, -0.3408738076686859, -0.43611615896224976, -0.04744882136583328, 0.49591532349586487, 0.28419220447540283, -0.17402301728725433, -0.36842262744903564, 0.2424679696559906, -0.3395521640777588, -0.004748656880110502, -0.429517924785614, 0.9989370107650757, -0.03162555769085884, -0.27668437361717224, 0.15140396356582642, 0.852698028087616, 0.2718574106693268, -0.21688134968280792, -0.5443496704101562, -1.1571152210235596, 0.26764604449272156, 0.28325772285461426, 0.9806735515594482, -0.8057751655578613, -0.7812817692756653, -0.26569196581840515, -0.15238463878631592, -0.12091663479804993, -0.6726381778717041, 0.17884625494480133, -0.5723776817321777, 0.4481313228607178, -0.15255975723266602, -0.8131231069564819, 0.3638969659805298, 0.02805911749601364, -0.41375118494033813, -0.061061397194862366, -0.13859665393829346, 1.044683814048767, -1.1730432510375977, 0.04133477807044983, 0.3285585939884186, 0.23351341485977173, -0.563038170337677, 1.1444998979568481, 0.13939081132411957, -0.27287840843200684, -0.03625401481986046, -0.30923205614089966, -0.017433002591133118, -0.4448721408843994, -0.02223210036754608, -0.3010571300983429, 0.15309888124465942, 0.8185709714889526, -0.42753031849861145, 1.2041645050048828, -0.23138292133808136, 0.7889202833175659, -0.29541900753974915, -0.933182954788208, 0.3065497875213623, 0.27297377586364746, -0.0013843289343640208, -0.5851890444755554, 0.31871497631073, 0.25590652227401733, -0.3204628527164459, 0.12901726365089417, 0.7431185245513916, 0.8726305365562439, -0.26353156566619873, -0.024350423365831375, 0.36695554852485657, -0.0438552051782608, 0.15103574097156525, 0.3372255861759186, 0.5195359587669373, 0.36932218074798584, 0.38290324807167053, -0.07323165982961655, 0.28078243136405945, -1.148671269416809, 0.08685958385467529, 0.7449966669082642, 0.5738057494163513, 1.0169700384140015, 0.3688952624797821, -0.6131674647331238, -0.2793932557106018, -0.10262201726436615, 0.6269509792327881, 1.4478965997695923, 0.10018350929021835, -0.26211363077163696, -0.4504731595516205, -0.054064273834228516, -0.21190185844898224, 0.25293582677841187, -0.6098899245262146, -0.5208578109741211, -0.5051377415657043, -0.9548250436782837, 1.072587013244629, 0.7277274131774902, 0.7644266486167908, -0.6609083414077759, -0.1830889880657196, -0.25425809621810913, 0.10453741252422333, -0.6882725358009338, -0.7608568668365479, 0.43427813053131104, -0.1770104318857193, 0.009683557786047459, 0.14277394115924835, 0.04187985137104988, -0.0009551562834531069, -0.9071835279464722, 0.5854017734527588, -0.9331216812133789, 0.028082845732569695, -0.11076255142688751, 0.6057823300361633, -0.44171398878097534, -0.47110670804977417, 0.2318817377090454, 0.026696855202317238, 0.03829258680343628, 0.35063380002975464, 0.22533154487609863, -0.17140455543994904, -0.04696798697113991, -0.20788000524044037, -0.07808399945497513, 0.21976348757743835, -0.02031785063445568, 0.5896987915039062, -0.1856696903705597, 0.10356330126523972, -0.901156485080719, 0.7976663112640381, 0.11658517271280289, -0.13080844283103943, 0.22248980402946472, -0.3721974194049835, -0.04772254452109337, 0.6106175780296326, -0.7093250751495361, -0.160733163356781, -0.8205777406692505, 0.07478237897157669, -0.5631064176559448, -0.023484906181693077, 0.14388877153396606, 0.023012056946754456, 0.32281041145324707, -0.02430562488734722, 0.6520270705223083, 0.08386995643377304, -0.18674886226654053, 0.4471541941165924, -0.6160423159599304, 0.7929216623306274, 0.36177515983581543, -0.05912841483950615, -0.39838939905166626, -0.21540077030658722, -0.45582303404808044, -0.6060474514961243, -0.4830401837825775, 0.09448938816785812, -0.1793394237756729, -0.04867930710315704, -0.11302001029253006, -1.5381488800048828, 0.0934959128499031, -1.0327427387237549, -0.25401365756988525, -0.29072949290275574, -0.13257895410060883, -0.11775264143943787, -1.0400269031524658, -1.2966164350509644, -0.8558779954910278, -0.25360703468322754, -0.7618207335472107, 0.026886483654379845, 0.08396652340888977, -0.4953272044658661, -0.5330180525779724, -0.2523978650569916, -0.29459279775619507, 1.3867405652999878, -0.8290576934814453, 0.5699640512466431, -0.3253776431083679, -0.14420348405838013, -0.45467451214790344, 0.43551748991012573, 0.7258816957473755, 0.2395106852054596, 0.4862470328807831, -0.9491122961044312, 0.7115181684494019, -0.15660932660102844, -0.08069684356451035, 0.12431572377681732, 0.5874861478805542, 1.026425838470459, -0.1546487957239151, -0.2649795413017273, 0.25272539258003235, 1.155957818031311, -0.22124725580215454, 0.335449755191803, 0.35446760058403015, 1.2121893167495728, 0.26831936836242676, -0.07943358272314072, 0.44414040446281433, 0.426455557346344, 0.3485018312931061, 0.40000537037849426, 0.00721863005310297, 0.07655002176761627, -0.6398650407791138, 0.5204539895057678, 1.6500918865203857, 0.0353008434176445, -0.000517229491379112, -0.7775319218635559, 0.8251641392707825, -1.4281930923461914, -1.2621155977249146, 0.8310865163803101, 0.8288209438323975, 0.3786056935787201, -0.3284461200237274, -0.391595721244812, 0.05772509425878525, 0.7258273959159851, 0.6085404753684998, -0.05679371580481529, -0.799430787563324, -0.14896954596042633, 0.3692868649959564, 0.3118235170841217, 0.8976126313209534, -0.3375701308250427, 0.7924527525901794, 14.883697509765625, 0.9785634279251099, -0.11854484677314758, 0.3851241171360016, 0.7182642817497253, 0.20272871851921082, -0.3760070502758026, -0.4425846040248871, -1.1951981782913208, 0.5643773078918457, 1.1709197759628296, 0.22457733750343323, 0.4753767251968384, 0.22297246754169464, -0.016759011894464493, 0.23929257690906525, -0.7211573123931885, 0.9788300395011902, 0.37295177578926086, -1.194699764251709, -0.028874801471829414, 0.021807998418807983, -0.13079166412353516, 0.35503673553466797, 1.141842246055603, 0.6694472432136536, 0.7433236837387085, -0.8123486638069153, 0.8126305341720581, 0.43567904829978943, 0.6842206120491028, -0.2994583249092102, 0.5328725576400757, 0.28077083826065063, -0.9454041719436646, -0.1182553619146347, -0.8106395602226257, -1.0802980661392212, 0.06664154678583145, 0.12686721980571747, -0.15500441193580627, -0.6126559972763062, -0.04100007563829422, 0.824870765209198, 0.3087395429611206, 0.4817349910736084, -0.4064675271511078, 0.49062857031822205, -0.2199689745903015, -0.09926170855760574, 0.40976566076278687, 0.5610464215278625, 0.09907940030097961, -0.11972783505916595, 0.2641860544681549, -0.01900600641965866, -0.22543904185295105, 0.644852340221405, -0.5433717370033264, -0.32598423957824707, -0.15780650079250336, -0.25529947876930237, 0.39113548398017883, 0.7307612299919128, 0.7185335755348206, 0.36653029918670654, -0.011067749001085758, 0.22479920089244843, 0.5978310704231262, -0.005077235866338015, -0.5456835627555847, -0.495745986700058, 0.5769532918930054, -0.4407227337360382, 0.08486758917570114, 1.002084493637085, -0.22717958688735962, -0.26177331805229187, -0.5936611890792847, -0.09758840501308441, 0.1872434914112091, -0.9988412857055664, -0.6964005827903748, 1.0382839441299438, -0.46909964084625244, -0.17700251936912537, 0.12941645085811615, -0.7030189633369446, -0.2300083190202713, 0.3848002254962921, -1.4021326303482056, -0.3847166895866394, 0.34351345896720886, -0.25940555334091187, -0.42708471417427063, 0.11815807968378067, 0.7873244881629944, 0.29312872886657715, -0.4230504035949707, 0.1967502236366272, 0.1296638697385788, 0.19388382136821747, -0.26847660541534424, -1.245793104171753, 0.6510779857635498, 0.27521276473999023, -0.14834178984165192, 0.45931777358055115, 0.06006716191768646, 0.6880852580070496, -0.38551148772239685, 0.20591676235198975, 0.896519660949707, -0.9078778624534607, -0.3724540174007416, -0.6239644885063171, -0.8393426537513733, 0.45474544167518616, 0.4632534682750702, 0.07037615776062012, 0.17457056045532227, 0.08582053333520889, -0.4915623962879181, -0.6086043119430542, -0.5248914361000061, -0.11503623425960541, 0.4989401400089264, -0.8924605846405029, -0.6684200763702393, -0.33748966455459595, 0.13086296617984772, -0.8913265466690063, -0.74882572889328, -0.31807228922843933, 0.30546680092811584, -0.07456715404987335, 1.2257307767868042, -0.45428353548049927, 0.7400057315826416, 0.6902705430984497, -0.28504079580307007, -0.9179341197013855, -0.3518783152103424, -1.0351216793060303, 0.02678925171494484, 0.30006808042526245, 0.6521278023719788, -0.5759705901145935, 0.3427266776561737, 0.9628472328186035, 0.13139410316944122, -0.6092641949653625, -0.6482420563697815, -0.4926270842552185, -0.29202690720558167, -0.5021714568138123, 0.49641287326812744, 0.16570040583610535, 0.2949461340904236, -0.03133714199066162, 0.2710318863391876, 0.39944255352020264, -0.4254249334335327, -0.9292436242103577, -0.47309648990631104, -0.04963028430938721, -0.1023150160908699, -0.8680978417396545, -0.6879841685295105, -1.7432732582092285, 0.07739941775798798, -1.2050596475601196, 0.2708490788936615, -1.1351631879806519, -0.5960640907287598, -0.11236273497343063, -0.9202559590339661, 0.2616913616657257, 0.126535564661026, -0.6127150654792786, -0.3879723846912384, -0.637129008769989, -0.52046138048172, 0.8739011287689209, 0.797387957572937, -0.833487331867218, 0.3152206838130951, 0.16383309662342072, -0.32077834010124207, 0.191337451338768, 0.4759702682495117, -0.7242077589035034, -0.7694872617721558, -1.288682222366333, 0.5787581205368042, -0.0022848269436508417, -0.14867709577083588, -0.5819931030273438, 1.0268299579620361, 0.47489821910858154, -0.29818782210350037, 0.16055253148078918, 0.7922199368476868, -0.9824391603469849, -0.5348982214927673, 0.14309732615947723, -1.1993430852890015, 0.33797183632850647, -0.34730851650238037, -0.5351782441139221, -0.6782827973365784, 0.5217188596725464, -0.010473378002643585, -0.9766154885292053, -0.6883371472358704, 0.5537319779396057, -0.6601713299751282, 0.1039949357509613, -0.16096533834934235, -0.601435124874115, -1.1727957725524902, -0.4489983916282654, -0.07499685883522034, 0.15913602709770203, -0.6566986441612244, 0.8547684550285339, 0.677129328250885, -1.1414358615875244, 0.09529610723257065, 0.17186985909938812, -0.10817133635282516, 0.07865337282419205, 0.313127338886261, 0.4339427351951599, -0.346810519695282, 0.48321640491485596, 0.1515740156173706, 0.3189188539981842, -0.8163829445838928, -0.005547621753066778, 1.134603500366211, -0.190156027674675, -0.07575871795415878, 1.524430513381958, -0.41408419609069824, -1.1453986167907715, 0.3991803228855133, -1.2299416065216064, -0.518395185470581, -0.4137282967567444, 0.44994691014289856, 0.4086415469646454, -0.35361963510513306, -0.2665911614894867, -0.6074535250663757, 0.1875368356704712, -0.016698665916919708, -0.3997017443180084, 0.4934501051902771, -0.015572460368275642, -0.34783557057380676, 0.7761238217353821, 1.1675705909729004, -0.6775354146957397, -0.6864661574363708, -0.665067195892334, -0.20881274342536926, -0.4389432370662689, 0.1617545634508133, 0.021502399817109108, -1.0266685485839844, 1.0419528484344482, 0.19587838649749756, 0.4352228343486786, -0.17171843349933624, 0.041759196668863297, -0.013357591815292835, 0.6106486320495605, -0.2753690183162689, -0.577355146408081, -0.5877264142036438, 1.2042707204818726, 0.9574121236801147, -0.5205872654914856, -0.12994806468486786, -0.3531741201877594, -0.7308166027069092, 0.7113941311836243, 0.4815825819969177, 0.025880398228764534, 0.6646941900253296, 0.07924812287092209, -0.09163768589496613, 0.3035047650337219, -0.9326866865158081, -0.20168210566043854, 0.49992308020591736, 1.155748724937439, 0.6105010509490967, 0.15396563708782196, 0.639714241027832, 0.7972673177719116, -0.08089535683393478, -0.16968250274658203, 0.37812086939811707, 0.20504805445671082, -0.24923951923847198, 0.04199305549263954, 0.32928550243377686, 0.6625855565071106, -0.8609343767166138, -0.7805029153823853, 0.3505074381828308, 0.5249013900756836, -0.030608344823122025, 0.5449472665786743, 0.9222581386566162, 0.08813347667455673, 0.29880180954933167, -0.021949276328086853, 0.6461137533187866, -0.23718860745429993, -0.45990875363349915, -0.6403467059135437, -0.6103052496910095, -0.12463425099849701, -0.2470952570438385, -0.40994325280189514, -0.5235690474510193, -0.34237268567085266, 0.204344242811203, 0.22133664786815643, 0.103483647108078, 0.9517845511436462, 0.33258265256881714, 0.9098544120788574, -0.1844405084848404, -0.38052868843078613, -0.4273625612258911, -0.8561151027679443, -0.21176788210868835, -0.4258773624897003, -0.04773186892271042, 0.07654894143342972, -0.12727802991867065, -0.5418439507484436]}, "authors": [{"authorId": "2108801920", "name": "Shengjie Luo"}, {"authorId": "2119028865", "name": "Shanda Li"}, {"authorId": "150311931", "name": "Shuxin Zheng"}, {"authorId": "2110264337", "name": "Tie-Yan Liu"}, {"authorId": "39060743", "name": "Liwei Wang"}, {"authorId": "1391126980", "name": "Di He"}], "references": [{"paperId": "407238453abd02a9edf48032eaa19819d6d150ff", "title": "Benchmarking Graphormer on Large-Scale Molecular Modeling Datasets"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f", "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"}, {"paperId": "6d2d6dc9fb6b5a440b2d1a1d69ae0214946a28ed", "title": "First Place Solution of KDD Cup 2021 & OGB Large-Scale Challenge Graph Prediction Track"}, {"paperId": "5863d7b35ea317c19f707376978ef1cc53e3534c", "title": "Rethinking Graph Transformers with Spectral Attention"}, {"paperId": "94576783bc73bf55a0091203a3d45a0a4665a1ae", "title": "Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding"}, {"paperId": "9389af659f14239319186dff1cef49e8ece742c8", "title": "OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "849b88ddc8f8cabc6d4246479b275a1ee65d0647", "title": "A Generalization of Transformer Networks to Graphs"}, {"paperId": "b62edbf6e619eeed886c63e51fdff2c3d94f998f", "title": "Graph convolutions that can finally model local structure"}, {"paperId": "f1e5e65941617604923225cc4bf464e370fcae67", "title": "Combining Label Propagation and Simple Models Out-performs Graph Neural Networks"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "467a90e1315ad10be812e420f89551d818b15a83", "title": "Hierarchical Inter-Message Passing for Learning on Molecular Graphs"}, {"paperId": "82b5ee0ae468cd2e0b62df96f42b5b5480c75510", "title": "Infinite attention: NNGP and NTK for deep attention networks"}, {"paperId": "965652c0e426c5b42d7218d7429025be7ac542bf", "title": "DeeperGCN: All You Need to Train Deeper GCNs"}, {"paperId": "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "993377a3fc8334558463b82053904e3d684f29c0", "title": "SIGN: Scalable Inception Graph Neural Networks"}, {"paperId": "19c0d004bd0e42a6449d8b7717cbda4431a67e65", "title": "Principal Neighbourhood Aggregation for Graph Nets"}, {"paperId": "e8984c6e6c24aab26c332728a5fff616dfb3adbb", "title": "Learning to Encode Position for Transformer with Continuous Dynamical Model"}, {"paperId": "f64e1d6bc13aae99aab5449fc9ae742a9ba7761e", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "ea9a516d5cb0b298f0df50e82b3e0400b72fcdff", "title": "Microsoft Academic Graph: When experts are not enough"}, {"paperId": "d0e28f5dc1feae19e41087a92a87992977fd85af", "title": "Encoding word order in complex embeddings"}, {"paperId": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d", "title": "Are Transformers universal approximators of sequence-to-sequence functions?"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "bb713d56a39a040b35e4f9e036fb4422f543e614", "title": "On the Relationship between Self-Attention and Convolutional Layers"}, {"paperId": "37a23c43ddf09ea97b82b38e2827a2229cfae545", "title": "Novel positional encodings to enable tree-based transformers"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "7e71eedb078181873a56f2adcfef9dddaeb95602", "title": "Simplifying Graph Convolutional Networks"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9", "title": "How Powerful are Graph Neural Networks?"}, {"paperId": "fe9ea9160d3a357519765c3f7c1b829119f5a2d2", "title": "ResNet with one-neuron hidden layers is a Universal Approximator"}, {"paperId": "928f9dccb806a3278d20d82cc53781c5f44e2bb1", "title": "Constituency Parsing with a Self-Attentive Encoder"}, {"paperId": "69ac3b35887eb42e8fe554619fc7255e6e95a4cb", "title": "Fast Parametric Learning with Activation Memorization"}, {"paperId": "680aafd3d51e666b297e27b93d9554cc2caf1c4d", "title": "An Analysis of Neural Language Modeling at Multiple Scales"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "921196c32213a229245a9705ee4768bc941e7a26", "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"}, {"paperId": "a5fe578a6b9f51ce19263676e6395421fedc6d2d", "title": "Residual Gated Graph ConvNets"}, {"paperId": "5cba70a9b08ad6dc1ec06e269cff5e1dca34df14", "title": "Approximating Continuous Functions by ReLU Nets of Minimal Width"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "53bfec9b34e40000d9f2174c8ac6191087fe4d57", "title": "The Expressive Power of Neural Networks: A View from the Width"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6b7d6e6416343b2a122f8416e69059ce919026ef", "title": "Inductive Representation Learning on Large Graphs"}, {"paperId": "e24cdf73b3e7e590c2fe5ecac9ae8aa983801367", "title": "Neural Message Passing for Quantum Chemistry"}, {"paperId": "cd8a9914d50b0ac63315872530274d158d6aff09", "title": "Modeling Relational Data with Graph Convolutional Networks"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "f09f7888aa5aeaf88a2a44aea768d9a8747e97d2", "title": "Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs"}, {"paperId": "2d7782c225e0fc123d6e227f2cb253e58279ac73", "title": "Improving Neural Language Models with a Continuous Cache"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "36eff562f65125511b5dfab68ce7f7a943c27478", "title": "Semi-Supervised Classification with Graph Convolutional Networks"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "79d1b330f0ef51f63ecb9b291dd5a05de5a858c0", "title": "Toeplitz and Circulant Matrices: A Review"}, {"paperId": "d35f1e533b72370683d8fa2dabff5f0fc16490cc", "title": "Approximation capabilities of multilayer feedforward networks"}, {"paperId": "8da1dda34ecc96263102181448c94ec7d645d085", "title": "Approximation by superpositions of a sigmoidal function"}, {"paperId": "386cbc45ceb59a7abb844b5078e5c944f17723b4", "title": "On the approximate realization of continuous mappings by neural networks"}, {"paperId": "f8d37fc9764f952e9f3fa5c7293c0f39b0c7821a", "title": "Real Analysis: Modern Techniques and Their Applications"}, {"paperId": "d08a0eb7024dff5c4fabd58144a38031633d4e1a", "title": "Benchmarking Graph Neural Networks"}, {"paperId": "acf87283fa8ae426f1a4987b345b401bf2913f61", "title": "Do Transformers Really Perform Badly for Graph Representation?"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "722c52711a8014694d68839f0ffc52ba8f7fc621", "title": "Approximation and estimation bounds for artificial neural networks"}, {"paperId": null, "title": "To demonstrate the power of our method and for fair comparison, we set the parameter budget of the model to be less than 500K following [15, 63"}, {"paperId": null, "title": "Graph Isomorphism Network [62] with Virtual Node"}, {"paperId": null, "title": "Following [67], we include results of several competitive baselines: (1) Graph Convolutional Network"}, {"paperId": null, "title": "Two representative Transformer-based models GraphTransformer (GT) [14] and Spectral Attention Network (SAN)"}, {"paperId": null, "title": "Label Propagation [28]; 3) Simplified Graph Convolution"}, {"paperId": null, "title": "Scalable Inception Graph Neural Network (SIGN) [52]; 5) MLP with Correct and Smooth Procedure (MLP+C&S)"}, {"paperId": null, "title": "Graph Isomorphism Network with Edge feature and Virtual Node (GINE-VN) [5, 19]; 6) Deeper Graph Convolutional Network (DeeperGCN)"}, {"paperId": null, "title": "c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}, {"paperId": null, "title": "GatedGCN-PE); 7) Message Passing Neural Network (MPNN(sum)) [19]; 8) Hierarchical Inter Message Passing (HIMP)"}, {"paperId": null, "title": "The PCQM4M dataset contains more than 3.8 million molecular graphs in total, which is currently the largest graph-level prediction dataset. The state-of-the-art architecture"}]}