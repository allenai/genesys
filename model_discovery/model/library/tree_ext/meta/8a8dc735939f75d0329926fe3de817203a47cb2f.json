{"paperId": "8a8dc735939f75d0329926fe3de817203a47cb2f", "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs", "abstract": "State-of-the-art large language models (LLMs) have become indispensable tools for various tasks. However, training LLMs to serve as effective assistants for humans requires careful consideration. A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework. In this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals, dedicating substantial focus to the core component of RLHF -- the reward model. Our study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. We characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.", "venue": "arXiv.org", "year": 2024, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward and improves the understanding of the role of reward models and methods for their training."}, "embedding": {"model": "specter_v2", "vector": [0.18281954526901245, 0.8148596882820129, -0.5494356155395508, -0.11175795644521713, -0.4267933964729309, 0.09274078905582428, 0.41144829988479614, -0.09690359234809875, -0.6425818204879761, 0.5961472988128662, 0.1451749950647354, -0.07685438543558121, 0.42073994874954224, 0.32225075364112854, -0.7246372699737549, 0.11880767345428467, -1.05382239818573, -0.05876631289720535, -0.5199717879295349, -0.4597005546092987, -0.401269793510437, -0.7226327657699585, -1.02202570438385, 0.06596294045448303, 0.5207251310348511, 0.18212087452411652, 0.3056197762489319, 1.217342495918274, 0.0002872978802770376, 0.9333576560020447, 0.453595370054245, -0.11047396808862686, 0.348951131105423, -0.14116796851158142, -0.36089563369750977, 0.1310911923646927, -0.06840114295482635, -0.9279422760009766, -0.6028261780738831, 0.6351666450500488, -0.19443483650684357, 0.3088743984699249, 0.42247503995895386, -0.4243389666080475, -0.4566291272640228, 0.5777556300163269, 0.6024772524833679, 0.4760909676551819, 0.3857197165489197, -0.31769582629203796, 1.462684154510498, -1.2094244956970215, 0.16010645031929016, 1.5036771297454834, 0.22473135590553284, 0.5333672761917114, -0.3998560607433319, -0.5874159932136536, 0.7998988628387451, -0.5175886154174805, -0.49173083901405334, -0.020083900541067123, -0.5117196440696716, -0.020691905170679092, 1.2566577196121216, -0.4176163971424103, -0.08669275045394897, 0.8738426566123962, -0.1594032645225525, 1.654289960861206, 0.26503807306289673, -0.8445636630058289, -0.1041470393538475, 0.5995018482208252, 0.3300173580646515, 1.0162241458892822, -0.6994979977607727, 0.9478011727333069, -0.8294923901557922, -0.48292553424835205, 0.523655891418457, -0.2013188749551773, -0.3789839446544647, -0.3277010917663574, 0.015147019177675247, 0.9517540335655212, 0.32432258129119873, 0.5570334792137146, -0.3714107573032379, 0.807485818862915, 0.3632190227508545, 0.8544052839279175, 0.08122216910123825, 0.3779522478580475, -0.10588479787111282, 0.058249399065971375, -0.19841906428337097, 0.4744606912136078, -0.1552966833114624, 0.6636614203453064, -0.08787135779857635, 0.2405354082584381, -1.1258561611175537, 0.40903210639953613, 1.4938695430755615, -0.06468968838453293, 0.44868403673171997, -0.9544573426246643, 0.3247308135032654, -1.001596212387085, 0.9627955555915833, -0.3373640477657318, -0.17534001171588898, -0.2557726502418518, -0.4378591477870941, -1.0143892765045166, -0.18467330932617188, 0.09884181618690491, -0.35088732838630676, 1.0750988721847534, -0.47649556398391724, -0.5682716369628906, 0.05002470314502716, 0.8429203629493713, 0.14258724451065063, 0.6013205051422119, 0.23584675788879395, -0.27903178334236145, 0.13881514966487885, -0.6196779012680054, -0.723095715045929, -0.8993176221847534, 0.9340560436248779, 0.17417070269584656, 0.7700752019882202, 0.007724843919277191, -1.1137100458145142, -1.0393860340118408, -1.2144248485565186, 0.39769822359085083, 0.18055716156959534, 0.3721061646938324, 1.4195353984832764, 0.5104830265045166, -0.988914430141449, 0.9918562173843384, -0.0073892115615308285, 0.2725052237510681, 0.13368497788906097, 0.4417807161808014, 0.27702176570892334, -0.8616482019424438, -1.3728126287460327, 0.44673603773117065, 0.0858549252152443, -0.6800183057785034, -0.5299571752548218, -0.15286321938037872, -1.4949246644973755, -0.4486082196235657, 0.17676423490047455, -0.3272227942943573, 1.7646682262420654, -0.5706416964530945, -1.285689353942871, 0.22044634819030762, -0.08682507276535034, -0.06808964163064957, 0.764365017414093, -0.3178216218948364, -0.7281023263931274, -0.3072882890701294, -0.33797258138656616, 0.5167623162269592, 0.3135669529438019, -0.5852914452552795, -0.07738987356424332, 0.3339107036590576, -0.007694321218878031, -0.1617216020822525, -0.3168918192386627, 0.5482280254364014, -0.01317564770579338, -0.27227115631103516, -0.3419209420681, 0.6473062038421631, -0.3139452338218689, 0.006905282381922007, -0.06029517576098442, -0.9750979542732239, 0.4229283630847931, -0.14596831798553467, 1.1025171279907227, -0.7271672487258911, -1.0989055633544922, 0.09795427322387695, -0.12719382345676422, -0.10428236424922943, -0.8385135531425476, 0.369274377822876, -0.09049223363399506, 0.11189903318881989, -0.4963056743144989, -0.9669279456138611, -0.07707885652780533, -0.12483765929937363, -0.33837494254112244, 0.12448519468307495, 0.0031024166382849216, 0.6024033427238464, -1.5453799962997437, 0.1599626988172531, -0.08777463436126709, 0.2742783725261688, -0.9350520968437195, 1.6703084707260132, -0.46372872591018677, 0.5181134939193726, -0.259071409702301, -0.6475911736488342, 0.0315638929605484, -0.3228890895843506, 0.2795882225036621, 0.2726987600326538, 0.27016735076904297, 0.30099207162857056, -0.25647568702697754, 1.4983932971954346, -0.47318440675735474, 0.3905419409275055, -0.1404489129781723, -0.4599337875843048, 0.2717462182044983, 0.7738965749740601, -0.1316021978855133, -0.5058125853538513, 0.17620855569839478, 0.5509657263755798, -0.6581483483314514, 0.03291858732700348, 0.5500210523605347, 0.6631081700325012, -0.10075025260448456, 0.646687388420105, 0.37932994961738586, -0.012082992121577263, 0.4826633334159851, 0.33774685859680176, 0.7276220321655273, 0.5617615580558777, 0.6368931531906128, 0.03567611426115036, 0.3736761212348938, -1.2557578086853027, -0.34191086888313293, 0.9350411295890808, 0.5734047889709473, 0.6870238780975342, -0.1854448914527893, -0.9634774327278137, 0.043453481048345566, -0.235260471701622, 0.7458593249320984, 1.2613581418991089, -0.303779274225235, -0.15029101073741913, -0.6224470138549805, -0.7245059013366699, -0.0875234380364418, 0.5433791279792786, -0.5586240291595459, -0.23116037249565125, -0.43614694476127625, -0.9995856285095215, 0.5525376796722412, 0.1927751898765564, 0.5560414791107178, -0.7328511476516724, -0.22361987829208374, -0.09130637347698212, 0.38761261105537415, -0.3972215950489044, -1.0086700916290283, 0.23643264174461365, -0.34637919068336487, -0.025227637961506844, -0.5269970297813416, 0.21066416800022125, 0.22318772971630096, -0.5819766521453857, 0.8469979166984558, -0.08190536499023438, 0.007340396288782358, 0.4584804177284241, 0.6603464484214783, -0.41374799609184265, -1.120698094367981, -0.39296501874923706, 0.3915916979312897, -0.20001277327537537, -0.2974766492843628, 0.6885794401168823, 0.16669593751430511, -0.00751894898712635, -0.26955679059028625, -0.11324980109930038, 0.318837434053421, 0.24051450192928314, 0.09430522471666336, -0.6089296936988831, 0.44066116213798523, -1.2620104551315308, 1.5011897087097168, -0.09272985905408859, -0.18739773333072662, 0.23764966428279877, -1.0817289352416992, -0.033031851053237915, 0.63193678855896, -0.6850778460502625, -0.40999332070350647, -0.7519384622573853, 0.3972116708755493, 0.01870492473244667, -0.0223422572016716, 0.015584958717226982, 0.6342178583145142, 0.14579042792320251, 0.5836694240570068, 0.3781348168849945, 0.6126058101654053, 0.06777678430080414, 0.5822122097015381, -0.4256958067417145, 0.29876646399497986, -0.14450500905513763, -0.18949083983898163, -0.43964558839797974, -0.6617008447647095, -0.6796399354934692, -0.4832533001899719, 0.10426869243383408, 0.4434475898742676, -0.5890762805938721, -0.24748431146144867, -0.8051591515541077, -0.8620498776435852, -0.40597259998321533, -1.0805668830871582, -0.27867433428764343, 0.05810137465596199, 0.20124688744544983, -0.5651866793632507, -0.9195018410682678, -1.166785478591919, -1.1520742177963257, -0.2503933310508728, -1.2780277729034424, -0.0905204713344574, -0.1529574990272522, -0.6821885108947754, -0.3326801359653473, 0.09965882450342178, -0.3562094271183014, 0.6524506211280823, -1.1666438579559326, 0.7706022262573242, 0.41688618063926697, -0.15617501735687256, -0.09318063408136368, 0.49724629521369934, 0.6518983244895935, -0.08354397118091583, -0.286973774433136, -1.230252742767334, -0.37251564860343933, -0.18227055668830872, -0.9741536974906921, -0.20943094789981842, 0.22500959038734436, 0.8156076669692993, -0.2640745937824249, -0.052841078490018845, -0.05697721242904663, 0.7749006748199463, -0.5220271348953247, -0.23915182054042816, 0.12363048642873764, 0.7923920154571533, 0.3690941631793976, -0.02427516132593155, 0.7586460709571838, 0.35034438967704773, 0.4454806447029114, -0.1348254531621933, 0.002463779179379344, 0.15880028903484344, -0.9778786897659302, 1.0967190265655518, 0.8967337608337402, 0.30134743452072144, 0.15963943302631378, -0.5258568525314331, 0.0952039361000061, -1.505977988243103, -0.35549551248550415, 0.7518773078918457, 1.2801413536071777, 0.6242515444755554, -0.2696216404438019, 0.09363064914941788, -0.05553553253412247, 0.37619075179100037, 0.19343559443950653, -0.25394123792648315, -0.7189165353775024, 0.2470664083957672, -0.013578925281763077, 0.09615509957075119, 1.0506881475448608, -0.155300572514534, 0.07045335322618484, 14.885058403015137, 0.5398770570755005, 0.16354140639305115, 0.547106921672821, 0.9513915181159973, 0.29082590341567993, -0.7112602591514587, -0.47317492961883545, -0.6208670735359192, -0.17899706959724426, 1.2105661630630493, 0.22333993017673492, 1.1656701564788818, -0.0339193157851696, 0.23752152919769287, -0.018492955714464188, -0.5148494243621826, 0.7926878929138184, 0.26625579595565796, -0.9922833442687988, 0.5692514777183533, -0.07613319903612137, 0.3694516718387604, 0.4295494556427002, 0.6530700325965881, 1.0380054712295532, 0.7045212388038635, -0.5858046412467957, 0.9165782332420349, 0.2401103675365448, 0.9744430184364319, -0.22917689383029938, 0.15209919214248657, 0.9808545708656311, -0.5646311044692993, -0.37419113516807556, -0.2505570948123932, -1.349871277809143, 0.055330246686935425, -0.0575985424220562, -0.8302115797996521, -0.5784226059913635, -0.42583969235420227, 0.49936679005622864, 0.15994632244110107, -0.1271735578775406, -0.29599371552467346, 0.6761868000030518, 0.1009254902601242, -0.04523846507072449, 0.13665606081485748, 0.38577985763549805, 0.10596036165952682, -0.025447623804211617, -0.087314672768116, -0.27272120118141174, 0.3451002240180969, 0.5814870595932007, -0.533315896987915, -0.015854230150580406, -0.6083135604858398, -0.12131614983081818, -0.046320922672748566, 0.17219652235507965, 1.1485168933868408, 0.4809485673904419, -0.3923075497150421, 0.14671465754508972, 0.8977257609367371, 0.5521888732910156, 0.28787466883659363, 0.4099429249763489, 0.21079081296920776, -0.4452844560146332, -0.49280643463134766, 0.76076740026474, 0.005866532679647207, -0.3105969727039337, -0.6617468595504761, -0.6841614842414856, 0.07253210991621017, -0.8028386831283569, -0.7443580627441406, 0.5388445258140564, -0.23121750354766846, -0.45132842659950256, -0.062413591891527176, -0.2570076882839203, 0.3097255229949951, 0.5191007852554321, -1.1236979961395264, -0.39905792474746704, 0.8019920587539673, -0.28499263525009155, -0.5878207683563232, -0.17455609142780304, 1.4228973388671875, 0.034639276564121246, -0.03727513179183006, 0.43060797452926636, 0.12582537531852722, -0.44476649165153503, -0.04942116513848305, -0.8632938861846924, 0.5926958322525024, -0.21347858011722565, -0.23451289534568787, 0.45154502987861633, 0.2646205723285675, 0.22840900719165802, -0.5441362857818604, -0.012607394717633724, 0.531898021697998, -0.72657710313797, -0.6868477463722229, -0.5120433568954468, -0.7371155619621277, 0.17297157645225525, 0.2136049121618271, -0.1818777471780777, 0.21878720819950104, -0.007628259249031544, -0.3732098639011383, 0.0645727813243866, -0.9573959708213806, 0.1323617547750473, 0.36682796478271484, -0.6957552433013916, -0.016391610726714134, -0.12392456084489822, 0.2838766574859619, -0.8663375377655029, -0.0956234335899353, -0.32250943779945374, 0.060622688382864, -0.21902596950531006, 0.7200751304626465, -0.6766964793205261, 0.3422081768512726, 0.4609432816505432, -0.02729024924337864, -1.1321130990982056, -0.10999699681997299, -1.3501951694488525, -0.07912186533212662, -0.22775918245315552, 0.8286768198013306, -0.2984154224395752, 0.13973000645637512, 0.8115220665931702, 0.33673563599586487, -0.3211585581302643, -0.6323122978210449, -0.22254003584384918, -0.0694095641374588, -0.6668375134468079, 0.0972408875823021, -0.3709714710712433, 0.036824870854616165, 0.31010332703590393, -0.24564583599567413, 0.5116417407989502, -0.08661561459302902, -1.1015206575393677, 0.3900747001171112, 0.10610511898994446, -0.4749835133552551, -0.4125172197818756, 0.2291613668203354, -1.6211860179901123, -0.11968500167131424, -1.2669953107833862, 0.29396674036979675, -0.700092613697052, -0.43534010648727417, 0.12887081503868103, -0.2821884751319885, -0.35659652948379517, 0.2134421020746231, -0.9436090588569641, -0.21087875962257385, -0.3244779706001282, -0.51468825340271, 1.1878468990325928, 1.3840017318725586, -0.6678264737129211, 0.09165956825017929, 0.13287635147571564, -0.07725922763347626, 0.12171217799186707, 0.3456467092037201, -0.60712730884552, -1.0686711072921753, -1.00843346118927, 0.3631950914859772, 0.1528453677892685, -0.02144700288772583, -0.36634284257888794, 0.13923698663711548, 0.28493958711624146, 0.0804516151547432, 0.6404068470001221, 0.28423625230789185, -0.9186168313026428, -0.537014365196228, 0.546460747718811, -1.2851626873016357, 0.2522522807121277, -0.17844903469085693, -0.07255371659994125, -0.11666359007358551, 0.5456383228302002, 0.11851833760738373, -1.0902409553527832, -0.047829519957304, 0.47507724165916443, -0.7186457514762878, 0.10162287205457687, -0.33817705512046814, 0.14619415998458862, -0.7827303409576416, -0.7825830578804016, 0.009371386840939522, 0.43029147386550903, -0.42628395557403564, 0.8540216684341431, 0.7278369069099426, -1.290116548538208, -0.04535355046391487, 0.49351009726524353, 0.14591573178768158, -0.10784231871366501, 0.298869788646698, 0.10526493191719055, -0.05806785821914673, 0.35470694303512573, 0.5025482177734375, 0.6421805024147034, -0.6691803932189941, -0.2396906018257141, 0.9246779084205627, -0.6945196986198425, -0.1889982670545578, 1.0482349395751953, -0.16733700037002563, -1.5145337581634521, 0.5357551574707031, -0.8460436463356018, -0.38946354389190674, -0.9299942255020142, 0.617851972579956, 0.1710778921842575, -0.19526374340057373, 0.2732306718826294, -0.09640637040138245, -0.3802075684070587, -0.33779269456863403, -0.7036231160163879, 0.3022285997867584, -0.23494596779346466, 0.20953050255775452, 0.8769404292106628, 0.8990166187286377, -0.548892617225647, -1.0888423919677734, -0.2919997572898865, -0.23509761691093445, -0.2665179967880249, 0.27890923619270325, -0.6868558526039124, -0.48781055212020874, 0.6534512042999268, 0.9076690077781677, -0.1655173897743225, -0.294193297624588, -0.22611822187900543, -0.05537337437272072, 0.7072596549987793, 0.44617652893066406, -0.7732189893722534, -0.47379669547080994, 1.0320570468902588, 1.5494636297225952, -1.0579067468643188, 0.3050977289676666, 0.10720027983188629, -0.9055385589599609, 0.717042088508606, 0.5607026815414429, 0.10278506577014923, 0.7363153696060181, -0.5319024324417114, 0.30376136302948, 0.21904778480529785, -0.8870981335639954, 0.20968089997768402, 0.6150642037391663, 0.8965195417404175, 1.288671612739563, 0.777395486831665, -0.12580417096614838, 0.7792012095451355, 0.09755881875753403, 1.0059932470321655, 0.767634391784668, 0.7085400819778442, -0.18261203169822693, -0.33663150668144226, 0.13433367013931274, 0.6847129464149475, -0.302038311958313, -0.37777847051620483, 0.3321143388748169, 0.22022487223148346, 0.1174408495426178, 0.5752806663513184, 0.022062884643673897, 0.011419585905969143, 0.4820098578929901, -0.2301182895898819, 0.7353900671005249, -0.4310576915740967, -0.39431044459342957, -0.2911345064640045, -0.3499978482723236, -0.16395992040634155, -0.15471485257148743, -0.4833425283432007, -0.5867111086845398, 0.08690911531448364, 0.5105955600738525, 0.1705188900232315, -0.2531950771808624, 1.1753593683242798, 0.3839675486087799, 0.15981200337409973, 0.13459858298301697, -0.1637243628501892, -1.0101934671401978, -1.167902946472168, -0.2726047933101654, -0.594478964805603, 0.27910760045051575, -0.36565062403678894, -0.28620922565460205, -0.548765242099762]}, "authors": [{"authorId": "2275344575", "name": "Shreyas Chaudhari"}, {"authorId": "2114841965", "name": "Pranjal Aggarwal"}, {"authorId": "46258988", "name": "Vishvak Murahari"}, {"authorId": "2590556", "name": "Tanmay Rajpurohit"}, {"authorId": "51043791", "name": "A. Kalyan"}, {"authorId": "2135381714", "name": "Karthik Narasimhan"}, {"authorId": "33341943", "name": "A. Deshpande"}, {"authorId": "2296662415", "name": "Bruno Castro da Silva"}], "references": [{"paperId": "6f9dbae279fa0c3a90d12f3b0f271dc8e6274817", "title": "A Survey of Reinforcement Learning from Human Feedback"}, {"paperId": "1460d33f547ee40c560174dc0f6898f4802f4cf8", "title": "Calibrated Language Models Must Hallucinate"}, {"paperId": "1b0e3360b3341fc411a6c7841173a8d78ac2ab43", "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs"}, {"paperId": "f3460dc3ae5cfd41099d576a3bb77411e1fc2e3f", "title": "A General Theoretical Paradigm to Understand Learning from Human Preferences"}, {"paperId": "e5d0857feca845b474b89565d513ff599629851d", "title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model"}, {"paperId": "cb3968152f7d93f53d24b00279a90d5071ddc85a", "title": "Understanding the Effects of RLHF on LLM Generalisation and Diversity"}, {"paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"}, {"paperId": "59a2203ef6ea159bb41540bd282e29e80a8ad579", "title": "A Long Way to Go: Investigating Length Correlations in RLHF"}, {"paperId": "f279b73a1f6034be4261009f2810a01b9f2fb6e3", "title": "Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment"}, {"paperId": "c12db2e67d1fb289266faa5507ff112c9a062465", "title": "Efficient RLHF: Reducing the Memory Usage of PPO"}, {"paperId": "929305892d4ddae575a0fc23227a8139f7681632", "title": "Jailbroken: How Does LLM Safety Training Fail?"}, {"paperId": "6af986a2cab884fbd30ad6da2928dc19c12d83a7", "title": "InstructEval: Systematic Evaluation of Instruction Selection Methods"}, {"paperId": "534c58762e69d7afbcb0f6a7e53c07484f6d4891", "title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models"}, {"paperId": "0e2f8491b7af5f715c8ac7e3a7fd96494bd417d8", "title": "Rewarded soups: towards Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"}, {"paperId": "370e51386abb7b999728e08b74f0a77fbd064834", "title": "Fine-Tuning Language Models with Advantage-Induced Policy Alignment"}, {"paperId": "e2e52461194bc81351da7caa978ac42e9e9549cc", "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training"}, {"paperId": "be8db99310602d66bba64bcf41a572c45816fbfc", "title": "Let's Verify Step by Step"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "155aec5cff650263a4c71136f97570611d1bba7a", "title": "The Curse of Recursion: Training on Generated Data Makes Models Forget"}, {"paperId": "5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35", "title": "The False Promise of Imitating Proprietary LLMs"}, {"paperId": "04b07a44718555a24d98e5bf2120cb587b87b000", "title": "PruMUX: Augmenting Data Multiplexing with Model Compression"}, {"paperId": "86f163283adcab14fc7f2c021027b1139432b0af", "title": "Anthropomorphization of AI: Opportunities and Risks"}, {"paperId": "5b8f0460d408a8688d9ee0cba127c779d3291d99", "title": "Aligning Large Language Models through Synthetic Feedback"}, {"paperId": "2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4", "title": "Sources of Hallucination by Large Language Models on Inference Tasks"}, {"paperId": "d8c78221e4366d6a72a6b3e41e35b706cc45c01d", "title": "Training Diffusion Models with Reinforcement Learning"}, {"paperId": "627964b3dc7211f81902afeaa79d9357b7b2440c", "title": "Continually Improving Extractive QA via Human Feedback"}, {"paperId": "58af2d4fcca54c14334d1efd975554b4eb78cd4d", "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback"}, {"paperId": "9b4f7c97c0b83a80c32bc0b93595cbcfb4ecb16d", "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"}, {"paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156", "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"}, {"paperId": "74b05bba46db21e589a2cc0f916f81069b0368ef", "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation"}, {"paperId": "3ab661db57d924f4ff1706e05ac807873ca00e0a", "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment"}, {"paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae", "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"}, {"paperId": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28", "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models"}, {"paperId": "16d83e930a4dab2d49f5d276838ddce79df3f787", "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models"}, {"paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f", "title": "Self-Refine: Iterative Refinement with Self-Feedback"}, {"paperId": "83edcfbb206ddad38a971d605da09390604248ea", "title": "BloombergGPT: A Large Language Model for Finance"}, {"paperId": "c11810fa8887b678facea62da4607c4898360308", "title": "Training Language Models with Language Feedback at Scale"}, {"paperId": "28dafb94a87daa71ad3edf9f04f3c8c32753398b", "title": "Improving Code Generation by Training with Natural Language Feedback"}, {"paperId": "e9fd878dc28013edff217fafb04ed96e4f480a58", "title": "Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle"}, {"paperId": "0671fd553dd670a4e820553a974bc48040ba0819", "title": "Reflexion: language agents with verbal reinforcement learning"}, {"paperId": "6c2e438068e101d31af55add9f6ac6cac7159bc8", "title": "Practical and ethical challenges of large language models in education: A systematic scoping review"}, {"paperId": "c28af43206867cb5529164e3dd6d9ea8b7cfe7f2", "title": "Active Reward Learning from Multiple Teachers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "c23c2c8df918e4f327dbc27c9471a1485360933e", "title": "MUX-PLMs: Pre-training Language Models with Data Multiplexing"}, {"paperId": "c5120b546f1bd99df5bd2e2bf44db5c7c46d1545", "title": "Pretraining Language Models with Human Preferences"}, {"paperId": "d2170504c4ad9403bea118ae8debdfda95978546", "title": "The Wisdom of Hindsight Makes Language Models Better Instruction Followers"}, {"paperId": "cb3125e4f63f3d058a2a39270ecb585e86c3d1ff", "title": "Chain of Hindsight Aligns Language Models with Feedback"}, {"paperId": "993e28920d7a546472f43c3ac4339649d0b9c7d2", "title": "SemSup-XC: Semantic Supervision for Zero and Few-shot Extreme Classification"}, {"paperId": "4f16e5f4793cdf2a184ae9259ea0f0c04b889eb4", "title": "Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons"}, {"paperId": "db6f58b6aca4931afbd7d9760fea13cbc2c1953c", "title": "The Role of Baselines in Policy Gradient Optimization"}, {"paperId": "2cd72e71299c5d62d5cdb1164df5236172d418c4", "title": "Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits"}, {"paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "title": "Large language models encode clinical knowledge"}, {"paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654", "title": "Constitutional AI: Harmlessness from AI Feedback"}, {"paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e", "title": "Solving math word problems with process- and outcome-based feedback"}, {"paperId": "d40df59a1df8048a671bce60260b817e109a33f8", "title": "Reward Gaming in Conditional Text Generation"}, {"paperId": "a214eea51a026b048a52949042ef8afd7f3a1715", "title": "Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control"}, {"paperId": "fb3dc5e20e0a71134ca916f0d6d8d41f01225b4b", "title": "Scaling Laws for Reward Model Overoptimization"}, {"paperId": "2aab6ca1a8dae3f3db6d248231ac3fa4e222b30a", "title": "Re3: Generating Longer Stories With Recursive Reprompting and Revision"}, {"paperId": "912a39c2e0e4a35747531669cfa952d2c5627729", "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"}, {"paperId": "891edceb78a274b0c2494d8176bc4d6f6e3f9cbc", "title": "Calibrating Sequence likelihood Improves Conditional Language Generation"}, {"paperId": "74eae12620bd1c1393e268bddcb6f129a5025166", "title": "Improving alignment of dialogue agents via targeted human judgements"}, {"paperId": "eef33138ee3fbef970c74697b46acf60462d690c", "title": "The alignment problem from a deep learning perspective"}, {"paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7", "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"}, {"paperId": "609fd0ffd05e6730287fcc603267c4cabcbe3e5a", "title": "Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback"}, {"paperId": "017cfe49fce1cb77bda2e1791f7203f5ebac0ac7", "title": "Director: Generator-Classifiers For Supervised Language Modeling"}, {"paperId": "9f9b61e429e85e37d6df0e3c478a074f7e6cb9fc", "title": "Models of human preference for learning reward functions"}, {"paperId": "cf18a9f5a334e574f1d1f6ffdd64b6dac11fe9be", "title": "RL with KL penalties is better viewed as Bayesian inference"}, {"paperId": "aa4d9972af3264d032dbee58501ed4ac49477103", "title": "Scaling Laws and Interpretability of Learning from Repeated Data"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "0cceebb6b796c210b55ebf6b98032b5c631db0e1", "title": "Quality-Aware Decoding for Neural Machine Translation"}, {"paperId": "3a6a97a50695d43d95a015bbb554b2bc0d40394e", "title": "Don\u2019t Blame the Annotator: Bias Already Starts in the Annotation Instructions"}, {"paperId": "fd7c3c8fbe8cf88bd967ead02738b43081e306a7", "title": "Training Language Models with Language Feedback"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "65c5e3e3be6e881c7d2feef4c0c3d07323f3aeff", "title": "Make The Most of Prior Data: A Solution for Interactive Text Summarization with Preference Feedback"}, {"paperId": "7a74fa8620b8dd63a45a5eaece1d0a94ad37646d", "title": "Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "9e82736043eebe3f71eb86cbef6e2ac45306ece5", "title": "Structured Pruning Learns Compact and Accurate Models"}, {"paperId": "d90ea09d6bb1e17cb07c7e6edb325a755ed1a493", "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "8666f9f379389a5dff31e72fb0f992a37763ba41", "title": "Teaching language models to support answers with verified quotes"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "ac2906e0019b2d7621162413bb967e3915a4ab60", "title": "SemSup: Semantic Supervision for Simple and Scalable Zero-shot Generalization"}, {"paperId": "8179b8645596fd931c623db9ca3ed474a64917d0", "title": "Investigations of Performance and Bias in Human-AI Teamwork in Hiring"}, {"paperId": "a86d04a60bfdd196f217b2f4a01ef08a8c83896d", "title": "DataMUX: Data Multiplexing for Neural Networks"}, {"paperId": "00438218d81c2d50fc96592e16c07ae720440bb6", "title": "Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration"}, {"paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64", "title": "Red Teaming Language Models with Language Models"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e", "title": "A General Language Assistant as a Laboratory for Alignment"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "3bac792e35cbebcd1a84099b335790f8f2279766", "title": "Effects of dataset size and interactions on the prediction performance of logistic regression and deep learning models"}, {"paperId": "3a1501829ce7205f25939dd26e1089df920c9988", "title": "Reward is enough"}, {"paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156", "title": "Recursively Summarizing Books with Human Feedback"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "ef18db2a18ac61e72783a613328842ce86ef00bf", "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "a109b995dfeb444417f66545c67bce210bd11650", "title": "Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation"}, {"paperId": "048c6c8e89422749ad164d91160767ef738c27a4", "title": "Interactive Learning from Activity Description"}, {"paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1", "title": "Learning to summarize from human feedback"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "dbeeca8466e0c177ec67c60d529899232415ca87", "title": "On Faithfulness and Factuality in Abstractive Summarization"}, {"paperId": "4ae52766028e69186052ea8f33a137fbbbdb986a", "title": "BLEURT: Learning Robust Metrics for Text Generation"}, {"paperId": "f9ef88bfc78baeb24e697b05c307cf019f8a3630", "title": "Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f788ea49e7fbd55369fbfa0945c53dd40c9d3b", "title": "Deep Bayesian Reward Learning from Preferences"}, {"paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97", "title": "Deep double descent: where bigger models and more data hurt"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "c3172ea74996bc7d390a1bebdc53e373db903b1d", "title": "On the Weaknesses of Reinforcement Learning for Neural Machine Translation"}, {"paperId": "57daffd65a5d73a439903f3e50950c21c9eba687", "title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog"}, {"paperId": "eb011ccdf9ea739ea86be85b268a4d958266b624", "title": "Sample Efficient Text Summarization Using a Single Pre-Trained Transformer"}, {"paperId": "04babf1157c27c2d8abe24be5c45c6e99078daf0", "title": "Towards Coherent and Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators"}, {"paperId": "b0b96270a9bbeb9f3ec040e70114d565fbcaaed9", "title": "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!"}, {"paperId": "3e6cde685fdf321d7edf9319f7b07c01ff79c11a", "title": "Reward learning from human preferences and demonstrations in Atari"}, {"paperId": "9d4d8509f6da094a7c31e063f307e0e8592db27f", "title": "A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress"}, {"paperId": "15919637566348de8ca1e054151c24cc864b0f0e", "title": "Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning"}, {"paperId": "15a06d8601539b5eb6df5baf6bc4c3bdefb34855", "title": "Deep Reinforcement Learning for Sequence-to-Sequence Models"}, {"paperId": "669e6be7cd92ba6bda39d9e3a030e72fde07a418", "title": "Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback"}, {"paperId": "6e187ded899fb4c0d02b55a711f3e3522a49f50e", "title": "Toward Diverse Text Generation with Inverse Reinforcement Learning"}, {"paperId": "ca5409a7f6a4cd2a47862bc46b1d4f3117f3a070", "title": "Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems"}, {"paperId": "270e9b0681e895ae5adf937ea0cca9eb3718c721", "title": "Can Neural Machine Translation be Improved with User Feedback?"}, {"paperId": "baf47cd0b471a9bb7b2230fec0b680fc9b3c4783", "title": "Unified Pragmatic Models for Generating and Following Instructions"}, {"paperId": "36fd42195d46bdc0dc94327f66846505e979e25d", "title": "Deep Reinforcement Learning: A Brief Survey"}, {"paperId": "59094d64844ee21e32560fb08db6d53cc3af0c51", "title": "Inverse Reward Design"}, {"paperId": "a3be639d7e915b5f4e1499e52e1fcfd0940a31e5", "title": "Paraphrase Generation with Deep Reinforcement Learning"}, {"paperId": "94c5b38b6446e7dd8832f65c16bc7fad99031256", "title": "Learning Robot Objectives from Physical Human Interaction"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "429ed4c9845d0abd1f8204e1d7705919559bc2a2", "title": "Hindsight Experience Replay"}, {"paperId": "abcbfc9742e8f4825cfc536091fd414e08d03998", "title": "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6a8dbea5e40831bd6e987c03b76487f45ac49599", "title": "Deal or No Deal? End-to-End Learning of Negotiation Dialogues"}, {"paperId": "225ab689f41cef1dc18237ef5dab059a49950abf", "title": "Curiosity-Driven Exploration by Self-Supervised Prediction"}, {"paperId": "ae0e5ca59ae70df8ddb722a9c6bec6e4e6e2b743", "title": "Reward Shaping in Episodic Reinforcement Learning"}, {"paperId": "0dc964d025422fb0905b2065a5d09fd59b9e6e77", "title": "The limits of automatic summarisation according to ROUGE"}, {"paperId": "3d3b4ec7789f634e0752d50484dad7d2ea2460d5", "title": "Dialogue Learning With Human-In-The-Loop"}, {"paperId": "0d24a0695c9fc669e643bad51d4e14f056329dec", "title": "An Actor-Critic Algorithm for Sequence Prediction"}, {"paperId": "6e90fd78e8a3b98af3954aae5209703aa966603e", "title": "Unifying Count-Based Exploration and Intrinsic Motivation"}, {"paperId": "129cbad01be98ee88a930e31898cb76be79c41c1", "title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation"}, {"paperId": "69e76e16740ed69f4dc55361a3d319ac2f1293dd", "title": "Asynchronous Methods for Deep Reinforcement Learning"}, {"paperId": "a04697970bf7e584c279e2feb1496a1ffa374cb5", "title": "Bandit structured prediction for learning from partial feedback in statistical machine translation"}, {"paperId": "9f2a8e923965b23c11066a2ead79658208f1fae1", "title": "Minimum Risk Training for Neural Machine Translation"}, {"paperId": "b7aee9dfb027d6061c6a653684c0fa9a9bba750d", "title": "Sequence Level Training with Recurrent Neural Networks"}, {"paperId": "d1f037a0a46a7372fff7540d57d2d92d733151f1", "title": "Learning preferences for manipulation tasks from online coactive feedback"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a82795a372cec5266e7247054e9aace8f3b6a4f3", "title": "A Survey of Multi-Objective Sequential Decision-Making"}, {"paperId": "d94cf94459ccce18eb1205cec1bfb3f38108d717", "title": "Continuous Measurement Scales in Human Evaluation of Machine Translation"}, {"paperId": "1b758d72bf841c890afc3067bc323ffd31512cd7", "title": "Policy\u2010Gradient Algorithms"}, {"paperId": "58bd0afc8a1b98e16a67ebda436e60c6f6410f56", "title": "A Joint Model of Language and Perception for Grounded Attribute Learning"}, {"paperId": "c395728fc8673d8dd6d361bdde71b9d25260d38a", "title": "Ranking vs. Preference: A Comparative Study of Self-reporting"}, {"paperId": "5fc5c5a4e489e781de434567d946e6eb65c44f60", "title": "Learning to rank for information retrieval"}, {"paperId": "fce6b98e3e5007fd9d645b972a3ae09bf2c79f7f", "title": "TAMER: Training an Agent Manually via Evaluative Reinforcement"}, {"paperId": "c8221c054459e37edbf313668523d667fe5c1536", "title": "Maximum Entropy Inverse Reinforcement Learning"}, {"paperId": "fd62bc380b66500c31a8f1a8b566bcaea25d1652", "title": "Bayesian Inverse Reinforcement Learning"}, {"paperId": "668b1277fbece28c4841eeab1c97e4ebd0079700", "title": "Pattern Recognition and Machine Learning"}, {"paperId": "74d2ad28be32a5802a1b15d4e9a430db2234a3dd", "title": "Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics"}, {"paperId": "a20f0ce0616def7cc9a87446c228906cd5da093b", "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation"}, {"paperId": "94066dc12fe31e96af7557838159bde598cb4f10", "title": "Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping"}, {"paperId": "4c96ca25d889251e20e33d01f24eec175301ab94", "title": "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition"}, {"paperId": "1a4e67d3705492d0af88623b0e62818a16084fca", "title": "The Analysis of Permutations"}, {"paperId": "7d47ee5f84103529f84297c98c21dadb4742e3ff", "title": "RANK ANALYSIS OF INCOMPLETE BLOCK DESIGNS THE METHOD OF PAIRED COMPARISONS"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": "7c363f962654392d67a3323cfeba4ae9cf1dec32", "title": "\u201cTechnique for the Measurement of Attitudes, A\u201d"}, {"paperId": "f6264b11ec0dd9133f1d88a5288a3267a93182f8", "title": "What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "4425633f85c540308c93925eb3335cda3faca2f1", "title": "Multi-objective decision-theoretic planning"}, {"paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3", "title": "ALVINN, an autonomous land vehicle in a neural network"}, {"paperId": "a1173d20ffef2f1ccb1ea2023dcce7c45747a373", "title": "Reinforcement"}, {"paperId": "b225a9eb169a3530289bf834d3b6e785947959ee", "title": "Neuro-Dynamic Programming"}, {"paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614", "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"}, {"paperId": null, "title": "The reward hypothesis"}, {"paperId": "0a8149fb5aa8a5684e7d530c264451a5cb9250f5", "title": "Recent Advances in Hierarchical Reinforcement Learning"}, {"paperId": "97efafdb4a3942ab3efba53ded7413199f79c054", "title": "Reinforcement Learning: An Introduction"}, {"paperId": "c8d353bc901fa3fde1072baf8b4278f688240621", "title": "Models of human behaviour and confidence in judgement: A review"}, {"paperId": "2bfec25cdd5cfa130e6fffe846ab87fbe041e71c", "title": "Theory of algorithms"}, {"paperId": "658ec045aa412141be2e3813a0759bfcb615a488", "title": "Econometric Models of Probabilistic Choice"}, {"paperId": "abafd744e85dbc3122c5463e12f9edcab6770a11", "title": "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION 1 Intrinsic Motivation Systems for Autonomous Mental Development"}, {"paperId": null, "title": "Self-consuming"}, {"paperId": null, "title": "Specific versus general principles"}, {"paperId": null, "title": "Statistical rejection sampling"}, {"paperId": null, "title": "Reward model ensembles"}, {"paperId": null, "title": "The reward function used for training"}]}