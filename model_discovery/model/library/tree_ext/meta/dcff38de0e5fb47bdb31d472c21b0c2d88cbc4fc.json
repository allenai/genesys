{"paperId": "dcff38de0e5fb47bdb31d472c21b0c2d88cbc4fc", "title": "AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models", "abstract": "There are growing interests in adapting large-scale language models using parameter-efficient fine-tuning methods. However, accelerating the model itself and achieving better inference efficiency through model compression has not been thoroughly explored yet. Model compression could provide the benefits of reducing memory footprints, enabling low-precision computations, and ultimately achieving cost-effective inference. To combine parameter-efficient adaptation and model compression, we propose AlphaTuning consisting of post-training quantization of the pre-trained language model and fine-tuning only some parts of quantized parameters for a target task. Specifically, AlphaTuning works by employing binary-coding quantization, which factorizes the full-precision parameters into binary parameters and a separate set of scaling factors. During the adaptation phase, the binary values are frozen for all tasks, while the scaling factors are fine-tuned for the downstream task. We demonstrate that AlphaTuning, when applied to GPT-2 and OPT, performs competitively with full fine-tuning on a variety of downstream tasks while achieving>10x compression ratio under 4-bit quantization and>1,000x reduction in the number of trainable parameters.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2022, "citationCount": 21, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2210.03858", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "It is demonstrated that AlphaTuning, when applied to GPT-2 and OPT, performs competitively with full fine-tuning on a variety of downstream tasks while achieving>10x compression ratio under 4-bit quantization and>1,000x reduction in the number of trainable parameters."}, "embedding": {"model": "specter_v2", "vector": [0.17728297412395477, 0.36866381764411926, -0.4799124598503113, -0.03537939116358757, -0.26769599318504333, -0.04842564836144447, 0.4758438766002655, -0.16224098205566406, -0.8308808207511902, -0.36714980006217957, 0.4127374589443207, -0.17009782791137695, 0.4397053122520447, 0.15191254019737244, -0.23027576506137848, 0.2976619303226471, -0.8998687863349915, 0.6832031011581421, 0.058313269168138504, -0.6077795028686523, -0.5350431799888611, -0.787847638130188, -0.9885333776473999, 0.0559280626475811, 0.48981159925460815, 0.8820747137069702, -0.023174840956926346, 1.198258638381958, -0.567986249923706, -0.5696362257003784, 0.4092710316181183, -0.4389176368713379, 0.4117897152900696, -0.057731907814741135, 0.08245259523391724, -0.1159740686416626, 0.08281463384628296, -0.22413840889930725, -0.20478680729866028, 0.9753190279006958, -0.26967889070510864, 0.4148631691932678, 0.6554491519927979, -0.4216685891151428, -0.617882251739502, 0.38217321038246155, 0.7008013129234314, 0.8685860633850098, -0.7078376412391663, -0.4208783507347107, 0.8151097297668457, -0.9372677206993103, -0.04626135900616646, 1.4825189113616943, 0.5648537278175354, 0.6138460636138916, -0.3627147674560547, -0.7947835326194763, 0.5053765773773193, -0.21036432683467865, -1.1457594633102417, -0.412192165851593, -0.4331941306591034, -0.0016495618037879467, 2.1783971786499023, -0.27060702443122864, 0.07648175209760666, 0.5147550106048584, 0.16579793393611908, 0.731159508228302, -0.30813103914260864, -0.77409827709198, 0.002534946659579873, 0.2198161780834198, -0.39547112584114075, 0.8249036073684692, -0.3059632182121277, 0.2142302691936493, -0.7768583297729492, -0.33685576915740967, 0.3039654791355133, -0.549359142780304, 0.2654854357242584, -0.12033679336309433, 0.08045606315135956, 0.8997276425361633, 0.31738153100013733, 0.4773000180721283, 0.08290129899978638, 0.8829288482666016, 0.669599711894989, 0.40871647000312805, 0.5197007060050964, 0.52821946144104, -0.5359663963317871, -0.005027794744819403, -0.9064946174621582, 0.17469102144241333, -0.06948257982730865, 0.9607981443405151, -0.07630874961614609, 0.18358194828033447, -0.9425813555717468, 0.6822593212127686, 1.2795578241348267, 0.20157043635845184, 0.6978351473808289, -0.7043249011039734, 0.5709987878799438, -0.9007031321525574, 0.05767298862338066, -0.36575576663017273, -0.3767928183078766, -0.2681681215763092, -0.9645090103149414, -1.4719794988632202, -0.796921968460083, 0.03773114085197449, -0.9247546195983887, 0.937109112739563, 0.02119465544819832, -0.01921469159424305, -0.04157351702451706, 0.321981817483902, 0.26802557706832886, 0.9094091653823853, 0.5641249418258667, -0.18811669945716858, 1.2083775997161865, -0.8775745034217834, -0.7297454476356506, -1.032776951789856, 0.6457943916320801, -0.17849460244178772, 0.7489749789237976, -0.25757578015327454, -1.2654329538345337, -0.8621005415916443, -0.8054523468017578, -0.21485745906829834, -0.7289263010025024, 0.28814321756362915, 0.8313474059104919, 0.6608976125717163, -0.7468620538711548, 0.6645646095275879, -0.29203397035598755, -0.26307934522628784, 0.48361971974372864, 0.28547754883766174, 0.3478485345840454, -0.1551026701927185, -1.4972840547561646, -0.019100693985819817, 0.5910421013832092, -0.8230994939804077, 0.2488759160041809, -0.835549533367157, -0.7458316087722778, 0.21041789650917053, 0.31422749161720276, -0.8804203271865845, 1.3117979764938354, -0.18940654397010803, -1.7601099014282227, 0.43534332513809204, -0.10268589109182358, 0.1338936686515808, 0.22896620631217957, 0.12338227033615112, -0.6232617497444153, -0.33873996138572693, -0.7868208885192871, 0.810422420501709, 0.9033859372138977, 0.11390751600265503, -0.12010856717824936, 0.32353752851486206, -0.31897759437561035, 0.3124063014984131, -0.5965374708175659, 0.6052263975143433, -1.2691106796264648, -0.3881700038909912, 0.18932822346687317, 0.4253039062023163, -0.11239200830459595, -0.17338255047798157, -0.21058881282806396, -0.9608062505722046, 0.6167067885398865, -0.02328486181795597, 1.2518218755722046, -0.9528324604034424, -0.5669329762458801, 0.1541108340024948, -0.2568017244338989, 0.23217524588108063, -0.8540263175964355, 0.506212592124939, 0.08337818831205368, 0.6997363567352295, -0.44174525141716003, -1.2736728191375732, 0.3171604573726654, -0.10852532088756561, -0.3892674446105957, -0.56307053565979, 0.1081324890255928, 1.124706745147705, -0.663702666759491, 0.052767373621463776, -0.3598695695400238, 0.6909509897232056, -1.4625831842422485, 1.2312918901443481, -0.8952655792236328, 0.36463630199432373, 0.0713953748345375, -0.1744757443666458, 0.05715196952223778, -0.2821696400642395, 0.42434361577033997, -0.3792564570903778, 0.16661573946475983, 0.3051949441432953, -0.08338901400566101, 1.3962810039520264, -0.5910335183143616, 0.03148077800869942, -0.14371412992477417, -0.2692233920097351, 0.11647025495767593, 0.2944698929786682, -0.2866310775279999, -0.7807444930076599, 0.33325400948524475, 0.5259320139884949, -0.6317826509475708, 0.2852120101451874, 0.9553816914558411, 0.6167218089103699, -0.526520848274231, -0.10208681970834732, 0.6910680532455444, -0.6785224676132202, 0.6985272765159607, 0.09758835285902023, 0.5086860656738281, 0.2527942359447479, 0.5763014554977417, 0.14732134342193604, 0.334102988243103, -0.9989999532699585, -0.2027318775653839, 0.3562358319759369, 0.5071188807487488, 0.5620723366737366, 0.3445087671279907, -0.5590760111808777, -0.7619711756706238, -0.09468942135572433, 0.49435362219810486, 1.9067182540893555, -0.7154674530029297, -0.30821484327316284, -0.9646580219268799, -0.3198143541812897, -0.0793776586651802, 0.04611670598387718, -0.621064305305481, -0.48194730281829834, -0.43535763025283813, -1.2855688333511353, 0.8167243003845215, -0.11658840626478195, 1.036021113395691, -0.09709670394659042, 0.007047343999147415, -0.2598830461502075, 0.40165337920188904, -0.7468280792236328, -0.9484516382217407, 0.5338840484619141, -0.730221688747406, 0.43074584007263184, 0.14769865572452545, 0.0852494090795517, -0.12674249708652496, -0.9592201709747314, 1.002747654914856, -0.24470169842243195, 0.06285002082586288, -0.16239874064922333, 0.527405858039856, -0.4956173300743103, -1.1905806064605713, 0.7533814311027527, 0.19158601760864258, -0.07732515037059784, 0.2463987022638321, 0.46960416436195374, 0.21137632429599762, 0.006872090976685286, -0.46671411395072937, -0.06222248077392578, -0.04764115810394287, -0.046292465180158615, 1.0593554973602295, -0.3133753538131714, -0.09361481666564941, -1.553580641746521, 1.235171914100647, 0.21993374824523926, -0.7518031001091003, 0.478787899017334, -0.8300681114196777, -0.06294149160385132, 0.9951789975166321, -0.6381720304489136, -0.5022733211517334, -1.1362615823745728, -0.06815434247255325, -0.3780568242073059, 0.028518859297037125, 0.15254494547843933, 0.419623464345932, -0.0003189417766407132, 0.45604485273361206, 0.2557894289493561, 0.08855868875980377, -0.3145390748977661, 0.6474514603614807, -0.6486776471138, 0.48070648312568665, 0.2811833322048187, 0.24166101217269897, -0.43368449807167053, -0.14512784779071808, -0.6947394609451294, -0.273028165102005, -0.3103843033313751, -0.2627497613430023, 0.1980092078447342, 0.20818208158016205, -0.5110145807266235, -0.4972129166126251, -0.4278131425380707, -0.6637306213378906, -0.5634749531745911, 0.08665265142917633, -0.3031321167945862, -0.15876412391662598, -1.142953872680664, -1.1890016794204712, -0.23724967241287231, -0.8067519664764404, -1.161496639251709, 0.46353310346603394, -0.0074547757394611835, -0.2891348898410797, -0.7786272168159485, 0.18613119423389435, -0.46326786279678345, 1.2548911571502686, -1.174005389213562, 1.0809129476547241, 0.2566206753253937, 0.16780459880828857, 0.060788385570049286, 0.12386273592710495, 0.5923542380332947, -0.5139195919036865, 0.47892525792121887, -1.0759081840515137, 0.09829309582710266, -0.6693918108940125, -0.30908679962158203, 0.2502257525920868, 0.2801184058189392, 0.9882009625434875, -0.2893732190132141, -0.3173915147781372, 1.2168500423431396, 1.305405855178833, -0.786806046962738, -0.12876582145690918, 0.19534383714199066, 0.828779935836792, -0.3275933563709259, -0.4357982873916626, 0.6680746674537659, 0.29505831003189087, 0.8737049698829651, -0.08538159728050232, 0.02625039406120777, -0.39430099725723267, -0.7435004711151123, 0.2539435625076294, 2.1566693782806396, 0.35477590560913086, -0.3528847098350525, -0.859190046787262, 0.08779337257146835, -0.8469133377075195, -0.2558882534503937, 0.9862141013145447, 0.9210882782936096, 0.7270993590354919, -0.527738094329834, -0.25902166962623596, -0.46639472246170044, 0.01828021928668022, 0.17449477314949036, -0.6369209885597229, -1.2646024227142334, 0.21457380056381226, 0.22968032956123352, 0.3503972589969635, 0.6156265139579773, -0.4116227924823761, 0.7226147651672363, 14.398781776428223, 1.1344335079193115, -0.04695768654346466, 0.9539346098899841, 0.7372675538063049, 0.07441935688257217, -0.4326247274875641, -0.6484147310256958, -1.1262496709823608, 0.1701200008392334, 1.6358143091201782, 0.366659939289093, 1.1890034675598145, -0.15652666985988617, 0.20960402488708496, 0.3655466139316559, 0.05970100313425064, 0.6539475917816162, 0.37465986609458923, -1.258136510848999, 0.5306752920150757, 0.30092284083366394, 0.6627811789512634, 0.57317054271698, 0.9601690173149109, 1.2742842435836792, 0.3429132103919983, -0.38741227984428406, 0.34861937165260315, 0.2389805018901825, 1.1507526636123657, -0.30068135261535645, 0.16661009192466736, 0.346073716878891, -0.7744613885879517, -0.31507164239883423, -0.7763396501541138, -0.8701993227005005, 0.2665163278579712, 0.06242332607507706, -0.8101184964179993, -0.7352602481842041, 0.13060802221298218, 0.342864990234375, -0.47489553689956665, 0.27629825472831726, -0.22915880382061005, 0.9521588087081909, -0.24279667437076569, 0.17164482176303864, 0.32219401001930237, 0.06526460498571396, -0.07707350701093674, 0.3182526230812073, 0.1092301458120346, 0.03433416038751602, -0.06037715822458267, 0.3263280987739563, -0.7211126089096069, 0.05829393491148949, 0.03431185334920883, -0.11735650897026062, 0.21546050906181335, 0.44871968030929565, 0.5967647433280945, 0.12757614254951477, -0.37685278058052063, 0.3722871243953705, 0.9830098152160645, 0.36112090945243835, -0.40662622451782227, 0.31652531027793884, 0.3641375005245209, -0.5960797071456909, 0.14799325168132782, 0.6434135437011719, -0.1733245998620987, -0.5074186325073242, -0.8137104511260986, -0.6142835021018982, 0.34082159399986267, -0.7527257204055786, -0.47623464465141296, 0.7299290895462036, -0.0002966037718579173, -0.0296404380351305, 0.06428610533475876, -0.5015344619750977, 0.17502374947071075, 0.4292486608028412, -1.2654385566711426, -0.4602721333503723, 0.8552683591842651, -0.6018154621124268, -0.5136783719062805, -0.2862231135368347, 1.6296741962432861, 0.16831976175308228, -0.6372528672218323, 0.6106794476509094, 0.6608365774154663, -0.4735069274902344, -0.02246321178972721, -0.5578194856643677, 1.2202715873718262, 0.1338420957326889, -0.34175464510917664, 0.2809453308582306, 0.0691051259636879, 0.21155311167240143, -0.6589987874031067, -0.4213537573814392, 0.7473062872886658, -0.561884343624115, -0.40957269072532654, -0.9255141615867615, -1.026921272277832, 0.10145223885774612, 0.46668675541877747, -0.16373208165168762, 0.5758411288261414, 0.2092713564634323, -0.8379416465759277, -0.20075175166130066, -0.9605758190155029, 0.08388436585664749, 0.47507625818252563, -0.6992323398590088, 0.10014736652374268, -0.014965515583753586, 0.6033098101615906, -1.307765245437622, -0.9126264452934265, -0.23994386196136475, 0.04590873792767525, 0.012483310885727406, 1.113529086112976, -0.2728084623813629, 0.3258418142795563, 1.0156800746917725, -0.17198462784290314, -1.08928382396698, -0.28529638051986694, -0.7100186944007874, -0.2102585732936859, -0.14313282072544098, 0.8025785088539124, -0.3940504193305969, 0.07500142604112625, 0.8243519067764282, 0.09320764988660812, -0.6200491786003113, -0.7947956919670105, -0.2535746693611145, 0.2093060165643692, -0.36242347955703735, 0.7692619562149048, -0.27354761958122253, -0.37339410185813904, 0.11624778807163239, 0.6094794869422913, 0.6462875604629517, -0.42214250564575195, -1.2527137994766235, 0.41959184408187866, 0.3099318742752075, -0.5976871848106384, -0.6114280223846436, -0.18702468276023865, -1.33574640750885, -0.24798379838466644, -1.0609135627746582, -0.024352623149752617, -0.49576956033706665, -0.46421629190444946, -0.1079137846827507, 0.02530502714216709, -0.06495939940214157, 0.2694435119628906, 0.13918840885162354, -0.24689152836799622, -0.33294105529785156, -0.5230498313903809, 1.0094561576843262, 0.9898440837860107, -0.6858460903167725, -0.056145064532756805, -0.10653664171695709, 0.3039000630378723, 0.27685147523880005, 0.7771090865135193, -0.38027051091194153, -0.9654387831687927, -1.7003746032714844, 0.4394569993019104, -0.47587254643440247, -0.42689257860183716, -0.5116344094276428, 0.4489456117153168, 0.1940554529428482, -0.1946549117565155, 0.4247279167175293, 0.5254572629928589, -0.8173492550849915, -0.46657559275627136, 0.208854541182518, -0.7029958367347717, 0.5251790285110474, 0.6877127885818481, -0.300938218832016, -0.060926202684640884, 0.6273635625839233, 0.16314420104026794, -0.8285489082336426, -0.7101016640663147, 0.44003498554229736, -0.5529663562774658, 0.37742310762405396, -0.7863221168518066, 0.23863248527050018, -0.9813457727432251, -0.2475157231092453, 0.2590706944465637, 0.07591910660266876, -0.15047432482242584, 0.6881614923477173, 0.009361845441162586, -0.9848285913467407, 0.36622631549835205, 0.8281494379043579, 0.04760430380702019, 0.01625831425189972, 0.4945692718029022, 0.4344944357872009, -0.44835084676742554, 0.9165478348731995, 0.44148197770118713, 0.38813844323158264, -0.6865823864936829, -0.08177781850099564, 0.7518215179443359, -0.6623796224594116, 0.1281369924545288, 1.3808459043502808, -0.509529709815979, -1.486915946006775, 0.05682556703686714, -1.632369041442871, -0.36955684423446655, -0.47906580567359924, 0.35916757583618164, -0.0633445680141449, 0.39187395572662354, -0.04213743284344673, -0.3201780319213867, 0.17157407104969025, -0.022213753312826157, -0.5501692295074463, 0.2933829724788666, -0.5233361124992371, -0.22283118963241577, 0.5917617678642273, 1.0095953941345215, -0.7769020199775696, -0.9084469676017761, -0.6860082745552063, -0.5190129280090332, -0.11525384336709976, 0.6132379770278931, -0.6729143261909485, -0.633496105670929, 0.6452397704124451, 0.5481407642364502, -0.006975692231208086, 0.39856013655662537, -0.36415746808052063, 0.13790366053581238, 0.8754268288612366, -0.06767001003026962, -0.9739331603050232, -0.6844624280929565, 1.2906174659729004, 1.072210669517517, -0.9744511246681213, 0.41299039125442505, -0.047195784747600555, -0.7497625946998596, 0.6892738342285156, 0.234526127576828, 0.3740002512931824, 1.0286709070205688, -0.3802046477794647, 0.16407637298107147, 0.29420003294944763, -1.341960072517395, -0.035169634968042374, 1.203532338142395, 0.8740270733833313, 0.7148550152778625, 0.5084402561187744, 0.20827874541282654, 0.6864190697669983, -0.2242884635925293, -0.019663214683532715, 0.324824720621109, 0.17442992329597473, -0.18846403062343597, -0.13044044375419617, -0.19538484513759613, 0.8685213923454285, -0.36420121788978577, -0.682511031627655, 0.25480589270591736, 0.5333108901977539, 0.517838716506958, 0.28875619173049927, 1.0366185903549194, -0.09888467192649841, 0.6139788627624512, 0.19838838279247284, 0.5917112231254578, -0.6805629730224609, -0.36361294984817505, -0.05876905471086502, -0.6754125952720642, -0.16729775071144104, -0.004737630486488342, -0.27259787917137146, -0.12030065804719925, -0.2145071029663086, 0.5765196681022644, 0.12138286978006363, 0.6056103706359863, 1.0222804546356201, 0.794950544834137, 0.49943670630455017, -0.47219574451446533, -0.5940662622451782, -0.6402190327644348, -1.1307185888290405, 0.0865793526172638, -0.5999922752380371, -0.4801836609840393, -0.1626327633857727, 0.07737403362989426, -0.27708420157432556]}, "authors": [{"authorId": "12693169", "name": "S. Kwon"}, {"authorId": "2144193082", "name": "Jeonghoon Kim"}, {"authorId": "2187296055", "name": "Jeongin Bae"}, {"authorId": "31760501", "name": "Kang Min Yoo"}, {"authorId": "2116515859", "name": "Jin-Hwa Kim"}, {"authorId": "120751934", "name": "Baeseong Park"}, {"authorId": "46239568", "name": "Byeongwook Kim"}, {"authorId": "2112436632", "name": "Jung-Woo Ha"}, {"authorId": "40188877", "name": "Nako Sung"}, {"authorId": "122808525", "name": "Dongsoo Lee"}], "references": [{"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "1a5a9d7fdfed0e5d3f5c1b00d6356361b8c57941", "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "a6f5b8f114b3eabbcd7f3f62091a481ca6f7f243", "title": "Predictability and Surprise in Large Generative Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "9c4753ef43d2928866dc5bf6cec53d03373ec2fa", "title": "SimMIM: a Simple Framework for Masked Image Modeling"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "c28b7dfe341f1e13a5a98efbce7946ef795cf9b8", "title": "SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0", "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"}, {"paperId": "a6d8d04962f84ae6225e72723869a002b9fc8036", "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"}, {"paperId": "e553407be283d018e275f472d4d2fd709a6c9248", "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "656ed155c2d345c19d9bff4b50f2ae00db8407cc", "title": "Compacter: Efficient Low-Rank Hypercomplex Adapter Layers"}, {"paperId": "0adec918885dff698acf359988ed79a543157f80", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"paperId": "1d5f5df837139d4ae8af23e3634295594c5b85db", "title": "Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters"}, {"paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"}, {"paperId": "6dffdeb81ebc761a8cce1e36b8d140d5b8d2a0e3", "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "0932abfd0fb90e8a28f7bd195633c9891bfd7ecb", "title": "Pruning Neural Networks at Initialization: Why are We Missing the Mark?"}, {"paperId": "755f16fdbbfb0b128adb3cda9c8c2799aea34290", "title": "Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation"}, {"paperId": "f46c562229c5bc419bbbfb63239431590e4b340a", "title": "Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "6e3f8187f8fef3e11578a73f32da07d33dbf8235", "title": "DART: Open-Domain Structured Data Record to Text Generation"}, {"paperId": "49a049dc85e2380dde80501a984878341dd8efdf", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"}, {"paperId": "31a68965ea4af87295cb83d8fe72e31f651a4ee1", "title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "27338d00461eda47ea931f7214cb2910f67bfba6", "title": "BiQGEMM: Matrix Multiplication with Lookup Table for Binary-Coding-Based Quantized DNNs"}, {"paperId": "84ef2cf4f73bb4eae7ae63fbca04a4d774b75ac7", "title": "Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs"}, {"paperId": "f9700e31a1d0ae34d4571ab056dfb268c1543349", "title": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "3366e9eb81880d172752d4397cb8e9e6de02b935", "title": "Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model"}, {"paperId": "dc160709bbe528b506a37ead334f60d258413357", "title": "Learned Step Size Quantization"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "2735dd87f42f60dd8f50def5ae51bbbf95318235", "title": "Improving Neural Network Quantization without Retraining using Outlier Channel Splitting"}, {"paperId": "47d79963ac69111d8dc82a228d26e6a746a4d087", "title": "Transformers"}, {"paperId": "0688d16bdcecb4a165dd93b5cd55267b83e33fa5", "title": "Retraining-Based Iterative Weight Quantization for Deep Neural Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "be83160b94893f2ef5edd5e68a3f3f8b089a87fd", "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "5c717445179991e002333182df3b233fe502e357", "title": "Learning to Generalize: Meta-Learning for Domain Generalization"}, {"paperId": "a4c40532e68728fbeab5d9415f6ad8e9530db360", "title": "The WebNLG Challenge: Generating Text from RDF Data"}, {"paperId": "531a7f2c659787165df4fd5b4580590b953448e4", "title": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "40fbb2a926e46f59341b8aa7c4359a9602a9f5b5", "title": "Network Sketching: Exploiting Binary Structure in Deep CNNs"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7", "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"}, {"paperId": "5cea23330c76994cb626df20bed31cc2588033df", "title": "Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets"}, {"paperId": "104f7a96eba307056e1038e183ee8c24d009ba13", "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "77e73174e606c0820a52a940088832b32d9a033e", "title": "Efficient Large-Scale Language Model Training on GPU Clusters"}, {"paperId": null, "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "2019) is a conversation dataset containing 16k summaries. Given a dialog, the goal is to generate summarizations to evaluate dialog understanding and natural language generation"}, {"paperId": "8b89ad0ad4151024160dc961586610dd20d09837", "title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"}, {"paperId": null, "title": "2018), and this paper set the number of iterations as 15 when the Alternating method is selected for PTQ process"}, {"paperId": null, "title": "2021)) is an open-domain text generation dataset with 82k examples, which are extracted from several datasets including WebNLG 2017 and Cleaned E2E"}, {"paperId": null, "title": "Ran El-Yaniv, and Yoshua Bengio"}, {"paperId": null, "title": "Methods in Natural"}]}