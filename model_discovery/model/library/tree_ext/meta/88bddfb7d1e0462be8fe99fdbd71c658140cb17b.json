{"paperId": "88bddfb7d1e0462be8fe99fdbd71c658140cb17b", "title": "From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities", "abstract": "The multimodal task of Visual Question Answering (VQA) encompassing elements of Computer Vision (CV) and Natural Language Processing (NLP), aims to generate answers to questions on any visual input. Over time, the scope of VQA has expanded from datasets focusing on an extensive collection of natural images to datasets featuring synthetic images, video, 3D environments, and various other visual inputs. The emergence of large pre-trained networks has shifted the early VQA approaches relying on feature extraction and fusion schemes to vision language pre-training (VLP) techniques. However, there is a lack of comprehensive surveys that encompass both traditional VQA architectures and contemporary VLP-based methods. Furthermore, the VLP challenges in the lens of VQA haven't been thoroughly explored, leaving room for potential open problems to emerge. Our work presents a survey in the domain of VQA that delves into the intricacies of VQA datasets and methods over the field's history, introduces a detailed taxonomy to categorize the facets of VQA, and highlights the recent trends, challenges, and scopes for improvement. We further generalize VQA to multimodal question answering, explore tasks related to VQA, and present a set of open problems for future investigation. The work aims to navigate both beginners and experts by shedding light on the potential avenues of research and expanding the boundaries of the field.", "venue": "arXiv.org", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work presents a survey in the domain of VQA that delves into the intricacies of V QA datasets and methods over the field's history, introduces a detailed taxonomy to categorize the facets of VZA, and highlights the recent trends, challenges, and scopes for improvement."}, "embedding": {"model": "specter_v2", "vector": [0.18266607820987701, 0.135380357503891, 0.11125455796718597, -0.39604994654655457, -0.5718646049499512, -0.23558329045772552, 0.27451565861701965, -0.01625959388911724, -0.5866283774375916, -0.6127040982246399, 0.42818012833595276, 0.2956427335739136, 0.23136550188064575, -0.12722869217395782, 0.010882762260735035, 0.4326435327529907, -0.48207417130470276, 0.08883891254663467, 0.22319424152374268, -1.0119953155517578, 0.1896428018808365, -0.6720970273017883, -1.5745528936386108, 0.6065559387207031, 0.1486499011516571, 0.7688222527503967, -0.004025095608085394, 1.484758973121643, -0.34963691234588623, 0.551439642906189, 0.37257108092308044, 0.06422251462936401, 0.0026654291432350874, 0.13391563296318054, -0.6723650693893433, 0.35692134499549866, 0.9602494239807129, -0.6378324627876282, -0.6753436326980591, 0.6549032926559448, 0.10486966371536255, 0.6593945622444153, 0.5145586133003235, -0.9388492107391357, -0.674550473690033, -0.07625817507505417, 0.6289072632789612, 0.4754139482975006, 0.5763232111930847, -0.35184207558631897, 1.6382339000701904, -1.9016963243484497, 0.4869195818901062, 1.7840120792388916, 0.16107240319252014, 0.7305125594139099, 0.0181984081864357, -0.23832155764102936, 0.31736063957214355, 0.07935132831335068, -0.6449748873710632, -0.24505092203617096, -0.27596259117126465, -0.5989309549331665, 1.2993190288543701, -0.4080106317996979, -0.30447521805763245, 0.4202197790145874, -0.13447518646717072, 1.4459233283996582, -0.20804941654205322, -0.8939367532730103, 0.052401598542928696, -0.2807140052318573, 0.28129199147224426, 0.7927402853965759, -0.9073495268821716, 0.3013911247253418, -0.8315450549125671, 0.031334586441516876, 0.4746733009815216, -0.3195897936820984, -0.30024224519729614, -0.5498839020729065, -0.43574318289756775, 1.0435736179351807, 0.8704525828361511, 0.4274437725543976, -0.01989567093551159, 0.39663824439048767, 0.3298686146736145, 0.38694101572036743, -0.5651788711547852, 0.44931599497795105, 0.037061773240566254, 0.7759206295013428, -0.6536005735397339, 0.4614645838737488, 0.09117688983678818, 0.817466676235199, 0.01146198995411396, -0.16223996877670288, -0.9419220089912415, 0.26550933718681335, 1.4037855863571167, 0.13084310293197632, -0.11094117164611816, -0.9373838901519775, 0.5128839015960693, -0.5406210422515869, 0.7064676284790039, -0.8358477354049683, -0.1517919898033142, 0.08548205345869064, -0.33209285140037537, -0.7173148393630981, -0.06633386760950089, -0.05218895897269249, -1.3464192152023315, 0.6925484538078308, -0.234514981508255, -0.5549559593200684, 0.25400641560554504, 1.0607695579528809, 1.1434811353683472, 0.8126723766326904, 0.5062271356582642, 0.7309725880622864, 1.6962965726852417, -0.5884354114532471, -0.3404678702354431, -1.0091944932937622, 0.07978114485740662, -0.36512380838394165, 0.7442240715026855, -0.36149516701698303, -0.7225522994995117, -1.4711651802062988, -0.8465825915336609, -0.4745104908943176, -0.5026894807815552, 0.4379432797431946, 0.633324384689331, 0.29560214281082153, -1.7334315776824951, -0.033319562673568726, 0.6940673589706421, -0.8534958958625793, 0.1941436380147934, 0.186681866645813, -0.14644190669059753, -1.0224809646606445, -0.9426896572113037, 0.18505653738975525, -0.36999550461769104, -0.49057769775390625, -0.4665071666240692, 0.3514336943626404, -1.6156209707260132, -0.2173088639974594, 0.350598007440567, -1.0482105016708374, 1.089102029800415, -0.4962281286716461, -0.6174127459526062, 0.6777312159538269, -0.7997331023216248, 0.3695279657840729, 0.03957754001021385, -0.28742724657058716, -0.6107093095779419, -0.02232985571026802, 0.11518594622612, 1.5264047384262085, 0.7968159317970276, -0.33412832021713257, -0.2516796886920929, 0.2558436989784241, 0.22735366225242615, 0.05821935087442398, -0.1226000189781189, 0.9912525415420532, -0.5011007785797119, -0.0338875949382782, 0.7253996729850769, 0.9919788241386414, -0.13577410578727722, 0.2564972937107086, -0.12907777726650238, -1.1092333793640137, 0.940269410610199, 0.2199532687664032, -0.1318509727716446, -0.846893846988678, -0.4758337438106537, -0.3546127378940582, 0.30605456233024597, -0.6057196259498596, -1.4538798332214355, 0.8725290298461914, -0.06997887045145035, 0.15497659146785736, -0.21490417420864105, -1.077244758605957, -0.16010330617427826, -0.05444590747356415, -0.7293379306793213, -0.18217512965202332, 0.6595326066017151, 1.2251932621002197, -0.8357359170913696, -0.28578299283981323, 0.1400749385356903, 0.06590922921895981, -0.5565062165260315, 1.3330318927764893, -0.8723911643028259, 0.08878051489591599, -0.3299388885498047, 0.3309166729450226, 0.21209867298603058, -0.2143174409866333, -0.20034587383270264, -0.7899078130722046, -0.11769665032625198, -0.1547560840845108, -0.18523477017879486, 1.6038044691085815, 0.2272139936685562, 0.8328994512557983, -0.27739548683166504, -0.33685073256492615, 0.4436233341693878, 0.46960800886154175, -0.19975502789020538, -0.4917695224285126, 0.4307154715061188, 0.20192180573940277, -0.7979616522789001, -0.46109727025032043, 0.8855579495429993, 0.9710163474082947, -0.5288332104682922, -0.03625466674566269, 0.5061813592910767, -0.6721506118774414, 0.46319958567619324, 0.5236262679100037, 0.5872542858123779, 0.24440842866897583, -0.06513072550296783, 0.16642174124717712, 0.5821232795715332, -0.3725195825099945, -0.3050152659416199, 0.8460947275161743, 0.4181607961654663, 1.521636724472046, 0.24266433715820312, -1.0290513038635254, -0.11879121512174606, -0.28832224011421204, 0.9933115243911743, 1.6184265613555908, 0.32223406434059143, 0.014901354908943176, -0.5973625183105469, -0.7275263667106628, -0.48919999599456787, -0.440158486366272, -0.8455172777175903, 0.41280075907707214, -0.10992875695228577, -0.4502456784248352, 0.5463725328445435, 0.3229867219924927, 0.988008975982666, -0.9882330894470215, -0.2898283302783966, -0.6405982971191406, -0.09043027460575104, -0.8946914672851562, -0.8919332027435303, -0.1897546350955963, -0.17391875386238098, -0.6623302102088928, -0.3869076073169708, -0.6341730952262878, 0.5791783332824707, -0.3601371943950653, 1.0695035457611084, -0.5316761136054993, -0.21842534840106964, 0.7020100355148315, 0.07304082065820694, -0.47692805528640747, -0.42556795477867126, -0.4100436568260193, -0.2755841910839081, -0.043669186532497406, 0.5890766978263855, 0.8577162027359009, -0.09795985370874405, 0.5004224181175232, -0.7767195701599121, -0.1813770979642868, 0.3846490681171417, -0.19348400831222534, 0.9300100207328796, -0.768683671951294, 0.38411208987236023, -0.41399022936820984, 0.973064124584198, 0.03580408915877342, -0.17987768352031708, 0.3848263919353485, -0.15277966856956482, -0.3964362144470215, -0.04694139212369919, -0.48170897364616394, 0.10418643057346344, -0.0809319019317627, 0.04194597899913788, -0.2986152172088623, -0.6762343049049377, 0.1722908318042755, 0.104379802942276, -0.15603984892368317, 0.6301243901252747, 0.08945450186729431, 0.5927094221115112, 0.29193487763404846, 0.9090930819511414, -0.6809236407279968, 0.8160185813903809, 0.26133379340171814, 0.24571843445301056, 0.13167285919189453, -0.3584950566291809, -0.6923374533653259, -0.4532235264778137, -0.6034568548202515, -0.18449217081069946, -0.889757513999939, 0.816952645778656, -0.5677788257598877, -0.6933097839355469, -0.4452114701271057, -1.207755446434021, 0.3825738728046417, -0.2507142722606659, -0.34838345646858215, -0.16243097186088562, -0.691722571849823, -0.539855420589447, -0.3428505063056946, -0.42206937074661255, -0.8912230134010315, 0.3030712604522705, 0.5604748129844666, -0.33115360140800476, 0.3097684681415558, 0.07920054346323013, 0.002009881194680929, 0.5096648931503296, -0.18042846024036407, 0.8837871551513672, 0.1697002500295639, -0.27025866508483887, -0.4065234661102295, -0.30611294507980347, 0.5341662168502808, 0.06302770227193832, -0.014625566080212593, -0.9090471863746643, 0.19506804645061493, 0.06481403112411499, -0.5709083080291748, 0.4894903302192688, 0.21832287311553955, 0.16335323452949524, 0.8288049697875977, 0.14460328221321106, -0.01965920999646187, 1.5972530841827393, -0.5480191707611084, 0.02016901783645153, -0.2823779582977295, 0.8797013163566589, 1.030487060546875, -0.036027565598487854, 0.23930712044239044, 0.6363474130630493, -0.11409325897693634, 0.6169881224632263, -0.42347341775894165, -0.751982569694519, -0.5338239669799805, 0.4627053439617157, 0.813977062702179, 0.48071765899658203, -0.33872175216674805, -1.3397307395935059, 1.075234293937683, -1.1188665628433228, -0.20013220608234406, 0.21153917908668518, 0.46050214767456055, -0.3196161687374115, -0.7707169055938721, -0.05133482813835144, -0.6329457759857178, 0.5737404823303223, 0.3995347321033478, -0.21459607779979706, -0.41786232590675354, -0.4972170293331146, 0.42505061626434326, -0.35336917638778687, 0.5334923267364502, -0.7823225855827332, 0.16346032917499542, 14.262999534606934, 0.31639763712882996, 0.11221146583557129, 0.2785191535949707, 0.5436357855796814, 0.6357058882713318, -0.6758049726486206, -0.048375364392995834, -0.7414515018463135, -0.846256673336029, 0.47550809383392334, 0.7949466109275818, 0.08963834494352341, 0.08215900510549545, 0.05380702018737793, -0.055278073996305466, -0.9528869390487671, 1.204018473625183, 0.8815338015556335, -0.9773910641670227, 0.4690342843532562, 0.1320381462574005, 0.05478988587856293, 0.6024319529533386, 0.8230862021446228, 0.7805944681167603, 0.002113205147907138, -0.8684895634651184, 0.5218278765678406, 0.36620834469795227, 0.7161664366722107, 0.16109466552734375, 0.5237679481506348, 0.32229578495025635, -1.0679185390472412, -0.5207874178886414, -0.7204199433326721, -0.774218738079071, 0.17230138182640076, -0.4990430474281311, -0.6008466482162476, -0.11072304099798203, -0.1769714057445526, 0.2636815011501312, -0.350210964679718, 0.5271668434143066, -0.17521174252033234, 0.12646622955799103, -0.09951359778642654, -0.30977270007133484, 0.1905682533979416, 0.7001694440841675, 0.42131051421165466, -0.5777087807655334, -0.0718766525387764, 0.25927960872650146, 0.3344253599643707, 0.3966057598590851, -0.7159066200256348, 0.2545672655105591, -0.4443061947822571, -0.026227418333292007, -0.6132895350456238, 0.5861483216285706, 0.11513986438512802, 0.807799756526947, -0.46026986837387085, 0.2004987895488739, 0.23646950721740723, 0.4960049092769623, 0.15306012332439423, -0.21314649283885956, 0.09476757794618607, -0.13985922932624817, 0.3132918179035187, 0.43352675437927246, -0.10083253681659698, -0.4346362352371216, -0.5575366020202637, -0.17913368344306946, 0.8218579292297363, -1.1149203777313232, -1.251942753791809, 1.3320362567901611, -0.46430695056915283, -0.9155846238136292, 0.008119861595332623, -0.7266367077827454, -0.2676207721233368, 0.4683920741081238, -1.341907262802124, -0.9564775824546814, -0.46605271100997925, 0.15116362273693085, 0.11481247842311859, -0.20674769580364227, 1.1750402450561523, -0.17790375649929047, 0.32471996545791626, -0.4444403648376465, -0.6833266615867615, 0.37035033106803894, 0.07337865978479385, -0.5230416059494019, 0.4174025356769562, 0.523770809173584, 0.4112696647644043, -0.1681007444858551, -0.04089207947254181, -0.059176817536354065, -0.818351686000824, 0.13869737088680267, 0.4843275249004364, -0.9894612431526184, -1.0703247785568237, -0.5973142385482788, -0.7182201147079468, -0.2811034917831421, 0.7755771279335022, 0.047294653952121735, 0.2915109694004059, -0.038149066269397736, -0.7322808504104614, 0.020154215395450592, -1.2051784992218018, 0.03394909203052521, -0.010044895112514496, -1.000576376914978, -0.6404286026954651, -0.21499766409397125, 0.2848559021949768, -0.5982640385627747, -0.07288524508476257, -0.0359029546380043, 0.2719605267047882, 0.10247107595205307, 1.2035801410675049, -0.7218139171600342, 0.8088911175727844, 0.1330781877040863, -0.7024788856506348, -0.16907921433448792, 0.26549214124679565, -0.3274458944797516, 0.31931445002555847, -0.0969623476266861, 0.6492253541946411, 0.010267595760524273, 0.5427610874176025, 0.8312854170799255, 0.9448347091674805, -0.38272735476493835, -0.47670629620552063, 0.47662368416786194, -0.1721792370080948, -0.5621550679206848, -0.05186288803815842, -0.384242445230484, -0.6924033164978027, 0.24819537997245789, 0.4596679210662842, 0.9868543148040771, -0.3497028648853302, -0.3162471652030945, 0.7081432342529297, -0.16296397149562836, 0.0031309020705521107, -0.15726137161254883, -0.41703566908836365, -1.700228214263916, -0.37036192417144775, -0.7368983030319214, 0.3893851339817047, -1.7220370769500732, -0.7797017097473145, 0.6843838095664978, -0.2007567137479782, -0.04254813864827156, 0.437762051820755, -0.16314467787742615, 0.031919002532958984, -0.42086780071258545, -0.6380769610404968, 0.6551504731178284, 1.1861366033554077, -1.0224299430847168, 0.04273078590631485, -0.15728379786014557, -0.2643406391143799, 0.1989888846874237, 0.25439023971557617, -0.2165590226650238, -1.2434911727905273, -1.3533530235290527, 0.3279494345188141, 0.07829425483942032, 0.1883033663034439, -0.8010848760604858, 1.1045345067977905, 0.5314690470695496, 0.21312402188777924, -0.45512205362319946, 0.9241616725921631, -0.6106472015380859, -1.0518168210983276, -0.05756586790084839, -0.6894567608833313, 0.1391977071762085, 0.1213144063949585, -0.34743109345436096, -0.7449128031730652, 0.6208387017250061, -0.0773407593369484, -0.8976548314094543, -1.2923880815505981, 0.566234827041626, -0.48179683089256287, 0.2065400928258896, -0.07865898311138153, -0.32048726081848145, -0.9057928323745728, -0.9144713878631592, -0.3245075047016144, 0.9185459613800049, -0.748650848865509, 1.03779935836792, 1.7521185874938965, -0.9820424914360046, -0.4491264224052429, 0.16415837407112122, 0.4035976231098175, 0.016257382929325104, 0.9745893478393555, 0.13644520938396454, -0.3013071119785309, -0.001857590745203197, -0.043427154421806335, -0.004662077873945236, -0.803693950176239, -0.020925214514136314, 0.9616743326187134, 0.29504403471946716, 0.32689905166625977, 1.5425561666488647, -0.10922820121049881, -1.1511878967285156, 0.05843709781765938, -0.9755508899688721, -1.117613673210144, 0.12599299848079681, 0.5469399690628052, -0.6427119374275208, -0.26247909665107727, -0.5063692331314087, -0.5755736827850342, 0.7380566000938416, 0.252031147480011, -1.0183374881744385, 0.04846293851733208, -0.186538428068161, -0.2922971844673157, 0.22130221128463745, 0.8661330342292786, -0.5545668005943298, -0.7352191209793091, -0.3928486704826355, -0.23989735543727875, -0.44364720582962036, -0.10999956727027893, -0.3939395844936371, -0.45287811756134033, 0.9407016038894653, 0.8776450157165527, 0.3047686219215393, 0.627091646194458, 0.4166363775730133, 0.28626886010169983, 0.6425734162330627, -0.34892401099205017, -0.2627180516719818, 0.04526733234524727, 1.0804880857467651, 1.6837345361709595, -1.2106977701187134, -0.043520890176296234, -0.46629634499549866, -0.47536802291870117, 1.3154414892196655, 0.535725474357605, -0.20580875873565674, 0.9974278211593628, -0.7553477883338928, 0.4687398672103882, -0.1369839459657669, -0.5884538888931274, -0.6479594707489014, 1.6324878931045532, 1.522355079650879, 0.3751525282859802, 0.010679831728339195, 0.46931374073028564, 0.6559461951255798, 0.11340166628360748, 0.18950556218624115, 0.5545299053192139, 0.32279378175735474, -0.43330472707748413, -0.3695029318332672, -0.1631462275981903, 0.44334325194358826, -0.07184086740016937, -0.12897653877735138, -0.2889539897441864, 0.8377867341041565, 0.5404602289199829, 1.2813581228256226, 0.8593524098396301, -0.20165225863456726, 0.9188240170478821, 0.02165871672332287, 0.506495475769043, -0.8313748836517334, -0.32679611444473267, -0.5399000644683838, -0.6701986193656921, 0.0959368571639061, -0.6217488646507263, -0.7153905034065247, -0.33541569113731384, 0.3311844766139984, 0.7373132705688477, -0.26588553190231323, 0.28358256816864014, 0.8816470503807068, 0.46723228693008423, 0.11866503953933716, -0.6231809854507446, 0.005047626793384552, 0.02110034041106701, -1.0077168941497803, 0.5775198340415955, -0.6466513872146606, 0.23011371493339539, -0.3191487789154053, -0.06981007009744644, -0.23121540248394012]}, "authors": [{"authorId": "2264461838", "name": "Md Farhan Ishmam"}, {"authorId": "2191098096", "name": "Md Sakib Hossain Shovon"}, {"authorId": "2248137628", "name": "M. F. Mridha"}, {"authorId": "2263776802", "name": "Nilanjan Dey"}], "references": [{"paperId": "838422b5ddaaa5637ba86056d1e964409bb2f016", "title": "Robust Visual Question Answering: Datasets, Methods, and Future Challenges"}, {"paperId": "3b6179c293df29e31d31cea46476f104ab6950f2", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World"}, {"paperId": "bf7025a2e5dbb3c09deae02a1aa98a256ca559e2", "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"}, {"paperId": "f22d71c7ce9720ba1f717a4f1181488200e78198", "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"}, {"paperId": "d1efa2cde9adc02169e73f07e82e06f0f7b2862b", "title": "Too Large; Data Reduction for Vision-Language Pre-Training"}, {"paperId": "c8a145ecdf84015a8a38ef21c3baa954fa84368e", "title": "The multi-modal fusion in visual question answering: a review of attention mechanisms"}, {"paperId": "31a7d8c4a5ab6bab522494b57270249105c8748e", "title": "BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"}, {"paperId": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371", "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models"}, {"paperId": "bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f", "title": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities"}, {"paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773", "title": "Evaluating Object Hallucination in Large Vision-Language Models"}, {"paperId": "f4793adffd6f67ffcb93ccfc5672ab301b8a2b96", "title": "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"}, {"paperId": "473008e002aed063257afbd9988e5cc483dec35e", "title": "PDFVQA: A New Dataset for Real-World VQA on PDF Documents"}, {"paperId": "25fe608f0eeffaf19c59e29aab5c5cd1691e5cfc", "title": "VAQA: Visual Arabic Question Answering"}, {"paperId": "80043cc6551a6b677698b6d0a946a4d3d92e8214", "title": "FVQA 2.0: Introducing Adversarial Samples into Fact-based Visual Question Answering"}, {"paperId": "b60dfaa68bd7d280c6bd2fe87595abab8e5a2a4e", "title": "Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images"}, {"paperId": "af997821231898a5f8d0fd78dad4eec526acabe5", "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"}, {"paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c", "title": "Language Is Not All You Need: Aligning Perception with Language Models"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "d0ce8506f1b9ed93574f0e804a3ba909d4458780", "title": "Champion Solution for the WSDM2023 Toloka VQA Challenge"}, {"paperId": "1b4c0a28b0c2a30cc3b84a3222e795c90357bc8a", "title": "SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images"}, {"paperId": "70feb009bc1e8b1cb8dff64bf9fd67789636438b", "title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models"}, {"paperId": "c82b1522825df6321460c7fe8ddc12c69edac57b", "title": "A Deep Learning-Based Bengali Visual Question Answering System"}, {"paperId": "cb6365a1aa3133318ce7fa2461b6d1d48cd8152e", "title": "Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning"}, {"paperId": "b287a2765e5bceb732de39dafdf70594dc9cd664", "title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"}, {"paperId": "26fd105d0b5a458979c012cddb3ba2de943388c4", "title": "Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"}, {"paperId": "77937788fdbdefe4dd775075b5768c03eaaac008", "title": "AVQA: A Dataset for Audio-Visual Question Answering on Videos"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "7344bc4c6b9880bd5240feb0601710caf5442e27", "title": "Towards Multi-Lingual Visual Question Answering"}, {"paperId": "02251886950770e82b3d68564d60cdfe15e73199", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"}, {"paperId": "1e8b23fb381e5f24964a5b85db4c8cc2ea99eafa", "title": "VizWiz-FewShot: Locating Objects in Images Taken by People With Visual Impairments"}, {"paperId": "2765cef1c00991ec0cdf37c197caf7a30b7c5c45", "title": "Visual Question Answering"}, {"paperId": "9845cfaa0adff93c9dbd3c9340db4f7c489682ea", "title": "Indic Visual Question Answering"}, {"paperId": "e73d5bb335edd5f7b7770a4b708b2169c0b6fd82", "title": "ViQuAE, a Dataset for Knowledge-based Visual Question Answering about Named Entities"}, {"paperId": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c", "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"}, {"paperId": "47a67e76ed84260ff19f7a948d764005d1edf1c9", "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"}, {"paperId": "60ee030773ba1b68eb222a265b052ca028353362", "title": "GIT: A Generative Image-to-text Transformer for Vision and Language"}, {"paperId": "a8260077135246476a0b0601495ef08e56c21a50", "title": "Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset"}, {"paperId": "aa2f5628f040b239b73ad0de1dc62d6172677fab", "title": "HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval"}, {"paperId": "f5c165b6317896a65151050201c737536fa17c31", "title": "mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections"}, {"paperId": "23b18bf32bc09b9756b49a3764f77d2d4aeb456b", "title": "Gender and Racial Bias in Visual Question Answering Datasets"}, {"paperId": "a26a7a74f1e5fd562be95c3611a0680759fbdf84", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"}, {"paperId": "7f5170b8ec68629164a98f8dfa1d2cbef5bbe5f5", "title": "All You May Need for VQA are Image Captions"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "78ad010fe89b5e54befdced487c3848242afb981", "title": "GRIT: General Robust Image Task Benchmark"}, {"paperId": "21d2f55c247f83f3290ffa5311081005458cf8fe", "title": "Towards Lightweight Transformer Via Group-Wise Transformation for Vision-and-Language Tasks"}, {"paperId": "3f968b0e35efb84a2e14449e89765239a898bb66", "title": "An analysis of graph convolutional networks and recent datasets for visual question answering"}, {"paperId": "22ef74d60e8dabf3167b8288ffb0a6faa8797736", "title": "CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations"}, {"paperId": "b611c501269224702d1a9942c8600a31ec66ab28", "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"}, {"paperId": "346fafd67f3a029a7e2ba47ead5adc489bf89827", "title": "CARETS: A Consistency And Robustness Evaluative Test Suite for VQA"}, {"paperId": "18bd22b1b6091bec3c4b8f51ef97c7f11d7f110e", "title": "CLIP Models are Few-Shot Learners: Empirical Studies on VQA and Visual Entailment"}, {"paperId": "4cb65c1541acde658a95bb8a038cefdade703da5", "title": "Video Question Answering: Datasets, Algorithms and Challenges"}, {"paperId": "24ed74ed29c057cba8b52fff4edd2c0d7f408716", "title": "VLP: A Survey on Vision-language Pre-training"}, {"paperId": "79d400814b67bc5a1749a02b2e2916bcc43ceab6", "title": "Delving Deeper into Cross-lingual Visual Question Answering"}, {"paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "455869f88df82b07ef7d5ab0dab5c28c6620daa1", "title": "Grounding Answers for Visual Questions Asked by Visually Impaired People"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "3f43b4239c6955b4c6647c0801fbbbcdea91a320", "title": "LaTr: Layout-Aware Transformer for Scene-Text VQA"}, {"paperId": "b8747e65efb8031ee32efadb396b216a903c663e", "title": "Change Detection Meets Visual Question Answering"}, {"paperId": "21ec90872abd986c12afe39bebe807732ffa70c9", "title": "Florence: A New Foundation Model for Computer Vision"}, {"paperId": "e34b699cef0a711a8cb9c39ecea20ac2df1578f5", "title": "Medical Visual Question Answering: A Survey"}, {"paperId": "118962f61df9ab6c8310d5a3eb0ab61f22802360", "title": "Language bias in Visual Question Answering: A Survey and Taxonomy"}, {"paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"}, {"paperId": "94ff111c4d81bd03f159321728ceec8b4711c89d", "title": "An Empirical Study of Training End-to-End Vision-and-Language Transformers"}, {"paperId": "9bcf3b43f2323a194036cc52c6878a9b1dc7e058", "title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning"}, {"paperId": "204c46e14aaf081af3ff104717a7ae8c41b6b806", "title": "Beyond OCR + VQA: Involving OCR into the Flow for Robust and Accurate TextVQA"}, {"paperId": "32d59ab951be74be351f9777da2cbc71bb68c3c1", "title": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models"}, {"paperId": "575dd5357f5cea38f51457c402265095e00b5fb3", "title": "Guiding Visual Question Generation"}, {"paperId": "d5a66d2599f8d133abbc46bbd30974ff026b1614", "title": "A survey of methods, datasets and evaluation metrics for visual question answering"}, {"paperId": "071440ccd1084d20d345fafd0bcaa5993f71fb04", "title": "xGQA: Cross-Lingual Visual Question Answering"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "2d2c0b2ca556f27f9adcecf636f7a2eb8f7f2a49", "title": "EDUBOT-A Chatbot For Education in Covid-19 Pandemic and VQAbot Comparison"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "9cdd420edb0c56eb4c003903fffb23b104454c42", "title": "Zero-shot Visual Question Answering using Knowledge Graph"}, {"paperId": "11f606fd65df3de5d99c0034d8dd4ec5205090f1", "title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training"}, {"paperId": "af04aebcc5e935253590537793439487d5510ebc", "title": "A Picture May Be Worth a Hundred Words for Visual Question Answering"}, {"paperId": "1902865a19947389998e3b1729a9746cdfcccd88", "title": "VQA-Aid: Visual Question Answering for Post-Disaster Damage Assessment and Analysis"}, {"paperId": "d5611a92619548e7f2af5adb04070574c0dacac1", "title": "InfographicVQA"}, {"paperId": "8866ad5f6d12fa86c689da3802882b7383a6c2c6", "title": "Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering"}, {"paperId": "809b231e915c35db47cb81abfd8600f4c0f9fa10", "title": "Kaleido-BERT: Vision-Language Pre-training on Fashion Domain"}, {"paperId": "111c844bb562ed4fac9ffe5647356345a508dbfd", "title": "Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities"}, {"paperId": "535411d279224ac4b062982ebe941abfafb43dab", "title": "A comparative study of language transformers for video question answering"}, {"paperId": "b76054d7820249fd4ebd31ed8db949a8ad8b6cbc", "title": "Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering"}, {"paperId": "3fb9e014d52a2082141acefdeaddbd81fd422b24", "title": "Visual Question Answering: which investigated applications?"}, {"paperId": "f5bffa6cda1a4e57dbfeff4a6d470669d84cf10d", "title": "Knowledge-aware Zero-Shot Learning: Survey and Perspective"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "5e5fbc41106db9acaaf3a365801051e477f0e984", "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning"}, {"paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "title": "Taming Transformers for High-Resolution Image Synthesis"}, {"paperId": "8deceb13cb3afcfbaab06a2c655f1935445635fe", "title": "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "1fe32a88a2e4162f4b3fe73ffa1fcb120bd5b1bf", "title": "Visual Grounding Via Accumulated Attention"}, {"paperId": "0b09448f7543453cc066416f547292dc1e4471f6", "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks"}, {"paperId": "18ba4e542a5206a40e308f54ceffc6786b7d94d2", "title": "Visual Question Answering on Image Sets"}, {"paperId": "b40bfcf339de3f0dba08fabb2b58b9368ff4c51a", "title": "DocVQA: A Dataset for VQA on Document Images"}, {"paperId": "2f5f81bc516a6d085d39479378af1fc27104f91e", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning"}, {"paperId": "96cdfbd9440b87a85994ba8c074bd5184ab54dd2", "title": "Change Detection Based on Artificial Intelligence: State-of-the-Art and Challenges"}, {"paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"}, {"paperId": "9b6345d70ac406bae9c24591c360680682a9af08", "title": "Visual Grounding Methods for VQA are Working for the Wrong Reasons!"}, {"paperId": "667b88984df0c5c11ac07899ffb5509185abdf57", "title": "Visual question answering: a state-of-the-art review"}, {"paperId": "598a2ee223e2949c3b28389e922c1892b4717d2a", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"}, {"paperId": "0a08cb38aa4a5a4bc2f6fa0c4d379d23e874c0b7", "title": "Visual Question Answering for Cultural Heritage"}, {"paperId": "3ba94f4dd7db8c697401aa54e63ad318423fc83d", "title": "Multi-modal Dense Video Captioning"}, {"paperId": "e0714f730d557d68e552284b3c9bd6567f116ca5", "title": "CLEVR-XAI: A benchmark dataset for the ground truth evaluation of neural network explanations"}, {"paperId": "fc0b46a0f3720e6c29c1a913aaa3de4a0699f713", "title": "PathVQA: 30000+ Questions for Medical Visual Question Answering"}, {"paperId": "8b2c80788f789d4ce7849c13943fa920d9e3c95f", "title": "Captioning Images Taken by People Who Are Blind"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "8d27640ce75557156de13fb827b64446ef9cc0e4", "title": "Visual Question Answering on 360\u00b0 Images"}, {"paperId": "91118408f8192c2addade2a0401a32c3bbd47818", "title": "Information fusion in visual question answering: A Survey"}, {"paperId": "f331d1df4627efac02d81a696151b55b1f4dc707", "title": "Assessing the Robustness of Visual Question Answering"}, {"paperId": "f1e8a99a7e17d449559b26ee0db2e8f1f47ce7ad", "title": "SpeechBERT: An Audio-and-Text Jointly Learned Language Model for End-to-End Spoken Question Answering"}, {"paperId": "4bffe09f186fa0490a852896dd6d265eedfdd548", "title": "KnowIT VQA: Answering Knowledge-Based Questions about Videos"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "2b37cc68c9819cf0f980676935007d4135d8ac8c", "title": "Clotho: an Audio Captioning Dataset"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "6648b4db5f12c30941ea78c695e77aded19672bb", "title": "Unified Vision-Language Pre-Training for Image Captioning and VQA"}, {"paperId": "c496f00f2b85b496abdc36e4dc4105541248fd3c", "title": "PlotQA: Reasoning over Scientific Plots"}, {"paperId": "e729973defae3df9cd7576f1b54266a08069ef4e", "title": "Biometric surveillance using visual question answering"}, {"paperId": "1097cf8cf5961589ff693b069002e7181e24e631", "title": "OCR-VQA: Visual Question Answering by Reading Text in Images"}, {"paperId": "1a9de6d7ef12aa5c75a55b9088d8bf249674f20a", "title": "Visual Question Answering using Deep Learning: A Survey and Performance Analysis"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "2bc1c8bd00bbf7401afcb5460277840fd8bab029", "title": "Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "59fa415c5052a2d98712d6b8f5ec5a1b61758fed", "title": "CRIC: A VQA Dataset for Compositional Reasoning on Vision and Commonsense"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "d47f20de7195fea82e48315fea52ccf06dc301d3", "title": "LEAF-QA: Locate, Encode & Attend for Figure Question Answering"}, {"paperId": "f8a48678094adbe421d61d0045361bfc635a2900", "title": "Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods"}, {"paperId": "a8c9f671d43bb73c670d65e12d5c9df55a82e66e", "title": "A Systematic Review of Studies on Educational Robotics"}, {"paperId": "8a1744da011375d711ed75fc2d160c6fdca2cf89", "title": "Deep Modular Co-Attention Networks for Visual Question Answering"}, {"paperId": "b4cb47cdfa26f59d4216e826bad6ecc4e4cb9047", "title": "VizWiz-Priv: A Dataset for Recognizing the Presence and Purpose of Private Visual Information in Images Taken by Blind People"}, {"paperId": "0033346700dc450ac22c9b704eab0e906d868662", "title": "Scene Text Visual Question Answering"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "e3bb5773205477ae4711524a9d4ae739bee40349", "title": "Challenges and Prospects in Vision and Language Research"}, {"paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907", "title": "Towards VQA Models That Can Read"}, {"paperId": "5dcd897887329bce0ce132a06efc9ead64896909", "title": "Reasoning"}, {"paperId": "d0bfd3cb732471a0843a39d2d047caf60a844466", "title": "RAVEN: A Dataset for Relational and Analogical Visual REasoNing"}, {"paperId": "fb751f1d2ac984670a57663aec80ee8b4ac189d6", "title": "CLEVR-Dialog: A Diagnostic Dataset for Multi-Round Reasoning in Visual Dialog"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "3c54b796cc10cb530f77caa4d18e1c80ac863822", "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding"}, {"paperId": "9695676deace8c05d4e95274b92f20ed1e97470c", "title": "CLEVR-Ref+: Diagnosing Visual Reasoning With Referring Expressions"}, {"paperId": "ca5241c5d120fc793512c9988ea22654aa6d8823", "title": "Gamification of a Visual Question Answer System"}, {"paperId": "a678b68abd4047d5342f64725f57a04647a47711", "title": "From Known to the Unknown: Transferring Knowledge to Answer Questions about Novel Visual and Semantic Concepts"}, {"paperId": "6dfc2ff03534a4325d06c6f88c3144831996629b", "title": "From Recognition to Cognition: Visual Commonsense Reasoning"}, {"paperId": "634161e4759616dbe06f0b1465999d3df122f366", "title": "TallyQA: Answering Complex Counting Questions"}, {"paperId": "7e27d44e3fac723ccb703e0a83b22711bd42efe8", "title": "A Comprehensive Survey of Deep Learning for Image Captioning"}, {"paperId": "4921243268c81d0d6db99053a9d004852225a622", "title": "Object Hallucination in Image Captioning"}, {"paperId": "e7e1313061b0d56364bd2c41f017deb954bb05db", "title": "TVQA: Localized, Compositional Video Question Answering"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "391af839051826ec317a6ea61010734baf536551", "title": "Question-Guided Hybrid Convolution for Visual Question Answering"}, {"paperId": "02c409f51bb4ee0b31375447a2b9b64fbe7427d4", "title": "Word-to-region attention network for visual question answering"}, {"paperId": "afe3a0d463e2f099305c745ddbf943844583795d", "title": "Learning Visual Question Answering by Bootstrapping Hard Attention"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "1bfc74bad04b407d1792a70d73a3f5dc0be0506d", "title": "Cross-Dataset Adaptation for Visual Question Answering"}, {"paperId": "d2de5d94461d66e6b97e6825ae0fea3d6d925382", "title": "R-VQA: Learning Visual Relation Facts with Semantic Attention for Visual Question Answering"}, {"paperId": "a5d10341717c0519cf63151b496a6d2ed67aa05f", "title": "Bilinear Attention Networks"}, {"paperId": "f7cc85bed2a3d0b0ef1c0e0258f5b60ee4bb4622", "title": "Improved Fusion of Visual and Language Representations by Dense Symmetric Co-attention for Visual Question Answering"}, {"paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"}, {"paperId": "dd2f8bb5fa881797fad0448547e307a18bf897da", "title": "Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions"}, {"paperId": "7289a240c9425bc7cad87b3b835e5f0cac22f488", "title": "DVQA: Understanding Data Visualizations via Question Answering"}, {"paperId": "90873a97aa9a43775e5aeea01b03aea54b28bfbd", "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering"}, {"paperId": "0605a012aeeee9bef773812a533c4f3cb7fa5a5f", "title": "Interpretable Counting for Visual Question Answering"}, {"paperId": "f3aa42c3e441b335619f1038e566831feb9a2941", "title": "An educational robot system of visual question answering for preschoolers"}, {"paperId": "cc5ac1d3083b6663482ba6830dfa3bf65343286c", "title": "Embodied Question Answering"}, {"paperId": "9a6268d2bc1221ea154097feadea0c58f234d02f", "title": "Co-attending Free-form Regions and Detections with Multi-modal Multiplicative Feature Embedding for Visual Question Answering"}, {"paperId": "a26d01dc2b9a4e435c015914690354f67a9b6a08", "title": "A Novel Framework for Robustness Analysis of Visual QA Models"}, {"paperId": "141c9f6817331a6f3cccf82ebda5c8fd66c88b98", "title": "Visual Question Answering: A Tutorial"}, {"paperId": "057b80e235b10799d03876ad25465208a4c64caf", "title": "Video Question Answering via Gradually Refined Attention over Appearance and Motion"}, {"paperId": "8cc4a4bc7f9a989130d5ac129d24426a444fe161", "title": "Recent Advances in Zero-Shot Recognition: Toward Data-Efficient Understanding of Visual Content"}, {"paperId": "2125074798ad4110f7310ef9e9bfb172e60774cb", "title": "iVQA: Inverse Visual Question Answering"}, {"paperId": "0e23229289b1fbea14bc425718bc0a227d100b8e", "title": "Survey of Recent Advances in Visual Question Answering"}, {"paperId": "0c0f41d3162e76500d4639557ff4463bd246e395", "title": "Beyond Bilinear: Generalized Multimodal Factorized High-Order Pooling for Visual Question Answering"}, {"paperId": "b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81", "title": "Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge"}, {"paperId": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8", "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"}, {"paperId": "c071a1ad68310fed7f0876b6f01cb7b135043bc3", "title": "Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension"}, {"paperId": "a9e28863c7fb963b40a379c5a4e0da00eb031933", "title": "A Corpus of Natural Language for Visual Reasoning"}, {"paperId": "d740d0a960368633ed32fc84877b8391993acdca", "title": "Multi-level Attention Networks for Visual Question Answering"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "title": "Multimodal Machine Learning: A Survey and Taxonomy"}, {"paperId": "fe466e84fa2e838adc3c37ee327cd68004ae08fe", "title": "MUTAN: Multimodal Tucker Fusion for Visual Question Answering"}, {"paperId": "637648198f9e91654ce27eaaa40512f2dc870fc1", "title": "Survey of Visual Question Answering: Datasets and Techniques"}, {"paperId": "25936a0b60af41d0679afcfc0c23ffed91ab3243", "title": "CrowdVerge: Predicting If People Will Agree on the Answer to a Visual Question"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "b2f521c02c6ed3080c5fe123e938cdf4555e6fd2", "title": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering"}, {"paperId": "d674b540dcd968bc302ea4360df3f4e85e994b55", "title": "Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering"}, {"paperId": "46801d3a6053261a3af362de35ac0015fdd6d881", "title": "The Relationship Between Professional Burnout and Quality and Safety in Healthcare: A Meta-Analysis"}, {"paperId": "915b5b12f9bdebc321e970ecd713458c3479d70e", "title": "An Analysis of Visual Question Answering Algorithms"}, {"paperId": "03eb382e04cca8cca743f7799070869954f1402a", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"}, {"paperId": "7d52fa1a9021d3596930ad2d5121e9d125113ab2", "title": "The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions"}, {"paperId": "00672bec9dd0e755f9c375f912c0f1c3918a6b74", "title": "MarioQA: Answering Questions by Watching Gameplay Videos"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "1b80416cc2b05954941ac7e7dcbcc358c10e5ace", "title": "Visual Dialog"}, {"paperId": "778ce81457383bd5e3fdb11b145ded202ebb4970", "title": "Semantic Compositional Networks for Visual Captioning"}, {"paperId": "8a8224266b8ab1483f6548307ab96227147f34da", "title": "Zero-Shot Visual Question Answering"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "f651593fa6c83d717fc961482696a53b6fca5ab5", "title": "Dual Attention Networks for Multimodal Reasoning and Matching"}, {"paperId": "0d0a05fa74dd5ed8aed59bd7a92420e034d64cf1", "title": "FigureSeer: Parsing Result-Figures in Research Papers"}, {"paperId": "6d92ce1c4f7f0ccfe068e663903e4dd614a15ede", "title": "Visual question answering: Datasets, algorithms, and future challenges"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "f8645f298cc67a7ab751488f3945dc1beaffe8da", "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering Models"}, {"paperId": "95d3001ed7782fecea29bdb41e598aa5b41a615b", "title": "A Shared Task on Multimodal Machine Translation and Crosslingual Image Description"}, {"paperId": "6721e26e65c6b72dafae74fd1e7a6f2e6023a312", "title": "A Comprehensive Survey on Cross-modal Retrieval"}, {"paperId": "88c307c51594c6d802080a0780d0d654e2e2891f", "title": "Visual question answering: A survey of methods and datasets"}, {"paperId": "ebe5081b8a24b4740db929b6eae75f28f8edbc58", "title": "Answer-Type Prediction for Visual Question Answering"}, {"paperId": "b60630911d7746fba06de7c34abe98c9a61c6bcc", "title": "FVQA: Fact-Based Visual Question Answering"}, {"paperId": "12f7de07f9b00315418e381b2bd797d21f12b419", "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"}, {"paperId": "1afb710a5b35a2352a6495e4bf6eef66808daf1b", "title": "Multimodal Residual Learning for Visual QA"}, {"paperId": "fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b", "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering"}, {"paperId": "05f3f8f6f97db00bafa2efd2ac9aac570603c0c6", "title": "TGIF: A New Dataset and Benchmark on Animated GIF Description"}, {"paperId": "7214daf035ab005b3d1e739750dd597b4f4513fa", "title": "A Focused Dynamic Attention Model for Visual Question Answering"}, {"paperId": "e18ec2c9f0b4a817b8cf0435822bbc879d7db698", "title": "A Diagram is Worth a Dozen Images"}, {"paperId": "8ae09bb88506aa667ac01642f0cbc9dbb30a628d", "title": "Generating Natural Questions About an Image"}, {"paperId": "6acd385b2742f65359efb99543ebfb9a0d1b850f", "title": "Image Captioning and Visual Question Answering Based on Attributes and External Knowledge"}, {"paperId": "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "title": "Dynamic Memory Networks for Visual and Textual Question Answering"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "98ea4abc9bf0e30eb020db2075c9c8a039a848a3", "title": "Learning to Compose Neural Networks for Question Answering"}, {"paperId": "b8e2e9f3ba008e28257195ec69a00e07f260131d", "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7", "title": "MovieQA: Understanding Stories in Movies through Question-Answering"}, {"paperId": "267fb4ac632449dbe84f7acf17c8c7527cb25b0d", "title": "Simple Baseline for Visual Question Answering"}, {"paperId": "175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22", "title": "Where to Look: Focus Regions for Visual Question Answering"}, {"paperId": "20dbdf02497aa84510970d0f5e8b599073bca1bc", "title": "Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge from External Sources"}, {"paperId": "385c18cc4024a3b3206c508c512e037b9c00b8f3", "title": "Image Question Answering Using Convolutional Neural Network with Dynamic Parameter Prediction"}, {"paperId": "1cf6bc0866226c1f8e282463adc8b75d92fba9bb", "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering"}, {"paperId": "5fa973b8d284145bf0ced9acf2913a74674260f6", "title": "Yin and Yang: Balancing and Answering Binary Visual Questions"}, {"paperId": "def584565d05d6a8ba94de6621adab9e301d375d", "title": "Visual7W: Grounded Question Answering in Images"}, {"paperId": "0b0a1cd432413978e4ef3d0418ebf3bb07af6c7a", "title": "Explicit Knowledge-based Reasoning for Visual Question Answering"}, {"paperId": "21c99706bb26e9012bfb4d8d48009a3d45af59b2", "title": "Neural Module Networks"}, {"paperId": "2c1890864c1c2b750f48316dc8b650ba4772adc5", "title": "Stacked Attention Networks for Image Question Answering"}, {"paperId": "452059171226626718eb677358836328f884298e", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"}, {"paperId": "56ffece2817a0363f551210733a611830ba1155d", "title": "Aligning where to see and what to tell: image caption with region-based attention and scene factorization"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "98bd5dd1740f585bf25320ba504e2c1ae57f2e5f", "title": "Learning to Answer Questions from Image Using Convolutional Neural Network"}, {"paperId": "a58a582b95a07932cb248f1b739e4ad739ead6b9", "title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering"}, {"paperId": "2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1", "title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question"}, {"paperId": "62a956d7600b10ca455076cd56e604dfd106072a", "title": "Exploring Models and Data for Image Question Answering"}, {"paperId": "bd7bd1d2945a58cdcc1797ba9698b8810fe68f60", "title": "Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "7ffdbc358b63378f07311e883dddacc9faeeaf4b", "title": "Fast R-CNN"}, {"paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, {"paperId": "7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f", "title": "Multiple Object Recognition with Visual Attention"}, {"paperId": "ac3ee98020251797c2b401e1389461df88e52e62", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, {"paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "title": "Show and tell: A neural image caption generator"}, {"paperId": "3d29e1c4f1c2b079cf6b5dd458fa6cee246955f9", "title": "Towards a Visual Turing Challenge"}, {"paperId": "ac64fb7e6d2ddf236332ec9f371fe85d308c114d", "title": "A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input"}, {"paperId": "dab7e605237ad4f4fe56dcba2861b8f0a57112be", "title": "Wikidata"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "19b505d471c88708de949dffd788a3529b66d7c8", "title": "Acquiring Comparative Commonsense Knowledge from the Web"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "44040913380206991b1991daf1192942e038fe31", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}, {"paperId": "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"}, {"paperId": "2be456d47795df387b4ca9273e17a785ee184e67", "title": "Answering visual questions with conversational crowd assistants"}, {"paperId": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "title": "Linguistic Regularities in Continuous Space Word Representations"}, {"paperId": "1ad8eacf71dda8fcfbf5ed0e1b86f5bd3b3d0ca0", "title": "Visual challenges in the everyday lives of blind people"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "6539bd0b923c54c7c79281ab1821d09c65c92c6e", "title": "Crowdsourcing subjective fashion advice using VizWiz: challenges and opportunities"}, {"paperId": "c1994ba5946456fc70948c549daf62363f13fa2d", "title": "Indoor Segmentation and Support Inference from RGBD Images"}, {"paperId": "398c296d0cc7f9d180f84969f8937e6d3a413796", "title": "Multi-column deep neural networks for image classification"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": "8a960ea40fe7b32a1ee702a84f64ec1de5c3e7fe", "title": "VizWiz: nearly real-time answers to visual questions"}, {"paperId": "b183eede721c9b6ed649365b23861b51f9a1669d", "title": "Raven's Progressive Matrices"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "68116b384f440acb2e8219e4b7e7c9e9007f0c37", "title": "Green computing"}, {"paperId": "2b2c30dfd3968c5d9418bb2c14b2382d3ccc64b2", "title": "DBpedia: A Nucleus for a Web of Open Data"}, {"paperId": "e8b12467bdc20bde976750b8a28decdb33246d1d", "title": "Histograms of oriented gradients for human detection"}, {"paperId": "69897a2986da1719fea53232630810a4febb6eeb", "title": "An ontology approach to object-based image retrieval"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "bfab4ffa229c8af0174a683ff1eda524c4f59d00", "title": "Can artificial neural networks learn language models?"}, {"paperId": "f9f836d28f52ad260213d32224a6d227f8e8849a", "title": "Object recognition from local scale-invariant features"}, {"paperId": "0479efee5558edd13115f37dac41ccd9272138d4", "title": "Image feature extraction by sparse coding and independent component analysis"}, {"paperId": "6ee0b719a7056e2ecc6722f6fa7480bdb2c76195", "title": "Feature extraction using wavelet transform for neural network based image classification"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "52b7bf3ba59b31f362aa07f957f1543a29a4279e", "title": "Support-Vector Networks"}, {"paperId": "26d172f0a4d7e903ce388f3159059f9c5463e5c5", "title": "Algebraic feature extraction of image for recognition"}, {"paperId": "4f9c70de38568aabaa440961651820b949e35b8d", "title": "Optical Character Recognition"}, {"paperId": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "title": "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability"}, {"paperId": "8112c4305b88d85199267e9e03d3a0aca4432059", "title": "The approximation of one matrix by another of lower rank"}, {"paperId": "94d5f555219cf3a657b3fb80325bbb7624f42176", "title": "Zur Theorie der orthogonalen Funktionensysteme"}, {"paperId": "eaed51bdcb4f4000734cc596f8a16fb1dce9862c", "title": "Asymmetric Siamese Networks for Semantic Change Detection in Aerial Images"}, {"paperId": null, "title": "A con-vnet for the 2020s"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "b76e98a0a023d37c6534aa2ead09c8ff595f0bae", "title": "A Robustly Optimized BERT Pre-training Approach with Post-training"}, {"paperId": null, "title": "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale"}, {"paperId": null, "title": "els are few-shot learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "589951bd421e2b701225fe6626fe980d94ad2770", "title": "Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task"}, {"paperId": "880760777e3671593ba50b7a17b0d30b655fc86d", "title": "Visual Question Answering : Datasets , Methods , Challenges and Oppurtunities"}, {"paperId": "18f9a6045ba01cb079c4fa49a630d71bbd27cd92", "title": "Descriptor : A dataset of clinically generated visual questions and answers about radiology images"}, {"paperId": "ec99eca7c86601b8c909147f8caa8c35975bc44e", "title": "Contextual Inter-modal Attention for Multi-modal Sentiment Analysis"}, {"paperId": null, "title": "Yfcc100m: The new data in multimedia research"}, {"paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3", "title": "ALVINN, an autonomous land vehicle in a neural network"}, {"paperId": null, "title": "Artificial general intelligence, volume 2"}, {"paperId": null, "title": "Political journalism: New challenges"}, {"paperId": "8c8543f707d1ee56692deefb8f777dc88f2e3869", "title": "A comparative study of neural network based feature extraction paradigms"}, {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4", "title": "Gradient-based learning applied to document recognition"}, {"paperId": "402627e4eb8c95e4aae3026fd921aa08cd792006", "title": "Contextual correlates of semantic similarity"}, {"paperId": "9b2541b8d8ca872149b4dabd2ccdc0cacc46ebf5", "title": "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition"}, {"paperId": null, "title": "Learning internal"}, {"paperId": null, "title": "The influence of pattern similarity and transfer learning upon training of a base perceptron b2"}, {"paperId": null, "title": "Hadamard product for low-rank bilinear pooling"}]}