{"paperId": "a8427ce5aee6d62800c725588e89940ed4910e0d", "title": "An Attentive Survey of Attention Models", "abstract": "Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy that groups existing techniques into coherent categories. We review salient neural architectures in which attention has been incorporated and discuss applications in which modeling attention has shown a significant impact. We also describe how attention has been used to improve the interpretability of neural networks. Finally, we discuss some future research directions in attention. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.", "venue": "ACM Transactions on Intelligent Systems and Technology", "year": 2019, "citationCount": 539, "influentialCitationCount": 22, "openAccessPdf": {"url": "https://arxiv.org/pdf/1904.02874", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A taxonomy that groups existing techniques into coherent categories in attention models is proposed, and how attention has been used to improve the interpretability of neural networks is described."}, "embedding": {"model": "specter_v2", "vector": [0.13648803532123566, 0.7280951738357544, -0.22628265619277954, -0.14625383913516998, 0.0694582536816597, 0.539664089679718, 0.4343646764755249, -0.4179745316505432, -0.3815593421459198, -0.4368017911911011, 0.54824298620224, 0.7503931522369385, 0.2314663827419281, 0.11839450895786285, -0.24376316368579865, 0.24883687496185303, -0.5664290189743042, -0.06962209194898605, 0.26639553904533386, -0.09324178844690323, 0.302780419588089, -0.7227887511253357, -1.4736323356628418, 0.4582507014274597, -0.27724194526672363, 0.9850738644599915, 0.5507045388221741, 0.9682859182357788, 0.02994224801659584, 0.7850409150123596, 0.18893101811408997, -0.23055674135684967, -0.11166702210903168, 0.06678569316864014, -0.3295731842517853, -0.3368719220161438, 1.007550597190857, -0.2561112344264984, -0.8006985783576965, 0.9591015577316284, -0.2894188463687897, 0.45911696553230286, 0.37597203254699707, -0.7777072787284851, -0.07164014130830765, 0.6130973696708679, 0.4479828178882599, 1.3735787868499756, -0.4852834641933441, -0.5671001076698303, 1.2374752759933472, -1.0424327850341797, 0.016986917704343796, 2.0720579624176025, 0.24431507289409637, 0.4547819197177887, -0.19427737593650818, -0.4648015797138214, 1.4506961107254028, 0.12218896299600601, -0.6113297939300537, -0.3048422932624817, 0.5475382208824158, -0.16570037603378296, 1.7780945301055908, -0.3581739366054535, -0.007088833022862673, 0.5545380711555481, 0.5150757431983948, 1.6401336193084717, -0.16384653747081757, -1.1187478303909302, -0.07319550216197968, 0.2941861152648926, 0.8802324533462524, 0.40561336278915405, -0.42348015308380127, 0.06347135454416275, -1.0334306955337524, -0.09565748274326324, 0.9360364675521851, -0.21205255389213562, -0.10708976536989212, -0.12716034054756165, -0.24068374931812286, 0.8403027653694153, 0.6798053979873657, 0.5361217260360718, -0.890778124332428, 0.9527575969696045, 0.08004916459321976, 0.13110694289207458, -0.3995557427406311, 0.42765358090400696, 0.5874330997467041, 0.5616422891616821, -0.5522286891937256, -0.1952512115240097, -0.24907028675079346, 1.2309496402740479, -0.1742178350687027, 0.6537147760391235, -0.5569033026695251, 0.2786552906036377, 1.4838178157806396, -0.20218950510025024, 0.20036101341247559, -0.8519518375396729, 0.213412806391716, -0.69745272397995, 0.13193728029727936, -1.3012738227844238, 0.1617499142885208, -0.47935950756073, -0.26123836636543274, -0.5590851306915283, -0.4061819612979889, 0.22001749277114868, -0.5236926078796387, 0.7611925005912781, -0.685265839099884, -0.3668038845062256, -0.44861871004104614, 0.5855690836906433, 0.04181830957531929, 0.36804139614105225, 0.7381590604782104, 0.5427344441413879, 1.2156836986541748, -0.8333812355995178, -0.9374647736549377, -1.3061764240264893, -0.4371664226055145, -0.26362916827201843, 0.04028625786304474, -0.1537458896636963, -1.1712920665740967, -1.183483362197876, -1.0264511108398438, -0.16296592354774475, -0.3020508587360382, 0.22333016991615295, 1.1492478847503662, -0.18478359282016754, -1.089135766029358, 0.7870718836784363, -0.183518648147583, -0.31445690989494324, 0.512726366519928, 0.6646991968154907, 0.5219302773475647, 0.13188250362873077, -0.8597466349601746, 0.44298055768013, 0.06294053047895432, -0.6025024056434631, -0.12998764216899872, -0.06313183158636093, -1.0474519729614258, 0.02781681902706623, 0.014748852699995041, -0.8668678998947144, 1.1294790506362915, -1.174538016319275, -0.6968801021575928, 0.4732532501220703, -0.769633948802948, -0.011999883688986301, -0.5259900093078613, -0.45760002732276917, -0.8957177996635437, 0.10735158622264862, -0.3092266321182251, 0.962511420249939, 0.6799644827842712, -0.3332490921020508, -0.493714302778244, -0.023750076070427895, -0.1225401982665062, -0.20392313599586487, -0.5175728797912598, 1.333615779876709, -0.5284812450408936, -0.4644603431224823, 0.34034961462020874, 0.8412680625915527, 0.5328918099403381, 0.02107223868370056, 0.04141918942332268, -0.9223091006278992, 0.6537589430809021, 0.2034279704093933, 1.2639671564102173, -0.7062288522720337, -0.7107268571853638, -0.2864963114261627, -0.08776171505451202, -0.28111499547958374, -0.85740727186203, 0.17559103667736053, -0.7682595252990723, 0.002627581125125289, 0.22109414637088776, -0.8938331604003906, -0.28188586235046387, -0.30214595794677734, -0.599947452545166, -0.16111284494400024, 0.30224549770355225, 0.9810774922370911, -0.5994195938110352, -0.044989801943302155, -0.12969286739826202, 0.09212251752614975, -0.6804786920547485, 1.2311936616897583, -0.22457852959632874, -0.1564570814371109, -0.08846093714237213, -0.1417922079563141, -0.08310374617576599, -0.32636815309524536, -0.2217075079679489, -0.5686925649642944, -0.032591450959444046, 0.40214845538139343, -0.4073227643966675, 0.9857672452926636, -0.12563878297805786, 1.2951836585998535, 0.2001906782388687, -0.5508089661598206, 0.11339845508337021, 0.42703571915626526, -0.4991207420825958, -0.3849967420101166, 0.6234663128852844, -0.3342144191265106, -0.41360369324684143, -0.01573053188621998, 0.43961647152900696, 1.2860885858535767, -0.03092120960354805, 0.2710431218147278, 0.641792893409729, 0.02089613862335682, -0.11572395265102386, 0.20551422238349915, 0.18628013134002686, 0.2594173550605774, 0.33517780900001526, -0.26187193393707275, -0.055744364857673645, -1.0051692724227905, -0.013830844312906265, 0.662872314453125, 0.5309364795684814, 0.9285839200019836, 0.313301682472229, -1.1329002380371094, 0.0670803114771843, 0.06419751048088074, 0.7185741066932678, 1.5916717052459717, -0.04249929264187813, 0.21321235597133636, -0.6307019591331482, -0.13504952192306519, -0.4587531089782715, 0.36196619272232056, -0.8798416256904602, -0.36202043294906616, -0.4964611232280731, -0.7559654116630554, 0.48808297514915466, 0.7680246233940125, 0.7420292496681213, -1.104459524154663, -0.7388379573822021, -0.18592165410518646, 0.712566614151001, -0.5168676972389221, -0.9539096355438232, 0.4422534704208374, -0.6865281462669373, -0.44677528738975525, 0.2604924738407135, -0.2910080552101135, 0.47898179292678833, -0.37770187854766846, 1.1010074615478516, -0.43485379219055176, 0.0529765784740448, 0.22398924827575684, 1.1877624988555908, -1.3585284948349, 0.1082390770316124, 0.10171370208263397, 0.140623539686203, 0.27159422636032104, 0.6993685364723206, 0.474369615316391, -0.21281753480434418, 0.3470149338245392, -0.4658242166042328, 0.015547669492661953, 0.1947236806154251, -0.12492489814758301, 0.6845419406890869, -0.4677793085575104, 0.43434861302375793, -1.0867583751678467, 0.7696155309677124, -0.007025783881545067, -0.46757230162620544, 0.0766405537724495, -0.447581946849823, -0.28255990147590637, 0.0066598160192370415, -0.5611552000045776, -0.6499056220054626, -0.6760929226875305, 0.6160871386528015, -0.6509034633636475, -0.619455873966217, 0.26244598627090454, 0.5214863419532776, -0.35839763283729553, 0.364982932806015, -0.00974320899695158, 0.30230456590652466, 0.17262493073940277, -0.2788148522377014, -0.7567976713180542, 0.597374439239502, 0.5124695897102356, -0.5063207149505615, 0.005119767505675554, -0.34279710054397583, -0.8635694980621338, -0.6752415299415588, -0.4126752018928528, -0.05143441632390022, -0.6596283912658691, -0.09466014057397842, -0.393065482378006, -0.7803816795349121, -0.04844963550567627, -1.1377242803573608, -0.22012850642204285, 0.2578437030315399, 0.27270984649658203, -0.14459508657455444, -1.239682674407959, -0.7815446257591248, -0.918786346912384, -0.3388079106807709, -0.6176219582557678, -0.21856260299682617, 0.7369483709335327, -0.7986329197883606, -0.49414291977882385, -0.2926890552043915, -0.4279760420322418, 1.2129524946212769, -0.5420425534248352, 0.8648234009742737, 0.02132556028664112, -0.23506878316402435, -0.5779969096183777, 0.3128383457660675, 0.23948775231838226, -0.0382477231323719, 0.09448233991861343, -1.3581573963165283, 0.36500757932662964, 0.1561642438173294, -0.03593837097287178, 0.3434509336948395, 0.7248832583427429, 1.1551560163497925, 0.14882558584213257, -0.4266945421695709, 0.17050908505916595, 1.1804696321487427, -0.4568270146846771, -0.09908615052700043, 0.2236432433128357, 0.7084258198738098, 0.6294777393341064, -0.23239953815937042, 0.19052457809448242, 0.43349769711494446, 0.752069354057312, 0.8450657725334167, -0.0617535263299942, -0.4434340000152588, -0.40727946162223816, -0.14736329019069672, 0.9859102368354797, -0.20821323990821838, -0.030283713713288307, -0.8667297959327698, 0.8731421232223511, -1.4244441986083984, -0.8514268398284912, 0.8395647406578064, 1.0307506322860718, 0.047952182590961456, -0.13104145228862762, -0.34886041283607483, -0.5728248357772827, 1.2031009197235107, 0.2131471186876297, -0.5829849243164062, -0.6880416870117188, -0.11489319801330566, 0.16819776594638824, -0.22401615977287292, 0.8798221349716187, -0.7951704263687134, 0.4496619999408722, 14.517433166503906, 0.08992025256156921, -0.2165059745311737, 0.32466748356819153, 0.486009806394577, 0.1066817045211792, -0.18871362507343292, -0.3740139603614807, -1.207640528678894, 0.1250848025083542, 0.8634181022644043, 0.44874581694602966, 0.513805627822876, 0.3998262286186218, -0.26940909028053284, -0.22958676517009735, -0.5741893649101257, 0.8674982190132141, 0.7520392537117004, -0.9502583742141724, 0.4313596785068512, 0.12864293158054352, 0.06468715518712997, 0.3836597204208374, 0.7823753952980042, 0.6474869251251221, 0.16523416340351105, -0.40316352248191833, 0.6023745536804199, 0.8755407929420471, 0.21653921902179718, -0.051917776465415955, 0.41573840379714966, 0.49942830204963684, -0.8736746907234192, -0.4156390130519867, -0.7885839939117432, -1.2207274436950684, 0.13349419832229614, -0.4141143560409546, 0.16929715871810913, -0.842464804649353, 0.04456498846411705, 0.18595293164253235, -0.24758096039295197, 0.7760399580001831, -0.44220900535583496, 0.4561949074268341, -0.24424366652965546, -0.3733205199241638, 0.1530236154794693, 0.9347426295280457, 0.43137967586517334, -0.0009789115283638239, -0.2601128816604614, 0.27701982855796814, -0.24748025834560394, 0.6886731386184692, -0.3729167580604553, -0.35010388493537903, -0.2050480991601944, 0.05106787383556366, 0.18467168509960175, 0.6609760522842407, 0.8081900477409363, 0.46422913670539856, -0.09091971069574356, -0.03204680234193802, 0.4187697470188141, 0.6517874002456665, -0.17482198774814606, -0.5044464468955994, 0.31578904390335083, -0.17315873503684998, 0.18294142186641693, 0.8291537165641785, -0.33778777718544006, -0.19250576198101044, -0.8055532574653625, 0.1536337435245514, 0.7325581312179565, -1.4260001182556152, -0.6113329529762268, 1.2454729080200195, -0.09041819721460342, 0.0025522189680486917, 0.36352017521858215, -0.87832111120224, -0.6391198039054871, 0.7638071179389954, -1.2441164255142212, -0.4867989420890808, -0.3336828947067261, -0.08109056949615479, 0.02807851880788803, -0.2849877178668976, 1.1977628469467163, -0.27284666895866394, -0.6277607083320618, 0.09060441702604294, -0.7210830450057983, -0.2420075684785843, -0.054920367896556854, -0.6665504574775696, 0.26072901487350464, 0.456329882144928, -0.11190449446439743, 0.6895043849945068, -0.04064110293984413, 0.2131229192018509, -0.29925480484962463, -0.021661803126335144, 0.7722070217132568, -0.7097084522247314, -0.4706665873527527, -0.2897894084453583, -1.192445158958435, 0.6555966734886169, 1.163040041923523, -0.1680315136909485, 0.21727754175662994, 0.4068142771720886, -0.47444161772727966, -0.21102474629878998, -0.5497047305107117, 0.08360786736011505, -0.05661030113697052, -0.6030360460281372, -1.0791923999786377, -0.6997193694114685, -0.09150302410125732, -0.4425513744354248, -0.11500631272792816, -0.6547934412956238, -0.011923183687031269, -0.20532526075839996, 0.8913178443908691, -0.8146495819091797, 0.17659834027290344, 0.4743328094482422, -0.01776338741183281, -0.795424222946167, -0.8614498972892761, -0.5838046669960022, -0.2525791525840759, 0.009940359741449356, 0.4402175843715668, -0.554118275642395, 0.11653182655572891, 1.1481140851974487, -0.0017998621333390474, -0.22771231830120087, -0.7869073152542114, 0.22681841254234314, -0.3598952293395996, -0.38664305210113525, 0.0988275334239006, -0.17937855422496796, -0.3507201671600342, 0.2059658169746399, 0.5963969230651855, 0.6565411686897278, -0.10780245065689087, -0.6003136038780212, 0.11204291135072708, -0.4929044842720032, 0.11927199363708496, -1.035866618156433, -0.3128295838832855, -1.6099506616592407, 0.008269262500107288, -0.5946969985961914, -0.10261519998311996, -1.0559899806976318, -0.7223104238510132, 0.38584327697753906, -0.7016416788101196, 0.379125714302063, 0.32802119851112366, -0.4791630804538727, -0.3375071585178375, -0.2354411631822586, -0.6471189856529236, 0.45792168378829956, 0.6326550841331482, -0.5514981150627136, 0.21144366264343262, 0.19219084084033966, -0.23347583413124084, 0.7823976874351501, 0.6728222966194153, -0.6199626922607422, -0.5962820649147034, -1.6957206726074219, 0.1865370273590088, -0.3973517119884491, 0.031689729541540146, -1.2672946453094482, 1.4535531997680664, 0.4759259521961212, 0.33181437849998474, -0.1424030363559723, 0.9032889604568481, -0.9964043498039246, -0.6045195460319519, 0.05105197802186012, -0.8327696919441223, 0.2100650668144226, 0.5330828428268433, -0.23448872566223145, -0.5823962688446045, 0.5626120567321777, 0.25893905758857727, -0.933434009552002, -0.8724647760391235, 0.30924656987190247, -0.9084155559539795, 0.07083506137132645, 0.04411572217941284, -0.4800378680229187, -1.3052088022232056, -0.3735836148262024, -0.16718950867652893, 0.2733650207519531, -0.5815872550010681, 0.9800173044204712, 0.7372888922691345, -1.0958718061447144, 0.2958693206310272, 0.6508858799934387, -0.01708889752626419, -0.2597377896308899, 0.23255540430545807, 0.46582940220832825, 0.05203472822904587, 0.8341055512428284, -0.19143030047416687, 0.17858244478702545, -0.6863221526145935, -0.0028241651598364115, 0.9809856414794922, 0.48080551624298096, 0.1475871503353119, 1.2729357481002808, 0.10459136217832565, -0.5183308720588684, 0.7047639489173889, -0.9459797143936157, -1.0345849990844727, 0.10520672798156738, 0.959587812423706, 0.053458768874406815, -0.22022852301597595, -0.4448799192905426, -0.35035184025764465, 0.42181435227394104, 0.15422387421131134, -0.32305628061294556, 0.17216309905052185, -0.23263749480247498, -0.15651586651802063, 0.9954850673675537, 0.3703773617744446, -0.8146623969078064, -1.1757874488830566, -0.5040993094444275, -0.2668846547603607, -0.20051001012325287, 0.3951381742954254, -0.4137214720249176, -1.103633999824524, 1.1681183576583862, 0.5291722416877747, 0.17479823529720306, 0.059998735785484314, -0.053950294852256775, -0.8167867660522461, 0.435746967792511, 0.028119713068008423, -0.5717095136642456, -0.3982422351837158, 1.1978052854537964, 1.667325735092163, -0.7267875075340271, 0.1690247654914856, -0.35768523812294006, -0.6528725028038025, 1.162786841392517, 0.791029155254364, -0.430419385433197, 0.8892334699630737, -0.7414659857749939, 0.18021413683891296, 0.05535614863038063, -1.113662600517273, 0.16954703629016876, 1.096713662147522, 1.0654263496398926, 0.5001494288444519, 0.1413714736700058, 0.5469085574150085, 1.078677773475647, 0.2759045660495758, -0.12356584519147873, 0.3401890993118286, 0.23968808352947235, -0.5568335056304932, 0.49136534333229065, -0.2505147159099579, 0.6706053018569946, -0.7692444920539856, -0.3013956546783447, 0.07716485112905502, 1.0246104001998901, 0.022573458030819893, 0.839569628238678, 1.015002727508545, 0.3656679093837738, 0.6790834069252014, 0.15425002574920654, 0.4245651960372925, -0.7269144058227539, -0.5561686158180237, 0.12027174234390259, -0.4181295335292816, -0.09080354869365692, -0.4785436689853668, -0.7895686030387878, 0.2956138849258423, 0.5082101821899414, 0.06195732578635216, -0.14268983900547028, 0.3411795496940613, 0.5835949778556824, 0.8216418027877808, 0.8244972825050354, -0.5413388609886169, -0.4311571419239044, -0.002195741981267929, -1.4117511510849, 0.14023053646087646, -0.9056742787361145, -0.10191719979047775, -0.6268558502197266, -0.26821616291999817, -0.2386358678340912]}, "authors": [{"authorId": "144959773", "name": "S. Chaudhari"}, {"authorId": "2767134", "name": "Gungor Polatkan"}, {"authorId": "2051517", "name": "R. Ramanath"}, {"authorId": "1750278", "name": "Varun Mithal"}], "references": [{"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "d56b859564880b02c1e87c5880492ee66a0ae090", "title": "Social explorative attention based recommendation for content distribution platforms"}, {"paperId": "8f2be495f8d4be177d3fa4f90ceac39772235862", "title": "Hierarchical Bi-Directional Self-Attention Networks for Paper Review Rating Recommendation"}, {"paperId": "6dd1dd0f2b52b3b37be7229363101fa4fbbf1a50", "title": "Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "37464bb258a529584332d4561e13efb9c10e2ffe", "title": "Decentralized policy learning with partial observation and mechanical constraints for multiperson modeling"}, {"paperId": "5755e608f9d45520d994e94bae47cea1dd72f41a", "title": "DETERRENT: Knowledge Guided Graph Attention Network for Detecting Healthcare Misinformation"}, {"paperId": "fe5b999305bdaee6e3fc0868df77e99adde39b3a", "title": "Self-Attention Guided Copy Mechanism for Abstractive Summarization"}, {"paperId": "43e2b9382c12872fe9a48647741046dc2a3bc196", "title": "A Graphical and Attentional Framework for Dual-Target Cross-Domain Recommendation"}, {"paperId": "09e69bf0926e55cd277a3ef5b1450ba083719cb9", "title": "Sparse and Continuous Attention Mechanisms"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "3d61a34611c6171f203286119f76ec52f8016580", "title": "Towards Transparent and Explainable Attention Models"}, {"paperId": "54c7445f319823c7dcc948c830e75e2fa7460b33", "title": "Exploring Self-Attention for Image Recognition"}, {"paperId": "8e85df955707cb7959299d83b6b8c8a28b2b53dd", "title": "Relational Graph Attention Network for Aspect-based Sentiment Analysis"}, {"paperId": "082d14b82d0c969112ee3b99699f694b429fff23", "title": "Generative Attention Networks for Multi-Agent Behavioral Modeling"}, {"paperId": "9a21740d87976bf76f4a9668a9da631035302fb2", "title": "Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms"}, {"paperId": "016a3ba7adcae71f5a23ed2663d8062ae1da63e6", "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"}, {"paperId": "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "5a3a1aed872ef687e91bc98ea5acf5041fd342ae", "title": "SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition"}, {"paperId": "3cefc68260c429b5914b1b3529dce8a229aecec3", "title": "Neural Machine Translation With GRU-Gated Attention Model"}, {"paperId": "fc41d75288a81dd7e30087480f51821fc6572d95", "title": "GMAN: A Graph Multi-Attention Network for Traffic Prediction"}, {"paperId": "38b8c34c31f98fe43b0a478af24868365035cbb8", "title": "Hyper-SAGNN: a self-attention based graph neural network for hypergraphs"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "ee3c825e6bd1cd5bc3a8bb62a3dbab13ff1c5b73", "title": "Monotonic Multihead Attention"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0ffe34b004cdaf6805e58b0ebb7fb672bf61f6d", "title": "DAN: Deep Attention Neural Network for News Recommendation"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "33c6f05eac12622146fec4868735daa78f79f80a", "title": "Stacked Self-Attention Networks for Visual Question Answering"}, {"paperId": "8a1744da011375d711ed75fc2d160c6fdca2cf89", "title": "Deep Modular Co-Attention Networks for Visual Question Answering"}, {"paperId": "fc77daef4c31a4a160b89288d7ff4c8b266ab52a", "title": "Hierarchical User and Item Representation with Three-Tier Attention for Recommendation"}, {"paperId": "135112c7ba1762d65f39b1a61777f26ae4dfd8ad", "title": "Is Attention Interpretable?"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "de02ec03f6a71246e505862a7195894601fbab99", "title": "KGAT: Knowledge Graph Attention Network for Recommendation"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "3cee801d10f410f0feb1a2390776a01ba2765001", "title": "Sparse Sequence-to-Sequence Models"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"}, {"paperId": "00b7efbf14a54cced4b9f19e663b70ffbd01324b", "title": "Heterogeneous Graph Attention Network"}, {"paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f", "title": "Attention is not Explanation"}, {"paperId": "b642cc45b6d746d914ac77f79c55b872d1dbce8d", "title": "Attentional Encoder Network for Targeted Sentiment Classification"}, {"paperId": "bc6dfc6bda2d929fec91042dce1831fd07999b39", "title": "Improved Knowledge Distillation via Teacher Assistant"}, {"paperId": "ab456c1ed181c5c48a34adb61395d4806a0ba949", "title": "Attention in Natural Language Processing"}, {"paperId": "9a7def005efb5b4984886c8a07ec4d80152602ab", "title": "Attention, please! A Critical Review of Neural Attention Models in Natural Language Processing"}, {"paperId": "4e05b1a6f9e03661941b7663f3abb1646535a619", "title": "NAIRS: A Neural Attentive Interpretable Recommendation System"}, {"paperId": "c4afa2b3eda95a1194313394901e0e96e24cefaa", "title": "Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "58695c4827c55b409e0151d9288675847a44d380", "title": "Visual Interrogation of Attention-Based Models for Natural Language Inference and Machine Comprehension"}, {"paperId": "31b3b0a526683048f69e703d5f098aea0e8a0ce0", "title": "Area Attention"}, {"paperId": "6d578d1b367a986de4eaf6b70e8eda0bcc86e1f1", "title": "QA2Explanation: Generating and Evaluating Explanations for Question Answering Systems over Knowledge Graph"}, {"paperId": "833a6052983fa55181b92516d0e1097d74585c58", "title": "NAIS: Neural Attentive Item Similarity Model for Recommendation"}, {"paperId": "e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1", "title": "Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures"}, {"paperId": "97faeefa771e8cc8e55159e2bd03e6f5eef249a8", "title": "Self-Attentive Sequential Recommendation"}, {"paperId": "cc23c580b7d8063415fb6eb512053d1079b849de", "title": "Attention Models in Graphs"}, {"paperId": "59d502851cd20f28af03eef1d15dc83d3a7bb300", "title": "Graph Classification using Structural Attention"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "0b42168f22713c48d8b0ce3bc8f680d347184d88", "title": "Sequential Recommender System based on Hierarchical Attention Networks"}, {"paperId": "f11198dc277af1c5547ab3ae8d562fc8c16a7c70", "title": "Graph-to-Sequence Learning using Gated Graph Neural Networks"}, {"paperId": "66e7130bd3d14e12b953ea155cf4fc1879488651", "title": "A Hierarchical Attention Model for Social Contextual Image Recommendation"}, {"paperId": "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "title": "Self-Attention Generative Adversarial Networks"}, {"paperId": "fb2734de76d9713f97d1156aed4c8eca8fc4d542", "title": "Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM"}, {"paperId": "b0a274852b09cd888ac46a4ff3833a990db92466", "title": "Attention-via-Attention Neural Machine Translation"}, {"paperId": "d44fdde76605fddd1c411f4aa13126b65cd98bb5", "title": "Dynamic Meta-Embeddings for Improved Sentence Representations"}, {"paperId": "c70218603f0af1be5d063056cbe629e042141a86", "title": "Learn To Pay Attention"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "f7325d232c7ac7d2daaf6605377058db5b5b83cc", "title": "A Survey of Methods for Explaining Black Box Models"}, {"paperId": "90f3bd3141026e3a15358149e7de42a3c7ed7f31", "title": "Multi-attention Recurrent Network for Human Communication Comprehension"}, {"paperId": "f0afdccf2903039d202085a771953a171dfd57b1", "title": "Monotonic Chunkwise Attention"}, {"paperId": "063598cdaff79852c3647d074506120889c5c17b", "title": "ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "49a5b5e65078eff512083d9de413d49a8aadc064", "title": "Watch Your Step: Learning Node Embeddings via Graph Attention"}, {"paperId": "adc276e6eae7051a027a4c269fb21dae43cadfed", "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding"}, {"paperId": "624ffc3675964e156e21e79e6b223b808cf3752f", "title": "Deeper Attention to Abusive User Content Moderation"}, {"paperId": "8f3db9e062b2a5e0c3eab6997af2bad7f23fcd93", "title": "Interactive Visualization and Manipulation of Attention-based Neural Machine Translation"}, {"paperId": "5e40be36d11483923cb12260bed6e8f7ed355ff3", "title": "Interactive Attention Networks for Aspect-Level Sentiment Classification"}, {"paperId": "ce2d5b5856bb6c9ab5c2390eb8b180c75a162055", "title": "Recent Trends in Deep Learning Based Natural Language Processing"}, {"paperId": "e33bc5c83f2cea403a5521385ee8e2794b311275", "title": "MAM-RNN: Multi-level Attention Model Based RNN for Video Captioning"}, {"paperId": "f46f399fa85c1e9a2830e4ca17734f1949e80f15", "title": "Link Prediction via Ranking Metric Dual-Level Attention Network Learning"}, {"paperId": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8", "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"}, {"paperId": "0b6f172def2f4b37ea85969b4d99e789c647726b", "title": "Deep Learning Based Recommender System"}, {"paperId": "cfc2d39011c1ae0e9f0bdf64a13616142e155de5", "title": "Dipole: Diagnosis Prediction in Healthcare via Attention-based Bidirectional Recurrent Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "4550a4c714920ef57d19878e31c9ebae37b049b2", "title": "Massive Exploration of Neural Machine Translation Architectures"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "786d1ca11bfd95d8c845ef4a90b2d55092c2fb2f", "title": "Coupled Multi-Layer Attentions for Co-Extraction of Aspect and Opinion Terms"}, {"paperId": "4c41104e871bccbd56494350a71d77a7f1da5bb0", "title": "Understanding Neural Networks through Representation Erasure"}, {"paperId": "d42d2a4112e0a751424624ac0b78980fa9fe9d96", "title": "GAKE: Graph Aware Knowledge Embedding"}, {"paperId": "82bb306038446302cedd20fa986d20640ed88a2e", "title": "Attention-based LSTM for Aspect-level Sentiment Classification"}, {"paperId": "455afd748e8834ef521e4b67c7c056d3c33429e2", "title": "Hierarchical Attention Networks for Document Classification"}, {"paperId": "f314339651cb25e4234e0b96fe8bd87206847993", "title": "Iterative Alternating Neural Attention for Machine Reading"}, {"paperId": "7a67159fc7bc76d0b37930b55005a69b51241635", "title": "Abstractive Sentence Summarization with Attentive Recurrent Neural Networks"}, {"paperId": "fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b", "title": "Hierarchical Question-Image Co-Attention for Visual Question Answering"}, {"paperId": "b28f7e2996b6ee2784dd2dbb8212cfa0c79ba9e7", "title": "Aspect Level Sentiment Classification with Deep Memory Network"}, {"paperId": "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "title": "Dynamic Memory Networks for Visual and Textual Question Answering"}, {"paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"}, {"paperId": "c1e3a26fb88c6720f4e84b7118e6f2df7dc8efa3", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification"}, {"paperId": "f660ea723b62f69b9f4c439724a6b73357e1d3c3", "title": "Survey on the attention based RNN model and its applications in computer vision"}, {"paperId": "87119572d1065fb079e1dee8fcdb6c4811143f96", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems"}, {"paperId": "1ac30af5522c7a50ec4d1ee43fd2bd8652a9bd52", "title": "A Neural Attention Model for Abstractive Sentence Summarization"}, {"paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation"}, {"paperId": "3056add22b20e3361c38c0472d294a79d4031cb4", "title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition"}, {"paperId": "654a3e53fb41d8168798ee0ee61dfab73739b1ed", "title": "Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks"}, {"paperId": "452059171226626718eb677358836328f884298e", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"}, {"paperId": "b624504240fa52ab76167acfe3156150ca01cf3b", "title": "Attention-Based Models for Speech Recognition"}, {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend"}, {"paperId": "9653d5c2c7844347343d073bbedd96e05d52f69b", "title": "Pointer Networks"}, {"paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "title": "End-To-End Memory Networks"}, {"paperId": "5f425b7abf2ed3172ed060df85bb1885860a297e", "title": "Describing Videos by Exploiting Temporal Structure"}, {"paperId": "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca", "title": "DRAW: A Recurrent Neural Network For Image Generation"}, {"paperId": "4d8f2d14af5991d4f0d050d22216825cac3157bd", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"}, {"paperId": "7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f", "title": "Multiple Object Recognition with Visual Attention"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "8a756d4d25511d92a45d0f4545fa819de993851d", "title": "Recurrent Models of Visual Attention"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "88c2ad2be335264540b905b70da3bd49523e13ed", "title": "Auto Learning Attention"}, {"paperId": "b6c50b1e72a69229a18624bee76c53187f56ff6a", "title": "Towards Understanding Attention-Based Speech Recognition Models"}, {"paperId": null, "title": "Self-attention with linear complexity"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "5755783f5614bd84a7dfd0785003edefa210e7b3", "title": "Compositional De-Attention Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "f8c78c66d2e0842ca28670a31bddc801820ae197", "title": "Recent Trends in Deep Learning Based Natural Language Processing [Review Article]"}, {"paperId": "6aeba44efdc57dd32bcf76611dc6d8f52a1a3503", "title": "A Genre-Aware Attention Model to Improve the Likability Prediction of Books"}, {"paperId": "b7d35bc87f101453437a72210d62fc25caa49634", "title": "Association for the Advancement of Artificial Intelligence"}, {"paperId": "14821ac1bf09890a857fca2a6c324e8c85f2c0d0", "title": "Smooth regression analysis"}, {"paperId": "05175204318c3c01e3301fd864553071039605d2", "title": "On Estimating Regression"}, {"paperId": null, "title": "Association for Computational Linguistics, 3311\u20133324"}, {"paperId": null, "title": "Article 53. Publication date"}]}