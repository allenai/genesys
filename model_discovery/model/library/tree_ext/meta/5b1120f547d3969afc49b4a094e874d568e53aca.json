{"paperId": "5b1120f547d3969afc49b4a094e874d568e53aca", "title": "Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization", "abstract": "We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.", "venue": "arXiv.org", "year": 2024, "citationCount": 4, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is demonstrated that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning."}, "embedding": {"model": "specter_v2", "vector": [0.10577403008937836, 0.7367569804191589, -0.5197926163673401, -0.1553472876548767, -0.1543997973203659, -0.3876071870326996, 0.5186116099357605, -0.2888004183769226, -0.177559494972229, -0.2364400029182434, 0.01647256128489971, -0.5179263353347778, -0.11966656148433685, 0.01588648185133934, -0.029652444645762444, 0.053855087608098984, -0.9344022274017334, 0.4903692901134491, 0.1545526683330536, -0.40303170680999756, -0.20482304692268372, -0.3651767373085022, -0.9092426896095276, 0.45192012190818787, 0.5395746827125549, 0.8335760831832886, -0.25307324528694153, 0.7241175770759583, -0.40152451395988464, 1.0722436904907227, 0.577438473701477, -0.4826277494430542, 0.0748913362622261, 0.4059997797012329, -0.42597439885139465, -0.3167388439178467, 0.461748868227005, -0.28014710545539856, -0.5764827132225037, 0.5811510682106018, -0.36445796489715576, 0.3194802403450012, 0.8238533735275269, -0.9661868810653687, -0.5965799689292908, 0.7694200873374939, 1.056573748588562, 0.5959229469299316, -0.38691261410713196, -0.5601433515548706, 1.528533935546875, -1.4212517738342285, 0.33175167441368103, 1.2027175426483154, 0.8629904389381409, 0.7645912766456604, -0.23020824790000916, -0.7734906673431396, 0.9013584852218628, -0.06738428771495819, -1.1768128871917725, -0.33216530084609985, -0.315360426902771, -0.015318836085498333, 1.834794282913208, -0.5481762886047363, -0.1222846731543541, -0.07548650354146957, -0.26259854435920715, 1.5973109006881714, 0.02243638224899769, -0.22432808578014374, 0.028958749026060104, 0.5269221067428589, 0.34739115834236145, 0.921021580696106, -0.1455485075712204, -0.10760892182588577, -0.9569870233535767, -0.04869827628135681, 0.7460304498672485, -0.06330066919326782, -0.3816976249217987, -0.6405372619628906, 0.057216349989175797, 0.45881080627441406, 0.6007640361785889, 0.6476129293441772, -0.4692364037036896, 0.742164671421051, 0.2564907371997833, 0.714928388595581, -0.1481340229511261, 0.4375257194042206, -0.6090513467788696, 0.6489518880844116, -0.9156970977783203, 0.6255393028259277, 0.35670530796051025, 1.1724755764007568, 0.07185427099466324, 0.268788605928421, -0.4603021740913391, 0.20526237785816193, 1.5048222541809082, -0.12182089686393738, 0.518262505531311, -0.6177305579185486, 0.3465864062309265, -0.3281344175338745, 0.1163153424859047, -0.36473581194877625, -0.2413293719291687, -0.708458662033081, -0.5656961798667908, -1.1727980375289917, -0.4387643039226532, 0.12754391133785248, -0.8203308582305908, 0.668633222579956, -0.6595991253852844, -0.064341239631176, 0.2788017988204956, 0.5899204015731812, 0.9012930393218994, 0.21507801115512848, 0.8946199417114258, 0.32892540097236633, 0.6535859107971191, -0.5730825066566467, -0.3990124464035034, -1.1597315073013306, 1.0317105054855347, 0.2796861231327057, -0.08415135741233826, -0.14547772705554962, -1.2321827411651611, -0.74053555727005, -0.8182275295257568, -0.022141579538583755, -0.651404619216919, 0.01814049296081066, 1.395983099937439, 0.6768267154693604, -1.1705870628356934, 0.6231943964958191, 0.18134263157844543, -0.14984747767448425, 0.732145369052887, 0.4297911822795868, 0.10981035232543945, -0.8329014778137207, -1.0125588178634644, 0.40017667412757874, 0.6872156858444214, -0.49054527282714844, -0.5177478790283203, -1.0329644680023193, -1.1568161249160767, -0.07969018071889877, 0.4431323707103729, -1.0262941122055054, 1.5600730180740356, -0.19331084191799164, -1.0314271450042725, 0.723624587059021, -0.29948529601097107, -0.046440720558166504, 0.5305675268173218, -0.013397173956036568, -0.5967168211936951, -0.33565548062324524, -0.34062960743904114, 0.35437819361686707, 0.2636807858943939, -0.5788645148277283, -0.009320762939751148, 0.18697760999202728, -0.24872377514839172, 0.019647354260087013, 0.18349726498126984, 0.5656358599662781, -0.023685498163104057, -0.12841857969760895, 0.6133286356925964, 0.9050502777099609, 0.030290713533759117, -0.013738427311182022, 0.08407500386238098, -1.3560006618499756, 0.5070421695709229, -0.1417357623577118, 0.8602778315544128, -0.9204350113868713, -0.6162689328193665, 0.04863900691270828, 0.11969440430402756, -0.24896641075611115, -0.3362479507923126, 0.4571197032928467, -0.3699195683002472, 0.5536239743232727, -0.5608440041542053, -0.8854697942733765, 0.39460426568984985, 0.11335103958845139, -0.8227310180664062, -0.44224122166633606, 0.2532978057861328, 1.2578856945037842, -0.885262131690979, 0.14865967631340027, -0.14423035085201263, 0.25384286046028137, -1.1643790006637573, 1.1279963254928589, -0.7333315014839172, -0.19930726289749146, -0.34139180183410645, -0.24337534606456757, 0.1805146485567093, -0.4901759624481201, 0.5095388889312744, -0.4259657561779022, 0.10024980455636978, 0.5910274982452393, -0.6267126202583313, 1.3010329008102417, -0.5248009562492371, 0.2618604004383087, -0.18329757452011108, -0.7838417291641235, 0.11219365894794464, 0.5481005311012268, -0.6980462074279785, -0.378318190574646, 0.12106472998857498, 0.5823900103569031, -0.4438270628452301, 0.0946151465177536, 0.8331559300422668, 0.1940220296382904, -0.014203098602592945, 0.1181173101067543, 0.750212550163269, -0.6158223748207092, 0.2520132064819336, 0.35337626934051514, 0.7880575656890869, 0.3113579750061035, 0.2905157208442688, -0.07629207521677017, 0.23864537477493286, -0.7277578711509705, -0.14239022135734558, 0.6193555593490601, 1.154191017150879, 0.6312503218650818, 0.09493264555931091, -0.7123026251792908, -0.19497857987880707, -0.12154948711395264, 0.49923235177993774, 1.6846659183502197, -0.1135985255241394, -0.2719992399215698, -0.257417231798172, -0.3476337790489197, -0.14632248878479004, 0.6728724837303162, -0.33701610565185547, -0.28367879986763, -0.3252609968185425, -0.8234253525733948, 0.7297042608261108, 0.6911377310752869, 1.5105735063552856, -0.4125828742980957, -0.23909258842468262, -0.27500662207603455, 0.4687986671924591, -0.6851275563240051, 0.06184397265315056, 0.46135735511779785, -0.7992399334907532, -0.011707578785717487, 0.11163879930973053, -0.2062792330980301, 0.4451313316822052, -0.6424007415771484, 1.0596232414245605, -0.03215403109788895, -0.08893824368715286, 0.5074514150619507, 1.0447895526885986, -0.17272846400737762, -0.5425937175750732, 0.21296946704387665, -0.35841599106788635, -0.20805609226226807, 0.29877322912216187, 0.4140075743198395, -0.008289278484880924, 0.31288549304008484, -0.40872639417648315, 0.2879151403903961, 0.40444567799568176, -0.09547264128923416, 0.383150577545166, 0.38316893577575684, -0.04937824606895447, -1.5653711557388306, 0.6479634046554565, -0.2392662912607193, -0.03400847688317299, 0.3250257968902588, -0.7900834083557129, -0.32559531927108765, 0.3990667760372162, -0.7198081612586975, -0.5650811791419983, -1.3555372953414917, 0.648707389831543, -0.20133209228515625, -0.28364017605781555, 0.4717574417591095, 0.11127811670303345, -0.19131767749786377, 0.3016551733016968, 0.6912292838096619, 0.15509836375713348, -0.03632257133722305, 0.7200632095336914, -1.1710082292556763, 0.3764030337333679, -0.042986176908016205, 0.5205914378166199, -0.41641977429389954, -0.2442910224199295, -0.5029832720756531, -0.2562538981437683, -0.1397242397069931, 0.40989482402801514, 0.11589384078979492, -0.046490926295518875, -0.8304763436317444, -0.7457484006881714, 0.18590056896209717, -0.5923748016357422, -1.1125072240829468, 0.24177716672420502, -0.6023762226104736, -0.18122725188732147, -1.462053656578064, -1.1019495725631714, -0.6769939064979553, -0.07774455100297928, -0.719233512878418, 0.23078738152980804, -0.06484432518482208, -0.5375970602035522, -1.0247468948364258, -0.32247087359428406, -0.3002670705318451, 1.4972432851791382, -0.9670794606208801, 0.8899517059326172, -0.23405146598815918, -0.8519344925880432, 0.10008083283901215, -0.14792482554912567, 0.3915348947048187, -0.5685246586799622, -0.03931201249361038, -1.2097289562225342, 0.4896973967552185, -0.3613698184490204, -0.3734602630138397, 0.05428706109523773, -0.14647288620471954, 0.8605803847312927, -0.44541093707084656, -0.5803751945495605, 0.23597700893878937, 1.3233611583709717, -0.5585002303123474, 0.45620301365852356, 0.2817041873931885, 1.0588223934173584, 0.3773048222064972, -0.4774564802646637, 0.19387376308441162, 0.8592227101325989, 0.16570264101028442, -0.12331143766641617, 0.2903934121131897, 0.0961708202958107, -0.6430474519729614, 0.3609813451766968, 0.7858975529670715, -0.10707337409257889, 0.11356456577777863, -1.0688852071762085, 0.5023496150970459, -1.2616523504257202, -0.6829937696456909, 0.8245545625686646, 1.0017101764678955, 0.5921850800514221, -0.49030178785324097, -0.6255172491073608, -0.18705466389656067, 0.018105534836649895, -0.09004514664411545, -0.24996311962604523, -0.27161234617233276, 0.1930699497461319, 0.6975451111793518, 0.3431640565395355, 0.43971988558769226, -0.008520321920514107, 0.5557197332382202, 14.568251609802246, 0.9551653265953064, 0.2200784981250763, 0.48251426219940186, 0.4999265968799591, 0.7815423011779785, -0.9578700065612793, -0.10184811800718307, -0.938660204410553, -0.621979296207428, 0.9830366373062134, 0.278984934091568, 0.7098456621170044, 0.24985869228839874, -0.7891941666603088, 0.3340025246143341, -0.5641106963157654, 0.4918946623802185, 0.38955166935920715, -1.2681139707565308, 0.7209532260894775, -0.1448068767786026, 0.28424015641212463, 0.011585859581828117, 0.9552903771400452, 1.2536031007766724, 0.6294691562652588, -0.8961911201477051, 0.7148805856704712, 0.14378219842910767, 0.7509709596633911, 0.054266028106212616, 0.38906344771385193, 0.4231712222099304, -0.9172366261482239, -0.2476966828107834, -0.1903933435678482, -1.0614432096481323, -0.26817312836647034, -0.16553521156311035, -0.7569258809089661, -0.30320513248443604, -0.2529941201210022, 0.7376989126205444, 0.03968048840761185, 0.43015554547309875, -0.600208044052124, 0.4637291729450226, -0.11197799444198608, 0.2923422157764435, -0.15618246793746948, 0.8694170117378235, -0.15580061078071594, -0.16161714494228363, 0.2829601466655731, 0.002596074715256691, 0.05104943364858627, 0.7608567476272583, -0.4753102660179138, -0.2137201577425003, -0.35513052344322205, -0.4314414858818054, 0.0010398068698123097, 0.6906057000160217, 0.46385443210601807, 0.052775800228118896, -0.2821873724460602, 0.09975975006818771, 0.7457535862922668, 0.6701735258102417, 0.04565611854195595, 0.18356165289878845, 0.18461482226848602, -0.43533971905708313, 0.1315096765756607, 0.5806146264076233, -0.33668676018714905, -0.46975263953208923, -0.7587345838546753, -0.3797339200973511, 0.6779059767723083, -0.9628666043281555, -0.6893897652626038, 0.49560385942459106, -0.11780700087547302, -0.22582361102104187, 0.34096238017082214, -0.9743250608444214, -0.14794696867465973, 0.3721989095211029, -1.4995311498641968, -1.0157208442687988, -0.11491870135068893, -0.13559909164905548, -0.5545998811721802, -0.10742935538291931, 1.260477900505066, 0.03997739031910896, -0.04641856625676155, 0.04245114326477051, -0.6041996479034424, -0.15678875148296356, -0.2428659349679947, -1.0488816499710083, 0.6813451647758484, -0.4759455919265747, -0.1329527646303177, 0.9922059774398804, 0.05539869889616966, 0.36175066232681274, -0.9828652739524841, -0.1582319289445877, 0.8245922327041626, -1.1621029376983643, -0.3022635281085968, -0.48551541566848755, -1.2008122205734253, 0.9682150483131409, 0.23689649999141693, -0.13020575046539307, 0.39624327421188354, 0.15202385187149048, -1.2689478397369385, -0.03739603981375694, -1.025316596031189, 0.1406734585762024, 0.6742827892303467, -0.8751093149185181, -0.772217333316803, -0.18476815521717072, -0.04859626293182373, -1.1128160953521729, -0.560752809047699, -0.2034285068511963, 0.261790007352829, 0.013975839130580425, 0.9616352319717407, -0.21299445629119873, 0.3729857802391052, 0.4407203495502472, 0.0630069226026535, -0.6502359509468079, -0.1035120040178299, -0.7525486946105957, 0.030082115903496742, 0.09847403317689896, 1.0626044273376465, -1.1817035675048828, 0.02801644429564476, 1.3628336191177368, 0.06592375040054321, -0.17566817998886108, -0.22352491319179535, 0.06636640429496765, 0.1196126714348793, -0.6849327683448792, 0.34199613332748413, 0.1927897334098816, -0.20905369520187378, 0.14661701023578644, 1.0256370306015015, 0.8966956734657288, -0.3150574266910553, -0.8394447565078735, 0.48839062452316284, -0.18082015216350555, -0.237143874168396, -0.7761269807815552, -0.48494523763656616, -1.2861480712890625, -0.012674464844167233, -1.322620153427124, 0.2202349752187729, -1.157947301864624, -0.5939439535140991, -0.42097267508506775, -0.37402909994125366, 0.31709524989128113, 0.3753724992275238, -0.3604614734649658, -0.8601190447807312, -0.598590612411499, -0.5716092586517334, 0.41177693009376526, 0.7288570404052734, -0.6312128901481628, 0.29986798763275146, -0.17657916247844696, -0.27652397751808167, 0.22052064538002014, 0.506709635257721, -0.2538447380065918, -1.0651919841766357, -1.1525578498840332, 0.7985142469406128, -0.30269306898117065, 0.34381604194641113, -0.6777383089065552, 1.0385303497314453, 0.5749220848083496, 0.01036418043076992, 0.439844012260437, 0.15799017250537872, -1.1098324060440063, -0.22748951613903046, 0.07949013262987137, -0.7993134260177612, 0.03579218313097954, 0.7495066523551941, -0.5186030268669128, -0.10862889140844345, 0.6470856666564941, -0.27195391058921814, -1.180917739868164, -1.1442711353302002, 0.18740734457969666, -0.7565273642539978, 0.299674391746521, -0.4504823088645935, 0.16853100061416626, -1.4245681762695312, -0.1176111027598381, 0.06937039643526077, 0.5728285908699036, -0.19723710417747498, 0.49812400341033936, 0.463401198387146, -0.902942955493927, 0.2729184031486511, 0.31763944029808044, 0.15354903042316437, 0.40645796060562134, 0.8609123229980469, 0.3483402132987976, -0.3367762565612793, 0.3238906264305115, 0.4617241621017456, 0.46177056431770325, -0.41842254996299744, -0.2931351363658905, 1.1030011177062988, -0.24619169533252716, -0.3561196029186249, 1.1458995342254639, 0.13582628965377808, -1.3398135900497437, 0.08635345846414566, -1.3518192768096924, -0.5389811992645264, -0.7190498113632202, 0.6728817224502563, -0.011200929991900921, -0.2235107272863388, 0.4512809216976166, -0.4483472406864166, 0.4662291407585144, -0.20412831008434296, -0.3800840973854065, 0.28107380867004395, 0.19671283662319183, -0.49612173438072205, 0.7299830317497253, 0.3369765281677246, -0.84916090965271, -0.4357532560825348, -0.612211287021637, -0.20822790265083313, 0.22682400047779083, 0.243068128824234, -0.8514843583106995, -0.6304301023483276, 0.5130464434623718, -0.10054419189691544, 0.17464624345302582, -0.06330589205026627, -0.23621372878551483, -0.5057060122489929, 1.0337918996810913, 0.30437901616096497, -0.125894233584404, -0.6916638016700745, 0.8850173354148865, 1.2338255643844604, -0.7779168486595154, 0.25574904680252075, -0.5736817717552185, -0.7272672057151794, 0.9070963263511658, 0.32273656129837036, -0.060667239129543304, 0.6029883623123169, -0.5384365916252136, 0.027893472462892532, -0.26960161328315735, -1.121472954750061, -0.08513975888490677, 0.9537270069122314, 1.0889136791229248, 0.853053629398346, 0.34098759293556213, 0.725011944770813, 0.909767746925354, -0.007171375676989555, 0.5225987434387207, 0.12600715458393097, 0.5369560718536377, -0.11700046807527542, 0.38415369391441345, 0.4329802989959717, 0.46968045830726624, -0.3214966654777527, -0.8825950622558594, 0.12833169102668762, 0.9872361421585083, 0.10816323012113571, 0.3531796932220459, 0.4502539336681366, 0.45867499709129333, 0.40810009837150574, 0.6875489354133606, 0.8613315224647522, -0.6641966700553894, -0.3573179841041565, -0.6458525061607361, -0.20809264481067657, -0.019120458513498306, -0.39276570081710815, -0.05582873523235321, -0.5929448008537292, -0.09227113425731659, 0.46377670764923096, 0.2707996070384979, 0.09423595666885376, 1.334837794303894, 0.4561440348625183, 0.7269015312194824, -0.8615493774414062, -0.17942343652248383, -0.3392183482646942, -0.7605481147766113, -0.0005558346747420728, -0.7589104771614075, -0.16681057214736938, -0.5224021673202515, -0.5997616648674011, -0.4205462634563446]}, "authors": [{"authorId": "7425689", "name": "Boshi Wang"}, {"authorId": "2268495181", "name": "Xiang Yue"}, {"authorId": "1758652", "name": "Yu Su"}, {"authorId": "2239158325", "name": "Huan Sun"}], "references": [{"paperId": "ccf6d9d7cec0d4aa9b50c99659765668d6e7c6c2", "title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step"}, {"paperId": "84ec06d8ee11fab19a09872a75b7f4f9f20f80b9", "title": "A Primer on the Inner Workings of Transformer-based Language Models"}, {"paperId": "b82a667aa74f48f0ebb9fc658d479b04fb40498c", "title": "Do language models plan ahead for future tokens?"}, {"paperId": "b5bffe41155052a43010ec7197f832e81c546268", "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking"}, {"paperId": "41b47f33a24feefd6728bdc1339d0d4ff5fec7be", "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"}, {"paperId": "c36f638781953f995eb3079f882174846b9ac16a", "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?"}, {"paperId": "7d417465bdf254f8b4491c0e4adbace8f49010ab", "title": "Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition"}, {"paperId": "5a20aa49b81b4e14fdb36814e557b3da60259ce9", "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems"}, {"paperId": "f288e2238ac8725baa7ca9874bbc3fed1e89a632", "title": "Data Engineering for Scaling Language Models to 128K Context"}, {"paperId": "f76b280d9201cf0ae43717afe05ce15edeb13bb1", "title": "Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation"}, {"paperId": "57e7af0b69325fafb371ef5d502e39ef9c90ef7e", "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"}, {"paperId": "88aeeaec712a38d741c59b8e6faab2709e678759", "title": "Critical Data Size of Language Models from a Grokking Perspective"}, {"paperId": "600d9287efc4703bdb99ce39b5e8b37da0baa6f6", "title": "The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning"}, {"paperId": "49a02664e552320a43cea541c07ca312e45350b5", "title": "Future Lens: Anticipating Subsequent Tokens from a Single Hidden State"}, {"paperId": "4411e6b32865933cab87696c2738cb7a204e4240", "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation"}, {"paperId": "6d130ebff26dbbf00e3c601973872ff0c0c4235f", "title": "How do Language Models Bind Entities in Context?"}, {"paperId": "bcd84a2b8f9ae40a908f375425f113c82f8dd739", "title": "Sparse Universal Transformer"}, {"paperId": "6db6e6e71cc54435265643e19fcbdc7f3ba4c772", "title": "Crystal: Introspective Reasoners Reinforced with Self-Feedback"}, {"paperId": "c16c05ca0a3d24519405849fd24604fc1ce47751", "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods"}, {"paperId": "47daf5f81470564f94adcac672405c2cd39dd186", "title": "Physics of Language Models: Part 3.2, Knowledge Manipulation"}, {"paperId": "77b603850094ff749c9040772f8169a75145d506", "title": "Explaining grokking through circuit efficiency"}, {"paperId": "e91972eaf32db6d46cdcc70c0ffa6d0de6a16382", "title": "Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok"}, {"paperId": "6001a5d38df9a043421a670357842d8df71d656b", "title": "Grokking of Hierarchical Structure in Vanilla Transformers"}, {"paperId": "7d97c17a75beb89f938eaac1d3ca60ac2245fb2e", "title": "Faith and Fate: Limits of Transformers on Compositionality"}, {"paperId": "c2260403fd5cb2de73491323433e48b6ec36872c", "title": "Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective"}, {"paperId": "56e952fd463accff09cf2e35432aaabd7c7c57f3", "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions"}, {"paperId": "aec826ff336ca442697d5f908ab1668f1ea18987", "title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model"}, {"paperId": "9a3edb5c6b0e8c84c94ea99a9ab647b1209f650f", "title": "Why think step-by-step? Reasoning emerges from the locality of experience"}, {"paperId": "1a6af6214f33187bc2c9d78bb1dc28cf5b038e16", "title": "A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "762ca2711eb167f19b79e39c175708ca15e1f5d7", "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens"}, {"paperId": "fb6ecf67c275fe775a12ad4ddf2c8dc7cfad1348", "title": "Unifying Grokking and Double Descent"}, {"paperId": "f680d47a51a0e470fcb228bf0110c026535ead1b", "title": "Progress measures for grokking via mechanistic interpretability"}, {"paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45", "title": "Towards Reasoning in Large Language Models: A Survey"}, {"paperId": "9492ee1435e183cb62b65d8d7f39be0dfd17377a", "title": "Nonparametric Masked Language Modeling"}, {"paperId": "6edd112383ad494f5f2eba72b6f4ffae122ce61f", "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"}, {"paperId": "e82e3f4347674b75c432cb80604d38ee630d4bf6", "title": "Transformers Learn Shortcuts to Automata"}, {"paperId": "e070ff286709db28312e08b52b05539debe88146", "title": "Measuring and Narrowing the Compositionality Gap in Language Models"}, {"paperId": "ed99a2572fb5f4240aa6068e3bf274832e831306", "title": "Recitation-Augmented Language Models"}, {"paperId": "ad065eed8e1f727a8b0d8675802e4ffb1fcb87b4", "title": "Omnigrok: Grokking Beyond Algorithmic Data"}, {"paperId": "ff53cb49cb18e71e622fce7d96692e813630e878", "title": "The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon"}, {"paperId": "da1d6445b6b64ce9eb4587ba8abbdc490f648ec1", "title": "Training Language Models with Memory Augmentation"}, {"paperId": "8b293973061026d9d0eed90e71e30928e029171e", "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models"}, {"paperId": "20de79ec4fe682b68930eb4dcd91b1801b8d4731", "title": "Towards Understanding Grokking: An Effective Theory of Representation Learning"}, {"paperId": "cf36236015c9f93f15bfafbf282f69e08bdc9c16", "title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"}, {"paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be", "title": "STaR: Bootstrapping Reasoning With Reasoning"}, {"paperId": "3f4d11971f2c64be9125a7fe99c019588bbebf16", "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "9d40837175577bb0009b138269b422f6d5820d00", "title": "Transformer Memory as a Differentiable Search Index"}, {"paperId": "996445d847f06e99b0bd259345408a0cf1bce87e", "title": "Locating and Editing Factual Associations in GPT"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "a1d1983a7b19845141e6505bd32dc395e5a136ba", "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "f46c562229c5bc419bbbfb63239431590e4b340a", "title": "Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "6f2b90ee5a0feea87264148c25a874f84bae20a0", "title": "Are Pretrained Language Models Symbolic Reasoners over Knowledge?"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0", "title": "A Primer in BERTology: What We Know About How BERT Works"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "5e0cffc51e8b64a8f11326f955fa4b4f1803e3be", "title": "oLMpics-On What Language Model Pre-training Captures"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "f6e6c948a2074e38e0a4e9099c0f63773c6013dd", "title": "Causality"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "856fe866bcce5e7a540655bea6ecc7406bdcfcba", "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"}, {"paperId": "11adc8bd35bd897502f9b5452ab7ac668ec9b0fb", "title": "The Implicit Bias of Gradient Descent on Separable Data"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "784ee73d5363c711118f784428d1ab89f019daa5", "title": "Hybrid computing using a neural network with dynamic external memory"}, {"paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "title": "End-To-End Memory Networks"}, {"paperId": "a169ca513b7e7aef5087dbb6b3c57b8516599c22", "title": "Center"}, {"paperId": "78d08b8ab4132defffe98ec7f80a51452203f70d", "title": "Investigating Gender Bias in Language Models Using Causal Mediation Analysis"}, {"paperId": null, "title": "nostalgebraist"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Less is more for alignment"}, {"paperId": null, "title": "New models and developer products announced at devday"}]}