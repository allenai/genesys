{"paperId": "36697944858ab17ca23b23ae2043aa6c0b2e3d5d", "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention", "abstract": "This paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information. Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency.", "venue": "arXiv.org", "year": 2023, "citationCount": 5, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes a new model architecture, referred to as Zebra, that efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers."}, "embedding": {"model": "specter_v2", "vector": [0.5606745481491089, 0.32613706588745117, -0.2642771601676941, -0.33133310079574585, -0.16632646322250366, -0.06537486612796783, 0.6253098249435425, -0.09388018399477005, -0.4364635944366455, -0.20076224207878113, 0.5767413377761841, -0.0069653731770813465, 0.559985876083374, 0.0847051665186882, 0.11053851246833801, 0.13114920258522034, -0.9422523379325867, 0.34148553013801575, 0.11045835167169571, -0.6369655728340149, 0.2829078435897827, -0.9293658137321472, -0.5612400770187378, 0.29759782552719116, 0.38773006200790405, 0.43417057394981384, 0.7254974246025085, 1.0351147651672363, -0.5453115701675415, 0.44983571767807007, 0.1256849467754364, -0.21263258159160614, -0.23814694583415985, -0.28864404559135437, -0.29068610072135925, -0.30572131276130676, 0.41005900502204895, -0.4651636779308319, -0.07969191670417786, 0.7675248384475708, -0.10273294895887375, 0.5134824514389038, 0.523201584815979, -0.18432144820690155, -0.04129637032747269, 1.0955930948257446, 0.517974317073822, 0.7324501276016235, -0.3095509707927704, -0.7006921768188477, 1.5791683197021484, -1.597373127937317, 0.3093288838863373, 1.652572512626648, 0.32270777225494385, 0.6558986902236938, 0.035100191831588745, -0.7021854519844055, 1.3417798280715942, 0.31377580761909485, -0.8695026636123657, -0.32667189836502075, 0.14290469884872437, 0.14252397418022156, 2.3671822547912598, -0.47788485884666443, -0.16621719300746918, 0.6145336627960205, 0.2389063835144043, 1.4826236963272095, -0.7882784008979797, -0.9961193799972534, -0.4109105169773102, -0.45519572496414185, 0.40681105852127075, 0.6749078631401062, -0.5141071677207947, -0.1540113240480423, -0.8921215534210205, 0.1101454496383667, 0.2994154691696167, -0.3067862093448639, 0.01892406865954399, 0.22625640034675598, -0.6138457655906677, 0.6043923497200012, 0.21447570621967316, 0.7664775848388672, -0.007217515725642443, 0.5184110403060913, 0.508370578289032, 0.2523104250431061, 0.21639849245548248, 0.4025900959968567, -0.1882539987564087, 0.40448200702667236, -0.8244099020957947, 0.34851014614105225, 0.0347004234790802, 1.0531660318374634, -0.4911445677280426, 0.18180160224437714, -0.5263488292694092, 0.5509929656982422, 1.324460744857788, 0.01395783293992281, 0.35853034257888794, -0.4122195541858673, 0.6976146697998047, -0.6145919561386108, 0.1626734584569931, -0.6993066668510437, -0.28472915291786194, -0.28474336862564087, -0.6557016372680664, -1.3710215091705322, -0.4049455523490906, -0.12121391296386719, -0.2292066514492035, 0.964464008808136, -0.07986848801374435, 0.11074179410934448, -0.13674762845039368, 0.1294051557779312, 0.3906032145023346, 0.7866311073303223, 0.6403981447219849, 0.18342851102352142, 0.9979065656661987, -0.9197473526000977, -0.7386018633842468, -1.5637791156768799, 0.5915380120277405, -0.20064237713813782, 0.40699419379234314, 0.06734693795442581, -1.036275863647461, -1.012860655784607, -1.035276174545288, -0.2931700348854065, -0.679448127746582, 0.16225580871105194, 0.4460564851760864, 0.09439341723918915, -0.7334030866622925, 0.6622398495674133, -0.19557364284992218, -0.15186233818531036, 0.27338138222694397, -0.07337697595357895, 0.6057358384132385, -0.5040069818496704, -1.5003705024719238, 0.3567550480365753, 0.3159139156341553, -0.5247088670730591, -0.21763452887535095, -0.6086463928222656, -1.0797463655471802, 0.01974916271865368, 0.4885804355144501, -0.4395061433315277, 1.4724348783493042, -0.14153428375720978, -1.4457812309265137, 0.5557906031608582, -0.6530956029891968, 0.21441242098808289, -0.09129266440868378, -0.48812413215637207, -0.667824923992157, -0.43137094378471375, -0.5312240123748779, 0.4100600779056549, 0.40458834171295166, 0.3309011161327362, -0.3191109597682953, 0.0038844372611492872, -0.5490609407424927, 0.046446748077869415, -0.16652528941631317, 1.062382459640503, -0.5227060317993164, -0.5293979048728943, 0.21619251370429993, 0.7868421077728271, 0.04480656608939171, -0.6132004857063293, -0.3241243362426758, -1.3920408487319946, 0.8693464398384094, -0.2535010278224945, 1.2718961238861084, -0.6736555695533752, -0.4489826261997223, -0.3976880609989166, -0.27715736627578735, -0.13878236711025238, -0.8656376004219055, 0.8857046961784363, -0.05160495266318321, 0.30451029539108276, -0.23440295457839966, -1.3783514499664307, -0.03376730531454086, -0.1833769679069519, -0.8450448513031006, -0.4858957529067993, 0.21533705294132233, 1.175789475440979, -1.1766964197158813, -0.12885351479053497, -0.22783637046813965, 0.1782321184873581, -0.9557222127914429, 1.2711106538772583, -0.8053367733955383, 0.26154428720474243, -0.3691449761390686, -0.2738562822341919, -0.258410781621933, -0.34553056955337524, 0.6612094640731812, -0.3617321848869324, -0.07977796345949173, 0.6891639232635498, 0.034208524972200394, 1.4261858463287354, -0.5556655526161194, 0.6768566370010376, -0.1633865386247635, -0.5197604894638062, 0.1554255485534668, 0.4390043616294861, -0.5780349969863892, -0.29967814683914185, 0.4996145963668823, 0.046353064477443695, -0.786923885345459, 0.06267006695270538, 0.9172656536102295, 0.9506695866584778, -0.521246612071991, 0.12896698713302612, 0.49158143997192383, -0.40305909514427185, 0.49997085332870483, 0.41831499338150024, 0.28798237442970276, 0.5122866034507751, 0.5808858275413513, -0.1322517991065979, 0.5702177882194519, -0.6426291465759277, -0.31553182005882263, 0.3951970040798187, 0.927742063999176, 0.7809178829193115, 0.5105924010276794, -0.491163045167923, -0.4261281192302704, 0.5121456980705261, 0.8912388682365417, 1.87468421459198, 0.06273126602172852, -0.17826421558856964, -0.8556786775588989, -0.17956426739692688, -0.5047124028205872, 0.4862231910228729, -0.33845382928848267, 0.002604243578389287, -0.7931318879127502, -0.9248374104499817, 0.597219705581665, 0.3851640820503235, 0.9743499755859375, -0.7021969556808472, -0.20801123976707458, 0.004674355499446392, -0.056766778230667114, -0.9015428423881531, -0.967453122138977, 0.10721185803413391, -0.8017308712005615, -0.32379597425460815, 0.41246819496154785, -0.25057876110076904, 0.04994027689099312, -0.5786094069480896, 1.1873257160186768, -0.49211183190345764, -0.12824498116970062, -0.03137575462460518, 0.4370477795600891, -0.6473370790481567, -0.724852442741394, 0.30208712816238403, -0.2226594239473343, 0.037216585129499435, 0.5492470860481262, 0.6822776198387146, -0.06854142993688583, 0.11500903964042664, -0.3299733102321625, 0.4364411532878876, 0.131269633769989, 0.1216418519616127, 0.6921977400779724, -0.516717791557312, 0.3445028066635132, -1.5769275426864624, 0.950689971446991, 0.040054626762866974, -0.4350871443748474, 0.3710126280784607, -0.45528820157051086, -0.423076331615448, 0.4917584955692291, -0.7863301634788513, -0.7208361029624939, -1.0473154783248901, 0.21715191006660461, 0.02355150319635868, -0.2324521541595459, 0.48899421095848083, -0.21835242211818695, 0.3852247893810272, 0.30216169357299805, 0.2767292857170105, 0.2422841638326645, -0.3228360116481781, 0.5522656440734863, -0.7579900026321411, 0.6006917953491211, 0.4469427168369293, -0.31009727716445923, -0.370268851518631, -0.3211880028247833, -0.8894477486610413, -0.35995370149612427, -0.5969685316085815, -0.4385187327861786, -0.2330833077430725, 0.409605473279953, -0.42098909616470337, -0.569100022315979, 0.10868900269269943, -1.1103895902633667, -0.6158522367477417, 0.6334385275840759, -0.4070781171321869, -0.027118636295199394, -0.9263216853141785, -1.1913784742355347, -0.1956070065498352, -0.7708008885383606, -0.7019360065460205, 0.41341036558151245, -0.13009336590766907, -0.7248952388763428, -0.9531931281089783, 0.03544335067272186, -0.4526272118091583, 1.156182050704956, -0.38432151079177856, 1.0044394731521606, -0.11110639572143555, -0.296063095331192, -0.5472389459609985, 0.3841826617717743, 0.45167577266693115, -0.059582557529211044, 0.15131059288978577, -0.7462785243988037, 0.19181323051452637, -0.14571714401245117, 0.019560113549232483, 0.03521467000246048, 0.16169501841068268, 0.6798877716064453, -0.028311481699347496, -0.6841331124305725, 0.16978910565376282, 1.4633872509002686, -0.3880675733089447, -0.06568235903978348, -0.20446836948394775, 0.9960905909538269, 0.25990772247314453, -0.19159719347953796, 0.5729417204856873, 0.41160842776298523, 0.24499283730983734, 0.2694277763366699, 0.04306437447667122, -0.20799748599529266, -0.5370079278945923, 0.6581142544746399, 1.8369066715240479, 0.1855139136314392, -0.4127914309501648, -0.9830495119094849, 0.7283709049224854, -0.917724609375, -0.6372590661048889, 0.7208166718482971, 0.7418899536132812, 0.5043430328369141, -0.5483856797218323, -0.6521960496902466, -0.42269131541252136, 0.5100671052932739, 0.5346686840057373, -0.3962988257408142, -0.5530924797058105, 0.07910240441560745, 0.21803683042526245, -0.09384217113256454, 0.9811617732048035, -0.45275360345840454, 0.8129078149795532, 14.5695161819458, 0.9085721373558044, -0.047345586121082306, 0.711044192314148, 0.6459696888923645, 0.20320558547973633, -0.18389996886253357, -0.3614409267902374, -1.4555326700210571, -0.361158162355423, 1.2329533100128174, 0.0667300596833229, 0.708275318145752, 0.07460398972034454, 0.15072035789489746, 0.2827172875404358, -0.7573175430297852, 0.5575926899909973, 0.9135985374450684, -1.0499191284179688, 0.6108169555664062, -0.15019217133522034, 0.31594419479370117, 0.6566879749298096, 0.47053876519203186, 0.9276871681213379, 0.3188427984714508, -0.48559510707855225, 0.5284281373023987, 0.10264939814805984, 0.6891116499900818, 0.0015760462265461683, 0.296735554933548, 0.8691695332527161, -0.995309591293335, -0.25870198011398315, -0.6812500953674316, -1.0816324949264526, 0.11863799393177032, -0.11602859199047089, -0.565246045589447, -0.37342768907546997, -0.5030006170272827, 1.018406867980957, 0.0029066859278827906, 0.14164434373378754, -0.5418474674224854, 0.7785360813140869, 0.22484096884727478, -0.3955577611923218, 0.20550905168056488, 0.3297058641910553, 0.33958700299263, 0.28375571966171265, -0.010785735212266445, 0.1755446046590805, 0.05173593759536743, 0.33162617683410645, -0.5649399757385254, 0.27209195494651794, -0.2829236090183258, -0.06935279071331024, 0.009354104287922382, 0.6008279323577881, 0.45731818675994873, -0.10344275087118149, -0.6751917004585266, 0.23384703695774078, 0.6326697468757629, 0.5071060061454773, -0.041954003274440765, -0.23729240894317627, 0.10396118462085724, -0.682133674621582, 0.15393014252185822, 0.38336381316185, -0.4847864508628845, -0.5639898180961609, -0.9441335201263428, -0.30688315629959106, 0.4307928681373596, -0.785909116268158, -0.7284059524536133, 0.9820371270179749, -0.02467130310833454, -0.4388725161552429, -0.030951090157032013, -0.916464626789093, -0.4884556233882904, 0.631667971611023, -1.2716753482818604, -0.5575200915336609, -0.05176176130771637, -0.18418239057064056, 0.25119948387145996, 0.10190150886774063, 1.2094939947128296, -0.011811467818915844, -0.35713645815849304, 0.1634523868560791, -0.016801362857222557, -0.016638929024338722, -0.16629643738269806, -0.6108786463737488, 0.9734242558479309, 0.3811051845550537, -0.31546640396118164, 0.39202114939689636, -0.21002212166786194, 0.01985320635139942, -0.7553768157958984, -0.2963832914829254, 1.0558933019638062, -1.0450236797332764, -0.3195889890193939, -0.9699153304100037, -1.0598937273025513, 0.5086929202079773, 0.7048854231834412, -0.577276885509491, 0.32435381412506104, 0.2927300035953522, -0.5167625546455383, -0.11809264868497849, -0.49638694524765015, 0.09520067274570465, 0.2857704162597656, -0.8273528814315796, -0.600997269153595, -0.1375391036272049, 0.42551350593566895, -0.9876176118850708, -0.47123435139656067, -0.6160427331924438, 0.10941550880670547, 0.419063538312912, 1.0260885953903198, -0.2893434166908264, 0.6701028943061829, 0.9262534379959106, -0.1973714977502823, -0.8579740524291992, 0.04420548304915428, -0.7800198793411255, 0.08677909523248672, 0.007790012285113335, 0.7685301899909973, -0.4004926383495331, -0.19192107021808624, 0.7298733592033386, 0.22283123433589935, -0.4585542678833008, -0.4879109561443329, -0.08727474510669708, 0.33986324071884155, -0.6066485643386841, 0.4590900242328644, -0.06180762127041817, 0.17279264330863953, 0.29686903953552246, 0.4478691518306732, 0.6501485109329224, -0.2912113666534424, -0.5895124673843384, 0.4163985252380371, 0.07374680042266846, 0.041950855404138565, -0.8954542279243469, -0.2161775380373001, -1.7315531969070435, -0.24947023391723633, -0.8354678750038147, 0.09540174901485443, -0.8994299173355103, -0.5850966572761536, 0.3574411869049072, -0.297267884016037, 0.1048261970281601, -0.11743400990962982, -0.5727929472923279, -0.5161210298538208, -0.5672096014022827, -0.6734100580215454, 0.82794189453125, 1.0257396697998047, -0.8085588216781616, 0.0004433525900822133, -0.1412442922592163, 0.3092435896396637, 0.08907537162303925, 0.513685405254364, -0.04475118964910507, -0.8708312511444092, -1.6161748170852661, 0.4800579845905304, -0.016707682982087135, -0.2647847831249237, -0.5486649870872498, 0.7716015577316284, 0.2275458127260208, -0.2643235921859741, -0.11354454606771469, 0.18343491852283478, -0.48830363154411316, -0.7445284128189087, 0.11523249000310898, -1.0589489936828613, 0.20714332163333893, 0.3717567026615143, -0.573509156703949, -0.6034690737724304, 0.7150256037712097, -0.16053512692451477, -1.1224092245101929, -0.9888368248939514, 0.4699786305427551, -0.6632097959518433, 0.1964295655488968, -0.47725024819374084, 0.04419964179396629, -1.0373094081878662, -0.37457576394081116, 0.21590587496757507, 0.5993034839630127, -0.47376781702041626, 1.0740010738372803, 0.6080735325813293, -0.8131020069122314, -0.07319934666156769, 0.5144272446632385, 0.02422601543366909, 0.3772939443588257, 0.7258520722389221, 0.2300700843334198, -0.35778719186782837, 0.5864013433456421, 0.4895157516002655, 0.18684500455856323, -0.9913002848625183, 0.040207184851169586, 0.8127579092979431, -0.4445480406284332, -0.24103771150112152, 1.3649965524673462, 0.054745424538850784, -0.9578209519386292, -0.016936341300606728, -1.493754506111145, -0.808925211429596, -0.2440429925918579, 1.1936780214309692, 0.12996205687522888, 0.04298196732997894, 0.025093287229537964, -0.36948126554489136, 0.5105045437812805, -0.07333865761756897, -0.425632506608963, 0.5422043204307556, -0.5042862296104431, -0.6032938957214355, 0.9574699401855469, 0.6887382864952087, -0.6176082491874695, -0.9457811117172241, -0.7666661739349365, -0.14387769997119904, 0.14805033802986145, 0.34584861993789673, -0.5435022115707397, -0.44266805052757263, 1.0370500087738037, 0.07586412131786346, 0.14395256340503693, 0.3050830066204071, -0.3737044334411621, 0.2295721471309662, 0.6992450952529907, 0.0653638243675232, -0.5997612476348877, -0.518317461013794, 1.4695446491241455, 1.6587538719177246, -0.8430639505386353, 0.06712621450424194, -0.1099914163351059, -0.6996192932128906, 1.0014444589614868, 0.1222691759467125, -0.12870624661445618, 0.8481107950210571, -0.3979496359825134, 0.20839214324951172, 0.30339518189430237, -1.408170223236084, 0.2803114056587219, 0.9889674186706543, 0.9870197176933289, 0.6643686890602112, 0.25513723492622375, 0.23017647862434387, 0.9537734985351562, 0.2775362730026245, 0.03318297490477562, 0.312519907951355, 0.28628212213516235, -0.31128859519958496, -0.10722915828227997, 0.05587993189692497, 0.3219873309135437, -0.6380590200424194, -0.8092274069786072, 0.2222893238067627, 0.6486966609954834, -0.03266115114092827, 0.8290834426879883, 0.8217412829399109, 0.30082523822784424, 0.3735974431037903, 0.39501285552978516, 0.38163259625434875, -0.619756281375885, -0.2228250354528427, -0.21834208071231842, -0.5839391946792603, -0.10676258057355881, -0.07029106467962265, -0.5146165490150452, -0.22271479666233063, -0.030886506661772728, 0.4069438874721527, -0.22787432372570038, 0.30711859464645386, 1.17685067653656, 0.33672183752059937, 0.5245945453643799, -0.41430848836898804, -0.3987473249435425, -0.22041048109531403, -1.5076477527618408, 0.17160993814468384, -0.44983962178230286, -0.08491455763578415, 0.02481083571910858, 0.1854054033756256, 0.056404806673526764]}, "authors": [{"authorId": "50982080", "name": "Kaiqiang Song"}, {"authorId": "2250363276", "name": "Xiaoyang Wang"}, {"authorId": "2173531", "name": "Sangwoo Cho"}, {"authorId": "2243367575", "name": "Xiaoman Pan"}, {"authorId": "2256336899", "name": "Dong Yu"}], "references": [{"paperId": "6d8896632ca2af8310273f6774e1dfb3140770f7", "title": "M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models"}, {"paperId": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0", "title": "Effective Long-Context Scaling of Foundation Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "b0db25e317cf856f1ec1ca3df0e573d850ed4696", "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4", "title": "The Impact of Positional Encoding on Length Generalization in Transformers"}, {"paperId": "7d29cc9cdeba6d2d338342d32abcedcc432e4aeb", "title": "MeetingBank: A Benchmark Dataset for Meeting Summarization"}, {"paperId": "eb511ae6b9f04e4936891d26787f274b48b99d57", "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding"}, {"paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445", "title": "LIMA: Less Is More for Alignment"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "5735e49e501c8e51e9be4079592e46e047747b03", "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"}, {"paperId": "d2fe7536b347a7039a241bb60d507880ada686e8", "title": "OASum: Large-Scale Open Domain Aspect-based Summarization"}, {"paperId": "d9d12205007ac48b03d921225f9cdaf90f7c3fdd", "title": "Model Criticism for Long-Form Text Generation"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "1487b51b327028576bc480120be96ef84efa6723", "title": "Towards Abstractive Grounded Summarization of Podcast Transcripts"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "2525df3bfc0f2c1c7abda2691f9f4b2f58fc1fc6", "title": "MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "7ffc1b425026e916cd6db37c79df3e08e8f47ee6", "title": "OpenMEVA: A Benchmark for Evaluating Open-ended Story Generation Metrics"}, {"paperId": "a4ffce66918cfb33150a60bf8e26419199e63b01", "title": "BookSum: A Collection of Datasets for Long-form Narrative Summarization"}, {"paperId": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "9dc624d7258d1a56117ca720aea953ce46b66b21", "title": "Efficient Attentions for Long Document Summarization"}, {"paperId": "aa28873534c24e4a8c5deb7bff723cd5fc69a6f0", "title": "QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "6a1b25f7a67395ad1e676027322913acbb0a0635", "title": "CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review"}, {"paperId": "9623e9e461647a10a8f14419a9abe40482e9eb47", "title": "MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "fb50f6f4f81f361bf1c6dbc93cc8fab5aee12fdf", "title": "Extractive Opinion Summarization in Quantized Transformer Spaces"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "3df83a60f55c64b40e6dbcd99cf9f67894a0736e", "title": "Do Transformers Need Deep Long-Range Memory?"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "ebf59587f8f170ff4241c42263bbfb9da5bd2135", "title": "ELI5: Long Form Question Answering"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "40345901fd28cbf65791c34671db6548b1089ed4", "title": "BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization"}, {"paperId": "cc27ec53160d88c25fc5096c0df65536eb780de4", "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a", "title": "The NarrativeQA Reading Comprehension Challenge"}, {"paperId": "bb567cf793a6c4df1a652489c5ce866fe044f0e5", "title": "Supervised and Unsupervised Transfer Learning for Question Answering"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "dd95f96e3322dcaee9b1e3f7871ecc3ebcd51bfe", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"}, {"paperId": "21633a4ef076e21ec116856a003efb1bb033a470", "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": null, "title": "Social IQa: Com-monsense reasoning about social interactions"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "OpenAI. 2023."}, {"paperId": null, "title": "2023. How long can open-source llms truly promise on context length?"}, {"paperId": null, "title": "2023. Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "BigScience Workshop"}, {"paperId": null, "title": "2022. SQuAL-ITY: Building a long-document summarization dataset the hard way"}, {"paperId": null, "title": "2022. MuSiQue: Multi-hop questions via single-hop question composition"}, {"paperId": null, "title": "2023. Lm-infinite: Simple on-the-fly length generalization for large language models"}, {"paperId": null, "title": "2022. QuALITY: Question answering with long input texts,"}, {"paperId": null, "title": "torch.einsum ( \u2032 ...hqd, ...khd \u2212 > ...qhd \u2032 , attn_prob , V local ) ;"}, {"paperId": null, "title": "2022. Rethinking attention with per-formers"}, {"paperId": null, "title": "2022. Introducing"}, {"paperId": null, "title": "reshape ( new_shape ) . transpose (1"}, {"paperId": null, "title": "new_shape \u2190 ( bsz, n _ heads, n _ blocks, w, 2 \u2217 w )"}, {"paperId": null, "title": "2022. Red teaming language models to"}, {"paperId": null, "title": "2023. Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models"}, {"paperId": null, "title": "2023c. Lon-glora: Efficient fine-tuning of long-context large language models"}, {"paperId": null, "title": "2022. Chain-of-thought prompting elicits reasoning in large language models"}, {"paperId": null, "title": "2023. A length-extrapolatable transformer"}]}