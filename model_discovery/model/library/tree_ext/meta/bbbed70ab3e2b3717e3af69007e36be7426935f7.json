{"paperId": "bbbed70ab3e2b3717e3af69007e36be7426935f7", "title": "Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning", "abstract": "Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at different scales, thus preventing learning rate transfer. But what causes these differences in the sharpness dynamics? Through a connection between the spectra of the Hessian and the NTK matrix, we argue that the cause lies in the presence (for $\\mu$P) or progressive absence (for the NTK regime) of feature learning, which results in a different evolution of the NTK, and thus of the sharpness. We corroborate our claims with a substantial suite of experiments, covering a wide range of datasets and architectures: from ResNets and Vision Transformers trained on benchmark vision datasets to Transformers-based language models trained on WikiText", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work finds empirical evidence that learning rate transfer can be attributed to the fact that under $\\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time."}, "embedding": {"model": "specter_v2", "vector": [0.13888269662857056, 0.906775712966919, -0.3897262513637543, 0.052520401775836945, -0.14401797950267792, 0.043657492846250534, 0.8090339303016663, -0.5308358073234558, -0.6180375814437866, -0.5782138109207153, 0.20969349145889282, 0.03421096131205559, -0.04507178068161011, 0.1454729288816452, -0.2901739180088043, 0.04335906356573105, -0.7341069579124451, -0.009475422091782093, 0.19511593878269196, -0.2826266288757324, -0.27413424849510193, -0.17303340137004852, -0.8964093923568726, -0.3903450667858124, 0.20166315138339996, 1.2945939302444458, -0.1337301880121231, 0.7702855467796326, -0.13324449956417084, 0.29909569025039673, 0.7857733964920044, -0.4783167541027069, 1.031988263130188, 0.5285565257072449, -0.27231675386428833, 0.1493154913187027, 0.8708447813987732, -0.2621755003929138, -0.7936389446258545, 1.1691864728927612, -0.3371634781360626, 0.261938214302063, 0.5001940131187439, -0.8317477703094482, -0.2798340916633606, 0.2968441843986511, 0.3403530418872833, 0.7631561160087585, -1.0297482013702393, -0.01553211361169815, 1.130674123764038, -0.9615186452865601, 0.10770512372255325, 1.4174690246582031, 0.7802965641021729, 0.26628053188323975, -0.2632672190666199, -0.48684659600257874, 0.6585838198661804, -0.21295496821403503, -0.8603798747062683, -0.2378721833229065, -0.18082886934280396, -0.24762630462646484, 1.5575754642486572, -1.0311812162399292, -0.26550203561782837, 0.2741091847419739, 0.09536873549222946, 1.0660512447357178, 0.031254496425390244, -0.7706637382507324, -0.3438994288444519, 0.4183802306652069, 0.056543223559856415, 1.0649772882461548, -0.42037978768348694, 0.5726789832115173, -0.8946480751037598, -0.3998197615146637, 0.8141019940376282, -0.41087019443511963, -0.019897330552339554, -0.2410096973180771, 0.05983295291662216, 0.8179805278778076, 0.7847476005554199, 0.0542643778026104, -0.26843151450157166, 1.2213881015777588, 1.0094139575958252, 0.8951742053031921, 0.44340914487838745, 0.10385974496603012, -0.6559739708900452, 0.4080459475517273, -0.37557175755500793, -0.25241512060165405, -0.05356258153915405, 0.9288387894630432, 0.20847350358963013, 0.25539568066596985, -0.252999484539032, 0.46630361676216125, 1.112371563911438, -0.0723315104842186, 0.37612003087997437, -0.39002180099487305, 0.7529183030128479, -0.42892903089523315, -0.2633242607116699, -0.283796101808548, -0.42066147923469543, -0.48122307658195496, -0.9219147562980652, -0.926423192024231, -0.05511750280857086, -0.15551415085792542, -0.5639204382896423, 1.0504399538040161, 0.011500689201056957, -0.24937118589878082, 0.06732646375894547, 0.7209139466285706, 0.07465066015720367, 0.5112355351448059, 0.43564948439598083, 0.33388930559158325, 0.8443106412887573, -0.31010743975639343, -0.02227524295449257, -0.7149226665496826, 0.677185595035553, -0.24039965867996216, 0.9332700371742249, 0.03694050759077072, -1.1060611009597778, -0.7240108847618103, -0.9231045246124268, 0.3295772075653076, -0.5072223544120789, 0.2223111093044281, 1.161226511001587, 0.5240313410758972, -0.8659222722053528, 1.1101343631744385, -0.09613917022943497, -0.3465707004070282, 0.595787525177002, 0.6756404042243958, 0.14239579439163208, -0.11671868711709976, -1.035086989402771, 0.6163373589515686, 0.5635541677474976, -0.4883287847042084, 0.04458676651120186, -1.0381289720535278, -0.9894217252731323, 0.29809972643852234, 0.35108447074890137, -0.9396955966949463, 1.1437116861343384, -0.31535887718200684, -1.2831032276153564, 0.2884218096733093, 0.7483901977539062, 0.33148854970932007, 0.587554395198822, -0.21589595079421997, -0.03840525075793266, -0.3901636600494385, -0.7765525579452515, 0.5763868689537048, 0.7979505062103271, 0.002653968520462513, -0.08271004259586334, -0.06414161622524261, -0.501488208770752, -0.09356340765953064, -0.8685517907142639, 0.5880496501922607, -0.11292709410190582, -0.06921673566102982, 0.16024231910705566, 0.2625195384025574, 0.01064071711152792, 0.07408738136291504, -0.3168375492095947, -1.1908998489379883, 0.6366827487945557, 0.20243100821971893, 0.5886537432670593, -0.9804912805557251, -0.4864397943019867, 0.12656833231449127, 0.19715292751789093, -0.3036848306655884, -0.7960442304611206, 0.36380788683891296, -0.2303660809993744, 0.41422584652900696, 0.20512282848358154, -0.8845654129981995, 0.6237384676933289, -0.349460244178772, -0.6509113311767578, 0.22213660180568695, 0.14914767444133759, 0.7574803233146667, -0.45836424827575684, 0.345260351896286, 0.0387306846678257, 0.5962576866149902, -0.7194448113441467, 1.0315008163452148, -0.09434351325035095, -0.5343111753463745, 0.24422147870063782, -0.24621213972568512, 0.17641974985599518, -0.45493555068969727, 0.1691959798336029, -0.6964564919471741, -0.006302315276116133, 0.25292935967445374, -0.41631636023521423, 1.02813720703125, -0.5990480780601501, 0.6585657000541687, 0.16145604848861694, -0.7229781746864319, -0.07844167202711105, 0.2777506411075592, -0.4249141216278076, -0.13394470512866974, 0.27044880390167236, 0.4275761544704437, -0.7118802666664124, 0.6394574046134949, 0.5896434783935547, 0.7042381763458252, -0.4858313202857971, -0.15731459856033325, 0.9520999789237976, -0.5247878432273865, 0.0770949050784111, 0.3462139964103699, 0.3165605068206787, 0.05320761725306511, 0.3716832101345062, -0.21510830521583557, 0.029113203287124634, -1.0564972162246704, -0.2148502916097641, 0.718083381652832, 0.48613646626472473, 0.742579996585846, 0.6370814442634583, -0.9254019856452942, -0.8762775659561157, -0.6552917957305908, 0.199107363820076, 1.7117215394973755, -0.45365816354751587, -0.1367262452840805, -0.6328065991401672, -0.2069263607263565, -0.27694860100746155, -0.35252001881599426, -0.5546380877494812, -0.3781391382217407, -0.5006063580513, -1.374489188194275, 0.845595121383667, -0.17855702340602875, 1.0108343362808228, -0.13321848213672638, -0.19806578755378723, -0.4186260402202606, 0.993335485458374, -0.7890401482582092, -0.7747482657432556, 0.45943647623062134, -0.7544138431549072, 0.05753698945045471, -0.03053162805736065, 0.09392718225717545, 0.10496174544095993, -0.9473986029624939, 0.49005308747291565, -0.5193406343460083, -0.34617751836776733, 0.6242247819900513, 0.9350931644439697, -1.033133625984192, -0.7621429562568665, 0.6914438009262085, 0.5620495676994324, -0.1842840611934662, -0.5149168968200684, 0.49556687474250793, -0.4203752875328064, -0.09971784055233002, -0.8031573295593262, -0.04119955003261566, 0.42010483145713806, -0.06092585623264313, 0.18691959977149963, -0.285757452249527, 0.46110013127326965, -0.904412567615509, 1.2831642627716064, 0.3018767833709717, -0.9922273755073547, 0.5005942583084106, -0.9510201215744019, -0.08182532340288162, 0.736685037612915, -0.9085491895675659, -0.3574591875076294, -0.6414775252342224, 0.5138425230979919, -0.6090141534805298, 0.03624381870031357, 0.11764922738075256, 0.8807768821716309, -0.36632657051086426, 0.5280814170837402, 0.20165085792541504, 0.5457710027694702, -0.05751268193125725, 0.5376010537147522, -1.0451241731643677, 0.3208302855491638, 0.24266134202480316, 0.4220513105392456, -0.0583038367331028, 0.055650126188993454, -0.822601318359375, -0.6602514386177063, -0.16464267671108246, -0.19939632713794708, -0.35603824257850647, -0.25127869844436646, -0.35497137904167175, -0.8129903078079224, -0.005850530695170164, -0.48864877223968506, -0.5087880492210388, -0.23718152940273285, -0.1345931738615036, -0.6447661519050598, -1.288692831993103, -1.6866580247879028, -0.31411102414131165, -0.7284495830535889, -1.203708529472351, 0.15451295673847198, 0.3566674292087555, 0.06161463260650635, -0.6905771493911743, -0.24137566983699799, -0.7547205686569214, 1.2227435111999512, -1.1107983589172363, 1.0922578573226929, 0.2513176500797272, -0.12986886501312256, 0.23584283888339996, -0.2937045693397522, 0.6826671957969666, -0.6206164956092834, 0.22376254200935364, -1.2255523204803467, 0.14145708084106445, -0.571599543094635, -0.8297562599182129, 0.14259541034698486, 0.5922607779502869, 0.5643054246902466, -0.4775542616844177, 0.08209720253944397, 0.7037481665611267, 1.655282735824585, -1.4202494621276855, 0.10714410990476608, 0.34550920128822327, 0.8692271709442139, 0.2626759111881256, -0.7362223267555237, 0.7220900058746338, 0.027020227164030075, 0.2071869969367981, 0.21802718937397003, -0.38523513078689575, -0.41086748242378235, -0.7746226787567139, 0.4292125999927521, 1.5054374933242798, 0.5759781002998352, 0.39110150933265686, -0.9061965942382812, -0.003950152080506086, -1.3643264770507812, -0.418584942817688, 1.0567597150802612, 0.9217255711555481, 0.5022448897361755, -0.15652859210968018, -0.24010442197322845, -0.5857170224189758, 0.13644571602344513, 0.28122881054878235, -0.5162537693977356, -0.8580169081687927, 0.22454670071601868, -0.12133950740098953, 0.8042095899581909, 0.43278923630714417, -0.14528833329677582, 0.8435373902320862, 14.713584899902344, 1.2310564517974854, -0.554016649723053, 0.6408796906471252, 0.61617112159729, 0.2859663963317871, -0.0036437357775866985, -0.3861333429813385, -0.9960089325904846, -0.3146395683288574, 1.1620686054229736, 0.14325444400310516, 0.8269433379173279, 0.07546432316303253, -0.02933380752801895, 0.24080665409564972, -0.39216142892837524, 1.0810118913650513, -0.04756375774741173, -1.5609211921691895, 0.45237332582473755, 0.1584809571504593, 0.7182482481002808, 1.3186240196228027, 1.0922149419784546, 0.8644713163375854, 0.3519345223903656, -0.2543419897556305, 0.5966876745223999, 0.11576207727193832, 0.9209619164466858, -0.0102454274892807, 0.006526565179228783, 0.6652860641479492, -0.7689655423164368, -0.24838495254516602, -0.6757597327232361, -0.7524531483650208, -0.06009824946522713, 0.3005831837654114, -0.8268792033195496, -0.7719146013259888, -0.0726710632443428, 0.3539891242980957, -0.09742939472198486, 0.4524769186973572, 0.03290299326181412, 0.48449811339378357, -0.5440398454666138, 0.07171553373336792, 0.2450724095106125, 0.29996415972709656, 0.07304079830646515, 0.32749685645103455, -0.07971126586198807, -0.6397770643234253, 0.35604748129844666, 0.705451488494873, -0.922154426574707, 0.06811851263046265, -0.11156798154115677, -0.17967642843723297, -0.07356751710176468, 0.8629065155982971, 0.49606773257255554, 0.04520738497376442, -0.20908886194229126, 0.21728301048278809, 1.1373227834701538, 0.490087628364563, -0.007328628096729517, -0.31030744314193726, 0.4789525866508484, -0.7675348520278931, -0.3299184739589691, 0.3617458939552307, -0.3641433119773865, -0.6849815249443054, -0.7031224370002747, -0.18959277868270874, 0.35879525542259216, -0.7645866274833679, -1.0654487609863281, 0.48103025555610657, -0.1437976360321045, -0.039148490875959396, 0.9268501996994019, -0.7443670630455017, 0.012036027386784554, 0.21720397472381592, -1.7593311071395874, -0.7506834864616394, 0.560869038105011, -0.03908305615186691, -0.2081894874572754, -0.0020902054384350777, 0.6669510006904602, 0.12103098630905151, -0.5918256044387817, 0.4482595920562744, 0.002017524093389511, -0.06233770400285721, -0.18912316858768463, -0.9263542294502258, 0.6096876263618469, -0.3397570252418518, -0.18203899264335632, 0.19217221438884735, -0.15786603093147278, 0.7046211361885071, -0.6506478786468506, 0.10264451801776886, -0.0458810068666935, -0.5033150315284729, 0.11383774131536484, -0.36499714851379395, -0.755791962146759, 0.22340957820415497, 0.2754039466381073, 0.028119781985878944, -0.06734257936477661, 0.4179351031780243, -1.0510739088058472, -0.40365496277809143, -0.462533563375473, 0.2216162085533142, 0.13634131848812103, -1.2149004936218262, -0.15184363722801208, 0.055730707943439484, 0.2752108573913574, -0.8310388922691345, -0.7568602561950684, 0.10613085329532623, 0.05432107299566269, -0.29357802867889404, 0.7878508567810059, -0.48276814818382263, 0.13874055445194244, 0.9584669470787048, 0.010434879921376705, -0.7496820688247681, 0.0831303745508194, -1.0260711908340454, 0.3859712779521942, 0.06104113161563873, 0.5805035829544067, -0.8046466112136841, 0.3717917799949646, 0.7558724880218506, -0.16341999173164368, -0.37645187973976135, -0.8370469808578491, -0.17575603723526, 0.08897862583398819, -0.8612513542175293, -0.05388673394918442, -0.11331024020910263, -0.07876365631818771, 0.12710538506507874, 0.5975953340530396, 0.8982366919517517, 0.23747174441814423, -1.2079523801803589, -0.2890494763851166, -0.06507151573896408, -0.055249009281396866, -1.0959168672561646, -0.6378464698791504, -1.4250366687774658, -0.0663587898015976, -1.3153729438781738, -0.2915806770324707, -0.3915754556655884, -0.33506450057029724, -0.3819310665130615, -0.29087963700294495, -0.14492684602737427, 0.8160358667373657, -0.3727821707725525, -0.07332304120063782, -0.08139792084693909, -0.33560824394226074, 1.0584465265274048, 0.6982149481773376, -0.6902114152908325, -0.10318051278591156, 0.07903049141168594, 0.27772057056427, 0.953089714050293, 0.6534637808799744, -0.30414295196533203, -0.8918564319610596, -1.3605248928070068, 0.9394869208335876, -0.16793841123580933, -0.028523903340101242, -0.5202613472938538, 0.6656312942504883, 0.21941377222537994, -0.05730878561735153, 0.6945934295654297, 0.36087533831596375, -1.0243420600891113, -0.7704566717147827, 0.3017102777957916, -0.889499843120575, -0.08131556212902069, 0.4495523273944855, -0.2550053000450134, 0.11852053552865982, 0.5861879587173462, 0.361978143453598, -0.545766294002533, -0.6579827070236206, 0.6022092700004578, -0.3574416935443878, 0.2542252242565155, -0.6382561922073364, -0.12998896837234497, -1.2902657985687256, 0.03535329923033714, -0.31302839517593384, 0.19605956971645355, -0.28509318828582764, 0.706438422203064, 0.3766959607601166, -1.1299827098846436, 0.22323831915855408, 0.11201642453670502, 0.12518340349197388, 0.056113190948963165, 0.4680206775665283, 0.48846787214279175, -0.37713193893432617, 0.3233134150505066, 0.34605735540390015, 0.17513690888881683, -0.5463995337486267, -0.2973524332046509, 0.8505620360374451, 0.015792259946465492, -0.6415722370147705, 1.0825695991516113, 0.37851041555404663, -1.4203290939331055, 0.42194807529449463, -1.070019006729126, 0.14027993381023407, -0.25787198543548584, 0.2253868579864502, 0.23470044136047363, -0.10620781034231186, 0.4742744565010071, 0.014101848937571049, 0.18896540999412537, -0.16400423645973206, -0.3319315016269684, 0.20825417339801788, -0.3731588125228882, -0.12775181233882904, 0.5925081372261047, 1.0297398567199707, -1.0237287282943726, -1.2853337526321411, -1.132939338684082, -0.1904255896806717, -0.07945142686367035, 0.598746657371521, -0.0048384228721261024, -0.8773555755615234, 0.7891591191291809, 0.9030736684799194, -0.007079590577632189, -0.019243797287344933, 0.38399043679237366, -0.38301119208335876, 0.6644492149353027, 0.01868806779384613, -0.9458032250404358, -0.23303687572479248, 1.128940224647522, 1.3812452554702759, -1.0424009561538696, 0.3754449784755707, -0.3067483901977539, -0.4269599914550781, 0.8436501622200012, 0.5301052927970886, -0.09623211622238159, 0.8813665509223938, -0.2178681641817093, -0.04749872907996178, -0.18780238926410675, -1.1496657133102417, 0.010643653571605682, 0.9538172483444214, 0.5428674221038818, 0.2397533804178238, 0.15614251792430878, 0.4911598861217499, 0.4468517601490021, -0.45690661668777466, 0.0030232220888137817, 0.6336146593093872, 0.0269122663885355, -0.6087691187858582, 0.8345593214035034, -0.14577466249465942, 0.6500111818313599, -0.6661972403526306, -0.34671550989151, 0.23641029000282288, 0.7888394594192505, -0.05901075899600983, 0.0683799758553505, 0.5879186987876892, 0.0031615146435797215, 0.7497456073760986, -0.07537601888179779, 0.8158562183380127, -0.20296302437782288, -0.42060303688049316, -0.09534578025341034, -0.7540150284767151, 0.19210831820964813, 0.18340907990932465, -0.3790901005268097, -0.3793562352657318, -0.38960912823677063, -0.23515675961971283, -0.2603515088558197, 0.42141133546829224, 0.7353147864341736, 0.30446597933769226, 0.3738701343536377, -0.23318204283714294, -0.7953065633773804, -1.0806150436401367, -0.5504444241523743, -0.009474296122789383, -0.27847129106521606, 0.04552849754691124, 0.0866725891828537, -0.4624235928058624, -0.5448266267776489]}, "authors": [{"authorId": "1781789550", "name": "Lorenzo Noci"}, {"authorId": "2007246756", "name": "Alexandru Meterez"}, {"authorId": "2265384015", "name": "Thomas Hofmann"}, {"authorId": "51931942", "name": "Antonio Orvieto"}], "references": [{"paperId": "1de3c3f956b613ae43057cce5090e5dd7d16de93", "title": "The Feature Speed Formula: a flexible approach to scale hyper-parameters of deep neural networks"}, {"paperId": "7371637145bd660a0cf92c83e864e788dfe8be79", "title": "Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos"}, {"paperId": "196803e4e33df1c35f1f05ba988c9508cc80c989", "title": "Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks"}, {"paperId": "b477787bbf822912c9b72af4f143d3b9eb57b99b", "title": "Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks"}, {"paperId": "ad91394aaa1dad451e1ea52acb73b525c9574642", "title": "Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit"}, {"paperId": "7a53033ea3601b731af0d3b911a8acebee66e799", "title": "Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit"}, {"paperId": "31a310772a0ce58692f0dcce9a0a972262bf95f3", "title": "Trajectory Alignment: Understanding the Edge of Stability Phenomenon via Bifurcation Theory"}, {"paperId": "c1738f21ea2460e1015d590906a4f43e155f60c8", "title": "The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit"}, {"paperId": "5875e7cc6d8941f078e32b51b8d36fc08f9c1774", "title": "Feature-Learning Networks Are Consistent Across Widths At Realistic Scales"}, {"paperId": "9aed6fe4e1ed92843c52b44431da73794903f52a", "title": "Depth Dependence of \u03bcP Learning Rates in ReLU MLPs"}, {"paperId": "ac96d733007b1f9bf078e5b66de4048dfae70286", "title": "ASDL: A Unified Interface for Gradient Preconditioning in PyTorch"}, {"paperId": "fe1be27f0f3ad3399ae5aea1e5d3eb06251a64af", "title": "Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "f739de44605f35481066fdff0f9be89d8a5728d6", "title": "Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width"}, {"paperId": "1ed9d79a1689526020b45551bdca174bfd27c24b", "title": "Width and Depth Limits Commute in Residual Networks"}, {"paperId": "e08e71820c09043b92b4614f4038e8da71e0e427", "title": "Handbook of Convergence Theorems for (Stochastic) Gradient Methods"}, {"paperId": "0a89a4bf945f7fe280b6e12080268362942e8866", "title": "Maximal Initial Learning Rates in Deep ReLU Networks"}, {"paperId": "b941c4bea1a71066f6f32275641aea1efc99b21b", "title": "Meta-Principled Family of Hyperparameter Scaling Strategies"}, {"paperId": "f6c05626d3f94ccf0935ad1e9c144d1f41ae8135", "title": "Understanding Edge-of-Stability Training Dynamics with a Minimalist Example"}, {"paperId": "0484a65707706e11eaa53fde8817c3122f07550d", "title": "On the infinite-depth limit of finite-width neural networks"}, {"paperId": "f21a88af78583bd7959b121b800eed5c1f7b7b99", "title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability"}, {"paperId": "84f6ab620eb7b112e5b2ca64b305970894e679c1", "title": "Adaptive Gradient Methods at the Edge of Stability"}, {"paperId": "5eeb80dc67590422db64ca95ec0aded24799cfb6", "title": "Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse"}, {"paperId": "eb7d596fb4ab48c2db1b0ee6d8127df7a5b5ebeb", "title": "The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization"}, {"paperId": "29b8fcb5426e40ccd51c3f87c58232cb832c546d", "title": "Self-consistent dynamical field theory of kernel evolution in wide neural networks"}, {"paperId": "0f3b6cb07a8edb78a40ee478708eedcd03242503", "title": "Understanding Gradient Descent on Edge of Stability in Deep Learning"}, {"paperId": "247086a46f289035a5c74d758c359890a568f596", "title": "Understanding the unstable convergence of gradient descent"}, {"paperId": "0b0d7d87c58d41b92d907347b778032be5966f60", "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"}, {"paperId": "2ad7d5aec1e72abb0be74954d2fc8a242fea390c", "title": "Analytic Insights into Structure and Rank of Neural Network Hessian Maps"}, {"paperId": "bd99a7473fbc05f174462001a5f5139e98e32cf0", "title": "Precise characterization of the prior predictive distribution of deep ReLU networks"}, {"paperId": "b79ad53508c38c9964483b16fffb7d2e4a3ad0e1", "title": "The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width Limit at Initialization"}, {"paperId": "518838d706dd78481813b4a40745ffdeecaa2e34", "title": "Vanishing Curvature and the Power of Adaptive Methods in Randomly Initialized Deep Networks"}, {"paperId": "6d820827d44159c5e0cc6c65065a41ab4a4bd14c", "title": "Asymptotics of representation learning in finite Bayesian neural networks"}, {"paperId": "026bb8a1066f50ddc8797e1341353603149a8cb8", "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability"}, {"paperId": "4cf3f2d9152e0b3ac87161be27daed2bb6604078", "title": "Hyperparameter Transfer Learning with Adaptive Complexity"}, {"paperId": "064cdfbd39babf8941fcf96f5d9b264785cf24bf", "title": "Hyperparameter Transfer Across Developer Adjustments"}, {"paperId": "758cf7cd62bb8b62a6a4bc550a34e0a574bbbcb2", "title": "Stable ResNet"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "8095cb807d1c7577376c470bb6d43702634524c2", "title": "Tensor Programs II: Neural Tangent Kernel for Any Architecture"}, {"paperId": "82b5ee0ae468cd2e0b62df96f42b5b5480c75510", "title": "Infinite attention: NNGP and NTK for deep attention networks"}, {"paperId": "30ddacf5db38b5c9b4788c97bfc2dd73ab6ca040", "title": "The large learning rate phase of deep learning: the catapult mechanism"}, {"paperId": "77aecf26808e57974db6d4feb5ee1a7f375cb4c9", "title": "The Break-Even Point on Optimization Trajectories of Deep Neural Networks"}, {"paperId": "71022c0c51f1e06384ff211467d04230dee96f51", "title": "Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss"}, {"paperId": "6e5d89c2b3b5ead2c3ab389534de62a28c1e8e6e", "title": "PyHessian: Neural Networks Through the Lens of the Hessian"}, {"paperId": "867526be4632f76c8795a7a13aa9aa74003bcb0f", "title": "Non-Gaussian processes and neural networks at finite widths"}, {"paperId": "2d8591778f3af1bc98bee3fe9ac932282780abd9", "title": "Finite Depth and Width Corrections to the Neural Tangent Kernel"}, {"paperId": "36451fe94b7b1f978b8301cf3f7305566bd3b454", "title": "Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks"}, {"paperId": "1029daa28aa772e441470e61bdd610c222e92932", "title": "On Exact Computation with an Infinitely Wide Neural Net"}, {"paperId": "9f9fc406c76255fec51a6196ce167c0ff1d1efc0", "title": "Wide neural networks of any depth evolve as linear models under gradient descent"}, {"paperId": "fa260bd84ebb6e87fdba500d12b08e8c643f1b02", "title": "Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit"}, {"paperId": "b4f174539e0123fd6bfdc1c6e97b353472a10eff", "title": "SGD: General Analysis and Improved Rates"}, {"paperId": "62a0bafd54099a79d78a013611a0c7d38e237032", "title": "On Lazy Training in Differentiable Programming"}, {"paperId": "d9a230f886cd4fa2ab019585580e26098c8992c5", "title": "Products of Many Large Random Matrices and Gradients in Deep Neural Networks"}, {"paperId": "58fc942fee2a1bd98296259267095289cc1f2a96", "title": "Truncated Back-propagation for Bilevel Optimization"}, {"paperId": "43e33e80d74205e860dd4b8e26b7c458c60e201a", "title": "Deep Convolutional Networks as shallow Gaussian Processes"}, {"paperId": "5f9e2f6d4a844189b2e34da8fd0ba282f3f36c6f", "title": "On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length"}, {"paperId": "7a84a692327534fd227fa1e07fcb3816b633c591", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"paperId": "9c7de616d16e5643e9e29dfdf2d7d6001c548132", "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport"}, {"paperId": "1269e191091eadeed6f246cf5a6692b178bb4d94", "title": "Super-convergence: very fast training of neural networks using large learning rates"}, {"paperId": "075556dd42900a6bc4552a2f2531ba21b9b7b4c0", "title": "Deep Neural Networks as Gaussian Processes"}, {"paperId": "8151fbde700614ab25d6165b9ce5f76456c180d4", "title": "Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond"}, {"paperId": "892f9a2f69241feec647856cd26bed37e04fd747", "title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization"}, {"paperId": "edbe43ac3b71814a68aa84ea81de9df8abf8ca7f", "title": "Second-order Optimization for Neural Networks"}, {"paperId": "c6c745d7fae9aad4294549d829f7e7415ffb1709", "title": "Non-stochastic Best Arm Identification and Hyperparameter Optimization"}, {"paperId": "93bc65d2842b8cc5f3cf72ebc5b8f75daeacea35", "title": "Scalable Bayesian Optimization Using Deep Neural Networks"}, {"paperId": "e2820bffe5b42cb7d88b7f65c12171c62ab4aae2", "title": "Gradient-based Hyperparameter Optimization through Reversible Learning"}, {"paperId": "0173ca962e4ab3d084c89568345e06f67d3d7efc", "title": "Hyperparameter Search in Machine Learning"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "189ef6bea81fe8ee3ed3214b9394f7e2027eff98", "title": "New Insights and Perspectives on the Natural Gradient Method"}, {"paperId": "d0b0c3e5a1e768490bc9b759685930541957508b", "title": "Introductory Lectures on Convex Optimization - A Basic Course"}, {"paperId": "4d289c568cf52f9b448e81e5fdfbbac9f99c3090", "title": "Efficient Transfer Learning Method for Automatic Hyperparameter Tuning"}, {"paperId": "2e2089ae76fe914706e6fa90081a79c8fe01611e", "title": "Practical Bayesian Optimization of Machine Learning Algorithms"}, {"paperId": "188e247506ad992b8bc62d6c74789e89891a984f", "title": "Random Search for Hyper-Parameter Optimization"}, {"paperId": "6c7384845f7d8347a6daf393ce1586e03dca971b", "title": "Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "0b9f042b3588ed8588e907d7d0e139a008c517cd", "title": "Scalable Hyperparameter Transfer Learning"}, {"paperId": "0800f8b1f48b6e09525270ddbfeb2ad550e3e97f", "title": "Chaos In Dynamical Systems"}, {"paperId": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed", "title": "Efficient BackProp"}, {"paperId": "a22b36cf5dba3e85eb064220be7ef03be4efba48", "title": "Bayesian learning for neural networks"}]}