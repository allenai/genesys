{"paperId": "c57467e652f3f9131b3e7e40c23059abe395f01d", "title": "SpectFormer: Frequency and Attention is what you need in a Vision Transformer", "abstract": "Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT \\cite{dosovitskiy2020image}, DeIT, \\cite{touvron2021training}) similar to the original work in textual models or more recently based on spectral layers (Fnet\\cite{lee2021fnet}, GFNet\\cite{rao2021global}, AFNO\\cite{guibas2021efficient}). We hypothesize that both spectral and multi-headed attention plays a major role. We investigate this hypothesis through this work and observe that indeed combining spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance, it improves the top-1 accuracy by 2\\% on ImageNet compared to both GFNet-H and LiT. SpectFormer-S reaches 84.25\\% top-1 accuracy on ImageNet-1K (state of the art for small version). Further, Spectformer-L achieves 85.7\\% that is the state of the art for the comparable base version of the transformers. We further ensure that we obtain reasonable results in other scenarios such as transfer learning on standard datasets such as CIFAR-10, CIFAR-100, Oxford-IIIT-flower, and Standford Car datasets. We then investigate its use in downstream tasks such of object detection and instance segmentation on the MS-COCO dataset and observe that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved. Hence, we believe that combined spectral and attention layers are what are needed for vision transformers.", "venue": "arXiv.org", "year": 2023, "citationCount": 17, "influentialCitationCount": 2, "openAccessPdf": {"url": "http://arxiv.org/pdf/2304.06446", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers and investigates its use in downstream tasks such of object detection and instance segmentation on the MS-COCO dataset and observes that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved."}, "embedding": {"model": "specter_v2", "vector": [0.8272873759269714, 0.6766538023948669, -0.24033820629119873, 0.14819368720054626, -0.39351803064346313, 0.007601067423820496, 0.900511622428894, -0.5401678681373596, -0.2956203818321228, -0.4600857198238373, 0.7147469520568848, 0.5025339126586914, 0.33933505415916443, -0.2282392978668213, -0.060834553092718124, 0.16213259100914001, -0.3825468122959137, 0.20382606983184814, 0.44764822721481323, -0.23945948481559753, 0.26599207520484924, -0.8535944819450378, -1.0750086307525635, 0.2359836846590042, 0.2002466470003128, 1.0403088331222534, 0.592487633228302, 0.9844551086425781, 0.04822392016649246, 0.518578052520752, 0.26359042525291443, -0.46311816573143005, -0.018502037972211838, -0.14191965758800507, -0.6828171610832214, 0.25220125913619995, 0.8983973264694214, 0.018783969804644585, -0.5466111302375793, 0.9174190163612366, -0.1554841697216034, 0.14548934996128082, 0.8718860745429993, -0.7321279048919678, -0.3884807825088501, 0.7008945941925049, 0.4749038815498352, 1.0853644609451294, -0.578240692615509, -0.5385745763778687, 1.3878992795944214, -1.5761646032333374, 0.09263158589601517, 1.4381799697875977, 0.28730618953704834, 0.32171356678009033, -0.14128845930099487, -0.4693808853626251, 0.5836113691329956, 0.5889472365379333, -0.4950297772884369, -0.20396406948566437, 0.009454961866140366, -0.20119908452033997, 1.8600049018859863, -0.7369017004966736, -0.00026575743686407804, 0.5016875863075256, -0.16415058076381683, 1.2209842205047607, -0.21663109958171844, -0.612025260925293, -0.5044317841529846, -0.02885814942419529, 0.44734904170036316, 0.7661473751068115, -0.5457327365875244, 0.030785653740167618, -1.2181544303894043, 0.04112153872847557, 0.5630566477775574, 0.04707532376050949, -0.11174937337636948, -0.0695795789361, -0.39938363432884216, 0.7840916514396667, 0.9232336282730103, 0.9820526242256165, -0.07839542627334595, 1.0797100067138672, 0.5182011723518372, -0.10577961802482605, -0.10226856917142868, 0.26219531893730164, 0.33181262016296387, 0.8899697065353394, -0.7366549372673035, -0.07065117359161377, -0.24866485595703125, 1.162230134010315, 0.09460089355707169, 0.5645865201950073, -0.5736903548240662, 0.10944748669862747, 1.3473602533340454, 0.18643711507320404, 0.3775870203971863, -0.6463229060173035, -0.08396599441766739, -1.000504732131958, -0.296474814414978, -0.7967202663421631, 0.13321371376514435, -0.7504580020904541, -0.7165895700454712, -1.2573251724243164, -0.37826505303382874, 0.7397815585136414, -1.004665732383728, 0.5268831253051758, -0.6231082081794739, 0.3909547030925751, -0.16146889328956604, 0.714601457118988, 0.6736524105072021, 0.4495992660522461, 0.6294097900390625, 0.5363621115684509, 1.2017155885696411, -1.1031078100204468, -0.46206337213516235, -1.0264172554016113, -0.184292271733284, -0.464321106672287, 0.37457960844039917, -0.1525641232728958, -0.8537597060203552, -1.1802892684936523, -0.7885465621948242, 0.019583793357014656, -0.501281201839447, 0.319529265165329, 0.9774658679962158, 0.440105140209198, -0.9537689089775085, 0.6364095211029053, 0.17482146620750427, -0.43898043036460876, 0.7381351590156555, 0.048245254904031754, 0.5530276298522949, 0.05796458199620247, -1.2875854969024658, 0.5383167266845703, 0.10661730915307999, -0.7312700748443604, -0.6729376912117004, -0.41095465421676636, -0.9975737929344177, 0.36479803919792175, 0.37686896324157715, -0.8876885771751404, 1.2872302532196045, -0.7696304321289062, -0.6730724573135376, 0.7622296810150146, -0.4312717020511627, -0.29046544432640076, -0.18862296640872955, -0.2253129929304123, -0.27592530846595764, -0.08763318508863449, 0.015688316896557808, 0.5616496205329895, 1.027238368988037, -0.22397615015506744, -0.40330103039741516, 0.2480269819498062, -0.4775663912296295, -0.016336334869265556, -0.6004185676574707, 0.8388882279396057, -0.1563890278339386, -0.3958606421947479, 0.08727840334177017, 0.8211617469787598, 0.4307684302330017, -0.29713284969329834, -0.08731058239936829, -1.1552547216415405, 1.1124032735824585, 0.6579407453536987, 0.3560670018196106, -1.070225715637207, -0.7307322025299072, -0.08627662062644958, -0.0069173541851341724, -0.2523663341999054, -0.7035559415817261, 0.2604806423187256, -0.44934287667274475, 0.057353001087903976, 0.09229260683059692, -0.8273177742958069, 0.03942041099071503, -0.402622252702713, -0.8042775392532349, 0.09249524027109146, 0.5074836015701294, 0.9889285564422607, -0.6987715363502502, -0.16887891292572021, 0.04069837927818298, 0.5318637490272522, -0.9977260828018188, 0.7937960028648376, -0.21473270654678345, -0.3432713449001312, -0.0168001651763916, -0.07826374471187592, -0.24804328382015228, -0.7023435235023499, 0.2806183099746704, -1.0817909240722656, 0.17590974271297455, 0.36509209871292114, -0.11408445984125137, 1.2661395072937012, 0.04039956256747246, 0.6622528433799744, -0.24159394204616547, -0.9697784781455994, 0.19358216226100922, 0.35908418893814087, -0.30067554116249084, -0.6425514221191406, 0.44729897379875183, 0.30600205063819885, -0.6456999182701111, 0.4374515116214752, 0.6655632853507996, 0.8550211191177368, -0.07575885206460953, -0.006772889290004969, 1.3347299098968506, -0.27954524755477905, -0.013484110124409199, 0.7646311521530151, 0.35756731033325195, 0.440887451171875, 0.29262295365333557, -0.5438215136528015, 0.25595155358314514, -0.9231590032577515, -0.306605726480484, 0.574045717716217, 0.16558869183063507, 1.362133264541626, 0.3270164728164673, -0.6445093750953674, -0.567427933216095, -0.01963181421160698, 0.5889753103256226, 1.645119309425354, 0.05651942268013954, -0.11100639402866364, -0.7518615126609802, -0.11821507662534714, -0.6206530332565308, -0.36781615018844604, -0.5056959986686707, -0.3637107312679291, -0.2042606621980667, -1.2663382291793823, 0.6291292309761047, 0.5427268147468567, 1.243518352508545, -0.5493397116661072, -0.7147026062011719, -0.3849671185016632, 0.0009747142903506756, -0.8791977167129517, -1.0989279747009277, 0.5676798224449158, 0.01740112155675888, -0.29236260056495667, -0.33504366874694824, -0.38684388995170593, -0.008017448708415031, -0.09846600890159607, 0.8330317139625549, -0.6746788620948792, -0.6187453269958496, 0.6129825711250305, 0.4145263135433197, -0.9211670756340027, -0.031657859683036804, 0.43196043372154236, -0.2723389267921448, 0.44998762011528015, 0.02522229216992855, 0.20015078783035278, -0.2574557363986969, 0.13931234180927277, -0.375487357378006, -0.15219274163246155, -0.21836404502391815, 0.1821497529745102, 0.6340352892875671, -0.3490684926509857, 0.19974097609519958, -1.176127314567566, 0.4061891734600067, -0.050194595009088516, -0.05682757869362831, 0.04555322229862213, -0.12616898119449615, -0.5989671945571899, 0.15277358889579773, -0.6536545753479004, -0.2313166856765747, -0.28568270802497864, 0.7360684275627136, -0.6793321371078491, -0.06491606682538986, 0.016913199797272682, 0.33566203713417053, -0.0472840815782547, 0.7871652245521545, 0.40838533639907837, -0.13403929769992828, 0.3102148473262787, 0.4300132691860199, -1.062216877937317, 0.9052627086639404, 0.24796214699745178, 0.14782798290252686, 0.06683466583490372, -0.03524118661880493, -0.8133493661880493, -0.8569017648696899, -0.8437058329582214, -0.2435731142759323, -0.688629150390625, 0.4024668037891388, -0.6750551462173462, -0.9212534427642822, 0.2399229258298874, -0.8692600131034851, -0.010399563238024712, 0.1309574991464615, -0.4045058786869049, -0.5600055456161499, -1.1243401765823364, -0.5707110166549683, -0.6936026811599731, -0.5016096234321594, -0.7143212556838989, 0.34874847531318665, 0.5022768378257751, -0.09685791283845901, -0.4657505452632904, -0.47263404726982117, -0.26495131850242615, 1.2592674493789673, -0.3849351108074188, 0.48478373885154724, -0.24852417409420013, -0.37344038486480713, 0.27918747067451477, -0.31590351462364197, 0.12719115614891052, -0.3423523008823395, -0.05470705032348633, -1.4329991340637207, 0.3999830186367035, -0.2664315700531006, -0.6249149441719055, 0.9544483423233032, 0.1947317123413086, 1.108760118484497, 0.2728749215602875, -0.3749941289424896, 0.26577669382095337, 1.4958488941192627, -0.7085625529289246, 0.2423873394727707, 0.06885208189487457, 0.9160348773002625, -0.07079153507947922, -0.42865607142448425, 0.43917009234428406, 0.7436351180076599, -0.03312467783689499, 0.6793578267097473, -0.28271475434303284, -0.6783128976821899, -0.43863096833229065, 0.16714979708194733, 0.8220971822738647, -0.11267662048339844, -0.13348910212516785, -1.190914273262024, 0.9581881761550903, -1.2230991125106812, -0.9887201189994812, 0.6420136094093323, 0.5079256296157837, 0.14207038283348083, -0.4464241862297058, 0.03587532043457031, -0.04124702513217926, 0.6776506304740906, 0.3325745165348053, -0.17898529767990112, -0.297070175409317, -0.311717689037323, 0.828934371471405, 0.3868306577205658, 0.5238534808158875, -0.9132806658744812, 0.4940222501754761, 14.770535469055176, 0.7927440404891968, -0.07811111211776733, 0.6639881730079651, 0.7262483835220337, 0.4219296872615814, -0.13532182574272156, 0.04294402152299881, -1.1065928936004639, -0.44020700454711914, 0.9250469207763672, 0.07964940369129181, 0.4635048806667328, 0.35387495160102844, -0.6835862994194031, 0.03532710671424866, -0.39735665917396545, 0.9286122918128967, 0.9052567481994629, -1.3692811727523804, 0.36693140864372253, 0.18084166944026947, 0.15363410115242004, 0.45268309116363525, 1.1081839799880981, 0.36180710792541504, 0.13640257716178894, -0.5529680252075195, 0.7723849415779114, 0.28655657172203064, 1.0753382444381714, -0.03258413076400757, 0.11021742224693298, 0.2198689728975296, -1.2259747982025146, -0.30717599391937256, -0.6189886331558228, -0.7187129259109497, -0.15081344544887543, 0.00840843003243208, -0.44753068685531616, -0.45789793133735657, 0.18793198466300964, 0.7613569498062134, 0.19434358179569244, 0.5389547944068909, 0.17791883647441864, 0.5201104879379272, 0.12810064852237701, -0.13042794167995453, 0.30984291434288025, 1.2041782140731812, 0.2288329005241394, 0.2628132998943329, -0.06260887533426285, 0.237609401345253, 0.30055558681488037, 0.5681853294372559, -0.5570903420448303, -0.47296226024627686, -0.1823456734418869, 0.013708824291825294, -0.20369325578212738, 1.1991039514541626, -0.023417046293616295, 0.16152861714363098, -0.31973469257354736, 0.14255169034004211, 0.21909305453300476, 0.18646016716957092, -0.7054870128631592, -0.2866017818450928, -0.15633468329906464, -0.19182856380939484, 0.34651321172714233, 0.5208234786987305, -0.01069393940269947, -0.6557624340057373, -1.1843385696411133, -0.06216651573777199, 0.6537429094314575, -1.0348080396652222, -0.7707718014717102, 0.9314545392990112, -0.2597436010837555, -0.006240353919565678, 0.7758537530899048, -0.901077151298523, -0.2935417890548706, 0.4293569326400757, -1.6435295343399048, -0.956159770488739, -0.22415880858898163, 0.14094692468643188, 0.011892753653228283, -0.2487027645111084, 0.8817351460456848, 0.13499076664447784, 0.2216518223285675, 0.037016335874795914, -0.9841652512550354, -0.048195600509643555, -0.14331133663654327, -0.9066770076751709, 0.4113454222679138, 0.1717502623796463, 0.17125941812992096, -0.14621713757514954, 0.0732676312327385, 0.47978803515434265, -0.5157308578491211, -0.05704369768500328, 0.6543732285499573, -0.6398788690567017, -0.24619990587234497, -0.6312574148178101, -0.7960019111633301, 0.1046089380979538, 0.9101902842521667, 0.24274320900440216, 0.03993324190378189, 0.06958035379648209, -0.9291919469833374, -0.00751922931522131, -0.6645930409431458, -0.42838039994239807, 0.44935911893844604, -1.1608167886734009, -0.5623912811279297, -0.10596735775470734, -0.13031746447086334, -0.9037854075431824, -0.2639809548854828, -0.17202557623386383, 0.11803679913282394, -0.2491258978843689, 1.0433381795883179, -0.1511976271867752, 0.4181425869464874, 0.5878222584724426, -0.4311034083366394, -0.6033151149749756, -0.5016211271286011, -0.5786082148551941, 0.20473666489124298, 0.21445488929748535, 0.4285523295402527, -0.627986490726471, 0.1318732053041458, 0.23217013478279114, 0.18499694764614105, -0.20038196444511414, -0.6737171411514282, 0.07017725706100464, -0.25077158212661743, -0.19002798199653625, -0.06851320713758469, -0.006717859301716089, -0.14101283252239227, 0.438211590051651, 0.48311305046081543, 0.26267343759536743, 0.19409653544425964, -0.6505860686302185, 0.0026244758628308773, -0.29483741521835327, -0.09186702221632004, -0.41369402408599854, -0.6931743025779724, -1.113304615020752, -0.13460302352905273, -1.130710482597351, -0.24087820947170258, -0.6060310006141663, 0.10841742902994156, 0.5315693616867065, -0.5031495094299316, 0.3266335129737854, 0.33936449885368347, 0.24511483311653137, 0.08531805127859116, -0.6993536949157715, -0.7771902680397034, 0.8932252526283264, 0.9962890148162842, -0.8124252557754517, 0.5497850179672241, -0.3190782964229584, -0.2724810242652893, 0.49453261494636536, 0.1835537999868393, -0.4334888458251953, -0.8660708665847778, -0.9421452879905701, 0.3451923727989197, -0.1704920083284378, 0.49745503067970276, -1.0961167812347412, 1.0528329610824585, 0.5735517740249634, 0.18224914371967316, 0.0951550155878067, 0.4387599229812622, -0.7721399068832397, -0.7030091881752014, 0.34888580441474915, -0.6504715085029602, -0.21040628850460052, -0.029540393501520157, -0.4910166561603546, -0.427134245634079, 0.8296937942504883, 0.48442667722702026, -1.3608094453811646, -0.8086476922035217, 0.4602326452732086, -0.4598028361797333, 0.26516133546829224, -0.21768204867839813, -0.24396350979804993, -1.4097950458526611, -0.3952741324901581, -0.2919324040412903, 0.10264281183481216, -0.863484799861908, 0.8175275921821594, 0.5846678614616394, -1.0522030591964722, 0.004772537387907505, 0.20231828093528748, -0.16151511669158936, -0.23487325012683868, 0.4760601818561554, 0.2646194100379944, 0.10171478241682053, 0.47571420669555664, -0.033291857689619064, 0.00760107534006238, -0.5822106599807739, -0.03033546917140484, 1.4026463031768799, 0.1021547019481659, -0.19740872085094452, 1.1495879888534546, 0.20853513479232788, -0.5491613745689392, 0.25633278489112854, -0.7208830714225769, -0.7312328219413757, 0.16673867404460907, 0.613457202911377, -0.05469314008951187, -0.13227365911006927, -0.23094885051250458, -0.7181065678596497, 0.4744659960269928, -0.008800237439572811, -0.5457282066345215, 0.4129835069179535, 0.2008807361125946, 0.023955482989549637, 0.4239576756954193, 0.5582655072212219, -1.0626978874206543, -0.9023690819740295, -0.846563458442688, -0.4997142255306244, -0.0906267911195755, 0.3847832977771759, -0.3983500003814697, -1.0601277351379395, 0.9884499311447144, 0.7371375560760498, 0.7773837447166443, 0.4086487591266632, 0.07341945171356201, -0.1956748068332672, 0.4081781208515167, 0.045389704406261444, -0.5647414922714233, -0.01920618861913681, 1.6360448598861694, 1.3046032190322876, -0.39176324009895325, -0.02847445383667946, -0.6771947145462036, -0.6506467461585999, 0.8252406716346741, 0.6358068585395813, -0.6464507579803467, 0.956599771976471, -0.13988657295703888, -0.3609989285469055, 0.16418328881263733, -1.0122954845428467, -0.7599788308143616, 0.9621002078056335, 1.418390154838562, 0.6331051588058472, -0.0709029883146286, 0.4669739305973053, 0.6769546866416931, 0.14347124099731445, -0.19442975521087646, 0.5043994784355164, -0.1386847347021103, -0.5062733292579651, 0.5811771154403687, -0.05648021027445793, 0.6093859076499939, -0.8117709755897522, -0.30816617608070374, -0.04478304833173752, 0.47332069277763367, 0.24680614471435547, 0.31006842851638794, 0.8172418475151062, 0.32289913296699524, 0.694206178188324, 0.20738781988620758, 0.8504071235656738, -0.6364045143127441, -0.3751501739025116, 0.026086045429110527, -0.8060529232025146, 0.026165015995502472, -0.777265727519989, -0.7385993003845215, -0.42796841263771057, 0.21658796072006226, -0.09548039734363556, -0.3967106342315674, 0.09244471043348312, 0.9806830286979675, 0.32321226596832275, 0.9339877367019653, -0.34595346450805664, -0.9313385486602783, -0.05284999683499336, -0.8570562601089478, 0.14056295156478882, -0.5335327386856079, 0.2182500958442688, -0.534691333770752, -0.06327733397483826, 0.20101802051067352]}, "authors": [{"authorId": "48494603", "name": "Badri N. Patro"}, {"authorId": "145460361", "name": "Vinay P. Namboodiri"}, {"authorId": "1791302", "name": "Vijay Srinivas Agneeswaran"}], "references": [{"paperId": "a9d13e00c30fa82ec201587719efc11a11fceb7e", "title": "Efficiency 360: Efficient Vision Transformers"}, {"paperId": "3e448df5aa191f7a3945d0fd609c8bc5966a2333", "title": "HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions"}, {"paperId": "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4", "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning"}, {"paperId": "bf6ce546c589fa8054b3972b266532664914bd21", "title": "Fast Vision Transformers with HiLo Attention"}, {"paperId": "dbf6e95cb618f207f029276a6df11f4a9a6313d4", "title": "Inception Transformer"}, {"paperId": "d2f63b56fc6bc373f5c023454c2b253326962865", "title": "DeiT III: Revenge of the ViT"}, {"paperId": "f634a09747e4ca11754a9bfdccf7485c884f9f86", "title": "TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation"}, {"paperId": "259c681c76335540e13081efad584efdf9101868", "title": "DaViT: Dual Attention Vision Transformers"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc", "title": "MaxViT: Multi-Axis Vision Transformer"}, {"paperId": "c67dcbfcb26789a79c94c75b953b472d314be990", "title": "Error Correction Code Transformer"}, {"paperId": "48e84128b0f288f544176138805a97fbe592a1dd", "title": "DynaMixer: A Vision MLP Architecture with Dynamic Mixing"}, {"paperId": "f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1", "title": "UniFormer: Unifying Convolution and Self-Attention for Visual Recognition"}, {"paperId": "e5cb26148791b57bfd36aa26ce2401e231d01b57", "title": "Vision Transformer with Deformable Attention"}, {"paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e", "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"}, {"paperId": "b476c932e959cfe645911786f1a070c70b5375c6", "title": "An Image Patch is a Wave: Phase-Aware Vision MLP"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"}, {"paperId": "a9c352cce882f31e7f28dbe96794e10b089c6623", "title": "Scaled ReLU Matters for Training Vision Transformers"}, {"paperId": "58970a426b687bb080b7fed3b4b78ab1ebaa56f4", "title": "Hire-MLP: Vision MLP via Hierarchical Rearrangement"}, {"paperId": "dfa8694c3329c31205cd2454fa59c9e607b43695", "title": "Contextual Transformer Networks for Visual Recognition"}, {"paperId": "f75cddf2d42ed01b34686704eb3504becef67442", "title": "CycleMLP: A MLP-like Architecture for Dense Prediction"}, {"paperId": "71363797140647ebb3f540584de0a8758d2f7aa2", "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision"}, {"paperId": "0b036cd5dfc49d835d0c759c8ca31d89f2410e65", "title": "CMT: Convolutional Neural Networks Meet Vision Transformers"}, {"paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"}, {"paperId": "9b6af0e358e76d22f209c75b1702c3e6ea7815b1", "title": "Global Filter Networks for Image Classification"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "1fb10189c500e4902cd1b5afd406f57323d21be8", "title": "VOLO: Vision Outlooker for Visual Recognition"}, {"paperId": "2435ffb8ed3212156d6b6f19f633a861399cf30e", "title": "Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "2e8149dafb864ec3675087c99bf5572fcf4eb170", "title": "RegionViT: Regional-to-Local Attention for Vision Transformers"}, {"paperId": "a0964686d80e173529efca6377f47e6a1b2fe69a", "title": "Less is More: Pay Less Attention in Vision Transformers"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "cc9f3a61ea4eaabf43cbb30cd1dd718074932679", "title": "All Tokens Matter: Token Labeling for Training Better Vision Transformers"}, {"paperId": "5b68522f58b61e7235b852677337ef3725075fd9", "title": "Co-Scale Conv-Attentional Image Transformers"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "0eff37167876356da2163b2e396df2719adf7de9", "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"}, {"paperId": "96da196d6f8c947db03d13759f030642f8234abf", "title": "DeepViT: Towards Deeper Vision Transformer"}, {"paperId": "610b302950a19acef1c45456111dcd495f638c18", "title": "ConViT: improving vision transformers with soft convolutional inductive biases"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "title": "Bottleneck Transformers for Visual Recognition"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "2f7dc1ee85e9f6a97810c66016e09ffeed684f03", "title": "Fourier Neural Operator for Parametric Partial Differential Equations"}, {"paperId": "da60e046aac895b5775ed34bde45beb86aad0fe8", "title": "Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection"}, {"paperId": "fb93ca1e004cbdcb93c8ffc57357189fa4eb6770", "title": "ResNeSt: Split-Attention Networks"}, {"paperId": "c2c083df88e88223e1a411e61040b94c233b1b63", "title": "MMDetection: Open MMLab Detection Toolbox and Benchmark"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "04957e40d47ca89d38653e97f728883c0ad26e5d", "title": "Cascade R-CNN: Delving Into High Quality Object Detection"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "1a857da1a8ce47b2aa185b91b5cb215ddef24de7", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "f6e0856b4a9199fa968ac00da612a9407b5cb85c", "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "a83cec6a91701bd8500f8c43ad731d4353c71d55", "title": "3D Object Representations for Fine-Grained Categorization"}, {"paperId": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649", "title": "Understanding the difficulty of training deep feedforward neural networks"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "02b28f3b71138a06e40dbd614abf8568420ae183", "title": "Automated Flower Classification over a Large Number of Classes"}, {"paperId": "20b844e395355b40fa5940c61362ec40e56027aa", "title": "Neural networks"}, {"paperId": "6f20e254e3993538c79e0ff2b9b8f198d3359cb3", "title": "Receptive fields of single neurones in the cat's striate cortex"}, {"paperId": "955f90930d48750e7239478b4eed440eb84131cd", "title": "FourierFormer: Transformer Meets Generalized Fourier Integral Theorem"}, {"paperId": "32e0057c9a06d23182ff41553c9df1c9a8c4b757", "title": "Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}]}