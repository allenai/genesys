{"paperId": "5be7e6b04c5a240cff340034aae2b57c677e211f", "title": "A Survey on Efficient Inference for Large Language Models", "abstract": "Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.", "venue": "arXiv.org", "year": 2024, "citationCount": 12, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A comprehensive survey of the existing literature on efficient LLM inference is presented, analyzing the primary causes of the inefficient LLM inference and introducing a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization."}, "embedding": {"model": "specter_v2", "vector": [0.08537863940000534, 0.5583800077438354, -0.2186448872089386, -0.0033212071284651756, -0.6926754117012024, -0.08776144683361053, 0.6204878091812134, -0.20790717005729675, -0.6895163059234619, -0.07963521033525467, 0.33748164772987366, -0.22722475230693817, 0.43907642364501953, 0.45694172382354736, -0.23722383379936218, 0.02625059150159359, -0.9007332921028137, 0.446724534034729, -0.45051610469818115, -0.021431587636470795, -0.3627704679965973, -0.7795173525810242, -0.8934997916221619, -0.17973588407039642, 0.2881914973258972, 0.5582811832427979, 0.6499168872833252, 1.0156052112579346, -0.4776226282119751, 0.269226610660553, 0.4629178047180176, -0.7360278964042664, 0.26837289333343506, -0.14071042835712433, -0.07842782884836197, -0.2145550698041916, 0.2089516520500183, -0.7423335313796997, -0.5631402134895325, 0.9960853457450867, -0.28078046441078186, 0.5003958344459534, 0.6686245203018188, -0.7324073910713196, -0.19228428602218628, 0.9017437100410461, 0.5257704854011536, 0.9931483864784241, -0.3655288815498352, -0.6266703605651855, 1.5974290370941162, -1.733951449394226, -0.007714929990470409, 1.9523991346359253, 0.34282201528549194, -0.06370687484741211, -0.46544018387794495, -0.879218578338623, 0.8405041098594666, -0.016066189855337143, -1.3789845705032349, -0.8516355156898499, -0.369091272354126, 0.18298815190792084, 2.2763092517852783, -0.10252013802528381, -0.3209350109100342, 0.7095438838005066, -0.011018672026693821, 1.3456107378005981, -0.3811073303222656, -0.8715445399284363, -0.2540968358516693, -0.09600107371807098, 0.007043501827865839, 0.8845720887184143, -0.2538725733757019, 0.14558984339237213, -1.0241706371307373, -0.40602582693099976, 0.0027158951852470636, -0.47008469700813293, 0.0472918376326561, 0.131554514169693, 0.03899306431412697, 1.0813158750534058, -0.13639891147613525, 0.353426069021225, -0.11156034469604492, 0.7044380903244019, 0.26603031158447266, 0.24338379502296448, 0.5560006499290466, 0.2724323868751526, -0.44997885823249817, 0.18276764452457428, -0.9629145264625549, 0.34619665145874023, 0.2678069472312927, 0.9225634336471558, -0.7014474272727966, 0.5177878141403198, -0.8067306280136108, 0.4839879870414734, 1.5288485288619995, 0.35834649205207825, 0.45641031861305237, -0.6225822567939758, 0.5490009784698486, -0.9795156121253967, -0.09856849163770676, -0.7062110900878906, -0.24597515165805817, -0.2850203514099121, -0.7094079852104187, -1.4395004510879517, -0.41432711482048035, 0.0012582209892570972, -0.33336907625198364, 0.875068187713623, -0.3681611716747284, 0.07565858960151672, 0.1461172103881836, 0.30831378698349, 0.2193540781736374, 1.0537794828414917, 0.19553285837173462, -0.39692413806915283, 1.0797499418258667, -0.9194095730781555, -0.8755829334259033, -1.341284990310669, 0.7682093977928162, -0.343067467212677, 0.0670834481716156, -0.23657876253128052, -1.2562295198440552, -0.6643320918083191, -0.646078884601593, -0.23151043057441711, -0.23072005808353424, 0.7237926125526428, 1.0118818283081055, 0.2078244686126709, -0.882899820804596, 0.3129749298095703, -0.014842170290648937, 0.0843297690153122, 0.25378134846687317, 0.3201952874660492, 0.41500645875930786, -0.3350386619567871, -1.3039675951004028, 0.3534547984600067, 0.31773510575294495, -0.3149699866771698, -0.01616368629038334, -0.25464898347854614, -1.3894755840301514, 0.16992925107479095, 0.3087162673473358, -0.40150579810142517, 1.266527771949768, -0.1717587411403656, -1.5570399761199951, 0.45457395911216736, -0.6003497242927551, -0.07299857586622238, 0.1434646099805832, -0.21938180923461914, -0.5680017471313477, -0.6989467740058899, -0.3698372542858124, 0.5039604306221008, 0.6994243264198303, 0.12269101291894913, -0.6162177920341492, 0.1551772654056549, -0.466193825006485, 0.07224087417125702, -0.2669428586959839, 1.127765417098999, -0.7993785738945007, -0.4379448890686035, 0.4348563551902771, 0.4539865553379059, -0.28304776549339294, -0.2359284609556198, -0.6161394119262695, -1.2143285274505615, 0.45189663767814636, -0.5579342842102051, 1.362093448638916, -0.6718022227287292, -0.6071673035621643, -0.0983041450381279, -0.1711447685956955, -0.0866963192820549, -0.8148787617683411, 0.4716002345085144, -0.17232903838157654, 0.14680932462215424, -0.23791955411434174, -1.5555354356765747, 0.020071929320693016, -0.2227119356393814, -0.6822589039802551, 0.08460147678852081, -0.01723562739789486, 0.9340237975120544, -0.7681175470352173, -0.0927773043513298, -0.12895669043064117, 0.29605555534362793, -1.1151237487792969, 1.443352222442627, -0.35986873507499695, 0.01180132757872343, -0.0031398064456880093, -0.4039364159107208, 0.2646770477294922, -0.38496294617652893, 0.9740262627601624, -0.19499443471431732, -0.16056403517723083, 0.5363967418670654, -0.5045546293258667, 1.06238853931427, -0.4548927843570709, 0.8020175695419312, -0.24710416793823242, -0.5786568522453308, -0.29297178983688354, 0.37877342104911804, -0.43858563899993896, -0.5263394117355347, 0.42107322812080383, 0.32332634925842285, -0.5329479575157166, 0.33827459812164307, 0.5944640636444092, 1.0326356887817383, -0.33779191970825195, 0.31113970279693604, 0.553713858127594, 0.22468653321266174, 0.4741745591163635, 0.3852591812610626, 0.3649578392505646, 0.3051851689815521, 0.7125879526138306, -0.08016668260097504, 0.5900811553001404, -1.0271514654159546, -0.18890835344791412, 0.5239729285240173, 1.1687010526657104, 0.5202130079269409, 0.2823648452758789, -0.5684309601783752, -0.3067167401313782, 0.24066351354122162, 0.5592226386070251, 1.6664798259735107, -0.3379347622394562, -0.3521032929420471, -0.8492204546928406, -0.22739233076572418, -0.21965397894382477, 0.31286385655403137, -0.1250550001859665, 0.15463507175445557, -0.6091898083686829, -1.2007066011428833, 0.8774659037590027, 0.06359734386205673, 0.39526844024658203, -0.39293888211250305, 0.21104320883750916, -0.18865349888801575, -0.14481957256793976, -1.1527435779571533, -0.6325434446334839, 0.3021487891674042, -0.7450506091117859, -0.034779783338308334, 0.20814286172389984, 0.03066295199096203, 0.481919527053833, -0.8847172260284424, 0.93394535779953, -0.4875465929508209, -0.06186172366142273, -0.36864346265792847, 0.7558006644248962, -0.6441529393196106, -0.9278506636619568, 0.23392891883850098, 0.4381607472896576, -0.06675833463668823, 0.5098866820335388, 0.7471864819526672, 0.6524378657341003, -0.32098686695098877, 0.04740585759282112, 0.35607460141181946, 0.17232801020145416, -0.1632130891084671, 0.7044951915740967, -0.3112466037273407, -0.047851476818323135, -1.178112506866455, 0.9320545792579651, -0.0782686397433281, -1.0492063760757446, 0.2099389135837555, -0.86580890417099, 0.1150941327214241, 0.7459166646003723, -0.7863017916679382, -0.5173571109771729, -0.7820631861686707, -0.04450671374797821, -0.3918766975402832, -0.0969654768705368, 0.1583656221628189, 0.282685250043869, 0.5068392753601074, -0.10850486904382706, 0.6742963790893555, 0.38591068983078003, -0.6189442276954651, 0.42488721013069153, -0.3553663194179535, 0.42676249146461487, 0.4101218581199646, 0.21869505941867828, -0.4973923861980438, -0.344413161277771, -0.6949767470359802, -0.5081871747970581, -0.3946254849433899, -0.12840591371059418, 0.030182698741555214, 0.02253335900604725, -0.7133051753044128, -0.6923418641090393, -0.20998956263065338, -1.4078319072723389, 0.14502660930156708, 0.92281174659729, 0.1594490110874176, 0.10133921355009079, -0.9785512685775757, -1.3484238386154175, -0.807349443435669, -0.8740652203559875, -0.9640868902206421, 0.5511266589164734, -0.2676088809967041, -0.5263717770576477, -0.5894272327423096, -0.1766134798526764, -0.19722311198711395, 0.893330454826355, -1.2918857336044312, 1.2236721515655518, -0.23023073375225067, -0.22059489786624908, -0.2294466644525528, 0.3740788996219635, 0.21126870810985565, -0.4827654957771301, 0.1042875126004219, -1.090172529220581, 0.013746106065809727, -0.5817611217498779, 0.2271256446838379, -0.1619577556848526, 0.6111189126968384, 0.8910378813743591, -0.4730520248413086, -0.7700641751289368, 0.6631402373313904, 1.22283935546875, -0.8973628878593445, -0.29699423909187317, -0.16203390061855316, 1.0071966648101807, 0.0451667346060276, -0.15041744709014893, 0.7030185461044312, 0.33393362164497375, 0.7878556847572327, 0.15846861898899078, 0.04874275252223015, 0.18755251169204712, -0.4626724123954773, 0.8124023675918579, 2.046377420425415, 0.24274392426013947, -0.25818976759910583, -0.6886919736862183, 0.4214191436767578, -1.3168107271194458, -0.5366209149360657, 0.35043972730636597, 0.9013983011245728, 0.40807390213012695, -0.39511585235595703, -0.3504146337509155, -0.4756441116333008, 0.41277429461479187, 0.2631150782108307, -0.21408702433109283, -0.7460957765579224, 0.08358018100261688, 0.026908840984106064, 0.24955862760543823, 0.7754000425338745, -0.4617210924625397, 0.835582435131073, 14.36638069152832, 1.0521138906478882, 0.23090623319149017, 0.9271074533462524, 0.6900405287742615, 0.0854145959019661, -0.2884600758552551, -0.3116626441478729, -1.5638483762741089, 0.12118914723396301, 1.4851778745651245, 0.17315003275871277, 0.912686824798584, 0.22415578365325928, 0.25926825404167175, 0.11401290446519852, -0.5664615631103516, 1.202232837677002, 0.6737469434738159, -1.2088942527770996, 0.582895815372467, 0.14159095287322998, 0.18431542813777924, 0.5138990879058838, 0.29038289189338684, 1.0049655437469482, 0.4904922842979431, -0.5434830784797668, 0.5112298727035522, -0.0021817090455442667, 0.7601077556610107, -0.3935554325580597, 0.4665602743625641, 0.9542661309242249, -1.1636232137680054, -0.300182968378067, -0.7459042072296143, -1.2178807258605957, 0.048150185495615005, 0.12872762978076935, -0.4999491572380066, -0.46590062975883484, -0.247634619474411, 0.5873555541038513, 0.022596776485443115, 0.18371492624282837, -0.07779965549707413, 0.9211471676826477, -0.21970757842063904, 0.042541101574897766, 0.13316310942173004, 0.0015916179399937391, 0.3514207899570465, 0.14836648106575012, 0.16738133132457733, -0.08145146816968918, -0.07399938255548477, 0.47597959637641907, -0.5905965566635132, 0.2096419334411621, -0.2819092571735382, -0.26147347688674927, 0.17695677280426025, 0.6162354350090027, 0.7579469084739685, 0.2083778828382492, -0.5618864893913269, 0.4467647969722748, 1.1389458179473877, 0.4158572256565094, -0.29638513922691345, 0.5175356864929199, 0.6123598217964172, -0.834607720375061, 0.22230468690395355, 0.7521750926971436, 0.06471046060323715, -0.6423606872558594, -0.7171919941902161, -0.578886866569519, 0.5286739468574524, -0.6235593557357788, -0.732659637928009, 0.6458564400672913, -0.10874050855636597, -0.1608334183692932, -0.011715772561728954, -0.6184200048446655, 0.20491662621498108, 0.5789997577667236, -1.136505365371704, -0.4365951120853424, 0.8812610507011414, -0.5077708959579468, 0.24197238683700562, -0.0879867821931839, 1.503724455833435, 0.2830319106578827, -0.7912726998329163, 0.4056296944618225, 0.4568661153316498, -0.009990747086703777, -0.5873458385467529, -0.1722923368215561, 0.9443610906600952, 0.7208195924758911, 0.03646795079112053, 0.4427119791507721, -0.13969998061656952, 0.19681911170482635, -0.9243581295013428, -0.0984177216887474, 1.2892171144485474, -0.8026424050331116, -0.6339328289031982, -1.2074439525604248, -0.6346800327301025, 0.3695102334022522, 0.32441335916519165, -0.3847523629665375, 0.31653475761413574, 0.2651759684085846, -0.4547264873981476, -0.025894975289702415, -0.22305020689964294, 0.19469338655471802, 0.4360605478286743, -0.6178438663482666, 0.1679326593875885, 0.048368580639362335, 0.26181113719940186, -0.8764955401420593, -0.34860774874687195, -0.3710084855556488, -0.04948168247938156, 0.7410587072372437, 1.1002923250198364, -0.6973137259483337, 0.5732682943344116, 0.8048791289329529, -0.43994826078414917, -0.7831647992134094, -0.20751719176769257, -0.8944587707519531, -0.7026629447937012, -0.20520398020744324, 1.060414433479309, -0.24243363738059998, -0.31342753767967224, 0.7819086909294128, 0.6546306610107422, -0.4792359173297882, -0.8093923926353455, -0.4598751664161682, -0.21082931756973267, -0.5690323710441589, 0.11360648274421692, -0.23945756256580353, -0.5337361097335815, 0.46941691637039185, 0.02722761780023575, 1.123220443725586, 0.03782709687948227, -0.4020983576774597, 0.48164984583854675, -0.13973477482795715, -0.14709733426570892, -0.4552050530910492, 0.03352408483624458, -1.7119024991989136, 0.6669394969940186, -1.1836344003677368, 0.2214203178882599, -0.898629367351532, -0.17377731204032898, 0.20591755211353302, 0.05243854969739914, -0.11957751214504242, 0.24543261528015137, -0.2993434965610504, -0.47565606236457825, -0.38980308175086975, -0.5394515991210938, 0.7881618142127991, 0.4367423951625824, -0.5144593119621277, 0.4508715867996216, 0.20309828221797943, 0.5286961197853088, 0.3635287582874298, 0.26096266508102417, -0.8679659366607666, -0.7375349402427673, -1.4760693311691284, 0.44285523891448975, -0.13142509758472443, -0.4294680953025818, -0.16433516144752502, 0.6147067546844482, 0.08990570902824402, -0.15789231657981873, 0.21120548248291016, 0.5636339783668518, -1.0362441539764404, -0.44262027740478516, 0.4157586097717285, -0.7284473776817322, 0.2500852346420288, 0.03908591344952583, -0.3931087553501129, -0.34696105122566223, 0.4124366343021393, 0.0007896448951214552, -0.7778886556625366, -0.6532682180404663, 0.6511275768280029, -0.6555995345115662, 0.33406296372413635, -0.3590768277645111, -0.05159379914402962, -0.5587189197540283, -0.5865628719329834, 0.3336033523082733, 0.07190049439668655, -0.6949096918106079, 1.1089351177215576, 0.36156898736953735, -0.9126721620559692, -0.10504075884819031, 0.565584659576416, -0.12919026613235474, -0.09552553296089172, 0.08148633688688278, 0.44484326243400574, -0.13355125486850739, 0.9025040864944458, 0.7748748064041138, 0.4576132595539093, -1.3551301956176758, -0.13709791004657745, 0.7935954928398132, -0.5769590735435486, -0.26251929998397827, 1.0573070049285889, -0.12518849968910217, -1.1711640357971191, 0.32695114612579346, -0.937873899936676, -0.33635637164115906, -0.4751960337162018, 0.46928495168685913, 0.31767743825912476, 0.08885368704795837, -0.175360769033432, -0.6607027649879456, -0.11999242007732391, 0.13198770582675934, -0.46491146087646484, 0.6231947541236877, -0.6280523538589478, -0.45855024456977844, 0.5356168746948242, 1.2228456735610962, -0.48376360535621643, -0.788283109664917, -0.6302887201309204, -0.2932896614074707, -0.21291610598564148, 0.597639799118042, -0.168151393532753, -0.6459886431694031, 0.5744236707687378, 0.15998423099517822, -0.06665020436048508, -0.013777900487184525, -0.17518098652362823, 0.21508784592151642, 0.9272788763046265, 0.04732661694288254, -1.0676268339157104, -1.1449027061462402, 1.500260829925537, 1.184502363204956, -1.2231266498565674, 0.2740253210067749, -0.29795217514038086, -0.7887052893638611, 0.6405253410339355, 0.16368265450000763, 0.23200799524784088, 0.9113186597824097, -0.07709945738315582, 0.053922224789857864, 0.28127938508987427, -1.216850996017456, -0.22504238784313202, 1.169974684715271, 0.4555896818637848, 0.8442988395690918, 0.24903225898742676, 0.08055275678634644, 0.930475115776062, 0.06905213743448257, -0.11310028284788132, 0.09109625965356827, 0.17108236253261566, -0.10912726819515228, 0.12298571318387985, -0.029411910101771355, 0.8612591028213501, -0.9318075180053711, -1.216442584991455, 0.38691389560699463, 0.5377708077430725, -0.053795430809259415, 0.7257648706436157, 0.9830027222633362, 0.5863325595855713, 0.07816419750452042, 0.2638314366340637, 0.48854565620422363, -0.5010214447975159, 0.13889378309249878, -0.16209252178668976, -0.30865010619163513, -0.1415964663028717, 0.11085546761751175, -0.3870493769645691, -0.2606196999549866, -0.2646452486515045, 0.15499715507030487, -0.3362308442592621, 0.6331177949905396, 1.4311717748641968, 0.500103771686554, 0.2789663076400757, -0.21693146228790283, -0.10712054371833801, -0.5025750994682312, -1.1669368743896484, -0.19092495739459991, -0.6619605422019958, -0.28321152925491333, 0.1109720915555954, -0.31996187567710876, -0.15230728685855865]}, "authors": [{"authorId": "2112253370", "name": "Zixuan Zhou"}, {"authorId": "6636914", "name": "Xuefei Ning"}, {"authorId": "2241616962", "name": "Ke Hong"}, {"authorId": "48737592", "name": "Tianyu Fu"}, {"authorId": "2264991200", "name": "Jiaming Xu"}, {"authorId": "2242132627", "name": "Shiyao Li"}, {"authorId": "2297766989", "name": "Yuming Lou"}, {"authorId": "2289835082", "name": "Luning Wang"}, {"authorId": "2297998322", "name": "Zhihang Yuan"}, {"authorId": "2264969921", "name": "Xiuhong Li"}, {"authorId": "2283520504", "name": "Shengen Yan"}, {"authorId": "144290348", "name": "Guohao Dai"}, {"authorId": "2297826464", "name": "Xiao-Ping Zhang"}, {"authorId": "2287809786", "name": "Yuhan Dong"}, {"authorId": "2283814845", "name": "Yu Wang"}], "references": [{"paperId": "af35e5cd1b22ae7d3c5f8a95c3d5ebc308fabe72", "title": "Llumnix: Dynamic Scheduling for Large Language Model Serving"}, {"paperId": "a7919a3c6dbdcc524776a3102110d637836ad2e0", "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models"}, {"paperId": "a7ed153f848cd8a299c866f50d74b55f16d606ce", "title": "SpinQuant: LLM quantization with learned rotations"}, {"paperId": "48c1bf8bab85f4d6a2490a4d3efc8b1fb4a2b261", "title": "Is Flash Attention Stable?"}, {"paperId": "d50c7d1287d93539f5c4cc312ae9cfaaaf570fe6", "title": "Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting"}, {"paperId": "59b9d7d8f5bab537b7f9e48307d5436ba4922725", "title": "Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models"}, {"paperId": "eb06e95dd3eb5a916e52d2e463f474ef4967d8ca", "title": "LoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism"}, {"paperId": "394ff691966b419e0a6f24c661aa3c7d45b66bd2", "title": "SEER-MoE: Sparse Expert Efficiency through Regularization for Mixture-of-Experts"}, {"paperId": "dd85e6cab147d237a0b1ab6f674570d3efb4d4a0", "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"}, {"paperId": "114c1120a936dbe4de24bc121997fea150bf4331", "title": "AffineQuant: Affine Transformation Quantization for Large Language Models"}, {"paperId": "2be7acee3dfd9c4eb99755158d8c770ac2a36715", "title": "FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines"}, {"paperId": "d2421cffac277e230cb97fc2355b32e03dd8bb1f", "title": "ExeGPT: Constraint-Aware Resource Scheduling for LLM Inference"}, {"paperId": "4706711dc4e1e16424db9f454a1f3b092b972785", "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression"}, {"paperId": "20f090e35ad598fba2404e550c2462dc9da03a10", "title": "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve"}, {"paperId": "8b8ac20444ef31690f1ad1d80181d121fbde5bab", "title": "Evaluating Quantized Large Language Models"}, {"paperId": "02ac355296f001a010b1db115d909c052767ccb3", "title": "Stable LM 2 1.6B Technical Report"}, {"paperId": "7351898febca53d01453283c9b1a541b662e1ed3", "title": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models"}, {"paperId": "dbf829c977c121c3704d070d7800d29fe5914756", "title": "LLM Inference Unveiled: Survey and Roofline Model Insights"}, {"paperId": "2cea424c7dce71042c24d43317521abdc4c0ffb4", "title": "Large Multimodal Agents: A Survey"}, {"paperId": "f7310dac21abc6ba357bcd5e75fb2e6957a97303", "title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases"}, {"paperId": "b42e5a92890053ef48f794311c28c45e9fe55ddd", "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models"}, {"paperId": "12a717fa6ac55d742c200e2ddb9acc162632a22e", "title": "Adaptive Skeleton Graph Decoding"}, {"paperId": "50caaa898eb3605a58a550c7aebacdc1cff3b6da", "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation"}, {"paperId": "2fe05b1f953da5dcf6ec5fe7bc72bfb3dbd9ea30", "title": "Model Compression and Efficient Inference for Large Language Models: A Survey"}, {"paperId": "9da427202cc48370fd66359f5d72ff5ff3bc8b57", "title": "Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks"}, {"paperId": "d3a23b571143a45c5d7406fbc395750ac5cfef95", "title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"}, {"paperId": "8fbf2eb4587b5c271979c3f96eee1b109496143e", "title": "QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks"}, {"paperId": "d0ac639d6ed814eac74b6c39eb5ad46854d8fcc4", "title": "Rethinking Optimization and Architecture for Tiny Language Models"}, {"paperId": "a3e000e0d7f64c1d094c2a8bf6f43992cbabe91b", "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"}, {"paperId": "a74a20be53e5767648b5970e30b2d81a9ba8293f", "title": "A Survey on Transformer Compression"}, {"paperId": "f1a9e0830bc36c048fa4659beaa62609869895b5", "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding"}, {"paperId": "b085968c4362fb286ad6c5ef71a5db9630da0498", "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization"}, {"paperId": "a7e7fa7ca4a32a92f578adf07c613088fa89f5b0", "title": "A Comprehensive Survey of Compression Algorithms for Language Models"}, {"paperId": "7754ac3e8ff1286f17593159781487543cdddaba", "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns"}, {"paperId": "8f070e301979732e0dd73f6aa6170309cf73aa7d", "title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges"}, {"paperId": "21e53e51ff77a5f34f43cb8ca029909c3ad9f71e", "title": "Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads"}, {"paperId": "57e7af0b69325fafb371ef5d502e39ef9c90ef7e", "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"}, {"paperId": "72f77a393079431e4207b3afe678ee80b420e6f8", "title": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"}, {"paperId": "38c48a1cd296d16dc9c56717495d6e44cc354444", "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model"}, {"paperId": "cfabacfc676ea8804b5acbab169f2df5e5866d4d", "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models"}, {"paperId": "0cee098244c9978032702862a43a09f468f691a4", "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding"}, {"paperId": "8a824052a8ae2061d67f1b76f3610c20ca301f7f", "title": "APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding"}, {"paperId": "06d860a5bbb99a4eafdbbb2d5f6aa8dd5fd32cf4", "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security"}, {"paperId": "3eec0c1a7dc0d364d23e2e4544bf8772f5f8ffa3", "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference"}, {"paperId": "411114f989a3d1083d90afd265103132fee94ebe", "title": "Mixtral of Experts"}, {"paperId": "3695739b3a8b0e92b8ae90081124d098ae33b15c", "title": "FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs"}, {"paperId": "745594bd0dc3e9dc86f74e100cd2c98ed36256c0", "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts"}, {"paperId": "ee802ccb7fc3a322b824310ae6f29fc6a1e4314b", "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"}, {"paperId": "560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd", "title": "TinyLlama: An Open-Source Small Language Model"}, {"paperId": "fbce7dee52c869edb99408f4a454bdc8703930f5", "title": "Fairness in Serving Large Language Models"}, {"paperId": "47beae741f6a4470dbd286d4f3f05ba2031c52d2", "title": "Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference"}, {"paperId": "13261129251c9e8891cff02c3aee15c4df6a5630", "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"}, {"paperId": "d7eb6ff1007fbdad402719eba8494283ec575016", "title": "DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization"}, {"paperId": "e30666ed82670463aa47686e744f0c6f2a0e083d", "title": "Cascade Speculative Drafting for Even Faster LLM Inference"}, {"paperId": "ddacee7382548fd9976e846c92500cfa3b6741db", "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU"}, {"paperId": "a3dd3adc37cdb8991936f3feb109c20b6f892f3d", "title": "Extending Context Window of Large Language Models via Semantic Compression"}, {"paperId": "eb95c327260725498404eb43ec370d419b8d92c7", "title": "SGLang: Efficient Execution of Structured Language Model Programs"}, {"paperId": "a1bcf68d6ed2fec1ecaf16b67f2d19bc20c00ee6", "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models"}, {"paperId": "5851121df5ce46be5faea265c868ec0beabfce96", "title": "Efficient Large Language Models: A Survey"}, {"paperId": "383c598625110e0a4c60da4db10a838ef822fbcf", "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "51fb6598a3ebe36b371b096b4824d718e6e527fb", "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey"}, {"paperId": "ad9146d98ae95bbeeef460abe083ecc2c4798672", "title": "Splitwise: Efficient generative LLM inference using phase splitting"}, {"paperId": "2c0312c604f9f7638bb4533b39e0ae81e7f6ab12", "title": "The Falcon Series of Open Language Models"}, {"paperId": "929ed6412136fe42e6ef1eeb7ea0b4da693dee37", "title": "SpotServe: Serving Generative Large Language Models on Preemptible Instances"}, {"paperId": "4067a6f57f708dec4459d3d4322373e06c2b168c", "title": "PaSS: Parallel Speculative Sampling"}, {"paperId": "532c2c7a247d9e97d20abec1b2f4612984fdab93", "title": "REST: Retrieval-Based Speculative Decoding"}, {"paperId": "a6348981246adcd42e8ba39acf139da745696eff", "title": "Towards the Law of Capacity Gap in Distilling Language Models"}, {"paperId": "4d76206515d6b33903937474273885476fc2771e", "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs"}, {"paperId": "9529e50807f36acf3d2e4af994b5803c47e4746a", "title": "Atom: Low-bit Quantization for Efficient and Accurate LLM Serving"}, {"paperId": "a8b995f0da78a79447dfb18c2337972b044f4239", "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers"}, {"paperId": "ca53c1d1ba1a1386f860fa13d7729160571e1643", "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery"}, {"paperId": "ef9079f32e806e4d297cee28f36b2321acee9eb3", "title": "MCC-KD: Multi-CoT Consistent Knowledge Distillation"}, {"paperId": "ea1f648988c632a6dbab6d8b88432456aa021cfb", "title": "SpecTr: Fast Speculative Decoding via Optimal Transport"}, {"paperId": "43017a16dfe593c09533c5fb3c3612c83761a98a", "title": "Matrix Compression via Randomized Low Rank and Low Precision Factorization"}, {"paperId": "ddbd8fe782ac98e9c64dd98710687a962195dd9b", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"}, {"paperId": "51bccfc8d164812cf81b0284a5ec13bf4002ae3c", "title": "One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models"}, {"paperId": "4880ba8910bc320cb7c1aa943992a500f4c41f07", "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs"}, {"paperId": "56767c18bb5aaa2b6377624168bed1b6dcc4b94d", "title": "DistillSpec: Improving Speculative Decoding via Knowledge Distillation"}, {"paperId": "e764ad9ccf0b31a0c91a9220290930f083ad062a", "title": "Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability"}, {"paperId": "af8123ecdff838f63e4eba0b36b8babe4c5cee65", "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"}, {"paperId": "ffdc017b1d2b493feaac9efa854882fe23d50dcf", "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models"}, {"paperId": "ba5261e729c181e28a98dee2c08d7cf5fc7127a2", "title": "Online Speculative Decoding"}, {"paperId": "4c0428917aeee6aa7bd434f337d039f35996b736", "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"}, {"paperId": "abdb0f9d1486dbb024c4bc9f8f9dc40464c58715", "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning"}, {"paperId": "2392b6d3a5cad9e5cf349169eaeee848266adf6a", "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"}, {"paperId": "b12541867632737e826b7b01c7fbe1c4222d8655", "title": "Compressing Context to Enhance Inference Efficiency of Large Language Models"}, {"paperId": "23af54b82c951317f1fc1841164d8a441a2d8120", "title": "RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation"}, {"paperId": "4e13ecf80443a4135d516b7ba77eca82b5c6d347", "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple"}, {"paperId": "bfeda6c7aa7899a80adb01894555b09d24756a59", "title": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "945db0077b6d19b720f5998b3f61300013c4f885", "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"}, {"paperId": "8ec117feff6ee10e3b20a19ac101fee5c99e14d7", "title": "LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression"}, {"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "8df524e0c50903d0b2c4be338081906d13ea42af", "title": "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding"}, {"paperId": "0c72450890a54b68d63baa99376131fda8f06cf9", "title": "The Rise and Potential of Large Language Model Based Agents: A Survey"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "e26888285436bc7998e5c95102a9beb60144be5e", "title": "Textbooks Are All You Need II: phi-1.5 technical report"}, {"paperId": "464cf829eaaeb2b3bafc84cc9203790e95102049", "title": "Norm Tweaking: High-performance Low-bit Quantization of Large Language Models"}, {"paperId": "d315ca681e95b73f2a6a6115d1e218dec9720d6f", "title": "QuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive Algorithm"}, {"paperId": "a9caf21a845cb0b1b1d453c052188de118006093", "title": "SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills"}, {"paperId": "eb2c2330177f765038a2b17e2ee3498965865797", "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"}, {"paperId": "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3", "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models"}, {"paperId": "5df422fc18974d687febd171adcac35b3012c50a", "title": "Discrete Prompt Compression With Reinforcement Learning"}, {"paperId": "7ac38c3398f2696754bec69f296468e7a8237a64", "title": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"}, {"paperId": "338d8f3b199abcebc85f34016b0162ab3a9d5310", "title": "A Survey on Model Compression for Large Language Models"}, {"paperId": "43e624ddeed82df944a6cae0dedec3372438e243", "title": "Accelerating LLM Inference with Staged Speculative Decoding"}, {"paperId": "1dede9d21db0be1c58208e1f970e57aac4fc45f8", "title": "Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty"}, {"paperId": "56b828717f32251a5e0f0be9c0113077f23c8429", "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees"}, {"paperId": "aeb9454987c3f85563cf7a5d2cb7f3d502d3398d", "title": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "60b0476a97c00e355df28ba35422764a7fbe88e8", "title": "In-context Autoencoder for Context Compression in a Large Language Model"}, {"paperId": "e28f4687b9ddf562807d12d9799add07aa191d51", "title": "Efficient Transformer Inference with Statically Structured Sparse Attention"}, {"paperId": "ce9435c82dc9b576f2037aa2f4357a520be9b2aa", "title": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference"}, {"paperId": "af67be0fff8d087a0d8554b6e8998ab12409bbda", "title": "TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"}, {"paperId": "2fd0aa038cf1009e265e9cbddab8ea6a8e03016a", "title": "FLuRKA: Fast and accurate unified Low-Rank&Kernel Attention"}, {"paperId": "7a6a298efb965ce9a351a3212f6f536e94dbbb03", "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \u201cThink\u201d Step-by-Step"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "bc8428e270a5474cabfaff578d44955f757ccacd", "title": "LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation"}, {"paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6", "title": "A Simple and Effective Pruning Approach for Large Language Models"}, {"paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b", "title": "Textbooks Are All You Need"}, {"paperId": "d2d0371158803df93a249c9f7237ffd79b875816", "title": "Sparse Modular Activation for Efficient Sequence Modeling"}, {"paperId": "3f5e63168d0ae1af41c3434e9e3e7e84dda9a5d8", "title": "FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction"}, {"paperId": "a5d3a865b71f3f424ba31e037848028f60161478", "title": "Propagating Knowledge Updates to LMs Through Distillation"}, {"paperId": "0a067fab18c67d4a386efa846c080f8afff5e8f3", "title": "Block-State Transformers"}, {"paperId": "f5359f596e0306599b4aa4157e6fe03567b35c01", "title": "Knowledge Distillation of Large Language Models"}, {"paperId": "3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc", "title": "SqueezeLLM: Dense-and-Sparse Quantization"}, {"paperId": "0423fc7bc1880b850d07aec8ebd9217a70626572", "title": "S3: Increasing GPU Utilization during Generative Inference for Higher Throughput"}, {"paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df", "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "8c5a7de7452b61cb81d6f7124ad021997e0a79c1", "title": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning"}, {"paperId": "d203c764fb5dec2b053be667c8b06e516ea6ef10", "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention"}, {"paperId": "5fc366d7301f147883ee985cff008839abe19cc7", "title": "LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"}, {"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, {"paperId": "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf", "title": "Adapting Language Models to Compress Contexts"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "02529b2666a536053a2e2940de5b28de36fd594b", "title": "Lion: Adversarial Distillation of Proprietary Large Language Models"}, {"paperId": "b2ec81b572fd5f0a5f5de843e3c62985b7d9c5a1", "title": "Lifting the Curse of Capacity Gap in Distilling Language Models"}, {"paperId": "017010b941d902a467f6d329ae5e74fd67e67912", "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"}, {"paperId": "8ce6ad6d8a73757309d3b9f525cf15cb68e32397", "title": "Efficient Prompting via Dynamic In-Context Learning"}, {"paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"}, {"paperId": "1fbaf2d8b69ef6e42a38c233f5d01bea70bad5b7", "title": "Fast Distributed Inference Serving for Large Language Models"}, {"paperId": "585f8b9725f5f5e5495c3508d39f70d1c053e190", "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance"}, {"paperId": "aad167be3c902388ea625da4117fcae4325b8b7d", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"}, {"paperId": "56fa65d8dc41708082f9b2ef7752c49cee9ebe01", "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation"}, {"paperId": "389ec3e8902a5dcfcde1adec735854e93f845937", "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"}, {"paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "e92a5332390f0ba94615935541da4da9bed56512", "title": "OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization"}, {"paperId": "2a44c6b7f291f625314a82ba3131e605009fd533", "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "574beee702be3856d60aa482ec725168fe64fc99", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"}, {"paperId": "8f48c75e1354c88a84a67abb60789083c12e5037", "title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "1462a0e5b7db47301bb0995db56426e1f4a0ac7d", "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "2b66cc9e3b46cf2cd30c4ffdca596480c8de6331", "title": "ZipLM: Inference-Aware Structured Pruning of Language Models"}, {"paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c", "title": "Accelerating Large Language Model Decoding with Speculative Sampling"}, {"paperId": "07b14c24833400b79978b0a5f084803337e30a15", "title": "REPLUG: Retrieval-Augmented Black-Box Language Models"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "d0a970c06f6405610c607a16e59ea832770ad1bc", "title": "DISCO: Distilling Counterfactuals with Large Language Models"}, {"paperId": "a128b1c47e6842605fb95bceae930d2135fc38fc", "title": "Pretraining Without Attention"}, {"paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44", "title": "Large Language Models Are Reasoning Teachers"}, {"paperId": "f9ad1fffa1cc76fd5db3ff758c0839492c5147c4", "title": "In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models"}, {"paperId": "126a4776ff8315fd506766cb8f3c722cf746ad9e", "title": "Teaching Small Language Models to Reason"}, {"paperId": "397e0e0d20f00d8fbfecd2fd36b14f13e2181d0e", "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"}, {"paperId": "8fd462f6248d5e3f1b6602697c09489086b5655f", "title": "Distilling Reasoning Capabilities into Smaller Language Models"}, {"paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644", "title": "Fast Inference from Transformers via Speculative Decoding"}, {"paperId": "43014fc85c4860487336579ec98f509fec1803f7", "title": "MegaBlocks: Efficient Sparse Training with Mixture-of-Experts"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "b0c5c673c690c644a7d4af73adb783bd98486181", "title": "Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences"}, {"paperId": "240300b1da360f22bf0b82c6817eacebba6deed4", "title": "What Makes Convolutional Models Great on Long Sequence Modeling?"}, {"paperId": "22b58dce1a13382418b8372bbd50ed3b2533f899", "title": "ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs"}, {"paperId": "4afda39036206dcb3f97829dccb897f1fc80f459", "title": "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models"}, {"paperId": "33be243ac9dd8723e6267dea45fd6a6172d4f6a5", "title": "Less is More: Task-aware Layer-wise Distillation for Language Model Compression"}, {"paperId": "b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc", "title": "Liquid Structural State-Space Models"}, {"paperId": "13270b9759cf0296b5a346fbb58b706e8ad0a982", "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design"}, {"paperId": "30a7390ec0103684eba9fb6bde1983d706fb57b3", "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"}, {"paperId": "86891d00499eebe86d3f1e39143d412addf2652b", "title": "DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "6d7d141c75af752ffc0d8a6184cca3f9323d6c74", "title": "Simplified State Space Layers for Sequence Modeling"}, {"paperId": "2ef60a4ea4ea53056be811ff55679eb59fb4b586", "title": "Confident Adaptive Language Modeling"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "76d40153acfbb35a7eb8272a4215854cafa10e78", "title": "PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "5eeb828685e44ca5b8ebafb34a9fa4d51c9186df", "title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models"}, {"paperId": "2e700ff36108119f5ed19a53bd2eaa22b42ec3d8", "title": "Tutel: Adaptive Mixture-of-Experts at Scale"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "c9550f0d1940ee1adf1549c9a0d699ef896dbefd", "title": "StableMoE: Stable Routing Strategy for Mixture of Experts"}, {"paperId": "9e82736043eebe3f71eb86cbef6e2ac45306ece5", "title": "Structured Pruning Learns Compact and Accurate Models"}, {"paperId": "fb145e1e49d3269d8223c7710e22b45438613ff0", "title": "A Fast Post-Training Pruning Framework for Transformers"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "436278ce3b85265dc5cface29e63c714fe979d23", "title": "LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models"}, {"paperId": "d05141dc0900140f7146bb71e1f7402cf896ea87", "title": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation"}, {"paperId": "f677ef460670e63a0e9a0bd048cd881b4b55d92f", "title": "Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "bbc57e1b3cf90e09b64377f13de455793bc81ad5", "title": "Mixture-of-Experts with Expert Choice Routing"}, {"paperId": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3", "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models"}, {"paperId": "802a5d24c78f713e282b003d99b4afd924bd7568", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "217913c84a4bdbe5cee3630d70480fda8d44bfb0", "title": "Magic Pyramid: Accelerating Inference with Early Exiting and Token Pruning"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "561f9f5abb2c0960a886ab6221c821295f0461a1", "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "03662672662f49e6b06148e94b407b60b0bb72f3", "title": "A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "dd0a27aa2285bc64798fa76944400ab6d9ce3025", "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a", "title": "CKConv: Continuous Kernel Convolution For Sequential Data"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "7c6c31412c5dad22543bb71e31620e8868d644a3", "title": "FTRANS: energy-efficient acceleration of transformers using FPGA"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8", "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "90a1491ac32e732c93773354e4e665794ed4d490", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "5d34881ff68bd203ff790187e7e5c9e034389cfa", "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time"}, {"paperId": "1c332cfa211400fc6f56983fb01a6692046116dd", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "49e5b09480189fc9b2316a54f9d1e55cf0097c8b", "title": "Lightweight and Efficient End-To-End Speech Recognition Using Low-Rank Transformer"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "5e04881e91bff952d102d967c4ffb498ec30d4af", "title": "Blockwise Parallel Decoding for Deep Autoregressive Models"}, {"paperId": "6dbb9e4b2e3b67dc4e1634989511f67d41373dd0", "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "67d968c7450878190e45ac7886746de867bf673d", "title": "Neural Architecture Search with Reinforcement Learning"}, {"paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, {"paperId": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "title": "Optimal Brain Surgeon and general network pruning"}, {"paperId": "e0c7559b44997364540a26fb46a92edff56348ad", "title": "Auction algorithms for network flow problems: A tutorial introduction"}, {"paperId": null, "title": "ModelTC, \u201cLightllm,\u201d"}, {"paperId": null, "title": "\u201cflashinfer,\u201d"}, {"paperId": null, "title": "\u201cJamba: Ai21\u2019s groundbreaking ssm-transformer model,\u201d"}, {"paperId": "d1a6b3a5efde3783b53f822dc8dd00aaac934b95", "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification"}, {"paperId": "81051b830a4f5606106765902a51ba281c9230f9", "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"}, {"paperId": null, "title": "\u201cEagle: Lossless acceleration of llm decoding by feature extrapolation,\u201d"}, {"paperId": "39f0d1b894130852ee9f39a5df58905a09645c81", "title": "Multistage Collaborative Knowledge Distillation from Large Language Models"}, {"paperId": "182c6d1d30859f227dca3606c743e178e8ae6780", "title": "Structural Pruning of Large Language Models via Neural Architecture Search"}, {"paperId": "72c03b873e8c5cd86b15bf73186df341da4731c9", "title": "Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models"}, {"paperId": "51cda783aa6a97e0b3b5915a2bb5a35f31f3c083", "title": "GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models"}, {"paperId": "0088b2b6f7983a9ac1b53e34a307d68a3383f42c", "title": "Structured Pruning for Efficient Generative Pre-trained Language Models"}, {"paperId": "3d473cbb7a377cf960abff31748a1a39bb6c7d7c", "title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"}, {"paperId": "343d24c4dcfaff2132373d218561a23fbd53e934", "title": "OWQ: Lessons learned from activation outliers for weight quantization in large language models"}, {"paperId": "4b56eef2862f7f553686f1dd190c56017122a6a0", "title": "PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels"}, {"paperId": null, "title": "\u201cOpenllama: An open reproduction of llama,\u201d"}, {"paperId": null, "title": "\u201cPaving the way to efficient architectures: Stripedhyena-7b, open source models offering a glimpse into a world beyond transformers,\u201d"}, {"paperId": "9d7a75601e0e50dd68d40cfb8ef0e891dad797a6", "title": "Orca: A Distributed Serving System for Transformer-Based Generative Models"}, {"paperId": "4a984ec8286b19bb0c033e6e4df198a0421b0c17", "title": "Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting"}, {"paperId": "a58ddffb0424021dbc450f3ccdbe3beccb180a05", "title": "Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models"}, {"paperId": "b515de6c2b4a5f08b2f3169f1f1322678deb3257", "title": "publicly available"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "0639baa6dfb35d962e46eb0c38763d769ffb0946", "title": "Models"}, {"paperId": null, "title": "\u201cDy-namic neural networks: A survey,\u201d"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "\u201cSet transformer: A framework for attention-based permutation-invariant neural networks,\u201d"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607", "title": "A Survey on Context Learning"}, {"paperId": null, "title": "\u201cFastertransformer: About transformer related optimization, including bert, gpt,\u201d"}, {"paperId": null, "title": "\u201ccublas: Basic linear algebra on nvidia gpus,\u201d"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "\u201cVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,\u201d"}, {"paperId": null, "title": "\u201cLlm-mq: Mixed-precision quantization for efficient llm deployment,\u201d"}, {"paperId": null, "title": "\u201cBoosting llm reasoning: Push the limits of few-shot learning with reinforced in-context pruning,\u201d"}, {"paperId": null, "title": "\u201cSharegpt,\u201d"}, {"paperId": null, "title": "\u201cMLC-LLM,\u201d"}, {"paperId": null, "title": "\u201cFastgemv: High-speed gemv kernels,\u201d"}, {"paperId": null, "title": "\u201cLmdeploy,\u201d"}, {"paperId": null, "title": "\u201cTransformers: State-of-the-art machine learning for pytorch, tensorflow, and jax.\u201d"}, {"paperId": null, "title": "\u201cPad: Program-aided distillation specializes large models in reasoning,\u201d"}, {"paperId": null, "title": "\u201cOpenppl: A high-performance deep learning inference platform,\u201d"}, {"paperId": null, "title": "\u201cHow long can context length of open-source llms truly promise?\u201d"}, {"paperId": null, "title": "\u201cInference of meta\u2019s llama model (and others) in pure c/c++,\u201d"}, {"paperId": null, "title": "\u201cOptimizing inference on large language models with nvidia tensorrt-llm, now"}]}