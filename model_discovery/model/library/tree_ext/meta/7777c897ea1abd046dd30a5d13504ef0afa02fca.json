{"paperId": "7777c897ea1abd046dd30a5d13504ef0afa02fca", "title": "SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models", "abstract": "Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention."}, "embedding": {"model": "specter_v2", "vector": [0.5870561599731445, 0.5075096487998962, -0.44203242659568787, -0.037355609238147736, -0.7230289578437805, -0.6259110569953918, 0.5948319435119629, -0.20639590919017792, -0.28375715017318726, -0.18627454340457916, 0.8214730024337769, -0.15011823177337646, 0.5011483430862427, 0.2797938585281372, 0.06649130582809448, -0.0743929073214531, -0.7699782252311707, 0.3776136338710785, 0.23596327006816864, -0.3877270817756653, 0.13141736388206482, -0.9611687064170837, -0.6041918396949768, 0.4895612597465515, 0.6850152015686035, 0.14474119246006012, 0.5762037634849548, 0.9013383984565735, -0.5051935315132141, 0.2215813547372818, 0.258309543132782, -0.43337777256965637, -0.014209559187293053, -0.21613729000091553, -0.6709508895874023, -0.21758808195590973, 0.30820366740226746, 0.07082722336053848, -0.18982264399528503, 0.5592221617698669, -0.07454519718885422, 0.24448736011981964, 0.019444385543465614, -0.6828731298446655, -0.3117391765117645, 1.2758920192718506, 0.5386654138565063, 0.8713250756263733, -0.275040864944458, -0.635106086730957, 1.411917805671692, -1.3524274826049805, 0.35597753524780273, 1.428892970085144, 0.5493815541267395, 0.3663395047187805, -0.22196660935878754, -0.6034520864486694, 0.6597309708595276, 0.32054632902145386, -0.6592317223548889, -0.48508185148239136, -0.00046681187814101577, 0.06166699156165123, 1.7536019086837769, -0.24869787693023682, 0.41047653555870056, 0.7896342277526855, -0.13082969188690186, 1.3036588430404663, -0.3252580761909485, -0.8263880610466003, -0.20154762268066406, -0.1632544845342636, 0.07038282603025436, 0.5741488337516785, -0.43556246161460876, -0.056558649986982346, -0.8271512389183044, -0.238290935754776, 0.31481558084487915, 0.005692227277904749, 0.20262691378593445, -0.13298901915550232, -0.41508808732032776, 0.6857439875602722, 0.2466772198677063, 1.2669709920883179, 0.19592060148715973, 0.8302693963050842, 0.692623496055603, 0.16280099749565125, 0.2852107584476471, 0.2597285509109497, -0.22958019375801086, -0.0659012645483017, -1.1144407987594604, 0.5269759893417358, -0.27783289551734924, 1.0487675666809082, -0.0916704535484314, 0.1703520268201828, -0.994092583656311, 0.22487494349479675, 1.0551939010620117, 0.007463964633643627, 0.536297082901001, -1.0778371095657349, 0.36109432578086853, -0.5798411965370178, 0.06908237189054489, -0.401790052652359, -0.18922393023967743, -0.03547825664281845, -0.5816660523414612, -1.5798910856246948, -0.17174603044986725, -0.008368950337171555, -0.5224841833114624, 0.69933021068573, -0.2374049425125122, 0.3875673711299896, -0.20614248514175415, 0.3703668713569641, 0.3502298593521118, 0.49040257930755615, 0.15778158605098724, -0.006908480543643236, 0.9183588027954102, -0.9949349164962769, -0.6476921439170837, -1.3633733987808228, 0.7666940689086914, -0.3743765354156494, 0.5133801102638245, -0.2617941200733185, -1.4675406217575073, -0.5932572484016418, -0.4908197224140167, -0.17516939342021942, -0.5921723246574402, -0.002941397251561284, 0.7687489986419678, 0.25984814763069153, -1.1759238243103027, 0.48786553740501404, -0.22796575725078583, -0.4468255639076233, 0.5389124751091003, -0.42764654755592346, 0.4056762158870697, -0.4044570326805115, -1.4819793701171875, 0.2506989538669586, -0.11337299644947052, -0.42392680048942566, -0.14250536262989044, -0.5673688650131226, -0.9802247881889343, -0.018522998318076134, 0.736758828163147, -0.021801721304655075, 1.555877685546875, -0.22036436200141907, -1.0211073160171509, 0.5727782845497131, -0.6336808800697327, 0.03940635547041893, 0.3089837431907654, -0.22249628603458405, -0.30753451585769653, -0.38170570135116577, -0.010510087013244629, 0.306672066450119, 0.634110152721405, 0.06910516321659088, -0.21057066321372986, 0.04054482281208038, -0.27513033151626587, -0.07204610109329224, 0.03676771745085716, 0.9193613529205322, -0.5030889511108398, -0.586973249912262, -0.062485210597515106, 0.5460668206214905, -0.14124983549118042, -0.22796960175037384, -0.19256065785884857, -1.0624979734420776, 1.12228262424469, 0.11220070719718933, 1.1145938634872437, -0.6621772646903992, -0.7774539589881897, -0.4250905513763428, -0.09051165729761124, -0.1039598286151886, -0.7381187677383423, 0.7437116503715515, -0.16106373071670532, 0.28440308570861816, -0.3759075403213501, -0.964570164680481, 0.08809993416070938, -0.07412170618772507, -0.6463825106620789, -0.23729351162910461, 0.09279357641935349, 1.197984218597412, -1.0989867448806763, -0.2952684164047241, -0.045758623629808426, 0.2585356831550598, -1.44590425491333, 1.0176552534103394, -0.7102149724960327, 0.31839191913604736, -0.008266530930995941, -0.18832513689994812, -0.034196920692920685, -0.4353015422821045, 0.633085310459137, -0.08342662453651428, -0.18058599531650543, 0.3714904487133026, -0.04373273625969887, 1.6664026975631714, -0.3967856168746948, 0.5723766684532166, 0.05107530951499939, -0.683441162109375, -0.2164924293756485, 0.5021711587905884, -0.0666952058672905, -0.32527095079421997, 0.1825571209192276, 0.029286203905940056, -0.6507049798965454, 0.0032857642509043217, 0.9916207790374756, 0.7626847624778748, -0.5080734491348267, 0.07410319894552231, 0.6356837749481201, -0.2430005669593811, 0.6047995090484619, 0.812665581703186, 0.5506111979484558, 0.5953229069709778, 0.2430477738380432, -0.21501685678958893, 0.248138427734375, -0.5416265726089478, -0.04103756323456764, 0.35339540243148804, 0.3648729920387268, 0.7094370722770691, 0.6218658685684204, -0.3448497951030731, -0.4631355106830597, 0.48775774240493774, 0.7731034755706787, 1.983856201171875, -0.10057583451271057, -0.4859672784805298, -1.185721516609192, -0.3329925537109375, -0.4038058817386627, 0.038146376609802246, -0.5434813499450684, 0.02323945425450802, -0.8157562613487244, -0.6477040648460388, 0.9920317530632019, 0.24879318475723267, 0.6343461275100708, -0.6540490388870239, -0.4302428960800171, -0.029120350256562233, -0.38078591227531433, -1.0896849632263184, -1.0253993272781372, 0.18419009447097778, -0.23823535442352295, -0.22848835587501526, -0.06129637360572815, -0.25786063075065613, -0.3195229172706604, -0.3755457103252411, 0.9529218077659607, -0.3546333312988281, -0.47068947553634644, 0.00471062446013093, 0.25463318824768066, -0.38947975635528564, -0.6534376740455627, 0.2740098834037781, -0.19069617986679077, -0.23798969388008118, 0.0927315205335617, 0.42873260378837585, -0.059313878417015076, 0.11247917264699936, -0.2735399603843689, 0.1715707778930664, -0.054231271147727966, 0.18462005257606506, 0.39191049337387085, -0.5286329984664917, 0.11188005656003952, -1.2362587451934814, 0.6065523028373718, 0.0785083919763565, -0.36718663573265076, 0.2113424688577652, -0.4897962510585785, -0.49936968088150024, 0.4608643054962158, -0.6981572508811951, -0.6638231873512268, -1.1845382452011108, 0.3324745297431946, -0.14766962826251984, -0.12204329669475555, -0.10024163872003555, 0.15493980050086975, 0.4379355311393738, 0.2380591481924057, 0.6043985486030579, 0.22986334562301636, -0.3107466697692871, 1.005468726158142, -0.595229983329773, 0.4103684723377228, 0.2242683619260788, -0.13890090584754944, -0.033330027014017105, -0.27806153893470764, -0.7606508135795593, -0.41047272086143494, -0.5693619847297668, -0.39362913370132446, -0.20341652631759644, 0.482735812664032, -0.39458122849464417, -0.891838014125824, -0.1053551509976387, -1.1299333572387695, -0.7025110125541687, 0.2781485617160797, -0.5486629009246826, -0.4000842273235321, -0.7707090377807617, -1.2584484815597534, -0.5596320629119873, -0.6159985661506653, -0.9414173364639282, 0.5697060823440552, -0.23487208783626556, -0.4689271152019501, -0.6338882446289062, 0.17749489843845367, -0.15038633346557617, 1.1304681301116943, -0.7182071805000305, 0.7519737482070923, 0.02899826690554619, -0.22770699858665466, 0.04519825428724289, -0.02663380280137062, 0.011880889534950256, -0.2601446509361267, 0.12789663672447205, -0.7441052198410034, 0.13958333432674408, -0.6417251229286194, -0.0954051986336708, 0.04688001051545143, 0.2770562171936035, 0.7196093797683716, -0.1852775663137436, -0.7035229206085205, 0.09906280040740967, 1.1388354301452637, -0.5949462652206421, 0.09705120325088501, -0.11845790594816208, 0.9526350498199463, 0.23866450786590576, 0.011921041645109653, 0.5344005823135376, 0.46707791090011597, 0.23838326334953308, 0.26200371980667114, -0.08139558881521225, -0.12308308482170105, -0.5193492770195007, 0.39337825775146484, 1.5456769466400146, 0.11176931113004684, -0.08684615045785904, -1.0798802375793457, 1.0489552021026611, -1.1636905670166016, -1.1750881671905518, 0.5451521873474121, 0.48775726556777954, 0.47346970438957214, -0.6902321577072144, -0.13007237017154694, -0.2287101298570633, 0.6199462413787842, 0.41103804111480713, -0.2365584373474121, -0.6528516411781311, 0.16666671633720398, 0.29973605275154114, 0.0039864834398031235, 0.7157536149024963, -0.12501317262649536, 0.9403766393661499, 15.190094947814941, 0.9044138193130493, 0.05072510614991188, 0.8999089002609253, 0.4369353950023651, -0.04038049280643463, -0.31424227356910706, 0.15853144228458405, -0.9649900197982788, 0.01143033616244793, 1.233160376548767, -0.09393105655908585, 0.7929231524467468, 0.02000247687101364, 0.5049980282783508, 0.36033567786216736, -0.23501677811145782, 0.28129905462265015, 0.46271812915802, -1.3531323671340942, 0.38604915142059326, 0.12375454604625702, 0.14716818928718567, 0.6099995374679565, 0.5162084102630615, 0.8284375667572021, 0.6461512446403503, -0.1762666255235672, 0.62233567237854, -0.08124624192714691, 1.1696453094482422, -0.07132143527269363, 0.23628079891204834, 0.41546908020973206, -0.9834785461425781, -0.3332618176937103, -0.49045050144195557, -1.1329671144485474, 0.3043408691883087, 0.002879383973777294, -0.4257506728172302, -0.41254791617393494, 0.05020640417933464, 0.7789899110794067, 0.12902459502220154, 0.1606048345565796, -0.054943833500146866, 0.7535169124603271, 0.45996662974357605, -0.07682015001773834, 0.5296742916107178, 0.3731335997581482, 0.4558934271335602, 0.3914557099342346, 0.109675832092762, 0.039069969207048416, 0.020150108262896538, 0.3430200219154358, -0.4203362464904785, -0.012161703780293465, -0.28156396746635437, -0.2709602415561676, 0.31795844435691833, 0.7168180346488953, 0.35346460342407227, -0.035123445093631744, -0.40017059445381165, 0.3505345284938812, 0.6575947999954224, 0.12031473964452744, -0.380645215511322, -0.19463792443275452, 0.3575836420059204, -0.21540798246860504, 0.19143280386924744, 0.681328535079956, -0.02644755318760872, -0.41731879115104675, -0.8712972402572632, -0.22232644259929657, 0.34552595019340515, -0.9689009189605713, -0.5136064291000366, 0.9541478157043457, -0.1003267914056778, -0.4939199984073639, 0.19606320559978485, -0.40227922797203064, -0.28904157876968384, 0.2514488399028778, -1.2496395111083984, -0.9346300959587097, 0.22686955332756042, 0.07108212262392044, 0.018506154417991638, 0.4749948978424072, 1.2706046104431152, 0.16056329011917114, -0.1807539463043213, 0.20026710629463196, 0.2787374258041382, -0.0401555672287941, 0.06598713248968124, -0.8444705009460449, 0.9895954132080078, 0.402016282081604, -0.34047016501426697, 0.2946271598339081, 0.18439511954784393, 0.29181283712387085, -0.80743408203125, -0.31043481826782227, 0.9375892281532288, -1.1033427715301514, -0.13967451453208923, -1.2181391716003418, -0.8017997741699219, 0.45376282930374146, 0.702739417552948, 0.004442677367478609, 0.19317349791526794, -0.1906103640794754, -0.4013957381248474, -0.07433551549911499, -0.965865969657898, 0.16476739943027496, 0.9940739870071411, -0.6927592754364014, -0.40511274337768555, -0.20267561078071594, 0.7413651347160339, -0.9050723314285278, -0.3291139602661133, -0.6563941240310669, 0.30439382791519165, -0.13878905773162842, 0.5426751971244812, -0.0820222795009613, 0.5951261520385742, 1.1647018194198608, -0.18694517016410828, -0.7733432650566101, -0.09476817399263382, -1.3575210571289062, 0.05059290677309036, 0.3888586759567261, 0.5050766468048096, -0.3259906470775604, 0.4347619414329529, 0.417647123336792, 0.16762609779834747, -0.48560434579849243, -0.8334808349609375, -0.08990480750799179, 0.3662905991077423, -0.590462863445282, 0.3527894914150238, 0.30067163705825806, 0.5134900808334351, 0.2223016321659088, 0.28388500213623047, 0.2187890261411667, -0.22361187636852264, -0.6316863298416138, 0.36049801111221313, -0.06284022331237793, 0.26389428973197937, -0.4464626908302307, -0.17945577204227448, -1.3079091310501099, -0.23020733892917633, -1.104547381401062, 0.26168933510780334, -0.7256067395210266, -0.5197878479957581, -0.12225967645645142, -0.14588424563407898, 0.11159895360469818, 0.39219608902931213, -0.13811948895454407, -0.3520866632461548, -0.9101811051368713, -0.6452789306640625, 0.694409191608429, 1.0388798713684082, -0.6447869539260864, 0.22617554664611816, -0.19652390480041504, -0.15065313875675201, 0.33098217844963074, 0.38044726848602295, -0.4835576117038727, -0.6031863689422607, -1.295044183731079, 0.3457218110561371, -0.06311694532632828, -0.14666210114955902, -0.3746322691440582, 0.17904415726661682, 0.02837667241692543, -0.5223076343536377, -0.1276899129152298, 0.26258575916290283, -0.7872671484947205, -0.7890814542770386, 0.24793045222759247, -0.7561562061309814, 0.41140109300613403, 0.014515559189021587, -0.4811394512653351, 0.0982423722743988, 0.9243147969245911, -0.011402968317270279, -1.2255483865737915, -0.5946886539459229, 0.26408839225769043, -1.0246087312698364, 0.5954643487930298, -0.5162333250045776, -0.1452617049217224, -1.0223774909973145, -0.40023428201675415, -0.0722198486328125, 0.27421924471855164, -0.36895695328712463, 1.1024787425994873, 0.15405212342739105, -0.860770046710968, 0.022873979061841965, 0.16169340908527374, -0.005314509384334087, 0.17578527331352234, 0.6357874870300293, 0.5849315524101257, -0.033924393355846405, 0.5012937188148499, 0.3698533773422241, 0.1379091739654541, -1.0156432390213013, 0.24794428050518036, 0.5951374173164368, -0.4230775237083435, -0.18372786045074463, 0.9836301803588867, -0.4952181875705719, -0.9943175911903381, 0.059311363846063614, -1.3986642360687256, -0.4163641035556793, -0.29301756620407104, 0.6558296084403992, 0.49641725420951843, -0.09755592793226242, -0.2567688226699829, -0.7818033695220947, 0.20355452597141266, 0.009394625201821327, -0.5771702527999878, 0.5847416520118713, -0.4912157952785492, -0.46311140060424805, 0.49298030138015747, 0.7058223485946655, -0.38566523790359497, -0.6080374717712402, -0.6409035325050354, -0.5217104554176331, -0.03595639392733574, 0.15763521194458008, -0.5734909176826477, -0.20702075958251953, 0.947748601436615, 0.2649129331111908, 0.5709928870201111, -0.22791975736618042, -0.01013511884957552, 0.4039739966392517, 0.5205891728401184, 0.05372892692685127, -0.6711185574531555, -0.4059073030948639, 1.4074915647506714, 1.1425225734710693, -0.5663203001022339, -0.18394289910793304, -0.020795293152332306, -0.7598296999931335, 0.36639541387557983, 0.35507944226264954, 0.08887933194637299, 0.6945146918296814, -0.01668410375714302, 0.3160668611526489, 0.24873203039169312, -1.3585346937179565, -0.22985541820526123, 0.42809247970581055, 1.018983244895935, 0.6895614266395569, 0.1227395236492157, 0.1251915693283081, 0.796329140663147, 0.00998096726834774, 0.13455720245838165, 0.5222366452217102, 0.48022741079330444, -0.15571796894073486, 0.08709435164928436, 0.05663690343499184, 0.3929581344127655, -0.9182258248329163, -0.7198407649993896, 0.03850765526294708, 0.31498318910598755, 0.2939337193965912, 0.8080826997756958, 0.8911812901496887, 0.44939693808555603, 0.2513548731803894, 0.20076380670070648, 0.6007668972015381, -0.5501417517662048, -0.16270461678504944, -0.04695548489689827, -0.7161371111869812, -0.2500568926334381, -0.10010619461536407, -0.5687577724456787, -0.32043972611427307, -0.1802336871623993, -0.007051132153719664, 0.20595599710941315, 0.15275703370571136, 1.2060701847076416, 0.7064882516860962, 0.5188744068145752, -0.29762163758277893, -0.713748574256897, -0.01387257594615221, -1.1228936910629272, 0.26545605063438416, -0.5098729133605957, -0.05354637652635574, 0.0335795134305954, 0.00436125323176384, -0.005499717313796282]}, "authors": [{"authorId": "2305696370", "name": "Hengyu Zhang"}], "references": [{"paperId": "31372726dbfcd9ea92a2e08a62ae5439d989c824", "title": "Equipping Transformer with Random-Access Reading for Long-Context Understanding"}, {"paperId": "73377049a6c47e103113957cd3875fb85c2ec407", "title": "Can LLMs substitute SQL? Comparing Resource Utilization of Querying LLMs versus Traditional Relational Databases"}, {"paperId": "a5a5de7f4a2ac60773e4f3de78e1299998aceb7b", "title": "Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange"}, {"paperId": "96c0f2ec1e96b89ad5f442fc0828be750d124393", "title": "Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models"}, {"paperId": "713806165610c237f551a7b68e6b09b3ded75502", "title": "SparQ Attention: Bandwidth-Efficient LLM Inference"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "b6346f9fa093b8e85df712485a2b851b9f680dac", "title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea", "title": "Instruction Tuning with GPT-4"}, {"paperId": "76b19363b10d7ea783e4a6494eae40d73c8e9628", "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models"}, {"paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d", "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "687733531aa65ea28b8a389399d3cbce9c99cee8", "title": "LSG Attention: Extrapolation of pretrained Transformers to long sequences"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a", "title": "Automatic differentiation in PyTorch"}, {"paperId": "09aec82196628ea81ed3e044bb74acc6a4426b32", "title": "CHESS upgrade 1995: Improved radiation shielding (abstract)"}, {"paperId": "8d350f2d767a70d55275a17d0b3dfcc80b2e0fee", "title": "Perplexity\u2014a measure of the difficulty of speech recognition tasks"}, {"paperId": "4e1aed9eee0cf0df5d29eebfaa84428141327c70", "title": "UZH_Pandas at SimpleText@CLEF-2023: Alpaca LoRA 7B and LENS Model Selection for Scientific Literature Simplification"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "0b03ad2bdc2df0d109661f76dcaf46e609617a44", "title": "Spoken Document Retrieval: 1998 Evaluation and Investigation of New Metrics"}, {"paperId": null, "title": "How long can context length of open-source llms truly promise?"}, {"paperId": null, "title": "Introducing mpt-30b: Raising the bar for open-source foundation models"}, {"paperId": null, "title": "Together Computer"}]}