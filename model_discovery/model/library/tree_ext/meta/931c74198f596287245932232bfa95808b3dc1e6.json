{"paperId": "931c74198f596287245932232bfa95808b3dc1e6", "title": "Attention is Naturally Sparse with Gaussian Distributed Input", "abstract": "The computational intensity of Large Language Models (LLMs) is a critical bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism in transformer architectures. Addressing this, sparse attention emerges as a key innovation, aiming to reduce computational load while maintaining model performance. This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs. By establishing a set of foundational assumptions and employing a methodical theoretical approach, we unravel the intrinsic characteristics of attention score sparsity and its implications on computational efficiency. Our main contribution lies in providing a detailed theoretical examination of how sparsity manifests in attention mechanisms, offering insights into the potential trade-offs between computational savings and model effectiveness. This work not only advances our understanding of sparse attention but also provides a scaffold for future research in optimizing the computational frameworks of LLMs, paving the way for more scalable and efficient AI systems.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This study presents a rigorous theoretical analysis of the sparsity in attention scores within LLMs, particularly under the framework of Gaussian inputs, unraveling the intrinsic characteristics of attention score sparsity and its implications on computational efficiency."}, "embedding": {"model": "specter_v2", "vector": [0.5914056897163391, 0.972353458404541, -0.23327240347862244, 0.15383951365947723, -0.5835838913917542, 0.1915273219347, 0.6467261910438538, -0.260761559009552, -0.11199728399515152, -0.47510331869125366, 0.13091880083084106, 0.2825344204902649, -0.0392252579331398, -0.1369878649711609, -0.2935733199119568, 0.10275980830192566, -0.8658822774887085, 0.5884353518486023, 0.0469551645219326, -0.10651657730340958, -0.03520067036151886, -0.8915805220603943, -1.454543948173523, 0.048900000751018524, 0.21258820593357086, 1.063412070274353, 0.4758056700229645, 0.6759952306747437, -0.24310529232025146, 0.7954504489898682, 0.46347686648368835, -0.43307602405548096, 0.398333877325058, -0.1528203934431076, -0.08894061297178268, -0.5629873275756836, 0.6228459477424622, -0.27723386883735657, -0.8592321276664734, 1.2686331272125244, -0.3808557391166687, 0.09370622038841248, 0.6220651865005493, -0.859307587146759, -0.5321767330169678, 0.8771297335624695, 0.5775793194770813, 0.9267175793647766, -0.3034345805644989, -0.28961241245269775, 1.8020362854003906, -1.4146738052368164, 0.22189123928546906, 1.8926939964294434, 0.14701856672763824, 0.057283781468868256, -0.2609504461288452, -0.5851108431816101, 0.9170954823493958, 0.3667919933795929, -0.9813008308410645, -0.5119501352310181, 0.027981538325548172, 0.25742825865745544, 1.8661028146743774, -0.3478619456291199, -0.04695894569158554, 0.28736209869384766, 0.1843816488981247, 1.7105076313018799, -0.0020876466296613216, -1.0090450048446655, 0.11552812904119492, 0.0452406145632267, -0.007551271002739668, 0.7183266282081604, -0.28067994117736816, 0.13727658987045288, -0.9593288898468018, -0.42080217599868774, 0.5971449613571167, -0.23525932431221008, 0.1470623016357422, -0.39836356043815613, -0.15984344482421875, 0.8759100437164307, 0.4682891368865967, 0.6819731593132019, -0.5604375004768372, 0.8754547238349915, 0.09337940812110901, 0.5473976731300354, -0.06631127744913101, 0.3775005340576172, 0.16176682710647583, 0.29370594024658203, -0.8678037524223328, 0.2929609715938568, 0.21983085572719574, 1.113843560218811, -0.23772172629833221, 0.6634423732757568, -0.6606693863868713, 0.2661108672618866, 1.3745332956314087, 0.549546480178833, 0.46997812390327454, -0.6352118253707886, 0.2750234007835388, -0.6856858134269714, 0.19928649067878723, -1.361382246017456, 0.2586977481842041, -0.27626803517341614, -0.6012396812438965, -1.1079403162002563, -0.3505740165710449, 0.5040127038955688, -0.6240020990371704, 0.9655678868293762, -0.5760781764984131, -0.16716237366199493, -0.4315558671951294, 0.4976096749305725, 0.1560412347316742, 0.516645610332489, 0.43215903639793396, 0.43742457032203674, 0.8440583348274231, -0.3571173846721649, -0.5790176391601562, -1.5836708545684814, 0.5897465944290161, -0.231772318482399, 0.3127327859401703, -0.007605017628520727, -1.59734046459198, -0.8182668685913086, -0.6259634494781494, 0.060094770044088364, -0.12121564894914627, 0.377582848072052, 1.0571209192276, -0.014022314921021461, -1.3040461540222168, 0.37160930037498474, -0.3682110905647278, 0.08025263249874115, 0.8642114400863647, 0.5909315943717957, 0.23048663139343262, -0.6618949174880981, -1.0902259349822998, 0.678673267364502, -0.08622672408819199, -0.7787013053894043, -0.24197058379650116, -0.3610227406024933, -1.4450348615646362, 0.3024041950702667, 0.26889196038246155, -0.46442854404449463, 1.1911062002182007, -0.6442888975143433, -0.8478101491928101, 0.8227055668830872, -0.6503655314445496, -0.033918820321559906, -0.32896026968955994, 0.049908772110939026, -0.3061313033103943, -0.08799775689840317, -0.035095926374197006, 0.587272584438324, 0.7705647945404053, 0.0509021021425724, -0.2220943570137024, -0.1173328161239624, -0.28249794244766235, -0.09916479885578156, -0.20598965883255005, 1.0530797243118286, -0.35222867131233215, -0.10450025647878647, 0.4185800552368164, 0.5032954812049866, -0.05609128251671791, -0.07238391041755676, 0.036047790199518204, -1.309118628501892, 0.18951497972011566, -0.02606404945254326, 1.1272194385528564, -0.9202924966812134, -0.2654815912246704, 0.1508515477180481, 0.06274095177650452, -0.3159402012825012, -0.6745772361755371, 0.4781113564968109, -0.47120848298072815, 0.0606987439095974, -0.1819436103105545, -0.8871973156929016, 0.3063787817955017, -0.22363413870334625, -0.6517990231513977, 0.10553345829248428, 0.3182721734046936, 1.047515869140625, -0.7845349311828613, -0.20021530985832214, -0.02808416448533535, -0.13769219815731049, -1.0568960905075073, 1.1768308877944946, -0.32190948724746704, -0.23195375502109528, -0.3098026216030121, -0.12862449884414673, -0.13635046780109406, -0.5449411273002625, 0.3914344310760498, -0.5035596489906311, 0.25312140583992004, 0.5263369083404541, -0.3855263590812683, 1.267730951309204, -0.26138508319854736, 0.5676217675209045, -0.03768698498606682, -1.0298148393630981, 0.13417674601078033, 0.4228613078594208, -0.4510606825351715, -0.7895899415016174, 0.45835500955581665, 0.35249242186546326, -0.161289781332016, 0.29811766743659973, 0.45524197816848755, 0.8544849753379822, -0.5506195425987244, 0.2825469672679901, 0.7371363639831543, -0.42770323157310486, 0.4891805350780487, 0.6661436557769775, 0.6761464476585388, -0.06453362107276917, 0.6450889110565186, -0.23588228225708008, 0.23934434354305267, -0.8098321557044983, -0.4198853671550751, 0.8464171886444092, 0.7913196682929993, 0.5727905035018921, 0.3895094096660614, -0.831303596496582, -0.4573344886302948, 0.16349823772907257, 0.6284341812133789, 1.4566532373428345, -0.25949057936668396, -0.1289776712656021, -0.6258172988891602, 0.19737035036087036, -0.26661357283592224, 0.3807891011238098, -0.25190258026123047, -0.31578928232192993, -0.5494710206985474, -0.8719568252563477, 0.5744022727012634, 0.5173718333244324, 0.31289923191070557, -0.7691728472709656, -0.514507532119751, -0.21095067262649536, 0.6087086796760559, -0.7468221783638, -0.5018112659454346, 0.4804368019104004, -0.3387978672981262, -0.1934410184621811, 0.04992377758026123, -0.3092024326324463, 0.44557100534439087, -0.6334841251373291, 1.1118009090423584, -0.7722920179367065, -0.2683594822883606, 0.46942099928855896, 1.0867717266082764, -1.022767424583435, -0.4585372507572174, -0.07375754415988922, 0.2817869186401367, -0.034823887050151825, 0.6932756900787354, 0.30749601125717163, -0.0603686086833477, -0.04944700747728348, -0.30497094988822937, 0.2687779664993286, 0.35125619173049927, 0.1199755147099495, 0.8561224937438965, -0.5417249202728271, 0.06700613349676132, -0.9633409976959229, 0.6098136901855469, -0.1648496389389038, -0.6113367080688477, 0.14973364770412445, -0.46320781111717224, -0.4717545509338379, 0.5885611176490784, -0.6461635231971741, -0.2739533483982086, -0.7676989436149597, 0.4892866611480713, -0.31214043498039246, -0.4713706374168396, 0.16481980681419373, 0.39193135499954224, -0.20354554057121277, 0.27301666140556335, 0.6078726053237915, 0.252538800239563, 0.18992283940315247, 0.40357279777526855, -0.8211402893066406, 0.432672381401062, 0.046956900507211685, -0.09972001612186432, 0.03004089742898941, -0.29479268193244934, -0.5962202548980713, -0.37765225768089294, -0.06988272815942764, 0.1480960249900818, -0.11881379783153534, -0.1814107894897461, -0.7494667768478394, -0.7017651200294495, -0.0908094123005867, -0.9787023663520813, -0.10286286473274231, 0.16171300411224365, -0.1727590262889862, -0.4743974506855011, -1.2597733736038208, -1.3837844133377075, -0.7347209453582764, -0.620317816734314, -1.1362700462341309, 0.5752696394920349, -0.0007060404750518501, -0.84684818983078, -0.2881369888782501, -0.2722164988517761, -0.5671861171722412, 1.363700032234192, -1.0162396430969238, 1.1273900270462036, -0.42983436584472656, -0.5489656329154968, -0.636543333530426, 0.28917399048805237, -0.015781983733177185, -0.4222875237464905, -0.2956739068031311, -0.7878945469856262, -0.01349521242082119, -0.07524140179157257, -0.5249233841896057, -0.2791525423526764, 0.5487625598907471, 0.8654287457466125, -0.1718152016401291, -0.5238152146339417, 0.2318427413702011, 1.3060206174850464, -0.5326654314994812, -0.28336548805236816, -0.07136733829975128, 0.8880206942558289, 0.19179479777812958, -0.35165977478027344, 0.544162929058075, 0.5192123055458069, 0.5767146348953247, 0.4475246071815491, 0.10127096623182297, -0.08854608237743378, -0.5813851356506348, 0.48871758580207825, 1.6470803022384644, 0.3177188038825989, 0.08744809031486511, -0.8866040110588074, 0.6412414908409119, -1.2267991304397583, -0.9268140196800232, 0.8665623068809509, 0.9713973999023438, 0.1969643533229828, -0.2789353132247925, -0.7076798677444458, -0.2566240429878235, 0.43633952736854553, 0.16015182435512543, -0.4562414586544037, -0.44375625252723694, -0.22738319635391235, 0.8956276178359985, 0.16905030608177185, 0.3691687285900116, -0.284472793340683, 0.5169677734375, 14.80618953704834, 1.001889705657959, 0.21567341685295105, 0.8746457099914551, 0.6885115504264832, 0.0025989320129156113, -0.5110855102539062, -0.17206671833992004, -1.0841761827468872, -0.04059213772416115, 1.1246962547302246, -0.14769692718982697, 0.7372382283210754, 0.3480167090892792, -0.2918541729450226, 0.16430334746837616, -0.7730090618133545, 0.9340603351593018, 0.7813159227371216, -1.1888587474822998, 0.6228305697441101, -0.026227416470646858, 0.4504490792751312, 0.6453847885131836, 0.6508089900016785, 0.6920151710510254, 0.754037082195282, -0.7206785678863525, 0.5887071490287781, 0.28513479232788086, 0.735349714756012, 0.06428388506174088, 0.05662290006875992, 0.6672381162643433, -0.9850366115570068, -0.21356134116649628, -0.6544283032417297, -1.3570435047149658, 0.2014370709657669, -0.12864792346954346, 0.08486126363277435, -0.932062029838562, -0.33482274413108826, 0.621905505657196, 0.1574535220861435, 0.5106989145278931, -0.1466054469347, 0.5754658579826355, -0.5988706350326538, 0.1310025006532669, 0.19030173122882843, 0.6115992665290833, 0.15119972825050354, -0.2988356947898865, 0.40316978096961975, -0.11546099931001663, 0.24948108196258545, 0.9636070728302002, -0.0790390893816948, -0.04930244758725166, -0.7500367164611816, -0.24822638928890228, 0.24132491648197174, 0.4380281865596771, 0.6650214791297913, 0.14229059219360352, -0.5671013593673706, 0.1280582696199417, 0.7948653697967529, 0.2550915777683258, 0.21289955079555511, 0.08342006802558899, 0.19500568509101868, -0.6358535885810852, 0.2518942952156067, 0.8342812061309814, -0.19628261029720306, -0.5097368359565735, -0.790544331073761, -0.7144061923027039, 0.4658222794532776, -0.9106126427650452, -0.9282013773918152, 0.6233994960784912, -0.10563479363918304, 0.014778955839574337, 0.317058265209198, -0.7630215287208557, -0.24500179290771484, 0.5811349749565125, -0.9453455209732056, -0.8864323496818542, 0.632274329662323, -0.11844092607498169, 0.0963042750954628, -0.280291885137558, 1.5280859470367432, -0.08216527849435806, -0.19733557105064392, 0.37473994493484497, -0.306341290473938, -0.4604845345020294, -0.5625354051589966, -0.5584023594856262, 0.6508722305297852, 0.17949558794498444, 0.12260279059410095, 0.5411496758460999, 0.11815675348043442, 0.15364167094230652, -1.0895051956176758, 0.392720103263855, 1.0147526264190674, -0.8884394764900208, -0.10162826627492905, -0.3823900520801544, -0.6688529849052429, 0.523078978061676, 0.30443570017814636, -0.2724338173866272, 0.18001043796539307, 0.04659615829586983, -0.5510029792785645, 0.08430518209934235, -0.37635427713394165, 0.01908312737941742, 0.18191933631896973, -1.018268346786499, -0.5750244855880737, -0.15394796431064606, -0.11758355051279068, -0.9863668084144592, -0.18683858215808868, -0.37538623809814453, -0.19436752796173096, 0.17701199650764465, 0.9314290285110474, -0.5726372003555298, 0.3879922032356262, 0.6196823120117188, -0.1940620094537735, -0.6583645939826965, -0.5045463442802429, -0.5829890370368958, -0.34894663095474243, -0.32091885805130005, 0.3380144536495209, -0.3458951711654663, -0.0271536223590374, 0.9272660613059998, 0.16515475511550903, -0.1832888275384903, -1.021064043045044, 0.26702681183815, -0.185177281498909, -0.864224910736084, -0.046453047543764114, -0.3629745543003082, 0.024760756641626358, -0.07668127864599228, 0.29004740715026855, 0.5820890665054321, 0.03756776824593544, -0.5141497850418091, 0.17571714520454407, -0.13199740648269653, 0.0040773977525532246, -0.8268935680389404, -0.6574058532714844, -1.5728693008422852, 0.07427609711885452, -1.1672654151916504, -0.06364956498146057, -0.7527487874031067, -0.4986497461795807, 0.03663787245750427, -0.34542176127433777, 0.16952906548976898, 0.4526583254337311, -0.12103220820426941, -0.4248866140842438, -0.2294052094221115, -0.7719339728355408, 0.6479973196983337, 0.4932709038257599, -0.8379684090614319, 0.6178900599479675, 0.0025121686048805714, -0.03786282613873482, 0.6210095286369324, 0.32194387912750244, -0.7086988687515259, -0.680778980255127, -1.4374938011169434, 0.7119225263595581, -0.14012494683265686, -0.011974639259278774, -0.7792049646377563, 1.0765657424926758, 0.5034273862838745, -0.36337658762931824, 0.22738094627857208, 0.6876851916313171, -1.121795892715454, -0.4389115869998932, 0.251130074262619, -0.9329375624656677, 0.23633331060409546, 0.2636909782886505, -0.25617286562919617, -0.4713818430900574, 0.68961101770401, -0.15712924301624298, -1.25381338596344, -0.5358572006225586, 0.5150247812271118, -0.6921865940093994, 0.3736998736858368, -0.17134037613868713, -0.07307267189025879, -0.9017546772956848, -0.49603259563446045, -0.04307802766561508, 0.1169203519821167, -0.3294806182384491, 0.8440155982971191, 0.603701651096344, -1.2209563255310059, 0.23333144187927246, 0.6690874099731445, 0.1663191318511963, -0.012318197637796402, 0.3090895116329193, 0.25570106506347656, -0.015993958339095116, 0.9622740745544434, 0.26330822706222534, 0.27656880021095276, -0.7418534755706787, -0.15205273032188416, 0.5552758574485779, -0.16008202731609344, -0.24866357445716858, 1.1739885807037354, 0.12852400541305542, -0.7696959376335144, 0.31785252690315247, -0.9605339765548706, -0.46176591515541077, -0.5035157799720764, 0.7643873691558838, -0.2366640269756317, -0.28315070271492004, -0.25368553400039673, -0.3260323405265808, 0.22974184155464172, 0.02058165892958641, -0.1617002636194229, 0.3117885887622833, -0.03676113858819008, -0.5757303833961487, 0.7216750383377075, 0.4618142545223236, -0.9731096029281616, -0.446109414100647, -0.9020885229110718, -0.38407430052757263, -0.19030442833900452, 0.5263203382492065, -0.04921945184469223, -0.6394855380058289, 0.9351688027381897, 0.3899514377117157, 0.21617966890335083, -0.16075751185417175, 0.23159103095531464, -0.3057803809642792, 0.5273614525794983, 0.6109212040901184, -0.36809930205345154, -0.7214037775993347, 1.318464994430542, 1.2197822332382202, -0.7791690230369568, -0.19972576200962067, -0.15259334444999695, -0.559148907661438, 0.558300793170929, 0.4842917323112488, -0.2731732428073883, 0.5713716149330139, -0.08411111682653427, -0.4169693887233734, -0.18800559639930725, -1.2195675373077393, -0.08719179779291153, 1.1177939176559448, 0.872484028339386, 0.6653597354888916, 0.3637225031852722, 0.059113409370183945, 0.934447169303894, 0.046562742441892624, 0.44592955708503723, -0.025319650769233704, 0.46707025170326233, -0.5637778639793396, 0.4351881742477417, -0.11726514250040054, 1.057708501815796, -0.7721673846244812, -1.0235979557037354, 0.27291956543922424, 0.2235284447669983, 0.020543860271573067, 0.40103405714035034, 0.9710912108421326, 0.5086969137191772, 0.41801172494888306, 0.1830892264842987, 0.7831722497940063, -0.6046046018600464, 0.2134004533290863, -0.30329981446266174, -0.6758704781532288, -0.0848214402794838, -0.1708538830280304, -0.24001279473304749, 0.08338814228773117, 0.08545683324337006, 0.10875588655471802, -0.17956270277500153, -0.04559284821152687, 1.0482208728790283, 0.7595182657241821, 0.3562717139720917, -0.756641685962677, -0.5395159721374512, -0.5586755275726318, -1.0467751026153564, 0.23246510326862335, -0.3268473148345947, -0.6248071789741516, -0.31056469678878784, -0.26019197702407837, -0.3159978687763214]}, "authors": [{"authorId": "3437152", "name": "Yichuan Deng"}, {"authorId": "2260334842", "name": "Zhao Song"}, {"authorId": "2233128774", "name": "Chiwun Yang"}], "references": [{"paperId": "eede8ab9a9cf3b71b61cff7a3bc6fcc9bcf1f2a1", "title": "BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model"}, {"paperId": "ed1aaad1beac180a33d74a539300a1fcb62a15ec", "title": "Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models"}, {"paperId": "faf7488da821133dbf34fe006676adfda4a764a0", "title": "Outlier-Efficient Hopfield Layers for Large Transformer-Based Models"}, {"paperId": "77386366ddbb5e446f13d4b766893b1419559775", "title": "Proxyformer: Nystr\u00f6m-Based Linear Transformer with Trainable Proxy Tokens"}, {"paperId": "55af74baab2bd597164d01361dc43dd8f2a05762", "title": "Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning"}, {"paperId": "b49d93f27ebaab542fb939d2b099c95c83341e6f", "title": "Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic"}, {"paperId": "22910f92c164971ff6ae886ece9c586c703c7153", "title": "On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis"}, {"paperId": "123150159d9b7230be1c60b8b8e20663dcc22884", "title": "The Fine-Grained Complexity of Gradient Computation for Training Large Language Models"}, {"paperId": "28863db0ed342b2a465a718fe835ccbd2e54c3e3", "title": "STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction"}, {"paperId": "5a41001f766f07484de4590134b695abd5fb4251", "title": "One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space"}, {"paperId": "e2eba58a0edff3b41091764d8292e79eb1547ee0", "title": "A Theoretical Insight into Attack and Defense of Gradient Leakage in Transformer"}, {"paperId": "23784d956043af0c247d7e985f15be407a3edd68", "title": "A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning"}, {"paperId": "95240dda409e28acccdc5cf619ad0c036cf4292d", "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"}, {"paperId": "3d2d60f74f008264a4c939ce923662302271227a", "title": "Provable Guarantees for Neural Networks via Gradient Feature Learning"}, {"paperId": "69285a2ece3779cfa13a480a04cf51cdc83e36dc", "title": "Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights"}, {"paperId": "72c02e167b0d43b6700d9bad9f116585fa17116e", "title": "Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention"}, {"paperId": "8a3ac655efb78d7fabd8cf86a7ec4820fd5889d0", "title": "An Automatic Learning Rate Schedule Algorithm for Achieving Faster Convergence and Steeper Descent"}, {"paperId": "93e58491830abe1eb965ab37ec64fa97263f6048", "title": "HyperAttention: Long-context Attention in Near-Linear Time"}, {"paperId": "504b333ea6e13f92d76b6835c18e48a9d822d246", "title": "How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation"}, {"paperId": "0a9030dd6cc2d438509f7568c1081d58c2523d5c", "title": "Fine-tune Language Models to Approximate Unbiased In-context Learning"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "6b8926d178db457303b6a7a5f2c3e638563e5d01", "title": "A Unified Scheme of ResNet and Softmax"}, {"paperId": "abb79cc72fab35bfeb50585a121375b9bebafbb0", "title": "On Sparse Modern Hopfield Model"}, {"paperId": "68ed29e5d398e6030cddcc575f1977973c8b0791", "title": "A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"}, {"paperId": "420533d1fc0ef9e652a228a669a858ad7d7162ea", "title": "Solving Attention Kernel Regression Problem via Pre-conditioner"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "761af20e7966e8a7d899975a332ac7eee3f92116", "title": "How to Protect Copyright Data in Optimization of Large Language Models?"}, {"paperId": "b169cbff7d5a11afac18f929d5c69ea0933a4da1", "title": "GradientCoin: A Peer-to-Peer Decentralized Large Language Models"}, {"paperId": "0316f43c8a1d40edb5835baf469bf2e089166073", "title": "When and How Does Known Class Help Discover Unknown Ones? Provable Understanding Through Spectral Analysis"}, {"paperId": "13c1a09ed284602d7c21e76c1c63460bb77a4568", "title": "Zero-th Order Algorithm for Softmax Attention Optimization"}, {"paperId": "e0cef31283a8b1292f95765242967098c321853b", "title": "Efficient SGD Neural Network Training via Sublinear Activated Neuron Identification"}, {"paperId": "602cea8acd2a25dee3c2970d4f05174cd6f91982", "title": "In-Context Learning for Attention Scheme: from Single Softmax Regression to Multiple Softmax Regression via a Tensor Trick"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "d193675b92fbfbf22ed82fda35cd2e73587e33bd", "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "ad4b365630f1c13d74d78f0f5d8cee87ef356d41", "title": "Fine-Tuning Language Models with Just Forward Passes"}, {"paperId": "6cb35dd6e1338faa0c3d6a6b0020bbcbcc18653d", "title": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training"}, {"paperId": "5e11fec80f7ea81f1e456157dddfe803f5ce4c82", "title": "Fast Submodular Function Maximization"}, {"paperId": "a5125c0eab822af20f55c969696aeaff6f4fdcf9", "title": "Attention Scheme Inspired Softmax Regression"}, {"paperId": "f13c766f995917f855ccb7e0b567a7b95146c4db", "title": "Randomized and Deterministic Attention Sparsification Algorithms for Over-parameterized Feature Dimension"}, {"paperId": "2dd27bed0b030941c25a4ed119b6fe6362d4186b", "title": "Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models"}, {"paperId": "8625290c1208297b56fbd723cff12b4bb0c7538c", "title": "Solving Regularized Exp, Cosh and Sinh Regression Problems"}, {"paperId": "ea683506bf202e612ea0c644affd052a09d90e36", "title": "A General Algorithm for Solving Rank-one Matrix Sensing"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "85c9e5f6705074d0d8187e14a2deb28501d9189f", "title": "Domain Generalization via Nuclear Norm Regularization"}, {"paperId": "9d3463da77f6288da6fa16631034293a733bd719", "title": "The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning"}, {"paperId": "39ed1c33af6f0a5fbc16354afcb223a03c9c139b", "title": "Fast Attention Requires Bounded Entries"}, {"paperId": "92c1430d29f1b4b26077370d09fbc9dc06fe901c", "title": "Task-Specific Skill Localization in Fine-tuned Language Models"}, {"paperId": "14c44df22b0d5361b61181fbf2ec0977ad189abf", "title": "KDEformer: Accelerating Transformers via Kernel Density Estimation"}, {"paperId": "b6903ef4a4b48f9afce769df10a43106091dcece", "title": "Adaptive and Dynamic Multi-Resolution Hashing for Pairwise Summations"}, {"paperId": "f17dd738a71e72212164df44aec48540e139193c", "title": "Dynamic Kernel Sparsifiers"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "9a1d94a930168918a1a1e1939b089d16d58d7865", "title": "A Kernel-Based View of Language Model Fine-Tuning"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "34295b799b7aff716222e4a5f99745ce198d8817", "title": "A Sublinear Adversarial Training Algorithm"}, {"paperId": "d2b297c553b5820ec114bfb1d037a537f2f66aad", "title": "A Theoretical Analysis on Feature Learning in Neural Networks: Emergence from Inputs and Advantage over Fixed Features"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "5344798dae132f5e881bd6e11b674275a5715903", "title": "Attention Mechanism for Neural Machine Translation: A survey"}, {"paperId": "6ab55155b287249376e3c498b238bff2e6ced34a", "title": "Attentive Walk-Aggregating Graph Neural Networks"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "32e0f1bf36ccee6ed552909c2c76f9d6b1c760ef", "title": "A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "1b3692177d70f0daaa07560991f97e5c44939c28", "title": "Deep Online Fused Video Stabilization"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "bf442ab269074665a68e4dbbe19e4efc97862541", "title": "Large Memory Layers with Product Keys"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "bf4b2a8a13a4e7ec3001c86b08aebd05c260fb86", "title": "Comparing Measures of Sparsity"}, {"paperId": "bd754fe83021ef0b521729444b827f7c1559e7f6", "title": "An Online and Unified Algorithm for Projection Matrix Vector Multiplication with Application to Empirical Risk Minimization"}, {"paperId": "4b56eef2862f7f553686f1dd190c56017122a6a0", "title": "PolySketchFormer: Fast Transformers via Sketches for Polynomial Kernels"}, {"paperId": "c49c292e1fb1d215c88828a52134b7ccfa52be44", "title": "Sparse Attention with Learning to Hash"}, {"paperId": null, "title": "Optimizing language models for dialogue"}, {"paperId": "b745b5512ad3b1f652dc0cbb5ddf5a940f397f7d", "title": "MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Adaptively sparse trans-formers"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Adore: Differentially oblivious relational database operators"}, {"paperId": null, "title": "(cid:32)Lukasz"}]}