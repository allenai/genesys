{"paperId": "cd471b5ef162906ef3d9a84398b3f98e9ee4bf56", "title": "A Review on Language Models as Knowledge Bases", "abstract": "Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufficiently large (web) corpus will encode a significant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a major advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem a LM should have to fully act as a KB, and review the recent literature with respect to those aspects.", "venue": "arXiv.org", "year": 2022, "citationCount": 121, "influentialCitationCount": 8, "openAccessPdf": {"url": "http://arxiv.org/pdf/2204.06031", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper presents a set of aspects that it is deemed a pretrained Language Models should have to fully act as a KB, and reviews the recent literature with respect to those aspects."}, "embedding": {"model": "specter_v2", "vector": [-0.1291309893131256, 0.9928745627403259, -0.8859068751335144, -0.07899508625268936, -0.588654637336731, -0.3385656774044037, 0.3354724049568176, -0.3566015064716339, -0.6575834155082703, 0.17633412778377533, 0.42819127440452576, -0.25754961371421814, 0.32700011134147644, 0.11576034873723984, -0.6223058104515076, 0.06088020280003548, -0.7180393934249878, 0.5851429104804993, 0.0509127713739872, -0.2814443111419678, -0.410448282957077, -0.8013206124305725, -0.3991430103778839, 0.18622025847434998, 0.30688947439193726, -0.05729931220412254, 0.23583711683750153, 0.7837744951248169, -0.5449670553207397, 0.4145571291446686, 0.6413418054580688, -0.45222264528274536, -0.1719551533460617, 0.14989504218101501, -0.37808433175086975, -0.07611791789531708, 0.485078364610672, -0.39744943380355835, -0.7325500845909119, 0.8275983333587646, -0.12834285199642181, 0.09074846655130386, 0.26763686537742615, -0.6646915674209595, -0.031331390142440796, 1.3412933349609375, 1.0398106575012207, 0.7939571142196655, -0.4472527503967285, -0.5571194887161255, 1.2480190992355347, -1.1451774835586548, 0.636979877948761, 1.4762614965438843, 0.331350713968277, 0.6809908151626587, -0.49978628754615784, -0.5379391312599182, 0.6961700320243835, 0.3644997775554657, -1.2316837310791016, -0.8222389221191406, -0.23517708480358124, 0.09067922830581665, 2.1576385498046875, -0.48410263657569885, -0.21508045494556427, 0.1325807422399521, -0.515821099281311, 1.3948216438293457, -0.20781384408473969, -1.4474109411239624, -0.35543280839920044, 0.9574679732322693, 0.0821823850274086, 1.0754368305206299, -0.36023250222206116, -0.0013610012829303741, -1.185465931892395, -0.32194504141807556, 0.2764972448348999, -1.0164666175842285, -0.6033825278282166, -0.3489449918270111, -0.09798594564199448, 1.0012614727020264, 0.04902048781514168, 0.8664346933364868, -0.210407093167305, 0.4527376592159271, 0.07956361770629883, 0.5335506200790405, -0.023165935650467873, 0.357744425535202, -0.6057233214378357, 0.5876243114471436, -1.0818158388137817, 0.10414024442434311, 0.47831085324287415, 0.6401347517967224, 0.21268607676029205, 0.3140358030796051, -0.5101296305656433, 0.4097255766391754, 1.767694354057312, 0.047039199620485306, 0.5399354696273804, -0.6306207180023193, 0.6436175107955933, -0.5602908134460449, 0.6624220013618469, -0.5727548003196716, -0.2586120069026947, -0.21207933127880096, 0.011149254627525806, -1.5509346723556519, -0.39198747277259827, -0.18935580551624298, -0.5809467434883118, 1.1128090620040894, -0.0340251661837101, 0.2621806263923645, 0.552627682685852, 0.3963180482387543, 0.5239591598510742, 1.0009902715682983, 0.3355277478694916, -0.4389531910419464, 0.45284920930862427, -0.5069636702537537, -1.0061566829681396, -0.7869250774383545, 1.1115102767944336, 0.2395685464143753, 0.007681041955947876, -0.4543982446193695, -1.0236427783966064, -1.0549763441085815, -0.6637860536575317, 0.17092330753803253, -0.6537255048751831, 0.35398826003074646, 0.8842257261276245, 0.8090240955352783, -1.3884209394454956, 0.6592652797698975, -0.05472467839717865, -0.13222137093544006, -0.0746234729886055, 0.668113648891449, 0.3030102252960205, -0.2979404926300049, -1.5662020444869995, 0.6013794541358948, 0.7311028242111206, -0.5748546719551086, -0.19356347620487213, -0.3589590787887573, -1.1966959238052368, -0.3808149993419647, 0.5072149038314819, -0.7325513958930969, 1.5547362565994263, -0.13117998838424683, -1.8050098419189453, 0.5980827212333679, -0.6213221549987793, -0.21819989383220673, 0.22783558070659637, -0.12257329374551773, -1.0300490856170654, -0.3348579704761505, -0.5577262043952942, 0.6285282969474792, 0.23604772984981537, -0.49367496371269226, -0.3592039942741394, 0.2340691089630127, 0.08514013886451721, -0.1015850082039833, -0.5773951411247253, 0.6021763682365417, -0.8914608359336853, -0.014988268725574017, 0.30542123317718506, 0.26569315791130066, -0.12508565187454224, -0.34812337160110474, -0.7813865542411804, -0.6964338421821594, 0.5336651802062988, -0.36848193407058716, 1.428208827972412, -0.4556822180747986, -0.43185558915138245, 0.2494736611843109, -0.6842195987701416, 0.2259131371974945, -1.0891953706741333, 0.7068118453025818, -0.23682209849357605, 0.6763209104537964, -0.14406129717826843, -0.8964465856552124, -0.00344736292026937, -0.0042757005430758, -0.6579317450523376, -0.3267592787742615, 0.1389697790145874, 0.735691249370575, -0.7752587199211121, -0.12450803071260452, 0.053332265466451645, 0.5385763049125671, -0.8481154441833496, 1.061730146408081, -0.6279305219650269, 0.20966720581054688, 0.002821295754984021, -0.3021916151046753, 0.1529615819454193, -0.0776604413986206, 0.7349456548690796, -0.254463791847229, -0.34591540694236755, 0.2377365678548813, -0.6926147937774658, 1.0553995370864868, -0.5448029041290283, 0.4837048649787903, 0.09351641684770584, -0.5347402095794678, -0.45657187700271606, 1.1377172470092773, -0.5104267001152039, 0.07614687830209732, -0.1914246678352356, 0.5287634134292603, -0.31642961502075195, -0.037179648876190186, 0.35994380712509155, 0.6638751029968262, 0.4051741063594818, 0.07897979021072388, 0.626367449760437, -0.4604809284210205, 0.8221663236618042, 0.27812495827674866, 0.5242531299591064, 0.24506144225597382, 0.17746615409851074, 0.2253580242395401, 1.039050817489624, -0.3663003146648407, -0.03724830970168114, 0.4885939061641693, 1.1516319513320923, 0.4364803433418274, -0.13428553938865662, -0.5080568194389343, -0.04739683121442795, 0.23256075382232666, 0.6907659769058228, 1.2432177066802979, -0.4349963068962097, -0.15691633522510529, -0.8318229913711548, -0.4448452889919281, -0.3742317855358124, 0.9638035893440247, -0.23555323481559753, 0.1697929948568344, -0.8636028170585632, -1.1653687953948975, 0.7207788825035095, 0.20792795717716217, 0.758175253868103, -0.4645271599292755, 0.21149049699306488, -0.08518128097057343, 0.13210304081439972, -0.6828705072402954, -0.2539372444152832, 0.12025777250528336, -1.0259698629379272, -0.34717249870300293, -0.08052118122577667, -0.23976285755634308, 0.3477208912372589, -0.940144419670105, 0.7857362031936646, -0.3447614312171936, 0.17003308236598969, 0.07766829431056976, 0.8650782108306885, -0.9430238604545593, -0.9952911734580994, -0.044638197869062424, 0.5876469612121582, -0.2116202563047409, 0.08965391665697098, 0.6930772066116333, 0.30465376377105713, -0.0063284714706242085, -0.6294420957565308, 0.15224748849868774, 0.12457651644945145, 0.2137792706489563, 0.7506532073020935, 0.008477364666759968, 0.36400699615478516, -1.776963472366333, 0.8862946629524231, 0.1889449805021286, -0.571898341178894, 0.5890900492668152, -0.5192862153053284, -0.20703107118606567, 0.34459081292152405, -0.5605238676071167, -0.692439079284668, -0.732830822467804, 0.0149346012622118, 0.17943556606769562, -0.2303582727909088, 0.8960950374603271, 0.06144005060195923, -0.054919466376304626, 0.1995561718940735, 0.4264797270298004, 0.3509860932826996, -0.39774227142333984, 0.7547242045402527, -0.7274627685546875, 0.10770248621702194, 0.6757163405418396, 0.43020913004875183, -0.22740304470062256, -0.6039560437202454, -0.9017003178596497, -0.12957575917243958, -0.4408212900161743, -0.47088804841041565, -0.13956546783447266, -0.5942932367324829, -0.6745815873146057, -0.24147015810012817, -0.39310988783836365, -1.0248298645019531, -0.46306395530700684, 0.3509368598461151, -0.038744568824768066, -0.31509944796562195, -0.7989868521690369, -1.0630773305892944, -0.7553719878196716, -0.4348304867744446, -0.42273539304733276, 0.060263797640800476, 0.13761551678180695, -0.4447450637817383, -1.0852727890014648, 0.4952697455883026, -0.028280578553676605, 1.0891345739364624, -0.9893086552619934, 1.5134294033050537, 0.1963609755039215, 0.08794256299734116, -0.24928320944309235, 0.655086100101471, 0.5484890937805176, -0.4050913155078888, 0.11270030587911606, -0.8760817646980286, 0.15653014183044434, -0.3324979543685913, -0.5234986543655396, 0.19503085315227509, 0.6630598902702332, 0.5753882527351379, -0.387118935585022, -0.6260680556297302, 0.04462842270731926, 1.5507969856262207, -0.7502050399780273, -0.07420624047517776, 0.2602028250694275, 0.5906352996826172, 0.36599084734916687, -0.5969401001930237, 0.02476353757083416, 0.1266254484653473, 0.6414660215377808, -0.05570085719227791, 0.04517100751399994, 0.017265640199184418, -0.2603175938129425, 0.7495553493499756, 1.6509275436401367, 0.08725528419017792, -0.3716218173503876, -0.8585842251777649, 0.44187676906585693, -1.1814812421798706, -0.33041027188301086, 0.6545024514198303, 0.7445101737976074, 0.8120946288108826, -0.46492817997932434, -0.23429693281650543, -0.048505693674087524, 0.7445166110992432, -0.13188576698303223, -0.45887744426727295, -0.31395223736763, -0.09517349302768707, 0.29601266980171204, 0.165781170129776, 1.0365217924118042, -0.3581385016441345, 0.7125948071479797, 14.408257484436035, 0.6380950808525085, 0.10515140742063522, 0.5970682501792908, 0.18423224985599518, 0.36970990896224976, -0.21396557986736298, -0.027564145624637604, -1.3732166290283203, -0.5349330306053162, 1.370613932609558, 0.24107049405574799, 0.45479804277420044, 0.10688551515340805, 0.3751595616340637, -0.04514065012335777, -0.2523970901966095, 0.4430646002292633, 0.611366331577301, -1.3260018825531006, 0.9107702374458313, 0.21782457828521729, 0.35979947447776794, 0.808566689491272, 0.6134395599365234, 1.0838698148727417, 0.21625864505767822, -0.4562380909919739, 0.45399555563926697, 0.31633755564689636, 0.47592228651046753, 0.06650549173355103, 0.8524788022041321, 0.9647797346115112, -0.45873239636421204, -0.32960614562034607, -0.687556803226471, -0.74704909324646, 0.49923717975616455, 0.15553848445415497, -0.8755490183830261, -0.386111319065094, -0.6671279668807983, 0.03626730293035507, -0.10853283107280731, 0.534110426902771, -0.247115820646286, 1.158545970916748, -0.37641388177871704, 0.17501939833164215, 0.027100475504994392, 0.6442037224769592, 0.26421722769737244, 0.029123034328222275, 0.1524926722049713, 0.20706014335155487, 0.09999997168779373, 0.15650451183319092, -0.6841846108436584, 0.24790777266025543, -0.11270689964294434, -0.7008283734321594, -0.06422531604766846, 0.5581101775169373, 0.7003759145736694, 0.09332913905382156, -0.00415786961093545, 0.0897534042596817, 0.33081671595573425, 0.4170967936515808, 0.16379034519195557, 0.037125326693058014, -0.009696067310869694, -0.37218770384788513, -0.11804016679525375, 0.5854848027229309, 0.5086511373519897, -0.49456241726875305, -0.9049489498138428, -0.3076539635658264, 0.5329859256744385, -0.7073183059692383, -0.6188085675239563, 0.957388699054718, -0.24557365477085114, -0.3979935050010681, 0.18275168538093567, -1.1822034120559692, -0.008841325528919697, 0.3403394818305969, -1.6303447484970093, -0.7216371893882751, 0.7520456314086914, -0.10943485051393509, -0.6354873180389404, -0.08691829442977905, 1.7956211566925049, -0.20433469116687775, -0.7102261781692505, -0.27568843960762024, 0.4400253891944885, 0.0625009536743164, -0.09260307252407074, -0.8660426735877991, 0.1705649495124817, -0.19205908477306366, 0.4147011935710907, 0.5411435961723328, 0.10607116669416428, -0.24688364565372467, -0.9800352454185486, -0.17731873691082, 0.8951002955436707, -1.163854956626892, -0.6543775200843811, -0.06724034249782562, -1.1094681024551392, 0.38503190875053406, 0.5883787274360657, -0.6170474886894226, 0.5073204040527344, 0.3257783055305481, -0.12916766107082367, 0.014110916294157505, -0.7972230911254883, -0.08458856493234634, 0.24986425042152405, -0.45240020751953125, -0.5200489163398743, 0.4644707143306732, -0.10185283422470093, -1.0984073877334595, -0.3559061884880066, 0.007898963987827301, -0.4206288754940033, 0.5100225210189819, 0.5729873776435852, -0.7752587199211121, -0.09104184061288834, 0.45604977011680603, 0.10698647797107697, -0.9655789136886597, 0.12359672039747238, -0.7947508692741394, -0.3739355206489563, 0.10490042716264725, 1.1546754837036133, -0.24721203744411469, 0.3447152078151703, 0.8859710693359375, 0.24486052989959717, -0.2256133258342743, -0.6916870474815369, -0.483856737613678, -0.2635969817638397, -0.5932779908180237, -0.09518405050039291, 0.05143285170197487, -0.03697335347533226, 0.07778153568506241, 0.5894038677215576, 1.1125184297561646, -0.33007681369781494, -1.0466489791870117, 0.6458607912063599, -0.13346190750598907, -0.12332584708929062, -0.28627851605415344, -0.10516379028558731, -1.0943512916564941, 0.48438048362731934, -1.3338595628738403, 0.18694163858890533, -1.2693746089935303, -0.3926547169685364, -0.07739875465631485, -0.2598910629749298, 0.06059175357222557, 0.6892408132553101, -0.42162543535232544, -0.9334776401519775, -0.4545053541660309, -0.4328097403049469, 0.26131853461265564, 0.41556593775749207, -0.7009249925613403, -0.1939985156059265, 0.11051932722330093, 0.3534003496170044, 0.7422888278961182, 0.6316178441047668, -0.4561316668987274, -0.9589751362800598, -1.665381908416748, 0.5541612505912781, -0.07304176688194275, -0.4962643086910248, -0.1312541961669922, 0.9741843342781067, 0.41573140025138855, -0.07749196141958237, 0.2704382836818695, 0.7268125414848328, -0.7113184928894043, -0.2119075357913971, 0.38856202363967896, -0.7979578971862793, 0.003769974922761321, 0.3497806787490845, -0.2459116131067276, -0.7066174149513245, 0.07397786527872086, -0.1512284129858017, -1.2696027755737305, -0.9192527532577515, 0.061680231243371964, -0.7771809101104736, -0.22020022571086884, -0.4742647707462311, -0.027355719357728958, -0.6066281199455261, -0.6064841747283936, -0.2599866986274719, 0.4766075909137726, -0.5648120641708374, 0.7197327613830566, 0.3355933725833893, -0.5725150108337402, -0.10077544301748276, 0.17861990630626678, -0.09091053903102875, -0.3778114914894104, 0.8786697387695312, 0.4161790907382965, -0.46135401725769043, 1.1978826522827148, 0.5808264017105103, 0.38249433040618896, -0.949682354927063, -0.5816376209259033, 0.7291879057884216, -0.2692386209964752, -0.11839917302131653, 1.3886735439300537, -0.7974690794944763, -1.2244207859039307, 0.4704327881336212, -1.0790075063705444, -0.9002406597137451, -0.3227638900279999, 0.8302291631698608, 0.11642901599407196, 0.12303684651851654, -0.33700793981552124, -0.30394306778907776, 0.461025208234787, 0.08733636885881424, -0.7840431928634644, 0.656222403049469, -0.39762115478515625, -0.5977816581726074, 0.8024202585220337, 0.17406418919563293, -0.2729964852333069, 0.020983435213565826, -0.6519219279289246, 0.019555728882551193, 0.03245178610086441, 0.4756399989128113, -0.6363067626953125, -0.39822646975517273, 0.9063283801078796, 0.6822205185890198, -0.13952724635601044, -0.25818419456481934, -0.17154951393604279, -0.14589795470237732, 1.0467774868011475, 0.2525927722454071, -1.186497449874878, -0.683118999004364, 1.3190711736679077, 1.260549783706665, -1.0537865161895752, 0.2256782501935959, -0.4434680640697479, -0.6727298498153687, 1.3144266605377197, 0.5194509029388428, 0.41809719800949097, 1.1788524389266968, -0.46110278367996216, 0.5315213203430176, 0.2823316752910614, -1.0465725660324097, -0.10491078346967697, 0.6593956351280212, 0.9072792530059814, 0.9035841226577759, 0.3084263801574707, 0.17084233462810516, 0.8672828078269958, 0.08004375547170639, 0.5835599303245544, 0.6294447183609009, 0.6081565022468567, -0.5272446274757385, -0.4548059105873108, 0.06940975785255432, 0.3451473116874695, -0.5514057874679565, -0.47616374492645264, -0.3881736993789673, 0.9548157453536987, 0.5256214737892151, 0.5704872608184814, 0.2857479751110077, 0.20451712608337402, 0.49429261684417725, 0.8530369997024536, 0.6105843186378479, -1.1668704748153687, 0.03998049348592758, -0.39798587560653687, -0.05502022057771683, 0.23032650351524353, -0.7474684119224548, -0.19201691448688507, -0.4612438976764679, 0.3280784785747528, -0.30563855171203613, 0.25970587134361267, 0.7551576495170593, 1.0999621152877808, 0.5350349545478821, -0.0062558953650295734, -0.6249680519104004, -0.07730485498905182, -0.5570535063743591, -1.2115256786346436, -0.02034158445894718, -0.9218699336051941, -0.6425414085388184, -0.13419069349765778, -0.0439249686896801, 0.026348020881414413]}, "authors": [{"authorId": "2006905770", "name": "Badr AlKhamissi"}, {"authorId": "2162508414", "name": "Millicent Li"}, {"authorId": "1709797", "name": "Asli Celikyilmaz"}, {"authorId": "1700007", "name": "Mona T. Diab"}, {"authorId": "2320509", "name": "Marjan Ghazvininejad"}], "references": [{"paperId": "916a06a6d51aa93de27aac2f3e14faed08dd6706", "title": "Formal Mathematics Statement Curriculum Learning"}, {"paperId": "aa7ed7e04182d0588bb53376e472afb59e236c36", "title": "Rethinking Explainability as a Dialogue: A Practitioner's Perspective"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "4d1e6c441a0b6aa2b9ec500a5064dc307d797b0c", "title": "Relational Memory-Augmented Language Models"}, {"paperId": "c783e1fb3ce8514f981925ee590c00884660ee4e", "title": "CM3: A Causal Masked Multimodal Model of the Internet"}, {"paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8", "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"}, {"paperId": "92173d081b15824d22a9ef070e118744ceee8052", "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"}, {"paperId": "4a247cbfca9dcf91e2da24e6d2d84601a9041a8f", "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs"}, {"paperId": "3ecc1bb0171e7540410fbd454fed473b8b6fa437", "title": "On Semantic Cognition, Inductive Generalization, and Language Models"}, {"paperId": "9286ac6e9b1aacd7d93496eb4615ae7678876d2a", "title": "Fast Model Editing at Scale"}, {"paperId": "290867638c5ca520de5c48aa4336f196d426c226", "title": "Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey"}, {"paperId": "c4b95abe16439fddd1db33e9aa386bec8a667e39", "title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models"}, {"paperId": "521ccc898395a2818fced22b4cf371b0e5121f94", "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models"}, {"paperId": "ecad7a322ede6de0013b46dc64429eed4c43e8af", "title": "BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief"}, {"paperId": "4c5f4ddc68be643fb34ea969bf2c105ff7538995", "title": "Can Language Models be Biomedical Knowledge Bases?"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "9b56086e420ecb216f85d408a25264f640e46705", "title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "e248b22b7107ab057a0ea42a4dd075a5a4b2df26", "title": "How to Query Language Models?"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "ff2f48fe6438adcaf860aac0f41c584568beafb5", "title": "CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with Minimal Supervision"}, {"paperId": "e596b8adbffa546dbc163e817fb3de72744ec4f6", "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models"}, {"paperId": "9c954f8ac18080d46ef7069357c9dbd9cfb88486", "title": "Combining Feature and Instance Attribution to Detect Artifacts"}, {"paperId": "ac8d33e4c0a45e227a47353f3f26fbb231482dc1", "title": "Time-Aware Language Models as Temporal Knowledge Bases"}, {"paperId": "ead441f3e9db042ffdeaf469f70bbe4b127d9060", "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models"}, {"paperId": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a", "title": "Towards Understanding and Mitigating Social Biases in Language Models"}, {"paperId": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c", "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks"}, {"paperId": "f50abcae0477351e9d2814547158d348ebf59bcb", "title": "Empowering Language Understanding with Counterfactual Reasoning"}, {"paperId": "da454295392cf4caaa39cc465734237ffe55392f", "title": "PTR: Prompt Tuning with Rules for Text Classification"}, {"paperId": "dc2699529f7980426e67dd80dd61adf12e2e595a", "title": "Entailment as Few-Shot Learner"}, {"paperId": "2bb1e1a5b9a16f6828fe94736cea5dab264533a6", "title": "Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "240b0caabb415578bdea4da7d0a32bdff2e8163f", "title": "Editing Factual Knowledge in Language Models"}, {"paperId": "0672f88d5dc762002b515ca4a0a9f101017fea35", "title": "Probing Across Time: What Does RoBERTa Know and When?"}, {"paperId": "209f9bde2dee7cf1677801586562ffe56d435d38", "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"}, {"paperId": "3950df97ea527009a32569cb7016bc3df1383dca", "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering"}, {"paperId": "d331de3b6bebb0f9af1fddf1b730ec057a7026d4", "title": "Relational World Knowledge Representation in Contextual Language Models: A Review"}, {"paperId": "f2885c6a25756cf81aa23b41bc62696a5be5c94d", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall"}, {"paperId": "b2ad1812d0d8f0f1eb19c17e0bccfce4399dd575", "title": "An Empirical Comparison of Instance Attribution Methods for NLP"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "33ce3cd897a3473973f338c154f3fe5c1175643c", "title": "Reasoning Over Virtual Knowledge Bases With Open Predicate Relations"}, {"paperId": "46c585ee9abf76779ea4b863d2da4358efd0d1d3", "title": "Adaptive Semiparametric Language Models"}, {"paperId": "ba233d75aa403092bda0bffc026be7913673ad69", "title": "Mind the Gap: Assessing Temporal Generalization in Neural Language Models"}, {"paperId": "73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19", "title": "Measuring and Improving Consistency in Pretrained Language Models"}, {"paperId": "75f1c7dadb4ed733fb4d3a4cc47b9cbde9ad98cc", "title": "Combining pre-trained language models and structured knowledge"}, {"paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74", "title": "What Makes Good In-Context Examples for GPT-3?"}, {"paperId": "36aff757c91d67ece4f1d0b90550f2bc70e58c16", "title": "Of Non-Linearity and Commutativity in BERT"}, {"paperId": "8bdba45e46471ce23ac2dde24c849623997daaa7", "title": "Analyzing Commonsense Emergence in Few-shot Knowledge Models"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "89181e26874c4ea418937d7a6980d1476d4c0b0b", "title": "Multi-Task Retrieval for Knowledge-Intensive Tasks"}, {"paperId": "25c3b294b9ed2786c4476a25e8b36ebf49fd5b4b", "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning"}, {"paperId": "4a54d58a4b20e4f3af25cea3c188a12082a95e02", "title": "Transformer Feed-Forward Layers Are Key-Value Memories"}, {"paperId": "87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1", "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language"}, {"paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba", "title": "Extracting Training Data from Large Language Models"}, {"paperId": "9d1f9406ed676171d9975e27606c95633ca898b1", "title": "On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT"}, {"paperId": "5270b626feb66c8c363e93ba6608daae93c5003b", "title": "Modifying Memories in Transformer Models"}, {"paperId": "70a1a6a93d7498daac4b76854a9a34179e5f1013", "title": "On the Practical Ability of Recurrent Neural Networks to Recognize Hierarchical Languages"}, {"paperId": "3ee955bfb656e30a337b22a1149b5ecc91a91217", "title": "Language Models are Open Knowledge Graphs"}, {"paperId": "616610e0b0a31ab4bac1c64fd0b65c2572185522", "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT"}, {"paperId": "708dcd8456426cd609c89a86344e0007c04c80bf", "title": "X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models"}, {"paperId": "b4013141cceb937c46ea5f84f8c06f6bf1215106", "title": "PRover: Proof Generation for Interpretable Reasoning over Rules"}, {"paperId": "eedf2748a9a1ba2779cde95fd8bad9c2260d5317", "title": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention"}, {"paperId": "4b322cf280f459deb6d9e2eb2430d1a28141934c", "title": "A Survey of the State of Explainable AI for Natural Language Processing"}, {"paperId": "89cb62dc83c1b1895267bd28639fbf5bb7ed21a4", "title": "Measuring Systematic Generalization in Neural Proof Generation with Transformers"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "7e8457393ff1b40ddd099f195af9d3b14c5a934f", "title": "Language Generation with Multi-hop Reasoning on Commonsense Knowledge Graph"}, {"paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9", "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"paperId": "5fe0a4af3bd1479d5e39fbda2215c86bce54722b", "title": "Generative Language Modeling for Automated Theorem Proving"}, {"paperId": "51ae2c451a1a05293334a509b71c9c9e0377d35c", "title": "Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries"}, {"paperId": "07b95736960731b49b6ce5aa0b29f10bd0586a6d", "title": "On Commonsense Cues in BERT for Solving Commonsense Tasks"}, {"paperId": "bcbac71ac64cd6a6aaae41e37ebe960f508ab741", "title": "Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge"}, {"paperId": "f0d6db2186d8bf2d8530f01de6c6518bb9711392", "title": "Interpretability and Analysis in Neural NLP"}, {"paperId": "8668fd1cb5cab820f8b2a136b2ef4adfad6c4dc1", "title": "Knowledge-Aware Language Model Pretraining"}, {"paperId": "f56fc41eff96afbc64030ce5e97d444706e4997c", "title": "Leap-Of-Thought: Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "0696ad8beb0d765973aa5cdbc6e118889d3583b0", "title": "Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions"}, {"paperId": "b2a839e3ee68e81b863b73ee08c6626c94477fef", "title": "WT5?! Training Text-to-Text Models to Explain their Predictions"}, {"paperId": "3d61a34611c6171f203286119f76ec52f8016580", "title": "Towards Transparent and Explainable Attention Models"}, {"paperId": "016368185723d0ec99aafa4b5927300590d0647f", "title": "Entities as Experts: Sparse Memory Access with Entity Supervision"}, {"paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0", "title": "A Primer in BERTology: What We Know About How BERT Works"}, {"paperId": "a9e6222e71dd101d444b7192b3a0636c71edb0a4", "title": "Differentiable Reasoning over a Virtual Knowledge Base"}, {"paperId": "9e12539d92088001e08b1e903c490127c479de4c", "title": "Transformers as Soft Reasoners over Language"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "845b4941d8c016aa5f8967da2f86d38ef6c18fa3", "title": "A Survey on Knowledge Graphs: Representation, Acquisition, and Applications"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "5e0cffc51e8b64a8f11326f955fa4b4f1803e3be", "title": "oLMpics-On What Language Model Pre-training Captures"}, {"paperId": "a75649771901a4881b44c0ceafa469fcc6e6f968", "title": "How Can We Know What Language Models Know?"}, {"paperId": "01f2b214962997260020279bd1fd1f8f372249d4", "title": "Evaluating Commonsense in Pre-trained Language Models"}, {"paperId": "56cafbac34f2bb3f6a9828cd228ff281b810d6bb", "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"}, {"paperId": "2bd5b4aed18400bf1a1cc866d9b8d931aa047290", "title": "E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT"}, {"paperId": "68c1bf884f0fc0e86641466a1f1fa67e79f16a17", "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "4338266891240f968b3968cf7727fed394bae3ce", "title": "Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations"}, {"paperId": "06a73ad09664435f8b3cd90293f4e05a047cf375", "title": "K-BERT: Enabling Language Representation with Knowledge Graph"}, {"paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09", "title": "Knowledge Enhanced Contextual Word Representations"}, {"paperId": "199ff73d2f728e997f860b62a2322823d3e3d9e8", "title": "Designing and Interpreting Probes with Control Tasks"}, {"paperId": "31184789ef4c3084af930b1e0dede3215b4a9240", "title": "KG-BERT: BERT for Knowledge Graph Completion"}, {"paperId": "f98e135986414cccf29aec593d547c0656e4d82c", "title": "Commonsense Knowledge Mining from Pretrained Models"}, {"paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3", "title": "Language Models as Knowledge Bases?"}, {"paperId": "66117f82def0c69a3b9cc77eb3e2694b0245ca86", "title": "Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning"}, {"paperId": "a550f576ff20b8cce98f3ddad0043d3783fbc9b4", "title": "Abductive Commonsense Reasoning"}, {"paperId": "ce177672b00ddf46e4906157a7e997ca9338b8b9", "title": "Attention is not not Explanation"}, {"paperId": "a0e49f65b6847437f262c59d0d399255101d0b75", "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"}, {"paperId": "335613303ebc5eac98de757ed02a56377d99e03a", "title": "What Does BERT Learn about the Structure of Language?"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "96901acc92d68350443774596fa2b38bc522a0ce", "title": "Barack\u2019s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling"}, {"paperId": "b3564be8b79f25585acb035f3deaf4ae93c26d8f", "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "135112c7ba1762d65f39b1a61777f26ae4dfd8ad", "title": "Is Attention Interpretable?"}, {"paperId": "94e43de5dca9355a60a225565aa26bd1cc065c3e", "title": "Episodic Memory in Lifelong Language Learning"}, {"paperId": "455a8838cde44f288d456d01c76ede95b56dc675", "title": "A Structural Probe for Finding Syntax in Word Representations"}, {"paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "title": "ERNIE: Enhanced Language Representation with Informative Entities"}, {"paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee", "title": "What do you learn from context? Probing for sentence structure in contextualized word representations"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "031e4e43aaffd7a479738dcea69a2d5be7957aa3", "title": "ERNIE: Enhanced Representation through Knowledge Integration"}, {"paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6", "title": "Analysing Mathematical Reasoning Abilities of Neural Models"}, {"paperId": "1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f", "title": "Attention is not Explanation"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "efeab0dcdb4c1cce5e537e57745d84774be99b9a", "title": "Assessing BERT's Syntactic Abilities"}, {"paperId": "c242438dac5aa4d9b13766c14240bb8426690d58", "title": "e-SNLI: Natural Language Inference with Natural Language Explanations"}, {"paperId": "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "title": "Dissecting Contextual Word Embeddings: Architecture and Representation"}, {"paperId": "d2d79513f32c4d09b6255b18514d7ad07ebf43fe", "title": "Explainable artificial intelligence: A survey"}, {"paperId": "28cd5ae35c4b07e03471a0a9e67d871c9e0d20e7", "title": "ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations"}, {"paperId": "7262bc3674c4c063526eaf4d2dcf54eecea7bf77", "title": "Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations"}, {"paperId": "fa025e5d117929361bcf798437957762eb5bb6d4", "title": "Zero-Shot Relation Extraction via Reading Comprehension"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1", "title": "Understanding Black-box Predictions via Influence Functions"}, {"paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"paperId": "0fa5142f908afc94c923ca2adbe14a5673bc76eb", "title": "A Neural Knowledge Language Model"}, {"paperId": "467d5d8fc766e73bfd3e9415f75479823f92c2f7", "title": "Rationalizing Neural Predictions"}, {"paperId": "d516daff247f7157fccde6649ace91d969cd1973", "title": "The mythos of model interpretability"}, {"paperId": "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb", "title": "Improving Neural Machine Translation Models with Monolingual Data"}, {"paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "title": "A large annotated corpus for learning natural language inference"}, {"paperId": "ac3ee98020251797c2b401e1389461df88e52e62", "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39", "title": "Memory Networks"}, {"paperId": "dab7e605237ad4f4fe56dcba2861b8f0a57112be", "title": "Wikidata"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification"}, {"paperId": "9c9d5ebcfcf6e22ba6a31ac5b5858ca48937b665", "title": "Measuring and repairing inconsistency in probabilistic knowledge bases"}, {"paperId": "c1d96c0f421f8b519cafdbb4e499faa1c797ed9b", "title": "Transition-based Dependency Parsing with Rich Non-local Features"}, {"paperId": "743bf822e133cb381112ab364319168359b31d3e", "title": "Expert systems in production planning and scheduling: A state-of-the-art survey"}, {"paperId": "371e676cb72a5a91b1cff11aeba75d628f2dd3cb", "title": "Easy Cases of Probabilistic Satisfiability"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "7f0ea2dd48c88319b6a2e0679aa3b68c9a1de0da", "title": "The Influence Curve and Its Role in Robust Estimation"}, {"paperId": "494aedf82da4755badc1fe74e4d21cf5fc029e9d", "title": "Programs with common sense"}, {"paperId": "df1b7ec5e3feca2174fbd301e4915909c2fcae04", "title": "The logic theory machine-A complex information processing system"}, {"paperId": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "title": "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability"}, {"paperId": "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7", "title": "MENTION MEMORY : INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH ENTITY MENTION ATTENTION"}, {"paperId": "8c23e7627896b7571648896b7d61b6ef76ece982", "title": "Locating and Editing Factual Knowledge in GPT"}, {"paperId": "2af476f7c2a7c040fc9ab7750bf41a84f66aa947", "title": "Knowledge Based Multilingual Language Model"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "a8e2ffdd1021f47f9626cbfd5662068c0f07ee48", "title": "Interactively Generating Explanations for Transformer Language Models"}, {"paperId": "8424082e3bf4792462eb112d7ebcecf5b0dc3613", "title": "Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"}, {"paperId": "d84183f96462f2cf3023f7a6121b3f7ae274d208", "title": "HILDIF: Interactive Debugging of NLI Models Using Influence Functions"}, {"paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1", "title": "Few-shot Learning with Multilingual Language Models"}, {"paperId": null, "title": "2021b. An empirical comparison"}, {"paperId": null, "title": "McMillanMajor, and Shmargaret Shmitchell"}, {"paperId": null, "title": "introduce initial work understanding how LMs-as-KBs can be used in more general settings"}, {"paperId": null, "title": "fashion. Knowing this, existing research focus on taking advantage of the storage capabilities of KBs and integrating this capability into LMs"}, {"paperId": null, "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Atomic: An atlas of machine commonsense for if-then reasoning"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "A survey on dialogue systems"}, {"paperId": null, "title": "exter-nal information, such as information about entities in a sentence"}, {"paperId": null, "title": "What Is a Paraphrase? Computational Linguistics, 39(3):463\u2013472"}, {"paperId": null, "title": "What Is a Para-phrase?"}, {"paperId": "cb3dcb13abd096a33780e6268ee4aaa583b198e8", "title": "A Smorgasbord of Features for Statistical Machine Translation"}, {"paperId": null, "title": "Her Majesty's Stationary Office"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}, {"paperId": null, "title": "retrieval-based model that gathers the appropriate nodes of a KG. Models that implicitly contain knowledge, such as existing pretrained LMs"}, {"paperId": null, "title": "integrating knowledge about general and rare entities, since representing millions of entities within LMs is dif\ufb01cult when LMs have a limited vocabulary"}, {"paperId": null, "title": "Recent work often forgo the potential to deploy these LMs-as-KBs in the real world. To improve existing LM and overcome memory limitations"}, {"paperId": null, "title": "paradigm and the solutions proposed by these models that improve LM performance"}, {"paperId": null, "title": "A mathematical framework for transformer circuits"}]}