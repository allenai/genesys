{"paperId": "3dd6bc488283f1e4cc967d98a6a6c3d7f1a6cf76", "title": "Zamba: A Compact 7B SSM Hybrid Model", "abstract": "In this technical report, we present Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale. Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost. Due to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high-quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay. We open-source the weights and all checkpoints for Zamba, through both phase 1 and annealing phases.", "venue": "arXiv.org", "year": 2024, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Zamba is a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale and pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost."}, "embedding": {"model": "specter_v2", "vector": [0.314362496137619, 0.5188934803009033, -0.9077654480934143, -0.10270863771438599, -0.17447547614574432, -0.14936397969722748, 0.9747709631919861, -0.290380597114563, -0.29461419582366943, -0.23451003432273865, 0.11906580626964569, -0.5239545702934265, 0.34442365169525146, 0.0815974697470665, -0.5312657356262207, -0.04101517051458359, -0.7350242137908936, 0.6848822832107544, 0.24573646485805511, -0.5459490418434143, 0.11150211840867996, -0.9074931144714355, -0.9192039370536804, 0.2918667197227478, 0.34500908851623535, 0.8590251803398132, 0.33180052042007446, 0.7701379060745239, -0.8724109530448914, 0.37420785427093506, 0.5081785321235657, -0.34861528873443604, 0.011946254409849644, -0.04194566234946251, -0.9369635581970215, -0.27311888337135315, 0.6930956840515137, -0.25933969020843506, -0.10347545892000198, 0.6050803065299988, -0.3400140404701233, 0.34076663851737976, 0.3452165424823761, -0.9622862935066223, -0.044032346457242966, 1.1579797267913818, 0.8375325798988342, 0.8112451434135437, -0.5120013356208801, -0.32849130034446716, 1.2855759859085083, -1.0814152956008911, 0.08530344814062119, 1.2740821838378906, 0.48915576934814453, 0.7479841113090515, -0.513762354850769, -0.7774154543876648, 0.6487948298454285, 0.35766229033470154, -0.28334376215934753, -0.529823362827301, -0.0507686473429203, 0.29222479462623596, 1.9022654294967651, -0.028705138713121414, 0.3421522378921509, 1.012985110282898, -0.3656933009624481, 1.691190481185913, 0.11272399872541428, -0.7971189022064209, -0.2547827959060669, 0.07941710948944092, 0.6398710608482361, 0.7921414375305176, -0.34355759620666504, 0.18414291739463806, -1.0037277936935425, 0.017380937933921814, 0.28318390250205994, 0.1424318104982376, 0.2534179091453552, -0.2769022285938263, -0.2941661775112152, 0.6588746309280396, 0.363155722618103, 0.8994174599647522, -0.2395663559436798, 0.7805331349372864, 0.2081582397222519, 0.22113555669784546, -0.07602331787347794, 0.11634068936109543, -0.24322904646396637, 0.11095301061868668, -0.9360374212265015, 0.34712761640548706, -0.3924901783466339, 0.633223831653595, -0.047097790986299515, 0.5062728524208069, -0.678237795829773, 0.23421931266784668, 1.730305790901184, 0.2720423936843872, 0.7795969843864441, -0.7435824275016785, 0.36147892475128174, -0.46352818608283997, -0.020285772159695625, -0.19917204976081848, 0.059887275099754333, 0.0197904035449028, -0.29235947132110596, -1.147565484046936, -0.5261952877044678, 0.5235902667045593, -1.1782058477401733, 0.7186568379402161, -0.14529483020305634, 0.5090250968933105, -0.07760072499513626, 0.48019275069236755, 0.7409844398498535, 0.7089465260505676, 0.35800427198410034, 0.3230268955230713, 0.5806477665901184, -0.9980037212371826, -0.6723958849906921, -1.0864015817642212, 0.4278726577758789, 0.11456044018268585, -0.010606171563267708, -0.20412303507328033, -1.4555823802947998, -0.8609433174133301, -0.8187183141708374, 0.17129075527191162, -0.7765896916389465, -0.4351646304130554, 1.1380316019058228, 0.5075968503952026, -0.8696975111961365, 0.6515186429023743, -0.6069884300231934, -0.1066398024559021, 0.43829146027565, 0.44298067688941956, 0.4008176624774933, -0.14087353646755219, -1.5093728303909302, 0.721700131893158, 0.5034837126731873, -0.5740414261817932, -0.32679256796836853, -0.8203900456428528, -1.1960147619247437, -0.11753743141889572, 0.2171594202518463, -0.3778110146522522, 1.5903159379959106, -0.2706896960735321, -1.4926766157150269, 0.4856760799884796, -0.5021443367004395, -0.16984324157238007, -0.16155439615249634, 0.17114286124706268, -0.6341557502746582, -0.4192620813846588, -0.3429366946220398, 0.7644132375717163, 0.6595528721809387, -0.06901093572378159, -0.2148941308259964, 0.11437048763036728, -0.2671838104724884, -0.3100799322128296, -0.275380939245224, 0.6675841808319092, -0.05685852840542793, -0.1843358725309372, 0.3874290883541107, 0.6107015609741211, 0.12104199826717377, -0.303835928440094, -0.431898295879364, -0.9213098287582397, 0.6685943007469177, -0.01981149986386299, 1.0267893075942993, -0.6027381420135498, -0.6343417763710022, 0.13545610010623932, -0.38826173543930054, 0.36661505699157715, -0.9124288558959961, 0.273760586977005, -0.656807005405426, 0.3806438744068146, -0.260873019695282, -0.9090671539306641, 0.08722640573978424, -0.01944531686604023, -0.4671778976917267, -0.37309062480926514, 0.25758716464042664, 1.1306029558181763, -1.0789518356323242, -0.25281822681427, 0.36766907572746277, 0.14484141767024994, -1.1006790399551392, 1.220797061920166, -0.26926931738853455, -0.07911711931228638, -0.03743594512343407, -0.20462480187416077, -0.026898464187979698, -0.3243638575077057, 0.47233012318611145, -0.24726302921772003, -0.027979424223303795, 1.340847134590149, -0.45631125569343567, 1.6677496433258057, -0.5710487961769104, 0.16112922132015228, 0.13134975731372833, -0.6631669998168945, 0.05477934703230858, 0.5392669439315796, -0.2230193167924881, -0.5808643102645874, 0.2582712173461914, 0.5010417103767395, -0.3947227895259857, 0.36044415831565857, 0.6982441544532776, 0.82753586769104, -0.025425447151064873, 0.17530575394630432, 0.9921932220458984, -0.3799898028373718, 0.32119640707969666, 0.7178378701210022, 0.4779816269874573, 0.3048689067363739, -0.0030371113680303097, -0.35206377506256104, 0.06842506676912308, -0.7336233258247375, -0.11587031930685043, 0.2769475281238556, 0.4701639413833618, 0.7709563970565796, 0.012166709639132023, -0.83879554271698, -0.14850981533527374, -0.18958356976509094, 0.5177325010299683, 1.3832688331604004, -0.04398186877369881, -0.26777538657188416, -0.5434942841529846, -0.4081721007823944, -0.41005271673202515, 0.37964364886283875, -0.3428390324115753, -0.2890543043613434, -0.5900844931602478, -0.8718016147613525, 0.6514170169830322, 0.1071787104010582, 1.0294536352157593, -0.8829696774482727, -0.3527294099330902, -0.21631409227848053, -0.039204344153404236, -0.3107372224330902, -0.5533954501152039, 0.5659722089767456, -0.7815930843353271, -0.2046629637479782, 0.23824766278266907, -0.13412031531333923, 0.0541522279381752, -0.4450112283229828, 0.9733293056488037, -0.3013918399810791, 0.02549000084400177, -0.224233478307724, 0.49084681272506714, -0.356579452753067, -0.7598387002944946, 0.5259396433830261, 0.05176766216754913, -0.1024816632270813, 0.04153285548090935, 0.4283629059791565, 0.009149745106697083, 0.004186717793345451, -0.5633488893508911, 0.4442790448665619, -0.13972702622413635, 0.15697966516017914, 0.3013232350349426, -0.683542788028717, 0.07820182293653488, -1.0102884769439697, 0.3756542503833771, 0.039994433522224426, -0.3020212948322296, 0.13240405917167664, -0.7767040133476257, -0.16180235147476196, -0.12933751940727234, -0.5554846525192261, -0.43077710270881653, -0.9026225805282593, -0.02975795418024063, -0.16253450512886047, -0.23650379478931427, -0.0036659524776041508, 0.43008309602737427, 0.42198288440704346, -0.07470420002937317, 0.6008259654045105, 0.2118404507637024, -0.1335214227437973, 0.7719563841819763, -0.5465738773345947, 0.386058509349823, 0.3327987492084503, 0.0047574155032634735, -0.20278413593769073, -0.2402162104845047, -0.4451238811016083, -0.38885605335235596, -0.3789271116256714, -0.06087121367454529, -0.12617917358875275, 0.14494368433952332, -0.32589611411094666, -0.9209822416305542, 0.23489977419376373, -0.9403659105300903, -0.7233501672744751, 0.2737763822078705, -0.39124950766563416, -0.28905048966407776, -1.0166410207748413, -1.3609225749969482, -0.6208996772766113, -0.7241175770759583, -0.9538116455078125, 0.30424225330352783, 0.007912032306194305, -0.7331447005271912, -0.6266040802001953, 0.09730231016874313, -0.16454707086086273, 1.2266911268234253, -0.6419453620910645, 0.6807683110237122, -0.03837873786687851, -0.25207147002220154, -0.10867280513048172, 0.5670747756958008, 0.35123810172080994, -0.23403754830360413, 0.6220968961715698, -0.5944592356681824, 0.39429354667663574, -0.4336525499820709, -0.5269373059272766, 0.46782371401786804, 0.49675530195236206, 0.4380814731121063, -0.11433616280555725, -0.28607839345932007, 0.35712361335754395, 0.8722687363624573, -0.5140213966369629, 0.029910096898674965, 0.3433588743209839, 0.6722444295883179, 0.10206212103366852, -0.2034049928188324, 0.07740592211484909, 0.39800557494163513, 0.4798009395599365, 0.7305766940116882, 0.1152382493019104, 0.06546171754598618, -0.3399477005004883, 0.7039299607276917, 1.3812601566314697, 0.2451072782278061, 0.15411415696144104, -1.0419375896453857, 0.856506884098053, -1.0208425521850586, -0.7177202105522156, 0.5847902297973633, 0.7162846326828003, 0.7442378401756287, -0.5714382529258728, -0.27651092410087585, 0.4138161540031433, 0.6929950714111328, 0.6464375853538513, -0.02310972288250923, -0.9559640884399414, 0.0015770884929224849, 0.9333212375640869, 0.19436582922935486, 0.798227846622467, -0.20700205862522125, 0.8316578269004822, 15.063268661499023, 1.0299229621887207, -0.14835411310195923, 0.6480370759963989, 0.5783866047859192, -0.21262505650520325, -0.514642596244812, -0.19956515729427338, -1.1707820892333984, 0.2704909145832062, 1.6652859449386597, 0.6578295230865479, 0.9310131072998047, -0.07637562602758408, -0.22466522455215454, 0.19409792125225067, -0.3795880079269409, 0.3839789032936096, 0.45509153604507446, -1.4311163425445557, 0.3622691035270691, -0.10723736882209778, 0.29605400562286377, 0.7693867683410645, 1.1376391649246216, 0.8296884894371033, 0.7441412806510925, -0.43543845415115356, 0.8464715480804443, 0.06353861838579178, 0.7319716811180115, -0.11279330402612686, 0.2556658983230591, 0.36658212542533875, -0.6993260979652405, -0.034690454602241516, -0.019666269421577454, -1.0518755912780762, 0.21300561726093292, -0.30000633001327515, -0.47941267490386963, -0.5986557602882385, 0.10664296895265579, 0.5758101344108582, 0.31780707836151123, 0.23793406784534454, -0.2520424723625183, 0.7893195748329163, -0.10334409028291702, 0.09667608141899109, 0.569516122341156, 0.38122206926345825, 0.09081173688173294, -0.0990447998046875, 0.07736492902040482, -0.2104683816432953, -0.1680016964673996, 0.5849385261535645, -0.28852900862693787, -0.4377857446670532, -0.20749661326408386, -0.31258824467658997, -0.20389173924922943, 0.9013730883598328, 0.6516962051391602, 0.02673386223614216, -0.1705627739429474, 0.32383406162261963, 0.4128836691379547, -0.08155222982168198, -0.6655340194702148, -0.05570564046502113, 0.5246543884277344, -0.3759012520313263, 0.11968878656625748, 0.7225049138069153, 0.02379838190972805, -0.3341159224510193, -0.9980399012565613, -0.3242749571800232, 0.4345405399799347, -0.7887288331985474, -0.38980963826179504, 0.8272944688796997, -0.2937566339969635, 0.19155126810073853, -0.11092893034219742, -0.7231294512748718, -0.3257184326648712, 0.4002912640571594, -1.5943477153778076, -0.8642430305480957, 0.3691958785057068, -0.05968846380710602, -0.35818976163864136, 0.28452160954475403, 1.1903600692749023, 0.14268645644187927, -0.3725391626358032, -0.1772487312555313, -0.12406020611524582, -0.13838689029216766, -0.35113728046417236, -0.8274291157722473, 0.9936724901199341, 0.14492987096309662, 0.23825232684612274, 0.24695177376270294, -0.19965583086013794, 0.41073620319366455, -0.6370376348495483, -0.24926342070102692, 0.7409496307373047, -1.2778466939926147, -0.3368043005466461, -0.7219042181968689, -0.6589851379394531, 0.6455900073051453, 0.7458908557891846, -0.06487461179494858, 0.171219602227211, -0.02095506526529789, -0.715800404548645, -0.17494405806064606, -0.749482274055481, -0.10551208257675171, 0.4938051998615265, -0.6394093036651611, -0.7484084963798523, -0.49701815843582153, 0.4187479615211487, -0.9023870825767517, -0.47491395473480225, -0.13559558987617493, 0.3583148419857025, -0.2109142243862152, 0.9621114134788513, -0.6554569602012634, 0.4270537793636322, 1.0777215957641602, -0.08787550777196884, -1.1661008596420288, -0.06810267269611359, -0.7957065105438232, -0.4056437611579895, 0.1947142481803894, 0.8329770565032959, -0.5432330369949341, 0.3140791654586792, 0.5832828283309937, 0.128919318318367, -0.5581792593002319, -0.7980044484138489, -0.5072708129882812, -0.22364898025989532, -0.5408707857131958, 0.7122188806533813, 0.19741509854793549, 0.1838076114654541, 0.08851539343595505, 0.559022843837738, 0.1839449405670166, -0.2752748727798462, -0.6854841709136963, 0.1852288842201233, -0.018329478800296783, -0.11428000032901764, -0.7883602976799011, -0.2759876847267151, -1.1158771514892578, -0.06406638026237488, -1.1380707025527954, 0.035199522972106934, -0.7470605969429016, -0.32114189863204956, -0.19766591489315033, -0.82154780626297, 0.34360530972480774, 0.1518373340368271, -0.01599758118391037, -0.5874113440513611, -0.9034886956214905, -0.642120897769928, 0.6199555993080139, 0.5561138391494751, -1.009739875793457, 0.2669455111026764, -0.03902381286025047, 0.4769937992095947, 0.1683325469493866, 0.5816196799278259, -0.47793832421302795, -0.6978689432144165, -1.3527048826217651, 0.2001236081123352, -0.1972665637731552, -0.17314210534095764, -0.5118887424468994, 0.28121525049209595, 0.2168617844581604, -0.5521125197410583, 0.10105343908071518, 0.5728746652603149, -0.8670475482940674, -0.28609201312065125, 0.5233021974563599, -0.6996205449104309, 0.27341482043266296, 0.09952881932258606, -0.8916621804237366, -0.24158993363380432, 0.4742310345172882, 0.12188836932182312, -0.9499168992042542, -0.49457883834838867, 0.5383378267288208, -0.8882583975791931, -0.08316502720117569, -0.40745753049850464, -0.3467217683792114, -0.9326424598693848, -0.2450166493654251, 0.07137607783079147, 0.2926671802997589, -0.527786910533905, 1.0179351568222046, 0.28405505418777466, -1.2285187244415283, 0.2973097264766693, 0.5171495676040649, -0.041672252118587494, 0.014028768055140972, 0.2643076777458191, 0.5930343866348267, -0.25703221559524536, 0.6636936664581299, -0.14262349903583527, 0.12406808882951736, -0.6664779782295227, -0.1458297222852707, 0.5615639090538025, -0.25191476941108704, 0.271124005317688, 1.0084699392318726, -0.6662008166313171, -1.1752439737319946, 0.11115091294050217, -1.6058359146118164, -0.49079999327659607, -0.6819773316383362, 0.4875373840332031, 0.35353294014930725, 0.026202933862805367, -0.2237556278705597, -0.4354361295700073, -0.003925166558474302, 0.13887056708335876, -0.6740819811820984, 0.7172895073890686, 0.04564812406897545, -0.3633979856967926, 0.8915518522262573, 0.49417683482170105, -1.0173832178115845, -0.5084693431854248, -0.547722578048706, -0.39824607968330383, 0.11250347644090652, 0.4428745210170746, -0.5481014251708984, -0.953804075717926, 1.0211814641952515, 0.41425591707229614, 0.5173271298408508, -0.14460833370685577, -0.2983407974243164, 0.41791096329689026, 0.49414166808128357, 0.07041031122207642, -0.37876978516578674, -0.5942302346229553, 1.2269443273544312, 0.7981863021850586, -1.0351977348327637, -0.15885832905769348, -0.008951966650784016, -0.4370437562465668, 0.4162479341030121, 0.36408719420433044, -0.02228936180472374, 0.7189765572547913, -0.5191239714622498, 0.12993010878562927, 0.3167744576931, -1.2683203220367432, -0.3640531599521637, 0.552216649055481, 1.1058350801467896, 0.5666073560714722, 0.36121416091918945, 0.17629969120025635, 0.8180583715438843, 0.3176262378692627, -0.3292676508426666, 0.1577148139476776, 0.639665961265564, -0.16798698902130127, -0.3324289917945862, 0.20041079819202423, 0.5950794816017151, -0.6754059791564941, -0.7439574599266052, 0.0509403757750988, 0.7065705060958862, 0.3207728862762451, 0.7777671813964844, 1.156437873840332, 0.21611657738685608, 0.1467939168214798, 0.3331030607223511, 0.8516551852226257, -0.4160221517086029, -0.4723205864429474, -0.484523206949234, -0.4389333128929138, -0.3139120638370514, -0.27655264735221863, -0.43291789293289185, -0.5533012747764587, -0.4988471567630768, -0.0768277496099472, 0.33763769268989563, 0.6326301693916321, 1.0771260261535645, 0.47984907031059265, 0.3939124047756195, -0.44121673703193665, -0.5758970379829407, -0.1484612077474594, -1.261639952659607, 0.16670598089694977, -0.331424355506897, -0.11299186944961548, -0.3445519506931305, 0.10667813569307327, -0.46596142649650574]}, "authors": [{"authorId": "2282541297", "name": "Paolo Glorioso"}, {"authorId": "2282542187", "name": "Quentin Anthony"}, {"authorId": "2282542302", "name": "Yury Tokpanov"}, {"authorId": "2303399328", "name": "James Whittington"}, {"authorId": "2303399767", "name": "Jonathan Pilault"}, {"authorId": "2303800652", "name": "Adam Ibrahim"}, {"authorId": "150045277", "name": "Beren Millidge"}], "references": [{"paperId": "abdceff7d7983cdede9a5aabe6a476d4c72e41a3", "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"}, {"paperId": "c9ff9fbbe21985b35d6a070b67f22a0e065c4328", "title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars"}, {"paperId": "49873ee415619efd9e1e4c16f73ee066ff008c1f", "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies"}, {"paperId": "cbaf689fd9ea9bc939510019d90535d6249b3367", "title": "Jamba: A Hybrid Transformer-Mamba Language Model"}, {"paperId": "05c1dc502ed51162580ccd320d5668d2fec94a7a", "title": "Mechanistic Design and Scaling of Hybrid Architectures"}, {"paperId": "d0075dd5603c9d477edf0a41f59ab6dcf0e91976", "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models"}, {"paperId": "b842b83a7ff5dff8e3b83915d8c15423b6085728", "title": "Gemma: Open Models Based on Gemini Research and Technology"}, {"paperId": "d53fe76bd2795a19ddf52d012917782f6f6f2c1e", "title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models"}, {"paperId": "9da427202cc48370fd66359f5d72ff5ff3bc8b57", "title": "Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks"}, {"paperId": "57a6c75ebb987ea29a1f904de23f72451e095032", "title": "Is Mamba Capable of In-Context Learning?"}, {"paperId": "ac45bbf9940512d9d686cf8cd3a95969bc313570", "title": "OLMo: Accelerating the Science of Language Models"}, {"paperId": "3169a2478154e26fd7f63fdf43cf3a24f1007962", "title": "BlackMamba: Mixture of Experts for State-Space Models"}, {"paperId": "189fde3f4dfa105bb51472a8945618f395919560", "title": "Repeat After Me: Transformers are Better than State Space Models at Copying"}, {"paperId": "2905dc5ad70b462f4f5543df3047dffadb5c0e4e", "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"}, {"paperId": "9266dc3c65334e36a12fef7e4b231091d346b8a4", "title": "LLM360: Towards Fully Transparent Open-Source LLMs"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "e26888285436bc7998e5c95102a9beb60144be5e", "title": "Textbooks Are All You Need II: phi-1.5 technical report"}, {"paperId": "11cf88dce827bd67cbfa60400306318022e736d5", "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification"}, {"paperId": "193955704f66923ac20a664bd184ed4663b2bdf9", "title": "Continual Pre-Training of Large Language Models: How to (re)warm your model?"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "61e608a70faefb8faaf92124c4a8a7a8bf1fe099", "title": "Scaling MLPs: A Tale of Inductive Bias"}, {"paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b", "title": "Textbooks Are All You Need"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "9b4f7c97c0b83a80c32bc0b93595cbcfb4ecb16d", "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "1ee074a4af366da2e58160a83b97783289b64812", "title": "AlphaFold2 and its applications in the fields of biology and medicine"}, {"paperId": "fd80177fc93c87058840b3868713ee002bd9ab98", "title": "Downstream Datasets Make Surprisingly Good Pretraining Corpora"}, {"paperId": "34a9b0c7ddc9012b8657c4bc1de5f14d45e646b4", "title": "Hybrid predictive coding: Inferring, fast and slow"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "7d1e512888a2fa4e838c12a02ae7fce867d322a8", "title": "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale"}, {"paperId": "06e40b7a703079c280f8f0886ac2bd984cd318ce", "title": "Relating transformers to models and neural representations of the hippocampal formation"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "2c95d8c6e040ebb9d5ed1fa60039fbee210416ff", "title": "Going in circles is the way forward: the role of recurrence in visual inference"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "f509100a8b6ec4036798fe857ea7ca75572b8278", "title": "The Tolman-Eichenbaum Machine: Unifying Space and Relational Memory through Generalization in the Hippocampal Formation"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "5fe0ea8436f7e258a188bd9ffb6aebcd216f1782", "title": "Curriculum Learning for Natural Answer Generation"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "8de174ab5419b9d3127695405efd079808e956e8", "title": "Curriculum learning"}, {"paperId": "d5ddb30bf421bdfdf728b636993dc48b1e879176", "title": "Learning and development in neural networks: the importance of starting small"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": null, "title": "OLMo 1.7\u20137B: A 24 point improvement on MMLU"}, {"paperId": null, "title": "Reproduction of Mamba-370M by Zyphra"}, {"paperId": null, "title": "Eagle and finch"}, {"paperId": null, "title": "peS2o (Pretraining Efficiently on S2ORC) Dataset"}, {"paperId": null, "title": "Gemini: a family of highly capable multimodal models"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "Transformer Engine: A library for accelerating Transformer models on NVIDIA GPUs"}, {"paperId": null, "title": "modeling with selective state spaces"}]}