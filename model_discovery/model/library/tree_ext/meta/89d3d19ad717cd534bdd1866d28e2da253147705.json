{"paperId": "89d3d19ad717cd534bdd1866d28e2da253147705", "title": "EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism", "abstract": "We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared to standard LLM training, as well as outstanding inference speedup without compromising output quality. To facilitate further research and adoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM.", "venue": "arXiv.org", "year": 2023, "citationCount": 9, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism, and achieves great training efficiency with negligible computational overhead compared to standard LLM training, as well as outstanding inference speedup without compromising output quality."}, "embedding": {"model": "specter_v2", "vector": [-0.014814506284892559, 0.29213568568229675, -0.5504481196403503, 0.023143626749515533, -0.13914379477500916, -0.013736114837229252, 0.5429179668426514, 0.00654204934835434, -0.8793208599090576, -0.754528820514679, 0.41761186718940735, -0.4859853982925415, 0.5911319851875305, 0.26498982310295105, -0.30028030276298523, 0.5760911107063293, -1.240509033203125, 0.20487438142299652, 0.026000333949923515, -0.06601912528276443, -0.19712160527706146, -0.15491870045661926, -1.2117862701416016, 0.2629183232784271, 0.23837289214134216, 0.47833141684532166, 0.011915568262338638, 1.3219342231750488, -0.542879581451416, 0.5076045989990234, 0.4691447913646698, -0.06104035675525665, 0.15484949946403503, 0.12697972357273102, -0.18826352059841156, -0.11997191607952118, 0.02470690943300724, -0.6874609589576721, -0.326671302318573, 0.4440669119358063, -0.3219156265258789, 0.4965243935585022, 0.19369187951087952, -0.7823539972305298, -0.028616968542337418, 0.8041286468505859, 0.14902785420417786, 0.8531208038330078, -0.6754944324493408, -0.5099004507064819, 1.037280797958374, -2.0400912761688232, 0.32836753129959106, 1.467147946357727, 0.839926540851593, 0.04309588670730591, -0.21688641607761383, -0.3796862065792084, 0.851893961429596, -0.14246055483818054, -0.9813777804374695, -0.7852739691734314, -0.39928901195526123, -0.08204498887062073, 2.1424055099487305, -0.3305685520172119, 0.16708345711231232, 0.5079289674758911, -0.11236365139484406, 1.4645365476608276, -0.27429598569869995, -0.7463957071304321, -0.015342566184699535, 0.015541472472250462, 0.3516996204853058, 0.7902350425720215, -0.48124873638153076, 0.4375058710575104, -0.8483410477638245, -0.22558338940143585, 0.15248925983905792, 0.0764276534318924, 0.44771334528923035, 0.020218612626194954, 0.031524550169706345, 0.7192875146865845, 0.4001060426235199, 0.5133668184280396, -0.3116428852081299, 0.7029326558113098, 0.5907990336418152, 0.17716263234615326, 0.5114057064056396, -0.24254044890403748, -0.46086519956588745, 0.16697390377521515, -1.224666714668274, 0.1925404965877533, 0.17252150177955627, 0.700778603553772, -0.4805186092853546, 0.2760452926158905, -0.851341187953949, 0.04179709404706955, 1.33292555809021, 0.12538069486618042, 0.16773256659507751, -0.6379234194755554, 0.22697225213050842, -0.8259944319725037, -0.13236072659492493, -0.3260210156440735, -0.38748934864997864, -0.6600059270858765, -0.8809444904327393, -0.9074833989143372, -0.7095365524291992, -0.0450332909822464, -0.737492024898529, 0.5884811878204346, 0.15797482430934906, 0.5333042740821838, 0.32813480496406555, 0.43083637952804565, 0.5907846689224243, 0.8754920363426208, 0.3647983968257904, 0.09373130649328232, 1.1407712697982788, -1.5279079675674438, -0.1149689331650734, -1.1887725591659546, 1.1082221269607544, 0.017803944647312164, -0.02132362127304077, -0.4084220230579376, -1.2456307411193848, -0.9437091946601868, -0.5141997933387756, -0.20932503044605255, -0.6002684235572815, 0.44532740116119385, 1.1543339490890503, 0.0374932661652565, -1.0130934715270996, 0.6763942837715149, -0.5353856682777405, 0.20136074721813202, 0.2855193018913269, 0.19992397725582123, 0.3333968222141266, -0.10979535430669785, -0.9750282764434814, 0.016922762617468834, 0.21531105041503906, -0.16530826687812805, -0.2640795409679413, -0.6637259721755981, -0.8493326902389526, 0.029308130964636803, 0.0795435830950737, -0.6462256908416748, 1.6520849466323853, -0.05358235538005829, -1.3421902656555176, 0.6111137270927429, -0.41845569014549255, -0.02785780094563961, 0.284062922000885, -0.07678361982107162, -0.4705404043197632, -0.4019629657268524, -0.4962427616119385, 0.7743099927902222, 0.6676787734031677, 0.1346067190170288, -0.4026603400707245, -0.14392727613449097, -0.46437129378318787, 0.000403803976951167, -0.2231581211090088, 1.0167303085327148, -0.7367182970046997, 0.04731806367635727, 0.2508995234966278, 0.7095538377761841, -0.6471143364906311, -0.22780615091323853, -0.368013471364975, -1.011214256286621, 0.7737070322036743, -0.24139101803302765, 0.9818243980407715, -1.2217860221862793, -0.7843520641326904, -0.011164110153913498, 0.3692038059234619, 0.03650560602545738, -0.7267225980758667, 0.36441296339035034, -0.19868820905685425, 0.2593432366847992, 0.20497483015060425, -1.4904282093048096, 0.12483770400285721, -0.5183423757553101, -0.7883079648017883, -0.3227802813053131, 0.14773628115653992, 0.9789621829986572, -0.9470754265785217, 0.04711097478866577, -0.36123523116111755, 0.6031686663627625, -1.259359359741211, 1.2619993686676025, -0.5103110074996948, 0.19810432195663452, -0.12865392863750458, -0.3913862705230713, 0.11780209839344025, -0.3327575623989105, 0.5587159395217896, -0.25394406914711, -0.33407315611839294, 0.33965611457824707, -0.6603851914405823, 1.0157625675201416, -0.6050601005554199, 0.3187868595123291, 0.1915328949689865, -0.3961217999458313, -0.08894585818052292, 0.17755481600761414, -0.6427910923957825, -0.4315243363380432, 0.4781595766544342, 0.7888083457946777, -0.6841346025466919, 0.6855381727218628, 1.1405893564224243, 1.0271387100219727, -0.31342706084251404, 0.3277563452720642, 0.520972490310669, 0.26387161016464233, 0.4853464663028717, 0.6097787022590637, 0.47289204597473145, 0.713405191898346, 0.19777168333530426, 0.10547575354576111, 0.20774804055690765, -1.0098587274551392, 0.10198763757944107, 0.6705483198165894, 0.7157395482063293, 0.2797236442565918, 0.3642473518848419, -0.630901575088501, -0.37835371494293213, 0.18782424926757812, 0.5397365689277649, 1.595913290977478, -0.5614405274391174, -0.28879690170288086, -0.7820596694946289, -0.36004963517189026, -0.31457796692848206, 0.03018677979707718, 0.056988731026649475, 0.017076866701245308, -0.6275820136070251, -0.8905413746833801, 0.954321563243866, 0.43682748079299927, 0.6175869107246399, -0.478451132774353, -0.49684199690818787, -0.4624185860157013, 0.5756057500839233, -1.1058659553527832, -0.4566343128681183, 0.6503557562828064, -1.0775853395462036, 0.29094865918159485, 0.1755986213684082, -0.08314872533082962, 0.43113914132118225, -0.907741367816925, 1.041074514389038, -0.26362788677215576, -0.4630911946296692, 0.004185429774224758, 0.5934677124023438, -0.3372001051902771, -0.9511362910270691, 0.35837045311927795, 0.25749585032463074, -0.38979247212409973, 0.33866360783576965, 0.5281667709350586, 0.1990552693605423, -0.4718573987483978, -0.17605705559253693, 0.4952639639377594, 0.027895962819457054, -0.47165337204933167, 0.8335967659950256, -0.24546192586421967, -0.0958409458398819, -1.266588568687439, 0.6860867738723755, 0.004081325139850378, -0.8013990521430969, -0.03644238039851189, -0.6936987638473511, -0.07681569457054138, 1.0247325897216797, -0.5733010172843933, -0.3225257396697998, -1.0899938344955444, -0.2588809132575989, -0.6338518857955933, -0.3158959150314331, -0.05597049370408058, 0.7922788858413696, 0.46666744351387024, 0.08397319912910461, 0.306964635848999, -0.024542225524783134, -0.5486970543861389, 0.8032504916191101, -0.604604959487915, 0.4604160785675049, 0.09010857343673706, 0.15187998116016388, -0.2857934236526489, -0.12923526763916016, -0.8797147870063782, -0.39293497800827026, -0.5331242084503174, -0.30604851245880127, -0.0981818437576294, 0.08093626797199249, -0.9060739278793335, -0.735723614692688, 0.0304703526198864, -1.283270239830017, -0.42874160408973694, 0.23531515896320343, -0.055551979690790176, -0.019274989143013954, -1.1907339096069336, -1.8006632328033447, -0.6916069984436035, -1.0630625486373901, -1.050403356552124, 0.8319221138954163, 0.056944407522678375, -0.3949362337589264, -0.8303281664848328, -0.14876829087734222, -0.7293274998664856, 0.8547407984733582, -0.8844366073608398, 0.9234334826469421, -0.10946634411811829, -0.3107599914073944, 0.0274012703448534, 0.23626436293125153, 0.2843756675720215, -1.108412742614746, 0.4174826741218567, -0.777612566947937, 0.2570173144340515, -0.7792806029319763, -0.2670228183269501, 0.19748640060424805, 0.38789424300193787, 0.5818638205528259, 0.19700837135314941, -0.7537094950675964, 0.8021663427352905, 1.351208209991455, -0.9428575038909912, -0.106130450963974, -0.16456201672554016, 0.8959827423095703, -0.2168312668800354, -0.3803567588329315, 0.8366628289222717, 0.16305294632911682, 0.14836591482162476, 0.09117500483989716, -0.3766651153564453, -0.07130265235900879, -0.7073084712028503, 0.7116997241973877, 2.1329550743103027, 0.6271989345550537, 0.024875309318304062, -0.8035816550254822, 0.5036156177520752, -1.0034080743789673, -0.41775330901145935, 0.40703320503234863, 0.9119954109191895, 0.2078239470720291, -0.24776768684387207, -0.3196902573108673, -0.6191818118095398, 0.2864157557487488, 0.5247337818145752, -0.4064023792743683, -1.2444742918014526, 0.3224319517612457, 0.4870249927043915, 0.24266070127487183, 0.5352192521095276, -0.09016619622707367, 0.8537479639053345, 14.414060592651367, 0.794666051864624, 0.27832651138305664, 0.672890305519104, 1.0731403827667236, 0.35799747705459595, -0.4644038677215576, 0.08306652307510376, -1.8146240711212158, -0.4552867114543915, 1.8246159553527832, 0.624439537525177, 0.5895068049430847, 0.2744363844394684, 0.27104175090789795, 0.2596569359302521, -0.6484326720237732, 0.44294342398643494, 0.2925918400287628, -1.348263144493103, 0.1588459312915802, 0.08028898388147354, 0.3886848986148834, 1.0341649055480957, 0.5714997053146362, 1.2980128526687622, 0.6909181475639343, -0.27669477462768555, 0.48381030559539795, 0.3066079020500183, 0.9366281032562256, -0.31237173080444336, 0.23211917281150818, 0.9083326458930969, -1.0984652042388916, -0.07924008369445801, -0.39777863025665283, -1.2057441473007202, 0.15835851430892944, 0.3110758662223816, -0.5733175277709961, -0.769440233707428, -0.18752671778202057, 0.7141587138175964, 0.221910759806633, 0.15752536058425903, 0.05295438691973686, 0.5702268481254578, -0.5893329977989197, 0.229702889919281, 0.30643075704574585, 0.2726472318172455, 0.10975407063961029, -0.04390883818268776, 0.16560977697372437, -0.16144461929798126, 0.13932526111602783, 0.7981016635894775, -0.7992200255393982, -0.044415466487407684, -0.3020131289958954, -0.2897424101829529, -0.09374556690454483, 0.7436982989311218, 0.22360333800315857, 0.11283024400472641, -0.5102723836898804, 0.7377249002456665, 1.0075266361236572, 0.25012096762657166, -0.2760257422924042, 0.1908121407032013, 0.2088964432477951, -0.8119307160377502, 0.3154429495334625, 0.4695766270160675, 0.3205834925174713, -0.5290312170982361, -0.7459580898284912, -0.4202874004840851, 0.26307207345962524, -0.6824286580085754, -0.4435468912124634, 0.6858386993408203, -0.31851574778556824, -0.1901448369026184, 0.1217193678021431, -0.7766231894493103, -0.17383915185928345, 0.41682568192481995, -1.094773292541504, -0.390733540058136, 0.8402393460273743, -0.5343924164772034, -0.26206356287002563, 0.26141130924224854, 1.454856514930725, 0.3643921911716461, -0.5398303270339966, 0.13316510617733002, 0.5070038437843323, 0.024546608328819275, -0.8093957304954529, -0.34055909514427185, 1.3482109308242798, 0.6240799427032471, 0.12558245658874512, 0.2073683738708496, -0.4091956913471222, 0.287629097700119, -0.998082160949707, -0.29853466153144836, 0.9992871880531311, -0.7497353553771973, -0.4412626028060913, -1.0957884788513184, -0.2596965432167053, 0.6081339120864868, 0.23841990530490875, 0.08749512583017349, 0.2971985638141632, 0.3102532923221588, -0.6345577239990234, 0.021290969103574753, -0.14931665360927582, 0.3266955614089966, 0.7051035761833191, -0.7390080094337463, 0.13639158010482788, -0.06259604543447495, 0.7004460692405701, -1.100181221961975, -0.7256672978401184, -0.1500285416841507, -0.003317694179713726, -0.09540709108114243, 0.9535101056098938, -0.388370543718338, 0.7352492809295654, 0.967967689037323, -0.07247574627399445, -0.6463711261749268, 0.3525095582008362, -0.8440784811973572, -0.6337240934371948, 0.0583561547100544, 0.8443411588668823, -0.07666617631912231, 0.18648988008499146, 0.39429739117622375, 0.3706726133823395, -0.7045610547065735, -0.19238893687725067, -0.18098394572734833, 0.1257767677307129, -0.9753294587135315, 0.13555343449115753, -0.40959855914115906, -0.48081326484680176, 0.38952675461769104, 0.3602803945541382, 0.7679935693740845, -0.05799233168363571, -0.43717512488365173, 0.6488109827041626, -0.06043640896677971, -0.5766443014144897, -0.49698585271835327, -0.2697170674800873, -1.6387840509414673, 0.09726298600435257, -1.4111888408660889, -0.07638134062290192, -0.7775793671607971, -0.2105202078819275, 0.026884419843554497, -0.09243664890527725, -0.0025291594211012125, 0.4155263602733612, -0.2090921700000763, -0.5414571762084961, -0.6620005369186401, -0.9070872068405151, 0.9373849034309387, 0.6662108302116394, -0.24353350698947906, 0.07350083440542221, 0.04348062723875046, 0.5429782867431641, 0.5026143193244934, 0.22604221105575562, -0.6087484955787659, -0.8747795224189758, -1.3051713705062866, 0.5773401856422424, 0.06639185547828674, -0.1559504270553589, -0.5010640621185303, 0.5788230895996094, 0.6415649056434631, -0.09369975328445435, -0.2502915859222412, 0.06634014844894409, -0.6826533675193787, -0.4869227409362793, 0.32982397079467773, -0.5331724882125854, 0.3162294328212738, 0.6737635135650635, -0.6127161383628845, -0.13986538350582123, 0.5529804229736328, -0.37891194224357605, -0.8075207471847534, -1.1599034070968628, 0.6792136430740356, -0.6442134976387024, 0.3874766230583191, -0.5857152342796326, -0.05494500324130058, -0.7640717625617981, -0.03092881105840206, 0.010588649660348892, 0.2490072250366211, -0.3628689646720886, 0.9587591886520386, 0.3984120786190033, -0.8825209736824036, 0.06145792081952095, 0.6263787746429443, -0.3798779249191284, 0.12976303696632385, 0.788151204586029, 0.7148578763008118, -0.4191204011440277, 0.50555819272995, 0.7305256128311157, 0.19653964042663574, -1.0037190914154053, -0.12293197214603424, 0.6659623384475708, -0.877229630947113, -0.5604326725006104, 1.2769708633422852, -0.41567283868789673, -1.6246349811553955, 0.12606869637966156, -0.9888318777084351, -0.3664538562297821, -0.7836184501647949, 0.6661537885665894, 0.03743739798665047, -0.053912203758955, -0.028699537739157677, -0.6846099495887756, 0.033010803163051605, -0.07538945972919464, -0.3741436004638672, 0.6548685431480408, 0.03651415929198265, -0.590166449546814, 0.5857804417610168, 1.178863525390625, -0.3543022572994232, -0.7201727032661438, -0.7221651077270508, -0.2884942889213562, 0.260103702545166, 0.6607673168182373, -0.1500094085931778, -0.4803982973098755, 0.7450790405273438, 0.4479213356971741, -0.2547573745250702, 0.3644900619983673, -0.22192086279392242, 0.493908166885376, 0.7592468857765198, 0.1820412427186966, -0.9583943486213684, -0.8506779670715332, 1.3768454790115356, 0.7166186571121216, -0.7841596603393555, 0.29438695311546326, 0.03405490517616272, -0.4781077802181244, 0.8191242814064026, 0.2276296615600586, -0.06423725932836533, 0.9876266121864319, 0.29712966084480286, -0.025048820301890373, 0.1367364525794983, -1.2519348859786987, -0.0748533234000206, 0.8248960971832275, 0.3996548354625702, 0.7709737420082092, 0.20193080604076385, 0.2676495611667633, 0.8058427572250366, 0.28136536478996277, 0.1116086095571518, 0.03127773106098175, 0.49339401721954346, -0.0876278504729271, -0.0030182430054992437, 0.17447131872177124, 0.6378307342529297, -0.7906271815299988, -1.2279314994812012, 0.45620018243789673, 0.4528019428253174, 0.15776127576828003, 0.6084979176521301, 1.0762171745300293, 0.27301913499832153, 0.19165900349617004, 0.11040239036083221, 0.538790225982666, -0.5039765238761902, -0.3254036605358124, -0.10161928832530975, -0.4246065616607666, -0.09296537190675735, 0.248252272605896, -0.07391831278800964, -1.0133510828018188, -0.5058350563049316, 0.49243414402008057, -0.01125065516680479, 0.518131673336029, 1.2670270204544067, 0.8797587752342224, 0.6856082081794739, -0.17081104218959808, -0.24481211602687836, -0.6990266442298889, -0.5571377277374268, -0.16983947157859802, -0.6778169274330139, -0.33091193437576294, 0.5323099493980408, -0.15148629248142242, -0.46144309639930725]}, "authors": [{"authorId": "2272979112", "name": "Yanxi Chen"}, {"authorId": "2211993531", "name": "Xuchen Pan"}, {"authorId": "2237607166", "name": "Yaliang Li"}, {"authorId": "2266389996", "name": "Bolin Ding"}, {"authorId": "2145786974", "name": "Jingren Zhou"}], "references": [{"paperId": "27b1b41ac386814af6f7bfd4646781f3f32b8a38", "title": "Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy"}, {"paperId": "dc888056d185572671060c1430deb9978fa4d917", "title": "Apparate: Rethinking Early Exits to Tame Latency-Throughput Tensions in ML Serving"}, {"paperId": "73fc756ea53f70d899f0154fe00ccb123d3bcd86", "title": "Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment"}, {"paperId": "7a2cc2fdb0f0df3752bb4668224489535bdd9a06", "title": "Learning to Skip for Language Modeling"}, {"paperId": "81b7d8a9311c1f2d20fd1bc1d0d9835ce19d4b24", "title": "DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models"}, {"paperId": null, "title": "Accelerating LLaMA Inference by Enabling Intermediate Layer Decoding via Instruction Tuning with LITE"}, {"paperId": "564855d475ed9197dd7516594557ff886ff623e5", "title": "Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding"}, {"paperId": "231c1fee5f0e48722f025961bf3016ca5e4c6072", "title": "DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers"}, {"paperId": "788477fadc464576ae1b059785245c9581e4e13f", "title": "Data-Juicer: A One-Stop Data Processing System for Large Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "ce9435c82dc9b576f2037aa2f4357a520be9b2aa", "title": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference"}, {"paperId": "697473129688fc261a5ffe529ece2f85320ce5a2", "title": "ScaleFL: Resource-Adaptive Federated Learning with Heterogeneous Clients"}, {"paperId": "ce913026f693101e54d3ab9152e107034d81fce1", "title": "Holistic Evaluation of Language Models"}, {"paperId": "3556722b4703a21abafd2f9388743202943f4503", "title": "Accelerating Transformer Inference for Translation via Parallel Decoding"}, {"paperId": "5495046b456059ad820ce26157ebe102147011b9", "title": "The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "dcd8dff20f4f490acd5f94001d34c774167f053e", "title": "Jump to Conclusions: Short-Cutting Transformers with Linear Transformations"}, {"paperId": "c6ff0f377d4a8700aff52312e240a32ab12f3316", "title": "SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "0a6906bd6f026d3da3031c641ed03081bd0b574e", "title": "Full Stack Optimization of Transformer Inference: a Survey"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "b7d12aec8a0152ec4921dfa43ab525a63b334385", "title": "Speculative Decoding with Big Little Decoder"}, {"paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644", "title": "Fast Inference from Transformers via Speculative Decoding"}, {"paperId": "4e9625d1f3d591a2d4ea7dc7ff0f683fde6d682b", "title": "PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "56999888ce25ce3422d687f04e4fe196eae21b6c", "title": "Semi-HFL: semi-supervised federated learning for heterogeneous devices"}, {"paperId": "2ef60a4ea4ea53056be811ff55679eb59fb4b586", "title": "Confident Adaptive Language Modeling"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "6b117a8dcaa161562b0a69afbb9811e11afb5b3e", "title": "Decentralized Training of Foundation Models in Heterogeneous Environments"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "802a5d24c78f713e282b003d99b4afd924bd7568", "title": "A Survey on Dynamic Neural Networks for Natural Language Processing"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "10f3ca78e194552427ebe9173b19d1b910469e27", "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"}, {"paperId": "13f07349ae44483e7f1177ab3939a5f0ee82503e", "title": "Adaptive Inference through Early-Exit Networks: Design, Challenges and Directions"}, {"paperId": "9c053552dfa6184f7dc56d620bcb1e8f22c729a3", "title": "Accelerating BERT Inference for Sequence Labeling via Early-Exit"}, {"paperId": "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d", "title": "Consistent Accelerated Inference via Confident Adaptive Transformers"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "837ac4ed6825502f0460caec45e12e734c85b113", "title": "Dynamic Neural Networks: A Survey"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "21a4cd35f19cfe8df1065b066b16edd048d2535d", "title": "DAPPLE: a pipelined data parallel approach for training large models"}, {"paperId": "89461319475a12229c72efa0c3517cc4f5931834", "title": "ELF: An Early-Exiting Framework for Long-Tailed Classification"}, {"paperId": "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e", "title": "Memory-Efficient Pipeline-Parallel DNN Training"}, {"paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8", "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "90a1491ac32e732c93773354e4e665794ed4d490", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"}, {"paperId": "2238dda06c345a5f1ed87681a10d246c0fac0587", "title": "Why Should We Add Early Exits to Neural Networks?"}, {"paperId": "c5cc2340766d68ece08bb1520d357bcf8c03ad48", "title": "The Right Tool for the Job: Matching Model and Instance Complexities"}, {"paperId": "5d34881ff68bd203ff790187e7e5c9e034389cfa", "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time"}, {"paperId": "1c332cfa211400fc6f56983fb01a6692046116dd", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "4585611042d2be0d997ee135e3fe219d668db9ec", "title": "Depth-Adaptive Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "00c957711b12468cb38424caccdf5291bb354033", "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "a3143eaa68040d366848a9c324b29d3f56f97a5d", "title": "Shallow-Deep Networks: Understanding and Mitigating Network Overthinking"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "125ccd810f43f1cba83c6681836d000f83d1886d", "title": "Multi-Scale Dense Networks for Resource Efficient Image Classification"}, {"paperId": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "896de8418884f4aab1ae4a60027500c9e8baffc3", "title": "BranchyNet: Fast inference via early exiting from deep neural networks"}, {"paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "fba71eefd060e30f3516fdd46df9a191cd0aaaf7", "title": "Conditional Computation in Neural Networks for faster models"}, {"paperId": "500378d0818a23fd4a6d398837a79cb4d30bd652", "title": "HOLMES"}, {"paperId": "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "title": "Deeply-Supervised Nets"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": "69a5847a626c450692fe9c5b296a68cac3e1ff2f", "title": "HadSkip: Homotopic and Adaptive Layer Skipping of Pre-trained Language Models for Efficient Inference"}, {"paperId": "d000ee1e00a940f58b0a6570acf99ec81baeeab8", "title": "SkipBERT: Efficient Inference with Shallow Layer Skipping"}, {"paperId": "7b37c0a4976c4d2a5a440d494fbb0f3daede2a00", "title": "BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression"}, {"paperId": "97d825693c8fce2211700e6075195abade77ec32", "title": "Llama"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": null, "title": "OpenAI"}, {"paperId": null, "title": ": Large-Scale Training and Inference of E. M."}, {"paperId": "1d27a56a8133f947a5a0217b00241d26f585f834", "title": "This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."}, {"paperId": null, "title": "Internlm: A multilingual language model with progressively enhanced capabilities"}, {"paperId": null, "title": "A simple romance between multi-exit vision transformer and token reduction"}, {"paperId": null, "title": "Refined open source dataset by data-juicer"}]}