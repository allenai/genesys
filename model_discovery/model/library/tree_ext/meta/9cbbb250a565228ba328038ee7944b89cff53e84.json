{"paperId": "9cbbb250a565228ba328038ee7944b89cff53e84", "title": "Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning", "abstract": "The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning.", "venue": "arXiv.org", "year": 2023, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.12219", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners and discovers that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions."}, "embedding": {"model": "specter_v2", "vector": [0.28179869055747986, 0.6816673874855042, -0.01640709675848484, -0.156619593501091, 0.22962740063667297, -0.3391816318035126, 1.0142085552215576, -0.4012390971183777, -0.593075692653656, -0.2592529058456421, 0.30094826221466064, -0.4532061219215393, 0.012090091593563557, 0.0330687090754509, -0.14493373036384583, -0.1412528157234192, -1.2099828720092773, 0.6289164423942566, 0.1322127729654312, -0.8385494947433472, -0.26126012206077576, -0.3593928813934326, -0.8351253867149353, -0.18572278320789337, 0.4359371066093445, 0.09815750271081924, 0.23639853298664093, 1.1970210075378418, -0.12693268060684204, 0.47115176916122437, 0.3218836784362793, -0.0017268664669245481, 0.2763530910015106, -0.0742311179637909, 0.018391204997897148, 0.044151950627565384, 0.19811053574085236, -0.9560889005661011, -0.6741887331008911, 0.540692925453186, -0.22081036865711212, 0.10939932614564896, 0.7736288905143738, -0.4478188455104828, -1.0822052955627441, 0.7361843585968018, 0.5810508728027344, 1.1753658056259155, -0.1493033915758133, -0.2863476276397705, 0.9041025638580322, -1.0870040655136108, 0.21699926257133484, 1.502694010734558, 0.2728455364704132, 0.9204865097999573, -0.2701873779296875, -0.7280965447425842, 1.1134936809539795, -0.16535380482673645, -0.6693034172058105, 0.15994153916835785, -0.20500953495502472, -0.3881320059299469, 1.4851503372192383, -0.9331251978874207, 0.10651935636997223, 1.0983079671859741, 0.4782838523387909, 1.3874973058700562, 0.2976350784301758, -0.7423570156097412, -0.04162495210766792, 0.24053408205509186, 0.17999714612960815, 0.9877976775169373, -0.5001537203788757, 0.7435901761054993, -0.761540949344635, 0.26625654101371765, 0.9111341834068298, -0.20051857829093933, -0.04735447093844414, -0.12639576196670532, -0.406489759683609, 0.9114454984664917, 0.42077234387397766, 0.9810130000114441, -0.29736432433128357, 0.5786364674568176, 0.20373666286468506, 0.7064036130905151, -0.08312337100505829, 0.16738218069076538, -0.10622560232877731, 0.4749695062637329, -0.4893730878829956, 0.29595568776130676, 0.029460417106747627, 0.9411500692367554, -0.24056486785411835, 0.33065882325172424, -0.9623086452484131, -0.11332079023122787, 1.31082284450531, -0.006139880511909723, 0.7776638865470886, -0.6197205185890198, 0.09776695817708969, -0.8191380500793457, 0.37857404351234436, -0.5592032074928284, -0.6037327647209167, -0.3398629128932953, -0.8728724122047424, -1.1221189498901367, -0.42033129930496216, -0.12301892787218094, -1.061237096786499, 1.0662541389465332, 0.04621930792927742, -0.047287460416555405, -0.18087337911128998, 0.5667399764060974, 0.10193642228841782, 0.6680071949958801, 0.5718495845794678, 0.10303902626037598, 0.5888369083404541, -0.45956507325172424, -0.7263405919075012, -1.1876593828201294, 0.8458495140075684, 0.2581306993961334, 0.7501928210258484, -0.003334285458549857, -1.182655930519104, -1.0998250246047974, -0.9365445971488953, -0.05373632162809372, -0.5428504347801208, 0.07676353305578232, 1.4052761793136597, 0.8382046222686768, -1.107277512550354, 0.521287739276886, -0.26525363326072693, -0.10294059664011002, 0.38090723752975464, 0.33537784218788147, 0.18412117660045624, -0.7526189684867859, -1.3910317420959473, 0.30960583686828613, 0.3115052878856659, -0.6907222867012024, -0.894441545009613, -0.48052915930747986, -1.0510939359664917, -0.194192036986351, 0.4048740565776825, -0.5948062539100647, 1.2643241882324219, -0.12137582898139954, -1.4110957384109497, 0.6954525709152222, 0.08471705764532089, -4.968380744685419e-05, 0.42524152994155884, -0.11113923043012619, -0.5708566308021545, -0.3783152997493744, -0.34776610136032104, 0.6744220852851868, 0.6244757175445557, -0.25432372093200684, -0.13761469721794128, -0.2147282212972641, -0.08196628093719482, -0.26132145524024963, -0.4096384048461914, 0.5110560059547424, -0.16754615306854248, -0.48356810212135315, 0.27603060007095337, 0.43437960743904114, 0.05277562886476517, -0.24810673296451569, 0.10628344118595123, -1.2421709299087524, 0.6844777464866638, -0.08494960516691208, 0.942112147808075, -1.127131700515747, -0.29283466935157776, -0.36012446880340576, 0.1325734704732895, -0.19292210042476654, -1.0889558792114258, 0.7291243076324463, -0.05487338453531265, 0.4608178734779358, -0.26776978373527527, -0.9559550881385803, 0.2994849383831024, 0.17814067006111145, -0.7285536527633667, -0.3911786377429962, 0.09201085567474365, 1.20158052444458, -0.7604416608810425, 0.16854962706565857, -0.161587193608284, 0.5215528011322021, -1.1863617897033691, 1.269177794456482, -0.6322689652442932, 0.7360388040542603, -0.10576174408197403, -0.5235311985015869, -0.19546882808208466, -0.36812064051628113, 0.23161010444164276, -0.07061929255723953, 0.416412353515625, -0.09314998984336853, -0.4664763808250427, 1.5404523611068726, -0.4036897122859955, 0.29254910349845886, -0.06633543223142624, -0.6406416296958923, 0.13994960486888885, 0.23416589200496674, -0.3503628969192505, -0.4345693588256836, -0.038168832659721375, 0.13390406966209412, -0.08999819308519363, 0.0028578797355294228, 0.5477781891822815, 0.3292961120605469, -0.44103577733039856, 0.3231447637081146, 0.5722607970237732, -0.7673107981681824, 0.7064648866653442, 0.6568809151649475, 0.7787504196166992, 0.44104307889938354, 0.5231680274009705, -0.04332750290632248, 0.2945832312107086, -1.011296272277832, -0.4304755628108978, 0.899072527885437, 0.5245873928070068, 0.28385496139526367, 0.18868301808834076, -0.7150378823280334, -0.26450833678245544, -0.3361040949821472, 0.7080144882202148, 1.975022792816162, -0.469708651304245, 0.0662660300731659, -0.6236183643341064, -0.2000175565481186, -0.23011529445648193, 0.5251047015190125, -0.8670254349708557, -0.4380791485309601, -0.5400248765945435, -0.9219782948493958, 0.3836556673049927, 0.521146297454834, 0.9683316946029663, -0.38506853580474854, 0.05030481889843941, -0.034695930778980255, 0.882229745388031, -0.7185297608375549, -0.6873365640640259, 0.008894110098481178, -0.3862985670566559, 0.3411416709423065, -0.2172495722770691, -0.04753616452217102, -0.08465985208749771, -0.32904571294784546, 0.8276070952415466, -0.33053702116012573, -0.3803967237472534, 0.6972757577896118, 0.7520261406898499, -0.6404497027397156, -1.1557198762893677, 0.1466505080461502, 0.32918113470077515, -0.2266809344291687, 0.23182986676692963, 0.4682164788246155, -0.1799529790878296, 0.2809470593929291, -0.8670228123664856, -0.11909569054841995, 0.3404093384742737, -0.10924163460731506, 0.32616159319877625, -0.24651087820529938, 0.3419126570224762, -1.412237286567688, 1.4291654825210571, 0.14482837915420532, -0.5974418520927429, 0.35409682989120483, -0.689446747303009, -0.36999863386154175, 0.379117488861084, -1.1455624103546143, -0.6067113280296326, -1.0532008409500122, 0.44365280866622925, 0.07277266681194305, -0.5157312154769897, 0.17326273024082184, 0.7616901993751526, -0.024513766169548035, 0.5351336598396301, 0.5327863097190857, 0.09256769716739655, 0.37155720591545105, 0.8623830080032349, -0.9065967798233032, 0.2958742380142212, 0.2606416940689087, 0.3943658173084259, -0.0768212378025055, -0.15926550328731537, -0.8137186169624329, -0.6312427520751953, 0.19211579859256744, -0.5307444334030151, -0.3058910369873047, 0.5643629431724548, -0.8261970281600952, -1.0537850856781006, 0.02076839469373226, -0.46247598528862, -0.8382990956306458, 0.0824410617351532, -0.42760300636291504, -0.21698002517223358, -1.093349575996399, -1.448211908340454, -0.5935440063476562, 0.04123995453119278, -0.7543045282363892, 0.5253266096115112, -0.002754681743681431, -0.5743091702461243, -1.0411474704742432, 0.17754532396793365, -0.3965531885623932, 1.2210663557052612, -1.0624098777770996, 1.0481321811676025, 0.18429286777973175, -0.39350682497024536, -0.4546447992324829, 0.6085487008094788, 0.36790230870246887, -0.1278611421585083, 0.1904478669166565, -0.8164089322090149, -0.18636655807495117, -0.2378448098897934, -0.6621361970901489, 0.18253332376480103, 0.31823721528053284, 0.6715254783630371, 0.1943567544221878, -0.3798234164714813, -0.03718893229961395, 0.9701473116874695, -0.39738768339157104, -0.1684393733739853, -0.07087153941392899, 1.0469764471054077, 0.2773773670196533, -0.5798742175102234, 0.4518660604953766, 0.7084439992904663, 0.27553194761276245, -0.3720160126686096, 0.35819679498672485, -0.0006663579843007028, -0.8043994307518005, 0.6875192523002625, 1.5112388134002686, 0.18940259516239166, -0.15606272220611572, -1.2125089168548584, 0.15000374615192413, -0.7049242258071899, -0.574999213218689, 0.9819180369377136, 0.7921149134635925, 0.5160320401191711, -0.294631689786911, -0.4051104784011841, -0.18911410868167877, -0.01799030974507332, 0.19226373732089996, -0.5837591290473938, -0.5134382247924805, 0.10035400837659836, 0.16428899765014648, -0.2926580607891083, 0.8316859602928162, -0.3177821636199951, 0.6534002423286438, 14.643564224243164, 0.713157057762146, -0.04632564261555672, 0.847300112247467, 0.6952480673789978, 0.3950459063053131, -0.6202816367149353, -0.3372632563114166, -1.1998134851455688, -0.27806228399276733, 1.0577863454818726, 0.373483806848526, 0.8930581212043762, -0.16076363623142242, -0.1778361052274704, 0.12190517038106918, -0.4993267059326172, 0.19173705577850342, 0.49321436882019043, -1.140679955482483, 0.4655141234397888, 0.32068654894828796, 0.6357009410858154, 0.3823772966861725, 1.1996275186538696, 1.2145549058914185, 0.7609266042709351, -0.27552980184555054, 0.4192083477973938, 0.20826707780361176, 0.6659308075904846, 0.42968398332595825, -0.25755947828292847, 1.2035114765167236, -0.4475521445274353, -0.5983485579490662, -0.3326711654663086, -1.0650684833526611, 0.31913894414901733, -0.20276501774787903, -0.4963505268096924, -0.8631998896598816, -0.46695104241371155, 0.5411096811294556, 0.05784562975168228, -0.36473023891448975, -0.5773119330406189, 0.8309727907180786, -0.04755840077996254, 0.02986302599310875, 0.3436473608016968, 0.24973909556865692, -0.08303125202655792, -0.3231047987937927, 0.03794442117214203, 0.34982362389564514, 0.2767854332923889, 0.484200119972229, -0.4212779104709625, -0.18845655024051666, -0.5236690044403076, -0.3548297584056854, -0.25887957215309143, 0.4161739647388458, 0.4632430374622345, 0.21755573153495789, -0.5231084227561951, 0.4480911195278168, 0.7958254218101501, 0.47047799825668335, -0.12949785590171814, 0.5978822708129883, -0.25392404198646545, -0.5715459585189819, -0.029971063137054443, -0.1067921370267868, -0.03706444427371025, -0.448880672454834, -0.911450982093811, -0.41600301861763, 0.3076244592666626, -1.0396605730056763, -1.1249167919158936, 0.6709690093994141, -0.18224786221981049, 0.001422800705768168, 0.04274408146739006, -0.7636688351631165, -0.17634329199790955, 0.2651230990886688, -1.2455979585647583, -0.7273654341697693, 0.1729055941104889, -0.24210043251514435, -0.18183104693889618, -0.1624508649110794, 1.2213261127471924, -0.1017957478761673, -0.5404465198516846, -0.25970491766929626, -0.01657000556588173, -0.35760846734046936, -0.25979024171829224, -0.9722877740859985, 0.8388963341712952, -0.0340101420879364, 0.20293495059013367, 0.22116079926490784, 0.08881373703479767, -5.414211955212522e-06, -0.9309582710266113, 0.361821711063385, 0.29980209469795227, -1.0408111810684204, -0.2288190871477127, -0.8698634505271912, -1.130853295326233, 0.6536162495613098, 0.5786004066467285, -0.40098491311073303, 0.32234200835227966, 0.09524618089199066, -0.6045690774917603, -0.04116363450884819, -0.5783972144126892, 0.29364752769470215, 0.4856335520744324, -0.635444164276123, -0.46028295159339905, 0.1655767858028412, 0.3995152413845062, -0.9725440144538879, -0.4006834924221039, -0.3460337817668915, -0.04189400374889374, -0.1821901947259903, 0.9681479334831238, -0.7225366234779358, 0.530388593673706, 1.1965522766113281, 0.1525074988603592, -0.8037538528442383, -0.5084261298179626, -0.831609845161438, 0.1441211998462677, 0.1359443962574005, 0.5427804589271545, -0.7063729166984558, -0.085566945374012, 1.079820990562439, 0.5406171083450317, -0.1709926873445511, -0.4459167718887329, -0.22950473427772522, 0.6447498798370361, -0.5797955393791199, 0.13794659078121185, -0.10986536741256714, -0.5351749658584595, 0.36814171075820923, 0.3498736321926117, 0.8219400644302368, -0.15980809926986694, -0.9198564887046814, 0.545612633228302, 0.046289872378110886, -0.2194686084985733, -0.6662317514419556, -0.16785548627376556, -1.6614598035812378, -0.054151859134435654, -1.1211565732955933, -0.12191642820835114, -0.9177902340888977, -0.9000076651573181, 0.039987582713365555, -0.023813894018530846, -0.3157978057861328, 0.16033011674880981, -0.7206932306289673, -0.41645145416259766, -0.4794374108314514, -0.1103477030992508, 0.8558931350708008, 1.1370142698287964, -0.681000828742981, 0.0499553456902504, -0.0648936852812767, 0.09646227955818176, 0.3181932270526886, 0.503425121307373, -0.3410108685493469, -1.2758915424346924, -1.3718732595443726, 0.4802807569503784, -0.026866555213928223, -0.21837662160396576, -0.770335853099823, 0.7474862337112427, 0.21813251078128815, 0.03892238438129425, 0.3875424563884735, 0.5898608565330505, -0.8544166088104248, -0.7537030577659607, 0.35026711225509644, -1.1024748086929321, 0.3234832286834717, 0.21703919768333435, 0.15232965350151062, 0.1475415676832199, 0.8313031196594238, -0.18998852372169495, -1.1468409299850464, -0.7949944734573364, 0.5953198671340942, -0.9310064911842346, 0.2310846447944641, 0.10430590063333511, 0.2756361663341522, -1.0993236303329468, -0.613355815410614, -0.07182788848876953, 0.40268945693969727, -0.5553511381149292, 0.9810673594474792, 0.545438289642334, -0.6982381343841553, -0.052901193499565125, 0.49935755133628845, -0.13399171829223633, -0.048796750605106354, 0.9918630719184875, 0.2105415165424347, -0.11088310927152634, 0.6323773264884949, 0.4542018175125122, 0.3355101943016052, -0.45706048607826233, 0.45655861496925354, 0.671049952507019, -0.4871075749397278, -0.17393173277378082, 1.2682212591171265, 0.2530915141105652, -1.0729483366012573, 0.17655964195728302, -1.2113360166549683, -0.6804388165473938, -0.5113199949264526, 0.658257246017456, -0.3654335141181946, -0.6833518147468567, 0.3184495270252228, -0.03258466720581055, 0.6818798780441284, -0.3043974041938782, -0.7665162682533264, 0.37155255675315857, -0.44711142778396606, -0.10604012757539749, 1.03923761844635, 0.2852655053138733, -0.5803592801094055, -1.1365761756896973, -0.7382515072822571, -0.3755829334259033, -0.048315100371837616, 0.3640933930873871, -0.6146668791770935, -0.30717945098876953, 0.9882923364639282, 0.7100656628608704, 0.47806262969970703, 0.038037873804569244, 0.06386285275220871, -0.2522018253803253, 0.7595775127410889, 0.1698523759841919, -0.3412805497646332, -0.05595364049077034, 1.0071640014648438, 1.5859708786010742, -0.9974807500839233, 0.18364453315734863, -0.13172601163387299, -0.9854714870452881, 0.7364429831504822, 0.5723006725311279, -0.10255187004804611, 1.1840840578079224, -0.2287449687719345, 0.32058534026145935, -0.006511159706860781, -1.154330849647522, -0.09665179997682571, 0.9037822484970093, 1.0625425577163696, 0.7212985754013062, 0.2905498743057251, 0.17872603237628937, 0.6613360643386841, 0.13383923470973969, 0.4852902293205261, 0.614716112613678, 0.4667753577232361, -0.4167943596839905, 0.26367467641830444, 0.2416037619113922, 0.3588997423648834, -0.3336634039878845, -0.3838541805744171, 0.28971511125564575, 0.6355753540992737, 0.05931785702705383, 0.351544052362442, 0.5381127595901489, 0.4621368646621704, 0.4074345827102661, 0.1821107715368271, 0.7081674933433533, -0.5066564083099365, 0.07254506647586823, -0.2817261815071106, -1.0185548067092896, 0.12382887303829193, -0.2810845673084259, -0.22058939933776855, -0.2813573181629181, 0.05773557350039482, 0.5555509924888611, -0.43353837728500366, 0.18581843376159668, 1.291082739830017, 0.5642901659011841, 0.47706881165504456, -0.37577611207962036, -0.42545679211616516, -0.8023779988288879, -0.9840145111083984, 0.09252263605594635, -0.5138975977897644, -0.1050991341471672, -0.5557534098625183, -0.2533711791038513, -0.25930994749069214]}, "authors": [{"authorId": "2153258452", "name": "Jiasheng Ye"}, {"authorId": "24018493", "name": "Zaixiang Zheng"}, {"authorId": "145854784", "name": "Yu Bao"}, {"authorId": "2072789464", "name": "Lihua Qian"}, {"authorId": "144966687", "name": "Quanquan Gu"}], "references": [{"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "be8db99310602d66bba64bcf41a572c45816fbfc", "title": "Let's Verify Step by Step"}, {"paperId": "d9ffb44ee3c8ec0b6692df8a90451384c1edd89b", "title": "Likelihood-Based Diffusion Language Models"}, {"paperId": "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c", "title": "Scaling Data-Constrained Language Models"}, {"paperId": "49cbfa5848aaa74ce01d29ee5328f1a2b466ce27", "title": "David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs"}, {"paperId": "375de1279cc1685a116403bc378ba94aaf6a172e", "title": "Diffusion Language Models Generation Can Be Halted Early"}, {"paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"}, {"paperId": "54b6e5dcef733c151adef0ac06430f63cb301a36", "title": "AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation"}, {"paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3", "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality"}, {"paperId": "bf9f9ec648119baecb6f33a87bac2e7cbf322b38", "title": "Directed Acyclic Transformer Pre-training for High-quality Non-autoregressive Text Generation"}, {"paperId": "dfa0fd70a186b0f7f44e833b4173ed97a4f5de31", "title": "Understanding Causality with Large Language Models: Feasibility and Opportunities"}, {"paperId": "e3950d18cc81edbeafd83129a646a62850cad62a", "title": "A Cheaper and Better Diffusion Language Model with Soft-Masked Noise"}, {"paperId": "574beee702be3856d60aa482ec725168fe64fc99", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "020a50f6a7154850ac81e3cde69ad8198ded6751", "title": "DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises"}, {"paperId": "1f898d66acabff511a3871b82799aa73c0055402", "title": "A Reparameterized Discrete Diffusion Model for Text Generation"}, {"paperId": "e962f95e03a50ff2f3a0fe7840daebac04578c46", "title": "Structure-informed Language Models Are Protein Designers"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "fbd49b25bdab98c171af49962a41139c73dacbde", "title": "Specializing Smaller Language Models towards Multi-Step Reasoning"}, {"paperId": "03fb95e6be583ca954c3d00812a9e9a40f118e51", "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language"}, {"paperId": "a1186d7d9a9ef258c76afef1177e4f348061a537", "title": "SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers"}, {"paperId": "126a4776ff8315fd506766cb8f3c722cf746ad9e", "title": "Teaching Small Language Models to Reason"}, {"paperId": "a979742220a88b1d32e1fbe72c41e8ba3007053c", "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models"}, {"paperId": "22775e58932cdfbd273a2a835a22c5d86800a458", "title": "Continuous diffusion for categorical data"}, {"paperId": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e", "title": "Solving math word problems with process- and outcome-based feedback"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "0b9770a377b3f96cef9f268cee1791d39a0d4893", "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "69144d537f90f214d5b07a7c79121d16afd7da16", "title": "DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models"}, {"paperId": "663a41c866d49ce052801fbc88947d39764cad29", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"}, {"paperId": "62f0db3a5ad5c795ec18fc7a6e7b01836809df57", "title": "Language Models are Multilingual Chain-of-Thought Reasoners"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "498ac9b2e494601d20a3d0211c16acf2b7954a54", "title": "Imagen Video: High Definition Video Generation with Diffusion Models"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "1386b8a11929cf02da291c56aca353e33bbc22ed", "title": "Diffusion-LM Improves Controllable Text Generation"}, {"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners"}, {"paperId": "7472f26ffa0f056a70e461d6918ae5ffecda2c92", "title": "Directed Acyclic Transformer for Non-Autoregressive Machine Translation"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "c57293882b2561e1ba03017902df9fc2f289dea2", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "0e3d6a7c9c04cf3ba9c902724548846a5ade04b4", "title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation"}, {"paperId": "7dbb386a617eacc954940c9540d9cb262529b8b1", "title": "Equivariant Diffusion for Molecule Generation in 3D"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "7c597874535c1537d7ddff3b3723015b4dc79d30", "title": "MaskGIT: Masked Generative Image Transformer"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "title": "High-Resolution Image Synthesis with Latent Diffusion Models"}, {"paperId": "2da2a44f78e1bd9735d94fee3bd944d47d45742b", "title": "Step-unrolled Denoising Autoencoders for Text Generation"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "b43edd1cdbef498d4f4c27bfc7ca5456480a5c40", "title": "The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "91b32fc0a23f0af53229fceaae9cce43a0406d2e", "title": "Structured Denoising Diffusion Models in Discrete State-Spaces"}, {"paperId": "92bf1c069747374fbc3efb55e7a916a3e2d736da", "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"}, {"paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794", "title": "Diffusion Models Beat GANs on Image Synthesis"}, {"paperId": "c553280c1fc1d0bc7b94683bb75910e309b0d579", "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling"}, {"paperId": "add5f3f820b393e7ce5ed467814253824ecc484b", "title": "Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions"}, {"paperId": "633e2fbfc0b21e959a244100937c5853afca4853", "title": "Score-Based Generative Modeling through Stochastic Differential Equations"}, {"paperId": "9e6763597414d865237fdd065eed50bbc5ff14f5", "title": "Limitations of Autoregressive Models and Their Alternatives"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "34bf13e58c7226d615afead0c0f679432502940e", "title": "DiffWave: A Versatile Diffusion Model for Audio Synthesis"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "5c126ae3421f05768d8edd97ecd44b1364e2c99a", "title": "Denoising Diffusion Probabilistic Models"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "1e5b826ddf0754f6e93234ba1260bd939c255e7f", "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145", "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"}, {"paperId": "5efadc9019ce3378a0eb6c8f939cdde6c8918b1e", "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models"}, {"paperId": "b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb", "title": "A Call for Clarity in Reporting BLEU Scores"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "15e81c8d1c21f9e928c72721ac46d458f3341454", "title": "Non-Autoregressive Neural Machine Translation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2dcef55a07f8607a819c21fe84131ea269cc2e3c", "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": "24425954960ce968e5f14360fbdd0605abcadfcf", "title": "Difformer: Empowering Diffusion Model on Embedding Space for Text Generation"}, {"paperId": "b1dd72ad1a1061dec9764d03d572f28486017db4", "title": "Diff-Glat: Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning"}, {"paperId": "032a2ce40f1dd6b78109c67468c5ea8559c1532f", "title": "Non-Autoregressive Sequence Generation"}, {"paperId": "e512964293671abbdc409f313d127cbe85ffe5cd", "title": "GENIE : Large Scale Pre-training for Generation with Diffusion Model"}, {"paperId": null, "title": "2022) to sample training data from different subsets"}, {"paperId": null, "title": "On the few-shot settings, we randomly select demonstrations. We will also release our code and data for better reproducibility"}, {"paperId": null, "title": "RQ 2. On scaling model sizes. It has been widely observed that the larger the model size, the more competent the language models become, a.k.a ., the scaling law"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "1254c99a381d1f9d6abb23c74be060e4e7876985", "title": "Book Reviews: Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition"}, {"paperId": "21e8b63c2142ed45cf0f8dccbc406a0360da2264", "title": "Quantified Representation of Uncertainty and Imprecision"}, {"paperId": "09260da50abc7a39b3f00281c5fce6fa604de88b", "title": "Graphical Models for Probabilistic and Causal Reasoning"}, {"paperId": "c1e3f2d537e50e0d5263e4731ab6c7983acd6687", "title": "Prediction and Entropy of Printed English"}, {"paperId": null, "title": "OpenAI. 2023."}]}