{"paperId": "4375abf311b0ef51644551276153efaf11426916", "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving", "abstract": "We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models."}, "embedding": {"model": "specter_v2", "vector": [0.22123022377490997, -0.0773330107331276, -0.7330034375190735, -0.4111385941505432, -0.9562099575996399, -0.24953719973564148, 0.4539669454097748, -0.1201840490102768, -0.4240940809249878, -0.21866895258426666, 0.4646575450897217, 0.013081380166113377, 0.20740906894207, 0.3295971751213074, -0.3752654790878296, 0.4568846523761749, -1.0571537017822266, 0.3087206482887268, -0.236719012260437, -0.3332751989364624, -0.17453110218048096, -0.3714704215526581, -1.0453115701675415, 0.2690458297729492, 0.5637098550796509, 0.611589789390564, 0.2183590829372406, 1.3219783306121826, -0.13905474543571472, 0.20287521183490753, 0.5814229846000671, 0.0014286800287663937, 0.40999719500541687, -0.1020774096250534, 0.1680840700864792, -0.2197716236114502, 0.4552922546863556, -0.7443342804908752, -0.43734750151634216, 0.5089634656906128, -0.02513565868139267, 0.09820585697889328, 0.012486648745834827, -0.9512654542922974, -0.3453848361968994, 0.4879244863986969, 0.604896605014801, 0.5296846032142639, 0.09428565949201584, -0.6268739700317383, 1.7852604389190674, -1.398370623588562, 0.03728533163666725, 1.6237956285476685, 0.4929715692996979, 0.043255504220724106, -0.3108958601951599, -0.30074042081832886, 0.822219729423523, -0.3423909544944763, -0.8699674606323242, -0.7381656765937805, -0.4512215554714203, 0.08713934570550919, 1.3473912477493286, -0.007152883801609278, -0.2605748772621155, 0.2396993339061737, -0.10082056373357773, 1.5610630512237549, -0.0862373411655426, -1.0475317239761353, 0.15567362308502197, -0.11593355983495712, 0.43833127617836, 0.30946916341781616, -0.031339485198259354, 0.061386723071336746, -0.8702631592750549, -0.566558301448822, 0.05268726497888565, 0.032917216420173645, -0.09044438600540161, -0.3741074800491333, 0.0400933213531971, 0.6160777807235718, 0.004301685839891434, 0.6264622211456299, -0.16666951775550842, 1.0788847208023071, 0.2558245360851288, 0.4644543528556824, 0.40733611583709717, 0.2312624156475067, -0.24231910705566406, -0.09634904563426971, -1.0710246562957764, 0.3297712802886963, 0.47520747780799866, 1.276978611946106, -0.6428678035736084, -0.5433208346366882, -0.5425339937210083, 0.581251859664917, 1.5508012771606445, 0.2965068817138672, 0.10855606943368912, -0.8429286479949951, 0.37827637791633606, -0.8044227361679077, 0.7346971035003662, -0.40501129627227783, -0.05693329498171806, -0.10073130577802658, -0.11915174126625061, -1.3788810968399048, -0.4891764521598816, 0.17711207270622253, -0.10786710679531097, 0.8326014280319214, -0.4563561975955963, 0.11472435295581818, 0.039270199835300446, 0.5461353659629822, -0.12095793336629868, 0.7367470860481262, 0.10888925939798355, -0.3033715486526489, 0.4985993802547455, -0.6791139841079712, -0.40414509177207947, -1.2803839445114136, 1.182605266571045, -0.5315485596656799, 0.8129784464836121, -0.501775860786438, -1.11332106590271, -0.6332657337188721, -0.9388449192047119, 0.18847069144248962, -0.43913376331329346, 0.22342431545257568, 1.0036553144454956, 0.5787419676780701, -1.4223874807357788, 0.17378325760364532, -0.16629761457443237, -0.4661578834056854, 0.14863726496696472, 0.040181614458560944, 0.2847417891025543, -0.9807663559913635, -1.0496569871902466, 0.3393653929233551, -0.23888176679611206, -0.7646438479423523, -0.07133238017559052, -0.4634106755256653, -1.1043418645858765, -0.08447932451963425, 0.40996095538139343, -0.3697521388530731, 1.2657548189163208, -0.1919766366481781, -0.8992824554443359, 0.2720920741558075, -0.3023063540458679, 0.35102006793022156, 0.43632930517196655, -0.5443766117095947, -0.8946155905723572, -0.5993766188621521, -0.2393665909767151, 0.10425770282745361, -0.026319418102502823, -0.5652058124542236, -0.4153556823730469, 0.08913854509592056, -0.03601439669728279, 0.27658092975616455, 0.07754204422235489, 0.873204231262207, -0.7173330783843994, -0.02642464078962803, 0.2912704646587372, 0.6664196252822876, -0.3200228214263916, 0.4082476496696472, 0.055954258888959885, -0.6339720487594604, 0.9835894107818604, -0.22064703702926636, 1.4114372730255127, -0.7688155174255371, -0.961899995803833, 0.014477429911494255, -0.17205063998699188, -0.2555018663406372, -0.9588486552238464, 0.9119475483894348, 0.3660503327846527, 0.6257622241973877, -0.15054763853549957, -1.1339292526245117, 0.2537858188152313, -0.1180044412612915, -0.4638538062572479, -0.10713176429271698, -0.3409470021724701, 0.6519392728805542, -0.7994638085365295, 0.06228179484605789, -0.21900616586208344, 0.06920797377824783, -0.8366142511367798, 1.3016514778137207, -0.9683902263641357, -0.15992465615272522, -0.24664811789989471, 0.14536282420158386, 0.3178320527076721, -0.3148525655269623, 0.6057043671607971, 0.009140709415078163, 0.11312049627304077, 0.18296632170677185, -0.4119427800178528, 1.2315720319747925, -0.6950224041938782, 0.37823569774627686, 0.3007863461971283, -0.17118781805038452, -0.15813738107681274, 0.2419639527797699, -0.057618148624897, -0.4137611389160156, -0.08763402700424194, 0.6282733082771301, -0.7473885416984558, 0.2046942263841629, 0.7344410419464111, 1.059973955154419, -0.44983142614364624, 0.5146909356117249, -0.1600855439901352, 0.035222556442022324, 0.47019121050834656, 0.3114719092845917, 0.6886860728263855, 0.45370858907699585, 0.40659967064857483, -0.15625199675559998, 0.36767449975013733, -0.9324556589126587, -0.2586981952190399, 0.8687707185745239, 0.21807481348514557, 0.4971170425415039, 0.1575307846069336, -0.4422103762626648, -0.664828896522522, 0.1773388683795929, 0.6519010663032532, 1.9684100151062012, 0.22640465199947357, -0.24342533946037292, -0.8804614543914795, -0.32804861664772034, 0.11193259805440903, 0.34815332293510437, -0.12331279367208481, 0.33482760190963745, -0.6272135376930237, -0.7006785869598389, 0.6841601133346558, 0.3851432502269745, 0.4555820822715759, -0.3502175211906433, 0.09221327304840088, -0.05093688517808914, 0.13181455433368683, -0.9146342873573303, -0.9726974368095398, -0.23956692218780518, -0.5821393132209778, -0.10214350372552872, 0.04815777763724327, 0.002930189250037074, -0.1389978528022766, -0.13725054264068604, 1.0697243213653564, 0.2393147051334381, 0.11094437539577484, 0.7224336862564087, 0.2616867423057556, -0.6016422510147095, -1.048984169960022, -0.03348950296640396, 0.5118577480316162, -0.3087748885154724, 0.5436432957649231, 0.6212330460548401, 0.07002364099025726, 0.346971333026886, -0.40894562005996704, 0.21400226652622223, 0.3543849289417267, -0.3382258117198944, 0.6420683860778809, -0.7381993532180786, 0.4832102060317993, -1.111998438835144, 1.1875981092453003, -0.150846466422081, -0.44196635484695435, 0.8773796558380127, -1.1103819608688354, -0.6727473735809326, 0.22768321633338928, -0.514809787273407, 0.1717587113380432, -1.1889976263046265, 0.46780121326446533, 0.02261974662542343, -0.23324958980083466, 0.2439468801021576, 0.07898883521556854, 0.31286704540252686, -0.14299577474594116, 0.689469575881958, 0.33605465292930603, -0.4068771004676819, 0.6979323625564575, -0.2796686887741089, -0.02114146761596203, -0.149171382188797, -0.5017408728599548, -0.30405279994010925, -0.34888508915901184, -0.7166802883148193, -0.3317168354988098, -0.6763557195663452, -0.32534611225128174, -0.19614192843437195, -0.2554669976234436, -0.4614051878452301, -0.2521648108959198, -0.4630363881587982, -0.9694174528121948, -0.15633773803710938, 0.6173610687255859, -0.41649341583251953, -0.4530443251132965, -1.0870895385742188, -1.1379199028015137, -0.8752550482749939, -0.518134593963623, -1.0894535779953003, 0.5938588380813599, -0.4653032720088959, -0.25776371359825134, -0.32451513409614563, -0.02472221478819847, -0.3557851314544678, 1.1527273654937744, -0.7213820219039917, 0.9968370199203491, -0.21737253665924072, -0.27143415808677673, -0.7440381050109863, -0.006390344817191362, 0.19967269897460938, -0.21718822419643402, 0.10201352834701538, -1.0256277322769165, -0.07726725190877914, -0.5029804706573486, 0.1372368037700653, -0.33342739939689636, 0.41252321004867554, 0.6507694721221924, -0.06142978370189667, -0.6284812688827515, 0.005527678411453962, 1.5338985919952393, -0.6770739555358887, -0.36209797859191895, -0.08837167173624039, 0.9321005940437317, 0.2932579219341278, -0.0031850761733949184, 0.5416515469551086, 0.14863403141498566, 0.5498369932174683, -0.09013769030570984, 0.22910287976264954, -0.048922810703516006, -0.6644026637077332, 0.7078182101249695, 1.6253844499588013, 0.5420781970024109, -0.537910521030426, -0.9346103668212891, 0.3909930884838104, -1.595080852508545, -0.5288422107696533, 0.8621095418930054, 1.1379417181015015, -0.0938638225197792, -0.33570146560668945, 0.03866267949342728, -0.5197207927703857, 0.3188053071498871, 0.425995796918869, -0.41663193702697754, -0.5053678750991821, -0.13997988402843475, -0.26098722219467163, 0.06515920907258987, 0.7682966589927673, -0.2523505687713623, 0.42522698640823364, 14.822068214416504, 0.9597111344337463, 0.23972709476947784, 0.936010479927063, 0.9453538656234741, -0.050244562327861786, -0.5332516431808472, -0.17360441386699677, -1.0621986389160156, 0.17199862003326416, 1.7573904991149902, -0.10939625650644302, 0.7205150127410889, 0.3447328805923462, 0.6010249853134155, 0.025826456025242805, -0.9998358488082886, 0.6085181832313538, 0.5420874357223511, -1.1766128540039062, 0.5657732486724854, 0.385733962059021, 0.2878356873989105, 0.6831772327423096, 0.4781896770000458, 0.7765567898750305, 0.5288954973220825, -0.6014428734779358, 0.7007535696029663, 0.4380955696105957, 1.1319386959075928, -0.06604235619306564, 0.5555503368377686, 0.8844233155250549, -0.782275378704071, -0.0674528032541275, -0.5709182024002075, -1.1472296714782715, 0.26122668385505676, 0.12888599932193756, -0.3372281491756439, -0.671661913394928, -0.42351701855659485, 0.43051159381866455, 0.1316072642803192, 0.2768040895462036, 0.5285964012145996, 0.8861271142959595, -0.6807173490524292, -0.13680443167686462, 0.08651984483003616, 0.29808303713798523, 0.05283599719405174, 0.06019220128655434, 0.024575084447860718, -0.08031131327152252, 0.6722405552864075, 0.06529013067483902, -0.9106631278991699, 0.5160008072853088, -0.7672867178916931, -0.26625439524650574, 0.4665433466434479, 0.178670272231102, 0.7029446959495544, 0.08344976603984833, -0.647542417049408, 0.4246997535228729, 0.46217018365859985, 0.3573501408100128, 0.03444671258330345, 0.6280510425567627, 0.7284287214279175, -0.32780131697654724, -0.3360409438610077, 0.5011324882507324, -0.34352394938468933, -0.4109330177307129, -0.3214845359325409, -0.37839946150779724, 0.4607093334197998, -0.7709075212478638, -0.9938181042671204, 0.4212258458137512, -0.26106369495391846, -0.40026527643203735, 0.1811908483505249, -0.44327038526535034, -0.02707185223698616, 0.5188472867012024, -1.2152047157287598, -0.9383397102355957, 0.6057839393615723, -0.6071259379386902, -0.027783473953604698, 0.3616500794887543, 1.4324673414230347, 0.20312348008155823, -0.3974589407444, 0.28524312376976013, 0.8917168974876404, -0.2822163701057434, 0.11541465669870377, -0.5286858677864075, 0.6878015398979187, 0.4259142577648163, -0.011576569639146328, 0.47930923104286194, -0.005703695584088564, -0.012884414754807949, -1.0308057069778442, -0.3040042519569397, 0.7174196243286133, -1.2883461713790894, -0.42531058192253113, -0.8903259634971619, -0.5959281921386719, 0.3540467619895935, 0.06905259937047958, 0.07243160158395767, 0.5062482953071594, 0.11188007891178131, -0.5403807759284973, -0.2667556703090668, -1.0696943998336792, 0.2487778216600418, 0.8341715335845947, -0.8841235041618347, 0.34050247073173523, 0.04911961406469345, 0.6120647192001343, -0.8350726962089539, -0.5939677953720093, -0.24352917075157166, 0.301702618598938, -0.09179960191249847, 0.5576828122138977, -0.49066436290740967, 0.38313496112823486, 0.7775590419769287, -1.0583152770996094, -0.3554198741912842, 0.16981375217437744, -0.923171877861023, -0.2894442677497864, -0.17447951436042786, 1.0836975574493408, -0.401879221200943, -0.0018789103487506509, 1.0669927597045898, 0.6593306064605713, -0.7524740695953369, -0.5518273115158081, -0.3288286626338959, 0.2175576239824295, -0.7942405343055725, 0.6068424582481384, -0.09606944024562836, -0.3344639241695404, -0.16096007823944092, 0.3277401626110077, 1.0655667781829834, -0.0391586497426033, -0.5412841439247131, 0.5642933249473572, -0.20831400156021118, 0.058377306908369064, -0.6755427718162537, -0.2155236303806305, -1.7073986530303955, -0.20504869520664215, -0.7850596308708191, 0.052118200808763504, -1.0007743835449219, -0.40477195382118225, -0.3505580723285675, -0.21973001956939697, -0.7287920117378235, 0.5279794335365295, -0.5805179476737976, -0.11068898439407349, -0.04942987859249115, -1.0079020261764526, 0.4097915589809418, 0.7390382885932922, -0.2689884603023529, 0.32377520203590393, 0.15232358872890472, 0.15683765709400177, 0.26597821712493896, 0.3353707492351532, -0.19851082563400269, -0.8165534138679504, -1.1077800989151, 0.19130666553974152, -0.0418802946805954, -0.3975410461425781, -0.2419375628232956, 0.4694736897945404, -0.2685876488685608, -0.18292367458343506, 0.06317634880542755, 0.47184816002845764, -1.2770389318466187, -0.6228237152099609, 0.16312305629253387, -1.2085446119308472, 0.6059657335281372, -0.06323707848787308, -0.03762996569275856, -0.01002176571637392, 0.7869537472724915, -0.20762331783771515, -0.9793752431869507, -0.7705889940261841, 0.4585004150867462, -0.3588777184486389, 0.02682124637067318, -0.11033911257982254, 0.09260837733745575, -0.7752193808555603, -0.993418276309967, -0.023022178560495377, 0.45845291018486023, 0.08877954632043839, 1.0230013132095337, 0.3286261260509491, -1.1406726837158203, -0.17543956637382507, 0.05182171240448952, 0.2625180780887604, -0.32375067472457886, 0.1679076999425888, 0.16891436278820038, -0.5774873495101929, 0.5336295366287231, 0.8472251296043396, 0.0473787859082222, -0.8948893547058105, -0.005954761989414692, 0.6877310872077942, -0.2841355502605438, -0.1342434287071228, 0.8867021203041077, -0.4201805591583252, -1.0463944673538208, 0.10498753935098648, -1.1295713186264038, -0.07927502691745758, -0.9000445604324341, 0.9054685831069946, 0.09443778544664383, -0.08457119762897491, -0.06392925977706909, -0.3912147879600525, -0.05651253089308739, 0.03257171809673309, -0.6287029385566711, 0.3757511079311371, -0.8091493248939514, -0.5250852108001709, 0.841116726398468, 0.8272016644477844, -0.2479439228773117, -0.6673932075500488, -0.6552799940109253, 0.18405358493328094, -0.3466428816318512, 0.3514634668827057, -0.6787675023078918, 0.07569659501314163, 0.3360610008239746, 0.799527645111084, 0.4373190999031067, -0.09224851429462433, 0.03560351952910423, 0.40396592020988464, 0.7973792552947998, 0.7280268669128418, -0.6419594883918762, -1.0869979858398438, 0.6575682759284973, 1.4344452619552612, -1.1074138879776, 0.3379853367805481, 0.022765228524804115, -0.2785857319831848, 0.6305591464042664, 0.4817870259284973, 0.29410868883132935, 0.9760111570358276, 0.1387692540884018, 0.4773046374320984, 0.304044634103775, -1.1098649501800537, 0.1769317090511322, 0.7514305114746094, 0.7600526213645935, 0.9806687235832214, 0.9434152841567993, 0.002188944024965167, 0.7788782715797424, 0.3120918273925781, 0.3172067403793335, 0.6451955437660217, 0.773637056350708, -0.6264204382896423, -0.11952903866767883, -0.07777746766805649, 0.8896416425704956, -0.39811453223228455, -0.8234562277793884, 0.04328633472323418, 0.5409351587295532, -0.028414906933903694, 0.7445281147956848, 0.8021878004074097, 0.26914316415786743, 0.327649861574173, 0.47629281878471375, 0.3472312390804291, -0.7381640076637268, 0.1718541979789734, -0.2087828516960144, -0.06922939419746399, -0.16364961862564087, -0.03444097936153412, -0.5558750033378601, -0.16583606600761414, -0.683579683303833, 0.2080082893371582, -0.06182516738772392, 0.20874470472335815, 0.9870808720588684, 0.9179721474647522, -0.2062564641237259, -0.3224623501300812, -0.12042734771966934, -0.6442840695381165, -1.04776930809021, -0.12449735403060913, -0.5358688831329346, -0.31620505452156067, 0.18848830461502075, 0.07898923009634018, -0.48512914776802063]}, "authors": [{"authorId": "2302327872", "name": "Pai Zeng"}, {"authorId": "2302322712", "name": "Zhenyu Ning"}, {"authorId": "2302350607", "name": "Jieru Zhao"}, {"authorId": "2302334566", "name": "Weihao Cui"}, {"authorId": "2302362588", "name": "Mengwei Xu"}, {"authorId": "2286749413", "name": "Liwei Guo"}, {"authorId": "2280208638", "name": "Xusheng Chen"}, {"authorId": "2280137293", "name": "Yizhou Shan"}], "references": [{"paperId": "6d227a30452f773cea678fa8872ed43566c4f394", "title": "Capabilities of Gemini Models in Medicine"}, {"paperId": "5be7e6b04c5a240cff340034aae2b57c677e211f", "title": "A Survey on Efficient Inference for Large Language Models"}, {"paperId": "eb06e95dd3eb5a916e52d2e463f474ef4967d8ca", "title": "LoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism"}, {"paperId": "3fd5bc3077d04965eaa3498372c39bbdd09d55e4", "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"}, {"paperId": "e61fde1309a9f5aab2060ace6f709711823c9ca5", "title": "Understanding Emergent Abilities of Language Models from the Loss Perspective"}, {"paperId": "3d45fc603e34934fc589b9547307815f7723de34", "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression"}, {"paperId": "cf93c04b73d50ba097151ee3e2a9f47fda4a8525", "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences"}, {"paperId": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6", "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference"}, {"paperId": "275b005c33a315ad603f236cd5766efe07ef6a54", "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding"}, {"paperId": "c9603ec967879c24973b5bd48861df2e5555932e", "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens"}, {"paperId": "ef1b02dc1b82f9955fc4760fcefd92c0fff9f227", "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference"}, {"paperId": "f395f022548d1d1f11e231f42d4852dbff5e9376", "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference"}, {"paperId": "26e13e1da4f47c93c9ad0daf9cc9e2bb4ffd063d", "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory"}, {"paperId": "2e8ca21114ecefac88fd2b3a2daacae352b1907f", "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models"}, {"paperId": "21e53e51ff77a5f34f43cb8ca029909c3ad9f71e", "title": "Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads"}, {"paperId": "cfabacfc676ea8804b5acbab169f2df5e5866d4d", "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models"}, {"paperId": "23b09ed66024fdd04d6713b9ba621b866f033d20", "title": "The What, Why, and How of Context Length Extension Techniques in Large Language Models - A Detailed Survey"}, {"paperId": "ee802ccb7fc3a322b824310ae6f29fc6a1e4314b", "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"}, {"paperId": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey"}, {"paperId": "36697944858ab17ca23b23ae2043aa6c0b2e3d5d", "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention"}, {"paperId": "eb95c327260725498404eb43ec370d419b8d92c7", "title": "SGLang: Efficient Execution of Structured Language Model Programs"}, {"paperId": "3597c2eff349260c59e7cb2bff31a31db5cf50fc", "title": "Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use"}, {"paperId": "ade22704be8a0fc3730d320cc7934b2ccbcd97e4", "title": "Striped Attention: Faster Ring Attention for Causal Transformers"}, {"paperId": "c0a822ac56e47b6e4b99d5552d2998647abe8234", "title": "Levels of AGI for Operationalizing Progress on the Path to AGI"}, {"paperId": "9529e50807f36acf3d2e4af994b5803c47e4746a", "title": "Atom: Low-bit Quantization for Efficient and Accurate LLM Serving"}, {"paperId": "908dad62c0e43d80e3e3cb3c0402f7c71c70499c", "title": "MemGPT: Towards LLMs as Operating Systems"}, {"paperId": "4c0428917aeee6aa7bd434f337d039f35996b736", "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"}, {"paperId": "2392b6d3a5cad9e5cf349169eaeee848266adf6a", "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models"}, {"paperId": "1266477120913d274346b044b4cc72ea893b1382", "title": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"}, {"paperId": "02ad9f3fefe33cb9ca546591bec65dbdf7766c80", "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "ff01d3dab60dd4b7426c884b009dda83540c0c1e", "title": "Attention Sorting Combats Recency Bias In Long Context Language Models"}, {"paperId": "a51ac7a5e8f6454268ac16ecdc52ecac98ce54d9", "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"}, {"paperId": "73290ecbec2f38d1d647ddef1ada69cee41725b3", "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training"}, {"paperId": "0c72450890a54b68d63baa99376131fda8f06cf9", "title": "The Rise and Potential of Large Language Model Based Agents: A Survey"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "a7fc585cc4c2b6822646b2c410e0c427a20798f2", "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models"}, {"paperId": "338d8f3b199abcebc85f34016b0162ab3a9d5310", "title": "A Survey on Model Compression for Large Language Models"}, {"paperId": "703035b483c181953de1b55b5fd59cd4cd4cf211", "title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "60b0476a97c00e355df28ba35422764a7fbe88e8", "title": "In-context Autoencoder for Context Compression in a Large Language Model"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "3f5e63168d0ae1af41c3434e9e3e7e84dda9a5d8", "title": "FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "2f7364d8e5cf94315bf8905f57de9c5543e9a4bf", "title": "Adapting Language Models to Compress Contexts"}, {"paperId": "fce42753155280051ac64817404b4e1d3be6ebaa", "title": "MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks"}, {"paperId": "0a5af6c39fe47901e8a69ec538d6ebb95a30a23a", "title": "Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "5278a8eb2ba2429d4029745caf4e661080073c81", "title": "Generative Agents: Interactive Simulacra of Human Behavior"}, {"paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f", "title": "Self-Refine: Iterative Refinement with Self-Feedback"}, {"paperId": "0671fd553dd670a4e820553a974bc48040ba0819", "title": "Reflexion: language agents with verbal reinforcement learning"}, {"paperId": "a7d298cbbac81bd47396edbf6a97fbc1fa0cf5c6", "title": "AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference With Transformers"}, {"paperId": "68adb03744692247fb834406798894db9fe77010", "title": "A Survey on Long Text Modeling with Transformers"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "9b069ba5259d229bfd4fe3ac3768148e2d1092f8", "title": "ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention"}, {"paperId": "4afda39036206dcb3f97829dccb897f1fc80f459", "title": "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "cbff35378657225ece138c33e6a23afb3b46b41f", "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "d84b292c2e90d0b0edfedc33141d305d8e9de5df", "title": "DTQAtten: Leveraging Dynamic Token-based Quantization for Efficient Attention Architecture"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "c67b1a62b868a758791c88d5465c7b6d53510fc3", "title": "Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "3563f56be0cca8d3769b94604a40cfa11ee7335a", "title": "Continuous"}, {"paperId": "16e623059ffccab60f4c35be028a2d4f10933515", "title": "Sequence Parallelism: Long Sequence Training from System Perspective"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "67ee20536c30a225b86902af2f091e28e5e19b40", "title": "Memformer: A Memory-Augmented Transformer for Sequence Modeling"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "168fc3525f7b97695a97b04e257ee9bd1e832acb", "title": "Memory Transformer"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "7945684818786fcb32cf92bace2566d7d6bc8945", "title": "LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation"}, {"paperId": "ec1f582446aa24f3f0920123ee6f05feea0b5e0a", "title": "Online normalizer calculation for softmax"}, {"paperId": "dc48bc1a4d81e0f37603013fd2a95644dc233bd0", "title": "Functional Interpolation for Relative Positions Improves Long Context Transformers"}, {"paperId": "adc8b62fd2bd644c140c7c42275a9d2d913ad8a8", "title": "Blockwise Parallel Transformers for Large Context Models"}, {"paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af", "title": "Are Emergent Abilities of Large Language Models a Mirage?"}, {"paperId": "4747e72c5bc706c50e76953188f0144df18992d0", "title": "Communicative Agents for Software Development"}, {"paperId": "9d7a75601e0e50dd68d40cfb8ef0e891dad797a6", "title": "Orca: A Distributed Serving System for Transformer-Based Generative Models"}, {"paperId": "619e1ab052bcdb0f95334c5d6b1ebd04a6b3b857", "title": "CAP Theorem"}, {"paperId": null, "title": "Model tells you what"}, {"paperId": null, "title": "THE AI INDEX REPORT"}, {"paperId": null, "title": "The shift from models to compound ai systems"}, {"paperId": null, "title": "Hardware-software co-design enabling static and dynamic sparse attention mechanisms"}, {"paperId": null, "title": "Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"}, {"paperId": null, "title": "H 2 o: Heavy-hitter oracle for efficient generative inference of large"}, {"paperId": null, "title": "Vision transformer acceleration"}]}