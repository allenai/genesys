{"paperId": "992ec0118708752892dadd07eb37c41446b122cf", "title": "Quantized Distributed Training of Large Models with Convergence Guarantees", "abstract": "Communication-reduction techniques are a popular way to improve scalability in data-parallel training of deep neural networks (DNNs). The recent emergence of large language models such as GPT has created the need for new approaches to exploit data-parallelism. Among these, fully-sharded data parallel (FSDP) training is highly popular, yet it still encounters scalability bottlenecks. One reason is that applying compression techniques to FSDP is challenging: as the vast majority of the communication involves the model's weights, direct compression alters convergence and leads to accuracy loss. We present QSDP, a variant of FSDP which supports both gradient and weight quantization with theoretical guarantees, is simple to implement and has essentially no overheads. To derive QSDP we prove that a natural modification of SGD achieves convergence even when we only maintain quantized weights, and thus the domain over which we train consists of quantized points and is, therefore, highly non-convex. We validate this approach by training GPT-family models with up to 1.3 billion parameters on a multi-node cluster. Experiments show that QSDP preserves model accuracy, while completely removing the communication bottlenecks of FSDP, providing end-to-end speedups of up to 2.2x.", "venue": "International Conference on Machine Learning", "year": 2023, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2302.02390", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "To derive QSDP, a variant of FSDP which supports both gradient and weight quantization with theoretical guarantees, is simple to implement and has essentially no overheads, it is proved that a natural modification of SGD achieves convergence even when it only maintains quantized weights."}, "embedding": {"model": "specter_v2", "vector": [0.06329113990068436, 0.5004304051399231, -0.2108808010816574, 0.05288796126842499, -0.803297758102417, 0.3744830787181854, 0.36381348967552185, -0.018971964716911316, -0.976067066192627, -0.14801180362701416, 0.24407893419265747, -0.028188267722725868, 0.3155241012573242, -0.20658040046691895, -0.38238027691841125, -0.0756237730383873, -1.2476428747177124, 0.480624794960022, 0.22358888387680054, -0.4798167645931244, -0.3041125535964966, -0.5547181367874146, -1.0859448909759521, -0.1390577256679535, 0.44635289907455444, 1.0102211236953735, -0.040075186640024185, 1.152161955833435, -0.31635409593582153, -0.09508929401636124, 0.22724592685699463, -0.0221971794962883, 0.5503358244895935, -0.01457363460212946, -0.2598319351673126, -0.26167336106300354, 0.0665786936879158, -0.6385738253593445, -0.35666877031326294, 1.054538607597351, -0.01506710983812809, 0.3095521032810211, 0.06790931522846222, -0.9065036177635193, 0.10534469783306122, 0.005210817791521549, 0.24809247255325317, 0.6992027163505554, -0.7228776812553406, -0.820819079875946, 0.9054197669029236, -1.1296910047531128, 0.04378494620323181, 1.5270352363586426, 0.6259379386901855, 0.5157334804534912, -0.5159644484519958, -0.4578264355659485, 0.2879471778869629, -0.28313493728637695, -0.9717298150062561, -0.2427891492843628, -0.522169291973114, 0.17091111838817596, 1.8803163766860962, -0.128785640001297, -0.044217485934495926, -0.054680515080690384, 0.017992092296481133, 0.837542712688446, -0.04562264308333397, -0.6922588348388672, 0.39048513770103455, -0.3647109866142273, 0.19878150522708893, 0.9187273979187012, -0.04930322989821434, 0.4945135712623596, -1.1699827909469604, -0.5966589450836182, 0.008801238611340523, 0.22974583506584167, 0.39857217669487, -0.4456307888031006, 0.21611538529396057, 0.9093613028526306, 0.36704859137535095, -0.10298464447259903, -0.28030499815940857, 1.2792876958847046, 0.9154483079910278, 0.3383510708808899, 0.8180211782455444, -0.20148377120494843, -0.339536190032959, -0.3028423488140106, -0.9395147562026978, 0.3625294268131256, 0.39169275760650635, 0.5516260266304016, -0.02387440763413906, 0.08904380351305008, -0.2193305939435959, 0.43335115909576416, 1.0576704740524292, 0.5617561340332031, 0.3784825801849365, -0.39409133791923523, 0.48972147703170776, -0.6535555124282837, -0.13803736865520477, -0.5616199970245361, -0.27800336480140686, -0.008012557402253151, -1.2681894302368164, -0.8602576851844788, -1.2724050283432007, -0.3733848035335541, -0.6103672385215759, 0.5545631051063538, -0.02491237223148346, 0.5312080979347229, 0.17586596310138702, 0.8236435055732727, 0.004595405887812376, 1.0744988918304443, 0.05502868816256523, 0.15756037831306458, 0.8424716591835022, -0.7348297834396362, -0.3619648218154907, -0.7879380583763123, 0.5165197253227234, -0.006100773345679045, 0.07626520842313766, -0.06801706552505493, -1.533270001411438, -0.8856293559074402, -0.8334469199180603, 0.18565423786640167, -0.21451610326766968, -0.41977399587631226, 0.8129236102104187, 0.4186645746231079, -1.2232997417449951, 1.3125797510147095, -1.0093278884887695, -0.1092093214392662, 0.9054465889930725, 0.9532159566879272, 0.3476479947566986, -0.20749545097351074, -0.6751654744148254, 0.02953820303082466, 0.13055108487606049, -0.980192244052887, -0.008063425309956074, -0.762310802936554, -0.5850300192832947, 0.11432819813489914, 0.0680854469537735, -0.7034108638763428, 1.6121010780334473, 0.09331603348255157, -1.1257266998291016, 0.2755584716796875, -0.21023115515708923, -0.3372458219528198, 0.4461917579174042, -0.206285759806633, -0.3422148525714874, 0.02982821688055992, -0.447381854057312, 0.5642658472061157, 0.6564801335334778, -0.038213517516851425, -0.42105594277381897, -0.13678966462612152, -0.7966449856758118, 0.05651162937283516, -0.9405202269554138, 0.723516047000885, -0.7644422054290771, -0.2453838735818863, 0.30069881677627563, 0.4490465223789215, -0.37052544951438904, 0.14227378368377686, -0.37132349610328674, -0.8886071443557739, 1.0235968828201294, 0.036882638931274414, 0.6404234170913696, -1.1450122594833374, -0.6180800795555115, 0.23497381806373596, 0.0362674780189991, 0.05256638675928116, -0.523043692111969, 0.6227969527244568, -0.1303502917289734, 0.5784035325050354, -0.23760196566581726, -1.3736225366592407, 0.4517713487148285, 0.3214869499206543, -0.774867832660675, -0.07866732031106949, 0.01772514171898365, 0.826056718826294, -0.601791501045227, 0.16621087491512299, -0.4508788287639618, 0.3725030720233917, -1.1732121706008911, 1.2344788312911987, -0.6451979875564575, -0.03923199698328972, -0.0228554829955101, -0.6137765645980835, 0.5039156079292297, -0.2523944675922394, 0.7133638858795166, -0.25794893503189087, 0.09754303842782974, 0.613178014755249, -0.5671982765197754, 1.2351651191711426, -0.4153229892253876, 0.014790089800953865, 0.14477406442165375, -0.8518316745758057, 0.31274524331092834, 0.25236138701438904, 0.19871313869953156, -0.7750641107559204, 0.24216178059577942, 0.47382715344429016, -0.7783545851707458, 0.5672632455825806, 0.9431246519088745, 0.7034764885902405, -0.3945218324661255, 0.6531382203102112, 0.4552811086177826, -0.524269700050354, 0.5566197633743286, 0.46094685792922974, 0.4827633202075958, 0.39521586894989014, 0.19796468317508698, -0.18941378593444824, -0.18394769728183746, -0.7360103726387024, -0.22762218117713928, 0.31959837675094604, 0.609695315361023, 0.6914955377578735, 0.5952168107032776, -0.944640576839447, -0.9111458659172058, 0.06801553070545197, 0.611864447593689, 1.4817901849746704, -0.11149051785469055, -0.12365278601646423, -0.7084269523620605, -0.46638768911361694, -0.060773637145757675, -0.28990158438682556, 0.04472929239273071, -0.14080128073692322, -0.5679780840873718, -1.3710689544677734, 0.9229040145874023, 0.39065834879875183, 1.1223640441894531, 0.2236744910478592, -0.3612406551837921, -0.5937089323997498, 0.8375194668769836, -0.5556451678276062, -0.4701266288757324, 0.8952301144599915, -0.8430741429328918, 0.05324377492070198, 0.02104727178812027, 0.006953263655304909, 0.2869509756565094, -0.5536564588546753, 0.9964378476142883, -0.2245979756116867, -0.240901380777359, -0.2154078334569931, 0.5737923383712769, -1.0923724174499512, -0.7725561857223511, 0.32817596197128296, 0.2615513205528259, -0.5407297611236572, 0.10853099822998047, 0.35705244541168213, 0.1798630803823471, -0.029500722885131836, -0.5823256373405457, 0.3852708041667938, -0.27000635862350464, 0.0983186736702919, 0.7486456036567688, -0.19057440757751465, 0.10368716716766357, -1.2101798057556152, 1.1348974704742432, 0.037845395505428314, -0.6929581165313721, -0.01940341293811798, -0.5748887658119202, 0.10746898502111435, 0.605060338973999, -0.3486570417881012, -0.06951957195997238, -1.0065759420394897, -0.2601379156112671, -0.7317913174629211, -0.24116632342338562, -0.09811877459287643, 1.1341969966888428, -0.12926530838012695, 0.36818400025367737, 0.2621428370475769, 1.04519522190094, -0.2008683979511261, 0.30994054675102234, -0.7408221960067749, 0.34980183839797974, -0.13768622279167175, 0.16619090735912323, 0.09835473448038101, 0.2672860026359558, -0.7453012466430664, -0.1544627696275711, -0.2213021069765091, -0.12776848673820496, 0.17416004836559296, 0.2113715559244156, -0.594112753868103, -0.8897728323936462, -0.4987644851207733, -0.996993899345398, -0.6678574085235596, -0.1920241117477417, -0.10367424786090851, -0.16184720396995544, -0.8828646540641785, -1.6410208940505981, -0.27647799253463745, -1.2206188440322876, -0.9268425107002258, 0.7501790523529053, -0.028668448328971863, -0.3135550022125244, -0.5111261010169983, 0.012736176140606403, -1.0543997287750244, 1.035995602607727, -0.6326203942298889, 0.9277161359786987, -0.07677434384822845, 0.17412397265434265, -0.19135765731334686, -0.07567542791366577, 0.6315126419067383, -0.8710206747055054, 0.11581805348396301, -0.5936163067817688, -0.07567381858825684, -0.8201843500137329, -0.7677522301673889, 0.16571679711341858, 0.2337018847465515, 0.9837115406990051, 0.032578133046627045, -0.28273266553878784, 0.7539207339286804, 1.5788663625717163, -1.0323303937911987, -0.25168633460998535, -0.5547419190406799, 0.8447056412696838, -0.5094844698905945, -0.6907450556755066, 0.7920123338699341, -0.006931109353899956, 0.49978744983673096, 0.3407633900642395, -0.39914530515670776, -0.16890689730644226, -0.18044263124465942, 0.15348264575004578, 1.855707049369812, 0.5083821415901184, -0.19559048116207123, -0.7118054032325745, 0.1758156716823578, -1.2314319610595703, -0.2208610624074936, 0.6419466137886047, 0.8414162993431091, 0.07507620006799698, -0.37726855278015137, 0.15568286180496216, -0.08862332999706268, 0.41473445296287537, 0.2582160234451294, -0.5415552854537964, -0.9972158670425415, 0.2883080244064331, 0.9189388155937195, 0.8470579981803894, 0.20498619973659515, -0.17345429956912994, 0.0535992830991745, 14.60319709777832, 1.0951755046844482, -0.15420469641685486, 0.983110785484314, 0.7954548597335815, -0.06075137481093407, -0.11864233762025833, -0.6285737752914429, -1.0022746324539185, -0.007988371886312962, 1.6217119693756104, -0.12342429161071777, 0.5943373441696167, 0.30031269788742065, 0.04317151755094528, 0.15985657274723053, -0.3604795038700104, 0.7430909872055054, 0.38375625014305115, -1.461084246635437, -0.1250409483909607, 0.4247400164604187, 0.82767653465271, 1.197791576385498, 0.6695662140846252, 0.6753369569778442, 0.6484751105308533, -0.24062864482402802, 0.36235636472702026, 0.010878661647439003, 1.2578259706497192, -0.47255802154541016, 0.3836376965045929, 0.8367310762405396, -0.5689210891723633, -0.023325491696596146, -0.7659400105476379, -1.3738770484924316, 0.17930614948272705, 0.2572859227657318, -0.569365382194519, -0.17527036368846893, 0.026136284694075584, 0.6059225797653198, 0.5861835479736328, 0.2697739005088806, 0.09607250988483429, 0.7123470902442932, -0.8437615036964417, -0.045610763132572174, 0.0675698071718216, 0.2694896161556244, -0.3827936351299286, -0.19749026000499725, 0.18454158306121826, -0.28661683201789856, 0.4032316207885742, 0.26912757754325867, -0.6870643496513367, -0.20312727987766266, -0.07811036705970764, 0.19070345163345337, -0.17278948426246643, 0.47433897852897644, 0.7595403790473938, 0.021641243249177933, -0.7290786504745483, 0.6230959296226501, 0.697519838809967, -0.2109958976507187, -0.15974192321300507, 0.1885177195072174, 0.4225952923297882, -0.7772732973098755, -0.07582025229930878, 0.3650108277797699, -0.3448845446109772, -0.3143278658390045, -0.9015125632286072, -0.2186184674501419, 0.5455446243286133, -0.4139295816421509, -0.9952833652496338, 0.6389449834823608, -0.019583169370889664, -0.1661680042743683, 0.5282854437828064, -0.5242444276809692, -0.041804779320955276, 0.7808494567871094, -1.2212989330291748, 0.055928926914930344, 0.35919544100761414, -0.377407044172287, -0.5270151495933533, 0.2995564639568329, 1.6851615905761719, 0.40267330408096313, -0.6716580390930176, -0.18461792171001434, 0.26041752099990845, -0.23260581493377686, -0.41293177008628845, -0.6658198833465576, 1.1999577283859253, 0.3115842640399933, -0.15376855432987213, -0.26000964641571045, -0.5093796253204346, 0.16721361875534058, -1.0750700235366821, -0.4476590156555176, 0.4254552125930786, -0.07731319218873978, -0.33781614899635315, -0.69294273853302, -0.5332559943199158, 0.11838335543870926, 0.21416530013084412, 0.18127784132957458, 0.5423535108566284, 0.016189010813832283, -0.1386551707983017, -0.3582353889942169, -0.7698346376419067, -0.10756605863571167, 0.31777945160865784, -0.6223812699317932, -0.12729763984680176, 0.28922832012176514, 0.472163587808609, -1.232291340827942, -0.6971204280853271, -0.2873624563217163, -0.3572863042354584, -0.06747989356517792, 1.0089049339294434, -0.15572400391101837, 0.9495586156845093, 1.2561733722686768, -0.15867029130458832, -0.8115134239196777, 0.3909565806388855, -0.7471974492073059, -0.2565845549106598, -0.3971450626850128, 0.07601285725831985, -0.26881736516952515, 0.8074724674224854, 0.7197822332382202, -0.10014932602643967, -0.5511919856071472, -0.7282662391662598, -0.2269199788570404, -0.04437697306275368, -0.6691349148750305, 0.34617042541503906, -0.43010878562927246, -0.09858342260122299, -0.13157732784748077, 0.4882926642894745, 0.5886382460594177, 0.03118087723851204, -0.6603647470474243, 0.4894321858882904, 0.07367032021284103, -0.3017708361148834, -0.4489876925945282, -0.6463320851325989, -1.7991218566894531, 0.024926086887717247, -1.178792953491211, -0.26610293984413147, -0.6739853620529175, -0.542732834815979, -0.15586796402931213, 0.08142294734716415, -0.04524080082774162, 0.5010057091712952, 0.21077437698841095, -0.28469985723495483, -0.305428683757782, -0.7835755348205566, 0.9863716959953308, 0.65113365650177, -0.2378055900335312, 0.22254231572151184, -0.35697662830352783, 0.5578393936157227, 0.35985660552978516, 0.7709590792655945, -0.5700530409812927, -0.9930763244628906, -1.583235263824463, 0.35162612795829773, 0.09446511417627335, 0.11620145291090012, -0.9787737131118774, 0.7704305052757263, 0.5593052506446838, -0.3184141516685486, 0.05673474073410034, 0.4223819077014923, -0.8835424184799194, -0.7763652801513672, 0.7745279669761658, -0.6961764097213745, 0.2188420593738556, 0.45356976985931396, -0.2854654788970947, -0.3301117420196533, 1.068219542503357, -0.08260487765073776, -0.6614086031913757, -0.7622864842414856, 0.6577757000923157, -0.2982536554336548, 0.34626272320747375, -0.08264659345149994, 0.12834584712982178, -1.1294853687286377, -0.16639560461044312, -0.09898099303245544, 0.3239775598049164, -0.36697864532470703, 0.2698759436607361, 0.19181233644485474, -0.9095935821533203, 0.2515472173690796, 0.6501593589782715, -0.5197407007217407, 0.6101126670837402, 0.6827338337898254, 0.4096430242061615, -0.780963122844696, 0.3707795739173889, 0.2872045040130615, 0.21596021950244904, -0.7477858066558838, 0.04406311735510826, 0.703669548034668, -0.3250703513622284, -0.5182451605796814, 1.286916971206665, -0.8280748128890991, -1.211728811264038, 0.05929611623287201, -1.1826975345611572, 0.17035363614559174, -0.7047744393348694, 0.4512101709842682, 0.4688829481601715, 0.4392622411251068, 0.38674473762512207, -0.3248920440673828, 0.1624143421649933, 0.37616971135139465, -0.4036789536476135, 0.6617506742477417, -0.10270345211029053, -0.5111479759216309, 0.5472874045372009, 1.1201763153076172, -0.8584499955177307, -1.0206191539764404, -1.0153239965438843, -0.7338271737098694, -0.19486352801322937, 0.5224601030349731, -0.14299649000167847, -0.7886295318603516, 0.7800635099411011, 0.6444757580757141, 0.562217116355896, 0.27421891689300537, -0.35716676712036133, 0.6719412803649902, 0.3999195396900177, 0.29903310537338257, -0.6091111302375793, -0.620532751083374, 1.056654691696167, 0.7665793299674988, -0.6477956771850586, 0.5875803232192993, -0.4407598674297333, -0.710543155670166, 0.9500164985656738, 0.08628587424755096, -0.37993818521499634, 1.052257776260376, 0.3437175154685974, -0.16987483203411102, 0.04015675187110901, -1.281447172164917, -0.10485439002513885, 0.6988630890846252, 0.6536144018173218, 0.33242854475975037, 0.41950303316116333, 0.10343385487794876, 0.8228773474693298, -0.014519386924803257, 0.2449508160352707, -0.0799388587474823, 0.6071087121963501, -0.17960838973522186, -0.16787093877792358, -0.13158747553825378, 0.9138813614845276, -0.7545825839042664, -0.3495263159275055, 0.5667463541030884, 0.35338732600212097, 0.3152662515640259, 0.44983258843421936, 1.3795746564865112, -0.12285412847995758, 0.5663016438484192, -0.13823363184928894, 0.32611942291259766, -0.6154975891113281, -0.40007227659225464, 0.10040424019098282, -0.5042113065719604, -0.20232383906841278, 0.010083390399813652, -0.05864132195711136, -0.8374330997467041, -1.0341089963912964, 0.6717587113380432, 0.21959297358989716, 0.7531828284263611, 0.8424335718154907, 0.8114320635795593, 0.9050875306129456, -0.061169106513261795, -0.6675611734390259, -1.113089919090271, -0.5347118973731995, -0.25219911336898804, -0.13980241119861603, -0.43609386682510376, 0.5108193755149841, 0.20183207094669342, -0.37202173471450806]}, "authors": [{"authorId": "40551712", "name": "I. Markov"}, {"authorId": "2869958", "name": "Adrian Vladu"}, {"authorId": "145461472", "name": "Qi Guo"}, {"authorId": "3311387", "name": "Dan Alistarh"}], "references": [{"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1", "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "e2df6ae1b3485449364ce2a5356ab09600fc3632", "title": "Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "d42c50b154dbc41cf869c702f9e4c40fd0cea4da", "title": "OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning"}, {"paperId": "6c673f92ac808d9ccbaf3fc35e8da9cd66caf847", "title": "Petals: Collaborative Inference and Fine-tuning of Large Models"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "6b117a8dcaa161562b0a69afbb9811e11afb5b3e", "title": "Decentralized Training of Foundation Models in Heterogeneous Environments"}, {"paperId": "b7a4c84f699d85716b7b26de29e832ec5f928ded", "title": "Fine-tuning Language Models over Slow Networks using Activation Compression with Guarantees"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "cba03d033bd65e99031eab8badd164d7a3f5126e", "title": "Iterative Hard Thresholding with Adaptive Regularization: Sparser Solutions Without Sacrificing Runtime"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "5b53e64259bf57c6f23921b48a0a5e89a3cbd6ff", "title": "CGX: adaptive system support for communication-efficient deep learning"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "a9bebc22b682906a9ebb4d4cce4b31d230131385", "title": "Adaptive Gradient Quantization for Data-Parallel SGD"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "cf58e9efbe072f44d1b2224f9dd6776f41d692cd", "title": "Distributed Learning Systems with First-Order Methods"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "c71da533053d79ad267b1d74814a43dda7c584fb", "title": "Dynamic Model Pruning with Feedback"}, {"paperId": "8f03e529dc7ce4dd036864045013223bcd133ec3", "title": "Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning"}, {"paperId": "73e728ef0905e48233270e5e151d2e00c7637801", "title": "Moniqua: Modulo Quantized Communication in Decentralized SGD"}, {"paperId": "962bdacb3941053788d69696826c9b4307652cce", "title": "Towards Unified INT8 Training for Convolutional Neural Network"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "3a73f042b7502842017459e3d2c376da993106af", "title": "Asynchronous Decentralized SGD with Quantized and Local Updates"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "596f0411d1bd2db054d9ba1a4521ad37ffbeba35", "title": "DoubleSqueeze: Parallel Stochastic Gradient Descent with Double-Pass Error-Compensated Compression"}, {"paperId": "b4a632a7097e7d0631250884dfc6e1f76b376996", "title": "PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization"}, {"paperId": "44b3b3bb40a9055eccdf86ea1702f6ae8b38934c", "title": "Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication"}, {"paperId": "7c22a6a07e89461178b794681c675b209332ee15", "title": "Error Feedback Fixes SignSGD and other Gradient Compression Schemes"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "42ec3db12a2e4628885451b13035c2e975220a25", "title": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"paperId": "b611636f3cfe7b9aa41a606bec1d9fa72e1359ae", "title": "ATOMO: Communication-efficient Learning via Atomic Sparsification"}, {"paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"}, {"paperId": "fae2a5101789afd51c1ececb28c75537c88734ec", "title": "Scalable Methods for 8-bit Training of Neural Networks"}, {"paperId": "a59eabc552bd9f5d503521d6ae4c14230d0aed1c", "title": "Decentralization Meets Quantization"}, {"paperId": "d8c09661b1bebfb690f0566167c87d64c5628d73", "title": "Demystifying Parallel and Distributed Deep Learning"}, {"paperId": "7f0d3991feb75f6f2fd50d0f2f80d820f20054cc", "title": "ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning"}, {"paperId": "cc1f98ec6fd3ae77468ee8adbe38ea0a445196e5", "title": "Communication Quantization for Data-Parallel Training of Deep Neural Networks"}, {"paperId": "c9d64aaa2007b60ef7814acc895dd90f15578a20", "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding"}, {"paperId": "07f5bae91cd45eafe82f3548a43268eb5c84df7a", "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\u0141ojasiewicz Condition"}, {"paperId": "3439a127e45fb763881f03ef3ec735a1db0e0ccc", "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs"}, {"paperId": "0da75214f4d1f636355bb0212262c5c5e55b746c", "title": "Iterative Thresholding for Sparse Approximations"}, {"paperId": null, "title": "Mosaic LLMs (part 2): Gpt-3 quality for <$500k"}, {"paperId": null, "title": "FairScale"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "UCI machine learning repository"}, {"paperId": null, "title": "2017] is provided in terms of the `2 norm of the vector, but pays an additional"}, {"paperId": "9ee76c41dd161df75cb50ac06d2868afec63b0db", "title": "Scalable distributed DNN training using commodity GPU cloud computing"}, {"paperId": "2002ac9bbffa3c0970ecd442bd7bf9f44bf5f2f7", "title": "Sparse Recovery Algorithms: Sufficient Conditions in Terms of RestrictedIsometry Constants"}, {"paperId": "fbc6562814e08e416e28a268ce7beeaa3d0708c8", "title": "Large-Scale Machine Learning with Stochastic Gradient Descent"}, {"paperId": null, "title": "tc(8) Linux User's Manual"}, {"paperId": null, "title": "Mosaicml examples"}]}