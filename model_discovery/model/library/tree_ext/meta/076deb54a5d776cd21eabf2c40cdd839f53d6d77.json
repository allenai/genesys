{"paperId": "076deb54a5d776cd21eabf2c40cdd839f53d6d77", "title": "giMLPs: Gate with Inhibition Mechanism in MLPs", "abstract": "This paper presents a new model architecture, gate with inhibition MLP (giMLP).The gate with inhibition on CycleMLP (gi-CycleMLP) can produce equal performance on the ImageNet classification task, and it also improves the BERT, Roberta, and DeBERTaV3 models depending on two novel techniques. The first is the gating MLP, where matrix multiplications between the MLP and the trunk Attention input in further adjust models' adaptation. The second is inhibition which inhibits or enhances the branch adjustment, and with the inhibition levels increasing, it offers models more muscular features restriction. We show that the giCycleMLP with a lower inhibition level can be competitive with the original CycleMLP in terms of ImageNet classification accuracy. In addition, we also show through a comprehensive empirical study that these techniques significantly improve the performance of fine-tuning NLU downstream tasks. As for the gate with inhibition MLPs on DeBERTa (giDeBERTa) fine-tuning, we find it can achieve appealing results on most parts of NLU tasks without any extra pretraining again. We also find that with the use of Gate With Inhibition, the activation function should have a short and smooth negative tail, with which the unimportant features or the features that hurt models can be moderately inhibited. The experiments on ImageNet and twelve language downstream tasks demonstrate the effectiveness of Gate With Inhibition, both for image classification and for enhancing the capacity of nature language fine-tuning without any extra pretraining.", "venue": "arXiv.org", "year": 2022, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2208.00929", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "The experiments on ImageNet and twelve language downstream tasks demonstrate the effectiveness of Gate With Inhibition, both for image classification and for enhancing the capacity of nature language fine-tuning without any extra pretraining."}, "embedding": {"model": "specter_v2", "vector": [-0.04516889527440071, 0.6373977065086365, -0.21976244449615479, 0.16130724549293518, 0.5194571614265442, -0.20022433996200562, 0.6133275628089905, -0.47404608130455017, -0.706168532371521, -0.029779663309454918, 0.5191805958747864, -0.04251938685774803, 0.7969977855682373, -0.020395535975694656, -0.14393378794193268, 0.008283589966595173, -0.5481983423233032, 0.3553884029388428, 0.06700196117162704, -0.6665605306625366, 0.021724754944443703, -0.53630131483078, -0.8876709342002869, 0.29490897059440613, -0.0911199152469635, 0.560267984867096, 0.3998279273509979, 1.1590771675109863, -0.3787374496459961, 0.5881450176239014, 0.40492603182792664, 0.06220805272459984, 0.3091273903846741, 0.1066141277551651, -0.35810092091560364, -0.31596943736076355, 0.2658899426460266, -0.17985506355762482, -0.40719491243362427, 0.6978915929794312, -0.1610238403081894, 0.28439632058143616, 0.5421231389045715, -0.40833547711372375, -0.267226904630661, 0.47092193365097046, 0.36833012104034424, 1.120497703552246, -0.5167346000671387, 0.007414424791932106, 1.0953519344329834, -1.2108731269836426, 0.06941130757331848, 1.6641346216201782, 0.20824453234672546, 0.881661057472229, -0.11046164482831955, -0.7498136758804321, 0.9805333018302917, 0.11952618509531021, -0.7400369644165039, -0.059463877230882645, 0.18588435649871826, -0.16459494829177856, 2.188556432723999, -0.48591917753219604, -0.16154278814792633, 0.5352760553359985, 0.325985312461853, 1.3090877532958984, -0.16817030310630798, -0.9315758943557739, -0.46831774711608887, 0.2903691232204437, 0.27175799012184143, 0.8905831575393677, -0.47724881768226624, 0.6482468843460083, -0.8387706279754639, 0.15945498645305634, 0.9851849675178528, -0.449791818857193, 0.05770350620150566, 0.44825172424316406, -0.38367417454719543, 0.8754379153251648, 0.4734746813774109, 0.5922783613204956, -0.3319430351257324, 0.8930653929710388, 0.3002457916736603, 0.6501128077507019, -0.1556725800037384, 0.5485907793045044, -0.21357300877571106, 0.7140735387802124, -0.15913577377796173, -0.10707932710647583, 0.03886571153998375, 0.8120633959770203, 0.0330059640109539, 0.3078415095806122, -0.8092832565307617, -0.011258896440267563, 1.3960119485855103, -0.39881274104118347, 0.48883289098739624, -0.5782983899116516, 0.45729896426200867, -0.4399411380290985, -0.08140744268894196, -0.758764922618866, -0.2411961555480957, -0.5708121657371521, -0.7025485038757324, -0.6279218792915344, -0.21789629757404327, 0.004433735739439726, -0.9552210569381714, 0.8883646726608276, -0.3696763813495636, -0.20004776120185852, 0.09155197441577911, 0.3601745069026947, 0.45858389139175415, 0.550560712814331, 0.821223795413971, 0.6256530284881592, 1.0628482103347778, -1.1246788501739502, -0.4172706604003906, -1.3822238445281982, 0.4985170066356659, -0.23601467907428741, 0.3904919922351837, -0.29976993799209595, -0.6837342381477356, -1.1760435104370117, -0.8853490352630615, -0.04699026420712471, -0.5759794116020203, 0.31596454977989197, 1.1401644945144653, 0.24477161467075348, -1.0658599138259888, 0.6933902502059937, -0.2791464328765869, -0.44292768836021423, 0.2871971130371094, 0.5082134008407593, 0.3254487216472626, 0.09634672105312347, -1.4894431829452515, 0.3929959237575531, 0.7894114851951599, -0.2644159197807312, -0.2365802377462387, -0.6287075281143188, -0.7880359292030334, -0.012020840309560299, 0.11165569722652435, -1.0607643127441406, 0.8722590804100037, -0.643822193145752, -1.0102078914642334, 0.8620825409889221, -0.06115572154521942, 0.13854514062404633, 0.3000977039337158, -0.11104869842529297, -0.31523656845092773, -0.5527029037475586, -0.2914896607398987, 1.30043363571167, 0.49829351902008057, -0.03460141643881798, -0.272266149520874, 0.04683929309248924, -0.15453779697418213, 0.2468191683292389, -0.299970418214798, 0.8016775250434875, -0.47919976711273193, -0.23403586447238922, 0.2898389399051666, 0.7788046002388, 0.19570747017860413, -0.07484712451696396, -0.39353859424591064, -1.1801773309707642, 0.5933597683906555, -0.008654518984258175, 0.8731018304824829, -0.9331427216529846, -0.8681257963180542, -0.4800996780395508, -0.18802662193775177, -0.221405029296875, -1.039176106452942, 0.3574018180370331, -0.35121679306030273, 0.5887954831123352, 0.20511414110660553, -0.9113956689834595, -0.25840237736701965, -0.04144679009914398, -0.3969031274318695, -0.42687445878982544, 0.6515272259712219, 1.1162649393081665, -0.8331759572029114, -0.08091343194246292, -0.3015671968460083, 0.02888590097427368, -1.2232929468154907, 0.9834786653518677, -0.46281349658966064, 0.47837600111961365, -0.25298506021499634, 0.00672405818477273, -0.18166540563106537, -0.0968463271856308, -0.11480815708637238, -0.5232946872711182, -0.03175535053014755, 0.48356378078460693, -0.4576939642429352, 1.2342841625213623, -0.39482080936431885, 0.795750081539154, 0.08703885972499847, -0.49133479595184326, 0.07933437824249268, 0.4422547519207001, -0.6767322421073914, -0.1787792146205902, 0.4119049310684204, 0.1593833714723587, -0.15110673010349274, 0.3159145414829254, 0.4153255224227905, 0.8014342188835144, -0.2781252861022949, -0.014873100444674492, 0.9094424843788147, -0.3133198618888855, 0.4316222667694092, 0.18912145495414734, 0.34685218334198, 0.38009554147720337, 0.5479094982147217, -0.2937971353530884, 0.5101512670516968, -0.7618488669395447, 0.18472033739089966, 0.7625166177749634, 0.3210480511188507, 0.9972609877586365, 0.49798285961151123, -0.8804035186767578, -0.07017090171575546, -0.00920459721237421, 0.6815084218978882, 1.5992305278778076, -0.4113350510597229, 0.41752222180366516, -0.5988108515739441, -0.34549814462661743, -0.3171272873878479, -0.11964860558509827, -0.5654098391532898, -0.26854807138442993, -0.40119045972824097, -1.2427767515182495, 0.8803116679191589, 0.33572080731391907, 1.2343239784240723, -1.1817588806152344, 0.11050626635551453, -0.09754840284585953, 0.4836185872554779, -0.6442549824714661, -0.678816020488739, 0.3872303068637848, -0.7092432975769043, -0.07943038642406464, 0.3586243987083435, -0.5777673721313477, 0.2237708568572998, -0.9016740322113037, 1.1419265270233154, -0.3930213451385498, -0.24981023371219635, -0.1047375276684761, 0.7989830374717712, -0.8698249459266663, -0.9050766229629517, 0.7252480983734131, 0.12929023802280426, -0.04253610968589783, 0.5600144267082214, 0.41170886158943176, -0.07135198265314102, 0.16743776202201843, -0.5682379007339478, 0.18623492121696472, 0.2501187324523926, 0.07287117838859558, 0.8415866494178772, -0.29872703552246094, 0.5986137390136719, -1.2730121612548828, 1.051703691482544, 0.28150901198387146, -0.4911734461784363, -0.07537005841732025, -0.34911274909973145, -0.152936190366745, 0.5010576844215393, -0.5446158647537231, -0.42631852626800537, -0.733394980430603, 0.2744584381580353, -0.18833933770656586, -0.3545219600200653, 0.3482181429862976, 0.13902166485786438, -0.18392105400562286, 0.7721899747848511, 0.20392988622188568, 0.23506595194339752, -0.028698962181806564, 0.5586171746253967, -1.0239074230194092, 0.4806414544582367, 0.31854701042175293, -0.013987494632601738, -0.03685222938656807, -0.3896827697753906, -0.7871372103691101, -0.6670364737510681, -0.4969191551208496, -0.22450047731399536, -0.2952039837837219, 0.3617558181285858, -0.6011070609092712, -0.798805832862854, 0.07830554246902466, -0.9983934760093689, -0.2491994947195053, -0.04289668798446655, 0.03576958552002907, -0.1198846623301506, -1.2759331464767456, -1.2334328889846802, -0.1816006302833557, -0.431540846824646, -0.6411113739013672, -0.22565039992332458, 0.46459898352622986, -0.1420689821243286, -0.7436630129814148, -0.26042884588241577, -0.37313613295555115, 1.1440000534057617, -0.575442373752594, 0.9818345308303833, -0.029169125482439995, -0.09833718091249466, -0.20151717960834503, 0.15450185537338257, 1.0545141696929932, 0.014905447140336037, 0.04319142922759056, -1.0445564985275269, 0.44283103942871094, 0.08000157028436661, -0.33226117491722107, 0.7252586483955383, 0.30209147930145264, 0.546012282371521, 0.24774345755577087, -0.5309740900993347, 0.5874402523040771, 1.3038924932479858, -0.6989324688911438, 0.12319017946720123, 0.3947632610797882, 0.944834291934967, 0.3826698958873749, -0.4152606129646301, 0.20457977056503296, 0.12658120691776276, -0.307435005903244, 0.45942702889442444, -0.08455389738082886, -0.4797687530517578, -0.7002044320106506, 0.26307180523872375, 1.273596167564392, 0.10441602766513824, -0.13044486939907074, -1.0782265663146973, 0.8109228014945984, -1.1110942363739014, -0.35362058877944946, 0.7847123742103577, 0.6363975405693054, 0.4215635359287262, -0.23750391602516174, -0.8085970878601074, -0.3092065751552582, 0.6481096148490906, 0.4257243275642395, -0.5052679777145386, -0.8539260625839233, -0.19950658082962036, -0.0016941602807492018, 0.2975344657897949, 0.9822822213172913, -0.604392945766449, 1.0558191537857056, 14.716339111328125, 0.47084158658981323, -0.1345595121383667, 0.4586081802845001, 0.8144528269767761, 0.2807821035385132, -0.18439413607120514, -0.24231678247451782, -1.5361957550048828, -0.3226936161518097, 0.41036659479141235, 0.6226829290390015, 0.6814427375793457, 0.03917849808931351, 0.030954590067267418, 0.07299003005027771, -0.3706932067871094, 0.6037485003471375, 0.3125399649143219, -1.0147490501403809, 0.6450150609016418, -0.028480317443609238, 0.507945716381073, 1.0265434980392456, 0.46366748213768005, 1.010166883468628, 0.20732459425926208, -0.241542249917984, 0.2083052545785904, 0.22451022267341614, 0.513405442237854, 0.06617147475481033, 0.30968573689460754, 0.7065513730049133, -0.6962350010871887, -0.29046228528022766, -0.8419708609580994, -1.351440668106079, -0.1395902931690216, -0.17596842348575592, -0.4306066930294037, -0.7860510349273682, -0.27752798795700073, 0.5560891032218933, -0.03255880996584892, 0.13929884135723114, -0.8973948955535889, 0.7059914469718933, -0.27755263447761536, -0.06712273508310318, 0.34426823258399963, 0.3583812713623047, 0.5031127333641052, 0.04716700688004494, -0.4613989591598511, 0.21353264153003693, 0.03815757483243942, 0.8964514136314392, -0.7139349579811096, -0.3021339476108551, 0.049280427396297455, -0.300555557012558, -0.33804571628570557, 0.8422556519508362, 0.26303377747535706, 0.3996739089488983, -0.28623735904693604, 0.24250108003616333, 0.4885596036911011, 0.661950945854187, -0.2984284460544586, -0.10902012884616852, 0.38943517208099365, -0.30923476815223694, -0.02544792927801609, 0.47424817085266113, -0.44345322251319885, -0.5508347749710083, -0.7066022753715515, 0.2008017897605896, 0.25008606910705566, -0.6481772661209106, -0.8151763677597046, 1.0989261865615845, -0.3016572594642639, -0.15182650089263916, 0.4170745313167572, -0.8891114592552185, -0.6353111863136292, 0.9579347372055054, -1.779344081878662, -0.7246558666229248, 0.17995452880859375, -0.1271108090877533, -0.2736400365829468, -0.23675210773944855, 1.046729326248169, 0.005719962064176798, -0.42759275436401367, -0.0838124006986618, -0.2762601971626282, 0.02940000593662262, -0.17420834302902222, -0.786930501461029, 0.9606488347053528, 0.1534881591796875, -0.03915300965309143, -0.03932701051235199, -0.1093570739030838, 0.3735670745372772, -0.2408832609653473, -0.21378320455551147, 0.7423060536384583, -0.7327897548675537, -0.3732295036315918, -0.6850306391716003, -0.8145095705986023, 0.5137818455696106, 0.6423735022544861, -0.12947949767112732, 0.18555589020252228, 0.1506892442703247, -0.4342486560344696, 0.02119937166571617, -0.6351508498191833, 0.011609419248998165, 0.28239667415618896, -0.5004892945289612, -0.6149754524230957, -0.07964493334293365, 0.28161147236824036, -0.7622405886650085, -0.08411167562007904, -0.35925057530403137, -0.2692469656467438, 0.029057160019874573, 1.0714662075042725, -0.8559079170227051, 0.3823089301586151, 0.49247175455093384, -0.09308697283267975, -0.9705014824867249, -0.3909853398799896, -0.8857142329216003, 0.3268349766731262, 0.15627296268939972, 0.968558132648468, -0.8822903037071228, 0.4428638517856598, 0.5233702063560486, 0.05526812747120857, -0.3866003453731537, -0.5418447852134705, -0.23652584850788116, 0.16557899117469788, -0.6586582660675049, 0.09428378194570541, -0.26928430795669556, -0.6073800921440125, 0.31546661257743835, 0.8844479322433472, 0.5235025882720947, -0.3712475001811981, -0.8163444399833679, -0.10663039237260818, -0.08548196405172348, -0.29210272431373596, -0.6208277344703674, -0.4538891017436981, -1.5659199953079224, -0.1777672916650772, -1.1964997053146362, -0.20281250774860382, -1.0422061681747437, -0.6463848352432251, 0.20824816823005676, -0.5602022409439087, 0.3447984755039215, 0.08482909947633743, -0.20955874025821686, -0.15785764157772064, -0.2338162064552307, -0.15140283107757568, 0.8196803331375122, 1.0908098220825195, -0.7436427474021912, -0.38142767548561096, 0.21933697164058685, -0.3626091480255127, 0.6495253443717957, 0.7738730311393738, -0.5745885968208313, -0.6884600520133972, -1.5623581409454346, 0.5897960662841797, -0.4727425277233124, -0.26489678025245667, -0.9771444201469421, 0.9149860739707947, 0.5119615793228149, 0.10017362236976624, 0.11737320572137833, 0.27666187286376953, -0.48740965127944946, -1.026965618133545, 0.3131006062030792, -0.4666949212551117, -0.009233446791768074, 0.4552428424358368, -0.32495608925819397, -0.7252321243286133, 0.7467169761657715, 0.1333642601966858, -1.357999324798584, -1.0698267221450806, 0.4072519838809967, -0.7374705672264099, 0.20726655423641205, -0.4489346444606781, -0.32245904207229614, -0.9442782998085022, -0.10886096209287643, -0.4223223924636841, 0.7555636167526245, -0.7487983107566833, 1.0226280689239502, 0.24219286441802979, -1.0914136171340942, -0.17704026401042938, 0.675125241279602, -0.20350266993045807, 0.19610610604286194, 0.21462850272655487, 0.3697230815887451, -0.5422167778015137, 0.5281622409820557, 0.12551645934581757, 0.28544899821281433, -0.47336506843566895, -0.09576837718486786, 1.1095625162124634, -0.5447099804878235, 0.1741512268781662, 1.3979638814926147, 0.3700912892818451, -1.5596586465835571, 0.17943710088729858, -1.1345266103744507, -0.9272904396057129, 0.014316440559923649, 0.7180618643760681, -0.18313613533973694, 0.09455237537622452, 0.1232227087020874, -0.3460659682750702, 0.15879179537296295, -0.1390915960073471, -0.49200236797332764, 0.29881617426872253, -0.39139071106910706, -0.2577740252017975, 0.6525331735610962, 0.553700864315033, -1.0765842199325562, -1.0611821413040161, -0.615780770778656, -0.32036641240119934, 0.2495999038219452, 0.3402920365333557, -0.6810128092765808, -0.820745050907135, 0.9555838704109192, 0.3633907735347748, 0.20212599635124207, 0.386806845664978, -0.3599962592124939, -0.2861408591270447, 0.6585493683815002, -0.14604265987873077, -0.671864926815033, -0.49854448437690735, 1.7667200565338135, 1.54267156124115, -1.0153292417526245, -0.07438668608665466, -0.1994733214378357, -0.7152022123336792, 0.950213611125946, 0.7938311100006104, -0.5673278570175171, 1.035383939743042, -0.5918086171150208, 0.3489881753921509, 0.17028839886188507, -1.2015297412872314, -0.2447885125875473, 0.49570295214653015, 0.940591037273407, 0.5724188685417175, 0.09886888414621353, -0.11278389394283295, 0.7966564297676086, 0.1559818983078003, -0.1470717042684555, 0.5675849914550781, 0.27064377069473267, -0.31208258867263794, 0.035425737500190735, -0.2178127020597458, 0.546714723110199, -0.6429382562637329, -0.5597516298294067, -0.1399301290512085, 1.2153717279434204, 0.23436594009399414, 0.6813392639160156, 0.826131284236908, 0.19247426092624664, 0.6879338622093201, 0.23017549514770508, 0.49162304401397705, -0.8590689897537231, -0.6298686265945435, -0.15974868834018707, -0.8998687863349915, 0.09006285667419434, 0.1294364482164383, -0.3971012532711029, -0.3462689518928528, 0.031208055093884468, 0.19487440586090088, -0.343784898519516, 0.45325982570648193, 0.7983224987983704, 0.5365344285964966, 0.8623570799827576, -0.32874199748039246, -0.45127660036087036, -0.31538477540016174, -1.4386107921600342, 0.056355491280555725, -0.7161832451820374, -0.25471362471580505, -0.046430803835392, -0.4237051010131836, -0.3180222511291504]}, "authors": [{"authorId": "2112792311", "name": "C. Kang"}, {"authorId": "2179879413", "name": "Jindich Prokop"}, {"authorId": "1902093678", "name": "Lei Tong"}, {"authorId": "13211796", "name": "Huiyu Zhou"}, {"authorId": "2179981693", "name": "Yong Hu"}, {"authorId": "2179883732", "name": "Daneil Novak"}], "references": [{"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "972706306f85b1bfb40c7d35c796ad5174eb0c9c", "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"}, {"paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"}, {"paperId": "1921489a2801f053d0906079f0015ba6e68dd505", "title": "DIET-SNN: A Low-Latency Spiking Neural Network With Direct Input Encoding and Leakage and Threshold Optimization"}, {"paperId": "a9c214e846188adb645021cd7b1964b8ea1fef6f", "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer"}, {"paperId": "f75cddf2d42ed01b34686704eb3504becef67442", "title": "CycleMLP: A MLP-like Architecture for Dense Prediction"}, {"paperId": "a344e45d634a4c9140694e322a7dbb368c6a6b6a", "title": "TRESK channel contributes to depolarization-induced shunting inhibition and modulates epileptic seizures."}, {"paperId": "9b6af0e358e76d22f209c75b1702c3e6ea7815b1", "title": "Global Filter Networks for Image Classification"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "e3a3e85c5a32af29e13b3561f6cf070de70651de", "title": "Pay Attention to MLPs"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "445fd15f64650d2cfd58833ec6642235888ee584", "title": "Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69", "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "0093492360c7592d3c6d61f91b678c497486549c", "title": "Effective and Efficient Computation with Multiple-timescale Spiking Recurrent Neural Networks"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "1b7b8fb572274a1d48751f2fb18c12ee1f54e2cf", "title": "Spiking Neural Networks and Online Learning: An Overview and Perspectives"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "title": "Local Relation Networks for Image Recognition"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "eaaaed86d1b811fb4690e20ec532d4298c10e324", "title": "Enabling Spike-Based Backpropagation for Training Deep Neural Network Architectures"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "1a13bec308614ed56b9af2532a3d5e4cd8e2af94", "title": "Deep Learning in Spiking Neural Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "b79e5e4622a95417deec313cd543617b19611bea", "title": "Deep Learning using Rectified Linear Units (ReLU)"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "b20ace3f79c502b29e2e3e0a211880957652e622", "title": "Going Deeper in Spiking Neural Networks: VGG and Residual Architectures"}, {"paperId": "915cc4b359863f256957485c8a60f2cceb78ab5f", "title": "Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "424a6e62084d919bfc2e39a507c263e5991ebdad", "title": "Self-Normalizing Neural Networks"}, {"paperId": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "f63e917638553414526a0cc8550de4ad2d83fe7a", "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "ccf259641190567899e5003299d20dfcfc2c32f6", "title": "Visual input evokes transient and strong shunting inhibition in visual cortical neurons"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "008e8948401932435e54d1f1c5c7da05727fbbee", "title": "Global Filter Networks for Image Classification"}, {"paperId": null, "title": "2021a) MLP 15M 3.0G"}, {"paperId": null, "title": "Electra: Pre-training text encoders as discriminators rather than generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": null, "title": "Model Family Para FLPOs Scale Top-1"}]}