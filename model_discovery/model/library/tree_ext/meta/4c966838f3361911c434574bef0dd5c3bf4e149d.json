{"paperId": "4c966838f3361911c434574bef0dd5c3bf4e149d", "title": "LoCoCo: Dropping In Convolutions for Long Context Compression", "abstract": "This paper tackles the memory hurdle of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for Long Context Compression (LoCoCo). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward\"drop-in\"integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to 3482 tokens into a 128-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to 0.2791 compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": null}, "embedding": {"model": "specter_v2", "vector": [0.2787051200866699, 0.00012933617108501494, -0.4423423409461975, 0.08354496210813522, -0.4933055341243744, -0.23897390067577362, 0.5816813707351685, -0.05089070275425911, -0.8358076214790344, -0.3325709104537964, 0.5842779874801636, -0.1063535213470459, 0.5810598731040955, 0.5237071514129639, -0.08276005834341049, -0.053023066371679306, -0.8754804134368896, -0.17604689300060272, -0.11165376007556915, -0.043893467634916306, -0.18377868831157684, -0.7995225787162781, -1.0082498788833618, 0.3710967004299164, 0.6207767128944397, 0.7804222702980042, 0.5486367344856262, 1.1324548721313477, -0.6030768752098083, 0.320956826210022, 0.5421050190925598, -0.25103700160980225, 0.14564844965934753, 0.10816895961761475, 0.10945556312799454, -0.4674672782421112, 0.3294089734554291, -0.3661080300807953, -0.39444056153297424, 0.8473638892173767, -0.12147839367389679, 0.2522513270378113, 0.15375252068042755, -0.5052272081375122, -0.3036220073699951, 0.9351723194122314, 0.6406810283660889, 0.5994817614555359, -0.5176366567611694, -0.6185516715049744, 1.1907546520233154, -1.6178292036056519, -0.38991421461105347, 1.4499287605285645, 0.8831827044487, 0.537946343421936, -0.16287201642990112, -0.33937397599220276, 1.2079335451126099, 0.03457695245742798, -0.5614556074142456, -0.4660985767841339, -0.08643097430467606, 0.17565785348415375, 1.9959458112716675, 0.014555580914020538, 0.4597737491130829, 0.6793756484985352, 0.02489524520933628, 1.438963532447815, -0.4649807810783386, -1.0084069967269897, -0.021318281069397926, 0.025519350543618202, 0.3421558141708374, 0.4318264126777649, -0.2934444546699524, 0.6026683449745178, -0.5567660331726074, -0.24928328394889832, 0.17891596257686615, 0.21440252661705017, 0.5850197672843933, 0.3591301739215851, -0.04789113625884056, 0.614264965057373, 0.38595354557037354, 0.8659363985061646, -0.1625378429889679, 1.2881927490234375, 0.6403237581253052, 0.1735076904296875, 0.4397714138031006, 0.19100886583328247, -0.17095813155174255, -0.11608019471168518, -1.0801326036453247, 0.40501415729522705, -0.12820084393024445, 1.0574100017547607, -0.5352843403816223, 0.3264594078063965, -0.8228417634963989, 0.33745184540748596, 1.085087776184082, 0.03793107718229294, 0.3764026463031769, -0.6009429097175598, 0.3504522144794464, -1.1406123638153076, -0.145033061504364, -0.15999135375022888, 0.10114121437072754, -0.30218398571014404, -0.4774336814880371, -1.269889235496521, -0.783043384552002, 0.03542615473270416, -0.5763616561889648, 0.9637241363525391, -0.4427889585494995, 0.7245345115661621, 0.15907569229602814, 0.1644517183303833, 0.216037318110466, 0.26343515515327454, 0.6853429675102234, 0.07502930611371994, 1.0891563892364502, -1.4070336818695068, -0.2818949818611145, -1.2168140411376953, 0.8238393664360046, -0.6102971434593201, 0.41548994183540344, -0.4527185261249542, -1.0801706314086914, -1.0603976249694824, -0.8382883071899414, -0.22346171736717224, -0.8026995658874512, 0.09459017217159271, 1.033958911895752, 0.5613654255867004, -1.1021629571914673, 0.8284371495246887, -0.25354596972465515, 0.04821525886654854, 0.4179617762565613, -0.04205012321472168, 0.7044389843940735, -0.27521470189094543, -1.5577309131622314, 0.22739067673683167, 0.11468664556741714, -0.40550094842910767, -0.03481234982609749, -1.0131200551986694, -1.025815486907959, 0.16119809448719025, 0.2920437157154083, -0.5149574279785156, 1.555624008178711, -0.261808842420578, -0.7984009385108948, 0.29732179641723633, -0.40245547890663147, 0.17103993892669678, 0.31844446063041687, -0.7383026480674744, -0.7594894766807556, -0.46100249886512756, -0.25068435072898865, 0.7165391445159912, 0.5525678992271423, -0.003998244181275368, -0.5313199162483215, 0.45756956934928894, -0.14576783776283264, 0.23494872450828552, -0.16282814741134644, 0.9154466390609741, -0.4570971727371216, -0.40900230407714844, 0.10755827277898788, 0.692145824432373, -0.11246656626462936, -0.25349587202072144, 0.005267878528684378, -0.8146753311157227, 1.1878923177719116, -0.08137138932943344, 0.9532965421676636, -0.9061877131462097, -1.3760615587234497, -0.34938347339630127, -0.23654840886592865, 0.26591283082962036, -0.9071635007858276, 0.5170236825942993, -0.06933201849460602, 0.3918558359146118, 0.24174639582633972, -1.3164715766906738, 0.17773741483688354, -0.1256781667470932, -0.8019312620162964, -0.7095932960510254, 0.5396197438240051, 1.2675666809082031, -1.1725795269012451, -0.00021847733296453953, -0.03994356840848923, 0.6320167183876038, -1.2289376258850098, 1.0224932432174683, -0.5566614866256714, 0.11551808565855026, -0.0033972386736422777, 0.14643064141273499, 0.17344753444194794, 0.17738428711891174, 0.2660214602947235, -0.23502129316329956, -0.20794647932052612, 0.8189581632614136, 0.20765987038612366, 1.270761489868164, -0.7302175760269165, 0.48783183097839355, 0.00016263441648334265, -0.38571587204933167, -0.10482805967330933, 0.4980611801147461, -0.1894100159406662, -0.3501500189304352, 0.3940785527229309, 0.35267889499664307, -0.6470879912376404, 0.12038375437259674, 1.4086428880691528, 0.71272873878479, -0.695888876914978, -0.16774089634418488, 0.5428858995437622, -0.20101703703403473, 0.4146791100502014, 0.5564249753952026, 0.4457806348800659, 0.6037988662719727, 0.2096884846687317, 0.2348329722881317, 0.26124218106269836, -1.0032116174697876, -0.05506882444024086, 0.8374843597412109, 0.36602118611335754, 0.9032092690467834, 0.5584695339202881, -0.4752940237522125, -0.6533939838409424, 0.4564075469970703, 0.763276994228363, 1.5679491758346558, -0.24698075652122498, 0.06873293220996857, -0.9754969477653503, -0.5260232090950012, -0.1202770471572876, -0.07650521397590637, -0.19255991280078888, -0.14742302894592285, -0.5549699068069458, -0.6439403295516968, 1.220658540725708, 0.4299526512622833, 0.576026201248169, -0.7039229273796082, -0.08816947788000107, 0.10808758437633514, 0.1519489586353302, -1.1057647466659546, -0.7480102777481079, 0.6862602233886719, -0.5777891874313354, 0.15412354469299316, 0.3120751678943634, -0.12042397260665894, -0.0683131068944931, -0.4925052225589752, 0.9344643354415894, -0.2506456673145294, -0.4811957776546478, 0.07353826612234116, 0.17091505229473114, -0.5412316918373108, -0.871685266494751, 0.39821135997772217, 0.2146439403295517, -0.4406207501888275, 0.7047075033187866, 0.46087101101875305, 0.07513376325368881, -0.6488783955574036, -0.2187841236591339, 0.23421891033649445, 0.1490083634853363, 0.03764653950929642, 0.7532097697257996, -0.7964348793029785, 0.23699842393398285, -1.0439285039901733, 1.0039891004562378, -0.05731804296374321, -0.30874210596084595, 0.24659787118434906, -0.7608962655067444, -0.4604111313819885, 0.8308156728744507, -0.6728652715682983, -0.26540407538414, -1.1093415021896362, 0.029606128111481667, -0.6735311150550842, -0.4194395840167999, 0.09368965029716492, 0.2972164750099182, 0.458519846200943, -0.23447048664093018, 0.4766159653663635, 0.02693937160074711, -0.3836914002895355, 0.7936537861824036, -0.6308314204216003, 0.7402844429016113, -0.0316169410943985, -0.27349480986595154, -0.2527438700199127, -0.3534536361694336, -0.538327157497406, -0.28243526816368103, -0.5992739796638489, -0.4648740887641907, -0.43421903252601624, 0.16586890816688538, -0.6140118837356567, -0.6212167739868164, -0.30543628334999084, -0.972213625907898, -0.22810262441635132, -0.037183575332164764, -0.10282197594642639, -0.12053260207176208, -1.0484780073165894, -1.2078174352645874, -0.8857123255729675, -0.7498424649238586, -0.9233843684196472, 0.34493449330329895, 0.2322048395872116, -0.3555528223514557, -0.8627153635025024, -0.24373279511928558, -0.7447705268859863, 1.1569567918777466, -0.776016891002655, 0.5752639174461365, -0.02401062846183777, -0.04019352048635483, -0.3589448928833008, 0.02308172732591629, 0.66670161485672, -0.1499992161989212, 0.2698662579059601, -1.3244047164916992, 0.11530700325965881, -0.28263935446739197, -0.11756909638643265, 0.4723628759384155, 0.09928548336029053, 0.5216695666313171, -0.10111424326896667, -0.6628831028938293, 0.5622759461402893, 1.5441937446594238, -0.5101762413978577, -0.044349949806928635, -0.25185734033584595, 0.790479838848114, -0.3164944052696228, -0.01247088797390461, 0.33687523007392883, -0.2274942547082901, 0.27881383895874023, 0.08309703320264816, -0.04957219213247299, -0.32926246523857117, -0.6763056516647339, 0.4831151068210602, 1.860155701637268, 0.4003448784351349, 0.2725558578968048, -0.6320449113845825, 0.6743320226669312, -1.4154040813446045, -0.8437831997871399, 0.7216596007347107, 0.5908364057540894, 0.5345125794410706, -0.4733595848083496, -0.5130619406700134, -0.48896366357803345, 0.18312466144561768, 0.5201184749603271, -0.23136907815933228, -1.1708242893218994, 0.14529746770858765, 0.3065722584724426, 0.10706575959920883, 0.8492347002029419, -0.5920635461807251, 0.8607310652732849, 14.499141693115234, 0.9277901649475098, -0.3082757890224457, 0.6993839740753174, 0.6947525143623352, -0.18317578732967377, -0.42679572105407715, 0.042996957898139954, -1.440918207168579, 0.25330811738967896, 1.1573654413223267, 0.41305485367774963, 0.6016613841056824, 0.36688607931137085, 0.3382129669189453, 0.07101312279701233, -0.6302263736724854, 0.6079149842262268, 0.513701856136322, -1.0570846796035767, 0.13631783425807953, -0.12988226115703583, 0.5230696201324463, 0.668554961681366, 0.8909685015678406, 1.095277190208435, 0.3820520341396332, -0.31566938757896423, 0.7536910176277161, 0.38089221715927124, 1.084229588508606, -0.10991553962230682, 0.26413264870643616, 0.3041488826274872, -0.9646879434585571, -0.24073533713817596, -0.6197967529296875, -1.3026671409606934, 0.22945915162563324, 0.12183468043804169, -0.5308745503425598, -0.42267468571662903, -0.33099496364593506, 0.6080697774887085, -0.06478665769100189, 0.2908826470375061, 0.11745613068342209, 0.6890334486961365, -0.07870375365018845, -0.14914360642433167, 0.8219968676567078, 0.33358702063560486, 0.11412840336561203, 0.5376520752906799, -0.2200552225112915, -0.12501133978366852, 0.05843961983919144, 0.6077548265457153, -0.41598382592201233, -0.43405458331108093, -0.10008641332387924, -0.11021192371845245, 0.06517335027456284, 0.7483766674995422, 0.4953179657459259, -0.16020946204662323, -0.7587667107582092, 0.3443116247653961, 0.6749279499053955, 0.45606061816215515, -0.7119714021682739, 0.07959286868572235, 0.7625505328178406, -0.8995904326438904, 0.3828763961791992, 0.6410597562789917, -0.3624524474143982, -0.6806002855300903, -0.6020612120628357, 0.0025582551024854183, -0.15770000219345093, -0.5833234190940857, -0.5907160639762878, 0.7453786134719849, -0.11649265140295029, -0.3639194667339325, 0.11597780138254166, -0.4895067512989044, -0.4364261031150818, 0.6757332682609558, -1.5194447040557861, -0.6948844194412231, 0.3412167727947235, -0.5747870802879333, -0.3631163537502289, 0.1650765985250473, 1.1803276538848877, 0.6876388192176819, -0.1776973158121109, 0.3478320240974426, 0.4657226800918579, -0.0037629068829119205, -0.16449274122714996, -0.5058294534683228, 1.143554925918579, 0.43189117312431335, -0.1142863780260086, 0.09371829032897949, -0.17845730483531952, 0.10528284311294556, -0.29690462350845337, -0.3581719994544983, 0.9767814874649048, -0.46110278367996216, -0.5870268940925598, -0.9678190350532532, -0.598730206489563, 0.4339023530483246, 0.4176679849624634, 0.08249526470899582, 0.39230450987815857, 0.10703368484973907, -0.7095966935157776, -0.03637266904115677, -0.6915671825408936, 0.36600586771965027, 0.48379141092300415, -0.87281334400177, -0.06317924708127975, -0.40671372413635254, 0.7885174751281738, -1.064703106880188, -0.2984664738178253, -0.47768452763557434, 0.38807255029678345, -0.10641530901193619, 1.0783209800720215, -0.5064063668251038, 0.8075929284095764, 1.1749110221862793, -0.3740575313568115, -0.6053717136383057, 0.1248784065246582, -0.772800087928772, -0.2274939864873886, 0.3379567265510559, 0.475260466337204, -0.1956552267074585, 0.18399019539356232, 0.9378763437271118, -0.002859896281734109, -0.8225053548812866, -0.6715214848518372, -0.2697492837905884, 0.3991639018058777, -0.9901885390281677, 0.667316734790802, -0.1506369411945343, 0.20904111862182617, -0.22189143300056458, 0.4743993282318115, 0.2804968059062958, -0.28790637850761414, -1.0124850273132324, -0.06128556281328201, 0.35767656564712524, -0.0659625381231308, -0.7237414121627808, -0.5283768177032471, -1.4014625549316406, -0.08533191680908203, -0.9997942447662354, 0.026717159897089005, -0.374210000038147, -0.4318299889564514, -0.4867197275161743, -0.5263276100158691, -0.08132226020097733, -0.10534229129552841, -0.3311190605163574, 0.06424196809530258, -0.6171070337295532, -0.7075343132019043, 0.7027314305305481, 0.6924734711647034, -0.5503965616226196, 0.20994848012924194, 0.023347090929746628, 0.00829406175762415, 0.3060269057750702, 0.3945906460285187, -0.4028354287147522, -0.5095539689064026, -1.3210055828094482, 0.22015467286109924, -0.26044756174087524, -0.020734615623950958, -0.8590721487998962, 0.600737452507019, 0.3919161260128021, -0.020684191957116127, -0.14009197056293488, 0.38446691632270813, -1.0182304382324219, -0.8267397880554199, 0.24090392887592316, -0.9510279893875122, 0.1285959780216217, 0.5051094889640808, -0.5770220756530762, -0.2799232006072998, 0.463613361120224, -0.2062729448080063, -0.9272144436836243, -1.0610496997833252, 0.20319482684135437, -0.614899218082428, 0.06013296917080879, -0.5435543060302734, 0.11129677295684814, -0.9455015063285828, -0.2855966091156006, -0.15569396317005157, 0.06082636117935181, -0.5401991009712219, 1.2162405252456665, 0.4526882767677307, -1.4748039245605469, 0.12662449479103088, 0.6142233610153198, -0.23392361402511597, 0.015953587368130684, 0.533336341381073, 0.5658836364746094, -0.34659552574157715, 0.7407481074333191, 0.4337485432624817, 0.2679452896118164, -1.170859932899475, 0.2459123432636261, 0.6317777633666992, -0.6871563792228699, -0.10820920020341873, 1.0959758758544922, -0.5083996057510376, -0.9849686622619629, 0.16848117113113403, -1.6656173467636108, -0.3160035014152527, -0.439040869474411, 0.8423910737037659, 0.01219252310693264, 0.23442129790782928, 0.03846430033445358, -0.449220210313797, -0.11414951086044312, -0.2245970070362091, -0.7371978759765625, 0.1808777153491974, -0.43194112181663513, -0.1569465696811676, 0.3997427225112915, 1.3470722436904907, -0.6211448311805725, -1.284030556678772, -0.7181972861289978, -0.554776668548584, -0.29719099402427673, 0.2029305398464203, -0.3771543800830841, -0.42652609944343567, 0.6200469732284546, 0.4351995587348938, 0.2438470870256424, -0.05977732315659523, -0.23966021835803986, 0.384494423866272, 0.39884474873542786, 0.030270807445049286, -0.8014014363288879, -0.7007812261581421, 1.5559706687927246, 1.0542480945587158, -0.8632749319076538, 0.2254388928413391, 0.1309870481491089, -0.5851098299026489, 0.6389606595039368, 0.4157475233078003, -0.2284029722213745, 1.1193829774856567, 0.33818286657333374, 0.34904080629348755, 0.7233229875564575, -1.3493537902832031, -0.23290321230888367, 0.4548146426677704, 1.1127967834472656, 0.8993532657623291, 0.5331609845161438, 0.20514751970767975, 1.1233060359954834, 0.2607128918170929, -0.06737677752971649, 0.09844137728214264, 0.6365397572517395, -0.09894917905330658, -0.3020179271697998, -0.22949406504631042, 0.7495896816253662, -0.9099936485290527, -1.367194652557373, 0.4290303587913513, 0.4437420070171356, 0.6264853477478027, 0.6759926676750183, 1.2904940843582153, 0.024633612483739853, 0.2166459709405899, 0.5266691446304321, 0.33840394020080566, -0.3156522214412689, -0.7567939758300781, 0.09943633526563644, -0.7738495469093323, -0.02631554752588272, 0.14965276420116425, -0.6536595225334167, -0.34426283836364746, -0.4022638201713562, 0.29381465911865234, -0.2056104838848114, 0.3885287344455719, 1.033221960067749, 0.6003344058990479, 0.986725926399231, -0.5326946973800659, -0.5959018468856812, -0.38587072491645813, -0.8760050535202026, 0.039368972182273865, -0.2831786572933197, 0.024127131327986717, 0.17904533445835114, 0.15744167566299438, -0.24714721739292145]}, "authors": [{"authorId": "2209882676", "name": "Ruisi Cai"}, {"authorId": "2249538771", "name": "Yuandong Tian"}, {"authorId": "2284563898", "name": "Zhangyang Wang"}, {"authorId": "2249538643", "name": "Beidi Chen"}], "references": [{"paperId": "0595dac8260443365dfbe4821787419736baaa66", "title": "Extending LLMs' Context Window with 100 Samples"}, {"paperId": "a9468d8bfa6bd016dfd3128c4e8408e30eb8549b", "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning"}, {"paperId": "1be73fa3e856c33d0aed1d9e46693523e7fa3c60", "title": "Zoology: Measuring and Improving Recall in Efficient Language Models"}, {"paperId": "713806165610c237f551a7b68e6b09b3ded75502", "title": "SparQ Attention: Bandwidth-Efficient LLM Inference"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "cb0ac335adda4ceef9987cbcbca9129e71c37f0a", "title": "Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions"}, {"paperId": "a27dced654158b905c7447aae1aa294ebc8ecaf0", "title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer"}, {"paperId": "4c0428917aeee6aa7bd434f337d039f35996b736", "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "5e0cb1c4b91a7486e1c2b15a44a0be56bd74bdc0", "title": "Effective Long-Context Scaling of Foundation Models"}, {"paperId": "73290ecbec2f38d1d647ddef1ada69cee41725b3", "title": "PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "6bef46eccb4c7f521e4f255a01595ebf9994ae22", "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20", "title": "Wordcraft: Story Writing With Large Language Models"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "6281c40c66febca1d8003bcc6fdfd2189b30c38f", "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "da43a455e65f8d1fec2ac72932ac2dd6c6ddc20d", "title": "Evaluating Factuality in Generation with Dependency-level Entailment"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba", "title": "Convolutional Neural Networks for Sentence Classification"}, {"paperId": "e3aa232577bb427b1f3a34acbdef84bd85734042", "title": "LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models"}, {"paperId": null, "title": "Linformer"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Ntk-aware scaled rope"}]}