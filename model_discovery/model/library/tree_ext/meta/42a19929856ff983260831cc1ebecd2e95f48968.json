{"paperId": "42a19929856ff983260831cc1ebecd2e95f48968", "title": "Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models", "abstract": "Large Multimodal Model (LMM) is a hot research topic in the computer vision area and has also demonstrated remarkable potential across multiple disciplinary fields. A recent trend is to further extend and enhance the perception capabilities of LMMs. The current methods follow the paradigm of adapting the visual task outputs to the format of the language model, which is the main component of a LMM. This adaptation leads to convenient development of such LMMs with minimal modifications, however, it overlooks the intrinsic characteristics of diverse visual tasks and hinders the learning of perception capabilities. To address this issue, we propose a novel LMM architecture named Lumen, a Large multimodal model with versatile vision-centric capability enhancement. We decouple the LMM's learning of perception capabilities into task-agnostic and task-specific stages. Lumen first promotes fine-grained vision-language concept alignment, which is the fundamental capability for various visual tasks. Thus the output of the task-agnostic stage is a shared representation for all the tasks we address in this paper. Then the task-specific decoding is carried out by flexibly routing the shared representation to lightweight task decoders with negligible training efforts. Comprehensive experimental results on a series of vision-centric and VQA benchmarks indicate that our Lumen model not only achieves or surpasses the performance of existing LMM-based approaches in a range of vision-centric tasks while maintaining general visual understanding and instruction following capabilities. The code will be released at https://github.com/SxJyJay/Lumen.", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Comprehensive experimental results indicate that the proposed Lumen model not only achieves or surpasses the performance of existing LMM-based approaches in a range of vision-centric tasks while maintaining general visual understanding and instruction following capabilities."}, "embedding": {"model": "specter_v2", "vector": [0.3861904442310333, 0.30512943863868713, -0.1911337524652481, -0.22574937343597412, -0.4349088668823242, 0.09928039461374283, 0.5297378897666931, 0.03662153333425522, -0.7000488638877869, -0.810314416885376, 0.39427727460861206, 0.17748062312602997, 0.4699614942073822, 0.38554176688194275, 0.15898968279361725, 0.3250656723976135, -0.6237716674804688, 0.25225380063056946, 0.09969481080770493, -0.8732156157493591, 0.21063180267810822, -0.856579601764679, -1.2814570665359497, 0.5201263427734375, 0.4275771975517273, 0.5212618708610535, 1.0113779306411743, 1.4141838550567627, -0.2402992844581604, 0.22885529696941376, 0.2040472775697708, -0.16925281286239624, -0.15953299403190613, 0.20398877561092377, -0.21002064645290375, 0.25477999448776245, 0.6839247345924377, -0.4935038387775421, -0.2614292502403259, 0.7654151916503906, 0.1180606260895729, 0.067512147128582, 0.7511442303657532, -0.7225347757339478, -0.387018620967865, 0.38334721326828003, 0.30897659063339233, 0.24810457229614258, -0.3727734684944153, -0.47812238335609436, 1.2589956521987915, -1.7231652736663818, 0.10508072376251221, 1.894778847694397, 0.12723906338214874, 0.6497189998626709, -0.03576236590743065, -0.3356859087944031, 0.814095139503479, -0.17129434645175934, -0.685981035232544, -0.28233346343040466, -0.35857972502708435, -0.3676440417766571, 1.5404856204986572, -0.545175313949585, -0.4947143793106079, 0.819073498249054, 0.5340436100959778, 1.5985584259033203, -0.11676454544067383, -0.9670752286911011, -0.3348112404346466, -0.2745761275291443, 0.4544430375099182, 0.9472709894180298, -0.5458276867866516, 0.33313149213790894, -0.9832379221916199, 0.2336442470550537, 0.6127071380615234, -0.17459025979042053, -0.06111661344766617, -0.3825119137763977, -0.7794685959815979, 0.7651212811470032, 0.6358375549316406, 0.3308177888393402, 0.07006833702325821, 0.5611730813980103, 0.5136944055557251, 0.23809121549129486, -0.36562487483024597, 0.010509682819247246, 0.0881807953119278, 0.6261884570121765, -0.7416260838508606, 0.044703952968120575, -0.23436762392520905, 1.0787341594696045, -0.22127676010131836, 0.2502831518650055, -1.00553297996521, 0.3720708191394806, 1.9085931777954102, 0.3363700807094574, 0.6411919593811035, -0.6201580762863159, 0.5584914684295654, -0.9212917685508728, 0.2596505582332611, -0.45224252343177795, -0.3410232365131378, -0.20840562880039215, -0.15579190850257874, -0.9272332787513733, -0.4680900573730469, 0.22715841233730316, -1.1592622995376587, 0.8394272327423096, -0.4172685742378235, -0.3032185733318329, 0.4008074104785919, 0.24110765755176544, 0.908541202545166, 0.3896147906780243, 0.8636883497238159, 0.4278702139854431, 1.2153058052062988, -0.9048962593078613, -0.4553085267543793, -1.1431758403778076, 0.6266220211982727, -0.6621155142784119, 0.47596317529678345, -0.3419627845287323, -1.2434582710266113, -1.7425106763839722, -0.9014935493469238, -0.3464750349521637, -0.33856526017189026, 0.6225710511207581, 0.8751252889633179, 0.2567954361438751, -1.6011543273925781, 0.2719546854496002, 0.1118399128317833, -0.3799463212490082, 0.15514706075191498, 0.2588214576244354, 0.3172256052494049, -0.6140281558036804, -0.7697802782058716, 0.14883826673030853, 0.2239336520433426, -0.32517117261886597, -0.555000364780426, 0.09388510137796402, -1.8622604608535767, -0.26436159014701843, 0.061870645731687546, -0.7005665898323059, 1.3694567680358887, -0.3176451027393341, -0.8165302872657776, 0.5597153306007385, -0.6834720969200134, 0.5602387189865112, -0.31075939536094666, -0.1621609628200531, -0.5257536172866821, -0.1828099936246872, -0.2678648829460144, 1.483614444732666, 0.7415450215339661, -0.6348876357078552, -0.5057723522186279, 0.2749313712120056, -0.13468115031719208, 0.47777917981147766, -0.41152119636535645, 1.1045725345611572, -0.5968704223632812, -0.23058097064495087, 0.5749666690826416, 0.7270954251289368, 0.11241340637207031, 0.023892052471637726, -0.2108195722103119, -1.0501817464828491, 0.6128716468811035, 0.30793824791908264, 0.4968931972980499, -0.9366343021392822, -0.5069051384925842, -0.43362507224082947, 0.0989772230386734, -0.15761134028434753, -1.5369991064071655, 0.8080285787582397, 0.10631521046161652, -0.06346599757671356, -0.08405563980340958, -1.7889872789382935, 0.1285087913274765, -0.10275721549987793, -0.4558563232421875, -0.45739543437957764, 0.18381689488887787, 1.2971662282943726, -1.128765344619751, -0.27039778232574463, -0.0933976098895073, 0.026097023859620094, -0.4743141829967499, 1.2802382707595825, -0.534397542476654, 0.18331727385520935, 0.028076229616999626, 0.03902895376086235, 0.04288504645228386, -0.4556407332420349, 0.47367873787879944, -0.8199392557144165, 0.23060199618339539, -0.02232293412089348, 0.1447506695985794, 2.0601966381073, -0.2623177766799927, 0.8721324801445007, -0.24734683334827423, -0.21768145263195038, 0.3000197410583496, 0.6264351606369019, -0.4744372069835663, -0.7596495151519775, 0.5182262063026428, 0.10768653452396393, -0.25012415647506714, 0.05605071038007736, 1.0432757139205933, 0.9631994366645813, -0.39795616269111633, 0.14002133905887604, 0.4509729743003845, -0.28211042284965515, 0.4527056813240051, 0.6057895421981812, 0.160469651222229, 0.3474360406398773, 0.32977256178855896, 0.052063342183828354, 0.5403280258178711, -0.8911147117614746, -0.5133677124977112, 0.6993021965026855, 0.44073575735092163, 1.1162134408950806, 0.12134663015604019, -0.425870418548584, 0.11795874685049057, -0.47723740339279175, 0.7593460083007812, 1.8019357919692993, 0.23791924118995667, -0.04567078500986099, -0.543738842010498, -0.20087605714797974, -0.4575677812099457, -0.2441205233335495, -0.433183491230011, -0.14603480696678162, -0.1836022138595581, -0.6597521305084229, 0.5293228030204773, 0.6740996241569519, 0.8510966897010803, -0.8789352774620056, -0.4117107689380646, -0.32144325971603394, 0.14863897860050201, -0.9572526216506958, -0.9503663182258606, -0.17896690964698792, -0.5264853835105896, -0.4618787467479706, -0.13282088935375214, -0.5519171953201294, 0.41019368171691895, -0.17727841436862946, 1.1475095748901367, -0.7072612643241882, -0.4569550156593323, 0.814093828201294, 0.3458595871925354, -0.7997909784317017, -0.8590511083602905, -0.4773222804069519, -0.19795861840248108, -0.09880388528108597, 0.46757572889328003, 1.2494999170303345, -0.17308424413204193, 0.0874592512845993, -0.4820668697357178, 0.3455839455127716, 0.5738667845726013, -0.1273166835308075, 1.0015521049499512, -0.8817200660705566, 0.15455719828605652, -0.6944380402565002, 1.1368207931518555, 0.30376702547073364, -0.1447669118642807, 0.5110821723937988, -0.4123452305793762, -0.6860928535461426, -0.13552485406398773, -0.732096254825592, -0.5051426291465759, -0.6739746928215027, 0.5296862125396729, -0.16107763350009918, -0.5726373195648193, 0.1512175053358078, 0.46529531478881836, -0.16387566924095154, 0.20874272286891937, 0.6566444039344788, 0.5342386364936829, 0.08582739531993866, 0.7596613764762878, -0.5305684804916382, 0.872669517993927, 0.08786161988973618, -0.21028894186019897, -0.01727624237537384, -0.19217745959758759, -0.7079912424087524, -0.2970205545425415, -0.6730891466140747, -0.3614780306816101, -0.5084365606307983, 0.6451154351234436, -0.9548394680023193, -0.9643255472183228, -0.1368788480758667, -1.4635052680969238, 0.008678644895553589, 0.4257158637046814, -0.05676405876874924, -0.1510273516178131, -0.7012696862220764, -0.9704352021217346, -0.1255892962217331, -0.5254431962966919, -1.4427390098571777, 0.5029227137565613, 0.26993897557258606, -0.534390389919281, -0.4552817642688751, -0.039919693022966385, -0.05497201532125473, 0.7780919671058655, -0.7028878331184387, 0.8938550353050232, 0.062138594686985016, -0.18723374605178833, -0.224838525056839, -0.04748467728495598, 0.7419375777244568, 0.25940966606140137, 0.0958019569516182, -1.0885051488876343, -0.1992039978504181, -0.12134156376123428, -0.3341157138347626, 0.4161793291568756, 0.42058706283569336, 0.41613927483558655, 0.9244547486305237, -0.3292643427848816, 0.2186133712530136, 1.4551515579223633, -0.4564779996871948, -0.18461206555366516, -0.3065093755722046, 1.397287130355835, 0.7638710141181946, -0.16363748908042908, 0.22592248022556305, 1.0001709461212158, 0.31789112091064453, 0.5209604501724243, -0.33422696590423584, -0.7652595043182373, -0.22460249066352844, 0.7513677477836609, 1.4836580753326416, 0.2739585340023041, -0.3163885772228241, -1.059305191040039, 0.600716769695282, -1.2116655111312866, 0.0098191499710083, 0.6254906058311462, 0.9335981011390686, 0.018559986725449562, -0.5760067105293274, -0.45016416907310486, -0.6301261186599731, 0.5308206677436829, 0.3842625021934509, -0.014281985349953175, -1.0026930570602417, 0.05437185615301132, 0.20197249948978424, -0.3895542323589325, 0.6794897317886353, -0.5629857182502747, 0.497167706489563, 13.992794036865234, 0.5276938676834106, -0.020044932141900063, 0.5803998708724976, 0.4713936448097229, 0.4550239145755768, -0.5026885867118835, -0.39133650064468384, -1.2603607177734375, -0.3999606966972351, 1.073713779449463, 0.7211720943450928, 0.04892620071768761, -0.2539403736591339, -0.0549832247197628, 0.03980157524347305, -0.9498425126075745, 0.9098316431045532, 0.9755680561065674, -0.8382970094680786, 0.9039381742477417, -0.028582805767655373, 0.206083744764328, 0.5338430404663086, 1.150577187538147, 0.865543782711029, -0.058689504861831665, -0.6174193620681763, 0.4525754153728485, 0.16574063897132874, 0.8018214702606201, -0.05185375362634659, 0.16374659538269043, 0.41692668199539185, -1.270517349243164, -0.2932546138763428, -0.7309150695800781, -0.9508611559867859, 0.17531508207321167, -0.6634082198143005, -0.42349332571029663, -0.3309187591075897, -0.6161965727806091, 0.6960992217063904, -0.15513616800308228, 0.5292765498161316, -0.18354696035385132, 0.28793904185295105, 0.5149510502815247, -0.038971371948719025, 0.41637659072875977, 0.5444044470787048, 0.3322981894016266, 0.06830687820911407, -0.15015935897827148, -0.38964545726776123, 0.4922378957271576, 0.2879936695098877, -0.4187653064727783, -0.02498987875878811, -0.35204416513442993, 0.08452055603265762, -0.2612363398075104, 0.7607819437980652, 0.2783035337924957, 0.6488214135169983, -0.8546820282936096, 0.3515840172767639, 0.44459015130996704, 0.49320387840270996, -0.06948912143707275, 0.331277072429657, 0.15522487461566925, -1.0772736072540283, 0.22060392796993256, 0.1716030091047287, 0.02392321452498436, -0.26188424229621887, -0.5974761247634888, -0.16737604141235352, 0.2851237952709198, -0.9273542761802673, -0.8556641936302185, 0.9731072783470154, -0.10916970670223236, -0.6405444741249084, 0.08492595702409744, -1.2420706748962402, -0.5357299447059631, 0.591585099697113, -1.666534662246704, -1.3600353002548218, -0.03693847358226776, -0.028621746227145195, 0.2485063225030899, -0.32498040795326233, 1.326753854751587, -0.17847126722335815, -0.20598287880420685, 0.27521181106567383, -0.35029521584510803, -0.13879777491092682, 0.26590389013290405, -0.5421063303947449, 0.5102479457855225, 0.41499072313308716, 0.23828236758708954, 0.07558468729257584, -0.19692453742027283, -0.04501017928123474, -0.8132818341255188, -0.0460699237883091, 0.643975019454956, -0.8606725931167603, -0.6260597109794617, -0.750756025314331, -0.628949761390686, 0.020487835630774498, 0.8044071793556213, -0.17333412170410156, 0.4322172701358795, 0.08231339603662491, -0.7560709118843079, -0.009776685386896133, -0.5423181056976318, 0.20624715089797974, -0.019388239830732346, -0.8983034491539001, -0.33401229977607727, 0.09529758989810944, 0.4842681884765625, -0.8135473132133484, -0.316924512386322, -0.5084364414215088, 0.12405125051736832, -0.019649941474199295, 1.2632310390472412, -0.47306665778160095, 0.6596216559410095, 0.711466908454895, -0.5546032786369324, -0.765552818775177, 0.2777903079986572, -0.569599449634552, 0.11586593836545944, -0.14712341129779816, 0.6349794864654541, -0.18279843032360077, -0.3049582839012146, 0.5722715258598328, 0.10563361644744873, -0.587864339351654, -0.35481444001197815, -0.01535485778003931, 0.08418595790863037, -0.6714485287666321, -0.15802974998950958, -0.6847686171531677, -0.3528403341770172, 0.07742220163345337, 0.6375772953033447, 0.9364500045776367, 0.020567286759614944, -0.437892884016037, 0.33170443773269653, -0.031078612431883812, -0.30457594990730286, -0.3909643590450287, -0.4351786673069, -1.479819416999817, -0.13287168741226196, -1.1050232648849487, 0.3298300802707672, -1.097459316253662, -0.3414049446582794, 0.4969187080860138, -0.6572977900505066, 0.09926709532737732, 0.1054779663681984, -0.22630169987678528, 0.08680814504623413, -0.25447335839271545, -0.8209059238433838, 0.7753130197525024, 1.4424657821655273, -0.7171204686164856, 0.12121885269880295, -0.1400575041770935, -0.012935388833284378, 0.2874771058559418, 0.23286595940589905, -0.0891973227262497, -1.463269829750061, -1.6646223068237305, 0.1807975322008133, -0.11041145026683807, 0.049156151711940765, -1.0114325284957886, 0.9525447487831116, 0.5018287301063538, -0.09791558235883713, -0.1751803457736969, 0.8261008858680725, -0.647135317325592, -1.4138761758804321, 0.59012770652771, -0.7987721562385559, 0.38166922330856323, 0.6090747714042664, -0.4750034511089325, -0.6412584781646729, 0.7388732433319092, -0.3242949843406677, -0.8724075555801392, -1.5793192386627197, 0.3946952521800995, -0.359520822763443, -0.1954360008239746, -0.23402738571166992, 0.02364349737763405, -1.3407831192016602, -0.39732977747917175, 0.11249484121799469, 0.6065681576728821, -0.5945873856544495, 0.9918299913406372, 1.582502841949463, -0.7105172276496887, -0.35361576080322266, 0.40742555260658264, 0.4497062861919403, -0.05836525186896324, 0.656893253326416, 0.34367167949676514, -0.13006633520126343, 0.39533406496047974, -0.0673033818602562, -0.048548050224781036, -1.1794004440307617, -0.10312937945127487, 0.6733986735343933, 0.009861545637249947, 0.2570232152938843, 1.3375542163848877, 0.2567160129547119, -1.0240706205368042, 0.13374605774879456, -0.7735719680786133, -1.0156946182250977, -0.24137666821479797, 0.8165462017059326, -0.2634671926498413, -0.3526661694049835, -0.5258345007896423, -0.32402265071868896, 0.7145651578903198, -0.4520728588104248, -0.6904942989349365, -0.21084043383598328, -0.4559957981109619, -0.2880944609642029, 0.5122907757759094, 0.813386857509613, -0.8777329921722412, -1.0051515102386475, -0.466830313205719, -0.6568101048469543, 0.27793169021606445, 0.06917029619216919, -0.39668184518814087, -0.5900041460990906, 1.0817619562149048, 0.7847861051559448, -0.31653833389282227, 0.22318702936172485, -0.17532320320606232, 0.14451941847801208, 0.8189525604248047, -0.21484129130840302, -0.4323215186595917, -0.14245560765266418, 1.1748207807540894, 1.2854682207107544, -1.4575296640396118, 0.0070603895001113415, -0.3737483024597168, -0.9350441098213196, 1.155001163482666, 0.5660771727561951, 0.2203848510980606, 0.4674496054649353, -0.5212777256965637, 0.589737057685852, 0.18648794293403625, -0.8079777956008911, -0.28556931018829346, 1.6112706661224365, 1.6080089807510376, 1.002055048942566, 0.39282265305519104, 0.08191535621881485, 0.24254582822322845, 0.3350902795791626, -0.10050702095031738, 0.5772740244865417, 0.46567773818969727, -0.4831186532974243, -0.3339907228946686, 0.14269712567329407, 0.4789680540561676, -0.001041825977154076, -0.7137221693992615, 0.6761281490325928, 0.5387229323387146, 0.2882095277309418, 1.067214012145996, 1.0125716924667358, 0.20437175035476685, 0.3365797996520996, 0.01806013099849224, 0.791053056716919, -1.014875054359436, -0.12285623699426651, -0.04717768728733063, -0.8861474990844727, -0.09357891231775284, -0.5151888132095337, -0.2534395158290863, -0.7040493488311768, 0.6085401177406311, 0.888607382774353, -0.8315110802650452, 0.67509526014328, 1.3287478685379028, 0.35408198833465576, 0.699999988079071, -0.5463098287582397, -0.3821433186531067, -0.3968658149242401, -0.9618921279907227, 0.22340308129787445, -0.4118328094482422, -0.22863474488258362, -0.4077397882938385, 0.20182548463344574, -0.16340263187885284]}, "authors": [{"authorId": "2149629531", "name": "Yang Jiao"}, {"authorId": "2144337489", "name": "Shaoxiang Chen"}, {"authorId": "2269829514", "name": "Zequn Jie"}, {"authorId": "2155658104", "name": "Jing Chen"}, {"authorId": "2152343776", "name": "Lin Ma"}, {"authorId": "2159938240", "name": "Yueping Jiang"}], "references": [{"paperId": "af9676f9beaeca214bfbf7f2897828820d48abdc", "title": "LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs"}, {"paperId": "b50d19c5c298f6562c3b3c6c3822a351bdc89260", "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"}, {"paperId": "76803ea9ccc93a4f277aae5d4714ea79a99e55d6", "title": "Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models"}, {"paperId": "f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560", "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"}, {"paperId": "619184447595337a9fe3dca72c4e951e7ab7467c", "title": "To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning"}, {"paperId": "1ddbd08ad8cf22a5c66c4242194c4286328533bf", "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "8946891e94831adc8cddb0d32311cce2445c96d2", "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"}, {"paperId": "819f477065088220a6f706cd9ef76dbcb4b4c134", "title": "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists"}, {"paperId": "75ca0ce6493c94bb48766b042243023a5439beeb", "title": "InstructDiffusion: A Generalist Modeling Interface for Vision Tasks"}, {"paperId": "ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7", "title": "LISA: Reasoning Segmentation via Large Language Model"}, {"paperId": "4309d572a37d655779f9dce6a2c98c66334132de", "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"}, {"paperId": "962ccf1fc49c83817fb031e5b24b81b19cdfb89d", "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"}, {"paperId": "e2a58fd18961c3941102989e3a3d0d27c615e015", "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"}, {"paperId": "3b6179c293df29e31d31cea46476f104ab6950f2", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World"}, {"paperId": "051549d8ef56937b2f4d113afdcf8c7586d3770b", "title": "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"}, {"paperId": "3b8871e4c25d3aaca2bee6606c07bc870337253c", "title": "Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions"}, {"paperId": "1ec4bc98fafa8d338f676f7a1b1b1131e8ca978e", "title": "GRES: Generalized Referring Expression Segmentation"}, {"paperId": "ee156428803c5bd6e7372f6b27d74bcf88390db3", "title": "NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario"}, {"paperId": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "c56a51728678e5b2e3ff95e51caf21d267439c36", "title": "ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "8591c167b8cad7e48e82f4674e44fffd3e669d09", "title": "OmniTracker: Unifying Object Tracking by Tracking-with-Detection"}, {"paperId": "7fc39b00981e017864ed01f9d5fdc27a1553e11a", "title": "RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose"}, {"paperId": "a58d3f890a602e820a7bed761f0706f128751a12", "title": "Universal Instance Perception as Object Discovery and Retrieval"}, {"paperId": "c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0", "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"}, {"paperId": "64caaab51d8339f1b99874d3bddb79debbe661ca", "title": "mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "967907503b24423b9b74621051811fcf684e3957", "title": "Generalized Decoding for Pixel, Image, and Language"}, {"paperId": "30a3731f09e7a391e79a28fa736fa6bdd8331866", "title": "Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks"}, {"paperId": "aae2f4d5d2907153edef9e0af110d46930f814c2", "title": "OmniVL: One Foundation Model for Image-Language and Video-Language Tasks"}, {"paperId": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c", "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"}, {"paperId": "06761cb27e14aa55a6c3d98b949898aa26416698", "title": "A Unified Sequence Interface for Vision Tasks"}, {"paperId": "5ffca83dd35d1e66ea82967240c825b220c8d2a4", "title": "Contextual Instance Decoupling for Robust Multi-Person Pose Estimation"}, {"paperId": "60ee030773ba1b68eb222a265b052ca028353362", "title": "GIT: A Generative Image-to-text Transformer for Vision and Language"}, {"paperId": "5922f437512158970c417f4413bface021df5f78", "title": "A Generalist Agent"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "add12473900be92c3ff36d07585011ec33e0a736", "title": "SeqTR: A Simple yet Universal Network for Visual Grounding"}, {"paperId": "5eb67cc9c572a70be8ad3e873cd5e58131d605b0", "title": "Suspected Objects Matter: Rethinking Model's Prediction for One-stage Visual Grounding"}, {"paperId": "07546f1f0b35012a5d8ca850580438f306f3bdd3", "title": "MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "ca1d5aa8f63707931692bf6a62642becf928c1fa", "title": "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation"}, {"paperId": "91dc75f94da13452a54ad5c03fab2c5fda87e9ba", "title": "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks"}, {"paperId": "22312f763328cf540791de8c2449ea1e7436f476", "title": "UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling"}, {"paperId": "14e41d9eb6331b1b5ac0a5fe1344de06d5d908a4", "title": "Two-stage Visual Cues Enhancement Network for Referring Image Segmentation"}, {"paperId": "3e075efc541c7d2b357199655e11f084686e8575", "title": "Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?"}, {"paperId": "19b3b074d38b250d024920732ae51a8ffa0996dd", "title": "Pix2seq: A Language Modeling Framework for Object Detection"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "7ba9c013988eaff5cd186d73704af329d027872d", "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"}, {"paperId": "6b291dea9bb80fea41f4ffa17aaad52ab5d26ada", "title": "Learning To Count Everything"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "8d1bc83cc65d30e4619c49f53115012a209fd8c9", "title": "Multi-Task Collaborative Network for Joint Referring Expression Comprehension and Segmentation"}, {"paperId": "c5ff974a69fd0c760b4855b819e61e89f31cfffe", "title": "Objects365: A Large-Scale, High-Quality Dataset for Object Detection"}, {"paperId": "4d158f43fc1dfe148f63ad2c7162b51ee7e743cc", "title": "PolarMask: Single Shot Instance Segmentation With Polar Representation"}, {"paperId": "c2c083df88e88223e1a411e61040b94c233b1b63", "title": "MMDetection: Open MMLab Detection Toolbox and Benchmark"}, {"paperId": "6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2", "title": "Objects as Points"}, {"paperId": "3ff40f0760bd8d3c46d72147b0f5b0d4aee2a24f", "title": "AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding"}, {"paperId": "79cfb51a51fc093f66aac8e858afe2e14d4a1f20", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f94feceb5b725c6b303b758a0e5e90215b0174d3", "title": "Learning Non-maximum Suppression"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "29efbe391950ae438c63d86ad5c82b2942efb0b4", "title": "Modeling Context in Referring Expressions"}, {"paperId": "1c3884e43bba39c50e6172c403c057659ef3ce83", "title": "Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "b6a0f30260302a2001da9999096cfdd89bc1f7fb", "title": "The Hungarian method for the assignment problem"}, {"paperId": "5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a", "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"}, {"paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"}, {"paperId": null, "title": "Edinburgh Research Explorer The PASCAL Visual Object Classes (VOC) Challenge"}, {"paperId": null, "title": ": A comprehensive evaluation benchmark for multimodal large language"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale"}]}