{"paperId": "45294f7227d0228f3d26b2796824f46524ad7a76", "title": "HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning", "abstract": "In modern large language models (LLMs), increasing sequence lengths is a crucial challenge for enhancing their comprehension and coherence in handling complex tasks such as multi-modal question answering. However, handling long context sequences with LLMs is prohibitively costly due to the conventional attention mechanism's quadratic time and space complexity, and the context window size is limited by the GPU memory. Although recent works have proposed linear and sparse attention mechanisms to address this issue, their real-world applicability is often limited by the need to re-train pre-trained models. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which simultaneously reduces the training and inference time complexity from $O(T^2)$ to $O(T \\log T)$ and the space complexity from $O(T^2)$ to $O(T)$. To this end, we devise a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly. HiP is training-free as it only utilizes the pre-trained attention scores to spot the positions of the top-$k$ most significant elements for each query. Moreover, it ensures that no token is overlooked, unlike the sliding window-based sub-quadratic attention methods, such as StreamingLLM. Extensive experiments on diverse real-world benchmarks demonstrate that HiP significantly reduces prompt (i.e., prefill) and decoding latency and memory usage while maintaining high generation performance with little or no degradation. As HiP allows pretrained LLMs to scale to millions of tokens on commodity GPUs with no additional engineering due to its easy plug-and-play deployment, we believe that our work will have a large practical impact, opening up the possibility to many long-context LLM applications previously infeasible.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Hierarchically Pruned Attention (HiP) is proposed, a dynamic sparse attention mechanism that generates an attention mask through a novel tree-search-like algorithm for a given query on the fly, opening up the possibility to many long-context LLM applications previously infeasible."}, "embedding": {"model": "specter_v2", "vector": [0.32273679971694946, 0.7617526650428772, 0.06292752176523209, -0.0597882941365242, -0.8455090522766113, -0.15388964116573334, 0.2822894752025604, -0.09177836775779724, -0.15412434935569763, -0.15696981549263, 0.27627062797546387, 0.2187899649143219, 0.5525521636009216, 0.10113710165023804, -0.15341795980930328, 0.3289802670478821, -0.9324726462364197, 0.3834630846977234, 0.2159813940525055, -0.3367575407028198, 0.2540409564971924, -1.2506682872772217, -1.3291091918945312, 0.535932719707489, 0.6989712119102478, 0.4343630075454712, 0.5154584050178528, 1.0097131729125977, -0.46843403577804565, 0.38264524936676025, -0.03821725770831108, -0.4754238724708557, -0.1548265963792801, -0.179772287607193, -0.5251030921936035, -0.41667115688323975, 0.38499751687049866, -0.16234543919563293, -0.1914876252412796, 0.4753004312515259, 0.07432837039232254, 0.36837494373321533, 0.23123571276664734, -0.3931426405906677, -0.542579710483551, 0.922500729560852, 0.47379955649375916, 0.6000325083732605, 0.283029168844223, -0.7428735494613647, 1.6385843753814697, -1.6108486652374268, 0.20065949857234955, 1.7224818468093872, 0.1544991433620453, 0.6179972887039185, -0.16259676218032837, -0.3514736294746399, 1.0548220872879028, 0.6152452230453491, -0.9791194200515747, -0.45429113507270813, -0.017319248989224434, 0.13277001678943634, 1.9944604635238647, -0.011001013219356537, -0.0010147689608857036, 0.25769707560539246, -0.22797030210494995, 1.6446478366851807, -0.5587539076805115, -1.170480489730835, -0.3347318470478058, -0.2264644354581833, 0.15332671999931335, 0.6840273141860962, -0.3334788680076599, 0.032823946326971054, -0.8711881041526794, -0.2339187115430832, 0.40087148547172546, -0.2814137935638428, 0.24406075477600098, 0.01767355389893055, -0.25889167189598083, 0.7457488179206848, 0.21304866671562195, 1.035710334777832, -0.07140415906906128, 0.7086158394813538, 0.20728321373462677, 0.08293332159519196, -0.2022639513015747, 0.5486708879470825, -0.18031129240989685, 0.27726590633392334, -1.0769938230514526, 0.5191601514816284, 0.04704270139336586, 1.0603556632995605, -0.047067154198884964, 0.12729384005069733, -0.8962866067886353, 0.3528175354003906, 1.148722767829895, -0.008916078135371208, 0.5036738514900208, -0.7861202955245972, 0.30970776081085205, -0.5031554102897644, -0.002592835109680891, -0.496909499168396, 0.06884486228227615, 0.010259355418384075, -0.5474012494087219, -1.3486438989639282, -0.4172351062297821, -0.04127645492553711, -0.47598618268966675, 0.7411798238754272, 0.018631909042596817, 0.14684495329856873, -0.13838431239128113, 0.5207738876342773, 0.6405031681060791, 0.6014425158500671, 0.13712164759635925, 0.42478296160697937, 1.1964422464370728, -0.9260978102684021, -0.4533977508544922, -1.291054129600525, 0.7306936979293823, -0.30577412247657776, 0.42195335030555725, -0.10362999141216278, -1.268775224685669, -0.8004468679428101, -0.7042349576950073, -0.4495633840560913, -0.49751919507980347, 0.3359036147594452, 0.8910280466079712, 0.2676997482776642, -0.9643634557723999, 0.42399442195892334, -0.10840509086847305, -0.019899317994713783, 0.15087248384952545, -0.003357140114530921, 0.4976164400577545, -0.9337719082832336, -1.4764318466186523, 0.09376577287912369, 0.007989378646016121, -0.2727189064025879, -0.09708001464605331, -0.35320228338241577, -1.4153655767440796, 0.044806789606809616, 0.5613047480583191, -0.521754264831543, 1.4899793863296509, 0.1698354184627533, -1.335886001586914, 0.6031241416931152, -0.9427642822265625, 0.2780761420726776, -0.2607465982437134, -0.3436003029346466, -0.44022902846336365, -0.3831038475036621, 0.009770756587386131, 0.6926614046096802, 0.4310433864593506, 0.1389850229024887, -0.32021287083625793, 0.22837740182876587, -0.13689564168453217, -0.16390275955200195, 0.20448444783687592, 0.7777801752090454, -0.6201779842376709, -0.13608689606189728, 0.3964231014251709, 0.6315413117408752, -0.14817194640636444, -0.6611770391464233, -0.5550143718719482, -1.4493885040283203, 0.5688089728355408, 0.03917074203491211, 1.1034417152404785, -0.6465965509414673, -0.675383985042572, -0.522918701171875, -0.16464772820472717, -0.22146162390708923, -0.8695728778839111, 0.7061222791671753, -0.1516961306333542, 0.3267732262611389, -0.3622356355190277, -1.1939853429794312, 0.27334684133529663, -0.3985241949558258, -0.36494970321655273, -0.515681266784668, 0.4701189696788788, 1.3078798055648804, -0.9586043953895569, -0.3687250018119812, -0.11970828473567963, 0.024238063022494316, -1.2969796657562256, 1.114011287689209, -0.8665997982025146, 0.21684330701828003, -0.3537725806236267, 0.13627763092517853, -0.21604788303375244, -0.29088693857192993, 0.1293308138847351, -0.027751004323363304, -0.1438053548336029, 0.7281578779220581, -0.32377752661705017, 1.562235951423645, -0.33221688866615295, 0.45502743124961853, -0.3044532239437103, -0.4909767806529999, 0.07947443425655365, 0.4279681146144867, -0.4850144386291504, -0.47997766733169556, -0.15607589483261108, 0.3538804352283478, -0.624238908290863, -0.23470456898212433, 0.9704633951187134, 1.0686131715774536, -0.6142125725746155, -0.1110614612698555, 0.43865659832954407, -0.28480374813079834, 0.7042011022567749, 0.5237178206443787, 0.6264232993125916, 0.483795166015625, 0.33802372217178345, 0.2259143739938736, 0.5637663006782532, -0.6599223017692566, -0.06031747907400131, 0.7410999536514282, 0.9553127288818359, 1.023323655128479, 0.5043763518333435, -0.7583615183830261, -0.23940862715244293, 0.6168014407157898, 0.841571033000946, 1.9053837060928345, -0.08698976039886475, -0.0883026048541069, -0.6666110754013062, -0.245112806558609, -0.3991176187992096, 0.3206489086151123, -0.38287797570228577, 0.116618312895298, -0.7318632006645203, -0.9674796462059021, 0.748926043510437, -0.013890751637518406, 0.6892327666282654, -0.9108736515045166, -0.33674606680870056, -0.0493515320122242, -0.02810373529791832, -1.0071702003479004, -0.7333130240440369, 0.050749607384204865, -0.25923457741737366, -0.3621116280555725, -0.007827207446098328, -0.3536858856678009, 0.0664999783039093, -0.8376477360725403, 1.2386424541473389, -0.46666181087493896, -0.5028391480445862, 0.037225134670734406, 0.2185024917125702, -0.4662383794784546, -0.19673024117946625, 0.35672953724861145, -0.39420220255851746, -0.11197026073932648, 0.5336209535598755, 0.6150959730148315, -0.16739225387573242, 0.0006420603604055941, -0.31911325454711914, 0.09204274415969849, 0.1994771510362625, -0.06588199734687805, 0.9362341165542603, -0.7462596893310547, 0.17866292595863342, -1.283437967300415, 0.4704684913158417, 0.009475343860685825, -0.542961061000824, 0.18959933519363403, -0.3420010805130005, -0.43560975790023804, 0.43862441182136536, -0.6395070552825928, -0.2835913300514221, -0.8717835545539856, 0.1777435839176178, -0.08977492898702621, -0.5252120494842529, 0.28563085198402405, 0.057603053748607635, 0.229612797498703, 0.12684273719787598, 0.537462055683136, -0.11379966139793396, -0.2235654890537262, 0.700993537902832, -1.0632041692733765, 0.3944735527038574, 0.020850740373134613, -0.05916677415370941, -0.37664711475372314, -0.24495016038417816, -0.7584448456764221, -0.575173556804657, -0.43756675720214844, -0.1263893097639084, -0.08975875377655029, 0.16098664700984955, -0.33817583322525024, -0.8961530327796936, -0.14492981135845184, -1.2430367469787598, -0.18302014470100403, 0.14929421246051788, -0.3914852738380432, -0.06736577302217484, -0.7085894346237183, -1.1270171403884888, -0.6637458205223083, -0.6817022562026978, -0.7676682472229004, 0.9403232336044312, -0.024264711886644363, -0.425270676612854, -0.3345217704772949, 0.2862139046192169, -0.26134970784187317, 0.8535391092300415, -0.3598366379737854, 0.922846794128418, -0.042688194662332535, -0.7214667797088623, -0.6758185029029846, 0.3721158802509308, 0.012606138363480568, -0.28223034739494324, -0.10033935308456421, -0.7710968255996704, 0.22160542011260986, -0.23201559484004974, -0.31917208433151245, 0.08602432161569595, 0.43031877279281616, 0.6822776794433594, -0.1727779656648636, -0.5619614720344543, 0.19943936169147491, 1.4966015815734863, -0.5663765072822571, 0.1262868046760559, 0.017832906916737556, 0.9025915265083313, 0.4564470648765564, -0.045892421156167984, 0.5735347867012024, 0.7067062258720398, 0.43430885672569275, 0.4870249330997467, 0.019836263731122017, -0.022019868716597557, -0.32835376262664795, 0.5196853280067444, 1.6747963428497314, 0.4515547752380371, -0.002247626194730401, -0.9451225399971008, 0.9632151126861572, -1.1427932977676392, -0.7221584916114807, 0.3253507614135742, 0.6717047691345215, 0.22130456566810608, -0.7454507946968079, -0.4414580166339874, -0.47260817885398865, 0.5564498901367188, 0.28649547696113586, -0.5192211270332336, -0.5854485630989075, -0.12174279987812042, 0.3226160407066345, -0.4761146306991577, 0.7088706493377686, -0.13446027040481567, 0.8272904753684998, 14.657382011413574, 0.849437952041626, 0.30875399708747864, 0.5762615203857422, 0.841801106929779, -0.20620514452457428, -0.27470123767852783, -0.024104340001940727, -1.5209611654281616, -0.28749045729637146, 0.9546672701835632, 0.06840028613805771, 0.5065527558326721, 0.2858988642692566, -0.0013002203777432442, 0.038774698972702026, -0.9128603339195251, 0.752429723739624, 0.7707602977752686, -1.0186797380447388, 0.41476598381996155, 0.11497840285301208, 0.3489988148212433, 0.44128507375717163, 0.5868453979492188, 1.1636639833450317, 0.3616030216217041, -0.5725117325782776, 0.3606594204902649, 0.3946988880634308, 0.9524081349372864, 0.09274154156446457, 0.33426275849342346, 0.514135479927063, -1.1691724061965942, -0.23311543464660645, -0.8663069605827332, -1.1640022993087769, 0.17208905518054962, 0.10887235403060913, -0.38526609539985657, -0.3836706578731537, -0.23501965403556824, 0.529330849647522, -0.12603534758090973, 0.5991498231887817, -0.22606588900089264, 0.46950605511665344, -0.010277374647557735, -0.3417200744152069, 0.32184019684791565, 0.6196337342262268, 0.32215920090675354, 0.037543389946222305, 0.3509974777698517, 0.2123587429523468, -0.09601166844367981, 0.7578490972518921, -0.3288753926753998, 0.02254985086619854, -0.21682575345039368, 0.029012266546487808, 0.12985487282276154, 0.7563048005104065, 0.8897911906242371, 0.20730355381965637, -0.5123777389526367, 0.1686181128025055, 0.8315337896347046, 0.17578087747097015, -0.13892990350723267, -0.27457574009895325, 0.12988147139549255, -0.6429328322410583, 0.4179779589176178, 0.7084212303161621, -0.202224463224411, -0.5661561489105225, -0.6208658814430237, -0.46114382147789, 0.5003795027732849, -0.7236725687980652, -0.634528636932373, 0.9558227062225342, -0.4870379567146301, -0.26951682567596436, -0.31126073002815247, -0.3713555932044983, -0.31010884046554565, 0.6122044920921326, -1.3898099660873413, -0.6546779870986938, 0.4480842649936676, -0.37683218717575073, 0.02479737438261509, 0.23526032269001007, 1.405105471611023, 0.015563585795462132, -0.4818679988384247, 0.23550505936145782, -0.16736480593681335, -0.13410790264606476, -0.3841858506202698, -0.8409624695777893, 0.9052934646606445, 0.4236236810684204, -0.2054859697818756, 0.11312349140644073, -0.14104673266410828, 0.2783815264701843, -0.7158907651901245, -0.14131687581539154, 1.1759165525436401, -1.080272912979126, -0.8004329204559326, -0.7731625437736511, -0.8977112770080566, 0.32664936780929565, 0.5269627571105957, -0.3402915596961975, 0.37171781063079834, 0.33966580033302307, -0.255592405796051, -0.032470669597387314, -0.5780066847801208, 0.10250456631183624, 0.15398991107940674, -0.6929817795753479, -0.45061826705932617, -0.08671928942203522, 0.7564313411712646, -1.072925090789795, -0.3190896213054657, -0.49242356419563293, 0.22200000286102295, -0.03874249383807182, 0.9403629302978516, -0.14862073957920074, 0.7223742008209229, 0.9437738656997681, -0.029920918866991997, -0.4215141832828522, -0.16413740813732147, -0.9877873659133911, -0.33979469537734985, 0.31038621068000793, 0.7754133939743042, -0.2015962302684784, 0.4100940525531769, 0.9490453004837036, 0.3879907429218292, -0.728306233882904, -0.9101241827011108, 0.07878423482179642, 0.09918323904275894, -0.48099765181541443, 0.6768279075622559, 0.003295292379334569, 0.10013113170862198, 0.08302999287843704, 0.443644255399704, 0.8686795234680176, -0.20978766679763794, -0.313535213470459, 0.3774624466896057, 0.13234886527061462, -0.07981325685977936, -0.5016181468963623, -0.3785282373428345, -1.7267491817474365, -0.3076838552951813, -0.8485435247421265, 0.21643587946891785, -0.9843176007270813, -0.6760037541389465, 0.09347948431968689, -0.23032253980636597, 0.024971770122647285, -0.13310056924819946, -0.3683356046676636, -0.5027530193328857, -0.8178002238273621, -0.9723562002182007, 0.784584641456604, 0.9122980833053589, -0.609642744064331, 0.2664047181606293, 0.002640776801854372, -0.06354287266731262, 0.11817393451929092, 0.290218710899353, -0.47003450989723206, -0.4031664729118347, -1.6570427417755127, 0.5849867463111877, 0.27485209703445435, -0.029038285836577415, -0.7681021094322205, 0.8905529975891113, 0.6328704953193665, -0.3301999568939209, -0.3836798667907715, 0.3267243504524231, -0.5197267532348633, -0.8762246966362, 0.2139197736978531, -0.9368125796318054, 0.2463548183441162, 0.24424976110458374, -0.7248430252075195, -0.5106777548789978, 0.37537506222724915, -0.37163108587265015, -1.3466318845748901, -0.6918788552284241, 0.5418968200683594, -0.6000842452049255, 0.576974630355835, -0.6586226224899292, -0.1557716578245163, -0.9760201573371887, -0.49272698163986206, -0.01929585635662079, 0.5574668049812317, -0.6233619451522827, 0.8859847187995911, 0.6940906047821045, -1.0267198085784912, -0.2550536096096039, 0.1952693909406662, 0.12848512828350067, 0.54851233959198, 0.8468153476715088, 0.34875771403312683, -0.11173196136951447, 0.8926711082458496, 0.8237374424934387, 0.22093631327152252, -0.771760106086731, 0.16300535202026367, 0.829307496547699, -0.7266923189163208, 0.058046579360961914, 1.4282903671264648, -0.28136706352233887, -1.0810997486114502, -0.0008263713680207729, -1.3593121767044067, -0.6059948205947876, -0.3732568025588989, 0.746089518070221, 0.2010185271501541, -0.039934512227773666, -0.10786353796720505, -0.5867480039596558, 0.2876370847225189, -0.08984525501728058, -0.42851486802101135, 0.4946099817752838, -0.3175928592681885, -0.7258309721946716, 0.6620639562606812, 0.7452420592308044, -0.6502013206481934, -0.34393030405044556, -0.7433189153671265, -0.08188945055007935, 0.07938053458929062, 0.056942954659461975, -0.4039657413959503, -0.2681378424167633, 0.9215320944786072, 0.11449349671602249, 0.6963058710098267, 0.04885299503803253, 0.0644349530339241, 0.32885560393333435, 0.8092422485351562, 0.06399223208427429, -0.2974929213523865, -0.5037126541137695, 1.4220783710479736, 1.464819312095642, -0.9399141073226929, -0.26442214846611023, -0.16755834221839905, -0.574294924736023, 0.5010780096054077, 0.5402517914772034, -0.07418599724769592, 0.38065245747566223, -0.5348923802375793, -0.13990148901939392, 0.15856599807739258, -1.4445453882217407, -0.26778823137283325, 1.043172001838684, 1.1099714040756226, 0.9343487024307251, -0.06121516227722168, 0.29221343994140625, 0.9597254991531372, -0.04422853887081146, -0.02306426130235195, 0.3578018248081207, 0.29280611872673035, -0.29517489671707153, -0.07260175049304962, -0.14151427149772644, 0.6580432653427124, -0.8586432933807373, -1.0689431428909302, 0.1862122267484665, 0.43089932203292847, 0.3512989580631256, 0.899204432964325, 0.6005428433418274, 0.5456405878067017, 0.5423229932785034, 0.3009650409221649, 0.5412219166755676, -0.8776688575744629, -0.03136778995394707, -0.21421971917152405, -0.7810909748077393, -0.24001531302928925, 0.12292298674583435, -0.7321551442146301, 0.018667150288820267, 0.11509739607572556, 0.007630983833223581, 0.24970881640911102, 0.11356252431869507, 1.0300025939941406, 0.7788762450218201, 0.606403648853302, -0.438022255897522, -0.5881394147872925, -0.04939870163798332, -1.1616352796554565, 0.13801567256450653, -0.6259323954582214, -0.03337046876549721, 0.05291430652141571, 0.008003195747733116, -0.28675487637519836]}, "authors": [{"authorId": "2116623957", "name": "Heejun Lee"}, {"authorId": "2307075970", "name": "Geon Park"}, {"authorId": "3445691", "name": "Youngwan Lee"}, {"authorId": "2253795038", "name": "Jina Kim"}, {"authorId": "2306786717", "name": "Wonyoung Jeong"}, {"authorId": "2306785832", "name": "Myeongjae Jeon"}, {"authorId": "2265627157", "name": "Sung Ju Hwang"}], "references": [{"paperId": "0f0d757e764a7f21d7aaa329c835571b247dd937", "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"}, {"paperId": "f1a9e0830bc36c048fa4659beaa62609869895b5", "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding"}, {"paperId": "713806165610c237f551a7b68e6b09b3ded75502", "title": "SparQ Attention: Bandwidth-Efficient LLM Inference"}, {"paperId": "8b28792f8405b737229afb92c99c579b86d8aa98", "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations"}, {"paperId": "93e58491830abe1eb965ab37ec64fa97263f6048", "title": "HyperAttention: Long-context Attention in Near-Linear Time"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "0983883619a0ca597d055d0e58da2f514052913d", "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"}, {"paperId": "5d321194696f1f75cf9da045e6022b2f20ba5b9c", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644", "title": "Fast Inference from Transformers via Speculative Decoding"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "ec34748a74c84f67edde7cc763922fa6d4486022", "title": "Transformer Acceleration with Dynamic Sparse Attention"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "d5e3a0896d0bbd7ef5c9df35191a0f91efeac02f", "title": "Medusa"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": null, "title": "Lmms-eval: Accelerating the development of large multimoal models"}, {"paperId": null, "title": "Improved reasoning, ocr, and world knowledge"}, {"paperId": null, "title": "Sparse linear attention with estimated attention mask, 2023"}, {"paperId": null, "title": "Efficient finetuning of quantized LLMs"}, {"paperId": null, "title": "Accel-erating large language model serving with tree-based speculative inference and verification"}, {"paperId": null, "title": ": Rethinking softmax"}, {"paperId": null, "title": "H 2 o: Heavy-hitter oracle for efficient generative inference of large language models"}, {"paperId": null, "title": "F Negative Social Impact In this paper, we do not perform a careful investigation on LLM alignment performance with HiP"}]}