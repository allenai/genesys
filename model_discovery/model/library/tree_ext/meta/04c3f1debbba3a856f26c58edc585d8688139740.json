{"paperId": "04c3f1debbba3a856f26c58edc585d8688139740", "title": "Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform", "abstract": "Since its introduction, the transformers architecture has seen great adoption in NLP applications, but it also has limitations. Although the self-attention mechanism allows for generating very rich representations of the input text, its effectiveness may be limited in specialized domains such as legal, where, for example, language models often have to process very long texts. In this paper, we explore alternatives to replace the attention-based layers with simpler token-mixing mechanisms: Hartley and Fourier transforms. Using these non-parametric techniques, we train models with long input documents from scratch in the legal domain setting. We also introduce a new hybrid Seq2Seq architecture, a no-attention-based encoder connected with an attention-based decoder, which performs quite well on existing summarization tasks with much less compute and memory requirements. We believe that similar, if not better performance, as in the case of long correlations of abstractive text summarization tasks, can be achieved by adopting these simpler infrastructures. This not only makes training models from scratch accessible to more people, but also contributes to the reduction of the carbon footprint during training.", "venue": "arXiv.org", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper explores alternatives to replace the attention-based layers with simpler token-mixing mechanisms: Hartley and Fourier transforms, and introduces a new hybrid Seq2Seq architecture, a no-attention-based encoder connected with an attention- based decoder that performs quite well on existing summarization tasks with much less compute and memory requirements."}, "embedding": {"model": "specter_v2", "vector": [0.16411170363426208, 1.0176846981048584, -0.3425512909889221, -0.12482181191444397, -1.0355401039123535, -0.5554022789001465, 0.9088306427001953, -0.05374594032764435, -0.48197418451309204, 0.16989830136299133, 1.032083511352539, 0.27169862389564514, 0.3112548887729645, -0.011253098025918007, -0.5021273493766785, -0.07252310961484909, -0.8280070424079895, 0.014340639114379883, 0.10525352507829666, -0.14339382946491241, -0.1780581921339035, -0.7807105183601379, -0.3956901431083679, 0.05213150009512901, 0.6796359419822693, 0.23294764757156372, -0.568061351776123, 0.987121045589447, -0.47183817625045776, 0.5303341150283813, 0.017849992960691452, -0.5931006669998169, 0.5202717185020447, -0.1903623342514038, -0.37580522894859314, -0.7479614615440369, 0.7315818071365356, -0.4427832067012787, -0.7442445158958435, 0.719611644744873, -0.05435997247695923, 0.17671264708042145, 0.6580015420913696, -0.17989014089107513, -0.5333086848258972, 1.4168483018875122, 0.5960146188735962, 0.7761752605438232, 0.24321945011615753, -0.42697250843048096, 1.661672830581665, -0.6548256278038025, 0.7306506037712097, 1.3961760997772217, 0.48887836933135986, 0.5010378360748291, -0.04480845481157303, -0.4808252453804016, 0.5131564736366272, 0.5522916316986084, -0.6258412003517151, -0.3198002278804779, -0.23082436621189117, -0.12680290639400482, 1.77779221534729, -0.31628337502479553, 0.0797642320394516, 0.5906625986099243, 0.020969688892364502, 1.5031508207321167, -0.31956323981285095, -0.3248593807220459, -0.2623746693134308, -0.2397122085094452, 0.4623414874076843, 0.3597707450389862, -0.26667651534080505, 0.18517519533634186, -0.9155698418617249, -0.4710746109485626, 0.05879272520542145, -0.16739486157894135, -0.037203676998615265, 0.3405524790287018, 0.1415562480688095, 0.6645498275756836, 0.2913111448287964, 1.0897808074951172, -0.24343959987163544, 0.9208276271820068, 0.7598889470100403, 0.3181648552417755, 0.3702189326286316, 0.40950727462768555, 0.0907764583826065, 0.16996444761753082, -1.3182164430618286, 0.5915302634239197, -0.14785310626029968, 0.8607845902442932, 0.025407759472727776, 0.5027291178703308, -1.0023561716079712, -0.024684011936187744, 0.8403635025024414, 0.251075804233551, 0.30462542176246643, -0.8387784361839294, 0.19365400075912476, -0.879938542842865, 0.05431343615055084, -0.41257593035697937, -0.016230808570981026, 0.00860687904059887, -1.0060595273971558, -1.4885680675506592, -0.5147212147712708, -0.17721253633499146, -0.6463146805763245, 0.6043927073478699, -0.3677869141101837, 0.0878593698143959, 0.49884217977523804, 0.3004830777645111, 0.6114668250083923, 1.06097412109375, 0.1537778675556183, -0.2809407711029053, 0.8485910892486572, -0.4810062050819397, -1.0539637804031372, -0.7817885875701904, 0.5100743174552917, -0.04713018238544464, -0.20051921904087067, -0.13121297955513, -1.5843156576156616, -0.8438774943351746, -0.8770914077758789, -0.3700309991836548, -0.40060093998908997, 0.05467389151453972, 0.5767087340354919, 0.3670046329498291, -0.6948192715644836, 1.1562862396240234, -0.2807133197784424, -0.3035644590854645, 0.5948621034622192, -0.12980905175209045, 0.09768837690353394, -0.1752680242061615, -1.538909673690796, 0.5418229103088379, 0.28682249784469604, -0.625786542892456, 0.36512187123298645, -0.7830594778060913, -1.2459027767181396, 0.23221644759178162, 0.4677187502384186, -0.42673540115356445, 1.4501643180847168, 0.43953654170036316, -1.141617774963379, 0.09517461061477661, -0.5472263097763062, -0.36716657876968384, 0.5551462173461914, -0.701818585395813, -0.43484824895858765, -0.3149198293685913, 0.10269585251808167, 0.14841152727603912, 0.21139900386333466, -0.12616600096225739, -0.21176540851593018, 0.2326631397008896, -0.3094104528427124, -0.5267675518989563, -0.029920248314738274, 0.5912148952484131, -0.23798714578151703, -0.10091249644756317, -0.32373571395874023, 0.7567920088768005, -0.0023213468957692385, -0.8998857140541077, -0.6998951435089111, -1.3663264513015747, 0.5771422386169434, -0.22015896439552307, 0.9242857694625854, -0.8358988165855408, -0.7648516893386841, -0.2906886637210846, -0.313953161239624, 0.267494797706604, -0.7038028836250305, 0.9234620928764343, -0.24210485816001892, 0.4882204830646515, -0.5791547894477844, -0.8813282251358032, 0.3214046061038971, -0.10644731670618057, -0.8103744387626648, -0.0030775764025747776, 0.45273151993751526, 1.3564115762710571, -0.6937805414199829, -0.08527503162622452, -0.23510292172431946, 0.2071150690317154, -1.063639521598816, 1.0762543678283691, -0.6047794222831726, 0.10768338292837143, -0.05775654315948486, -0.7267346978187561, 0.4094246029853821, 0.2561643421649933, 0.24022307991981506, -0.13731567561626434, -0.4286907911300659, 0.49172183871269226, -0.2997719943523407, 1.28927743434906, -0.03949069231748581, 0.7014073133468628, -0.22230570018291473, -0.6897486448287964, -0.2507881820201874, 0.45496252179145813, 0.07068284600973129, -0.40679365396499634, 0.050423819571733475, 0.12238699942827225, -0.7831655740737915, -0.350810170173645, 0.7832248210906982, 0.7208998203277588, -0.453461617231369, 0.3074144124984741, 0.6801428198814392, -0.17827863991260529, 1.1186368465423584, 0.6433082818984985, 1.013118863105774, 0.6658691763877869, 0.6051511764526367, 0.27112388610839844, 0.2363463044166565, -0.5123823881149292, 0.1815824806690216, 0.4468488097190857, 1.0333008766174316, 0.9027310609817505, 0.7882953882217407, -0.5983365178108215, -0.6726084351539612, 0.13410396873950958, 0.5771297216415405, 0.9056074619293213, -0.5848438739776611, -0.5758500099182129, -0.8472214937210083, -0.517549991607666, -0.031641632318496704, 0.3823360204696655, -0.3803916871547699, -0.11543140560388565, -0.9762545228004456, -0.7565358877182007, 0.9792277216911316, 0.311042845249176, 0.9866266846656799, -0.34195369482040405, -0.13626106083393097, -0.17378054559230804, 0.1110226958990097, -0.8283405303955078, -0.6674723625183105, -0.04206765815615654, 0.15482230484485626, -0.26491257548332214, -0.222606360912323, 0.4198012053966522, 0.006420615594834089, -1.150536298751831, 0.36496463418006897, -0.5118790864944458, -0.3157823979854584, 0.11242933571338654, 0.2796593904495239, -0.5411881804466248, -0.8872025012969971, 0.3060328960418701, -0.01551034115254879, -0.3489658534526825, 0.10083878040313721, 0.45849430561065674, -0.034132156521081924, -0.6166267395019531, -0.6486237645149231, -0.457123726606369, 0.1870265007019043, -0.11956057697534561, -0.0478086918592453, 0.14316244423389435, 0.06977631151676178, -1.4276164770126343, 0.8644021153450012, 0.3666320741176605, -0.46877321600914, 0.03953444957733154, -0.444080650806427, -0.20788829028606415, 0.8283875584602356, -0.4388262927532196, -0.3251188099384308, -0.8841057419776917, -0.09875274449586868, 0.16455858945846558, -0.375044584274292, 0.3100862205028534, 0.012485475279390812, 0.7248382568359375, 0.32036975026130676, 0.3845950961112976, 0.2054135799407959, -0.13660313189029694, 0.8721603155136108, -0.6443257331848145, 0.9301549792289734, 0.5388323068618774, 0.7631474733352661, -7.162572728702798e-05, -0.03858278691768646, -0.8962591290473938, -0.5292772650718689, -0.5620091557502747, -0.36529311537742615, -0.18102192878723145, 0.6283515095710754, -0.374650239944458, -0.9215282201766968, -0.30912283062934875, -1.4591689109802246, -0.08757701516151428, -0.33668065071105957, -0.2653466761112213, -0.23205392062664032, -0.5876469016075134, -1.153615117073059, -0.8960272073745728, -0.5412963628768921, -0.23823212087154388, 0.5942006707191467, -0.06209615617990494, -0.5647855997085571, -0.27834057807922363, 0.32304489612579346, -0.3665347993373871, 0.7063167095184326, -0.2969752550125122, 0.6570457816123962, -0.10708251595497131, -0.3754182457923889, -0.3272998034954071, 0.2139647752046585, 0.6959545612335205, -0.45856785774230957, 0.24440759420394897, -0.7094603776931763, 0.3201104998588562, -0.6386777758598328, -0.34741857647895813, 0.5622418522834778, 0.6296869516372681, -0.012458501383662224, -0.4880066215991974, -0.31030136346817017, 0.06203341856598854, 1.372731328010559, -0.6527904272079468, 0.33884304761886597, -0.15597189962863922, 0.7395889163017273, 0.2338591367006302, -0.1943100243806839, 0.4888911843299866, 0.18578767776489258, 0.20510514080524445, -0.30084744095802307, -0.17778846621513367, -0.01063219178467989, -0.48454371094703674, 0.7633258104324341, 1.3051013946533203, -0.01781490445137024, -0.22174593806266785, -0.7902525663375854, 0.3727385997772217, -1.543813943862915, -1.6034430265426636, 0.34590399265289307, 0.19386476278305054, 0.21608448028564453, -0.4941418766975403, -0.05696535483002663, -0.07071135193109512, 0.1266060173511505, 0.6085754632949829, -0.4661825895309448, -0.5209293365478516, -0.17178773880004883, 0.2897217869758606, 0.4039207696914673, 0.81621253490448, -0.3022744655609131, 0.7740684151649475, 15.014851570129395, 0.7280204892158508, 0.1893431842327118, 0.4545328915119171, 0.1092587560415268, 0.0076814014464616776, -0.2069425731897354, 0.011254560202360153, -1.1491131782531738, -0.3161894381046295, 1.1705760955810547, -0.4705374538898468, 0.3523982763290405, -0.09044375270605087, 0.510169267654419, -0.056495022028684616, -0.4674762487411499, 0.19565266370773315, 0.7096858620643616, -1.5051380395889282, 0.2682490646839142, 0.4364743232727051, 0.3150199353694916, 0.5034500956535339, 0.5483261346817017, 0.9351337552070618, 0.6628586649894714, 0.018968412652611732, 0.43621355295181274, 0.53783118724823, 0.9316689372062683, -0.14240624010562897, 0.7022120356559753, 0.6035073399543762, -0.7068100571632385, -0.3898187577724457, -0.8937381505966187, -0.768449604511261, 0.6076326966285706, 0.28290385007858276, -0.31000831723213196, 0.3492451012134552, -0.18110188841819763, 1.0833797454833984, 0.27445703744888306, 0.3150727450847626, -0.13263341784477234, 0.6924918293952942, 0.08956100791692734, 0.10265234112739563, 0.3389708399772644, 0.3252018094062805, 0.2620082497596741, 0.2645176351070404, 0.25182032585144043, 0.46510711312294006, -0.11906145513057709, 0.451000452041626, -0.514485776424408, 0.05649817734956741, -0.451223224401474, -0.4713315963745117, -0.3530312478542328, 0.5085893869400024, 0.8952107429504395, -0.10885103791952133, -0.303814172744751, 0.2375558614730835, 0.39726337790489197, 0.20696841180324554, -0.2835789620876312, -0.4365259110927582, 0.09187001734972, -0.04766976833343506, 0.14768607914447784, 0.39298737049102783, 0.0535564087331295, -0.5571455359458923, -0.547881543636322, -0.013539987616240978, 0.347788542509079, -0.6138438582420349, -0.7759388089179993, 0.8721345663070679, -0.013026213273406029, -0.5123066902160645, -0.07252389937639236, -0.40713444352149963, -0.40362176299095154, 0.27516868710517883, -1.5517816543579102, -0.4060400724411011, 0.5620075464248657, -0.5261755585670471, -0.28024864196777344, -0.017346931621432304, 1.1104873418807983, 0.10733657330274582, -0.1671168953180313, -0.3188821077346802, 0.45112496614456177, 0.13676853477954865, 0.18562231957912445, -1.1220388412475586, 0.955157458782196, 0.328741192817688, -0.49918869137763977, 0.3938239514827728, 0.2943192422389984, -0.32730063796043396, -0.6454660892486572, -0.08045867830514908, 0.9984171986579895, -0.9391745328903198, -0.4885095953941345, -0.8087993264198303, -0.8125261664390564, 0.12471252679824829, 1.0735719203948975, -0.6145482063293457, 0.22796127200126648, -0.1321788728237152, -0.3795982301235199, -0.15246284008026123, -0.7750324010848999, -0.01798662543296814, 0.5638493895530701, -0.5726261138916016, -0.41228824853897095, 0.1716943383216858, 0.28935402631759644, -0.7807723879814148, -0.5674548745155334, -0.1654031127691269, 0.2156497836112976, -0.29897215962409973, 0.5223872065544128, -0.20480334758758545, 0.7331936359405518, 0.8590549826622009, 0.3391627073287964, -0.7429633140563965, -0.2041272222995758, -1.1584457159042358, 0.19764840602874756, 0.5870968699455261, 0.5111972093582153, -0.2746794819831848, 0.4585517346858978, 0.822385311126709, 0.1466580331325531, -0.19044920802116394, -0.924676775932312, -0.6920318603515625, 0.5116331577301025, -0.3631718158721924, 0.4390750825405121, -0.06967969238758087, 0.1965724229812622, 0.04250016063451767, 0.24249742925167084, 0.688580334186554, -0.2008856236934662, -0.8612586259841919, 0.6596390604972839, -0.12373607605695724, 0.31449493765830994, -0.46285581588745117, -0.47365453839302063, -1.0550658702850342, 0.4211672246456146, -1.0128726959228516, 0.030820658430457115, -1.545462727546692, -0.3499675393104553, 0.7908919453620911, 0.37327995896339417, 0.10516364872455597, 0.37754103541374207, -0.40599530935287476, -0.37326714396476746, -0.8798063397407532, -0.5665595531463623, 0.9157493710517883, 0.7787835001945496, -1.132631540298462, 0.24070075154304504, -0.1817111223936081, -0.05288359150290489, 0.15300168097019196, 0.2051732838153839, -0.5231651067733765, -0.43155187368392944, -1.024882435798645, 0.2755393385887146, 0.021941931918263435, -0.12620452046394348, -0.8371291756629944, 0.6799018979072571, 0.5637996792793274, 0.040648482739925385, -0.34373295307159424, 0.22145430743694305, -0.813079833984375, -0.5773510932922363, 0.33118391036987305, -0.8362832069396973, 0.09178327769041061, -0.10155649483203888, -0.4199962615966797, -0.34057629108428955, 0.6279318332672119, -0.053956497460603714, -1.1092803478240967, 0.16649359464645386, 0.39965566992759705, -0.7830888628959656, 0.2508355379104614, -0.42053380608558655, -0.24008694291114807, -1.2361223697662354, -0.4389326572418213, -0.08149003982543945, 0.5710658431053162, -0.29928159713745117, 0.7385446429252625, -0.07876036316156387, -0.910332202911377, -0.2945840060710907, 0.138930544257164, -0.3737471103668213, 0.07123208045959473, 0.8355037569999695, 0.047432854771614075, -0.14166438579559326, 0.844839334487915, 0.5735827088356018, 0.45337870717048645, -0.7944129109382629, -0.19580502808094025, 0.3189254701137543, -0.5818722248077393, -0.3755325376987457, 0.5652784705162048, -0.7439749240875244, -0.8953131437301636, 0.04720132797956467, -1.1348189115524292, -0.7454338073730469, -0.024534787982702255, 0.6416253447532654, 0.5234294533729553, -0.4264805316925049, -0.32268795371055603, -0.23241737484931946, 0.44657108187675476, 0.01994946226477623, -0.7114489674568176, 0.8944297432899475, 0.011505480855703354, -0.2160438746213913, 0.3917217552661896, 0.8395704030990601, -0.21900463104248047, -0.3512204587459564, -0.5566710233688354, -0.2110772430896759, 0.04867551103234291, 0.04577406495809555, -0.1915348321199417, -0.24211052060127258, 0.721635639667511, -0.023196492344141006, 0.8066844940185547, -0.2900241017341614, 0.017957940697669983, 0.6190417408943176, 0.6995352506637573, -0.481637567281723, -0.9756495356559753, -0.8315065503120422, 1.4160256385803223, 1.341315746307373, -0.38178277015686035, 0.6432598233222961, -0.12874697148799896, -0.6759457588195801, 0.7259297966957092, 0.16432145237922668, -0.17529016733169556, 0.5221654772758484, -0.03485897555947304, 0.29173824191093445, 0.11612024158239365, -0.9859488010406494, -0.5733194351196289, 0.6335874199867249, 0.999920129776001, 0.6548636555671692, -0.1693682074546814, -0.25275638699531555, 1.0555121898651123, -0.3066204786300659, 0.15221934020519257, 0.6310197710990906, 0.36219871044158936, -0.08946351706981659, -0.28277188539505005, -0.02310847118496895, 0.5747392177581787, -0.8552559018135071, -0.2330942451953888, -0.2528190314769745, 0.5546703338623047, 0.33522266149520874, 0.5860891938209534, 0.49035942554473877, 0.40183380246162415, 0.9927263259887695, 0.396243691444397, 0.4632406234741211, -0.4816855490207672, -0.4063046872615814, -0.03378897160291672, -0.4500952661037445, -0.23336337506771088, -0.3227424621582031, -1.083976149559021, -0.7405695915222168, -0.1971188634634018, 0.19679512083530426, 0.5822091102600098, 0.14242778718471527, 1.2885160446166992, 0.4801289141178131, 0.4078163206577301, -0.4085485339164734, -0.6877607107162476, -0.5305054187774658, -1.080316185951233, -0.28533828258514404, -0.4243621528148651, 0.2806064784526825, 0.269145131111145, -0.22831077873706818, -0.17891475558280945]}, "authors": [{"authorId": "2265754120", "name": "Daniele Giofr'e"}, {"authorId": "2265753476", "name": "Sneha Ghantasala"}], "references": [{"paperId": "be76b0f32e287d866bc7aefc700052f9825ed3ce", "title": "BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?"}, {"paperId": "b79323b909a7671449f0e06b9bd5d8473c58003b", "title": "Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities"}, {"paperId": "7618f17179bb316002cb6cc472d61382776af6b7", "title": "Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT"}, {"paperId": "b77cb53e9a0d9fbe77b4b6d0982ed6fc9ad6f1a8", "title": "pNLP-Mixer: an Efficient all-MLP Architecture for Language"}, {"paperId": "5ac3e887de399e6c9149b0356e8992460be51ebd", "title": "New Approaches to Long Document Summarization: Fourier Transform Based Attention in a Transformer Model"}, {"paperId": "fd33e77884e69f6bc099990fc2790248af2749d9", "title": "LexGLUE: A Benchmark Dataset for Legal Language Understanding in English"}, {"paperId": "9be7473b765e0c87d055a461e3a66b8ff286bead", "title": "Hartley Stochastic Computing For Convolutional Neural Networks"}, {"paperId": "3a9efbd662326ec9338d4eaa32543f77a0834974", "title": "An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models"}, {"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms"}, {"paperId": "1e3e65e7773b7869d9bd7f5394b54199e48195e6", "title": "Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "6a1b25f7a67395ad1e676027322913acbb0a0635", "title": "CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review"}, {"paperId": "f5643a083f7410e5b2f8b5b4a1ee0baf6fcff52e", "title": "Multi-label legal document classification: A deep learning-based approach with label-attention and domain-specific pre-training"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "4329ceeac964cd2d061d4e151c505905636f76fa", "title": "MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities"}, {"paperId": "a2f38d03fd363e920494ad65a5f0ad8bd18cd60b", "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "edc90c96bc3b18ad73815db01795be92d0300417", "title": "BillSum: A Corpus for Automatic Summarization of US Legislation"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028", "title": "SciBERT: A Pretrained Language Model for Scientific Text"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "c80c3ff4156fd4c3a0dd0649ea8efc8559eecde6", "title": "Hartley Spectral Pooling for Deep Learning"}, {"paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e", "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "853d4d94651c6d9f8ed4d114e1eb21f15f786daa", "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents"}, {"paperId": "e03a297c4dcfd9dece1a642c8d640908535fcca7", "title": "Hartley transform and the use of the Whitened Hartley spectrum as a tool for phase spectral processing"}, {"paperId": null, "title": "Rethinking attention with per-formers"}, {"paperId": null, "title": "LEGAL-BERT: The Muppets straight out of Law School"}, {"paperId": null, "title": "Multi-Task Learning for Abstractive and Ex-tractive Summarization"}, {"paperId": null, "title": "Attention Is All You Need. arXiv:1706.03762 [cs"}, {"paperId": "6f446441441ce26bb410be56e4cdf99e57d84bf2", "title": "48th Annual Meeting of the Association for Computational Linguistics"}, {"paperId": null, "title": "Computing with the Hart-ley transform"}, {"paperId": "4f8ba5b22eaa7fe623f6760dec42b78e3b47241f", "title": "Faculty Opinions recommendation of BioBERT: a pre-trained biomedical language representation model for biomedical text mining."}, {"paperId": null, "title": "2022. Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences. arXiv:2201."}]}