{"paperId": "c3c469b8d3392aa1117e1d82bd3357d2c12d87ce", "title": "Raptor-T: A Fused and Memory-Efficient Sparse Transformer for Long and Variable-Length Sequences", "abstract": "Transformer-based models have made significant advancements across various domains, largely due to the self-attention mechanism's ability to capture contextual relationships in input sequences. However, processing long sequences remains computationally expensive for Transformer models, primarily due to the <inline-formula><tex-math notation=\"LaTeX\">$O(n^{2})$</tex-math><alternatives><mml:math><mml:mi>O</mml:mi><mml:mo stretchy=\"false\">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy=\"false\">)</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3389507.gif\"/></alternatives></inline-formula> complexity associated with self-attention. To address this, sparse attention has been proposed to reduce the quadratic dependency to linear. Nevertheless, deploying the sparse transformer efficiently encounters two major obstacles: 1) Existing system optimizations are less effective for the sparse transformer due to the algorithm's approximation properties leading to fragmented attention, and 2) the variability of input sequences results in computation and memory access inefficiencies. We present Raptor-T, a cutting-edge transformer framework designed for handling long and variable-length sequences. Raptor-T harnesses the power of the sparse transformer to reduce resource requirements for processing long sequences while also implementing system-level optimizations to accelerate inference performance. To address the fragmented attention issue, Raptor-T employs fused and memory-efficient Multi-Head Attention. Additionally, we introduce an asynchronous data processing method to mitigate GPU-blocking operations caused by sparse attention. Furthermore, Raptor-T minimizes padding for variable-length inputs, effectively reducing the overhead associated with padding and achieving balanced computation on GPUs. In evaluation, we compare Raptor-T's performance against state-of-the-art frameworks on an NVIDIA A100 GPU. The experimental results demonstrate that Raptor-T outperforms FlashAttention-2 and FasterTransformer, achieving an impressive average end-to-end performance improvement of 3.41X and 3.71X, respectively.", "venue": "IEEE transactions on computers", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Raptor-T harnesses the power of the sparse transformer to reduce resource requirements for processing long sequences while also implementing system-level optimizations to accelerate inference performance, and introduces an asynchronous data processing method to mitigate GPU-blocking operations caused by sparse attention."}, "embedding": {"model": "specter_v2", "vector": [0.6238877773284912, 0.32939770817756653, -0.6209978461265564, 0.0021533190738409758, -0.341338574886322, -0.03131528198719025, 0.36785292625427246, -0.24069689214229584, -0.19861550629138947, -0.11731445789337158, 0.42783114314079285, 0.3435608744621277, 0.46883049607276917, -0.1143801212310791, -0.40501394867897034, -0.02393832989037037, -1.0612635612487793, 0.295580118894577, 0.36536845564842224, 0.033772386610507965, 0.14283506572246552, -0.788066029548645, -1.6047528982162476, 0.29816800355911255, 0.5298768877983093, 0.7314982414245605, 0.3044988811016083, 0.9606261849403381, -0.4272974729537964, 0.44355493783950806, 0.7994527220726013, -0.31581270694732666, 0.1791844666004181, -0.2142484337091446, -0.13414965569972992, -0.3914323151111603, 0.5479364991188049, -0.09082488715648651, -0.7149617671966553, 0.729495644569397, -0.33836179971694946, 0.3741395175457001, 0.3277820348739624, -0.7331857085227966, -0.03660895302891731, 0.7698875665664673, 0.5300480127334595, 0.975847065448761, -0.4365605115890503, -0.7124050855636597, 1.5381309986114502, -1.1869335174560547, 0.25530990958213806, 1.2702630758285522, 0.5304332971572876, 0.033683039247989655, -0.3869326412677765, -0.528911828994751, 0.4943373203277588, 0.31516170501708984, -0.7179774045944214, -0.39957866072654724, -0.08266938477754593, -0.34229952096939087, 1.992142915725708, -0.10671421885490417, 0.050103358924388885, 0.6925954222679138, 0.39627519249916077, 1.2101274728775024, -0.22187098860740662, -0.6871836185455322, -0.1081993505358696, -0.25286704301834106, 0.4750155806541443, 1.0408029556274414, -0.2444271445274353, 0.5887623429298401, -1.1852197647094727, -0.27677711844444275, 0.6399118900299072, 0.15641409158706665, 0.5521774888038635, -0.2959999740123749, -0.10173094272613525, 0.6867258548736572, 0.01064957957714796, 0.8284221887588501, -0.1947649121284485, 0.24695585668087006, 0.566398024559021, -0.08816582709550858, -0.042632248252630234, -0.07141302525997162, 0.1755976378917694, 0.0212259404361248, -1.0414601564407349, 0.2931040823459625, -0.072542704641819, 1.0848017930984497, 0.019983669742941856, 0.38986626267433167, -0.6534371972084045, 0.2198634296655655, 1.2231578826904297, 0.217218279838562, 0.5058108568191528, -0.6446225643157959, 0.02556203305721283, -0.5129500031471252, 0.013937348499894142, -1.1200107336044312, -0.25734901428222656, -0.41265252232551575, -0.6990271210670471, -1.172675609588623, -0.3555142879486084, 0.5784722566604614, -0.4486154019832611, 0.612199068069458, -0.2341538965702057, 0.5203999876976013, -0.2043057680130005, 0.36382681131362915, 0.6296461820602417, 0.39594194293022156, 0.26051396131515503, -0.10188836604356766, 0.9888874888420105, -0.9649213552474976, -0.6687469482421875, -1.446793794631958, 0.17762720584869385, -0.1305428445339203, -0.1559334397315979, -0.35727059841156006, -1.140190839767456, -1.0604246854782104, -0.5834440588951111, 0.06342920660972595, -0.3561815619468689, -0.07423114031553268, 0.9705942273139954, -0.16202084720134735, -0.8962640762329102, 0.8724222779273987, -0.42755821347236633, -0.09632285684347153, 0.47377657890319824, -0.023704519495368004, 0.37832000851631165, -0.14493712782859802, -1.4302898645401, 0.10645896941423416, 0.07520845532417297, -0.6132087111473083, -0.39976969361305237, -0.6715545654296875, -1.380466341972351, 0.12509454786777496, 0.5328765511512756, -0.30667561292648315, 1.0242146253585815, -0.16949766874313354, -0.7788297533988953, 0.7867853045463562, -0.7195765972137451, -0.42017170786857605, 0.007503811735659838, -0.35876035690307617, -0.48545652627944946, -0.1055334061384201, 0.27445945143699646, 0.04210071638226509, 0.40806645154953003, -0.2963944375514984, -0.419320672750473, -0.206881582736969, -0.7234673500061035, -0.17037586867809296, 0.04614583030343056, 0.9116635918617249, -0.8554642200469971, -0.4362044930458069, 0.03604968264698982, 0.5235305428504944, -0.11849505454301834, -0.23018211126327515, -0.1943352222442627, -0.6845420598983765, 0.5645177364349365, 0.14044848084449768, 1.3676869869232178, -0.6729671359062195, -1.0310254096984863, -0.04968961700797081, -0.11485735327005386, -0.10159924626350403, -0.5469945669174194, 0.5659391283988953, -0.50319904088974, 0.01600421406328678, -0.3502357602119446, -0.6590391397476196, -0.2643837332725525, 0.09722942113876343, -0.8204526305198669, -0.1054656133055687, -0.06560114771127701, 1.04843270778656, -1.3875812292099, -0.4199158549308777, 0.11123698949813843, 0.3053494095802307, -0.9765368700027466, 1.491478681564331, -0.5219351053237915, -0.17144934833049774, -0.21712449193000793, -0.24827440083026886, -0.11597597599029541, -0.6051613092422485, 0.531502366065979, -0.3635854125022888, -0.3014220893383026, 0.5116162300109863, -0.1393412947654724, 1.1537275314331055, -0.02663053199648857, 0.47814616560935974, -0.27345821261405945, -0.6234557032585144, 0.477523535490036, 0.3283330798149109, 0.36104512214660645, -0.7474414110183716, 0.45082366466522217, -0.28284594416618347, -0.6473073959350586, -0.1116906926035881, 0.7610596418380737, 0.9927756190299988, -0.07753431797027588, 0.20963406562805176, 0.45128950476646423, 0.2776776850223541, 0.5653583407402039, 0.5924938321113586, 0.8207480311393738, 0.6945460438728333, 0.5391628742218018, -0.2737756669521332, 0.1353600025177002, -0.7891930341720581, 0.24836201965808868, 0.733379602432251, 0.6622214317321777, 0.814361572265625, 0.2813762426376343, -0.7037708163261414, -0.48412439227104187, 0.5086658000946045, 0.4314589202404022, 1.6017959117889404, -0.13138356804847717, -0.3613017499446869, -0.6994509696960449, -0.13568845391273499, -0.2247704565525055, -0.05823657289147377, -0.6966212391853333, -0.27906525135040283, -0.4571594297885895, -0.8979731202125549, 0.8639487624168396, 0.5108059644699097, 0.8703390955924988, -0.7249036431312561, -0.6653010249137878, 0.18760870397090912, 0.061114080250263214, -0.8233570456504822, -0.7760666012763977, 0.5670641660690308, -0.4654732644557953, -0.24938678741455078, 0.5541269183158875, -0.054906804114580154, -0.06295912712812424, -0.438871830701828, 1.2931187152862549, -0.6552569270133972, -0.5246620774269104, 0.33813217282295227, 0.4588458836078644, -0.4068804681301117, -0.25445637106895447, -0.051216915249824524, -0.042874932289123535, -0.06524714827537537, 0.583560049533844, 0.1431095153093338, 0.010668091475963593, 0.23826220631599426, -0.1416139155626297, 0.07654588669538498, 0.06184039264917374, 0.442739337682724, 0.8136446475982666, -0.2849477529525757, -0.06344097852706909, -1.4205687046051025, -0.02285127341747284, -0.19085972011089325, -0.46083635091781616, -0.10685911029577255, -0.5528406500816345, -0.190434530377388, 0.6993706822395325, -0.5789792537689209, -0.5236597657203674, -0.6736809611320496, 0.426805317401886, -0.6097336411476135, -0.2442183643579483, -0.05711566284298897, 0.1081947535276413, 0.1784072369337082, 0.39455291628837585, 0.7282274961471558, 0.2922685742378235, 0.13088087737560272, 0.40511611104011536, -0.9057890176773071, 0.9296637177467346, -0.16251282393932343, -0.17945516109466553, 0.15514612197875977, -0.01189213152974844, -0.9185667634010315, -0.5283660292625427, -0.6661334037780762, -0.22194965183734894, -0.20452383160591125, 0.09484105557203293, -0.4234728515148163, -1.1752761602401733, -0.10826064646244049, -0.8293744325637817, -0.39608895778656006, 0.5282541513442993, -0.2980806231498718, -0.4679970443248749, -0.7401225566864014, -1.1863610744476318, -0.7373859286308289, -0.8303042650222778, -0.8110007047653198, 0.3286774754524231, 0.08831605315208435, -0.3918718695640564, -0.6709039211273193, 0.15434889495372772, -0.5257475972175598, 0.7976764440536499, -0.3101305067539215, 0.3724142014980316, -0.17579126358032227, -0.42478519678115845, -0.003931604325771332, -0.027822382748126984, -0.06080359220504761, -0.1512155532836914, 0.23588649928569794, -0.747883141040802, 0.29378992319107056, -0.19412747025489807, 0.16249851882457733, -0.03893472999334335, 0.40503644943237305, 0.8583102822303772, -0.12822549045085907, -0.8518775701522827, 0.3899715840816498, 1.6151447296142578, -0.2441566288471222, 0.13265199959278107, 0.10324684530496597, 0.7801379561424255, 0.021845631301403046, 0.41432955861091614, 0.8237429261207581, 0.2883374094963074, 0.5884236693382263, 0.37815430760383606, -0.007353649474680424, 0.05346791818737984, -0.3863644003868103, 0.1639365553855896, 1.1540461778640747, 0.19478318095207214, -0.013940500095486641, -0.908315122127533, 1.0959347486495972, -1.380302906036377, -1.3741791248321533, 0.5455042719841003, 0.7547435760498047, 0.3706260621547699, -0.472964882850647, -0.06954056024551392, -0.4240211248397827, 0.576789140701294, 0.2731492221355438, -0.6438210606575012, -0.40038979053497314, -0.023041754961013794, 0.5125195384025574, 0.31127625703811646, 0.5940369367599487, -0.38937899470329285, 0.37649211287498474, 15.271415710449219, 0.5687249302864075, 0.05491053685545921, 0.27306830883026123, 0.6330710649490356, 0.33827951550483704, -0.043790724128484726, -0.009835309349000454, -1.4862713813781738, 0.2362433522939682, 1.4015886783599854, -0.027583742514252663, 0.3311903774738312, 0.5144067406654358, 0.13494336605072021, 0.08157409727573395, -0.505835771560669, 0.5565739274024963, 0.786312997341156, -1.0527750253677368, 0.21007387340068817, -0.027284765616059303, -0.11860876530408859, 0.19116416573524475, 0.6164190769195557, 0.46326321363449097, 0.7999560236930847, -0.35545283555984497, 0.16661056876182556, 0.49906498193740845, 0.8968580365180969, -0.24153605103492737, 0.2409706562757492, 0.019865404814481735, -1.3962459564208984, 0.11766193062067032, -0.698994517326355, -1.1418462991714478, 0.05612689256668091, 0.23682422935962677, -0.5863445997238159, -0.21445883810520172, 0.1565409004688263, 1.0128037929534912, 0.3444707691669464, 0.43623173236846924, 0.034604523330926895, 0.5807039737701416, 0.356371134519577, 0.30152973532676697, 0.23078176379203796, 0.33239078521728516, 0.22122500836849213, 0.3069407045841217, 0.10113607347011566, 0.5125906467437744, 0.05955714359879494, 0.2757355570793152, 0.19513282179832458, -0.38048914074897766, -0.6902341842651367, -0.15333592891693115, 0.18804951012134552, 0.8549811244010925, 0.46712374687194824, 0.20989830791950226, -0.5253878235816956, 0.04730046167969704, 0.6715891361236572, -0.08214086294174194, -0.6748854517936707, 0.0323825441300869, 0.6954017281532288, -0.43697115778923035, 0.285835862159729, 0.7582358717918396, -0.44176238775253296, -0.3573128283023834, -1.1046710014343262, -0.6645951271057129, 0.885650098323822, -0.6097243428230286, -0.7912209630012512, 1.066325306892395, -0.27303776144981384, -0.455784410238266, -0.05678488314151764, -0.6441633105278015, -0.18115076422691345, 0.16300062835216522, -0.8052058219909668, -0.39897620677948, 0.27661651372909546, -0.3876923620700836, -0.1735585480928421, 0.10043718665838242, 1.1060014963150024, 0.042444705963134766, -0.47614896297454834, 0.27846193313598633, -0.030971048399806023, -0.07186425477266312, -0.1275135725736618, -0.5487275123596191, 0.8507968783378601, 0.4719679057598114, -0.2851405441761017, 0.804432213306427, 0.2571282982826233, -0.1367003172636032, -0.8410197496414185, -0.39010730385780334, 0.40846502780914307, -0.8861003518104553, 0.0035347945522516966, -0.6627814769744873, -1.2783633470535278, 0.5497433543205261, 0.6172865629196167, -0.02618410810828209, 0.4329763650894165, -0.03676054626703262, -0.43865087628364563, -0.13016389310359955, -0.6156092286109924, -0.11302439123392105, 0.90203857421875, -0.49962612986564636, -0.5004984140396118, -0.6417419910430908, 0.18743272125720978, -1.03119957447052, -0.790934145450592, -0.1635565310716629, 0.41279536485671997, -0.44977763295173645, 1.2756876945495605, -0.15684613585472107, 0.7895580530166626, 0.945325493812561, -0.0012651687720790505, -0.5303135514259338, -0.37695813179016113, -0.7709083557128906, -0.3990445137023926, 0.3441610038280487, 0.19803683459758759, -0.33249834179878235, 0.3376907408237457, 0.6501870155334473, 0.20543836057186127, -0.514204204082489, -0.4160659611225128, -0.3148150146007538, -0.2995178997516632, -0.3194522261619568, 0.22589552402496338, -0.03935439512133598, 0.13348641991615295, 0.1105317696928978, 0.16381044685840607, 0.6254807114601135, 0.32231515645980835, -0.2494666427373886, 0.2324223816394806, -0.10335346311330795, -0.09614603221416473, -0.41577574610710144, -0.46821537613868713, -1.2347153425216675, -0.04120367392897606, -0.9198451042175293, 0.4011891782283783, -0.5587189793586731, -0.24663753807544708, 0.06236221268773079, -0.2766210734844208, 0.1344745010137558, 0.04042679816484451, -0.32792723178863525, -0.8952666521072388, -0.7453904747962952, -0.7124024033546448, 0.5669095516204834, 0.6864088773727417, -0.4488864839076996, 0.30586832761764526, -0.22564882040023804, 0.13419659435749054, 0.1742180436849594, 0.38046249747276306, -0.5515060424804688, -0.5027390718460083, -0.9832244515419006, 0.1837913990020752, 0.1469712108373642, -0.07584251463413239, -0.594523012638092, 0.8613768815994263, 0.16050024330615997, -0.23753215372562408, -0.5738757848739624, 0.39565399289131165, -0.9499956369400024, -0.12349654734134674, 0.4368446469306946, -0.6778870820999146, 0.16926485300064087, 0.398526668548584, -0.4292822480201721, -0.2710039019584656, 0.8849993944168091, 0.1942329853773117, -1.1411819458007812, -0.7026979327201843, 0.6021058559417725, -0.6397324204444885, 0.5459910035133362, -0.23160816729068756, 0.0803099274635315, -1.2607903480529785, -0.19013820588588715, 0.1264766901731491, 0.41795089840888977, -0.31256103515625, 1.148260474205017, 0.5854530334472656, -1.113108515739441, 0.35281339287757874, 0.507736325263977, -0.07594592124223709, 0.0776963159441948, 0.6902981400489807, 0.16354240477085114, 0.1366175413131714, 0.743182361125946, -0.17129741609096527, 0.1510830819606781, -0.9820951223373413, 0.4908655881881714, 0.47002676129341125, -0.4734264016151428, -0.3244653642177582, 0.7088071703910828, -0.0788881778717041, -0.49648144841194153, -0.0006368986214511096, -0.6704572439193726, -0.7313240170478821, -0.2691699266433716, 0.9366092681884766, 0.6203005313873291, -0.4466441571712494, -0.44388797879219055, -0.7875435948371887, 0.2767465114593506, -0.25539514422416687, -0.18844279646873474, 0.7130559086799622, -0.11783970147371292, -0.5037927031517029, 0.6649908423423767, 0.5434054732322693, -0.7148020267486572, -0.8296531438827515, -0.8413912057876587, -0.5202044248580933, -0.39297401905059814, 0.18360090255737305, -0.14370708167552948, -0.5201998353004456, 0.5648381114006042, 0.35056403279304504, 0.32794949412345886, -0.05657016113400459, -0.2996821403503418, 0.16243110597133636, 0.3693690001964569, 0.0888916477560997, -0.3767487704753876, -0.6774488687515259, 1.178849220275879, 1.084568738937378, -0.4857350289821625, 0.05331427603960037, -0.44068044424057007, -0.5888155698776245, 0.6517574787139893, 0.6662592887878418, -0.18411728739738464, 0.5677226185798645, 0.21838238835334778, -0.10490704327821732, 0.00957967434078455, -1.3363993167877197, -0.5021438598632812, 0.8603229522705078, 1.1641262769699097, 0.5501121878623962, -0.10122326016426086, 0.3037685453891754, 0.7819942831993103, 0.25872328877449036, 0.2721960246562958, 0.14039166271686554, 0.2856220602989197, -0.4140084683895111, 0.14825347065925598, -0.12059438228607178, 0.8837108612060547, -0.7039939761161804, -0.7690219283103943, 0.45456892251968384, 0.46834057569503784, 0.00043494527926668525, 0.6735197901725769, 1.0740216970443726, 0.016281235963106155, 0.14763416349887848, -0.07422127574682236, 0.416584849357605, -0.5190625190734863, -0.13062578439712524, -0.06657713651657104, -0.6328406929969788, -0.45523467659950256, -0.18231090903282166, -0.7430210113525391, -0.3790562152862549, 0.10347066074609756, 0.4113246500492096, 0.13918887078762054, -0.06609926372766495, 1.1228266954421997, 0.697263240814209, 0.6514456868171692, -0.2469184398651123, -0.27713650465011597, 0.0371859185397625, -0.8181216716766357, 0.31242260336875916, -0.5074194073677063, -0.24645930528640747, -0.49767839908599854, -0.13435183465480804, -0.1513451784849167]}, "authors": [{"authorId": "2295845990", "name": "Hulin Wang"}, {"authorId": "2007708782", "name": "Donglin Yang"}, {"authorId": "2111131052", "name": "Yaqi Xia"}, {"authorId": "2274734490", "name": "Zheng Zhang"}, {"authorId": "2297058601", "name": "Qigang Wang"}, {"authorId": "2260856326", "name": "Jianping Fan"}, {"authorId": "2278825938", "name": "Xiaobo Zhou"}, {"authorId": "1778706", "name": "Dazhao Cheng"}], "references": [{"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "22b58dce1a13382418b8372bbd50ed3b2533f899", "title": "ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "33d15b2d2a434ab33a2a88585604f4728a324baf", "title": "Efficient Classification of Long Documents Using Transformers"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "b7b3343b45c785ccab1c94beecc28ea91e041685", "title": "E.T.: Re-Thinking Self-Attention for Transformer Models on GPUs"}, {"paperId": "952305d9bbdaeaf7adb7ef12b94f221570c5d52d", "title": "LightSeq2: Accelerated Training for Transformer-Based Models on GPUs"}, {"paperId": "0c775d7ed34fb4690b4291490778649ae75c48d2", "title": "TurboTransformers: an efficient GPU serving system for transformer models"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "00c957711b12468cb38424caccdf5291bb354033", "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "4f4de89e3adb6742433a8edc667fe07422082a15", "title": "Denoising based Sequence-to-Sequence Pre-training for Text Generation"}, {"paperId": "1a9954d86466a7e4de6f98ddee452ceb50e15d86", "title": "DocBERT: BERT for Document Classification"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "104715e1097b7ebee436058bfd9f45540f269845", "title": "Reading Wikipedia to Answer Open-Domain Questions"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "869eba611194fff8c172bee572b286970c6a6589", "title": "Summary"}, {"paperId": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "title": "Recurrent Neural Network Regularization"}, {"paperId": "eff3e2a802a63b15ce57498611165eccca0ddbe3", "title": "The average distances in random graphs with given expected degrees"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "4270413df12b36feb26a44a9111a3694fde8fffb", "title": "At the University of North Carolina"}, {"paperId": null, "title": "\u201cELECTRA: Pre-training text encoders as discriminators rather than generators,\u201d"}, {"paperId": null, "title": "\u201cNvidia a100 tensor core GPU architecture.\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "ec3071fb918ad69ec80df1ca9cf1fdeb386a9603", "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"}, {"paperId": "4954fa180728932959997a4768411ff9136aac81", "title": "This Paper Is Included in the Proceedings of the 12th Usenix Symposium on Operating Systems Design and Implementation (osdi '16). Tensorflow: a System for Large-scale Machine Learning Tensorflow: a System for Large-scale Machine Learning"}, {"paperId": null, "title": "agreement with IEEE. Restrictions apply"}, {"paperId": null, "title": "We leverage sparse attention to reduce resource requirements for long sequences, introducing an novel fused and memory-ef\ufb01cient sparse multi-head attention mechanism"}, {"paperId": null, "title": "\u201cTensorRT.\u201d"}, {"paperId": null, "title": "\u201cFasterTransformer.\u201d"}, {"paperId": null, "title": "We conduct a comprehensive analysis of the current sparse attention based transformer, identifying its performance bottlenecks in system implementation"}]}