{"paperId": "edbec80e5bdbfcc096103b11491a42ca7794ac4d", "title": "Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models", "abstract": "Despite their prevalence in deep-learning communities, over-parameterized models convey high demands of computational costs for proper training. This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy. Empirical evidence reveals that when scaling down into network modules, such as heads in self-attention models, we can observe varying learning patterns implicitly associated with each module's trainability. To describe such modular-level learning capabilities, we introduce a novel concept dubbed modular neural tangent kernel (mNTK), and we demonstrate that the quality of a module's learning is tightly associated with its mNTK's principal eigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module learns features with better convergence, while those miniature ones may impact generalization negatively. Inspired by the discovery, we propose a novel training strategy termed Modular Adaptive Training (MAT) to update those modules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively, concentrating the model on learning common features and ignoring those inconsistent ones. Unlike most existing training schemes with a complete BP cycle across all network modules, MAT can significantly save computations by its partially-updating strategy and can further improve performance. Experiments show that MAT nearly halves the computational cost of model training and outperforms the accuracy of baselines.", "venue": "Neural Information Processing Systems", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy and proposes a novel training strategy termed Modular Adaptive Training (MAT), which can significantly save computations by its partially-updating strategy and can further improve performance."}, "embedding": {"model": "specter_v2", "vector": [-0.03912059962749481, 1.1080142259597778, -0.6380736827850342, -0.15752704441547394, -0.058541569858789444, 0.421975314617157, 0.20204129815101624, -0.6320316195487976, -0.5498284697532654, -0.5183512568473816, 0.029668502509593964, 0.37348103523254395, 0.11656713485717773, 0.13211990892887115, -0.10281927138566971, -0.15437796711921692, -1.16907799243927, -0.07614581286907196, 0.4818029999732971, -0.025822622701525688, -0.45702216029167175, -0.6801130771636963, -1.0387829542160034, 0.08463655412197113, 0.5064356923103333, 0.8656435608863831, 0.18982642889022827, 0.7250208258628845, 0.010140307247638702, 0.10251804441213608, 0.7340816259384155, -0.72748863697052, 0.5755013823509216, 0.3752003312110901, -0.25518134236335754, 0.3131834864616394, 0.6491762399673462, -0.2557169497013092, -0.869534969329834, 0.8574478030204773, -0.5747523903846741, 0.11577973514795303, 0.37631794810295105, -0.8827585577964783, 0.2304752767086029, 0.5738891959190369, 0.8159478902816772, 0.7170403599739075, -1.1444228887557983, -0.3608494699001312, 1.3548310995101929, -1.2637091875076294, -0.23463940620422363, 1.4213238954544067, 0.7937584519386292, 0.4114416241645813, -0.8190621733665466, -0.6098594665527344, 0.7292020320892334, -0.12260501831769943, -0.9188095331192017, -0.0561475045979023, 0.03906897455453873, -0.3286484181880951, 1.5404284000396729, -1.0023576021194458, -0.32334092259407043, 0.2501935362815857, 0.45308810472488403, 1.390564203262329, -0.13291844725608826, -0.342937171459198, -0.3637648820877075, 0.46495842933654785, 0.1547011286020279, 0.830944299697876, -0.0336729921400547, 0.22241848707199097, -0.9382879734039307, -0.1970767080783844, 0.6766575574874878, -0.08226893097162247, 0.45098677277565, -0.49077752232551575, 0.027370061725378036, 1.0023020505905151, 0.7398685812950134, 0.3040187656879425, -0.6009402871131897, 1.1944730281829834, 0.25217729806900024, 0.49288269877433777, 0.16853342950344086, 0.11583517491817474, -0.42165467143058777, 0.7376545667648315, -0.7331226468086243, 0.26595476269721985, -0.1785646378993988, 0.9118785262107849, 0.7520501613616943, 0.3293648362159729, 0.011300232261419296, 0.37189698219299316, 1.5140787363052368, -0.2459573894739151, 0.5107420086860657, -0.7202327251434326, 0.4009106755256653, 0.16180609166622162, -0.36263829469680786, -0.5210735201835632, -0.6883200407028198, -0.5437406897544861, -0.9608975648880005, -0.9858851432800293, -0.647007405757904, 0.15472599864006042, -0.5435622930526733, 0.8245794773101807, -0.20765504240989685, -0.0895165428519249, -0.09123818576335907, 0.2987271249294281, -0.07127529382705688, 0.11414607614278793, 0.9204155802726746, 0.46181678771972656, 0.8422796726226807, -1.057511329650879, -0.24854741990566254, -0.9714186787605286, 0.5665019750595093, -0.013507109135389328, -0.07773907482624054, -0.33675646781921387, -1.1834245920181274, -1.2942142486572266, -1.0570076704025269, 0.5580047965049744, -0.2699258029460907, 0.2713920772075653, 1.4231336116790771, 0.3544163405895233, -0.829067587852478, 1.3994449377059937, -0.2541142702102661, -0.42918097972869873, 0.6548386812210083, 0.7805638313293457, -0.04672025144100189, -0.12378721684217453, -1.0083588361740112, 0.2959177494049072, 0.7254835367202759, -0.8031396865844727, -0.37320587038993835, -0.7840994000434875, -0.7701600790023804, -0.13807545602321625, 0.02920529805123806, -1.2179497480392456, 1.5361008644104004, -0.7103936076164246, -1.0299433469772339, 0.7846421599388123, 0.20579858124256134, 0.011661866679787636, 0.19590327143669128, 0.08881448209285736, -0.6547122001647949, 0.008715227246284485, -0.4314020574092865, 0.6939522624015808, 0.7916431427001953, -0.0904519185423851, -0.38572949171066284, -0.009713014587759972, -0.1888972371816635, 0.001282412908039987, -0.5461145639419556, 0.6276627779006958, -0.29159003496170044, -0.29032692313194275, 0.6833168864250183, 0.3754948377609253, -0.18677638471126556, 0.05062870308756828, -0.20803441107273102, -1.4007859230041504, 0.4950745701789856, 0.45484447479248047, 0.8935622572898865, -1.0759679079055786, -0.8022386431694031, -0.011424045078456402, 0.19764116406440735, -0.2575772702693939, -0.5401836633682251, 0.685856282711029, -0.33220869302749634, 0.2038135528564453, -0.016293758526444435, -1.2625703811645508, 0.3391166031360626, -0.16145208477973938, -0.19997291266918182, 0.0004101078666280955, 0.6912367939949036, 0.8333581686019897, -0.6462467312812805, 0.05431164801120758, -0.5535566210746765, 0.08731535077095032, -0.9531781673431396, 1.254778504371643, -0.2837353050708771, -0.3399565815925598, 0.08236045390367508, -0.10429723560810089, 0.47643762826919556, -0.5190526843070984, 0.2969806492328644, -0.6567126512527466, -0.08276679366827011, 0.34413716197013855, -0.6487578749656677, 1.3721507787704468, -0.0551849864423275, 0.5752849578857422, -0.028778497129678726, -0.6766299605369568, -0.34199851751327515, 0.602699875831604, 0.12887777388095856, -0.23612239956855774, 0.2656509578227997, 0.19553279876708984, -0.657630980014801, 0.3282370865345001, 0.8073464035987854, 0.9599632620811462, -0.1584998518228531, 0.15287311375141144, 1.2742395401000977, 0.008255703374743462, 0.11320410668849945, 0.3325461745262146, 0.3154177963733673, 0.291696161031723, 0.32965224981307983, -0.44859033823013306, 0.031080856919288635, -0.9993227124214172, 0.0259375162422657, 0.4620985686779022, 0.2993927299976349, 0.37545904517173767, 0.31086960434913635, -0.6878699064254761, -0.6440482139587402, -0.23988065123558044, 0.203237384557724, 2.022982120513916, -0.19298772513866425, 0.07769957184791565, -0.7484083771705627, -0.33214008808135986, 0.17318952083587646, -0.40100759267807007, -0.48364850878715515, -0.5299820899963379, -0.4335656464099884, -1.4782650470733643, 0.9980747103691101, 0.3233056664466858, 1.2744537591934204, -0.16751651465892792, -0.3052169680595398, 0.040961168706417084, 0.8412169218063354, -0.551217257976532, -0.7114006876945496, 0.776002824306488, -0.7260459661483765, -0.27399805188179016, 0.028986090794205666, -0.14436006546020508, 0.3810725808143616, -0.3451599180698395, 0.6236985921859741, -0.6345105171203613, -0.16051112115383148, 0.5034406185150146, 0.882671594619751, -0.7211624383926392, -0.5106739401817322, 0.3421494960784912, 0.47890275716781616, 0.2825782001018524, 0.09531214088201523, 0.5245813727378845, -0.4642602503299713, 0.6699163913726807, -0.641525387763977, 0.4890424311161041, 0.5422207713127136, 0.16091667115688324, 0.47376030683517456, -0.0383785106241703, 0.41642051935195923, -1.3417260646820068, 0.8973445892333984, -0.07021953165531158, -0.24380934238433838, 0.09120401740074158, -1.0371936559677124, -0.17745471000671387, 0.2181391716003418, -1.1494954824447632, -0.445515900850296, -0.8125990629196167, 0.6928465366363525, -0.2413887083530426, -0.10833822190761566, 0.0695008635520935, 0.39070042967796326, -0.8866855502128601, 0.6885150074958801, 0.18262583017349243, 0.1366664469242096, 0.06112313270568848, 0.33312126994132996, -1.3462395668029785, 0.52610844373703, 0.11723967641592026, 0.6591576337814331, -0.10790786892175674, -0.08533921837806702, -0.5584007501602173, -0.2483173906803131, -0.5670577883720398, -0.5337766408920288, -0.3385290205478668, -0.10951819270849228, -0.89132159948349, -0.6786662340164185, 0.27044713497161865, -0.49766847491264343, -0.44669073820114136, 0.17543655633926392, 0.030692674219608307, -0.28182926774024963, -1.1401622295379639, -1.1425431966781616, -0.4379119575023651, -0.661102294921875, -0.6990000009536743, -0.1504266858100891, 0.5963125228881836, -0.03120814822614193, -0.6979923844337463, -0.11210101842880249, -0.6348093748092651, 1.4437131881713867, -0.972174346446991, 0.7762165069580078, 0.03804836794734001, -0.46722885966300964, -0.22012296319007874, -0.2107824981212616, 0.7472403049468994, -0.7271092534065247, 0.2063368260860443, -1.2563401460647583, 0.25754547119140625, -0.6062628030776978, -0.38145703077316284, 0.32578903436660767, 0.2242598533630371, 0.6794871687889099, -0.13435105979442596, -0.41576433181762695, 0.6607891917228699, 1.3870478868484497, -1.0962388515472412, -0.10808556526899338, 0.212355837225914, 0.7704183459281921, 0.39859017729759216, -0.6216461658477783, 0.47558531165122986, 0.7601578235626221, 0.4448321759700775, 0.2856576442718506, -0.461210697889328, -0.10651176422834396, -0.6219311952590942, 0.14736445248126984, 1.320591926574707, 0.36053717136383057, 0.4339190423488617, -0.5987194180488586, 0.4407449960708618, -1.3603534698486328, -0.567704975605011, 0.7799797058105469, 0.8032394051551819, 0.09538485109806061, -0.28919968008995056, -0.04867403581738472, -0.5218629240989685, 0.14025795459747314, 0.03028610348701477, -0.5665765404701233, -0.4649884104728699, -0.20467983186244965, 0.19351497292518616, 0.6083045601844788, -0.08473667502403259, -0.7639507055282593, 0.9019815325737, 14.602816581726074, 0.6615164875984192, 0.0719570443034172, 0.7417194247245789, 0.3938836455345154, 0.9100239276885986, -0.2774710953235626, 0.2868875563144684, -0.9939137697219849, -0.11878104507923126, 1.1909313201904297, 0.03310232236981392, 0.46124565601348877, 0.5267255902290344, -0.194345161318779, 0.06243075802922249, -0.3322162330150604, 0.39694759249687195, 0.06810836493968964, -1.098768711090088, 0.2570940852165222, 0.21736466884613037, 0.6040713787078857, 0.8248752951622009, 0.7194373607635498, 1.0542305707931519, 0.3421742618083954, 0.032861076295375824, 0.21614627540111542, 0.32375261187553406, 0.5038557052612305, 0.10464591532945633, 0.19206303358078003, 0.5027645826339722, -1.184594750404358, -0.1594664752483368, -1.063240647315979, -0.6254892945289612, -0.11905132979154587, -0.029699018225073814, -0.5838026404380798, -0.3453563451766968, -0.12072782218456268, 0.4316340684890747, -0.39907342195510864, 1.023046851158142, -0.09439242631196976, 0.47150397300720215, -0.2064870297908783, 0.3562004864215851, -0.04796271026134491, 0.6483405232429504, -0.025835899636149406, 0.28833165764808655, -0.2935120761394501, -0.4624587595462799, 0.11648181080818176, 0.6024242043495178, -0.5854835510253906, -0.10957561433315277, 0.0005490264156833291, -0.20084436237812042, -0.28434622287750244, 1.238924264907837, 0.9651792645454407, 0.2355882227420807, -0.3202521502971649, 0.3393336832523346, 0.7878621816635132, 0.48750486969947815, -0.25205469131469727, 0.011933884583413601, 0.6633270978927612, -0.6615998148918152, -0.4295843839645386, 0.3313926160335541, -0.14125284552574158, -0.5656954646110535, -0.9583520889282227, 0.05127705633640289, 0.651174008846283, -0.7949883341789246, -0.8753833174705505, 0.9244723916053772, -0.09772995859384537, -0.4204241633415222, 0.48702338337898254, -0.9390683770179749, -0.48547637462615967, 0.5847915410995483, -1.339241623878479, -0.8913210034370422, 0.11757663637399673, -0.052342381328344345, -0.5317885279655457, -0.34526535868644714, 1.2329422235488892, 0.4739515483379364, -0.8914358019828796, 0.6384737491607666, -0.22257593274116516, -0.21271491050720215, -0.059153590351343155, -0.683030366897583, 0.8618303537368774, 0.0798129141330719, -0.3880634307861328, 0.40044087171554565, -0.3289508819580078, 0.6754681468009949, -0.6287036538124084, -0.126196026802063, 0.47793763875961304, -0.2978087365627289, 0.03358183801174164, -0.2807239890098572, -1.0094935894012451, 0.25706642866134644, 0.5994933247566223, -0.3093385696411133, 0.2646509110927582, 0.28270113468170166, -0.897316575050354, -0.4649968445301056, -0.5877872109413147, -0.003189306240528822, 0.22467964887619019, -1.0683958530426025, -0.164908766746521, 0.05156222730875015, 0.21524153649806976, -0.7051277160644531, -0.8469700813293457, -0.0163526963442564, 0.042687512934207916, -0.16094335913658142, 0.8877036571502686, -0.5992259383201599, -0.05736977979540825, 0.7834137082099915, 0.08798196911811829, -0.7362254858016968, -0.43210750818252563, -0.7939527034759521, 0.09998694807291031, -0.04709606617689133, 0.48662757873535156, -0.8503283262252808, 0.5979866981506348, 1.0681134462356567, -0.2048058956861496, -0.5796666145324707, -0.6516971588134766, 0.1490752100944519, -0.26103606820106506, -0.8122570514678955, -0.18628428876399994, -0.11144378781318665, -0.035788848996162415, -0.37157031893730164, 0.5487821698188782, 0.8464972972869873, 0.4138246774673462, -0.9285450577735901, 0.4491910934448242, -0.4060741662979126, -0.0057638478465378284, -1.0395010709762573, -0.9530340433120728, -1.5510207414627075, -0.2552121579647064, -1.3629541397094727, -0.3108968436717987, -0.5350828170776367, -0.4603084623813629, -0.12316510081291199, -0.6618041396141052, -0.09751804172992706, 0.40333566069602966, -0.2804109752178192, -0.5935760140419006, -0.008499938994646072, -0.47032099962234497, 0.7473592162132263, 0.8110777139663696, -0.6614156365394592, 0.06947459280490875, 0.4834347069263458, 0.06904160231351852, 0.5000381469726562, 0.6846873760223389, -0.5827164053916931, -0.5398792624473572, -1.1662720441818237, 0.49560651183128357, -0.4308581054210663, 0.18801409006118774, -1.101028323173523, 1.370304822921753, 0.3055382966995239, 0.005106406286358833, 0.07169371843338013, 0.2976642847061157, -0.9182981252670288, -0.43095988035202026, 0.27089059352874756, -0.38520604372024536, 0.4082567095756531, 0.2723935544490814, -0.3563334047794342, -0.08888295292854309, 0.6906694173812866, 0.23937568068504333, -0.7744285464286804, -1.022417426109314, 0.7620012164115906, -0.5910903215408325, -0.01943851448595524, -0.2910473942756653, -0.22213375568389893, -1.3194986581802368, 0.2246471792459488, -0.08487768471240997, 0.5088167190551758, -0.4690978229045868, 0.5827353000640869, 0.3626028299331665, -1.1956226825714111, 0.2745882570743561, 0.7085120677947998, 0.33409109711647034, 0.08709447085857391, 1.157201886177063, 0.7719793319702148, -0.3005763590335846, 0.45242899656295776, 0.11542171239852905, 0.1755581796169281, -0.17432478070259094, -0.1711423248052597, 1.1238921880722046, -0.266273558139801, -0.15383300185203552, 1.1659990549087524, 0.224631205201149, -1.1412798166275024, 0.27722424268722534, -1.072450876235962, -0.25163301825523376, -0.3901593089103699, 0.22566868364810944, 0.1652107983827591, -0.32071104645729065, -0.011575630865991116, -0.20279601216316223, 0.1464657485485077, -0.23035094141960144, -0.1162971556186676, 0.05782931298017502, -0.221918523311615, -0.1073623076081276, 0.5334228873252869, 0.6805570125579834, -1.283880352973938, -1.423095941543579, -1.1180850267410278, -0.13673779368400574, 0.10799778252840042, 0.09450322389602661, -0.03709457814693451, -0.7284483313560486, 0.4709998071193695, 0.8046079874038696, 0.16842599213123322, -0.05562269687652588, -0.02603672258555889, -0.3798827826976776, 0.7686131000518799, 0.20039935410022736, -0.8828355669975281, -0.21086488664150238, 0.8658376336097717, 0.8444983959197998, -0.8925498723983765, 0.45896071195602417, -0.02173842117190361, -0.6614656448364258, 1.1089441776275635, 0.6214665770530701, -0.18480493128299713, 1.0961806774139404, -0.8606137037277222, 0.022165322676301003, 0.15628986060619354, -1.2115404605865479, -0.46610137820243835, 1.4800440073013306, 0.5846298336982727, 0.3304271399974823, 0.5409498810768127, 0.4968552589416504, 0.9589092135429382, 0.19402916729450226, -0.39938637614250183, 0.1833687126636505, 0.16420413553714752, -0.6585147380828857, 0.261213481426239, 0.42835360765457153, 0.578100860118866, -0.5734322667121887, -0.1506863832473755, 0.4923587441444397, 0.8826518058776855, 0.4336529076099396, 0.34235331416130066, 0.6103229522705078, -0.425420880317688, 0.563371479511261, 0.340658038854599, 0.4144241213798523, -0.4083032011985779, -0.31533846259117126, -0.08050394803285599, -0.7243468761444092, -0.299796462059021, -0.2354467660188675, -0.1515754759311676, -0.1435825526714325, -0.04668136313557625, 0.1000431552529335, -0.5546150207519531, 0.7283235192298889, 0.718174397945404, 0.5936841368675232, 0.7071123719215393, 0.10438568145036697, -0.8016085028648376, -0.7030904293060303, -0.8935319781303406, -0.09232831746339798, -0.3266605734825134, -0.4099198877811432, -0.3163571059703827, -0.38203972578048706, -0.5645198225975037]}, "authors": [{"authorId": "2023783880", "name": "Yubin Shi"}, {"authorId": "2301262648", "name": "Yixuan Chen"}, {"authorId": "2257291757", "name": "Mingzhi Dong"}, {"authorId": "2301166464", "name": "Xiaochen Yang"}, {"authorId": "2259817156", "name": "Dongsheng Li"}, {"authorId": "2268830944", "name": "Yujiang Wang"}, {"authorId": "2237150162", "name": "Robert P. Dick"}, {"authorId": "2248285664", "name": "Qin Lv"}, {"authorId": "49339835", "name": "Yingying Zhao"}, {"authorId": "2268853628", "name": "Fan Yang"}, {"authorId": "2298215514", "name": "Tun Lu"}, {"authorId": "2258958923", "name": "Ning Gu"}, {"authorId": "144456488", "name": "L. Shang"}], "references": [{"paperId": "b6da4e11e24da4e863bbc1c5c7bd6080d0906b98", "title": "NTK-SAP: Improving neural network pruning by aligning training dynamics"}, {"paperId": "4e2905f3c9ec6c3625a415954e8232836a4ecb4f", "title": "A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel"}, {"paperId": "cf8f5baabeba69ddca9f8b65e05d467b50253e0a", "title": "Winning the Lottery Ahead of Time: Efficient Early Network Pruning"}, {"paperId": "b069ff1ac62b1c64d2a0c56812ebc798c98b296c", "title": "Fast Finite Width Neural Tangent Kernel"}, {"paperId": "63746313a1ec95f77650084988bc89691689c427", "title": "Spectral Bias Outside the Training Set for Deep Networks in the Kernel Regime"}, {"paperId": "09d1005208d1dcb530d3ff0ae40f98cc331a60f5", "title": "TorchNTK: A Library for Calculation of Neural Tangent Kernels of PyTorch Models"}, {"paperId": "e5516bc158bb8d973dbebbcae9bd99cd33e6cce7", "title": "Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization"}, {"paperId": "2d3352e13f9d84ebad2a34ede9cb7341893b095b", "title": "KNAS: Green Neural Architecture Search"}, {"paperId": "2dcc6d63e840217306ecdef527ca66706eddc2cb", "title": "Group Fisher Pruning for Practical Network Compression"}, {"paperId": "e6307c6ef9c66a613935e7b531c4aa50864af8f8", "title": "Multirate Training of Neural Networks"}, {"paperId": "6503d119bed4d7de7b46c7ca812f4a8315916769", "title": "Tensor Programs IIb: Architectural Universality of Neural Tangent Kernel Training Dynamics"}, {"paperId": "8d84c38f5fce1bd1b4ae1d55400c8fb7fa5d19c8", "title": "Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "f3d52d22a04e96f63e710091a50ce4d244338854", "title": "Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "573cf589caa7e131c17fa5c45fa817f3dce6cfa5", "title": "Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers"}, {"paperId": "8da3ed272c07733ca46eab023b03c7411dfcfc42", "title": "Loss landscapes and optimization in over-parameterized non-linear systems and neural networks"}, {"paperId": "c114ce10c4a315d92c3815f54bc9893e7e6ef182", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow"}, {"paperId": "37a1d26a5e7ee404fdf4f6f9aeede91045671dcd", "title": "Disentangling Trainability and Generalization in Deep Neural Networks"}, {"paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97", "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "3f1edf11bc3ca1f0ed116cf72ff51d092ad8644b", "title": "Generalization Guarantees for Neural Networks via Harnessing the Low-rank Structure of the Jacobian"}, {"paperId": "29090beb90c184a9aaf7aa610bfed5ee1631d2f2", "title": "Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks"}, {"paperId": "4ac62731b802c727f916e8deefda1a992991505d", "title": "Are All Layers Created Equal?"}, {"paperId": "14558cb69319eed0d5bfc5648aafcd09d882f443", "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"paperId": "aa3a7e7f4fbb3587122d869056b4f0509a6318ae", "title": "A Note on Lazy Training in Supervised Differentiable Programming"}, {"paperId": "4ae204f99e4b3864a628c47e41e0e54a16c36906", "title": "Predicting the Computational Cost of Deep Learning Models"}, {"paperId": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0", "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d", "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"}, {"paperId": "7a84a692327534fd227fa1e07fcb3816b633c591", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "8501e330d78391f4e690886a8eb8fac867704ea6", "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "081651b38ff7533550a3adfc1c00da333a8fe86c", "title": "How transferable are features in deep neural networks?"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "497eb64754cace3afd79aab6062fddf8ca21b60f", "title": "The effective rank: A measure of effective dimensionality"}, {"paperId": "7db5f7ea2d1f9aa04c6e5c3d46a8fee5548c0904", "title": "Over-parameterized Model Optimization with Polyak-\u0141ojasiewicz Condition"}, {"paperId": "86ee946179119b65c57171d8a2ddaa1eebc0e7ed", "title": "Exposing and Exploiting Fine-Grained Block Structures for Fast and Accurate Sparse Training"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}]}