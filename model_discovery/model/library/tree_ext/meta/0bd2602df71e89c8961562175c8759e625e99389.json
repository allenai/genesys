{"paperId": "0bd2602df71e89c8961562175c8759e625e99389", "title": "ModuleFormer: Learning Modular Large Language Models From Uncurated Data", "abstract": "Large Language Models (LLMs) have achieved remarkable results. But existing models are expensive to train and deploy, and it is also difficult to expand their knowledge beyond pre-training data without forgetting previous knowledge. This paper proposes a new neural network architecture, ModuleFormer , that leverages modularity to improve the efficiency and flexibility of large language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE). Unlike previous SMoE-based modular language model [Gururangan et al., 2021], which requires domain-labeled data to learn domain-specific experts, ModuleFormer can induce modularity from uncurated data with its new load balancing and load concentration losses. ModuleFormer is a modular architecture that includes two different types of modules, new stick-breaking attention heads, and feedforward experts. Different modules are sparsely activated conditions on the input token during training and inference. In our experiment, we found that the modular architecture enables three important abilities for large pre-trained language models: 1) Efficiency, since ModuleFormer only activates a subset of its modules for each input token, thus it could achieve the same performance as dense LLMs with more than two times throughput; 2) Extendability, ModuleFormer is more immune to catastrophic forgetting than dense LLMs and can be easily extended with new modules to learn new knowledge that is not included in the training data; 3) Specialisation, finetuning ModuleFormer could specialize a subset of modules to the finetuning task, and the task-unrelated modules could be easily pruned for a lightweight deployment.", "venue": "arXiv.org", "year": 2023, "citationCount": 12, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/2306.04640", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A new neural network architecture, ModuleFormer, that leverages modularity to improve the efficiency and flexibility of large language models and is more immune to catastrophic forgetting than dense LLMs."}, "embedding": {"model": "specter_v2", "vector": [-0.06511767208576202, 0.7775976657867432, -0.14102476835250854, 0.21525508165359497, -0.2230377048254013, -0.18136435747146606, 0.39847174286842346, -0.3314705789089203, -0.21722149848937988, -0.02066378854215145, 0.4761732816696167, 0.0432862713932991, 0.6709374785423279, 0.20722118020057678, -0.2593248188495636, 0.2967185080051422, -1.0295072793960571, 0.2375294268131256, 0.11198244988918304, 0.027686746791005135, -0.4739707112312317, -0.7661069631576538, -1.0492862462997437, 0.23834328353405, 0.640426516532898, 0.31237921118736267, 0.5906725525856018, 0.5471624135971069, -0.35197949409484863, 0.6433291435241699, 0.5208942294120789, -0.03080175444483757, 0.2876611649990082, -0.007348738145083189, -0.22404541075229645, 0.22412316501140594, 0.25279319286346436, -0.16829346120357513, -0.34757059812545776, 0.5242668986320496, -0.18917350471019745, 0.14874090254306793, 0.12117505073547363, -0.31866487860679626, 0.017283475026488304, 1.190210223197937, 0.5115765333175659, 0.42475414276123047, -0.20861400663852692, -0.35370364785194397, 1.098402976989746, -1.4654169082641602, 0.1779043972492218, 1.6418324708938599, 0.6381397843360901, 0.4592864215373993, -0.6061598658561707, -0.9133373498916626, 0.9824056625366211, -0.09566310793161392, -1.1118571758270264, -0.4197726547718048, -0.21738101541996002, -0.3255304992198944, 2.112238883972168, -0.5545370578765869, -0.2585847079753876, 0.49403315782546997, 0.08044341951608658, 1.2449253797531128, -0.25955769419670105, -0.5127272009849548, -0.3165512681007385, 0.31397545337677, 0.0938759595155716, 0.8173111081123352, -0.2426784485578537, 0.01058804802596569, -0.8982619047164917, -0.11118661612272263, 0.3848380148410797, 0.07938536256551743, 0.3558364510536194, -0.06176092103123665, -0.09847584366798401, 0.7320888638496399, 0.2759867012500763, 0.8928504586219788, -0.14221015572547913, 0.5355308055877686, 0.42206817865371704, 0.42085811495780945, 0.2726806700229645, 0.32938212156295776, -0.44370532035827637, 0.6356422901153564, -0.8387292623519897, -0.01775551028549671, 0.10071128606796265, 0.9335858821868896, 0.09422744810581207, 0.5199000239372253, -0.07621891051530838, 0.19288821518421173, 1.5034252405166626, 0.11273746937513351, 0.9278252124786377, -0.442473441362381, 0.3801589906215668, -0.18543922901153564, -0.19591614603996277, -0.5888776779174805, -0.8351457118988037, -0.6972351670265198, -1.0521025657653809, -1.3540090322494507, -0.9555422067642212, -0.08669760823249817, -0.5082805156707764, 1.008232831954956, -0.4736308455467224, -0.1519165188074112, -0.18668605387210846, -0.027210339903831482, 0.630711019039154, 0.5851702690124512, 0.5581041574478149, 0.27127745747566223, 0.6797053813934326, -0.9360575675964355, -0.3837985098361969, -1.202877402305603, 0.6307274699211121, -0.22295869886875153, -0.2281179130077362, -0.338265597820282, -1.194817066192627, -1.171165108680725, -0.6630691885948181, 0.2949806749820709, -0.5246531963348389, 0.034651920199394226, 1.1223011016845703, 0.4531535506248474, -0.6932032108306885, 0.49263283610343933, -0.32903724908828735, -0.39533689618110657, 0.2629780173301697, 0.33610159158706665, 0.07959005236625671, -0.46135637164115906, -1.5079927444458008, 0.6845101714134216, 0.6394956707954407, -0.5930262207984924, -0.6080722212791443, -0.5115438103675842, -1.2218021154403687, -0.04154690355062485, 0.0435541495680809, -0.7659618258476257, 1.224940538406372, -0.7337018847465515, -0.9454091787338257, 0.5333871841430664, -0.3398069739341736, 0.14396090805530548, 0.02044738084077835, 0.08898042142391205, -0.5040848255157471, -0.555446982383728, -0.35613805055618286, 0.43627840280532837, 0.5438581109046936, -0.03911196067929268, -0.19102296233177185, 0.3903658986091614, -0.29287824034690857, -0.4582644999027252, -0.6022793650627136, 0.9874415397644043, -0.0015210827114060521, -0.15992134809494019, 0.2801874279975891, 0.8485773801803589, -0.20570705831050873, -0.3679993748664856, -0.19533391296863556, -1.0392472743988037, 0.5728452801704407, -0.3314618766307831, 0.9163009524345398, -0.9966741800308228, -0.4887043833732605, -0.14845408499240875, 0.06768544018268585, -0.07592000812292099, -0.9576632976531982, 0.8696383237838745, -0.5612834095954895, 0.31753507256507874, -0.14468654990196228, -1.3737051486968994, 0.201433002948761, -0.3168250620365143, -0.23413674533367157, -0.1362898200750351, 0.3480927348136902, 1.093528389930725, -1.0303369760513306, 0.01741691119968891, -0.23629339039325714, 0.062125228345394135, -1.0137546062469482, 1.2656108140945435, -0.562892496585846, -0.08341319859027863, 0.1804036796092987, -0.11486859619617462, 0.4044605791568756, -0.3059319853782654, 0.5400957465171814, -0.1503342241048813, 0.05822387710213661, 0.3867358863353729, -0.7669149041175842, 1.671182632446289, -0.7559292316436768, 0.4958869516849518, 0.20246165990829468, -0.6464410424232483, 0.037646688520908356, 0.714667797088623, -0.03804309666156769, -0.24592536687850952, 0.1845802217721939, 0.22880437970161438, -0.7124195694923401, -0.03348512947559357, 0.5622896552085876, 0.27665185928344727, -0.2998044490814209, 0.30082350969314575, 0.9097273945808411, 0.0701703205704689, 0.42567238211631775, 0.551002025604248, 0.7624500393867493, 0.06622768938541412, 0.6455514430999756, 0.09325161576271057, 0.21695031225681305, -1.1679023504257202, 0.12903305888175964, 0.22162044048309326, 0.7376177310943604, 0.7110181450843811, 0.3783610761165619, -0.6416841149330139, -0.2592432498931885, -0.2665536403656006, 0.6502029299736023, 1.5176821947097778, -0.5911628603935242, -0.03217969462275505, -0.40097877383232117, -0.11099612712860107, -0.20076392590999603, -0.06689389795064926, -0.010879679583013058, -0.05165259540081024, -0.3304690718650818, -1.155813455581665, 0.5590416193008423, 0.11973489820957184, 0.7777112126350403, -0.5255215764045715, -0.161591574549675, -0.05197805166244507, 0.16616939008235931, -0.6453762650489807, -0.4130556285381317, 0.4891473054885864, -0.6737667322158813, -0.16245698928833008, 0.1572185456752777, -0.46576163172721863, 0.5341184139251709, -0.41093745827674866, 1.2236552238464355, -0.8991906642913818, -0.23970364034175873, 0.35312092304229736, 0.30676770210266113, -0.36817947030067444, -0.7834358811378479, 0.3848804831504822, 0.4854511320590973, 0.05804389715194702, 0.07124745845794678, 0.15910737216472626, 0.25362440943717957, -0.0027104190085083246, -0.6182268857955933, 0.5173121690750122, 0.4918327033519745, -0.039198607206344604, 0.1692981719970703, 0.09897051751613617, 0.39157775044441223, -1.4180011749267578, 0.7490078806877136, -0.5692883133888245, -0.2736009359359741, 0.07485955953598022, -0.38238856196403503, -0.20813418924808502, 0.5443851947784424, -0.7532628178596497, -0.3157888352870941, -0.4725711941719055, 0.26542678475379944, -0.20161041617393494, -0.3165140450000763, 0.38578304648399353, 0.08476544916629791, 0.21422140300273895, 0.1532452553510666, 0.5235050916671753, 0.16823074221611023, -0.46846938133239746, 0.5983448028564453, -0.5759536623954773, 0.22078600525856018, 0.31105878949165344, 0.4521949887275696, -0.11251749843358994, -0.6450528502464294, -0.7384615540504456, -0.14374126493930817, -0.5435014963150024, -0.358225017786026, -0.11877398937940598, -0.2463943213224411, -1.215403437614441, -0.5359225273132324, 0.19764256477355957, -1.21297025680542, -0.2886624038219452, 0.20636750757694244, -0.36356663703918457, 0.22355186939239502, -0.9554262757301331, -1.0482017993927002, -0.6744564771652222, -0.7029447555541992, -0.45247772336006165, 0.257589191198349, 0.24162328243255615, -0.7986094355583191, -0.8667773604393005, 0.06344615668058395, -0.3951977491378784, 1.1675962209701538, -0.6364315748214722, 0.4690237045288086, 0.003502396633848548, -0.25948432087898254, -0.6313843727111816, 0.0453309565782547, 0.579789400100708, -0.09978976100683212, -0.07339612394571304, -1.0526052713394165, -0.047828808426856995, -0.29696759581565857, -0.393671452999115, 0.3497381806373596, 0.05718185380101204, 0.43972253799438477, 0.04906783998012543, -0.34642860293388367, 0.2594253718852997, 1.4375571012496948, -1.22183358669281, -0.16126751899719238, -0.27448827028274536, 0.8797158598899841, 0.2714233696460724, -1.229258418083191, 0.5760429501533508, 0.5368841290473938, 0.43671029806137085, 0.19488364458084106, -0.3009970188140869, -0.14242848753929138, -0.6758479475975037, 0.9190337657928467, 1.8097326755523682, 0.5767309665679932, 0.24255678057670593, -0.8678238391876221, 0.707512617111206, -1.2209240198135376, -0.8015291094779968, 0.6066176891326904, 0.6086578369140625, 0.5034759044647217, -0.36905789375305176, -0.3453647196292877, -0.4952344298362732, 0.2016073763370514, 0.03246480971574783, -0.5203420519828796, -0.5578172206878662, -0.24858781695365906, 0.37838664650917053, 0.12096446007490158, 0.6362755298614502, -0.44793274998664856, 0.45311039686203003, 14.840922355651855, 0.751655101776123, 0.03828214854001999, 1.2100750207901, 0.36340776085853577, -0.036578066647052765, -0.5628440380096436, -0.013426867313683033, -1.1807602643966675, 0.1284647285938263, 1.1958816051483154, 0.2164379358291626, 0.5732001662254333, 0.1036071702837944, 0.3110864460468292, 0.06840435415506363, -0.46819373965263367, 0.6807273030281067, 0.7065053582191467, -0.7939388751983643, 0.2452021837234497, -0.17353230714797974, 0.8894503116607666, 0.8544918894767761, 0.5172603130340576, 1.0304754972457886, 0.5821873545646667, -0.34257009625434875, 0.4417160749435425, 0.45585018396377563, 0.5723568797111511, 0.4121905267238617, 0.3181368112564087, 1.036895751953125, -1.1496751308441162, -0.3955885171890259, -0.518435001373291, -1.3626377582550049, 0.27465060353279114, 0.2798353433609009, -0.35572224855422974, -0.3979766368865967, -0.5700575113296509, 0.7832748889923096, -0.25841087102890015, 0.6626492738723755, -0.17384909093379974, 0.5011579394340515, -0.21835118532180786, 0.424244225025177, -0.11436665058135986, 1.0531576871871948, 0.13628238439559937, 0.1307896077632904, -0.46099722385406494, -0.731160581111908, 0.30638328194618225, 0.7924205660820007, -0.6381906867027283, -0.09261798858642578, -0.27097731828689575, -0.03534969314932823, -0.2505457401275635, 0.8712558746337891, 0.7944290041923523, 0.22961033880710602, -0.7294031381607056, 0.14912424981594086, 0.7704437375068665, 0.27968406677246094, -0.0751618966460228, 0.19781047105789185, 0.17785151302814484, -0.38724276423454285, -0.21392561495304108, 0.35025376081466675, -0.2730051577091217, -0.8772938251495361, -0.6768428087234497, -0.46885693073272705, 0.4466831088066101, -0.7504416704177856, -1.0800284147262573, 0.6403351426124573, -0.42051276564598083, -0.5260599255561829, 0.14627127349376678, -0.4896285831928253, -0.2717282474040985, 0.8952252268791199, -1.112455129623413, -0.9572616219520569, 0.5354618430137634, -0.46100643277168274, -0.1611144244670868, -0.2505315840244293, 1.362909197807312, 0.46262118220329285, -0.8359177112579346, 0.14961424469947815, 0.257833331823349, 0.11320763826370239, -0.126956507563591, -0.384197860956192, 0.7220290303230286, 0.43400296568870544, -0.410978227853775, 0.2439640909433365, -0.26226335763931274, 0.1228022426366806, -0.5516209602355957, -0.15404745936393738, 1.123205304145813, -1.0351296663284302, -0.22735142707824707, -0.9422698616981506, -0.6197918653488159, 0.40649402141571045, 0.6469283699989319, -0.6884450316429138, 0.558922529220581, 0.4002767503261566, -0.9447571635246277, -0.3115077018737793, -0.5550481677055359, -0.3105636239051819, 0.18912513554096222, -1.0071587562561035, -0.5788349509239197, 0.04835464060306549, 0.44316375255584717, -0.6618077158927917, -0.535449743270874, -0.3292044699192047, 0.16972535848617554, 0.2718171179294586, 0.8563553690910339, -0.6023083329200745, 0.4953669309616089, 0.8609421253204346, 0.20125623047351837, -0.8390018939971924, -0.22076481580734253, -0.7451618313789368, -0.3181605637073517, 0.09895668923854828, 0.642913281917572, -0.6427260041236877, -0.15262871980667114, 0.8901705145835876, 0.16312837600708008, -0.31983646750450134, -0.7400316596031189, -0.25002866983413696, 0.1442994326353073, -0.6155799031257629, -0.1524330973625183, -0.08883243799209595, 0.3069842457771301, 0.23151399195194244, 0.29390469193458557, 0.4338308870792389, 0.1831205040216446, -0.606515109539032, 0.3917902112007141, -0.21087609231472015, -0.43970611691474915, -0.8349695205688477, -0.6339043974876404, -1.3424925804138184, 0.2854268252849579, -1.3911120891571045, -0.061681583523750305, -0.9018173813819885, 0.1166326031088829, 0.43332499265670776, -0.28257042169570923, 0.207469642162323, 0.14977148175239563, -0.4585777521133423, -0.3477276861667633, -0.4430009722709656, -0.9002591967582703, 0.8764917850494385, 0.7153851985931396, -0.9995995759963989, -0.005598682910203934, 0.0014734198339283466, -0.04898553714156151, 0.36802050471305847, 0.4640108346939087, -0.5547831654548645, -0.5792273283004761, -1.525720238685608, 0.6039968132972717, -0.526328980922699, -0.14135141670703888, -0.836667001247406, 0.9118176698684692, 0.56817227602005, -0.13470414280891418, 0.21882811188697815, 0.0277548935264349, -0.8917997479438782, -0.31886208057403564, 0.6119937896728516, -0.578853189945221, 0.20651964843273163, 0.5125698447227478, -0.6018474698066711, -0.25330039858818054, 0.4944654405117035, -0.26305681467056274, -1.364798903465271, -0.7855133414268494, 0.29268699884414673, -1.0597610473632812, 0.19764837622642517, -0.26542195677757263, 0.2549611032009125, -1.0908162593841553, -0.287500262260437, -0.055159687995910645, 0.5710633397102356, -0.5238806009292603, 1.1061166524887085, 0.4177018105983734, -1.0181424617767334, -0.2133982926607132, 0.4443744719028473, -0.12248264998197556, 0.25010475516319275, 0.9698472619056702, 0.3592509627342224, -0.39416489005088806, 0.6888619661331177, 0.4878121614456177, 0.6057828664779663, -0.46888402104377747, 0.27004387974739075, 0.9032480716705322, -0.5600401163101196, 0.11452125757932663, 1.1193039417266846, -0.15312527120113373, -1.3021594285964966, 0.38310107588768005, -1.1281471252441406, -0.2600352466106415, -0.20376534759998322, 0.5386735200881958, -0.14447368681430817, -0.3502010107040405, 0.022262975573539734, -0.3819827437400818, 0.07293388247489929, 0.23479323089122772, 0.0006635052268393338, 0.6948021054267883, -0.29670846462249756, -0.7038992047309875, 0.9175924062728882, 0.8153924345970154, -0.7203232049942017, -0.3347577452659607, -1.0144392251968384, -0.3464258313179016, 0.23467764258384705, 0.21575862169265747, -0.2852439880371094, -0.5700485706329346, 0.5157666206359863, 0.5326442122459412, 0.2186255007982254, 0.22861692309379578, 0.003327758749946952, 0.1993982046842575, 0.47361814975738525, 0.3546384274959564, -0.5809815526008606, -0.5367660522460938, 1.329552412033081, 1.2672410011291504, -1.0238138437271118, 0.23625978827476501, 0.16322121024131775, -0.2777057886123657, 0.7565252780914307, 0.3131881654262543, 0.17643730342388153, 1.273894190788269, -0.25476330518722534, -0.10399404913187027, 0.43421417474746704, -1.335476040840149, -0.44512733817100525, 0.7972765564918518, 0.5914756655693054, 1.001090407371521, 0.5444172620773315, -0.02313649095594883, 1.008344054222107, 0.3002672791481018, 0.14459794759750366, 0.2402845025062561, 0.3026584982872009, -0.3813594579696655, -0.35000646114349365, -0.0624888613820076, 0.6072182059288025, 0.021491490304470062, -0.606301486492157, 0.13041706383228302, -0.21673858165740967, 0.7726569175720215, 0.8230457305908203, 0.7366747260093689, -0.07250042259693146, 0.5220716595649719, 0.4490647614002228, 0.2939520478248596, -0.9760335087776184, -0.4021567702293396, -0.2574750781059265, -0.4974312484264374, -0.06995920091867447, -0.26624709367752075, -0.3849634528160095, 0.06612086296081543, -0.3852640688419342, 0.3443811535835266, -0.5274043679237366, 0.2498278021812439, 1.1739493608474731, 0.7459185719490051, 0.7464239597320557, 0.05380752310156822, -0.47120538353919983, -0.2533935010433197, -1.3042478561401367, -0.14855681359767914, -0.1476641595363617, -0.3850793242454529, -0.014916660264134407, -0.32521677017211914, -0.24589049816131592]}, "authors": [{"authorId": "2714199", "name": "Yikang Shen"}, {"authorId": "2144388883", "name": "Zheyu Zhang"}, {"authorId": "2219272865", "name": "Tianyou Cao"}, {"authorId": "48988637", "name": "Shawn Tan"}, {"authorId": "2111329651", "name": "Zhenfang Chen"}, {"authorId": "2056157586", "name": "Chuang Gan"}], "references": [{"paperId": "7556332c94d177155c7255e36ab8ea78632adce7", "title": "Continual Learning and Catastrophic Forgetting"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "5b993855e5452e3a70fd7ff0790d8fb96f7cdc01", "title": "Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners"}, {"paperId": "3820231d31540ecb05d94c74d959a2f61d3136ea", "title": "Mixture of Attention Heads: Selecting Attention Heads Per Token"}, {"paperId": "51b950bfcaba4bad321e7342b32833d42f42c914", "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners"}, {"paperId": "7a25155364476839b6d1fc0653cd8611327ab9ba", "title": "mGPT: Few-Shot Learners Go Multilingual"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd", "title": "Internet-augmented language models through few-shot prompting for open-domain question answering"}, {"paperId": "4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904", "title": "Few-shot Learning with Multilingual Generative Language Models"}, {"paperId": "e528466e2aff981511d4ca6e063211297c0b4175", "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "917c63f2186119166b3379f5d2816bb1a2f39b09", "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling"}, {"paperId": "2fd1312b8507aae41bace1dd89712754a81fbc49", "title": "PonderNet: Learning to Ponder"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "fcfc9648561a221750b8085790ad9ba1bebb1800", "title": "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models"}, {"paperId": "789b5441743c2e38cf4c38749ed820c0671d81b1", "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "1187c70c4011f935642084e84186284ac0add3d0", "title": "Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning"}, {"paperId": "4bd7cc7d1fd2454956bafae1e00d2507bcbf5702", "title": "Learning to Continually Learn"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9d5c95cb7d17878b24fa66d9c6798efcd58bf65a", "title": "Regularization Shortcomings for Continual Learning"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "454655f4e14e3892bfb574bd283ac5d4184847f4", "title": "Pruning from Scratch"}, {"paperId": "1565dbd85a32d62c50347ff1c27f3f7aaa8e5ab1", "title": "IMHO Fine-Tuning Improves Claim Detection"}, {"paperId": "2a567ebd78939d0861d788f0fedff8d40ae62bf2", "title": "Publicly Available Clinical BERT Embeddings"}, {"paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028", "title": "SciBERT: A Pretrained Language Model for Scientific Text"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "4a1004ecd34118116344633c7cdcc34493c423ee", "title": "Rethinking the Value of Network Pruning"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "ee53c9480132fc0d09b1192226cb2c460462fd6d", "title": "Channel Pruning for Accelerating Very Deep Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d89ee98810039d2061ed42ee8026da49c503d16b", "title": "Learning multiple visual domains with residual adapters"}, {"paperId": "470d11b8ca4586c930adbbfc3f60bff08f2a0161", "title": "Meta Networks"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "2e55ba6c97ce5eb55abd959909403fe8da7e9fe9", "title": "Overcoming catastrophic forgetting in neural networks"}, {"paperId": "765db2669241e94f81f412ece531115a9b322c31", "title": "Low-rank bases for factorized hidden layer adaptation of DNN acoustic models"}, {"paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets"}, {"paperId": "8f3b80ddc0dd62e6c3369fabb1715990c29e9b9a", "title": "Learning without Forgetting"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "eae22611309fd49da6afca531a3237ea5ea1a0c6", "title": "Towards implicit complexity control using variable-depth deep neural networks for automatic speech recognition"}, {"paperId": "21c99706bb26e9012bfb4d8d48009a3d45af59b2", "title": "Neural Module Networks"}, {"paperId": "a467954bcd76c3b0a02a00950d6a0705e79840f9", "title": "Memory retention \u2013 the synaptic stability versus plasticity dilemma"}, {"paperId": "161ffb54a3fdf0715b198bb57bd22f910242eb49", "title": "Multitask Learning"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c213af6582c0d518a6e8e14217611c733eeb1ef1", "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "Natural language processing with transformers"}]}