{"paperId": "482ffdfc1e3594f5a22f51e3b9ee31e1c74f85de", "title": "TRAMS: Training-free Memory Selection for Long-range Language Modeling", "abstract": "The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric, and the results indicate an improvement without having additional training or adding additional parameters."}, "embedding": {"model": "specter_v2", "vector": [-0.04470371827483177, 0.3338318169116974, -0.21563789248466492, -0.2675149142742157, -0.08877544105052948, -0.08209210634231567, 0.7275451421737671, -0.4090263843536377, -0.8576963543891907, 0.12079823017120361, 0.4755667448043823, 0.31851235032081604, 0.16129659116268158, -0.07252363115549088, -0.11533070355653763, 0.24486345052719116, -0.8504641652107239, 1.1687445640563965, 0.3186158239841461, -0.3316206634044647, -0.3632708787918091, -0.6340984106063843, -0.888430655002594, 0.05939718708395958, 0.4450603425502777, 0.15744586288928986, 0.33763596415519714, 0.8039240837097168, -0.6509068012237549, 0.7405893206596375, 0.36263397336006165, -0.38561296463012695, 0.06371418386697769, 0.35595136880874634, -0.528029203414917, -0.6007168292999268, 0.3126661777496338, -0.2820810079574585, -0.5758378505706787, 0.5437529683113098, -0.09783918410539627, 0.26793867349624634, 0.40064895153045654, -0.6265740394592285, -0.20592471957206726, 1.1351191997528076, 0.8187665939331055, 0.7104163765907288, -0.3776034414768219, -0.5815562605857849, 1.587524175643921, -1.4881433248519897, 0.2508273422718048, 1.4138673543930054, 0.20681370794773102, 0.4803652763366699, -0.036119136959314346, -0.7485764622688293, 0.788559079170227, 0.2901151180267334, -1.032402753829956, -0.41582202911376953, 0.02854083850979805, -0.07236605882644653, 2.4363443851470947, -0.10364802181720734, 0.04640022665262222, 0.3860604166984558, -0.16896386444568634, 1.624407410621643, -0.6404737234115601, -1.0095415115356445, -0.8491505980491638, -0.20741452276706696, 0.7404772043228149, 0.8078253865242004, -0.5320499539375305, 0.042536500841379166, -0.8281010985374451, -0.18995089828968048, 0.6361976861953735, -0.10651031881570816, 0.20319780707359314, 0.03204996511340141, -0.5590048432350159, 0.6671569347381592, 0.264388769865036, 1.1979154348373413, -0.26790305972099304, 0.6832668781280518, 0.30284690856933594, 0.4900673031806946, -0.12692482769489288, 0.7347971796989441, 0.05287201330065727, 0.29523175954818726, -0.854688286781311, 0.3083030879497528, -0.07947694510221481, 0.9004563093185425, -0.17303091287612915, 0.3005273640155792, -0.963394284248352, 0.5471169948577881, 1.2004597187042236, 0.1242285817861557, 0.6418797373771667, -0.8195508122444153, 0.13524127006530762, -0.45892420411109924, 0.06204236298799515, -0.6440127491950989, -0.11140040308237076, -0.22538457810878754, -0.3560156524181366, -1.378212571144104, -0.3427237570285797, 0.18338559567928314, -0.5896822810173035, 0.8471523523330688, -0.5227228999137878, 0.05890476703643799, -0.10027023404836655, 0.36395058035850525, 0.5920581221580505, 0.7012116312980652, 0.5417227745056152, 0.1942266970872879, 0.5932135581970215, -1.2776694297790527, -0.4181986451148987, -1.6222952604293823, 0.6443854570388794, 0.12950488924980164, 0.3790638744831085, -0.13377177715301514, -1.337701439857483, -0.6602009534835815, -0.76377272605896, -0.19307219982147217, -0.5936805605888367, 0.12364869564771652, 1.056963324546814, -0.1885591447353363, -0.7642908692359924, 0.485045850276947, -0.08356065303087234, -0.06141354888677597, -0.13790538907051086, 0.23849521577358246, 0.15486164391040802, -0.5840098261833191, -2.0227067470550537, 0.5878925323486328, 0.12190492451190948, -0.473362535238266, -0.22108094394207, -0.2816995680332184, -0.9429290294647217, -0.02776452898979187, 0.3419550955295563, -0.41236451268196106, 1.0966379642486572, 0.022917984053492546, -0.9521585702896118, 0.6682283878326416, -0.4593159556388855, 0.12818336486816406, 0.004213979467749596, -0.4362294673919678, -0.5993236899375916, -0.9765720367431641, -0.11327964067459106, 0.7213870882987976, 0.1697712540626526, 0.12183591723442078, -0.09452199935913086, 0.0782187208533287, 0.012121863663196564, 0.11377200484275818, -0.5952978730201721, 1.0037013292312622, -0.6778531074523926, 0.05774378776550293, -0.058946702629327774, 0.5599052309989929, 0.02395406737923622, -0.4660886526107788, -0.3481713831424713, -1.2842968702316284, 0.3541223108768463, 0.3093199133872986, 1.4463733434677124, -0.7052502036094666, -0.3374468684196472, -0.4251481294631958, -0.1837690770626068, -0.08909648656845093, -0.866800844669342, 0.6710439920425415, -0.2467314600944519, 0.08871961385011673, -0.056843917816877365, -0.9569553136825562, 0.04718746244907379, 0.04926355928182602, -0.43207356333732605, -0.6082336902618408, 0.011265626177191734, 1.190001130104065, -0.760546088218689, -0.29337775707244873, -0.0798855572938919, 0.14920729398727417, -0.7300505042076111, 1.3947330713272095, -0.6549354791641235, -0.16067633032798767, -0.19783444702625275, -0.26340237259864807, -0.14209850132465363, -0.12796734273433685, 0.24964049458503723, 0.0011940391268581152, -0.4205131530761719, 0.6150690317153931, 0.06328315287828445, 1.3356099128723145, -0.3471597135066986, 0.4870969355106354, -0.5534708499908447, -0.4257015585899353, 0.11699800938367844, 0.22439667582511902, -0.20699039101600647, -0.5097397565841675, 0.10218070447444916, 0.5649300217628479, -0.4437759518623352, 0.2081010788679123, 0.7020149827003479, 0.7976637482643127, -0.5829885601997375, 0.02468772605061531, 0.5868406295776367, 0.042291197925806046, 0.6328780055046082, 0.3955715596675873, 0.7082436084747314, 0.24504147469997406, 0.27285951375961304, -0.08124136924743652, 0.5709714293479919, -0.4723902940750122, -0.09682989120483398, 0.4713144600391388, 0.9014124274253845, 0.8372140526771545, 0.1688353568315506, -0.850228488445282, -0.021363992244005203, 0.5353198647499084, 0.7512325644493103, 2.0629353523254395, -0.1622582972049713, -0.3323739171028137, -0.6718962788581848, -0.009254843927919865, -0.49768340587615967, 0.6410395503044128, -0.4087046682834625, -0.09770596772432327, -0.9513088464736938, -0.8522059321403503, 1.1032187938690186, 0.3441973328590393, 0.7650128602981567, -0.8626968860626221, -0.3898088335990906, -0.08307895064353943, -0.0028869726229459047, -0.48607173562049866, -0.7910482883453369, 0.4699964225292206, -0.19909736514091492, -0.022077642381191254, -0.16480837762355804, -0.2903948128223419, -0.19854843616485596, -0.6594597697257996, 1.122542142868042, -0.3505859076976776, -0.258516401052475, 0.004403663799166679, 0.9324097633361816, -0.628085196018219, -0.30173394083976746, 0.42057642340660095, 0.058506935834884644, -0.25952526926994324, 0.8310219049453735, 0.6494936943054199, 0.21211692690849304, 0.24024231731891632, -0.29496103525161743, 0.17201471328735352, 0.05479634180665016, 0.08241012692451477, 0.8175593614578247, -0.46744078397750854, -0.16038592159748077, -1.2667196989059448, 0.7067548036575317, 0.292033851146698, -0.44683191180229187, 0.6250507831573486, -0.5333641171455383, -0.2993134558200836, 0.4821912348270416, -0.9972444176673889, -0.16394823789596558, -0.9645827412605286, 0.3984282314777374, -0.4618653357028961, -0.022924479097127914, 0.6335383653640747, -0.011613158509135246, 0.4975440204143524, 0.2323698252439499, 0.6700921058654785, -0.013366556726396084, -0.35644474625587463, 0.5262158513069153, -0.8113754391670227, 0.5692897439002991, 0.17326952517032623, -0.03073887899518013, -0.6113501191139221, -0.44449955224990845, -0.8299863338470459, -0.4058995842933655, -0.4581120014190674, -0.31406205892562866, -0.1515445113182068, 0.2129005789756775, -0.421505868434906, -0.4500536024570465, 0.09608318656682968, -1.2832927703857422, -0.3062081038951874, 0.2995140552520752, -0.2722170352935791, 0.14246515929698944, -1.2738966941833496, -1.1955795288085938, -0.45302286744117737, -0.5431496500968933, -0.874441385269165, 0.10927869379520416, -0.2209530770778656, -0.4986587464809418, -0.7875991463661194, 0.4109291434288025, -0.3746718168258667, 1.4887195825576782, -0.662849485874176, 1.1730402708053589, -0.24642899632453918, -0.2936909794807434, -0.42250415682792664, 0.41154128313064575, 0.08218306303024292, -0.4633328914642334, 0.08363451808691025, -0.741018533706665, 0.31378912925720215, -0.40253737568855286, -0.27881017327308655, 0.30494454503059387, 0.41967839002609253, 0.4768027663230896, 0.044808294624090195, -0.8088246583938599, 0.054396044462919235, 1.0724334716796875, -0.577445387840271, 0.1995009034872055, 0.48458606004714966, 1.1876124143600464, 0.35129109025001526, -0.029665667563676834, 0.27385470271110535, 0.5639935731887817, 0.43099111318588257, 0.2715831995010376, -0.1931096613407135, 0.07279472798109055, -0.3228575885295868, 0.4866008162498474, 1.7248570919036865, -0.0676751434803009, -0.3431522846221924, -1.1321693658828735, 0.7662525177001953, -1.5868573188781738, -0.7470306754112244, 0.6377770304679871, 0.6833728551864624, 0.5226371884346008, -0.4337148368358612, -0.17701630294322968, -0.596662163734436, 0.28522372245788574, 0.34470364451408386, -0.5562052130699158, -0.6032754182815552, -0.2266726940870285, 0.18502704799175262, -0.10720321536064148, 0.8598007559776306, -0.20724014937877655, 0.7755020260810852, 14.5194673538208, 0.7491655349731445, -0.18231190741062164, 0.5512856245040894, 0.4987214505672455, 0.14361770451068878, -0.4941955506801605, -0.44625383615493774, -1.2959980964660645, -0.4902917742729187, 1.244303822517395, -0.232633575797081, 0.6178099513053894, 0.2529854476451874, -0.44404956698417664, 0.3732435405254364, -0.77540522813797, 0.6459841132164001, 0.5716139078140259, -0.8595917820930481, 0.6095171570777893, 0.09027281403541565, -0.24899724125862122, 0.7875416278839111, 0.6983461380004883, 0.9692857265472412, 0.557157039642334, -0.6384082436561584, 0.5974600911140442, 0.5545254349708557, 0.8690928220748901, -0.18912506103515625, 0.325547456741333, 0.6438677906990051, -0.6905707716941833, -0.3621762692928314, -1.0517476797103882, -1.068704605102539, 0.02950136363506317, 0.0749686136841774, -0.3333136737346649, -0.815605878829956, -0.42617562413215637, 0.6754657626152039, -0.036822449415922165, 0.3151910901069641, -0.10889167338609695, 0.5415180325508118, -0.02096729166805744, -0.08305501192808151, 0.339985191822052, 0.578903079032898, 0.4355112612247467, 0.14290280640125275, 0.14002929627895355, -0.09554564952850342, 0.13302665948867798, 0.49252086877822876, -0.3738378882408142, 0.07479085773229599, -0.3631505072116852, -0.4326391816139221, 0.12771405279636383, 0.845542311668396, 0.8715641498565674, 0.37310296297073364, -0.39888647198677063, 0.1822204738855362, 0.9017281532287598, -0.10209793597459793, -0.42757847905158997, -0.051974423229694366, 0.24824151396751404, -0.01861247792840004, -0.17139777541160583, 0.6144310832023621, 0.20643997192382812, -0.48885446786880493, -0.799106776714325, -0.2757737934589386, 0.5861158967018127, -0.5120524168014526, -0.7589733600616455, 1.5436969995498657, -0.020830176770687103, -0.21130113303661346, -0.24432887136936188, -0.7628685832023621, -0.287352979183197, 0.5626255869865417, -1.409938097000122, -1.0699223279953003, 0.30110567808151245, 0.054458506405353546, -0.2281622588634491, 0.037949394434690475, 1.6608716249465942, 0.45761099457740784, -0.47394075989723206, 0.12127336114645004, 0.06516528874635696, 0.024136031046509743, -0.13352125883102417, -0.746568500995636, 0.5149314403533936, 0.13613517582416534, -0.04551529884338379, 0.509198784828186, -0.0402233749628067, 0.2107323706150055, -0.692068338394165, -0.19395828247070312, 1.2564048767089844, -0.9853515028953552, -0.5978847742080688, -0.8804115653038025, -1.127371907234192, 0.6174705624580383, 0.7999602556228638, -0.45939621329307556, 0.37590208649635315, 0.2510588765144348, -0.48333269357681274, 0.007357272785156965, -0.5501391887664795, -0.0005569501663558185, 0.6177712678909302, -0.5810684561729431, -0.39422497153282166, 0.07133909314870834, 0.4097048342227936, -0.9134329557418823, -0.39832672476768494, -0.5908258557319641, 0.24535484611988068, 0.2764557898044586, 1.0598615407943726, -0.4405461251735687, 0.4709262251853943, 0.7983829975128174, -0.17394490540027618, -0.9369670748710632, -0.5331960916519165, -0.6460301280021667, 0.150964617729187, 0.19348682463169098, 1.1972001791000366, -0.46365252137184143, -0.19875772297382355, 1.3980810642242432, 0.3998658061027527, -0.22703371942043304, -0.5727234482765198, -0.362776517868042, 0.44000449776649475, -0.29710161685943604, 0.5441495180130005, 0.006342565640807152, 0.24326863884925842, 0.46617260575294495, 0.215067058801651, 0.3759590983390808, -0.525313138961792, -0.6031844615936279, 0.12648536264896393, -0.36353468894958496, -0.008878480643033981, -0.36606401205062866, -0.1011221781373024, -1.3662841320037842, 0.23129644989967346, -0.9616789817810059, 0.15963883697986603, -1.1516728401184082, -0.11930935084819794, -0.09306748956441879, -0.35960516333580017, 0.31995245814323425, 0.3890085220336914, -0.5746116042137146, -0.574926495552063, -0.348381906747818, -0.6995884776115417, 0.5504356026649475, 0.6711649298667908, -0.8101431727409363, 0.20018059015274048, -0.1532701700925827, 0.2578062415122986, 0.11042848974466324, 0.26298394799232483, -0.6278084516525269, -0.6676619052886963, -1.8543676137924194, 0.4302792549133301, -0.08664339780807495, -0.509793221950531, -0.7192529439926147, 1.0066338777542114, 0.07452137023210526, -0.27610212564468384, 0.14114999771118164, 0.6789330244064331, -0.8930339217185974, -0.18184097111225128, 0.25837334990501404, -0.9156912565231323, 0.4553421437740326, 0.01783701963722706, -1.1612424850463867, -0.5017481446266174, 0.5423499345779419, -0.12789656221866608, -1.3573647737503052, -0.7365128397941589, 0.480956494808197, -0.6153022646903992, 0.13519538938999176, -0.4188211262226105, -0.21608257293701172, -1.0465850830078125, -0.30542099475860596, 0.08119726181030273, 0.6701127290725708, -0.5826444625854492, 1.115262746810913, 0.47053390741348267, -1.0877330303192139, -0.07225963473320007, 0.3561229407787323, 0.08462448418140411, 0.044868070632219315, 0.27571243047714233, 0.34098634123802185, 0.03887452930212021, 0.8906381726264954, 0.1614048182964325, 0.4862178862094879, -1.0564091205596924, -0.12634991109371185, 1.044526219367981, -0.7686121463775635, -0.06976079940795898, 1.2593333721160889, -0.24857546389102936, -1.3017501831054688, 0.078246109187603, -0.9749636650085449, -0.9902509450912476, -0.6008738279342651, 0.9054757952690125, 0.20856589078903198, -0.22837549448013306, -0.4470715820789337, -0.803155243396759, 0.08259503543376923, -0.17600823938846588, -0.5199574828147888, 0.7012123465538025, -0.09298063069581985, -0.5527718663215637, 0.8747695088386536, 0.61495441198349, -0.6265732049942017, -0.21782836318016052, -0.6139633059501648, -0.2621268033981323, -0.18151405453681946, 0.2919606566429138, -0.4733767807483673, -0.5576589703559875, 1.0034469366073608, 0.09720448404550552, 0.3682597577571869, -0.22464093565940857, -0.20450672507286072, 0.20493993163108826, 0.748449444770813, 0.19168832898139954, -0.4648169279098511, -0.6078582406044006, 1.808728814125061, 1.594824194908142, -0.7190917134284973, -0.23710976541042328, -0.04888756200671196, -0.9229925870895386, 0.8540050983428955, 0.8435454964637756, 0.052527569234371185, 0.6564610004425049, -0.16546204686164856, 0.18440021574497223, -0.06689812988042831, -1.3769057989120483, -0.12761642038822174, 0.8449378609657288, 1.1147830486297607, 1.124448537826538, 0.18931998312473297, 0.3137590289115906, 0.8608086109161377, 0.15723054111003876, 0.15660057961940765, 0.2512526512145996, 0.27936235070228577, -0.3525380492210388, -0.19735202193260193, 0.31747958064079285, 0.4699174463748932, -0.41048237681388855, -0.7822839617729187, -0.1796904057264328, 0.6752036213874817, 0.0037571333814412355, 0.5230414271354675, 0.7110973000526428, 0.21285076439380646, 0.7328441739082336, 0.5078309774398804, 0.4544922113418579, -0.5723074078559875, -0.2745470404624939, -0.29894477128982544, -0.4477631449699402, -0.03210766986012459, -0.15865623950958252, -0.5068499445915222, -0.3890193998813629, -0.27427101135253906, 0.04021703824400902, 0.20749863982200623, -0.027378197759389877, 0.9096887111663818, 0.479518324136734, 0.44851967692375183, -0.7743024826049805, -0.13832490146160126, -0.28275129199028015, -1.2983119487762451, -0.03596238046884537, -0.7479791641235352, -0.40383660793304443, -0.39209067821502686, 0.042834389954805374, -0.7755322456359863]}, "authors": [{"authorId": "2261476365", "name": "Haofei Yu"}, {"authorId": "35504092", "name": "Cunxiang Wang"}, {"authorId": "2261496744", "name": "Yue Zhang"}, {"authorId": "2237804371", "name": "Wei Bi"}], "references": [{"paperId": "d9964ab436eefd21f923a4bc833c6b66692c7f00", "title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text"}, {"paperId": "dbc368bc8b49347dd27679894524fa62f88492c9", "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input"}, {"paperId": "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0", "title": "Hybrid Random Features"}, {"paperId": "e0cbbca02b332f398c6639b3bea0613f79166220", "title": "ABC: Attention with Bounded-memory Control"}, {"paperId": "64a29bee2e1ad29547d590a3cc26274f4c537145", "title": "Not All Memories are Created Equal: Learning to Forget by Expiring"}, {"paperId": "c0f709acf38eb27702b0fbce1215db0ebaa2de2b", "title": "SMYRF: Efficient Attention using Asymmetric Clustering"}, {"paperId": "29168348f4729d418df5acc8a5fce4f1c428a7e3", "title": "Sparsifying Transformer Models with Trainable Representation Pooling"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "fc18676ef52caf30004ba7a2ce884b84c2a5e552", "title": "When and Why is Document-level Context Useful in Neural Machine Translation?"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "7e53b30a8869e2f04f2834d8040588598bf1da2d", "title": "Checkpoint"}, {"paperId": "e20ff55e87e2b3ef02ae0529880bb705f5efbcae", "title": "Document-Level Neural Machine Translation with Hierarchical Attention Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "article and includes 28 thousand Wikipedia articles"}, {"paperId": null, "title": "Large text compression benchmark"}, {"paperId": null, "title": "Enwik8"}, {"paperId": "6a07515741593b34b9a60db857703b6255b76122", "title": "What will you need"}, {"paperId": null, "title": "B Training Configurations Since we do inference experiments based on a trained model, we separately train two Transformer-XL"}, {"paperId": null, "title": "2022. Efficient transformers: A survey"}, {"paperId": null, "title": "2022b. Efficient attention via control vari-ates"}, {"paperId": null, "title": "WikiText-103 , we use a 16-layer transformer architecture with 10 heads, 410 hid dim, 0.1 dropout ratio, 0.0 attention dropout ratio, 2100 inner dim,"}, {"paperId": null, "title": "2022b. Random feature attention"}, {"paperId": null, "title": "2022a. Linear complexity randomized self-attention mechanism"}, {"paperId": null, "title": "Transformer-XL models for WikiText-103 and enwik8 . For the training stage, we use Adam (Kingma and Ba, 2014) to optimize with a batch size=60, learning rate=2.5e-4, target length=150"}, {"paperId": null, "title": "10M characters separately. enwik8 has no pre-processing stage and is directly used. bpc (bit per character) is defined as an evaluation metric and we report results on both the dev set and test set"}, {"paperId": null, "title": "memory length=150, and a cosine scheduler warmup steps"}, {"paperId": null, "title": "for all inference experiments to avoid unfair comparison. All experiments including training and inference are conducted using 4 2080Ti GPUs. It takes 280 GPU hours to train the enwik8 model"}]}