{"paperId": "05b22d6ec2cff81bcfbac2a6cf67bc1e9ef0f60a", "title": "Improving Transformer with an Admixture of Attention Heads", "abstract": "Transformers with multi-head self-attention have achieved remarkable success in sequence modeling and beyond. However, they suffer from high computational and memory complexities for computing the attention matrix at each head. Recently, it has been shown that those attention matrices lie on a low-dimensional manifold and, thus, are redundant. We propose the Transformer with a Finite Admixture of Shared Heads (FiSHformers), a novel class of efficient and flexible transformers that allow the sharing of attention matrices between attention heads. At the core of FiSHformer is a novel finite admixture model of shared heads (FiSH) that samples attention matrices from a set of global attention matrices. The number of global attention matrices is much smaller than the number of local attention matrices generated. FiSHformers directly learn these global attention matrices rather than the local ones as in other transformers, thus significantly improving the computational and memory efficiency of the model. We empirically verify the advantages of the FiSHformer over the baseline transformers in a wide range of practical applications including language modeling, machine translation, and image classification. On the WikiText-103, IWSLT\u201914 De-En and WMT\u201914", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 15, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The proposed Transformer with a Finite Admixture of Shared Heads (FiSHformers), a novel class of efficient and flexible transformers that allow the sharing of attention matrices between attention heads, empirically verify the advantages of the FiSHformer over the baseline transformers in a wide range of practical applications including language modeling, machine translation, and image classification."}, "embedding": {"model": "specter_v2", "vector": [0.15292330086231232, 0.75115966796875, -0.39938047528266907, -0.033935900777578354, -0.26394766569137573, 0.2206273227930069, 0.6185906529426575, -0.4353426992893219, -0.1697072982788086, -0.24492593109607697, 0.7747865319252014, 0.255259245634079, 0.25385743379592896, 0.30830663442611694, -0.19042813777923584, -0.06976868212223053, -0.9191259145736694, 0.4411836266517639, -0.019663577899336815, -0.31452834606170654, -0.05871853232383728, -0.9280304312705994, -0.9910116791725159, -0.09325597435235977, 0.5808283686637878, 0.5271990895271301, 0.7838305234909058, 0.6437237858772278, -0.5817049145698547, 0.7629286050796509, 0.6148780584335327, -0.5246825814247131, 0.36866429448127747, -0.21816898882389069, -0.42091667652130127, 0.24937905371189117, 0.6013855934143066, -0.46047335863113403, -0.5464537143707275, 0.8646145462989807, -0.3107379972934723, 0.29133501648902893, 0.45533519983291626, -0.6548730134963989, -0.08353780210018158, 0.9687501788139343, 0.5140125751495361, 0.6757819652557373, -0.4537021219730377, -0.7868928909301758, 1.4957493543624878, -1.8182519674301147, 0.20774909853935242, 1.4469465017318726, 0.3462121784687042, 0.2663733661174774, 0.007836965844035149, -0.6866738200187683, 0.6607458591461182, 0.6725188493728638, -0.9698308706283569, -0.57436203956604, 0.09537172317504883, -0.08563664555549622, 1.550307035446167, -0.14158251881599426, -0.043012406677007675, 0.15349261462688446, 0.37087690830230713, 1.4980313777923584, -0.17753534018993378, -0.612155020236969, -0.1357244998216629, -0.24938273429870605, 0.39300575852394104, 1.1476081609725952, -0.7765808701515198, 0.20903517305850983, -1.186423659324646, -0.1612122654914856, 0.3715708255767822, -0.014153581112623215, -0.01202098187059164, -0.61811763048172, -0.3900262117385864, 0.8529335856437683, 0.2897816598415375, 0.9896308183670044, -0.19639889895915985, 0.3620419502258301, 0.40622347593307495, 0.008297347463667393, 0.2058708667755127, 0.13517561554908752, 0.03336293622851372, 0.6945534944534302, -0.8001846671104431, 0.20272259414196014, 0.04952964931726456, 1.1657359600067139, -0.05311514064669609, 0.034184884279966354, -0.5783776640892029, 0.38764405250549316, 1.524413824081421, 0.10629157721996307, 0.6101828813552856, -0.718097984790802, -0.10212636739015579, -0.81329745054245, -0.12661407887935638, -1.1138782501220703, -0.13261950016021729, -0.3576206564903259, -1.2295217514038086, -1.1224883794784546, -0.5961125493049622, 0.37268704175949097, -0.617900550365448, 0.8227628469467163, -0.24967122077941895, 0.42469385266304016, -0.49557897448539734, 0.38629743456840515, 0.08132448047399521, 0.5379348993301392, 0.38997790217399597, -0.12380953878164291, 0.9565156698226929, -1.253219723701477, -1.293046236038208, -1.1331088542938232, 0.6644235253334045, -0.24551765620708466, -0.16282248497009277, -0.23382394015789032, -1.273183822631836, -0.9711565375328064, -0.6203743815422058, 0.0684276670217514, -0.1325642615556717, 0.032101619988679886, 0.729918897151947, 0.00985315814614296, -1.1688032150268555, 0.6269789338111877, -0.25465908646583557, -0.2983904778957367, 0.43904340267181396, 0.45012369751930237, 0.06957580894231796, -0.3491038978099823, -1.1868082284927368, 0.5333011746406555, -0.045401204377412796, -0.47911784052848816, -0.19900575280189514, -0.7216456532478333, -1.117108702659607, -0.028011631220579147, 0.555667519569397, -0.5327233672142029, 1.1320130825042725, -0.3648751676082611, -1.3608603477478027, 0.569617748260498, -0.5868543982505798, -0.018577439710497856, 0.18269509077072144, -0.4898943305015564, -0.23502667248249054, -0.7633702754974365, 0.2370765209197998, 0.37042737007141113, 0.6998637318611145, 0.26874420046806335, -0.2031707912683487, -0.319316029548645, -0.7400180101394653, 0.001358047709800303, -0.30200979113578796, 0.8853392004966736, -0.5468364357948303, -0.3864285349845886, 0.15678183734416962, 0.5211717486381531, 0.09226474165916443, -0.09280502796173096, -0.4924269914627075, -1.0767868757247925, 1.0087542533874512, 0.2127804458141327, 0.9323983788490295, -0.9152223467826843, -0.23870381712913513, -0.4110768735408783, 0.004294104408472776, -0.3092723488807678, -0.8506165146827698, 0.3066818118095398, -0.4966907203197479, 0.11772678792476654, -0.165935218334198, -1.3515989780426025, 0.07004468888044357, -0.045860134065151215, -0.5541511178016663, 0.06457743793725967, -0.10167721658945084, 1.0206048488616943, -0.9561142325401306, -0.17464973032474518, 0.08462922275066376, 0.1862138956785202, -0.908342182636261, 1.61065673828125, -0.032199371606111526, -0.2585769295692444, 0.1492358297109604, -0.5806450843811035, 0.16733841598033905, -0.5113362073898315, 0.5521475076675415, -0.5317041873931885, 0.05834555998444557, 0.3188307583332062, -0.7253121137619019, 1.3217517137527466, -0.24786701798439026, 0.4527501165866852, 0.17794078588485718, -0.7275925874710083, 0.1831708699464798, 0.3458470404148102, -0.13870234787464142, -0.3937824070453644, 0.40349915623664856, 0.21548621356487274, -0.5335736870765686, 0.419808954000473, 0.5075023770332336, 0.7626132369041443, 0.03194303810596466, 0.1829037070274353, 0.6846209168434143, -0.18076622486114502, -0.06712053716182709, 0.5395599007606506, 0.5469374656677246, 0.1906954050064087, 0.7192496657371521, -0.791924774646759, 0.4441347122192383, -1.0688773393630981, -0.13808216154575348, 0.3392965495586395, 0.5904160141944885, 0.9559099674224854, -0.19678597152233124, -0.751060962677002, -0.3036530017852783, 0.2851308584213257, 0.7586429119110107, 1.8467810153961182, -0.07595638185739517, -0.2926492393016815, -0.8461792469024658, -0.08888756483793259, -0.7148564457893372, -0.028283152729272842, -0.6866658926010132, -0.298469215631485, -0.3021564185619354, -0.6358886957168579, 0.5013351440429688, 0.47261059284210205, 1.4211989641189575, -0.25885918736457825, -0.3250241279602051, -0.09583555907011032, 0.13632051646709442, -0.7965224385261536, -1.4168498516082764, 0.4255768954753876, -0.2364380806684494, -0.11538369953632355, -0.21143457293510437, -0.48012998700141907, -0.036792755126953125, -0.2942727506160736, 0.9658181071281433, -0.6014886498451233, 0.093611940741539, 0.2435539811849594, 0.5850279927253723, -0.8097258806228638, -0.49812838435173035, 0.29088345170021057, 0.4264315664768219, 0.3363688588142395, 0.3260000944137573, 0.5648335814476013, 0.3639613091945648, 0.16056284308433533, -0.1266912817955017, 0.3164047598838806, -0.001179122133180499, 0.24317434430122375, 0.42321738600730896, -0.6628455519676208, -0.13748691976070404, -1.1130495071411133, 0.4565819203853607, -0.004331670235842466, -0.2507334351539612, 0.08973153680562973, -0.12970757484436035, -0.22914987802505493, 0.21016722917556763, -0.45759662985801697, -0.27561479806900024, -0.5808405876159668, 0.4037605822086334, -0.6139177083969116, -0.09008108824491501, 0.009242713451385498, -0.04375625401735306, 0.19692355394363403, -0.03817934915423393, 0.5617616772651672, 0.7107406258583069, 0.0870094820857048, 0.5005621910095215, -0.8786014914512634, 0.7574957609176636, 0.5990278720855713, 0.1632835417985916, -0.20892129838466644, -0.18258528411388397, -0.9268859624862671, -0.41246655583381653, -0.6153237819671631, -0.4564291834831238, -0.10102628916501999, 0.44747862219810486, -0.6062735319137573, -0.7122827768325806, 0.4049973785877228, -1.4487004280090332, -0.2040574550628662, 0.36168837547302246, -0.2777177691459656, -0.5380204916000366, -0.8620815277099609, -1.010697364807129, -0.3267894983291626, -0.6962069869041443, -0.8366849422454834, 0.179578959941864, -0.1761232167482376, -0.7457040548324585, -0.6540812849998474, -0.05674925073981285, -0.5850623250007629, 1.2482283115386963, -1.0878586769104004, 0.552924633026123, -0.29389485716819763, -0.4842309057712555, 0.1533471792936325, 0.3370991349220276, 0.1778009831905365, 0.22912637889385223, -0.21564731001853943, -0.6087892651557922, 0.3330515921115875, -0.012716161087155342, 0.1285485327243805, -0.22411486506462097, 0.4625413417816162, 0.6902154088020325, -0.2571882903575897, -0.6051013469696045, 0.2946142554283142, 1.0791172981262207, -0.5429361462593079, 0.3091064393520355, 0.13597998023033142, 0.9990730285644531, 0.24198567867279053, -0.38505855202674866, 0.6841676831245422, 0.6367843747138977, 0.5484410524368286, 0.44423118233680725, 0.014902562834322453, -0.1763172745704651, -0.12412049621343613, 0.5317987203598022, 1.855916142463684, 0.2858876585960388, 0.07917174696922302, -0.930351197719574, 0.9757223725318909, -1.1784439086914062, -0.9478088021278381, 0.6352899670600891, 0.46738457679748535, 0.2029401808977127, -0.946209728717804, -0.17936059832572937, -0.4913124442100525, 0.6107279658317566, 0.345700740814209, -0.35244810581207275, -0.38432058691978455, -0.09422323107719421, 0.6002238392829895, 0.10484002530574799, 0.9222697615623474, -0.28007248044013977, 0.32878485321998596, 14.915289878845215, 0.6652920246124268, 0.003851782064884901, 0.7888516187667847, 0.6020010113716125, 0.18164324760437012, -0.10159946978092194, -0.19021375477313995, -0.9792748093605042, 0.10279659181833267, 1.0123178958892822, 0.0008311081328429282, 0.4921548664569855, 0.10319163650274277, -0.27264663577079773, 0.42885205149650574, -0.682113766670227, 0.9594151377677917, 0.8437745571136475, -1.4372656345367432, 0.5013904571533203, 0.14397457242012024, 0.1529136300086975, 0.42181098461151123, 0.7432548999786377, 0.3769911825656891, 0.5243008732795715, -0.3283710777759552, 0.568539559841156, 0.44426366686820984, 0.8327897191047668, 0.08225441724061966, 0.2842276096343994, 0.6644065380096436, -0.8512140512466431, -0.10081028193235397, -0.4277613162994385, -1.1679167747497559, 0.19454175233840942, 0.023861246183514595, -0.05226178094744682, -0.33291441202163696, 0.007389768026769161, 0.975409984588623, 0.30093929171562195, 0.17737610638141632, 0.09746842086315155, 0.31103357672691345, 0.09636914730072021, 0.13208670914173126, 0.21492831408977509, 0.3414696753025055, 0.2656056880950928, 0.006360095459967852, 0.210942342877388, 0.3166912794113159, 0.15768970549106598, 0.47247758507728577, -0.30354875326156616, -0.057946983724832535, -0.34583577513694763, 0.16766631603240967, -0.060610439628362656, 0.8252288103103638, 0.966733992099762, 0.12999962270259857, -0.5625690817832947, 0.4423587918281555, 0.5807607173919678, -0.27450841665267944, -0.2655528783798218, -0.10719504207372665, 0.45197033882141113, -0.1800943911075592, -0.10220923274755478, 0.5122977495193481, -0.4249907433986664, -0.379381000995636, -1.1680821180343628, -0.6282536387443542, 0.39396727085113525, -0.9731183052062988, -0.8871619701385498, 0.9690801501274109, -0.34058859944343567, -0.43585553765296936, 0.24884486198425293, -0.56818687915802, -0.2847004532814026, 0.5623151659965515, -0.6814588308334351, -0.5510145425796509, 0.04597332328557968, -0.21823903918266296, 0.1256396621465683, -0.2531644403934479, 1.1148192882537842, 0.09003327786922455, -0.3283640742301941, 0.10605762153863907, -0.23857809603214264, -0.15431173145771027, 0.013547573238611221, -0.7100200653076172, 0.7298107147216797, 0.1960461437702179, -0.2180466502904892, 0.39292848110198975, 0.2750498652458191, 0.5060697793960571, -0.8967821002006531, -0.18051870167255402, 0.8207922577857971, -0.8529588580131531, -0.2481563538312912, -0.6444838643074036, -1.2727059125900269, 0.5535076260566711, 1.1402102708816528, -0.1511828899383545, 0.34987252950668335, -0.04759352654218674, -0.26827773451805115, -0.06655475497245789, -0.3933756649494171, 0.14706288278102875, 0.7254883646965027, -0.8453365564346313, -0.6303108930587769, -0.035143669694662094, 0.08028333634138107, -0.9437688589096069, -0.29467105865478516, -0.42364296317100525, -0.05186229944229126, 0.05905307084321976, 0.8084978461265564, -0.35782578587532043, 0.6829147934913635, 0.8901093006134033, -0.17619487643241882, -0.8692741990089417, -0.6298348903656006, -1.0615180730819702, -0.025158509612083435, 0.18439170718193054, 0.16879883408546448, -0.2951860725879669, 0.11181043088436127, 0.8639244437217712, 0.32805928587913513, -0.47725430130958557, -0.7473856806755066, -0.2802111804485321, -0.08000512421131134, -0.3945751488208771, 0.24255762994289398, -0.005284206476062536, 0.057471759617328644, 0.5419005751609802, 0.030851028859615326, 0.3297053873538971, 0.3859558403491974, -0.9761335253715515, 0.5163519978523254, -0.2810477614402771, 0.08169364929199219, -0.46506327390670776, -0.5099406838417053, -1.6633594036102295, 0.3138121962547302, -1.1614421606063843, 0.10995720326900482, -0.6670834422111511, 0.07399441301822662, 0.4047105610370636, -0.01780957169830799, 0.11660486459732056, -0.053994107991456985, -0.3186141550540924, -0.026848552748560905, -0.5374740958213806, -0.667178213596344, 0.9357237219810486, 0.6193556189537048, -0.8689002990722656, 0.3063340485095978, -0.5851130485534668, -0.08875899761915207, 0.27892667055130005, 0.3969983458518982, -0.4176490902900696, -0.7370451092720032, -1.232588529586792, -0.08684197813272476, -0.03899991139769554, 0.1268177032470703, -0.8639301657676697, 1.1751054525375366, 0.38602378964424133, -0.2623191773891449, -0.21433383226394653, 0.30320942401885986, -0.9761385321617126, -0.14866498112678528, 0.3726586401462555, -0.8553552627563477, 0.35008230805397034, 0.08481234312057495, -0.6143665313720703, -0.3017705976963043, 1.0570776462554932, 0.27265289425849915, -1.1686112880706787, -0.8209432363510132, 0.674555778503418, -0.764901340007782, 0.18930132687091827, -0.054522059857845306, -0.09564494341611862, -0.8471778631210327, -0.5946308970451355, 0.07479971647262573, 0.4568771719932556, -0.4335232377052307, 0.90179443359375, 0.39978834986686707, -1.3558058738708496, -0.059902142733335495, 0.7716112732887268, -0.15980148315429688, -0.07623722404241562, 0.7945517897605896, 0.5429959893226624, 0.34734398126602173, 0.44938111305236816, 0.015564766712486744, 0.02501492388546467, -1.0965359210968018, 0.16160576045513153, 1.0542317628860474, -0.46417492628097534, -0.08113738149404526, 1.0083725452423096, 0.07498712092638016, -0.8966487646102905, -0.11682570725679398, -0.5706560015678406, -0.4720391035079956, -0.3312114477157593, 0.9971756935119629, 0.428130567073822, -0.19815531373023987, -0.5515727400779724, -0.8075550198554993, 0.4698580205440521, -0.4277545213699341, -0.29695597290992737, 0.4791017174720764, -0.37973928451538086, -0.6668326258659363, 0.6005942821502686, 0.9470977783203125, -0.722655177116394, -0.6561452150344849, -0.9192596077919006, -0.4832022190093994, -0.07835684716701508, 0.22820709645748138, 0.03384857624769211, -0.6534437537193298, 0.8040854930877686, 0.5899534225463867, 0.14487439393997192, 0.03379509225487709, -0.0619257427752018, 0.2595565915107727, 0.2676696479320526, 0.13828681409358978, -0.4410698711872101, -0.4390861988067627, 1.5895259380340576, 1.5940606594085693, -0.3512735962867737, -0.08256395161151886, -0.4332907199859619, -0.7829626798629761, 0.6805374026298523, 0.30367809534072876, -0.17987549304962158, 0.9397594928741455, -0.041687849909067154, 0.13543564081192017, 0.1636432260274887, -1.1701316833496094, -0.23531866073608398, 0.9705662131309509, 1.2829761505126953, 0.6240844130516052, 0.09568111598491669, 0.27548491954803467, 0.8472980856895447, 0.35900044441223145, 0.1662103235721588, 0.629813551902771, 0.2125038057565689, -0.2362007200717926, 0.07574095577001572, -0.0669107437133789, 0.6601176261901855, -0.47535252571105957, -0.5756489634513855, -0.03454052284359932, 0.28934672474861145, -0.11842618137598038, 0.4846484661102295, 0.9406388998031616, -0.03893350809812546, 0.433982789516449, 0.36284714937210083, 0.19711270928382874, -0.6133712530136108, 0.0306247528642416, -0.39900386333465576, -1.0655051469802856, -0.36096838116645813, -0.4724826216697693, -0.6788942813873291, -0.1886252909898758, -0.11743218451738358, 0.13407056033611298, -0.17019358277320862, -0.056027404963970184, 1.3097596168518066, 0.8331906795501709, 0.4830535352230072, -0.16215135157108307, -0.5853825807571411, -0.5791406035423279, -1.0377259254455566, -0.1812037229537964, -0.30341026186943054, 0.021648911759257317, -0.41726812720298767, -0.20282170176506042, -0.02621653862297535]}, "authors": [{"authorId": "150322732", "name": "T. Nguyen"}, {"authorId": "2116488139", "name": "Tam Nguyen"}, {"authorId": "2216306719", "name": "Hai Do"}, {"authorId": "145546032", "name": "Khai Nguyen"}, {"authorId": "3147132", "name": "Vishwanath Saragadam"}, {"authorId": "2056567815", "name": "Minh Pham"}, {"authorId": "2170125220", "name": "Duy Khuong Nguyen"}, {"authorId": "3526349", "name": "Nhat Ho"}, {"authorId": "103583159", "name": "S. Osher"}], "references": [{"paperId": "260b9388c90b497218b591a9a0e2742b7e0951e5", "title": "Momentum Transformer: Closing the Performance Gap Between Self-attention and Its Linearization"}, {"paperId": "277dd3d3d16c293d4ecc1e495dbd7f40e6706696", "title": "An Exponentially Increasing Step-size for Parameter Estimation in Statistical Models"}, {"paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a", "title": "Flowformer: Linearizing Transformers with Conservation Flows"}, {"paperId": "48af9b314181b04edcc0b7224ffe4689036b755f", "title": "Improving Transformers with Probabilistic Attention Keys"}, {"paperId": "37abe53ed31caa23ae833b2e67bb4aa1892e8d25", "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention"}, {"paperId": "7d67b5cfd19927b30f07de25145f20bf95766e3c", "title": "Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "372cce47fa16c538946972e6a7ac8420e64000b0", "title": "Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "2051548f7681c96d603de932ee23406c525276f9", "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning"}, {"paperId": "9f0272bb258506fdc0ee7d8951593914d4f9c39d", "title": "Analyzing Individual Neurons in Pre-trained Language Models"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "61325245e98920a69b40e18c069fda0c1cf00f21", "title": "MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "b70252f0de1c7832fdd6c862af35f6667349fe55", "title": "Transformer VAE: A Hierarchical Model for Structure-Aware and Interpretable Music Representation Learning"}, {"paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4", "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "3b504f939e55d567652737ef093c1087cd40689b", "title": "Analyzing Redundancy in Pretrained Transformer Models"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "42b0d32a7e644b657e34ce056c84d215d9f62187", "title": "Universal"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "199ff73d2f728e997f860b62a2322823d3e3d9e8", "title": "Designing and Interpreting Probes with Control Tasks"}, {"paperId": "9d7902e834d5d1d35179962c7a5b9d16623b0d39", "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "84898960f68fa78296a102edc8ac81739f9a9408", "title": "Gaussian Transformer: A Lightweight Approach for Natural Language Inference"}, {"paperId": "830995ef17cc291c13f42dfd9f462137de1d2179", "title": "Augmenting Self-attention with Persistent Memory"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "a9d5771c96f9671e556c2bcbbd1c340b4495e00e", "title": "Sharp Analysis of Expectation-Maximization for Weakly Identifiable Models"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "d8abb8206b913d185b4bd406880131c13759a6ff", "title": "The UEA multivariate time series classification archive, 2018"}, {"paperId": "5e26191eb389546a67d3c80c360b8f5bfffe464c", "title": "Singularity, misspecification and the convergence rate of EM"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "a2d407962bb1f5fcd209114f5687d4c11bf9dfad", "title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations"}, {"paperId": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "2144f5c7e3a9ed9f20138be5fb02503390c19fc5", "title": "Results of the WMT14 Metrics Shared Task"}, {"paperId": "e2b3ce2e0d66238fba443abece60641f98036114", "title": "Approximation of probability distributions by convex mixtures of Gaussian measures"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "66ce7e4c2d44343513aac4088571e12337dadc0c", "title": "Inference of population structure using multilocus genotype data: linked loci and correlated allele frequencies."}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "52413b3078a2c877ec588e39726f539038eee1c5", "title": "Entropies and rates of convergence for maximum likelihood and Bayes estimation for mixtures of normal densities"}, {"paperId": "fa02f9123abacd5ba13d41e937d99c077da8d3f6", "title": "Inference of population structure using multilocus genotype data."}, {"paperId": "955f90930d48750e7239478b4eed440eb84131cd", "title": "FourierFormer: Transformer Meets Generalized Fourier Integral Theorem"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f", "title": "AUTO-ENCODING VARIATIONAL BAYES"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "439 Set transformer: A framework for attention-based permutation-invariant neural networks"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "81aace0e90c6a962059b117c24db0d856f340f41", "title": "Report on the 11th IWSLT evaluation campaign"}, {"paperId": "4574d77fff19e093782178595a8988a7f3aa1969", "title": "Latent Dirichlet Allocation"}, {"paperId": null, "title": "Understanding back-translation at scale"}, {"paperId": null, "title": "Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content"}, {"paperId": null, "title": "Did you discuss any potential negative societal impacts of your work?"}, {"paperId": null, "title": "you used crowdsourcing or conducted research with human subjects"}, {"paperId": null, "title": "Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?"}, {"paperId": null, "title": "Did you report error bars (e.g., with respect to the random seed after running experiments multiple times"}, {"paperId": null, "title": "Have you read the ethics review guidelines and ensured that your paper conforms to them"}, {"paperId": null, "title": "Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}, {"paperId": null, "title": "(a) Did you state the full set of assumptions of all theoretical results?"}, {"paperId": null, "title": "If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots"}, {"paperId": null, "title": "code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators?"}, {"paperId": null, "title": "c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}]}