{"paperId": "c49c292e1fb1d215c88828a52134b7ccfa52be44", "title": "Sparse Attention with Learning to Hash", "abstract": "Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 16, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively is proposed, which is applicable to the wide range of pre-trained Transformer models."}, "embedding": {"model": "specter_v2", "vector": [0.09990808367729187, 0.4552687406539917, -0.5358678102493286, -0.04128988832235336, -0.6483429670333862, -0.18217229843139648, 0.442950963973999, 0.1618926078081131, -0.44202929735183716, -0.09871828556060791, 0.6623786091804504, 0.5535187125205994, 0.2945936620235443, -0.07966247946023941, -0.37233608961105347, 0.0487421490252018, -0.6707661151885986, 0.8920146822929382, 0.4049123525619507, -0.3622185289859772, 0.04276777431368828, -1.0462480783462524, -1.3423120975494385, 0.2746295928955078, 0.3237265348434448, 0.6345207095146179, 0.2060704082250595, 0.5966002345085144, -0.6074975728988647, 0.19157764315605164, 0.6411136388778687, -0.4186268746852875, 0.34227249026298523, 0.046831440180540085, -0.39528024196624756, -0.5662050247192383, 0.27031436562538147, -0.3513867259025574, -0.8299102783203125, 0.6179620027542114, 0.04194093868136406, 0.3103068768978119, 0.5880558490753174, -0.739058792591095, -0.40533605217933655, 0.7990944981575012, 0.5971603393554688, 0.7363776564598083, -0.019001483917236328, -0.8800936341285706, 1.7233936786651611, -1.2975555658340454, 0.027864066883921623, 1.3240008354187012, 0.4012143313884735, 0.35773584246635437, -0.13674497604370117, -0.6789577603340149, 0.20542572438716888, 0.6600169539451599, -0.9563825130462646, -0.2510477900505066, -0.39481741189956665, 0.043631285429000854, 1.8087106943130493, -0.22281190752983093, 0.06523440778255463, 0.20950117707252502, -0.2881845533847809, 1.441681146621704, -0.3539196252822876, -0.6232742071151733, -0.2944953143596649, -0.15198345482349396, 0.2492140531539917, 0.9178857803344727, -0.5222524404525757, 0.37587985396385193, -1.0253663063049316, -0.4835333824157715, 0.11475647985935211, 0.25582265853881836, 0.36669108271598816, -0.6297405958175659, -0.10790460556745529, 1.0743544101715088, 0.3491169214248657, 0.9265626668930054, -0.020528797060251236, 0.8860834240913391, 0.6647512316703796, 0.1179596409201622, -0.33474403619766235, 0.6231467723846436, 0.17714379727840424, -0.05643606185913086, -0.8960452079772949, 0.3721767067909241, 0.06754942983388901, 1.052241325378418, 0.16694453358650208, 0.08147923648357391, -0.68864905834198, 0.24464051425457, 1.0609872341156006, 0.21948082745075226, 0.8438884019851685, -0.5163295269012451, 0.03618287667632103, -0.6883857846260071, -0.08273503929376602, -0.7677692770957947, 0.21236178278923035, 0.1128268614411354, -0.7656044960021973, -1.451625943183899, -0.4031628966331482, 0.7644842863082886, -0.5888563394546509, 0.6965703964233398, -0.4350533187389374, 0.3676171898841858, -0.3184159994125366, 0.18497613072395325, 0.5960832834243774, 0.6931818127632141, 0.09378210455179214, 0.24450336396694183, 1.0157369375228882, -0.7958707809448242, -0.5694259405136108, -0.9665570855140686, 1.1645855903625488, -0.2763223350048065, 0.4050011932849884, -0.305495023727417, -1.0660747289657593, -1.0232744216918945, -0.6253337860107422, -0.14284971356391907, -0.7972877025604248, -0.4444764256477356, 0.7327908873558044, 0.314045250415802, -0.8394598960876465, 0.44521698355674744, -0.30094295740127563, -0.3526729941368103, 0.42723822593688965, 0.24383506178855896, 0.09602341055870056, -0.6506780982017517, -1.9765522480010986, 0.4261816442012787, 0.42458611726760864, -1.0045979022979736, -0.3978610932826996, -0.6403517127037048, -1.515968918800354, 0.26283660531044006, 0.4196023941040039, -0.1789914220571518, 0.8934645056724548, 0.3496149182319641, -0.8800514936447144, 0.7842981219291687, -0.5869492292404175, -0.3162710666656494, 0.16699686646461487, -0.4226135015487671, 0.02362620271742344, -0.8260230422019958, -0.03698692470788956, 0.3004485070705414, 0.45485860109329224, -0.17679783701896667, 0.03135998174548149, 0.1947818100452423, -0.43974390625953674, 0.01461765542626381, -0.6289401650428772, 0.6510166525840759, -0.43013933300971985, -0.4614681601524353, 0.030420590192079544, 0.5459731221199036, 0.13451021909713745, -0.334667831659317, -0.16219483315944672, -1.1029835939407349, 1.1792811155319214, 0.2851100265979767, 1.151160478591919, -0.9219049215316772, -0.5917943716049194, -0.006699786055833101, 0.015831056982278824, -0.06166500598192215, -0.7848276495933533, 1.1810606718063354, -0.5348742008209229, 0.11731930077075958, -0.3612948954105377, -0.8250544667243958, 0.2724955976009369, 0.003823766252025962, -0.871677041053772, -0.09531897306442261, -0.39925652742385864, 1.2758041620254517, -1.134299874305725, 0.006350117735564709, 0.0055487193167209625, 0.0910726860165596, -1.1468603610992432, 1.4963477849960327, -0.4760086238384247, -0.5340031385421753, -0.0798315778374672, -0.08360131084918976, -0.09012367576360703, -0.30677926540374756, 0.12080037593841553, -0.4197821021080017, 0.17361722886562347, 0.39790263772010803, -0.155380517244339, 1.1649893522262573, -0.3444652259349823, 0.1880144327878952, -0.49685096740722656, -1.028387427330017, 0.2955739498138428, 0.49065372347831726, 0.17446529865264893, -0.5411872267723083, -0.015991460531949997, 0.24618341028690338, -0.6886621713638306, -0.07904639840126038, 0.6322610974311829, 0.540067732334137, -0.5236632823944092, 0.1890900582075119, 0.4647814929485321, -0.49269646406173706, 0.3592902719974518, 0.6632603406906128, 0.8571077585220337, 0.0011318012839183211, 0.8627756237983704, 0.01338103599846363, 0.2544914186000824, -1.053444504737854, -0.3179665505886078, 0.9252317547798157, 1.0117098093032837, 0.7743069529533386, 0.028292741626501083, -0.8858956694602966, -0.8449708819389343, 0.3934064209461212, 0.7682124972343445, 1.8231875896453857, 0.023936254903674126, -0.6639052629470825, -0.5760000944137573, -0.4048461318016052, -0.030404329299926758, 0.0045157624408602715, -0.8021642565727234, -0.5950630903244019, -0.31085553765296936, -0.9639943242073059, 0.8220144510269165, 0.47226211428642273, 0.8268780708312988, -0.4697084128856659, -0.2682340145111084, -0.10494469106197357, 0.07836109399795532, -0.5585215091705322, -0.92884761095047, 0.5068885684013367, -0.08438649773597717, 0.034469109028577805, 0.03088996931910515, -0.10465668141841888, -0.16980108618736267, -0.37730005383491516, 0.7598541378974915, -0.37530431151390076, -0.23528973758220673, 0.13539081811904907, 0.28131771087646484, -0.7458344101905823, -0.40557950735092163, 0.6964574456214905, 0.17004674673080444, -0.14169806241989136, 0.6279398202896118, 0.3031582534313202, 0.16131296753883362, 0.36524200439453125, -0.7151422500610352, -0.447100430727005, 0.093720942735672, 0.47320714592933655, 0.610301673412323, -0.24734623730182648, -0.07365242391824722, -1.1439621448516846, 0.3980969786643982, -0.1807379126548767, -0.3060791492462158, 0.34296661615371704, -0.5475496649742126, -0.25824540853500366, 0.5230839848518372, -0.7724885940551758, 0.28114554286003113, -0.574775218963623, 0.20234939455986023, -0.5774638652801514, -0.003617845242843032, 0.4659680128097534, 0.05241040885448456, 0.007916057482361794, 0.2598017156124115, 0.9520927667617798, 0.504456102848053, 0.02530646324157715, 0.5539252758026123, -0.9388986229896545, 0.543218195438385, -0.07887043058872223, 0.48634472489356995, 0.06726612150669098, -0.21662327647209167, -0.6071508526802063, -0.7473401427268982, -0.7912054061889648, -0.20935408771038055, 0.18235699832439423, -0.025067711248993874, -0.5001426339149475, -0.9106574654579163, -0.20032146573066711, -0.9588529467582703, -0.15928210318088531, -0.06695036590099335, -0.3979519307613373, -0.717496931552887, -0.711983859539032, -0.9782732129096985, -0.6607247591018677, -0.42712754011154175, -0.9971026182174683, 0.48800840973854065, -0.26077377796173096, -0.535973846912384, -0.41211822628974915, 0.11770813912153244, -0.2608802914619446, 1.13418710231781, -0.6790285110473633, 0.7911249995231628, -0.45444008708000183, -0.44042786955833435, -0.45476233959198, 0.13858206570148468, 0.19427375495433807, 0.09721001237630844, -0.2860456705093384, -0.8012347221374512, -0.16346333920955658, -0.14618726074695587, -0.6788421273231506, 0.2830792963504791, 0.16798359155654907, 1.1333354711532593, -0.3993554711341858, -0.6981527209281921, 0.33902719616889954, 1.4934868812561035, -0.2181881219148636, 0.43718042969703674, 0.24005696177482605, 1.1989564895629883, 0.045842818915843964, 0.0839669480919838, 0.8097458481788635, 0.42024555802345276, 0.7202897071838379, 0.2956375777721405, 0.1205626130104065, 0.07631789892911911, -0.4696211516857147, 0.5016600489616394, 1.4703818559646606, 0.44128483533859253, -0.0216950885951519, -1.1190526485443115, 0.8375964760780334, -1.4113152027130127, -1.3382656574249268, 1.118356466293335, 0.6787132024765015, 0.6002162098884583, -0.744104266166687, 0.1262594759464264, -0.3690073788166046, 0.35655683279037476, 0.32686689496040344, -0.4184702932834625, -0.2583717703819275, -0.1454300433397293, 0.6067466735839844, 0.2496730387210846, 0.5621272325515747, -0.2626567482948303, 0.5721290111541748, 14.7531099319458, 1.3542685508728027, 0.003434621263295412, 0.6205193400382996, 0.5651830434799194, 0.04452703148126602, -0.19617637991905212, -0.04971693083643913, -1.2239586114883423, 0.060236722230911255, 1.0500560998916626, -0.18707673251628876, 0.5378633737564087, 0.29802244901657104, -0.4894370436668396, 0.42486003041267395, -0.652374804019928, 0.8201426267623901, 0.845731258392334, -1.1889722347259521, 0.6360830068588257, 0.37342917919158936, 0.08077821880578995, 0.20260611176490784, 0.9617977738380432, 0.7259519100189209, 0.5526964664459229, -0.6753838658332825, 0.48810043931007385, 0.4805944561958313, 1.096092939376831, -0.07587985694408417, 0.5773987174034119, 0.2722592353820801, -1.0545423030853271, -0.5074871778488159, -0.6659173369407654, -0.8243882060050964, 0.4011293351650238, -0.0551987923681736, -0.7694647908210754, -0.5655720233917236, -0.10893429070711136, 0.7992585301399231, 0.3101905882358551, 0.6489113569259644, 0.28625407814979553, 0.256582111120224, 0.014156108722090721, 0.07310949265956879, 0.39892733097076416, 0.7881348133087158, -0.09123077243566513, 0.2258811891078949, 0.29035019874572754, 0.09527730196714401, 0.21711358428001404, 0.006242889445275068, -0.3408164381980896, 0.09641768038272858, -0.7763786315917969, -0.12949569523334503, 0.20684099197387695, 0.482166051864624, 0.8753859996795654, 0.12213753908872604, -0.6095709800720215, 0.09077838808298111, 0.6013005375862122, -0.2894201874732971, -0.40755495429039, -0.2656383216381073, 0.20896315574645996, 0.08506216108798981, 0.19645263254642487, 0.7818437218666077, -0.10852479189634323, -0.3346248269081116, -1.0302647352218628, -0.6328843235969543, 0.961503267288208, -0.5722483992576599, -1.07561457157135, 0.8025504350662231, -0.09089171141386032, -0.26899805665016174, 0.07266849279403687, -0.18261148035526276, 0.09628373384475708, 0.11088081449270248, -1.1039789915084839, -0.4800010323524475, 0.21073712408542633, -0.24595047533512115, -0.5192367434501648, -0.21833626925945282, 1.4058737754821777, 0.2082791030406952, 0.16649162769317627, 0.3983513414859772, 0.2540932595729828, 0.0385395884513855, 0.12296153604984283, -0.8980088233947754, 0.8105049133300781, -0.22731861472129822, -0.09675970673561096, 0.5395804047584534, 0.18727977573871613, 0.053863126784563065, -0.4942772686481476, -0.1839791238307953, 0.7323963642120361, -1.0451250076293945, -0.39652904868125916, -1.1190681457519531, -1.30616295337677, 0.2987738847732544, 0.5328978300094604, -0.1477804034948349, 0.5617541074752808, -0.027314959093928337, -1.0050829648971558, -0.35473430156707764, -0.8263424038887024, -0.11377252638339996, 0.49573010206222534, -1.202521800994873, -0.5118075013160706, -0.05142855644226074, 0.3344135880470276, -0.9447286128997803, -0.565141499042511, -0.03815556317567825, 0.18596313893795013, 0.02959265001118183, 1.2413418292999268, -0.5124256014823914, 0.6270995140075684, 0.817513108253479, -0.1462589055299759, -0.44691377878189087, -0.34663599729537964, -0.9664192199707031, -0.35355672240257263, -0.013829016126692295, 0.41033700108528137, -0.2719166576862335, 0.7074702382087708, 0.7022887468338013, 0.5266254544258118, -0.4430895745754242, -0.6444881558418274, -0.47007429599761963, -0.2016172856092453, -0.17398717999458313, 0.6005083322525024, 0.2830677032470703, 0.13703450560569763, 0.029938628897070885, 0.040146712213754654, 0.36620184779167175, -0.15616491436958313, -0.7762895822525024, 0.38382336497306824, -0.049379851669073105, 0.023460296913981438, -0.3571978211402893, -0.5456627607345581, -1.2891203165054321, -0.04607084020972252, -1.1622508764266968, 0.09077338129281998, -0.6342820525169373, -0.036753516644239426, 0.1942691206932068, -0.26576972007751465, 0.30340468883514404, 0.34432026743888855, -0.2713974118232727, -0.5258179903030396, -0.6192642450332642, -0.6260932087898254, 1.0056474208831787, 0.45827996730804443, -0.7130699157714844, 0.41358882188796997, -0.2629663944244385, -0.16706441342830658, -0.20977285504341125, 0.41303420066833496, -0.7760046720504761, -0.6999733448028564, -0.8937591314315796, 0.14591920375823975, 0.03551844134926796, -0.18952800333499908, -0.5518130660057068, 0.9900297522544861, 0.345720112323761, -0.4572747051715851, -0.09167986363172531, 0.6887646317481995, -1.0402671098709106, -0.2971016764640808, 0.5715012550354004, -1.1053900718688965, 0.3724992573261261, -0.011897487565875053, -0.6602804660797119, -0.6119289398193359, 0.5577385425567627, 0.2526817321777344, -1.3912570476531982, -0.1982618123292923, 0.8044594526290894, -0.4568023085594177, 0.49983730912208557, -0.24198086559772491, -0.14290973544120789, -0.9991065859794617, -0.8705142140388489, 0.024967031553387642, 0.45769190788269043, -0.2220272570848465, 0.8290992975234985, 0.5167654156684875, -1.454573631286621, 0.15536142885684967, 0.3999805152416229, 0.534315824508667, 0.0704527273774147, 0.7082917094230652, 0.21650168299674988, 0.07798194885253906, 0.6835269927978516, 0.3369874060153961, 0.5244349837303162, -1.0775446891784668, 0.6317691802978516, 0.6891441345214844, -0.43088498711586, 0.047532107681035995, 1.033250331878662, 0.08038254827260971, -0.8873766660690308, 0.1490911841392517, -0.8232198357582092, -0.41968923807144165, -0.09863913059234619, 0.5912818312644958, 0.11917522549629211, -0.18400247395038605, -0.19924236834049225, -0.5805432796478271, 0.18819600343704224, -0.5442076921463013, -0.24789530038833618, 0.8051827549934387, 0.0795382484793663, -0.7575885653495789, 0.606669545173645, 1.1988521814346313, -0.7607858777046204, -0.3347206115722656, -1.0599610805511475, -0.17125238478183746, -0.5454418659210205, -0.1927553117275238, 0.02064613439142704, -0.456892728805542, 0.5239871740341187, 0.3211683928966522, 0.6573740243911743, -0.3394562900066376, 0.0573161244392395, 0.5806486010551453, 0.8665322661399841, -0.13255824148654938, -0.3875267505645752, -0.034756701439619064, 1.687477469444275, 1.47064208984375, -0.5020847916603088, -0.17407968640327454, -0.08042086660861969, -0.5587499141693115, 0.6402405500411987, 0.5784320831298828, 0.07787222415208817, 0.9225625395774841, 0.0060787564143538475, -0.11357022821903229, -0.08290676772594452, -1.0973519086837769, -0.4197542369365692, 0.8821829557418823, 1.275081753730774, 0.5702090859413147, -0.10269404202699661, 0.41946983337402344, 0.41903942823410034, 0.17071978747844696, 0.060897812247276306, 0.44368162751197815, 0.368564635515213, -0.37767839431762695, -0.1844734400510788, 0.08635679632425308, 0.7927919626235962, -0.6561781167984009, -0.85858154296875, 0.25980570912361145, 0.38667550683021545, 0.03444185480475426, -0.009352064691483974, 0.7036207914352417, -0.07264695316553116, 0.4556180536746979, 0.37441977858543396, 0.08267141133546829, -0.8374056220054626, -0.014348921366035938, -0.6691290736198425, -1.0163222551345825, -0.06378430128097534, -0.22322328388690948, -0.8665164709091187, -0.3013458847999573, -0.004249663092195988, 0.2769351303577423, 0.3437356650829315, -0.29463085532188416, 1.0163326263427734, 0.5452187657356262, -0.17233826220035553, -0.3111811578273773, -0.181490957736969, -0.3549104928970337, -0.8045446276664734, -0.12416251003742218, -0.2339535504579544, -0.05998727306723595, -0.4596083462238312, 0.005052573513239622, -0.3919176459312439]}, "authors": [{"authorId": "48064856", "name": "Zhiqing Sun"}, {"authorId": "35729970", "name": "Yiming Yang"}, {"authorId": "122535709", "name": "Shinjae Yoo"}], "references": [{"paperId": "d291149a75d7ac194382bd61e515eb40ed0aa106", "title": "Predicting Attention Sparsity in Transformers"}, {"paperId": "fc46ccb83dc121c33de7ab6bdedab7d970780b2f", "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting"}, {"paperId": "e32a12b14e212506115cc6804667b3d8297917e1", "title": "Poolingformer: Long Document Modeling with Pooling Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "35a9749df07a2ab97c51af4d260b095b00da7676", "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "c0f709acf38eb27702b0fbce1215db0ebaa2de2b", "title": "SMYRF: Efficient Attention using Asymmetric Clustering"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "0cd82dfae930ac4b57c0e959f744f2d10bf87649", "title": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "712cd873d7370db280f4ceaaf000dc49f76b59fe", "title": "On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "520ddb38b59b8fae2209ddc7c6640462cf153eec", "title": "Sparse and Constrained Attention for Neural Machine Translation"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "2d024cda9043514b9335bd1fcdc288b74983220e", "title": "A Survey on Learning to Hash"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "e3b958f93eb28d8cb6780480501a414c11361206", "title": "SLURM: Simple Linux Utility for Resource Management"}, {"paperId": null, "title": "Flax: A neural network library and ecosystem for jax. Version 0.3 , 3"}, {"paperId": null, "title": "A COMPLETE DERIVATION OF THE LHA OBJECTIVE Let Q,K, V \u2208 RN\u00d7dh denote the query, key, and value vectors of the attention mechanism, where N is the sequence length and dh is the hidden size"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Jax: composable transformations of python+ numpy programs"}, {"paperId": null, "title": "The dropout ratio is set to 0.2"}, {"paperId": "3bf2e6941dbb87ac0d2c771c159e1e27366a26e3", "title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "ab207d80d97092f12103a568238770c6627cdb9a", "title": "transformations on a"}]}