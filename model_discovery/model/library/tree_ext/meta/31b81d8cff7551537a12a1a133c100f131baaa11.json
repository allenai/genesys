{"paperId": "31b81d8cff7551537a12a1a133c100f131baaa11", "title": "Understanding Int4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases", "abstract": "Improving the deployment efficiency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be effective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this study, we explore the feasibility of employing INT4 weight and activation (W4A4) quantization for language models. Our findings indicate that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. To materialize the performance gain using W4A4, we develop a highly optimized end-to-end W4A4 encoder inference pipeline supporting different quantization strategies. Our INT4 pipeline is $8.5\\times$ faster for latency-oriented scenarios and up to $3\\times$ for throughput-oriented scenarios compared to the inference of FP16, and improves the SOTA BERT INT8 performance from FasterTransformer by up to $1.7\\times$. We provide insights into the failure cases when applying W4A4 to decoder-only models, and further explore the compatibility of INT4 quantization with other compression methods, like pruning and layer reduction.", "venue": "International Conference on Machine Learning", "year": 2023, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This study explores the feasibility of employing INT4 weight and activation (W4A4) quantization for language models, and provides insights into the failure cases when applying W 4A4 to decoder-only models, as well as exploring the compatibility of INT4 quantization with other compression methods, like pruning and layer reduction."}, "embedding": {"model": "specter_v2", "vector": [0.42305004596710205, 0.3626321256160736, -0.8318712115287781, -0.03592390939593315, -0.28262606263160706, 0.1640627533197403, 0.24419161677360535, 0.07551909238100052, -0.4813300669193268, -0.6748756766319275, 0.5856272578239441, -0.3483036458492279, 0.47357404232025146, 0.01263299398124218, -0.047833845019340515, 0.6876217722892761, -0.8205132484436035, 0.1484604775905609, 0.08700811117887497, 0.0631360411643982, -0.10031959414482117, -0.4660448729991913, -1.1497291326522827, 0.4765739440917969, 0.12492866069078445, 1.2002607583999634, -0.29349419474601746, 0.7652732133865356, -0.6999098062515259, 0.40634313225746155, 0.4367467761039734, -0.49875617027282715, 0.14799398183822632, 0.3184174597263336, -0.11534637212753296, -0.19256509840488434, 0.09545198082923889, -0.6819981932640076, -0.39156660437583923, 0.922304630279541, -0.41344913840293884, -0.06270503252744675, 0.19245736300945282, -0.5513494610786438, -0.33851438760757446, 1.0805575847625732, 0.3064943552017212, 0.645046055316925, -0.518612265586853, -0.6463590264320374, 1.3246855735778809, -1.5563712120056152, -0.17465467751026154, 1.6139062643051147, 0.3959549069404602, 0.14932063221931458, -0.4937320351600647, -0.8802748918533325, 0.21556925773620605, -0.08055514842271805, -1.2531455755233765, -0.8185865879058838, -0.419377863407135, 0.0487549789249897, 1.599062204360962, -0.15760140120983124, 0.08809451758861542, 0.2111644595861435, 0.10040930658578873, 1.1732250452041626, -0.032786089926958084, -0.7286473512649536, 0.17346933484077454, -0.31820929050445557, 0.5460368990898132, 0.8781894445419312, -0.05681794881820679, -0.01584702730178833, -1.0582305192947388, -0.28720250725746155, 0.3466240167617798, -0.16435123980045319, 0.5686135292053223, -0.06575498729944229, -0.23934456706047058, 0.4009111225605011, 0.11676043272018433, 0.5643059015274048, 0.014372481033205986, 1.3801039457321167, 0.9087245464324951, 0.20209214091300964, 0.3069978356361389, -0.044121354818344116, -0.057871013879776, 0.1842919886112213, -1.2961047887802124, 0.028925379738211632, -0.4174201488494873, 0.9117082357406616, -0.18777312338352203, 0.5469707250595093, -0.8880046606063843, -0.0017614501994103193, 1.345969319343567, 0.4697738587856293, 0.13275352120399475, -0.941419780254364, 0.3165929317474365, -0.6655604243278503, -0.012781734578311443, -0.19171124696731567, 0.15260371565818787, -0.29087477922439575, -0.7949740290641785, -1.2941690683364868, -0.9006181359291077, 0.25451669096946716, -1.225926160812378, 0.5424560308456421, -0.7161245942115784, 0.2224571853876114, -0.18735279142856598, -0.12058918178081512, 0.3371202051639557, 0.3926165997982025, 0.4096730649471283, 0.05393441766500473, 1.2163913249969482, -0.7023256421089172, -0.6806653141975403, -1.094232439994812, 0.8832713961601257, -0.4505985379219055, 0.036576926708221436, -0.24054555594921112, -1.5920321941375732, -1.0575767755508423, -0.779366135597229, -0.1584034264087677, -0.17558076977729797, 0.16861531138420105, 0.682023286819458, 0.2911413609981537, -1.5518815517425537, 0.4430311918258667, -0.4885143041610718, 0.08302374929189682, 0.4216498136520386, 0.44555312395095825, 0.4809429943561554, 0.027329513803124428, -1.0787479877471924, 0.1425510048866272, 0.23759624361991882, -0.7603384256362915, -0.1100444346666336, -0.7919914722442627, -1.2229630947113037, 0.21770663559436798, -0.011843830347061157, -0.48983243107795715, 1.5075634717941284, 0.06714099645614624, -1.2717117071151733, 0.36064741015434265, -0.7619863748550415, -0.07702570408582687, -0.2226107120513916, -0.22387981414794922, -0.7175180315971375, -0.3406311571598053, -0.25182801485061646, 0.6150144338607788, 0.557952344417572, 0.05344388633966446, -0.23508115112781525, 0.22856904566287994, -0.28237271308898926, -0.1973208338022232, -0.21614395081996918, 1.3571782112121582, -0.6321964263916016, -0.3413598835468292, 0.4194668233394623, 0.7485128045082092, -0.2355959713459015, 0.23010651767253876, -0.5019401907920837, -0.7560221552848816, 0.8902235627174377, 0.006241281982511282, 1.3469367027282715, -1.2001581192016602, -0.9181718826293945, 0.2977786958217621, 0.06525109708309174, 0.012367213144898415, -0.4018194377422333, 0.3842003047466278, -0.3799208998680115, 0.895577609539032, 0.05345892533659935, -1.0887424945831299, 0.1866721361875534, -0.3701940178871155, -0.8586384654045105, -0.533624529838562, 0.1490142047405243, 1.0209696292877197, -0.42731040716171265, -0.183164581656456, -0.06829510629177094, 0.4198344945907593, -1.0795100927352905, 1.0030152797698975, -0.39873966574668884, -0.021467367187142372, -0.07491977512836456, -0.07916203141212463, 0.20518279075622559, -0.3514326512813568, 0.291879802942276, -0.570918083190918, -0.28401651978492737, 0.6361782550811768, -0.11204497516155243, 1.318631649017334, -0.4956004023551941, 0.30268698930740356, -0.022243134677410126, -0.0898313969373703, 0.32605963945388794, 0.37047824263572693, -0.17773030698299408, -0.46950146555900574, 0.46644940972328186, 0.6971891522407532, -0.38086286187171936, 0.6188726425170898, 0.9924353957176208, 0.8639453649520874, -0.5191693305969238, 0.10853088647127151, 0.4484310448169708, -0.17657819390296936, 0.5945119857788086, 0.30518296360969543, 0.9152752161026001, 0.1491178572177887, 0.17941641807556152, -0.23063455522060394, 0.3899586498737335, -1.145709753036499, -0.2598053812980652, 0.41757407784461975, 0.5117068886756897, 0.9138855934143066, 0.640283465385437, -0.5496455430984497, -0.5697466731071472, -0.20259299874305725, 0.44359803199768066, 1.3676129579544067, -0.2012537121772766, -0.5989879369735718, -0.5537610054016113, -0.11395717412233353, -0.3540334105491638, 0.2667511999607086, -0.1402168869972229, -0.2294456511735916, -0.5532271862030029, -0.5411722660064697, 1.1435438394546509, 0.25916311144828796, 0.8853545784950256, -0.12018603086471558, -0.21683882176876068, -0.40372148156166077, 0.3565857410430908, -1.0898042917251587, -0.43890494108200073, 0.6605169177055359, -0.6771421432495117, 0.4092686176300049, 0.26593056321144104, 0.10798017680644989, 0.11352471262216568, -0.7454050779342651, 0.6991895437240601, -0.3845856785774231, 0.042520470917224884, -0.07431460916996002, 0.5419261455535889, -0.2790790796279907, -1.0236732959747314, 0.0242513045668602, 0.020196136087179184, -0.3057083487510681, 0.5293972492218018, 0.45992985367774963, 0.0446133129298687, -0.2406104952096939, -0.3431720435619354, 0.27743127942085266, 0.11522215604782104, -0.3774433434009552, 0.5985369682312012, -0.1738906353712082, -0.5619313716888428, -0.8820549249649048, 0.7324146032333374, 0.13185781240463257, -0.2985092103481293, 0.12387323379516602, -0.690498948097229, -0.037827447056770325, 0.5356525182723999, -0.043104954063892365, -0.25144118070602417, -1.2028154134750366, 0.14896930754184723, -0.6418693661689758, 0.08517961949110031, 0.16413699090480804, 0.6094231605529785, 0.3998831808567047, -0.1863962709903717, 0.4275147318840027, 0.3131459653377533, -0.3178691864013672, 0.5524528622627258, -0.7270082235336304, 0.2351943552494049, 0.09010638296604156, 0.15413077175617218, -0.052064407616853714, -0.09138190746307373, -0.6539188027381897, -0.13232029974460602, -0.20285636186599731, 0.056974973529577255, -0.0011034940835088491, 0.3116331994533539, -0.7467538714408875, -0.4217756688594818, -0.3541581630706787, -1.4804600477218628, 0.01361627597361803, 0.05310230702161789, -0.441301167011261, 0.09323005378246307, -1.0261025428771973, -1.4272841215133667, -0.5104159712791443, -0.8998367190361023, -1.433241367340088, 0.8397648930549622, -0.2586354911327362, -0.5652710795402527, -0.1448555290699005, -0.5408197641372681, -0.47781914472579956, 0.7928963899612427, -0.9944390654563904, 0.8787180185317993, 0.08774423599243164, -0.1904590129852295, 0.07457730174064636, -0.329110711812973, 0.4763677418231964, -0.47894981503486633, 0.7129806876182556, -1.0259194374084473, 0.4063258171081543, -0.5752704739570618, -0.13309453427791595, 0.21536269783973694, 0.11675836145877838, 1.367805004119873, -0.004450966604053974, -0.395906925201416, 0.4860999882221222, 1.1913150548934937, -0.7037550210952759, 0.24897712469100952, -0.10320167243480682, 0.9370903968811035, -0.24354787170886993, -0.5045600533485413, 0.7582228779792786, 0.05877823010087013, 0.6889516711235046, 0.07742679864168167, -0.04736022278666496, -0.25698792934417725, -0.4435044825077057, 0.9312481880187988, 1.9213149547576904, 0.5338488817214966, -0.19924670457839966, -0.9915132522583008, 0.3390033543109894, -0.8183383345603943, -0.3838346302509308, 0.4776195287704468, 0.8795090913772583, 0.3402549922466278, -0.05013507604598999, -0.4466630518436432, 0.16373640298843384, 0.3527195453643799, 0.5578023195266724, -0.04639061167836189, -1.462019681930542, 0.2437083125114441, 0.7379031181335449, 0.7222029566764832, 0.6600160598754883, -0.18393605947494507, 0.3861755430698395, 14.78432846069336, 0.9472510814666748, -0.24495774507522583, 0.6399316787719727, 0.8069372177124023, 0.46357500553131104, -0.7208015322685242, -0.00017570042109582573, -1.3684240579605103, 0.0888436958193779, 1.7435671091079712, -0.12903910875320435, 0.2020879089832306, -0.050244495272636414, 0.20522832870483398, 0.4353531301021576, -0.39602580666542053, 0.5336264967918396, 0.25237786769866943, -1.4219743013381958, 0.7767624258995056, 0.10397183150053024, -0.06974348425865173, 0.5797240734100342, 0.8418010473251343, 0.6786974668502808, 0.4242396652698517, -0.6841022372245789, 0.7803693413734436, -0.09572151303291321, 1.5342652797698975, -0.13366827368736267, 0.3065691888332367, 0.3526250123977661, -1.2165119647979736, -0.04686589539051056, -0.4111042618751526, -1.295556664466858, 0.1260296255350113, 0.29750916361808777, -1.1485298871994019, -0.668074905872345, -0.16770121455192566, 0.5221811532974243, 0.12043309211730957, 0.1707819700241089, -0.09876623749732971, 1.0259367227554321, -0.4250522255897522, 0.24077963829040527, 0.03773032873868942, 0.756490170955658, -0.030509335920214653, 0.12657150626182556, 0.23685917258262634, -0.3499089479446411, 0.09527262300252914, 0.44340837001800537, -0.49868908524513245, -0.1776508241891861, -0.24570052325725555, -0.16224735975265503, -0.030834142118692398, 0.3895305395126343, 0.30977314710617065, 0.12810118496418, -0.4761601388454437, 0.3165000081062317, 0.44128504395484924, 0.28250226378440857, -0.6006511449813843, 0.4137958884239197, 0.533315122127533, -0.1660674512386322, 0.01791100762784481, 0.4216211438179016, -0.22563336789608002, -0.5508185625076294, -0.846375048160553, -0.39556917548179626, 0.13166360557079315, -0.5936944484710693, -0.24380424618721008, 0.6523398160934448, 0.08426938951015472, -0.2592865526676178, 0.2092781960964203, -0.5489701628684998, 0.06708011776208878, 0.3273165822029114, -1.3488736152648926, -0.6676957607269287, 0.48455673456192017, -0.4249681234359741, -0.26745742559432983, 0.29352033138275146, 1.4915505647659302, 0.513943076133728, -0.18370994925498962, 0.23697321116924286, 0.14389382302761078, -0.031520575284957886, -0.4599846601486206, -0.5920330882072449, 1.2227439880371094, 0.447381854057312, -0.16291239857673645, 0.49482712149620056, -0.10198018699884415, 0.1867997646331787, -0.9607703685760498, -0.5216869711875916, 0.9976220726966858, -0.41940242052078247, -0.022605858743190765, -1.0449470281600952, -0.48253223299980164, 0.4953281581401825, 0.30306780338287354, 0.04228101298213005, 0.2840844392776489, -0.039310939610004425, -0.6345123648643494, -0.26691919565200806, -0.28623223304748535, 0.3378547430038452, 0.46644696593284607, -1.0054222345352173, 0.3605436086654663, -0.3709842264652252, 0.4902631342411041, -1.0255577564239502, -0.6684272885322571, 0.1925644427537918, 0.11152259260416031, -0.18989303708076477, 0.8871818780899048, 0.08229737728834152, 0.9238893985748291, 0.5609272718429565, -0.3756449222564697, -0.27314507961273193, -0.07118628919124603, -0.9311954975128174, -0.2887953817844391, -0.1454213708639145, 0.8498053550720215, -0.15438172221183777, 0.5210523009300232, 0.7429327368736267, 0.1549333930015564, -0.4221927523612976, -0.9576187133789062, -0.3070509135723114, -0.02073623426258564, -0.8192384839057922, 0.4172356128692627, -0.7460333704948425, -0.2504369616508484, 0.13400281965732574, 0.36076799035072327, 0.36543259024620056, -0.17321836948394775, -0.9902099967002869, 0.30650508403778076, 0.23969215154647827, -0.06493230164051056, -0.5013794302940369, -0.8323354721069336, -1.4990657567977905, 0.059281133115291595, -1.0968736410140991, 0.013764377683401108, -0.5459270477294922, -0.3251815140247345, -0.2820378541946411, -0.3233003616333008, 0.023136558011174202, 0.660883903503418, 0.19430264830589294, -0.42277273535728455, -0.3736227750778198, -0.6312082409858704, 0.8675251603126526, 0.7396565079689026, -0.7548512816429138, 0.5844509601593018, -0.31454113125801086, 0.4183107912540436, 0.4770657420158386, 0.23203912377357483, -0.37160050868988037, -1.1735117435455322, -1.7323744297027588, 0.4379505217075348, -0.3126995861530304, -0.23001784086227417, -0.8233741521835327, 0.5611538887023926, 0.45842310786247253, -0.16193558275699615, 0.16141444444656372, 0.43793952465057373, -0.7774660587310791, -0.33135858178138733, 0.5337671041488647, -0.8425642251968384, 0.47670677304267883, 0.5678569674491882, -0.6685578227043152, -0.24085572361946106, 0.711679220199585, -0.13463395833969116, -0.569845974445343, -1.00541353225708, 0.37204423546791077, -0.4838075637817383, 0.08029703050851822, -0.4424891173839569, -0.12211894243955612, -0.8667572140693665, -0.30572009086608887, 0.17478884756565094, -0.1727944314479828, -0.16369904577732086, 0.9325240850448608, 0.7255591750144958, -0.805446982383728, -0.09160558134317398, 0.7034461498260498, -0.07097014039754868, -0.1733526736497879, 0.06222303956747055, 0.35931333899497986, -0.7529463768005371, 0.48280069231987, 0.4995180368423462, 0.059560663998126984, -0.9993085265159607, -0.19898013770580292, 0.33674144744873047, -0.45758581161499023, -0.15564380586147308, 1.2039557695388794, -0.25360649824142456, -0.5867548584938049, -0.17326922714710236, -1.7451601028442383, -0.22387664020061493, -0.7627960443496704, 0.5668644905090332, -0.027467407286167145, 0.31352582573890686, -0.10762353986501694, -0.7515072822570801, -0.15715016424655914, -0.18326431512832642, -0.6743817329406738, -0.12010787427425385, -0.022170791402459145, -0.6771163940429688, 0.5303513407707214, 1.0277291536331177, -0.38256487250328064, -0.14097590744495392, -0.4201911687850952, -0.2360599786043167, 0.07499293982982635, 0.4748009443283081, -0.0032489346340298653, -0.6025971174240112, 0.9359887838363647, 0.40130001306533813, 0.2054092139005661, 0.20180879533290863, -0.4596570134162903, 0.5973796844482422, 0.23262304067611694, 0.49308550357818604, -0.3335532248020172, -0.8175643682479858, 1.3854870796203613, 0.8873531222343445, -0.4814198613166809, 0.5106728672981262, -0.2823960781097412, -0.2770538032054901, 0.8289282321929932, 0.09932032227516174, 0.04576946794986725, 1.150234580039978, 0.4907718300819397, 0.05710650980472565, 0.4373558759689331, -1.1266378164291382, -0.10334789007902145, 0.8604037165641785, 0.541905403137207, 1.1682612895965576, 0.41460034251213074, 0.22681957483291626, 0.6456707119941711, 0.1542922407388687, 0.3430400788784027, 0.2603321969509125, 0.49373534321784973, -0.21500322222709656, -0.24235033988952637, 0.041960325092077255, 0.8483019471168518, -0.7686523199081421, -1.1696499586105347, 0.4514455199241638, 0.40481480956077576, 0.5229226350784302, 0.45972007513046265, 1.213484525680542, -0.36248770356178284, 0.27751216292381287, 0.11182955652475357, 0.5101784467697144, -0.8612031936645508, -0.38721245527267456, -0.08995034545660019, -0.3112589120864868, -0.08640560507774353, 0.1185552179813385, -0.004613386932760477, -0.732603907585144, -0.6090414524078369, 0.40298646688461304, 0.04191382974386215, 0.28130459785461426, 0.8375276923179626, 0.8102736473083496, 0.5348846316337585, -0.4588417112827301, -0.15368303656578064, -0.3943469524383545, -0.6936149597167969, 0.09462223947048187, -0.6702373027801514, -0.1645011156797409, 0.12251172959804535, 0.019839540123939514, -0.2558305859565735]}, "authors": [{"authorId": "2129511744", "name": "Xiaoxia Wu"}, {"authorId": "2133092553", "name": "Cheng Li"}, {"authorId": "3394222", "name": "Reza Yazdani Aminabadi"}, {"authorId": "9088433", "name": "Z. Yao"}, {"authorId": "2145020341", "name": "Yuxiong He"}], "references": [{"paperId": "53535d38fe259a3aa7c911edd8048d764e09e8e1", "title": "The case for 4-bit precision: k-bit Inference Scaling Laws"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "30a7390ec0103684eba9fb6bde1983d706fb57b3", "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "c022f75b00d795c6297d6a9ea948856ea4d365a1", "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"}, {"paperId": "7dd1557758ca670431cf7486790fe7332e581400", "title": "Compressing Pre-trained Transformers via Low-Bit NxM Sparsity for Natural Language Understanding"}, {"paperId": "64a8a7eb8360f4003af85c2c8a3bf245fb2f94ae", "title": "Extreme Compression for Pre-trained Transformers Made Simple and Efficient"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "73c722148ed4a5301dc75ae291b647a1915b8ecd", "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"}, {"paperId": "0bffff14430c6fa212c3924cb68190734bd61e2d", "title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "5e8a892cc33a9fb9e701d5e10333e7b8545bf4a5", "title": "LEAP: Learnable Pruning for Transformer-based Models"}, {"paperId": "4267d94ca28170f8bab97888757691f84da09356", "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit"}, {"paperId": "90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb", "title": "Accelerating Sparse Deep Neural Networks"}, {"paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e", "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "755f16fdbbfb0b128adb3cda9c8c2799aea34290", "title": "Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "159dc82a5ee901716b0154051988b5408acfc861", "title": "LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "57f123c95ecf9d901be3a53291f53302740451e2", "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d", "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "f6a4bf043af1a9ec7f104a7b7ab56806b241ceda", "title": "Model compression via distillation and quantization"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "6a821cb17b30c26218e3eb5c20d609dc04a47bcb", "title": "Exploring the Regularity of Sparse Structure in Convolutional Neural Networks"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "10a2482088e469dd40f49bdc9978b292b3f7bb1f", "title": "Ternary Weight Networks"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "71b4ca7440a06094f58a4dace1a455ad6430bead", "title": "Recognizing Textual Entailment: Models and Applications Ido Dagan1, Dan Roth2, Mark Sammons2, and Fabio Massimo Zanzotto3 (1Bar-Ilan University, Israel, 2University of Illinois, Urbana, IL, and 3University of Rome \u201cTor Vergata,\u201d Italy) Morgan & Claypool (Synthesis Lectures on Human Language Technolo"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": null, "title": "NVIDIA"}, {"paperId": "b4829f2e7968be42bead06b1710abee6f5a0afc2", "title": "A Comprehensive Study on Post-Training Quantization for Large Language Models"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": null, "title": "To quantize a weight matrix W \u2208 Rdin\u00d7dout in a model, we apply the group-wise quantization method proposed in Shen et al"}, {"paperId": "3c61e6b55597cf37b19d2e4b38fc66b9c85c97b9", "title": "Ultra-Low Precision 4-bit Training of Deep Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "First quora dataset release: Question pairs"}, {"paperId": null, "title": "CUTLASS: Fast Linear Algebra in CUDA C++"}, {"paperId": "bf5415ffc086cc8746a4b82898f4e05b63b224e6", "title": "Convolutional Neural Network with INT4 Optimization on Xilinx Devices"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "Employing CUDA Graphs in a Dynamic Environment"}, {"paperId": null, "title": "GPU workstation for deep learning"}]}