{"paperId": "7294c426b8a95975ca932eaf8f700acdd3d950b2", "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models", "abstract": "Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.", "venue": "arXiv.org", "year": 2024, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms."}, "embedding": {"model": "specter_v2", "vector": [0.30943581461906433, 0.8164641857147217, -0.47265109419822693, -0.17637285590171814, -0.29123711585998535, -0.1718912571668625, 0.639698326587677, -0.16261114180088043, -0.4036618769168854, -0.17680507898330688, 0.49223077297210693, -0.15934282541275024, 0.7329018712043762, 0.2094721496105194, -0.33551716804504395, -0.04336371645331383, -1.2077220678329468, 0.025783047080039978, -0.034728869795799255, 0.27442458271980286, 0.0277300626039505, -0.7668693661689758, -1.0793261528015137, 0.5128036737442017, 0.29320383071899414, 0.494462788105011, 0.6333810687065125, 0.95754075050354, -0.47251084446907043, 0.5617743134498596, 0.20240893959999084, -0.16550227999687195, 0.11197777092456818, 0.06545956432819366, -0.3671201467514038, -0.36326220631599426, 0.7406666874885559, -0.22225381433963776, -0.6771997213363647, 0.6330800652503967, -0.29039284586906433, 0.2679575979709625, 0.461823046207428, -0.6544868350028992, -0.2984589636325836, 1.090871810913086, 0.33778947591781616, 0.8789986371994019, -0.3602864742279053, -0.5884202122688293, 1.3464514017105103, -1.3513602018356323, -0.08423271030187607, 1.6233099699020386, 0.24213732779026031, 0.04402489960193634, 0.18257054686546326, -0.6917971968650818, 1.0855354070663452, 0.33857473731040955, -0.831125020980835, -0.1521831452846527, -0.16983862221240997, -0.09336497634649277, 2.222071886062622, -0.25643423199653625, 0.12762939929962158, 0.5739827752113342, 0.01364469900727272, 1.7023296356201172, -0.21389201283454895, -1.1994987726211548, -0.3985772430896759, 0.045724254101514816, 0.6994038224220276, 0.5604774355888367, -0.4201674461364746, 0.32058924436569214, -0.7297364473342896, -0.12339607626199722, 0.4308249354362488, -0.3578953146934509, 0.25470900535583496, 0.37434983253479004, -0.39902636408805847, 0.749041736125946, 0.2636549770832062, 0.7221588492393494, -0.10848524421453476, 0.8409164547920227, 0.42755985260009766, -0.2355273962020874, -0.06349107623100281, 0.16684184968471527, -0.04697655886411667, 0.4544552266597748, -1.005565881729126, 0.2176002562046051, -0.18696606159210205, 1.3029793500900269, -0.4991548955440521, 0.5488007664680481, -0.7711174488067627, -0.101622074842453, 1.1727828979492188, 0.24487270414829254, 0.35725048184394836, -0.453924298286438, 0.26708391308784485, -0.8151232004165649, -0.015575657598674297, -0.6066927313804626, -0.2658214271068573, -0.3475854694843292, -0.6645997762680054, -0.9409964084625244, -0.3351324200630188, 0.29300424456596375, -0.6790950298309326, 0.5290888547897339, -0.3338943421840668, 0.1805625855922699, -0.22310790419578552, 0.12288966029882431, 0.49181246757507324, 0.8908861875534058, 0.4932907819747925, 0.0879179909825325, 1.186279535293579, -1.0132919549942017, -0.6828002333641052, -1.359607219696045, 0.38256576657295227, -0.2245057076215744, 0.1856025606393814, -0.13414062559604645, -1.219159483909607, -0.974534809589386, -0.9362063407897949, -0.17763547599315643, -0.5143930912017822, 0.4405146837234497, 1.2285149097442627, 0.05666756629943848, -0.5950162410736084, 0.6021110415458679, -0.3482200503349304, 0.11864548921585083, 0.5464727282524109, -0.10343771427869797, 0.5175561308860779, -0.20555172860622406, -1.513274908065796, 0.22631412744522095, 0.5491434931755066, -0.4617599844932556, -0.07313951104879379, -0.697833240032196, -1.1515923738479614, 0.2649056315422058, 0.19056814908981323, -0.028475696220993996, 1.2156119346618652, -0.2900503873825073, -0.8500452041625977, 0.8317398428916931, -0.8671234846115112, -0.11872625350952148, 0.05450015515089035, -0.31345972418785095, -0.3020155727863312, -0.742078959941864, -0.2167700082063675, 0.7680573463439941, 0.564051628112793, -0.10553223639726639, -0.41676101088523865, 0.26907938718795776, -0.28725454211235046, -0.24578574299812317, -0.4808556139469147, 0.8830219507217407, -0.6648963689804077, -0.3418470621109009, -0.13222633302211761, 0.4480091333389282, 0.174880251288414, -0.5286307334899902, -0.4514480531215668, -1.3917913436889648, 0.3879052698612213, -0.04310906305909157, 1.3348774909973145, -0.7463229894638062, -0.9696025848388672, -0.33924275636672974, -0.33787357807159424, -0.02074408158659935, -0.5925069451332092, 0.4632871448993683, -0.6110896468162537, 0.42367422580718994, 0.049838483333587646, -1.0191030502319336, 0.019937241449952126, -0.20075124502182007, -0.7285898923873901, -0.2212875783443451, -0.039376746863126755, 1.2519985437393188, -0.9374297261238098, -0.25995731353759766, -0.00892569124698639, 0.07143054157495499, -0.8189378976821899, 1.3369779586791992, -0.5501706004142761, -0.037446971982717514, 0.05278254672884941, -0.3559528887271881, -0.049970757216215134, -0.45870524644851685, 0.5171645283699036, -0.2733004689216614, -0.5149466395378113, 0.6081507205963135, -0.0358000211417675, 1.1861544847488403, -0.2994363009929657, 0.7654816508293152, -0.5927634835243225, -0.6555872559547424, -0.10744746029376984, 0.4141453504562378, -0.4241923987865448, -0.5250961184501648, 0.25451529026031494, -0.1664465069770813, -0.7913589477539062, -0.16653437912464142, 0.6566641330718994, 1.132586121559143, -0.40345409512519836, 0.18384242057800293, 0.5680962800979614, 0.07617060095071793, 0.3845517635345459, 0.6327608227729797, 0.5564253926277161, 0.4101177453994751, 0.6947916746139526, -0.1741490513086319, 0.10026638954877853, -0.7782817482948303, -0.18885941803455353, 0.719358503818512, 0.9565407633781433, 0.7445917129516602, 0.6940370202064514, -0.9304894804954529, -0.79454106092453, 0.42256608605384827, 0.45981597900390625, 1.4001730680465698, -0.39550352096557617, 0.06353652477264404, -0.5607084631919861, -0.08880117535591125, -0.2271590530872345, 0.44766107201576233, -0.6090498566627502, 0.077135369181633, -0.7076656222343445, -1.0016008615493774, 0.8236104846000671, 0.45669132471084595, 0.6610798239707947, -0.8399969339370728, -0.5154019594192505, -0.24426484107971191, 0.19707784056663513, -1.0294721126556396, -0.8231254816055298, 0.37954181432724, -0.20592452585697174, 0.12388673424720764, 0.36499521136283875, -0.12789323925971985, 0.2431182861328125, -0.5709081888198853, 1.079418420791626, -0.6308750510215759, -0.6214242577552795, -0.021809618920087814, 0.5308430790901184, -1.0382412672042847, -0.46204835176467896, 0.3903215825557709, -0.061213042587041855, -0.24828526377677917, 0.47438645362854004, 0.661034107208252, 0.09565946459770203, -0.4574067294597626, -0.3233996331691742, 0.297559529542923, 0.1370188146829605, -0.05043553560972214, 0.3919280469417572, -0.36113253235816956, 0.1553965061903, -1.037194013595581, 0.5274801850318909, 0.03074570745229721, -0.6517283916473389, 0.24548642337322235, -0.29520127177238464, -0.3207235038280487, 0.409434974193573, -0.7675925493240356, -0.29295530915260315, -0.6787765026092529, 0.31382545828819275, -0.49162518978118896, -0.4498121738433838, 0.06515669822692871, 0.27734172344207764, 0.2705936133861542, 0.1537259966135025, 0.28466498851776123, -0.08193036913871765, 0.06848426908254623, 0.6084368228912354, -0.875921905040741, 0.6835180521011353, 0.43479061126708984, -0.1361652910709381, -0.09852397441864014, -0.328883558511734, -0.8931149840354919, -0.8744098544120789, -0.5566366314888, -0.6548090577125549, -0.23304483294487, 0.443107008934021, -0.5565440058708191, -1.14966881275177, 0.05523945763707161, -1.5326951742172241, -0.303819864988327, 0.45893460512161255, -0.014727616682648659, 0.174819216132164, -0.8582651019096375, -0.912913978099823, -0.8301440477371216, -0.6013202667236328, -0.5043295621871948, 0.32335537672042847, 0.07876140624284744, -0.49931660294532776, -0.9010469913482666, -0.08785588294267654, -0.4228140711784363, 0.9015403389930725, -0.4599957764148712, 0.7561088800430298, 0.03668968379497528, -0.34782978892326355, -0.6275793313980103, 0.24541662633419037, 0.273254930973053, -0.29260578751564026, 0.12025950103998184, -1.0218706130981445, 0.2763729393482208, -0.46441105008125305, -0.3450010120868683, 0.14874956011772156, 0.5078781247138977, 0.9306700825691223, -0.270816832780838, -0.6171268820762634, 0.16086453199386597, 1.3334345817565918, -0.48536646366119385, 0.04856766760349274, 0.1001444086432457, 1.090045690536499, 0.10773247480392456, -0.08716707676649094, 0.7912213802337646, 0.4132051467895508, 0.6609571576118469, 0.353679358959198, -0.24167630076408386, -0.07795318961143494, -0.3695259988307953, 0.3319259583950043, 1.309394121170044, 0.27689170837402344, -0.1024242639541626, -0.82487553358078, 0.8823660016059875, -1.1379858255386353, -1.2564940452575684, 0.39202454686164856, 0.8477373123168945, 0.33267366886138916, -0.3573060631752014, -0.5196651220321655, -0.40396422147750854, 0.534429669380188, 0.3769442141056061, -0.3836437165737152, -0.8722449541091919, 0.036690328270196915, 0.23223590850830078, 0.31320783495903015, 0.8808364272117615, -0.6828873753547668, 0.6269999742507935, 14.904172897338867, 0.9007588028907776, -0.08577314019203186, 0.4986453354358673, 0.6592377424240112, -0.005745685193687677, -0.2124275416135788, -0.2690252661705017, -1.753838062286377, 0.07716266810894012, 0.9957281947135925, 0.1224788948893547, 0.48787638545036316, 0.1613176316022873, 0.27868032455444336, 0.24535514414310455, -0.4627287685871124, 0.6189139485359192, 0.859247088432312, -1.2543436288833618, 0.2548893988132477, 0.06923455744981766, 0.3270627558231354, 0.3618863821029663, 0.5968943238258362, 0.678734540939331, 0.5274177193641663, -0.5834994912147522, 0.6427410244941711, 0.3472232520580292, 0.8971183896064758, -0.12060681730508804, 0.3942047655582428, 0.5716667771339417, -1.2268810272216797, 0.06479115039110184, -0.7935994863510132, -1.1306959390640259, 0.44749414920806885, 0.40128132700920105, -0.6474120616912842, -0.43619340658187866, -0.2568730115890503, 0.5890480279922485, 0.21840490400791168, 0.3935902416706085, -0.3330081105232239, 0.8190159201622009, -0.04794721677899361, -0.018272673711180687, 0.7149158716201782, 0.8425383567810059, -0.05440570414066315, 0.32268980145454407, 0.0020189175847917795, 0.12912747263908386, 0.1186072900891304, 0.5470990538597107, -0.263743132352829, -0.13023613393306732, -0.2734241187572479, -0.22009027004241943, 0.14194172620773315, 0.8702569007873535, 0.5663012862205505, 0.07100693881511688, -0.4885447025299072, 0.19180983304977417, 0.9769489169120789, -0.022262640297412872, -0.7741082906723022, 0.05438375473022461, 0.12071523815393448, -0.6940197348594666, 0.24821113049983978, 0.39745330810546875, -0.3474639654159546, -0.5345126390457153, -0.8014810681343079, -0.3000412583351135, 0.3281697630882263, -0.7005890607833862, -0.505741536617279, 0.8768182396888733, -0.18210481107234955, -0.16738653182983398, 0.2563217282295227, -0.6847570538520813, -0.5597102642059326, 0.5306877493858337, -1.1116652488708496, -0.8541176915168762, 0.14849677681922913, -0.46894609928131104, 0.07375012338161469, 0.1953437775373459, 1.1474045515060425, 0.22660735249519348, -0.38978415727615356, 0.35491225123405457, -0.08374376595020294, -0.022871563211083412, -0.2977157235145569, -0.6805286407470703, 1.131637454032898, 0.2295481264591217, -0.20206469297409058, 0.20701172947883606, -0.014811806380748749, 0.15081673860549927, -0.49640747904777527, -0.19875966012477875, 1.0964114665985107, -0.8730279803276062, -0.3860413432121277, -0.9787461161613464, -1.0655392408370972, 0.5665732026100159, 0.8061058521270752, -0.3841208815574646, 0.2095097452402115, 0.3221800923347473, -0.5235167145729065, 0.23677803575992584, 0.00140389206353575, -0.17247843742370605, 0.14182478189468384, -0.42704230546951294, -0.37361595034599304, -0.07474968582391739, 0.542107343673706, -0.9775189161300659, -0.5234341621398926, -0.4099521338939667, 0.42407214641571045, 0.08256318420171738, 1.0209802389144897, -0.48163464665412903, 0.59762042760849, 1.057202696800232, 0.2449580579996109, -0.5479857921600342, -0.3962216079235077, -0.5792430639266968, 0.03530048951506615, 0.18406696617603302, 0.6366350650787354, -0.1496586948633194, 0.13355231285095215, 0.5680313110351562, 0.12026230245828629, -0.44734618067741394, -0.525550127029419, -0.4013260006904602, 0.22040976583957672, -0.8181691765785217, -0.025931362062692642, 0.11724817752838135, 0.2090611606836319, -0.029920291155576706, -0.07613493502140045, 0.7880157232284546, 0.09045242518186569, -0.41971179842948914, 0.1722649782896042, 0.1700606346130371, -0.05657065659761429, -0.37915778160095215, -0.39390864968299866, -1.4511244297027588, 0.2991398870944977, -1.1562533378601074, 0.026635147631168365, -0.8790252804756165, -0.2192748337984085, -0.05482105910778046, 0.1122848242521286, 0.45276394486427307, -0.002713855355978012, -0.49620524048805237, -0.3803524672985077, -0.6329309940338135, -0.5170175433158875, 0.6019147038459778, 0.6427571177482605, -0.5742678046226501, 0.34841954708099365, 0.09326839447021484, 0.46060898900032043, 0.3432208299636841, 0.4518490433692932, -0.4331115484237671, -0.44203391671180725, -1.4213162660598755, 0.3775651156902313, -0.07215525954961777, -0.1332230418920517, -0.6396957039833069, 0.7554377913475037, 0.429534912109375, -0.14672145247459412, -0.2730633020401001, 0.232181578874588, -0.5045061707496643, -0.754734456539154, 0.2998516261577606, -0.9429606199264526, -0.0010841829935088754, 0.5844635963439941, -0.6741814017295837, -0.14384914934635162, 0.34751421213150024, 0.005184599664062262, -1.1446510553359985, -0.6550007462501526, 0.481975793838501, -0.7236308455467224, 0.2473587989807129, -0.09787486493587494, 0.13482679426670074, -1.037396788597107, -0.2932569980621338, 0.028867628425359726, 0.3846474587917328, -0.6540085077285767, 1.2526451349258423, 0.24029994010925293, -1.0448837280273438, 0.10188424587249756, 0.35722947120666504, -0.19095547497272491, 0.0686841532588005, 0.6176431775093079, 0.20946961641311646, -0.0914510190486908, 1.1196608543395996, 0.3683261275291443, 0.20854751765727997, -1.288150668144226, 0.3431536853313446, 0.36692607402801514, -0.6101475358009338, -0.21078146994113922, 1.1082842350006104, 0.011680024676024914, -0.9428935050964355, 0.5477837324142456, -1.2043074369430542, -0.6730607151985168, -0.19244937598705292, 0.8715474605560303, 0.16598035395145416, -0.12014524638652802, 0.16557620465755463, -0.4647148549556732, 0.22432908415794373, 0.040119387209415436, -0.26841944456100464, 0.25834381580352783, -0.174291729927063, -0.3108477294445038, 0.9584835767745972, 0.9078500866889954, -0.6721400022506714, -0.6348996758460999, -0.5748751759529114, -0.46241629123687744, 0.06894971430301666, 0.30770692229270935, -0.2070627361536026, -0.4045097827911377, 1.0834747552871704, 0.11520425975322723, 0.41629430651664734, -0.383127897977829, -0.04037819430232048, -0.03643650561571121, 0.3968553841114044, 0.06229407712817192, -0.2919938266277313, -0.7333142161369324, 1.5766793489456177, 1.259189486503601, -0.7627831697463989, 0.20752421021461487, -0.14507418870925903, -0.899927020072937, 1.0459976196289062, 0.4340825378894806, 0.02880394458770752, 0.5697314739227295, 0.08336567878723145, -0.1444120556116104, 0.2017616480588913, -1.2672911882400513, -0.08577743917703629, 0.826011598110199, 0.6240531802177429, 1.1052374839782715, 0.2765384912490845, 0.04990731552243233, 0.5892283320426941, 0.12565137445926666, 0.08768632262945175, 0.2774375379085541, 0.4206612706184387, -0.3789764642715454, 0.14401935040950775, -0.20387418568134308, 0.7122789621353149, -0.7740626335144043, -0.9826838970184326, 0.6179964542388916, 0.4430369436740875, 0.20371945202350616, 0.4870484471321106, 1.1415358781814575, 0.5723285675048828, 0.2706449031829834, 0.2872297465801239, 0.5891441702842712, -0.3867638409137726, -0.08042546361684799, -0.3190942704677582, -0.8323652744293213, -0.09820786118507385, -0.07237960398197174, -0.6222057342529297, -0.501129150390625, -0.0642588883638382, 0.21698026359081268, -0.24158179759979248, 0.24248482286930084, 1.1176602840423584, 0.42470312118530273, 0.8927241563796997, -0.4492775797843933, -0.8648115992546082, -0.005950555671006441, -1.1209162473678589, 0.03718139976263046, -0.7988017797470093, -0.33259299397468567, -0.12518452107906342, -0.2207375466823578, -0.007916956208646297]}, "authors": [{"authorId": "2171650015", "name": "Zhen Qin"}, {"authorId": "2225238340", "name": "Weigao Sun"}, {"authorId": "2179703418", "name": "Dong Li"}, {"authorId": "2116517206", "name": "Xuyang Shen"}, {"authorId": "8397429", "name": "Weixuan Sun"}, {"authorId": "2266275708", "name": "Yiran Zhong"}], "references": [{"paperId": "434d751d355d7a7c20efa570e785c76286245e77", "title": "Hierarchically Gated Recurrent Neural Network for Sequence Modeling"}, {"paperId": "e066f2c97445ed21aaf05a1f525d0673fb082808", "title": "Multimodal Variational Auto-encoder based Audio-Visual Segmentation"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "d782a43e36f2cd3c7f633c12251928219454ed95", "title": "Improving Audio-Visual Segmentation with Bidirectional Generation"}, {"paperId": "2a38daf98d506477f8180806f503409d5036eaf4", "title": "TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer"}, {"paperId": "2f0203386f3dcbffb47c9f7fe2d19d373d9dda2f", "title": "Exploring Transformer Extrapolation"}, {"paperId": "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8", "title": "Linearized Relative Positional Encoding"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "f35f5aedc30e2c5ded210d9c91ba6e84bd029425", "title": "Toeplitz Neural Network for Sequence Modeling"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "b86634ac23cb154821da382448429492b5d0a404", "title": "Fine-grained Audible Video Description"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "ac608a4a6b19b3208e560eee5daadb3cc18638a2", "title": "Efficient Attention via Control Variates"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "5735e49e501c8e51e9be4079592e46e047747b03", "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "6be32b4321f95b79bb2e37feeab0c3c7f902195e", "title": "Vicinity Vision Transformer"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f", "title": "Linear Complexity Randomized Self-attention Mechanism"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "77b60fdaf00ba3287c07d5584df9be38bf8dcabc", "title": "MAP: Low-data Regime Multimodal Learning with Adapter-based Pre-training and Prompting"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Socialiqa: Commonsense reasoning about social interactions"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "cosformer: Rethink-ing softmax in attention"}, {"paperId": null, "title": "A multi-level multi-discipline"}, {"paperId": null, "title": "Lightning Attention-2"}]}