{"paperId": "a298d956dc3472541adc3d4bf2908755e5ff54d4", "title": "A Probabilistic Framework for Pruning Transformers Via a Finite Admixture of Keys", "abstract": "Pairwise dot product-based self-attention is key to the success of transformers which achieve state-of-the-art performance across a variety of applications in language and vision, but are costly to compute. It has been shown that most attention scores and keys in transformers are redundant and can be removed without loss of accuracy. In this paper, we develop a novel probabilistic framework for pruning attention scores and keys in transformers. We first formulate an admixture model of attention keys whose input data to be clustered are attention queries. We show that attention scores in self-attention correspond to the posterior distribution of this model when attention keys admit a uniform prior distribution. We then relax this uniform prior constraint and let the model learn these priors from data, resulting in a new Finite Admixture of Keys (FiAK). The learned priors are used for pruning away redundant attention scores and keys in the baseline transformers, improving the diversity of attention patterns that the models capture. We corroborate the efficiency of transformers pruned with FiAK on the ImageNet object classification and WikiText-103 language modeling tasks. Our experiments demonstrate that transformers pruned with FiAK yield similar or better accuracy than the baseline dense transformers while being much more efficient in terms of memory and computational cost.", "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "year": 2023, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A novel probabilistic framework for pruning attention scores and keys in transformers is developed and it is demonstrated that transformers pruned with FiAK yield similar or better accuracy than the baseline dense transformers while being much more efficient in terms of memory and computational cost."}, "embedding": {"model": "specter_v2", "vector": [0.13873180747032166, 0.8175114393234253, -0.1434512585401535, -0.11606625467538834, -0.37722012400627136, 0.12616857886314392, 0.7737028002738953, -0.2144765704870224, -0.5411031246185303, -0.6193330883979797, 0.544511079788208, 0.45294344425201416, 0.26008254289627075, 0.16412235796451569, -0.31164705753326416, 0.3827768564224243, -0.43708983063697815, 0.4693269431591034, 0.5037708878517151, -0.24248866736888885, -0.12257051467895508, -0.9910486340522766, -1.5119268894195557, 0.03327906131744385, 0.6751795411109924, 0.9001696705818176, 0.3374708592891693, 0.6679502725601196, -0.6075122952461243, 0.5254666805267334, 0.3472231924533844, -0.5890620946884155, 0.23824600875377655, 0.1981218457221985, -0.3450488746166229, -0.17786307632923126, 0.833889901638031, -0.12314312160015106, -0.646247386932373, 0.9980899691581726, -0.38472726941108704, 0.3539406657218933, 0.5038419365882874, -0.6172136068344116, -0.5501813888549805, 0.7371928691864014, 0.8785238862037659, 0.6523483395576477, -0.74267578125, -0.9212683439254761, 1.4896143674850464, -1.5526583194732666, 0.08489380031824112, 1.429357647895813, 0.3017937242984772, 0.05225856602191925, -0.2507559657096863, -0.5985607504844666, 0.8494384288787842, 0.48019516468048096, -1.2044563293457031, -0.30058884620666504, -0.03704380616545677, -0.2795524001121521, 2.0026087760925293, -0.36464235186576843, -0.07849188148975372, -0.09339827299118042, -0.09925109148025513, 1.5739502906799316, -0.5074391961097717, -0.7387940287590027, -0.21718017756938934, 0.04423235356807709, 0.5852354764938354, 0.9707421660423279, -0.5890962481498718, -0.03650730103254318, -0.7082986235618591, -0.2673038840293884, 0.3072960078716278, 0.16770760715007782, 0.28845924139022827, -0.48359864950180054, 0.13772203028202057, 0.5505586266517639, 0.543415367603302, 0.8224985599517822, -0.3389344811439514, 1.0094503164291382, 0.1922980397939682, 0.08141452074050903, 0.07928390055894852, 0.2885754108428955, -0.028368737548589706, 0.6185449361801147, -0.7990127801895142, 0.1047849953174591, -0.3055429756641388, 1.2960848808288574, 0.0732068344950676, -0.08627886325120926, -0.7282351851463318, 0.5115190744400024, 1.3429855108261108, -0.029627881944179535, 0.33030322194099426, -0.9798608422279358, -0.11319305002689362, -0.248338520526886, -0.18405993282794952, -0.8410695195198059, 0.16139018535614014, -0.15710239112377167, -0.7080937623977661, -1.118023157119751, -0.603162944316864, 0.4362398087978363, -0.7621735334396362, 0.7501201629638672, -0.12884770333766937, 0.04040641710162163, -0.42917105555534363, 0.2905418276786804, 0.27241456508636475, 0.43490612506866455, 0.5529438257217407, 0.4147898852825165, 1.0293080806732178, -0.9876372814178467, -0.46951109170913696, -0.7899344563484192, 0.7039678692817688, -0.17796286940574646, 0.19048826396465302, -0.0771758109331131, -1.3106993436813354, -1.0000311136245728, -0.6893625855445862, -0.29543569684028625, -0.6002468466758728, 0.31889206171035767, 0.7577475905418396, 0.1768324375152588, -1.133558750152588, 0.6370453834533691, -0.24170683324337006, -0.3844127357006073, 0.9211353659629822, 0.33893537521362305, 0.12333036214113235, -0.5696603655815125, -1.2230544090270996, 0.40470975637435913, -0.034122541546821594, -0.732285737991333, -0.28858891129493713, -0.7666364908218384, -1.0552536249160767, 0.19140903651714325, 0.5055175423622131, -0.5261935591697693, 1.3091744184494019, -0.348560094833374, -0.9255656003952026, 0.8204275369644165, -0.4297756254673004, 0.0066068097949028015, 0.08126168698072433, -0.35298269987106323, -0.18207040429115295, -0.5326741337776184, -0.04109309986233711, 0.6697701215744019, 0.6826489567756653, 0.050427015870809555, -0.31763607263565063, 0.037236928939819336, -0.17628802359104156, -0.10891175270080566, -0.5729242563247681, 0.9583684802055359, -0.7494862675666809, -0.25345149636268616, 0.3450070917606354, 0.7362079620361328, 0.38500767946243286, -0.14983901381492615, -0.24397417902946472, -1.6660213470458984, 0.7729398012161255, 0.4104287624359131, 0.6516353487968445, -1.0040971040725708, -0.5656995177268982, -0.4662399888038635, 0.13631707429885864, 0.07077398896217346, -0.7293431162834167, 0.3980807363986969, -0.4076736867427826, 0.5139845013618469, -0.004558681510388851, -1.209525465965271, 0.1711285412311554, -0.1001778244972229, -0.5341649651527405, -0.3294772207736969, 0.1482781022787094, 1.184698462486267, -0.9198494553565979, -0.1139586940407753, -0.08302121609449387, 0.22887219488620758, -0.9648401737213135, 1.3023568391799927, -0.6514050960540771, -0.3604363203048706, 0.007349724415689707, -0.03620598092675209, -0.06231321021914482, -0.142960786819458, 0.09630988538265228, -0.48610326647758484, -0.0022753223311156034, 0.6672089695930481, -0.08675583451986313, 1.371416687965393, -0.19187957048416138, 0.2479528784751892, -0.10625538975000381, -0.8417867422103882, 0.12981916964054108, 0.19630959630012512, -0.09571214020252228, -0.6261485815048218, 0.13296595215797424, -0.10010945051908493, -0.8091902732849121, 0.1584557145833969, 0.5666497945785522, 0.9398800134658813, -0.343478262424469, 0.24383795261383057, 0.7865613102912903, -0.25338688492774963, 0.14555670320987701, 0.6389641165733337, 1.0874907970428467, 0.251708984375, 0.4752885401248932, -0.25270622968673706, 0.37261462211608887, -0.6346752643585205, -0.1373811960220337, 0.5849763751029968, 0.6181364059448242, 0.8974888324737549, 0.3375599682331085, -0.7734314799308777, -0.40708282589912415, 0.06905459612607956, 0.6509056091308594, 1.8978180885314941, -0.32235583662986755, -0.1958889216184616, -0.8120182752609253, -0.5784830451011658, -0.5138255953788757, -0.048082221299409866, -0.6474519968032837, -0.2654382288455963, -0.34915176033973694, -0.7895954251289368, 0.7088778614997864, 0.3574763834476471, 1.1629481315612793, -0.42038244009017944, -0.19430892169475555, -0.2044440507888794, 0.30917060375213623, -0.6659091114997864, -0.9153968095779419, 0.3509095311164856, -0.03082139417529106, -0.15588617324829102, 0.12550942599773407, -0.30050528049468994, 0.236033633351326, -0.2572644352912903, 1.1512410640716553, -0.4715217649936676, -0.4378596246242523, 0.5523586273193359, 0.35569533705711365, -0.9077625870704651, -0.27403977513313293, 0.3438284993171692, 0.11217693239450455, -0.0648011639714241, 0.8298665881156921, 0.49857014417648315, -0.025987327098846436, 0.1932767629623413, -0.3333331048488617, -0.1927555650472641, 0.09027434140443802, 0.05737016722559929, 0.8347116708755493, -0.24169237911701202, -0.2304387092590332, -1.171757698059082, 0.4513666331768036, -0.06306854635477066, -0.4211195409297943, 0.30829161405563354, -0.32332196831703186, -0.40334492921829224, 0.36960914731025696, -0.6557173728942871, -0.3488471210002899, -0.644981324672699, 0.5748454928398132, -0.7137556076049805, -0.3085344135761261, 0.11830291152000427, 0.08784167468547821, 0.22341494262218475, 0.45280879735946655, 0.31244030594825745, 0.29354041814804077, 0.09287706017494202, 0.720797598361969, -0.9221771955490112, 0.8250669836997986, 0.06251278519630432, -0.14150576293468475, -0.027683289721608162, -0.36696749925613403, -1.126222848892212, -0.46232596039772034, -0.705852210521698, -0.08257044851779938, 0.008023861795663834, 0.2707271873950958, -0.5969610810279846, -0.63617342710495, 0.07711278647184372, -1.2233697175979614, -0.31526046991348267, -0.1376916468143463, -0.5656740665435791, -0.11413748562335968, -0.9397046566009521, -0.7309172749519348, -0.6279955506324768, -0.5847045183181763, -0.6413714289665222, 0.3563750088214874, 0.07268176227807999, -0.5027046799659729, -0.3168810307979584, 0.1002717986702919, -0.40215086936950684, 1.2100740671157837, -0.9360430240631104, 0.6926572322845459, -0.22064396739006042, -0.6508238315582275, -0.34524497389793396, 0.032532740384340286, 0.6690020561218262, -0.21364152431488037, 0.03694017603993416, -0.7758620381355286, 0.505433201789856, -0.19986014068126678, -0.28843504190444946, 0.38631343841552734, 0.38061022758483887, 0.8457265496253967, -0.03496427461504936, -0.601722002029419, 0.28265053033828735, 1.2856371402740479, -0.6939117908477783, 0.18594662845134735, -0.19437485933303833, 1.0562286376953125, 0.38222163915634155, -0.3009622097015381, 0.21900655329227448, 0.9223633408546448, 0.35772109031677246, 0.6442969441413879, -0.05123073607683182, -0.16744327545166016, -0.4138808846473694, 0.18875740468502045, 1.3244950771331787, 0.4428631067276001, 0.13321927189826965, -0.9856652617454529, 1.122544765472412, -1.4360027313232422, -0.7396572828292847, 0.556645393371582, 0.6632224321365356, 0.12071137130260468, -0.9075388312339783, -0.2499602884054184, -0.7310312390327454, 0.3474774956703186, -0.021243061870336533, -0.4976423978805542, -0.2041887491941452, -0.023288048803806305, 0.6961454153060913, 0.2520371079444885, 0.5696468353271484, -0.7245471477508545, 0.7180790901184082, 14.759604454040527, 0.6048856377601624, 0.1307518631219864, 1.0855530500411987, 0.467588871717453, 0.14528582990169525, -0.14559541642665863, 0.01098847109824419, -1.2341080904006958, -0.251272976398468, 0.8269563913345337, 0.24198007583618164, 0.3958751857280731, 0.27020934224128723, -0.4821748733520508, 0.21551331877708435, -0.5293405652046204, 0.5285096764564514, 0.7447458505630493, -0.9604328274726868, 0.38900643587112427, 0.1618412733078003, 0.2923282980918884, 0.8906985521316528, 0.7970585227012634, 0.7280958890914917, 1.0221335887908936, -0.37662413716316223, 0.3207567632198334, 0.6504148840904236, 0.8293352127075195, 0.3358388841152191, 0.2015342265367508, 0.46265801787376404, -0.6179613471031189, -0.1927127093076706, -0.7889475226402283, -0.9944004416465759, -0.006759136449545622, 0.09798094630241394, -0.2679539620876312, -0.5733631253242493, 0.10462465137243271, 0.4756600260734558, 0.0565837062895298, 0.3344225287437439, -0.09278474748134613, 0.14876289665699005, -0.2771535813808441, 0.0008017158834263682, 0.15288734436035156, 0.7380284070968628, 0.015051455236971378, 0.11526777595281601, 0.1941138058900833, 0.298697829246521, 0.15946410596370697, 0.7666906118392944, -0.5366725325584412, -0.13268394768238068, -0.3264324367046356, -0.07172643393278122, -0.05174104869365692, 0.9942421913146973, 1.0843526124954224, -0.02495489828288555, -0.41556939482688904, 0.28048425912857056, 0.8181648850440979, 0.19969992339611053, -0.35802826285362244, -0.1734505295753479, 0.5035932660102844, -0.1368587613105774, 0.25224611163139343, 0.5873128771781921, -0.04390377551317215, -0.4688692092895508, -0.75816810131073, 0.05244840681552887, 0.7261604070663452, -1.0411385297775269, -0.9878183603286743, 0.8294035196304321, -0.1326107531785965, -0.22509066760540009, 0.2747284173965454, -0.6717450618743896, -0.25664132833480835, 0.5858957767486572, -0.9856405258178711, -1.00365149974823, -0.1235206350684166, -0.4134587347507477, -0.17736877501010895, -0.2613425850868225, 1.3129656314849854, 0.0795566514134407, -0.13977691531181335, 0.22492270171642303, -0.37687692046165466, -0.051179222762584686, -0.25793251395225525, -0.8919382095336914, 0.6995106935501099, 0.07165876775979996, -0.01274137943983078, 0.4151308536529541, -0.1622420847415924, 0.27736538648605347, -0.4608437716960907, -0.16995865106582642, 0.8509362936019897, -1.0290570259094238, -0.63470458984375, -0.6490160822868347, -1.0069959163665771, 0.5585002899169922, 0.7234965562820435, 0.08134166896343231, 0.38572341203689575, 0.1345873326063156, -0.7107067704200745, -0.1401539295911789, -0.556083619594574, -0.09316250681877136, 0.5750722885131836, -0.7617166638374329, -0.5623058676719666, 0.05773166939616203, 0.25554659962654114, -0.7787031531333923, -0.46960219740867615, -0.33960556983947754, 0.1795797199010849, -0.02269182726740837, 1.0554441213607788, -0.5215635299682617, 0.6045393347740173, 0.6145271062850952, -0.2683540880680084, -0.8103742003440857, -0.4137808680534363, -0.7305354475975037, -0.20517498254776, 0.2726050019264221, 0.3925325870513916, -0.40050721168518066, 0.278923898935318, 1.1491533517837524, 0.1574697196483612, -0.3820808231830597, -0.617844820022583, -0.1967134028673172, 0.15412117540836334, -0.565378725528717, 0.31369003653526306, 0.06444895267486572, 0.16613510251045227, 0.23471249639987946, 0.8167291283607483, 0.35357287526130676, 0.16410741209983826, -1.0798768997192383, 0.5762406587600708, -0.4689911901950836, 0.1522022783756256, -0.7021315693855286, -0.6868798732757568, -1.4585918188095093, 0.06388065218925476, -1.1216078996658325, -0.07191990315914154, -0.9075455665588379, -0.3287429213523865, -0.10189148038625717, -0.4228335916996002, 0.10944663733243942, 0.45017290115356445, -0.3307931125164032, -0.6556618213653564, -0.5626389980316162, -0.8064779043197632, 0.6884156465530396, 0.2391156256198883, -1.0528297424316406, 0.16905662417411804, -0.20064735412597656, -0.47950324416160583, 0.4035235345363617, 0.40206873416900635, -0.6361268758773804, -0.8066505193710327, -1.5608587265014648, 0.3603861331939697, -0.5445760488510132, 0.0094296308234334, -0.7404508590698242, 1.4465771913528442, 0.5924242734909058, -0.30749741196632385, -0.19410093128681183, 0.3830074965953827, -0.9272261261940002, -0.5288018584251404, -0.06710229068994522, -0.7613911032676697, 0.21242070198059082, 0.010689576156437397, -0.42807695269584656, -0.3447173535823822, 0.6518089771270752, 0.0011478383094072342, -1.5617448091506958, -0.6600470542907715, 0.4147166311740875, -0.3579016327857971, 0.23179122805595398, -0.23718848824501038, 0.05096099153161049, -1.2223116159439087, -0.40631988644599915, -0.06360526382923126, 0.6445667743682861, -0.4029051661491394, 0.8933419585227966, 0.42759057879447937, -1.3602876663208008, 0.13121619820594788, 0.39425933361053467, 0.13802967965602875, 0.5321930050849915, 0.7854477167129517, 0.5738993883132935, -0.052289485931396484, 0.5951116681098938, 0.31323230266571045, 0.11672051250934601, -0.7700000405311584, 0.2544473707675934, 0.8863793611526489, -0.3723890483379364, -0.029949700459837914, 1.243890643119812, 0.09915872663259506, -0.9542127847671509, 0.21485114097595215, -1.1018667221069336, -0.45215359330177307, -0.21906885504722595, 0.6898356080055237, 0.1875799596309662, 0.08580666035413742, -0.15206871926784515, -0.6266540288925171, 0.13472630083560944, -0.39888229966163635, -0.5020199418067932, 0.6192205548286438, -0.02001665160059929, -0.7784443497657776, 0.40228715538978577, 0.8478639125823975, -0.9890775680541992, -0.7736055850982666, -1.2098758220672607, -0.40524908900260925, -0.11884558200836182, 0.4083254039287567, -0.17855340242385864, -0.35201331973075867, 0.8605853915214539, 0.31638526916503906, 0.6181135177612305, -0.010009082965552807, 0.16126617789268494, -0.008989629335701466, 0.6643792986869812, -0.2654178738594055, -0.7027033567428589, -0.3608457148075104, 1.398674726486206, 1.4555909633636475, -0.5058342218399048, 0.025793245062232018, -0.3152662217617035, -0.6508551239967346, 0.3630068302154541, 0.6262673139572144, -0.35888800024986267, 1.0668398141860962, -0.16070346534252167, -0.125792995095253, -0.20189955830574036, -1.1861560344696045, -0.5446801781654358, 0.9740301370620728, 1.4061533212661743, 0.5753357410430908, 0.025967001914978027, 0.4765613079071045, 0.7925550937652588, 0.377549946308136, -0.05833024904131889, 0.11728765070438385, 0.16614291071891785, -0.14588378369808197, 0.17571771144866943, 0.3098578155040741, 0.67466801404953, -0.6536264419555664, -0.5227627158164978, -0.13797542452812195, 0.3928319811820984, 0.3743717074394226, 0.3057723641395569, 1.1465855836868286, -0.040630489587783813, 0.8243171572685242, 0.5185081362724304, 0.3545689284801483, -0.7370716333389282, 0.010069607757031918, -0.4424431622028351, -0.9357523918151855, 0.07551246136426926, -0.26094916462898254, -0.9354730844497681, 0.05786489322781563, 0.12597285211086273, 0.07045219838619232, -0.5149730443954468, 0.25465989112854004, 1.1018095016479492, 0.9611931443214417, 0.6383997201919556, -0.5590518712997437, -0.1232512816786766, -0.48820027709007263, -1.0916504859924316, 0.2965027391910553, -0.5244765877723694, -0.34079018235206604, -0.4267110228538513, 0.10324237495660782, -0.35890713334083557]}, "authors": [{"authorId": "150322732", "name": "T. Nguyen"}, {"authorId": "2116488139", "name": "Tam Nguyen"}, {"authorId": "2216446883", "name": "Long Bui"}, {"authorId": "2216306719", "name": "Hai Do"}, {"authorId": "2170125220", "name": "Duy Khuong Nguyen"}, {"authorId": "2134814752", "name": "Dung D. Le"}, {"authorId": "1401969371", "name": "Hung The Tran"}, {"authorId": "3526349", "name": "Nhat Ho"}, {"authorId": "103583159", "name": "S. Osher"}, {"authorId": "144908066", "name": "Richard Baraniuk"}], "references": [{"paperId": "48af9b314181b04edcc0b7224ffe4689036b755f", "title": "Improving Transformers with Probabilistic Attention Keys"}, {"paperId": "7a27cc0cc37931e85315ed41333f01cb6de18c02", "title": "Differentiable Subset Pruning of Transformer Heads"}, {"paperId": "37abe53ed31caa23ae833b2e67bb4aa1892e8d25", "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention"}, {"paperId": "c156b1b30e3dd9284615e5304f2fb2826c09d0ff", "title": "Learned Token Pruning for Transformers"}, {"paperId": "7d67b5cfd19927b30f07de25145f20bf95766e3c", "title": "Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation"}, {"paperId": "efbe9f591090018f78b42c84613c8afda9292fdb", "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "372cce47fa16c538946972e6a7ac8420e64000b0", "title": "Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "a50d31c082521817a1e74cae584963a63163ca70", "title": "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search"}, {"paperId": "9f0272bb258506fdc0ee7d8951593914d4f9c39d", "title": "Analyzing Individual Neurons in Pre-trained Language Models"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "61325245e98920a69b40e18c069fda0c1cf00f21", "title": "MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "b70252f0de1c7832fdd6c862af35f6667349fe55", "title": "Transformer VAE: A Hierarchical Model for Structure-Aware and Interpretable Music Representation Learning"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "3b504f939e55d567652737ef093c1087cd40689b", "title": "Analyzing Redundancy in Pretrained Transformer Models"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "94f94e8892261d0377159379ca5a166ceae19a14", "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "199ff73d2f728e997f860b62a2322823d3e3d9e8", "title": "Designing and Interpreting Probes with Control Tasks"}, {"paperId": "9d7902e834d5d1d35179962c7a5b9d16623b0d39", "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "84898960f68fa78296a102edc8ac81739f9a9408", "title": "Gaussian Transformer: A Lightweight Approach for Natural Language Inference"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "512b8ef0002e0bfd0ecb5ab17d533c1762eb9786", "title": "Set Transformer"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "a2d407962bb1f5fcd209114f5687d4c11bf9dfad", "title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations"}, {"paperId": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "a99e5bf7273da127be1fcdf4f3cb911f17550304", "title": "Probabilistic Latent Semantic Analysis"}, {"paperId": "05b22d6ec2cff81bcfbac2a6cf67bc1e9ef0f60a", "title": "Improving Transformer with an Admixture of Attention Heads"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Processing (EMNLP-IJCNLP)"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "4574d77fff19e093782178595a8988a7f3aa1969", "title": "Latent Dirichlet Allocation"}, {"paperId": null, "title": "Long Papers)"}, {"paperId": null, "title": "Explanation: As in mixed pruned model via FiAK, pruning k fraction of (k j"}, {"paperId": null, "title": "license agreement with IEEE. Restrictions apply"}]}