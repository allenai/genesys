{"paperId": "584ca135b61482fd89247113da87d784f738dbfa", "title": "Foundational Models Defining a New Era in Vision: A Survey and Outlook", "abstract": "Vision systems to see and reason about the compositional nature of visual scenes are fundamental to understanding our world. The complex relations between objects and their locations, ambiguities, and variations in the real-world environment can be better described in human language, naturally governed by grammatical rules and other modalities such as audio and depth. The models learned to bridge the gap between such modalities coupled with large-scale training data facilitate contextual reasoning, generalization, and prompt capabilities at test time. These models are referred to as foundational models. The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box, having interactive dialogues by asking questions about an image or video scene or manipulating the robot's behavior through language instructions. In this survey, we provide a comprehensive review of such emerging foundational models, including typical architecture designs to combine different modalities (vision, text, audio, etc), training objectives (contrastive, generative), pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous. We discuss the open challenges and research directions for foundational models in computer vision, including difficulties in their evaluations and benchmarking, gaps in their real-world understanding, limitations of their contextual understanding, biases, vulnerability to adversarial attacks, and interpretability issues. We review recent developments in this field, covering a wide range of applications of foundation models systematically and comprehensively. A comprehensive list of foundational models studied in this work is available at \\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.", "venue": "arXiv.org", "year": 2023, "citationCount": 56, "influentialCitationCount": 6, "openAccessPdf": {"url": "https://arxiv.org/pdf/2307.13721", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A comprehensive review of emerging foundational models in computer vision, including typical architecture designs to combine different modalities, training objectives, pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous."}, "embedding": {"model": "specter_v2", "vector": [0.5170530080795288, 0.49510136246681213, -0.23644015192985535, -0.13607855141162872, -0.09656307846307755, 0.1843462884426117, 0.730940580368042, -0.3856554627418518, -0.42680278420448303, -0.6976556777954102, 0.006983035709708929, 0.17935320734977722, 0.2022450864315033, 0.17310576140880585, -0.669183075428009, 0.17405208945274353, -0.9287081360816956, 0.04569876939058304, 0.5706855058670044, -0.4618231952190399, -0.33736923336982727, -0.44117578864097595, -1.2226628065109253, 0.34491005539894104, 0.1575103998184204, 0.532529890537262, 0.2876027226448059, 1.8243242502212524, 0.2642354667186737, 0.7382091879844666, 0.4505343735218048, -0.016364166513085365, 0.2093254178762436, 0.06349378079175949, 0.0016276543028652668, 0.1497894525527954, 0.6030644178390503, -0.857979416847229, -1.2304638624191284, 0.6861605048179626, -0.2781795859336853, 0.1371065229177475, 0.51792311668396, -0.8960689902305603, -0.567859947681427, 0.029587944969534874, 0.7801297903060913, 0.6681685447692871, 0.0931999683380127, -0.32291194796562195, 1.534639835357666, -0.9892041087150574, 0.4168410003185272, 1.8021376132965088, 0.5305448174476624, 0.9523793458938599, -0.45856013894081116, -0.09177714586257935, 0.8940991163253784, 0.15949280560016632, -0.19910301268100739, -0.25913962721824646, 0.048232827335596085, -0.26742273569107056, 1.135426640510559, -0.5850982666015625, 0.029389474540948868, 0.9986885786056519, 0.12099339812994003, 1.5137418508529663, 0.06082705035805702, -1.1588116884231567, 0.1382502168416977, -0.11311114579439163, 0.2646169364452362, 0.8276761174201965, -0.1583857536315918, 0.893305242061615, -1.0460597276687622, 0.10169161856174469, 0.6043886542320251, -0.13481470942497253, -0.23802049458026886, -0.3374535143375397, -0.33510828018188477, 0.899212121963501, 0.8872169852256775, 0.04752609133720398, -0.17656797170639038, 1.093692421913147, 0.3152291774749756, 0.08131279051303864, -0.7011076211929321, -0.0348370224237442, 0.16985906660556793, 0.8179993033409119, -0.4924289286136627, 0.3255070149898529, 0.11369519680738449, 0.8809638023376465, -0.22566920518875122, -0.055977463722229004, -0.27761581540107727, -0.1449231654405594, 1.6954900026321411, 0.39846494793891907, 0.1216614842414856, -0.9286304116249084, 0.11995643377304077, -0.7983031272888184, 0.5464644432067871, -0.6654518842697144, -0.32656335830688477, -0.08581952005624771, -0.23797975480556488, -0.4107455611228943, -0.22858473658561707, 0.36539697647094727, -1.267578125, 0.6225450038909912, -0.18875963985919952, 0.07084408402442932, 0.3371749818325043, 0.8128853440284729, 0.3409179151058197, 0.6466397643089294, 0.4151260256767273, 0.5272701978683472, 0.6629689335823059, -0.9107407927513123, -0.35679593682289124, -1.1371440887451172, 0.057808756828308105, -0.3640216886997223, 0.45204421877861023, -0.2004995495080948, -0.715024471282959, -1.505035638809204, -1.152439832687378, -0.04743150621652603, -0.2758788466453552, 0.4772082567214966, 1.3021684885025024, 0.2536216080188751, -1.0571404695510864, 0.5053929090499878, -0.4391726553440094, -0.3204275965690613, 0.39201870560646057, 0.07114536315202713, 0.19527360796928406, -0.5220869183540344, -0.8039746284484863, 0.24683520197868347, 0.12279373407363892, -0.44933104515075684, -0.9201802015304565, 0.22499334812164307, -1.3782864809036255, -0.39559102058410645, 0.1280471384525299, -0.9769757986068726, 1.532584547996521, -0.1251457780599594, -0.6832791566848755, 0.44915032386779785, -0.30648040771484375, -0.26921504735946655, 0.31193023920059204, -0.5627022981643677, -0.4433676302433014, 0.1624915897846222, 0.029867062345147133, 1.0228148698806763, 0.5116183161735535, -0.8650155067443848, -0.3660377860069275, 0.2414640188217163, 0.1365615725517273, -0.2982114553451538, 0.19660484790802002, 0.8238639831542969, -0.35697996616363525, 0.019438715651631355, 0.5927223563194275, 0.5191214084625244, -0.3494126796722412, 0.163995623588562, -0.15883922576904297, -0.7550200819969177, 1.1376858949661255, 0.10962120443582535, 0.21369905769824982, -1.06761634349823, -0.9696873426437378, -0.3235461711883545, -0.02181265503168106, -0.5637295246124268, -0.8668636083602905, 0.2736126184463501, 0.1113085150718689, 0.062024638056755066, -0.06036500260233879, -1.206896185874939, 0.1517191380262375, 0.014814968220889568, -1.0909937620162964, 0.22884413599967957, 0.465835303068161, 0.9888424873352051, -0.9009581208229065, -0.10789792984724045, 0.2595035135746002, 0.3082713186740875, -0.9006546139717102, 1.372441291809082, -0.8171236515045166, 0.1489698588848114, -0.10952388495206833, 0.023136647418141365, -0.28268373012542725, -0.3566040098667145, 0.012386404909193516, -0.1833154857158661, 0.21916507184505463, 0.29468968510627747, -0.34164249897003174, 1.5489611625671387, -0.05865909531712532, 0.6806294918060303, -0.2547484338283539, -0.3290579617023468, 0.22383850812911987, 0.31261110305786133, -0.5741444230079651, -0.31061187386512756, 0.4195725917816162, 0.10844863951206207, -0.48797571659088135, 0.0071837035939097404, 0.6517668962478638, 0.9705334305763245, -0.19001291692256927, -0.09265288710594177, 0.5476749539375305, -0.43885737657546997, 0.3595905005931854, 0.5549362897872925, 0.6732226610183716, 1.0146938562393188, -0.022227546200156212, 0.48463428020477295, 0.000413514266256243, -0.7356523871421814, -0.04662615433335304, 0.9311855435371399, 0.36447104811668396, 1.4266084432601929, 0.06352385133504868, -1.1300736665725708, -0.45575767755508423, -0.06467605382204056, 0.6659253835678101, 1.2621972560882568, 0.46662628650665283, -0.1420188993215561, -0.7557404637336731, -0.20558318495750427, -0.6776925325393677, 0.04405387490987778, -0.8360580801963806, 0.07676353305578232, -0.31986844539642334, -0.6610585451126099, 0.6931099891662598, 0.7969226241111755, 1.2868077754974365, -0.6367635726928711, -0.8797934651374817, -0.18642893433570862, 0.2854924499988556, -1.1056805849075317, -0.4949997067451477, 0.19135093688964844, -0.2951805293560028, -0.6154828071594238, 0.3033633828163147, -0.24754495918750763, 0.16084639728069305, -0.4674290120601654, 0.621433675289154, -0.3000278174877167, -0.7462243437767029, 0.7741777300834656, 0.3776875138282776, -0.8605553507804871, -0.6763497591018677, -0.4180756211280823, -0.18924851715564728, -0.06221510097384453, 0.03898792341351509, 0.45006030797958374, -0.11517459154129028, 0.36017921566963196, -0.5005444288253784, 0.036754317581653595, -0.10469917207956314, 0.2139464169740677, 0.8154094815254211, -0.39267149567604065, -0.030622463673353195, -0.7504069209098816, 0.4057713747024536, 0.49244481325149536, -0.4214130938053131, 0.08164237439632416, -0.39974841475486755, -0.5688347816467285, 0.37879878282546997, -0.8823666572570801, -0.19331058859825134, -0.2249823659658432, 0.534980058670044, -0.6452516317367554, -0.7443691492080688, -0.17206566035747528, 0.34595850110054016, 0.05314337834715843, 0.9200582504272461, 0.12012726068496704, 0.15769276022911072, 0.1931006908416748, 0.43738824129104614, -1.1831692457199097, 1.0369441509246826, -0.12656933069229126, 0.01751234009861946, 0.37762877345085144, 0.16193221509456635, -0.5203650593757629, -0.62198406457901, -0.4872967004776001, -0.3967384696006775, -0.8038161396980286, 0.4050643742084503, -0.6511086821556091, -0.6887879371643066, -0.18427406251430511, -1.4522716999053955, -0.5048570036888123, -0.10550088435411453, -0.1816929578781128, -0.9252703189849854, -0.9924175143241882, -0.8516137003898621, -0.7475534677505493, 0.19583331048488617, -0.7939466834068298, 0.4647611379623413, 0.4805091321468353, -0.1886511594057083, -0.6387607455253601, 0.17212063074111938, -0.408979207277298, 0.4830879867076874, 0.18745505809783936, 0.552496075630188, 0.147419735789299, -0.5562782883644104, -0.25629091262817383, 0.09748438745737076, 0.3317915201187134, -0.44006797671318054, 0.4525504410266876, -1.519648551940918, 0.39197543263435364, -0.29433053731918335, -0.47881650924682617, 0.7073873281478882, 0.39242586493492126, 0.04320394992828369, 0.41603681445121765, -0.1317257136106491, 0.35359036922454834, 1.225010871887207, -0.41350257396698, 0.0811980813741684, 0.3193541467189789, 0.9174020886421204, 0.3742941617965698, -0.0754714384675026, 0.3412347137928009, 0.4992484450340271, 0.07384506613016129, 0.6822286248207092, -0.19712676107883453, -0.834238588809967, -1.012176752090454, 0.7189049124717712, 0.43597647547721863, 0.07044753432273865, 0.03614155948162079, -0.990800678730011, 0.6122587323188782, -1.6928730010986328, -0.6426613926887512, 0.8274264931678772, 0.6241072416305542, -0.16245515644550323, -0.16858850419521332, -0.04491875320672989, -0.2694739103317261, 0.9081577658653259, 0.20628756284713745, -0.6637049317359924, -0.398904949426651, -0.3274514675140381, -0.030009962618350983, -0.2824428677558899, 0.6822347640991211, -0.896624743938446, 0.25722166895866394, 14.731017112731934, 0.19804765284061432, -0.12726175785064697, 0.4439724087715149, 0.6141665577888489, 0.8386607766151428, -0.09871096163988113, 0.28024977445602417, -1.2628531455993652, -0.623854398727417, 1.1343332529067993, 0.6564281582832336, 0.37841883301734924, 0.44148513674736023, -0.3432440757751465, -0.029527200385928154, -0.8329789042472839, 0.7423228025436401, 0.8015704154968262, -1.1740939617156982, 0.1453579068183899, 0.07175499945878983, 0.21112897992134094, 0.5981497764587402, 1.0735256671905518, 0.5532922148704529, 0.7023288607597351, -0.4262430667877197, 0.9867377877235413, 0.20707473158836365, 0.8256050944328308, 0.461303174495697, 0.18090234696865082, 0.49011340737342834, -0.7944716215133667, -0.2922140657901764, -0.780110239982605, -0.9991551637649536, 0.20989419519901276, -0.41435861587524414, -0.6116026043891907, -0.3932476043701172, 0.04229571297764778, 0.5357129573822021, -0.06631961464881897, 0.5307407379150391, -0.5442237854003906, 0.5071165561676025, -0.4028201401233673, 0.347419410943985, 0.1412271112203598, 1.013208031654358, 0.1891530305147171, -0.06437572836875916, -0.06964324414730072, 0.03331625461578369, 0.7080940008163452, 0.4366753101348877, -0.26674577593803406, -0.46221303939819336, -0.44682925939559937, 0.013297914527356625, -0.5520005822181702, 0.40843477845191956, 0.0715722069144249, 0.6809679269790649, -0.23735469579696655, -0.02801245078444481, 0.023296747356653214, 0.5522781014442444, -0.4272468090057373, 0.137054905295372, -0.15005704760551453, -0.4316469728946686, 0.18209189176559448, 0.376670241355896, -0.19916310906410217, -0.48409998416900635, -0.4769793748855591, -0.04975282773375511, 0.4707754850387573, -1.0354653596878052, -0.696761429309845, 1.1000950336456299, -0.021009715273976326, -0.21084076166152954, 0.2570625841617584, -1.2014119625091553, -0.46350008249282837, -0.12744638323783875, -1.644957184791565, -1.2542122602462769, -0.11383788287639618, 0.1396883875131607, 0.047476138919591904, -0.15215928852558136, 1.248039722442627, -0.42196735739707947, 0.15589703619480133, -0.2031412273645401, -0.6241342425346375, 0.06513910740613937, -0.08732156455516815, -1.058954119682312, 0.7142640948295593, 0.5344537496566772, 0.3016849458217621, 0.09467905014753342, 0.4833223223686218, 0.2950582206249237, -0.5371230840682983, 0.20111604034900665, 0.13491494953632355, -1.0368903875350952, -0.5945205688476562, -0.4172631502151489, -0.39547398686408997, 0.30580267310142517, 0.47488176822662354, 0.25994986295700073, 0.09021726995706558, 0.13401912152767181, -1.0740009546279907, -0.1172344908118248, -0.8258931636810303, -0.030074508860707283, 0.22750669717788696, -0.6654545068740845, -0.6177666187286377, -0.14383141696453094, 0.4888535737991333, -0.9990477561950684, 0.03809181973338127, -0.04606567695736885, 0.20447157323360443, -0.5861184597015381, 0.9160745739936829, -0.3578208386898041, 0.6686182022094727, 0.2720368504524231, -0.7169471979141235, -0.4244353771209717, 0.1369321197271347, -1.121618628501892, -0.08610476553440094, -0.22230809926986694, 0.37259483337402344, -0.28898563981056213, -0.03919067606329918, 0.9092428684234619, 0.2036159783601761, -0.2668852210044861, -0.4925509989261627, -0.19726616144180298, -0.21173392236232758, -0.709322452545166, 0.06221039593219757, -0.8508289456367493, -0.5620721578598022, 0.16738437116146088, 0.4509710967540741, 1.0341459512710571, 0.04340706765651703, -0.3212003707885742, 0.8332453966140747, -0.041963525116443634, -0.668860912322998, -0.014110658317804337, -0.9139634370803833, -1.7518107891082764, -0.17186471819877625, -0.5496672987937927, 0.46509960293769836, -1.0875083208084106, -0.6854283213615417, 0.09073422849178314, -0.2870343327522278, -0.19754663109779358, 0.6284119486808777, -0.1379263550043106, 0.14214563369750977, -0.30449387431144714, -0.7518229484558105, 0.6449559330940247, 1.0987293720245361, -0.7739282250404358, 0.004921076353639364, 0.07060687243938446, -0.016229476779699326, 0.8536797761917114, 0.13387612998485565, -0.3273293077945709, -0.8262940049171448, -0.9821316003799438, 0.3370884954929352, -0.21515746414661407, 0.6404396891593933, -1.0640418529510498, 1.0802500247955322, 0.17184342443943024, 0.21422991156578064, 0.05860179662704468, 0.9643085598945618, -1.0056428909301758, -0.7491710186004639, 0.26576510071754456, -0.859878420829773, -0.11196461319923401, 0.1555202305316925, 0.06608270108699799, -0.3669206202030182, 0.7223848700523376, -0.09076794236898422, -0.7453773021697998, -1.0071971416473389, 0.102204330265522, -0.03745514899492264, -0.2127467393875122, -0.15012586116790771, -0.05899452045559883, -1.4382388591766357, -0.4389859437942505, -0.4758891761302948, 0.2661520540714264, -0.3296034336090088, 0.630386233329773, 1.0149693489074707, -0.9478415846824646, 0.0641990602016449, 0.3569835126399994, -0.07207750529050827, 0.6188257336616516, 0.5672836899757385, 0.07464727014303207, -0.4304126799106598, 0.41312915086746216, 0.1282234638929367, -0.09494762122631073, -0.5629090070724487, 0.12063843011856079, 0.7529935836791992, -0.03595009446144104, -0.48498934507369995, 1.1388139724731445, 0.3153873383998871, -0.6309760808944702, 0.3804793357849121, -0.9198139309883118, -0.38313359022140503, -0.6431134343147278, 0.482791543006897, -0.15407390892505646, -0.5633554458618164, -0.20410534739494324, -0.31311434507369995, 0.6423915028572083, -0.2240648716688156, -0.7083253860473633, 0.36549705266952515, -0.14028026163578033, 0.11225590109825134, 0.8004034757614136, 0.24304208159446716, -0.7337026000022888, -1.6123560667037964, -0.5803136825561523, -0.562911331653595, -0.3337620496749878, -0.08093196153640747, -0.4977820813655853, -0.23064808547496796, 1.0626503229141235, 0.5802494883537292, 0.3733913004398346, 0.33692046999931335, 0.25534841418266296, 0.04182335361838341, 0.729015588760376, 0.1805887669324875, -0.435233473777771, -0.11989046633243561, 0.9930500984191895, 1.4384355545043945, -0.7886124849319458, 0.3631250858306885, -0.5407384037971497, -0.4948081076145172, 1.2026747465133667, 0.5523430705070496, -0.7701963782310486, 0.767302930355072, -0.41236039996147156, 0.23776023089885712, 0.2765297591686249, -0.9045215249061584, -0.08126252889633179, 0.7630742192268372, 1.5794786214828491, -0.11665695905685425, 0.2258831262588501, 0.7004263997077942, 0.8620960712432861, 0.020893940702080727, -0.04642094671726227, 0.6583471894264221, 0.4326593279838562, -0.22869223356246948, 0.16092301905155182, -0.22175119817256927, -0.14123165607452393, -0.44337204098701477, 0.06327689439058304, 0.19537584483623505, 0.9566376209259033, 0.41358059644699097, 0.8967669010162354, 0.4601183533668518, -0.0294171292334795, 0.5019329190254211, -0.14107471704483032, 0.9571495652198792, -0.6308159828186035, 0.13778232038021088, 0.2607567310333252, -0.7249488830566406, -0.08252738416194916, -0.6926278471946716, -0.8797162175178528, -0.6329783797264099, 0.3775668144226074, 0.20646145939826965, -0.5595260262489319, 0.509968638420105, 0.8109588027000427, 0.31454554200172424, 0.5415450930595398, -0.4000781178474426, -0.5894078612327576, -0.3220626711845398, -0.6931654214859009, 0.7041316628456116, -0.6691927313804626, 0.3591731786727905, -0.6526837348937988, -0.43807461857795715, 0.5043597221374512]}, "authors": [{"authorId": "144987292", "name": "Muhammad Awais"}, {"authorId": "40894826", "name": "Muzammal Naseer"}, {"authorId": "2111180748", "name": "Salman Siddique Khan"}, {"authorId": "3288214", "name": "R. Anwer"}, {"authorId": "2951229", "name": "Hisham Cholakkal"}, {"authorId": "2111863589", "name": "M. Shah"}, {"authorId": "152790163", "name": "Ming Yang"}, {"authorId": "2358803", "name": "F. Khan"}], "references": [{"paperId": "962ccf1fc49c83817fb031e5b24b81b19cdfb89d", "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "094883e42bb9a41f602c0715c1059bc431e33fb2", "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"}, {"paperId": "587352c3b95c90de6d37f061c8e117f42be0b575", "title": "Building Cooperative Embodied Agents Modularly with Large Language Models"}, {"paperId": "df710c46594c04fb59ef9a93d3b4e1cb387a1b2b", "title": "Embodied Task Planning with Large Language Models"}, {"paperId": "d1500f1dbd62e26ef0753f31e845078f58479968", "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners"}, {"paperId": "725b762ba2c18d2cbd621b942ba544d82faa1375", "title": "Segment Anything Meets Point Tracking"}, {"paperId": "d8156b4043c2eeba96d52fd31d215c607c860fb1", "title": "RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation"}, {"paperId": "ebdb59f41d6cf5146fcc6bbf7784e0595338ddc7", "title": "SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation"}, {"paperId": "03251361c1d67c6b5badffc7059fdd7fbfea1fed", "title": "Statler: State-Maintaining Language Models for Embodied Reasoning"}, {"paperId": "a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3", "title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"}, {"paperId": "37defa9aa690c033f9c4d1510df32ae5e996f716", "title": "RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation Based on Visual Foundation Model"}, {"paperId": "145be18b3e2e1baf3fed09e919da68da6e14a839", "title": "Stone Needle: A General Multimodal Large-scale Model Framework towards Healthcare"}, {"paperId": "efc694164312006c543ef745611348ef64e68dda", "title": "Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"}, {"paperId": "3b6179c293df29e31d31cea46476f104ab6950f2", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World"}, {"paperId": "8724579d3f126e753a0451d98ff57b165f722e72", "title": "Are aligned neural networks adversarially aligned?"}, {"paperId": "fdaa21fce5ee5aadf1bb8c55d5f7c3155b905846", "title": "Faster Segment Anything: Towards Lightweight SAM for Mobile Applications"}, {"paperId": "c4a9b08b89354f068cf34b1ef641a979cd70a1d7", "title": "3DSAM-adapter: Holistic Adaptation of SAM from 2D to 3D for Promptable Medical Image Segmentation"}, {"paperId": "dc2b36fb20490c0d540e01e74efd868d2e17faa3", "title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models"}, {"paperId": "4ecdeb22bf74bdf57250dc7057c509890780ddb1", "title": "SoftGPT: Learn Goal-Oriented Soft Object Manipulation Skills by Generative Pre-Trained Heterogeneous Graph Transformer"}, {"paperId": "f3a8d7ae2ea571ca879368167ab51b71cfa423bf", "title": "TaCA: Upgrading Your Visual Foundation Model with Task-agnostic Compatible Adapter"}, {"paperId": "c01e94a36be5b3578fedb17205205b330290a778", "title": "Fast Segment Anything"}, {"paperId": "b01dd81fc103c7864a61f80085cb33bf69d90eb1", "title": "VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution"}, {"paperId": "0aaee2b99ec6f659657658416e88ad7f4161ac7f", "title": "Mass-Producing Failures of Multimodal Systems with Language Models"}, {"paperId": "ef5c3052dcff67abd9bbb699e637847d1bf2120c", "title": "SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling"}, {"paperId": "d92c797f587ce7f1b001920ab9e6b7d31960bd77", "title": "RemoteCLIP: A Vision Language Foundation Model for Remote Sensing"}, {"paperId": "66b7272b9ae1fd3f7cd66b2a5c69e43a29b2660f", "title": "CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents"}, {"paperId": "0983883619a0ca597d055d0e58da2f514052913d", "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"}, {"paperId": "1474d4248e1cbcc91183456cdf1e7272e8a931de", "title": "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model"}, {"paperId": "08706687b02ae65e2303011d0013e96b79007d2c", "title": "Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models"}, {"paperId": "3e5017606b8d05ba9918c508d042089dc866ca62", "title": "Neural World Models for Computer Vision"}, {"paperId": "29cd4e8504df8c762b0b6eef8299584118feeb88", "title": "Image Captioners Are Scalable Vision Learners Too"}, {"paperId": "16b42fc85f4c073aa00c410cbdce965d7c6f8d4d", "title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning"}, {"paperId": "66d7d8dc54ea3dff10a11df2f29dc2104df86a57", "title": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models"}, {"paperId": "4c4d176c6e28f48041f215d563f6ee8633534cff", "title": "Valley: Video Assistant with Large Language model Enhanced abilitY"}, {"paperId": "763c16a74ea7a93ad148421d91cdad07cf575561", "title": "AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "cc78babfacce48e715dac56886d7dd9746cfcab0", "title": "RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit"}, {"paperId": "bf7025a2e5dbb3c09deae02a1aa98a256ca559e2", "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"}, {"paperId": "d47524cd5c3c4b57af2e5a29f6f91c420310f236", "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning"}, {"paperId": "9541cf136f442e992f10021c53081f33c73a2ed0", "title": "Recognize Anything: A Strong Image Tagging Model"}, {"paperId": "66e61357bf464bbfcf27b744a3dbb9cdc4f34d31", "title": "DeSAM: Decoupled Segment Anything Model for Generalizable Medical Image Segmentation"}, {"paperId": "5fb7afae5fcacae1d40f109a348b43e00aa5d486", "title": "Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models"}, {"paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb", "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models"}, {"paperId": "0c45810f5afe30d6c777390505e3d0a9ef63f9af", "title": "On the Robustness of Segment Anything"}, {"paperId": "3dbbe6909d7b53dd49e059c7f61a3613045a8db0", "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation"}, {"paperId": "00cb69a9f280317d1c59ac5827551ee9b10642b8", "title": "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"}, {"paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0", "title": "Adversarial Demonstration Attacks on Large Language Models"}, {"paperId": "08b562aa8066c2342f0d03824221dea18f0a18d2", "title": "Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "4f480bae3196dbbc27ab383bce33478ea963f9b3", "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models"}, {"paperId": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773", "title": "Evaluating Object Hallucination in Large Vision-Language Models"}, {"paperId": "1856bebc4cb35e68368d9c83bd2ac2d26cd4bcfa", "title": "A Comprehensive Survey on Segment Anything Model for Vision and Beyond"}, {"paperId": "9d7cbf4c81da9058f6349b5170f67dcc798597c9", "title": "ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter"}, {"paperId": "a9fbf441bd074b6daeae0903a040aefaab61e757", "title": "An Inverse Scaling Law for CLIP Training"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "bbdc4118df106d4ba7af9d7d94d7f0a1144c11e2", "title": "Segment and Track Anything"}, {"paperId": "7dc6da87eaa6f830354feb2db14023cab8678c91", "title": "ImageBind One Embedding Space to Bind Them All"}, {"paperId": "e7a4e987dc250ac6a016ee2011bc7a552cfa8e8a", "title": "TidyBot: Personalized Robot Assistance with Large Language Models"}, {"paperId": "9f9b96e48670477640c12930fca69fee8afcfaff", "title": "Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models"}, {"paperId": "6f8b9192b1f215254ee7625d752710182c05d2f9", "title": "Caption Anything: Interactive Image Description with Diverse Multimodal Controls"}, {"paperId": "0046306876ff2d5600699327e52bc29fa5e9ec91", "title": "Transfer Visual Prompt Generator across LLMs"}, {"paperId": "570079bbdd8758dfe865097e05719313c9c1301a", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"}, {"paperId": "f9570989919338079088270a9cf1a7afc8db8093", "title": "DataComp: In search of the next generation of multimodal datasets"}, {"paperId": "c56a51728678e5b2e3ff95e51caf21d267439c36", "title": "ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System"}, {"paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"}, {"paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"}, {"paperId": "5af106fb1cdeda43e369016ab5ece11f450ba485", "title": "Multimodal Grounding for Embodied AI via Augmented Reality Headsets for Natural Language Driven Task Planning"}, {"paperId": "564dbc73a0099f2aac8f9edcda4f98aaa2cddb14", "title": "Learnable Ophthalmology SAM"}, {"paperId": "49f9882d5fd442f02f9c9dff780336f6dce2da4f", "title": "Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation"}, {"paperId": "c0cf0971c153a84bbf0729d289b3b960969bb5dd", "title": "Track Anything: Segment Anything Meets Videos"}, {"paperId": "88abef771472c3aa46c53d5d626a0d0c3b66e8cd", "title": "Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness"}, {"paperId": "7a12fe8bb115dfbf6c9ee889ecf005be5d6f555c", "title": "Can GPT-4 Perform Neural Architecture Search?"}, {"paperId": "798868aa3d1cb0f83a3f97265a8e0d17d649e1de", "title": "Robot-Enabled Construction Assembly with Automated Sequence Planning based on ChatGPT: RoboGPT"}, {"paperId": "4c8ef2db0c77aba453783f5211ebafc6695d3835", "title": "ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"}, {"paperId": "f603b9dc81dfea56d437e967b724636d4d72d000", "title": "LLM as A Robotic Brain: Unifying Egocentric Memory and Control"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "df958800014d310b6df34ad83d771314d68fbb2d", "title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text"}, {"paperId": "0819c1e60c13b9797f937282d06b54d252d9d6ec", "title": "Segment Everything Everywhere All at Once"}, {"paperId": "2ba2a875161b6f09815817542f02f1ac9171952a", "title": "What does CLIP know about a red circle? Visual prompt engineering for VLMs"}, {"paperId": "acff3604e6065bf48fedcd95da90ca74f66733c7", "title": "SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM"}, {"paperId": "23e544ee04e725a85fa8ff239e2954f3688fd68b", "title": "Advancing Medical Imaging with Language Models: A Journey from N-grams to ChatGPT"}, {"paperId": "0d502a1e300336ae628f5c8b99ee4d3766c8f60b", "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"}, {"paperId": "38179848e2d6a3ad373b1793848816111428ac36", "title": "OpenAGI: When LLM Meets Domain Experts"}, {"paperId": "0ebc861f5478561f12941e6b48aad30574e996d8", "title": "Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"}, {"paperId": "7a84a26647892daa9d10dbfc97c0382619ac2f4d", "title": "ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application"}, {"paperId": "90af7c6cdf4b3359f6d275afb436f54f60082364", "title": "SegGPT: Segmenting Everything In Context"}, {"paperId": "83bfe9264c46d656a0d1c7dd86cb2ccd254d9d60", "title": "ERRA: An Embodied Representation and Reasoning Architecture for Long-Horizon Language-Conditioned Manipulation Tasks"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "690df0820f35a47e1ce44f90e6ddb4132aa09267", "title": "Vision-Language Models for Vision Tasks: A Survey"}, {"paperId": "07f07d4d59fdbc3596284f51057cb006779d42c1", "title": "A Bibliometric Review of Large Language Models Research from 2017 to 2023"}, {"paperId": "9faa2b0e5cb93f20df0555c3c350fab0b2eccf3a", "title": "Foundation models for generalist medical artificial intelligence"}, {"paperId": "84b6fecf016d74512869c698c66c83729abdf359", "title": "Self-Supervised Multimodal Learning: A Survey"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "380a4d6873736427886cd00ed9c137abe97b8da9", "title": "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding"}, {"paperId": "a757999ed260d7bc45484dc6b4456bf33fe6f679", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"}, {"paperId": "a08b7123a7158f1a7fbbc18e8b5aaebd47980ecf", "title": "EVA-CLIP: Improved Training Techniques for CLIP at Scale"}, {"paperId": "06e5828341aa3926e1d839039363b0673b9461cc", "title": "Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting"}, {"paperId": "574beee702be3856d60aa482ec725168fe64fc99", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"}, {"paperId": "af6db7ae134ebad3fc12c34ff3a3c2139aa97bd8", "title": "Detecting Everything in the Open World: Towards Universal Object Detection"}, {"paperId": "3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1", "title": "EVA-02: A Visual Representation for Neon Genesis"}, {"paperId": "c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4", "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"}, {"paperId": "c5b2243baf88a00db2d4e4f9edb33cde08eb153f", "title": "eP-ALM: Efficient Perceptual Augmentation of Language Models"}, {"paperId": "00a48c76e123ab77f301bf4dfd88b9b376b234c6", "title": "Chat with the Environment: Interactive Multimodal Perception Using Large Language Models"}, {"paperId": "f0ff5c29ee9e4eeec6b72d38871b67e08993b617", "title": "MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "2ce8331b0d85e8c336811a7692e8b105f6a9e373", "title": "Can Large Language Models design a Robot?"}, {"paperId": "6e754273d54a91371efbc928cd6b156364d517da", "title": "ViperGPT: Visual Inference via Python Execution for Reasoning"}, {"paperId": "35ccd924de9e8483bdcf144cbf2edf09be157b7e", "title": "Text-to-image Diffusion Models in Generative AI: A Survey"}, {"paperId": "69cfdc8df16ae63b7acba4ac6f727f78b86893c3", "title": "ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions"}, {"paperId": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0", "title": "Task and Motion Planning with Large Language Models for Object Rearrangement"}, {"paperId": "c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0", "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"}, {"paperId": "af997821231898a5f8d0fd78dad4eec526acabe5", "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"}, {"paperId": "323400245885e08ad498cd108e30e18020662278", "title": "Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "f02d56e630986997e0aea3d92bf53e0f363ce401", "title": "Prismer: A Vision-Language Model with Multi-Task Experts"}, {"paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c", "title": "Language Is Not All You Need: Aligning Perception with Language Models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "3599a236f285af48782fc30b1341d13ec7320735", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"}, {"paperId": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050", "title": "Multimodal Chain-of-Thought Reasoning in Language Models"}, {"paperId": "9348656b761f7b76fb65cfe6fac55386b04a3a8a", "title": "A Comprehensive Survey of Continual Learning: Theory, Method and Application"}, {"paperId": "6173520a1eb2814d067e8c5fd16212b7cbf6ee78", "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "74e7edf7c6436bb4992ca9c9df9476c9ccf31919", "title": "Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks"}, {"paperId": "967907503b24423b9b74621051811fcf684e3957", "title": "Generalized Decoding for Pixel, Image, and Language"}, {"paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45", "title": "Towards Reasoning in Large Language Models: A Survey"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "16de2006e2960ba410772c6b6d460b83c0a5cc4b", "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning"}, {"paperId": "9ceaeff7117965832f4c05fd6355d021862d0a82", "title": "Images Speak in Images: A Generalist Painter for In-Context Visual Learning"}, {"paperId": "cfca7eedc6ede9d363d1662280a74d78dcdc9d4a", "title": "Scaling Language-Image Pre-Training via Masking"}, {"paperId": "af1c871282ec122869d03f5420ef5d9143358a91", "title": "Visual Programming: Compositional visual reasoning without training"}, {"paperId": "9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6", "title": "Ignore Previous Prompt: Attack Techniques For Language Models"}, {"paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"}, {"paperId": "01c630e9ce06040b0f3f540dd3f7681f01e84a9d", "title": "Unifying Flow, Stereo and Depth Estimation"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "67299867b0c0e14bc947fc7f10cbfb6e339118a1", "title": "Decoupling Features in Hierarchical Propagation for Video Object Segmentation"}, {"paperId": "b287a2765e5bceb732de39dafdf70594dc9cd664", "title": "Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"}, {"paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"}, {"paperId": "0e34addae55a571d7efd3a5e2543e86dd7d41a83", "title": "Interactive Language: Talking to Robots in Real Time"}, {"paperId": "07099fe26ee8850c9ccba6fe2ee139d67289b67c", "title": "Foundation Transformers"}, {"paperId": "25425e299101b13ec2872417a14f961f4f8aa18e", "title": "VIMA: General Robot Manipulation with Multimodal Prompts"}, {"paperId": "d3135733aa39dec20ce72aa138589dda27c8406d", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "efa1647594b236361610a20d507127f0586a379b", "title": "Diffusion Models in Vision: A Survey"}, {"paperId": "e342165a614588878ad0f4bc9bacf3905df34d08", "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications"}, {"paperId": "1114863be2a713a14771ccacb5c9436fb4a375e2", "title": "MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining"}, {"paperId": "02251886950770e82b3d68564d60cdfe15e73199", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"}, {"paperId": "599be9043ef3571f65758cf36e184c9dc1781baf", "title": "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers"}, {"paperId": "620369d6ed3ed68c3e4374d6ddf282e0b036d2f8", "title": "Masked Vision and Language Modeling for Multi-modal Representation Learning"}, {"paperId": "01724c36660359545e1368fc80c99f4bde44a190", "title": "XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model"}, {"paperId": "cdf54c147434c83a4a380916b6c1279b0ca19fc2", "title": "LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"}, {"paperId": "4c2668b3ae22fa592716480ec56012775b139f52", "title": "Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning"}, {"paperId": "32c9b3859086d15184989454eb878638659e64c6", "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge"}, {"paperId": "4c559d29e19f1226353f52ffe9f8068db1cef943", "title": "Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone"}, {"paperId": "06761cb27e14aa55a6c3d98b949898aa26416698", "title": "A Unified Sequence Interface for Vision Tasks"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "622428f5122ad12a40229e1768ecb929fd747ee7", "title": "Multimodal Learning With Transformers: A Survey"}, {"paperId": "a8fd9c1625011741f74401ff9bdc1c584e25c86d", "title": "Language Models are General-Purpose Interfaces"}, {"paperId": "49b5ffebdbcbd683010a2558a19eaa9b21cd8c34", "title": "GLIPv2: Unifying Localization and Vision-Language Understanding"}, {"paperId": "47a67e76ed84260ff19f7a948d764005d1edf1c9", "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"}, {"paperId": "60ee030773ba1b68eb222a265b052ca028353362", "title": "GIT: A Generative Image-to-text Transformer for Vision and Language"}, {"paperId": "9dae204dad41633188022002a04c8aa67c79a4e1", "title": "Simple Open-Vocabulary Object Detection with Vision Transformers"}, {"paperId": "a26a7a74f1e5fd562be95c3611a0680759fbdf84", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "66ee488cf3dad5bb83804124367460edddd3c271", "title": "Vision-and-Language Pretrained Models: A Survey"}, {"paperId": "6cd66bafd46f027c43519c880c0e47e30d72b1c3", "title": "Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories"}, {"paperId": "5e76879aaea118b532fb24a50b721076d4c6ae93", "title": "Unified Contrastive Learning in Image-Text-Label Space"}, {"paperId": "259c681c76335540e13081efad584efdf9101868", "title": "DaViT: Dual Attention Vision Transformers"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "624eb5680f4198f2c1fd7afdf0f981bc810a1ba7", "title": "Video Polyp Segmentation: A Deep Learning Perspective"}, {"paperId": "fa717a2e31f0cef4e26921f3b147a98644d2e64c", "title": "Focal Modulation Networks"}, {"paperId": "32db2d409384575aeae453acc45220b51fe96301", "title": "Unsupervised Domain Adaptation for Nighttime Aerial Tracking"}, {"paperId": "0b5f27a5766c5d1394a6282ad94fec21d620bd6b", "title": "GroupViT: Semantic Segmentation Emerges from Text Supervision"}, {"paperId": "04248a087a834af24bfe001c9fc9ea28dab63c26", "title": "A Survey of Vision-Language Pre-Trained Models"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "e9581d9758062f76e029bd19a58c4ae976cfb414", "title": "SLIP: Self-supervision meets Language-Image Pre-training"}, {"paperId": "8d737dc6a91a7bfde20aed7bb13d100476de5ae3", "title": "Scaling Open-Vocabulary Image Segmentation with Image-Level Labels"}, {"paperId": "f427ccb1d97cee3fde8abf0f5442f859531f5bf1", "title": "Mask2Former for Video Instance Segmentation"}, {"paperId": "e77c484af99fc1eb3d3c36699ac81822e98cb74d", "title": "Image Segmentation Using Text and Image Prompts"}, {"paperId": "837173ef1f260adc0d50b76675915776e1cc8ade", "title": "RegionCLIP: Region-based Language-Image Pretraining"}, {"paperId": "2fd6f77540c1cc8e70b96208ccf9971b4251fc02", "title": "FLAVA: A Foundational Language And Vision Alignment Model"}, {"paperId": "5341b412383c43f4a693ad63ec4489e3ec7688c8", "title": "Grounded Language-Image Pre-training"}, {"paperId": "0a7e7347e16bf13d710f6f3d30748baabdbb96ad", "title": "Extract Free Dense Labels from CLIP"}, {"paperId": "2299c08033af3e2f7d1f6a958aadb15f10ddd0ef", "title": "Object-aware Video-language Pre-training for Retrieval"}, {"paperId": "76a2b197b5427ffd1d3470c6d3ea026588eb5d0a", "title": "CRIS: CLIP-Driven Referring Image Segmentation"}, {"paperId": "21ec90872abd986c12afe39bebe807732ffa70c9", "title": "Florence: A New Foundation Model for Computer Vision"}, {"paperId": "da4261a957eaa96bf626e9641ef68ebed1d5333f", "title": "RedCaps: web-curated image-text data created by the people, for the people"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "f675c62abfa788ea0be85d3124eba15a14d5e9d6", "title": "FILIP: Fine-grained Interactive Language-Image Pre-Training"}, {"paperId": "94ff111c4d81bd03f159321728ceec8b4711c89d", "title": "An Empirical Study of Training End-to-End Vision-and-Language Transformers"}, {"paperId": "b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df", "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"}, {"paperId": "9bcf3b43f2323a194036cc52c6878a9b1dc7e058", "title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning"}, {"paperId": "f3a332ff1b73acda482e5d83696b2c701f487819", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"}, {"paperId": "848eb8367785910c2fe31372605954ad8f9dfe6c", "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video"}, {"paperId": "2fa558f583322888f786734eb89b6a1ec34b8459", "title": "SAR Ship Detection Dataset (SSDD): Official Release and Comprehensive Data Analysis"}, {"paperId": "12ce370b38cc69403e81980f33b413650900105c", "title": "Multi-Task Self-Training for Learning General Representations"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "ee1e4158395060077c12174b85abdf18ee9aaa0b", "title": "LLVIP: A Visible-infrared Paired Dataset for Low-light Vision"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "89200d67ab3dd3b1dad7dc544abb3cc6cbff5e3c", "title": "The MineRL BASALT Competition on Learning from Human Feedback"}, {"paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"}, {"paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d", "title": "Multimodal Few-Shot Learning with Frozen Language Models"}, {"paperId": "fda4530df9eec0e3f714dba3459ac50dab17d89c", "title": "Audioclip: Extending Clip to Image, Text and Audio"}, {"paperId": "c401e01c9ee32fab7d02670d1c754f44fc1ff99e", "title": "CLIP2Video: Mastering Video-Text Retrieval via Image CLIP"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "979a9f247700d00ff2c3f0612d5eb001379f93c8", "title": "The Medical Segmentation Decathlon"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35", "title": "Emerging Properties in Self-Supervised Vision Transformers"}, {"paperId": "cf9b8da26d9b92e75ba49616ed2a1033f59fce14", "title": "Open-vocabulary Object Detection via Vision and Language Knowledge Distillation"}, {"paperId": "7ba9c013988eaff5cd186d73704af329d027872d", "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "57076b4306819203521d14dfad08693c2f5452f7", "title": "Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation"}, {"paperId": "bac87bdb1cabc35fafb8176a234d332ebcc02864", "title": "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval"}, {"paperId": "0b6f13177a90a02d44a41c62659988561f56c168", "title": "WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training"}, {"paperId": "98e565fa06f6c7bf7c46833b5106b26dc45130c4", "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "394be105b87e9bfe72c20efe6338de10604e1a11", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"}, {"paperId": "141a5033d9994242b18bb3b217e79582f1ee9306", "title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"}, {"paperId": "c16835c8e535ebd9c10a550ca9455fe384a14449", "title": "High-Performance Large-Scale Image Recognition Without Normalization"}, {"paperId": "cb596bffc5c5042c254058b62317a57fa156fea4", "title": "Unifying Vision-and-Language Tasks via Text Generation"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "0fd47bf484a05001ce787747cf5a879b9202ebfa", "title": "ViNG: Learning Open-World Navigation with Visual Goals"}, {"paperId": "62886cb3d09c247754ea2a11cc7dbe18eed7f563", "title": "Contrastive Learning of Relative Position Regression for One-Shot Object Localization in 3D Medical Images"}, {"paperId": "1ce28b6d15661e327c5bacd7dd89aae8e6985527", "title": "Just Ask: Learning to Answer Questions from Millions of Narrated Videos"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "0593d3da080f886fa020541a1e1c675f4fdd37c6", "title": "On Robustness and Transferability of Convolutional Neural Networks"}, {"paperId": "022622e024890d6e044ac50e2da6b44c59bdf418", "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization"}, {"paperId": "3e7f5f4382ac6f9c4fef6197dd21abf74456acd1", "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners"}, {"paperId": "a1ebc58f6c055eac8555d9850cb849a38c22c67d", "title": "PhraseCut: Language-Based Image Segmentation in the Wild"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "3f6570fd55dc5855f93a56150e6d99c7944a1c1e", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes"}, {"paperId": "87c0dd990287d92796c7dc83edba6f52a2f52e21", "title": "A Multi-Organ Nucleus Segmentation Challenge"}, {"paperId": "33eadd4e666a894306a22ba0839c5e0cef77280e", "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "439369de9514e41e0f03fed552d8f6e5aebf51b2", "title": "Connecting Vision and Language with Localized Narratives"}, {"paperId": "4d55dfb3db28064d0313332fce937e036a2f72a3", "title": "The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 Challenge"}, {"paperId": "349461d60b4d3d34dc97147e4b9ec2b9bd611be8", "title": "Kvasir-SEG: A Segmented Polyp Dataset"}, {"paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc", "title": "Momentum Contrast for Unsupervised Visual Representation Learning"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c5ff974a69fd0c760b4855b819e61e89f31cfffe", "title": "Objects365: A Large-Scale, High-Quality Dataset for Object Detection"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "0c55b3a00349e8e8c38ae3f7f82405342df03b2a", "title": "HarDNet: A Low Memory Traffic Network"}, {"paperId": "1097cf8cf5961589ff693b069002e7181e24e631", "title": "OCR-VQA: Visual Question Answering by Reading Text in Images"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "65a9c7b0800c86a196bc14e7621ff895cc6ab287", "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f902a64f7d08aaa6bfca7463e8729952ddc6134e", "title": "LVIS: A Dataset for Large Vocabulary Instance Segmentation"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907", "title": "Towards VQA Models That Can Read"}, {"paperId": "69455376f5ad52cac5b72d5e8c6cf03fb466b55c", "title": "Cross-Modal Self-Attention Network for Referring Image Segmentation"}, {"paperId": "28b74bb7c8b08cceb2430ec2d54dfa0f3225d796", "title": "VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"}, {"paperId": "653d92ca61d77a906eabb880f40cac12f6f1dc12", "title": "YOLACT: Real-Time Instance Segmentation"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "889c81b4d7b7ed43a3f69f880ea60b0572e02e27", "title": "Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression"}, {"paperId": "4e0bb8c1c683b43357c5d5216f6b74ff2cb32434", "title": "Do ImageNet Classifiers Generalize to ImageNet?"}, {"paperId": "0655dcaa39cf41a3609974840f91300d73b4aed1", "title": "The Liver Tumor Segmentation Benchmark (LiTS)"}, {"paperId": "53edb3776cea0e6ee4de742d7bb906355fd8279b", "title": "Fully Convolutional Networks for Multisource Building Extraction From an Open Aerial and Satellite Imagery Data Set"}, {"paperId": "807257e0a934d3600db9d9b4a12f7194ac8ae2c2", "title": "Classification is a Strong Baseline for Deep Metric Learning"}, {"paperId": "7e27d44e3fac723ccb703e0a83b22711bd42efe8", "title": "A Comprehensive Survey of Deep Learning for Image Captioning"}, {"paperId": "c7798bbbcd2e85593a2b2d37b92dabac63420cd8", "title": "Deep Retinex Decomposition for Low-Light Enhancement"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "ebf35073e122782f685a0d6c231622412f28a53b", "title": "A High-Quality Denoising Dataset for Smartphone Cameras"}, {"paperId": "0f885fd46064d271d4404cf9bb3d758e1a6f8d55", "title": "Exploring the Limits of Weakly Supervised Pretraining"}, {"paperId": "da30d00c9490768e7726725482e3ecbd102f18cd", "title": "Video Object Segmentation with Language Referring Expressions"}, {"paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"}, {"paperId": "9ea50b3408f993853f1c5e374690e5fbe73c2a3c", "title": "Continual Lifelong Learning with Neural Networks: A Review"}, {"paperId": "5b01eaef54a653ba03ddd5a978690380fbc19bfc", "title": "Diversity is All You Need: Learning Skills without a Reward Function"}, {"paperId": "2064020586d5832b55f80a7dffea1fd90a5d94dd", "title": "Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents"}, {"paperId": "057b80e235b10799d03876ad25465208a4c64caf", "title": "Video Question Answering via Gradually Refined Attention over Appearance and Motion"}, {"paperId": "1a857da1a8ce47b2aa185b91b5cb215ddef24de7", "title": "Focal Loss for Dense Object Detection"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "5ba2218b708ca64ab556e39d5997202e012717d5", "title": "Audio Set: An ontology and human-labeled dataset for audio events"}, {"paperId": "7493389667058116dbc7e808987f129325ee60d7", "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results"}, {"paperId": "0095b9f73c000f2609fc81ffb7769df7cd77bda1", "title": "COCO-Stuff: Thing and Stuff Classes in Context"}, {"paperId": "2a94c84383ee3de5e6211d43d16e7de387f68878", "title": "Feature Pyramid Networks for Object Detection"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "1b80416cc2b05954941ac7e7dcbcc358c10e5ace", "title": "Visual Dialog"}, {"paperId": "88512be44744615f4baa8e14f600f036db4c2433", "title": "Semantic Understanding of Scenes Through the ADE20K Dataset"}, {"paperId": "29efbe391950ae438c63d86ad5c82b2942efb0b4", "title": "Modeling Context in Referring Expressions"}, {"paperId": "b133e361e2f8af22b823d25060b2e7c47f690985", "title": "Segmentation from Natural Language Expressions"}, {"paperId": "8ecc6ecd5b77665b261cce1a96d431d9f43bdaea", "title": "Gland segmentation in colon histology images: The glas challenge contest"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "9993e93f69050c2faa2985b94040f72ac311e82b", "title": "Automated Polyp Detection in Colonoscopy Videos Using Shape and Context Information"}, {"paperId": "b8e2e9f3ba008e28257195ec69a00e07f260131d", "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "def584565d05d6a8ba94de6621adab9e301d375d", "title": "Visual7W: Grounded Question Answering in Images"}, {"paperId": "e65142010431ffc089b272a1174214e00693e503", "title": "Generation and Comprehension of Unambiguous Object Descriptions"}, {"paperId": "0245171475005c171bd359fe9f461f5b6b7fb634", "title": "WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians"}, {"paperId": "b73e2d40da901ad3da3813670a51c52803d7af7e", "title": "SUN RGB-D: A RGB-D scene understanding benchmark suite"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "6364fdaa0a0eccd823a779fcdd489173f938e91a", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, {"paperId": "831af4347d2bcbc0fb21ecc346556e2459e25efb", "title": "Multi-class geospatial object detection and geographic image classification based on collection of part detectors"}, {"paperId": "92c141447f51b6732242376164ff961e464731c8", "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes"}, {"paperId": "616b246e332573af1f4859aa91440280774c183a", "title": "The Pascal Visual Object Classes Challenge: A Retrospective"}, {"paperId": "3419ccd5c94d301ee08d716d037f0c3c6a62e78e", "title": "The Role of Context for Object Detection and Semantic Segmentation in the Wild"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "d5e0460472a2ce2494f721e05f24898470e03e5e", "title": "Toward embedded detection of polyps in WCE images for early diagnosis of colorectal cancer"}, {"paperId": "44040913380206991b1991daf1192942e038fe31", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "26e209f7f6c6ced7376e7b0dab4d7f1c2ba8d702", "title": "3D Slicer as an image computing platform for the Quantitative Imaging Network."}, {"paperId": "c1994ba5946456fc70948c549daf62363f13fa2d", "title": "Indoor Segmentation and Support Inference from RGBD Images"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": "72729882f8fa3d9084eaece513f6bf9630be5901", "title": "Collecting Highly Parallel Data for Paraphrase Evaluation"}, {"paperId": "6bbdd6785bf24cdd1ee6c3e159fed34aebaca98f", "title": "ImageNet: Constructing a large-scale image database"}, {"paperId": "cca9200d9da958b7f90eab901b2f30c04f1e0e9c", "title": "Image Parsing: Unifying Segmentation, Detection, and Recognition"}, {"paperId": "d150f5ae307893aa3196cc55a08a72b297036d1e", "title": "Expression"}, {"paperId": null, "title": "Clipa-v2: Scaling clip training with 81.1accuracy within a $10,000 budget; an extra $4,000 unlocks"}, {"paperId": "7c33f9977cbe3e6205bdd55197c71ddd8efb73ff", "title": "MedLSAM: Localize and Segment Anything Model for 3D Medical Images"}, {"paperId": "4dd869a2c17cacd03b3e8a9b7efcd48f40651925", "title": "Segment Anything in Medical Images"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023"}, {"paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"}, {"paperId": "a2bcacc8fefb859c94c69d524b2368bb4792f9b1", "title": "Adversarial Prompting for Black Box Foundation Models"}, {"paperId": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045", "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models"}, {"paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af", "title": "Are Emergent Abilities of Large Language Models a Mirage?"}, {"paperId": "85d254414570340d49d9541056e8833b1822f7a0", "title": "SAM Fails to Segment Anything? - SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, and More"}, {"paperId": "c88158d40dcc7e225995cf282785f16fd8dee34d", "title": "RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback"}, {"paperId": "e6b79c12032884be401da08177a7c33ca02a6985", "title": "Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots"}, {"paperId": null, "title": "Sung-Ho Bae, and In So Kweon. Attacksam: Towards evaluating adversarial robustness of segment anything model"}, {"paperId": null, "title": "Coyo- 700m: Image-text pair dataset. https://github.com/ kakaobrain/coyo-dataset, 2022"}, {"paperId": "71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f", "title": "A Survey on Generative Diffusion Model"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "of the Neural Information Processing Systems Track on Datasets and Benchmarks 1"}, {"paperId": "cd0255491af35c5bc5f7cfa473718c19f67fcac3", "title": "URVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "8b55402ffee2734bfc7d5d7595500916e1ef04e8", "title": "nocaps: novel object captioning at scale"}, {"paperId": "b8e19d9154f49e951d30d8673585aa654b559cfc", "title": "Using NLG for speech synthesis of mathematical sentences"}, {"paperId": null, "title": "Openimages: A public dataset for large-scale multi-label and multi-class image classification"}, {"paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607", "title": "A Survey on Context Learning"}, {"paperId": null, "title": "Yfcc100m: The new data in multimedia research"}, {"paperId": "72a07410dbec0d67f45b4c6d5d49e38062b7da01", "title": "From the SelectedWorks of Marcel Adam Just 1990 What one intelligence test measures : A theoretical account of the processing in the Raven Progressive Matrices Test"}, {"paperId": "b93e9e3ec0c8d1fafa60fdf0fc184d2f710c112b", "title": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies"}, {"paperId": null, "title": "A stochastic grammar of images"}, {"paperId": "769f04cfc1bed71e0021e9d181141eae50fdb5c4", "title": "Raven Progressive Matrices"}, {"paperId": null, "title": "German I Parisi, Ronald Kemker, Jose L Part, Christopher"}, {"paperId": null, "title": "Yolo by ultralytics"}, {"paperId": null, "title": "Introducing chatgpt"}, {"paperId": null, "title": "Accessed: 2023-06-30"}, {"paperId": null, "title": "Christian Rupprecht, and An-drea"}, {"paperId": null, "title": "Learning to play minecraft with video pretraining (vpt)"}, {"paperId": null, "title": "How to jailbreak"}, {"paperId": null, "title": "Image-text pair dataset"}]}