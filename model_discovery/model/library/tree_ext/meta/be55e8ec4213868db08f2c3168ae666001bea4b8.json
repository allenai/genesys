{"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling", "abstract": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.", "venue": "International Conference on Machine Learning", "year": 2023, "citationCount": 696, "influentialCitationCount": 100, "openAccessPdf": {"url": "http://arxiv.org/pdf/2304.01373", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters is introduced, demonstrating that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics."}, "embedding": {"model": "specter_v2", "vector": [0.006059546023607254, 0.2826000154018402, -0.5204105377197266, -0.32845911383628845, -0.4148992896080017, -0.3047656714916229, 1.040968418121338, -0.4174967408180237, -0.7279025316238403, -0.3693505525588989, 0.3891858458518982, 0.004831826314330101, 0.32638218998908997, 0.27135154604911804, -0.39394891262054443, 0.2311616688966751, -1.1339797973632812, 0.4713608920574188, -0.38469424843788147, -0.30608952045440674, -0.3562680184841156, -0.8648995757102966, -1.0728487968444824, -0.02941993810236454, 0.3216102719306946, 0.15670235455036163, 0.16637764871120453, 0.9235976338386536, -0.15570101141929626, 0.1670408844947815, 0.46685341000556946, -0.39658138155937195, 0.23621752858161926, 0.049422118812799454, -0.22921328246593475, -0.13124921917915344, 0.4606330692768097, -0.33910199999809265, -0.5872628688812256, 0.39547625184059143, -0.271748423576355, 0.38179415464401245, 0.38018545508384705, -0.5563578009605408, -0.38193315267562866, 0.8518211245536804, 0.5629919171333313, 0.6321620941162109, -0.11254243552684784, -0.5484174489974976, 0.9773879051208496, -1.5058757066726685, 0.4807122051715851, 1.6516965627670288, 0.5318855047225952, 0.46576958894729614, -0.3682064116001129, -0.7709159851074219, 0.4637017548084259, -0.35939478874206543, -0.6686209440231323, -0.5515185594558716, -0.4941731095314026, -0.058105047792196274, 1.5460532903671265, -0.3815809488296509, -0.2595594823360443, 0.7452912330627441, 0.03517383709549904, 1.2997632026672363, 0.19902221858501434, -1.1124871969223022, -0.5775641798973083, 0.28523093461990356, 0.38000425696372986, 0.8153107762336731, -0.6495372653007507, 0.25188148021698, -1.0358095169067383, -0.22404251992702484, 0.17538894712924957, -0.11441873013973236, 0.12004710733890533, 0.09774437546730042, -0.5039964318275452, 0.9752321839332581, 0.010010285302996635, 1.0345078706741333, 0.18551866710186005, 0.7877203226089478, 0.6997421979904175, 0.6858792304992676, 0.511201798915863, 0.25405624508857727, -0.2938551902770996, 0.12763725221157074, -1.0142924785614014, 0.4530230164527893, 0.33279484510421753, 0.8996291756629944, -0.22611424326896667, -0.07147760689258575, -0.9494438767433167, 0.44866976141929626, 1.2214272022247314, 0.06624075025320053, 0.7584386467933655, -0.8584107756614685, 0.2158540040254593, -1.003234624862671, 0.1400841325521469, -0.6291885375976562, -0.4009592533111572, -0.253756046295166, -0.5032950043678284, -1.6207892894744873, -0.3151901662349701, 0.2567023038864136, -0.7632889747619629, 1.0388582944869995, -0.34969082474708557, -0.04045524820685387, -0.06018821895122528, 0.1807558834552765, 0.6420397162437439, 0.763316810131073, 0.47309422492980957, -0.18620188534259796, 0.992330014705658, -0.7678160667419434, -0.7241597771644592, -1.2031632661819458, 1.0490059852600098, -0.5339025855064392, 0.6014760136604309, -0.3906325697898865, -1.3889111280441284, -0.8415853381156921, -0.634537398815155, -0.19745896756649017, -0.24432410299777985, 0.7147533893585205, 1.1561096906661987, 0.5594053268432617, -0.972377598285675, 0.9230334758758545, -0.2155117690563202, -0.4652608335018158, 0.06278013437986374, 0.04138623923063278, 0.1222137063741684, -0.6721926927566528, -1.4988574981689453, 0.5214208960533142, 0.07576548308134079, -0.6868785619735718, -0.1544114500284195, -0.46380382776260376, -1.0690661668777466, -0.20879338681697845, 0.343657523393631, -0.12060557305812836, 1.4057657718658447, -0.08608882874250412, -1.114787220954895, 1.1722362041473389, -0.4893766939640045, 0.16595540940761566, 0.3047109842300415, -0.25502777099609375, -0.6860066652297974, -0.6263202428817749, -0.25080835819244385, 0.5021337866783142, 0.488565593957901, 0.06276494264602661, 0.16739626228809357, 0.12820638716220856, -0.3925175964832306, 0.056221041828393936, -0.25733673572540283, 1.1060609817504883, -0.6460543274879456, -0.2865737974643707, 0.3617926836013794, 0.17310048639774323, 0.21487519145011902, -0.38842785358428955, -0.09798116981983185, -1.0574822425842285, 0.3475121855735779, -0.4789472818374634, 1.183517336845398, -1.1013200283050537, -0.5857183337211609, -0.17742909491062164, -0.3483070433139801, -0.23728440701961517, -0.6915064454078674, 0.9242038130760193, 0.20892122387886047, 0.612938642501831, -0.12705621123313904, -1.9119484424591064, 0.2825114130973816, -0.08209630101919174, -0.4646380543708801, -0.21738877892494202, -0.262494295835495, 1.00027334690094, -0.6278430819511414, 0.029820406809449196, -0.2146257609128952, 0.1979811191558838, -0.9529715776443481, 0.9921491146087646, -0.48820844292640686, 0.20921166241168976, 0.3227328956127167, -0.2796081602573395, -0.014291908591985703, -0.3677746057510376, 0.42458435893058777, -0.1196717694401741, -0.2285788208246231, 0.21291406452655792, -0.15582245588302612, 1.538705825805664, -0.4561946988105774, 0.2647661864757538, -0.0398922823369503, -0.1159081682562828, -0.25235486030578613, 0.43673792481422424, -0.35001030564308167, -0.4629865288734436, 0.4474433362483978, 0.6882104873657227, -0.66892409324646, 0.2373620718717575, 0.9591026902198792, 0.5557200312614441, -0.5034460425376892, 0.2833017110824585, 0.7266775965690613, -0.031087158247828484, 0.555337131023407, 0.7324913740158081, 0.2308112382888794, 0.27017977833747864, 0.08507248014211655, -0.3288652300834656, 0.07476165145635605, -0.9691065549850464, -0.3955192267894745, 0.6683288216590881, 0.49475693702697754, 0.694226861000061, 0.19335591793060303, -0.658473789691925, -0.486569344997406, -0.09149979799985886, 0.5649630427360535, 2.108055353164673, -0.2476736456155777, -0.22092105448246002, -0.6243287324905396, 0.22940810024738312, -0.2056483030319214, 0.0638195276260376, -0.7239542603492737, -0.03206045180559158, -0.806862473487854, -0.7729852199554443, 0.6268408894538879, -0.2333800047636032, 0.7106015682220459, -0.7673026323318481, -0.1875835508108139, -0.38783082365989685, 0.3236815929412842, -0.7093185186386108, -0.7387253046035767, 0.13626880943775177, -0.3827100992202759, 0.3585425913333893, -0.3528367877006531, -0.34806662797927856, -0.16378432512283325, -0.5099208354949951, 0.9202147722244263, -0.07591572403907776, -0.1690523624420166, 0.3156217932701111, 0.537301778793335, -0.6106115579605103, -1.2011473178863525, 0.37357181310653687, 0.4822089970111847, -0.26439598202705383, -0.12827321887016296, 0.5581549406051636, 0.3074744939804077, 0.017202790826559067, -0.4356876611709595, 0.4627821445465088, 0.18955188989639282, -0.13016770780086517, 0.452025830745697, -0.6299777030944824, 0.2317790538072586, -1.495236873626709, 0.8895711302757263, -0.17482906579971313, -0.31798529624938965, 0.6671059131622314, -0.5825010538101196, -0.5265249609947205, 0.21853600442409515, -0.7983976602554321, -0.456892728805542, -1.0739877223968506, 0.5549501776695251, -0.060319751501083374, 0.0844939574599266, 0.241178497672081, 0.6886031627655029, 0.34209969639778137, 0.2618105709552765, 0.5734445452690125, 0.6352757811546326, -0.29133835434913635, 0.5075090527534485, -0.808435320854187, 0.41133323311805725, 0.23418250679969788, 0.18216247856616974, -0.22353555262088776, -0.3279545307159424, -0.8283692002296448, -0.48649662733078003, -0.39161401987075806, -0.2679651975631714, -0.4131053388118744, 0.08182860165834427, -0.5927554965019226, -0.7477017045021057, 0.29066890478134155, -0.9356623888015747, -0.2357402741909027, 0.7444359660148621, -0.1768602877855301, -0.17755760252475739, -1.0834760665893555, -1.6977689266204834, -0.525862455368042, -0.6363341212272644, -0.8532331585884094, 0.386001318693161, 0.12052962929010391, -0.5709409713745117, -0.5456503033638, 0.2512241005897522, -0.2321578711271286, 1.212411880493164, -0.6581512689590454, 0.8541198968887329, 0.0320211686193943, 0.17152780294418335, -0.4417140483856201, 0.42086198925971985, 0.3093582093715668, -0.40452367067337036, 0.5678215622901917, -0.965903103351593, -0.26676470041275024, -0.4828712046146393, -0.6242387294769287, 0.12986034154891968, 0.41678208112716675, 0.5199790596961975, 0.006601979490369558, -0.6076815128326416, 0.12520787119865417, 0.9791232347488403, -1.000802993774414, -0.27271801233291626, -0.006875691935420036, 0.7039903402328491, 0.48958835005760193, -0.4527842104434967, 0.6161074638366699, 0.4232884347438812, 0.4620090126991272, -0.31266799569129944, 0.2976720631122589, 0.269453763961792, -0.614885687828064, 0.7831044793128967, 1.9068547487258911, 0.28745293617248535, -0.04664721339941025, -0.9936949610710144, 0.36260586977005005, -0.8632391095161438, -0.6835830807685852, 0.7050535082817078, 0.9032106995582581, 0.6235648393630981, -0.5699861645698547, -0.02787255123257637, -0.40309420228004456, 0.24153640866279602, 0.6322336792945862, -0.3238925337791443, -0.734711229801178, -0.013748959638178349, 0.11713917553424835, -0.045504238456487656, 0.4419272541999817, -0.4021775722503662, 0.6373085975646973, 14.825103759765625, 0.9710568189620972, 0.02560722827911377, 0.8818392157554626, 0.6783267259597778, -0.08086808025836945, -0.4983486831188202, -0.4810625910758972, -1.06674063205719, -0.16760605573654175, 1.4513494968414307, 0.04021136462688446, 1.2256667613983154, -0.019341005012392998, -0.03075437806546688, 0.03273911029100418, -0.23883342742919922, 0.3340674638748169, 0.5302330255508423, -1.0201352834701538, 0.2675113081932068, -0.10735276341438293, 0.7144492268562317, 0.7992892265319824, 1.0788164138793945, 1.1442897319793701, 0.21106787025928497, -0.30132347345352173, 0.5473542809486389, 0.2957480847835541, 0.8289234638214111, -0.10514949262142181, 0.018174396827816963, 0.9051293134689331, -0.5558894276618958, -0.15680962800979614, -0.5401001572608948, -1.2693359851837158, 0.16591353714466095, -0.23898622393608093, -0.5787202715873718, -0.44165828824043274, -0.17089255154132843, 0.5624247789382935, -0.20826247334480286, 0.012994037941098213, 0.30168718099594116, 0.9088345766067505, -0.03644896671175957, 0.052123941481113434, 0.2595299780368805, 0.5542959570884705, 0.17719221115112305, -0.01420981902629137, 0.034898046404123306, -0.19700275361537933, -0.0386783629655838, 0.16734948754310608, -0.7604321241378784, 0.3842467665672302, -0.3832756578922272, -0.6762667298316956, 0.04312775284051895, 0.6866083741188049, 0.29198718070983887, 0.19549845159053802, -0.4213641285896301, 0.4473593235015869, 0.7623935341835022, 0.1307273507118225, -0.12433574348688126, 0.5940501093864441, 0.2334025353193283, -0.39456820487976074, -0.35738426446914673, 0.2584547996520996, 0.00807260349392891, -0.5236654281616211, -0.5914912223815918, -0.2811865210533142, 0.28373563289642334, -0.5678096413612366, -0.8091803193092346, 0.634149432182312, -0.021341465413570404, -0.18961530923843384, 0.012559359893202782, -0.37905287742614746, -0.20283794403076172, 0.5970402359962463, -1.1156083345413208, -0.9723479151725769, 0.5410277843475342, -0.5764212012290955, -0.2723740339279175, 0.0681205540895462, 1.4838017225265503, 0.2784716784954071, -0.30228346586227417, 0.25910237431526184, 0.5889305472373962, -0.3393423855304718, 0.008372782729566097, -0.36439502239227295, 1.125246524810791, 0.07225833088159561, -0.005231315735727549, 0.6834965944290161, 0.34135982394218445, 0.21842409670352936, -0.8848903775215149, 0.15030884742736816, 1.1499862670898438, -0.9614128470420837, -0.4421676695346832, -0.85402512550354, -0.9447900056838989, 0.49684420228004456, 0.5074952244758606, -0.31425589323043823, 0.3093220591545105, 0.4199877083301544, -0.43996256589889526, 0.1744057983160019, -0.6600659489631653, -0.029354406520724297, 0.5648090839385986, -0.9232101440429688, 0.09792839735746384, 0.30626922845840454, 0.4677651822566986, -1.0130739212036133, -0.3815624415874481, -0.30337774753570557, 0.3649657964706421, 0.030157646164298058, 0.5559678673744202, -0.5232155323028564, 0.5626291036605835, 1.304247260093689, -0.329805463552475, -0.9232827425003052, -0.06975023448467255, -0.7182512879371643, 0.07178682833909988, 0.21845877170562744, 0.7095747590065002, -0.2556171119213104, -0.06949217617511749, 0.8937709331512451, 0.4273829460144043, -0.5791125893592834, -0.5734878778457642, -0.5282971262931824, 0.44567927718162537, -0.43171900510787964, 0.06516280025243759, -0.1110539585351944, -0.23366710543632507, -0.12403927743434906, -0.10638011246919632, 0.4872247874736786, -0.220114067196846, -0.8316255211830139, 0.4454857110977173, -0.21154700219631195, -0.40400031208992004, -0.43211573362350464, 0.11869978904724121, -0.9140307903289795, 0.1530565768480301, -1.2881791591644287, 0.0785328820347786, -0.6397707462310791, -0.2822016775608063, -0.1516152024269104, 0.05829985812306404, -0.16696330904960632, 0.3882244825363159, -0.21571208536624908, -0.172774538397789, -0.1575925648212433, -0.5780136585235596, 0.7522028684616089, 1.0540406703948975, -0.349674254655838, 0.14197243750095367, 0.04935251548886299, 0.7465451955795288, 0.26387476921081543, 0.5887007117271423, -0.4095599353313446, -0.7990282773971558, -1.5681445598602295, 0.5236853957176208, -0.2041134387254715, -0.24740612506866455, -0.6292870044708252, 0.22174327075481415, 0.47516265511512756, -0.33098265528678894, 0.3042137920856476, 0.44822385907173157, -0.41214269399642944, -0.0945792943239212, 0.2525080442428589, -0.9052371382713318, 0.5395710468292236, 0.24471791088581085, -0.6865370273590088, 0.07287148386240005, 0.6661709547042847, -0.16753467917442322, -1.005807638168335, -0.6132125854492188, 0.6422526240348816, -0.436347097158432, 0.26029497385025024, -0.2571626603603363, 0.12861591577529907, -0.8618447780609131, -0.42371392250061035, -0.22300440073013306, 0.2765454053878784, -0.13583554327487946, 1.3014529943466187, -0.10723753273487091, -1.141628623008728, -0.0357975997030735, 0.46513664722442627, 0.15081870555877686, -0.47375527024269104, 0.8591347932815552, 0.2872616946697235, -0.20617985725402832, 0.5343834757804871, 0.2456168383359909, 0.3681548237800598, -1.098404049873352, -0.33161410689353943, 0.5104227066040039, -0.5081641674041748, 0.0061386143788695335, 1.274949312210083, -0.4126615822315216, -1.3932286500930786, 0.06977756321430206, -0.9017621278762817, -0.42297694087028503, -0.33643004298210144, 0.5557303428649902, 0.1842155158519745, -0.2351788729429245, -0.4851631224155426, -0.2390749603509903, 0.006012461613863707, -0.1216309666633606, -0.691551148891449, 0.42850029468536377, -0.4664583206176758, -0.32429686188697815, 1.2935014963150024, 1.2096846103668213, -0.4262855350971222, -0.5006648302078247, -0.7870936989784241, -0.46038782596588135, 0.10182278603315353, -0.020249636843800545, -0.5116590261459351, -0.16388057172298431, 0.6309714317321777, 0.31845057010650635, 0.41383635997772217, -0.397144079208374, -0.02989119477570057, 0.33440348505973816, 0.5395463705062866, 0.4153206944465637, -0.8001897931098938, -0.6158155798912048, 1.4975082874298096, 1.1529302597045898, -1.2293956279754639, 0.1791987270116806, 0.1123695969581604, -0.8604254126548767, 0.4803696870803833, 0.19125902652740479, 0.22138448059558868, 0.9680796265602112, -0.6354103088378906, 0.28176337480545044, 0.19395142793655396, -0.9978434443473816, 0.32794132828712463, 0.8553529977798462, 0.18572120368480682, 0.9926851391792297, 0.7319523096084595, -0.31736934185028076, 0.7914170622825623, -0.06274624168872833, 0.30531933903694153, 0.3165774643421173, 0.513721764087677, -0.31993934512138367, 0.012195883318781853, 0.35315829515457153, 0.8447856307029724, -0.6835008859634399, -0.546254575252533, 0.3191750943660736, 0.482308030128479, 0.13792718946933746, 0.30089282989501953, 0.5919241309165955, 0.18357053399085999, 0.22760257124900818, 0.6227556467056274, 0.6920511722564697, -0.6046692728996277, -0.32374125719070435, -0.5254424214363098, -0.5513380169868469, 0.1538776159286499, -0.06983473151922226, -0.6549122333526611, -0.31122398376464844, -0.5617367029190063, 0.3488495349884033, -0.15421243011951447, 0.3513032793998718, 1.3309484720230103, 0.6649706363677979, -0.04430748522281647, -0.2555946111679077, -0.6489807963371277, -0.5661077499389648, -1.2616171836853027, -0.277805358171463, -0.3259631097316742, -0.3032678961753845, -0.1330386996269226, -0.2445439249277115, -0.49107611179351807]}, "authors": [{"authorId": "103476203", "name": "Stella Biderman"}, {"authorId": "2184031883", "name": "Hailey Schoelkopf"}, {"authorId": "1404060481", "name": "Quentin G. Anthony"}, {"authorId": "2070768742", "name": "Herbie Bradley"}, {"authorId": "2212970046", "name": "Kyle O'Brien"}, {"authorId": "2162462983", "name": "Eric Hallahan"}, {"authorId": "2168771748", "name": "Mohammad Aflah Khan"}, {"authorId": "2162467233", "name": "Shivanshu Purohit"}, {"authorId": "2162462141", "name": "USVSN Sai Prashanth"}, {"authorId": "34885007", "name": "Edward Raff"}, {"authorId": "2213349418", "name": "Aviya Skowron"}, {"authorId": "35566806", "name": "Lintang Sutawika"}, {"authorId": "1986356851", "name": "Oskar van der Wal"}], "references": [{"paperId": "c51192d7440807dc98cc4374fb5d919390d70b0b", "title": "OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization"}, {"paperId": "deb8f26509ae320fc975b32922416cb156c61bbd", "title": "Emergent and Predictable Memorization in Large Language Models"}, {"paperId": "762ca2711eb167f19b79e39c175708ca15e1f5d7", "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "c6ee979c2da4b55a8486abae4cd720422ab09b26", "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories"}, {"paperId": "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e", "title": "Training Trajectories of Language Models Across Scales"}, {"paperId": "92b37b91483fa3a52563f409c1345a1615a551cd", "title": "Undesirable biases in NLP: Averting a crisis of measurement"}, {"paperId": "75f7e9e2b59fb640ef9d1dff94097175daf46c4d", "title": "Large Language Models Struggle to Learn Long-Tail Knowledge"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "f240b39aa5aa30b3e4f7a8e68d681a13a9a72054", "title": "EleutherAI: Going Beyond \"Open Science\" to \"Science in the Open\""}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "6a0b1e2a323941de37a5ce46c2adcd37b5064fcb", "title": "Efficient Gender Debiasing of Pre-trained Indic Language Models"}, {"paperId": "4721dd8c4f2681e231040cc5deebdbf938f58392", "title": "Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions"}, {"paperId": "38d7dcac16e0d640d4fa2a1fa29320b0e5fcbdad", "title": "The Birth of Bias: A case study on the evolution of gender bias in an English language model"}, {"paperId": "6c46b7b401be5ada79f00b36cb8e5b41286ae2aa", "title": "Measuring Forgetting of Memorized Training Examples"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "77d25889a59041629e8b2a81aad56544144c6878", "title": "Are Large Pre-Trained Language Models Leaking Your Personal Information?"}, {"paperId": "8b293973061026d9d0eed90e71e30928e029171e", "title": "Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models"}, {"paperId": "aa4d9972af3264d032dbee58501ed4ac49477103", "title": "Scaling Laws and Interpretability of Learning from Repeated Data"}, {"paperId": "c08c2fd7b269bd9d6c1162adc3b059cdf21d40a6", "title": "Lifting the Curse of Multilinguality by Pre-training Modular Transformers"}, {"paperId": "3ae3716f125d71e9daccac3dafa4fab7482fb16a", "title": "Data Governance in the Age of Large-Scale Data-Driven Language Technology"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "1fafaccebc4a74898a74c606f846318c4c2c7536", "title": "On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"}, {"paperId": "6979ce65b9f657672cd3a0b9217ead51511c1838", "title": "VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "c57293882b2561e1ba03017902df9fc2f289dea2", "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents"}, {"paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da", "title": "InCoder: A Generative Model for Code Infilling and Synthesis"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "72a6b51c492aa9333b857477ff19f76c37053aa9", "title": "Quantifying Societal Bias Amplification in Image Captioning"}, {"paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "title": "A systematic evaluation of large language models of code"}, {"paperId": "28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d", "title": "Quantifying Memorization Across Neural Language Models"}, {"paperId": "af46b5ee6d0c1aada1c482d53018a50909aa4c90", "title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b562ce4b85a97de5a9e909c7dcd090d0fa5f19ec", "title": "A Systematic Study of Bias Amplification"}, {"paperId": "cdc554a3e8d3758e68bedebcb32473c100ef50fc", "title": "Documenting Geographically and Contextually Diverse Data Sources: The BigScience Catalogue of Language Data and Resources"}, {"paperId": "bd44f34b47c8a4b6947695853fc2814ac69664a6", "title": "Datasheet for the Pile"}, {"paperId": "9ba99b612b54d9c8ab1ac19014774b9c8e854aea", "title": "Submix: Practical Private Prediction for Large-Scale Language Models"}, {"paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "title": "High-Resolution Image Synthesis with Latent Diffusion Models"}, {"paperId": "4497f092aa44d4b47659226fad348a1834efd416", "title": "Acquisition of chess knowledge in AlphaZero"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "db742f7c4f7edd24e77482560df7d09c9033b3da", "title": "Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?"}, {"paperId": "de1fdaf92488f2f33ddc0272628c8543778d0da9", "title": "Scaling Laws for Neural Machine Translation"}, {"paperId": "d48d1e80b6ea9708fa3a09d1556a7ced3b147da2", "title": "Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation"}, {"paperId": "752985595ce02327b76bb11e0afafb9d31522215", "title": "Scaling Effect of Self-Supervised Speech Models"}, {"paperId": "b1e509bfee5f08dffc1e1b247c63a0260ed09fcc", "title": "A Scaling Law for Syn2real Transfer: How Much Is Your Pre-training Effective?"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "5b540745f4b51f95bf90fb3420e51edb037fc51a", "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis"}, {"paperId": "61e06615f6cee5ed1ba7d22b801925b69a45653b", "title": "The Values Encoded in Machine Learning Research"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3", "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus"}, {"paperId": "6803adc7d8b891be652d18815f830f7a42a0f5b5", "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "ac40415570e7d51efb237c5e4b4dbe4e93919409", "title": "Membership Inference Attacks on Machine Learning: A Survey"}, {"paperId": "4383a975c09b72ba2f1a77cd779bb6965dbfb2fb", "title": "Scaling Laws for Transfer"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba", "title": "Extracting Training Data from Large Language Models"}, {"paperId": "e0cc14403b3d57572e837f9f3b1cca87d70e2bcd", "title": "Cross-Loss Influence Functions to Explain Deep Network Representations"}, {"paperId": "71a85e735a3686bef8cce3725ae5ba82e2cabb1b", "title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc", "title": "Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"}, {"paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "2c5a8950cf0a13e229ad19093ba064495fda8de7", "title": "A Neural Scaling Law from the Dimension of the Data Manifold"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "92343cecdc990380de362b969eec60081959f507", "title": "Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"}, {"paperId": "c2867275ec80f4d5325cf3451291e72628fb69b6", "title": "Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model"}, {"paperId": "a4e67bcbf912e13cebbb1241d05d1ca0a1df9df8", "title": "Identifying and Reducing Gender Bias in Word-Level Language Models"}, {"paperId": "eefa0df7c5678fa6004f8b48dbbc1c2696702fee", "title": "An Empirical Model of Large-Batch Training"}, {"paperId": "de338882ca81b4ecf486f10b12ca71d4131d09f4", "title": "Understanding the Origins of Bias in Word Embeddings"}, {"paperId": "0be19fd9896e5d40222c690cc3ff553adc7c0e27", "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods"}, {"paperId": "520ec00dc35475e0554dbb72f27bd2eeb6f4191d", "title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks"}, {"paperId": "135bafc83e9a73c88e759f98a28edfdb5c02f81d", "title": "Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints"}, {"paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "d4fb836846b79d8692df8bf54d20d1a9d02ffe7d", "title": "Debiasing Pre-Trained Language Models via Efficient Fine-Tuning"}, {"paperId": "72128b2da0ffb784861889462070570b21017b9f", "title": "French CrowS-Pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English"}, {"paperId": "1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a", "title": "Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy"}, {"paperId": "7b0f98f51040700aae3cd9f0e3432dedcd69fb30", "title": "When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories"}, {"paperId": null, "title": "2022)) they largely are not publicly available (less than 10 checkpoints available, only for the 2.7b, 6.7b, and 13b parameter models). Additionally, the training dataset for OPT is not public"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "On the sizes of openai api models"}, {"paperId": "3e65f572322e192fe36ae52a8a7f025b0685dfc6", "title": "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets"}, {"paperId": null, "title": "WuDao: Pretrain the world. Keynote adress at the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, 2021"}, {"paperId": null, "title": "Pretrain the world"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation"}, {"paperId": null, "title": "Mistral -a journey towards reproducible language model training"}, {"paperId": null, "title": "Which model is helpful in solving privacy, memorization, and bias problems?"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Trivi-aqa"}, {"paperId": null, "title": "We use rotary embeddings introduced by Su et al. (2021) and now in widespread use (Black et al., 2022; Chowdhery et al., 2022; Zeng et al., 2022) as our positional embedding type of choice"}, {"paperId": null, "title": "We use Flash Attention (Dao et al., 2022) during training for improved device throughput"}, {"paperId": null, "title": "Zero-shot results on standard NLP benchmarks for the BLOOM model suite, reported for comparison with Pythia's performance. Task 125M 350M 1.3B 2"}, {"paperId": null, "title": "Five-shot results on selected NLP Benchmarks, for the fully-trained Pythia (Deduplicated) suite"}]}