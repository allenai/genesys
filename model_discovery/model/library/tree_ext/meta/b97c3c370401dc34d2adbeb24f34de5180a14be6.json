{"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture", "abstract": "In recent years, attention-based models have achieved impressive performance in natural language processing and computer vision applications by effectively capturing contextual knowledge from the entire sequence. However, the attention mechanism inherently contains a large number of redundant connections, imposing a heavy computational burden on model deployment. To this end, sparse attention has emerged as an attractive approach to reduce the computation and memory footprint, which involves the sampled dense-dense matrix multiplication (SDDMM) and sparse-dense matrix multiplication (SpMM) at the same time, thus requiring the hardware to eliminate zero-valued operations effectively. Existing techniques based on irregular sparse patterns or regular but coarse-grained patterns lead to low hardware efficiency or less computation saving. This paper proposes Sanger, a framework that harvests sparsity in the attention mechanism through synergistic hardware and software co-design. The software part prunes the attention matrix into a dynamic structured pattern, and the hardware part features a reconfigurable architecture that exploits such patterns. Specifically, we dynamically sparsify vanilla attention based on a quantized prediction of the attention matrix. Then, the sparse mask is re-arranged into structured blocks that are more amenable to hardware implementation. The hardware design of Sanger features a score-stationary dataflow that keeps sparse scores stationary in the PE to avoid decoding overhead. Using this dataflow and a reconfigurable systolic array design, we can unify the computation of SDDMM and SpMM operations. Typically, the PEs can be configured during runtime to support different data access and partial sum accumulation schemes. Experiments on BERT show that Sanger can prune the model to 0.08 - 0.27 sparsity without accuracy loss, achieving 4.64X, 22.7X, 2.39X, and 1.47X speedup compared to V100 GPU, AMD Ryzen Threadripper 3970X CPU, as well as the state-of-the-art attention accelerators A3 and SpAtten.", "venue": "Micro", "year": 2021, "citationCount": 79, "influentialCitationCount": 13, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3466752.3480125", "status": "BRONZE"}, "tldr": {"model": "tldr@v2.0.0", "text": "Sanger is proposed, a framework that harvests sparsity in the attention mechanism through synergistic hardware and software co-design that dynamically sparsify vanilla attention based on a quantized prediction of the attention matrix."}, "embedding": {"model": "specter_v2", "vector": [0.660812258720398, 0.18577247858047485, -0.295480340719223, 0.18185210227966309, -0.03227071464061737, 0.26115554571151733, 0.35971319675445557, -0.08339109271764755, -0.22308474779129028, -0.36368265748023987, 0.42905181646347046, 0.23135726153850555, 0.6968397498130798, 0.05766359344124794, -0.039524663239717484, 0.04507746174931526, -0.9894418120384216, 0.030117742717266083, 0.28973859548568726, -0.3473217785358429, 0.2552737593650818, -0.9090401530265808, -1.252915620803833, 0.5211390256881714, -0.043913815170526505, 0.5613686442375183, 0.7172089219093323, 1.2384973764419556, -0.28350621461868286, 0.5576157569885254, 0.40058648586273193, -0.06423084437847137, 0.19897869229316711, -0.2317916303873062, 0.055045660585165024, -0.5556282997131348, 0.4738508462905884, -0.02100350707769394, -0.4805126190185547, 0.8344058990478516, -0.2260286509990692, 0.16369453072547913, 0.24749189615249634, -0.15311308205127716, -0.1938282698392868, 0.7288946509361267, 0.10691598802804947, 1.1251109838485718, 0.01436940673738718, -0.49420633912086487, 1.2958500385284424, -1.2294386625289917, 0.06003149598836899, 1.109977126121521, 0.4272252023220062, 0.32876747846603394, -0.04248523712158203, -0.3463813364505768, 0.7589643597602844, 0.37681910395622253, -0.7121202945709229, -0.30679282546043396, 0.13598056137561798, -0.14291012287139893, 2.0720744132995605, -0.058130111545324326, 0.3699484169483185, 0.5946433544158936, 0.29716917872428894, 1.1805369853973389, -0.572981595993042, -1.0549174547195435, -0.07915160804986954, -0.4259863793849945, 0.6173681020736694, 0.8249834775924683, -0.08317103236913681, 0.3679661452770233, -1.1397567987442017, -0.2794654369354248, 0.5074191093444824, 0.1127806007862091, 0.5387943983078003, -0.049708250910043716, -0.3857702910900116, 0.689681351184845, 0.3653358221054077, 0.85789555311203, -0.4522233307361603, 0.7329084873199463, 0.6011937856674194, 0.10580342262983322, -0.4207440912723541, 0.27067050337791443, 0.22758406400680542, 0.46483317017555237, -1.0377252101898193, -0.1492418646812439, -0.05804511159658432, 1.0221247673034668, -0.3089941740036011, 0.7162202596664429, -0.6423128843307495, 0.02902725711464882, 1.1346614360809326, -0.02649575285613537, 0.544414758682251, -0.2915554344654083, -0.15657386183738708, -0.7227292060852051, -0.24812667071819305, -1.036551594734192, -0.15730468928813934, -0.1848592907190323, -1.167709469795227, -1.0285884141921997, -0.6638549566268921, 0.30363792181015015, -0.7501553893089294, 0.4514249265193939, -0.49404823780059814, 0.6040064096450806, -0.050955235958099365, 0.26637396216392517, 0.6977722644805908, 0.4882611632347107, 0.22700157761573792, 0.27856236696243286, 1.222606897354126, -1.0103323459625244, -0.33617323637008667, -1.3206483125686646, -0.06845275312662125, -0.4155077040195465, 0.1884729117155075, -0.04864077642560005, -1.2281631231307983, -0.9951651096343994, -0.9524669051170349, -0.06226397305727005, -0.2261919230222702, 0.2589259147644043, 1.1227439641952515, 0.1810467690229416, -0.9370001554489136, 0.36443307995796204, -0.590728223323822, -0.21760626137256622, 0.42625173926353455, 0.189350426197052, 0.7084314227104187, -0.17898519337177277, -1.0667937994003296, 0.055960893630981445, -0.05253905430436134, -0.4061809480190277, -0.2693210244178772, -0.37182292342185974, -0.9995546340942383, 0.37446630001068115, 0.08838707208633423, -0.6048707365989685, 0.9495559334754944, -0.25146037340164185, -1.036062240600586, 0.6359268426895142, -0.558694064617157, -0.29671013355255127, -0.40200474858283997, 0.008212234824895859, -0.5027249455451965, -0.24482470750808716, -0.400459885597229, 0.3011557459831238, 0.5778138041496277, 0.35082000494003296, -0.03186538815498352, 0.313247948884964, -0.4630652070045471, -0.4676845967769623, -0.26501044631004333, 0.9483942985534668, -0.6236453056335449, -0.33651867508888245, 0.6041334271430969, 0.6526599526405334, -0.43620163202285767, -0.39898842573165894, -0.2250024825334549, -0.4188773036003113, 0.6266804933547974, 0.1995418518781662, 1.2224383354187012, -0.9360429644584656, -0.7764344215393066, -0.04517805948853493, 0.024698130786418915, 0.006708803586661816, -0.7350071668624878, 0.27945610880851746, -0.42919740080833435, -0.021823786199092865, -0.0167836993932724, -0.3774321675300598, -0.4817606806755066, -0.4287259578704834, -0.4823206067085266, -0.13759686052799225, 0.3571006655693054, 1.0859997272491455, -1.1045726537704468, -0.08342252671718597, -0.1059907004237175, 0.3395386338233948, -1.519744634628296, 1.0501292943954468, -0.6555284261703491, 0.03564972057938576, -0.17816971242427826, 0.3232389986515045, -0.051459502428770065, -0.49634188413619995, 0.7676540613174438, -0.4385768175125122, -0.13809244334697723, 0.39677056670188904, -0.20412500202655792, 1.4520267248153687, -0.15460452437400818, 0.8079596757888794, 0.027891667559742928, -0.6177787184715271, 0.6879337430000305, 0.10487008839845657, -0.2571772634983063, -0.5745404958724976, 0.4597684144973755, 0.038679324090480804, -0.21109265089035034, -0.017509153112769127, 1.0238176584243774, 1.2035233974456787, -0.5463054776191711, -0.35307878255844116, 0.3983669579029083, -0.1728828251361847, 0.24050070345401764, 0.4698322117328644, 1.0825458765029907, 0.023093698546290398, 0.9297699928283691, -0.2822567820549011, 0.44947996735572815, -0.7185405492782593, 0.020676247775554657, 0.6759836077690125, 0.6265014410018921, 0.7632676362991333, 0.301177054643631, -0.9198599457740784, -0.2613639235496521, 0.4885985553264618, 0.8128212690353394, 1.4670531749725342, -0.12371569871902466, 0.03229522705078125, -0.3930889070034027, 0.15947987139225006, -0.5430924892425537, -0.27079489827156067, -0.1969136744737625, -0.061497706919908524, -0.7159982919692993, -0.9876923561096191, 0.6433672904968262, 0.07938063889741898, 0.6886404752731323, -0.8853580355644226, -0.6890357732772827, -0.27651092410087585, 0.42410364747047424, -0.5981975197792053, -0.7379582524299622, 0.5469037890434265, -0.14864884316921234, -0.11834940314292908, 0.37859413027763367, -0.5052595138549805, 0.18199622631072998, -0.2841603457927704, 1.4283466339111328, -0.38809773325920105, -0.5221332907676697, 0.09359326213598251, 0.26844263076782227, -0.5837960839271545, -0.5897976160049438, 0.2941848337650299, -0.14176957309246063, -0.14558325707912445, 0.7131487727165222, 0.16328121721744537, -0.03667496144771576, -0.28906965255737305, -0.2554298937320709, 0.007396000437438488, 0.13540783524513245, 0.5185562968254089, 0.8979687094688416, -1.0285495519638062, 0.12444993853569031, -1.0131725072860718, 0.34426409006118774, -0.15680131316184998, -0.5051775574684143, -0.22994662821292877, 0.060051415115594864, -0.41808849573135376, 0.48194989562034607, -0.7312108278274536, -0.05325261130928993, -0.47874578833580017, 0.09083038568496704, -0.4789313077926636, -0.11015186458826065, 0.16279621422290802, 0.32501405477523804, -0.07301903516054153, 0.601947546005249, 0.5117583274841309, 0.17864055931568146, 0.36641883850097656, 0.18632851541042328, -0.6946004033088684, 0.72721928358078, 0.2700861096382141, -0.3383307456970215, -0.04560760408639908, -0.11812257766723633, -1.1211050748825073, -0.28375163674354553, -0.39355504512786865, -0.191705122590065, -0.046840038150548935, 0.17233450710773468, -0.570317268371582, -1.0563856363296509, -0.3271091878414154, -1.2511038780212402, -0.07368290424346924, 0.01569620706140995, -0.18427276611328125, -0.11767878383398056, -0.7467367053031921, -0.8607179522514343, -0.6168135404586792, -1.0024129152297974, -0.9157694578170776, 0.11677113175392151, 0.10403304547071457, -0.6856163740158081, -0.14128775894641876, 0.20246733725070953, -0.7789701819419861, 0.9739750027656555, -0.24683141708374023, 0.5472859740257263, -0.14626429975032806, -0.5138915181159973, -0.33256787061691284, 0.1811821311712265, 0.1724090278148651, -0.27482104301452637, 0.15013594925403595, -0.6658335328102112, 0.5174353718757629, -0.22410506010055542, -0.11356203258037567, 0.514527440071106, 0.39551594853401184, 0.714380145072937, 0.08552336692810059, -0.39420509338378906, 0.39631056785583496, 1.2492214441299438, -0.22477523982524872, 0.35895484685897827, -0.10838882625102997, 0.8075097799301147, -0.22988620400428772, -0.2497844099998474, 0.7408168315887451, 0.16061446070671082, 0.6450708508491516, 0.3262181878089905, 0.0650748759508133, -0.41322460770606995, -0.16322891414165497, 0.36337515711784363, 1.6770199537277222, 0.384566992521286, 0.14943937957286835, -0.8104574084281921, 0.6729028820991516, -1.0539028644561768, -1.133744239807129, 0.3684096038341522, 0.6207578778266907, 0.25572890043258667, -0.4579419195652008, -0.4892579913139343, -0.3461196720600128, 0.4131908416748047, 0.42890650033950806, -0.552868664264679, -1.0807931423187256, -0.10846789926290512, 0.5527571439743042, -0.13104136288166046, 0.587547242641449, -0.10300493240356445, 0.6768266558647156, 15.297447204589844, 0.5394813418388367, -0.11812567710876465, 0.48854348063468933, 0.6885318756103516, -0.04521307721734047, -0.11333737522363663, -0.24502243101596832, -1.2855784893035889, -0.05954639986157417, 1.1334983110427856, 0.22889366745948792, 0.3439975678920746, 0.5652822852134705, -0.1131153330206871, 0.28182709217071533, -0.4409796893596649, 0.7665606141090393, 0.746562659740448, -1.4168883562088013, 0.08805699646472931, -0.14813561737537384, 0.4922543466091156, 0.5463849306106567, 0.6469123363494873, 0.6272530555725098, 0.4535312056541443, -0.14663396775722504, 0.2438904047012329, 0.6084566712379456, 0.8340601325035095, 0.28375962376594543, 0.03483056277036667, 0.19664587080478668, -1.102715253829956, -0.01833411678671837, -0.7030497193336487, -1.3134562969207764, -0.01687898114323616, 0.13917319476604462, -0.4297395646572113, -0.6084474921226501, -0.3752361536026001, 0.7645754218101501, 0.5186043381690979, 0.39027243852615356, -0.07171665132045746, 0.2964615225791931, 0.10040107369422913, -0.3890160620212555, 0.006029854994267225, 0.6411228775978088, 0.3630981743335724, 0.23382192850112915, -0.09176452457904816, 0.09229423105716705, 0.07628434896469116, 0.5055703520774841, -0.3426295518875122, -0.3684803545475006, -0.3052598834037781, -0.16669787466526031, -0.1469893753528595, 0.6832759380340576, 0.5815037488937378, 0.17328423261642456, -0.721222460269928, 0.221798375248909, 0.6115883588790894, -0.18558458983898163, -0.39703917503356934, -0.019713301211595535, 0.3579266369342804, -0.6284869313240051, 0.375323623418808, 0.3231208622455597, -0.48954644799232483, -0.8183862566947937, -0.9126133918762207, -0.7911419868469238, 0.4661775529384613, -0.5131212472915649, -0.6093720197677612, 1.063353180885315, -0.688836932182312, -0.23321709036827087, 0.05458969995379448, -0.6438215374946594, -0.48900362849235535, 0.6122549772262573, -1.1137934923171997, -0.33988434076309204, 0.10672963410615921, -0.4205605685710907, 0.008825212717056274, -0.36268383264541626, 1.4179010391235352, -0.04515673965215683, -0.3622209131717682, 0.09725530445575714, -0.4015198051929474, -0.4964725375175476, -0.3594541549682617, -0.27090582251548767, 1.1747615337371826, 0.5682123899459839, 0.15954925119876862, 0.09372957050800323, 0.08939863741397858, 0.04011628031730652, -1.0225626230239868, 0.07083740085363388, 0.5357421040534973, -0.7593271732330322, -0.26142409443855286, -0.8666613698005676, -0.8899245262145996, 0.27224770188331604, 0.36309292912483215, -0.26299774646759033, 0.3201432526111603, 0.06502016633749008, -0.4605591893196106, 0.003239296143874526, -0.6113041639328003, 0.08404373377561569, 0.3763687014579773, -0.5114471912384033, -0.38301289081573486, -0.3756045997142792, 0.47935032844543457, -1.2186082601547241, -0.12851424515247345, -0.3289530873298645, 0.48212364315986633, -0.27427396178245544, 1.306554913520813, -0.33982059359550476, 0.7966781258583069, 0.706737756729126, -0.25020110607147217, -0.22050470113754272, -0.26298007369041443, -0.6765057444572449, -0.5604413747787476, -0.2404438853263855, 0.38966453075408936, -0.1791893094778061, 0.6005386114120483, 0.5182915925979614, 0.11573448032140732, -0.43243658542633057, -0.6062369346618652, 0.24008908867835999, -0.39891505241394043, -0.5322506427764893, 0.3514758050441742, -0.15918006002902985, 0.21443387866020203, 0.1805175095796585, 0.043422237038612366, 0.6621202826499939, -0.46053189039230347, -0.3348301351070404, 0.34762945771217346, 0.013297145254909992, -0.5239118933677673, -0.4622740149497986, -0.8056321740150452, -1.4262537956237793, -0.3640071749687195, -0.9632763266563416, 0.008909545838832855, -0.3026590943336487, -0.29079166054725647, 0.2949707806110382, -0.47545748949050903, 0.23058874905109406, 0.1839524656534195, -0.13941402733325958, -0.5039794445037842, -0.5376313328742981, -0.6138068437576294, 0.6093862652778625, 0.6710005402565002, -0.7522676587104797, -0.3214646577835083, -0.010770480148494244, -0.2804652750492096, 0.48189854621887207, 0.5200464725494385, -0.35221755504608154, -0.2777971923351288, -1.237587571144104, 0.4168470501899719, 0.0335402637720108, -0.03655530884861946, -1.045851469039917, 1.104009747505188, 0.4003691077232361, -0.1477588564157486, -0.18927642703056335, 0.44345924258232117, -0.820403516292572, -0.6247929334640503, 0.5978524684906006, -0.6319984793663025, 0.14637669920921326, 0.6638980507850647, -0.5281572341918945, -0.07243315130472183, 0.5914607644081116, 0.049979764968156815, -0.9549465179443359, -1.0434911251068115, 0.2371038943529129, -0.5380527973175049, 0.2295137345790863, -0.3411349058151245, 0.04117609187960625, -1.0728836059570312, -0.02761826105415821, -0.07409366965293884, 0.0789051279425621, -0.6178528666496277, 0.7468600273132324, 0.6763445138931274, -0.823330819606781, 0.2108631730079651, 0.6603538990020752, -0.24166958034038544, 0.020133117213845253, 0.5555706024169922, 0.3255828022956848, -0.25871768593788147, 0.5040283799171448, 0.060343705117702484, 0.022206725552678108, -0.9734959602355957, 0.27042698860168457, 0.5572841167449951, -0.4977893531322479, -0.15577571094036102, 0.9690896272659302, -0.28129827976226807, -0.37455224990844727, -0.10643795132637024, -1.1486057043075562, -0.4417751729488373, -0.2932039499282837, 0.8446108102798462, 0.05621962994337082, 0.011360740289092064, -0.017939921468496323, -0.7640775442123413, 0.039779700338840485, -0.32359302043914795, -0.03515297919511795, 0.3057827949523926, 0.11625116318464279, -0.44993579387664795, 0.2792870104312897, 0.3544043004512787, -0.4839955270290375, -0.476474404335022, -0.7567770481109619, -0.4814320504665375, -0.14167053997516632, 0.28020796179771423, -0.11346308887004852, -0.6513283252716064, 0.653695821762085, 0.3585905134677887, 0.3032500743865967, 0.4049898087978363, -0.25286605954170227, 0.002606823807582259, 0.6565179824829102, 0.09313984215259552, 0.0024262352380901575, -0.3585875630378723, 1.6114472150802612, 1.296486735343933, -0.6758641004562378, 0.43762996792793274, -0.22231066226959229, -0.6305049061775208, 0.7109414339065552, 0.39153584837913513, -0.4713974893093109, 0.8069330453872681, 0.15982021391391754, -0.09013461321592331, 0.09472405165433884, -1.0454061031341553, -0.44400259852409363, 0.7659197449684143, 0.9591205716133118, 0.9261335730552673, 0.1741098165512085, 0.019800588488578796, 1.0722150802612305, 0.10994158685207367, 0.1544782519340515, 0.48805680871009827, 0.4094061255455017, -0.09250915795564651, -0.06019573286175728, -0.06234490126371384, 0.7452949285507202, -0.5222866535186768, -1.1907602548599243, 0.42764851450920105, 0.27055811882019043, 0.05640844255685806, 0.48709702491760254, 1.1003862619400024, -0.046822573989629745, 0.5286815166473389, -0.1987752616405487, 0.3433101773262024, -0.8308994174003601, -0.47220662236213684, -0.09300336241722107, -0.7960460782051086, -0.10753247141838074, -0.10501284897327423, -0.5818492770195007, -0.2679847180843353, -0.02722940221428871, 0.2954568564891815, -0.3136943280696869, 0.14693136513233185, 0.5687084794044495, 1.0272390842437744, 0.9698057770729065, -0.3728589713573456, -0.696319043636322, 0.028156844899058342, -0.9207691550254822, 0.1831936240196228, -0.9778869152069092, -0.10007141530513763, 0.12465211749076843, 0.053295839577913284, -0.113773413002491]}, "authors": [{"authorId": "4744742", "name": "Liqiang Lu"}, {"authorId": "2119287974", "name": "Yicheng Jin"}, {"authorId": "2040280585", "name": "Hangrui Bi"}, {"authorId": "2087056889", "name": "Zizhang Luo"}, {"authorId": "2152926482", "name": "Peng Li"}, {"authorId": "72259333", "name": "Tao Wang"}, {"authorId": "2117875640", "name": "Yun Liang"}], "references": [{"paperId": "a14434f80b063d202e9ef429fb13531bfe11e5ad", "title": "OMNI: A Framework for Integrating Hardware and Software Optimizations for Sparse CNNs"}, {"paperId": "ae464dc54a594de682fddd59479736b1e65bbf52", "title": "TENET: A Framework for Modeling Tensor Dataflow Based on Relation-centric Notation"}, {"paperId": "fc27f18b50a95b6c07ccb517cd5c385582d9a4cf", "title": "HASCO: Towards Agile HArdware and Software CO-design for Tensor Computation"}, {"paperId": "febf1120c258f1720ab4d32f7064c98214c985b2", "title": "An Efficient Hardware Design for Accelerating Sparse CNNs With NAS-Based Models"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "53439309acd147a51555dfcbe797beab652b25c5", "title": "Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise Sparsity"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "7c6c31412c5dad22543bb71e31620e8868d644a3", "title": "FTRANS: energy-efficient acceleration of transformers using FPGA"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "fcf1b4473a0af1f3ebc0fd556ee30c9309ff6345", "title": "SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training"}, {"paperId": "7d32ff65fb0e144ffdfc12540a0207c60f38df9c", "title": "Tensaurus: A Versatile Accelerator for Mixed Sparse-Dense Tensor Computations"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "b03cf6324ecf7a295a4aeae5970c88d1a1c3f336", "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "a3ef6ee560e93e6f58be2b28f27aed0eb86dc463", "title": "Fine-tune BERT with Sparse Self-Attention Mechanism"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "87a494fcc23aa9cea191f75e3b599e63d3bdab35", "title": "SMASH: Co-designing Software Compression and Hardware-Accelerated Indexing for Efficient Sparse Matrix Operations"}, {"paperId": "52d655cadb4ab977b951c1d57e740688f54032dd", "title": "Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs"}, {"paperId": "36c60a5f39f09e6b268d87718ffdc47e8dccc9cf", "title": "Boosting the Performance of CNN Accelerators with Dynamic Fine-Grained Channel Gating"}, {"paperId": "79a0c2069ee5575608eefc4976131c06d5d779ba", "title": "ShapeShifter: Enabling Fine-Grain Data Width Adaptation in Deep Learning"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "556c0d0a6fb1024a9853259bc860cba476af17e0", "title": "Maestro: A Memory-on-Logic Architecture for Coordinated Parallel Use of Many Systolic Arrays"}, {"paperId": "b67fd0612e3f72faee0fed9b1e930b69ed7ee98d", "title": "Sparse ReRAM Engine: Joint Exploration of Activation and Weight Sparsity in Compressed Neural Networks"}, {"paperId": "016f69185ef561d7dfadd2be9901957d55864bb7", "title": "SeerNet: Predicting Convolutional Neural Network Feature-Map Sparsity Through Low-Bit Quantization"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "9f9c4dd9a761a708cfcec6951ff67ce8953978c0", "title": "Bit-Tactical: A Software/Hardware Approach to Exploiting Value and Bit Sparsity in Neural Networks"}, {"paperId": "c8e20fa558be80be367c76b0f5487cdde250b832", "title": "An Efficient Hardware Accelerator for Sparse Convolutional Neural Networks on FPGAs"}, {"paperId": "fe7ceb03b12c0dbd50290be632dacdccac72af77", "title": "Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: Column Combining Under Joint Optimization"}, {"paperId": "9675ceedb1b0fcc3e5e83fcd4a649d326d04b051", "title": "A High-Speed and Low-Complexity Architecture for Softmax Function in Deep Learning"}, {"paperId": "2a84a30a489cd6f59725b80164b24c227d86c160", "title": "Cambricon-S: Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware Approach"}, {"paperId": "5e1f02ec3d9830c369facba2c141a344c566980d", "title": "PermDNN: Efficient Compressed DNN Architecture with Permuted Diagonal Matrices"}, {"paperId": "0682bfa5cca15726aab6c00ecfac91eb44379626", "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices"}, {"paperId": "3a62371be6c22131984da35c74a4a535dccc6768", "title": "SpWA: An Efficient Sparse Winograd Convolutional Neural Networks Accelerator on FPGAs"}, {"paperId": "a45c9dc026f10b1d19a2c98e1ddcb9c2f9a8abf8", "title": "Prediction Based Execution on Deep Neural Networks"}, {"paperId": "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "title": "Self-Attention Generative Adversarial Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "061edcbe54d23e6c8688525503ebc643c30d44a0", "title": "OuterSPACE: An Outer Product Based Sparse Matrix Multiplication Accelerator"}, {"paperId": "5f0da3cedda449b72fe36fa78798651a038f515c", "title": "MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects"}, {"paperId": "8899094797e82c5c185a0893896320ef77f60e64", "title": "Non-local Neural Networks"}, {"paperId": "efb1fc03a08db9fee8aa0e56ca079bda3f217fe7", "title": "Large-scale Cloze Test Dataset Created by Teachers"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "402f850dff86fb601d34b2841e6083ac0f928edd", "title": "SCNN: An accelerator for compressed-sparse convolutional neural networks"}, {"paperId": "b71ae4f14d329268baa5d280734054b449e6ea1b", "title": "ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA"}, {"paperId": "bc20f523a6e97800340e57a94d79926fce05572c", "title": "Cambricon-X: An accelerator for sparse neural networks"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "5ec594e9f5ca4b629be28625cd78c882514ea3be", "title": "Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks"}, {"paperId": "2e2b189f668cf2c06ebc44dc9b166648256cf457", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network"}, {"paperId": "ffdaa12ef011de9dbf43be46d45a3abcc8288965", "title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks"}, {"paperId": "91c587158d19fdaeda6b453efa54064543a03e9d", "title": "Tensor-matrix products with a compressed sparse tensor"}, {"paperId": "bd6507b5c9deaf87bda81e59ce15b2309df0bf37", "title": "ShiDianNao: Shifting vision processing closer to the sensor"}, {"paperId": "68837728232463651283edbb7ef0c93b2f502b2b", "title": "PuDianNao: A Polyvalent Machine Learning Accelerator"}, {"paperId": "4d8f2d14af5991d4f0d050d22216825cac3157bd", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"}, {"paperId": "4157ed3db4c656854e69931cb6089b64b08784b9", "title": "DaDianNao: A Machine-Learning Supercomputer"}, {"paperId": "31c36d445367ba204244bb74893c5654e31c3869", "title": "cuDNN: Efficient Primitives for Deep Learning"}, {"paperId": "22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd", "title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning"}, {"paperId": "665f89a20b05472d82df0a12f2dd63e8fcc4f3ea", "title": "Hidden factors and hidden topics: understanding rating dimensions with review text"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "d50d7cd78126cd4df5dda62f1a75c89d095bc8a5", "title": "Convolution engine: balancing efficiency & flexibility in specialized computing"}, {"paperId": "021464b67bb87cf6132b2eb5b0c4a61f31ec2775", "title": "Chisel: Constructing hardware in a Scala embedded language"}, {"paperId": null, "title": "NVIDIA"}, {"paperId": null, "title": "Oneapi-Src/oneDNN"}, {"paperId": null, "title": "Intel"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Show,AttendandTell:Neural"}, {"paperId": "31af4b8793e93fd35e89569ccd663ae8777f0072", "title": "The Netflix Prize"}, {"paperId": null, "title": "A.3.3 Software dependencies. Software experiments require CUDA SDK 10.1 or higher and Python 3.7 or higher. Other dependent Python packages are listed in requirements"}, {"paperId": null, "title": "MICRO \u201921, October 18\u201322, 2021, Virtual Event, Greece"}]}