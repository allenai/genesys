{"paperId": "d83f751e56af189402e3041e82562a342cf67aff", "title": "Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey", "abstract": "With the rapid growth in the volume of data sets, models, and devices in the domain of deep learning, there is increasing attention on large-scale distributed deep learning. In contrast to traditional distributed deep learning, the large-scale scenario poses new challenges that include fault tolerance, scalability of algorithms and infrastructures, and heterogeneity in data sets, models, and resources. Due to intensive synchronization of models and sharing of data across GPUs and computing nodes during distributed training and inference processes, communication efficiency becomes the bottleneck for achieving high performance at a large scale. This article surveys the literature over the period of 2018-2023 on algorithms and technologies aimed at achieving efficient communication in large-scale distributed deep learning at various levels, including algorithms, frameworks, and infrastructures. Specifically, we first introduce efficient algorithms for model synchronization and communication data compression in the context of large-scale distributed training. Next, we introduce efficient strategies related to resource allocation and task scheduling for use in distributed training and inference. After that, we present the latest technologies pertaining to modern communication infrastructures used in distributed deep learning with a focus on examining the impact of the communication overhead in a large-scale and heterogeneous setting. Finally, we conduct a case study on the distributed training of large language models at a large scale to illustrate how to apply these technologies in real cases. This article aims to offer researchers a comprehensive understanding of the current landscape of large-scale distributed deep learning and to reveal promising future research directions toward communication-efficient solutions in this scope.", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A comprehensive understanding of the current landscape of large-scale distributed deep learning is offered to offer researchers a comprehensive understanding of the current landscape and to reveal promising future research directions toward communication-efficient solutions in this scope."}, "embedding": {"model": "specter_v2", "vector": [0.008372091688215733, 0.18067137897014618, -0.2128039449453354, 0.14065313339233398, -0.3574756383895874, 0.1359458714723587, 0.8102373480796814, 0.04627583920955658, -0.6355141401290894, 0.2057022601366043, 0.09851362556219101, 0.14015109837055206, 0.35862448811531067, -0.1528175324201584, -0.4053144156932831, -0.13600412011146545, -0.9797277450561523, 0.670282781124115, 0.24894914031028748, 0.43495145440101624, -0.1558207869529724, -0.0701804906129837, -1.2278945446014404, 0.019539985805749893, 0.25566565990448, 1.0360617637634277, 0.21000145375728607, 1.0464506149291992, -0.36613139510154724, 0.3904443085193634, -0.09977438300848007, 0.23048089444637299, 0.24780169129371643, -0.17940956354141235, -0.46929407119750977, -0.28042739629745483, 0.3465525805950165, -0.6739457249641418, -0.6192433834075928, 0.8714516758918762, 0.19576293230056763, 0.2450655698776245, -0.05752916261553764, -0.7705259919166565, 0.3937610983848572, -0.061764609068632126, 0.2718164026737213, 0.6347653865814209, -0.5689815878868103, -0.720517635345459, 0.40802258253097534, -0.8561812043190002, 0.03747562691569328, 1.1728779077529907, 0.7145673632621765, 0.07850536704063416, -0.20313431322574615, -1.0039005279541016, -0.012059831991791725, -0.022492654621601105, -0.5828042030334473, -0.21291062235832214, -0.15989424288272858, -0.5029389262199402, 1.714564323425293, -0.2885531783103943, 0.024697840213775635, 0.3798893392086029, 0.18264257907867432, 0.6936797499656677, 0.16420628130435944, -0.7222310304641724, 0.16299889981746674, -0.26110371947288513, 0.4493308961391449, 0.6415727138519287, 0.16978538036346436, 0.06968508660793304, -1.181292176246643, -0.6011633276939392, 0.30746230483055115, 0.5005980134010315, 0.489140123128891, -0.11313869059085846, 0.03587782010436058, 0.7517704367637634, 0.21025684475898743, 0.04344996064901352, -0.43318411707878113, 1.266188144683838, 0.8766905665397644, 0.45036935806274414, 0.6823309063911438, -0.18967169523239136, -0.11287248134613037, 0.31413066387176514, -0.9725692868232727, -0.11818572133779526, 0.5464357137680054, 0.7555027008056641, -0.20059995353221893, 0.0015173774445429444, -0.11523149162530899, 0.39314451813697815, 0.9646694660186768, 0.22252745926380157, 0.5505303144454956, -0.5912357568740845, 0.46294793486595154, -0.7801286578178406, -0.16496440768241882, -0.5455955266952515, -0.21396833658218384, -0.2723537087440491, -1.3500330448150635, -0.7859737873077393, -0.8682250380516052, -0.376461386680603, -3.974688297603279e-06, 0.33642131090164185, -0.3712042272090912, 0.7911735773086548, 0.32085928320884705, 0.4584518373012543, 0.3115693926811218, 0.8563165068626404, -0.10555993765592575, 0.07026360929012299, 0.9386109709739685, -1.227628469467163, -0.3354364335536957, -0.919837236404419, 0.2308548241853714, -0.33832597732543945, -0.20758897066116333, -0.3472702205181122, -1.4019771814346313, -0.9208199977874756, -0.8331464529037476, 0.33529335260391235, -0.2804538607597351, -0.1581670343875885, 1.0484248399734497, 0.005325411446392536, -0.9812556505203247, 0.7134455442428589, -0.7308828234672546, -0.6749247908592224, 0.4248509407043457, 0.469918429851532, 0.47999390959739685, -0.0964358001947403, -0.9303026795387268, 0.23814193904399872, -0.07391432672739029, -0.8586329221725464, -0.6312347054481506, -0.26648932695388794, -0.2483007311820984, 0.08185163885354996, -0.3670196533203125, -0.9317191243171692, 1.1722315549850464, -0.31249138712882996, -1.0375170707702637, 0.501553475856781, 0.19670896232128143, -0.3130626976490021, 0.7750722169876099, 0.007633576635271311, -0.36380088329315186, 0.02871781960129738, -0.2797026038169861, 0.28712689876556396, 0.3900786340236664, 0.14492300152778625, -0.04408864676952362, -0.01664571277797222, -0.6776994466781616, -0.169638991355896, -0.6371903419494629, 1.2186431884765625, -0.8821683526039124, -0.24760392308235168, 0.33503100275993347, 0.416710764169693, -0.44794681668281555, 0.02499360777437687, -0.11849794536828995, -0.7061430811882019, 1.222880244255066, 0.07005518674850464, 0.3056205213069916, -0.863389253616333, -0.8790796995162964, 0.17526258528232574, 0.09767696261405945, -0.08322042226791382, -0.7057522535324097, 0.7138136029243469, -0.05116462707519531, -0.06094363331794739, 0.13572053611278534, -1.1389961242675781, -0.21515321731567383, 0.012918961234390736, -0.7955390214920044, 0.057994939386844635, -0.43535691499710083, 0.6728881001472473, -0.30393338203430176, 0.5145833492279053, -0.40949326753616333, 0.181773379445076, -1.0522822141647339, 1.6300232410430908, -0.492181658744812, 0.21608206629753113, -0.07357630133628845, -0.43051502108573914, 0.6498180031776428, -0.6544764041900635, 1.0225342512130737, -0.00228591775521636, 0.04496769607067108, 0.5603697299957275, -0.35602492094039917, 1.5074712038040161, -0.6585577726364136, 0.48657673597335815, 0.40702536702156067, -1.0324575901031494, 0.3878556787967682, 0.4402622878551483, 0.1657266467809677, -0.7299307584762573, 0.4450956881046295, 0.33086636662483215, -0.7510572075843811, 0.2689639627933502, 0.7112326622009277, 1.3559575080871582, -0.45451128482818604, 0.8152205944061279, 0.4902151823043823, -0.21106350421905518, 0.12490448355674744, 0.38081878423690796, 0.5944838523864746, -0.13665896654129028, -0.02710605040192604, -0.11529430001974106, -0.0065987431444227695, -1.2084280252456665, -0.4319850206375122, 0.7152044177055359, 0.5092560052871704, 0.4688449501991272, 0.596214234828949, -0.6324666738510132, -0.606063723564148, 0.05919848009943962, 0.84774249792099, 1.3054298162460327, 0.16622668504714966, 0.22492597997188568, -0.9303122162818909, -0.48266494274139404, 0.04466211795806885, -0.6056472063064575, 0.18715104460716248, 0.1405646800994873, -0.37722858786582947, -1.1437474489212036, 0.7689106464385986, 0.0960412546992302, 1.062758445739746, -0.5293748378753662, -0.464855432510376, -0.7428580522537231, 0.5067368745803833, -1.0636898279190063, -0.11671023815870285, 0.5844801664352417, -0.8258318305015564, -0.1504160761833191, -0.00167209398932755, -0.32861146330833435, 0.43771663308143616, -0.1889423131942749, 1.1234956979751587, -0.3433476388454437, -0.3784022331237793, 0.3311897814273834, 0.8342700004577637, -0.8762528896331787, -0.597331702709198, 0.11108119785785675, 0.07030189037322998, -0.26287519931793213, -0.04831016808748245, 0.13797834515571594, -0.0015234097372740507, -0.2036188244819641, -0.3768865764141083, 0.1426699161529541, -0.006258599925786257, -0.20926326513290405, 0.7264543771743774, -0.22156943380832672, 0.26680368185043335, -1.0843782424926758, 1.192866563796997, -0.4940798282623291, -0.8833098411560059, 0.021815646439790726, -0.597459077835083, -0.27573758363723755, 0.6035863161087036, -0.6381675601005554, -0.04610315337777138, -1.1303207874298096, 0.23051854968070984, -0.9125243425369263, -0.4697107672691345, 0.017414329573512077, 0.928809404373169, -0.2106151431798935, 0.10560757666826248, 0.4082816541194916, 0.9885037541389465, -0.29715052247047424, 0.1566261500120163, -0.45523881912231445, 0.28196367621421814, -0.07541892677545547, -0.2379806488752365, 0.03545099124312401, -0.09237229824066162, -0.6247963309288025, -0.19680708646774292, 0.018106447532773018, 0.01350395753979683, -0.23363295197486877, 0.14581701159477234, -0.8822593092918396, -0.5516253113746643, 0.16935046017169952, -1.4936000108718872, -0.4431677460670471, 0.2332155704498291, 0.18693619966506958, -0.12127072364091873, -0.9956536293029785, -1.571275234222412, -0.10313069820404053, -1.4548877477645874, -1.1938927173614502, 0.1304604560136795, 0.07637707889080048, -0.33597642183303833, -0.6830418109893799, 0.0032337759621441364, -0.5361437797546387, 1.1617854833602905, -0.6272410750389099, 0.524876594543457, -0.2583862245082855, -0.2947443723678589, -0.18577739596366882, -0.39007461071014404, 0.483811616897583, -0.6911560297012329, -0.03459756821393967, -0.7204232215881348, -0.2255629599094391, -0.48502546548843384, -0.7509652972221375, 0.04924110323190689, 0.39356258511543274, 1.3378263711929321, 0.28078964352607727, -0.2747640311717987, 0.8467336893081665, 1.3727214336395264, -0.9887236952781677, -0.31368789076805115, -0.2018750011920929, 0.7732559442520142, 0.003515036078169942, -0.8184468150138855, 0.8882439136505127, 0.03979862108826637, 0.5388280153274536, 0.2982051372528076, -0.6654080748558044, -0.6785076260566711, -0.1106388047337532, -0.023081453517079353, 1.968542456626892, 0.7589480876922607, 0.11720627546310425, -0.6660079956054688, 0.17626690864562988, -1.1268244981765747, -0.41892215609550476, 0.667361319065094, 0.8822848796844482, 0.2264857292175293, -0.3272991180419922, -0.22185398638248444, -0.5261266827583313, 0.7223449349403381, 0.5297622084617615, -0.4759477972984314, -0.8559702038764954, 0.2867256999015808, 0.8500622510910034, 0.559847891330719, 0.20715759694576263, -0.2616465985774994, 0.19786229729652405, 14.737428665161133, 1.0113877058029175, -0.2612493932247162, 1.3348586559295654, 0.6977135539054871, -0.04932105541229248, 0.16601423919200897, -0.4619717001914978, -0.7897321581840515, 0.4056077301502228, 1.6242026090621948, 0.11367778480052948, 0.499198853969574, 0.5473865866661072, -0.1370261162519455, -0.06797283887863159, -0.4136893153190613, 1.0072219371795654, 0.3175737261772156, -1.5035182237625122, 0.05998121201992035, 0.030813634395599365, 1.0613094568252563, 1.3453751802444458, 0.4770927429199219, 0.2994791269302368, 0.7358628511428833, -0.3554575741291046, 0.318332314491272, 0.25993582606315613, 1.271284818649292, -0.37089234590530396, 0.7359263300895691, 0.7328097820281982, -0.7532834410667419, 0.22542908787727356, -0.5994004011154175, -1.332966685295105, -0.023600418120622635, 0.4719467759132385, -0.04048740118741989, -0.3124498128890991, -0.117460235953331, 0.7134843468666077, 0.3930897116661072, 0.508603036403656, -0.15215224027633667, 0.6111806631088257, -0.407064288854599, -0.07559068500995636, -0.32312703132629395, 0.3172517418861389, -0.3693089485168457, -0.06338987499475479, -0.013510093092918396, -0.49343231320381165, 0.6226548552513123, 0.2028174251317978, -0.9250550270080566, -0.27162325382232666, -0.19226016104221344, 0.10660731047391891, 0.3140915632247925, 0.4246036112308502, 0.5585847496986389, -0.057061534374952316, -0.8359325528144836, 0.7632213830947876, 0.8489143252372742, 0.0526772141456604, -0.41667771339416504, -0.027698716148734093, 0.6855130195617676, -0.7963576912879944, -0.40617823600769043, 0.3396594822406769, -0.7567579746246338, -0.49315762519836426, -0.8310609459877014, -0.25131767988204956, 0.5106009244918823, -0.7834481596946716, -1.039692997932434, 1.0294499397277832, -0.5290452241897583, -0.041550084948539734, 0.6273053884506226, -0.42348167300224304, -0.41679471731185913, 0.871641993522644, -1.3333740234375, -0.29889681935310364, -0.06143226474523544, 0.06455996632575989, -0.34244289994239807, -0.37529924511909485, 1.260214924812317, 0.5080714821815491, -0.8101529479026794, 0.05208972468972206, 0.44102561473846436, -0.1131022721529007, -0.06177955120801926, -0.006418194621801376, 0.7354460954666138, 0.42890486121177673, -0.36827173829078674, -0.5685343146324158, -0.4081447422504425, 0.30744585394859314, -0.7313965559005737, -0.5859544277191162, 0.47152742743492126, -0.13588929176330566, -0.010151471011340618, -0.8300780653953552, -0.6769177317619324, 0.4254949986934662, 0.5489652752876282, 0.28493165969848633, 0.3384421467781067, -0.16010884940624237, -0.6322148442268372, -0.4393741488456726, -0.6010701060295105, -0.13736338913440704, 0.13236159086227417, -1.0320545434951782, 0.1323494166135788, 0.5687364339828491, 0.492681622505188, -0.8690105676651001, -0.5778146386146545, -0.3493961691856384, -0.25047555565834045, -0.15453791618347168, 1.0651025772094727, -0.15242990851402283, 0.6289899349212646, 1.2212787866592407, 0.020392784848809242, -0.4834885895252228, 0.6261752843856812, -0.9893995523452759, -0.018787236884236336, -0.19585846364498138, -0.08770540356636047, -0.8596875667572021, 0.5678889155387878, 0.4358840882778168, 0.35484597086906433, -0.20643554627895355, -0.6999968886375427, -0.16724397242069244, -0.04423876851797104, -0.43254199624061584, 0.36128970980644226, 0.06097738444805145, -0.3141865134239197, -0.30079004168510437, 0.31744101643562317, 0.7306997776031494, 0.10339538007974625, -0.3048321306705475, 0.8214696645736694, -0.2961699962615967, -0.3137953281402588, -0.6671699285507202, -0.37085962295532227, -1.69119131565094, 0.34024059772491455, -1.269426941871643, -0.27673909068107605, -0.554737389087677, -0.19023577868938446, -0.31546688079833984, -0.29879680275917053, -0.21396468579769135, 0.7881689071655273, -0.03070138581097126, -0.3397912383079529, 0.11954889446496964, -0.9341622591018677, 0.8286833167076111, 0.516560971736908, -0.1373073011636734, 0.23423725366592407, -0.24237266182899475, 0.9496108293533325, 0.5181294679641724, 0.6365915536880493, -0.7125931978225708, -0.666549801826477, -1.4682996273040771, -0.27575230598449707, 0.055461201816797256, 0.002788411220535636, -1.1076515913009644, 0.9715535640716553, 0.4067186415195465, -0.18011203408241272, 0.22560124099254608, 0.16249461472034454, -1.1580908298492432, -0.2065909504890442, 0.8031420111656189, -0.7209954857826233, 0.2509802579879761, 0.11921960860490799, -0.397972971200943, -0.1753755509853363, 0.9517200589179993, 0.02057092823088169, -0.22167855501174927, -0.6810905933380127, 0.44788625836372375, -0.5904945731163025, 0.32047370076179504, 0.3952668309211731, 0.2915083169937134, -1.450500726699829, -0.2166679948568344, -0.19163841009140015, 0.08323630690574646, -0.4831289052963257, 0.686058759689331, -0.10016485303640366, -0.8598892688751221, 0.19811402261257172, 0.5718966126441956, -0.5627234578132629, 0.0022958319168537855, 0.53679358959198, 0.8474277257919312, -1.0488183498382568, 0.25451305508613586, 0.06347154825925827, 0.19563046097755432, -0.9394148588180542, -0.1517171412706375, 0.903730571269989, -0.3241390883922577, -0.2794375717639923, 1.0377211570739746, -0.5442620515823364, -1.0217336416244507, 0.4362523555755615, -0.5451444387435913, -0.06268657743930817, -0.34269243478775024, 0.35773396492004395, 0.42879536747932434, 0.17896458506584167, 0.6735941171646118, -0.36275798082351685, -0.026625264436006546, 0.3929380774497986, -0.07207927852869034, 0.4804275333881378, -0.13180622458457947, -0.9253172874450684, 0.48219916224479675, 0.86235111951828, -0.6368749737739563, -0.9847931861877441, -1.0092495679855347, -0.5021529197692871, -0.10674557834863663, 0.39260298013687134, -0.13715766370296478, -1.3005218505859375, 0.7384433746337891, 0.5664275884628296, 0.39458322525024414, 0.35330304503440857, 0.17368000745773315, 0.5127476453781128, 0.35407641530036926, 0.35560524463653564, -0.4579772353172302, -0.3921660780906677, 1.2429392337799072, 1.0090363025665283, -0.8571592569351196, 0.42446237802505493, -0.2868492007255554, -0.14069518446922302, 0.9399648308753967, 0.3376929759979248, -0.29087746143341064, 1.2399630546569824, 0.3141316771507263, -0.03413413092494011, 0.19625018537044525, -1.118403673171997, 0.16534924507141113, 0.05955195426940918, 0.3718538284301758, 0.4816622734069824, 0.3769857585430145, -0.19125190377235413, 0.8937999606132507, 0.39827480912208557, -0.08171746134757996, 0.05908774212002754, 0.6174387335777283, 0.047076527029275894, 0.14649488031864166, -0.22149887681007385, 0.38533371686935425, -0.7274681925773621, -0.2954147458076477, 0.17627419531345367, 0.2810804545879364, 0.06730829179286957, 0.7907491326332092, 1.4854834079742432, 0.052582722157239914, 0.1807003617286682, -0.4088069200515747, 0.28261467814445496, -0.5153718590736389, -0.4626370966434479, -0.08405090123414993, -0.6032686829566956, -0.4127459228038788, -0.09537947177886963, 0.011946241371333599, -0.38137122988700867, -1.0340526103973389, 0.6993863582611084, 0.3347262740135193, 0.8373997807502747, 0.7437924146652222, 1.169285774230957, 1.0651757717132568, -0.06245001032948494, -0.9287980794906616, -0.5812434554100037, -0.7246938347816467, -0.26787349581718445, -0.40681886672973633, -0.5964635610580444, 0.4622507393360138, -0.05076117813587189, -0.6321423649787903]}, "authors": [{"authorId": "2295732747", "name": "Feng Liang"}, {"authorId": "2295793981", "name": "Zhen Zhang"}, {"authorId": "2115608328", "name": "Haifeng Lu"}, {"authorId": "2264958744", "name": "Victor C. M. Leung"}, {"authorId": "2296218297", "name": "Yanyi Guo"}, {"authorId": "2264954424", "name": "Xiping Hu"}], "references": [{"paperId": "2896f07cb07a0c519f25e7c645014c6fbfd71ba4", "title": "Synchronize Only the Immature Parameters: Communication-Efficient Federated Learning By Freezing Parameters Adaptively"}, {"paperId": "02af185257ddf835d49f821f1cf2f099b774d98e", "title": "YOGA: Adaptive Layer-Wise Model Aggregation for Decentralized Federated Learning"}, {"paperId": "929279fb0536bcf883457c69d0ce0eb28e7b1915", "title": "ResFed: Communication0Efficient Federated Learning With Deep Compressed Residuals"}, {"paperId": "95e218a72c9942f9f60fe209f8afdbce5c43a7d1", "title": "Accelerating Federated Learning With Data and Model Parallelism in Edge Computing"}, {"paperId": "bb17b201d9f3411707cb7374fd19bdb74719087e", "title": "FAST: Enhancing Federated Learning Through Adaptive Data Sampling and Local Training"}, {"paperId": "883b5a6cbbf3c499c8402204a657abf1e836d310", "title": "Deep Learning Workload Scheduling in GPU Datacenters: A Survey"}, {"paperId": "30c493ed26e38786e7ddfbec279fa121566b198f", "title": "A Stochastic Approach for Scheduling AI Training Jobs in GPU-Based Systems"}, {"paperId": "10cb1af640c855c17dd9e66d34be91a92b0d3c72", "title": "ASHL: An Adaptive Multi-Stage Distributed Deep Learning Training Scheme for Heterogeneous Environments"}, {"paperId": "174f881f77cb7d7adfadb956e00193d927ac06f6", "title": "Communication Optimization Algorithms for Distributed Deep Learning Systems: A Survey"}, {"paperId": "250dc8f7059e4a23985c06edd7b5b2ca9291a693", "title": "Topologies in distributed machine learning: Comprehensive survey, recommendations and future directions"}, {"paperId": "d6a1cb0abd8f082498634e5e7ee5e5fa5bf8170a", "title": "To Transmit or Predict: An Efficient Industrial Data Transmission Scheme With Deep Learning and Cloud-Edge Collaboration"}, {"paperId": "8fae24ba59f06b5097ed4f8d6e2f2efde144280a", "title": "Towards GPU Memory Efficiency for Distributed Training at Scale"}, {"paperId": "3b2ba249a942209525c2c6ef6131b0364b0aca3f", "title": "Adaptive Control of Local Updating and Model Compression for Efficient Federated Learning"}, {"paperId": "628e918632edb5f60ada2cb6562799a97205ca9f", "title": "GRID: Gradient Routing With In-Network Aggregation for Distributed Training"}, {"paperId": "e97addc2c9d137ca53a73d41ad59083c1a4cf214", "title": "Oobleck: Resilient Distributed Training of Large Models Using Pipeline Templates"}, {"paperId": "795c9f8571f01162eefd30c4773d95c47cb6025c", "title": "Communication compression techniques in distributed deep learning: A survey"}, {"paperId": "56ee5f6b57f2d93c4fea55b696236a92fc2dcb83", "title": "FedDD: Toward Communication-Efficient Federated Learning With Differential Parameter Dropout"}, {"paperId": "dda9e433ac572636b57d05c1a1bf529fb4df2973", "title": "FaST-GShare: Enabling Efficient Spatio-Temporal GPU Sharing in Serverless Computing for Deep Learning Inference"}, {"paperId": "63c1052d76098022b1d3a3811e1bfcaca9077bca", "title": "Hydra: Deadline-Aware and Efficiency-Oriented Scheduling for Deep Learning Jobs on Heterogeneous GPUs"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "4c55dd37ab4c41565226a3c8194166d7412b0a02", "title": "Large Language Models Empowered Autonomous Edge AI for Connected Intelligence"}, {"paperId": "74bd049ccab069131f0ff16968a73765262b34d4", "title": "Accelerating Federated Learning With Cluster Construction and Hierarchical Aggregation"}, {"paperId": "6f36cc547e9465c749f765d2434e08d8a19173f0", "title": "Mapless Navigation With Safety-Enhanced Imitation Learning"}, {"paperId": "2880f2ad2a331332be17cad1de77c7c89d50bba6", "title": "OSP: Boosting Distributed Model Training with 2-stage Synchronization"}, {"paperId": "78e826afdfea6c10d73bb963f84a217c33a031c1", "title": "Software-Hardware Co-design of Heterogeneous SmartNIC System for Recommendation Models Inference and Training"}, {"paperId": "64e4aff81788ca0b31d94a7d592a1870ffa2389d", "title": "GOAT: Gradient Scheduling with Collaborative In-Network Aggregation for Distributed Training"}, {"paperId": "343867970e6feea44fb134244168d04f0f3d164b", "title": "Enabling Switch Memory Management for Distributed Training with In-Network Aggregation"}, {"paperId": "15822ae7035dec57b3126dac84386a415cb4d59e", "title": "Lyra: Elastic Scheduling for Deep Learning Clusters"}, {"paperId": "d3562d9e310873f49a3f10c93dc1db2e4e0b93ec", "title": "A2TP: Aggregator-aware In-network Aggregation for Multi-tenant Learning"}, {"paperId": "ac54fee4b4160e6cdf56fa95652a102abacf91d5", "title": "Accelerating Distributed DNN Training via Transport Layer Scheduling"}, {"paperId": "bb4720ca4cc773e3ad09415e8c49a46418418125", "title": "Enhancing Decentralized Federated Learning for Non-IID Data on Heterogeneous Devices"}, {"paperId": "7a945907336c80a6f5df34664afbcfe62d61f0ba", "title": "Decentralized P2P Federated Learning for Privacy-Preserving and Resilient Mobile Robotic Systems"}, {"paperId": "5a6e344dfa48e20cec414fb0691698beefa8cd3e", "title": "Communication-Efficient Distributed Learning: An Overview"}, {"paperId": "f9b9f93aa9ba91fbfe03c926abad42d49f33e98b", "title": "DFS: Joint data formatting and sparsification for efficient communication in Distributed Machine Learning"}, {"paperId": "f8ae23caf5047f21acd178496da29d71a10602bd", "title": "Elastic Resource Management for Deep Learning Applications in a Container Cluster"}, {"paperId": "c59abe4ec214b34364a87a9aa0e4a53e7804461e", "title": "Accelerating Wireless Federated Learning via Nesterov's Momentum and Distributed Principle Component Analysis"}, {"paperId": "83edcfbb206ddad38a971d605da09390604248ea", "title": "BloombergGPT: A Large Language Model for Finance"}, {"paperId": "74e3ce19755b1d812573d42dac54c08d3f972b9f", "title": "In-Network Aggregation with Transport Transparency for Distributed Training"}, {"paperId": "116f63722179f665696d8720ddc3911d80fd7b78", "title": "Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging"}, {"paperId": "f8e95d07e78369d3b792753734a58d1c14dd2f24", "title": "A deep-learning algorithm to classify skin lesions from mpox virus infection"}, {"paperId": "e14403acf5e5602bf8d566336d909b31cb717b04", "title": "GossipFL: A Decentralized Federated Learning Framework With Sparsified and Adaptive Communication"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "3822cb0a089a66f2ad88e7960fcae6ebdc4e4427", "title": "DeAR: Accelerating Distributed Deep Learning with Fine-Grained All-Reduce Pipelining"}, {"paperId": "34e14fdf011987ecf8abac4dc18c776e1554c51c", "title": "FSP: Towards Flexible Synchronous Parallel Frameworks for Distributed Machine Learning"}, {"paperId": "2359f07311c32f64ae5f7a701a8490c69be90f82", "title": "Achieving Lightweight and Privacy-Preserving Object Detection for Connected Autonomous Vehicles"}, {"paperId": "ffdd341df1e84caca4d717959af2a82d009aa14c", "title": "Signal Detection in GSM-Based In-Band Full-Duplex Communication Using DNN"}, {"paperId": "5278b81db686b4d36143941bff1c683bea963a63", "title": "SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient"}, {"paperId": "3e9e607210e278d33f5b32b340bb5eb7482dfef5", "title": "MSCCLang: Microsoft Collective Communication Language"}, {"paperId": "fc426ebe45e0b7dac5052947d7b482847a197da8", "title": "Tereis: A Package-Based Scheduling in Deep Learning Systems"}, {"paperId": "e6a5fc5c430e2ac0d3a4cc318bc2fd5d281881e4", "title": "Cloud-IIoT-Based Electronic Health Record Privacy-Preserving by CNN and Blockchain-Enabled Federated Learning"}, {"paperId": "637c212a287c394e50710ccd1f2f8e45677fe793", "title": "Task-Oriented Communications for NextG: End-to-end Deep Learning and AI Security Aspects"}, {"paperId": "d06f82ffeb481027571935bef019f7bc8bf22d1a", "title": "Communication-Efficient Federated Learning for Heterogeneous Edge Devices Based on Adaptive Gradient Quantization"}, {"paperId": "10166aba7157880473a264f87a96c33e8fe24589", "title": "Distributed Real-Time Scheduling in Cloud Manufacturing by Deep Reinforcement Learning"}, {"paperId": "4a09fdb268a342198279a74ddfe1ed7150e89e20", "title": "PS+: A Simple yet Effective Framework for Fast Training on Parameter Server"}, {"paperId": "22bc6d8c6fe96758ccf7099fcf482fce5525889f", "title": "Titan: a scheduler for foundation model fine-tuning workloads"}, {"paperId": "d54ebb7b83e81013dc75e12b31fd9e29e587cf4c", "title": "iGniter: Interference-Aware GPU Resource Provisioning for Predictable DNN Inference in the Cloud"}, {"paperId": "f70719547a33377ac22cfc106768233c45278eba", "title": "In-network aggregation for data center networks: A survey"}, {"paperId": "5ada70309e4a16b90994546c954b2ff4ece7c9fd", "title": "Solving task scheduling problems in cloud manufacturing via attention mechanism and deep reinforcement learning"}, {"paperId": "e2f20d95928dcaa2293a0970e431adbf3d1635e0", "title": "Efficient Online Scheduling for Coflow-Aware Machine Learning Clusters"}, {"paperId": "fc3375362be027c3ec411b399a377b29dee878cd", "title": "Adaptive synchronous strategy for distributed machine learning"}, {"paperId": "1ad7a323cebbd186a6f3c9a1f0caf0af94ce91bd", "title": "HammingMesh: A Network Topology for Large-Scale Deep Learning"}, {"paperId": "baa467a4dccf87bc7e2c5a4ea6fd5e401d962d39", "title": "AutoPipe: A Fast Pipeline Parallelism Approach with Balanced Partitioning and Micro-batch Slicing"}, {"paperId": "692d4af64f8ab534134591a043fbb748b669b8cb", "title": "AC-SGD: Adaptively Compressed SGD for Communication-Efficient Distributed Learning"}, {"paperId": "e66384bfc81d0c9357ce5e692a1b42a30bbe48e1", "title": "Adaptive and Efficient GPU Time Sharing for Hyperparameter Tuning in Cloud"}, {"paperId": "84ac0401115752b9e5c67f550f489307ab2859e6", "title": "HSP: Hybrid Synchronous Parallelism for Fast Distributed Deep Learning"}, {"paperId": "007737b9ed27743944263d42763f00262e368502", "title": "Communication-Efficient Federated Learning with Adaptive Quantization"}, {"paperId": "192bafbddba47ffc89b7201473ca640240727955", "title": "A Framework for Neural Network Inference on FPGA-Centric SmartNICs"}, {"paperId": "ff749d42b2822b6124f404c19c8bf5fd904f8cd9", "title": "Sniper: cloud-edge collaborative inference scheduling with neural network similarity modeling"}, {"paperId": "7bc444a2063d2334fb8f0a66b363dc13ec1b71b0", "title": "AMBLE: Adjusting mini-batch and local epoch for federated learning with heterogeneous devices"}, {"paperId": "b3ef89e0ff008bdad2fd4ea247a4b30424ed2441", "title": "OSDL: Dedicated optical slice provisioning in support of distributed deep learning"}, {"paperId": "d479efb49f3aae2ab1b3a0b89755a5a7af40fdf2", "title": "Sharper Convergence Guarantees for Asynchronous SGD for Distributed and Federated Learning"}, {"paperId": "72127bcfd54da6281874f689ccd7ad8edac7e54e", "title": "Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays"}, {"paperId": "6b117a8dcaa161562b0a69afbb9811e11afb5b3e", "title": "Decentralized Training of Foundation Models in Heterogeneous Environments"}, {"paperId": "b2bc82192fdbb58d0d244768addac5fa93c9962e", "title": "Beamer: Stage-Aware Coflow Scheduling to Accelerate Hyper-Parameter Tuning in Deep Learning Clusters"}, {"paperId": "9cbe8abf59c7a2717cfcb78c96e5bd5dcac7ba3e", "title": "A privacy-preserving content-based image retrieval method based on deep learning in cloud computing"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "533beb3cc4b99f05ac2f3baae90e1bab07fd93a2", "title": "Out-of-order backprop: an effective scheduling technique for deep learning"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "1ff32b615497085a999f17192a09914628ebf77c", "title": "The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining"}, {"paperId": "4c397c32fe121f32445bb5ca90e3a8d07b9e785e", "title": "A Full Dive Into Realizing the Edge-Enabled Metaverse: Visions, Enabling Technologies, and Challenges"}, {"paperId": "d4a06a8ce3df7165c18abdd02129b46c60af945b", "title": "HealthCloud: A system for monitoring health status of heart patients using machine learning and cloud computing"}, {"paperId": "fdacbdcc5e998c250bfc8ea00bda34d898a7798b", "title": "FederatedGrids: Federated Learning and Blockchain-Assisted P2P Energy Sharing"}, {"paperId": "2d9c43e1133f17c73408774840c93382f3cc8f84", "title": "TopoOpt: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "9e4a8177823d044deca512f5849a3f61af553a15", "title": "Near-optimal sparse allreduce for distributed deep learning"}, {"paperId": "754fbf0b104717641f5bdac9c6efdfb7474e24f8", "title": "Spatiotemporal Costmap Inference for MPC Via Deep Inverse Reinforcement Learning"}, {"paperId": "d1e3f04fb2c6a84c35d1ee3be6cb5fbd2b029043", "title": "Dynamic GPU Energy Optimization for Machine Learning Training Workloads"}, {"paperId": "8b67d9535efd60194d37f6c76b67d53bd8015be5", "title": "Horus: Interference-Aware and Prediction-Based Scheduling in Deep Learning Systems"}, {"paperId": "88b1ebd97bf135bffca3f14085a8a927b4d9474f", "title": "Performance Optimization of Allreduce Operation for Multi-GPU Systems"}, {"paperId": "157bddb85786b980c962e649bed33d8ebbf7b4a1", "title": "Edge-Cloud Polarization and Collaboration: A Comprehensive Survey for AI"}, {"paperId": "cb1cc03b04079b89a1f42d736cba4d2ee3e4f7c3", "title": "TACCL: Guiding Collective Algorithm Synthesis using Communication Sketches"}, {"paperId": "43332a71939ae7f3bd4756cba2c5ef0763b5cfac", "title": "Varuna: scalable, low-cost training of massive deep learning models"}, {"paperId": "e581a00152e3764d56277a409bc2c46f6092c484", "title": "Chronus: A Novel Deadline-aware Scheduler for Deep Learning Training Jobs"}, {"paperId": "5b8f5a218ceecf164886135741a7416553dce7e2", "title": "Morphling: Fast, Near-Optimal Auto-Configuration for Cloud-Native Model Serving"}, {"paperId": "eeb66c3463857244d8e8efff8922757ea509de63", "title": "Decentralized Control of Quadrotor Swarms with End-to-end Deep Reinforcement Learning"}, {"paperId": "ac35dffd21c16b02e140a36726b3a21d266cab0f", "title": "Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters"}, {"paperId": "d9548bd87381749c8822d34abc786de8c7e305f7", "title": "Prophet: Speeding up Distributed DNN Training with Predictable Communication Scheduling"}, {"paperId": "c0c36424f691c9d934aee7b9c6c6c86429a57b45", "title": "SiP-ML: high-bandwidth optical network interconnects for machine learning training"}, {"paperId": "0738451518ae3f6941261230d3143f5e61404cca", "title": "Rethinking gradient sparsification as total error minimization"}, {"paperId": "10f3ca78e194552427ebe9173b19d1b910469e27", "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"}, {"paperId": "d79e8947f481c8ad5703611964a9f6c9bddce551", "title": "Communication-Efficient Federated Learning with Adaptive Parameter Freezing"}, {"paperId": "3d2861d4fa3816afb672014eaee03978b965a1e0", "title": "Asynchronous Stochastic Optimization Robust to Arbitrary Delays"}, {"paperId": "fecfd8fe8b6ad03ac81467a838d8878ee650de48", "title": "Privacy-Preserved Federated Learning for Autonomous Driving"}, {"paperId": "cf30fb61a5943781144c8442563e3ef9c38df871", "title": "Training Graph Neural Networks with 1000 Layers"}, {"paperId": "25890b3be92e8037585377320fbb09c0c679595c", "title": "Communication-efficient SGD: From Local SGD to One-Shot Averaging"}, {"paperId": "88d21b9a5ae3aeac6a509c6a19f84d7ab39ad964", "title": "A Taxonomy and Survey of Edge Cloud Computing for Intelligent Transportation Systems and Connected Vehicles"}, {"paperId": "9591957bb641996aab63cd2f483b54308cdb22af", "title": "A Survey of Deep Learning Techniques for Cybersecurity in Mobile Networks"}, {"paperId": "cc4294064134d32a075048a43be698ce6832cfab", "title": "A Unified Framework for Planning with Learned Neural Network Transition Models"}, {"paperId": "546264399f3914e87f1ad6aaf2f1a132aba6073c", "title": "A Sum-of-Ratios Multi-Dimensional-Knapsack Decomposition for DNN Resource Scheduling"}, {"paperId": "1f47fc5d783e517147f66c4c1a1465e8bab35c63", "title": "Pervasive AI for IoT Applications: A Survey on Resource-Efficient Distributed Artificial Intelligence"}, {"paperId": "f938cffd498ffb81ee9d66b4cd473e82c2e12c72", "title": "From distributed machine learning to federated learning: a survey"}, {"paperId": "a3893072ac8aaaa01cbd754a59fabc4d6c1adb2c", "title": "Communication-efficient federated learning"}, {"paperId": "181198db16562aec7d0409871345027b406242a9", "title": "ScaleCom: Scalable Sparsified Gradient Compression for Communication-Efficient Distributed Training"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "a025ad05916ddf3281a2bca0cdffbaed99a2c9dd", "title": "Federated Learning Meets Blockchain in Edge Computing: Opportunities and Challenges"}, {"paperId": "16a7dcb0332e02c502174a6210e8cfcde9f8762a", "title": "Semi-Decentralized Federated Learning With Cooperative D2D Local Model Aggregations"}, {"paperId": "43db70f525556f868559a1e71e632612fb57dcd0", "title": "Learned Gradient Compression for Distributed Deep Learning"}, {"paperId": "fdf8681679cf89f9f7b1f0cc560b7cc950e97b09", "title": "Distributed Deep Learning for Remote Sensing Data Interpretation"}, {"paperId": "2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6", "title": "Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"}, {"paperId": "68faf57a657e6e4e73a912a00cfe4245571b1c1d", "title": "Cross-Gradient Aggregation for Decentralized Learning from Non-IID data"}, {"paperId": "7b39b5f1e7b0822a627b1b2c3a37dbdeaa383f0e", "title": "GSSP: Eliminating Stragglers Through Grouping Synchronous for Distributed Deep Learning in Heterogeneous Cluster"}, {"paperId": "1836c0c53bc8c77aa56d45834c3a3ff910089bc2", "title": "IntSGD: Adaptive Floatless Compression of Stochastic Gradients"}, {"paperId": "994a75c50c0ab67f43250f2ead45cd841c378e28", "title": "EGC: Entropy-based gradient compression for distributed deep learning"}, {"paperId": "4066d78b637c2b8e57de5ffd53950134a551de85", "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed"}, {"paperId": "8b2e1137639aa764aa3c6ecbd1c1aa9787126c7e", "title": "An Efficient Statistical-based Gradient Compression Technique for Distributed Training Systems"}, {"paperId": "f030656c4441093852ca0ad0d533b156a7c8a884", "title": "FedHome: Cloud-Edge Based Personalized Federated Learning for In-Home Health Monitoring"}, {"paperId": "ce5ff3b5f8595ed10155f6626e7127aa9ef0ac66", "title": "A High-Throughput, Resource-Efficient Implementation of the RoCEv2 Remote DMA Protocol for Network-Attached Hardware Accelerators"}, {"paperId": "6d0c42fb3fe2160708b8afe4d130521e46fb902c", "title": "Communication optimization strategies for distributed deep neural network training: A survey"}, {"paperId": "af9806c22e8b78d9d995c5f8f85cf7bf367a873a", "title": "Job scheduling for large-scale machine learning clusters"}, {"paperId": "a9bebc22b682906a9ebb4d4cce4b31d230131385", "title": "Adaptive Gradient Quantization for Data-Parallel SGD"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "f57db358459390590bc838663025dae0f8d51ebf", "title": "Towards Scalable Distributed Training of Deep Learning on Public Cloud Clusters"}, {"paperId": "9d64a1dc36c851a334bd60e0721588af443adb63", "title": "GSLICE: controlled spatial sharing of GPUs for a scalable inference platform"}, {"paperId": "ff56d18c46b6aa3c4fc03927ee0402ae0410522c", "title": "Interlocking Backpropagation: Improving depthwise model-parallelism"}, {"paperId": "cc7c2b1e6eb6475f066982ba21a4cbb0e75a1acf", "title": "TensorExpress: In-Network Communication Scheduling for Distributed Deep Learning"}, {"paperId": "54a0ae308d380106f7417f62b196d93d56cfb839", "title": "Semisupervised Distributed Learning With Non-IID Data for AIoT Service Platform"}, {"paperId": "68a439902fd7d8005222f75d354a41fba6d60741", "title": "Efficient sparse collective communication and its application to accelerate distributed deep learning"}, {"paperId": "6de9eb6cb68185dd311be3093375527e5b56330d", "title": "Learning Latent Representation for IoT Anomaly Detection"}, {"paperId": "1dcd3ce2221eff4213487249978f7ec844f1c611", "title": "Communication-efficient Decentralized Machine Learning over Heterogeneous Networks"}, {"paperId": "23fd84cb4422850dcbae301a53cd51ea25facd03", "title": "An Overview of Efficient Interconnection Networks for Deep Neural Network Accelerators"}, {"paperId": "2adcdc9e9e81147499e1372f992d25ac6265fb29", "title": "Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning"}, {"paperId": "461a7bf0c14df0fb08d3c59bdec491778a861270", "title": "Synthesizing optimal collective algorithms"}, {"paperId": "7c064f21163f085476b65cc1f12bd4f823c7b99c", "title": "Distributed Training and Inference of Deep Learning Models for Multi-Modal Land Cover Classification"}, {"paperId": "2f4d6d3748ac6822711fe0bbd4cf6d2e66fa6613", "title": "Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads"}, {"paperId": "fd2a2caefd0c986701aa676d14839c723e570edf", "title": "E-LAS: Design and Analysis of Completion-Time Agnostic Scheduling for Distributed Deep Learning Cluster"}, {"paperId": "4b6661347d5b58250130b89145dbd34ce310f2a0", "title": "Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization"}, {"paperId": "23d69b7b5eb1c7c2423d213c8ccbdad96c329918", "title": "Geryon: Accelerating Distributed CNN Training by Network-Level Flow Scheduling"}, {"paperId": "9222062c20aa6a7fad8899da0453269fb804d6ac", "title": "Communication-Efficient Distributed Deep Learning with Merged Gradient Sparsification on GPUs"}, {"paperId": "31f8c61248d6994c6f53942436756356cbefc332", "title": "Automating Cloud Deployment for Deep Learning Inference of Real-time Online Services"}, {"paperId": "3488b8dfa3d98b4e4f999ecc7753dabc77d096a0", "title": "FFT-based Gradient Sparsification for the Distributed Training of Deep Neural Networks"}, {"paperId": "49a049dc85e2380dde80501a984878341dd8efdf", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"}, {"paperId": "4d622c861cb481abef684e7a9db90dc555631505", "title": "A Scalable, High-Performance, and Fault-Tolerant Network Architecture for Distributed Machine Learning"}, {"paperId": "20438e2a38a0c4723fbd9de50b44b7335f6f43cb", "title": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "9d9dbb4487aca2b62ca3659446d7010ac65aa642", "title": "HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism"}, {"paperId": "ee81584f7e426b891765fe122655fe466b00a072", "title": "SecureNLP: A System for Multi-Party Privacy-Preserving Natural Language Processing"}, {"paperId": "7c8351ac6e11327a12cb7a0c24debd3a4ce81b7d", "title": "JPAS: Job-progress-aware flow scheduling for deep learning clusters"}, {"paperId": "a87900762e93ff3448fc5468dd35b7983ca79070", "title": "Distributed Stochastic Gradient Descent with Event-Triggered Communication"}, {"paperId": "94580a0ae3f4b2fe0ab8ae0359c1afbb6ab50dc5", "title": "HierTrain: Fast Hierarchical Edge AI Learning With Hybrid Parallelism in Mobile-Edge-Cloud Computing"}, {"paperId": "b6e8d81bc70af67a71d288563f5eb0daf67efc3b", "title": "DNN-based Localization from Channel Estimates: Feature Design and Experimental Results"}, {"paperId": "e6196f0fb4867d2f2186c6df666ae687689f174a", "title": "PLink: Discovering and Exploiting Locality for Accelerated Distributed Training on the public Cloud"}, {"paperId": "69170faf00279bdcdfd91290a3756d539f9fb6e6", "title": "Communication-Efficient Distributed Deep Learning: A Comprehensive Survey"}, {"paperId": "47a8355e76c3675c481f135cce3a5911c74aeac3", "title": "Communication-Efficient Edge AI: Algorithms and Systems"}, {"paperId": "a6e731a3b7dc4f34a95201c37cc4309ab92318f0", "title": "Is Local SGD Better than Minibatch SGD?"}, {"paperId": "da93d65ce761de78992989dae8d2af8aeeada54d", "title": "Adaptive Gradient Sparsification for Efficient Federated Learning: An Online Learning Approach"}, {"paperId": "67b6fa1b19113619a70eb443f056ffdabaa92ca7", "title": "Deep Learning Research and Development Platform: Characterizing and Scheduling with QoS Guarantees on GPU Clusters"}, {"paperId": "f9a855ae59579d16dca6a5133cd8daddd3305582", "title": "A Survey on Distributed Machine Learning"}, {"paperId": "3ea5bde61362d0dfcbe2751da8dd380adc9f7945", "title": "Swift machine learning model serving scheduling: a region based reinforcement learning approach"}, {"paperId": "c125df9f9ea11890f049477a265be00f13d7fad1", "title": "Energy Efficient Federated Learning Over Wireless Communication Networks"}, {"paperId": "1e009f755503bffd7644fcd0a45939c54b838b37", "title": "BlueConnect: Decomposing all-reduce for deep learning on heterogeneous network hierarchy"}, {"paperId": "cb340ba0b1b3c56e5003bfeb51ab6e4b60148364", "title": "Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "8b9b2f1ec32040c30d2c296ac1688a6627a8fc12", "title": "Nexus: a GPU cluster engine for accelerating DNN-based video analysis"}, {"paperId": "8221b23813858ee536997385097c1ef611184346", "title": "Blink: Fast and Generic Collectives for Distributed ML"}, {"paperId": "e20848622d5145744127b82367f9b671c7ddb08e", "title": "SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum"}, {"paperId": "b213b1a2a80e4d1fb0daeabbdd7196ee4b745fae", "title": "Optimizing on-demand GPUs in the Cloud for Deep Learning Applications Training"}, {"paperId": "2734952fe7a88de7cfb16be2dba3cb655c87022d", "title": "FfDL: A Flexible Multi-tenant Deep Learning Platform"}, {"paperId": "40fb2468c3a77c68fe703a6e614f4ad25bd4e3dd", "title": "Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence"}, {"paperId": "1955dcdd21eeaf0f275ecfbd8266766957d77468", "title": "NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization"}, {"paperId": "d3bf41750875a1a3ef5c4b4ad73917457065acbc", "title": "Cynthia: Cost-Efficient Cloud Resource Provisioning for Predictable Distributed Deep Neural Network Training"}, {"paperId": "44ea486d6d9d33ea654ba880404c99086e603d7d", "title": "Nanily: A QoS-Aware Scheduling for DNN Inference Workload in Clouds"}, {"paperId": "3e9a40a567c4a95b591530ff5771296b478a0f0c", "title": "Machine Learning at the Network Edge: A Survey"}, {"paperId": "8000a4a63ac97ffb84917f910e2ce747e48d409f", "title": "Qsparse-Local-SGD: Distributed SGD With Quantization, Sparsification, and Local Computations"}, {"paperId": "8dbd03d92fb9556a14cb1eed0ee85951bab0c06b", "title": "Natural Compression for Distributed Deep Learning"}, {"paperId": "928cd808aba140ec298508df87c5579811ff2f41", "title": "Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing"}, {"paperId": "2f682a99bd8ff2e5c7b418760a6d36eb0247e311", "title": "On the Computation and Communication Complexity of Parallel SGD with Dynamic Batch Sizes for Stochastic Non-Convex Optimization"}, {"paperId": "b4a632a7097e7d0631250884dfc6e1f76b376996", "title": "PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "2ec9681a021ffed564ac364def44e9fbcedacd48", "title": "Deep Learning-based Job Placement in Distributed Machine Learning Clusters"}, {"paperId": "5133edcf640bc4cf9beb773059fa79f6e04dc7c9", "title": "Impact of Network Topology on the Performance of DML: Theoretical Analysis and Practical Factors"}, {"paperId": "48dec1ce3cfdef6667208cee20eee8a64c9d3302", "title": "Round-Robin Synchronization: Mitigating Communication Bottlenecks in Parameter Servers"}, {"paperId": "6b39bea0ae1720dcbc1ed19ffa697114c4d356c4", "title": "Scalable Deep Learning on Distributed Infrastructures"}, {"paperId": "84c932fcd7226a6d84a4643dddc97a1f795a4922", "title": "Fast Distributed Deep Learning over RDMA"}, {"paperId": "db86f5553d23c66c128a29075e8866c4831c4a1f", "title": "Communication-Efficient Federated Deep Learning With Layerwise Asynchronous Model Update and Temporally Weighted Aggregation"}, {"paperId": "1fb744bacb54c3ab4c1884e0c6da2e9e3cc666e4", "title": "Social Network Identification Through Image Classification With CNN"}, {"paperId": "46bb5162da1ad9b3591a9e056550135e7a50bc2b", "title": "Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect"}, {"paperId": "29081e5c63d24fa8952e938dba68956cd47ac81d", "title": "Robust and Communication-Efficient Federated Learning From Non-i.i.d. Data"}, {"paperId": "ca513657a09369be8cec83fda6a5948e744c78a6", "title": "Scaling Distributed Machine Learning with In-Network Aggregation"}, {"paperId": "1f5ce22d0bef55ee414fbcaadc2d66a396761979", "title": "Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications"}, {"paperId": "79cf9462a583e1889781868cbf8c31e43b36dd2f", "title": "Towards Federated Learning at Scale: System Design"}, {"paperId": "7c22a6a07e89461178b794681c675b209332ee15", "title": "Error Feedback Fixes SignSGD and other Gradient Compression Schemes"}, {"paperId": "54ddd67944520f249b906ba4e817188686eae94d", "title": "Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads"}, {"paperId": "22c844d939f755f07b4e78ab830586f10cb0fa6d", "title": "A Distributed Synchronous SGD Algorithm with Global Top-k Sparsification for Low Bandwidth Networks"}, {"paperId": "1284ed4bf6a043ecf8cebca09e4811f1e3b83b65", "title": "Federated Optimization in Heterogeneous Networks"}, {"paperId": "7f7bb204806ed819b323953cf6ca04cc65a27698", "title": "Wireless Network Intelligence at the Edge"}, {"paperId": "d84e53711e5d61c61d832febbd75b21f4a8a2956", "title": "A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "b2c8e834ac5f7be68b9ca3691d39925036dd74a3", "title": "Measuring the Effects of Data Parallelism on Neural Network Training"}, {"paperId": "2582c54766c387f7ee80a21b91a7f5bae262896a", "title": "Deep Learning with Long Short-Term Memory for Time Series Prediction"}, {"paperId": "6445ddea3defe714a60ca2c933a1f140de4b362b", "title": "Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD"}, {"paperId": "79c8f930bb66c82421b84617e4b6c0b2855cd063", "title": "Applications of Deep Reinforcement Learning in Communications and Networking: A Survey"}, {"paperId": "0606676f16d581fa453f6b7b8a14fc7c4af8d025", "title": "Gandiva: Introspective Cluster Scheduling for Deep Learning"}, {"paperId": "63c52246e0d4dfd5a3f9b6c6c2a1acc48815af42", "title": "Rafiki"}, {"paperId": "2d8c43aa050203e2b49cd8021d0f65c7d2cca00e", "title": "The Convergence of Sparsified Gradient Methods"}, {"paperId": "38f1ef8ab96e5e0195abcd197bf6df47eb308e8a", "title": "Sparsified SGD with Memory"}, {"paperId": "93ef5b740fa1b54929ead6eb177e0698d7f19719", "title": "Don't Use Large Mini-Batches, Use Local SGD"}, {"paperId": "da8a949f9c9f1df3a38f12c2cac97b789c705465", "title": "A Survey of Machine and Deep Learning Methods for Internet of Things (IoT) Security"}, {"paperId": "e0e152748ea0badfbd798dfed3ac743abb58af26", "title": "Parallel Restarted SGD with Faster Convergence and Less Communication: Demystifying Why Model Averaging Works for Deep Learning"}, {"paperId": "d29a248bfe7ab08eb99d841f5fd6287f64b8d394", "title": "Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization"}, {"paperId": "b611636f3cfe7b9aa41a606bec1d9fa72e1359ae", "title": "ATOMO: Communication-efficient Learning via Atomic Sparsification"}, {"paperId": "fae2a5101789afd51c1ececb28c75537c88734ec", "title": "Scalable Methods for 8-bit Training of Neural Networks"}, {"paperId": "ccf18d7ead5402dfcf3f2775b40bc78a9a27087f", "title": "LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning"}, {"paperId": "7cfa76a82be96c74b2eff514265b7fd271a179cd", "title": "Local SGD Converges Fast and Communicates Little"}, {"paperId": "93a06eb066fe58ed7d036e46e4cee53483e16bb8", "title": "Optimus: an efficient dynamic resource scheduler for deep learning clusters"}, {"paperId": "e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da", "title": "Adaptive Federated Learning in Resource Constrained Edge Computing Systems"}, {"paperId": "42f7bd35df5a280ccd47115a30901afab9f0776b", "title": "D2: Decentralized Training over Decentralized Data"}, {"paperId": "8a5d0579590465494c9aba58a857af43b190b6a6", "title": "Deep Learning in Mobile and Wireless Networking: A Survey"}, {"paperId": "d8c09661b1bebfb690f0566167c87d64c5628d73", "title": "Demystifying Parallel and Distributed Deep Learning"}, {"paperId": "e0c0043c43c03de6e2c26a605dc8b8e08872a8a0", "title": "SparCML: high-performance sparse communication for machine learning"}, {"paperId": "33416f2dc49db24cca520a3b234f02463a4e833e", "title": "Characterizing Implicit Bias in Terms of Optimization Geometry"}, {"paperId": "99b3d005763e699613a58b2b7eac5b4ba466567d", "title": "3LC: Lightweight and Effective Traffic Compression for Distributed Machine Learning"}, {"paperId": "97884ff15e0a4e83f534b7b13979e519d1c50a54", "title": "signSGD: compressed optimisation for non-convex problems"}, {"paperId": "2e5cf1a6ab44ac44acbec9daa1a870194f1e5553", "title": "Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?"}, {"paperId": "92495abbac86394cb759bec15a763dbf49a8e590", "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training"}, {"paperId": "f7b4139930a42642c71c2af04a013989b7d99be1", "title": "Topology-Aware GPU Scheduling for Learning Workloads in Cloud Environments"}, {"paperId": "db0cc2f21b20cbc0ab8946090967399c25709614", "title": "Practical Secure Aggregation for Privacy-Preserving Machine Learning"}, {"paperId": "1caff87770b1cbddcf94edc0a9b1bce029324765", "title": "Gradient Sparsification for Communication-Efficient Distributed Optimization"}, {"paperId": "36f13179cdfc13017df535fdee582d58067301f3", "title": "Deep packet: a novel approach for encrypted traffic classification using deep learning"}, {"paperId": "e6e64043c66b83a8787a69a70d7c0a85fd9d23c3", "title": "Scaling SGD Batch Size to 32K for ImageNet Training"}, {"paperId": "b07018924334ddc6ec0e8cb07ba0093d5e8c7997", "title": "On the convergence properties of a K-step averaging stochastic gradient descent algorithm for nonconvex optimization"}, {"paperId": "7f0d3991feb75f6f2fd50d0f2f80d820f20054cc", "title": "ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"paperId": "3f1ab8b484f7881a68c8562ff908390742e4ba90", "title": "Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent"}, {"paperId": "4bdb91a6e47385292ab7a18e8901a6a25f50cc6b", "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning"}, {"paperId": "e8437e3b32f091f62f3796556435e139db130f90", "title": "Sparse Communication for Distributed Gradient Descent"}, {"paperId": "0a5ff7336879c99513dca6fce6ef44984ebf3f55", "title": "Clipper: A Low-Latency Online Prediction Serving System"}, {"paperId": "f173803acc6bbb11dffa7342d3fab5f4a381230f", "title": "Randomized Distributed Mean Estimation: Accuracy vs. Communication"}, {"paperId": "dbcee766687b947548803d41c66e7962f2aaebf7", "title": "Distributed Mean Estimation with Limited Communication"}, {"paperId": "c9d64aaa2007b60ef7814acc895dd90f15578a20", "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "07f5bae91cd45eafe82f3548a43268eb5c84df7a", "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\u0141ojasiewicz Condition"}, {"paperId": "8b053389eb8c18c61b84d7e59a95cb7e13f205b7", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"}, {"paperId": "ffa641a39315830fcb73ca78b09df69fbc180ce9", "title": "Asynchrony begets momentum, with an application to deep learning"}, {"paperId": "d1dbf643447405984eeef098b1b320dee0b3b8a7", "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data"}, {"paperId": "dc8bd1f2fd5662c97e3777a695e70ed52550dc86", "title": "Strategies and Principles of Distributed Machine Learning on Big Data"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "7547eb271adbcb860bd47bc2da1bc8a7fce01ebc", "title": "Efficient Coflow Scheduling Without Prior Knowledge"}, {"paperId": "b7cf49e30355633af2db19f35189410c8515e91f", "title": "Deep Learning with Limited Numerical Precision"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "d1e4365de165463e51134f10bf3939f2b00a6667", "title": "Deep learning with Elastic Averaging SGD"}, {"paperId": "4922cd952592c854fbed1ebfaf794ed8595acba0", "title": "Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning"}, {"paperId": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "title": "Communication Efficient Distributed Machine Learning with the Parameter Server"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "3439a127e45fb763881f03ef3ec735a1db0e0ccc", "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs"}, {"paperId": "a058935fd019c2367fd32c16cd1ce6983a29aafb", "title": "Efficient mini-batch training for stochastic optimization"}, {"paperId": "acbd13c7be621a7284da4ab9d8caa40f1a558ce2", "title": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server"}, {"paperId": "47c141bd9dc9f59090705b6336aed68bfa7ddb47", "title": "pFabric: minimal near-optimal datacenter transport"}, {"paperId": "d1208ac421cf8ff67b27d93cd19ae42b8d596f95", "title": "Deep learning with COTS HPC systems"}, {"paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91", "title": "On the importance of initialization and momentum in deep learning"}, {"paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e", "title": "Large Scale Distributed Deep Networks"}, {"paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e", "title": "On the difficulty of training recurrent neural networks"}, {"paperId": null, "title": "OSA: An Optical Switching Architecture for Data Center Networks With Unprecedented Flexibility"}, {"paperId": "5b8bcb3d2b8e2f4111c61003abbfbb5963187870", "title": "Jellyfish: Networking Data Centers Randomly"}, {"paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, {"paperId": "a825470434b4cf7ae528b40616fd979a3b2678c7", "title": "Helios: a hybrid electrical/optical switch architecture for modular data centers"}, {"paperId": "63850d0911a0e7971e6897f6e2182729c1e2d729", "title": "c-Through: part-time optics in data centers"}, {"paperId": "f7f38f4d4a0fc0e0da963b5166730e6321b6c171", "title": "Data center TCP (DCTCP)"}, {"paperId": "fe33ba23625e0039b6bddf69a63f43dfe22928b1", "title": "BCube: a high performance, server-centric network architecture for modular data centers"}, {"paperId": "bd2e28376aca5a8ef1cf6068a3aeb1ca6d0e1abd", "title": "Dcell: a scalable and fault-tolerant network structure for data centers"}, {"paperId": "704ee1ed2c95bedd7808a92e879bd30cba818739", "title": "A scalable, commodity data center network architecture"}, {"paperId": "145dcba6ff585990ff051e9e0dbd52296ebda6c6", "title": "Blue Gene/L torus interconnection network"}, {"paperId": "3fe5ceaad03325a16241d3fac806911249f8beb5", "title": "Results of a prototype television bandwidth compression scheme"}, {"paperId": "31d3c657987119aaa1e0b8306c0abbe84bf35222", "title": "A Survey on Scheduling Techniques in Computing and Network Convergence"}, {"paperId": "7889cc4e9565c9e7bd725509d7da4597e6a9b576", "title": "CocktailSGD: Fine-tuning Foundation Models over 500Mbps Networks"}, {"paperId": "6597b53ea7f7c1909870c9be6ed54695ec36d3b6", "title": "Hydro: Surrogate-Based Hyperparameter Tuning Service in Datacenters"}, {"paperId": "9674364f0f53801b3f2ad6efcc11b32578ca7366", "title": "Transparent GPU Sharing in Container Clouds for Deep Learning Workloads"}, {"paperId": "b6299b08bfecc60245de0cea6cebfa82c27a51b7", "title": "AntTune: An Efficient Distributed Hyperparameter Optimization System for Large-Scale Data"}, {"paperId": "8f4e626c2783b0fada28d206c3584ed6674ba53f", "title": "Beware of Fragmentation: Scheduling GPU-Sharing Workloads with Fragmentation Gradient Descent"}, {"paperId": "6322985bb03f3fcc2982ec8d2eba4c93e38865f0", "title": "Distributed Artificial Intelligence Empowered by End-Edge-Cloud Computing: A Survey"}, {"paperId": "d541bd870ae3579e4b9b31645ba919c457cb3557", "title": "MIPD: An Adaptive Gradient Sparsification Framework for Distributed DNNs Training"}, {"paperId": "15ee22a56b654d51a93dd01a3f2293bd6c446b9e", "title": "DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks"}, {"paperId": "22cf24bad4fa50d4a54cb2b6fce7a1d22d980058", "title": "Communication-efficient Distributed Learning for Large Batch Optimization"}, {"paperId": "781e0b5148c5d5b710674d7274f573ccc323b3aa", "title": "QSFL: A Two-Level Uplink Communication Optimization Framework for Federated Learning"}, {"paperId": "da37fb0a9071d1ceda0c7e359a049c6c7e627a01", "title": "Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization"}, {"paperId": "146ac1952faf171a1c198b2023200be1d1fcd14a", "title": "Cocktail: A Multidimensional Optimization for Model Serving in Cloud"}, {"paperId": "b099ca8410efd3ebbb43259832318ead30f3249a", "title": "SLA-Driven ML Inference Framework for Clouds with Hetergeneous Accelerators"}, {"paperId": "de036947d8cfc1b6164d55b2470fd8020f086c4c", "title": "Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing"}, {"paperId": "167530160d46955035172f1b90ed2a94d6e13f72", "title": "Cooperative SGD: A Unified Framework for the Design and Analysis of Local-Update SGD Algorithms"}, {"paperId": "ca1402619a80c140c650961d899319c2928744f0", "title": "Piper: Multidimensional Planner for DNN Parallelization"}, {"paperId": "100e224fd1c43538caa34d535725dc3b539fa9bb", "title": "Liquid: Intelligent Resource Estimation and Network-Efficient Scheduling for Deep Learning Jobs on Distributed GPU Clusters"}, {"paperId": "920241246c82176173d442fd43214b2762be6e56", "title": "Elastic Resource Sharing for Distributed Deep Learning"}, {"paperId": "d87ceea3f5d0582997840a107f2bd6c5a1aadbde", "title": "In-network Aggregation for Shared Machine Learning Clusters"}, {"paperId": "775a0d2671905999474cef4a76b397b266741868", "title": "Zico: Efficient GPU Memory Sharing for Concurrent DNN Training"}, {"paperId": "752afa8e951c21b52a8288700b8760e22dd1dd52", "title": "DeepReduce: A Sparse-tensor Communication Framework for Federated Deep Learning"}, {"paperId": "021f6b1e8c328a4ce0e90dbd0d74e11e90aa4224", "title": "INFaaS: Automated Model-less Inference Serving"}, {"paperId": "4043ad5c730d4d2f09d71aa563ff8fce7b834e35", "title": "Fluid: Resource-aware Hyperparameter Tuning Engine"}, {"paperId": "c90a5375329ea537d4ac266297b1c5cca89af19b", "title": "ATP: In-network Aggregation for Multi-tenant Learning"}, {"paperId": "a46f01e587dc0ecb5ed3afb2b55711a0ec1472e8", "title": "Asynchronous Distributed Learning : Adapting to Gradient Delays without Prior Knowledge"}, {"paperId": "96acd6b1c1528d3f50f71083c88d84d619b5b8b8", "title": "PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications"}, {"paperId": "57bc3e71a889013981b191b6431bec3dc45eba9f", "title": "AntMan: Dynamic Scaling on GPU Clusters for Deep Learning"}, {"paperId": null, "title": "\u201cThe error-feedback framework: Better rates for sgd with delayed gradients and compressed updates,\u201d"}, {"paperId": null, "title": "\u201cPytorch distributed: Experiences on accelerating data parallel training,\u201d"}, {"paperId": null, "title": "IEEE COMMUNICATIONS SURVEYS & TUTORIALS"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "6e8001afb2e24b648ae9ceb4e00c20ded4a5ee99", "title": "The role of over-parametrization in generalization of neural networks"}, {"paperId": "8d2d560bf1c4c6930d2d1411e48b642bd5b81179", "title": "Tiresias: A GPU Cluster Manager for Distributed Deep Learning"}, {"paperId": "124399d90a678e7652194c58cff197e5646f02fc", "title": "Amazon\u2019s Machine Learning Toolkit: Sagemaker"}, {"paperId": "9ee76c41dd161df75cb50ac06d2868afec63b0db", "title": "Scalable distributed DNN training using commodity GPU cloud computing"}, {"paperId": "74309511fab3a7947a45ab228c1f1c675782dd8a", "title": "Methods of Simultaneous Iteration for Calculating Eigenvectors of Matrices"}, {"paperId": null, "title": "\u201cMlaas in the wild: Workload analysis and scheduling in large-scale heterogeneous gpu clusters,\u201d"}, {"paperId": null, "title": "Nvlink and nvswitch"}, {"paperId": null, "title": "\u201cUnified memory for cuda,\u201d"}, {"paperId": null, "title": "\u201cTrans-former and cnn hybrid deep neural network for semantic segmentation of very-high-resolution remote sensing imagery,\u201d"}, {"paperId": null, "title": "\u201cNvidia multi-process service,\u201d"}]}