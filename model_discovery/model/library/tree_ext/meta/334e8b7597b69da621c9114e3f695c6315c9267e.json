{"paperId": "334e8b7597b69da621c9114e3f695c6315c9267e", "title": "FP8-LM: Training FP8 Large Language Models", "abstract": "In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 39% reduction in real memory usage but also ran 75% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 37%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at {https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.", "venue": "arXiv.org", "year": 2023, "citationCount": 9, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is shown that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters, which largely reduces the training costs for large foundation models."}, "embedding": {"model": "specter_v2", "vector": [-0.019691497087478638, 0.44054079055786133, -0.6558235883712769, 0.14270325005054474, -0.23452776670455933, 0.1284787803888321, 0.6048676371574402, -0.20395386219024658, -0.5376300811767578, -0.23333142697811127, 0.2965453863143921, -0.3748070001602173, 0.6053522825241089, 0.013696607202291489, -0.26679858565330505, 0.13375087082386017, -1.0332906246185303, 0.3074812889099121, -0.24889333546161652, -0.04814565181732178, -0.24244743585586548, -0.5624173283576965, -0.9061500430107117, 0.12468959391117096, 0.576137900352478, 0.5052913427352905, 0.13995738327503204, 1.1438624858856201, -0.7216626405715942, 0.41164612770080566, 0.37777119874954224, -0.33954063057899475, 0.2791091799736023, 0.34602901339530945, -0.3210320472717285, 0.16274921596050262, 0.2894296944141388, -0.42895591259002686, -0.2203489989042282, 0.7291999459266663, -0.04937800392508507, 0.17135179042816162, 0.02823910117149353, -0.7368934750556946, -0.030093582347035408, 0.7255773544311523, 0.15443658828735352, 0.5159727931022644, -0.6428472995758057, -0.011910793371498585, 1.0770351886749268, -1.438659429550171, -0.16757410764694214, 1.3660387992858887, 0.5738731622695923, 0.18392877280712128, -0.2511611580848694, -0.5212435126304626, 0.5377136468887329, -0.49506834149360657, -0.7071637511253357, -0.3813707232475281, -0.5853581428527832, -0.10068592429161072, 2.3991620540618896, -0.41603565216064453, 0.11013005673885345, 0.3690612018108368, 0.10752253234386444, 1.3064720630645752, -0.28118452429771423, -0.9732881784439087, -0.2953116297721863, -0.08516351133584976, 0.3710207939147949, 1.119044542312622, -0.2903629243373871, 0.4181160628795624, -0.9793739318847656, -0.24519450962543488, 0.3556250035762787, -0.24432849884033203, 0.3364574611186981, 0.008571573533117771, -0.389157772064209, 0.7606925368309021, 0.4178437888622284, 0.6329230070114136, -0.025381209328770638, 0.8946097493171692, 0.5160280466079712, 0.30229422450065613, 0.37014758586883545, 0.04251594468951225, -0.29457715153694153, 0.08202562481164932, -0.9222329258918762, 0.2599371075630188, -0.06758791953325272, 0.6088826060295105, -0.08322763442993164, 0.5079115033149719, -0.701601505279541, 0.280107319355011, 1.6033276319503784, 0.2973673939704895, 0.5870800018310547, -0.45266449451446533, 0.728479266166687, -0.6806125640869141, -0.10664427280426025, -0.4254191517829895, -0.33445754647254944, -0.4860020875930786, -0.7332970499992371, -1.1642088890075684, -0.4199085235595703, -0.10229349881410599, -0.8592157959938049, 0.7787256240844727, -0.022530656307935715, 0.3730296790599823, 0.46511009335517883, 0.42204493284225464, 0.7301294803619385, 0.9708398580551147, 0.36637914180755615, 0.059874098747968674, 0.8882819414138794, -1.536319375038147, -0.28261569142341614, -1.1747907400131226, 1.0962390899658203, -0.4539332091808319, 0.24450500309467316, -0.2360440492630005, -1.2969046831130981, -0.9285451769828796, -0.811857283115387, -0.053300175815820694, -0.34488508105278015, 0.3894871175289154, 1.3243107795715332, 0.5440518856048584, -1.1849286556243896, 0.7057293057441711, -0.3353273868560791, 0.06593704968690872, 0.10028738528490067, 0.4628571569919586, 0.5417004227638245, -0.2220562994480133, -1.4376132488250732, 0.12923429906368256, 0.3917319178581238, -0.42248550057411194, -0.10941370576620102, -0.7322402000427246, -0.9318343997001648, -0.25249624252319336, 0.11187472194433212, -0.3239890933036804, 1.6170347929000854, -0.012809942476451397, -1.502454400062561, 0.688421368598938, 0.02757280506193638, 0.030871950089931488, 0.3058852553367615, -0.02128387801349163, -0.6832893490791321, -0.4163746237754822, -0.4670865833759308, 1.1383854150772095, 0.3820004165172577, 0.35882291197776794, -0.015815408900380135, 0.2562161684036255, -0.19526897370815277, 0.22072158753871918, -0.26511335372924805, 1.007515549659729, -0.634052038192749, -0.19633591175079346, 0.16098569333553314, 0.5189711451530457, -0.42399147152900696, -0.30956903100013733, -0.31862881779670715, -0.7849387526512146, 0.724053144454956, -0.02678258903324604, 0.9256725907325745, -0.9194940328598022, -1.0954618453979492, -0.06694060564041138, -0.12930963933467865, 0.3453473448753357, -0.8676879405975342, 0.3473646640777588, -0.2158108800649643, 0.209424689412117, 0.05071578919887543, -1.1589503288269043, 0.21557709574699402, -0.26676565408706665, -0.786928653717041, -0.2793426215648651, -0.007991477847099304, 1.086333990097046, -0.8065817952156067, -0.15807566046714783, -0.03898860886693001, 0.5175176858901978, -1.192553997039795, 1.0290615558624268, -0.7289086580276489, 0.08977674692869186, 0.19228962063789368, -0.4113743007183075, 0.2773588299751282, -0.410930335521698, 0.579014778137207, -0.27524176239967346, -0.33529916405677795, 0.7321075797080994, -0.47160154581069946, 1.5483201742172241, -0.5984870195388794, 0.22696442902088165, 0.0010731234215199947, -0.10323066264390945, 0.2153416872024536, 0.40622997283935547, -0.3084372580051422, -0.4725107252597809, 0.340160071849823, 0.6547077298164368, -0.5269403457641602, 0.49097713828086853, 1.1662189960479736, 0.8270559310913086, -0.06365371495485306, 0.5082364678382874, 0.601040244102478, -0.21201276779174805, 0.7237685918807983, 0.37466877698898315, 0.25850915908813477, 0.25057002902030945, 0.1459304392337799, -0.20633438229560852, 0.44093093276023865, -0.9292986392974854, -0.44948744773864746, 0.46566176414489746, 0.5869525074958801, 0.4973220229148865, 0.3798370659351349, -0.7179898023605347, -0.2719171345233917, 0.08335503935813904, 0.7910432815551758, 1.5149037837982178, -0.46921128034591675, -0.11714844405651093, -0.6920433044433594, -0.2968481779098511, -0.16486185789108276, -0.07244806736707687, 0.20708584785461426, 0.050183866173028946, -0.6359761357307434, -1.3103922605514526, 0.8647080063819885, 0.16486279666423798, 0.9589487314224243, -0.7671093344688416, -0.4492819607257843, -0.4440779387950897, 0.5822625756263733, -0.7761304378509521, -0.7438158392906189, 0.6172821521759033, -0.9273576736450195, 0.11102426052093506, -0.0403478667140007, -0.22721828520298004, 0.19110523164272308, -0.4790123999118805, 0.9970350861549377, -0.3534795045852661, -0.28754952549934387, 0.008427244611084461, 0.7964935898780823, -0.11768057197332382, -0.8354835510253906, 0.22885307669639587, 0.04335742071270943, -0.298818975687027, 0.14084994792938232, 0.5578241944313049, 0.2648938000202179, -0.19660769402980804, -0.06739384680986404, 0.341884046792984, 0.17623001337051392, -0.16889232397079468, 0.6580809950828552, -0.33865299820899963, -0.29163023829460144, -1.015175223350525, 0.9186459183692932, 0.3543371558189392, -0.5976435542106628, 0.43741101026535034, -0.793925404548645, -0.27015990018844604, 0.44722217321395874, -0.643244206905365, -0.21638762950897217, -1.170045256614685, 0.1793469339609146, -0.34057796001434326, 0.011154755018651485, -0.004076658748090267, 0.40924063324928284, -0.17244166135787964, 0.12471405416727066, 0.4074195325374603, 0.4762518107891083, -0.10662643611431122, 0.7887136340141296, -0.7839704155921936, 0.17741194367408752, -0.13019318878650665, -0.20260316133499146, -0.676347553730011, -0.22975358366966248, -0.6980759501457214, -0.34718695282936096, -0.2713495194911957, -0.03385200724005699, -0.15182210505008698, 0.08656871318817139, -0.6418954730033875, -0.6196658611297607, 0.28826114535331726, -1.1293604373931885, -0.5898657441139221, 0.2954516112804413, -0.09283000230789185, -0.10286790877580643, -1.0505369901657104, -1.5412976741790771, -0.3541041612625122, -1.0527987480163574, -1.385175347328186, 0.5084521174430847, -0.008463319391012192, -0.23182885348796844, -0.7046599984169006, 0.1452048420906067, -0.5036020874977112, 0.9974128603935242, -1.1290236711502075, 0.7184018492698669, 0.31790444254875183, -0.05191010981798172, 0.05086440220475197, 0.1262509673833847, 0.5010071396827698, -0.512622594833374, 0.6749817132949829, -0.8089182376861572, 0.15705513954162598, -0.4928451180458069, -0.7093256711959839, 0.23916788399219513, 0.38682660460472107, 0.7311304807662964, 0.055001094937324524, -0.3762814700603485, 0.6149343848228455, 1.2930967807769775, -0.9483842253684998, 0.14088086783885956, -0.006241421215236187, 1.0529385805130005, -0.21275931596755981, -0.36957892775535583, 0.47482359409332275, 0.029989246279001236, 0.37777310609817505, 0.0038836973253637552, -0.24507111310958862, 0.06137982755899429, -0.3327910006046295, 0.6592280864715576, 1.8673067092895508, 0.6404386162757874, -0.09624262154102325, -1.0049587488174438, 0.09847857058048248, -0.8953501582145691, -0.3612387180328369, 0.4016941785812378, 0.8503115773200989, 0.5269437432289124, -0.11067187786102295, -0.12926118075847626, -0.08023139089345932, 0.5275274515151978, 0.8200752139091492, -0.1663600206375122, -1.2049615383148193, 0.07186924666166306, 0.3264937996864319, 0.12100576609373093, 0.6276900768280029, -0.27053722739219666, 0.7414747476577759, 14.677474021911621, 1.034592628479004, -0.4149242043495178, 0.6371938586235046, 1.1314327716827393, 0.11605174094438553, -0.3099125027656555, -0.48047876358032227, -1.5032644271850586, -0.1192912682890892, 1.6814640760421753, 0.4457137882709503, 0.6548717617988586, 0.25708889961242676, -0.010829677805304527, 0.09441965818405151, -0.18995024263858795, 0.7013424038887024, 0.16317741572856903, -1.2983399629592896, 0.3984442949295044, 0.08475776761770248, 0.48867860436439514, 0.8156173825263977, 0.9808779358863831, 0.9949356317520142, 0.35547909140586853, -0.37497228384017944, 0.5608582496643066, -0.21673989295959473, 1.0762200355529785, -0.23484919965267181, 0.054510220885276794, 0.44772782921791077, -0.8569358587265015, 0.034983910620212555, -0.6412665247917175, -1.376327633857727, 0.0485580675303936, 0.4051165282726288, -0.8519608974456787, -0.576161801815033, -0.5487977862358093, 0.6985638737678528, 0.21424297988414764, 0.22514347732067108, -0.11779820919036865, 0.6607553958892822, -0.19686472415924072, 0.04057621955871582, 0.5826869010925293, 0.264369398355484, -0.08769442141056061, 0.060106176882982254, 0.009954486973583698, -0.38316380977630615, 0.2781023681163788, 0.31053030490875244, -0.412336140871048, -0.04677677899599075, -0.03409355506300926, -0.4475307762622833, -0.3520430326461792, 1.004506230354309, 0.2398911565542221, 0.3928860127925873, -0.5982881188392639, -0.00999128632247448, 0.9363871812820435, -0.0030562414322048426, -0.7100827097892761, 0.6393853425979614, 0.4331219792366028, -0.7805166244506836, -0.07124883681535721, 0.24593040347099304, -0.006251313723623753, -0.5446356534957886, -0.7929813861846924, -0.6305128931999207, 0.15796829760074615, -0.5310481190681458, -0.3863315284252167, 0.8291983604431152, -0.5341731905937195, -0.18110519647598267, 0.23791387677192688, -0.9032641053199768, -0.2515682876110077, 0.4828175902366638, -1.319137454032898, -0.5514929294586182, 0.6765583753585815, -0.5135052800178528, -0.4926574230194092, -0.09638771414756775, 1.2881758213043213, 0.33941370248794556, -0.6209093332290649, 0.14131908118724823, 0.15446369349956512, -0.20127126574516296, -0.4967067539691925, -0.4181191921234131, 1.2187013626098633, 0.25558292865753174, -0.01151404157280922, 0.053960468620061874, -0.33352378010749817, 0.09076333045959473, -0.9158352017402649, -0.36744070053100586, 0.6778110861778259, -0.23115189373493195, -0.1262117177248001, -0.8757063150405884, -0.4622194468975067, 0.19121411442756653, 0.3155418932437897, 0.024957777932286263, 0.3318774700164795, 0.2020878791809082, -0.4946718215942383, -0.08346603065729141, -0.6316556930541992, 0.1107400581240654, 0.39159688353538513, -0.7091380953788757, 0.3075868487358093, 0.3018691837787628, 0.6939857602119446, -1.3830947875976562, -0.3941168785095215, -0.27984684705734253, -0.02385391667485237, -0.20977048575878143, 1.1499754190444946, -0.3156532049179077, 0.8582714200019836, 1.041863203048706, -0.32139986753463745, -0.7822324633598328, 0.3175757825374603, -0.8637959361076355, -0.2553134262561798, -0.00428253086283803, 0.8209855556488037, -0.5449618697166443, 0.37021517753601074, 0.8163122534751892, 0.2066728174686432, -0.8459295034408569, -0.3592844307422638, -0.3054851293563843, -0.06395388394594193, -0.747546911239624, 0.4600922465324402, -0.36977988481521606, -0.09348282963037491, 0.0036476964596658945, 0.4232318103313446, 0.34056851267814636, -0.38915178179740906, -0.49326232075691223, 0.3828620910644531, 0.5490922927856445, -0.5434554219245911, -0.5611217617988586, -0.46980175375938416, -1.5430864095687866, 0.14716050028800964, -1.3353780508041382, 0.07906948029994965, -0.36321917176246643, -0.332440048456192, -0.41036224365234375, -0.23878203332424164, -0.012348896823823452, 0.307471364736557, -0.04929737001657486, -0.5203700065612793, -0.5498929619789124, -0.6773152351379395, 0.9027734398841858, 0.9401974081993103, -0.7104464769363403, -0.020817143842577934, -0.06788937747478485, 0.5670332312583923, 0.19827091693878174, 0.4046573042869568, -0.23770764470100403, -1.0199947357177734, -1.546746850013733, 0.3742181658744812, -0.14361484348773956, -0.1543506383895874, -0.9706162214279175, 0.2453734576702118, 0.1943150907754898, -0.24726007878780365, 0.18003396689891815, 0.04809558391571045, -0.5194514989852905, -0.4945450723171234, 0.6137731075286865, -0.5315772891044617, 0.41601985692977905, 0.688607394695282, -0.9006380438804626, -0.17322058975696564, 0.42727166414260864, -0.18166010081768036, -0.8557642102241516, -0.8392284512519836, 0.49274975061416626, -0.34919342398643494, 0.2712773084640503, -0.3568075895309448, 0.12271186709403992, -1.3026131391525269, 0.05244123935699463, 0.20595523715019226, 0.27022796869277954, -0.3217792809009552, 0.7665776014328003, 0.215770423412323, -0.9941575527191162, 0.2839803695678711, 0.6646472811698914, -0.588186502456665, -0.07312923669815063, 0.6263109445571899, 0.6157565116882324, -0.8892961740493774, 0.7833159565925598, 0.28345927596092224, -0.011335785500705242, -0.7801901698112488, -0.06365571916103363, 1.0187758207321167, -0.8998841643333435, -0.21252767741680145, 1.3715633153915405, -0.3791336119174957, -1.3323010206222534, 0.17308209836483002, -1.147822380065918, -0.3844442069530487, -0.8201737403869629, 0.7256268262863159, 0.1244218721985817, 0.4157733917236328, -0.10434624552726746, -0.5131855010986328, 0.03673463687300682, -0.12601107358932495, -0.516295313835144, 0.4124661684036255, 0.12685663998126984, -0.5145553946495056, 0.4375823736190796, 0.9545872211456299, -0.6346680521965027, -0.6843985319137573, -0.40961888432502747, -0.6338276863098145, 0.06979529559612274, 0.47659435868263245, -0.3800343871116638, -0.9701077342033386, 0.8874481916427612, 0.4266226887702942, 0.011354406364262104, 0.06651397794485092, -0.7357947826385498, 0.3205392062664032, 0.5138429403305054, 0.36149343848228455, -0.5698595643043518, -0.5574264526367188, 1.4619758129119873, 0.8664802312850952, -0.9869912266731262, 0.30287861824035645, -0.3530150055885315, -0.7577703595161438, 0.9631539583206177, 0.5074678063392639, 0.15148770809173584, 0.8874920010566711, 0.05178561061620712, 0.20659902691841125, 0.318153440952301, -1.0445281267166138, 0.004012586548924446, 0.8278833627700806, 0.49137333035469055, 1.1132535934448242, 0.41725438833236694, 0.09562169760465622, 0.7822426557540894, 0.1476408839225769, 0.053986046463251114, 0.23925884068012238, 0.6477257013320923, -0.10879207402467728, -0.21372422575950623, -0.04892849177122116, 0.7593398690223694, -0.41044268012046814, -0.9270631074905396, 0.5517904758453369, 0.3832755982875824, 0.32895517349243164, 0.4472516179084778, 0.977908194065094, -0.0763109102845192, -0.0590812973678112, 0.2418576031923294, 0.9642492532730103, -0.45879000425338745, -0.5241836309432983, -0.05471809580922127, -0.5657668709754944, -0.07167477160692215, 0.06334211677312851, -0.09913282841444016, -0.6247733235359192, -0.7267792224884033, 0.4773082435131073, 0.06914793699979782, 0.45940810441970825, 1.1647881269454956, 0.6172495484352112, 0.6145753264427185, -0.6028503179550171, -0.6272246241569519, -0.5785994529724121, -0.791418194770813, -0.18148267269134521, -0.8865501284599304, -0.6037877202033997, -0.015874512493610382, 0.0842059999704361, -0.6395835876464844]}, "authors": [{"authorId": "2243685256", "name": "Houwen Peng"}, {"authorId": "2262215229", "name": "Kan Wu"}, {"authorId": "2107995927", "name": "Yixuan Wei"}, {"authorId": "2262447706", "name": "Guoshuai Zhao"}, {"authorId": "2262399127", "name": "Yuxiang Yang"}, {"authorId": "2109371439", "name": "Ze Liu"}, {"authorId": "2262896284", "name": "Yifan Xiong"}, {"authorId": "2254136087", "name": "Ziyue Yang"}, {"authorId": "2142574851", "name": "Bolin Ni"}, {"authorId": "2262335444", "name": "Jingcheng Hu"}, {"authorId": "2262451401", "name": "Ruihang Li"}, {"authorId": "2273515255", "name": "Miaosen Zhang"}, {"authorId": "2226781562", "name": "Chen Li"}, {"authorId": "2136771175", "name": "Jia Ning"}, {"authorId": "2262398543", "name": "Ruizhe Wang"}, {"authorId": "2262491300", "name": "Zheng Zhang"}, {"authorId": "2262395565", "name": "Shuguang Liu"}, {"authorId": "2262215365", "name": "Joe Chau"}, {"authorId": "2262465375", "name": "Han Hu"}, {"authorId": "2281748184", "name": "Peng Cheng"}], "references": [{"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa", "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843", "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "f3a6115e5fb2237df938976e005468f0b18da797", "title": "The Stack: 3 TB of permissively licensed source code"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "b37d57edf4a84da158ab8d77921d4aa39faceb32", "title": "FP8 Formats for Deep Learning"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "3836ccb33191799e748e8e96f85a813eaf650ff8", "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6", "title": "Analysing Mathematical Reasoning Abilities of Neural Models"}, {"paperId": "9a1093af92d315def21b90918faf08665157051a", "title": "Training Deep Neural Networks with 8-bit Floating Point Numbers"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "892e53fe5cd39f037cb2a961499f42f3002595dd", "title": "Bag of Tricks for Efficient Text Classification"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "247feaf26d9605f9adf0ee1364790f006adff445", "title": "Finding Alternative Translations in a Large Corpus of Movie Subtitle"}, {"paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7", "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation"}, {"paperId": "3c61e6b55597cf37b19d2e4b38fc66b9c85c97b9", "title": "Ultra-Low Precision 4-bit Training of Deep Neural Networks"}, {"paperId": null, "title": "Books3"}, {"paperId": "e6cc6a7bd4db3e7604bae6a654ec29aa8542dafc", "title": "Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": "28135fd3e80dda50a673cd556f10b9b972005d27", "title": "Binarized Neural Networks"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": null, "title": "h100 tensor core gpu architecture"}, {"paperId": null, "title": "Bing webmaster tools"}, {"paperId": null, "title": "HuggingFace"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90quality"}, {"paperId": null, "title": "Openchat: Advancing open-source language models with imperfect data"}, {"paperId": null, "title": "Alpacaeval: An automatic evaluator of instruction-following models"}, {"paperId": null, "title": "We thank Baining Guo and Lidong Zhou for their guidance and support for this project"}, {"paperId": null, "title": "Using fp8 with transformer engine"}, {"paperId": null, "title": "Redpajama-data: an open source recipe to reproduce llama training dataset"}]}