{"paperId": "ae7606881a405a773dae6cce8efd87fe0b9b73d9", "title": "Zero-Shot Tokenizer Transfer", "abstract": "Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The results make substantial strides toward detaching LMs from their tokenizer, and it is shown that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training."}, "embedding": {"model": "specter_v2", "vector": [0.13902340829372406, 0.7784824967384338, -0.5453261137008667, 0.039978209882974625, -0.28713828325271606, -0.23601673543453217, 0.42418360710144043, -0.4264235496520996, -0.4651876389980316, -0.36054205894470215, 0.5027369856834412, -0.26098939776420593, 0.8429929614067078, 0.008494092151522636, -0.18205979466438293, 0.3000812828540802, -0.5872102975845337, 0.00095629773568362, -0.0390966571867466, -0.4188469350337982, -0.2245490700006485, -1.19502854347229, -0.6513552665710449, 0.39117079973220825, 0.5105085372924805, -0.14679859578609467, 0.09856891632080078, 0.5161955952644348, -0.8035503029823303, 0.5575488805770874, 0.2605080306529999, -0.5960067510604858, 0.2221297323703766, -0.15388786792755127, -0.5097412467002869, -0.18707074224948883, 0.2634992003440857, -0.5796652436256409, -0.19746091961860657, 0.8367459774017334, -0.2864546775817871, 0.10405605286359787, 0.2789827287197113, -0.8414589762687683, -0.37824514508247375, 1.4247528314590454, 0.538305938243866, 0.21599799394607544, -0.3286638557910919, -0.3565337061882019, 0.9133896827697754, -1.2469995021820068, 0.5635759830474854, 1.3979458808898926, 0.47516176104545593, 0.8391185998916626, -0.12329313904047012, -0.7268974781036377, 0.6559370756149292, 0.028884941712021828, -1.070981740951538, -0.6907678246498108, -0.33660921454429626, -0.07565120607614517, 1.9998700618743896, -0.7175770998001099, 0.03625374287366867, 0.3602638840675354, -0.14239726960659027, 1.2227743864059448, -0.28237679600715637, -0.8157615661621094, -0.5879433751106262, 0.575543999671936, 0.020613260567188263, 0.8299483060836792, -0.6771713495254517, 0.02009289525449276, -0.7756878137588501, 0.4864868223667145, 0.22037385404109955, -0.3418963551521301, 0.10925958305597305, -0.19948258996009827, -0.379696249961853, 0.38876596093177795, 0.15794527530670166, 0.8786658644676208, 0.15386399626731873, 0.7387155890464783, 1.1112149953842163, 0.724082350730896, 0.29705965518951416, 0.39673998951911926, -0.16972266137599945, 0.05625878646969795, -0.8518459796905518, -0.2390538603067398, 0.29464012384414673, 1.0003864765167236, 0.03691456466913223, 0.750230610370636, -0.9149214625358582, 0.3111286461353302, 0.9630414843559265, -0.15789707005023956, 0.6556657552719116, -0.638498842716217, 0.49655881524086, -0.9660292863845825, 0.17070409655570984, -0.5035854578018188, 0.2418896108865738, -0.09041811525821686, -0.6126081943511963, -1.4168386459350586, -0.5784560441970825, -0.11696683615446091, -0.7156134843826294, 0.9766913652420044, -0.20598357915878296, 0.35796019434928894, 0.29893922805786133, 0.31387466192245483, 0.9533983469009399, 0.870849609375, 0.401215523481369, 0.07313165813684464, 0.6019912362098694, -0.4912010729312897, -0.6575903296470642, -0.6849400997161865, 1.2798796892166138, -0.4072584807872772, 0.47809985280036926, -0.4885421097278595, -1.3507938385009766, -0.9133431911468506, -0.896102786064148, 0.060416411608457565, -0.9460247755050659, 0.27626124024391174, 0.45594748854637146, 0.7733665704727173, -0.8990808129310608, 0.8733203411102295, -0.28963786363601685, -0.2097105234861374, 0.19960111379623413, 0.3394584357738495, 0.07458814233541489, -0.40026623010635376, -1.4793156385421753, 0.7643247246742249, 0.4847627580165863, -0.5722517967224121, 0.019017377868294716, -0.9061674475669861, -1.3078789710998535, -0.012777844443917274, 0.3525063991546631, -0.34322163462638855, 1.4496688842773438, -0.22525528073310852, -1.9743746519088745, 0.9667952656745911, -0.4488471746444702, 0.4461734890937805, 0.379654198884964, -0.06870678067207336, -0.5367826223373413, -0.7860305905342102, -0.0936635434627533, 0.6284603476524353, 0.416156142950058, -0.10709743946790695, 0.010423604398965836, 0.6866918206214905, -0.3278357684612274, -0.25629234313964844, -0.7484581470489502, 0.7510600686073303, -0.41525760293006897, 0.00859558954834938, 0.11603516340255737, 0.6830320358276367, 0.5150344967842102, -0.487898588180542, -0.7447834610939026, -1.093165397644043, 0.7014304399490356, 0.1343652755022049, 0.8772518634796143, -1.0086605548858643, -0.5554262399673462, -0.4081505537033081, -0.42831817269325256, 0.22458960115909576, -0.7218252420425415, 1.0354526042938232, -0.48140841722488403, 0.7241018414497375, 0.000462692987639457, -1.0724692344665527, 0.5098147392272949, -0.39311346411705017, -0.7032501697540283, -0.21350643038749695, -0.055598169565200806, 1.0792813301086426, -0.9522111415863037, -0.11212436854839325, 0.3320385813713074, 0.317926824092865, -0.855027973651886, 1.1361076831817627, 0.1502804309129715, -0.0658063068985939, 0.07725061476230621, -0.5008070468902588, 0.13901031017303467, -0.011844268999993801, 0.47509071230888367, -0.5910526514053345, -0.3133525550365448, 0.4439491033554077, -0.17115680873394012, 1.4519375562667847, -0.9102996587753296, 0.33625832200050354, -0.17481867969036102, -0.992051362991333, 0.08001700788736343, 0.8680060505867004, 0.006505612749606371, -0.3005869686603546, 0.3700278699398041, 0.43513259291648865, -0.4513591527938843, 0.12040316313505173, 0.931722104549408, 0.5567457675933838, -0.5139442682266235, 0.1170731633901596, 0.5930923819541931, -0.4267289638519287, 0.21399804949760437, 0.35532912611961365, 0.6219702363014221, 0.276737242937088, 0.3761502504348755, -0.031542759388685226, 0.23862619698047638, -0.7212881445884705, -0.035117607563734055, 0.2574317157268524, 0.6994128823280334, 0.69994056224823, 0.33334946632385254, -0.49262291193008423, -0.40032461285591125, -0.3135615289211273, 0.7293596863746643, 1.522295594215393, -0.030205421149730682, -0.3610627055168152, -0.9207132458686829, -0.2803739011287689, -0.29932305216789246, 0.4374186098575592, -0.23270387947559357, -0.6111313104629517, -0.766471266746521, -0.6168859601020813, 0.7121976613998413, -0.1148940697312355, 1.2150835990905762, -0.2901018559932709, -0.2521018385887146, 0.0056752897799015045, -0.003639932256191969, -0.9107537865638733, -0.8319713473320007, 0.172938734292984, -0.387328177690506, 0.4152204692363739, -0.26763492822647095, -0.3926944434642792, 0.15577392280101776, -0.5065794587135315, 0.8363058567047119, -0.5924676060676575, -0.29048657417297363, 0.09854716807603836, 0.47373130917549133, -0.44936105608940125, -1.0467382669448853, 0.6582150459289551, 0.26336202025413513, -0.24633066356182098, 0.1764502227306366, 0.6270068883895874, -0.04939565062522888, 0.12076368927955627, -0.3540777564048767, 0.31421083211898804, -0.0021888853516429663, -0.2199319750070572, 0.03954647481441498, -0.3971041142940521, 0.30617135763168335, -1.424466609954834, 0.7740878462791443, 0.07174717634916306, -0.23663297295570374, 0.5274860858917236, -0.4191555678844452, -0.5599198341369629, 0.6808139085769653, -0.5664944648742676, -0.42773595452308655, -1.0504064559936523, 0.11289317160844803, -0.15974169969558716, 0.13785558938980103, 0.2958188056945801, 0.09638753533363342, 0.34689751267433167, -0.33170291781425476, 0.24847321212291718, 0.309525728225708, -0.08356361091136932, 1.1534842252731323, -0.6740118861198425, 0.5350940227508545, 0.4795351028442383, 0.5626456141471863, -0.0707983523607254, -0.7069938778877258, -0.521918535232544, -0.16191138327121735, -0.2872200310230255, -0.1701909303665161, -0.41996848583221436, 0.29024818539619446, -0.6201218962669373, -0.7665624618530273, 0.293091744184494, -1.3148181438446045, -0.4552071690559387, -0.00397647637873888, -0.2930870056152344, -0.08464343100786209, -0.9961927533149719, -1.176735758781433, -0.16238468885421753, -0.328880250453949, -0.934836745262146, 0.11076511442661285, -0.20385001599788666, -0.8539313673973083, -0.6120900511741638, 0.0306980162858963, -0.7826540470123291, 1.0746276378631592, -1.063307523727417, 0.9765531420707703, 0.31616270542144775, -0.10012628138065338, -0.10486719012260437, 0.32341793179512024, 0.5837556719779968, -0.09091754257678986, 0.3441905379295349, -0.7289823889732361, -0.08583565801382065, -0.540851354598999, -0.47409316897392273, 0.07116149365901947, 0.34327951073646545, 0.5502967834472656, -0.22198551893234253, -0.5388262271881104, 0.5894480347633362, 1.5074540376663208, -0.5437767505645752, 0.22364071011543274, 0.21860302984714508, 1.0644903182983398, 0.5080742835998535, -0.6372969150543213, 0.4073409140110016, 0.5650723576545715, 0.5456661581993103, -0.07701845467090607, -0.40621089935302734, -0.06304014474153519, -0.6381506323814392, 1.188961148262024, 2.0807645320892334, 0.6721533536911011, -0.2398223578929901, -0.9304572343826294, 0.7193740606307983, -1.213448166847229, -0.6575829982757568, 0.710749089717865, 0.5804293751716614, 0.4936698377132416, -0.5786316990852356, -0.7121050357818604, -0.611678421497345, 0.7676176428794861, 0.21750739216804504, -0.42041805386543274, -1.2879536151885986, 0.38211652636528015, 0.41530200839042664, 0.4535844027996063, 0.6111103296279907, -0.056466735899448395, 1.2857427597045898, 14.513060569763184, 0.969811201095581, -0.2553875744342804, 0.7592465877532959, 0.24632157385349274, 0.22331945598125458, -0.26878559589385986, -0.17149198055267334, -1.2694755792617798, -0.18259426951408386, 1.2112839221954346, -0.4382854104042053, 0.4179627001285553, 0.12675140798091888, 0.07423721998929977, 0.3450821042060852, -0.5685945749282837, 0.6362561583518982, 0.6291419863700867, -1.1976394653320312, 0.3858451545238495, 0.1773577332496643, 0.5221284031867981, 0.580627977848053, 0.9308814406394958, 1.0819939374923706, 0.6262983083724976, -0.6043789386749268, 0.5095411539077759, -0.017643963918089867, 1.0890378952026367, -0.12825605273246765, 0.47242817282676697, 0.4290136694908142, -0.9656023979187012, -0.06908468157052994, -0.5171273350715637, -0.9046347141265869, 0.4656984508037567, 0.19570240378379822, -0.6713023781776428, -0.4201013445854187, -0.45937901735305786, 0.794489860534668, 0.6622371673583984, 0.24084621667861938, -0.20153149962425232, 0.6685295104980469, 0.03970920667052269, 0.10416639596223831, 0.24584583938121796, 0.5572370290756226, 0.15972720086574554, 0.27606940269470215, 0.2422359734773636, -0.36431798338890076, 0.09663934260606766, 0.5619108080863953, -0.942146897315979, 0.02733731083571911, -0.606959879398346, -0.015278834849596024, 0.16660615801811218, 0.5005171895027161, 0.47751230001449585, -0.1590077430009842, -0.5272852182388306, 0.6932088136672974, 0.5844058394432068, 0.20781423151493073, -0.22413736581802368, -0.36218181252479553, 0.41053470969200134, -0.3604497015476227, 0.10127421468496323, 0.431600421667099, 0.0644310712814331, -0.8002010583877563, -0.6838682889938354, -0.102882519364357, -0.07232484966516495, -0.6411699652671814, -0.7282634377479553, 0.5528603792190552, -0.10268596559762955, -0.8071182370185852, -0.0028175991028547287, -0.5064326524734497, -0.4978237748146057, 0.27926105260849, -1.4634534120559692, -0.5966662168502808, 0.4758602976799011, -0.47416073083877563, -0.47651588916778564, 0.016527019441127777, 1.2442950010299683, 0.21821285784244537, -0.4329972565174103, 0.3551953136920929, 0.6225301623344421, 0.27715909481048584, 0.09520639479160309, -0.6195302605628967, 0.9362908005714417, 0.04820898920297623, -0.5443053245544434, 0.3250808119773865, -0.04450778663158417, 0.426565557718277, -0.7214468717575073, -0.4943162202835083, 0.791611909866333, -0.7397015690803528, -0.4963405728340149, -0.8476959466934204, -1.0647883415222168, 0.2866322100162506, 1.008865237236023, -0.4112583100795746, 0.4346329867839813, 0.35192152857780457, -0.5414679646492004, -0.39811497926712036, -0.5662926435470581, 0.3152258098125458, 0.4789486825466156, -1.0181539058685303, -0.37343519926071167, -0.261987566947937, 0.3545479476451874, -0.9219935536384583, -0.6516508460044861, -0.28884053230285645, 0.3047921657562256, 0.09328343719244003, 0.8130524754524231, -0.30114006996154785, 0.6950231790542603, 1.0607898235321045, 0.2937217652797699, -1.2776466608047485, 0.3594074249267578, -1.0198054313659668, 0.6141532063484192, 0.7859271764755249, 0.5581488609313965, -0.39059674739837646, 0.22033517062664032, 0.8881151676177979, 0.009690535254776478, -0.1108374372124672, -0.8098124265670776, -0.7017666101455688, 0.6152925491333008, -0.47603747248649597, 0.48565027117729187, 0.3684886693954468, 0.555589497089386, 0.002100122394040227, 0.26354676485061646, 0.1130932867527008, -0.1457056701183319, -1.1712640523910522, 0.5647985935211182, 0.342530757188797, 0.13222938776016235, -0.6585971117019653, -0.46424272656440735, -1.357224464416504, 0.646032452583313, -1.6407685279846191, 0.27365580201148987, -0.9093611836433411, -0.24669583141803741, 0.3653392791748047, -0.01504562422633171, 0.08436156064271927, 0.5896804928779602, -0.41986891627311707, -0.0697440654039383, -0.6540068984031677, -0.45373985171318054, 1.0518829822540283, 0.7171735167503357, -0.761389970779419, 0.1374131590127945, -0.5074667930603027, 0.08230791985988617, 0.33044934272766113, 0.5049126744270325, -0.5766052603721619, -0.9199767708778381, -1.8545634746551514, 0.33454251289367676, -0.0911882221698761, -0.2454030066728592, -0.5378339886665344, 0.5927783250808716, 0.5216370224952698, -0.5473126173019409, 0.19086894392967224, -0.0829334482550621, -0.6340965032577515, -0.4140152633190155, 0.29400256276130676, -0.8253998160362244, 0.28236061334609985, 0.23503771424293518, -0.9821901917457581, -0.32807457447052, 0.5158282518386841, 0.06277842819690704, -1.4010192155838013, -0.9530465602874756, 0.41507089138031006, -1.1271806955337524, 0.4116099178791046, -0.21436628699302673, -0.5485696792602539, -1.0385410785675049, -0.3266412913799286, -0.02963913604617119, 0.431999146938324, -0.23276638984680176, 0.9288886189460754, 0.09883341193199158, -0.9984193444252014, -0.05129731073975563, 0.19950418174266815, 0.036140866577625275, -0.46794578433036804, 0.6015080809593201, 0.13642936944961548, -0.22367164492607117, 0.3363739252090454, 0.12075961381196976, 0.35127827525138855, -0.7998186945915222, -0.26376602053642273, 0.7464443445205688, -0.32385528087615967, -0.2407253384590149, 1.0137673616409302, -0.3740095794200897, -1.3778988122940063, -0.11576172709465027, -1.2355198860168457, 0.09090423583984375, -0.24769218266010284, 0.2555790841579437, 0.1520073413848877, -0.031969938427209854, -0.1352061778306961, -0.3964182138442993, 0.3545624017715454, -0.1857057362794876, -0.5862685441970825, 0.26484501361846924, -0.3329610526561737, -0.5615530014038086, 0.8407421708106995, 1.367090106010437, -1.021116852760315, -0.2703940272331238, -0.516236424446106, -0.5942586064338684, 0.13864406943321228, 0.35742032527923584, -0.03428908810019493, -0.4758461117744446, 1.2114970684051514, 0.3857603371143341, -0.007932646200060844, -0.2910534739494324, -0.16353130340576172, 0.32589277625083923, 0.6680364608764648, -0.010010186582803726, -1.0278284549713135, -0.4885715842247009, 1.4737510681152344, 0.868066668510437, -1.0193384885787964, -0.1679094284772873, -0.3117537796497345, -0.5872465372085571, 0.7565771341323853, 0.6510385870933533, 0.2934691607952118, 0.9710164666175842, -0.22708119451999664, 0.37847667932510376, 0.5326053500175476, -1.1532925367355347, -0.15848107635974884, 0.17594455182552338, 0.9104180932044983, 0.9370864033699036, 0.3937329649925232, 0.18119646608829498, 0.3837849795818329, -0.2136232852935791, 0.24565881490707397, 0.8230282068252563, 0.3442101776599884, -0.21205788850784302, -0.3819257318973541, 0.35520249605178833, 0.4818360507488251, -0.43942558765411377, -0.725508987903595, 0.18864776194095612, 0.7616115212440491, 0.21070504188537598, 0.47658130526542664, 1.0003904104232788, 0.08590087294578552, 0.6578401327133179, 0.39923909306526184, 0.41092413663864136, -0.7277595400810242, -0.5052958130836487, -0.5333701968193054, -0.5256127119064331, 0.12284436821937561, -0.07618086040019989, -0.4020100235939026, -0.4403465688228607, -0.7972123026847839, 0.2221844345331192, 0.07852582633495331, 0.218669131398201, 1.2080096006393433, 0.4087178707122803, 0.5986131429672241, -0.08251424878835678, -0.38956865668296814, -0.4226222038269043, -0.7067336440086365, -0.1487654447555542, -0.19988371431827545, -0.2471792995929718, 0.14557020366191864, 0.07316350191831589, 0.059701599180698395]}, "authors": [{"authorId": "2090357303", "name": "Benjamin Minixhofer"}, {"authorId": "3381663", "name": "E. Ponti"}, {"authorId": "2267339029", "name": "Ivan Vuli'c"}], "references": [{"paperId": "e6ffa22c9f53c7dc97da365bfe868951ff7ddf42", "title": "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models"}, {"paperId": "b10e5a2b622a2da27b1a888a117ad5d9237a31e1", "title": "Greed is All You Need: An Evaluation of Tokenizer Inference Methods"}, {"paperId": "975a546b207b399fa5ef44952eefe48b97b200c2", "title": "Fast Vocabulary Transfer for Language Model Compression"}, {"paperId": "ac45bbf9940512d9d686cf8cd3a95969bc313570", "title": "OLMo: Accelerating the Science of Language Models"}, {"paperId": "ad1bb59e3e18a0dd8503c3961d6074f162baf710", "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research"}, {"paperId": "e1f4b94479bfcb735a1a0add178a2337def07c9b", "title": "Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning"}, {"paperId": "c61065446ad3f2851b6553afeb5e6afc3fabdf94", "title": "OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining"}, {"paperId": "70ffc32ecb69740bd508a7863a2d303d4eb4f208", "title": "xVal: A Continuous Number Encoding for Large Language Models"}, {"paperId": "40e0b9361d88b1879891eb6d16de110b30bf6c62", "title": "OctoPack: Instruction Tuning Code Large Language Models"}, {"paperId": "fc84f5b58e68871f3d6889dc2a93dffa7e107be2", "title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback"}, {"paperId": "9a2f47777b99a92effb4e998b7082e1e92ae13bc", "title": "Improving Language Plasticity via Pretraining with Active Forgetting"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "2651f0179874bd010f58d2c9fa7d118807c80977", "title": "TIES-Merging: Resolving Interference When Merging Models"}, {"paperId": "17fbffb05fa14e21d1c506fd5f0f568b955fe983", "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models"}, {"paperId": "879a7f5abdb7ab803d48172d4f0830965f989d46", "title": "Language Model Tokenizers Introduce Unfairness Between Languages"}, {"paperId": "412e266cddfd87c79087a88ba1e4d11b89a45a13", "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers"}, {"paperId": "aad167be3c902388ea625da4117fcae4325b8b7d", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "66817b3f93901a86079307af230d9e626146120c", "title": "Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training"}, {"paperId": "71ba5f845bd22d42003675b7cea970ca9e590bcc", "title": "Editing Models with Task Arithmetic"}, {"paperId": "5e52d654fd31f04c1bd884cd5480e6af8c95ad50", "title": "Efficient Transformers with Dynamic Token Pooling"}, {"paperId": "a9e20180153f6c139a4b6f2791b535fa6ffc3959", "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84", "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904", "title": "Few-shot Learning with Multilingual Generative Language Models"}, {"paperId": "d617f51833860dc50d202af7f80be71304b2e994", "title": "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}, {"paperId": "eb0024439858af7cc951ce2efa5a6533c3781799", "title": "WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models"}, {"paperId": "f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0", "title": "Why don\u2019t people use character-level machine translation?"}, {"paperId": "e79d1206292bc5e67ba19737d87d4b2ea4a37105", "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"}, {"paperId": "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"}, {"paperId": "969287b8a96e242793b11f0dbb99ec341228106f", "title": "Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"}, {"paperId": "0d4b5c9a071557f4eb12f63f785dbc89071d4272", "title": "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"}, {"paperId": "56446cb1da48cbe6e19e5051ed80c3861021e5ba", "title": "As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages"}, {"paperId": "bc87279d4b32a425377ff18ab63f7ecf95ff228c", "title": "Rethinking embedding coupling in pre-trained language models"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "d97e7561fa7710213ccd4f8128044ea6849be377", "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning"}, {"paperId": "60b8ad6177230ad5402af409a6edb5af441baeb4", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "9e9d919c1de684ca42c8b581ec62c7aa685f431e", "title": "On the Cross-lingual Transferability of Monolingual Representations"}, {"paperId": "9f4c37f154946e141a67ae2816c70b19241b3224", "title": "BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance"}, {"paperId": "8199b4c196b09d6176816e4d7db8d6f3d65e07c1", "title": "From English To Foreign Languages: Transferring Pre-trained Language Models"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "3cfd09d6d14fb9a34ee72a487cd610d168a60530", "title": "Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5", "title": "XNLI: Evaluating Cross-lingual Sentence Representations"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "c99179ca3784e3465fd9ed049d7f34b50d39393e", "title": "Ensemble learning: A survey"}, {"paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e", "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "8e32e1f02b7060ce419a964b800d0927a2e1d69c", "title": "Mimicking Word Embeddings using Subword RNNs"}, {"paperId": "563783de03452683a9206e85fe6d661714436686", "title": "HyperNetworks"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "cb7c0f9aa1060e237ce2d29260334538e0e04a56", "title": "FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models"}, {"paperId": "2f07f97563a73d9b691ec6144e4bba25a347ab87", "title": "An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": null, "title": "Jieba chinese word segmentation tool"}, {"paperId": null, "title": ": Token-free selective state space"}, {"paperId": null, "title": "Getting the most out of your tokenizer for pre-training"}, {"paperId": null, "title": "V22.1: User\u2019s manual for cplex"}]}