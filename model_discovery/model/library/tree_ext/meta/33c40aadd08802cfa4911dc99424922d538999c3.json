{"paperId": "33c40aadd08802cfa4911dc99424922d538999c3", "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "abstract": "Natural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.", "venue": "Applied Sciences", "year": 2024, "citationCount": 5, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://www.mdpi.com/2076-3417/14/5/2074/pdf?version=1709291698", "status": "GOLD"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs, and explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset."}, "embedding": {"model": "specter_v2", "vector": [0.34859758615493774, 0.6181573867797852, -0.4464988708496094, -0.19764387607574463, -0.4144654870033264, -0.3772589862346649, 0.5228391289710999, -0.10220222175121307, -0.7383461594581604, -0.035808466374874115, 0.4671149253845215, -0.14208441972732544, 0.49635207653045654, 0.22211787104606628, -0.3700554072856903, 0.618892252445221, -0.8025609254837036, 0.6660348773002625, -0.1334342658519745, -0.46497291326522827, -0.5853350758552551, -0.7762808799743652, -0.6438383460044861, 0.09725694358348846, 0.731841504573822, 0.14611738920211792, 0.4980763792991638, 0.8843422532081604, -0.3766744136810303, 0.33614373207092285, 0.7731977701187134, -0.17558032274246216, 0.14572109282016754, -0.04052604362368584, -0.2719855010509491, -0.21584096550941467, 0.48115891218185425, -0.3700518310070038, -0.4205135405063629, 0.5628155469894409, -0.1916189342737198, 0.428379088640213, 0.45943334698677063, -0.5234326124191284, -0.40356987714767456, 1.1590220928192139, 0.519845187664032, 0.6177511215209961, -0.21030190587043762, -0.372923344373703, 1.2322311401367188, -1.307449460029602, 0.3565649092197418, 1.8012865781784058, 0.4923148453235626, 0.7346206903457642, -0.5621563196182251, -0.9751535654067993, 0.503579318523407, -0.15365231037139893, -0.7876545786857605, -0.5273532867431641, -0.13743391633033752, -0.22749847173690796, 2.335292100906372, -0.4578443169593811, -0.2710307538509369, 0.44610539078712463, -0.27888455986976624, 1.5979564189910889, -0.1945817917585373, -0.9991440773010254, -0.6092439293861389, 0.15925706923007965, 0.42710429430007935, 0.9669573307037354, -0.4428805410861969, 0.42160436511039734, -0.9190430045127869, -0.1644602119922638, 0.4220598638057709, -0.3618094027042389, -0.28364208340644836, 0.003687056712806225, -0.38430672883987427, 1.1742362976074219, 0.39558374881744385, 0.9954952597618103, -0.006105358246713877, 0.7908936738967896, 0.37685707211494446, 0.5644204020500183, 0.10977526009082794, 0.528026819229126, -0.34337711334228516, 0.42779961228370667, -0.9251276850700378, 0.01660633273422718, 0.008914549835026264, 0.9717479348182678, -0.00801827758550644, 0.5876350998878479, -0.7515157461166382, 0.43922358751296997, 1.2602880001068115, 0.0037955529987812042, 0.7549757957458496, -0.5059266686439514, 0.6735840439796448, -0.5459571480751038, 0.018586833029985428, -0.11604129523038864, -0.40497392416000366, -0.40748992562294006, -0.7892706394195557, -1.5131555795669556, -0.16822277009487152, -0.22987958788871765, -0.7898443937301636, 1.136685848236084, -0.41666531562805176, -0.06419452279806137, 0.40303996205329895, 0.1498228758573532, 0.48014742136001587, 0.9287290573120117, 0.6374598145484924, -0.032605305314064026, 0.9250503778457642, -0.8141596913337708, -0.6013019680976868, -1.3715485334396362, 1.152375340461731, 0.06301675736904144, 0.3176710903644562, -0.5617730021476746, -1.1448365449905396, -1.113332986831665, -0.5766047239303589, -0.1760806441307068, -0.4485967457294464, 0.6820458769798279, 1.1807612180709839, 0.5253573656082153, -0.9366663694381714, 0.5287926197052002, 0.0006195231690071523, -0.24684274196624756, 0.07956050336360931, 0.10659357160329819, -0.020868048071861267, -0.5038526058197021, -1.7194501161575317, 0.29307985305786133, 0.7283453941345215, -0.5008310079574585, -0.42247694730758667, -0.5436092019081116, -1.5003167390823364, 0.012510309927165508, 0.0849519893527031, -0.581899106502533, 1.2285295724868774, -0.24147692322731018, -1.0928800106048584, 0.7757970094680786, -0.6808021664619446, -0.36514806747436523, 0.22010594606399536, -0.18185603618621826, -0.6458547711372375, -0.9175182580947876, -0.38781654834747314, 0.8351848125457764, 0.24996037781238556, -0.14533786475658417, -0.20265820622444153, 0.10547967255115509, -0.2929932177066803, -0.1399592161178589, -0.4429273009300232, 0.9570046663284302, -0.36435356736183167, -0.09473390132188797, 0.5868312120437622, 0.6345987319946289, -0.20212019979953766, -0.5611044764518738, -0.3495366871356964, -1.2202813625335693, 0.4953220784664154, -0.1832774579524994, 1.001376748085022, -0.8278708457946777, -0.9184511303901672, -0.21842516958713531, -0.2654411196708679, 0.04130043089389801, -1.15072500705719, 0.9386584758758545, -0.4330836534500122, 0.32687583565711975, -0.2413541078567505, -1.2331784963607788, 0.0693277046084404, -0.40720656514167786, -0.7419083118438721, -0.2961617410182953, 0.15818220376968384, 1.293087124824524, -1.1218732595443726, 0.13955040276050568, -0.06535445153713226, 0.1799984574317932, -1.031810998916626, 1.186926245689392, -0.6479724049568176, 0.07532283663749695, -0.0800047367811203, -0.33584585785865784, 0.07670874893665314, -0.22132724523544312, 0.49778231978416443, -0.3210931122303009, -0.22775506973266602, 0.35483303666114807, -0.34379205107688904, 1.5685408115386963, -0.6616421341896057, 0.46898865699768066, -0.15090984106063843, -0.44208231568336487, -0.12727461755275726, 0.7783544659614563, -0.45979318022727966, -0.17777834832668304, 0.18019290268421173, 0.5080823302268982, -0.2846372723579407, 0.4898322522640228, 0.672071635723114, 0.5495670437812805, -0.052050888538360596, 0.5366717576980591, 0.8104265928268433, -0.31171128153800964, 0.8611055612564087, 0.5372724533081055, 0.702569842338562, 0.24338895082473755, 0.44131574034690857, -0.02218732424080372, 0.513066291809082, -0.5915493369102478, -0.024693338200449944, 0.5600938200950623, 0.7833381295204163, 0.6514400839805603, -0.006931527983397245, -0.40567702054977417, -0.11563828587532043, 0.012762480415403843, 0.725594162940979, 1.7026089429855347, -0.7209230065345764, 0.13690349459648132, -0.7897124290466309, -0.3230518400669098, -0.14992229640483856, 0.4353746473789215, -0.35574665665626526, 0.07437112182378769, -0.6429606080055237, -0.8872653245925903, 0.6876038908958435, 0.4987877905368805, 0.8369131684303284, -0.7092352509498596, 0.11309485137462616, 0.12701240181922913, 0.3369680345058441, -0.9324814677238464, -0.685746967792511, 0.3036181628704071, -0.90000981092453, -0.21695862710475922, 0.032379746437072754, -0.22309274971485138, 0.25103655457496643, -0.5150437355041504, 1.0591456890106201, -0.6993314623832703, -0.0633625015616417, -0.034786347299814224, 0.6618432402610779, -0.8204436302185059, -1.2220386266708374, 0.4533938467502594, 0.2644806504249573, -0.18775731325149536, 0.38775578141212463, 0.6686113476753235, 0.46054622530937195, -0.022090472280979156, -0.4037376344203949, 0.26709166169166565, 0.22142629325389862, -0.051497284322977066, 0.8039870262145996, -0.3615109324455261, 0.43337881565093994, -1.8890235424041748, 1.2038034200668335, 0.23312002420425415, -0.539851725101471, 0.6021965146064758, -0.4662516415119171, -0.23640340566635132, 0.5914385914802551, -0.6471915245056152, -0.6524640321731567, -0.46569204330444336, 0.4074219763278961, -0.17851845920085907, -0.3820716142654419, 0.4568995237350464, 0.25038227438926697, 0.42535150051116943, 0.3746883273124695, 0.6167067885398865, 0.38773638010025024, -0.3766159415245056, 0.7459174394607544, -0.7547882199287415, 0.20680059492588043, 0.5183019638061523, 0.7005596160888672, -0.4146176874637604, -0.7211053967475891, -0.6254071593284607, -0.49670735001564026, -0.5168063640594482, -0.28235340118408203, 0.015831008553504944, 0.10578206181526184, -0.8024369478225708, -0.3684888482093811, -0.07739827036857605, -1.1408830881118774, -0.3762613832950592, 0.23792144656181335, -0.00224764971062541, -0.009933303110301495, -1.0715844631195068, -1.184226393699646, -0.6761888265609741, -0.7009468674659729, -0.7178510427474976, 0.20486980676651, 0.03453066945075989, -0.3856799304485321, -0.8283544778823853, 0.13683992624282837, -0.10834981501102448, 1.1411749124526978, -0.9950927495956421, 1.1633843183517456, 0.03643021732568741, -0.04188563674688339, -0.34866398572921753, 0.03561200946569443, 0.5251844525337219, -0.23789773881435394, -0.1361975520849228, -1.0631369352340698, -0.10371867567300797, -0.3849351406097412, -0.5864841341972351, 0.2857797145843506, 0.5813841819763184, 0.3231302797794342, 0.47265034914016724, -0.582377552986145, 0.45554250478744507, 1.4148467779159546, -0.5891668200492859, -0.22868533432483673, -0.25964200496673584, 1.0628050565719604, 0.2814858555793762, -0.5457712411880493, 0.024064477533102036, 0.36366724967956543, 0.1116185113787651, 0.0661504790186882, -0.23182576894760132, -0.05265700817108154, -0.791012167930603, 0.5925822257995605, 1.5235917568206787, 0.26006948947906494, -0.3415711522102356, -1.0987989902496338, 0.5980904698371887, -1.1268993616104126, -0.29830458760261536, 0.4822121560573578, 0.6656272411346436, 0.670590341091156, -0.5165485143661499, -0.6464110612869263, -0.464461088180542, 0.49387434124946594, 0.34649258852005005, -0.6272063255310059, -0.5750275254249573, -0.2442883551120758, 0.08633098006248474, 0.14372779428958893, 0.5803964734077454, -0.511355459690094, 1.1574498414993286, 14.27988338470459, 0.9703283905982971, 0.3246804475784302, 0.769146740436554, 0.43991896510124207, 0.43836861848831177, -0.40226081013679504, -0.17355816066265106, -1.5618611574172974, -0.6112443804740906, 1.1043163537979126, 0.12148747593164444, 0.8115242123603821, 0.2002546489238739, 0.3693913221359253, 0.0826723650097847, -0.6558673977851868, 0.7761015295982361, 0.4067477583885193, -1.2286020517349243, 0.8837760090827942, 0.13173645734786987, 0.5089263319969177, 0.8750025629997253, 0.42912232875823975, 1.130041241645813, 0.5986124277114868, -0.523205578327179, 0.23690706491470337, 0.29072970151901245, 0.8854142427444458, 0.25998562574386597, 0.4406392276287079, 1.0547828674316406, -0.9636525511741638, -0.3517373204231262, -0.6943414807319641, -0.9910654425621033, 0.2863346040248871, -0.013303902000188828, -0.6197807192802429, -0.5466293096542358, -0.7431929111480713, 0.6589238047599792, -0.016672862693667412, 0.3048286437988281, -0.5106855034828186, 1.05782949924469, -0.18854357302188873, 0.11576107889413834, 0.319703072309494, 0.3657015264034271, 0.30303916335105896, 0.051244787871837616, 0.08650494366884232, -0.015218553133308887, 0.18701839447021484, 0.5213649868965149, -0.9549660086631775, 0.2659885287284851, -0.43428802490234375, -0.45554906129837036, -0.19474492967128754, 0.6960386037826538, 0.7536055445671082, 0.2886831760406494, -0.3293970823287964, 0.11075901985168457, 0.9043771028518677, 0.31153833866119385, -0.019698750227689743, 0.149278923869133, 0.033810265362262726, -0.30706557631492615, -0.26471367478370667, 0.4114944040775299, 0.07447368651628494, -0.7314902544021606, -0.7177404761314392, -0.2991774380207062, 0.4413502812385559, -0.5841654539108276, -0.8128710985183716, 0.9761807918548584, -0.20643316209316254, -0.3450325131416321, -0.2504270374774933, -0.8084419965744019, -0.20583653450012207, 0.8458046913146973, -1.5975207090377808, -0.9412387609481812, 0.5021601319313049, -0.30721139907836914, -0.11366117745637894, -0.35564473271369934, 1.3738397359848022, -0.16115182638168335, -0.708171546459198, 0.15005826950073242, 0.41489529609680176, 0.21903343498706818, -0.4272584319114685, -0.6471996903419495, 1.0024628639221191, 0.29554322361946106, 0.15010055899620056, 0.28611138463020325, -0.29691827297210693, 0.08042281866073608, -0.9581582546234131, -0.3189021646976471, 1.0691395998001099, -0.6880947351455688, -0.4775395095348358, -0.8124747276306152, -0.8742998242378235, 0.5615677237510681, 0.5164482593536377, -0.43187060952186584, 0.3709298372268677, 0.29307588934898376, -0.5808112025260925, 0.14471375942230225, -0.8218223452568054, -0.07050870358943939, 0.5287171006202698, -0.7878991365432739, -0.23364657163619995, 0.18542084097862244, 0.4087851643562317, -0.7325777411460876, -0.40755340456962585, -0.30784106254577637, -0.1885138601064682, 0.47641685605049133, 0.6115796566009521, -0.7869081497192383, 0.2374553680419922, 0.9591248035430908, 0.024171994999051094, -1.0473796129226685, 0.006331232376396656, -0.8844192028045654, -0.04212132841348648, 0.15763339400291443, 1.1879446506500244, -0.7380099296569824, 0.2883160412311554, 0.8180766701698303, 0.22118167579174042, -0.22750094532966614, -0.6618219614028931, -0.5707658529281616, 0.06668906658887863, -0.5736837983131409, 0.15117156505584717, 0.06785742193460464, -0.36848628520965576, 0.5715337991714478, 0.6012595295906067, 0.5477492213249207, -0.13431847095489502, -0.6160012483596802, 0.5683062672615051, -0.3543110489845276, -0.18494684994220734, -0.24049316346645355, -0.0785808339715004, -1.389020323753357, 0.45737946033477783, -1.560793399810791, -0.12241481989622116, -1.3232040405273438, -0.2516310513019562, -0.07079079002141953, 0.16824322938919067, 0.21525803208351135, 0.3590415120124817, -0.4926801919937134, -0.5209165215492249, -0.5906378626823425, -0.013909386470913887, 0.7364723086357117, 0.7663614749908447, -0.6419420838356018, -0.25223517417907715, 0.025750936940312386, 0.3637472987174988, 0.31335633993148804, 0.5642192363739014, -0.45044228434562683, -1.1448942422866821, -1.8853449821472168, 0.24217231571674347, 0.09892508387565613, -0.271808385848999, -0.6928889155387878, 0.65915447473526, 0.5242609977722168, -0.2829429805278778, 0.03171577677130699, 0.31968265771865845, -0.6920529007911682, -0.5023534297943115, 0.416187584400177, -0.7434510588645935, 0.11950118094682693, 0.40679845213890076, -0.48850739002227783, -0.6230302453041077, 0.23198077082633972, -0.10132340341806412, -1.2373497486114502, -0.7794733643531799, 0.522040069103241, -0.43066513538360596, 0.357515811920166, -0.30556797981262207, -0.11148504167795181, -0.8615743517875671, -0.5740605592727661, -0.007408392149955034, 0.7115907669067383, -0.6845544576644897, 0.7943366765975952, 0.4651029109954834, -0.6979919672012329, -0.44522029161453247, 0.4571467339992523, 0.03932870551943779, -0.059718020260334015, 0.7390004396438599, 0.1653352528810501, -0.23744848370552063, 0.8673506379127502, 0.6665376424789429, 0.3763424754142761, -0.7875863313674927, -0.244583860039711, 0.967307448387146, -0.7430794835090637, 0.0883089229464531, 1.1617062091827393, -0.19560161232948303, -1.6309257745742798, 0.04845298081636429, -1.11391282081604, -0.6589352488517761, -0.24434369802474976, 0.6145746111869812, -0.056421179324388504, -0.07710891962051392, -0.2844950258731842, -0.044909194111824036, 0.07676471769809723, -0.059344131499528885, -0.5280697345733643, 0.7439902424812317, -0.09073265641927719, -0.2602962553501129, 0.6731885075569153, 0.8533876538276672, -0.7758386731147766, -0.5688632726669312, -0.8672997951507568, -0.3369200825691223, 0.19292455911636353, 0.6247004866600037, -0.5578746795654297, -0.6788826584815979, 0.8702547550201416, 0.4069632291793823, 0.18861153721809387, -0.09613576531410217, -0.24931496381759644, 0.4076325297355652, 0.6949504017829895, 0.22350360453128815, -0.7406833171844482, -1.0117017030715942, 1.7274307012557983, 1.3127870559692383, -1.3000820875167847, -0.18339723348617554, -0.5832125544548035, -0.6644158959388733, 0.7766206860542297, 0.40212351083755493, 0.2295951247215271, 1.0361642837524414, -0.6296795010566711, 0.1037416085600853, 0.2135218381881714, -1.1075326204299927, -0.339470237493515, 0.8783868551254272, 0.8368951082229614, 1.2725179195404053, 0.4436887204647064, -0.016903864219784737, 0.781949520111084, 0.01488571334630251, 0.2590794563293457, 0.3197801411151886, 0.25087565183639526, -0.22935745120048523, -0.13812948763370514, 0.17317906022071838, 0.7310501337051392, -0.26116621494293213, -0.8571339249610901, 0.103836290538311, 0.8122978210449219, 0.14419688284397125, 0.7658240795135498, 0.6927577257156372, 0.4703245460987091, 0.626858651638031, 0.47841551899909973, 0.41618961095809937, -0.9151147603988647, -0.4185096025466919, -0.3601568341255188, -0.5582407712936401, 0.062346287071704865, 0.03160007670521736, -0.42080453038215637, -0.43134304881095886, 0.32100099325180054, 0.09634823352098465, 0.05932736024260521, 0.47764861583709717, 0.9472348093986511, 0.38402026891708374, 0.24897709488868713, -0.4459162652492523, -0.11744067072868347, -0.695651650428772, -1.5787550210952759, -0.23508399724960327, -0.6116229891777039, -0.40368011593818665, 0.06586660444736481, -0.1229432225227356, -0.26322120428085327]}, "authors": [{"authorId": "2289385425", "name": "Rajvardhan Patil"}, {"authorId": "117730513", "name": "V. Gudivada"}], "references": [{"paperId": "46ae68f188e2ce1bad703878e8f14e710ecb6009", "title": "Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "bafe023fb072045dc0cd50316382a61c8dcb9fae", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d", "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "1bb6d5761903c7ac978188ae36e2648905e95dc5", "title": "Transcending Scaling Laws with 0.1% Extra Compute"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "d219d971b16c708b8debc731e0c541ae218c7caf", "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "914254fac74a2da051cccf6ca16afcaad416a079", "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "15190e8b459bd85d546286f7d7da61b4f4f3f58a", "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3", "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models"}, {"paperId": "5cbe278b65a81602a864184bbca37de91448a5f5", "title": "Competition-level code generation with AlphaCode"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904", "title": "Few-shot Learning with Multilingual Generative Language Models"}, {"paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170", "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "0794333779d775abc2053052d1e7009066cbd4f1", "title": "SaFeRDialogues: Taking Feedback Gracefully after Conversational Safety Failures"}, {"paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79", "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"}, {"paperId": "2d4f66046bb436864cd6bf589e3a931c405f9f44", "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"}, {"paperId": "a6fdb277d0a4b09899f802bda3359f5c2021a156", "title": "Recursively Summarizing Books with Human Feedback"}, {"paperId": "a6d8d04962f84ae6225e72723869a002b9fc8036", "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "2ef4ab54d00203f9ac610213ac3abc8e1fe541b4", "title": "Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "00a95c2e2af1c6ef7ba41fe502a8cc729cdd284d", "title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "56fa0b9cba4d9aee5ccc327365b3b3a721031c69", "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"}, {"paperId": "4f09e6ec1b7d4390d23881852fd7240994abeb58", "title": "A statistical interpretation of term specificity and its application in retrieval"}, {"paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74", "title": "What Makes Good In-Context Examples for GPT-3?"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "abaadb4c6affc4d874c4f59bfac60686e851cb5e", "title": "Pre-training Text-to-Text Transformers for Concept-centric Common Sense"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "7eda139d737eea10fc1d95364327a41ec0cee4a4", "title": "CoLAKE: Contextualized Language and Knowledge Embedding"}, {"paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1", "title": "Learning to summarize from human feedback"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "c014f8bc3b521453a93a13bb2c90700fcf462738", "title": "Limits to Depth Efficiencies of Self-Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "babeda48b10a4d638252118f2238d05a06f4ec55", "title": "StereoSet: Measuring stereotypical bias in pretrained language models"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "4f03e69963b9649950ba29ae864a0de8c14f1f86", "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "c7fc1cac162c0e2a934704184c7554fd6b6253f0", "title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model"}, {"paperId": "56cafbac34f2bb3f6a9828cd228ff281b810d6bb", "title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "7a15950dc71079285a4eaf195de5aadd87c41b40", "title": "Fine-Tuning Language Models from Human Preferences"}, {"paperId": "bfeb827d06c1a3583b5cc6d25241203a81f6af09", "title": "Knowledge Enhanced Contextual Word Representations"}, {"paperId": "5f994dc8cae24ca9d1ed629e517fcc652660ddde", "title": "ERNIE: Enhanced Language Representation with Informative Entities"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7", "title": "Gender Bias in Coreference Resolution"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "title": "Learned in Translation: Contextualized Word Vectors"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "e9839782321e25ed87f1a14abf7107e0d037e630", "title": "Multimodal Word Distributions"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "59761abc736397539bdd01ad7f9d91c8607c0457", "title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM"}, {"paperId": "e2dba792360873aef125572812f3673b1a85d850", "title": "Enriching Word Vectors with Subword Information"}, {"paperId": "233bc1bbdf5c4c08da204f545b1eaf15876ea786", "title": "Word Representations via Gaussian Embedding"}, {"paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "title": "GloVe: Global Vectors for Word Representation"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "title": "Distributed Representations of Words and Phrases and their Compositionality"}, {"paperId": "f6b51c8753a871dc94ff32152c00c01e94f90f09", "title": "Efficient Estimation of Word Representations in Vector Space"}, {"paperId": "577d19a115f9ef6f002483fcf88adbb3b5479556", "title": "Independent component analysis: algorithms and applications"}, {"paperId": "a1066659ec1afee9dce586f6f49b7d44527827e1", "title": "A Statistical Approach to Machine Translation"}, {"paperId": "d5f169880e30e1f76827d72f862555d00b01bed9", "title": "A vector space model for automatic indexing"}, {"paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b", "title": "Crosslingual Generalization through Multitask Finetuning"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "fe025b18b78e0f91ea598bbd58f896dfc4f65644", "title": "Comparing and Combining Dimension Reduction Techniques for Efficient Text Clustering"}, {"paperId": "aea5a0b817c09915b118cc13c7d6016045582e03", "title": "Computer Evaluation of Indexing and Text Processing"}, {"paperId": "decd9bc0385612bdf936928206d83730718e737e", "title": "Distributional Structure"}, {"paperId": null, "title": "Storage cost for finetuned models: a large language model usually takes hundreds of gigabytes (GBs) to store, and as many model copies as the number of downstream tasks need to be stored"}, {"paperId": null, "title": "Computational cost for pretraining: large language models require thousands of GPUs with several weeks of pretraining"}, {"paperId": null, "title": "Equipment cost for inference: it is expected to use multiple GPUs to infer a large language model"}]}