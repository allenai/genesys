{"paperId": "955f90930d48750e7239478b4eed440eb84131cd", "title": "FourierFormer: Transformer Meets Generalized Fourier Integral Theorem", "abstract": "Multi-head attention empowers the recent success of transformers, the state-of-the-art models that have achieved remarkable success in sequence modeling and beyond. These attention mechanisms compute the pairwise dot products between the queries and keys, which results from the use of unnormalized Gaussian kernels with the assumption that the queries follow a mixture of Gaussian distribution. There is no guarantee that this assumption is valid in practice. In response, we \ufb01rst interpret attention in transformers as a nonparametric kernel regression. We then propose the FourierFormer, a new class of transformers in which the dot-product kernels are replaced by the novel generalized Fourier integral kernels. Different from the dot-product kernels, where we need to choose a good covariance matrix to capture the dependency of the features of data, the generalized Fourier integral kernels can automatically capture such dependency and remove the need to tune the covariance matrix. We theoretically prove that our proposed Fourier integral kernels can ef\ufb01-ciently approximate any key and query distributions. Compared to the conventional transformers with dot-product attention, FourierFormers attain better accuracy and reduce the redundancy between attention heads. We empirically corroborate the advantages of FourierFormers over the baseline transformers in a variety of practical applications including language modeling and image classi\ufb01cation.", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 24, "influentialCitationCount": 2, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The FourierFormer is proposed, a new class of transformers in which the dot-product kernels are replaced by the novel generalized Fourier integral kernels, which can automatically capture the dependency of the features of data and remove the need to tune the covariance matrix."}, "embedding": {"model": "specter_v2", "vector": [0.3738795518875122, 0.11319130659103394, -0.462953120470047, -0.32602745294570923, -0.5246993899345398, 0.03631287440657616, 0.7214928865432739, -0.4575662910938263, -0.5000360012054443, -0.6986888647079468, 0.4085446894168854, 0.44200900197029114, 0.10072727501392365, 0.06845088303089142, -0.377140074968338, 0.059057511389255524, -0.7812966108322144, 0.34625664353370667, 0.28948482871055603, -0.029106400907039642, -0.0028239204548299313, -0.871680498123169, -1.5112559795379639, 0.043550316244363785, 0.1778000295162201, 1.0657498836517334, 0.4216920733451843, 1.1580342054367065, -0.36397597193717957, 0.2419683188199997, 0.6571877002716064, -0.3450547456741333, 0.2991497218608856, 0.15381500124931335, -0.11986830830574036, -0.19600355625152588, 0.4581736624240875, -0.5816491842269897, -0.9302365779876709, 0.9810766577720642, -0.06989262998104095, 0.5241193175315857, 1.030198097229004, -0.6789140105247498, -0.7092460989952087, 0.41161930561065674, 0.6922447085380554, 0.7728037238121033, -0.38753020763397217, -0.8335263729095459, 1.5108643770217896, -1.5555272102355957, -0.017789894714951515, 1.381972312927246, 0.24093523621559143, 0.09746250510215759, -0.18934902548789978, -0.507902979850769, 0.5652143955230713, 0.36513668298721313, -0.8644117116928101, -0.31097617745399475, -0.46540695428848267, -0.2996545732021332, 1.617234230041504, -0.38747262954711914, -0.12820912897586823, 0.31553661823272705, 0.16506515443325043, 1.5796325206756592, -0.2503357231616974, -0.6987353563308716, -0.3605003356933594, -0.04032548516988754, 0.05987738445401192, 0.8823187947273254, -0.9894211292266846, 0.4892413318157196, -1.2166829109191895, -0.38640615344047546, 0.5653818845748901, 0.299250990152359, -0.29563474655151367, -0.30222904682159424, -0.30197107791900635, 0.85286945104599, 0.38126447796821594, 0.4525272250175476, -0.40343016386032104, 0.903898298740387, 0.42220333218574524, 0.11750040203332901, 0.0787789523601532, -0.03078019805252552, 0.07318715751171112, -0.05263761430978775, -0.7883107662200928, 0.23389744758605957, 0.0435449592769146, 0.9921016693115234, -0.11656293272972107, -0.17318418622016907, -0.5395442247390747, 0.17345543205738068, 1.571513295173645, 0.34118759632110596, 0.4801599085330963, -0.3158561885356903, -0.23017670214176178, -0.9056447148323059, 0.058177828788757324, -0.73824542760849, 0.29576194286346436, 0.1761699616909027, -0.47864922881126404, -1.2438794374465942, -0.6111268997192383, 0.8095073699951172, -0.9032619595527649, 0.6105062961578369, -0.36725127696990967, 0.06800751388072968, -0.4762769639492035, 0.5100993514060974, 0.3338156044483185, 0.6325533390045166, 0.5202103853225708, 0.4544951319694519, 1.0551103353500366, -0.6849958896636963, -0.6728467345237732, -0.4234232008457184, 0.7074216604232788, -0.47027453780174255, 0.41691288352012634, -0.1913754791021347, -0.8364279866218567, -1.1626935005187988, -0.4905044734477997, 0.07125508040189743, -0.6639138460159302, 0.29834863543510437, 0.9479683041572571, 0.28511950373649597, -1.1882761716842651, 0.5611549019813538, -0.14940060675144196, -0.38835158944129944, 0.37033164501190186, 0.34627124667167664, -0.09330298006534576, -0.48736655712127686, -1.3457707166671753, 0.37673071026802063, 0.1410141885280609, -0.9548835754394531, -0.2943556606769562, -0.5854369401931763, -1.1152641773223877, 0.08614888042211533, 0.4467664361000061, -0.6020249128341675, 1.3055752515792847, -0.08274169266223907, -1.0038129091262817, 0.5937354564666748, -0.5133331418037415, 0.19831280410289764, -0.08822470158338547, -0.7677801251411438, -0.4089825451374054, -0.5032979249954224, -0.008227402344346046, 0.2680347263813019, 0.6064367294311523, 0.1020231693983078, -0.35221484303474426, 0.10195387154817581, -0.6292536854743958, 0.018051939085125923, -0.4501689076423645, 1.1719022989273071, -0.5232903361320496, -0.5143306255340576, 0.024438979104161263, 0.7034300565719604, 0.301685094833374, 0.006016116123646498, -0.06722503900527954, -1.2328944206237793, 1.1758062839508057, 0.10044775903224945, 1.1178542375564575, -1.1202256679534912, -0.650333821773529, -0.16446024179458618, -0.03851252421736717, -0.2095947414636612, -0.7754397988319397, 1.098256230354309, -0.33671700954437256, 0.11102139949798584, 0.08041395246982574, -1.0146763324737549, 0.1886797547340393, -0.2558111548423767, -0.7867501378059387, -0.14731936156749725, 0.034186769276857376, 1.3909306526184082, -0.8372837901115417, 0.21376028656959534, -0.05533822998404503, 0.22784775495529175, -0.5854028463363647, 1.5499646663665771, -0.2288048267364502, -0.6003026366233826, -0.14664725959300995, -0.2588613033294678, -0.1220964714884758, -0.6606271862983704, -0.3013898432254791, -0.6392040252685547, 0.09568783640861511, 0.2581748962402344, -0.10343826562166214, 1.3623842000961304, 0.14096961915493011, 0.2744141221046448, -0.5101948976516724, -0.8390228152275085, 0.28841641545295715, 0.46818655729293823, 0.048370361328125, -0.4275246262550354, 0.07825113832950592, 0.3463219106197357, -0.7299811840057373, -0.05592237785458565, 0.6367966532707214, 0.8516215682029724, -0.18237857520580292, 0.4953005313873291, 0.5868265628814697, -0.21775172650814056, -0.18336865305900574, 0.346925288438797, 0.6837934851646423, 0.47168222069740295, 0.6420319080352783, -0.17853538691997528, 0.11824135482311249, -1.2392165660858154, -0.6585161089897156, 0.7828453183174133, 0.6876268982887268, 0.8549981713294983, -0.0441749133169651, -0.781254768371582, -0.19486571848392487, -0.3960239887237549, 0.8333318829536438, 2.02714204788208, -0.18875543773174286, -0.31165748834609985, -0.5656008124351501, -0.3129551112651825, -0.20630617439746857, -0.2374325841665268, -0.8797021508216858, -0.38742923736572266, 0.06252367049455643, -1.0103622674942017, 0.7799160480499268, 0.6472281217575073, 0.8338981866836548, -0.4687369465827942, -0.3506028950214386, -0.1788133829832077, 0.3363751471042633, -0.6049377918243408, -1.361477017402649, 0.3964216411113739, -0.02141563594341278, -0.08926890045404434, -0.14366084337234497, -0.0516003854572773, -0.09376408159732819, -0.3400610387325287, 0.643650233745575, -0.8769159317016602, -0.5950732827186584, 0.7770571708679199, 0.22725945711135864, -0.8996789455413818, -0.24245202541351318, 0.15572558343410492, 0.09944198280572891, 0.23722465336322784, 0.8085469007492065, 0.325844943523407, -0.09989847987890244, 0.33660268783569336, -0.7065309882164001, -0.04541527107357979, 0.45119860768318176, 0.31610602140426636, 0.49796929955482483, -0.6047173738479614, -0.14263014495372772, -0.8943849205970764, 0.8438245058059692, -0.035349875688552856, -0.23162071406841278, 0.4675666093826294, -0.7571415901184082, -0.30723872780799866, 0.007728072348982096, -0.9352555274963379, -0.05775047466158867, -0.49891576170921326, 0.5189859867095947, -0.6676355004310608, -0.12875677645206451, 0.5198275446891785, 0.25060415267944336, 0.022185880690813065, 0.4531635046005249, 0.8942215442657471, 0.7241801619529724, 0.3081965148448944, 0.5914714932441711, -0.6877692341804504, 0.719986081123352, -0.0891197919845581, 0.08672934025526047, 0.004890278913080692, -0.3675694763660431, -0.794253408908844, 0.015362147241830826, -1.1428598165512085, -0.07539741694927216, -0.2844991683959961, 0.20086972415447235, -0.5145208835601807, -1.008542776107788, 0.11111778020858765, -1.0353672504425049, 0.02987831085920334, 0.17047274112701416, -0.01980467140674591, -0.680708110332489, -0.9434561729431152, -0.9883719682693481, -0.7393323183059692, -0.6290428638458252, -0.7922384738922119, 0.5623063445091248, 0.06958665698766708, -0.36720970273017883, -0.5702407360076904, 0.03758858144283295, -0.6801676154136658, 1.380702018737793, -0.8938888311386108, 0.682373583316803, -0.345506876707077, -0.5424638390541077, -0.37349632382392883, -0.15264958143234253, 0.4923696219921112, 0.3240666687488556, 0.1623402088880539, -0.9844446182250977, 0.3522031009197235, -0.26211047172546387, -0.09185852855443954, 0.4202132821083069, 0.5978493690490723, 0.9740769267082214, 0.07204050570726395, -0.5821774005889893, 0.19206205010414124, 1.2933493852615356, -0.30889013409614563, -0.018824199214577675, -0.2901204526424408, 0.807027280330658, 0.14950165152549744, -0.1505473256111145, 0.8965791463851929, 0.6445361375808716, 0.5402640700340271, 0.2183792144060135, 0.12947075068950653, 0.2935033142566681, -0.5393825173377991, 0.597517192363739, 1.4071729183197021, 0.014758042059838772, -0.14264686405658722, -0.8391482830047607, 0.8402613997459412, -1.633561134338379, -1.0480886697769165, 0.7502593398094177, 0.8400957584381104, -0.02382880263030529, -0.8303254842758179, 0.20586636662483215, -0.9683658480644226, 0.24831907451152802, 0.27530327439308167, -0.27710407972335815, -0.10908691585063934, 0.05991143733263016, 0.6379317045211792, 0.28993862867355347, 0.2977283298969269, -0.7299967408180237, 0.32417193055152893, 14.737385749816895, 0.7166780233383179, -0.2174084484577179, 0.6240296959877014, 0.4834589958190918, 0.300505667924881, -0.1884007304906845, -0.3548074960708618, -1.0692119598388672, 0.15650443732738495, 1.0283308029174805, 0.008586262352764606, 0.46578681468963623, 0.3893759250640869, -0.19248564541339874, 0.22197210788726807, -0.615749716758728, 1.0948997735977173, 0.7709446549415588, -1.041735291481018, 0.17452524602413177, 0.4077499210834503, -0.07955251634120941, 0.5589185357093811, 0.9776800870895386, 0.4322611093521118, 0.5701929926872253, -0.750937819480896, 0.3736385703086853, 0.8986514210700989, 0.8111053705215454, 0.20594647526741028, 0.20432589948177338, 0.5580214262008667, -1.1576906442642212, 0.15482263267040253, -0.433927446603775, -0.7914910912513733, 0.3434629440307617, -0.01532840821892023, -0.6349226236343384, -0.13200008869171143, 0.07216418534517288, 0.6840559244155884, 0.5392248034477234, 0.4606904685497284, 0.23234331607818604, 0.47662416100502014, -0.2177192121744156, 0.003697797888889909, 0.17320454120635986, 0.7298011183738708, -0.3147231638431549, 0.22870245575904846, 0.36289313435554504, 0.12511655688285828, 0.4469652771949768, 0.5193291306495667, -0.24155588448047638, 0.16690295934677124, -0.3345147967338562, -0.3796614110469818, -0.17920905351638794, 0.661607027053833, 0.7102699875831604, 0.19787666201591492, -0.6849423050880432, 0.48868027329444885, 0.4113979637622833, 0.39296355843544006, -0.20014908909797668, -0.06104017049074173, 0.6082053184509277, -0.22892311215400696, 0.1975037008523941, 0.7694870233535767, 0.25219452381134033, -0.06861826777458191, -0.8575943112373352, -0.10464927554130554, 0.47646158933639526, -0.7916771769523621, -1.454879641532898, 0.4844333231449127, 0.08701864629983902, -0.09303086251020432, 0.3056691884994507, -0.9111613631248474, -0.134257510304451, 0.9756065607070923, -1.1233885288238525, -0.8394262790679932, 0.10598853975534439, -0.15523330867290497, -0.23000146448612213, -0.008714874275028706, 1.4717952013015747, 0.08423130959272385, 0.36643049120903015, 0.07835657894611359, -0.28934991359710693, 0.09573087096214294, 0.20649108290672302, -0.7764541506767273, 0.9393176436424255, -0.09315665066242218, -0.05112964287400246, 0.6610525846481323, -0.11750082671642303, -0.013908118940889835, -0.4989365339279175, 0.2525918483734131, 0.45760613679885864, -0.947902500629425, -0.34433242678642273, -0.5391088128089905, -0.9685694575309753, 0.11944964528083801, 0.4119575023651123, -0.08278754353523254, 0.7000575661659241, 0.07866454124450684, -0.7360185384750366, -0.030880630016326904, -0.1626550704240799, -0.4482233226299286, 0.41794997453689575, -1.2340010404586792, -0.08741682767868042, -0.21803517639636993, 0.1172875389456749, -0.9413989782333374, -0.6078647971153259, -0.19446147978305817, 0.5185584425926208, 0.032365910708904266, 0.9453281164169312, -0.532208263874054, 0.37439268827438354, 0.6802630424499512, -0.32206299901008606, -0.7280120253562927, -0.434884250164032, -0.8360536098480225, -0.45973798632621765, -0.3821755349636078, 0.2197359949350357, -0.5289469361305237, 0.21116089820861816, 0.7058807015419006, 0.36093300580978394, -0.5395699143409729, -0.4719890356063843, -0.18563945591449738, -0.4598195552825928, -0.5480829477310181, 0.19464416801929474, -0.1276037096977234, -0.1326325237751007, -0.2859402894973755, 0.20316791534423828, 0.38576260209083557, -0.2662753760814667, -1.0086930990219116, 0.3850066661834717, -0.39935067296028137, -0.284537672996521, -0.6867443919181824, -0.8344563841819763, -1.7621339559555054, -0.2502433955669403, -0.6658047437667847, 0.25446298718452454, -0.6876540184020996, -0.10639882832765579, 0.28508201241493225, -0.4112265706062317, -0.010487785562872887, 0.060308787971735, -0.7355436086654663, -0.5331394672393799, -0.3878086805343628, -0.29009810090065, 0.8805654048919678, 0.8442010879516602, -0.501258909702301, 0.43886229395866394, 0.2268192023038864, 0.05482597276568413, -0.1459003984928131, 0.1558481901884079, -0.256560742855072, -1.0044703483581543, -0.7136688828468323, 0.22223614156246185, -0.2384185642004013, 0.2064121663570404, -0.4572158753871918, 1.1009068489074707, 0.16613197326660156, -0.20958106219768524, -0.022524485364556313, 0.706398606300354, -0.8008032441139221, -0.2719249725341797, 0.14288420975208282, -0.875167191028595, 0.195537731051445, -0.21860000491142273, -0.2744576334953308, -0.32972252368927, 1.0096089839935303, -0.27178955078125, -1.1220921277999878, -0.5690819621086121, 1.0191593170166016, -0.18395832180976868, 0.04183964803814888, -0.0871882513165474, -0.15858924388885498, -1.020418405532837, -0.6915847063064575, 0.12297282367944717, -0.0012626947136595845, -0.36859676241874695, 1.0148365497589111, 0.8871498703956604, -1.3324493169784546, 0.2641671895980835, 0.46332231163978577, 0.5088659524917603, -0.003509370144456625, 0.7152062654495239, 0.5753961205482483, 0.20858192443847656, 0.6367895603179932, 0.2513063848018646, 0.3021080791950226, -1.124175786972046, 0.09435390681028366, 0.5884224772453308, -0.13294276595115662, 0.11340576410293579, 1.3628276586532593, 0.3636931777000427, -0.897836446762085, 0.029993658885359764, -1.175865650177002, -0.16323401033878326, -0.3498234450817108, 0.7454544305801392, -0.14753945171833038, -0.161508247256279, -0.37862563133239746, -0.42649438977241516, 0.064666748046875, -0.5354351997375488, -0.5882954597473145, 0.3319118618965149, 0.24785268306732178, -0.2922172248363495, 0.6480209827423096, 1.0062650442123413, -1.1923589706420898, -0.6749396324157715, -1.209566593170166, -0.49536725878715515, -0.38605475425720215, -0.050260744988918304, -0.16722601652145386, -0.11496219784021378, 0.7618563175201416, 0.5731492042541504, 0.45507410168647766, 0.20212426781654358, 0.27327531576156616, -0.025755299255251884, 0.2581561803817749, -0.21885226666927338, -0.4962227940559387, -0.3469158411026001, 1.2557693719863892, 1.196711540222168, -0.5535224080085754, 0.08764415979385376, -0.5010658502578735, -0.5943244695663452, 0.915423572063446, 0.3981478810310364, 0.0145360566675663, 1.2477385997772217, -0.10530257970094681, 0.04986650124192238, -0.20271390676498413, -1.097333312034607, -0.5054601430892944, 1.4554545879364014, 1.4719465970993042, 0.6252317428588867, 0.4076533019542694, 0.49923989176750183, 0.4777924120426178, 0.3651226758956909, 0.2257358878850937, -0.08733158558607101, 0.40512046217918396, -0.5043573379516602, -0.10160595178604126, 0.15398573875427246, 1.1081011295318604, -0.45021480321884155, -0.5042959451675415, 0.34258410334587097, 0.35519978404045105, -0.1220240369439125, -0.20513688027858734, 0.6587253212928772, -0.14376170933246613, 0.5765498280525208, 0.5184096693992615, 0.35502421855926514, -0.5880206227302551, 0.17890621721744537, -0.31225892901420593, -1.0703073740005493, -0.03709852322936058, -0.47479525208473206, -0.766029953956604, -0.0001136956489062868, 0.05465422198176384, 0.2245916873216629, -0.3190923035144806, 0.14326167106628418, 1.101131796836853, 0.6451396942138672, 0.2277020812034607, -0.47097542881965637, -0.29195287823677063, -0.641074538230896, -1.0517834424972534, -0.07369963079690933, -0.4224449694156647, -0.15404899418354034, -0.525855302810669, -0.04520720615983009, 0.16143132746219635]}, "authors": [{"authorId": "150322732", "name": "T. Nguyen"}, {"authorId": null, "name": "Minh Pham"}, {"authorId": "2116488139", "name": "Tam Nguyen"}, {"authorId": "145546032", "name": "Khai Nguyen"}, {"authorId": "103583159", "name": "S. Osher"}, {"authorId": "3526349", "name": "Nhat Ho"}], "references": [{"paperId": "260b9388c90b497218b591a9a0e2742b7e0951e5", "title": "Momentum Transformer: Closing the Performance Gap Between Self-attention and Its Linearization"}, {"paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a", "title": "Flowformer: Linearizing Transformers with Conservation Flows"}, {"paperId": "fe9d978f7718474e9613bac114c398614f09be71", "title": "Sinkformers: Transformers with Doubly Stochastic Attention"}, {"paperId": "48af9b314181b04edcc0b7224ffe4689036b755f", "title": "Improving Transformers with Probabilistic Attention Keys"}, {"paperId": "23d11338be48471b3979b13eb172ec67fc22244b", "title": "Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model"}, {"paperId": "37abe53ed31caa23ae833b2e67bb4aa1892e8d25", "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "94eae578e6af3382f6449506965639f18aab3fa0", "title": "Video Swin Transformer"}, {"paperId": "0d46cbaf914da31a06ef2753e00b7f47e055e70d", "title": "Probabilistic Attention for Interactive Segmentation"}, {"paperId": "5e58a73db001140b10abb9b6746a578185b69a53", "title": "Introduction to the finite element method"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "5863d7b35ea317c19f707376978ef1cc53e3534c", "title": "Rethinking Graph Transformers with Spectral Attention"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "72f207c777e4a17180cc54ccc6a743d5f43227af", "title": "Choose a Transformer: Fourier or Galerkin"}, {"paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6", "title": "Multiscale Vision Transformers"}, {"paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "title": "ViViT: A Video Vision Transformer"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "bfdd9e62ef361c79e643f38690b4eb6832050e15", "title": "Multivariate Smoothing via the Fourier Integral Theorem and Fourier Kernel"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "ff50b46b4e1cc0fd9beb832fc3468785b635a824", "title": "PCT: Point cloud transformer"}, {"paperId": "e1d082562981a9f51649c60663aa484ee623dbb0", "title": "Point Transformer"}, {"paperId": "2051548f7681c96d603de932ee23406c525276f9", "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning"}, {"paperId": "9f0272bb258506fdc0ee7d8951593914d4f9c39d", "title": "Analyzing Individual Neurons in Pre-trained Language Models"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "45988d39ab1b0e5199e1f0f31952760bc763e611", "title": "The Lipschitz Constant of Self-Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4", "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"}, {"paperId": "3b504f939e55d567652737ef093c1087cd40689b", "title": "Analyzing Redundancy in Pretrained Transformer Models"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "199ff73d2f728e997f860b62a2322823d3e3d9e8", "title": "Designing and Interpreting Probes with Control Tasks"}, {"paperId": "9d7902e834d5d1d35179962c7a5b9d16623b0d39", "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "81e1d123a85562555befb0243256b1a0d9fca014", "title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145", "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d8abb8206b913d185b4bd406880131c13759a6ff", "title": "The UEA multivariate time series classification archive, 2018"}, {"paperId": "f0ded4902d7f9c111e50047f8c9494effb7282d1", "title": "Sorting out Lipschitz function approximation"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "230273d17d05d3edebbe5aeda1726d2bfc1edf8a", "title": "Multivariate Kernel Smoothing and Its Applications"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "84de7d27e2f6160f634a483e8548c499a2cda7fa", "title": "Spectral Normalization for Generative Adversarial Networks"}, {"paperId": "0bd8c29a206c46dccca63c010a95734018c98d2e", "title": "Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks"}, {"paperId": "8899094797e82c5c185a0893896320ef77f60e64", "title": "Non-local Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "013efe3ff541e518c51f08d1b62a62e0c57c0b14", "title": "Parseval Networks: Improving Robustness to Adversarial Examples"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "a2d407962bb1f5fcd209114f5687d4c11bf9dfad", "title": "All-but-the-Top: Simple and Effective Postprocessing for Word Representations"}, {"paperId": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "379daa0dabeffa72b9a0d8c48b361236c03fb302", "title": "Introduction to Nonparametric Estimation"}, {"paperId": "0eb36599c7e976b6e52a3693bbe30995cdbbb330", "title": "All of Nonparametric Statistics"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "518b8879e5166dba7ee79e6eef463bb67eddc093", "title": "Comparison of Smoothing Parameterizations in Bivariate Kernel Density Estimation"}, {"paperId": "5425f78e0414cd14a467afbfacfa6939c256701b", "title": "On the Optimal Rates of Convergence for Nonparametric Deconvolution Problems"}, {"paperId": "ea0425e69e3b1325bd95b62b20c129fad540e9ad", "title": "Mean Square Error Properties of Density Estimates"}, {"paperId": "de28c165623adabcdba0fdb18b65eba685aaf31d", "title": "On Estimation of a Probability Density Function and Mode"}, {"paperId": "df0eb0bee6fb77b53fb20c622a0994f7ba75ab44", "title": "Lectures on Fourier Integrals"}, {"paperId": "2c455f0da2bd86a9b9ea432d1485049073d7c63d", "title": "Remarks on Some Nonparametric Estimates of a Density Function"}, {"paperId": "6e9aa1f5e5f8d8ce269d5b1dca66c9e2a92b15b9", "title": "The Fourier Integral and Certain of Its Applications"}, {"paperId": "05b22d6ec2cff81bcfbac2a6cf67bc1e9ef0f60a", "title": "Improving Transformer with an Admixture of Attention Heads"}, {"paperId": "ba612bafda906c9e26b8e81d2548a7dde6434183", "title": "Probabilistic Transformer For Time Series Analysis"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "81aace0e90c6a962059b117c24db0d856f340f41", "title": "Report on the 11th IWSLT evaluation campaign"}, {"paperId": "bb7626dad199ff13efdb2c40026690ff45df7753", "title": "Kernel estimators for multivariate regression"}, {"paperId": "a9b212c3e1e6ca4e7afc7b6fd8daf22d7acf148d", "title": "Error analysis for general multtvariate kernel estimators"}, {"paperId": "05175204318c3c01e3301fd864553071039605d2", "title": "On Estimating Regression"}, {"paperId": null, "title": "c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}, {"paperId": null, "title": "Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}]}