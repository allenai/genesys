{"paperId": "117a9f9227ebe4acc67ff8a52eb9db2ab16390c8", "title": "Data Management For Large Language Models: A Survey", "abstract": "Data plays a fundamental role in the training of Large Language Models (LLMs). Effective data management, particularly in the formulation of a well-suited training dataset, holds significance for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning phases. Despite the considerable importance of data management, the current research community still falls short in providing a systematic analysis of the rationale behind management strategy selection, its consequential effects, methodologies for evaluating curated datasets, and the ongoing pursuit of improved strategies. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey provides a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various noteworthy aspects of data management strategy design: data quantity, data quality, domain/task composition, etc. Looking toward the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through effective data management practices. The collection of the latest papers is available at https://github.com/ZigeW/data_management_LLM.", "venue": "arXiv.org", "year": 2023, "citationCount": 13, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This survey provides a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various noteworthy aspects of data management strategy design: data quantity, data quality, domain/task composition, etc."}, "embedding": {"model": "specter_v2", "vector": [-0.04452915117144585, 0.2437715083360672, -0.5698267817497253, -0.21403244137763977, -0.5502095818519592, -0.1389562487602234, 0.5025908946990967, -0.16071346402168274, -1.0330091714859009, -0.03703484311699867, 0.3914364278316498, 0.11800019443035126, 0.01709292270243168, 0.44331812858581543, 0.13413174450397491, 0.2020019143819809, -0.8820311427116394, 0.9219790697097778, -0.20080697536468506, -0.2705841064453125, -0.46688348054885864, -0.7254816889762878, -0.9124194383621216, -0.10219406336545944, 0.42705678939819336, 0.3724987804889679, 0.6348110437393188, 0.7083929777145386, -0.6515946984291077, 0.1769683063030243, 0.5134511590003967, -0.2284233719110489, 0.1739887297153473, 0.06517046689987183, -0.29185256361961365, 0.18182624876499176, 0.46965643763542175, -0.47895729541778564, -0.546309232711792, 0.7239317297935486, -0.08345098793506622, 0.6002887487411499, 0.29787856340408325, -0.8250420689582825, -0.1683756411075592, 0.8011363744735718, 0.7109478712081909, 0.7375664710998535, -0.5917949080467224, -0.856257438659668, 1.1330221891403198, -1.5634783506393433, 0.6489296555519104, 1.2934588193893433, 0.27653419971466064, 0.5001581311225891, -0.45585229992866516, -0.8774477243423462, 0.7150420546531677, -0.19557303190231323, -1.0935413837432861, -0.6317484974861145, -0.12342634052038193, -0.3278653025627136, 1.8783714771270752, -0.06358661502599716, -0.30110689997673035, 0.3493300676345825, -0.29530471563339233, 1.5206935405731201, -0.22734041512012482, -0.9692162871360779, -0.3054047226905823, 0.39920344948768616, 0.3203050196170807, 0.8376783132553101, -0.5372927188873291, 0.44231978058815, -1.210981011390686, -0.337801069021225, 0.1552564650774002, -0.6367327570915222, 0.0401742197573185, -0.41824251413345337, -0.1398114413022995, 0.9935166239738464, -0.14818227291107178, 0.8039057850837708, -0.029346056282520294, 0.37991777062416077, 0.4620682895183563, 0.7070979475975037, 0.3964765965938568, 0.788162112236023, -0.5703756809234619, 0.2283821702003479, -0.937163770198822, 0.32767724990844727, 0.0677385926246643, 0.9625328779220581, -0.3774206042289734, 0.3285505771636963, -0.8297643661499023, 0.7901891469955444, 1.3031643629074097, -0.04239828512072563, 0.9384080171585083, -0.534049391746521, 0.6936081051826477, -0.5228145718574524, 0.3107253909111023, -0.30517715215682983, -0.46744877099990845, -0.3694325387477875, -0.5224863886833191, -1.4237381219863892, -0.3034627437591553, 0.04461401328444481, -0.6663970947265625, 1.0595182180404663, -0.26010122895240784, -0.25540396571159363, 0.24463029205799103, 0.34694239497184753, 0.5466975569725037, 0.5351821184158325, 0.37288913130760193, -0.24106737971305847, 1.0138094425201416, -0.6092572212219238, -0.7705292105674744, -1.2511807680130005, 1.021869421005249, -0.16531553864479065, 0.4289339482784271, -0.5223849415779114, -1.336865782737732, -0.9554721117019653, -0.44334176182746887, -0.18469098210334778, -0.6022607088088989, 0.8125747442245483, 0.9919449090957642, 0.24068103730678558, -1.1842949390411377, 0.2997052073478699, -0.09573033452033997, -0.24884027242660522, 0.1372831016778946, 0.03707021474838257, -0.11428873240947723, -0.35545799136161804, -1.6069790124893188, 0.5341050624847412, 0.318904846906662, -0.5775188207626343, -0.5180970430374146, -0.5295513272285461, -1.1244224309921265, -0.4182296395301819, 0.19792886078357697, -0.2720508277416229, 1.1609212160110474, -0.13436812162399292, -1.1752946376800537, 1.0419100522994995, -0.29209837317466736, 0.035850685089826584, 0.6835829019546509, -0.15233349800109863, -0.7533196806907654, -0.8684994578361511, -0.2773330509662628, 0.5181967616081238, 0.3184138238430023, 0.06353889405727386, -0.3947769105434418, 0.2973191440105438, -0.3624590039253235, 0.2612549662590027, -0.6108301877975464, 1.1187461614608765, -0.7690585851669312, -0.43944650888442993, 0.430213987827301, 0.7644268274307251, -0.4096373915672302, -0.16523616015911102, -0.2503690719604492, -1.0969326496124268, 0.17782089114189148, -0.2813465893268585, 1.4263488054275513, -0.8021757006645203, -0.4486396610736847, -0.2592058777809143, -0.1289614737033844, 0.013290302827954292, -0.9967730641365051, 0.9229342341423035, -0.16411933302879333, 0.44538962841033936, -0.021491384133696556, -1.639011263847351, 0.08083254843950272, -0.28771206736564636, -0.5021184086799622, -0.3345127999782562, -0.0825934186577797, 0.7738634347915649, -0.8471282124519348, 0.1635236293077469, 0.02046731300652027, 0.0610678493976593, -0.9446240663528442, 1.3661844730377197, -0.572222888469696, 0.1301174759864807, 0.020058436319231987, -0.19876421988010406, 0.21421653032302856, 0.004803791176527739, 0.6664794087409973, -0.25059106945991516, -0.4666757881641388, 0.4687942564487457, -0.46523311734199524, 1.4573147296905518, -0.43255552649497986, 0.31059008836746216, 0.10508795082569122, -0.2650817036628723, 0.13659507036209106, 0.6331396102905273, 0.07019435614347458, -0.36736324429512024, 0.300766259431839, 0.6295531392097473, -0.7366145253181458, 0.21756531298160553, 0.9164918661117554, 0.5153531432151794, -0.3052036464214325, 0.1385095715522766, 0.47903239727020264, -0.1449873447418213, 1.0652995109558105, 0.1661970466375351, 0.6497735381126404, 0.07917662709951401, 0.25424692034721375, -0.17100170254707336, 0.4136483073234558, -0.40857893228530884, 0.02992524392902851, 0.5762537717819214, 0.5921751856803894, 0.5488368272781372, -0.0024521667510271072, -0.3984062373638153, 0.03674968332052231, 0.004234721418470144, 0.5653135180473328, 1.9665988683700562, -0.3788875639438629, -0.3163734972476959, -0.6973003149032593, -0.18731294572353363, -0.054306913167238235, 0.12511055171489716, -0.4206611216068268, 0.2157638520002365, -0.6830282211303711, -1.0021809339523315, 0.6546676158905029, 0.10020297020673752, 0.4601719081401825, -0.7356052398681641, 0.014636329375207424, -0.13224755227565765, 0.28979068994522095, -0.7033783793449402, -0.4668278098106384, 0.46813687682151794, -0.8449727296829224, -0.316280335187912, -0.008179093711078167, -0.3139228820800781, 0.15807804465293884, -0.5528119802474976, 1.182928204536438, -0.445944607257843, 0.06525139510631561, -0.02774263359606266, 0.484344482421875, -0.6820672750473022, -1.1024765968322754, 0.41392773389816284, 0.403733491897583, -0.3811884820461273, 0.37546440958976746, 0.7220417857170105, 0.7674579620361328, 0.13511760532855988, -0.40252500772476196, 0.5673550367355347, 0.19718614220619202, 0.018826797604560852, 0.9516240954399109, -0.03519356623291969, 0.22207431495189667, -1.6535604000091553, 1.1836110353469849, 0.08598189055919647, -0.7927618026733398, 0.5573976039886475, -0.578217625617981, -0.43280765414237976, 0.37662819027900696, -0.6018163561820984, -0.32852768898010254, -0.752290666103363, 0.28530818223953247, -0.4635169208049774, -0.1745045930147171, 0.6920315623283386, 0.24896562099456787, 0.4211573898792267, 0.08392736315727234, 0.5273173451423645, 0.4642718434333801, -0.9573712944984436, 0.6912885308265686, -0.4921141564846039, 0.11133700609207153, 0.4877399504184723, 0.20982156693935394, -0.3819463849067688, -0.7389328479766846, -0.9819828271865845, -0.325885534286499, -0.45825085043907166, -0.5335231423377991, -0.03487205505371094, 0.031843625009059906, -0.6955574154853821, 0.07509816437959671, 0.13242089748382568, -0.7817269563674927, -0.20965634286403656, 0.32610052824020386, -0.16466762125492096, -0.041378263384103775, -1.4701136350631714, -1.1870956420898438, -0.5394645929336548, -0.506818950176239, -1.0433064699172974, 0.20126283168792725, -0.3557875454425812, -0.4068332612514496, -0.713907778263092, 0.1412798911333084, 0.054867517203092575, 1.2473477125167847, -0.9186270833015442, 1.281764030456543, -0.28661850094795227, 0.20582516491413116, -0.4396763741970062, 0.35111239552497864, 0.46115630865097046, -0.04980969429016113, 0.1131354346871376, -0.7622537612915039, -0.18854989111423492, -0.3509829640388489, -0.4817250967025757, 0.10320716351270676, 0.38906800746917725, 0.6875560879707336, 0.3996237814426422, -0.43137598037719727, 0.45115959644317627, 1.1882866621017456, -0.7204440236091614, -0.589130699634552, -0.09523019194602966, 0.870886504650116, 0.34881725907325745, -0.10875416547060013, 0.5167427659034729, 0.3951021134853363, 0.6368907690048218, -0.026463590562343597, -0.2350333034992218, -0.0647243782877922, -0.5480616092681885, 0.43853527307510376, 1.9687318801879883, 0.6371923089027405, -0.20261475443840027, -1.024544596672058, 0.7377486824989319, -0.9999440312385559, -0.1252293735742569, 0.6604495048522949, 0.8688746094703674, 0.8079537749290466, -0.4390822649002075, -0.21396182477474213, -0.5005344748497009, 0.15319357812404633, 0.5501437783241272, -0.3035331666469574, -0.3625364601612091, 0.11570855975151062, 0.15937142074108124, -0.06188444793224335, 0.5490315556526184, -0.8759851455688477, 0.7352219223976135, 14.38904094696045, 1.0275466442108154, 0.19114120304584503, 0.9554001092910767, 0.3720845878124237, 0.11542832851409912, -0.3407588303089142, -0.5602133274078369, -1.3935600519180298, -0.10037264972925186, 1.115973949432373, -0.0904020145535469, 0.7531634569168091, 0.2396605759859085, 0.4609498381614685, 0.05033746361732483, -0.5476540923118591, 0.5957210063934326, 0.3594871759414673, -1.2828422784805298, 0.7502711415290833, 0.12887868285179138, 0.3319409489631653, 0.8801723718643188, 0.42148494720458984, 1.002930760383606, 0.4274981915950775, -0.5422381162643433, 0.6980198621749878, 0.15051084756851196, 0.8679289817810059, 0.0033178108278661966, 0.5367474555969238, 0.9221479296684265, -0.9316672682762146, -0.13647113740444183, -0.6310720443725586, -0.998995840549469, 0.16087745130062103, -0.07089381664991379, -0.5104656219482422, -0.4875726103782654, -0.5716877579689026, 0.5769698023796082, -0.3737235963344574, 0.0571134090423584, 0.20128010213375092, 0.9069300889968872, -0.13284127414226532, 0.1476537138223648, 0.1974862962961197, 0.2118440866470337, 0.3141769468784332, -0.06977859139442444, -0.09855566173791885, -0.4750310182571411, 0.4060693085193634, 0.15976345539093018, -0.7591856718063354, 0.3237623870372772, -0.43548503518104553, -0.6407157778739929, 0.23317594826221466, 0.6213815212249756, 0.6362574100494385, 0.4517110586166382, -0.5308629870414734, -0.027772193774580956, 1.1297699213027954, -0.04225236177444458, 0.12393870949745178, 0.4894374907016754, 0.8583338260650635, -0.36300235986709595, -0.31776607036590576, 0.42029258608818054, 0.010446148924529552, -0.578658938407898, -0.6394581198692322, -0.5017852783203125, 0.6326727271080017, -0.5129133462905884, -1.0218764543533325, 0.898916482925415, 0.03555471450090408, -0.4900161921977997, 0.2033640593290329, -0.7723329067230225, 0.20119595527648926, 0.6960318684577942, -1.2988477945327759, -0.7334380149841309, 0.46974122524261475, -0.4361637830734253, 0.004865709692239761, -0.3182053565979004, 1.6127384901046753, 0.33714041113853455, -0.8665714859962463, 0.11154213547706604, 0.7307924628257751, 0.20327819883823395, -0.24640005826950073, -0.575827956199646, 0.7619885206222534, 0.20695403218269348, 0.21778987348079681, 0.35369810461997986, -0.19579656422138214, -0.06541294604539871, -0.8865724802017212, -0.4439104497432709, 1.138871669769287, -0.9520575404167175, -0.22508354485034943, -1.0080645084381104, -0.9349954128265381, 0.25743919610977173, 0.23207981884479523, -0.4243144392967224, 0.5948997735977173, 0.44805312156677246, -0.250582754611969, 0.030749598518013954, -0.6475497484207153, -0.1456926316022873, 0.7978554964065552, -0.6647262573242188, -0.14702267944812775, 0.3450389802455902, 0.15624041855335236, -1.187469720840454, -0.39952465891838074, -0.3272950351238251, -0.43248552083969116, 0.45078903436660767, 0.8501582145690918, -0.9584693312644958, 0.2024577558040619, 0.7820454835891724, -0.39912256598472595, -1.114698052406311, -0.11118946969509125, -0.7013874053955078, 0.07005975395441055, -0.04502207040786743, 1.0363637208938599, -0.17042571306228638, 0.2994600832462311, 0.9067703485488892, 0.3820021450519562, -0.48018503189086914, -0.8281382322311401, -0.313762366771698, 0.009483798407018185, -0.6616880297660828, 0.14883850514888763, -0.0493367537856102, -0.48461347818374634, 0.13911333680152893, 0.2231551706790924, 0.547398567199707, -0.3100181818008423, -0.6534100770950317, 0.5915654301643372, -0.17869366705417633, 0.14886434376239777, -0.47600945830345154, -0.10472908616065979, -1.1665312051773071, 0.23157057166099548, -1.4477757215499878, -0.04671613872051239, -0.9676034450531006, -0.3807726204395294, -0.3838871121406555, -0.31983283162117004, -0.054410170763731, 0.5372143983840942, -0.5187804698944092, -0.5781579613685608, -0.3652401566505432, -0.50171959400177, 0.8236608505249023, 0.6775757670402527, -0.2040010690689087, -0.1661907285451889, -0.038416072726249695, 0.3083777129650116, 0.4438035786151886, 0.4364981949329376, -0.4551335275173187, -1.1068462133407593, -1.7372294664382935, 0.19176098704338074, -0.18543243408203125, -0.4806213676929474, -0.3195229470729828, 0.5685117244720459, -0.12865319848060608, -0.4413507878780365, 0.1070822924375534, 0.26407599449157715, -0.8295480012893677, -0.21278010308742523, 0.020350495353341103, -0.8864316940307617, 0.21321722865104675, 0.2963661253452301, -0.7669458389282227, -0.694513738155365, 0.3179318904876709, -0.04808954522013664, -0.9155613780021667, -1.039556860923767, 0.6422662138938904, -0.6196152567863464, 0.46335092186927795, -0.2526129186153412, 0.0922890156507492, -0.5779459476470947, -0.3364637494087219, 0.003724636510014534, 0.6873285174369812, -0.5021345019340515, 1.2529231309890747, 0.32944074273109436, -1.2571625709533691, -0.166538804769516, 0.633415937423706, 0.24467837810516357, -0.17977598309516907, 0.4575069546699524, 0.35885709524154663, -0.29408523440361023, 0.5082026720046997, 0.3599230945110321, 0.5471866726875305, -1.2789230346679688, -0.5459340810775757, 0.9971691370010376, -0.6592478156089783, -0.02874043397605419, 1.6509747505187988, -0.3185734748840332, -1.5187020301818848, 0.3947562277317047, -1.070895791053772, -0.6562325954437256, -0.07683619111776352, 0.7260884642601013, 0.1337435245513916, 0.25868865847587585, -0.32118234038352966, -0.5476470589637756, -0.159719318151474, -0.17027267813682556, -0.48647311329841614, 0.7484728693962097, -0.38565635681152344, -0.3939952254295349, 0.9093996286392212, 0.9109070897102356, -0.6962777376174927, -0.690539538860321, -0.7724306583404541, -0.07756324857473373, -0.23035940527915955, 0.6031484603881836, -0.6150966286659241, -0.6506856083869934, 0.3809443712234497, 0.356733500957489, 0.31897440552711487, -0.0010163553524762392, -0.23185934126377106, 0.13263945281505585, 0.6128243207931519, 0.3922174572944641, -1.1857062578201294, -0.8393593430519104, 1.544421911239624, 1.5475202798843384, -1.2311935424804688, 0.004586480557918549, -0.035948045551776886, -0.8192678093910217, 0.4484826624393463, 0.47600477933883667, 0.47578054666519165, 1.2272634506225586, -0.2936285734176636, 0.6147783994674683, -0.17802007496356964, -1.2172229290008545, -0.28756409883499146, 1.1713272333145142, 0.7996150851249695, 1.093522548675537, 0.6075449585914612, 0.04761737212538719, 0.9428362250328064, -0.011408825404942036, 0.25104695558547974, 0.1894935667514801, 0.7347573637962341, -0.32802197337150574, -0.24180425703525543, 0.38038477301597595, 0.9675737023353577, -0.27981480956077576, -0.6464720964431763, -0.009723406285047531, 0.8178541660308838, 0.4214840233325958, 0.4983154833316803, 0.7502642273902893, 0.05002991110086441, 0.7701587677001953, 0.6575764417648315, 0.04386229068040848, -0.7953718304634094, -0.19818115234375, -0.2635592222213745, -0.07015244662761688, -0.13738694787025452, -0.15191902220249176, -0.552020251750946, -0.004958166275173426, 0.049891307950019836, 0.34072259068489075, 0.03797883167862892, 0.36753273010253906, 0.9519359469413757, 0.6664463877677917, 0.2270398885011673, -0.4650963544845581, 0.12214303761720657, -0.6391131281852722, -1.3478422164916992, -0.3262658417224884, -0.4832479953765869, -0.48766279220581055, -0.2745506763458252, -0.23266121745109558, -0.3533162474632263]}, "authors": [{"authorId": "2269759828", "name": "Zige Wang"}, {"authorId": "2249763710", "name": "Wanjun Zhong"}, {"authorId": "46395829", "name": "Yufei Wang"}, {"authorId": "2269768949", "name": "Qi Zhu"}, {"authorId": "2258717400", "name": "Fei Mi"}, {"authorId": "2239032344", "name": "Baojun Wang"}, {"authorId": "2238661808", "name": "Lifeng Shang"}, {"authorId": "2257942536", "name": "Xin Jiang"}, {"authorId": "2249841180", "name": "Qun Liu"}], "references": [{"paperId": "91e8ce403704a38bee8a5df90d99979a796d1741", "title": "Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning"}, {"paperId": "f8f9b9e5a1b9dd67fe61bd358f1d90f6cceeb4a1", "title": "Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace"}, {"paperId": "53f087d7f5ac5a24910c0fa0162079a2c35d8f64", "title": "DoGE: Domain Reweighting with Generalization Estimation"}, {"paperId": "c3068c7947ca290496c4d0280904686ba0b2903f", "title": "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning"}, {"paperId": "bb5e0e725b23006277ac8ffdb9a409a609d91e7b", "title": "LoBaSS: Gauging Learnability in Supervised Fine-tuning Data"}, {"paperId": "3b918b15178bcc84fd22af5094fe1efbcd388e72", "title": "Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration"}, {"paperId": "ac953b7bdd3271bff71dd0c13e3d707f1410b829", "title": "D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning"}, {"paperId": "5160224f7daf64fd490ed6d517bef316e383a311", "title": "An Empirical Study of Instruction-tuning Large Language Models in Chinese"}, {"paperId": "5088a04d1a9f42b967f3dcf791145e8aa367fc54", "title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition"}, {"paperId": "3a0e65b8af17adfb0f5f6db2fc80ec908a776fbb", "title": "Automatic Pair Construction for Contrastive Post-training"}, {"paperId": "29f032fc875576b5c3c6b1c2d76af8639bacfb88", "title": "OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"}, {"paperId": "39bb5d44735c07b1e1f4341a2d4bc8d5e783f491", "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training"}, {"paperId": "b549adab842e48b037d8aca84336296f84a9fa81", "title": "Can Large Language Models Understand Real-World Complex Instructions?"}, {"paperId": "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b", "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages"}, {"paperId": "a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9", "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"}, {"paperId": "e26888285436bc7998e5c95102a9beb60144be5e", "title": "Textbooks Are All You Need II: phi-1.5 technical report"}, {"paperId": "1ce1738d7f224ebd7ad98e23692404f06697b5f4", "title": "Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models"}, {"paperId": "11cf88dce827bd67cbfa60400306318022e736d5", "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification"}, {"paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f", "title": "Instruction Tuning for Large Language Models: A Survey"}, {"paperId": "96f6ad72733599db609332987ec6b65e30f11d07", "title": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs"}, {"paperId": "c74a13b251b6af6dfce49eeb128b1c0e2ddf955d", "title": "Tree-Instruct: A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment"}, {"paperId": "46ac88bb0acbf736840ff8a392cec2bf43d917e1", "title": "Exploring Format Consistency for Instruction Tuning"}, {"paperId": "ab82cd7194fc609d2d29e6b1a4399d6120f97fdd", "title": "Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning"}, {"paperId": "7e7d7799ffb0ad44169dbeab2326a150830db1c3", "title": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data"}, {"paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"}, {"paperId": "fbd2c8089870814449f9254a711041bbae145a82", "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources"}, {"paperId": "b26b07597645522782e919c32f3f84c54a4b7cbf", "title": "NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks"}, {"paperId": "8c5a7de7452b61cb81d6f7124ad021997e0a79c1", "title": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning"}, {"paperId": "b458fc5261595f44b36325e5eaea1f874d65138f", "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"}, {"paperId": "5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35", "title": "The False Promise of Imitating Proprietary LLMs"}, {"paperId": "c6ac708b65b24c20f80831d518c1795ce8133ad5", "title": "ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"}, {"paperId": "7742233d33da13910d0303e4ec8814a4e26e96e9", "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation"}, {"paperId": "a122863d239643453195424c04067e89406246e1", "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"}, {"paperId": "2ad8183c72a90511383a32ccaeea313eb85f4085", "title": "DetGPT: Detect What You Need via Reasoning"}, {"paperId": "2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4", "title": "Sources of Hallucination by Large Language Models on Inference Tasks"}, {"paperId": "1567bcac0ab09269c9d0ff33c9a406132417fab9", "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity"}, {"paperId": "7c217cc7524251f42887438834912e06129c3299", "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis"}, {"paperId": "87e26319485793e9cef3c5461b914b7f646a6162", "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning"}, {"paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445", "title": "LIMA: Less Is More for Alignment"}, {"paperId": "9b4f7c97c0b83a80c32bc0b93595cbcfb4ecb16d", "title": "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining"}, {"paperId": "5c7aaee5651221893ea0e67c363cab4c4be53b83", "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning"}, {"paperId": "5471114e37448bea2457b74894b1ecb92bbcfdf6", "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models"}, {"paperId": "570079bbdd8758dfe865097e05719313c9c1301a", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"}, {"paperId": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a", "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"}, {"paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions"}, {"paperId": "78f599fbd62dcc4a8dbab9d2f6056815dfc5b84c", "title": "The MiniPile Challenge for Data-Efficient Language Models"}, {"paperId": "ae736662f64d56f3ab1894fbd9c45f8f37251843", "title": "OpenAssistant Conversations - Democratizing Large Language Model Alignment"}, {"paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f", "title": "Self-Refine: Iterative Refinement with Self-Feedback"}, {"paperId": "a757999ed260d7bc45484dc6b4456bf33fe6f679", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"}, {"paperId": "8fc90497d9043fdf35e71302b7c2e79bb907144f", "title": "Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases"}, {"paperId": "12c6be503e4e5b7c9cb1810152d4364f26628a8d", "title": "Data-centric Artificial Intelligence: A Survey"}, {"paperId": "638b08154fbb71fd34db2aae6cb40045577fe0de", "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "86d03160e6f05deb17d0169e515f5a55d6361f7c", "title": "Exploring the Benefits of Training Expert Language Models over Instruction Tuning"}, {"paperId": "74013b7cfa0fc524803350fca51341004565eb22", "title": "Data Selection for Language Models via Importance Resampling"}, {"paperId": "8cf502f7569f2691eaf1681ef58a3741914b8125", "title": "Dis/similarities in the design and development of legal and algorithmic normative systems: the case of Perspective API"}, {"paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d", "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "d03a9b2a0e090cc9fd2ba0a457ecea35372f1018", "title": "Demystifying Prompts in Language Models via Perplexity Estimation"}, {"paperId": "5eba60c0c5e5f1f0c6c652dfd48b07caeebf13f4", "title": "Data-Efficient Finetuning Using Cross-Task Nearest Neighbors"}, {"paperId": "dd03f3d66853fca52c21610f4aa26eaeafc218c5", "title": "BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "6cf65eb8aa66116e14a97bb8f71552359ff814ba", "title": "Will we run out of data? Limits of LLM scaling based on human-generated data"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "041f5dbfcd07a3369ac44a6b902ee4b145eccf2b", "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation"}, {"paperId": "7ca41cc5fc364b713aba5b573ae4ada801fd788a", "title": "Noise-Robust De-Duplication at Scale"}, {"paperId": "41644f156a381d7baba2651fe092bd1d506c65d4", "title": "The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence"}, {"paperId": "aa4d9972af3264d032dbee58501ed4ac49477103", "title": "Scaling Laws and Interpretability of Learning from Repeated Data"}, {"paperId": "e9f28c98a00766e484810598886cf48b0de66cfa", "title": "On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "f2b0869b17bace854d73c19b449e3f88b9fed82e", "title": "How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "7016eb4f34611f97fe8c99176246e314678e03f4", "title": "A New Generation of Perspective API: Efficient Multilingual Character-level Transformers"}, {"paperId": "55c36748f2a7c060c3313349c730b053ed03fbf7", "title": "Deduplicating Training Data Mitigates Privacy Risks in Language Models"}, {"paperId": "0a4b8b161931799d5c6bc3ecf07c53bae0e9e502", "title": "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "4fc3542e18d685102e0b334cb9bcce4c8b63ed93", "title": "Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b", "title": "Reframing Instructional Prompts to GPTk\u2019s Language"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "6df1bae40c0cf96f6d7c7800209c44f1c5f3fb65", "title": "An Empirical Exploration in Quality Filtering of Text Data"}, {"paperId": "305edc10dc375f2c10f11e9e865eb3d039f80f9c", "title": "Data Quality for Machine Learning Tasks"}, {"paperId": "a6e25ca9ee9d3e45c6d1957c0dc3324a9816c34e", "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3", "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus"}, {"paperId": "4ae632b89089b38ce41d307a6cda4727e42aaab3", "title": "Detoxifying Language Models Risks Marginalizing Minority Voices"}, {"paperId": "6803adc7d8b891be652d18815f830f7a42a0f5b5", "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "68a6515b48e29e60d7993083a37090b8cfa36d4f", "title": "Overview and Importance of Data Quality for Machine Learning Tasks"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "5d0e2635a1ebe2c9347529975bc876d4286c9ab7", "title": "Distributionally Robust Neural Networks"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97", "title": "Deep double descent: where bigger models and more data hurt"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "77568c594470f9aa029f92774e2c12ab0451d9bb", "title": "Distributionally Robust Language Modeling"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd", "title": "Measuring Bias in Contextualized Word Representations"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f17c6e164ccc7ec1ad91b3fbbafe8f84664e9803", "title": "Efficient k-nearest neighbor graph construction for generic similarity measures"}, {"paperId": "70085f28eb99e5bbba6f5abf7ea964f17eba26ea", "title": "MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment"}, {"paperId": "838962cef6bf99c2b99afb26dc5d6cf82366ef20", "title": "Similarity Measures"}, {"paperId": "8addb1718c2bc6bbb0d82cd1a57b41198bf65965", "title": "On the resemblance and containment of documents"}, {"paperId": "05aa63683f7d027c18f560d4472ac87c1ea754fe", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only"}, {"paperId": "ac771182d1780c863954243809d1e144433919f9", "title": "Aligning Large Language Models with Human: A Survey"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": null, "title": "Analyzing and addressing the difference in toxicity prediction between different comments with same semantic meaning in google\u2019s perspective"}, {"paperId": null, "title": "What\u2019s in the box? a preliminary analysis of undesirable content in the common crawl corpus"}, {"paperId": "3514558fa26db73922f34a1b7f804052c58a5a1c", "title": "E CONOMICAL H YPERPARAMETER O PTIMIZATION WITH B LENDED S EARCH S TRATEGY"}, {"paperId": "f462abb6de423bda264ff8516762dfa8026b6040", "title": "Using the SIR algorithm to simulate posterior distributions"}, {"paperId": null, "title": "2023c. Al-pagasus: Training a better alpaca with fewer data"}, {"paperId": null, "title": "2023a. Self-alignment with instruction back-translation"}, {"paperId": null, "title": "2023. In-struction mining: High-quality instruction data selection for large language models"}, {"paperId": null, "title": "2023. Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "2023. Selfee: Iterative self-revising llm empowered by self-feedback generation"}, {"paperId": null, "title": "2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models"}, {"paperId": null, "title": "2023c. Oasis: Data curation and assessment system for pretraining of large language models"}, {"paperId": null, "title": "2023. Follow-bench: A multi-level fine-grained constraints following benchmark for large language models"}, {"paperId": null, "title": "2023. Scaling relationship on learning mathematical reasoning with large language models"}, {"paperId": null, "title": "2023. Codegen2: Lessons for training llms on programming and natu-ral languages"}, {"paperId": null, "title": "Language Technologies"}, {"paperId": null, "title": "OpenAI. 2023."}, {"paperId": null, "title": "2023. Phi-2: The surprising power of small language models"}, {"paperId": null, "title": "2023. Visual instruction tuning"}, {"paperId": null, "title": "2023 When less is more: Investigating data pruning for pretraining llms at scale"}, {"paperId": null, "title": "2022. An empirical survey of the effectiveness of debiasing techniques for pre-trained language models"}, {"paperId": null, "title": "2023. Instruction tuning with gpt-4"}, {"paperId": null, "title": "2023. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo"}, {"paperId": null, "title": "2022. Improving language models by retrieving from tril-lions of tokens"}]}