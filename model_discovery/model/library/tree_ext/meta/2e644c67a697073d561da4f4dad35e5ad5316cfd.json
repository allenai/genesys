{"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity", "abstract": "Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.", "venue": "Neural Information Processing Systems", "year": 2021, "citationCount": 118, "influentialCitationCount": 8, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "For the first time, a softmax-free transformer or SOFT is proposed, which significantly improves the computational efficiency of existing ViT variants and allows much longer token sequences to be permitted in SOFT, resulting in superior trade-off between accuracy and complexity."}, "embedding": {"model": "specter_v2", "vector": [0.26600226759910583, 0.9537529349327087, -0.05030600354075432, 0.09053105115890503, -0.06013799086213112, 0.16485626995563507, 0.8672484159469604, -0.062088411301374435, -0.6153302788734436, -1.0044697523117065, 0.47742825746536255, 0.7232639193534851, 0.68523108959198, -0.26904141902923584, -0.3915747106075287, -0.043803587555885315, -0.6359370946884155, -0.3593153655529022, 0.28843265771865845, -0.4502039849758148, -0.03984491899609566, -0.7892991900444031, -1.506801962852478, 0.33544921875, 0.24658994376659393, 1.0474494695663452, 0.5989538431167603, 1.0010877847671509, -0.6008216142654419, 0.5430644750595093, 0.34016671776771545, -0.2577719986438751, 0.4437938332557678, 0.10892648994922638, -0.5648502111434937, 0.057398248463869095, 0.7117293477058411, -0.14978203177452087, -0.788034975528717, 0.9498597383499146, 0.033337581902742386, 0.3594273030757904, 0.6156401038169861, -0.32199564576148987, -0.6570117473602295, 0.6792958378791809, 0.7631668448448181, 0.5688705444335938, -0.4986288845539093, -0.48861345648765564, 1.1822364330291748, -1.5416415929794312, 0.09084530919790268, 1.5157805681228638, 0.17249047756195068, 0.27598831057548523, 0.13041305541992188, -0.25216156244277954, 0.7964175343513489, 0.5271196961402893, -0.9703416228294373, -0.5244011878967285, -0.05480657517910004, -0.21095897257328033, 2.1747632026672363, -0.6033849120140076, -0.07632007449865341, 0.39173293113708496, 0.3585629165172577, 1.4114582538604736, -0.043586160987615585, -0.7706841826438904, -0.209165558218956, 0.14944866299629211, 0.03664630278944969, 0.9608136415481567, -0.7560490369796753, 0.18133722245693207, -1.01962411403656, 0.3526056706905365, 0.7383865714073181, 0.029173726215958595, 0.7641605734825134, -0.21590928733348846, -0.22024168074131012, 0.5287848711013794, 0.7278435230255127, 0.8385435342788696, -0.3369123041629791, 0.8000103235244751, 0.6586925387382507, 0.23673997819423676, -0.10555624961853027, 0.12788532674312592, 0.3991827666759491, 0.512694239616394, -0.8710451126098633, -0.0929793193936348, -0.48748400807380676, 0.9365341663360596, 0.1462143212556839, 0.8961687684059143, -0.828757643699646, 0.4316554069519043, 1.307033896446228, 0.40125733613967896, 0.6358178853988647, -0.25962063670158386, -0.10715124756097794, -0.8802895545959473, -0.36678969860076904, -1.0874220132827759, 0.3545721769332886, -0.2756774425506592, -0.8920800089836121, -0.7116712927818298, -0.3396693170070648, 0.5382328629493713, -1.3591753244400024, 0.4104207754135132, -0.36840787529945374, 0.014785460196435452, 0.047187600284814835, 0.42722365260124207, 0.8399137854576111, 0.45180338621139526, 0.5461785793304443, 0.6197754144668579, 1.5465067625045776, -0.9074641466140747, -0.43459513783454895, -0.9002215266227722, 0.043290697038173676, -0.38284286856651306, 0.1730698049068451, -0.36601904034614563, -1.238954782485962, -1.5792582035064697, -0.655021607875824, -0.07967501878738403, -0.9750056862831116, 0.3175096809864044, 0.9246886968612671, 0.33087074756622314, -1.0082980394363403, 0.745053768157959, -0.14658963680267334, -0.4566670358181, 0.646763801574707, 0.05906270444393158, 0.12316059321165085, 0.0006166524835862219, -1.2709141969680786, 0.4751635193824768, 0.029639137908816338, -0.38162288069725037, -0.40008822083473206, -0.26677969098091125, -1.5768029689788818, 0.1635526567697525, 0.3130594491958618, -0.4498347342014313, 0.8940526843070984, -0.502617597579956, -1.2412868738174438, 0.8696017861366272, -0.5551717877388, -0.11812112480401993, 0.09804889559745789, -0.2612895369529724, -0.310311883687973, -0.39449888467788696, -0.15007665753364563, 0.7851670384407043, 1.2932301759719849, -0.1618497520685196, -0.06869708746671677, 0.20374728739261627, -0.5240095853805542, 0.14577479660511017, -0.5184054374694824, 0.9713613390922546, -0.5270490646362305, -0.5650362968444824, 0.4164416790008545, 0.7003964185714722, 0.08698779344558716, -0.5721215009689331, -0.2351350635290146, -1.131230354309082, 0.41237011551856995, 0.5178133249282837, 0.24804425239562988, -1.1066991090774536, -1.1816928386688232, -0.04962925612926483, -0.044087912887334824, -0.08087390661239624, -0.9624233245849609, 0.7093724012374878, -0.4336946904659271, -0.06710752099752426, 0.4899168908596039, -0.97561115026474, 0.3155961334705353, -0.3042927384376526, -0.5806816220283508, 0.050252243876457214, 0.40690088272094727, 1.2877460718154907, -0.8185299634933472, -0.4664781987667084, 0.17998957633972168, 0.43896961212158203, -0.7209810614585876, 1.2001862525939941, -0.020288415253162384, -0.5723868012428284, 0.18323688209056854, 0.00947192870080471, 0.3520861864089966, -0.11368484050035477, 0.06075337901711464, -0.7527722120285034, -0.11224079877138138, 0.34935668110847473, -0.3051632344722748, 1.4012433290481567, -0.16583026945590973, 0.8292772173881531, -0.5720053315162659, -0.8496230840682983, 0.16198159754276276, 0.17130008339881897, 0.03391857072710991, -0.7252732515335083, 0.44136855006217957, -0.5001385807991028, -0.8778981566429138, 0.24343694746494293, 1.0201269388198853, 1.1281397342681885, -0.20315304398536682, -0.17917685210704803, 0.9371252059936523, -0.18982340395450592, 0.10635450482368469, 0.4947962462902069, 0.6189270615577698, 0.31551140546798706, 0.19706948101520538, -0.07677038758993149, -0.04126672074198723, -0.9768461585044861, -0.1068587377667427, 1.0928645133972168, 0.6856399774551392, 1.257401943206787, -0.00609568553045392, -0.5789919495582581, -0.13734211027622223, -0.4132271707057953, 0.6763899326324463, 1.6235718727111816, -0.13802365958690643, -0.21299894154071808, -0.8995856642723083, -0.19972962141036987, -0.4517378807067871, -0.6186357140541077, -0.549627959728241, -0.42721059918403625, 0.06528566777706146, -0.7839652299880981, 0.85855633020401, 0.3220049738883972, 1.079541563987732, -0.747176468372345, -0.4866475462913513, 0.04605942219495773, 0.4749676585197449, -0.9869011640548706, -0.8989871144294739, 0.3052751123905182, 0.023656141012907028, 0.1582244634628296, -0.34376898407936096, -0.3560517728328705, 0.5388813614845276, -0.2192472368478775, 0.591973602771759, -1.0070499181747437, -0.8645545244216919, 0.3748840391635895, 0.15820321440696716, -0.7595146298408508, -0.24132759869098663, 0.10225476324558258, 0.27053388953208923, 0.23822951316833496, 0.4060497283935547, 0.5351130962371826, -0.1324123740196228, 0.074696384370327, -0.358591228723526, -0.26056528091430664, 0.47069478034973145, 0.06327654421329498, 0.7537103891372681, -0.048965420573949814, -0.04730305075645447, -0.7363062500953674, 0.6641864776611328, 0.4103429615497589, -0.022293386980891228, -0.012844438664615154, -0.7429457306861877, -0.26899197697639465, 0.3079662024974823, -0.6714485287666321, -0.45153239369392395, -0.16682840883731842, 0.2878613770008087, -0.5495107173919678, -0.4024876356124878, -0.03762126713991165, 0.1584835648536682, -0.3710681200027466, 0.4161718487739563, 0.8295932412147522, 0.31819337606430054, 0.23069830238819122, 0.7621616125106812, -0.8995602130889893, 1.0732736587524414, 0.23728328943252563, 0.49650782346725464, 0.16958880424499512, -0.41826239228248596, -0.9921113848686218, -0.2581985592842102, -0.6613354682922363, -0.18101322650909424, -0.5989239811897278, 0.5554484128952026, -0.7744976878166199, -1.1678287982940674, 0.3411511778831482, -0.8133125901222229, -0.034111082553863525, -0.374917209148407, -0.10699217021465302, -0.33787283301353455, -0.5823966860771179, -0.854174017906189, -0.609275758266449, -0.5847722291946411, -0.6922467350959778, -0.11561260372400284, 0.37124744057655334, -0.030140673741698265, -0.2987389862537384, -0.3136424720287323, -0.5111579895019531, 1.3108971118927002, -0.8049781322479248, 0.08016696572303772, 0.13365988433361053, -0.3003324866294861, -0.2598907947540283, -0.3145943582057953, 0.8321854472160339, 0.04566580802202225, 0.0770958885550499, -1.1585406064987183, 0.3594110310077667, -0.42538291215896606, -0.15109790861606598, 0.6167152523994446, 0.5532731413841248, 0.5414426326751709, -0.030269410461187363, -0.33975937962532043, 0.3988807499408722, 1.623706340789795, -0.5951553583145142, 0.47845903038978577, -0.0015083443140611053, 0.9288305640220642, 0.0768246129155159, -0.34951329231262207, 0.6560422778129578, 0.8897338509559631, 0.2640365958213806, 0.36888372898101807, -0.9396311640739441, -0.4931296110153198, -0.5124802589416504, 0.549225389957428, 1.2248649597167969, 0.09566245973110199, 0.14573746919631958, -0.7928763628005981, 1.1155166625976562, -1.2927595376968384, -1.358812689781189, 0.6909571886062622, 0.5418447256088257, 0.10401347279548645, -0.22576285898685455, -0.30945608019828796, -0.6588484048843384, 0.41023629903793335, 0.4760383069515228, -0.24434322118759155, -0.8586660623550415, -0.4717472493648529, 0.6199424266815186, 0.660624623298645, 0.5608627796173096, -0.7385696172714233, 1.1745398044586182, 14.495967864990234, 0.7717923521995544, -0.23521073162555695, 0.43412306904792786, 0.3910786211490631, 0.5108264684677124, 0.0820871889591217, 0.08711712062358856, -0.8396278023719788, -0.5454762578010559, 0.28296545147895813, 0.3492724597454071, 0.4680910110473633, 0.47541525959968567, 0.04137153550982475, 0.14470906555652618, -0.3183341324329376, 1.0653247833251953, 0.869243323802948, -1.124550223350525, 0.21607592701911926, 0.11929356306791306, 0.1424407809972763, 0.5444193482398987, 0.8323509693145752, 0.6420930027961731, 0.39946410059928894, -0.39425593614578247, 0.5245617032051086, 0.7650281190872192, 0.7500569820404053, 0.08970189094543457, -0.03411521017551422, -0.016973065212368965, -1.1397554874420166, 0.007860765792429447, -0.891494870185852, -0.9323360919952393, -0.15792958438396454, 0.3038921058177948, -0.4400880038738251, -0.3268822431564331, 0.1892305165529251, 0.4722314774990082, -0.07497239112854004, 0.4048544466495514, 0.1534682661294937, 0.3601994514465332, 0.153822660446167, -0.08842326700687408, 0.3961503505706787, 0.8114115595817566, 0.12518352270126343, 0.6646906733512878, -0.19974477589130402, 0.07743599265813828, 0.1550384908914566, 0.46870067715644836, -0.20869797468185425, -0.022596005350351334, 0.07786976546049118, -0.09081362187862396, -0.3847207725048065, 1.0823370218276978, 0.2707710564136505, -0.10711503028869629, -0.16570702195167542, 0.727989673614502, 0.31543049216270447, 0.1435026377439499, -0.42795422673225403, -0.6784260272979736, 0.5122872591018677, -0.4073517322540283, 0.5438262820243835, 0.44504502415657043, -0.2844734787940979, -0.6706170439720154, -0.7302556037902832, 0.042667046189308167, 0.1704370677471161, -0.8324669003486633, -0.8574278354644775, 1.1995490789413452, -0.3642607629299164, -0.21946150064468384, 0.23595675826072693, -0.7140682339668274, -0.3912288248538971, 0.5973234176635742, -1.4282777309417725, -0.8102290630340576, -0.06939089298248291, -0.12779177725315094, -0.35858768224716187, -0.3650600016117096, 0.7998830676078796, 0.08118569850921631, 0.160725399851799, 0.5285480618476868, -0.12337486445903778, 0.26124638319015503, 0.12965796887874603, -0.6651918292045593, 0.8678062558174133, 0.3988751173019409, -0.0976676195859909, 0.2866378426551819, 0.14237168431282043, 0.29301026463508606, -0.8301190733909607, 0.14699208736419678, 0.7253367304801941, -0.3831798732280731, -0.6577110290527344, -0.9593355059623718, -0.8098706603050232, -0.27227431535720825, 0.954155445098877, 0.1403336375951767, -0.13214746117591858, -0.08494605869054794, -0.6782008409500122, -0.326882004737854, -0.6258747577667236, 0.1258949488401413, 0.4008086621761322, -1.0499054193496704, -0.38231152296066284, -0.161164790391922, -0.22270934283733368, -0.799959659576416, -0.5730322599411011, -0.6322397589683533, 0.2789503037929535, -0.12766341865062714, 1.41858971118927, -0.40843263268470764, 0.3803737461566925, 0.6421671509742737, 0.025394022464752197, -0.7590508460998535, -0.4383675158023834, -0.8277084827423096, 0.3770504593849182, 0.47911810874938965, 0.26150208711624146, -0.5006930232048035, 0.38368943333625793, 0.6640802025794983, 0.13219092786312103, -0.29203498363494873, -0.6626563668251038, -0.46521633863449097, -0.3168988525867462, -0.6122486591339111, 0.04543457552790642, 0.11439771205186844, 0.34446418285369873, -0.13307088613510132, 0.42182642221450806, 0.34601497650146484, 0.07320211827754974, -0.8659864664077759, 0.30812180042266846, 0.16559872031211853, 0.1595960110425949, -0.5674207806587219, -0.9452688694000244, -1.4293584823608398, -0.07896751165390015, -1.1486806869506836, 0.21005135774612427, -1.3622177839279175, -0.42552268505096436, 0.4189848303794861, -0.5570083856582642, 0.548988401889801, 0.3496924042701721, -0.2960641384124756, -0.11072716861963272, -0.67684006690979, -0.47966790199279785, 0.7076049447059631, 0.9038912653923035, -1.0636180639266968, 0.030222011730074883, -0.34066471457481384, -0.257522851228714, 0.2896345555782318, 0.39703214168548584, -0.3958221673965454, -0.6842909455299377, -1.0155916213989258, 0.031854670494794846, -0.2064247578382492, 0.10563812404870987, -1.1261959075927734, 1.2077349424362183, 0.27127331495285034, 0.24204279482364655, -0.026484176516532898, 0.4570459723472595, -0.6734407544136047, -0.841286301612854, 0.20439688861370087, -0.7279794216156006, -0.07482333481311798, -0.006322844885289669, -0.8085339665412903, -0.46855592727661133, 0.8359508514404297, 0.18655462563037872, -1.3138388395309448, -0.974269449710846, 0.5424668788909912, -0.8421545624732971, 0.15441164374351501, -0.4130353629589081, -0.685466468334198, -1.0359148979187012, -0.41039031744003296, -0.2844049334526062, 0.031198449432849884, -0.8224009275436401, 1.2358319759368896, 1.0507817268371582, -1.223049283027649, 0.015013461001217365, 0.4929140508174896, 0.18214914202690125, -0.218885138630867, 0.9421082139015198, 0.18430806696414948, 0.18701913952827454, 0.2857761085033417, -0.1693919152021408, 0.3102438151836395, -0.8332669138908386, 0.17088371515274048, 0.9186179041862488, 0.038991779088974, 0.05623700097203255, 0.9072654247283936, 0.21018266677856445, -0.60660719871521, 0.02491205558180809, -1.1233528852462769, -0.44172507524490356, 0.1860518455505371, 0.4850335121154785, -0.09617092460393906, 0.026797419413924217, -0.5452470779418945, -0.5843109488487244, 0.1922612190246582, -0.36708498001098633, -0.7256467342376709, 0.24714043736457825, -0.21794964373111725, -0.02131357602775097, 0.11099784076213837, 0.8973326086997986, -0.9078331589698792, -0.9579549431800842, -1.0153346061706543, -0.8235332369804382, -0.3162000775337219, 0.4983231723308563, -0.01674766093492508, -0.8481034636497498, 0.6646274924278259, 1.0088424682617188, 0.35450801253318787, 0.06048835068941116, -0.11354220658540726, -0.05877475440502167, 0.6808509230613708, -0.5136446952819824, -1.3075751066207886, -0.2868199944496155, 1.5107545852661133, 0.9443219304084778, -0.7892601490020752, -0.11580251902341843, -0.23154066503047943, -0.8090782165527344, 0.5179756283760071, 0.3821262717247009, -0.3082532584667206, 1.019357681274414, -0.23018303513526917, 0.36656779050827026, 0.17369994521141052, -0.8535211682319641, -1.0984801054000854, 1.197077989578247, 1.5092889070510864, 0.5222620368003845, 0.07589425891637802, 0.5140633583068848, 0.5220268964767456, 0.1944069117307663, -0.11787976324558258, 0.2516867220401764, 0.24602574110031128, -0.36320799589157104, 0.20103248953819275, -0.14669620990753174, 0.47624918818473816, -0.5183293223381042, -0.6832230687141418, 0.3537852466106415, 0.22388994693756104, 0.4355899691581726, 0.5273662805557251, 1.0663419961929321, -0.18293793499469757, 0.7223108410835266, 0.05603107064962387, 0.7222883701324463, -0.24576435983181, -0.3609573543071747, -0.0917399451136589, -1.1484310626983643, -0.032403215765953064, -0.16221575438976288, -0.7849119901657104, -0.4893292188644409, 0.11319739371538162, 0.30108118057250977, -0.4551546573638916, 0.6543248891830444, 0.9564857482910156, 0.4874862730503082, 0.6275118589401245, -0.1088685467839241, -0.6450093984603882, -0.10755042731761932, -0.7197774648666382, 0.14456310868263245, -0.29200124740600586, -0.07179620116949081, -0.2684428095817566, 0.21107056736946106, 0.1923079937696457]}, "authors": [{"authorId": "31727033", "name": "Jiachen Lu"}, {"authorId": "31824299", "name": "Jinghan Yao"}, {"authorId": "2136182354", "name": "Junge Zhang"}, {"authorId": "2259619084", "name": "Xiatian Zhu"}, {"authorId": "2143534132", "name": "Hang Xu"}, {"authorId": "2153577066", "name": "Weiguo Gao"}, {"authorId": "1691522", "name": "Chunjing Xu"}, {"authorId": "145406421", "name": "T. Xiang"}, {"authorId": "2152827279", "name": "Li Zhang"}], "references": [{"paperId": "f829a355de02c08567927154d3045a6eb5425c91", "title": "Is Attention Better Than Matrix Decomposition?"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "5b68522f58b61e7235b852677337ef3725075fd9", "title": "Co-Scale Conv-Attentional Image Transformers"}, {"paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325", "title": "Going deeper with Image Transformers"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "610b302950a19acef1c45456111dcd495f638c18", "title": "ConViT: improving vision transformers with soft convolutional inductive biases"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a", "title": "Conditional Positional Encodings for Vision Transformers"}, {"paperId": "cec7872b194aadf54140578b9be52939eb1112e9", "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78", "title": "Bottleneck Transformers for Visual Recognition"}, {"paperId": "d29430adccb805ab57b349afa8553954347b3197", "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "54c7445f319823c7dcc948c830e75e2fa7460b33", "title": "Exploring Self-Attention for Image Recognition"}, {"paperId": "2709167f1c3a03fa5b970a665ea48ed243aab582", "title": "Designing Network Design Spaces"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3827ecf84bc0429b73d9c57a6b2b55e723d4cfba", "title": "Dynamic Graph Message Passing Networks"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "8899094797e82c5c185a0893896320ef77f60e64", "title": "Non-local Neural Networks"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "709f3102347a44748a7bbcbe4b1273b07dc71661", "title": "On Iterative Computation of Generalized Inverses and Associated Projections"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Pytorch image models"}, {"paperId": null, "title": "Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation"}, {"paperId": "62f77aac48515022bee3d95d383486e307395766", "title": "Positive definite kernels: past, present and future"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "b6fff8b8ea77f157913986e7af53951d9fc1128e", "title": "Using the Nystr\u00f6m Method to Speed Up Kernel Machines"}]}