{"paperId": "83edcfbb206ddad38a971d605da09390604248ea", "title": "BloombergGPT: A Large Language Model for Finance", "abstract": "The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.", "venue": "arXiv.org", "year": 2023, "citationCount": 447, "influentialCitationCount": 29, "openAccessPdf": {"url": "http://arxiv.org/pdf/2303.17564", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work presents BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data, and constructs a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet."}, "embedding": {"model": "specter_v2", "vector": [-0.7276933193206787, 0.5555556416511536, -0.4643295705318451, -0.14093241095542908, -0.26258519291877747, -0.6958743929862976, 0.877820611000061, -0.08812998235225677, -0.4919811487197876, 0.25549012422561646, 0.8156437277793884, -0.28120532631874084, 0.6070167422294617, -0.04813610017299652, -0.11229895800352097, 0.1783672422170639, -0.7461891174316406, 0.8213761448860168, -0.6696064472198486, 0.14340655505657196, -0.07792440056800842, -0.5177592039108276, -0.3010830283164978, -0.16851936280727386, -0.18220354616641998, 0.5788514614105225, 0.0011444601695984602, 0.4099310338497162, -0.7789179682731628, 0.7362393736839294, 0.24455496668815613, -0.7327083945274353, 0.3175269365310669, 0.34832367300987244, -0.6360928416252136, -0.07124163955450058, -0.025943338871002197, -0.4887441396713257, -0.4204864203929901, 0.509363055229187, -0.28163784742355347, 0.32887566089630127, 0.46452146768569946, -0.5476044416427612, -0.4333115220069885, 0.9817467331886292, 0.7374408841133118, 0.613888680934906, -0.1824963539838791, -0.5273466110229492, 1.5526679754257202, -1.2188483476638794, 0.5013748407363892, 1.6859058141708374, 0.2852594256401062, 0.31708329916000366, 0.10486002266407013, -1.0659691095352173, 0.5894582271575928, -0.20923078060150146, -0.6036579608917236, -0.01678384281694889, -0.2667038142681122, -0.18170462548732758, 2.0555014610290527, -0.5966379046440125, -0.05775637552142143, 0.3874013125896454, 0.0022172443568706512, 1.2427572011947632, 0.19381605088710785, -0.6600001454353333, -0.3368203043937683, 0.20695969462394714, 0.3240341544151306, 0.6073513031005859, -0.26641136407852173, 0.34680551290512085, -0.8613907098770142, -0.3871423602104187, 0.5398456454277039, -0.23198650777339935, 0.1223423108458519, 0.48197492957115173, -0.3180254101753235, 0.9045233726501465, -0.014416279271245003, 0.7162189483642578, -0.16240139305591583, 0.64988774061203, 0.1691870540380478, 0.3197784721851349, 0.06904776394367218, 0.8036006689071655, -0.7095818519592285, 0.3063332438468933, -0.6960459351539612, 0.6057578921318054, 0.5169855952262878, 0.6310747265815735, -0.0780731588602066, 0.2611062526702881, -0.8536377549171448, -0.06557921320199966, 1.5657049417495728, -0.07423065602779388, 0.7088937759399414, -0.7929749488830566, 0.388351708650589, -0.4757012128829956, 0.6255454421043396, -0.6557060480117798, -0.297956258058548, -0.21505357325077057, -0.16523116827011108, -1.5582168102264404, -0.22798822820186615, -0.2644078731536865, -0.5737682580947876, 0.7097218632698059, -0.42157047986984253, -0.009668021462857723, 0.5044505596160889, 0.32064589858055115, 0.9674821496009827, 0.7513048052787781, 0.3434290587902069, 0.11069103330373764, 1.2499738931655884, -0.6752037405967712, -0.5726876258850098, -1.2939661741256714, 0.6585936546325684, -0.43140360713005066, -0.2770000398159027, -0.37483012676239014, -1.055356740951538, -0.5139881372451782, -0.3834303021430969, -0.045944493263959885, -0.5116068124771118, 0.35952094197273254, 1.1749347448349, 0.8054561018943787, -0.6922978758811951, 0.2829682230949402, 0.08678961545228958, -0.13025712966918945, 0.3444484770298004, 0.07728490978479385, 0.15077650547027588, -0.34997668862342834, -1.7444825172424316, 0.4201401174068451, 0.47055602073669434, -0.713793933391571, -0.1293271780014038, -0.3102606534957886, -1.0027154684066772, 0.08589784055948257, 0.10518929362297058, -0.22973129153251648, 1.4086633920669556, 0.08410762250423431, -0.9378321170806885, 1.1128699779510498, -0.46135413646698, -0.29889115691185, 0.31617534160614014, -0.10792870819568634, -0.7259908318519592, -0.8416640758514404, 0.022576913237571716, 0.4656412601470947, 0.20016901195049286, 0.19507868587970734, 0.029360005632042885, 0.48413288593292236, -0.2212481051683426, -0.16312144696712494, -0.31837600469589233, 1.1748590469360352, -0.325272798538208, -0.45084816217422485, 0.024824103340506554, 0.7908589243888855, -0.015704860910773277, -0.43927523493766785, -0.2711048126220703, -1.1037529706954956, 0.4825912117958069, -0.18188901245594025, 0.7331328988075256, -0.7114723324775696, -1.3008499145507812, -0.33572179079055786, -0.35084933042526245, -0.034852534532547, -0.5978059768676758, 0.6502542495727539, -0.5499577522277832, 0.6070222854614258, 0.02925698831677437, -1.3302823305130005, 0.12479618936777115, 0.1465739756822586, -0.8943096399307251, -0.2538258731365204, 0.4570026993751526, 1.1395527124404907, -0.4618793725967407, 0.09335508942604065, -0.14581841230392456, -0.01124426256865263, -1.0794272422790527, 1.2694846391677856, -0.47753405570983887, 0.04284557327628136, -0.528461217880249, 0.002537194872274995, 0.3491438329219818, -0.49696895480155945, 0.5081068873405457, -0.04216441884636879, -0.3511511981487274, 0.7003642916679382, -0.7918267250061035, 0.9722443222999573, -0.12586797773838043, 0.2101595103740692, 0.04718361422419548, -0.7422434687614441, 0.42660465836524963, 0.5250144600868225, 0.019906485453248024, -0.39989808201789856, 0.4301118552684784, 0.6048871874809265, -0.9848858118057251, -0.01532763335853815, 0.7306200861930847, 0.24446189403533936, -0.3509560823440552, 0.5786896347999573, 0.4627598524093628, -0.19518938660621643, 0.5644092559814453, 0.28992342948913574, 0.430424302816391, 0.15592637658119202, 0.5588428378105164, -0.08821234107017517, 0.2861919105052948, -0.5038765668869019, 0.24906955659389496, 0.3408065736293793, 0.5401329398155212, 0.5886176824569702, 0.6998816132545471, -0.6011029481887817, -0.03662531077861786, 0.17937816679477692, 0.33859947323799133, 1.064117670059204, -0.37843742966651917, -0.1924646645784378, -0.36231109499931335, -0.23627112805843353, 0.1605783998966217, -0.007171875797212124, -0.2284771054983139, 0.5650452971458435, -0.909727156162262, -1.3201500177383423, 0.714715301990509, 0.22470726072788239, 0.7110222578048706, -0.7910031676292419, -0.089623361825943, -0.21413160860538483, -0.15577949583530426, -0.9519823789596558, -0.45763257145881653, 0.1658724546432495, -0.3677801787853241, -0.21470090746879578, -0.044069401919841766, -0.24212071299552917, 0.0353902205824852, -1.0497816801071167, 1.1748632192611694, -0.8425999283790588, 0.02269209735095501, -0.053576912730932236, 0.745405912399292, -0.726719319820404, -0.806640625, -0.07660113275051117, 0.08935736119747162, -0.2743018567562103, 0.6200703382492065, 0.7676565647125244, 0.3481733202934265, 0.1477908343076706, -0.42912736535072327, 0.026272641494870186, 0.15464884042739868, 0.250384122133255, 0.41317230463027954, -0.07604189217090607, 0.3387085795402527, -1.2121224403381348, 0.7974637150764465, -0.10658080875873566, -0.8512035012245178, 0.14297479391098022, -0.5740072131156921, -0.15206918120384216, 0.21578292548656464, -0.3896459937095642, -0.3879166841506958, -0.8793225884437561, 0.1897050142288208, -0.17196965217590332, -0.25456809997558594, 0.8146371841430664, 0.2203577607870102, 0.4007730185985565, -0.1398511528968811, 0.12924815714359283, 0.053761716932058334, -0.37651094794273376, 0.6097292304039001, -0.6252241134643555, 0.20145875215530396, 0.12496141344308853, -0.015023842453956604, -0.1031886637210846, -0.19741889834403992, -0.400173544883728, -0.6040855646133423, -0.4980432391166687, 0.0031632098834961653, -0.1772884726524353, 0.0008290595142170787, -0.7851387858390808, -0.9348589777946472, 0.3299894332885742, -0.9702619314193726, 0.06938502192497253, 0.0022490264382213354, 0.05789768695831299, 0.13405193388462067, -0.9612976908683777, -1.373112440109253, -0.6940299272537231, -0.39844921231269836, -0.4443773031234741, 0.2810370922088623, 0.30720055103302, -0.13684098422527313, -0.8032753467559814, 0.042928051203489304, -0.12008631229400635, 0.773086428642273, -0.4720686376094818, 0.9134873151779175, -0.3820807933807373, -0.24458663165569305, -0.6072164177894592, 0.006227470003068447, 0.4852416217327118, -0.16604088246822357, 0.11248558014631271, -0.6214540004730225, 0.301820307970047, -0.07924927771091461, -0.5014839768409729, 0.15172173082828522, 0.5415351390838623, 0.42782798409461975, 0.12778319418430328, -0.339764267206192, 0.2049870491027832, 1.3549870252609253, -0.9290059208869934, -0.3289484977722168, 0.2754438519477844, 0.6282691359519958, 0.4278498589992523, -0.18178491294384003, 0.957261860370636, 0.4809943437576294, 0.057590655982494354, 0.0751790925860405, 0.26168617606163025, 0.46951934695243835, -0.38143685460090637, 0.3129289150238037, 1.0605907440185547, 0.5179890990257263, -0.4491713047027588, -1.258913278579712, 0.800329327583313, -0.8499129414558411, -0.36639633774757385, 0.4384702146053314, 0.5828924775123596, 0.4061799645423889, -0.30778682231903076, -0.5544763803482056, -0.24286429584026337, 0.35985416173934937, 0.5604738593101501, -0.25132954120635986, -0.5316793918609619, -0.45561131834983826, 0.5900141596794128, 0.48504793643951416, 0.29006141424179077, -0.6062676310539246, 0.7222589254379272, 14.937335014343262, 0.49441754817962646, -0.12789498269557953, 0.5234005451202393, 0.8064884543418884, 0.03659050911664963, -0.36903735995292664, -0.22212448716163635, -1.6191483736038208, -0.1502881497144699, 1.1885844469070435, -0.15632006525993347, 0.2037941813468933, 0.15080055594444275, 0.1720406413078308, 0.0008606219780631363, -0.1937897801399231, 0.6519392728805542, 0.4629645347595215, -1.3715248107910156, 0.3504396080970764, 0.36578091979026794, 0.44743236899375916, 0.6768498420715332, 0.6614888310432434, 0.8890178203582764, 0.758112370967865, -0.8528962731361389, 0.6519905924797058, 0.08878586441278458, 0.4923539161682129, 0.02952941507101059, 0.786367654800415, 0.7237359285354614, -0.8085954785346985, -0.35517075657844543, -0.6542840003967285, -1.2925729751586914, 0.5459496974945068, 0.44156384468078613, -0.653488278388977, -0.21234393119812012, -0.41999760270118713, 0.8919253945350647, 0.5579942464828491, 0.1465090662240982, -0.08820497244596481, 0.798658549785614, -0.09775124490261078, 0.09688752889633179, 0.3882652819156647, 0.4616863429546356, 0.23145601153373718, -0.09989616274833679, 0.2402881681919098, 0.1340467482805252, 0.11210387200117111, 0.42067012190818787, -1.042970061302185, 0.2837967276573181, 0.08253118395805359, -0.916385293006897, -0.10033643990755081, 0.8203651905059814, 0.3737088441848755, 0.30847635865211487, -0.5890868902206421, -0.08412785828113556, 0.483404278755188, 0.11406072974205017, -0.19054794311523438, 0.43768012523651123, 0.689751923084259, -0.15495678782463074, 0.1159282773733139, 0.32942694425582886, -0.2060839980840683, -0.7278376817703247, -0.711043655872345, 0.04200915992259979, 0.6198671460151672, -0.5556375980377197, -1.0589613914489746, 0.6584188342094421, -0.542486846446991, -0.33456119894981384, -0.05950132757425308, -1.2528748512268066, -0.04785948991775513, 1.0906490087509155, -1.8206287622451782, -0.6366128921508789, 0.4221310317516327, -0.21466639637947083, -0.6955934762954712, -0.016908960416913033, 1.4241745471954346, 0.16920210421085358, -0.4376247525215149, -0.42348363995552063, 0.40996378660202026, 0.5096302032470703, 0.13439176976680756, -0.6637788414955139, 1.0270158052444458, 0.024952925741672516, -0.20503170788288116, 0.1934930682182312, -0.022701626643538475, -0.018263118341565132, -0.5350412130355835, -0.3189280033111572, 1.498828649520874, -1.0407874584197998, -0.19402095675468445, -0.6329972743988037, -0.5267670154571533, 0.42557668685913086, 0.4218253493309021, -0.43256503343582153, 0.559918224811554, 0.13503043353557587, -0.5554895401000977, 0.2585732340812683, -1.0972553491592407, -0.3996543288230896, -0.038159988820552826, -0.5536633133888245, -0.5885782837867737, 0.4432128965854645, 0.08806401491165161, -0.8826668858528137, -0.43153125047683716, -0.51695716381073, 0.0198697317391634, 0.13170622289180756, 0.7271519303321838, -0.6977391242980957, 0.8646053075790405, 1.024664282798767, -0.36331525444984436, -0.5819917917251587, 0.007329670712351799, -0.6514926552772522, -0.02939310111105442, 0.27023032307624817, 0.5472449660301208, -0.9388287663459778, 0.5094444751739502, 1.0426048040390015, 0.5207867622375488, -0.4588940739631653, -0.7153371572494507, -0.5602529048919678, 0.495759516954422, -0.4432525038719177, 0.6406431794166565, 0.2811003029346466, -0.2497761845588684, -0.3259352743625641, 0.3183610439300537, 0.37161070108413696, -0.44931039214134216, -0.30346226692199707, 0.33060264587402344, -0.38623911142349243, -0.3597828447818756, -0.5592707991600037, -0.3547837734222412, -1.5893646478652954, 0.5859490633010864, -0.9160085916519165, -0.32787176966667175, -1.002921223640442, -0.3272606134414673, 0.04130157455801964, -0.17657160758972168, 0.25698432326316833, 0.6534743905067444, -0.4160842299461365, -0.5446333885192871, -0.3587459921836853, 0.17624340951442719, 0.6001203656196594, 0.8022512197494507, -0.49583008885383606, 0.37956175208091736, -0.0567876435816288, -0.09793321788311005, 0.07262244820594788, 0.2710950970649719, -0.4291281998157501, -0.25770658254623413, -1.1762278079986572, 0.3739214837551117, 0.06946656107902527, -0.3742462992668152, -0.19733907282352448, 0.3981040418148041, 0.18495401740074158, -0.20406994223594666, -0.08975820243358612, 0.4708993434906006, -0.4810851216316223, -0.5079038143157959, 0.28087395429611206, -0.4667307436466217, 0.3396836519241333, 0.21558892726898193, -0.777725338935852, -0.8062347173690796, 0.4794064164161682, -0.1828174889087677, -1.2879549264907837, -0.3515380620956421, 0.6740356087684631, -0.787038266658783, 0.2596190571784973, -0.17682461440563202, -0.027374641969799995, -1.009725570678711, -0.38874784111976624, 0.007264101877808571, 0.6942377686500549, -0.4505484402179718, 0.9206892848014832, -0.14260460436344147, -0.7758439779281616, -0.29295316338539124, 0.5020394325256348, -0.2657073736190796, -0.19029325246810913, 0.23348918557167053, -0.04288700222969055, -0.46606144309043884, 1.3651494979858398, 0.32500410079956055, 0.7905218601226807, -0.7284880876541138, -0.2731834948062897, 0.8119580745697021, -0.9110789895057678, 0.12856021523475647, 1.491767168045044, -0.10607537627220154, -1.1287305355072021, 0.0586644783616066, -1.3514952659606934, -1.0804228782653809, -0.017582880333065987, 0.5199499726295471, -0.1593337506055832, 0.01873178221285343, -0.40473371744155884, -0.4790002703666687, -0.061742525547742844, 0.24177759885787964, -0.4247596561908722, 0.5280376672744751, -0.17246833443641663, -0.5309451222419739, 0.4970027804374695, 0.7596594095230103, -0.5540953278541565, -0.20860621333122253, -0.6668092608451843, -0.23060519993305206, -0.12562456727027893, 0.1985262632369995, -0.4898807406425476, -0.7218199372291565, 0.4187612533569336, 0.09742550551891327, 0.4738851487636566, 0.03474879264831543, -0.19340521097183228, 0.21292729675769806, 0.5971648693084717, 0.30067357420921326, -0.543976902961731, -0.7202132344245911, 1.848341464996338, 1.0082461833953857, -1.0235013961791992, 0.03456003591418266, -0.13155557215213776, -0.7134385108947754, 0.8904896974563599, 0.18992531299591064, 0.10914143919944763, 1.3705657720565796, -0.23496854305267334, 0.2848673462867737, 0.035013455897569656, -1.016912579536438, 0.1159883588552475, 0.4483962655067444, 0.4515896737575531, 1.0702483654022217, 0.5368492007255554, -0.24162593483924866, 1.288851022720337, -0.21757659316062927, 0.2443188726902008, 0.6657019257545471, 0.41505399346351624, 0.06119309738278389, -0.5898946523666382, -0.010435049422085285, 0.7715569138526917, -0.9980905652046204, -0.612541675567627, -0.2898727059364319, 0.5044599771499634, 0.2471645325422287, 0.6016070246696472, 0.7792567014694214, 0.5671737194061279, 0.43221190571784973, 0.40009331703186035, 0.13886360824108124, -0.6048343777656555, -0.6855587363243103, -0.23828157782554626, -0.33827921748161316, -0.0475257933139801, -0.5091609358787537, -0.8317407369613647, -0.5182071924209595, -0.46599552035331726, 0.404083251953125, 0.3366222381591797, 0.34376123547554016, 1.1862258911132812, 0.4374934732913971, -0.05574021860957146, -0.4133404493331909, 0.033538173884153366, -0.4157494604587555, -1.01289701461792, -0.8801453709602356, -0.7185976505279541, -0.40322989225387573, -0.12480449676513672, -0.21660222113132477, -0.46071192622184753]}, "authors": [{"authorId": "50425845", "name": "Shijie Wu"}, {"authorId": "2329943", "name": "Ozan Irsoy"}, {"authorId": "2144851146", "name": "Steven Lu"}, {"authorId": "2213265733", "name": "Vadim Dabravolski"}, {"authorId": "1782853", "name": "Mark Dredze"}, {"authorId": "3159346", "name": "Sebastian Gehrmann"}, {"authorId": "2071549", "name": "P. Kambadur"}, {"authorId": "2064188376", "name": "D. Rosenberg"}, {"authorId": "2065970358", "name": "Gideon Mann"}], "references": [{"paperId": "ce913026f693101e54d3ab9152e107034d81fce1", "title": "Holistic Evaluation of Language Models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "89184ab496b2a1ae31e068e628479b4cd8f4b9d2", "title": "Do We Still Need Clinical Language Models?"}, {"paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "title": "Large language models encode clinical knowledge"}, {"paperId": "a640cdafc10181517b7694ab589db515595b3490", "title": "Evaluating Human-Language Model Interaction"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "0882a2b2787b35dbcc6e341c953d964b77abd4df", "title": "When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "9fc461d7f3981fb7fa8ed74f072746903875b08d", "title": "Autoregressive Structured Prediction with Language Models"}, {"paperId": "663a41c866d49ce052801fbc88947d39764cad29", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"}, {"paperId": "d96997265f8146e93b4c9350f19d55e46d1317f0", "title": "ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "74eae12620bd1c1393e268bddcb6f129a5025166", "title": "Improving alignment of dialogue agents via targeted human judgements"}, {"paperId": "83851f1a32d41975582ca62355858ab5e34738f7", "title": "News Summarization and Evaluation in the Era of GPT-3"}, {"paperId": "44279244407a64431810f982be6d0c7da4429dd7", "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"}, {"paperId": "914254fac74a2da051cccf6ca16afcaad416a079", "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"}, {"paperId": "6edccbd83a9aae204785d4821f97855677c33866", "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "26133033149afb4b45e5d0a4bd1dc712a236810e", "title": "ProGen2: Exploring the Boundaries of Protein Language Models"}, {"paperId": "ca4f45318c13078005b7e7cb134d79d8fb568f2b", "title": "GODEL: Large-Scale Pre-Training for Goal-Directed Dialog"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8", "title": "UL2: Unifying Language Learning Paradigms"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "3ae3716f125d71e9daccac3dafa4fab7482fb16a", "title": "Data Governance in the Age of Large-Scale Data-Driven Language Technology"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "cd733ce920e055415e9a9a7d90d3ec89f8750866", "title": "MiCS: Near-linear Scaling for Training Gigantic Model on Public Cloud"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d", "title": "Quantifying Memorization Across Neural Language Models"}, {"paperId": "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb", "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "d617f51833860dc50d202af7f80be71304b2e994", "title": "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "ce8f2077fae890cc1967d1a9aa5c544127f1ca47", "title": "Amazon SageMaker Model Parallelism: A General and Flexible Framework for Large Model Training"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "2d4f66046bb436864cd6bf589e3a931c405f9f44", "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"}, {"paperId": "992129aa96c97fa3b2ced0ddbc4c7d07bfaaf821", "title": "PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation"}, {"paperId": "d64e57b9780f30f5b49bf620fdfb8584651b7f85", "title": "Challenges in Detoxifying Language Models"}, {"paperId": "e3480d9395e692833b722b2e957d51139985f310", "title": "General-Purpose Question-Answering with Macaw"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"}, {"paperId": "99053e3a708fc27709c9dab33110dc98b187c158", "title": "FinQA: A Dataset of Numerical Reasoning over Financial Data"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "b42d20ec9580ebd76860890a1d7a7fdcc742677e", "title": "Modeling Protein Using Large-scale Pretrain Language Model"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "61e06615f6cee5ed1ba7d22b801925b69a45653b", "title": "The Values Encoded in Machine Learning Research"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3", "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus"}, {"paperId": "4ae632b89089b38ce41d307a6cda4727e42aaab3", "title": "Detoxifying Language Models Risks Marginalizing Minority Voices"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "4383a975c09b72ba2f1a77cd779bb6965dbfb2fb", "title": "Scaling Laws for Transfer"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba", "title": "Extracting Training Data from Large Language Models"}, {"paperId": "76ad0d37bd3845431b3ca9d07f8db74c82752298", "title": "Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art"}, {"paperId": "806adbb35ed4a95f51518f5962fd59685ad4706b", "title": "Query-Key Normalization for Transformers"}, {"paperId": "40327cc7fd2b328b7cbcd0ab03eddaf696d4576b", "title": "Best Practices for Managing Data Annotation Projects"}, {"paperId": "f987bf3b41ab98e1c755973c89f783ef445ab31a", "title": "Impact of News on the Commodity Market: Dataset and Results"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "c014f8bc3b521453a93a13bb2c90700fcf462738", "title": "Limits to Depth Efficiencies of Self-Attention"}, {"paperId": "58e825d96454aeba9c780e5bfb1c1f872ad9e603", "title": "Exploring Cross-sentence Contexts for Named Entity Recognition with BERT"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "72cdd6ebe0221fb568ef20534f44ba5b35190a56", "title": "BERTweet: A pre-trained language model for English Tweets"}, {"paperId": "9b539d413393047b28bb7be9b195f142aaf7a80e", "title": "Recipes for Building an Open-Domain Chatbot"}, {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"}, {"paperId": "b0b0dddb8310e01b9407a21674c2d33a23a6e967", "title": "Byte Pair Encoding is Suboptimal for Language Model Pretraining"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "388e2fcdcefbe0834e153ab2a0be127092f9674d", "title": "DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation"}, {"paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d", "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "7102bb3fe73bd057ff161d9db5214a267c1ef312", "title": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"}, {"paperId": "39e801ca0dbc69c3697f118e24dac964abb63d4a", "title": "The CommitmentBank: Investigating projection in naturally occurring discourse"}, {"paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b", "title": "WinoGrande"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "2fa3f7ce620a1c7155daef6620dd6bb0e01934f3", "title": "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT"}, {"paperId": "b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"}, {"paperId": "156d217b0a911af97fa1b5a71dc909ccef7a8028", "title": "SciBERT: A Pretrained Language Model for Scientific Text"}, {"paperId": "1e43c7084bdcb6b3102afaf301cce10faead2702", "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}, {"paperId": "a5b66ee341cb990f7f70a124b5fab3316d3b7e27", "title": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"}, {"paperId": "a925f818f787e142c5f6bcb7bbd7ede2deb34860", "title": "WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "99ad0533f84c110da2d0713d5798e6e14080b159", "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"}, {"paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e", "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"}, {"paperId": "7191680b572ee7145f1a9d95ff11ab1ff44259f3", "title": "WWW'18 Open Challenge: Financial Opinion Mining and Question Answering"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "f4dd755a7f6238bf7f1b8671e57e098f248402e4", "title": "Natural language based financial forecasting: a survey"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "8da4e6fcf185fbba07fa8f6d5f83568c7cee722e", "title": "Natural Language Processing in Accounting, Auditing and Finance: A Synthesis of the Literature with a Roadmap for Future Research"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "2b5d7d3baef51c66cce0f8dc4807d7b88bcb9239", "title": "How Twitter is Changing the Nature of Financial News Discovery"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "85b68477a6e031d88b963833e15a4b4fc6855264", "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "4211bff1388da30a3b7dfd35d6aef2032900ca5c", "title": "Good debt or bad debt: Detecting semantic orientations in economic texts"}, {"paperId": "ed6262b569c0a62c51d941228c54f34e563af022", "title": "Japanese and Korean voice search"}, {"paperId": "93c20e38c85b69fc2d2eb314b3c1217913f7db11", "title": "Generating Text with Recurrent Neural Networks"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "aee835ea79cdc045568a4092a3883b3129599445", "title": "Statistical machine translation"}, {"paperId": "ba786c46373892554b98df42df7af6f5da343c9d", "title": "Large Language Models in Machine Translation"}, {"paperId": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"}, {"paperId": "3de5d40b60742e3dfa86b19e7f660962298492af", "title": "Class-Based n-gram Models of Natural Language"}, {"paperId": "32a175b36ec7f2f08cb3dfac30ce141e144ec9e9", "title": "Continuous speech recognition by statistical methods"}, {"paperId": "bb0656031cb17adf6bac5fd0fe8d53dd9c291508", "title": "An empirical analysis of compute-optimal large language model training"}, {"paperId": "1b3eeb693c717c18955d920af11f7adb6382f430", "title": "Bernice: A Multilingual Pre-trained Encoder for Twitter"}, {"paperId": null, "title": "Language models of protein sequences at the scale of evolution enable accurate structure prediction"}, {"paperId": null, "title": "Scaling language modeling with pathways"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"}, {"paperId": null, "title": "Dawn Xiaodong Song,\u00dalfar Erlingsson, Alina Oprea, and Colin Raffel"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "44ee91d83d3b804780d8ec43ee5af0e41d3b0787", "title": "Domain Adaption of Named Entity Recognition to Support Credit Risk Assessment"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "db8885a0037fe47d973ade79d696586453710233", "title": "The Sixth PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "title": "The Third PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "136326377c122560768db674e35f5bcd6de3bc40", "title": "The Second PASCAL Recognising Textual Entailment Challenge"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": null, "title": "Aitor Soroa, Alham Fikri Aji"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}]}