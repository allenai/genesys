{"paperId": "98755a7ca75afbb29a19f8129b9f25796ad0e0b7", "title": "Language models scale reliably with over-training and on downstream tasks", "abstract": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e.,\"Chinchilla optimal\"regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.", "venue": "arXiv.org", "year": 2024, "citationCount": 12, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The perplexity of a language model to its downstream task performance is related to its downstream task performance by proposing a power law, and this law is used to predict top-1 error averaged over downstream tasks for the two aforementioned models."}, "embedding": {"model": "specter_v2", "vector": [-0.05659840255975723, 0.2363748997449875, -0.32973456382751465, -0.3300337791442871, -0.694042980670929, -0.2943470776081085, 0.6074252724647522, -0.47228091955184937, -0.8481577038764954, -0.21383719146251678, 0.307656466960907, -0.4271993339061737, 0.20158104598522186, 0.2917393743991852, -0.17822475731372833, 0.040989894419908524, -0.9995499849319458, 0.4521084427833557, -0.36474236845970154, -0.3032912313938141, -0.47501060366630554, -0.6015656590461731, -0.7122732400894165, -0.02448686584830284, 0.4749065041542053, 0.3657469153404236, 0.03598078340291977, 1.0659021139144897, -0.17605416476726532, -0.1948578804731369, 0.15701554715633392, -0.4935803711414337, 0.35241442918777466, 0.040514495223760605, -0.21474330127239227, -0.12981174886226654, 0.35788604617118835, -0.5326436161994934, -0.43895697593688965, 0.7338818311691284, -0.16518010199069977, 0.4217280149459839, 0.4336685836315155, -0.7236089110374451, -0.5535290241241455, 1.0976731777191162, 0.42882010340690613, 0.8826886415481567, -0.4269276261329651, -0.3791302442550659, 1.056033730506897, -1.0390734672546387, -0.027892818674445152, 1.5888335704803467, 0.5507872104644775, 0.4116796553134918, -0.48870205879211426, -0.685370922088623, 0.4180549383163452, -0.2904043197631836, -0.9833601117134094, -0.293798565864563, -0.5488114953041077, 0.1220955103635788, 1.786873698234558, -0.5526467561721802, -0.2406725287437439, 0.4118260145187378, -0.12807227671146393, 1.2168633937835693, 0.2663809657096863, -0.800446093082428, -0.3967779278755188, -0.09564842283725739, 0.29008305072784424, 0.543079137802124, -0.057792194187641144, 0.23334093391895294, -1.2462944984436035, -0.16361021995544434, 0.17073678970336914, -0.39150190353393555, 0.14082814753055573, 0.19251631200313568, -0.33226826786994934, 0.6753972768783569, 0.3303906321525574, 0.6436519026756287, -0.0007259592530317605, 0.9719582200050354, 0.8394414782524109, 0.5325101017951965, 0.5576436519622803, 0.09913163632154465, -0.5161482691764832, 0.2195640504360199, -0.9891532063484192, 0.19583219289779663, -0.08966517448425293, 1.0191545486450195, -0.3989352583885193, 0.2779085636138916, -0.5743854641914368, 0.35592037439346313, 1.3630646467208862, 0.43639594316482544, 0.7794487476348877, -0.46498334407806396, 0.5324352383613586, -0.8907183408737183, -0.05564681068062782, 0.02638988383114338, -0.566842257976532, -0.24759900569915771, -0.6650484800338745, -1.3858650922775269, -0.17326413094997406, 0.16738145053386688, -0.968146026134491, 0.807407021522522, -0.32333171367645264, -0.09770897030830383, 0.09425072371959686, 0.436062216758728, 0.1694169044494629, 0.6585224866867065, 0.606995165348053, 0.05410991609096527, 0.8167628049850464, -0.4518112540245056, -0.5064662098884583, -0.6748977303504944, 1.1572636365890503, -0.5269108414649963, 0.8155580759048462, -0.29016754031181335, -1.2890269756317139, -0.9631783366203308, -0.5992803573608398, 0.22683027386665344, -0.4631733298301697, 0.35621002316474915, 0.9614260792732239, 0.6186855435371399, -1.1516449451446533, 0.9024677276611328, -0.3542788028717041, -0.40500736236572266, 0.23699167370796204, 0.21466515958309174, 0.05178264528512955, -0.5550649762153625, -1.234961748123169, 0.4651776850223541, 0.5747498273849487, -0.7223268747329712, -0.20570968091487885, -0.6697410941123962, -0.8301396369934082, -0.23523026704788208, 0.024683687835931778, -0.2195245325565338, 1.3811029195785522, 0.11823844164609909, -1.1235407590866089, 0.6386874318122864, -0.20733146369457245, 0.18373657763004303, 0.5169179439544678, -0.2347356081008911, -0.7688797116279602, -0.6432005167007446, -0.17800244688987732, 0.4722798764705658, 0.15713392198085785, -0.1878604292869568, -0.37182390689849854, 0.05562873184680939, -0.45061197876930237, -0.031033048406243324, -0.2944554090499878, 0.7500529289245605, -0.34867939352989197, -0.3717884421348572, 0.27356600761413574, 0.3357706665992737, 0.10203779488801956, -0.32477134466171265, -0.39450839161872864, -1.2484289407730103, 0.35288628935813904, -0.4623885452747345, 0.8835520148277283, -0.9208900928497314, -0.8695840835571289, -0.02213652804493904, -0.3924334645271301, -0.18884436786174774, -0.7254495620727539, 0.72892826795578, 0.015656741335988045, 0.9744114279747009, -0.22427117824554443, -1.3618563413619995, 0.4399619400501251, -0.19363990426063538, -0.5793622136116028, -0.0818595215678215, 0.031427908688783646, 1.005496859550476, -0.5725011229515076, 0.2944071590900421, -0.14913393557071686, 0.30487021803855896, -0.9987012147903442, 0.9499016404151917, -0.10951465368270874, 0.043012671172618866, 0.3809787631034851, -0.2529315650463104, 0.22496788203716278, -0.41886723041534424, 0.568255603313446, 0.053182315081357956, 0.0672530084848404, 0.26835620403289795, -0.20128560066223145, 1.5381243228912354, -0.4549700617790222, 0.27953919768333435, -0.0742320790886879, -0.30963003635406494, -0.1523135006427765, 0.33695247769355774, -0.46980181336402893, -0.4578407406806946, 0.3229110836982727, 0.685817301273346, -0.1409108191728592, 0.4814571142196655, 0.8118530511856079, 0.7500014305114746, -0.5476612448692322, 0.3712996542453766, 0.5467066168785095, -0.28541702032089233, 0.7593587040901184, 0.270460844039917, 0.21914465725421906, 0.4976104497909546, 0.21201840043067932, -0.2927391529083252, 0.28516143560409546, -0.7693760991096497, -0.4443140923976898, 0.6038684248924255, 0.5030884146690369, 0.3258243501186371, 0.2973805069923401, -0.7438639998435974, -0.7513502240180969, -0.36765286326408386, 0.3755984306335449, 2.09248423576355, -0.1445971131324768, -0.07097189128398895, -0.7689582705497742, -0.26174280047416687, 0.0996389389038086, 0.2092791646718979, -0.29620161652565, -0.08590122312307358, -0.6668179035186768, -1.2073484659194946, 0.9499328136444092, 0.07997608929872513, 0.7497326731681824, -0.2926531434059143, 0.20125152170658112, -0.362758070230484, 0.47822701930999756, -0.9954043626785278, -0.563801109790802, 0.10379640758037567, -0.7012752890586853, 0.08589126169681549, -0.06889120489358902, 0.11950903385877609, -0.21686317026615143, -0.5213221907615662, 0.7259703874588013, 0.20646217465400696, -0.1056729331612587, -0.19535520672798157, 0.37920787930488586, -0.80433589220047, -1.1674391031265259, 0.4835752248764038, 0.5562125444412231, -0.3637146055698395, 0.2439296692609787, 0.5327642560005188, 0.20719975233078003, -0.10120447725057602, -0.5633465051651001, 0.1972428262233734, 0.17071248590946198, -0.021765567362308502, 0.34626486897468567, -0.2570844292640686, 0.2551930248737335, -1.069183111190796, 1.3710583448410034, 0.2916542887687683, -0.6472156643867493, 0.49057382345199585, -0.8784101009368896, -0.21107785403728485, 0.5223052501678467, -0.8382098078727722, -0.12651801109313965, -1.0038896799087524, 0.5166112184524536, -0.17469531297683716, 0.0719570741057396, -0.006732029374688864, 0.4795422852039337, 0.47787436842918396, 0.2631770372390747, 0.4605187177658081, 0.2458876520395279, -0.506168782711029, 0.6007831692695618, -0.4617338478565216, 0.004554900340735912, 0.31523025035858154, 0.3065943121910095, -0.16237081587314606, -0.1560140699148178, -0.8269562125205994, -0.3067028820514679, -0.22414147853851318, -0.25717565417289734, -0.23321285843849182, -0.02477969229221344, -0.5061783790588379, -0.9723289608955383, -0.016037123277783394, -0.8723865747451782, -0.3772129714488983, 0.70354825258255, 0.07092861086130142, -0.17986993491649628, -1.1003190279006958, -1.6374759674072266, -0.5940654873847961, -0.6106823086738586, -0.8919522762298584, 0.6287574172019958, 0.03389778360724449, -0.3282080292701721, -0.37744608521461487, -0.2821711003780365, -0.2942619025707245, 0.8299165964126587, -1.0929518938064575, 0.907317042350769, -0.13521204888820648, 0.29116103053092957, -0.23559610545635223, 0.15398070216178894, 0.5506398677825928, -0.6606885194778442, 0.5385168194770813, -1.1427689790725708, -0.139142706990242, -0.7497753500938416, -0.44140341877937317, 0.14110369980335236, 0.40147775411605835, 0.6051843762397766, 0.10481283813714981, -0.4909127652645111, 0.3480849862098694, 1.2166409492492676, -1.1154186725616455, -0.2803700566291809, -0.06296869367361069, 0.855133593082428, 0.2345237135887146, -0.40145182609558105, 0.3740856945514679, 0.21214306354522705, 0.4775824248790741, -0.4586586058139801, -0.05813433229923248, -0.08462901413440704, -0.7937572002410889, 0.5674699544906616, 1.7266652584075928, 0.1486048549413681, -0.1991739571094513, -1.3057821989059448, 0.1595880389213562, -0.8872445225715637, -0.32199621200561523, 0.4377985894680023, 0.9382261633872986, 0.41522935032844543, -0.1789148449897766, -0.072304368019104, 0.04520381987094879, 0.3842316269874573, 0.16510598361492157, -0.1252574324607849, -0.601890504360199, 0.029088711366057396, -0.0046949065290391445, 0.26926884055137634, 0.49307748675346375, -0.3550088107585907, 0.9801999926567078, 15.073471069335938, 1.0121405124664307, 0.08583072572946548, 0.9545552134513855, 0.848584771156311, 0.019761862233281136, -0.2936440706253052, -0.269706666469574, -1.0880496501922607, 0.2834814190864563, 1.4619591236114502, 0.08379264920949936, 1.2600008249282837, 0.07313954085111618, 0.331928551197052, 0.26717934012413025, -0.3699765205383301, 0.6417943835258484, 0.3605859875679016, -0.9149633049964905, 0.45672503113746643, 0.3203458786010742, 0.7599434852600098, 0.7985302805900574, 0.3524427115917206, 1.1792341470718384, 0.5246422290802002, -0.5104272365570068, 0.6444072723388672, -0.04581742733716965, 1.0955747365951538, -0.23472505807876587, 0.3657429814338684, 0.6199787855148315, -0.6974260210990906, -0.10255990922451019, -0.45764896273612976, -1.1752780675888062, 0.22705645859241486, 0.2159503847360611, -0.8574032783508301, -0.5589632987976074, -0.15181860327720642, 0.20303422212600708, 0.1718953549861908, 0.17738273739814758, -0.36911606788635254, 1.0585169792175293, -0.30962762236595154, -0.24685676395893097, -0.062056541442871094, 0.6101616024971008, 0.006780584342777729, 0.22760574519634247, -0.02317209169268608, -0.2888379395008087, 0.3185146749019623, 0.29215431213378906, -1.0679302215576172, 0.4069649279117584, -0.04556643217802048, -0.34449219703674316, 0.2550381124019623, 0.6490733027458191, 0.2560408115386963, -0.2311885803937912, -0.032649580389261246, 0.32509469985961914, 0.762706995010376, 0.42905354499816895, -0.31453484296798706, 0.33398154377937317, 0.6470330953598022, -0.40257006883621216, -0.3898482918739319, 0.14875969290733337, -0.19900792837142944, -0.4408988356590271, -0.42895129323005676, -0.28363654017448425, 0.1599816083908081, -0.7074968218803406, -0.9756751656532288, 0.7776587605476379, -0.1011868342757225, -0.2012389600276947, 0.26642197370529175, -0.9108381867408752, 0.12064072489738464, 0.6530651450157166, -1.7398992776870728, -0.93329918384552, 0.6261644959449768, -0.44026491045951843, -0.02853739634156227, 0.11543925106525421, 1.2118741273880005, 0.17626625299453735, -0.4522969722747803, 0.25376150012016296, 0.3091733157634735, 0.019890526309609413, -0.3718913495540619, -0.6468783617019653, 1.1855119466781616, 0.24709445238113403, -0.027506405487656593, 0.4230167269706726, -0.21313193440437317, 0.26395565271377563, -0.5808786153793335, 0.14001518487930298, 0.8450239896774292, -0.7434590458869934, -0.028103716671466827, -0.8486936688423157, -0.6653958559036255, 0.062183283269405365, 0.21675966680049896, -0.30269208550453186, 0.19911400973796844, 0.5875034928321838, -0.7084347009658813, -0.21647079288959503, -0.3018653690814972, 0.0993279293179512, 0.9084814190864563, -0.9422691464424133, -0.11986705660820007, 0.22829586267471313, 0.2806776463985443, -0.9007351398468018, -0.7859686017036438, -0.13481974601745605, 0.014182007871568203, 0.09101786464452744, 0.445244699716568, -0.8953160047531128, 0.5663156509399414, 1.0159668922424316, -0.23197510838508606, -0.9003978371620178, 0.12413249909877777, -0.9395914673805237, 0.24266678094863892, -0.065050408244133, 0.9869179725646973, -0.3542288839817047, -0.19973577558994293, 0.7861772179603577, 0.2549387812614441, -0.253233939409256, -0.7831828594207764, -0.5422718524932861, 0.26639020442962646, -0.7831860780715942, 0.564352810382843, -0.0761771947145462, -0.20286676287651062, -0.06468581408262253, 0.6048690676689148, 0.5479665398597717, -0.19174183905124664, -0.8719978332519531, 0.5663436055183411, -0.19115623831748962, -0.10885205864906311, -0.9273568987846375, -0.3140541613101959, -1.287424921989441, 0.23376917839050293, -1.4495898485183716, 0.16871020197868347, -0.6760718822479248, -0.3484777808189392, -0.30665379762649536, -0.14785553514957428, -0.04795067012310028, 0.22990620136260986, -0.0771663561463356, -0.09538262337446213, -0.02325926534831524, -0.1636834740638733, 0.7422182559967041, 0.9526140689849854, -0.4180961847305298, 0.1635853350162506, -0.049213238060474396, 0.4299449026584625, 0.3068915605545044, 0.5280463099479675, -0.44155341386795044, -0.7927688360214233, -1.6264872550964355, 0.43128758668899536, -0.2512913644313812, -0.25194093585014343, -0.384829580783844, 0.15274572372436523, 0.42986902594566345, -0.09257766604423523, 0.5792231559753418, 0.4836394488811493, -0.6506817936897278, -0.7146135568618774, 0.34579670429229736, -0.9401403069496155, 0.5168538689613342, 0.213551864027977, -0.47444236278533936, 0.11130957305431366, 0.6465656757354736, -0.24635790288448334, -0.6433935165405273, -0.357515811920166, 0.3583635985851288, -0.4375300109386444, 0.38313767313957214, -0.2876199781894684, 0.1262921392917633, -0.7986016273498535, -0.2751551568508148, -0.011956308037042618, 0.022114688530564308, -0.21995843946933746, 1.1044782400131226, -0.11397358775138855, -0.8400335311889648, -0.2246333658695221, 0.5154773592948914, -0.013675815425813198, -0.16007840633392334, 0.245933398604393, 0.49662795662879944, -0.5598564743995667, 0.47440996766090393, 0.5987088084220886, -0.10785674303770065, -0.945030689239502, -0.3284066319465637, 0.5184855461120605, -0.3162630796432495, 0.024489641189575195, 1.0858904123306274, -0.6506811380386353, -1.4135173559188843, 0.10436379164457321, -1.2843371629714966, 0.0001630577608011663, -0.673857569694519, 0.48216164112091064, 0.2824755012989044, 0.07071531563997269, -0.16709373891353607, -0.24350763857364655, -0.055690404027700424, 0.04446936398744583, -0.5248163938522339, 0.32379505038261414, -0.1603565514087677, -0.19808222353458405, 0.5949028730392456, 0.8508073687553406, -0.449616938829422, -0.45324647426605225, -0.6332924365997314, -0.24889564514160156, 0.04614977538585663, 0.2538606822490692, -0.288617879152298, -0.36654412746429443, 0.6487041115760803, 0.3152790665626526, 0.5748509168624878, -0.3697468936443329, -0.23118874430656433, 0.13549447059631348, 0.535693347454071, 0.4860072433948517, -0.8091627359390259, -1.0248860120773315, 1.3333823680877686, 1.03908109664917, -1.3380675315856934, 0.30003026127815247, -0.02444785088300705, -0.6695062518119812, 0.5842021107673645, 0.2789211571216583, 0.12448985874652863, 0.8957838416099548, -0.3520481586456299, 0.3403928875923157, 0.28997287154197693, -0.833291232585907, -0.14366792142391205, 0.8999546766281128, 0.4285567104816437, 0.9195843935012817, 0.8456777334213257, -0.17535071074962616, 0.8412511944770813, -0.34762340784072876, 0.10803701728582382, 0.11947425454854965, 0.3579254746437073, -0.0930977612733841, 0.10557004064321518, -0.0064859455451369286, 0.7289135456085205, -0.5562866926193237, -0.8198651671409607, 0.08296813815832138, 0.8409952521324158, 0.08899712562561035, 0.6852600574493408, 0.8608990907669067, 0.01916683465242386, 0.3528066873550415, 0.36611539125442505, 0.5584512948989868, -0.2830609381198883, -0.2938872277736664, -0.24896425008773804, -0.48281756043434143, 0.13291685283184052, -0.16581283509731293, -0.2182314097881317, -0.5911279320716858, -0.8354513049125671, 0.08064328134059906, -0.17288121581077576, 0.6529940962791443, 1.2123217582702637, 0.5767958760261536, 0.20507188141345978, -0.34666669368743896, -0.6452335119247437, -0.888636589050293, -1.3918585777282715, -0.21144363284111023, -0.00018327457655686885, -0.3536280393600464, 0.16389551758766174, -0.17966791987419128, -0.3585013449192047]}, "authors": [{"authorId": "1387466862", "name": "S. Gadre"}, {"authorId": "1438310376", "name": "Georgios Smyrnis"}, {"authorId": "34961417", "name": "Vaishaal Shankar"}, {"authorId": "40895369", "name": "Suchin Gururangan"}, {"authorId": "52193502", "name": "Mitchell Wortsman"}, {"authorId": "2287946530", "name": "Rulin Shao"}, {"authorId": "72847120", "name": "Jean-Pierre Mercat"}, {"authorId": "46372713", "name": "Alex Fang"}, {"authorId": "2287821949", "name": "Jeffrey Li"}, {"authorId": "150299584", "name": "Sedrick Scott Keh"}, {"authorId": "2291068532", "name": "Rui Xin"}, {"authorId": "2174178585", "name": "Marianna Nezhurina"}, {"authorId": "2291068185", "name": "Igor Vasiljevic"}, {"authorId": "2191688", "name": "J. Jitsev"}, {"authorId": "2257301126", "name": "Alexandros G. Dimakis"}, {"authorId": "1387994137", "name": "Gabriel Ilharco"}, {"authorId": "2265621012", "name": "Shuran Song"}, {"authorId": "2283843631", "name": "Thomas Kollar"}, {"authorId": "2444742", "name": "Y. Carmon"}, {"authorId": "2298523", "name": "Achal Dave"}, {"authorId": "2291068369", "name": "Reinhard Heckel"}, {"authorId": "2037383772", "name": "Niklas Muennighoff"}, {"authorId": "2253541812", "name": "Ludwig Schmidt"}], "references": [{"paperId": "a509670659e8b054e2b7d1b6f8a0bc722398fa62", "title": "PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation"}, {"paperId": "31ef7b614fa58a4e71c87f2c38b1e9edd7bbb424", "title": "A Survey on Data Selection for Language Models"}, {"paperId": "0e8176ecced2a01ca3c7dc31e8a3f72d0a7d3767", "title": "Generative Representational Instruction Tuning"}, {"paperId": "2296629527ebbd6f8c897df7cf5cdbac3f0cc15b", "title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model"}, {"paperId": "88cd6e0f7c2f1fe2bd4796a6729447d60bfeffc9", "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning"}, {"paperId": "a73da8bdc130f0e78063b4f6efa09e9debc3569f", "title": "Scaling Laws for Downstream Task Performance of Large Language Models"}, {"paperId": "c0d8e5ee66c279299012cc3b8d0519011b3f4998", "title": "KTO: Model Alignment as Prospect Theoretic Optimization"}, {"paperId": "ac45bbf9940512d9d686cf8cd3a95969bc313570", "title": "OLMo: Accelerating the Science of Language Models"}, {"paperId": "ad1bb59e3e18a0dd8503c3961d6074f162baf710", "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research"}, {"paperId": "8554b7c4ec2466326f5bd55335082edd83183f94", "title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models"}, {"paperId": "82f75d838e92196864131bad25b1abc3b5d40a6f", "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws"}, {"paperId": "1a3f7e23ef8f0bf06d0efa0dc174e4e361226ead", "title": "Paloma: A Benchmark for Evaluating Language Model Fit"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "1a849f298dcd49fe99ebca6749c0564585aa3018", "title": "FinGPT: Large Generative Models for a Small Language"}, {"paperId": "8e817e7c898a6a52f0abd5acfb9de9e313b13ccf", "title": "The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing & Attribution in AI"}, {"paperId": "f5789596531fad358c3166fdb5bd72d8e661c32c", "title": "Small-scale proxies for large-scale Transformer training instabilities"}, {"paperId": "40e0b9361d88b1879891eb6d16de110b30bf6c62", "title": "OctoPack: Instruction Tuning Code Large Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b", "title": "Textbooks Are All You Need"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "560e6237b48c5c7b280f1c0035b3660546c8d4a9", "title": "Analyzing the Sample Complexity of Self-Supervised Image Reconstruction Methods"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c", "title": "Scaling Data-Constrained Language Models"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "f9570989919338079088270a9cf1a7afc8db8093", "title": "DataComp: In search of the next generation of multimodal datasets"}, {"paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da", "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"}, {"paperId": "68c834c19cd126bbd6d25a3572d7205cfed76271", "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a", "title": "SantaCoder: don't reach for the stars!"}, {"paperId": "16de2006e2960ba410772c6b6d460b83c0a5cc4b", "title": "Reproducible Scaling Laws for Contrastive Language-Image Learning"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "61f329722cd94291898c2c8131606a55f7a07219", "title": "Broken Neural Scaling Laws"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "fb9f9e98d35340875905730e1a80221fec818944", "title": "Revisiting Neural Scaling Laws in Language and Vision"}, {"paperId": "6edccbd83a9aae204785d4821f97855677c33866", "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?"}, {"paperId": "45122c8f76a4e2fd0163d1f0522db37e97ea4721", "title": "Beyond neural scaling laws: beating power law scaling via data pruning"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "0b0d7d87c58d41b92d907347b778032be5966f60", "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"}, {"paperId": "e404bdfaa858b3c25540aa5d2c5dfe20c16ead37", "title": "Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments"}, {"paperId": "37ddb9305c8c9120c21a2fae5a851ce8e4384a9c", "title": "Data Scaling Laws in NMT: The Effect of Noise and Architecture"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170", "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ca9047c78d48b606c4e4f0c456b1dda550de28b2", "title": "Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers"}, {"paperId": "7d5c661fa9a4255ee087e861f820564ea2e2bd6b", "title": "BBQ: A hand-built bias benchmark for question answering"}, {"paperId": "c206a6e7f51f5e1b6bfc479a174b66ad88ada2db", "title": "Exploring the Limits of Large Scale Pre-training"}, {"paperId": "2d4f66046bb436864cd6bf589e3a931c405f9f44", "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"}, {"paperId": "de1fdaf92488f2f33ddc0272628c8543778d0da9", "title": "Scaling Laws for Neural Machine Translation"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "f977d79980ed2dfc7b2194fe680895e49b3b60a9", "title": "From LSAT: The Progress and Challenges of Complex Reasoning"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3", "title": "Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b", "title": "Explaining neural scaling laws"}, {"paperId": "4383a975c09b72ba2f1a77cd779bb6965dbfb2fb", "title": "Scaling Laws for Transfer"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "87da4c27f7b2ddac63654458b06c9d708d042f6a", "title": "Feature Learning in Infinite-Width Neural Networks"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "f10a04a77fd1cd719792de374a60f3fd03f6b944", "title": "Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc", "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "2c5a8950cf0a13e229ad19093ba064495fda8de7", "title": "A Neural Scaling Law from the Dimension of the Data Manifold"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "703b96f65fb349fe2a2deb896ae07e6eb552b391", "title": "JEC-QA: A Legal-Domain Question Answering Dataset"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "d28c18a3c2a0afdc0a8634d18345af8d36e1f948", "title": "A Constructive Prediction of the Generalization Error Across Scales"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0c3c4c88c7b07596221ac640c7b7102686e3eae3", "title": "PubMedQA: A Dataset for Biomedical Research Question Answering"}, {"paperId": "4cf963e5fd88825ac62ad6cce364447e5d2dfb2b", "title": "Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "083afaf8c8eb4d3f61a884221b340c3436c4bc13", "title": "Beyond human-level accuracy: computational challenges in deep learning"}, {"paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094", "title": "CoQA: A Conversational Question Answering Challenge"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7", "title": "Gender Bias in Coreference Resolution"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "a1c922be467d1c0c64b963e65dae41778b81b2a0", "title": "Deep Learning Scaling is Predictable, Empirically"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "424aef7340ee618132cc3314669400e23ad910ba", "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"}, {"paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "2c2e5a40ae5e948c9a8d5e81fe029a1d6f5c8090", "title": "Evaluation of machine translation"}, {"paperId": "d0d3f4d1003db0fb637519ef5d8bb140e7df8355", "title": "May the source be with you."}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af", "title": "Are Emergent Abilities of Large Language Models a Mirage?"}, {"paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b", "title": "Crosslingual Generalization through Multitask Finetuning"}, {"paperId": "eec7ad0270eda7298c139af6e2676599f1fd53f6", "title": "Data and Parameter Scaling Laws for Neural Machine Translation"}, {"paperId": null, "title": "ELECTRA: Pre-training text encoders as discriminators rather than generators"}, {"paperId": null, "title": "SciPy 1.0 Contributors"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Social IQa: Commonsense reasoning about social interactions"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Together Computer"}, {"paperId": null, "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms"}, {"paperId": null, "title": "Tying word vectors and word 17"}, {"paperId": null, "title": "Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length"}, {"paperId": null, "title": "Scaling beyond language modeling. There is a large body of work on scaling neural networks beyond language modeling, for example in computer vision [58, 124, 103, 1, 2],"}]}