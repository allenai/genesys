{"paperId": "efc50d7ee5bcd8039ba5d686d7d943be3ff199b0", "title": "Efficient Multimodal Large Language Models: A Survey", "abstract": "In the past year, Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance in tasks such as visual question answering, visual understanding and reasoning. However, the extensive model size and high training and inference costs have hindered the widespread application of MLLMs in academia and industry. Thus, studying efficient and lightweight MLLMs has enormous potential, especially in edge computing scenarios. In this survey, we provide a comprehensive and systematic review of the current state of efficient MLLMs. Specifically, we summarize the timeline of representative efficient MLLMs, research state of efficient structures and strategies, and the applications. Finally, we discuss the limitations of current efficient MLLM research and promising future directions. Please refer to our GitHub repository for more details: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.", "venue": "arXiv.org", "year": 2024, "citationCount": 6, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This survey provides a comprehensive and systematic review of the current state of efficient MLLMs, including the timeline of representative efficient MLLMs, research state of efficient structures and strategies, and the applications."}, "embedding": {"model": "specter_v2", "vector": [-0.04677671566605568, 0.35911476612091064, -0.05177520588040352, -0.371287077665329, -0.3500475287437439, -0.004721040837466717, 0.5919803380966187, 0.05163722485303879, -0.7484095692634583, -0.4561508595943451, 0.5237632989883423, 0.24883945286273956, 0.39092254638671875, 0.1636062115430832, -0.1625625342130661, 0.5506672859191895, -0.63219153881073, 0.5805138945579529, 0.10871360450983047, -0.2223910093307495, -0.1301087886095047, -0.46635475754737854, -1.4791152477264404, 0.429945707321167, 0.2098318487405777, 0.584501326084137, 0.3448999226093292, 1.2784110307693481, -0.523082971572876, 0.679140031337738, 0.5074244141578674, -0.27331361174583435, -0.24055324494838715, 0.25829190015792847, -0.38614508509635925, -0.1093275398015976, 0.29521721601486206, -0.6603586077690125, -0.7033165693283081, 0.2538253366947174, -0.009768364951014519, 0.615666925907135, 0.5054669380187988, -0.8648423552513123, -0.3877229690551758, 0.48120442032814026, 0.5441399812698364, 0.2622826099395752, -0.012298382818698883, -0.7071413993835449, 1.4237186908721924, -1.6396658420562744, 0.2939812242984772, 1.95475435256958, 0.0363643541932106, 0.47384077310562134, -0.283431738615036, -0.5933833122253418, 0.9194366931915283, 0.3110322654247284, -0.9727817177772522, -0.5744207501411438, -0.3787429630756378, 0.12427712231874466, 1.964370608329773, -0.14863382279872894, -0.2728743851184845, 0.38740187883377075, -0.37164899706840515, 1.5911940336227417, -0.048450976610183716, -0.8378635048866272, -0.12071648240089417, -0.023291748017072678, 0.31690773367881775, 1.2540719509124756, -0.6055694818496704, 0.14192579686641693, -1.2141112089157104, -0.23205801844596863, 0.39622658491134644, -0.2655419409275055, -0.2711750566959381, -0.25125783681869507, -0.34474101662635803, 1.000378966331482, 0.44914504885673523, 0.443092942237854, 0.10494872182607651, 0.7185528874397278, 0.14519286155700684, 0.2842886447906494, -0.18735164403915405, 0.18375512957572937, -0.3857603371143341, 0.4231260418891907, -0.8300677537918091, 0.5615039467811584, 0.4077885150909424, 0.8661168217658997, -0.45395269989967346, -0.11475183814764023, -0.8451645374298096, 0.3651209771633148, 1.6469975709915161, 0.09368930757045746, 0.3844839334487915, -0.8833150863647461, 0.15875612199306488, -0.48380047082901, 0.16720689833164215, -0.42419034242630005, -0.4700154662132263, 0.05162544548511505, -0.4269053637981415, -1.0887802839279175, -0.32862746715545654, 0.1639547199010849, -0.6855053305625916, 0.6655948162078857, -0.36085912585258484, -0.07501506060361862, 0.0008760574273765087, 0.3268851041793823, 0.9234088063240051, 1.05050528049469, 0.5974258184432983, 0.23959693312644958, 1.2138315439224243, -0.9993040561676025, -0.4618988633155823, -1.3636835813522339, 1.018235683441162, -0.3148471415042877, 0.45708370208740234, -0.3489997088909149, -1.1375418901443481, -1.073388934135437, -0.5905638337135315, -0.5870481729507446, -0.5855159759521484, 0.6064355373382568, 0.869588315486908, 0.27760258316993713, -1.3130707740783691, -0.21106411516666412, -0.08187036961317062, -0.256788432598114, 0.09625817835330963, 0.37739017605781555, 0.18480221927165985, -0.8467573523521423, -0.9500206112861633, 0.35598087310791016, -0.08960266411304474, -0.4779115915298462, -0.3458140790462494, 0.17468813061714172, -1.6877673864364624, 0.06375597417354584, 0.23557133972644806, -0.849248468875885, 1.239557147026062, 0.13433142006397247, -1.1304707527160645, 0.5374356508255005, -0.9013032913208008, 0.22427569329738617, 0.0755869448184967, -0.0923660472035408, -0.5559805631637573, -0.3329392373561859, -0.6098765730857849, 1.2902100086212158, 0.5529808402061462, -0.2628747820854187, -0.2454562932252884, 0.20638015866279602, -0.12345238029956818, 0.10594203323125839, -0.11697126179933548, 1.331872582435608, -0.6866471171379089, -0.02664599008858204, 0.6174817681312561, 0.6430978775024414, -0.2765219211578369, 0.22218655049800873, -0.06637726724147797, -1.293852686882019, 0.8223376274108887, 0.022287975996732712, 0.9565591216087341, -1.0022680759429932, -0.6452140212059021, -0.486186683177948, 0.221223846077919, -0.554617702960968, -1.28277587890625, 0.6508132219314575, -0.27744612097740173, 0.1749877780675888, -0.23113159835338593, -1.6687862873077393, 0.3579954206943512, -0.1059887483716011, -0.6169136166572571, -0.12052915245294571, 0.10471697151660919, 1.3539501428604126, -0.5255018472671509, -0.264340341091156, -0.024463312700390816, 0.2751159071922302, -0.8344252109527588, 1.5143320560455322, -0.7842537760734558, -0.19828641414642334, -0.12627695500850677, -0.07444143295288086, -0.020607946440577507, -0.7888917326927185, 0.17281799018383026, -0.40476253628730774, -0.027900980785489082, 0.014119591563940048, -0.49149465560913086, 1.501410722732544, -0.23470060527324677, 0.6957270503044128, -0.39739546179771423, -0.3254031538963318, -0.03457250818610191, 0.41453903913497925, -0.5840674638748169, -0.3858460783958435, 0.29170313477516174, 0.1541050225496292, -0.6752487421035767, -0.11329393088817596, 0.7650260329246521, 0.9100933074951172, -0.2811994254589081, 0.4301600158214569, 0.30189257860183716, -0.2415107935667038, 0.6386812925338745, 0.5631430745124817, 0.5182734131813049, 0.2806493639945984, 0.1062328889966011, 0.3920293152332306, 0.5931187868118286, -0.9993736743927002, -0.6560570597648621, 0.7650756239891052, 0.8096330761909485, 0.9633980989456177, 0.2047521322965622, -0.6420734524726868, 0.046124089509248734, 0.2338235080242157, 0.6834401488304138, 1.8524863719940186, 0.04067286476492882, -0.22399400174617767, -0.5770978331565857, -0.19311445951461792, -0.47058069705963135, 0.32484787702560425, -0.1421065777540207, 0.20446817576885223, -0.4677004814147949, -1.0410194396972656, 0.7525507807731628, 0.7042415142059326, 0.6678498983383179, -0.685539722442627, -0.32619601488113403, -0.4385277032852173, -0.05559000372886658, -1.0685992240905762, -0.4497077763080597, -0.08965373039245605, -0.656736433506012, -0.22856733202934265, 0.08069223165512085, -0.30930158495903015, 0.4365268349647522, -0.7209293842315674, 1.155507206916809, -0.6370740532875061, -0.2155354917049408, 0.2864791452884674, 0.5444493889808655, -0.4360850155353546, -0.5063604712486267, -0.10240806639194489, -0.21201927959918976, -0.04295375570654869, 0.4655781090259552, 1.003995418548584, 0.14766648411750793, 0.1821412444114685, -0.5667527318000793, 0.40163666009902954, 0.23812595009803772, -0.3670852482318878, 0.7469832301139832, -0.5121980309486389, -0.048358071595430374, -0.9519084692001343, 0.924921989440918, 0.1103348433971405, -0.48173344135284424, 0.5102412700653076, -0.6000935435295105, -0.23649032413959503, 0.043700288981199265, -0.8056150674819946, -0.3423268496990204, -0.5129534602165222, 0.29095423221588135, -0.20029109716415405, -0.5599743127822876, 0.34734562039375305, 0.2631492614746094, 0.0914166271686554, 0.031017059460282326, 0.4352237284183502, 0.21967743337154388, -0.19895237684249878, 0.7773627042770386, -0.6566658020019531, 0.3177371919155121, 0.11684445291757584, 0.055052366107702255, -0.025304187089204788, -0.3728195130825043, -0.6257662177085876, -0.3564373850822449, -0.47365710139274597, -0.04100975766777992, -0.3301258683204651, 0.30126678943634033, -0.772470235824585, -0.925957202911377, -0.0587138868868351, -1.361764907836914, -0.009927641600370407, 0.4679237902164459, -0.3119773864746094, -0.059704676270484924, -0.9383818507194519, -1.2096202373504639, -0.6218120455741882, -0.20783336460590363, -0.9636589884757996, 0.7409340739250183, 0.21231554448604584, -0.563016414642334, -0.4010043144226074, -0.1921222060918808, 0.03153087943792343, 0.9228547215461731, -0.5973199009895325, 1.2522650957107544, 0.06825064867734909, -0.6175359487533569, -0.7638798356056213, -0.13793225586414337, 0.1448294222354889, -0.5552696585655212, 0.011792844161391258, -0.9887450337409973, 0.01111788209527731, -0.12343037873506546, -0.36347609758377075, 0.2686851918697357, 0.5282086730003357, 0.8573099970817566, 0.44682684540748596, -0.5962182283401489, 0.0375155545771122, 1.2869027853012085, -0.7289953827857971, -0.37550681829452515, -0.24909791350364685, 0.9646124243736267, 0.43725696206092834, -0.5993877649307251, 0.5561681985855103, 0.7800917625427246, 0.2884046733379364, 0.41938483715057373, -0.22041571140289307, -0.056857798248529434, -0.4295235872268677, 0.6968786716461182, 1.3835018873214722, 0.5801734924316406, -0.5235174298286438, -0.8403053283691406, 0.49360498785972595, -1.4364501237869263, -0.15014486014842987, 0.3246276378631592, 0.5458932518959045, 0.11521047353744507, -0.5906021595001221, -0.42559099197387695, -0.7105361819267273, 0.36801841855049133, 0.7113815546035767, -0.39078110456466675, -0.5663681030273438, -0.39995625615119934, 0.3873788118362427, -0.0827985480427742, 0.644583523273468, -0.437076210975647, 0.37749987840652466, 14.466047286987305, 0.651603102684021, 0.30610308051109314, 0.6651777029037476, 0.8618974685668945, 0.06640595942735672, -0.30229300260543823, -0.05098183825612068, -1.2477328777313232, -0.578184962272644, 1.2402737140655518, 0.37745237350463867, 0.5689979195594788, 0.0281982459127903, 0.04125923290848732, -0.01336018554866314, -0.9471170902252197, 0.992094874382019, 0.7620011568069458, -1.175431489944458, 0.481153279542923, -0.0838034525513649, 0.11702682822942734, 0.6176209449768066, 0.7120242118835449, 0.9243161678314209, 0.379844069480896, -1.0051757097244263, 0.421798974275589, 0.5140866041183472, 0.7363609075546265, 0.06094043701887131, 0.4857032001018524, 0.8272401690483093, -1.2027244567871094, -0.45891880989074707, -0.6354691982269287, -1.0783042907714844, 0.4083437919616699, -0.46357208490371704, -0.17608825862407684, -0.6743676662445068, -0.3993871212005615, 0.5741276741027832, -0.2667638957500458, 0.4033838212490082, -0.10332425683736801, 0.44735074043273926, -0.32973429560661316, -0.01619984209537506, 0.18639707565307617, 0.5197233557701111, 0.008675646968185902, -0.35908272862434387, 0.17418606579303741, 0.024537112563848495, 0.15873701870441437, 0.5358721017837524, -0.6895574927330017, 0.1588415801525116, -0.3093963861465454, -0.35447958111763, -0.2107330709695816, 0.7138218879699707, 0.399865061044693, 0.4762531816959381, -0.8821660876274109, 0.18080365657806396, 0.6212969422340393, 0.5235065221786499, -0.22418144345283508, 0.39320024847984314, 0.004573021084070206, -0.4305548369884491, 0.2761386036872864, 0.5237582325935364, 0.08670772612094879, -0.5846615433692932, -0.6380033493041992, -0.3868854343891144, 0.8736358880996704, -0.9991614818572998, -0.8633046746253967, 0.9018666744232178, -0.2070695161819458, -0.20211467146873474, 0.1352781057357788, -0.8057374358177185, 0.0010377339785918593, 0.4746616780757904, -1.5452042818069458, -1.0159380435943604, 0.1578020602464676, -0.1910204291343689, 0.03804398700594902, 0.05601279437541962, 1.4861655235290527, -0.024281486868858337, -0.503628671169281, 0.03449885919690132, -0.15776103734970093, -0.05338582023978233, -0.3251388967037201, -0.43452709913253784, 0.5401167869567871, 0.26605224609375, 0.30918413400650024, 0.10105667263269424, -0.12653890252113342, 0.3042323887348175, -0.9540403485298157, 0.01418179739266634, 1.1698052883148193, -1.04051673412323, -0.7312947511672974, -0.7859494090080261, -0.736348032951355, 0.31951048970222473, 0.531203031539917, -0.2843407690525055, 0.4101235270500183, 0.028747519478201866, -0.7049345374107361, 0.0071214307099580765, -0.5805280208587646, 0.18777580559253693, 0.2051268070936203, -0.7985721826553345, -0.0794210284948349, 0.20010513067245483, 0.3604048788547516, -0.7823488116264343, -0.2090752124786377, -0.14217469096183777, -0.010217512026429176, 0.3143341541290283, 0.9667171239852905, -0.6748231649398804, 0.7768025994300842, 0.41176339983940125, -0.39854753017425537, -0.3325076103210449, 0.098905548453331, -0.5737014412879944, -0.5858061909675598, -0.36893418431282043, 0.771162211894989, -0.2978721559047699, 0.22350339591503143, 1.1771926879882812, 0.7737347483634949, -0.504900336265564, -0.6380288600921631, 0.20977719128131866, -0.2572357952594757, -0.5381227731704712, 0.1896142065525055, -0.5517255663871765, -0.48633092641830444, 0.28679487109184265, 0.533099353313446, 0.9569323062896729, -0.32249167561531067, -0.2346733808517456, 0.4610462784767151, -0.2126045972108841, -0.10777422785758972, -0.5708000659942627, -0.1883525550365448, -1.8734122514724731, 0.2804234027862549, -0.9561565518379211, -0.010175526142120361, -1.3522958755493164, -0.2863931953907013, 0.43757426738739014, -0.08004369586706161, 0.06811464577913284, 0.40509796142578125, -0.32748928666114807, -0.37047216296195984, -0.7339741587638855, -0.6689348816871643, 0.7084972858428955, 0.8423976302146912, -0.5287303328514099, 0.44374486804008484, -0.05843445658683777, 0.13883452117443085, 0.24858908355236053, 0.057543665170669556, -0.3983743488788605, -1.0299060344696045, -1.510475516319275, 0.647190511226654, 0.20775669813156128, -0.040183063596487045, -0.38764122128486633, 1.0520285367965698, 0.4049503207206726, -0.07766659557819366, -0.0935254916548729, 0.7051160931587219, -0.8392373323440552, -0.7128585577011108, 0.3391096889972687, -1.1341644525527954, 0.4647224247455597, 0.2957580089569092, -0.4824942648410797, -0.681279182434082, 0.5065022706985474, -0.05994821712374687, -0.9166465997695923, -0.85139399766922, 0.5963984131813049, -0.5087822079658508, -0.02062842808663845, -0.26961255073547363, -0.02001134864985943, -1.0697816610336304, -0.8483454585075378, -0.14233234524726868, 0.5303726196289062, -0.4626592695713043, 1.025025486946106, 1.0844231843948364, -0.9122121930122375, -0.3732573091983795, 0.44765570759773254, 0.25724977254867554, 0.08449755609035492, 0.5958042740821838, 0.30539047718048096, -0.3672526776790619, 0.5835139751434326, 0.6532487273216248, 0.42871686816215515, -1.1579846143722534, -0.03337981179356575, 0.9339607954025269, -0.35705792903900146, 0.08755091577768326, 1.5571237802505493, 0.1049351915717125, -1.1484661102294922, 0.24493025243282318, -1.0982850790023804, -0.853969931602478, -0.34781351685523987, 0.6672645211219788, 0.019756261259317398, -0.10485566407442093, -0.1512967050075531, -0.466512531042099, 0.27261924743652344, 0.09073175489902496, -0.626410186290741, 0.4736782908439636, -0.2200164496898651, -0.8119856715202332, 0.575235903263092, 1.0424022674560547, -0.9027904868125916, -0.5399959683418274, -0.5986461639404297, -0.11100707948207855, -0.01473745983093977, 0.36350056529045105, -0.39305567741394043, -0.5363357067108154, 0.7656972408294678, 0.48360592126846313, 0.12239738553762436, 0.41764214634895325, 0.2546049952507019, 0.30180951952934265, 0.9233980178833008, -0.3331720232963562, -0.562518298625946, -0.629078209400177, 1.1482993364334106, 1.3950979709625244, -1.105621099472046, -0.09550002962350845, -0.2561783790588379, -0.5178317427635193, 0.9586055278778076, 0.33934274315834045, 0.23264941573143005, 1.0236365795135498, -0.13549187779426575, 0.08209605515003204, -0.022358104586601257, -1.4580024480819702, -0.08822464197874069, 1.2348475456237793, 0.6557552814483643, 0.8687185645103455, 0.266437292098999, 0.1465226411819458, 0.6772205233573914, 0.14199739694595337, 0.1659800261259079, 0.2650088965892792, 0.4397643506526947, -0.2046554833650589, -0.11416175961494446, 0.22588030993938446, 0.628093957901001, -0.11762014031410217, -0.7173892259597778, 0.20360660552978516, 0.5220125317573547, 0.15027186274528503, 0.9503017663955688, 0.8195030689239502, 0.6086109280586243, 0.5230331420898438, 0.46602678298950195, 0.6862518191337585, -0.6236199140548706, 0.10671835392713547, -0.2799740731716156, -0.5112323760986328, -0.161728635430336, -0.1467665731906891, -0.4008692502975464, -0.4892168343067169, -0.0818624272942543, 0.5819281935691833, -0.46001026034355164, 0.4469853937625885, 1.1860222816467285, 0.5719864368438721, 0.2236870378255844, -0.6537292003631592, 0.25773119926452637, -0.37741774320602417, -0.9049878716468811, 0.15082460641860962, -0.6384344696998596, -0.20280881226062775, -0.017115863040089607, -0.11998774856328964, -0.12008372694253922]}, "authors": [{"authorId": "2267492043", "name": "Yizhang Jin"}, {"authorId": "2302162727", "name": "Jian Li"}, {"authorId": "2284727141", "name": "Yexin Liu"}, {"authorId": "2302155099", "name": "Tianjun Gu"}, {"authorId": "2302286722", "name": "Kai Wu"}, {"authorId": "2275290482", "name": "Zhengkai Jiang"}, {"authorId": "2219723109", "name": "Muyang He"}, {"authorId": "2284660362", "name": "Bo Zhao"}, {"authorId": "2302505492", "name": "Xin Tan"}, {"authorId": "2066402135", "name": "Zhenye Gan"}, {"authorId": "2628601", "name": "Yabiao Wang"}, {"authorId": "2302227792", "name": "Chengjie Wang"}, {"authorId": "2109606150", "name": "Lizhuang Ma"}], "references": [{"paperId": "4062c4310c1fa0b6f2136c7bfd5e6dcd265c6a11", "title": "Memory-Space Visual Prompting for Efficient Vision-Language Fine-Tuning"}, {"paperId": "e66f54d35d7218dc2d4de380981f49e7e32648ac", "title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference"}, {"paperId": "3b62644272dc3a8c3f4698e29aa5d55cbc19e660", "title": "Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey"}, {"paperId": "24354722e36c358b69893ab05220d6f2291989d1", "title": "TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning"}, {"paperId": "d4dd5be156a0f29f45e15c1df69e24fa7bd751ec", "title": "TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models"}, {"paperId": "9738dde55ab77cc26271a5753db7dd7851176fd6", "title": "BRAVE: Broadening the visual encoding of vision-language models"}, {"paperId": "d7ffcd4c5225ee752c5af9375e426fb6b5c7f88c", "title": "HRVDA: High-Resolution Visual Document Assistant"}, {"paperId": "01ae1c181dcb5117491affae728065e5e62bf074", "title": "InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions from 336 Pixels to 4K HD"}, {"paperId": "49873ee415619efd9e1e4c16f73ee066ff008c1f", "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies"}, {"paperId": "a4f48e62aedc705dcb757f5ad8f732c5b364539d", "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact Language Model"}, {"paperId": "9f3bc73b27d83a15baf81a4221322c04c522ff2b", "title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models"}, {"paperId": "b38845e9adbeeeab37519a2fc30e899411b4a36a", "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models"}, {"paperId": "2eb799664c1eb6b4d83cacd43b66348fbe15b58e", "title": "Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models"}, {"paperId": "c0ef72d02b93065e77c506e23ce9acbbcd945893", "title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models"}, {"paperId": "40e996a7c3e914a67c708704fa9b4c54ea70f36e", "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference"}, {"paperId": "6d49ed0ea24b9c218f5ec6731cd261ce618df2ac", "title": "VL-Mamba: Exploring State Space Models for Multimodal Learning"}, {"paperId": "dbdfe71fdf641bebac5a052a60de75342871e3df", "title": "When Do We Not Need Larger Vision Models?"}, {"paperId": "6675bcf6dc97c87da7afda223938ec7e51ecc3b2", "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training"}, {"paperId": "3421d12d98956c36c5d39c0ef023b378b6922068", "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models"}, {"paperId": "c50ee01a2d2973a8ce4a0d4b4a31e98d0c261a19", "title": "Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models"}, {"paperId": "59d2cfb02f40363fbc48c1ba4d769d0e1c0e93f2", "title": "m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers"}, {"paperId": "6a6751f59c5dbc80823b3cf47c3aaae063991b86", "title": "TinyLLaVA: A Framework of Small-scale Large Multimodal Models"}, {"paperId": "f6b9ccd7533b58e14d284191f1a576b0c764b3d5", "title": "ALLaVA: Harnessing GPT4V-Synthesized Data for Lite Vision-Language Models"}, {"paperId": "a1714677252a39d1835824efb185beb0113ca189", "title": "Efficient Multimodal Learning from Data-centric Perspective"}, {"paperId": "ec8e2b45c4601730015608a58e33409224a81228", "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models"}, {"paperId": "a091bf215c716a146140f81c751712db628c8e20", "title": "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model"}, {"paperId": "cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3", "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"}, {"paperId": "41f12456780aecd204a210ce04b1a92d022b8c4c", "title": "Small Language Model Meets with Reinforced Vision Vocabulary"}, {"paperId": "7260442ef9c0448f07ce3803efd49cebaffcebe9", "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism"}, {"paperId": "ece33ee67d74c29cd2a83c505e5bf0b818f9c2a1", "title": "LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model"}, {"paperId": "98ab627dd147db88b5e5cfa9a74f1bd8da110021", "title": "TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones"}, {"paperId": "6a33e58ef961a3a0a5657518b2be86395eb7c8d0", "title": "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"}, {"paperId": "4b1b5e219fb41a7413599c3b2ca6a7fdf045d1a5", "title": "Generative Multimodal Models are In-Context Learners"}, {"paperId": "2141ed804636a1cf339d606cd03fd3b3e9582133", "title": "VILA: On Pre-training for Visual Language Models"}, {"paperId": "4f5654ec1dfc04478be42d03eee8e6db6bd9ca14", "title": "Honeybee: Locality-enhanced Projector for Multimodal LLM"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "b50d19c5c298f6562c3b3c6c3822a351bdc89260", "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"}, {"paperId": "f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560", "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"}, {"paperId": "107fb6eec2febbae12db29bf3e311aaf5680027c", "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"}, {"paperId": "619184447595337a9fe3dca72c4e951e7ab7467c", "title": "To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning"}, {"paperId": "ad13b213681b6f634bc83a264df246e83dd9a9d9", "title": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"}, {"paperId": "1ddbd08ad8cf22a5c66c4242194c4286328533bf", "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "8946891e94831adc8cddb0d32311cce2445c96d2", "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"}, {"paperId": "8eb99f1ed884356871ddbcf1377b82359071906a", "title": "LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment"}, {"paperId": "f349e5e8f0d18c948c1ffd92d3791db2b0ba2e55", "title": "Data Filtering Networks"}, {"paperId": "99c6b85cacacd076b24496ab898ce5d700c0180b", "title": "CAIT: Triple-Win Compression towards High Accuracy, Fast Inference, and Favorable Transferability For ViTs"}, {"paperId": "dcb74fb63acd87d3db0a77de89720300fb28b50a", "title": "A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking"}, {"paperId": "8ce219059d777c2333ee21cb2af2aad71275c98f", "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"}, {"paperId": "94972e30504017156ef5b5debc419bf6edc67384", "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"}, {"paperId": "4309d572a37d655779f9dce6a2c98c66334132de", "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"}, {"paperId": "304f8b4edea01fdb5a2f7f8b998c83188deeccff", "title": "Towards Generalist Biomedical AI"}, {"paperId": "b37b1dc72b1882858f5120f2cd6883134089a6ed", "title": "MMBench: Is Your Multi-modal Model an All-around Player?"}, {"paperId": "451a3f03aca4aa87b93981364842137417549e58", "title": "SVIT: Scaling up Visual Instruction Tuning"}, {"paperId": "c7a7104df3db13737a865ede2be8146990fa4026", "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"}, {"paperId": "948e8cfae92c2004f2dd5c9316f5972f8baaea21", "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"}, {"paperId": "f4bdec0cf595720bc8ee5df2196324bac8f52ab4", "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources"}, {"paperId": "5d321194696f1f75cf9da045e6022b2f20ba5b9c", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"}, {"paperId": "0b6b756bcd3f4b4afa0d2d1fac8eecaf35e79aa1", "title": "BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models"}, {"paperId": "21c70bb9093ad46768f8dd36d3a8614bffe2243a", "title": "Bit-shrinking: Limiting Instantaneous Sharpness for Improving Post-training Quantization"}, {"paperId": "ad4b365630f1c13d74d78f0f5d8cee87ef356d41", "title": "Fine-Tuning Language Models with Just Forward Passes"}, {"paperId": "c3edb12342b7c6af51f8e36fdef11c1b057b44c2", "title": "BinaryViT: Towards Efficient and Accurate Binary Vision Transformers"}, {"paperId": "9c3a9b4821daa03cb5369041d59d2714329a3811", "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "07be590365e7fb76680be4ed67a5505763ec2d96", "title": "Boost Vision Transformer with GPU-Friendly Sparsity and Quantization"}, {"paperId": "206400aba5f12f734cdd2e4ab48ef6014ea60773", "title": "Evaluating Object Hallucination in Large Vision-Language Models"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "d48cb91b9e555194f7494c4d4bb9815021d3ee45", "title": "VideoChat: Chat-Centric Video Understanding"}, {"paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "df958800014d310b6df34ad83d771314d68fbb2d", "title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text"}, {"paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891", "title": "DINOv2: Learning Robust Visual Features without Supervision"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "bdb68c5e2369633b20e733774ac66eb4600c34d1", "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models"}, {"paperId": "35aba190f28b5c39df333c06ca21f46bd4845eba", "title": "Sigmoid Loss for Language Image Pre-Training"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "e60b6836b45ad0ae02a5fa663c8c31119f0c0a94", "title": "X-Pruner: eXplainable Pruning for Vision Transformers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "a05ebb7be3de58b28af75f9c4d340fa03eeb7fb1", "title": "Quantformer: Learning Extremely Low-Precision Vision Transformers"}, {"paperId": "f35016b3180808fa97d59acbdecf47d6e2ed2819", "title": "Rethinking Vision Transformers for MobileNet Size and Speed"}, {"paperId": "8b87d39baf53d982bad7df8ab6c5c8e67c124c67", "title": "NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers"}, {"paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"}, {"paperId": "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5", "title": "BiViT: Extremely Compressed Binary Vision Transformers"}, {"paperId": "e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9", "title": "LAION-5B: An open large-scale dataset for training next generation image-text models"}, {"paperId": "85e959eef45114974c8f8643e88af23936fff3d1", "title": "DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation"}, {"paperId": "3137c3ae4d21cf19d1bec8648f5f2935b5a3378e", "title": "CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models"}, {"paperId": "dfdb2894d50e095ce97f994ed6cee38554c4c84f", "title": "Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer"}, {"paperId": "b5e71069f091d52f474a2928ed07b6546157af82", "title": "Towards Accurate Post-Training Quantization for Vision Transformer"}, {"paperId": "d3135733aa39dec20ce72aa138589dda27c8406d", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"}, {"paperId": "f27c847e2909f30745f4a3528b574f5acfd76ea7", "title": "Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization"}, {"paperId": "2fe71acc2c3f1e75b6149dea72838f0b594ad013", "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers"}, {"paperId": "d451901a6a12c61179289cac7a4588a86c234112", "title": "Width & Depth Pruning for Vision Transformers"}, {"paperId": "47a67e76ed84260ff19f7a948d764005d1edf1c9", "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"}, {"paperId": "dd1139cfc609c2f3263d02e97176d5275caebc0a", "title": "EfficientFormer: Vision Transformers at MobileNet Speed"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "3a1dbfb6875bfac8251627d60db313623fbb8b04", "title": "DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers"}, {"paperId": "58c486ad4020177f5ed3d9f2883f3fc327b55770", "title": "MiniViT: Compressing Vision Transformers with Weight Multiplexing"}, {"paperId": "dae903091d2c5f82c4595e53e8144ccce93f8fb9", "title": "SepViT: Separable Vision Transformer"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "28ed0086dd0f51a8965f7e952b6ee933cdf44179", "title": "Training-free Transformer Architecture Search"}, {"paperId": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a", "title": "Unified Visual Transformer Compression"}, {"paperId": "202967f77c4384bce80eaf2fa5737259008267d3", "title": "Learning to Merge Tokens in Vision Transformers"}, {"paperId": "1b3e38bf2e42ede4aeab2f6fde8f4103562a53f6", "title": "TerViT: An Efficient Ternary Vision Transformer"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "5ab70d95ca49702a3dd49b39d9396d8136b52311", "title": "Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space"}, {"paperId": "722d71a19e4049b30a03d1028158881560432135", "title": "SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning"}, {"paperId": "8144ca1f78c045cb001815090bcf8a726e37e0ad", "title": "Adaptive Token Sampling for Efficient Vision Transformers"}, {"paperId": "39a620939887c9fc1f9bdd7ecfabde985a4aad3a", "title": "PTQ4ViT: Post-training Quantization for Vision Transformers with Twin Uniform Quantization"}, {"paperId": "b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df", "title": "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "cbb9446dcb53bb5efda262942f7c7b0f5b3b7195", "title": "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP"}, {"paperId": "a5c41f188b0eb0acb444cb4899bf6af378ee9ede", "title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"}, {"paperId": "c723187a2230749b1e706df2217e928c8271a660", "title": "Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation"}, {"paperId": "d645bd08fc19d52164695f9cd5ae863345459a06", "title": "AutoFormer: Searching Transformers for Visual Recognition"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "e43eaeca5077d01061a38aebd24f8e3fa5948ad9", "title": "Co-advise: Cross Inductive Bias Distillation"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "efbe9f591090018f78b42c84613c8afda9292fdb", "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration"}, {"paperId": "33fd56e5067a1e8a9713378af3e1c1c08d5ce93b", "title": "Patch Slimming for Efficient Vision Transformers"}, {"paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"}, {"paperId": "93efaf8c27940aaef145d8bcbca957be634d26e5", "title": "Vision Transformer Pruning"}, {"paperId": "4b06c7e29280b1c6bc05c9df39023b48fef02c93", "title": "Escaping the Big Data Paradigm with Compact Transformers"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "394be105b87e9bfe72c20efe6338de10604e1a11", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"}, {"paperId": "c4708542be64861d140ba95dd92099cd8d604cd7", "title": "Big Self-Supervised Models Advance Medical Image Classification"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "b40bfcf339de3f0dba08fabb2b58b9368ff4c51a", "title": "DocVQA: A Dataset for VQA on Document Images"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "33eadd4e666a894306a22ba0839c5e0cef77280e", "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "f902a64f7d08aaa6bfca7463e8729952ddc6134e", "title": "LVIS: A Dataset for Large Vocabulary Instance Segmentation"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907", "title": "Towards VQA Models That Can Read"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "e65142010431ffc089b272a1174214e00693e503", "title": "Generation and Comprehension of Unambiguous Object Descriptions"}, {"paperId": "51fa7c573fcc98b05c2d15685d64463c40d57cff", "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature"}, {"paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "92c141447f51b6732242376164ff961e464731c8", "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": "5fd1b67482e4b9fe70b480c0fd0467900d6db436", "title": "PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile"}, {"paperId": "5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a", "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": "b8a919f4a2aaa97bef19aa43e01f8bc347693b73", "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training"}, {"paperId": "2f0df7d27b28c80d5b389f30d8d243d62b76bc48", "title": "Mixture-of-Experts"}, {"paperId": null, "title": "Set trans-former: A framework for attention-based permutation-invariant neural networks"}, {"paperId": null, "title": "Mobilevlm: A fast, reproducible and strong vision language assistant for mobile devices"}, {"paperId": null, "title": "Minicpm-v 2.0: An efficient end-side mllm with strong ocr and understanding capabilities"}, {"paperId": null, "title": "Multi-modal alignment-guided dynamic token pruning for accelerating vision-language"}, {"paperId": null, "title": "tiny vision language model"}, {"paperId": null, "title": ": Exploring object-level perception in videos"}, {"paperId": null, "title": ": A comprehensive evaluation benchmark for multimodal large language"}, {"paperId": null, "title": "Modularization empowers"}, {"paperId": null, "title": "small multimodal models to bridge biomedical competency gap"}, {"paperId": null, "title": "mplug-2: A modular-ized multi-modal foundation model across text"}, {"paperId": null, "title": "Adapting mixture of vision experts to multimodal"}, {"paperId": null, "title": "an lmm perceiving any aspect ratio"}]}