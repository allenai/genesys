{"paperId": "2475b38a76a9c2dc67f74446e2e686815764b0f2", "title": "EcoFormer: Energy-Saving Attention with Linear Complexity", "abstract": "Transformer is a transformative framework that models sequential data and has achieved remarkable performance on a wide range of tasks, but with high computational and energy cost. To improve its efficiency, a popular choice is to compress the models via binarization which constrains the floating-point values into binary ones to save resource consumption owing to cheap bitwise operations significantly. However, existing binarization methods only aim at minimizing the information loss for the input distribution statistically, while ignoring the pairwise similarity modeling at the core of the attention. To this end, we propose a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. The kernelized hash functions are learned to match the ground-truth similarity relations extracted from the attention map in a self-supervised way. Based on the equivalence between the inner product of binary codes and the Hamming distance as well as the associative property of matrix multiplication, we can approximate the attention in linear complexity by expressing it as a dot-product of binary codes. Moreover, the compact binary representations of queries and keys enable us to replace most of the expensive multiply-accumulate operations in attention with simple accumulations to save considerable on-chip energy footprint on edge devices. Extensive experiments on both vision and language tasks show that EcoFormer consistently achieves comparable performance with standard attentions while consuming much fewer resources. For example, based on PVTv2-B0 and ImageNet-1K, Ecoformer achieves a 73% on-chip energy footprint reduction with only a 0.33% performance drop compared to the standard attention. Code is available at https://github.com/ziplab/EcoFormer.", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 19, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2209.09004", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space and enables most of the expensive multiply-accumulate operations in attention with simple accumulations to save considerable on-chip energy footprint on edge devices."}, "embedding": {"model": "specter_v2", "vector": [0.3414050042629242, 0.4542927145957947, -0.2727539539337158, -0.14285063743591309, -0.59747314453125, 0.4342363178730011, 0.5175520777702332, 0.14888936281204224, -0.613208532333374, -0.7346497178077698, 0.3201170265674591, 0.562847375869751, 0.598983883857727, -0.022953497245907784, -0.1795329451560974, 0.1513267457485199, -0.6365576982498169, 0.1639994978904724, 0.3798534870147705, -0.38219553232192993, 0.14316704869270325, -0.7883846163749695, -1.2999516725540161, 0.018376260995864868, 0.07484892755746841, 1.3573753833770752, 0.46671849489212036, 0.848228931427002, -0.1646038293838501, 0.15644598007202148, 0.49563419818878174, -0.41447919607162476, 0.20920920372009277, 0.08955543488264084, -0.49309754371643066, -0.4785870611667633, 0.3715910315513611, -0.22986437380313873, -0.674293577671051, 1.0607331991195679, -0.07468800991773605, 0.46368059515953064, 0.600836455821991, -0.6497118473052979, -0.47383642196655273, 0.2957160770893097, 0.7104277610778809, 0.7920846343040466, -0.5580154657363892, -0.8123652935028076, 1.539180040359497, -1.5226751565933228, -0.3130001425743103, 1.1989898681640625, 0.3098784387111664, 0.14237625896930695, -0.2911226451396942, -0.4509827792644501, 0.44672441482543945, 0.6662418246269226, -0.9161285161972046, -0.4737057089805603, -0.33132439851760864, 0.021677212789654732, 2.0526115894317627, -0.2905319333076477, 0.23838186264038086, 0.39590969681739807, 0.35698050260543823, 1.1757450103759766, -0.3289909362792969, -0.6903397440910339, -0.042612574994564056, 0.04664260149002075, 0.5790458917617798, 0.7673535346984863, -0.4817044734954834, 0.1891893893480301, -1.0650876760482788, -0.21694272756576538, 0.025146622210741043, 0.5334372520446777, 0.6304982304573059, -0.3891952633857727, -0.26087072491645813, 0.8828275799751282, 0.8003039360046387, 0.4611927568912506, -0.2797207236289978, 1.0424138307571411, 0.5329330563545227, -0.16713359951972961, -0.3156382143497467, 0.1895342469215393, 0.36650583148002625, 0.070851631462574, -0.9522965550422668, 0.05646984651684761, -0.1818501353263855, 1.027262568473816, -0.040895018726587296, 0.2318788468837738, -0.5544227957725525, 0.12924568355083466, 1.2959961891174316, 0.5648277997970581, 0.6200770139694214, -0.14423032104969025, -0.06070128455758095, -0.9781932234764099, -0.22697210311889648, -1.143574833869934, 0.2725990116596222, 0.06835928559303284, -0.8147862553596497, -1.1942287683486938, -1.068827509880066, 0.778459906578064, -1.1020629405975342, 0.29178309440612793, -0.6106144785881042, 0.08933571726083755, -0.2804897129535675, 0.22934070229530334, 0.667089581489563, 0.6686024069786072, 0.18079142272472382, 0.5876296162605286, 1.3607627153396606, -1.0561672449111938, -0.447324275970459, -1.0239648818969727, 0.10856615006923676, -0.3061352074146271, 0.1506519615650177, -0.13898342847824097, -1.2805876731872559, -1.3898704051971436, -0.7333737015724182, -0.3523993194103241, -0.9993831515312195, 0.0459565669298172, 0.6983609795570374, 0.39707762002944946, -1.089484691619873, 0.7056034207344055, -0.7187371850013733, -0.22766771912574768, 0.6644888520240784, 0.3258126676082611, 0.5611323118209839, -0.12051974982023239, -1.419135570526123, 0.39277368783950806, 0.1976548731327057, -0.5991541147232056, -0.365867555141449, -0.3239218592643738, -1.2264926433563232, 0.4626655578613281, -0.07116208225488663, -0.18679895997047424, 0.9854447841644287, -0.12179107218980789, -0.8517950177192688, 0.5736228823661804, -0.5236017107963562, -0.057541146874427795, -0.41524162888526917, -0.3816620409488678, -0.2683476209640503, -0.4997859299182892, -0.3707168400287628, 0.6556517481803894, 0.9286697506904602, 0.17307713627815247, -0.11349140852689743, 0.379986971616745, -0.5565854907035828, -0.015417213551700115, -0.9211546182632446, 0.976990282535553, -0.7181206345558167, -0.5278526544570923, 0.2555291950702667, 0.5289904475212097, 0.018751448020339012, -0.1034054085612297, -0.23254871368408203, -0.9450541734695435, 0.648658275604248, 0.1687905341386795, 0.8009610772132874, -1.1336628198623657, -0.7566681504249573, 0.1812252402305603, 0.17069274187088013, -0.10737640410661697, -0.5711813569068909, 0.5120647549629211, -0.6639549732208252, -0.14714139699935913, 0.09009795635938644, -1.081329345703125, 0.3828095495700836, -0.48899737000465393, -0.7283036708831787, 0.009813865646719933, 0.1545485258102417, 1.401053547859192, -0.7685039639472961, 0.021759742870926857, -0.207784503698349, 0.4910254180431366, -0.9402727484703064, 1.3143706321716309, -0.06216498091816902, -0.8008310198783875, 0.10068528354167938, 0.23608431220054626, 0.02203420177102089, -0.5633499622344971, 0.23216409981250763, -0.9368711709976196, 0.3290821611881256, 0.4604986310005188, -0.014305737800896168, 1.3422620296478271, -0.06436558067798615, 0.7782918214797974, -0.6058066487312317, -0.9824703335762024, 0.5152219533920288, -0.0657610222697258, -0.07466116547584534, -0.8912609219551086, 0.5608006715774536, -0.03750970959663391, -0.6526260375976562, 0.309999018907547, 1.0998866558074951, 0.9960978031158447, -0.6236679553985596, 0.02730657160282135, 0.6041639447212219, -0.10956697911024094, -0.27902087569236755, 0.4423608183860779, 0.6621941924095154, 0.006547336932271719, 0.7220003604888916, -0.0936862975358963, 0.014248804189264774, -1.066847562789917, -0.17320293188095093, 0.7485389709472656, 0.4451577961444855, 0.8794040083885193, -0.05733463540673256, -0.7653628587722778, -0.4728926420211792, -0.056532327085733414, 0.5485214591026306, 1.5438551902770996, 0.2904529571533203, -0.3235341012477875, -0.5002550482749939, -0.22587864100933075, -0.3404850661754608, -0.4332972764968872, -0.49830949306488037, -0.6972808837890625, 0.08987588435411453, -1.2130084037780762, 0.5352669954299927, 0.5868611335754395, 0.875791072845459, -0.5872107148170471, -0.6294153332710266, -0.36912816762924194, -0.05949026718735695, -0.778013288974762, -0.9535110592842102, 0.8705272674560547, 0.0648605078458786, 0.34192997217178345, 0.13468140363693237, 0.03034348413348198, 0.30193522572517395, -0.4352651536464691, 0.9582430720329285, -0.8136669993400574, -0.7470805644989014, 0.1799326241016388, 0.06433263421058655, -0.797048807144165, -0.11069482564926147, 0.29463526606559753, -0.005917602218687534, 0.05213499069213867, 0.8521347641944885, 0.2614309787750244, -0.13902296125888824, 0.10585012286901474, -0.29709020256996155, -0.22561122477054596, 0.18698105216026306, 0.23582863807678223, 0.8321974277496338, -0.4930517375469208, -0.3148348927497864, -0.5801966190338135, 0.294987291097641, -0.24973073601722717, -0.20546388626098633, -0.009663734585046768, -0.6934663653373718, -0.36354872584342957, 0.41298532485961914, -0.6716153025627136, 0.14218619465827942, -0.3476865291595459, 0.33139169216156006, -0.803050696849823, -0.07528706640005112, 0.08463981002569199, 0.2997094988822937, -0.13983681797981262, 0.5485450029373169, 1.0075165033340454, 0.17421554028987885, 0.34257394075393677, 0.2939148247241974, -0.8661646842956543, 0.7903223037719727, 0.2096080183982849, 0.24389441311359406, 0.09988275170326233, -0.22438247501850128, -0.5820238590240479, -0.1492309868335724, -0.5980203747749329, -0.2458728402853012, -0.22350366413593292, 0.3931039869785309, -0.590593695640564, -1.0787607431411743, -0.22390151023864746, -1.0331271886825562, 0.0035577253438532352, 0.1497744768857956, -0.1906297504901886, -0.41642287373542786, -0.8569684028625488, -0.7456917762756348, -0.47504889965057373, -0.8585768938064575, -1.070573091506958, 0.22598925232887268, 0.1893264502286911, -0.395432710647583, -0.3288486897945404, -0.19877518713474274, -0.4154638648033142, 1.3056113719940186, -0.6638333201408386, 0.4767169654369354, -0.47026363015174866, -0.30038851499557495, -0.44407129287719727, -0.20220114290714264, 0.6171211004257202, -0.15499475598335266, -0.1796267330646515, -1.1488522291183472, 0.16406504809856415, -0.4681779146194458, -0.3947221636772156, 0.49195361137390137, 0.2331715226173401, 1.137271761894226, 0.29075103998184204, -0.6266772150993347, 0.48317986726760864, 1.4428961277008057, -0.3671119809150696, 0.2961241602897644, -0.1426325887441635, 1.037912130355835, -0.18618188798427582, -0.04546941816806793, 0.8093799352645874, 0.5562793612480164, 0.7874643206596375, 0.3734378218650818, -0.3530079126358032, -0.1275557279586792, -0.11131364107131958, 0.5716482996940613, 1.3934367895126343, 0.38970357179641724, 0.22859108448028564, -1.0708650350570679, 1.0248113870620728, -1.3910980224609375, -0.746464192867279, 0.9053102135658264, 0.7262337803840637, 0.32356294989585876, -0.3196064531803131, -0.13499020040035248, -0.5246871709823608, 0.2332644909620285, 0.2937016189098358, -0.4060212969779968, -0.6591742634773254, 0.0654342845082283, 0.8603410720825195, 0.40388306975364685, 0.3665034770965576, -0.5403594970703125, 0.34995418787002563, 14.656871795654297, 1.0369361639022827, -0.38929253816604614, 0.7339683175086975, 0.6666743159294128, 0.1791117787361145, -0.01220451295375824, -0.04425980523228645, -1.1371554136276245, 0.16418297588825226, 1.3846161365509033, 0.089659683406353, 0.41359880566596985, 0.5014721155166626, -0.4854019284248352, 0.34223249554634094, -0.9756439924240112, 1.1643126010894775, 0.9927383065223694, -1.1372443437576294, -0.19267326593399048, 0.31832408905029297, -0.05869350954890251, 0.6227659583091736, 0.9036465883255005, 0.6532921195030212, 0.4572557210922241, -0.6267192959785461, 0.6616359353065491, 0.7000149488449097, 0.9421889781951904, 0.19182631373405457, 0.32687363028526306, -0.011279675178229809, -1.3447623252868652, -0.1296866089105606, -0.9367117881774902, -0.709041178226471, 0.004339406732469797, 0.1577456146478653, -0.47298407554626465, -0.43260976672172546, 0.3389364778995514, 0.7530220746994019, 0.12619376182556152, 0.4411364495754242, 0.5229665637016296, 0.13860644400119781, -0.06678435951471329, -0.2784152626991272, -0.02310025505721569, 0.9777631759643555, -0.20707820355892181, 0.33778440952301025, -0.10270274430513382, -0.07213887572288513, 0.2525022327899933, 0.5166004300117493, -0.3776368498802185, -0.1325458437204361, -0.07881178706884384, -0.06548678874969482, 0.03781232610344887, 0.9970795512199402, 0.3123093247413635, 0.112461157143116, -0.6102775931358337, 0.5849144458770752, 0.41668009757995605, -0.08357496559619904, -0.8175770044326782, -0.5884467363357544, 0.40333396196365356, -0.7082621455192566, 0.5453983545303345, 0.8025699853897095, -0.230990469455719, -0.44932258129119873, -0.6728315949440002, -0.23836155235767365, 0.4713500440120697, -0.6908807754516602, -0.805748462677002, 1.0673129558563232, -0.2650599777698517, -0.18448932468891144, 0.5315452218055725, -0.7013077139854431, -0.17781423032283783, 0.6286876797676086, -1.4037188291549683, -0.35201549530029297, -0.23849762976169586, -0.17239680886268616, -0.35978707671165466, -0.11811060458421707, 1.3950830698013306, 0.49529027938842773, 0.06280554085969925, 0.5471788644790649, -0.20323163270950317, -0.08764463663101196, 0.07547200471162796, -0.5842798948287964, 0.9660680890083313, 0.09244414418935776, -0.3122560679912567, 0.1964779943227768, -0.07240399718284607, 0.19252707064151764, -0.586179792881012, -0.1761765331029892, 0.7411237955093384, -0.7921321392059326, -0.3439667224884033, -1.046120285987854, -0.7981683611869812, -0.022380467504262924, 0.4155192971229553, 0.1834346354007721, 0.4484706521034241, -0.049161843955516815, -0.7494584918022156, -0.5643123388290405, -0.47116535902023315, -0.25629517436027527, 0.09721489250659943, -1.0822296142578125, -0.3722050189971924, -0.10343070328235626, 0.11774927377700806, -0.8081648349761963, -0.7686814665794373, -0.3718160092830658, 0.3627413809299469, 0.016211483627557755, 1.5158729553222656, -0.24232888221740723, 0.8756385445594788, 0.9481183290481567, -0.4823310375213623, -0.46843135356903076, -0.2666206955909729, -0.49892741441726685, -0.25456762313842773, -0.23361772298812866, 0.2784479856491089, -0.2687572240829468, 0.42088583111763, 0.8284865617752075, 0.34838324785232544, -0.5503068566322327, -0.503571093082428, -0.06462650001049042, -0.44828566908836365, -0.4802178144454956, 0.15574203431606293, -0.02062390372157097, 0.1817052960395813, -0.20989291369915009, 0.4180796146392822, 0.31527283787727356, -0.054800231009721756, -0.5699889659881592, 0.13138185441493988, -0.2757831811904907, -0.23812822997570038, -0.3622837960720062, -1.0439218282699585, -1.4188530445098877, -0.09424445033073425, -1.1320126056671143, 0.40638843178749084, -0.38077589869499207, -0.08078356832265854, 0.03159894049167633, -0.6312934756278992, 0.4014987051486969, 0.12768226861953735, -0.0419340580701828, -0.13770338892936707, -0.5531090497970581, -0.6134361028671265, 1.0563846826553345, 0.3536950647830963, -0.7105740308761597, 0.43548426032066345, -0.29513832926750183, -0.13632380962371826, -0.06234311684966087, 0.20906810462474823, -0.6220996379852295, -0.6154000759124756, -0.8277086019515991, 0.2683504521846771, -0.16562184691429138, 0.22323237359523773, -0.9354954957962036, 1.330070972442627, 0.34457603096961975, -0.08658453077077866, -0.022162016481161118, 0.7596079111099243, -1.0873539447784424, -0.6118422746658325, 0.6442461013793945, -0.8312853574752808, 0.24290774762630463, 0.21329303085803986, -0.5689558982849121, -0.330907940864563, 0.7710899710655212, 0.12450212985277176, -0.6861013770103455, -0.7667616605758667, 0.5942932963371277, -0.3102644383907318, 0.003910114523023367, -0.16069434583187103, -0.17547397315502167, -1.2839994430541992, -0.5282825827598572, -0.21052874624729156, 0.04693344607949257, -0.7383798956871033, 0.8493203520774841, 0.8752160668373108, -1.4261173009872437, 0.2785171866416931, 0.4371821880340576, 0.16308529675006866, -0.0572257898747921, 0.40721216797828674, 0.6433142423629761, 0.02246389165520668, 0.5533928275108337, -0.18160413205623627, 0.35830727219581604, -0.952235996723175, 0.3069625198841095, 0.7938762307167053, -0.04297022521495819, 0.15443524718284607, 1.2088725566864014, -0.1072438657283783, -0.5942245125770569, 0.2914196252822876, -1.0810362100601196, -0.22964972257614136, -0.06633306294679642, 0.3960531949996948, 0.011193453334271908, 0.3719443380832672, -0.18798840045928955, -0.8296945095062256, -0.005279319826513529, -0.42631590366363525, -0.30131521821022034, 0.4250529110431671, 0.11875209212303162, -0.42234307527542114, 0.2893839180469513, 1.1853965520858765, -1.1761966943740845, -0.5222364068031311, -1.1639126539230347, -0.6459599733352661, -0.2744913101196289, 0.40133532881736755, 0.09876466542482376, -0.8345062136650085, 0.7267228364944458, 0.8723975419998169, 0.41682884097099304, 0.3645474910736084, 0.00806580763310194, 0.23845797777175903, 0.8645162582397461, -0.3471703827381134, -0.5938106775283813, -0.2408321499824524, 1.4041309356689453, 0.9793215394020081, -0.7633996605873108, 0.06694457679986954, -0.31939995288848877, -0.49968037009239197, 0.7454481720924377, 0.40285348892211914, -0.281753271818161, 1.3814703226089478, 0.08920750021934509, -0.37438830733299255, 0.11052770167589188, -0.9868762493133545, -0.3666878938674927, 1.2688775062561035, 1.363547682762146, 0.34577563405036926, 0.14971964061260223, 0.5001939535140991, 0.8836766481399536, 0.2170412391424179, -0.29028505086898804, 0.1066400483250618, 0.4070073068141937, -0.29218074679374695, 0.07471747696399689, -0.2649439871311188, 0.730343759059906, -0.5116320252418518, -0.9176459312438965, 0.46988075971603394, 0.004086365457624197, 0.05629272758960724, 0.2185783088207245, 1.1618424654006958, -0.3657991886138916, 0.5622766613960266, 0.07327407598495483, 0.21814900636672974, -0.4898501932621002, -0.0839771255850792, -0.3940886855125427, -1.1037788391113281, -0.26890987157821655, -0.15378549695014954, -0.6453417539596558, -0.2756175696849823, -0.15833178162574768, 0.4216817617416382, -0.37408098578453064, 0.3713846206665039, 0.9025649428367615, 0.6434515118598938, 0.7860695123672485, -0.09884054958820343, -0.6815906167030334, -0.3000345826148987, -0.703149676322937, 0.19463388621807098, -0.4867528975009918, -0.06294231116771698, -0.2905880808830261, 0.22730804979801178, 0.14205709099769592]}, "authors": [{"authorId": "49270464", "name": "Jing Liu"}, {"authorId": "1840579673", "name": "Zizheng Pan"}, {"authorId": "11270586", "name": "Haoyu He"}, {"authorId": "2152629962", "name": "Jianfei Cai"}, {"authorId": "3194022", "name": "Bohan Zhuang"}], "references": [{"paperId": "fb2307f7ce7c6868429ee3ee15d6eaf311ecba5c", "title": "BiBERT: Accurate Fully Binarized BERT"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "c0f709acf38eb27702b0fbce1215db0ebaa2de2b", "title": "SMYRF: Efficient Attention using Asymmetric Clustering"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "ed9aef35fd6b96f0f357d703143c0941331b5968", "title": "Training Binary Neural Networks with Real-to-Binary Convolutions"}, {"paperId": "8f8ccef969a3c1b0f2b2448e07b714214f245f6b", "title": "Deep Multi-View Enhancement Hashing for Image Retrieval"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "70f04285f1af8b0dc41a9339f2512ae6dcc41266", "title": "AdderNet: Do We Really Need Multiplications in Deep Learning?"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "e61b99889e6f811b5c2657a534ad0de468c545a9", "title": "XNOR-Net++: Improved binary neural networks"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "1f7723bfb6eaf8375b0cb523ff59690d853902cd", "title": "High-Performance FPGA-Based CNN Accelerator With Block-Floating-Point Arithmetic"}, {"paperId": "0cdef47d342cae516f67efd6e4991f4825043b70", "title": "Regularizing Activation Distribution for Training Binarized Deep Networks"}, {"paperId": "dc160709bbe528b506a37ead334f60d258413357", "title": "Learned Step Size Quantization"}, {"paperId": "78e1c397fe469ae4dfce3770c8352c397d63fc43", "title": "Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation"}, {"paperId": "7c42d7ff616efc45a42b264b0da6c74e8141a9ed", "title": "ProxQuant: Quantized Neural Networks via Proximal Operators"}, {"paperId": "3cd3f1585ced02cbb56a9e1428176a6c2b211da2", "title": "Learning to Quantize Deep Networks by Optimizing Quantization Intervals With Task Loss"}, {"paperId": "4bd23d951846832bdf550df574cdec07bc08dec1", "title": "Bi-Real Net: Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "d18cc16563b9e2cef58bbc2a9d212b0ca72de36c", "title": "Loss-aware Weight Quantization of Deep Networks"}, {"paperId": "69e220145b5a7886f9c92da8a253b6cc97181f57", "title": "Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Network"}, {"paperId": "8eecf4cf29de06c754f282cf0cb603aa369e6ba1", "title": "Towards Accurate Binary Convolutional Neural Network"}, {"paperId": "7b8d67593c4ab1b1e3eccc158daee76703b328aa", "title": "Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "8b053389eb8c18c61b84d7e59a95cb7e13f205b7", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"}, {"paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7", "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"}, {"paperId": "011e6146995d5d63c852bd776f782cc6f6e11b7b", "title": "Fast Training of Triplet-Based Deep Binary Embedding Networks"}, {"paperId": "2e2b189f668cf2c06ebc44dc9b166648256cf457", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network"}, {"paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations"}, {"paperId": "50645e3dc912d597e89d59bffb96ccc0f8e1aefa", "title": "Practical and Optimal LSH for Angular Distance"}, {"paperId": "67a2a783ef4bda774876c37bf9826341e33017f9", "title": "Discrete Graph Hashing"}, {"paperId": "11ccebed975bedf09b4893d7e3c31be897a1f527", "title": "Fast Supervised Hashing with Decision Trees for High-Dimensional Data"}, {"paperId": "947620a1854655ed91a86b90d12695e05be85983", "title": "1.1 Computing's energy problem (and what we can do about it)"}, {"paperId": "3832057ac487f43e885cdb485a6ca1462834bb8d", "title": "Estimating or Propagating Gradients Through Stochastic Neurons"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "80953468bd75833b755e7e4c25abe009dfd84cf0", "title": "Semi-Supervised Hashing for Large-Scale Search"}, {"paperId": "55a62990a82bf9205e507f73d42583df46062806", "title": "A new approach to interdomain routing based on secure multi-party computation"}, {"paperId": "88ed6b8d4209bb0e113b1fc77a4924f79e412a0e", "title": "Supervised hashing with kernels"}, {"paperId": "87a99f1e27c8f1a13f5c1971f4c38b0fa59456ef", "title": "Kernelized Locality-Sensitive Hashing"}, {"paperId": "d8e4936a405bdf58830be2559e9a39e533a0ed42", "title": "Hashing with Graphs"}, {"paperId": "5ff8c048a042d37a99a9f9c5acb0d82972a45ad0", "title": "Minimal Loss Hashing for Compact Binary Codes"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "3f1e54ed3bd801766e1897d53a9fc962524dd3c2", "title": "Locality-sensitive hashing scheme based on p-stable distributions"}, {"paperId": "42bb0ac384fb87933be67f63b98d90a45d2fe6e9", "title": "Similarity estimation techniques from rounding algorithms"}, {"paperId": "2e74388f55f2cc704c4de410578887a53a9433b0", "title": "Similarity Search in High Dimensions via Hashing"}, {"paperId": "1076a9b8181a7f9eb069d38ca10876a3202d2e89", "title": "Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization"}, {"paperId": "c49c292e1fb1d215c88828a52134b7ccfa52be44", "title": "Sparse Attention with Learning to Hash"}, {"paperId": "17eaf8f5d53407baa3c8ce3bdf5cc1a3503182eb", "title": "Adder Attention for Vision Transformer"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "28135fd3e80dda50a673cd556f10b9b972005d27", "title": "Binarized Neural Networks"}, {"paperId": "f447d9de5b62d40345a5da90c60bbc47c967baab", "title": "Ieee Transaction on Pattern Analysis and Machine Intelligence 1 Iterative Quantization: a Procrustean Approach to Learning Binary Codes for Large-scale Image Retrieval"}, {"paperId": null, "title": "Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content"}, {"paperId": null, "title": "Did you discuss any potential negative societal impacts of your work?"}, {"paperId": null, "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating?"}, {"paperId": null, "title": "Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?"}, {"paperId": null, "title": "Have you read the ethics review guidelines and ensured that your paper conforms to them"}, {"paperId": null, "title": "code, data, models) or curating/releasing new assets... (a) If your work uses existing assets"}, {"paperId": null, "title": "(a) Did you state the full set of assumptions of all theoretical results?"}, {"paperId": null, "title": "(a) Did you include the code, data, and instructions needed to reproduce the main experimental results"}, {"paperId": null, "title": "(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Error bars are not reported because it would be too computationally expensive"}, {"paperId": null, "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]"}, {"paperId": null, "title": "c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?"}, {"paperId": null, "title": "b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?"}]}