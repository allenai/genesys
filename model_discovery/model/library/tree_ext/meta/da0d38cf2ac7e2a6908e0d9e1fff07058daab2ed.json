{"paperId": "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed", "title": "Sparse is Enough in Scaling Transformers", "abstract": "Large Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.", "venue": "Neural Information Processing Systems", "year": 2021, "citationCount": 83, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as the authors scale up the model size."}, "embedding": {"model": "specter_v2", "vector": [0.4497902989387512, 0.7527130842208862, -0.4343961179256439, -0.11511119455099106, -0.46684882044792175, -0.2496585100889206, 0.8692848086357117, -0.07102324813604355, 0.006447961088269949, -0.18148468434810638, 1.1050981283187866, 0.21040286123752594, 0.3273930847644806, 0.11091439425945282, -0.1343904286623001, 0.18622608482837677, -0.6154419779777527, 0.3470532298088074, 0.0662190318107605, -0.5923675894737244, 0.008280924521386623, -0.8738341927528381, -0.6041844487190247, 0.11789125204086304, 0.6272708177566528, 0.4338661730289459, 0.3129863739013672, 0.7446880340576172, -0.3975996673107147, 0.7861860394477844, 0.18436285853385925, -0.34192973375320435, -0.08635588735342026, -0.30794206261634827, -0.23494163155555725, -0.19677291810512543, 0.7041530013084412, -0.35660552978515625, -0.47185438871383667, 0.7616481184959412, -0.25653502345085144, -0.10248762369155884, 0.3572930097579956, -0.6998345255851746, -0.1293732225894928, 1.1732118129730225, 0.5330476760864258, 0.8130858540534973, -0.004501132760196924, -0.6769481301307678, 1.617049217224121, -1.090947151184082, 0.022139044478535652, 1.5269044637680054, 0.5826929807662964, 0.24037547409534454, -0.07713660597801208, -0.5917532444000244, 1.1038951873779297, 0.3434326946735382, -0.6588785648345947, -0.5611897706985474, 0.001165464404039085, 0.18878138065338135, 2.1129262447357178, -0.3610921800136566, 0.29831841588020325, 0.6078970432281494, -0.23008859157562256, 1.436123013496399, -0.3396466374397278, -0.2989237904548645, -0.43900439143180847, -0.26193690299987793, 0.36035165190696716, 0.8027437329292297, -0.2837260365486145, -0.31682124733924866, -1.2631263732910156, 0.00431551318615675, 0.1687292754650116, -0.1612524539232254, -0.18647484481334686, 0.15639472007751465, -0.29908138513565063, 0.6256837844848633, 0.36150503158569336, 0.8524844646453857, -0.3210584819316864, 0.7113683819770813, 0.5986331105232239, 0.2943015396595001, 0.17427349090576172, 0.32270804047584534, -0.11882515996694565, 0.2874300479888916, -1.2032114267349243, 0.017080308869481087, 0.1887010931968689, 0.7864352464675903, -0.48721104860305786, 0.4208028018474579, -0.8987893462181091, 0.19913363456726074, 1.096214771270752, 0.4173901081085205, 0.42440274357795715, -0.8256620168685913, 0.49076640605926514, -0.5357670187950134, 0.0424107126891613, -0.8473203778266907, -0.24056880176067352, -0.39836353063583374, -1.0413217544555664, -1.444652795791626, -0.5705586671829224, 0.2954098880290985, -0.3235446810722351, 0.9240582585334778, -0.5202991962432861, 0.20791511237621307, -0.1213788390159607, 0.12913227081298828, 0.6357782483100891, 0.9836698174476624, 0.10833635181188583, -0.3828778862953186, 0.7745497822761536, -0.9835031628608704, -1.025299072265625, -1.2856956720352173, 0.5714111924171448, -0.2530142664909363, 0.1652379333972931, 0.4155399799346924, -1.2544498443603516, -0.7335593104362488, -0.8474958539009094, -0.1311180740594864, -0.11513729393482208, 0.15130381286144257, 0.8462674021720886, 0.4206033945083618, -0.9013711810112, 0.6931127309799194, -0.37611162662506104, -0.2904203236103058, 0.6536498665809631, 0.06605271250009537, 0.11204878240823746, -0.33747580647468567, -1.049296259880066, 0.39457598328590393, -0.13186988234519958, -0.60715651512146, -0.07188010960817337, -0.6265263557434082, -1.2116384506225586, 0.20895370841026306, 0.16497491300106049, -0.5468700528144836, 1.0310754776000977, -0.03785805404186249, -1.317908763885498, 0.3311392068862915, -0.8183148503303528, -0.19040711224079132, -0.0021403664723038673, -0.638939619064331, -0.10511259734630585, -0.2059231549501419, -0.018992355093359947, 0.09668487310409546, 0.5800888538360596, 0.39379554986953735, -0.18782150745391846, 0.22963018715381622, -0.625130832195282, -0.23355138301849365, -0.3834492266178131, 1.0968825817108154, -0.23960691690444946, -0.009930082596838474, 0.1746518313884735, 1.0306857824325562, 0.2920927405357361, -0.5904330611228943, -0.5524283647537231, -1.306650996208191, 0.8074280619621277, -0.2807557284832001, 1.3913357257843018, -0.7436080574989319, -0.1602858006954193, -0.4139607846736908, 0.038802988827228546, -0.039384499192237854, -0.867933988571167, 0.7337315678596497, -0.21491284668445587, 0.2945201098918915, -0.1089622974395752, -1.2684434652328491, 0.23677900433540344, -0.21210752427577972, -0.943417489528656, 0.04901577904820442, 0.383510947227478, 1.1047735214233398, -0.780078113079071, 0.011591392569243908, 0.15073511004447937, 0.31920111179351807, -1.1566964387893677, 1.2494349479675293, -0.3620814383029938, 0.02487063594162464, -0.33187976479530334, -0.28932690620422363, -0.11383133381605148, -0.739657461643219, 0.6216694116592407, -0.5427505373954773, 0.02144319750368595, 0.8318494558334351, -0.3778902292251587, 1.258238434791565, -0.22540949285030365, 0.6456426978111267, -0.20756027102470398, -1.2259390354156494, 0.24185152351856232, 0.3081997334957123, -0.1415027529001236, -0.554023027420044, 0.1932285726070404, 0.2737864851951599, -0.829511821269989, 0.3188750743865967, 0.5146698355674744, 0.7987791895866394, -0.49451708793640137, 0.01567879505455494, 0.8848474025726318, -0.31907129287719727, 0.7120211124420166, 0.8905928134918213, 1.0384613275527954, 0.32591933012008667, 0.5014744400978088, 0.18455646932125092, 0.1345214992761612, -0.7652955055236816, 0.10675660520792007, 0.5237773656845093, 0.8071128129959106, 1.0681178569793701, 0.7879073023796082, -0.45277759432792664, -0.6898393630981445, 0.051728539168834686, 0.9172608256340027, 1.3951525688171387, -0.3521408438682556, -0.6411858797073364, -0.7166564464569092, -0.06283769011497498, -0.5621679425239563, 0.20924687385559082, -0.291342169046402, -0.16579872369766235, -0.578501284122467, -0.923831582069397, 0.6282156705856323, 0.236259326338768, 0.9131617546081543, -0.14481249451637268, -0.10486496239900589, -0.3083679974079132, -0.3134939670562744, -1.1083723306655884, -0.4999290108680725, 0.17548468708992004, -0.5987948775291443, -0.40484970808029175, 0.16155527532100677, 0.09323577582836151, -0.28213730454444885, -0.5980145931243896, 1.282610297203064, -0.38307225704193115, 0.20223572850227356, 0.24968859553337097, 0.45828884840011597, -0.5427631735801697, -0.40376797318458557, 0.3985244333744049, 0.16636545956134796, -0.17368581891059875, 0.25367307662963867, 0.1454250067472458, 0.04582120105624199, -0.0685114711523056, -0.1453019678592682, 0.2568970024585724, 0.04027977213263512, 0.08949558436870575, 0.2970200777053833, -0.25289401412010193, 0.19063898921012878, -1.0588703155517578, 0.6643087267875671, -0.040210504084825516, -0.3653603792190552, 0.3069602847099304, -0.3571031987667084, -0.5567200779914856, 0.45530420541763306, -0.5230746269226074, -0.05733593925833702, -1.1198536157608032, 0.32505762577056885, -0.07635369151830673, 0.11397084593772888, 0.20718450844287872, 0.06195928156375885, 0.5610466003417969, 0.08783797919750214, 0.6646968722343445, 0.15974906086921692, -0.22052302956581116, 0.3094767928123474, -0.7831692695617676, 0.5607273578643799, 0.832515299320221, 0.19173036515712738, -0.24788911640644073, -0.13146299123764038, -0.8945080041885376, -0.8801121711730957, -0.266766220331192, 0.013180672191083431, -0.077180415391922, 0.2306998372077942, -0.660293459892273, -0.7658679485321045, -0.04342341795563698, -1.1410578489303589, -0.2381940633058548, 0.05452287569642067, -0.502978503704071, -0.11235977709293365, -0.8186595439910889, -1.2025612592697144, -0.4184523820877075, -0.7941086292266846, -0.6896588802337646, 0.7607515454292297, 0.14086830615997314, -0.5584829449653625, -0.4118489623069763, -0.004003988113254309, -0.23194344341754913, 1.0076498985290527, -0.46512845158576965, 0.5180268883705139, -0.2860123813152313, -0.40881985425949097, -0.3783336579799652, 0.26655080914497375, 0.6290637254714966, -0.4152264893054962, 0.2362671196460724, -0.7074130773544312, 0.196522518992424, -0.38427576422691345, -0.13469663262367249, 0.24921450018882751, 0.5686594843864441, 0.39323464035987854, -0.1645165979862213, -0.4868713915348053, 0.33939850330352783, 1.05666983127594, -1.0951848030090332, 0.04930434376001358, 0.1810871809720993, 1.1075937747955322, 0.06033576279878616, -0.35454559326171875, 0.6887325644493103, 0.36684712767601013, 0.3123694062232971, 0.16930687427520752, 0.15591932833194733, -0.3000050187110901, -0.830862820148468, 0.818404495716095, 1.8301509618759155, 0.3265780210494995, -0.5193243026733398, -0.861716091632843, 0.6141670942306519, -1.3977898359298706, -1.184433102607727, 0.47421324253082275, 0.5739129185676575, 0.23451679944992065, -0.5306108593940735, -0.18731272220611572, 0.0810958668589592, 0.25917649269104004, 0.4333649277687073, -0.3171955645084381, -0.5591033697128296, -0.20211201906204224, 0.635562002658844, 0.19416303932666779, 0.6812171339988708, -0.13112618029117584, 0.5433359146118164, 14.952839851379395, 0.7812239527702332, 0.0066520413383841515, 0.7174391746520996, 0.49199891090393066, -0.25029027462005615, -0.3344822824001312, 0.04890735447406769, -1.3027944564819336, 0.107900470495224, 1.1322858333587646, -0.01244179904460907, 0.6313209533691406, -0.20913377404212952, 0.21920077502727509, 0.08683351427316666, -0.6051144003868103, 0.6450234651565552, 0.758640706539154, -1.244563102722168, 0.5582585334777832, 0.023544007912278175, 0.3208897113800049, 0.5896740555763245, 0.6267493367195129, 0.6574128866195679, 0.28970661759376526, -0.5661917328834534, 0.36569806933403015, 0.3189782500267029, 0.8109666705131531, -0.0728730782866478, 0.4986622631549835, 0.7152257561683655, -0.6813172101974487, -0.36436718702316284, -0.4224931597709656, -1.2859195470809937, 0.6170864105224609, 0.25564175844192505, -0.298956036567688, -0.29507049918174744, -0.07517951726913452, 1.3605196475982666, -0.09556682407855988, 0.25398823618888855, -0.5892640352249146, 1.199203372001648, -0.47634586691856384, -0.004236825741827488, 0.35733261704444885, 0.38698241114616394, 0.2809825539588928, 0.1475331038236618, 0.2709023952484131, -0.019254066050052643, 0.0882113054394722, 0.2321905791759491, -0.530744731426239, -0.10545904189348221, -0.36376479268074036, -0.5245730876922607, 0.09034723788499832, 0.6130565404891968, 0.7108060717582703, 0.05184144899249077, -0.5282934904098511, 0.2881033420562744, 0.41614988446235657, 0.04167298972606659, -0.2624504864215851, -0.03440709412097931, 0.07356340438127518, -0.1397973746061325, 0.13864094018936157, 0.6214658617973328, -0.2364593744277954, -0.8369503617286682, -0.7317347526550293, -0.6091262102127075, 0.58359295129776, -0.9734730124473572, -0.5989008545875549, 0.43125301599502563, -0.1351359486579895, -0.3159767687320709, 0.24629098176956177, -0.3402198553085327, -0.461229532957077, 0.3480433523654938, -1.5446504354476929, -0.9432888031005859, 0.17341038584709167, -0.2655481696128845, 0.0340905636548996, -0.3074239492416382, 0.9507091045379639, -0.03186766058206558, -0.3855482041835785, -0.07528083771467209, -0.07096226513385773, -0.23011089861392975, -0.41936367750167847, -0.9096782803535461, 0.7926951050758362, 0.48582637310028076, -0.035683173686265945, 0.04961422085762024, 0.3818172812461853, 0.40379947423934937, -1.0271079540252686, 0.19962802529335022, 1.149011254310608, -1.0052350759506226, -0.05656157433986664, -0.610569179058075, -0.9094774723052979, 0.5430620908737183, 0.8402664661407471, -0.4701480269432068, 0.343685120344162, 0.21228843927383423, -0.5109319686889648, -0.1045922338962555, -0.6473275423049927, -0.2892730236053467, 0.4522339999675751, -0.5468903183937073, -0.5531991124153137, -0.05551004782319069, 0.35127538442611694, -0.6060366034507751, -0.3271290063858032, -0.20246607065200806, 0.002531715203076601, -0.11941880732774734, 0.8287433981895447, -0.21584950387477875, 0.7587088942527771, 0.8650429248809814, -0.1470504105091095, -0.8060852885246277, -0.18885217607021332, -1.2234231233596802, -0.1966739147901535, 0.34037575125694275, 0.3062242567539215, -0.5060580372810364, 0.08605673909187317, 0.3581511676311493, 0.38048338890075684, -0.15648001432418823, -0.6891257762908936, -0.12844771146774292, 0.0848950520157814, -0.15146739780902863, 0.047901201993227005, -0.11560074239969254, 0.020685072988271713, 0.6984388828277588, 0.2382620871067047, 0.5644358992576599, 0.11882523447275162, -0.9628089666366577, 0.50638347864151, -0.44722822308540344, 0.09724679589271545, -0.8305761814117432, -0.4196988046169281, -1.583970069885254, 0.04382305219769478, -0.8851086497306824, -0.2881554961204529, -1.557843565940857, -0.19871674478054047, 0.6437774896621704, 0.09461808204650879, 0.18994159996509552, 0.3210600018501282, -0.27406349778175354, -0.25688835978507996, -0.6236894726753235, -1.0106316804885864, 0.8070012331008911, 0.8194046020507812, -0.8679990172386169, 0.3717639744281769, -0.2451065480709076, -0.26130303740501404, 0.44118064641952515, 0.47930842638015747, -0.35564935207366943, -0.6597345471382141, -1.2605456113815308, 0.7513560652732849, -0.21282616257667542, -0.16261130571365356, -0.39634594321250916, 0.6905932426452637, 0.5741782188415527, -0.16467535495758057, -0.11304809898138046, 0.41686543822288513, -0.6478671431541443, -0.5805912017822266, 0.1864839494228363, -0.6606458425521851, 0.13222935795783997, 0.11860153079032898, -0.5416527986526489, -0.2660822570323944, 0.7734569311141968, -0.039302729070186615, -0.9000236988067627, -0.34616196155548096, 0.3133648633956909, -0.6304019093513489, 0.42746472358703613, -0.6257364153862, -0.1043248251080513, -1.254011869430542, -0.7270272970199585, 0.14761342108249664, 0.5131990313529968, -0.34835919737815857, 0.902651309967041, 0.13056650757789612, -1.0059378147125244, -0.1818007230758667, 0.1803002506494522, -0.2965680658817291, -0.08753779530525208, 0.3987838625907898, 0.3952406644821167, -0.10439906269311905, 0.33999451994895935, 0.5203648209571838, 0.1336444616317749, -0.923434317111969, -0.2293664962053299, 0.6102975010871887, -0.6139839887619019, -0.3774055540561676, 0.9723091721534729, -0.4571131467819214, -0.6018291115760803, 0.21898826956748962, -1.100617527961731, -0.8266758918762207, -0.06730084121227264, 0.8154209852218628, 0.18864776194095612, -0.190115824341774, 0.05005727335810661, -0.5477588176727295, 0.4295811653137207, 0.24672405421733856, -0.28504273295402527, 1.0617882013320923, -0.16724787652492523, -0.6759428977966309, 0.8133687376976013, 0.6260080933570862, -0.7363700866699219, -0.37543267011642456, -0.8306819200515747, -0.4984704852104187, -0.059168972074985504, 0.5605836510658264, -0.40586116909980774, -0.572895348072052, 0.9081707000732422, 0.32816097140312195, 0.6312062740325928, 0.3714669644832611, -0.008108408190310001, 0.17218993604183197, 0.8871595859527588, -0.10328785330057144, -0.5175836682319641, -0.44890403747558594, 1.443693995475769, 1.2957226037979126, -0.8253148794174194, 0.5014573931694031, -0.08112131059169769, -0.7085374593734741, 0.5087344646453857, -0.06886809319257736, -0.32532742619514465, 0.5524492859840393, -0.003262520534917712, -0.37727832794189453, 0.09105845540761948, -1.319948673248291, -0.12696778774261475, 0.7429131865501404, 0.9867321848869324, 0.9251633882522583, 0.0011777994222939014, -0.023307954892516136, 0.8100528120994568, -0.0004598505038302392, 0.1681017279624939, 0.3866913914680481, 0.13068459928035736, -0.4167008101940155, 0.20058158040046692, 0.2211097776889801, 0.6534518003463745, -0.843429684638977, -0.572097659111023, 0.018086319789290428, 0.2799489200115204, -0.14217253029346466, 0.8430157899856567, 0.7031630873680115, 0.513592541217804, 0.7646726965904236, 0.019089609384536743, 0.810962438583374, -0.6413232088088989, -0.33921268582344055, -0.16778497397899628, -0.5009189248085022, -0.3707236349582672, -0.3274325132369995, -0.6959319114685059, 0.0625058263540268, -0.2779199779033661, 0.27142760157585144, 0.17459489405155182, 0.07999492436647415, 1.2345324754714966, 0.5865375995635986, 0.6318255066871643, -0.2515026330947876, -0.704888641834259, -0.38328874111175537, -1.3345351219177246, 0.036008939146995544, -0.5154151916503906, 0.002757446840405464, -0.14954951405525208, -0.27487412095069885, -0.2712622582912445]}, "authors": [{"authorId": "25898662", "name": "Sebastian Jaszczur"}, {"authorId": "2841893", "name": "Aakanksha Chowdhery"}, {"authorId": "1579862074", "name": "Afroz Mohiuddin"}, {"authorId": "40527594", "name": "Lukasz Kaiser"}, {"authorId": "32861932", "name": "Wojciech Gajewski"}, {"authorId": "47407464", "name": "H. Michalewski"}, {"authorId": "2036077", "name": "Jonni Kanerva"}], "references": [{"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f", "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "96d654dbd6d77c8b3d4fe13ee4111feee4e4fa85", "title": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "48745e3485f84cc5a2dab8e1ce41de0a38afb490", "title": "Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "9baab08fbe37369856688b2abe5b3c90cce1682c", "title": "Compression of Deep Learning Models for Text: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "f46c562229c5bc419bbbfb63239431590e4b340a", "title": "Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69", "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"}, {"paperId": "143b0bd73e7e765945e0b8620f84f52878617560", "title": "Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "01b15017ac59b8d6f2ce3598c4a7d6358c211426", "title": "A Divide-and-Conquer Approach to the Summarization of Long Documents"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f4061bd225b3be5b3f5b18eb1a229ce991efefeb", "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "132ae47905b1a648c095da54b8533e87cf642897", "title": "Fully Quantized Transformer for Machine Translation"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "86027b08a9f42f0f21607486bbfd3eb620d3785e", "title": "Gumbel-Matrix Routing for Flexible Multi-task Learning"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "36e30516683032634975c53e60f3737b6e35ff80", "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217", "title": "The State of Sparsity in Deep Neural Networks"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "853d4d94651c6d9f8ed4d114e1eb21f15f786daa", "title": "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents"}, {"paperId": "1723f1bb6fa033d638d0127e056470a9431246c9", "title": "Discrete Autoencoders for Sequence Models"}, {"paperId": "ad45b1291067120bf9e55ac7424eb627e0aab149", "title": "Training RNNs as Fast as CNNs"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "29e944711a354c396fad71936f536e83025b6ce0", "title": "Categorical Reparameterization with Gumbel-Softmax"}, {"paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "title": "Neural GPUs Learn Algorithms"}, {"paperId": "e6cc6a7bd4db3e7604bae6a654ec29aa8542dafc", "title": "Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "b12ccd118974839db290f15c989649b2b5188636", "title": "Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation"}]}