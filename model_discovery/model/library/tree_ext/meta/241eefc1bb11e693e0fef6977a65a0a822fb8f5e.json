{"paperId": "241eefc1bb11e693e0fef6977a65a0a822fb8f5e", "title": "LoRA+: Efficient Low Rank Adaptation of Large Models", "abstract": "In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$ 2X SpeedUp), at the same computational cost as LoRA.", "venue": "arXiv.org", "year": 2024, "citationCount": 39, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper shows that Low Rank Adaptation as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension), and proposes a new algorithm that improves performance and finetuning speed, at the same computational cost as LoRA."}, "embedding": {"model": "specter_v2", "vector": [-0.11056247353553772, 0.7146747708320618, -0.33974093198776245, 0.4315856695175171, -0.11706439405679703, -0.1128169521689415, 0.8769293427467346, -0.30472829937934875, -0.21797910332679749, -0.1745254099369049, 0.35293200612068176, 0.29312434792518616, 0.139755979180336, -0.005629034247249365, -0.3788692355155945, 0.03882070258259773, -1.2662461996078491, 0.4581237733364105, -0.08967229723930359, -0.31702443957328796, -0.37533554434776306, -0.12107177078723907, -0.6553050875663757, 0.03413836658000946, 0.03362657129764557, 0.9999566078186035, 0.02866561897099018, 0.7862887382507324, -0.2894875109195709, 0.05409844219684601, 0.960625946521759, -0.012401817366480827, 0.9406376481056213, -0.30120888352394104, -0.13106215000152588, -0.28100958466529846, 0.2659556269645691, -0.19347666203975677, -0.5384237766265869, 0.41392001509666443, -0.29708316922187805, 0.5383456349372864, 0.2921237647533417, -0.3133198916912079, -0.1638176143169403, 0.3324783146381378, -0.050201013684272766, 0.6591697931289673, -0.11829330027103424, -0.19690339267253876, 1.042431354522705, -1.4680650234222412, 0.1759624034166336, 1.5288039445877075, 0.9610428810119629, 0.29982510209083557, -0.5836302638053894, -0.6852003335952759, 0.6214442253112793, -0.10357337445020676, -1.3811712265014648, -0.18452323973178864, -0.26870372891426086, -0.13717862963676453, 1.4867432117462158, -0.6326377987861633, -0.20779535174369812, 0.8271703124046326, 0.06947629153728485, 0.7415042519569397, 0.31045717000961304, -0.41617530584335327, -0.20595252513885498, 0.6244453191757202, 0.14642392098903656, 0.541365385055542, -0.2701314687728882, 0.36843162775039673, -0.9907459616661072, -0.195205420255661, 0.5011001229286194, -0.5725891590118408, 0.05564700439572334, -0.15928250551223755, -0.09242681413888931, 0.7732462286949158, 0.5505500435829163, 0.02399463765323162, -0.32791218161582947, 0.7157943844795227, 0.4812515079975128, 0.7867662310600281, -0.044419314712285995, 0.7296276092529297, -0.13386015594005585, 0.5380365252494812, -0.2827024757862091, -0.15269026160240173, 0.17751632630825043, 0.8434445261955261, -0.10702330619096756, 0.34083321690559387, 0.08526246249675751, 0.4908624589443207, 1.0079140663146973, -0.2351069152355194, 0.588141143321991, -0.6226664781570435, -0.007224461063742638, -0.4753187596797943, -0.08909851312637329, -0.9113465547561646, -0.6343384981155396, -0.7776485681533813, -1.4389657974243164, -0.9472883939743042, -0.5493769645690918, 0.25843876600265503, 0.10369490832090378, 0.7866427302360535, -0.3767329752445221, 0.21272091567516327, -0.21969828009605408, 0.5724326372146606, 0.18325521051883698, 0.9330763220787048, 0.25792834162712097, -0.022155387327075005, 0.6040669679641724, -0.9474038481712341, -0.5366526246070862, -1.0277881622314453, 0.4007963538169861, 0.048744987696409225, 0.7717398405075073, 0.35855624079704285, -1.191062331199646, -1.0818161964416504, -0.876153290271759, 0.24513372778892517, -0.733479380607605, 0.388354629278183, 1.1570969820022583, 0.08277175575494766, -0.22078515589237213, 0.5799509286880493, -0.21909187734127045, -0.3991653323173523, 0.28428512811660767, 0.6670700311660767, -0.05574454739689827, -0.1959933191537857, -1.2913340330123901, 0.7118366956710815, 0.3525552451610565, -0.2564588189125061, -0.5800425410270691, -0.5318912267684937, -0.6252219676971436, -0.1086149662733078, 0.096324123442173, -0.8468206524848938, 0.7212549448013306, -0.12845379114151, -1.1433074474334717, 0.2502239942550659, 0.43466031551361084, -0.041744768619537354, 0.5563327074050903, -0.30710142850875854, -0.5344777703285217, -0.5074198842048645, -0.6952429413795471, 0.3473648428916931, 0.28311192989349365, -0.26349490880966187, -0.209141805768013, 0.3152156472206116, -0.26148632168769836, -0.47477737069129944, -0.6700496673583984, 0.47459620237350464, -0.387948602437973, -0.17131569981575012, 0.21322612464427948, 0.4234551191329956, -0.3319474756717682, -0.03867354989051819, 0.08842245489358902, -0.6132681369781494, 0.6040908694267273, -0.01265097875148058, 1.1383342742919922, -0.9390043616294861, -0.14931052923202515, 0.35246673226356506, 0.026633508503437042, -0.545616865158081, -1.1290923357009888, 0.44642946124076843, -0.5563953518867493, 0.3689717948436737, 0.0040807765908539295, -0.8097339272499084, -0.12203212827444077, -0.08505713194608688, -0.6702485084533691, 0.3395751416683197, 0.16867585480213165, 0.5881357192993164, -0.8885953426361084, 0.5190730094909668, -0.14672701060771942, 0.3783053755760193, -1.191447377204895, 1.4423787593841553, 0.022148888558149338, 0.1664579212665558, 0.13286975026130676, -0.43206343054771423, 0.09386664628982544, -0.5093220472335815, 0.29279908537864685, -0.5196093320846558, 0.3275279700756073, 0.2740335464477539, -0.8921691179275513, 1.3358404636383057, -1.0031095743179321, 0.5700422525405884, 0.19145368039608002, -0.2879682183265686, 0.1890452653169632, 0.04179628565907478, 0.011812074109911919, -0.17242929339408875, 0.23355242609977722, 0.49704909324645996, -0.6738789677619934, 0.3011711537837982, 0.1805395483970642, 0.8410830497741699, -0.43319571018218994, -0.016444265842437744, 0.7783066034317017, -0.17985117435455322, 0.23925673961639404, 0.4535932242870331, 0.2737101912498474, 0.227231964468956, 0.653985857963562, 0.1847047507762909, 0.3748994767665863, -1.032738447189331, -0.22144092619419098, 0.6812651753425598, 1.0868115425109863, 0.6929873824119568, 0.5968800187110901, -0.9980243444442749, -0.6798387765884399, -0.34779006242752075, 0.36459052562713623, 1.0971088409423828, -0.28231459856033325, -0.37192681431770325, -0.18193906545639038, -0.018500428646802902, -0.3181093633174896, -0.27312615513801575, -0.5838703513145447, -0.5067257285118103, -0.6681588292121887, -1.3394582271575928, 0.7361922860145569, 0.18157194554805756, 0.7824691534042358, 0.08784548938274384, 0.12221762537956238, -0.30217888951301575, 0.6715613007545471, -0.847547173500061, -0.4849018454551697, 0.3161284029483795, -0.48404058814048767, -0.3031145930290222, 0.2060413360595703, 0.14725197851657867, 0.13304078578948975, -0.8163949847221375, 0.6591662168502808, -0.9083672761917114, -0.03362596407532692, 0.18855726718902588, 0.47083914279937744, -0.279440313577652, -0.6281160116195679, 0.5845566391944885, 1.0000137090682983, 0.2382451891899109, -0.09289896488189697, 0.12383072078227997, 0.2472616732120514, 0.13331033289432526, -0.6074341535568237, 0.21347153186798096, 0.5869680643081665, 0.3676297068595886, 0.5367974042892456, -0.3882804214954376, 0.67066490650177, -1.509710431098938, 1.2692654132843018, -0.17846597731113434, -0.765999972820282, -0.11864733695983887, -0.9915763139724731, -0.287973552942276, 0.813821017742157, -1.0145134925842285, 0.0793820321559906, -0.25444915890693665, -0.033723440021276474, -0.454946905374527, -0.10850745439529419, 0.27828148007392883, 0.47142136096954346, -0.523982048034668, 0.8252177834510803, 0.027830161154270172, 0.4003418982028961, -0.6196401119232178, 0.5201140642166138, -0.7423797249794006, 0.691338062286377, 0.2706543505191803, 0.8179488182067871, -0.09983248263597488, 0.344221830368042, -0.7527967691421509, -0.5831019282341003, -0.18224720656871796, -0.7784902453422546, -0.020819207653403282, -0.2220381647348404, -1.0066920518875122, -0.5102943181991577, -0.45180338621139526, -0.5451930165290833, -0.5951865911483765, -0.12098579108715057, 0.06019274517893791, -0.3163648843765259, -1.0339933633804321, -1.5403088331222534, -0.12690168619155884, -0.5088087916374207, -0.846835732460022, -0.18355117738246918, 0.18957890570163727, -0.5776373744010925, -0.4682820737361908, -0.12713934481143951, -0.8096084594726562, 0.9065037369728088, -0.33525702357292175, 0.6098575592041016, 0.06873881816864014, -0.058985453099012375, -0.4627976417541504, 0.13152165710926056, 0.4472295045852661, -0.4543273150920868, -0.15246273577213287, -0.8958529233932495, -0.047882575541734695, -0.6115352511405945, -0.3847501575946808, 0.1315658986568451, 0.09130267053842545, 0.6155174970626831, -0.3623057007789612, -0.11708284914493561, 0.8636646866798401, 1.2615021467208862, -1.3102962970733643, -0.14830026030540466, 0.5133440494537354, 1.1185232400894165, 0.30483153462409973, -0.5211391448974609, 1.0225669145584106, -0.18400533497333527, 0.6564481854438782, 0.2645283043384552, -0.31179219484329224, -0.6330429911613464, -0.7877569794654846, 0.6739234924316406, 1.5651636123657227, 0.41993069648742676, 0.5749728679656982, -0.6757452487945557, 0.31653013825416565, -0.884307861328125, -0.5284671187400818, 0.4879291355609894, 1.1783530712127686, 0.18237148225307465, -0.2880847454071045, -0.043271858245134354, -0.680351972579956, 0.04496045410633087, 0.03196610510349274, -0.4153890907764435, -0.5656718611717224, -0.3219209313392639, 0.16799581050872803, 0.5884693264961243, 0.6843154430389404, -0.06854598969221115, 0.4078957438468933, 15.058448791503906, 0.7578139901161194, 0.21610264480113983, 0.8753830790519714, 0.6062921285629272, -0.2298056036233902, 0.014432678930461407, -0.2329585999250412, -0.8709215521812439, -0.1931518018245697, 1.3465317487716675, 0.29433825612068176, 1.1538008451461792, 0.40100669860839844, 0.44047489762306213, 0.4912906885147095, -0.5816454291343689, 1.1155914068222046, 0.43130210041999817, -1.185893177986145, 0.43603208661079407, 0.1476784199476242, 0.898583173751831, 0.6959696412086487, 0.694905698299408, 1.0098000764846802, 0.19587485492229462, -0.34775060415267944, 0.09803185611963272, 0.5564773678779602, 0.9946377873420715, -0.2391865849494934, 0.5577428936958313, 0.4425519108772278, -0.9473706483840942, -0.7942671179771423, -0.926660418510437, -0.9361414909362793, 0.21364440023899078, 0.0853409692645073, -0.31619495153427124, -0.8374607563018799, 0.08457744866609573, 0.8966580629348755, -0.7278280258178711, 0.6950696110725403, -0.10883639752864838, 0.2837880849838257, -0.452511727809906, 0.7882411479949951, 0.23559881746768951, 0.2756679058074951, -0.02815786376595497, -0.025755487382411957, -0.01309272088110447, -0.0056676194071769714, 0.25855720043182373, 0.7101301550865173, -0.8233069181442261, -0.14437490701675415, -0.3076555132865906, -0.01714072749018669, -0.06939373165369034, 1.178629755973816, 0.9932525157928467, 0.0845680981874466, -0.42631995677948, 0.20586512982845306, 0.5704846382141113, 0.39804938435554504, -0.0011194656835868955, -0.4375753104686737, 0.3700331747531891, 0.017206959426403046, -0.8494497537612915, 0.3495997190475464, -0.42670920491218567, -0.7360570430755615, -1.0774105787277222, -0.7047793865203857, 0.6489275097846985, -0.9359453320503235, -1.3014283180236816, 0.8953199982643127, -0.3347242474555969, -0.6544803977012634, 0.30592358112335205, -0.8883657455444336, 0.19607901573181152, 0.5218866467475891, -1.2337979078292847, -0.5916978716850281, 0.01855938881635666, -0.21497711539268494, 0.04823952913284302, -0.4657592177391052, 0.9802976846694946, 0.20730575919151306, -0.7621431946754456, 0.2034679353237152, 0.2911841571331024, -0.23164483904838562, 0.24580256640911102, -0.18564742803573608, 0.3264153301715851, 0.2316063940525055, -0.39919430017471313, 0.37565961480140686, 0.1535448133945465, 0.5311785340309143, -1.044516682624817, 0.09673942625522614, -0.05795234441757202, -0.9153804779052734, 0.23231981694698334, -0.7739500403404236, -0.9590551257133484, 0.46268290281295776, 0.14620451629161835, 0.10998223721981049, 0.3123764991760254, 0.36947980523109436, -0.973044753074646, -0.6078966856002808, -0.6439111232757568, -0.24390658736228943, -0.09780431538820267, -1.0293986797332764, -0.2599337100982666, -0.05635472759604454, 0.05333477258682251, -0.7137070894241333, -1.0290768146514893, -0.11165735870599747, 0.06260015815496445, 0.2759912312030792, 1.1681737899780273, -0.6105901598930359, 0.31230267882347107, 0.5424110889434814, 0.1560751050710678, -0.6872835755348206, -0.11539583653211594, -0.7845114469528198, -0.43213126063346863, -0.2658625543117523, 0.3463820517063141, -0.3777846097946167, -0.02692943811416626, 0.49761325120925903, 0.20350894331932068, -0.27577948570251465, -0.6402164101600647, -0.38528764247894287, -0.08820558339357376, -0.23111127316951752, -0.32216596603393555, 0.029581937938928604, 0.09264948219060898, 0.37120935320854187, -0.10662655532360077, 1.1300904750823975, 7.044287485769019e-05, -0.9787275791168213, 0.5217020511627197, -0.3058443069458008, -0.4666115343570709, -0.7621557712554932, -0.3031221330165863, -1.5855144262313843, -0.09968066960573196, -1.0465476512908936, -0.4087367653846741, -0.5191407799720764, -0.5774124264717102, 0.25006359815597534, -0.17851026356220245, -0.035599235445261, 0.9423776865005493, 0.039000675082206726, -0.2650465965270996, 0.0244101881980896, -0.5881716012954712, 0.7454251050949097, 0.49366918206214905, -0.7388826608657837, -0.42937958240509033, 0.04234548285603523, 0.38023999333381653, 0.6462909579277039, 0.31120386719703674, -0.48521026968955994, -0.4294370114803314, -1.0584040880203247, 1.050168514251709, -0.3560419976711273, -0.14183609187602997, -0.9491832256317139, 0.5967578887939453, 0.4358927607536316, 0.4884093701839447, 0.6109522581100464, 0.45708051323890686, -1.0550028085708618, -0.26906341314315796, 0.3964608907699585, -1.0784438848495483, 0.5827881693840027, 0.3123317360877991, -0.1800757199525833, -0.08197550475597382, 0.7901256084442139, 0.3557170331478119, -0.9083433151245117, -0.437378853559494, 0.3096199631690979, -0.8069496154785156, -0.41328510642051697, -0.42069554328918457, 0.013173403218388557, -0.8790798783302307, -0.3229999542236328, -0.39947617053985596, 0.341744065284729, -0.39456450939178467, 0.9771687984466553, 0.3034380376338959, -1.3195362091064453, 0.10752851516008377, 0.4828779697418213, 0.15394707024097443, -0.1547585427761078, 0.3559837341308594, 0.6844980120658875, -0.3299304246902466, 0.30219632387161255, -0.029849201440811157, 0.7875697612762451, -0.20754721760749817, 0.30248987674713135, 0.7965410351753235, -0.19471941888332367, -0.17352445423603058, 1.0418055057525635, -0.00697284284979105, -1.0184115171432495, 0.12963660061359406, -0.7175991535186768, -0.39440837502479553, -0.11674000322818756, 0.8670645356178284, 0.2589143216609955, -0.37648317217826843, -0.02489686757326126, -0.23310591280460358, 0.4142291843891144, 0.04774290323257446, 0.14117570221424103, 0.9385946393013, -0.7125769257545471, -0.501036524772644, 0.854253351688385, 0.7993884086608887, -0.5330857634544373, -0.7334743142127991, -0.7510820031166077, -0.23041437566280365, -0.32751229405403137, 0.27731454372406006, -0.2959112823009491, -0.6364134550094604, 0.6127832531929016, 1.1638367176055908, -0.24615301191806793, 0.6161714792251587, 0.009156414307653904, -0.24439452588558197, 1.1101778745651245, 0.13034720718860626, -0.8609846830368042, -0.18524785339832306, 0.9564259052276611, 1.4729585647583008, -0.6872208118438721, 0.6788005828857422, 0.032522253692150116, -0.4740666449069977, 1.1038299798965454, 0.514886736869812, -0.30369389057159424, 0.6033090949058533, -0.44814127683639526, -0.45879992842674255, 0.3657297194004059, -1.2717833518981934, -0.12488209456205368, 0.9370592832565308, 0.8145965337753296, 0.4911688268184662, -0.2250220775604248, -0.10299985110759735, 0.9911168217658997, 0.11777675151824951, -0.2147860825061798, 1.0462374687194824, -0.5416634678840637, -0.1811627447605133, 0.02656569331884384, -0.12556755542755127, 0.688661515712738, -0.4067186415195465, -0.19509509205818176, 0.07216359674930573, 0.48962557315826416, 0.03588062897324562, 0.33327001333236694, 0.4728955328464508, -0.16565939784049988, 0.8196686506271362, -0.21788661181926727, 0.06645673513412476, -0.3689594566822052, -0.4058968424797058, 0.34694966673851013, -0.5857337713241577, -0.4899698793888092, 0.334845632314682, -0.2859175205230713, -0.07478249818086624, -0.33609166741371155, 0.3588157892227173, -0.12832440435886383, 0.1655287742614746, 0.5024936199188232, 0.40909072756767273, 0.4617488980293274, -0.014535792171955109, -0.6939592361450195, -0.6361595392227173, -0.9132042527198792, -0.13161306083202362, -0.3336769640445709, -0.15934960544109344, -0.16493983566761017, -0.49541911482810974, -0.35702088475227356]}, "authors": [{"authorId": "46183987", "name": "Soufiane Hayou"}, {"authorId": "2284680456", "name": "Nikhil Ghosh"}, {"authorId": "2284822332", "name": "Bin Yu"}], "references": [{"paperId": "9153dc5312394e604cedc43af2a17a7ad4bb6743", "title": "The Expressive Power of Low-Rank Adaptation"}, {"paperId": "0d7f24578340aae6df610ed95aaa276b9c3ddcd3", "title": "VeRA: Vector-based Random Matrix Adaptation"}, {"paperId": "af8123ecdff838f63e4eba0b36b8babe4c5cee65", "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "b477787bbf822912c9b72af4f143d3b9eb57b99b", "title": "Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks"}, {"paperId": "ad91394aaa1dad451e1ea52acb73b525c9574642", "title": "Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit"}, {"paperId": "7a53033ea3601b731af0d3b911a8acebee66e799", "title": "Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "c1738f21ea2460e1015d590906a4f43e155f60c8", "title": "The Shaped Transformer: Attention Models in the Infinite Depth-and-Width Limit"}, {"paperId": "fbd2c8089870814449f9254a711041bbae145a82", "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "b8c236dc5963dac36b0d8e419beb5876e3a18f96", "title": "Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "0484a65707706e11eaa53fde8817c3122f07550d", "title": "On the infinite-depth limit of finite-width neural networks"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "0b0d7d87c58d41b92d907347b778032be5966f60", "title": "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "026bb8a1066f50ddc8797e1341353603149a8cb8", "title": "Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability"}, {"paperId": "758cf7cd62bb8b62a6a4bc550a34e0a574bbbcb2", "title": "Stable ResNet"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e663797275a20c0ab960772fe74e86c855b33767", "title": "On the Impact of the Activation Function on Deep Neural Networks Training"}, {"paperId": "9b15a6f2434b9274cd1228eed4288b98cd316394", "title": "Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "7a84a692327534fd227fa1e07fcb3816b633c591", "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "4fdc7df2c737141a1bf5aec27a438b77d01f8af0", "title": "Deep Information Propagation"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "262120c2c734a9c6ccccaf591e7a4b6e5cfe515d", "title": "A theory of transfer learning with applications to active learning"}, {"paperId": "6c7384845f7d8347a6daf393ce1586e03dca971b", "title": "Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "b87274e6d9aa4e6ba5148898aa92941617d2b6ed", "title": "Efficient BackProp"}, {"paperId": null, "title": "1 . 0 e 5 . 0 e 1 . 0 e 5 . 0 e 1 . 0 e A 1 . 0 e 5 . 0 e 1 . 0 e 5 . 0 e 1 . 0 e A Figure 19. Train and test loss of Llama-7b finetuned on MNLI in the same setting as Figure 5 right panel"}, {"paperId": null, "title": "large"}]}