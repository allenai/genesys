{"paperId": "002431a77880b796a620db6c529f7586e6a659b9", "title": "An Efficient Training Accelerator for Transformers With Hardware-Algorithm Co-Optimization", "abstract": "Transformers have achieved significant success in deep learning, and training Transformers efficiently on resource-constrained platforms has been attracting continuous attention for domain adaptions and privacy concerns. However, deploying Transformers training on these platforms is still challenging due to its dynamic workloads, intensive computations, and massive memory accesses. To address these issues, we propose an Efficient Training Accelerator for TRansformers (TRETA) through a hardware-algorithm co-optimization strategy. First, a hardware-friendly mixed-precision training algorithm is presented based on a compact and efficient data format, which significantly reduces the computation and memory requirements. Second, a flexible and scalable architecture is proposed to achieve high utilization of computing resources when processing arbitrary irregular general matrix multiplication (GEMM) operations during training. These irregular GEMMs lead to severe under-utilization when simply mapped on traditional systolic architectures. Third, we develop training-oriented architectures for the crucial Softmax and layer normalization functions in Transformers, respectively. These area-efficient modules have unified and flexible microarchitectures to meet various computation requirements of different training phases. Finally, TRETA is implemented under Taiwan Semiconductor Manufacturing Company (TSMC) 28-nm technology and evaluated on multiple benchmarks. The experimental results show that our training framework achieves the same accuracy as the full precision baseline. Moreover, TRETA can achieve 14.71 tera operations per second (TOPS) and 3.31 TOPS/W in terms of throughput and energy efficiency, respectively. Compared with prior arts, the proposed design shows 1.4\u2013 $24.5\\times $ speedup and 1.5\u2013 $25.4\\times $ energy efficiency improvement.", "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems", "year": 2023, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "An Efficient Training Accelerator for TRansformers (TRETA) is proposed through a hardware-algorithm co-optimization strategy and training-oriented architectures for the crucial Softmax and layer normalization functions in Transformers are developed."}, "embedding": {"model": "specter_v2", "vector": [0.6217244267463684, 0.699226975440979, -0.5529111623764038, -0.032430123537778854, -0.1982763409614563, 0.32990390062332153, 0.500449538230896, -0.16760216653347015, -0.24006851017475128, -0.6039289236068726, 0.29853948950767517, -0.32126596570014954, 0.5448945164680481, -0.20684026181697845, -0.03511253744363785, -0.2357354760169983, -0.8530794382095337, -0.10570894926786423, -0.06159219890832901, -0.23218511044979095, 0.06219344958662987, -0.20500177145004272, -0.8348661065101624, 0.20045307278633118, 0.20161326229572296, 1.404570460319519, -0.11077125370502472, 0.4354703724384308, -0.540451169013977, 0.5586434602737427, 0.3560304343700409, -0.5883832573890686, 0.5939959287643433, 0.6021494269371033, -0.1613854318857193, 0.05335965007543564, 0.5231162905693054, -0.40555688738822937, -0.6485706567764282, 1.2382349967956543, -0.21653816103935242, 0.05810180678963661, 0.23456178605556488, -0.7274402379989624, 0.14898277819156647, 0.15717871487140656, 0.2232780158519745, 0.7006686925888062, -1.3164247274398804, -0.5214636325836182, 1.0015127658843994, -1.123947024345398, -0.6098500490188599, 0.9803207516670227, 0.4400525689125061, -0.078618124127388, 0.10286390036344528, -0.8148666620254517, -0.166751429438591, -0.10308604687452316, -0.5570529699325562, -0.530779242515564, 0.269347608089447, -0.029893305152654648, 1.7837495803833008, -0.2770882844924927, 0.044587645679712296, 0.29742905497550964, 0.37562382221221924, 0.9899659752845764, 0.15744629502296448, -0.6389437913894653, 0.1265994757413864, -0.42535996437072754, 0.45742103457450867, 0.7877942323684692, 0.2329884171485901, 0.3335922956466675, -1.0306516885757446, -0.09427576512098312, 0.414730042219162, 0.4463311433792114, 0.9176443815231323, -0.058273155242204666, -0.2873784899711609, 0.6493391990661621, 0.8378559947013855, 0.33841097354888916, -0.10787060856819153, 1.1160252094268799, 0.7956188321113586, 0.253702849149704, -0.21366892755031586, 0.3576227128505707, 0.20175549387931824, 0.5533260703086853, -1.2142164707183838, -0.2812037467956543, -0.5472758412361145, 0.9708530902862549, -0.39431387186050415, 0.7834926843643188, -0.5123725533485413, 0.006150582805275917, 0.7698894739151001, 0.1695205718278885, 0.1327318549156189, -0.22350069880485535, 0.7018794417381287, -0.37600114941596985, -0.3460903763771057, -0.3553813099861145, -0.15586836636066437, -0.9331919550895691, -1.6292754411697388, -0.7102410197257996, -0.7506992816925049, 0.349227637052536, -1.1920701265335083, 0.1281442642211914, -0.849585771560669, 0.5007978081703186, 0.33586886525154114, 0.14787155389785767, 0.41092053055763245, 0.5211405754089355, -0.057517681270837784, 0.16181248426437378, 1.4080677032470703, -1.7095810174942017, -0.555354654788971, -1.206254482269287, 0.16661953926086426, -0.411227285861969, 0.023981278762221336, 0.01822671853005886, -1.206795573234558, -1.0237760543823242, -1.3547353744506836, -0.25988444685935974, -0.5358316898345947, 0.3848055899143219, 1.3781027793884277, 0.7153849005699158, -1.0694798231124878, 0.5584560036659241, -0.6905665397644043, 0.18421867489814758, 0.6583608984947205, 0.8133595585823059, 1.1031711101531982, -0.0033439851831644773, -0.9125720262527466, -0.017846811562776566, 0.0361030288040638, -0.37624624371528625, -0.3429124355316162, -0.6366100311279297, -0.5443832874298096, 0.482098251581192, -0.16562137007713318, -0.8219885230064392, 1.099923849105835, -0.1270209699869156, -0.9308152198791504, 0.7588932514190674, 0.02405191957950592, -0.19375449419021606, -0.04303181916475296, 0.340055376291275, -0.6932246685028076, -0.5172819495201111, -0.2591712176799774, 0.15852618217468262, 0.6640387773513794, 0.4470440745353699, -0.15024051070213318, 0.028974616900086403, -0.25505101680755615, -0.03327593207359314, -0.6628280282020569, 0.9162544012069702, -0.5750619173049927, -0.4646134376525879, 0.7223131060600281, 0.6007112264633179, -0.7456426620483398, -0.18170571327209473, -0.6867255568504333, -0.41988158226013184, 0.6967552900314331, 0.3314133286476135, 0.8597707152366638, -1.0983201265335083, -1.318357229232788, 0.43129023909568787, 0.34052199125289917, 0.0660661980509758, -0.08045712858438492, 0.1440006047487259, -0.4563601613044739, -0.21208800375461578, 0.36271435022354126, -0.9186016321182251, 0.12573426961898804, -0.4849308133125305, -0.8436458110809326, -0.2912374436855316, -0.02118469960987568, 1.0187170505523682, -0.40486064553260803, 0.40122169256210327, -0.28511545062065125, 0.21516509354114532, -0.928240180015564, 0.9430320858955383, -0.0022445712238550186, -0.30879127979278564, -0.11799547076225281, 0.444789856672287, 0.4452274441719055, -0.8755828738212585, 0.9685490727424622, -1.1423673629760742, 0.03394028916954994, 0.6751583218574524, -0.033760178834199905, 1.2035399675369263, -0.3817487955093384, 0.5326218008995056, 0.14206603169441223, -0.861616313457489, 0.3495214283466339, 0.2280731499195099, -0.1533718705177307, -0.8112939596176147, 0.9492966532707214, 0.5806028842926025, -0.4180213212966919, 0.5833151340484619, 1.0369579792022705, 0.9138492345809937, -0.6809149980545044, -0.08878294378519058, 0.5639356374740601, -0.25757884979248047, 0.1120813712477684, 0.06497013568878174, 0.681694746017456, -0.48814883828163147, 0.21942496299743652, -0.5021631717681885, 0.1930086612701416, -0.982917070388794, -0.09676098078489304, 0.3544480800628662, 0.4671735465526581, 0.2757023572921753, 0.6001270413398743, -0.6023505926132202, -0.5311161875724792, -0.35938480496406555, 0.4066002368927002, 1.324832797050476, -0.10067348927259445, 0.11512286216020584, -0.5465633273124695, -0.21373926103115082, -0.3128676414489746, -0.6284449696540833, 0.19063813984394073, -0.09968095272779465, -0.24334019422531128, -1.5110807418823242, 0.9676442742347717, 0.4125097095966339, 1.489298939704895, -0.09292228519916534, -0.5955415964126587, -0.9089124202728271, 0.612456738948822, -1.0555583238601685, -0.8198583722114563, 1.111846923828125, -0.9899546504020691, 0.07476574927568436, 0.2344900369644165, -0.4643554985523224, 0.6566230058670044, -0.47077468037605286, 0.9062078595161438, -0.4479885697364807, -0.17568416893482208, -0.40862607955932617, 0.8631916046142578, -0.3841610848903656, -0.45718589425086975, 0.17974312603473663, -0.07740052044391632, -0.005401014816015959, 0.5597302913665771, -0.5424141883850098, 0.18566769361495972, -0.2225000560283661, -0.003838066942989826, -0.02309436723589897, 0.7263316512107849, 0.18164455890655518, 0.6074771285057068, -0.09470752626657486, -0.22460030019283295, -1.2942246198654175, 0.8343604207038879, 0.32093948125839233, -0.49882254004478455, 0.04712677001953125, -0.7819136381149292, 0.08734209090471268, 0.6946145296096802, -0.533043622970581, 0.11887967586517334, -0.7091767191886902, 0.19789470732212067, -0.9821537137031555, 0.4023296535015106, -0.15818394720554352, 0.10059252381324768, -0.5943250060081482, 0.7257114052772522, 0.49835294485092163, 0.2867552638053894, 0.49710795283317566, 0.13810186088085175, -0.7582526206970215, 0.8498438000679016, 0.08186204731464386, 0.5592024922370911, -0.1024087443947792, 0.3946833610534668, -0.46468156576156616, -0.4035506844520569, -0.2156773805618286, -0.10891009867191315, -0.2618228793144226, 0.08433201909065247, -0.8250008821487427, -0.7794191837310791, -0.09811565279960632, -0.7869089841842651, 0.06646934151649475, 0.16581138968467712, -0.20576895773410797, -0.014300938695669174, -1.2581562995910645, -1.1591476202011108, -0.29310932755470276, -1.573032259941101, -1.6201181411743164, 0.36797305941581726, 0.25738832354545593, 0.27988752722740173, -0.45493918657302856, -1.1182774305343628, -0.31612300872802734, 1.2557339668273926, -0.28271812200546265, 0.4370502531528473, -0.03450658917427063, -0.657268226146698, 0.14680787920951843, -0.40454840660095215, 0.3303675353527069, -0.5403428673744202, 0.10776989161968231, -1.0995548963546753, 0.3267684280872345, -0.3871864974498749, -0.33723923563957214, 0.38372090458869934, 0.20811133086681366, 1.0695728063583374, 0.2989332675933838, -0.5992437601089478, 1.0911543369293213, 1.1931158304214478, -0.5272194743156433, 0.4234819710254669, -0.0690239742398262, 0.8927120566368103, -0.6807119250297546, -0.11655537039041519, 0.6369386315345764, -0.3928055763244629, 0.48812949657440186, 0.39804571866989136, -0.34262487292289734, 0.23748861253261566, 0.12558747828006744, 0.2282966524362564, 1.247791051864624, 0.6895154714584351, 0.4241023361682892, -0.6337732672691345, 0.48536601662635803, -0.6644465327262878, -0.40449172258377075, 0.3695391118526459, 0.7000778317451477, 0.005276606883853674, 0.45363500714302063, -0.4268363118171692, 0.2720728814601898, 0.27415862679481506, 0.9432733058929443, -0.5028138160705566, -1.0658503770828247, 0.38065025210380554, 1.0813716650009155, 1.2553764581680298, 0.2379147708415985, -0.29142892360687256, 0.1668931394815445, 14.442721366882324, 1.1007399559020996, -0.44845253229141235, 0.767471969127655, 0.6758061647415161, 0.3085249662399292, -0.011910312809050083, -0.1534738540649414, -1.009821891784668, 0.3204728960990906, 1.3437368869781494, 0.303120493888855, 0.32519665360450745, 0.7441355586051941, -0.19423651695251465, 0.4269852638244629, -0.24381445348262787, 0.9127277135848999, 0.4024353325366974, -1.828674554824829, -0.18703001737594604, 0.3706563711166382, 0.33675646781921387, 0.4955635666847229, 1.0716193914413452, 0.6854845881462097, 0.23136013746261597, -0.42549780011177063, 0.19628287851810455, -0.2673434317111969, 1.3419493436813354, -0.4808270037174225, 0.43716195225715637, 0.28208839893341064, -1.2921627759933472, 0.1661601960659027, -0.5300189256668091, -1.261841893196106, -0.05084095522761345, 0.8579672574996948, -0.8253872990608215, -0.27730226516723633, -0.4202772080898285, 0.6722099781036377, 0.1577266901731491, 0.2595019042491913, -0.32179737091064453, 0.396596223115921, -0.16721531748771667, 0.01922551915049553, 0.1752374917268753, 0.32221201062202454, -0.3540053069591522, 0.057465676218271255, 0.1917152851819992, -0.22893959283828735, 0.3132263123989105, 0.24103304743766785, -0.5503538846969604, -0.6685872673988342, 0.06372486054897308, -0.3002602756023407, -0.04931105300784111, 1.199459433555603, 0.11650197952985764, 0.2014872282743454, -0.5326039791107178, 0.29111936688423157, 0.28133708238601685, -0.3103695809841156, -0.7732587456703186, -0.28904086351394653, 0.4478907883167267, -0.5155361890792847, -0.3814545273780823, 0.39187091588974, -1.1867847442626953, -0.621311604976654, -0.6469087600708008, -0.3429582715034485, 0.30723175406455994, -0.4982954263687134, -0.3279472589492798, 1.0000852346420288, -0.8368613719940186, -0.5870419144630432, 0.7133667469024658, -1.0664533376693726, -0.4145759046077728, 0.6046780347824097, -1.3918423652648926, -0.6435345411300659, -0.24734367430210114, -0.21535877883434296, -0.5805009603500366, -0.2758730351924896, 0.746362030506134, 0.7012661099433899, -0.26349693536758423, 0.5351852178573608, -0.1447964608669281, 0.0358630046248436, -0.4156709313392639, -0.3778201937675476, 1.4629360437393188, 0.5842246413230896, -0.5917038917541504, 0.15282773971557617, 0.08515601605176926, 0.26962482929229736, -0.8619526624679565, -0.3726053237915039, 0.4510633945465088, 0.11217880994081497, 0.1543026864528656, -1.0214860439300537, -0.38165444135665894, 0.28814399242401123, 0.22090987861156464, 0.23618637025356293, 0.4516060948371887, 0.1279514580965042, -0.4362512528896332, -0.29777467250823975, -0.5290098786354065, -0.004742446821182966, 0.45354968309402466, -0.8946446776390076, 0.26312583684921265, 0.277886301279068, 0.17186272144317627, -1.614875078201294, -0.5238591432571411, -0.07943154871463776, -0.18155072629451752, -0.41907885670661926, 1.3075066804885864, 0.30916911363601685, 0.8895415663719177, 0.8305012583732605, -0.2577870488166809, -0.2927893400192261, 0.2218216061592102, -0.8332382440567017, 0.018368249759078026, -0.3171112835407257, 0.29591983556747437, -0.3635372221469879, 1.3383347988128662, 0.41553929448127747, -0.29457518458366394, -0.6677940487861633, -0.4847245216369629, -0.007873228751122952, -0.7478625178337097, -0.3698088526725769, 0.18125946819782257, 0.24073247611522675, 0.06606117635965347, 0.04156110808253288, 0.24870982766151428, 0.725770115852356, 0.03804443031549454, -0.20266729593276978, 0.21609912812709808, -0.08425488322973251, -0.5564790964126587, -0.4271004796028137, -0.7542774677276611, -1.79351007938385, -0.2584226131439209, -1.3530845642089844, -0.22668933868408203, -0.2142026424407959, -0.4504868686199188, -0.19094866514205933, -0.3239416480064392, 0.0800948441028595, 0.2891700565814972, 0.019437160342931747, -0.47720029950141907, -0.5637606978416443, -0.33871975541114807, 0.7591911554336548, 0.7888302206993103, -0.5737099647521973, 0.2903967499732971, 0.06034937500953674, 0.061038900166749954, -0.038648445159196854, 0.15891125798225403, -0.10465417802333832, -0.9657461047172546, -1.0688090324401855, 0.09924815595149994, 0.14270026981830597, -0.2771027684211731, -1.4679577350616455, 1.0153976678848267, 0.07720962911844254, -0.31687676906585693, 0.20296625792980194, 0.3299506902694702, -1.0137944221496582, -0.49956899881362915, 0.5140400528907776, -0.24956031143665314, 0.2350843995809555, 0.3462901711463928, -0.9526188969612122, -0.10636606812477112, 0.8439347743988037, 0.3198767900466919, -0.2886487543582916, -0.814811646938324, 0.5540421605110168, -0.3679139018058777, -0.08581260591745377, -0.08412878215312958, -0.0657401978969574, -1.508661150932312, -0.016967613250017166, 0.038330528885126114, 0.4674517810344696, -0.3012202978134155, 0.2847810387611389, 0.3592577278614044, -1.248239517211914, 0.549481987953186, 0.8750562071800232, -0.6319370865821838, 0.1961769461631775, 0.3468869924545288, 0.9865388870239258, -0.8619949817657471, 0.3654559552669525, -0.11215172708034515, 0.25525861978530884, -0.8077507019042969, 0.02468760870397091, 0.8911157846450806, -1.0366146564483643, 0.13607481122016907, 1.2338058948516846, -0.4350607395172119, -0.685368537902832, 0.09911227226257324, -1.4510425329208374, -0.20902496576309204, -0.46391385793685913, 0.2915961444377899, -0.06283831596374512, 0.6888982653617859, 0.47883811593055725, -0.583375096321106, -0.2187410444021225, 0.06657278537750244, -0.01764230616390705, 0.07289808988571167, 0.4727095067501068, -0.23749196529388428, -0.08075829595327377, 0.9715766906738281, -0.5441228151321411, -0.7084631323814392, -0.9207929968833923, -0.4711087644100189, -0.27660471200942993, 0.5368577241897583, 0.08596690744161606, -1.4282511472702026, 0.9070382714271545, 0.550123393535614, 0.2918384373188019, 1.0197035074234009, -0.5236117243766785, 0.5709141492843628, 0.39994779229164124, 0.29725319147109985, -0.48150160908699036, -0.3113524913787842, 1.3668218851089478, 0.3356580436229706, -0.45165568590164185, 0.8226498365402222, -0.8806067109107971, -0.4797769784927368, 0.9854024648666382, 0.369440495967865, -0.033998094499111176, 0.8731985092163086, 0.6427122950553894, -0.2174065113067627, -0.03400207310914993, -0.7029780745506287, -0.38536596298217773, 0.6892600655555725, 0.742415189743042, 0.6839574575424194, 0.13181611895561218, 0.282373309135437, 1.0378347635269165, 0.04432377591729164, 0.00311001087538898, 0.36687028408050537, 0.42630013823509216, -0.009694374166429043, -0.1349361538887024, -0.17805084586143494, 1.0196176767349243, -0.6544176340103149, -0.7943084239959717, 0.5975926518440247, 0.5415195226669312, 0.24815525114536285, 0.22390222549438477, 1.0652555227279663, -0.6788199543952942, 0.5966002345085144, -0.34575363993644714, 0.3870459794998169, -0.3590983748435974, -0.3313295841217041, -0.1858271062374115, -0.6884568929672241, -0.1728617250919342, 0.016778547316789627, -0.12290459126234055, -0.6389349699020386, -0.56853187084198, 0.735825777053833, 0.0022360743023455143, 0.40296873450279236, 0.7577317953109741, 0.5264593362808228, 1.3990721702575684, 0.135747030377388, -0.9492678642272949, -0.38451555371284485, -0.38436079025268555, -0.25787752866744995, -0.7351330518722534, -0.08736016601324081, 0.015182900242507458, -0.061732083559036255, -0.6436542868614197]}, "authors": [{"authorId": "2146641923", "name": "Haikuo Shao"}, {"authorId": "152319896", "name": "Jinming Lu"}, {"authorId": "1887160141", "name": "Meiqi Wang"}, {"authorId": "2239628722", "name": "Zhongfeng Wang"}], "references": [{"paperId": "84ed091b4f7875323e512f17f5fe46eecbf66174", "title": "A 95.6-TOPS/W Deep Learning Inference Accelerator With Per-Vector Scaled 4-bit Quantization in 5 nm"}, {"paperId": "7256a8c91573d55a695cc8b5e987ed5f8c6713cf", "title": "FlexBlock: A Flexible DNN Training Accelerator With Multi-Mode Block Floating Point Support"}, {"paperId": "1c1c186fa5bd75953b2874af4f74750c57c8d4a7", "title": "A 28nm 27.5TOPS/W Approximate-Computing-Based Transformer Processor with Asymptotic Sparsity Speculating and Out-of-Order Computing"}, {"paperId": "292ca9ddd7496cb451616f97207d7852ff3f37a6", "title": "ETA: An Efficient Training Accelerator for DNNs Based on Hardware-Algorithm Co-Optimization"}, {"paperId": "1acdbbf540934b9e2b181b756949ef642f18ee26", "title": "A 7-nm Four-Core Mixed-Precision AI Chip With 26.2-TFLOPS Hybrid-FP8 Training, 104.9-TOPS INT4 Inference, and Workload-Aware Throttling"}, {"paperId": "8a0a7170977cf5c94d9079b351562077b78df87a", "title": "A White Paper on Neural Network Quantization"}, {"paperId": "cc2f943b4fda183c86286e78e020e9019dfdc36c", "title": "Cambricon-Q: A Hybrid Architecture for Efficient Training"}, {"paperId": "77366bef01df1ab277149b330336a0ef9c5041c4", "title": "Transformer"}, {"paperId": "7240d82352e1eea633edc925c0903a72920e642b", "title": "Distribution Adaptive INT8 Quantization for Training CNNs"}, {"paperId": "131ee42e4839d153333e17f46facdb6806e98c73", "title": "Developing Real-Time Streaming Transformer Transducer for Speech Recognition on Large-Scale Dataset"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "7343a3d36b79fedb0d9e5e5eaf80d6f508107c6f", "title": "NITI: Training Integer Neural Networks Using Integer-Only Arithmetic"}, {"paperId": "a3df54c1a83e45bc18301313b187efbd4a81bcad", "title": "The Hardware and Algorithm Co-Design for Energy-Efficient DNN Processor on Edge/Mobile Devices"}, {"paperId": "4d5254407ec01e6c151fb4f102547ff10fe6c9ed", "title": "A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim"}, {"paperId": "1728cb805a9573b59330890ba9723e73d6c3c974", "title": "Knowledge Distillation: A Survey"}, {"paperId": "27e06ec936710072f2ac8bf237dcba7a93e6909f", "title": "A 3.0 TFLOPS 0.62V Scalable Processor Core for High Compute Utilization AI Training and Inference"}, {"paperId": "4de08637d620ae781783a91b46f82ee8d9be405f", "title": "Low-Rank Compression of Neural Nets: Learning the Rank of Each Layer"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "fcf1b4473a0af1f3ebc0fd556ee30c9309ff6345", "title": "SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training"}, {"paperId": "962bdacb3941053788d69696826c9b4307652cce", "title": "Towards Unified INT8 Training for Convolutional Neural Network"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e65c84e2778d7b13b7541e6b14ff790b624a24ec", "title": "A Study of BFLOAT16 for Deep Learning Training"}, {"paperId": "57814675c33626aeb6e90bbd714a88f7925b3e00", "title": "7.7 LNPU: A 25.3TFLOPS/W Sparse Deep-Neural-Network Learning Processor with Fine-Grained Mixed Precision of FP8-FP16"}, {"paperId": "60b373c65ad0032dda6c200f77147993ddca6a73", "title": "Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation"}, {"paperId": "921cb84f4b7ea75a2632c7e7ceadb213b4d7d8fd", "title": "A Scalable Multi- TeraOPS Deep Learning Processor Core for AI Trainina and Inference"}, {"paperId": "41a78e2885b5dc8c719495a33985b5f4880f5b48", "title": "Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition"}, {"paperId": "339f5523a0cdeea315dc5affc27379ca57398264", "title": "Training DNNs with Hybrid Block Floating Point"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "35b91b365ceb016fb3e022577cec96fb9b445dc5", "title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "title": "Building a Large Annotated Corpus of English: The Penn Treebank"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "9914ff9a2202e32a07cb55d628ab259fd930b644", "title": "An Overview of Energy-Efficient Hardware Accelerators for On-Device Deep-Neural-Network Training"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Institute of Computer Technology, Peking University, Beijing, China"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4", "title": "Gradient-based learning applied to document recognition"}, {"paperId": null, "title": "TRETA WITH HARDWARE-ALGORITHM CO-OPTIMIZATION"}, {"paperId": null, "title": "an Assistant Professor Integrated Circuits, Sun Yat-sen China"}]}