{"paperId": "78a08ff468e1f69bcc6463dad95f9476f3177f48", "title": "Manifold-Preserving Transformers are Effective for Short-Long Range Encoding", "abstract": "Multi-head self-attention-based Transformers have shown promise in different learning tasks. Albeit these models exhibit significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformers and their variants fail to preserve layer-wise contextual information. Transformers usually project tokens onto sparse manifolds and fail to preserve mathematical equivalence among the token representations. In this work, we propose TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between a pair of tokens. We propose a simple alternative to dot-product attention to ensure Lipschitz continuity. This allows TransJect to learn injective mappings to transform token representations to different manifolds with similar topology and preserve Euclidean distance between every pair of tokens in subsequent layers. Evaluations across multiple benchmark short- and long-sequence classification tasks show maximum improvements of 6.8% and 5.9%, respectively, over the variants of Transformers. Additionally, TransJect displays 79% better performance than Transformer on the language modeling task. We further highlight the shortcomings of multi-head self-attention from the statistical physics viewpoint. Although multi-head self-attention was incepted to learn different abstraction levels within the networks, our empirical analyses suggest that different attention heads learn randomly and unorderly. In contrast, TransJect adapts a mixture of experts for regularization; these experts are more orderly and balanced and learn different sparse representations from the input sequences. TransJect exhibits very low entropy and can be efficiently scaled to larger depths.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2023, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between a pair of tokens and proposes a simple alternative to dot-product attention to ensure Lipschitz continuity, and highlights the shortcomings of multi-head self-attention from the statistical physics viewpoint."}, "embedding": {"model": "specter_v2", "vector": [0.1349124014377594, 0.6741235852241516, -0.2754296362400055, -0.2410333752632141, -0.20021763443946838, -0.02450515702366829, 0.5518786907196045, -0.06976927071809769, -0.4300079941749573, -0.25320136547088623, 0.7819468379020691, 0.41807129979133606, 0.5379646420478821, 0.1424880474805832, -0.44993090629577637, 0.032154589891433716, -1.0774911642074585, -0.09171042591333389, -0.030739400535821915, -0.07338150590658188, -0.1213904321193695, -0.7628232836723328, -0.9966493844985962, -0.03664184361696243, 0.38454118371009827, 0.5722209811210632, 0.44315028190612793, 0.7429969310760498, -0.8255443572998047, 0.6668716073036194, 0.45448940992355347, -0.5649070143699646, 0.03974727913737297, -0.11777246743440628, -0.5258944630622864, -0.3698244094848633, 0.48983460664749146, -0.22152850031852722, -0.6717965602874756, 0.8864832520484924, -0.32857954502105713, 0.10724705457687378, 0.7163978815078735, -0.4493062496185303, -0.168867826461792, 1.0044258832931519, 0.9804496765136719, 0.46774184703826904, -0.36239713430404663, -0.8521469235420227, 1.6166960000991821, -1.3322583436965942, 0.1299053132534027, 1.1530004739761353, 0.49582940340042114, 0.5171722769737244, -0.21876730024814606, -0.7128612995147705, 0.7757123112678528, 0.4343929886817932, -0.9907326698303223, -0.31585654616355896, 0.0455215685069561, -0.2227792590856552, 1.7209542989730835, -0.30673038959503174, 0.09866845607757568, 0.5454249382019043, 0.1194780245423317, 1.4063265323638916, -0.13871532678604126, -0.622056782245636, -0.3814339339733124, 0.11631634831428528, 0.48862674832344055, 1.1214741468429565, -0.5586928725242615, 0.42727312445640564, -1.1103981733322144, 0.32153692841529846, 0.531589150428772, 0.1042342558503151, 0.49166402220726013, -0.31944674253463745, -0.21242325007915497, 0.4669414162635803, 0.5695505738258362, 0.974895715713501, -0.11369168013334274, 0.9852137565612793, 0.9143568277359009, 0.16132129728794098, -0.09001874178647995, 0.17355293035507202, 0.2947009205818176, 0.13109366595745087, -0.5554102063179016, -0.061659470200538635, -0.08068937063217163, 1.2414413690567017, 0.1570678949356079, 0.6276532411575317, -0.3765926659107208, 0.1408645510673523, 1.425650954246521, -0.15839332342147827, 0.4942317008972168, -0.5032267570495605, -0.0028910688124597073, -0.8753206729888916, -0.1739063709974289, -0.8856574892997742, -0.03931465744972229, -0.46005985140800476, -0.8076745867729187, -1.2982558012008667, -0.5112192630767822, 0.40234285593032837, -0.4764590263366699, 0.8939123153686523, -0.41563543677330017, 0.7293183207511902, -0.14936323463916779, 0.42212679982185364, 0.6843835115432739, 0.6417418718338013, 0.5288563966751099, 0.3087339699268341, 0.8801474571228027, -0.8140915036201477, -0.8471121191978455, -0.7691982388496399, 0.9201301336288452, 0.14299635589122772, 0.2568453252315521, -0.281064510345459, -1.0624799728393555, -0.8757171630859375, -0.9429711103439331, -0.00912538357079029, -0.7570241093635559, -0.051021985709667206, 0.9173137545585632, 0.4269982576370239, -0.8158367276191711, 1.1762615442276, -0.4894392192363739, -0.09935035556554794, 0.705377995967865, 0.10578897595405579, -0.010897369123995304, -0.3071049153804779, -1.5074137449264526, 0.6881269216537476, 0.3195231556892395, -0.40466710925102234, -0.43349525332450867, -0.8890148997306824, -1.7912780046463013, 0.1366308480501175, 0.17302776873111725, -0.3519741892814636, 0.9415165781974792, 0.03168663755059242, -1.4977439641952515, 0.6528518795967102, -0.5807821750640869, -0.027981961145997047, 0.16920335590839386, -0.25918111205101013, -0.3939072787761688, -0.5408000946044922, 0.11253183335065842, 0.07541661709547043, 0.3996826708316803, -0.3960115313529968, -0.17798686027526855, 0.10697780549526215, -0.6826712489128113, -0.05193672329187393, -0.5021221041679382, 0.9674392938613892, 0.011453468352556229, -0.2165449559688568, -0.05129629373550415, 0.8165761232376099, 0.3381803333759308, -0.6359170079231262, -0.6277425289154053, -1.1885522603988647, 0.7009341716766357, 0.20836381614208221, 0.8135157227516174, -0.9955517649650574, -0.7502638101577759, -0.40519043803215027, -0.11620564013719559, -0.00903137493878603, -0.7019012570381165, 0.7523536086082458, -0.5343145728111267, 0.5283730030059814, -0.2211507260799408, -1.317644715309143, 0.39260685443878174, -0.05603431910276413, -0.6927833557128906, -0.022355208173394203, 0.1226632297039032, 1.0727975368499756, -1.0991261005401611, -0.27219000458717346, 0.2888585031032562, 0.3126542568206787, -0.7382684350013733, 1.4345976114273071, 0.04014900326728821, -0.16207805275917053, 0.224289670586586, -0.46043598651885986, -0.0669218972325325, -0.12339185178279877, 0.160480335354805, -0.42139390110969543, -0.04021322727203369, 0.9000481367111206, -0.28073325753211975, 1.4250884056091309, -0.6029338240623474, 0.6052992343902588, -0.15812769532203674, -1.1341954469680786, 0.1339271068572998, 0.3129049241542816, 0.10625143349170685, -0.26570555567741394, 0.03414244204759598, 0.052458278834819794, -0.7864793539047241, 0.07385531812906265, 0.6143411993980408, 0.38417038321495056, -0.21905262768268585, 0.058113839477300644, 0.7297188639640808, -0.15293757617473602, 0.27539095282554626, 0.46813634037971497, 0.8834782242774963, 0.5597296357154846, 0.39644911885261536, -0.2026168406009674, -0.06597474217414856, -0.9943285584449768, 0.08609739691019058, 0.27829042077064514, 0.6030946969985962, 0.9003589153289795, 0.2802611291408539, -0.5570099353790283, -0.4869765639305115, -0.13099944591522217, 0.566360354423523, 1.3794915676116943, 0.09525115042924881, -0.5642682909965515, -0.7371528744697571, -0.35817283391952515, -0.26634806394577026, 0.2726181745529175, -0.5422201156616211, -0.4774381220340729, -0.5892654657363892, -0.7356055974960327, 0.9090161919593811, 0.3841066062450409, 1.100805401802063, -0.6004257798194885, -0.31413546204566956, 0.14470326900482178, 0.39616888761520386, -0.785235583782196, -0.9189276695251465, 0.5101781487464905, -0.2077091485261917, 0.12430580705404282, 0.09591808170080185, -0.20359466969966888, -0.06450461596250534, -0.6933703422546387, 0.8848389983177185, -0.6805565357208252, -0.41089746356010437, 0.20288123190402985, 0.5373319983482361, -0.6718005537986755, -0.5826451778411865, 0.5272250175476074, 0.1480174958705902, -0.06405279785394669, 0.2724911570549011, 0.3266933262348175, 0.1300726979970932, -0.006077370140701532, -0.3201708495616913, -0.09413932263851166, 0.04041322320699692, 0.27541816234588623, 0.32277393341064453, 0.051451876759529114, 0.13102976977825165, -1.3050004243850708, 0.26716598868370056, 0.030854379758238792, 0.059892721474170685, 0.050258178263902664, -0.6187894344329834, -0.2440832406282425, 0.7480030655860901, -0.5919667482376099, -0.4083801805973053, -0.7244547605514526, 0.4483962655067444, -0.5925319194793701, -0.42907899618148804, 0.23234671354293823, -0.04817089065909386, 0.3372892141342163, -0.0567428395152092, 0.7569804191589355, 0.3212401270866394, 0.07395833730697632, 0.5491701364517212, -0.9877948760986328, 0.538945198059082, 0.26736825704574585, 0.33200013637542725, -0.028906244784593582, -0.16900157928466797, -1.060538411140442, -0.5612895488739014, -0.595754086971283, -0.36576277017593384, -0.20328983664512634, 0.3914393484592438, -0.3502357006072998, -0.9778791069984436, 0.14527249336242676, -1.0047427415847778, -0.45860522985458374, -0.13987091183662415, -0.3584004342556, -0.3609832227230072, -0.938007652759552, -1.1768114566802979, -0.7216641902923584, -0.24697250127792358, -0.5506947636604309, 0.22290851175785065, -0.076343834400177, -0.3352351188659668, -0.9717876315116882, -0.15879370272159576, -0.7883229851722717, 1.1394805908203125, -0.682338535785675, 0.5392259359359741, -0.07936444133520126, -0.3169121742248535, -0.07618696242570877, 0.2169681191444397, 0.5590744018554688, 0.23723851144313812, 0.03308562561869621, -0.7623397707939148, 0.34873753786087036, -0.35490575432777405, -0.10109555721282959, 0.1506527066230774, 0.25333961844444275, 0.8419696092605591, -0.398104190826416, -0.5458991527557373, 0.5046125054359436, 1.5231770277023315, -0.5189750790596008, 0.3392648696899414, 0.05472102016210556, 1.3776123523712158, 0.4695306122303009, -0.5709772706031799, 0.34143832325935364, 0.7736865878105164, 0.26647821068763733, 0.3262999653816223, -0.02059977687895298, -0.07707469910383224, -0.7826765179634094, 0.7189168930053711, 1.8977299928665161, 0.4296378791332245, 0.4145563840866089, -0.8428446650505066, 1.14090895652771, -1.0552423000335693, -1.2632157802581787, 0.7632200717926025, 0.5864853262901306, 0.38017240166664124, -0.6695136427879333, -0.24091845750808716, -0.43567487597465515, 0.5368384122848511, 0.2558372914791107, -0.5224246382713318, -0.5698540806770325, -0.0403997041285038, 0.4028393626213074, 0.3918130695819855, 0.5550641417503357, -0.5014208555221558, 0.7295951843261719, 14.796696662902832, 0.7199626564979553, -0.12491539120674133, 0.4388880729675293, 0.4417599141597748, 0.41642701625823975, -0.33311155438423157, 0.033286821097135544, -1.1350094079971313, 0.11045444756746292, 1.161264181137085, 0.1608821451663971, 0.3663997948169708, 0.03183067589998245, -0.032981984317302704, 0.5598297119140625, -0.807185173034668, 0.6168763041496277, 0.5162663459777832, -1.1649261713027954, 0.07265771180391312, 0.08839534968137741, -0.010606678202748299, 0.15119656920433044, 0.8580490350723267, 0.6848371624946594, 0.7626199126243591, -0.7252089381217957, 0.6749848127365112, 0.5647400617599487, 1.1724438667297363, 0.04183986410498619, 0.19222600758075714, 0.46964016556739807, -1.1391379833221436, 0.043921247124671936, -0.5758121609687805, -0.9744834303855896, 0.16113875806331635, 0.18504741787910461, -0.6463016867637634, -0.2145323008298874, -0.1482122838497162, 1.2976034879684448, 0.35791251063346863, 0.37011316418647766, -0.05703216791152954, 0.8769445419311523, 0.1512209177017212, -0.14392748475074768, 0.2969318628311157, 0.45844197273254395, 0.28269055485725403, 0.6268220543861389, -0.09294313192367554, 0.10207036137580872, 0.07179027795791626, 0.663001537322998, -0.13705898821353912, -0.16239094734191895, -0.45177194476127625, -0.25037649273872375, 0.002843111287802458, 0.6733773350715637, 0.9213420152664185, 0.09783193469047546, -0.28720852732658386, 0.2882661521434784, 0.7623318433761597, -0.01834554225206375, -0.12472826987504959, -0.46685683727264404, 0.455537348985672, -0.3520336151123047, -0.023004529997706413, 0.4428218603134155, -0.46779438853263855, -0.5389591455459595, -0.9180120825767517, 0.11605299264192581, 0.42399659752845764, -0.6597182154655457, -1.118401288986206, 0.7972820997238159, -0.12554651498794556, -0.38699766993522644, 0.13703610002994537, -0.8178423047065735, -0.3565700650215149, 0.23460228741168976, -1.3663499355316162, -0.5245434641838074, 0.13358235359191895, -0.3933912515640259, -0.3581741154193878, -0.006490099709481001, 1.2238274812698364, 0.16186466813087463, -0.393596887588501, 0.24448339641094208, -0.037218619138002396, 0.36334365606307983, -0.2789507210254669, -1.452028751373291, 0.5445654988288879, 0.06916387379169464, -0.1516263633966446, 0.551866888999939, 0.11939749121665955, 0.21751423180103302, -0.6508250832557678, -0.2101895958185196, 0.735897958278656, -0.8590684533119202, -0.3163534700870514, -0.943385124206543, -1.1575424671173096, 0.4664071500301361, 0.634955883026123, -0.1060648038983345, 0.444408118724823, 0.1598747819662094, -0.6876325607299805, -0.29778215289115906, -0.5893808603286743, 0.0706356093287468, 0.696613073348999, -1.0908207893371582, -0.40554702281951904, -0.3050956130027771, 0.10996637493371964, -0.874040424823761, -0.6848199367523193, -0.49926823377609253, 0.23702658712863922, -0.12913942337036133, 0.8741077184677124, -0.36957240104675293, 0.7076488137245178, 0.9038945436477661, 0.08803831785917282, -1.0754234790802002, -0.08701255917549133, -1.0684348344802856, 0.1671435534954071, 0.5115674138069153, 0.34907984733581543, -0.5524320602416992, 0.31396982073783875, 0.851769208908081, 0.059620875865221024, -0.3977124094963074, -0.7562071084976196, -0.6115266680717468, 0.14567190408706665, -0.39988765120506287, 0.4320759177207947, 0.44973134994506836, 0.2974991500377655, 0.2593649923801422, 0.36783015727996826, 0.2303796112537384, 0.20722109079360962, -1.0222219228744507, 0.08262559026479721, 0.06923474371433258, 0.45365065336227417, -0.7602430582046509, -0.8659168481826782, -1.2580909729003906, 0.5025683045387268, -1.5263941287994385, 0.16334375739097595, -0.9868792295455933, -0.3711774945259094, -0.20254002511501312, -0.3305255174636841, 0.042384255677461624, 0.24977155029773712, -0.7092904448509216, -0.3817159831523895, -0.6711929440498352, -0.5453519821166992, 0.996006429195404, 0.6157167553901672, -0.7766526341438293, 0.3875952363014221, -0.2363579422235489, 0.036694008857011795, 0.020437654107809067, 0.3823670446872711, -0.5325562953948975, -0.4346499741077423, -1.2435046434402466, 0.016418322920799255, -0.18150237202644348, 0.15832720696926117, -0.5581009984016418, 0.6423856019973755, 0.37485966086387634, -0.2512081563472748, 0.1785825788974762, 0.2433052510023117, -1.0231587886810303, -0.3741501569747925, 0.29636281728744507, -1.130740761756897, 0.03646121174097061, -0.09127450734376907, -0.30837997794151306, -0.40154412388801575, 0.6210228800773621, 0.2043088972568512, -1.2326740026474, -0.6432284116744995, 0.6629417538642883, -0.48974502086639404, 0.12154705822467804, -0.2218731939792633, -0.27752789855003357, -1.0749940872192383, -0.43698084354400635, 0.048337798565626144, 0.26526257395744324, -0.3638092875480652, 0.8821029663085938, 0.6139110922813416, -1.3532353639602661, -0.017523285001516342, 0.2074015885591507, -0.03211148828268051, 0.3050558865070343, 0.6532096266746521, 0.04789485037326813, -0.11500686407089233, 0.426918625831604, 0.2105323076248169, 0.24528400599956512, -0.6674768328666687, 0.008397866040468216, 0.9940608739852905, -0.3996565341949463, -0.3624216318130493, 0.8966426849365234, -0.12030699849128723, -0.9805094003677368, 0.1142086535692215, -0.9989540576934814, -0.569281816482544, -0.2680664360523224, 0.437858521938324, 0.2925623655319214, -0.3888912796974182, -0.4454691708087921, -0.7192888855934143, 0.26044684648513794, -0.22693823277950287, -0.5245939493179321, 0.653339684009552, -0.13605812191963196, -0.26280033588409424, 0.9276228547096252, 1.3976420164108276, -0.7514637112617493, -0.8048546314239502, -0.8529451489448547, -0.2388731986284256, -0.15993370115756989, 0.21293465793132782, -0.07513338327407837, -0.3220288157463074, 0.9832043647766113, 0.22778908908367157, 0.7106236815452576, -0.28477925062179565, -0.13982081413269043, 0.15759722888469696, 0.34285107254981995, -0.0017165234312415123, -0.5451905727386475, -0.4315069019794464, 1.4530141353607178, 1.2737655639648438, -0.4447741210460663, -0.1061379462480545, -0.4659908413887024, -0.7048835158348083, 0.5764198899269104, 0.32086166739463806, -0.2730138897895813, 0.8963087201118469, -0.11386758089065552, 0.014649428427219391, 0.09949817508459091, -1.1126129627227783, -0.3029416501522064, 0.7384273409843445, 1.4105113744735718, 0.6203168630599976, 0.32651761174201965, 0.6134529113769531, 0.7480344772338867, -0.06789530813694, -0.26566365361213684, 0.6848174333572388, 0.19035112857818604, -0.09629341959953308, 0.21879510581493378, 0.3430749177932739, 0.6249476075172424, -0.8115556240081787, -0.504964292049408, 0.2256557196378708, 0.31769636273384094, -0.08494583517313004, 0.3501107394695282, 0.9012714624404907, -0.0071894098073244095, 0.44769787788391113, 0.31789860129356384, 0.36687618494033813, -0.7700446248054504, -0.16286054253578186, -0.5940906405448914, -0.6208646297454834, -0.21552826464176178, -0.3751412332057953, -0.8415882587432861, -0.6997084021568298, 0.11328744888305664, 0.1364019364118576, 0.022623371332883835, 0.596782922744751, 1.263063669204712, 0.5482873320579529, 0.532446026802063, 0.03546170890331268, -0.33573684096336365, -0.5273014307022095, -0.9411342144012451, -0.21966427564620972, -0.19618083536624908, 0.27093154191970825, -0.3496095836162567, -0.13896213471889496, -0.021515944972634315]}, "authors": [{"authorId": "34920835", "name": "Ayan Sengupta"}, {"authorId": "2249917924", "name": "Md. Shad Akhtar"}, {"authorId": "2249914540", "name": "Tanmoy Chakraborty"}], "references": [{"paperId": "3f60b7b86908ee2a6024539675b7c0b05a33871b", "title": "A Theoretical View on Sparsely Activated Networks"}, {"paperId": "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed", "title": "Sparse is Enough in Scaling Transformers"}, {"paperId": "4b0541eccd8f98852d6807a14fbac17f775c7b40", "title": "Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\u00f6m Method"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "4a54d58a4b20e4f3af25cea3c188a12082a95e02", "title": "Transformer Feed-Forward Layers Are Key-Value Memories"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "4adb5927adc628ce2acd6371ebefab9c8053741c", "title": "Entropy and the Brain: An Overview"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6349901133797ab211cfd8c8dfcdc57828f20dfb", "title": "A Mathematical Theory of Attention"}, {"paperId": "bf39265558138d05f78ca4569e8d55f3f3627bba", "title": "Deep Isometric Learning for Visual Recognition"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "45988d39ab1b0e5199e1f0f31952760bc763e611", "title": "The Lipschitz Constant of Self-Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "ca4ecf116a9b97ce525a01f3f51117877688ddf5", "title": "Similarity Analysis of Contextual Word Representation Models"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "b3ea2d9c8e5ea3b87ace121f0bece71565abc187", "title": "Quantifying the Carbon Emissions of Machine Learning"}, {"paperId": "88b8a12f8025af3289023410eb51c9860961f2f4", "title": "Trivializations for Gradient-Based Optimization on Manifolds"}, {"paperId": "112fd54ee193237b24f2ce7fce79e399609a29c5", "title": "The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives"}, {"paperId": "79dc04d2a429e53ab61d00e87aa9b55ed9d840e8", "title": "Hippocampal Network Reorganization Underlies the Formation of a Temporal Association Memory"}, {"paperId": "ad6309d1ea001098189425f54d069ef12abcb583", "title": "Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10, 000-Layer Vanilla Convolutional Neural Networks"}, {"paperId": "0d3c46a3cbfe06cec259fec954b6ff6df6c1a566", "title": "Learning long-range spatial dependencies with horizontal gated-recurrent units"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "bdfe20258f8a17316db1f3151e43a465b2065a1e", "title": "Brain entropy and human intelligence: A resting-state fMRI study"}, {"paperId": "9edf1a25ebc182355c5584fb7f5d234e75ccf3d1", "title": "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice"}, {"paperId": "6a06a44dc8538a226df169e5a7ed659e5db3f9e4", "title": "Reversible Architectures for Arbitrarily Deep Residual Neural Networks"}, {"paperId": "3a6d4cd0768ae8768e733280d362bdb4d25924e7", "title": "The Reversible Residual Network: Backpropagation Without Storing Activations"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "6e997fec1412abb4b630d0e6d4df95813a01e093", "title": "Exponential expressivity in deep neural networks through transient chaos"}, {"paperId": "b2f7c95f1ae20b2b9c15a0f846a70293d499aca8", "title": "Neuronal factors determining high intelligence"}, {"paperId": "f63e917638553414526a0cc8550de4ad2d83fe7a", "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"}, {"paperId": "d181ee5ea23ce57b0e673dfc5724dcc316013429", "title": "Why are deep nets reversible: A simple theory, with implications for training"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "68e5cee9e78e366b3ad52d7186d59d9cf9b22773", "title": "Simple models of human brain functional networks"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "0ac695a0e3d220ed51808f9a4893cc4f9497ecf1", "title": "Odor Representations in Olfactory Cortex: \u201cSparse\u201d Coding, Global Inhibition, and Oscillations"}, {"paperId": "e95ec6ee6952e1f8737f398fd82a912642b1e4fa", "title": "Quantum mechanics in the brain"}, {"paperId": "3537fcd0ff99a3b3cb3d279012df826358420556", "title": "A global geometric framework for nonlinear dimensionality reduction."}, {"paperId": "df1242793c6204e2703ccb2a476171ad0e478993", "title": "Multilayer Neural Networks: One or Two Hidden Layers?"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "bea5780d621e669e8069f05d0f2fc0db9df4b50f", "title": "Convolutional Deep Belief Networks on CIFAR-10"}, {"paperId": null, "title": "(cid:80) ni =1 x 2 i =1 i =1 Being a convex sum, K 2 M \u2264 \u03a3 21 , which completes the proof"}, {"paperId": null, "title": "= i =1 \u03a3 Hence, K 2 = sup n (cid:88) \u03a3 2 x 2"}, {"paperId": null, "title": "2023. On emergence of activation sparsity in trained transformers"}, {"paperId": null, "title": "R n \u00d7 n , \uf8eb (cid:80) nj =1 M j, 1 (cid:80)"}, {"paperId": null, "title": "long sequence time-series forecasting"}, {"paperId": null, "title": "Squaring both the side, we decompose M as U \u03a3 V T , where U and V are orthogonal matrices"}, {"paperId": null, "title": "stochastic matrix) The largest absolute value any eigenvalue of a square stochastic matrix equal to 1 . Proof. For any square stochastic matrix M"}, {"paperId": null, "title": "Fast Convergence at Large Depth"}, {"paperId": null, "title": "2022. Reversible vision transformers"}, {"paperId": null, "title": "Lemma 2 (Activation bound of linear maps) For a matrix M \u2208 R n \u00d7 m , K M is same as the largest absolute singular value, under || . || 2"}]}