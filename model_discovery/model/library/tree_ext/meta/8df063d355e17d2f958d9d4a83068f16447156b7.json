{"paperId": "8df063d355e17d2f958d9d4a83068f16447156b7", "title": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks", "abstract": "This study investigates how weight decay affects the update behavior of individual neurons in deep neural networks through a combination of applied analysis and experimentation. Weight decay can cause the expected magnitude and angular updates of a neuron's weight vector to converge to a steady state we call rotational equilibrium. These states can be highly homogeneous, effectively balancing the average rotation -- a proxy for the effective learning rate -- across different layers and neurons. Our work analyzes these dynamics across optimizers like Adam, Lion, and SGD with momentum, offering a new simple perspective on training that elucidates the efficacy of widely used but poorly understood methods in deep learning. We demonstrate how balanced rotation plays a key role in the effectiveness of normalization like Weight Standardization, as well as that of AdamW over Adam with L2-regularization. Finally, we show that explicitly controlling the rotation provides the benefits of weight decay while substantially reducing the need for learning rate warmup.", "venue": "", "year": 2023, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This study investigates how weight decay affects the update behavior of individual neurons in deep neural networks through a combination of applied analysis and experimentation, offering a new simple perspective on training that elucidates the efficacy of widely used but poorly understood methods in deep learning."}, "embedding": {"model": "specter_v2", "vector": [-0.3774893879890442, 0.6272139549255371, -0.5278780460357666, -0.024845650419592857, 0.32175689935684204, 0.4325386583805084, 0.6994549632072449, -0.8354426026344299, -0.8862996101379395, -0.41038578748703003, 0.49044153094291687, 0.17263907194137573, 0.14040781557559967, -0.14181660115718842, -0.4206937849521637, -0.6583780646324158, -0.8887293338775635, 0.07426150143146515, -0.07161768525838852, -0.37325724959373474, -0.40592241287231445, -0.2931453287601471, -0.9324238300323486, -0.12495715171098709, -0.24716509878635406, 0.9979963898658752, -0.4808209538459778, 0.9428879618644714, -0.2895719110965729, 0.7294564843177795, 0.6232050061225891, -0.397248774766922, 1.0141717195510864, -0.1984546184539795, -0.3460565209388733, -0.1808721125125885, 0.5995216965675354, -0.6741382479667664, -0.7376797795295715, 1.045517086982727, -0.30772218108177185, 0.31061676144599915, 0.2181396186351776, -0.6314933896064758, 0.08515006303787231, 0.2476438283920288, 0.35384634137153625, 0.7897414565086365, -0.8217530250549316, -0.4049934446811676, 0.9735434055328369, -0.9128608107566833, 0.6742451786994934, 1.239780306816101, 0.8938034176826477, 0.533720850944519, -0.34851279854774475, -0.8978610634803772, 0.9211344718933105, -0.31596508622169495, -0.5166031718254089, 0.04167943447828293, 0.18497824668884277, -0.2878502309322357, 0.9150368571281433, -0.5660637617111206, 0.15550172328948975, 0.48472917079925537, 0.40042948722839355, 1.1424461603164673, -0.1058816984295845, -0.7180091738700867, 0.028587326407432556, 0.15799565613269806, 0.5660746097564697, 0.6589652895927429, 0.27742424607276917, 0.31788820028305054, -1.1850281953811646, 0.07707339525222778, 0.3556419014930725, -0.015251951292157173, -0.026412969455122948, -0.6197371482849121, -0.08392126113176346, 0.6286441683769226, 0.6087740063667297, 0.11363174021244049, -0.8309645652770996, 1.4324480295181274, 0.45723140239715576, 0.6402510404586792, 0.5444469451904297, 0.02780485339462757, -0.2749413549900055, 0.4217183291912079, -0.5711811184883118, 0.32571473717689514, 0.004609960131347179, 0.2839202582836151, -0.08745044469833374, 0.32133641839027405, -0.31861454248428345, 0.44375088810920715, 1.598608374595642, -0.30206120014190674, 0.8143379092216492, -0.5267341136932373, 0.39591023325920105, -0.5293676257133484, 0.020497769117355347, -1.0244104862213135, -0.3298986554145813, -0.8842300772666931, -0.9803354144096375, -0.31353816390037537, -0.3606777489185333, 0.043759748339653015, -0.8816001415252686, 1.157165288925171, 0.12387697398662567, 0.024731162935495377, -0.09460809826850891, 0.9668922424316406, 0.11524176597595215, 0.6644334197044373, 0.2100171148777008, 0.21567192673683167, 0.8235254287719727, -0.5161169767379761, -0.7996249794960022, -0.8314266204833984, 0.4029753804206848, -0.11508388817310333, -0.026729566976428032, -0.15718905627727509, -1.4798752069473267, -0.7469189763069153, -1.2824708223342896, 0.6762399673461914, -0.17512637376785278, 0.08415942639112473, 1.375980019569397, 0.2929251492023468, -0.5125815868377686, 1.3942992687225342, -0.9007271528244019, -0.31691864132881165, 0.6860485672950745, 0.385501891374588, 0.1031438335776329, 0.054944202303886414, -0.5315912961959839, 0.8292828798294067, 0.2747619152069092, -0.46449872851371765, 0.18104027211666107, -0.8855230808258057, -0.4512239396572113, -0.303301602602005, -0.2585228979587555, -1.074671745300293, 1.1998745203018188, -0.3492388427257538, -1.3061790466308594, 0.34048810601234436, 0.2789668142795563, 0.05670817941427231, 0.6551256775856018, -0.13807812333106995, -0.27699583768844604, -0.3972729444503784, -0.611201822757721, 0.6598589420318604, 0.30767717957496643, -0.5437749624252319, 0.03089405596256256, -0.30838537216186523, -0.7755012512207031, -0.14693184196949005, -0.5765998959541321, 0.48334136605262756, -0.1604451835155487, -0.31754031777381897, 1.0611014366149902, 0.36441656947135925, 0.21904894709587097, -0.018889982253313065, 0.11667341738939285, -0.880775511264801, 0.4450831413269043, 0.08264563232660294, 0.6437250375747681, -0.5739555358886719, -0.6482493281364441, 0.2382490336894989, 0.04506296664476395, -0.24575072526931763, -0.546963095664978, 0.12671002745628357, -0.6700335144996643, 0.05278582498431206, 0.1907016783952713, -1.1781798601150513, -0.025661064311861992, 0.12425079941749573, -0.6013538241386414, 0.4469201862812042, 0.1933218091726303, 0.9221034049987793, -0.7216260433197021, 0.5463470816612244, -0.16599255800247192, 0.17108437418937683, -0.9891974925994873, 1.157209873199463, 0.4504537880420685, -0.21923841536045074, 0.49460655450820923, -0.43005335330963135, 0.28430676460266113, -0.07404555380344391, 0.43012842535972595, -0.8344709873199463, 0.13508392870426178, 0.5736709237098694, -1.0358948707580566, 1.038860559463501, -0.24859459698200226, 0.695972204208374, 0.2093845009803772, -1.1630336046218872, -0.03432931378483772, 0.3134232759475708, 0.4031885862350464, -0.47184622287750244, 0.8831439018249512, 0.29534679651260376, -0.6304311156272888, 0.6258966326713562, 0.49731218814849854, 0.4443027377128601, -0.08908513188362122, 0.36248093843460083, 1.2909713983535767, -0.1556333750486374, 0.41280797123908997, 0.11579976975917816, 0.3415663540363312, 0.4365338087081909, -0.05508982017636299, -0.4041667580604553, -0.524764895439148, -0.968558669090271, -0.17378117144107819, 0.34505680203437805, 0.4842364490032196, 1.080023169517517, 0.44474783539772034, -1.145678997039795, -0.4591377377510071, -0.6210538744926453, 0.42356550693511963, 1.1767873764038086, -0.030964471399784088, 0.19371627271175385, -0.6866528987884521, 0.14828187227249146, -0.12703989446163177, -0.08965912461280823, -0.8493910431861877, -0.8613779544830322, -0.7560790181159973, -1.7583976984024048, 0.3552609086036682, -0.0025852969847619534, 0.846827507019043, 0.034635260701179504, -0.3232313394546509, -0.30980032682418823, 0.9086938500404358, -0.21403992176055908, -0.3229229152202606, 0.6127009987831116, -0.90581214427948, 0.07646850496530533, -0.07916346192359924, 0.15812982618808746, 0.22434279322624207, -0.5759124159812927, 0.9695795774459839, -0.5250113010406494, -0.06844940781593323, 0.22566772997379303, 1.228808045387268, -1.1853116750717163, -0.41845646500587463, 0.30885806679725647, 0.6437267065048218, 0.17732658982276917, -0.4193388521671295, 0.06916262209415436, -0.6353372931480408, 0.23774752020835876, -0.7206907868385315, 0.04034984111785889, -0.1049434095621109, 0.10776983201503754, 0.2546481490135193, -0.7172649502754211, 0.37086230516433716, -0.6477180123329163, 1.4260168075561523, 0.021394871175289154, -0.4576070308685303, -0.04178324714303017, -0.9771104454994202, 0.28010180592536926, -0.05656444653868675, -0.8802943825721741, -0.16339269280433655, -1.2194554805755615, 0.39148929715156555, -0.2547118663787842, 0.36144810914993286, -0.2555992603302002, 1.020346760749817, -0.028465885668992996, 0.5966984629631042, -0.6218258142471313, 0.9422509670257568, -0.16924649477005005, 0.49893417954444885, -0.9108386039733887, 0.8898162841796875, 0.06702052056789398, 0.05648290738463402, 0.29846420884132385, -0.012194527313113213, -0.12997247278690338, -0.6129862666130066, -0.19888317584991455, 0.10123568773269653, -0.6459743976593018, 0.046311426907777786, -0.5315057635307312, -0.9651375412940979, 0.06271712481975555, -0.4664347171783447, -0.24480438232421875, -0.36197590827941895, 0.3141518533229828, -0.17744764685630798, -1.5001908540725708, -1.0932035446166992, -0.42920243740081787, -0.9307900667190552, -0.9320589303970337, -0.29382339119911194, 0.27910879254341125, -0.6528419852256775, -0.5199998617172241, 0.00551819521933794, -0.9283210635185242, 1.287003517150879, -0.6530362367630005, 0.3449077010154724, 0.1710944026708603, 0.13145992159843445, -0.5126112699508667, 0.2649900019168854, 0.7376981973648071, -0.5250493884086609, 0.42091426253318787, -0.4248198866844177, 0.21710605919361115, -0.39816802740097046, -0.4527791142463684, 0.13443820178508759, 0.3305993676185608, 0.40188950300216675, -0.09657640755176544, 0.05506052076816559, 0.7570945024490356, 1.05227530002594, -0.9110825657844543, -0.2622106075286865, 0.49230650067329407, 0.5598452687263489, 0.30435511469841003, -0.7702874541282654, 0.45818984508514404, 0.028339941054582596, 0.3858312666416168, 0.6063909530639648, -0.363025963306427, -0.6889001727104187, -0.5092243552207947, -0.09615541249513626, 1.6902728080749512, 0.21718187630176544, 0.2179756462574005, -0.43341222405433655, 0.2267507165670395, -1.4675060510635376, -0.6851364374160767, 0.7443375587463379, 0.8530056476593018, 0.47335636615753174, -0.24961918592453003, -0.2391083687543869, -0.18630270659923553, 0.19032970070838928, 0.29330742359161377, -0.3710246682167053, -0.5462124943733215, 0.13703382015228271, 0.31563451886177063, 1.026465654373169, 0.48896360397338867, 0.029425514861941338, 0.07314448058605194, 14.925722122192383, 0.16180351376533508, -0.38896241784095764, 0.6658890247344971, 0.64228755235672, -0.30392593145370483, -0.22134797275066376, -0.5702760815620422, -0.9194608330726624, 0.21035683155059814, 0.650834858417511, 0.3771442472934723, 0.6525707244873047, 0.24231135845184326, -0.2096908688545227, 0.37906983494758606, -0.10730190575122833, 0.9666563868522644, 0.39036452770233154, -1.5053218603134155, -0.011094960384070873, -0.1425376981496811, 0.9493308663368225, 1.3319581747055054, 0.6140374541282654, 0.4944591522216797, 0.805141031742096, 0.1405707746744156, 0.9098662734031677, 0.6604930758476257, 0.4484671652317047, -0.22843068838119507, 0.6244903802871704, 0.18450330197811127, -0.2848983108997345, 0.001065855729393661, -0.37884655594825745, -1.0732711553573608, 0.18903902173042297, 0.2351250797510147, -0.06522193551063538, -0.5749676823616028, 0.11864694952964783, 0.11308173090219498, 0.32505854964256287, 0.302104115486145, -0.10289637744426727, 0.6350563764572144, -0.7103197574615479, 0.24616098403930664, 0.3591177761554718, 0.14934667944908142, -0.1752767562866211, -0.016460133716464043, 0.027795813977718353, -0.3550519347190857, -0.25022274255752563, 0.472085565328598, -0.9697650671005249, -0.08766205608844757, 0.45678988099098206, 0.10755456984043121, -0.24863144755363464, 0.7095096707344055, 0.44687333703041077, -0.003902004100382328, 0.2791556119918823, 0.15411755442619324, 0.7032036185264587, 0.20420339703559875, -0.12540756165981293, -0.15706121921539307, 1.2672337293624878, -0.5485051274299622, -0.5106433033943176, 0.19019398093223572, -0.6867157220840454, -0.7043383717536926, -0.6455579996109009, -0.007518244441598654, 0.22140389680862427, -0.8830388784408569, -0.7689288258552551, 0.6445060968399048, 0.04097069427371025, 0.02202186919748783, 0.7063560485839844, -0.9768925905227661, -0.48958367109298706, 0.05944400653243065, -1.1281208992004395, 0.06132121756672859, -0.18102028965950012, -0.5639616250991821, -0.610984742641449, -0.2613407075405121, 0.3924317955970764, 0.3585120141506195, -0.9633148312568665, 0.3062606751918793, 0.20932255685329437, -0.3619155287742615, -0.10516885668039322, -0.5885325074195862, 0.7677934765815735, 0.23163267970085144, 0.19274847209453583, 0.7021445035934448, 0.1681797206401825, 0.6132384538650513, -0.5670216679573059, 0.3686910569667816, -0.01202236395329237, -0.5943359136581421, 0.12589134275913239, -0.07114708423614502, -0.8514376878738403, 0.22844164073467255, 0.5955925583839417, 0.3507288992404938, 0.137490376830101, 0.16991648077964783, -0.4859764873981476, -0.7527782320976257, -0.41052988171577454, 0.23921474814414978, 0.32160207629203796, -0.2526332139968872, -0.46919187903404236, -0.0810970813035965, -0.005364260170608759, -1.015015721321106, -0.32725033164024353, 0.0147428372874856, 0.30191922187805176, 0.007851339876651764, 1.1875814199447632, -1.0634466409683228, 0.24079890549182892, 0.5719941854476929, -0.026676060631871223, -0.9417126774787903, -0.10775094479322433, -0.7697111964225769, 0.21924781799316406, 0.022124221548438072, 0.21711945533752441, -0.7676535844802856, 0.5019392371177673, 0.6197242736816406, 0.04882563650608063, -0.6877550482749939, -0.7733754515647888, -0.06669836491346359, -0.10943113267421722, -0.9338818192481995, 0.23717786371707916, -0.21566730737686157, -0.4424310326576233, -0.23413920402526855, 0.41686081886291504, 0.37609386444091797, -0.20818595588207245, -1.0432995557785034, 0.3350154459476471, -0.021218111738562584, 0.1526854932308197, -1.072197675704956, -0.880102813243866, -1.4267677068710327, -0.1968369483947754, -1.3527894020080566, -0.44737187027931213, -1.1250383853912354, -0.8390315175056458, 0.16772590577602386, -0.6986315846443176, -0.08509822189807892, 0.5293681025505066, 0.3948352038860321, -0.3352060616016388, 0.0381670705974102, -0.49085772037506104, 0.843945324420929, 0.3865281343460083, -0.5129505395889282, -0.06474684923887253, -0.0248869601637125, 0.2794533967971802, 0.7401589155197144, 0.8198276162147522, -0.5733901262283325, -0.5160611867904663, -1.4751795530319214, 0.9102920293807983, -0.9345740079879761, -0.3541870415210724, -0.9642358422279358, 0.6814361810684204, 0.34521928429603577, 0.3929644227027893, 0.07467816770076752, 0.22395198047161102, -0.9390767812728882, -0.1527620255947113, 0.4321122169494629, -0.4007154107093811, 0.2590048015117645, 0.0895182341337204, -0.4774980843067169, 0.008346968330442905, 0.7691723704338074, 0.39685314893722534, -0.4494918882846832, -0.1575348824262619, 0.4781494736671448, -0.5039965510368347, 0.07762971520423889, -0.19428688287734985, -0.3275154232978821, -1.0418448448181152, -0.10574334859848022, -0.004262152127921581, 0.2335822731256485, -0.461923211812973, 0.8886124491691589, -0.06901603192090988, -1.6145638227462769, 0.5111843943595886, 0.46313825249671936, -0.1562546342611313, -0.16118676960468292, 0.1599792242050171, 0.36227986216545105, -0.9048469662666321, -0.0787302553653717, -0.00031712540658190846, 0.10768764466047287, -0.3340267241001129, -0.5041067004203796, 0.9142630696296692, 0.05665634945034981, -0.09110192954540253, 1.3899366855621338, -0.42151904106140137, -1.3536889553070068, 0.24294738471508026, -1.1149160861968994, 0.05811179429292679, 0.10669486224651337, 0.218253493309021, 0.5232369899749756, 0.40211188793182373, 0.40917569398880005, -0.17797064781188965, 0.15566566586494446, 0.11074091494083405, -0.3441008925437927, 0.4405372738838196, -0.18975408375263214, -0.16093577444553375, 0.8094244599342346, 0.9560553431510925, -0.9639537930488586, -1.0942715406417847, -0.7117727994918823, -0.43033507466316223, -0.23232078552246094, 0.46258223056793213, -0.03962403163313866, -1.4201780557632446, 0.4638509750366211, 0.99407958984375, -0.03162744268774986, -0.1153353825211525, -0.12655606865882874, -0.19238507747650146, 0.5696710348129272, -0.1757194846868515, -0.8653112053871155, -0.42103996872901917, 0.856946587562561, 0.9878057241439819, -0.4283824563026428, 0.841372549533844, 0.4107523560523987, -0.23462487757205963, 0.987410843372345, 0.3158131539821625, -0.41716182231903076, 0.7632778882980347, -0.3519746661186218, 0.5072382092475891, -0.044168923050165176, -0.9856728315353394, 0.19867971539497375, 0.806414783000946, 0.3213367462158203, 0.5590864419937134, 0.24736399948596954, -0.03739989921450615, 0.7847886085510254, -0.26679369807243347, -0.15491162240505219, 0.29255911707878113, 0.4134720265865326, -0.08715037256479263, 0.35100147128105164, 0.10674811154603958, 0.5383437275886536, -0.7451847195625305, 0.35588255524635315, 0.3072836995124817, 0.8645226955413818, 0.19971539080142975, -0.057381439954042435, 1.0599204301834106, 0.05051715299487114, 0.9483513236045837, 0.22544749081134796, 0.6309720873832703, 0.0796801820397377, -0.24763484299182892, -0.46464455127716064, -0.8160319328308105, -0.24211035668849945, -0.25325483083724976, -0.36273086071014404, -0.18294328451156616, -0.3411642014980316, 0.06737348437309265, -0.06159982830286026, 1.0017907619476318, 0.7064428329467773, 0.3824375867843628, 0.9187761545181274, -0.17601457238197327, -0.7806393504142761, -0.7307969927787781, -0.8850064873695374, 0.29454630613327026, -0.09730352461338043, -0.3071886897087097, -0.4947717487812042, -0.5651702880859375, -0.5543678998947144]}, "authors": [{"authorId": "122055646", "name": "Atli Kosson"}, {"authorId": "2219037377", "name": "Bettina Messmer"}, {"authorId": "2456863", "name": "Martin Jaggi"}], "references": [{"paperId": "bb857c72e7c75fc3d4b44b1dcaa66c62ea10a2e1", "title": "Analyzing and Improving the Training Dynamics of Diffusion Models"}, {"paperId": "2f1df8c0806d5756929a09aba60ac7f43778a938", "title": "Why Do We Need Weight Decay in Modern Deep Learning?"}, {"paperId": "4785645a36baef0ec0cbda453fa132b4bc0c5567", "title": "When and Why Momentum Accelerates SGD: An Empirical Study"}, {"paperId": "f5dd8ab972e48b41121664ae05af0da17f1731a1", "title": "Ghost Noise for Regularizing Deep Neural Networks"}, {"paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458", "title": "Symbolic Discovery of Optimization Algorithms"}, {"paperId": "824de92c9323809222c1b04eaf9ca067c0f58c57", "title": "Training Scale-Invariant Neural Networks on the Sphere Can Happen in Three Regimes"}, {"paperId": "00a836a546fae1e7afaa6c51c4d4829f2060575c", "title": "On the SDEs and Scaling Rules for Adaptive Gradient Algorithms"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "00754af9cd0eaf613a99b13fb19e422b09beeee4", "title": "Robust Training of Neural Networks using Scale Invariant Architectures"}, {"paperId": "73c85c91abc67aa875ae9aef1e635664041353b4", "title": "FixNorm: Dissecting Weight Decay for Training Deep Neural Networks"}, {"paperId": "55fac1b87b453d40f6f997a51a9eb9dd009b4ee8", "title": "On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)"}, {"paperId": "12dc981a26290e18439f865464605dac639b08d3", "title": "Learning by Turning: Neural Architecture Aware Optimisation"}, {"paperId": "c16835c8e535ebd9c10a550ca9455fe384a14449", "title": "High-Performance Large-Scale Image Recognition Without Normalization"}, {"paperId": "feeae38fd404fdc17cad19d80461843059216fde", "title": "Characterizing signal propagation to close the performance gap in unnormalized ResNets"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "49acd17fe6c7b05c5887f5c4f17e6b5a89bd64c8", "title": "On the Overlooked Pitfalls of Weight Decay and How to Mitigate Them: A Gradient-Norm Perspective"}, {"paperId": "7dfac3bf747fefc62f5bdaa124a609992a18b974", "title": "Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate"}, {"paperId": "642bc89ef9efc79b6c8b0b6bd9295d5bc11d594b", "title": "A spherical analysis of Adam with Batch Normalization"}, {"paperId": "e11e8d81c68cc782f564ed78e595b66790719804", "title": "AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "00521e876d109fdcf344e027f218a768499060c8", "title": "An Exponential Learning Rate Schedule for Deep Learning"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "b9aecad9e5dd0ed2023e9f82b3d7fee935ef3536", "title": "Online Normalization for Training Neural Networks"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "caa2704320e3742bc611c30092acd7a7eb87d5d4", "title": "Weight Standardization"}, {"paperId": "eefa0df7c5678fa6004f8b48dbbc1c2696702fee", "title": "An Empirical Model of Large-Batch Training"}, {"paperId": "b2c8e834ac5f7be68b9ca3691d39925036dd74a3", "title": "Measuring the Effects of Data Parallelism on Neural Network Training"}, {"paperId": "ba618ec05a9dbef75310c5e4bcce8a559e0270b5", "title": "Three Mechanisms of Weight Decay Regularization"}, {"paperId": "a82fc0115c1802d48d352b35595204738fad84f0", "title": "Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes"}, {"paperId": "d08b35243edc5be07387a9ed218070b31e502901", "title": "Group Normalization"}, {"paperId": "48f0b0ab8ccdca861e374f26c3e54e1720cfb22e", "title": "Norm matters: efficient and accurate normalization schemes in deep networks"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "d80d5862f5406a46225e91a71bb046eda451c430", "title": "Projection Based Weight Normalization for Deep Neural Networks"}, {"paperId": "1fc97231d1a2139ae9f54c15a530b4714b6701b4", "title": "Centered Weight Normalization in Accelerating Training of Deep Neural Networks"}, {"paperId": "1e3d18beaf3921f561e1b999780f29f2b23f3b7d", "title": "Large Batch Training of Convolutional Networks"}, {"paperId": "7379455504274d7ff46a37bb4a0ccf81b60df86f", "title": "L2 Regularization versus Batch and Weight Normalization"}, {"paperId": "be0ef77fb0345c5851bb5d297f3ed84ae3c581ee", "title": "Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "3d2c6941a9b4608ba52b328369a3352db2092ae0", "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "6fe02ad979baad659f04c3376a77dbb2cb4699a5", "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "bb9608d6904ddf57d307a13b1f2e372d742836e3", "title": "Fast Mixing of Stochastic Gradient Descent with Normalization and Weight Decay"}, {"paperId": null, "title": "2022b). A.3 PROJECTED OPTIMIZATION Some existing works are based on projections onto a sphere but do not scale the update to be proportional to the weight magnitude"}, {"paperId": null, "title": "Rotational Optimizer Variants keep the relative learning rate at a fixed level and do not suffer from the effect"}, {"paperId": "5e94cb6d4d6d0c61439da0658929de5bd4a8d60a", "title": "Spherical Motion Dynamics: Learning Dynamics of Normalized Neural Network using SGD and Weight Decay"}, {"paperId": null, "title": "How Weight Decay Balances Learning Across Neural Networks"}, {"paperId": null, "title": "Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2019) also known as Centered Weight Normalization (Huang et al., 2017), where each filter is scale and shift invariant"}, {"paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82", "title": "Deep Learning"}, {"paperId": null, "title": "D-1 We pre-process the data by normalizing it with mean (0 . 49140 . 48220 . 4465) and std (0 . 20230 . 19940 . 2010)"}, {"paperId": null, "title": "D-2 We use the standard data augmentation from He et al."}, {"paperId": "81aace0e90c6a962059b117c24db0d856f340f41", "title": "Report on the 11th IWSLT evaluation campaign"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "4b53e3f719ff983eef867c6d8deac5dbe38aecb4", "title": "Some methods of speeding up the convergence of iteration methods"}, {"paperId": null, "title": "Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks baseline training exhibited a smaller rotation towards the end"}, {"paperId": null, "title": "We focus on the case where the weights are scale-invariant, defining \u02dc g = \u2225 \u03c9 \u2225 g as the gradient corresponding to a unit weight norm based on the inverse proportionality from Equation 6"}, {"paperId": null, "title": "we list additional details referenced in the"}, {"paperId": null, "title": "D-6 For this experiment we use the nanoGPT library (Karpathy, 2023)"}]}