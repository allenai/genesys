{"paperId": "3f877562995d1408b0b3abd5dfbbe8eeecb6061e", "title": "From Understanding to Utilization: A Survey on Explainability for Large Language Models", "abstract": "Explainability for Large Language Models (LLMs) is a critical yet challenging aspect of natural language processing. As LLMs are increasingly integral to diverse applications, their\"black-box\"nature sparks significant concerns regarding transparency and ethical use. This survey underscores the imperative for increased explainability in LLMs, delving into both the research on explainability and the various methodologies and tasks that utilize an understanding of these models. Our focus is primarily on pre-trained Transformer-based LLMs, such as LLaMA family, which pose distinctive interpretability challenges due to their scale and complexity. In terms of existing methods, we classify them into local and global analyses, based on their explanatory objectives. When considering the utilization of explainability, we explore several compelling methods that concentrate on model editing, control generation, and model enhancement. Additionally, we examine representative evaluation metrics and datasets, elucidating their advantages and limitations. Our goal is to reconcile theoretical and empirical understanding with practical implementation, proposing exciting avenues for explanatory techniques and their applications in the LLMs era.", "venue": "arXiv.org", "year": 2024, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This survey underscores the imperative for increased explainability in LLMs, delving into both the research on explainability and the various methodologies and tasks that utilize an understanding of these models."}, "embedding": {"model": "specter_v2", "vector": [0.2213413566350937, 0.9728536009788513, -0.38031283020973206, -0.1132604330778122, -0.3677220046520233, -0.3569091558456421, 0.7520686388015747, 0.027467651292681694, 0.07828231900930405, -0.10610436648130417, 0.44442883133888245, -0.5844797492027283, 0.15741068124771118, 0.2445937544107437, -0.23990395665168762, 0.4020726978778839, -0.6356891989707947, 0.42874664068222046, -0.30215132236480713, -0.12257307767868042, -0.3853300213813782, -0.568053126335144, -0.7698312401771545, 0.16570192575454712, 0.47237324714660645, 0.05414759740233421, 0.3620799481868744, 0.8071998357772827, -0.4645178020000458, 0.523609459400177, 0.43930402398109436, -0.4694370627403259, -0.24217554926872253, -0.07912027835845947, -0.3096730709075928, 0.14211106300354004, 0.008175992406904697, -0.4673827588558197, -0.5037279725074768, 0.537635087966919, -0.6522022485733032, 0.18315282464027405, 0.6340369582176208, -1.2996697425842285, -0.6903301477432251, 1.295175552368164, 0.6666455268859863, 0.9446289539337158, -0.20472556352615356, -0.5649823546409607, 1.8256608247756958, -1.3447651863098145, 0.497271329164505, 1.7058688402175903, 0.24246062338352203, 0.5071769952774048, -0.7731621861457825, -0.638081431388855, 0.5513120293617249, -0.015500190667808056, -0.8353700041770935, -0.3391028344631195, -0.38541269302368164, 0.012379644438624382, 2.088419198989868, -0.24153487384319305, -0.1918599009513855, 0.41120147705078125, -0.13452249765396118, 1.58542001247406, -0.04051676392555237, -0.8473418951034546, -0.4562188684940338, 0.23727941513061523, 0.2519071102142334, 1.0280704498291016, -0.21525758504867554, 0.31486210227012634, -0.7812769412994385, -0.07140257954597473, 0.37195879220962524, -0.3422311842441559, -0.49454164505004883, -0.1607777178287506, -0.4299595355987549, 1.058752417564392, 0.10026425868272781, 0.7565728425979614, 0.0435129776597023, 0.7268807291984558, 0.3347380757331848, 0.4354510009288788, -0.1439920961856842, 0.47143426537513733, -0.5592261552810669, 0.42829766869544983, -0.9248749613761902, 0.37801679968833923, 0.10172976553440094, 0.9532589912414551, -0.22573024034500122, 0.4368922710418701, -0.5614338517189026, 0.015722237527370453, 1.5111831426620483, 0.18204918503761292, 0.6855191588401794, -0.8462049961090088, 0.07304160296916962, -0.5363672971725464, 0.10026232898235321, -0.4634034037590027, -0.27088525891304016, -0.3689365088939667, -0.38294342160224915, -1.404125690460205, -0.3593573272228241, 0.2915427088737488, -1.0440706014633179, 1.0013868808746338, -0.4636426568031311, -0.3318987786769867, 0.02981993556022644, 0.14155296981334686, 0.5310722589492798, 0.7604244351387024, 0.3226163685321808, -0.23262910544872284, 0.8058026432991028, -0.8483920097351074, -0.8128522038459778, -1.3652758598327637, 1.0006810426712036, -0.1563796103000641, 0.2545085549354553, -0.5980868339538574, -1.0279102325439453, -0.8915818929672241, -0.6202826499938965, -0.05948381870985031, -0.2648639678955078, 0.716637134552002, 1.1563385725021362, 0.4281540513038635, -0.930298924446106, 0.22134259343147278, -0.410795122385025, -0.11126671731472015, 0.0475081130862236, 0.14435751736164093, 0.08940645307302475, -0.3469600975513458, -1.3173060417175293, 0.3242678940296173, 0.1357598900794983, -0.9086605906486511, -0.5204689502716064, -0.8664160966873169, -1.7194682359695435, 0.08324593305587769, 0.035196419805288315, -0.4767991304397583, 1.2715359926223755, 0.15128092467784882, -0.7453396916389465, 0.5495727062225342, -0.8379991054534912, -0.1364438533782959, -0.15253959596157074, -0.16785672307014465, -0.5130423903465271, -0.5593326687812805, 0.22894828021526337, 0.44407665729522705, 0.452597051858902, -0.46844926476478577, -0.46523064374923706, 0.48510080575942993, -0.084036685526371, -0.477705717086792, 0.21844865381717682, 1.0856683254241943, -0.03850559890270233, -0.33110159635543823, 0.3441380262374878, 0.5251439213752747, 0.0029033715836703777, -0.24337255954742432, -0.6417542099952698, -0.9524991512298584, 0.737471342086792, -0.4102528393268585, 1.286070466041565, -0.757550835609436, -0.6352775692939758, -0.2532842457294464, 0.22330829501152039, -0.39095059037208557, -0.5397990942001343, 0.6560512781143188, -0.8996081352233887, 1.0088014602661133, -0.44939371943473816, -1.1584744453430176, 0.24133752286434174, -0.20321053266525269, -1.0053797960281372, -0.3626779019832611, 0.34053826332092285, 0.8428657650947571, -0.42215484380722046, 0.25164246559143066, 0.26111626625061035, 0.04045527055859566, -0.8148171305656433, 1.2426406145095825, -0.27667781710624695, 0.0373271107673645, -0.16017159819602966, -0.10815977305173874, -0.044074032455682755, -0.3672660291194916, 0.1466534584760666, -0.3299963176250458, -0.2147017866373062, 0.37697312235832214, -0.31536051630973816, 0.9481786489486694, -0.3623254597187042, 0.7659416794776917, -0.5292099118232727, -0.36362141370773315, -0.08619054406881332, 0.6509884595870972, -0.5137973427772522, 0.001873099827207625, 0.3666931688785553, 0.5233563780784607, -0.24903690814971924, 0.32666441798210144, 0.5613500475883484, 0.22170068323612213, -0.3194810748100281, 0.7593616247177124, 0.552811324596405, -0.3957248330116272, 0.7884575724601746, 0.6305328011512756, 0.773421049118042, 0.21034307777881622, 0.7386250495910645, 0.05633709952235222, 0.4172583222389221, -0.4258538484573364, -0.16426415741443634, 0.3788195848464966, 0.7685629725456238, 1.067983627319336, 0.40595564246177673, -0.5532616376876831, -0.3750418722629547, 0.19902533292770386, 0.7053515911102295, 1.4078789949417114, -0.17666523158550262, -0.7147758603096008, -0.7417283058166504, -0.22612224519252777, -0.3212756812572479, 0.7363255620002747, -0.48065999150276184, -0.16179385781288147, -0.4312102198600769, -0.679198682308197, 0.9859955906867981, 0.5117896199226379, 0.49684494733810425, -0.42458227276802063, -0.15768450498580933, -0.21999748051166534, -0.06350129097700119, -0.7664580345153809, -0.46033960580825806, 0.5290612578392029, -0.48661744594573975, -0.22643262147903442, 0.3384416997432709, 0.03873886540532112, 0.28920429944992065, -0.7024084329605103, 0.7509124279022217, -0.21509386599063873, -0.3753960430622101, 0.18451453745365143, 0.7831864953041077, -0.8369560837745667, -1.3860421180725098, 0.10707640647888184, 0.1302867829799652, -0.40916892886161804, 0.46724456548690796, 0.8252453207969666, 0.6642197966575623, 0.10017547011375427, -0.5905798077583313, 0.2758480906486511, -0.021413592621684074, 0.13212615251541138, 0.8112632036209106, -0.14612933993339539, -0.48652511835098267, -1.2591782808303833, 0.9031659364700317, 0.26866596937179565, -0.3123205900192261, 0.4796353876590729, -0.9330638647079468, 0.08524078875780106, 0.537381112575531, -0.5555992722511292, -0.36065319180488586, -0.8584405183792114, 0.8599502444267273, 0.02678225375711918, -0.4570106863975525, 0.4835240840911865, -0.07405248284339905, 0.5607320666313171, -0.016526436433196068, 0.5677870512008667, 0.2746169865131378, -0.2685737609863281, 0.7398951649665833, -1.010949969291687, 0.08960627764463425, 0.36972570419311523, 0.509101927280426, -0.31462982296943665, -0.4253042936325073, -0.40865346789360046, -0.2717839777469635, -0.1757735013961792, -0.10161004215478897, -0.0051508755423128605, 0.2152385711669922, -0.5159009099006653, -0.4219822287559509, -0.20460502803325653, -1.4055242538452148, 0.285053014755249, 0.27246010303497314, -0.2752474248409271, -0.053799621760845184, -1.2311614751815796, -1.0983246564865112, -0.4867650270462036, -0.1375332921743393, -1.0221410989761353, 0.550144612789154, -0.05560207739472389, -0.7609243988990784, -0.7799174785614014, -0.13791419565677643, 0.04715399444103241, 0.8361849188804626, -0.5056199431419373, 1.7619524002075195, -0.27015262842178345, -0.29903140664100647, -0.02105228789150715, 0.19047805666923523, 0.3054213225841522, -0.15259137749671936, 0.44605401158332825, -0.8553294539451599, 0.10292740166187286, -0.2380409687757492, -0.18852736055850983, -0.0076369051821529865, 0.5329251289367676, 0.3309880793094635, 0.021948780864477158, -0.6698330640792847, 0.14246729016304016, 1.1792737245559692, -0.6507619023323059, -0.2937813103199005, 0.31624236702919006, 1.0572071075439453, 0.45864665508270264, -0.475189745426178, 0.2639046013355255, 0.5552321672439575, 0.011800888925790787, 0.26608043909072876, -0.2122841626405716, 0.16771505773067474, -0.7985167503356934, 0.7387892603874207, 1.3820605278015137, 0.2554745376110077, -0.21013574302196503, -1.2151408195495605, 0.6735645532608032, -1.199501633644104, -0.3945273160934448, 0.4051535427570343, 0.4317941665649414, 0.3248225748538971, -0.5311257839202881, -0.8631502389907837, 0.0587247833609581, 0.6615073084831238, 0.32498395442962646, -0.25430530309677124, -0.7060375213623047, -0.16808125376701355, 0.2650952637195587, 0.08705128729343414, 0.6502972841262817, -0.38832271099090576, 0.5382879972457886, 14.548027992248535, 0.9461141228675842, 0.3632237911224365, 0.5431435704231262, 0.6467497944831848, 0.577880322933197, -0.9397220611572266, 0.3034190237522125, -1.0781943798065186, -0.7450345158576965, 1.269149661064148, -0.4978967607021332, 0.5354040265083313, 0.18593065440654755, 0.3099432587623596, 0.2909914553165436, -0.5559527277946472, 0.5400782227516174, 0.5360474586486816, -1.1052303314208984, 1.158072829246521, 0.31884148716926575, 0.19943146407604218, 0.10148966312408447, 0.39340028166770935, 0.8372383713722229, 0.3456588387489319, -0.9757672548294067, 0.8142215013504028, -0.19376182556152344, 0.6849318146705627, -0.33679893612861633, 0.5042866468429565, 0.7307879328727722, -1.2188280820846558, -0.29911643266677856, -0.681214451789856, -1.133723258972168, 0.3221074342727661, 0.00018480046128388494, -0.4862512946128845, -0.5824200510978699, -0.6171501874923706, 0.7296435236930847, -0.17593607306480408, -0.013492792844772339, -0.4299914538860321, 0.8856946229934692, 0.024559564888477325, 0.32623520493507385, 0.046466294676065445, 0.5971236228942871, 0.629966676235199, -0.5815134644508362, 0.31916511058807373, -0.07491160184144974, -0.048080988228321075, 0.8949275612831116, -0.6094335317611694, 0.022319190204143524, -0.4843055307865143, -0.7406802177429199, -0.16092224419116974, 0.6015294194221497, 0.1725470870733261, 0.4502711296081543, -0.30817633867263794, 0.03666883334517479, 0.45920103788375854, 0.14331872761249542, -0.20865541696548462, -0.0019082734361290932, 0.4425869584083557, -0.1539369523525238, 0.1914118081331253, 0.7173662185668945, -0.003615712281316519, -0.5769546031951904, -0.6608108282089233, -0.6007630228996277, 0.2576279044151306, -0.9279466867446899, -0.5824381113052368, 0.9878841042518616, -0.18098872900009155, -0.3327820301055908, -0.07146378606557846, -0.6775184869766235, -0.19379198551177979, 0.24427510797977448, -1.2040817737579346, -1.2194206714630127, 0.5882752537727356, -0.34136074781417847, -0.01724890060722828, -0.0930013358592987, 1.6657387018203735, -0.23791852593421936, -0.29433903098106384, -0.1856939047574997, 0.05186609923839569, -0.11840413510799408, -0.3222011923789978, -0.979979395866394, 0.9249784350395203, 0.4327799081802368, 0.42235681414604187, 0.8294597864151001, 0.4593735933303833, 0.0989190861582756, -0.40171727538108826, -0.11918847262859344, 1.3995901346206665, -1.236730933189392, -0.18460693955421448, -0.7082263827323914, -0.839320957660675, 0.4467473328113556, 0.41283077001571655, -0.5081542134284973, 0.44965773820877075, 0.1348600536584854, -0.480253666639328, 0.1802155077457428, -0.6449075937271118, -0.2534857988357544, 0.37084025144577026, -0.8229096531867981, -0.5409185290336609, -0.2188243567943573, 0.5315986275672913, -0.8384314775466919, -0.11472801864147186, -0.4673486649990082, -0.09989488869905472, 0.2659022808074951, 0.36884307861328125, -0.8628212809562683, 0.7817195653915405, 0.41750138998031616, -0.24719476699829102, -1.1438982486724854, -0.17122237384319305, -1.0004560947418213, 0.07796043157577515, 0.3227328658103943, 1.427777886390686, -0.07827673852443695, -0.2919355630874634, 1.246662974357605, 0.48623985052108765, -0.20988789200782776, -0.6797601580619812, -0.12252815812826157, 0.016584884375333786, -0.8833408951759338, 0.590354323387146, -0.4071962237358093, -0.2775951325893402, 0.34203603863716125, 0.39038169384002686, 1.0106520652770996, -0.3305163085460663, -0.46971121430397034, 0.37408512830734253, -0.10357731580734253, -0.10984835773706436, -0.2725413143634796, -0.47202545404434204, -1.189070701599121, 0.39317500591278076, -1.085578203201294, -0.005475491285324097, -1.3648600578308105, -0.2342662811279297, 0.5489275455474854, 0.1322581171989441, 0.07729683071374893, 0.4337804913520813, -0.4255465567111969, -0.42525431513786316, -0.3854869306087494, -0.09320803731679916, 0.7733066082000732, 0.6657940745353699, -0.739578366279602, 0.3576813042163849, -0.022904066368937492, -0.02830418385565281, 0.35445067286491394, 0.11545874923467636, -0.9009864330291748, -1.106752872467041, -1.5094321966171265, 0.20005983114242554, -0.21937568485736847, -0.13202320039272308, -0.13293874263763428, 0.35594746470451355, 0.3593771457672119, -0.31325864791870117, 0.5273420810699463, -0.0674496665596962, -0.8081001043319702, -0.17819762229919434, 0.4704551100730896, -0.9695450067520142, 0.46833351254463196, 0.36795714497566223, -0.352532297372818, -0.653560221195221, 0.5238839387893677, -0.15827874839305878, -1.0928345918655396, -0.5225869417190552, 0.3705264925956726, -0.8092080950737, 0.05933807045221329, -0.14739002287387848, -0.14116501808166504, -0.8106114864349365, -0.6274582743644714, -0.24050022661685944, 0.21787673234939575, -0.38389238715171814, 1.168338418006897, 0.48095715045928955, -0.9248262047767639, 0.1426353007555008, 0.5247591137886047, 0.13891635835170746, -0.4247616231441498, 0.11712054908275604, 0.013124283403158188, -0.26973289251327515, 0.8690234422683716, 0.5382640361785889, 0.5349703431129456, -1.1692396402359009, -0.33879929780960083, 0.7949514389038086, -0.475757896900177, -0.23627281188964844, 1.1877726316452026, 0.036603040993213654, -1.2806205749511719, -0.07574377208948135, -1.5218923091888428, -0.6193529963493347, -0.6645090579986572, 0.8425848484039307, -0.019546233117580414, -0.2806492745876312, -0.3346198499202728, -0.5095390677452087, -0.013238461688160896, 0.03356517106294632, -0.5458269119262695, 0.6265698075294495, -0.729265570640564, -0.5373053550720215, 0.7937489748001099, 0.5429099202156067, -0.5462251901626587, -0.3169088065624237, -0.100772425532341, -0.07910415530204773, -0.15421326458454132, 0.30740684270858765, -0.26075708866119385, -0.46203702688217163, 0.8380391001701355, 0.28941041231155396, 0.47948694229125977, -0.08884043246507645, -0.008124005980789661, 0.33870914578437805, 0.5640407800674438, 0.2529478967189789, -0.47498729825019836, -1.0565838813781738, 1.2760334014892578, 1.3132082223892212, -1.0794126987457275, 0.13228105008602142, -0.361349880695343, -0.8959077596664429, 1.1442424058914185, 0.09413900971412659, 0.46889978647232056, 0.9136683344841003, -0.2984243929386139, 0.3022609055042267, 0.033674318343400955, -1.319825530052185, -0.0939827710390091, 0.76910001039505, 1.063222885131836, 1.1152981519699097, 0.3951944410800934, 0.09031160920858383, 1.1982553005218506, -0.10219674557447433, 0.09844804555177689, 0.9634849429130554, 0.5140766501426697, -0.3293147683143616, -0.15947963297367096, 0.2476099580526352, 0.8055028915405273, -0.646686315536499, -0.9551884531974792, -0.003024138743057847, 1.1060786247253418, 0.2294148951768875, 0.8510181903839111, 0.3776112496852875, 0.4360024929046631, 0.38120144605636597, 0.43521496653556824, 0.22905153036117554, -0.880926251411438, 0.048487141728401184, -0.24901872873306274, -0.39520618319511414, -0.03036574274301529, -0.4323144257068634, -0.23440305888652802, -0.9243323802947998, -0.3251831531524658, 0.008141444995999336, 0.2565649747848511, 0.42957067489624023, 1.5527418851852417, 0.4561968147754669, 0.22464095056056976, -0.41865113377571106, 0.3052437901496887, -0.25406360626220703, -1.0724059343338013, -0.5189924240112305, -0.856770396232605, -0.01643689163029194, -0.3777623772621155, -0.5745036005973816, 0.24509228765964508]}, "authors": [{"authorId": "2280924847", "name": "Haoyan Luo"}, {"authorId": "2280334342", "name": "Lucia Specia"}], "references": [{"paperId": "297211bc86653d9ebbe694a75141c9a1c6c11e69", "title": "In-Context Learning Creates Task Vectors"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "26089bdfdbca1e6eaaceca71e3116b715bec6d47", "title": "Explainability for Large Language Models: A Survey"}, {"paperId": "a95ee02fd85a3734bf19bb4bbc6ef3ea22c96cc9", "title": "PMET: Precise Model Editing in a Transformer"}, {"paperId": "1c0c13edd4442ceb7eac70bbcaebaf84512f9a3c", "title": "Overthinking the Truth: Understanding how Language Models Process False Demonstrations"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "40255f5b512ba3762d1a91214a865bbe62349b14", "title": "ALPACA: A new semi-analytic model for metal absorption lines emerging from clumpy galactic environments"}, {"paperId": "405f8f5f1c6df1b3343c812832479aad5180b65f", "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model"}, {"paperId": "3327bdc202fb4b3673c2fa1ac18ab436adc72fef", "title": "DecompX: Explaining Transformers Decisions by Propagating Token Decomposition"}, {"paperId": "e74ea82b8eda84925b6defdfca9ed33b50d6d2ef", "title": "Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases"}, {"paperId": "771fd139a18595e21449b981f541c93a4e833399", "title": "Sequential Integrated Gradients: a simple but effective method for explaining language models"}, {"paperId": "7bd4ca8706a79983d31ab74e6c79bfdfd949602e", "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning"}, {"paperId": "133b97e40017a9bbbadd10bcd7f13088a97ca3cc", "title": "Dissecting Recall of Factual Associations in Auto-Regressive Language Models"}, {"paperId": "a206a0c96d6076c6ab081288b0c2c95d3c7efd64", "title": "Inspecting and Editing Knowledge Representations in Language Models"}, {"paperId": "dcd8dff20f4f490acd5f94001d34c774167f053e", "title": "Jump to Conclusions: Short-Cutting Transformers with Linear Transformations"}, {"paperId": "762ca2711eb167f19b79e39c175708ca15e1f5d7", "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens"}, {"paperId": "5969eff0e72e4a5bc0c7392c700be74a01ac2822", "title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations"}, {"paperId": "875b8602b0fa9ff22f8232a13323d72a6f5bd9f8", "title": "Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps"}, {"paperId": "a9be51698e7c2247853b7b6f1f70fc4d6d7ef605", "title": "Transformer-Patcher: One Mistake worth One Neuron"}, {"paperId": "30c0cdc414f68211d5d0514df027cec22e005174", "title": "A Survey on In-context Learning"}, {"paperId": "7c23ce3547ae837ff633b14e457a93de6d270b37", "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary"}, {"paperId": "6edd112383ad494f5f2eba72b6f4ffae122ce61f", "title": "Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small"}, {"paperId": "d5295f7ddcf281f3d30a7579d5ce482036a8e27c", "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task"}, {"paperId": "2fe1ac0b09cc0f50eb83eef6c7c6b45ac8b12413", "title": "Mass-Editing Memory in a Transformer"}, {"paperId": "c7fa5c2172a4624d6baa91e66344e4520d3028ad", "title": "Analyzing Transformers in Embedding Space"}, {"paperId": "1d650f1afd45c59ff907396fe8b678595dcb85ea", "title": "Memory-Based Model Editing at Scale"}, {"paperId": "5470f044662ada6ca1ce9b23e7e4bba0ffc87b87", "title": "GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers"}, {"paperId": "cf36236015c9f93f15bfafbf282f69e08bdc9c16", "title": "Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space"}, {"paperId": "bb1c9cb431e771660cffdda1d80a7f15ff40c764", "title": "Measuring the Mixing of Contextual Information in the Transformer"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e", "title": "A General Language Assistant as a Laboratory for Alignment"}, {"paperId": "90f7b61762e4454b9cba3afbb10c5f07465cff85", "title": "Grad-SAM: Explaining Transformers via Gradient Self-Attention Maps"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "ce9ca56036307217ea565644d3d3bd74b879e045", "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP"}, {"paperId": "4a54d58a4b20e4f3af25cea3c188a12082a95e02", "title": "Transformer Feed-Forward Layers Are Key-Value Memories"}, {"paperId": "ae06bc1e8e67c27b89329ebcfe61b71625d853f6", "title": "A Diagnostic Study of Explainability Techniques for Text Classification"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "76a9f336481b39515d6cea2920696f11fb686451", "title": "Quantifying Attention Flow in Transformers"}, {"paperId": "131c19dc3d460e1973d4b015cc9f888bae4f200b", "title": "Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias"}, {"paperId": "1686203adc5f2dbc18627ce64f66d33eb81432a5", "title": "Self-Attention Attribution: Interpreting Information Interactions Inside Transformer"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "1657220981714a6c312b364dbb51d604521f894e", "title": "Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection"}, {"paperId": "9a21740d87976bf76f4a9668a9da631035302fb2", "title": "Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms"}, {"paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3", "title": "Language Models as Knowledge Bases?"}, {"paperId": "455a8838cde44f288d456d01c76ede95b56dc675", "title": "A Structural Probe for Finding Syntax in Word Representations"}, {"paperId": "74e9053d6f44f4507bd40bbea999ee65f0cbefb2", "title": "Pathologies of Neural Models Make Interpretations Difficult"}, {"paperId": "fa025e5d117929361bcf798437957762eb5bb6d4", "title": "Zero-Shot Relation Extraction via Reading Comprehension"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "442e10a3c6640ded9408622005e3c2a8906ce4c2", "title": "A Unified Approach to Interpreting Model Predictions"}, {"paperId": "f302e136c41db5de1d624412f68c9174cf7ae8be", "title": "Axiomatic Attribution for Deep Networks"}, {"paperId": "e81317c5166b7c53654ea2ffa91484a08bc772ff", "title": "Investigating the influence of noise and distractors on the interpretation of neural networks"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "2079b1607b8d795c58d269c31a1651c4819d772a", "title": "Local Interpretation of Transformer Based on Linear Decomposition"}, {"paperId": "13b8060acc3db1fc555f6e55368f6d02899a1698", "title": "FairPrism: Evaluating Fairness-Related Harms in Text Generation"}, {"paperId": null, "title": "will you find these shortcuts?"}, {"paperId": "bcf35117a0cb08fba6b4d00f9aa4f499946b57d9", "title": "Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "why should i trust you?"}, {"paperId": null, "title": "Models, reasoning and inference"}, {"paperId": null, "title": "2023a. Locating and editing"}, {"paperId": null, "title": "2023. Dola: Decoding by contrasting layers improves factuality in large language models"}, {"paperId": null, "title": "2023. Editing"}, {"paperId": null, "title": "2023. Survey of hallucination in natural language generation"}, {"paperId": null, "title": "2022. Perturbation augmentation for fairer NLP"}, {"paperId": null, "title": "2022. Knowledge neurons in pretrained transformers"}, {"paperId": null, "title": "Transformer Circuits. 2022. Mechanistic interpretations of transformer circuits"}, {"paperId": null, "title": "2022. Few-shot out-of-domain transfer learning of natural language explanations in a label-abundant setup"}, {"paperId": null, "title": "2023. Interpreting transformer\u2019s attention dynamic memory and visualizing the semantic information flow of gpt"}, {"paperId": null, "title": "2022. Efficiently scaling transformer inference"}]}