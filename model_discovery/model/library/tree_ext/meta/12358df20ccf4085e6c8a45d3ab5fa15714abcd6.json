{"paperId": "12358df20ccf4085e6c8a45d3ab5fa15714abcd6", "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications", "abstract": "This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to better specialize in the current domain compared to stand-alone fine-tuning, (ii) training across a diverse range of domains enhances both backward and forward knowledge transfer, and (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning. We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs, and has the potential to play a key role in guiding the direction of future research in the field.", "venue": "arXiv.org", "year": 2024, "citationCount": 8, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This research introduces a new benchmark designed to measure the adaptability of LLMs to changing data landscapes in practical scenarios, and finds that smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning."}, "embedding": {"model": "specter_v2", "vector": [0.06490565091371536, 0.22901669144630432, -0.19173645973205566, -0.062005866318941116, -0.3095335066318512, -0.24380435049533844, 0.49747416377067566, -0.07553192973136902, -0.8127052187919617, 0.25963684916496277, 0.04888829216361046, -0.34780287742614746, -0.10617129504680634, 0.2292313277721405, -0.38354676961898804, 0.4635443687438965, -1.2342747449874878, 0.7756657600402832, 0.23704807460308075, -0.6058748960494995, -0.7977356910705566, -0.5141763687133789, -0.7686428427696228, 0.07469209283590317, 0.8227283358573914, 0.3962438702583313, 0.5004991292953491, 0.7674593925476074, -0.19974228739738464, 0.39242762327194214, 0.4217710494995117, -0.35718056559562683, 0.4304003119468689, -0.18677176535129547, -0.4729773700237274, 0.16973377764225006, 0.48149117827415466, -0.38621217012405396, -0.5224698185920715, 0.4172583520412445, -0.3587189316749573, 0.5225672125816345, 0.4294187128543854, -0.2905430197715759, -0.5440733432769775, 0.4883050322532654, 0.9632436633110046, 0.7760027050971985, 0.028584295883774757, -0.6172243356704712, 1.00033438205719, -1.3927229642868042, 0.516599714756012, 1.2997723817825317, 1.1258455514907837, 0.6204047203063965, -0.4506871700286865, -0.8698694109916687, 0.8990693092346191, 0.014885532669723034, -0.8257763385772705, -0.352935254573822, -0.24424760043621063, -0.3061453104019165, 1.7124078273773193, -0.664164662361145, 0.03809668868780136, 0.221168652176857, -0.042669426649808884, 1.4793339967727661, -0.22717326879501343, -0.8763952255249023, -0.22649091482162476, 0.5498221516609192, 0.16939756274223328, 0.6440756320953369, -0.3972248136997223, 0.4736579954624176, -0.8210362792015076, -0.09419277310371399, 0.4083980619907379, -0.22044065594673157, -0.002580442000180483, -0.33479082584381104, -0.17352256178855896, 0.5108010768890381, 0.29838812351226807, 1.0242398977279663, -0.4640157222747803, 0.4384281635284424, 0.29678642749786377, 1.093610405921936, 0.4012957215309143, 0.5759807825088501, -0.43063822388648987, 0.4512587785720825, -0.948082447052002, 0.0458596870303154, -0.07983756810426712, 0.7262125015258789, 0.13896246254444122, 0.15392686426639557, -0.34509068727493286, 0.6715272068977356, 1.0795692205429077, -0.35462120175361633, 0.8639368414878845, -0.48571500182151794, 0.41112565994262695, -0.5400770306587219, 0.3322700262069702, -0.4123040735721588, -0.41821691393852234, -0.5284137725830078, -0.43085598945617676, -1.3529561758041382, -0.38479378819465637, 0.04299696534872055, -0.5607438087463379, 0.9771141409873962, -0.010970079340040684, 0.2622585594654083, 0.16323894262313843, 0.17472071945667267, 0.2024284154176712, 0.26883137226104736, 0.7068638205528259, -0.24152974784374237, 0.44173532724380493, -0.25665032863616943, -0.29926061630249023, -1.1864166259765625, 0.9848809242248535, -0.03642779961228371, 0.24710613489151, -0.3586863875389099, -1.486302375793457, -1.2728171348571777, -0.88130122423172, 0.13233110308647156, -0.6638571619987488, -0.011416764929890633, 1.1645805835723877, 0.3032267391681671, -1.0439380407333374, 0.6958487629890442, -0.20769189298152924, -0.4836118221282959, 0.30282217264175415, 0.14387139678001404, 0.1370110958814621, -0.9200657606124878, -1.6560158729553223, 0.5846214890480042, 0.8291635513305664, -0.7833165526390076, -0.7628849148750305, -0.6384854316711426, -0.9782633781433105, -0.39423704147338867, 0.28899484872817993, -0.5537942051887512, 1.2694511413574219, -0.754022479057312, -0.7869783043861389, 0.6746803522109985, -0.0010897265747189522, -0.09537532925605774, 0.6419299840927124, 0.035685304552316666, -1.165669560432434, -0.5167114734649658, -0.24737359583377838, 0.6414685249328613, 0.25955119729042053, -0.26558107137680054, -0.35957440733909607, 0.09792955964803696, -0.13046894967556, 0.2121315747499466, -0.6593820452690125, 0.19983379542827606, -0.20502352714538574, -0.4470955431461334, 0.4436466097831726, 0.8063282370567322, 0.061137981712818146, -0.28936222195625305, 0.5827983617782593, -1.0237088203430176, 0.48417842388153076, -0.4567106068134308, 1.0178626775741577, -1.0055654048919678, -0.7290996313095093, -0.14612741768360138, -0.16923342645168304, -0.2169972062110901, -1.2787963151931763, 0.8524876236915588, 0.004222575109452009, 0.15787285566329956, -0.14648476243019104, -1.1441203355789185, -0.226218119263649, -0.2406962811946869, -0.41138845682144165, -0.3114185631275177, -0.06275185942649841, 1.0384488105773926, -1.2198165655136108, 0.16125404834747314, -0.1833953857421875, 0.19745507836341858, -1.0078771114349365, 0.8768312931060791, -0.8827860355377197, 0.2924449145793915, 0.20308303833007812, -0.2538447976112366, 0.25105223059654236, -0.20700566470623016, 0.7032157778739929, -0.0038005320820957422, 0.03262590244412422, 0.4401049315929413, -0.4076632261276245, 1.5129978656768799, -0.6357095241546631, 0.4230048954486847, -0.004704103339463472, -0.62602299451828, -0.12224861234426498, 0.5903669595718384, -0.059406768530607224, -0.5226054787635803, 0.04815452918410301, 0.37637895345687866, -0.5643685460090637, 0.4587957561016083, 0.9028925895690918, 0.10137069970369339, -0.20480339229106903, 0.16901013255119324, 0.9525226354598999, -0.13464874029159546, 0.701289713382721, 0.3773408532142639, 0.5064566135406494, 0.2109844833612442, 0.2794093191623688, -0.20304295420646667, 0.4317670464515686, -0.5158938765525818, 0.13754507899284363, 0.46771514415740967, 0.7078811526298523, 0.19904902577400208, -0.30628180503845215, -0.6010587215423584, -0.24299170076847076, -0.0745898112654686, 0.4508747160434723, 2.0433731079101562, -0.2583531439304352, 0.158102348446846, -0.5776678323745728, -0.49080267548561096, 0.03745156526565552, 0.6702679991722107, -0.5941829681396484, -0.32112836837768555, -0.5802490711212158, -0.6467233300209045, 0.4232794940471649, 0.43742212653160095, 1.051689863204956, -0.4992770552635193, -0.19233863055706024, 0.12504293024539948, 0.4083947241306305, -0.4121825695037842, -0.5957822799682617, 0.19495101273059845, -0.8542853593826294, -0.07574701309204102, -0.28026649355888367, -0.5557389259338379, 0.049433931708335876, -0.33293402194976807, 1.0112568140029907, -0.3285311758518219, -0.13575343787670135, 0.7019597887992859, 0.6286031603813171, -0.5973702669143677, -1.2378555536270142, 0.14765995740890503, 0.43496009707450867, -0.08644036948680878, -0.009592908434569836, 0.6779344081878662, 0.163920059800148, 0.47834110260009766, -0.5997254848480225, 0.30457624793052673, 0.24926915764808655, 0.2750765085220337, 0.8245527148246765, 0.1717528998851776, 0.787968099117279, -1.530693769454956, 1.3128896951675415, -0.03771023079752922, -0.6318230628967285, 0.9106692671775818, -0.700224757194519, -0.717685341835022, 0.6141995787620544, -1.0127061605453491, -0.7371611595153809, -1.1091793775558472, 0.4423616826534271, -0.11644479632377625, -0.26213130354881287, 0.6809139251708984, 0.5920838713645935, -0.0029554490465670824, 0.49999547004699707, 0.7401285767555237, 0.3147212564945221, -0.36115944385528564, 0.4232158660888672, -0.9825505018234253, 0.3189573585987091, 0.25981128215789795, 0.24360401928424835, -0.3127900958061218, -0.5312013030052185, -0.9287987351417542, -0.29713812470436096, -0.30782264471054077, -0.5687516927719116, -0.27528315782546997, -0.16882449388504028, -0.8504731059074402, -0.1871308833360672, 0.037767767906188965, -0.6081100702285767, -0.8785808086395264, 0.014837833121418953, -0.009396997280418873, -0.13527141511440277, -1.1006282567977905, -1.0216732025146484, -0.567211925983429, -0.10600964725017548, -0.6298824548721313, -0.13484102487564087, -0.24956847727298737, -0.5489985942840576, -1.3064533472061157, 0.1457216888666153, -0.3644658327102661, 1.3070869445800781, -1.0605062246322632, 1.1353439092636108, 0.023379147052764893, 0.1451525241136551, -0.27519744634628296, 0.47272542119026184, 0.5936426520347595, -0.08099086582660675, -0.4193589985370636, -0.7923859357833862, -0.5247738361358643, -0.16681136190891266, -0.595903217792511, 0.04550367221236229, -0.0545073002576828, 0.4743557572364807, 0.07472781836986542, -0.5370060205459595, 0.11585836112499237, 1.276304006576538, -0.2512195110321045, -0.3319145739078522, 0.26561272144317627, 0.4412134289741516, 0.21080900728702545, -0.056178927421569824, 0.2952785789966583, 0.5038968324661255, 0.09722225368022919, -0.36706680059432983, 0.005380927585065365, -0.5207102298736572, -0.7894750237464905, 0.43035271763801575, 1.8454006910324097, 0.2514508068561554, 0.0736817866563797, -1.2733423709869385, 0.5735396146774292, -0.9331166744232178, -0.42589113116264343, 1.323408842086792, 1.096072793006897, 0.7960605621337891, -0.6662625670433044, -0.32963427901268005, -0.5132395625114441, 0.1777612417936325, -0.07357212901115417, -0.7389100790023804, 0.11576391756534576, -0.0027698257472366095, 0.1941944658756256, -0.0277231615036726, 0.69013512134552, -0.4330492615699768, 0.9376015067100525, 14.60617733001709, 1.0405014753341675, 0.11784476786851883, 1.1296156644821167, 0.5424066781997681, 0.4266386032104492, -0.5803612470626831, -0.38849005103111267, -0.9369349479675293, -0.30155277252197266, 1.1253446340560913, 0.21647906303405762, 1.1957013607025146, 0.04175471514463425, -0.31924423575401306, 0.01127078291028738, -0.5852676033973694, 0.274852991104126, 0.3502967655658722, -1.2014572620391846, 0.8800914883613586, -0.06456006318330765, 0.7438924908638, 0.6344065070152283, 0.7564272880554199, 1.2197455167770386, 0.48730409145355225, -0.29355528950691223, 0.6525833606719971, 0.33008232712745667, 1.0346733331680298, 0.023694129660725594, 0.2828848958015442, 0.855830729007721, -0.7938613891601562, -0.39173635840415955, -0.5719348788261414, -0.7479885220527649, 0.12704388797283173, -0.3740922510623932, -0.9255716800689697, -0.6301519870758057, -0.362182080745697, 0.6032244563102722, -0.3155128061771393, 0.13793829083442688, 0.08848089724779129, 0.6146571040153503, 0.06160201132297516, 0.4204382002353668, 0.030130231752991676, 0.6555926203727722, 0.17710673809051514, -0.09973924607038498, -0.04825301840901375, -0.190562903881073, 0.023748470470309258, 0.1099795401096344, -0.7398143410682678, 0.005230492912232876, -0.5041009187698364, -0.29059261083602905, 0.14589045941829681, -0.08274807780981064, 0.5592822432518005, 0.1322311908006668, -0.5143100619316101, 0.3763073682785034, 0.9907304048538208, 0.3740510642528534, 0.08436547964811325, 0.27313801646232605, 0.31128931045532227, -0.2988985776901245, -0.2751951813697815, 0.362619012594223, -0.23515582084655762, -0.44702214002609253, -0.8031096458435059, -0.35385996103286743, 0.5784147381782532, -0.6932872533798218, -1.0332574844360352, 0.9520524144172668, -0.04046984389424324, -0.5212022066116333, 0.10887331515550613, -0.5721346139907837, 0.09105626493692398, 0.31351351737976074, -1.2328563928604126, -0.6650235652923584, 0.20684541761875153, 0.12981384992599487, -0.19574317336082458, -0.5196628570556641, 1.1526089906692505, 0.34344327449798584, -0.30440929532051086, 0.3570024371147156, 0.6795132756233215, -0.22002145648002625, 0.029841534793376923, -0.3128797113895416, 0.7024542689323425, -0.15333011746406555, 0.08901797980070114, 0.3629402816295624, -0.31675511598587036, -0.34690478444099426, -0.90596604347229, -0.33091920614242554, 0.764660120010376, -0.8937621116638184, -0.42510828375816345, -0.7529765367507935, -1.2546284198760986, 0.8593816757202148, 0.39475828409194946, -0.5227375626564026, 0.3347684442996979, 0.34817788004875183, -0.3354572355747223, -0.14133791625499725, -1.0518195629119873, 0.07676321268081665, 0.6280434131622314, -0.8298792839050293, -0.6533861756324768, 0.2214886099100113, 0.42938902974128723, -0.821932315826416, -0.5498773455619812, -0.1318548172712326, -0.07701686024665833, 0.08508174121379852, 0.9484382271766663, -0.6819109916687012, -0.29076504707336426, 1.0414596796035767, -0.033452969044446945, -1.0231043100357056, -0.23750989139080048, -0.546350359916687, 0.37588295340538025, 0.06205902621150017, 0.972596287727356, -0.8900638818740845, -0.14859211444854736, 0.756171703338623, 0.15101608633995056, -0.14525552093982697, -0.7335405945777893, -0.4811834394931793, 0.22469870746135712, -0.7822949886322021, 0.28071245551109314, 0.23982618749141693, -0.314699649810791, 0.0655372366309166, 0.5343984961509705, 0.6299128532409668, -0.45890364050865173, -0.89983731508255, 0.32433339953422546, -0.3159598410129547, -0.10244577378034592, -0.4977134168148041, -0.23747821152210236, -1.3197351694107056, -0.11230260878801346, -1.3444206714630127, -0.09024592489004135, -0.9535832405090332, -0.8115695714950562, -0.3776305615901947, -0.5066558122634888, -0.2830367088317871, 0.5797203779220581, -0.5753002166748047, -0.4568527936935425, -0.29234519600868225, -0.8197102546691895, 0.8128663301467896, 0.8999373316764832, -0.6282971501350403, -0.30315887928009033, -0.19381561875343323, 0.3689039647579193, 0.5729560256004333, 0.535190224647522, -0.43806353211402893, -1.0510034561157227, -1.6189701557159424, 0.2825212776660919, -0.172044038772583, -0.32274365425109863, -0.8682660460472107, 0.8930308818817139, 0.3286515772342682, 0.1212569996714592, 0.06994132697582245, 0.49051767587661743, -1.0483777523040771, -0.09011143445968628, 0.17566847801208496, -0.6625798940658569, 0.26744428277015686, 0.8428716659545898, -0.29367250204086304, -0.3244187533855438, 0.7385032176971436, 0.10014776140451431, -1.1265743970870972, -1.3144545555114746, 0.34516993165016174, -0.6180447936058044, 0.4444039762020111, 0.14360882341861725, -0.1563093066215515, -0.7273445129394531, -0.3186947703361511, -0.36836326122283936, 0.8445081114768982, -0.2861254811286926, 0.7893030047416687, 0.6126708984375, -1.1925725936889648, -0.14963488280773163, 0.7768653631210327, -0.014762413688004017, -0.05547283589839935, 1.1668028831481934, 0.3794867694377899, -0.45086485147476196, 0.5489596128463745, 0.2732974886894226, 0.4657563865184784, -0.667201042175293, -0.027777064591646194, 0.8687810301780701, -0.3477844297885895, 0.13175533711910248, 1.2954061031341553, 0.09347411245107651, -1.4005850553512573, 0.30092862248420715, -0.6459570527076721, -0.3712051212787628, -0.08634588867425919, 0.4353902041912079, 0.42501336336135864, -0.20742902159690857, 0.0346715934574604, -0.22701428830623627, 0.28188639879226685, -0.2882528603076935, -0.583561897277832, 0.6320383548736572, -0.5461085438728333, -0.22204722464084625, 0.9826238751411438, 0.5604416728019714, -0.6978021264076233, -1.0159538984298706, -0.922943651676178, -0.18986454606056213, 0.00437482725828886, 0.2659008204936981, -0.9983595609664917, -0.1043267548084259, 0.4106498956680298, 0.7654432058334351, 0.0788416936993599, 0.029255034402012825, 0.12375666946172714, 0.037384431809186935, 0.7499410510063171, 0.47876930236816406, -0.9282358884811401, -0.5423720479011536, 1.3673079013824463, 1.4860788583755493, -1.1643428802490234, 0.1261117160320282, 0.371145099401474, -0.7313932776451111, 0.7675938010215759, 0.6671163439750671, 0.1673630326986313, 1.0093286037445068, -0.7474629878997803, 0.33406969904899597, 0.22904570400714874, -1.150745153427124, -0.0221907589584589, 1.103954553604126, 1.001725196838379, 0.7171968221664429, 0.4323434829711914, 0.11352451890707016, 0.7196994423866272, 0.27192530035972595, 0.39173850417137146, 0.07210136950016022, 0.3481013774871826, -0.49470609426498413, 0.11818931996822357, 0.25624749064445496, 0.46691814064979553, -0.14358778297901154, -0.5305907130241394, 0.015643764287233353, 0.8016502261161804, 0.4359876215457916, 0.3587363660335541, 0.7836600542068481, -0.006570246070623398, 0.4839027523994446, 0.6899572014808655, 0.5190025568008423, -0.3964124619960785, -0.16795292496681213, -0.3710857629776001, -0.5089569687843323, 0.43891745805740356, -0.043429896235466, -0.5517345666885376, 0.010020969435572624, 0.10535602271556854, 0.39534711837768555, 0.06928172707557678, 0.07518404722213745, 0.8278924822807312, 0.7337714433670044, 0.292853444814682, -0.664326548576355, -0.014417954720556736, -0.647697389125824, -1.177964448928833, 0.12663820385932922, -0.421957790851593, -0.1341525912284851, -0.37739068269729614, 0.043252069503068924, -0.48907482624053955]}, "authors": [{"authorId": "2276608430", "name": "cCaugatay Yildiz"}, {"authorId": "2271649487", "name": "Nishaanth Kanna Ravichandran"}, {"authorId": "2222148307", "name": "Prishruit Punia"}, {"authorId": "2260370196", "name": "Matthias Bethge"}, {"authorId": "2445273", "name": "B. Ermi\u015f"}], "references": [{"paperId": "bd0cd89337cc40d39d3a4cbe9c8709e06e877f3e", "title": "Continual Learning for Large Language Models: A Survey"}, {"paperId": "28fde851680a40fbbc5c6a44bd3ac6f5ca4ad284", "title": "Orthogonal Subspace Learning for Language Model Continual Learning"}, {"paperId": "838cd69a0b6c9c244a6eebb0f4742c0625132de6", "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning"}, {"paperId": "193955704f66923ac20a664bd184ed4663b2bdf9", "title": "Continual Pre-Training of Large Language Models: How to (re)warm your model?"}, {"paperId": "ba33a32c5676030d66c2988b0ea4525938b0b57b", "title": "Towards Robust and Efficient Continual Language Learning"}, {"paperId": "3fe8b8c3476dcbfeb7592366ae4118f4581be9ee", "title": "RDumb: A simple approach that questions our progress in continual test-time adaptation"}, {"paperId": "e9b3e82b1c9eb4136df28e94f24cd823431be93b", "title": "Lifelong Language Pretraining with Distribution-Specialized Experts"}, {"paperId": "d789725525d2fbb801bf49979064397674138d76", "title": "Investigating Forgetting in Pre-Trained Representations Through Continual Learning"}, {"paperId": "e3ec55e9e6720194a0ed5d4033d93a941c8a4f99", "title": "Continual Pre-training of Language Models"}, {"paperId": "86478f285356b5c8d27423e6b939634d9e010fba", "title": "Progressive Prompts: Continual Learning for Language Models"}, {"paperId": "e3f839b01567ae73af822a3da5e160dac2fb4708", "title": "Adapting a Language Model While Preserving its General Knowledge"}, {"paperId": "bd8412c233bf3815d8e905911b58e12bd6f279da", "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model"}, {"paperId": "7c3a735c7567b5b54581ba09612db4d18a5dacac", "title": "M2D2: A Massively Multi-Domain Language Modeling Dataset"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3", "title": "Fine-tuned Language Models are Continual Learners"}, {"paperId": "b3bc37a15aa74c523d656ad89b1896651f5eef72", "title": "Continual Pre-Training Mitigates Forgetting in Language and Vision"}, {"paperId": "bc7984bfcfae537dbe633eeeb8d69c42a994c724", "title": "ELLE: Efficient Lifelong Pre-training for Emerging Data"}, {"paperId": "b2d1fb4f78d24f03119f28a516ccabfc9591e71f", "title": "An Empirical Investigation of the Role of Pre-training in Lifelong Learning"}, {"paperId": "ed8931af08ce757a92a01ed43a0619522e10e8ff", "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora"}, {"paperId": "917c63f2186119166b3379f5d2816bb1a2f39b09", "title": "DEMix Layers: Disentangling Domains for Modular Language Modeling"}, {"paperId": "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b", "title": "Explaining neural scaling laws"}, {"paperId": "373bc164d7b552f8782988e7da6b0d00092a20b0", "title": "Continual Lifelong Learning in Natural Language Processing: A Survey"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3161e2b6787d304c29dddb7d5fc188ca41be7bda", "title": "LAMOL: LAnguage MOdeling for Lifelong Language Learning"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "a4bc4b98a917174ac2ab14bd5e66d64306079ab5", "title": "BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2722b9e5ab8da95f03e578bb65879c452c105385", "title": "Catastrophic forgetting in connectionist networks"}, {"paperId": "b584309dccafe64a7d6b96f064554330cbe1bbd3", "title": "Effect of scale on catastrophic forgetting in neural networks"}, {"paperId": "8e125d392ea0d8240be654d90a28838711a5bd36", "title": "Pretrained Language Model in Continual Learning: A Comparative Study"}, {"paperId": "c5a343cc71fee6efe15ef88039462be0b6c708a5", "title": "Lifelong Language Learning with Adapter based Transformers"}, {"paperId": "5c5751d45e298cea054f32b392c12c61027d2fe7", "title": "S2ORC: The Semantic Scholar Open Research Corpus"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": "1c46943103bd7b7a2c7be86859995a4144d1938b", "title": "Visualizing Data using t-SNE"}, {"paperId": null, "title": "The risks of further pretraining a converged model"}, {"paperId": null, "title": "Randomizing training domain order significantly improves knowledge accumulation"}, {"paperId": null, "title": "Rethinking scaling laws for CL"}]}