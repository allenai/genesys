{"paperId": "915b10732f264bebd952c806db93692bf2cf8b08", "title": "SpaceByte: Towards Deleting Tokenization from Large Language Modeling", "abstract": "Tokenization is widely used in large language models because it significantly improves performance. However, tokenization imposes several disadvantages, such as performance biases, increased adversarial vulnerability, decreased character-level modeling performance, and increased modeling complexity. To address these disadvantages without sacrificing performance, we propose SpaceByte, a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling. SpaceByte consists of a byte-level Transformer model, but with extra larger transformer blocks inserted in the middle of the layers. We find that performance is significantly improved by applying these larger blocks only after certain bytes, such as space characters, which typically denote word boundaries. Our experiments show that for a fixed training and inference compute budget, SpaceByte outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "SpaceByte is a novel byte-level decoder architecture that closes the performance gap between byte-level and subword autoregressive language modeling and outperforms other byte-level architectures and roughly matches the performance of tokenized Transformer architectures."}, "embedding": {"model": "specter_v2", "vector": [0.1230086013674736, 0.8275270462036133, -0.34290745854377747, 0.06126122921705246, -0.7262560725212097, -0.20691533386707306, 0.8225652575492859, -0.14731645584106445, -0.570525050163269, -0.5014373660087585, 0.6752604246139526, -0.2570160925388336, 1.0489596128463745, 0.00639895536005497, -0.3197328746318817, 0.0043389201164245605, -0.6955859661102295, 0.017519153654575348, -0.12789571285247803, -0.026720328256487846, -0.6244171857833862, -0.461727112531662, -0.5426985025405884, 0.0875387191772461, 0.26507529616355896, 0.5310196280479431, -0.11100836843252182, 0.47584068775177, -0.5399669408798218, 0.4861528277397156, 0.2837340831756592, -0.4957665503025055, 0.2517920136451721, -0.15622727572917938, -0.09380587190389633, -0.10158322006464005, 0.5825083255767822, -0.743894636631012, -0.48923274874687195, 0.7942043542861938, -0.13267935812473297, 0.3164243996143341, 0.05630818009376526, -0.6120298504829407, 0.11007197201251984, 1.3223260641098022, 0.6301161646842957, 0.7286872863769531, -0.6262423396110535, -0.6917956471443176, 1.17482590675354, -1.290567398071289, -0.00779788987711072, 1.7095364332199097, 0.6390241980552673, 0.4899234175682068, -0.3027347922325134, -1.063059687614441, 1.0580230951309204, 0.0704733207821846, -0.743815004825592, -0.6501142382621765, -0.03234150633215904, -0.15663768351078033, 2.0585741996765137, -0.24616973102092743, 0.11543118208646774, 0.664206862449646, -0.2566232681274414, 1.4851908683776855, -0.14214277267456055, -0.4846029281616211, -0.3431980609893799, -0.020892364904284477, 0.35493549704551697, 0.7646250128746033, -0.19564653933048248, 0.20869575440883636, -0.7464048862457275, -0.34133777022361755, 0.37017038464546204, -0.1984892636537552, 0.3649013936519623, -0.011264234781265259, -0.16198702156543732, 0.9942045211791992, 0.010563966818153858, 0.6169542670249939, -0.3105902671813965, 1.2785217761993408, 0.6789978742599487, -0.21377474069595337, 0.37461596727371216, 0.2672486901283264, 0.1476617455482483, 0.2539856731891632, -1.186429738998413, 0.13589470088481903, -0.08165571093559265, 0.7232682108879089, -0.4591137170791626, 1.2454065084457397, -0.8557994961738586, 0.09961245954036713, 1.3295373916625977, 0.47659608721733093, 0.567273736000061, -0.5726538300514221, 0.47425130009651184, -1.212883710861206, -0.012461653910577297, -0.5088000893592834, 0.04648026451468468, -0.5887951254844666, -0.6329835057258606, -1.369300365447998, -0.36562618613243103, 0.07250481843948364, -0.880771279335022, 0.9229080677032471, -0.32708075642585754, 0.6739996671676636, 0.0009602338541299105, -1.8692706362344325e-05, 0.37775298953056335, 0.8108765482902527, 0.6390693187713623, -0.06960929930210114, 0.8935515284538269, -1.0548630952835083, -0.6155245900154114, -1.0493346452713013, 0.6358941793441772, -0.16895264387130737, -0.07835671305656433, -0.0685870498418808, -1.3355973958969116, -0.8085792660713196, -0.7233799695968628, 0.078542560338974, -0.45990797877311707, 0.12489596009254456, 0.7570769190788269, 0.8062635064125061, -0.7365815043449402, 0.5916515588760376, -0.768988311290741, 0.08123414218425751, 0.4458491802215576, -0.00883976649492979, 0.41259273886680603, 0.06655382364988327, -1.4050772190093994, 0.4315531253814697, 0.11439026892185211, -0.5801543593406677, -0.5357576608657837, -0.6765398979187012, -1.1841683387756348, 0.13608543574810028, 0.23407450318336487, -0.059657853096723557, 1.246018409729004, 0.23134630918502808, -1.363857626914978, 0.5794952511787415, -0.6826367974281311, -0.5159831047058105, 0.19905446469783783, -0.46164581179618835, -0.5746515393257141, -0.5795318484306335, -0.37722864747047424, 0.010676966980099678, 0.8253703713417053, -0.20539258420467377, -0.14613434672355652, 0.08090202510356903, -0.41800063848495483, -0.35837894678115845, -0.41029539704322815, 0.9313924312591553, -0.47282472252845764, -0.7154603600502014, 0.28930023312568665, 0.43196842074394226, -0.1799187958240509, -0.5313679575920105, -0.7481747269630432, -0.9811549782752991, 0.4624751806259155, -0.4014514088630676, 0.9672890901565552, -0.973885178565979, -0.9061089754104614, 0.0914730355143547, -0.2159978151321411, 0.2546067237854004, -0.6491707563400269, 0.6833680272102356, -0.433348685503006, 0.4429301619529724, 0.02160971239209175, -1.391310453414917, 0.18353326618671417, -0.31864622235298157, -1.2865984439849854, -0.06421749293804169, 0.014129213988780975, 0.8350066542625427, -0.5969117879867554, 0.13171152770519257, 0.12411906570196152, 0.39975517988204956, -1.112752079963684, 1.1821552515029907, -0.22752134501934052, -0.05046900361776352, -0.2914479076862335, -0.19443833827972412, 0.19246061146259308, 0.13110008835792542, 0.39138323068618774, -0.23263590037822723, -0.13442663848400116, 0.8665812015533447, -0.34060728549957275, 0.9234359860420227, -0.6412310004234314, 0.32102808356285095, -0.02053811401128769, -0.7971511483192444, 0.16162939369678497, 0.4039621651172638, -0.11854802072048187, -0.20746931433677673, 0.2422749549150467, 0.26514655351638794, -0.7629736661911011, 0.5778971910476685, 0.7503302097320557, 0.7614757418632507, -0.2201835662126541, 0.21075157821178436, 0.367452472448349, -0.26871150732040405, 0.21966871619224548, 0.5358498692512512, 0.4678078889846802, 0.2149055302143097, 0.24942053854465485, 0.3696502149105072, 0.21118876338005066, -0.9827364087104797, 0.2299831062555313, 0.5517098307609558, 0.7333954572677612, 0.695936918258667, 0.3751583993434906, -0.38346239924430847, -0.6652230620384216, -0.0772300735116005, 0.6275741457939148, 1.393611192703247, -0.5764601826667786, -0.4630358815193176, -0.6765891909599304, -0.1820669025182724, 0.14628079533576965, 0.30521807074546814, -0.055939048528671265, 0.012863012962043285, -0.7854662537574768, -0.8682866096496582, 1.1991690397262573, 0.3513650894165039, 0.7787672877311707, -0.33792293071746826, 0.011661705560982227, -0.1854235678911209, 0.22011646628379822, -1.1870501041412354, -0.456251323223114, 0.26811134815216064, -0.510131299495697, 0.5185376405715942, 0.16530942916870117, 0.23944157361984253, 0.03625727817416191, -0.6634938716888428, 0.7509616017341614, -0.5302671790122986, -0.005152471829205751, 0.12127489596605301, 0.44145992398262024, -0.7858904600143433, -1.0497890710830688, 0.10149224102497101, 0.3788204789161682, -0.044415075331926346, 0.1862085908651352, 0.5656831860542297, 0.26134514808654785, -0.26436981558799744, -0.18543940782546997, 0.060558710247278214, 0.067051962018013, -0.12063749134540558, 0.3748939335346222, -0.41079550981521606, -0.2594267427921295, -1.285512924194336, 0.6852560043334961, 0.20369172096252441, -0.6012923121452332, 0.21790564060211182, -0.6640084981918335, -0.34263095259666443, 0.6179668307304382, -0.5246689915657043, -0.21199291944503784, -0.9437606930732727, -0.016974465921521187, -0.3481784164905548, 0.11898365616798401, 0.14453500509262085, 0.3457927703857422, 0.4631134569644928, -0.2625935673713684, 0.7978789210319519, 0.3931303322315216, -0.41390007734298706, 0.7230244874954224, -1.0575535297393799, 0.5170542597770691, 0.20519156754016876, 0.4603620767593384, 0.18100684881210327, -0.4416075050830841, -0.8461626172065735, -0.23980891704559326, -0.44895920157432556, 0.021479040384292603, -0.13233283162117004, 0.1682269126176834, -0.901162326335907, -0.6467036008834839, 0.32308757305145264, -1.1649824380874634, -0.21487657725811005, 0.0703548714518547, 0.09535384923219681, -0.008360743522644043, -0.8587392568588257, -1.403459072113037, -0.8120959401130676, -0.5732125639915466, -0.8907636404037476, 0.3955407738685608, -0.18194712698459625, -0.4182140827178955, -0.4965288043022156, -0.23076851665973663, -0.2476203590631485, 0.917759358882904, -0.7324100136756897, 0.7519947290420532, -0.421997994184494, -0.3379117250442505, -0.538806140422821, 0.40192779898643494, 0.8921173214912415, -0.5540795922279358, 0.2099466472864151, -1.0424972772598267, -0.08964413404464722, -0.5466930866241455, -0.2708543837070465, 0.5238311886787415, 0.3056861162185669, 0.8102081418037415, -0.13714036345481873, -0.5225416421890259, 0.7210670113563538, 1.1402441263198853, -0.6076577305793762, 0.12512800097465515, -0.01947978138923645, 1.1441272497177124, 0.04273362085223198, -0.3864223062992096, 0.8686432838439941, 0.04999757185578346, 0.5150318145751953, 0.24397842586040497, -0.30729660391807556, -0.007640372961759567, -0.7194076776504517, 0.7233154773712158, 1.5240060091018677, 0.40548408031463623, -0.17586061358451843, -0.9496384263038635, 0.7789770364761353, -0.9842823147773743, -1.0164073705673218, 0.6996381282806396, 0.8405075073242188, 0.16412390768527985, -0.4965088367462158, -0.25610023736953735, -0.1534801572561264, 0.46741002798080444, 0.7325092554092407, -0.43007516860961914, -1.037866234779358, 0.10722122341394424, 0.46785491704940796, 0.5328624844551086, 0.7872834205627441, -0.32318952679634094, 1.0386637449264526, 14.832898139953613, 1.0446960926055908, -0.44541049003601074, 0.6596440672874451, 0.6041196584701538, 0.13812187314033508, -0.620993971824646, -0.19085539877414703, -1.5802146196365356, -0.057481732219457626, 0.9901816248893738, 0.08345459401607513, 0.49052512645721436, 0.3623104989528656, 0.2527003884315491, 0.4560447633266449, -0.1261408030986786, 0.4449838399887085, 0.6424354910850525, -1.1358476877212524, 0.4612904191017151, 0.15215638279914856, 0.0478467233479023, 0.7722099423408508, 0.9741392731666565, 0.9426782131195068, 0.8663200736045837, -0.5684241652488708, 0.6901034712791443, -0.15774895250797272, 0.62501460313797, -0.029695970937609673, 0.19894017279148102, 0.5441751480102539, -0.8509806990623474, 0.03428596258163452, -0.515124499797821, -1.0493042469024658, 0.21878571808338165, 0.2425445020198822, -0.872829794883728, -0.6788209080696106, -0.3889716863632202, 0.5597066283226013, 0.2004282921552658, 0.04735373705625534, -0.1946663111448288, 1.382594108581543, -0.4784995913505554, 0.0636855810880661, 0.39648500084877014, 0.38470110297203064, 0.34022828936576843, 0.04851015284657478, 0.1374264508485794, 0.08800856024026871, 0.08579367399215698, 0.2957414388656616, -0.3564445972442627, -0.17641663551330566, -0.10700132697820663, -0.5643808245658875, 0.3375973701477051, 0.7412852644920349, 0.550986111164093, -0.10900121182203293, -0.4068365693092346, 0.25925520062446594, 0.5383666157722473, 0.3066561818122864, -0.5445843935012817, 0.2908347547054291, 0.5313378572463989, -0.4319961965084076, 0.23976631462574005, 0.1670794039964676, -0.21445322036743164, -0.5571330189704895, -0.8594784140586853, -0.44958195090293884, 0.3615305721759796, -0.6095965504646301, -0.2640155553817749, 0.8921124935150146, -0.3216952979564667, -0.16071529686450958, 0.07377537339925766, -0.5148770809173584, -0.4267210364341736, 0.651235818862915, -1.079032063484192, -0.9385563731193542, 0.4995571970939636, -0.1410752534866333, -0.3337253928184509, -0.23189863562583923, 1.3805633783340454, 0.03553421422839165, -0.6740121245384216, 0.324219673871994, 0.6763904094696045, 0.30082815885543823, -0.2960480749607086, -0.5588527917861938, 1.3091480731964111, 0.5666685700416565, 0.10077148675918579, 0.4198056161403656, 0.3209216594696045, 0.08544108271598816, -0.6047598123550415, -0.14580868184566498, 0.8968058228492737, -0.812149703502655, -0.1838102489709854, -1.0212212800979614, -0.5658429265022278, 0.5642653107643127, 0.356123149394989, -0.277773380279541, 0.35890528559684753, 0.3963320255279541, -0.864857017993927, -0.17508813738822937, -0.40343382954597473, 0.008042016997933388, 0.15257345139980316, -0.8691349029541016, -0.10153618454933167, -0.15224982798099518, 0.4188067317008972, -0.8216238021850586, -0.4780108332633972, -0.32223907113075256, 0.21440118551254272, 0.0900830626487732, 1.074087381362915, -0.5173081159591675, 0.6456271409988403, 1.1679292917251587, -0.1378961056470871, -0.9423998594284058, 0.03037455677986145, -0.9462842345237732, 0.08045726269483566, 0.48441609740257263, 0.5108106136322021, -0.6227919459342957, 0.39636874198913574, 0.9484749436378479, 0.13231435418128967, -0.23072965443134308, -0.8170747756958008, -0.6175865530967712, 0.46775463223457336, -0.7737926840782166, 0.2953207194805145, 0.09028740227222443, -0.05817769095301628, -0.1970224529504776, 0.1021808534860611, 0.5586019158363342, -0.02026786282658577, -0.840528666973114, 0.519868016242981, 0.42933353781700134, -0.029542937874794006, -0.40783068537712097, -0.43906986713409424, -1.3150978088378906, 0.47226306796073914, -1.1911228895187378, -0.16992895305156708, -0.7320770621299744, -0.5963537693023682, -0.0161378663033247, -0.1856822520494461, 0.23573580384254456, 0.4502294659614563, -0.16658996045589447, -0.0948851928114891, -0.5627262592315674, -0.18586799502372742, 0.8096179366111755, 0.5835155844688416, -0.8835201263427734, 0.5371164083480835, 0.10903300344944, 0.4574574828147888, 0.4730861783027649, 0.35618025064468384, -0.4294562339782715, -0.6916041374206543, -1.4513623714447021, 0.010821845382452011, -0.17071890830993652, -0.06879398226737976, -0.636673092842102, 0.4787571132183075, 0.3031666874885559, -0.18973250687122345, -0.02642674371600151, 0.546428382396698, -0.7614167928695679, -0.6461265087127686, 0.6713278889656067, -0.8703581094741821, 0.21295413374900818, 0.2083297073841095, -0.5529497861862183, -0.2543642818927765, 0.8276764750480652, -0.08338566869497299, -1.0414750576019287, -0.6899170279502869, 0.5231642723083496, -0.8971982598304749, 0.12896689772605896, -0.21751922369003296, -0.13806003332138062, -0.9641222357749939, -0.5545659065246582, -0.18514515459537506, 0.26162290573120117, -0.5777512192726135, 1.0708674192428589, 0.2230803668498993, -0.8499871492385864, -0.13620324432849884, 0.4384235143661499, -0.2937551438808441, -0.21018680930137634, 0.38299036026000977, 0.22811272740364075, -0.4759168326854706, 0.7085705995559692, 0.45484215021133423, 0.7145658135414124, -1.171878457069397, -0.12528258562088013, 0.5458604693412781, -0.6272860765457153, -0.32078230381011963, 1.4751399755477905, -0.23417189717292786, -0.8426042795181274, 0.28279027342796326, -1.2993125915527344, -0.48938795924186707, -0.21088236570358276, 0.5161125063896179, 0.2802099287509918, -0.15132910013198853, -0.2301599532365799, -0.428129106760025, -0.15444162487983704, -0.3173559904098511, -0.6874603629112244, 0.3020266592502594, -0.03347794711589813, -0.3883596360683441, 0.6176653504371643, 0.8533481359481812, -0.25401073694229126, -0.4855667054653168, -0.808748185634613, -0.6902158856391907, -0.05501188710331917, 0.40917208790779114, 0.013760799542069435, -0.5124557614326477, 0.9849817156791687, 0.21962285041809082, 0.269806444644928, -0.4255107045173645, -0.5432797074317932, 0.5238840579986572, 0.5708088278770447, 0.14216473698616028, -0.8301633596420288, -0.7464001774787903, 1.5262011289596558, 1.0443743467330933, -0.6734054088592529, 0.352379709482193, -0.2748126983642578, -0.7033818364143372, 0.5700011849403381, 0.3814903795719147, -0.16867898404598236, 0.9880095720291138, -0.0815383717417717, 0.3248402774333954, 0.5481053590774536, -0.9036831259727478, -0.2576574683189392, 0.572969377040863, 0.7120897173881531, 0.9872288107872009, 0.36023861169815063, 0.18391485512256622, 0.9445630311965942, -0.045327045023441315, -0.5048461556434631, 0.34070703387260437, 0.23212631046772003, -0.1425679326057434, -0.13690689206123352, -0.4188331067562103, 0.6613674163818359, -0.9897812604904175, -1.0844132900238037, 0.04626692086458206, 0.5385293364524841, -0.00811099074780941, 0.5426254868507385, 1.020459771156311, 0.09313564002513885, 0.30578362941741943, 0.2531990110874176, 0.2940061688423157, -0.517958402633667, -0.42056775093078613, -0.09550352394580841, -0.26635822653770447, -0.22192680835723877, 0.17097824811935425, -0.3911842703819275, -0.5091724991798401, -0.5548308491706848, 0.3213190734386444, 0.08503390103578568, 0.5142514109611511, 1.061340570449829, 0.2689613401889801, 0.6152423024177551, -0.15226909518241882, -0.36659112572669983, -0.7665761113166809, -0.8395267128944397, -0.45142629742622375, -0.40682247281074524, -0.1393834501504898, 0.16075891256332397, -0.08447054773569107, -0.2836757302284241]}, "authors": [{"authorId": "2297775122", "name": "Kevin Slagle"}], "references": [{"paperId": "e27c9c39ecb67e8e7708d8d53bec2f891cfa7a40", "title": "Training LLMs over Neurally Compressed Text"}, {"paperId": "a1f23f04421cdc62e80fe9f04c1ce60f4a6af9f0", "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models"}, {"paperId": "8efaa8206874c2f7a79bba2a9bcba542e4cabf31", "title": "MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling"}, {"paperId": "a6e2dca754f3dc625a9da5f10f9b7a57079bfd27", "title": "MambaByte: Token-free Selective State Space Model"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "28c75ab7f6951f1c2bb349b34abc3204ba8b9498", "title": "Toucan: Token-Aware Character Level Language Modeling"}, {"paperId": "a401510c434b2274b299e9444085df0b18808aaa", "title": "Learn Your Tokens: Word-Pooled Tokenization for Language Modeling"}, {"paperId": "f5789596531fad358c3166fdb5bd72d8e661c32c", "title": "Small-scale proxies for large-scale Transformer training instabilities"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "17fbffb05fa14e21d1c506fd5f0f568b955fe983", "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "879a7f5abdb7ab803d48172d4f0830965f989d46", "title": "Language Model Tokenizers Introduce Unfairness Between Languages"}, {"paperId": "412e266cddfd87c79087a88ba1e4d11b89a45a13", "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers"}, {"paperId": "ea8197cb357af6f89e8b6e5548897236a24719b1", "title": "What is the best recipe for character-level encoder-only modelling?"}, {"paperId": "bb553de910c20a31c951c6211d9af19ad202e260", "title": "A Systematic Analysis of Vocabulary and BPE Settings for Optimal Fine-tuning of NMT: A Case Study of In-domain Translation"}, {"paperId": "2c8935ec872eca14636a090e5f6b49bc1c90c30d", "title": "Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "bc518f89f1ea8244ca019fd03c8c70f9d3c7fe9d", "title": "MANTa: Efficient Gradient-Based Tokenization for Robust End-to-End Language Modeling"}, {"paperId": "ac7a19bc7dd06fd2b6501d3ff7098a29793d7da6", "title": "Subword-Delimited Downsampling for Better Character-Level Translation"}, {"paperId": "5e52d654fd31f04c1bd884cd5480e6af8c95ad50", "title": "Efficient Transformers with Dynamic Token Pooling"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "0232715f9089e3a2fc002cff6737bb9939805b8d", "title": "Local Byte Fusion for Neural Machine Translation"}, {"paperId": "ad6c25a46a083e02dbfcdd4b6341945d517b5e31", "title": "A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning"}, {"paperId": "231e768f0cd280faa0f725bb353262cb4fed08d1", "title": "Hierarchical Transformers Are More Efficient Language Models"}, {"paperId": "e79d1206292bc5e67ba19737d87d4b2ea4a37105", "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"}, {"paperId": "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d", "title": "Consistent Accelerated Inference via Confident Adaptive Transformers"}, {"paperId": "969287b8a96e242793b11f0dbb99ec341228106f", "title": "Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation"}, {"paperId": "837ac4ed6825502f0460caec45e12e734c85b113", "title": "Dynamic Neural Networks: A Survey"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "db8fdf93aaf27cfd93fa0a41d78ce11b61a3a6dc", "title": "The Depth-to-Width Interplay in Self-Attention."}, {"paperId": "473921de1b52f98f34f37afd507e57366ff7d1ca", "title": "CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters"}, {"paperId": "806adbb35ed4a95f51518f5962fd59685ad4706b", "title": "Query-Key Normalization for Transformers"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "f2fc9ef411846dd577c26225ce93f50bb1fa760b", "title": "Multi-scale Transformer Language Models"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "a39398f68ae7e042f2ef5009e31b4e6a20fd5736", "title": "Learning Deep Transformer Models for Machine Translation"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "9e34a9afb4a0236de1b85b9f0bca3ba615691be8", "title": "Convolutional Networks with Adaptive Inference Graphs"}, {"paperId": "f37ea0b173dd0403a5028c12746082d31dff60bb", "title": "SkipNet: Learning Dynamic Routing in Convolutional Networks"}, {"paperId": "e16cfe727e27be27115d0f842375c46e7e3f384b", "title": "BlockDrop: Dynamic Inference Paths in Residual Networks"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "63e39cdf1ad884da6bc69096bb3413b5b1100559", "title": "Using the Output Embedding to Improve Language Models"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "fa6abf6195fda6817d3a67e2106ecc4a1a010817", "title": "Peter"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e", "title": "On the difficulty of training recurrent neural networks"}, {"paperId": null, "title": "Compression Represents Intelligence Linearly arXiv e-prints"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Pseudocode See Listing 1 for Pytorch pseudocode for the SpaceByte forward method. The implementation of SpaceByte that we used in our experiments"}]}