{"paperId": "88695b5bb6462872ce1dd946cff00dd6ebabf2d9", "title": "Scaling TransNormer to 175 Billion Parameters", "abstract": "We present TransNormerLLM, the \ufb01rst linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and ef\ufb01ciency. TransNormerLLM evolves from the previous linear attention architecture TransNormer [32] by making advanced modi\ufb01cations that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, and inference acceleration and stabilization. Speci\ufb01cally, we use LRPE [34] together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over 20% . Furthermore, we have developed a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior ef\ufb01ciency during both training and inference stages. Scalability is at the heart of our model\u2019s design, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, all while maintaining outstanding performance metrics. Rigorous validation of our model design is achieved through a series of comprehensive experiments on our self-collected corpus, boasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure data quality and relevance, we implement a new self-cleaning strategy to", "venue": "arXiv.org", "year": 2023, "citationCount": 15, "influentialCitationCount": 2, "openAccessPdf": {"url": "https://arxiv.org/pdf/2307.14995", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A robust inference algorithm is developed that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior ef\ufb01ciency during both training and inference stages."}, "embedding": {"model": "specter_v2", "vector": [-0.14313557744026184, 0.43616172671318054, -0.6927779912948608, -0.11999791115522385, -0.458465039730072, -0.2638363838195801, 0.7761058807373047, -0.2850388288497925, -0.47838693857192993, -0.2247992604970932, 0.7552324533462524, 0.1353631317615509, 0.8240998983383179, 0.41672104597091675, -0.37244996428489685, 0.028961094096302986, -1.087138295173645, 0.2510826587677002, -0.2932504415512085, -0.27184009552001953, 0.13800881803035736, -0.7183053493499756, -1.0225839614868164, 0.019571196287870407, 0.3495597243309021, 0.5386126041412354, 0.07874097675085068, 0.683501124382019, -0.5148215293884277, 0.2808431088924408, 0.3378922641277313, -0.07312974333763123, 0.11953279376029968, 0.16134709119796753, -0.34856125712394714, -0.1992236226797104, 0.4246358275413513, -0.11935655027627945, -0.12693437933921814, 0.8504235744476318, -0.363261878490448, 0.3428604006767273, 0.16219770908355713, -0.5017443895339966, -0.7626803517341614, 1.0108304023742676, 0.46951302886009216, 0.6771653294563293, -0.4025290012359619, -0.427996963262558, 1.1831392049789429, -1.7830626964569092, 0.2991536855697632, 1.3153959512710571, 0.46715161204338074, 0.2097654491662979, -0.2996447682380676, -0.6289369463920593, 0.7497707009315491, 0.16452321410179138, -0.9822348356246948, -0.4895975887775421, -0.039692431688308716, -0.16559836268424988, 2.1058433055877686, -0.2764231562614441, 0.11157559603452682, 0.602318525314331, -0.19149160385131836, 1.312134861946106, -0.14390544593334198, -0.692837655544281, -0.22447587549686432, 0.0880991742014885, 0.4661163091659546, 0.30691975355148315, -0.36859753727912903, 0.11871474981307983, -0.8614934086799622, -0.19184960424900055, 0.1729922890663147, -0.13285592198371887, 0.3841553032398224, 0.20070503652095795, -0.34369173645973206, 0.5743216872215271, 0.2220851331949234, 1.1235814094543457, -0.4637422263622284, 1.053439736366272, 0.8219849467277527, 0.15103667974472046, 0.363960325717926, 0.38007277250289917, -0.28338173031806946, -0.12992878258228302, -1.0496867895126343, 0.09308334439992905, -0.06881730258464813, 0.9290253520011902, -0.017084557563066483, 0.6068382263183594, -0.6457529664039612, -0.08000021427869797, 1.269948959350586, 0.37607231736183167, 0.5680009126663208, -0.6237382888793945, 0.32864508032798767, -0.6005844473838806, -0.024238239973783493, -0.6097889542579651, -0.2023053765296936, -0.3340439200401306, -0.8750894665718079, -1.4257174730300903, -0.7947281002998352, 0.051479436457157135, -0.7269881367683411, 0.7794234156608582, -0.19216041266918182, -0.07524407655000687, -0.09462030231952667, 0.37555116415023804, 0.6094653606414795, 0.6452992558479309, 0.3024802803993225, 0.15233679115772247, 1.0391381978988647, -0.9830963015556335, -0.739850640296936, -1.1183215379714966, 0.9840602874755859, -0.48980528116226196, 0.21923428773880005, -0.14709044992923737, -1.3416707515716553, -0.8153468370437622, -0.6035431623458862, -0.02195868082344532, -0.57557213306427, 0.4404078722000122, 0.9872806668281555, 0.2951200008392334, -0.7346766591072083, 0.4853852093219757, -0.5327757000923157, -0.05302247405052185, 0.24885830283164978, -0.01495184563100338, 0.2008669525384903, -0.15531812608242035, -1.49354088306427, 0.29276424646377563, 0.32087835669517517, -0.16411802172660828, -0.035380639135837555, -0.8414412140846252, -1.1130980253219604, -0.09554298967123032, 0.1631951928138733, 0.17316503822803497, 1.2533063888549805, 0.04366537928581238, -1.0866155624389648, 0.5531542301177979, -0.5330524444580078, 0.15215951204299927, 0.03585314005613327, -0.3148176372051239, -0.58050537109375, -0.8038195371627808, -0.006629063282161951, 0.3601391017436981, 0.4899149239063263, 0.01470306795090437, -0.0207024198025465, 0.16751223802566528, -0.45169422030448914, -0.206346794962883, -0.5644508004188538, 0.9685080051422119, -0.40575796365737915, -0.23407508432865143, 0.11424165219068527, 0.6336230039596558, 0.15072603523731232, -0.767492949962616, -0.5877537727355957, -1.2410520315170288, 0.7175371050834656, 0.040816184133291245, 1.0169456005096436, -0.8867360949516296, -0.6873977780342102, 0.0033649937249720097, -0.07966721057891846, -0.013179818168282509, -0.7702817916870117, 0.6649059057235718, -0.6706475615501404, 0.5109906792640686, 0.3231601119041443, -1.3501578569412231, 0.3291110694408417, -0.2392181158065796, -0.7490049004554749, -0.08865629136562347, 0.024592768400907516, 1.1604384183883667, -0.850243330001831, 0.05946330726146698, 0.0732339397072792, 0.6698710918426514, -0.9826235771179199, 1.1465778350830078, -0.342345267534256, 0.08245395869016647, 0.025576403364539146, -0.1643884778022766, 0.14337465167045593, -0.29135096073150635, 0.3679750859737396, -0.6643774509429932, -0.3990863561630249, 0.5678724050521851, -0.4582138955593109, 1.0864660739898682, -0.46178117394447327, 0.5531174540519714, 0.11823239177465439, -0.6189800500869751, 0.2981500029563904, 0.4286770224571228, 0.10843934118747711, -0.6813802123069763, 0.36034539341926575, 0.5343096852302551, -0.8446168303489685, -0.12937554717063904, 1.019909381866455, 0.8727110624313354, -0.37161698937416077, 0.147169828414917, 0.49862217903137207, 0.07786504179239273, 0.6068999171257019, 0.5470095872879028, 0.4548257887363434, 0.4930093288421631, 0.1417144536972046, -0.26774802803993225, 0.13957883417606354, -0.6948186159133911, 0.024935338646173477, 0.3914617896080017, 0.7411478757858276, 0.5853315591812134, 0.3555656373500824, -0.7640751600265503, -0.6248966455459595, 0.5767262578010559, 0.5306336879730225, 1.6685303449630737, -0.4353484511375427, -0.3195503354072571, -0.7140136361122131, -0.09250286221504211, -0.3098895847797394, 0.07115855813026428, -0.48719775676727295, 0.04492836818099022, -0.9755784869194031, -1.1954665184020996, 0.7743043303489685, 0.20335491001605988, 0.8033543825149536, -0.694223165512085, -0.13696588575839996, -0.4087279438972473, 0.08317065238952637, -0.8124979138374329, -0.7531008720397949, 0.275191992521286, -0.08104898780584335, 0.18237276375293732, 0.1195889562368393, 0.055927518755197525, 0.2269529104232788, -0.9250892400741577, 1.0479573011398315, -0.4385395050048828, -0.1481996327638626, 0.06702988594770432, 0.36945217847824097, -0.33075428009033203, -0.6805284023284912, 0.6944853663444519, 0.4770883619785309, -0.06495445966720581, 0.4248584508895874, 0.47483158111572266, -0.06362425535917282, -0.20237651467323303, -0.3879009485244751, -0.01169782318174839, 0.0757610946893692, -0.19467215240001678, 0.5334158539772034, -0.5469900965690613, 0.04138931632041931, -1.1476730108261108, 0.8567028641700745, -0.13940711319446564, -0.37139829993247986, 0.01703609526157379, -0.5898724794387817, -0.3481234908103943, 0.5080577731132507, -0.6865746974945068, -0.20989233255386353, -0.9129294753074646, 0.1666090339422226, -0.33968472480773926, -0.01105030719190836, 0.25583788752555847, 0.4915430247783661, 0.3121977150440216, 0.28744426369667053, 0.5706298351287842, -0.06990328431129456, -0.37937164306640625, 0.8269068002700806, -0.6774318814277649, 0.6281728744506836, 0.2522314488887787, 0.213908851146698, -0.17218849062919617, -0.31805211305618286, -0.809199333190918, -0.5038278698921204, -0.31159108877182007, -0.2594870924949646, -0.24149492383003235, 0.26918619871139526, -0.5426310300827026, -1.273439884185791, -0.18772657215595245, -1.1946269273757935, -0.12237028032541275, 0.033216655254364014, -0.06326223909854889, 0.10975658148527145, -0.865825355052948, -1.1924457550048828, -0.8070291876792908, -0.8834049105644226, -0.9378295540809631, 0.39375168085098267, -0.1538453996181488, -0.4771130681037903, -0.7603793740272522, 0.11468114703893661, -0.4912945330142975, 1.3390651941299438, -0.5763126015663147, 0.22642351686954498, -0.05953408032655716, -0.15070456266403198, -0.45537880063056946, 0.13021928071975708, 0.7578083872795105, -0.34575822949409485, 0.3031599819660187, -0.7863261699676514, 0.03354772552847862, -0.4273250997066498, -0.396854043006897, 0.10132674127817154, 0.1927156299352646, 0.6223310828208923, -0.12224403023719788, -0.535539984703064, 0.34958869218826294, 1.162453532218933, -0.9492635130882263, 0.14312419295310974, 0.22241643071174622, 1.320381999015808, 0.026203781366348267, -0.34440043568611145, 0.5639042258262634, 0.5002387762069702, 0.43796589970588684, 0.17967285215854645, -0.2470056712627411, -0.08119723200798035, -0.3442699611186981, 0.5750518441200256, 2.025871753692627, 0.168620765209198, 0.04310845211148262, -0.8565505146980286, 0.8568906784057617, -1.16957688331604, -0.8400102257728577, 0.30026641488075256, 0.5682428479194641, 0.5471301674842834, -0.6215874552726746, -0.29734501242637634, -0.4160222113132477, 0.3810197412967682, 0.25532618165016174, -0.4496089816093445, -0.8644893169403076, -0.043621815741062164, 0.02226814068853855, 0.3119772970676422, 0.6341263651847839, -0.15857945382595062, 0.7209556698799133, 14.960143089294434, 0.8640173077583313, -0.1397598534822464, 0.6461957693099976, 0.6690410375595093, -0.0789850577712059, -0.27059489488601685, -0.23688307404518127, -1.4983248710632324, 0.05942031368613243, 1.1579185724258423, 0.12626522779464722, 0.7482216954231262, 0.21209321916103363, 0.3825514018535614, 0.5009735822677612, -0.41418641805648804, 0.6108612418174744, 0.7845119833946228, -1.2915486097335815, -0.0017651456873863935, 0.2033533900976181, 0.5586269497871399, 0.958901584148407, 0.7215896248817444, 1.1077085733413696, 0.5815813541412354, -0.38435325026512146, 0.5293081402778625, 0.44773200154304504, 0.9945636987686157, -0.05274408310651779, 0.4514326751232147, 0.37407609820365906, -0.9465556144714355, 0.05365188792347908, -0.3423042297363281, -1.284958004951477, 0.41600632667541504, 0.4636036157608032, -0.5030125379562378, -0.5738840699195862, -0.0523039773106575, 0.8059665560722351, 0.1989900767803192, 0.07073783129453659, -0.08093453198671341, 0.8744330406188965, -0.13090448081493378, -0.020366717129945755, 0.4218568503856659, 0.5609684586524963, 0.04912716895341873, 0.4675699770450592, -0.06625586748123169, -0.23502881824970245, 0.19428342580795288, 0.6780382990837097, -0.7201532125473022, 0.06623809784650803, 0.06174894794821739, -0.4400879442691803, 0.1338108330965042, 1.1652342081069946, 0.7390973567962646, -0.10922246426343918, -0.25612643361091614, 0.10733203589916229, 0.6600077152252197, 0.29406675696372986, -0.354671448469162, -0.18493449687957764, 0.3005296587944031, -0.440177857875824, 0.025320162996649742, 0.39638492465019226, -0.1877327710390091, -0.6847668290138245, -0.36041247844696045, -0.37885481119155884, -0.013026650063693523, -0.5546519756317139, -0.771403431892395, 1.0378762483596802, -0.3673628270626068, -0.2512991726398468, -0.033939313143491745, -0.5233054161071777, -0.06691104918718338, 0.6969917416572571, -1.4182835817337036, -0.8275704383850098, 0.4051608741283417, -0.579690158367157, -0.6148940324783325, 0.1901518553495407, 1.2197624444961548, 0.6112588047981262, -0.7103902697563171, 0.13217952847480774, 0.18627749383449554, 0.0387350432574749, -0.06253362447023392, -0.660447895526886, 1.1176517009735107, 0.21842247247695923, -0.44489729404449463, 0.49943774938583374, 0.1704159379005432, 0.10733719170093536, -0.8546353578567505, -0.24532006680965424, 1.0342403650283813, -0.9550583362579346, -0.15029850602149963, -1.1168266534805298, -0.9448626637458801, 0.6567838191986084, 0.6818339228630066, -0.12260711938142776, 0.20558889210224152, 0.42103663086891174, -0.5997314453125, -0.10674462467432022, -0.4840763211250305, -0.1021968275308609, 0.18519490957260132, -0.6766597032546997, -0.20960091054439545, 0.23162099719047546, 0.42544713616371155, -1.0775407552719116, -0.9099875092506409, -0.44374001026153564, 0.14911898970603943, 0.26839765906333923, 0.8221427202224731, -0.2526000440120697, 0.6067739725112915, 0.8595845103263855, -0.06531161814928055, -0.7126860022544861, 0.01994066685438156, -0.9755104184150696, -0.14009949564933777, 0.5232656002044678, 0.6164697408676147, -0.29162320494651794, 0.243342325091362, 0.929578423500061, 0.1857575923204422, -0.5377206802368164, -0.6959635019302368, -0.4919027090072632, 0.19942888617515564, -0.591693639755249, 0.45859161019325256, 0.13411933183670044, 0.3169040381908417, 0.11967945098876953, 0.12157128006219864, 0.18156303465366364, -0.015256806276738644, -0.8219646215438843, 0.2417762726545334, 0.1336953192949295, 0.1364000141620636, -0.48344555497169495, -0.4243835210800171, -1.3654253482818604, 0.05077630653977394, -1.2080754041671753, 0.030311018228530884, -0.8276209831237793, -0.19748717546463013, -0.11063556373119354, -0.08876194804906845, 0.3634227514266968, 0.3988770842552185, 0.10954684764146805, -0.45396625995635986, -0.46212682127952576, -0.803553581237793, 1.0550432205200195, 0.809554398059845, -0.6266785860061646, 0.1340879499912262, -0.40799859166145325, 0.1335197389125824, 0.2666638195514679, 0.3553672134876251, -0.3494090735912323, -0.28271159529685974, -1.4623674154281616, 0.7138150334358215, -0.46334728598594666, -0.24568839371204376, -0.3751398026943207, 0.6516904830932617, 0.5102225542068481, -0.24263300001621246, 0.043821822851896286, 0.24678002297878265, -0.5831776261329651, -0.6574164628982544, 0.18745435774326324, -0.7465307116508484, 0.40866053104400635, 0.25364813208580017, -0.9550739526748657, -0.23013871908187866, 0.8348336815834045, -0.30410805344581604, -0.8816472291946411, -0.5616635680198669, 0.45029497146606445, -0.6096749305725098, 0.2853110432624817, -0.4517233967781067, 0.040406644344329834, -1.1983495950698853, -0.25871533155441284, -0.038116853684186935, 0.49444010853767395, -0.5772058367729187, 1.1410950422286987, 0.022939631715416908, -1.319685697555542, -0.16334091126918793, 0.39263102412223816, -0.040116745978593826, -0.13871538639068604, 0.49666959047317505, 0.6146005392074585, -0.12690407037734985, 0.6999317407608032, 0.5168124437332153, 0.20609629154205322, -0.7715219855308533, 0.12648791074752808, 0.5390108823776245, -0.3630123436450958, -0.20157615840435028, 1.0301222801208496, -0.5063768625259399, -1.1972228288650513, 0.16519266366958618, -1.3950481414794922, -0.5642144680023193, -0.36527881026268005, 0.7026892304420471, 0.25063174962997437, 0.14540062844753265, -0.4100485146045685, -0.666024386882782, -0.013298409059643745, -0.14887212216854095, -0.373665452003479, 0.6166414022445679, -0.15719301998615265, -0.6808878779411316, 0.5579518675804138, 1.046279788017273, -0.43057647347450256, -0.3739396929740906, -0.7769127488136292, -0.4859289526939392, 0.1204235777258873, 0.5145256519317627, -0.388884961605072, -0.640916109085083, 0.5832929611206055, 0.471582293510437, 0.659001886844635, -0.26412373781204224, -0.10134993493556976, 0.30196788907051086, 0.6211985945701599, -0.22418712079524994, -0.8473043441772461, -0.6008033752441406, 1.482698917388916, 1.1779087781906128, -0.7374923825263977, 0.1294274777173996, 0.11093074083328247, -0.6668795347213745, 0.5643383264541626, 0.21377380192279816, -0.03210623934864998, 0.7378137707710266, -1.2777966730936896e-05, 0.0020073573105037212, 0.024126527830958366, -1.0378879308700562, -0.15885145962238312, 0.5316227674484253, 0.5925845503807068, 1.1976745128631592, 0.3271174430847168, 0.14311832189559937, 0.5323177576065063, 0.1448611319065094, 0.1425928920507431, 0.29305300116539, 0.25176000595092773, 0.019905541092157364, -0.052834946662187576, -0.1141723245382309, 0.4423096179962158, -0.5694831013679504, -0.8582773804664612, 0.30930641293525696, 0.3965725600719452, 0.48709243535995483, 0.5162258744239807, 0.9990086555480957, 0.24717678129673004, 0.5465524196624756, -0.010368802584707737, 0.22851699590682983, -0.3682824373245239, -0.42078015208244324, -0.1303088217973709, -0.6866072416305542, -0.08948773890733719, -0.12418974936008453, -0.5894719362258911, -0.5958500504493713, -0.41374385356903076, 0.25276294350624084, 0.1040281280875206, 0.6805577278137207, 1.100767731666565, 0.7949685454368591, 0.6719059348106384, -0.26673299074172974, -0.7992088198661804, -0.25095120072364807, -1.251344084739685, -0.09862664341926575, -0.1570407897233963, -0.22010304033756256, 0.03215397894382477, -0.2395114302635193, -0.39169424772262573]}, "authors": [{"authorId": "2171650015", "name": "Zhen Qin"}, {"authorId": "2179703418", "name": "Dong Li"}, {"authorId": "2225238340", "name": "Weigao Sun"}, {"authorId": "8397429", "name": "Weixuan Sun"}, {"authorId": "2116517206", "name": "Xuyang Shen"}, {"authorId": "2118234357", "name": "Xiaodong Han"}, {"authorId": "2156252901", "name": "Yunshen Wei"}, {"authorId": "2154762987", "name": "Baohong Lv"}, {"authorId": "2288583390", "name": "Fei Yuan"}, {"authorId": "2288690225", "name": "Xiao Luo"}, {"authorId": "2281747139", "name": "Yu Qiao"}, {"authorId": "2266275708", "name": "Yiran Zhong"}], "references": [{"paperId": "2f0203386f3dcbffb47c9f7fe2d19d373d9dda2f", "title": "Exploring Transformer Extrapolation"}, {"paperId": "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8", "title": "Linearized Relative Positional Encoding"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "f35f5aedc30e2c5ded210d9c91ba6e84bd029425", "title": "Toeplitz Neural Network for Sequence Modeling"}, {"paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da", "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "54155c2977a977bf129849455dcae3a2b79b3f41", "title": "Simple Hardware-Efficient Long Convolutions for Sequence Modeling"}, {"paperId": "ac608a4a6b19b3208e560eee5daadb3cc18638a2", "title": "Efficient Attention via Control Variates"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "86c8d930b492a4f9cadc6c60aecdaaded49acc86", "title": "Neural Architecture Search on Efficient Transformers and Beyond"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f", "title": "Linear Complexity Randomized Self-attention Mechanism"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "e65c84e2778d7b13b7541e6b14ff790b624a24ec", "title": "A Study of BFLOAT16 for Deep Learning Training"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": null, "title": "Scaling language modeling with pathways, 2022"}, {"paperId": null, "title": "The long-document"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "A suite for analyzing large"}]}