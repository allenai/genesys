{"paperId": "39638f73ecd4080592d9fd9270fd7aa25df02f63", "title": "Dive into Big Model Training", "abstract": "The increasing scale of model size and continuous improvement of performance herald the arrival of the Big Model era. In this report, we explore what and how the big model training works by diving into training objectives and training methodologies. Specifically,training objectives describe how to leverage web-scale data to develop extremely capable and incredibly large models based on self-supervised learning, and training methodologies which are based on distributed training describe how to make big model training a reality. We summarize the existing training methodologies into three main categories: training parallelism, memory-saving technologies, and model sparsity design. Training parallelism can be categorized into data, pipeline, and tensor parallelism according to the dimension of parallelism that takes place. Memory-saving technologies are orthogonal and complementary to training parallelism. And model sparsity design further scales up the model size with a constant computational cost. A continuously updated paper list of big model training is provided at https://github.com/qhliu26/BM-Training.", "venue": "arXiv.org", "year": 2022, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2207.11912", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "What and how the big model training works is explored by diving into training objectives and training methodologies, which describe how to leverage web-scale data to develop extremely capable and incredibly large models based on self-supervised learning."}, "embedding": {"model": "specter_v2", "vector": [-0.19944877922534943, 0.38348129391670227, -0.5799530148506165, 0.05977964401245117, -0.016250822693109512, 0.021614668890833855, 0.365185022354126, -0.32779237627983093, -0.29417338967323303, -0.3700256049633026, 0.11459487676620483, -0.043409913778305054, 0.5036271214485168, 0.10849431157112122, -0.2064826786518097, 0.0520925335586071, -1.3732463121414185, 0.4177919924259186, 0.17677843570709229, -0.21660515666007996, -0.2580760717391968, -0.2207220494747162, -1.3916492462158203, 0.46322154998779297, 0.4712069034576416, 1.0123260021209717, -0.15340909361839294, 0.9113937020301819, -0.23975436389446259, 0.27725881338119507, 0.6869641542434692, 0.06227857992053032, 0.611524224281311, 0.3231385350227356, -0.13907915353775024, 0.11010636389255524, 0.6545971632003784, -0.3461757004261017, -0.4319510757923126, 0.46990472078323364, -0.22955211997032166, 0.18333078920841217, 0.034164246171712875, -1.1302834749221802, 0.40800580382347107, 0.3107757568359375, 0.3752245008945465, 1.0691739320755005, -0.8037275075912476, -0.3486593961715698, 0.9898875951766968, -1.439213514328003, 0.15403860807418823, 1.1280843019485474, 0.8242075443267822, 0.3104301691055298, -0.4858854115009308, -0.5203619599342346, 0.058015063405036926, -0.07459045946598053, -0.6822687983512878, -0.3942165970802307, -0.037617579102516174, -0.5041536092758179, 1.756799340248108, -0.3299548625946045, -0.16175854206085205, 0.3382335305213928, 0.12835408747196198, 1.2478468418121338, 0.25424861907958984, -0.585465133190155, 0.12188951671123505, 0.40038102865219116, 0.34616485238075256, 1.003928303718567, -0.09580878913402557, 0.6718085408210754, -1.4768365621566772, -0.3302125632762909, 0.5944687724113464, -0.03857995197176933, 0.41817861795425415, -0.48418185114860535, -0.23646113276481628, 0.9541857242584229, 0.24053955078125, 0.5288310050964355, -0.520911693572998, 0.8734279870986938, 0.5513746738433838, 0.36645200848579407, 0.1712190955877304, 0.26969075202941895, -0.3600941300392151, 0.32162466645240784, -1.1421185731887817, 0.05805713310837746, 0.3419594168663025, 0.5484567880630493, 0.0437183678150177, 0.28977516293525696, -0.24756605923175812, 0.2258557826280594, 0.9019568562507629, -0.13496409356594086, 0.5961862206459045, -0.7992497682571411, 0.6929698586463928, -0.4272368848323822, 0.13506381213665009, -0.5954917669296265, -0.6510513424873352, -0.7401725649833679, -1.2267935276031494, -0.6402451992034912, -0.6804865598678589, -0.4577626883983612, -0.8021214604377747, 0.05849526822566986, 0.018597133457660675, 0.36998191475868225, 0.1757458597421646, 0.701251745223999, 0.3520159125328064, 0.5435754060745239, 0.35191604495048523, 0.38184624910354614, 0.8424795866012573, -1.3429251909255981, -0.0688273012638092, -0.9806253910064697, 0.5495983362197876, -0.08988112956285477, -0.0626591220498085, -0.1946243941783905, -1.4965225458145142, -1.388069748878479, -0.9392651319503784, 0.5901699066162109, -0.24936385452747345, -0.09264741092920303, 1.736261248588562, 0.17608840763568878, -0.9470445513725281, 1.1821813583374023, -0.6563076376914978, -0.22265473008155823, 0.3960392475128174, 0.24080052971839905, -0.047954291105270386, -0.23402614891529083, -1.0578495264053345, -0.0873529464006424, 0.3869795799255371, -0.7131308317184448, -0.5023568272590637, -0.7202816009521484, -0.5981571674346924, 0.22539465129375458, 0.2891269326210022, -0.6301915645599365, 1.107335090637207, -0.23866191506385803, -0.49863919615745544, 0.635200023651123, -0.0046087149530649185, -0.17010633647441864, 0.7285993099212646, 0.287213534116745, -0.7040517926216125, -0.09107957780361176, -0.31982704997062683, 0.5247077941894531, 0.6332371234893799, -0.4001750946044922, -0.244367316365242, -0.16028566658496857, -0.39931198954582214, -0.13817858695983887, -0.5502898693084717, 0.8126884698867798, -0.42498376965522766, -0.05756183713674545, 0.6245683431625366, 0.6696346998214722, -0.4900108277797699, 0.07866223156452179, -0.004059451632201672, -0.6853089928627014, 0.8366594314575195, 0.21435649693012238, 0.9009589552879333, -0.97307950258255, -0.9532185196876526, 0.13709282875061035, 0.2627642750740051, -0.04611865058541298, -0.5542036890983582, 1.030964970588684, -0.38174256682395935, 0.39341631531715393, 0.014559388160705566, -0.8563642501831055, -0.05038410425186157, 0.11650720238685608, -0.5221773982048035, -0.6589497923851013, 0.048707976937294006, 0.8607856631278992, -0.60770583152771, -0.11575709283351898, -0.4081697165966034, 0.08312348276376724, -1.14653742313385, 1.268359661102295, -0.6110032200813293, 0.0439496710896492, -0.1571091264486313, -0.09901628643274307, 0.37586867809295654, -0.7278333902359009, 0.756512463092804, -0.5220312476158142, 0.025498734787106514, 0.0394454263150692, -1.1600334644317627, 1.1124658584594727, -0.21957364678382874, -0.013819700106978416, 0.36393389105796814, -0.6671488285064697, 0.018380848690867424, 0.47231218218803406, 0.43736475706100464, -0.26147595047950745, -0.03534755855798721, 0.5763373374938965, -0.5536089539527893, 0.07813357561826706, 0.7949223518371582, 0.7725725173950195, 0.21396823227405548, 0.35475701093673706, 0.6898964047431946, -0.11956622451543808, 0.5525743365287781, 0.2677682042121887, 0.5620359182357788, 0.25184473395347595, -0.26916250586509705, -0.18770048022270203, -0.06971347332000732, -0.3284226059913635, 0.029768547043204308, 0.668861985206604, 0.5696534514427185, 0.36803099513053894, 0.4723590612411499, -0.953342080116272, -0.7067604064941406, 0.17980819940567017, 0.6101506948471069, 1.6466031074523926, -0.37453365325927734, -0.1944885402917862, -0.5295206904411316, -0.7229793071746826, 0.41140612959861755, -0.40244680643081665, -0.3125566244125366, 0.14747175574302673, -0.4735465347766876, -1.1876623630523682, 0.5157261490821838, 0.40570303797721863, 1.4064490795135498, -0.44662725925445557, -0.2915586531162262, -0.4297376871109009, 0.8381558060646057, -0.6254739165306091, -0.26533377170562744, 0.6439258456230164, -1.1936537027359009, -0.2947891056537628, 0.21803142130374908, -0.2888938784599304, 0.13945861160755157, -0.1743215173482895, 1.1157124042510986, -0.06858216226100922, -0.02587510086596012, 0.1507791429758072, 0.6559130549430847, -0.7065019607543945, -0.6384938955307007, 0.20521733164787292, 0.2945936620235443, -0.24729110300540924, 0.15978221595287323, 0.07639788836240768, -0.004457333590835333, 0.39543038606643677, -0.40099436044692993, 0.5232603549957275, 0.30174192786216736, -0.028310971334576607, 0.746703565120697, 0.17717429995536804, -0.03478049859404564, -1.4506809711456299, 0.7217385768890381, -0.3007884919643402, -0.4605439305305481, 0.14686079323291779, -0.9713104367256165, -0.16002553701400757, 0.501144289970398, -0.6442633271217346, 0.10860289633274078, -0.9535936713218689, 0.37384822964668274, -0.8643300533294678, -0.33391273021698, 0.2209729254245758, 0.7715790867805481, -0.49928683042526245, 0.7859717011451721, 0.2644975185394287, 0.6702858805656433, -0.4727966785430908, 0.31728115677833557, -0.9515222311019897, 0.2973838150501251, 0.05163797363638878, 0.3123442530632019, -0.06986744701862335, 0.13913112878799438, -0.5686256885528564, -0.6119794845581055, -0.6013858318328857, -0.6006820201873779, 0.03616658225655556, 0.08376578986644745, -0.5722241997718811, -0.8210034966468811, -0.1888481229543686, -0.44166040420532227, -0.6534645557403564, 0.07122911512851715, -0.12072918564081192, -0.04469810053706169, -0.9044283628463745, -1.4121659994125366, -0.3328583538532257, -0.8610582947731018, -0.7171032428741455, 0.2738288342952728, 0.03994033485651016, 0.2630009353160858, -0.7236236929893494, -0.26503580808639526, -0.17999203503131866, 1.3027846813201904, -0.5065394043922424, 0.5239593386650085, -0.09368425607681274, -0.13548782467842102, -0.21219895780086517, -0.30290043354034424, 0.7455133199691772, -0.6758679747581482, -0.06289079785346985, -0.6336192488670349, 0.06384187936782837, -0.2433142066001892, -0.7360105514526367, 0.30593085289001465, -0.07756786048412323, 0.8298305869102478, 0.04637297987937927, -0.4723595976829529, 0.5376728177070618, 1.5050570964813232, -1.1096479892730713, -0.09941078722476959, -0.3138284981250763, 1.2477104663848877, 0.12090130150318146, -0.4829970896244049, 0.5686350464820862, -0.15878663957118988, 0.2692655920982361, 0.2335491180419922, -0.4820886254310608, -0.3650922179222107, -0.5583193302154541, 0.062212176620960236, 1.5497182607650757, 0.3496834337711334, 0.48991212248802185, -1.216863751411438, 0.2516992390155792, -1.1578295230865479, -0.7230786681175232, 0.395562082529068, 0.8136709332466125, 0.16260208189487457, -0.056668877601623535, -0.08562694489955902, -0.35605335235595703, 0.05266813933849335, 0.22242797911167145, -0.5821924805641174, -0.46498745679855347, 0.48716679215431213, 0.5672186017036438, 0.5261141657829285, 0.15741612017154694, -0.3405941128730774, 0.2897283136844635, 14.951276779174805, 1.078075647354126, -0.06968846917152405, 0.7457137107849121, 0.5900911092758179, 0.4823359549045563, -0.08922187238931656, -0.1596652716398239, -1.2862663269042969, -0.06813834607601166, 1.641730785369873, 0.1617928147315979, 0.7451533675193787, 0.7710712552070618, 0.003991483710706234, 0.05809984356164932, -0.13732324540615082, 0.7507137060165405, 0.27813681960105896, -1.6790339946746826, 0.14086580276489258, 0.4748597741127014, 0.6106345057487488, 0.9557230472564697, 0.32995718717575073, 1.0858416557312012, 0.350691556930542, -0.2797567844390869, -0.01637118123471737, 0.13128432631492615, 0.9186072945594788, -0.10303008556365967, 0.5888293981552124, 1.0054746866226196, -0.7873565554618835, -0.07960410416126251, -0.5959998965263367, -1.2096549272537231, -0.037935566157102585, 0.29987064003944397, -0.8631551861763, -0.05279215797781944, -0.3130529224872589, 0.46833688020706177, -0.2582800090312958, 0.3365897536277771, 0.0312538705766201, 0.9610300660133362, -0.3774242401123047, 0.5269483923912048, 0.17487551271915436, 0.42814600467681885, -0.2529352605342865, -0.32407820224761963, -0.13490989804267883, -0.3055053651332855, 0.6961957216262817, 0.07307310402393341, -0.5824410915374756, -0.15782085061073303, -0.2412719875574112, -0.34954169392585754, -0.4656963646411896, 1.1657541990280151, 0.7963920831680298, -0.00639675697311759, -0.3895053565502167, 0.30556583404541016, 0.8535155057907104, -0.08784954994916916, -0.18439218401908875, 0.31471124291419983, 0.15547767281532288, -0.08630505949258804, -0.22853034734725952, 0.07331973314285278, -0.44875967502593994, -0.7229847311973572, -0.8620785474777222, -0.43873992562294006, 0.65143221616745, -0.7300528287887573, -1.1202504634857178, 0.6401039958000183, -0.2614896297454834, -0.602785050868988, 0.16805993020534515, -0.9204763770103455, -0.401358425617218, 0.7371747493743896, -1.0777502059936523, -0.624184250831604, -0.20921531319618225, -0.34595954418182373, -0.3701384961605072, -0.041220489889383316, 1.3784152269363403, 0.3098958432674408, -0.6230019330978394, -0.16642944514751434, 0.09542849659919739, 0.0763859823346138, -0.44101375341415405, -0.6133925318717957, 1.0636732578277588, 0.2387310117483139, 0.1796729862689972, -0.022395163774490356, -0.28074249625205994, 0.16861914098262787, -0.8287837505340576, -0.1528986394405365, 0.530741274356842, -0.8013370633125305, 0.06298099458217621, -0.6873513460159302, -1.1496132612228394, 0.14326636493206024, 0.3152599036693573, 0.060733381658792496, 0.4061213731765747, 0.3605061173439026, -0.6154202818870544, -0.2712870240211487, -0.8451733589172363, -0.13921336829662323, 0.5762474536895752, -0.5565678477287292, -0.1303916573524475, 0.4545079171657562, 0.4585288465023041, -0.8983818888664246, -1.1241838932037354, -0.1689929962158203, 0.03462706506252289, -0.33607763051986694, 0.7794570922851562, -0.6644335389137268, 0.693480908870697, 1.2597951889038086, -0.17768442630767822, -0.5102304220199585, 0.2304927259683609, -0.5397927165031433, -0.27039164304733276, -0.177247554063797, 0.4659234285354614, -0.42595288157463074, 0.9794667363166809, 1.0015358924865723, 0.21077071130275726, -0.49196797609329224, -0.3250139653682709, -0.381502240896225, -0.2775094509124756, -0.6848183274269104, 0.10882461816072464, 0.14853321015834808, -0.21251769363880157, 0.05039172247052193, 0.4549122452735901, 0.5364331603050232, 0.278708815574646, -0.5681276917457581, 0.8343462347984314, -0.19230543076992035, -0.6195055842399597, -0.1731417179107666, -0.2570897340774536, -1.056561827659607, -0.39956584572792053, -1.3941115140914917, -0.3409518301486969, -0.9196566939353943, -0.5398022532463074, -0.20303234457969666, -0.21538253128528595, 0.09254979342222214, 0.45218586921691895, -0.15404778718948364, -0.45699694752693176, 0.09016753733158112, -0.5484520196914673, 0.3367937505245209, 0.9992635250091553, -0.09503094851970673, -0.0847742110490799, -0.07971562445163727, 0.31974613666534424, 0.373762309551239, 0.731824517250061, -0.3811675012111664, -0.7226147055625916, -0.8269751667976379, 0.1934228390455246, 0.19834692776203156, 0.03842145949602127, -1.4025564193725586, 0.5889641642570496, 0.09080978482961655, 0.08900345116853714, 0.055124830454587936, 0.22606505453586578, -0.9557666778564453, -0.3776552081108093, 0.2360510677099228, -0.4082404673099518, 0.13218297064304352, 0.3069351017475128, -0.5407921671867371, -0.29558828473091125, 0.6413308382034302, 0.015892650932073593, -1.052357792854309, -1.1896804571151733, 0.5300355553627014, -0.03041650913655758, 0.09662006050348282, -0.22832639515399933, 0.08109450340270996, -1.0667169094085693, 0.1560429036617279, -0.321903258562088, 0.770896315574646, -0.29589059948921204, 0.5808777809143066, -0.010783353820443153, -1.0737764835357666, 0.3174094259738922, 0.66041100025177, -0.35243719816207886, -0.13718369603157043, 1.0305317640304565, 0.566659688949585, -0.42849570512771606, 0.24200525879859924, -0.10698661208152771, 0.2906979024410248, -0.621264636516571, 0.2594795525074005, 0.7258557677268982, -0.7041012644767761, -0.10317022353410721, 0.9334019422531128, -0.5877790451049805, -1.1352981328964233, -0.0179635901004076, -0.6328032612800598, -0.3753633499145508, -0.400321364402771, 0.6042700409889221, 0.30242612957954407, -0.0058800396509468555, 0.36998966336250305, -0.4443037807941437, -0.04805488884449005, 0.20047038793563843, -0.3937104344367981, 0.4998593330383301, 0.06171996146440506, -0.2470858246088028, 0.8674414157867432, 0.7952068448066711, -0.752001941204071, -1.1351957321166992, -0.519145667552948, -0.21445541083812714, -0.16438128054141998, 0.2841354012489319, -0.6192562580108643, -0.688409686088562, 0.894751250743866, 0.6422591805458069, 0.5157278776168823, 0.028704779222607613, -0.29109108448028564, 0.44320374727249146, 0.5211099982261658, 0.5107558369636536, -0.9063290357589722, -0.23760126531124115, 0.9542076587677002, 1.1943583488464355, -1.2402868270874023, 0.42143887281417847, -0.22174005210399628, -1.0165066719055176, 1.007292628288269, 0.718531608581543, -0.1639777272939682, 1.041290044784546, -0.45790401101112366, 0.021596897393465042, -0.18623188138008118, -1.1659396886825562, -0.32878145575523376, 0.8939858675003052, 0.6580769419670105, 0.41920462250709534, 0.22723685204982758, 0.32980090379714966, 0.5995558500289917, 0.17133690416812897, 0.46558481454849243, 0.21719464659690857, 0.4053638279438019, -0.5904710292816162, 0.10800733417272568, -0.03879358246922493, 0.9067270755767822, -0.4778745174407959, -0.3676479160785675, 0.5412871241569519, 0.8620256781578064, 0.4617036283016205, 0.7586197853088379, 0.7331521511077881, -0.28555625677108765, 0.6993451714515686, 0.41819286346435547, 0.1406227946281433, -0.6340066194534302, -0.550186812877655, 0.021285051479935646, -0.3492123484611511, -0.3227704167366028, 0.12011706829071045, 0.05797433853149414, -0.44875431060791016, -0.65665602684021, 0.7148982882499695, 0.2354602813720703, 0.6167388558387756, 0.6383667588233948, 0.9631959199905396, 0.4300253391265869, 0.15868164598941803, -0.9178294539451599, -0.970160722732544, -0.9376527667045593, -0.4180886447429657, -0.36657166481018066, -0.7856149673461914, -0.046219438314437866, -0.4718020260334015, -0.44136348366737366]}, "authors": [{"authorId": "2154975538", "name": "Qinghua Liu"}, {"authorId": "2144645929", "name": "Yuxiang Jiang"}], "references": [{"paperId": "114cc72d93c73b97ba03ed8c4e4a8d937b344607", "title": "An Efficient 2D Method for Training Super-Large Deep Learning Models"}, {"paperId": "2793b9395e81cc60cd2722ef1134ff98bb207e84", "title": "The AI Index 2022 Annual Report"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "408efcc963ab871558e5e145b54ad7ae2aeb11c7", "title": "BaGuaLu: targeting brain scale pretrained models with over 37 million cores"}, {"paperId": "82ba96443173da0b8b3e870c5ab8f41109a67203", "title": "StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5", "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "135cd111d89645ecf4c62022df68f3a1c4ca652d", "title": "LiMuSE: Lightweight Multi-Modal Speaker Extraction"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "416dab850fda842b13a4f28164514d98f836fff7", "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "87d5b61f5b6fdb0a57fc66b5c5bb428c398eaa86", "title": "Deep learning for AI"}, {"paperId": "4fffa5245d3972077c83614c2a08a47cb578631e", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"}, {"paperId": "d8df456f790381f4ddb388be24a546625bd75ee2", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "f46c562229c5bc419bbbfb63239431590e4b340a", "title": "Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328", "title": "Sparse GPU Kernels for Deep Learning"}, {"paperId": "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e", "title": "Memory-Efficient Pipeline-Parallel DNN Training"}, {"paperId": "706f756b71f0bf51fc78d98f52c358b1a3aeef8e", "title": "Self-Supervised Learning: Generative or Contrastive"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271", "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks"}, {"paperId": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d", "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "db2d3dc613169b519f1a2dd35e0473dc2e848025", "title": "Fast Sparse ConvNets"}, {"paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc", "title": "Momentum Contrast for Unsupervised Visual Representation Learning"}, {"paperId": "fd431005d26100f5453590080683cbae9dc1189f", "title": "Checkmate: Breaking the Memory Wall with Optimal Tensor Rematerialization"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "ceb2ebef0b41e31c1a21b28c2734123900c005e2", "title": "A Style-Based Generator Architecture for Generative Adversarial Networks"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "7b7482c433da5e3b52224524206f36bda4c14edf", "title": "Diversity and Depth in Per-Example Routing Models"}, {"paperId": "a82fc0115c1802d48d352b35595204738fad84f0", "title": "Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes"}, {"paperId": "a8e1b91b0940a539aca302fb4e5c1f098e4e3860", "title": "LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks"}, {"paperId": "b227f3e4c0dc96e5ac5426b85485a70f2175a205", "title": "Representation Learning with Contrastive Predictive Coding"}, {"paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"}, {"paperId": "ffc8e2f565e29b6718d1a823e2824eb62fe2a3ed", "title": "Knowledge Distillation with Adversarial Samples Supporting Decision Boundary"}, {"paperId": "acdf151b8efc2c6b05662d69f27531afc557dc85", "title": "Training and Inference with Integers in Deep Neural Networks"}, {"paperId": "f466157848d1a7772fb6d02cdac9a7a5e7ef982e", "title": "Neural Discrete Representation Learning"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "d21ebaab3f715dc7178966ff146711882e6a6fee", "title": "Globally and locally consistent image completion"}, {"paperId": "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression"}, {"paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "df0402517a7338ae28bc54acaac400de6b456a46", "title": "WaveNet: A Generative Model for Raw Audio"}, {"paperId": "8b053389eb8c18c61b84d7e59a95cb7e13f205b7", "title": "DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7", "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"}, {"paperId": "25fb5a6abcd88ee52bdb3165b844c941e90eb9bf", "title": "Revisiting Distributed Synchronous SGD"}, {"paperId": "41f1d50c85d3180476c4c7b3eea121278b0d8474", "title": "Pixel Recurrent Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "8388f1be26329fa45e5807e968a641ce170ea078", "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "6364fdaa0a0eccd823a779fcdd489173f938e91a", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "title": "Communication Efficient Distributed Machine Learning with the Parameter Server"}, {"paperId": "6bdb186ec4726e00a8051119636d4df3b94043b5", "title": "Caffe: Convolutional Architecture for Fast Feature Embedding"}, {"paperId": "acbd13c7be621a7284da4ab9d8caa40f1a558ce2", "title": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server"}, {"paperId": "8f3d22c0caf53655c6542aed0a6101629db7d5f3", "title": "Scalable inference in latent variable models"}, {"paperId": "6f4e48c2a5de9337d147ebbb7d0ff0e555adceca", "title": "Bandwidth optimal all-reduce algorithms for clusters of workstations"}, {"paperId": "355d44f53428b1ac4fb2ab468d593c720640e5bd", "title": "Greedy Layer-Wise Training of Deep Networks"}, {"paperId": "766cd91c0d8650495529cab7d4eeed482729cf89", "title": "Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation"}, {"paperId": "f6d8a7fc2e2d53923832f9404376512068ca2a57", "title": "Hierarchical Mixtures of Experts and the EM Algorithm"}, {"paperId": "8665c9b459e4161825baf1f25b5141f41a5085ff", "title": "A bridging model for parallel computation"}, {"paperId": "d5f8fc9b0db83d0c11c9534525d0c56e609f1068", "title": "2.5-dimensional Distributed Model Training"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "bfc4b683d3221b631c8aed9861cb87cdb3ad78e4", "title": "High-performance, Distributed Training of Large-scale Deep Learning Recommendation Models"}, {"paperId": null, "title": "\u201cSelene supercomputer,\u201d"}, {"paperId": "08588107b9e37f9601bb5c801aa46b918cc3c8ec", "title": "A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters"}, {"paperId": null, "title": "\u201cScalable second order optimization for deep learning,\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "c68796f833a7151f0a63d1d1608dc902b4fdc9b6", "title": "GENERATIVE ADVERSARIAL NETS"}, {"paperId": null, "title": "Apex (a pytorch extension)"}, {"paperId": null, "title": "\u201cNvidia tesla v100 gpu architecture,\u201d"}, {"paperId": null, "title": "\u201cScalable universal matrix multiplication algorithms: 2d and 3d variations on a theme,\u201d"}, {"paperId": "387818263c504a2813c75a1a5ed697f94977b1eb", "title": "A High-Performance Message-Passing Library for the AP3000"}, {"paperId": "4954fa180728932959997a4768411ff9136aac81", "title": "This Paper Is Included in the Proceedings of the 12th Usenix Symposium on Operating Systems Design and Implementation (osdi '16). Tensorflow: a System for Large-scale Machine Learning Tensorflow: a System for Large-scale Machine Learning"}, {"paperId": null, "title": "\u201c2d tensor parallelism,\u201d"}, {"paperId": null, "title": "How to train really large models on many gpus?\" lilianweng.github.io, 2021"}]}