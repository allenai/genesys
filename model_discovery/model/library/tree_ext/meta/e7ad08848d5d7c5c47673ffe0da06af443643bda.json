{"paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda", "title": "Large Language Models are Zero-Shot Reasoners", "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 2411, "influentialCitationCount": 233, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Experimental results demonstrate that the Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics, symbolic reasoning, and other logical reasoning tasks, without any hand-crafted few-shot examples."}, "embedding": {"model": "specter_v2", "vector": [0.03685176745057106, 1.0636053085327148, -0.18352879583835602, -0.1362924724817276, 0.023189537227153778, -0.3951355814933777, 0.8285366892814636, 0.10546912997961044, -0.42760276794433594, -0.19798627495765686, 0.40454337000846863, -0.6187358498573303, 0.07861141860485077, -0.020459448918700218, -0.0690537616610527, 0.18386416137218475, -0.4507993459701538, 0.6368489265441895, 0.06304094195365906, -0.613162100315094, 0.1807205229997635, -0.659746527671814, -0.5373354554176331, 0.1384962648153305, 0.32664549350738525, -0.12006796151399612, -0.15671192109584808, 1.0454593896865845, -0.623460054397583, 1.192287564277649, 0.532009482383728, -0.5720849633216858, 0.03016805835068226, 0.017585912719368935, -0.41086557507514954, -0.5564361214637756, 0.1293879896402359, -0.8717721700668335, -0.5496156811714172, 0.6364780068397522, -0.2188040316104889, 0.17758260667324066, 0.5990333557128906, -0.7798827290534973, -0.5270403027534485, 1.018686294555664, 0.8010865449905396, 0.49713826179504395, 0.2275153547525406, -0.020339379087090492, 1.6397260427474976, -1.4632954597473145, 0.4383397400379181, 1.5884201526641846, 0.5390862822532654, 0.6183279752731323, 0.01486615464091301, -0.3322177231311798, 0.6452093124389648, 0.20110267400741577, -0.7435690760612488, -0.33733922243118286, -0.3812546730041504, 0.15938028693199158, 1.9655613899230957, -0.6537615656852722, -0.2559534013271332, 0.4828951060771942, 0.08100718259811401, 1.3473902940750122, -0.015729019418358803, -1.1263686418533325, 0.14663442969322205, -0.009320016019046307, 0.8763877749443054, 0.63052898645401, -0.2848571836948395, 0.5038323998451233, -0.8143332004547119, -0.010264712385833263, 0.8316584229469299, 0.018307525664567947, -0.2949497103691101, -0.0059427497908473015, -0.6389021277427673, 0.4204238951206207, 0.22123679518699646, 0.8715391159057617, -0.14646993577480316, 0.3989351987838745, 0.30037426948547363, 0.8588727116584778, -0.6191312670707703, 0.4860289394855499, -0.41947516798973083, 0.4798841178417206, -0.46510636806488037, 0.5554787516593933, 0.6220220327377319, 1.150361180305481, -0.321635901927948, 0.036929652094841, -1.0790400505065918, -0.1902603954076767, 1.280465006828308, -0.178505539894104, 0.31153440475463867, -1.0015689134597778, 0.1089446097612381, -0.5006380677223206, 0.8972702622413635, -0.6904246807098389, -0.43742525577545166, -0.014665907248854637, -0.33483436703681946, -1.1198846101760864, 0.201300248503685, 0.007876704446971416, -0.5938978791236877, 0.7261645197868347, -0.23582139611244202, -0.03368605673313141, 0.4248298704624176, 0.791729211807251, 1.0616697072982788, 0.5798190832138062, 0.524893581867218, -0.07937400788068771, 0.7864655256271362, -0.6306236386299133, -0.7342743873596191, -0.9841364026069641, 1.242875099182129, 0.13644395768642426, 0.4841103255748749, -0.22892892360687256, -1.0765197277069092, -0.8532836437225342, -0.8479920625686646, -0.33420687913894653, -0.435689777135849, 0.3440732955932617, 1.3003653287887573, 0.20742659270763397, -0.8291299343109131, 0.5374686121940613, 0.02533748745918274, -0.23580731451511383, -0.19059406220912933, 0.02524912729859352, 0.41499945521354675, -1.0898444652557373, -1.516920566558838, 0.5145117044448853, 0.5293630361557007, -0.4001529812812805, -0.19671869277954102, -0.6871830224990845, -1.3243381977081299, 0.03287187218666077, 0.9255838990211487, -0.5633273720741272, 1.7305629253387451, 0.32952800393104553, -1.131422758102417, 0.6448133587837219, -0.29858776926994324, 0.06694156676530838, 0.4326910674571991, -0.20292170345783234, -0.6362281441688538, -0.13898970186710358, 0.1719401329755783, 0.9078067541122437, 0.07153952121734619, -0.3406001627445221, 0.005793142598122358, 0.5467532873153687, 0.21082498133182526, -0.3652958869934082, 0.1662549376487732, 0.8809220194816589, -0.049703530967235565, 0.04199362173676491, 0.48216795921325684, 0.9142371416091919, -0.005499391350895166, 0.10294491052627563, -0.2757735252380371, -1.4485552310943604, 0.29174381494522095, 0.20460328459739685, 0.7175483703613281, -0.8860799670219421, -1.185019612312317, -0.6842740774154663, -0.0063658542931079865, -0.1718427836894989, -0.6908878684043884, 0.6527891755104065, -0.3689926564693451, 0.4186425805091858, -0.3817182183265686, -0.5346010327339172, 0.322103887796402, -0.2071218639612198, -0.8739137053489685, -0.20846064388751984, 0.08641917258501053, 1.3175338506698608, -1.229539394378662, -0.2620247006416321, 0.2805720567703247, -0.17027050256729126, -0.8182200193405151, 1.2281023263931274, -0.8335593938827515, -0.08171411603689194, -0.19611091911792755, -0.2806774377822876, -0.004865666851401329, -0.44816622138023376, 0.1881905198097229, -0.460633248090744, -0.18905310332775116, 0.20363172888755798, -0.649355411529541, 1.4781228303909302, 0.0892346054315567, 0.28498339653015137, -0.3933524787425995, -0.3468252122402191, -0.07248576730489731, 0.7803050875663757, -0.856579065322876, -0.3700941503047943, 0.07270607352256775, 0.3237244188785553, -0.21335026621818542, -0.1856318861246109, 0.33813637495040894, 0.37212109565734863, -0.36098194122314453, 0.6156864762306213, 0.3980594873428345, -0.35278624296188354, 0.5875951051712036, 1.0134464502334595, 0.8416927456855774, 0.1796676069498062, 0.9662659764289856, -0.018819645047187805, 0.3788112699985504, -0.31803810596466064, -0.29286056756973267, 0.7859727740287781, 0.7269729375839233, 0.8688104152679443, 0.37374547123908997, -0.9710486531257629, -0.2964165508747101, 0.29057085514068604, 0.8149846196174622, 1.513753890991211, 0.28474268317222595, -0.2327522188425064, -0.4801819920539856, -0.14864037930965424, -0.5462539196014404, 1.02232027053833, -0.20509561896324158, -0.06282896548509598, -0.5899310111999512, -0.4632755219936371, 0.7112268209457397, 0.5776243209838867, 1.1432439088821411, -0.8476909399032593, -0.5063470005989075, -0.08098502457141876, 0.3782486021518707, -0.9387720823287964, -0.1274390071630478, 0.10466966778039932, -0.40438276529312134, -0.41645997762680054, 0.05724473297595978, -0.09580638259649277, 0.27419430017471313, -0.3047565817832947, 1.050663948059082, -0.2879894971847534, -0.35586991906166077, 0.5549147129058838, 0.6923994421958923, -0.24309851229190826, -0.6212230920791626, -0.34222739934921265, -0.03390821814537048, -0.6405970454216003, 0.6981112360954285, 0.6482552886009216, 0.23941004276275635, 0.04661184921860695, -0.2688620686531067, 0.2137645035982132, 0.13037587702274323, -0.009913250803947449, 0.293357253074646, -0.27980729937553406, 0.21072818338871002, -1.211963415145874, 0.9906766414642334, 0.10754666477441788, -0.2919296622276306, 0.6222420334815979, -0.8110155463218689, -0.24003024399280548, 0.7411054968833923, -0.6510030031204224, -0.5458300113677979, -1.009931206703186, 0.723198413848877, -0.013716243207454681, -0.9528542757034302, 0.19671988487243652, 0.10313732177019119, 0.3004901707172394, 0.5301283597946167, 0.5114394426345825, 0.06250479072332382, 0.12368783354759216, 1.1194636821746826, -0.8935636281967163, 0.6427947878837585, 0.011249188333749771, 0.10417430847883224, -0.43626466393470764, -0.15759511291980743, -0.41684988141059875, -0.8072613477706909, -0.2625056207180023, -0.211744025349617, -0.1285300999879837, 0.1305946707725525, -0.6282961964607239, -0.9928432106971741, 0.020390911027789116, -1.3656026124954224, -0.5083976984024048, 0.09369907528162003, -0.5378774404525757, -0.19334380328655243, -0.973143458366394, -1.2328994274139404, -0.5964528322219849, -0.14436854422092438, -0.13361291587352753, 0.4760836064815521, 0.04881562665104866, -0.7895931601524353, -0.7239348292350769, 0.06615829467773438, -0.4097675383090973, 0.5866363048553467, -0.7814615368843079, 0.9979326725006104, -0.43633270263671875, -0.3780037462711334, -0.13368958234786987, 0.1277761310338974, -0.194542795419693, -0.14761339128017426, -0.06597829610109329, -0.5797469019889832, 0.3575965464115143, 0.13288991153240204, -0.7932512760162354, -0.05582742765545845, -0.04343389719724655, 0.028776710852980614, -0.1002042219042778, -0.48204028606414795, -0.30672192573547363, 1.3682061433792114, -0.41827818751335144, 0.1383785605430603, -0.03252200782299042, 0.9911149740219116, 0.4565238654613495, 0.2955869436264038, 0.34457021951675415, 0.52008056640625, -0.03930520638823509, -0.03491457179188728, 0.3026443123817444, 0.21639618277549744, -0.3795279860496521, 0.6628369092941284, 0.6798712611198425, 0.2877325117588043, 0.10247984528541565, -1.4803520441055298, 0.5219215750694275, -1.3342374563217163, -0.5355684161186218, 0.28442052006721497, 0.5197392702102661, 0.5431144833564758, -0.38756826519966125, -0.8299140930175781, -0.22772201895713806, 0.5347029566764832, -0.048494599759578705, -0.39826083183288574, -0.30064353346824646, -0.00679422914981842, 0.15960364043712616, -0.2908343970775604, 0.5699070692062378, -0.19383513927459717, 0.8973779678344727, 14.610806465148926, 0.7446666955947876, 0.19656217098236084, 0.26676124334335327, 0.5576830506324768, 0.293035089969635, -0.380730003118515, 0.10094873607158661, -1.1845388412475586, -0.9004987478256226, 1.2079946994781494, 0.03672606870532036, 0.34619390964508057, 0.22519606351852417, -0.14953787624835968, -0.023238390684127808, -0.9915703535079956, 0.5643054842948914, 0.4764682352542877, -1.291164755821228, 0.6183235049247742, -0.21135245263576508, 0.17859816551208496, -0.12783856689929962, 0.4930369257926941, 1.0679033994674683, 0.5972195267677307, -0.6224567294120789, 0.6056584715843201, 0.194901242852211, 0.8192631602287292, -0.007089565508067608, 0.35128992795944214, 1.1542993783950806, -0.6005226373672485, -0.4551653563976288, -0.5716419816017151, -1.449245572090149, 0.14536698162555695, -0.13676570355892181, -0.5961320400238037, -0.5376954078674316, -0.9081912040710449, 0.6488813757896423, 0.21504853665828705, 0.1762620210647583, -0.4597492516040802, 0.37765344977378845, 0.18992933630943298, -0.07206253707408905, 0.24553704261779785, 0.6267509460449219, 0.21828888356685638, -0.34854599833488464, 0.13634617626667023, 0.8498032689094543, -0.03785184398293495, 1.0915335416793823, -0.08771604299545288, -0.10779772698879242, -0.5004346370697021, -0.2749911844730377, 0.03619939088821411, 0.5671393275260925, 0.19839511811733246, 0.008615772239863873, -0.38539278507232666, 0.025083689019083977, 0.5712781548500061, 0.21527336537837982, 0.08854704350233078, -0.14152364432811737, -0.43730276823043823, -0.6917582750320435, -0.08165135234594345, 0.4949944317340851, -0.39962783455848694, -0.6346919536590576, -0.8429714441299438, -0.5167752504348755, 0.23590950667858124, -1.0700260400772095, -0.7977257966995239, 0.40107882022857666, -0.40363210439682007, -0.552604079246521, -0.07747814804315567, -0.7714800238609314, -0.6067306995391846, -0.14108861982822418, -1.0922083854675293, -0.8124521374702454, -0.0381486751139164, -0.2673243284225464, -0.11271669715642929, -0.027184784412384033, 1.3316386938095093, -0.31796446442604065, -0.19527190923690796, -0.21949191391468048, -0.31206759810447693, 0.0006830778438597918, -0.04369667172431946, -0.9805601835250854, 0.8271892666816711, -0.017362909391522408, -0.13182903826236725, 0.960300624370575, 0.31762415170669556, -0.012641075998544693, -0.774692952632904, 0.17806299030780792, 0.9405498504638672, -1.1559115648269653, -0.4874759018421173, -0.6339741945266724, -1.1028707027435303, 0.36671265959739685, 0.40181195735931396, -0.12164276838302612, 0.12391525506973267, -0.10756450891494751, -0.6600712537765503, 0.1392231285572052, -0.8486126661300659, 0.5052399039268494, 0.3667553961277008, -0.6278003454208374, -1.0532325506210327, 0.16690491139888763, 0.6691290736198425, -0.9076454639434814, -0.30800020694732666, -0.31816768646240234, 0.0551605261862278, -0.21124456822872162, 0.7985060214996338, -0.6065951585769653, 0.816967248916626, 0.6802324652671814, 0.03361981362104416, -0.37417834997177124, 0.2208240032196045, -0.883050799369812, -0.057895246893167496, 0.2926698923110962, 0.8402025103569031, -0.6115327477455139, -0.1942843645811081, 1.7716374397277832, 0.361615926027298, -0.19568288326263428, -0.3872498869895935, -0.3611007630825043, 0.37152785062789917, -0.6007016897201538, 0.42286646366119385, 0.051493093371391296, 0.26035863161087036, 0.4869859516620636, 0.677291214466095, 0.6456787586212158, -0.10046898573637009, -0.4075888693332672, 0.47851571440696716, -0.11351718753576279, -0.17548978328704834, -0.1951722502708435, 0.12015722692012787, -1.7081823348999023, 0.016837147995829582, -0.9374087452888489, 0.3963008522987366, -1.2939454317092896, -0.5267192721366882, 0.3605990707874298, -0.13995209336280823, 0.16393421590328217, 0.39604151248931885, -0.9444568157196045, -0.588671863079071, -0.3719199597835541, -0.764812707901001, 0.49576491117477417, 0.8542182445526123, -0.6601144671440125, 0.1874723583459854, -0.09143461287021637, -0.297120600938797, 0.26386138796806335, 0.21759815514087677, -0.4725428819656372, -0.8953667879104614, -0.9858853816986084, 0.6565775275230408, 0.6135933995246887, 0.3450778126716614, -0.7501260638237, 1.2668426036834717, 0.3486076593399048, -0.31752246618270874, -0.013329895213246346, 0.13532821834087372, -0.8465363383293152, -0.732688307762146, 0.31584373116493225, -1.1865884065628052, -0.293154239654541, 0.2608218193054199, -0.4411497414112091, -0.06919248402118683, 0.23556207120418549, -0.24382580816745758, -1.5434327125549316, -0.7849416136741638, -0.11956710368394852, -0.6361377835273743, 0.06468583643436432, -0.14757978916168213, -0.019767947494983673, -1.137642741203308, -0.27389365434646606, 0.05275020748376846, 0.6915332078933716, -0.2660471200942993, 0.4508694112300873, 0.5875104665756226, -0.9305232167243958, 0.06854116171598434, -0.029063362628221512, 0.02886442095041275, 0.3427107036113739, 0.494554728269577, 0.18778538703918457, -0.2579619884490967, 1.0095502138137817, 0.34128284454345703, 0.4977837800979614, -0.5742384791374207, 0.22920921444892883, 0.7840354442596436, -0.3502785563468933, -0.6603217124938965, 0.8729960322380066, -0.14021191000938416, -1.1524213552474976, 0.3375498950481415, -1.191990613937378, -0.6317347884178162, -0.5879446864128113, 0.8208150863647461, 0.14561043679714203, -0.4092952013015747, 0.35477471351623535, -0.3816135823726654, 0.4557265341281891, -0.362153559923172, -0.6320648193359375, 0.4951201379299164, -0.08474709838628769, -0.48482444882392883, 0.8640872836112976, 0.35496267676353455, -0.9068092107772827, -0.6563646793365479, -0.45278239250183105, 0.05572086200118065, 0.09207746386528015, 0.020966501906514168, -0.8487151861190796, 0.3328317701816559, 0.8278647661209106, -0.049074601382017136, 0.26780715584754944, -0.19067999720573425, -0.24750752747058868, -0.08833982795476913, 1.3577662706375122, 0.3817159831523895, -0.3128451108932495, -0.29906001687049866, 1.3034335374832153, 1.3890011310577393, -1.2275949716567993, 0.06126480549573898, -0.3538391590118408, -0.7470982074737549, 1.2125325202941895, 0.81931471824646, -0.0758141577243805, 0.4828934073448181, -0.3314075767993927, 0.22133837640285492, -0.07955799996852875, -1.2363438606262207, 0.1692793220281601, 0.4909486174583435, 0.8424732089042664, 0.7152266502380371, -0.017909878864884377, 0.2324468344449997, 1.2810609340667725, 0.1972503811120987, 0.5575875043869019, 0.840461254119873, 0.6070374250411987, -0.5619094371795654, 0.38046351075172424, 0.11485140770673752, 0.4171503484249115, -0.19052720069885254, -0.9111319780349731, -0.18057748675346375, 0.8143939971923828, -0.17158858478069305, 0.6531132459640503, 0.38750138878822327, 0.6008100509643555, 0.37215089797973633, 0.5120298266410828, 0.530210018157959, -0.8637074828147888, -0.047104377299547195, -0.3879956007003784, -0.6068204641342163, 0.00936764758080244, -0.014343500137329102, -0.33521780371665955, -1.0542914867401123, -0.15334223210811615, 0.25401586294174194, -0.1886715292930603, -0.38049623370170593, 1.7879072427749634, 0.28478139638900757, 0.1732475608587265, -0.750735342502594, -0.11269029229879379, -0.7449847459793091, -0.8318760395050049, 0.0463678203523159, -0.9815369248390198, -0.12190171331167221, -0.28173285722732544, -0.4136040210723877, -0.410062313079834]}, "authors": [{"authorId": "2081836120", "name": "Takeshi Kojima"}, {"authorId": "2046135", "name": "S. Gu"}, {"authorId": "1557386977", "name": "Machel Reid"}, {"authorId": "2153732825", "name": "Yutaka Matsuo"}, {"authorId": "1715282", "name": "Yusuke Iwasawa"}], "references": [{"paperId": "4247f45a5730e3bda5836e2bc7941e30f5b91cb7", "title": "Board"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "cb5e3f085caefd1f3d5e08637ab55d39e61234fc", "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"}, {"paperId": "23dd78e424d32f6a48660dcd67ce994b8a7db8be", "title": "STaR: Bootstrapping Reasoning With Reasoning"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "f4df78183261538e718066331898ee5cad7cad05", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "92173d081b15824d22a9ef070e118744ceee8052", "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "9ba50f992ccd92f428503ea6246157260a26cd77", "title": "Do Prompt-Based Models Really Understand the Meaning of Their Prompts?"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "0adec918885dff698acf359988ed79a543157f80", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "13c4e5a6122f3fa2663f63e49537091da6532f35", "title": "Are NLP Models really able to Solve Simple Math Word Problems?"}, {"paperId": "ac3cdb50606f7770eef8e4cd951840a4f71287a0", "title": "Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"}, {"paperId": "59641c10ed7431a3cf841f308367dc2dc0281b74", "title": "What Makes Good In-Context Examples for GPT-3?"}, {"paperId": "346081161bdc8f18e2a4c4af7f51d35452b5cb01", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "f30444fbb6ad806168e2564db4815cd27faa7fd9", "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "9808d59113029d96f48a0376b1578dbab5427bb4", "title": "Unsupervised Commonsense Question Answering with Self-Talk"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "874e9318c09c711ecd48a903b3824a3a03e2cd62", "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "b123a0d46ad917b79c43c5ae981e03ed2458ed11", "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "4c6fe6179c408e1fbb3871af13d1a8e64f766e54", "title": "Solving General Arithmetic Word Problems"}, {"paperId": "2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9", "title": "MAWPS: A Math Word Problem Repository"}, {"paperId": "17230f5b3956188055a48c5f4f61d131cce0662f", "title": "Parsing Algebraic Word Problems into Equations"}, {"paperId": "a7862e14b4c20cefd6dc4f611f8aa866fabf130b", "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization"}, {"paperId": "f48eed915cbb9c6592cdb9df80c1edaeb46959af", "title": "A measure of intelligence"}, {"paperId": "4c960fa48fbbc303c363a9938e8976ea7efd1c45", "title": "The structure of human intelligence: It is verbal, perceptual, and image rotation (VPR), not fluid and crystallized"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"}, {"paperId": null, "title": "Auto-Prompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "696f089615068aeab9399ad78938e694bc1c80a1", "title": "The Cattell-Horn-Carroll Theory of Cognitive Abilities: Past, Present, and Future."}, {"paperId": "bd87757bec32839742d676442dde389fce68764c", "title": "Heuristics and Biases: Individual Differences in Reasoning: Implications for the Rationality Debate?"}, {"paperId": "eecc21f0e74619c55af33166874e14303e949545", "title": "Individual differences in reasoning: Implications for the rationality debate?-Open Peer Commentary-Reversing figure and ground in the rationality debate: An evolutionary perspective"}, {"paperId": null, "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, {"paperId": null, "title": "(a) Did you state the full set of assumptions of all theoretical results"}, {"paperId": null, "title": "with respect to the random seed after running experiments multiple times)? [No] Our paper mainly used GPT-3 API with greedy decoding"}, {"paperId": null, "title": "Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation"}, {"paperId": null, "title": "Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?"}, {"paperId": null, "title": "Have you read the ethics review guidelines and ensured that your paper conforms to them"}, {"paperId": null, "title": "code, data, models) or curating/releasing new assets... (a) If your work uses existing assets"}, {"paperId": null, "title": "Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}, {"paperId": null, "title": "If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots"}, {"paperId": null, "title": "Did you describe the limitations of your work"}]}