{"paperId": "5278b81db686b4d36143941bff1c683bea963a63", "title": "SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient", "abstract": "Many deep learning applications benefit from using large models with billions of parameters. Training these models is notoriously expensive due to the need for specialized HPC clusters. In this work, we consider alternative setups for training large models: using cheap\"preemptible\"instances or pooling existing resources from multiple regions. We analyze the performance of existing model-parallel algorithms in these conditions and find configurations where training larger models becomes less communication-intensive. Based on these findings, we propose SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices. SWARM creates temporary randomized pipelines between nodes that are rebalanced in case of failure. We empirically validate our findings and compare SWARM parallelism with existing large-scale training approaches. Finally, we combine our insights with compression strategies to train a large Transformer language model with 1B shared parameters (approximately 13B before sharing) on preemptible T4 GPUs with less than 200Mb/s network.", "venue": "International Conference on Machine Learning", "year": 2023, "citationCount": 21, "influentialCitationCount": 5, "openAccessPdf": {"url": "http://arxiv.org/pdf/2301.11913", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work considers alternative setups for training large models: using cheap\"preemptible\" instances or pooling existing resources from multiple regions, and proposes SWARM parallelism, a model-parallel training algorithm designed for poorly connected, heterogeneous and unreliable devices."}, "embedding": {"model": "specter_v2", "vector": [0.14017441868782043, 0.4911668300628662, -0.572469174861908, 0.3633566200733185, -0.2819567620754242, 0.21404272317886353, 0.6968332529067993, -0.05835040286183357, -0.6583230495452881, -0.6131526827812195, 0.34082889556884766, -0.1410139501094818, 0.34409576654434204, 0.13076664507389069, -0.3188876211643219, -0.15258722007274628, -1.0943453311920166, 0.3764026463031769, 0.4101002812385559, -0.21984033286571503, -0.2974228262901306, -0.1591440737247467, -1.2564462423324585, 0.201088547706604, 0.4146265387535095, 0.9557873606681824, 0.11025475710630417, 0.7301077246665955, -0.22224077582359314, 0.4176006317138672, 0.16066129505634308, 0.4101833999156952, 0.4953027069568634, 0.16138705611228943, -0.19439052045345306, -0.11048796027898788, 0.14664605259895325, -0.0939701497554779, -0.38781049847602844, 0.48614946007728577, -0.10589197278022766, 0.16063447296619415, -0.26410365104675293, -1.0445939302444458, 0.6113812327384949, 0.15529802441596985, 0.3066200911998749, 0.7508764863014221, -0.9160453677177429, -0.3838060796260834, 0.6101675629615784, -0.9413190484046936, -0.17389515042304993, 0.9535505175590515, 0.801430881023407, -0.10591858625411987, -0.2266101837158203, -0.6552478671073914, 0.2064734399318695, -0.256049782037735, -0.5899067521095276, -0.5918356776237488, 0.10282719880342484, 0.050652552396059036, 2.120999813079834, -0.11394891887903214, 0.4423438608646393, 0.24255146086215973, 0.28747257590293884, 1.2176294326782227, 0.05524606257677078, -0.7961905598640442, 0.05352112278342247, 0.1869167536497116, 0.3437708616256714, 0.701862633228302, -0.18054141104221344, 0.29852649569511414, -1.2473865747451782, -0.519007682800293, 0.29089319705963135, 0.14414376020431519, 0.47004434466362, -0.2950853109359741, -0.1718243807554245, 0.7517089247703552, 0.4732661843299866, 0.48858383297920227, -0.36726638674736023, 0.9268941879272461, 0.8221505880355835, 0.3433094918727875, 0.35504281520843506, 0.4706256687641144, -0.0363808311522007, 0.17869707942008972, -1.1552629470825195, 0.09995116293430328, 0.21597766876220703, 0.8159953355789185, -0.24587032198905945, 0.18723513185977936, -0.277179479598999, 0.19799289107322693, 1.241507649421692, 0.07103713601827621, 0.6196538805961609, -0.5826632380485535, 0.21456532180309296, -0.7898703813552856, -0.08499087393283844, -0.06450837850570679, -0.3485443890094757, -0.5906864404678345, -0.8991341590881348, -1.0239801406860352, -0.8921188116073608, -0.13435646891593933, -0.43555209040641785, 0.09683409333229065, -0.05825313925743103, 0.7846515774726868, 0.42581260204315186, 0.26826536655426025, 0.4794638156890869, 0.7745844125747681, 0.1878872662782669, 0.38491666316986084, 1.0404644012451172, -1.6669657230377197, -0.11024171113967896, -1.1170169115066528, 0.42968130111694336, -0.37543585896492004, -0.11405780911445618, -0.21113166213035583, -1.6378084421157837, -1.1182559728622437, -0.9550514221191406, 0.21461938321590424, -0.43778902292251587, -0.027265891432762146, 1.118403673171997, 0.14755332469940186, -1.1951817274093628, 1.1831719875335693, -0.8689103126525879, -0.2996082603931427, 0.4628334045410156, 0.5841919183731079, 0.5533729791641235, -0.07960740476846695, -0.8458536863327026, -0.13776883482933044, 0.169167622923851, -0.841927170753479, -0.2811136245727539, -1.0275622606277466, -0.5021167397499084, 0.0526619590818882, -0.1256292313337326, -0.6590075492858887, 1.1889111995697021, -0.03650381788611412, -1.1520112752914429, 0.5674793720245361, -0.0619923435151577, -0.1443500965833664, 0.3126657009124756, 0.11354430019855499, -0.43789422512054443, 0.06105554848909378, -0.5874487161636353, 0.534975528717041, 0.818836510181427, -0.09859348088502884, 0.03569234162569046, -0.03128940984606743, -0.4188530445098877, 0.23399096727371216, -0.6232109069824219, 1.1526541709899902, -0.5451778173446655, 0.05561872199177742, 0.22238336503505707, 0.16826656460762024, -0.41348105669021606, -0.2561059594154358, -0.20823682844638824, -0.6828110218048096, 0.9338521957397461, 0.2800405025482178, 0.5042972564697266, -1.1365410089492798, -0.9301456809043884, 0.2966088056564331, 0.06588445603847504, 0.11584284156560898, -0.668545663356781, 0.7912953495979309, -0.34544461965560913, 0.24844932556152344, 0.15174242854118347, -1.4336971044540405, -0.04340275749564171, 0.0285448357462883, -0.6839526891708374, -0.31032341718673706, -0.012521822936832905, 0.7384552955627441, -0.5475625395774841, 0.09143486618995667, -0.28601232171058655, 0.5844630599021912, -1.298713207244873, 1.316928744316101, -0.36277589201927185, 0.1908826231956482, 0.03250759467482567, -0.4526549279689789, 0.2812930643558502, -0.6818894743919373, 0.8200103044509888, -0.4879102408885956, -0.21098506450653076, 0.7680583000183105, 0.008810062892735004, 1.2554091215133667, -0.591424286365509, 0.10796739161014557, 0.41071170568466187, -0.7707393765449524, 0.14114952087402344, 0.38106998801231384, 0.148348867893219, -0.40798869729042053, 0.4586406648159027, 0.622822105884552, -0.5129562616348267, 0.3608085811138153, 1.0735961198806763, 0.9662041664123535, -0.1365177184343338, 0.29189199209213257, 0.39456281065940857, -0.6346746683120728, 0.40460875630378723, 0.4667939245700836, 0.548838198184967, 0.32284367084503174, -0.38804855942726135, -0.3284011483192444, -0.21245379745960236, -0.8315650224685669, -0.22815506160259247, 0.4225258529186249, 0.49777546525001526, 0.28949418663978577, 0.6286869645118713, -0.675035297870636, -0.7503207921981812, 0.21388208866119385, 0.5656471252441406, 1.3820841312408447, -0.23946204781532288, -0.1596444994211197, -0.649791419506073, -0.22136124968528748, -0.12385236471891403, -0.38448190689086914, 0.15547466278076172, -0.22081701457500458, -0.6766008138656616, -1.4302186965942383, 0.6655175089836121, 0.13097132742404938, 1.156441330909729, -0.42837607860565186, -0.9098138809204102, -0.6904672980308533, 0.9430855512619019, -0.9000356793403625, -0.22272703051567078, 0.8552056550979614, -0.8531020283699036, -0.011077579110860825, 0.12891478836536407, -0.2635147273540497, 0.12084855884313583, -0.07815705984830856, 1.2046014070510864, 0.10758838057518005, -0.18812716007232666, -0.008430656976997852, 0.542312502861023, -0.7853395342826843, -0.6793539524078369, 0.315349817276001, 0.11676904559135437, -0.41828224062919617, -0.026770906522870064, 0.09421985596418381, -0.04305709898471832, -0.21851018071174622, -0.19780702888965607, 0.29884105920791626, 0.03875688835978508, -0.007024370599538088, 0.7813410758972168, -0.09896621108055115, -0.1776435524225235, -0.9867879748344421, 0.902298092842102, -0.06599362939596176, -0.6038617491722107, 0.24820630252361298, -0.5697559118270874, -0.26899197697639465, 0.7610450387001038, -0.6117206811904907, -0.07268169522285461, -1.2856369018554688, 0.15997712314128876, -0.8920440673828125, 0.005978317931294441, -0.21707047522068024, 0.751011848449707, -0.4198618233203888, 0.07763466238975525, 0.6253438591957092, 0.9520816206932068, -0.06798552721738815, 0.022429484874010086, -0.8335476517677307, 0.05411309003829956, 0.14067411422729492, -0.20404407382011414, -0.12935030460357666, 0.17562590539455414, -1.0081706047058105, -0.2750528156757355, -0.2172885537147522, -0.04368966817855835, -0.21277782320976257, 0.21843960881233215, -0.7712550163269043, -0.8773255348205566, -0.07261981815099716, -0.7738368511199951, -0.7536022067070007, -0.1424330621957779, -0.03892963007092476, -0.19983257353305817, -1.0777641534805298, -1.5721341371536255, -0.08297254890203476, -1.1888118982315063, -1.520999789237976, 0.7001717686653137, 0.4531136155128479, -0.15757592022418976, -0.7456722855567932, -0.07697248458862305, -0.36289873719215393, 1.3213162422180176, -0.5923895239830017, 0.3732350170612335, -0.2510196566581726, -0.12286568433046341, -0.00926383025944233, -0.06298378854990005, 0.40654847025871277, -0.912571370601654, 0.1805698722600937, -0.8185948729515076, -0.2428811639547348, -0.4071919322013855, -0.7781060338020325, 0.1262986809015274, 0.23579205572605133, 1.02851140499115, 0.29435935616493225, -0.6565209627151489, 0.8498647212982178, 1.2687199115753174, -0.9037505984306335, -0.08458232879638672, -0.08570462465286255, 0.9750667214393616, -0.1649305671453476, -0.5193800926208496, 0.746652364730835, -0.22701823711395264, 0.5253180861473083, 0.15551887452602386, -0.6763954162597656, -0.2960623800754547, -0.2830650210380554, 0.13954196870326996, 1.8748459815979004, 0.9495922327041626, 0.025878550484776497, -0.8138524293899536, 0.05413654074072838, -0.8049184679985046, -0.1307733952999115, 0.716304361820221, 0.7707808017730713, -0.027206672355532646, -0.26769572496414185, -0.105374276638031, -0.19051800668239594, 0.22338934242725372, 0.713965654373169, -0.4713208079338074, -0.7652627825737, 0.2687110900878906, 0.7815508842468262, 0.5951916575431824, 0.4151979088783264, -0.05891024321317673, 0.4607444703578949, 14.786242485046387, 1.0762194395065308, -0.23986011743545532, 0.9443883895874023, 0.6510031223297119, -0.19175489246845245, -0.29227322340011597, -0.16143453121185303, -1.0143126249313354, 0.25370725989341736, 1.9994847774505615, 0.3949485123157501, 0.7211573719978333, 0.11058617383241653, 0.049111299216747284, -0.01449687872081995, -0.21010728180408478, 0.5859172344207764, 0.39974749088287354, -1.5055060386657715, -0.022898348048329353, 0.2620800733566284, 0.6210262775421143, 1.3061717748641968, 0.8857743144035339, 0.6641529202461243, 0.5303949117660522, -0.3213408887386322, 0.4278716444969177, -0.01697571389377117, 0.7971900701522827, -0.5826370120048523, 0.2817743122577667, 0.48846814036369324, -0.8046412467956543, 0.15785087645053864, -0.48787009716033936, -0.9725488424301147, 0.22034424543380737, 0.14008039236068726, -0.4862191677093506, -0.30499544739723206, -0.11322008073329926, 0.8514125347137451, -0.023109782487154007, 0.3999479115009308, 0.09361745417118073, 0.6676907539367676, -0.6210184693336487, -0.007404811214655638, 0.3034582734107971, 0.28403040766716003, -0.271630197763443, -0.2596707344055176, -0.055940352380275726, -0.4380185008049011, 0.547089159488678, 0.04770173877477646, -0.4820249378681183, -0.46588993072509766, -0.1957298070192337, -0.22561156749725342, 0.13834647834300995, 0.8158633708953857, 0.32349562644958496, 0.02855430543422699, -0.5690356492996216, 0.4007214903831482, 0.6233657598495483, -0.2756047546863556, -0.31943607330322266, 0.20824624598026276, 0.41094693541526794, -0.6970404386520386, -0.34873446822166443, 0.495637983083725, -0.24857455492019653, -0.38613346219062805, -0.9986903071403503, -0.5164397358894348, 0.10573317855596542, -0.7027379274368286, -0.35036584734916687, 1.0226293802261353, -0.15953025221824646, -0.13980567455291748, 0.5281702876091003, -0.7844514846801758, -0.4060104787349701, 0.47835901379585266, -1.482460379600525, -0.6775948405265808, 0.2317357361316681, -0.27278822660446167, -0.5955871343612671, -0.1550508439540863, 1.270843744277954, 0.20543892681598663, -0.6038699746131897, -0.06219170242547989, 0.43740314245224, -0.445446252822876, -0.5201079845428467, -0.36975404620170593, 1.1116902828216553, 0.053161345422267914, -0.17708851397037506, -0.40820997953414917, -0.017884444445371628, 0.22959955036640167, -0.95567786693573, -0.32482632994651794, 0.5231000781059265, -0.5306909084320068, -0.1339605748653412, -0.8499460816383362, -0.7014704346656799, 0.3982430696487427, 0.6400149464607239, 0.11215804517269135, 0.6301077604293823, -0.059552256017923355, -0.33336445689201355, -0.2637607455253601, -0.7973586916923523, -0.1314994841814041, 0.385526180267334, -0.9625563025474548, 0.2023458629846573, 0.737421452999115, 0.633038341999054, -1.1681956052780151, -1.0133417844772339, -0.3080708086490631, -0.07800177484750748, -0.46641820669174194, 0.884609043598175, 0.04476168751716614, 0.5503990650177002, 1.3404911756515503, 0.16951672732830048, -0.6835130453109741, 0.478480726480484, -1.0704195499420166, 0.05573458597064018, -0.19253358244895935, 0.22840747237205505, -0.7515406608581543, 0.8026794195175171, 0.8081218004226685, -0.06208054721355438, -0.6429519653320312, -0.5511218905448914, -0.2958575189113617, -0.14420513808727264, -0.4008106589317322, 0.6339666843414307, -0.06155175343155861, 0.17659856379032135, -0.20557637512683868, 0.5679844617843628, 0.6468730568885803, 0.00783142726868391, -0.7052919864654541, 0.39123812317848206, 0.08332200348377228, -0.33783531188964844, -0.46817290782928467, -0.3005644381046295, -1.2073614597320557, 0.08689037710428238, -1.2077186107635498, -0.28570854663848877, -0.24574410915374756, -0.537906289100647, -0.605437159538269, -0.07170186191797256, 0.09597846865653992, 0.6310489773750305, -0.020352868363261223, -0.344878226518631, -0.22283019125461578, -0.802451491355896, 0.8870008587837219, 0.5971140265464783, -0.26905375719070435, -0.033714596182107925, -0.453911691904068, 0.46314191818237305, 0.388855904340744, 0.7495124340057373, -0.20057743787765503, -0.9489675164222717, -1.3724433183670044, 0.15868452191352844, -0.1534319967031479, -0.10735094547271729, -1.3503804206848145, 0.6122639179229736, 0.6370706558227539, -0.31789693236351013, 0.32728514075279236, 0.3943963646888733, -1.1430058479309082, -0.13837076723575592, 0.6984444856643677, -0.5027685761451721, 0.5939695239067078, 0.6050198078155518, -0.7089142203330994, -0.13826394081115723, 0.6961917877197266, 0.08984935283660889, -0.7780121564865112, -0.701539933681488, 0.1763734519481659, -0.44043341279029846, 0.19608259201049805, 0.10555920749902725, 0.24026620388031006, -1.340887188911438, 0.260608047246933, 0.06843411177396774, 0.4441699981689453, 0.08888866007328033, 0.7964094281196594, 0.034464143216609955, -0.7186970114707947, 0.5263765454292297, 0.7470893859863281, -0.6469449400901794, 0.22657938301563263, 0.9244093298912048, 0.8527612686157227, -0.9643570780754089, 0.2480425089597702, 0.29084062576293945, 0.05879516899585724, -0.9250333905220032, 0.09266921132802963, 0.9128181338310242, -0.6750145554542542, -0.18095040321350098, 1.2040014266967773, -0.7107326984405518, -1.0558141469955444, -0.0377819798886776, -1.0626919269561768, -0.391397088766098, -0.5067059397697449, 0.5462315678596497, 0.15631429851055145, 0.2449229210615158, 0.29463550448417664, -0.43081557750701904, 0.20287196338176727, 0.01623660884797573, -0.14280228316783905, 0.42390573024749756, 0.10255023092031479, -0.4130062758922577, 0.6009786128997803, 0.8743773698806763, -0.6093958020210266, -1.077653408050537, -0.7091125249862671, -0.7036221623420715, -0.06692050397396088, 0.7289029359817505, -0.033459968864917755, -1.003930687904358, 0.713015079498291, 0.480432391166687, 0.44172433018684387, 0.18020328879356384, -0.006513891275972128, 0.530408501625061, 0.21369071304798126, 0.2150743454694748, -0.49862024188041687, -0.46768322587013245, 1.0885488986968994, 0.6627066135406494, -0.7266739010810852, 0.5240940451622009, -0.1989711970090866, -0.6757834553718567, 0.48705509305000305, 0.21726003289222717, -0.2137421816587448, 1.106921672821045, 0.2735162079334259, -0.14521071314811707, -0.06465853750705719, -1.1371352672576904, 0.1782970428466797, 0.39879310131073, 0.5893377661705017, 0.5232626795768738, 0.4140875041484833, 0.2939634621143341, 0.5119423270225525, 0.043806493282318115, 0.08270073682069778, 0.24729453027248383, 0.9063543081283569, 0.08476532250642776, -0.04276043549180031, -0.040988415479660034, 0.718594491481781, -0.6154114007949829, -0.6576648354530334, 0.46416711807250977, 0.521440327167511, 0.26481151580810547, 0.7064354419708252, 1.491342544555664, -0.0402553491294384, 0.32186293601989746, -0.23067446053028107, 0.4459877908229828, -0.4309076964855194, -0.8706378936767578, -0.007331421598792076, -0.5111291408538818, -0.34514614939689636, -0.10165311396121979, -0.2304636836051941, -0.46598103642463684, -0.6567602157592773, 0.729303777217865, 0.48656806349754333, 0.5582036375999451, 0.7425662875175476, 1.1761332750320435, 1.1476198434829712, -0.08580504357814789, -1.3092182874679565, -0.5290815830230713, -0.6126746535301208, -0.32953447103500366, -0.5460869669914246, -0.44683995842933655, 0.21820947527885437, -0.12515395879745483, -0.6892749071121216]}, "authors": [{"authorId": "1491753352", "name": "Max Ryabinin"}, {"authorId": "3239480", "name": "Tim Dettmers"}, {"authorId": "152690218", "name": "Michael Diskin"}, {"authorId": "2113838061", "name": "Alexander Borzunov"}], "references": [{"paperId": "6b117a8dcaa161562b0a69afbb9811e11afb5b3e", "title": "Decentralized Training of Foundation Models in Heterogeneous Environments"}, {"paperId": "5ce7d930b87a16ae9a7b5f789181a6e021f4df5d", "title": "Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large DNNs"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "f76a85b34dee7d3a20937a9d016bcbed2e947f2f", "title": "Distributed Deep Learning in Open Collaborations"}, {"paperId": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "title": "Scaling Vision with Sparse Mixture of Experts"}, {"paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0", "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794", "title": "Diffusion Models Beat GANs on Image Synthesis"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "5d21acef02a2035d0c5e3b8f70d4d3c9a164855f", "title": "1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB\u2019s Convergence Speed"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "597926ab9dbd05baca191c409ca430047044297b", "title": "Distributed Deep Learning Using Volunteer Computing-Like Paradigm"}, {"paperId": "2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6", "title": "Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "4066d78b637c2b8e57de5ffd53950134a551de85", "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8", "title": "Scaling Laws for Autoregressive Generative Modeling"}, {"paperId": "8d9fec3009fd9e65e419c82aa80efa25dbf32697", "title": "A Refined Laser Method and Faster Matrix Multiplication"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "198b42dcc3a6dbd254fa25ab6dd23f3e32592950", "title": "Multi-node Bert-pretraining: Cost-efficient Approach"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "c06aac175e463a03ddd30806535ffde4ecc65196", "title": "Verizon"}, {"paperId": "0771a9c573de33306b68d7ffe5564e285e4c74bb", "title": "Breaking (Global) Barriers in Parallel Stochastic Optimization With Wait-Avoiding Group Averaging"}, {"paperId": "bc3706d600729f1b9007c91052258c7c22864f69", "title": "Binary Neural Networks: A Survey"}, {"paperId": "d3214da488806bf4c870080fca18a7f3ecba1e99", "title": "Strassen\u2019s Algorithm Reloaded on GPUs"}, {"paperId": "d9611af1459e34afda31c40cd68c56257a88c8a1", "title": "Interleaved Weighted Round-Robin: A Network Calculus Analysis"}, {"paperId": "54949ee7eca6f314436372e36c4fc5b358ad1b5d", "title": "Machine Learning on Volatile Instances"}, {"paperId": "69170faf00279bdcdfd91290a3756d539f9fb6e6", "title": "Communication-Efficient Distributed Deep Learning: A Comprehensive Survey"}, {"paperId": "a137be632a6248c31f564e076d7587d2206b4f2d", "title": "Communication-Efficient Distributed Deep Learning: A Comprehensive Survey"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "6a5c0fc737b6fbd6672fc4265b5e0ca38de17416", "title": "Training Large Neural Networks with Constant Memory using a New Execution Algorithm"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "19eaa4ac17550fab2917d3f6121ed25e6d857a58", "title": "Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "1340978c92e7cbcf9abe87888150c60984e2964b", "title": "PipeMare: Asynchronous Pipeline Parallel DNN Training"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "a435c7f6999d98c30b45032cd4de60ac9d17fbff", "title": "Scaling Back-propagation by Parallel Scan Algorithm"}, {"paperId": "bf442ab269074665a68e4dbbe19e4efc97862541", "title": "Large Memory Layers with Product Keys"}, {"paperId": "0e4c168b03076eb577c1f29e1bb9bad4ae4a439c", "title": "Scaling the Summit: Deploying the World's Fastest Supercomputer"}, {"paperId": "36ea757ad92b5aa94916a40c7b526af28fcccf02", "title": "Making Asynchronous Stochastic Gradient Descent Work for Transformers"}, {"paperId": "b4a632a7097e7d0631250884dfc6e1f76b376996", "title": "PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "86f9f5a46b19c96f9844eec9d383594151a81ff6", "title": "Speeding up Deep Learning with Transient Servers"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "1f4073cd8989b248c38571c6c7a138dfcb732b67", "title": "A hybrid GPU cluster and volunteer computing platform for scalable deep learning"}, {"paperId": "a5aa03a948052d9520ebf8ba0557cad454476307", "title": "A Tight Convergence Analysis for Stochastic Gradient Descent with Delayed Updates"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "d8c09661b1bebfb690f0566167c87d64c5628d73", "title": "Demystifying Parallel and Distributed Deep Learning"}, {"paperId": "92495abbac86394cb759bec15a763dbf49a8e590", "title": "Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3f1ab8b484f7881a68c8562ff908390742e4ba90", "title": "Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent"}, {"paperId": "61d779905348d0fff4533d4db570932d2c4cb118", "title": "Proteus: agile ML elasticity through tiered reliability in dynamic resource markets"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "3a29aa4eff48624752c07059a44d3288a678c8ab", "title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "b649a98ce77ece8cd7638bb74ab77d22d9be77e7", "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "510a6ec82a2af8cdee185418c3296cb8e2b9716c", "title": "8-Bit Approximations for Parallelism in Deep Learning"}, {"paperId": "a5733ff08daff727af834345b9cfff1d0aa109ec", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "f5b8d11b845ee57755e5eef3847e331902e16221", "title": "Matrix Multiplication on High-Density Multi-GPU Architectures: Theoretical and Experimental Investigations"}, {"paperId": "1b82d54e9a3b06c603d7987ba3ecf437425f6330", "title": "Training deep neural networks with low precision multiplications"}, {"paperId": "e69c8b5df8a4178b1c8c7f154a761147a6f030be", "title": "Project Adam: Building an Efficient and Scalable Deep Learning Training System"}, {"paperId": "3439a127e45fb763881f03ef3ec735a1db0e0ccc", "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs"}, {"paperId": "80d800dfadbe2e6c7b2367d9229cc82912d55889", "title": "One weird trick for parallelizing convolutional neural networks"}, {"paperId": "899c621817d2c156fcaf2b9909016b280d05d7ac", "title": "Errata to: How Mechanics Shaped the Modern World"}, {"paperId": "d1208ac421cf8ff67b27d93cd19ae42b8d596f95", "title": "Deep learning with COTS HPC systems"}, {"paperId": "b7b915d508987b73b61eccd2b237e7ed099a2d29", "title": "Maxout Networks"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "3127190433230b3dc1abd0680bb58dced4bcd90e", "title": "Large Scale Distributed Deep Networks"}, {"paperId": "36f49b05d764bf5c10428b082c2d96c13c4203b9", "title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent"}, {"paperId": "3ba5619e6f2928aed12c7dc8f5fbf3c693edaf65", "title": "Understanding the efficiency of GPU algorithms for matrix-matrix multiplication"}, {"paperId": "6a775ba9287f33ba62a7a4f353dd1e0a77aa236a", "title": "Predictive Resource Management for Wearable Computing"}, {"paperId": "eb51cb223fb17995085af86ac70f765077720504", "title": "Kademlia: A Peer-to-Peer Information System Based on the XOR Metric"}, {"paperId": "766cd91c0d8650495529cab7d4eeed482729cf89", "title": "Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "a7fd97b0dc1f5ac0ede3abff70f537ed29ed914e", "title": "Weighted Round-Robin Cell Multiplexing in a General-Purpose ATM Switch Chip"}, {"paperId": "c8d90974c3f3b40fa05e322df2905fc16204aa56", "title": "Adaptive Mixtures of Local Experts"}, {"paperId": "1992fee8d33e66e32a213ecf047f9022adbff92e", "title": "Matrix multiplication via arithmetic progressions"}, {"paperId": "052b1d8ce63b07fec3de9dbb583772d860b7c769", "title": "Learning representations by back-propagating errors"}, {"paperId": "69e68bfaadf2dccff800158749f5a50fe82d173b", "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position"}, {"paperId": null, "title": "Fine-tuning language models over slow networks using activation quantization with guarantees"}, {"paperId": "77e73174e606c0820a52a940088832b32d9a033e", "title": "Efficient Large-Scale Language Model Training on GPU Clusters"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/meshtransformer-jax"}, {"paperId": "d931f84abfc4550c10ceb113b142c8eb3e07571e", "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training"}, {"paperId": null, "title": "Monthly ip latency data"}, {"paperId": "6cbb968ef5d81f33134c7022b12241d1bbef47ff", "title": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021"}, {"paperId": null, "title": "2021) using the infrastructure that is several times cheaper"}, {"paperId": null, "title": "Estimated energy consumption 29,899.23 kW"}, {"paperId": null, "title": "2021) measures GPU utilization as a fraction of theoretical peak FLOP/s of their GPUs. In contrast, we only measure what fraction of time the GPU is running the model, regardless of efficiency"}, {"paperId": null, "title": "SWARM Parallelism: Training Large Models matic sharding"}, {"paperId": null, "title": "The error-feedback framework: Sgd with delayed gradients"}, {"paperId": null, "title": "2020) with the batch size"}, {"paperId": null, "title": "Microsoft announces new supercomputer, lays out vision for future ai work"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Openwebtext corpus"}, {"paperId": null, "title": "2019), but we did not investigate them in this work due to potential convergence issues. How much failure can SWARM handle? As long as there is at least one operational peer at every pipeline stage"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "URL http://proceedings.mlr.press/ v28/goodfellow13.html"}, {"paperId": null, "title": "Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp"}, {"paperId": "95c4ebb6df40abc74c9cf36994c0f914be3b04bd", "title": "ASSOCIATION FOR COMPUTING MACHINERY"}, {"paperId": "417c09e305453f1a2ea0ad4b3cb7091708ad2ad9", "title": "Discorsi e dimostrazioni matematiche intorno a due nuove scienze (1638)"}, {"paperId": "1d27a56a8133f947a5a0217b00241d26f585f834", "title": "This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."}, {"paperId": null, "title": "Multidimensional planner for DNN parallelization"}, {"paperId": null, "title": "Automating inter-and intra-operator"}, {"paperId": null, "title": "TorchElastic"}, {"paperId": null, "title": "ElasticHorovod"}, {"paperId": null, "title": "Fairscale: A general purpose modular pytorch library for high performance and large scale training"}, {"paperId": null, "title": "Models Can Be Surprisingly Communication-Efficient"}]}