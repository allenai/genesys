{"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers", "abstract": null, "venue": "AI Open", "year": 2021, "citationCount": 699, "influentialCitationCount": 18, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This survey provides a comprehensive review of various Transformer variants and proposes a new taxonomy of X-formers from three perspectives: architectural modification, pre-training, and applications."}, "embedding": {"model": "specter_v2", "vector": [0.5547599196434021, 0.9118508696556091, -0.2526181638240814, -0.06471671164035797, -0.19674235582351685, -0.0055763693526387215, 0.45134684443473816, -0.15951839089393616, -0.152374267578125, -0.5453686118125916, 0.5374171137809753, 0.1752774715423584, 0.4444512128829956, -0.15699198842048645, -0.10125576704740524, -0.19540733098983765, -0.7861058712005615, 0.013960273005068302, 0.2707056999206543, -0.4403621256351471, -0.2150314599275589, -0.5950931310653687, -1.3774352073669434, 0.3882676661014557, 0.2950286865234375, 1.101973533630371, -0.14014604687690735, 0.5859877467155457, -0.53158038854599, 0.6534134149551392, 0.6212821006774902, -0.5192282199859619, 0.4733813405036926, 0.09131704270839691, -0.14219260215759277, 0.14274932444095612, 0.6658311486244202, -0.4566152095794678, -0.5904499292373657, 0.8149146437644958, -0.3007437288761139, 0.16680260002613068, 0.10858871787786484, -0.8996380567550659, 0.013469820842146873, 1.0198380947113037, 0.8578640818595886, 0.7700657844543457, -0.6065144538879395, -0.7291438579559326, 1.6930792331695557, -1.3230597972869873, -0.43270790576934814, 1.4034159183502197, 0.6385923624038696, 0.23510035872459412, -0.46611732244491577, -0.7278193235397339, 0.5616989135742188, 0.24660417437553406, -0.4951031804084778, -0.7145868539810181, 0.42762649059295654, 0.05595023185014725, 1.7519209384918213, -0.0659756287932396, 0.21508121490478516, 0.17766904830932617, -0.0032174193765968084, 1.5865440368652344, 0.3213195204734802, -0.9235585927963257, -0.037816714495420456, -0.16823847591876984, 0.2983463406562805, 1.1409053802490234, -0.6602199077606201, 0.40286558866500854, -1.3565117120742798, -0.16590481996536255, 0.4757255017757416, 0.02118494361639023, -0.09619602560997009, -0.4069712460041046, -0.044318459928035736, 0.8990393280982971, 0.4288020133972168, 1.0744798183441162, -0.29596370458602905, 0.38833191990852356, 0.630770206451416, 0.2508534789085388, -0.4915325939655304, 0.5627648234367371, 0.0706271156668663, 0.647661566734314, -1.0688623189926147, -0.11448924243450165, -0.31924134492874146, 0.6369156837463379, -0.29665783047676086, 0.95730060338974, -0.603077232837677, 0.5313869118690491, 1.1778651475906372, -0.0741080790758133, 0.359348863363266, -0.6954540014266968, 0.12165820598602295, -0.461008757352829, -0.3358601927757263, -0.38991475105285645, 0.2057875543832779, -0.29693764448165894, -0.9242406487464905, -0.8098735213279724, -0.4430340826511383, 0.7634056806564331, -1.246075987815857, 0.49143439531326294, -1.00482177734375, 0.24903500080108643, -0.0019545061513781548, 0.33960363268852234, 0.3549221158027649, 0.35047081112861633, 0.359457790851593, -0.09604515135288239, 0.9708756804466248, -0.8603951334953308, -0.8391732573509216, -1.0142542123794556, 0.06209579482674599, -0.23242583870887756, 0.20029933750629425, 0.14187127351760864, -0.9604433178901672, -1.0875649452209473, -1.153011679649353, 0.1392974704504013, -0.3606916069984436, 0.22870603203773499, 1.0441884994506836, 0.5759475827217102, -1.4515529870986938, 0.5530102252960205, -0.10934390872716904, -0.03501949831843376, 0.3644011616706848, 0.587965726852417, 0.37837594747543335, -0.008520136587321758, -1.1681444644927979, 0.03470504656434059, 0.15349489450454712, -0.667583167552948, -0.29307106137275696, -0.27481400966644287, -1.116720199584961, -0.002258694265037775, 0.12477675825357437, -0.2914496660232544, 1.1555513143539429, -0.1024995818734169, -1.1782374382019043, 0.4224027991294861, -0.40017858147621155, -0.015848243609070778, -0.42065176367759705, -0.0493607297539711, -0.4044983685016632, -0.33277642726898193, -0.00902379211038351, 0.4710482060909271, 0.687886655330658, -0.42493736743927, -0.5187177062034607, 0.17312435805797577, -0.010053199715912342, -0.12280867248773575, -0.5303106307983398, 0.6225699186325073, -0.365667462348938, -0.5464707016944885, 0.7931175231933594, 0.9781370759010315, -0.21720367670059204, 0.05335986986756325, -0.4800015091896057, -0.7241920828819275, 0.6023692488670349, 0.0486408956348896, 1.2424719333648682, -1.2157105207443237, -0.9523797035217285, 0.05196751281619072, -0.0857115089893341, -0.21473990380764008, -0.7688682675361633, 0.2102838158607483, -0.7051751017570496, -0.10881710052490234, -0.09418988227844238, -0.7408932447433472, -0.04673177748918533, 0.07082664221525192, -1.0121265649795532, -0.15699249505996704, 0.17084765434265137, 0.8182771801948547, -0.8284788727760315, -0.031212028115987778, 0.27616259455680847, 0.26665782928466797, -0.786292314529419, 1.2630393505096436, 0.09498605877161026, -0.3575306236743927, 0.2098691314458847, 0.2790694534778595, 0.17802445590496063, -0.16980388760566711, 0.5779921412467957, -0.7255053520202637, 0.1403462141752243, 0.7079606056213379, -0.5214225649833679, 1.0900777578353882, -0.12173717468976974, 0.6242523193359375, -0.12303848564624786, -0.8357933163642883, 0.2396937757730484, 0.6895760893821716, -0.13056376576423645, -0.45355668663978577, 0.7367307543754578, 0.341819703578949, -0.6190387010574341, 0.7925699949264526, 0.8112488985061646, 0.4284406304359436, -0.16344574093818665, -0.029099280014634132, 0.7908127903938293, -0.5051674246788025, 0.2695627510547638, 0.2877396047115326, 0.7541700005531311, -0.008406434208154678, 0.4842948019504547, -0.08607514202594757, 0.2346223145723343, -0.8712844252586365, 0.131092831492424, 0.49282824993133545, 0.535621702671051, 0.9045430421829224, 0.16484405100345612, -0.5230397582054138, -0.22418718039989471, -0.4159018397331238, 0.5764073729515076, 1.527533769607544, -0.267011821269989, -0.2954956293106079, -0.3216674327850342, -0.2734895646572113, -0.7448915243148804, 0.00799537729471922, -0.28401634097099304, -0.03747430816292763, -0.4001004993915558, -0.7674742341041565, 1.12874174118042, 0.7349787950515747, 1.0672036409378052, -0.27379557490348816, -0.4954792261123657, -0.1024966761469841, 0.05071921646595001, -0.8016888499259949, -0.5001835823059082, 0.872858464717865, -0.9573938250541687, -0.51334148645401, 0.2961052358150482, -0.4300324022769928, 0.39598581194877625, -0.7004578709602356, 0.9519480466842651, -0.5575427412986755, 0.1328977346420288, -0.007235465571284294, 0.5987237095832825, -0.38158610463142395, -0.5361198782920837, -0.3368145823478699, 0.21204222738742828, 0.07674499601125717, 0.588200569152832, 0.004263310693204403, 0.48306483030319214, 0.11102572828531265, -0.531664252281189, -0.1799720823764801, 0.09516769647598267, 0.39818328619003296, 1.0487045049667358, 0.017707014456391335, -0.2933690547943115, -0.8384009003639221, 0.8933048248291016, 0.7424904704093933, -0.3191620111465454, 0.18526646494865417, -0.6305122971534729, -0.16855338215827942, 0.48343637585639954, -0.4700208604335785, 0.03839878365397453, -0.39175382256507874, 0.41513848304748535, -0.6206666827201843, 0.18593892455101013, -0.1476697474718094, 0.006949900183826685, 0.0764547809958458, 0.21552586555480957, 0.4769682288169861, 0.06853905320167542, 0.01586216129362583, 0.5541855692863464, -0.5421533584594727, 0.5559596419334412, 0.1582748144865036, 0.3029528856277466, 0.025948956608772278, -0.5066145062446594, -0.5471741557121277, -0.2605075538158417, -0.33495214581489563, -0.027468951418995857, -0.34141722321510315, -0.1356019377708435, -0.7392112612724304, -0.6509432196617126, 0.1541358381509781, -0.738807201385498, 0.2203139215707779, -0.2793913185596466, -0.3972626328468323, -0.234566792845726, -1.0496292114257812, -1.2519041299819946, -0.5410733819007874, -0.7297362089157104, -1.0395373106002808, 0.11978348344564438, 0.19339746236801147, 0.06678269803524017, -0.15778319537639618, -0.436197429895401, -0.2812345325946808, 0.9314000606536865, -0.5468412637710571, 1.0293976068496704, -0.373618483543396, -0.4247775971889496, 0.20868277549743652, 0.27818772196769714, 0.7081758379936218, 0.022112924605607986, -0.06353697180747986, -1.4785089492797852, 0.3208184540271759, 0.1983039826154709, 0.0631568655371666, 0.3286792039871216, 0.10284598171710968, 0.5364482998847961, 0.006086976267397404, -0.29669585824012756, 0.2427944391965866, 1.2347038984298706, -0.17867429554462433, 0.09206345677375793, 0.2032936066389084, 0.5684866309165955, -0.19751949608325958, -0.2931005656719208, 0.31994983553886414, 0.04128621891140938, 0.25577086210250854, 0.3950001895427704, 0.09131345897912979, -0.1690334528684616, -0.2689858675003052, 0.3654263913631439, 1.7201855182647705, 0.1520431786775589, 0.12963628768920898, -0.9569829106330872, 1.0085554122924805, -0.9161427021026611, -0.6013739109039307, 1.097198486328125, 0.5017129778862, 0.3783898949623108, -0.08705931901931763, -0.4618282616138458, 0.8221784234046936, 0.6981914043426514, 0.6660218834877014, -0.1378048211336136, -0.780571699142456, -0.2510354220867157, 1.1607539653778076, 0.11963935941457748, 0.6685490608215332, -0.47773557901382446, 0.22353631258010864, 15.012656211853027, 0.7586095333099365, -0.19010776281356812, 0.4055759012699127, 0.36015209555625916, 0.3162142336368561, -0.5792620778083801, 0.12851077318191528, -0.6465479135513306, -0.3762090504169464, 0.5349173545837402, 0.35171836614608765, 0.6668208837509155, 0.2673311233520508, -0.6059985160827637, 0.40132248401641846, -0.520408034324646, 0.602164626121521, 0.48238706588745117, -1.3462193012237549, 0.4343391954898834, 0.1050003319978714, -0.16093941032886505, 0.30825135111808777, 0.6888922452926636, 0.720031201839447, 0.3996586203575134, -0.23810093104839325, 0.6635438799858093, -0.09631399810314178, 1.0000070333480835, -0.04940740019083023, 0.33867979049682617, 0.04149625822901726, -1.3511170148849487, -0.5532174706459045, -0.4896617531776428, -1.1799033880233765, -0.0747034028172493, 0.3415338397026062, -0.4284568727016449, -0.4984913468360901, -0.09470244497060776, 0.61620032787323, 0.034757521003484726, 0.41816240549087524, -0.5691960453987122, 0.7981937527656555, -0.5239135026931763, 0.4569012522697449, 0.20256991684436798, 0.4967259168624878, 0.3614828586578369, -0.41440263390541077, 0.060251038521528244, 0.1693313717842102, 0.10183113068342209, 0.04609965160489082, -0.34009090065956116, -0.11570802330970764, -0.270219624042511, -0.49773019552230835, -0.139267235994339, 0.6834837198257446, 0.2053012102842331, 0.38240912556648254, -0.3518180847167969, 0.02882395125925541, 0.26162293553352356, 0.14084069430828094, -0.5551018714904785, -0.5822445154190063, 0.36706364154815674, -0.023156633600592613, 0.21264076232910156, 0.9225528836250305, -0.28414303064346313, -0.6071885824203491, -0.7751125693321228, -0.554633617401123, 0.5602158308029175, -0.6936835646629333, -0.5867652893066406, 1.4304289817810059, -0.5045493245124817, -0.727563202381134, 0.43006283044815063, -0.73527991771698, -0.3112868070602417, 0.03399364650249481, -1.19136381149292, -0.7960310578346252, 0.12288611382246017, -0.10302343219518661, -0.572165846824646, -0.5599325895309448, 1.1591445207595825, 0.12333831191062927, 0.133360356092453, 0.23480896651744843, -0.4106442630290985, 0.3061273992061615, -0.5275819301605225, -0.6984789967536926, 0.5604948401451111, 0.6465926766395569, 0.3662887215614319, 0.26582416892051697, 0.1897611767053604, 0.6680751442909241, -0.2820436656475067, 0.012464371509850025, 0.7640027403831482, -0.18280039727687836, -0.37354928255081177, -0.6463245749473572, -0.7299439907073975, 0.34144270420074463, 0.8019558191299438, -0.18997065722942352, 0.47585880756378174, -0.0058176834136247635, -0.8398149013519287, -0.4565618336200714, -0.8010942339897156, 0.06387960910797119, 0.6114741563796997, -1.162917137145996, -0.6416609883308411, -0.6285226345062256, 0.11066604405641556, -0.773200511932373, -0.43279072642326355, 0.1264445185661316, -0.12412714213132858, -0.47962427139282227, 1.0415263175964355, 0.026349389925599098, 0.25382551550865173, 0.44309309124946594, -0.6150493025779724, -0.6410163044929504, -0.15653815865516663, -0.9008450508117676, 0.1707657128572464, 0.05581320449709892, 0.3913201689720154, -0.3807932138442993, 0.42755335569381714, 0.5113856196403503, 0.1413392871618271, -0.5148100256919861, -1.373035192489624, -0.010625717230141163, -0.3987208306789398, -0.7500548362731934, 0.554390013217926, -0.1958385407924652, 0.02245444804430008, 0.40997710824012756, 0.5666646957397461, 0.4091373682022095, -0.13950878381729126, -0.4917396306991577, -0.07440892606973648, -0.13767193257808685, -0.14476215839385986, -0.4597005844116211, -0.5570968389511108, -1.4969772100448608, 0.021801339462399483, -1.2698769569396973, 0.1332482248544693, -1.0515412092208862, -0.5903943777084351, 0.014515738934278488, -0.7178100347518921, 0.7207947373390198, 0.5237485766410828, -0.03194859251379967, 0.006892112549394369, -0.48596295714378357, -0.1731945276260376, 0.671225905418396, 0.6428796052932739, -1.1844009160995483, 0.0740118995308876, -0.15650150179862976, -0.09040214121341705, 0.2180684208869934, 0.3430313467979431, -0.44246435165405273, -0.9374642968177795, -1.0290511846542358, -0.06515593826770782, -0.20144852995872498, -0.3825066089630127, -1.117447853088379, 0.8450843691825867, 0.4089250862598419, -0.2883862853050232, 0.1885417401790619, 0.8178939819335938, -1.1114561557769775, -0.46942582726478577, 0.3286658227443695, -0.5634828805923462, 0.16973556578159332, 0.2871275544166565, -0.7544451355934143, -0.8119909763336182, 0.7776991128921509, 0.44822683930397034, -0.82503741979599, -0.7054579257965088, 0.24210138618946075, -0.8114184141159058, -0.11065676808357239, -0.2183658331632614, -0.5038591623306274, -0.8701143860816956, -0.2240784913301468, -0.23669292032718658, -0.07654233276844025, -0.49228864908218384, 0.8186602592468262, 0.6395683288574219, -1.345217227935791, 0.33433589339256287, 0.9217095971107483, -0.06551091372966766, -0.3699982762336731, -0.21478573977947235, 0.2649762034416199, -0.10311585664749146, 0.39926207065582275, -0.047796830534935, 0.24788381159305573, -0.9293900728225708, -0.3187394440174103, 0.9300896525382996, -0.5897683501243591, -0.04904667288064957, 1.3920819759368896, -0.384103000164032, -0.8235099911689758, 0.23543763160705566, -1.3769655227661133, -0.7480844259262085, -0.20599864423274994, 0.4594886302947998, -0.12598304450511932, -0.12051527202129364, -0.2992805540561676, -0.6662668585777283, 0.049767326563596725, 0.16137321293354034, -0.5132196545600891, 0.7211514115333557, 0.4018007516860962, -0.6043621897697449, 0.5632928609848022, 0.22276456654071808, -0.6601122617721558, -0.5935905575752258, -0.6060371398925781, -0.3550100326538086, -0.4485953152179718, 0.3672552704811096, -0.17210818827152252, -1.2944997549057007, 0.9616849422454834, 0.7594925165176392, 0.3689366281032562, 0.8228710889816284, -0.4258219003677368, 0.16333743929862976, 0.44697630405426025, 0.39225560426712036, -0.6724624633789062, -0.3827170729637146, 1.2614549398422241, 1.077352523803711, -0.16861557960510254, 0.29685118794441223, -0.416578471660614, -0.4875749945640564, 0.8104162216186523, 0.16569072008132935, -0.04312192276120186, 1.1924036741256714, 0.30637839436531067, 0.47175806760787964, 0.3525746166706085, -0.9384831786155701, -0.39066168665885925, 0.6188086867332458, 1.3653446435928345, 0.7476659417152405, 0.159097358584404, 0.38440391421318054, 1.0946636199951172, -0.047753773629665375, 0.1889415681362152, 0.5579720139503479, 0.5187497735023499, -0.07897928357124329, -0.3629925549030304, -0.053760018199682236, 0.791452944278717, -0.6193248629570007, -0.6929734945297241, 0.26292935013771057, 0.536878764629364, 0.46944302320480347, 0.9209328293800354, 0.6438997983932495, -0.47637197375297546, 0.8072920441627502, 0.2892286777496338, 0.5834349393844604, -0.47850021719932556, -0.5161004066467285, 0.04161250218749046, -0.7340899109840393, -0.1723470538854599, -0.5918546319007874, -0.17158885300159454, -0.2551221549510956, 0.046874407678842545, -0.23035453259944916, 0.2162870466709137, 0.1872042566537857, 0.6607896089553833, 0.27257588505744934, 0.6829095482826233, -0.40537911653518677, -0.19342224299907684, -0.25234538316726685, -1.0043883323669434, -0.17887814342975616, -0.7090680003166199, -0.001957803266122937, -0.18566924333572388, -0.19850492477416992, -0.13279564678668976]}, "authors": [{"authorId": "2115348804", "name": "Tianyang Lin"}, {"authorId": "2115828967", "name": "Yuxin Wang"}, {"authorId": "2144226697", "name": "Xiangyang Liu"}, {"authorId": "1767521", "name": "Xipeng Qiu"}], "references": [{"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "title": "Hash Layers For Large Sparse Models"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "84daddd294fa3cc12596b5785f81c2a153d2fb1d", "title": "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling"}, {"paperId": "77366bef01df1ab277149b330336a0ef9c5041c4", "title": "Transformer"}, {"paperId": "1a57318be32b740aef1d9b2070db6c0cc565ab0a", "title": "Memory-Efficient Differentiable Transformer Architecture Search"}, {"paperId": "7777a31341fa4bfbd25b96a5320681af8dccf3af", "title": "Exploring Sparse Expert Models and Beyond"}, {"paperId": "9631f5bc3e6d345db42425824f1e7d21d35efa0c", "title": "Early Exiting with Ensemble Internal Classifiers"}, {"paperId": "9c053552dfa6184f7dc56d620bcb1e8f22c729a3", "title": "Accelerating BERT Inference for Sequence Labeling via Early-Exit"}, {"paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa", "title": "CogView: Mastering Text-to-Image Generation via Transformers"}, {"paperId": "e32a12b14e212506115cc6804667b3d8297917e1", "title": "Poolingformer: Long Document Modeling with Pooling Attention"}, {"paperId": "8b8f7c580bb94ace0676be7a5c424b27b1194913", "title": "Learning Shared Semantic Space for Speech-to-Text Translation"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "b6382a7351c0c595f91472ac71d3b2d87b3c4844", "title": "ViViT: A Video Vision Transformer"}, {"paperId": "712bf9c7202b8dec3d06491a380bbac9c9600fbc", "title": "Mask Attention Networks: Rethinking and Strengthen Transformer"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "a11676f2864b2d923bb9facc9f6548c812f9e005", "title": "M6: A Chinese Multimodal Pretrainer"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "df6e4e70af7b3f0a7eb630ed8f36538e6258bc4b", "title": "LazyFormer: Self Attention with Lazy Update"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a", "title": "Conditional Positional Encodings for Vision Transformers"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "d46ccff4a72e48c6865599221a71c4dfd3c954ac", "title": "SETransformer: Speech Enhancement Transformer"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "afad10da0a3b83a4f2a94e8c16c84ac64338e9fe", "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer"}, {"paperId": "5e5fbc41106db9acaaf3a365801051e477f0e984", "title": "UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning"}, {"paperId": "6914a7997ff4be207fa7b3472a9c5879abaec646", "title": "RealFormer: Transformer Likes Residual Attention"}, {"paperId": "35a9749df07a2ab97c51af4d260b095b00da7676", "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"}, {"paperId": "2e1db8cb373f4d4a51d44308b7a457886d855fbb", "title": "End-to-End Object Detection with Adaptive Clustering Transformer"}, {"paperId": "fef57f4fba2e4f6b71de44c11995252d01b5406b", "title": "Reformer-TTS: Neural Speech Synthesis with Reformer Network"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "131ee42e4839d153333e17f46facdb6806e98c73", "title": "Developing Real-Time Streaming Transformer Transducer for Speech Recognition on Large-Scale Dataset"}, {"paperId": "d387600e5150b381a306221a5bc9bd92aa99157b", "title": "Memformer: The Memory-Augmented Transformer"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "3d2660faebfcc1b7c7777f48b32a2eebec346bab", "title": "Guiding Attention for Self-Supervised Learning with Transformers"}, {"paperId": "f257e1c1d10a4d8388cc132a51351e1b5e594576", "title": "On the Sub-Layer Functionalities of Transformer Decoder"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "55c4a747855c74210919c45f7899e1f79e4c97f5", "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data"}, {"paperId": "54bc3e055d05e44c010febc669e8dea394643efc", "title": "Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size"}, {"paperId": "8d39bd11f54f7889eca269899e52323f2702c8fa", "title": "Temporal Context Aggregation for Video Retrieval with Contrastive Learning"}, {"paperId": "b0cd93e95fb6885db47d755a4c631158b0198047", "title": "DeLighT: Very Deep and Light-weight Transformer"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "a9d0fb74e6f23b7e02b0c0a0f5271a7db0a6eb13", "title": "Switching Poisson Gamma Dynamical Systems"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "14216c91c7d02e58717204f04131107778a84e7b", "title": "Multi-Head Attention: Collaborate Instead of Concatenate"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4abdbcf983f78cf3f5bf6e2032503f0e534f6ca8", "title": "BERT Loses Patience: Fast and Robust Inference with Early Exit"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "07a9f47885cae97efb7b4aa109392128532433da", "title": "Hard-Coded Gaussian Attention for Neural Machine Translation"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "39206ad9fe0859a0043bd8caea2e3f8202b67533", "title": "Improving End-to-End Speech Synthesis with Local Recurrent Neural Network Enhanced Transformer"}, {"paperId": "90a1491ac32e732c93773354e4e665794ed4d490", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "8d908042f139575d6688c745e94156c9df6eae07", "title": "Understanding the Difficulty of Training Transformers"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "7066df8fd89cca546d1ef3d66679cb15eba48d50", "title": "FLAT: Chinese NER Using Flat-Lattice Transformer"}, {"paperId": "016a3ba7adcae71f5a23ed2663d8062ae1da63e6", "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "4076e421d1758fdb68411242044cd45747b7e35b", "title": "PowerNorm: Rethinking Batch Normalization in Transformers"}, {"paperId": "e8984c6e6c24aab26c332728a5fff616dfb3adbb", "title": "Learning to Encode Position for Transformer with Continuous Dynamical Model"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "e52051204cb1179584f3b008c9d38848b52c1f28", "title": "ReZero is All You Need: Fast Convergence at Large Depth"}, {"paperId": "26080498fb851b6239114f0871a4957bea3d3684", "title": "Talking-Heads Attention"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "b1c39d042fdf8f00a407b0df734764beb6c3b062", "title": "Low-Rank Bottleneck in Multi-head Attention Models"}, {"paperId": "2118905985d445af8f9686a1b66b703df7842b97", "title": "Controlling Computation versus Quality for Neural Sequence Models"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "d14e56568dc5f57ccdae899d84f91e34ad847670", "title": "How Much Position Information Do Convolutional Neural Networks Encode?"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "d0e28f5dc1feae19e41087a92a87992977fd85af", "title": "Encoding word order in complex embeddings"}, {"paperId": "fc9c52f55ffe0e860b1bb4222fe86cce60c05551", "title": "Meshed-Memory Transformer for Image Captioning"}, {"paperId": "988236ed9defc9d040a5cc3844849d846c9dbd85", "title": "Multi-Scale Self-Attention for Text Classification"}, {"paperId": "2a02c967dd9848064bca0aa69ea6c75b3765d0ee", "title": "Low-Rank and Locality Constrained Self-Attention for Sequence Modeling"}, {"paperId": "40922d386116975853a743b1d810c1e0f03e886a", "title": "Understanding and Improving Layer Normalization"}, {"paperId": "7340a9fbe36587bebb3df1bbbfbfce2bcb576d7f", "title": "Iterative Answer Prediction With Pointer-Augmented Multimodal Transformers for TextVQA"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "3ff8d265f4351e4b1fdac5b586466bee0b5d6fff", "title": "Improving Transformer Models by Reordering their Sublayers"}, {"paperId": "d6b414487787d0b6efd735a3236a690ad13aae70", "title": "TENER: Adapting Transformer Encoder for Named Entity Recognition"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "703685e969fed715e13937c11d7ecc5cc7c4dfd0", "title": "Transformers without Tears: Improving the Normalization of Self-Attention"}, {"paperId": "29962ae812e7142f56f5f67c2db9d00ab3dfa4c4", "title": "T-GSA: Transformer with Gaussian-Weighted Self-Attention for Speech Enhancement"}, {"paperId": "366244acdd930e488ae224ab6e2a92dc24aa7e06", "title": "Axial Attention in Multidimensional Transformers"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "a99855603e7689a4529a1573f0bc4716adb246c1", "title": "Improving Multi-Head Attention with Capsule Networks"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "84898960f68fa78296a102edc8ac81739f9a9408", "title": "Gaussian Transformer: A Lightweight Approach for Natural Language Inference"}, {"paperId": "449892c8e095a97b4c9e058ae5be1e9177d805b7", "title": "R-Transformer: Recurrent Neural Network Enhanced Transformer"}, {"paperId": "bf442ab269074665a68e4dbbe19e4efc97862541", "title": "Large Memory Layers with Product Keys"}, {"paperId": "830995ef17cc291c13f42dfd9f462137de1d2179", "title": "Augmenting Self-attention with Persistent Memory"}, {"paperId": "81e1d123a85562555befb0243256b1a0d9fca014", "title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View"}, {"paperId": "a39398f68ae7e042f2ef5009e31b4e6a20fd5736", "title": "Learning Deep Transformer Models for Machine Translation"}, {"paperId": "7cc730da554003dda77796d2cb4f06da5dfd5592", "title": "Hierarchical Transformers for Multi-Document Summarization"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "3928b2177086532775fbf607ae3e05a0375a5061", "title": "Language Modeling with Deep Transformers"}, {"paperId": "203b543bfa1e564bb80ff4229b43174d7c71b0c0", "title": "HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"}, {"paperId": "f2bb7e2f5a1afad5370159c15760c44df93c0438", "title": "Very Deep Self-Attention Networks for End-to-End Speech Recognition"}, {"paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145", "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "6c9bfe765f076422256fef909bdca186b5880c52", "title": "Information Aggregation for Multi-Head Attention with Routing-by-Agreement"}, {"paperId": "c41a11c0e9b8b92b4faaf97749841170b760760a", "title": "VideoBERT: A Joint Model for Video and Language Representation Learning"}, {"paperId": "2a31319e73d4486716168b65cdf7559baeda18ce", "title": "Star-Transformer"}, {"paperId": "132b07740db20df2c36d6f939d296a7e941feac7", "title": "Insertion-based Decoding with Automatically Inferred Generation Order"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "de20b6488e148a19ae6c63defbfca8a6373e4110", "title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction"}, {"paperId": "fdbdd4e0461d23905104460a02a176907d945f44", "title": "Multi-Head Attention with Disagreement Regularization"}, {"paperId": "1af138dc72fa855cc3bc9c0b83750b461c26e29d", "title": "Modeling Localness for Self-Attention Networks"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "9f2dd5cc190fc713f1339fca838a5537931744f8", "title": "Neural Speech Synthesis with Transformer Network"}, {"paperId": "e20ff55e87e2b3ef02ae0529880bb705f5efbcae", "title": "Document-Level Neural Machine Translation with Hierarchical Attention Networks"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "8fec5d6ac57e90f459e7330775165f2671abc445", "title": "Training Deeper Neural Machine Translation Models with Transparent Attention"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "b227f3e4c0dc96e5ac5426b85485a70f2175a205", "title": "Representation Learning with Contrastive Predictive Coding"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "3a58efcc4558727cc5c131c44923635da4524f33", "title": "Relational inductive biases, deep learning, and graph networks"}, {"paperId": "6e45251b16cd423f3c025f004959c6d2b26efab0", "title": "Accelerating Neural Transformer via an Average Attention Network"}, {"paperId": "41a78e2885b5dc8c719495a33985b5f4880f5b48", "title": "Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition"}, {"paperId": "642c1b4a9da95ea4239708afc5929a5007a1870d", "title": "Tensor2Tensor for Neural Machine Translation"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "603caed9430283db6c7f43169555c8d18e97a281", "title": "Matrix capsules with EM routing"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "c4c06578f4870e4b126e6837907929f3c900b99f", "title": "Dynamic Routing Between Capsules"}, {"paperId": "7cfa5c97164129ce3630511f639040d28db1d4b7", "title": "FiLM: Visual Reasoning with a General Conditioning Layer"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "d89ee98810039d2061ed42ee8026da49c503d16b", "title": "Learning multiple visual domains with residual adapters"}, {"paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "aab5002a22b9b4244a8329b140bd0a86021aa2d1", "title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "df0402517a7338ae28bc54acaac400de6b456a46", "title": "WaveNet: A Generative Model for Raw Audio"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "04cca8e341a5da42b29b0bc831cb25a0f784fa01", "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "2e2aa8fc9af9c51948d1aff238f17a6e13eae191", "title": "Complexity"}, {"paperId": "2ff74d426e712522030057624510c03713fa77ba", "title": "Linear Transformers Are Secretly Fast Weight Memory Systems"}, {"paperId": null, "title": "Predictive Attention Transformer: Improving Transformer with Attention Map Prediction"}, {"paperId": "dc35daba3fb34b2e6a5b12530badb7b799262bbf", "title": "On Position Embeddings in BERT"}, {"paperId": "5366919840236059252c7f8f510dfb36df9e3206", "title": "TransGAN: Two Transformers Can Make One Strong GAN"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "n.d.]. On Position Embeddings in BERT, url = https://openreview.net/forum?id=onxoVA9FxMw, year = 2021"}, {"paperId": null, "title": "Addressing Some Limitations of Transformers with Feedback Memory"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "81a4fd3004df0eb05d6c1cef96ad33d5407820df", "title": "A Comprehensive Survey on Graph Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "9dca2dcad6798862d2756b1727cea332902f8fc8", "title": "TRANSFORMER WITH GAUSSIAN WEIGHTED SELF-ATTENTION FOR SPEECH ENHANCEMENT"}, {"paperId": null, "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": null, "title": "Prototype and Memory Compression . This class of methods reduces the number of queries or key-value memory pairs to reduce the size of the attention matrix"}, {"paperId": null, "title": "Structural prior . Self-attention does no assume any structural bias over inputs"}, {"paperId": null, "title": "Due to the limited receptive field of convolutional layers"}, {"paperId": null, "title": "The improvements on attention mechanism can be divided into several directions"}, {"paperId": "6038d62f22be3162324d3cb5214512966fc6ddb0", "title": "Music Transformer \uae30\ubc18 \uc74c\uc545"}]}