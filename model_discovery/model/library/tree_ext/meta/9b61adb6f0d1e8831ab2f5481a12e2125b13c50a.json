{"paperId": "9b61adb6f0d1e8831ab2f5481a12e2125b13c50a", "title": "Flowformer: Linearizing Transformers with Conservation Flows", "abstract": "Transformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.", "venue": "International Conference on Machine Learning", "year": 2022, "citationCount": 55, "influentialCitationCount": 6, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper linearize Transformers free from specific inductive biases based on the flow network theory and proposes the Flow-Attention mechanism of linear complexity, which yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning."}, "embedding": {"model": "specter_v2", "vector": [0.19396497309207916, 0.43027520179748535, -0.6329352855682373, 0.015966998413205147, -0.07939665764570236, -0.291372686624527, 0.6914248466491699, 0.15842761099338531, -0.2633417844772339, 0.15200862288475037, 0.6966018676757812, 0.21145713329315186, 0.19985829293727875, -0.21012941002845764, -0.5028553009033203, 0.05025750398635864, -1.250702142715454, 0.12968610227108002, -0.09888181835412979, -0.0544283501803875, 0.37519973516464233, -0.41308537125587463, -0.9356752634048462, 0.222732812166214, 0.29666033387184143, 0.959388792514801, -0.10323265194892883, 0.7546662092208862, -0.7089030742645264, 1.359476089477539, 0.44621706008911133, -0.2513052225112915, 0.49570730328559875, -0.024336304515600204, -0.6686764359474182, -0.5485594868659973, 0.5861696600914001, -0.4293152987957001, -0.8520059585571289, 0.6081704497337341, -0.5006092190742493, 0.25230807065963745, 0.2651885747909546, -1.0985199213027954, -0.0725875049829483, 0.9393105506896973, 0.7212513089179993, 0.9466246962547302, -0.23131947219371796, -0.5479556918144226, 1.7090564966201782, -1.1563169956207275, -0.12434215098619461, 1.3245881795883179, 0.263933390378952, 0.0767359808087349, -0.22159834206104279, -0.6183499097824097, 1.0542339086532593, 0.5621002316474915, -0.2852798402309418, -0.21661953628063202, -0.19152650237083435, -0.19241970777511597, 1.8540383577346802, -0.2779226005077362, 0.49185651540756226, 0.30544981360435486, -0.004735732916742563, 1.421793818473816, -0.07061337679624557, -0.4700549244880676, 0.17491690814495087, 0.05938126891851425, 0.2008686661720276, 0.8755462765693665, -0.39813610911369324, 0.26291605830192566, -1.4968359470367432, -0.1562693864107132, 0.7637538909912109, 0.27571508288383484, 0.13315066695213318, -0.4112153649330139, -0.2538681626319885, 0.7250588536262512, 0.8383741974830627, 0.5678372383117676, -0.5801284313201904, 0.952937126159668, 0.7490506172180176, 0.037449248135089874, -0.16301293671131134, 0.5532003045082092, 0.06681418418884277, 0.32483023405075073, -0.25197550654411316, -0.03368402272462845, -0.1191878616809845, 0.815447986125946, 0.12366195023059845, 0.4867088198661804, -0.5616326332092285, -0.020031629130244255, 1.39113450050354, 0.21790234744548798, 0.6314735412597656, -0.859944224357605, 0.08493231236934662, -0.4669407308101654, 0.19102980196475983, -1.0919759273529053, 0.14935940504074097, -0.473095178604126, -0.7928059101104736, -1.1215779781341553, -0.2681443691253662, 0.4901878833770752, -0.5569740533828735, 0.8800711035728455, -0.6010888814926147, 0.258375346660614, -0.35421377420425415, 0.7161181569099426, 0.039919644594192505, 0.8981917500495911, 0.47345635294914246, -0.0880088359117508, 0.6300929188728333, -1.0286511182785034, -0.8594431281089783, -1.1261677742004395, 0.427846759557724, -0.04902332276105881, 0.03503464162349701, -0.1323445439338684, -1.30963933467865, -0.975780725479126, -0.6130405068397522, 0.4608721435070038, -0.6119897365570068, -0.375634104013443, 1.4994444847106934, 0.1935834139585495, -0.9575828909873962, 0.6861886382102966, -0.2804540693759918, -0.04473637416958809, 0.7859306931495667, 0.28856661915779114, 0.2347307652235031, -0.02382664754986763, -1.2599226236343384, 0.4236244857311249, 0.16463050246238708, -0.20178823173046112, -0.5581644773483276, -1.2224338054656982, -1.2006903886795044, 0.3268469274044037, 0.1574702113866806, -0.588359534740448, 0.9749933481216431, -0.568539023399353, -0.9933721423149109, 0.2875787913799286, -0.24156774580478668, -0.071525938808918, 0.2789144217967987, -0.551904559135437, -0.01831316575407982, -0.3653552532196045, 0.1763174682855606, 0.4317797124385834, 0.28399279713630676, -0.2868044674396515, -0.4230424463748932, 0.17923888564109802, -0.26794925332069397, -0.3615323007106781, -0.6019467115402222, 0.646014392375946, 0.16020220518112183, -0.05776895955204964, -0.08432336896657944, 1.0393999814987183, -0.2773270905017853, 0.06779760867357254, -0.09712439775466919, -0.7822179198265076, 0.40561729669570923, 0.2514059245586395, 0.9324489235877991, -0.5149105191230774, -0.6221827268600464, 0.02991333045065403, 0.23673076927661896, -0.012812245637178421, -0.7336006164550781, 0.8372206687927246, -0.581634521484375, 0.3043667674064636, -0.2540641129016876, -0.6072359681129456, -0.0010809454834088683, 0.14810651540756226, -1.00175940990448, -0.08211241662502289, 0.258263498544693, 0.997493326663971, -1.4022619724273682, -0.03823836147785187, 0.27001622319221497, 0.025725478306412697, -1.0272879600524902, 1.7445968389511108, -0.057691432535648346, -0.2040734738111496, -0.218712717294693, -0.5135695934295654, 0.19590134918689728, -0.4432674050331116, 0.5102586150169373, -0.33666741847991943, -0.11767426133155823, 0.5996832847595215, -0.6415342092514038, 1.3877986669540405, -0.5737531185150146, 0.49739688634872437, -0.17317500710487366, -0.9858427047729492, 0.5094079971313477, 0.137640118598938, 0.49114471673965454, -0.8705713152885437, -0.015813317149877548, 0.14859892427921295, -0.6918903589248657, -0.14669956266880035, 0.7096894979476929, 0.7241405844688416, -0.15039168298244476, 0.32455915212631226, 0.7512353658676147, 0.08695056289434433, 0.45569267868995667, 0.8542534708976746, 0.7932606339454651, 0.27480414509773254, 0.5959323048591614, -0.12701863050460815, 0.38918811082839966, -0.9374805688858032, 0.0483492873609066, 0.6728630065917969, 0.4601578712463379, 0.39535626769065857, 0.37886151671409607, -0.9095515608787537, -0.37137681245803833, -0.30267319083213806, 0.9152843952178955, 1.358028531074524, 0.04802059754729271, -0.5634408593177795, -0.21173328161239624, -0.3366515636444092, -0.05152580142021179, -0.4107103645801544, -0.5497617721557617, -0.32482001185417175, -0.7449849843978882, -0.835771918296814, 0.5289793610572815, 0.9107588529586792, 0.9260310530662537, -0.6748486161231995, -0.4243135452270508, -0.05979384481906891, 0.16232767701148987, -0.5369396805763245, -0.7404201626777649, 0.472103476524353, -0.3795980215072632, 0.03317542001605034, 0.2890671491622925, -0.043845031410455704, 0.2142399698495865, -0.6897458434104919, 0.846444845199585, -0.6265919804573059, -0.2459692806005478, 0.2594277560710907, 0.7029799818992615, -0.5970711708068848, -0.17470683157444, 0.10782305151224136, -0.04993370547890663, 0.24185100197792053, 0.1742178350687027, 0.18622052669525146, -0.10162398219108582, -0.09174496680498123, -0.5172951817512512, -0.06372680515050888, 0.10585924237966537, 0.18957005441188812, 0.2388165295124054, -0.5879616141319275, 0.2942855954170227, -0.9220460653305054, 0.7405341267585754, -0.2457539141178131, -0.40492525696754456, 0.09658681601285934, -0.6155872941017151, -0.1204695999622345, 0.6687442660331726, -0.7706759572029114, 0.010057772509753704, -0.5087869167327881, 0.5651569366455078, -0.48557165265083313, -0.04365528002381325, -0.006425522267818451, 0.1576412469148636, 0.1372978687286377, 0.26358655095100403, 0.27989456057548523, -0.032906316220760345, 0.19180168211460114, 0.7746176719665527, -0.979936420917511, 0.5986980199813843, 0.09963662922382355, 0.10699700564146042, -0.10808002203702927, -0.04519591107964516, -0.2226967215538025, -0.4936371147632599, -0.6247431039810181, -0.06790867447853088, -0.6865398287773132, -0.19770987331867218, -0.2355383187532425, -1.7738521099090576, -0.1167076826095581, -1.234445571899414, -0.3756532073020935, -0.2843486964702606, -0.2697891294956207, -0.33570125699043274, -1.1044479608535767, -1.0042095184326172, -1.068068027496338, -0.4535682797431946, -0.607727587223053, -0.19855421781539917, 0.20242902636528015, -0.5037301778793335, -0.9691354036331177, 0.0736771747469902, -0.6610462069511414, 1.3994526863098145, -0.743364155292511, 0.5256661176681519, 0.12596236169338226, -0.7230592370033264, 0.20047405362129211, 0.060668181627988815, 0.17536526918411255, 0.195785254240036, -0.20995202660560608, -1.250868558883667, 0.520776093006134, -0.5547095537185669, -0.44299808144569397, 0.09716347604990005, 0.462998628616333, 0.6931357383728027, -0.4859244227409363, -0.23145948350429535, 0.09404603391885757, 1.3255937099456787, -0.3408004343509674, 0.22952783107757568, 0.17071382701396942, 1.1038466691970825, 0.49929335713386536, -0.21792501211166382, 0.8919063210487366, 0.33075761795043945, 0.23604106903076172, 0.543469250202179, -0.3577435314655304, -0.006886868271976709, -0.8810829520225525, 0.3806729018688202, 1.4370582103729248, 0.4782869219779968, 0.010444303043186665, -0.6505508422851562, 0.9181973934173584, -1.782139778137207, -1.2050975561141968, 0.7975528240203857, 0.6521331667900085, 0.32145723700523376, -0.45988425612449646, 0.029668617993593216, 0.1867169439792633, 0.36742663383483887, 0.3333820104598999, -0.3967036008834839, -0.7913870811462402, 0.2814779281616211, 0.4784889221191406, 0.47326532006263733, 1.0386698246002197, -0.027774354442954063, 0.2911801338195801, 15.003156661987305, 0.5441463589668274, -0.2464611828327179, 0.3176235258579254, 0.30820006132125854, 0.2536025941371918, -0.4092305898666382, 0.09569880366325378, -0.7110042572021484, 0.014832357876002789, 1.0271623134613037, -0.09078648686408997, 0.5416622757911682, -0.01639404334127903, -0.057234250009059906, 0.0610971674323082, -0.6894245743751526, 0.7831496596336365, 0.35521984100341797, -1.2101026773452759, 0.41434532403945923, -0.05581172555685043, -0.054537333548069, 0.07950247824192047, 0.6987239718437195, 0.33766284584999084, 0.6336804628372192, -0.46623480319976807, 0.8480154275894165, 0.48981332778930664, 0.6295486092567444, 0.4189363718032837, 0.2758691906929016, 0.09593335539102554, -1.4136704206466675, -0.4312968850135803, -0.4415314197540283, -1.4772924184799194, 0.49294906854629517, 0.27563872933387756, -0.6919122338294983, -0.2274627983570099, -0.17457614839076996, 1.1363639831542969, 0.5620225667953491, 0.8015514612197876, -0.13052281737327576, 0.5811322331428528, 0.05650171637535095, -0.07590392976999283, 0.05243990942835808, 0.7661066651344299, -0.25666743516921997, 0.1959146410226822, -0.43546390533447266, -0.33774998784065247, 0.26689857244491577, 0.6426823139190674, -0.6194507479667664, -0.004879681393504143, -0.28412801027297974, -0.10271624475717545, 0.19487643241882324, 1.0626012086868286, 0.5246095061302185, 0.05267104506492615, -0.14760057628154755, -0.22668054699897766, 0.5709047317504883, 0.02226623333990574, -0.3762763440608978, -0.24259372055530548, 0.35499539971351624, -0.3268785774707794, -0.4553774893283844, 0.8444411158561707, -0.5216221809387207, -0.4178372621536255, -0.7642049193382263, -0.22244270145893097, 0.5854302644729614, -0.8706511855125427, -1.1022568941116333, 0.7484657764434814, -0.18529999256134033, -0.3672468960285187, 0.4792732894420624, -0.5725815892219543, -0.2771753668785095, 0.5992527008056641, -1.4098516702651978, -0.3625371754169464, -0.10044395178556442, -0.17457973957061768, -0.3283321261405945, 0.21085532009601593, 1.125190258026123, 0.48998671770095825, -0.17280302941799164, -0.2223179191350937, -0.6511175036430359, -0.13351891934871674, 0.029901720583438873, -1.1266688108444214, 0.4928242266178131, 0.24915367364883423, -0.12393243610858917, 0.4237249493598938, -0.09074172377586365, 0.28946366906166077, -0.4106828272342682, 0.035069216042757034, 0.692034125328064, -0.6414119601249695, 0.18879234790802002, -0.8458417057991028, -0.9995007514953613, 0.3704164922237396, 0.691692590713501, -0.0799245834350586, 0.1505029946565628, -0.05852833762764931, -0.813440203666687, -0.6569914221763611, -0.36016494035720825, -0.3581353724002838, 0.44686102867126465, -0.6027243733406067, -0.20016224682331085, -0.2975747883319855, -0.09358961880207062, -0.766295313835144, -0.48798590898513794, -0.5245240926742554, 0.2934553027153015, -0.021687891334295273, 1.1879671812057495, -0.6920872926712036, 0.7496757507324219, 0.7006223201751709, 0.11972125619649887, -0.8806077241897583, -0.34422481060028076, -0.9842838048934937, 0.16719280183315277, 0.3057743012905121, 0.4106719195842743, -0.704651415348053, 0.2593253552913666, 0.6561367511749268, 0.2205687314271927, 0.049127716571092606, -0.2336389720439911, -0.003995239268988371, -0.13250769674777985, -0.37486425042152405, 0.28387221693992615, 0.26012495160102844, 0.3756594955921173, 0.16306208074092865, 0.24638299643993378, 0.2181253284215927, -0.16526150703430176, -0.6724324822425842, 0.03455139324069023, 0.049520496279001236, 0.08747684955596924, -0.7584567666053772, -0.5079954266548157, -1.247130274772644, 0.10147462040185928, -1.2439184188842773, 0.30507469177246094, -1.185410499572754, -0.2268299162387848, -0.15964829921722412, -0.3829202353954315, 0.5063585638999939, 0.19597865641117096, -0.5533291697502136, -0.21487829089164734, -0.5676104426383972, -0.6407666206359863, 0.779511570930481, 0.8332177400588989, -0.7093251347541809, 0.19238747656345367, -0.35368576645851135, -0.1728145331144333, -0.03811437636613846, 0.5313075184822083, -0.7085779309272766, -0.7449287176132202, -1.090933084487915, 0.39773982763290405, -0.05953226611018181, -0.0811379924416542, -0.4240417778491974, 1.0716172456741333, 0.08246291428804398, 0.02334544248878956, 0.32411739230155945, 0.4416254162788391, -0.9889020919799805, -0.44542601704597473, 0.6438295841217041, -1.3574022054672241, 0.25244367122650146, 0.32375210523605347, -0.7452903389930725, -0.18529504537582397, 0.8954952359199524, -0.11070819199085236, -0.9819021224975586, -0.3596344590187073, 0.5816097259521484, -0.7550314664840698, 0.42810139060020447, -0.3808647692203522, 0.024554895237088203, -1.4433540105819702, -0.4293614327907562, 0.3454184830188751, 0.4624781906604767, -0.8060034513473511, 0.8073920011520386, 0.3040047585964203, -1.1410088539123535, 0.20892077684402466, 0.4751501977443695, -0.30862918496131897, -0.0687384232878685, 0.3691962957382202, 0.2648303210735321, -0.18391910195350647, 0.49679282307624817, -0.5253080129623413, 0.23944969475269318, -0.38341784477233887, 0.270264208316803, 0.7663774490356445, -0.5734739899635315, 0.3037002682685852, 0.7607733607292175, -0.0034706173464655876, -0.6723470687866211, 0.4076836109161377, -0.9318150281906128, -0.6524423360824585, -0.2747674882411957, 0.7087929844856262, 0.42004653811454773, -0.34108516573905945, 0.13875702023506165, -0.7342158555984497, 0.30325475335121155, 0.029619382694363594, -0.278096079826355, 0.452777624130249, 0.04927784204483032, -0.34812623262405396, 0.587783932685852, 0.6064503788948059, -0.6465156674385071, -0.4976142346858978, -0.5870787501335144, -0.5763035416603088, -0.8118458390235901, 0.17085207998752594, -0.13422627747058868, -1.0247176885604858, 1.0908470153808594, 0.39383918046951294, 0.7560386061668396, 0.12985843420028687, -0.21522122621536255, -0.08461591601371765, 0.023974178358912468, -0.010886168107390404, -0.27064988017082214, -0.06588621437549591, 0.9958616495132446, 1.4271072149276733, -0.2835998833179474, 0.14567677676677704, -0.36662694811820984, -0.63546222448349, 0.856437623500824, 0.62476646900177, -0.2749142348766327, 0.5559563040733337, 0.17245565354824066, 0.12486360967159271, 0.24164772033691406, -1.1874593496322632, -0.27817168831825256, 0.34106943011283875, 1.5088876485824585, 0.5623255968093872, -0.06349772959947586, 0.16510772705078125, 0.5019347667694092, 0.41001614928245544, 0.3319357633590698, 0.5992231369018555, 0.5175503492355347, 0.010267249308526516, -0.11089000850915909, 0.003848640713840723, 0.3698440194129944, -0.4576262831687927, -0.1857650876045227, 0.1502072662115097, 0.20808927714824677, -0.07249460369348526, 0.6058772802352905, 0.9015359282493591, 0.2701980769634247, 0.714924693107605, -0.37717950344085693, 0.47858425974845886, -0.22127960622310638, -0.69589763879776, -0.28142207860946655, -0.8940837979316711, -0.5049659013748169, -0.7472819685935974, -0.2166188508272171, -0.42027732729911804, -0.4513380229473114, 0.17290739715099335, 0.30291086435317993, 0.09384863823652267, 0.6627039313316345, 0.4814656674861908, 0.8356905579566956, 0.18171803653240204, -0.5239115953445435, 0.000387941108783707, -0.8643299341201782, 0.14686568081378937, -0.1921766698360443, 0.09882629662752151, -0.38554006814956665, -0.22793082892894745, 0.098660908639431]}, "authors": [{"authorId": "2051867856", "name": "Haixu Wu"}, {"authorId": "2154707054", "name": "Jialong Wu"}, {"authorId": "2111064536", "name": "Jiehui Xu"}, {"authorId": "2144499343", "name": "Jianmin Wang"}, {"authorId": "2054275000", "name": "Mingsheng Long"}], "references": [{"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "f10d9715c1b5e2f07ef5c32fa3231358bdda94b4", "title": "You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling"}, {"paperId": "2e644c67a697073d561da4f4dad35e5ad5316cfd", "title": "SOFT: Softmax-free Transformer with Linear Complexity"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "fc46ccb83dc121c33de7ab6bdedab7d970780b2f", "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting"}, {"paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f", "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"}, {"paperId": "f864d4d2267abba15eb43db54f58286aef78292b", "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"}, {"paperId": "c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500", "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "35a9749df07a2ab97c51af4d260b095b00da7676", "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "2051548f7681c96d603de932ee23406c525276f9", "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "0272b14dd471fe7b81df703af1b71d7600b77215", "title": "Accelerating Online Reinforcement Learning with Offline Datasets"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "5e7bc93622416f14e6948a500278bfbe58cd3890", "title": "Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "a326d9f2d2d351001fece788165dbcbb524da2e4", "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2deafb11372f085d504db87fd626e478d8e965aa", "title": "ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "1d514906fcc522aa08bc05156fdca68401173edf", "title": "Unsupervised Scalable Representation Learning for Multivariate Time Series"}, {"paperId": "d8abb8206b913d185b4bd406880131c13759a6ff", "title": "The UEA multivariate time series classification archive, 2018"}, {"paperId": "0d3c46a3cbfe06cec259fec954b6ff6df6c1a566", "title": "Learning long-range spatial dependencies with horizontal gated-recurrent units"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "26bc9195c6343e4d7f434dd65b4ad67efe2be27a", "title": "XGBoost: A Scalable Tree Boosting System"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "1ac57524ba2d2a69c1bb6defed7352a06fd7050d", "title": "Using Dynamic Time Warping to Find Patterns in Time Series"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "98.8%) and Flowformer (ours, 98.8%)"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs, 2018"}, {"paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3", "title": "ALVINN, an autonomous land vehicle in a neural network"}, {"paperId": "5c65d095600d6c647426fa3bc45031b208882d5f", "title": "Batch Reinforcement Learning"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "68d54f9dacbb5416c1aafb3399c072497c320021", "title": "Network Flows: Theory, Algorithms, and Applications"}, {"paperId": null, "title": "Since the information flow is the one-way flow from values to results in attention mechanism, thus we only consider the source-sink flow as shown in Figure 1"}, {"paperId": "830ccb44084d9d6cdcb70d623df5012ae4835142", "title": "Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters"}, {"paperId": null, "title": "An autonomous land vehicle in a neural network. Technical report, Carnegie Melon Univ. Pittsburgh, PA. Arti\ufb01cial"}]}