{"paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250", "title": "Transformers learn in-context by gradient descent", "abstract": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .", "venue": "International Conference on Machine Learning", "year": 2022, "citationCount": 260, "influentialCitationCount": 30, "openAccessPdf": {"url": "http://arxiv.org/pdf/2212.07677", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "It is suggested that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations and how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks."}, "embedding": {"model": "specter_v2", "vector": [0.31030941009521484, 0.9422518610954285, -0.48733335733413696, -0.0034828600473701954, -0.1048865020275116, 0.006631636992096901, 0.942628800868988, -0.20068617165088654, -0.18656836450099945, 0.016538100317120552, 0.5121980905532837, -0.0746261328458786, 0.04538913071155548, -0.022982627153396606, -0.33208662271499634, -0.32731321454048157, -1.1872400045394897, 0.06867332011461258, 0.04960492253303528, -0.3900459408760071, 0.11945889890193939, -0.7402065992355347, -0.8555793762207031, 0.10514892637729645, 0.3238990008831024, 0.8634243607521057, 0.1910141557455063, 0.7275794744491577, -0.45543497800827026, 0.7865509390830994, 0.7156565189361572, -0.031415827572345734, 0.5661759972572327, -0.1353139728307724, -0.36671584844589233, -0.14648392796516418, 0.8485842347145081, -0.1556401401758194, -0.2954981327056885, 0.4542336165904999, -0.3664661943912506, 0.12057218700647354, 0.4200855791568756, -0.4984132647514343, -0.05830199271440506, 0.6291145086288452, 0.7985250353813171, 0.6318652033805847, -0.31821388006210327, -0.6231801509857178, 1.6680792570114136, -0.9064238667488098, 0.11699370294809341, 0.9141858220100403, 0.5755375623703003, 0.3730146586894989, -0.43544095754623413, -0.11140146851539612, 0.933494508266449, 0.21447162330150604, -0.5798255205154419, -0.1764204055070877, 0.25574252009391785, -0.15347552299499512, 1.5786432027816772, -0.10585213452577591, 0.316407173871994, 0.5672874450683594, 0.12572181224822998, 1.1659343242645264, 0.2260955572128296, -0.8940621614456177, -0.20609745383262634, 0.5737794041633606, 0.01607578434050083, 0.9746227860450745, -0.47656744718551636, 0.4215361177921295, -0.9507042169570923, 0.049691688269376755, 0.43837660551071167, 0.09367155283689499, 0.1607038825750351, -0.6090508103370667, -0.15657371282577515, 0.5658825039863586, 0.7344191074371338, 0.9240956902503967, -0.6026052832603455, 1.1947722434997559, 0.33583909273147583, 0.5887817740440369, -0.048167094588279724, 0.405030757188797, -0.08969349414110184, 0.7165470123291016, -0.6872149109840393, 0.08775175362825394, -0.13646605610847473, 0.4660843312740326, 0.26548463106155396, 0.40773487091064453, -0.2625148892402649, 0.1574740707874298, 1.3987622261047363, -0.35301533341407776, 1.1379358768463135, -0.4401460886001587, 0.4391714036464691, -0.11997181922197342, 0.046653032302856445, -0.7337144613265991, -0.4289880394935608, -0.6088541746139526, -0.6317583918571472, -0.7785603404045105, -0.4165283739566803, 0.3389299511909485, -0.2689121663570404, 0.8937336206436157, 0.1773061603307724, -0.08978161960840225, -0.11626371741294861, 0.6423981785774231, -0.043778177350759506, 0.4013865888118744, 0.15111805498600006, 0.08906291425228119, 0.5646481513977051, -0.7858255505561829, -0.4287738800048828, -0.758618175983429, 0.7757509350776672, 0.4584101140499115, 0.7411166429519653, -0.08844392001628876, -0.8045058846473694, -1.13937246799469, -1.093601942062378, 0.17678746581077576, -0.5633233785629272, -0.10973967611789703, 1.2369517087936401, 0.39042237401008606, -1.0007038116455078, 1.2350462675094604, -0.5230788588523865, -0.42796260118484497, 0.13222618401050568, 0.27402979135513306, 0.16886316239833832, 0.19262833893299103, -0.846268892288208, 0.4716508090496063, 0.2983863055706024, -0.2467835545539856, -0.6446065902709961, -0.9083070158958435, -0.8556800484657288, -0.26734933257102966, 0.50957852602005, -0.6176066398620605, 1.299940586090088, -0.5144244432449341, -1.283898115158081, 0.6998547911643982, 0.24205751717090607, -0.1226864829659462, 0.855912446975708, -0.11829435080289841, -0.4948207437992096, -0.5786392688751221, 0.017811210826039314, 0.05704771354794502, 0.3775661587715149, -0.6596316695213318, -0.6210569143295288, -0.4926115572452545, -0.39135825634002686, -0.042412564158439636, -0.319416880607605, 0.3098403215408325, -0.19836901128292084, 0.18300150334835052, 0.5007300972938538, 0.8391513228416443, -0.1568949669599533, -0.33215922117233276, -0.34599241614341736, -0.9819151759147644, 0.7189186215400696, 0.14406153559684753, 1.2555172443389893, -0.6025510430335999, -0.8826075196266174, 0.2731793522834778, 0.17895548045635223, -0.25526049733161926, -0.4015626013278961, 0.5983521938323975, -0.5776589512825012, 0.30758413672447205, -0.14556942880153656, -0.7903069257736206, -0.018034562468528748, -0.1560797095298767, -0.9590842127799988, -0.21143613755702972, 0.06538033485412598, 0.9189465641975403, -1.1151469945907593, 0.2562045753002167, 0.01568199321627617, 0.013279481790959835, -0.7062711119651794, 1.3516926765441895, -0.1928008645772934, 0.14834776520729065, -0.19818657636642456, -0.35675162076950073, -0.25714701414108276, -0.09421252459287643, 0.3888988792896271, -0.1661122888326645, -0.23171210289001465, 0.7170447111129761, -0.5190625190734863, 1.2544523477554321, -0.7942054271697998, 0.576928973197937, 0.017036916688084602, -1.0295579433441162, 0.06277600675821304, 0.19493423402309418, 0.12889419496059418, -0.5968801379203796, 0.06759607791900635, 0.25598856806755066, -0.7120940089225769, 0.08317360281944275, 0.6129063963890076, 0.8380293250083923, -0.13199694454669952, 0.16994698345661163, 0.7048936486244202, -0.5406296253204346, 0.0801289901137352, 0.30361098051071167, 0.5171481370925903, 0.9127960801124573, 0.5216500759124756, -0.09221459925174713, 0.04662108048796654, -0.8158716559410095, -0.02791391871869564, 0.6182771325111389, 0.7338945865631104, 0.6563431620597839, 0.42049071192741394, -0.9046289324760437, -0.42468661069869995, -0.08013107627630234, 0.4908536672592163, 1.3258658647537231, -0.4097510576248169, -0.39734482765197754, -0.8524259924888611, -0.46807119250297546, -0.20035849511623383, 0.47367197275161743, -1.0145412683486938, -0.37528014183044434, -0.453278511762619, -1.1913950443267822, 0.33667734265327454, 0.307807981967926, 1.3342312574386597, -0.34494447708129883, -0.17393410205841064, 0.3202848434448242, 0.58443683385849, -0.4925960898399353, -0.47652706503868103, 0.503812849521637, -0.8376515507698059, -0.29820576310157776, 0.23186209797859192, -0.08866482228040695, 0.18880948424339294, -0.6805921792984009, 0.9308664798736572, -0.15526527166366577, -0.3461282253265381, 0.3934168517589569, 0.8384508490562439, -0.7011052370071411, -0.37515488266944885, 0.5177855491638184, 0.2095145881175995, 0.10127690434455872, 0.03276906535029411, -0.05745725706219673, -0.30406513810157776, -0.17628686130046844, -0.4715392589569092, -0.0036762095987796783, -0.2165043205022812, 0.003587327664718032, 0.2669024169445038, -0.3426723778247833, -0.007066949736326933, -1.359200358390808, 1.1283715963363647, -0.1485489308834076, -0.6390660405158997, 0.17281851172447205, -1.492223858833313, -0.07544504106044769, 0.453962117433548, -0.45135098695755005, -0.04403136298060417, -0.8265664577484131, 0.6887354850769043, -0.07121725380420685, -0.3847140669822693, -0.10344363749027252, 0.23311389982700348, 0.0717732310295105, 0.8601417541503906, -0.006409473717212677, 0.12367112189531326, -0.024298520758748055, 0.5886216759681702, -0.8675999045372009, 0.7278949022293091, -0.07465308904647827, 0.15647077560424805, -0.050989944487810135, 0.2051408290863037, -0.3725363314151764, -0.6867679357528687, -0.15613073110580444, -0.0908454954624176, -0.4401218593120575, 0.19727036356925964, -0.5754125118255615, -1.0564314126968384, -0.2385302037000656, -0.18090108036994934, -1.1570637226104736, -0.18773241341114044, -0.6609768271446228, -0.5400626063346863, -0.9411296248435974, -0.6418935060501099, -0.5551459789276123, -0.723416268825531, -0.5627536773681641, 0.0855272114276886, 0.21252554655075073, -0.20983614027500153, -1.1505191326141357, -0.021995199844241142, -0.5839723348617554, 1.2169305086135864, -0.13805586099624634, 0.149070605635643, 0.1537199467420578, -0.14608962833881378, 0.22538690268993378, 0.5255627632141113, 0.43241122364997864, 0.2910538911819458, 0.06800863891839981, -0.7906234264373779, 0.6803836226463318, -0.4342218339443207, -0.30481427907943726, -0.060575321316719055, 0.024886487051844597, 0.7092396020889282, -0.250151127576828, -0.35486239194869995, 0.7832624316215515, 1.5808393955230713, -0.865336537361145, 0.1250811070203781, 0.6007947325706482, 1.0951186418533325, 0.4411807060241699, -0.17765721678733826, 0.3204907178878784, 0.44724810123443604, 0.6106762290000916, 0.36872532963752747, -0.3932170271873474, 0.04423476383090019, -0.8524851202964783, 0.3412553071975708, 0.6086906790733337, -0.005652344319969416, 0.8643237352371216, -0.8054026365280151, 0.7449808716773987, -1.4053164720535278, -0.4374200701713562, 0.5101531147956848, 0.9295156002044678, 0.6309431791305542, -0.43022364377975464, 0.24126721918582916, -0.06628189235925674, -0.11820138990879059, -0.15324197709560394, -0.2562636733055115, -0.6182509064674377, 0.2361455261707306, 0.3271019756793976, 0.49288293719291687, 0.8401275277137756, -0.27548283338546753, 0.8286327719688416, 15.22994327545166, 0.4560050070285797, -0.17559881508350372, 0.27710577845573425, 0.7002624869346619, 0.3145594596862793, -0.3318420350551605, 0.16705405712127686, -0.8658259510993958, 0.02821282111108303, 0.9578491449356079, 0.6359244585037231, 1.0062282085418701, 0.32768869400024414, 0.12877067923545837, 0.07162262499332428, -0.8313983678817749, 0.40566280484199524, 0.2799399197101593, -0.8886957168579102, -0.03058585152029991, 0.23841920495033264, 0.34279245138168335, 0.5277590751647949, 0.9393632411956787, 0.7750265598297119, 0.8231428861618042, 0.19874441623687744, 0.6349831223487854, 0.5556867718696594, 1.2080941200256348, 0.1046031266450882, -0.2051917165517807, 0.2731972336769104, -0.5234829187393188, -0.6040018200874329, -0.5400561094284058, -0.8843668103218079, -0.1455816626548767, 0.15620790421962738, -0.8345776200294495, -0.254564106464386, 0.23337441682815552, 0.25801074504852295, 0.08843386173248291, 0.05650145187973976, -0.7556875348091125, 0.7995367646217346, -0.1996213048696518, 0.32435381412506104, 0.4487454295158386, 0.04090956225991249, -0.13680481910705566, 0.2590794861316681, -0.12277370691299438, -0.022003764286637306, 0.03806716576218605, 0.5505445003509521, -0.4357617497444153, -0.15811020135879517, -0.2713218629360199, 0.2572486698627472, -0.4337329566478729, 1.1968746185302734, 0.9875518083572388, 0.06844870746135712, 0.1361803412437439, 0.25984200835227966, 0.49254459142684937, 0.21802477538585663, -0.06646749377250671, -0.5087919235229492, 0.6136024594306946, -0.12273585051298141, -0.19467653334140778, 0.47078970074653625, -0.2705681324005127, -0.45004400610923767, -0.6562983989715576, -0.320489764213562, 0.3063841760158539, -1.0697274208068848, -0.7218128442764282, 0.7129346132278442, -0.30303341150283813, -0.39680564403533936, -0.0642373114824295, -0.89255291223526, -0.4689747989177704, 0.38375142216682434, -1.7022379636764526, -0.4122355580329895, -0.24144883453845978, 0.0005075593944638968, -0.6545844078063965, -0.23387794196605682, 0.7001346945762634, -0.13985368609428406, -0.5625290274620056, 0.13764731585979462, 0.19802148640155792, -0.44320911169052124, 0.04077231511473656, -1.6921840906143188, 0.7107234001159668, -0.06133522093296051, 0.11970900744199753, 0.3217940032482147, 0.036456819623708725, 0.6596275568008423, -0.37558218836784363, 0.22993598878383636, 0.2963390350341797, -1.1260522603988647, -0.11885686963796616, -0.6985359191894531, -0.8007813692092896, 0.5513021349906921, 0.53037428855896, -0.05258643999695778, 0.1770450472831726, 0.15403762459754944, -0.5297127962112427, -0.6639810800552368, -0.7525011897087097, 0.3024025559425354, 0.719500720500946, -0.31918781995773315, -0.4540572464466095, -0.1447351574897766, 0.09153516590595245, -0.8568090200424194, -0.5833141803741455, -0.4489949345588684, -0.12373701483011246, -0.29045358300209045, 0.8819879293441772, -0.73656165599823, 0.4090890884399414, 0.6012535691261292, 0.0712282732129097, -0.8997355103492737, -0.16809600591659546, -1.2942463159561157, 0.05678195133805275, 0.3931652307510376, 0.4567214846611023, -0.7799493074417114, 0.3878481090068817, 0.9484929442405701, 0.1698702573776245, -0.6281514763832092, -0.29234129190444946, -0.13594947755336761, -0.11357936263084412, -0.48095229268074036, 0.7302843332290649, 0.36378464102745056, -0.10589780658483505, 0.06356288492679596, 0.7894307374954224, 0.41545429825782776, 0.09648462384939194, -0.8994818925857544, 0.561885416507721, -0.16649214923381805, -0.05486449971795082, -0.6209978461265564, -0.6625761389732361, -1.1259621381759644, -0.6000181436538696, -1.190962553024292, -0.07813215255737305, -1.2240818738937378, -0.841779351234436, -0.3325662910938263, -0.4387549161911011, 0.12525273859500885, 0.11282587796449661, -0.7993806004524231, -0.4009990394115448, -0.40135225653648376, -0.6990692615509033, 0.6081523895263672, 0.5357281565666199, -0.3993161916732788, -0.2185642421245575, -0.0830245167016983, -0.5028552412986755, 0.1250193566083908, 0.6391305327415466, -0.49563220143318176, -0.7303053736686707, -0.819214940071106, 0.6010830998420715, -0.43319374322891235, -0.10318758338689804, -0.8071044087409973, 0.25955089926719666, 0.2753596603870392, 0.14686797559261322, 0.5697992444038391, -0.3165604770183563, -0.9330296516418457, -0.5206291675567627, -0.14327554404735565, -0.8181955218315125, -0.02625705860555172, -0.11505191028118134, -0.5452910661697388, -0.10270922631025314, 0.4312666356563568, -0.3613262474536896, -0.9303117990493774, -0.5556578636169434, 0.33280226588249207, -0.5299209952354431, -0.02782496251165867, -0.41587579250335693, -0.3861841559410095, -0.8124889135360718, -0.1913537085056305, -0.0339738205075264, 0.5234835743904114, -0.3942743241786957, 0.7160324454307556, -0.26448994874954224, -1.293088436126709, 0.29877930879592896, 0.13510723412036896, 0.062205374240875244, 0.3485322892665863, 0.3104913830757141, 0.19247804582118988, -0.3361656069755554, 0.08722435683012009, 0.11853617429733276, 0.2518700659275055, -0.24477320909500122, 0.04169232025742531, 1.1103178262710571, -0.7381860613822937, -0.11857789009809494, 0.6718153357505798, -0.19858713448047638, -1.2791681289672852, 0.04213671013712883, -1.178376317024231, -0.577495813369751, -0.44228649139404297, 0.5055493712425232, 0.1547384411096573, 0.05380070582032204, 0.6162627935409546, -0.011974142864346504, 0.3762610852718353, -0.008957257494330406, -0.33400318026542664, 0.8043992519378662, -0.14947564899921417, -0.05598831549286842, 0.6704322099685669, 0.5387217998504639, -0.5872873663902283, -1.1372698545455933, -1.0028311014175415, -0.08357936143875122, -0.14450007677078247, 0.35863399505615234, -0.4737090468406677, -0.755580484867096, 0.6780924797058105, 0.8612258434295654, 0.3907049000263214, 0.40198731422424316, -0.023107590153813362, -0.42862245440483093, 0.8583513498306274, -0.16234228014945984, -1.073874592781067, -0.21260705590248108, 0.5714675188064575, 1.593645691871643, -0.8049364686012268, 0.6435850262641907, 0.11006753891706467, -0.6993992328643799, 0.9746279716491699, 0.4449319541454315, -0.35638436675071716, 0.6710700988769531, -0.4972211420536041, 0.31412121653556824, -0.10364869236946106, -1.0869183540344238, -0.36283445358276367, 0.5313373804092407, 1.4421319961547852, 0.5646530985832214, 0.30924949049949646, 0.44879457354545593, 0.9140925407409668, -0.18635517358779907, -0.02342934161424637, 0.4122086465358734, 0.295168399810791, -0.12751524150371552, -0.26997536420822144, -0.13407552242279053, 0.3691190481185913, -0.5802649855613708, -0.2063661813735962, 0.015943529084324837, 0.7767238616943359, 0.08275961875915527, 0.4798210859298706, 0.8065774440765381, -0.1524089127779007, 0.808219850063324, 0.07128889113664627, 0.373766154050827, -0.36902496218681335, -0.6581546068191528, -0.49226292967796326, -0.6176297664642334, -0.3524239957332611, 0.08865604549646378, -0.7997320890426636, -0.3584047257900238, 0.35895755887031555, 0.16333654522895813, -0.15148551762104034, 0.7618995904922485, 0.6701208353042603, 0.6687321662902832, 0.5699700117111206, 0.26010972261428833, -0.51197749376297, -0.7528309226036072, -1.0607181787490845, 0.3051755130290985, -0.32915064692497253, -0.10941022634506226, -0.31952565908432007, -0.6439996957778931, -0.397048681974411]}, "authors": [{"authorId": "145167136", "name": "J. Oswald"}, {"authorId": "51440033", "name": "Eyvind Niklasson"}, {"authorId": "72142084", "name": "E. Randazzo"}, {"authorId": "3105061", "name": "J. Sacramento"}, {"authorId": "2050989525", "name": "A. Mordvintsev"}, {"authorId": "3422677", "name": "A. Zhmoginov"}, {"authorId": "3316311", "name": "Max Vladymyrov"}], "references": [{"paperId": "69c85405cc1986a41f6387d869aa1648a5668d6f", "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers"}, {"paperId": "93fdf5cf598aefb0335f001039e83494dc721c3a", "title": "General-Purpose In-Context Learning by Meta-Learning Transformers"}, {"paperId": "7aa801b907b59b8ee4cfb1296d9dac22c5164c5d", "title": "What learning algorithm is in-context learning? Investigations with linear models"}, {"paperId": "42e1790c7979796634d15920b4a08990e847243e", "title": "Transformers generalize differently from information stored in context vs in weights"}, {"paperId": "c90a99eeb57019732a6cc996bb9eaf13faedf00f", "title": "In-context Learning and Induction Heads"}, {"paperId": "3ba8f78b0567f12fa944687750bf58f0e33f5894", "title": "Random initialisations performing above chance and how to find them"}, {"paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9", "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes"}, {"paperId": "72c8ee66ef05e1e6fb143522a7ca15fcb568f378", "title": "Beyond Backpropagation: Bilevel Optimization Through Implicit Differentiation and Equilibrium Propagation"}, {"paperId": "146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd", "title": "Data Distributional Properties Drive Emergent In-Context Learning in Transformers"}, {"paperId": "c3112a62284b1f7b699b5aad3adb2d837f7f4e12", "title": "HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning"}, {"paperId": "a1d1983a7b19845141e6505bd32dc395e5a136ba", "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets"}, {"paperId": "ac24be2fa900db41abf62185f91d22006cbaece7", "title": "Learning where to learn: Gradient sparsity in meta and continual learning"}, {"paperId": "4dc48bd4e1c0e5986b36eca8339bd45e944d8a82", "title": "The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61", "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "76e3ad12881e7ab1c36318d8f8818eca3f828349", "title": "Meta Learning Backpropagation And Improving It"}, {"paperId": "acbd97dbb88658aaa0f88499d1e207ea51962871", "title": "Meta-Learning via Hypernetworks"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5", "title": "Hopfield Networks is All You Need"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "bd17620c6cb5ca97ef773499223d1509d123745f", "title": "Dive into Deep Learning"}, {"paperId": "aa63ac11aa9dcaa9edd4c88db18bec87e0834328", "title": "Graph Transformer Networks"}, {"paperId": "abf5478c24664a1380b7e213a3ab1c4af54775d0", "title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML"}, {"paperId": "3e411d95097a12e63cc4812ab4c3fdaba50df85e", "title": "Deep Declarative Networks"}, {"paperId": "9a618cca0d2fc78db1be1aed70517401cb3f3859", "title": "Deep Equilibrium Models"}, {"paperId": "e13ca1bd60af3325afc64dc09979e3322818e365", "title": "Meta-Learning with Warped Gradient Descent"}, {"paperId": "7ee12d3bf8e0ce20d281b4550e39a1ee53839452", "title": "Risks from Learned Optimization in Advanced Machine Learning Systems"}, {"paperId": "fc437af6204008647ea49f81058d5fdaddf75ead", "title": "Meta-Learning With Differentiable Convex Optimization"}, {"paperId": "770538e506051da89f57f48062db8f374ebf2334", "title": "Meta-Curvature"}, {"paperId": "04f739a0c29b75877243731aeead512bf0ed1dff", "title": "Meta-Learning with Latent Embedding Optimization"}, {"paperId": "7b0aad12a6917b7d444ba2f87c7f8ccc5357797a", "title": "Meta-Learning Probabilistic Inference for Prediction"}, {"paperId": "208cd4b25768f0096fb2e80e7690473da0e2a563", "title": "Meta-learning with differentiable closed-form solvers"}, {"paperId": "d4c4a5f0c71eba13d2827b24f70bbfdc3bd858ec", "title": "Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace"}, {"paperId": "2bdebf2fb0f5c21907fcaae6d87c7ba5811e778a", "title": "Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm"}, {"paperId": "d33ad6a25264ba1747d8c93f6621c7f90a7ec601", "title": "Meta-SGD: Learning to Learn Quickly for Few Shot Learning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"paperId": "0076b232181e4e5be58dce8354a813ad2bbf663a", "title": "OptNet: Differentiable Optimization as a Layer in Neural Networks"}, {"paperId": "c91ae35dbcb6d479580ecd235eabf98374acdb55", "title": "Using Fast Weights to Attend to the Recent Past"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "71683e224ab91617950956b5005ed0439a733a71", "title": "Learning to learn by gradient descent by gradient descent"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "bc22e87a26d020215afe91c751e5bdaddd8e4922", "title": "Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"}, {"paperId": "d130325c41947a41a55a4431e9e8e15be89da8ea", "title": "Learning a synaptic learning rule"}, {"paperId": "44b6da0cd36c0fa2f7d3485c6dc0b6d2fbe379bb", "title": "Learning how to learn"}, {"paperId": "85d346297ec37a451d4500afad53fed8f281af92", "title": "Adaptive Switching Circuits"}, {"paperId": "74fd7e2750614b8b405a9e45d639331c3ed9d811", "title": "The Evolution of Learning: An Experiment in Genetic Connectionism"}, {"paperId": "7257eacd80458e70c74494eb1b6759b52ff21399", "title": "Using fast weights to deblur old memories"}, {"paperId": "bdaec1b3eb9a8f7a2b296be009a148c35236f3ce", "title": "Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-. hook"}, {"paperId": "14821ac1bf09890a857fca2a6c324e8c85f2c0d0", "title": "Smooth regression analysis"}, {"paperId": "05175204318c3c01e3301fd864553071039605d2", "title": "On Estimating Regression"}, {"paperId": "a6bcf0e9b5034c4c9cbea839baadd69a42b05cc1", "title": "Learning curves for stochastic gradient descent in linear feedforward networks"}]}