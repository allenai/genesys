{"paperId": "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2", "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks", "abstract": "Attention mechanisms, primarily designed to capture pairwise correlations between words, have become the backbone of machine learning, expanding beyond natural language processing into other domains. This growth in adaptation comes at the cost of prohibitively large memory requirements and computational complexity, especially at higher number of input elements. This limitation is due to inherently limited data reuse opportunities and quadratic growth in memory footprints, leading to severe memory-boundedness and limited scalability of input elements. This work addresses these challenges by devising a tailored dataflow optimization, called FLAT, for attention mechanisms without altering their functionality. This dataflow processes costly attention operations through a unique fusion mechanism, transforming the memory footprint quadratic growth to merely a linear one. To realize the full potential of this bespoke mechanism, we propose a tiling approach to enhance the data reuse across attention operations. Our method both mitigates the off-chip bandwidth bottleneck as well as reduces the on-chip memory requirement. FLAT delivers 1.94x (1.76x) speedup and 49% and (42%) of energy savings compared to the state-of-the-art Edge (Cloud) accelerators with no customized dataflow optimization. When on-chip resources are scarce (20 KB-200 KB), FLAT yields, on average, 1.5x end-to-end latency reduction across a diverse range of conventional attention-based models with input sequence lengths ranging from 512-token to 64K-token. Our evaluations demonstrate that state-of-the-art DNN dataflow applied to attention operations reach the efficiency limit for inputs above 512 elements. In contrast, FLAT unblocks transformer models for inputs with up to 64K elements.", "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems", "year": 2021, "citationCount": 24, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://dl.acm.org/doi/pdf/10.1145/3575693.3575747", "status": "BRONZE"}, "tldr": {"model": "tldr@v2.0.0", "text": "These evaluations demonstrate that state-of-the-art DNN dataflow applied to attention operations reach the efficiency limit for inputs above 512 elements, and propose a tiling approach to enhance the data reuse across attention operations."}, "embedding": {"model": "specter_v2", "vector": [0.35779857635498047, 0.29588326811790466, -0.38776490092277527, 0.045124977827072144, -0.019735140725970268, 0.06427125632762909, 0.39635366201400757, 0.21376165747642517, -0.6644214987754822, -0.44398102164268494, 0.5226650834083557, -0.052831701934337616, 0.6273929476737976, -0.09384964406490326, -0.06624731421470642, 0.39930713176727295, -0.7521217465400696, 0.18151825666427612, 0.03348218649625778, -0.28798750042915344, 0.11739648878574371, -0.5021189451217651, -1.5322189331054688, 0.24693159759044647, 0.20916512608528137, 0.8207108378410339, 0.5041750073432922, 0.8615656495094299, -0.6703893542289734, 0.4331885874271393, 0.6828235983848572, -0.23265494406223297, 0.06933142989873886, 0.22123028337955475, -0.425811231136322, -0.5547046065330505, 0.4578574299812317, -0.46033650636672974, -0.2969920337200165, 0.8280331492424011, -0.2835923731327057, 0.308391809463501, -0.010242699645459652, -0.6559114456176758, -0.33835047483444214, 1.072601079940796, 0.5755863785743713, 0.7682772278785706, -0.6332066059112549, -0.5509117245674133, 1.456259846687317, -1.5968416929244995, -0.07028765231370926, 1.4169474840164185, 0.23044079542160034, 0.3183697760105133, -0.04734724387526512, -0.45773330330848694, 0.773883044719696, 0.1763886660337448, -0.49430087208747864, -0.6715911030769348, -0.18183933198451996, 0.06205740571022034, 2.083991289138794, -0.2764453589916229, 0.26385298371315, 0.08056677877902985, 0.2880910634994507, 1.2459980249404907, -0.31152552366256714, -0.9912147521972656, 0.07400801032781601, -0.4510710835456848, 0.7154298424720764, 0.6912395358085632, 0.004540988244116306, -0.19541360437870026, -1.0030455589294434, 0.05227935314178467, 0.6683456301689148, 0.5745766758918762, 0.6387966871261597, 0.016350138932466507, -0.0245865099132061, 0.4515855312347412, 0.4385601878166199, 0.7381402254104614, -0.5091005563735962, 0.8292388916015625, 0.7073016166687012, 0.04329065978527069, -0.0977986752986908, 0.14248768985271454, 0.04296204820275307, 0.2991122901439667, -0.7772980332374573, 0.10184693336486816, -0.13696123659610748, 1.1261365413665771, -0.03474194183945656, 0.6014562845230103, -0.7640271186828613, -0.008237677626311779, 1.0690784454345703, 0.24094954133033752, 0.06770793348550797, -0.3094087243080139, 0.23294171690940857, -0.732576310634613, -0.18471185863018036, -0.37880924344062805, 0.05352342501282692, -0.07168629765510559, -0.8203736543655396, -1.1627970933914185, -0.5983537435531616, 0.3297562003135681, -0.8891669511795044, 0.47463563084602356, -0.6912258863449097, 0.07744280248880386, -0.055256836116313934, 0.28510385751724243, 0.49801376461982727, 0.21776776015758514, 0.460273414850235, 0.30144375562667847, 1.456007480621338, -1.1631886959075928, -0.5969251990318298, -1.3282941579818726, 0.4262178838253021, -0.5630311965942383, 0.17253270745277405, -0.46754688024520874, -1.502531886100769, -0.9320946335792542, -0.6078561544418335, -0.1851203292608261, -0.5607010126113892, -0.2066541314125061, 1.0193276405334473, 0.26079434156417847, -1.3080421686172485, 0.5129119157791138, -0.21413996815681458, -0.17809878289699554, 0.5018914341926575, 0.3562542796134949, 0.5375874638557434, -0.23902364075183868, -1.3722678422927856, 0.2507004737854004, -0.011311136186122894, -0.580524742603302, -0.23899748921394348, -0.7321889400482178, -1.0480222702026367, 0.4298495650291443, -0.03505494073033333, -0.5509418845176697, 1.4349902868270874, -0.40972092747688293, -1.0334616899490356, 0.4098825752735138, -0.48908793926239014, -0.04274511709809303, -0.4670545756816864, -0.485176146030426, -0.5988709330558777, -0.41924571990966797, -0.39144349098205566, 0.5885909199714661, 0.7078865766525269, 0.2964093089103699, -0.24438484013080597, 0.22440296411514282, -0.24914774298667908, -0.2028055191040039, -0.6712560653686523, 1.2120753526687622, -0.5256950259208679, -0.26462817192077637, 0.11423549056053162, 0.6573135852813721, -0.09938431531190872, -0.40877851843833923, -0.24359361827373505, -0.7147582769393921, 0.7394282817840576, 0.2814234793186188, 1.22748863697052, -0.9463902115821838, -1.2751634120941162, -0.1478571742773056, -0.022105125710368156, 0.2069702297449112, -0.6604645252227783, 0.45128312706947327, -0.5724378228187561, 0.251991331577301, 0.3865583837032318, -0.8438116908073425, -0.11734746396541595, -0.4750309884548187, -0.8404370546340942, -0.5623104572296143, 0.23618477582931519, 1.1132092475891113, -0.758836030960083, 0.09419697523117065, -0.12649023532867432, 0.46996793150901794, -1.2404357194900513, 1.2408963441848755, -0.1709449738264084, -0.4121839106082916, -0.06716804206371307, 0.01608983986079693, 0.23765477538108826, -0.30299216508865356, 0.6057260632514954, -0.7114186882972717, -0.2355802357196808, 0.642134964466095, -0.07021497189998627, 1.3296146392822266, -0.6236251592636108, 0.517871081829071, 0.13052116334438324, -0.35850363969802856, 0.48233240842819214, 0.25228118896484375, -0.23000603914260864, -0.6494737863540649, 0.5808817148208618, 0.2392578423023224, -0.3720799386501312, 0.4012528955936432, 1.43889319896698, 1.2459564208984375, -0.5920946002006531, 0.23078344762325287, 0.6941505670547485, 0.007716840133070946, 0.4231167137622833, 0.2937493920326233, 0.8940312266349792, 0.032688550651073456, 0.48845911026000977, -0.5376012325286865, 0.5203268527984619, -1.0481109619140625, -0.12104163318872452, 0.36303380131721497, 0.5173147916793823, 0.5011615753173828, 0.231809601187706, -0.80507493019104, -0.35202986001968384, 0.1913965940475464, 0.7922179698944092, 1.898962378501892, -0.056413162499666214, 0.07700886577367783, -0.9225320816040039, -0.26946550607681274, -0.49965983629226685, -0.03779960796236992, 0.0593390092253685, 0.07389568537473679, -0.6363785266876221, -0.6244638562202454, 0.8162118196487427, 0.7024422883987427, 0.7515593767166138, -0.8782262206077576, -0.5964908599853516, -0.3914121985435486, 0.2537238597869873, -1.0123265981674194, -0.9949983954429626, 0.5302169919013977, -0.508010745048523, 0.26435181498527527, 0.43921801447868347, -0.2010551393032074, 0.4645043909549713, -0.31818264722824097, 1.16402006149292, -0.7436704635620117, -0.42432138323783875, -0.060791920870542526, 0.5089949369430542, -0.6286707520484924, -0.4030933380126953, 0.4213838279247284, 0.0066433800384402275, -0.38373976945877075, 0.8057411909103394, 0.40431898832321167, -0.10107215493917465, -0.08429181575775146, -0.37122783064842224, -0.028920704498887062, 0.32236820459365845, -0.08323463052511215, 0.7218466997146606, -0.8483075499534607, 0.0997111052274704, -0.95893394947052, 0.7880146503448486, 0.07496093958616257, -0.3375113606452942, 0.010174442082643509, -0.5698042511940002, -0.06714612990617752, 0.671002209186554, -0.33374395966529846, -0.4110448360443115, -0.7845967411994934, 0.46150505542755127, -0.6510927081108093, -0.20212721824645996, -0.10457155853509903, 0.25989821553230286, 0.16583429276943207, -0.22780601680278778, 0.45448827743530273, 0.11777658015489578, 0.09134163707494736, 0.5631215572357178, -0.6088858842849731, 0.4987722635269165, 0.07411070168018341, -0.40148913860321045, -0.11908342689275742, -0.15951725840568542, -0.6114640235900879, 0.05721953883767128, -0.2388792634010315, -0.0895623192191124, -0.2857266068458557, 0.13980095088481903, -0.6569363474845886, -1.106885313987732, -0.20662273466587067, -1.5257163047790527, 0.16242404282093048, -0.00122936035040766, -0.13242805004119873, 0.01667708158493042, -1.3956259489059448, -1.1822938919067383, -0.6747161746025085, -1.282625675201416, -1.0179047584533691, 0.0734637975692749, 0.3620480000972748, -0.5913902521133423, -0.4993811845779419, -0.23550109565258026, -0.6479563117027283, 1.1469001770019531, -0.9022343158721924, 0.8350788354873657, -0.4141324758529663, -0.5220329761505127, -0.08726639300584793, -0.05668843165040016, 0.1964682638645172, -0.1243368610739708, 0.23776036500930786, -1.2386116981506348, 0.3463192880153656, -0.5225412845611572, -0.06520721316337585, 0.3339459002017975, 0.3695870041847229, 0.9274773001670837, 0.12968440353870392, -0.36938127875328064, 0.1548212468624115, 1.539636492729187, -0.4371092915534973, 0.1372310072183609, -0.21093954145908356, 1.0115374326705933, -0.031134510412812233, -0.4986569583415985, 0.5785213112831116, 0.25799354910850525, 0.4244913160800934, 0.40161529183387756, -0.1275739073753357, -0.22589819133281708, -0.35046008229255676, 0.5096854567527771, 1.8440511226654053, 0.19404077529907227, -0.22213782370090485, -0.9051597118377686, 0.9076822996139526, -1.1945931911468506, -0.5929761528968811, 0.5874369144439697, 0.9126395583152771, 0.08094532042741776, -0.20673537254333496, -0.44761714339256287, -0.23968550562858582, 0.8732296228408813, 0.4358111619949341, -0.29120177030563354, -1.6683927774429321, 0.1783655732870102, 0.8382822275161743, 0.5994896292686462, 0.6619857549667358, -0.215312197804451, 0.590234637260437, 14.588532447814941, 0.7317383885383606, -0.1853051781654358, 0.532402753829956, 0.6488497853279114, 0.026361513882875443, -0.6381213665008545, -0.13348011672496796, -1.5050944089889526, -0.13697245717048645, 1.3197333812713623, -0.055482104420661926, 0.4694203734397888, 0.23287048935890198, -0.021707404404878616, 0.2791427671909332, -0.6701973080635071, 0.4833264946937561, 0.6318278312683105, -1.2655094861984253, 0.47395992279052734, -0.009400877170264721, 0.17561592161655426, 0.45436593890190125, 0.7300736308097839, 0.6594492197036743, 0.7964670658111572, -0.420777291059494, 0.38517943024635315, 0.29440438747406006, 0.9353700280189514, -0.16578857600688934, 0.2615680992603302, 0.005775372963398695, -1.3247356414794922, -0.08541997522115707, -0.28811463713645935, -1.3732080459594727, 0.025112586095929146, 0.4336232841014862, -0.6500567197799683, -0.4408435821533203, -0.25077012181282043, 0.48046302795410156, 0.6503283977508545, 0.07803826034069061, -0.1355578899383545, 0.11920785903930664, 0.01527345646172762, -0.23149816691875458, 0.011346298269927502, 0.8214328289031982, -0.1155429258942604, 0.46466267108917236, -0.08522313088178635, -0.3765096068382263, -0.0023241271264851093, 0.5560848116874695, -0.7244917154312134, -0.2443237453699112, -0.2358771562576294, -0.2658029794692993, 0.276973158121109, 1.0497785806655884, 0.22741004824638367, 0.4832476079463959, -0.8667672276496887, 0.1572849601507187, 0.5159919857978821, -0.03792324289679527, -0.3800153136253357, -0.24435731768608093, 0.7257378697395325, -0.4039553105831146, 0.23780615627765656, 0.747614324092865, -0.4647364020347595, -0.21763209998607635, -0.8981156349182129, -0.19174045324325562, 0.3161788880825043, -0.7305778861045837, -0.5687247514724731, 1.0310027599334717, -0.323815256357193, -0.19643522799015045, 0.3274747431278229, -0.888918399810791, -0.49449658393859863, 0.6983909010887146, -1.2862054109573364, -0.6732894778251648, 0.2063070833683014, -0.21211417019367218, -0.3841855525970459, 0.22089150547981262, 1.353218674659729, 0.48860684037208557, -0.29331541061401367, 0.25174373388290405, -0.1881101131439209, 0.07622134685516357, -0.10227426886558533, -0.3569069504737854, 1.1007091999053955, 0.45984137058258057, -0.31890377402305603, 0.19617505371570587, -0.12591594457626343, 0.312160462141037, -0.9427547454833984, -0.3024175763130188, 1.1066439151763916, -0.2580142021179199, -0.3878060281276703, -0.7985479831695557, -0.5672525763511658, 0.411916047334671, 0.709973156452179, 0.09113039821386337, 0.3929494321346283, 0.1871497482061386, -0.6465597748756409, -0.4244699478149414, -0.5072718858718872, 0.29392334818840027, 0.4188832640647888, -0.9979302883148193, -0.07449636608362198, -0.5991007685661316, 0.46090230345726013, -0.8969627618789673, -0.512230396270752, -0.38221779465675354, 0.21704018115997314, -0.016029726713895798, 0.9652935266494751, -0.321092814207077, 1.039600133895874, 0.930670976638794, 0.03669467568397522, -0.566641092300415, -0.10123122483491898, -0.8520936369895935, -0.06008688732981682, 0.18714360892772675, 0.41767820715904236, -0.3396196961402893, 0.26564282178878784, 0.9575961828231812, 0.11863133311271667, -0.22213692963123322, -0.5668001770973206, 0.017439302057027817, -0.14551176130771637, -0.7372302412986755, 0.6931832432746887, -0.11519161611795425, 0.39893874526023865, 0.06971436738967896, 0.6499159336090088, 0.21602289378643036, -0.3051096200942993, -0.5469411015510559, -0.01259016152471304, -0.054824717342853546, 0.1348709762096405, -0.7508707642555237, -0.45574888586997986, -1.1871802806854248, 0.09520542621612549, -0.9953868985176086, 0.12822186946868896, -0.4692568778991699, -0.39345985651016235, 0.10569906234741211, -0.5719757676124573, 0.293054461479187, 0.1669251173734665, -0.5206601023674011, -0.33118483424186707, -0.543463945388794, -0.4668172597885132, 0.5918521285057068, 0.7735956907272339, -0.476144939661026, 0.22640596330165863, -0.4757939279079437, -0.08826950192451477, 0.30766481161117554, 0.48405933380126953, -0.194332554936409, -0.6287139058113098, -1.6293855905532837, 0.21464504301548004, -0.1566336750984192, -0.1975529044866562, -0.6750574111938477, 1.3644722700119019, 0.2925678491592407, -0.44059550762176514, 0.03434544801712036, 0.06191316619515419, -0.9476539492607117, -0.7664794325828552, 0.4850880801677704, -0.74142986536026, 0.5105764269828796, 0.9111722111701965, -0.7933153510093689, -0.31525862216949463, 0.8534188866615295, -0.21968169510364532, -0.747905433177948, -1.0757718086242676, 0.540454626083374, -0.6217600107192993, 0.32714810967445374, -0.15765593945980072, -0.0077767400071024895, -1.4033256769180298, -0.135268434882164, 0.2557078003883362, 0.028280576691031456, -0.6734942197799683, 0.9514088034629822, 0.5350242853164673, -1.0003650188446045, -0.026117751374840736, 0.7497125864028931, -0.3631700575351715, 0.1912086457014084, 0.4189766049385071, 0.5001644492149353, -0.31797319650650024, 0.9807401895523071, -0.2891244888305664, 0.41056543588638306, -1.0924674272537231, 0.10879041254520416, 0.39266929030418396, -0.6432999968528748, 0.29945117235183716, 1.0932724475860596, -0.2990918755531311, -0.577702522277832, -0.2364465445280075, -1.4625085592269897, -0.4814615249633789, -0.27335089445114136, 0.5908560156822205, 0.24018166959285736, 0.44329842925071716, 0.036540437489748, -0.8069375157356262, -0.18929901719093323, -0.42064064741134644, -0.21336238086223602, 0.10420069098472595, 0.06265350431203842, -0.4124971330165863, 0.14274802803993225, 0.4355277121067047, -0.6134680509567261, -0.40254464745521545, -0.8685280680656433, -0.4759793281555176, 0.01258246973156929, 0.6417143940925598, 0.19984304904937744, -0.9455636739730835, 0.8609593510627747, 0.292941689491272, 0.4316752552986145, 0.3160388171672821, -0.49584057927131653, 0.2131875604391098, 0.20434021949768066, 0.14753690361976624, -0.4827686548233032, -0.789332926273346, 1.7199504375457764, 0.9035603404045105, -0.30077308416366577, 0.030027322471141815, -0.3818361163139343, -0.4748077094554901, 0.9151660799980164, 0.6008778214454651, -0.0844949334859848, 0.9998003840446472, 0.7825005650520325, -0.19115988910198212, 0.5523278713226318, -1.1778351068496704, -0.31464532017707825, 0.3550313413143158, 0.80827397108078, 1.0580613613128662, 0.3058786392211914, 0.05166862905025482, 1.0100681781768799, 0.5681057572364807, 0.40133291482925415, 0.18151237070560455, 0.5053309798240662, -0.1423095017671585, -0.17712019383907318, -0.14850136637687683, 0.48360970616340637, -0.7008873224258423, -1.3166863918304443, 0.40399599075317383, 0.3330268859863281, 0.23856225609779358, 0.35423770546913147, 1.5872420072555542, -0.08105094730854034, 0.5540394186973572, 0.060834381729364395, 0.3950214087963104, -0.4783150553703308, -0.6347363591194153, -0.29218438267707825, -0.702817440032959, -0.07493628561496735, -0.0037493763957172632, -0.2945564389228821, -0.6427299976348877, -0.8228896260261536, 0.37438443303108215, -0.23587098717689514, 0.3367586135864258, 0.8850314021110535, 0.9548811316490173, 1.083142876625061, -0.4503435790538788, -0.4712592363357544, 0.14751537144184113, -0.8215671181678772, 0.22720836102962494, -0.5813192129135132, -0.38447898626327515, 0.305719792842865, 0.24687935411930084, -0.10106085985898972]}, "authors": [{"authorId": "27057088", "name": "Sheng-Chun Kao"}, {"authorId": "1929462", "name": "Suvinay Subramanian"}, {"authorId": "1839673849", "name": "Gaurav Agrawal"}, {"authorId": "2112229", "name": "A. Yazdanbakhsh"}, {"authorId": "145984583", "name": "T. Krishna"}], "references": [{"paperId": "f841f3d912be52a621aab1a979632e9daeab6599", "title": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation"}, {"paperId": "bbd5eb0924ec07a83dbc99151a09f463300c0001", "title": "Accelerating attention through gradient-based learned runtime pruning"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "d352df83b48a221867b2bcd42e72f6491b6445d9", "title": "Data-Driven Offline Optimization For Architecting Hardware Accelerators"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "a29ac37db6b87c198f2f8a29e46a715c4a96e92d", "title": "SM6: A 16nm System-on-Chip for Accurate and Noise-Robust Attention-Based NLP Applications : The 33rd Hot Chips Symposium \u2013 August 22-24, 2021"}, {"paperId": "1a883522f3c0051d70be1f8cbdb8989a77395006", "title": "Long-Short Transformer: Efficient Transformers for Language and Vision"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "3aef285d724db47d1218c09a0cb7c25573828c4b", "title": "CoSA: Scheduling by Constrained Optimization for Spatial Accelerators"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "2405a2046c9e00d601d3f5451e43f1afd9889983", "title": "Understanding the Design-Space of Sparse/Dense Multiphase GNN dataflows on Spatial Accelerators"}, {"paperId": "d051b4e5f7e400b6c3fdd05945e6aabf6b33cae1", "title": "Mind mappings: enabling efficient algorithm-accelerator mapping space search"}, {"paperId": "765ae4aee20af83b39c35cf81626bb85948552f1", "title": "An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks"}, {"paperId": "61a0bc1c9ba2783b99cd64f74b58663a82ad2818", "title": "GCNAX: A Flexible and Energy-efficient Accelerator for Graph Convolutional Neural Networks"}, {"paperId": "eb0931c39904a40c6cb4aa35c9b21d5e3b7dc856", "title": "Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "1020da59ab3db2b3051fb558559d7fdcd2c7e57b", "title": "TransTrack: Multiple-Object Tracking with Transformer"}, {"paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c", "title": "Taming Transformers for High-Resolution Image Synthesis"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "78211429633a99c5ca1b66ba0a7bc6def79c7f40", "title": "GAMMA: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "2b2056bf5763e32811a69769fa8c223160125f9e", "title": "DNNFusion: accelerating deep neural networks execution with advanced operator fusion"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "1cc730bb43da33f910a90ff1d9da9890f114baa7", "title": "Data Orchestration in Deep Learning Accelerators"}, {"paperId": "c50e20fd223807a6a692103bcd5530f0388c15e7", "title": "GRIP: A Graph Neural Network Accelerator Architecture"}, {"paperId": "aa81b7c9aea67e666b2c1261f4a0b161976a6bcf", "title": "ZigZag: A Memory-Centric Rapid DNN Accelerator Design Space Exploration Framework"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "3836ccb33191799e748e8e96f85a813eaf650ff8", "title": "Data Movement Is All You Need: A Case Study on Optimizing Transformers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "ab5f0004c5f3317689e8457e1c8d8390ccbee522", "title": "A domain-specific supercomputer for training deep neural networks"}, {"paperId": "97bac618fc866ae7656660f3965e9aae37993232", "title": "Efficient Processing of Deep Neural Networks"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "ecdddb58d2d98e2814b6b7a9cc5ded2b547236d6", "title": "A Multi-Neural Network Acceleration Architecture"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "fcf1b4473a0af1f3ebc0fd556ee30c9309ff6345", "title": "SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f3058ac35927720d2a229984b10524e36d87d7dc", "title": "HyGCN: A GCN Accelerator with Hybrid Architecture"}, {"paperId": "069e0d896da7c79faeee4cf057548d5da7ce885e", "title": "FlauBERT: Unsupervised Language Model Pre-training for French"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "8051e77dd37ba276b10a53da5b130d6b755c68c3", "title": "Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "8efd76fafff43a1b420f8cda092a8bab8d545732", "title": "Simba: Scaling Deep-Learning Inference with Multi-Chip-Module-Based Architecture"}, {"paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "title": "Structured Pruning of Large Language Models"}, {"paperId": "de292ff5582e6b0c7ade02b8664176137b25979a", "title": "dMazeRunner"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "540f074cb6f16563a357741837e41c44c0a38234", "title": "Reweighted Proximal Pruning for Large-Scale Language Representation"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "85ad7241d619ff924e61037cd3a769614641184e", "title": "DeepTools: Compiler and Execution Runtime Extensions for RaPiD AI Accelerator"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "b1051e81e527d841f0936c604aa6966c719e876d", "title": "TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators"}, {"paperId": "9ded246119861dd325e7004b2050bba310e08797", "title": "Timeloop: A Systematic Approach to DNN Accelerator Evaluation"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "cc14e2e99ef12b01ecb1e869b46b9eb50e2179bd", "title": "HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array"}, {"paperId": "2b4696bf4bc923a139e8508086f854ca52b690d0", "title": "UniProt: a worldwide hub of protein knowledge"}, {"paperId": "b4e1264d08d976fe03ede65f34027107e9191c6e", "title": "TGPA: Tile-Grained Pipeline Architecture for Low Latency CNN Inference"}, {"paperId": "cc4f98003151ff13bd4c875a5212096f1d24a9ae", "title": "SCALE-Sim: Systolic CNN Accelerator"}, {"paperId": "fb507ada871d1e8c29e376dbf7b7879689aa89f9", "title": "Music Transformer: Generating Music with Long-Term Structure"}, {"paperId": "117fd3a77f887f827e7f3521964b51eb788d33c5", "title": "Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "0682bfa5cca15726aab6c00ecfac91eb44379626", "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices"}, {"paperId": "cb91c2f8d3cac0b655a39be318b603334eb18987", "title": "Learning to Optimize Tensor Programs"}, {"paperId": "ee6136c6af582d1e71c126cfa39eaac799c2b7a5", "title": "GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial Networks"}, {"paperId": "1f0bbcbcea15b60b39012e9aedf4dac42dff9411", "title": "Understanding Reuse, Performance, and Hardware Cost of DNN Dataflow: A Data-Centric Approach"}, {"paperId": "e52ee298889fb135101e5ad219692f03d5f01857", "title": "MAESTRO: An Open-source Infrastructure for Modeling Dataflows within Deep Learning Accelerators"}, {"paperId": "8c7310477fd027193cd040288f0aa9824c80b91f", "title": "Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code"}, {"paperId": "0ef460c47377c3b9482d8177cbcafad1730a91a5", "title": "Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling"}, {"paperId": "5f0da3cedda449b72fe36fa78798651a038f515c", "title": "MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "c2e1139691c3a337831e36ee7afeab8817ab5d48", "title": "The tensor algebra compiler"}, {"paperId": "dbef80d8c7784618159a2e281c6eacdb16716dca", "title": "SCALEDEEP: A scalable compute architecture for learning and evaluating deep networks"}, {"paperId": "b02b8096567addbe78ab3e39d8dd8f105d428eb0", "title": "Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "733a765e1f54eb86dbaabe03d7ba66d72363f665", "title": "TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory"}, {"paperId": "ce6403e99465e5e8a48d5c2017fc23976e29fe59", "title": "FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks"}, {"paperId": "72ed74f00d0f7312f7ed96d93ed43f0052d526bc", "title": "Fused-layer CNN accelerators"}, {"paperId": "ffdaa12ef011de9dbf43be46d45a3abcc8288965", "title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "3468a27c2e3019e1216ee9fe8bbf1ed3a0155ff4", "title": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks"}, {"paperId": "4d23db55e6671a82c95dacec33b2967a4b8b677d", "title": "Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines"}, {"paperId": "e8294122747a16df4f566c39c2ed6d629bd8af74", "title": "Improving effective bandwidth through compiler enhancement of global cache reuse"}, {"paperId": "4f39adb2021bc54d7a15fc5a00056545b85a217d", "title": "Maximizing Loop Parallelism and Improving Data Locality via Loop Fusion and Distribution"}, {"paperId": "d766378b3fc29efa46931cc8411a0c21faa14d91", "title": "Vector Register Allocation"}, {"paperId": "623d38e327221c52579b906adc21b2a3a706b651", "title": "Collective Loop Fusion for Array Contraction"}, {"paperId": "c1605344b3ecc3243046662aa46a2251a1b0867f", "title": "Optimizing supercompilers for supercomputers"}, {"paperId": null, "title": "Flat: An Optimized Dataflow for Mitigating Attention Bottlenecks ASPLOS '23"}, {"paperId": null, "title": "Tushar Krishna for ! ! =[! \" , ! \" + \" # ): for \u210e ! =[\u210e \" , \u210e \" + $ # ): for % ! =[%"}, {"paperId": null, "title": "Self-attention based context-aware 3d object detection"}, {"paperId": null, "title": "Boosting NVIDIAMLPerf Training v1.1 Performance with Full Stack Optimization. https://tinyurl.com/3dku474c"}, {"paperId": "08ee34a64247c0fe3c22b9f3c0848eb921041a8d", "title": "Supplementary Material: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "ec2de959c0dfaa4e65fd5baed88bbc5b0e99d28f", "title": "Mind Mappings: Enabling Ef\ufb01cient Algorithm-Accelerator Mapping Space Search Extended"}, {"paperId": null, "title": "V100 gpu architecture. Online verf\u00fcgbar unter http://images"}, {"paperId": null, "title": "Nvidia T4 Tensor Core GPU"}, {"paperId": "6a630ac89d7c0a57eb7bf4cb30dd5946bcf3ccce", "title": "google,\u6211,\u8428\u5a1c"}]}