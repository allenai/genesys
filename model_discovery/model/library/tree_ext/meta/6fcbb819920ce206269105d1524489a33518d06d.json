{"paperId": "6fcbb819920ce206269105d1524489a33518d06d", "title": "Recovering Private Text in Federated Learning of Language Models", "abstract": "Federated learning allows distributed users to collaboratively train a model while keeping each user's data private. Recently, a growing body of work has demonstrated that an eavesdropping attacker can effectively recover image data from gradients transmitted during federated learning. However, little progress has been made in recovering text data. In this paper, we present a novel attack method FILM for federated learning of language models (LMs). For the first time, we show the feasibility of recovering text from large batch sizes of up to 128 sentences. Unlike image-recovery methods that are optimized to match gradients, we take a distinct approach that first identifies a set of words from gradients and then directly reconstructs sentences based on beam search and a prior-based reordering strategy. We conduct the FILM attack on several large-scale datasets and show that it can successfully reconstruct single sentences with high fidelity for large batch sizes and even multiple sentences if applied iteratively. We evaluate three defense methods: gradient pruning, DPSGD, and a simple approach to freeze word embeddings that we propose. We show that both gradient pruning and DPSGD lead to a significant drop in utility. However, if we fine-tune a public pre-trained LM on private text without updating word embeddings, it can effectively defend the attack with minimal data utility loss. Together, we hope that our results can encourage the community to rethink the privacy concerns of LM training and its standard practices in the future.", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 51, "influentialCitationCount": 7, "openAccessPdf": {"url": "http://arxiv.org/pdf/2205.08514", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This paper presents a novel attack method FILM for federated learning of language models (LMs) and shows the feasibility of recovering text from large batch sizes of up to 128 sentences, and evaluates three defense methods: gradient pruning, DPSGD, and a simple approach to freeze word embeddings that are proposed."}, "embedding": {"model": "specter_v2", "vector": [0.23420101404190063, 0.16932964324951172, -0.2756773829460144, 0.14798285067081451, -0.8065305948257446, -0.5129560232162476, 0.7813981175422668, -0.5589491128921509, -0.6225533485412598, -0.21001043915748596, 0.37901571393013, -0.4989302158355713, 0.32907217741012573, 0.088552325963974, -0.5341145396232605, -0.06163336709141731, -1.0209767818450928, -0.3885224163532257, -0.8118449449539185, -0.030902337282896042, -0.08836346864700317, -0.3249788284301758, -0.5693585276603699, 0.47856810688972473, 0.259723424911499, 0.5278306603431702, -0.34331101179122925, 1.1949944496154785, -0.44131964445114136, 0.5407087802886963, 0.3433016538619995, -0.40525582432746887, 0.6465743780136108, 0.09670513868331909, -0.4674195647239685, 0.2538139522075653, 0.4796466529369354, -0.8634220361709595, -1.0378611087799072, 0.9589146971702576, -0.01937691494822502, 0.13702276349067688, -0.07653629034757614, -0.3616665303707123, -0.322391539812088, 0.42346006631851196, 0.48754966259002686, 0.5038118362426758, -0.02302739955484867, -0.18010231852531433, 0.6399412751197815, -1.4351739883422852, 0.2845386564731598, 1.1379079818725586, 0.2364513874053955, 0.6941861510276794, -0.14124900102615356, -0.7955045700073242, 0.5101755857467651, 0.06432531774044037, -1.0166294574737549, -0.4765090346336365, -0.4241971969604492, -0.0964750126004219, 1.278907060623169, -0.11953262984752655, -0.5035069584846497, 0.4898254871368408, 0.0009206908289343119, 1.0855048894882202, 0.20512481033802032, -0.7899070978164673, -0.3813202381134033, 0.17874392867088318, 0.0385163389146328, 0.9703494310379028, -0.07161007821559906, 0.27138811349868774, -1.3281019926071167, -0.44993117451667786, -0.20838534832000732, 0.17120467126369476, -0.274941086769104, -0.1805993765592575, 0.5233449339866638, 0.6450135111808777, -0.17227207124233246, 0.22475415468215942, 0.2114226073026657, 0.7558993101119995, 0.5278040170669556, 0.4378480315208435, 0.44691169261932373, 0.016004765406250954, 0.10717888176441193, 0.3636557161808014, -0.8904682397842407, 0.21087217330932617, 0.5428492426872253, 0.25313377380371094, -0.3600218594074249, -0.3375583589076996, -0.7360256910324097, -0.1472686231136322, 1.2161322832107544, 0.15855436027050018, 0.35735973715782166, -0.4358178973197937, 1.070772647857666, -1.190781593322754, 0.18046368658542633, -0.6206841468811035, 0.28236159682273865, 0.1867092102766037, -0.6467853784561157, -0.8274517059326172, -0.44864216446876526, -0.41783416271209717, -0.6717890501022339, 0.7621793150901794, -0.5991290807723999, 0.5084182620048523, 0.08941695839166641, 0.6939243078231812, 0.4051351547241211, 0.716917097568512, -0.12926678359508514, 0.14948469400405884, 0.5259698033332825, -0.8541924953460693, -0.4697142243385315, -0.896982729434967, 0.6773490309715271, -0.5576870441436768, 0.35595783591270447, -0.044813066720962524, -1.1494497060775757, -0.4843321144580841, -1.210810899734497, 0.15742601454257965, -0.5851138234138489, -0.049642905592918396, 0.7512166500091553, 0.9141306281089783, -0.6701967120170593, 1.3874748945236206, -0.4319106638431549, -0.2894474267959595, 0.9453923106193542, 0.7314026951789856, 0.00830664299428463, -0.6446696519851685, -1.2667478322982788, 0.2700439393520355, -0.3228493332862854, -0.9084898233413696, -0.1845390349626541, -0.33746537566185, -0.7367310523986816, -0.05532273277640343, 0.08601991832256317, -0.6296712756156921, 0.92701655626297, 0.3964393436908722, -1.2237367630004883, 0.8256180286407471, -0.017237236723303795, -0.03556924685835838, 1.0058679580688477, -0.49820250272750854, -0.7346090078353882, -0.22445322573184967, -0.5179182887077332, -0.16286423802375793, 0.8685120344161987, -0.023244382813572884, 0.28233349323272705, 0.08994529396295547, -0.5193315744400024, -0.13261553645133972, -0.9782835245132446, 0.5202096104621887, -0.38932347297668457, -0.046225666999816895, 0.1666097790002823, 0.7189685106277466, 0.04507097229361534, -0.09585268795490265, -0.8925270438194275, -0.3988473117351532, 1.5558876991271973, -0.37570104002952576, 0.8536523580551147, -0.9109732508659363, -0.8223098516464233, 0.23736274242401123, -0.0744541585445404, 0.13723766803741455, -0.8514522910118103, 1.1087491512298584, -0.33887481689453125, 1.2004618644714355, 0.17513492703437805, -1.7298129796981812, 0.12542831897735596, -0.22767075896263123, -1.2207705974578857, 0.5123587250709534, -0.3515816628932953, 0.7737419605255127, -0.1986015886068344, 0.447133332490921, -0.1158011332154274, 0.2858169376850128, -0.7760075330734253, 1.4148492813110352, -0.2483750283718109, 0.5129647254943848, 0.03136775270104408, -0.4632958769798279, 0.6109064221382141, -0.17635126411914825, 0.5250664353370667, -0.12448551505804062, 0.10026119649410248, 0.5041535496711731, -0.2972981035709381, 1.3263851404190063, -0.5262922048568726, 0.34438079595565796, 0.37806662917137146, -0.6914641261100769, -0.08263499289751053, 0.5749566555023193, 0.17308250069618225, 0.09340707957744598, 0.16146160662174225, 0.08486615866422653, -0.9862880706787109, -0.19221118092536926, 0.6587377786636353, 0.7984094023704529, 0.019258761778473854, 0.8694671392440796, 0.22924600541591644, -0.3080821633338928, 0.08892739564180374, 0.4283403754234314, 0.9701389074325562, 0.4172649383544922, -0.08787789195775986, 0.5995446443557739, 0.3371902108192444, -1.3506966829299927, -0.6015797853469849, 0.9371518492698669, 0.7706976532936096, 1.1470954418182373, 0.24971920251846313, -0.8068384528160095, -0.22228844463825226, 0.045527927577495575, 0.8118317723274231, 1.0060209035873413, 0.033589765429496765, -0.7275603413581848, -0.7336629629135132, -0.9444271326065063, 0.21750408411026, -0.3615477681159973, -0.2057633250951767, -0.2133571207523346, -0.27242469787597656, -1.1748806238174438, 1.074819803237915, -0.5311717391014099, 1.0500351190567017, 0.01820296235382557, 0.3645937740802765, -0.9590765833854675, 0.2191600501537323, -0.7946617007255554, -0.7899709939956665, 0.08304820954799652, -0.0997408851981163, -0.11540984362363815, 0.1117188110947609, 0.12839661538600922, 0.49556681513786316, -0.42290136218070984, 0.40189170837402344, 0.17348459362983704, -0.6048644185066223, 0.7063643336296082, 0.5934709906578064, -0.315655916929245, -0.9900799989700317, 0.17057324945926666, 0.16216759383678436, 0.11853203922510147, 0.02073371410369873, 0.39602646231651306, 0.027180245146155357, -0.3325689733028412, -0.7224713563919067, -0.0708257183432579, -0.09895215928554535, 0.09049632400274277, -0.15870247781276703, -0.3178277909755707, -0.005749312229454517, -1.447622299194336, 1.2300573587417603, -0.2627856433391571, -0.2780001759529114, 0.1898789405822754, -0.9539681077003479, -0.20873671770095825, 0.7619720697402954, -0.7601354718208313, -0.20646753907203674, -1.0603610277175903, -0.021903613582253456, -0.524269700050354, -0.17141833901405334, 0.07398033887147903, 0.4435754418373108, -0.22378985583782196, 0.1634366810321808, 0.5063378810882568, 0.8350502252578735, -0.3581451177597046, 0.7936051487922668, -0.6461108922958374, 0.423880934715271, -0.03063766285777092, 0.7167704105377197, 0.21593640744686127, -0.06638926267623901, -0.4931866526603699, -0.20162427425384521, -0.11385265737771988, -0.005122140049934387, -0.37338027358055115, -0.08556649833917618, -0.4814220070838928, -0.8738442659378052, -0.008645715191960335, -1.1519346237182617, -0.26058486104011536, 0.06557568162679672, -0.21505053341388702, -0.4937545955181122, -0.3690333068370819, -1.3612244129180908, -0.6453995108604431, -0.8441277742385864, -0.7790693044662476, -0.08168892562389374, -0.020189721137285233, -0.544748842716217, -0.5333359241485596, -0.19622592628002167, -0.29683127999305725, 0.9674158096313477, -0.7261766791343689, 0.6751835942268372, 0.014112244360148907, -0.2508937120437622, -0.748109757900238, 0.269521027803421, 0.6207666397094727, -0.25736385583877563, -0.06750895082950592, -1.066057801246643, -0.28636541962623596, 0.0014979453990235925, -0.49168020486831665, 0.3439289331436157, -0.04534197971224785, 0.9778950810432434, -0.5125842094421387, -0.4281175136566162, 1.1475341320037842, 1.5660367012023926, -0.9352908134460449, 0.11121829599142075, -0.09985263645648956, 0.837972104549408, 0.28892844915390015, -0.8050968647003174, 1.1059592962265015, -0.5106217265129089, 0.0020128139294683933, -0.026208680123090744, -0.17192304134368896, -0.010708973743021488, -0.766638457775116, 0.7097765803337097, 0.948151707649231, 1.0483298301696777, -0.36638492345809937, -0.5601606369018555, 0.18168848752975464, -1.4314175844192505, -0.9441626071929932, 0.6540693044662476, 1.0074026584625244, 0.06641757488250732, -0.4356805980205536, -0.43192633986473083, -0.33716684579849243, 0.5483978986740112, 0.6082608699798584, -0.4659197926521301, -0.8081196546554565, 0.1986473649740219, 0.7713316082954407, 0.8185710310935974, 0.4184807240962982, -0.557430624961853, 0.045093316584825516, 14.573857307434082, 0.7151598334312439, -0.18468190729618073, 0.797429621219635, 0.6413876414299011, -0.32001397013664246, 0.009709755890071392, -0.17615002393722534, -0.8508909344673157, 0.12266805022954941, 0.9112622737884521, 0.0006634146557189524, 0.7935245633125305, -0.12811189889907837, -0.023295244202017784, 0.2564718723297119, -0.20683109760284424, 0.7986128926277161, 0.6794783473014832, -1.527707576751709, -0.0511341318488121, 0.45702800154685974, 0.6084796190261841, 0.700477659702301, 1.5648821592330933, 0.3226468563079834, 0.5113740563392639, -0.8903668522834778, 0.3921908438205719, 0.142696350812912, 1.2054075002670288, -0.03505203500390053, 0.11783987283706665, 1.0010981559753418, -0.14752332866191864, -0.2774226665496826, -0.6788737773895264, -1.0848370790481567, 0.2144802063703537, 0.18536598980426788, -0.459391713142395, -0.1888340413570404, -0.2507231533527374, 0.5190683603286743, 0.3459346294403076, -0.040917374193668365, 0.043630603700876236, 0.736773669719696, -0.27812761068344116, 0.3478192985057831, -0.05277664214372635, 0.4056493639945984, 0.16551585495471954, -0.19693399965763092, 0.1060732752084732, -0.43064987659454346, 0.21963731944561005, 0.314594030380249, -0.9732128381729126, 0.09584763646125793, -0.7415034174919128, -0.3019190728664398, -0.2173541933298111, 0.7353259921073914, 1.105158805847168, 0.526725709438324, -0.8546174764633179, 0.7941088080406189, 0.7612095475196838, 0.16632454097270966, 0.20391103625297546, 0.27318495512008667, 0.37251490354537964, -0.06391464918851852, -0.14752990007400513, 0.19087247550487518, -0.32503587007522583, -0.8181073665618896, -0.5551561117172241, -0.3500839173793793, 0.5444562435150146, -0.2521998882293701, -1.2714111804962158, 0.5287490487098694, -0.5474671721458435, -0.3566208481788635, 0.29090115427970886, -0.5010932087898254, -0.3777994215488434, 1.0454844236373901, -1.0365350246429443, -0.525004506111145, 0.7448650598526001, -0.3911689817905426, -0.7018201947212219, -0.3956944942474365, 1.3417202234268188, 0.17079387605190277, -0.3029021620750427, 0.19106264412403107, 0.5519104599952698, 0.20364820957183838, 0.2584461271762848, -0.44854387640953064, 0.9988163113594055, 0.7106358408927917, 0.22276009619235992, 0.2589450180530548, -0.355928897857666, -0.05902481451630592, -1.0419081449508667, -0.09623760730028152, 0.5448799133300781, -0.7842059135437012, -0.5567940473556519, -0.7067566514015198, -0.6078383922576904, 0.12761324644088745, 0.530059814453125, -0.05169520899653435, 0.6023408770561218, 0.03617659583687782, -0.6720843315124512, 0.03321627900004387, -1.0441489219665527, 0.25966131687164307, 0.211356058716774, -1.2499891519546509, 0.24407806992530823, 0.4432172477245331, -0.08417602628469467, -0.9685307145118713, -0.2370416522026062, -0.3064832389354706, 0.08313266187906265, -0.24194122850894928, 0.8315575122833252, -0.6037460565567017, 0.9868932366371155, 1.1288975477218628, -0.13176275789737701, -0.646934986114502, 0.5135108828544617, -1.3433587551116943, -0.249765083193779, 0.20481105148792267, 0.1493748277425766, -0.2116253823041916, 0.7733988761901855, 1.2755076885223389, 0.8806504011154175, -0.19489417970180511, -0.2971777319908142, -0.19711220264434814, 0.45691248774528503, -0.6312838196754456, 0.25664740800857544, -0.2137557715177536, 0.12179016321897507, -0.40640830993652344, -0.19135738909244537, 0.7191277146339417, 0.04065868631005287, -0.8695909380912781, 0.8285753726959229, 0.4162142276763916, -0.3642468750476837, -0.34525489807128906, -0.10674877464771271, -1.7330189943313599, 0.4546869099140167, -1.5463676452636719, -0.38116493821144104, -0.5111227631568909, -0.5233291387557983, 0.4202426075935364, 0.20549026131629944, -0.335682213306427, 0.5553582906723022, -0.04767583683133125, 0.29127901792526245, -0.4515979290008545, -0.17210203409194946, 0.5187237858772278, 0.6468607783317566, -0.7650161981582642, 0.4514562487602234, 0.30745020508766174, -0.021577587351202965, 0.09187798202037811, 0.5182000994682312, -0.7204331755638123, -0.797537624835968, -1.1488081216812134, -0.04806319624185562, -0.06834476441144943, 0.0887601301074028, -0.5933821201324463, 0.8588281273841858, 0.31435626745224, 0.17277894914150238, 0.3390471339225769, 0.14819972217082977, -0.9570163488388062, -0.6043985486030579, 0.24023452401161194, -1.0567291975021362, -0.03238694369792938, -0.08970949798822403, -0.5201875567436218, 0.1562606692314148, 0.434937447309494, 0.046414587646722794, -0.8913179039955139, -0.4006446599960327, 0.7661814093589783, -0.490779846906662, 0.04436486214399338, -0.21552148461341858, 0.3674413561820984, -1.374066710472107, -1.0293506383895874, -0.5753606557846069, 0.24327218532562256, -0.18811891973018646, 0.8525717854499817, 0.37114566564559937, -0.8359764814376831, 0.01291064452379942, 0.6531243920326233, -0.6158158183097839, -0.32276883721351624, 0.9590818881988525, 0.0986248329281807, -0.3334522843360901, 0.05829117074608803, 0.6658564209938049, 0.6506686806678772, -1.0311585664749146, 0.16857802867889404, 0.9297982454299927, -0.9011088609695435, -0.2954210340976715, 1.1148122549057007, -0.30169492959976196, -0.8200587630271912, 0.1366279423236847, -0.28537145256996155, 0.06562920659780502, -0.7261117100715637, 0.6889501214027405, 0.07679279148578644, 0.483720600605011, 0.2357308715581894, -0.5436751246452332, 0.2148258090019226, 0.036084871739149094, -0.8461788296699524, 0.24790474772453308, -0.8084201216697693, -0.35111096501350403, -0.02363438345491886, 1.4665226936340332, 0.060650784522295, -0.6651276350021362, -0.8793447017669678, -0.6503004431724548, -0.4177636206150055, 0.29188111424446106, -0.31602180004119873, -0.5412544012069702, 0.5455496311187744, 0.5765175819396973, 0.32884156703948975, 0.006135955452919006, -0.3710707724094391, 0.7919071912765503, 0.41360118985176086, -0.173532173037529, -0.4380638003349304, -0.51048344373703, 0.974648654460907, 0.9161907434463501, -0.8164369463920593, 0.6836144924163818, -0.18684403598308563, -0.3797524869441986, 1.2311145067214966, -0.002527613425627351, -0.09873783588409424, 1.4033281803131104, 0.3494195342063904, 0.4336905777454376, 0.5899008512496948, -1.185150384902954, -0.06472957879304886, 0.7896470427513123, 0.6909796595573425, 0.4506447911262512, 0.021727053448557854, -0.2489147186279297, 0.7957794666290283, 0.43951624631881714, 0.26444169878959656, 0.4905005991458893, 0.6894726753234863, 0.22257095575332642, -0.8675302863121033, -0.5411368012428284, 0.661301851272583, -1.3517436981201172, -0.49803322553634644, -0.017770320177078247, 0.19212043285369873, 0.42935892939567566, 0.8937513828277588, 0.5186437368392944, -0.575905978679657, 0.24915577471256256, 0.5383081436157227, 0.3955816924571991, -0.6492000222206116, -0.4766053557395935, -0.5568904280662537, -0.643690824508667, -0.09607920050621033, 0.08597591519355774, -0.3654690086841583, -0.3348561227321625, -0.9040447473526001, 0.7615612745285034, -0.11124642938375473, 0.21822550892829895, 1.2303043603897095, 0.5804856419563293, -0.1268586665391922, 0.4119420349597931, -0.44123440980911255, -0.9543858766555786, -0.521550178527832, -0.3932701051235199, -0.08301477134227753, -0.1269245445728302, 0.624354898929596, -0.3843882381916046, -0.13398167490959167]}, "authors": [{"authorId": "2143081868", "name": "Samyak Gupta"}, {"authorId": "108053318", "name": "Yangsibo Huang"}, {"authorId": "49164966", "name": "Zexuan Zhong"}, {"authorId": "4800645", "name": "Tianyu Gao"}, {"authorId": "2158261179", "name": "Kai Li"}, {"authorId": "50536468", "name": "Danqi Chen"}], "references": [{"paperId": "2cb272df6d7d4de6be6aef5eacd1017ef6cb2df7", "title": "LAMP: Extracting Text from Gradients with Language Model Priors"}, {"paperId": "f2cd15c1925ef54d58b3d71506d7113d7911a8c2", "title": "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models"}, {"paperId": "580f363e891e3e1bf92c589453247e7d1f5278c7", "title": "When the Curious Abandon Honesty: Federated Learning Is Not Private"}, {"paperId": "98dabedfbab42072b2bc62282ab48eb6ff6026dc", "title": "Evaluating Gradient Inversion Attacks and Defenses in Federated Learning"}, {"paperId": "e27be0d4f964713229f665a0a335efed8fbd42c9", "title": "Gradient Inversion with Generative Image Prior"}, {"paperId": "56874f9aef515902c5a49d84d10f629f8dcd5f40", "title": "Differentially Private Fine-tuning of Language Models"}, {"paperId": "6a758ada5c48a2ae48d1392d12ce4f4e1977e0dd", "title": "Large Language Models Can Be Strong Differentially Private Learners"}, {"paperId": "0412441ad2559c44075ca991b2b4ca533ac03dc5", "title": "Studying word order through iterative shuffling"}, {"paperId": "d8e375017de9fd2c875b6925cbd6c64519a9c558", "title": "MedGPT: Medical Concept Prediction from Clinical Narratives"}, {"paperId": "d5781dfbcf133e31535de32e325d68638d61e413", "title": "Membership Inference Attacks on Deep Regression Models for Neuroimaging"}, {"paperId": "0fd50e1483f761ba2bae44de54c5e8db6e35de5a", "title": "See through Gradients: Image Batch Recovery via GradInversion"}, {"paperId": "07033abe45c0e2556e8c3fa1db1879b42dd45915", "title": "TAG: Gradient Attack on Transformer-based Language Models"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "408837d491e83f0a9d12f9eaade1089c42a1fbcf", "title": "Fidel: Reconstructing Private Training Samples from Weight Updates in Federated Learning"}, {"paperId": "df7d26339adf4eb0c07160947b9d2973c24911ba", "title": "Extracting Training Data from Large Language Models"}, {"paperId": "8448010d9adad18bf36070c012770a10ecb21c76", "title": "Privacy and Robustness in Federated Learning: Attacks and Defenses"}, {"paperId": "2ae3f6b82dd8503da380b373520e0d0713627cf2", "title": "R-GAP: Recursive Gradient Attack on Privacy"}, {"paperId": "91531cb745eaa5b60d9abdbe19b8926267e64baf", "title": "TextHide: Tackling Data Privacy for Language Understanding Tasks"}, {"paperId": "d288c1fdfe83b934ec86155c7661950ac98ac1e1", "title": "InstaHide: Instance-hiding Schemes for Private Distributed Learning"}, {"paperId": "7b005563638a5d7be3d9058fe0474ef8d6fd685e", "title": "Understanding Unintended Memorization in Federated Learning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "698ab1cc02a79596a87f92d5a0882ab1a7aee266", "title": "Inverting Gradients - How easy is it to break privacy in federated learning?"}, {"paperId": "0e0eaa200bfe4cdaf15a18e9faccf4dd1d1c0f4c", "title": "Information Leakage in Embedding Models"}, {"paperId": "0b09edd6be4e55167f055c00def29e6b221d8ee6", "title": "Federated pretraining and fine tuning of BERT using clinical notes from multiple silos"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "0b5f443d9fae1d5cb90994025e0ce21ddc49c21c", "title": "iDLG: Improved Deep Leakage from Gradients"}, {"paperId": "782e7fba4dfa8e2095a71cfcf85995543f37ac75", "title": "Analyzing Information Leakage of Updates to Natural Language Models"}, {"paperId": "49bdeb07b045dd77f0bfe2b44436608770235a23", "title": "Federated Learning: Challenges, Methods, and Future Directions"}, {"paperId": "57633ff5c6f0708be25e651f51eef29d2fbfe48b", "title": "BEHRT: Transformer for Electronic Health Records"}, {"paperId": "5507d267bbf0b4cdb9f893c3c0960a45016f7010", "title": "Deep Leakage from Gradients"}, {"paperId": "cf4aa38ae31b43fd07abe13b4ffdb265babb7be1", "title": "The Curious Case of Neural Text Degeneration"}, {"paperId": "b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb", "title": "ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"}, {"paperId": "2a567ebd78939d0861d788f0fedff8d40ae62bf2", "title": "Publicly Available Clinical BERT Embeddings"}, {"paperId": "b72b6fae30561a7e29392e04e82ed1ad7bce8e78", "title": "Federated Learning for Mobile Keyboard Prediction"}, {"paperId": "30e0ffeb519a4df2d4a2067e899c5fb5c5e85e70", "title": "Exploiting Unintended Feature Leakage in Collaborative Learning"}, {"paperId": "530a4ab0308bc98995ffd64207135ca0ae36db7f", "title": "Privacy-Preserving Deep Learning via Additively Homomorphic Encryption"}, {"paperId": "520ec00dc35475e0554dbb72f27bd2eeb6f4191d", "title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf", "title": "A Closer Look at Memorization in Deep Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "a97dc4eba33a2756d73178cd945f465fb4e193b1", "title": "Practical Secure Aggregation for Federated Learning on User-Held Data"}, {"paperId": "e4dd95c4341ec7d14317a3d97022773a0822906c", "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "e9a986c8ff6c2f381d026fe014f6aaa865f34da7", "title": "Deep Learning with Differential Privacy"}, {"paperId": "d1dbf643447405984eeef098b1b320dee0b3b8a7", "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "39e3dfbe0ef3e8662a9b190e7e659e7d855e7794", "title": "Catastrophic Data Leakage in Vertical Federated Learning"}, {"paperId": "61f1e81de95648e044b5a63f01a8846c85539739", "title": "User Label Leakage from Gradients in Federated Learning"}, {"paperId": "334b586ae771f0566c3c3d32a4c7ec4fafa88f34", "title": "Information Leaks in Federated Learning"}, {"paperId": null, "title": "Industrial-strength Natural Language Processing in Python"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2019)\u2019s attack with different initialization. The original sentence is \u201cHe consequently ordered the construction of Fort Oswego at the mouth of the Oswego River."}, {"paperId": null, "title": "Table 5 shows the attack results of Zhu et al. (2019) with different batch sizes. Their gradientmatching optimization fails to recover sentences with different batch sizes"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": null, "title": "Artificial Intelligence: A Modern Approach"}, {"paperId": "64c22174a3a43c1224c88ba37eae6421a0086730", "title": "The Enron Corpus: A New Dataset for Email Classi(cid:12)cation Research"}, {"paperId": null, "title": "Speech understanding systems : A summary of results of the five - year research effort"}, {"paperId": null, "title": "a) Did you include the full text of instructions given to participants and screenshots, if applicable?"}, {"paperId": null, "title": "c) Did you include any new assets either in the supplemental material or as a URL? [Yes]"}, {"paperId": null, "title": "(b) Did you mention the license of the assets? [Yes]"}, {"paperId": null, "title": "Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable"}, {"paperId": null, "title": "(a) Did you state the full set of assumptions of all theoretical results"}, {"paperId": null, "title": "Did you discuss any potential negative societal impacts of your work? [Yes] , see Sec"}, {"paperId": null, "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating?"}, {"paperId": null, "title": "Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] , see Fig 3"}, {"paperId": null, "title": "Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?"}, {"paperId": null, "title": "Have you read the ethics review guidelines and ensured that your paper conforms to them"}, {"paperId": null, "title": "If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots"}, {"paperId": null, "title": "(a) Did you include the code, data, and instructions needed to reproduce the main experimental results"}, {"paperId": null, "title": "Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation"}, {"paperId": null, "title": "If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators?"}]}