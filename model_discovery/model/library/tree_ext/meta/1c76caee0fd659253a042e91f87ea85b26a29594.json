{"paperId": "1c76caee0fd659253a042e91f87ea85b26a29594", "title": "Explore the Limits of Omni-modal Pretraining at Scale", "abstract": "We propose to build omni-modal intelligence, which is capable of understanding any modality and learning universal representations. In specific, we propose a scalable pretraining paradigm, named Multimodal Context (MiCo), which can scale up the numbers of modalities and amount of data, together with the model parameters, in the pretraining process. With MiCo, the pretrained models show significant emergent abilities in multimodal learning, which are evaluated on the following tasks: i) single-modality perception benchmarks of 10 different modalities, ii) 25 cross-modality understanding tasks of retrieval, question-answering, captioning, and iii) 18 multimodal large language model benchmarks. Our models establish 37 new records for state-of-the-art performance. We hope that our research could contribute to the development of omni-modal intelligence. Code and Models are at https://github.com/invictus717/MiCo", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A scalable pretraining paradigm, named Multimodal Context (MiCo), which can scale up the numbers of modalities and amount of data, together with the model parameters, in the pretraining process, is proposed, which shows significant emergent abilities in multimodal learning."}, "embedding": {"model": "specter_v2", "vector": [0.4625713527202606, 0.5682662129402161, -0.061299972236156464, -0.600132167339325, -0.5871155858039856, 0.07537991553544998, 0.8187590837478638, -0.2498944103717804, -0.7836319804191589, -0.40482938289642334, 0.7792945504188538, 0.16308297216892242, 0.33278584480285645, 0.36852601170539856, 0.0016053190920501947, 0.6140655875205994, -0.7210544943809509, 0.43569061160087585, -0.12440969049930573, -0.7573102116584778, -0.21560552716255188, -0.7585659027099609, -1.2017755508422852, 0.6765636801719666, 0.25404495000839233, 0.7361282706260681, 0.4723183214664459, 1.0968936681747437, -0.1029520109295845, 0.22311635315418243, 0.5485128164291382, -0.04423563927412033, -0.1672562211751938, 0.17901696264743805, -0.6028494834899902, -0.17317865788936615, 0.3949767053127289, -0.7804179787635803, -0.4645090401172638, 0.5578936338424683, -0.09479384869337082, -0.032531533390283585, 0.697484016418457, -0.6303513050079346, -0.5843280553817749, 0.7720659375190735, 0.9432902932167053, 0.30216145515441895, 0.33628177642822266, -0.6731083393096924, 1.3854455947875977, -1.6618273258209229, 0.3501044809818268, 1.9221642017364502, 0.315628319978714, 1.2489278316497803, -0.2521935999393463, -0.33727747201919556, 0.6859171390533447, 0.024266254156827927, -0.37564006447792053, -0.3993338346481323, -0.19404633343219757, 0.17028610408306122, 1.2000590562820435, -0.3425370454788208, -0.38681116700172424, 0.8193944096565247, -0.24527812004089355, 1.7336903810501099, 0.012268979102373123, -1.0967241525650024, -0.25394555926322937, 0.11557444930076599, 0.3424314558506012, 0.4341638386249542, -0.9397300481796265, 0.5511674284934998, -1.1156114339828491, 0.2766079008579254, 0.2301432490348816, 0.08230608701705933, -0.29241544008255005, 0.0366218201816082, -1.0868016481399536, 0.5574334263801575, 0.7496827244758606, 0.9047808647155762, -0.2494705319404602, 0.47892478108406067, 0.6211636662483215, 0.7528494000434875, -0.7348455786705017, 0.46846088767051697, -0.27117154002189636, 0.1739153414964676, -0.5843392610549927, -0.10064996033906937, 0.1366099864244461, 0.8675904273986816, -0.16365478932857513, -0.2886175215244293, -0.40396848320961, 0.24039168655872345, 1.6446243524551392, 0.15234653651714325, 0.5808145403862, -0.7879892587661743, 0.32383179664611816, -0.4803934693336487, 0.11884051561355591, -0.4179174304008484, -0.6038550734519958, 0.1134476587176323, -0.4675895571708679, -1.2454514503479004, -0.3078783452510834, -0.00836732517927885, -1.1614466905593872, 0.95918208360672, -0.29235947132110596, -0.49741697311401367, 0.41963982582092285, 0.6777787208557129, 1.081937313079834, 0.6192255020141602, 0.5956334471702576, 0.49587520956993103, 1.1030893325805664, -0.6586061120033264, -0.5531290769577026, -1.080604910850525, 0.863376259803772, 0.003561070654541254, 0.4766891896724701, -0.29140642285346985, -0.851584255695343, -1.1205785274505615, -0.8991402983665466, -0.39138728380203247, -0.8156957626342773, 0.1896122694015503, 1.349562644958496, 0.5670557022094727, -0.9840924143791199, 0.171083003282547, -0.17709945142269135, -0.6201151013374329, 0.02459017001092434, 0.48792511224746704, -0.31184351444244385, -0.9224314093589783, -1.216956377029419, 0.22462767362594604, 0.6396016478538513, -0.16124865412712097, -0.5778118371963501, -0.07204004377126694, -1.4461182355880737, -0.3108247220516205, 0.19653305411338806, -0.780621349811554, 1.2012196779251099, -0.384860634803772, -1.1141263246536255, 0.5441813468933105, -0.733471691608429, 0.5349435210227966, 0.06358270347118378, -0.2145494520664215, -1.3114542961120605, 0.1416134089231491, -0.29092085361480713, 1.5018585920333862, 0.32328230142593384, -0.40708592534065247, -0.32884731888771057, -0.12299107015132904, 0.010022376663982868, 0.3827418088912964, -0.38809430599212646, 0.9044593572616577, -0.341018944978714, 0.284501850605011, 0.802679717540741, 0.8235638737678528, 0.1561012864112854, -0.2202729433774948, -0.4819285273551941, -0.9157054424285889, 0.9080743193626404, 0.27927181124687195, 0.3925638794898987, -1.4074324369430542, -0.24224987626075745, -0.17133986949920654, -0.016662167385220528, -0.6570417881011963, -1.423427700996399, 0.6655811071395874, -0.17982511222362518, 0.2017926424741745, -0.09445810317993164, -1.3007577657699585, 0.4112812578678131, -0.1664820909500122, -0.24665267765522003, -0.6496544480323792, 0.12734545767307281, 1.2726173400878906, -0.8685843348503113, -0.22435510158538818, -0.020573779940605164, 0.17975182831287384, -0.9542071223258972, 1.0692226886749268, -0.7230793833732605, 0.45161572098731995, 0.04181266948580742, 0.42684099078178406, -0.13797052204608917, -0.36057859659194946, 0.3186485767364502, -0.8915553092956543, 0.39721623063087463, 0.28502845764160156, -0.5646789073944092, 1.9984631538391113, -0.3856467604637146, 0.5420656204223633, -0.007878913544118404, -0.2732284367084503, -0.28049787878990173, 0.739460825920105, -0.3190101087093353, -0.5894802212715149, 0.4529886543750763, 0.5800034403800964, -0.47379130125045776, -0.07768884301185608, 0.8773772120475769, 0.28016674518585205, -0.5577734708786011, -0.07744383066892624, 0.912088930606842, -0.33778074383735657, 0.3298472762107849, 0.507201611995697, 0.5078061819076538, 0.030408883467316628, -0.11514653265476227, 0.21707488596439362, 0.41662490367889404, -0.98502516746521, -0.6104186177253723, 0.5177348256111145, 0.5187105536460876, 0.9652365446090698, 0.026959294453263283, -0.33628925681114197, 0.00878746435046196, -0.1322130262851715, 0.44105032086372375, 2.0451815128326416, 0.18304765224456787, 0.17854808270931244, -0.45187628269195557, -0.24479982256889343, -0.6609675288200378, 0.3989112675189972, -0.6453822255134583, 0.012178005650639534, 0.04946465417742729, -0.7375028133392334, 0.47165101766586304, 0.6459717154502869, 1.1411007642745972, -0.8605386018753052, -0.09186366200447083, -0.550781786441803, 0.0940278097987175, -0.8684442639350891, -0.48220452666282654, 0.17857785522937775, -0.522817850112915, -0.35723450779914856, 0.03716769814491272, -0.327747642993927, 0.16393007338047028, -0.8616167902946472, 0.8356767296791077, -0.7384412884712219, 0.19233675301074982, 0.9612917900085449, 0.519197940826416, -0.42891034483909607, -0.7571234703063965, 0.19929759204387665, -0.039277154952287674, 0.08246871829032898, 0.5786848068237305, 0.7056580185890198, -0.08750265091657639, 0.40828248858451843, -0.9049574136734009, 0.38404589891433716, 0.2987324595451355, 0.1506599634885788, 0.7917540669441223, -0.603219747543335, 0.5065890550613403, -1.0978583097457886, 0.5791718363761902, -0.15992707014083862, 0.140106201171875, 0.33011797070503235, -0.10993906110525131, -0.5976022481918335, 0.14079523086547852, -0.5724001526832581, -0.4698774218559265, -0.5255778431892395, 0.12893061339855194, 0.14172609150409698, -0.9298673868179321, 0.6743761897087097, -0.16410361230373383, 0.32534295320510864, 0.4145018756389618, 0.35865962505340576, 0.1803451031446457, -0.006859547924250364, 0.7424966096878052, -0.5831131339073181, 0.6194707751274109, 0.25902771949768066, 0.05211801081895828, 0.08142057061195374, -0.052125927060842514, -0.7390795350074768, -0.5492281317710876, -0.36278173327445984, -0.3052443265914917, -0.4889305830001831, 0.49085670709609985, -0.600227415561676, -0.8310477137565613, -0.28045889735221863, -0.8881571292877197, -0.16211137175559998, 0.08235587179660797, 0.027408452704548836, -0.3229019045829773, -1.162131667137146, -1.061726450920105, -0.4677135944366455, -0.1065751314163208, -0.7416591048240662, 0.5624850988388062, 0.298613578081131, -0.414937287569046, -0.6115156412124634, -0.066889688372612, -0.040916670113801956, 0.7752462029457092, -0.6703755855560303, 0.7624066472053528, -0.0857427790760994, -0.08818088471889496, -0.717303991317749, -0.11046599596738815, 0.7508965730667114, -0.3268165588378906, -0.1758723109960556, -1.138614296913147, 0.3843365013599396, -0.40917253494262695, -0.6431488990783691, 0.30682113766670227, 0.0016610975144430995, 0.2615150511264801, 0.8344398736953735, -0.2620009183883667, 0.05491501837968826, 1.5025628805160522, -0.4548934996128082, 0.18497887253761292, 0.040104057639837265, 0.9989646077156067, 0.4509318172931671, -0.3332560956478119, 0.11725888401269913, 0.7563124299049377, 0.0930715799331665, 0.48653098940849304, 0.20817117393016815, -0.1807689219713211, -0.49898645281791687, 0.5740293264389038, 1.231953740119934, 0.2233024686574936, -0.019503392279148102, -1.0920542478561401, 0.5811894536018372, -1.075089931488037, -0.4310297966003418, 0.8198347687721252, 0.5718618631362915, 0.3163250684738159, -0.7399589419364929, -0.2520558834075928, -0.39278775453567505, 0.3467029333114624, -0.014452971518039703, -0.4415993392467499, -0.3599073886871338, -0.12301602214574814, 0.015744956210255623, -0.5118794441223145, 0.648923933506012, -0.5324676632881165, 0.28417885303497314, 14.248709678649902, 0.4002571403980255, 0.09702438116073608, 0.6139612793922424, 0.4428653120994568, 0.5164823532104492, -0.6705163717269897, -0.33198732137680054, -0.9163820147514343, -0.3777458369731903, 1.206206202507019, 0.9325380921363831, 0.7887966632843018, -0.034772250801324844, -0.4321896433830261, -0.026039866730570793, -1.5072802305221558, 0.746589183807373, 0.6360377669334412, -0.8090620636940002, 0.37465888261795044, -0.022667931392788887, 0.2849186360836029, 0.7863015532493591, 0.9401861429214478, 1.1671713590621948, -0.07494257390499115, -0.469026654958725, 0.41195806860923767, 0.5676755905151367, 1.1196472644805908, -0.042111724615097046, 0.58072829246521, 0.4512493908405304, -0.5017580986022949, -0.644059419631958, -0.09770333766937256, -0.7412128448486328, 0.5011911392211914, -0.549685001373291, -0.2038601189851761, -0.5204421877861023, -0.4573104977607727, 0.6322494149208069, -0.18750283122062683, 0.2150459736585617, -0.053769662976264954, 0.24379917979240417, -0.43198564648628235, -0.17595873773097992, -0.014652217738330364, 0.9520598649978638, 0.3751031756401062, -0.1486354023218155, -0.08122330158948898, -0.06532628834247589, -0.050179049372673035, 0.3257753849029541, -0.7138667702674866, -0.06138364598155022, -0.191017284989357, -0.17686091363430023, -0.3059674799442291, 0.48487627506256104, 0.6851917505264282, 0.4686659574508667, -0.7995791435241699, 0.28502997756004333, 0.06096481904387474, 0.44013094902038574, 0.09191469103097916, -0.08655394613742828, -0.1860039085149765, -0.8384279012680054, -0.3146458864212036, 0.6342477798461914, -0.21612165868282318, -0.754522979259491, -0.5357295274734497, -0.3182908296585083, 0.7526105642318726, -0.9773916006088257, -1.0981504917144775, 0.798543393611908, -0.25514569878578186, -0.5519957542419434, -0.12247627973556519, -0.9153116345405579, -0.062080804258584976, 0.23204106092453003, -1.4416232109069824, -1.2351689338684082, -0.030543657019734383, 0.040249232202768326, -0.5129652619361877, -0.22925762832164764, 1.6183842420578003, 0.22901596128940582, -0.15779128670692444, -0.018766123801469803, -0.26686346530914307, -0.012541355565190315, -0.2131452113389969, -0.9819887280464172, -0.07540663331747055, -0.05151256546378136, 0.038714684545993805, 0.07480618357658386, 0.09663644433021545, 0.3656575679779053, -1.0670244693756104, 0.5169553160667419, 0.5633425712585449, -0.9603728652000427, -0.4663384258747101, -0.548120379447937, -0.5622317790985107, 0.4351416528224945, 0.737542986869812, -0.2901430130004883, 0.7580091953277588, 0.64555823802948, -0.8343459367752075, -0.13494937121868134, -0.9927103519439697, 0.44442903995513916, 0.2407291680574417, -1.0784066915512085, -0.6516976952552795, 0.024966314435005188, 0.4642024040222168, -0.7024024724960327, -0.5818389058113098, 0.026712331920862198, 0.3659977614879608, 0.01211064588278532, 0.7935218214988708, -0.526677131652832, 0.6981746554374695, 0.7024363875389099, -0.7809270620346069, -0.613810658454895, 0.5308998227119446, -0.44185686111450195, -0.12505465745925903, -0.06427637487649918, 1.1015270948410034, -0.36514025926589966, -0.059974946081638336, 1.192999005317688, 0.15300780534744263, -0.29264935851097107, -0.6917574405670166, 0.2213924676179886, -0.16220661997795105, -0.6781886219978333, 0.24915271997451782, -0.3373153805732727, -0.029714981094002724, 0.5620427131652832, 0.7210814952850342, 0.902361273765564, -0.3634885847568512, -0.8293449878692627, 0.4006904363632202, -0.13075518608093262, -0.25060784816741943, -0.3246002197265625, -0.485914021730423, -1.5865657329559326, -0.27163398265838623, -1.0587736368179321, 0.09818026423454285, -1.1707388162612915, -0.4995482861995697, 0.4469541013240814, -0.4832136332988739, 0.05942589417099953, 0.3460906147956848, -0.18615233898162842, 0.0020437766797840595, -0.590570330619812, -1.0610594749450684, 0.7714269757270813, 1.1218235492706299, -0.6484749913215637, -0.015672896057367325, -0.24883146584033966, -0.3406609296798706, 0.3176896870136261, 0.26977089047431946, 0.1202847808599472, -0.8504400849342346, -1.4520643949508667, 0.4991599917411804, 0.18982920050621033, 0.13327358663082123, -1.0968446731567383, 0.8013849854469299, 0.875325083732605, -0.07345212996006012, -0.12910965085029602, 0.9364146590232849, -1.3683617115020752, -0.7521181702613831, -0.06875306367874146, -1.0923373699188232, 0.020270658656954765, 0.41382238268852234, -0.4904426336288452, -0.6362956762313843, 0.48819613456726074, -0.2293921411037445, -0.6652340888977051, -1.262021780014038, 0.151277095079422, -0.2949194610118866, 0.08682826161384583, -0.38960349559783936, -0.04906284064054489, -1.2138774394989014, -0.5649399161338806, -0.6151501536369324, 0.6689603924751282, -0.6862336993217468, 0.6228644251823425, 0.8654656410217285, -1.1220613718032837, -0.10291777551174164, 0.22732189297676086, 0.5486493110656738, 0.017065895721316338, 1.0012149810791016, 0.5329232811927795, -0.06363086402416229, 0.2979999780654907, 0.1911490261554718, -0.06295957416296005, -0.7347722053527832, -0.07397856563329697, 0.9489464163780212, 0.027284981682896614, 0.1048094853758812, 1.2264509201049805, -0.17248885333538055, -1.5697941780090332, 0.3561460077762604, -0.8222521543502808, -0.8316649198532104, -0.15525619685649872, 0.7914148569107056, -0.306427538394928, -0.5974429249763489, -0.14694736897945404, -0.22164756059646606, 0.31737855076789856, -0.1483159065246582, -1.0975630283355713, 0.4745734632015228, -0.31112098693847656, -0.4770599901676178, 0.8742468953132629, 1.0903126001358032, -0.8183029890060425, -0.40753549337387085, -0.7254339456558228, -0.27656248211860657, 0.3015914857387543, 0.04021501913666725, -1.0539731979370117, -0.38331809639930725, 0.8052107095718384, 0.8230239748954773, 0.5146103501319885, 0.5952836871147156, 0.4373539388179779, 0.21112720668315887, 1.0347050428390503, -0.10350363701581955, -0.5698592066764832, -0.3284662067890167, 1.2263656854629517, 1.4706004858016968, -1.5236399173736572, -0.31228402256965637, -0.27553996443748474, -0.559480607509613, 0.8234993815422058, 0.3244004547595978, 0.1254759281873703, 1.0313372611999512, -0.7771174311637878, 0.23723413050174713, 0.0862739309668541, -0.8971008062362671, -0.23552517592906952, 1.271841287612915, 1.3193635940551758, 0.7049382328987122, 0.528512716293335, 0.2992805242538452, 0.7188708186149597, 0.1638326197862625, 0.25652098655700684, 0.2648090422153473, 0.20958386361598969, -0.21762670576572418, 0.10185451805591583, 0.2619137167930603, 0.6803044676780701, -0.11179009824991226, -0.5019785761833191, 0.4195306599140167, 0.6489729881286621, 0.2967529594898224, 0.9992533922195435, 0.5468675494194031, 0.1324600726366043, 0.7102586627006531, 0.4434904158115387, 0.6004631519317627, -0.6918559670448303, 0.14185644686222076, -0.2613467276096344, -0.4401584267616272, -0.11084291338920593, -0.7300200462341309, -0.6294415593147278, -0.6580880880355835, 0.35467642545700073, 0.5916738510131836, -0.33582043647766113, 0.6102083921432495, 1.315726637840271, 0.5359770655632019, 0.41497451066970825, -0.6856919527053833, -0.5215964317321777, -0.2727852761745453, -1.3296093940734863, 0.20351922512054443, -0.3691661059856415, -0.022814670577645302, -0.21552921831607819, -0.3659682273864746, -0.5852037072181702]}, "authors": [{"authorId": "2192339453", "name": "Yiyuan Zhang"}, {"authorId": "2218650724", "name": "Handong Li"}, {"authorId": "2307457137", "name": "Jing Liu"}, {"authorId": "2268400784", "name": "Xiangyu Yue"}], "references": [{"paperId": "8092f6a18c525f7c6eacee2a842e80985b177e74", "title": "GeoWizard: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image"}, {"paperId": "53c3c3984649ca82a2f85629dae01087e9e72991", "title": "OneLLM: One Framework to Align All Modalities with Language"}, {"paperId": "ea3448eb86a233189631d914721e587d45931b64", "title": "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"}, {"paperId": "486c2df78cbb770a90a55f7fa3fe19102fba2c24", "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"}, {"paperId": "1f04099e29e0aac20338fd23c0245054b62715f9", "title": "UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition"}, {"paperId": "cc01693fcd63931eb3d089b39b64a1c99fa0a5b6", "title": "DiffSpectralNet : Unveiling the Potential of Diffusion Models for Hyperspectral Image Classification"}, {"paperId": "124d4d374fbef2016fa9880489871a58a7450644", "title": "Improved Baselines with Visual Instruction Tuning"}, {"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report"}, {"paperId": "f2f9c02a7eb484dd7b7ac46892856e3f278eed77", "title": "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model"}, {"paperId": "70192d260e19943e04ec7b17d49f803954fd4363", "title": "BT-Adapter: Video Conversation is Feasible Without Video Instruction Tuning"}, {"paperId": "54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f", "title": "ImageBind-LLM: Multi-modality Instruction Tuning"}, {"paperId": "94972e30504017156ef5b5debc419bf6edc67384", "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"}, {"paperId": "4309d572a37d655779f9dce6a2c98c66334132de", "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"}, {"paperId": "83c48aa341850af478247e3b34ba1ee1db9f1236", "title": "Meta-Transformer: A Unified Framework for Multimodal Learning"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "b37b1dc72b1882858f5120f2cd6883134089a6ed", "title": "MMBench: Is Your Multi-modal Model an All-around Player?"}, {"paperId": "697e0add95e880bd42e00bef838181e105f91981", "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"}, {"paperId": "948e8cfae92c2004f2dd5c9316f5972f8baaea21", "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"}, {"paperId": "bf7025a2e5dbb3c09deae02a1aa98a256ca559e2", "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"}, {"paperId": "5d321194696f1f75cf9da045e6022b2f20ba5b9c", "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"}, {"paperId": "d8020d1247699ae7578f2e17fd47771f4c326832", "title": "Interpretable weather forecasting for worldwide stations with a unified deep model"}, {"paperId": "4e33c5756aa18d248cf50fef9382acda1e0f65da", "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset"}, {"paperId": "309074a33c9a96bdcdde6bf8ad35fc03e716a2cd", "title": "Multi-Scale Attention for Audio Question Answering"}, {"paperId": "c6ac708b65b24c20f80831d518c1795ce8133ad5", "title": "ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"}, {"paperId": "ad22af138fa1d1490cda0301abf8159a7c30c5a2", "title": "Pengi: An Audio Language Model for Audio Tasks"}, {"paperId": "d48cb91b9e555194f7494c4d4bb9815021d3ee45", "title": "VideoChat: Chat-Centric Video Understanding"}, {"paperId": "7dc6da87eaa6f830354feb2db14023cab8678c91", "title": "ImageBind One Embedding Space to Bind Them All"}, {"paperId": "570079bbdd8758dfe865097e05719313c9c1301a", "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"}, {"paperId": "03755613d50e1958a97bfaad2efb976f786fbb70", "title": "VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "3dfed62c61f650eb114f0f0aa26b4e7d37b963a6", "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research"}, {"paperId": "26eb6745006e94317cfb634123fe3015702fb224", "title": "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks"}, {"paperId": "805e1fa8250fd8aad356f6705f4957d412d2549a", "title": "MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models"}, {"paperId": "574beee702be3856d60aa482ec725168fe64fc99", "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "89a1dbbfd4c96d90b769f5d3427bd970b082898e", "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers"}, {"paperId": "26a217cd87d4c054361fea6e74f8803a5a415293", "title": "ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding"}, {"paperId": "78281482c1fdad8e167bab39cc9955c73d58ae8f", "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"}, {"paperId": "dfb22e1bfecafb4e37e6b3c0f4fd547669ced6ea", "title": "FeatureCut: An Adaptive Data Augmentation for Automated Audio Captioning"}, {"paperId": "4c94d04afa4309ec2f06bdd0fe3781f91461b362", "title": "DreamFusion: Text-to-3D using 2D Diffusion"}, {"paperId": "d3135733aa39dec20ce72aa138589dda27c8406d", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"}, {"paperId": "28630034bb29760df01ab033b743e30b37f336ae", "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model"}, {"paperId": "02251886950770e82b3d68564d60cdfe15e73199", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"}, {"paperId": "794c5d5ca20e71eae416da91cf1fed0a8ef15658", "title": "Masked Autoencoders that Listen"}, {"paperId": "a970c8fadef8497576660b288c52c0ec8eebdc12", "title": "Zero-Shot Video Question Answering via Frozen Bidirectional Language Models"}, {"paperId": "a26a7a74f1e5fd562be95c3611a0680759fbdf84", "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "c5d8c27bf231d6289e2e1acbcecd8ff5162cba53", "title": "Clotho-AQA: A Crowdsourced Dataset for Audio Question Answering"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "12124de2038fda868fcb93c3da1996dd157e0390", "title": "Learning to Answer Questions in Dynamic Audio-Visual Scenarios"}, {"paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "c3d086d0f50ff9efa28d56616ed127547d836e55", "title": "Omnivore: A Single Model for Many Visual Modalities"}, {"paperId": "177e957f5cd93229c9794ea652c646d2557b4a69", "title": "A ConvNet for the 2020s"}, {"paperId": "c10075b3746a9f3dd5811970e93c8ca3ad39b39d", "title": "High-Resolution Image Synthesis with Latent Diffusion Models"}, {"paperId": "21ec90872abd986c12afe39bebe807732ffa70c9", "title": "Florence: A New Foundation Model for Computer Vision"}, {"paperId": "e1a3e6856b6ac6af3600b5954392e5368603fd1b", "title": "Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"}, {"paperId": "cb8dcaf8e5fe7256577c6bc83e11dd64d8f3ae31", "title": "Towards artificial general intelligence via a multimodal foundation model"}, {"paperId": "848eb8367785910c2fe31372605954ad8f9dfe6c", "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video"}, {"paperId": "f1a66139fa051370018a3539f15e60f728e437ca", "title": "Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans"}, {"paperId": "821ad6c9f0fecb5fabb486a5a87a93b7ea65bcc0", "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding"}, {"paperId": "5e00596fa946670d894b1bdaeff5a98e3867ef13", "title": "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"}, {"paperId": "b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1", "title": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"}, {"paperId": "fda4530df9eec0e3f714dba3459ac50dab17d89c", "title": "Audioclip: Extending Clip to Image, Text and Audio"}, {"paperId": "70b4c724a0f22198f9a04f504b1b298299e4cc37", "title": "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation"}, {"paperId": "63c74d15940af1af9b386b5762e4445e54c73719", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models"}, {"paperId": "f0524b3005720bcff886bcb0227f7f0dd924ff07", "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "5faa6624e8f0053ca28a9c7c564cd377bba109f1", "title": "Slow-Fast Auditory Streams for Audio Recognition"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "1b055049c568be70d6a762679cdb93f630d5d6e6", "title": "Tabular Transformers for Modeling Multivariate Time Series"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "ddaea2321d29952a4c72a8377bda7520362a9044", "title": "Reliable Tuberculosis Detection Using Chest X-Ray With Deep Learning, Segmentation and Visualization"}, {"paperId": "10d11f0045dc7f217c7f01bc6cbb47929e9b8808", "title": "Self-Supervised MultiModal Versatile Networks"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "b5ef0f91663f0cbd6910dec9a890c138f7ec10e0", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "add2f205338d70e10ce5e686df4a690e2851bdfc", "title": "Momentum Contrast for Unsupervised Visual Representation Learning"}, {"paperId": "2b37cc68c9819cf0f980676935007d4135d8ac8c", "title": "Clotho: an Audio Captioning Dataset"}, {"paperId": "4aa6298b606941a282d735fa3143da293199d2ca", "title": "VL-BERT: Pre-training of Generic Visual-Linguistic Representations"}, {"paperId": "4f2c1af57c056102806a184517313804f66e7447", "title": "ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering"}, {"paperId": "8a1744da011375d711ed75fc2d160c6fdca2cf89", "title": "Deep Modular Co-Attention Networks for Visual Question Answering"}, {"paperId": "c4798919e74411d87f7745840e45b8bcf61128ff", "title": "AudioCaps: Generating Captions for Audios in The Wild"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "af1f7739283bdbd2b7a94903041f6d6afd991907", "title": "Towards VQA Models That Can Read"}, {"paperId": "28b74bb7c8b08cceb2430ec2d54dfa0f3225d796", "title": "VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "c484af87a09e20ef0206f1838fe8124e94e8094c", "title": "Time Series"}, {"paperId": "a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"}, {"paperId": "057b80e235b10799d03876ad25465208a4c64caf", "title": "Video Question Answering via Gradually Refined Attention over Appearance and Motion"}, {"paperId": "4060f7d9f58afd5b0e0e9f94e4838f5267e61ff0", "title": "RGB-Infrared Cross-Modality Person Re-identification"}, {"paperId": "ee909ad489244016cf301bb7d7d8eeea423dbf35", "title": "Localizing Moments in Video with Natural Language"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6", "title": "The Kinetics Human Action Video Dataset"}, {"paperId": "96dd1fc39a368d23291816d57763bc6eb4f7b8d6", "title": "Dense-Captioning Events in Videos"}, {"paperId": "5ba2218b708ca64ab556e39d5997202e012717d5", "title": "Audio Set: An ontology and human-labeled dataset for audio events"}, {"paperId": "e10a5e0baf2aa87d804795af071808a9377cc80a", "title": "Towards Automatic Learning of Procedures From Web Instructional Videos"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "154c22ca5eef149aedc8a986fa684ca1fd14e7dc", "title": "Movie Description"}, {"paperId": "05f3f8f6f97db00bafa2efd2ac9aac570603c0c6", "title": "TGIF: A New Dataset and Benchmark on Animated GIF Description"}, {"paperId": "b8e2e9f3ba008e28257195ec69a00e07f260131d", "title": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language"}, {"paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "c1994ba5946456fc70948c549daf62363f13fa2d", "title": "Indoor Segmentation and Support Inference from RGBD Images"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "4059fca710965cdcbb72d0ca9c05d77e99d5ace3", "title": "Multimedia learning"}, {"paperId": "f329ae90bb7e6655b8ca3a19c4d06dbed7792d6f", "title": "InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding"}, {"paperId": "5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a", "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"}, {"paperId": "ecce44df1956db4ec486539c6543345344809958", "title": "Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "828b08fcb29d019a6166492b48f34447f46bf06e", "title": "Modality Synergy Complement Learning with Cascaded Aggregation for Visible-Infrared Person Re-Identification"}, {"paperId": "acf87283fa8ae426f1a4987b345b401bf2913f61", "title": "Do Transformers Really Perform Badly for Graph Representation?"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "8b55402ffee2734bfc7d5d7595500916e1ef04e8", "title": "nocaps: novel object captioning at scale"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Question Answering \u2022 VQAv2 [79"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "general-purpose vision-language models"}, {"paperId": null, "title": "Gemini: a family of highly capable multimodal models"}]}