{"paperId": "2a38daf98d506477f8180806f503409d5036eaf4", "title": "TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer", "abstract": "We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanisms, tensor normalization, and inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism for smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over $20\\%$. Furthermore, we develop a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. We also implement an efficient model parallel schema for TransNormerLLM, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, i.e., LLMs with 175B parameters. We validate our model design through a series of ablations and train models with sizes of 385M, 1B, and 7B on our self-collected corpus. Benchmark results demonstrate that our models not only match the performance of state-of-the-art LLMs with Transformer but are also significantly faster. Code is released at: https://github.com/OpenNLPLab/TransnormerLLM.", "venue": "", "year": 2023, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "TransNormerLLM is presented, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency and develops a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length."}, "embedding": {"model": "specter_v2", "vector": [-0.018765956163406372, 0.5206958651542664, -0.5616719126701355, -0.05924754589796066, -0.3631715476512909, -0.2036779373884201, 0.8507348895072937, -0.344892293214798, -0.4318296015262604, -0.28495413064956665, 0.8458875417709351, -0.06360632181167603, 0.689556896686554, 0.3060692548751831, -0.26916465163230896, 0.09361772984266281, -0.88153475522995, 0.2833011746406555, -0.39164069294929504, -0.40286803245544434, 0.025229955092072487, -0.6859753131866455, -0.852709949016571, 0.07828625291585922, 0.2610921263694763, 0.39358755946159363, 0.1371288150548935, 0.7279886603355408, -0.47145533561706543, 0.4220837950706482, 0.4897977113723755, -0.20717942714691162, 0.013954528607428074, 0.12734921276569366, -0.36663782596588135, -0.2930305302143097, 0.2999740242958069, -0.1212235614657402, -0.20165695250034332, 0.7760691046714783, -0.3975931704044342, 0.4669443964958191, 0.2553090453147888, -0.5028333067893982, -0.695370614528656, 1.0813429355621338, 0.37007224559783936, 0.5664190649986267, -0.35222187638282776, -0.46199482679367065, 1.3313686847686768, -1.7113823890686035, 0.21344605088233948, 1.4355707168579102, 0.39743348956108093, 0.24151670932769775, -0.28139248490333557, -0.5870709419250488, 0.8667448163032532, -0.0022302481811493635, -0.9511619806289673, -0.49556273221969604, -0.10633965581655502, -0.14921419322490692, 2.007488489151001, -0.20059959590435028, 0.19006788730621338, 0.632139265537262, -0.263214111328125, 1.413771390914917, -0.1963053047657013, -0.7531605362892151, -0.3174660801887512, -0.014470349997282028, 0.4044709801673889, 0.3863859176635742, -0.44654783606529236, 0.22513513267040253, -0.8029071688652039, -0.041019778698682785, 0.19038692116737366, 0.034574948251247406, 0.24202825129032135, 0.2927054762840271, -0.516896665096283, 0.5636194944381714, 0.2091372013092041, 1.1452531814575195, -0.47452259063720703, 0.9925581216812134, 0.7283799052238464, 0.13955038785934448, 0.31443238258361816, 0.24889548122882843, -0.2883066236972809, -0.09505394846200943, -1.2021217346191406, 0.2942715287208557, -0.05322566255927086, 0.8002259731292725, -0.16263766586780548, 0.6645599007606506, -0.8049061298370361, -0.04176855832338333, 1.231199860572815, 0.42723873257637024, 0.43153080344200134, -0.7902186512947083, 0.25692418217658997, -0.6761220693588257, -0.20424528419971466, -0.5223293304443359, -0.16505032777786255, -0.38463878631591797, -0.8397032022476196, -1.415863037109375, -0.7124723792076111, 0.1958344727754593, -0.7467169761657715, 0.8225364685058594, -0.34143391251564026, -0.0621052086353302, -0.013030923902988434, 0.3323613405227661, 0.6364918351173401, 0.798695981502533, 0.3800695538520813, 0.20297572016716003, 1.0363377332687378, -0.8979807496070862, -0.6626878380775452, -1.3063746690750122, 1.0694156885147095, -0.4548390209674835, 0.24978359043598175, -0.17287851870059967, -1.2685383558273315, -0.7362398505210876, -0.5045133233070374, -0.1099182516336441, -0.6144272685050964, 0.3909749984741211, 1.0081244707107544, 0.39220675826072693, -0.7548510432243347, 0.34220612049102783, -0.527522087097168, -0.017262877896428108, 0.18497563898563385, -0.08572661131620407, 0.3216022551059723, -0.32592296600341797, -1.5687685012817383, 0.36673450469970703, 0.2935561537742615, -0.15620771050453186, -0.022933349013328552, -0.6739786863327026, -1.2219655513763428, -0.12793634831905365, 0.13096410036087036, 0.1061834767460823, 1.295576810836792, 0.0326974056661129, -1.1499888896942139, 0.5746358633041382, -0.5674164295196533, 0.24124030768871307, -0.05722251534461975, -0.3203190565109253, -0.597022533416748, -0.8036161065101624, -0.10732165724039078, 0.4116275906562805, 0.32871872186660767, 0.10890909284353256, -0.07044464349746704, 0.2206508070230484, -0.34034815430641174, -0.23345540463924408, -0.4433347284793854, 1.2206823825836182, -0.34971094131469727, -0.13359490036964417, -0.1181003674864769, 0.6041663885116577, 0.04144326224923134, -0.7192844152450562, -0.4428550899028778, -1.2328308820724487, 0.7398560643196106, -0.08716395497322083, 0.9912839531898499, -0.9014348387718201, -0.7278691530227661, -0.12006889283657074, -0.004903184715658426, 0.03683749958872795, -0.7432796955108643, 0.638582706451416, -0.7131600379943848, 0.30473586916923523, 0.24246269464492798, -1.312101125717163, 0.3631700873374939, -0.18407265841960907, -0.7162536978721619, -0.08328455686569214, -0.0703355148434639, 1.2914258241653442, -0.843346118927002, 0.03340606018900871, -0.0009983540512621403, 0.5753871202468872, -1.0163811445236206, 1.1183351278305054, -0.3194979131221771, 0.16422103345394135, 0.008260448463261127, -0.1624372899532318, 0.001812121132388711, -0.41556262969970703, 0.266716331243515, -0.48751235008239746, -0.4522998631000519, 0.5019127130508423, -0.3225451111793518, 1.236620545387268, -0.6372857093811035, 0.6448017358779907, -0.07293543219566345, -0.6758841276168823, 0.223446786403656, 0.45821160078048706, 0.03478517755866051, -0.5881157517433167, 0.3085099160671234, 0.5694851875305176, -0.6894075274467468, -0.01584964618086815, 0.9410918354988098, 0.7231799364089966, -0.42540547251701355, 0.14036059379577637, 0.4024043083190918, 0.13123567402362823, 0.6499884128570557, 0.5173181891441345, 0.3549160361289978, 0.6070498824119568, 0.19953417778015137, -0.1884765475988388, 0.22765670716762543, -0.7739623785018921, 0.04185989871621132, 0.3915203809738159, 0.7870609760284424, 0.5299794673919678, 0.41425129771232605, -0.7202432155609131, -0.49395304918289185, 0.5366645455360413, 0.5705615282058716, 1.7685362100601196, -0.398366779088974, -0.3563428223133087, -0.46463102102279663, 0.07715864479541779, -0.3839839994907379, 0.1645965427160263, -0.2440086454153061, 0.04415322467684746, -1.0595788955688477, -1.0838016271591187, 0.7817180156707764, 0.27750542759895325, 0.5372440218925476, -0.6557148694992065, -0.09982229024171829, -0.44397300481796265, -0.1146271824836731, -0.9679328203201294, -0.777320921421051, 0.22614756226539612, -0.12586310505867004, 0.3103798031806946, -0.10387611389160156, 0.03977770358324051, 0.1657547801733017, -1.0872230529785156, 1.0140893459320068, -0.7381214499473572, -0.21759308874607086, 0.0331735797226429, 0.4474388659000397, -0.32759273052215576, -0.7286487221717834, 0.5680558085441589, 0.3883732557296753, 0.009456765837967396, 0.45791175961494446, 0.6432892084121704, 0.06962446123361588, -0.17750735580921173, -0.32113879919052124, 0.2938748896121979, 0.02408970519900322, -0.2815445363521576, 0.4322353005409241, -0.5939268469810486, -0.0359208807349205, -1.1491037607192993, 0.7073351144790649, 0.02463439479470253, -0.4788517653942108, 0.025780094787478447, -0.6235402226448059, -0.3652467727661133, 0.5344876646995544, -0.7072067260742188, -0.3451135754585266, -0.9604179263114929, 0.2463667094707489, -0.22654835879802704, -0.039500296115875244, 0.42539554834365845, 0.4218159317970276, 0.45824897289276123, 0.11130732297897339, 0.5840948820114136, -0.06823325902223587, -0.36927953362464905, 0.780881404876709, -0.6889219880104065, 0.3639789819717407, 0.19844602048397064, 0.27314484119415283, -0.22974947094917297, -0.23955973982810974, -0.8463730812072754, -0.5313353538513184, -0.36366137862205505, -0.10213622450828552, -0.33707892894744873, 0.34956714510917664, -0.529732882976532, -1.2559138536453247, -0.16212980449199677, -1.3354716300964355, -0.16519352793693542, 0.14429914951324463, 0.033411115407943726, 0.12743201851844788, -0.9610905051231384, -1.2976493835449219, -0.719706118106842, -0.7659395337104797, -1.037703275680542, 0.45765385031700134, -0.18094883859157562, -0.4883715808391571, -0.8485941290855408, 0.08565578609704971, -0.364946573972702, 1.2616993188858032, -0.5865744948387146, 0.5092884302139282, -0.06667367368936539, -0.26770660281181335, -0.5186548233032227, 0.17748107016086578, 0.705822765827179, -0.4137641489505768, 0.3721033036708832, -0.66753089427948, 0.07457561045885086, -0.5131274461746216, -0.32897689938545227, 0.04864632710814476, 0.3836711347103119, 0.5769795179367065, -0.00572215486317873, -0.5825499892234802, 0.2470351606607437, 1.0910190343856812, -0.7934567332267761, 0.24861246347427368, 0.10766175389289856, 1.2899081707000732, 0.12668980658054352, -0.4790928065776825, 0.5953196883201599, 0.5356208086013794, 0.36471566557884216, 0.10685327649116516, -0.42423176765441895, 0.0161516722291708, -0.4330764710903168, 0.734636664390564, 2.1052045822143555, 0.01641474850475788, -0.028632311150431633, -1.103373646736145, 0.8222933411598206, -1.1287994384765625, -0.8381998538970947, 0.4208831489086151, 0.5142577290534973, 0.47140824794769287, -0.6291794776916504, -0.526651918888092, -0.4626200199127197, 0.3615121841430664, 0.48688748478889465, -0.25901269912719727, -0.923406720161438, -0.05526774376630783, 0.11592645198106766, 0.16321399807929993, 0.7375798225402832, -0.02124985121190548, 0.8208276033401489, 14.774514198303223, 0.9172635674476624, -0.09521611034870148, 0.7681663632392883, 0.6951739192008972, 0.06368568539619446, -0.327144593000412, -0.10607361793518066, -1.540753960609436, -0.13292770087718964, 1.2312085628509521, -0.0026036950293928385, 0.7323302626609802, 0.15378352999687195, 0.43627941608428955, 0.5532046556472778, -0.4373292624950409, 0.7040888667106628, 0.6473406553268433, -1.1758230924606323, 0.21673111617565155, 0.18502740561962128, 0.4391906261444092, 0.9133078455924988, 0.6937481164932251, 1.1132866144180298, 0.6151530742645264, -0.47208172082901, 0.47851571440696716, 0.5372190475463867, 0.929212749004364, -0.024006469175219536, 0.5073930025100708, 0.5538788437843323, -1.0564707517623901, 0.045979760587215424, -0.35235241055488586, -1.4014580249786377, 0.31975457072257996, 0.45267313718795776, -0.1339898407459259, -0.5818780064582825, -0.26894277334213257, 0.934746265411377, 0.2617829442024231, 0.06674430519342422, -0.05946997553110123, 0.8030908107757568, -0.07382729649543762, -0.06629664450883865, 0.42534536123275757, 0.4522477686405182, 0.21751542389392853, 0.3460571765899658, 0.030729563906788826, -0.24598415195941925, 0.2819337248802185, 0.7925390005111694, -0.6263764500617981, 0.0704391598701477, -0.0025782014708966017, -0.5914912223815918, -0.025968121364712715, 1.0254939794540405, 0.6941072344779968, -0.013918277807533741, -0.35340920090675354, -0.0028836040291935205, 0.5590103268623352, 0.3976494073867798, -0.41691651940345764, -0.26105791330337524, 0.36602309346199036, -0.3503817915916443, -0.07265275716781616, 0.4178813099861145, -0.06456547230482101, -0.7198106050491333, -0.5084713101387024, -0.3202245235443115, 0.028495311737060547, -0.6870490312576294, -0.6969009637832642, 1.0812997817993164, -0.30433833599090576, -0.19241659343242645, -0.021584030240774155, -0.6804875135421753, 0.017369478940963745, 0.84183269739151, -1.4075294733047485, -0.9155781269073486, 0.49973413348197937, -0.4704073667526245, -0.48111990094184875, 0.14243201911449432, 1.2652034759521484, 0.7051909565925598, -0.6731317639350891, -0.0006416200776584446, 0.15511468052864075, 0.08693446218967438, -0.3033083975315094, -0.5557172894477844, 0.9648662209510803, 0.27696532011032104, -0.4187711179256439, 0.4651131331920624, 0.2302398830652237, 0.14216972887516022, -0.8658173680305481, -0.20240148901939392, 1.2476556301116943, -0.9822171926498413, -0.04508339986205101, -1.1676605939865112, -0.7598331570625305, 0.6332763433456421, 0.6364343166351318, -0.12610241770744324, 0.2228555828332901, 0.48408254981040955, -0.5782131552696228, -0.18156495690345764, -0.3257875442504883, -0.06426741927862167, 0.12413926422595978, -0.7476102709770203, -0.19461214542388916, 0.20003025233745575, 0.5342899560928345, -0.988981306552887, -0.7954103946685791, -0.4004329741001129, 0.16394831240177155, 0.38502001762390137, 0.8516395688056946, -0.2797212302684784, 0.7157357931137085, 0.8715611696243286, -0.24127383530139923, -0.688854455947876, -0.017840668559074402, -1.0442708730697632, -0.16755075752735138, 0.29726019501686096, 0.6877291798591614, -0.32841962575912476, 0.1953342705965042, 0.833207368850708, 0.193760946393013, -0.5490315556526184, -0.5758488774299622, -0.5420008301734924, 0.14355386793613434, -0.7752890586853027, 0.39530041813850403, 0.04810301586985588, 0.25764766335487366, 0.11204443126916885, 0.1176447719335556, 0.28693887591362, -0.04886387288570404, -0.7351527810096741, 0.0934220626950264, -0.003934640437364578, 0.07789472490549088, -0.5352580547332764, -0.5734202861785889, -1.4538592100143433, 0.18890327215194702, -1.3294183015823364, -0.08069092780351639, -1.045181393623352, -0.20342281460762024, 0.004042401909828186, -0.049368176609277725, 0.43414106965065, 0.4484160840511322, -0.01832372322678566, -0.42759814858436584, -0.6419762372970581, -0.6765296459197998, 1.14859139919281, 0.9527925848960876, -0.6008522510528564, 0.21463067829608917, -0.25919193029403687, 0.2613418400287628, 0.27765563130378723, 0.17857682704925537, -0.5161106586456299, -0.36929336190223694, -1.4254891872406006, 0.8001812696456909, -0.27439820766448975, -0.14817646145820618, -0.3173357844352722, 0.5143839120864868, 0.39960792660713196, -0.28429755568504333, -0.005015433765947819, 0.27423861622810364, -0.5238979458808899, -0.659652054309845, 0.3000180721282959, -0.8498988151550293, 0.47317421436309814, 0.21474048495292664, -1.0159393548965454, -0.29291337728500366, 0.8514823317527771, -0.25839608907699585, -0.9068534970283508, -0.5082102417945862, 0.6248316764831543, -0.8515490293502808, 0.3059779703617096, -0.5006188750267029, -0.07935705780982971, -1.208893895149231, -0.35661423206329346, -0.08566740900278091, 0.3675163686275482, -0.7640007138252258, 1.2095820903778076, 0.22820092737674713, -1.2704758644104004, -0.25029751658439636, 0.3078494966030121, 0.021073702722787857, -0.3712072968482971, 0.45866858959198, 0.5896091461181641, -0.11243298649787903, 0.7965434193611145, 0.6546282172203064, 0.28535398840904236, -0.8706294894218445, 0.013341127894818783, 0.6865702271461487, -0.408083975315094, -0.2129131704568863, 1.06696617603302, -0.32079213857650757, -1.3753938674926758, 0.1358184516429901, -1.3115190267562866, -0.5140644907951355, -0.5236796140670776, 0.75977623462677, 0.22927220165729523, -0.023067491129040718, -0.3923534154891968, -0.5928687453269958, 0.006277022417634726, -0.13534760475158691, -0.3816867768764496, 0.7028757929801941, -0.170058935880661, -0.7770549058914185, 0.6748811602592468, 1.1160955429077148, -0.6225326061248779, -0.1925216168165207, -0.7499229311943054, -0.39156731963157654, 0.23865637183189392, 0.6186556816101074, -0.34726548194885254, -0.6727586984634399, 0.6962465643882751, 0.4439997673034668, 0.5068790316581726, -0.1698143184185028, -0.10185940563678741, 0.38151922821998596, 0.6477524042129517, -0.1300983875989914, -0.8936566114425659, -0.760215699672699, 1.710583209991455, 1.0964763164520264, -0.7557949423789978, 0.015948541462421417, 0.12817426025867462, -0.735894501209259, 0.6728762984275818, 0.16019286215305328, 0.2552514374256134, 0.8781988024711609, 0.07929430902004242, 0.012960896827280521, -0.001842506811954081, -1.008367896080017, -0.18235363066196442, 0.6978228688240051, 0.5004432201385498, 1.3058804273605347, 0.4105643630027771, 0.07885012030601501, 0.5578927397727966, 0.18143172562122345, 0.2997535765171051, 0.2794188857078552, 0.2877965271472931, -0.0713195651769638, -0.03226421773433685, -0.058739688247442245, 0.4622515141963959, -0.6404849290847778, -0.9786363244056702, 0.45256465673446655, 0.25629234313964844, 0.3944074213504791, 0.5003938674926758, 0.9558507800102234, 0.4379842281341553, 0.5041384100914001, 0.09759176522493362, 0.2553940415382385, -0.33679354190826416, -0.2890426516532898, -0.1999039500951767, -0.638379693031311, -0.17413200438022614, -0.0971420407295227, -0.4058896005153656, -0.5466952919960022, -0.5280503630638123, 0.38846921920776367, -0.07776941359043121, 0.5608581900596619, 1.3383519649505615, 0.5435566306114197, 0.6926435232162476, -0.41678375005722046, -0.6379489302635193, -0.18460272252559662, -1.0423356294631958, -0.24785639345645905, -0.34224221110343933, -0.13914255797863007, 0.13864262402057648, -0.10810720920562744, -0.2858564853668213]}, "authors": [{"authorId": "2171650015", "name": "Zhen Qin"}, {"authorId": "2179703418", "name": "Dong Li"}, {"authorId": "2225238340", "name": "Weigao Sun"}, {"authorId": "8397429", "name": "Weixuan Sun"}, {"authorId": "2116517206", "name": "Xuyang Shen"}, {"authorId": "2118234357", "name": "Xiaodong Han"}, {"authorId": "2156252901", "name": "Yunshen Wei"}, {"authorId": "2154762987", "name": "Baohong Lv"}, {"authorId": "40247395", "name": "Fei Yuan"}, {"authorId": "2168258085", "name": "Xiao Luo"}, {"authorId": "145858545", "name": "Y. Qiao"}, {"authorId": "2015152", "name": "Yiran Zhong"}], "references": [{"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "8bc8b9ae855bc0aa19e7223899440ffbdc61f4d8", "title": "Linearized Relative Positional Encoding"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "f35f5aedc30e2c5ded210d9c91ba6e84bd029425", "title": "Toeplitz Neural Network for Sequence Modeling"}, {"paperId": "a0e7c31d723608e03f30fc92ffc2a604a7a039da", "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"}, {"paperId": "f393aff1593c2d370ec0ae004910d18e40524967", "title": "Resurrecting Recurrent Neural Networks for Long Sequences"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "54155c2977a977bf129849455dcae3a2b79b3f41", "title": "Simple Hardware-Efficient Long Convolutions for Sequence Modeling"}, {"paperId": "ac608a4a6b19b3208e560eee5daadb3cc18638a2", "title": "Efficient Attention via Control Variates"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "86c8d930b492a4f9cadc6c60aecdaaded49acc86", "title": "Neural Architecture Search on Efficient Transformers and Beyond"}, {"paperId": "ca444821352a4bd91884413d8070446e2960715a", "title": "On the Parameterization and Initialization of Diagonal State Space Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "1944cebf4e41a10ea7bd02ce30404c18c9c4e04f", "title": "Linear Complexity Randomized Self-attention Mechanism"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "e65c84e2778d7b13b7541e6b14ff790b624a24ec", "title": "A Study of BFLOAT16 for Deep Learning Training"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5c3391bde2bb1b3d737913ee8caa01492a782732", "title": "WHO Technical Report"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": "8b31a1fd9d1a9da66f6fce60857f170b693bd916", "title": "[Workshop]."}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Socialiqa: Commonsense reasoning about social interactions"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": null, "title": "A method for stochastic optimization"}, {"paperId": null, "title": "General language model pretraining with autoregressive blank"}, {"paperId": null, "title": "Training compute-optimal"}, {"paperId": null, "title": "Falcon-40b: an open large language model with state-of-the-art performance"}, {"paperId": null, "title": "Scaling language models: Methods,"}, {"paperId": null, "title": "multi-level multi-discipline"}, {"paperId": null, "title": "A suite for analyzing large"}, {"paperId": null, "title": "Introducing mpt-7b: A new standard for open-source, commercially"}, {"paperId": null, "title": "Openllama: An open reproduction of llama"}, {"paperId": null, "title": "hungry hippos: Towards language modeling with state space models"}]}