{"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers", "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.", "venue": "Neural Information Processing Systems", "year": 2020, "citationCount": 885, "influentialCitationCount": 137, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work presents a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation, and demonstrates that the monolingual model outperforms state-of-the-art baselines in different parameter size of student models."}, "embedding": {"model": "specter_v2", "vector": [0.15625977516174316, 1.143729567527771, -0.15517474710941315, 0.2906721234321594, -0.6542338132858276, -0.5919645428657532, 0.7544614672660828, 0.19808010756969452, -0.19793595373630524, -0.1393379122018814, 0.46688634157180786, -0.32844623923301697, 0.5129225254058838, 0.006963880266994238, -0.4272928237915039, 0.25609076023101807, -0.29949548840522766, 0.5136670470237732, 0.17539604008197784, -0.3610265851020813, -0.21151068806648254, -0.8984291553497314, -0.8023531436920166, -0.12429383397102356, 1.0252450704574585, 0.5962632298469543, 0.32435768842697144, 0.5910125374794006, -0.6866259574890137, 0.14343903958797455, 0.14440158009529114, -0.5079957246780396, 0.23116932809352875, 0.09149179607629776, -0.4109034836292267, -0.07203704863786697, 0.5951732993125916, -0.2142968624830246, -0.3780725598335266, 0.9345109462738037, -0.12447585165500641, -0.008745781145989895, 0.28935250639915466, -0.4570627212524414, -0.8178349733352661, 1.3044921159744263, 0.9185593128204346, 0.8083332777023315, -0.17573796212673187, -0.5460038781166077, 0.9078110456466675, -1.0654634237289429, -0.20895370841026306, 0.9302719831466675, 0.26554134488105774, 0.48326027393341064, -0.5544201731681824, -0.5532335042953491, 0.25347569584846497, 0.34986910223960876, -0.9894349575042725, -0.05807742476463318, 0.07949533313512802, -0.10777764767408371, 1.7034372091293335, -0.20421133935451508, 0.008418990299105644, 0.1325942575931549, -0.42890721559524536, 1.3656463623046875, -0.04672536253929138, -0.5651485919952393, -0.4817982316017151, 0.5237523913383484, -0.2397109866142273, 0.9334039688110352, -0.4701511859893799, -0.14866885542869568, -0.2986379861831665, 0.10726460814476013, 0.28796637058258057, -0.03310419246554375, 0.06548885256052017, -0.3136216998100281, 0.11637529730796814, 0.6143906116485596, 0.25421375036239624, 0.729809045791626, 0.001366872456856072, 0.44606995582580566, 0.6897622346878052, 0.5170264840126038, 0.3178904950618744, 0.6477087140083313, -0.38946712017059326, 0.26861700415611267, -1.2843129634857178, 0.036325935274362564, 0.1613979935646057, 0.8633022308349609, 0.1659211665391922, 0.2971093952655792, -0.4288836419582367, -0.018934423103928566, 1.1514850854873657, 0.21269071102142334, 1.0862888097763062, -0.6893222332000732, 0.35236573219299316, -0.6456779837608337, -0.5623520016670227, -0.43480145931243896, -0.007115309592336416, -0.36766090989112854, -0.8546332716941833, -1.7944406270980835, -0.6272108554840088, 0.23371650278568268, -0.883902907371521, 0.8666538596153259, -0.3171173930168152, 0.22176966071128845, 0.31263530254364014, 0.19599422812461853, 0.7690718770027161, 0.5621572136878967, 0.23502831161022186, 0.016128920018672943, 1.0852833986282349, -0.8648175597190857, -0.36363252997398376, -1.0571004152297974, 0.6285305619239807, -0.16705848276615143, 0.25320765376091003, -0.021121354773640633, -1.6162291765213013, -0.9012822508811951, -0.7058987617492676, 0.026944322511553764, -0.6141601800918579, -0.009602556005120277, 0.8286949396133423, 0.4239955544471741, -0.9639712572097778, 0.8187739253044128, -0.28186386823654175, -0.4476093053817749, 0.37435442209243774, 0.015177953988313675, -0.09484553337097168, -0.24076002836227417, -1.353460431098938, 0.3710007965564728, 0.3207358121871948, -0.6185885667800903, -0.29183512926101685, -0.8737540245056152, -1.3333156108856201, 0.1483503133058548, 0.36998817324638367, -0.3451458215713501, 1.500840425491333, 0.018245410174131393, -1.0949338674545288, 0.9807820320129395, -0.09999442845582962, -0.2872755825519562, 0.2209385186433792, -0.15610159933567047, -0.1152450367808342, -0.47084635496139526, 0.1359683722257614, 0.319551020860672, 0.48845505714416504, 0.09874969720840454, -0.013575868681073189, 0.27336975932121277, -0.1756841391324997, -0.17661023139953613, -0.30575263500213623, 0.5541430115699768, -0.6672696471214294, -0.3459562659263611, 0.4352295398712158, 0.9288373589515686, 0.23949091136455536, -0.6211744546890259, -0.8241955041885376, -1.2397187948226929, 0.7002964019775391, 0.13468024134635925, 0.9283856153488159, -0.7970820069313049, -0.6614014506340027, -0.280824214220047, 0.07396721839904785, 0.505387544631958, -0.9181321859359741, 0.511091947555542, -0.6207143068313599, 0.6061758995056152, -0.24164433777332306, -1.1111005544662476, 0.06261245906352997, -0.008476581424474716, -0.8138352632522583, -0.47494328022003174, 0.03946230560541153, 1.2489656209945679, -0.7553014755249023, -0.02266726829111576, 0.13298043608665466, 0.28474438190460205, -1.1891834735870361, 1.4447423219680786, -0.6581525802612305, 0.3455040752887726, -0.007034228183329105, 0.03600772097706795, 0.49781206250190735, -0.2675761878490448, 0.16429297626018524, -0.3660200536251068, -0.11857409030199051, 0.9697906374931335, -0.10596959292888641, 1.53301203250885, -0.3274540901184082, 0.44267168641090393, 0.004004171583801508, -1.3140124082565308, 0.32612335681915283, 0.3107340931892395, 0.020813586190342903, -0.6988661885261536, 0.1254158318042755, 0.41773703694343567, -0.6081432104110718, 0.05150647088885307, 0.7991321086883545, 0.783632755279541, -0.2894783616065979, 0.16484853625297546, 0.9626179337501526, -0.8144128918647766, 0.6933172345161438, 0.4641640782356262, 0.6291776895523071, 0.24286751449108124, 0.21284453570842743, 0.08363562822341919, 0.5363785028457642, -0.8882824778556824, -0.2141105681657791, 0.1379428654909134, 0.7016061544418335, 0.7012481093406677, 0.7254117131233215, -0.512313723564148, -0.3122562766075134, -0.06572768837213516, 0.5683563947677612, 1.3107874393463135, -0.573328971862793, -0.4852274954319, -0.9798154234886169, -0.4996156096458435, -0.378226637840271, 0.01340214442461729, -0.2740890383720398, -0.24980786442756653, -0.6315442323684692, -0.9653452634811401, 0.7605816721916199, 0.08101660013198853, 1.2116578817367554, -0.21538038551807404, -0.20152628421783447, -0.1204603835940361, 0.23272697627544403, -0.837694525718689, -0.5169467329978943, 0.6472227573394775, -0.48916858434677124, -0.36782947182655334, -0.07895824313163757, -0.47310012578964233, 0.1700521558523178, -0.7337955832481384, 1.0221848487854004, -0.9456409811973572, -0.5085934400558472, 0.2981758117675781, 0.5081791281700134, -0.593355119228363, -0.6119439005851746, 0.5726306438446045, -0.025441745296120644, -0.5902478694915771, 0.40317994356155396, -0.10660889744758606, -0.08940967917442322, -0.16476020216941833, -0.7276149392127991, -0.15405039489269257, -0.1080319732427597, -0.2849835157394409, 0.8500036001205444, -0.0302243884652853, -0.39810246229171753, -1.373255968093872, 0.9678950905799866, 0.1569530963897705, -0.4525896906852722, -0.05438097566366196, -0.9539097547531128, -0.06898706406354904, 0.6019784808158875, -0.82743901014328, 0.11361134797334671, -0.963978111743927, 0.2576858401298523, -0.27022817730903625, 0.06668662279844284, 0.238602876663208, 0.07363865524530411, 0.2630074620246887, 0.2158031016588211, 0.6834898591041565, 0.46967244148254395, -0.3768119215965271, 1.0407872200012207, -0.9582065343856812, 0.7234705090522766, 0.12489660829305649, 0.5730225443840027, -0.374013751745224, -0.19021126627922058, -0.43807587027549744, -0.6305778622627258, 0.060247890651226044, 0.1422550082206726, 0.3548036813735962, 0.11164852976799011, -0.8126922249794006, -0.9056645631790161, -0.036077503114938736, -0.7663639187812805, -0.578924834728241, -0.5348326563835144, -0.49394840002059937, -0.44732606410980225, -0.847845196723938, -1.2526814937591553, -0.22177448868751526, -0.8519408702850342, -1.0007317066192627, 0.13392089307308197, 0.24058160185813904, -0.024320995435118675, -0.6881986856460571, 0.13569438457489014, -0.509368360042572, 1.4239126443862915, -1.0307753086090088, 0.5836092233657837, -0.40529897809028625, -0.3440132439136505, -0.08184102177619934, -0.14873626828193665, 0.955367922782898, -0.14428089559078217, -0.07734709233045578, -0.618182897567749, 0.5217957496643066, -0.3472839593887329, -0.5402125716209412, 0.3260168433189392, 0.26824304461479187, 0.6849353909492493, -0.5515031814575195, -0.298708438873291, 0.9400674104690552, 1.3950945138931274, -0.9335817694664001, 0.3913339376449585, 0.3991547226905823, 0.8616335391998291, -0.06186675280332565, -0.4427059292793274, 0.19379572570323944, 0.9862426519393921, -0.017501723021268845, 0.5002210140228271, -0.17201542854309082, -0.02281760983169079, -0.7001160979270935, 0.13910457491874695, 1.7243136167526245, 0.5359993577003479, 0.15848200023174286, -0.7590292096138, 0.6518774628639221, -0.8854942321777344, -0.4261147379875183, 0.8740252256393433, 0.5383855104446411, 0.7921866178512573, -0.5761498212814331, -0.5901461839675903, -0.11579510569572449, -0.0713726133108139, 0.30045247077941895, -0.3014585077762604, -0.9254686236381531, -0.07359156012535095, 0.8852853775024414, 0.5228497982025146, 0.48788899183273315, -0.33635732531547546, 1.0463016033172607, 14.6703462600708, 0.8281399607658386, -0.14929579198360443, 0.7771467566490173, 0.4924146831035614, 0.618536114692688, -0.36290109157562256, -0.10393518209457397, -1.2137563228607178, -0.16073347628116608, 0.9618003964424133, 0.3700861632823944, 0.5305682420730591, 0.23714229464530945, -0.3673401474952698, 0.09990637749433517, -0.25631821155548096, 0.5503991842269897, 0.8608418107032776, -0.817711353302002, 0.09933923184871674, 0.161293625831604, 0.24653127789497375, 0.4640883505344391, 1.1359875202178955, 1.0819212198257446, 0.608906090259552, -0.03426996245980263, 0.17800189554691315, 0.44571688771247864, 1.2278964519500732, 0.25220564007759094, 0.19170799851417542, 0.40323033928871155, -0.4947701096534729, -0.37911298871040344, -0.6688663363456726, -0.9031107425689697, 0.1807524412870407, 0.283805251121521, -0.24027778208255768, -0.9646720886230469, -0.038154829293489456, 0.4244815707206726, 0.007082612719386816, 0.2971106469631195, -0.47060155868530273, 0.5185999274253845, -0.18224911391735077, 0.44332560896873474, 0.3254047632217407, 0.42762625217437744, 0.15436667203903198, -0.0655372142791748, 0.3609188199043274, -0.1256425529718399, 0.03567257523536682, 0.7046825885772705, -0.5486677289009094, -0.22862470149993896, 0.10045696049928665, -0.4147029221057892, -0.16065213084220886, 0.9511486291885376, 0.9823227524757385, -0.05270735174417496, -0.10162453353404999, 0.7071141600608826, 0.5599481463432312, 0.04995732009410858, -0.2695451080799103, -0.34428152441978455, 0.37642064690589905, 0.029638852924108505, 0.17434188723564148, 0.5513227581977844, 0.06172632426023483, -0.5061858892440796, -0.8075948357582092, -0.29520463943481445, 0.3027750253677368, -1.025927186012268, -0.30674394965171814, 1.0992794036865234, -0.4705769717693329, -0.4035424292087555, -0.20977364480495453, -0.6631671190261841, -0.19843904674053192, 0.5180201530456543, -1.614861011505127, -0.5929179191589355, 0.30737507343292236, -0.41240161657333374, -0.5118351578712463, -0.3105180263519287, 1.4326109886169434, 0.27250194549560547, -0.3314809203147888, 0.3254247307777405, 0.04504653066396713, -0.19427436590194702, 0.16847671568393707, -1.055177927017212, 0.9069542288780212, -0.047613002359867096, -0.34538841247558594, 0.02724778652191162, 0.2668156623840332, 0.4117980897426605, -0.709129810333252, 0.08974432945251465, 0.9047983884811401, -0.7959097623825073, -0.10282618552446365, -0.5151700377464294, -0.6129344701766968, 0.24944578111171722, 0.6375414729118347, -0.08499288558959961, 0.38992926478385925, -0.3575596511363983, -0.9278220534324646, -0.19947122037410736, -1.0760337114334106, -0.11657322943210602, 0.21003365516662598, -0.810875654220581, -0.2173115313053131, 0.09199830889701843, 0.7078936100006104, -1.0583322048187256, -0.684170126914978, -0.03963804990053177, -0.29715198278427124, -0.05045399069786072, 1.1626101732254028, -0.19864486157894135, 0.8284980058670044, 1.1295831203460693, -0.06163020804524422, -0.8214473724365234, 0.2978651821613312, -1.2314542531967163, -0.48503267765045166, 0.5949466824531555, 0.6522473692893982, -0.28983446955680847, 0.4639439582824707, 1.1079165935516357, 0.12460010498762131, -0.4887189269065857, -0.3200843632221222, -0.14172720909118652, 0.3285363018512726, -0.7020688652992249, 0.5627935528755188, 0.20401248335838318, 0.238312229514122, 0.12960705161094666, 0.8814888596534729, 0.14026610553264618, -0.04312008246779442, -0.9633402228355408, 0.37243208289146423, -0.18778054416179657, -0.05855162441730499, -0.3682194650173187, -0.7686130404472351, -1.4220916032791138, -0.12658429145812988, -1.258184552192688, -0.22953686118125916, -1.1603583097457886, -0.3995044231414795, -0.09047793596982956, -0.1334356814622879, -0.055286895483732224, 0.43285009264945984, -0.003691962221637368, -0.4583325684070587, -0.6523621678352356, -0.4402800500392914, 0.7504412531852722, 0.7596322298049927, -0.8751652240753174, -0.17430326342582703, 0.03196505084633827, -0.5621184706687927, 0.233761727809906, 0.4065624177455902, -0.19811418652534485, -0.6589165925979614, -1.5132317543029785, 0.36226752400398254, -0.3840791583061218, -0.25584691762924194, -0.6682122349739075, 0.6673163771629333, 0.7123950719833374, -0.5427454113960266, 0.04520242661237717, 0.19851556420326233, -0.9467346668243408, -1.1007895469665527, 0.15429680049419403, -0.36949867010116577, 0.12384773045778275, 0.3695262670516968, -0.8310614824295044, -0.2892935872077942, 0.6213351488113403, -0.24455644190311432, -1.3278623819351196, -0.18974077701568604, 0.48470523953437805, -0.6923450231552124, 0.4123270511627197, -0.6600412130355835, -0.08585143089294434, -1.0724509954452515, -0.42301124334335327, -0.31476205587387085, 0.29522019624710083, -0.7622119784355164, 0.6221901178359985, -0.17479540407657623, -1.0931589603424072, 0.18174947798252106, 0.6830224394798279, -0.057992078363895416, 0.3675059676170349, 0.4393599331378937, 0.5726826190948486, -0.5242161154747009, 0.6974590420722961, 0.4492400586605072, 0.4426540732383728, -0.3146284818649292, 0.3363417387008667, 0.5467445850372314, -0.7077167630195618, 0.06441418826580048, 1.1735618114471436, -0.4415954649448395, -1.23171865940094, 0.2484198808670044, -1.3716909885406494, -0.30279600620269775, 0.061749886721372604, 0.44130682945251465, 0.048590101301670074, -0.1716877520084381, -0.04951309412717819, -0.1689014732837677, 0.1774628460407257, -0.02099510468542576, -0.4723856449127197, 1.0379592180252075, -0.07866638898849487, -0.632759153842926, -0.05680128186941147, 0.9070327281951904, -0.5929348468780518, -0.4726191461086273, -1.3784071207046509, -0.6334474682807922, 0.3177146017551422, 0.36544209718704224, -0.3659971356391907, -0.8895039558410645, 0.9580140113830566, 0.30041033029556274, 0.5677301287651062, 0.46263065934181213, 0.07947490364313126, 0.4011618196964264, 0.6760205030441284, -0.5037903189659119, -0.5411682724952698, -0.5526577830314636, 1.4847095012664795, 0.8991789221763611, -0.9078788757324219, -0.05676530674099922, -0.2707440257072449, -0.38208144903182983, 0.589771032333374, 0.2926212549209595, 0.025640007108449936, 0.9394999742507935, 0.005105454474687576, -0.03232841566205025, 0.30496060848236084, -0.7497000694274902, -0.8669751286506653, 0.6029137969017029, 1.3390589952468872, 0.6986104249954224, 0.10283456742763519, 0.1578710824251175, 1.1116364002227783, -0.16258418560028076, -0.12190985679626465, 0.33805200457572937, 0.12106294929981232, -0.5237802863121033, -0.2911439538002014, -0.004636475816369057, 0.49308568239212036, -0.858863115310669, -0.9816151261329651, 0.2709139585494995, 0.3918333947658539, 0.3373006284236908, 0.46652477979660034, 1.1829969882965088, -0.17206202447414398, 0.6031132936477661, 0.19192855060100555, 0.4945998787879944, -0.5615013837814331, 0.01749005727469921, -0.2130410075187683, -0.8868852257728577, -0.09479972720146179, 0.01816808432340622, -0.3194001019001007, -0.07446399331092834, -0.15728522837162018, 0.16523759067058563, 0.10773468017578125, 0.8176229596138, 1.0042409896850586, 0.4735647737979889, 0.6561660170555115, -0.2402970939874649, -0.21835258603096008, -0.5049301981925964, -1.2154498100280762, 0.12756043672561646, -0.6188080310821533, -0.393036425113678, 0.22000040113925934, -0.2126532793045044, -0.4602893590927124]}, "authors": [{"authorId": "51456429", "name": "Wenhui Wang"}, {"authorId": "49807919", "name": "Furu Wei"}, {"authorId": "145307652", "name": "Li Dong"}, {"authorId": "10699417", "name": "Hangbo Bao"}, {"authorId": "144610884", "name": "Nan Yang"}, {"authorId": "92660691", "name": "Ming Zhou"}], "references": [{"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "9c5a239b75bade55c830b164e2fadc424e879137", "title": "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models"}, {"paperId": "f64e1d6bc13aae99aab5449fc9ae742a9ba7761e", "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7", "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "2e347a977f14eca7cc5bbbb4c71145b75637340c", "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering"}, {"paperId": "b6b2ebcd2f1b25ca87813832a60ab056c008e31d", "title": "Knowledge Distillation from Internal Representations"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "cc3be86706e0aff342e67b6ab84293fddc98423d", "title": "MobileBERT: Task-Agnostic Compression of BERT by Progressive Knowledge Transfer"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "8f23c219233fb67eedb1e4333c8458edb1c5cd68", "title": "Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering"}, {"paperId": "2f9d4887d0022400fc40c774c4c78350c3bc5390", "title": "Small and Practical BERT Models for Sequence Labeling"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "93ad19fbc85360043988fa9ea7932b7fdf1fa948", "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation"}, {"paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb", "title": "Text Summarization with Pretrained Encoders"}, {"paperId": "335613303ebc5eac98de757ed02a56377d99e03a", "title": "What Does BERT Learn about the Structure of Language?"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "0de0a44b859a3719d11834479112314b4caba669", "title": "A Multiscale Visualization of Attention in the Transformer Model"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "145b8b5d99a2beba6029418ca043585b90138d12", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"}, {"paperId": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89", "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"}, {"paperId": "9f1c5777a193b2c3bb2b25e248a156348e5ba56d", "title": "Cloze-driven Pretraining of Self-attention Networks"}, {"paperId": "425482094edea7deef11287bc27d73b84ee7c32e", "title": "Improved Knowledge Distillation via Teacher Assistant: Bridging the Gap Between Student and Teacher"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "1c3112ef8a346b9817382ed34a8c146c53d5bcf5", "title": "XNLI: Evaluating Cross-lingual Sentence Representations"}, {"paperId": "305b2cf37e5dece81e95c92883d5a6e28ac93b22", "title": "Don\u2019t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization"}, {"paperId": "48fdf50da3d2bbd3b85ea9d17bbf3d173f6164ea", "title": "Attention-Guided Answer Distillation for Machine Reading Comprehension"}, {"paperId": "7af89df3691d8c33aaf1858f7cc51da1bc9549a9", "title": "Bottom-Up Abstractive Summarization"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535", "title": "A Simple Method for Commonsense Reasoning"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "d8b6e965b771e3d0707dd8caa57224a0dfbb886e", "title": "Harvesting Paragraph-level Question-Answer Pairs from Wikipedia"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "title": "Universal Language Model Fine-tuning for Text Classification"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "f7b032a4df721d4ed2bab97f6acd33d62477b7a5", "title": "Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "8604f376633af8b347e31d84c6150a93b11e34c2", "title": "FitNets: Hints for Thin Deep Nets"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "9e85832b04cc3700c2c26d6ba93fdeae39cac04a", "title": "Introduction to the CoNLL-2000 Shared Task Chunking"}, {"paperId": "08ee34a64247c0fe3c22b9f3c0848eb921041a8d", "title": "Supplementary Material: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "3ed3310558a40388d24c77a43d7b3f13eb6e3d3c", "title": "Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks"}, {"paperId": "8ff46c88964a36985f2b45933a3d47b81bd87bd0", "title": "Quora Question Pairs"}, {"paperId": null, "title": "URL http://arxiv.org/abs/1607.06450. 8We use the visualization tool from Vig"}, {"paperId": "0f8468de03ee9f12d693237bec87916311bf1c24", "title": "The Seventh PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "db8885a0037fe47d973ade79d696586453710233", "title": "The Sixth PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "351ec42df2b60c6042addf96e6b98673bbaf4dfd", "title": "The Fourth PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "de794d50713ea5f91a7c9da3d72041e2f5ef8452", "title": "The Third PASCAL Recognizing Textual Entailment Challenge"}, {"paperId": "136326377c122560768db674e35f5bcd6de3bc40", "title": "The Second PASCAL Recognising Textual Entailment Challenge"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}]}