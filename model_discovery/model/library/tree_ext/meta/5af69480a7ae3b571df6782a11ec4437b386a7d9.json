{"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks", "abstract": "The self-attention mechanism is rapidly emerging as one of the most important key primitives in neural networks (NNs) for its ability to identify the relations within input entities. The self-attention-oriented NN models such as Google Transformer and its variants have established the state-of-the-art on a very wide range of natural language processing tasks, and many other self-attention-oriented models are achieving competitive results in computer vision and recommender systems as well. Unfortunately, despite its great benefits, the self-attention mechanism is an expensive operation whose cost increases quadratically with the number of input entities that it processes, and thus accounts for a significant portion of the inference runtime. Thus, this paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self-attention mechanism. Specifically, based on the intuition that not all relations are equal, we devise a novel approximation scheme that significantly reduces the amount of computation by efficiently filtering out relations that are unlikely to affect the final output. With the specialized hardware for this approximate self-attention mechanism, ELSA achieves a geomean speedup of 58.1\u00d7 as well as over three orders of magnitude improvements in energy efficiency compared to GPU on self-attention computation in modern NN models while maintaining less than 1% loss in the accuracy metric.", "venue": "International Symposium on Computer Architecture", "year": 2021, "citationCount": 89, "influentialCitationCount": 15, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self-attention mechanism."}, "embedding": {"model": "specter_v2", "vector": [0.037302013486623764, 0.8465657830238342, -0.33193764090538025, -0.2909466624259949, 0.39874812960624695, 0.1378958821296692, 0.42576950788497925, -0.021207936108112335, -0.7403742671012878, -0.36227262020111084, 0.5009063482284546, 0.5059303045272827, 0.5516579151153564, -0.4538714587688446, -0.40063202381134033, 0.3394709825515747, -0.6226681470870972, 0.11521784961223602, 0.2649458348751068, -0.33217546343803406, 0.006365485955029726, -0.7318636178970337, -1.7561862468719482, -0.025410691276192665, 0.5252604484558105, 0.7678250670433044, 0.08763640373945236, 0.7920578718185425, -0.4371373653411865, 0.41652145981788635, 0.5862907767295837, -0.3106228709220886, 0.3178357183933258, 0.16301098465919495, -0.4169299006462097, -0.6780850291252136, 0.6043452024459839, -0.31330546736717224, -0.5675350427627563, 0.9944896101951599, -0.20544950664043427, 0.4298558831214905, 0.18438424170017242, -0.644731879234314, -0.42126917839050293, 0.9443888664245605, 1.0462875366210938, 0.8013195991516113, -0.4303484857082367, -0.6767659187316895, 1.565935492515564, -1.4509074687957764, 0.35920485854148865, 1.1295356750488281, 0.6359342336654663, 0.2782272398471832, -0.00854521244764328, -0.6930831074714661, 0.2524513900279999, 0.18788979947566986, -0.887768030166626, -0.6365680694580078, 0.06959705054759979, 0.030078183859586716, 2.168728828430176, -0.17258207499980927, -0.15604561567306519, 0.39411285519599915, 0.049962375313043594, 1.287237286567688, 0.022625843062996864, -0.7570652961730957, 0.1660255342721939, 0.09661023318767548, 0.46938207745552063, 0.9976286292076111, -0.24907471239566803, 0.16404660046100616, -1.1278913021087646, -0.23063084483146667, 0.8227653503417969, 0.11891943961381912, 0.5632458925247192, -0.414789617061615, -0.18168702721595764, 0.7388573288917542, 0.5650039911270142, 0.7853660583496094, -0.552291214466095, 0.615296483039856, 0.4082752466201782, 0.13340193033218384, -0.17025963962078094, 0.4986385107040405, 0.12944476306438446, 0.42233461141586304, -0.9479895830154419, 0.3011243939399719, 0.20607022941112518, 1.0863490104675293, 0.20132581889629364, 0.27682504057884216, -0.4699229896068573, 0.2837563157081604, 1.267814040184021, -0.014916554093360901, 0.657867431640625, -0.9511948823928833, -0.1210804358124733, -0.3006671965122223, -0.11343857645988464, -0.9698420763015747, -0.08299308270215988, -0.6114417314529419, -1.0259718894958496, -0.818165123462677, -0.6022073030471802, 0.12809091806411743, -0.29279524087905884, 0.45761317014694214, -0.055523984134197235, -0.015880169346928596, 0.011074409820139408, 0.44047239422798157, 0.7136412262916565, 0.18051326274871826, 0.10957864671945572, 0.43801307678222656, 1.3252192735671997, -1.5315136909484863, -0.776305615901947, -0.9969504475593567, 0.40359941124916077, -0.16459959745407104, -0.11945520341396332, -0.6583555936813354, -1.438033103942871, -1.0575698614120483, -0.6760579347610474, -0.162663996219635, -0.5679124593734741, 0.39477601647377014, 1.2059658765792847, -0.03261775150895119, -1.0228188037872314, 1.3417733907699585, -0.1326962262392044, -0.3683242201805115, 0.4689057767391205, 0.4976847767829895, 0.23878616094589233, 0.20711031556129456, -1.4426610469818115, 0.20163224637508392, 0.22356636822223663, -0.35110044479370117, 0.240349680185318, -0.09842441976070404, -1.0275933742523193, 0.42837777733802795, 0.4709503650665283, -0.7901980876922607, 1.1668777465820312, -0.1267412155866623, -0.9473266005516052, 0.5940152406692505, -0.3738194704055786, -0.32849207520484924, 0.09051606804132462, -0.14450828731060028, -0.7912806868553162, -0.4787669777870178, 0.16847752034664154, 0.4147561490535736, 0.29516732692718506, -0.2167121022939682, -0.320270299911499, 0.043219611048698425, -0.06192285940051079, 0.03514935076236725, -0.492269366979599, 0.9221608638763428, -0.7750191688537598, -0.2466154396533966, 0.443335622549057, 0.4422663450241089, -0.0690588653087616, -0.06879058480262756, -0.47984468936920166, -1.5867764949798584, 0.41035473346710205, 0.6904566884040833, 1.0418899059295654, -0.7920204401016235, -1.2247827053070068, -0.08956529945135117, 0.23346824944019318, 0.023083806037902832, -0.5235660672187805, 0.2732127606868744, -0.34180840849876404, 0.1477571427822113, 0.2841099798679352, -0.8585687875747681, -0.10609202831983566, -0.067779541015625, -0.29734858870506287, -0.46493446826934814, -0.0029926106799393892, 0.9102699160575867, -1.0216705799102783, -0.1502014696598053, -0.3085852861404419, 0.4025377631187439, -0.8742161989212036, 1.2757023572921753, -0.23313993215560913, -0.7517008185386658, -0.11378102004528046, -0.30043795704841614, 0.020762460306286812, -0.3593637943267822, 0.1675247997045517, -0.6583712697029114, -0.3651491701602936, 0.6636597514152527, -0.5159956812858582, 1.1213511228561401, -0.04291584715247154, 0.5910908579826355, -0.17371807992458344, -0.0761701837182045, 0.12435147911310196, 0.28076422214508057, -0.15052253007888794, -0.5615090727806091, 0.3739115297794342, 0.22236387431621552, -0.4093673825263977, 0.2692904472351074, 0.8824971318244934, 1.6819707155227661, -0.0073394314385950565, 0.307256817817688, 0.633729100227356, 0.18186847865581512, 0.36891746520996094, 0.2712823450565338, 0.759731650352478, 0.5092636942863464, 0.6472113132476807, -0.22193235158920288, 0.29505613446235657, -0.7592359185218811, 0.12129775434732437, 0.8021509051322937, 0.4506825804710388, 0.58511883020401, -0.08800312131643295, -0.8185193538665771, -0.40503713488578796, 0.7462732791900635, 0.6283575892448425, 1.6991522312164307, -0.1611810177564621, -0.18695130944252014, -1.1632170677185059, -0.43368127942085266, -0.444319486618042, 0.24491654336452484, -0.5787515640258789, -0.10319920629262924, -0.35123032331466675, -0.6895046830177307, 0.8617686629295349, 0.6229252219200134, 1.174351453781128, -0.72077476978302, -0.6181135773658752, -0.20383186638355255, 0.4503300189971924, -0.7522705793380737, -0.6624673008918762, 0.5592796802520752, -0.531291663646698, 0.002896530320867896, 0.20576314628124237, -0.03287374600768089, 0.19013941287994385, -0.25247445702552795, 1.0119333267211914, -0.45035672187805176, 0.11073370277881622, -0.135502889752388, 0.6998834013938904, -0.7071403861045837, -0.2431478649377823, 0.22743724286556244, 0.3101511597633362, -0.17468884587287903, 0.8210997581481934, 0.2554975748062134, -0.3530116379261017, 0.23973719775676727, -0.01518310233950615, -0.2037501484155655, 0.20689845085144043, -0.04041532427072525, 0.7527956962585449, 0.1175391748547554, -0.38632696866989136, -1.399005651473999, 0.6516913771629333, -0.11359916627407074, -0.1192379891872406, 0.014054759405553341, -1.2309874296188354, 0.08208436518907547, 0.5797510147094727, -0.6222603917121887, -0.31082043051719666, -0.6405972838401794, 0.6054475903511047, -0.40232667326927185, -0.49995550513267517, -0.11814852803945541, 0.2277267575263977, -0.23702920973300934, 0.5875656008720398, 0.4225086271762848, 0.021612074226140976, 0.06683481484651566, 0.358025461435318, -0.5936381816864014, 0.9715096354484558, 0.10897640883922577, 0.04469329118728638, -0.044504426419734955, -0.06158807501196861, -0.4512879550457001, -0.6785444617271423, -0.6028703451156616, -0.1203499585390091, -0.02485252171754837, 0.2087278515100479, -0.7605288624763489, -1.361249327659607, -0.40649816393852234, -0.924342930316925, -0.020676180720329285, 0.0013632727786898613, -0.19590017199516296, 0.0904921144247055, -0.9786285161972046, -0.8591949939727783, -0.9589579701423645, -1.0168120861053467, -0.5901882648468018, 0.11830298602581024, 0.4670548439025879, -0.21866029500961304, -0.5204951763153076, -0.20140895247459412, -0.4030168950557709, 1.2454861402511597, -0.7272723317146301, 0.1744028478860855, -0.3844262361526489, -0.3840300738811493, -0.1442195475101471, -0.361489862203598, 0.16998837888240814, -0.3064200282096863, -0.021743979305028915, -0.797960638999939, 0.751573920249939, 0.04984624683856964, 0.06422862410545349, 0.2729828655719757, 0.20237666368484497, 0.7041832208633423, -0.06025303900241852, -0.6564417481422424, 0.3486490547657013, 1.5241316556930542, -0.6051462888717651, 0.0245184525847435, 0.38248613476753235, 1.2225475311279297, 0.2632497251033783, -0.30646511912345886, 0.3147445619106293, 0.9849548935890198, 0.49693921208381653, 0.521779477596283, -0.43369758129119873, -0.08612075448036194, -0.32335925102233887, 0.15295983850955963, 1.191730260848999, -0.27372461557388306, 0.012348836287856102, -0.7700445652008057, 0.6309004426002502, -1.5339317321777344, -0.8322731852531433, 0.3214898109436035, 0.5885076522827148, 0.06503654271364212, -0.38379302620887756, -0.18232597410678864, -0.5555537343025208, 0.4282829463481903, 0.15715472400188446, -0.781328022480011, -0.5732827186584473, -0.430942565202713, 0.2508409321308136, 0.7101535797119141, 0.24192048609256744, -0.6998842358589172, 0.6557183265686035, 14.78482437133789, 0.4798848628997803, 0.203970804810524, 0.42013019323349, 0.6553071141242981, 0.41218245029449463, -0.15378382802009583, 0.013936769217252731, -1.3570005893707275, -0.2689294219017029, 1.175255298614502, 0.11729484051465988, 0.6639866232872009, 0.5844918489456177, -0.13625144958496094, 0.27465954422950745, -0.45982691645622253, 0.6602641344070435, 0.7303757667541504, -1.16852605342865, -0.02246505580842495, 0.1832931935787201, 0.12976592779159546, 0.7414581775665283, 0.4190780818462372, 0.7008706331253052, 0.9785606861114502, -0.14454270899295807, 0.19607402384281158, 0.6565202474594116, 0.8307234644889832, 0.10738906264305115, 0.48492148518562317, 0.4093035161495209, -0.8107640743255615, -0.2306511402130127, -1.0120190382003784, -1.1430716514587402, -0.24837319552898407, 0.28172650933265686, -0.3164708614349365, -0.42399945855140686, 0.14319683611392975, 0.49358969926834106, 0.08533216267824173, 0.3287525773048401, -0.029049081727862358, 0.07979792356491089, -0.07596426457166672, -0.1753620207309723, 0.05643419548869133, 0.5533424615859985, 0.12375498563051224, -0.03203409165143967, 0.1677960604429245, 0.3153577446937561, -0.2258342206478119, 0.7646270394325256, -0.5296628475189209, 0.015423218719661236, -0.06045116111636162, -0.20635956525802612, -0.06286408752202988, 1.1621907949447632, 0.6720336675643921, 0.04008127376437187, -0.3436101973056793, 0.5663039088249207, 0.801611065864563, -0.05490575358271599, -0.19708357751369476, -0.1854463815689087, 0.17120321094989777, -0.3302530348300934, 0.12768973410129547, 0.8005664348602295, -0.40501970052719116, -0.6341512799263, -0.785304605960846, -0.1986415982246399, 0.3241458237171173, -1.0628777742385864, -0.6670081615447998, 1.5526607036590576, -0.8490641117095947, -0.23232023417949677, 0.13081322610378265, -0.7301496267318726, -0.39433926343917847, 0.40135908126831055, -1.0516998767852783, -0.45994505286216736, 0.13998009264469147, -0.5175817012786865, -0.6206940412521362, -0.2607203722000122, 1.6071043014526367, -0.0173628069460392, -0.38541966676712036, 0.1717745065689087, -0.03765375167131424, 0.024894755333662033, 0.0031356695108115673, -0.9297627806663513, 0.9921988844871521, 0.19325865805149078, -0.10968118906021118, 0.7645239233970642, 0.2838910222053528, 0.1889801025390625, -0.5069820880889893, -0.08086623251438141, 1.0361815690994263, -0.6008755564689636, -0.6102493405342102, -0.6693066954612732, -0.885026216506958, 0.12196158617734909, 0.773476779460907, 0.11692405492067337, 0.30603596568107605, 0.2852414548397064, -0.6400989294052124, -0.04049745947122574, -0.7731583714485168, 0.38729327917099, 0.8534335494041443, -0.7896186113357544, -0.2540501654148102, -0.08229152113199234, 0.12827540934085846, -0.9876200556755066, -0.7262434363365173, -0.2908228933811188, 0.4168873727321625, -0.603711724281311, 1.1406385898590088, -0.29232513904571533, 0.7219582796096802, 0.5163513422012329, -0.02387499064207077, -0.42576324939727783, -0.2843378186225891, -0.8505442142486572, -0.5962299108505249, 0.3314914405345917, 0.5595080852508545, -0.37518131732940674, 0.8567661046981812, 1.3738757371902466, 0.5215598344802856, -0.6403154730796814, -0.12997862696647644, -0.2178640365600586, -0.6158405542373657, -0.11559270322322845, 0.6485221982002258, 0.3843386769294739, -0.31336095929145813, 0.3788668215274811, 0.5710292458534241, 0.5707705616950989, 0.2102954387664795, -0.4592811167240143, 0.18182606995105743, -0.5926855802536011, 0.0037832821253687143, -0.47436389327049255, -0.23202607035636902, -1.2033761739730835, -0.06465970724821091, -0.9294291138648987, -0.2517704963684082, -1.1234482526779175, -0.5289362668991089, 0.04350477084517479, -0.6777395606040955, -0.09204030781984329, 0.3648444414138794, -0.3809315264225006, -0.7980884909629822, -0.2859500050544739, -0.6583757400512695, 0.49373432993888855, 0.5406982898712158, -0.4376432001590729, -0.009063702076673508, 0.28106120228767395, -0.3354024887084961, 0.4327206313610077, 0.4578467011451721, -0.7237449884414673, -0.37268975377082825, -1.1981912851333618, 0.46700361371040344, -0.1602807641029358, 0.035507459193468094, -0.89202481508255, 1.7176047563552856, 0.48493191599845886, -0.06466692686080933, -0.29646918177604675, 0.22906135022640228, -1.0456894636154175, -0.39806464314460754, 0.166730597615242, -0.937863826751709, 0.27762243151664734, -0.2164107859134674, -0.588525116443634, -0.22308826446533203, 0.6425793766975403, -0.39414945244789124, -1.1047544479370117, -0.5245287418365479, 0.4235433042049408, -0.2032075673341751, 0.07195783406496048, -0.4264571964740753, -0.3425554633140564, -1.0861449241638184, -0.24891497194766998, -0.03987536206841469, 0.3274281322956085, -0.4020792841911316, 0.7626976370811462, 0.10747755318880081, -1.312428593635559, 0.2862408757209778, 0.3509620130062103, -0.07211729884147644, 0.26569944620132446, 0.7162548899650574, 0.25309911370277405, -0.11132985353469849, 0.5651665329933167, -0.025612736120820045, 0.6334560513496399, -0.792656421661377, 0.20082181692123413, 0.9152446985244751, -0.4226917028427124, -0.0019892575219273567, 1.01861572265625, -0.3955892026424408, -1.0621051788330078, -0.01985681615769863, -1.1493388414382935, -0.9260764718055725, -0.27603816986083984, 0.40025100111961365, 0.4016021490097046, -0.24755078554153442, -0.46026673913002014, -0.5902712345123291, -0.11186418682336807, -0.21290071308612823, -0.17321674525737762, 0.7942495942115784, -0.0768536627292633, -0.4892246127128601, 0.254996657371521, 0.37921783328056335, -0.434320867061615, -0.8691521883010864, -1.2253509759902954, -0.3130075931549072, 0.008593309670686722, 0.739158570766449, -0.19276340305805206, -0.5633805990219116, 0.42113128304481506, 0.4601697325706482, 0.7085443735122681, -0.013336059637367725, -0.3150891363620758, 0.4266171157360077, 0.8857208490371704, -0.32897910475730896, -0.9929860830307007, -1.0319136381149292, 1.2386958599090576, 1.0086314678192139, -0.6871291399002075, 0.1701057255268097, -0.6078550219535828, -0.5739079713821411, 0.7864802479743958, 0.6601652503013611, -0.30670273303985596, 0.8737315535545349, -0.10991941392421722, -0.04148897901177406, -0.07921089977025986, -1.251584768295288, -0.5148414969444275, 0.881188154220581, 0.7680127620697021, 0.6696639657020569, 0.05743752792477608, 0.3419191241264343, 0.862185001373291, 0.13714785873889923, 0.13979221880435944, 0.2311122864484787, 0.0711224228143692, -0.043079208582639694, 0.17494316399097443, 0.2472229152917862, 0.5866072177886963, -0.5244410037994385, -0.8109683394432068, 0.2203071564435959, 0.31348717212677, 0.5099664330482483, 0.4010602831840515, 0.9255268573760986, -0.1635853350162506, 0.7769254446029663, -0.06289920955896378, 0.091466523706913, -0.588995099067688, -0.5139421224594116, -0.05838629603385925, -0.5776202082633972, -0.2581697702407837, -0.2058861255645752, -0.6221628785133362, -0.7322046756744385, -0.03952665999531746, -0.09195823222398758, -0.11983438581228256, 0.7903391122817993, 0.867728590965271, 1.074304223060608, 0.8277451992034912, -0.07730484753847122, -0.26392483711242676, -0.11140570044517517, -0.8130762577056885, 0.22839581966400146, -0.6685006618499756, -0.443465918302536, -0.43639224767684937, -0.3320448696613312, -0.8603557348251343]}, "authors": [{"authorId": "38568116", "name": "Tae Jun Ham"}, {"authorId": "2119392434", "name": "Yejin Lee"}, {"authorId": "2072585887", "name": "Seong Hoon Seo"}, {"authorId": "92271305", "name": "Soo-Uck Kim"}, {"authorId": "2111539060", "name": "Hyunji Choi"}, {"authorId": "2111891693", "name": "Sungjun Jung"}, {"authorId": "66152513", "name": "Jae W. Lee"}], "references": [{"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "fc9c52f55ffe0e860b1bb4222fe86cce60c05551", "title": "Meshed-Memory Transformer for Image Captioning"}, {"paperId": "51a920c3d201eec57bcc2e97e0268304f53b5161", "title": "TreeGen: A Tree-Based Transformer Architecture for Code Generation"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295", "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "8e06964a39e5eb2d526ea54f3af9cbaf5d435bc8", "title": "Manna: An Accelerator for Memory-Augmented Neural Networks"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "97cbcb427c0a530a8854b7038508684e01e4c515", "title": "Thanks for Nothing: Predicting Zero-Valued Activations with Lightweight Convolutional Neural Networks"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb", "title": "Text Summarization with Pretrained Encoders"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "bf442ab269074665a68e4dbbe19e4efc97862541", "title": "Large Memory Layers with Product Keys"}, {"paperId": "830995ef17cc291c13f42dfd9f462137de1d2179", "title": "Augmenting Self-attention with Persistent Memory"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "7edacd94dc1509803d9bbcc1d92fea780d71cb3e", "title": "MnnFast: A Fast and Scalable System Architecture for Memory-Augmented Neural Networks"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "419fe8b8edec6f849fecfd0d2bb11cacc4705180", "title": "Deep Session Interest Network for Click-Through Rate Prediction"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "27ac832ee83d8b5386917998a171a0257e2151e2", "title": "Attention Augmented Convolutional Networks"}, {"paperId": "9f9c4dd9a761a708cfcec6951ff67ce8953978c0", "title": "Bit-Tactical: A Software/Hardware Approach to Exploiting Value and Bit Sparsity in Neural Networks"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "fe7ceb03b12c0dbd50290be632dacdccac72af77", "title": "Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: Column Combining Under Joint Optimization"}, {"paperId": "08588a4e596b02f22ac77dc8300aaabc27cb66b4", "title": "AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks"}, {"paperId": "70d5ceb59118334e1a6eed33a149234413147b92", "title": "Deep Interest Evolution Network for Click-Through Rate Prediction"}, {"paperId": "97faeefa771e8cc8e55159e2bd03e6f5eef249a8", "title": "Self-Attentive Sequential Recommendation"}, {"paperId": "947787e31cd2b70e49326ee7e2740d2fd3e1d3fa", "title": "SiMul: An Algorithm-Driven Approximate Multiplier Design for Machine Learning"}, {"paperId": "79c113913fcbbcab659314d766e21ac4e682fa2e", "title": "ComPEND: Computation Pruning through Early Negative Detection for ReLU in a Deep Neural Network Accelerator"}, {"paperId": "f2b70dd0312393c53d840796df004d3c3c940b49", "title": "SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks"}, {"paperId": "8e64c651b89ea4bad78cf0642876cb12e233e2a5", "title": "Energy-Efficient Neural Network Accelerator Based on Outlier-Aware Low-Precision Computation"}, {"paperId": "c64d7291064b70bbf35335f63248f234d908cd81", "title": "Compensated-DNN: Energy Efficient Low-Precision Deep Neural Networks by Compensating Quantization Errors"}, {"paperId": "1324c98f35256b3fe64fae9f777e2f7d76f194c0", "title": "A Configurable Cloud-Scale DNN Processor for Real-Time AI"}, {"paperId": "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "title": "Self-Attention Generative Adversarial Networks"}, {"paperId": "fcdc620b16cc44e3d0c5155a05dca565476ea73b", "title": "Energy-Efficient Inference Accelerator for Memory-Augmented Neural Networks on an FPGA"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "063598cdaff79852c3647d074506120889c5c17b", "title": "ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation"}, {"paperId": "a63fac14f215e42dc75ddfef84a2652a28f68731", "title": "Low Precision RNNs: Quantizing RNNs Without Losing Accuracy"}, {"paperId": "71f8e5f3c11531b446e1f3fa107e041d17254956", "title": "qLUT"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "402f850dff86fb601d34b2841e6083ac0f928edd", "title": "SCNN: An accelerator for compressed-sparse convolutional neural networks"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "8d1367e120b6d1b4c96b051f1a8154167602afc1", "title": "Energy-Efficient Reduce-and-Rank Using Input-Adaptive Approximations"}, {"paperId": "b71ae4f14d329268baa5d280734054b449e6ea1b", "title": "ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA"}, {"paperId": "bc20f523a6e97800340e57a94d79926fce05572c", "title": "Cambricon-X: An accelerator for sparse neural networks"}, {"paperId": "784ee73d5363c711118f784428d1ab89f019daa5", "title": "Hybrid computing using a neural network with dynamic external memory"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2e2b189f668cf2c06ebc44dc9b166648256cf457", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network"}, {"paperId": "ffdaa12ef011de9dbf43be46d45a3abcc8288965", "title": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks"}, {"paperId": "276ebc620a8976026bd2d03582b9ecfa3738d43c", "title": "The MovieLens Datasets: History and Context"}, {"paperId": "2d33cbf5e62d0cbff5079b0ea0678892c4cc982e", "title": "Fast Orthogonal Projection Based on Kronecker Product"}, {"paperId": "aa0a93a15642ea1bfdde1313a292674cccbd61d2", "title": "Fixed-Point Implementations of the Reciprocal, Square Root and Reciprocal Square Root Functions"}, {"paperId": "bd6507b5c9deaf87bda81e59ce15b2309df0bf37", "title": "ShiDianNao: Shifting vision processing closer to the sensor"}, {"paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "title": "End-To-End Memory Networks"}, {"paperId": "b7cf49e30355633af2db19f35189410c8515e91f", "title": "Deep Learning with Limited Numerical Precision"}, {"paperId": "4157ed3db4c656854e69931cb6089b64b08784b9", "title": "DaDianNao: A Machine-Learning Supercomputer"}, {"paperId": "c1126fbffd6b8547a44c58b192b36b08b18299de", "title": "Neural Turing Machines"}, {"paperId": "22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd", "title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning"}, {"paperId": "6109372a7a17649b0a64d25e4e087d653772fb5e", "title": "Learning Binary Codes for High-Dimensional Data Using Bilinear Projections"}, {"paperId": "f03b9dbe43f8bccab2e6499e26e0be4f5bd14283", "title": "A Theoretical Analysis of NDCG Type Ranking Measures"}, {"paperId": "64d83ccbcb1d87bfafee57f0c2d49043ee3f565b", "title": "Super-Bit Locality-Sensitive Hashing"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "42bb0ac384fb87933be67f63b98d90a45d2fe6e9", "title": "Similarity estimation techniques from rounding algorithms"}, {"paperId": "2271e8555c4d6c04ea4c1fb57735b6a489cbd6fd", "title": "Powering by a Table Look-Up and a Multiplication with Operand Modification"}, {"paperId": "e6da45c79cf4c0b051b17ec9354927c61ed38776", "title": "RETROSPECTIVE: Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing"}, {"paperId": "2a21e5bb911c481a159fb9eea57e201df54745bf", "title": "Chisel"}, {"paperId": null, "title": "Linformer: Selfattention with linear complexity"}, {"paperId": null, "title": "NVIDIA V100 Tensor Core GPU"}, {"paperId": null, "title": "Torchscript"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "a07609c2ed39d049d3e59b61408fb600c6ab0950", "title": "GPU Kernels for Block-Sparse Weights"}, {"paperId": null, "title": "Intel Xeon gold 6128 processor"}, {"paperId": "5bfecd14937da569eabec0afea710db846d3899b", "title": "Stripes: Bit-serial deep neural network computing"}, {"paperId": null, "title": "Minerva: Enabling lowpower, highly-accurate deep neural network accelerators"}, {"paperId": null, "title": "A theoretical dummy dumm dumm dumm , \u201d in Proceedings of the 26 th Annual Conference on Learning Theory"}, {"paperId": null, "title": "Cloud TPU system architecture"}, {"paperId": null, "title": "Cerebras systems: Achieving industry best ai performance through a systems approach"}, {"paperId": null, "title": "ALBERT official implementation"}, {"paperId": null, "title": "Goya hl-10x datasheet"}, {"paperId": null, "title": "Synopsys Design Compiler"}]}