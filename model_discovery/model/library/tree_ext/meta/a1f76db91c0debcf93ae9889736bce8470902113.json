{"paperId": "a1f76db91c0debcf93ae9889736bce8470902113", "title": "Large Language Models: A Survey", "abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.", "venue": "arXiv.org", "year": 2024, "citationCount": 58, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper reviews some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discusses their characteristics, contributions and limitations, and gives an overview of techniques developed to build, and augment LLMs."}, "embedding": {"model": "specter_v2", "vector": [-0.015109140425920486, 0.5934717655181885, -0.38837647438049316, -0.029774708673357964, -0.26665592193603516, -0.3926801383495331, 0.8921640515327454, -0.21881891787052155, -0.44808313250541687, -0.0902813971042633, 0.4242929518222809, -0.022885866463184357, 0.4807851016521454, 0.2637850046157837, -0.12619608640670776, 0.3145485818386078, -0.6771483421325684, 0.6474554538726807, -0.11176681518554688, -0.34778469800949097, -0.3302803635597229, -0.8649575114250183, -0.7789530754089355, -0.1028367355465889, 0.5504902601242065, 0.18054813146591187, 0.3283957242965698, 0.9782946109771729, -0.5460637807846069, 0.3025485873222351, 0.46710556745529175, -0.4062771499156952, -0.09951212257146835, -0.12880833446979523, -0.1700707972049713, -0.1060432568192482, 0.09572089463472366, -0.3833523690700531, -0.3530869781970978, 0.6613205075263977, -0.31477224826812744, 0.49220404028892517, 0.3849756121635437, -0.664544403553009, -0.3762204647064209, 0.9785320162773132, 0.45050546526908875, 0.4667147696018219, -0.24285192787647247, -0.408360093832016, 1.2176185846328735, -1.3371866941452026, 0.29686489701271057, 1.8376400470733643, 0.5805472135543823, 0.4475685656070709, -0.5898388028144836, -0.6833430528640747, 0.6049836277961731, -0.42326509952545166, -0.9453184604644775, -0.3288450539112091, -0.22612912952899933, 0.04494239762425423, 2.266021728515625, -0.24905706942081451, -0.13958053290843964, 0.6084831357002258, -0.15154819190502167, 1.3911744356155396, -0.07165523618459702, -0.7758603096008301, -0.518615186214447, 0.1551544964313507, 0.10069168359041214, 0.9034178853034973, -0.42090240120887756, 0.14461994171142578, -0.8034366965293884, -0.44452258944511414, 0.22519411146640778, -0.42418763041496277, -0.27931976318359375, -0.03785224258899689, -0.3215024471282959, 1.047764539718628, -0.07774698734283447, 0.8303638100624084, 0.1566990613937378, 0.6599870324134827, 0.2603772282600403, 0.31841376423835754, 0.2787487804889679, 0.39517679810523987, -0.3699003756046295, 0.3547520935535431, -0.9130630493164062, 0.36155301332473755, 0.2938512861728668, 1.0000841617584229, -0.48659348487854004, 0.20576955378055573, -0.6932166218757629, 0.48120051622390747, 1.3729186058044434, 0.22812063992023468, 0.7658800482749939, -0.681058406829834, 0.5332531332969666, -0.5506020784378052, -0.05818584933876991, -0.31325891613960266, -0.36899498105049133, -0.31348878145217896, -0.7591850161552429, -1.373170018196106, -0.495029091835022, -0.03457310050725937, -0.632887065410614, 0.9936873912811279, -0.4870835542678833, -0.22063469886779785, 0.21014784276485443, 0.0831950455904007, 0.5420776009559631, 1.128248929977417, 0.3353941738605499, -0.07938064634799957, 0.8761194348335266, -0.9627878665924072, -0.6624449491500854, -1.2460269927978516, 0.9903964996337891, -0.36333560943603516, 0.4021986424922943, -0.22300562262535095, -1.12366783618927, -0.8972201943397522, -0.5644426941871643, -0.24544955790042877, -0.5379117131233215, 0.8030425310134888, 0.9212039113044739, 0.47549232840538025, -0.9371196627616882, 0.47063058614730835, -0.10322941094636917, -0.11856278032064438, -0.0512130968272686, 0.16901718080043793, 0.20356586575508118, -0.45177480578422546, -1.4669357538223267, 0.2656995356082916, 0.333126425743103, -0.6454976797103882, -0.20105592906475067, -0.3054172396659851, -1.1951946020126343, -0.1958027333021164, -0.012250312604010105, -0.49593034386634827, 1.32331383228302, 0.21233759820461273, -1.289544939994812, 0.6436628103256226, -0.42868906259536743, -0.07422448694705963, 0.3513917326927185, -0.1894911229610443, -0.4481586217880249, -0.8543297648429871, -0.604816734790802, 0.71364426612854, 0.35099077224731445, 0.25150296092033386, -0.149208202958107, 0.4459350109100342, -0.18452905118465424, -0.1615891009569168, -0.3595309555530548, 1.1265376806259155, -0.7683476805686951, -0.42838236689567566, 0.3314897418022156, 0.3536933362483978, -0.29357433319091797, -0.29689931869506836, -0.5247392654418945, -1.167181134223938, 0.7958710193634033, -0.3084518313407898, 1.1129728555679321, -0.7839381694793701, -0.552278459072113, -0.5092175602912903, -0.1527853161096573, -0.07031737267971039, -0.8169712424278259, 0.8454281687736511, -0.1404344141483307, 0.6112793684005737, -0.19527432322502136, -1.7159353494644165, 0.14276275038719177, -0.22936011850833893, -0.6387003660202026, -0.2408207654953003, 0.10611169785261154, 1.1210215091705322, -0.643706202507019, 0.09453471750020981, -0.03959375619888306, 0.3503301739692688, -0.8675221800804138, 1.2211639881134033, -0.8054805994033813, 0.07156865298748016, -0.19258056581020355, -0.3357985019683838, 0.1247398778796196, -0.3484187722206116, 0.5942447185516357, -0.027615979313850403, -0.12268753349781036, 0.4197322726249695, -0.38999465107917786, 1.286450743675232, -0.43778738379478455, 0.45338740944862366, -0.3174459636211395, -0.16128131747245789, 0.0193881094455719, 0.5386873483657837, -0.5758492946624756, -0.05360047146677971, 0.3547345995903015, 0.5515652894973755, -0.6686633229255676, 0.38469094038009644, 0.675744354724884, 0.7412348389625549, -0.20446141064167023, 0.5290956497192383, 0.6029791831970215, -0.48237788677215576, 0.5323781371116638, 0.4419132471084595, 0.44654223322868347, 0.24316437542438507, 0.5417993068695068, 0.08408920466899872, 0.653528094291687, -0.8254102468490601, -0.3333530128002167, 0.46270227432250977, 0.8173165321350098, 0.8393563032150269, 0.2479817122220993, -0.5251112580299377, -0.23037569224834442, 0.25675421953201294, 0.9652789235115051, 1.409592866897583, -0.4361618757247925, -0.2913934588432312, -0.7531861066818237, -0.2487330436706543, -0.3159978985786438, 0.29060521721839905, -0.3266626298427582, 0.09800620377063751, -0.6442977786064148, -1.1130093336105347, 0.8991297483444214, 0.10725276172161102, 0.7000331282615662, -0.4873705506324768, 0.08901987969875336, -0.2330869883298874, 0.04005700349807739, -1.0049902200698853, -0.7546079158782959, 0.20397284626960754, -0.9863841533660889, -0.045311808586120605, -0.01652437075972557, -0.12045657634735107, 0.17863401770591736, -0.7980088591575623, 0.8872168660163879, -0.6014638543128967, -0.019155507907271385, -0.1603323221206665, 0.6964935064315796, -0.6681345105171204, -1.184759497642517, 0.1784553974866867, 0.41295307874679565, -0.06623322516679764, 0.20718392729759216, 0.6668882369995117, 0.5902202725410461, 0.048156995326280594, -0.3041841387748718, 0.45884451270103455, 0.055896952748298645, -0.06502920389175415, 0.8218676447868347, -0.1008499339222908, -0.30382850766181946, -1.5436619520187378, 1.153516173362732, 0.30887213349342346, -0.6975713968276978, 0.5080219507217407, -0.5174238085746765, -0.12934091687202454, 0.6000617146492004, -0.8600406050682068, -0.593008816242218, -0.7508905529975891, 0.3119564354419708, -0.3432115614414215, -0.12135294824838638, 0.39350759983062744, 0.25346025824546814, 0.37316060066223145, 0.05466119572520256, 0.6808580160140991, 0.3101724684238434, -0.7236236929893494, 0.5949873924255371, -0.6652167439460754, 0.3710249066352844, 0.4324183464050293, 0.36952507495880127, -0.40595805644989014, -0.52675861120224, -0.49125492572784424, -0.3210926651954651, -0.3542686402797699, -0.22732937335968018, -0.07588328421115875, 0.024436350911855698, -0.6914870738983154, -0.316203773021698, 0.20766876637935638, -1.1837016344070435, -0.007197636179625988, 0.6542014479637146, -0.2431582659482956, 0.013904282823204994, -0.9807243347167969, -1.2331433296203613, -0.595114529132843, -0.8079214096069336, -0.8969594240188599, 0.5183069705963135, 0.11550794541835785, -0.40465882420539856, -0.9071609377861023, 0.21462275087833405, -0.11699357628822327, 1.0066848993301392, -0.6249539256095886, 1.1744967699050903, -0.04911334067583084, -0.0672692134976387, -0.5899240970611572, 0.4639943540096283, 0.5159505009651184, -0.36079466342926025, 0.3676636219024658, -1.0187076330184937, -0.03498174995183945, -0.26744264364242554, -0.2334127277135849, 0.12868835031986237, 0.539878249168396, 0.6108055114746094, 0.12343833595514297, -0.7325618863105774, 0.3205152153968811, 1.2761234045028687, -0.9662260413169861, -0.3748129606246948, -0.14562126994132996, 0.9446683526039124, 0.15003074705600739, -0.5391133427619934, 0.39068061113357544, 0.29480263590812683, 0.4818720519542694, 0.07058586925268173, -0.025105468928813934, 0.06187945604324341, -0.6688644886016846, 0.6788614392280579, 1.5919173955917358, 0.3466835916042328, -0.6301352977752686, -1.0385067462921143, 0.4540664553642273, -1.087113857269287, -0.33804088830947876, 0.4456084966659546, 0.8131058216094971, 0.5562976598739624, -0.20134104788303375, -0.42451128363609314, -0.41601914167404175, 0.5776236057281494, 0.44773945212364197, -0.26397135853767395, -0.8267400860786438, -0.35688120126724243, 0.13731630146503448, -0.03115244396030903, 0.6194868683815002, -0.4650377631187439, 0.7312230467796326, 14.655263900756836, 1.0524505376815796, 0.18485811352729797, 0.7523040175437927, 0.6516963839530945, 0.10665149986743927, -0.34617361426353455, -0.042904701083898544, -1.521301031112671, -0.2349320352077484, 1.4306219816207886, 0.022142482921481133, 0.7221797704696655, 0.1647888571023941, 0.2500282824039459, 0.22273720800876617, -0.488288551568985, 0.7716338634490967, 0.5224530100822449, -1.1664406061172485, 0.6218215227127075, 0.2503371834754944, 0.48625773191452026, 0.8919504880905151, 0.6473981738090515, 0.9705395102500916, 0.5339874029159546, -0.5837312340736389, 0.5106735229492188, 0.2555656433105469, 0.658052384853363, -0.03594181686639786, 0.5164322257041931, 1.1354142427444458, -0.9044635891914368, -0.31043195724487305, -0.5871387124061584, -1.1212910413742065, 0.24064889550209045, 0.10850085318088531, -0.4686262011528015, -0.48350250720977783, -0.45478153228759766, 0.7709784507751465, -0.20124439895153046, 0.18261857330799103, -0.18018083274364471, 0.9185572266578674, -0.3724556863307953, -0.1350620537996292, 0.3733924329280853, 0.31409570574760437, 0.32647523283958435, -0.07677015662193298, 0.17841969430446625, -0.0833975300192833, 0.0885380282998085, 0.6149280071258545, -0.8098176121711731, 0.2507552206516266, -0.188188374042511, -0.796734631061554, -0.09654220938682556, 0.6988596320152283, 0.5513388514518738, 0.2627588212490082, -0.6038272380828857, 0.2111416459083557, 0.5684756636619568, 0.1289353221654892, -0.17905034124851227, 0.18711446225643158, 0.27424633502960205, -0.45822563767433167, -0.04870128631591797, 0.5461341738700867, 0.19721409678459167, -0.6423807740211487, -0.5268425345420837, -0.641701340675354, 0.3761975169181824, -0.7212490439414978, -0.7665385603904724, 0.9666929841041565, -0.3411746025085449, -0.37592217326164246, -0.008878367021679878, -0.6422032713890076, -0.1880548894405365, 0.9605918526649475, -1.3782618045806885, -0.9017530679702759, 0.6961160898208618, -0.5328568816184998, -0.09346814453601837, -0.2879027724266052, 1.4645168781280518, -0.22215649485588074, -0.5532587766647339, 0.045906517654657364, 0.45053866505622864, 0.11104132235050201, -0.48151710629463196, -0.47440096735954285, 1.243790626525879, 0.22893618047237396, 0.12574005126953125, 0.33729732036590576, -0.010129989124834538, 0.17414455115795135, -0.7423938512802124, -0.27605435252189636, 1.2038229703903198, -0.9682584404945374, -0.4102861285209656, -1.0042378902435303, -0.8167754411697388, 0.20812609791755676, 0.42385369539260864, -0.39390242099761963, 0.21614611148834229, 0.16874995827674866, -0.6779916882514954, 0.3581567108631134, -0.5929432511329651, -0.11150337010622025, 0.32949185371398926, -0.7028383016586304, -0.09833253920078278, 0.16454105079174042, 0.4850980043411255, -0.922870397567749, -0.36325302720069885, -0.2914648652076721, -0.06590864062309265, 0.33633971214294434, 0.763840913772583, -0.5302414894104004, 0.558400571346283, 0.846712589263916, -0.1722557246685028, -0.8882989883422852, 0.09553977102041245, -1.0636879205703735, -0.30055201053619385, 0.0523390918970108, 0.9408453106880188, -0.4070032835006714, 0.031033935025334358, 0.7518887519836426, 0.6206845641136169, -0.5586541295051575, -0.5345754027366638, -0.619605541229248, 0.2671613395214081, -0.47631093859672546, 0.3476780652999878, -0.1776108294725418, -0.23590488731861115, 0.30649447441101074, 0.14227311313152313, 0.6940549612045288, -0.2547627389431, -0.5365275740623474, 0.5975582599639893, -0.3635505139827728, -0.4370124638080597, -0.6283748745918274, -0.07532422989606857, -1.4207992553710938, 0.2589828073978424, -1.031369924545288, -0.18571238219738007, -0.9448538422584534, -0.12169864773750305, 0.18058110773563385, 0.06358397006988525, 0.04732396826148033, 0.46882134675979614, -0.38403260707855225, -0.5574880838394165, -0.41916269063949585, -0.26042500138282776, 0.7616177797317505, 0.8865616917610168, -0.6693810224533081, -0.034097328782081604, 0.08537333458662033, 0.45349472761154175, 0.4012746512889862, 0.46264785528182983, -0.5446634292602539, -1.0795011520385742, -1.7326951026916504, 0.4260313808917999, 0.06951292604207993, -0.2747460603713989, -0.5446528196334839, 0.552414059638977, 0.30392986536026, -0.3476659655570984, 0.08867199718952179, 0.41280442476272583, -0.46003657579421997, -0.3242233693599701, 0.41153785586357117, -0.9271329045295715, 0.3809899091720581, 0.25800129771232605, -0.5240911841392517, -0.48385855555534363, 0.4765321910381317, -0.04016652703285217, -1.0814143419265747, -0.3544079065322876, 0.5411343574523926, -0.6066133379936218, 0.17215152084827423, -0.33985480666160583, -0.012640371918678284, -0.8725776076316833, -0.7118344902992249, -0.08030728995800018, 0.36079517006874084, -0.4442101716995239, 1.2380963563919067, 0.1775374859571457, -0.8100911974906921, -0.2517695724964142, 0.39346352219581604, 0.057786550372838974, -0.29737406969070435, 0.40748852491378784, 0.3108210861682892, -0.3014961779117584, 1.0706490278244019, 0.8106111884117126, 0.4050595760345459, -1.213219404220581, -0.18800941109657288, 1.0991644859313965, -0.6589246988296509, -0.34889110922813416, 1.5567148923873901, -0.2202269732952118, -1.462874174118042, 0.12938007712364197, -1.2627226114273071, -0.727454662322998, -0.5402365326881409, 0.6712985038757324, -0.16223861277103424, 0.0028404067270457745, -0.4032762944698334, -0.26012519001960754, 0.05220138281583786, 0.10054506361484528, -0.49729475378990173, 0.7046563029289246, -0.24525302648544312, -0.6052449345588684, 0.6792464256286621, 0.9719855189323425, -0.5620349049568176, -0.4800872802734375, -0.7581571340560913, -0.2914498746395111, 0.08661288022994995, 0.44712767004966736, -0.5207166075706482, -0.46379831433296204, 0.7094910144805908, 0.36737194657325745, 0.20019978284835815, 0.23332178592681885, -0.21974241733551025, 0.4434494376182556, 0.8162157535552979, 0.21350596845149994, -0.7571171522140503, -0.8840358257293701, 1.658981442451477, 1.2857563495635986, -1.2813953161239624, 0.12830819189548492, -0.25566500425338745, -0.6781198382377625, 0.7289034724235535, 0.035249292850494385, 0.33615341782569885, 1.1782761812210083, -0.4166523814201355, -0.018092071637511253, 0.2769165635108948, -1.368098258972168, 0.07410226762294769, 1.0062379837036133, 0.5338748693466187, 1.1476472616195679, 0.32913580536842346, 0.011211898177862167, 0.8401848077774048, -0.004160945303738117, 0.009335492737591267, 0.4290103614330292, 0.44984081387519836, -0.1126568466424942, -0.26255637407302856, 0.127938374876976, 0.5111454725265503, -0.45576566457748413, -0.7418918609619141, -0.07483629137277603, 0.5950915217399597, 0.35544729232788086, 0.710627019405365, 0.4012002944946289, 0.2969343364238739, 0.32833269238471985, 0.4534616768360138, 0.3337358832359314, -0.8533074259757996, -0.21771882474422455, -0.23978857696056366, -0.5252928137779236, 0.10062780231237411, 0.011218513362109661, -0.4600144922733307, -0.1838250309228897, -0.323493629693985, 0.19464421272277832, -0.035864606499671936, 0.450944185256958, 1.2794294357299805, 0.39338651299476624, 0.039046984165906906, -0.32522010803222656, -0.05729354918003082, -0.4764954447746277, -1.2320561408996582, -0.31333351135253906, -0.648206889629364, -0.30416661500930786, -0.003274229820817709, -0.1251574009656906, -0.2665116488933563]}, "authors": [{"authorId": "2164604", "name": "Shervin Minaee"}, {"authorId": "2273053608", "name": "Tom\u00e1\u0161 Mikolov"}, {"authorId": "1620486779", "name": "Narjes Nikzad"}, {"authorId": "89845455", "name": "M. Chenaghlu"}, {"authorId": "2166511", "name": "R. Socher"}, {"authorId": "2283771739", "name": "Xavier Amatriain"}, {"authorId": "2257313530", "name": "Jianfeng Gao"}], "references": [{"paperId": "ce4e101950554c41d3b35f5b297722abd1ce6403", "title": "Automated Unit Test Improvement using Large Language Models at Meta"}, {"paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32", "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"}, {"paperId": "8c1243e089621d09025e1e51e8e01cb2cb20eabf", "title": "Knowledge Fusion of Large Language Models"}, {"paperId": "0487a4cd47dafb27bdcc91c870ac86022fbc1be5", "title": "TrustLLM: Trustworthiness in Large Language Models"}, {"paperId": "6f657eaca838919ce7283b9fe435fe99ce8961bb", "title": "Agent AI: Surveying the Horizons of Multimodal Interaction"}, {"paperId": "560c6f24c335c2dd27be0cfa50dbdbb50a9e4bfd", "title": "TinyLlama: An Open-Source Small Language Model"}, {"paperId": "c3d1832ed0444f75d44116fabbdda891aebc4b01", "title": "LLaMA Pro: Progressive LLaMA with Block Expansion"}, {"paperId": "575f403261d5f99526f0b4dfc8644352d6c4467a", "title": "DocLLM: A layout-aware generative language model for multimodal document understanding"}, {"paperId": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d", "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"}, {"paperId": "cdcf3f36866ef1e16eba26d57c2324362247ba84", "title": "Zephyr: Direct Distillation of LM Alignment"}, {"paperId": "c85268696fe1435605ae66a18653cfdcf8153753", "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "0c72450890a54b68d63baa99376131fda8f06cf9", "title": "The Rise and Potential of Large Language Model Based Agents: A Survey"}, {"paperId": "fa75a55760e6ea49b39b83cb85c99a22e1088254", "title": "NExT-GPT: Any-to-Any Multimodal LLM"}, {"paperId": "e26888285436bc7998e5c95102a9beb60144be5e", "title": "Textbooks Are All You Need II: phi-1.5 technical report"}, {"paperId": "cb587eaea753ee38013afb7e5b6bc8fba1248d04", "title": "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "28c6ac721f54544162865f41c5692e70d61bccab", "title": "A Survey on Large Language Model based Autonomous Agents"}, {"paperId": "2dfb9171e180dcb0af23d305e024d43d311708ab", "title": "Giraffe: Adventures in Expanding Context Lengths in LLMs"}, {"paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f", "title": "Instruction Tuning for Large Language Models: A Survey"}, {"paperId": "e01ab53663e5df5961a021506a9cb09f4efc3788", "title": "Challenges and Applications of Large Language Models"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "888728745dbb769e29ed475d4f7661eebe1a71cf", "title": "A Survey on Evaluation of Large Language Models"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "c12b80b44d9acfe6cd92fdf965264c4b706c367c", "title": "ToolQA: A Dataset for LLM Question Answering with External Tools"}, {"paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b", "title": "Textbooks Are All You Need"}, {"paperId": "a65cbc88c5cb7af1103c060e48f63f95ad403b3c", "title": "Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models"}, {"paperId": "9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6", "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap"}, {"paperId": "fbd2c8089870814449f9254a711041bbae145a82", "title": "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources"}, {"paperId": "0244aeb7c6927e2fb0c2e668687e160a00737dbe", "title": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "7e72eb196b7c90b3a5d6385af536fe8e5934fb82", "title": "ConvGenVisMo: Evaluation of Conversational Generative Vision Models"}, {"paperId": "7d8905a1fd288068f12c8347caeabefd36d0dd6c", "title": "Gorilla: Large Language Model Connected with Massive APIs"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "bd5deadc58ee45b5e004378ba1d54a96bc947b4a", "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"}, {"paperId": "90027ca7802645671a69b00b65e1fa94e6b63544", "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"}, {"paperId": "2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4", "title": "Sources of Hallucination by Large Language Models on Inference Tasks"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "e0384ba36555232c587d4a80d527895a095a9001", "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820", "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"}, {"paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c", "title": "Towards Expert-Level Medical Question Answering with Large Language Models"}, {"paperId": "88884b8806262a4095036041e3567d450dba39f7", "title": "Active Retrieval Augmented Generation"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156", "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"}, {"paperId": "886e0962479ec6dac563666399ca4c96a468fcaa", "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "a8148b9dce6101b080e499b4c48cbe267d745131", "title": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents"}, {"paperId": "0671fd553dd670a4e820553a974bc48040ba0819", "title": "Reflexion: language agents with verbal reinforcement learning"}, {"paperId": "0d42221038c05cee8443c5b5af838505ee137dc3", "title": "ART: Automatic multi-step reasoning and tool-use for large language models"}, {"paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876", "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c", "title": "Language Is Not All You Need: Aligning Perception with Language Models"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "e5c72b92c48d68594b290c84a8904da7c8335554", "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "3599a236f285af48782fc30b1341d13ec7320735", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"}, {"paperId": "81fa4bd479191424a5cd9c4c7647e34e49c3078b", "title": "A Survey on Evaluation Metrics for Machine Translation"}, {"paperId": "2029349c55c1dba3493c5b3bd25152f18ba21ae2", "title": "Augmented Language Models: a Survey"}, {"paperId": "ca75356503c540ec9207ba203abe5884564598af", "title": "Transformer models: an introduction and catalog"}, {"paperId": "53d128ea815bcc0526856eb5a9c42cc977cb36a7", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools"}, {"paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0", "title": "Large language models encode clinical knowledge"}, {"paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d", "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"}, {"paperId": "db4ab91d5675c37795e719e997a2827d3d83cd45", "title": "Towards Reasoning in Large Language Models: A Survey"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7", "title": "PAL: Program-aided Language Models"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722", "title": "Large Language Models Are Human-Level Prompt Engineers"}, {"paperId": "1bb6d5761903c7ac978188ae36e2648905e95dc5", "title": "Transcending Scaling Laws with 0.1% Extra Compute"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2", "title": "Automatic Chain of Thought Prompting in Large Language Models"}, {"paperId": "99832586d55f540f603637e458a292406a0ed75d", "title": "ReAct: Synergizing Reasoning and Acting in Language Models"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "74eae12620bd1c1393e268bddcb6f129a5025166", "title": "Improving alignment of dialogue agents via targeted human judgements"}, {"paperId": "914254fac74a2da051cccf6ca16afcaad416a079", "title": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model"}, {"paperId": "f3cf71c51b882fe3111d71c4bf104297d38197f8", "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "a8fd9c1625011741f74401ff9bdc1c584e25c86d", "title": "Language Models are General-Purpose Interfaces"}, {"paperId": "aa4d9972af3264d032dbee58501ed4ac49477103", "title": "Scaling Laws and Interpretability of Learning from Repeated Data"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "0f733817e82026f7c29909a51cb4df7d2685f0e7", "title": "PromptChainer: Chaining Large Language Model Prompts through Visual Programming"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "3def68bd0f856886d34272840a7f81588f2bc082", "title": "Survey of Hallucination in Natural Language Generation"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "6e3f5d05b9da767df3db251030f9a082b8120e2f", "title": "Milvus"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "002c256d30d6be4b23d365a8de8ae0e67e4c9641", "title": "Improving language models by retrieving from trillions of tokens"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "b4e2e29bf0ea891610618b0615b540d4ed5a6004", "title": "Rome was built in 1776: A Case Study on Factual Correctness in Knowledge-Grounded Response Generation"}, {"paperId": "12bc45e2268a5742d21a8a37109f8793417cefcc", "title": "More Than Reading Comprehension: A Survey on Datasets and Metrics of Textual Question Answering"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "319b84be7a843250bc81d7086f79a4126d550277", "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "bb3425318de7eed5641cda147d61c9a057b9d054", "title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks"}, {"paperId": "1ccd031f28dccfb226f6c0c588c93a97a50bf95f", "title": "Measuring Coding Challenge Competence With APPS"}, {"paperId": "3feeb45cb468550bfa12e2ac8a1a4112d2dbfc1a", "title": "Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c", "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"}, {"paperId": "36f141fc5bc6813073736cf886e264606d9403bf", "title": "Q^{2}: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "8256f48f759cf85044db251cc512f965834945b3", "title": "Rethinking Positional Encoding in Language Pre-training"}, {"paperId": "1728cb805a9573b59330890ba9723e73d6c3c974", "title": "Knowledge Distillation: A Survey"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"}, {"paperId": "29e86cbeacf1e2235cc320ad240956012b294646", "title": "Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "adc61e21eafecfbf6ebecc570f9f913659a2bfb2", "title": "Deep Learning--based Text Classification"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "503bbfbc6c9654303ce993cf1dae31034dda308c", "title": "Generating Persona Consistent Dialogues by Exploiting Natural Language Inference"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "02cbb0db288af2c83b48a023f245812bd22a2408", "title": "Handling Divergent Reference Texts when Evaluating Table-to-Text Generation"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "145b8b5d99a2beba6029418ca043585b90138d12", "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "bd575d03dd5db721728ecabf6715f9425c593c0f", "title": "Manual"}, {"paperId": "9065ace5366ef548cf81bd9f239f1d132c1ef412", "title": "FlowQA: Grasping Flow in History for Conversational Machine Comprehension"}, {"paperId": "22655979df781d222eaf812b0d325fa9adf11594", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"}, {"paperId": "39e734da43eb8c72e9549b42e96760545036f8e5", "title": "QuAC: Question Answering in Context"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "99ad0533f84c110da2d0713d5798e6e14080b159", "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "cbd569036fc72ae7ff747350b91816440282596b", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "b1e20420982a4f923c08652941666b189b11b7fe", "title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task"}, {"paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "15f102c3c9f4d4fe6ba105e221df48c6e8902b3b", "title": "From captions to visual concepts and back"}, {"paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "title": "Show and tell: A neural image caption generator"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches"}, {"paperId": "fdb813d8b927bdd21ae1858cafa6c34b66a36268", "title": "Learning deep structured semantic models for web search using clickthrough data"}, {"paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17", "title": "Generating Sequences With Recurrent Neural Networks"}, {"paperId": "cb45e9217fe323fbc199d820e7735488fca2a9b3", "title": "Strategies for training large scale neural network language models"}, {"paperId": "954f688810694e53b136ea9b1c8945dd34091b17", "title": "Finite Mixture Models"}, {"paperId": "d4a258df43cc14e46988de9a4a7b2f0ea817529b", "title": "Continuous Space Language Models for Statistical Machine Translation"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "6c2b28f9354f667cd5bd07afc0471d8334430da7", "title": "A Neural Probabilistic Language Model"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": "f7566f1797eb36429acbb09e581d6b2918a50760", "title": "Foundations of Statistical Natural Language Processing"}, {"paperId": "119b5e8927f98e3fea76cbb57d7e053c24ac5c18", "title": "Fast Text Compression with Neural Networks"}, {"paperId": "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "title": "An Empirical Study of Smoothing Techniques for Language Modeling"}, {"paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260", "title": "Finding Structure in Time"}, {"paperId": "319f22bd5abfd67ac15988aa5c7f705f018c3ccd", "title": "Learning internal representations by error propagation"}, {"paperId": "60e3bd7cdf2fc948f2171e9726c4f874fba84a9a", "title": "Guidance"}, {"paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"}, {"paperId": "5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a", "title": "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"}, {"paperId": "f40aeae3e522ada1f6a9f326841b01ef5c8657b6", "title": "Unifying Language Learning Paradigms"}, {"paperId": null, "title": "\u201cJurassic-1: Technical details and evaluation,\u201d"}, {"paperId": null, "title": "CoRR , vol. abs/2110.14168"}, {"paperId": "00fd969c88ca880654520d2184e30bf73a2f3992", "title": "Microsoft"}, {"paperId": "17560c961150110a3a4d58b8d4c14e83e1ac1934", "title": "Facebook"}, {"paperId": null, "title": "\u201cElectra: Pre-training text encoders as discriminators rather than generators,\u201d"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "\u201cSocialiqa: Commonsense reasoning about social interactions,\u201d"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "\u201cDeep contextualized word representations. corr abs/1802.05365 (2018),\u201d"}, {"paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607", "title": "A Survey on Context Learning"}, {"paperId": "51891710e30da33c4ced4ae7daee1593e0cb5cc4", "title": "Machine Learning: The High Interest Credit Card of Technical Debt"}, {"paperId": "ea67efe9866b245ea2b0bbb526239fbd7070f635", "title": "An Introduction to Information Retrieval"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "231f6de83cfa4d641da1681e97a11b689a48e3aa", "title": "Statistical methods for speech recognition"}, {"paperId": "c1e3f2d537e50e0d5263e4731ab6c7983acd6687", "title": "Prediction and Entropy of Printed English"}, {"paperId": null, "title": "\u201cGemini: a family of highly capable multimodal models,\u201d"}, {"paperId": null, "title": "kickstart distributed training and terms of"}, {"paperId": null, "title": "skypilot"}, {"paperId": null, "title": "Yandex"}, {"paperId": null, "title": "ColossalAI"}, {"paperId": null, "title": "babyagi"}, {"paperId": null, "title": "EleutherAI"}, {"paperId": null, "title": "\u201cAlpaca: A strong, replicable instruction-following model,\u201d"}, {"paperId": null, "title": "llama index"}, {"paperId": null, "title": "weaviate"}, {"paperId": null, "title": "X. team"}, {"paperId": null, "title": "\u201cStripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models,\u201d"}, {"paperId": null, "title": "BMTrain"}, {"paperId": null, "title": "FastChat"}, {"paperId": null, "title": "\u201cIntroducing mpt-7b: a new standard for open-source, commercially usable llms,\u201d"}, {"paperId": null, "title": "Nvidia"}, {"paperId": null, "title": "\u201cKoala: A dialogue model for academic research,\u201d"}, {"paperId": null, "title": "text-generation-inference"}, {"paperId": null, "title": "qdrant"}, {"paperId": null, "title": "\u201cStable beluga models.\u201d"}, {"paperId": null, "title": "\u201cVigogne: French instruction-following and chat models,\u201d"}, {"paperId": null, "title": "Introducing chatgpt"}, {"paperId": null, "title": "(Year of publication, e.g., 2023) Question answering using retrieval augmented generation with foundation models in amazon sagemaker jumpstart. Accessed: Date of access, e.g., December 5, 2023"}, {"paperId": null, "title": "Megatron"}, {"paperId": null, "title": "optimizations. for Low-Rank Adaptation of Large Language Models. It reduces the number of trainable parameters by learning pairs of rank-decompostion matrices while freezing the original weights"}, {"paperId": null, "title": "unprecedented scale and speed for With DeepSpeed you can"}, {"paperId": null, "title": "with substantially increased usability and novel LoRA"}, {"paperId": null, "title": "Intrinsic Hallucinations : These directly conflict with the source material, introducing factual inaccuracies or logical inconsistencies"}, {"paperId": null, "title": "claude"}, {"paperId": null, "title": "Deployment There are various frameworks and libraries developed for LLM training, evaluation, and deployment, and covering every"}, {"paperId": null, "title": "embedchain"}, {"paperId": null, "title": "prompttools"}, {"paperId": null, "title": "Some of the popular frameworks which are useful for LLM training includes (note that some of them can be used beyond"}, {"paperId": null, "title": "\u201cHughes hallucination evaluation model (hhem) leaderboard"}]}