{"paperId": "1f133158a8973fb33fea188f20517cd7e69bfe7f", "title": "FNet: Mixing Tokens with Fourier Transforms", "abstract": "We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2021, "citationCount": 402, "influentialCitationCount": 56, "openAccessPdf": {"url": "https://aclanthology.org/2022.naacl-main.319.pdf", "status": "HYBRID"}, "tldr": {"model": "tldr@v2.0.0", "text": "The FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs)."}, "embedding": {"model": "specter_v2", "vector": [0.5095124840736389, 0.5303792953491211, -0.47098690271377563, 0.05336719751358032, -0.2778884768486023, -0.40936070680618286, 0.9198580980300903, -0.1945648491382599, -0.46737343072891235, -0.4299929141998291, 0.7525198459625244, 0.2760871946811676, 0.6492973566055298, -0.13289107382297516, -0.5680596232414246, -0.047460660338401794, -0.755717933177948, 0.04037094488739967, 0.039324596524238586, -0.3415580689907074, 0.20769718289375305, -0.8239911198616028, -1.3137067556381226, 0.18053583800792694, 0.46036240458488464, 0.7177616357803345, 0.00710399029776454, 0.8632705211639404, -0.6496211886405945, 0.6334279775619507, 0.4514431357383728, -0.30172261595726013, 0.42854034900665283, 0.17196355760097504, -0.5236531496047974, -0.39898672699928284, 0.47506359219551086, -0.2496907263994217, -0.5918089151382446, 0.6044964790344238, -0.12001170217990875, 0.13846555352210999, 0.5717121362686157, -0.886318027973175, -0.31985223293304443, 1.2285115718841553, 0.7896658778190613, 0.7775360941886902, -0.35798248648643494, -0.16502517461776733, 1.3800104856491089, -1.1467503309249878, 0.09001452475786209, 1.089640736579895, 0.6791585087776184, 0.24290591478347778, -0.14877521991729736, -0.7713153958320618, 0.5504739880561829, 0.05995803326368332, -0.4464312493801117, -0.5275292992591858, -0.16464804112911224, -0.047884583473205566, 2.156527042388916, -0.3865167498588562, 0.3462972342967987, 0.7710150480270386, -0.06642520427703857, 1.4145854711532593, -0.021965468302369118, -0.6019837260246277, -0.5798872113227844, 0.062228310853242874, 0.09003759175539017, 0.7578158974647522, -0.5109372138977051, 0.39298513531684875, -1.1281206607818604, 0.0016983009409159422, 0.778277575969696, -0.08402512967586517, 0.393853098154068, -0.09880317747592926, -0.7088856101036072, 0.4029625952243805, 0.432620108127594, 1.0415514707565308, -0.23132890462875366, 1.0894372463226318, 0.9162980914115906, 0.2749079167842865, -0.08250727504491806, 0.386920690536499, 0.16765086352825165, -0.09248792380094528, -0.7877565026283264, 0.14388668537139893, -0.04694022610783577, 1.017189383506775, -0.017604947090148926, 0.6685450077056885, -0.615445613861084, -0.16786937415599823, 1.1191946268081665, 0.14888451993465424, 0.6369813084602356, -0.739793062210083, 0.28192946314811707, -0.7149016857147217, -0.047732602804899216, -0.3572210967540741, 0.23837928473949432, -0.3888033628463745, -0.6873906254768372, -1.256779432296753, -0.2711483836174011, 0.40283796191215515, -0.6688346266746521, 0.6805776953697205, -0.3107169270515442, 0.38727521896362305, 0.17980679869651794, 0.14932683110237122, 0.808472216129303, 0.5356452465057373, 0.34021666646003723, 0.37650200724601746, 1.0215212106704712, -1.0679339170455933, -0.6369055509567261, -0.8660995960235596, 0.5637207627296448, -0.4445619285106659, 0.24162030220031738, -0.2535683810710907, -1.2823106050491333, -1.1240136623382568, -0.7549351453781128, -0.18182232975959778, -0.6283696889877319, 0.028712576255202293, 1.1370662450790405, 0.30651864409446716, -1.1200859546661377, 0.9916661381721497, -0.422221839427948, 0.11160409450531006, 0.47713568806648254, 0.2896687388420105, 0.08588995784521103, 0.13735045492649078, -1.428149938583374, 0.3283255398273468, 0.45591509342193604, -0.4872860312461853, -0.2436486929655075, -0.8586931228637695, -1.3360004425048828, 0.12073016911745071, 0.1117807999253273, -0.336835652589798, 1.3840320110321045, -0.20288383960723877, -1.1708415746688843, 0.7914608120918274, -0.3528876304626465, -0.12992258369922638, 0.4500108063220978, -0.15066072344779968, -0.396533340215683, -0.3535284399986267, 0.022411705926060677, 0.7037302255630493, 0.6115519404411316, -0.18559665977954865, -0.18200933933258057, 0.4783426523208618, -0.29770752787590027, -0.07092083990573883, -0.18607932329177856, 0.8543605208396912, 0.08384818583726883, -0.31840217113494873, 0.011658942326903343, 0.7451756000518799, 0.23550446331501007, -0.5171838402748108, -0.4701727628707886, -0.9473078846931458, 0.6621473431587219, 0.19920311868190765, 0.7329636216163635, -0.9777607321739197, -1.0927339792251587, -0.17252500355243683, 0.06570548564195633, -0.09147301316261292, -0.5594422817230225, 0.7058113217353821, -0.5441077947616577, 0.5102697014808655, 0.1609981209039688, -0.8885825872421265, 0.33708086609840393, -0.29692909121513367, -0.673204243183136, -0.36893555521965027, 0.3293275535106659, 1.1766797304153442, -1.1805641651153564, -0.200792595744133, 0.32701489329338074, 0.5434415340423584, -1.1289490461349487, 1.0353561639785767, -0.16386353969573975, -0.028543392196297646, -0.04252611845731735, -0.46934643387794495, -0.2250247448682785, -0.34705719351768494, 0.09670296311378479, -0.647903323173523, -0.23509539663791656, 0.8172005414962769, -0.279887855052948, 1.387277603149414, -0.47505322098731995, 0.6856071949005127, -0.23408617079257965, -0.5986295342445374, 0.20001152157783508, 0.5237305164337158, 0.050826627761125565, -0.6107271909713745, 0.30524319410324097, 0.5470153093338013, -0.5152732133865356, 0.04990287870168686, 0.9734933972358704, 0.7470785975456238, -0.2819083333015442, 0.07798143476247787, 0.671854555606842, -0.24762092530727386, 0.4332030415534973, 0.256465345621109, 0.769561767578125, 0.7547940015792847, 0.4319360554218292, -0.07067323476076126, -0.05718518793582916, -0.5883393287658691, -0.3338051438331604, 0.44351905584335327, 0.6839385032653809, 0.5595390200614929, 0.16350848972797394, -0.8306702375411987, -0.6190923452377319, -0.21340809762477875, 0.6187674403190613, 1.4201421737670898, -0.19544199109077454, -0.28301772475242615, -0.7839064002037048, -0.1725086122751236, -0.2585105299949646, 0.02439086139202118, -0.47711965441703796, -0.5658104419708252, -0.8384970426559448, -0.6345801949501038, 1.2653820514678955, 0.43994200229644775, 1.437153935432434, -0.8376222848892212, -0.7866138815879822, -0.23445641994476318, 0.031962793320417404, -0.8029794692993164, -0.7789905071258545, 0.7080292701721191, -0.4401591420173645, -0.024007322266697884, 0.10612725466489792, -0.2432853877544403, 0.0031740383710712194, -0.6597368121147156, 0.8605986833572388, -0.6858314871788025, -0.37056031823158264, 0.024104975163936615, 0.6303115487098694, -0.21704012155532837, -0.8428724408149719, 0.47511178255081177, -0.07547410577535629, -0.3344567120075226, 0.04719937965273857, 0.08635294437408447, -0.132811039686203, 0.07903517037630081, -0.5801067352294922, 0.2720199525356293, 0.12696146965026855, -0.04220588132739067, 0.4629024565219879, -0.31825098395347595, -0.07252456992864609, -1.0415993928909302, 0.6617951393127441, 0.26233619451522827, 0.04888416826725006, 0.03474286198616028, -0.6631851196289062, -0.01454534474760294, 0.6730414628982544, -0.8189146518707275, -0.1484411507844925, -0.8460198640823364, 0.3569207787513733, -0.34580618143081665, -0.322842001914978, -0.17709457874298096, 0.251206636428833, 0.21158865094184875, 0.345050573348999, 0.754673421382904, -0.08698682487010956, 0.1674019694328308, 0.7345846891403198, -0.8127577900886536, 0.9144501686096191, 0.12084552645683289, 0.2615376114845276, -0.1013517826795578, -0.224551722407341, -0.4681856632232666, -0.20499269664287567, -0.32223623991012573, -0.22352007031440735, -0.4859565794467926, 0.4055866599082947, -0.49814802408218384, -1.3798094987869263, 0.19064703583717346, -0.8839021325111389, -0.4607924818992615, -0.0796370804309845, -0.38478660583496094, -0.6399113535881042, -0.9726242423057556, -1.1673778295516968, -0.6992765665054321, -0.5844069123268127, -0.9431925415992737, 0.5977393984794617, 0.18881043791770935, -0.2840769588947296, -0.6759850978851318, -0.1925976723432541, -0.3836348056793213, 1.2853034734725952, -0.20702794194221497, 0.368423730134964, -0.2593607008457184, -0.2851713001728058, -0.03489214554429054, 0.32653379440307617, 0.42005297541618347, -0.020924607291817665, 0.4924063980579376, -1.1776412725448608, 0.5098585486412048, -0.3633841574192047, -0.4161592721939087, 0.39321351051330566, 0.060664910823106766, 0.602515459060669, -0.32407212257385254, -0.4812981188297272, 0.3745598793029785, 1.2901358604431152, -0.5153136849403381, 0.10430946946144104, 0.3933852016925812, 0.8920006155967712, -0.12324624508619308, -0.39669105410575867, 0.5088834762573242, 0.4850321114063263, 0.21352450549602509, 0.20510698854923248, -0.11963774263858795, -0.05469490960240364, -0.8459370136260986, 0.7872719764709473, 1.18266761302948, 0.11424693465232849, -0.02737269550561905, -0.9251688718795776, 0.5044803023338318, -1.1490075588226318, -1.1092015504837036, 0.6160242557525635, 0.7586072683334351, 0.43026188015937805, -0.2882954776287079, -0.06530684977769852, 0.18848572671413422, 0.6435771584510803, 0.4036906361579895, -0.46699678897857666, -0.7338239550590515, -0.04733813554048538, 0.6389472484588623, 0.42397356033325195, 0.31371235847473145, -0.3325199484825134, 0.7185105085372925, 15.065084457397461, 0.850246787071228, -0.27331259846687317, 0.3065110146999359, 0.5705526471138, 0.20060983300209045, -0.3533625304698944, -0.1317352056503296, -1.048488736152649, -0.12698780000209808, 1.2973872423171997, 0.19348031282424927, 0.8911432027816772, 0.004383772611618042, -0.09930863231420517, 0.4013562500476837, -0.7251588106155396, 0.4677601158618927, 0.2281804382801056, -1.2229453325271606, 0.17852790653705597, 0.11391010135412216, 0.13300791382789612, 0.4351709187030792, 0.9320054054260254, 0.8265606164932251, 0.7088610529899597, -0.5232263207435608, 0.7060393691062927, 0.24459707736968994, 1.1213295459747314, -0.06198718398809433, 0.0625094324350357, 0.2463001012802124, -1.3820878267288208, 0.09732043743133545, -0.1965760588645935, -1.1347756385803223, 0.013367350213229656, 0.008945777080953121, -0.7750412821769714, -0.365106463432312, -0.2169884592294693, 0.9136136174201965, 0.47800713777542114, 0.32540377974510193, -0.04038407653570175, 0.5550520420074463, -0.1479375809431076, -0.012235351838171482, 0.5277844071388245, 0.7678129076957703, 0.07402773946523666, 0.2884362041950226, 0.05168870836496353, -0.21963609755039215, -0.027391944080591202, 0.704264223575592, -0.3726508319377899, -0.23683573305606842, -0.4608677327632904, -0.6482476592063904, -0.00936729647219181, 0.9270583391189575, 0.27601391077041626, 0.0931110680103302, -0.021218033507466316, 0.40918678045272827, 0.34478792548179626, 0.04876655712723732, -0.6699147820472717, -0.11763252317905426, 0.3940381705760956, -0.45346787571907043, -0.11287025362253189, 0.6163209676742554, 0.11354662477970123, -0.5179060101509094, -0.6915698051452637, -0.2722560465335846, 0.24848824739456177, -0.6017221212387085, -0.518125057220459, 0.8619962930679321, -0.2429853230714798, -0.6328268647193909, -0.0001997016806853935, -0.6382171511650085, -0.3538607060909271, 0.13481715321540833, -1.4666701555252075, -0.823004961013794, 0.24881607294082642, -0.3061741590499878, -0.39557239413261414, 0.1736276000738144, 1.0947456359863281, 0.17581558227539062, 0.16734769940376282, 0.1464918553829193, -0.16938868165016174, 0.27537333965301514, -0.24031896889209747, -0.9596919417381287, 1.2500630617141724, -0.10361621528863907, -0.02460545301437378, 0.3431588113307953, 0.11366970092058182, 0.2884008586406708, -0.680166482925415, 0.05527931824326515, 0.6069707274436951, -0.5510464906692505, -0.2799476385116577, -0.7965138554573059, -0.5694055557250977, 0.4193768799304962, 0.6937661170959473, -0.09987746924161911, 0.28336167335510254, -0.2103445827960968, -0.8730570077896118, 0.05473164841532707, -0.8230287432670593, 0.006366548594087362, 0.8239602446556091, -0.8440986275672913, -0.0487605482339859, -0.24923725426197052, 0.31285709142684937, -1.192640781402588, -0.7192321419715881, -0.3190677762031555, 0.5106363892555237, -0.359097421169281, 1.0079301595687866, -0.321412593126297, 0.8628296256065369, 0.9848455190658569, 0.062041688710451126, -0.8423196077346802, 0.05233997106552124, -1.117029070854187, 0.12295715510845184, 0.39672815799713135, 0.5196691751480103, -0.7166450619697571, 0.15902243554592133, 0.5162213444709778, -0.0027802749536931515, -0.3027072846889496, -0.6401975154876709, -0.454153448343277, -0.03968217968940735, -0.515964150428772, 0.5492363572120667, 0.12532103061676025, 0.3214953541755676, -0.02788420580327511, 0.4585843086242676, -0.1061764732003212, -0.2654106616973877, -0.9160764813423157, 0.216407909989357, 0.08342333137989044, -0.16175290942192078, -0.9051456451416016, -0.7568365931510925, -1.1019266843795776, 0.11812391877174377, -1.4773041009902954, 0.06779128313064575, -0.8303936719894409, -0.3983331024646759, 0.03887486085295677, -0.22915352880954742, 0.44621288776397705, 0.363721638917923, -0.14499354362487793, -0.4601898491382599, -0.6441975235939026, -0.3996131420135498, 0.6427662968635559, 1.056024193763733, -0.7245425581932068, 0.2927766442298889, -0.08797501027584076, -0.025585617870092392, 0.17399978637695312, 0.428111732006073, -0.4142581820487976, -0.4082719087600708, -1.0770868062973022, 0.47152620553970337, -0.03168971464037895, -0.024620048701763153, -0.8794992566108704, 0.46427398920059204, 0.5363183617591858, -0.048651572316884995, 0.3182179033756256, 0.2863192558288574, -0.8834226131439209, -0.39098310470581055, 0.508384644985199, -0.9632724523544312, 0.5566043853759766, 0.2689626216888428, -0.7786815762519836, -0.16488167643547058, 0.853496253490448, -0.1921263486146927, -1.093914270401001, -0.4790738523006439, 0.309295654296875, -0.6455584764480591, 0.23141181468963623, -0.3642374575138092, -0.4445321559906006, -1.2733087539672852, -0.23846742510795593, 0.07666579633951187, 0.30146655440330505, -0.3500378131866455, 0.9870693683624268, 0.12332246452569962, -1.2953572273254395, 0.2795920968055725, 0.34137094020843506, -0.20766311883926392, 0.05109699070453644, 0.3433704078197479, 0.3871425986289978, -0.13881824910640717, 0.39570900797843933, -0.03390916809439659, 0.30503928661346436, -0.7773351073265076, -0.08871331810951233, 0.8356508016586304, -0.6060157418251038, -0.1863146871328354, 0.7319816946983337, -0.4336801767349243, -0.8911866545677185, 0.0244416706264019, -1.5874462127685547, -0.3944992125034332, -0.43778568506240845, 0.35337716341018677, 0.30074694752693176, -0.08127794414758682, -0.21360564231872559, -0.40059027075767517, 0.18574446439743042, 0.10127944499254227, -0.6534853577613831, 0.5153782963752747, 0.3629419803619385, 0.001281908480450511, 0.634297251701355, 0.775643527507782, -0.5255793929100037, -0.7322394847869873, -0.6653096675872803, -0.6520012021064758, 0.0505247600376606, 0.05503441393375397, -0.14120450615882874, -0.6321808099746704, 0.8795314431190491, 0.23577018082141876, 0.5111022591590881, -0.009755085222423077, -0.33171460032463074, 0.21552129089832306, 0.6221756339073181, -0.06384497880935669, -0.5586627125740051, -0.2442185878753662, 1.324671745300293, 0.7625112533569336, -0.4174111485481262, -0.29802221059799194, -0.22948996722698212, -0.6035429239273071, 0.838280200958252, 0.3576769232749939, -0.2680770754814148, 0.43908920884132385, 0.22420094907283783, -0.051217056810855865, 0.3772873282432556, -1.0117462873458862, -0.3059450089931488, 0.41948503255844116, 1.0497885942459106, 0.9174843430519104, 0.1728270947933197, 0.3947737514972687, 0.5699499845504761, -0.30881255865097046, 0.1016460731625557, 0.09862206131219864, 0.48282527923583984, -0.20369523763656616, -0.018068181350827217, 0.12464843690395355, 0.7306101322174072, -0.6933079361915588, -0.800129234790802, 0.13546809554100037, 0.682104229927063, 0.37137651443481445, 0.3763529658317566, 0.8221080899238586, -0.11699704080820084, 0.4064342975616455, 0.3660462200641632, 0.8575789332389832, -0.21227407455444336, -0.830262303352356, -0.15376082062721252, -0.6592942476272583, 0.013233395293354988, -0.28274595737457275, -0.6416094303131104, -0.8282105922698975, -0.31182798743247986, 0.14186100661754608, 0.1400865614414215, 0.3900166153907776, 1.1350600719451904, 0.3153514862060547, 0.7815713286399841, -0.23939315974712372, -0.761883556842804, -0.07320365309715271, -0.9567016959190369, 0.2906709611415863, -0.5787622928619385, -0.14895237982273102, -0.21884137392044067, 0.0012198389740660787, -0.22708967328071594]}, "authors": [{"authorId": "1405626394", "name": "J. Lee-Thorp"}, {"authorId": "1643737606", "name": "J. Ainslie"}, {"authorId": "2085636189", "name": "Ilya Eckstein"}, {"authorId": "1722671", "name": "Santiago Onta\u00f1\u00f3n"}], "references": [{"paperId": "3dfb1f50f2a34a699c339dabaa6f9b3a977973de", "title": "LongT5: Efficient Text-To-Text Transformer for Long Sequences"}, {"paperId": "2365410a710b421b2295cdca0074946cb50bb1d4", "title": "Are Pretrained Convolutions Better than Pretrained Transformers?"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "a8951ef48dc71f6a1778bbed964f09480fcc3f4c", "title": "Rethinking FUN: Frequency-Domain Utilization Networks"}, {"paperId": "38d5e7774e79861315e043dc2dd764d051516d74", "title": "Language Through a Prism: A Spectral Approach for Multiscale Language Representations"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "96d654dbd6d77c8b3d4fe13ee4111feee4e4fa85", "title": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding"}, {"paperId": "6044e943a7f7e8741431028fdbdaf63754cd8d5f", "title": "Pre-trained Summarization Distillation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "2f7dc1ee85e9f6a97810c66016e09ffeed684f03", "title": "Fourier Neural Operator for Parametric Partial Differential Equations"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "0b991a1a5bcdb13646ac0b6873d09bde4cc36fb5", "title": "Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "07a9f47885cae97efb7b4aa109392128532433da", "title": "Hard-Coded Gaussian Attention for Neural Machine Translation"}, {"paperId": "a238109c3969ae681eee0d4f1bf2012f28850593", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "9fd1ff433f2488a922375c3ccaa74abe2d1db914", "title": "Acceleration of Convolutional Neural Network Using FFT-Based Split Convolutions"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "57f123c95ecf9d901be3a53291f53302740451e2", "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7402b604f14b8b91c53ed6eed04af92c59636c97", "title": "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "199ff73d2f728e997f860b62a2322823d3e3d9e8", "title": "Designing and Interpreting Probes with Control Tasks"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "a039ea239e37f53a2cb60c68e0a1967994353166", "title": "Analyzing the Structure of Attention in a Transformer Language Model"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c", "title": "BERT Rediscovers the Classical NLP Pipeline"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "b56d065e56866dce8ceaca4af78a8f058aecd6a7", "title": "Learning Long Term Dependencies via Fourier Recurrent Units"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "67355b7f4edede98a3d568c9d8951bd738e280c5", "title": "FFT-based deep learning deployment in embedded systems"}, {"paperId": "0200506b4a0b582859ef24b9a946871d29dde0b4", "title": "FCNN: Fourier Convolutional Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "0173ad23353c52f02f9913758fa0e27ceee190c5", "title": "Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add"}, {"paperId": "54c3e878bf0ff2fdde16e439b5579ee99ee0d0d8", "title": "ACDC: A Structured Efficient Linear Layer"}, {"paperId": "bf76be8df2f2bc56edac98a5d0dfc19c85882eaa", "title": "Structured Transforms for Small-Footprint Deep Learning"}, {"paperId": "75d7f695563f5c9f705be99d51ec65b679766337", "title": "Fast fourier transform for feature extraction and neural network for classification of electrocardiogram signals"}, {"paperId": "5934400081d9541339da0f16d2613263f1a4c2a2", "title": "An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "title": "Fast Training of Convolutional Networks through FFTs"}, {"paperId": "7c6736272860b6422a05f4124fd8db264f53c3c5", "title": "Fault diagnosis and prognosis using wavelet packet decomposition, Fourier transform and artificial neural network"}, {"paperId": "5e53fe8e9ab1714ecbf4120f64fbfcb118d39045", "title": "Cardiac arrhythmias detection in an ECG beat signal using fast fourier transform and artificial neural network"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "aab62ec74bf64364184895ffe1252672e3fd9501", "title": "Fast Object/Face Detection Using Neural Networks and Fast Fourier Transform"}, {"paperId": "f8e9b050c93af6dea582563f61b6460b590bc3af", "title": "The Design and Implementation of FFTW3"}, {"paperId": "ab6bfdba4edc848ced52a11b537d8fe34ee954c3", "title": "Real-time discrimination of ventricular tachyarrhythmia with Fourier-transform neural network"}, {"paperId": "c1b47c7092c8f130a8d226302b9a5075edf6ee03", "title": "Using Fourier-neural recurrent networks to fit sequential input/output data"}, {"paperId": "04113e8974341f97258800126d05fd8df2751b7e", "title": "Universal approximation bounds for superpositions of a sigmoidal function"}, {"paperId": "8da1dda34ecc96263102181448c94ec7d645d085", "title": "Approximation by superpositions of a sigmoidal function"}, {"paperId": "9f8a814d7151340f97143c52236fe41d5cb0a737", "title": "On the Equivalence Between One-Dimensional Discrete Walsh-Hadamard and Multidimensional Discrete Fourier Transforms"}, {"paperId": "0e6beb95b5150ce99b108acdefabf70ccd3fee30", "title": "An algorithm for the machine calculation of complex Fourier series"}, {"paperId": "f89352fab8680ca76be2b0f03e758547bf095c26", "title": "A note on more e\ufb03cient architectures for NLP"}, {"paperId": null, "title": "2021a). We also perform a very small sweep over the embedding dimension and batch"}, {"paperId": "990d042905b8a41f601d3fdd975bc4051a852bcf", "title": "Data Movement Is All You Need: A Case Study of Transformer Networks"}, {"paperId": null, "title": "FNet\u2019s mixing weights, on"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "72ca4211f3710b3df90bc86d50f00988b08335f0", "title": "Fast Fourier Transformed Transformers: Circulant Weight Matrices for NMT Compression"}, {"paperId": null, "title": "Linformer (Wang et al., 2020) and Linear Transformer (Katharopoulos et al., 2020)"}, {"paperId": "65c0712514efa9139fce00bb17362c6cc0779950", "title": "Learning Long-Term Dependencies with"}, {"paperId": "d463daca03d09bccbc810d0ebdb30d58d7c2f57c", "title": "ForeNet : fourier recurrent networks for time series prediction"}]}