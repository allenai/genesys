{"paperId": "0a6906bd6f026d3da3031c641ed03081bd0b574e", "title": "Full Stack Optimization of Transformer Inference: a Survey", "abstract": "Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.", "venue": "arXiv.org", "year": 2023, "citationCount": 58, "influentialCitationCount": 4, "openAccessPdf": {"url": "https://arxiv.org/pdf/2302.14017", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work surveys different approaches for efficient Transformer inference, and finds that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Trans transformer inference."}, "embedding": {"model": "specter_v2", "vector": [0.2714506685733795, 0.5292439460754395, -0.6714733839035034, 0.171373188495636, 0.01344256941229105, -0.2055487483739853, 0.5780815482139587, -0.5146811008453369, -0.4871962368488312, -0.6164231896400452, 0.45148301124572754, -0.454133003950119, 0.5206207036972046, -0.13114216923713684, -0.15909522771835327, 0.016292575746774673, -0.5243910551071167, 0.006084414664655924, 0.4951515197753906, -0.213796466588974, 0.008038641884922981, -0.27461037039756775, -1.3284865617752075, 0.17361801862716675, -0.14002937078475952, 1.4302456378936768, 0.03186202421784401, 0.7092666625976562, -0.6237106919288635, 0.7557064294815063, 0.5321971774101257, -0.16629153490066528, 0.30254238843917847, 0.3722071349620819, -0.016724422574043274, -0.020985864102840424, 0.43516024947166443, -0.8418176770210266, -0.34730592370033264, 1.1457796096801758, -0.2171204537153244, 0.045741528272628784, -0.23896682262420654, -1.0339630842208862, 0.21449342370033264, 0.5879431366920471, 0.7518669366836548, 0.973633348941803, -1.1636478900909424, -0.3386625349521637, 1.078686237335205, -1.2307541370391846, -0.26888176798820496, 1.4510748386383057, 0.5244845747947693, 0.2978869378566742, -0.27849268913269043, -0.7361035943031311, 0.6030126810073853, -0.08455371856689453, -0.5621630549430847, -0.8550925850868225, 0.1939244121313095, -0.22461062669754028, 1.9793601036071777, -0.347850501537323, 0.029449118301272392, 0.4429763853549957, 0.17032556235790253, 0.9933070540428162, -0.2566184997558594, -0.46264857053756714, 0.11777877807617188, -0.18398109078407288, 0.45640823245048523, 0.74311363697052, -0.13719815015792847, 0.2932593822479248, -0.7100003957748413, 0.1290418654680252, 0.4510631263256073, 0.47826623916625977, 0.6096886992454529, 0.08600400388240814, -0.09751998633146286, 0.7370452284812927, 0.5261935591697693, 0.2708313763141632, -0.3885360062122345, 1.0775870084762573, 0.8522217273712158, 0.2163374423980713, 0.08138655871152878, 0.3952272832393646, -0.04352765530347824, 0.35910308361053467, -1.0815092325210571, 0.03530679643154144, -0.12523582577705383, 0.9596616625785828, -0.06420290470123291, 0.6947476267814636, -0.5613433718681335, -0.04915061593055725, 1.2337900400161743, 0.029886996373534203, 0.19290123879909515, -0.7692249417304993, 0.38400763273239136, -0.5674077272415161, -0.5129888653755188, 0.11161178350448608, 0.0006862368318252265, -0.48679953813552856, -0.7780144810676575, -0.4943311810493469, -0.699852705001831, 0.25839343667030334, -1.090648889541626, 0.49075624346733093, -0.8850805163383484, 0.4968636929988861, 0.23492102324962616, 0.15639320015907288, 0.4681442975997925, 0.3939245641231537, 0.3328554034233093, 0.55253666639328, 1.0763020515441895, -1.4630767107009888, -0.32042041420936584, -1.1945115327835083, 0.1573239415884018, -0.39982870221138, -0.23163734376430511, -0.37457817792892456, -1.4458168745040894, -0.8693931698799133, -0.9881743788719177, 0.017706764861941338, -0.5357284545898438, -0.13543617725372314, 1.1184041500091553, 0.22192075848579407, -1.5181546211242676, 0.6583158373832703, -0.5700166821479797, -0.03610759228467941, 0.4236540198326111, 0.6608880758285522, 0.5255438089370728, 0.16287174820899963, -0.9053173661231995, 0.09487743675708771, 0.38089150190353394, -0.3992791473865509, -0.6744445562362671, -0.8684276938438416, -0.7396316528320312, 0.40517306327819824, -0.09012001752853394, -0.7807724475860596, 1.4611197710037231, -0.2562955319881439, -0.9249579906463623, 0.5120959281921387, 0.04472503066062927, -0.2405347228050232, -0.20720703899860382, 0.21532773971557617, -0.3345705568790436, -0.47928738594055176, -0.43972504138946533, 0.5447397828102112, 0.6737409830093384, 0.2998214066028595, -0.4672101140022278, 0.1389780193567276, 0.29659709334373474, -0.19517675042152405, -0.5267595648765564, 1.237386703491211, -0.4841558337211609, 0.12761858105659485, 0.4574131965637207, 0.7223826050758362, -0.3094366490840912, 0.054609909653663635, -0.5469130277633667, -0.5673640966415405, 0.7867065668106079, 0.03980344906449318, 0.9255465269088745, -0.9696033596992493, -1.1257506608963013, 0.0465114451944828, 0.36814847588539124, 0.10768236219882965, -0.4304944574832916, 0.02616339735686779, -0.6028072834014893, 0.06339279562234879, 0.6129962801933289, -1.1367676258087158, -0.07438989728689194, -0.3453303575515747, -0.6331056952476501, -0.519960880279541, 0.04266908019781113, 0.9420857429504395, -0.5569250583648682, 0.14878953993320465, 0.0995807871222496, 0.6031904220581055, -1.1125400066375732, 1.4320347309112549, -0.19033081829547882, -0.3000123202800751, -0.026698030531406403, 0.20018383860588074, 0.2698189318180084, -0.6629825234413147, 0.6061713695526123, -1.0369144678115845, -0.20659558475017548, 0.8848308324813843, -0.002428420353680849, 1.3495970964431763, -0.4235835671424866, 0.5995749831199646, 0.43977946043014526, -0.7897638082504272, 0.31397339701652527, 0.22177043557167053, -0.14369292557239532, -0.8046936392784119, 0.7177779674530029, 0.3598625957965851, -0.43564409017562866, 0.6797418594360352, 0.9663098454475403, 0.9531124234199524, -0.23957465589046478, -0.04727180674672127, 0.901195764541626, 0.2765212655067444, 0.45486897230148315, 0.2087469846010208, 0.9017091393470764, -0.08450940251350403, -0.17979523539543152, -0.45248231291770935, 0.0889982059597969, -1.0622080564498901, 0.048908255994319916, 0.26882386207580566, 0.5068958401679993, 0.5094572305679321, 0.7505263090133667, -0.4914957880973816, -0.5038094520568848, -0.5504928827285767, 0.48686647415161133, 1.376047968864441, -0.4930102825164795, 0.214168980717659, -0.8475315570831299, -0.27699533104896545, -0.6944015622138977, -0.2063363492488861, 0.1324925720691681, -0.2792748212814331, -0.4739251136779785, -1.104925513267517, 0.8799178600311279, 0.43827205896377563, 1.2849880456924438, -0.40935295820236206, -0.5480421185493469, -0.37039506435394287, 0.5433078408241272, -1.2470039129257202, -0.4583851397037506, 0.9511040449142456, -1.206912636756897, -0.13617973029613495, 0.5539456009864807, -0.21346276998519897, 0.49795767664909363, -0.6231223940849304, 1.2049641609191895, -0.4511016607284546, -0.0749751403927803, -0.2486383467912674, 0.8662391901016235, -0.2735227048397064, -0.4322024881839752, 0.4265117049217224, -0.27497002482414246, -0.6421615481376648, 0.5382499098777771, 0.020825667306780815, 0.17723195254802704, -0.1102685034275055, -0.38372454047203064, 0.25053730607032776, 0.18588317930698395, -0.07002945244312286, 0.728567898273468, -0.08939070999622345, -0.3226053714752197, -1.0764199495315552, 0.9520170092582703, 0.2491285502910614, -0.9143741726875305, 0.056708548218011856, -1.0296926498413086, 0.3438054323196411, 0.559359610080719, -0.36111387610435486, -0.13170264661312103, -1.0821939706802368, 0.02143963612616062, -1.2986974716186523, 0.19738619029521942, -0.339692622423172, 0.03672994673252106, -0.11793391406536102, -0.15911656618118286, 0.2342119961977005, 0.6158503293991089, -0.11114244163036346, 0.17938128113746643, -1.0160431861877441, 0.6329946517944336, 0.1481333076953888, -0.02239857241511345, -0.10675544291734695, 0.12397361546754837, -0.34021100401878357, -0.5492247939109802, -0.13473670184612274, 0.0949883908033371, -0.3079443573951721, 0.3574754595756531, -0.845146894454956, -0.6924731731414795, -0.08929559588432312, -1.0511022806167603, -0.11288019269704819, 0.3582526743412018, -0.32206305861473083, 0.07916044443845749, -1.6782245635986328, -1.3494737148284912, -0.29607897996902466, -1.3740546703338623, -1.7725539207458496, 0.25257354974746704, 0.32901349663734436, -0.004413909278810024, -0.9418046474456787, -0.5864516496658325, -0.6074238419532776, 1.3720115423202515, -0.5369896292686462, 0.7847176194190979, -0.30527594685554504, -0.6141751408576965, 0.2937675714492798, -0.3630395531654358, 0.8446096181869507, -0.6015782952308655, 0.34505754709243774, -1.3525842428207397, 0.43626123666763306, -0.3681918978691101, 0.11637720465660095, 0.24259699881076813, 0.3869367837905884, 0.7399581670761108, 0.1270427256822586, -0.28503963351249695, 1.0181853771209717, 1.4990026950836182, -0.6540947556495667, 0.4390198588371277, 0.3910970389842987, 0.9797983765602112, -0.5526799559593201, -0.4938996434211731, 0.37820640206336975, -0.1290920525789261, 0.34918326139450073, 0.8509564399719238, -0.2715546786785126, -0.26276230812072754, -0.46243828535079956, 0.4713921844959259, 1.4105886220932007, 0.47181251645088196, 0.11560790240764618, -0.43678921461105347, 0.4082275629043579, -1.1097133159637451, -0.5070581436157227, 1.0411268472671509, 0.7651242017745972, -0.026335764676332474, 0.2658158242702484, -0.36497193574905396, 0.07603976130485535, 0.5415447354316711, 0.6861706376075745, -0.3054751753807068, -1.3041216135025024, 0.353807270526886, 0.9060947299003601, 1.1728323698043823, 0.5221074819564819, -0.0705595389008522, 0.4512089192867279, 14.206048965454102, 0.7683417201042175, -0.4832257330417633, 0.5901411771774292, 0.42812249064445496, 0.0791952982544899, -0.3252950608730316, 0.20306184887886047, -1.7266286611557007, 0.2404625564813614, 1.5889554023742676, 0.36492905020713806, 0.6504120230674744, 0.6006213426589966, -0.00424231356009841, 0.28040382266044617, -0.5209506154060364, 0.6347357034683228, 0.3510983884334564, -1.7021576166152954, 0.14632637798786163, 0.19589954614639282, 0.04743565618991852, 0.8054194450378418, 0.8019197583198547, 0.848389208316803, 0.6111783981323242, -0.15515035390853882, 0.4716607332229614, 0.08104629069566727, 0.9548757076263428, -0.33848077058792114, 0.5409559011459351, 0.11409777402877808, -1.0053361654281616, -0.03443104401230812, -0.3948971629142761, -1.3350437879562378, 0.11492530256509781, 0.5099723935127258, -0.7707251906394958, -0.9383745193481445, -0.05973287671804428, 0.7376565337181091, 0.15378352999687195, 0.35331135988235474, -0.4586116373538971, 0.4893311560153961, -0.3352932333946228, 0.4165555536746979, 0.3277319669723511, 0.41692861914634705, -0.2561474144458771, -0.06512891501188278, -0.02958502247929573, -0.6981142163276672, 0.24475085735321045, 0.5371680855751038, -0.704008936882019, -0.9166175127029419, -0.017467930912971497, -0.3898772597312927, 0.07304420322179794, 1.060509204864502, 0.2884920537471771, 0.4075775444507599, -0.6186615824699402, 0.1382748931646347, 0.4365875720977783, 0.089371457695961, -0.8028402328491211, 0.03195374459028244, 1.0862051248550415, -0.1805766522884369, 0.0011200174922123551, 0.7859847545623779, -0.410887748003006, -0.5249542593955994, -0.8896173238754272, -0.19276277720928192, 0.2678615152835846, -0.808047354221344, -0.12338381260633469, 1.0699739456176758, -0.1320054531097412, -0.18265441060066223, 0.21781031787395477, -1.2551326751708984, -0.5238548517227173, 0.39101091027259827, -1.7146281003952026, -0.5736658573150635, 0.12010312080383301, -0.17910997569561005, -0.5473228693008423, -0.17954973876476288, 1.1635535955429077, 0.9221296310424805, -0.5211855173110962, 0.18695950508117676, -0.25095731019973755, 0.15506340563297272, -0.4139364957809448, -0.3668476343154907, 1.1294699907302856, 0.6412091255187988, -0.4743436574935913, 0.006514917127788067, 0.1584780067205429, 0.6817046403884888, -0.6566755771636963, -0.3587985932826996, 0.590927243232727, -0.11881771683692932, 0.1810050904750824, -0.7688999772071838, -0.08939850330352783, 0.6791661977767944, 0.5289177894592285, 0.06397505104541779, 0.09999531507492065, 0.06276760995388031, -0.7886648774147034, -0.45531994104385376, -0.6946341395378113, 0.11149020493030548, 0.4650985300540924, -0.9472386240959167, 0.2052164226770401, -0.1382014900445938, 0.5947780609130859, -1.1995307207107544, -0.648037314414978, -0.12281733751296997, -0.05673450231552124, -0.394401878118515, 1.3956087827682495, -0.13259489834308624, 0.7107143402099609, 0.8436334729194641, -0.30465561151504517, -0.3980420231819153, 0.10546082258224487, -0.6956096291542053, -0.24493372440338135, 0.163303405046463, 0.5993221402168274, -0.6358373761177063, 0.7504768371582031, 0.7933080196380615, -0.1826070100069046, -0.3604099154472351, -0.4690857529640198, -0.07136396318674088, -0.3471927344799042, -0.6632171273231506, 0.548031210899353, -0.48741552233695984, -0.3895817697048187, 0.14366881549358368, 0.6231426000595093, 0.365624338388443, 0.14240427315235138, -0.6360037922859192, -0.10010626912117004, -0.051736608147621155, -0.23947331309318542, -0.6089213490486145, -1.0266615152359009, -1.2752549648284912, 0.42474889755249023, -1.0975075960159302, -0.1885276734828949, -0.680533230304718, -0.43834277987480164, 0.005180320702493191, -0.6585531234741211, 0.21620674431324005, 0.6791172027587891, -0.13720478117465973, -0.4241189658641815, -0.5706111788749695, -0.7028883695602417, 0.3654436767101288, 0.5360023975372314, -0.6255900859832764, 0.4751581847667694, -0.2524738907814026, -0.027140231803059578, 0.447518527507782, 0.4008052349090576, -0.13204926252365112, -0.9943958520889282, -1.6046324968338013, 0.09495681524276733, -0.16198299825191498, -0.06961314380168915, -1.066988229751587, 1.1109812259674072, 0.6019384264945984, -0.46314966678619385, 0.1452254056930542, 0.2111760377883911, -1.088466763496399, -0.5121248364448547, 0.21254967153072357, -0.2621133625507355, 0.6426505446434021, 0.7146523594856262, -1.0140893459320068, -0.32869580388069153, 0.7655317187309265, 0.17069624364376068, -0.6950640082359314, -1.3475430011749268, 0.5119004845619202, -0.5734536647796631, 0.00941014289855957, -0.4625987410545349, -0.2691093385219574, -1.3147908449172974, 0.136495441198349, 0.013726519420742989, 0.14943939447402954, -0.38036009669303894, 0.6782485246658325, 0.4831274747848511, -1.0851984024047852, 0.5097110867500305, 0.66838538646698, -0.357892245054245, 0.05374103784561157, 0.08639112114906311, 0.7003108859062195, -0.5643995404243469, 0.5811673402786255, -0.20780977606773376, 0.22253917157649994, -0.7761604189872742, 0.052912838757038116, 0.8012600541114807, -0.7389296889305115, 0.0634385421872139, 1.217223882675171, -0.18105626106262207, -0.7806150317192078, 0.04437486082315445, -1.5133850574493408, -0.23204095661640167, -0.40974441170692444, 0.6326023936271667, 0.08042384684085846, 0.5030003786087036, 0.24339745938777924, -0.4765472114086151, -0.3639659881591797, -0.13319383561611176, -0.2762792408466339, 0.31239354610443115, 0.18014170229434967, -0.5267085433006287, 0.0652625635266304, 1.0664228200912476, -1.2601858377456665, -0.9736629724502563, -0.8031313419342041, -0.3426462709903717, 0.08388882875442505, 1.0147502422332764, 0.10857617110013962, -1.5157132148742676, 0.8053315281867981, 0.5173935294151306, -0.1821414828300476, 0.8751772046089172, -0.32609453797340393, 0.24970103800296783, 0.23707330226898193, 0.18811827898025513, -0.39460065960884094, -0.7987915277481079, 1.3636295795440674, 0.4416939914226532, -0.5007227659225464, 0.40535008907318115, -0.30343717336654663, -0.10491803288459778, 0.8017847537994385, 0.36906272172927856, -0.09853049367666245, 1.1910196542739868, 0.8344796299934387, -0.1483190655708313, 0.30418315529823303, -1.1156413555145264, -0.3017549216747284, 0.1969330608844757, 0.6694617867469788, 0.6617023944854736, 0.005983317736536264, 0.5537883639335632, 0.874492883682251, 0.3593093156814575, 0.026142779737710953, 0.07030024379491806, 0.44398176670074463, -0.10612999647855759, -0.08805708587169647, -0.12236854434013367, 0.6659014821052551, -0.7935635447502136, -1.160462498664856, 0.5616380572319031, 0.7020448446273804, 0.23401108384132385, 0.6167237758636475, 1.4796171188354492, -0.3882409334182739, 0.6054370403289795, -0.19602210819721222, 0.43460649251937866, -0.3208640217781067, -0.49574393033981323, -0.022234000265598297, -0.36350515484809875, -0.4769808053970337, 0.06327171623706818, -0.20452913641929626, -0.5069805383682251, -0.8550654649734497, 0.568037748336792, -0.10050269216299057, 0.8830348253250122, 0.7995312809944153, 0.45485302805900574, 1.338863492012024, -0.3084859549999237, -0.7709636688232422, -0.20840491354465485, -0.6070941090583801, 0.0771511048078537, -0.7316409349441528, -0.3311633765697479, -0.140642449259758, -0.4569234848022461, -0.4376252293586731]}, "authors": [{"authorId": "2109586102", "name": "Sehoon Kim"}, {"authorId": "2029486869", "name": "Coleman Hooper"}, {"authorId": "2209989425", "name": "Thanakul Wattanawong"}, {"authorId": "2111622895", "name": "Minwoo Kang"}, {"authorId": "2209989113", "name": "Ruohan Yan"}, {"authorId": "2062998049", "name": "Hasan Gen\u00e7"}, {"authorId": "36031131", "name": "Grace Dinh"}, {"authorId": "145391991", "name": "Qijing Huang"}, {"authorId": "1732330", "name": "K. Keutzer"}, {"authorId": "1717098", "name": "Michael W. Mahoney"}, {"authorId": "40259322", "name": "Y. Shao"}, {"authorId": "10419477", "name": "A. Gholami"}], "references": [{"paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c", "title": "Accelerating Large Language Model Decoding with Speculative Sampling"}, {"paperId": "512ff5037b28be7415d318ae6e8eeb0abb8c7013", "title": "DTATrans: Leveraging Dynamic Token-Based Quantization With Accuracy Compensation Mechanism for Efficient Transformer Architecture"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "17a8bd6a5763f6607863348ce1757ac2ad3417fd", "title": "Accelerating Transformer Networks through Recomposing Softmax Layers"}, {"paperId": "d5177925836efe74c26c7118bef4c0669f3715dc", "title": "Demystifying Map Space Exploration for NPUs"}, {"paperId": "13270b9759cf0296b5a346fbb58b706e8ad0a982", "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design"}, {"paperId": "b37d57edf4a84da158ab8d77921d4aa39faceb32", "title": "FP8 Formats for Deep Learning"}, {"paperId": "4b10310731ace7f3b905bbb0c368aa154e39644a", "title": "Cerebras Architecture Deep Dive: First Look Inside the HW/SW Co-Design for Deep Learning : Cerebras Systems"}, {"paperId": "3594b5cd084e09c078bd05afb145fef9fa00f847", "title": "The Groq Software-defined Scale-out Tensor Streaming Multiprocessor : From chips-to-systems architectural overview"}, {"paperId": "003c08471fe579d98e82cf5c0cac03897403fb55", "title": "FP8 Quantization: The Power of the Exponent"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "a262a3d81f72e4b07558e636489e85ca88045156", "title": "An Algorithm\u2013Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers"}, {"paperId": "2ef60a4ea4ea53056be811ff55679eb59fb4b586", "title": "Confident Adaptive Language Modeling"}, {"paperId": "9fb327c55a30b9771a364f45f33f77778756a164", "title": "I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference"}, {"paperId": "cbff35378657225ece138c33e6a23afb3b46b41f", "title": "SALO: an efficient spatial accelerator enabling hybrid sparse attention mechanisms for long sequences"}, {"paperId": "2a682d3ecda1a3003d5b249867eae53c35c35e68", "title": "A 17\u201395.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm"}, {"paperId": "480df62560c275cb910c856c0415d0062118363d", "title": "Exocompilation for productive programming of hardware accelerators"}, {"paperId": "41aa0f5adab5c718fd8c474bf478ed855a6f3bc6", "title": "Squeezeformer: An Efficient Transformer for Automatic Speech Recognition"}, {"paperId": "dd1139cfc609c2f3263d02e97176d5275caebc0a", "title": "EfficientFormer: Vision Transformers at MobileNet Speed"}, {"paperId": "8c8fb86eb0b86fd5778c30f51f58915aa3f5a927", "title": "Speformer: An Efficient Hardware-Software Cooperative Solution for Sparse Spectral Transformer"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "9e82736043eebe3f71eb86cbef6e2ac45306ece5", "title": "Structured Pruning Learns Compact and Accurate Models"}, {"paperId": "fb145e1e49d3269d8223c7710e22b45438613ff0", "title": "A Fast Post-Training Pruning Framework for Transformers"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "82a99e0d626847fadb6be938f979a4aec573e9a1", "title": "Hardware Approximate Techniques for Deep Neural Network Accelerators: A Survey"}, {"paperId": "6da9a81b75e7ad02867860753d1aa276673a3a77", "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "844dd81f30d27ab9e42e11a5abb532095b36ac06", "title": "NN-LUT: neural approximation of non-linear operations for efficient transformer inference"}, {"paperId": "c67b1a62b868a758791c88d5465c7b6d53510fc3", "title": "Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention"}, {"paperId": "b97c3c370401dc34d2adbeb24f34de5180a14be6", "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"}, {"paperId": "5a060dc5a5bcac0879f17841a5cf70bb4302cc47", "title": "Token Pooling in Vision Transformers"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2", "title": "Primer: Searching for Efficient Transformers for Language Modeling"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "97f26219c5258b5bef84fe02a511de43133c72b3", "title": "Searching for Efficient Multi-Stage Vision Transformers"}, {"paperId": "1ad98a9e89ee373994b56058840c59a89833666a", "title": "Efficient Conformer: Progressive Downsampling and Grouped Attention for Automatic Speech Recognition"}, {"paperId": "ddcff7b456ea45987de6eef48166c90086f2d9e5", "title": "SambaNova SN10 RDU:Accelerating Software 2.0 with Dataflow"}, {"paperId": "4ac782a4d5db8ef988d2e6f4a24dda0db027c4d5", "title": "Graphcore"}, {"paperId": "a66686e60a3eda0c606e036403cf0a07a5962595", "title": "Mobile-Former: Bridging MobileNet and Transformer"}, {"paperId": "66775d9f16b3f4ca43dba2b31c7c42ca6dcba72b", "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer"}, {"paperId": "c156b1b30e3dd9284615e5304f2fb2826c09d0ff", "title": "Learned Token Pruning for Transformers"}, {"paperId": "fc46ccb83dc121c33de7ab6bdedab7d970780b2f", "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting"}, {"paperId": "e0c34a13e0b9be47828a7ab278aac44314fa3a1b", "title": "IOOpt: automatic derivation of I/O complexity bounds for affine programs"}, {"paperId": "5af69480a7ae3b571df6782a11ec4437b386a7d9", "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"}, {"paperId": "afa72122ba06b6a694c21cf67d82620662e4917c", "title": "A full-stack search technique for domain optimized deep learning accelerators"}, {"paperId": "5a09edeb26f9f116f2c0503cd020f38fb943f79b", "title": "BERT Busters: Outlier Dimensions that Disrupt Transformers"}, {"paperId": "3aef285d724db47d1218c09a0cb7c25573828c4b", "title": "CoSA: Scheduling by Constrained Optimization for Spatial Accelerators"}, {"paperId": "ae464dc54a594de682fddd59479736b1e65bbf52", "title": "TENET: A Framework for Modeling Tensor Dataflow Based on Relation-centric Notation"}, {"paperId": "dd0a27aa2285bc64798fa76944400ab6d9ce3025", "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"}, {"paperId": "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d", "title": "Consistent Accelerated Inference via Confident Adaptive Transformers"}, {"paperId": "3623a39bfbbefff942a2f370d76dd18fbc1d9139", "title": "Demystifying BERT: Implications for Accelerator Design"}, {"paperId": "79cc47c050877823222f39c5f2198256a8a38a9f", "title": "Integer-Only Zero-Shot Quantization for Efficient Speech Recognition"}, {"paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e", "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"}, {"paperId": "b080ba53a471348e7e76234decdf14e730fea7db", "title": "Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers"}, {"paperId": "d051b4e5f7e400b6c3fdd05945e6aabf6b33cae1", "title": "Mind mappings: enabling efficient algorithm-accelerator mapping space search"}, {"paperId": "21d0613c3e7fe2cb31f34441c1604edc9882fa45", "title": "NVIDIA A100 Tensor Core GPU: Performance and Innovation"}, {"paperId": "ccf4e60f5d36585d2f045aa7c0694ebc687a060d", "title": "ZigZag: Enlarging Joint Architecture-Mapping Design Space Exploration for DNN Accelerators"}, {"paperId": "4f103e260635a37a06fdde5e08a364096ed9c088", "title": "AlphaNet: Improved Training of Supernets with Alpha-Divergence"}, {"paperId": "de983239063bf87fafc549e651ea133088b1831f", "title": "Hessian-Aware Pruning and Optimal Neural Implant"}, {"paperId": "2273a3c9de32afa1818e6e8988684f6353af2b7c", "title": "A Comprehensive Survey on Hardware-Aware Neural Architecture Search"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "437d3bab7da11a3cdef8284cbac98f6e8f7522dc", "title": "Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "3af8a493cf756f9fe72623204a11e378a9cd71a5", "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"}, {"paperId": "6dbe18f41615ec60b10e4698a0c82fd8c0b05f5a", "title": "HAWQV3: Dyadic Neural Network Quantization"}, {"paperId": "78211429633a99c5ca1b66ba0a7bc6def79c7f40", "title": "GAMMA: Automating the HW Mapping of DNN Models on Accelerators via Genetic Algorithm"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "378bfce88ed3139f48fba4deeafc96846c31251d", "title": "Transferable Graph Optimizers for ML Compilers"}, {"paperId": "a50d31c082521817a1e74cae584963a63163ca70", "title": "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search"}, {"paperId": "6ce863bb510b204edb49aa90bf764da73810768a", "title": "Sparse Quantized Spectral Clustering"}, {"paperId": "b6451cfb71be72f8f9e0f5d2f529fea231adb382", "title": "Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer"}, {"paperId": "9e7b1b221fcf2c20f8cc79a7c56e5162d113e248", "title": "ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning"}, {"paperId": "a9997d7eb2296fe65abca995794627bc6170ffc7", "title": "Achieving high-performance the functional way: a functional pearl on expressing high-performance optimizations as rewrite strategies"}, {"paperId": "4d5254407ec01e6c151fb4f102547ff10fe6c9ed", "title": "A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim"}, {"paperId": "8acc99c96a9cce2a14e049f756f608dab3491f24", "title": "MCUNet: Tiny Deep Learning on IoT Devices"}, {"paperId": "7c6c31412c5dad22543bb71e31620e8868d644a3", "title": "FTRANS: energy-efficient acceleration of transformers using FPGA"}, {"paperId": "f46c562229c5bc419bbbfb63239431590e4b340a", "title": "Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "730353ede6bf5a1a478fe2e44d34b9436ca48a1c", "title": "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML Models: A Survey and Insights"}, {"paperId": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69", "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"}, {"paperId": "09bda461aa4911d0513e8e46dd39a4113947e450", "title": "Ansor : Generating High-Performance Tensor Programs for Deep Learning"}, {"paperId": "0d5cb85a6825ce7cd48c9b2ce49f3c1dc0daf8e1", "title": "A Comprehensive Survey of Neural Architecture Search"}, {"paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "7a013d654969a367eb87bf55a639d63a578a1003", "title": "Accurately computing the log-sum-exp and softmax functions"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"}, {"paperId": "0009bb60a3156707d9576d8e9b9aa5ea99e437f8", "title": "MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of DNN Mappings"}, {"paperId": "90a1491ac32e732c93773354e4e665794ed4d490", "title": "DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "e4afee97378ce41c703b9c4ee88ca442347d81c1", "title": "FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions"}, {"paperId": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416", "title": "Poor Man's BERT: Smaller and Faster Transformer Models"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "1c332cfa211400fc6f56983fb01a6692046116dd", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"}, {"paperId": "a8c0ac6588012d91c81b83b6cbd16c40e2e5edd2", "title": "BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models"}, {"paperId": "e70d609ce18cd61799b087bf3a5e14c1ce70a41a", "title": "Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey"}, {"paperId": "661a9b0b62c7327ebea00384402b66eefda2653c", "title": "OPTIMUS: OPTImized matrix MUltiplication Structure for Transformer neural network accelerator"}, {"paperId": "ac6b446d545ea995295c14a3e7e5ff7d57955677", "title": "Compute Solution for Tesla's Full Self-Driving Computer"}, {"paperId": "8da3ed272c07733ca46eab023b03c7411dfcfc42", "title": "Loss landscapes and optimization in over-parameterized non-linear systems and neural networks"}, {"paperId": "2414e08e3eb233a2a6736156bc9e87dac57fc567", "title": "Marvel: A Data-centric Compiler for DNN Operators on Spatial Accelerators"}, {"paperId": "09dad6c1d72e53d2da1cee9a60c111e084472847", "title": "The Deep Learning Compiler: A Comprehensive Survey"}, {"paperId": "d3c6c635b9cfd8890c7244d3db4be53d45944963", "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"}, {"paperId": "94f94e8892261d0377159379ca5a166ceae19a14", "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"}, {"paperId": "6e5d89c2b3b5ead2c3ab389534de62a28c1e8e6e", "title": "PyHessian: Neural Networks Through the Lens of the Hessian"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "0e0a24ca2869e56ffe47a87d360b78b6994c8f58", "title": "Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via Full-Stack Integration"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "8051e77dd37ba276b10a53da5b130d6b755c68c3", "title": "Accelergy: An Architecture-Level Energy Estimation Methodology for Accelerator Designs"}, {"paperId": "f9b990566338411ca302787687a5d0c5dc68b692", "title": "MAGNet: A Modular Accelerator Generator for Neural Networks"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "7823292e5c4b05c47af91ab6ddf671a0da709e82", "title": "Once for All: Train One Network and Specialize it for Efficient Deployment"}, {"paperId": "78b6d0b2a12de2e7c106e8b4a81a6b29cf5c47b7", "title": "DaVinci: A Scalable Architecture for Neural Network Computing"}, {"paperId": "56559fcd8b588357051e0d3c46631bdb435b963b", "title": "Towards artificial general intelligence with hybrid Tianjic chip architecture"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "f90a7bc396e205b204d5d6066a10162f84b128f9", "title": "Learning to optimize halide with tree search and random programs"}, {"paperId": "661d142c23cb2a3207d5f1ba2ac7ff61f2d4fb2f", "title": "Triton: an intermediate language and compiler for tiled neural network computations"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9", "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d", "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"}, {"paperId": "79e523beb1e1411a241edde0464b07c2ebc231d1", "title": "Single Path One-Shot Neural Architecture Search with Uniform Sampling"}, {"paperId": "9ded246119861dd325e7004b2050bba310e08797", "title": "Timeloop: A Systematic Approach to DNN Accelerator Evaluation"}, {"paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217", "title": "The State of Sparsity in Deep Neural Networks"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "45532bffbfbb5553da0b2d0844e95a1b37e59147", "title": "FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search"}, {"paperId": "9675ceedb1b0fcc3e5e83fcd4a649d326d04b051", "title": "A High-Speed and Low-Complexity Architecture for Softmax Function in Deep Learning"}, {"paperId": "2a84a30a489cd6f59725b80164b24c227d86c160", "title": "Cambricon-S: Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware Approach"}, {"paperId": "f323407464c4cd492d3fc1afd7170eab08f44d9b", "title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware"}, {"paperId": "b6b918fd7f5162fe28c88743db276bffc080576b", "title": "Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search"}, {"paperId": "117fd3a77f887f827e7f3521964b51eb788d33c5", "title": "Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators"}, {"paperId": "e73b0250971586e7d5a2ff71254a8acc60f9715a", "title": "Neural Architecture Optimization"}, {"paperId": "570a172dd7c85d23d320d8cbd10e6332a14643b3", "title": "Adaptive Tiling: Applying Fixed-size Systolic Arrays To Sparse Convolutional Neural Networks"}, {"paperId": "693c97ecedb0a84539b7162c95e89fa3cd84ca73", "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "0682bfa5cca15726aab6c00ecfac91eb44379626", "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices"}, {"paperId": "45b7b5514a65126d39a51d5a68da53e7aa244c1f", "title": "Understanding and Simplifying One-Shot Architecture Search"}, {"paperId": "b3b78f05ec663ba8f0ce7de8d32599e97fc77abe", "title": "GAP-8: A RISC-V SoC for AI at the Edge of the IoT"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "8563b6545a8ff8d17a74da1f70f57c4a7d9a38bc", "title": "DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures"}, {"paperId": "8ec3355c3bf851720df2a2f4d41c314f33d13ad2", "title": "Polyhedral auto-transformation with no integer linear programming"}, {"paperId": "63135bec9d1d19d6fa372e52fbfe68467d4b3e88", "title": "FireSim: FPGA-Accelerated Cycle-Exact Scale-Out System Simulation in the Public Cloud"}, {"paperId": "f2b70dd0312393c53d840796df004d3c3c940b49", "title": "SnaPEA: Predictive Early Activation for Reducing Computation in Deep Convolutional Neural Networks"}, {"paperId": "ec1f582446aa24f3f0920123ee6f05feea0b5e0a", "title": "Online normalizer calculation for softmax"}, {"paperId": "0261195ae7e1545caefb0ea7afb92bd66bfbb790", "title": "Glow: Graph Lowering Compiler Techniques for Neural Networks"}, {"paperId": "8c7310477fd027193cd040288f0aa9824c80b91f", "title": "Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "d16b21f3e99171c86365679435f9f03766750639", "title": "NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications"}, {"paperId": "885e6466fec0a25910bf526f256ec54b2e2c8853", "title": "An Approach for Finding Permutations Quickly: Fusion and Dimension matching"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "716b8a99dd90ae046388bb2fda81602505b51b62", "title": "Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions"}, {"paperId": "fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "title": "Efficient Neural Architecture Search via Parameter Sharing"}, {"paperId": "50bdda28de3dcf82a0e10f9ec13eea248b19edb5", "title": "Regularized Evolution for Image Classifier Architecture Search"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "917c48805635e074c16478c8eca2c40789384378", "title": "Lectures on Randomized Numerical Linear Algebra"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "5f79398057bf0bbda9ff50067bc1f2950c2a2266", "title": "Progressive Neural Architecture Search"}, {"paperId": "856451974cce2d353d5d8a5a72104984a252375c", "title": "Hierarchical Representations for Efficient Architecture Search"}, {"paperId": "c2e1139691c3a337831e36ee7afeab8817ab5d48", "title": "The tensor algebra compiler"}, {"paperId": "8a1ce657dd41a4f49990a4769000dc8049b83404", "title": "Practical Block-Wise Neural Network Architecture Generation"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "d0611891b9e8a7c5731146097b6f201578f47b2f", "title": "Learning Transferable Architectures for Scalable Image Recognition"}, {"paperId": "5ffb598866917e853087c2600671ecf851d459ca", "title": "Using Dataflow to Optimize Energy Efficiency of Deep Neural Network Accelerators"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "402f850dff86fb601d34b2841e6083ac0f928edd", "title": "SCNN: An accelerator for compressed-sparse convolutional neural networks"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "2dfeb5a90abc49ab2a80a492a01a4e2c8e92ec22", "title": "In-datacenter performance analysis of a tensor processing unit"}, {"paperId": "733a765e1f54eb86dbaabe03d7ba66d72363f665", "title": "TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory"}, {"paperId": "3f116042f50a499ab794bcc1255915bee507413c", "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey"}, {"paperId": "67d968c7450878190e45ac7886746de867bf673d", "title": "Neural Architecture Search with Reinforcement Learning"}, {"paperId": "6cd5dfccd9f52538b19a415e00031d0ee4e5b181", "title": "Designing Neural Network Architectures using Reinforcement Learning"}, {"paperId": "bc20f523a6e97800340e57a94d79926fce05572c", "title": "Cambricon-X: An accelerator for sparse neural networks"}, {"paperId": "72ed74f00d0f7312f7ed96d93ed43f0052d526bc", "title": "Fused-layer CNN accelerators"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "9b240a87b11d085641d6640f73cc3cc2d678e305", "title": "Automatically scheduling halide image processing pipelines"}, {"paperId": "930f8370e1eef4618c5460c9c9034bb78f0012fd", "title": "ASAP7: A 7-nm finFET predictive process design kit"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "5ec594e9f5ca4b629be28625cd78c882514ea3be", "title": "Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks"}, {"paperId": "5a3f0e14112d8cc14ba19e5ab8c33fc42487c4bf", "title": "The Pluto+ Algorithm"}, {"paperId": "592d2e65489f23ebd993dbdc0c84eda9ac8aadbe", "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size"}, {"paperId": "2e2b189f668cf2c06ebc44dc9b166648256cf457", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "62df84d6a4d26f95e4714796c2337c9848cc13b5", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems"}, {"paperId": "e61e896ee96c8d6ae13c34c9d66e8ee879489f75", "title": "PENCIL: A Platform-Neutral Compute Intermediate Language for Accelerator Programming"}, {"paperId": "bd6507b5c9deaf87bda81e59ce15b2309df0bf37", "title": "ShiDianNao: Shifting vision processing closer to the sensor"}, {"paperId": "4157ed3db4c656854e69931cb6089b64b08784b9", "title": "DaDianNao: A Machine-Learning Supercomputer"}, {"paperId": "31c36d445367ba204244bb74893c5654e31c3869", "title": "cuDNN: Efficient Primitives for Deep Learning"}, {"paperId": "199784f05c34bf4f9669cf79617d12dd40e36001", "title": "Hardware implementation of the exponential function using Taylor series"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "1ccaac0fdcc5ab37a45d0cc616feeaa67a3d4ca1", "title": "OpenTuner: An extensible framework for program autotuning"}, {"paperId": "233b1774f28c9972df2dfcf20dfbb0df45792bd0", "title": "A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks"}, {"paperId": "6bdb186ec4726e00a8051119636d4df3b94043b5", "title": "Caffe: Convolutional Architecture for Fast Feature Embedding"}, {"paperId": "947620a1854655ed91a86b90d12695e05be85983", "title": "1.1 Computing's energy problem (and what we can do about it)"}, {"paperId": "22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd", "title": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "4d23db55e6671a82c95dacec33b2967a4b8b677d", "title": "Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines"}, {"paperId": "e0766d67cd9a89358ab7e1621dcf6c04a6d96bc5", "title": "When polyhedral transformations meet SIMD code generation"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "3411840cd1aebcc21dd3306e3caa8e7878566a7e", "title": "Neural Acceleration for General-Purpose Approximate Programs"}, {"paperId": "2e2089ae76fe914706e6fa90081a79c8fe01611e", "title": "Practical Bayesian Optimization of Machine Learning Algorithms"}, {"paperId": "f0f4757aa2f923a349e8357e73850a78e9b80fee", "title": "A practical automatic polyhedral parallelizer and locality optimizer"}, {"paperId": "7fac481446222ea8f0a4271bdc69d6e1ce940c9e", "title": "A parameterized floating-point exponential function for FPGAs"}, {"paperId": "d755f461dddae76068f401409ba59c85a2436305", "title": "LLVM: a compilation framework for lifelong program analysis & transformation"}, {"paperId": null, "title": "Full stack optimization of transformer inference: a survey"}, {"paperId": "e6da45c79cf4c0b051b17ec9354927c61ed38776", "title": "RETROSPECTIVE: Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing"}, {"paperId": "ccf15b75d3ed3287c0ac524666578ed785bff1a3", "title": "Big Little Transformer Decoder"}, {"paperId": "065364f2592275cd0a75686d80df82a9bf2e1cfc", "title": "Searching for BurgerFormer with Micro-Meso-Macro Space Design"}, {"paperId": "b8a919f4a2aaa97bef19aa43e01f8bc347693b73", "title": "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training"}, {"paperId": "7d2a78a1f713b71c3a337247d042c5c2f0b2da84", "title": "EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "bc4bf86b9bd3bc4311ca64485a02323024f81ad4", "title": "Accelerating Attention through Gradient-Based Learned Runtime Pruning"}, {"paperId": null, "title": "Chatgpt: Optimizing language models for dialogue"}, {"paperId": null, "title": "Lamda: Languagemodels for dialog applications"}, {"paperId": "cbde5598c1a78285adfcfd77fb3636f5498987a0", "title": "EBERT: Efficient BERT Inference with Dynamic Structured Pruning"}, {"paperId": "42b65c3871ef550cc6d8e9668ef6646aa1db975c", "title": "An Optimized Data\ufb02ow for Mitigating Attention Performance Bottlenecks"}, {"paperId": "7bcc4aef957f92cf7f4d9873de13edf7b26a98a7", "title": "Neural Architecture Search and Hardware Accelerator Co-Search: A Survey"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "62852a55a64be883f529a1c236e6ca60fba3f9b1", "title": "A Learned Performance Model for Tensor Processing Units"}, {"paperId": null, "title": "Analytical characterization and design space exploration for optimization of cnns"}, {"paperId": null, "title": "Xla: Compiling machine learning for peak performance"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "DMazeRunner: Executing perfectly nested loops on dataflow accelerators"}, {"paperId": null, "title": "HAQ: Hardware-awareautomatedquantization"}, {"paperId": "ec3071fb918ad69ec80df1ca9cf1fdeb386a9603", "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "URL https://data. quora. com/First-Quora-Dataset-Release-Question-Pairs"}, {"paperId": null, "title": "New movidius myriad x vpu packs a custom neural compute engine"}, {"paperId": null, "title": "The log-sum-exp trick in machine learning, 2016"}, {"paperId": null, "title": "The max trick when computing softmax"}, {"paperId": null, "title": "The NVIDIA Deep Learning Accelerator"}, {"paperId": "58ef44a34ef69b880964b0e3527374e8fa8b10c4", "title": "Polly \u2013 Polyhedral optimization in LLVM"}, {"paperId": "d13002f50d6481ce0b43242c541e202870a50aac", "title": "Author manuscript, published in \"International Symposium on Code Generation and Optimization (CGO'11) (2011)\" Predictive Modeling in a Polyhedral Optimization Space"}, {"paperId": "3364bc50921a9566d61ef8cb73baa82341725e4b", "title": "CACTI 6.0: A Tool to Model Large Caches"}, {"paperId": "f8b1b43f284f1246ca015cc002ac949bb67c5645", "title": "Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": null, "title": "The libm library and floatingpoint arithmetic in hp-ux for itanium-based systems"}, {"paperId": "4954fa180728932959997a4768411ff9136aac81", "title": "This Paper Is Included in the Proceedings of the 12th Usenix Symposium on Operating Systems Design and Implementation (osdi '16). Tensorflow: a System for Large-scale Machine Learning Tensorflow: a System for Large-scale Machine Learning"}]}