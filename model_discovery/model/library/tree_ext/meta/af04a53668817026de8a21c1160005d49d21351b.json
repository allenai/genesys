{"paperId": "af04a53668817026de8a21c1160005d49d21351b", "title": "Visual Attention Methods in Deep Learning: An In-Depth Survey", "abstract": "Inspired by the human cognitive system, attention is a mechanism that imitates the human cognitive awareness about specific information, amplifying critical details to focus more on the essential aspects of data. Deep learning has employed attention to boost performance for many applications. Interestingly, the same attention design can suit processing different data modalities and can easily be incorporated into large networks. Furthermore, multiple complementary attention mechanisms can be incorporated into one network. Hence, attention techniques have become extremely attractive. However, the literature lacks a comprehensive survey on attention techniques to guide researchers in employing attention in their deep models. Note that, besides being demanding in terms of training data and computational resources, transformers only cover a single category in self-attention out of the many categories available. We fill this gap and provide an in-depth survey of 50 attention techniques, categorizing them by their most prominent features. We initiate our discussion by introducing the fundamental concepts behind the success of the attention mechanism. Next, we furnish some essentials such as the strengths and limitations of each attention category, describe their fundamental building blocks, basic formulations with primary usage, and applications specifically for computer vision. We also discuss the challenges and general open questions related to attention mechanisms. Finally, we recommend possible future research directions for deep attention. All the information about visual attention methods in deep learning is provided at \\href{https://github.com/saeed-anwar/VisualAttention}{https://github.com/saeed-anwar/VisualAttention}", "venue": "Information Fusion", "year": 2022, "citationCount": 59, "influentialCitationCount": 1, "openAccessPdf": {"url": "http://arxiv.org/pdf/2204.07756", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work provides an in-depth survey of 50 attention techniques, categorizing them by their most prominent features, and discusses the challenges and general open questions related to attention mechanisms."}, "embedding": {"model": "specter_v2", "vector": [0.20151063799858093, 0.650294303894043, -0.23223493993282318, 0.06824468076229095, 0.10741092264652252, 0.4883419871330261, 0.5922592282295227, -0.19936005771160126, -0.3506169617176056, -0.5366886258125305, 0.4726034998893738, 0.7955896854400635, 0.2129066437482834, -0.1768551915884018, -0.1019938737154007, 0.10179904848337173, -0.5661014318466187, -0.1483040452003479, 0.3301563858985901, -0.23294778168201447, 0.5271266102790833, -0.6294174790382385, -1.5086537599563599, 0.35164839029312134, -0.03359054774045944, 0.9184232950210571, 0.5972551107406616, 1.156489372253418, -0.24504996836185455, 0.6138811707496643, 0.3671007454395294, -0.2953531742095947, 0.11099924892187119, 0.09498654305934906, -0.6293849349021912, 0.010658983141183853, 0.966859757900238, -0.33760127425193787, -0.8984904289245605, 1.1081948280334473, -0.20052936673164368, 0.5452924966812134, 0.5235308408737183, -0.48483920097351074, -0.20845240354537964, -0.034195948392152786, 0.26950377225875854, 1.0299878120422363, -0.27404922246932983, -0.4518370032310486, 1.243447184562683, -1.2647348642349243, -0.062075696885585785, 1.7642736434936523, 0.16336074471473694, 0.5371795296669006, -0.1379368156194687, -0.2625606656074524, 0.8008319735527039, 0.024453988298773766, -0.2920629382133484, -0.14996792376041412, 0.538189709186554, -0.3251054286956787, 1.4905359745025635, -0.5441964268684387, -0.09647111594676971, 0.6550226211547852, 0.3467569053173065, 1.6568758487701416, -0.09533194452524185, -1.0293102264404297, -0.07113131880760193, 0.16279160976409912, 0.6389830112457275, 0.6901343464851379, -0.22527308762073517, 0.17610949277877808, -0.9489005208015442, 0.07148930430412292, 0.8396427035331726, -0.026903031393885612, 0.06504151225090027, -0.37684401869773865, -0.15701796114444733, 1.019634485244751, 0.9046720266342163, 0.3936803638935089, -0.6964948773384094, 0.8111594915390015, 0.11017540842294693, -0.13241760432720184, -0.3421718180179596, 0.3553881049156189, 0.5882216095924377, 0.8096387982368469, -0.49443987011909485, -0.24320942163467407, -0.2207818180322647, 1.1890994310379028, 0.047654829919338226, 0.2763357162475586, -0.4013018310070038, 0.29160118103027344, 1.15306556224823, 0.0022118370980024338, 0.13304655253887177, -0.6015135645866394, 0.3570059835910797, -0.662982702255249, -0.1758756935596466, -1.163938045501709, -0.04197226092219353, -0.49900010228157043, -0.8942697644233704, -0.14566737413406372, -0.26602962613105774, 0.3931110203266144, -0.7699738144874573, 0.34526365995407104, -0.4479299783706665, -0.5528218150138855, -0.23059551417827606, 0.8212974667549133, 0.6210834980010986, 0.3818221986293793, 0.4245392680168152, 0.7332304120063782, 1.4256364107131958, -0.9479379653930664, -0.5501236915588379, -1.2088875770568848, -0.4641944169998169, -0.5475348234176636, 0.5084625482559204, 0.11620716750621796, -1.1562659740447998, -1.4256349802017212, -0.9886948466300964, -0.4072420299053192, -0.48369860649108887, 0.20330952107906342, 1.1553343534469604, -0.15572072565555573, -0.9460400342941284, 0.8195104598999023, 0.10382998734712601, -0.5165624618530273, 0.788448691368103, 0.2023167908191681, 0.426273375749588, -0.2067200243473053, -0.7524536848068237, 0.3083082139492035, 0.03742315620183945, -0.6094478964805603, -0.3483794927597046, 0.05300028994679451, -1.0653207302093506, 0.02527322620153427, -0.000808015582151711, -0.8488054275512695, 0.9072040915489197, -0.8956472873687744, -0.7337912321090698, 0.8386596441268921, -0.5517502427101135, 0.1351165622472763, 0.046385474503040314, -0.4365514814853668, -0.44203981757164, 0.047554533928632736, -0.22740894556045532, 0.8230693340301514, 0.7813559770584106, -0.2741517722606659, -0.3105446696281433, 0.17552974820137024, -0.27180424332618713, -0.10856162756681442, -0.7849485874176025, 0.9625480771064758, -0.8252749443054199, -0.6801756024360657, 0.4642118215560913, 0.7003025412559509, 0.3427167236804962, 0.05802316591143608, -0.11687348037958145, -1.1057156324386597, 0.7967134714126587, 0.4340902864933014, 0.5875605940818787, -0.8979901075363159, -0.7070481181144714, -0.30537712574005127, 0.25432273745536804, -0.18819941580295563, -0.661082923412323, 0.18437659740447998, -0.21953025460243225, -0.11412695795297623, 0.17119432985782623, -1.0569394826889038, -0.13541598618030548, -0.319272518157959, -0.5936270952224731, 0.08139447867870331, 0.3460025489330292, 0.888293445110321, -0.7974588871002197, -0.07817166298627853, -0.006315890233963728, 0.33473050594329834, -0.719864010810852, 1.3772146701812744, -0.20770813524723053, -0.12446887791156769, -0.20999211072921753, 0.07497888803482056, 0.26039984822273254, -0.5688456892967224, -0.11732355505228043, -0.7440755367279053, 0.08053305745124817, 0.2577994763851166, -0.09802166372537613, 1.2488384246826172, -0.11083690077066422, 1.340986728668213, -0.05832661688327789, -0.5287149548530579, 0.2505403459072113, 0.1820612996816635, -0.4719611406326294, -0.5771787166595459, 0.2779345214366913, -0.5307319164276123, -0.7608392834663391, -0.05626465007662773, 0.612241804599762, 1.4615904092788696, -0.034660208970308304, 0.14253570139408112, 0.7916016578674316, -0.0925753116607666, -0.22194407880306244, 0.4890112578868866, 0.32700756192207336, 0.34224846959114075, 0.25508829951286316, -0.2435336410999298, -0.011722538620233536, -0.8957010507583618, -0.1510709971189499, 0.8787701725959778, 0.4898422062397003, 1.3063358068466187, 0.19368667900562286, -1.1944539546966553, -0.13603076338768005, 0.005927264224737883, 0.7438313961029053, 1.6362775564193726, 0.19664287567138672, 0.12994983792304993, -0.45850199460983276, -0.7166875600814819, -0.21022929251194, -0.12187055498361588, -1.0698895454406738, -0.12088780105113983, -0.07483162730932236, -0.9336574673652649, 0.4586889147758484, 0.4882488250732422, 1.340247631072998, -0.9726940393447876, -0.7253055572509766, -0.3023620843887329, 0.6264654994010925, -0.7919865250587463, -0.8158190250396729, 0.33797687292099, -0.5829863548278809, -0.45668449997901917, -0.0331772156059742, -0.14312264323234558, 0.6178342700004578, -0.23830997943878174, 1.1338391304016113, -0.695526123046875, -0.06758003681898117, 0.375632643699646, 0.8065338134765625, -1.1396394968032837, 0.1662699580192566, 0.006471056956797838, -0.1308164894580841, 0.24336348474025726, 0.7661399245262146, 0.4592497646808624, -0.39623281359672546, 0.33248981833457947, -0.4217471182346344, -0.10356061160564423, 0.39506271481513977, -0.27749571204185486, 0.7394923567771912, -0.4639664590358734, 0.16455088555812836, -1.0785984992980957, 0.6735239028930664, -0.006553006358444691, -0.22452156245708466, -0.06401088833808899, -0.5410920977592468, -0.14320768415927887, 0.022919470444321632, -0.6827735900878906, -0.6027414202690125, -0.4033087193965912, 0.6634349822998047, -0.6068241000175476, -0.7010357975959778, -0.08612557500600815, 0.47458288073539734, -0.5163797736167908, 0.7338947057723999, 0.04541479051113129, 0.22161681950092316, 0.08842311054468155, -0.12222127616405487, -0.9671550989151001, 0.8056846857070923, 0.3429996073246002, -0.13932131230831146, 0.1769075095653534, -0.08941540122032166, -0.7304233312606812, -0.9120175242424011, -0.5464223623275757, -0.27124786376953125, -0.6827768683433533, 0.3884870111942291, -0.23924782872200012, -0.72006756067276, -0.05520972982048988, -1.1683361530303955, -0.10303030908107758, 0.184705913066864, 0.14461980760097504, -0.26229557394981384, -1.0949770212173462, -0.4658336341381073, -0.5865080952644348, -0.6112329363822937, -0.8397642374038696, -0.06084812805056572, 0.6737751364707947, -0.3816228210926056, -0.563454270362854, -0.233724445104599, -0.5359134674072266, 1.2353914976119995, -0.22069251537322998, 0.44796785712242126, 0.006389410700649023, -0.2864468991756439, -0.3989737033843994, -0.29789429903030396, 0.27684760093688965, 0.1544901728630066, -0.12404196709394455, -1.5537965297698975, 0.2896294891834259, -0.04467321187257767, -0.3671918511390686, 0.782879650592804, 0.49015066027641296, 1.0878313779830933, 0.22797472774982452, -0.14435292780399323, 0.4018107056617737, 1.464454174041748, -0.6681618690490723, 0.11462514102458954, 0.13060437142848969, 0.9689061641693115, 0.5796952247619629, -0.375948041677475, 0.4936216473579407, 0.5138331651687622, 0.2164721041917801, 0.7712867259979248, -0.4984954297542572, -0.9984643459320068, -0.4572501480579376, -0.09866047650575638, 0.6848790645599365, -0.31268224120140076, 0.11775460839271545, -1.0945876836776733, 0.7964307069778442, -1.0913282632827759, -0.8508197069168091, 0.7874849438667297, 1.0011239051818848, 0.06267209351062775, -0.1327923834323883, -0.29777270555496216, -0.5947843790054321, 0.9115551710128784, 0.4174351692199707, -0.5893334150314331, -0.5838751196861267, -0.2324468195438385, 0.3921055793762207, -0.008691455237567425, 0.7470530271530151, -0.9985092282295227, 0.3127986490726471, 14.588637351989746, 0.12522567808628082, -0.12194733321666718, 0.3736031949520111, 0.62971031665802, 0.3744458854198456, 0.024448532611131668, -0.1739424765110016, -1.115390419960022, -0.06120637431740761, 0.2631860375404358, 0.5085659027099609, 0.321203351020813, 0.33763137459754944, -0.5400083661079407, -0.06267283111810684, -0.7063400745391846, 1.2422231435775757, 0.7460882663726807, -1.0876927375793457, 0.2993793189525604, 0.24404044449329376, 0.23030461370944977, 0.4473574161529541, 0.6966686248779297, 0.5253944993019104, 0.39751434326171875, -0.38915297389030457, 0.366016685962677, 0.7649363875389099, 0.574973464012146, 0.08325406908988953, 0.3602992594242096, 0.3079708516597748, -1.0572458505630493, -0.2529197633266449, -0.7699756026268005, -0.8539267778396606, -0.21674507856369019, 0.028477277606725693, 0.1244419664144516, -0.5733230113983154, 0.21284501254558563, 0.3946574330329895, -0.3091805577278137, 0.6420324444770813, -0.43289074301719666, 0.3727705478668213, -0.0807328075170517, -0.36179953813552856, 0.3859604001045227, 1.0293240547180176, 0.35666775703430176, 0.4185974597930908, -0.4110538065433502, 0.31578996777534485, 0.06176776811480522, 0.5727483034133911, -0.41735517978668213, -0.4127315282821655, -0.15226265788078308, 0.12315220385789871, -0.17144037783145905, 0.7328509092330933, 0.7190103530883789, 0.45984888076782227, -0.19806279242038727, 0.1674579530954361, 0.40653619170188904, 0.5135529637336731, -0.017382372170686722, -0.7383870482444763, 0.16130316257476807, -0.3168274760246277, 0.24241186678409576, 0.6577902436256409, -0.48723623156547546, -0.3124529719352722, -0.7746956944465637, -0.0500771589577198, 1.0691821575164795, -1.058008074760437, -1.0075783729553223, 1.2273027896881104, -0.37501075863838196, -0.21571965515613556, 0.7772123217582703, -0.9226486682891846, -0.6676014065742493, 0.7288875579833984, -1.4785780906677246, -0.5441607236862183, -0.7727570533752441, 0.05490498244762421, 0.01414946373552084, -0.473978191614151, 0.7615712285041809, -0.19494380056858063, -0.20342767238616943, 0.17576085031032562, -0.9195126891136169, -0.038449399173259735, 0.11333539336919785, -0.9168369770050049, 0.4942825138568878, 0.5026959776878357, -0.23766696453094482, 0.14160221815109253, 0.0460466593503952, 0.19357673823833466, -0.7292540073394775, 0.017913637682795525, 0.4390043318271637, -0.6364961862564087, -0.679314374923706, -0.5012878179550171, -1.1042028665542603, 0.24831750988960266, 1.0349373817443848, 0.0996154174208641, -0.1919875144958496, 0.45304644107818604, -0.7606541514396667, -0.21438084542751312, -0.5954287648200989, -0.32894185185432434, -0.047460198402404785, -0.6631299257278442, -0.7768412828445435, -0.3422066271305084, -0.19867482781410217, -0.7103155255317688, -0.0023419857025146484, -0.4510752558708191, 0.10061028599739075, -0.39288759231567383, 1.0360361337661743, -0.6533505320549011, 0.4264839291572571, 0.5344469547271729, -0.20096081495285034, -0.4763810634613037, -0.5539209842681885, -0.7363290786743164, -0.14986836910247803, 0.03547951951622963, 0.07559900730848312, -0.46018874645233154, 0.3713153898715973, 0.9554932713508606, 0.40733999013900757, -0.18261203169822693, -0.26429304480552673, 0.2072189897298813, -0.3563593626022339, -0.5020665526390076, -0.065076544880867, 0.019587228074669838, -0.325893759727478, 0.011019399389624596, 0.5436722040176392, 0.6104370951652527, 0.38323235511779785, -0.39175668358802795, 0.47763994336128235, -0.6026636362075806, -0.05283186957240105, -0.8834950923919678, -0.5459424257278442, -1.8238445520401, -0.26325151324272156, -0.775420069694519, -0.2670791745185852, -1.1221710443496704, -0.44434472918510437, 0.334166556596756, -0.5245973467826843, 0.18724268674850464, 0.32653695344924927, -0.2592400908470154, -0.1275816112756729, -0.2797207832336426, -0.5707359910011292, 0.5950627326965332, 0.9086761474609375, -0.6175476312637329, 0.24138784408569336, 0.20882587134838104, -0.2720038592815399, 0.9256749153137207, 0.44596001505851746, -0.621129035949707, -0.6290555596351624, -1.3976398706436157, 0.3139183521270752, -0.496064692735672, 0.24096564948558807, -1.1515392065048218, 1.5154187679290771, 0.4073076844215393, 0.5088285207748413, -0.24589362740516663, 0.5974238514900208, -0.8631615042686462, -1.0955451726913452, -0.010320248082280159, -0.8509533405303955, -0.02388918027281761, 0.4940083622932434, -0.22005777060985565, -0.49674272537231445, 0.6323409080505371, 0.41290098428726196, -0.8847389817237854, -1.1376590728759766, 0.5905824899673462, -0.3508607745170593, 0.25850027799606323, -0.15099792182445526, -0.23206856846809387, -1.4791030883789062, -0.5670971274375916, -0.24414388835430145, 0.26624956727027893, -0.8664701581001282, 0.793764054775238, 0.8896253705024719, -1.255947232246399, 0.05199897661805153, 0.7771249413490295, -0.0045591555535793304, -0.0030021173879504204, 0.4181351661682129, 0.31347769498825073, -0.03444581851363182, 0.2943591773509979, -0.18937000632286072, -0.022986050695180893, -0.5674509406089783, 0.11935366690158844, 1.0686321258544922, 0.2719677686691284, -0.042255304753780365, 1.1490010023117065, 0.4335303008556366, -0.5916181206703186, 0.6726822257041931, -0.6586179733276367, -0.7601175904273987, 0.2586387097835541, 0.7789000272750854, -0.011080354452133179, -0.0157659612596035, -0.07127882540225983, -0.521752119064331, 0.5410382747650146, 0.08124033361673355, -0.3630940616130829, 0.12007761746644974, -0.09237011522054672, 0.048690538853406906, 0.49612173438072205, 0.7881876826286316, -0.5943271517753601, -1.3483002185821533, -0.9483019709587097, -0.34983938932418823, -0.40822720527648926, 0.5249999761581421, -0.3764573335647583, -1.358138918876648, 1.1937836408615112, 0.6508018970489502, 0.521575391292572, 0.5661590099334717, 0.11171964555978775, -0.48159515857696533, 0.7599953413009644, -0.29289430379867554, -0.530929684638977, -0.01921594701707363, 1.1633855104446411, 1.656775951385498, -1.0205069780349731, 0.33228668570518494, -0.209059476852417, -0.8370608687400818, 1.258721113204956, 0.7542215585708618, -0.4902474582195282, 0.9931538701057434, -0.7483301758766174, -0.10178438574075699, -0.06574583798646927, -0.8671631217002869, -0.39555808901786804, 1.47438383102417, 1.1195565462112427, 0.371288001537323, -0.1735488772392273, 0.611483633518219, 0.7000729441642761, 0.29692548513412476, -0.20957671105861664, 0.33713892102241516, 0.2476322054862976, -0.4538959562778473, 0.661884605884552, -0.29245468974113464, 0.4511512517929077, -0.7129382491111755, -0.1326923370361328, 0.03583335503935814, 0.9354523420333862, 0.14961877465248108, 0.6130040884017944, 0.9895250797271729, 0.17727087438106537, 0.5866793394088745, -0.22315064072608948, 0.5694677233695984, -0.39349719882011414, -0.341601699590683, -0.08879421651363373, -0.7209775447845459, -0.24389852583408356, -0.546599805355072, -0.908890426158905, 0.20401164889335632, 0.2732212543487549, 0.3570781648159027, -0.3628181219100952, 0.37502288818359375, 0.5498763918876648, 0.7008345723152161, 0.7384032011032104, -0.37242746353149414, -0.5227344632148743, 0.0755058228969574, -1.1368658542633057, 0.2536882162094116, -0.5795716047286987, 0.07499168068170547, -0.6170889735221863, -0.3428936302661896, -0.141412615776062]}, "authors": [{"authorId": "145867132", "name": "Mohammed Hassanin"}, {"authorId": "49053414", "name": "Saeed Anwar"}, {"authorId": "144380582", "name": "Ibrahim Radwan"}, {"authorId": "2358803", "name": "F. Khan"}, {"authorId": "1747500", "name": "A. Mian"}], "references": [{"paperId": "294f24c926bcafe2a71f942e8c62361db4f0d7d5", "title": "Prophet Attention: Predicting Attention with Future Attention"}, {"paperId": "8d3ceb3c4b114b02cc624314c8b9ebfa9423b92f", "title": "Semantic Line Detection Using Mirror Attention and Comparative Ranking and Matching"}, {"paperId": "271f4348307da60d4ff5fa1f7a6f2d3a56398726", "title": "CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation"}, {"paperId": "45f686be3b96302ede327645227134e1c304dbab", "title": "Attention mechanisms in computer vision: A survey"}, {"paperId": "97b2021ce76105db5ef04389eed09ee56b076327", "title": "Investigating Attention Mechanism in 3D Point Cloud Object Detection"}, {"paperId": "cd3f158ab6af106ebab04880d8e266796f62d312", "title": "Bayesian Attention Belief Networks"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "a96d038246c4ad4a16bf97de09329bdcd0dc2ddf", "title": "Centroid Transformers: Learning to Abstract with Attention"}, {"paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b", "title": "Transformers in Vision: A Survey"}, {"paperId": "1834a1ff37052895c42906ceb163d9306badc00b", "title": "FcaNet: Frequency Channel Attention Networks"}, {"paperId": "ac0500599c6d9d7367850e76cf176894dafaa974", "title": "Mitigating the Impact of Adversarial Attacks in Very Deep Networks"}, {"paperId": "30b7e54e8d1875e435bbb55bbaa9b62ba5399082", "title": "Robust Watermarking Using Inverse Gradient Attention"}, {"paperId": "96d654dbd6d77c8b3d4fe13ee4111feee4e4fa85", "title": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "83243b3639bbb42566bc300b4999db9a2b7a93c3", "title": "Bayesian Attention Modules"}, {"paperId": "c0f709acf38eb27702b0fbce1215db0ebaa2de2b", "title": "SMYRF: Efficient Attention using Asymmetric Clustering"}, {"paperId": "b956064d5fe007c32cac9b2c4f69ef14300ea276", "title": "High-Order Attention Networks for Medical Image Segmentation"}, {"paperId": "6a69359b1d13341cad010017c5906f66eaecbd52", "title": "Kalman Filtering Attention for User Behavior Modeling in CTR Prediction"}, {"paperId": "1fe32a88a2e4162f4b3fe73ffa1fcb120bd5b1bf", "title": "Visual Grounding Via Accumulated Attention"}, {"paperId": "a51547aabbb94e7347fdad77ad8ff3c76182995a", "title": "Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference"}, {"paperId": "e5d74fa6fb97eb73bd804163906a965659c53f71", "title": "Curriculum Enhanced Supervised Attention Network for Person Re-Identification"}, {"paperId": "a56d030489511da56119b4dab06a14d22b141b64", "title": "SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition"}, {"paperId": "d2b9d6455bd97c8ed284cd6a638a9015cbd1f9f3", "title": "SPAN: Spatial Pyramid Attention Network forImage Manipulation Localization"}, {"paperId": "a8bbe21e085bf2dfe439e3ebee6da708b4c337de", "title": "End-to-End Low Cost Compressive Spectral Imaging with Spatial-Spectral Self-Attention"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "5156381d63bb3e873533b08f203cb56c2d79b6c9", "title": "Object-Centric Learning with Slot Attention"}, {"paperId": "f237473bba70ec13737dbd4ef8f0f67c3cb5c3ff", "title": "Learning Selective Self-Mutual Attention for RGB-D Saliency Detection"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "fb93ca1e004cbdcb93c8ffc57357189fa4eb6770", "title": "ResNeSt: Split-Attention Networks"}, {"paperId": "4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0", "title": "X-Linear Attention Networks for Image Captioning"}, {"paperId": "f3965da29f40bfc5f29a92990601c208f45de746", "title": "Spatial Attention Pyramid Network for Unsupervised Domain Adaptation"}, {"paperId": "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000", "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation"}, {"paperId": "a1f1d17a350d30d55de52b15c9fe7fea4ba6ff13", "title": "Visual Grounding in Video for Unsupervised Word Translation"}, {"paperId": "4fcf637197abc8022c5c301f755a26f91c06b78b", "title": "GATCluster: Self-Supervised Gaussian-Attention Network for Image Clustering"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f0e287d883755757314fbc628007df2c6709c0bb", "title": "Towards Robust Image Classification Using Sequential Attention Models"}, {"paperId": "03887d5a9829301f6e5cb2d47655db68430f4d16", "title": "One-Shot Object Detection with Co-Attention and Co-Excitation"}, {"paperId": "a4cc0701170331a1fd0e58bad962bd7f39f5efc9", "title": "GhostNet: More Features From Cheap Operations"}, {"paperId": "5f21bccbc65af943ce4c12c6eee7ae7d7d680a2a", "title": "Cross Attention Network for Few-shot Classification"}, {"paperId": "8cb34cbdcf65c23ef98430441b14a648c4e8d992", "title": "ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks"}, {"paperId": "acdfa233c64ddc2ab108585ee2985b2eac00b442", "title": "Self-Critical Attention Learning for Person Re-Identification"}, {"paperId": "289976cbe82222a30a388b4af380244cb39360d0", "title": "ACFNet: Attentional Class Feature Network for Semantic Segmentation"}, {"paperId": "9cf884f732d0658af38bafe7ad520a2be3bbebf9", "title": "Exploring Reciprocal Attention for Salient Object Detection by Cooperative Learning"}, {"paperId": "061f205009b39438e39448164064233162884252", "title": "Mixed High-Order Attention Network for Person Re-Identification"}, {"paperId": "918ee34e404ba5872d1ceae4d42dedb7f674c9f1", "title": "Expectation-Maximization Attention Networks for Semantic Segmentation"}, {"paperId": "2a687609ac1cecb9b20ba52d4f5d72ba14e0eaf2", "title": "Sampled Softmax with Random Fourier Features"}, {"paperId": "45557cc70cd6989ab6b03e5aeb787e34299099f7", "title": "Natural Adversarial Examples"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "a4a2d99d1c237d0818971ec9205e89128c57fb02", "title": "Towards Interpretable Reinforcement Learning Using Attention Augmented Agents"}, {"paperId": "eb337033885ff9e34f48d6ae0c810ef5b709efbb", "title": "Attention-Based Dropout Layer for Weakly Supervised Object Localization"}, {"paperId": "fd2a0a326db4f034fe22340c20b7bacd9a14c3d6", "title": "Second-Order Attention Network for Single Image Super-Resolution"}, {"paperId": "ef22bd2831dbb21adc1204745fe48bad01f2a314", "title": "Deep Gaussian Processes with Importance-Weighted Variational Inference"}, {"paperId": "061d6d5f3df0db70b12f9e90bec327e19b7259c1", "title": "Local Relation Networks for Image Recognition"}, {"paperId": "27ac832ee83d8b5386917998a171a0257e2151e2", "title": "Attention Augmented Convolutional Networks"}, {"paperId": "7f683133908ee1ed4d7055b15dcee1de853810f6", "title": "Real Image Denoising With Feature Attention"}, {"paperId": "6a28112ffdd3ad18d2425b94ef967ca7a605f6c3", "title": "Large Margin Multi-Modal Multi-Task Feature Extraction for Image Classification"}, {"paperId": "bd63fef46192e7dd3eec6ab5ed0c2afa19556b3a", "title": "Pyramid Feature Attention Network for Saliency Detection"}, {"paperId": "41071dbbbcbb27af3fec70de045f19c28535f5b7", "title": "Feature Denoising for Improving Adversarial Robustness"}, {"paperId": "5f4a22ee70ca613d9c0630eafc96364fe365fdf8", "title": "Efficient Attention: Attention with Linear Complexities"}, {"paperId": "5132500b23d2da47129b3f4f68dd30947a29e502", "title": "CCNet: Criss-Cross Attention for Semantic Segmentation"}, {"paperId": "37eaca9f4319c0f20ca9ccadf2389d1fed0cf772", "title": "RePr: Improved Training of Convolutional Filters"}, {"paperId": "2e46eac625e70261e43fa765c22a2828e5dd2659", "title": "An Introductory Survey on Attention Mechanisms in NLP Problems"}, {"paperId": "b7339c1deeb617c894cc08c92ed8c2d4ab14b4b5", "title": "A2-Nets: Double Attention Networks"}, {"paperId": "7e27d44e3fac723ccb703e0a83b22711bd42efe8", "title": "A Comprehensive Survey of Deep Learning for Image Captioning"}, {"paperId": "1b438d1798b0321baece242be83ae7dc69ce35b9", "title": "Multi-modal self-paced learning for image classification"}, {"paperId": "929bef0066bad871ba971b673c053112d055d29f", "title": "Actor-Attention-Critic for Multi-Agent Reinforcement Learning"}, {"paperId": "ad655c25e052fa4eeed53421344aca6f239c4c9d", "title": "Dual Attention Network for Scene Segmentation"}, {"paperId": "5cab3ce511ec8345d16a28c00094a2800b3919ce", "title": "Boosted Attention: Leveraging Human Attention for Image Captioning"}, {"paperId": "174cd8e98f17b3f5bda1c8e16cb39e3dec800f74", "title": "Multi-scale Context Intertwining for Semantic Segmentation"}, {"paperId": "ba23953a294108c05b1cecd023156b1b7f994b22", "title": "Reverse Attention for Salient Object Detection"}, {"paperId": "5d608b616efb2ae57669003b6c1067d1bb7c0b4c", "title": "Hierarchical Bilinear Pooling for Fine-Grained Visual Recognition"}, {"paperId": "cc23c580b7d8063415fb6eb512053d1079b849de", "title": "Attention Models in Graphs"}, {"paperId": "de95601d9e3b20ec51aa33e1f27b1880d2c44ef2", "title": "CBAM: Convolutional Block Attention Module"}, {"paperId": "7998468d99ab07bb982294d1c9b53a3bf3934fa6", "title": "Object Detection With Deep Learning: A Review"}, {"paperId": "01ffd50b3b82c7adac54d15cb84944b87c32525f", "title": "Latent Alignment and Variational Attention"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "6d658d1e0a5c80bd2a509ada4edda0817ae85386", "title": "Pathwise Derivatives Beyond the Reparameterization Trick"}, {"paperId": "acb9331a9489656a8db1630c0075af5a40c2a140", "title": "Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling"}, {"paperId": "3611f11b2eef86c7c85a5e9daa91a4cb4df02728", "title": "Uncertainty-Aware Attention for Reliable Interpretation and Prediction"}, {"paperId": "a8f3dc53e321fbb2565f5925def4365b9f68d1af", "title": "Self-Attention Generative Adversarial Networks"}, {"paperId": "f7cc85bed2a3d0b0ef1c0e0258f5b60ee4bb4622", "title": "Improved Fusion of Visual and Language Representations by Dense Symmetric Co-attention for Visual Question Answering"}, {"paperId": "e746c8eec81384bd37dede9700be9c8a3700f936", "title": "Context Encoding for Semantic Segmentation"}, {"paperId": "45dd2a3cd7c27f2e9509b023d702408f5ac11c9d", "title": "Stacked Cross Attention for Image-Text Matching"}, {"paperId": "2622d2467f19bc60427f8ea495515e7da82316c9", "title": "Tell Me Where to Look: Guided Attention Inference Network"}, {"paperId": "1be42f20ff086a04092b4be73e105a318ffa4322", "title": "Harmonious Attention Network for Person Re-identification"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "c70218603f0af1be5d063056cbe629e042141a86", "title": "Learn To Pay Attention"}, {"paperId": "fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "title": "Efficient Neural Architecture Search via Parameter Sharing"}, {"paperId": "5d727286a59d7e2681b6fac5fa269e782849f007", "title": "Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling"}, {"paperId": "b564a25159090c6f82cda494008081055f8917f2", "title": "Variational Attention for Sequence-to-Sequence Models"}, {"paperId": "f322eef6a4c965910e03f6997b1bc2acd413e273", "title": "Learning Semantic Concepts and Order for Image and Sentence Matching"}, {"paperId": "8899094797e82c5c185a0893896320ef77f60e64", "title": "Non-local Neural Networks"}, {"paperId": "bfe284e4338e62f0a61bb33398353efd687f206f", "title": "Learning to Compare: Relation Network for Few-Shot Learning"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "605886b07ec27492e8d5bc48f79ddc39c02ec190", "title": "Learning Deep Context-Aware Features over Body and Latent Parts for Person Re-identification"}, {"paperId": "adc276e6eae7051a027a4c269fb21dae43cadfed", "title": "DiSAN: Directional Self-Attention Network for RNN/CNN-free Language Understanding"}, {"paperId": "f3959539f8612d5fd364c8b7adedafb7534b18ce", "title": "Multi-modal Image Classification Using Low-Dimensional Texture Features for Genomic Brain Tumor Recognition"}, {"paperId": "fb37561499573109fc2cebb6a7b08f44917267dd", "title": "Squeeze-and-Excitation Networks"}, {"paperId": "557412c5aee48fb10f97fc95a4e097e40f10700d", "title": "PiCANet: Learning Pixel-Wise Contextual Attention for Saliency Detection"}, {"paperId": "8e9ad6f8b2bc97f0412fa0cc243ac6975864534a", "title": "Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"}, {"paperId": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8", "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"}, {"paperId": "5f5c075e8675d00a2e12bf12a99dafeba444e199", "title": "Semantic Segmentation with Reverse Attention"}, {"paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks"}, {"paperId": "ee4a012a4b12d11d7ab8c0e79c61e807927a163c", "title": "Rethinking Atrous Convolution for Semantic Image Segmentation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "a2cf2817e0ee69fc39a1d9bc1066f2f6fa66b40a", "title": "Hierarchical Attentive Recurrent Tracking"}, {"paperId": "536210d839dd439b14637354e23aabd7689afae0", "title": "Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-identification"}, {"paperId": "11d3bac980b8d3d72f82719e47a4406916224bd6", "title": "Concrete Dropout"}, {"paperId": "86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6", "title": "The Kinetics Human Action Video Dataset"}, {"paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0", "title": "Convolutional Sequence to Sequence Learning"}, {"paperId": "339f052b22b1362c6e22b3e8db8eeebe6eebd49d", "title": "Continuously Differentiable Exponential Linear Units"}, {"paperId": "584225123ea64c2ea4bffe2ef595ffa4bba15ef7", "title": "Paying Attention to Descriptions Generated by Image Captioning Models"}, {"paperId": "21063765fc3dc7884dc2a28c68e6c7174ab70af2", "title": "NormFace: L2 Hypersphere Embedding for Face Verification"}, {"paperId": "2c462c81970b8d7e30c19c87bb5d4ad59fc306a2", "title": "Is Second-Order Information Helpful for Large-Scale Visual Recognition?"}, {"paperId": "ff7bcaa4556cb13fc7bf03e477172493546172cd", "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?"}, {"paperId": "a456265138c088a894301c0433dae938705a9bec", "title": "Deep Sets"}, {"paperId": "9f4d7d622d1f7319cc511bfef661cd973e881a4c", "title": "Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning"}, {"paperId": "88513e738a95840de05a62f0e43d30a67b3c542e", "title": "SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning"}, {"paperId": "f24bbac95cc735c42559d53a33eaf5d689e316c1", "title": "Low-Rank Bilinear Pooling for Fine-Grained Classification"}, {"paperId": "5582bebed97947a41e3ddd9bd1f284b73f1648c2", "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"}, {"paperId": "5694e46284460a648fe29117cbc55f6c9be3fa3c", "title": "Densely Connected Convolutional Networks"}, {"paperId": "c70ad19c90491e2de8de686b6a49f9bbe44692c0", "title": "Seeing with Humans: Gaze-Assisted Neural Image Captioning"}, {"paperId": "768f7353718c6d95f2d63f954f2236369a409135", "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm"}, {"paperId": "950619635df80e87c6f25b486cc5eaad4d71d0b0", "title": "DSD: Dense-Sparse-Dense Training for Deep Neural Networks"}, {"paperId": "12f7de07f9b00315418e381b2bd797d21f12b419", "title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"}, {"paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference"}, {"paperId": "b0e6fd0c22865fa8ca7a02d18681087052c9f6c5", "title": "A Survey on Bayesian Deep Learning"}, {"paperId": "51db1f3c8dfc7d4077da39c96bb90a6358128111", "title": "Deep Networks with Stochastic Depth"}, {"paperId": "13fe71da009484f240c46f14d9330e932f8de210", "title": "Long Short-Term Memory-Networks for Machine Reading"}, {"paperId": "6f24d7a6e1c88828e18d16c6db20f5329f6a6827", "title": "Variational Inference: A Review for Statisticians"}, {"paperId": "31f9eb39d840821979e5df9f34a6e92dd9c879f2", "title": "Learning Deep Features for Discriminative Localization"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "e24c261f5cfcd58a595efb7ca684aedcb2a2f22c", "title": "Scalable Person Re-identification: A Benchmark"}, {"paperId": "327dc2fd203a7049f3409479ab68e5e2a83cd352", "title": "Compact Bilinear Pooling"}, {"paperId": "2c1890864c1c2b750f48316dc8b650ba4772adc5", "title": "Stacked Attention Networks for Image Question Answering"}, {"paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0", "title": "Effective Approaches to Attention-based Neural Machine Translation"}, {"paperId": "b624504240fa52ab76167acfe3156150ca01cf3b", "title": "Attention-Based Models for Speech Recognition"}, {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend"}, {"paperId": "4443e2e5bfd112562ecef578f772cee3c692f19f", "title": "Variational Dropout and the Local Reparameterization Trick"}, {"paperId": "6140cbe70713d92f53c7e48dc31633fae961d9c7", "title": "Multi-scale Recognition with DAG-CNNs"}, {"paperId": "7e463877264e70d53c844cf4b1bf3b15baec8cfb", "title": "ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks"}, {"paperId": "3d3f789a56dca288b2c8e23ef047a2b342184950", "title": "Bilinear CNN Models for Fine-Grained Visual Recognition"}, {"paperId": "4d8f2d14af5991d4f0d050d22216825cac3157bd", "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"}, {"paperId": "7845f1d3e796b5704d4bd37a945e0cf3fb8bbf1f", "title": "Multiple Object Recognition with Visual Attention"}, {"paperId": "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd", "title": "Deeply-Supervised Nets"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "title": "Recurrent Neural Network Regularization"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "36358eff7c34de64c0ce8aa42cf7c4da24bf8e93", "title": "Deep Metric Learning for Person Re-identification"}, {"paperId": "8a756d4d25511d92a45d0f4545fa819de993851d", "title": "Recurrent Models of Visual Attention"}, {"paperId": "1cb8f556350a653b41245020496a2aa85eefb7b9", "title": "Multi-fold MIL Training for Weakly Supervised Object Localization"}, {"paperId": "6bd36e9fd0ef20a3074e1430a6cc601e6d407fc3", "title": "DeepReID: Deep Filter Pairing Neural Network for Person Re-identification"}, {"paperId": "2b55f034a3874ad4a4b7f389e6f89e3bf2d1801e", "title": "Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels"}, {"paperId": "7f1b111f0bb703b0bd97aba505728a9b0d9b2a54", "title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "dc80bdd4a2f5403dbfa897f70856f444d697ce57", "title": "Asynchronous stochastic optimization for sequence training of deep neural networks"}, {"paperId": "327d3df8ea2020882827d6bace1e26c9d24309c2", "title": "The dropout learning algorithm"}, {"paperId": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "title": "Network In Network"}, {"paperId": "031e27c412207dce06c6f2dca9cbb6ed3884fc02", "title": "Directed Acyclic Graph Kernels for Action Recognition"}, {"paperId": "c6489d8dc7ddd117c4ec24576fe182a56ada47e7", "title": "Scene recognition and weakly supervised object localization with deformable part-based models"}, {"paperId": "751bb267e24d67f9076d5ec50c781144f5d88939", "title": "Multi-label visual classification with label exclusive context"}, {"paperId": "d63cdc1d1f023c63f8aa3b64cd5e853670680c3e", "title": "Discriminative learning in sequential pattern recognition"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "17832f01dab8145143eb73978ed4946f2722f87d", "title": "Compositional pattern producing networks: A novel abstraction of development"}, {"paperId": "d37fc9e9c4fedc32865b08661e7fb950df1f8fbe", "title": "Kernel methods in machine learning"}, {"paperId": "9c842b2926fd60b9e6ff80fee28c65e7c1ae5f1d", "title": "A non-local algorithm for image denoising"}, {"paperId": "cc58597d33d5ff5a51d7810512e2b90b056840ca", "title": "Automatic image captioning"}, {"paperId": "13eebab618907153ccc751a27c32cd882ae99121", "title": "Toward stochastic deep convective parameterization in general circulation models"}, {"paperId": "2e74388f55f2cc704c4de410578887a53a9433b0", "title": "Similarity Search in High Dimensions via Hashing"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "e526a65b9ef5afb6639fd3a062f4045d24448232", "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning"}, {"paperId": "bc22e87a26d020215afe91c751e5bdaddd8e4922", "title": "Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"}, {"paperId": "3d6ae6e9b59a871fd9259836ac9b6b7628f697f2", "title": "Principles of Neural Science"}, {"paperId": "d36efb9ad91e00faa334b549ce989bfae7e2907a", "title": "Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper"}, {"paperId": "a37e55b6bb39b50a31ac47100fb2f7ce10cc725b", "title": "Supplementary File: Image Super-Resolution with Non-Local Sparse Attention"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "88c2ad2be335264540b905b70da3bd49523e13ed", "title": "Auto Learning Attention"}, {"paperId": "d50d4a7f08b8dea3ca4a2be1b89a15a18a78410d", "title": "Deep Variational Inference"}, {"paperId": "dcbee03e26dac7646858135185cfb96518d26462", "title": "Handbook of Variational Methods for Nonlinear Geometric Data"}, {"paperId": "a35ccb023311ac50fd03370cfac1ab9548d2079a", "title": "RANet: Region Attention Network for Semantic Segmentation"}, {"paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f", "title": "AUTO-ENCODING VARIATIONAL BAYES"}, {"paperId": null, "title": "Nerf: Representing scenes as neural radiance fields for view synthesis"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Set transformer: A framework for attention-based permutation-invariant neural networks"}, {"paperId": null, "title": "Hadamard product for low-rank bilinear pooling"}, {"paperId": "4f8d648c52edf74e41b0996128aa536e13cc7e82", "title": "Deep Learning"}, {"paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614", "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning"}, {"paperId": "b1b8a329df99d7f26fcd99ae49fd2654cdddafb6", "title": "A simple approach to Bayesian network computations"}, {"paperId": "1a879afd11dd159981fead221c20357a71d6639c", "title": "The Hadamard Product"}, {"paperId": null, "title": "Self-Attention Mechanism is used in transformers and has revolutionized various computer vision tasks, including"}, {"paperId": null, "title": "Spatio-Temporal Attention extends the concept to video data, allowing models to attend to specific spatial regions and temporal frames"}, {"paperId": null, "title": "Cross-Modal Attention allows the model to align and attend to relevant information between different modalities, enabling tasks like image-text matching and visual question answering"}, {"paperId": null, "title": "Multi-Head Attention enhances the representation power and has been applied in complex computer vision tasks like image segmentation and generative modeling"}]}