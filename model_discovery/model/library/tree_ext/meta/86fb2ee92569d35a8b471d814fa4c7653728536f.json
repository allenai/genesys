{"paperId": "86fb2ee92569d35a8b471d814fa4c7653728536f", "title": "Wide Attention Is The Way Forward For Transformers", "abstract": "The Transformer is an extremely powerful and prominent deep learning architecture. In this work, we challenge the commonly held belief in deep learning that going deeper is better, and show an alternative design approach that is building wider attention Transformers. We demonstrate that wide single layer Transformer models can compete with or outperform deeper ones in a variety of Natural Language Processing (NLP) tasks when both are trained from scratch. The impact of changing the model aspect ratio on Transformers is then studied systematically. This ratio balances the number of layers and the number of attention heads per layer while keeping the total number of attention heads and all other hyperparameters constant. On average, across 4 NLP tasks and 10 attention types, single layer wide models perform 0.3% better than their deep counterparts. We show an in-depth evaluation and demonstrate how wide models require a far smaller memory footprint and can run faster on commodity hardware, in addition, these wider models are also more interpretable. For example, a single layer Transformer on the IMDb byte level text classification has 3.1x faster inference latency on a CPU than its equally accurate deeper counterpart, and is half the size. We therefore put forward wider and shallower models as a viable and desirable alternative for small models on NLP tasks, and as an important area of research for domains beyond this.", "venue": "arXiv.org", "year": 2022, "citationCount": 7, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2210.00640", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work challenges the commonly held belief in deep learning that going deeper is better, and shows an alternative design approach that is building wider attention Transformers, and puts forward wider and shallower models as a viable and desirable alternative for small models on NLP tasks."}, "embedding": {"model": "specter_v2", "vector": [0.4005677103996277, 0.76249760389328, -0.520491898059845, 0.11411787569522858, -0.3419792354106903, -0.1845310777425766, 0.6191772222518921, -0.21292650699615479, -0.5170716643333435, -0.12340950220823288, 0.6188738346099854, -0.5374171137809753, 0.5037868022918701, 0.08103224635124207, -0.029175857082009315, 0.13711553812026978, -0.24545542895793915, 0.488694965839386, -0.0736442357301712, -0.3813456892967224, -0.09392987191677094, -0.46969300508499146, -0.7357400059700012, 0.03311719372868538, 0.12287281453609467, 0.8437209725379944, 0.15453633666038513, 0.7379576563835144, -0.6473628282546997, 0.5625557899475098, 0.61274653673172, -0.5720633268356323, 0.22746151685714722, 0.49122580885887146, -0.6749269366264343, -0.43133917450904846, 0.7764548659324646, -0.1256687492132187, -0.15516461431980133, 0.9326401352882385, -0.22441457211971283, 0.1732892543077469, 0.32256680727005005, -0.8788644075393677, -0.3954493999481201, 1.0696802139282227, 0.5920833945274353, 0.7550215721130371, -0.45374423265457153, -0.43895241618156433, 1.6489696502685547, -1.233694076538086, 0.07994044572114944, 1.556991457939148, 0.7264713048934937, 0.38249731063842773, -0.10934970527887344, -0.8397547006607056, 0.5431833267211914, 0.15839245915412903, -0.6528143286705017, -0.39794135093688965, -0.34677430987358093, 0.08439339697360992, 2.2606630325317383, -0.5327656269073486, -0.37528982758522034, 0.4502534568309784, 0.12466846406459808, 1.4529966115951538, -0.18400907516479492, -0.6729139685630798, -0.5852859616279602, -0.1487301141023636, 0.6033731698989868, 0.6079826951026917, -0.36567726731300354, -0.1722974181175232, -0.7606050968170166, -0.24709422886371613, -0.006277963053435087, 0.02922423556447029, -0.07176867127418518, 0.12918199598789215, -0.2701821029186249, 0.68666011095047, 0.41776973009109497, 0.9209138751029968, -0.3347509205341339, 0.7694178223609924, 0.7084842324256897, 0.3280280828475952, -0.0876147598028183, 0.8165203332901001, -0.5190944671630859, 0.368152379989624, -0.927997887134552, 0.08570639044046402, 0.19698859751224518, 0.9086161255836487, -0.2794094383716583, 0.1590907871723175, -0.8213350772857666, 0.16150318086147308, 1.1572730541229248, 0.3368561565876007, 0.3101382851600647, -0.6433408260345459, 0.45176875591278076, -0.5059053897857666, -0.152595654129982, -0.47129881381988525, -0.1335054636001587, -0.3024039566516876, -0.8048585057258606, -1.1000339984893799, -0.16325686872005463, -0.16283570230007172, -0.7010313868522644, 0.6623026132583618, -0.5232114195823669, -0.1895381659269333, 0.22361305356025696, 0.1203448697924614, 0.5787003040313721, 0.6500855088233948, 0.4369707405567169, 0.3179188370704651, 1.4044668674468994, -0.924053966999054, -0.6665882468223572, -1.262672781944275, 0.5573974847793579, -0.38368263840675354, 0.3632966876029968, 0.2083771973848343, -1.012038230895996, -0.6812314391136169, -0.6032899618148804, -0.3508366644382477, -0.7005516886711121, 0.053911272436380386, 0.7988169193267822, 0.585858941078186, -0.9999127984046936, 0.5709164142608643, 0.09599778056144714, -0.3741001486778259, 0.5808706283569336, 0.04488954320549965, 0.22711682319641113, -0.39851677417755127, -1.457173466682434, 0.4547169506549835, 0.24085432291030884, -0.5498475432395935, -0.24914424121379852, -0.7148019075393677, -1.0617238283157349, 0.34237152338027954, 0.2767626941204071, -0.44393306970596313, 1.428113341331482, 0.009050719439983368, -1.081908106803894, 0.5495314598083496, -0.34502291679382324, 0.07215356081724167, -0.06179016828536987, -0.3792347311973572, -0.3662528991699219, -0.7405595779418945, -0.10019798576831818, 0.41823744773864746, 0.6048557162284851, 0.29930609464645386, -0.47450023889541626, 0.19504059851169586, -0.17255134880542755, -0.3442971110343933, -0.6874668002128601, 1.2030824422836304, -0.21673724055290222, -0.11056966334581375, -0.006141753401607275, 0.6407632231712341, 0.056775402277708054, -0.15874190628528595, -0.5942413806915283, -1.3616960048675537, 0.9479484558105469, 0.10217493772506714, 0.8728099465370178, -0.9642512202262878, -0.7232900261878967, -0.2458772212266922, 0.06160876899957657, -0.07196731120347977, -0.7278416752815247, 0.5373473167419434, -0.4312594532966614, 0.35817083716392517, 0.1729525476694107, -0.9896491169929504, 0.3181767761707306, -0.12859977781772614, -0.8464345932006836, -0.24165526032447815, 0.04475050047039986, 0.9806482791900635, -0.5812472701072693, 0.05554407462477684, -0.2385159283876419, 0.2928621470928192, -0.8351308107376099, 1.1685893535614014, -0.5574524402618408, -0.3787280023097992, -0.11203128099441528, -0.17235985398292542, 0.002692907117307186, -0.31496191024780273, 0.07413844019174576, -0.7684736251831055, -0.15973135828971863, 0.7198203802108765, -0.12740817666053772, 1.1433550119400024, -0.8477832674980164, 0.5137618780136108, 0.2825402021408081, -0.6035537123680115, 0.04991893097758293, 0.6059705018997192, -0.5567557215690613, -0.36715826392173767, 0.28541556000709534, 0.47725236415863037, -0.5766766667366028, 0.3275703489780426, 0.7650737166404724, 0.8056026697158813, -0.24286679923534393, 0.3469714820384979, 0.5946794152259827, -0.16954679787158966, 0.28473350405693054, 0.3974858820438385, 0.8464558124542236, 0.26074081659317017, 0.345924973487854, -0.23494625091552734, 0.32608407735824585, -0.6623085737228394, 0.08563140034675598, 0.5148317217826843, 0.7810581922531128, 0.7710281014442444, 0.7106611132621765, -0.6447917222976685, -0.3891788423061371, 0.18262197077274323, 0.46911951899528503, 1.674554467201233, -0.6591632962226868, -0.11857087165117264, -0.6307553648948669, -0.6117064356803894, -0.19332347810268402, 0.14342378079891205, -0.17019422352313995, 0.1147235855460167, -0.5391144156455994, -1.1819038391113281, 1.088559627532959, 0.18069852888584137, 0.8159270882606506, -0.5672787427902222, -0.31728145480155945, -0.4435900151729584, 0.037793997675180435, -1.1333651542663574, -0.5373015403747559, 0.5858186483383179, -0.4740767478942871, -0.2166815996170044, 0.0446193553507328, 0.03484494984149933, 0.12501749396324158, -0.838490903377533, 0.9692181348800659, -0.7561626434326172, -0.17594152688980103, 0.2804687023162842, 0.5237603187561035, -0.5558534264564514, -0.6833577752113342, 0.5678086876869202, -0.14603807032108307, -0.12384890019893646, 0.667003333568573, 0.45958030223846436, 0.23661692440509796, -0.036853399127721786, -0.24131496250629425, -0.09094026684761047, 0.3967488706111908, 0.02968859113752842, 0.5734283328056335, -0.30316048860549927, -0.1421545296907425, -1.3765162229537964, 1.0835375785827637, -0.0176544152200222, -0.37812069058418274, 0.38339847326278687, -0.7417876720428467, -0.03589222952723503, 0.8013392686843872, -0.5881233811378479, -0.24995748698711395, -0.8406617045402527, 0.002732135821133852, -0.437200665473938, -0.2813747525215149, 0.22244520485401154, 0.0053244698792696, 0.17912721633911133, 0.17211736738681793, 0.39173734188079834, 0.38842862844467163, -0.36896589398384094, 0.6386104822158813, -0.5569453239440918, 0.3317408859729767, 0.3304677903652191, 0.08696259558200836, -0.4777050316333771, -0.22083806991577148, -0.8236526846885681, -0.6150965690612793, -0.31462571024894714, 0.05585414916276932, 0.30569976568222046, 0.21153675019741058, -0.302590012550354, -0.7460225224494934, 0.018058374524116516, -1.2485443353652954, -0.14731745421886444, 0.09387917071580887, -0.4642719328403473, 0.17489556968212128, -1.1210711002349854, -1.3052117824554443, -0.18438516557216644, -0.988759458065033, -0.9841575622558594, 0.6777094006538391, 0.08865485340356827, -0.5806975960731506, -0.7938123941421509, -0.3060082197189331, -0.12135770171880722, 1.4246976375579834, -0.7745926976203918, 1.0912765264511108, -0.5143746733665466, -0.15368221700191498, -0.3736315965652466, -0.164106085896492, 0.4305732250213623, -0.21037514507770538, 0.16989347338676453, -1.0495216846466064, 0.4192691147327423, -0.048949144780635834, -0.39815324544906616, 0.14232130348682404, 0.1708800047636032, 0.526599645614624, -0.18563011288642883, -0.5993404984474182, 0.3897946774959564, 1.3307214975357056, -0.9749878644943237, 0.19746223092079163, 0.22058942914009094, 1.0619776248931885, -0.08391273021697998, -0.4588260054588318, 0.37630510330200195, 0.347223162651062, 0.11120965331792831, 0.36313217878341675, 0.020729968324303627, -0.3176698088645935, -0.4072614014148712, 0.2520058751106262, 1.1338526010513306, 0.3612617552280426, -0.29802295565605164, -1.4097429513931274, 0.6602311730384827, -0.9890578389167786, -0.6478486657142639, 0.5586111545562744, 0.38423722982406616, 0.46545860171318054, -0.2817067801952362, -0.8275896310806274, -0.13740894198417664, 0.3115667402744293, 0.4516400396823883, -0.27308908104896545, -1.0170074701309204, 0.11818724125623703, 0.637941300868988, 0.4923325479030609, 0.6879057288169861, -0.3533369302749634, 0.6648610234260559, 14.730566024780273, 1.047894835472107, -0.18637734651565552, 0.7330498695373535, 0.4827509820461273, 0.2623736262321472, -0.29394781589508057, -0.33742624521255493, -1.9453433752059937, -0.35189762711524963, 0.9232574105262756, 0.21989598870277405, 0.35434532165527344, 0.23494413495063782, -0.21380704641342163, 0.3380891978740692, -0.5407409071922302, 0.5724743008613586, 0.5134952664375305, -1.1263933181762695, 0.7053955793380737, 0.2898561656475067, 0.011647299863398075, 0.7026485800743103, 0.6428149342536926, 0.6968019604682922, 0.7642271518707275, -0.7060126066207886, 0.40121763944625854, -8.23198352009058e-05, 0.7491288185119629, 0.2746735215187073, 0.6959267258644104, 0.6971727609634399, -0.8039029240608215, -0.4595310688018799, -0.4546877443790436, -1.2352933883666992, -0.14191579818725586, 0.39459913969039917, -0.5410588383674622, -0.6293727159500122, -0.14387190341949463, 0.8727256059646606, 0.12905091047286987, 0.17368246614933014, -0.44609376788139343, 0.5649450421333313, -0.35061848163604736, -0.11458218842744827, 0.4604839086532593, 1.017926573753357, 0.27515289187431335, 0.27338850498199463, -0.026465624570846558, -0.334852933883667, 0.21111470460891724, 0.3822234869003296, -0.7011470794677734, -0.048668719828128815, -0.15811879932880402, -0.36535078287124634, -0.05053027719259262, 1.276231050491333, 0.4860912561416626, 0.44639909267425537, -0.40806347131729126, 0.19172054529190063, 0.6436023712158203, 0.13736492395401, 0.0573752336204052, -0.37676185369491577, 0.4622228741645813, -0.4184129238128662, 0.17006778717041016, 0.7913094758987427, -0.33696797490119934, -0.4253247082233429, -0.7604386210441589, -0.582345187664032, 0.6912749409675598, -0.6448660492897034, -0.7483906745910645, 0.9237151145935059, -0.5329390168190002, -0.16673828661441803, 0.4945857524871826, -1.0848623514175415, -0.2638035714626312, 0.7807462811470032, -1.643486499786377, -0.9862630367279053, 0.4462810456752777, -0.17054669559001923, -0.2952946722507477, -0.043385617434978485, 1.205675721168518, 0.15525026619434357, -0.2952163815498352, 0.12860119342803955, -0.06898217648267746, 0.5058793425559998, -0.2516494691371918, -0.8955228924751282, 0.9189226031303406, 0.3056224584579468, -0.2416137456893921, 0.22906607389450073, -0.05255972966551781, 0.3019399344921112, -0.9258270263671875, -0.18866094946861267, 1.1655638217926025, -1.0128252506256104, -0.13787315785884857, -0.6694518327713013, -0.7054846286773682, 0.5046981573104858, 0.7388659119606018, -0.21981202065944672, 0.22487464547157288, 0.4658856987953186, -0.9338407516479492, -0.22392725944519043, -0.773946225643158, -0.14501753449440002, 0.26612016558647156, -0.819062352180481, -0.5480650663375854, -0.1440100520849228, 0.5401025414466858, -0.9520025849342346, -0.6767574548721313, -0.22054386138916016, 0.27077820897102356, 0.4255209267139435, 0.8612610697746277, -0.49451902508735657, 0.8351645469665527, 1.0048967599868774, -0.2419462651014328, -0.6194860935211182, -0.006514196749776602, -0.6955398321151733, 0.0863172858953476, 0.17267026007175446, 0.9548152685165405, -0.9011824131011963, 0.2903415262699127, 0.9442822337150574, 0.3050459623336792, -0.3711738586425781, -0.6491687893867493, -0.20440945029258728, 0.21870313584804535, -0.4338769018650055, 0.34264683723449707, -0.06798513233661652, 0.12120826542377472, 0.34328827261924744, 0.6718164682388306, 0.5457646250724792, 0.09395822137594223, -0.6094723343849182, 0.13337542116641998, -0.45240476727485657, 0.19220124185085297, -0.7636473178863525, -0.49383795261383057, -1.3980352878570557, 0.17236445844173431, -1.0749386548995972, -0.06061277911067009, -1.2769652605056763, -0.6256033182144165, 0.016801372170448303, -0.08206222951412201, 0.5970169901847839, 0.3002259433269501, -0.4063534438610077, -0.49661731719970703, -0.616502046585083, -0.32116714119911194, 0.4579107165336609, 0.8099271059036255, -0.7849962115287781, 0.6313965916633606, 0.18404293060302734, -0.3264193832874298, 0.3944937288761139, 0.3862154185771942, -0.41551482677459717, -0.7954903841018677, -1.3518037796020508, 0.5093132257461548, -0.23077599704265594, -0.2679610848426819, -0.5221746563911438, 1.0586434602737427, 0.4203139841556549, -0.396607369184494, 0.39182496070861816, 0.32415351271629333, -1.159255027770996, -0.8338790535926819, 0.2933434844017029, -0.7873235940933228, 0.12057361006736755, 0.32325366139411926, -0.754447340965271, -0.29502248764038086, 0.5310238599777222, -0.03066406399011612, -1.1098620891571045, -0.6357651352882385, 0.4775736927986145, -0.603821873664856, 0.4503318667411804, -0.4422396719455719, -0.18074935674667358, -1.2741657495498657, 0.029100071638822556, 0.13672944903373718, 0.6611157059669495, -0.5309752821922302, 0.9841502904891968, 0.13739867508411407, -0.6300339698791504, 0.08778361976146698, 0.20501984655857086, -0.18503189086914062, 0.03130609542131424, 0.13358062505722046, 0.4322478473186493, -0.3572751581668854, 0.7038511037826538, 0.3315478563308716, 0.24499259889125824, -1.046650767326355, -0.0917450413107872, 0.9209423065185547, -0.5882318615913391, -0.5627897381782532, 1.1943683624267578, -0.4905170798301697, -1.1742868423461914, 0.3265915811061859, -1.575704574584961, -0.7270691394805908, -0.2211812138557434, 0.850334644317627, -0.07311738282442093, 0.1856946051120758, 0.22187858819961548, -0.5120410323143005, 0.310625284910202, -0.017213933169841766, -0.23723489046096802, 0.4824078679084778, -0.09195179492235184, -0.7889465093612671, 0.4460371732711792, 0.6499438881874084, -0.7572391033172607, -0.24918010830879211, -0.9363036155700684, -0.1722850501537323, 0.09756483137607574, 0.43426185846328735, -0.5328766703605652, -0.6819385290145874, 1.040353775024414, 0.20117919147014618, 0.617845356464386, 0.1959417462348938, -0.19341978430747986, 0.39193692803382874, 0.8973817229270935, 0.04650432989001274, -0.32491374015808105, -0.9088770747184753, 1.645487904548645, 1.1850608587265015, -0.7195394039154053, 0.051384735852479935, -0.3564439117908478, -0.48049187660217285, 0.901516318321228, 0.38707512617111206, 0.2894817888736725, 1.0663957595825195, 0.35669341683387756, 0.045369889587163925, 0.039636578410863876, -1.1340473890304565, -0.029302485287189484, 0.5180830359458923, 0.9246953129768372, 0.7837240099906921, -0.06961573660373688, 0.28670161962509155, 0.9715873599052429, 0.0820237249135971, 0.13262100517749786, 0.35875028371810913, 0.3236117959022522, -0.1709779053926468, 0.014001712203025818, -0.0675324872136116, 0.6868730783462524, -1.109805703163147, -1.0707392692565918, 0.16034139692783356, 0.6046047806739807, 0.24971866607666016, 0.34815648198127747, 0.6363181471824646, 0.3542611002922058, 0.7314915060997009, 0.4406447112560272, 0.3144456446170807, -0.6924545168876648, -0.5442406535148621, -0.7612950801849365, -0.4913909137248993, -0.05504072457551956, 0.028403842821717262, -0.412308931350708, -0.19101771712303162, -0.2756303548812866, 0.39937829971313477, 0.19705472886562347, 0.30587077140808105, 0.9970830082893372, 0.5080690979957581, 0.5608130693435669, -0.6362606883049011, -0.4402768015861511, -0.45988693833351135, -1.0383154153823853, -0.09344198554754257, -0.8955555558204651, -0.08585832267999649, 0.07224670052528381, -0.251714289188385, -0.3134983479976654]}, "authors": [{"authorId": "72934867", "name": "Jason Brown"}, {"authorId": "2109919449", "name": "Yiren Zhao"}, {"authorId": "47473421", "name": "Ilia Shumailov"}, {"authorId": "2768514", "name": "R. Mullins"}], "references": [{"paperId": "86c8d930b492a4f9cadc6c60aecdaaded49acc86", "title": "Neural Architecture Search on Efficient Transformers and Beyond"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "255c526f78bbfec35d4ed73b6dd8eacd9ef2c0b3", "title": "RankNAS: Efficient Neural Architecture Search by Pairwise Ranking"}, {"paperId": "ffcd58f453f207d48075627da011f62782334c8f", "title": "Go Wider Instead of Deeper"}, {"paperId": "67040b931c1a384426c44ae73f9553e97f08cf6a", "title": "PVT v2: Improved baselines with Pyramid Vision Transformer"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "f156ecbbb9243522275490d698c6825f4d2e01af", "title": "Explainable AI: A Review of Machine Learning Interpretability Methods"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "ffb0bbe26f1cbc0ab22ef34784248f2dcd3a5e5c", "title": "Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"}, {"paperId": "e3794413679237f7a9a2f7e03eb7ea2ccac0ae93", "title": "Synthesizer: Rethinking Self-Attention for Transformer Models"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "ac6b446d545ea995295c14a3e7e5ff7d57955677", "title": "Compute Solution for Tesla's Full Self-Driving Computer"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "59c7f27be8b09596714384f4a35face7cb74ad11", "title": "NAT: Neural Architecture Transformer for Accurate and Compact Architectures"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "d7701e78e0bfc92b03a89582e80cfb751ac03f26", "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3070a1bd503c3767def898bbd50c7eea2bbf29c9", "title": "Wider or Deeper: Revisiting the ResNet Model for Visual Recognition"}, {"paperId": "1c4e9156ca07705531e45960b7a919dc473abb51", "title": "Wide Residual Networks"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "e01eae8dea6fbaa1ae7fc83535053932268df430", "title": "The ACL anthology network corpus"}, {"paperId": "2f47a4c37c01d3ad4e6c4b074ff61468f1e976b8", "title": "Attention-based Interpretability with Concept Transformers"}, {"paperId": "d312615628e265a4ca7e5bb757a4f9a6a6f51db8", "title": "Deeper vs Wider: A Revisit of Transformer Configuration"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "The attention weights across all 48 heads with predicted classification for the single layer IMDb token level text classification Transformer model on two unseen and similar examples"}]}