{"paperId": "90619f1d57d19998ff97656410c78a254fda6e1f", "title": "Linguistic Collapse: Neural Collapse in (Large) Language Models", "abstract": "Neural collapse ($\\mathcal{NC}$) is a phenomenon observed in classification tasks where top-layer representations collapse into their class means, which become equinorm, equiangular and aligned with the classifiers. These behaviors -- associated with generalization and robustness -- would manifest under specific conditions: models are trained towards zero loss, with noise-free labels belonging to balanced classes, which do not outnumber the model's hidden dimension. Recent studies have explored $\\mathcal{NC}$ in the absence of one or more of these conditions to extend and capitalize on the associated benefits of ideal geometries. Language modeling presents a curious frontier, as \\textit{training by token prediction} constitutes a classification task where none of the conditions exist: the vocabulary is imbalanced and exceeds the embedding dimension; different tokens might correspond to similar contextual embeddings; and large language models (LLMs) in particular are typically only trained for a few epochs. This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\\mathcal{NC}$. We find that $\\mathcal{NC}$ properties that develop with scaling are linked to generalization. Moreover, there is evidence of some relationship between $\\mathcal{NC}$ and generalization independent of scale. Our work therefore underscores the generality of $\\mathcal{NC}$ as it extends to the novel and more challenging setting of language modeling. Downstream, we seek to inspire further research on the phenomenon to deepen our understanding of LLMs -- and neural networks at large -- and improve existing architectures based on $\\mathcal{NC}$-related properties.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper empirically investigates the impact of scaling the architectures and training of causal language models (CLMs) on their progression towards $\\mathcal{NC}$ and finds that properties that develop with scaling are linked to generalization."}, "embedding": {"model": "specter_v2", "vector": [0.04881221801042557, 0.7440584897994995, -0.3956819772720337, 0.16854114830493927, -0.3718634843826294, -0.0774281844496727, 0.6360995173454285, -0.19895006716251373, -0.5292074680328369, -0.23591914772987366, 0.5130031704902649, -0.09037002176046371, 0.14882074296474457, 0.24407805502414703, -0.15120911598205566, -0.02668851613998413, -0.9580246806144714, 0.16019374132156372, -0.3299901485443115, -0.20296846330165863, -0.26662683486938477, -0.5568429231643677, -1.0052728652954102, 0.1283658742904663, 0.24649019539356232, 0.7171385884284973, -0.3565453290939331, 0.9297935366630554, -0.27798303961753845, 0.5355685949325562, 0.49928659200668335, -0.5861504077911377, 0.6385048031806946, 0.0025890502147376537, 0.06390031427145004, -0.27929338812828064, 0.8993250131607056, -0.5663928389549255, -0.8058962821960449, 1.0346391201019287, -0.45106247067451477, 0.48294559121131897, 0.5255059599876404, -0.8395206332206726, -0.5058871507644653, 0.6530193090438843, 0.7013042569160461, 0.7124584913253784, -0.37561753392219543, -0.5331931114196777, 1.2830438613891602, -1.1647511720657349, 0.19764816761016846, 1.7476799488067627, 0.7865304946899414, 0.4596199095249176, -0.5130569338798523, -0.9490402340888977, 0.7717238664627075, 0.07975591719150543, -1.0622426271438599, -0.442239910364151, -0.010944953188300133, -0.20055845379829407, 1.3691771030426025, -0.6555503010749817, -0.548572838306427, 0.5372991561889648, -0.03870033100247383, 1.0780808925628662, 0.2509673535823822, -1.0628620386123657, 0.19927071034908295, 0.3065769374370575, 0.3015134036540985, 0.6014258861541748, 0.15042486786842346, 0.6392810940742493, -1.053676724433899, -0.45062077045440674, 0.03798806667327881, -0.16020484268665314, 0.14543503522872925, -0.14675195515155792, 0.27589818835258484, 0.8288161158561707, 0.5872518420219421, 0.4856632649898529, -0.09054186940193176, 1.1439359188079834, 0.5708305239677429, 0.5905246734619141, 0.35185617208480835, 0.2752121686935425, -0.20274850726127625, 0.5619465708732605, -0.9015581011772156, 0.2267766296863556, 0.12304367870092392, 0.8121087551116943, -0.1239718645811081, 0.4164793789386749, 0.03896573930978775, 0.10459796339273453, 0.9444370865821838, -0.17210406064987183, 0.8765766620635986, -0.695007860660553, 0.4535139501094818, -0.4801003634929657, -0.00891019031405449, -0.8487628102302551, -0.46769899129867554, -0.3734198808670044, -1.1581507921218872, -0.9193271994590759, -0.5180698037147522, 0.49367189407348633, -0.48298996686935425, 1.1989986896514893, -0.7276850342750549, 0.11396756768226624, 0.04023442044854164, 0.4458983242511749, -0.031102251261472702, 0.32198384404182434, 0.4780462384223938, 0.03160388022661209, 0.6169533729553223, -0.30584222078323364, -0.4924299716949463, -0.9379374980926514, 0.6237437129020691, -0.15462122857570648, 0.3986259698867798, -0.19678309559822083, -1.1372942924499512, -1.0939024686813354, -1.2084137201309204, 0.2797830402851105, -0.9396826028823853, 0.10136988759040833, 0.7999968528747559, 0.5168352723121643, -0.8046059012413025, 0.9646308422088623, -0.36721694469451904, -0.18524573743343353, 0.6882410645484924, 0.268082857131958, 0.028126221150159836, -0.6220755577087402, -0.8609556555747986, 0.7223420143127441, 0.4437997043132782, -0.5046668648719788, -0.00627392902970314, -0.23352693021297455, -0.9823164343833923, -0.1399601399898529, 0.2192852795124054, -0.6162441968917847, 0.8426597118377686, -0.0876997634768486, -0.5847885012626648, 0.4191320240497589, -0.15629316866397858, 0.06591102480888367, 0.43082940578460693, -0.2569468915462494, -0.4927794933319092, -0.37205228209495544, -0.08976102620363235, 0.4825071692466736, 0.3179149627685547, -0.818848192691803, 0.1567094773054123, -0.06051888316869736, -0.7335677742958069, -0.3795106112957001, -0.39178308844566345, 0.18689143657684326, 0.2542742192745209, -0.2798137664794922, 0.36407074332237244, 0.6929746270179749, 0.07579054683446884, -0.3402744233608246, -0.4440777003765106, -1.2708419561386108, 0.14677469432353973, 0.1257021129131317, 1.023537278175354, -0.8187305331230164, -0.8875177502632141, -0.36576905846595764, 0.20170271396636963, -0.2983338236808777, -0.6672857999801636, 0.6471389532089233, -0.37251076102256775, 1.1105976104736328, -0.2584521174430847, -1.5127301216125488, 0.23052480816841125, -0.11921737343072891, -1.0259438753128052, 0.16684846580028534, -0.0002273581485496834, 0.9179714322090149, -0.5868546366691589, 0.16304899752140045, -0.044459208846092224, -0.194878488779068, -1.0087841749191284, 0.9131421446800232, -0.11194881796836853, -0.3342098891735077, 0.3253958821296692, -0.2450747936964035, 0.2867923080921173, -0.03944170102477074, 0.410800576210022, -0.2734730839729309, -0.09826544672250748, 0.2556440234184265, -0.4928683936595917, 1.0586892366409302, -0.2508997917175293, 0.6179502606391907, -0.21695870161056519, -1.0150525569915771, -0.2765425145626068, 0.3008520305156708, -0.4200490117073059, -0.29470062255859375, 0.4503425359725952, 0.40838027000427246, -0.46663421392440796, 0.32349202036857605, 0.8714127540588379, 0.09543115645647049, -0.7726211547851562, 0.17867718636989594, 0.8288395404815674, -0.22267688810825348, 0.3397476375102997, 0.32297763228416443, 0.5292640328407288, 0.17597340047359467, 0.4615642726421356, -0.16632890701293945, -0.04810770973563194, -0.8942105174064636, -0.1167193204164505, 0.7610357999801636, 0.4261162281036377, 0.873930811882019, 0.5355629920959473, -0.831113874912262, -0.7693204283714294, -0.5564103126525879, 0.24604198336601257, 1.4260708093643188, -0.31419137120246887, -0.3322754502296448, -0.5008132457733154, 0.023737946525216103, -0.020585661754012108, 0.32359984517097473, -0.690905749797821, -0.5842393040657043, -0.5873994827270508, -1.1184736490249634, 1.0627880096435547, 0.045154161751270294, 0.49600690603256226, -0.297789603471756, 0.12044661492109299, -0.25406748056411743, 0.45347240567207336, -0.8477555513381958, -0.12590573728084564, 0.28922879695892334, -0.5460688471794128, -0.0842244029045105, 0.2903751730918884, 0.061386607587337494, -0.023005178198218346, -0.6809093952178955, 0.6107564568519592, -0.4654134511947632, -0.3087974190711975, 0.0178508460521698, 0.888264000415802, -0.7666681408882141, -0.9545478820800781, 0.1566004604101181, 0.6599121689796448, -0.1941848248243332, 0.13420937955379486, 0.48444199562072754, -0.003850624430924654, -0.31127387285232544, -0.7973633408546448, 0.020080910995602608, -0.25070223212242126, 0.09892752021551132, 0.2464880794286728, -0.03520283102989197, 0.6225005984306335, -0.8145720958709717, 1.094387173652649, -0.09767074137926102, -0.38808542490005493, 0.37621405720710754, -0.9870909452438354, -0.3497772216796875, 1.013837456703186, -0.6518389582633972, -0.04731309786438942, -1.4024968147277832, 0.7141613960266113, -0.5949255228042603, -0.3144640326499939, 0.20316648483276367, 0.5639206767082214, 0.6067146062850952, 0.3245234489440918, -0.004960392601788044, 0.11671769618988037, -0.4181339144706726, 0.6709657907485962, -0.6852883100509644, 0.6894956231117249, 0.21798110008239746, 0.18822601437568665, 0.35728541016578674, 0.05819365754723549, -0.6141643524169922, -0.5678677558898926, -0.29167959094047546, -0.6735429763793945, 0.0049340566620230675, 0.15366093814373016, -0.549447774887085, -0.7183315753936768, -0.06495561450719833, -0.889163613319397, -0.09189007431268692, -0.029323512688279152, -0.17529088258743286, -0.3046399652957916, -1.3035645484924316, -1.4156241416931152, -0.7071694731712341, 0.01142115518450737, -0.7419689893722534, 0.06460516899824142, 0.11260567605495453, -0.46836429834365845, -0.7421562671661377, -0.11061746627092361, -0.4409360885620117, 1.031275749206543, -0.42632871866226196, 1.3855973482131958, -0.3164411783218384, -0.10368906706571579, -0.5640466809272766, 0.1115732342004776, 0.6448881030082703, -0.4849378764629364, 0.22458289563655853, -1.0883883237838745, -0.09653519839048386, -0.2630399763584137, -0.44261807203292847, 0.2279857099056244, 0.06270138174295425, 0.30469366908073425, -0.3042871952056885, -0.4134988784790039, 0.25393280386924744, 1.6872433423995972, -0.7207653522491455, -0.27167823910713196, -0.06455319374799728, 0.7446835041046143, 0.6288601756095886, -0.8322189450263977, 0.3293520510196686, 0.2454335242509842, 0.12539128959178925, 0.21270610392093658, -0.049568455666303635, -0.48614323139190674, -1.0747652053833008, 0.23888158798217773, 2.2417242527008057, 0.6896331310272217, -0.0971682071685791, -0.9771729111671448, 0.49990490078926086, -1.2118170261383057, -0.6036847829818726, 0.8069536685943604, 1.0285022258758545, 0.03874186426401138, -0.43172159790992737, -0.6631528735160828, -0.3108122944831848, 0.286964476108551, 0.24898655712604523, -0.4084571599960327, -0.4508441984653473, -0.23893623054027557, 0.281955748796463, 0.2724178433418274, 0.5580857992172241, -0.22962994873523712, 0.8252949714660645, 14.950142860412598, 0.8606244921684265, -0.10202197730541229, 0.9787392616271973, 1.0032994747161865, -0.13274124264717102, -0.3864485025405884, 0.10677102208137512, -1.022378921508789, -0.060008008033037186, 1.0992262363433838, 0.16090364754199982, 0.6530342102050781, -0.04099879786372185, -0.08430317789316177, 0.2745025157928467, -0.3710612952709198, 0.5906185507774353, 0.8056383728981018, -1.165276288986206, 0.6484866738319397, 0.2563362717628479, 0.7720312476158142, 0.8408544659614563, 0.6576876044273376, 0.8408602476119995, 0.7170107364654541, -0.6929880380630493, 0.6953283548355103, 0.23490479588508606, 1.0834851264953613, 0.014985539950430393, 0.3832240700721741, 0.7448985576629639, -0.6289172768592834, -0.2794020473957062, -0.9799832105636597, -1.4562921524047852, 0.1577705442905426, -0.03496905788779259, -0.26674261689186096, -0.5913941860198975, -0.32963499426841736, 0.27309295535087585, -0.34089112281799316, 0.0689198225736618, 0.0783863440155983, 0.6118362545967102, -0.29595088958740234, 0.30928006768226624, 0.036837294697761536, 0.659527063369751, 0.26352936029434204, -0.15831685066223145, -0.12378542870283127, -0.36287179589271545, 0.24257934093475342, 0.7704507112503052, -0.5877026319503784, -0.025366082787513733, -0.4675658643245697, -0.09931539744138718, 0.05938251316547394, 0.4202055037021637, 0.5392172932624817, -0.1283709704875946, -0.23527821898460388, 0.1644420623779297, 1.1658515930175781, 0.4970991611480713, 0.12410066276788712, 0.14436767995357513, 0.5085304975509644, -0.5866712927818298, -0.3197237551212311, 0.3936113715171814, -0.46930575370788574, -0.8315340280532837, -0.6197805404663086, 0.08156643807888031, 0.3460894823074341, -0.8742672801017761, -1.3445199728012085, 0.9368416666984558, -0.00046002172166481614, -0.1045805960893631, 0.3452403247356415, -0.9330076575279236, -0.1952420473098755, -0.02630622126162052, -1.2628753185272217, -0.5896100401878357, 0.34605392813682556, -0.2623134255409241, -0.23609572649002075, -0.348615825176239, 1.2124998569488525, 0.049111880362033844, -0.674968957901001, 0.2676779329776764, 0.18816950917243958, -0.09194986522197723, -0.2866950035095215, -0.6514292359352112, 0.8281368017196655, 0.2940607964992523, -0.1562013477087021, 0.5593242645263672, -0.07231927663087845, 0.3504221439361572, -0.22961507737636566, 0.2485148012638092, 0.8999503254890442, -0.8899662494659424, -0.02151964046061039, -0.8295508623123169, -0.745322048664093, 0.23917843401432037, 0.4176344871520996, -0.07458291947841644, 0.36782366037368774, 0.2561543881893158, -1.023701548576355, -0.13891521096229553, -0.30269554257392883, 0.3038877546787262, 0.3360748291015625, -1.2151771783828735, -0.6028860807418823, -0.18822552263736725, 0.1837785392999649, -0.2545052170753479, -0.28219494223594666, 0.03189464658498764, -0.014723156578838825, -0.39683037996292114, 0.6590747237205505, -0.9071872234344482, 0.6325350999832153, 0.7730715274810791, -0.5914011597633362, -0.7537317276000977, -0.1837150603532791, -0.8473762273788452, 0.3700387477874756, 0.3055840730667114, 0.2784334421157837, -0.4432777464389801, 0.17916566133499146, 0.7497099041938782, 0.07664783298969269, -0.15398164093494415, -1.2712130546569824, -0.2856432795524597, 0.7155416011810303, -0.9658697247505188, 0.21949490904808044, 0.04944666102528572, 0.27723097801208496, 0.14512258768081665, 0.43591734766960144, 0.5691884160041809, -0.04366825148463249, -0.9040589332580566, 0.023135537281632423, -0.12028487771749496, 0.2834615111351013, -1.1175650358200073, -0.7263765931129456, -1.3023686408996582, 0.386324405670166, -1.3970115184783936, -0.26053935289382935, -0.8295563459396362, -0.6720023155212402, -0.07661061733961105, -0.36263880133628845, -0.1892368048429489, 0.8104972839355469, 0.04067448154091835, 0.08034974336624146, -0.03445429354906082, -0.3656853139400482, 0.7637892365455627, 0.21981415152549744, -0.48535260558128357, 0.19390510022640228, -0.06240786984562874, 0.10480096191167831, 0.8346385955810547, 0.5274819135665894, -0.5180535912513733, -0.5327754020690918, -1.3083274364471436, 0.6070640087127686, -0.2053377628326416, 0.18196530640125275, -0.643073558807373, 0.8649191856384277, 0.5534003376960754, -0.23774480819702148, 0.14789722859859467, 0.4713648557662964, -0.9745196104049683, -0.6995124220848083, 0.30327653884887695, -0.9955799579620361, 0.5144872665405273, 0.19714562594890594, -0.2912709414958954, -0.27086514234542847, 0.5201613306999207, -0.12559564411640167, -0.9256556034088135, -0.17102789878845215, 0.29834428429603577, -0.6340833902359009, 0.28569725155830383, -0.1990046352148056, -0.04983257129788399, -1.4515548944473267, -0.035361532121896744, -0.22282418608665466, 0.1430404782295227, -0.268277645111084, 1.2909152507781982, 0.15789467096328735, -1.322243332862854, 0.09695707261562347, 0.3340465724468231, 0.13713021576404572, -0.06723406910896301, 0.3999347686767578, 0.04889898747205734, -0.38049250841140747, 0.6198751926422119, 0.4653639495372772, 0.44209229946136475, -0.36413833498954773, 0.13430368900299072, 0.6798406839370728, -0.17212434113025665, 0.06750494986772537, 1.0477575063705444, 0.20610618591308594, -1.3554999828338623, 0.24835258722305298, -1.0760493278503418, 0.003866574726998806, -0.014450572431087494, 0.18036310374736786, 0.6378560662269592, -0.10826753079891205, -0.014450447633862495, -0.3671988546848297, 0.09689082205295563, 0.16423846781253815, -0.37395110726356506, 0.27323219180107117, -0.5577989220619202, -0.2584615647792816, 1.042840600013733, 1.41466224193573, -0.7042684555053711, -0.9015369415283203, -0.8095113039016724, -0.15463775396347046, 0.027957232668995857, 0.21988262236118317, -0.26808395981788635, -0.17265541851520538, 0.7646225690841675, 0.5006850361824036, 0.5602781772613525, -0.33508560061454773, -0.007258557714521885, -0.2571936547756195, 0.21125854551792145, 0.09712954610586166, -0.7957478165626526, -0.5971900224685669, 0.9179328680038452, 1.231618881225586, -0.7100030183792114, 0.47894203662872314, 0.13678625226020813, -0.23217718303203583, 0.7398888468742371, 0.4436914026737213, -0.35448092222213745, 1.0193918943405151, -0.04460147023200989, 0.1588258147239685, 0.026051608845591545, -1.4983773231506348, 0.0125674894079566, 0.7801826596260071, 0.5250726342201233, 0.44655510783195496, 0.554993748664856, 0.07919008284807205, 0.9132193922996521, -0.5651203989982605, -0.2800818085670471, 0.8129677772521973, 0.06207015737891197, -0.030236145481467247, 0.47345292568206787, 0.06287121772766113, 0.4714702069759369, -0.6559670567512512, -0.7677479982376099, -0.027581164613366127, 0.9557175636291504, 0.426275372505188, 0.8181806206703186, 1.2365201711654663, -0.06957094371318817, 0.4705808162689209, 0.43994635343551636, 0.19355343282222748, -0.05898712947964668, -0.006363899912685156, -0.23651990294456482, -0.8196638226509094, 0.48966458439826965, -0.018355030566453934, -0.4173853099346161, -0.20712685585021973, -0.5054891705513, -0.04461819678544998, -0.4706366956233978, 0.2897989749908447, 1.1052130460739136, 0.45552632212638855, 0.45783790946006775, -0.4239636957645416, -0.407001256942749, -0.8033403754234314, -0.8853991627693176, 0.052093468606472015, 0.04771461337804794, 0.09246840327978134, -0.2026665359735489, -0.5904873609542847, -0.2549794316291809]}, "authors": [{"authorId": "2303612154", "name": "Robert Wu"}, {"authorId": "2734935", "name": "V. Papyan"}], "references": [{"paperId": "5340bb0f855607422783cfeac77449da16304059", "title": "Neural collapse inspired semi-supervised learning with fixed classifier"}, {"paperId": "46d9b358d015b2f26982b92e20e92f65f7f70995", "title": "Learning optimal inter-class margin adaptively for few-shot class-incremental learning via neural collapse-based meta-learning"}, {"paperId": "3b01285a2babda0ce3f8f9b2933f0c1a7dfa7368", "title": "Unifying Low Dimensional Observations in Deep Learning Through the Deep Linear Unconstrained Feature Model"}, {"paperId": "1b949d08426cb4b20955f0e0fad36de556f88fec", "title": "Average gradient outer product as a mechanism for deep neural collapse"}, {"paperId": "0356e74584459d5fbfeee35da6d9c747241fc779", "title": "Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias"}, {"paperId": "d4d9fa21685c64d12c81b1c5fc0d0382ef427a67", "title": "EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector"}, {"paperId": "8469221ec0fd952c7c5cc1b6613a6023862ab0a7", "title": "Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination"}, {"paperId": "cfbd9f8fa472dd7b395c64fe6124402d736182f8", "title": "Towards Demystifying the Generalization Behaviors When Neural Collapse Emerges"}, {"paperId": "732329e1af9b1822a3d0ea9fea473e9e22dd44fa", "title": "Generalized Neural Collapse for a Large Number of Classes"}, {"paperId": "23d3910e0d36837db00e98c6075294a166a7642c", "title": "How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability"}, {"paperId": "970f6631362dec77cde3c19dc4726eb233c0e09c", "title": "On the Role of Neural Collapse in Meta Learning Models for Few-shot Learning"}, {"paperId": "14b31861313eb5e9e2d5020a8d920c8d1aaca857", "title": "Neural Collapse for Unconstrained Feature Model under Cross-entropy Loss with Imbalanced Data"}, {"paperId": "e114e832d81fa5511c0c5e2f620ba1306ff0dac9", "title": "Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks"}, {"paperId": "49f058e506c63d7d643c4608ad79e780871d639d", "title": "Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants"}, {"paperId": "ebe7f05b655f1ea4d550e20abea4f9c0e3c7b35a", "title": "Hierarchical Task-Incremental Learning with Feature-Space Initialization Inspired by Neural Collapse"}, {"paperId": "278a5e27f70347c78d5ff6131a276883a9e5a42e", "title": "A Neural Collapse Perspective on Feature Evolution in Graph Neural Networks"}, {"paperId": "28af04b2ae30b60e023be52a60e548acbe567fc9", "title": "Understanding Prompt Tuning for V-L Models Through the Lens of Neural Collapse"}, {"paperId": "f574f4e4f94c0268014d8c10dd85d5df34a46561", "title": "The Tunnel Effect: Building Data Representations in Deep Neural Networks"}, {"paperId": "4c3a4956cb2f088359c50ed38b0102f6742d449b", "title": "Neural (Tangent Kernel) Collapse"}, {"paperId": "e88e7bf8f1755933244be53a16772cbced205fde", "title": "Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model"}, {"paperId": "ccbba99b3b98ece8086858af11d665fab3b449af", "title": "A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization"}, {"paperId": "7fbf9db97e2e0526ff451af61592aa722d012a54", "title": "No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier"}, {"paperId": "402fb091fccc9921f938bfa81de0b978896a674e", "title": "Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap"}, {"paperId": "e8f1a4d9262f509bb6c4541cd5030b6c48340e48", "title": "Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity"}, {"paperId": "7f73cc1ef3e73313657132296b8d0fcb6382fb62", "title": "Inducing Neural Collapse in Deep Long-tailed Learning"}, {"paperId": "944019ebfc83037da9c4db8a48f80f8e5387b9dc", "title": "Dynamics in Deep Classifiers Trained with the Square Loss: Normalization, Low Rank, Neural Collapse, and Generalization Bounds"}, {"paperId": "2b093de0ea57fee916e45173e32454052367a67d", "title": "Understanding Imbalanced Semantic Segmentation Through Neural Collapse"}, {"paperId": "cbd043f8c399c90ea4921ddb3c85cab6475c0c6a", "title": "Perturbation Analysis of Neural Collapse"}, {"paperId": "9d125f45b1d2dea01f05281470bc08e12b6c7cba", "title": "Toy Models of Superposition"}, {"paperId": "f0e19341b34fa83ecae5a1646086928d5247c763", "title": "Neural Collapse with Normalized Features: A Geometric Analysis over the Riemannian Manifold"}, {"paperId": "5d6d0bb2df73f3c6da10884481303f165ae8d7c2", "title": "Linking Neural Collapse and L2 Normalization with Improved Out-of-Distribution Detection in Deep Neural Networks"}, {"paperId": "095a24fd666bd6b1b36566b33e30219132c6f2b6", "title": "Imbalance Trouble: Revisiting Neural-Collapse Geometry"}, {"paperId": "52fa0b5e6f9d7ab1981fe79365d20fe04fd2a9da", "title": "Feature selection with gradient descent on two-layer networks in low-rotation regimes"}, {"paperId": "f4723ed9bf2be70e63865bc97f3b903e2762c269", "title": "Neural Collapse: A Review on Modelling Principles and Generalization"}, {"paperId": "eb7d596fb4ab48c2db1b0ee6d8127df7a5b5ebeb", "title": "The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization"}, {"paperId": "e41d045e4809fc04452b7c9dd81a5ae76b7df21a", "title": "Neural Collapse in Deep Homogeneous Classifiers and The Role of Weight Decay"}, {"paperId": "4ee5303372ad4ecec39bf9107ee54967e7e2ea25", "title": "Inducing Neural Collapse in Imbalanced Learning: Do We Really Need a Learnable Classifier at the End of Deep Neural Network?"}, {"paperId": "b96cb62450cb77926c712d5cd50d1764101de605", "title": "On the Implicit Bias Towards Minimal Depth of Deep Neural Networks"}, {"paperId": "9268cec27bbbfdcf497595319b6a61eea027cabf", "title": "Limitations of Neural Collapse for Understanding Generalization in Deep Learning"}, {"paperId": "96fcc167db6d974febac29ebada14c6559c2d6e9", "title": "Extended Unconstrained Features Model for Exploring Deep Neural Collapse"}, {"paperId": "637cd4e8b05b4f2e41164d8a5d00594f4b0b21e1", "title": "NeuroMixGDP: A Neural Collapse-Inspired Random Mixup for Private Data Release"}, {"paperId": "e006d7e65070195bd8ba368ca9a31a3540047665", "title": "Nearest Class-Center Simplification through Intermediate Layers"}, {"paperId": "8f0d609618838b20631469ffa7fc78928bba8ca0", "title": "On the Role of Neural Collapse in Transfer Learning"}, {"paperId": "a7ff4618c8de5227c21871266ce660649e53e267", "title": "Probing neural networks with t-SNE, class-specific projections and a guided tour"}, {"paperId": "79eff96fd96b688ff5368f7edb506598bc53d354", "title": "Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path"}, {"paperId": "24c8adb79ad4ee7070fc0d48807b7f6a42148a74", "title": "A Geometric Analysis of Neural Collapse with Unconstrained Features"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "5d06f68124c258c5c70c18a029a8bbdb1efec897", "title": "Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training"}, {"paperId": "42d8306324126d023a4d7683e8ac0aaacf682ddb", "title": "Explicit regularization and implicit bias in deep network classifiers trained with the square loss"}, {"paperId": "84fe64fe5b58235574e2ae0bf3210295e8e83c9c", "title": "Separation and Concentration in Deep Networks"}, {"paperId": "1f9017a14942e771e45c196abdf69df40a452f68", "title": "Neural Collapse with Cross-Entropy Loss"}, {"paperId": "66293949b23f4da394d93d90ce60f3c383905fd8", "title": "On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers"}, {"paperId": "f831865ef22a04273b18ba8429936b1e12cb82ae", "title": "Neural collapse with unconstrained features"}, {"paperId": "353f0e3ce1597486509950b6ddbd1ca81a8fe9d9", "title": "Traces of Class/Cross-Class Structure Pervade Deep Learning Spectra"}, {"paperId": "806e27fb9d8781c3c6e918734453418ebbeae96c", "title": "Prevalence of neural collapse during the terminal phase of deep learning training"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "6aa86110a83febcb6064348644470d90678aa7cd", "title": "Revealing the Structure of Deep Neural Networks via Convex Duality"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "9d7902e834d5d1d35179962c7a5b9d16623b0d39", "title": "How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings"}, {"paperId": "096803265d747a37b8b032b8e71ef2e42f4bcd41", "title": "Bfloat16 Processing for Neural Networks"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "ad6547ae941130b2436e357cf868ff716c587159", "title": "An Introduction to Finite Tight Frames"}, {"paperId": "ef9ddbc35676ce8ffc2a8067044473727839dbac", "title": "Breaking the Softmax Bottleneck: A High-Rank RNN Language Model"}, {"paperId": "6e5c730c356c4d74cab8687a79ec95ada19783ba", "title": "The strange geometry of skip-gram with negative sampling"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "1ab5c006caf3bf8c128fdfad80e58277cb8b1455", "title": "Learning with Noisy Labels"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "bf55afc2fb9db97d1fb6344a917cf5ead157c171", "title": "Just relax: convex programming methods for identifying sparse signals in noise"}, {"paperId": "ab3591ebf1e52134cb2106f1a6f0bfd3b17e7c77", "title": "GRASSMANNIAN FRAMES WITH APPLICATIONS TO CODING AND COMMUNICATION"}, {"paperId": "9c285c38feb7cc83fa235c7e9456535db283acdc", "title": "The Utilization of Multiple Measurements in Problems of Biological Classification"}, {"paperId": "ab21376e43ac90a4eafd14f0f02a0c87502b6bbf", "title": "THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS"}, {"paperId": "ef5dd59b4e1effa613e9c5b686f9e08970a0ba21", "title": "Memorization-Dilation: Modeling Neural Collapse Under Noise"}, {"paperId": "a98c3e1dcf8de4aa2f7e5b0ba11c709be7481ad3", "title": "A Study of Neural Collapse for Text Classification"}, {"paperId": "92b8437f0fe8524b7904b0cf51014d2a3e8adc77", "title": "Feature learning in deep classifiers through Intermediate Neural Collapse"}, {"paperId": "bb02b6ebcd32197389e6487679ae9fdcbff5423a", "title": "Linear Convergence Analysis of Neural Collapse with Unconstrained Features"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "6d12a1d23b21a9b170118a56386552bc5d4727de", "title": "A Mathematical Theory of Communication"}, {"paperId": "7236864d8c2f62defea559465462c43a4b4b6b47", "title": "Stable recovery of sparse overcomplete representations in the presence of noise"}, {"paperId": "b54bcfca3fddc26b8889739a247a25e445818149", "title": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition"}, {"paperId": "2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac", "title": "Human behavior and the principle of least effort"}, {"paperId": null, "title": "NC 1 ) Within-class variability is reduced across model scale (more so by width than depth) and training (up to 6 epochs), and is tightly correlated with validation performance"}, {"paperId": null, "title": "\u00ac C2) Imbalanced classes: The distribution of tokens in natural language is typically very imbalanced [8, 9], as is the case in TinyStories [2]"}, {"paperId": null, "title": "High-dimensional sgd"}, {"paperId": null, "title": "Demystifying batch normalization in relu"}, {"paperId": null, "title": "A law of data separation"}, {"paperId": null, "title": "Improved generalization bounds for transfer learning via neural collapse"}, {"paperId": null, "title": "C3) Ambiguous contexts: There may exist very similar or even identical contexts followed by different tokens in the natural language data [10]."}, {"paperId": null, "title": "Progressive feedforward collapse of resnet"}, {"paperId": null, "title": "Neural collapse meets differential privacy: Curious behaviors of noisySGD with near-perfect representation"}]}