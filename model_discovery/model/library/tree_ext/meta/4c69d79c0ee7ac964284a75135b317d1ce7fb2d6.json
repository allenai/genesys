{"paperId": "4c69d79c0ee7ac964284a75135b317d1ce7fb2d6", "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference", "abstract": "Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs. This paper introduces\"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as\"key\"tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.", "venue": "Conference on Machine Learning and Systems", "year": 2024, "citationCount": 12, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper introduces\"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization and evaluates Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms."}, "embedding": {"model": "specter_v2", "vector": [0.11421062797307968, 0.552337110042572, -0.05429301783442497, 0.07857406139373779, -0.8786725401878357, -0.5160114169120789, 0.912256121635437, -0.047248464077711105, -0.25314173102378845, -0.3448215126991272, 0.8077644109725952, -0.01642598770558834, 0.22769634425640106, -0.02905421517789364, -0.1924249678850174, -0.25450360774993896, -0.7134605646133423, 0.4560573399066925, -0.32238081097602844, 0.07204557210206985, 0.07395414263010025, -0.7437843084335327, -0.875700056552887, 0.10574611276388168, 0.33468353748321533, 0.34562763571739197, 0.34707507491111755, 1.1579848527908325, -0.4239823520183563, 0.049105867743492126, 0.37479275465011597, -0.4299740791320801, -0.2532137930393219, -0.07963742315769196, -0.28578853607177734, -0.3720022737979889, 0.17675915360450745, -0.4481816589832306, -0.38143882155418396, 0.7170562148094177, 0.09131523221731186, 0.07913301140069962, 0.49511051177978516, -0.7529147267341614, -0.39742639660835266, 1.2517430782318115, 0.846881628036499, 0.7312872409820557, -0.1865541785955429, -0.5183912515640259, 1.9055383205413818, -1.9124751091003418, 0.325499027967453, 1.5222662687301636, 0.4263172149658203, 0.17249105870723724, -0.5376195311546326, -0.29922184348106384, 1.0968068838119507, 0.11249051988124847, -1.0332095623016357, -0.5847027897834778, 0.2539685070514679, 0.10231497883796692, 2.2522389888763428, 0.0036786836571991444, 0.6630786061286926, 0.5084088444709778, -0.24727711081504822, 1.8530265092849731, -0.5754842162132263, -0.6482299566268921, -0.15193362534046173, -0.13377143442630768, 0.2904314398765564, 0.7078498005867004, -0.29544273018836975, -0.02775629609823227, -0.9537496566772461, -0.4402737617492676, 0.5485112071037292, -0.5557959675788879, 0.12058510631322861, 0.20953774452209473, -0.4617854058742523, 0.9617019295692444, 0.007466141600161791, 0.6273849606513977, -0.2563549876213074, 0.967880368232727, 0.36272573471069336, -0.22925375401973724, 0.46866902709007263, 0.3132055997848511, -0.02621740661561489, -0.06033816933631897, -1.1355571746826172, 0.6513839364051819, 0.25255516171455383, 0.4931209683418274, -0.3278086483478546, 0.6005876660346985, -1.0634852647781372, 0.1346910297870636, 1.506603479385376, 0.481082558631897, 0.6454204320907593, -0.6571800112724304, 0.3055954873561859, -0.8408567309379578, 0.08502215147018433, -0.8511178493499756, 0.004726047161966562, -0.253772109746933, -0.34994766116142273, -1.7758303880691528, -0.5350286960601807, 0.10543970763683319, -0.6800971031188965, 0.7351382970809937, 0.005445186980068684, 0.38141945004463196, 0.18351516127586365, 0.16820020973682404, 0.7665234804153442, 0.9037219882011414, 0.06749439239501953, 0.20010659098625183, 0.7195504307746887, -0.8184889554977417, -0.528906524181366, -1.4062964916229248, 0.8028584122657776, -0.1869029402732849, 0.18799006938934326, -0.2766202986240387, -1.4836502075195312, -0.8059152960777283, -0.5524979829788208, -0.13621032238006592, -0.5345878601074219, 0.6345106363296509, 1.0619152784347534, 0.3537987172603607, -1.1602786779403687, 0.3771095275878906, -0.3705568313598633, 0.13225962221622467, 0.1574021279811859, 0.2000206708908081, 0.46497246623039246, -0.2911140024662018, -1.3827276229858398, 0.05460333079099655, 0.09624525159597397, -0.22426775097846985, -0.03758171573281288, -0.7105592489242554, -1.206925630569458, 0.2716476023197174, 0.09281639009714127, -0.5548754334449768, 1.3269139528274536, 0.3989659547805786, -1.224036693572998, 0.14985813200473785, -0.57265305519104, 0.40000465512275696, -0.1267225295305252, -0.3087436258792877, -0.11974464356899261, -0.49327945709228516, 0.006576878484338522, 0.5874543190002441, 0.6524636149406433, 0.03418297320604324, -0.3378726840019226, 0.22228577733039856, -0.5028879046440125, 0.13934668898582458, -0.18817280232906342, 0.7226178050041199, -0.5369448065757751, -0.08858422935009003, 0.4458087682723999, 0.5461108684539795, -0.01027890294790268, -0.6285015344619751, -0.5040003061294556, -1.3496341705322266, 0.5486763119697571, -0.24764792621135712, 1.1168208122253418, -0.6503798961639404, -0.605684220790863, -0.062117625027894974, 0.2001635581254959, -0.11570341140031815, -0.490443617105484, 0.8575246930122375, -0.2723774313926697, 0.21232002973556519, 0.32157033681869507, -1.064577579498291, 0.5366033911705017, -0.2862187623977661, -0.7382548451423645, -0.5490955710411072, 0.3433091342449188, 1.4379984140396118, -0.9951601624488831, -0.17033110558986664, 0.08096350729465485, 0.457712858915329, -0.859056830406189, 1.0837681293487549, -0.27070000767707825, -0.20331613719463348, -0.5563897490501404, -0.3671165406703949, -0.14479504525661469, -0.3135852813720703, 0.6333510279655457, -0.595723569393158, -0.08180062472820282, 0.7169219851493835, -0.6386293768882751, 1.1046746969223022, 0.05057571828365326, 0.27758580446243286, -0.4640933573246002, -0.5464330911636353, 0.2007593810558319, 0.4530392289161682, -0.5317843556404114, -0.2534739077091217, 0.01954171247780323, 0.6852684617042542, -0.6130737066268921, 0.4956858158111572, 1.032849907875061, 0.809593915939331, -0.6550686359405518, 0.23683643341064453, 0.5397167205810547, -0.33068424463272095, 0.8030300140380859, 0.874789297580719, 0.7015672326087952, 0.37636223435401917, 0.280159056186676, 0.08711500465869904, 0.13227316737174988, -0.6853237748146057, -0.207810178399086, 0.40771880745887756, 1.0759029388427734, 0.8130026459693909, 0.5388879776000977, -0.5597200393676758, -0.3739662766456604, 0.09500767290592194, 0.9862783551216125, 1.1588943004608154, -0.4875721335411072, -0.6244590282440186, -0.8201894164085388, -0.08529011160135269, -0.18422308564186096, 0.32845577597618103, -0.2761456370353699, -0.25999677181243896, -0.6115949749946594, -1.0617042779922485, 0.9362305402755737, 0.6364954710006714, 0.4121679663658142, -0.5806615352630615, 0.001798810437321663, 0.004897831939160824, -0.20073585212230682, -0.7280191779136658, -0.4990237355232239, 0.18675586581230164, -0.3837907016277313, 0.13645343482494354, 0.16975721716880798, 0.07903803884983063, 0.25486958026885986, -0.5980648398399353, 0.9733429551124573, -0.5399720668792725, -0.180358424782753, 0.28651729226112366, 0.6244059801101685, -0.3790019452571869, -0.5511534810066223, 0.17834463715553284, 0.1532054841518402, -0.3823421001434326, 0.28864648938179016, 0.5437633395195007, -0.055213429033756256, -0.5001298785209656, 0.09069833159446716, 0.7141184210777283, 0.017251623794436455, 0.09757576137781143, 0.600159227848053, -0.4303150177001953, -0.2123890221118927, -0.9799948334693909, 0.9780561327934265, 0.2302461564540863, -0.31297650933265686, 0.30859991908073425, -0.6453056931495667, -0.4121999442577362, 0.4649481773376465, -0.5785324573516846, -0.35227474570274353, -1.0328084230422974, 0.36498647928237915, 0.04364801198244095, -0.005513160023838282, 0.44149041175842285, 0.029314661398530006, 0.5709916353225708, -0.41063135862350464, 0.8849800229072571, -0.013826943002641201, -0.1837310642004013, 0.9733419418334961, -0.9167966842651367, 0.5847456455230713, -0.06774531304836273, 0.23686818778514862, -0.5024564266204834, -0.4423752725124359, -0.6447194218635559, -0.17524783313274384, -0.2862393260002136, -0.13833899796009064, -0.4791151285171509, 0.09589552134275436, -0.47586286067962646, -0.7321078181266785, 0.08594996482133865, -1.3607597351074219, -0.07127470523118973, 0.07458953559398651, -0.3029397428035736, 0.21986991167068481, -0.9994625449180603, -1.2299586534500122, -0.5645211935043335, -1.0483969449996948, -0.9020552635192871, 0.702604353427887, -0.04196412116289139, -0.5708170533180237, -0.886693000793457, 0.37174472212791443, -0.2500767111778259, 0.8389821648597717, -0.6212992668151855, 0.997622549533844, -0.0012780710821971297, -0.27479955554008484, -0.38106831908226013, 0.1647820621728897, 0.1813465654850006, -0.5423217415809631, 0.3575582802295685, -0.6895099878311157, 0.16024085879325867, -0.7325027585029602, 0.09755533933639526, -0.10904378443956375, 0.5938617587089539, 0.08429693430662155, -0.17673802375793457, -0.7581400871276855, 0.20162563025951385, 1.311805248260498, -0.7649911642074585, -0.164593905210495, -0.10676974803209305, 1.384744644165039, 0.0410289540886879, -0.19989390671253204, 0.6501128077507019, 0.6359297037124634, 0.5455963611602783, -0.025802431628108025, -0.2686896026134491, -0.15182141959667206, -0.750329852104187, 0.8198763132095337, 2.12823748588562, 0.2766609191894531, -0.28816157579421997, -0.9172322750091553, 0.7903938293457031, -1.633400559425354, -0.5538238883018494, 0.4133778512477875, 0.5079948902130127, 0.29429200291633606, -0.7113946080207825, -0.3619410991668701, -0.13965162634849548, 0.2846750319004059, 0.3513743281364441, -0.32089945673942566, -0.4944612681865692, 0.2858165204524994, 0.3497740924358368, -0.08070692420005798, 0.6513803005218506, -0.10938847064971924, 0.9637810587882996, 14.472692489624023, 1.0191593170166016, 0.13537998497486115, 0.4580134153366089, 0.6695442795753479, 0.02556031383574009, -0.6921318769454956, -0.10922554135322571, -1.7315744161605835, -0.17619094252586365, 1.4042612314224243, -0.22780759632587433, 0.23179064691066742, 0.18892084062099457, 0.39977023005485535, 0.20768198370933533, -0.5773515701293945, 0.7193350195884705, 0.6772752404212952, -1.4792616367340088, 0.7007966041564941, 0.28229984641075134, 0.05340659245848656, 0.5142552852630615, 0.676526665687561, 0.7588446140289307, 0.613984227180481, -0.42084449529647827, 0.8552070260047913, 0.11547643691301346, 0.654067873954773, -0.3256278336048126, 0.36455100774765015, 0.30437031388282776, -1.2089418172836304, -0.010892287828028202, -0.7550637722015381, -1.0337847471237183, 0.39752036333084106, 0.2989368736743927, -0.8472204804420471, -0.5209503173828125, -0.48449304699897766, 0.7198069095611572, -0.16441762447357178, 0.1867837756872177, -0.28802669048309326, 1.003749966621399, -0.12096153944730759, -0.12906068563461304, 0.44774991273880005, 0.29516172409057617, 0.15514406561851501, 0.12673242390155792, 0.5650361180305481, -0.05769902467727661, 0.15953558683395386, 0.7191627621650696, -0.0808611512184143, -0.07523933798074722, -0.2726914584636688, -0.45273396372795105, 0.2991443872451782, 1.1130236387252808, 0.19557581841945648, 0.10444038361310959, -0.37237852811813354, 0.3474132716655731, 0.834178626537323, -0.014512430876493454, -0.14675098657608032, 0.36843687295913696, 0.32310009002685547, -0.5384289622306824, 0.4355957806110382, 0.4489307701587677, 0.16913393139839172, -0.5028551816940308, -0.8939444422721863, -0.2034570425748825, 0.08531038463115692, -0.832169771194458, -0.6026098728179932, 0.5906953811645508, 0.10049179196357727, 0.06289473921060562, -0.5272528529167175, -0.6335292458534241, -0.4998623728752136, 0.4347243905067444, -1.2320383787155151, -1.2364332675933838, 0.6249954104423523, -0.26821741461753845, -0.4271012544631958, 0.2535052001476288, 1.6349821090698242, -0.16665656864643097, -0.596372663974762, 0.12071430683135986, 0.3042142987251282, 0.07965824753046036, -0.7490684390068054, -0.9941555857658386, 1.3884552717208862, 0.5847218632698059, 0.4277561902999878, 0.4068121314048767, 0.07771313935518265, -0.061687104403972626, -1.22367525100708, -0.06226002052426338, 1.1583245992660522, -1.0324360132217407, -0.3931686580181122, -1.0564463138580322, -0.5667447447776794, 0.12651269137859344, 0.2670031487941742, -0.3735467493534088, 0.43125253915786743, 0.3199506103992462, -0.3458492159843445, -0.04831475391983986, -0.42326003313064575, -0.020028172060847282, 0.20612378418445587, -0.5539983510971069, 0.03693639487028122, 0.03302660956978798, 0.46490955352783203, -1.029807448387146, -0.48653939366340637, -0.954160213470459, 0.20915605127811432, 0.42210668325424194, 1.1361626386642456, 0.018425121903419495, 0.3521468937397003, 0.825459897518158, -0.07456780970096588, -1.0454944372177124, -0.4135924279689789, -0.8690881729125977, -0.2338312715291977, 0.40093180537223816, 0.9447960257530212, -0.20354509353637695, 0.4317153990268707, 1.1775541305541992, 0.2805473506450653, -0.29406797885894775, -0.584918737411499, 0.011690289713442326, -0.028979191556572914, -0.7552353143692017, 0.3703401982784271, -0.2955523729324341, 0.05621545389294624, 0.4891316294670105, 0.4772707521915436, 0.5802825093269348, -0.1493029147386551, -0.575376033782959, 0.4231839179992676, 0.06945427507162094, -0.03879515826702118, -0.3625112473964691, -0.4507896304130554, -1.2018702030181885, -0.08451342582702637, -1.1919015645980835, 0.07664885371923447, -1.3915109634399414, -0.05895696580410004, 0.1530085951089859, -0.086577869951725, 0.2680642604827881, 0.102302685379982, -0.09693355858325958, -0.5277304649353027, -0.7610141038894653, -0.6130875945091248, 0.7140127420425415, 0.6875768303871155, -0.6734722852706909, 0.15592247247695923, -0.3222011923789978, -0.11156917363405228, 0.04605405777692795, 0.1736980676651001, -0.40343177318573, -0.9654319286346436, -1.4693766832351685, 0.8450285196304321, -0.12613554298877716, 0.040890321135520935, -0.35923460125923157, 0.781655490398407, 0.6203097701072693, -0.06851910799741745, -0.2603223919868469, 0.48806703090667725, -0.1834358274936676, -0.165577694773674, 0.05296815559267998, -0.5518295764923096, 0.33605343103408813, 0.12818029522895813, -0.6649405360221863, -0.05533812567591667, 0.694587230682373, -0.7539128065109253, -0.9869720935821533, -0.7057317495346069, 0.31169837713241577, -0.635303258895874, 0.0892060250043869, -0.7769814133644104, -0.3466298580169678, -0.9691694974899292, -0.5301331281661987, 0.23223011195659637, -0.06270342320203781, -0.6593746542930603, 0.7423511147499084, 0.982696533203125, -1.1986124515533447, -0.20791174471378326, 0.41578206419944763, -0.1757819950580597, -0.01110351923853159, 0.2545928359031677, 0.3398438096046448, 0.07612340897321701, 0.6899049878120422, 0.6838325262069702, 0.31071746349334717, -1.1978790760040283, -0.503402829170227, 0.2635173499584198, -0.8081910014152527, -0.5142125487327576, 1.336594820022583, -0.48260298371315, -0.9904002547264099, -0.36673691868782043, -1.5145725011825562, -0.5615496039390564, -0.3172743320465088, 0.7022323608398438, 0.08455866575241089, -0.032232508063316345, -0.2076955884695053, -0.5021851062774658, -0.03068791702389717, -0.22504442930221558, -0.809677004814148, 0.750705361366272, -0.13682715594768524, -0.46124160289764404, 0.5299534797668457, 0.7033500075340271, -0.8916760683059692, -0.4528752267360687, -0.4875718057155609, -0.4588015079498291, -0.12959341704845428, 0.8794369697570801, -0.2922385036945343, -0.4565676152706146, 0.7570744156837463, 0.21549531817436218, 0.3147297501564026, -0.3241713047027588, -0.15715602040290833, 0.742255687713623, 0.5267564058303833, 0.11535920202732086, -0.6593614816665649, -0.6073443293571472, 1.3197380304336548, 1.051987886428833, -0.48672112822532654, 0.13256105780601501, 0.11332795768976212, -0.7538545727729797, 0.7475881576538086, 0.03152448311448097, 0.12781065702438354, 0.7149589657783508, 0.006506251636892557, -0.0204791147261858, -0.18828633427619934, -1.4448046684265137, 0.004281959496438503, 1.0168215036392212, 1.1241475343704224, 0.8125313520431519, 0.534351110458374, 0.2701314091682434, 1.0361716747283936, 0.015173649415373802, -0.06394164264202118, 0.19356630742549896, 0.4078608751296997, -0.236456498503685, -0.22645767033100128, 0.01872188225388527, 0.8044072985649109, -0.5163669586181641, -1.1619558334350586, 0.10056097060441971, 0.4147355258464813, -0.087842658162117, 0.6314126253128052, 0.7744168639183044, 0.31443992257118225, 0.21947993338108063, 0.270999938249588, 0.6905341148376465, -0.4905989468097687, -0.22355936467647552, 0.1977814882993698, -0.1999906748533249, -0.04165376350283623, 0.006775225512683392, -0.7217636704444885, -0.45003363490104675, -0.304328978061676, -0.0379723496735096, 0.29357028007507324, 0.40404728055000305, 1.2274625301361084, 0.6109786033630371, 0.41949784755706787, -0.646374523639679, -0.7373367547988892, -0.22555597126483917, -0.9104304909706116, -0.16212508082389832, -0.6067169904708862, -0.660244882106781, -0.13231655955314636, -0.12497607618570328, -0.23545949161052704]}, "authors": [{"authorId": "2143684319", "name": "Muhammad Adnan"}, {"authorId": "2291138227", "name": "Akhil Arunkumar"}, {"authorId": "2291140196", "name": "Gaurav Jain"}, {"authorId": "2291140463", "name": "Prashant J. Nair"}, {"authorId": "2291139370", "name": "Ilya Soloveychik"}, {"authorId": "2291139443", "name": "Purushotham Kamath"}], "references": [{"paperId": "364d04149674df5ae8501007768cd085c4c12cee", "title": "ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching"}, {"paperId": "7a54aad06171f59149aca5380863c62729c70b41", "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM"}, {"paperId": "a48a3cfde9e9a6f02821ea28698012e4d3e1cd73", "title": "QAQ: Quality Adaptive Quantization for LLM KV Cache"}, {"paperId": "37ffdf2c97a846d20418201d22604004ed1a98ba", "title": "D\u00e9j\u00e0Vu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "3493104b05ea7721d862b3dc99b97f8678b80596", "title": "Ad-Rec: Advanced Feature Interactions to Address Covariate-Shifts in Recommendation Networks"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "3f6341f3535f95de5e25932a293dc016a9c76e8b", "title": "FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "a935ba7ce7fd44fe372c6860504fbc164f012f03", "title": "HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention"}, {"paperId": "f78fe02f681a0a9a6867b007bd39e3884de64a91", "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization"}, {"paperId": "3692f4df9d11af68f9b9c9a526667db3f99e552c", "title": "Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "2c9436969f7b4502ef8cb8993d5d6a4473a0dc83", "title": "Heterogeneous Acceleration Pipeline for Recommendation System Training"}, {"paperId": "dca4d9abbc82e57dfa52f932e893d467a63e0682", "title": "Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2", "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks"}, {"paperId": "ce6b6046ad26e0f29932b539919c58333aedf1eb", "title": "EL-Attention: Memory Efficient Lossless Attention for Generation"}, {"paperId": "9dc624d7258d1a56117ca720aea953ce46b66b21", "title": "Efficient Attentions for Long Document Summarization"}, {"paperId": "b49bcd1b71e295494068868cdd543c1a96cc9b74", "title": "Accelerating Recommendation System Training by Leveraging Popular Choices"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "94f94e8892261d0377159379ca5a166ceae19a14", "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "690edf44e8739fd80bdfb76f40c9a4a222f3bba8", "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "515a21e90117941150923e559729c59f5fdade1c", "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "237a780a7b36140baac5d118f5e019f22c91536e", "title": "Generalized Gumbel distribution"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": null, "title": "Linformer"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": null, "title": "Categorical repa-rameterization with gumbel-softmax"}, {"paperId": "d20bfd7a53f4367c277ae3b921994a0c2fc3b4ec", "title": "D\u00e9ja vu."}, {"paperId": null, "title": "Efficient fine-tuning of long-context large language models"}, {"paperId": null, "title": "How long can context length of open-source llms truly promise?"}, {"paperId": null, "title": "Introducing mpt-7b: A new standard for open-source,"}, {"paperId": null, "title": "KV Cache reduction through key tokens selection for Efficient Generative Inference"}, {"paperId": null, "title": "H _ 2 o: Heavy-hitter oracle for efficient generative inference of large language models"}, {"paperId": null, "title": ": Training generalized multi-query transformer models from multi-head check-points"}, {"paperId": null, "title": "Falcon-40b: an open large language model with state-of-the-art performance"}]}