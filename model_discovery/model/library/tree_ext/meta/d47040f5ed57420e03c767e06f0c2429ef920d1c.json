{"paperId": "d47040f5ed57420e03c767e06f0c2429ef920d1c", "title": "Fibottention: Inceptive Visual Representation Learning with Diverse Attention Across Heads", "abstract": "Visual perception tasks are predominantly solved by Vision Transformer (ViT) architectures, which, despite their effectiveness, encounter a computational bottleneck due to the quadratic complexity of computing self-attention. This inefficiency is largely due to the self-attention heads capturing redundant token interactions, reflecting inherent redundancy within visual data. Many works have aimed to reduce the computational complexity of self-attention in ViTs, leading to the development of efficient and sparse transformer architectures. In this paper, viewing through the efficiency lens, we realized that introducing any sparse self-attention strategy in ViTs can keep the computational overhead low. However, these strategies are sub-optimal as they often fail to capture fine-grained visual details. This observation leads us to propose a general, efficient, sparse architecture, named Fibottention, for approximating self-attention with superlinear complexity that is built upon Fibonacci sequences. The key strategies in Fibottention include: it excludes proximate tokens to reduce redundancy, employs structured sparsity by design to decrease computational demands, and incorporates inception-like diversity across attention heads. This diversity ensures the capture of complementary information through non-overlapping token interactions, optimizing both performance and resource utilization in ViTs for visual representation learning. We embed our Fibottention mechanism into multiple state-of-the-art transformer architectures dedicated to visual tasks. Leveraging only 2-6% of the elements in the self-attention heads, Fibottention in conjunction with ViT and its variants, consistently achieves significant performance boosts compared to standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image classification, video understanding, and robot learning tasks.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A general, efficient, sparse architecture for approximating self-attention with superlinear complexity that is built upon Fibonacci sequences is proposed, named Fibottention, which embeds the Fibottention mechanism into multiple state-of-the-art transformer architectures dedicated to visual tasks."}, "embedding": {"model": "specter_v2", "vector": [0.5530327558517456, 0.6834399700164795, -0.2549355626106262, 0.34656810760498047, 0.10985332727432251, 0.19885100424289703, 0.579021155834198, -0.26207879185676575, -0.10919848084449768, -0.5887974500656128, 0.39984506368637085, 0.4299542307853699, 0.2975391745567322, -0.010444660671055317, -0.4270835220813751, 0.15760259330272675, -0.7249578237533569, -0.09613306075334549, 0.31311655044555664, -0.34492388367652893, 0.2935980260372162, -0.7582957148551941, -1.6349499225616455, 0.34397798776626587, 0.32664477825164795, 1.185762643814087, 0.4662388563156128, 0.8245915770530701, -0.2999693751335144, 0.6294363141059875, 0.47865843772888184, -0.43767470121383667, 0.5185180902481079, -0.04196310043334961, -0.5387219190597534, 0.17455528676509857, 0.9136885404586792, -0.07946144044399261, -0.8388442993164062, 0.8719440698623657, -0.01923835463821888, 0.09557164460420609, 0.6818401217460632, -0.6933609247207642, -0.5542468428611755, 0.45135363936424255, 0.3970361351966858, 0.637570858001709, -0.3693714439868927, -0.4750312268733978, 1.3039262294769287, -1.4138798713684082, 0.2764606773853302, 1.5491610765457153, 0.3265277147293091, 0.4695139229297638, -0.18987122178077698, -0.6006248593330383, 0.6692180633544922, 0.5615623593330383, -0.5817895531654358, -0.5359210968017578, -0.033720534294843674, -0.16922298073768616, 1.5060936212539673, -0.8631417751312256, -0.04307907819747925, 0.39507853984832764, 0.3754993677139282, 1.5633292198181152, -0.16151510179042816, -0.8720906972885132, -0.03730737045407295, -0.21769088506698608, 0.0872931182384491, 1.1006102561950684, -0.2633230686187744, 0.33060818910598755, -1.3752299547195435, 0.5134414434432983, 0.9293885231018066, -0.0831110030412674, 0.16992779076099396, -0.6227636933326721, -0.1527543067932129, 0.6335077285766602, 0.9878838062286377, 0.6243304014205933, -0.3155020475387573, 1.116167664527893, 0.6961374282836914, 0.12905748188495636, -0.4120960831642151, 0.005444603972136974, 0.6075003147125244, 0.8028720021247864, -0.3890993893146515, -0.2583615183830261, -0.24379344284534454, 1.3122411966323853, 0.1382584273815155, 0.6281796097755432, -0.7567093372344971, 0.11068202555179596, 1.2338885068893433, -0.03254571184515953, 0.5636067986488342, -0.657263457775116, 0.02954324334859848, -0.774293065071106, -0.17427018284797668, -0.7601267099380493, 0.26973891258239746, -0.2703113853931427, -1.0819969177246094, -0.6486870646476746, -0.31737998127937317, 0.7154555320739746, -1.2442889213562012, 0.41745230555534363, -0.19654157757759094, -0.21414130926132202, -0.44639307260513306, 0.4650402069091797, 0.550075352191925, 0.16785435378551483, 0.49388161301612854, 0.4061388671398163, 1.307682752609253, -0.8385431170463562, -0.3571561872959137, -0.9079464077949524, 0.013662062585353851, -0.10674848407506943, 0.023321691900491714, 0.11154721677303314, -1.22310471534729, -1.5558518171310425, -0.45376914739608765, -0.08964332938194275, -0.27398648858070374, 0.2106543779373169, 1.077711820602417, 0.09033157676458359, -1.046199083328247, 0.6788603663444519, -0.00204078177921474, -0.37276333570480347, 0.8409932255744934, 0.14045874774456024, 0.050187669694423676, -0.5638493299484253, -0.8072969913482666, 0.2990947663784027, 0.09191323816776276, -0.5496867299079895, -0.7728888988494873, -0.18660569190979004, -1.5825393199920654, -0.0030372191686183214, 0.22963744401931763, -0.9048711657524109, 1.027863621711731, -0.45777323842048645, -0.7484936118125916, 0.7405259013175964, -0.9417224526405334, -0.04238172620534897, 0.06783422082662582, -0.2596263289451599, -0.07262739539146423, -0.329358845949173, 0.02449902705848217, 0.8039939403533936, 1.0896564722061157, -0.5988979339599609, -0.3049037754535675, -0.13152636587619781, -0.4645703136920929, -0.024154052138328552, -0.153594508767128, 0.7466211318969727, -0.7353121042251587, -0.2205241173505783, 0.5637028813362122, 0.7503746747970581, 0.17449113726615906, 0.015437815338373184, -0.14904505014419556, -1.3952690362930298, 0.751873254776001, 0.8552476763725281, 0.4074244499206543, -0.9653053879737854, -0.6518563628196716, -0.36823827028274536, 0.21990959346294403, -0.09909724444150925, -0.9218326210975647, 0.6851392388343811, -0.35722798109054565, 0.21255385875701904, 0.1831616759300232, -1.0856692790985107, -0.014347491785883904, -0.10936886072158813, -0.7042896747589111, 0.0957903042435646, 0.43000364303588867, 1.2164027690887451, -1.0893385410308838, -0.43137112259864807, 0.2554011940956116, 0.1574171483516693, -0.6928865909576416, 0.9034090042114258, -0.1342451274394989, -0.08315714448690414, 0.015039755031466484, 0.024555129930377007, 0.03841584920883179, -0.4682033061981201, 0.14867638051509857, -0.9156123399734497, -0.09846679866313934, 0.3831975758075714, -0.24830205738544464, 1.3276965618133545, -0.23491254448890686, 0.6582793593406677, -0.21808366477489471, -0.8947858214378357, 0.1418900340795517, 0.13469240069389343, -0.10718908160924911, -0.8001123070716858, 0.2937338650226593, -0.2133181095123291, -0.6753544807434082, 0.10991600900888443, 0.7549670338630676, 1.017128825187683, -0.13478460907936096, -0.15438799560070038, 0.9809606671333313, -0.2030998170375824, 0.24480412900447845, 0.5946999788284302, 0.7708348035812378, 0.5365535020828247, 0.3693322241306305, -0.1970536708831787, 0.01914951764047146, -0.7603989243507385, -0.27188336849212646, 0.8005509376525879, 0.3573768734931946, 1.0572361946105957, 0.017451103776693344, -0.8560258746147156, -0.34747299551963806, -0.05810842663049698, 0.4557536840438843, 1.5750312805175781, 0.17408789694309235, -0.17862972617149353, -0.2948905825614929, -0.14538311958312988, -0.38006898760795593, -0.4639365077018738, -0.8157545924186707, -0.29845499992370605, -0.13697120547294617, -0.4050293266773224, 0.5732856392860413, 0.4723931849002838, 1.3367866277694702, -1.0445433855056763, -0.8284069299697876, -0.1330370157957077, 0.41012662649154663, -0.6003691554069519, -0.7617597579956055, 0.7093737125396729, -0.11192156374454498, -0.2686159908771515, -0.14244353771209717, -0.5573044419288635, 0.2602922320365906, 0.16309599578380585, 1.164452314376831, -0.8810461163520813, -0.877891480922699, 0.3159083425998688, 0.29216158390045166, -1.0002920627593994, -0.15266679227352142, -0.2327124923467636, 0.0969318151473999, -0.09499211609363556, 0.22433950006961823, 0.5713185667991638, -0.3089640736579895, 0.28287556767463684, -0.40896937251091003, -0.28225696086883545, 0.25003618001937866, 0.22013384103775024, 0.7730917930603027, -0.24985484778881073, -0.06471548229455948, -0.9686698317527771, 0.3869328796863556, 0.027765249833464622, 0.09303780645132065, -0.17918522655963898, -0.45861056447029114, -0.3846244513988495, 0.3512386977672577, -0.6810155510902405, -0.10540633648633957, -0.27854710817337036, 0.835626482963562, -0.8297106027603149, -0.34110310673713684, -0.32416826486587524, 0.4917565584182739, -0.3554322123527527, 0.6323164701461792, 0.7061771750450134, 0.3403269946575165, 0.6518690586090088, 0.7461482882499695, -1.158595085144043, 0.8924936652183533, 0.03982199355959892, 0.17115281522274017, 0.2973064184188843, 0.035454533994197845, -0.9334877729415894, -0.4871618151664734, -0.7145093083381653, -0.07676579058170319, -0.520970344543457, 0.4053444266319275, -0.8176766633987427, -1.276502013206482, 0.18640325963497162, -1.0333366394042969, -0.17290319502353668, -0.2500418722629547, -0.4231695532798767, -0.557222306728363, -0.7132337689399719, -0.6302595734596252, -0.834625244140625, -0.5321841239929199, -0.6273443698883057, 0.4624491333961487, 0.2191649079322815, -0.21886171400547028, -0.17553570866584778, -0.24007028341293335, -0.5479255318641663, 0.9272564649581909, -0.7032881379127502, 0.2460600882768631, 0.40881192684173584, -0.683471143245697, 0.23143811523914337, -0.4755469262599945, 0.2710117697715759, 0.13983123004436493, -0.004675523843616247, -0.9268150329589844, 0.18952439725399017, -0.4582218825817108, -0.7788820266723633, 0.7039945125579834, 0.4731837809085846, 0.8260281085968018, -0.1466524451971054, -0.464332640171051, 0.4905255436897278, 1.6568078994750977, -0.5898537039756775, 0.39905035495758057, -0.0629958063364029, 1.0552023649215698, 0.34272298216819763, -0.48515641689300537, 0.7624191641807556, 0.8218163847923279, 0.2141232043504715, 0.8982778787612915, -0.4430510103702545, -0.6399503946304321, -0.6727160811424255, 0.12823167443275452, 0.6946840286254883, 0.24643291532993317, -0.0790901631116867, -0.8195258378982544, 1.124565601348877, -1.1479768753051758, -1.139382004737854, 0.5294246673583984, 0.762559711933136, -0.07442440837621689, -0.5118193626403809, -0.12597443163394928, -0.3940509557723999, 0.5569746494293213, 0.15006232261657715, -0.595878005027771, -0.45859310030937195, -0.17318296432495117, 0.5656282305717468, 0.4293367266654968, 0.09159796684980392, -0.47108694911003113, 0.5810981392860413, 14.736671447753906, 0.46138617396354675, -0.16935329139232635, 0.38025718927383423, 0.5739377737045288, 0.6823819875717163, -0.15251609683036804, -0.049967292696237564, -1.0693812370300293, -0.4129961431026459, 0.41478556394577026, 0.3062995672225952, 0.25048506259918213, 0.2742646336555481, -0.3661026358604431, 0.11630961298942566, -0.5132811665534973, 0.9999174475669861, 0.7448844909667969, -1.0547324419021606, 0.6110530495643616, 0.10818324238061905, 0.2526392638683319, 0.46681350469589233, 0.8521053194999695, 0.7913082838058472, 0.6606509685516357, -0.5936586856842041, 0.5486857891082764, 0.2500852942466736, 1.1124987602233887, 0.274150013923645, -0.2194187194108963, 0.3677007555961609, -1.3776870965957642, -0.2963375151157379, -0.9778510928153992, -1.0511335134506226, -0.03490014746785164, -0.18320050835609436, -0.37120068073272705, -0.2798003852367401, 0.14889435470104218, 0.7118759751319885, 0.4005633294582367, 0.507757842540741, 0.1197347566485405, 0.19083276391029358, 0.051182638853788376, 0.037137340754270554, 0.11650726199150085, 0.9211089015007019, -0.08333092927932739, -0.005784377455711365, 0.012182051315903664, 0.20868420600891113, 0.057414885610342026, 0.5214828848838806, -0.3302808105945587, -0.4607725739479065, -0.6163830161094666, 0.38624632358551025, -0.039271075278520584, 0.8604968786239624, 0.6084632873535156, 0.223340705037117, -0.046308811753988266, 0.37899845838546753, 0.6029275059700012, 0.07702553272247314, -0.3350525498390198, -0.1550629884004593, 0.04923374578356743, -0.43943753838539124, 0.46297967433929443, 0.5697314739227295, -0.37302032113075256, -0.30025044083595276, -0.8378642201423645, -0.23324738442897797, 0.4071802794933319, -0.9581947326660156, -0.8518535494804382, 1.0204135179519653, -0.2538512945175171, -0.49340564012527466, 0.5779533386230469, -0.9797659516334534, -0.6391803622245789, 0.3548387289047241, -1.3455151319503784, -0.9350938200950623, -0.25938940048217773, 0.00675239460542798, -0.07809141278266907, -0.08568158745765686, 0.8188779950141907, -0.24448472261428833, 0.06520552188158035, 0.27563735842704773, -0.5140039324760437, -0.2039826363325119, -0.13605327904224396, -0.8164477348327637, 0.7335962653160095, 0.13198190927505493, 0.1183914989233017, 0.3361152410507202, 0.16039550304412842, 0.3582826554775238, -1.0283401012420654, -0.02509235590696335, 0.46513083577156067, -0.5944314002990723, -0.6554186344146729, -0.49576568603515625, -0.7904749512672424, -0.01499682106077671, 0.7361183762550354, 0.30507171154022217, -0.0949215218424797, 0.12359069287776947, -0.7741389870643616, -0.07952050119638443, -0.6418702006340027, 0.0206533782184124, 0.621514618396759, -0.6991691589355469, -0.5299782156944275, -0.24121536314487457, -0.3620871603488922, -0.7242528200149536, -0.16769665479660034, -0.3767257034778595, 0.39656922221183777, -0.48440682888031006, 1.1138802766799927, -0.2777951955795288, 0.7390766739845276, 0.5923447012901306, -0.033913079649209976, -0.5646469593048096, -0.6108058094978333, -0.8143849968910217, -0.04317181929945946, 0.2850158214569092, -0.10061494261026382, -0.1599542647600174, 0.3809163272380829, 0.5042786002159119, 0.055995140224695206, -0.31395116448402405, -0.8183133006095886, -0.17368923127651215, -0.0854477509856224, -0.5111707448959351, -0.2313729226589203, -0.28577932715415955, 0.2915045917034149, 0.3416886627674103, 0.4559922516345978, 0.12898559868335724, 0.369247168302536, -0.7939794659614563, 0.684983491897583, -0.08657404780387878, 0.09889329969882965, -0.5292330384254456, -0.6898815631866455, -1.5699071884155273, -0.36557549238204956, -1.0102019309997559, 0.1270962953567505, -1.1923884153366089, -0.43933090567588806, 0.30586883425712585, -0.37968021631240845, 0.23090390861034393, 0.48125994205474854, 0.05126510187983513, 0.0026319888420403004, -0.3455941379070282, -0.8403359651565552, 0.9280540347099304, 1.252284049987793, -0.8868568539619446, 0.2871421277523041, -0.3933401107788086, -0.3101668357849121, 0.3728107511997223, 0.24781917035579681, -0.27269941568374634, -0.6557582020759583, -1.1650980710983276, -0.05636965110898018, -0.27866417169570923, 0.19613279402256012, -1.3541406393051147, 1.1427366733551025, 0.5321479439735413, 0.3933710753917694, -0.15881697833538055, 0.4222920835018158, -0.8851120471954346, -0.6905507445335388, 0.51250821352005, -0.6683260798454285, 0.026400558650493622, 0.18189141154289246, -0.2228263020515442, -0.24881435930728912, 0.8760786652565002, 0.18479600548744202, -1.4294557571411133, -1.1303622722625732, 0.44447821378707886, -0.48444703221321106, 0.3042377233505249, 0.004176269751042128, -0.5983182787895203, -1.2539530992507935, -0.3730885088443756, 0.1743132621049881, 0.41439157724380493, -0.4818122982978821, 0.7840746641159058, 1.0499318838119507, -1.060317039489746, 0.03703371435403824, 0.6455275416374207, 0.325074702501297, 0.31707584857940674, 0.796116828918457, 0.03101980686187744, 0.22591164708137512, 0.4177456200122833, -0.35687974095344543, -0.004432126879692078, -0.5757115483283997, 0.5530080199241638, 0.7050979733467102, -0.2172132134437561, -0.05676845461130142, 1.1596635580062866, 0.43002694845199585, -0.4729355275630951, 0.15628452599048615, -0.8559765219688416, -0.47242674231529236, -0.009798210114240646, 0.4618397057056427, 0.07320382446050644, -0.3376908600330353, -0.4113394021987915, -0.636486828327179, 0.5930412411689758, -0.33102163672447205, -0.34142690896987915, 0.09945130348205566, -0.02426186390221119, 0.13653860986232758, 0.2732185423374176, 0.6501797437667847, -0.9045172333717346, -1.077682614326477, -0.9373763799667358, -0.8896691203117371, -0.12309903651475906, 0.202641099691391, 0.05529465898871422, -0.5589872598648071, 0.8468366265296936, 0.49627918004989624, 0.36162465810775757, 0.10077325254678726, -0.07509409636259079, -0.17828203737735748, 0.6393060684204102, 0.2643188536167145, -0.5240042805671692, 0.011506393551826477, 1.4658321142196655, 1.4661856889724731, -0.8452325463294983, 0.04348389804363251, -0.236765518784523, -0.7620247006416321, 0.6040538549423218, 0.624659538269043, -0.6429779529571533, 0.6355481743812561, -0.5169869661331177, -0.10805360972881317, 0.018658405169844627, -1.114359736442566, -0.8129210472106934, 1.2284034490585327, 1.592509388923645, 0.7682957053184509, -0.2933759093284607, 0.4422391355037689, 0.16765308380126953, 0.24306470155715942, 0.2205023616552353, 0.206106036901474, 0.002428423846140504, -0.3444857895374298, 0.5056898593902588, 0.030712543055415154, 0.5440589189529419, -0.3979417085647583, -0.4381496012210846, 0.35157161951065063, 0.34762799739837646, 0.360738605260849, 0.6321699023246765, 0.9813130497932434, -0.10550693422555923, 0.6394401788711548, -0.15782971680164337, 0.9355603456497192, -0.5591814517974854, -0.11742079257965088, -0.0468527115881443, -1.1042309999465942, -0.1667095124721527, -0.45510604977607727, -0.5529733896255493, -0.09241722524166107, 0.19294345378875732, -0.043248485773801804, -0.34477636218070984, 0.3667844235897064, 0.7322254180908203, 0.5780918002128601, 0.7405626177787781, -0.2633562684059143, -0.9064092040061951, -0.38726624846458435, -0.9231333136558533, 0.3278960585594177, -0.26942071318626404, -0.003286113729700446, -0.5899542570114136, 0.10736747086048126, 0.2145211547613144]}, "authors": [{"authorId": "2308469241", "name": "Ali Khaleghi Rahimian"}, {"authorId": "2306265813", "name": "Manish Kumar Govind"}, {"authorId": "13238892", "name": "Subhajit Maity"}, {"authorId": "2160860706", "name": "Dominick Reilly"}, {"authorId": "2308470067", "name": "Christian Kummerle"}, {"authorId": "2271800064", "name": "Srijan Das"}, {"authorId": "2276608498", "name": "A. Dutta"}], "references": [{"paperId": "295364abf15c24b4ccc11f62882160f51fe915eb", "title": "EdgeTran: Device-Aware Co-Search of Transformers for Efficient Inference on Mobile Edge Platforms"}, {"paperId": "53627c07e8970297391b252afa74dc6edb5b86d3", "title": "Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment"}, {"paperId": "0d04360ef9feba8230026468c4c7c03d075bec3b", "title": "Multiview Aerial Visual Recognition (MAVREC): Can Multi-view Improve Aerial Visual Perception?"}, {"paperId": "5173fe619a921169f56f803c102c2a1aec7e50d1", "title": "Just Add \u03c0! Pose Induced Video Transformers for Understanding Activities of Daily Living"}, {"paperId": "06b9735c0d4aaf5b8e3d8f2ce57b0bc49d892a06", "title": "Vision Big Bird: Random Sparsification for Full Attention"}, {"paperId": "c12db2c60e8989f646a29ad4f4d24475e860ad91", "title": "LongNet: Scaling Transformers to 1, 000, 000, 000 Tokens"}, {"paperId": "fae53cfc54eed4f2b602cce39d5519782e948d03", "title": "Crossway Diffusion: Improving Diffusion-based Visuomotor Policy via Self-supervised Learning"}, {"paperId": "e658f8a80f3d859629a0eb0d338e4bf8d6d6c493", "title": "SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer"}, {"paperId": "19921cefb2470b2f5d984ab9ce92ebb94aedf2ea", "title": "Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers"}, {"paperId": "4f8b5da162ca609195076f17c392ccf583b281ae", "title": "Generalizing the Wythoff Array and other Fibonacci Facts to Tribonacci Numbers"}, {"paperId": "a883336e5c2e9f46f5012343227a6be4671c9ca0", "title": "Dilated Neighborhood Attention Transformer"}, {"paperId": "2fe71acc2c3f1e75b6149dea72838f0b594ad013", "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers"}, {"paperId": "d2425b430fbf5b8ddf9cf2309c36a80a71e5a449", "title": "OmniMAE: Single Model Masked Pretraining on Images and Videos"}, {"paperId": "ad7bcec33f5206d4f28687a6a5a950de67010651", "title": "Neighborhood Attention Transformer"}, {"paperId": "2babc9ba9dd301d6e61117302bd2a200f7b422e2", "title": "DOTA: detect and omit weak attentions for scalable transformer acceleration"}, {"paperId": "3b03753bff6d934c6d403d1039ecaca0c98e9a69", "title": "StARformer: Transformer with State-Action-Reward Representations for Visual Reinforcement Learning"}, {"paperId": "5c74b1021aebd0575c339ce4dfd47e183009a9c5", "title": "Implicit Behavioral Cloning"}, {"paperId": "3032844d6ac6882ccb03e7a2c22a0026b210ac05", "title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "e775e649d815a02373eac840cf5e33a04ff85c95", "title": "CvT: Introducing Convolutions to Vision Transformers"}, {"paperId": "610b302950a19acef1c45456111dcd495f638c18", "title": "ConViT: improving vision transformers with soft convolutional inductive biases"}, {"paperId": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8", "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention"}, {"paperId": "fa08b41ccdfc5d8771adfbc34c176fa237d4646c", "title": "Is Space-Time Attention All You Need for Video Understanding?"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "3e7f5f4382ac6f9c4fef6197dd21abf74456acd1", "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d", "title": "Are Transformers universal approximators of sequence-to-sequence functions?"}, {"paperId": "c17f395738bc3494974283ba9460c516a948f7ef", "title": "Toyota Smarthome: Real-World Activities of Daily Living"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "0cf535110808d33fdf4db3ffa1621dea16e29c0d", "title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "36e30516683032634975c53e60f3737b6e35ff80", "title": "Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting"}, {"paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d", "title": "Stand-Alone Self-Attention in Vision Models"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "2a31319e73d4486716168b65cdf7559baeda18ce", "title": "Star-Transformer"}, {"paperId": "d823863b0baee2c5797322528fc692a2c2377c70", "title": "Fibonacci and Lucas Numbers With Applications"}, {"paperId": "ef761435c1af2b3e5caba5e8bbbf5aeab69d934e", "title": "Learning Clip Representations for Skeleton-Based 3D Action Recognition"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "13d9323a8716131911bfda048a40e2cde1a76a46", "title": "Structured Attention Networks"}, {"paperId": "091e4d3c85dc0a8212afea875cd3b162d273d46b", "title": "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis"}, {"paperId": "b0de8697aa2db045cc46f3da6d9bc266ec7c0f6b", "title": "The Extra Fibonacci Series and the Empire State Building"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "5de214630011554bd07b41ec5bd493c7f65c532e", "title": "Cross-View Action Modeling, Learning, and Recognition"}, {"paperId": "93aa063d330ed7509f55b1297138cf20300fb969", "title": "Kernels for Vector-Valued Functions: a Review"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "9d70c40a7d9e82630a73bc0a375649b6ba631a7b", "title": "Fibonacci\u2019s Liber Abaci: A Translation into Modern English of Leonardo Pisano\u2019s Book of Calculation"}, {"paperId": "ca07aa57eb5de3b9bf7e05dc6c03079295834ac3", "title": "Efficient Deployment of Transformer Models on Edge TPU Accelerators: A Real System Evaluation"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "6538de0702aee53cc3f5c4ba9e01e087d0c754fc", "title": "Fibonacci Numbers"}, {"paperId": null, "title": "A Stolarsky array of Wythoff pairs"}, {"paperId": null, "title": "A modification of the game of nim"}, {"paperId": null, "title": "Introducing Meta Llama 3: The most capable openly available LLM to date"}, {"paperId": null, "title": "Gemini Team"}]}