{"paperId": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0", "title": "Qwen Technical Report", "abstract": "Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.", "venue": "arXiv.org", "year": 2023, "citationCount": 585, "influentialCitationCount": 91, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.16609", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts, and includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques."}, "embedding": {"model": "specter_v2", "vector": [-0.10688143223524094, 0.733156681060791, -0.7044842839241028, 0.0182303786277771, -0.4100481867790222, -0.3891359269618988, 0.665054976940155, -0.21089422702789307, -0.38967621326446533, 0.13493582606315613, 0.3738698363304138, -0.5790785551071167, 0.31415534019470215, 0.2739390730857849, -0.20088183879852295, 0.12790632247924805, -0.9236162304878235, 0.40189099311828613, -0.27831026911735535, -0.5805549621582031, -0.036996208131313324, -0.6473800539970398, -0.4569106996059418, -0.04433517903089523, 0.5341787338256836, -0.21082435548305511, 0.15684840083122253, 1.1351423263549805, -0.13932031393051147, 0.9120535850524902, 0.2664582431316376, -0.2139979898929596, -0.015631111338734627, -0.21309618651866913, -0.5032747387886047, -0.006561609450727701, 0.08552548289299011, -0.4568312168121338, -0.3043712079524994, 0.44886451959609985, -0.4345017671585083, 0.017660818994045258, 0.36312851309776306, -0.7871146202087402, -0.33343276381492615, 1.063696265220642, 0.6046603918075562, 0.7850301265716553, 0.013444885611534119, 0.0437360517680645, 1.276064157485962, -0.676703929901123, 0.6173689961433411, 1.2636183500289917, 0.4940333962440491, 0.41032227873802185, -0.29391199350357056, -0.7045883536338806, 0.1717195361852646, -0.42542025446891785, -0.7468774914741516, -0.1719779670238495, -0.33860650658607483, -0.2610609233379364, 1.7774444818496704, -0.3201506733894348, 0.09612923115491867, 0.46909549832344055, 0.18837498128414154, 1.078608751296997, -0.21588632464408875, -0.9491845369338989, -0.10559787601232529, 0.35103747248649597, 0.005253356881439686, 1.0890358686447144, -0.4888516664505005, 0.6245911121368408, -0.8069472312927246, -0.294219434261322, 0.7124384641647339, -0.33776524662971497, -0.13473324477672577, -0.2723870873451233, -0.6650020480155945, 0.8608645796775818, 0.28643977642059326, 1.0058006048202515, -0.04143455997109413, 0.6495450735092163, 0.29539111256599426, 0.5570226311683655, -0.20326174795627594, 0.5088055729866028, -0.2699599266052246, 0.1837311089038849, -0.3273114264011383, 0.22077524662017822, -0.060204584151506424, 0.951785147190094, -0.23762843012809753, 0.30131345987319946, -0.9105573296546936, 0.23475852608680725, 1.4356796741485596, -0.1318379044532776, 0.5877788066864014, -1.0603513717651367, 0.5656115412712097, -0.32288262248039246, 0.7507539391517639, -0.23445530235767365, -0.40478354692459106, -0.10567372292280197, -0.3839121460914612, -1.028108835220337, -0.38969215750694275, 0.042728982865810394, -0.5903037786483765, 0.7778666019439697, -0.2350429892539978, -0.3167423903942108, 0.5786685943603516, 0.5237163305282593, 0.45728829503059387, 0.4712805151939392, 0.5300449728965759, -0.011159551329910755, 0.7358629703521729, -0.8311524391174316, -0.9655659198760986, -1.1906139850616455, 1.0383691787719727, -0.02121838368475437, 0.08474641293287277, -0.48502981662750244, -1.104825735092163, -0.5732259750366211, -0.8305140733718872, 0.038349006325006485, -0.3297235369682312, 0.5302847027778625, 1.3821585178375244, 0.3316022455692291, -0.8999995589256287, 0.5344606041908264, -0.1252610683441162, -0.2907121181488037, -0.10029143840074539, 0.14139676094055176, 0.08260714262723923, -0.29566699266433716, -0.9865397214889526, 0.4836582839488983, 0.6783981323242188, -0.48245376348495483, -0.23270466923713684, 0.019712192937731743, -1.3836698532104492, -0.5582207441329956, 0.35803449153900146, -0.2440987229347229, 1.8700505495071411, -0.05853329971432686, -1.2830578088760376, 0.535784125328064, -0.26462024450302124, -0.051309648901224136, 0.4241252541542053, 0.057977695018053055, -0.8983194828033447, -0.3150903582572937, -0.06697230786085129, 0.4086819887161255, 0.14213018119335175, -0.562772274017334, -0.17296244204044342, 0.4260614812374115, 0.23498766124248505, -0.313593327999115, -0.22431932389736176, 0.9474807381629944, -0.25680121779441833, -0.18908704817295074, -0.020344814285635948, 0.5796663165092468, -0.22804558277130127, 0.02747652679681778, -0.2557000517845154, -0.7378259301185608, 0.41776904463768005, -0.20401519536972046, 1.2104851007461548, -0.8324980139732361, -0.4453962743282318, -0.27456679940223694, -0.2117067575454712, -0.08479171246290207, -0.868842601776123, 0.8817744851112366, -0.41278162598609924, 0.5593271851539612, -0.5730268955230713, -0.7224283814430237, -0.0645986944437027, 0.04361763969063759, -0.35934165120124817, -0.10925152897834778, 0.1657257378101349, 1.1636829376220703, -1.1022121906280518, -0.10738807916641235, 0.12190426886081696, -0.06638512015342712, -0.7767311334609985, 1.5729204416275024, -0.36638343334198, 0.2917502224445343, -0.04183046519756317, -0.3824417293071747, -0.066218301653862, -0.22915488481521606, 0.6412503123283386, 0.15935058891773224, -0.4588995575904846, 0.309003621339798, -0.17261527478694916, 1.470152735710144, -0.6166759133338928, 0.03308664262294769, 0.2674819529056549, -0.13566076755523682, -0.1100572720170021, 0.7408230304718018, -0.30916735529899597, -0.3997098505496979, 0.253206729888916, 0.4675271809101105, -0.2372656613588333, -0.1278611123561859, 0.3910960853099823, 0.5103208422660828, -0.31018880009651184, 0.4905552268028259, 0.5887263417243958, -0.7065884470939636, 0.7152343392372131, 0.35402756929397583, 0.6201002597808838, 0.32433590292930603, 0.9629444479942322, -0.011956281960010529, 0.2015981376171112, -0.6062257885932922, -0.13232088088989258, 0.27028369903564453, 0.5104293823242188, 0.4056910276412964, 0.11463136970996857, -0.8278937935829163, -0.1436472088098526, -0.2534151077270508, 0.854668140411377, 1.3598179817199707, -0.05730842053890228, -0.27467429637908936, -0.6193235516548157, -0.4813816547393799, -0.09027604758739471, 0.4484504461288452, -0.4098370671272278, -0.29486149549484253, -1.0045124292373657, -0.5664403438568115, 0.6974174380302429, 0.38479506969451904, 0.7212905287742615, -0.9063185453414917, -0.5573791861534119, -0.25711217522621155, 0.24602371454238892, -0.542442262172699, -0.7736654281616211, 0.3624943494796753, -0.6817737817764282, -0.45694485306739807, 0.18424642086029053, -0.5281773805618286, 0.16198202967643738, -0.655446469783783, 0.9445180892944336, 0.1761358380317688, -0.31916773319244385, 0.010097290389239788, 0.6773561835289001, -0.6474639177322388, -1.233434796333313, 0.07982692122459412, 0.005351242143660784, -0.66459059715271, -0.060341909527778625, 0.6803340315818787, 0.48672017455101013, 0.005336593370884657, -0.6956321597099304, 0.29834404587745667, 0.3714998960494995, 0.2897183895111084, -0.28525111079216003, -0.67926424741745, -0.11985944956541061, -1.0572630167007446, 1.0262501239776611, 0.275312215089798, -0.47140970826148987, 0.5201703906059265, -0.518459677696228, -0.10358244925737381, 0.3610532581806183, -0.534395694732666, -0.19515939056873322, -1.0801644325256348, 0.46836668252944946, 0.5171682834625244, -0.23589888215065002, 0.2526763379573822, 0.6105112433433533, 0.21596573293209076, 0.1755010038614273, 0.3214854896068573, 0.6122229099273682, 0.07731091976165771, 0.5887439250946045, -0.3717837631702423, 0.15939787030220032, 0.3024384379386902, 0.6001265645027161, -0.40822359919548035, -0.661103367805481, -0.4501184821128845, -0.09868976473808289, -0.16076606512069702, 0.03970152139663696, -0.22519251704216003, 0.2518654763698578, -0.6070485711097717, -1.0337038040161133, -0.1294233351945877, -1.3500412702560425, -0.5919694304466248, 0.3584567606449127, -0.0782279297709465, -0.3184698224067688, -0.845561146736145, -1.3969184160232544, -0.9253920912742615, -0.16030029952526093, -1.071205973625183, 0.43813037872314453, -0.15019330382347107, -0.7860013842582703, -0.49888527393341064, 0.3273000121116638, -0.060776080936193466, 0.5508468747138977, -0.7695184946060181, 1.1838635206222534, 0.4496098458766937, -0.2579309642314911, 0.04367228224873543, 0.449424147605896, 0.14432261884212494, 0.025159472599625587, 0.361149400472641, -0.29604971408843994, 0.08005140721797943, -0.6035646796226501, -0.69866544008255, -0.42859920859336853, 0.04698909446597099, 0.4021833539009094, 0.02785208635032177, -0.37414395809173584, -0.13207478821277618, 1.1826579570770264, -0.3950411081314087, -0.1479102224111557, 0.39191049337387085, 0.5408152937889099, 0.483175665140152, -0.15145674347877502, 0.4630931317806244, 0.34506717324256897, 0.6703597903251648, 0.19013437628746033, -0.15731123089790344, 0.14777527749538422, -0.34047210216522217, 0.5620387196540833, 1.2020779848098755, -0.04813196137547493, 0.07739946246147156, -1.3021670579910278, 0.6252013444900513, -1.1942733526229858, -0.5123393535614014, 0.7310310006141663, 0.5642619729042053, 0.7849459648132324, -0.6068152189254761, -0.14959348738193512, -0.12905195355415344, 0.5188061594963074, 0.2901330888271332, -0.03150811418890953, -0.7482383847236633, 0.2912730574607849, 0.11734233051538467, -0.1273811310529709, 0.8797305226325989, -0.25494185090065, 0.4915977120399475, 15.2335844039917, 0.5673815608024597, 0.24691376090049744, 0.5829064249992371, 0.2671544551849365, 0.19628259539604187, -0.4639120101928711, -0.5182398557662964, -0.7031238079071045, -0.3735654354095459, 1.1758742332458496, 0.012943468056619167, 1.1917115449905396, 0.05815554037690163, 0.17882010340690613, 0.07226615399122238, -0.3453105390071869, 0.5347657203674316, 0.34529221057891846, -1.0178234577178955, 0.36626380681991577, 0.17645061016082764, 0.03673693165183067, 0.3862003684043884, 0.5439419746398926, 1.0484905242919922, 0.8697232604026794, -0.6497716903686523, 0.9427540302276611, -0.13395865261554718, 0.8385303020477295, -0.27724793553352356, 0.520948052406311, 0.8754558563232422, -0.9256385564804077, -0.6926596760749817, -0.27115896344184875, -1.2097302675247192, 0.3212313652038574, -0.20769087970256805, -0.5358999371528625, -0.24487297236919403, -0.3750620186328888, 0.4455094039440155, 0.46431779861450195, 0.03498286008834839, -0.38116273283958435, 0.17183949053287506, 0.24190284311771393, -0.1387147605419159, 0.20567743480205536, 0.18027134239673615, 0.06567765772342682, -0.19173868000507355, -0.050558555871248245, 0.12025114893913269, 0.06906837970018387, 0.5188919901847839, -0.6538843512535095, -0.15144389867782593, -0.7153314352035522, -0.6546601057052612, -0.20970135927200317, 0.5186320543289185, 0.6164382696151733, 0.47781744599342346, -0.4169398844242096, 0.17087510228157043, 0.936033308506012, 0.17877346277236938, -0.4531020522117615, 0.16520197689533234, 0.4060586392879486, -0.5162540674209595, -0.18072202801704407, 0.06300105154514313, -0.017358530312776566, -0.32530614733695984, -0.8404985666275024, -0.5103310346603394, 0.09907719492912292, -0.7968093752861023, -0.40064799785614014, 1.1115151643753052, -0.17219841480255127, -0.7349244356155396, -0.04321517050266266, -0.7118310332298279, -0.0018116566352546215, 0.3563692569732666, -1.4547220468521118, -0.6678370833396912, 0.45818036794662476, -0.4169572591781616, -0.429882675409317, -0.2082318663597107, 1.5231475830078125, -0.2467062920331955, -0.44988444447517395, -0.19096270203590393, 0.1669914275407791, 0.06597334891557693, -0.28687942028045654, -0.7718532681465149, 0.5334687232971191, 0.1157427504658699, -0.10906396806240082, 0.7157738208770752, 0.22367188334465027, -0.14418171346187592, -0.4454806447029114, -0.04969911277294159, 0.674205482006073, -1.1159764528274536, -0.16397717595100403, -0.8229843974113464, -0.3923225700855255, 0.4462873637676239, 0.8409259915351868, -0.3017366826534271, 0.45907971262931824, -0.11284815520048141, -0.56379634141922, 0.1850958913564682, -0.9469866752624512, 0.26836854219436646, 0.5799283385276794, -0.6976920366287231, -0.5816864967346191, 0.09978470206260681, 0.5093519687652588, -0.8380708694458008, -0.39486002922058105, -0.368421345949173, 0.0529802143573761, -0.1251138150691986, 0.5583524107933044, -0.6834368705749512, 0.640137255191803, 0.7566877603530884, 0.19675755500793457, -1.1473106145858765, -0.09546105563640594, -1.0210657119750977, -0.036974549293518066, -0.22395499050617218, 0.9428473711013794, -0.44832101464271545, -0.07881534844636917, 0.8345752954483032, 0.31536799669265747, -0.1877071112394333, -0.7626842260360718, -0.43978241086006165, 0.15985475480556488, -0.3643644154071808, 0.4733007252216339, 0.055139247328042984, 0.28686875104904175, -0.05501259118318558, 0.31238773465156555, 0.6516010761260986, -0.44670528173446655, -0.5726483464241028, 0.5172369480133057, 0.07451522350311279, -0.36751610040664673, -0.3861393928527832, -0.06785930693149567, -0.8991692066192627, 0.17046606540679932, -1.1406244039535522, 0.5076798796653748, -0.8664366006851196, -0.3113275170326233, 0.27643534541130066, 0.0722852498292923, 0.30420413613319397, 0.4531748592853546, -0.33870452642440796, -0.48482635617256165, -0.40115973353385925, -0.37938418984413147, 0.9411926865577698, 0.9551088213920593, -0.8648332357406616, 0.23026597499847412, -0.31329116225242615, 0.07206785678863525, 0.09582501649856567, 0.5330809354782104, -0.505156934261322, -0.7316858768463135, -1.430280327796936, 0.20785443484783173, 0.024020833894610405, 0.019770776852965355, -0.7047373652458191, 0.1914113461971283, 0.11740349978208542, -0.4473610818386078, 0.5571005940437317, 0.09839297831058502, -0.8941465020179749, -0.41874390840530396, 0.7617964148521423, -0.7984001040458679, 0.5696647763252258, 0.2837275266647339, -0.5165871381759644, -0.2978099286556244, 0.661651074886322, -0.18292585015296936, -1.043426513671875, -0.23972205817699432, 0.27267688512802124, -1.1769680976867676, -0.14977099001407623, 0.2822819948196411, 0.01146333385258913, -0.9245983958244324, -0.31005027890205383, 0.2664339542388916, 0.4781075119972229, -0.2196730077266693, 0.955137312412262, 0.1669275313615799, -0.8433724641799927, 0.24748806655406952, 0.26010021567344666, 0.2843497693538666, -0.4031382203102112, -0.002801364753395319, 0.23981313407421112, -0.6614189743995667, 0.6360260844230652, 0.13363485038280487, 0.6287941932678223, -0.959533154964447, -0.17249001562595367, 0.7450308799743652, -0.44954001903533936, 0.11979278922080994, 0.8946385979652405, -0.7898242473602295, -1.7580347061157227, 0.1096302792429924, -1.2708353996276855, -0.568601667881012, -1.0128779411315918, 0.4971082806587219, -0.15122941136360168, -0.24489913880825043, -0.22077491879463196, -0.3521823287010193, 0.010271980427205563, 0.030863743275403976, -0.4828451871871948, 0.6391382813453674, 0.0704706460237503, -0.4597519636154175, 0.8447093367576599, 0.41936734318733215, -0.4932655990123749, -0.45261746644973755, 0.010811244137585163, -0.23439615964889526, 0.0711713507771492, 0.15732373297214508, -0.6156018376350403, -0.3545636236667633, 0.656834065914154, 0.35767310857772827, 0.3570423424243927, -0.3391215205192566, -0.1383230984210968, 0.001540656085126102, 0.5708052515983582, 0.6068713665008545, -0.8304731845855713, -0.5533275604248047, 1.1871933937072754, 1.5734994411468506, -0.9530665874481201, 0.2254256159067154, -0.42488300800323486, -0.8338208198547363, 0.8651877045631409, 0.7000970840454102, 0.40949469804763794, 0.4883589446544647, -0.10305119305849075, 0.6995478272438049, 0.04272871091961861, -0.7352483868598938, -0.011233569122850895, 0.3039468228816986, 0.8596745133399963, 1.225817084312439, 0.6297961473464966, -0.15308663249015808, 0.5010356903076172, 0.09396132826805115, 0.4751717746257782, 0.664344310760498, 1.1122634410858154, 0.03163352608680725, -0.47595158219337463, 0.039547115564346313, 0.5759731531143188, -0.040743641555309296, -0.41153234243392944, -0.19186893105506897, 0.9724748730659485, 0.2514687478542328, 0.5380163192749023, 0.3517254889011383, 0.31523188948631287, 0.5048763751983643, 0.06481313705444336, 0.43489018082618713, -0.8863040208816528, -0.5440940856933594, -0.48023855686187744, -0.3435571789741516, -0.1570511907339096, -0.3254234194755554, -0.2925594449043274, -0.4547005891799927, -0.1902540922164917, 0.2303374707698822, 0.4407413899898529, -0.014150960370898247, 1.3296899795532227, 0.39494648575782776, 0.3603953421115875, -0.5846838355064392, -0.4699189066886902, -0.6320230960845947, -1.214123249053955, -0.4056658446788788, -0.7388320565223694, -0.10703465342521667, -0.5096225142478943, -0.15810811519622803, -0.6239374876022339]}, "authors": [{"authorId": "41211611", "name": "Jinze Bai"}, {"authorId": "2247821453", "name": "Shuai Bai"}, {"authorId": "2247867894", "name": "Yunfei Chu"}, {"authorId": "2248072386", "name": "Zeyu Cui"}, {"authorId": "2247877609", "name": "Kai Dang"}, {"authorId": "2249717428", "name": "Xiaodong Deng"}, {"authorId": "2303618719", "name": "Yang Fan"}, {"authorId": "2054600625", "name": "Wenhang Ge"}, {"authorId": "2249483230", "name": "Yu Han"}, {"authorId": "152159016", "name": "Fei Huang"}, {"authorId": "151471590", "name": "Binyuan Hui"}, {"authorId": "2247869389", "name": "Luo Ji"}, {"authorId": "2223106060", "name": "Mei Li"}, {"authorId": "35996608", "name": "Junyang Lin"}, {"authorId": "2248039532", "name": "Runji Lin"}, {"authorId": "7415571", "name": "Dayiheng Liu"}, {"authorId": "2197482380", "name": "Gao Liu"}, {"authorId": "46655401", "name": "Chengqiang Lu"}, {"authorId": "1515662094", "name": "K. Lu"}, {"authorId": "47793076", "name": "Jianxin Ma"}, {"authorId": "47447639", "name": "Rui Men"}, {"authorId": "40936467", "name": "Xingzhang Ren"}, {"authorId": "19169659", "name": "Xuancheng Ren"}, {"authorId": "2111727840", "name": "Chuanqi Tan"}, {"authorId": "2110171536", "name": "Sinan Tan"}, {"authorId": "49365463", "name": "Jianhong Tu"}, {"authorId": "2155302144", "name": "Peng Wang"}, {"authorId": "2217429986", "name": "Shijie Wang"}, {"authorId": "2203795932", "name": "Wei Wang"}, {"authorId": "2247967520", "name": "Shengguang Wu"}, {"authorId": "2249082188", "name": "Benfeng Xu"}, {"authorId": "2110641662", "name": "Jin Xu"}, {"authorId": "143936592", "name": "An Yang"}, {"authorId": "2257352480", "name": "Hao Yang"}, {"authorId": "37081450", "name": "Jian Yang"}, {"authorId": "2243424858", "name": "Jian Yang"}, {"authorId": "2237626195", "name": "Shusheng Yang"}, {"authorId": "2153951714", "name": "Yang Yao"}, {"authorId": "2249451832", "name": "Bowen Yu"}, {"authorId": "48613402", "name": "Yu Bowen"}, {"authorId": "2114128654", "name": "Hongyi Yuan"}, {"authorId": "2112340945", "name": "Zheng Yuan"}, {"authorId": "2108091429", "name": "Jianwei Zhang"}, {"authorId": "2210472743", "name": "Xing Zhang"}, {"authorId": "29343468", "name": "Yichang Zhang"}, {"authorId": "2116702333", "name": "Zhenru Zhang"}, {"authorId": "2192678144", "name": "Chang Zhou"}, {"authorId": "2237981776", "name": "Jingren Zhou"}, {"authorId": "2141874108", "name": "Xiaohuan Zhou"}, {"authorId": "2248127832", "name": "Tianhang Zhu"}], "references": [{"paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce", "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"}, {"paperId": "c96297261467b5daa2d01227496a70d444602434", "title": "Baichuan 2: Open Large-scale Language Models"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9", "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"}, {"paperId": "24d52678c887331b9da0368e8a2f58bec07f7203", "title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf"}, {"paperId": "e2f1f04f648a8863d11439aa4c80ee65d6caccda", "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models"}, {"paperId": "819bbdc2dac9e13d9ca3e2508a6e063186ce5e40", "title": "YaRN: Efficient Context Window Extension of Large Language Models"}, {"paperId": "fc6a2f7478f68adefd69e2071f27e38aa1647f2f", "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"}, {"paperId": "0b0debb710366cdff461938c80763eace1651af6", "title": "Code Llama: Open Foundation Models for Code"}, {"paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7", "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"}, {"paperId": "40e0b9361d88b1879891eb6d16de110b30bf6c62", "title": "OctoPack: Instruction Tuning Code Large Language Models"}, {"paperId": "dd3fb89d1201d46fa80b6a9519114599c01c11ac", "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"}, {"paperId": "91206346edbe28abb606d7b3425cd455d4019d4f", "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models"}, {"paperId": "703035b483c181953de1b55b5fd59cd4cd4cf211", "title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework"}, {"paperId": "7a5b44ea10a51708e18786595c8d70b18950da11", "title": "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios"}, {"paperId": "41a2e7c079179ae94557d3198de674a16a5987a6", "title": "Refining ChatGPT-Generated Code: Characterizing and Mitigating Code Quality Issues"}, {"paperId": "c21bc84900ea40395173a23a5d3b5a8d2deefba5", "title": "Evaluating the Performance of Large Language Models on a Neurology Board-Style Examination"}, {"paperId": "19db2d61f20a6c439cc79f28ef4c9e4bf26cd20e", "title": "Preference Ranking Optimization for Human Alignment"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "3b6179c293df29e31d31cea46476f104ab6950f2", "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World"}, {"paperId": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6", "title": "CMMLU: Measuring massive multitask language understanding in Chinese"}, {"paperId": "454c8fef2957aa2fb13eb2c7a454393a2ee83805", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct"}, {"paperId": "07bfba1087176d862c953a55df389e3d5b3d38ac", "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences"}, {"paperId": "50f44ef10335d59cec145b15effae20ff22c1fdb", "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory"}, {"paperId": "0d1c76d45afa012ded7ab741194baf142117c495", "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"}, {"paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb", "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models"}, {"paperId": "e4f10c448aaea9cba800e8ed324c46f13725e952", "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers"}, {"paperId": "763eb8d43e2f8a5d9da26269a4985efd1c099a5b", "title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "a122863d239643453195424c04067e89406246e1", "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"}, {"paperId": "c3a59e1e405e7c28319e5a1c5b5241f9b340cf63", "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "236c7dafea3df7ecffb5f18ec780d12f2f27d4b0", "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"}, {"paperId": "9ada8fa11b1cdece31f253acae50b62df8d5f823", "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156", "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"}, {"paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"}, {"paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions"}, {"paperId": "d2b4b4804479714ad0e406e8ab73fa3e72069216", "title": "Phoenix: Democratizing ChatGPT across Languages"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "68c834c19cd126bbd6d25a3572d7205cfed76271", "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"}, {"paperId": "748698bd4387afd08594e0dc8150c2afa210d9ae", "title": "RRHF: Rank Responses to Align Language Models with Human Feedback without tears"}, {"paperId": "bafe023fb072045dc0cd50316382a61c8dcb9fae", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"}, {"paperId": "8fc90497d9043fdf35e71302b7c2e79bb907144f", "title": "Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases"}, {"paperId": "af5c7848417882012203ac21399977ebda695a2b", "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"}, {"paperId": "817e52b815560f95171d8fa60f78dd965e885a65", "title": "How well do Large Language Models perform in Arithmetic tasks?"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "53d128ea815bcc0526856eb5a9c42cc977cb36a7", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a", "title": "SantaCoder: don't reach for the stars!"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654", "title": "Constitutional AI: Harmlessness from AI Feedback"}, {"paperId": "d232d97761490828f20e9b77d2c91a135a7270ee", "title": "OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "663a41c866d49ce052801fbc88947d39764cad29", "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"}, {"paperId": "99832586d55f540f603637e458a292406a0ed75d", "title": "ReAct: Synergizing Reasoning and Acting in Language Models"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "593edb7f66bf6ec3eef943691c18aec9f976bc51", "title": "Code Translation with Compiler Representations"}, {"paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77", "title": "Solving Quantitative Reasoning Problems with Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "5437e8adab596d7294124c0e798708e050e25321", "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"}, {"paperId": "5922f437512158970c417f4413bface021df5f78", "title": "A Generalist Agent"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da", "title": "InCoder: A Generative Model for Code Infilling and Synthesis"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "d3dd80269f2542cc173afb3a1df24b582a1e4af2", "title": "Overcoming a Theoretical Limitation of Self-Attention"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e", "title": "A General Language Assistant as a Laboratory for Alignment"}, {"paperId": "92173d081b15824d22a9ef070e118744ceee8052", "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models"}, {"paperId": "cbf98ebe967e0f3f3236e7932f37013b98244e94", "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "39d05ffbc06fdca54ea6a90cd6d7fca202809aaa", "title": "Understanding Dataset Difficulty with V-Usable Information"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "df6f3c607ae3956db722f76f3f81e672c3dfa803", "title": "CodeQA: A Question Answering Dataset for Source Code Comprehension"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "6dd1a0f657ff547b1736c2d8b30451f49a9a48f5", "title": "OCNLI: Original Chinese Natural Language Inference"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1", "title": "Learning to summarize from human feedback"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6", "title": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "678fd7c48efe21434148b4b3482c2b8b3ee618fc", "title": "Deep Neural Solver for Math Word Problems"}, {"paperId": "dce6f9d4017b1785979e7520fd0834ef8cf02f4b", "title": "Proximal Policy Optimization Algorithms"}, {"paperId": "5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd", "title": "Deep Reinforcement Learning from Human Preferences"}, {"paperId": "88caa4a0253a8b0076176745ebc072864eab66e1", "title": "Language Modeling with Gated Convolutional Networks"}, {"paperId": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": null, "title": "Falcon-40B: An open large language model with state-of-the-art performance, 2023"}, {"paperId": null, "title": "InternLM: A multilingual language model with progressively enhanced capabilities"}, {"paperId": null, "title": "OpenCompass: A universal evaluation platform for foundation models, 2023"}, {"paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"}, {"paperId": "7ca954844bc1dd405bc43445b1c990e42d865095", "title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Scale Language Model Society"}, {"paperId": "ad97671a924a9b3a060fee857e561f140ec79dd7", "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents"}, {"paperId": "4972b88f8f324a4fa18e921f62a9857af2b5fc7b", "title": "Crosslingual Generalization through Multitask Finetuning"}, {"paperId": null, "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data"}, {"paperId": null, "title": "MOSS: Training conversational language models from synthetic data, 2023a"}, {"paperId": null, "title": "How far can camels go?"}, {"paperId": null, "title": "AutoGPT: The heart of the open-source agent ecosystem"}, {"paperId": null, "title": "An important next step on our AI journey"}, {"paperId": null, "title": "Stanford Alpaca: An instruction-following LLaMA model"}, {"paperId": null, "title": "Baichuan-7B: A large-scale 7B pretraining language model developed by BaiChuan-Inc"}, {"paperId": null, "title": "ChatGLM2-6B: An open bilingual chat LLM"}, {"paperId": null, "title": "Hugging Face. Transformers agents"}, {"paperId": null, "title": "Raising the bar for open-source foundation models"}, {"paperId": null, "title": "Free Dolly: Introducing the world's first truly open instruction-tuned LLM"}, {"paperId": null, "title": "Evaluation benchmark for code intepreter"}, {"paperId": null, "title": "Alpaca-CoT: An instruction-tuning platform with unified interface of instruction collection, parameter-efficient methods, and large language models"}, {"paperId": null, "title": "Evaluation benchmark for tool usage through ReAct prompting, 2023b"}, {"paperId": null, "title": "The magical effect of the Bias term: RoPE + Bias = better length extrapolation, 2023b. URL https://spaces.ac.cn/archives/9577"}, {"paperId": null, "title": "Improving transformer: Length extrapolation ability and position robustness"}, {"paperId": null, "title": "tiktoken: A fast BPE tokeniser for use with OpenAI\u2019s models, 2022"}, {"paperId": null, "title": "The Thirty-Second Innovative Applications of Artificial Intelligence Conference"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": null, "title": "SocialIQA: Commonsense reasoning about social interactions"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "c8c4ab59ac29973a00df4e5c8df3773a3c59995a", "title": "Searching for Activation Functions"}, {"paperId": null, "title": "Attention is all you need. Advances in neural information processing systems"}, {"paperId": null, "title": "Layer normalization. CoRR"}, {"paperId": null, "title": "Joining various clubs and extracurricular activities to broaden his horizons and social circle"}, {"paperId": null, "title": "Qwen-14B-Chat (RLHF) \u53ef\u4ee5\u4f7f\u7528Python\u7684re\u6a21\u5757\u6765\u5b9e\u73b0\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u3002\u4ee5\u4e0b\u662f\u4e00 \u4e2a\u793a\u4f8b\u4ee3\u7801\uff1a"}, {"paperId": null, "title": "NTK-aware scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation"}, {"paperId": null, "title": "Striving to obtain scholarships and other honors"}, {"paperId": null, "title": "Studying professional knowledge and skills seriously and striving for excellent grades"}, {"paperId": null, "title": "Generative ai for math"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality"}, {"paperId": null, "title": "SFT) \u4f60\u53ef\u4ee5\u4f7f\u7528Python\u7684re\u6a21\u5757\u6765\u5b9e\u73b0\u8fd9\u4e2a\u6b63\u5219\u8868\u8fbe\u5f0f\u5339\u914d\u3002\u4ee5 \u4e0b\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff1a import re def match phone numbers(phone numbers): pattern = r '\u02c6139d9$' return [phone number for phone number in phone numbers if re"}]}