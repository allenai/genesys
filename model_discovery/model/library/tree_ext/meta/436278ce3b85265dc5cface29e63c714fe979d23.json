{"paperId": "436278ce3b85265dc5cface29e63c714fe979d23", "title": "LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models", "abstract": "The Transformer architecture is ubiquitously used as the building block of large-scale autoregressive language models. However, finding architectures with the optimal trade-off between task performance (perplexity) and hardware constraints like peak memory utilization and latency is non-trivial. This is exacerbated by the proliferation of various hardware. We leverage the somewhat surprising empirical observation that the number of decoder parameters in autoregressive Transformers has a high rank correlation with task performance, irrespective of the architecture topology. This observation organically induces a simple Neural Architecture Search (NAS) algorithm that uses decoder parameters as a proxy for perplexity without need for any model training. The search phase of our training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of perplexity versus any hardware performance cost. We evaluate LTS on diverse devices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer backbones: GPT-2 and Transformer-XL. Results show that the perplexity of 16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster runtime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero and one-shot settings, LTS Pareto-frontier models achieve higher average accuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x lower latency. LTS extracts the Pareto-frontier in under 3 hours while running on a commodity laptop. We effectively remove the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling.", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 9, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The search phase of this training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs and effectively removes the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling."}, "embedding": {"model": "specter_v2", "vector": [0.32720690965652466, 0.6715742349624634, -0.08524288982152939, -0.07047321647405624, -0.28263962268829346, -0.03576267510652542, 0.31522780656814575, -0.7520118355751038, -0.39470747113227844, -0.5667346715927124, 0.659963846206665, -0.2985415756702423, 0.4586954116821289, 0.26467442512512207, 0.30764448642730713, 0.08879122883081436, -0.5992724299430847, 0.39897653460502625, -0.11331618577241898, -0.49870753288269043, -0.1983957588672638, -0.39995160698890686, -1.1806567907333374, -0.1370997428894043, 0.07645352929830551, 1.0870307683944702, 0.2555580735206604, 0.6892601251602173, -0.5104051828384399, 0.25990310311317444, 0.5964238047599792, -0.29686567187309265, 0.18196149170398712, 0.14309674501419067, -0.14971385896205902, -0.2524292767047882, 0.5898265838623047, -0.26943323016166687, -0.40086475014686584, 0.923288106918335, 0.013517181389033794, 0.07139965891838074, 0.45320823788642883, -0.6716678738594055, -0.058916687965393066, 0.38327792286872864, 0.8699532151222229, 0.6434964537620544, -0.733341634273529, -0.30294710397720337, 1.4215599298477173, -1.3545395135879517, -0.16038581728935242, 1.494278073310852, 0.625623881816864, 0.1699122190475464, 0.010441024787724018, -0.8852106928825378, 0.6839389204978943, -0.27619069814682007, -0.796650767326355, -0.7313902378082275, 0.07995407283306122, -0.15186649560928345, 1.778414249420166, -0.45033255219459534, 0.05581694468855858, 0.76300448179245, 0.3593858778476715, 1.1714521646499634, -0.03173123672604561, -0.5372375845909119, 0.07409825921058655, -0.385383278131485, 0.4484764337539673, 0.8830839991569519, -0.4502210021018982, 0.04475347325205803, -0.8642435073852539, -0.20498208701610565, 0.4294222295284271, -0.20673659443855286, 0.17927880585193634, -0.26408320665359497, -0.0069417282938957214, 0.9154980778694153, 0.38727855682373047, 0.7038406729698181, -0.4108697175979614, 0.8321879506111145, 0.5334462523460388, 0.35703226923942566, 0.21977107226848602, 0.5954972505569458, -0.05822626128792763, 0.5219541788101196, -1.0626260042190552, 0.05041937157511711, -0.07010304927825928, 0.8884837031364441, -0.34830406308174133, 1.0112848281860352, -0.8159893751144409, 0.36812302470207214, 1.4746617078781128, 0.3373406231403351, 0.61961430311203, -0.933553159236908, 0.19401536881923676, -0.6625164151191711, -0.2268557995557785, -0.2856901288032532, -0.4203430712223053, -0.48665177822113037, -0.7630573511123657, -1.0049344301223755, -0.6988102197647095, 0.17418353259563446, -0.8109512329101562, 0.4706587493419647, -0.3869018256664276, 0.27908387780189514, 0.01754775643348694, 0.3367517590522766, 0.34603264927864075, 0.5167659521102905, 0.15869835019111633, 0.16807186603546143, 1.1799907684326172, -1.250529408454895, -0.5067949891090393, -1.0831849575042725, 0.6445748805999756, -0.20805154740810394, 0.49033501744270325, 0.1162533238530159, -1.227677822113037, -0.6176731586456299, -0.8907331824302673, -0.144500270485878, -0.28507116436958313, 0.37382590770721436, 0.9865706562995911, 0.8593838810920715, -1.4595650434494019, 0.5230542421340942, -0.2279203087091446, -0.03000938706099987, 0.39148643612861633, 0.30531182885169983, 0.49396687746047974, -0.08052566647529602, -0.8085880279541016, 0.0945739895105362, 0.08682677149772644, -0.4658251106739044, -0.27858707308769226, -0.5505870580673218, -0.713492751121521, 0.19234733283519745, 0.010936280712485313, -0.562813937664032, 1.1598566770553589, -0.1741560995578766, -1.83784818649292, 0.5725654363632202, -0.18779711425304413, -0.05818590149283409, -0.2321668118238449, -0.30195504426956177, -0.6607027053833008, -0.5124413967132568, -0.5111979246139526, 0.5052987933158875, 0.7988615036010742, 0.34480810165405273, 0.008078114129602909, 0.174169659614563, -0.5583371520042419, -0.0871051624417305, -0.5433512330055237, 1.1711225509643555, -0.4961465299129486, -0.2922618091106415, 0.3567443788051605, 0.2611752152442932, -0.2694268524646759, -0.25436916947364807, -0.3853781223297119, -0.928516685962677, 0.4081907272338867, -0.008536238223314285, 1.1445648670196533, -0.9163799285888672, -0.8231413960456848, 0.06525535881519318, 0.23983243107795715, -0.0495653860270977, -0.9991357922554016, 0.2185061275959015, -0.29991352558135986, 0.19732077419757843, 0.1637929379940033, -1.2517657279968262, 0.20476488769054413, -0.35146933794021606, -0.988971471786499, -0.20857495069503784, -0.08392840623855591, 0.867561399936676, -0.47106924653053284, 0.06561894714832306, -0.2977672815322876, 0.47402894496917725, -0.9769670963287354, 1.0760446786880493, -0.37422245740890503, 0.08924447745084763, -0.3435108959674835, -0.011504722759127617, -0.17144779860973358, -0.40160664916038513, 0.5625544786453247, -0.2614944577217102, -0.16997355222702026, 0.6540358662605286, 0.01618589274585247, 1.5882717370986938, -0.5517339706420898, 0.6742516160011292, -0.0816597193479538, -0.40275123715400696, 0.7270261645317078, 0.35937735438346863, -0.3452966809272766, -1.0203466415405273, 0.5565888285636902, 0.7981100082397461, -0.44950318336486816, 0.6624916195869446, 0.8371666669845581, 1.3970580101013184, -0.3142840266227722, 0.087816521525383, 0.6368885636329651, -0.2083667665719986, 0.06417205929756165, 0.004724100697785616, 0.9579792618751526, 0.1493355631828308, 0.047218985855579376, -0.31898778676986694, 0.13247308135032654, -1.0341224670410156, -0.09871958196163177, 0.3672553300857544, 0.6680057644844055, 0.7197070121765137, 0.20598410069942474, -0.8537982702255249, -0.5290796160697937, -0.22397586703300476, 0.8351255655288696, 1.5930274724960327, -0.7433845400810242, -0.15491390228271484, -0.7156323790550232, 0.10666114836931229, -0.776622474193573, 0.028616279363632202, 0.17758500576019287, 0.17466440796852112, -0.8248271942138672, -0.7810877561569214, 1.073282241821289, -0.2500014901161194, 0.8961203694343567, -0.4968772232532501, -0.6152496337890625, -0.43229734897613525, 0.33165669441223145, -1.0379027128219604, -0.6233407258987427, 0.009577923454344273, -0.7755028009414673, 0.264926552772522, -0.1646091192960739, 0.11768221855163574, -0.20814871788024902, -0.7508607506752014, 1.094443678855896, -0.626665472984314, -0.07105474919080734, 0.18514898419380188, 0.8040338158607483, -0.6148120164871216, -0.5086507797241211, 0.3964911103248596, 0.26450061798095703, -0.19228772819042206, 0.09576782584190369, -0.04308546334505081, -0.09740158915519714, -0.30150869488716125, 0.35609352588653564, 0.037790924310684204, 0.16523785889148712, -0.2576431632041931, 0.8026110529899597, -0.5465807318687439, -0.25532597303390503, -0.815022885799408, 1.2448135614395142, 0.20345865190029144, -0.9258739948272705, 0.48825693130493164, -0.8433789610862732, 0.025057384744286537, 0.4109780192375183, -0.31814441084861755, -0.09498878568410873, -0.9860895276069641, 0.2256418913602829, -0.9327263832092285, -0.07350679486989975, 0.05254595726728439, 0.5914270877838135, 0.2641621530056, 0.21407727897167206, 0.4382268190383911, 0.22992083430290222, -0.27413415908813477, 0.3941980302333832, -0.8859160542488098, 0.5971694588661194, 0.39893290400505066, -0.13020265102386475, -0.34978991746902466, -0.010229449719190598, -0.8742185831069946, -0.28363144397735596, -0.14556606113910675, 0.09767629951238632, -0.31047070026397705, 0.29004397988319397, -0.6999173164367676, -0.7623293399810791, -0.2036927342414856, -1.1549763679504395, -0.13382317125797272, 0.5056864023208618, -0.19806428253650665, -0.11369358003139496, -1.4776623249053955, -0.943656325340271, -0.9006240367889404, -1.3859162330627441, -1.3321963548660278, 0.7324491739273071, -0.1713722050189972, -0.3780257999897003, -0.40952715277671814, 0.12839649617671967, -0.4954504370689392, 1.211733102798462, -0.8326980471611023, 0.8576571345329285, -0.2648458778858185, -0.3074384331703186, 0.05444496497511864, 0.2065351903438568, 0.34275877475738525, -0.7346338629722595, 0.193783238530159, -1.0568301677703857, 0.26770344376564026, -0.5347208976745605, -0.2574390470981598, -0.022144056856632233, 0.6234467029571533, 0.7015969753265381, -0.08198070526123047, -0.28255918622016907, 0.675450325012207, 1.1397552490234375, -0.6229926347732544, 0.3063182532787323, 0.04525764286518097, 1.083573341369629, -0.026863504201173782, -0.7050312757492065, 0.41249847412109375, 0.2739529013633728, 0.7321982383728027, 0.12170194834470749, 0.046112511307001114, -0.29178228974342346, -0.3812456429004669, 0.522675633430481, 1.7722492218017578, 0.4213620722293854, -0.13441292941570282, -1.0435726642608643, 0.30108174681663513, -0.9615976214408875, -0.2832978367805481, 0.44394683837890625, 0.8497254252433777, -0.013853770680725574, -0.27754735946655273, -0.21821942925453186, -0.416162371635437, 0.1391027867794037, 0.42587170004844666, -0.2349424958229065, -1.2390018701553345, 0.03718288242816925, 0.8026204109191895, 0.2814333736896515, 0.2645627558231354, -0.24051271378993988, 0.6706397533416748, 14.746639251708984, 0.9317297339439392, -0.36190342903137207, 0.6725999116897583, 0.8093675374984741, 0.06909976154565811, -0.3391488194465637, -0.3346599042415619, -1.1969709396362305, 0.08487048745155334, 1.700935959815979, 0.26283419132232666, 0.6184937357902527, 0.4501675069332123, 0.06955458968877792, 0.7596265077590942, -0.33279725909233093, 0.963887631893158, 0.298971951007843, -1.4509772062301636, 0.3876183331012726, 0.31978073716163635, 0.11775573343038559, 1.006739616394043, 0.9453208446502686, 0.7913705110549927, 0.5978968143463135, -0.7043496370315552, 0.5645243525505066, 0.239109069108963, 0.9281949996948242, 0.03426303714513779, -0.056363239884376526, 0.3117779791355133, -0.8487875461578369, -0.1768467277288437, -0.39635440707206726, -1.1150093078613281, 0.0734962746500969, 0.1336204707622528, -0.5695290565490723, -0.6796687841415405, -0.08018665760755539, 0.3125823736190796, 0.2844977080821991, 0.3450068235397339, -0.1704852133989334, 0.8088085055351257, -0.4373849034309387, -0.0929204523563385, 0.5436444282531738, 0.21591348946094513, 0.08255879580974579, -0.19909195601940155, 0.40825384855270386, -0.16266277432441711, 0.029207482933998108, 0.36500680446624756, -0.688393771648407, -0.03576758876442909, -0.3825816810131073, -0.43791016936302185, 0.20712816715240479, 0.6844178438186646, 0.3897707462310791, 0.5982680320739746, -0.12169773131608963, 0.2691100835800171, 0.8131774067878723, 0.26703065633773804, -0.15168939530849457, 0.26715829968452454, 0.5089809894561768, -0.809286892414093, 0.005264932755380869, 0.2658502459526062, -0.17821964621543884, -0.35542818903923035, -0.7503474354743958, -0.6471316814422607, 0.25517886877059937, -1.1213785409927368, -0.4415958821773529, 1.1943594217300415, -0.5164545178413391, 0.06781592220067978, 0.1770477443933487, -0.9586542248725891, -0.3046453893184662, 0.45325130224227905, -1.1846389770507812, -0.7494418621063232, 0.5623283386230469, -0.2272830456495285, 0.0836486965417862, -0.4685833156108856, 1.404093861579895, 0.17118717730045319, -0.6258554458618164, 0.2088353931903839, -0.007257345598191023, -0.30444392561912537, -0.5638510584831238, -0.36789754033088684, 1.067548155784607, 0.32866960763931274, 0.01438720989972353, -0.1379665732383728, 0.06403438001871109, 0.35880163311958313, -0.8348830342292786, 0.07761850953102112, 0.6502172350883484, -0.5978595018386841, 0.0855836421251297, -0.9376389980316162, -0.5601293444633484, 0.3451397120952606, 0.4230646789073944, -0.5380933284759521, 0.09413114190101624, 0.34681496024131775, -0.8735364079475403, 0.04819941893219948, -0.10424421727657318, 0.36578476428985596, 0.6566309332847595, -0.7561493515968323, 0.038093771785497665, -0.02097698487341404, 0.7364422678947449, -0.9025896787643433, -0.26325926184654236, -0.21571679413318634, 0.47614550590515137, -0.22310012578964233, 0.9803217649459839, -0.2539721429347992, 0.6378005743026733, 1.2723276615142822, -0.2168933004140854, -0.6643896102905273, 0.07899457216262817, -0.9861810803413391, 0.17576606571674347, -0.2570391297340393, 0.7369028329849243, -0.6910735964775085, 0.3825777471065521, 0.7392998337745667, -0.22124509513378143, -0.5066277980804443, -0.9520330429077148, -0.37110447883605957, -0.13546745479106903, -0.5152180790901184, 0.4854419231414795, -0.2314944714307785, -0.49655765295028687, 0.34463801980018616, 0.3288787007331848, 0.5634711980819702, -0.3362727165222168, -0.8034664392471313, 0.1317521035671234, -0.25025516748428345, -0.026536205783486366, -0.9772727489471436, -0.31298884749412537, -1.3232444524765015, 0.22676776349544525, -0.9161037802696228, -0.16311125457286835, -0.6397562026977539, -0.9749665856361389, -0.27220794558525085, -0.06634511053562164, 0.08669978380203247, 0.8727692365646362, -0.5270939469337463, -0.43325844407081604, -0.1715414673089981, -0.4790757894515991, 0.9275772571563721, 0.5347793102264404, -0.5969952344894409, 0.0400298535823822, -0.07827281951904297, 0.4149717688560486, 0.4761183559894562, 0.5257698893547058, -0.20148712396621704, -1.143358826637268, -1.6358426809310913, 0.3923572897911072, -0.1753835678100586, -0.28660792112350464, -0.8353971838951111, 0.7361123561859131, 0.6179225444793701, -0.7100924253463745, 0.3411064147949219, 0.5948315858840942, -0.8289786577224731, -0.3445778787136078, 0.17594407498836517, -0.6089909076690674, 0.8155902028083801, 0.26891079545021057, -0.6808444261550903, 0.049247220158576965, 0.7033836245536804, -0.3359145522117615, -0.6067489981651306, -0.8711833953857422, 0.3952188491821289, -0.5707192420959473, 0.10208650678396225, -0.719382107257843, -0.3216138780117035, -1.271088719367981, -0.11412250250577927, 0.08750423789024353, 0.2815074324607849, -0.5746408104896545, 0.8920573592185974, 0.6165340542793274, -0.8940857648849487, 0.026475699618458748, 0.2965823709964752, -0.19976051151752472, -0.5050843954086304, 0.4221043288707733, 0.5717785358428955, -0.43840524554252625, 0.8326829075813293, 0.0431935079395771, 0.29578784108161926, -0.9057751893997192, -0.272656112909317, 0.9058022499084473, -0.7980120182037354, -0.15920211374759674, 1.087907314300537, -0.11137697845697403, -1.0116376876831055, 0.1518382579088211, -1.028817057609558, -0.43767791986465454, -0.5598351955413818, 0.3709145784378052, 0.09925327450037003, 0.25312918424606323, 0.056793324649333954, -0.4493940472602844, -0.08074286580085754, 0.06273909658193588, -0.5917001366615295, 0.19574283063411713, 0.15618087351322174, -0.2498595267534256, 0.41255348920822144, 0.818214476108551, -0.9788876175880432, -0.41667476296424866, -0.7903398275375366, -0.22580167651176453, 0.32576605677604675, 0.7477355599403381, 0.05876008793711662, -0.7681533694267273, 0.8508886098861694, 0.8788344264030457, -0.08171585947275162, 0.3807013928890228, -0.39715415239334106, 0.04324811324477196, 0.7004433870315552, 0.23999351263046265, -0.5390431880950928, -0.43170875310897827, 1.6902128458023071, 1.2503029108047485, -0.5160776376724243, 0.25061020255088806, -0.3433968424797058, -0.5124765038490295, 0.6980712413787842, 0.11116737872362137, -0.10098450630903244, 0.8910683393478394, 0.036966945976018906, -0.005379341077059507, -0.3131950795650482, -1.1501647233963013, -0.2826823592185974, 0.8878712058067322, 0.7231540083885193, 0.8430896401405334, 0.26993128657341003, 0.2556559145450592, 0.49405521154403687, -0.3982871174812317, -0.04210580512881279, 0.03732979670166969, 0.3367239534854889, -0.1736781895160675, 0.18246212601661682, 0.04133583605289459, 0.604615330696106, -0.44309601187705994, -1.1480695009231567, 0.3596039116382599, 0.3760458827018738, -0.2551857531070709, 0.4162377417087555, 1.198727011680603, 0.14625908434391022, 0.6131362318992615, 0.22992214560508728, 0.41086193919181824, -0.34599825739860535, -0.13507069647312164, 0.03809982165694237, -0.33567339181900024, -0.31794223189353943, 0.048237551003694534, -0.21499091386795044, -0.2087315320968628, -0.21237677335739136, 0.6154112815856934, -0.26538383960723877, 0.5292730331420898, 1.0341312885284424, 0.8861561417579651, 0.6401170492172241, -0.39821675419807434, -0.5397669672966003, -0.32869842648506165, -0.5388539433479309, 0.07559230178594589, -1.1661523580551147, -0.39303168654441833, -0.19926415383815765, 0.17532704770565033, -0.5622601509094238]}, "authors": [{"authorId": "51900416", "name": "Mojan Javaheripi"}, {"authorId": "144977605", "name": "Gustavo de Rosa"}, {"authorId": "2153292652", "name": "Subhabrata Mukherjee"}, {"authorId": "47973411", "name": "S. Shah"}, {"authorId": "4047530", "name": "T. L. Religa"}, {"authorId": "2157424631", "name": "C. C. T. Mendes"}, {"authorId": "121645690", "name": "S\u00e9bastien Bubeck"}, {"authorId": "3018662", "name": "F. Koushanfar"}, {"authorId": "1780951", "name": "Debadeepta Dey"}], "references": [{"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "28ed0086dd0f51a8965f7e952b6ee933cdf44179", "title": "Training-free Transformer Architecture Search"}, {"paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca", "title": "DeepNet: Scaling Transformers to 1, 000 Layers"}, {"paperId": "9202a718ce05395b6e17d5301e3a2e8b1021f31b", "title": "Prune Once for All: Sparse Pre-Trained Language Models"}, {"paperId": "2d4f66046bb436864cd6bf589e3a931c405f9f44", "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers"}, {"paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2", "title": "Primer: Searching for Efficient Transformers for Language Modeling"}, {"paperId": "de1fdaf92488f2f33ddc0272628c8543778d0da9", "title": "Scaling Laws for Neural Machine Translation"}, {"paperId": "4a884bd0b69851b4eff4bbc2ce0bdd7458f5298d", "title": "Analyzing and Mitigating Interference in Neural Architecture Search"}, {"paperId": "ef18db2a18ac61e72783a613328842ce86ef00bf", "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models"}, {"paperId": "47354f49a4768719add414ea853977cb868faf25", "title": "AutoBERT-Zero: Evolving BERT Backbone from Scratch"}, {"paperId": "d645bd08fc19d52164695f9cd5ae863345459a06", "title": "AutoFormer: Searching Transformers for Visual Recognition"}, {"paperId": "1a57318be32b740aef1d9b2070db6c0cc565ab0a", "title": "Memory-Efficient Differentiable Transformer Architecture Search"}, {"paperId": "47a941fcbb267726655c8df70aa376072ca2e0ee", "title": "Bag of Baselines for Multi-objective Joint Neural Architecture Search and Hyperparameter Optimization"}, {"paperId": "6dc84d38f6d8d964456b127d6f45c5b4de73bb86", "title": "Zero-Cost Proxies for Lightweight NAS"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "9ed4321e552d069ff6aa6f88480809e23927131d", "title": "Surrogate NAS Benchmarks: Going Beyond the Limited Search Spaces of Tabular NAS Benchmarks"}, {"paperId": "faef3261446023bed7e59fda75a0b819986903bd", "title": "NAS-Bench-301 and the Case for Surrogate Benchmarks for Neural Architecture Search"}, {"paperId": "ffb0bbe26f1cbc0ab22ef34784248f2dcd3a5e5c", "title": "Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition"}, {"paperId": "ea614fce53597c9996f8926807a0702d776547a4", "title": "Evaluating Efficient Performance Estimators of Neural Architectures"}, {"paperId": "b91f161bde9756d184f1b5640721e801fa67201e", "title": "Reproducible and Efficient Benchmarks for Hyperparameter Optimization of Neural Machine Translation Systems"}, {"paperId": "3b0fb765716ef6861a84abffcbe40643857c613b", "title": "Pruning neural networks without any data by iteratively conserving synaptic flow"}, {"paperId": "25c371d565b387dbf22207a954a9549557698c21", "title": "Neural Architecture Search without Training"}, {"paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "c114ce10c4a315d92c3815f54bc9893e7e6ef182", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "69599593f93023e2f91ef6673ee9860f85777d98", "title": "NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "7823292e5c4b05c47af91ab6ddf671a0da709e82", "title": "Once for All: Train One Network and Specialize it for Efficient Deployment"}, {"paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b", "title": "WinoGrande"}, {"paperId": "4ebce2425e231031f89a4a68dc52a151cd735d03", "title": "PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search"}, {"paperId": "6f561c0a27827e699db0353a0caa9075a03e9701", "title": "Efficient Forward Architecture Search"}, {"paperId": "9c919f435d6a27706e1c55ca180b96ee7c122609", "title": "A Survey on Neural Architecture Search"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "bc789aef715498e79a74f857fa090ece9e383bf1", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d", "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "77e5aa8c33a9cb9dd3f0874b09fd84389360e88e", "title": "Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "fe9b8aac9fa3bfd9724db5a881a578e471e612d7", "title": "Efficient Neural Architecture Search via Parameter Sharing"}, {"paperId": "d4d3008262697d379d0cb1642e39a8e0c756ab2c", "title": "Faster gaze prediction with dense networks and Fisher pruning"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "5d833331b0e22ff359db05c62a8bca18c4f04b68", "title": "One billion word benchmark for measuring progress in statistical language modeling"}, {"paperId": null, "title": "Nas-bert"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Neural architecture search: A survey"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?"}, {"paperId": null, "title": "c) Did you report error bars"}, {"paperId": null, "title": "Hugging Face"}, {"paperId": null, "title": "Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content"}, {"paperId": null, "title": "(a) Did you state the full set of assumptions of all theoretical results"}, {"paperId": null, "title": "data splits, hyperparameters, how they were chosen)? [Yes] See Appendix B (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times"}, {"paperId": null, "title": "Transformer-xl for pytorch"}, {"paperId": null, "title": "Codebase for open pre-trained transformers"}, {"paperId": null, "title": "a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The source code for LTS"}, {"paperId": null, "title": "Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?"}, {"paperId": null, "title": "3, 5, and Appendix E (c) Did you discuss any potential negative societal impacts of your work?"}, {"paperId": null, "title": "data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Appendix B (b) Did you mention the license of the assets?"}]}