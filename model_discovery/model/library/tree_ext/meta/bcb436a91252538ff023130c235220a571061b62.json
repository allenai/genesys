{"paperId": "bcb436a91252538ff023130c235220a571061b62", "title": "SUBFORMER: A PARAMETER REDUCED TRANS-", "abstract": "The advent of the Transformer can arguably be described as a driving force behind many of the recent advances in natural language processing. However, despite their sizeable performance improvements, as recently shown, the model is severely over-parameterized, being parameter inefficient and computationally expensive to train. Inspired by the success of parameter-sharing in pre-trained deep contextualized word representation encoders, we explore parameter-sharing methods in Transformers, with a specific focus on encoder-decoder models for sequence-to-sequence tasks such as Machine Translation. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer, a parameter efficient Transformer-based model which combines the newly proposed Sandwich-style parameter sharing technique and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization, and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters. On the WMT\u201914 English-German test set, we show we can perform equally well, and even sometimes outperform (+0.1 BLEU score) the Transformer-base model while using 40% fewer parameters. We also perform equally well as Transformer-big with 40% fewer parameters, achieve performance within 0.1 BLEU with 70% fewer parameters, and outperform the model by 0.7 BLEU with 12M fewer parameters. We also outperform the standard Transformer-XL model, achieving a significant 3.6 lower perplexity with 37% fewer parameters.1", "venue": "", "year": 2020, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "The Subformer is developed, a parameter efficient Transformer-based model which combines the newly proposed Sandwich-style parameter sharing technique and self-attentive embedding factorization (SAFE), and experiments show that the Subformer can outperform the Transformer even when using significantly fewer parameters."}, "embedding": {"model": "specter_v2", "vector": [0.369467556476593, 0.5756388902664185, -0.5500636696815491, 0.015940485522150993, -0.5113376975059509, -0.4429313838481903, 0.4825924336910248, -0.4011577367782593, -0.4949735999107361, -0.038136713206768036, 1.1998684406280518, -0.043640393763780594, 0.22786638140678406, 0.1927245855331421, 0.013059264980256557, -0.02620328776538372, -0.6666179299354553, 0.3349723517894745, -0.06268275529146194, -0.626567542552948, -0.2737996280193329, -0.7537402510643005, -0.2173418402671814, 0.05966980382800102, 0.42278939485549927, 0.14920513331890106, 0.38794782757759094, 0.5986146926879883, -0.20317380130290985, -0.15126651525497437, 0.3718411326408386, -0.24621054530143738, 0.41065531969070435, -0.058420032262802124, -0.24091996252536774, 0.09248217940330505, 0.11432263255119324, -0.6454540491104126, -0.24098803102970123, 0.7097815871238708, -0.10369795560836792, 0.21668429672718048, 0.40326452255249023, -0.6707356572151184, -0.05909429490566254, 1.2932575941085815, 0.850066602230072, 0.5440891981124878, -0.2548338770866394, -0.7786946296691895, 1.5982609987258911, -1.5036965608596802, 0.20991982519626617, 1.2152247428894043, 0.3742024302482605, 0.39469313621520996, -0.2152901440858841, -0.49392083287239075, 0.4848076403141022, 0.021581536158919334, -0.939524233341217, -0.29887640476226807, -0.02355870231986046, 0.19356118142604828, 2.1416542530059814, -0.3878972828388214, 0.376064658164978, 0.4665403366088867, -0.1283322125673294, 0.8341622948646545, -0.1561192274093628, -0.327370822429657, -0.5927245616912842, -0.12624616920948029, 0.23690474033355713, 1.0118606090545654, -0.2792515754699707, 0.1013556644320488, -0.9137635231018066, -0.10356982797384262, 0.4157866835594177, 0.21222789585590363, -0.1354515254497528, -0.3738435208797455, -0.7497161030769348, 0.6277468204498291, 0.23048661649227142, 0.6017900705337524, 0.03718489035964012, 0.798912525177002, 0.6803006529808044, 0.5645268559455872, 0.533678412437439, 0.6237187385559082, -0.37122365832328796, 0.5022151470184326, -0.8181620836257935, 0.35364601016044617, -0.06187352538108826, 0.6287551522254944, 0.16592714190483093, 0.20296545326709747, -0.5650224685668945, 0.32475852966308594, 1.4341129064559937, 0.5534894466400146, 0.5929107069969177, -0.7840877175331116, 0.24256181716918945, -0.66270911693573, 0.3649138808250427, -0.7734785676002502, -0.3048644959926605, -0.30715617537498474, -0.6501330137252808, -1.9549938440322876, -0.8549754023551941, 0.33798345923423767, -0.47272950410842896, 1.0935008525848389, -0.45372694730758667, 0.21510665118694305, 0.13797259330749512, 0.2670465409755707, 0.5743384957313538, 1.0146210193634033, 0.17371544241905212, -0.4111766219139099, 1.1353838443756104, -0.9317612648010254, -1.038387417793274, -1.0848585367202759, 1.0504056215286255, -0.30301910638809204, 0.32374176383018494, -0.12198163568973541, -1.1933741569519043, -0.593629777431488, -1.1953364610671997, 0.06021431088447571, -0.5499504208564758, 0.4599428176879883, 0.7142294645309448, 0.5492825508117676, -0.8293416500091553, 0.8607528805732727, -0.1870362013578415, -0.28911757469177246, 0.1671544760465622, 0.32451045513153076, -0.3867281973361969, -0.26806071400642395, -1.3590807914733887, 0.5105054974555969, 0.5062288641929626, -0.32247108221054077, -0.15341170132160187, -1.1697818040847778, -1.0669695138931274, -0.03545132279396057, 0.3977682888507843, -1.093986988067627, 0.9449520111083984, 0.2965652346611023, -1.7471368312835693, 0.4526883065700531, -0.4824311435222626, 0.004457762930542231, -0.04696393758058548, -0.8232839107513428, -0.3344266414642334, -0.39916759729385376, -0.3184594511985779, 0.061510082334280014, 0.5095928907394409, 0.002330400049686432, 0.2933166027069092, 0.2545529305934906, -0.34918949007987976, 0.3165454566478729, -0.4077524244785309, 0.9588683247566223, -0.3389607071876526, -0.14989790320396423, 0.35207176208496094, 0.5208365321159363, 0.05769753083586693, -0.5059546828269958, -0.7039706707000732, -1.0039278268814087, 0.5893301367759705, 0.16285359859466553, 1.3262944221496582, -0.8185531497001648, -0.07008969038724899, -0.057719796895980835, 0.08480969071388245, 0.06479448825120926, -0.8567399382591248, 1.0221482515335083, -0.6663814187049866, 0.46002501249313354, -0.28813979029655457, -1.3145049810409546, 0.32522886991500854, -0.05682143568992615, -0.9821171164512634, 0.22649849951267242, 0.1128901019692421, 1.0044748783111572, -0.935741662979126, 0.2925889790058136, 0.23444518446922302, 0.33446791768074036, -0.9139642119407654, 1.1986445188522339, -0.1298176646232605, -0.14086543023586273, -0.029080724343657494, -0.6217202544212341, 0.09721934050321579, -0.2810388207435608, 0.7026361227035522, -1.0642366409301758, 0.11595190316438675, 0.5270270109176636, -0.6232749223709106, 1.6228108406066895, -0.3357716202735901, 0.39478594064712524, -0.27530500292778015, -0.792717456817627, 0.34771376848220825, 0.2746610939502716, 0.008645345456898212, -0.3225752115249634, 0.4141180217266083, 0.5606316328048706, -0.6479989290237427, 0.6285450458526611, 0.9441704154014587, 0.540603756904602, -0.5854056477546692, 0.32402172684669495, 0.7158938050270081, -0.4477781653404236, 0.6920101642608643, 0.8781194090843201, 0.7341492176055908, 0.33894726634025574, 0.8064223527908325, -0.36983779072761536, 0.46795618534088135, -1.163682222366333, 0.1740153729915619, -0.09915400296449661, 0.4258856773376465, 1.0061533451080322, 0.3302060067653656, -0.5026697516441345, -0.7902817726135254, -0.35562360286712646, 0.9661792516708374, 1.3934732675552368, -0.30165770649909973, -0.8471224904060364, -0.7788392305374146, -0.14548282325267792, -0.46956926584243774, -0.16715890169143677, -0.3204509913921356, -0.4717048406600952, -0.8576581478118896, -1.0718950033187866, 0.6145186424255371, 0.3218948543071747, 1.0027849674224854, 0.018446357920765877, -0.16653399169445038, -0.2497768998146057, -0.27815210819244385, -0.9466784000396729, -0.9394267797470093, 0.15864935517311096, -0.4708952307701111, -0.10544051229953766, -0.10851476341485977, 0.24030737578868866, -0.014850801788270473, -0.25650930404663086, 1.0698819160461426, -0.5681305527687073, 0.19500049948692322, 0.04827779904007912, 0.35093656182289124, -0.40036171674728394, -0.6768414378166199, 0.2810240089893341, 0.3433322012424469, -0.1958874762058258, 0.28108248114585876, 0.4543759524822235, -0.19053497910499573, 0.27887776494026184, -0.34179916977882385, 0.18612918257713318, -0.1855672001838684, 0.22145159542560577, 0.34428179264068604, -0.5914275050163269, -0.11020924150943756, -1.4881037473678589, 1.0078233480453491, 0.18827836215496063, -0.08599816262722015, 0.2115115374326706, -0.5469725131988525, -0.34430551528930664, 0.31050339341163635, -0.427035927772522, -0.011958335526287556, -1.2288455963134766, 0.5380990505218506, 0.07179466634988785, 0.35199230909347534, 0.16627733409404755, -0.2055872082710266, 0.7021520137786865, -0.03196529671549797, 0.30905452370643616, 0.4381788671016693, -0.21075023710727692, 0.9585877656936646, -0.8389111161231995, 0.3630532920360565, 0.5499197244644165, 0.44753122329711914, -0.14433442056179047, -0.0821794718503952, -0.40430355072021484, -0.44332170486450195, -0.5359830856323242, -0.03635330870747566, -0.0643056184053421, 0.28581738471984863, -0.7922379374504089, -0.41544634103775024, 0.1608278453350067, -1.172884464263916, -0.019962411373853683, 0.10517964512109756, -0.30862727761268616, -0.27320122718811035, -0.782547652721405, -0.8210662007331848, 0.05218964442610741, -0.5503937005996704, -1.205509066581726, 0.3353860676288605, -0.27699029445648193, -0.5982460379600525, -0.6113897562026978, 0.31203213334083557, -0.22476941347122192, 0.9663219451904297, -0.780541718006134, 0.8559005856513977, -0.3595261573791504, -0.007116036955267191, 0.05949999392032623, 0.06159842014312744, 0.8762284517288208, -0.3950955271720886, 0.013047034852206707, -0.46166110038757324, 0.036385428160429, -0.8664670586585999, 0.06451594084501266, 0.2878362536430359, 0.28560546040534973, 0.039935141801834106, 0.18068163096904755, -0.4432391822338104, 0.6242309212684631, 1.311653971672058, -1.1152889728546143, 0.3135407269001007, 0.059709351509809494, 1.1585572957992554, 0.07762620598077774, -0.28840887546539307, 0.5615089535713196, 0.32376423478126526, 0.3578740358352661, -0.08091820776462555, -0.2759819030761719, -0.2684561610221863, -0.7456842660903931, 0.7530829310417175, 1.8723692893981934, 0.23593005537986755, -0.5300469994544983, -0.6924780011177063, 0.6951647400856018, -1.435685157775879, -0.47155287861824036, 0.6925283074378967, 0.7478635311126709, 0.33870676159858704, -0.7674281597137451, -0.05335349217057228, -0.08451713621616364, 0.2917148768901825, 0.6977084279060364, -0.0021828636527061462, -0.834690272808075, -0.25881829857826233, 0.322594553232193, 0.6652383804321289, 0.7313992381095886, -0.12050558626651764, 0.9865231513977051, 14.78772258758545, 0.7647908926010132, 0.32548704743385315, 0.7480304837226868, 0.2110605388879776, -0.03998376429080963, -0.660718560218811, 0.09407972544431686, -1.0340017080307007, 0.049956049770116806, 1.3830881118774414, 0.035634685307741165, 0.39383071660995483, -0.43326982855796814, 0.27744001150131226, 0.2779587507247925, -0.48148754239082336, 0.7770296335220337, 0.5676887631416321, -1.2153189182281494, 0.45786866545677185, 0.29423075914382935, 0.17839911580085754, 0.3840669095516205, 0.932793140411377, 0.46872398257255554, 0.44167158007621765, -0.7173874378204346, 0.24800057709217072, 0.1914437860250473, 0.7588289976119995, -0.13114924728870392, 0.42131832242012024, 0.29069486260414124, -0.6575431227684021, -0.2441689670085907, -0.7588536739349365, -1.1249370574951172, 0.5125644207000732, 0.37988805770874023, -0.42773357033729553, -0.0034599867649376392, -0.37852752208709717, 1.0908440351486206, 0.4962300658226013, 0.06906165927648544, -0.22132126986980438, 0.758416473865509, -0.2824833393096924, 0.13623365759849548, 0.10432993620634079, -0.0065629188902676105, 0.4532938003540039, -0.0050170463509857655, 0.28548941016197205, 0.18123793601989746, 0.016579102724790573, -0.00863333698362112, -0.4293389916419983, -0.07240153104066849, -0.3344977796077728, -0.24519135057926178, 0.498754620552063, 0.6470012664794922, 0.7365304827690125, 0.11921421438455582, -0.2627113461494446, 0.3509710133075714, 0.4370659291744232, 0.2978277802467346, -0.23467117547988892, 0.06470512598752975, 0.5268849730491638, -0.21031345427036285, -0.1099328026175499, 0.15548360347747803, 0.06495098024606705, -0.5263916850090027, -0.9353870153427124, -0.36317363381385803, -0.002156980102881789, -0.7840217351913452, -0.3377911150455475, 0.6162440180778503, 0.15817047655582428, -0.6382904052734375, 0.18036645650863647, -0.6150938868522644, 0.09583718329668045, 0.2827513813972473, -1.3938127756118774, -0.5901702046394348, 0.6806591749191284, -0.4157399833202362, -0.505616307258606, -0.31286880373954773, 1.2606234550476074, 0.1020137146115303, -0.594984769821167, 0.2319432646036148, 0.31424131989479065, -0.2805154323577881, 0.2333957850933075, -0.714275598526001, 0.5540225505828857, 0.19606219232082367, -0.26934799551963806, 0.6544983386993408, 0.4035358130931854, 0.41551104187965393, -1.1335186958312988, 0.10566405951976776, 0.918281614780426, -0.9012454748153687, -0.12429875880479813, -0.8010600805282593, -0.6369869709014893, 0.09899425506591797, 0.7072241306304932, -0.3222475051879883, 0.7576515078544617, -0.0032248813658952713, -0.8347640037536621, -0.39205124974250793, -0.8236821889877319, -0.03396392613649368, 0.7652134299278259, -0.9082224369049072, -0.3652441203594208, -0.0008478850941173732, 0.4154975116252899, -0.8091195821762085, -0.6131073832511902, -0.20736759901046753, 0.04087631776928902, 0.02232312597334385, 1.240973949432373, -0.35062456130981445, 0.6188108921051025, 0.7800824046134949, -0.30216413736343384, -1.4341708421707153, -0.2204100340604782, -1.3448668718338013, 0.04085291922092438, 0.09857713431119919, 0.5003702640533447, -0.6009257435798645, 0.07515812665224075, 0.31024110317230225, 0.20326049625873566, -0.3318098485469818, -0.6885437965393066, -0.2666876018047333, 0.2888539433479309, -0.18692727386951447, 0.4325955808162689, -0.2699487507343292, 0.06419847160577774, 0.594014585018158, 0.127847358584404, 0.46008843183517456, -0.3023556172847748, -1.328956127166748, 0.3496924340724945, 0.05232208967208862, 0.11085641384124756, -0.34426748752593994, -0.45533648133277893, -1.5397942066192627, 0.35906782746315, -1.0705196857452393, -0.07831033319234848, -1.1200894117355347, 0.03660023212432861, 0.587140679359436, -0.09175965189933777, -0.19407393038272858, 0.21068845689296722, 0.06605303287506104, -0.3607264757156372, -0.5655366778373718, -0.6247648000717163, 0.9695534706115723, 0.7526227235794067, -0.6229750514030457, 0.48626866936683655, -0.6148455739021301, -0.0856504812836647, 0.10123513638973236, 0.46161457896232605, -0.17582650482654572, -1.310241460800171, -1.573785424232483, 0.8454464673995972, -0.28164511919021606, 0.13630789518356323, -0.05795392021536827, 0.3896484673023224, 0.4226871132850647, -0.5338332056999207, -0.012419149279594421, 0.5267329812049866, -0.8895083665847778, -0.39352360367774963, 0.2938927710056305, -0.45801305770874023, 0.5448421239852905, -0.16159163415431976, -0.369448184967041, 0.004657865036278963, 0.9423072934150696, -0.3347181975841522, -0.9156144857406616, -0.6024525165557861, 0.253154993057251, -0.7970826029777527, 0.24577723443508148, -0.2708803713321686, -0.2706054747104645, -1.2272453308105469, -0.2634153664112091, 0.18045583367347717, 0.1664857268333435, -0.2535712718963623, 0.5337580442428589, 0.3886106312274933, -1.0766435861587524, -0.24787068367004395, 0.2666199803352356, -0.057171568274497986, -0.4505154490470886, 0.5272489786148071, 0.19868513941764832, -0.15856486558914185, 0.783398449420929, 0.5908975005149841, 0.44708994030952454, -0.8245223760604858, -0.22025138139724731, 0.6461107134819031, -0.7166934013366699, -0.2745214104652405, 1.2432992458343506, -0.3310389816761017, -0.9999996423721313, -0.30257856845855713, -0.8062179088592529, -0.30811768770217896, -0.22268815338611603, 0.6218759417533875, 0.46367621421813965, -0.09787692129611969, -0.1327468454837799, -0.7969309687614441, 0.4122448265552521, -0.2565074861049652, -0.6075839400291443, 0.5332798361778259, -0.28152135014533997, -0.5459572672843933, 0.3874640166759491, 1.2342087030410767, -0.69270920753479, -0.352167546749115, -1.0500428676605225, -0.7322697043418884, -0.15878979861736298, 0.7197954654693604, -0.33815833926200867, -0.42399272322654724, 0.5334005355834961, 0.4317442774772644, 0.23724019527435303, 0.1264469474554062, -0.2858119308948517, 0.5207329988479614, 0.5064302682876587, -0.18097905814647675, -0.7005603909492493, -0.4493708908557892, 1.386000394821167, 1.1477136611938477, -0.5505084991455078, 0.6297022700309753, -0.6838732361793518, -0.6064578890800476, 0.49595215916633606, -0.3547734320163727, -0.4318298101425171, 0.9530459642410278, 0.07822985202074051, 0.011480259709060192, 0.21841083467006683, -1.1263010501861572, -0.22061483561992645, 0.8351215124130249, 1.0428037643432617, 0.7160560488700867, 0.5357588529586792, -0.17192544043064117, 0.9472014904022217, 0.17533525824546814, -0.2622741460800171, 0.636516273021698, 0.07972056418657303, -0.2872238755226135, -0.4079101085662842, 0.21383829414844513, 0.20095670223236084, -0.28834837675094604, -0.7392681837081909, -0.04368959739804268, 0.3912888169288635, -0.2055886685848236, 0.6020089983940125, 0.8741182684898376, 0.03998185321688652, 0.5949814319610596, 0.15777495503425598, 0.6378452777862549, -0.6629766821861267, -0.4388762414455414, -0.08681444823741913, -0.3856401741504669, -0.4832417666912079, -0.3600022792816162, -0.31323668360710144, -0.23877088725566864, -0.3462023437023163, 0.290619432926178, 0.04427414387464523, 0.2695287764072418, 1.3867160081863403, 0.7153623700141907, 0.48123207688331604, 0.04931359365582466, -0.3434310555458069, -0.6411618590354919, -0.9973940849304199, -0.2025856226682663, -0.41241198778152466, -0.26163071393966675, -0.39916813373565674, -0.19460445642471313, -0.5414500832557678]}, "authors": [], "references": [{"paperId": "b0cd93e95fb6885db47d755a4c631158b0198047", "title": "DeLighT: Very Deep and Light-weight Transformer"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "738215a396f6eee1709c6b521a6199769f0ce674", "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"}, {"paperId": "94c30d3c03beace3ba0d7014e0b1852bb892133c", "title": "Non-autoregressive Machine Translation with Disentangled Context Transformer"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "841d43cf4015042a4ee45745c5b6f2c59c184da5", "title": "DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "1cf1ec50e75f2c9f3064a411ea1428695a1df910", "title": "Fully Quantized Transformer for Improved Translation"}, {"paperId": "9a618cca0d2fc78db1be1aed70517401cb3f3859", "title": "Deep Equilibrium Models"}, {"paperId": "53a77e8f73f2ca422d6e38fa9ecc490231ac044c", "title": "Neural Text Generation with Unlikelihood Training"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "5efadc9019ce3378a0eb6c8f939cdde6c8918b1e", "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models"}, {"paperId": "b5aa927c906101b3f8854a29f374551e3ea64474", "title": "Pre-trained language model representations for language generation"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "7e960b6479d1071b507d0014776adecb3480748d", "title": "Exploiting Deep Representations for Neural Machine Translation"}, {"paperId": "47d79963ac69111d8dc82a228d26e6a746a4d087", "title": "Transformers"}, {"paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d", "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "bb669de2fce407df2f5cb2f8c51dedee3f467e04", "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"}, {"paperId": "680aafd3d51e666b297e27b93d9554cc2caf1c4d", "title": "An Analysis of Neural Language Modeling at Multiple Scales"}, {"paperId": "9c5c89199114858eafbe50b46d77d38ffd03b28a", "title": "Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "9b4a861151fabae1dfd61c917d031c86d26be704", "title": "Controllable Abstractive Summarization"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "668db48c6a79826456341680ee1175dfc4cced71", "title": "Get To The Point: Summarization with Pointer-Generator Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "f37076f426023241f19cdc2fb0a0fd733a6fa7fa", "title": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d", "title": "Sequence to Sequence Learning with Neural Networks"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91", "title": "On the importance of initialization and momentum in deep learning"}, {"paperId": "a538b05ebb01a40323997629e171c91aa28b8e2f", "title": "Rectified Linear Units Improve Restricted Boltzmann Machines"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation"}, {"paperId": null, "title": "LANGUAGE MODELING When training our langauge model, we use 8 GPUs with 1536 tokens per GPU and an update frequency of 3, following Welleck et"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "fe82735fe8ae2163a37aa2787eee0db8efc745b6", "title": "transformers . zip : Compressing Transformers with Pruning and Quantization"}, {"paperId": "e4351041d25c272a008bcd5765868dc3a28fe470", "title": "Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks"}, {"paperId": null, "title": "Sharing weights across layers l \u2208 [1 , . . . , L \u2212 1] such that layer L remains independent \u2014denoted as All-Shared (except last)"}, {"paperId": null, "title": "Sharing every two layers, i."}, {"paperId": null, "title": "We train all base/small models on 8 NVIDIA Tesla V100 GPUs. For all big/large models"}]}