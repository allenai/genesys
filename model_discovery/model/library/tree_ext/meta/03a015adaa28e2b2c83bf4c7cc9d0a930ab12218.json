{"paperId": "03a015adaa28e2b2c83bf4c7cc9d0a930ab12218", "title": "Let the Code LLM Edit Itself When You Edit the Code", "abstract": "In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing \\underline{\\textbf{Positional \\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85% compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance.", "venue": "", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work investigates a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly, and introduces positional positional encoding (PIE), a building upon the rotary positional encoding."}, "embedding": {"model": "specter_v2", "vector": [0.05881625413894653, 0.18623040616512299, -0.7049967646598816, -0.034323032945394516, -0.48613375425338745, -0.4335933029651642, 0.29285046458244324, 0.32140132784843445, -0.354716956615448, -0.5680387616157532, 0.41960182785987854, -0.41886553168296814, 0.37341514229774475, 0.1762663573026657, -0.5503788590431213, 0.47937989234924316, -0.6146112680435181, -0.12796340882778168, 0.17367546260356903, -0.049277305603027344, 0.34784606099128723, -0.5969110727310181, -1.2428569793701172, 0.6304711699485779, 1.1215898990631104, 0.1728644073009491, -0.03883384168148041, 0.7872682213783264, -0.849349856376648, 0.8578879237174988, 0.5159490704536438, -0.5657196044921875, 0.34784388542175293, 0.06759808212518692, -0.29016074538230896, -0.22116896510124207, -0.11255879700183868, -0.2819218933582306, 0.008169751614332199, 0.5182611346244812, -0.4828704297542572, 0.013727101497352123, 0.34099993109703064, -0.781627893447876, -0.4741617739200592, 1.0338246822357178, 0.15412665903568268, 0.3931386172771454, 0.024959957227110863, -0.5759437680244446, 0.6887191534042358, -1.6248588562011719, 0.36423903703689575, 1.0016772747039795, 1.029240608215332, 0.252871572971344, 0.06314844638109207, -0.12242580205202103, 0.3669581711292267, -0.19837433099746704, -1.1661417484283447, -0.6053531169891357, -0.3503974676132202, -0.7656500339508057, 2.020303964614868, -0.4801580309867859, -0.06848873198032379, -0.21566101908683777, 0.003015275113284588, 1.2325774431228638, -0.7048655152320862, -0.5785646438598633, 0.08267747610807419, 0.16669663786888123, -0.11299900710582733, 1.1825207471847534, -0.24167704582214355, 0.07221362739801407, -0.8923682570457458, -0.6217532753944397, 0.3569256067276001, -0.14591841399669647, 0.3461875915527344, -0.8647459745407104, 0.02957797236740589, 0.30951541662216187, -0.035917654633522034, 0.644058883190155, -0.043223194777965546, 0.9400910139083862, 0.621281623840332, 0.09920036792755127, 0.23014980554580688, 0.19738687574863434, -0.006259675603359938, -0.5695405006408691, -1.2263212203979492, 0.17458084225654602, -0.2389913648366928, 0.9767492413520813, -0.29268622398376465, 0.17440012097358704, -0.6895710825920105, 0.49105092883110046, 1.2571735382080078, -0.518793523311615, 0.33394747972488403, -0.7080574035644531, 0.31725817918777466, -0.365922212600708, 0.190824493765831, -0.05821790546178818, 0.297544002532959, -0.14822380244731903, -0.49977511167526245, -0.6506495475769043, -0.6520869135856628, 0.055880796164274216, -0.685569167137146, 0.13667747378349304, -0.34558555483818054, 0.3652477562427521, 0.12630584836006165, -0.01973695307970047, 0.31371158361434937, 0.42289671301841736, 0.018068697303533554, 0.22289161384105682, 0.09734320640563965, -1.1653661727905273, -0.3152921199798584, -1.20522940158844, 1.2982888221740723, -0.7587743997573853, -0.27401673793792725, -0.4735291600227356, -1.5028579235076904, -1.0075345039367676, -0.8684141635894775, -0.1439765840768814, -0.21664568781852722, 0.7065291404724121, 1.0639331340789795, -0.06869452446699142, -1.2554669380187988, 1.1402219533920288, -0.1825110763311386, 0.05022355914115906, 0.3882273733615875, -0.37943440675735474, 0.1517944186925888, -0.43419742584228516, -0.8453157544136047, 0.12220577150583267, -0.005442141555249691, -0.5689045786857605, 0.0023733992129564285, -0.9865785241127014, -1.261265516281128, -0.42127764225006104, 0.4775095283985138, -0.1648135632276535, 1.6989946365356445, 0.4139581620693207, -0.8998926877975464, 0.23118168115615845, -0.3160792887210846, 0.3042275905609131, 0.6667841672897339, -0.006538881454616785, -0.34171104431152344, -1.0309607982635498, -0.03165190666913986, 0.21973635256290436, 0.34768611192703247, -0.533943235874176, -0.6131796836853027, 0.642322838306427, -0.04810561239719391, -0.18148957192897797, 0.15923786163330078, 0.9523054361343384, -0.7664982080459595, -0.40268266201019287, 0.351048082113266, 0.6027469038963318, -0.16277475655078888, 0.3174887001514435, -0.08066713064908981, -0.6131535172462463, 0.524843156337738, -0.145479217171669, 1.3975956439971924, -1.0812902450561523, -0.6799537539482117, -0.2830532491207123, -0.2635182738304138, -0.03541750833392143, -0.5523150563240051, 0.745755136013031, -0.3213512599468231, 0.6672078967094421, -0.58506178855896, -1.1372674703598022, -0.1146756261587143, -0.5156846046447754, -0.45192253589630127, -0.3607465922832489, -0.09739846736192703, 0.9841989278793335, -0.7903455495834351, 0.40583524107933044, -0.5236870050430298, 0.18519411981105804, -1.1457717418670654, 0.8779613375663757, -0.21613888442516327, -0.3171241879463196, 0.19904543459415436, -0.2371181696653366, 0.3079981207847595, 0.10079821944236755, 0.19266930222511292, -0.11943709850311279, -0.5784459114074707, 0.5047303438186646, -0.3782205879688263, 1.913701057434082, -0.43979009985923767, 0.3370828628540039, -0.27099665999412537, -0.06543116271495819, 0.22817838191986084, 0.3572397232055664, -0.12456822395324707, -0.026875315234065056, -0.12314356863498688, 0.5796652436256409, -0.4846060276031494, 0.18793773651123047, 1.025348424911499, 0.9610786437988281, -0.26542386412620544, 0.6786824464797974, 0.1893494874238968, -0.38699662685394287, 0.6012426614761353, 0.6263375282287598, 1.0068323612213135, 0.6784396767616272, 0.6558509469032288, 0.28347426652908325, 0.4627481698989868, -0.6047146916389465, -0.02524767443537712, 0.5924931168556213, 0.8327344655990601, 0.8966549038887024, 0.5016831159591675, -0.7028564214706421, -0.6799520254135132, 0.22610166668891907, 0.7850506901741028, 1.2430843114852905, 0.019053775817155838, -0.49680015444755554, -0.6918234825134277, -0.5059911608695984, 0.1841903179883957, 0.5395342707633972, -0.40318191051483154, -0.7384461164474487, -0.5063919425010681, -0.6404522657394409, 1.210984468460083, 0.650858461856842, 0.90343177318573, -0.2966736853122711, -0.28366678953170776, -0.034477993845939636, 0.2638806998729706, -0.44262856245040894, -0.8929892778396606, 0.40286335349082947, -0.3462993800640106, 0.11405286937952042, 0.36152979731559753, -0.27101650834083557, 0.42075586318969727, -0.34281694889068604, 0.5018695592880249, 0.23248422145843506, -0.7823049426078796, -0.08596141636371613, -0.11554620414972305, -0.4665588438510895, -1.3793625831604004, 0.050327494740486145, -0.13924835622310638, -0.5739116072654724, 0.26717129349708557, 0.534765899181366, 0.32395464181900024, -0.029056215658783913, -0.48619264364242554, 0.7555456161499023, -0.014164777472615242, -0.480183482170105, 0.28651100397109985, -0.1853753626346588, -0.41616585850715637, -1.0034900903701782, 1.1306414604187012, 0.08021113276481628, -0.5244863033294678, 0.3860417902469635, -0.7244819402694702, -0.15860316157341003, 0.5034496784210205, -0.3784419894218445, -0.3418254852294922, -1.169062852859497, 0.285776823759079, 0.09604469686746597, 0.11148807406425476, 0.33338800072669983, 0.7404642105102539, 0.05341621860861778, 0.15805819630622864, 0.6719280481338501, 0.3740161657333374, -0.6455883383750916, 0.6258949041366577, -0.6179351210594177, 0.6731583476066589, 0.018014322966337204, 0.5949438810348511, -0.32158687710762024, -0.7315624356269836, 0.0795237347483635, 0.14839160442352295, -0.434443861246109, -0.5859741568565369, -0.24944859743118286, -0.11268144845962524, -0.6906923055648804, -0.2708090841770172, -0.05200900509953499, -1.8188793659210205, -0.27052050828933716, 0.09259685128927231, -0.41520965099334717, -0.15402501821517944, -0.7847774624824524, -1.053537130355835, -0.48325878381729126, -0.22639738023281097, -1.25602388381958, 0.4164562523365021, -0.44240960478782654, -0.2886752784252167, 0.05804542824625969, 0.14366021752357483, -0.5449808239936829, 0.9851694703102112, -0.9098094701766968, 1.1075454950332642, 0.5826236009597778, -0.23841625452041626, -0.10025357455015182, 0.08822993189096451, 0.36168935894966125, -0.00932485144585371, 0.8136597275733948, -0.2190810889005661, 0.24693627655506134, -0.4278140068054199, -0.020213069394230843, 0.07452044636011124, -0.18799947202205658, 0.9856076836585999, 0.0850403681397438, -0.6669718623161316, 0.31868237257003784, 1.4793076515197754, 0.0005156900151632726, 0.13021446764469147, 0.1327698975801468, 1.0728758573532104, 0.12949341535568237, 0.20153139531612396, 1.1491819620132446, 0.06979265064001083, 0.5280394554138184, 0.2135457992553711, 0.1032114177942276, -0.1000034436583519, -0.498196542263031, 0.5136213898658752, 1.5340497493743896, 0.0009542893385514617, -0.05952348932623863, -1.2907655239105225, 0.5397614240646362, -1.5396784543991089, -0.8231925368309021, 0.40063363313674927, 0.7577438354492188, 0.6784241199493408, -0.7914782166481018, -0.6193322539329529, -0.1396019011735916, 0.34260058403015137, 0.3721967041492462, -0.15360406041145325, -1.0076706409454346, 0.6038306355476379, -0.10775643587112427, 0.2139950692653656, 0.5633920431137085, -0.22697576880455017, 0.4941655695438385, 14.569351196289062, 0.893363356590271, 0.3079676032066345, 0.702715277671814, 0.3399389982223511, 0.1487443894147873, -0.606118381023407, -0.29210418462753296, -1.2430698871612549, -0.03142493963241577, 0.8457615375518799, -0.5226876735687256, 0.6187002062797546, 0.5734981298446655, 0.17226245999336243, -0.012982022948563099, -0.6316324472427368, 0.6986120343208313, 0.6492362022399902, -1.7870250940322876, 0.38947585225105286, 0.2001146823167801, 0.4689610004425049, 0.3711513876914978, 0.7863562703132629, 0.709876537322998, 0.5652148723602295, -0.5800537467002869, 1.03843355178833, 0.17651741206645966, 1.2340466976165771, -0.3553309142589569, 0.5775419473648071, 0.49367594718933105, -1.2075660228729248, -0.2608643174171448, -0.9116989374160767, -0.9951693415641785, 0.47419852018356323, 0.32239392399787903, -0.9468992948532104, -0.5386401414871216, -0.23015667498111725, 0.8118804693222046, -0.2717090845108032, 0.8305343985557556, -0.03423036262392998, -0.15632376074790955, 0.6601865887641907, 0.3777184784412384, -0.07197316735982895, 0.5526651740074158, -0.11515491455793381, 0.1491386443376541, 0.5237690210342407, -0.26953136920928955, -0.09671960771083832, 0.39988917112350464, -0.7254089117050171, 0.05243932083249092, -0.6334347724914551, -0.10340584069490433, -0.041292380541563034, 0.4031340479850769, 0.49902454018592834, 0.2905409336090088, -0.5097501277923584, 0.6637576222419739, 0.9043449759483337, -0.1422194391489029, -0.9371605515480042, 0.11441969871520996, 0.926616907119751, -0.01736089214682579, 0.0950179398059845, 0.32515785098075867, -0.09904801845550537, -0.30352267622947693, -0.556784987449646, -0.36978858709335327, -0.20440085232257843, -0.688273549079895, -0.25811710953712463, 0.9889797568321228, -0.3024331033229828, -1.1168484687805176, -0.026723334565758705, -0.46917542815208435, -0.11234070360660553, 0.19109991192817688, -0.9753013849258423, -0.23986154794692993, 0.21546785533428192, -0.5272321701049805, -0.20867791771888733, 0.030509449541568756, 1.026173710823059, 0.4293413758277893, 0.19646695256233215, 0.07936053723096848, 0.4922049939632416, 0.06035103276371956, 0.17869777977466583, -0.697950541973114, 1.3142238855361938, 0.817420244216919, -0.23795494437217712, 0.6310536861419678, -0.28191280364990234, -0.31399601697921753, -0.36770254373550415, -0.8808197379112244, 0.5655356049537659, -0.837099552154541, -0.48551005125045776, -0.9231181740760803, -0.8860446214675903, 0.2989538609981537, 0.32358720898628235, -0.184219092130661, 0.5553781390190125, -0.32559534907341003, -0.9917984008789062, -0.043333783745765686, -0.7292054891586304, 0.15397466719150543, 1.0813394784927368, -1.0421240329742432, 0.12445195019245148, -0.05763383209705353, 0.6694319248199463, -1.3658570051193237, -0.5628179311752319, -0.050359707325696945, 0.09372750669717789, -0.5562703609466553, 0.7675994634628296, 0.01497881580144167, 1.2190542221069336, 0.29384294152259827, 0.4376038610935211, -1.0165534019470215, -0.3149784207344055, -0.6844161748886108, 0.2680028975009918, 0.22776636481285095, 1.0022333860397339, 0.21638701856136322, 0.16459010541439056, 1.1342946290969849, 0.04041655361652374, -0.3867822587490082, -0.3785382807254791, -0.22534380853176117, -0.07780741155147552, -1.0370014905929565, 0.6105066537857056, -0.006947895046323538, 0.27615463733673096, -0.8065097332000732, 0.16241899132728577, 0.31238606572151184, -0.6149099469184875, -0.7706065773963928, 0.6038796901702881, 0.035120923072099686, -0.14383205771446228, -0.44402453303337097, -0.38243424892425537, -1.074156641960144, -0.14361006021499634, -1.2053313255310059, 0.7551119327545166, -0.9322406053543091, -0.385754257440567, 0.03881891444325447, -0.008992539718747139, 0.13785400986671448, 0.43391120433807373, -0.3806907534599304, -0.6074368953704834, -0.5807978510856628, -0.60910964012146, 0.7088229656219482, 0.5560131669044495, -0.7200097441673279, 0.08757279068231583, -0.3219301104545593, -0.10489273816347122, 0.32558467984199524, 0.2778956890106201, -0.7300335168838501, -1.0338197946548462, -1.6696650981903076, 0.6421729326248169, -0.12596505880355835, -0.12999168038368225, -0.8175309896469116, 0.9434086084365845, 0.44868505001068115, -0.25774142146110535, 0.1304888129234314, -0.2979467511177063, -0.63609778881073, -0.07182182371616364, 0.4682152271270752, -0.9672803282737732, 0.5587380528450012, 0.4257950186729431, -0.6278383731842041, -0.3621159791946411, 0.1919601857662201, -0.2629069983959198, -0.9206485748291016, -0.9502820372581482, 0.502345860004425, -0.8874766826629639, -0.16022013127803802, -0.0732545480132103, -0.1513868272304535, -1.282918930053711, -0.1990959793329239, 0.4688072204589844, -0.06538588553667068, 0.2515949606895447, 1.5298212766647339, 0.47986099123954773, -1.0398640632629395, 0.16815215349197388, 0.3873130977153778, 0.30118149518966675, -0.045999832451343536, 0.48157668113708496, 0.3856652081012726, -0.9099093079566956, -0.045564763247966766, 0.6140605211257935, 0.5546410083770752, -1.1310371160507202, 0.12648974359035492, 0.4094751179218292, -0.2867734432220459, -0.06074551120400429, 1.5330554246902466, -0.5881962776184082, -1.0025759935379028, -0.2266916036605835, -1.3748140335083008, -0.220328688621521, -0.674471378326416, 0.8505121469497681, 0.2406519055366516, 0.352658212184906, -0.18134446442127228, -0.8248116970062256, 0.016674943268299103, -0.5819517374038696, -0.3557533025741577, 0.44180721044540405, 0.010231937281787395, -0.9501696228981018, 0.40319639444351196, 0.6987616419792175, -0.14006008207798004, -0.5951511263847351, -0.3034881055355072, -0.36374714970588684, -0.4130999743938446, -0.053852569311857224, -0.4101433753967285, -0.3889903128147125, 0.5707629323005676, 0.11050423234701157, -0.03911975026130676, -0.5904101133346558, -0.2535921335220337, -0.01837870478630066, 0.5112130045890808, 0.5882758498191833, -0.6437761783599854, -0.7401960492134094, 1.0250359773635864, 1.2493635416030884, -0.563643753528595, 0.38067513704299927, -0.20296518504619598, -0.8638198971748352, 0.9042218327522278, 0.6498384475708008, 0.38092511892318726, 0.8193967938423157, 0.3594230115413666, 0.27488595247268677, 0.5941933989524841, -1.0175034999847412, 0.29190778732299805, 0.5977962613105774, 1.0764096975326538, 1.1249138116836548, -0.07083237916231155, 0.16546733677387238, 0.7497317790985107, 0.333954781293869, 0.3722662031650543, 1.0902087688446045, 0.8524831533432007, -0.023580040782690048, -0.4655458331108093, 0.31187424063682556, 0.971575140953064, -0.2292831540107727, -0.9076412320137024, 0.3916163146495819, 0.27756956219673157, 0.8083778023719788, 0.6316506266593933, 0.6114675998687744, -0.08540378510951996, 0.14353996515274048, 0.3364602327346802, 0.2397172749042511, -0.9608467221260071, -0.3409525156021118, -0.3365055024623871, -0.603519082069397, -0.3537123501300812, -0.01918180100619793, -0.6197366714477539, -0.8151222467422485, -0.3135576546192169, 0.5138909816741943, 0.4200732707977295, 0.24635934829711914, 0.8381319046020508, 1.0049073696136475, 0.649176836013794, -0.5856510400772095, -0.29431989789009094, -0.029575413092970848, -0.5932538509368896, -0.18413589894771576, -0.3922974467277527, -0.4271716773509979, -0.03088540770113468, 0.005491672083735466, -0.014157853089272976]}, "authors": [{"authorId": "2266802709", "name": "Zhenyu He"}, {"authorId": "2309528953", "name": "Jun Zhang"}, {"authorId": "2108801920", "name": "Shengjie Luo"}, {"authorId": "2283486084", "name": "Jingjing Xu"}, {"authorId": "2157136438", "name": "Z. Zhang"}, {"authorId": "2309818554", "name": "Di He"}], "references": [{"paperId": "b1f5087ab3e782f718a1393bed242b4b412e648b", "title": "Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis"}, {"paperId": "20f090e35ad598fba2404e550c2462dc9da03a10", "title": "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve"}, {"paperId": "9d932de1d2f51067b6481745f28a2db345293d48", "title": "Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation"}, {"paperId": "1b5db3170c195508ff24fee8eda0d4987e806f0b", "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty"}, {"paperId": "1f2a20a6efaf83214861dddae4a38a83ae18fe32", "title": "DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence"}, {"paperId": "57e7af0b69325fafb371ef5d502e39ef9c90ef7e", "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"}, {"paperId": "532c2c7a247d9e97d20abec1b2f4612984fdab93", "title": "REST: Retrieval-Based Speculative Decoding"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "8df524e0c50903d0b2c4be338081906d13ea42af", "title": "Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding"}, {"paperId": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05", "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"}, {"paperId": "43e624ddeed82df944a6cae0dedec3372438e243", "title": "Accelerating LLM Inference with Staged Speculative Decoding"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "f97413a497d47c739d41d237917e6566154647b4", "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems"}, {"paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2", "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "017010b941d902a467f6d329ae5e74fd67e67912", "title": "LLM-Pruner: On the Structural Pruning of Large Language Models"}, {"paperId": "b6d6c33298b852cf63edac233deca70530d69a2a", "title": "PaLM 2 Technical Report"}, {"paperId": "42a14d824caa3348046eb34c37e2ab7985faa7a3", "title": "High-throughput Generative Inference of Large Language Models with a Single GPU"}, {"paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c", "title": "Accelerating Large Language Model Decoding with Speculative Sampling"}, {"paperId": "e3d1175f5b522220c31f96c5c6753a0757aae471", "title": "Rethinking the Expressive Power of GNNs via Graph Biconnectivity"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "5735e49e501c8e51e9be4079592e46e047747b03", "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis"}, {"paperId": "9575afb5702bc33d7df14c48feeee5901ea00369", "title": "A Length-Extrapolatable Transformer"}, {"paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644", "title": "Fast Inference from Transformers via Speculative Decoding"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "379e42895f6d40ab9e9559609f505aba89145a5d", "title": "Efficiently Scaling Transformer Inference"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "bf8d98979f1dfd1df492be21d760d5c0e7f22359", "title": "One Transformer Can Understand Both 2D & 3D Molecular Data"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a", "title": "Your Transformer May Not be as Powerful as You Expect"}, {"paperId": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b", "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f", "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "b4d207a2096aee4a3764933373eef6edb574c952", "title": "Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks"}, {"paperId": "69a72ff5b30642d11c96635e99aadad3140d33a7", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"}, {"paperId": "73e0f38ab49b19b86321016b773e15f1d02e3a72", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"}, {"paperId": null, "title": "Transformers: State-of-the-Art Natural Language Processing"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "8ee2351221b72fca5eef4c42147ed67071903d93", "title": "IntelliCode compose: code generation using transformer"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "34a4e6818d680875ff0bef9a76de0376118446d1", "title": "Sparse Sinkhorn Attention"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28", "title": "Blockwise Self-Attention for Long Document Understanding"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "5e04881e91bff952d102d967c4ffb498ec30d4af", "title": "Blockwise Parallel Decoding for Deep Autoregressive Models"}, {"paperId": "6398cb8f2af1c988a097ed1e1cefb380195edfb8", "title": "(Preprint)"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "dc48bc1a4d81e0f37603013fd2a95644dc233bd0", "title": "Functional Interpolation for Relative Positions Improves Long Context Transformers"}, {"paperId": "363668677c459ebc0ff494655f993a93a0251009", "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers"}, {"paperId": "d1a6b3a5efde3783b53f822dc8dd00aaac934b95", "title": "SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification"}, {"paperId": "104f7a96eba307056e1038e183ee8c24d009ba13", "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "acf87283fa8ae426f1a4987b345b401bf2913f61", "title": "Do Transformers Really Perform Badly for Graph Representation?"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}]}