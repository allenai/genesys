{"paperId": "96d104dfe727f78a35faaafe81481f3672b485ee", "title": "Large Language Models are Visual Reasoning Coordinators", "abstract": "Visual reasoning requires multimodal perception and commonsense cognition of the world. Recently, multiple vision-language models (VLMs) have been proposed with excellent commonsense reasoning ability in various domains. However, how to harness the collective power of these complementary VLMs is rarely explored. Existing methods like ensemble still struggle to aggregate these models with the desired higher-order communications. In this work, we propose Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning. Our key insight is that a large language model (LLM) can efficiently coordinate multiple VLMs by facilitating natural language communication that leverages their distinct and complementary capabilities. Extensive experiments demonstrate that our instruction tuning variant, Cola-FT, achieves state-of-the-art performance on visual question answering (VQA), outside knowledge VQA, visual entailment, and visual spatial reasoning tasks. Moreover, we show that our in-context learning variant, Cola-Zero, exhibits competitive performance in zero and few-shot settings, without finetuning. Through systematic ablation studies and visualizations, we validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VLMs; it then coordinates them to enable impressive visual reasoning capabilities.", "venue": "Neural Information Processing Systems", "year": 2023, "citationCount": 27, "influentialCitationCount": 3, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning by facilitating natural language communication that leverages their distinct and complementary capabilities, and validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VL Ms and coordinates them to enable impressive visual reasoning capabilities."}, "embedding": {"model": "specter_v2", "vector": [0.08168500661849976, 0.3149661719799042, -0.02166980318725109, 0.02141484059393406, -0.5003586411476135, -0.2902975082397461, 0.7686834335327148, 0.09186987578868866, -0.4877736270427704, -0.4953991770744324, 0.311734676361084, -0.2536608576774597, 0.13038572669029236, -0.14141416549682617, -0.21647168695926666, 0.4443376958370209, -0.7082162499427795, 0.5540869832038879, 0.41856470704078674, -0.8345090746879578, 0.021937018260359764, -0.6616784334182739, -1.4172509908676147, 0.48093366622924805, 0.6901893615722656, 0.40839913487434387, 0.10745640099048615, 1.4292765855789185, -0.41170546412467957, 1.0704715251922607, 0.44120076298713684, -0.26865237951278687, 0.05431726947426796, 0.13100498914718628, -0.475352942943573, -0.22822235524654388, 0.4767358601093292, -0.645968496799469, -0.6498783230781555, 0.6708813309669495, -0.07940300554037094, 0.39855971932411194, 0.7227938771247864, -0.8973501920700073, -0.7206099629402161, 0.33036428689956665, 0.6186923384666443, 0.3458130359649658, 0.056536875665187836, -0.40298113226890564, 1.6914809942245483, -1.3734116554260254, 0.4957304894924164, 1.7827954292297363, 0.03513791784644127, 0.4681263267993927, 0.1048782616853714, -0.19825923442840576, 0.6378164291381836, 0.5706225633621216, -1.0056995153427124, -0.25563061237335205, -0.23499232530593872, -0.04721812531352043, 1.8720049858093262, -0.2899513840675354, -0.19831208884716034, 0.23754864931106567, 0.06086297705769539, 1.3536852598190308, -0.11502881348133087, -1.2926068305969238, 0.10812991112470627, -0.19420389831066132, 0.23719580471515656, 1.013921856880188, -0.3937413990497589, -0.0008596463594585657, -1.0259336233139038, -0.06564817577600479, 0.7108471989631653, -0.2934419810771942, -0.35315781831741333, -0.4844996929168701, -0.42966189980506897, 0.7182111740112305, 0.5721411108970642, 0.5132583379745483, -0.032949961721897125, 0.7584878206253052, 0.16932152211666107, 0.675772488117218, -0.5515410304069519, 0.24409639835357666, 0.10964928567409515, 0.5548189282417297, -0.5112625360488892, 0.6805052161216736, 0.17596304416656494, 1.0847829580307007, -0.43602505326271057, -0.2753473222255707, -1.007035732269287, 0.11572012305259705, 1.5609562397003174, 0.0827176570892334, 0.2978504002094269, -1.0337128639221191, 0.4097870886325836, -0.4883497655391693, 0.6920269727706909, -0.4945644438266754, -0.23552249372005463, 0.18530529737472534, -0.19506916403770447, -0.6244000792503357, -0.29650670289993286, 0.4961917996406555, -0.6633999347686768, 0.6327990889549255, -0.10441166162490845, -0.18219484388828278, 0.2779487669467926, 0.5816402435302734, 1.2057806253433228, 0.4583039879798889, 0.4073949158191681, 0.24368536472320557, 1.1751561164855957, -0.7938419580459595, -0.40540266036987305, -1.1120823621749878, 0.8840323090553284, -0.13493497669696808, 0.6629548072814941, -0.26705998182296753, -1.3446465730667114, -1.0397217273712158, -0.8308364152908325, -0.42624011635780334, -0.5638696551322937, 0.3583013713359833, 0.6911398768424988, -0.020182518288493156, -1.2464556694030762, 0.05701903998851776, 0.21124601364135742, -0.32300493121147156, 0.18748703598976135, -0.021287323907017708, 0.18438000977039337, -1.12055504322052, -1.2687678337097168, 0.6789600849151611, 0.006510572042316198, -0.4644017517566681, -0.442221999168396, 0.02374010905623436, -1.5553297996520996, -0.32334059476852417, 0.5618091821670532, -1.0529088973999023, 1.2999112606048584, 0.22047564387321472, -0.7665413022041321, 0.5754253268241882, -0.5326289534568787, 0.2684532105922699, 0.46172067523002625, -0.21277061104774475, -0.5302706956863403, 0.03653436899185181, -0.1476719081401825, 0.9174922704696655, 0.3493306040763855, -0.6223475933074951, -0.19274316728115082, 0.08601049333810806, 0.3415334224700928, 0.1366545557975769, 0.30977484583854675, 0.753276526927948, -0.5504946112632751, 0.14458699524402618, 0.4309462308883667, 0.9789490103721619, 0.025997068732976913, 0.2506192922592163, 0.07166628539562225, -1.3127899169921875, 0.5570504069328308, 0.2874907851219177, 0.7594616413116455, -0.8805858492851257, -0.3433036506175995, -0.40424060821533203, 0.3266776502132416, -0.4298728406429291, -1.1290818452835083, 0.7370184063911438, 0.05487530678510666, 0.29659050703048706, -0.24074241518974304, -1.066910743713379, 0.21479463577270508, 0.07705740630626678, -0.7595164179801941, -0.23509906232357025, 0.19540274143218994, 1.5765550136566162, -0.8523604869842529, -0.516577422618866, 0.06933273375034332, -0.10005054622888565, -0.750496506690979, 1.2496033906936646, -1.0250420570373535, 0.13340961933135986, -0.19026806950569153, -0.19215165078639984, -0.14647988975048065, -0.5401453971862793, 0.43002623319625854, -0.5850293636322021, 0.1039694994688034, 0.03532378375530243, -0.3610530495643616, 1.3351975679397583, -0.19825014472007751, 0.6086034178733826, -0.36690637469291687, -0.452918142080307, -0.06456250697374344, 0.33387279510498047, -0.8795104622840881, -0.6379950642585754, 0.3192731738090515, 0.06825310736894608, -0.5130836367607117, -0.6459567546844482, 0.4822014272212982, 0.7115093469619751, -0.30450883507728577, 0.2139468789100647, 0.2981964349746704, -0.7989760041236877, 0.6438810229301453, 0.8002564311027527, 0.9039522409439087, 0.5063915252685547, 0.4787651002407074, 0.5183984637260437, 0.4304804801940918, -0.5765058398246765, -0.7001579999923706, 1.0340474843978882, 0.4372495114803314, 0.8787953853607178, 0.3064604103565216, -0.7138324975967407, -0.211837500333786, -0.12952637672424316, 0.7196038365364075, 1.7751166820526123, 0.6215434074401855, -0.2201576828956604, -0.46189039945602417, -0.17223894596099854, -0.4383218586444855, 0.6126015186309814, -0.7292758226394653, -0.03985975682735443, -0.34696465730667114, -0.2561931610107422, 0.6677950024604797, 0.7231740951538086, 1.1119581460952759, -0.9827075600624084, -0.6630425453186035, -0.6890054941177368, -0.2143896222114563, -1.0511047840118408, -0.2093449831008911, -0.2539729177951813, -0.046277932822704315, -0.584494948387146, 0.274642676115036, -0.19434599578380585, 0.18633915483951569, -0.1382208615541458, 1.3157117366790771, -0.38436511158943176, -0.5245155692100525, 1.0503792762756348, 0.2517390251159668, -0.39979466795921326, -0.7601125240325928, -0.6392588019371033, -0.5093568563461304, -0.5307209491729736, 0.43303975462913513, 0.633205771446228, 0.14651349186897278, -0.038018885999917984, -0.7105658650398254, 0.12703131139278412, 0.20911814272403717, -0.2088528275489807, 0.6758842468261719, -0.4075890779495239, -0.1617678999900818, -0.7961930632591248, 0.8779788017272949, -0.005092016886919737, -0.3607390522956848, 0.7216558456420898, -0.35635918378829956, -0.5295287370681763, 0.17970043420791626, -0.7593882083892822, -0.3270542025566101, -1.2130812406539917, 0.9355073571205139, -0.3145202398300171, -0.693583071231842, 0.18016479909420013, 0.14053001999855042, 0.030746759846806526, 0.5976333618164062, 0.4377291798591614, 0.2929896116256714, 0.48473626375198364, 1.2306631803512573, -0.9888957142829895, 0.5125994682312012, 0.058248210698366165, 0.1232222244143486, 0.008564683608710766, -0.009760945104062557, -0.5797164440155029, -0.34259817004203796, -0.3993948698043823, -0.30605682730674744, -0.555942177772522, 0.6694034337997437, -0.6914126873016357, -0.9287389516830444, -0.30140450596809387, -1.4954776763916016, -0.40537774562835693, 0.23277565836906433, -0.545749843120575, -0.23736563324928284, -0.7943593263626099, -0.7157697081565857, -0.4047326147556305, -0.011434253305196762, -0.8584674000740051, 0.556978166103363, 0.05984678491950035, -0.7424660921096802, -0.4589264988899231, -0.016907457262277603, 0.09945594519376755, 0.6248996257781982, -0.4608057737350464, 0.9937612414360046, 0.16983847320079803, -0.5771445035934448, -0.3693172037601471, -0.1371568888425827, -0.010638226754963398, -0.2844758629798889, -0.06788108497858047, -0.681898295879364, 0.31104451417922974, -0.19558696448802948, -0.8143990635871887, 0.2645323574542999, 0.05576777085661888, 0.5084567666053772, 0.3794006109237671, -0.3423120975494385, -0.27989181876182556, 1.4648381471633911, -0.6853646039962769, -0.029933370649814606, 0.05274663120508194, 1.096360445022583, 0.6860780119895935, 0.008196876384317875, 0.42669400572776794, 0.9311758875846863, 0.11499421298503876, 0.6543278098106384, 0.23961882293224335, -0.20130325853824615, -0.20556338131427765, 0.5646715760231018, 0.7446582317352295, 0.03243410587310791, -0.028715956956148148, -1.1209626197814941, 0.6432525515556335, -1.5208837985992432, -0.6657305359840393, 0.3596012592315674, 0.6255719661712646, 0.12817618250846863, -0.7557353377342224, -0.47983241081237793, -0.6531435251235962, 0.32015523314476013, 0.20982390642166138, -0.509186863899231, -0.10542204976081848, -0.0891655907034874, 0.41139471530914307, -0.43244078755378723, 0.6551200151443481, -0.3418390452861786, 0.3070664703845978, 14.453544616699219, 0.819707989692688, 0.46971213817596436, 0.5753601789474487, 0.37781229615211487, 0.07377447187900543, -0.7593690156936646, -0.08507028222084045, -0.7135606408119202, -1.0042470693588257, 0.819853663444519, 0.48307687044143677, 0.462004154920578, 0.019755838438868523, -0.14238883554935455, -0.1587771624326706, -0.7233551144599915, 0.6211118698120117, 0.8776581883430481, -1.2504329681396484, 0.8005669116973877, -0.11220188438892365, 0.26593485474586487, 0.34341904520988464, 0.7008440494537354, 1.0658211708068848, 0.5147129893302917, -0.9324728846549988, 0.6192631721496582, 0.3351442813873291, 1.0247477293014526, -0.1524427980184555, 0.27391907572746277, 0.6415278911590576, -1.1848945617675781, -0.3839607536792755, -0.48113882541656494, -0.8875407576560974, 0.005425059702247381, -0.5828132629394531, -0.08539189398288727, -0.39414921402931213, -0.20139798521995544, 0.25831130146980286, -0.09263402223587036, 0.08845019340515137, -0.46102553606033325, -0.025886308401823044, 0.34528252482414246, -0.224482461810112, 0.12464556843042374, 0.6431705355644226, 0.17470449209213257, -0.5353591442108154, -0.065779909491539, 0.44921332597732544, 0.007989923469722271, 0.7896521091461182, -0.4070843458175659, 0.051723603159189224, -0.7133047580718994, -0.07577905803918839, 0.01467812154442072, 0.6039998531341553, 0.4775760769844055, 0.386789470911026, -0.7797134518623352, 0.170662522315979, 0.9334034323692322, 0.5120818614959717, -0.3988495469093323, -0.043018925935029984, -0.20346753299236298, -0.49084070324897766, 0.4401059150695801, 0.4701695442199707, -0.05080164968967438, -0.519237220287323, -0.39196690917015076, -0.44461384415626526, 0.394668310880661, -1.2172576189041138, -0.8522200584411621, 0.6721360087394714, -0.0717344656586647, -0.7014845013618469, 0.19516132771968842, -0.9168589115142822, -0.7830780148506165, -0.053299568593502045, -1.254046082496643, -1.3758792877197266, -0.29418227076530457, 0.037572264671325684, -0.0827617272734642, -0.023965492844581604, 1.23960280418396, -0.6401579976081848, 0.1079973354935646, -0.14261411130428314, -0.30537232756614685, -0.13947694003582, 0.001307677011936903, -0.8050333261489868, 0.3322804570198059, 0.07006941735744476, 0.11578070372343063, 0.2915554940700531, 0.24002023041248322, 0.07142607867717743, -0.8762512803077698, 0.01733948290348053, 0.538480818271637, -1.2637357711791992, -0.5177017450332642, -0.5298156142234802, -0.8870065808296204, 0.28894779086112976, 0.539895236492157, -0.018964026123285294, 0.45507270097732544, -0.15721495449543, -0.9795964956283569, 0.15570703148841858, -0.9313417673110962, 0.599877655506134, 0.2836358845233917, -0.815919041633606, -0.7884051203727722, 0.13001148402690887, 0.588694155216217, -0.8025307655334473, -0.269132137298584, -0.2978331446647644, 0.03790180757641792, -0.17343732714653015, 1.0292195081710815, -0.48457071185112, 1.005746841430664, 0.33074742555618286, -0.47929656505584717, -0.22524702548980713, 0.22372883558273315, -0.560689389705658, -0.28057488799095154, -0.10084420442581177, 0.5649888515472412, -0.3516119718551636, -0.3746030032634735, 1.2209970951080322, 0.6898666024208069, -0.49824267625808716, -0.6353151202201843, 0.0710386112332344, 0.3099362552165985, -0.7853574156761169, 0.15335842967033386, -0.33732858300209045, 0.05864493548870087, 0.2549593448638916, 0.7502784729003906, 1.0702513456344604, -0.07706615328788757, -0.3909267485141754, 0.6337831020355225, -0.12873496115207672, -0.2676530182361603, -0.5285100340843201, -0.23559528589248657, -1.3447308540344238, -0.08057039976119995, -0.9588896036148071, 0.2824949324131012, -1.447393774986267, -0.5106756091117859, 0.7857939004898071, 0.10317202657461166, -0.019801897928118706, 0.2558973729610443, -0.3574211895465851, -0.4098784923553467, -0.47643542289733887, -1.1571629047393799, 0.7167243361473083, 1.1932967901229858, -0.913432240486145, 0.3394087553024292, -0.30862680077552795, -0.031887125223875046, 0.3573562502861023, 0.19817382097244263, 0.08983027935028076, -1.1808758974075317, -1.0639948844909668, 0.5672007203102112, 0.06465625762939453, 0.42444390058517456, -0.5542945861816406, 1.033260464668274, 0.32842227816581726, -0.09009473770856857, -0.0649523138999939, 0.4627135992050171, -0.6647012233734131, -0.9032860994338989, 0.2276768833398819, -1.049418568611145, 0.08420686423778534, 0.39893078804016113, -0.27998512983322144, -0.3160150647163391, 0.5363723039627075, -0.06500132381916046, -1.261042594909668, -1.120886206626892, 0.058640509843826294, -0.654096782207489, 0.17158614099025726, 0.10744157433509827, 0.210241436958313, -1.1915313005447388, -0.7251074314117432, 0.04064864665269852, 0.8258954882621765, -0.2367212027311325, 0.6491270661354065, 1.0488207340240479, -1.0480327606201172, -0.19633141160011292, -0.01767430640757084, 0.48129257559776306, 0.4634082317352295, 0.8742223381996155, 0.2017005980014801, -0.3936556279659271, 0.2429213672876358, 0.429490327835083, 0.36787471175193787, -1.0521320104599, 0.14713986217975616, 0.8565327525138855, -0.1957162767648697, 0.02699410170316696, 1.2337597608566284, 0.032859060913324356, -1.0803074836730957, 0.12185139954090118, -0.8135325908660889, -1.088098168373108, -0.5386987328529358, 0.8089321255683899, -0.03587758541107178, -0.43745410442352295, 0.012659292668104172, -0.49713245034217834, 0.6198872327804565, -0.045817118138074875, -0.7731755375862122, 0.27876168489456177, -0.32350483536720276, -0.8422642946243286, 0.6896019577980042, 0.5038466453552246, -0.5457537770271301, -0.34698835015296936, -0.5518401265144348, -0.5182042121887207, 0.15021255612373352, 0.06603370606899261, -0.42891889810562134, 0.09927920252084732, 0.6715025305747986, 0.24690601229667664, 0.29176974296569824, 0.1202387660741806, 0.3940853774547577, -0.202339306473732, 1.0203831195831299, -0.14983059465885162, -0.2855727970600128, -0.04743093624711037, 0.9915770292282104, 1.8022754192352295, -1.202806830406189, 0.21113425493240356, -0.02470725029706955, -0.831313967704773, 1.223608374595642, 0.7902697324752808, -0.060130730271339417, 0.570229709148407, -0.4034806787967682, 0.320215106010437, -0.2123705893754959, -1.245803713798523, -0.2542996406555176, 1.1770356893539429, 1.275489330291748, 0.662970244884491, 0.24286919832229614, 0.3268730044364929, 0.5926408171653748, 0.3611428141593933, 0.16346819698810577, 0.650604248046875, 0.41571044921875, -0.38730815052986145, 0.2315220981836319, -0.035663675516843796, 0.3391195237636566, 0.16988779604434967, -0.6196075081825256, 0.08563315123319626, 0.8181169629096985, 0.2414221614599228, 1.1505706310272217, 0.7723031640052795, 0.592329204082489, 0.5770015716552734, 0.266580730676651, 0.932869017124176, -0.5494745969772339, 0.39328160881996155, -0.440047949552536, -0.6781920790672302, -0.22578473389148712, -0.5032736659049988, -0.24395425617694855, -0.9291597604751587, -0.01346100214868784, 0.391083687543869, -0.4247463047504425, -0.08581093698740005, 1.517062783241272, 0.4624759256839752, 0.5892243981361389, -0.8835239410400391, -0.12970425188541412, -0.32052502036094666, -0.8363049626350403, 0.38806939125061035, -0.46054235100746155, -0.25113803148269653, -0.4866972863674164, -0.30390310287475586, -0.45293816924095154]}, "authors": [{"authorId": "2146033755", "name": "Liangyu Chen"}, {"authorId": "2249781181", "name": "Boyi Li"}, {"authorId": "2255955638", "name": "Sheng Shen"}, {"authorId": "2295601", "name": "Jingkang Yang"}, {"authorId": "2243126534", "name": "Chunyuan Li"}, {"authorId": "2242659602", "name": "Kurt Keutzer"}, {"authorId": "2257173084", "name": "Trevor Darrell"}, {"authorId": "2145254462", "name": "Ziwei Liu"}], "references": [{"paperId": "039c0f0b4142823fe214ddf2470230e211c7b8ee", "title": "STAR: A Benchmark for Situated Reasoning in Real-World Videos"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "af3ab5da98e0807784b57e321ed887a3666a8ab6", "title": "Multimodal Foundation Models: From Specialists to General-Purpose Assistants"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "d47524cd5c3c4b57af2e5a29f6f91c420310f236", "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "d6d3604f369bb0415cbe814e43ca3131323b03e2", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning"}, {"paperId": "170c97c7215f42edfb20c2248f954879e91ef86e", "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"}, {"paperId": "e14c888548a3c15edb2c84e9f2127af7aba8cafb", "title": "Learning Situation Hyper-Graphs for Video Question Answering"}, {"paperId": "b9870e130f61ff900fe00dbcc5782c9b31773d32", "title": "Learning to Compress Prompts with Gist Tokens"}, {"paperId": "dca6c3927ade6481a1ae080f5c24decbfeced1be", "title": "Boosted Prompt Ensembles for Large Language Models"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "6e754273d54a91371efbc928cd6b156364d517da", "title": "ViperGPT: Visual Inference via Python Execution for Reasoning"}, {"paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c", "title": "Language Is Not All You Need: Aligning Perception with Language Models"}, {"paperId": "53d128ea815bcc0526856eb5a9c42cc977cb36a7", "title": "Toolformer: Language Models Can Teach Themselves to Use Tools"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "8fb24d7ae75d8009575c2316db90b181d0678fbc", "title": "VQA and Visual Reasoning: An Overview of Recent Datasets, Methods and Challenges"}, {"paperId": "70feb009bc1e8b1cb8dff64bf9fd67789636438b", "title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models"}, {"paperId": "e54b0f4cc59b3fae963f1675d7c55ea9c7eb1362", "title": "Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations"}, {"paperId": "cdef738dd79364d7f12ccad4eef0f38e2d37dd1c", "title": "Abstract Visual Reasoning with Tangram Shapes"}, {"paperId": "e89ed6bb1864558e3889f5f2fb8931643c633479", "title": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning"}, {"paperId": "af1c871282ec122869d03f5420ef5d9143358a91", "title": "Visual Programming: Compositional visual reasoning without training"}, {"paperId": "a5cb8f26acb71edd77ff9a143d3ddaab2367eb40", "title": "PromptCap: Prompt-Guided Task-Aware Image Captioning"}, {"paperId": "f3a13abf23afecf534c955954d70c3b0fc41d334", "title": "Composing Ensembles of Pre-trained Models via Iterative Consensus"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "26fd105d0b5a458979c012cddb3ba2de943388c4", "title": "Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"}, {"paperId": "65f056d32dac701240a52a5daf8cedb611b04ceb", "title": "Patching open-vocabulary models by interpolating weights"}, {"paperId": "69ded2668b5e59a8ba43c640de2cb71154472290", "title": "LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection"}, {"paperId": "9a66f6b5143d24aed9d3f4a8f13aa4e089f3a866", "title": "Reasoning about Actions over Visual and Linguistic Modalities: A Survey"}, {"paperId": "a8cf0f7a20f886acfb332071c2daaf58ba86a5ca", "title": "Recurrent Memory Transformer"}, {"paperId": "b17cc18e4130505b939f7d527082eb6be2a7fd5b", "title": "Rationale-Augmented Ensembles in Language Models"}, {"paperId": "a8fd9c1625011741f74401ff9bdc1c584e25c86d", "title": "Language Models are General-Purpose Interfaces"}, {"paperId": "d5bd67e5d438ee81f0f098ccbaf91246f6409c10", "title": "GAMR: A Guided Attention Model for (visual) Reasoning"}, {"paperId": "9c08d8fca57bac1998b79235f773cde27319a209", "title": "Sparse Mixture-of-Experts are Domain Generalizable Learners"}, {"paperId": "47a67e76ed84260ff19f7a948d764005d1edf1c9", "title": "A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"}, {"paperId": "7cdaa08890895e1ad92afb5fad429690ad7b1dac", "title": "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "354b48677e314ef2f47512c5a81723cfd17dd05d", "title": "Visual Spatial Reasoning"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "ada81a4de88a6ce474df2e2446ad11fea480616e", "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"}, {"paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2", "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"}, {"paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84", "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "5b44101b2372a33ec06e15ce4d20ad9a15518325", "title": "UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training"}, {"paperId": "02eae58e5ff7edb2f7cbb334e81c3af6b2768b59", "title": "A Review of Emerging Research Directions in Abstract Visual Reasoning"}, {"paperId": "bbc57e1b3cf90e09b64377f13de455793bc81ad5", "title": "Mixture-of-Experts with Expert Choice Routing"}, {"paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "59cb81d7763d8de23d9b4144c16287cb6808da1d", "title": "Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices"}, {"paperId": "ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45", "title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"}, {"paperId": "10bd4160b44803ada6a3d2e366c44b7e2a4ffe90", "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"}, {"paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7", "title": "MetaICL: Learning to Learn In Context"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "046a789748ede427cf9a22b18566a5dd7d65391d", "title": "A Diagnostic Study Of Visual Question Answering With Analogical Reasoning"}, {"paperId": "2672777d25562c9df6fc13b653181db62d39bece", "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "ac34c70ee85b048ad97328713c790f389656e4eb", "title": "Ecco: An Open Source Library for the Explainability of Transformer Language Models"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01", "title": "Scaling Vision with Sparse Mixture of Experts"}, {"paperId": "90357a6dc817e2f7cec477a51156675fbf545cf1", "title": "MERLOT: Multimodal Neural Script Knowledge Models"}, {"paperId": "c94bd1fc0ea4f7c67cd493f18f66d27e445fec40", "title": "Conversational question answering: a survey"}, {"paperId": "bc7adfeb554b8b86aa7e23975163c51ea225536d", "title": "Evaluating Deep Neural Network Ensembles by Majority Voting cum Meta-Learning scheme"}, {"paperId": "1c30efa04394f3e75d25ea1332a96cd354189dca", "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks"}, {"paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c", "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "0839722fb5369c0abaff8515bfc08299efc790a1", "title": "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"}, {"paperId": "346081161bdc8f18e2a4c4af7f51d35452b5cb01", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"}, {"paperId": "db9296eaa252231e24d066e8413bf29fb058ee45", "title": "Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering"}, {"paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a", "title": "Making Pre-trained Language Models Better Few-shot Learners"}, {"paperId": "1a9015e511ec3da873f6114eeb542905a92d7d62", "title": "KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA"}, {"paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1", "title": "Learning to summarize from human feedback"}, {"paperId": "c830b1d07becf502c1cfd85a68afbf3d188a9311", "title": "On the Importance of Diversity in Question Generation for QA"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "476ff888fe3917f92b221c522ffb7bfaa4e1861b", "title": "Open-Retrieval Conversational Question Answering"}, {"paperId": "f70e8d4115fb1547c3de19bd749de128d4be425d", "title": "StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics"}, {"paperId": "f0d74a6a345cde53df29ec39d5aa9db294c660fd", "title": "VisualCOMET: Reasoning About the Dynamic Context of a Still Image"}, {"paperId": "319983fdf503e8484d5409c75993c96d279a811f", "title": "e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations"}, {"paperId": "83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6", "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"}, {"paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d", "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "80ba19a949eee46af2bf58cd2f24949e009d884f", "title": "CLEVRER: CoLlision Events for Video REpresentation and Reasoning"}, {"paperId": "e2600cbc04da3284b61ef72223403f1dca3d2a98", "title": "A Weighted Majority Voting Ensemble Approach for Classification"}, {"paperId": "79c93274429d6355959f1e4374c2147bb81ea649", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers"}, {"paperId": "93d63ec754f29fa22572615320afe0521f7ec66d", "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"paperId": "5aec474c31a2f4b74703c6f786c0a8ff85c450da", "title": "VisualBERT: A Simple and Performant Baseline for Vision and Language"}, {"paperId": "874e9318c09c711ecd48a903b3824a3a03e2cd62", "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning"}, {"paperId": "8a1744da011375d711ed75fc2d160c6fdca2cf89", "title": "Deep Modular Co-Attention Networks for Visual Question Answering"}, {"paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70", "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"}, {"paperId": "50f76736c3090c6effac25400e5e40cc0b7b5ad9", "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision"}, {"paperId": "a7ac99d7cf3f568ab1a741392144b646b856ae0c", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"}, {"paperId": "3c54b796cc10cb530f77caa4d18e1c80ac863822", "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding"}, {"paperId": "6dfc2ff03534a4325d06c6f88c3144831996629b", "title": "From Recognition to Cognition: Visual Commonsense Reasoning"}, {"paperId": "913ce828d9cd564c3a0fbc726926c62c84b7ead0", "title": "Industrial Virtual Assistants: Challenges and Opportunities"}, {"paperId": "9d15ebe3f5aaf32a9f835f88703241461324c35b", "title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"}, {"paperId": "990a7b4eceedb6e053e6386269481bdfc42a1094", "title": "CoQA: A Conversational Question Answering Challenge"}, {"paperId": "c99179ca3784e3465fd9ed049d7f34b50d39393e", "title": "Ensemble learning: A survey"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "03eb382e04cca8cca743f7799070869954f1402a", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"}, {"paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e", "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"}, {"paperId": "3eda43078ae1f4741f09be08c4ecab6229046a5c", "title": "NewsQA: A Machine Comprehension Dataset"}, {"paperId": "5256b6d0ffe5b0dbcd979e2a8404326732b5ed51", "title": "Coarse-to-Fine Question Answering for Long Documents"}, {"paperId": "62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e", "title": "Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "c52acb4e4143ace520166691a29faaeaf892ac47", "title": "Extraction of Salient Sentences from Labelled Documents"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "a0456c27cdd58f197032c1c8b4f304f09d4c9bc5", "title": "Multiple Classifier Systems"}, {"paperId": "be90797bf5993526e66d108d2b11841e7634a5d3", "title": "Intelligent tutoring systems: An overview"}, {"paperId": "494aedf82da4755badc1fe74e4d21cf5fc029e9d", "title": "Programs with common sense"}, {"paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"}, {"paperId": "5f705919c2f25303d5df0178dd8bc3d1ceabe75f", "title": "CausalQA: A Benchmark for Causal Question Answering"}, {"paperId": null, "title": "Neuro-symbolic visual reasoning: Disentangling"}, {"paperId": "895ec0d51ee532a216024384ff374f6275ff749a", "title": "Intelligent Tutoring Systems"}, {"paperId": null, "title": "Prompting large language models with answer heuristics for knowledge-based visual question answering"}, {"paperId": "2aba84801935041774c1e2b749e0331efa322ed8", "title": "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence Learning from Natural Instructions"}]}