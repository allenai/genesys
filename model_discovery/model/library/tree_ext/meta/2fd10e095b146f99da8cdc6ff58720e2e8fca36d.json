{"paperId": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d", "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute", "abstract": "Large language models have become increasingly difficult to train because of the growing computation time and cost. In this work, we present SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling. SRU++ exhibits strong modeling capacity and training efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and Billion Word datasets, our model obtains better bits-per-character and perplexity while using 3x-10x less training cost compared to top-performing Transformer models. For instance, our model achieves a state-of-the-art result on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We further demonstrate that SRU++ requires minimal attention for near state-of-the-art performance. Our results suggest jointly leveraging fast recurrence with little attention as a promising direction for accelerating model training and inference.", "venue": "Conference on Empirical Methods in Natural Language Processing", "year": 2021, "citationCount": 41, "influentialCitationCount": 3, "openAccessPdf": {"url": "https://aclanthology.org/2021.emnlp-main.602.pdf", "status": "HYBRID"}, "tldr": {"model": "tldr@v2.0.0", "text": "This work presents SRU++, a highly-efficient architecture that combines fast recurrence and attention for sequence modeling that exhibits strong modeling capacity and training efficiency and suggests jointly leveragingFast recurrence with little attention as a promising direction for accelerating model training and inference."}, "embedding": {"model": "specter_v2", "vector": [0.4090327024459839, 0.2764624059200287, -0.3916517496109009, -0.05730659142136574, 0.11362604796886444, -0.1121482104063034, 0.7664329409599304, -0.36123350262641907, -0.6020210385322571, -0.3268979787826538, 0.7341464757919312, -0.3278334438800812, 0.6788280606269836, 0.15800884366035461, -0.12797079980373383, 0.4474214017391205, -0.6477058529853821, 0.45399558544158936, -0.17946504056453705, -0.12187749147415161, -0.05625627189874649, -0.6058553457260132, -1.0030912160873413, -0.07133441418409348, 0.3776628077030182, 0.2654074430465698, 0.5959888696670532, 0.8288648724555969, -0.6719077229499817, 0.974166214466095, 0.6790080666542053, -0.2514048218727112, 0.0275246761739254, -0.20877443253993988, -0.4880005717277527, -0.35865241289138794, 0.10210723429918289, -0.3921911418437958, -0.577131450176239, 0.7842309474945068, -0.36714228987693787, 0.40462854504585266, 0.5269932746887207, -0.4594508707523346, -0.29574355483055115, 1.008690357208252, 0.3724939823150635, 0.8224340677261353, -0.36071932315826416, -0.553829550743103, 1.345495343208313, -1.3482369184494019, 0.6059194803237915, 1.1841644048690796, 0.53050297498703, 0.5602417588233948, -0.13125792145729065, -0.6987749338150024, 1.0460572242736816, 0.2210140973329544, -0.9145707488059998, -0.3913167119026184, -0.12723635137081146, -0.06321972608566284, 2.3355085849761963, -0.320511132478714, 0.19144807755947113, 0.557975709438324, -0.2945330739021301, 1.5038609504699707, -0.32350441813468933, -0.5942564606666565, -0.45228904485702515, -0.40516218543052673, 0.6779116988182068, 0.9206356406211853, -0.3553692102432251, 0.1264580935239792, -0.8066884279251099, 0.01561387162655592, 0.5944271683692932, -0.0067299422807991505, 0.1000094935297966, 0.10765940696001053, -0.4045460820198059, 0.5334400534629822, 0.3996655344963074, 0.804486870765686, -0.32994338870048523, 1.0756727457046509, 0.5762782096862793, 0.14595884084701538, 0.15795479714870453, 0.20688016712665558, -0.05976822227239609, 0.024592973291873932, -0.8295280337333679, 0.03758100047707558, -0.10404010117053986, 1.142279863357544, -0.4448063373565674, 0.6165714859962463, -0.41609323024749756, 0.09929419308900833, 1.1201598644256592, -0.05241548269987106, 0.5410210490226746, -0.594138503074646, 0.06778065115213394, -0.7868475317955017, -0.15599702298641205, -0.41775140166282654, -0.08071037381887436, -0.4780389368534088, -1.0953524112701416, -1.4274296760559082, -0.5872287154197693, 0.311784952878952, -0.8228725790977478, 0.9140912890434265, -0.3536871671676636, 0.7355117797851562, -0.09739245474338531, 0.26944050192832947, 0.4841015636920929, 0.9000869989395142, 0.5082864165306091, 0.0012698709033429623, 0.9429529905319214, -1.022251844406128, -0.610170841217041, -1.145748257637024, 0.9530783891677856, -0.04595855250954628, 0.024742620065808296, -0.04121553525328636, -1.2473702430725098, -0.845029890537262, -0.6398274898529053, -0.26558539271354675, -0.35076287388801575, 0.2699465751647949, 1.100691556930542, 0.1864575445652008, -0.8717628121376038, 0.7045698761940002, -0.4822859466075897, 0.11489179730415344, 0.22113148868083954, -0.1769113391637802, 0.3543669879436493, -0.47468996047973633, -1.1817606687545776, 0.47867435216903687, 0.24351032078266144, -0.2945331633090973, -0.10001441836357117, -0.960547685623169, -1.1952317953109741, 0.07341780513525009, 0.055302415043115616, -0.40197843313217163, 1.3647176027297974, 0.019637465476989746, -1.377334475517273, 0.732998251914978, -0.7458190321922302, -0.14879900217056274, 0.16539223492145538, -0.19532202184200287, -0.4334360659122467, -0.7911847233772278, -0.4716987609863281, 0.4008704721927643, 0.17078325152397156, 0.38708510994911194, -0.15824545919895172, 0.23843595385551453, -0.6960734724998474, -0.4111630916595459, 0.01154883112758398, 1.2142221927642822, -0.3875415027141571, -0.20068897306919098, 0.2995786666870117, 0.49051305651664734, -0.16553401947021484, -0.5584632754325867, -0.23065319657325745, -1.3502329587936401, 0.8329859972000122, -0.20941025018692017, 1.0449004173278809, -0.7642567157745361, -0.4380374550819397, -0.2667565643787384, 0.03515184670686722, 0.09301237761974335, -0.7276639938354492, 0.43637025356292725, -0.5424708127975464, 0.3640837073326111, -0.07719442248344421, -1.1194541454315186, 0.016523679718375206, -0.25471335649490356, -0.833335816860199, -0.0692303404211998, 0.2660815417766571, 1.19034743309021, -1.0006519556045532, 0.1836918592453003, -0.08158542215824127, 0.5166535973548889, -1.1404492855072021, 1.1989998817443848, -0.3180692493915558, 0.05936582386493683, 0.11389032006263733, -0.4156944155693054, -0.05069132521748543, -0.5081887245178223, 0.6755357980728149, -0.3659683167934418, -0.41023051738739014, 0.745803952217102, -0.3192894160747528, 1.1685069799423218, -0.5890856385231018, 0.5672187805175781, -0.2746513783931732, -0.7121971845626831, 0.433382511138916, 0.13220830261707306, -0.4115905463695526, -0.4756574034690857, 0.357919305562973, 0.5115429162979126, -0.5056816339492798, 0.5926740169525146, 0.77022385597229, 1.0162163972854614, -0.3014518916606903, 0.3499840199947357, 0.46880701184272766, -0.10116156190633774, 0.6193000078201294, 0.5695359706878662, 0.6432254314422607, 0.4434843957424164, 0.3643516004085541, -0.13133276998996735, 0.2598752975463867, -0.9492642879486084, 0.062394093722105026, 0.44238075613975525, 0.7303430438041687, 0.7270306348800659, 0.3098446726799011, -0.8349012732505798, -0.7453429102897644, 0.5057530999183655, 0.6998247504234314, 1.530100703239441, -0.2376595288515091, -0.1921452432870865, -0.5911685228347778, 0.2561793327331543, -0.4702250063419342, 0.3361276090145111, -0.09310297667980194, -0.15108011662960052, -0.9126516580581665, -1.063524842262268, 0.8769092559814453, 0.28241947293281555, 0.803287148475647, -0.49818453192710876, -0.21973076462745667, -0.23998458683490753, 0.26420149207115173, -1.0088329315185547, -0.7045676112174988, 0.38028907775878906, -0.6371855139732361, 0.18661963939666748, 0.020853368565440178, -0.19097062945365906, 0.04666786268353462, -0.4338173568248749, 1.0474998950958252, -0.24642975628376007, -0.440659761428833, -0.1945641040802002, 0.7841175198554993, -0.40033695101737976, -0.829572856426239, 0.44128280878067017, 0.37456831336021423, -0.2137751579284668, 0.24476592242717743, 0.38511812686920166, 0.14979645609855652, -0.19023893773555756, -0.11717528104782104, 0.47345852851867676, -0.1955651193857193, -0.019364142790436745, 0.6090509295463562, -0.5722359418869019, -0.07009805738925934, -1.147963285446167, 0.3418594300746918, 0.20214241743087769, -0.6628091931343079, 0.3136674463748932, -0.5715290307998657, -0.19224055111408234, 0.520839273929596, -0.7960987091064453, -0.3245238959789276, -0.8115426301956177, 0.21014229953289032, -0.35714292526245117, 0.18076814711093903, 0.26871293783187866, 0.46280989050865173, 0.6121652126312256, -0.09963258355855942, 0.7372822165489197, 0.29795706272125244, -0.2289571464061737, 0.39256152510643005, -0.7775790691375732, 0.589717447757721, 0.3778557777404785, 0.0012097948929294944, -0.4461613893508911, -0.20524142682552338, -0.7615423798561096, -0.37718838453292847, -0.44281357526779175, -0.16713012754917145, -0.3147737383842468, 0.04816066473722458, -0.7775441408157349, -0.881470799446106, 0.17631545662879944, -1.4952119588851929, -0.3478792607784271, 0.5362421274185181, -0.2143578976392746, -0.02929047681391239, -1.0140509605407715, -1.3074169158935547, -0.790595531463623, -0.8261059522628784, -0.9731419682502747, 0.4525379240512848, 0.010796617716550827, -0.5097512006759644, -0.4786298871040344, 0.04634751379489899, -0.5143750905990601, 1.0278055667877197, -0.8700480461120605, 0.8409489393234253, 0.0670837014913559, -0.42840954661369324, -0.21577610075473785, 0.1845640391111374, 0.31237107515335083, -0.5615755319595337, 0.503287136554718, -0.755768895149231, 0.29195573925971985, -0.7921643853187561, -0.038710277527570724, 0.3979155719280243, 0.2906564474105835, 0.8234313726425171, 0.14221444725990295, -0.5859600901603699, 0.46779072284698486, 1.166801929473877, -0.6278923153877258, 0.3557294011116028, -0.07954993844032288, 1.1732207536697388, -0.2938375771045685, -0.4513636827468872, 0.4945616126060486, 0.4161054790019989, 0.38922059535980225, 0.15638917684555054, 0.025899112224578857, -0.12368170917034149, -0.6654548048973083, 0.648041307926178, 1.891461730003357, 0.08608175814151764, -0.06076328828930855, -1.1011065244674683, 0.6130421757698059, -1.1375837326049805, -1.024620532989502, 0.2402799278497696, 0.6775418519973755, 0.5334988832473755, -0.5028574466705322, -0.34877046942710876, -0.4797912836074829, 0.5704384446144104, 0.3439618647098541, -0.4073284864425659, -0.9023804068565369, -0.12801609933376312, 0.3912632465362549, 0.09335063397884369, 0.7191715836524963, 0.10570284724235535, 1.001705527305603, 14.6854887008667, 0.8080592155456543, -0.1845530867576599, 0.745545506477356, 0.9621146321296692, 0.002941844519227743, -0.5212296843528748, -0.17003360390663147, -1.6311520338058472, -0.13504701852798462, 1.3476864099502563, -0.07918225973844528, 0.5482209324836731, 0.3730030655860901, 0.0336308479309082, 0.4188934564590454, -0.46337777376174927, 0.7486679553985596, 0.34767037630081177, -1.5266190767288208, 0.5310093760490417, 0.05971125140786171, 0.32611793279647827, 0.8411187529563904, 0.5977293848991394, 1.0116195678710938, 0.7046576142311096, -0.6414473652839661, 0.48403608798980713, 0.3150288462638855, 0.8909924626350403, -0.23938481509685516, 0.31708645820617676, 0.7172982692718506, -0.942406415939331, 0.024371398612856865, -0.7029691338539124, -1.3575431108474731, 0.3239716589450836, 0.2744406759738922, -0.6118168830871582, -0.52820885181427, -0.49125540256500244, 1.0135682821273804, 0.49614977836608887, 0.2458844780921936, -0.14541831612586975, 0.72324138879776, -0.17541936039924622, -0.32572123408317566, 0.31438419222831726, 0.09881067276000977, 0.05728163570165634, 0.3320670425891876, 0.19726194441318512, 0.05149160325527191, -0.08151644468307495, 0.5052961707115173, -0.6086390018463135, -0.1796969175338745, -0.2882639169692993, -0.2725440561771393, 0.08197712153196335, 0.7356522083282471, 0.6116471290588379, -0.05662943795323372, -0.27912408113479614, 0.3316415250301361, 0.8406803011894226, -0.08191059529781342, -0.366610050201416, 0.23882457613945007, 0.424984872341156, -0.5843226313591003, -0.09572304040193558, 0.17175880074501038, -0.01318411622196436, -0.6319762468338013, -1.0091089010238647, -0.42866969108581543, 0.3047019839286804, -0.909878671169281, -0.45557042956352234, 0.9082161784172058, -0.5394487380981445, 0.07182060927152634, -0.043894678354263306, -0.7194504737854004, -0.22403840720653534, 0.5877777934074402, -1.4643052816390991, -0.7507209181785583, 0.3020727038383484, -0.506822407245636, -0.030502531677484512, 0.07802724093198776, 1.2553433179855347, 0.249954953789711, -0.6169887781143188, 0.060348521918058395, 0.09638529270887375, 0.09945515543222427, -0.9337045550346375, -0.5592290163040161, 1.2527183294296265, 0.4251822531223297, -0.014155425131320953, 0.293308287858963, 0.005446565803140402, 0.06282144039869308, -1.102263331413269, -0.28462448716163635, 1.2262808084487915, -0.7887012958526611, -0.13684670627117157, -0.9786043763160706, -0.8733033537864685, 0.5401917695999146, 0.27576038241386414, -0.3112589716911316, 0.15688805282115936, 0.40029028058052063, -0.8558897376060486, 0.05215323343873024, -0.10663450509309769, 0.3000229299068451, 0.8290309309959412, -0.8582684397697449, -0.02850009687244892, -0.1828393042087555, 0.4613029658794403, -0.824718713760376, -0.3571486175060272, -0.20969276130199432, 0.26153579354286194, 0.37493497133255005, 0.9824992418289185, -0.6914312243461609, 0.7953706383705139, 0.9032926559448242, 0.1474389284849167, -0.5598683953285217, -0.3862254023551941, -1.1163513660430908, -0.23920924961566925, 0.06352874636650085, 0.6187216639518738, -0.22544455528259277, -0.08565868437290192, 0.6667587161064148, 0.31477418541908264, -0.3609352111816406, -0.8069882392883301, -0.48998120427131653, 0.0009654968744143844, -0.576246440410614, 0.3356034755706787, 0.08569421619176865, 0.0038624547887593508, 0.2169504314661026, 0.040386926382780075, 0.33545830845832825, -0.4725542366504669, -0.7483235597610474, 0.2599957585334778, -0.1377260982990265, 0.11129345744848251, -0.8638070821762085, -0.4740264415740967, -1.5338759422302246, 0.3668341636657715, -1.3243504762649536, -0.0998692587018013, -0.8507084250450134, -0.2121184915304184, 0.11823035031557083, -0.19589638710021973, 0.11020369082689285, 0.07274604588747025, -0.3249472975730896, -0.6216832995414734, -0.6767110228538513, -0.4524577260017395, 0.8895040154457092, 0.6764199733734131, -0.6389568448066711, 0.26400479674339294, -0.20392939448356628, 0.3604484796524048, 0.0419689305126667, 0.41389200091362, -0.28520625829696655, -0.6187373399734497, -1.5268473625183105, 0.5490940809249878, -0.14662553369998932, -0.1464667171239853, -0.5150724053382874, 0.7311568856239319, 0.42522910237312317, -0.4769354462623596, -0.15046989917755127, 0.1603541076183319, -0.5930854678153992, -0.0985068827867508, 0.6663869023323059, -0.7885367274284363, 0.5971981883049011, 0.22165493667125702, -0.6617552042007446, -0.020469030365347862, 0.6437613368034363, -0.0186309814453125, -1.0297516584396362, -0.8322121500968933, 0.5304123759269714, -0.7371340990066528, 0.09379994124174118, -0.536971390247345, 0.06774459779262543, -0.9894090890884399, -0.3517124652862549, 0.328864723443985, 0.2649906873703003, -0.5198042392730713, 0.9815793037414551, 0.2848588228225708, -0.978468656539917, -0.2399703860282898, 0.21395708620548248, -0.2813730537891388, -0.07134363055229187, 0.49466535449028015, 0.37185463309288025, -0.21088390052318573, 0.644474446773529, 0.6134617328643799, 0.2689681053161621, -1.148985505104065, 0.16984692215919495, 0.7930587530136108, -0.5751087069511414, -0.5827105045318604, 1.0991184711456299, -0.2879181504249573, -1.0961744785308838, 0.0504731684923172, -1.2246347665786743, -0.39078831672668457, -0.3782179355621338, 0.8577030301094055, 0.19112101197242737, -0.08165640383958817, 0.03714536130428314, -0.5804176926612854, 0.06022632494568825, -0.38992854952812195, -0.28292155265808105, 0.8311342000961304, 0.10527203232049942, -0.5785998106002808, 0.6567639708518982, 0.8967360854148865, -0.9575690031051636, -0.41045209765434265, -0.9514535665512085, -0.25850486755371094, 0.2354869395494461, 0.6002948880195618, -0.08177065849304199, -0.5756769180297852, 0.802270233631134, 0.1644972264766693, 0.17429868876934052, -0.1735055297613144, -0.23326463997364044, 0.133153036236763, 0.5970718264579773, 0.1390775740146637, -0.6294005513191223, -0.8064190745353699, 1.964044451713562, 1.2461321353912354, -0.6139672994613647, 0.11755377054214478, -0.37118789553642273, -0.5952381491661072, 0.6017537713050842, 0.13658180832862854, -0.23925656080245972, 0.7177577614784241, -0.075350321829319, -0.12387831509113312, 0.28104376792907715, -1.332321286201477, -0.14093740284442902, 0.6705883741378784, 0.7569766640663147, 1.0819852352142334, 0.1148483157157898, 0.1251981258392334, 0.8191905617713928, 0.16437757015228271, 0.19439561665058136, 0.4248395264148712, 0.38080206513404846, -0.05368423089385033, 0.2972052991390228, 0.4775589108467102, 0.47084635496139526, -0.5525040030479431, -1.118153691291809, 0.3388347029685974, 0.36445027589797974, -0.257625937461853, 0.19624726474285126, 1.1607487201690674, 0.5123189687728882, 0.33636781573295593, 0.3279709219932556, 0.519001305103302, -0.35522037744522095, -0.1828596293926239, -0.25678136944770813, -0.5602137446403503, 0.011792117729783058, 0.0398515947163105, -0.5698391795158386, -0.4676109254360199, -0.5748486518859863, 0.16175509989261627, -0.06379484385251999, 0.36119285225868225, 1.1863205432891846, 0.5799667239189148, 0.6655464172363281, -0.32125499844551086, -0.30211201310157776, -0.3386809229850769, -0.9798488616943359, 0.16916008293628693, -0.5692907571792603, -0.1940717101097107, -0.046095650643110275, -0.015000642277300358, -0.24597932398319244]}, "authors": [{"authorId": "49986267", "name": "Tao Lei"}], "references": [{"paperId": "51b5db5c679be0ce9a39a2ee21def42bca165efe", "title": "Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "feeae38fd404fdc17cad19d80461843059216fde", "title": "Characterizing signal propagation to close the performance gap in unnormalized ResNets"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "4a41a98d40fab83a2ac9010ca097e3b2cff4b6da", "title": "Efficient Inference For Neural Machine Translation"}, {"paperId": "bffbf4931170d3fcb47b8b1aae6df1066302e825", "title": "Autoregressive Knowledge Distillation through Imitation Learning"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "cd4ffe5e014601a3d6b64121355d29a730591490", "title": "Fast Transformers with Clustered Attention"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "0170fc76e934ee643f869df18fb617d5357e8b4e", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"}, {"paperId": "8d908042f139575d6688c745e94156c9df6eae07", "title": "Understanding the Difficulty of Training Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "4076e421d1758fdb68411242044cd45747b7e35b", "title": "PowerNorm: Rethinking Batch Normalization in Transformers"}, {"paperId": "c79a8fd667f59e6f1ca9d54afc34f792e9079c7e", "title": "TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "8890eeda67d02117a589b0ba41c69419c40c7d5e", "title": "Accessing Higher-level Representations in Sequential Transformers with Feedback Memory"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "b9ed2fd3237539b0ad539dad8bdee15efbe0a26e", "title": "Single Headed Attention RNN: Stop Thinking With Your Head"}, {"paperId": "40922d386116975853a743b1d810c1e0f03e886a", "title": "Understanding and Improving Layer Normalization"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "ba08784bb30de51f72f88d5d64a64310d030db10", "title": "From Research to Production and Back: Ludicrously Fast Neural Machine Translation"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "3d109085f37405370a765dea876fd654e5ea3a1c", "title": "Optimizing Speech Recognition For The Edge"}, {"paperId": "2bf7c350a8280e7c593d46a60127f99b21517121", "title": "On the Variance of the Adaptive Learning Rate and Beyond"}, {"paperId": "3c5f1ab37f70db503636075e15b3173f86eea00b", "title": "Green AI"}, {"paperId": "830995ef17cc291c13f42dfd9f462137de1d2179", "title": "Augmenting Self-attention with Persistent Memory"}, {"paperId": "60396a7643fd25fba52d73028bc563f5ad651bb6", "title": "A Lightweight Recurrent Network for Sequence Modeling"}, {"paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13", "title": "Adaptive Attention Span in Transformers"}, {"paperId": "528b5f5356bc7ad91edc4dc074b0273e1e55fb03", "title": "Modeling Recurrence for Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f", "title": "Self-Attention with Relative Position Representations"}, {"paperId": "fa9decd1395cc2f39e9921f870ebc8a8ec2bd08d", "title": "Dynamic Evaluation of Neural Sequence Models"}, {"paperId": "7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7", "title": "Simple Recurrent Units for Highly Parallelizable Recurrence"}, {"paperId": "ae8d5be3caea59a21221f02ef04d49a86cb80191", "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "8501e330d78391f4e690886a8eb8fac867704ea6", "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks"}, {"paperId": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "title": "A Structured Self-attentive Sentence Embedding"}, {"paperId": "2d876ed1dd2c58058d7197b734a8e4d349b8f231", "title": "Quasi-Recurrent Neural Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "title": "Efficient softmax approximation for GPUs"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "56172b6fe2613c37d9790bde8ab6ccda14b35678", "title": "Persistent RNNs: Stashing Recurrent Weights On-Chip"}, {"paperId": "8e072ff7df01f944e08d40203474f49ea8521d57", "title": "Optimizing Performance of Recurrent Neural Networks on GPUs"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "5d833331b0e22ff359db05c62a8bca18c4f04b68", "title": "One billion word benchmark for measuring progress in statistical language modeling"}, {"paperId": "6471fd1cbc081fb3b7b5b14d6ab9eaaba02b5c17", "title": "Generating Sequences With Recurrent Neural Networks"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": null, "title": "Tuning weight decay and learning rate We find that tuning the weight decay and learning rate critical to the success of training SRU++ and achieving best results"}, {"paperId": "d49becf9c1203efb89a6336f8bcfed771a1a3e98", "title": "Fully Neural Network Based Speech Recognition on Mobile and Embedded Devices"}, {"paperId": null, "title": "2017), which results in a rounded learning rate of 0.0004"}, {"paperId": "0f30ffc16bfe9d0ad3fe0a721ad4fb636889e3dd", "title": "On a Chip"}, {"paperId": null, "title": "The human knowledge compression contest"}]}