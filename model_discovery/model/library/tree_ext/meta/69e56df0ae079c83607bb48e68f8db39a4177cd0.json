{"paperId": "69e56df0ae079c83607bb48e68f8db39a4177cd0", "title": "GiT: Towards Generalist Vision Transformer through Universal Language Interface", "abstract": "This paper proposes a simple, yet effective framework, called GiT, simultaneously applicable for various vision tasks only with a vanilla ViT. Motivated by the universality of the Multi-layer Transformer architecture (e.g, GPT) widely used in large language models (LLMs), we seek to broaden its scope to serve as a powerful vision foundation model (VFM). However, unlike language modeling, visual tasks typically require specific modules, such as bounding box heads for detection and pixel decoders for segmentation, greatly hindering the application of powerful multi-layer transformers in the vision domain. To solve this, we design a universal language interface that empowers the successful auto-regressive decoding to adeptly unify various visual tasks, from image-level understanding (e.g., captioning), over sparse perception (e.g., detection), to dense prediction (e.g., segmentation). Based on the above designs, the entire model is composed solely of a ViT, without any specific additions, offering a remarkable architectural simplification. GiT is a multi-task visual model, jointly trained across five representative benchmarks without task-specific fine-tuning. Interestingly, our GiT builds a new benchmark in generalist performance, and fosters mutual enhancement across tasks, leading to significant improvements compared to isolated training. This reflects a similar impact observed in LLMs. Further enriching training with 27 datasets, GiT achieves strong zero-shot results over various tasks. Due to its simple design, this paradigm holds promise for narrowing the architectural gap between vision and language. Code and models will be available at \\url{https://github.com/Haiyang-W/GiT}.", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "GiT is a multi-task visual model, jointly trained across five representative benchmarks without task-specific fine-tuning, which builds a new benchmark in generalist performance, and fosters mutual enhancement across tasks, leading to significant improvements compared to isolated training."}, "embedding": {"model": "specter_v2", "vector": [0.931071937084198, 0.5727590322494507, -0.06485603004693985, 0.1876533180475235, -0.3312775492668152, 0.17584948241710663, 0.7054626941680908, -0.4352959990501404, -0.4701506793498993, -0.8866927027702332, 0.07420629262924194, 0.08822950720787048, 0.8023748397827148, 0.0707581415772438, -0.22192510962486267, 0.21379207074642181, -0.502586841583252, 0.1109674945473671, 0.4808558523654938, -0.6382299065589905, -0.21319597959518433, -0.7595630288124084, -1.0934144258499146, 0.25416165590286255, 0.004247371107339859, 0.9585513472557068, 0.7671928405761719, 1.2670897245407104, -0.053594090044498444, 0.6275810599327087, 0.491598904132843, -0.38334351778030396, 0.036378491669893265, -0.0022556197363883257, -0.23108966648578644, 0.32149040699005127, 0.6985629796981812, -0.40758296847343445, -0.6554136276245117, 0.99226975440979, -0.15501457452774048, 0.22768352925777435, 0.7632871270179749, -0.8252585530281067, -0.8003875017166138, 0.47567158937454224, 0.3826473653316498, 0.3898085653781891, -0.4483010470867157, -0.2123701274394989, 1.726696491241455, -1.5006625652313232, 0.508857011795044, 1.7937092781066895, 0.3208511471748352, 0.7543746829032898, -0.052057720720767975, -0.3435317873954773, 1.0066744089126587, -0.07721056044101715, -0.6196748614311218, -0.21397390961647034, -0.3701304793357849, -0.1372612863779068, 1.8195021152496338, -0.7617955207824707, -0.06254463642835617, 1.098211407661438, 0.2988292872905731, 1.6323192119598389, -0.2593321204185486, -0.7291637659072876, -0.4846905767917633, -0.22577878832817078, 0.16307950019836426, 0.8380627036094666, -0.15669427812099457, 0.28518322110176086, -0.42673102021217346, 0.5406560897827148, 0.7538444995880127, -0.06266618520021439, -0.0941111296415329, -0.36172574758529663, -0.4049304723739624, 0.8325408697128296, 0.8247761726379395, 0.5603203177452087, 0.13175606727600098, 1.0168429613113403, 0.7235131859779358, -0.10441494733095169, -0.17809036374092102, 0.06566539406776428, -0.1463070511817932, 0.9567421078681946, -0.7681079506874084, 0.09345409274101257, -0.016013016924262047, 1.1216100454330444, -0.1983683556318283, 0.2748098373413086, -0.8640224933624268, 0.04023643583059311, 1.628399133682251, 0.3842049837112427, 0.13650141656398773, -0.5533286929130554, 0.04966697469353676, -0.9732696413993835, 0.11531630903482437, -0.8346792459487915, 0.09938448667526245, -0.33611905574798584, -0.7869942784309387, -1.0208507776260376, -0.39958566427230835, 0.5137979984283447, -1.6754720211029053, 0.7874627709388733, -0.5687640309333801, 0.34157392382621765, 0.011148005723953247, 0.44360244274139404, 0.9600807428359985, 0.5003430843353271, 0.778754711151123, 0.529758095741272, 1.224820613861084, -1.068326473236084, -0.4163683354854584, -1.2888894081115723, 0.0459032878279686, -0.26549774408340454, 0.37175843119621277, 0.038198322057724, -0.8159578442573547, -1.521763801574707, -0.7478384375572205, -0.7393556237220764, -0.5822877287864685, 0.6266846060752869, 1.0199854373931885, 0.5457754135131836, -1.3593940734863281, 0.21515725553035736, 0.044385358691215515, -0.5058983564376831, 0.6325922608375549, -0.11034609377384186, 0.27223166823387146, -0.4649297297000885, -0.8099971413612366, 0.4591418504714966, 0.08657176792621613, -0.20604175329208374, -0.9210836291313171, -0.046206872910261154, -1.5482944250106812, -0.16341497004032135, 0.4594559967517853, -0.9107348322868347, 1.347501516342163, -0.41845735907554626, -0.870710015296936, 0.9267858266830444, -0.7883707880973816, -0.07581926137208939, 0.049514349550008774, -0.5176813006401062, -0.24972417950630188, -0.22408050298690796, -0.03008953481912613, 1.0565558671951294, 0.9836699366569519, -0.3098961114883423, -0.26643526554107666, 0.39642003178596497, -0.23033440113067627, -0.24941985309123993, 0.04665643721818924, 0.9471733570098877, -0.9487972259521484, -0.1021975576877594, 0.4748136103153229, 0.6347392201423645, -0.15739353001117706, 0.0745602697134018, -0.31923019886016846, -0.91102534532547, 1.0226380825042725, -0.025898432359099388, 0.10525057464838028, -0.9718005061149597, -0.6981062889099121, -0.39833977818489075, 0.17968836426734924, -0.1760844588279724, -1.098271369934082, 0.13882651925086975, -0.08344056457281113, 0.25319066643714905, -0.21339115500450134, -1.526753306388855, 0.07030493021011353, -0.054119132459163666, -0.7097577452659607, -0.020531926304101944, 0.6208288669586182, 1.4018954038619995, -0.9845074415206909, -0.21738483011722565, 0.19463151693344116, 0.27398231625556946, -1.0540671348571777, 1.00733482837677, -0.8990017771720886, -0.017887311056256294, -0.3593865931034088, 0.08958730846643448, -0.13926205039024353, -0.4612119495868683, 0.1353154182434082, -0.7826166152954102, 0.23747467994689941, 0.08315695077180862, -0.009097793139517307, 1.3809019327163696, -0.13230477273464203, 0.9798039197921753, -0.11881409585475922, -0.6777176856994629, 0.5831447839736938, 0.4708281457424164, -0.6738276481628418, -0.7009536623954773, 0.8281310200691223, 0.2732712924480438, -0.5677770972251892, 0.28439652919769287, 0.8254340291023254, 1.037318468093872, -0.44891464710235596, -0.21952885389328003, 0.7789621353149414, -0.5676838159561157, 0.34336960315704346, 0.7261960506439209, 0.4826568365097046, 0.6130521893501282, 0.27404487133026123, 0.047723688185214996, 0.4902435839176178, -1.0339096784591675, -0.29711562395095825, 1.0180898904800415, 0.34181228280067444, 1.746589183807373, 0.3951442241668701, -0.6927650570869446, -0.5699759721755981, 0.016554392874240875, 0.8805972337722778, 1.4229707717895508, 0.13166894018650055, -0.10200711339712143, -0.585870087146759, 0.09529120475053787, -0.5226685404777527, -0.2965330183506012, -0.8020936846733093, -0.07737556844949722, 0.015997694805264473, -0.7750005722045898, 0.6233508586883545, 0.3050495684146881, 1.30868399143219, -0.7437202334403992, -0.25681397318840027, -0.46009013056755066, 0.07036246359348297, -1.2985068559646606, -0.5331613421440125, 0.2713605761528015, 0.04580223560333252, -0.32299086451530457, -0.0740637332201004, -0.17925862967967987, 0.2531760334968567, -0.3174121677875519, 0.9768410921096802, -0.5087260007858276, -0.9293480515480042, 0.7773672938346863, 0.41963768005371094, -0.4931843876838684, -0.7395548820495605, -0.13692277669906616, -0.37649667263031006, 0.1091487780213356, 0.34699156880378723, 0.8128170371055603, -0.16449135541915894, 0.029421187937259674, -0.34145066142082214, 0.18791355192661285, 0.2759346067905426, -0.04440693557262421, 1.0860310792922974, -0.8102644681930542, -0.11712507903575897, -0.9555395841598511, 0.5186930298805237, 0.2532169818878174, -0.09931579977273941, 0.27224552631378174, -0.2296818345785141, -0.7514570355415344, 0.2823096513748169, -0.6866618990898132, -0.43157604336738586, -0.3327849209308624, 0.33241215348243713, -0.709631085395813, -0.4983491599559784, -0.4970805048942566, 0.07176966220140457, -0.09977777302265167, 0.47465959191322327, 0.6584195494651794, 0.3082630932331085, -0.02429676242172718, 0.9658241868019104, -1.2421071529388428, 1.027296781539917, 0.15900400280952454, 0.12801045179367065, 0.0423336885869503, -0.11212877184152603, -0.6903413534164429, -0.5109105706214905, -0.6627839803695679, -0.2844552993774414, -0.5920718908309937, 0.9884268045425415, -1.0011661052703857, -0.6518352031707764, 0.05830018222332001, -1.2747719287872314, -0.1813739538192749, 0.27065685391426086, -0.6738632321357727, -0.526189386844635, -0.9309503436088562, -0.8805353045463562, -0.32734519243240356, -0.5719147324562073, -1.0661903619766235, 0.6372936964035034, 0.3428059220314026, -0.21638986468315125, -0.23989583551883698, -0.20422250032424927, -0.20903322100639343, 0.7999644875526428, -0.08238454908132553, 0.32947513461112976, -0.1841554492712021, -0.6953336596488953, -0.19102221727371216, -0.3673420548439026, 0.9232760071754456, -0.4866093099117279, 0.5273937582969666, -1.3214353322982788, 0.2107580155134201, -0.38666051626205444, -0.22703707218170166, 0.9311151504516602, 0.44138288497924805, 0.42493098974227905, 0.6646358370780945, -0.45705941319465637, 0.4417561888694763, 1.5325736999511719, -0.8452893495559692, 0.49572527408599854, 0.13340160250663757, 1.1335939168930054, -0.08581805974245071, -0.25430741906166077, 0.3513023257255554, 0.498164564371109, -0.24341534078121185, 0.5498306155204773, -0.5433652400970459, -0.802844226360321, -0.6648333072662354, 0.6945675611495972, 0.5277575850486755, 0.23068192601203918, -0.22745266556739807, -1.0239571332931519, 0.9915853142738342, -1.0538554191589355, -0.6807349324226379, 0.3469696044921875, 0.5254278182983398, -0.22266685962677002, -0.2692382037639618, -0.5094780325889587, -0.7144147157669067, 0.8365642428398132, 0.3733944296836853, -0.3437090516090393, -0.7139779329299927, -0.28408801555633545, 0.4545377194881439, 0.06510349363088608, 0.4204302430152893, -0.7515038251876831, 0.8667737245559692, 14.028743743896484, 0.7547900676727295, -0.18003176152706146, 0.556128203868866, 1.0601191520690918, 0.6502902507781982, -0.29417163133621216, 0.07712563872337341, -1.2343230247497559, -0.833793580532074, 0.8252012729644775, 0.3667150139808655, 0.027028709650039673, 0.46840086579322815, -0.21445158123970032, 0.21384933590888977, -0.6849202513694763, 0.7922535538673401, 0.8288828730583191, -1.0813992023468018, 0.6527953743934631, 0.028824130073189735, 0.22611942887306213, 0.8557019233703613, 1.0450557470321655, 0.9925912022590637, 0.375997394323349, -0.5992063283920288, 0.5521990656852722, 0.026704169809818268, 0.7970227599143982, 0.4213750660419464, -0.06759028136730194, 0.13892434537410736, -1.2906216382980347, -0.30485329031944275, -0.7485459446907043, -1.1196963787078857, 0.14598341286182404, -0.3355847895145416, -0.4647163450717926, -0.5223991870880127, 0.004174459725618362, 0.9096654653549194, -0.0805189236998558, 0.6501904129981995, -0.403768390417099, 0.4410153329372406, -0.2743743062019348, 0.11183594912290573, 0.45854175090789795, 0.6023041009902954, 0.34246107935905457, 0.11280391365289688, -0.07300688326358795, 0.10595539957284927, 0.26132047176361084, 0.7789904475212097, -0.422518253326416, -0.27613565325737, -0.32053083181381226, -0.011448351666331291, -0.6678639650344849, 0.9782091379165649, -0.2590658962726593, 0.30535510182380676, -0.4866359829902649, 0.4703104496002197, 0.19950196146965027, 0.3696574568748474, -0.4928986430168152, 0.22832289338111877, 0.17790785431861877, -0.7262853384017944, 0.8978981375694275, 0.3407953381538391, 0.04180110991001129, -0.7545576095581055, -0.81083744764328, -0.29817265272140503, 0.17290352284908295, -0.9255273342132568, -0.664413571357727, 1.0384963750839233, -0.40465638041496277, 0.006368342787027359, 0.29858776926994324, -1.123382329940796, -0.5794348120689392, 0.22892965376377106, -1.5755864381790161, -1.4042539596557617, 0.01409926638007164, 0.01290815882384777, 0.0464620403945446, -0.06939145922660828, 0.8580167889595032, -0.10818743705749512, -0.17336170375347137, 0.11448806524276733, -0.383685827255249, 0.2925492525100708, -0.3045312762260437, -0.5617194175720215, 1.037569284439087, 0.6240115761756897, 0.07550186663866043, -0.16048866510391235, 0.26287856698036194, 0.10897557437419891, -0.8477574586868286, -0.1101800873875618, 0.5735750794410706, -0.7517839670181274, -0.3954087495803833, -0.8357130289077759, -0.6391916275024414, 0.27097225189208984, 0.5103183388710022, 0.06333568692207336, -0.2799145579338074, 0.13595841825008392, -1.083013653755188, 0.15086184442043304, -0.6980089545249939, 0.10200512409210205, 0.1489056795835495, -0.9341272711753845, -0.22205112874507904, 0.06986628472805023, 0.46437910199165344, -0.9179638624191284, -0.06044822186231613, -0.20562587678432465, 0.18031533062458038, -0.1284012496471405, 1.3559539318084717, -0.030747568234801292, 0.751571774482727, 0.6289936900138855, -0.42188456654548645, -0.144097238779068, 0.07894254475831985, -0.8929522037506104, 0.3375301957130432, 0.2876206040382385, 0.5716258883476257, -0.3226965367794037, -0.24525126814842224, 0.7756685614585876, 0.3679450452327728, -0.3365899324417114, -0.5827010273933411, -0.07396500557661057, 0.11056385189294815, -0.8762561082839966, -0.37713152170181274, -0.7829561829566956, -0.4695931673049927, 0.16298538446426392, 0.23949921131134033, 0.6993881464004517, 0.0558176226913929, -0.7660276889801025, 0.6448218822479248, -0.08547171205282211, -0.37621891498565674, -0.22102223336696625, -0.6795833706855774, -1.765825629234314, -0.21611782908439636, -0.9212122559547424, 0.4350075125694275, -1.153690218925476, -0.3697548806667328, 0.5584855675697327, -0.4006879925727844, 0.28982114791870117, 0.33585745096206665, 0.19732581079006195, 0.3763806223869324, -0.4649128019809723, -0.9800365567207336, 0.6792663335800171, 1.3786686658859253, -0.8503333926200867, 0.4830192029476166, -0.1348273605108261, -0.12068982422351837, 0.4799513816833496, 0.036227036267519, -0.14910277724266052, -0.8749692440032959, -1.2536723613739014, 0.25872790813446045, -0.22983253002166748, 0.49827462434768677, -1.2261747121810913, 0.8629567623138428, 0.9276776313781738, 0.030969614163041115, 0.09138735383749008, 0.7062650918960571, -0.6305879354476929, -1.0853924751281738, 0.2541392147541046, -0.4985182285308838, -0.1589435636997223, 0.48753997683525085, -0.15612059831619263, -0.3854231536388397, 1.0156794786453247, 0.09204093366861343, -1.229527235031128, -1.4672690629959106, 0.4762934446334839, -0.34800994396209717, -0.0033939448185265064, -0.22241610288619995, -0.08691854029893875, -1.1961216926574707, -0.4227229356765747, -0.35040366649627686, 0.1845712810754776, -0.5703377723693848, 1.099375605583191, 1.248528242111206, -0.32410475611686707, -0.2603282928466797, 0.282715380191803, -0.07215553522109985, 0.18527068197727203, 0.651579737663269, 0.22729437053203583, -0.15355148911476135, 0.3683111071586609, 0.18065883219242096, -0.007840607315301895, -0.7111406922340393, 0.12980954349040985, 1.2191747426986694, -0.15155142545700073, -0.5243504643440247, 1.4202423095703125, 0.5203652381896973, -0.5773031711578369, 0.175435870885849, -1.2759852409362793, -0.638275146484375, -0.26163560152053833, 0.6392033696174622, -0.5820798873901367, -0.4622383117675781, -0.39278313517570496, -0.392463743686676, 0.6004526019096375, -0.5476606488227844, -0.5812152028083801, 0.1940881758928299, -0.1875396966934204, -0.24301643669605255, 0.332681804895401, 0.9042915105819702, -1.4581722021102905, -1.02128005027771, -0.6862487196922302, -0.8630161881446838, 0.04201728105545044, 0.2091006487607956, -0.3282375633716583, -0.6710505485534668, 1.0670499801635742, 0.6268911957740784, 0.04447818547487259, 0.39813879132270813, 0.0431780144572258, 0.04836178198456764, 0.6865283846855164, -0.3326808214187622, -0.21630805730819702, -0.20774327218532562, 1.3871197700500488, 1.2618134021759033, -0.9162418246269226, 0.10775277763605118, -0.5329461693763733, -0.6977246403694153, 0.9071356058120728, 0.3815309405326843, -0.5522510409355164, 0.6506954431533813, -0.473224014043808, -0.1827366054058075, 0.26391252875328064, -0.8990098237991333, -0.7883281111717224, 0.9775568246841431, 1.5307780504226685, 0.5772398710250854, 0.001742189982905984, 0.33776241540908813, 0.5687336921691895, 0.2861974537372589, 0.1422010213136673, 0.7117422223091125, -0.11424695700407028, -0.5559172034263611, 0.2809355556964874, 0.06339441984891891, 0.39569056034088135, -0.6720523238182068, -0.7566249966621399, 0.2624107897281647, 0.7408493757247925, 0.3577861785888672, 0.772338330745697, 0.8785734176635742, 0.42149272561073303, 0.5403802990913391, 0.07814318686723709, 0.661190390586853, -0.6864722371101379, -0.0920938178896904, -0.049842528998851776, -1.0682750940322876, -0.06386013329029083, -0.3825523853302002, -0.538463294506073, -0.24105270206928253, 0.30807480216026306, 0.48180150985717773, -0.7939305305480957, 0.29338884353637695, 1.2553025484085083, 0.1855410784482956, 0.6981562376022339, -0.8185114860534668, -0.6379404067993164, -0.03982803598046303, -0.714124321937561, 0.2793411910533905, -0.5085188746452332, 0.16147811710834503, -0.5200599431991577, -0.019505662843585014, 0.114928238093853]}, "authors": [{"authorId": "2118371876", "name": "Haiyang Wang"}, {"authorId": "2291164137", "name": "Hao Tang"}, {"authorId": "2292670885", "name": "Li Jiang"}, {"authorId": "2072683588", "name": "Shaoshuai Shi"}, {"authorId": "2057127601", "name": "Muhammad Ferjad Naeem"}, {"authorId": "2264420960", "name": "Hongsheng Li"}, {"authorId": "48920094", "name": "B. Schiele"}, {"authorId": "2291313590", "name": "Liwei Wang"}], "references": [{"paperId": "458111ac5a0f73bb35a2acf55298268be25ccfa2", "title": "Ferret: Refer and Ground Anything Anywhere at Any Granularity"}, {"paperId": "74af5ac1042ed4f921197b342b15108cdb41223f", "title": "UniTR: A Unified and Efficient Multi-Modal Transformer for Bird\u2019s-Eye-View Representation"}, {"paperId": "e2a58fd18961c3941102989e3a3d0d27c615e015", "title": "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"}, {"paperId": "6e6feae838ea78193185c7b1cc1542aaaf74216d", "title": "AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation"}, {"paperId": "42a30dc5470f54ec249f25d3c31e05d7c376c8e3", "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"}, {"paperId": "8bd6a2a89503be083176f2cc26fabedb79238cbd", "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b", "title": "Segment Anything"}, {"paperId": "c3e5a20b844c042d2174263d2fd5b30d8cc8f0b0", "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c", "title": "Accelerating Large Language Model Decoding with Speculative Sampling"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "7a0a7f47c1614b813e35e15a2c0c0a488ee5e0aa", "title": "DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets"}, {"paperId": "e0b63fd4dd74239a7eb1b75e0108ca55bcad782d", "title": "All in Tokens: Unifying Output Space of Visual Tasks via Soft Token"}, {"paperId": "967907503b24423b9b74621051811fcf684e3957", "title": "Generalized Decoding for Pixel, Image, and Language"}, {"paperId": "30a3731f09e7a391e79a28fa736fa6bdd8331866", "title": "Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks"}, {"paperId": "02251886950770e82b3d68564d60cdfe15e73199", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"}, {"paperId": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c", "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"}, {"paperId": "06761cb27e14aa55a6c3d98b949898aa26416698", "title": "A Unified Sequence Interface for Vision Tasks"}, {"paperId": "4b23a0f7ded1e4201dbb9a2d61397d86d2ad7c48", "title": "ReCo: Retrieve and Co-segment for Zero-shot Transfer"}, {"paperId": "bedf0d6e0623ab48349e3d2a493e7fbb79ca5ef5", "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs"}, {"paperId": "5922f437512158970c417f4413bface021df5f78", "title": "A Generalist Agent"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "6db6ac70465067c3835c60968a27c28c4045c0a8", "title": "Multimodal Token Fusion for Vision Transformers"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "a09cbcaac305884f043810afc4fa4053099b5970", "title": "Exploring Plain Vision Transformer Backbones for Object Detection"}, {"paperId": "9dc481ec44178e797466bbad968071917842156b", "title": "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa", "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "5341b412383c43f4a693ad63ec4489e3ec7688c8", "title": "Grounded Language-Image Pre-training"}, {"paperId": "658a017302d29e4acf4ca789cb5d9f27983717ff", "title": "Masked-attention Mask Transformer for Universal Image Segmentation"}, {"paperId": "91dc75f94da13452a54ad5c03fab2c5fda87e9ba", "title": "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks"}, {"paperId": "22312f763328cf540791de8c2449ea1e7436f476", "title": "UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling"}, {"paperId": "cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0", "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"}, {"paperId": "1ec56ae0d0e2afddee24737b72ac507fbd955186", "title": "LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation"}, {"paperId": "19b3b074d38b250d024920732ae51a8ffa0996dd", "title": "Pix2seq: A Language Modeling Framework for Object Detection"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "7ba9c013988eaff5cd186d73704af329d027872d", "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "394be105b87e9bfe72c20efe6338de10604e1a11", "title": "Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"}, {"paperId": "cb596bffc5c5042c254058b62317a57fa156fea4", "title": "Unifying Vision-and-Language Tasks via Text Generation"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504", "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection"}, {"paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8", "title": "Generative Pretraining From Pixels"}, {"paperId": "2f5f81bc516a6d085d39479378af1fc27104f91e", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation Learning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c5ff974a69fd0c760b4855b819e61e89f31cfffe", "title": "Objects365: A Large-Scale, High-Quality Dataset for Object Detection"}, {"paperId": "4d158f43fc1dfe148f63ad2c7162b51ee7e743cc", "title": "PolarMask: Single Shot Instance Segmentation With Polar Representation"}, {"paperId": "dfc7b58b67c31932b48586b3e23a43cc94695290", "title": "UNITER: UNiversal Image-TExt Representation Learning"}, {"paperId": "855748482b362041f578e4f369eccc5a1c505c1a", "title": "Explicit Shape Encoding for Real-Time Instance Segmentation"}, {"paperId": "c2c083df88e88223e1a411e61040b94c233b1b63", "title": "MMDetection: Open MMLab Detection Toolbox and Benchmark"}, {"paperId": "f902a64f7d08aaa6bfca7463e8729952ddc6134e", "title": "LVIS: A Dataset for Large Vocabulary Instance Segmentation"}, {"paperId": "9e475a514f54665478aac6038c262e5a6bac5e64", "title": "nuScenes: A Multimodal Dataset for Autonomous Driving"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "e65c2b0feddfe4c89e9955ca9b5ece6ef416628f", "title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning"}, {"paperId": "9217e28b2273eb3b26e4e9b7b498b4661e6e09f5", "title": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"}, {"paperId": "f466157848d1a7772fb6d02cdac9a7a5e7ef982e", "title": "Neural Discrete Representation Learning"}, {"paperId": "79828e6e9f137a583082b8b5a9dfce0c301989b8", "title": "The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes"}, {"paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d", "title": "Scene Parsing through ADE20K Dataset"}, {"paperId": "ee4a012a4b12d11d7ab8c0e79c61e807927a163c", "title": "Rethinking Atrous Convolution for Semantic Image Segmentation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "1a0912bb76777469295bb2c059faee907e7f3258", "title": "Mask R-CNN"}, {"paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"paperId": "0095b9f73c000f2609fc81ffb7769df7cd77bda1", "title": "COCO-Stuff: Thing and Stuff Classes in Context"}, {"paperId": "cb1c0d6be4c22c1f18b0ba20dddd93890f17add6", "title": "One-Shot Video Object Segmentation"}, {"paperId": "c6850869aa5e78a107c378d2e8bfa39633158c0c", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"}, {"paperId": "29efbe391950ae438c63d86ad5c82b2942efb0b4", "title": "Modeling Context in Referring Expressions"}, {"paperId": "f90d9c5615f4a0e3f9a1ce2a0075269b9bab6b5f", "title": "SPICE: Semantic Propositional Image Caption Evaluation"}, {"paperId": "385ae5201434ac8d903f1f6bb1b0d420a1ef2c4f", "title": "DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations"}, {"paperId": "c8c494ee5488fe20e0aa01bddf3fc4632086d654", "title": "The Cityscapes Dataset for Semantic Urban Scene Understanding"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "52d7eb0fbc3522434c13cc247549f74bb9609c5d", "title": "WIDER FACE: A Face Detection Benchmark"}, {"paperId": "e65142010431ffc089b272a1174214e00693e503", "title": "Generation and Comprehension of Unambiguous Object Descriptions"}, {"paperId": "b73e2d40da901ad3da3813670a51c52803d7af7e", "title": "SUN RGB-D: A RGB-D scene understanding benchmark suite"}, {"paperId": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270", "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"}, {"paperId": "11c9c31dff70de92ada9160c78ff8bb46b2912d6", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"}, {"paperId": "6364fdaa0a0eccd823a779fcdd489173f938e91a", "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation"}, {"paperId": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "title": "Microsoft COCO Captions: Data Collection and Evaluation Server"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "title": "Deep visual-semantic alignments for generating image descriptions"}, {"paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0", "title": "Fully convolutional networks for semantic segmentation"}, {"paperId": "92c141447f51b6732242376164ff961e464731c8", "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes"}, {"paperId": "3419ccd5c94d301ee08d716d037f0c3c6a62e78e", "title": "The Role of Context for Object Detection and Semantic Segmentation in the Wild"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": "3a9b175324ba11bc0e16c0633912d897b2fac4e2", "title": "The Pascal Visual Object Classes (VOC) Challenge"}, {"paperId": "ac9748ea3945eb970cc32a37db7cfdfd0f22e74c", "title": "Ridge-based vessel segmentation in color images of the retina"}, {"paperId": "b6a0f30260302a2001da9999096cfdd89bc1f7fb", "title": "The Hungarian method for the assignment problem"}, {"paperId": "c41a9b760b9b07104402aa2734659547db559251", "title": "RetinaFace: Single-shot Multi-level Face Localization in the Wild"}, {"paperId": null, "title": "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "8b55402ffee2734bfc7d5d7595500916e1ef04e8", "title": "nocaps: novel object captioning at scale"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Gemini: a family of highly capable multimodal models"}, {"paperId": null, "title": "Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "Introducing our multimodal models (2023)"}, {"paperId": null, "title": "Generalist Vision Transformer"}, {"paperId": null, "title": "ISPRS 2D Semantic Labeling Contest"}]}