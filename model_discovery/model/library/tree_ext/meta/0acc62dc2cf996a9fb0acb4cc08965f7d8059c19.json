{"paperId": "0acc62dc2cf996a9fb0acb4cc08965f7d8059c19", "title": "ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation", "abstract": "This study introduces an approach to optimize Parameter Efficient Fine Tuning (PEFT) for Pretrained Language Models (PLMs) by implementing a Shared Low Rank Adaptation (ShareLoRA). By strategically deploying ShareLoRA across different layers and adapting it for the Query, Key, and Value components of self-attention layers, we achieve a substantial reduction in the number of training parameters and memory usage. Importantly, ShareLoRA not only maintains model performance but also exhibits robustness in both classification and generation tasks across a variety of models, including RoBERTa, GPT-2, LLaMA and LLaMA2. It demonstrates superior transfer learning capabilities compared to standard LoRA applications and mitigates overfitting by sharing weights across layers. Our findings affirm that ShareLoRA effectively boosts parameter efficiency while ensuring scalable and high-quality performance across different language model architectures.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This study introduces an approach to optimize Parameter Efficient Fine Tuning (PEFT) for Pretrained Language Models (PLMs) by implementing a Shared Low Rank Adaptation (ShareLoRA), which effectively boosts parameter efficiency while ensuring scalable and high-quality performance across different language model architectures."}, "embedding": {"model": "specter_v2", "vector": [-0.08205870538949966, 0.18056750297546387, -0.3485792279243469, -0.17856694757938385, -0.43509986996650696, -0.3622485399246216, 0.7817396521568298, -0.47843658924102783, -0.5312252044677734, -0.14243097603321075, 0.4194476902484894, 0.3521428108215332, 0.25470393896102905, 0.1885470151901245, -0.23418307304382324, 0.25772055983543396, -0.827235221862793, 0.8303393125534058, 0.05875574052333832, -0.7029260396957397, -0.4419983923435211, -0.6650705933570862, -0.8500500321388245, -0.023911161348223686, 0.40086591243743896, 0.6838895082473755, 0.26048552989959717, 0.8257125616073608, -0.34602364897727966, -0.21913744509220123, 0.5131511688232422, -0.28487473726272583, 0.285023033618927, -0.20220762491226196, -0.18262121081352234, -0.05178520828485489, 0.1266835629940033, -0.2289515882730484, 0.13606685400009155, 0.6603692770004272, 0.11100727319717407, 0.3860507011413574, 0.8482386469841003, -0.35802000761032104, -0.3669104278087616, 0.5772361159324646, 0.4499278962612152, 0.7292520403862, -0.5735539793968201, -0.39852607250213623, 1.0324362516403198, -1.500966191291809, -0.05757768079638481, 1.4858927726745605, 0.5815601348876953, 0.41245126724243164, -0.298194020986557, -0.9368466138839722, 0.6299446821212769, -0.26735740900039673, -1.1736892461776733, -0.22663435339927673, -0.34904026985168457, -0.07183614373207092, 1.896838665008545, -0.5526093244552612, -0.3965645730495453, 0.4382593035697937, 0.07034926116466522, 1.0187691450119019, -0.0014698163140565157, -0.35620731115341187, -0.28904345631599426, 0.1776038408279419, -0.14875470101833344, 0.731605052947998, -0.4057719111442566, 0.06472013145685196, -0.9097727537155151, -0.2944689691066742, 0.32678303122520447, -0.6520056128501892, 0.22729960083961487, -0.23821786046028137, -0.14013442397117615, 0.7930260300636292, 0.2763177156448364, 0.6119565367698669, -0.12243987619876862, 0.8597158789634705, 0.3742120862007141, 0.38220497965812683, 0.5908016562461853, 0.6783989667892456, -0.5497696399688721, 0.4885116219520569, -0.7828690409660339, 0.2697979211807251, -0.009033738635480404, 0.9449474215507507, 0.08138849586248398, 0.17856080830097198, -0.8530311584472656, 0.5355658531188965, 1.342690348625183, 0.2643243074417114, 0.7783311009407043, -0.35005831718444824, 0.25360819697380066, -0.936669111251831, -0.04244082048535347, -0.5591805577278137, -0.2910201847553253, -0.5487164855003357, -0.9827657341957092, -1.4543207883834839, -0.9636495113372803, 0.07515421509742737, -0.6285880208015442, 1.0574588775634766, -0.28057366609573364, 0.028384508565068245, 0.03889521211385727, 0.45014986395835876, 0.44703444838523865, 0.777561366558075, 0.554533064365387, 0.25811466574668884, 0.9185559749603271, -0.9525606036186218, -0.48432648181915283, -1.0536742210388184, 0.6328486800193787, -0.3699488639831543, 0.490878164768219, -0.19094860553741455, -1.201436996459961, -0.623910665512085, -0.9202238321304321, -0.21813717484474182, -0.43180492520332336, 0.7097850441932678, 1.0701247453689575, 0.33822762966156006, -0.6802834272384644, 0.5705940127372742, -0.16584934294223785, -0.2977573573589325, 0.2633412182331085, 0.410917192697525, 0.04945966601371765, -0.1158011257648468, -1.5403372049331665, 0.2935450077056885, 0.5205664038658142, -0.49952635169029236, -0.07726455479860306, -0.6114329695701599, -0.6729881167411804, 0.054905012249946594, 0.34798291325569153, -0.7705547213554382, 1.3840333223342896, -0.2556781768798828, -1.8478673696517944, 0.6088048815727234, 0.01051186490803957, 0.13117825984954834, 0.47422054409980774, -0.16477511823177338, -0.5147333741188049, -0.6881431341171265, -0.45469117164611816, 0.6112027168273926, 0.8562314510345459, 0.16610877215862274, 0.050004635006189346, 0.5724087953567505, -0.4224032163619995, 0.3227684795856476, -0.36030644178390503, 0.8369196653366089, -0.9002725481987, -0.2536923289299011, 0.4240916967391968, 0.5014019012451172, 0.11943971365690231, -0.463473916053772, -0.35049939155578613, -1.177356243133545, 0.38393282890319824, 0.22138755023479462, 0.8346275091171265, -0.8616925477981567, -0.48662033677101135, 0.15682187676429749, -0.39640378952026367, -0.024778416380286217, -1.0629630088806152, 0.4666315019130707, -0.31437379121780396, 0.5030056834220886, -0.2604573667049408, -1.39493727684021, 0.15703752636909485, -0.36972954869270325, -0.5010591149330139, -0.19852662086486816, 0.43900105357170105, 1.066928744316101, -0.690193235874176, 0.24861584603786469, -0.4772207736968994, 0.46648669242858887, -1.2748353481292725, 1.1429872512817383, -0.6712088584899902, 0.27109387516975403, -0.39036184549331665, -0.46107926964759827, 0.20140685141086578, -0.5546373128890991, 0.44192394614219666, -0.4592497646808624, 0.04307341203093529, 0.46508362889289856, -0.5517817735671997, 1.5829718112945557, -0.4718174934387207, 0.10524914413690567, -0.17068791389465332, -0.020313696935772896, 0.1523512452840805, 0.19314922392368317, -0.20026721060276031, -0.5335320830345154, 0.1443978101015091, 0.8016045093536377, -0.9249449968338013, 0.4894736409187317, 0.7137647867202759, 0.8099427819252014, -0.486685574054718, 0.36802706122398376, 0.5505236983299255, -0.6333589553833008, 0.6297627687454224, 0.22005780041217804, 0.3488190770149231, 0.4104636013507843, 0.6098226308822632, -0.30878692865371704, 0.6346371173858643, -0.8864408135414124, -0.221071258187294, 0.436014324426651, 0.6612942218780518, 0.8210629224777222, 0.30320391058921814, -0.5737938284873962, -0.45535776019096375, -0.2250680774450302, 0.73054039478302, 1.8711086511611938, -0.4718801975250244, 0.10512099415063858, -0.6564188003540039, -0.13009272515773773, -0.21891111135482788, -0.21512995660305023, -0.3503133952617645, -0.08435294777154922, -0.6065277457237244, -1.4000756740570068, 0.5078670382499695, -0.21763169765472412, 0.9926628470420837, -0.3668059706687927, 0.2520303428173065, -0.31102609634399414, 0.25735417008399963, -0.5846951007843018, -1.1542829275131226, 0.2577071487903595, -0.23481333255767822, 0.2133597880601883, -0.24172250926494598, 0.0502970851957798, 0.00030284261447377503, -0.6118810176849365, 1.0000559091567993, -0.8146354556083679, 0.06423833221197128, 0.019069239497184753, 0.5839673280715942, -0.5924130082130432, -0.8741004467010498, 0.9128061532974243, 0.5462960600852966, 0.09929324686527252, 0.08585137873888016, 0.4409446120262146, 0.16665877401828766, 0.3333652913570404, -0.3691706955432892, 0.2789265513420105, 0.26962119340896606, -0.038876600563526154, 0.9554358720779419, -0.29983019828796387, 0.12424509972333908, -1.6478586196899414, 1.253915548324585, -0.05434275418519974, -0.7552767395973206, 0.34647735953330994, -0.7074303030967712, -0.158478781580925, 0.3909168243408203, -0.706415057182312, -0.3913710117340088, -0.9011985063552856, 0.4808642268180847, -0.030456745997071266, 0.158795565366745, 0.3599892556667328, 0.37941795587539673, 0.1942761093378067, 0.5084957480430603, 0.36002644896507263, 0.2469397485256195, -0.5693770051002502, 0.7012962698936462, -0.9470711350440979, 0.5513299703598022, 0.27084413170814514, 0.6161270141601562, -0.3267758786678314, -0.31951871514320374, -0.4561411142349243, -0.6215505599975586, -0.39787131547927856, -0.11581364274024963, 0.3025593161582947, 0.33466172218322754, -1.1850672960281372, -0.5882433652877808, 0.0961294025182724, -0.673835277557373, -0.3532886207103729, 0.3348987102508545, -0.10907194018363953, -0.23181766271591187, -1.3245277404785156, -1.1341811418533325, -0.18081097304821014, -1.035435438156128, -1.0539202690124512, 0.3292562663555145, -0.049069155007600784, -0.2100650817155838, -0.42245009541511536, 0.10415167361497879, -0.6208338737487793, 1.4256948232650757, -0.9706861972808838, 0.8041261434555054, 0.16892403364181519, -0.25743451714515686, -0.16730955243110657, 0.1392124891281128, 0.5279313325881958, -0.48085957765579224, 0.053192172199487686, -0.9321449995040894, -0.13229644298553467, -0.6741116046905518, -0.4032477140426636, 0.2083645462989807, 0.32613199949264526, 0.6528099775314331, 0.04897300899028778, -0.31231802701950073, 1.0522146224975586, 1.2894254922866821, -1.2963781356811523, -0.2406742423772812, 0.3634743094444275, 0.9192389249801636, 0.07834784686565399, -0.48410674929618835, 0.4300495386123657, 0.534610390663147, 0.537906289100647, -0.01730421371757984, -0.11199412494897842, -0.1053202673792839, -0.606187641620636, 0.6166174411773682, 1.7389885187149048, 0.45333316922187805, -0.2980039417743683, -0.8319469690322876, 0.2379230558872223, -0.9358851313591003, -0.434188574552536, 0.6387486457824707, 0.8978708982467651, 0.4059770703315735, -0.5934959053993225, -0.25826576352119446, -0.7897732853889465, 0.20942315459251404, 0.3075948655605316, -0.4797886908054352, -0.8191151022911072, -0.28642189502716064, 0.17969176173210144, 0.37197014689445496, 0.4558578431606293, -0.4319273829460144, 0.9951018691062927, 14.639784812927246, 0.9408355355262756, -0.0011951889609917998, 0.9830029010772705, 0.7804511189460754, -0.08708006143569946, -0.3902769088745117, -0.5696933269500732, -1.1155250072479248, -0.2975705862045288, 1.0316908359527588, 0.03338109701871872, 0.9467083811759949, 0.09412074089050293, 0.24258606135845184, 0.4801101088523865, -0.2560858428478241, 0.7143427729606628, 0.44495144486427307, -1.1779167652130127, 0.4574360251426697, 0.038117460906505585, 0.9704232215881348, 1.1049821376800537, 1.04473078250885, 0.9907169342041016, 0.5690039396286011, -0.44857552647590637, 0.21949081122875214, 0.5196653604507446, 1.0144635438919067, -0.1567038744688034, 0.2824251055717468, 0.21668000519275665, -0.6029444932937622, -0.12233535945415497, -0.5960575938224792, -0.8902393579483032, 0.102052241563797, 0.040711235255002975, -0.6102675795555115, -0.5996226072311401, -0.002903867047280073, 0.575580358505249, -0.02563503198325634, 0.3382984697818756, -0.23821185529232025, 0.7277438044548035, -0.3516733944416046, 0.15980783104896545, 0.2230176329612732, 0.42336297035217285, 0.009346265345811844, -0.05406340956687927, 0.28438282012939453, -0.1030532494187355, 0.06361144781112671, 0.5484868884086609, -0.8019462823867798, 0.21671272814273834, 0.24365520477294922, -0.029395800083875656, 0.12612715363502502, 0.863580048084259, 0.8500426411628723, 0.36364006996154785, -0.2695070207118988, 0.5464534163475037, 0.7665584087371826, 0.3546251654624939, -0.15338000655174255, 0.3553011119365692, 0.3947018086910248, -0.4662744998931885, -0.18675290048122406, 0.44802239537239075, 0.1185540109872818, -0.4712684154510498, -0.8314305543899536, -0.42495670914649963, 0.18988575041294098, -0.9437779188156128, -0.9794718623161316, 0.8268004059791565, -0.24068428575992584, -0.3544274568557739, 0.0495939627289772, -0.4055418372154236, -0.1460706889629364, 0.9511966109275818, -1.514267921447754, -0.8688027262687683, 0.6022745370864868, -0.5985462665557861, -0.5252519845962524, -0.5659164190292358, 1.2675052881240845, 0.05179586261510849, -0.6367634534835815, 0.4845132827758789, 0.5466907620429993, -0.5353784561157227, 0.20382389426231384, -0.3985375463962555, 0.999759316444397, 0.03205082193017006, -0.5177260637283325, 0.33926922082901, -0.04411524534225464, 0.27091607451438904, -0.7987685203552246, -0.2502516806125641, 0.8205763697624207, -0.7314023971557617, -0.21967336535453796, -0.8326103091239929, -0.7721099853515625, 0.06909441947937012, 0.30356308817863464, -0.20800763368606567, 0.3567803204059601, 0.2466547042131424, -0.8305284380912781, -0.17536862194538116, -0.691735029220581, -0.21729950606822968, 0.11316825449466705, -0.8982940316200256, 0.2996395230293274, 0.20073477923870087, 0.21400129795074463, -1.1587127447128296, -0.40921205282211304, -0.3054582178592682, -0.022535741329193115, 0.30334901809692383, 1.02170991897583, -0.2145182341337204, 0.3436186611652374, 0.7067532539367676, -0.09133174270391464, -0.9879592061042786, -0.2865764796733856, -1.0335705280303955, 0.18967552483081818, 0.058722760528326035, 0.8964371681213379, -0.5944271683692932, 0.09780031442642212, 0.42850199341773987, 0.47064584493637085, -0.35918036103248596, -0.47019535303115845, -0.18558531999588013, 0.27428609132766724, -0.38944894075393677, 0.6821709275245667, -0.08966989815235138, -0.5350102186203003, 0.2330273538827896, 0.15397323668003082, 0.8482487797737122, -0.2845551669597626, -1.1570302248001099, 0.6227536201477051, 0.00030785577837377787, -0.36201590299606323, -0.5381043553352356, 0.011524242348968983, -1.2628037929534912, -0.18708354234695435, -1.2282763719558716, -0.20307059586048126, -0.7312163710594177, -0.1369052678346634, -0.09767183661460876, -0.3204514682292938, 0.019222695380449295, 0.4038069248199463, 0.11013247072696686, -0.40662139654159546, -0.0871429368853569, -0.4839957058429718, 0.9964355826377869, 0.8929989337921143, -0.7100926041603088, -0.166362002491951, 0.006511148996651173, 0.14870233833789825, 0.17925292253494263, 0.6461588144302368, -0.38627585768699646, -0.7574506998062134, -1.7235718965530396, 0.8082951903343201, -0.40729689598083496, -0.19959186017513275, -0.737156093120575, 0.5796047449111938, 0.22040249407291412, -0.10203658044338226, 0.25380489230155945, 0.1775590181350708, -0.6799217462539673, -0.41924694180488586, 0.05073145404458046, -0.7723535299301147, 0.6077309250831604, 0.44894886016845703, -0.5574889183044434, -0.09072718769311905, 0.9643341302871704, -0.05435319244861603, -0.9905825853347778, -0.5486714839935303, 0.6831353902816772, -0.5682433843612671, 0.1386134773492813, -0.5990502834320068, 0.23063254356384277, -1.055589199066162, -0.44690895080566406, 0.24216042459011078, 0.5707429647445679, -0.4824812710285187, 0.9452125430107117, -0.05671785771846771, -1.0582256317138672, -0.3556094467639923, 0.6606159806251526, 0.11727841198444366, -0.08345341682434082, 0.6812238693237305, 0.4273144602775574, -0.48056691884994507, 0.7683884501457214, 0.5159553289413452, 0.5105873346328735, -0.2725890576839447, -0.3370712101459503, 1.0854021310806274, -0.7439475655555725, 0.17101752758026123, 1.425612449645996, -0.016297997906804085, -1.530582070350647, -0.23947769403457642, -0.8849146366119385, -0.47157901525497437, -0.35260412096977234, 0.5335525274276733, -0.09366092085838318, -0.06019959971308708, -0.27945807576179504, -0.4231731593608856, 0.10988234728574753, -0.2662626802921295, -0.5801753401756287, 0.6561760306358337, -0.42693719267845154, -0.45552748441696167, 0.5498030185699463, 0.979448676109314, -0.5917581915855408, -0.7754767537117004, -0.9384804964065552, -0.4860825836658478, 0.35219377279281616, 0.5613637566566467, -0.8885563611984253, -0.5579710602760315, 0.3500913083553314, 0.6752004027366638, -0.2400110960006714, 0.37978896498680115, -0.3510059714317322, -0.03478970378637314, 0.7408590912818909, -0.010705369524657726, -0.7842097282409668, -0.5512710809707642, 1.4876986742019653, 1.2475836277008057, -1.0881316661834717, 0.3807823956012726, -0.14412912726402283, -0.7879653573036194, 0.48054876923561096, 0.15052354335784912, 0.30088070034980774, 0.8026818037033081, -0.6018419861793518, 0.06504735350608826, 0.29012396931648254, -1.2564406394958496, -0.37038755416870117, 1.4003065824508667, 0.6772237420082092, 0.9626902341842651, 0.43846070766448975, 0.011079239659011364, 0.6691508889198303, -0.07572617381811142, -0.10306224972009659, 0.4665762186050415, -0.1220654770731926, -0.14546939730644226, -0.132095068693161, 0.09831429272890091, 0.6403239965438843, -0.5690746903419495, -0.6250338554382324, 0.18938370048999786, 0.3428287208080292, 0.17467789351940155, 0.3218492865562439, 0.8788185119628906, 0.20457179844379425, 0.5028188228607178, 0.06181124970316887, 0.1674196869134903, -0.6821529269218445, -0.33312922716140747, 0.0204413291066885, -0.7488173246383667, -0.057461079210042953, 0.16683697700500488, -0.40902015566825867, -0.24167786538600922, -0.23913037776947021, 0.34684839844703674, -0.11963765323162079, 0.6819139719009399, 1.0436543226242065, 0.5692725777626038, 0.4778432846069336, -0.14606912434101105, -0.6272410154342651, -0.6907472610473633, -1.1761640310287476, -0.11248970776796341, -0.46511927247047424, -0.4047841727733612, -0.1050315722823143, 0.0512784868478775, -0.6771523356437683]}, "authors": [{"authorId": "2152602955", "name": "Yurun Song"}, {"authorId": "120246321", "name": "Junchen Zhao"}, {"authorId": "2269144659", "name": "Ian G. Harris"}, {"authorId": "37541666", "name": "S. Jyothi"}], "references": [{"paperId": "8ce219059d777c2333ee21cb2af2aad71275c98f", "title": "LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning"}, {"paperId": "3f459219d75de63b5b7a26a8c6447ec1e79a985c", "title": "LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition"}, {"paperId": "0d2adcddccd72de47c263b6e4e0aab3dd0582a52", "title": "ReLoRA: High-Rank Training Through Low-Rank Updates"}, {"paperId": "16b42fc85f4c073aa00c410cbdce965d7c6f8d4d", "title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "c5d4f640a9fbe816d7574c57415554de100a0306", "title": "Improving aspect-based sentiment analysis with contrastive learning"}, {"paperId": "148644bf4ccef7e022b965304e8b3178be8af0fa", "title": "Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "43a87867fe6bf4eb920f97fc753be4b727308923", "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "bb3425318de7eed5641cda147d61c9a057b9d054", "title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "bdeec55f95fd6b73e3e4635459b14c7248543efb", "title": "AdapterDrop: On the Efficiency of Adapters in Transformers"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "63076064a7775f4ab6a094d93a233d32b7031686", "title": "Distant Supervision for Multi-Stage Fine-Tuning in Retrieval-Based Question Answering"}, {"paperId": "1187c70c4011f935642084e84186284ac0add3d0", "title": "Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "cf3d8147039c93d36b128786daa33c701e7e18ce", "title": "Exploiting Multilingualism through Multistage Fine-Tuning for Low-Resource Neural Machine Translation"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "531a7f2c659787165df4fd5b4580590b953448e4", "title": "The E2E Dataset: New Challenges For End-to-End Generation"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "2023. Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment"}, {"paperId": null, "title": "2023. Stanford alpaca: An instruction-following llama model"}, {"paperId": null, "title": "BigScience Workshop"}, {"paperId": null, "title": "2023. A framework for few-shot language model evaluation"}, {"paperId": null, "title": "2022. Training compute-optimal"}]}