{"paperId": "77b046c5d568b329a927cfc895ea2e6c8f43ff43", "title": "The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute", "abstract": "The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all our levels of compute, our LSTM baseline exhibits a predictable and more favourable scaling law. This is due to the improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws leads of both models results in an intersection at roughly 50,000 accelerator hours. We hope this work can serve as the foundation for meaningful and reproducible language modelling research.", "venue": "arXiv.org", "year": 2023, "citationCount": 7, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2309.11197", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "An experimental protocol is introduced that enables model comparisons based on equivalent compute, measured in accelerator hours, and provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput."}, "embedding": {"model": "specter_v2", "vector": [0.1497763991355896, 0.27673572301864624, -0.12013059854507446, -0.12401306629180908, -0.15082566440105438, -0.19571228325366974, 0.9106022715568542, -0.45147842168807983, -0.3936755359172821, -0.32079148292541504, 0.6246312856674194, -0.2589178681373596, 0.5316002368927002, 0.1598590463399887, -0.0907575786113739, -0.023714827373623848, -0.6671925783157349, 0.38583868741989136, -0.8938814401626587, -0.02592511847615242, -0.01563362218439579, -0.22325408458709717, -1.0708280801773071, 0.07007761299610138, 0.14100410044193268, 0.6069523096084595, 0.07912798970937729, 1.011674404144287, -0.5370731949806213, 0.4346437454223633, 0.43515321612358093, 0.059745002537965775, -0.030809171497821808, 0.059439342468976974, -0.4389685392379761, -0.3278285264968872, 0.4737328886985779, -0.004160071723163128, -0.37750858068466187, 0.4520280659198761, -0.16845855116844177, 0.37497952580451965, 0.5453053712844849, -0.39145177602767944, -0.22036810219287872, 0.6018179059028625, 0.8925646543502808, 0.855445384979248, -0.015301083214581013, -0.3966323137283325, 0.973831832408905, -1.345381498336792, 0.1991654336452484, 1.4417786598205566, 0.3649320602416992, 0.18928782641887665, -0.46513816714286804, -0.33334049582481384, 0.29860684275627136, -0.3387807309627533, -0.9643404483795166, -0.49450984597206116, -0.25356218218803406, -0.13944661617279053, 1.980308175086975, -0.2769284248352051, 0.06135280430316925, 0.4071539640426636, -0.03539765626192093, 1.4220634698867798, 0.26111772656440735, -0.9048621654510498, -0.3549622893333435, 0.4608990550041199, 0.3997717499732971, 0.6337408423423767, -0.44029688835144043, -0.20707973837852478, -0.7908079028129578, 0.06942713260650635, 0.4057176411151886, -0.1744166761636734, 0.04209506884217262, 0.250825434923172, -0.5707743167877197, 0.5406624674797058, 0.2211722433567047, 1.1600831747055054, -0.22073820233345032, 1.0355091094970703, 0.7570127248764038, 0.3811235725879669, 0.3521246910095215, 0.39151713252067566, -0.18848928809165955, 0.22288164496421814, -0.9937272071838379, 0.2407872974872589, 0.4320708215236664, 0.9949555993080139, -0.2202926129102707, 0.3918645679950714, -0.7102417349815369, -0.11059921234846115, 1.326385498046875, 0.03906691074371338, 0.8158044815063477, -0.2544213533401489, 0.18013419210910797, -0.7723716497421265, -0.05194070190191269, -0.6024158596992493, -0.23723973333835602, -0.3469749987125397, -0.7184808254241943, -1.6401517391204834, -0.6263970732688904, 0.10717180371284485, -0.9170342087745667, 0.12134554982185364, -0.38033172488212585, 0.12786583602428436, 0.3442131280899048, 0.3097708523273468, 0.9890411496162415, 0.633967399597168, 0.4626076817512512, 0.19808807969093323, 1.2732341289520264, -0.7172847986221313, -0.7246154546737671, -0.7839229106903076, 1.082108974456787, -0.27430784702301025, 0.33411499857902527, -0.06234384700655937, -1.3008816242218018, -0.9580745697021484, -0.5654104948043823, -0.3649463951587677, -0.6401845812797546, 0.7638484239578247, 1.258192539215088, 0.6914867758750916, -1.2247252464294434, 0.6274107098579407, -0.34956303238868713, -0.27113187313079834, 0.01416855026036501, 0.2344071865081787, 0.4471854865550995, -0.3612745702266693, -1.0525263547897339, 0.30249080061912537, 0.4202791750431061, -0.6032734513282776, 0.3456672728061676, -0.47124817967414856, -0.9768847823143005, 0.09397523105144501, -0.046108439564704895, -0.35142913460731506, 1.3123092651367188, -0.08697985112667084, -1.344042420387268, 0.8265789747238159, -0.6275420784950256, 0.0345706008374691, 0.016458267346024513, -0.10892734676599503, -0.5998432040214539, -0.6342797875404358, -0.6037273406982422, 0.41907262802124023, 0.37529271841049194, 0.12525713443756104, 0.11428871750831604, 0.3891313076019287, -0.4685671627521515, 0.21028998494148254, -0.17221294343471527, 1.2106969356536865, -0.4165797531604767, -0.11131347715854645, 0.43195104598999023, 0.3462229073047638, -0.008057410828769207, -0.565866231918335, -0.35016006231307983, -0.9753439426422119, 0.8393332362174988, -0.06883247196674347, 0.9856537580490112, -0.9737962484359741, -0.704376220703125, -0.05062699317932129, -0.0782303437590599, -0.41628730297088623, -0.3684976398944855, 0.5994181632995605, -0.3364572823047638, 0.6577984094619751, 0.24684979021549225, -1.0423920154571533, 0.13400793075561523, -0.26919007301330566, -0.5101926922798157, -0.14200153946876526, 0.0052248574793338776, 1.155759572982788, -0.48338064551353455, 0.176216259598732, -0.04696263372898102, 0.5094772577285767, -0.53597092628479, 0.8787112236022949, -0.3572615087032318, 0.1383296549320221, -0.1942562460899353, -0.34232276678085327, 0.022660454735159874, -0.4786704182624817, 0.44022753834724426, -0.4092770516872406, -0.3569565415382385, 0.713654100894928, -0.4229696989059448, 1.4035090208053589, -0.46968546509742737, 0.36020568013191223, -0.07454033195972443, -0.5291940569877625, 0.3966369926929474, 0.5556551218032837, -0.357278972864151, -0.38565465807914734, 0.3375079035758972, 0.735814094543457, -0.6466773748397827, 0.38543927669525146, 1.1081597805023193, 0.8292208313941956, -0.43145689368247986, 0.26980656385421753, 0.6057169437408447, -0.6009536385536194, 0.5836291313171387, 0.353754460811615, 0.3546420633792877, 0.2564902901649475, -0.03330730274319649, -0.42158257961273193, 0.4722021520137787, -0.9477918744087219, -0.6147316694259644, 0.174741730093956, 0.7129940986633301, 0.6897809505462646, 0.09180125594139099, -0.8192867636680603, -0.31726595759391785, 0.1987440437078476, 0.7696582078933716, 1.5399315357208252, -0.32573431730270386, -0.253785640001297, -0.7548816800117493, 0.07555082440376282, -0.300752729177475, 0.20801562070846558, 0.22543708980083466, 0.15907515585422516, -0.9091161489486694, -1.0704892873764038, 0.7064127922058105, -0.014518934302031994, 0.7425093054771423, -0.5741488337516785, -0.6532980799674988, -0.5130139589309692, 0.36970511078834534, -0.7542274594306946, -0.5423967838287354, 0.44776254892349243, -0.6513916254043579, 0.4196513891220093, 0.10035144537687302, -0.3018885850906372, 0.13510660827159882, -0.4180591106414795, 1.0335794687271118, -0.03110841102898121, -0.48165571689605713, 0.21790121495723724, 0.7435773015022278, -0.576672375202179, -0.8072077631950378, 0.26034268736839294, 0.26696252822875977, -0.4564281702041626, 0.21548758447170258, 0.4648936986923218, 0.038331639021635056, -0.41850852966308594, -0.3542509377002716, 0.3694244623184204, -0.0721600130200386, -0.2695144712924957, 0.49045464396476746, -0.2880978286266327, -0.40358030796051025, -0.9093174338340759, 1.1248165369033813, 0.142664834856987, -0.0855836272239685, 0.19230680167675018, -0.6810767650604248, -0.007766042370349169, 0.636631965637207, -0.6287072896957397, -0.10150487720966339, -0.8912125825881958, 0.6565797328948975, 0.12014977633953094, -0.34178170561790466, 0.38737425208091736, 0.4226192831993103, -0.013229290023446083, -0.07739176601171494, 0.5588465929031372, 0.14505387842655182, -0.3443017601966858, 0.7544624209403992, -0.7640810012817383, 0.32445478439331055, 0.37404510378837585, 0.026696883141994476, -0.3335176110267639, -0.1255890130996704, -0.804410457611084, -0.13250136375427246, -0.357926607131958, -0.17957870662212372, -0.32243168354034424, -0.0481390580534935, -0.5989869832992554, -0.6506097912788391, -0.04286396875977516, -1.061029076576233, -0.1064981147646904, 0.25458699464797974, -0.29189184308052063, -0.19772133231163025, -1.126447081565857, -1.3553608655929565, -0.7798530459403992, -1.1129735708236694, -1.0736994743347168, 0.5329042673110962, 0.24232101440429688, -0.41669178009033203, -0.6559721827507019, 0.02542622573673725, -0.5044799447059631, 1.04771089553833, -0.4992246627807617, 0.7831502556800842, -0.07736945152282715, -0.00962720438838005, -0.18222364783287048, 0.04551608860492706, 0.3649037778377533, -0.4622017741203308, 0.42736175656318665, -0.7342923283576965, -0.004284340422600508, -0.2572999596595764, -0.5768142938613892, 0.004568170290440321, 0.2644767463207245, 0.6136940121650696, -0.27899110317230225, -0.39290091395378113, 0.43149277567863464, 1.2294880151748657, -0.9166862964630127, 0.14143045246601105, -0.021632783114910126, 1.0462555885314941, 0.06386817246675491, -0.8120401501655579, 0.40676796436309814, 0.38171446323394775, 0.20141056180000305, -0.4356737732887268, -0.23696289956569672, -0.11497274041175842, -0.4849344491958618, 0.5476383566856384, 1.7638847827911377, 0.32116007804870605, -0.1705869436264038, -1.0328216552734375, 0.3094960153102875, -0.8932952880859375, -0.5278589725494385, 0.43250972032546997, 0.6277334690093994, 0.4693485200405121, -0.45558395981788635, -0.364330530166626, 0.06774911284446716, 0.1036456972360611, 0.21974864602088928, -0.24299724400043488, -1.1140985488891602, -0.03768698498606682, 0.5007527470588684, 0.2798471450805664, 0.16527460515499115, -0.38098883628845215, 0.7221361398696899, 15.007148742675781, 0.9889291524887085, -0.22627846896648407, 0.33559250831604004, 1.0783969163894653, 0.03160421550273895, -0.31170031428337097, -0.3003696799278259, -1.137803316116333, -0.025667820125818253, 1.8341537714004517, -0.10882598161697388, 0.8057639598846436, 0.2811228930950165, 0.20743784308433533, 0.15882273018360138, -0.41188815236091614, 0.8160073161125183, 0.575242817401886, -1.1608707904815674, 0.25168365240097046, 0.1792389154434204, 0.3542138338088989, 0.7081044912338257, 0.6939350366592407, 0.9882423877716064, 0.25178301334381104, -0.9271196126937866, 0.6070392727851868, 0.48381614685058594, 0.9108284711837769, -0.1286812424659729, 0.30261409282684326, 0.5995166897773743, -1.1384835243225098, -0.023981187492609024, -0.41483595967292786, -1.316543698310852, 0.12715432047843933, 0.1536693125963211, -0.8315941095352173, -0.3264752924442291, -0.35497918725013733, 0.4652223289012909, 0.09837046265602112, 0.34718337655067444, 0.11897651851177216, 0.716270923614502, -0.29194700717926025, -0.4988971948623657, 0.3895196318626404, 0.20744429528713226, -0.13755135238170624, 0.4452081024646759, -0.003423313843086362, -0.16001707315444946, 0.1529626101255417, 0.5417624711990356, -0.7511422634124756, 0.28794437646865845, -0.31059858202934265, -0.6111508011817932, -0.21236960589885712, 1.1338837146759033, -0.0016002273187041283, 0.08994703739881516, -0.1704225093126297, 0.4884055256843567, 0.4845190942287445, 0.09605168551206589, 0.07421132177114487, -0.0014500434044748545, 0.283443421125412, -0.4293684661388397, -0.05107762664556503, -0.06852757930755615, -0.28034892678260803, -0.4832543432712555, -0.6494356989860535, -0.43324005603790283, -0.08127343654632568, -0.6974025964736938, -0.5681127309799194, 0.8019914031028748, -0.20475171506404877, 0.09344078600406647, 0.031598109751939774, -0.983465850353241, -0.23756954073905945, 0.9195753931999207, -1.3044767379760742, -0.7776007056236267, 0.4084746539592743, -0.4758746325969696, -0.31454750895500183, 0.22040443122386932, 1.4913489818572998, -0.1779148280620575, -0.42678967118263245, -0.05383830517530441, 0.5459529161453247, -0.1484195441007614, -0.7031349539756775, -0.7402154207229614, 1.1992907524108887, 0.022577380761504173, 0.10522778332233429, 0.3527492880821228, 0.05956324189901352, 0.12998823821544647, -1.3381540775299072, 0.12910103797912598, 1.0184565782546997, -0.84144127368927, -0.19606919586658478, -1.1609246730804443, -0.6483629941940308, -0.04616314545273781, 0.19388069212436676, -0.5635770559310913, 0.04749337583780289, 0.02494429424405098, -0.5725874900817871, 0.39269769191741943, -0.8733673095703125, -0.07925792783498764, 0.45687440037727356, -1.0539277791976929, 0.03555147722363472, 0.4486197531223297, 0.5931940078735352, -1.1045881509780884, -0.5139495134353638, -0.4437274634838104, 0.15627457201480865, 0.3000919222831726, 0.7205972671508789, -0.179512619972229, 0.4824472963809967, 1.130359172821045, -0.07522624731063843, -0.5311769843101501, 0.23595768213272095, -0.9407599568367004, -0.23455393314361572, -0.16455097496509552, 0.7192718982696533, -0.2495107352733612, 0.18533039093017578, 1.156160831451416, 0.5402653813362122, -0.6296854019165039, -0.577690839767456, -0.22190748155117035, 0.02184932678937912, -0.6828839778900146, 0.36098620295524597, -0.25734415650367737, 0.3207578957080841, 0.24695968627929688, 0.6031793355941772, 0.03053968772292137, -0.599643886089325, -0.766746997833252, 0.30997177958488464, -0.1695050299167633, 0.1793033629655838, -0.7632319927215576, -0.5756749510765076, -1.3246220350265503, 0.02224462665617466, -1.2078297138214111, 0.06807786971330643, -0.7890875935554504, -0.1711810827255249, -0.1032632440328598, 0.08367562294006348, 0.09022950381040573, 0.6688340902328491, -0.21726621687412262, -0.465424120426178, -0.15299946069717407, -0.3277225196361542, 0.7752291560173035, 0.7185616493225098, -0.35216426849365234, -0.13183988630771637, -0.27744919061660767, 0.21948568522930145, 0.2338619828224182, 0.520291805267334, 0.06077250838279724, -0.7249594926834106, -1.3125836849212646, 0.320766806602478, -0.42560234665870667, -0.309266597032547, -0.8379033207893372, 0.4878411889076233, 0.5960932374000549, -0.33369743824005127, 0.18453891575336456, 0.12737415730953217, -0.3432115912437439, -0.35349777340888977, 0.1527748703956604, -0.7084257006645203, 0.5850705504417419, 0.46166855096817017, -0.7550973892211914, 0.2820427417755127, 0.5505982041358948, -0.3666953444480896, -0.8439688682556152, -0.2840937077999115, 0.17021112143993378, -0.6447643637657166, -0.30655068159103394, -0.6292616128921509, -0.21171383559703827, -0.8425635695457458, -0.26676613092422485, 0.23716889321804047, 0.05956565961241722, -0.2619336247444153, 1.0425156354904175, 0.31425923109054565, -0.8643927574157715, 0.06025220453739166, 0.4159667193889618, -0.3767073154449463, -0.314460813999176, 0.18424133956432343, 0.2644137740135193, -0.5012891292572021, 0.8416885733604431, 0.4746514856815338, 0.2213257998228073, -1.0495623350143433, -0.20839889347553253, 0.4820499122142792, -0.5774512887001038, -0.40095192193984985, 1.0669445991516113, -0.6410456299781799, -1.075732946395874, 0.17360982298851013, -1.5728822946548462, -0.4645426273345947, -0.5810272097587585, 0.7373456954956055, -0.09630409628152847, 0.22575260698795319, -0.3082592189311981, -0.19823867082595825, 0.005709818564355373, 0.08092983812093735, -0.7578514814376831, 0.3025440573692322, -0.10926005989313126, -0.21690700948238373, 0.8406606316566467, 0.8805153965950012, -0.9722011685371399, -0.12886914610862732, -0.543648362159729, -0.175161674618721, 0.2727077603340149, 0.6761441826820374, -0.38496819138526917, -0.2348332703113556, 0.646758496761322, 0.5898958444595337, 0.27253246307373047, 0.0981302261352539, -0.31235572695732117, 0.22052738070487976, 0.7731797099113464, 0.5448750257492065, -0.6537442207336426, -0.866327702999115, 1.5360249280929565, 1.1155855655670166, -0.6184881925582886, 0.44584134221076965, -0.1136656403541565, -0.5758901238441467, 0.823943018913269, -0.1253432184457779, 0.07798782736063004, 0.8113570213317871, -0.1867598295211792, -0.19721993803977966, -0.06024744361639023, -0.9369475841522217, -0.0006476104608736932, 0.8185205459594727, 0.4922737181186676, 1.4530742168426514, 0.7586507201194763, -0.3905330300331116, 0.6764878034591675, -0.18455113470554352, 0.2749846577644348, 0.5137348771095276, 0.8350726962089539, 0.01392871979624033, 0.07888814061880112, -0.08292943984270096, 0.636151134967804, -0.4227946400642395, -0.8889729380607605, 0.07521883398294449, 0.46016764640808105, 0.06700145453214645, 0.48179858922958374, 1.0229861736297607, 0.13829860091209412, 0.39006951451301575, 0.4743664264678955, 0.6644177436828613, -0.446289986371994, -0.6345609426498413, -0.15363672375679016, -0.46300965547561646, 0.11526656895875931, -0.16244693100452423, -0.4443919360637665, -0.6157126426696777, -0.6742659211158752, 0.32332488894462585, 0.07884736359119415, 0.8885547518730164, 1.1114529371261597, 0.8637623190879822, 0.13434506952762604, -0.4911831021308899, -0.9305540919303894, -0.32771435379981995, -1.0252366065979004, -0.2365477830171585, -0.5409541130065918, -0.5555329918861389, -0.09611766785383224, -0.07099153101444244, -0.2592240869998932]}, "authors": [{"authorId": "2064448663", "name": "Aleksandar Stanic"}, {"authorId": "2243276279", "name": "Dylan R. Ashley"}, {"authorId": "2243266019", "name": "Oleg Serikov"}, {"authorId": "3031520", "name": "Louis Kirsch"}, {"authorId": "79787170", "name": "Francesco Faccio"}, {"authorId": "145341374", "name": "J. Schmidhuber"}, {"authorId": "2243267340", "name": "Thomas Hofmann"}, {"authorId": "35328044", "name": "Imanol Schlag"}], "references": [{"paperId": "11cf88dce827bd67cbfa60400306318022e736d5", "title": "D4: Improving LLM Pretraining via Document De-Duplication and Diversification"}, {"paperId": "f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f", "title": "Instruction Tuning for Large Language Models: A Survey"}, {"paperId": "2a38daf98d506477f8180806f503409d5036eaf4", "title": "TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "5b0fe50dc6df8f4eba13f8177dcd4bbe5a2b0e23", "title": "A Survey of Techniques for Optimizing Transformer Inference"}, {"paperId": "881883842c2661b41bbfc999d56c763b1ceef0bd", "title": "No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models"}, {"paperId": "b069c32fcd77160f944ab3ba71ab6f0cfb782c68", "title": "Focused Transformer: Contrastive Training for Context Scaling"}, {"paperId": "1c1e65ea4be3e53e88ba83d082c4792b2e139f39", "title": "Rockmate: an Efficient, Fast, Automatic and Generic Tool for Re-materialization in PyTorch"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "3edfe647807c2a6c71ecb8d11e9a3b8a736855ee", "title": "Large Language Model Programs"}, {"paperId": "f51bc74814a3452009ea5ca262d9768d08149ee6", "title": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model"}, {"paperId": "ece77610adfb0fb162dd22ef694f2777393c319a", "title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3", "title": "PaLM-E: An Embodied Multimodal Language Model"}, {"paperId": "76b19363b10d7ea783e4a6494eae40d73c8e9628", "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models"}, {"paperId": "68adb03744692247fb834406798894db9fe77010", "title": "A Survey on Long Text Modeling with Transformers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "7ba5c3953550b2a4c3abe1dcfc031a9674e64465", "title": "PyTorch 2.0: The Journey to Bringing Compiler Technologies to the Core of PyTorch (Keynote)"}, {"paperId": "4b308ba40e67b0b4b25c6fde17195d5a456a2f41", "title": "Cramming: Training a Language Model on a Single GPU in One Day"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "3936fd3c6187f606c6e4e2e20b196dbc41cc4654", "title": "Constitutional AI: Harmlessness from AI Feedback"}, {"paperId": "93fdf5cf598aefb0335f001039e83494dc721c3a", "title": "General-Purpose In-Context Learning by Meta-Learning Transformers"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "6edccbd83a9aae204785d4821f97855677c33866", "title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?"}, {"paperId": "e3a9af420cd2c0c8241856da92374027fefb87be", "title": "Language Model Cascades"}, {"paperId": "f843233f76a5dff07bfa93a71a1cf13d8aa6a94a", "title": "Exploring Length Generalization in Large Language Models"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "746a9b434d05b47beb1bd6a96f4d5c89d9d8bd0a", "title": "Your Transformer May Not be as Powerful as You Expect"}, {"paperId": "9695824d7a01fad57ba9c01d7d76a519d78d65e7", "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "0286b2736a114198b25fb5553c671c33aed5d477", "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers"}, {"paperId": "4fd61f6b860acc9c5da8766b7c9064f0ec896301", "title": "A Modern Self-Referential Weight Matrix That Learns to Modify Itself"}, {"paperId": "b8cb9c0b02da96a9908665ae67692a6da4dd25a4", "title": "SCENIC: A JAX Library for Computer Vision Research and Beyond"}, {"paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0", "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "76e3ad12881e7ab1c36318d8f8818eca3f828349", "title": "Meta Learning Backpropagation And Improving It"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "c00fde4c93c8c56b918ebf9f9c5c72e9b44da3b3", "title": "Optimizer Benchmarking Needs to Account for Hyperparameter Tuning"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "e363ac5be887ad3ad398c873e57e8d84484b239d", "title": "On Empirical Comparisons of Optimizers for Deep Learning"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "c7f948eaf0665fba5a34337fb3004698920c4a49", "title": "Are we really making much progress? A worrying analysis of recent neural recommendation approaches"}, {"paperId": "c2867275ec80f4d5325cf3451291e72628fb69b6", "title": "Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model"}, {"paperId": "3694381e74445a8b9f8cb8d373e39626e47191b5", "title": "On the Turing Completeness of Modern Neural Network Architectures"}, {"paperId": "eefa0df7c5678fa6004f8b48dbbc1c2696702fee", "title": "An Empirical Model of Large-Batch Training"}, {"paperId": "b2c8e834ac5f7be68b9ca3691d39925036dd74a3", "title": "Measuring the Effects of Data Parallelism on Neural Network Training"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "c88e8d85fd5160b0793598bda037f977366acf7a", "title": "Are GANs Created Equal? A Large-Scale Study"}, {"paperId": "3299aee7a354877e43339d06abb967af2be8b872", "title": "Don't Decay the Learning Rate, Increase the Batch Size"}, {"paperId": "fa9decd1395cc2f39e9921f870ebc8a8ec2bd08d", "title": "Dynamic Evaluation of Neural Sequence Models"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5feb32a73dd1bd9e13f84a7b3344497a5545106b", "title": "FastText.zip: Compressing text classification models"}, {"paperId": "2d876ed1dd2c58058d7197b734a8e4d349b8f231", "title": "Quasi-Recurrent Neural Networks"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "892e53fe5cd39f037cb2a961499f42f3002595dd", "title": "Bag of Tricks for Efficient Text Classification"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": "e0945081b5b87187a53d4329cf77cd8bff635795", "title": "Highway Networks"}, {"paperId": "af7511aace07cf9f3b3b77e0dd2477e02de5bc3a", "title": "The Psycho-Biology Of Language: AN INTRODUCTION TO DYNAMIC PHILOLOGY"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "2ef075032d4ce31e75cf28fd50c3a945b03c8abb", "title": "RECURRENT NEURAL NETWORKS AND FINITE AUTOMATA"}, {"paperId": "1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8", "title": "A new algorithm for data compression"}, {"paperId": null, "title": "Github flash attention issue"}, {"paperId": null, "title": "automatic mixed precision training"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "0c0a778e6fdf7e36b1750c533dcc916f86608607", "title": "A Survey on Context Learning"}, {"paperId": null, "title": "European Organization For Nuclear Research and OpenAIRE. Zenodo"}, {"paperId": null, "title": "Large text compression benchmark"}, {"paperId": null, "title": "Gensim\u2013python framework for vector space modelling"}, {"paperId": "9819b600a828a57e1cde047bbe710d3446b30da5", "title": "Recurrent neural network based language model"}, {"paperId": "3f3d13e95c25a8f6a753e38dfce88885097cbd43", "title": "Untersuchungen zu dynamischen neuronalen Netzen"}, {"paperId": "7509b472cbe7b1fe71a8fccf60f34cc873d1ab63", "title": "Turing computability with neural nets"}, {"paperId": null, "title": "Learning to control fast-weight memories: An alternative to recurrent nets"}, {"paperId": "676c922b522bb1099e04c4a984ec44d1c6bac403", "title": "Dynamische neuronale Netze und das fundamentale raumzeitliche Lernproblem"}, {"paperId": null, "title": "Andrej Karpathy"}, {"paperId": null, "title": "A OOD Scale Plots The following are scale plots for the best GPT and qLSTM models on the out of distribution splits of languini"}, {"paperId": null, "title": "Tri Dao"}]}