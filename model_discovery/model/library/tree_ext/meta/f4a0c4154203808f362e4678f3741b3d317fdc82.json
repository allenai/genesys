{"paperId": "f4a0c4154203808f362e4678f3741b3d317fdc82", "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry", "abstract": "Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2)\"finetuned-conversion\"of task-specific Transformers into linear versions that recover task performance, and (3)\"pretrained-conversion\"of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or\"spiky\") weights and dot-product monotonicity. We further observe surprisingly simple feature maps that retain these properties and match softmax performance, but are inefficient to compute in linear attention. We thus propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention. Experiments show Hedgehog recovers over 99% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions up to 6 perplexity points on WikiText-103 with causal GPTs, and up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also enables pretrained-conversion. Converting a pretrained GPT-2 into a linear attention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for 125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into a viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B achieves 28.1 higher ROUGE-1 points over the base standard attention model, where prior linear attentions lead to 16.5 point drops.", "venue": "arXiv.org", "year": 2024, "citationCount": 11, "influentialCitationCount": 1, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Hedgehog is proposed, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity, and enables pretrained-conversion."}, "embedding": {"model": "specter_v2", "vector": [0.4071477949619293, 1.3459872007369995, -0.35317790508270264, 0.019520247355103493, -0.24984724819660187, -0.07990723103284836, 0.7911508679389954, -0.6328652501106262, 0.048577748239040375, -0.36077749729156494, 0.7271654009819031, -0.2280857414007187, 0.5157701373100281, 0.20579689741134644, -0.15471577644348145, 0.12313758581876755, -0.9078636765480042, 0.24357302486896515, -0.09702309221029282, -0.44535645842552185, -0.3030387759208679, -0.9845913052558899, -0.7907986044883728, 0.3070484697818756, 0.08268976956605911, 0.7117870450019836, 0.44127288460731506, 0.8432249426841736, -0.35563141107559204, 0.4387102425098419, 0.36060193181037903, -0.5270397067070007, 0.06355874240398407, -0.1753499060869217, -0.4623902440071106, 0.05731404572725296, 0.17840097844600677, -0.05167854204773903, -0.45488202571868896, 0.8101588487625122, -0.19200634956359863, 0.017072316259145737, 0.4722423553466797, -0.5392459034919739, -0.8736875653266907, 1.0336545705795288, 0.7783579230308533, 0.49503305554389954, -0.45615440607070923, -0.23797187209129333, 1.4866541624069214, -1.2592257261276245, -0.04037223756313324, 1.5825432538986206, 0.39560848474502563, 0.21073423326015472, -0.12916168570518494, -0.5191856026649475, 0.8980250954627991, 0.14055363833904266, -0.5930834412574768, -0.4316069781780243, -0.24498924612998962, 0.16532808542251587, 1.9862534999847412, -0.300102174282074, -0.02872532419860363, 0.6393536925315857, -0.028961695730686188, 1.377958059310913, 0.2238004058599472, -0.5953140258789062, -0.5402589440345764, 0.21444517374038696, 0.32032376527786255, 0.7769500613212585, -0.6943851709365845, 0.18469637632369995, -0.7042003870010376, 0.127274751663208, 0.1840430349111557, -0.19864164292812347, 0.0330238975584507, 0.2969236373901367, -0.09325966984033585, 0.566129744052887, 0.4812149107456207, 0.7905144095420837, -0.1387561857700348, 0.851664125919342, 0.44074341654777527, 0.35691070556640625, -0.07938679307699203, 0.473217248916626, -0.24118636548519135, 0.3352014720439911, -0.6805408000946045, -0.05843694508075714, 0.08719848096370697, 1.194740891456604, -0.2906641364097595, 0.2720451056957245, -0.8085651993751526, 0.017600983381271362, 1.3228574991226196, 0.09723278135061264, 0.45231086015701294, -0.5989993214607239, 0.3576113283634186, -0.662045955657959, -0.34933575987815857, -0.7420298457145691, -0.368901789188385, -0.4227149486541748, -0.7877709269523621, -1.094092845916748, -0.5088803172111511, 0.1270860880613327, -0.8562694787979126, 0.6767986416816711, -0.7039030194282532, -0.007747089955955744, -0.10281497985124588, 0.45701321959495544, 0.6063968539237976, 0.9683089852333069, 0.16083383560180664, 0.21716377139091492, 1.228334903717041, -1.063317060470581, -0.7311704754829407, -1.236088752746582, 0.3517298996448517, -0.48083144426345825, 0.6505609750747681, -0.1054651141166687, -1.0875226259231567, -0.5459557175636292, -0.8299408555030823, -0.14156609773635864, -0.6358405947685242, 0.4327163100242615, 0.9221319556236267, 0.4749935269355774, -0.8342162370681763, 0.5025847554206848, -0.18141835927963257, -0.014323905110359192, 0.5412566661834717, 0.3940409719944, 0.28846773505210876, -0.23976793885231018, -1.4108706712722778, 0.33466836810112, 0.29288309812545776, -0.5115383863449097, 0.017435267567634583, -0.7508483529090881, -1.0899535417556763, 0.3578638732433319, 0.29977738857269287, -0.39757758378982544, 1.284773349761963, -0.12417811900377274, -1.57175874710083, 0.707511305809021, -0.1573125571012497, 0.17979353666305542, 0.1301088184118271, -0.18302755057811737, -0.45750677585601807, -0.5861287117004395, -0.31399455666542053, 0.31158575415611267, 0.5444114208221436, 0.1955539882183075, -0.037286434322595596, 0.5030263066291809, -0.38229480385780334, -0.18869610130786896, -0.38401344418525696, 1.1047327518463135, -0.5998599529266357, -0.18372097611427307, -0.1902746558189392, 0.3903389573097229, 0.25638332962989807, -0.4692431092262268, -0.8056053519248962, -0.9867330193519592, 0.8234540820121765, -0.13042110204696655, 1.061415433883667, -0.9443527460098267, -0.6372839212417603, -0.15574926137924194, -0.11741490662097931, -0.008660685271024704, -0.5598238706588745, 0.22243686020374298, -0.41167882084846497, 0.3152421712875366, -0.19403208792209625, -1.2579997777938843, 0.5253409147262573, -0.20360803604125977, -0.5596948862075806, -0.20199833810329437, 0.4814937114715576, 1.3284237384796143, -0.7323529720306396, 0.019921261817216873, -0.1945234090089798, 0.3579593300819397, -1.0361555814743042, 1.2520405054092407, -0.3349348306655884, -0.09667129069566727, -0.04057859629392624, -0.007846967317163944, -0.24743209779262543, -0.7007571458816528, 0.39679673314094543, -0.3500237762928009, 0.14300283789634705, 0.6119170188903809, -0.22729994356632233, 0.9251933097839355, -0.5625545382499695, 0.8476040363311768, 0.025895697996020317, -0.7884336113929749, 0.18224382400512695, 0.3416844606399536, -0.4912353754043579, -0.49949753284454346, 0.21753811836242676, 0.3146948218345642, -0.6900700926780701, 0.23194193840026855, 0.6889380812644958, 0.8157052397727966, -0.5083776712417603, -0.09414881467819214, 0.7840080261230469, -0.16505323350429535, 0.03117942251265049, 0.17155367136001587, 0.7734545469284058, 0.38569775223731995, 0.5795799493789673, -0.16250881552696228, -0.05565420538187027, -0.9436297416687012, -0.08381308615207672, 0.41491907835006714, 0.8191819190979004, 1.0294095277786255, 0.7768058180809021, -0.6347188949584961, -0.44207099080085754, 0.10558285564184189, 0.6467419266700745, 1.6040433645248413, -0.3241884708404541, -0.23745739459991455, -0.6110833883285522, -0.27215906977653503, -0.3873361647129059, 0.030960166826844215, -0.3905911147594452, -0.302144855260849, -0.5964735746383667, -1.5242899656295776, 0.7771212458610535, -0.020530223846435547, 0.6965360641479492, -0.5702889561653137, -0.0026821058709174395, -0.17464523017406464, 0.4114467203617096, -1.0746567249298096, -0.8581887483596802, 0.5267001986503601, -0.009593070484697819, 0.1347643882036209, 0.0869649350643158, -0.11078144609928131, 0.0058859712444245815, -1.1151639223098755, 0.9000403881072998, -0.898709237575531, -0.14030513167381287, 0.18240571022033691, 0.5081139206886292, -0.4989008605480194, -0.6839765310287476, 0.5728039145469666, 0.10081512480974197, 0.11844360083341599, 0.10072647035121918, 0.19509121775627136, 0.09576083719730377, 0.09089278429746628, -0.05791593715548515, 0.07724995911121368, 0.5065628290176392, 0.2050454020500183, 0.5042891502380371, -0.528085470199585, 0.06094330549240112, -1.2513954639434814, 0.8216032385826111, -0.05970252677798271, -0.32554522156715393, 0.06907886266708374, -0.801155149936676, -0.1599934697151184, 0.7685059905052185, -0.6732531189918518, -0.3695271611213684, -0.7772188186645508, 0.36944493651390076, -0.31545355916023254, -0.08248673379421234, 0.18057991564273834, -0.003551140194758773, 0.316817045211792, 0.25428077578544617, 0.5576286911964417, 0.02355559729039669, -0.08770651370286942, 0.7884519100189209, -0.6343971490859985, 0.7173135280609131, 0.48748886585235596, 0.31023579835891724, -0.41049280762672424, -0.2517794370651245, -0.6541213393211365, -0.4554797410964966, -0.14243729412555695, 0.1603492796421051, -0.15169183909893036, 0.3104081153869629, -0.2520407736301422, -0.8431938886642456, -0.08458973467350006, -1.0152872800827026, -0.43742233514785767, 0.04332495480775833, -0.4388273358345032, -0.31585973501205444, -1.309548258781433, -1.1927117109298706, -0.45638126134872437, -0.6765760779380798, -1.1932909488677979, 0.4620685279369354, 0.2208244651556015, -0.4357958734035492, -0.6261570453643799, -0.31867390871047974, -0.37762248516082764, 1.242621898651123, -0.7209365963935852, 0.6880750060081482, -0.21133491396903992, -0.2580063045024872, -0.27526959776878357, -0.031239641830325127, 0.6290478110313416, -0.24772867560386658, 0.14509566128253937, -1.1348226070404053, 0.4651767909526825, -0.6547684669494629, -0.19015367329120636, 0.07837960869073868, 0.2970263659954071, 0.49647119641304016, -0.4888859987258911, -0.3911456763744354, 0.6634508371353149, 0.9178628921508789, -0.6887255907058716, 0.23248673975467682, 0.20455484092235565, 0.9763544201850891, 0.0655968189239502, -0.5809416770935059, 0.4310034513473511, 0.49616366624832153, 0.5026066899299622, 0.18726570904254913, -0.050065502524375916, -0.3046562373638153, -0.683355450630188, 0.7557823061943054, 1.472992181777954, 0.26209285855293274, -0.04483047500252724, -1.0062854290008545, 0.6227278113365173, -1.1359715461730957, -0.7289199829101562, 0.5376811623573303, 0.3317280411720276, 0.6247816681861877, -0.2412407249212265, -0.7171828150749207, -0.4631677567958832, 0.3087022304534912, 0.35905492305755615, -0.14467811584472656, -0.7846282124519348, -0.1943027228116989, 0.29622381925582886, 0.23725147545337677, 0.6974303722381592, -0.46338438987731934, 0.7316818833351135, 14.96278190612793, 0.8091450929641724, -0.07960010319948196, 0.7713359594345093, 0.8969850540161133, 0.04637904465198517, -0.05958518758416176, -0.16340406239032745, -1.3532642126083374, 0.19753220677375793, 1.1218382120132446, 0.19101668894290924, 0.6539788246154785, 0.1541975736618042, -0.11526850610971451, 0.610944390296936, -0.48711246252059937, 0.5660233497619629, 0.3702337443828583, -1.1604496240615845, 0.044767290353775024, 0.2750798165798187, 0.440157949924469, 0.49881118535995483, 1.1066590547561646, 0.9392393231391907, 0.7615881562232971, -0.5677372813224792, 0.45213013887405396, 0.5180008411407471, 0.9377733469009399, 0.134653702378273, 0.2938377559185028, 0.3015115261077881, -0.7305867671966553, -0.2785017490386963, -0.21964502334594727, -1.059248447418213, 0.25264817476272583, 0.1686033308506012, -0.4186227321624756, -0.6936184167861938, -0.024763360619544983, 0.6312195062637329, 0.20856355130672455, 0.1079004630446434, -0.35325831174850464, 0.7913336753845215, -0.2618189752101898, -0.05003560334444046, 0.4876936078071594, 0.5409309267997742, 0.3429230749607086, 0.4998680651187897, 0.08670581877231598, -0.017924539744853973, -0.044531241059303284, 0.747064471244812, -0.6882045865058899, -0.1265677809715271, -0.03411566838622093, -0.40096211433410645, -0.16217759251594543, 0.8728544116020203, 0.6346381306648254, 0.24944058060646057, -0.555148184299469, 0.3579615354537964, 0.6110474467277527, 0.3195900619029999, -0.05982787534594536, -0.3036513030529022, 0.36753857135772705, -0.39815300703048706, 0.1663050353527069, 0.7726324200630188, -0.19404363632202148, -0.4115034341812134, -0.5332697033882141, -0.392488569021225, 0.3607448935508728, -0.8855370283126831, -0.6721011400222778, 0.7091318964958191, -0.43182721734046936, -0.1651512235403061, 0.20210608839988708, -0.719200849533081, -0.24860572814941406, 0.64219069480896, -1.3409658670425415, -0.8879876732826233, 0.2917051315307617, -0.6886191368103027, -0.24217009544372559, -0.33351773023605347, 1.028615951538086, 0.28212249279022217, -0.3201516568660736, 0.2935694754123688, 0.03380614146590233, 0.03956535458564758, -0.10803699493408203, -0.9942026138305664, 1.032848834991455, 0.16426509618759155, -0.36455151438713074, -0.08937560021877289, 0.15520749986171722, 0.558586061000824, -0.6164698600769043, -0.05298585817217827, 0.9405737519264221, -0.9273237586021423, 0.0766175389289856, -0.8182917833328247, -0.6687266230583191, 0.5454694628715515, 0.7099595665931702, -0.034030620008707047, 0.128866508603096, 0.15149669349193573, -0.554439902305603, -0.09520471841096878, -0.26955002546310425, -0.045078493654727936, 0.4272858798503876, -0.7130470871925354, -0.4142218828201294, -0.018102817237377167, 0.6726555228233337, -1.0588526725769043, -0.6325994729995728, -0.44239166378974915, 0.1732729971408844, 0.0815148875117302, 0.8479810953140259, -0.5256921052932739, 0.8766387701034546, 1.0655566453933716, -0.06905846297740936, -1.025758147239685, -0.27720195055007935, -1.293406367301941, -0.2314620465040207, 0.12104426324367523, 0.7866480946540833, -0.39977338910102844, 0.09556259214878082, 0.758236825466156, -0.05498209223151207, -0.5764247179031372, -0.5372622609138489, -0.4054436683654785, -0.14256125688552856, -0.4723169505596161, 0.25158941745758057, 0.12864123284816742, 0.39121922850608826, 0.16730475425720215, 0.19018758833408356, 0.4465623199939728, -0.19398652017116547, -0.8406137228012085, 0.015191517770290375, -0.0046567153185606, -0.5064878463745117, -0.8470151424407959, -0.6867544054985046, -1.4848639965057373, 0.044960010796785355, -1.3471776247024536, 0.021317124366760254, -0.7172324657440186, -0.4589475691318512, 0.22460107505321503, -0.1412895768880844, 0.4936322569847107, 0.06282132863998413, -0.23687992990016937, -0.10812518745660782, -0.6462563872337341, -0.4320058524608612, 0.7877055406570435, 0.8447209000587463, -0.895768404006958, 0.08606097102165222, 0.16489718854427338, -0.28408846259117126, 0.16483037173748016, 0.3179381787776947, -0.5109289288520813, -0.4235081672668457, -1.305081844329834, 0.4862356185913086, -0.31236475706100464, -0.12506486475467682, -0.47115492820739746, 0.6272417902946472, 0.4477626085281372, -0.4764696955680847, 0.5670060515403748, 0.3252776265144348, -0.8040306568145752, -0.6094070076942444, 0.19377809762954712, -0.766305148601532, 0.11934232711791992, 0.19841472804546356, -0.6212512254714966, -0.2875820994377136, 0.6905335187911987, 0.0672474354505539, -0.9006159901618958, -0.5641247630119324, 0.25033795833587646, -0.5354366898536682, 0.23812443017959595, -0.5603528022766113, 0.1393207609653473, -1.3647327423095703, -0.47309398651123047, -0.3313910961151123, 0.2885073125362396, -0.7902685403823853, 1.0813028812408447, -0.06984052807092667, -1.226144552230835, 0.028534837067127228, 0.1986275464296341, -0.12193753570318222, -0.08859214186668396, 0.14348818361759186, 0.6177856922149658, -0.08643514662981033, 0.6112609505653381, 0.3861302137374878, 0.45678994059562683, -0.664631187915802, -0.0916791781783104, 0.8988403677940369, -0.5878610610961914, -0.32167282700538635, 0.9772849678993225, -0.13159802556037903, -1.2590436935424805, 0.3154362142086029, -1.3448127508163452, -0.334830105304718, -0.1400012969970703, 0.5299800634384155, -0.038508109748363495, 0.0440705381333828, -0.047616951167583466, -0.4280549883842468, 0.08779217302799225, 0.17266030609607697, -0.17721277475357056, 0.7265381217002869, -0.14766079187393188, -0.2229020595550537, 0.35939717292785645, 1.2536076307296753, -0.9962987899780273, -0.6833745241165161, -1.0123335123062134, -0.2502305209636688, 0.5251047611236572, 0.4888155162334442, -0.36652320623397827, -0.7912933826446533, 0.7737035751342773, 0.36971309781074524, 0.3742925226688385, 0.19147442281246185, 0.1953558325767517, -0.19098380208015442, 0.8475525975227356, -0.30479905009269714, -0.7125635743141174, -0.6262837052345276, 1.4599870443344116, 1.1509183645248413, -0.8521803617477417, 0.03128991648554802, -0.26783469319343567, -0.6971878409385681, 0.7854006290435791, -0.02603163570165634, 0.13240772485733032, 0.9421011209487915, 0.1883014589548111, -0.2770329713821411, 0.10971682518720627, -1.1363604068756104, -0.451848566532135, 0.8778724670410156, 0.9084984064102173, 0.6374643445014954, 0.36969271302223206, 0.32310616970062256, 1.096954584121704, -0.18537329137325287, -0.03997359052300453, 0.3934027850627899, 0.21076196432113647, -0.12917467951774597, 0.2669205367565155, -0.08133474737405777, 0.7406584620475769, -0.8570094704627991, -0.7927539348602295, 0.19963760673999786, 0.3231807351112366, 0.051558900624513626, 0.34753847122192383, 0.8265065550804138, 0.1533912569284439, 0.8237513899803162, 0.35718321800231934, 0.17550337314605713, -0.45804306864738464, -0.17790931463241577, -0.29356271028518677, -0.4547571539878845, -0.24169263243675232, 0.16240689158439636, -0.4618934988975525, -0.16181305050849915, -0.18869179487228394, 0.39696004986763, -0.41387641429901123, 0.23977956175804138, 1.221805214881897, 0.5767822265625, 0.8810782432556152, -0.2775988280773163, -0.7235624194145203, -0.3779989778995514, -0.9823563098907471, 0.03547815978527069, -0.7690104842185974, -0.0011619789293035865, 0.16827301681041718, -0.13446341454982758, 0.16526839137077332]}, "authors": [{"authorId": "2124860797", "name": "Michael Zhang"}, {"authorId": "2056438239", "name": "Kush S. Bhatia"}, {"authorId": "2006456125", "name": "Hermann Kumbong"}, {"authorId": "2259934664", "name": "Christopher R'e"}], "references": [{"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "39ed1c33af6f0a5fbc16354afcb223a03c9c139b", "title": "Fast Attention Requires Bounded Entries"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "ac608a4a6b19b3208e560eee5daadb3cc18638a2", "title": "Efficient Attention via Control Variates"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "f6d8beb02771791d628f7e0773d8906261ce707c", "title": "Fine-Tuning Pre-trained Transformers into Decaying Fast Weights"}, {"paperId": "ac2e15fbfe3ea338725f5d33d17a5a687609c431", "title": "On The Computational Complexity of Self-Attention"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21", "title": "cosFormer: Rethinking Softmax in Attention"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "4b0541eccd8f98852d6807a14fbac17f775c7b40", "title": "Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\u00f6m Method"}, {"paperId": "f27e8c4731c575bd5f5db4c93ad8588f684dcbd0", "title": "Hybrid Random Features"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61", "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"}, {"paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35", "title": "Emerging Properties in Self-Supervised Vision Transformers"}, {"paperId": "054e307c1edf4b28137ffcbce980fe81f0647d20", "title": "Finetuning Pretrained Transformers into RNNs"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "27bb43b3c7f3bb9412c9b1de8366638ec4efb1af", "title": "Exploring Alternatives to Softmax Function"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "f9700e31a1d0ae34d4571ab056dfb268c1543349", "title": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization"}, {"paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3", "title": "Transformer Dissection: An Unified Understanding for Transformer\u2019s Attention via the Lens of Kernel"}, {"paperId": "7cf64265882f7129b127ce0e27ff7bca9173aa58", "title": "The Context"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "c91ae35dbcb6d479580ecd235eabf98374acdb55", "title": "Using Fast Weights to Attend to the Recent Past"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "7a59fde27461a3ef4a21a249cc403d0d96e4a0d7", "title": "Random Features for Large-Scale Kernel Machines"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "I\u2019m with Hilary atm and won\u2019t let go of her for the rest of the day, so any option you guys choose is good for me"}]}