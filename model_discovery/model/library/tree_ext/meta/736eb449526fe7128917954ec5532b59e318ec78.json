{"paperId": "736eb449526fe7128917954ec5532b59e318ec78", "title": "Block-Recurrent Transformers", "abstract": "We introduce the Block-Recurrent Transformer, which applies a transformer layer in a recurrent fashion along a sequence, and has linear complexity with respect to sequence length. Our recurrent cell operates on blocks of tokens rather than single tokens during training, and leverages parallel computation within a block in order to make efficient use of accelerator hardware. The cell itself is strikingly simple. It is merely a transformer layer: it uses self-attention and cross-attention to efficiently compute a recurrent function over a large set of state vectors and tokens. Our design was inspired in part by LSTM cells, and it uses LSTM-style gates, but it scales the typical LSTM cell up by several orders of magnitude. Our implementation of recurrence has the same cost in both computation time and parameter count as a conventional transformer layer, but offers dramatically improved perplexity in language modeling tasks over very long sequences. Our model out-performs a long-range Transformer XL baseline by a wide margin, while running twice as fast. We demonstrate its effectiveness on PG19 (books), arXiv papers, and GitHub source code. Our code has been released as open source.", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 66, "influentialCitationCount": 9, "openAccessPdf": {"url": "https://arxiv.org/pdf/2203.07852", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": null}, "embedding": {"model": "specter_v2", "vector": [0.24270999431610107, 0.5490520596504211, -0.11617559939622879, -0.05837882682681084, -0.07424013316631317, -0.1498616486787796, 1.0413265228271484, -0.2560805678367615, -0.16963179409503937, -0.40297383069992065, 0.7340896129608154, -0.06464562565088272, 0.8348314166069031, -0.08039096742868423, -0.3306407630443573, -0.07666563987731934, -0.7280121445655823, 0.032676778733730316, -0.07990849763154984, -0.5108264684677124, 0.2961729168891907, -0.15748393535614014, -1.1898890733718872, 0.09479103237390518, 0.14324961602687836, 0.568227231502533, 0.3008286952972412, 0.8198565244674683, -0.4896495044231415, 1.043851613998413, 0.6124046444892883, -0.18830357491970062, 0.025779640302062035, -0.34897759556770325, -0.4072420597076416, -0.4580797851085663, 0.19953396916389465, -0.07426740229129791, -0.5403754115104675, 0.5625244379043579, -0.2577493190765381, 0.20878103375434875, 0.20102201402187347, -0.4357200860977173, 0.12278512120246887, 0.8588135242462158, 0.739501953125, 0.7117490172386169, -0.3100491166114807, -0.7349701523780823, 1.3518149852752686, -1.253029704093933, 0.07707113027572632, 1.3141056299209595, 0.5704177021980286, 0.4958767890930176, -0.2110537737607956, -0.8438769578933716, 0.9594599008560181, -0.0069755977019667625, -0.696613073348999, -0.5785998702049255, 0.05801461264491081, 0.1667717546224594, 2.210489511489868, -0.337693989276886, 0.44341540336608887, 0.4201313257217407, 0.07924497127532959, 1.3985223770141602, 0.22664178907871246, -0.5889288187026978, -0.6078104376792908, -0.03169157728552818, 0.4294891357421875, 0.9377240538597107, -0.3343002498149872, 0.35714152455329895, -0.9533554911613464, 0.2200450301170349, 0.5655564069747925, 0.35901308059692383, 0.026400486007332802, -0.23076976835727692, -0.3627387583255768, 0.49410560727119446, 0.5031945109367371, 1.0179989337921143, -0.40318697690963745, 0.8569008708000183, 0.5818455219268799, 0.1697118878364563, 0.04686177149415016, 0.22672314941883087, 0.03599892184138298, 0.04398629814386368, -0.5690966248512268, 0.2374652922153473, -0.17473506927490234, 0.7527933716773987, 0.13130290806293488, 0.6074215769767761, -0.40416213870048523, 0.21263520419597626, 1.3743064403533936, 0.19786104559898376, 0.6902912855148315, -0.3845556378364563, -0.004632746335119009, -0.6090445518493652, -0.15640990436077118, -0.7162343263626099, -0.023632794618606567, -0.5178287029266357, -0.8911672234535217, -1.1911230087280273, -0.792803943157196, 0.6498552560806274, -0.8365156054496765, 0.7481400966644287, -0.5066061615943909, 0.3637981712818146, 0.1728784143924713, 0.059140220284461975, 0.5854479670524597, 0.6841045022010803, 0.22146277129650116, -0.002412300556898117, 0.8872371315956116, -1.031619906425476, -0.7016700506210327, -0.8742944598197937, 0.7150708436965942, 0.03528813272714615, -0.09483002126216888, -0.13188424706459045, -1.314895510673523, -0.9902017712593079, -0.8490128517150879, 0.20747238397598267, -0.5505679845809937, -0.036794889718294144, 0.9146199226379395, 0.28838565945625305, -1.1963486671447754, 0.9248248934745789, -0.6688265204429626, 0.011077314615249634, 0.320608913898468, 0.1771177351474762, 0.3746649920940399, -0.006436513736844063, -1.0666786432266235, 0.5438418984413147, 0.04600835219025612, -0.1412055790424347, -0.32177606225013733, -0.7786628603935242, -1.0831801891326904, 0.12875968217849731, -0.19644996523857117, -0.29003721475601196, 1.526589274406433, -0.061333123594522476, -1.2940161228179932, 0.730997622013092, -0.6869476437568665, -0.10875988006591797, 0.0870312750339508, -0.21303178369998932, -0.3816629648208618, -0.5663886070251465, -0.15020862221717834, 0.34891194105148315, 0.459102600812912, -0.07514576613903046, -0.41352948546409607, -0.024311479181051254, -0.7380954623222351, -0.15475374460220337, -0.0668349489569664, 0.8987648487091064, -0.1999567598104477, -0.25717058777809143, 0.3745308220386505, 0.7142311334609985, 0.03714518994092941, -0.8881643414497375, -0.2716293931007385, -1.3193111419677734, 0.7945823073387146, 0.1450091451406479, 1.1862012147903442, -1.0184694528579712, -0.9480887055397034, -0.3047492504119873, -0.15731076896190643, -0.18765434622764587, -0.5419685244560242, 0.5290693044662476, -0.8869047164916992, 0.33976301550865173, 0.2247132807970047, -0.7843980193138123, -0.14536982774734497, -0.26085224747657776, -1.1972250938415527, -0.11180640012025833, 0.23596201837062836, 1.1570969820022583, -0.9484653472900391, 0.11944914609193802, 0.011455712839961052, 0.38582533597946167, -0.6803392171859741, 1.2656962871551514, 0.1493356078863144, 0.0019691085908561945, -0.12874546647071838, -0.30825915932655334, -0.06403777748346329, -0.4588639438152313, 0.3917282819747925, -0.3955324590206146, -0.34149736166000366, 1.0834869146347046, -0.3233957290649414, 1.12178635597229, -0.5711702704429626, 0.5379683375358582, -0.37290772795677185, -0.9502720832824707, 0.451215922832489, 0.08534153550863266, -0.3215870261192322, -0.7827194333076477, 0.14973720908164978, 0.3409309387207031, -0.7634532451629639, 0.7292875647544861, 0.7777455449104309, 0.7531059980392456, -0.26651909947395325, 0.27561384439468384, 0.5402624607086182, -0.2531066834926605, 0.2653491199016571, 0.6232922673225403, 0.8604074120521545, 0.5289549827575684, 0.2358410507440567, 0.08648769557476044, 0.2894693613052368, -1.0094796419143677, 0.11341451853513718, 0.5685891509056091, 0.722702145576477, 0.8598034381866455, 0.34100648760795593, -0.8110162615776062, -0.3770853579044342, 0.007656608708202839, 0.7716734409332275, 1.220395565032959, -0.39960718154907227, -0.33517318964004517, -0.46451684832572937, -0.148654043674469, -0.5652739405632019, 0.017284026369452477, -0.29233917593955994, -0.3244490325450897, -0.8959394097328186, -0.862751841545105, 0.8217992186546326, 0.3955334722995758, 1.2501643896102905, -1.0228729248046875, -0.5708473920822144, -0.24746419489383698, 0.22652088105678558, -0.7781705260276794, -0.3456050753593445, 0.679425060749054, -0.9081442356109619, -0.05041772127151489, 0.13720320165157318, -0.43164780735969543, 0.18829931318759918, -0.5728219151496887, 1.0988017320632935, -0.33996424078941345, -0.18509025871753693, 0.08210892230272293, 0.6642549633979797, -0.47055163979530334, -0.7479632496833801, 0.30152490735054016, 0.1194429099559784, -0.08394297957420349, 0.04565640911459923, 0.339731901884079, -0.12656688690185547, -0.17991405725479126, -0.042628467082977295, 0.2973599135875702, 0.07365164160728455, 0.1654587835073471, 0.5341528654098511, -0.27244704961776733, -0.04091354459524155, -1.1142586469650269, 0.551408588886261, 0.5058371424674988, -0.38406917452812195, 0.26768800616264343, -0.9551857709884644, -0.14967738091945648, 0.2143668681383133, -0.2636728286743164, -0.25013747811317444, -0.8652252554893494, 0.49408480525016785, -0.39903298020362854, 0.13518211245536804, -0.07278835028409958, 0.3651001751422882, 0.6021179556846619, -0.28468117117881775, 0.7457565069198608, 0.25548285245895386, 0.056839726865291595, 0.27516287565231323, -0.6735448241233826, 0.6133735179901123, 0.704536497592926, 0.15272913873195648, -0.09426471590995789, -0.22292475402355194, -0.47535017132759094, -0.5040299892425537, -0.36475491523742676, 0.16428710520267487, -0.320995032787323, -0.08120520412921906, -0.751846194267273, -1.1713331937789917, 0.45275768637657166, -1.0854023694992065, -0.38118696212768555, 0.08843275904655457, -0.534108579158783, -0.2986244559288025, -1.2098723649978638, -1.1609184741973877, -0.9906434416770935, -0.8628928065299988, -0.5788103342056274, 0.1859743297100067, 0.3602581322193146, -0.2259044647216797, -0.4941979944705963, 0.1415095329284668, -0.7853682041168213, 0.9126048684120178, -0.6911961436271667, 0.3916859030723572, 0.022402122616767883, -0.38142120838165283, -0.19378900527954102, 0.333787202835083, 0.5316879749298096, -0.10607676208019257, 0.18959803879261017, -0.8958300948143005, 0.20621588826179504, -0.3053300976753235, -0.06741205602884293, 0.4065464437007904, 0.19991198182106018, 0.7037559747695923, 0.10782406479120255, -0.6733707189559937, 0.27755725383758545, 1.2272757291793823, -0.3740924298763275, 0.39047756791114807, 0.24847301840782166, 0.8600053191184998, 0.041950520128011703, -0.33037126064300537, 0.30225273966789246, 0.3646908700466156, 0.2113681435585022, 0.10318216681480408, -0.3272528648376465, -0.05336306616663933, -0.8279839158058167, 0.7943952679634094, 1.5287646055221558, 0.2777041792869568, 0.24569275975227356, -0.7214385271072388, 0.8924186825752258, -1.077314019203186, -1.0585135221481323, 0.7476575374603271, 0.6566488146781921, 0.34438076615333557, -0.40175914764404297, -0.21356472373008728, 0.07839024066925049, 0.6136878132820129, 0.47349268198013306, -0.4952271282672882, -1.0489577054977417, -0.14143893122673035, 0.6956160068511963, 0.20529107749462128, 0.4754813015460968, -0.13892695307731628, 0.7856665849685669, 15.054922103881836, 0.5621951818466187, -0.1896427422761917, 0.4166543483734131, 0.6907795667648315, 0.13163185119628906, -0.30323293805122375, 0.022521017119288445, -1.516735315322876, -0.24643780291080475, 1.3061447143554688, 0.3503762185573578, 0.7899947762489319, 0.009904880076646805, -0.036043670028448105, 0.3366983234882355, -0.4365523159503937, 0.8032931089401245, 0.5675204396247864, -1.2333855628967285, 0.11482752859592438, 0.152350053191185, 0.04478369653224945, 0.8387977480888367, 0.8290591835975647, 0.9622514247894287, 0.8762272596359253, -0.43575549125671387, 0.47004052996635437, 0.500022828578949, 0.7459099292755127, -0.05520593374967575, 0.1639351099729538, 0.19521616399288177, -1.0441336631774902, -0.26899585127830505, -0.18833161890506744, -1.1374812126159668, 0.071192666888237, 0.14180821180343628, -0.4495510458946228, -0.5754420757293701, 0.16073669493198395, 1.289526343345642, 0.20686742663383484, -0.02334928885102272, -0.33452367782592773, 0.5794546008110046, -0.1641547679901123, -0.18215526640415192, 0.1375102549791336, 0.40318864583969116, -0.02521543577313423, 0.1957337111234665, 0.10823701322078705, 0.14349108934402466, -0.48536503314971924, 0.5241744518280029, -0.43445950746536255, -0.32889389991760254, -0.4702376127243042, -0.4872416853904724, -0.09847214818000793, 1.0588395595550537, 0.45198696851730347, 0.055208221077919006, -0.13358652591705322, 0.2225349396467209, 0.5546574592590332, 0.0980638861656189, -0.4173155426979065, -0.260830283164978, 0.36163339018821716, -0.35600122809410095, 0.4925841987133026, 0.5534549951553345, -0.16067203879356384, -0.6862421035766602, -0.9258014559745789, -0.27570870518684387, 0.2023652195930481, -1.075265645980835, -0.315700501203537, 0.9861811399459839, -0.516398012638092, -0.2882395088672638, -0.1204514279961586, -0.5822122097015381, -0.30769073963165283, 0.48173394799232483, -1.5673171281814575, -0.38371583819389343, 0.1319069117307663, -0.27550941705703735, -0.5365408658981323, -0.04832390695810318, 0.9489167928695679, -0.17835943400859833, -0.4353910982608795, -0.16533949971199036, -0.2871410846710205, 0.28796353936195374, -0.7805796265602112, -0.7445903420448303, 1.029001235961914, 0.1827734112739563, 0.05744026601314545, 0.5238239169120789, 0.14076435565948486, 0.3970620632171631, -0.7540602684020996, 0.14411690831184387, 0.9879480600357056, -0.6946030855178833, -0.13837051391601562, -0.7669033408164978, -0.7334408164024353, 0.4772830903530121, 0.5050162076950073, -0.04773004725575447, 0.05684347078204155, 0.24850879609584808, -0.7636009454727173, -0.5284023880958557, -0.37786898016929626, 0.15053345263004303, 0.6696715354919434, -0.9521984457969666, -0.2757090628147125, -0.577154815196991, 0.30312132835388184, -0.7134815454483032, -0.33307239413261414, -0.05484825745224953, 0.3322984576225281, -0.3277234137058258, 0.8600966334342957, -0.2505127191543579, 0.4314495921134949, 0.848050594329834, -0.0012642436195164919, -0.722777783870697, -0.04608180373907089, -0.9893031716346741, 0.06610725820064545, 0.36145198345184326, 0.4679814577102661, -0.6223913431167603, 0.038224801421165466, 0.5418961048126221, 0.21327689290046692, -0.5383713245391846, -0.5915813446044922, -0.22042027115821838, -0.25555935502052307, -0.33995938301086426, 0.4201504588127136, -0.10935550183057785, 0.0961921289563179, 0.2916543483734131, 0.7679125666618347, -0.051744136959314346, -0.39983874559402466, -0.4680415689945221, 0.19416818022727966, -0.1681346595287323, 0.27968448400497437, -0.8587839603424072, -0.5042131543159485, -1.427223801612854, 0.1159840002655983, -1.2299551963806152, -0.0044810473918914795, -1.076733112335205, -0.2366853803396225, 0.03741408511996269, -0.6581032276153564, 0.3273168206214905, 0.4116460382938385, -0.3111351728439331, -0.35487860441207886, -0.49023765325546265, -0.6291401982307434, 0.8941948413848877, 0.7794286608695984, -0.5754380226135254, 0.06884437799453735, -0.25145968794822693, -0.1541028618812561, 0.014238269068300724, 0.4469427168369293, -0.39516517519950867, -0.5244057774543762, -0.9194555282592773, 0.5079013109207153, -0.16268110275268555, -0.03852260857820511, -0.5809528827667236, 0.5086644887924194, 0.6229854822158813, -0.2184498906135559, -0.27893364429473877, -0.0029630258213728666, -0.46691417694091797, -0.2756163477897644, 0.6089098453521729, -0.6052355170249939, 0.664229154586792, 0.17738138139247894, -0.6013344526290894, -0.2582937777042389, 0.8930653929710388, -0.1556617021560669, -1.117950439453125, -0.3838411867618561, 0.3462124764919281, -0.8947252035140991, -0.11615161597728729, -0.4336550533771515, -0.45709508657455444, -1.030309796333313, -0.29363206028938293, 0.16585828363895416, 0.24589213728904724, -0.5739838480949402, 1.0500209331512451, 0.3686889111995697, -1.2564728260040283, 0.14543217420578003, 0.6298051476478577, -0.29563915729522705, -0.10001104325056076, 0.19143342971801758, 0.44450414180755615, -0.2509225010871887, 0.6427778005599976, 0.38901183009147644, 0.0757160559296608, -0.8920103311538696, -0.18824532628059387, 1.2060534954071045, -0.7740923762321472, -0.38295722007751465, 0.7230444550514221, -0.21572363376617432, -0.9133225679397583, 0.08421310782432556, -1.2790542840957642, -0.624731719493866, -0.1573329120874405, 0.5971788167953491, -0.07832343131303787, 0.009227734059095383, -0.037175145000219345, -0.4166978895664215, 0.2594163715839386, -0.191289484500885, -0.6115254759788513, 0.8542420268058777, -0.04226480424404144, -0.34049054980278015, 0.9174943566322327, 0.7608616352081299, -0.9811626076698303, -0.4874873459339142, -0.7592471241950989, -0.28659674525260925, 0.3164730668067932, 0.47631967067718506, -0.2364872395992279, -0.7665976881980896, 0.8202109336853027, 0.6309503316879272, 0.531089723110199, 0.34758228063583374, -0.22744221985340118, -0.013162179850041866, 0.5951505899429321, 0.12621469795703888, -0.552671492099762, -0.4869260787963867, 1.611626386642456, 1.2644459009170532, -0.5034447312355042, 0.2334231287240982, 0.08607171475887299, -0.6152812838554382, 0.6222898364067078, -0.04976484924554825, -0.3923705816268921, 1.094743013381958, -0.17385226488113403, 0.09855616837739944, 0.12761111557483673, -1.3891074657440186, -0.2959410846233368, 0.1434769481420517, 0.8507323861122131, 0.8773234486579895, 0.11908832937479019, 0.27077552676200867, 0.5684176087379456, 0.01778344437479973, 0.018098698928952217, 0.24209409952163696, 0.6102986335754395, -0.03802691027522087, -0.1314704567193985, 0.23199139535427094, 0.39450979232788086, -0.6640675067901611, -0.7122151255607605, 0.3109605610370636, 0.4165269732475281, -0.1912599802017212, 0.6344765424728394, 1.2545194625854492, 0.01762971468269825, 0.6678224802017212, 0.15577827394008636, 0.6534484028816223, -0.3046382963657379, -0.8289725184440613, 0.014279736205935478, -0.5716728568077087, -0.2769201695919037, -0.37737324833869934, -0.8888656497001648, -0.5749726891517639, -0.253784716129303, 0.13220514357089996, 0.18503904342651367, 0.4957922101020813, 0.8836190700531006, 0.5263331532478333, 0.5401319265365601, 0.15305931866168976, -0.3147727847099304, -0.27903491258621216, -1.1111218929290771, -0.055252645164728165, -0.475125253200531, 0.214851513504982, -0.10553158819675446, -0.08260611444711685, -0.42542049288749695]}, "authors": [{"authorId": "32913644", "name": "DeLesley S. Hutchins"}, {"authorId": "35328044", "name": "Imanol Schlag"}, {"authorId": "3374063", "name": "Yuhuai Wu"}, {"authorId": "52136425", "name": "Ethan Dyer"}, {"authorId": "3007442", "name": "Behnam Neyshabur"}], "references": [{"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "b7422b7a7830cd899b47b03e514d8151ffb74c03", "title": "SQuALITY: Building a Long-Document Summarization Dataset the Hard Way"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "0e802c0739771acf70e60d59c2df51cd7e8c50c0", "title": "Memorizing Transformers"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "12809bcb734beafeb47876f42e7b438e27fe99fe", "title": "General-purpose, long-context autoregressive modeling with Perceiver AR"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "6281c40c66febca1d8003bcc6fdfd2189b30c38f", "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences"}, {"paperId": "73d64ecbe3e846394444dab6c5e89ba33e5daa49", "title": "Memory transformer with hierarchical attention for long document processing"}, {"paperId": "e528466e2aff981511d4ca6e063211297c0b4175", "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"}, {"paperId": "f75d05e759447c2aedb7097728f29f9a520d9bc1", "title": "Do Long-Range Language Models Actually Use Long-Range Context?"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6", "title": "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61", "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"}, {"paperId": "b50815251c948f00baedccaf5f56c281ffa7650f", "title": "Staircase Attention for Recurrent Processing of Sequences"}, {"paperId": "64a29bee2e1ad29547d590a3cc26274f4c537145", "title": "Not All Memories are Created Equal: Learning to Forget by Expiring"}, {"paperId": "dfb37e6216e792bf6bd5a30c0fc7ad55df1cb71e", "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth"}, {"paperId": "b3bf9fe13195e9aa70e1dac04e01fcff7008e812", "title": "Perceiver: General Perception with Iterative Attention"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d", "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute"}, {"paperId": "1a703f08da01cf737cce3fb9064259b3f4b44e9c", "title": "Linear Transformers Are Secretly Fast Weight Programmers"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "806adbb35ed4a95f51518f5962fd59685ad4706b", "title": "Query-Key Normalization for Transformers"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "08066c80919620397e8e4e5372ff84caf401e675", "title": "Global Relational Models of Source Code"}, {"paperId": "d27669c82faf78ea08cceaa0a171b540cccc304d", "title": "ETC: Encoding Long and Structured Inputs in Transformers"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "657329c633709dd1ac34a30d57341b186b1a47c2", "title": "Efficient Content-Based Sparse Attention with Routing Transformers"}, {"paperId": "d08463bd665589d04619f04dbde84183ffcf2e63", "title": "Towards a Human-like Open-Domain Chatbot"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "449892c8e095a97b4c9e058ae5be1e9177d805b7", "title": "R-Transformer: Recurrent Neural Network Enhanced Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context"}, {"paperId": "bb669de2fce407df2f5cb2f8c51dedee3f467e04", "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "7ba9b6266569bd7b6a3c2ec64348c5b969a5ceb7", "title": "Simple Recurrent Units for Highly Parallelizable Recurrence"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "b92aa7024b87f50737b372e5df31ef091ab54e62", "title": "Training Very Deep Networks"}, {"paperId": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "title": "LSTM: A Search Space Odyssey"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "61639af1a89c69094bcc0ed40fad752832b037c3", "title": "Reducing the Ratio Between Learning Complexity and Number of Time Varying Variables in Fully Recurrent Nets"}, {"paperId": "2ae5a5507253aa3cada113d41d35fada1e84555f", "title": "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories"}, {"paperId": null, "title": "\u201cAddressing some limitations of transformers with feedback memory,\u201d"}, {"paperId": null, "title": "Github source code repository"}, {"paperId": null, "title": "using/curating? [Yes] See Appendix C.1. (e) Did you discuss whether the data you are using/curating contains personally identi\ufb01able information or offensive content? [Yes] See Appendix C.1. 14"}, {"paperId": null, "title": "Results are shown in"}, {"paperId": null, "title": "\u201cMeliad"}]}