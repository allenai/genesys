{"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model", "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}.", "venue": "International Conference on Learning Representations", "year": 2022, "citationCount": 847, "influentialCitationCount": 110, "openAccessPdf": {"url": "http://arxiv.org/pdf/2210.02414", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "An attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained is unveiled and a unique scaling property of GLM-130B is leveraged to reach INT4 quantization without post training, with almost no performance loss."}, "embedding": {"model": "specter_v2", "vector": [0.0156563613563776, 0.2643319368362427, -0.7018544673919678, 0.04042784869670868, -0.45870810747146606, 0.1956084817647934, 0.6306003332138062, -0.29148030281066895, -0.4815840721130371, -0.3609829843044281, 0.44840505719184875, -0.5613827109336853, 0.4926445782184601, 0.240330308675766, -0.07734772562980652, -0.08564102649688721, -1.1290969848632812, 0.3267819881439209, -0.6997671723365784, -0.2333289235830307, -0.17335310578346252, -0.781780481338501, -0.5650085806846619, -0.33508652448654175, 0.3869805932044983, 0.6871465444564819, 0.021614592522382736, 1.1350849866867065, -0.2600577473640442, 0.1477804034948349, 0.3777950704097748, -0.021273471415042877, 0.49857571721076965, -0.062453266233205795, -0.4486737549304962, 0.053661081939935684, 0.22015859186649323, -0.4676467776298523, -0.30716201663017273, 1.0209929943084717, -0.026926735416054726, 0.2371227890253067, 0.38523370027542114, -0.8322598934173584, -0.7038995623588562, 0.5379493236541748, 0.35115963220596313, 0.7180122137069702, -0.4162619113922119, -0.3604978621006012, 1.2941762208938599, -1.4207019805908203, -0.1319800317287445, 1.4038561582565308, 0.3223263919353485, 0.5571612119674683, -0.33054834604263306, -0.6580451130867004, 0.5297496914863586, -0.13184484839439392, -0.9667820334434509, -0.02054370380938053, -0.3330797255039215, -0.023607101291418076, 1.7477959394454956, -0.48748302459716797, 0.4496612250804901, 0.6247670650482178, 0.03633353114128113, 0.9610332250595093, 0.1922803372144699, -0.7978136539459229, -0.21509402990341187, 0.11596221476793289, 0.4752161502838135, 0.5966845750808716, -0.21754255890846252, 0.7622320055961609, -0.64546799659729, -0.05401390418410301, 0.12341246008872986, -0.05113688111305237, 0.2732909023761749, 0.253364235162735, -0.13901177048683167, 0.8326455354690552, 0.5636841058731079, 0.6121761202812195, 0.13392072916030884, 0.8240542411804199, 0.33868199586868286, 0.2803000807762146, 0.41846251487731934, 0.03380637243390083, -0.2092253863811493, 0.32109105587005615, -1.1050633192062378, 0.296098530292511, -0.03454593941569328, 0.8324944376945496, 0.1306956559419632, 0.23544760048389435, -0.5915167927742004, -0.3286750018596649, 1.5344535112380981, 0.25041070580482483, 0.37546077370643616, -0.604659378528595, 0.0944233313202858, -0.9441255331039429, -0.28719523549079895, -0.6710395812988281, -0.2459934800863266, -0.2526804208755493, -0.9719733595848083, -1.106437087059021, -0.7396238446235657, 0.24250243604183197, -0.952876627445221, 0.7475126385688782, -0.31850138306617737, 0.37959516048431396, 0.2998904287815094, 0.8289728164672852, 0.463575154542923, 0.8154938817024231, 0.1203744038939476, 0.3360835015773773, 0.8521237373352051, -1.147357702255249, -0.3473072052001953, -0.8841185569763184, 0.48718658089637756, -0.372558057308197, 0.27226778864860535, 0.003366130869835615, -0.8975590467453003, -0.9954043030738831, -0.9723779559135437, -0.21577246487140656, -0.5721302032470703, 0.19028764963150024, 1.1399894952774048, 0.8572936058044434, -1.2762905359268188, 0.46349847316741943, -0.17112180590629578, -0.10974796861410141, 0.35217636823654175, 0.1389598697423935, 0.5069449543952942, -0.49713075160980225, -1.2507308721542358, 0.16261419653892517, 0.47930610179901123, -0.49594277143478394, -0.4923202693462372, -0.6269527077674866, -0.7054774761199951, -0.05709283426403999, 0.10866189002990723, -0.327261745929718, 1.0271735191345215, 0.06284309178590775, -1.22528874874115, 0.9763104319572449, -0.39702075719833374, -0.12938633561134338, 0.23401986062526703, -0.16798733174800873, -0.6522035002708435, -0.4289821684360504, -0.12782056629657745, 0.9558261632919312, 0.392292320728302, 0.06162819638848305, 0.10008562356233597, 0.21531368792057037, -0.49600547552108765, 0.2427130788564682, -0.3704843819141388, 1.01963472366333, -0.8092852830886841, -0.49254822731018066, 0.3242041766643524, 0.30793917179107666, -0.3212631940841675, -0.023550737649202347, -0.803091287612915, -0.797920823097229, 0.6992596983909607, -0.3297329246997833, 0.8046947717666626, -1.1003987789154053, -0.9909152984619141, 0.12325157970190048, -0.12794895470142365, -0.046833381056785583, -0.4538118839263916, 0.07226809114217758, -0.3283853530883789, 0.6155215501785278, -0.1605696678161621, -1.235676646232605, 0.1815313696861267, -0.24958652257919312, -0.42612454295158386, 0.09494146704673767, 0.19068177044391632, 0.9630857110023499, -0.5629990100860596, 0.15331235527992249, -0.04528892785310745, 0.6032688617706299, -1.575149655342102, 0.899756669998169, -0.4187708795070648, 0.5630688071250916, 0.028468450531363487, -0.15929418802261353, 0.45949479937553406, -0.500565767288208, 0.2222963571548462, -0.040906231850385666, 0.05799093097448349, 0.54781174659729, -0.6746319532394409, 1.6968142986297607, -0.029991814866662025, 0.46973469853401184, -0.04362531751394272, -0.8601081371307373, 0.2902262508869171, 0.19064167141914368, -0.1404285728931427, -0.47347745299339294, 0.4142906069755554, 0.4406973421573639, -0.614520251750946, 0.8491038084030151, 0.8362787365913391, 0.7212390899658203, -0.4715908467769623, 0.2466447949409485, 0.3288186490535736, -0.26020127534866333, 0.473976731300354, 0.6665758490562439, 0.13998277485370636, 0.3526599407196045, 0.22199392318725586, -0.8182864189147949, 0.47876229882240295, -0.8163090944290161, -0.053540218621492386, 0.6598272919654846, 0.49024131894111633, 0.9298063516616821, 0.23317617177963257, -0.9569640755653381, -0.5382670760154724, -0.1099809855222702, 0.39947086572647095, 1.1634749174118042, -0.07337658107280731, -0.3045421242713928, -0.5782719254493713, 0.18515115976333618, -0.3357224762439728, -0.17386265099048615, -0.2517455816268921, -0.2496090680360794, -0.5798271894454956, -1.3962445259094238, 0.6954477429389954, -0.09350141137838364, 1.0368343591690063, -0.14886008203029633, -0.14200492203235626, -0.5982797741889954, 0.2633529603481293, -0.9073588848114014, -1.0163029432296753, 0.5340323448181152, -0.40257325768470764, 0.15692903101444244, -0.23438909649848938, -0.13919155299663544, 0.16467146575450897, -0.6177060604095459, 1.0454472303390503, -0.019739923998713493, -0.3753821551799774, -0.2710091471672058, 0.6284788846969604, -0.3181135058403015, -0.7980989217758179, 0.17742809653282166, 0.19096729159355164, -0.3773516118526459, 0.49691635370254517, 0.4158433973789215, -0.006984435021877289, -0.01907060667872429, -0.24970658123493195, 0.4171678423881531, -0.2625880837440491, 0.1698831170797348, 0.5588675737380981, -0.018757585436105728, -0.39896145462989807, -0.8080918788909912, 0.5496535301208496, 0.2238045036792755, -0.7413637638092041, -0.05864371731877327, -1.0941113233566284, -0.35376739501953125, 0.38358017802238464, -0.38170361518859863, -0.1551407128572464, -1.2081819772720337, 0.04750969260931015, -0.24798545241355896, -0.05857602506875992, 0.013975733891129494, 0.21899712085723877, 0.30883151292800903, 0.13742929697036743, 0.1997656226158142, -0.07968265563249588, -0.3250328302383423, 0.7140554785728455, -0.9690292477607727, 0.5004173517227173, -0.11448624730110168, 0.06877609342336655, -0.1638292670249939, -0.21946246922016144, -0.6306242942810059, -0.538303017616272, -0.314980149269104, -0.13460558652877808, 0.04898667335510254, 0.6662797927856445, -0.9944314360618591, -0.6535660028457642, 0.29668933153152466, -0.8502563238143921, -0.2858723998069763, 0.41904154419898987, -0.1142192929983139, -0.1574595719575882, -0.9680847525596619, -1.5191822052001953, -0.2411710023880005, -0.906545877456665, -1.3312692642211914, 0.6391207575798035, 0.17749765515327454, -0.09624601155519485, -0.5986037850379944, -0.29645836353302, -0.5294652581214905, 1.0131560564041138, -0.7150765061378479, 0.7079530954360962, 0.13461683690547943, -0.260023295879364, -0.0013629836030304432, -0.1060308888554573, 0.8521234393119812, -0.616864800453186, 0.4026682376861572, -0.8043968081474304, -0.013043027371168137, -0.5353336930274963, -0.29496899247169495, 0.4099773168563843, 0.5039070844650269, 0.6680354475975037, 0.16480636596679688, -0.26886624097824097, 1.1055785417556763, 1.4178521633148193, -0.9028146862983704, -0.017079122364521027, -0.025086261332035065, 1.004005789756775, -0.49582770466804504, -0.47457876801490784, 0.49445658922195435, 0.22931848466396332, 0.3759734034538269, 0.028792796656489372, -0.02427094802260399, -0.058765336871147156, -0.5008280873298645, 0.46257126331329346, 1.6281472444534302, 0.5580713152885437, -0.10533783584833145, -1.131545066833496, 0.7086702585220337, -0.5307835340499878, -0.33578386902809143, 0.5645518898963928, 0.7070373892784119, 0.6020850539207458, -0.14638651907444, -0.4052186608314514, 0.1169406995177269, 0.44157588481903076, 0.4595223069190979, -0.09890496730804443, -0.9368278980255127, -0.3415733277797699, 0.5156707167625427, 0.5217949748039246, 0.7549174427986145, -0.3546595275402069, 0.9424423575401306, 14.954081535339355, 0.873347818851471, -0.12770289182662964, 0.8242968320846558, 1.1981102228164673, 0.3152157962322235, -0.429856538772583, 0.07233396917581558, -1.5861135721206665, 0.014756379649043083, 1.3808612823486328, 0.3370837867259979, 0.9188743829727173, 0.14693224430084229, -0.1067357137799263, 0.12923286855220795, -0.26076415181159973, 0.5238073468208313, 0.5205557346343994, -1.1624430418014526, 0.062468744814395905, 0.5144224762916565, 0.8812263011932373, 0.8066216707229614, 0.7911027073860168, 0.8026425242424011, 0.5321611762046814, -0.5739339590072632, 0.5315608382225037, 0.0714176818728447, 1.0073230266571045, -0.08136540651321411, 0.05285058170557022, 0.48228055238723755, -0.7224252820014954, -0.0844314768910408, -0.7594689130783081, -1.393892765045166, 0.37113887071609497, 0.7328552603721619, -0.30093029141426086, -0.3320697844028473, 0.15550418198108673, 0.9149959683418274, 0.17253902554512024, 0.12452062219381332, -0.08127116411924362, 0.7551027536392212, -0.5811589956283569, 0.09310050308704376, 0.3832249045372009, 0.025180090218782425, 0.17125801742076874, -0.0940220057964325, 0.364616334438324, -0.3178570866584778, 0.4738108813762665, 0.6340545415878296, -0.4148685336112976, -0.1444290727376938, 0.14393815398216248, -0.2137894183397293, -0.2744450867176056, 0.5728912949562073, 0.3971671760082245, 0.11181621998548508, -0.5439386963844299, 0.3680182993412018, 0.2689933776855469, 0.15668140351772308, -0.670113742351532, 0.564757764339447, 0.49737489223480225, -0.9800676703453064, -0.11531127244234085, 0.3077941834926605, -0.2728966474533081, -0.7448905110359192, -0.7248184680938721, -0.46143221855163574, 0.13152390718460083, -0.42585426568984985, -0.5113380551338196, 0.7899978160858154, -0.5067368745803833, -0.014899684116244316, 0.4634319841861725, -0.8471354842185974, -0.04667902737855911, 0.9255401492118835, -1.6672532558441162, -0.4197193384170532, 0.4453379511833191, -0.24388329684734344, -0.7194048762321472, 0.06837307661771774, 1.3582404851913452, 0.508048951625824, -0.40625709295272827, 0.05342178791761398, 0.07383974641561508, -0.04918742924928665, -0.37094438076019287, -0.782519519329071, 1.483619213104248, 0.5689301490783691, 0.08702234923839569, 0.17915558815002441, -0.2065812349319458, 0.334419846534729, -0.7433041930198669, -0.11166215687990189, 0.7604047060012817, -0.6898766160011292, -0.4848470389842987, -0.9417412877082825, -0.4071819484233856, 0.2077716886997223, 0.4916490912437439, 0.031071731820702553, 0.20166873931884766, 0.020011557266116142, -0.5446676015853882, -0.2024030238389969, -0.40168318152427673, -0.04812965914607048, 0.5392395257949829, -0.6955907940864563, 0.2389800250530243, 0.09665325284004211, 0.5094444155693054, -1.0621144771575928, -0.6303425431251526, -0.06370130926370621, -0.058870721608400345, -0.47844892740249634, 1.1724770069122314, -0.154541015625, 0.5477178692817688, 1.0545883178710938, -0.49614396691322327, -0.6563102602958679, 0.11736603081226349, -0.9852553606033325, 0.28823497891426086, 0.05862981453537941, 0.4793270230293274, -0.39967605471611023, 0.25763174891471863, 0.5605427622795105, 0.32213857769966125, -0.4368732273578644, -0.6113312244415283, -0.3180944621562958, 0.3465496003627777, -0.7644215226173401, 0.3673495650291443, -0.2878529131412506, -0.25257861614227295, -0.010539574548602104, -0.06198541820049286, 0.42811718583106995, -0.26096460223197937, -0.8111298680305481, 0.5870758295059204, 0.08402374386787415, -0.358561247587204, -0.7069875001907349, -0.6158626079559326, -1.8590787649154663, -0.03843757510185242, -1.2714885473251343, 0.0009526017820462584, -0.46842828392982483, -0.22520947456359863, -0.11101032048463821, -0.23728235065937042, -0.16356563568115234, 0.05250246450304985, 0.23871512711048126, -0.3651716113090515, -0.5069327354431152, -0.17271606624126434, 0.7156657576560974, 0.7832604050636292, -0.6843206286430359, 0.5352877378463745, -0.08763299882411957, -0.02531379461288452, 0.19891461730003357, 0.3739407956600189, -0.31751054525375366, -0.6984557509422302, -1.4715936183929443, 0.20945782959461212, -0.4560767412185669, 0.02611004374921322, -1.0588120222091675, 0.38839343190193176, 0.6401233673095703, -0.23305702209472656, -0.026420144364237785, 0.3344752788543701, -0.8141474723815918, -0.40518829226493835, 0.44907253980636597, -0.6429120898246765, 0.41597694158554077, 0.2044312059879303, -0.5631704926490784, -0.25206759572029114, 0.9906997084617615, 0.042947567999362946, -0.9150247573852539, -0.7108396887779236, 0.5858227610588074, -0.42077505588531494, 0.3170955181121826, -0.615130603313446, 0.09422587603330612, -0.8755775690078735, -0.36164411902427673, -0.09120495617389679, 0.2880989909172058, -0.16479790210723877, 0.950942873954773, 0.10066400468349457, -1.1546341180801392, -0.11119870841503143, 1.013623595237732, -0.28221532702445984, -0.0071051642298698425, 0.22733233869075775, 0.34769177436828613, -0.6781514286994934, 0.8161191940307617, 0.4952687621116638, -0.093839131295681, -0.36621662974357605, 0.20827262103557587, 0.6317670345306396, -0.6912376284599304, -0.5435356497764587, 1.2912020683288574, -0.360736608505249, -1.2898800373077393, 0.2876386046409607, -1.0666131973266602, -0.023691914975643158, -0.922274649143219, 0.5815392732620239, 0.07048393785953522, 0.053833167999982834, -0.2463221698999405, -0.4508199393749237, 0.32415953278541565, 0.36958909034729004, -0.6068880558013916, 0.22810755670070648, -0.36441221833229065, -0.5040140151977539, 0.36532509326934814, 1.3932114839553833, -0.849713921546936, -0.6266869306564331, -0.7425413131713867, -0.5230366587638855, -0.026101551949977875, 0.452457457780838, -0.3359319269657135, -0.9080274701118469, 1.0716556310653687, 0.5845513343811035, 0.0802946388721466, 0.4493118226528168, -0.6118756532669067, 0.337942898273468, 0.5044386982917786, 0.12447799742221832, -0.6423731446266174, -0.8600296378135681, 1.3102667331695557, 0.48623907566070557, -1.0631871223449707, 0.41822847723960876, -0.03245972841978073, -0.5726954936981201, 0.45179250836372375, 0.18742163479328156, -0.17423099279403687, 1.2837029695510864, -0.16519778966903687, 0.23444335162639618, 0.3019469082355499, -0.7693995237350464, -0.010125922039151192, 1.0395007133483887, 0.44307154417037964, 0.7224761843681335, 0.39438632130622864, -0.008933763951063156, 0.6872375011444092, -0.25037702918052673, -0.2592923939228058, 0.15351776778697968, 0.24388527870178223, 0.19366538524627686, 0.1528099924325943, -0.002002889756113291, 0.44606536626815796, -0.7194437384605408, -1.139363408088684, 0.48719972372055054, 0.4433770477771759, 0.18941041827201843, 0.7823832035064697, 0.9454872608184814, 0.2187812179327011, -0.10257476568222046, -0.07475296407938004, 0.3881215453147888, -0.2673337161540985, -0.11729235202074051, 0.3282070755958557, -0.8609002828598022, -0.03319479152560234, -0.2258048951625824, -0.3089147210121155, -0.4081920385360718, -0.5952157974243164, 0.24714021384716034, 0.16596949100494385, 0.5996955633163452, 1.0530332326889038, 0.297096848487854, 0.4763396382331848, -0.06926731020212173, -0.5489950180053711, -0.46618473529815674, -0.7138833999633789, -0.35317593812942505, -0.457168847322464, -0.2757868766784668, 0.22309385240077972, -0.009292772971093655, -0.41990578174591064]}, "authors": [{"authorId": "2051712753", "name": "Aohan Zeng"}, {"authorId": "2111312892", "name": "Xiao Liu"}, {"authorId": "66395694", "name": "Zhengxiao Du"}, {"authorId": null, "name": "Zihan Wang"}, {"authorId": "2051311700", "name": "Hanyu Lai"}, {"authorId": "145573466", "name": "Ming Ding"}, {"authorId": "2109506541", "name": "Zhuoyi Yang"}, {"authorId": "2125063007", "name": "Yifan Xu"}, {"authorId": "2163967642", "name": "Wendi Zheng"}, {"authorId": "2186982651", "name": "Xiao Xia"}, {"authorId": "1403621152", "name": "W. Tam"}, {"authorId": "2124489983", "name": "Zixuan Ma"}, {"authorId": "2114921664", "name": "Yufei Xue"}, {"authorId": "2467444", "name": "Jidong Zhai"}, {"authorId": "1712168", "name": "Wenguang Chen"}, {"authorId": "47243067", "name": "P. Zhang"}, {"authorId": "2047998", "name": "Yuxiao Dong"}, {"authorId": "2148911956", "name": "Jie Tang"}], "references": [{"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "b17cc18e4130505b939f7d527082eb6be2a7fd5b", "title": "Rationale-Augmented Ensembles in Language Models"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "1d650f1afd45c59ff907396fe8b678595dcb85ea", "title": "Memory-Based Model Editing at Scale"}, {"paperId": "bd1331b233e84bab7eba503abc60b31ac08e7881", "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"}, {"paperId": "707bd332d2c21dc5eb1f02a52d4a0506199aae76", "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers"}, {"paperId": "2f291b0b59483e9c3c4a3391f34e6b29aff848a1", "title": "DeepStruct: Pretraining of Language Models for Structure Prediction"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af", "title": "Compression of Generative Pre-trained Language Models via Quantization"}, {"paperId": "e0995bad59c8638ea8c319bb7220c0f0b1ed5dca", "title": "DeepNet: Scaling Transformers to 1, 000 Layers"}, {"paperId": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca", "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "39c77e29a232a9fb62b3a3c89c50f487d73e27ce", "title": "Counterfactual Memorization in Neural Language Models"}, {"paperId": "a3184d40d390793232c99c89b57b8f65c16320b2", "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"}, {"paperId": "fb01415a0decfa3f3d6339930e95028ae1ff4170", "title": "Efficient Large Scale Language Modeling with Mixtures of Experts"}, {"paperId": "fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf", "title": "Ethical and social risks of harm from Language Models"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "4a247cbfca9dcf91e2da24e6d2d84601a9041a8f", "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs"}, {"paperId": "cbf98ebe967e0f3f3236e7932f37013b98244e94", "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"}, {"paperId": "2582a04918f6fe62dc142f2fca9ca0bb0b1d7895", "title": "NormFormer: Improved Transformer Pretraining with Extra Normalization"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "dfd104dd0ff28b1bde2fbd4c4d6d3ccb4761f639", "title": "Learning Compact Metrics for MT"}, {"paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79", "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd", "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"}, {"paperId": "e6a237ab883e503b10b73b3a411c0078c47c9830", "title": "Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"}, {"paperId": "4237cbebe788a97174f48dc398082739bbffe95b", "title": "FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark"}, {"paperId": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a", "title": "Towards Understanding and Mitigating Social Biases in Language Models"}, {"paperId": "1197ae4a62f0e0e4e3f3fb70396b5ff06ef371aa", "title": "CogView: Mastering Text-to-Image Generation via Transformers"}, {"paperId": "76a786b1acd6d1aca56e12a8a1db34569fdf9f3a", "title": "Societal Biases in Language Generation: Progress and Challenges"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "240b0caabb415578bdea4da7d0a32bdff2e8163f", "title": "Editing Factual Knowledge in Language Models"}, {"paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae", "title": "An Empirical Study of Training Self-Supervised Vision Transformers"}, {"paperId": "098370508aaf56f718a472511987ac2072d0f917", "title": "Detecting Hate Speech with GPT-3"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "ca2f1088d3e581b2c6c75cf0ebc96506d620f64d", "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c"}, {"paperId": "ce9ca56036307217ea565644d3d3bd74b879e045", "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "824cd8db8a68732db04f4d8b7139eb4475e59ff2", "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics"}, {"paperId": "f96318f202540a554cc28a8db03808e972ddcb38", "title": "Exploring Text-transformers in AAAI 2021 Shared Task: COVID-19 Fake News Detection in English"}, {"paperId": "346081161bdc8f18e2a4c4af7f51d35452b5cb01", "title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "6914a7997ff4be207fa7b3472a9c5879abaec646", "title": "RealFormer: Transformer Likes Residual Attention"}, {"paperId": "5270b626feb66c8c363e93ba6608daae93c5003b", "title": "Modifying Memories in Transformer Models"}, {"paperId": "b360427d0991143013da6a208ccf28bcc8028fab", "title": "Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training"}, {"paperId": "645bd6eadc247989abc5e0b0aa0be79ec8b11ea6", "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models"}, {"paperId": "399e7d8129c60818ee208f236c8dda17e876d21f", "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "ac04ed0f3ae0f5b269c9b3e0d1232007d60dbf7e", "title": "Memory-Efficient Pipeline-Parallel DNN Training"}, {"paperId": "706f756b71f0bf51fc78d98f52c358b1a3aeef8e", "title": "Self-Supervised Learning: Generative or Contrastive"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "24e4d3370dc366d6b353d1d6818a0df266bb31b9", "title": "MLSUM: The Multilingual Summarization Corpus"}, {"paperId": "babeda48b10a4d638252118f2238d05a06f4ec55", "title": "StereoSet: Measuring stereotypical bias in pretrained language models"}, {"paperId": "18318b10e7c2dd4ad292208f4399eb1d4dca5768", "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "b45d656ac8cc2e940609580cf291ee76ffcac20a", "title": "On Layer Normalization in the Transformer Architecture"}, {"paperId": "80376bdec5f534be78ba82821f540590ebce5559", "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?"}, {"paperId": "681f4fbce872f138cbac9cdd92e8f6ed89ba6f8d", "title": "Measurement and Fairness"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "9e1241f017a627beca2542e378a88c642c32098b", "title": "Semantic Noise Matters for Neural Natural Language Generation"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "b3ea2d9c8e5ea3b87ace121f0bece71565abc187", "title": "Quantifying the Carbon Emissions of Machine Learning"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8", "title": "Entity, Relation, and Event Extraction with Contextualized Span Representations"}, {"paperId": "81b4920ad488affaee27389ff9540b7fea90a4ce", "title": "\u201cGoing on a vacation\u201d takes longer than \u201cGoing for a walk\u201d: A Study of Temporal Commonsense Understanding"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b", "title": "WinoGrande"}, {"paperId": "d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea", "title": "Energy and Policy Considerations for Deep Learning in NLP"}, {"paperId": "867db5097ad6aaef098c60b0845785b440eca49a", "title": "GLTR: Statistical Detection and Visualization of Generated Text"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "d9f6ada77448664b71128bb19df15765336974a6", "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "11eaa4f1cba9281ecbc1ac44a6b3ba5817bf1a25", "title": "T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples"}, {"paperId": "9967cb4fd949039c6f04dd9f2f4c3331dbebe6f7", "title": "Gender Bias in Coreference Resolution"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "400e746bc8027c4b5f915cae6123cd1775484b4d", "title": "Position-aware Attention and Supervised Data Improve Slot Filling"}, {"paperId": "ffe28f6bf0e9e6bbca6313319aa1a5409d283d9b", "title": "Zero-Shot Learning\u2014A Comprehensive Evaluation of the Good, the Bad and the Ugly"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs"}, {"paperId": "c92970286c535992a86539b761357761e97a37ee", "title": "Towards Robust Linguistic Analysis using OntoNotes"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "e7e7b9a731678bf0494fe29cbebb42a822224cc6", "title": "Modeling Relations and Their Mentions without Labeled Text"}, {"paperId": "25e7efa59a5cf68e0fc9401e4c6fa7b2bfe3f1ae", "title": "Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "5aa70188f70d349580aed96c10a68f57dace2d33", "title": "A Linear Programming Formulation for Global Inference in Natural Language Tasks"}, {"paperId": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition"}, {"paperId": "fd268d35d3e3e4d368e94050163fba9ede42ffe2", "title": "The GENIA corpus: an annotated research abstract corpus in molecular biology domain"}, {"paperId": "8665c9b459e4161825baf1f25b5141f41a5085ff", "title": "A bridging model for parallel computation"}, {"paperId": "ec936b808e0fab9281c050ad4010cddec92c8cbe", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"}, {"paperId": null, "title": "Accelerated inference for large transformer models using nvidia triton inference server"}, {"paperId": null, "title": "leading to in-depth studies of LLMs\u2019 theory, capacity, and flaws. Researchers can also modify the model architecture and weights, to validate the proposed algorithms to improve LLMs Zhu et al"}, {"paperId": null, "title": "Jurassic-1: Technical details and evaluation"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "b9478e237b58160c65acd2c41894493d27e2c277", "title": "WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models"}, {"paperId": "3e65f572322e192fe36ae52a8a7f025b0685dfc6", "title": "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"}, {"paperId": null, "title": "Also, it is known that LLMs can suffer from problems in fairness, bias, privacy, and truthful"}, {"paperId": null, "title": "2021) later observe that Pre-LN is still unable to handle the vulnerable training"}, {"paperId": null, "title": "GPT-3 was estimated to use 500 tons of carbon emissions footprint"}, {"paperId": null, "title": "Ethos: an online hate speech detection dataset"}, {"paperId": "310b8117ae5ce3df8aa6304ad382525b9b46937e", "title": "The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task: Overview and Evaluation Results (WebNLG+ 2020)"}, {"paperId": null, "title": "2020) is a comprehensive language modeling benchmark that initially includes 22 different text datasets from diverse domains. We report our results over a part of 18 datasets"}, {"paperId": null, "title": "2020), or namely Crowdsourced Stereotype Pairs benchmark, is widely used for measuring biases for masked language models. It collects 1508 examples with nine conventional biases and adopts"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "22d64e6d55c34f1a1a5884305c940526d826deab", "title": "MultiWOZ 2."}, {"paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47", "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"}, {"paperId": null, "title": "transparency to all the researchers and promote the research to reduce the potential harm of LLMs, like algorithms to identify the synthetic text"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Improving language understanding with unsupervised learning"}, {"paperId": "fc090a68e45e0e6337136777d21c87b76a90ae72", "title": "From TreeBank to PropBank"}, {"paperId": null, "title": "\u2022 Predicate Recognition: given a segment of a sentence and its corresponding semantic role, identify which verb it is related to"}, {"paperId": null, "title": "It would not be possible to reach its current status if without the collaboration of multiple teams-the Knowledge Engineering Group (KEG)"}, {"paperId": null, "title": "According to d'Holbach, people always act according to"}, {"paperId": null, "title": "Semantic Role Labeling: the traditional task form, where a verb (i.e., predicate) is annotated in text, and the model is asked to generate related semantic roles"}, {"paperId": null, "title": "Semantic Role Filling: given a verb and a potential semantic role, the model is asked to judge whether the role exists in the sentence and generate it"}, {"paperId": null, "title": "A) not suitable for the young. (B) not suitable for the old. (C) important, but unpleasant. (D) none of the above"}, {"paperId": null, "title": "\u2022 Optimize A100 kernel's computing efficiency => A100 kernels prefer square-shaped inputs, and seq_len=2,048 is optimal for our hidden-state dimension"}, {"paperId": null, "title": "The timeline of major issues that training GLM-130B encountered and addressed, as of"}, {"paperId": null, "title": "Below is a prompted example with 1-shot priming. We predict the probability on"}, {"paperId": null, "title": "Technical Report 2022-10-06 (v1"}]}