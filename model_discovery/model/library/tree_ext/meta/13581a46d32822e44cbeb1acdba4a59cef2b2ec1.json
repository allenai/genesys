{"paperId": "13581a46d32822e44cbeb1acdba4a59cef2b2ec1", "title": "On Efficient Training of Large-Scale Deep Learning Models: A Literature Review", "abstract": "The field of deep learning has witnessed significant progress, particularly in computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. With the increasing demands on computational capacity, though numerous studies have explored the efficient training, a comprehensive summarization on acceleration techniques of training deep learning models is still much anticipated. In this survey, we present a detailed review for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) data-centric: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) model-centric, including acceleration of basic modules, compression training, model initialization and model-centric curriculum learning techniques, which focus on accelerating the training via reducing the calculations on parameters; (3) optimization-centric, including the selection of learning rate, the employment of large batchsize, the designs of efficient objectives, and model average techniques, which pay attention to the training policy and improving the generality for the large-scale models; (4) budgeted training, including some distinctive acceleration methods on source-constrained situations; (5) system-centric, including some efficient open-source distributed libraries/systems which provide adequate hardware support for the implementation of acceleration algorithms. By presenting this comprehensive taxonomy, our survey presents a comprehensive review to understand the general mechanisms within each component and their joint interaction.", "venue": "arXiv.org", "year": 2023, "citationCount": 24, "influentialCitationCount": 0, "openAccessPdf": {"url": "http://arxiv.org/pdf/2304.03589", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This survey presents a comprehensive review to understand the general mechanisms within each component and their joint interaction of training deep learning models, and presents this comprehensive taxonomy."}, "embedding": {"model": "specter_v2", "vector": [-0.08934328705072403, 0.39895200729370117, -0.581608235836029, -0.16245390474796295, -0.6140296459197998, 0.38059160113334656, 0.33438840508461, -0.2171424925327301, -0.798180341720581, -0.2579803466796875, 0.42327386140823364, 0.11507434397935867, 0.41009604930877686, 0.15947185456752777, -0.4425022304058075, -0.17441101372241974, -1.0728533267974854, 0.32312849164009094, 0.050341587513685226, 0.0014734284486621618, -0.26424187421798706, -0.17763879895210266, -1.1698280572891235, 0.011721549555659294, 0.3590024709701538, 0.953041672706604, 0.23788297176361084, 1.2904874086380005, -0.32207435369491577, 0.11904909461736679, 0.39249923825263977, -0.186231330037117, 0.6866443157196045, 0.07098112255334854, -0.12911956012248993, 0.012134650722146034, 0.41146203875541687, -0.762381374835968, -0.7982966899871826, 0.8554855585098267, -0.07728895545005798, 0.5125955939292908, 0.17116327583789825, -0.6429769396781921, 0.08055972307920456, -0.2759474217891693, 0.16461999714374542, 0.9883404970169067, -0.8549653887748718, -0.5135529637336731, 0.9493449330329895, -1.38487708568573, 0.004847382195293903, 1.4889707565307617, 0.6606299877166748, 0.2846744954586029, -0.21295331418514252, -0.56185382604599, 0.463054895401001, -0.12698444724082947, -0.26129093766212463, -0.24800735712051392, 0.32440295815467834, -0.2400476485490799, 1.633164882659912, -0.5091522932052612, 0.06260073930025101, 0.7098783850669861, -0.08505933731794357, 1.3440905809402466, -0.1700362116098404, -1.014113187789917, 0.13611550629138947, -0.04166069254279137, 0.4908224940299988, 0.9056916832923889, -0.05084182322025299, 0.4872342050075531, -1.2628791332244873, -0.2215423882007599, 0.43885478377342224, 0.17281438410282135, 0.25880593061447144, -0.21763579547405243, 0.35564273595809937, 1.4036399126052856, 0.4793825149536133, 0.09703115373849869, -0.3487852215766907, 0.9771676063537598, 0.944338858127594, 0.3393752872943878, 0.44224196672439575, 0.11807449907064438, -0.43248653411865234, 0.13970912992954254, -0.8200731873512268, 0.13124792277812958, -0.09827791154384613, 0.9838359355926514, -0.21010589599609375, 0.3223160207271576, -0.280383437871933, 0.48051005601882935, 1.2080618143081665, 0.057358499616384506, 0.5559284090995789, -0.5339025259017944, 0.6077446341514587, -0.637420654296875, -0.08854886889457703, -0.30477505922317505, -0.2193458080291748, -0.6491954922676086, -1.4690220355987549, -0.8234701752662659, -0.518109142780304, 0.02910628169775009, -0.6363966464996338, 0.5871599912643433, -0.08990060538053513, 0.37260764837265015, 0.2938103675842285, 0.6299014687538147, 0.04434794560074806, 1.0480738878250122, 0.17429989576339722, -0.12895642220973969, 0.8764714002609253, -1.3573435544967651, -0.3937602639198303, -1.2763888835906982, 0.46144551038742065, 0.02018885686993599, 0.4305450916290283, -0.17505323886871338, -1.2900108098983765, -1.1928958892822266, -0.6627534031867981, 0.05431276559829712, -0.0886695459485054, 0.17333278059959412, 1.5312281847000122, 0.2067503035068512, -0.8567537665367126, 0.9042527079582214, -0.6383113265037537, -0.07084427028894424, 0.6264559030532837, 0.16813074052333832, 0.5699728727340698, -0.27742090821266174, -0.9425094127655029, 0.04326825216412544, 0.08593548089265823, -0.5973765254020691, -0.39955776929855347, -0.7053998708724976, -0.8374585509300232, 0.02588556334376335, -0.3005264699459076, -0.5539547801017761, 1.382737636566162, -0.4538370966911316, -1.252939224243164, 0.5668290257453918, -0.2119530886411667, -0.1332591027021408, 0.4936341643333435, -0.07720299810171127, -0.2735970914363861, -0.18611611425876617, -0.33875972032546997, 0.5325922966003418, 0.7108038067817688, 0.013369155116379261, -0.22236114740371704, 0.09758681803941727, -0.21366718411445618, -0.2710147202014923, -0.8572204113006592, 0.7811362743377686, -0.8501965999603271, -0.3012854754924774, 0.4318966567516327, 0.6885857582092285, -0.729128897190094, 0.19953756034374237, 0.4382493495941162, -0.9809086322784424, 0.9508413076400757, -0.04328453540802002, 0.8221842050552368, -1.0156384706497192, -0.9195044636726379, 0.026519611477851868, -0.30682507157325745, 0.07504452764987946, -0.8710951805114746, 0.37021565437316895, -0.019175423309206963, -0.10473992675542831, 0.12576846778392792, -1.2000880241394043, -0.269572377204895, 0.13590681552886963, -0.9020195007324219, 0.0878281220793724, 0.17924869060516357, 0.7476073503494263, -0.8042011260986328, 0.2519702911376953, -0.31302398443222046, 0.38479429483413696, -1.5176453590393066, 1.39548921585083, -0.4357394278049469, 0.0713893473148346, 0.03681666776537895, -0.252122700214386, 0.40385687351226807, -0.5465120673179626, 0.4514753818511963, -0.26585397124290466, 0.1593772917985916, 0.37238609790802, -0.727720320224762, 1.4574353694915771, -0.4887614846229553, 0.690303385257721, 0.25499287247657776, -0.6188769936561584, -0.023424189537763596, 0.4670076072216034, -0.288971871137619, -0.25486621260643005, 0.5281995534896851, 0.4280349612236023, -0.8634219169616699, 0.13305051624774933, 0.8699406385421753, 0.9336721301078796, -0.09827021509408951, 0.286688894033432, 0.7002789974212646, -0.15848024189472198, 0.7267148494720459, 0.48847848176956177, -0.1516486555337906, 0.0588398240506649, -0.1660025268793106, 0.002634364180266857, 0.09911124408245087, -1.138972282409668, -0.20569822192192078, 0.4537856876850128, 0.48292094469070435, 0.3881227970123291, 0.35717537999153137, -0.7629920840263367, -0.8852550387382507, 0.3837510347366333, 0.6164104342460632, 1.6309369802474976, -0.3546496629714966, 0.06915780901908875, -0.665617823600769, -0.2955078184604645, -0.14786410331726074, -0.6030282974243164, 0.0433034710586071, 0.04962751641869545, -0.5928903818130493, -1.4686890840530396, 0.6505241394042969, 0.5817489624023438, 1.0833795070648193, -0.3874903917312622, -0.3279513716697693, -0.42737165093421936, 0.5634725689888, -0.7990449666976929, -0.5086554884910583, 0.5657423734664917, -1.3225188255310059, -0.14415186643600464, 0.04408383369445801, -0.4183041751384735, 0.5930031538009644, -0.6200696229934692, 0.7711398005485535, -0.15613849461078644, -0.15125925838947296, -0.09101714938879013, 0.7201569676399231, -0.8613581657409668, -0.5638050436973572, 0.3677891194820404, 0.37150079011917114, -0.26030755043029785, 0.2724698483943939, 0.2902776300907135, 0.4008817672729492, -0.3845306932926178, -0.26243871450424194, -0.045499321073293686, 0.18498454988002777, 0.13985320925712585, 0.985201358795166, -0.4044147729873657, 0.30047935247421265, -0.9780936241149902, 1.1988372802734375, 0.272434264421463, -1.0324821472167969, -0.1392582356929779, -0.76014244556427, -0.006490128114819527, 0.7481946349143982, -0.7817803025245667, -0.1561889946460724, -0.6944213509559631, -0.17164428532123566, -0.8695494532585144, 0.020792117342352867, 0.04415009170770645, 0.7674295902252197, -0.3281698226928711, 0.45975765585899353, 0.25336432456970215, 0.7162284255027771, -0.16962549090385437, 0.33445513248443604, -0.7610656023025513, 0.42893972992897034, -0.15874449908733368, 0.10554918646812439, -0.16893845796585083, -0.20533427596092224, -0.6399769186973572, -0.7161681056022644, -0.6260985136032104, -0.366397887468338, -0.22408555448055267, -0.1758909821510315, -0.7875916361808777, -0.4821958541870117, 0.06753658503293991, -0.9796197414398193, -0.35939547419548035, 0.07768993824720383, 0.1433059573173523, -0.11632060259580612, -1.2557361125946045, -1.573516607284546, -0.6899210810661316, -1.0688157081604004, -1.028388261795044, 0.42428943514823914, 0.39469513297080994, 0.38516688346862793, -0.7496766448020935, -0.0482090562582016, -0.4900512397289276, 1.1126694679260254, -0.707838237285614, 0.7432247400283813, -0.045675717294216156, -0.24069809913635254, -0.34482985734939575, -0.019714388996362686, 0.6743761301040649, -0.7594636678695679, 0.200086772441864, -1.1730841398239136, -0.2415524125099182, -0.20220497250556946, -0.7357184886932373, 0.30717578530311584, 0.5407012701034546, 0.8160697221755981, 0.22668617963790894, -0.10232573747634888, 0.872410774230957, 1.5161203145980835, -0.7946451902389526, -0.13863681256771088, -0.11055784672498703, 0.6646232604980469, -0.28440284729003906, -0.49670377373695374, 0.7598404884338379, -0.3419753313064575, 0.0125660989433527, 0.21653151512145996, -0.4781889021396637, -0.3219373822212219, -0.31346505880355835, 0.16821251809597015, 2.0344622135162354, 0.38433384895324707, 0.20033665001392365, -0.4788716733455658, 0.4510628283023834, -1.0699306726455688, -0.5039525032043457, 0.6766753792762756, 0.5138800144195557, 0.3735784590244293, -0.04581502825021744, -0.214260071516037, 0.017122289165854454, 0.44129425287246704, 0.6578295826911926, -0.5971199870109558, -0.624092161655426, 0.1908043920993805, 0.5484564304351807, 0.1821126490831375, 0.6631991863250732, -0.23040702939033508, 0.24999433755874634, 14.635076522827148, 0.9532422423362732, -0.09351152181625366, 0.9628207683563232, 0.9052140712738037, 0.12853550910949707, -0.028361305594444275, -0.597750723361969, -1.272888422012329, -0.16389943659305573, 1.3535962104797363, 0.2842296063899994, 0.6363896131515503, 0.9163747429847717, 0.09500418603420258, 0.25148048996925354, -0.3703867793083191, 1.0478472709655762, 0.26831215620040894, -1.6941800117492676, 0.3566689193248749, 0.09539827704429626, 0.7967684864997864, 0.9048869609832764, 0.6573992371559143, 1.1021559238433838, 0.3085439205169678, 0.0762450322508812, 0.44828712940216064, -0.1250779777765274, 0.9867827892303467, -0.1388927549123764, 0.7200223207473755, 0.8094044923782349, -0.9351334571838379, -0.23027707636356354, -0.9185194373130798, -1.2049355506896973, 0.0826127901673317, 0.2935383915901184, -0.18511620163917542, -0.34760281443595886, -0.3199024796485901, 0.6585848331451416, 0.02658657170832157, 0.45929068326950073, -0.36865025758743286, 0.6789280772209167, -0.3709934949874878, 0.045936595648527145, 0.4693381190299988, 0.019076477736234665, -0.22413654625415802, -0.018984494730830193, -0.16187192499637604, -0.1146254763007164, 0.46033456921577454, 0.14191080629825592, -0.8377035856246948, -0.1443033069372177, 0.09836427867412567, -0.09496939182281494, -0.16923043131828308, 0.6890996098518372, 0.7169759273529053, 0.06327977031469345, -0.4703972339630127, 0.21810781955718994, 0.9302541613578796, 0.1517201066017151, -0.4268135726451874, -0.11805874109268188, 0.2575930655002594, -0.7823758721351624, -0.09379212558269501, 0.42345038056373596, -0.41780900955200195, -0.7275913953781128, -0.515148937702179, -0.509920597076416, 0.28842926025390625, -0.647446870803833, -0.8097062706947327, 0.9071910381317139, -0.28513678908348083, 0.031086811795830727, 0.49666064977645874, -0.9514458179473877, -0.336062490940094, 0.4597093164920807, -1.3911432027816772, -0.31110525131225586, 0.1539939045906067, -0.06875378638505936, -0.0408322773873806, -0.22492234408855438, 1.1291401386260986, 0.6818894743919373, -0.9253486394882202, 0.1898728907108307, 0.22218601405620575, -0.13235917687416077, -0.6184960007667542, -0.16883735358715057, 0.8855721950531006, 0.8141165375709534, -0.2909509837627411, -0.1293697953224182, -0.20462659001350403, 0.49127089977264404, -0.8078725934028625, -0.4792042672634125, 0.49113819003105164, -0.24655181169509888, -0.12487182021141052, -0.8275373578071594, -0.7495498061180115, 0.3060266673564911, 0.04255002364516258, -0.017691047862172127, 0.047971710562705994, 0.27464058995246887, -0.477912575006485, -0.4997643530368805, -0.7954855561256409, -0.26192450523376465, 0.3468136787414551, -0.8941445350646973, 0.05302322283387184, 0.512743353843689, 0.44659459590911865, -1.1947741508483887, -0.33598843216896057, -0.44731777906417847, -0.4176545739173889, 0.13372300565242767, 1.0399001836776733, -0.46769610047340393, 0.45389866828918457, 1.2047797441482544, -0.2603493928909302, -0.415323942899704, 0.3101963400840759, -0.8092389702796936, -0.3443671464920044, -0.06123017147183418, 0.30835458636283875, -0.4904457926750183, 0.8230788707733154, 0.743557870388031, 0.061700690537691116, -0.7015822529792786, -0.8461745977401733, -0.2574509084224701, -0.20062723755836487, -0.6776090264320374, 0.18602915108203888, -0.24805904924869537, -0.40363815426826477, -0.1565627008676529, 0.4267578721046448, 0.764525830745697, -0.03396432846784592, -0.5009340047836304, 0.5409961938858032, -0.016150541603565216, -0.24345028400421143, -0.37835174798965454, -0.35529714822769165, -1.9043846130371094, -0.0044268351048231125, -1.3874304294586182, -0.2769971489906311, -0.6421270966529846, -0.401629239320755, -0.22101882100105286, -0.02950829453766346, -0.22925962507724762, 0.4131895899772644, -0.3745790421962738, -0.42751216888427734, -0.23889802396297455, -0.33934056758880615, 1.0011852979660034, 0.8095816373825073, -0.4010700583457947, -0.3089289963245392, -0.07363433390855789, 0.5942419767379761, 0.6120514273643494, 0.4743588864803314, -0.6785027384757996, -0.9938665628433228, -1.4314543008804321, 0.007076628506183624, -0.32637670636177063, -0.1888633519411087, -0.8569144010543823, 0.7857506275177002, 0.2247549593448639, -0.20238283276557922, 0.07421059906482697, 0.3347325921058655, -0.9822983741760254, -0.5858497023582458, 0.8354275226593018, -0.573027491569519, 0.17620691657066345, 0.4058995842933655, -0.23899301886558533, -0.43368566036224365, 0.3059656620025635, 0.38036617636680603, -0.5332831144332886, -0.7402278184890747, 0.8397083282470703, -0.31592121720314026, 0.30077359080314636, -0.3049599230289459, 0.2709171175956726, -0.8444271683692932, -0.0043543861247599125, 0.09177808463573456, 0.08944433182477951, -0.5810121893882751, 0.4142177104949951, 0.31118500232696533, -1.3597888946533203, 0.36839765310287476, 1.029801607131958, -0.5659082531929016, 0.4737358093261719, 0.38456135988235474, 0.5434750318527222, -0.7625895142555237, 0.4620644748210907, 0.32043471932411194, 0.09949440509080887, -0.6478995680809021, 0.04987078905105591, 0.8424718976020813, -0.8560956716537476, -0.21561577916145325, 1.450273871421814, -0.44573938846588135, -1.020440697669983, 0.7094018459320068, -1.2779654264450073, -0.14705242216587067, -0.34929370880126953, 0.6800896525382996, 0.2310021072626114, 0.01581898331642151, 0.47416016459465027, -0.2947230935096741, 0.040383387356996536, -0.12973186373710632, -0.21514245867729187, 0.828595757484436, 0.09015465527772903, -0.0663483589887619, 0.43513262271881104, 1.0277893543243408, -0.8614152073860168, -1.3718401193618774, -0.7203230857849121, -0.44015833735466003, -0.5378641486167908, 0.2602505683898926, 0.08049540221691132, -1.2309584617614746, 0.8316988945007324, 0.6754713654518127, 0.010276082903146744, 0.33913132548332214, -0.29262304306030273, 0.41679880023002625, 0.6558085680007935, 0.2685844302177429, -0.7411336898803711, -0.15108343958854675, 1.21748685836792, 1.239248514175415, -0.8650833964347839, 0.4949514865875244, -0.2469758838415146, -0.6304230690002441, 0.9490359425544739, 0.301816463470459, -0.25926145911216736, 1.1129140853881836, -0.4202055335044861, -0.07094307988882065, 0.2397785484790802, -0.9930731058120728, 0.22826290130615234, 0.6884883046150208, 0.7544781565666199, 0.3126504123210907, -0.12984436750411987, 0.3009184002876282, 1.1297422647476196, 0.16137531399726868, 0.023747391998767853, 0.3427731692790985, 0.37592291831970215, -0.2545793056488037, 0.11749283969402313, -0.06170801818370819, 0.6472975611686707, -0.7563508749008179, -0.40711209177970886, 0.9117622971534729, 0.7486385703086853, 0.08182507008314133, 0.6131990551948547, 0.9554513692855835, 0.3856770396232605, 0.5294408798217773, -0.08637771755456924, 0.446325421333313, -0.3516361713409424, -0.35321900248527527, 0.07682915031909943, -0.6877727508544922, -0.25450336933135986, -0.32876524329185486, -0.24099703133106232, -0.23201864957809448, -0.4904482364654541, 0.26238083839416504, 0.024143533781170845, 0.7002055644989014, 0.5827686190605164, 0.5332258939743042, 0.660414457321167, -0.22110483050346375, -0.9099457859992981, -0.7657452821731567, -0.8959298133850098, -0.11939581483602524, -0.29950568079948425, -0.06210778281092644, 0.10970872640609741, -0.48740625381469727, -0.006535152439028025]}, "authors": [{"authorId": "2144035454", "name": "Li Shen"}, {"authorId": "2204964085", "name": "Yan Sun"}, {"authorId": "2216987575", "name": "Zhiyuan Yu"}, {"authorId": "46573238", "name": "Liang Ding"}, {"authorId": "40434674", "name": "Xinmei Tian"}, {"authorId": "2135519749", "name": "Dacheng Tao"}], "references": [{"paperId": "ff18421b1904dbad56b0d0abcfc2d600f7695433", "title": "FFCV: Accelerating Training by Removing Data Bottlenecks"}, {"paperId": "114cc72d93c73b97ba03ed8c4e4a8d937b344607", "title": "An Efficient 2D Method for Training Super-Large Deep Learning Models"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "3599a236f285af48782fc30b1341d13ec7320735", "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"}, {"paperId": "a7f59b2162ae0ea2520753b1b9b17277490a9458", "title": "Symbolic Discovery of Optimization Algorithms"}, {"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "74013b7cfa0fc524803350fca51341004565eb22", "title": "Data Selection for Language Models via Importance Resampling"}, {"paperId": "643d53ad30e074eed974fb1cd9c1a85fea037fd0", "title": "Avalanche: A PyTorch Library for Deep Continual Learning"}, {"paperId": "fe5a72e0a4aeb5ea5058d9e4531858be5548dfe0", "title": "A Survey on Efficient Training of Transformers"}, {"paperId": "8d670bf64b21280ba58b1c41d11825cce46e1ce3", "title": "SpeeChain: A Speech Toolkit for Large-Scale Machine Speech Chain"}, {"paperId": "4b308ba40e67b0b4b25c6fde17195d5a456a2f41", "title": "Cramming: Training a Language Model on a Single GPU in One Day"}, {"paperId": "78db9c110ce3d082b4025ff6585419aa07144fb7", "title": "Accelerating Self-Supervised Learning via Efficient Training Strategies"}, {"paperId": "7928253927fc11ba7da64b428b6b97dce096672c", "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing"}, {"paperId": "9eb5a263870f9df8d066add5864685416b3dc8d9", "title": "ExtremeBERT: A Toolkit for Accelerating Pretraining of Customized BERT"}, {"paperId": "a92d2c468835571f3302ce9299fdf37b36c2e367", "title": "Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision Transformer Training"}, {"paperId": "c480a4735b0e3511192333e3a4da6b233bfe1790", "title": "EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones"}, {"paperId": "776e38d8d1bbfe4a5b7b80b803eb3c5df0e1a1b5", "title": "Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers"}, {"paperId": "649fa88a7cb51657a4c0f2e599da25b9cf0bf425", "title": "Profiling and Improving the PyTorch Dataloader for high-latency Storage: A Technical Report"}, {"paperId": "7c41075414dc6d2d16f237b90d63b2f5675139c2", "title": "Accelerating Parallel Stochastic Gradient Descent via Non-blocking Mini-batches"}, {"paperId": "2de76f208b90f97d74374d7dcd3c4180b3fc4b23", "title": "Hardware-aware Quantization/Mapping Strategies for Compute-in-Memory Accelerators"}, {"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "f8680579ede62cf7523c7ccc2d30df03fab3260a", "title": "K-SAM: Sharpness-Aware Minimization at the Speed of SGD"}, {"paperId": "ed2ae38081e7d893a1972268db84e3e628d70fb8", "title": "Large-batch Optimization for Dense Visual Predictions"}, {"paperId": "8b7da746b72fbe3bb5fec37e48265c6c8b89ba8d", "title": "RSC: Accelerate Graph Neural Networks Training via Randomized Sparse Computations"}, {"paperId": "e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd", "title": "The Devil in Linear Transformer"}, {"paperId": "1dff6b1b35e2d45d4db57c8b4e4395486c3e365f", "title": "Token Merging: Your ViT But Faster"}, {"paperId": "09312f70402847d4c2b5b5348d902ad0b2d4a0d5", "title": "Turbo Training with Token Dropout"}, {"paperId": "34e1c62586f0c86af60a6ff2c3e1121c1ebd779a", "title": "Stop Wasting My Time! Saving Days of ImageNet and BERT Training with Latest Weight Averaging"}, {"paperId": "f5241abdad7e43606f1c79fc71273d43de3f31c5", "title": "ButterflyFlow: Building Invertible Layers with Butterfly Matrices"}, {"paperId": "13270b9759cf0296b5a346fbb58b706e8ad0a982", "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design"}, {"paperId": "2475b38a76a9c2dc67f74446e2e686815764b0f2", "title": "EcoFormer: Energy-Saving Attention with Linear Complexity"}, {"paperId": "99934ef57a75f49afe68736cbc7dbf480687b552", "title": "Efficient Quantized Sparse Matrix Operations on Tensor Cores"}, {"paperId": "fd7e88a2313e176315d99fc299277e752d7703b7", "title": "Efficient Methods for Natural Language Processing: A Survey"}, {"paperId": "eb92cedb7e9182b8887a36d25f319196e11d9ca4", "title": "Prioritizing Samples in Reinforcement Learning with Reducible Loss"}, {"paperId": "02251886950770e82b3d68564d60cdfe15e73199", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"}, {"paperId": "81521a80f7aa939281863c15b42e446ff5a0e65a", "title": "Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models"}, {"paperId": "d97becb3abec3c6646f0524827bbde3b6b8cf598", "title": "TokenMix: Rethinking Image Mixing for Data Augmentation in Vision Transformers"}, {"paperId": "c5eafe6e4926eba1ff42340dddc112a23abfa7f1", "title": "Sequential Normalization: Embracing Smaller Sample Sizes for Normalization"}, {"paperId": "4f1e61051e6b0568fe36ec3dcf8a42cd0447ff1c", "title": "Adversarial Attack and Defense Strategies of Speaker Recognition Systems: A Survey"}, {"paperId": "0b20229534bb59c13df81bb40a2abc76c12f3024", "title": "A Data-Loader Tunable Knob to Shorten GPU Idleness for Distributed Deep Learning"}, {"paperId": "6aa6f15b6bbd0dd3433a88a35fd7b918e6b8431a", "title": "Cross-Domain Collaborative Normalization via Structural Knowledge"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "6a8db14262ca2017cb253e12b8daeb57989a38df", "title": "Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt"}, {"paperId": "b698dbfaf9b961502062cbfcbe05d319047d8495", "title": "Towards Understanding Sharpness-Aware Minimization"}, {"paperId": "6b117a8dcaa161562b0a69afbb9811e11afb5b3e", "title": "Decentralized Training of Foundation Models in Heterogeneous Environments"}, {"paperId": "f0c97c0cf5b8f9e16c0f9a878580b82fe537fe00", "title": "BitBlade: Energy-Efficient Variable Bit-Precision Hardware Accelerator for Quantized Neural Networks"}, {"paperId": "b65de6b99535e9c3b07fd672b363d4496306eafb", "title": "Re-parameterizing Your Optimizers rather than Architectures"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bf6ce546c589fa8054b3972b266532664914bd21", "title": "Fast Vision Transformers with HiLo Attention"}, {"paperId": "6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0", "title": "Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning"}, {"paperId": "403dcbddeb9c5ef234b51b422d1c9018b8171a89", "title": "A Scalable Pipeline for Gigapixel Whole Slide Imaging Analysis on Leadership Class HPC Systems"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "bbd5eb0924ec07a83dbc99151a09f463300c0001", "title": "Accelerating attention through gradient-based learned runtime pruning"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "159be298e25b7210ae577d7962cceb5e73aee687", "title": "Automated Progressive Learning for Efficient Training of Vision Transformers"}, {"paperId": "9365d464f67ba829ae18db0d260755720a0920b1", "title": "A Survey on Efficient Convolutional Neural Networks and Hardware Acceleration"}, {"paperId": "833fed7f1f9dfd0032feca11909d825188a7fead", "title": "DNN Training Acceleration via Exploring GPGPU Friendly Sparsity"}, {"paperId": "dc0102a51a9d33e104a4a3808a18cf17f057228c", "title": "Transformer Quality in Linear Time"}, {"paperId": "98850975e574e08695a9f32b4c8747dc7f8bcc17", "title": "Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam"}, {"paperId": "0c973f18d67a11a8d9c40a7c9e236faaeef1b05d", "title": "Feature wise normalization: An effective way of normalizing data"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "a90c3dbc091bb1f5fe68fa3730f073210cd2d57f", "title": "Mixhead: Breaking the low-rank bottleneck in multi-head attention language models"}, {"paperId": "53c3940f35b8b45d55ed49056282e1961954513d", "title": "Self-attention Does Not Need $O(n^2)$ Memory"}, {"paperId": "57ccd37bf5d8c0adc8bf9992490d80408354b116", "title": "One-Dimensional Deep Low-Rank and Sparse Network for Accelerated MRI"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "658a017302d29e4acf4ca789cb5d9f27983717ff", "title": "Masked-attention Mask Transformer for Universal Image Segmentation"}, {"paperId": "ba2869b0eea0d87e05dc7a9b48fcb7f9803dc406", "title": "A survey on textual entailment based question answering"}, {"paperId": "c5a7ad1ac5c462113f5c12a19d46596944f9b418", "title": "MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation"}, {"paperId": "da0d38cf2ac7e2a6908e0d9e1fff07058daab2ed", "title": "Sparse is Enough in Scaling Transformers"}, {"paperId": "b47d3f018d91a45dc7d5297c382990abf8ff7ec8", "title": "SimpleTrack: Understanding and Rethinking 3D Multi-object Tracking"}, {"paperId": "b39495876b494412e0918898db8f988e9f5fd69d", "title": "TransMix: Attend to Mix for Vision Transformers"}, {"paperId": "6351ebb4a3287f5f3e1273464b3b91e5df5a16d7", "title": "Masked Autoencoders Are Scalable Vision Learners"}, {"paperId": "f12975df157fe4462054ba1484dd4b4e5aaf5a17", "title": "Large-Scale Deep Learning Optimizations: A Comprehensive Survey"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "ae8210e2443572ad9a05e7e66058ea0919e6db9e", "title": "Eigencurve: Optimal Learning Rate Schedule for SGD on Quadratic Objectives with Skewed Hessian Spectrums"}, {"paperId": "e6a9488666a287000488772e94355a5732229c1e", "title": "How Important is Importance Sampling for Deep Budgeted Training?"}, {"paperId": "ad1aec71ff9e65bab89f3c7ad168b3c91ffceb4c", "title": "ZerO Initialization: Initializing Neural Networks with only Zeros and Ones"}, {"paperId": "b8cb9c0b02da96a9908665ae67692a6da4dd25a4", "title": "SCENIC: A JAX Library for Computer Vision Research and Beyond"}, {"paperId": "48af9b314181b04edcc0b7224ffe4689036b755f", "title": "Improving Transformers with Probabilistic Attention Keys"}, {"paperId": "952305d9bbdaeaf7adb7ef12b94f221570c5d52d", "title": "LightSeq2: Accelerated Training for Transformer-Based Models on GPUs"}, {"paperId": "11fe37ab6faf6bf85ad2f5746c154dec5412bd04", "title": "8-bit Optimizers via Block-wise Quantization"}, {"paperId": "b588110ee9e30a1b1ee130c3323f8eef308a59dc", "title": "Efficient Data Loader for Fast Sampling-Based GNN Training on Large Graphs"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "6d178f85c8ca9698b02d0f4f1f4a8f2fc4895411", "title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models"}, {"paperId": "0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4", "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"}, {"paperId": "4566c0d22ebf3c31180066ab23b6c445aeec78d5", "title": "Deduplicating Training Data Makes Language Models Better"}, {"paperId": "5d032bd2632b6f5847767f39ce247098c6bbc563", "title": "Combiner: Full Attention Transformer with Sparse Computation Cost"}, {"paperId": "ca6097aa4fc0bb5bffdb44d8429149cec2009210", "title": "REX: Revisiting Budgeted Training with an Improved Schedule"}, {"paperId": "39fce1f53887e22071e147192c7e2bb217ced18b", "title": "M-FAC: Efficient Matrix-Free Approximations of Second-Order Information"}, {"paperId": "71ca4c16fe6ba98bfa6a6a2b0b94151f35b809b4", "title": "KAISA: An Adaptive Second-Order Optimizer Framework for Deep Neural Networks"}, {"paperId": "bf38bdd65710ed4e3831c750275f5843058250bc", "title": "BAGUA: Scaling up Distributed Learning with System Relaxations"}, {"paperId": "e7b0b36ffbc583cd169c690c8d9a4da32bcd50cf", "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training"}, {"paperId": "e2f2662f0734e2edc2b4b36a734de111c7f8d54d", "title": "IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers"}, {"paperId": "cf5e6e3c50a798d87033e0e108e88b3647738bbe", "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers"}, {"paperId": "722ad6ac92286507437b31486f47987d6ece05c9", "title": "BEiT: BERT Pre-Training of Image Transformers"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "9bc1b2f8ab82921e8f226c732f51d0ce1bd48822", "title": "Progressive Multi-Granularity Training for Non-Autoregressive Translation"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530", "title": "A Survey of Transformers"}, {"paperId": "576c462dbc1f3d732b919ef1daac37a817123e52", "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias"}, {"paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de", "title": "Luna: Linear Unified Nested Attention"}, {"paperId": "cb9ea69a4a6c86c3ac2be7243cf170bf31441bda", "title": "StyleMix: Separating Content and Style for Enhanced Data Augmentation"}, {"paperId": "d8df456f790381f4ddb388be24a546625bd75ee2", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks"}, {"paperId": "63d8426ba1f51a8525dd19fd8ec92934ec71aea5", "title": "A Survey of Data Augmentation Approaches for NLP"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "72dd63d67588a42fc817bbb8d655b397f67425df", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep learning"}, {"paperId": "90d5e6f8d3b9f2617b3a3cf00fb02e730eb011cb", "title": "Accelerating Sparse Deep Neural Networks"}, {"paperId": "7694aae9766d5f1fe74d900cd82aee898cb6e8e9", "title": "How to Train BERT with an Academic Budget"}, {"paperId": "14c52ffa7ea9c1971d5d82ea369c946c98d056a9", "title": "LocalViT: Bringing Locality to Vision Transformers"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "8f8f73f0f208302546c825ed474432389ed63be4", "title": "EfficientNetV2: Smaller Models and Faster Training"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "0f8aa47ff8c6c49a347e192debe20ce4e5a4caea", "title": "Self-supervised Pretraining of Visual Features in the Wild"}, {"paperId": "6e466c1bca96e2847be32f60677517aae628a664", "title": "Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "2cd605106b88c85d7d8b865b1ef0f8c8293debf1", "title": "Zero-Shot Text-to-Image Generation"}, {"paperId": "51f46cb42668cfe3745ecf029d032bf30253574f", "title": "GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "39caa9091318480f3241117ecefba897548cc42a", "title": "Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity"}, {"paperId": "4066d78b637c2b8e57de5ffd53950134a551de85", "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40", "title": "Shortformer: Better Language Modeling using Shorter Inputs"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "9d2c96574019305a8c86cc5b84cb9f616ccf0eb3", "title": "When Do Curricula Work?"}, {"paperId": "106fb432d2b62f3824a9d6f4a1b30e1f8b6ea9d7", "title": "Sequence-level Mixed Sample Data Augmentation"}, {"paperId": "2310d893abf4ec900cb9e0c5da58284a37329780", "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping"}, {"paperId": "a5d6b9ed787b558e20d61bd8f5816317ef1b9a39", "title": "On the Transformer Growth for Progressive BERT Training"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "53af14897ccca4f8d6c465ee133e859f99ddd7e7", "title": "Shallow-to-Deep Training for Neural Machine Translation"}, {"paperId": "a2cd073b57be744533152202989228cb4122270a", "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "e00484961fb2f30d2d48a5f9853fa3ebab140cac", "title": "Improving Transformer Optimization Through Better Initialization"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "670f9d0d8cafaeaeea564c88645b9816b1146cef", "title": "Differentiable Augmentation for Data-Efficient GAN Training"}, {"paperId": "528dd0da358b4939d99eeb92548deccfeac48bd6", "title": "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "29e34b721a72357ab83717643c1566f977e6a761", "title": "SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization"}, {"paperId": "45a08126c8a599ff223243156fae614761b47e43", "title": "ReSprop: Reuse Sparsified Backpropagation"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "38643c2926b10f6f74f122a7037e2cd20d77c0f1", "title": "Supervised Contrastive Learning"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "64499484b88e4766cf078aecbadea20f171c0921", "title": "SuperMix: Supervising the Mixing Data Augmentation"}, {"paperId": "8771679aac0e90371340bd8c657317f5be113e81", "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "91e611c3e8705002438fb4439733e47ddec85b5d", "title": "fastai: A Layered API for Deep Learning"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "3d4015053dd365b328ce9ed2a946600115be57c3", "title": "Fast 2D Convolution Algorithms for Convolutional Neural Networks"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "7d668ed2d5b383c7ff9641974b2bc0c84f43e251", "title": "Adversarial AutoAugment"}, {"paperId": "02b1607af35b48f0bd716367caf6a7428b969369", "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty"}, {"paperId": "3fd7c9ba742dd2b435afa75217847e5087e2f2a8", "title": "PipeDream: generalized pipeline parallelism for DNN training"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "10eda4521c032adabaa8e70d6569e17370b29dcd", "title": "Root Mean Square Layer Normalization"}, {"paperId": "0accb5cb9d06b4cdc4ceda316bc51c8ba95e6838", "title": "PaddlePaddle: An Open-Source Deep Learning Platform from Industrial Practice"}, {"paperId": "70fe1f854bc59092ded4bf2939a6624a80e5e4c3", "title": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models"}, {"paperId": "9f73c3f86026c21d0e5e55c70462952c6ada1175", "title": "Accelerating Deep Learning by Focusing on the Biggest Losers"}, {"paperId": "87f6a7c014ce206ac5b57299c07e10667d194b39", "title": "Randaugment: Practical automated data augmentation with a reduced search space"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "20bec3a03d47c8ad32d696779e62d5ad87c7c0fa", "title": "Ordered SGD: A New Stochastic Optimization Framework for Empirical Risk Minimization"}, {"paperId": "8dc5f55d3ad0c1711081c78f6b97ee23b0971e13", "title": "Fast Deep Learning Training through Intelligently Freezing Layers"}, {"paperId": "c0aaee2337e5af680e5dca1bfc349a737dfec573", "title": "Fixing the train-test resolution discrepancy"}, {"paperId": "f8de25118af2abc4c48afb947d6ec298e05ef1e5", "title": "When Does Label Smoothing Help?"}, {"paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88", "title": "Efficient Training of BERT by Progressively Stacking"}, {"paperId": "60f7f9161eb88c71c893969fb0d6586f70be99fc", "title": "Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules"}, {"paperId": "ed17929e66da7f8fbc3666bf5eb613d302ddde0c", "title": "CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features"}, {"paperId": "6885dc17f1607f1947721ef4c430f1eda22f1228", "title": "Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints"}, {"paperId": "79586c6544b32f4d7ac32beae10a2a6c794f0915", "title": "AutoAssist: A Framework to Accelerate Training of Deep Neural Networks"}, {"paperId": "e808cac4b64a8c73ada719f76ad885454c71a74c", "title": "Fast AutoAugment"}, {"paperId": "031e4e43aaffd7a479738dcea69a2d5be7957aa3", "title": "ERNIE: Enhanced Representation through Knowledge Integration"}, {"paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280", "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"}, {"paperId": "4d62811fe8be061183170e255b702e31fb5d4200", "title": "Efficient Winograd or Cook-Toom Convolution Kernel Implementation on Widely Used Mobile CPUs"}, {"paperId": "162cad5df347bdac469331df540440b320b5aa21", "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks"}, {"paperId": "a1b1297c1057653e09f7662553ef984afc4a2c20", "title": "Quasi-Newton methods for machine learning: forget the past, just sample"}, {"paperId": "aa5741c74b7fac10680c1cfbdd49d9ffb5751a68", "title": "Using Pre-Training Can Improve Model Robustness and Uncertainty"}, {"paperId": "8b2c6ea7bbc5b616548dd07b46c492a32b6472d1", "title": "PruneTrain: fast neural network training by dynamic sparse model reconfiguration"}, {"paperId": "29309743870c825f9645a4803af727402462e513", "title": "Bag of Tricks for Image Classification with Convolutional Neural Networks"}, {"paperId": "54c4642d017830e1faddbb49f0377228d2b01493", "title": "HAQ: Hardware-Aware Automated Quantization With Mixed Precision"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "0df175bb6292b72feee5e8d27f7c5a76130d7eab", "title": "A Faster Algorithm for Reducing the Computational Complexity of Convolutional Neural Networks"}, {"paperId": "d170bd486e4c0fe82601e322b0e9e0dde63ab299", "title": "Adaptive Input Representations for Neural Language Modeling"}, {"paperId": "3fd9e04da135496bcd75e884b5cd2ecee656a67d", "title": "On the Computational Inefficiency of Large Batch Sizes for Stochastic Gradient Descent"}, {"paperId": "a82fc0115c1802d48d352b35595204738fad84f0", "title": "Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes"}, {"paperId": "1b59eea8ec4684381a885b59acd09c9151a49487", "title": "Manifold Mixup: Better Representations by Interpolating Hidden States"}, {"paperId": "f723eb3e7159f07b97464c8d947d15e78612abe4", "title": "AutoAugment: Learning Augmentation Policies from Data"}, {"paperId": "7f2406aba47ac90dcc92f890dca3d9b647d11894", "title": "Decoupled Parallel Backpropagation with Convergence Guarantee"}, {"paperId": "b8989afff14fb630ca58b6afa917fb42574228ee", "title": "Averaging Weights Leads to Wider Optima and Better Generalization"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "453f7610c6e66bfafcb989d7a9a08e889559f041", "title": "Not All Samples Are Created Equal: Deep Learning with Importance Sampling"}, {"paperId": "0679950558d72791f16031dd08c39367d8dd47b8", "title": "Shampoo: Preconditioned Stochastic Tensor Optimization"}, {"paperId": "f83a207712fd4cf41aded79e9e6c4345ba879128", "title": "Ray: A Distributed Framework for Emerging AI Applications"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "3299aee7a354877e43339d06abb967af2be8b872", "title": "Don't Decay the Learning Rate, Increase the Batch Size"}, {"paperId": "744fe47157477235032f7bb3777800f9f2f45e52", "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation"}, {"paperId": "1f9163789b549cc6f67e13f54c357317f62c906f", "title": "Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs)"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "f3b30bad610e381da366c4e81ed0b1f3c2b9c26a", "title": "Deep Growing Learning"}, {"paperId": "6fc2ccc1cbb555955291b0989251bd77240dd551", "title": "ImageNet Training in Minutes"}, {"paperId": "9e21d177a7dcfa4acfb674b93103b3d12bbb5b32", "title": "Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates"}, {"paperId": "2788a2461ed0067e2f7aaa63c449a24a237ec341", "title": "Random Erasing Data Augmentation"}, {"paperId": "eb35fdc11a325f21a8ce0ca65058f7480a2fc91f", "title": "Improved Regularization of Convolutional Neural Networks with Cutout"}, {"paperId": "1e3d18beaf3921f561e1b999780f29f2b23f3b7d", "title": "Large Batch Training of Convolutional Networks"}, {"paperId": "8760bc7631c0cb04e7138254e9fd6451b7def8ca", "title": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"}, {"paperId": "e8e22f40d56f176be671da449a2d280e14079f5d", "title": "Nonlinear Acceleration of Stochastic Algorithms"}, {"paperId": "a3f7a30abe44424e5ef8348a02cc103237ac5210", "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability"}, {"paperId": "a7484ac305e3746c9e612d558345d9664bd436de", "title": "FreezeOut: Accelerate Training by Progressively Freezing Layers"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "424a6e62084d919bfc2e39a507c263e5991ebdad", "title": "Self-Normalizing Neural Networks"}, {"paperId": "0d57ba12a6d958e178d83be4c84513f7e42b24e5", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"paperId": "7a530fcd15657d8b53caf261a40c7a20f4c4216d", "title": "Biased Importance Sampling for Deep Neural Network Training"}, {"paperId": "8501e330d78391f4e690886a8eb8fac867704ea6", "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks"}, {"paperId": "81607da4b18bd7ee838afc1ab9894e3c1d836ccc", "title": "On weight initialization in deep neural networks"}, {"paperId": "8ec5896b4490c6e127d1718ffc36a3439d84cb81", "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"}, {"paperId": "b022f2a277a4bf5f42382e86e4380b96340b9e86", "title": "SGDR: Stochastic Gradient Descent with Warm Restarts"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "4e5f9180301bed74f4f5dc03ab11e643f6bfce4e", "title": "Regularized nonlinear acceleration"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "b5c26ab8767d046cb6e32d959fdf726aee89bb62", "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58", "title": "Rethinking the Inception Architecture for Computer Vision"}, {"paperId": "f63e917638553414526a0cc8550de4ad2d83fe7a", "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"}, {"paperId": "23d1f6005128756b5d40f1384980da8351d70d50", "title": "Gradual DropIn of Layers to Train Very Deep Neural Networks"}, {"paperId": "f3b96ef2dc1fc5e14982f1b963db8db6a54183bb", "title": "Improving Neural Machine Translation Models with Monolingual Data"}, {"paperId": "97dc8df45972e4ed7423fc992a5092ba25b33411", "title": "All you need is a good init"}, {"paperId": "d5b4721c8188269b120d3d06149a04435753e755", "title": "Convolutional neural networks with low-rank regularization"}, {"paperId": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "title": "Character-level Convolutional Networks for Text Classification"}, {"paperId": "0d13dae976c95853039395d8544b7cd31987783f", "title": "That\u2019s So Annoying!!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using #petpeeve Tweets"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "f8e79ac0ea341056ef20f2616628b3e964764cfd", "title": "You Only Look Once: Unified, Real-Time Object Detection"}, {"paperId": "37b5dfe87d82ba8f310155165d5bf841dc92dea2", "title": "Cyclical Learning Rates for Training Neural Networks"}, {"paperId": "cb4dc7277d1c8c3bc76dd7425eb1cc7cbaf99487", "title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "f83f4cc19c4b3df2be878e2698981531931705c0", "title": "Butterfly Factorization"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "ecd29385eb214d75fc4b310489ab11977a5d1181", "title": "Random Walk Initialization for Training Very Deep Feedforward Networks"}, {"paperId": "6fc6803df5f9ae505cae5b2f178ade4062c768d0", "title": "Fully convolutional networks for semantic segmentation"}, {"paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327", "title": "Going deeper with convolutions"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "3439a127e45fb763881f03ef3ec735a1db0e0ccc", "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs"}, {"paperId": "99c970348b8f70ce23d6641e201904ea49266b6e", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"}, {"paperId": "aa7bfd2304201afbb19971ebde87b17e40242e91", "title": "On the importance of initialization and momentum in deep learning"}, {"paperId": "8729441d734782c3ed532a7d2d9611b438c0a09a", "title": "ADADELTA: An Adaptive Learning Rate Method"}, {"paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "title": "ImageNet classification with deep convolutional neural networks"}, {"paperId": "a336b26d37f00182a758acf0156b330b8b73cb27", "title": "Anderson Acceleration for Fixed-Point Iterations"}, {"paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "8de174ab5419b9d3127695405efd079808e956e8", "title": "Curriculum learning"}, {"paperId": "355d44f53428b1ac4fb2ab468d593c720640e5bd", "title": "Greedy Layer-Wise Training of Deep Networks"}, {"paperId": "03de8578480c53677c484e1facfced74f4f5b045", "title": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit"}, {"paperId": "d5ddb30bf421bdfdf728b636993dc48b1e879176", "title": "Learning and development in neural networks: the importance of starting small"}, {"paperId": "b7a080b5039b50d347d3492a056be96099294067", "title": "Quasi-Newton Methods, Motivation and Theory"}, {"paperId": "899b8bac810d3fc50e59425a3b6d7faf96470895", "title": "Iterative Procedures for Nonlinear Integral Equations"}, {"paperId": "34ddd8865569c2c32dec9bf7ffc817ff42faaa01", "title": "A Stochastic Approximation Method"}, {"paperId": "ae7f67c705c12391f7198527a0b962340ac8d39c", "title": "ChatAug: Leveraging ChatGPT for Text Data Augmentation"}, {"paperId": "a40f5b9acd81fad048a97562336b46d04cde4023", "title": "Budgeted Training for Vision Transformer"}, {"paperId": null, "title": "Flax: A neural network library and ecosystem for JAX"}, {"paperId": null, "title": "HRBP: Hardwarefriendly regrouping towards block-wise pruning for sparse training"}, {"paperId": "76fe1a92f8f043e9aae54a1f5f06632a1225cd1f", "title": "Trainable Weight Averaging for Fast Convergence and Better Generalization"}, {"paperId": "22cf24bad4fa50d4a54cb2b6fce7a1d22d980058", "title": "Communication-efficient Distributed Learning for Large Batch Optimization"}, {"paperId": "2eca6a0da2aee88b9407be46275e824e7a80aacc", "title": "Deep Model Assembling"}, {"paperId": "dc65a8d92704a8d235822ca90779fc1dbd088bca", "title": "A Deep Learning Dataloader with Shared Data Preparation"}, {"paperId": "7d2a78a1f713b71c3a337247d042c5c2f0b2da84", "title": "EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition"}, {"paperId": null, "title": "beta) channels last memory format in pytorch, 2022. URL https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html# beta-channels-last-memory-format-in-pytorch"}, {"paperId": null, "title": "What\u2019s in my ai? a comprehensive analysis of datasets used to train gpt-1, gpt-2, gpt-3, gpt-neox-20b, megatron-11b, mt-nlg, and gopher, 2022"}, {"paperId": "1554887c6bd76c443a477b27dbcab35877787b27", "title": "LightSeq: A High Performance Inference Library for Transformers"}, {"paperId": "f4fda917eb5c170de67d361b9fdc355599666dbe", "title": "Project CGX: Scalable Deep Learning on Commodity GPUs"}, {"paperId": "a09eceb434c700233ccc31ea02656018f168106d", "title": "Pre-training a BERT with Curriculum Learning by Increasing Block-Size of Input Text"}, {"paperId": "d5f8fc9b0db83d0c11c9534525d0c56e609f1068", "title": "2.5-dimensional Distributed Model Training"}, {"paperId": "28682682ce59cf90de9da7d62d1c17513fdac09a", "title": "Sequence Parallelism: Making 4D Parallelism Possible"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": "6954a6bb9d6f3e365b26b694c963ae1d62a03444", "title": "Fastformer: Additive Attention Can Be All You Need"}, {"paperId": "d931f84abfc4550c10ceb113b142c8eb3e07571e", "title": "Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training"}, {"paperId": null, "title": "Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX"}, {"paperId": null, "title": "The Mosaic ML Team"}, {"paperId": null, "title": "Haiku: Sonnet for JAX"}, {"paperId": null, "title": "Bfloat16: The secret to high performance on cloud tpus \u2014 google cloud blog, 2020"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "11d36cf5fc2fc35c52bd5a735cdd29c685bae95e", "title": "MetaInit: Initializing learning by learning to initialize"}, {"paperId": null, "title": "Residual learning without normalization via better initialization"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "Understanding back-translation at scale"}, {"paperId": null, "title": "Balso. Horovod: fast and easy distributed deep learning in Tensor- Flow"}, {"paperId": null, "title": "An empirical model of largebatch training"}, {"paperId": null, "title": "Fastai - progressive resizing"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": null, "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning"}, {"paperId": null, "title": "Fairscale: A general purpose modular pytorch library for high performance and large scale training"}, {"paperId": null, "title": "xformers: A modular and hackable transformer modelling library"}, {"paperId": null, "title": "The transformer family version 2.0. lilianweng.github.io"}]}