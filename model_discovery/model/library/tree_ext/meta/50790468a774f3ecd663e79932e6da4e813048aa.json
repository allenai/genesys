{"paperId": "50790468a774f3ecd663e79932e6da4e813048aa", "title": "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost", "abstract": "To overcome the quadratic cost of self-attention, recent works have proposed various sparse attention modules, most of which fall under one of two groups: 1) sparse attention under a hand-crafted patterns and 2) full attention followed by a sparse variant of softmax such as $\\alpha$-entmax. Unfortunately, the first group lacks adaptability to data while the second still requires quadratic cost in training. In this work, we propose SBM-Transformer, a model that resolves both problems by endowing each attention head with a mixed-membership Stochastic Block Model (SBM). Then, each attention head data-adaptively samples a bipartite graph, the adjacency of which is used as an attention mask for each input. During backpropagation, a straight-through estimator is used to flow gradients beyond the discrete sampling step and adjust the probabilities of sampled edges based on the predictive loss. The forward and backward cost are thus linear to the number of edges, which each attention head can also choose flexibly based on the input. By assessing the distribution of graphs, we theoretically show that SBM-Transformer is a universal approximator for arbitrary sequence-to-sequence functions in expectation. Empirical evaluations under the LRA and GLUE benchmarks demonstrate that our model outperforms previous efficient variants as well as the original Transformer with full attention. Our implementation can be found in https://github.com/sc782/SBM-Transformer .", "venue": "Neural Information Processing Systems", "year": 2022, "citationCount": 3, "influentialCitationCount": 0, "openAccessPdf": {"url": "https://arxiv.org/pdf/2210.15541", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "Empirical evaluations demonstrate that SBM-Transformer is a universal approximator for arbitrary sequence-to-sequence functions in expectation, and theoretically shows that it outperforms previous efficient variants as well as the original Transformer with full attention."}, "embedding": {"model": "specter_v2", "vector": [0.6042914986610413, 1.032710075378418, -0.524865448474884, -0.31108760833740234, -0.31717202067375183, 0.22978892922401428, 0.24048005044460297, -0.2922665476799011, 0.156818687915802, -0.013262897729873657, 0.292123407125473, 0.32647672295570374, 0.15286165475845337, -0.2716105580329895, -0.6484286189079285, 0.051554884761571884, -1.0641709566116333, 0.30775687098503113, 0.1767049878835678, -0.33405372500419617, 0.02259446121752262, -1.11878502368927, -0.9890225529670715, 0.062493644654750824, 0.5084621906280518, 0.8297058939933777, 0.3512056767940521, 0.672570526599884, -0.3985661268234253, 0.6007301807403564, 0.4423569440841675, -0.7817796468734741, 0.38973933458328247, -0.21825656294822693, -0.4486660063266754, 0.14785398542881012, 0.46692749857902527, 0.2855476438999176, -0.9671301245689392, 1.0827569961547852, -0.24188344180583954, 0.3248499929904938, 0.49427181482315063, -0.7503771781921387, -0.2967027425765991, 1.077553629875183, 0.32043737173080444, 0.7112575173377991, -0.10679154098033905, -0.6680454015731812, 1.7593739032745361, -0.8909664154052734, -0.09832761436700821, 1.3284423351287842, 0.2961444854736328, 0.6197470426559448, -0.6752101182937622, -0.7281495928764343, 0.9791718125343323, 0.6526486277580261, -0.6158019304275513, -0.28643301129341125, 0.21852286159992218, -0.05730435997247696, 1.573285698890686, -0.30712467432022095, 0.23908041417598724, 0.9999469518661499, -0.056306589394807816, 1.4671308994293213, 0.0641404315829277, -0.6040971875190735, -0.15717031061649323, -0.08363289386034012, 0.4920508563518524, 0.8527964353561401, -0.3000072240829468, 0.3679216802120209, -0.9081439971923828, 0.17091421782970428, -0.07294205576181412, 0.0594794899225235, 0.1468830555677414, -0.5410282015800476, 0.413761705160141, 0.5951319932937622, 0.6351622343063354, 0.4842454195022583, -0.2994266152381897, 1.0980713367462158, 0.42564520239830017, 0.10144080966711044, -0.23734191060066223, -0.04952191933989525, 0.16496716439723969, 0.1914338767528534, -0.7831152081489563, -0.15513703227043152, -0.06493082642555237, 1.3042961359024048, -0.0059303357265889645, 0.5849105715751648, -0.39022237062454224, 0.26345139741897583, 1.7010878324508667, -0.010746117681264877, 0.35874277353286743, -0.4106222093105316, 0.19086377322673798, -0.45855116844177246, -0.17013481259346008, -1.208746075630188, -0.22230562567710876, -0.3810003101825714, -0.9404498934745789, -0.9146265983581543, -0.3594193756580353, 0.48434746265411377, -1.068160057067871, 0.9052095413208008, -0.35840660333633423, 0.23530884087085724, -0.5364661812782288, 0.20796416699886322, 0.6452118158340454, 0.7044299840927124, -0.01585445925593376, 0.5179583430290222, 0.7169962525367737, -0.9153857231140137, -0.4760104715824127, -1.3608075380325317, 0.27978506684303284, 0.36863335967063904, 0.04687710851430893, 0.03269566595554352, -1.2285043001174927, -1.0663248300552368, -0.8317353129386902, 0.3847101628780365, -0.4716201424598694, -0.16291594505310059, 0.7680863738059998, 0.7949312329292297, -0.8148755431175232, 0.6613373756408691, -0.4516315758228302, -0.16295142471790314, 1.0170345306396484, 0.511124849319458, 0.06525543332099915, -0.6471238732337952, -1.5209380388259888, 0.3830731213092804, 0.2456289678812027, -0.3943685293197632, -0.4284699559211731, -0.3861925005912781, -1.5393494367599487, 0.3402399718761444, 0.5524582862854004, -0.3618380129337311, 0.7479398846626282, -0.5240982174873352, -1.187951922416687, 1.091525673866272, -0.30521300435066223, -0.13811489939689636, 0.17332996428012848, 0.3238086998462677, -0.3953471779823303, -0.536748468875885, -0.3252001702785492, 0.17813833057880402, 0.6004438996315002, -0.3727349042892456, -0.0501767061650753, -0.04493092745542526, -0.49813851714134216, -0.5690473318099976, -0.2019147276878357, 0.7317041158676147, -0.6356541514396667, -0.22550258040428162, 0.11098820716142654, 0.56163090467453, 0.05887593701481819, -0.3904218077659607, -0.5749949812889099, -1.1218222379684448, 0.4148743748664856, 0.20284995436668396, 0.952682614326477, -0.7778183817863464, -0.687755823135376, 0.19795767962932587, -0.056899312883615494, -0.2272464483976364, -0.7728930115699768, 0.47367942333221436, -0.7036123275756836, 0.17649491131305695, -0.34037601947784424, -0.8822625279426575, 0.09664758294820786, -0.15730611979961395, -0.04294514283537865, 0.1992347240447998, 0.08708613365888596, 0.9289059042930603, -1.1710597276687622, -0.03253677114844322, -0.015683988109230995, 0.031594328582286835, -1.0765420198440552, 0.8623982071876526, -0.31851130723953247, -0.2365993708372116, -0.07664306461811066, -0.15502935647964478, 0.14038759469985962, -0.7128050923347473, 0.4563249945640564, -0.250370055437088, 0.7800399661064148, 0.577079713344574, -0.5529184937477112, 1.1400346755981445, -0.3360961377620697, 0.4764389097690582, 0.15024085342884064, -1.4046716690063477, 0.4479732811450958, 0.1319676786661148, 0.14654816687107086, -0.4436134397983551, 0.24955637753009796, -0.0894511342048645, -0.39560937881469727, 0.14594975113868713, 0.13711418211460114, 0.9547016620635986, -0.1935500204563141, -0.2271052598953247, 0.8357150554656982, -0.14080587029457092, 0.5921600461006165, 0.45878279209136963, 1.073659896850586, 0.5067349076271057, 0.5533980131149292, -0.22063332796096802, 0.16009581089019775, -1.1354053020477295, 0.13549421727657318, 0.8042140007019043, 0.44328269362449646, 0.8205364942550659, 0.4544682204723358, -0.6648296117782593, -0.2603752911090851, 0.1933080404996872, 0.6852455139160156, 1.3795782327651978, -0.05710604786872864, -0.10881172865629196, -0.5300758481025696, -0.5410455465316772, -0.16932901740074158, 0.04314016178250313, -0.8701387643814087, -0.575904369354248, -0.054653048515319824, -1.2953705787658691, 0.1091066300868988, 0.29282909631729126, 0.8220147490501404, -0.7671217322349548, 0.20816271007061005, -0.0655849501490593, 0.2634984850883484, -0.7719067931175232, -0.7693156599998474, 0.7813807129859924, 0.10520555824041367, -0.2651771903038025, 0.23738980293273926, -0.22432801127433777, 0.39280468225479126, -0.7333389520645142, 1.0395861864089966, -0.8364191055297852, -0.24539123475551605, 0.22528935968875885, 0.33339837193489075, -0.431412011384964, -0.3928019404411316, 0.30795034766197205, -0.20072244107723236, 0.4026894271373749, 0.4158041179180145, -0.07347214221954346, -0.16012582182884216, 0.3587195575237274, -0.25354090332984924, 0.0053735231049358845, -0.01595694199204445, 0.1702236831188202, 0.2495211511850357, -0.19455468654632568, 0.07909944653511047, -1.44822096824646, 0.28985998034477234, -0.20776799321174622, -0.49450457096099854, -0.11657324433326721, -0.5748941898345947, -0.5011463761329651, 0.5458699464797974, -0.5498228073120117, -0.1595418006181717, -0.32180219888687134, 0.30153071880340576, -0.4039854109287262, -0.19160467386245728, 0.11885297298431396, -0.003975714091211557, -0.07011133432388306, 0.4258033037185669, 0.7175486087799072, 0.1294647604227066, 0.5994943380355835, 0.4560379385948181, -0.9167748093605042, 0.4333898723125458, 0.17330920696258545, 0.07149505615234375, -0.05552501603960991, 0.006295379716902971, -0.5868744254112244, -0.5909518599510193, -0.48319119215011597, 0.06896181404590607, -0.44394800066947937, 0.05413808301091194, -0.40458983182907104, -1.31864333152771, 0.024864591658115387, -0.5934402346611023, -0.6057651042938232, 0.05324190855026245, -0.4964793026447296, -0.5741124749183655, -0.9445083141326904, -0.8608289361000061, -0.7120499610900879, -0.36613330245018005, -0.7840852737426758, 0.10346871614456177, 0.17322249710559845, -0.30980226397514343, -0.5475531220436096, -0.2401444911956787, -0.18149937689304352, 0.8863605260848999, -0.5962112545967102, 0.3040519654750824, -0.30724018812179565, -0.4324508309364319, -0.6094797849655151, 0.2324007749557495, 0.36095431447029114, 0.14078524708747864, 0.24875755608081818, -0.7393567562103271, 0.3156869113445282, -0.5548430681228638, -0.4726485013961792, 0.24527934193611145, 0.947107195854187, 1.339630365371704, -0.07091959565877914, -0.572157621383667, 0.5043692588806152, 1.3056069612503052, -0.2809216380119324, 0.44796913862228394, 0.04245075210928917, 0.9538832902908325, 0.38225674629211426, -0.5880627036094666, 0.6305815577507019, 0.5296785831451416, 0.5205677151679993, 0.6837902665138245, 0.04619349166750908, 0.04669434577226639, -0.4804033637046814, 0.5997335910797119, 1.262377381324768, 0.5863538384437561, -0.2467101663351059, -0.978442370891571, 1.15829336643219, -1.283581018447876, -1.4882266521453857, 0.6609109044075012, 0.5058133006095886, 0.21845144033432007, -0.06521863490343094, 0.06952760368585587, -0.11565554887056351, 0.6740665435791016, 0.4838729202747345, -0.31119826436042786, -0.4467546343803406, 0.03716859593987465, 0.5349934101104736, 0.3549478352069855, 0.7155103087425232, -0.331311970949173, 0.4875997304916382, 15.02544116973877, 1.014932632446289, 0.10792575031518936, 0.2768840193748474, 0.9142137169837952, 0.2171221524477005, -0.24674467742443085, 0.05383656919002533, -1.1391136646270752, 0.8179925084114075, 1.0030053853988647, 0.3241027891635895, 0.8273501396179199, 0.2328101098537445, -0.12207157164812088, 0.32681119441986084, -0.5444154143333435, 0.5896521806716919, 0.6836495995521545, -1.0208028554916382, -0.1759897768497467, 0.1031925156712532, 0.3608333170413971, 0.6364028453826904, 0.7182062268257141, 0.7563408017158508, 1.1646541357040405, -0.480789452791214, 0.4314853847026825, 0.48573729395866394, 0.9193072319030762, 0.3116566836833954, 0.07820681482553482, 0.3447211980819702, -0.9840465188026428, -0.30823203921318054, -0.49503058195114136, -0.8107489943504333, 0.15474610030651093, 0.3078663945198059, -0.02126075141131878, -0.25449714064598083, 0.1500670462846756, 0.9829914569854736, 0.38757872581481934, 0.5569252967834473, -0.22910422086715698, 0.8883911967277527, -0.25259536504745483, -0.20350651443004608, 0.12200595438480377, 0.1896921694278717, 0.16833901405334473, 0.1203414648771286, -0.02192164957523346, -0.2452618032693863, 0.31806716322898865, 0.5666170716285706, -0.5814703106880188, -0.22290250658988953, -0.35754838585853577, -0.2055315524339676, -0.07090810686349869, 0.8205764889717102, 0.765662670135498, 0.1722775548696518, -0.7506304979324341, 0.08996592462062836, 0.5614421963691711, 0.05040095001459122, -0.3538467288017273, -0.2988310158252716, 0.010004720650613308, -0.49987953901290894, 0.2715594470500946, 0.9748777151107788, -0.3062886595726013, -0.26506027579307556, -0.8546954393386841, -0.5330356955528259, 0.6978206038475037, -1.0723893642425537, -1.003734827041626, 0.6815188527107239, -0.5614404678344727, -0.1421908438205719, 0.5085516571998596, -0.6354085803031921, -0.3569856584072113, 0.26783162355422974, -1.1491023302078247, -0.4488743543624878, -0.17835988104343414, -0.30112424492836, -0.30747342109680176, -0.07687480002641678, 0.8096078634262085, 0.3328164517879486, -0.5072388052940369, 0.21167559921741486, 0.04739026352763176, 0.038399167358875275, -0.3614346981048584, -1.2076469659805298, 1.0252634286880493, 0.22941364347934723, -0.07749198377132416, 0.024663422256708145, 0.09845606237649918, 0.3814448118209839, -0.4238254129886627, -0.0667734295129776, 0.20238222181797028, -0.8132975101470947, -0.002048455411568284, -0.9349042177200317, -0.8513758778572083, 0.6307080984115601, 0.4839801788330078, 0.26351597905158997, 0.35983678698539734, 0.14915110170841217, -0.39393430948257446, -0.655861496925354, -0.5197182297706604, -0.12797608971595764, 0.6468003392219543, -0.459275484085083, -0.25278449058532715, -0.23548659682273865, -0.12226692587137222, -0.8145259022712708, -0.19403409957885742, -0.22320953011512756, 0.08234904706478119, -0.04485849663615227, 0.9849804639816284, -0.5191137194633484, 0.8073143362998962, 0.8561520576477051, 0.09570097923278809, -0.8447259664535522, -0.6382981538772583, -1.2101655006408691, -0.14326487481594086, 0.190535306930542, 0.5154735445976257, -0.6969724297523499, 1.058650016784668, 0.5797659158706665, 0.163219153881073, -0.514600932598114, -0.7028526067733765, -0.2586343288421631, -0.654710590839386, -0.43390849232673645, 0.07145080715417862, 0.18114736676216125, 0.1327449381351471, -0.005932216998189688, 0.1038045585155487, 0.058449637144804, 0.03791512921452522, -0.6350126266479492, 0.32022345066070557, -0.1564493477344513, -0.0811442956328392, -0.7650315761566162, -0.9476911425590515, -1.6774876117706299, 0.09127091616392136, -1.389883041381836, 0.3185717463493347, -0.9648094177246094, -0.10703737288713455, 0.22914761304855347, -0.6008048057556152, 0.10972758382558823, 0.05194663256406784, -0.4288577735424042, -0.6276950836181641, -1.215150237083435, -0.567998468875885, 0.9066762924194336, 0.5110805034637451, -0.9638365507125854, 0.3577207624912262, 0.3409561514854431, -0.36987361311912537, 0.16307297348976135, 0.41934633255004883, -0.806336522102356, -0.456965833902359, -0.7581796050071716, 0.5345963835716248, -0.006242380011826754, -0.10030481219291687, -0.8664384484291077, 0.7335083484649658, 0.09071841835975647, -0.3566458821296692, 0.15809720754623413, 0.43071848154067993, -1.314835548400879, -0.31371524930000305, 0.3939281105995178, -0.7824069261550903, -0.28693217039108276, -0.050570301711559296, 0.0992465615272522, -0.25841107964515686, 0.40197545289993286, 0.27781304717063904, -1.113024115562439, -0.2793084681034088, 0.8177560567855835, -0.6144315600395203, 0.3959179222583771, -0.03632563725113869, -0.5217432975769043, -1.0912734270095825, -0.7824670672416687, -0.32897448539733887, 0.616658091545105, -0.6087902188301086, 0.6836655139923096, 0.3668563961982727, -1.1247872114181519, 0.0721866637468338, 0.30099454522132874, 0.03951305150985718, 0.2076016664505005, 0.7347656488418579, 0.3481733202934265, -0.25882479548454285, -0.010864573530852795, 0.14951540529727936, 0.24501846730709076, -0.5530369877815247, 0.37672486901283264, 1.0510404109954834, -0.32945871353149414, -0.0322815477848053, 1.1434639692306519, -0.17455948889255524, -0.975728452205658, 0.45107710361480713, -0.7357953190803528, -0.45977967977523804, -0.3750333786010742, 0.05527620390057564, 0.21609750390052795, -0.4285259544849396, 0.09796515852212906, -0.5901059508323669, 0.2656869888305664, 0.06525657325983047, 0.043009188026189804, 0.7464313507080078, -0.05303644761443138, -0.268440842628479, 0.4446074962615967, 0.7324445843696594, -0.5304672122001648, -0.7424066066741943, -0.9040122628211975, -0.5468958616256714, -0.13449716567993164, 0.053801991045475006, -0.11342495679855347, -1.0823122262954712, 0.8468624949455261, 0.1959492564201355, 0.7875705361366272, -0.09069108963012695, -0.10488206148147583, -0.2620766758918762, 0.39762699604034424, -0.2514495849609375, -0.6361461281776428, -0.3260604441165924, 1.0446836948394775, 1.0515328645706177, -0.7684264183044434, 0.1388700008392334, -0.5283597707748413, -0.6170899271965027, 0.5097757577896118, 0.6232013702392578, -0.32591065764427185, 1.101805329322815, -0.31423619389533997, -0.45636895298957825, -0.034220606088638306, -1.0379797220230103, -0.5788074731826782, 1.017026424407959, 1.285727620124817, 0.30462467670440674, 0.005323565565049648, 0.5249065160751343, 0.777460515499115, 0.3270152807235718, -0.14367932081222534, 0.43385863304138184, 0.056309785693883896, -0.1980750858783722, 0.11854346841573715, 0.19028329849243164, 1.0187098979949951, -0.8873963356018066, -0.3289410173892975, 0.20281705260276794, 0.2927931249141693, -0.045149195939302444, 0.6862747073173523, 0.7032546997070312, 0.03144661709666252, 0.4049329459667206, -0.08221615105867386, 0.4632701575756073, -0.607370913028717, -0.027200601994991302, -0.5467999577522278, -0.7142736911773682, -0.3172280192375183, -0.30615174770355225, -0.6588796973228455, 0.19213245809078217, 0.06484346836805344, -0.012079835869371891, -0.3353467881679535, -0.0007374391425400972, 0.8503930568695068, 0.5757579803466797, 0.8346771001815796, 0.20393821597099304, -0.30640682578086853, -0.19506356120109558, -0.9575915336608887, -0.18207113444805145, -0.07746411114931107, -0.11261669546365738, -0.24064646661281586, -0.415367066860199, -0.07102291285991669]}, "authors": [{"authorId": "2149157242", "name": "Sungjun Cho"}, {"authorId": "3387623", "name": "Seonwoo Min"}, {"authorId": "2144171168", "name": "Jinwoo Kim"}, {"authorId": "3056520", "name": "Moontae Lee"}, {"authorId": "1697141", "name": "Honglak Lee"}, {"authorId": "2241528", "name": "Seunghoon Hong"}], "references": [{"paperId": "4247f45a5730e3bda5836e2bc7941e30f5b91cb7", "title": "Board"}, {"paperId": "9dc624d7258d1a56117ca720aea953ce46b66b21", "title": "Efficient Attentions for Long Document Summarization"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "4badd753be64c5c5b57dd2bb2e515fbe0c0720d8", "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "deee48c5e0ac0407a1e002905caaf2b174bdb0e6", "title": "MSA Transformer"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "c0f709acf38eb27702b0fbce1215db0ebaa2de2b", "title": "SMYRF: Efficient Attention using Asymmetric Clustering"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd", "title": "Efficient Transformers: A Survey"}, {"paperId": "fbb80084e253ad12fa2085eabec88f3963561254", "title": "SparseRT: Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "02f377ca6e2823782db100bc4379a8499a65e2b6", "title": "Taming Unstructured Sparsity on GPUs via Latency-Aware Optimization"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "09e69bf0926e55cd277a3ef5b1450ba083719cb9", "title": "Sparse and Continuous Attention Mechanisms"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "40ca4fcfffa7ca9aa9b7ff06ecf3cd0436712d78", "title": "$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "b1c39d042fdf8f00a407b0df734764beb6c3b062", "title": "Low-Rank Bottleneck in Multi-head Attention Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "509b4661ed74a24c2ffdbf131f9e1c6a1783752d", "title": "Are Transformers universal approximators of sequence-to-sequence functions?"}, {"paperId": "381411d740562de1e766dc8cc833844eb99dde01", "title": "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks."}, {"paperId": "f6390beca54411b06f3bde424fb983a451789733", "title": "Adaptively Sparse Transformers"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "ad7129af0644dbcafa9aa2f111cb76526ea444a1", "title": "Defending Against Neural Fake News"}, {"paperId": "6007c7cc7634b7c10cebc0549b5533d8d650043b", "title": "Stochastic block models: A comparison of variants and inference methods"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "0d3c46a3cbfe06cec259fec954b6ff6df6c1a566", "title": "Learning long-range spatial dependencies with horizontal gated-recurrent units"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "8b354d76813bd5375e7e5c8d17f630bec5936a01", "title": "ListOps: A Diagnostic Dataset for Latent Tree Learning"}, {"paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4", "title": "Graph Attention Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "98c5afa2c7a3130ed0788aa1d5be1d62a9b10eca", "title": "Community detection and stochastic block models: recent developments"}, {"paperId": "cb2decf6d42855ebbd49678efd310fe864b97c22", "title": "A Note on Quickly Sampling a Sparse Matrix with Low Rank Expectation"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "d6f2f611da110b5b5061731be3fc4c7f45d8ee23", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"}, {"paperId": "41ae5e5860ab27e95e9a8e9dd2dabc5a4815b6f0", "title": "Hierarchical block structures and high-resolution model selection in large networks"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "5625bbaf7dfdf5b675c5213c917939870b5aa0a2", "title": "Stochastic blockmodels and community structure in networks"}, {"paperId": "61c68cc336394a9fc8ff0c044b6af6ad43b02ab8", "title": "Overlapping stochastic block models with application to the French political blogosphere"}, {"paperId": "d9b9fb207013bf8afb064f23f3dffc7edd005f73", "title": "Mixed Membership Stochastic Blockmodels"}, {"paperId": "6e3c56010e987b1f7b0ebc64d93d0442948cf389", "title": "Random graphs"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "8c63a34854f1c9b6051b6b89cadb16e7dba3365f", "title": "An Efficient Method for Generating Discrete Random Variables with General Distributions"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "8ff46c88964a36985f2b45933a3d47b81bd87bd0", "title": "Quora Question Pairs"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": null, "title": "The ACL Anthology network"}]}