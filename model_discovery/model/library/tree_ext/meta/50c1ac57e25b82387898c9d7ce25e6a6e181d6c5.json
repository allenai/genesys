{"paperId": "50c1ac57e25b82387898c9d7ce25e6a6e181d6c5", "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling", "abstract": "Long sequence modeling has gained broad interest as large language models (LLMs) continue to advance. Recent research has identified that a large portion of hidden states within the key-value caches of Transformer models can be discarded (also termed evicted) without affecting the perplexity performance in generating long sequences. However, we show that these methods, despite preserving perplexity performance, often drop information that is important for solving downstream tasks, a problem which we call information neglect. To address this issue, we introduce Chunked Instruction-aware State Eviction (CItruS), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states. In addition, we design a method for chunked sequence processing to further improve efficiency. Our training-free method exhibits superior performance on long sequence comprehension and retrieval tasks over several strong baselines under the same memory budget, while preserving language modeling perplexity.", "venue": "arXiv.org", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work introduces Chunked Instruction-aware State Eviction (CItruS), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states, and designs a method for chunked sequence processing to further improve efficiency."}, "embedding": {"model": "specter_v2", "vector": [0.33284375071525574, 0.7153082489967346, -0.5372139811515808, -0.13896961510181427, -0.5208512544631958, -0.3007141053676605, 0.4364352226257324, -0.047872792929410934, -0.40005916357040405, 0.019291633740067482, 0.7117458581924438, -0.3644718825817108, 0.6082646250724792, 0.5705001950263977, 0.07577233016490936, -0.027110829949378967, -0.885492742061615, 0.32219794392585754, -0.08684790879487991, -0.12940235435962677, 0.15354764461517334, -0.4819483757019043, -0.8774862289428711, 0.1248125359416008, 0.66262286901474, 0.43088269233703613, 0.6224072575569153, 0.7038571834564209, -0.6041152477264404, 0.6205291152000427, 0.15185119211673737, -0.10758377611637115, -0.24879875779151917, -0.10162954777479172, -0.5321208834648132, -0.12201238423585892, 0.2404097616672516, -0.353371798992157, -0.32590946555137634, 0.666053056716919, -0.15295317769050598, 0.18019123375415802, 0.4990570545196533, -0.9636954665184021, 0.04183375835418701, 1.0463541746139526, 0.6221544742584229, 0.5331264734268188, -0.31160083413124084, -0.6009190082550049, 1.4236271381378174, -1.204721212387085, 0.25419798493385315, 1.1531270742416382, 0.3125915825366974, 0.6064745783805847, 0.008105297572910786, -0.5639220476150513, 1.0553842782974243, 0.562587559223175, -0.7802428007125854, -0.5866743326187134, -0.05422135069966316, -0.07310917228460312, 1.9391473531723022, -0.4324969947338104, 0.2020007073879242, 0.6549038290977478, 0.3166517913341522, 1.6654869318008423, -0.16467851400375366, -0.6942573189735413, -0.20871658623218536, -0.27120065689086914, 0.9800392985343933, 0.9797013401985168, -0.6111081838607788, 0.23980367183685303, -0.6189624667167664, -0.06739784777164459, 0.10293113440275192, 0.021277476102113724, -0.15361133217811584, -0.29411759972572327, -0.1845894455909729, 0.6092008352279663, 0.11712391674518585, 0.5686356425285339, -0.13506591320037842, 0.5153507590293884, 0.2613320052623749, 0.02414947934448719, 0.044558603316545486, 0.3174363076686859, -0.3419092893600464, 0.17935530841350555, -1.194113850593567, -0.034812573343515396, 0.058268237859010696, 0.9430970549583435, -0.31131240725517273, 0.6149893999099731, -0.8770083785057068, 0.09399140626192093, 1.373974323272705, 0.0639105886220932, 0.5580661296844482, -0.7416476607322693, 0.04266666993498802, -0.8082611560821533, 0.5725656747817993, -0.193616583943367, -0.37674662470817566, -0.24572838842868805, -0.333841472864151, -1.4205018281936646, -0.6417974829673767, -0.02527947910130024, -0.938575267791748, 0.9969179034233093, -0.4889690577983856, 0.6564692854881287, 0.1086152121424675, 0.22735369205474854, 0.4529123306274414, 0.9858560562133789, 0.5513203740119934, -0.39619460701942444, 0.841810941696167, -0.9832082390785217, -0.6218029856681824, -1.367546558380127, 0.8698354959487915, 0.012944616377353668, 0.047904953360557556, 0.029670801013708115, -1.1184498071670532, -0.9330119490623474, -1.1033148765563965, -0.047903310507535934, -0.2504853904247284, 0.1410137414932251, 0.5920299291610718, 0.35413554310798645, -0.9556068778038025, 0.7243697047233582, -0.38228505849838257, 0.05182914808392525, 0.3168599605560303, 0.03008139692246914, 0.594850480556488, -0.4589027464389801, -1.2791064977645874, 0.38973602652549744, 0.2788547873497009, -0.559097170829773, -0.5482434034347534, -0.873086154460907, -1.322682499885559, 0.34199538826942444, 0.42823538184165955, -0.030398225411772728, 1.3462599515914917, -0.07189110666513443, -1.2247107028961182, 0.4900241494178772, -0.8687780499458313, -0.0584137886762619, -0.176761195063591, -0.48281723260879517, -0.5001024603843689, -0.3950563967227936, -0.40610867738723755, 0.51626056432724, 0.7919123768806458, -0.3269262909889221, -0.029537297785282135, -0.007731737103313208, -0.7942827939987183, -0.3537091314792633, -0.08859221637248993, 0.8585126996040344, -0.27520203590393066, -0.23750443756580353, 0.13842663168907166, 0.5186755061149597, -0.1103380024433136, -0.6783856153488159, -0.5850996375083923, -1.2294946908950806, 0.6953125596046448, -0.40690380334854126, 1.17832350730896, -0.5907590985298157, -0.6288920044898987, -0.39305055141448975, -0.030330875888466835, 0.27397963404655457, -0.961821973323822, 0.7657129168510437, -0.7276860475540161, 0.30015239119529724, -0.3082546591758728, -1.1107192039489746, 0.036692701280117035, -0.1333184689283371, -1.0381298065185547, -0.35843417048454285, -0.16788363456726074, 1.3061470985412598, -0.9775692224502563, -0.12852321565151215, -0.07150425761938095, 0.09794511646032333, -0.7505071759223938, 1.3307276964187622, -0.6685543656349182, 0.34338071942329407, -0.08099813014268875, -0.3776445984840393, 0.06097206100821495, -0.5247513055801392, 0.76842200756073, -0.3742615878582001, -0.07467187941074371, 0.5147654414176941, 0.04088790342211723, 1.2162398099899292, -0.48062029480934143, 0.3821318447589874, -0.3515842854976654, -0.6927134990692139, 0.37690469622612, 0.3502736985683441, -0.07426072657108307, -0.511970043182373, 0.3833269476890564, 0.03868597373366356, -0.5237289071083069, 0.08428001403808594, 0.7888945937156677, 0.8268834948539734, -0.4397711455821991, 0.46382811665534973, 0.4091259837150574, -0.09798567742109299, 0.2716312110424042, 0.8207945227622986, 0.537416398525238, 0.5226851105690002, 0.5119926333427429, -0.05765407159924507, 0.2513033151626587, -0.5720725059509277, 0.11525040119886398, 0.25379982590675354, 0.5860571265220642, 0.7279466390609741, 0.5690163373947144, -0.5950397253036499, -0.5295481085777283, 0.19407716393470764, 1.0074546337127686, 1.4159284830093384, -0.18522800505161285, -0.4896090030670166, -0.957961916923523, -0.5030425190925598, -0.5298179984092712, 0.8198595643043518, -0.21353568136692047, -0.4262751340866089, -0.6528051495552063, -0.5884852409362793, 0.6695663332939148, 0.6164024472236633, 0.796489953994751, -0.6936507821083069, -0.20132552087306976, -0.3179757595062256, -0.14193469285964966, -0.8802566528320312, -0.8171521425247192, 0.44223201274871826, -0.4490589499473572, -0.14724476635456085, 0.04055064171552658, -0.12066543847322464, -0.01213907916098833, -0.6766862869262695, 1.227406620979309, -0.3835136294364929, -0.0779452696442604, 0.2605406939983368, 0.41989365220069885, -0.5116392374038696, -0.8856661319732666, 0.4048458933830261, 0.05450992286205292, -0.25026366114616394, 0.4292334020137787, 0.8282855153083801, 0.18380779027938843, -0.2341231405735016, -0.18154461681842804, 0.3442782759666443, 0.1823403686285019, -0.10079500824213028, 0.44065502285957336, -0.7619889974594116, 0.260393351316452, -1.1197892427444458, 0.6032881140708923, 0.09678010642528534, -0.5066994428634644, 0.6770206689834595, -0.8517281413078308, -0.03807433694601059, 0.42735788226127625, -0.3449746370315552, -0.4726909101009369, -0.9029555916786194, -0.0007816088618710637, -0.48874324560165405, -0.29248517751693726, 0.22335168719291687, 0.14061346650123596, 0.722057044506073, -0.3458850085735321, 1.0197893381118774, 0.5932168364524841, -0.24411386251449585, 0.33408495783805847, -0.699898898601532, 0.5068300366401672, 0.5308929085731506, -0.3386850357055664, -0.5660290122032166, -0.39065122604370117, -0.8647913336753845, -0.5674296617507935, -0.43227052688598633, -0.4068394601345062, -0.17565087974071503, 0.3859218657016754, -0.2757585942745209, -0.8588299751281738, 0.15337592363357544, -1.271458625793457, -0.5985258221626282, 0.35173124074935913, -0.42097240686416626, -0.15531201660633087, -0.8896158337593079, -1.1320689916610718, -0.47116798162460327, -0.5468139052391052, -1.0919232368469238, 0.3337242305278778, -0.20881140232086182, -0.7552712559700012, -0.5658522248268127, 0.1644797921180725, -0.32251232862472534, 0.7221905589103699, -1.0604933500289917, 1.244382381439209, -0.29593104124069214, -0.21745462715625763, 0.04606960341334343, 0.33972403407096863, 0.4933241903781891, -0.4110398292541504, -0.012221242301166058, -0.7959668636322021, -0.03129863739013672, -0.5274251699447632, -0.23858937621116638, 0.20442138612270355, 0.5394374132156372, 0.8594533205032349, -0.03739515319466591, -0.6604644060134888, 0.20811590552330017, 1.1742775440216064, -0.3568344712257385, 0.5206921100616455, -0.47477826476097107, 1.1399928331375122, 0.3164409399032593, -0.08546313643455505, 0.4741038382053375, 0.0967908576130867, 0.34444689750671387, 0.2450643628835678, 0.24452050030231476, 0.049672119319438934, -0.7132646441459656, 0.7496994137763977, 1.779911756515503, 0.6639660000801086, -0.1813778430223465, -1.095955491065979, 0.6079448461532593, -1.0098440647125244, -0.3768913149833679, 0.6033151745796204, 0.7124283909797668, 0.29299503564834595, -0.8602989912033081, -0.5602915287017822, -0.2766983211040497, 0.3111855983734131, 0.6873303651809692, -0.5788716673851013, -0.7545585036277771, 0.4532509744167328, 0.46273645758628845, 0.2128937840461731, 0.9104964733123779, 0.005302150268107653, 0.9091127514839172, 14.842520713806152, 1.0007805824279785, -0.12103667110204697, 0.5471366047859192, 0.614894449710846, 0.30198872089385986, -0.438179075717926, -0.16629713773727417, -1.4464150667190552, -0.05370659753680229, 1.7467725276947021, 0.3407706320285797, 0.20425938069820404, 0.03481855243444443, 0.20886045694351196, 0.03921300917863846, -0.541468620300293, 0.7637035846710205, 0.6307710409164429, -1.2049307823181152, 0.43651631474494934, 0.13538296520709991, 0.09832894802093506, 0.28812330961227417, 0.768245279788971, 0.9236230850219727, 0.6159274578094482, -0.704096794128418, 0.4688273072242737, 0.26536786556243896, 0.8026204109191895, -0.17176489531993866, 0.19580675661563873, 0.8554558753967285, -0.618242621421814, -0.17144882678985596, -0.467593789100647, -0.844143271446228, 0.5102941989898682, 0.024031877517700195, -0.6482939124107361, -0.5306824445724487, -0.6390084624290466, 0.7950327396392822, 0.0820280984044075, 0.12221058458089828, -0.42837685346603394, 1.1671808958053589, 0.4088868796825409, -0.003656954038888216, 0.41187265515327454, 0.2416878044605255, 0.17131099104881287, -0.32491031289100647, -0.035033371299505234, 0.17656710743904114, 0.3300567865371704, 0.22213208675384521, -0.3734629452228546, -0.3948565423488617, -0.29291123151779175, -0.2903319001197815, 0.15085934102535248, 0.5207284092903137, 0.4278613030910492, 0.08570457249879837, -0.27423325181007385, 0.4408494234085083, 0.544904351234436, 0.1368464082479477, -0.21822813153266907, 0.1330888569355011, 0.3198608458042145, -0.5811827182769775, 0.27475452423095703, 0.2408706247806549, 0.06322121620178223, -0.591324508190155, -1.1407580375671387, -0.4892016649246216, 0.4028182625770569, -1.1184923648834229, -0.2619742155075073, 0.7303760647773743, -0.14086927473545074, -0.2880855202674866, -0.11059729754924774, -0.5318883657455444, -0.24970822036266327, 0.30233830213546753, -1.2561393976211548, -0.5918742418289185, 0.09870574623346329, -0.24472244083881378, -0.041029054671525955, 0.3141306936740875, 1.2799838781356812, -0.14413009583950043, -0.5196862816810608, -0.12309969961643219, 0.0739501342177391, 0.035270728170871735, -0.35615912079811096, -0.5899060964584351, 0.8413954377174377, 0.25980597734451294, -0.023492539301514626, 0.047412559390068054, -0.07949136942625046, -0.13201196491718292, -0.6107009053230286, -0.22420816123485565, 0.8840963244438171, -1.120778203010559, -0.7023345232009888, -0.872491717338562, -1.0291082859039307, 0.7473838925361633, 0.5663735270500183, -0.5200102925300598, 0.43855032324790955, -0.04396162927150726, -0.31595563888549805, 0.20097851753234863, -0.5206034779548645, 0.012229900807142258, 0.4792235493659973, -0.43842095136642456, -0.29442837834358215, -0.24902206659317017, 0.6100103259086609, -0.5416454672813416, -0.4285733699798584, -0.5844385027885437, 0.016032738611102104, 0.05578694865107536, 0.6967623233795166, -0.7438373565673828, 0.6019921898841858, 1.2551236152648926, 0.09569118171930313, -1.1033961772918701, -0.16482776403427124, -0.8285753130912781, -0.061582230031490326, 0.10841657966375351, 0.538540780544281, -0.5023647546768188, 0.430298388004303, 0.7700598835945129, 0.11712642014026642, -0.47669166326522827, -0.3130429685115814, -0.33387723565101624, 0.3498786985874176, -0.557952344417572, 0.3013874590396881, -0.1246943324804306, -0.27352529764175415, 0.37047967314720154, 0.19219334423542023, 0.6576861143112183, -0.0975121408700943, -0.540820300579071, 0.1300344616174698, -0.23405635356903076, 0.11545932292938232, -0.14551614224910736, -0.3143889009952545, -1.7629280090332031, 0.5127357244491577, -1.0432592630386353, 0.2197648584842682, -0.7840746641159058, -0.328761488199234, -0.04105442389845848, 0.034026507288217545, 0.22059379518032074, 0.16758668422698975, -0.7161974310874939, -0.43189793825149536, -0.5840898156166077, -0.7178164720535278, 0.9047138094902039, 0.6388994455337524, -0.4628918468952179, 0.283735066652298, -0.3391936719417572, 0.17981870472431183, 0.19845721125602722, 0.4995638132095337, -0.3578471541404724, -0.981488049030304, -1.2675668001174927, 0.23266376554965973, 0.2648547887802124, -0.17041340470314026, -0.4095771908760071, 0.4555072486400604, 0.3559296131134033, -0.2873803675174713, -0.4055975675582886, 0.3560459315776825, -0.6366935968399048, -0.5744540095329285, 0.6718766689300537, -0.8007546067237854, 0.0018532447284087539, 0.0423581600189209, -0.525468647480011, -0.15716058015823364, 0.38159990310668945, 0.11134056001901627, -1.1017322540283203, -1.3500806093215942, 0.620572030544281, -0.7808167338371277, 0.1971338838338852, -0.017161376774311066, -0.015002341009676456, -1.0910447835922241, -0.6072932481765747, -0.14452765882015228, 0.5439127683639526, -0.3595251441001892, 0.9025842547416687, 0.6140906810760498, -0.5971609354019165, -0.017105065286159515, 0.2735300064086914, -0.2303924709558487, -0.14758341014385223, 0.7859365344047546, 0.5266134142875671, -0.1329118311405182, 0.8252676725387573, 0.2320166379213333, 0.3916572034358978, -1.1252776384353638, 0.3579097390174866, 0.7248414158821106, -0.7636265158653259, -0.05443141981959343, 0.8013617396354675, -0.39164599776268005, -1.0192275047302246, 0.014810457825660706, -1.1236504316329956, -0.7416531443595886, -0.1409328579902649, 0.8663270473480225, 0.29109370708465576, 0.005572495050728321, 0.006519986316561699, -0.5779709219932556, 0.1759912222623825, -0.27630284428596497, -0.6071692109107971, 0.6233274936676025, -0.31912410259246826, -0.29015401005744934, 0.6610857844352722, 1.0737279653549194, -0.7248942255973816, -0.8260610103607178, -0.522249162197113, -0.23923712968826294, 0.29621750116348267, 0.3979063034057617, -0.3982795774936676, -0.6025605201721191, 0.88837069272995, 0.3369046151638031, -0.06282307952642441, -0.16188257932662964, -0.07269542664289474, 0.7293777465820312, 0.3841235935688019, 0.19185404479503632, -0.26357871294021606, -0.44931408762931824, 1.5625739097595215, 1.2430542707443237, -0.8438425064086914, 0.06717588752508163, -0.2729763090610504, -0.8009684085845947, 0.6842859983444214, 0.31911760568618774, 0.057520631700754166, 0.8761374354362488, -0.06274872273206711, 0.2200479954481125, 0.12229377031326294, -1.2685612440109253, -0.06937316060066223, 0.6726561784744263, 1.254817247390747, 0.5342701077461243, 0.20298415422439575, 0.4091181457042694, 0.936138927936554, 0.25575679540634155, 0.22518783807754517, 0.5370864868164062, 0.8023398518562317, -0.4751166105270386, -0.13316889107227325, 0.025669638067483902, 0.8608542084693909, -0.6764016151428223, -1.0459800958633423, 0.22864465415477753, 0.35743921995162964, -0.29475027322769165, 0.5336764454841614, 1.0821266174316406, 0.2899450361728668, 0.0797664225101471, 0.7719719409942627, 0.5889991521835327, -0.6499576568603516, -0.10050424933433533, -0.5375833511352539, -0.5258044004440308, -0.33577993512153625, 0.08042625337839127, -0.44062426686286926, -0.537821352481842, -0.06120191887021065, 0.4261828660964966, 0.259148508310318, 0.14553199708461761, 1.3267953395843506, 0.8505029082298279, 0.5935936570167542, -0.3894653022289276, -0.3227910101413727, -0.6391850113868713, -1.1005001068115234, -0.23543305695056915, -0.7718209028244019, -0.11101243644952774, -0.11449015885591507, 0.08218242973089218, 0.030494099482893944]}, "authors": [{"authorId": "144843219", "name": "Yu Bai"}, {"authorId": "2294385553", "name": "Xiyuan Zou"}, {"authorId": "2261394092", "name": "Heyan Huang"}, {"authorId": "2296722484", "name": "Sanxing Chen"}, {"authorId": "3155310", "name": "Marc-Antoine Rondeau"}, {"authorId": "2280864625", "name": "Yang Gao"}, {"authorId": "2280147671", "name": "Jackie Chi Kit Cheung"}], "references": [{"paperId": "d955c898f968df520218f88c1e14db154caf514d", "title": "Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification"}, {"paperId": "89d786457591d39091cf6ef4831f2bbd72698caf", "title": "TransformerFAM: Feedback attention is working memory"}, {"paperId": "3fd5bc3077d04965eaa3498372c39bbdd09d55e4", "title": "Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention"}, {"paperId": "e9576198e9ee767ede4b1ac6a739267aa52a9832", "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"}, {"paperId": "275b005c33a315ad603f236cd5766efe07ef6a54", "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding"}, {"paperId": "ab15463babf98fffc6f683fe2026de0725b5e1a9", "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey"}, {"paperId": "f6440a16ccc5c13d2a86af91b76e078685abfd16", "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor"}, {"paperId": "ce0d07a82ec152258d9ebf2496a16a6737cf89f6", "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts"}, {"paperId": "de41158515fa7260a0983e787650884a98eed811", "title": "On the Resurgence of Recurrent Models for Long Sequences - Survey and Research Opportunities in the Transformer Era"}, {"paperId": "f395f022548d1d1f11e231f42d4852dbff5e9376", "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference"}, {"paperId": "6ac92ad74a4f41f2479da7edb2b483dbf57f10b7", "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing"}, {"paperId": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "850538c1759c56a9f2dab8e84ec63801c41d6396", "title": "System 2 Attention (is something you might need too)"}, {"paperId": "297211bc86653d9ebbe694a75141c9a1c6c11e69", "title": "In-Context Learning Creates Task Vectors"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "b31a5884a8ebe96b6300839b28608b97f8f8ef76", "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "1733eb7792f7a43dd21f51f4d1017a1bffd217b5", "title": "Lost in the Middle: How Language Models Use Long Contexts"}, {"paperId": "f5afaccfe90268485a9961c5771ec5e71e9b806c", "title": "Extending Context Window of Large Language Models via Positional Interpolation"}, {"paperId": "b1fc02780a21bc0f61d8d8e70a8e8f6987c06bb0", "title": "Recurrent Attention Networks for Long-text Modeling"}, {"paperId": "1ea09be3944b6c9054a76e46101f91dd787b36ce", "title": "Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed", "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"}, {"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, {"paperId": "7bd4ca8706a79983d31ab74e6c79bfdfd949602e", "title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "dbc368bc8b49347dd27679894524fa62f88492c9", "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "8db711adf1beb3e0c2ec492f3936841d827404e9", "title": "The NLP Task Effectiveness of Long-Range Transformers"}, {"paperId": "365fbeeefe2b219ed36b3f3a603c8d7bae99d48a", "title": "RoR: Read-over-Read for Long Document Machine Reading Comprehension"}, {"paperId": "6da4b231148bf26677233d1f778d08a5d26f4313", "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference"}, {"paperId": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044", "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"}, {"paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"}, {"paperId": "56bab0a09fd88ddf0aaf5d4f7a684bc0e82d4f0c", "title": "Cooperative Self-training of Machine Reading Comprehension"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "f9700e31a1d0ae34d4571ab056dfb268c1543349", "title": "SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "efe4902a39c8ef332058ae7d156a6bffcd3c1341", "title": "Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "22655979df781d222eaf812b0d325fa9adf11594", "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "60b05f32c32519a809f21642ef1eb3eaf3848008", "title": "ROUGE: A Package for Automatic Evaluation of Summaries"}, {"paperId": "2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "title": "Learning Question Classifiers"}, {"paperId": "75895ce98904e8afaaa248f081a1da501bd2dbe2", "title": "Toward Semantics-Based Answer Pinpointing"}, {"paperId": "be1fed9544830df1137e72b1d2396c40d3e18365", "title": "A Cache-Based Natural Language Model for Speech Recognition"}, {"paperId": null, "title": "2024b. H2o: Heavy-hitter oracle for efficient generative inference of large language models"}, {"paperId": null, "title": "2024. Transformers are multi-state rnns"}, {"paperId": null, "title": "2023. Mistral 7b"}, {"paperId": "65face5d99dec0beb4655afd5703e192b032a96b", "title": "Annals of the New York Academy of Sciences Language and Thought Are Not the Same Thing: Evidence from Neuroimaging and Neurological Patients"}, {"paperId": null, "title": "2023. Lm-infinite: Zero-shot extreme length generalization for large language models"}, {"paperId": null, "title": "2022. Pretraining without attention"}, {"paperId": null, "title": "2023 Function vectors in large language models"}, {"paperId": null, "title": "2022. Learned token pruning for transformers"}, {"paperId": null, "title": "Instruction 4 Answer the question based on the given passages. Only give me the answer and do not output any other words"}]}