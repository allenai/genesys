{"paperId": "e4948367fffd12029184d4828718e6208f9ed64b", "title": "Generalizable and Stable Finetuning of Pretrained Language Models on Low-Resource Texts", "abstract": "Pretrained Language Models (PLMs) have advanced Natural Language Processing (NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses significant challenges such as instability and overfitting. Previous methods tackle these issues by finetuning a strategically chosen subnetwork on a downstream task, while keeping the remaining weights fixed to the pretrained weights. However, they rely on a suboptimal criteria for sub-network selection, leading to suboptimal solutions. To address these limitations, we propose a regularization method based on attention-guided weight mixup for finetuning PLMs. Our approach represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection. Furthermore, we employ a bi-level optimization (BLO) based framework on two separate splits of the training dataset, improving generalization and combating overfitting. We validate the efficacy of our proposed method through extensive experiments, demonstrating its superiority over previous methods, particularly in the context of finetuning PLMs on low-resource datasets. Our code is available at https://github.com/Sai-Ashish/Attention_guided_weight_mixup_BLO.", "venue": "North American Chapter of the Association for Computational Linguistics", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work proposes a regularization method based on attention-guided weight mixup for finetuning PLMs, which represents each network weight as a mixup of task-specific weight and pretrained weight, controlled by a learnable attention parameter, providing finer control over sub-network selection."}, "embedding": {"model": "specter_v2", "vector": [0.336844801902771, 0.9075802564620972, -0.2965487241744995, -0.036839090287685394, -0.25667905807495117, -0.0257093645632267, 0.2941255271434784, -0.39016664028167725, -0.8128283619880676, 0.07312555611133575, 0.6656855940818787, 0.046776991337537766, 0.08797525614500046, 0.13723009824752808, -0.07799302786588669, 0.24613317847251892, -0.6365113258361816, 0.4101647138595581, -0.2702053487300873, -0.5805620551109314, -0.49818554520606995, -0.7713404893875122, -0.2552400231361389, -0.06584799289703369, 0.4716222584247589, 0.13907890021800995, 0.35989701747894287, 0.6963174939155579, -0.33826592564582825, -0.1639888882637024, 0.5041218400001526, -0.4606669843196869, 0.3170136511325836, -0.20635539293289185, -0.40408051013946533, 0.13624106347560883, 0.45742401480674744, 0.07313980162143707, -0.5630138516426086, 1.2076762914657593, -0.28265050053596497, 0.4939965605735779, 0.6750993132591248, -0.36555513739585876, -0.38815826177597046, 1.4254804849624634, 0.5398489832878113, 0.7542704939842224, -0.537682056427002, -0.49275875091552734, 1.5554918050765991, -1.04728364944458, 0.2545499801635742, 1.7036763429641724, 0.5874730944633484, 0.5697547197341919, -0.9350473284721375, -1.1078468561172485, 0.8914700150489807, 0.0018285232363268733, -0.8481494188308716, -0.16024339199066162, 0.029571637511253357, -0.14841462671756744, 2.165546417236328, -0.5840787291526794, -0.12280736118555069, 0.5065405368804932, -0.012905703857541084, 1.407262921333313, -0.38230204582214355, -0.7321460843086243, -0.9816358089447021, 0.14451012015342712, 0.3803322911262512, 0.7599899172782898, -0.6167629957199097, 0.06767003983259201, -0.8149970769882202, -0.1827182173728943, 0.22454802691936493, -0.4091605544090271, -0.34269919991493225, 0.3275407552719116, -0.1979677379131317, 0.8603376150131226, 0.7494296431541443, 1.0522520542144775, -0.3933337330818176, 0.3280704915523529, 0.5491523742675781, 0.637316107749939, 0.23574817180633545, 0.8402626514434814, -0.4877195954322815, 0.9129584431648254, -0.896317183971405, 0.14946018159389496, -0.015956586226820946, 0.7247145175933838, 0.27970090508461, 0.08301392197608948, -0.8681313395500183, 0.3579765856266022, 1.4061176776885986, -0.4285210371017456, 0.5817176103591919, -0.47744226455688477, 0.6511254906654358, -0.5568094849586487, -0.22278499603271484, -0.9154951572418213, -0.6102959513664246, -0.3412759304046631, -0.9981986284255981, -1.446617841720581, -0.18505196273326874, -0.29126378893852234, -0.6189411282539368, 1.1775683164596558, -0.3428402543067932, -0.0023728646337985992, -0.24250726401805878, 0.3674545884132385, 0.2973652482032776, 0.8078945279121399, 0.643847644329071, 0.14984560012817383, 1.0267410278320312, -1.0142011642456055, -0.8247607946395874, -1.2943382263183594, 0.42239198088645935, -0.20519928634166718, 0.5040525794029236, -0.174251988530159, -1.15648353099823, -0.6911709308624268, -0.7229183316230774, -0.20895017683506012, -0.49780037999153137, 0.42933282256126404, 0.7440563440322876, 0.2709774076938629, -0.5448741912841797, 1.0674469470977783, 0.19834555685520172, -0.20610396564006805, 0.4357335567474365, 0.35904771089553833, 0.16074597835540771, -0.20097221434116364, -1.6949297189712524, 0.4971679449081421, 0.6706492900848389, -0.3408646881580353, 0.028328247368335724, -0.873489499092102, -0.7851359248161316, 0.23079189658164978, 0.2373492270708084, -0.70954430103302, 0.9346776604652405, -0.4796106815338135, -1.3902928829193115, 0.8368623852729797, -0.2583078145980835, -0.07640068978071213, 0.360948771238327, -0.22231100499629974, -0.4315624535083771, -0.42223790287971497, -0.3941437005996704, 0.6614342927932739, 0.5261399149894714, 0.14254221320152283, -0.04339224100112915, 0.25340795516967773, -0.43851208686828613, -0.009150402620434761, -0.6850014328956604, 0.8920356035232544, -0.36329931020736694, -0.35478585958480835, 0.07360216230154037, 0.8598774075508118, -0.02198048122227192, -0.7674921154975891, -0.7508894205093384, -1.4048618078231812, 0.16404989361763, -0.12948179244995117, 1.190325379371643, -0.657998263835907, -0.19355089962482452, 0.03362371772527695, 0.09367740154266357, -0.09734480828046799, -1.2302442789077759, 0.5371430516242981, -0.4038410484790802, 0.6395813226699829, 0.053209077566862106, -1.4514659643173218, 0.2553708255290985, -0.483977347612381, -0.35480424761772156, -0.12610448896884918, 0.3019842207431793, 1.2171690464019775, -0.8856790661811829, -0.015583474189043045, -0.2869955599308014, 0.5732404589653015, -1.3767406940460205, 0.9924611449241638, -0.42377620935440063, 0.2017614245414734, -0.05397377535700798, -0.5334740281105042, -0.06532461941242218, -0.25873255729675293, 0.3148934543132782, -0.791759729385376, 0.11540775001049042, 0.6148632764816284, -0.16247452795505524, 1.2144335508346558, -0.5642061829566956, 0.5339642763137817, 0.23051391541957855, -0.5179744958877563, -0.030370132997632027, 0.4298267960548401, -0.033438052982091904, 0.0197137501090765, 0.5325402617454529, 0.3492757976055145, -0.566260576248169, 0.48399731516838074, 0.7687942981719971, 0.43577608466148376, -0.20520073175430298, -0.029558302834630013, 0.8362749218940735, -0.10565181821584702, 0.7890293598175049, 0.2224707007408142, 0.5635473728179932, 0.15903091430664062, 0.4589010775089264, -0.16639280319213867, 0.5423451066017151, -0.7247546911239624, 0.09954265505075455, 0.2702693045139313, 0.5580623745918274, 0.6389193534851074, 0.5207470655441284, -0.42171967029571533, -0.31120565533638, 0.14551599323749542, 0.5577835440635681, 1.7852733135223389, -0.7054195404052734, 0.0972871333360672, -0.728376030921936, -0.36787480115890503, -0.35361534357070923, 0.02535385824739933, -0.4599516689777374, 0.09064151346683502, -0.5730506181716919, -1.3803577423095703, 0.5945749282836914, -0.29331618547439575, 0.8054449558258057, -0.6632623076438904, 0.4498356878757477, 0.08560945838689804, 0.1502687931060791, -0.8316567540168762, -1.1137182712554932, 0.35796016454696655, -0.4756302535533905, -0.4043678045272827, -0.016918085515499115, -0.13009309768676758, 0.05727282166481018, -0.8279377222061157, 1.1570398807525635, -0.6624600291252136, 0.017259491607546806, -0.09142345190048218, 0.7145625948905945, -0.5154533982276917, -0.5940187573432922, 0.8425556421279907, 0.14842480421066284, 0.3324790596961975, 0.4894302487373352, 0.40883538126945496, 0.06839384883642197, 0.0010514117311686277, -0.4260798692703247, 0.09257082641124725, -0.04243755340576172, 0.290214866399765, 0.6770875453948975, 0.07570300996303558, 0.47998982667922974, -1.8323396444320679, 1.098083257675171, -0.028584638610482216, -0.39465633034706116, 0.2885448932647705, -0.2889409065246582, -0.3648452162742615, 0.7087717056274414, -1.0511201620101929, -0.5430055856704712, -0.7648253440856934, 0.2881741523742676, -0.20889167487621307, -0.0789489895105362, 0.5369536280632019, 0.12491261214017868, 0.36118006706237793, 0.6826282739639282, -0.10436619818210602, 0.1042502149939537, -0.21775074303150177, 0.5107372403144836, -0.9974164962768555, 0.4104197919368744, 0.4863089919090271, 0.4330710470676422, -0.38468074798583984, -0.4472268521785736, -0.8853166103363037, -0.8493742942810059, -0.48181426525115967, -0.15969733893871307, 0.038157153874635696, 0.2501879334449768, -0.36818957328796387, -0.861850917339325, -0.2765559256076813, -0.7988837361335754, -0.6472063064575195, 0.3016599714756012, -0.22014719247817993, 0.013470029458403587, -1.2462831735610962, -1.3923683166503906, -0.35205894708633423, -0.9136037826538086, -0.8646830320358276, 0.06979678571224213, 0.21443183720111847, -0.5997207760810852, -0.9928640127182007, -0.10764185339212418, -0.06982928514480591, 1.0939704179763794, -0.5971466898918152, 0.9241939187049866, -0.043963901698589325, 0.31620824337005615, -0.42559146881103516, 0.006203019991517067, 0.6282494068145752, -0.24427776038646698, 0.1505747139453888, -1.012174367904663, 0.07568194717168808, -0.511676549911499, -0.4306443929672241, 0.4793446362018585, 0.5612922310829163, 0.6344812512397766, -0.1262134611606598, -0.48079994320869446, 0.759871244430542, 1.3989819288253784, -1.2514456510543823, -0.10833650827407837, 0.32810166478157043, 0.9513418078422546, 0.3279603123664856, -0.5659615397453308, 0.11953986436128616, 0.5315421223640442, 0.07047878205776215, -0.10287118703126907, -0.02119211107492447, -0.325061172246933, -0.8885476589202881, 0.494644433259964, 1.7063008546829224, 0.40639984607696533, -0.29336705803871155, -0.9108849763870239, 0.6103455424308777, -1.3598122596740723, -0.6559245586395264, 0.5381718277931213, 0.3809351325035095, 0.8224779963493347, -0.48243942856788635, -0.39253687858581543, -0.3568457067012787, 0.439048707485199, 0.35401734709739685, -0.41387319564819336, -0.442671537399292, 0.05358445644378662, 0.0843871608376503, 0.41808074712753296, 0.8755960464477539, -0.7963626384735107, 1.1102362871170044, 14.213854789733887, 0.8959998488426208, 0.19940032064914703, 0.8514112234115601, 0.5198430418968201, 0.17475248873233795, -0.37199586629867554, -0.45063143968582153, -1.6354889869689941, -0.3670017719268799, 0.8272120952606201, 0.2646179497241974, 1.196898102760315, -0.3148344159126282, 0.31098565459251404, 0.3639534115791321, -0.4609575569629669, 0.4827714264392853, 0.46858009696006775, -1.114374041557312, 0.46556365489959717, 0.11863528192043304, 0.6882850527763367, 0.8918612599372864, 0.43835267424583435, 1.0777277946472168, 0.5218495726585388, -0.42635324597358704, 0.2442101091146469, 0.39661332964897156, 0.5076576471328735, 0.024044319987297058, 0.38585683703422546, 0.823087751865387, -0.8435064554214478, -0.38675904273986816, -0.7969725131988525, -0.8084129095077515, 0.25307899713516235, 0.09338843822479248, -0.20012547075748444, -0.634955644607544, -0.175270214676857, 0.997147798538208, 0.004247815348207951, 0.22897130250930786, -0.33501458168029785, 0.6850742101669312, -0.23634560406208038, 0.05343342944979668, 0.5287989377975464, 0.33055803179740906, 0.38427960872650146, 0.4431101679801941, -0.2186521738767624, 0.027166729792952538, 0.2560342252254486, 0.8202545642852783, -0.9955735802650452, 0.13495828211307526, 0.14678530395030975, -0.2584577798843384, -0.023173188790678978, 1.124427318572998, 0.6389957070350647, 0.3737422525882721, -0.39737042784690857, 0.1274370551109314, 1.1250054836273193, 0.5638588070869446, 0.10246619582176208, -0.2534484565258026, 0.17128610610961914, -0.4954846501350403, -0.24790950119495392, 0.5081457495689392, -0.4409966468811035, -0.7128329873085022, -1.1366537809371948, -0.38229429721832275, 0.5049740672111511, -0.6335808038711548, -1.1127715110778809, 0.8538616299629211, -0.30455902218818665, 0.19612745940685272, 0.6072973012924194, -0.8691924810409546, -0.19514819979667664, 0.9683423042297363, -1.6345704793930054, -0.6717420220375061, 0.6472160220146179, -0.44343453645706177, -0.3629339337348938, -0.5274612903594971, 1.0523492097854614, 0.4136597514152527, -1.02776038646698, 0.36817941069602966, -0.17081677913665771, 0.15551328659057617, -0.07794074714183807, -0.6992670893669128, 0.582262396812439, 0.2670348882675171, -0.25128644704818726, 0.19102098047733307, -0.08346349745988846, 0.1478830873966217, -0.5904107689857483, -0.2638624608516693, 1.1842336654663086, -0.46495458483695984, 0.12048597633838654, -0.41322827339172363, -0.8622003793716431, 0.4999447166919708, 0.8066157102584839, -0.5031129717826843, 0.4713961184024811, 0.37606173753738403, -0.43860501050949097, 0.02397218905389309, -0.852686882019043, 0.1445281058549881, 0.20297499001026154, -0.7359191179275513, -0.4012429714202881, 0.12299825996160507, 0.14632810652256012, -0.8441082239151001, -0.703925371170044, -0.32582250237464905, -0.21832872927188873, 0.4098297655582428, 1.1083449125289917, -0.7175688743591309, 0.5488429069519043, 0.877256453037262, 0.02039548009634018, -1.2136784791946411, -0.4227674901485443, -0.9551384449005127, 0.29068174958229065, 0.24447956681251526, 1.0204893350601196, -0.4951503872871399, 0.16369017958641052, 0.9965765476226807, 0.32376745343208313, -0.3241901695728302, -0.4052809774875641, -0.14423435926437378, 0.06016818806529045, -0.45688724517822266, 0.09170587360858917, 0.0326877124607563, -0.19311118125915527, 0.40696412324905396, 0.6226822137832642, 0.7275606989860535, -0.04321649670600891, -0.9208621978759766, 0.17303399741649628, 0.04303976148366928, -0.13414372503757477, -1.0150359869003296, -0.026748990640044212, -1.3443511724472046, 0.4914284646511078, -1.56007981300354, -0.09650911390781403, -1.095041036605835, -0.5416892766952515, 0.09820931404829025, -0.163595512509346, 0.4463335871696472, -0.08658190816640854, -0.08366500586271286, -0.24396653473377228, -0.5843147039413452, -0.13766320049762726, 0.8575335144996643, 1.1568198204040527, -1.08853018283844, -0.32622435688972473, 0.237837553024292, -0.05236486718058586, 0.6941204071044922, 0.7503919005393982, -0.5120953321456909, -0.5689898133277893, -1.5032981634140015, 0.6434633135795593, -0.35166001319885254, -0.2496139407157898, -0.24862396717071533, 0.8858914375305176, 0.2230406105518341, -0.2533572018146515, 0.16945935785770416, 0.17308077216148376, -0.9701053500175476, -0.6809132099151611, 0.09662175178527832, -0.8173613548278809, -0.09068763256072998, 0.2211436927318573, -0.4528912901878357, -0.5210320949554443, 0.3988862633705139, 0.17777705192565918, -1.093332290649414, -0.3119010329246521, 0.618027925491333, -0.38202786445617676, 0.49185413122177124, -0.6696392297744751, 0.15568098425865173, -0.7764679193496704, -0.18071535229682922, -0.09770558774471283, 0.7571121454238892, -0.9943090677261353, 0.6229309439659119, 0.20353251695632935, -1.1352370977401733, -0.24184538424015045, 0.3709471821784973, -0.31868553161621094, 0.1454954445362091, 0.5442764163017273, 0.566525936126709, -0.3637695610523224, 0.486135870218277, 0.6096410751342773, 0.6863913536071777, -0.7451747059822083, -0.3767262399196625, 1.1663802862167358, -0.8944487571716309, -0.2238689512014389, 1.2464009523391724, -0.2500672936439514, -1.5592137575149536, 0.43683817982673645, -1.1210118532180786, -0.7167774438858032, 0.2872651219367981, 0.6035375595092773, 0.2551615834236145, -0.17794044315814972, -0.17942358553409576, -0.22163337469100952, 0.20002403855323792, 0.1058645099401474, -0.21599313616752625, 0.8828672766685486, -0.48807835578918457, -0.5430331826210022, 0.4810338616371155, 1.073984146118164, -0.8680140972137451, -0.8517817854881287, -1.097668170928955, -0.06334681063890457, 0.287878155708313, 0.7836816310882568, -0.5153040289878845, -0.9897860884666443, 0.5766383409500122, -0.2448965311050415, 0.44854363799095154, 0.22362715005874634, -0.4953760802745819, 0.1208275556564331, 0.613709032535553, -0.28089606761932373, -0.8702055215835571, -0.6833235025405884, 1.6021924018859863, 1.4187933206558228, -0.9117872714996338, 0.06790921837091446, -0.21382160484790802, -0.7519880533218384, 1.132807731628418, 0.14236198365688324, -0.3616728186607361, 1.168899655342102, -0.45413902401924133, -0.044914375990629196, 0.15375664830207825, -0.8362091779708862, -0.3975639343261719, 1.2745779752731323, 0.5920187830924988, 0.7911941409111023, 0.2628011703491211, 0.10056688636541367, 0.9859943985939026, -0.10368040204048157, -0.03664609417319298, 0.2865098714828491, -0.28762975335121155, -0.2043602168560028, -0.023163914680480957, -0.025468453764915466, 0.708876371383667, -0.9234437346458435, -0.457096129655838, 0.020939258858561516, 0.4817099869251251, 0.2970937192440033, 0.4969272017478943, 0.4945976734161377, 0.17701758444309235, 0.840377688407898, 0.3665035665035248, 0.33986902236938477, -0.5886778235435486, -0.8222320079803467, -0.0014753683935850859, -0.5468470454216003, -0.11097148805856705, -0.26551780104637146, -0.4502112567424774, -0.1728896200656891, -0.13714270293712616, 0.23112742602825165, -0.4685393273830414, 0.28562667965888977, 1.2284198999404907, 0.12954260408878326, 0.9204707145690918, -0.3910481333732605, -0.8768351674079895, -0.46639856696128845, -1.3878971338272095, 0.008949612267315388, -0.40707188844680786, -0.07782390713691711, 0.16681893169879913, -0.4654209017753601, -0.4329544007778168]}, "authors": [{"authorId": "1996182847", "name": "Sai Ashish Somayajula"}, {"authorId": "2277979113", "name": "Youwei Liang"}, {"authorId": "2292215662", "name": "Abhishek Singh"}, {"authorId": "2238890965", "name": "Li Zhang"}, {"authorId": "2268043257", "name": "Pengtao Xie"}], "references": [{"paperId": "a6e2bc7016a076e133b022dbefa5c19904e32869", "title": "Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively"}, {"paperId": "65f056d32dac701240a52a5daf8cedb611b04ceb", "title": "Patching open-vocabulary models by interpolating weights"}, {"paperId": "5be4f2074e4811fec280248e10e2f7b2faee54dd", "title": "Betty: An Automatic Differentiation Library for Multilevel Optimization"}, {"paperId": "54020e5fe48ebb250f27d744e20a63cac2988a84", "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"}, {"paperId": "a151bc8efa4f0aa81f48c42ecae91e6f0ede5433", "title": "Balance-Subsampled Stable Prediction Across Unknown Test Data"}, {"paperId": "f45261b7b53043c316f45f613cb735907b93fb5a", "title": "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning"}, {"paperId": "5b7faab4ac63912788d7c8108a93330b382b8c92", "title": "On the Variance of the Fisher Information for Deep Learning"}, {"paperId": "520bd2331cca8d5a9c032c186a2a0f7704ead6ff", "title": "R-Drop: Regularized Dropout for Neural Networks"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "a4b9530c820b5fdb9edd869555cc7a2c2a1d21ad", "title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "b88c11922cac84e5ea902f82d27ae21c3dda2e04", "title": "Better Fine-Tuning by Reducing Representational Collapse"}, {"paperId": "168dbec385e09a9eb8267d74c24eab83ad011180", "title": "Meta-Semi: A Meta-learning Approach for Semi-supervised Learning"}, {"paperId": "9566da1b6af07462bc0ba54f24e47ba8ea82adcf", "title": "Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning"}, {"paperId": "056935031bc5cf0aeeaa0946320de26e14a1817e", "title": "Revisiting Few-sample BERT Fine-tuning"}, {"paperId": "8b9d77d5e52a70af37451d3db3d32781b83ea054", "title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines"}, {"paperId": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583", "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "f2f3c83db919a2429c4fcad2d0a0ed4e5294354a", "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting"}, {"paperId": "baf60d13c98916b77b09bc525ede1cd610ed1db5", "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"}, {"paperId": "a84d359922a69916a05de7c91204b79d02c36cda", "title": "Optimizing Millions of Hyperparameters by Implicit Differentiation"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "222b9a7b8038120671a1610e857d3edbc7ac5550", "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models"}, {"paperId": "a2933c8d0e152264d1cd25ca8248b25d4b49038b", "title": "Meta-Learning with Implicit Gradients"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c", "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"}, {"paperId": "36afb8fbf8a2d6712a9533984e853e3804243a9a", "title": "Limitations of the Empirical Fisher Approximation"}, {"paperId": "4c909ca74217234831bf2900aa83a4761823f2b1", "title": "Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "b47381e04739ea3f392ba6c8faaf64105493c196", "title": "Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"}, {"paperId": "c1f457e31b611da727f9aef76c283a18157dfa83", "title": "DARTS: Differentiable Architecture Search"}, {"paperId": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "title": "Neural Network Acceptability Judgments"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "9ea50b3408f993853f1c5e374690e5fbe73c2a3c", "title": "Continual Lifelong Learning with Neural Networks: A Review"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "fda8159f997a07aec05cc2f6d3ac6c72c94988ba", "title": "A Review on Bilevel Optimization: From Classical to Evolutionary Approaches and Applications"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "512ca06114f5292f7d0b536ce030e319863c781a", "title": "Online Learning Rate Adaptation with Hypergradient Descent"}, {"paperId": "c889d6f98e6d79b89c3a6adf8a921f88fa6ba518", "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"paperId": "9be7e7579fbec5d45e3e6ea1c4465258225a183d", "title": "Initializing Bayesian Hyperparameter Optimization via Meta-Learning"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "9f62067945d991cd78a62cf647de17f01d1b54d3", "title": "Frustratingly Easy Domain Adaptation"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": null, "title": "Trans-formers: State-of-the-art natural language processing"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": null, "title": "while not converged do Update task weights W using Equation"}, {"paperId": null, "title": "2022. Peft: State-of-the-art parameter-efficient fine-tuning methods"}, {"paperId": null, "title": "2022a. Editing models with task arithmetic"}, {"paperId": null, "title": "2023. Bi-drop: Generalizable fine-tuning for pre-trained language models via adaptive subnetwork optimization"}]}