{"paperId": "ca1952a54cf85f0f449cb337f70b083100069c9c", "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification", "abstract": "KV cache stores key and value states from previous tokens to avoid re-computation, yet it demands substantial storage space, especially for long sequences. Adaptive KV cache compression seeks to discern the saliency of tokens, preserving vital information while aggressively compressing those of less importance. However, previous methods of this approach exhibit significant performance degradation at high compression ratios due to inaccuracies in identifying salient tokens. In this paper, we present ZipCache, an accurate and efficient KV cache quantization method for LLMs. First, we construct a strong baseline for quantizing KV cache. Through the proposed channel-separable tokenwise quantization scheme, the memory overhead of quantization parameters are substantially reduced compared to fine-grained groupwise quantization. To enhance the compression ratio, we propose normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix. Moreover, we develop an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention. Extensive experiments demonstrate that ZipCache achieves superior compression ratios, fast generation speed and minimal performance losses compared with previous KV cache compression methods. For instance, when evaluating Mistral-7B model on GSM8k dataset, ZipCache is capable of compressing the KV cache by $4.98\\times$, with only a $0.38\\%$ drop in accuracy. In terms of efficiency, ZipCache also showcases a $37.3\\%$ reduction in prefill-phase latency, a $56.9\\%$ reduction in decoding-phase latency, and a $19.8\\%$ reduction in GPU memory usage when evaluating LLaMA3-8B model with a input length of $4096$.", "venue": "arXiv.org", "year": 2024, "citationCount": 2, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes normalized attention score as an effective metric for identifying salient tokens by considering the lower triangle characteristics of the attention matrix, and develops an efficient approximation method that decouples the saliency metric from full attention scores, enabling compatibility with fast attention implementations like FlashAttention."}, "embedding": {"model": "specter_v2", "vector": [0.24509747326374054, 0.17936575412750244, -0.9977121949195862, 0.4104844629764557, -0.7813481092453003, 0.12879733741283417, 0.1986834853887558, 0.2034774273633957, -0.568215012550354, -0.2848148047924042, 0.6730225086212158, 0.12467603385448456, 0.6282604932785034, -0.051756851375103, -0.10425423830747604, -0.3509806990623474, -1.0803180932998657, -0.16918019950389862, 0.29326891899108887, 0.16238023340702057, 0.5623960494995117, -0.5541878342628479, -1.7225329875946045, 0.2374235987663269, 0.566779375076294, 1.2127084732055664, 0.1730683296918869, 1.0309488773345947, -0.6015707850456238, 0.40459075570106506, 0.538098156452179, 0.20712673664093018, -0.07055831700563431, 0.02184886299073696, -0.7459551095962524, -0.48341095447540283, 0.25076839327812195, -0.4826489984989166, -0.4133550226688385, 0.5720598101615906, 0.018057774752378464, 0.1843966245651245, 0.4104224443435669, -0.8060081601142883, 0.06742464005947113, 0.5856328010559082, 0.2695315480232239, 0.9514294266700745, -0.47654712200164795, -0.4891238212585449, 1.3347790241241455, -1.3131296634674072, -0.0463644303381443, 0.914402961730957, 0.3612944781780243, 0.12716467678546906, 0.041046421974897385, -0.012065615504980087, 0.4066358804702759, 0.6250661611557007, -0.7811498045921326, -0.32006117701530457, -0.22608886659145355, 0.0920657068490982, 1.6345304250717163, -0.06531278043985367, 0.394898921251297, 0.13750067353248596, 0.13455069065093994, 1.6980693340301514, -0.1957945078611374, -0.870916485786438, 0.20494318008422852, -0.2350122034549713, 0.5876616835594177, 0.708782434463501, -0.22029948234558105, 0.06840389966964722, -1.2430615425109863, -0.4434038996696472, 0.5913960933685303, -0.014763710089027882, 0.3692733347415924, -0.36914288997650146, -0.39912521839141846, 0.5645430684089661, 0.23556363582611084, 0.5884459614753723, -0.3390919268131256, 1.017603874206543, 0.9264762997627258, -0.10159361362457275, 0.03792782500386238, -0.07773774117231369, 0.3572116196155548, -0.36090704798698425, -1.008605718612671, 0.18107560276985168, -0.33908045291900635, 0.8358733057975769, -0.07447744160890579, 0.49562427401542664, -0.9846922755241394, -0.2542051076889038, 0.7806620001792908, 0.4735809564590454, 0.5380757451057434, -0.5319347977638245, -0.23582717776298523, -0.828196108341217, 0.41137930750846863, -0.4133762717247009, 0.15284878015518188, -0.07769496738910675, -0.8716181516647339, -0.9852851629257202, -0.7165746688842773, 0.6000282168388367, -0.7922455668449402, 0.1852070689201355, -0.24923107028007507, 0.7495094537734985, 0.018748801201581955, 0.5296218991279602, 0.6045421361923218, 0.4459218382835388, 0.15343913435935974, 0.07972587645053864, 1.0824323892593384, -0.9578385353088379, -0.5475534796714783, -0.3412747085094452, 0.6489404439926147, -0.3294476270675659, 0.31956747174263, -0.2554689645767212, -1.338351845741272, -1.0830706357955933, -0.8174053430557251, 0.06355095654726028, -0.40369802713394165, 0.09707959741353989, 0.886724591255188, 0.10052192956209183, -1.2671723365783691, 0.6605687141418457, -0.6552874445915222, 0.05245167016983032, 0.5065985918045044, 0.24298979341983795, 0.9385155439376831, 0.14815646409988403, -1.5844417810440063, 0.27032968401908875, 0.0222700797021389, -0.9700912833213806, -0.28443050384521484, -0.5457471013069153, -0.9906595945358276, 0.26920488476753235, 0.33676883578300476, -0.20002537965774536, 1.2317782640457153, 0.2776133120059967, -0.8047773241996765, 0.2619665861129761, -0.4496046304702759, -0.17114488780498505, 0.0001762805914040655, -0.3570629060268402, -0.256259948015213, 0.3625139892101288, -0.12309011071920395, 0.6658188104629517, 1.0102678537368774, -0.15249435603618622, -0.7318467497825623, 0.7372168302536011, -0.5467427372932434, 0.2829757034778595, -0.492420494556427, 0.6418894529342651, -0.3804247975349426, -0.16010397672653198, 0.2220650166273117, 0.6927105784416199, -0.1402992159128189, -0.25112465023994446, -0.10860873013734818, -0.7749431133270264, 0.7791294455528259, 0.31880858540534973, 1.404118299484253, -0.7764149904251099, -0.8923687934875488, -0.05181078985333443, 0.1156652644276619, 0.05876803398132324, -0.36379238963127136, 0.4289771318435669, -0.06362402439117432, 0.12140044569969177, 0.2759157121181488, -0.41108188033103943, 0.030403636395931244, -0.4820138216018677, -1.1033430099487305, -0.17781947553157806, -0.046285394579172134, 1.2564783096313477, -1.0954359769821167, -0.28906798362731934, -0.13564665615558624, 0.18213504552841187, -0.8626399636268616, 0.9681206941604614, -0.26168113946914673, 0.06818807870149612, -0.3828996419906616, 0.03452005609869957, 0.056301459670066833, -0.5170009732246399, 0.2676134705543518, -0.44666871428489685, -0.1895812302827835, 0.4920591711997986, -0.43258365988731384, 1.1088316440582275, 0.05711328238248825, 0.4842606782913208, -0.24164925515651703, -0.3155277371406555, 0.1474536955356598, 0.4819273352622986, 0.16019079089164734, -0.5015215873718262, 0.3900885283946991, 0.3403589427471161, -0.8286319971084595, 0.18762265145778656, 1.5312249660491943, 1.5331346988677979, -0.7609113454818726, 0.43763092160224915, 0.20031283795833588, 0.048600807785987854, 0.22497032582759857, 0.7198976278305054, 0.4486786425113678, 0.2795087993144989, 0.3003113567829132, -0.38015231490135193, 0.1588003933429718, -0.390951544046402, -0.20867787301540375, 0.9629601836204529, 0.30271172523498535, 0.7767716646194458, 0.3485265076160431, -0.7334216833114624, -0.7517336010932922, 0.1375529170036316, 0.6379061341285706, 1.3500105142593384, 0.622700572013855, -0.26669809222221375, -0.8760727643966675, -0.2545602023601532, -0.3446198105812073, -0.25603345036506653, -0.07631747424602509, -0.4795724153518677, -0.4612244665622711, -0.7772082686424255, 1.1438387632369995, 0.5092951655387878, 0.9867209792137146, -0.9083114862442017, -1.065356731414795, -0.3814949691295624, -0.19755743443965912, -0.5455045104026794, -0.5337011218070984, 0.5352685451507568, -0.21347731351852417, 0.1858997642993927, 0.06550215184688568, -0.21542400121688843, -0.04821886867284775, -0.20782119035720825, 1.1559475660324097, -0.08339262753725052, -0.2975083887577057, 0.3437952995300293, 0.13563144207000732, -0.5090495347976685, -0.5135475397109985, 0.36655324697494507, 0.2831639051437378, -0.38651242852211, 0.6132367253303528, 0.5703222751617432, -0.1737545281648636, -0.24756181240081787, -0.41818490624427795, 0.1676672250032425, 0.21812966465950012, 0.2623501121997833, 0.8987489938735962, -0.5269079208374023, 0.14732439815998077, -0.297008216381073, 0.8601334095001221, 0.2681719660758972, -0.11147532612085342, -0.07378487288951874, -0.6429880261421204, -0.35306069254875183, 0.28052178025245667, -0.636962890625, 0.041281308978796005, -1.0960582494735718, 0.3739028871059418, -0.5007645487785339, -0.2721180319786072, -0.06663598120212555, 0.5483540892601013, -0.06512505561113358, -0.047880109399557114, 1.0229085683822632, 0.14160345494747162, 0.05722879245877266, 0.43681880831718445, -0.28326159715652466, 0.8762645125389099, 0.012463241815567017, -0.26579537987709045, 0.08123452961444855, -0.17705610394477844, -0.7850936651229858, -0.06857913732528687, -0.5085337162017822, -0.6155145764350891, -0.43560320138931274, -0.006893394980579615, -0.5712718963623047, -0.9501931071281433, 0.1307489424943924, -0.8582494854927063, -0.1847076416015625, -0.062106333673000336, -0.07687156647443771, -0.7592684030532837, -0.6899037957191467, -1.2521154880523682, -0.44071871042251587, -1.1080621480941772, -1.313409686088562, 0.43583521246910095, 0.030650615692138672, -0.5034040808677673, -0.04532741382718086, -0.3151552081108093, -0.9864355325698853, 0.9944748878479004, -0.5784980654716492, 0.6827759146690369, 0.10615590214729309, -0.4365113377571106, -0.07470402121543884, 0.16432888805866241, 0.10153134912252426, -0.3898138999938965, 0.07090304791927338, -0.6899787783622742, 0.06395596265792847, -0.36219578981399536, -0.4184038043022156, 0.414932519197464, 0.2983885705471039, 0.8147535920143127, -0.02232312224805355, -0.5894232392311096, 0.07665565609931946, 1.382073163986206, -0.6283133029937744, 0.20074783265590668, 0.10444800555706024, 1.0014771223068237, -0.3644658029079437, 0.44100600481033325, 0.881722092628479, -0.03414219617843628, 0.4519420862197876, 0.3829067647457123, -0.36668694019317627, -0.20995131134986877, -0.047050975263118744, 0.4512697160243988, 1.7397245168685913, 0.44851818680763245, 0.11207626760005951, -0.8057608008384705, 0.9260251522064209, -1.2897995710372925, -0.7958744168281555, 0.4232373535633087, 0.7128806710243225, 0.2809240221977234, -0.3643619418144226, -0.08134814351797104, -0.026204712688922882, 0.6404898762702942, 0.39592692255973816, -0.4777902662754059, -0.8651701211929321, -0.027876831591129303, 0.2517453730106354, 0.3657746911048889, 0.3971062898635864, -0.5345969200134277, 0.7291262149810791, 15.069220542907715, 1.5105700492858887, -0.3021883964538574, 0.2533636689186096, 0.7846021056175232, 0.2104213386774063, -0.17013019323349, -0.11187203228473663, -1.1544007062911987, 0.18867716193199158, 1.4292876720428467, -0.06331811100244522, -0.06810684502124786, 0.24431706964969635, 0.06623376905918121, -0.2208985835313797, -0.19067075848579407, 0.5750337839126587, 0.2658443748950958, -1.4957305192947388, 0.357526570558548, 0.18414969742298126, 0.20025669038295746, 0.27811703085899353, 1.0877147912979126, 0.4588916301727295, 0.19934792816638947, -0.3687421977519989, 0.7754935622215271, 0.2100936770439148, 1.2352672815322876, -0.5882429480552673, 0.4116913378238678, 0.21085138618946075, -1.2606643438339233, 0.24151186645030975, -0.7048866748809814, -1.1185922622680664, 0.2186727225780487, 0.04863589629530907, -0.8303923606872559, -0.42245885729789734, -0.07832782715559006, 0.4232593774795532, 0.35746991634368896, 0.5151737332344055, 0.06630250811576843, 0.709895133972168, 0.04756451025605202, -0.2711530029773712, 0.6015005707740784, 0.6306042075157166, 0.22781194746494293, 0.10186129063367844, -0.1037188395857811, 0.005799468141049147, 0.229716956615448, 0.2063205987215042, -0.170480877161026, -0.42788103222846985, -0.23686309158802032, -0.006715679075568914, 0.21974171698093414, 0.8555939793586731, 0.22471953928470612, -0.1048417016863823, -0.31902745366096497, 0.42335328459739685, 0.27305856347084045, -0.13904868066310883, -0.6599870920181274, -0.4210386872291565, 0.6234529614448547, -0.6365007162094116, 0.34839382767677307, 0.7219067811965942, -0.42468321323394775, -0.4923281669616699, -0.9535810351371765, -0.3225027024745941, 0.12696044147014618, -0.5892006754875183, -0.21862393617630005, 0.9689784049987793, -0.07219081372022629, -0.8273898959159851, -0.0705515667796135, -0.1329873651266098, -0.14070871472358704, 0.3607345521450043, -1.1579443216323853, -0.5197107195854187, 0.28547921776771545, -0.4621112644672394, -0.6226404309272766, 0.41988515853881836, 1.293322205543518, -0.13090179860591888, 0.16008932888507843, 0.1321069747209549, 0.37924161553382874, -0.19329626858234406, -0.2561003565788269, -0.7365160584449768, 1.1488155126571655, 0.31441327929496765, -0.16868054866790771, -0.17219178378582, -0.025805030018091202, 0.027188794687390327, -0.6282415390014648, -0.3781834840774536, 0.387429803609848, -0.6847531795501709, -0.5752876400947571, -0.9590059518814087, -1.1403542757034302, -0.09032046794891357, 0.44348353147506714, 0.06457886844873428, 0.06297189742326736, -0.31877875328063965, 0.02355422079563141, 0.1325206160545349, -0.5833433866500854, 0.08486390113830566, 0.17313072085380554, -0.7287899255752563, 0.031906094402074814, -0.6803439259529114, 0.5583854913711548, -1.2211277484893799, -0.77091383934021, -0.09786252677440643, 0.2673812508583069, -0.46779099106788635, 1.0377172231674194, -0.09143461287021637, 0.5410111546516418, 0.9108613729476929, 0.0718691423535347, -0.6613608598709106, -0.16928374767303467, -0.7176411151885986, -0.33210983872413635, 0.1583736687898636, 0.10396314412355423, 0.1167338490486145, 0.7559550404548645, 0.45816710591316223, 0.334060937166214, -0.9063931703567505, -0.5950433015823364, 0.1370098888874054, -0.0674692690372467, -0.47238773107528687, 0.534256100654602, -0.27353617548942566, 0.3997468650341034, -0.3387623429298401, 0.2674980163574219, 0.4780214726924896, -0.11628406494855881, -0.2515377700328827, 0.106562539935112, 0.1899852156639099, 0.343630313873291, -0.48174765706062317, -0.8388621807098389, -1.5665135383605957, -0.3938748836517334, -0.8114141821861267, 0.132103830575943, -0.48677220940589905, -0.1943320482969284, -0.10314048826694489, -0.4620130658149719, 0.14269478619098663, 0.0464162640273571, 0.06835737824440002, -0.5841432809829712, -0.2829608619213104, -1.055760145187378, 0.8080233335494995, 0.8733531832695007, -0.46962085366249084, 0.059087518602609634, -0.30313584208488464, 0.22796645760536194, 0.22440919280052185, 0.568386971950531, -0.3772067129611969, -0.4380124509334564, -0.8270081877708435, -0.08353068679571152, -0.07606185972690582, -0.06621601432561874, -0.9488183856010437, 0.812464714050293, 0.6109741926193237, 0.06579535454511642, -0.6180487871170044, 0.2649767994880676, -0.381424218416214, -0.667336642742157, 0.5993986129760742, -0.8760156631469727, 0.09030511230230331, 0.3999378979206085, -0.601023256778717, -0.2875790297985077, 0.7282014489173889, -0.15024009346961975, -0.8430704474449158, -1.3051621913909912, 0.35462114214897156, -0.9237062335014343, 0.10000169277191162, -0.3720426857471466, -0.27768296003341675, -1.0749918222427368, -0.43365874886512756, -0.02145759016275406, 0.13482746481895447, 0.0020170120988041162, 0.9385793805122375, 0.5874907374382019, -1.4608747959136963, 0.07345698028802872, 0.37822291254997253, -0.13430291414260864, 0.009413758292794228, 0.5243867635726929, 0.6579900979995728, -0.2628566324710846, 0.8200951218605042, 0.11987758427858353, -0.10715164989233017, -0.9537551999092102, 0.2628631889820099, 0.17589235305786133, -0.24183861911296844, -0.23283161222934723, 0.617920458316803, -0.5906031131744385, -0.28027939796447754, 0.2282092571258545, -1.373576045036316, -0.679408609867096, -0.22167286276817322, 0.7745121121406555, 0.5094767808914185, 0.12477081269025803, -0.48101264238357544, -0.7188510894775391, 0.07801885902881622, -0.24064630270004272, -0.4467589259147644, -0.062308695167303085, -0.32059720158576965, -0.3652317523956299, 0.500067949295044, 0.7255298495292664, -0.6800254583358765, -0.6520869731903076, -0.6259148716926575, -0.37224504351615906, -0.10745607316493988, 0.5263928174972534, 0.05976531282067299, -0.22034071385860443, 0.7929090261459351, 0.9191397428512573, 0.48972809314727783, 0.18866704404354095, -0.3030591607093811, 0.6290591955184937, 0.4251483082771301, 0.40158188343048096, -0.6707964539527893, -0.44292357563972473, 1.2291216850280762, 0.595966100692749, -0.4536735415458679, 0.1582280695438385, -0.011847949586808681, -0.8071496486663818, 0.7464430928230286, 0.40773582458496094, -0.11964034289121628, 0.7441564798355103, 0.20051516592502594, 0.19145984947681427, 0.3717944324016571, -1.1195318698883057, -0.32143616676330566, 0.6188765168190002, 1.0091055631637573, 0.3106948435306549, 0.2653929591178894, 0.19561637938022614, 0.6816383004188538, 0.46917524933815, 0.06392485648393631, 0.6453317403793335, 0.9984549283981323, -0.735775351524353, 0.23354779183864594, -0.324189156293869, 0.9777683019638062, -0.8498322367668152, -0.8991405963897705, 0.4987713694572449, 0.3329312205314636, -0.11188536137342453, 0.43579286336898804, 1.4349863529205322, -0.30287277698516846, -0.08796750009059906, -0.15561990439891815, 0.3874789774417877, -0.3055686056613922, -0.2874584496021271, 0.191096231341362, -0.9025672078132629, -0.27665796875953674, -0.12258678674697876, -0.47453755140304565, -0.955538272857666, -0.5132070183753967, 0.1179005354642868, 0.5487876534461975, -0.10501456260681152, 0.6675211191177368, 1.0503877401351929, 0.5073226094245911, -0.382868230342865, -0.9694189429283142, -0.5329152941703796, -0.7191415429115295, -0.2517985999584198, -0.41407111287117004, 0.09086387604475021, -0.058786652982234955, 0.21083366870880127, -0.2136947363615036]}, "authors": [{"authorId": "2254280175", "name": "Yefei He"}, {"authorId": "2257252365", "name": "Luoming Zhang"}, {"authorId": "1739152923", "name": "Weijia Wu"}, {"authorId": "2302773646", "name": "Jing Liu"}, {"authorId": "2157472974", "name": "Hong Zhou"}, {"authorId": "3194022", "name": "Bohan Zhuang"}], "references": [{"paperId": "7a54aad06171f59149aca5380863c62729c70b41", "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM"}, {"paperId": "a0b07f40de7b307dfc40e5c569af0d14d4160e8e", "title": "Common 7B Language Models Already Possess Strong Math Capabilities"}, {"paperId": "89e0fde2dc60f8520c176a57396c6cad3af5dc40", "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization"}, {"paperId": "a3e000e0d7f64c1d094c2a8bf6f43992cbabe91b", "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"}, {"paperId": "ffdc017b1d2b493feaac9efa854882fe23d50dcf", "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models"}, {"paperId": "db633c6b1c286c0386f0078d8a2e6224e03a6227", "title": "Mistral 7B"}, {"paperId": "6c323c535365e1c7cbfd9703cbec3b5650a3346b", "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"}, {"paperId": "85b727f7bcc3523d71a91919a85247601221272d", "title": "Post Training Mixed Precision Quantization of Neural Networks using First-Order Information"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "6450356c5431eccfee22268fb7183cfa138bafee", "title": "Can ChatGPT Pass High School Exams on English Language Comprehension?"}, {"paperId": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7", "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"}, {"paperId": "7ac38c3398f2696754bec69f296468e7a8237a64", "title": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "e586a4591ba0303b769f2c07cbddaf1899cb72e4", "title": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2", "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"}, {"paperId": "d6eeb2898bd9bd34744194ef543062dda6c4531a", "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time"}, {"paperId": "936d32864240bde74f5824df3bd8343d8de08843", "title": "PTQD: Accurate Post-Training Quantization for Diffusion Models"}, {"paperId": "b45ec1cb2ba6b2d1ac24723fa836aee06a3db97a", "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "475c3014a68d545f1d2319f94fd3ab99fc3f6bec", "title": "Shortcut Learning of Large Language Models in Natural Language Understanding"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "434f4ecbfdea4496bbcd763427fc605bf11abddc", "title": "Token Dropping for Efficient BERT Pretraining"}, {"paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e", "title": "A systematic evaluation of large language models of code"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "7382e3383130298910b2d7c2fe2109e0cd13832e", "title": "Investigating Math Word Problems using Pretrained Multilingual Language Models"}, {"paperId": "6dffdeb81ebc761a8cce1e36b8d140d5b8d2a0e3", "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d", "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"}, {"paperId": "54c4642d017830e1faddbb49f0377228d2b01493", "title": "HAQ: Hardware-Aware Automated Quantization With Mixed Precision"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": null, "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"}, {"paperId": null, "title": "How long can open-source llms truly promise on context length?"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "d2a2677f6594a316101176c607ad6eae094bba44", "title": "HAWQ-V3: Dyadic Neural Network Quantization"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "How long can context length of open-source llms truly promise?"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}]}