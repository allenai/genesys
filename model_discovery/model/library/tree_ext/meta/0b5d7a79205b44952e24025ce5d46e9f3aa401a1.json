{"paperId": "0b5d7a79205b44952e24025ce5d46e9f3aa401a1", "title": "Low-Memory Neural Network Training: A Technical Report", "abstract": "Memory is increasingly often the bottleneck when training neural network models. Despite this, techniques to lower the overall memory requirements of training have been less widely studied compared to the extensive literature on reducing the memory requirements of inference. In this paper we study a fundamental question: How much memory is actually needed to train a neural network? To answer this question, we profile the overall memory usage of training on two representative deep learning benchmarks -- the WideResNet model for image classification and the DynamicConv Transformer model for machine translation -- and comprehensively evaluate four standard techniques for reducing the training memory requirements: (1) imposing sparsity on the model, (2) using low precision, (3) microbatching, and (4) gradient checkpointing. We explore how each of these techniques in isolation affects both the peak memory usage of training and the quality of the end model, and explore the memory, accuracy, and computation tradeoffs incurred when combining these techniques. Using appropriate combinations of these techniques, we show that it is possible to the reduce the memory required to train a WideResNet-28-2 on CIFAR-10 by up to 60.7x with a 0.4% loss in accuracy, and reduce the memory required to train a DynamicConv model on IWSLT'14 German to English translation by up to 8.7x with a BLEU score drop of 0.15.", "venue": "arXiv.org", "year": 2019, "citationCount": 85, "influentialCitationCount": 9, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper profiles the overall memory usage of training on two representative deep learning benchmarks and comprehensively evaluates four standard techniques for reducing the training memory requirements: imposing sparsity on the model, using low precision, microbatching, and gradient checkpointing."}, "embedding": {"model": "specter_v2", "vector": [0.39699116349220276, 0.27843669056892395, -0.2836719751358032, 0.0022100324276834726, -0.22129981219768524, 0.04465077817440033, 0.8262096047401428, -0.40671306848526, -0.9481922388076782, -0.25598204135894775, -0.019502675160765648, -0.0964941531419754, 0.7245362401008606, 0.012809786945581436, -0.12295158952474594, 0.0910724475979805, -0.6698644161224365, 0.09674662351608276, 0.2601795494556427, -0.20908856391906738, 0.006361219100654125, -0.14975355565547943, -0.9910377860069275, 0.1369187980890274, -0.04861302673816681, 1.2332746982574463, -0.12088331580162048, 0.8840535283088684, -0.17406409978866577, 0.697665810585022, 0.23982743918895721, -0.4348355829715729, 0.3006884753704071, -0.08072999119758606, -0.3941231071949005, -0.10281503200531006, 0.8005437850952148, -0.4596515893936157, -0.32057952880859375, 1.06082022190094, -0.04756055399775505, 0.37942659854888916, -0.16430066525936127, -0.6341668963432312, 0.054434288293123245, 0.17403361201286316, 0.3614143431186676, 1.3502064943313599, -0.754237949848175, -0.407250314950943, 0.7691615223884583, -1.498560905456543, 0.06248363479971886, 1.2630773782730103, 1.0320559740066528, 0.38084450364112854, -0.13039714097976685, -0.715296745300293, 0.7712790966033936, -0.029001355171203613, -0.8446933627128601, -0.7180713415145874, 0.18184782564640045, 0.06621134281158447, 1.925697922706604, -0.6796050071716309, -0.07092417776584625, 0.6325361132621765, 0.09622809290885925, 0.9369819760322571, -0.09720497578382492, -0.5255943536758423, -0.24676768481731415, 0.03606231510639191, 0.388559490442276, 0.9029667973518372, -0.1232554167509079, 0.11546109616756439, -1.16903817653656, 0.09037429094314575, 0.4979836940765381, 0.36237555742263794, 0.07218529284000397, -0.11552330106496811, -0.08632124960422516, 0.7776495814323425, 0.5480581521987915, 0.3599967062473297, -0.37457817792892456, 1.2051479816436768, 0.6708281636238098, 0.5149562358856201, 0.3010837435722351, 0.2094220519065857, -0.05166301131248474, 0.2893330752849579, -0.9911969304084778, 0.02262204699218273, -0.07842054963111877, 0.9432468414306641, -0.25220149755477905, 0.49669894576072693, -0.242116317152977, 0.41642534732818604, 0.7651881575584412, 8.42262597871013e-05, 0.5232479572296143, -0.5793017745018005, 0.5478323101997375, -1.0058023929595947, -0.2936972379684448, -0.6638173460960388, -0.13714492321014404, -0.753495454788208, -1.1874390840530396, -0.9192616939544678, -0.5134446620941162, -0.08492684364318848, -0.9703048467636108, 0.5141935348510742, -0.46641549468040466, 0.4915652573108673, 0.058575164526700974, 0.5337986350059509, 0.4881695806980133, 0.664954662322998, 0.2762395143508911, 0.3999048173427582, 0.9905502796173096, -1.2682026624679565, -0.1199406161904335, -1.283941388130188, 0.3512643575668335, -0.10562510788440704, 0.02991214022040367, -0.12356781959533691, -1.4027812480926514, -0.9422320127487183, -0.8304814100265503, 0.022778024896979332, -0.7199760675430298, 0.2551736831665039, 1.0547504425048828, 0.21726548671722412, -0.9716631174087524, 1.1483091115951538, -0.2612330913543701, 0.0024791061878204346, 0.8413048386573792, 0.5544085502624512, 0.45033591985702515, -0.3538442552089691, -0.9382838010787964, 0.5517174601554871, 0.04674685373902321, -0.35847488045692444, -0.4093892574310303, -0.5375384092330933, -0.5980550646781921, 0.2183268666267395, 0.0361173041164875, -0.7415934801101685, 1.1086190938949585, -0.5366386771202087, -0.955467939376831, 0.5160684585571289, -0.0009584507788531482, -0.19255311787128448, 0.1932574063539505, 0.04719492420554161, -0.502668559551239, 0.11082831025123596, -0.5686856508255005, 0.8121640086174011, 0.7462377548217773, 0.030879657715559006, -0.12315536290407181, 0.1352679580450058, -0.13311663269996643, -0.13701917231082916, -0.37503477931022644, 0.8448851108551025, -0.7454744577407837, -0.3326190114021301, 0.5343023538589478, 0.41188254952430725, -0.11617530882358551, -0.06191303953528404, -0.278408944606781, -0.8091927766799927, 0.9105532765388489, 0.1921028345823288, 0.5325568914413452, -0.8775630593299866, -0.8405431509017944, -0.07351130992174149, -0.07531599700450897, -0.08819906413555145, -0.779536247253418, -0.055451199412345886, -0.4792262017726898, 0.21954599022865295, 0.06808772683143616, -1.1876461505889893, -0.39441341161727905, -0.4247995615005493, -0.5987561345100403, 0.11504476517438889, 0.3548926115036011, 0.8958188891410828, -0.5382347106933594, 0.17062681913375854, -0.29812875390052795, 0.6482805609703064, -1.1836063861846924, 0.6640303730964661, -0.12352395057678223, 0.06896763294935226, 0.179107204079628, -0.07395375519990921, 0.1983495056629181, -0.7117049694061279, 0.6982066035270691, -0.8879700899124146, 0.3033515214920044, 0.5544602274894714, -0.14828212559223175, 1.236222267150879, -0.4248945116996765, 0.65133136510849, 0.03097502328455448, -0.9846518635749817, -0.057792726904153824, 0.19556593894958496, -0.3841640055179596, -0.5157755613327026, 0.5380778908729553, 0.1510074883699417, -0.6680240631103516, 0.8323505520820618, 0.6406290531158447, 0.9534637928009033, -0.26715314388275146, 0.14161795377731323, 0.8391492366790771, -0.07444725185632706, 0.37898606061935425, 0.35772618651390076, 0.23043735325336456, 0.1401546597480774, -0.01955113373696804, -0.015018176287412643, 0.20376817882061005, -0.6620339751243591, -0.055766090750694275, 0.7649434804916382, 0.6516616344451904, 0.6209293603897095, 0.6044209599494934, -0.8840837478637695, -0.9019136428833008, 0.2758382260799408, 0.578762412071228, 1.4067500829696655, -0.24007077515125275, 0.23589931428432465, -0.9160984754562378, -0.4947677552700043, -0.6435624957084656, -0.43565240502357483, -0.06580095738172531, -0.2679619789123535, -0.5930742621421814, -1.3758833408355713, 0.8396201133728027, 0.25977253913879395, 1.4206939935684204, -0.1417592167854309, -0.3536498248577118, -0.6824280023574829, 0.3690948188304901, -1.0747884511947632, -0.6711370348930359, 0.5811887979507446, -1.2576638460159302, -0.2043568640947342, -0.0552777536213398, -0.3419252038002014, 0.6384373903274536, -0.3233908414840698, 1.2335162162780762, -0.01192044373601675, -0.07686856389045715, 0.13101793825626373, 0.7480593323707581, -0.5676522254943848, -0.21409964561462402, 0.2908895015716553, 0.04041271656751633, -0.044859856367111206, 0.2570852041244507, 0.11059531569480896, -0.20615904033184052, -0.1260843276977539, -0.2131240963935852, 0.11384578049182892, 0.274779349565506, 0.2521125376224518, 0.7556107640266418, -0.17218679189682007, 0.250466525554657, -1.4171167612075806, 0.869771420955658, 0.0807899460196495, -0.5303309559822083, 0.2580345571041107, -0.831005871295929, -0.14101557433605194, 0.7849101424217224, -0.8121762871742249, -0.13051097095012665, -1.0489195585250854, -0.08627784252166748, -0.9162019491195679, 0.09363316744565964, 0.062298696488142014, 0.9063035845756531, -0.44252434372901917, 0.32567548751831055, 0.4796321392059326, 0.6220970749855042, -0.245992511510849, 0.21108338236808777, -1.1981208324432373, 0.7627074718475342, 0.33424699306488037, 0.07388762384653091, 0.09962555021047592, 0.10387983918190002, -0.5373719334602356, -0.6413881182670593, -0.11576607078313828, -0.18844296038150787, -0.19049446284770966, -0.06846095621585846, -0.822793185710907, -0.6332725882530212, 0.012458731420338154, -0.9809019565582275, -0.2429375797510147, 0.2443777173757553, 0.010987387038767338, 0.10136212408542633, -1.1211135387420654, -1.4562947750091553, -0.0863301083445549, -1.1007524728775024, -1.125788688659668, 0.06606820225715637, 0.1394759565591812, -0.1342220902442932, -0.7143096327781677, -0.5018001794815063, -0.5107115507125854, 1.4110288619995117, -0.734135091304779, 0.7547445297241211, -0.1288444995880127, -0.1977638602256775, -0.16376514732837677, -0.15191669762134552, 0.6216697692871094, -1.2028024196624756, -0.18922342360019684, -1.0993515253067017, 0.08087688684463501, -0.2793755531311035, -0.4460637867450714, 0.526011049747467, 0.3412739932537079, 0.7749956250190735, -0.00733625702559948, -0.16497214138507843, 0.8238199353218079, 1.5322895050048828, -0.9278225898742676, 0.18855556845664978, -0.08037474006414413, 0.8403078317642212, -0.3477882742881775, -0.6673311591148376, 0.48431921005249023, -0.2969556450843811, 0.10613652318716049, 0.27070295810699463, -0.25620535016059875, -0.5203361511230469, -0.413896769285202, 0.18369314074516296, 1.3883299827575684, 0.26270732283592224, 0.1967131495475769, -0.7237707376480103, 0.18102355301380157, -0.8533345460891724, -0.5422647595405579, 0.9352858662605286, 0.751996636390686, 0.2740305960178375, -0.07149142771959305, -0.4661286175251007, -0.3115350008010864, 0.4515113830566406, 0.5215831995010376, -0.3483388125896454, -0.5944007635116577, 0.07171978801488876, 0.7096582651138306, 0.8675562739372253, 0.42574262619018555, -0.3393845558166504, 0.5814368724822998, 14.68132495880127, 0.791386604309082, -0.5576662421226501, 1.0066150426864624, 0.7691665291786194, -0.19996178150177002, -0.010997925885021687, -0.29382216930389404, -1.2067478895187378, -0.13559481501579285, 1.3352923393249512, 0.5892952680587769, 0.8574101328849792, 0.6355767846107483, -0.20517978072166443, 0.12930813431739807, -0.41625067591667175, 1.5205987691879272, 0.7347268462181091, -1.9718265533447266, 0.23496322333812714, -0.016766542568802834, 0.478963166475296, 1.017789602279663, 0.8429213762283325, 0.8554508090019226, 0.28964611887931824, -0.33857792615890503, 0.4013579189777374, 0.06433708965778351, 1.2439453601837158, -0.2782299816608429, 0.6262933611869812, 0.32522621750831604, -0.6735484600067139, -0.24091100692749023, -0.798056423664093, -0.98626708984375, -0.0845634713768959, 0.2485949993133545, -0.24917744100093842, -0.7184121608734131, 0.015323600731790066, 0.5997301340103149, -0.33710968494415283, 0.27559301257133484, 0.026906874030828476, 0.49308136105537415, -0.2716931402683258, -0.024753836914896965, 0.2627471685409546, 0.2760508954524994, -0.1482681930065155, 0.10959697514772415, -0.16744168102741241, -0.20636150240898132, 0.19807586073875427, 0.2223075032234192, -1.1016300916671753, -0.3934679329395294, -0.22887423634529114, 0.02216874435544014, 0.05305780470371246, 0.7309458255767822, -0.02899615466594696, -0.21222767233848572, -0.45132890343666077, 0.6541394591331482, 0.7693684697151184, 0.30624905228614807, -0.5657936334609985, -0.03116530366241932, 0.1689896434545517, -0.736166775226593, 0.194258451461792, 0.3875032067298889, -0.4815296530723572, -0.8409107327461243, -0.7997860908508301, -0.33489081263542175, 0.20372281968593597, -0.8961122632026672, -0.32209527492523193, 1.0904686450958252, -0.4064057171344757, -0.028189701959490776, 0.5209342241287231, -0.8715153932571411, -0.20536385476589203, 0.38603946566581726, -1.4984276294708252, -0.29518425464630127, 0.16870233416557312, 0.02120913378894329, -0.28011825680732727, -0.29384365677833557, 1.2404978275299072, 0.6750832796096802, -0.39325064420700073, 0.31135693192481995, -0.18002931773662567, 0.3594396412372589, -0.5164602994918823, -0.06758906692266464, 0.8472843170166016, 0.6512839198112488, -0.15486113727092743, 0.08866855502128601, -0.3495836853981018, 0.5231156945228577, -0.8979325294494629, -0.4217670261859894, 0.758351743221283, -0.5069081783294678, -0.39835014939308167, -0.513970136642456, -0.35295480489730835, 0.5168910026550293, 0.2685852646827698, 0.22376728057861328, 0.22100239992141724, 0.27645474672317505, -0.8304396271705627, -0.42606931924819946, -0.7583953738212585, 0.09997610002756119, 0.38009053468704224, -1.1285535097122192, -0.0038746872451156378, 0.18749845027923584, 0.02146451361477375, -0.9033268690109253, -0.6541702747344971, -0.3959606885910034, -0.23855634033679962, -0.1284729540348053, 1.1220533847808838, -0.547225832939148, 0.7274392247200012, 0.8175100088119507, -0.08781788498163223, -0.550580620765686, 0.13424743711948395, -0.43998172879219055, 0.03185312822461128, -0.0728423148393631, 0.38328033685684204, -0.44742488861083984, 0.6463406085968018, 0.4222959280014038, 0.2587663233280182, -0.3951435089111328, -0.683211624622345, -0.16010682284832, -0.22789816558361053, -0.5666781663894653, 0.013665640726685524, -0.026983285322785378, -0.5271120071411133, 0.6177674531936646, 0.38466134667396545, 0.4489456117153168, -0.010463621467351913, -0.5615682601928711, 0.20194685459136963, -0.028343014419078827, -0.006534247193485498, -0.41572222113609314, -0.543075442314148, -1.6519793272018433, -0.05356266722083092, -1.155651569366455, -0.3531351685523987, -0.27266833186149597, -0.5635648369789124, -0.09878017753362656, 0.08106248825788498, 0.045048054307699203, 0.6808438897132874, 0.3947553336620331, -0.2186451405286789, -0.4039427638053894, -0.7203391790390015, 0.47066593170166016, 0.5395587682723999, -0.6885207295417786, 0.30276450514793396, -0.13945236802101135, 0.1318693906068802, 0.629478394985199, 0.4900963306427002, -0.5224571824073792, -0.9223644137382507, -1.6049044132232666, 0.36760804057121277, -0.11255490034818649, 0.29813531041145325, -1.1098986864089966, 1.0979441404342651, 0.6482611894607544, 0.10463958978652954, -0.225194051861763, 0.33686554431915283, -0.9698262214660645, -0.42373132705688477, 0.4658735990524292, -0.6382607221603394, 0.4473493695259094, 0.20023663341999054, -0.3496200740337372, -0.3507922291755676, 0.5444002151489258, 0.4763588607311249, -0.9231702089309692, -1.076635718345642, 0.4444783926010132, -0.3492947816848755, 0.2644672691822052, -0.47250887751579285, -0.06270968168973923, -1.304447054862976, -0.15115942060947418, -0.1512039601802826, 0.04222904145717621, -0.5059912800788879, 0.4696333706378937, 0.4842510223388672, -0.9656871557235718, 0.3635982871055603, 0.3231559991836548, -0.5947469472885132, 0.08635752648115158, 0.5470578670501709, 0.8825808763504028, -0.9233721494674683, 0.41252630949020386, 0.33685973286628723, 0.3362002968788147, -0.6790775060653687, 0.23148983716964722, 1.3442875146865845, -0.49809300899505615, -0.35718777775764465, 1.2911570072174072, -0.5187419652938843, -1.0540053844451904, 0.35001564025878906, -1.0384936332702637, -0.2498103231191635, -0.23916095495224, 0.7036948800086975, 0.12036940455436707, 0.5723891854286194, 0.4387994110584259, -0.7883730530738831, 0.010380175895988941, -0.020181508734822273, -0.49339669942855835, 0.40213698148727417, -0.056742701679468155, -0.28531283140182495, 0.37375879287719727, 1.2579450607299805, -0.7021085619926453, -1.0467705726623535, -1.070663332939148, -0.25808435678482056, 0.058985933661460876, 0.7225324511528015, -0.37954625487327576, -1.328525185585022, 0.9992854595184326, 0.5914556980133057, -0.18824665248394012, 0.3195900022983551, -0.5358535051345825, 0.224873885512352, 0.918073296546936, -0.03397864103317261, -0.7786086797714233, -0.5414268374443054, 1.5887131690979004, 0.9942317605018616, -0.8080182671546936, 0.5952929854393005, -0.2665308713912964, -0.515171229839325, 0.8012983202934265, 0.18320132791996002, -0.46222108602523804, 1.1912226676940918, -0.09701893478631973, -0.266668438911438, 0.09584440290927887, -0.9706334471702576, 0.031147032976150513, 0.43751758337020874, 0.5974783897399902, 0.33024856448173523, -0.3531576693058014, 0.6131529211997986, 0.7167544960975647, 0.0689588263630867, -0.03765710070729256, 0.049076005816459656, 0.37253767251968384, -0.05605215206742287, 0.49498844146728516, -0.10283156484365463, 0.5912581086158752, -1.180987000465393, -1.0897682905197144, 0.3692726194858551, 0.668993353843689, 0.16854509711265564, 0.4015830457210541, 1.3587599992752075, -0.1370154619216919, 0.5745242834091187, 0.24128741025924683, 0.6591181755065918, -0.2216194123029709, -0.5967057347297668, -0.11076285690069199, -0.24123144149780273, 0.0668472871184349, -0.16437993943691254, -0.41593196988105774, -0.06514866650104523, -0.8624982833862305, 0.4163183867931366, -0.12356008589267731, 0.5140084028244019, 0.9615753293037415, 0.6354511976242065, 0.8923071622848511, -0.33666032552719116, -0.9364023208618164, -0.5499332547187805, -0.7878943085670471, 0.2156306654214859, -0.3558740019798279, -0.38435980677604675, 0.05754219740629196, -0.315428227186203, 0.02064678817987442]}, "authors": [{"authorId": "145193121", "name": "N. Sohoni"}, {"authorId": "145284500", "name": "Christopher R. Aberger"}, {"authorId": "37866790", "name": "Megan Leszczynski"}, {"authorId": "1679327", "name": "Jian Zhang"}, {"authorId": "2114485554", "name": "C. R\u00e9"}], "references": [{"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "7738c18b70c54a4aa1730f30d026392901e8e6c9", "title": "Trained Quantization Thresholds for Accurate and Efficient Fixed-Point Inference of Deep Neural Networks"}, {"paperId": "4d7efe244661f6171116fa8e6cf91e554dd29535", "title": "Trained Uniform Quantization for Accurate and Efficient Neural Network Inference on Fixed-Point Hardware"}, {"paperId": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations"}, {"paperId": "68584a2156056718c734fa38cdc08d414cbcfc00", "title": "Buddy Compression: Enabling Larger Memory for Deep Learning and HPC Workloads on GPUs"}, {"paperId": "9842120a24cd316ef4dc1e97c266c8668a6dfa7d", "title": "moDNN: Memory Optimal Deep Neural Network Training on Graphics Processing Units"}, {"paperId": "26384278cf5d575fc32cb92c303fb648fa0d5217", "title": "The State of Sparsity in Deep Neural Networks"}, {"paperId": "8e2c65ff58b28a076883c99b96840e19b5e0b916", "title": "Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization"}, {"paperId": "3f3ecb0b178b2df70c3f1e90684895ef82a3f836", "title": "Assessment of Convolutional Neural Networks for Automated Classification of Chest Radiographs."}, {"paperId": "e5b2843d3fbb3260a3f84872230ed1437552b131", "title": "Memory-Efficient Adaptive Optimization for Large-Scale Learning"}, {"paperId": "fea820b7d953d32069e189af2961c28fd213470b", "title": "Pay Less Attention with Lightweight and Dynamic Convolutions"}, {"paperId": "96c82727dd5a80fef93007f888bb8569feb6bd85", "title": "Fixup Initialization: Residual Learning Without Normalization"}, {"paperId": "c37a1110d007a6c6a1e536527504eca5953a51f7", "title": "Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware"}, {"paperId": "3d24a29c777c52f2fde9c57e5cf2ab65bb56034b", "title": "Traditional and Heavy-Tailed Self Regularization in Neural Network Models"}, {"paperId": "54c4642d017830e1faddbb49f0377228d2b01493", "title": "HAQ: Hardware-Aware Automated Quantization With Mixed Precision"}, {"paperId": "df71a17df5350b0dbf8e5e084ae56a65cee9aaf8", "title": "HAQ: Hardware-Aware Automated Quantization"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "d4b86f40bd1b4baf99e76d95ee5912c13d4c52fc", "title": "Learning Compressed Transforms with Low Displacement Rank"}, {"paperId": "53586238026afa4264d079dff84a342fc04f5c28", "title": "Mini-batch Serialization: CNN Training with Inter-layer Data Reuse"}, {"paperId": "ba6985ee462ed8b09385a924aded5a45f74e7a59", "title": "Dynamic Sparse Graph for Efficient Deep Learning"}, {"paperId": "f0de041c8755d3058f04fe9e32ae15761d787c69", "title": "Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images"}, {"paperId": "1432654a204391b6e2ec197138be0f7c8cb83ae5", "title": "Coreset-Based Neural Network Compression"}, {"paperId": "21b786b3f870fc7fa247c143aa41de88b1fc6141", "title": "Glow: Generative Flow with Invertible 1x1 Convolutions"}, {"paperId": "1c42f8ab39e22225ffd3222baeba4863435220a0", "title": "Differentiable Learning-to-Normalize via Switchable Normalization"}, {"paperId": "12626dbe13f9211ad6f490469845154829ed39fc", "title": "Full Deep Neural Network Training On A Pruned Weight Budget"}, {"paperId": "aa0e749388e10318d9bfab43400e1f303a9e7394", "title": "Gist: Efficient Data Encoding for Deep Neural Network Training"}, {"paperId": "bf8fe437f779f2098f9af82b534aa51dc9edb06f", "title": "Scaling Neural Machine Translation"}, {"paperId": "fae2a5101789afd51c1ececb28c75537c88734ec", "title": "Scalable Methods for 8-bit Training of Neural Networks"}, {"paperId": "03cf148638e007ddb42ac49f91225712b6c66a08", "title": "Revisiting Small Batch Training for Deep Neural Networks"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "d08b35243edc5be07387a9ed218070b31e502901", "title": "Group Normalization"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "b9204dbe53402418567d785ef057e39354e1cf8a", "title": "Escort: Efficient Sparse Convolutional Neural Networks on GPUs"}, {"paperId": "d8ba516781ef654ce0d4b5f4b4871850ea1ccb5a", "title": "Escoin: Efficient Sparse Convolutional Neural Network Inference on GPUs"}, {"paperId": "11e7c4182a7813d5acf1be198c8c96d164fb95a2", "title": "i-RevNet: Deep Invertible Networks"}, {"paperId": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4", "title": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"}, {"paperId": "325093f2c5b33d7507c10aa422e96aa5b10a33f1", "title": "In-place Activated BatchNorm for Memory-Optimized Training of DNNs"}, {"paperId": "38fb1902c6a2ab4f767d4532b28a92473ea737aa", "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"}, {"paperId": "e60f693cb12132c7fffc34dc141bcc3c9dfd4961", "title": "MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks"}, {"paperId": "ccee800244908d2960830967e70ead7dd8266f7a", "title": "Deep Rewiring: Training very sparse deep networks"}, {"paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a", "title": "Automatic differentiation in PyTorch"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"paperId": "024d037d46ae933c7e12fd16af61953c7161773a", "title": "Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks"}, {"paperId": "2c031aea0d2eec0ff0e1947bb3c94dc81b46e3a5", "title": "CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-Circulant Weight Matrices"}, {"paperId": "07c83f544d0604e6bab5d741b0bf9a3621d133da", "title": "Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition"}, {"paperId": "6dbb9e4b2e3b67dc4e1634989511f67d41373dd0", "title": "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science"}, {"paperId": "3a6d4cd0768ae8768e733280d362bdb4d25924e7", "title": "The Reversible Residual Network: Backpropagation Without Storing Activations"}, {"paperId": "1fe1033a508caa43dea180f4faa135c57d931752", "title": "Plasticine: A reconfigurable architecture for parallel patterns"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "402f850dff86fb601d34b2841e6083ac0f928edd", "title": "SCNN: An accelerator for compressed-sparse convolutional neural networks"}, {"paperId": "4596eea57d200d92eb64ef999463c7b6ea309490", "title": "Optimizing and Visualizing Deep Learning for Benign/Malignant Classification in Breast Tumors"}, {"paperId": "02ed6039c45bf92926497560d0ace1d29fc9cb24", "title": "Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep Neural Networks"}, {"paperId": "8501e330d78391f4e690886a8eb8fac867704ea6", "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "af1765c5fa55a13f33bbca91a73c9e93d77b6056", "title": "Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models"}, {"paperId": "b760e1cb7dc6d7088af687f96d469937856d5a94", "title": "CNN-based Segmentation of Medical Imaging Data"}, {"paperId": "6ca52831119d81139c438962877ff50ce2140c63", "title": "The ZipML Framework for Training Models with End-to-End Low Precision: The Cans, the Cannots, and a Little Bit of Deep Learning"}, {"paperId": "d418295cd3027c43eccc5592ae5b8303ba8192be", "title": "Trained Ternary Quantization"}, {"paperId": "dd11a47d9537dac3cda94245b3aa1dc6eff36b0f", "title": "Streaming Normalization: Towards Simpler and More Biologically-plausible Normalizations for Online and Recurrent Learning"}, {"paperId": "7fcb90f68529cbfab49f471b54719ded7528d0ef", "title": "Federated Learning: Strategies for Improving Communication Efficiency"}, {"paperId": "d2e4147eecae6f914e9e1e9aece8fdd2eaed809f", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations"}, {"paperId": "7601b995303f953955004db7b9b8b206c0e02ff8", "title": "Learning Structured Sparsity in Deep Neural Networks"}, {"paperId": "fe9bc9944984cbc6f9f12e99c1065a47d662e24c", "title": "Faster CNNs with Direct Sparse Convolutions and Guided Pruning"}, {"paperId": "1c4e9156ca07705531e45960b7a919dc473abb51", "title": "Wide Residual Networks"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "592d2e65489f23ebd993dbdc0c84eda9ac8aadbe", "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size"}, {"paperId": "b5c26ab8767d046cb6e32d959fdf726aee89bb62", "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"}, {"paperId": "2e2b189f668cf2c06ebc44dc9b166648256cf457", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "bf76be8df2f2bc56edac98a5d0dfc19c85882eaa", "title": "Structured Transforms for Small-Footprint Deep Learning"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "efb5032e6199c80f83309fd866b25be9545831fd", "title": "Compressing Neural Networks with the Hashing Trick"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "5934400081d9541339da0f16d2613263f1a4c2a2", "title": "An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8", "title": "Adam: A Method for Stochastic Optimization"}, {"paperId": "27a99c21a1324f087b2f144adc119f04137dfd87", "title": "Deep Fried Convnets"}, {"paperId": "5cea23330c76994cb626df20bed31cc2588033df", "title": "Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets"}, {"paperId": "766cd91c0d8650495529cab7d4eeed482729cf89", "title": "Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c3cb27f9ef7176658f37b607e75cc2c37f5e0ea8", "title": "Accurate and Efficient 2-bit Quantized Neural Networks"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Fitting larger networks into memory"}, {"paperId": "71a916652f6ea6dbe34b4ea1cca141777d80954e", "title": "Compressing Deep Neural Networks with Probabilistic Data Structures"}, {"paperId": "9bc1c32ba191b7475b924a3e611f467bb86034bb", "title": "Speeding up ImageNet Training on Supercomputers"}, {"paperId": null, "title": "NVIDIA Apex: Tools for easy mixed-precision training in PyTorch"}, {"paperId": null, "title": "NVIDIA Tesla V100 GPU architecture"}, {"paperId": null, "title": "Ran El-Yaniv, and Yoshua Bengio"}, {"paperId": "eabf117aae614d6b65cc33a3a4bbb22b9a5014d9", "title": "GPU-Based Deep Learning Inference: A Performance and Power Analysis"}, {"paperId": null, "title": "TensorFlow: Large-scale machine learning on heterogeneous systems"}, {"paperId": "81aace0e90c6a962059b117c24db0d856f340f41", "title": "Report on the 11th IWSLT evaluation campaign"}, {"paperId": null, "title": "NVIDIA Corporation"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "8d3a318b62d2e970122da35b2a2e70a5d12cc16f", "title": "A method for solving the convex programming problem with convergence rate O(1/k^2)"}]}