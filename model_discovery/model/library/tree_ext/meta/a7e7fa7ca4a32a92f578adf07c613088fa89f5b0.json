{"paperId": "a7e7fa7ca4a32a92f578adf07c613088fa89f5b0", "title": "A Comprehensive Survey of Compression Algorithms for Language Models", "abstract": "How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.", "venue": "arXiv.org", "year": 2024, "citationCount": 4, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design, and discusses the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms."}, "embedding": {"model": "specter_v2", "vector": [-0.016383830457925797, 0.19187778234481812, -0.8178238868713379, 0.07330717891454697, -0.5129861831665039, -0.08452297747135162, 0.25636395812034607, -0.058047592639923096, -0.6007041335105896, -0.022560186684131622, 0.8698928952217102, -0.21252551674842834, -0.20882904529571533, 0.11870215833187103, -0.3462957441806793, 0.062365517020225525, -0.6710405945777893, 0.5076612830162048, -0.37471768260002136, -0.252418577671051, -0.31438612937927246, -0.5914835929870605, -1.1001168489456177, 0.0190782081335783, 0.7037891745567322, 0.5857146382331848, -0.085589699447155, 0.8182584643363953, -0.7298088669776917, 0.25606194138526917, 0.7350011467933655, -0.4852207899093628, 0.21651865541934967, -0.31291818618774414, -0.04949884116649628, -0.32577353715896606, -0.09019933640956879, -0.6832648515701294, -0.8157058954238892, 0.6976029276847839, -0.5104621052742004, 0.34694892168045044, 0.6278219223022461, -0.413446843624115, -0.115342877805233, 0.6344356536865234, 0.5385417342185974, 0.4111299514770508, -0.06289037317037582, -1.0058114528656006, 1.0849292278289795, -1.160033941268921, 0.06509272754192352, 1.541314959526062, 0.699824869632721, 0.02418363094329834, -0.2568873167037964, -0.5399249792098999, 0.018618591129779816, 0.030018450692296028, -1.3648391962051392, -0.3476582467556, -0.3170006573200226, 0.0382368378341198, 1.4860153198242188, 0.1407221257686615, -0.20366132259368896, 0.141021266579628, -0.06629074364900589, 0.9831038117408752, 0.08503599464893341, -0.7535504102706909, 0.09522456675767899, -0.0687403678894043, 0.1689596325159073, 0.8269343972206116, -0.05100303515791893, 0.3463520407676697, -0.8711421489715576, -0.5743709206581116, -0.19100317358970642, -0.08881912380456924, -0.2689555287361145, -0.10377310961484909, 0.4052106738090515, 1.0409331321716309, 0.021956777200102806, 0.420665442943573, 0.05360401049256325, 1.040573239326477, 0.5342490673065186, 0.2840866148471832, 0.3916710913181305, 0.1631484180688858, 0.0067482683807611465, 0.06457515805959702, -1.4036483764648438, 0.42426374554634094, 0.10649534314870834, 0.8856931924819946, -0.5428080558776855, 0.07610081136226654, -0.7524146437644958, 0.36623242497444153, 1.4160244464874268, 0.1274259239435196, 0.7210450172424316, -0.9238995909690857, 0.2546462416648865, -1.0302553176879883, 0.028603125363588333, -0.748569667339325, 0.1464582085609436, -0.45524317026138306, -1.0738614797592163, -1.6943340301513672, -0.9912846088409424, 0.14087127149105072, -0.7529929876327515, 0.58611661195755, -0.6381673216819763, 0.49621137976646423, 0.06334791332483292, 0.36951544880867004, 0.17662853002548218, 1.0682942867279053, -0.11850697547197342, -0.5259245038032532, 0.8431742787361145, -0.7834959626197815, -0.9568521976470947, -0.7639879584312439, 0.9811033606529236, -0.4225809574127197, 0.07079225033521652, -0.35285982489585876, -1.384188175201416, -0.6435406804084778, -0.8806793689727783, -0.21284106373786926, -0.12170720100402832, 0.31067296862602234, 0.9454535841941833, 0.4616628885269165, -1.086166262626648, 0.543617844581604, -0.31516486406326294, 0.07617836445569992, 0.3491777181625366, 0.14129538834095, 0.32101067900657654, -0.5701873302459717, -1.1155152320861816, 0.15125654637813568, 0.22626858949661255, -1.1511499881744385, 0.07331641018390656, -0.39189788699150085, -0.9099661707878113, 0.40782588720321655, 0.1294960230588913, -0.6319138407707214, 0.9308512210845947, 0.49748197197914124, -1.0767924785614014, 0.11180093884468079, -0.5129740834236145, -0.4940798282623291, 0.2479899674654007, -0.37789925932884216, -0.4792966842651367, 0.005258127581328154, -0.43829938769340515, 0.20872746407985687, 0.513913094997406, -0.2462538480758667, 0.07870011776685715, 0.6383508443832397, -0.6305438280105591, 0.021743254736065865, -0.4028471112251282, 0.9333006739616394, -0.9585863947868347, -0.8525773286819458, 0.8426192402839661, 0.32722893357276917, -0.237381249666214, 0.11385740339756012, -0.36805760860443115, -0.13345655798912048, 0.6451103091239929, -0.40143319964408875, 1.6444685459136963, -0.9186038970947266, -0.7167402505874634, 0.148038849234581, -0.30952465534210205, 0.04592093452811241, -0.8777745962142944, 0.7296740412712097, 0.04263874888420105, 0.7236471176147461, -0.662824809551239, -1.2223001718521118, 0.07828953117132187, 0.12499138712882996, -1.3016009330749512, -0.20292362570762634, -0.29817721247673035, 0.852631688117981, -0.328633189201355, 0.2327977418899536, -0.07882315665483475, 0.40921688079833984, -1.2719826698303223, 1.0007667541503906, -0.6195660829544067, -0.011850284412503242, 0.20615342259407043, -0.08709188550710678, 0.4652862548828125, -0.10955601930618286, 0.42755764722824097, 0.04676775261759758, 0.21623025834560394, 0.41567903757095337, -0.5947126150131226, 1.2310433387756348, -0.14910799264907837, 0.33912843465805054, 0.09911540150642395, -0.34269994497299194, 0.07599307596683502, 0.36666372418403625, -0.10080648213624954, -0.09102927893400192, 0.162138432264328, 0.7130475044250488, -0.6442136168479919, 0.4144134223461151, 0.6845236420631409, 0.42611241340637207, -0.3959026634693146, 0.4895138144493103, 0.34094294905662537, -0.5740018486976624, 0.8211389780044556, 0.5965200066566467, 0.5218327641487122, -0.061714597046375275, 0.3588850200176239, 0.2296493649482727, 0.399647057056427, -0.8813927173614502, -0.20445553958415985, 0.6560652256011963, 0.9199337363243103, 0.9237100481987, -0.0015380973927676678, -0.48575863242149353, -0.5902608633041382, 0.235637366771698, 0.7225383520126343, 1.104314923286438, -0.26766806840896606, -0.8141435384750366, -0.5365103483200073, -0.17704972624778748, 0.013792739249765873, 0.18642033636569977, -0.17448434233665466, -0.24906443059444427, -0.391387015581131, -1.270067572593689, 0.9404405951499939, 0.21257708966732025, 0.8754326105117798, 0.37683629989624023, 0.1664741039276123, -0.40742945671081543, 0.16177034378051758, -0.6321225762367249, -0.5767594575881958, 0.4210206866264343, -0.8365439176559448, 0.06368709355592728, -0.04475302994251251, 0.24379490315914154, -0.030186280608177185, -0.34882140159606934, 0.6829617619514465, -0.004143235273659229, 0.07218876481056213, -0.1792566478252411, 0.3506375551223755, -0.4465135633945465, -1.07991623878479, -0.02361953817307949, 0.4488706588745117, -0.5428078770637512, 0.3501585125923157, 0.24596203863620758, 0.4155929982662201, -0.20079824328422546, -0.6003583669662476, -0.09428975731134415, 0.037512779235839844, 0.06483754515647888, 0.8661882281303406, 0.1481398493051529, -0.42861756682395935, -0.8196306228637695, 1.2419031858444214, 0.1430007666349411, -0.5317505598068237, 0.3171631395816803, -0.8735776543617249, -0.003957146778702736, 0.583314061164856, -0.1820249855518341, -0.011100182309746742, -0.9411152005195618, 0.023231590166687965, -0.5853733420372009, 0.06528570502996445, 0.5871596336364746, 0.8267742991447449, 0.16413147747516632, -0.1043936237692833, 0.8682197332382202, 0.522825300693512, -0.5811293125152588, 0.781676709651947, -0.47836965322494507, 0.5096707344055176, 0.5095465183258057, 0.3894170820713043, -0.22077496349811554, 0.023935744538903236, -0.5501610636711121, -0.12842141091823578, -0.17629559338092804, 0.03004174679517746, 0.2335841804742813, -0.19841226935386658, -0.8322989344596863, 0.11372972279787064, -0.6269174218177795, -0.8659595251083374, 0.4364408254623413, 0.09227008372545242, 0.048464562743902206, -0.4558200240135193, -0.5945404171943665, -1.5512111186981201, -0.8582239151000977, -0.662979006767273, -1.0594123601913452, 0.44388440251350403, -0.4447072744369507, -0.32308313250541687, -0.12549839913845062, -0.029063807800412178, -0.2048727124929428, 0.9465049505233765, -1.0152240991592407, 1.2664352655410767, -0.12507490813732147, 0.00022812801762484014, -0.30225491523742676, 0.36928948760032654, 0.570006787776947, -0.4269348084926605, 0.412293940782547, -0.7113053798675537, -0.12285503000020981, -0.330610066652298, -0.12374117225408554, 0.6646631956100464, 0.5319643020629883, 1.0279078483581543, -0.5270589590072632, -0.6101087331771851, 0.7869755029678345, 1.1941548585891724, -0.7067824602127075, 0.046519432216882706, -0.34455934166908264, 0.5395066142082214, -0.24147319793701172, -0.30700811743736267, 0.7829867005348206, -0.30060315132141113, 0.6339511275291443, -0.15873633325099945, 0.36228814721107483, -0.11966558545827866, -0.6232315301895142, 0.40520352125167847, 2.374633550643921, 0.5247761607170105, -0.4002375602722168, -0.6561318039894104, 0.13032862544059753, -0.9590575695037842, -0.7112936973571777, 0.5352948904037476, 0.9309002161026001, 0.5911617875099182, -0.4345664083957672, -0.23446202278137207, 0.3658578097820282, 0.1307721883058548, 0.42530372738838196, -0.03766113519668579, -0.9190073013305664, -0.2909719944000244, 0.48405054211616516, 0.5097871422767639, 0.6718363165855408, -0.41671493649482727, 0.2500911355018616, 14.90438461303711, 1.2589519023895264, 0.09915908426046371, 0.8012109994888306, 0.6618422269821167, -0.22378112375736237, -0.3882436454296112, -0.19379588961601257, -0.8173484206199646, 0.45945608615875244, 1.5007665157318115, -0.010646512731909752, 0.6149436831474304, 0.13460715115070343, 0.09163164347410202, 0.3959464132785797, 0.17067046463489532, 0.7796056270599365, 0.37097904086112976, -1.583703875541687, 0.49890559911727905, 0.4657926559448242, 0.3838837742805481, 0.3357250690460205, 0.7247377634048462, 0.8047724366188049, 0.10279189795255661, -0.8105588555335999, 0.3787390887737274, 0.49215686321258545, 0.950742244720459, -0.39859557151794434, 0.5741780400276184, 0.8088231086730957, -0.7585335373878479, -0.4355320930480957, -0.7347079515457153, -1.167921543121338, 0.4556911587715149, 0.3636639714241028, -0.6152790188789368, -0.3212313950061798, -0.5794158577919006, 0.3573446273803711, 0.1471654325723648, 0.44418609142303467, 0.1830306202173233, 1.0277179479599, -0.8327351212501526, 0.1680271476507187, 0.27610906958580017, -0.04885643348097801, 0.06981932371854782, -0.09460701048374176, 0.6282451748847961, -0.1172565370798111, 0.1388062834739685, 0.01997843198478222, -0.6846922039985657, 0.11422897130250931, -0.46517321467399597, -0.504274845123291, 0.37379100918769836, -0.08443533629179001, 0.7657082676887512, 0.05129125714302063, -0.32679760456085205, 0.256099671125412, 0.2715300917625427, 0.2952164113521576, -0.3207590579986572, 0.06661226600408554, 0.4092481732368469, -0.08900899440050125, -0.07257836312055588, 0.5177971124649048, -0.40586748719215393, -0.33287984132766724, -0.5976715683937073, -0.5685972571372986, 0.4098074436187744, -0.6804678440093994, -0.49373164772987366, 0.7353392243385315, -0.08659972250461578, -0.5985681414604187, 0.10298006981611252, -0.2297619730234146, 0.4030180871486664, 0.37105000019073486, -0.9401950836181641, -0.06627542525529861, 0.6734839677810669, -0.7764632105827332, -0.5402415990829468, -0.2992412745952606, 1.4832309484481812, 0.263393759727478, -0.16120010614395142, 0.16378125548362732, 1.0940619707107544, -0.43816205859184265, -0.5026822090148926, -0.17943449318408966, 0.502798318862915, 0.3512812554836273, 0.20578451454639435, 0.5916595458984375, -0.017228569835424423, 0.0016268774634227157, -0.7235959768295288, -0.47111037373542786, 0.8352235555648804, -0.2902749478816986, -0.6399562954902649, -0.6847735643386841, -0.9776766896247864, 0.09122943133115768, 0.3023492991924286, -0.4788656532764435, 0.7465903759002686, -0.3181734085083008, -0.5096396803855896, 0.22793889045715332, -0.9188640117645264, 0.08141382038593292, 0.4128113090991974, -0.9374697208404541, 0.03778845816850662, 0.3272494673728943, 0.530138373374939, -0.7312569618225098, -0.23889046907424927, 0.23648346960544586, -0.1256079524755478, 0.008709953166544437, 1.0056755542755127, -0.4464380741119385, 0.9772400856018066, 0.6743035316467285, -0.8325048685073853, -0.3599918484687805, -0.07431039214134216, -0.845923662185669, -0.620844304561615, -0.29685309529304504, 0.600289523601532, -0.02547290548682213, 0.35896652936935425, 0.7230395078659058, 0.3759271502494812, -0.5669305324554443, -1.1805415153503418, -0.5445514917373657, 0.11643844842910767, -0.36531850695610046, 0.32325074076652527, -0.5168974995613098, -0.292271226644516, -0.2090526968240738, -0.021139860153198242, 0.24117951095104218, -0.5417150855064392, -0.7004197239875793, 0.34256434440612793, 0.11879934370517731, -0.16450855135917664, -0.6248489022254944, -0.16735512018203735, -1.6371959447860718, 0.2205270230770111, -1.2302435636520386, -0.22690580785274506, -0.7171310186386108, -0.2633203864097595, 0.20852786302566528, 0.16475780308246613, -0.5005850791931152, 0.6905783414840698, -0.04639488086104393, -0.16645540297031403, -0.2545318901538849, -0.2745875120162964, 1.169124960899353, 0.5037070512771606, -0.5640013217926025, 0.1686367690563202, -0.2460123747587204, 0.4103185534477234, 0.33359989523887634, 0.4358117878437042, -0.5756407380104065, -1.2338210344314575, -1.1813346147537231, 0.06819314509630203, -0.21249087154865265, -0.4092245399951935, -0.766880214214325, 0.4913659989833832, 0.26103881001472473, -0.284524142742157, 0.01778361201286316, 0.7277020215988159, -0.9066925048828125, -0.12913081049919128, 0.7153642773628235, -0.9777960181236267, 0.3876003623008728, 0.07167831063270569, -0.1344209462404251, -0.506759762763977, 0.4396570026874542, 0.048326265066862106, -0.6994155049324036, -0.025374744087457657, 0.29553747177124023, -0.5620457530021667, -0.0019293342484161258, -0.21984678506851196, 0.1468014121055603, -0.9696917533874512, -1.0568150281906128, 0.029237985610961914, -0.09348714351654053, -0.20495255291461945, 0.6639926433563232, 0.5688139200210571, -1.2161219120025635, -0.04273349791765213, 0.5876070261001587, -0.27101197838783264, -0.5563984513282776, 0.20388007164001465, 0.42416122555732727, -0.8303889036178589, 0.9677301645278931, 1.0102143287658691, 0.6263915300369263, -0.9475758075714111, -0.10631269961595535, 0.5075712203979492, -0.43748289346694946, -0.057151030749082565, 1.2835094928741455, -0.7565410137176514, -0.6744461059570312, 0.16910074651241302, -1.212960124015808, -0.15406499803066254, -0.3978908956050873, 0.5150474309921265, 0.22067278623580933, 0.16623589396476746, -0.3551180958747864, -0.500683605670929, -0.056150171905756, 0.24750246107578278, -0.7855855226516724, 0.36037296056747437, -0.2800139784812927, -0.576626181602478, 0.30020612478256226, 1.3095430135726929, -0.2791135907173157, -0.1837845891714096, -0.5952130556106567, -0.22694505751132965, -0.5660264492034912, 0.40804919600486755, -0.3441700339317322, -0.6123456954956055, 0.5938639640808105, 0.5835987329483032, 0.07881191372871399, 0.5153077244758606, -0.4321945905685425, 0.5207330584526062, 0.5798915028572083, 0.11480268836021423, -0.778782844543457, -0.8439781665802002, 1.451647162437439, 1.401265025138855, -0.45574286580085754, 0.9961350560188293, -0.37589865922927856, -0.5141198635101318, 0.4921708106994629, -0.42721548676490784, 0.11143612116575241, 1.278607964515686, 0.062216609716415405, -0.12197597324848175, 0.5034787058830261, -1.1374858617782593, -0.14190524816513062, 0.8438373804092407, 0.6956077218055725, 0.8535057902336121, 0.20807135105133057, -0.41570401191711426, 1.1001591682434082, -0.19089184701442719, 0.1524190455675125, 0.42138907313346863, 0.5051493048667908, -0.364473819732666, -0.1276196539402008, -0.24127213656902313, 1.0952227115631104, -0.779869019985199, -0.9452515840530396, 0.21320708096027374, 0.36740466952323914, 0.12700411677360535, 0.7189376950263977, 0.6815951466560364, -0.34789392352104187, -0.007699696347117424, 0.5937367677688599, 0.08630882948637009, -0.4970230758190155, -0.032116033136844635, 0.1915273219347, -0.4319095313549042, 0.18649175763130188, 0.08017949759960175, -0.24856513738632202, -0.1006493866443634, -0.5228235721588135, 0.48145607113838196, 0.26310208439826965, 0.28123435378074646, 0.786332368850708, 0.8972475528717041, -0.007889684289693832, -0.2353048175573349, 0.20971982181072235, -0.8520881533622742, -0.845431387424469, -0.43352624773979187, -0.555160641670227, -0.3882649540901184, -0.084734708070755, -0.22228528559207916, -0.3234803378582001]}, "authors": [{"authorId": "2108418112", "name": "Seungcheol Park"}, {"authorId": "2281832669", "name": "Jaehyeon Choi"}, {"authorId": "2281792832", "name": "Sojin Lee"}, {"authorId": "2281746333", "name": "U. Kang"}], "references": [{"paperId": "faab24bc6cd4a4dea6e82420d145f08445c05fc7", "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity"}, {"paperId": "d315ca681e95b73f2a6a6115d1e218dec9720d6f", "title": "QuantEase: Optimization-based Quantization for Language Models - An Efficient and Intuitive Algorithm"}, {"paperId": "7ac38c3398f2696754bec69f296468e7a8237a64", "title": "FineQuant: Unlocking Efficiency with Fine-Grained Weight-Only Quantization for LLMs"}, {"paperId": "d191dcb4096cfe8d0bc33f8c2905cefb92498c32", "title": "NUPES : Non-Uniform Post-Training Quantization via Power Exponent Search"}, {"paperId": "56b828717f32251a5e0f0be9c0113077f23c8429", "title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees"}, {"paperId": "aeb9454987c3f85563cf7a5d2cb7f3d502d3398d", "title": "ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "f7fb316c78dc4a4e413803daf6d4d80bb40140dd", "title": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "5fc366d7301f147883ee985cff008839abe19cc7", "title": "LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "a10843d1349fff8d2a7d9722f800802187fef67f", "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "8f48c75e1354c88a84a67abb60789083c12e5037", "title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation"}, {"paperId": "3d60a54a47b346608430344ff37935d897a14c09", "title": "Gradient-Free Structured Pruning with Unlabeled Data"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "0a6906bd6f026d3da3031c641ed03081bd0b574e", "title": "Full Stack Optimization of Transformer Inference: a Survey"}, {"paperId": "078ef926a9cdcea3f33abcd057d02d1194a8a97f", "title": "PowerQuant: Automorphism Search for Non-Uniform Quantization"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "85e959eef45114974c8f8643e88af23936fff3d1", "title": "DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation"}, {"paperId": "33be243ac9dd8723e6267dea45fd6a6172d4f6a5", "title": "Less is More: Task-aware Layer-wise Distillation for Language Model Compression"}, {"paperId": "003c08471fe579d98e82cf5c0cac03897403fb55", "title": "FP8 Quantization: The Power of the Exponent"}, {"paperId": "d151ced8700d84a2efe411a234a4cb2c595e8ca9", "title": "Language model compression with weighted low-rank factorization"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "c7d549ca504090312344a6b11274210a349d0464", "title": "Mr.BiQ: Post-Training Non-Uniform Quantization based on Minimizing the Reconstruction Error"}, {"paperId": "85167901371c71519241f8681db13fd85ba961e2", "title": "BiT: Robustly Binarized Multi-distilled Transformer"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "258d3488a9784c6829c278b31b202657b45c0655", "title": "SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression"}, {"paperId": "9e82736043eebe3f71eb86cbef6e2ac45306ece5", "title": "Structured Pruning Learns Compact and Accurate Models"}, {"paperId": "fb145e1e49d3269d8223c7710e22b45438613ff0", "title": "A Fast Post-Training Pruning Framework for Transformers"}, {"paperId": "73c722148ed4a5301dc75ae291b647a1915b8ecd", "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"}, {"paperId": "e4f82c0a13cae6739239ae0c25a554b6daff35af", "title": "Compression of Generative Pre-trained Language Models via Quantization"}, {"paperId": "08c2e7812ff224db1c877b4d14730d6288d529aa", "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression"}, {"paperId": "60bc501b98ff5bfcc464c3b1e19e5459e5f6feff", "title": "Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "ad471be93216ddbf8544721d50ee5aed14f07cae", "title": "UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning"}, {"paperId": "651114de017abc76b8e1aa2c1c30a370b463b6ee", "title": "RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation"}, {"paperId": "e095e7cdff07748f656740c9245b75ab7fd13b9c", "title": "EfficientBERT: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df", "title": "Program Synthesis with Large Language Models"}, {"paperId": "ef18db2a18ac61e72783a613328842ce86ef00bf", "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "339b2b711fb5b228d097b03ebc3e62a521779235", "title": "BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models"}, {"paperId": "066529517e46417825624f1416e200d15a6e3b64", "title": "One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers"}, {"paperId": "daab62304d0b1beeddad06846eaadce9c7610d9d", "title": "On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers"}, {"paperId": "dd0a27aa2285bc64798fa76944400ab6d9ce3025", "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"}, {"paperId": "3456c1e95d8d2f985a0701232dd55171b3cbd5e0", "title": "Lessons on Parameter Sharing across Layers in Transformers"}, {"paperId": "04e283adccf66742130bde4a4dedcda8f549dd7e", "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "9d6acac70b2d1fdb861a08b00766ef263109cd7f", "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks"}, {"paperId": "2a0ae7182b13789056e13dc1887904c923a92675", "title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "d1870f667cbd309df45a244c170d1d4ba36bac03", "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "b5b006dc558cb7fbd532d67e989173b536e8ac80", "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers"}, {"paperId": "e339c5d31ffc7029c1f72d567ac07b4606701c72", "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation"}, {"paperId": "4c3b044cc98def3defc3d562e5c0d811f7b0d200", "title": "LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding"}, {"paperId": "1013750582c20bbdf1164127b5f26b1e06e817e3", "title": "MixKD: Towards Efficient Distillation of Large-scale Language Models"}, {"paperId": "9169f9afdd42aaa60e34f57ff89c882a50e71229", "title": "An Investigation on Different Underlying Quantization Schemes for Pre-trained Language Models"}, {"paperId": "8fd0b70cfd6bbdaef9fbe1073afb3920cb61f80b", "title": "BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover\u2019s Distance"}, {"paperId": "0d5a3fd61911590e887927c39e3cedd36c9c3c8c", "title": "Why Skip If You Can Combine: A Simple Knowledge Distillation Technique for Intermediate Layers"}, {"paperId": "8fa19377b9cd6d2e9292522774c3a13108cd2ff5", "title": "Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior"}, {"paperId": "2758b4d56c87dd41431cae61bbdb52a4448f45d7", "title": "Pea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT"}, {"paperId": "0d8628b43972c85127c03bb1623dd3e044277ef5", "title": "AUBER: Automated BERT regularization"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "9baab08fbe37369856688b2abe5b3c90cce1682c", "title": "Compression of Deep Learning Models for Text: A Survey"}, {"paperId": "14216c91c7d02e58717204f04131107778a84e7b", "title": "Multi-Head Attention: Collaborate Instead of Concatenate"}, {"paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87", "title": "Linformer: Self-Attention with Linear Complexity"}, {"paperId": "4ca3b0ea12f02e2dea01a4aa505956bae5500a09", "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7", "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "1b0c8b26affd13e10ace5770e85478d60dcc368e", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"}, {"paperId": "c9d3c181d999b0e11c6e4c51b3f9aefd01489e0f", "title": "Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation"}, {"paperId": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39", "title": "Training with Quantization Noise for Extreme Model Compression"}, {"paperId": "39f8cc684f09ea2b43767f5b9590896774802759", "title": "On the effect of dropping layers of pre-trained transformer models"}, {"paperId": "3b504f939e55d567652737ef093c1087cd40689b", "title": "Analyzing Redundancy in Pretrained Transformer Models"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "1c332cfa211400fc6f56983fb01a6692046116dd", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"}, {"paperId": "e70d609ce18cd61799b087bf3a5e14c1ce70a41a", "title": "Model Compression and Hardware Acceleration for Neural Networks: A Comprehensive Survey"}, {"paperId": "738215a396f6eee1709c6b521a6199769f0ce674", "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "title": "A Simple Framework for Contrastive Learning of Visual Representations"}, {"paperId": "7fd582680ee61f6333a23bd0374f05cd6fd3dcb4", "title": "A comprehensive survey on model compression and acceleration"}, {"paperId": "94f94e8892261d0377159379ca5a166ceae19a14", "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"}, {"paperId": "54d4ff8d536b292149a4fa017c22349cf4e54ce4", "title": "AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "83b8108014e3db4f46354a28ae68193f143c4e7e", "title": "Structured Pruning of Large Language Models"}, {"paperId": "b6b2ebcd2f1b25ca87813832a60ab056c008e31d", "title": "Knowledge Distillation from Internal Representations"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "54b8fcf4cc95c0eee93910052018d6286dc78ad9", "title": "Winning the Lottery with Continuous Sparsification"}, {"paperId": "0cbf97173391b0430140117027edcaf1a37968c7", "title": "TinyBERT: Distilling BERT for Natural Language Understanding"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0", "title": "Natural Questions: A Benchmark for Question Answering Research"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "3e0b9b1e4d354bb7caef3fc90fa9828b483c6e81", "title": "Tied Transformers: Neural Machine Translation with Shared Encoder and Decoder"}, {"paperId": "3366e9eb81880d172752d4397cb8e9e6de02b935", "title": "Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "16c844fd4d97f3c6eb38b0d6527c87d184efedc3", "title": "The Evolved Transformer"}, {"paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e", "title": "Universal Transformers"}, {"paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c", "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "b79e5e4622a95417deec313cd543617b19611bea", "title": "Deep Learning using Rectified Linear Units (ReLU)"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "2ec7156913117949ab933f27f492d0149bc0031f", "title": "Learning Sparse Neural Networks through L0 Regularization"}, {"paperId": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference"}, {"paperId": "3647d6d0f151dc05626449ee09cc7bce55be497e", "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference"}, {"paperId": "13fe71da009484f240c46f14d9330e932f8de210", "title": "Long Short-Term Memory-Networks for Machine Reading"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "995c5f5e62614fcb4d2796ad2faab969da51713e", "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}, {"paperId": "62c76ca0b2790c34e85ba1cce09d47be317c7235", "title": "Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation"}, {"paperId": "a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "title": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon"}, {"paperId": "363668677c459ebc0ff494655f993a93a0251009", "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers"}, {"paperId": "5ef82a8c8aa50f99285f2143b57ca4e82da1af80", "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"}, {"paperId": "8264257f573696fc0a1ef7531c825041832197f8", "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases"}, {"paperId": "f20d99befd90863112a06caaa4e75339d9575a53", "title": "LightFormer: Light-weight Transformer Using SVD-based Weight Transfer and Parameter Sharing"}, {"paperId": "ba87303f323f09157a70f2c362c01aa34091c8b4", "title": "Tutoring Helps Students Learn Better: Improving Knowledge Distillation for BERT with Tutor Network"}, {"paperId": "cbde5598c1a78285adfcfd77fb3636f5498987a0", "title": "EBERT: Efficient BERT Inference with Dynamic Structured Pruning"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "d79b9042fe9c30027d49fdd6fe24e737b720af6b", "title": "DRONE: Data-aware Low-rank Compression for Large NLP Models"}, {"paperId": "24a099c7ac954f5795456235a245b1377f66244a", "title": "Universal-KD: Attention-based Output-Grounded Intermediate Layer Knowledge Distillation"}, {"paperId": "b60a16d2978b10ead102a6a6cd03dc940b1194cc", "title": "Compressing Pre-trained Language Models by Matrix Decomposition"}, {"paperId": null, "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "2023. SqueezeLLM: Dense-and-Sparse Quantization"}, {"paperId": null, "title": "2022. LLM.int8("}, {"paperId": null, "title": "2022. The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"}, {"paperId": null, "title": "2023. OWQ: Lessons learned from activation outliers for weight quantization in large language models"}, {"paperId": null, "title": "2022. FP8 Formats for Deep Learning"}, {"paperId": null, "title": "2022. BiBERT: Accurate Fully Binarized BERT"}, {"paperId": null, "title": "2023. How To Train Your (Compressed) Large Language Model"}]}