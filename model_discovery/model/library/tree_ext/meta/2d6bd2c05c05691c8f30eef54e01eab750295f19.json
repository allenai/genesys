{"paperId": "2d6bd2c05c05691c8f30eef54e01eab750295f19", "title": "AdaPipe: Optimizing Pipeline Parallelism with Adaptive Recomputation and Partitioning", "abstract": "Large language models (LLMs) have demonstrated powerful capabilities, requiring huge memory with their increasing sizes and sequence lengths, thus demanding larger parallel systems. The broadly adopted pipeline parallelism introduces even heavier and unbalanced memory consumption. Recomputation is a widely employed technique to mitigate the problem but introduces extra computation overhead. This paper proposes AdaPipe, which aims to find the optimized recomputation and pipeline stage partitioning strategy. AdaPipe employs adaptive recomputation to maximize memory utilization and reduce the computation cost of each pipeline stage. A flexible stage partitioning algorithm is also adopted to balance the computation between different stages. We evaluate AdaPipe by training two representative models, GPT-3 (175B) and Llama 2 (70B), achieving up to 1.32\u00d7 and 1.22\u00d7 speedup on clusters with NVIDIA GPUs and Ascend NPUs respectively.", "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This paper proposes AdaPipe, which aims to find the optimized recomputation and pipeline stage partitioning strategy, and employs adaptive recomputation to maximize memory utilization and reduce the computation cost of each pipeline stage."}, "embedding": {"model": "specter_v2", "vector": [-0.1033574715256691, 0.05997435376048088, -0.5000622272491455, -0.08164035528898239, -0.1756218820810318, -0.06815945357084274, 0.6820181608200073, -0.19170260429382324, -0.9908838272094727, -0.539006769657135, 0.43426963686943054, -0.3756723701953888, 0.474957674741745, 0.05166860297322273, -0.04126618430018425, 0.12701931595802307, -0.8316121101379395, 0.4018249213695526, 0.44899076223373413, -0.10185332596302032, 0.02761811390519142, 0.19008611142635345, -1.3051252365112305, 0.4912264943122864, 0.2815399467945099, 0.6162992119789124, -0.06201561540365219, 0.9588443040847778, -0.5676648616790771, 0.1100391298532486, 0.4254673421382904, 0.28462401032447815, 0.3716925382614136, 0.14331649243831635, -0.18839402496814728, -0.023414885625243187, 0.2285931408405304, -0.5291600227355957, -0.26744574308395386, 0.468330979347229, -0.3491775095462799, 0.7011978030204773, 0.09858416020870209, -0.7796523571014404, 0.3982199430465698, 0.4056823253631592, 0.008296533487737179, 1.0735633373260498, -0.8166828751564026, -0.6494637131690979, 0.4734344780445099, -1.9066941738128662, 0.011931318789720535, 1.053796648979187, 0.7139297723770142, -0.008424842730164528, 0.01403602585196495, -0.26795196533203125, 0.15966083109378815, 0.0038659723941236734, -1.0337306261062622, -0.677451491355896, 0.000977552612312138, -0.2603413164615631, 2.0621142387390137, 0.08349715918302536, -0.1484077423810959, -0.08421242237091064, 0.2270013839006424, 1.200373888015747, 0.2592337131500244, -0.7657511234283447, 0.24299167096614838, -0.07984232902526855, 0.8416878581047058, 0.6068962216377258, -0.1202467530965805, 0.09806204587221146, -1.1597234010696411, -0.6938506364822388, 0.31543633341789246, 0.003578664269298315, 0.48746156692504883, -0.04938877746462822, 0.07082473486661911, 0.7432866096496582, 0.10050348192453384, 0.6988100409507751, -0.43061211705207825, 0.4279153645038605, 0.35436710715293884, 0.02421675994992256, 0.3819226622581482, 0.012302566319704056, -0.17584367096424103, 0.3877633213996887, -1.254979133605957, 0.0762074887752533, 0.19161538779735565, 0.8647178411483765, -0.4366779923439026, -0.013157057575881481, -0.9082140326499939, 0.016314253211021423, 1.344308853149414, -0.21810150146484375, 0.5589008331298828, -0.3881920874118805, 0.31728821992874146, -0.5996886491775513, 0.1746976524591446, -0.4831797480583191, -0.656919002532959, -0.06072603911161423, -0.9904893636703491, -1.1365957260131836, -0.7039085030555725, -0.08930492401123047, -0.8360255360603333, 0.18354740738868713, 0.04023994505405426, 0.5854601860046387, 0.10352089256048203, 0.1721411943435669, 0.43815648555755615, 0.6306174397468567, 0.4870288670063019, -0.32057660818099976, 1.1729307174682617, -1.569737434387207, -0.29258567094802856, -0.8444613218307495, 0.5659255981445312, -0.2902389466762543, -0.27035874128341675, -0.6626337766647339, -1.124894142150879, -1.0214508771896362, -1.047682523727417, -0.15297654271125793, -0.3449840843677521, 0.34093236923217773, 0.947303056716919, 0.1336209774017334, -1.1999197006225586, 0.9041569828987122, -0.7650460600852966, -0.18016314506530762, 0.2672125995159149, 0.3002792000770569, 0.5330274701118469, 0.2554442286491394, -0.7655901312828064, -0.03641881048679352, 0.5298947095870972, -0.3232720196247101, 0.3607083857059479, -0.6997098326683044, -0.25516247749328613, 0.023368598893284798, -0.18541206419467926, -0.3987082839012146, 1.2145185470581055, -0.02893359214067459, -1.349081039428711, 0.4960693120956421, -0.14605848491191864, -0.09412112087011337, 0.2974027395248413, 0.10624894499778748, -0.4571089744567871, -0.35569286346435547, -0.6289339661598206, 0.573096752166748, 0.4776446521282196, 0.4879821538925171, -0.41844871640205383, 0.09674331545829773, -0.4954434931278229, 0.19233541190624237, -0.37902867794036865, 1.002261996269226, -0.7220088243484497, -0.27609190344810486, 0.6355825662612915, 0.346844345331192, -0.5629785060882568, 0.06299249827861786, -0.2885076105594635, -0.9527415633201599, 0.7794209718704224, -0.020089764147996902, 1.2059680223464966, -0.8508408665657043, -0.9144197702407837, -0.14917902648448944, -0.2582864761352539, 0.23933948576450348, -0.8376730680465698, 0.45693475008010864, -0.2635946273803711, 0.6009917855262756, -0.32346898317337036, -0.9693910479545593, -0.1776653528213501, 0.010427272878587246, -0.710060179233551, -0.41233932971954346, 0.15919558703899384, 1.0650440454483032, -0.45209479331970215, 0.24349454045295715, -0.4533453583717346, 0.03682873398065567, -1.1395885944366455, 1.205430507659912, -0.43142420053482056, 0.3226333260536194, -0.10516391694545746, -0.347316175699234, 0.7245048880577087, -0.42097198963165283, 0.7653117179870605, -0.45923322439193726, -0.721595287322998, 0.3354056775569916, -0.4197697639465332, 1.1350233554840088, 0.2277395874261856, 0.34083423018455505, -0.13866789638996124, -0.37724989652633667, -0.022923093289136887, 0.10199152678251266, -0.2611043453216553, -0.3475591838359833, 0.48474830389022827, 0.5733864307403564, -0.5058737397193909, 0.40766507387161255, 1.3549425601959229, 1.0867940187454224, -0.40503624081611633, 0.5995458960533142, 0.16791985929012299, -0.2969802916049957, 0.6037354469299316, 0.14927931129932404, 0.44281095266342163, 0.2165207415819168, 0.29597169160842896, -0.39016467332839966, 0.4441754221916199, -0.48102524876594543, 0.10870092362165451, 0.4666713774204254, 0.34992045164108276, 0.5014942288398743, 0.48157063126564026, -0.8097741007804871, -0.5156474709510803, 0.36273807287216187, 0.42060044407844543, 1.5338438749313354, -0.3039015531539917, 0.21397197246551514, -0.8020674586296082, -0.37791457772254944, -0.3606097400188446, -0.23802734911441803, 0.09024303406476974, 0.2647836208343506, -0.9518960118293762, -1.3609795570373535, 0.7517110705375671, 0.15937615931034088, 1.1760798692703247, -0.5837024450302124, -0.7114673852920532, -0.5869356989860535, 0.39740467071533203, -0.774914562702179, -0.5753228068351746, 0.2449035495519638, -0.9718742370605469, -0.05657476559281349, 0.1535598635673523, -0.266430526971817, 0.21682754158973694, -0.11684203892946243, 1.0105539560317993, -0.42456838488578796, -0.17073343694210052, -0.13414500653743744, 0.34300047159194946, -0.6417625546455383, -0.9937562942504883, 0.04627455398440361, 0.2041565477848053, -0.3800283372402191, 0.44273343682289124, 0.5264285206794739, 0.07548261433839798, -0.14992797374725342, -0.5621507167816162, 0.6065359711647034, -0.10205493122339249, -0.22893722355365753, 1.3878717422485352, -0.30191659927368164, -0.1644999384880066, -1.213747501373291, 1.0667455196380615, -0.166653111577034, -0.549002468585968, 0.330079585313797, -0.2803199291229248, -0.36075466871261597, 0.2708636224269867, -0.655727207660675, -0.34422793984413147, -1.2396111488342285, 0.04882100597023964, -0.4719352722167969, -0.021751347929239273, 0.17557646334171295, 0.870150089263916, -0.10143191367387772, 0.1534787267446518, 0.2722347378730774, 0.2507424056529999, -0.37309303879737854, 0.49430713057518005, -0.32616159319877625, 0.22368574142456055, 0.3532830476760864, -0.22821244597434998, -0.33792513608932495, -0.13919952511787415, -1.0011286735534668, -0.05486810579895973, -0.29632359743118286, -0.6230393648147583, 0.10781150311231613, -0.1921416074037552, -0.5830391049385071, -0.1943374127149582, 0.3383086025714874, -1.1535695791244507, -0.20703303813934326, -0.1196676641702652, -0.1414187103509903, -0.13255228102207184, -0.960519015789032, -1.5569212436676025, -0.10474567860364914, -1.2073085308074951, -0.9386193752288818, 0.26635631918907166, 0.5220463275909424, -0.06445927917957306, -0.1661273092031479, 0.0031359419226646423, -0.606673002243042, 1.1940531730651855, -0.4871039390563965, 0.9213854670524597, -0.42490026354789734, -0.12110348045825958, 0.03332699090242386, -0.16483688354492188, 0.14171993732452393, -0.35814130306243896, 0.33205169439315796, -0.7098956108093262, 0.07476658374071121, -0.09088259190320969, 0.012430715374648571, 0.1436455398797989, 0.014358270913362503, 0.9675235152244568, -0.013264392502605915, -0.6467511057853699, 0.3891827464103699, 1.1913408041000366, -0.7641799449920654, -0.278801828622818, -0.4466567039489746, 0.8508719205856323, 0.06621700525283813, -0.10176769644021988, 0.830348551273346, 0.017872821539640427, 0.37954628467559814, -0.21318359673023224, -0.06284656375646591, -0.07704188674688339, -0.16465848684310913, -0.19039517641067505, 2.3306570053100586, 0.8398205041885376, -0.07964088767766953, -0.9113937020301819, 0.15561139583587646, -1.1833521127700806, -0.2014981508255005, 0.4053618609905243, 0.8418688178062439, 0.5309075117111206, -0.6181932687759399, -0.44934943318367004, -0.45369967818260193, 0.15143315494060516, 0.4652117192745209, -0.6962151527404785, -1.0313187837600708, 0.14116890728473663, 0.6854335069656372, 0.14538942277431488, 0.2442503273487091, -0.34936705231666565, 0.5589035153388977, 14.883056640625, 0.8167446255683899, -0.13844774663448334, 0.7966526746749878, 1.1473784446716309, 0.34673449397087097, -0.3300311267375946, -0.2944154143333435, -1.195621371269226, 0.1395348161458969, 1.7603613138198853, 0.0393894761800766, -0.0632571130990982, 0.0036186277866363525, -0.020323164761066437, -0.06060444191098213, -0.3876121938228607, 0.4006151258945465, 0.34752732515335083, -1.606507658958435, 0.3645513653755188, -0.020408732816576958, 0.5654940009117126, 1.0167591571807861, 0.4359094798564911, 0.688718855381012, 0.12195797264575958, -0.11774523556232452, 0.18026778101921082, 0.35578644275665283, 0.6641841530799866, -0.4763230085372925, 0.46950387954711914, 0.773662805557251, -0.7772321701049805, 0.2587524354457855, -0.7798435688018799, -0.9937808513641357, 0.09471703320741653, 0.08428363502025604, -0.4178778827190399, -0.5227776169776917, -0.3347121775150299, 0.2561260461807251, 0.15989159047603607, 0.4062276780605316, -0.0025514336302876472, 0.39884066581726074, -0.474831759929657, -0.014248641207814217, -0.025187069550156593, 0.12090594321489334, 0.052345432341098785, -0.045515600591897964, 0.011999007314443588, 0.390520840883255, 0.21509112417697906, 0.3019182085990906, -0.5778799057006836, 0.007644125260412693, -0.04252953082323074, -0.6240199208259583, 0.28416261076927185, 0.8906657695770264, 0.11415747553110123, 0.17736966907978058, -0.41131484508514404, 0.544849693775177, 0.5680011510848999, -0.008983350358903408, -0.4190233051776886, 0.29371020197868347, 0.3963598906993866, -0.660591185092926, -0.10870623588562012, 0.02858065813779831, -0.2431458979845047, -0.33570122718811035, -0.9377104640007019, -0.26743656396865845, 0.09136059880256653, -0.5177194476127625, -0.5478024482727051, 0.9676676392555237, -0.06180105358362198, -0.41460222005844116, 0.2753903865814209, -0.782188892364502, -0.45331043004989624, 0.6719861030578613, -0.8822574615478516, -0.5768844485282898, 0.538722038269043, -0.4830925762653351, -0.19006383419036865, 0.11869405210018158, 1.3716682195663452, 0.06177080050110817, -0.7176406383514404, 0.1335815191268921, 0.12479498237371445, -0.38508373498916626, -0.5021446943283081, -0.12290699779987335, 1.3420864343643188, 0.5535600781440735, -0.012203933671116829, 0.03848322480916977, -0.3976652920246124, 0.2611614167690277, -0.9985411763191223, -0.2756670415401459, 0.6831235885620117, -0.3454304039478302, -0.2240697294473648, -0.5083790421485901, -1.0063848495483398, 0.24613577127456665, 0.43940556049346924, -0.11585897207260132, 0.8201175928115845, -0.340189129114151, -0.10757438093423843, 0.257564902305603, -0.8091646432876587, 0.437956303358078, 0.43301835656166077, -0.7808950543403625, -0.15028326213359833, 0.12500600516796112, 0.7433492541313171, -1.406764030456543, -0.8835706114768982, 0.057281967252492905, -0.01150634977966547, -0.18694207072257996, 0.8715147972106934, -0.3049921989440918, 0.5773943066596985, 0.9224202632904053, -0.20093555748462677, -0.1286921501159668, 0.2772577404975891, -0.7304399013519287, -0.1916913092136383, -0.2985994219779968, 0.600805401802063, -0.38313722610473633, 0.42552345991134644, 1.1596564054489136, 0.2541545033454895, -0.7068069577217102, -0.6450434327125549, 0.29564717411994934, 0.18306885659694672, -0.43637341260910034, 0.40674734115600586, -0.09820443391799927, -0.13178279995918274, -0.02761024236679077, 0.5228991508483887, 0.6443667411804199, -0.23065772652626038, -0.18116915225982666, 0.4709596335887909, -0.068043053150177, -0.41983309388160706, -0.5825372338294983, 0.03267931565642357, -1.4818408489227295, -0.3982904553413391, -0.856697678565979, 0.045638661831617355, -0.2531746029853821, -0.25720006227493286, -0.23856312036514282, -0.1368686705827713, 0.16440056264400482, 0.42300182580947876, -0.07169874757528305, -0.5909475684165955, -0.023598557338118553, -0.3467048108577728, 0.7309536337852478, 0.7883869409561157, -0.2784966826438904, -0.2900991439819336, -0.10290452092885971, 0.49534884095191956, 0.5627989172935486, 0.7298383712768555, -0.11609183996915817, -0.7815600633621216, -1.2367863655090332, -0.012594719417393208, -0.08648096024990082, -0.6410335898399353, -0.6764506697654724, 1.131771445274353, 0.2824782431125641, -0.2071726769208908, -0.387490838766098, 0.07658637315034866, -0.5191003680229187, -0.213548943400383, 0.5822461843490601, -0.2962314188480377, 0.49418017268180847, 1.0554043054580688, -0.5770201683044434, -0.11906705051660538, 0.45162492990493774, -0.08915335685014725, -0.5376591682434082, -1.452602505683899, 0.731498658657074, -0.7919251918792725, 0.07920870929956436, -0.10287696123123169, 0.09087523072957993, -1.0472240447998047, 0.6418101787567139, 0.07694629579782486, 0.5035317540168762, -0.31254908442497253, 0.9955239295959473, 0.164903923869133, -1.1236907243728638, 0.1277095377445221, 0.885918378829956, -0.4865860342979431, 0.19321446120738983, 0.7774510979652405, 0.8784392476081848, -0.9730689525604248, 0.5905852913856506, 0.18510425090789795, 0.2023194283246994, -1.1739109754562378, -0.17924143373966217, 0.6174353957176208, -0.8788573145866394, 0.008567479439079762, 1.3446629047393799, -0.5619127154350281, -1.0842194557189941, -0.44609513878822327, -0.981706976890564, -0.6818930506706238, -0.18702051043510437, 0.9155236482620239, 0.438029944896698, 0.054121166467666626, -0.0581650547683239, -0.5773167610168457, -0.09153210371732712, -0.14196757972240448, -0.2971339821815491, 0.13243208825588226, -0.006618523970246315, -0.8314038515090942, 0.5330685973167419, 0.5224676132202148, -0.5829866528511047, -0.8032249808311462, -0.9542720913887024, 0.02273816615343094, -0.024361057206988335, 0.15253552794456482, -0.04420009255409241, -0.25736501812934875, 0.5338525772094727, 0.5059270262718201, -0.058316972106695175, 0.42173677682876587, -0.5788331627845764, 0.3006323575973511, 0.46296748518943787, 0.5697111487388611, -0.5620540380477905, -0.4790380597114563, 1.7372671365737915, 1.3368935585021973, -0.6993858814239502, 0.6003348231315613, -0.16495877504348755, -0.505440354347229, 0.8993982672691345, 0.24796389043331146, -0.17106123268604279, 1.4168130159378052, 0.11010203510522842, -0.18197792768478394, 0.456671804189682, -0.9157751202583313, 0.10954150557518005, 0.7488816380500793, 0.5447878837585449, 0.7311252355575562, 0.5700603127479553, -0.5372998714447021, 0.9198375344276428, 0.40018898248672485, 0.04288322106003761, 0.3623962998390198, 0.8502340316772461, -0.1808595210313797, -0.4926699101924896, 0.25052541494369507, 0.5745881199836731, -0.41085734963417053, -0.7674633264541626, 0.35166826844215393, 0.6644776463508606, 0.0495503731071949, 0.6708969473838806, 1.492738127708435, -0.13166005909442902, 0.32697588205337524, 0.13151094317436218, 0.3028889298439026, -0.7561598420143127, -0.4295276403427124, -0.11154521256685257, -0.6083505153656006, 0.09182022511959076, -0.176736980676651, -0.07159080356359482, -0.6678191423416138, -0.6691179871559143, 0.40375879406929016, 0.04662908986210823, 0.4378199875354767, 0.9085791707038879, 1.1545149087905884, 0.6414915919303894, -0.12212385982275009, -0.7651540637016296, -0.6268423199653625, -1.0385628938674927, -0.0010054395534098148, -0.7518133521080017, -0.8792669773101807, 0.6138824820518494, 0.3983123302459717, -0.7669879198074341]}, "authors": [{"authorId": "113907105", "name": "Zhenbo Sun"}, {"authorId": "47709883", "name": "Huanqi Cao"}, {"authorId": "2146016408", "name": "Yuanwei Wang"}, {"authorId": "151384117", "name": "Guanyu Feng"}, {"authorId": "2118529462", "name": "Shengqi Chen"}, {"authorId": "2262527866", "name": "Haojie Wang"}, {"authorId": "2262468726", "name": "Wenguang Chen"}], "references": [{"paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95", "title": "Scaling Vision Transformers to 22 Billion Parameters"}, {"paperId": "43cefce076df7ee54505fd78a8a97129c0f6d36b", "title": "MPress: Democratizing Billion-Scale Model Training on Multi-GPU Servers via Memory-Saving Inter-Operator Parallelism"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "f12515166c1ccc3301746fd00ffbbd1852d09ad6", "title": "vPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training"}, {"paperId": "10f3ca78e194552427ebe9173b19d1b910469e27", "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines"}, {"paperId": "feba0c47bf12a02c3a725174bb53df78658a72a8", "title": "Pre-Trained Models: Past, Present and Future"}, {"paperId": "12b71736392209b4292471b7da0aed71ba2aa545", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "21a4cd35f19cfe8df1065b066b16edd048d2535d", "title": "DAPPLE: a pipelined data parallel approach for training large models"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "e7dd73e84efa11f0b0bc7654c93f7fc19058f5d0", "title": "Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "c591ffe721883f0360c1611d3c8c43dfa6e96c60", "title": "Optimal checkpointing for heterogeneous chains: how to train deep neural networks with limited memory"}, {"paperId": "00c957711b12468cb38424caccdf5291bb354033", "title": "ZeRO: Memory optimizations Toward Training Trillion Parameter Models"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "d79a26226393f687ddbc375e32055b40b8ad8d38", "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"}, {"paperId": "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f", "title": "Mesh-TensorFlow: Deep Learning for Supercomputers"}, {"paperId": "7dd7198bb8a61dd22879068e8c4619b32f0470f8", "title": "Supporting Very Large Models using Automatic Dataflow Graph Partitioning"}, {"paperId": "f971658ab845d7573c4bbb760d5e7e5332025254", "title": "Beyond Data and Model Parallelism for Deep Neural Networks"}, {"paperId": "c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd", "title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training"}, {"paperId": "3ea088eae8637530d1108065acab244f3b6c280d", "title": "Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks"}, {"paperId": "ca45e17cf41cf1fd0aa7c9536f0a27bc0f4d3b33", "title": "Superneurons: dynamic GPU memory management for training deep neural networks"}, {"paperId": "942deb7d865b7782c03176d95e3a0d56cb71009e", "title": "Training Deep Nets with Sublinear Memory Cost"}, {"paperId": "b1ee2e7040c396e2002022f876abc6dec61aa501", "title": "vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design"}, {"paperId": "41f3cbb3b2da10c945640a3ed87a6bd58f180abe", "title": "Scaling Distributed Machine Learning with the Parameter Server"}, {"paperId": "80d800dfadbe2e6c7b2367d9229cc82912d55889", "title": "One weird trick for parallelizing convolutional neural networks"}, {"paperId": "aa6ddad0a84eaa004e49142981d05c5f36cc585e", "title": "Hanayo: Harnessing Wave-Like Pipeline Parallelism for Enhanced Large Model Training Efficiency"}, {"paperId": "283e56e843edc2ab599b0cf8218b936a89a11875", "title": "BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models"}, {"paperId": "da37fb0a9071d1ceda0c7e359a049c6c7e627a01", "title": "Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization"}, {"paperId": "ca1402619a80c140c650961d899319c2928744f0", "title": "Piper: Multidimensional Planner for DNN Parallelization"}, {"paperId": "e19b81683e9c9cae0bf0935e96b10e8bbc09d1e1", "title": "Efficient Combination of Rematerialization and Offloading for Training DNNs"}, {"paperId": null, "title": "Zero-infinity:breakingtheGPUmemorywallforex-treme scale deep learning"}, {"paperId": null, "title": "Efficient large-scale language model trainingonGPUclustersusingmegatron-lm"}, {"paperId": "08588107b9e37f9601bb5c801aa46b918cc3c8ec", "title": "A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": null, "title": "Partitioning ASPLOS \u201924, April 27-May 1, 2024"}, {"paperId": null, "title": "Llama2:Openfoundationandfine-tunedchatmodels"}, {"paperId": null, "title": "Reducing activation recomputation in large"}, {"paperId": "1d27a56a8133f947a5a0217b00241d26f585f834", "title": "This paper is included in the Proceedings of the 16th USENIX Symposium on Operating Systems Design and Implementation."}, {"paperId": null, "title": "Huawei mindspore ai development framework"}]}