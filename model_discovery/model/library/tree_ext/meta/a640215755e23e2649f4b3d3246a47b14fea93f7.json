{"paperId": "a640215755e23e2649f4b3d3246a47b14fea93f7", "title": "Getting the most out of your tokenizer for pre-training and domain adaptation", "abstract": "Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to a wide range of use-cases. We find that when fine-tuning on more than 50 billion tokens, we can specialize the tokenizer of a pre-trained LLM to obtain large gains in generation speed and effective context size.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "It is shown that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance."}, "embedding": {"model": "specter_v2", "vector": [0.06613177806138992, 0.4803381562232971, -0.7034948468208313, 0.2898024916648865, -0.31137269735336304, -0.49600648880004883, 0.48001328110694885, 0.04462685063481331, -0.15028955042362213, -0.14116719365119934, 0.20376209914684296, -0.37281879782676697, 0.8056080341339111, 0.07043991982936859, -0.6669654846191406, 0.08210095763206482, -0.6855138540267944, -0.06273326277732849, -0.24761226773262024, -0.19550585746765137, 0.08225137740373611, -0.9877499938011169, -1.1070042848587036, 0.36268070340156555, 1.0052857398986816, 0.17726433277130127, -0.0925857275724411, 0.8745635747909546, -0.7712457776069641, 0.4256359934806824, -0.10280521959066391, -0.4914849102497101, 0.29765066504478455, 0.05115969479084015, -0.6169424653053284, -0.184207484126091, 0.23121264576911926, -0.0981883630156517, 0.25671106576919556, 0.48076343536376953, 0.13856647908687592, -0.11052215844392776, 0.05766032636165619, -0.9030486941337585, -0.37193164229393005, 1.3070645332336426, 0.20729759335517883, 0.3363802134990692, -0.24995450675487518, -0.3690142333507538, 0.9086633324623108, -0.9696296453475952, 0.3024147152900696, 0.9084411263465881, 0.6623152494430542, 0.6898959875106812, -0.3672805726528168, -0.038365576416254044, 0.2469220757484436, -0.32214900851249695, -0.8418210744857788, -0.516302764415741, -0.2437317818403244, -0.32711923122406006, 2.29126238822937, -0.28026196360588074, 0.05643773823976517, 0.5065513253211975, 0.0060081686824560165, 0.9824817180633545, -0.37230947613716125, -0.757947564125061, -0.08729897439479828, 0.08480515331029892, -0.16552984714508057, 0.8164569139480591, -0.0975087359547615, -0.1286284178495407, -0.6833726167678833, -0.19783133268356323, 0.3756222724914551, -0.4231306314468384, 0.3655158579349518, -0.24712616205215454, -0.3676442503929138, 0.21841205656528473, -0.010679284110665321, 1.1485668420791626, -0.12781502306461334, 1.0183271169662476, 0.9168352484703064, -0.008532827720046043, 0.039592087268829346, 0.8073161244392395, -0.14816242456436157, -0.266184538602829, -1.1565649509429932, 0.28335148096084595, -0.04693460464477539, 0.9263071417808533, 0.1276935487985611, 0.5716819167137146, -1.1363953351974487, 0.24359744787216187, 0.9140076637268066, 0.05691448971629143, 0.8003399968147278, -0.5539894104003906, 0.5334996581077576, -0.5696368217468262, 0.30638960003852844, 0.22288215160369873, 0.305929958820343, -0.021513856947422028, -0.3958709239959717, -1.0162943601608276, -0.7352102994918823, -0.37442612648010254, -1.0117980241775513, 0.6487302780151367, -0.010520443320274353, 0.3038794994354248, 0.3149589002132416, 0.44180402159690857, 0.6231381893157959, 0.3488875925540924, 0.27909529209136963, 0.09062079340219498, 0.5825340747833252, -0.9310762286186218, -0.24413911998271942, -0.9240860342979431, 0.7560311555862427, -0.7391055226325989, 0.24394141137599945, -0.5371019244194031, -1.7692625522613525, -1.0430231094360352, -0.9968715310096741, 0.03725980594754219, -0.471933513879776, 0.2803063690662384, 1.1912873983383179, 0.7642077803611755, -1.0190362930297852, 0.903536319732666, -0.3209589123725891, -0.08797837048768997, 0.08926450461149216, -0.004375032149255276, 0.4152548909187317, -0.08433984220027924, -1.2532800436019897, -0.0452536940574646, 0.25730201601982117, -0.9447667002677917, -0.22700868546962738, -0.9551811814308167, -1.2779741287231445, -0.24157074093818665, 0.11925695091485977, -0.5981611013412476, 1.578048825263977, -0.1913088709115982, -1.2421865463256836, 0.7767054438591003, -0.37734419107437134, 0.07188167423009872, 0.39939650893211365, 0.09078023582696915, -0.3516904413700104, -0.7796394228935242, 0.0733913704752922, 0.3444364666938782, 0.873668372631073, -0.3973862826824188, -0.1573217511177063, 0.765845537185669, 0.015156129375100136, -0.1330171525478363, -0.09898032248020172, 0.7746415734291077, -0.36791345477104187, -0.017467884346842766, -0.09600859880447388, 0.6618534326553345, 0.047234874218702316, -1.0248515605926514, -0.6728764772415161, -0.709054708480835, 0.4865012764930725, -0.026048190891742706, 1.1114941835403442, -0.9070520401000977, -0.8708530068397522, -0.2733413875102997, -0.4081633388996124, 0.1896694153547287, -0.5586683750152588, 0.773945152759552, -0.6514838337898254, 0.8183460235595703, -0.23918840289115906, -1.0232362747192383, 0.47060930728912354, -0.5931416749954224, -0.5069126486778259, -0.39894071221351624, 0.16556765139102936, 0.9233805537223816, -0.5903262495994568, 0.0051986477337777615, -0.2303347885608673, 0.3696974813938141, -1.102526307106018, 1.1305562257766724, -0.32739219069480896, -0.08149789273738861, 0.16741466522216797, -0.07212672382593155, 0.2560978829860687, -0.049048177897930145, 0.22075456380844116, -0.29422271251678467, -0.4199543297290802, 0.5682400465011597, -0.41945281624794006, 1.7690178155899048, -0.7225278615951538, 0.05304533615708351, -0.03276558592915535, -0.41255488991737366, 0.1842295378446579, 0.6571896076202393, 0.12199461460113525, -0.1956387609243393, -0.08041943609714508, 0.5313942432403564, -0.5141677260398865, -0.04515668377280235, 1.1553410291671753, 0.7924985885620117, -0.4315919578075409, 0.451665997505188, 0.41656607389450073, -0.42530250549316406, 0.9633592963218689, 0.3179709017276764, 0.9859395027160645, 0.44779640436172485, 0.2275041788816452, 0.2912295162677765, 0.318631649017334, -0.6444715857505798, 0.2519966959953308, 0.40985575318336487, 0.7345691323280334, 0.9313865900039673, 0.5404618382453918, -0.5028564929962158, -0.6883051991462708, -0.1536407321691513, 0.7520840764045715, 1.442139983177185, -0.058713171631097794, -0.40568435192108154, -1.1622776985168457, -0.6996974349021912, -0.030087994411587715, 0.49681708216667175, -0.252263605594635, -0.2067757248878479, -0.7391564249992371, -1.0657095909118652, 0.9896708130836487, 0.14690344035625458, 1.2051960229873657, -0.7416383028030396, -0.5769293904304504, -0.2948719263076782, 0.03830905258655548, -0.6731399893760681, -0.8329196572303772, 0.10468486696481705, -0.17552053928375244, 0.15723814070224762, 0.050319183617830276, -0.22328050434589386, 0.2978610396385193, -0.5746802091598511, 1.1353399753570557, -0.00391218438744545, -0.7541549801826477, 0.09634795784950256, 0.215670645236969, -0.30775272846221924, -1.186116337776184, 0.506540834903717, -0.2785934507846832, -0.5931355357170105, 0.004073173273354769, 0.3183358311653137, -0.03662929683923721, 0.05533091351389885, -0.5689997673034668, -0.03326449543237686, 0.1597641557455063, -0.08356741070747375, 0.43130356073379517, -0.18022724986076355, -0.462007075548172, -1.2636362314224243, 0.967190146446228, 0.1428980529308319, -0.23611855506896973, 0.4096701443195343, -0.8109860420227051, -0.2802489697933197, 0.5923054218292236, -0.48807477951049805, -0.5545153617858887, -1.3177660703659058, 0.19906076788902283, 0.16363584995269775, -0.1722281128168106, 0.001852178480476141, 0.12829719483852386, 0.024473290890455246, 0.2297459989786148, 0.48158732056617737, -0.11023034155368805, -0.027208415791392326, 0.7742132544517517, -1.0919657945632935, 0.19231411814689636, -0.22213594615459442, 0.396416574716568, -0.37387409806251526, -0.6273930668830872, -0.11728962510824203, 0.17471688985824585, 0.10080110281705856, 0.02661561593413353, -0.21429811418056488, 0.2420680820941925, -0.5116370916366577, -0.8140767216682434, 0.06871791183948517, -1.3526878356933594, -0.5766340494155884, -0.15372517704963684, -0.6020240783691406, -0.14292138814926147, -0.8045907020568848, -0.9606701135635376, -0.2837662696838379, -0.6992812752723694, -1.247567057609558, 0.4765256345272064, -0.022255366668105125, -0.494354248046875, -0.46315768361091614, 0.307856023311615, -0.21141774952411652, 1.014662265777588, -0.5761781334877014, 0.7945113182067871, 0.4258354902267456, -0.12491248548030853, 0.1254940629005432, 0.2834416329860687, 0.5116735696792603, -0.23542283475399017, 0.769025444984436, -0.7074578404426575, 0.003029530169442296, -0.520757257938385, -0.3583139181137085, 0.18332785367965698, 0.21396060287952423, 0.6203370690345764, 0.06084303557872772, -0.5567486882209778, 0.7735346555709839, 1.267301082611084, -0.7378427982330322, 0.3164682686328888, 0.5560039281845093, 1.1862781047821045, 0.013163655065000057, -0.18851186335086823, 0.8744121789932251, 0.1949797123670578, 0.34093862771987915, 0.18136045336723328, -0.35106244683265686, -0.13876087963581085, -0.48156753182411194, 0.9544338583946228, 1.3914058208465576, 0.03666124492883682, -0.1199931725859642, -1.139561414718628, 0.8410742282867432, -1.2668406963348389, -0.7307704091072083, 0.728584349155426, 0.8006945252418518, 0.7580738663673401, -0.5031701326370239, -0.27760669589042664, -0.19315063953399658, 0.7250580191612244, 0.40665340423583984, -0.4660196006298065, -1.4498008489608765, 0.3030174970626831, 0.5029044151306152, 0.36272403597831726, 0.2367166429758072, -0.2243431806564331, 0.7082990407943726, 14.697117805480957, 0.914813756942749, -0.05988554656505585, 0.5894017815589905, 0.5711210370063782, 0.07905363291501999, -0.6526736617088318, -0.13621267676353455, -1.203078031539917, 0.08939012140035629, 1.0408424139022827, -0.20564232766628265, 0.8248096704483032, 0.3948030173778534, -0.02174619399011135, -0.06208226457238197, -0.38686907291412354, 0.35213637351989746, 0.36093926429748535, -1.4175407886505127, 0.22331055998802185, 0.13075953722000122, 0.910975992679596, 0.13109682500362396, 1.0310075283050537, 1.0167421102523804, 0.40450701117515564, -0.4341413080692291, 0.7424542903900146, -0.19899941980838776, 1.1639513969421387, -0.46559134125709534, 0.5762007236480713, 0.11219637840986252, -0.8973790407180786, 0.028446080163121223, -0.6383983492851257, -0.9964668154716492, 0.2562660872936249, 0.46380871534347534, -1.3151413202285767, -0.18673980236053467, -0.22035843133926392, 0.5296062231063843, 0.10567425191402435, 0.26146212220191956, -0.37835046648979187, 0.7053222060203552, 0.42290812730789185, 0.4504590630531311, -0.03157569095492363, 0.7258514761924744, 0.08634795248508453, 0.34140393137931824, 0.3146275579929352, -0.5488632917404175, -0.06217125430703163, 0.6509871482849121, -0.6279906630516052, -0.09837809205055237, -0.07335790991783142, -0.34859174489974976, 0.12309303879737854, 0.9812197685241699, 0.246557354927063, 0.5039776563644409, -0.27570247650146484, 0.5469368696212769, 0.6249265670776367, 0.1456855982542038, -0.8596209287643433, -0.3203308880329132, 0.8019620180130005, -0.23349015414714813, 0.28638404607772827, 0.33961591124534607, -0.257906436920166, -0.34142857789993286, -0.4607051610946655, -0.46992042660713196, -0.02908223681151867, -0.28846609592437744, -0.3503352403640747, 0.660284698009491, -0.25116387009620667, -0.5594176054000854, -0.20826631784439087, -0.37263166904449463, -0.7900726199150085, 0.41485291719436646, -1.3808190822601318, -0.7613822817802429, 0.515917181968689, -0.4360203742980957, -0.8313186764717102, -0.17984046041965485, 1.1279547214508057, 0.058915071189403534, -0.49202707409858704, 0.6096590757369995, 0.3576575219631195, 0.06474220007658005, 0.4042050838470459, -1.1445884704589844, 1.6220757961273193, 0.5342151522636414, -0.41080835461616516, 0.5412710309028625, 0.23272033035755157, -0.03661585599184036, -0.8670616149902344, -0.355844646692276, 0.8144802451133728, -0.7162227630615234, -0.6071733236312866, -0.7815964221954346, -0.5585155487060547, 0.12578395009040833, 0.9162151217460632, -0.3043158948421478, 0.5049394369125366, 0.20505422353744507, -0.595120370388031, -0.37737563252449036, -1.2398059368133545, -0.06615334749221802, 0.6056036353111267, -0.6181918382644653, 0.13198913633823395, -0.06299994140863419, 0.6613577604293823, -1.411752700805664, -0.7238188982009888, -0.36657994985580444, -0.0011694581480696797, -0.6984639763832092, 0.8615520000457764, -0.008565028198063374, 1.025922417640686, 0.7200775146484375, 0.43831250071525574, -1.2892509698867798, 0.0310360174626112, -1.1753087043762207, 0.46617892384529114, 0.9715789556503296, 0.9077681303024292, -0.4953846335411072, 0.3785538971424103, 1.1339517831802368, 0.14846542477607727, -0.2659502327442169, -0.46569618582725525, -0.32542338967323303, 0.12252788990736008, -0.42038965225219727, 1.102040410041809, 0.13624554872512817, 0.4682689309120178, -0.4631660282611847, 0.45528334379196167, 0.36989760398864746, -0.35125741362571716, -0.618522047996521, 0.7554159164428711, 0.824562132358551, -0.4270220994949341, -0.41085299849510193, -0.3008342981338501, -0.6725261211395264, 0.1338960975408554, -1.0883644819259644, 0.3574516177177429, -0.6258716583251953, -0.24276810884475708, 0.0201985165476799, -0.1877550184726715, 0.05769873782992363, 0.09434407949447632, -0.21674290299415588, -0.4573020040988922, -0.8233352303504944, -0.5370216369628906, 0.6263424754142761, 0.6689931750297546, -0.7608168125152588, 0.029063526540994644, -0.3792974054813385, -0.01700742170214653, 0.11064291000366211, 0.5980386137962341, -0.42555326223373413, -0.7322084903717041, -1.5309762954711914, 0.5866994857788086, -0.20040832459926605, -0.16150279343128204, -0.7140005230903625, 0.08877605199813843, 0.28063586354255676, -0.09055742621421814, 0.5671280026435852, -0.3482408821582794, -0.1567133516073227, -0.309003084897995, 0.1072872132062912, -0.47835835814476013, 0.46980926394462585, 0.38821446895599365, -1.282776117324829, -0.182672381401062, 0.3740595877170563, -0.3229873776435852, -1.07845938205719, -0.8682452440261841, 0.3850629925727844, -0.6152557730674744, 0.2920212149620056, -0.11837942153215408, -0.07413472980260849, -1.1656666994094849, -0.012008365243673325, 0.4056316018104553, 0.04471270367503166, 0.3295886218547821, 0.838627815246582, 0.07609067112207413, -0.8732742667198181, 0.01509435847401619, 0.3119901716709137, -0.19013144075870514, -0.3017224073410034, 0.6387620568275452, 0.12431984394788742, -0.5653447508811951, 0.14394009113311768, 0.09030457586050034, 0.5890236496925354, -0.7688007950782776, 0.020462727174162865, 0.4455640912055969, -0.6771633625030518, 0.34658190608024597, 1.36020827293396, -0.7267405986785889, -0.8634864091873169, -0.4986494183540344, -1.1493109464645386, -0.0549355112016201, -0.8287777304649353, 0.4892387092113495, 0.2589637339115143, 0.14496645331382751, -0.08692953735589981, -0.7717747688293457, 0.18156476318836212, -0.20117641985416412, -0.696948230266571, 0.2168387770652771, -0.04218815639615059, -0.723747193813324, 0.23363694548606873, 0.5180904865264893, -0.18021413683891296, -0.4795306622982025, 0.1370924860239029, -0.6746673583984375, -0.12756505608558655, 0.05360221862792969, -0.35796499252319336, -0.6775649785995483, 0.7169954180717468, 0.23712657392024994, 0.45826950669288635, -0.024133799597620964, -0.34371572732925415, 0.4797143042087555, 0.3319278955459595, 0.18031679093837738, -0.9761088490486145, -0.7408318519592285, 0.9912739396095276, 0.711447536945343, -0.9146827459335327, 0.3014248013496399, -0.08384212851524353, -1.0571224689483643, 0.725671648979187, 0.682317852973938, 0.3016204833984375, 0.21806786954402924, -0.02292950078845024, 0.09542582929134369, 0.6520669460296631, -0.9776191115379333, -0.10750062018632889, 0.29412147402763367, 1.255634069442749, 1.34285569190979, 0.42379820346832275, -0.09505684673786163, 0.8731653094291687, -0.03607478737831116, -0.11336811631917953, 0.7147504091262817, 0.5707715153694153, 0.12702202796936035, -0.734641432762146, -0.2734862267971039, 0.3123587965965271, -0.8043740391731262, -0.7307950854301453, 0.1709648221731186, 0.5773599743843079, 0.9535719156265259, 0.6798064112663269, 0.6105091571807861, 0.017092224210500717, 0.28417205810546875, 0.38818225264549255, 0.7688611745834351, -0.6951620578765869, -0.7857486009597778, -0.40045487880706787, -0.33202236890792847, -0.37819361686706543, 0.14658422768115997, -0.7420237064361572, -0.7543027997016907, -0.6404324769973755, 0.5783106684684753, 0.42086976766586304, 0.6821298003196716, 0.9714428186416626, 0.9608885645866394, 0.6820814609527588, -0.14002715051174164, -0.8228238224983215, 0.11559592932462692, -0.7189406752586365, -0.10215123742818832, -0.1187155544757843, -0.41032183170318604, 0.051336150616407394, 0.16582722961902618, 0.14982561767101288]}, "authors": [{"authorId": "67175437", "name": "Gautier Dagan"}, {"authorId": "2282469774", "name": "Gabriele Synnaeve"}, {"authorId": "3361236", "name": "Baptiste Rozi\u00e8re"}], "references": [{"paperId": "d7fc6c7ee8bc07de753afcda09d5eda138c6fa64", "title": "Multi-word Tokenization for Sequence Compression"}, {"paperId": "975a546b207b399fa5ef44952eefe48b97b200c2", "title": "Fast Vocabulary Transfer for Language Model Compression"}, {"paperId": "abe90a291e7cf567ce5c9012a692beeae153068d", "title": "Think before you speak: Training Language Models With Pause Tokens"}, {"paperId": "e6b73466bab5e52ce0db19dd06d9353c26557dae", "title": "CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code"}, {"paperId": "5e07cb357ea8078af34c6e9154a0c746af3a4d9e", "title": "Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "bafe023fb072045dc0cd50316382a61c8dcb9fae", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c", "title": "XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models"}, {"paperId": "f3a6115e5fb2237df938976e005468f0b18da797", "title": "The Stack: 3 TB of permissively licensed source code"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da", "title": "InCoder: A Generative Model for Code Infilling and Synthesis"}, {"paperId": "38115e80d805fb0fb8f090dc88ced4b24be07878", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}, {"paperId": "f6b65866d6dc851ada1964728b647a3d4da34b03", "title": "Fine-Tuning Transformers: Vocabulary Transfer"}, {"paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7", "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"}, {"paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa", "title": "Representing Numbers in NLP: a Survey and a Vision"}, {"paperId": "0646bb09db4d1ba24150e69b71edcd4aff691b3c", "title": "Unified Pre-training for Program Understanding and Generation"}, {"paperId": "2cc3ab9fa41ba2804e301f7eae9598636e62422a", "title": "Investigating the Limitations of Transformers with Simple Arithmetic Tasks"}, {"paperId": "0d4b5c9a071557f4eb12f63f785dbc89071d4272", "title": "How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"}, {"paperId": "df56748cd4f52a58973b4ac52c0bf9156c5f52f0", "title": "Unsupervised Translation of Programming Languages"}, {"paperId": "5e788c833321b12671206b96a438c0e5b1202027", "title": "Finding the Optimal Vocabulary Size for Neural Machine Translation"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "c20c68c45127439139a08adb0b1f2b8354a94d6c", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"}, {"paperId": "d023e4c652dc21b2068a8527203a70d9eaf195d9", "title": "BPE-Dropout: Simple and Effective Subword Regularization"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "1906e3a2fda12641a42739e3fb6a8f8b1accc8dd", "title": "SPoC: Search-based Pseudocode to Code"}, {"paperId": "b5246fa284f86b544a7c31f050b3bd0defd053fd", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"}, {"paperId": "e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e", "title": "Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"}, {"paperId": "d07284a6811f1b2745d91bdb06b040b57f226882", "title": "Decoupled Weight Decay Regularization"}, {"paperId": "1518039b5001f1836565215eb047526b3ac7f462", "title": "Neural Machine Translation of Rare Words with Subword Units"}, {"paperId": null, "title": "Introducing decicoder: The new gold standard in efficient and accurate code generation"}, {"paperId": null, "title": "Training generalized multi-query transformer models from multi-head check-points"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "2bcf3e6c2b45c052a0bd0183cc29c03acc4b49ac", "title": "Human behavior and the principle of least effort"}, {"paperId": null, "title": "Replit\u2019s new ai model now available on hug-ging face"}, {"paperId": null, "title": "Tokenization and the noise-less channel"}, {"paperId": null, "title": "the source be with you! CoRR"}, {"paperId": null, "title": "guidance-ai"}, {"paperId": null, "title": "Introducing claude"}, {"paperId": null, "title": "Tokenmonster: Ungreedy subword tokenizer and vocabulary trainer for python, go and javascript"}, {"paperId": null, "title": "Mpt-7b-instruct: A model for short-form instruction following"}, {"paperId": null, "title": "Deepseek coder: A series of code language models"}]}