{"paperId": "894ed1aba8e42a4ec27ba53ecde383b14c5128ca", "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models", "abstract": "Generative pre-trained transformer (GPT) models have revolutionized the field of natural language processing (NLP) with remarkable performance in various tasks and also extend their power to multimodal domains. Despite their success, large GPT models like GPT-4 face inherent limitations such as considerable size, high computational requirements, complex deployment processes, and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage. The need for user-friendly, relatively small, and open-sourced alternative GPT models arises from the desire to overcome these limitations while retaining high performance. In this survey paper, we provide an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility. Through this extensive survey, we aim to equip researchers, practitioners, and enthusiasts with a thorough understanding of user-friendly and relatively small open-sourced models of large GPTs, their current state, challenges, and future research directions, inspiring the development of more efficient, accessible, and versatile GPT models that cater to the broader scientific community and advance the field of general artificial intelligence. The source contents are continuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.", "venue": "arXiv.org", "year": 2023, "citationCount": 3, "influentialCitationCount": 1, "openAccessPdf": {"url": "https://arxiv.org/pdf/2308.14149", "status": "CLOSED"}, "tldr": {"model": "tldr@v2.0.0", "text": "This survey paper provides an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility, and aims to equip researchers, practitioners, and enthusiasts with a thorough understanding of these models."}, "embedding": {"model": "specter_v2", "vector": [0.2163839340209961, 1.012850046157837, -0.13280338048934937, 0.11680980026721954, -0.48060983419418335, -0.5981314182281494, 0.5746439695358276, -0.10726390779018402, -0.15335065126419067, -0.4013492465019226, 0.3231491446495056, -0.4599672257900238, 0.18185827136039734, 0.04000471532344818, -0.38720107078552246, -0.061454832553863525, -0.7878289222717285, 0.9142628312110901, -0.3550127148628235, -0.15722937881946564, -0.6186311841011047, -0.7412276268005371, -0.6799870729446411, -0.11104705184698105, 0.5794257521629333, 0.287212997674942, 0.4322959780693054, 0.7341092228889465, -0.46588942408561707, 0.09108999371528625, 0.48894891142845154, -0.8971671462059021, 0.3648408055305481, 0.009424414485692978, -0.15098538994789124, 0.03284378722310066, 0.2593415081501007, -0.2100864052772522, -0.5454553365707397, 0.7681306004524231, -0.11630050092935562, 0.16816361248493195, 0.319740891456604, -0.8758628368377686, -0.8471470475196838, 1.3396002054214478, 0.8251588940620422, 0.5793542265892029, -0.10562634468078613, -0.25708144903182983, 1.5024422407150269, -1.0960845947265625, 0.19957123696804047, 1.722558617591858, 0.5871670842170715, 0.4675084948539734, -0.4178524315357208, -0.6165754199028015, 0.24602089822292328, -0.41256487369537354, -0.4274628758430481, -0.3738863468170166, 0.0320311039686203, 0.1072317585349083, 1.9665212631225586, -0.2939832806587219, 0.37902531027793884, 1.0132300853729248, -0.07473856955766678, 1.518961787223816, 0.051577966660261154, -0.9317635893821716, -0.4022185802459717, -0.086161769926548, -0.05445285513997078, 0.9739065170288086, -0.5961074233055115, 0.34722813963890076, -0.7630258202552795, -0.2577563524246216, 0.6422837972640991, -0.13692468404769897, 0.019438017159700394, 0.21197110414505005, -0.33842524886131287, 1.1437551975250244, 0.2892845571041107, 0.907214879989624, -0.25028666853904724, 0.20267657935619354, 0.1305946558713913, 0.4587566554546356, 0.0325653962790966, 0.38579678535461426, -0.45401760935783386, 0.35568341612815857, -0.9657621383666992, 0.36722174286842346, 0.1438794732093811, 0.8986601233482361, -0.19692768156528473, 0.14680060744285583, -1.1437747478485107, 0.27953556180000305, 1.4542326927185059, 0.4522092044353485, 0.6800775527954102, -0.6208990216255188, 0.4635993242263794, -0.3140532970428467, -0.15627971291542053, -0.2882753908634186, -0.2706608176231384, -0.19066685438156128, -0.5544619560241699, -1.8061268329620361, -0.11136306822299957, 0.09249825775623322, -1.3274757862091064, 0.6951507329940796, -0.44969063997268677, -0.2816905677318573, 0.37225672602653503, 0.2087189108133316, 0.6707525849342346, 0.8965001106262207, 0.4566965699195862, 0.02689404971897602, 1.0536954402923584, -0.6460118889808655, -0.7059358358383179, -1.5253119468688965, 0.6408605575561523, -0.007213992532342672, 0.469635546207428, -0.5340204834938049, -1.4163434505462646, -0.5746097564697266, -0.23590318858623505, -0.19255994260311127, -0.3102542757987976, 0.45746007561683655, 1.2482458353042603, 0.6530565619468689, -1.208717703819275, 0.10284967720508575, -0.26089319586753845, -0.6029208302497864, 0.36809009313583374, 0.19852276146411896, 0.16514329612255096, -0.5622293949127197, -1.2473385334014893, 0.03121080808341503, 0.46017155051231384, -0.36325061321258545, -0.5251756906509399, -0.14780059456825256, -1.3522597551345825, 0.13314570486545563, 0.27956530451774597, -0.71356600522995, 1.320367455482483, 0.07495410740375519, -1.233038067817688, 0.6990286707878113, -0.5726070404052734, -0.25692763924598694, -0.11975442618131638, 0.20695196092128754, -0.274097204208374, -0.41218024492263794, 0.1492519974708557, 0.7439899444580078, 0.45703554153442383, 0.04876387119293213, 0.06420795619487762, 0.11470303684473038, -0.17951710522174835, -0.04011327773332596, -0.2843322157859802, 1.0855457782745361, -0.4462586045265198, -0.2721562683582306, 0.4226873517036438, 0.3477849066257477, -0.2807926833629608, -0.2926645874977112, -0.6224603056907654, -1.0832892656326294, 0.4429762065410614, -0.1789342164993286, 0.7980843782424927, -0.8678908944129944, -0.8160797953605652, -0.1823827624320984, -0.15429435670375824, -0.3854454457759857, -0.7552581429481506, 0.7791826128959656, -0.47868239879608154, 0.5618240237236023, -0.3623241186141968, -0.9131883978843689, 0.22456856071949005, 0.15984727442264557, -0.6201907992362976, -0.4087730944156647, 0.30795806646347046, 1.4091609716415405, -0.8275477290153503, 0.015536383725702763, -0.01749376580119133, 0.15615586936473846, -0.8995038866996765, 1.045607089996338, -0.4648418426513672, 0.1272881031036377, -0.2808327078819275, 0.1410456895828247, -0.0632542297244072, -0.2946053147315979, 0.4274614453315735, -0.3001854121685028, -0.30181649327278137, 0.4864239990711212, -0.08849769830703735, 1.460113525390625, 0.05084702745079994, 0.27356165647506714, -0.2466375231742859, -0.6233279705047607, 0.1298743039369583, 0.9629582166671753, -0.3916952311992645, -0.4299134910106659, 0.564348042011261, 0.701665997505188, -0.3923077881336212, 0.42385077476501465, 0.6069111227989197, 0.4043544828891754, -0.33478203415870667, 0.04400140419602394, 0.8789412379264832, -0.2068277895450592, 0.7505035996437073, 0.395998477935791, 0.631050705909729, -0.03522762656211853, 0.1874900907278061, -0.2011062055826187, 0.06674076616764069, -0.7200732231140137, -0.027670428156852722, 0.38118812441825867, 0.7717273235321045, 0.7398597002029419, 0.5065916776657104, -0.467197984457016, -0.09370800852775574, 0.256660133600235, 0.6669788360595703, 1.4970241785049438, -0.7080764770507812, -0.2688189446926117, -0.6644108891487122, -0.32720422744750977, -0.6587617993354797, 0.10611212998628616, -0.26143383979797363, 0.3263651728630066, -0.39683109521865845, -1.140175461769104, 1.045458436012268, 0.14315170049667358, 0.6272789835929871, -0.6447635293006897, -0.21013057231903076, -0.2336159199476242, -0.10187209397554398, -0.7771812677383423, -0.37560147047042847, 0.5174577236175537, -0.9892547130584717, -0.20402538776397705, -0.21626590192317963, -0.41369473934173584, 0.08178191632032394, -1.0180854797363281, 0.7606004476547241, -0.81517094373703, 0.2037939876317978, 0.231785386800766, 0.6483805179595947, -0.7170809507369995, -0.8789235949516296, 0.070714570581913, -0.23603323101997375, -0.445972740650177, 0.27085262537002563, 0.6610243320465088, 0.6260396838188171, -0.0032232929952442646, -0.3183903098106384, 0.00641424348577857, 0.15725277364253998, 0.28301939368247986, 0.8914153575897217, -0.08102843910455704, -0.4984933137893677, -0.9464678168296814, 1.126932144165039, 0.3498792350292206, -0.46881958842277527, 0.5224923491477966, -0.3130072355270386, -0.25686338543891907, 0.45617395639419556, -0.6503618359565735, -0.5036766529083252, -0.6422511339187622, 0.07882396131753922, -0.12705206871032715, -0.2542990446090698, 0.6784579753875732, 0.104423388838768, 0.28977951407432556, 0.25116807222366333, 0.7668628096580505, 0.07374296337366104, -0.23257099092006683, 0.6030777096748352, -1.0748262405395508, 0.21577569842338562, 0.22146008908748627, 0.338563472032547, -0.43542492389678955, -0.39608731865882874, -0.543287992477417, -0.23608267307281494, 0.08642582595348358, 0.10090437531471252, -0.029810670763254166, -0.10167842358350754, -0.6339492201805115, -0.5755481123924255, 0.20155826210975647, -0.8332099914550781, -0.0246804840862751, 0.010692719370126724, -0.39509257674217224, 0.1497213989496231, -0.9940011501312256, -1.3867617845535278, -0.6904558539390564, -0.8965088725090027, -0.8648805022239685, 0.7638993859291077, 0.20015980303287506, 0.020498691126704216, -0.8220443725585938, 0.13622558116912842, -0.2156190723180771, 0.7823529243469238, -0.6851223707199097, 1.2075196504592896, -0.3397047817707062, -0.031925030052661896, -0.4225611686706543, 0.4572460651397705, 0.1438685953617096, -0.4307097792625427, 0.5032312273979187, -0.8222088813781738, 0.20595206320285797, -0.03524508699774742, -0.15965788066387177, -0.07271906733512878, 0.9135169982910156, 0.06020103394985199, -0.096015065908432, -0.6045882701873779, 0.2210976779460907, 1.0332117080688477, -0.4680904150009155, -0.022859865799546242, 0.044701483100652695, 0.7872624397277832, 0.2166311889886856, -0.4693335294723511, 0.23019662499427795, 0.5383396744728088, -0.047905709594488144, 0.15876583755016327, -0.08533470332622528, 0.2755741477012634, -0.7641651034355164, 0.5992048382759094, 1.4736378192901611, 0.023368272930383682, -0.7175487875938416, -1.2973761558532715, 0.6919491291046143, -1.1326490640640259, -0.45754873752593994, 0.4131275415420532, 0.4404311180114746, 0.2447367161512375, -0.3712972402572632, -0.44668149948120117, 0.04301803186535835, 0.49362078309059143, 0.40410152077674866, 0.057218603789806366, -0.4773991107940674, -0.43096259236335754, 0.6968488693237305, 0.1720867156982422, 0.23089954257011414, -0.2843253016471863, 0.764915943145752, 14.774060249328613, 0.9917446970939636, 0.18061406910419464, 0.6126871705055237, 0.5943748354911804, 0.6194127202033997, -0.5867936015129089, 0.06755350530147552, -0.9763097763061523, -0.33680251240730286, 1.3667556047439575, -0.01368001103401184, 0.8041883707046509, -0.005262334365397692, -0.013656564988195896, 0.20953352749347687, -0.4035649597644806, 0.6948157548904419, 0.3255200982093811, -0.9648522138595581, 0.7501346468925476, 0.3038843274116516, 0.1731596142053604, 0.8192178606987, 0.7176963686943054, 0.850744903087616, 0.4937630891799927, -0.3949560523033142, 0.5434519648551941, -0.032822635024785995, 0.5925521850585938, 0.08411740511655807, 0.5510215759277344, 0.5737805962562561, -1.0341901779174805, -0.5623904466629028, -0.5310205817222595, -1.134033441543579, 0.21048596501350403, 0.15530283749103546, -0.42481622099876404, -0.304126113653183, -0.1401768922805786, 0.7834942936897278, -0.2146165370941162, 0.5144088268280029, -0.5498918294906616, 0.922406017780304, -0.5284175276756287, 0.2746591567993164, 0.4421839416027069, 0.3067586123943329, 0.4865567684173584, -0.5946823954582214, 0.6869633197784424, -0.2665349245071411, 0.11143149435520172, 0.554810106754303, -0.5880281925201416, 0.07856453210115433, -0.3928375542163849, -0.9034709930419922, -0.2012113481760025, 0.8808193206787109, 0.049154046922922134, 0.4791439473628998, -0.375271737575531, 0.12234856933355331, 0.5082443952560425, 0.32883164286613464, -0.19871176779270172, 0.29955312609672546, -0.09305763244628906, -0.5645529627799988, 0.07066573202610016, 0.6704502105712891, 0.16618287563323975, -0.48248621821403503, -0.6824789047241211, -0.3167732059955597, 0.40904855728149414, -0.8416112065315247, -0.9519229531288147, 1.1889864206314087, -0.2871339023113251, -0.17282021045684814, 0.1654995232820511, -0.7715242505073547, -0.22322620451450348, 0.5793070197105408, -1.278874397277832, -1.471119999885559, 0.787469208240509, 0.05857479199767113, -0.39694732427597046, -0.15972290933132172, 1.4436575174331665, -0.1241268515586853, -0.7690309286117554, -0.058408595621585846, 0.1967993527650833, 0.1651913821697235, -0.6348250508308411, -0.6112576723098755, 1.1399751901626587, 0.5299973487854004, 0.31779542565345764, 0.29304981231689453, 0.1704760491847992, 0.04308434575796127, -0.871167778968811, 0.16253463923931122, 1.2381905317306519, -0.9427987337112427, -0.33520111441612244, -0.9240854978561401, -0.6361024379730225, 0.4863387644290924, 0.6927525997161865, -0.5342922210693359, 0.261526882648468, 0.2459716498851776, -0.12727396190166473, 0.05330662801861763, -0.9439255595207214, -0.10163181275129318, 0.44798752665519714, -0.814894437789917, -0.4568895697593689, 0.14458516240119934, 0.3215124011039734, -1.0717206001281738, -0.38771432638168335, -0.44256073236465454, 0.12320545315742493, 0.1979306936264038, 0.7060514688491821, -0.1254281997680664, 0.22193928062915802, 1.1605324745178223, -0.02726263925433159, -0.9423835277557373, -0.17728713154792786, -0.9298927783966064, -0.04548530653119087, 0.14111821353435516, 1.0697226524353027, -0.8250589966773987, 0.20465309917926788, 1.2848756313323975, 0.15236900746822357, -0.1988581269979477, -0.7687116265296936, -0.4056585133075714, -0.11352445930242538, -0.5690967440605164, 0.4657411575317383, -0.37284788489341736, -0.21611344814300537, 0.3361070156097412, 0.4877210557460785, 0.8164559006690979, -0.21465934813022614, -0.6168888807296753, 0.09566768258810043, -0.4641379415988922, -0.08418253064155579, -0.23907721042633057, -0.251076340675354, -1.3004645109176636, 0.08383959531784058, -1.1653460264205933, 0.065589539706707, -1.2644634246826172, -0.39734601974487305, 0.08943293243646622, -0.030826572328805923, 0.6523826718330383, 0.6209396123886108, -0.3866136968135834, -0.6554163694381714, -0.47937577962875366, 0.11774326860904694, 0.9284512996673584, 1.0269025564193726, -0.7449288964271545, -0.005281948950141668, -0.07568784803152084, -0.001507420209236443, 0.0838797315955162, 0.3038656711578369, -0.9053838849067688, -0.9626241326332092, -1.6048827171325684, 0.3687755763530731, -0.1060052141547203, 0.058577410876750946, -0.6515408158302307, 0.5689384341239929, 0.6229274868965149, -0.5450001955032349, 0.12670525908470154, 0.4711078703403473, -0.9341121315956116, -0.22064746916294098, 0.1821439415216446, -0.39528122544288635, 0.3596234917640686, 0.4560282230377197, -0.609695315361023, -0.567700207233429, 0.5470610857009888, -0.46677902340888977, -1.198204517364502, -0.19461804628372192, 0.6859325766563416, -0.7429224252700806, 0.1249680444598198, -0.4238605201244354, -0.3237631618976593, -0.9035959839820862, -0.19155611097812653, 0.00878584198653698, 0.3228180706501007, -0.5280126929283142, 0.6975160241127014, 0.4679260849952698, -0.7037617564201355, 0.044164396822452545, 0.7198636531829834, -0.017229413613677025, -0.2841881811618805, 0.3104827404022217, 0.3701034188270569, -0.18549314141273499, 0.9389269948005676, 0.3064732849597931, 0.3907947242259979, -0.8164570927619934, -0.25878486037254333, 0.650709867477417, -0.6275395154953003, -0.3469998836517334, 1.3139311075210571, -0.2746436297893524, -1.3011949062347412, -0.022148219868540764, -1.5874965190887451, -0.6511544585227966, -0.6018190383911133, 0.11594384908676147, -0.12097926437854767, -0.31332921981811523, -0.42672741413116455, -0.31515634059906006, 0.20262287557125092, 0.16047590970993042, -0.5090768337249756, 0.8175607919692993, -0.21068955957889557, -0.27085548639297485, 0.36608439683914185, 0.24525579810142517, -0.8729183673858643, -0.47494885325431824, -0.4703340530395508, -0.4865587055683136, -0.05326526239514351, 0.47721654176712036, -0.5581194162368774, -0.8140539526939392, 0.7546758651733398, 0.5339771509170532, 0.6636486649513245, 0.26020780205726624, 0.07609362155199051, 0.41664737462997437, 0.8037847876548767, 0.234039306640625, -0.3185487985610962, -1.072959303855896, 1.466563105583191, 0.9375430345535278, -0.8710737824440002, -0.3159918785095215, -0.44895994663238525, -0.9171571135520935, 0.8024582862854004, -0.1046636775135994, 0.28842535614967346, 0.7717441320419312, -0.4345235228538513, -0.12277952581644058, -0.31009840965270996, -0.8358209133148193, -0.15996065735816956, 0.7260962724685669, 1.041105031967163, 0.8262522220611572, 0.3340620696544647, 0.060734838247299194, 1.0354640483856201, -0.14398814737796783, 0.25375980138778687, 0.20825354754924774, 0.735633909702301, -0.05166005343198776, -0.24172291159629822, 0.1156083345413208, 0.9193107485771179, -0.5955681204795837, -1.1918237209320068, 0.00786604080349207, 0.5733518004417419, 0.21596494317054749, 0.8453125357627869, 0.34755292534828186, 0.40359294414520264, 0.6979058384895325, 0.4645921587944031, 0.6604897975921631, -0.6279797554016113, -0.3854641616344452, 0.13840748369693756, -0.57265704870224, 0.11922317743301392, -0.33368974924087524, -0.2886924147605896, -0.04147240146994591, -0.0011343385558575392, 0.12479795515537262, 0.18977174162864685, 0.26537829637527466, 1.0603994131088257, 0.33407261967658997, -0.008248220197856426, -0.6414384841918945, -0.2516304850578308, 0.1817263960838318, -1.1852123737335205, -0.26415905356407166, -0.9847075343132019, -0.39559268951416016, -0.11610624194145203, -0.12105640023946762, -0.291476845741272]}, "authors": [{"authorId": "1944690382", "name": "Kaiyuan Gao"}, {"authorId": "2112345574", "name": "Su He"}, {"authorId": "2152990262", "name": "Zhenyu He"}, {"authorId": null, "name": "Jiacheng Lin"}, {"authorId": "2171652249", "name": "Qizhi Pei"}, {"authorId": "2234370833", "name": "Jie Shao"}, {"authorId": "2256597384", "name": "Wei Zhang"}], "references": [{"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "0d2adcddccd72de47c263b6e4e0aab3dd0582a52", "title": "ReLoRA: High-Rank Training Through Low-Rank Updates"}, {"paperId": "888728745dbb769e29ed475d4f7661eebe1a71cf", "title": "A Survey on Evaluation of Large Language Models"}, {"paperId": "2922768fd451ecdb45f48c1a83eb57f54a91221b", "title": "Textbooks Are All You Need"}, {"paperId": "a0a79dad89857a96f8f71b14238e5237cbfc4787", "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena"}, {"paperId": "7c8254f6d95863fffeef2ba3d0d07f42b0f72e21", "title": "MolFM: A Multimodal Molecular Foundation Model"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "be8db99310602d66bba64bcf41a572c45816fbfc", "title": "Let's Verify Step by Step"}, {"paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2", "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"}, {"paperId": "31a7d8c4a5ab6bab522494b57270249105c8748e", "title": "BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"}, {"paperId": "a122863d239643453195424c04067e89406246e1", "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "8c7846c9805834dbe2fb0c8f48253b8d65b79d6a", "title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks"}, {"paperId": "1567bcac0ab09269c9d0ff33c9a406132417fab9", "title": "A Pretrainer\u2019s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "cb6cc7d28d06a0d7c0d3f0d7ee551bbc86dbc3aa", "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback"}, {"paperId": "ed1353d705eeabc0e916caba5fbae890eefe4f84", "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training"}, {"paperId": "546d0624adfc6e18fb87d8cc77e7705bb9ea7445", "title": "LIMA: Less Is More for Alignment"}, {"paperId": "81e7e82245c2f230eeb8aaaa1a2b2604c143754a", "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"}, {"paperId": "d6d3604f369bb0415cbe814e43ca3131323b03e2", "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning"}, {"paperId": "af0f0972232754215d611ce086c27fd0dbcf41c2", "title": "Panda LLM: Training Data and Evaluation for Open-Sourced Chinese Instruction-Following Large Language Models"}, {"paperId": "0046306876ff2d5600699327e52bc29fa5e9ec91", "title": "Transfer Visual Prompt Generator across LLMs"}, {"paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3", "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"}, {"paperId": "131f499e4d3503da93022d07fcf804a18483bea9", "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions"}, {"paperId": "ca6a2bc279be5a3349a22bfd6866ed633d18734b", "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "352420ee61a8da783ca7750170793613b18b8d9c", "title": "Tool Learning with Foundation Models"}, {"paperId": "e5adc219685c9941b9a3d029480af4a51c0ea05a", "title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"}, {"paperId": "302ee27524a717ddc21f332ca634b9211c6ec6aa", "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge"}, {"paperId": "9e8cb8c91a0acb6e661b58ad724aa758490f2bea", "title": "Instruction Tuning with GPT-4"}, {"paperId": "bce55193d9a887ad00774a9134df08cd521a85ae", "title": "DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task"}, {"paperId": "be55e8ec4213868db08f2c3168ae666001bea4b8", "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "a757999ed260d7bc45484dc6b4456bf33fe6f679", "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"}, {"paperId": "348a1efa54376fa39053e5e25d52bd0eb6a0ba68", "title": "Capabilities of GPT-4 on Medical Challenge Problems"}, {"paperId": "c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4", "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "6e754273d54a91371efbc928cd6b156364d517da", "title": "ViperGPT: Visual Inference via Python Execution for Reasoning"}, {"paperId": "af997821231898a5f8d0fd78dad4eec526acabe5", "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"}, {"paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6", "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050", "title": "Multimodal Chain-of-Thought Reasoning in Language Models"}, {"paperId": "a4a41319d5805a29316f24ed9519f09db77d4c29", "title": "Benchmarking Large Language Models for News Summarization"}, {"paperId": "f2b0017ddd77fa38760a18145e63553105a1a236", "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"}, {"paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb", "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"}, {"paperId": "cb29cf52f0f7d2e4324c68690a55b22890f2212d", "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection"}, {"paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d", "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization"}, {"paperId": "e65b346d442e9962a4276dc1c1af2956d9d5f1eb", "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "7d645a3fd276918374fd9483fd675c28e46506d1", "title": "Galactica: A Large Language Model for Science"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1", "title": "Scaling Instruction-Finetuned Language Models"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "83851f1a32d41975582ca62355858ab5e34738f7", "title": "News Summarization and Evaluation in the Era of GPT-3"}, {"paperId": "44279244407a64431810f982be6d0c7da4429dd7", "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"}, {"paperId": "d3135733aa39dec20ce72aa138589dda27c8406d", "title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"}, {"paperId": "88dd119dba5ee747851ade8f5d517b381614d918", "title": "Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence"}, {"paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7", "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned"}, {"paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1", "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "aa4d9972af3264d032dbee58501ed4ac49477103", "title": "Scaling Laws and Interpretability of Learning from Repeated Data"}, {"paperId": "bc8b82e8eb0b0714892e4ec7a54ebdf47c4fde96", "title": "Reducing Activation Recomputation in Large Transformer Models"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "26218bdcc3945c7edae7aa2adbfba4cd820a2df3", "title": "Flamingo: a Visual Language Model for Few-Shot Learning"}, {"paperId": "3b9b1aba877ecd3f7e508cbc78a41b623349902b", "title": "Translation between Molecules and Natural Language"}, {"paperId": "06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb", "title": "PaLM: Scaling Language Modeling with Pathways"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "741776172685b9717159a9fcd21841461bb33b14", "title": "MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "2f3efe44083af91cef562c1a3451eee2f8601d22", "title": "WebGPT: Browser-assisted question-answering with human feedback"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea", "title": "Training Verifiers to Solve Math Word Problems"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "f3a332ff1b73acda482e5d83696b2c701f487819", "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"}, {"paperId": "77d956cdab4508d569ae5741549b78e715fd0749", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269", "title": "Evaluating Large Language Models Trained on Code"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "774591fdd988eaaff3917e7c5171d044b0843e63", "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"}, {"paperId": "7e5008713c404445dd8786753526f1a45b93de12", "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"}, {"paperId": "bc37c6bdb8f39929a58b30464f72d6aa46cddc17", "title": "GPT Understands, Too"}, {"paperId": "50796b0f3edf9cb5ff1e447c298b33755378aa4f", "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling"}, {"paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5", "title": "Measuring Mathematical Problem Solving With the MATH Dataset"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "11adf5397466ecbec178f01ef644143d43138d09", "title": "Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text"}, {"paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a", "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"}, {"paperId": "fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7", "title": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams"}, {"paperId": "814a4f680b9ba6baba23b93499f4b48af1a27678", "title": "Measuring Massive Multitask Language Understanding"}, {"paperId": "053b1d7b97eb2c91fc3921d589c160b0923c70b1", "title": "Learning to summarize from human feedback"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "1a6f4495474f75ae1e8bbf407f70d9a874e5b4d6", "title": "The Pushshift Reddit Dataset"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "0c3c4c88c7b07596221ac640c7b7102686e3eae3", "title": "PubMedQA: A Dataset for Biomedical Research Question Answering"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b", "title": "WinoGrande"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "031e4e43aaffd7a479738dcea69a2d5be7957aa3", "title": "ERNIE: Enhanced Representation through Knowledge Integration"}, {"paperId": "0dac6dd48a4e2c33797bcdda2d71869a8812641f", "title": "DOLLY"}, {"paperId": "dda6fb309f62e2557a071522354d8c2c897a2805", "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"}, {"paperId": "29ddc1f43f28af7c846515e32cc167bc66886d0c", "title": "Parameter-Efficient Transfer Learning for NLP"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "b4df354db88a70183a64dbc9e56cf14e7669a6c0", "title": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "932a5de79d8a8ebb75ea0c43493450fd9922e738", "title": "Crowdsourcing Multiple Choice Science Questions"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db", "title": "VQA: Visual Question Answering"}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}, {"paperId": "71b7178df5d2b112d07e45038cb5637208659ff7", "title": "Microsoft COCO: Common Objects in Context"}, {"paperId": "8e080b98efbe65c02a116439205ca2344b9f7cd4", "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs"}, {"paperId": "168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "title": "Scikit-learn: Machine Learning in Python"}, {"paperId": "f7ed5d688ee975389f90458cc54c0b68c0ae4820", "title": "Humans"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260", "title": "Finding Structure in Time"}, {"paperId": "3f7983818b76a5f1b5daf9b605877ed401c8e73c", "title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules"}, {"paperId": null, "title": "\u201cOpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications,\u201d"}, {"paperId": null, "title": "\u201cOpenllama: An open reproduction of llama,\u201d"}, {"paperId": "d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43", "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"}, {"paperId": null, "title": "\u201cIntroducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs,\u201d"}, {"paperId": "cd2d1a0f73ba8c40f882a386cd367899785fb877", "title": "PMC-LLaMA: Further Finetuning LLaMA on Medical Papers"}, {"paperId": null, "title": "Open assistant"}, {"paperId": null, "title": "Xverse-13b"}, {"paperId": null, "title": "\u201cKoala: A dialogue model for academic research,\u201d"}, {"paperId": null, "title": "\u201cReleasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models,\u201d"}, {"paperId": null, "title": "Mlc llm"}, {"paperId": null, "title": "Introducing qwen-7b: Open foundation and humanaligned models"}, {"paperId": null, "title": "Ultrachat: A large-scale autogenerated multi-round dialogue data"}, {"paperId": null, "title": "Luotuo: An instructionfollowing chinese language model, lora tuning on llama"}, {"paperId": null, "title": "Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5turbo"}, {"paperId": "104f7a96eba307056e1038e183ee8c24d009ba13", "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"}, {"paperId": null, "title": "\u201cIntroducing ChatGPT,\u201d"}, {"paperId": null, "title": "Peft: State-of-the-art parameter-efficient finetuning methods"}, {"paperId": "53d8b356551a2361020a948f64454a6d599af69f", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "ed47fd8aecae53fb7fa01f1668d9a16f0d19221c", "title": "Conference on Neural Information Processing Systems"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": "1f4abf2d4e84dc7f0371caf14cdfbb6052f152d3", "title": "Firefly"}, {"paperId": "2d757edc96f1933e5a49ae6a251271fb9b7571bf", "title": "Wikipedia"}, {"paperId": "9f414c81159b5e8296a267d5ae95a5bf464061d1", "title": "The rating of chessplayers, past and present"}, {"paperId": null, "title": "\u201cElectra: Pre-training text encoders as discriminators rather than generators,\u201d"}, {"paperId": null, "title": "\u201cInternlm: A multilingual language model with progressively enhanced capabilities,\u201d"}, {"paperId": null, "title": "\u201cBaize: An open-source chat model with parameter-efficient tuning on self-chat data,\u201d"}, {"paperId": null, "title": "\u201cDataset card for sharegpt90k,\u201d"}, {"paperId": null, "title": "\u201cLamini: The llm engine for rapidly customizing models,\u201d"}, {"paperId": null, "title": "\u201cOpen llm leaderboard,\u201d"}, {"paperId": null, "title": "\u201cInstruct-tune llama on consumer hardware,\u201d"}, {"paperId": null, "title": "hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html"}, {"paperId": null, "title": "\u201cPandalm: Reproducible and automated language model assessment,\u201d"}, {"paperId": null, "title": "\u201cCommon crawl.\u201d"}, {"paperId": null, "title": "\u201cRedpajama-data: An open source recipe to reproduce llama training dataset,\u201d"}, {"paperId": null, "title": "\u201cVicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality,\u201d"}, {"paperId": null, "title": "\u201cMetagpt: The multi-agent framework,\u201d"}, {"paperId": null, "title": "\u201cxturing,\u201d"}, {"paperId": null, "title": "\u201cLinly,\u201d"}, {"paperId": null, "title": "\u201cGuanaco - Generative Universal Assistant for Natural-language Adaptive Context-aware Omnilingual outputs,\u201d"}, {"paperId": null, "title": "\u201cWelcome to h2o llm studio, a framework and no-code gui designed for fine-tuning state-of-the-art large language models (llms).\u201d"}, {"paperId": null, "title": "\u201ch2ogpt,\u201d"}, {"paperId": null, "title": "RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens"}, {"paperId": null, "title": "\u201cYulan-chat: An open-source bilingual chatbot,\u201d"}, {"paperId": null, "title": "UAE's Technology Innovation Institute Launches Open-Source \"Falcon 40B"}, {"paperId": null, "title": "\u201cChinese-vicuna: A chinese instruction-following llama-based model,\u201d"}]}