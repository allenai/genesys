{"paperId": "15190e8b459bd85d546286f7d7da61b4f4f3f58a", "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?", "abstract": "Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.", "venue": "International Conference on Machine Learning", "year": 2022, "citationCount": 127, "influentialCitationCount": 7, "openAccessPdf": {"url": "http://arxiv.org/pdf/2204.05832", "status": "GREEN"}, "tldr": {"model": "tldr@v2.0.0", "text": "A large-scale evaluation of modeling choices and their impact on zero-shot generalization finds that pretrained non-causal decoder models can be adapted into performant generative causal decoding models, using autoregressive language modeling as a downstream task."}, "embedding": {"model": "specter_v2", "vector": [0.22046412527561188, 0.9555085301399231, -0.00108020578045398, -0.11083432286977768, 0.004145971965044737, -0.18560262024402618, 0.957494854927063, -0.5617865324020386, -0.38796141743659973, -0.6408276557922363, 0.8580105304718018, -0.28902655839920044, 0.3933749198913574, -0.06662692874670029, 0.10431378334760666, 0.17656804621219635, -0.6312780380249023, 0.20970067381858826, 0.20733125507831573, -0.7006753087043762, -0.2977340817451477, -1.005859375, -0.846494197845459, 0.22616983950138092, 0.06098183989524841, -0.02263292297720909, 0.41072404384613037, 0.7477717399597168, -0.18140889704227448, 0.4670303463935852, 0.602575421333313, -0.5689038634300232, 0.12231588363647461, -0.1428903341293335, -0.18845953047275543, -0.06847263127565384, 0.26904165744781494, -0.6633856296539307, -0.6627941727638245, 0.6483154892921448, -0.19763211905956268, 0.38065266609191895, 0.51933354139328, -0.5513734817504883, -0.7027454376220703, 1.3452666997909546, 0.7758257389068604, 0.9383542537689209, -0.4078936278820038, -0.3410301208496094, 1.0352603197097778, -1.4213757514953613, 0.2480640560388565, 1.6528500318527222, 0.34556740522384644, 1.0008625984191895, -0.11685282737016678, -0.8607910871505737, 0.9041711688041687, 0.02141599729657173, -0.8348515629768372, -0.4867464303970337, -0.1119307205080986, -0.3276749551296234, 1.7864481210708618, -0.8479957580566406, -0.3346233069896698, 0.9683822989463806, 0.17485588788986206, 0.9334470629692078, -0.2254149615764618, -0.6036298871040344, -0.3427608907222748, -0.035535719245672226, 0.41311272978782654, 0.8418777585029602, -0.6939495205879211, 0.7054057121276855, -0.8535436391830444, 0.18096351623535156, 0.4094269275665283, -0.23064659535884857, -0.11335761845111847, 0.20418024063110352, -0.11563103646039963, 0.9302980303764343, 0.16685988008975983, 0.8763307929039001, -0.01840919442474842, 0.7549635171890259, 0.776099681854248, 0.5676684975624084, 0.04478390887379646, 0.25525331497192383, -0.024021752178668976, 0.6817173957824707, -0.8364198207855225, 0.13488264381885529, -0.04699105769395828, 1.0006027221679688, -0.436308354139328, 0.7468281388282776, -0.8583045601844788, -0.09863384813070297, 1.1576381921768188, -0.20666785538196564, 0.8668943047523499, -0.7592383623123169, -0.10670144855976105, -1.0241435766220093, -0.15038993954658508, -0.5246598124504089, -0.23013141751289368, -0.31867292523384094, -0.9155513644218445, -1.3425140380859375, -0.46172431111335754, 0.24265608191490173, -1.1554028987884521, 0.991886556148529, -0.46036389470100403, 0.15839026868343353, -0.039499904960393906, 0.45067232847213745, 0.939100444316864, 0.5203115344047546, 0.7028152942657471, 0.19025324285030365, 0.69999760389328, -0.47763562202453613, -0.6524840593338013, -0.9950189590454102, 0.7243915796279907, -0.26246803998947144, 0.6940249800682068, -0.1810166984796524, -1.0584145784378052, -1.003548264503479, -0.925632119178772, 0.008942564018070698, -0.43810585141181946, 0.16811485588550568, 0.9283859729766846, 0.8078385591506958, -1.014327883720398, 0.6937410235404968, -0.2547500729560852, -0.3582276403903961, 0.0066309296526014805, -0.1311742663383484, 0.07839129865169525, -0.3629799783229828, -1.6289769411087036, 0.4053233861923218, 0.33132708072662354, -0.6541310548782349, -0.7583334445953369, -0.8017193675041199, -1.135111927986145, 0.04043741151690483, 0.47300779819488525, -0.4695133864879608, 1.4539467096328735, -0.0844566747546196, -1.1736031770706177, 0.9355065822601318, -0.5249723196029663, -0.13237756490707397, 0.27213364839553833, -0.1236826628446579, -0.5080395936965942, -0.5580909848213196, -0.02684333547949791, 0.6154260635375977, 0.5742218494415283, -0.0765373706817627, -0.050764050334692, 0.11420220136642456, -0.2090979367494583, -0.3642255365848541, -0.43184393644332886, 0.8494521379470825, -0.333529531955719, -0.340745210647583, 0.24402764439582825, 0.8058342337608337, 0.19721825420856476, -0.7476545572280884, -0.2681955397129059, -1.3097211122512817, 0.5477274060249329, -0.20177415013313293, 0.7960010766983032, -0.9727780222892761, -0.8093730211257935, -0.47919562458992004, -0.06805511564016342, -0.37011203169822693, -0.7949339151382446, 0.6226536631584167, -0.5210192203521729, 0.7531113624572754, 0.12783735990524292, -1.129455327987671, 0.14240142703056335, -0.5225370526313782, -0.9918396472930908, -0.29496002197265625, -0.06819076836109161, 1.0700316429138184, -0.8442274332046509, 0.16780266165733337, 0.1508323848247528, 0.312160462141037, -1.1613553762435913, 1.1295419931411743, -0.1784035563468933, 0.19959162175655365, -0.08163783699274063, -0.6729562878608704, -0.16255660355091095, -0.22433951497077942, 0.24714690446853638, -0.5175133943557739, -0.0883965715765953, 0.4370917081832886, -0.07557329535484314, 1.403790831565857, -0.41766104102134705, 0.4454384744167328, -0.4117875397205353, -0.7026923298835754, 0.2163219451904297, 0.6778707504272461, -0.12135152518749237, -0.6967860460281372, 0.3858627676963806, 0.15260906517505646, -0.44572508335113525, 0.32653775811195374, 0.7605867385864258, 0.3178855776786804, -0.5391379594802856, 0.21816621720790863, 0.7853356003761292, -0.2177969068288803, 0.4041774570941925, 0.84039705991745, 0.822097897529602, 0.28776443004608154, 0.17016136646270752, -0.34795495867729187, 0.04636942967772484, -0.884742259979248, -0.18735522031784058, 0.6686131954193115, 0.821314811706543, 0.9476865530014038, 0.385453462600708, -0.5008468627929688, -0.6045399308204651, -0.34071213006973267, 0.7401039004325867, 1.849050760269165, -0.2133301943540573, -0.2350863218307495, -0.7108803391456604, 0.09664541482925415, -0.7099597454071045, 0.7730354070663452, -0.5418477058410645, -0.6208541989326477, -0.5114109516143799, -0.842729926109314, 0.4685628414154053, 0.09993594139814377, 0.9222912192344666, -0.33812177181243896, -0.06739643216133118, -0.231382355093956, 0.20156587660312653, -0.970751941204071, -0.5494860410690308, -0.01253483071923256, -0.32762640714645386, 0.16957077383995056, 0.06139479950070381, 0.12561455368995667, -0.19081738591194153, -0.3415297567844391, 1.0414459705352783, -0.48981401324272156, -0.44866564869880676, 0.3576904833316803, 0.3732583820819855, -0.561534583568573, -0.9581892490386963, 0.40460798144340515, 0.17769946157932281, -0.19373956322669983, 0.433165967464447, 0.6888836622238159, 0.010114360600709915, 0.24496665596961975, -0.2962431013584137, 0.22306692600250244, -0.10922936350107193, -0.21668797731399536, 0.21779794991016388, 0.02005455642938614, 0.17162203788757324, -1.3348575830459595, 1.0401935577392578, -0.034450747072696686, -0.5663158893585205, 0.10666875541210175, -0.5646955966949463, -0.3984161615371704, 0.5275436639785767, -0.7491186857223511, -0.7326774001121521, -0.903083860874176, 0.4112494885921478, -0.5344821810722351, -0.14798706769943237, 0.09750725328922272, 0.2665301561355591, 0.6759794354438782, 0.2926296293735504, 0.47380900382995605, -0.08205445110797882, -0.07934959977865219, 0.7372101545333862, -1.0879414081573486, 0.531559944152832, 0.6319071650505066, 0.4194801151752472, 0.10609054565429688, -0.4686146378517151, -0.8270212411880493, -0.7967638969421387, -0.3174758851528168, -0.23765158653259277, -0.4475330710411072, 0.7873777151107788, -0.7035332918167114, -0.9289308786392212, 0.12942562997341156, -0.9735496640205383, -0.5545998811721802, 0.31472277641296387, -0.41205576062202454, -0.04947099834680557, -1.236370325088501, -1.0324492454528809, -0.4633777141571045, -0.49799859523773193, -0.5509465932846069, 0.3443267345428467, -0.031397122889757156, -0.5173042416572571, -0.6840901374816895, -0.0775919258594513, -0.3496326208114624, 0.9686108231544495, -0.9837905764579773, 0.8882639408111572, -0.2214611917734146, -0.25722745060920715, -0.2782076597213745, 0.31656867265701294, 0.523698091506958, -0.3590756356716156, 0.09383290261030197, -1.0432451963424683, 0.025464681908488274, -0.10405445098876953, -0.3640071749687195, 0.5445795059204102, 0.3514676094055176, 0.6100476384162903, 0.3096531331539154, -0.4559359550476074, 0.0686817467212677, 1.3509987592697144, -0.555837094783783, 0.23589250445365906, -0.18036703765392303, 1.0169116258621216, 0.43135571479797363, -0.42564505338668823, 0.21676726639270782, 0.6578318476676941, 0.23303629457950592, 0.008875356055796146, 0.08399341255426407, -0.08398981392383575, -0.9167935252189636, 0.7176578044891357, 1.394526481628418, 0.14759810268878937, -0.04989944025874138, -1.0696468353271484, 0.8460322618484497, -0.9700658917427063, -1.1024243831634521, 0.44704142212867737, 0.8492704629898071, 0.16358396410942078, -0.35905855894088745, -0.7139892578125, -0.4555142819881439, 0.5203412175178528, 0.2509835362434387, -0.12766210734844208, -0.7715448141098022, 0.22606055438518524, 0.3929787278175354, 0.25007107853889465, 0.4894660711288452, -0.4348749816417694, 0.9367198944091797, 14.631482124328613, 0.8154833912849426, -0.03462549299001694, 1.028651475906372, 0.5697859525680542, 0.12991762161254883, -0.5545465350151062, -0.10328461974859238, -1.449981927871704, -0.2199096381664276, 1.34778892993927, -0.3089843988418579, 0.6672599911689758, 0.1481856256723404, 0.2696899175643921, 0.4349082410335541, -0.47006553411483765, 0.24065110087394714, 0.5445746779441833, -1.263602614402771, 0.5045527815818787, 0.04311801865696907, 0.5420193672180176, 0.6251450777053833, 0.8188011646270752, 1.238930344581604, 0.8069905042648315, -0.6285059452056885, 0.38040971755981445, 0.24301989376544952, 0.6130411624908447, 0.08708133548498154, 0.10508538782596588, 0.7471382021903992, -0.8227744102478027, -0.31548362970352173, -0.719054102897644, -1.0827826261520386, 0.39887553453445435, -0.0905526727437973, -0.4250301718711853, -0.6302546858787537, -0.20916692912578583, 0.44026800990104675, 0.2543349862098694, -0.06235739588737488, -0.40818044543266296, 0.9615554809570312, 0.16414974629878998, 0.13235178589820862, 0.3864502012729645, 0.6619179844856262, 0.24659618735313416, -0.3867247700691223, 0.12233363091945648, 0.1629437357187271, 0.07050526142120361, 0.6238856315612793, -0.5576891303062439, -0.05284770950675011, -0.3411400318145752, -0.1818833351135254, -0.039279527962207794, 0.5720202922821045, 0.5579212307929993, 0.2065528780221939, -0.47154897451400757, 0.25893327593803406, 0.5523306131362915, 0.20608696341514587, -0.36315909028053284, 0.17072415351867676, 0.04013838618993759, -0.05366332828998566, -0.00714902626350522, 0.4846264123916626, -0.03834819048643112, -0.6807060837745667, -0.9796411991119385, -0.20109610259532928, 0.36136242747306824, -1.2183740139007568, -0.887178897857666, 0.6570035219192505, -0.12868614494800568, -0.043781861662864685, -0.02805480919778347, -0.506456732749939, -0.4126119017601013, 0.2716602087020874, -1.024425745010376, -0.8848508596420288, 0.09560787677764893, -0.25659316778182983, 0.22084850072860718, -0.25010189414024353, 1.2299436330795288, 0.16655868291854858, -0.29126039147377014, 0.10581184178590775, -0.12064892053604126, 0.11113306134939194, -0.2527981102466583, -0.5559478998184204, 1.0539523363113403, 0.28250643610954285, -0.2860663831233978, 0.32742512226104736, 0.3198685050010681, 0.11994517594575882, -0.6744257807731628, -0.25104421377182007, 0.8929689526557922, -0.9163636565208435, -0.1181393712759018, -0.9335007667541504, -0.989348292350769, 0.4457252025604248, 0.8832069039344788, -0.7230731248855591, -0.0562116838991642, 0.5508320331573486, -0.6386059522628784, 0.09312073886394501, -0.37339362502098083, 0.20753420889377594, 0.5430576205253601, -0.9204634428024292, -0.624519944190979, -0.011957395821809769, 0.6409136652946472, -0.6803401708602905, -0.3843028247356415, -0.08093554526567459, -0.01734180375933647, -0.13232211768627167, 0.7391724586486816, -0.6065706014633179, 0.7905048727989197, 1.29192316532135, 0.03007836453616619, -0.8954972624778748, -0.22604894638061523, -0.8029451966285706, 0.5734284520149231, 0.5518772602081299, 0.6865547299385071, -0.4658658504486084, -0.031147124245762825, 1.1143049001693726, 0.04509790986776352, 0.03325910493731499, -0.698992133140564, -0.4939855933189392, 0.5299689173698425, -0.6854620575904846, 0.12469743937253952, -0.09401709586381912, -0.24025221168994904, 0.4024568498134613, 0.2349827140569687, 0.7118694186210632, -0.0563763827085495, -0.9640849828720093, 0.39205238223075867, 0.12014422565698624, -0.025437969714403152, -0.39663904905319214, -0.37452152371406555, -1.455341100692749, 0.2850463390350342, -1.2201992273330688, -0.06362703442573547, -1.031867504119873, -0.352711945772171, 0.44490286707878113, -0.2207465022802353, -0.05364242196083069, 0.49651703238487244, -0.2731512486934662, -0.28771308064460754, -0.6974532008171082, -0.23978261649608612, 0.8386570811271667, 0.730458676815033, -0.6926707625389099, 0.19956320524215698, -0.10931523144245148, 0.036918848752975464, 0.45372170209884644, 0.4680704176425934, -0.6993519067764282, -1.115540862083435, -1.4672940969467163, 0.31474289298057556, 0.015083801001310349, 0.3779618442058563, -0.581267237663269, 0.6811820864677429, 0.5011838674545288, -0.5101006031036377, 0.07349126040935516, 0.37868863344192505, -0.5414504408836365, -0.5719341039657593, 0.1643127053976059, -0.9244905710220337, 0.0875786542892456, 0.21174892783164978, -0.4291504919528961, -0.17789791524410248, 0.6420383453369141, 0.12259125709533691, -1.3623220920562744, -0.9963419437408447, 0.5069681406021118, -0.887718915939331, 0.36775586009025574, -0.14660699665546417, -0.028490830212831497, -1.0484987497329712, -0.5250967741012573, -0.4020105004310608, 0.2285199761390686, -0.5392236709594727, 1.3780145645141602, 0.4803558588027954, -0.9441277384757996, -0.1946207880973816, 0.25260114669799805, -0.03760397806763649, -0.5174266695976257, 1.0278507471084595, 0.22112378478050232, 0.28439924120903015, 0.7343140840530396, 0.4793858230113983, 0.4280165433883667, -0.8032941818237305, 0.07504237443208694, 1.015693187713623, -0.34502631425857544, -0.2874116897583008, 1.2138985395431519, 0.20161865651607513, -1.2524319887161255, -0.07363051176071167, -0.8431066274642944, -0.40361472964286804, -0.2196969836950302, 0.5600247979164124, 0.0873575434088707, -0.235622838139534, -0.03275584056973457, -0.2053583562374115, 0.18402357399463654, -0.13693571090698242, -0.778712809085846, 0.45275378227233887, -0.2855333089828491, -0.13878412544727325, 0.8423982262611389, 0.8878220915794373, -0.8310595750808716, -0.6159526109695435, -0.501352846622467, -0.36363083124160767, 0.0451330803334713, 0.6239418983459473, -0.3345290720462799, -0.6484083533287048, 0.998447597026825, 0.5278871059417725, 0.4788415729999542, -0.014056176878511906, 0.015842100605368614, -0.18173950910568237, 0.4936874210834503, 0.2843463718891144, -0.626057505607605, -0.46629396080970764, 1.4185550212860107, 1.2144269943237305, -0.9741707444190979, -0.11977677792310715, -0.27922892570495605, -0.8281415700912476, 0.7442415952682495, 0.6696948409080505, -0.15970134735107422, 1.1166685819625854, -0.3297024369239807, 0.17243237793445587, 0.10492867976427078, -1.4645692110061646, -0.3212912082672119, 0.6532607674598694, 0.7756280899047852, 0.892004668712616, 0.15127739310264587, 0.158266082406044, 0.705593466758728, -0.3033809959888458, 0.18430320918560028, 0.8166194558143616, -0.2531725764274597, -0.3558933436870575, 0.3994491994380951, 0.5140760540962219, 0.7054049968719482, -0.7460273504257202, -0.8987259268760681, 0.19950278103351593, 0.6189001202583313, -0.13685674965381622, 0.8479291796684265, 0.8708716630935669, 0.12710779905319214, 0.6155596375465393, 0.35673001408576965, 0.38192498683929443, -0.6602206230163574, -0.18752546608448029, -0.1770639717578888, -0.5731747150421143, -0.1023080125451088, 0.0904262512922287, -0.3758655786514282, -0.49350592494010925, -0.33443304896354675, 0.3542953431606293, -0.42058685421943665, 0.23271341621875763, 1.5436928272247314, 0.2637099027633667, 0.5887959599494934, -0.3311186134815216, -0.2490484118461609, -0.4515182673931122, -0.9313477873802185, 0.14240509271621704, -0.490427702665329, -0.30864590406417847, -0.20609095692634583, -0.1523580253124237, 0.07760589569807053]}, "authors": [{"authorId": "2135734748", "name": "Thomas Wang"}, {"authorId": "145625142", "name": "Adam Roberts"}, {"authorId": "80424302", "name": "Daniel Hesslow"}, {"authorId": "1379806208", "name": "Teven Le Scao"}, {"authorId": "3351938", "name": "Hyung Won Chung"}, {"authorId": "46181066", "name": "Iz Beltagy"}, {"authorId": "143945447", "name": "Julien Launay"}, {"authorId": "2402716", "name": "Colin Raffel"}], "references": [{"paperId": "bb15f3727f827a3cb88b5d3ca48415c09b40a88f", "title": "What Language Model to Train if You Have One Million GPU Hours?"}, {"paperId": "e47da75675b9a3fe02ef1efadca39bc8cdfcdc17", "title": "Designing Effective Sparse Expert Models"}, {"paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e", "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"}, {"paperId": "1ed66e048bb025e75aa5ea660545285212e5341f", "title": "Scaling Up Models and Data with t5x and seqio"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "7cbc2a7843411a1768ab762930707af0a3c33a19", "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"}, {"paperId": "b3848d32f7294ec708627897833c4097eb4d8778", "title": "LaMDA: Language Models for Dialog Applications"}, {"paperId": "842104ef0575823498f26cdd57b4b4dba655df9e", "title": "ZeroPrompt: Scaling Prompt-Based Pretraining to 1, 000 Tasks Improves Zero-Shot Generalization"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "17dd3555fd1ccf1141cf984347fa1b3fd6b009ca", "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization"}, {"paperId": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79", "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"}, {"paperId": "a6d8d04962f84ae6225e72723869a002b9fc8036", "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers"}, {"paperId": "ff0b2681d7b05e16c46dfb71d980cc2f605907cd", "title": "Finetuned Language Models Are Zero-Shot Learners"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "5aab57cc0530560d82c74c055f664280619d7e81", "title": "PROST: Physical Reasoning about Objects through Space and Time"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "7a16d9b4e04300d034502dc7dd58428714594e2c", "title": "Carbon Emissions and Large Neural Network Training"}, {"paperId": "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"}, {"paperId": "782bb86fc10dc134427d63d035d3559baffc371b", "title": "Moving beyond \u201calgorithmic bias is a data problem\u201d"}, {"paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb", "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e", "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling"}, {"paperId": "70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc", "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "d6599d4dfaeb78bea1f975db683aa653e26b3987", "title": "Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56", "title": "REALM: Retrieval-Augmented Language Model Pre-Training"}, {"paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45", "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"}, {"paperId": "112fd54ee193237b24f2ce7fce79e399609a29c5", "title": "The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives"}, {"paperId": "0c3c4c88c7b07596221ac640c7b7102686e3eae3", "title": "PubMedQA: A Dataset for Biomedical Research Question Answering"}, {"paperId": "c7462e0ee928f095a7fc40b91f1e7557d283ae8e", "title": "Release Strategies and the Social Impacts of Language Models"}, {"paperId": "81f5810fbbab9b7203b9556f4ce3c741875407bc", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans"}, {"paperId": "92343cecdc990380de362b969eec60081959f507", "title": "Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures"}, {"paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8", "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"}, {"paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88", "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"}, {"paperId": "9770fff7379a7ab9006b48939462354dda9a2053", "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions"}, {"paperId": "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad", "title": "HellaSwag: Can a Machine Really Finish Your Sentence?"}, {"paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc", "title": "Cross-lingual Language Model Pretraining"}, {"paperId": "84a6d47676c2d2c1414d3893d09e47d33906fb1c", "title": "WiC: 10, 000 Example Pairs for Evaluating Context-Sensitive Representations"}, {"paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca", "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"}, {"paperId": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention"}, {"paperId": "99ad0533f84c110da2d0713d5798e6e14080b159", "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"}, {"paperId": "451d4a16e425ecbf38c4b1cca0dcf5d9bec8255c", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"}, {"paperId": "54a13bcc9613dcaa76fb25fbe96572f376cfcca9", "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"}, {"paperId": "88bb0a28bb58d847183ec505dda89b63771bb495", "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"}, {"paperId": "8691706ad0cf5e83969658b2e6bfffdc379440c9", "title": "Generating Wikipedia by Summarizing Long Sequences"}, {"paperId": "932a5de79d8a8ebb75ea0c43493450fd9922e738", "title": "Crowdsourcing Multiple Choice Science Questions"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"}, {"paperId": "636a79420d838eabe4af7fb25d6437de45ab64e8", "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"}, {"paperId": "510e26733aaff585d65701b9f1be7ca9d5afc586", "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"}, {"paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5", "title": "Layer Normalization"}, {"paperId": "5ed791f810da580c78df6a052c6b9f2e258f6b0a", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context"}, {"paperId": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"}, {"paperId": "13fe71da009484f240c46f14d9330e932f8de210", "title": "Long Short-Term Memory-Networks for Machine Reading"}, {"paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "title": "Deep Residual Learning for Image Recognition"}, {"paperId": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "title": "Semi-supervised Sequence Learning"}, {"paperId": "687bac2d3320083eb4530bf18bb8f8f721477600", "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"}, {"paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs"}, {"paperId": "5cfbbf3cdff0f905874589bcd21b2646340a5447", "title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"}, {"paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f", "title": "The Winograd Schema Challenge"}, {"paperId": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "title": "\u201cCloze Procedure\u201d: A New Tool for Measuring Readability"}, {"paperId": null, "title": "2022) estimates the carbon emissions of their model training to be 240.5 tCO2e based on the net tCO2e per MWh of the datacenter during training and the energy usage of TPUv4 chips"}, {"paperId": null, "title": "Scaling language modeling with pathways"}, {"paperId": "1403e6b9adf7712c35ae56327d52fe54603b87e1", "title": "Few-shot Learning with Multilingual Language Models"}, {"paperId": null, "title": "2021], and forego systematic LM adaptation before multitask finetuning"}, {"paperId": null, "title": "2021] report for the original T5 model training"}, {"paperId": null, "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"}, {"paperId": null, "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? What changes can large-scale language models bring?"}, {"paperId": null, "title": "Flax: A neural network library and ecosystem for JAX, 2020"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": "92e121c6e114fe3cfb89370df03847c66a9b4e28", "title": "An Adversarial Winograd Schema Challenge at Scale"}, {"paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "title": "Improving Language Understanding by Generative Pre-Training"}, {"paperId": null, "title": "JAX: composable transformations of Python+NumPy programs"}, {"paperId": null, "title": "First quora dataset release: Question pairs"}, {"paperId": "a9075f6332542e12b2bf3cdbdb3a6ed44733fb41", "title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"}, {"paperId": "475354f10798f110d34792b6d88f31d6d5cb099e", "title": "Automatically Constructing a Corpus of Sentential Paraphrases"}, {"paperId": "e808f28d411a958c5db81ceb111beb2638698f47", "title": "The PASCAL Recognising Textual Entailment Challenge"}, {"paperId": null, "title": "Challenge Closed-Book Question Answering Easy Closed-Book Question Answering GLUE MRPC"}, {"paperId": "13167f9cd8c7906ca808b01d28dca6dd951da8a5", "title": "of the Association for Computational Linguistics"}, {"paperId": null, "title": "A framework for few-shot language model evaluation"}]}