{"paperId": "9a5c83d28a407c25eb1e133306f49597bc16c0c6", "title": "You Need to Pay Better Attention", "abstract": "Scaled Dot Product Attention (SDPA) is the backbone of many modern deep-learning models. It is so versatile that it has been used in natural language, vision, and multi-modal domains with very little change compared to its original formulation. This paper discusses why the current formulation is inefficient by delving into the mathematical details of the attention mechanism. We propose three improvements to mitigate these inefficiencies, thereby, introducing three enhanced attention mechanisms: Optimised, Efficient, and Super Attention. Optimised and Efficient Attention have one and two matrix multiplications fewer per head, respectively, and 25% and 50% fewer parameters, respectively, than standard SDPA, but perform similarly to standard SDPA in both vision and natural language tasks. They can be used in all applications where SDPA is used while offering smaller model sizes and faster training and inference without noticeable loss in performance. Super Attention introduces a new linear transformation on the values, transforming them from the left. It outperforms standard SPDA on vision and natural language tasks by up to 17% while having one fewer matrix multiplication per head and 25% fewer parameters than standard SDPA. Consequently, it is also faster than standard SDPA. Super Attention is ideal in applications where the attention layer's context length is fixed, such as Vision Transformers. In addition to providing mathematical reasoning, we evaluate the presented attention mechanisms on several datasets including MNIST, CIFAR100, ImageNet, IMDB Movie Reviews, and Amazon Reviews datasets, as well as combined Europarl and Anki English-Spanish datasets for neural machine translation.", "venue": "arXiv.org", "year": 2024, "citationCount": 1, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "Why the current formulation of Scalable Dot Product Attention is inefficient is discussed by delving into the mathematical details of the attention mechanism, and three improvements to mitigate these inefficiencies are proposed."}, "embedding": {"model": "specter_v2", "vector": [0.11177720874547958, 0.867195725440979, -0.3046587407588959, -0.3556612432003021, -0.09273512661457062, 0.2584671080112457, 0.9204643964767456, -0.3353355824947357, -0.5259763598442078, -0.12461308389902115, 0.4231575131416321, 0.4495314955711365, 0.38805922865867615, 0.04162706807255745, -0.07049796730279922, 0.03300194442272186, -0.3791930675506592, 0.39758238196372986, -0.12088315188884735, -0.553428590297699, 0.15709896385669708, -0.8734866380691528, -0.9453049898147583, -0.16182130575180054, -0.1675042062997818, 0.8050940632820129, 0.46262747049331665, 0.6705063581466675, -0.21934548020362854, 0.549548327922821, 0.3065515458583832, -0.6740259528160095, 0.28186941146850586, -0.029208948835730553, -0.46169087290763855, -0.11183080822229385, 0.423140287399292, -0.6483280062675476, -0.18152976036071777, 1.1835733652114868, -0.37691280245780945, 0.28059065341949463, 0.4336884021759033, -0.7100054025650024, -0.7860770225524902, 0.7101557850837708, 0.32840195298194885, 1.064036250114441, -0.2570013999938965, -0.37455809116363525, 1.5927623510360718, -1.3918054103851318, 0.1791144162416458, 1.582465410232544, 0.11822811514139175, 0.4502791464328766, -0.10930132120847702, -0.14646998047828674, 0.8642471432685852, 0.11874308437108994, -0.6026641130447388, 0.11008023470640182, 0.20861631631851196, 0.16036848723888397, 1.8538345098495483, -0.6307188868522644, -0.25798556208610535, 0.37893861532211304, 0.010538408532738686, 1.2623518705368042, 0.17023858428001404, -0.7110422849655151, -0.3992643654346466, 0.2934425175189972, 0.6237437129020691, 0.5137131810188293, -0.39581310749053955, 0.28537794947624207, -0.8678125143051147, 0.2427801936864853, 0.5988796949386597, 0.20246872305870056, 0.38046273589134216, -0.1760045289993286, -0.6336590647697449, 0.9399151802062988, 0.6257361173629761, 0.5856344103813171, -0.6418481469154358, 1.0843915939331055, 0.20123255252838135, 0.15231911838054657, -0.11369790136814117, 0.4877605438232422, -0.03916919603943825, 0.5898317098617554, -0.30603477358818054, 0.10212195664644241, -0.34690728783607483, 1.0967602729797363, 0.07607845216989517, 0.06343962997198105, -0.55156010389328, 0.047131821513175964, 1.2505483627319336, 0.3067314028739929, 0.19765235483646393, -0.9182681441307068, 0.10585358738899231, -0.7805784344673157, 0.14476674795150757, -1.1377313137054443, 0.002888680901378393, -0.2301441878080368, -0.6232538223266602, -0.9005171656608582, -0.3657897114753723, 0.20265278220176697, -0.5317565202713013, 0.6575897932052612, -0.475422739982605, -0.2764520049095154, -0.2706374228000641, 0.856040358543396, 0.3833170235157013, 0.5849882960319519, 0.3581031858921051, 0.4376162588596344, 1.0139251947402954, -0.9302821159362793, -0.8568636775016785, -1.2217603921890259, 0.061813127249479294, -0.29205265641212463, 0.4085327386856079, 0.1314747929573059, -1.1917893886566162, -0.9963287711143494, -1.066627025604248, -0.12113456428050995, -0.6980957388877869, 0.22786475718021393, 1.1977118253707886, 0.022694554179906845, -0.9384579062461853, 0.7279631495475769, -0.303701788187027, -0.16095323860645294, 0.5582141280174255, 0.47520938515663147, 0.39449986815452576, -0.40767160058021545, -1.1927587985992432, 0.6335574984550476, 0.04672326520085335, -0.4845598042011261, -0.03714779391884804, -0.23847262561321259, -1.156174659729004, -0.04174577817320824, 0.14718542993068695, -0.6894207000732422, 1.234370231628418, -0.6825467944145203, -1.0443897247314453, 0.7548975348472595, -0.18482285737991333, 0.02199988253414631, -0.09891603887081146, -0.291159063577652, -0.44504472613334656, -0.30906248092651367, 0.01052736584097147, 0.8622590899467468, 0.7959491014480591, 0.21008411049842834, -0.23676131665706635, 0.2166108340024948, 0.05977606400847435, -0.003941863775253296, -0.5391371250152588, 1.3282057046890259, -0.26922062039375305, -0.02761712297797203, 0.10680759698152542, 0.7680438756942749, 0.3763894736766815, 0.06755461543798447, -0.13865011930465698, -1.3112623691558838, 0.654923677444458, 0.5581789016723633, 0.7182630300521851, -0.8800984025001526, -0.45158764719963074, -0.3480025827884674, 0.0546395368874073, -0.1321282982826233, -0.6320993900299072, 0.02351311594247818, -0.5068751573562622, 0.1796570122241974, 0.04570605978369713, -0.8719958066940308, 0.39460742473602295, -0.0763237476348877, -0.3243117034435272, -0.010467126034200191, 0.6063396334648132, 1.0126111507415771, -0.7919893860816956, 0.030325183644890785, -0.012091119773685932, 0.2664177119731903, -0.6938120126724243, 1.0624947547912598, -0.2153874784708023, -0.18250541388988495, -0.23449520766735077, -0.44156986474990845, 0.302773654460907, -0.47605663537979126, -0.02976909466087818, -0.6323419809341431, -0.13106341660022736, 0.58934086561203, -0.49226072430610657, 1.1727336645126343, 0.02858244627714157, 0.8775534629821777, -0.1391768753528595, -0.8422430157661438, 0.4280695915222168, -0.17323726415634155, -0.40412893891334534, -0.7094674706459045, 0.5229690670967102, 0.09683632850646973, -0.7131089568138123, 0.2114562690258026, 0.5414831638336182, 0.884698212146759, -0.32482078671455383, 0.43407249450683594, 0.623845636844635, -0.3568350672721863, 0.11170899868011475, 0.5406816005706787, 0.15764233469963074, 0.5269056558609009, 0.5790500044822693, -0.21470963954925537, 0.35629528760910034, -0.43930208683013916, -0.22113241255283356, 0.36461442708969116, 0.3190561532974243, 0.8839682936668396, 0.573503315448761, -0.7930951714515686, -0.4671678841114044, 0.1222834587097168, 0.8932439088821411, 1.7840086221694946, 0.040541570633649826, 0.2952849864959717, -0.5277460217475891, -0.18709571659564972, -0.5469617247581482, -0.1290438324213028, -0.5744683146476746, -0.18894769251346588, -0.5421918630599976, -1.2118033170700073, 0.3495299220085144, 0.6675154566764832, 1.1508814096450806, -0.40530312061309814, -0.44382768869400024, -0.47566330432891846, 0.5594308376312256, -0.7099270224571228, -1.0613713264465332, 0.4317874014377594, -0.018531134352087975, 0.03524177521467209, -0.21912260353565216, -0.2876579761505127, 0.27547499537467957, -0.3316221833229065, 1.0675112009048462, -0.8071930408477783, -0.10476208478212357, 0.27380895614624023, 0.5229280591011047, -0.9758366942405701, -0.3622352182865143, -0.04406828060746193, 0.16976694762706757, 0.14108064770698547, 0.8700416684150696, 0.7740771174430847, -0.0971987172961235, 0.3263815939426422, -0.3489605784416199, 0.04947127774357796, 0.09733010083436966, 0.20701301097869873, 0.9399352669715881, -0.492701917886734, 0.07736989855766296, -0.9572755694389343, 0.6713818907737732, 0.12925583124160767, -0.6927610039710999, -0.12912511825561523, -0.7669401168823242, -0.2706691324710846, 0.10392951220273972, -0.6723371148109436, -0.3247148096561432, -0.5682482123374939, 0.5394583344459534, -0.24986465275287628, -0.269108384847641, 0.12199573218822479, 0.39557474851608276, 0.05440562218427658, 0.37356504797935486, -0.08101112395524979, 0.3964003324508667, 0.14174215495586395, 0.5903648138046265, -0.6921617984771729, 0.6678813695907593, 0.45577993988990784, -0.12349439412355423, -0.07739012688398361, -0.18737797439098358, -0.43998149037361145, -0.6710997819900513, -0.2470492273569107, -0.2965543866157532, -0.16895197331905365, 0.5239410996437073, -0.5371391177177429, -0.6936759352684021, 0.13177555799484253, -1.2172274589538574, 0.02713104337453842, -0.07157864421606064, 0.17986829578876495, -0.11773406714200974, -1.0625278949737549, -0.7743934988975525, -0.33165860176086426, -0.5080980658531189, -0.8118917942047119, 0.005792147945612669, 0.466936320066452, -0.2583097517490387, -0.4081220328807831, -0.26268503069877625, -0.48480233550071716, 1.3241652250289917, -0.6392155885696411, 0.5408611297607422, -0.2911774516105652, -0.21708250045776367, -0.47752058506011963, -0.24953220784664154, 0.9021546244621277, -0.16740618646144867, 0.11440323293209076, -0.8796402215957642, 0.722449779510498, -0.13811108469963074, -0.35185670852661133, 0.11520310491323471, 0.3363635241985321, 0.290058434009552, 0.14133384823799133, 0.024603936821222305, -0.08301600813865662, 1.14870023727417, -0.917943000793457, 0.11833761632442474, 0.2459387630224228, 1.0260006189346313, 0.35293757915496826, 0.23254458606243134, 0.5013856887817383, 0.5427082777023315, 0.34379473328590393, 0.601580798625946, -0.21038635075092316, -0.5276914834976196, -0.29778143763542175, 0.5409224629402161, 0.9809529781341553, 0.022228099405765533, 0.25898420810699463, -1.0270172357559204, 0.590389609336853, -1.037027359008789, -0.8227657079696655, 0.46436482667922974, 0.681460440158844, 0.18841150403022766, -0.25607216358184814, -0.23104605078697205, -0.5697606801986694, 0.8051617741584778, 0.33363160490989685, -0.48360323905944824, -0.9470135569572449, -0.25816094875335693, 0.4391142725944519, 0.20454835891723633, 0.6717767715454102, -0.44433555006980896, 0.5864472389221191, 15.120565414428711, 0.537903368473053, -0.22773918509483337, 0.664397120475769, 0.7535155415534973, 0.23429715633392334, -0.20567621290683746, -0.4157763123512268, -1.1369701623916626, -0.11451958119869232, 0.8607230186462402, 0.22164557874202728, 0.5555847883224487, 0.48431387543678284, -0.19495238363742828, 0.13192664086818695, -0.7518854141235352, 1.1846562623977661, 0.8197977542877197, -1.1649935245513916, 0.013288567773997784, 0.20284855365753174, 0.24128933250904083, 0.7218456864356995, 0.9461127519607544, 0.5115785002708435, 0.29985687136650085, -0.3732507526874542, 0.5270628929138184, 0.41355448961257935, 0.7110315561294556, 0.148725226521492, 0.656540036201477, 0.2720317542552948, -0.34872767329216003, -0.08525454998016357, -0.8217210173606873, -1.4129226207733154, -0.10059428215026855, 0.08731052279472351, -0.047407329082489014, -0.7598611116409302, -0.20680870115756989, 0.8747789859771729, 0.11098969727754593, -0.058062441647052765, -0.3337220847606659, 0.1352631002664566, -0.3370330035686493, -0.6475746035575867, 0.4451247453689575, 0.5206544995307922, 0.10040837526321411, -0.12494945526123047, -0.08765608072280884, -0.10318642109632492, -0.28335317969322205, 1.0869386196136475, -0.4709303677082062, -0.29050880670547485, -0.08596976101398468, -0.32367298007011414, -0.32471153140068054, 0.9325282573699951, 0.7187234163284302, 0.17706434428691864, -0.6881065964698792, 0.3167644441127777, 0.2408156543970108, 0.02278216928243637, -0.5115637183189392, -0.30830150842666626, 0.2579810619354248, -0.46508222818374634, 0.12598906457424164, 0.5854631066322327, -0.2776881158351898, -0.4416625499725342, -0.9496802687644958, -0.03413083031773567, 0.4505126178264618, -1.1997281312942505, -0.9860156178474426, 0.8890713453292847, -0.3499215841293335, -0.3339719772338867, 0.2699182629585266, -0.9683185815811157, -0.25190502405166626, 0.6897351145744324, -1.4921258687973022, -0.5455290675163269, -0.04493161663413048, -0.12779857218265533, -0.23524251580238342, -0.13367189466953278, 1.0045276880264282, 0.20576201379299164, -0.27384117245674133, -0.1509534865617752, -0.6194220781326294, -0.09091024100780487, -0.043448351323604584, -0.8837751746177673, 0.6321173310279846, 0.3990965187549591, -0.17153345048427582, 0.0567215196788311, 0.23421159386634827, 0.6140459775924683, -0.6355243921279907, 0.2906202971935272, 0.8264523148536682, -0.910434901714325, -0.13109149038791656, -0.16162165999412537, -0.4403891861438751, 0.14728640019893646, 0.901380717754364, 0.1278492510318756, 0.08063643425703049, 0.02110634185373783, -0.7104276418685913, -0.3518919050693512, -0.774303138256073, -0.39568468928337097, -0.06157732009887695, -0.7076890468597412, -0.7325412034988403, -0.11001595109701157, 0.16713200509548187, -0.7211448550224304, -0.30190548300743103, -0.6663215160369873, -0.026051124557852745, -0.061994824558496475, 0.9667328596115112, -0.6834998726844788, 0.7274369597434998, 0.7141425609588623, -0.36752182245254517, -0.6911209225654602, -0.2696363925933838, -0.761409342288971, 0.02589673362672329, 0.16527919471263885, 0.6412267684936523, -0.4982740879058838, -0.07354073971509933, 1.065502643585205, 0.37227562069892883, -0.40487489104270935, -0.3048895001411438, -0.04608629271388054, -0.2224739044904709, -0.5591466426849365, 0.393892765045166, -0.11611083149909973, -0.1068340390920639, 0.4550126791000366, 0.31632503867149353, 0.42513519525527954, -0.20188146829605103, -0.6581614017486572, -0.10955830663442612, -0.0029009836725890636, -0.03324539214372635, -0.7497745752334595, -0.6738158464431763, -1.8482400178909302, -0.3436417877674103, -0.7403497099876404, -0.15358306467533112, -1.050817847251892, -0.25115084648132324, 0.48201632499694824, -0.5648892521858215, 0.35162553191185, 0.27837473154067993, 0.15058410167694092, -0.313146710395813, -0.5437871813774109, -0.7138018608093262, 0.6420470476150513, 0.903587281703949, -0.8470344543457031, 0.29503366351127625, -0.20754961669445038, -0.4557986855506897, 0.5269410610198975, 0.36592912673950195, -0.39479193091392517, -0.5038760900497437, -1.5367467403411865, 0.8992002010345459, -0.44235941767692566, 0.1923140585422516, -0.5292440056800842, 0.8407787084579468, 0.44122615456581116, -0.24453593790531158, -0.08272914588451385, 0.470529168844223, -0.890013575553894, -0.8709985017776489, -0.009804709814488888, -0.8140665292739868, 0.03790998458862305, -0.13868330419063568, -0.5808361172676086, -0.4124382734298706, 0.7052559852600098, 0.21092243492603302, -0.8823217749595642, -0.8379250764846802, 0.4906854033470154, -0.34108835458755493, 0.12756706774234772, -0.1752917319536209, -0.2566169500350952, -1.3679364919662476, -0.3216955363750458, -0.07910337299108505, 0.25832173228263855, -0.8132604360580444, 0.7028292417526245, 0.3691166341304779, -1.215598464012146, 0.1342002898454666, 0.4340123236179352, 0.04525011405348778, -0.1710612028837204, 0.20038004219532013, 0.622779369354248, -0.37234097719192505, 0.580349326133728, -0.09305156767368317, 0.21614933013916016, -0.5756574273109436, 0.2555886209011078, 0.8240399360656738, 0.10684895515441895, -0.16961337625980377, 1.2157433032989502, -0.32597312331199646, -0.7693988084793091, 0.24935811758041382, -1.0453015565872192, -0.6668461561203003, 0.020832493901252747, 0.9421101212501526, 0.10356605052947998, -0.21457889676094055, -0.23425909876823425, -0.686615526676178, 0.3842143714427948, 0.00040366611210629344, -0.3284900486469269, 0.5205714106559753, -0.36815837025642395, -0.4762797951698303, 0.42273053526878357, 0.9472631812095642, -0.865789532661438, -0.9045207500457764, -0.9275796413421631, -0.4443337917327881, -0.18427008390426636, 0.45117315649986267, -0.531640350818634, -0.8670641183853149, 1.2207125425338745, 0.6472229957580566, 0.5502219200134277, 0.3664602041244507, 0.28507542610168457, -0.14089801907539368, 0.6839665770530701, -0.035214006900787354, -0.7517415881156921, -0.3196885287761688, 1.3496208190917969, 1.3105759620666504, -1.020646095275879, 0.31278562545776367, -0.3825077712535858, -1.0804656744003296, 0.6451049447059631, 0.3225577175617218, -0.31907492876052856, 0.794055163860321, -0.4911188781261444, -0.09234650433063507, 0.08108869194984436, -0.5989252328872681, -0.42301440238952637, 0.7496097087860107, 1.2304774522781372, 0.430459588766098, 0.26489099860191345, 0.1433224231004715, 1.0741362571716309, 0.3204778730869293, -0.022840121760964394, 0.5235650539398193, -0.08037152141332626, -0.37548187375068665, 0.22316467761993408, -0.06646370887756348, 0.5263197422027588, -0.6883923411369324, -0.5425527691841125, 0.19899526238441467, 0.6595628261566162, 0.3038792908191681, 0.39342060685157776, 0.7087032198905945, 0.38533246517181396, 0.5481927990913391, -0.09262058138847351, 0.5016692280769348, -0.5456201434135437, -0.09176433831453323, 0.03446086868643761, -0.6118817925453186, -0.13569414615631104, -0.6091225743293762, -0.7042003870010376, -0.4232693612575531, -0.14027251303195953, 0.22963477671146393, -0.5762356519699097, 0.45403072237968445, 0.9563231468200684, 0.5949438810348511, 0.6997495293617249, -0.2972487807273865, -0.5803011059761047, -0.3677673041820526, -0.8381180167198181, -0.06310585886240005, -0.5125048160552979, -0.11413497477769852, -0.4763795733451843, -0.33748921751976013, -0.2606147229671478]}, "authors": [{"authorId": "2210799544", "name": "Mehran Hosseini"}, {"authorId": "2166503863", "name": "Peyman Hosseini"}], "references": [{"paperId": "7754ac3e8ff1286f17593159781487543cdddaba", "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns"}, {"paperId": "104b0bb1da562d53cbda87aec79ef6a2827d191a", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models"}, {"paperId": "823ca4778e1027f2f0b356df051d762dcecaaba0", "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "b5e71069f091d52f474a2928ed07b6546157af82", "title": "Towards Accurate Post-Training Quantization for Vision Transformer"}, {"paperId": "dfacf0c04048c0a8c105976e9d2237ee269843ce", "title": "Understanding the effect of sparsity on neural networks robustness"}, {"paperId": "622428f5122ad12a40229e1768ecb929fd747ee7", "title": "Multimodal Learning With Transformers: A Survey"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "7a7a4f41f9ca5682b1444140799ca4dde44352f5", "title": "Overcoming Oscillations in Quantization-Aware Training"}, {"paperId": "56f6df1ac3479a8f374e87b038573967210081d4", "title": "Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "a8ca46b171467ceb2d7652fbfb67fe701ad86092", "title": "LoRA: Low-Rank Adaptation of Large Language Models"}, {"paperId": "cd395025cc4920a4fda76b7b98f12ae22e2e6513", "title": "Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution"}, {"paperId": "3cbe314cc5407a6c3249815b5173f22ea15173c2", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "805d950d6df9bdabd6b87d06de213909192341db", "title": "The carbon impact of artificial intelligence"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "631e3969b292f77035ca8d95a0b74347a336b627", "title": "Improved Gradient based Adversarial Attacks for Quantized Networks"}, {"paperId": "41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76", "title": "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"}, {"paperId": "ac99dd1deb9874bd65bc688bdf997b82c401b40b", "title": "Deep Neural Network Quantization via Layer-Wise Optimization Using Limited Training Data"}, {"paperId": "a4a2d99d1c237d0818971ec9205e89128c57fb02", "title": "Towards Interpretable Reinforcement Learning Using Attention Augmented Agents"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "59d0d7ccec2db66cad20cac5721ce54a8a058294", "title": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"paperId": "e7fd6848cb29ca221a7e17d823e06fb566f1f135", "title": "Mixed Precision Training"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "e8e9125704edbcf73999f2f452fe4a701163d6b6", "title": "RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism"}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "1c61f9ef06fe74505775a833ff849185757199e7", "title": "Learning Word Vectors for Sentiment Analysis"}, {"paperId": "d6e6c3243e9e4e6dd8f0bc783d7612b7d3863d5f", "title": "Matrix Analysis and Applied Linear Algebra"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "694b3c58712deefb59502847ba1b52b192c413e5", "title": "Europarl: A Parallel Corpus for Statistical Machine Translation"}, {"paperId": null, "title": "\u201cMnist handwritten digit database,\u201d"}, {"paperId": null, "title": "\u201cGemini: a family of highly capable multimodal models,\u201d"}]}