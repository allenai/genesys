{"paperId": "90b21dbad8969b74d704eed15a3d98722a88e464", "title": "Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models", "abstract": "Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.", "venue": "International Conference on Learning Representations", "year": 2021, "citationCount": 63, "influentialCitationCount": 13, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers and empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs."}, "embedding": {"model": "specter_v2", "vector": [0.23543505370616913, 0.7725062370300293, -0.33496612310409546, -0.09600710868835449, -0.1750338226556778, 0.23340633511543274, 0.5199716687202454, -0.6346215009689331, -0.32866671681404114, -0.2278435081243515, 0.4060211181640625, 0.1826174110174179, 0.5050426721572876, 0.15841057896614075, -0.44982337951660156, 0.021851567551493645, -0.8556851148605347, -0.04856798052787781, 0.21061521768569946, -0.2002643346786499, -0.18635061383247375, -0.41545569896698, -1.2473264932632446, 0.3225019872188568, -0.05064121633768082, 1.2458926439285278, 0.24475185573101044, 0.46110302209854126, -0.4872061014175415, 0.7253268957138062, 0.7350540161132812, -0.4095562696456909, 0.7248364090919495, 0.26752766966819763, -0.15287043154239655, -0.09089604020118713, 0.8631160855293274, -0.38427647948265076, -0.5674953460693359, 0.8861359357833862, -0.4389702379703522, 0.282548189163208, 0.16960272192955017, -0.6778314709663391, -0.006561328191310167, 0.7214413285255432, 0.31992998719215393, 0.8684583306312561, -0.9019426703453064, -0.261650413274765, 1.1659120321273804, -1.5352039337158203, -0.0323902890086174, 1.542281150817871, 0.9258055090904236, 0.12938107550144196, -0.45205724239349365, -1.0354845523834229, 0.7593386173248291, 0.08002915233373642, -0.8651402592658997, -0.4632736146450043, -0.22145552933216095, -0.05338568612933159, 2.00844407081604, -0.543796181678772, -0.005456672981381416, 0.6313439607620239, 0.04933701828122139, 1.0880833864212036, -0.08586632460355759, -0.4134175181388855, -0.41827481985092163, 0.029646582901477814, 0.2896937429904938, 1.1801934242248535, -0.2801298499107361, 0.18277965486049652, -1.2152516841888428, -0.008724285289645195, 0.543849527835846, 0.14487536251544952, 0.3289327323436737, -0.22058242559432983, -0.13053713738918304, 0.8832823634147644, 0.5656982064247131, 0.7846620082855225, -0.3683106303215027, 1.077190637588501, 0.4178083837032318, 0.39196574687957764, -0.2393825501203537, 0.5519518852233887, -0.19171087443828583, 0.5386639833450317, -1.1067156791687012, -0.081942118704319, 0.13771653175354004, 0.8123329281806946, 0.12577275931835175, 0.5712743401527405, -0.3952083885669708, 0.3458299934864044, 1.0856809616088867, 0.08304712176322937, 0.4201863706111908, -0.6689240336418152, 0.3722047209739685, -0.6982536911964417, -0.4883674085140228, -1.2285479307174683, -0.03854241222143173, -0.5452306270599365, -1.175220012664795, -1.127717137336731, -0.4138011634349823, 0.0706053078174591, -0.8180596828460693, 0.5994799733161926, -0.5334504842758179, 0.5100685954093933, -0.24907469749450684, 0.23394285142421722, 0.5766616463661194, 0.9388683438301086, 0.09124290198087692, 0.6156429052352905, 0.9711878895759583, -1.5487135648727417, -0.21076171100139618, -1.216934323310852, 0.2907719016075134, -0.12475096434354782, 0.1562306433916092, 0.19702401757240295, -1.0972893238067627, -1.090912103652954, -0.692837119102478, -0.017201196402311325, -0.3903694152832031, 0.23419500887393951, 1.255162239074707, 0.2887839674949646, -0.8698469400405884, 0.8371406197547913, -0.4383225440979004, -0.019997043535113335, 0.7878922820091248, 0.5938704609870911, -0.021523557603359222, -0.21129077672958374, -0.9035242795944214, 0.18727946281433105, 0.19461271166801453, -0.3631245195865631, -0.40288040041923523, -0.6509878039360046, -0.7623956799507141, 0.37522628903388977, 0.22294677793979645, -0.6155513525009155, 0.8363626003265381, -0.4711568355560303, -1.088882327079773, 0.5702810287475586, -0.11906830966472626, -0.37535718083381653, 0.05786699801683426, -0.07957498729228973, -0.3576505482196808, -0.12187667936086655, -0.5443318486213684, 0.8853321671485901, 0.8658591508865356, -0.057802338153123856, 0.08131498098373413, 0.3694014549255371, -0.2595907151699066, -0.29160743951797485, -0.5322062969207764, 0.677155077457428, -0.8064106106758118, -0.3502194583415985, 0.4132165014743805, 0.43418216705322266, -0.19392116367816925, -0.020978813990950584, -0.4342345893383026, -0.708226203918457, 0.677805483341217, 0.14256249368190765, 0.7217917442321777, -1.2943994998931885, -0.8892397880554199, 0.31615832448005676, 0.007988212630152702, -0.056851793080568314, -1.0572888851165771, 0.3794221580028534, -0.6073623895645142, 0.18756996095180511, 0.02871355414390564, -0.9582094550132751, -0.052783068269491196, -0.12940613925457, -0.7336288690567017, -0.08197277784347534, 0.3676694333553314, 0.7734901309013367, -0.4970937967300415, -0.06308058649301529, -0.11708299070596695, 0.6467846035957336, -1.504167079925537, 0.7731533050537109, -0.3422382175922394, -0.24806468188762665, 0.047451525926589966, -0.1696421355009079, -0.09286361187696457, -0.5190643072128296, 0.2444879114627838, -0.9333024024963379, 0.06184753403067589, 0.2818751931190491, -0.7469877004623413, 1.2238914966583252, -0.6521638035774231, 0.737922728061676, 0.265331506729126, -0.7647269368171692, 0.2705530524253845, 0.13081328570842743, -0.15901416540145874, -0.20432880520820618, 0.4744147062301636, 0.286390483379364, -0.7218692302703857, 0.30565011501312256, 0.445324569940567, 0.8632758855819702, 0.11763238906860352, 0.02268213778734207, 0.8771385550498962, -0.401626318693161, 0.509132981300354, 0.2798563539981842, 0.7120128273963928, -0.1372787207365036, 0.18451711535453796, -0.21886231005191803, 0.17827416956424713, -1.1051913499832153, -0.23113560676574707, 0.7222305536270142, 1.0220016241073608, 0.9315902590751648, 0.6550303101539612, -0.7908782958984375, -0.5032740831375122, 0.2689518928527832, 0.5536645650863647, 1.3547416925430298, -0.6534633040428162, -0.04421346262097359, -0.5608823299407959, -0.29732975363731384, -0.27582794427871704, -0.39765143394470215, -0.290211945772171, 0.11098543554544449, -0.3568723797798157, -1.0172920227050781, 0.46521008014678955, 0.12044265866279602, 1.190099835395813, -0.30119630694389343, 0.022185789421200752, -0.6260371208190918, 0.6470985412597656, -0.8841818571090698, -0.5736808776855469, 0.6007565259933472, -0.6160181760787964, -0.2846297323703766, 0.22770757973194122, -0.43236953020095825, 0.5044933557510376, -0.32454124093055725, 0.8748508095741272, -0.5291160941123962, -0.059355396777391434, 0.11496224254369736, 0.7656267285346985, -0.4275342524051666, -0.035828374326229095, 0.3359833061695099, 0.09660662710666656, 0.3731037378311157, 0.21051128208637238, -0.07303526997566223, -0.06477770954370499, -0.09971906989812851, -0.4331209659576416, 0.04672451317310333, 0.5856999158859253, 0.24706800282001495, 1.0868031978607178, -0.004727537743747234, -0.2008703500032425, -1.5286271572113037, 0.5487116575241089, -0.14262112975120544, -0.4433434307575226, -0.11518895626068115, -0.2613618075847626, 0.02243570238351822, 0.544309675693512, -0.7158597111701965, 0.0013001625193282962, -0.3810831308364868, 0.031726960092782974, -0.911686897277832, -0.05607856437563896, 0.3365534842014313, 0.6020539402961731, -0.43515127897262573, 0.5613023042678833, 0.1914519965648651, 0.49572259187698364, -0.12412482500076294, 0.43226557970046997, -1.2303229570388794, 0.5547168254852295, 0.16383278369903564, 0.3867800831794739, 0.26896747946739197, 0.07798478752374649, -0.9564262628555298, -0.7014685273170471, -0.3169877529144287, 0.13258440792560577, 0.09547190368175507, -0.18255896866321564, -0.9361236691474915, -0.6694099307060242, -0.24904558062553406, -0.5645097494125366, -0.3592541217803955, -0.06811646372079849, -0.07039100676774979, -0.09266161918640137, -1.1674531698226929, -1.1614435911178589, -0.24890105426311493, -0.7373082041740417, -1.1972993612289429, 0.37164804339408875, -0.09836837649345398, -0.25864794850349426, -0.4622654318809509, -0.6067765951156616, -0.6208840608596802, 1.2321500778198242, -0.5766407251358032, 0.6561706066131592, -0.09409830719232559, -0.21431581676006317, -0.39827314019203186, -0.3997425436973572, 1.0739808082580566, -0.7398088574409485, 0.31230971217155457, -0.9158951044082642, 0.21046417951583862, -0.10506987571716309, -0.4863072335720062, 0.4470965564250946, 0.478252112865448, 0.9946210980415344, -0.33201882243156433, -0.3574274182319641, 0.967011034488678, 1.3608530759811401, -1.1527631282806396, 0.08083983510732651, -0.059029918164014816, 0.9758105278015137, -0.02297006919980049, -0.8130905032157898, 0.5760326981544495, -0.014341024681925774, 0.038388289511203766, 0.2771281898021698, 0.06057104468345642, -0.4883890151977539, -0.6186657547950745, 0.2748720347881317, 1.485984206199646, 0.6968512535095215, 0.3417378067970276, -0.836857795715332, 0.5990522503852844, -0.8332781195640564, -0.7235602140426636, 0.6476073265075684, 0.2951951026916504, 0.2994120419025421, -0.14188548922538757, -0.4602718651294708, -0.35444170236587524, 0.19958873093128204, 0.5964856147766113, -0.24711674451828003, -0.45211508870124817, -0.3149692714214325, 0.6037236452102661, 0.6594556570053101, 0.45260992646217346, -0.32738229632377625, 0.2749786674976349, 14.703619956970215, 0.935527503490448, -0.3860991895198822, 0.6267924308776855, 0.926897406578064, -0.10034293681383133, -0.1347646564245224, -0.11282346397638321, -1.3953750133514404, -0.31296420097351074, 0.9970269799232483, 0.7045390009880066, 0.8641005754470825, 0.5513560175895691, -0.31076401472091675, 0.1949252039194107, -0.42114755511283875, 1.1991978883743286, 0.5911986827850342, -1.549117922782898, 0.30939480662345886, -0.11154234409332275, 0.8370142579078674, 0.9479817748069763, 1.1436445713043213, 0.7229137420654297, 0.40749838948249817, -0.5169381499290466, 0.31214046478271484, 0.43021923303604126, 0.8236374855041504, 0.2922525405883789, 0.15254275500774384, 0.3307671844959259, -0.6892787218093872, -0.4166312515735626, -0.6047309041023254, -1.0174298286437988, -0.24142444133758545, 0.3301871120929718, -0.38328999280929565, -0.8559423685073853, 0.21977664530277252, 1.1407723426818848, -0.13984127342700958, 0.41788291931152344, 0.01584680750966072, 0.4541931450366974, -0.7424819469451904, 0.13361836969852448, 0.6168426275253296, 0.4694923758506775, -0.2266203910112381, 0.014578714966773987, 0.35659918189048767, -0.28278985619544983, 0.13628248870372772, 0.3289625346660614, -0.674458920955658, -0.1316239982843399, -0.15488222241401672, -0.1630856990814209, -0.13389386236667633, 1.0746595859527588, 0.42714637517929077, 0.3060644567012787, -0.8358193635940552, -0.05084908381104469, 0.7976795434951782, 0.18318475782871246, -0.09842170029878616, 0.116499163210392, 0.1748092770576477, -0.38332879543304443, 0.47564512491226196, 0.3738895356655121, -0.31669455766677856, -0.6497741937637329, -0.9042482376098633, -0.679488480091095, 0.4365043342113495, -0.8991081118583679, -0.7752647995948792, 0.8259549140930176, -0.4099377989768982, 0.06763800978660583, 0.6323069930076599, -1.023411750793457, -0.12105987966060638, 0.3546508252620697, -1.3002897500991821, -0.4752150774002075, 0.2056347131729126, -0.06086938828229904, -0.4840669631958008, -0.4430357813835144, 1.059744954109192, 0.32832998037338257, -0.5442951321601868, 0.04188866540789604, -0.05876139923930168, -0.1098095253109932, -0.2675512135028839, -0.3156628906726837, 0.6606395840644836, 0.21111629903316498, 0.20484332740306854, 0.2320578694343567, -0.06008196249604225, 0.5461958050727844, -0.8518744707107544, -0.3519434928894043, 0.6468146443367004, -0.5423188209533691, 0.16185180842876434, -0.5019160509109497, -0.37520352005958557, 0.5875082612037659, 0.3492828607559204, 0.27005356550216675, 0.38420382142066956, 0.25153854489326477, -1.151495337486267, -0.5667303800582886, -0.5578064322471619, 0.12269531190395355, 0.25663837790489197, -0.8290917277336121, -0.03556837514042854, 0.18230371177196503, -0.08705143630504608, -0.8180768489837646, -0.5616404414176941, -0.18109719455242157, 0.1474355161190033, -0.3564464747905731, 0.9971815943717957, -0.2753049433231354, 0.7598666548728943, 0.9061779379844666, -0.3410133123397827, -0.5970016121864319, 0.06383276730775833, -0.758353054523468, -0.19259651005268097, -0.3897869884967804, 0.6710479855537415, -0.4973995089530945, 0.8636415600776672, 0.5803688168525696, 0.08397949486970901, -0.490431547164917, -0.5431001782417297, -0.17482149600982666, -0.5185016393661499, -0.5083165168762207, 0.08888810127973557, 0.010928056202828884, -0.308698445558548, 0.186819925904274, 0.4048088788986206, 0.3213980793952942, -0.37488314509391785, -0.772209107875824, 0.42731761932373047, 0.13825838267803192, -0.5244277715682983, -0.6385211944580078, -0.5432422757148743, -1.5070912837982178, 0.01921534724533558, -1.2526347637176514, -0.33452609181404114, -0.4953659474849701, -0.6428279280662537, -0.05409126356244087, -0.31516608595848083, 0.4835147559642792, 0.5862939357757568, 0.2509217858314514, -0.3882187604904175, -0.4634932279586792, -0.43865472078323364, 0.5844396948814392, 0.8059146404266357, -0.8900629878044128, 0.13614964485168457, -0.33121421933174133, -0.12691453099250793, 0.539719820022583, 0.4193580448627472, -0.30906248092651367, -0.8168938755989075, -1.5999056100845337, 0.6286794543266296, -0.1428965926170349, -0.05896685644984245, -1.4241334199905396, 1.043006181716919, 0.7052022218704224, -0.28071844577789307, 0.2925274074077606, 0.5708171129226685, -1.255577564239502, -0.2637144923210144, 0.3840561509132385, -0.6220980882644653, 0.14376656711101532, 0.27236953377723694, -0.5377916693687439, -0.7278444170951843, 0.6241278052330017, 0.22627413272857666, -0.9228891134262085, -0.6156579256057739, 0.7116691470146179, -0.12786924839019775, 0.1462874710559845, -0.31127282977104187, -0.020321257412433624, -1.1347376108169556, -0.23490484058856964, -0.24058130383491516, 0.21357516944408417, -0.8056217432022095, 0.4885733723640442, 0.31539949774742126, -0.9876602292060852, 0.27415329217910767, 0.6407968997955322, -0.41163861751556396, 0.14306239783763885, 0.6917795538902283, 0.7099041938781738, -0.7420190572738647, 0.4787709414958954, 0.13342447578907013, 0.4610294699668884, -0.5058729648590088, 0.16238997876644135, 0.9996007084846497, -0.3868793845176697, -0.25893884897232056, 1.5634325742721558, -0.10951399058103561, -0.7422682642936707, 0.5624979734420776, -1.3063913583755493, -0.2985684871673584, -0.1881963014602661, 0.6271031498908997, -0.02962934784591198, 0.17013755440711975, -0.00726271839812398, -0.138579860329628, 0.09571915119886398, -0.03383425623178482, 0.08966626226902008, 0.7590547204017639, 0.17292609810829163, -0.5314372777938843, 0.5053913593292236, 1.226377248764038, -0.7080845832824707, -1.0652072429656982, -1.0683411359786987, -0.544074535369873, -0.013540378771722317, 0.46764278411865234, -0.08971719443798065, -1.4678629636764526, 0.7426276206970215, 0.16325196623802185, 0.37973329424858093, 0.1734594851732254, -0.4125724136829376, 0.11242538690567017, 1.1736408472061157, -0.26738107204437256, -0.5661494731903076, -0.3267529606819153, 1.4593411684036255, 1.276991367340088, -0.5869466662406921, 0.35729774832725525, -0.7400085926055908, -0.606052041053772, 0.6728013157844543, 0.27404800057411194, -0.6695714592933655, 1.4692648649215698, -0.05830204859375954, -0.3807855248451233, 0.07401738315820694, -0.9736384153366089, -0.1510554403066635, 0.8403266668319702, 0.6984463334083557, 0.48271676898002625, -0.411242812871933, 0.5257195830345154, 0.9213813543319702, -0.24574998021125793, -0.3434155285358429, -0.07285179942846298, 0.1296166628599167, -0.07129200547933578, 0.506590723991394, 0.004924810025840998, 1.1143152713775635, -0.8346064686775208, -0.9270341396331787, 0.46359720826148987, 0.617367148399353, 0.35788238048553467, 0.31549176573753357, 0.9008470177650452, -0.0495915450155735, 0.6826926469802856, 0.19643540680408478, 0.5324647426605225, -0.5532823801040649, -0.43700870871543884, -0.17117556929588318, -0.5642494559288025, -0.410969078540802, -0.0838923379778862, -0.31603768467903137, 0.013303291983902454, -0.3781522512435913, 0.48813945055007935, -0.29614654183387756, 0.25852879881858826, 0.8004631996154785, 0.6113018989562988, 0.5886194109916687, -0.24020619690418243, -0.7790290713310242, -0.3200513422489166, -0.7785106897354126, -0.12037918716669083, -0.5454848408699036, -0.281269907951355, -0.11500006914138794, -0.20972099900245667, -0.05312119051814079]}, "authors": [{"authorId": "4319427", "name": "Beidi Chen"}, {"authorId": "24593911", "name": "Tri Dao"}, {"authorId": "102461072", "name": "Kaizhao Liang"}, {"authorId": "2142772202", "name": "Jiaming Yang"}, {"authorId": "2119235975", "name": "Zhao Song"}, {"authorId": "1755572", "name": "A. Rudra"}, {"authorId": "2114485554", "name": "C. R\u00e9"}], "references": [{"paperId": "5f895e84c1fea75de07b4f90da518273c2e57291", "title": "Scatterbrain: Unifying Sparse and Low-rank Attention Approximation"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "dc32a984b651256a8ec282be52310e6bd33d9815", "title": "Highly accurate protein structure prediction with AlphaFold"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "cc50f846ed7222698d130cddbc58ed4d547914ed", "title": "CPM: A Large-scale Generative Chinese Pre-trained Language Model"}, {"paperId": "7e9ff94476f41041c75e253e84f487db00e9c861", "title": "Long Range Arena: A Benchmark for Efficient Transformers"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "99ae1c1ac5f06574bafc6fb7acb25fc29eb0b754", "title": "Training (Overparametrized) Neural Networks in Near-Linear Time"}, {"paperId": "021739c140c4eefaf2bbb65658e119e5251576a3", "title": "Finding trainable sparse networks through Neural Tangent Transfer"}, {"paperId": "5aacc6e32563c290a32e629a8bfb641d8892c314", "title": "Optimal Lottery Tickets via SubsetSum: Logarithmic Over-Parameterization is Sufficient"}, {"paperId": "3b0fb765716ef6861a84abffcbe40643857c613b", "title": "Pruning neural networks without any data by iteratively conserving synaptic flow"}, {"paperId": "9e949c47d558dd8a5b5d9d67a8789723ed061e0f", "title": "Triple descent and the two kinds of overfitting: where and why do they appear?"}, {"paperId": "45a08126c8a599ff223243156fae614761b47e43", "title": "ReSprop: Reuse Sparsified Backpropagation"}, {"paperId": "b672afc19364cc801b54543f4f87408ac2c41078", "title": "Logarithmic Pruning is All You Need"}, {"paperId": "f8da8c55c0e7c4940a02347347dd232bc2bac0b5", "title": "The hardware lottery"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "66f0f35fc78bdf2af9de46093d49a428970cde2e", "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"}, {"paperId": "a68c3412e60560290400d2707596f82a914b7c00", "title": "Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps"}, {"paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa", "title": "Lite Transformer with Long-Short Range Attention"}, {"paperId": "dc418c0b5ac24a67fef336323ef0417600ba3718", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacot, Arthor, Gabriel, Franck and Hongler, Clement : Neural Tangent Kernel : Convergence and Generalization in Neural Networks"}, {"paperId": "c114ce10c4a315d92c3815f54bc9893e7e6ef182", "title": "Picking Winning Tickets Before Training by Preserving Gradient Flow"}, {"paperId": "1bcf4553d841ad78cf51b4d3d48a61f9f3c71ebf", "title": "Proving the Lottery Ticket Hypothesis: Pruning is All You Need"}, {"paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2", "title": "Scaling Laws for Neural Language Models"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "52184d7a541eff0b9537e75da7327dd41daba207", "title": "Sparse Weight Activation Training"}, {"paperId": "3f06d02513a2763e472d2b5d5db08e9061081b9e", "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis"}, {"paperId": "ea415809bf87ef4b99966c6c50de6cb996a02a97", "title": "Deep double descent: where bigger models and more data hurt"}, {"paperId": "2e3002f131e1815bda7a10303eff97f79dea01ec", "title": "Rigging the Lottery: Making All Tickets Winners"}, {"paperId": "c437640d0ecad7efad695adca55b75e6a8a97410", "title": "Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound"}, {"paperId": "c53ae5c2601de32c87dab796ab686c70e48c356f", "title": "One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers"}, {"paperId": "b1ac64438608aac1a8dfd0adf8fec8c6220f6bfd", "title": "Butterfly Transform: An Efficient FFT Based Neural Architecture Design"}, {"paperId": "6e13e111e85d499d781386b182fd855fbb053771", "title": "Deep Learning Recommendation Model for Personalization and Recommendation Systems"}, {"paperId": "abc41ef9e9cf77572be5fc4bd5357499d775ac87", "title": "Training Dynamics of Deep Networks using Stochastic Gradient Descent via Neural Tangent Kernel"}, {"paperId": "26e44b8106a5145126c59b6d0c3af326337447f9", "title": "Unifying Orthogonal Monte Carlo Methods"}, {"paperId": "1029daa28aa772e441470e61bdd610c222e92932", "title": "On Exact Computation with an Infinitely Wide Neural Net"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations"}, {"paperId": "71a906c1dca08af227d7311c2018932ea616bffa", "title": "SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems"}, {"paperId": "075da5ebbb890924267b4b163292ad21d0b100a0", "title": "Stabilizing the Lottery Ticket Hypothesis"}, {"paperId": "9f9fc406c76255fec51a6196ce167c0ff1d1efc0", "title": "Wide neural networks of any depth evolve as linear models under gradient descent"}, {"paperId": "9b81a4df6fbc2702f335ff984381a1634d1be23d", "title": "Toward Moderate Overparameterization: Global Convergence Guarantees for Training Shallow Neural Networks"}, {"paperId": "2710979fb01fab8f10110bc7b69497cb61477e60", "title": "Generalization Error Bounds of Gradient Descent for Learning Over-Parameterized Deep ReLU Networks"}, {"paperId": "14558cb69319eed0d5bfc5648aafcd09d882f443", "title": "Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks"}, {"paperId": "f86f1748d1b6d22870f4347fd5d65314ba800583", "title": "Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off"}, {"paperId": "611fe6e34df07ea1b2104899e49642b4531b53e9", "title": "Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers"}, {"paperId": "1228a5f81dd6d169858cc3378a59065166583126", "title": "On the Convergence Rate of Training Recurrent Neural Networks"}, {"paperId": "d6f6d1504cfedde4efb23e7ec0f42f006062c6a0", "title": "Gradient Descent Provably Optimizes Over-parameterized Neural Networks"}, {"paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d", "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"}, {"paperId": "42ec3db12a2e4628885451b13035c2e975220a25", "title": "A Convergence Theory for Deep Learning via Over-Parameterization"}, {"paperId": "ccb1bafdae68c635cbd30d49fda7dbf88a3ce1b6", "title": "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data"}, {"paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60", "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, {"paperId": "6ea8cbf0cc4cda3d981348a279b464524a8485cc", "title": "On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization"}, {"paperId": "d54f3f8fe8b982e68231f82903fb2e7119746a7f", "title": "Quadrature-based features for kernel approximation"}, {"paperId": "bce22675d77e1ef28e92f3793c02f8f5ccdb0ddd", "title": "A Two-pronged Progress in Structured Dense Matrix Vector Multiplication"}, {"paperId": "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"paperId": "8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec", "title": "Why Are Big Data Matrices Approximately Low Rank?"}, {"paperId": "773d5ddc414424a8948446ddaa5275b944f50891", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon"}, {"paperId": "1d782819afafe0d391e5b67151cb510e621f243d", "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs"}, {"paperId": "efbd381493bb9636f489b965a2034d529cd56bcd", "title": "Pointer Sentinel Mixture Models"}, {"paperId": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets"}, {"paperId": "c568ecbd28e1fe24f58d6566cc37d45e32fb25b9", "title": "Weighted low rank approximations with provable guarantees"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network"}, {"paperId": "78f0090d052d033d13a872e831eae16303a4d502", "title": "Variable Selection is Hard"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "cd7c5cbc246e42ebb1368f81ef403c988a1d4c89", "title": "Fast Approximation of Rotations and Hessians matrices"}, {"paperId": "03f384488db82150b2187f504fb1dc321160764a", "title": "High Dimensional Low Rank and Sparse Covariance Matrix Estimation via Convex Minimization"}, {"paperId": "4adcc0d8f32fe1d707c8bc4a96d93edf737c9593", "title": "Clustering Partially Observed Graphs via Convex Optimization"}, {"paperId": "c8831d7d318b8d59f9b958d250a58f253f08bd8a", "title": "Robust principal component analysis?"}, {"paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e", "title": "ImageNet: A large-scale hierarchical image database"}, {"paperId": "e1ada191566510f7efc2d4b7a76a3167bafe4c43", "title": "Perturbed Identity Matrices Have High Rank: Proof and Applications"}, {"paperId": "915df1a8dda45221204f3ecbf70b07d8b34d7ba8", "title": "Stable signal recovery from incomplete and inaccurate measurements"}, {"paperId": "059d1d0da45c2e7b59d8a8b650628d0fc6deac10", "title": "The diagonal decomposition technique applied to the dynamic programming solution of elliptic partial differential equations"}, {"paperId": "0e6beb95b5150ce99b108acdefabf70ccd3fee30", "title": "An algorithm for the machine calculation of complex Fourier series"}, {"paperId": null, "title": "2021] push the top-1 accuracy on various vision benchmarks to new highs after scaling"}, {"paperId": null, "title": "2021] from DeepMind solve a 50 year old grand challenge in protein"}, {"paperId": null, "title": "The main difference between our generative process"}, {"paperId": null, "title": "2020] sheds light on the good generalization behavior of overparameterized deep"}, {"paperId": null, "title": "2020] provides a logarithmic upper bound for the number of parameters"}, {"paperId": null, "title": "2020] found that subnetworks reach full accuracy only if they are stable against SGD"}, {"paperId": null, "title": "effective algorithms to compress models up to 49x and maintain comparable"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "2019b] points out that there is still a gap between NTK and the real"}, {"paperId": null, "title": "2019], the assumption holds when xi is not parallel with xj for i"}, {"paperId": null, "title": "2019], and rediscovered in Longformer and BigBird patterns as well. \u2022 Butterfly: this component corresponds to interaction between elements that are some fixed distance apart"}, {"paperId": null, "title": "2019a] is the first one to show generalization bound independent of the network"}, {"paperId": null, "title": "2019] shows impressive results on recommendation with a 21"}, {"paperId": null, "title": "2019] extend on the previous idea and prove that finite learning rate is enough for the model"}, {"paperId": null, "title": "Neural Tangent Kernel, Convergence, and Generalization Our analysis relies on the neural tangent kernel (NTK) [Jacot et al., 2018] of the network. Definition F.1"}, {"paperId": null, "title": "2018] speculates that overparameterization helps model optimization, and without"}, {"paperId": "a07609c2ed39d049d3e59b61408fb600c6ab0950", "title": "GPU Kernels for Block-Sparse Weights"}, {"paperId": "88cd4209db62a34d9cba0b9cbe9d45d1e57d21e5", "title": "Runtime Neural Pruning"}, {"paperId": null, "title": "Butter\ufb02y factorization"}, {"paperId": null, "title": "where M i is the i -th row of M . Without making any assumptions, such problems are in general computationally hard Foster et al."}, {"paperId": null, "title": "CUDA Programming: A Developer\u2019s Guide to Parallel Computing with GPUs"}, {"paperId": null, "title": "Experiment Details L.1 Datasets \u2022 Cifar10 [Krizhevsky et al., 2009] consists of 60000 coloured images of resolution"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "182c4a4074e8577c7ba5cbbc52249e41270c8d64", "title": "The Penn Treebank: An Overview"}, {"paperId": "552e26d425c6cd6d79e428716adcd2897ee62e0e", "title": "SPARSE CODING IN THE PRIMATE CORTEX"}, {"paperId": "b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "title": "Regression Shrinkage and Selection via the Lasso"}, {"paperId": null, "title": "Random Butterfly Transformations with Applications in Computational Linear Algebra"}, {"paperId": "e7297db245c3feb1897720b173a59fe7e36babb7", "title": "Optimal Brain Damage"}, {"paperId": null, "title": "Neural Tangent Kernel Our work rely heavily on neural tangent kernel in theoretical analysis"}]}