{"paperId": "faf7488da821133dbf34fe006676adfda4a764a0", "title": "Outlier-Efficient Hopfield Layers for Large Transformer-Based Models", "abstract": "We introduce an Outlier-Efficient Modern Hopfield Model (termed $\\mathrm{OutEffHop}$) and use it to address the outlier inefficiency problem of {training} gigantic transformer-based models. Our main contribution is a novel associative memory model facilitating \\textit{outlier-efficient} associative memory retrievals. Interestingly, this memory model manifests a model-based interpretation of an outlier-efficient attention mechanism (${\\rm Softmax}_1$): it is an approximation of the memory retrieval process of $\\mathrm{OutEffHop}$. Methodologically, this allows us to introduce novel outlier-efficient Hopfield layers as powerful alternatives to traditional attention mechanisms, with superior post-quantization performance. Theoretically, the Outlier-Efficient Modern Hopfield Model retains and improves the desirable properties of standard modern Hopfield models, including fixed point convergence and exponential storage capacity. Empirically, we demonstrate the efficacy of the proposed model across large-scale transformer-based and Hopfield-based models (including BERT, OPT, ViT, and STanHop-Net), benchmarking against state-of-the-art methods like $\\mathtt{Clipped\\_Softmax}$ and $\\mathtt{Gated\\_Attention}$. Notably, $\\mathrm{OutEffHop}$ achieves an average reduction of 22+\\% in average kurtosis and 26+\\% in the maximum infinity norm of model outputs across four models. Code is available at \\href{https://github.com/MAGICS-LAB/OutEffHop}{GitHub}; models are on \\href{https://huggingface.co/collections/magicslabnu/outeffhop-6610fcede8d2cda23009a98f}{Hugging Face Hub}; future updates are on \\href{https://arxiv.org/abs/2404.03828}{arXiv}.", "venue": "arXiv.org", "year": 2024, "citationCount": 10, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "An Outlier-Efficient Modern Hopfield Model is introduced and used to address the outlier inefficiency problem of {training} gigantic transformer-based models and a novel associative memory model facilitating outlier-efficient associative memory retrievals is introduced."}, "embedding": {"model": "specter_v2", "vector": [0.19125497341156006, 0.9783440232276917, -0.42385298013687134, 0.055634766817092896, -0.3422386646270752, 0.43238717317581177, 0.5101671814918518, -0.09435739368200302, -0.19788241386413574, -0.6098525524139404, 0.28970059752464294, 0.6304594874382019, 0.18595707416534424, 0.26584547758102417, -0.1775464415550232, -0.18148118257522583, -1.3374128341674805, 0.19870077073574066, -0.07543912529945374, -0.24165640771389008, 0.3336431384086609, -0.610765278339386, -1.1251931190490723, 0.2583499848842621, 0.022573821246623993, 1.521153450012207, -0.2678752839565277, 0.2609654664993286, -0.40116721391677856, 0.31129270792007446, 0.5414479374885559, -0.6315734386444092, 0.4960691034793854, 0.05993366241455078, -0.3094642162322998, 0.20340871810913086, 0.5329651236534119, -0.34051841497421265, -0.8255347013473511, 0.965275764465332, -0.056367505341768265, 0.2560296952724457, 0.6692579984664917, -0.8908666372299194, -0.6520048975944519, 0.8352307677268982, 0.7351411581039429, 0.6600677371025085, -0.9364141821861267, -0.3608831763267517, 1.2314484119415283, -1.673609972000122, 0.0975750982761383, 1.2832242250442505, 0.708480954170227, 0.19602730870246887, 0.13728033006191254, -0.8879310488700867, 0.5352177023887634, 0.3989701271057129, -1.2078269720077515, -0.4926801025867462, 0.2679777145385742, -0.1288638710975647, 1.1635125875473022, -0.8879329562187195, 0.10356387495994568, 0.9274255633354187, 0.3856121599674225, 1.373134732246399, 0.1775418370962143, -0.47677186131477356, 0.08353880792856216, -0.09698789566755295, 0.1349877566099167, 0.8416170477867126, -0.1620958149433136, 0.5096874833106995, -1.3289209604263306, 0.07753510028123856, 0.3640432357788086, 0.5505039691925049, 0.24045461416244507, -0.011616452597081661, 0.17831051349639893, 0.5780318975448608, 0.6327575445175171, 0.6545135378837585, -0.584912896156311, 0.8333010673522949, 0.29327675700187683, 0.4915737509727478, 0.0020881113596260548, 0.18492725491523743, 0.35816967487335205, 0.5551309585571289, -1.0295073986053467, 0.1663985699415207, -0.3803342282772064, 1.1823595762252808, -0.043155424296855927, 0.6943292021751404, -0.5023746490478516, 0.06471122056245804, 1.410328984260559, -0.03678566589951515, 0.6951654553413391, -0.5879466533660889, -0.295533686876297, -0.6128233671188354, 0.07807976007461548, -0.836575448513031, -0.11362169682979584, -0.5192339420318604, -1.0607659816741943, -1.0023249387741089, -0.5155271291732788, 0.9613750576972961, -1.1619055271148682, 0.38116493821144104, -1.1384738683700562, 0.31495025753974915, -0.11598469316959381, 0.5572277903556824, 0.33092182874679565, 0.32979872822761536, 0.288836270570755, 0.2656310796737671, 0.6752704381942749, -0.826657772064209, -0.6219459772109985, -0.9803478717803955, 0.20889361202716827, -0.15193527936935425, 0.039400264620780945, 0.09124334156513214, -1.432248592376709, -0.6888253092765808, -0.8275271654129028, 0.4551515281200409, -0.7156315445899963, -0.26509159803390503, 0.7156429290771484, -0.06130574643611908, -0.916168749332428, 1.1060676574707031, 0.02397632598876953, -0.07576756924390793, 0.8770490288734436, 0.5385289192199707, 0.38627034425735474, -0.24389804899692535, -1.1618658304214478, 0.4175167679786682, 0.4261527359485626, -0.056811053305864334, -0.4728517234325409, -0.1635054647922516, -0.770889937877655, -0.07729069888591766, 0.2571943700313568, -0.19776487350463867, 0.9562310576438904, -0.44668668508529663, -0.7350731492042542, 0.6709485054016113, -0.2123003453016281, -0.09120672196149826, 0.11143050342798233, 0.10808022320270538, -0.8308532238006592, -0.15378928184509277, -0.5038518309593201, 0.7152267098426819, 1.0005062818527222, -0.6371585726737976, 0.08785934001207352, -0.1458369493484497, -1.241773247718811, -0.17317810654640198, -0.5238248109817505, 0.6790850162506104, -0.3944755792617798, -0.6377633213996887, 0.7583163380622864, 0.6152461767196655, 0.13649553060531616, -0.20736926794052124, -0.2830246686935425, -0.9026305675506592, 0.7335582375526428, 0.1954168975353241, 1.1847224235534668, -1.0942291021347046, -0.997153103351593, 0.36560356616973877, 0.15849992632865906, -0.2171841412782669, -0.8319334387779236, 0.3137616515159607, -0.25447243452072144, 0.42972713708877563, 0.13264617323875427, -0.8688900470733643, 0.2432055026292801, -0.26454296708106995, -1.0355935096740723, 0.024695448577404022, 0.3345155417919159, 1.0212984085083008, -0.7695431113243103, 0.12984098494052887, -0.41128090023994446, 0.30688437819480896, -0.8276985287666321, 0.9482035636901855, 0.06951188296079636, -0.07149015367031097, 0.059950970113277435, 0.3982200622558594, -0.002382853301241994, -0.3563075065612793, 0.09289409220218658, -0.3912449777126312, 0.257087767124176, 0.19726748764514923, -0.6607452034950256, 1.2163732051849365, 0.07197272777557373, 0.4926672577857971, -0.48580560088157654, -0.5977890491485596, 0.5417705774307251, 0.5653733015060425, -0.11631722003221512, -0.665320634841919, 0.6170181632041931, 0.6011998653411865, -0.4869230091571808, 0.4820552170276642, 0.8997319340705872, 0.8924062252044678, -0.44034385681152344, -0.19881920516490936, 1.2249006032943726, 0.01699029840528965, 0.13080070912837982, -0.047860462218523026, 0.40139061212539673, -0.1213129386305809, 0.02442982606589794, -0.2750292718410492, -0.33693891763687134, -1.3424756526947021, -0.029322993010282516, 0.8670308589935303, 0.24485300481319427, 1.092954397201538, 0.24575002491474152, -1.208787202835083, -0.317194402217865, -0.3829882740974426, 0.36649689078330994, 1.06307852268219, -0.1810985803604126, -0.4099932312965393, -0.3546490967273712, 0.12430918961763382, -0.2694275379180908, -0.30678749084472656, -0.3757193088531494, -0.897668182849884, -0.16951997578144073, -1.3195072412490845, 0.9044026732444763, 0.28949373960494995, 1.0085551738739014, -1.1040090322494507, 0.054578181356191635, -0.22760112583637238, 0.5066400170326233, -0.48820021748542786, -0.503792941570282, 0.7055887579917908, -0.6739320158958435, 0.090251125395298, -0.0013733315281569958, -0.21943537890911102, 0.2179328203201294, -1.406933307647705, 0.808353066444397, -0.24276944994926453, -0.17923666536808014, 0.04476442560553551, 0.8276380300521851, -0.8966770768165588, -0.19838789105415344, -0.0390784926712513, 0.48039332032203674, 0.10845725238323212, 0.8216978311538696, 0.3923952281475067, -0.2852040231227875, 0.07736038416624069, -0.4880547523498535, -0.5925154685974121, 0.44615671038627625, 0.22720114886760712, 0.8373549580574036, -0.15137772262096405, 0.35029518604278564, -0.49330002069473267, 0.8101708889007568, 0.0919153094291687, -0.31518590450286865, -0.18527334928512573, -1.1716487407684326, -0.41403964161872864, 0.3758947253227234, -0.366044819355011, 0.2099771350622177, -0.9923567771911621, 0.6070558428764343, -0.8582704067230225, -0.18731898069381714, -0.03973432257771492, 0.5515640377998352, -0.20094262063503265, 0.3167203962802887, 0.44772520661354065, 0.06039385497570038, 0.357876718044281, 0.12439403682947159, -0.5519381165504456, 0.6790607571601868, 0.14398974180221558, 0.09058910608291626, -0.08336327970027924, -0.08751150965690613, -0.6793747544288635, -0.3588966727256775, -0.34413227438926697, -0.3038956820964813, -0.32066741585731506, -0.2518582046031952, -0.5577353835105896, -0.7795366644859314, 0.36233851313591003, -0.5781047344207764, -0.20680102705955505, -0.0726836547255516, -0.24648161232471466, -0.695536732673645, -1.4047530889511108, -1.5672487020492554, -1.1070035696029663, -0.5648030042648315, -1.0433305501937866, 0.25866711139678955, 0.015396314673125744, -0.4866371154785156, -0.25956425070762634, -0.6805939674377441, -0.8958612084388733, 1.178080439567566, -0.7591575980186462, 0.5533212423324585, -0.41685765981674194, -0.6438263058662415, -0.41133439540863037, 0.2756058871746063, 0.5971947312355042, -0.7032056450843811, 0.134229376912117, -1.1470073461532593, 0.38333603739738464, -0.6279581189155579, -0.22906729578971863, 0.5523104667663574, 0.3216862082481384, 0.9665878415107727, -0.18524101376533508, -0.31137675046920776, 0.3103339970111847, 1.2541730403900146, -0.36758434772491455, 0.3703729510307312, 0.13476857542991638, 0.6889608502388, 0.29556795954704285, -0.5328015089035034, 0.6953227519989014, 0.04379188269376755, 0.11778642237186432, 0.23107890784740448, 0.1753055304288864, -0.032048724591732025, -0.5837587714195251, 0.48299849033355713, 1.6894505023956299, 0.06885068118572235, 0.1553799957036972, -1.1534781455993652, 0.8113205432891846, -1.1102540493011475, -0.8615749478340149, 0.9741706848144531, 1.143820881843567, 0.06223159283399582, -0.07240655273199081, -0.4164538085460663, -0.07176259160041809, 0.514911413192749, 0.20016950368881226, -0.06615164875984192, -0.5475438833236694, -0.08427378535270691, 0.7191029787063599, 0.2242969274520874, 0.26364606618881226, -0.527313232421875, 0.641576886177063, 14.78337574005127, 0.5257905125617981, -0.1879541426897049, 0.2898518145084381, 0.862608790397644, 0.18249893188476562, -0.11591507494449615, -0.130435973405838, -0.9489470720291138, 0.37288838624954224, 1.327963948249817, 0.6141887307167053, 0.6154801845550537, 0.15651191771030426, -0.5043072700500488, 0.7899894118309021, -0.42163512110710144, 0.8100438714027405, 0.6220464706420898, -1.1626781225204468, 0.4679209291934967, 0.12198436260223389, 0.28314244747161865, 0.5274814963340759, 1.162781000137329, 0.8111639618873596, 0.13631761074066162, -0.5458265542984009, 0.5821998715400696, 0.6628829836845398, 0.7071198225021362, -0.3321923017501831, 0.5523834228515625, -0.35003662109375, -0.7661874890327454, -0.5763445496559143, -0.7446379661560059, -0.8123056888580322, -0.15337303280830383, 0.18218763172626495, 0.11883722990751266, -0.7589202523231506, 0.3357289135456085, 0.6814654469490051, -0.042669378221035004, 0.5435908436775208, 0.00968250073492527, 0.43128669261932373, -0.1845337152481079, 0.40534764528274536, 0.34637024998664856, 0.2966775596141815, 0.14942185580730438, -0.3075188994407654, 0.4161348342895508, -0.20689158141613007, 0.33815547823905945, 0.5163722038269043, -0.689831018447876, -0.08036104589700699, -0.5757132172584534, -0.45568424463272095, -0.007998333312571049, 0.7958789467811584, 0.2695086896419525, 0.14859497547149658, -0.10243429988622665, 0.48750975728034973, 0.4464600682258606, 0.48155882954597473, -0.033716343343257904, -0.4986288845539093, 0.338143527507782, -0.4001811742782593, -0.09571095556020737, 0.9112901091575623, -0.05077038332819939, -0.4236113131046295, -0.7819839119911194, -0.06258057057857513, 0.5512876510620117, -0.9750186204910278, -0.8590224385261536, 1.156304955482483, -0.5950068235397339, -0.25626200437545776, 0.47424426674842834, -0.7464291453361511, -0.24199600517749786, 0.21386373043060303, -1.3758924007415771, -0.7467808723449707, 0.07440441101789474, 0.040514279156923294, -0.5662574768066406, -0.47246962785720825, 1.1385091543197632, 0.25215354561805725, -0.4173823595046997, 0.1627454310655594, -0.14056681096553802, -0.04330159351229668, 0.060328707098960876, -0.6708436608314514, 0.5282324552536011, 0.11265501379966736, 0.012385724112391472, 0.3314555883407593, -0.015605004504323006, 0.38634830713272095, -0.29736265540122986, -0.10365291684865952, 0.6583254337310791, -0.8173716068267822, -0.17930063605308533, -0.7648251056671143, -0.9945827722549438, 0.6187719106674194, 0.43583568930625916, 0.29709798097610474, 0.10963992774486542, 0.1513804942369461, -0.5910374522209167, -0.29976239800453186, -0.4581829011440277, 0.31893932819366455, 0.38329973816871643, -0.8554915189743042, -0.46148088574409485, -0.210659921169281, -0.33022066950798035, -0.5062592029571533, -0.613823652267456, -0.15360547602176666, 0.08856986463069916, -0.33845970034599304, 1.0667333602905273, -0.37753230333328247, 0.5346952080726624, 0.825424313545227, -0.218908429145813, -0.9438886046409607, -0.30533498525619507, -1.0294232368469238, -0.3997238576412201, 0.0047577861696481705, 0.125235453248024, -0.49792182445526123, 0.3919260501861572, 0.42106205224990845, -0.05444344878196716, -0.5077757835388184, -0.6603691577911377, -0.3155989944934845, -0.15602698922157288, -0.5318300724029541, 0.24076975882053375, -0.21683843433856964, 0.1308150440454483, -0.12054549157619476, 0.4877241253852844, 0.27135878801345825, -0.017390212044119835, -0.8608446717262268, -0.38583067059516907, -0.5142634510993958, 0.4589711129665375, -0.8588939905166626, -0.9726820588111877, -0.933756947517395, 0.09974703192710876, -1.4537409543991089, 0.07878492772579193, -0.24329490959644318, -0.6513346433639526, 0.06711485236883163, -1.1257500648498535, 0.18094800412654877, 0.6617960333824158, 0.08379869908094406, -0.28591206669807434, -0.042520757764577866, -0.7097933292388916, 0.9317774772644043, 0.541462242603302, -0.7245534062385559, 0.193707674741745, -0.4035879969596863, 0.03490116074681282, 0.5399052500724792, 0.3711587190628052, -0.6719903945922852, -0.4725574254989624, -0.7679228186607361, 0.1761934459209442, -0.6687434911727905, 0.03334931656718254, -1.1686145067214966, 0.7628232836723328, 0.7833890914916992, 0.19290244579315186, 0.2988983988761902, 0.49204808473587036, -1.2808318138122559, -0.829673171043396, 0.2890537977218628, -0.5639401078224182, 0.37854325771331787, 0.005751965567469597, -0.2941874563694, -0.6539885997772217, 0.8139859437942505, 0.05768350884318352, -0.8039569854736328, -0.999416708946228, 0.8781595826148987, -0.6219395995140076, 0.08718330413103104, -0.29506823420524597, -0.10706141591072083, -1.2936344146728516, -0.4340422749519348, -0.39723867177963257, -0.05098925530910492, -0.31886762380599976, 1.070752501487732, 0.8314394950866699, -1.4022623300552368, 0.41756391525268555, 0.8113704919815063, 0.38446351885795593, -0.23664216697216034, 0.2116258144378662, 0.9492685198783875, -0.008748051710426807, 0.26439952850341797, -0.058265745639801025, 0.1748761683702469, 0.02288687974214554, 0.11006858199834824, 0.8198519349098206, -0.015189126133918762, -0.18613667786121368, 0.9406152963638306, -0.4163300395011902, -0.7746392488479614, 0.5135601162910461, -1.0987237691879272, -0.20742633938789368, 0.04843714460730553, 0.5285432934761047, 0.1339847892522812, 0.28796491026878357, -0.24836981296539307, -0.407487690448761, 0.11110371351242065, -0.25355079770088196, -0.066665880382061, 0.26922115683555603, 0.03965889662504196, -0.0049345470033586025, 0.6354376077651978, 0.7058998942375183, -1.21424400806427, -0.909380316734314, -0.6586212515830994, 0.14018166065216064, 0.11928687244653702, 0.41489219665527344, -0.047990407794713974, -1.0444798469543457, 0.8975205421447754, 1.1111375093460083, 0.0598323717713356, 0.15127772092819214, -0.30399397015571594, 0.1856200248003006, 1.0903981924057007, 0.15385951101779938, -1.1260957717895508, -0.038930684328079224, 1.1661027669906616, 1.0975664854049683, -0.7938387989997864, -0.009131464175879955, 0.07523591071367264, -0.5561431050300598, 0.8976750373840332, 0.20946072041988373, -0.45690494775772095, 1.085906982421875, -0.18602022528648376, 0.19497354328632355, -0.2957218885421753, -0.884147047996521, -0.36494171619415283, 0.8705400824546814, 0.8055169582366943, 0.5261565446853638, -0.13439017534255981, 0.6381571292877197, 1.033624529838562, 0.33748888969421387, 0.10078050941228867, 0.28207311034202576, 0.5364425778388977, -0.48975512385368347, 0.3751683533191681, -0.00770925497636199, 1.0460125207901, -0.8263988494873047, -0.7284914255142212, 0.25552889704704285, 0.5964001417160034, 0.060816578567028046, 0.4165920615196228, 1.0911730527877808, -0.021846294403076172, 0.5735595226287842, 0.6251038312911987, 0.5924863219261169, 0.14588604867458344, -0.3120206296443939, -0.10001521557569504, -1.1100307703018188, -0.25149664282798767, -0.07690771669149399, -0.0623745396733284, -0.18894320726394653, -0.0053841411136090755, 0.3360007405281067, -0.1955610066652298, 0.1029081642627716, 0.8346818685531616, 0.7503175139427185, 0.35914096236228943, -0.45236721634864807, -0.7997875213623047, -0.41516900062561035, -0.7563993334770203, -0.6584972143173218, 0.044646553695201874, -0.15664704144001007, -0.5006868243217468, -0.4128468632698059, -0.31834787130355835]}, "authors": [{"authorId": "102764428", "name": "Jerry Yao-Chieh Hu"}, {"authorId": "2295612138", "name": "Pei-Hsuan Chang"}, {"authorId": "1387720883", "name": "Haozheng Luo"}, {"authorId": "2295806983", "name": "Hong-Yu Chen"}, {"authorId": "2277230783", "name": "Weijian Li"}, {"authorId": "2295556071", "name": "Wei-Po Wang"}, {"authorId": "2193647521", "name": "Han Liu"}], "references": [{"paperId": "dc4dcc841c92792d439137d26ffc0261622b3a43", "title": "Tensor Attention Training: Provably Efficient Learning of Higher-order Transformers"}, {"paperId": "9a169c3f4b86bfd4b5c2c6825a6ca652fbd6c9a0", "title": "Energy-based Hopfield Boosting for Out-of-Distribution Detection"}, {"paperId": "c29f8eecec7598344efc36a42088071142092d6a", "title": "Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers"}, {"paperId": "b44e41fcaaead1b50f20fda0c78d395b51002f7f", "title": "Semantically-correlated memories in a dense associative model"}, {"paperId": "28b1330bde19391e5fc3f3851a6a31ea979dd586", "title": "Nonparametric Modern Hopfield Models"}, {"paperId": "eede8ab9a9cf3b71b61cff7a3bc6fcc9bcf1f2a1", "title": "BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model"}, {"paperId": "ed1aaad1beac180a33d74a539300a1fcb62a15ec", "title": "Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models"}, {"paperId": "bc8d58fcb7dbf6cc4942eda901a54412b2018a89", "title": "DNABERT-S: LEARNING SPECIES-AWARE DNA EMBEDDING WITH GENOME FOUNDATION MODELS"}, {"paperId": "22910f92c164971ff6ae886ece9c586c703c7153", "title": "On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis"}, {"paperId": "123150159d9b7230be1c60b8b8e20663dcc22884", "title": "The Fine-Grained Complexity of Gradient Computation for Training Large Language Models"}, {"paperId": "28863db0ed342b2a465a718fe835ccbd2e54c3e3", "title": "STanHop: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction"}, {"paperId": "504b333ea6e13f92d76b6835c18e48a9d822d246", "title": "How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation"}, {"paperId": "f73cd1c9eba950c04fbd81e1f024392978059d59", "title": "Scaling Laws for Associative Memories"}, {"paperId": "abb79cc72fab35bfeb50585a121375b9bebafbb0", "title": "On Sparse Modern Hopfield Model"}, {"paperId": "68ed29e5d398e6030cddcc575f1977973c8b0791", "title": "A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time"}, {"paperId": "0f4780f3f42dbe9755d54495ae17244cc88a7483", "title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome"}, {"paperId": "d193675b92fbfbf22ed82fda35cd2e73587e33bd", "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing"}, {"paperId": "6076a7a154697bd405e55041ca214554045b2c18", "title": "Feature Programming for Multivariate Time Series Prediction"}, {"paperId": "fb6dde045c52fe46b409e221e7ef5dc139756d55", "title": "Long Sequence Hopfield Memory"}, {"paperId": "11ae58636a5daf0ea1297f1c4ee94042fcebefa8", "title": "Birth of a Transformer: A Memory Viewpoint"}, {"paperId": "ca04700240c273a4ca237607ef5a1cbf9163fa16", "title": "Simplicial Hopfield networks"}, {"paperId": "fe2274ab4ea7cf20214fe2f1b89168f966094a4e", "title": "Context-enriched molecule representations improve few-shot drug discovery"}, {"paperId": "83edcfbb206ddad38a971d605da09390604248ea", "title": "BloombergGPT: A Large Language Model for Finance"}, {"paperId": "5d1f73ab5e3bdc93f10873a9ac8c7579bb2ebf8c", "title": "Conformal Prediction for Time Series with Modern Hopfield Networks"}, {"paperId": "39ed1c33af6f0a5fbc16354afcb223a03c9c139b", "title": "Fast Attention Requires Bounded Entries"}, {"paperId": "09ec56d18667455fa86372c0645f331657bc3341", "title": "Energy Transformer"}, {"paperId": "e8faee8333cff76cbcbffd2233e2240084c4fb60", "title": "Building transformers from neurons and astrocytes"}, {"paperId": "3f6243097a58e386aea1215fed4f372dee07a100", "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"}, {"paperId": "3f3c01adbdd433d515c19ac8cf6c61c905f0061a", "title": "History Compression via Language Models in Reinforcement Learning"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "59c70ad7c22cc5a76cb8a27953aee4dd0ee80e57", "title": "Improving Few- and Zero-Shot Reaction Template Prediction Using Modern Hopfield Networks"}, {"paperId": "eb95b02edfaeb28f528b5ee8b705388bb9a933be", "title": "CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP"}, {"paperId": "1cbb3d96242c3f47c3f40aada33616d0f5c07737", "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"}, {"paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0", "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"}, {"paperId": "4f68e07c6c3173480053fd52391851d6f80d651b", "title": "On the Opportunities and Risks of Foundation Models"}, {"paperId": "35a9749df07a2ab97c51af4d260b095b00da7676", "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"}, {"paperId": "b103e87c7727134927d3ffb06934a95c10c02fc0", "title": "GPT-3: Its Nature, Scope, Limits, and Consequences"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "c43d9cade31600400a0f62beb5bbcc1b548e009e", "title": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome"}, {"paperId": "6a8092b98771b8157437e71e351ae1231bdd8259", "title": "Large Associative Memory Problem in Neurobiology and Machine Learning"}, {"paperId": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5", "title": "Hopfield Networks is All You Need"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "01203341a8b5b7df21dec5359afe8cc388786ebf", "title": "Wiki-40B: Multilingual Language Model Dataset"}, {"paperId": "562bf6d0aac2c6362086ef4c80503de8ea56b340", "title": "Modern Hopfield Networks and Attention for Immune Repertoire Classification"}, {"paperId": "9a21740d87976bf76f4a9668a9da631035302fb2", "title": "Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms"}, {"paperId": "9807032064b278b72611ddbe504a3ce396d33517", "title": "Robust Quantization: One Model to Rule Them All"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "d78aed1dac6656affa4a04cbf225ced11a83d103", "title": "Revealing the Dark Secrets of BERT"}, {"paperId": "3366e9eb81880d172752d4397cb8e9e6de02b935", "title": "Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model"}, {"paperId": "03f8754ab20732ebda02ce6e65ec9bfcce17528a", "title": "Marian: Cost-effective High-Quality Neural Machine Translation in C++"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "84bad036fbe21a024975cefa71785930fdec758e", "title": "On a Model of Associative Memory with Huge Storage Capacity"}, {"paperId": "ed332c92664cd64843a7ba9373d992e9547230f6", "title": "Dense Associative Memory for Pattern Recognition"}, {"paperId": "0e6824e137847be0599bb0032e37042ed2ef5045", "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"}, {"paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "title": "ImageNet Large Scale Visual Recognition Challenge"}, {"paperId": "947620a1854655ed91a86b90d12695e05be85983", "title": "1.1 Computing's energy problem (and what we can do about it)"}, {"paperId": "8aabf5ba342d37edc1ebd89ba898c70b0a4ca121", "title": "On the Convergence of the Concave-Convex Procedure"}, {"paperId": "ffb607e61e10a3bb54463b334aaf5ea9c7c04be6", "title": "The Concave-Convex Procedure"}, {"paperId": "6dc04a1b6be52df56be6581fcd6460a0fd5c5521", "title": "Multilayer feedforward neural networks with single powers-of-two weights"}, {"paperId": "24b9eebe49cf7e00cf50cf7b7d9243386a23fe7c", "title": "Neurons with graded response have collective computational properties like those of two-state neurons."}, {"paperId": "98b4d4e24aab57ab4e1124ff8106909050645cfa", "title": "Neural networks and physical systems with emergent collective computational abilities."}, {"paperId": "10fd7180b2c0f14e5575b4892e74932b983af822", "title": "Central Limit Theorems for Empirical Measures"}, {"paperId": "b8b45b14df9029562b8995c6ab7fd90a8810f312", "title": "GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale"}, {"paperId": "5767a2320890e77654406dc559becb5b4c07d83d", "title": "Analysis of"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "554e706938a3987eda3d9421bf17fa3d64329ea9", "title": "CS229T/STAT231: Statistical Learning Theory (Winter 2016)"}, {"paperId": null, "title": "NIST hand-book of mathematical functions"}, {"paperId": "5d90f06bb70a0a3dced62413346235c02b1aa086", "title": "Learning Multiple Layers of Features from Tiny Images"}, {"paperId": "162d958ff885f1462aeda91cd72582323fd6a1f4", "title": "Gradient-based learning applied to document recognition"}, {"paperId": "a129781d52d8a1cd24582b940dabb32e8f8c35d2", "title": "Fast neural networks without multipliers"}, {"paperId": null, "title": "Outlier Efficient Modern Hopfield Model 276\u2013286, Florence, Italy"}, {"paperId": null, "title": "Outlier-efficient hopfield layers for large transformer-based models"}, {"paperId": null, "title": "Blog post: Exploring softmax1, or \u201ccommunity research for the win!\u201d"}, {"paperId": null, "title": "revealt does BERT look at? an analysis of BERT\u2019s attention"}]}