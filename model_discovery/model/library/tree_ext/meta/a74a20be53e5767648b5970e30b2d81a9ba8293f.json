{"paperId": "a74a20be53e5767648b5970e30b2d81a9ba8293f", "title": "A Survey on Transformer Compression", "abstract": "Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.", "venue": "arXiv.org", "year": 2024, "citationCount": 10, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "A comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models, primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design."}, "embedding": {"model": "specter_v2", "vector": [0.33771175146102905, 0.42604270577430725, -0.27780795097351074, 0.1813376545906067, -0.647720992565155, 0.07627705484628677, 0.47251370549201965, -0.08163821697235107, -0.5462109446525574, -0.3542121350765228, 0.5298392176628113, -0.190238818526268, 0.1473146378993988, 0.04640169441699982, -0.18520446121692657, -0.053574658930301666, -0.3234134912490845, 0.2734465003013611, 0.15439030528068542, -0.4084581136703491, -0.10202483087778091, -0.6550424695014954, -1.1626125574111938, -0.024672776460647583, 0.5375041961669922, 1.1359299421310425, 0.2711911201477051, 0.7713677883148193, -0.6854877471923828, 0.6322848796844482, 0.6476635932922363, -0.6302176713943481, 0.2441032975912094, -0.15318341553211212, -0.24838100373744965, -0.04678951948881149, 0.6231870055198669, -0.543758749961853, -0.7270949482917786, 0.872178852558136, -0.3934136629104614, 0.27418768405914307, 0.3646722137928009, -0.6644575595855713, -0.302357941865921, 0.5569673776626587, 0.7574762105941772, 0.8767017126083374, -0.5061230063438416, -0.5634500980377197, 1.0632864236831665, -1.131524682044983, -0.025954576209187508, 1.5155056715011597, 0.7671555876731873, 0.21585595607757568, -0.28709226846694946, -0.6230967044830322, 0.30718934535980225, 0.32673728466033936, -0.738053560256958, -0.613334596157074, 0.1127188578248024, 0.07853659987449646, 1.7377636432647705, -0.17923900485038757, 0.16759362816810608, 0.25225797295570374, 0.09099656343460083, 0.9795213937759399, -0.10395587980747223, -1.0116925239562988, -0.22915218770503998, -0.23046906292438507, 0.17839904129505157, 1.0704556703567505, -0.3066871762275696, 0.11977162212133408, -0.7485266923904419, -0.021780095994472504, 0.09645562618970871, 0.05378367379307747, -0.04110504686832428, -0.03204537183046341, 0.2950485944747925, 0.9975988864898682, 0.42220744490623474, 0.5112934708595276, -0.12139842659235, 1.032800316810608, 0.6676280498504639, 0.21530064940452576, 0.01254511158913374, 0.23781803250312805, 0.18535323441028595, 0.43256404995918274, -1.0551706552505493, 0.21284808218479156, -0.43412914872169495, 0.8796918988227844, -0.18481068313121796, 0.6508937478065491, -0.7773848176002502, 0.5030858516693115, 1.384533405303955, 0.08243848383426666, 0.6534742116928101, -1.1163853406906128, 0.35114726424217224, -0.8821201324462891, -0.32043343782424927, -0.6579798460006714, 0.34901490807533264, -0.35699889063835144, -1.0558446645736694, -1.2413548231124878, -0.68626469373703, 0.42193788290023804, -1.2331430912017822, 0.6233300566673279, -0.9611427187919617, 0.18662697076797485, 0.12656792998313904, 0.10814408212900162, 0.3885292410850525, 0.7915370464324951, 0.44269174337387085, -0.2118956297636032, 0.9346778988838196, -0.8768333196640015, -0.7581722736358643, -0.9394069910049438, 0.33793532848358154, -0.4022243320941925, 0.2530147135257721, -0.15378890931606293, -1.4513568878173828, -0.9400225281715393, -0.7663701772689819, -0.25500354170799255, -0.333800733089447, 0.05550304427742958, 0.8536275625228882, 0.14359581470489502, -1.1423680782318115, 0.7642806172370911, -0.38277778029441833, -0.14159776270389557, 0.8824372887611389, 0.23971466720104218, 0.36616259813308716, -0.4033956825733185, -1.113380789756775, 0.43142277002334595, 0.0914582684636116, -0.6920757293701172, -0.1955937147140503, -0.43185675144195557, -1.2117565870285034, 0.2543543577194214, -0.028863491490483284, -0.5177091956138611, 1.3230831623077393, 0.12355036288499832, -0.8209890723228455, 0.21945133805274963, -0.5891764760017395, -0.4903302490711212, -0.09963960200548172, -0.2601780593395233, -0.20547491312026978, -0.09001652151346207, -0.15421025454998016, 0.6990452408790588, 0.9703127145767212, -0.09267888218164444, -0.21148981153964996, 0.5265065431594849, -0.33037710189819336, -0.17192400991916656, -0.5258574485778809, 0.8229918479919434, -0.6764801740646362, -0.40435317158699036, 0.4663642644882202, 0.8117223381996155, -0.07947659492492676, -0.15273918211460114, -0.3875850439071655, -0.5738789439201355, 0.9444538950920105, -0.36474138498306274, 1.225457787513733, -0.9346965551376343, -0.9886155128479004, -0.24648459255695343, 0.03873622789978981, 0.05314983054995537, -0.9653041958808899, 0.44142213463783264, -0.30958420038223267, 0.36675742268562317, -0.276590496301651, -1.162137746810913, -0.04416656121611595, -0.13840505480766296, -1.2423224449157715, -0.40934932231903076, 0.12710604071617126, 1.2362713813781738, -0.35230860114097595, -0.018186602741479874, 0.23371580243110657, 0.5385579466819763, -0.9257487654685974, 0.8340632915496826, -0.43125662207603455, 0.04239945858716965, 0.11293073743581772, 0.029284043237566948, 0.04924404248595238, -0.2850501537322998, 0.22372400760650635, -0.5300142765045166, 0.14160378277301788, 0.6258374452590942, 0.024651138111948967, 1.19419264793396, -0.17347051203250885, 0.8572872281074524, -0.4569477438926697, -0.6250542998313904, 0.40194451808929443, 0.41785717010498047, -0.36922842264175415, -0.4656224250793457, 0.42713025212287903, 0.5219598412513733, -0.560355544090271, 0.6680241823196411, 0.874591052532196, 0.4327628016471863, -0.26415836811065674, -0.07015600055456161, 0.820154070854187, -0.5253854393959045, 0.782977819442749, 0.29753145575523376, 0.7251006960868835, -0.05216006934642792, 0.16826558113098145, 0.33895668387413025, 0.28939691185951233, -0.6117740273475647, -0.22254890203475952, 0.4776982367038727, 0.6026837229728699, 1.043347716331482, 0.208293154835701, -0.7005252242088318, -0.6198311448097229, 0.12869298458099365, 0.7179672122001648, 1.0718657970428467, -0.3005515933036804, -0.5939025282859802, -0.6911949515342712, -0.2230842560529709, -0.5456032752990723, 0.12083228677511215, -0.14133766293525696, -0.4658372700214386, -0.5485905408859253, -0.9420667290687561, 0.8941430449485779, 0.3236021399497986, 1.3029683828353882, -0.3976327180862427, -0.30007681250572205, -0.4522201716899872, 0.008887127041816711, -0.761705756187439, -0.5291057825088501, 0.6170896887779236, -1.0279912948608398, -0.25042250752449036, 0.04028143361210823, -0.156573086977005, 0.06894904375076294, -0.5120618343353271, 0.8144802451133728, -0.3393460810184479, -0.16322055459022522, -0.05539679527282715, 0.6852555871009827, -0.5273771286010742, -0.8759685158729553, 0.006962597370147705, -0.0265902578830719, -0.474203884601593, 0.5669890642166138, 0.3281041085720062, 0.334921658039093, -0.3371465802192688, -0.43393105268478394, -0.09792616963386536, 0.034805137664079666, 0.058538392186164856, 1.1746350526809692, -0.007411098573356867, -0.36715325713157654, -0.7018176317214966, 1.164984941482544, 0.6383992433547974, -0.6578541398048401, 0.2717099189758301, -0.8654423952102661, -0.1644592434167862, 0.6040986776351929, -0.30093616247177124, -0.07258337736129761, -0.814950704574585, 0.19984720647335052, -1.0332741737365723, -0.009571352042257786, 0.1558435708284378, 0.5541462302207947, -0.036770448088645935, 0.03919142112135887, 0.7657415866851807, 0.5081281661987305, -0.2739090621471405, 0.6861722469329834, -0.7504415512084961, 0.6574402451515198, 0.6388967633247375, 0.20189471542835236, -0.268373966217041, 0.002656525233760476, -0.5351822972297668, -0.3709443211555481, 0.05014103278517723, 0.04529307410120964, -0.059739433228969574, 0.13761797547340393, -0.5505998730659485, -0.31000807881355286, -0.19247637689113617, -0.9169689416885376, 0.297493577003479, -0.3067528307437897, -0.41495078802108765, -0.536383867263794, -0.9826191663742065, -1.331275463104248, -0.500307559967041, -0.9112023115158081, -1.2417348623275757, 0.16268964111804962, 0.1401415765285492, -0.11855451762676239, -0.4438517987728119, -0.10901100188493729, -0.3289950489997864, 1.2200967073440552, -0.7563616037368774, 1.3151016235351562, -0.10275068879127502, -0.281631737947464, -0.08044686913490295, 0.22922861576080322, 0.9189828634262085, -0.2838021516799927, 0.25817012786865234, -1.129619836807251, 0.2416054606437683, 0.03473379835486412, -0.387492835521698, 0.7187461256980896, 0.5919336080551147, 0.8582504391670227, -0.20284566283226013, -0.35306766629219055, 0.563822865486145, 1.4003431797027588, -0.4314955472946167, 0.3539162278175354, 0.11592268943786621, 0.7462170720100403, -0.21980711817741394, -0.5528360605239868, 0.13867470622062683, -0.06821643561124802, 0.1778957098722458, 0.3753253221511841, 0.3575851023197174, -0.48957768082618713, -0.6250188946723938, 0.3558700680732727, 2.129092216491699, 0.4039667546749115, -0.010855713859200478, -1.0017491579055786, 0.4107447862625122, -0.8542643785476685, -0.6672604084014893, 0.9678381085395813, 0.639777421951294, 0.36516475677490234, -0.252244770526886, -0.5021899938583374, 0.46591660380363464, 0.5758624076843262, 0.45766371488571167, -0.17735233902931213, -0.8330027461051941, -0.09828556329011917, 0.7422346472740173, 0.30050039291381836, 0.7312402129173279, -0.5248762369155884, 0.5026187300682068, 14.724328994750977, 1.3038629293441772, -0.18403902649879456, 0.6873101592063904, 0.6825249195098877, 0.2519812285900116, -0.3684491217136383, -0.14775216579437256, -1.1894989013671875, -0.2599075436592102, 1.183455467224121, 0.17033973336219788, 0.5797334313392639, 0.10479430854320526, -0.37625396251678467, 0.49941179156303406, 0.05444415658712387, 0.7992488741874695, 0.49883002042770386, -1.5642123222351074, 0.6961305737495422, 0.26366257667541504, -0.07167699187994003, 0.45668619871139526, 0.9337837100028992, 0.8764682412147522, 0.1678893268108368, -0.6258193850517273, 0.45360615849494934, 0.43491408228874207, 1.1763813495635986, -0.017784155905246735, 0.3289605379104614, 0.46883487701416016, -0.9944733381271362, -0.5006822943687439, -0.7068189978599548, -0.8947739601135254, 0.23395313322544098, 0.0050016422756016254, -0.3401080071926117, -0.6251795291900635, -0.37066927552223206, 0.7331931591033936, -0.20059248805046082, 0.36113008856773376, -0.1260925829410553, 0.8688691854476929, -0.7079528570175171, 0.42937904596328735, 0.43541592359542847, 0.26176807284355164, 0.2005622833967209, -0.34596920013427734, 0.29127800464630127, -0.3030620217323303, 0.05587824806571007, 0.361301988363266, -0.6131921410560608, -0.3004048466682434, -0.5174738764762878, -0.5341199636459351, -0.08942317962646484, 0.4108932614326477, 0.3201570212841034, 0.37520796060562134, -0.31536680459976196, 0.0014596934197470546, 0.5630331635475159, 0.28403788805007935, -0.3836531639099121, -0.3344379663467407, 0.07915864139795303, -0.22183012962341309, 0.7023257613182068, 0.8478548526763916, -0.23163533210754395, -0.4574632942676544, -0.6583024859428406, -0.2856552004814148, 0.23737479746341705, -1.0271435976028442, -0.44252026081085205, 1.3130298852920532, -0.11777593940496445, -0.4434225559234619, 0.29872938990592957, -0.5445945262908936, 0.007603705860674381, 0.27636364102363586, -1.4482550621032715, -0.5670366883277893, 0.3758503496646881, -0.2967710793018341, -0.24750465154647827, -0.4149998724460602, 1.4005502462387085, 0.14889998733997345, -0.019051609560847282, -0.08369749039411545, 0.03231750801205635, -0.1655544936656952, -0.7122800946235657, -0.3729364573955536, 0.5808554887771606, 0.39626339077949524, 0.3214524984359741, 0.0654035285115242, -0.286796897649765, 0.2989601194858551, -0.3960673213005066, -0.3413117229938507, 0.9493914246559143, -0.5011134743690491, -0.6150630712509155, -0.7145300507545471, -0.8955597877502441, 0.4366191625595093, 0.5473558902740479, -0.3006192743778229, 0.1778877079486847, -0.2930564284324646, -0.7857367992401123, -0.24185298383235931, -0.9540592432022095, 0.029853221029043198, 0.193534255027771, -0.9651269912719727, -0.3761831223964691, -0.15416307747364044, 0.621950089931488, -0.6862838864326477, -0.18165630102157593, -0.02838563732802868, -0.13748745620250702, -0.09205103665590286, 1.11492121219635, -0.2541370093822479, 0.8866866230964661, 1.016942024230957, -0.7346748113632202, -0.41231611371040344, -0.04538170248270035, -0.8788809776306152, -0.28170153498649597, 0.09892529249191284, 0.3819665014743805, -0.29372310638427734, 0.20680493116378784, 0.6721580028533936, 0.18473631143569946, -0.5742098093032837, -1.1353875398635864, -0.1886250227689743, 0.1611485630273819, -0.8253337144851685, 0.23164735734462738, -0.7385082244873047, -0.4731350839138031, 0.25589802861213684, 0.4420526325702667, -0.1362992376089096, -0.31490015983581543, -0.6883764266967773, -0.18117357790470123, 0.10782215744256973, 0.15605425834655762, -0.5962492823600769, -0.568613588809967, -1.3964937925338745, 0.11942306160926819, -1.2454698085784912, -0.12188716232776642, -1.029464602470398, -0.43548768758773804, 0.20166048407554626, -0.2177463322877884, 0.19370341300964355, 0.8076724410057068, 0.11819282174110413, 0.17296263575553894, -0.23030488193035126, -0.5813606381416321, 1.1071510314941406, 0.8359372019767761, -0.8657345771789551, 0.22003977000713348, -0.2350015491247177, -0.021744171157479286, 0.5771468281745911, 0.39121806621551514, -0.6080316305160522, -1.2547087669372559, -1.309535264968872, 0.003753466298803687, -0.3771684765815735, -0.1134176254272461, -0.9439953565597534, 0.6956237554550171, 0.7672632932662964, -0.28765028715133667, 0.08378451317548752, 0.6991016268730164, -0.8712188005447388, -0.654597818851471, 0.49896401166915894, -0.7813053727149963, 0.3314723074436188, 0.4971257746219635, -0.29561784863471985, -0.6232675909996033, 0.48677587509155273, 0.23036541044712067, -0.955038845539093, -0.5171627402305603, 0.3549509644508362, -0.6985698938369751, 0.22500766813755035, -0.4386950731277466, -0.11596259474754333, -1.1697416305541992, -0.6660702228546143, -0.1140674576163292, -0.20569640398025513, -0.5460996031761169, 0.6532881855964661, 0.9413455128669739, -1.3476388454437256, 0.35887113213539124, 0.67667555809021, -0.33432477712631226, -0.19947533309459686, 0.1156730055809021, 0.5964128971099854, -0.485642671585083, 0.9154608845710754, 0.4030475318431854, 0.38862860202789307, -0.8178249597549438, -0.15049271285533905, 0.8774451017379761, -0.5392475128173828, -0.3075309991836548, 1.0445890426635742, -0.5905206799507141, -0.7120076417922974, 0.20378798246383667, -1.4384241104125977, -0.6827071905136108, -0.3088547885417938, 0.6304890513420105, 0.0022251936607062817, 0.05635237693786621, -0.296249657869339, -0.3071405291557312, 0.23823799192905426, -0.000855573161970824, -0.7307507991790771, 0.5279579758644104, -0.09858700633049011, -0.5833495855331421, 0.47842010855674744, 0.8979017734527588, -0.9238553047180176, -0.6325966119766235, -0.7837371826171875, -0.37059304118156433, -0.4511094093322754, 0.3969113230705261, -0.4004112184047699, -0.880621075630188, 1.0399723052978516, 0.6428107023239136, 0.2848035991191864, 0.8386803865432739, -0.24872736632823944, 0.5379443168640137, 0.7007495760917664, -0.09379728138446808, -0.36727091670036316, -0.6029430031776428, 1.5769693851470947, 1.365435242652893, -0.5283345580101013, 0.4015049934387207, -0.3771820068359375, -0.6145104169845581, 1.0298876762390137, -0.08657101541757584, -0.2839507460594177, 1.1146228313446045, -0.025785427540540695, 0.040721502155065536, 0.3239681124687195, -1.151274561882019, -0.18992558121681213, 0.7223522067070007, 1.22151517868042, 0.516933798789978, -0.34217962622642517, 0.40099915862083435, 0.9569340944290161, -0.04098162800073624, 0.24260957539081573, 0.3308192491531372, 0.4667010009288788, -0.4407398998737335, -0.08996030688285828, -0.20161010324954987, 0.9985418915748596, -0.8197188377380371, -1.0058292150497437, 0.4781729578971863, 0.6399696469306946, 0.35127949714660645, 0.6149780750274658, 0.9619877338409424, -0.2320530116558075, 0.45742905139923096, 0.5002501010894775, 0.6158388257026672, -0.36207011342048645, -0.27556294202804565, 0.27641117572784424, -0.6363055109977722, 0.07453840970993042, -0.30158814787864685, -0.2711733877658844, -0.034262724220752716, -0.20082254707813263, 0.40526673197746277, -0.12041176110506058, 0.33019548654556274, 0.9029751420021057, 0.5412936806678772, 0.3697825074195862, -0.7014595866203308, -0.04551323130726814, -0.6079334020614624, -0.9613907337188721, 0.09764214605093002, -0.7134230732917786, -0.031070657074451447, -0.20149461925029755, -0.10460658371448517, 0.08857990801334381]}, "authors": [{"authorId": "103603255", "name": "Yehui Tang"}, {"authorId": "2274472972", "name": "Yunhe Wang"}, {"authorId": "2148899357", "name": "Jianyuan Guo"}, {"authorId": "2273928979", "name": "Zhijun Tu"}, {"authorId": "2277423285", "name": "Kai Han"}, {"authorId": "4520244", "name": "Hailin Hu"}, {"authorId": "2259752319", "name": "Dacheng Tao"}], "references": [{"paperId": "59756f4e786d3212f793b8b709a3d37ebbc16d94", "title": "Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models"}, {"paperId": "7351898febca53d01453283c9b1a541b662e1ed3", "title": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models"}, {"paperId": "49b7baceecd32f81a08aa8e84e2fe71c2b879ee6", "title": "DistiLLM: Towards Streamlined Distillation for Large Language Models"}, {"paperId": "b24e899ec0f77eef2fc87a9b8e50516367aa1f97", "title": "VMamba: Visual State Space Model"}, {"paperId": "38c48a1cd296d16dc9c56717495d6e44cc354444", "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model"}, {"paperId": "745594bd0dc3e9dc86f74e100cd2c98ed36256c0", "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts"}, {"paperId": "d2b0cec5c7e222a8f2cafa48257982c3cabf41e2", "title": "CBQ: Cross-Block Quantization for Large Language Models"}, {"paperId": "689c358c5f9b5b1693a8bcc7e6e0460012f5cf9e", "title": "Sequential Modeling Enables Scalable Learning for Large Vision Models"}, {"paperId": "7bbc7595196a0606a07506c4fb1473e5e87f6082", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"}, {"paperId": "90a9f862a478476881538c6a75b7b55d8aebe3cd", "title": "Data-Free Distillation of Language Model by Text-to-Text Transfer"}, {"paperId": "ffdc017b1d2b493feaac9efa854882fe23d50dcf", "title": "QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models"}, {"paperId": "564855d475ed9197dd7516594557ff886ff623e5", "title": "Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding"}, {"paperId": "fdc53c2c10742464087c0525f77e32604827a21d", "title": "Efficient Streaming Language Models with Attention Sinks"}, {"paperId": "8ec117feff6ee10e3b20a19ac101fee5c99e14d7", "title": "LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression"}, {"paperId": "633e3fe49fe9c314f7245f77401c2e4a95e925a9", "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs"}, {"paperId": "00e889fcfaf4396a20f37f681cf8b14f3e878879", "title": "LLMCad: Fast and Scalable On-device Large Language Model Inference"}, {"paperId": "dcb74fb63acd87d3db0a77de89720300fb28b50a", "title": "A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking"}, {"paperId": "eb2c2330177f765038a2b17e2ee3498965865797", "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"}, {"paperId": "3ed75cde4b35ba0b867d6162b8d23b3fe5198d10", "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers"}, {"paperId": "9cd53423a22dd5e83d8d5669600437c59b570cbf", "title": "Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping"}, {"paperId": "81b268e98042864c025b401eb6a54dcb566486d5", "title": "A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations"}, {"paperId": "131ba9932572c92155874db93626cf299659254e", "title": "FLatten Transformer: Vision Transformer using Focused Linear Attention"}, {"paperId": "0b777965bfb066dcef9a86510a5d7f305b71db94", "title": "Less is More: Focus Attention for Efficient DETR"}, {"paperId": "240103933ffe3dac2179cc160a2bd91299357a53", "title": "Retentive Network: A Successor to Transformer for Large Language Models"}, {"paperId": "53e393c419506f2dfa3ab40a9ecafa385aea67e5", "title": "Cumulative Spatial Knowledge Distillation for Vision Transformers"}, {"paperId": "ce9435c82dc9b576f2037aa2f4357a520be9b2aa", "title": "SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference"}, {"paperId": "4bf8c6ccd054a1144e19eafff431841a3b9a3e50", "title": "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning"}, {"paperId": "af67be0fff8d087a0d8554b6e8998ab12409bbda", "title": "TensorGPT: Efficient Compression of the Embedding Layer in LLMs based on the Tensor-Train Decomposition"}, {"paperId": "e9e4a6768d6213da872fe726b6e5c07f6184b12b", "title": "Variation-aware Vision Transformer Quantization"}, {"paperId": "7d22ad3573101337bca2091fb0114b377c4f3db6", "title": "A Simple and Effective Pruning Approach for Large Language Models"}, {"paperId": "f5359f596e0306599b4aa4157e6fe03567b35c01", "title": "Knowledge Distillation of Large Language Models"}, {"paperId": "3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc", "title": "SqueezeLLM: Dense-and-Sparse Quantization"}, {"paperId": "b94e95d76001a1273303f11c6cd429d17f626b9b", "title": "FasterViT: Fast Vision Transformers with Hierarchical Attention"}, {"paperId": "db9507cdd3e2d7d9c90ed185bd831e55c62dcec9", "title": "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration"}, {"paperId": "7a1e71cb1310c4a873e7a4e54d1a6dab0553adce", "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only"}, {"paperId": "6bd3ee1ca608bc66a490f63f2fb107d79b44f3e2", "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models"}, {"paperId": "da1946bb4220e743e8f46946397a9b31e609df74", "title": "VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale"}, {"paperId": "c193eb176985a81ae64f63c5e50b2f11cfb7c4e6", "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "50bf60b1439368caa941b386d1ed0c364dd7fe38", "title": "Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model"}, {"paperId": "a10843d1349fff8d2a7d9722f800802187fef67f", "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"}, {"paperId": "aca65ea2730e3f49c0ff6fb7761e66756dc82255", "title": "VanillaNet: the Power of Minimalism in Deep Learning"}, {"paperId": "026b3396a63ed5772329708b7580d633bb86bec9", "title": "RWKV: Reinventing RNNs for the Transformer Era"}, {"paperId": "5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200", "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"}, {"paperId": "c23249337a53a1ebb24af46091d43737e5c8bf3c", "title": "Weight-Inherited Distillation for Task-Agnostic BERT Compression"}, {"paperId": "b95c0abfd9b820fe73fcb12182e2f05c23635740", "title": "Patch-wise Mixed-Precision Quantization of Vision Transformer"}, {"paperId": "9a83aeadc8db65fb6da39ec977360541cddaff5c", "title": "EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention"}, {"paperId": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e", "title": "StarCoder: may the source be with you!"}, {"paperId": "aad167be3c902388ea625da4117fcae4325b8b7d", "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes"}, {"paperId": "56fa65d8dc41708082f9b2ef7752c49cee9ebe01", "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation"}, {"paperId": "389ec3e8902a5dcfcde1adec735854e93f845937", "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"}, {"paperId": "a5036f31f0e629dc661f120b8c3b1f374d479ab8", "title": "Visual Instruction Tuning"}, {"paperId": "61a4023aad982d435dcde9f40bb9c2a735e88a9c", "title": "RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer"}, {"paperId": "eb3415db1b322b1f7bf95f8697aed701b0d40f88", "title": "Inference with Reference: Lossless Acceleration of Large Language Models"}, {"paperId": "2a44c6b7f291f625314a82ba3131e605009fd533", "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models"}, {"paperId": "7c4ebbc31bdac379bac9c44fa0792233a5b67f7b", "title": "Q-DETR: An Efficient Low-Bit Quantized Detection Transformer"}, {"paperId": "362cbfd0d05e139cd6cf049754098a6e1520b910", "title": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"}, {"paperId": "163b4d6a79a5b19af88b8585456363340d9efd04", "title": "GPT-4 Technical Report"}, {"paperId": "e60b6836b45ad0ae02a5fa663c8c31119f0c0a94", "title": "X-Pruner: eXplainable Pruning for Vision Transformers"}, {"paperId": "57e849d0de13ed5f91d086936296721d4ff75a75", "title": "LLaMA: Open and Efficient Foundation Language Models"}, {"paperId": "998ac3e945857cf2676ee7efdbaf443a0c6f820a", "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models"}, {"paperId": "ae3ac5509c445327a23431409624a1333aa825b0", "title": "What Matters In The Structured Pruning of Generative Language Models?"}, {"paperId": "42d3b9e9111efb70a11167096738d7dd344f7e5a", "title": "Oscillation-free Quantization for Low-bit Vision Transformers"}, {"paperId": "a1f8082505c7e90b0a033e1b9da0a97d67aad66c", "title": "Accelerating Large Language Model Decoding with Speculative Sampling"}, {"paperId": "fbd49b25bdab98c171af49962a41139c73dacbde", "title": "Specializing Smaller Language Models towards Multi-Step Reasoning"}, {"paperId": "909ad57ce8caa6b390a65ae09db352d27d8f3996", "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot"}, {"paperId": "5a77b508302771fc083bf24e0bcda8553c9b5421", "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"}, {"paperId": "a9e3e5dd7b30890553b7ae1c41f932e99192bb44", "title": "Large Language Models Are Reasoning Teachers"}, {"paperId": "f9ad1fffa1cc76fd5db3ff758c0839492c5147c4", "title": "In-context Learning Distillation: Transferring Few-shot Learning Ability of Pre-trained Language Models"}, {"paperId": "a05ebb7be3de58b28af75f9c4d340fa03eeb7fb1", "title": "Quantformer: Learning Extremely Low-Precision Vision Transformers"}, {"paperId": "126a4776ff8315fd506766cb8f3c722cf746ad9e", "title": "Teaching Small Language Models to Reason"}, {"paperId": "529ddd65f6c252aaea91d6a8e5b55b7bc3951841", "title": "RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers"}, {"paperId": "8fd462f6248d5e3f1b6602697c09489086b5655f", "title": "Distilling Reasoning Capabilities into Smaller Language Models"}, {"paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644", "title": "Fast Inference from Transformers via Speculative Decoding"}, {"paperId": "8b87d39baf53d982bad7df8ab6c5c8e67c124c67", "title": "NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers"}, {"paperId": "2c994fadbb84fb960d8306ee138dbeef41a5b323", "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"}, {"paperId": "977351c92f156db27592e88b14dee2c22d4b312a", "title": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference"}, {"paperId": "964bd39b546f0f6625ff3b9ef1083f797807ef2e", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"}, {"paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6", "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"}, {"paperId": "7d29a84a589aa5655e5d3fed8d725ea472816599", "title": "Explanations from Large Language Models Make Small Reasoners Better"}, {"paperId": "dfdb2894d50e095ce97f994ed6cee38554c4c84f", "title": "Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer"}, {"paperId": "b5e71069f091d52f474a2928ed07b6546157af82", "title": "Towards Accurate Post-Training Quantization for Vision Transformer"}, {"paperId": "1d26c947406173145a4665dd7ab255e03494ea28", "title": "GLM-130B: An Open Bilingual Pre-trained Model"}, {"paperId": "3f6243097a58e386aea1215fed4f372dee07a100", "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"}, {"paperId": "70e91e16eb321067d9402710e14a40cf28311f73", "title": "Mega: Moving Average Equipped Gated Attention"}, {"paperId": "ac0730c8cf3a148b56f01def5ff0b809460481f8", "title": "PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers"}, {"paperId": "d40c667826c0274350104b199143adc502a41a38", "title": "ViTKD: Practical Guidelines for ViT feature knowledge distillation"}, {"paperId": "30a7390ec0103684eba9fb6bde1983d706fb57b3", "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning"}, {"paperId": "f27c847e2909f30745f4a3528b574f5acfd76ea7", "title": "Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization"}, {"paperId": "2fe71acc2c3f1e75b6149dea72838f0b594ad013", "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers"}, {"paperId": "2ef60a4ea4ea53056be811ff55679eb59fb4b586", "title": "Confident Adaptive Language Modeling"}, {"paperId": "9fb327c55a30b9771a364f45f33f77778756a164", "title": "I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference"}, {"paperId": "d451901a6a12c61179289cac7a4588a86c234112", "title": "Width & Depth Pruning for Vision Transformers"}, {"paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9", "title": "Long Range Language Modeling via Gated State Spaces"}, {"paperId": "dac3a172b504f4e33c029655e9befb3386e5f63a", "title": "Emergent Abilities of Large Language Models"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "87c5b281fa43e6f27191b20a8dd694eda1126336", "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"}, {"paperId": "bf6ce546c589fa8054b3972b266532664914bd21", "title": "Fast Vision Transformers with HiLo Attention"}, {"paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221", "title": "OPT: Open Pre-trained Transformer Language Models"}, {"paperId": "3a1dbfb6875bfac8251627d60db313623fbb8b04", "title": "DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers"}, {"paperId": "c26bb68806a992bf4fc85b5639e1657a445c4781", "title": "On the Representation Collapse of Sparse Mixture of Experts"}, {"paperId": "9e82736043eebe3f71eb86cbef6e2ac45306ece5", "title": "Structured Pruning Learns Compact and Accurate Models"}, {"paperId": "8342b592fe238f3d230e4959b06fd10153c45db1", "title": "Training Compute-Optimal Large Language Models"}, {"paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87", "title": "Diagonal State Spaces are as Effective as Structured State Spaces"}, {"paperId": "73c722148ed4a5301dc75ae291b647a1915b8ecd", "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"}, {"paperId": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a", "title": "Unified Visual Transformer Compression"}, {"paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c", "title": "Training language models to follow instructions with human feedback"}, {"paperId": "fb5324a22f3e3208a07634433d8e3e403876582f", "title": "Patch Similarity Aware Data-Free Quantization for Vision Transformers"}, {"paperId": "625270a54be430ad6262f848babb0d106af5b183", "title": "Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations"}, {"paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5", "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"}, {"paperId": "a3b42a83669998f65df60d7c065a70d07ca95e99", "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}, {"paperId": "13f7a106bb3814ad1fab25fd1356e99e91f402d3", "title": "Q-ViT: Fully Differentiable Quantization for Vision Transformer"}, {"paperId": "0d9b8ccb1135b8e380dd8015b080158c6aae3ae5", "title": "QuadTree Attention for Vision Transformers"}, {"paperId": "5ab70d95ca49702a3dd49b39d9396d8136b52311", "title": "Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space"}, {"paperId": "80d0116d77beeded0c23cf48946d9d10d4faee14", "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"}, {"paperId": "68f141724814839d556a989646194be88641b143", "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher"}, {"paperId": "e0d272e01929024f28f0f7cacf26177cd60b3ee7", "title": "Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity"}, {"paperId": "1ee05cd919590eaba129caa0fda5e850c87b75a5", "title": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer"}, {"paperId": "b476c932e959cfe645911786f1a070c70b5375c6", "title": "An Image Patch is a Wave: Phase-Aware Vision MLP"}, {"paperId": "39a620939887c9fc1f9bdd7ecfabde985a4aad3a", "title": "PTQ4ViT: Post-training Quantization for Vision Transformers with Twin Uniform Quantization"}, {"paperId": "57150ca7d793d6f784cf82da1c349edf7beb6bc2", "title": "MetaFormer is Actually What You Need for Vision"}, {"paperId": "ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51", "title": "Efficiently Modeling Long Sequences with Structured State Spaces"}, {"paperId": "ee8984a6712791d4e0f2c776dad8119a3b893dd9", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training"}, {"paperId": "574072ae4556649de1c59d8f284955730f5a71a0", "title": "A Short Study on Compressing Decoder-Based Language Models"}, {"paperId": "b6f616e9305e59c9dc7ccf33c311ede47584caf6", "title": "Kronecker Decomposition for GPT Compression"}, {"paperId": "521ccc898395a2818fced22b4cf371b0e5121f94", "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models"}, {"paperId": "c051ee2ad7ac203a26fa8f50eb6312424c729b27", "title": "Global Vision Transformer Pruning with Hessian-Aware Saliency"}, {"paperId": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e", "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer"}, {"paperId": "e9a09f8e474b4c74c700ebbe84d5b0696395a521", "title": "Towards Efficient Post-training Quantization of Pre-trained Language Models"}, {"paperId": "73bcf4577284fa116ee73487b7cbb85c8266eaa0", "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"}, {"paperId": "01b1293ddea9bcd6df1185b0b934503de01d6561", "title": "Block Pruning For Faster Transformers"}, {"paperId": "58970a426b687bb080b7fed3b4b78ab1ebaa56f4", "title": "Hire-MLP: Vision MLP via Hierarchical Rearrangement"}, {"paperId": "b4ac117b20e06d3e98d9eb3b28ca47e1a1e5dd5d", "title": "Automatic Mixed-Precision Quantization Search of BERT"}, {"paperId": "f75cddf2d42ed01b34686704eb3504becef67442", "title": "CycleMLP: A MLP-like Architecture for Dense Prediction"}, {"paperId": "71363797140647ebb3f540584de0a8758d2f7aa2", "title": "AS-MLP: An Axial Shifted MLP Architecture for Vision"}, {"paperId": "0b036cd5dfc49d835d0c759c8ca31d89f2410e65", "title": "CMT: Convolutional Neural Networks Meet Vision Transformers"}, {"paperId": "66775d9f16b3f4ca43dba2b31c7c42ca6dcba72b", "title": "GLiT: Neural Architecture Search for Global and Local Image Transformer"}, {"paperId": "c723187a2230749b1e706df2217e928c8271a660", "title": "Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation"}, {"paperId": "c156b1b30e3dd9284615e5304f2fb2826c09d0ff", "title": "Learned Token Pruning for Transformers"}, {"paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798", "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"}, {"paperId": "9b6af0e358e76d22f209c75b1702c3e6ea7815b1", "title": "Global Filter Networks for Image Classification"}, {"paperId": "48418b285a92376a38daafa664a2dd07d42e3fe3", "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers"}, {"paperId": "d645bd08fc19d52164695f9cd5ae863345459a06", "title": "AutoFormer: Searching Transformers for Visual Recognition"}, {"paperId": "c295391129426d89ec58cebb049d1cd2e976deec", "title": "Post-Training Quantization for Vision Transformer"}, {"paperId": "e43eaeca5077d01061a38aebd24f8e3fa5948ad9", "title": "Co-advise: Cross Inductive Bias Distillation"}, {"paperId": "2a805d0e1b067444a554c5169d189fa1f649f411", "title": "Scaling Vision Transformers"}, {"paperId": "0611d2f2ea6a3c8fb8534f42758a5a3e9c7bc8fe", "title": "Hash Layers For Large Sparse Models"}, {"paperId": "efbe9f591090018f78b42c84613c8afda9292fdb", "title": "Chasing Sparsity in Vision Transformers: An End-to-End Exploration"}, {"paperId": "f43b98fcc2d56c60fc71bce96374c1e6b8e12c66", "title": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer"}, {"paperId": "33fd56e5067a1e8a9713378af3e1c1c08d5ce93b", "title": "Patch Slimming for Efficient Vision Transformers"}, {"paperId": "2e8149dafb864ec3675087c99bf5572fcf4eb170", "title": "RegionViT: Regional-to-Local Attention for Vision Transformers"}, {"paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2", "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification"}, {"paperId": "07e987364bf0be1949e379f976f8dea675977337", "title": "MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens"}, {"paperId": "14b97585f136671742f6ce4151081e487b1fc1fe", "title": "Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition"}, {"paperId": "fb987ebe5ff5276fbbe6a5c5b16b6bfd759afa37", "title": "KVT: k-NN Attention for Boosting Vision Transformers"}, {"paperId": "d5e999aae76d5270ef272076979c809817458212", "title": "An Attention Free Transformer"}, {"paperId": "48a6aadf7fd6a1de64a6971ae3eeb24aae007bb5", "title": "ResMLP: Feedforward Networks for Image Classification With Data-Efficient Training"}, {"paperId": "fc92009ab34045f9e6d490684c7761f768e88c54", "title": "Beyond Self-Attention: External Attention Using Two Linear Layers for Visual Tasks"}, {"paperId": "67571d29190faea9fbd104acd16274f8c4edf254", "title": "MLP-Mixer: An all-MLP Architecture for Vision"}, {"paperId": "6709d5583f658f589ae6a2184805933aceb18849", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers"}, {"paperId": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f", "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"}, {"paperId": "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d", "title": "Consistent Accelerated Inference via Confident Adaptive Transformers"}, {"paperId": "93efaf8c27940aaef145d8bcbca957be634d26e5", "title": "Vision Transformer Pruning"}, {"paperId": "5b68522f58b61e7235b852677337ef3725075fd9", "title": "Co-Scale Conv-Attentional Image Transformers"}, {"paperId": "003326a15fc4a8833785a47a741d7712474fa256", "title": "LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference"}, {"paperId": "b15ea460c77a4ee8aa159a30ab0331deedfcf392", "title": "BASE Layers: Simplifying Training of Large, Sparse Models"}, {"paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880", "title": "Rethinking Spatial Dimensions of Vision Transformers"}, {"paperId": "0eff37167876356da2163b2e396df2719adf7de9", "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification"}, {"paperId": "ac591dbf261777e05d89c27f9a7bcb06f88aab5a", "title": "Scalable Vision Transformers with Hierarchical Pooling"}, {"paperId": "9ed25f101f19ea735ca300848948ed64064b97ca", "title": "Random Feature Attention"}, {"paperId": "0ae67202f0584afccefa770865d14a46655d2975", "title": "Transformer in Transformer"}, {"paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4", "title": "Learning Transferable Visual Models From Natural Language Supervision"}, {"paperId": "3e398bad2d8636491a1034cc938a5e024c7aa881", "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"}, {"paperId": "b4d207a2096aee4a3764933373eef6edb574c952", "title": "Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks"}, {"paperId": "5e38dc1ccf33ac1df09b8eb6476f110cb3d1966f", "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch"}, {"paperId": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85", "title": "Nystr\u00f6mformer: A Nystr\u00f6m-Based Algorithm for Approximating Self-Attention"}, {"paperId": "1d5c8c6e5a774d2fef8d92bd28670a6345a97f7a", "title": "CKConv: Continuous Kernel Convolution For Sequential Data"}, {"paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9", "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"}, {"paperId": "fdacf2a732f55befdc410ea927091cad3b791f13", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"}, {"paperId": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8", "title": "I-BERT: Integer-only BERT Quantization"}, {"paperId": "c375e121926db9551f224ff235018ea38bb159b7", "title": "BinaryBERT: Pushing the Limit of BERT Quantization"}, {"paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71", "title": "Training data-efficient image transformers & distillation through attention"}, {"paperId": "6f6f73e69ee0d9d5d7d088bb882db1851d98175a", "title": "Pre-Trained Image Processing Transformer"}, {"paperId": "1013750582c20bbdf1164127b5f26b1e06e817e3", "title": "MixKD: Towards Efficient Distillation of Large-scale Language Models"}, {"paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a", "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"}, {"paperId": "b285b90ab4d6d1d726efc726b92f4075363004e6", "title": "SCOP: Scientific Control for Reliable Neural Network Pruning"}, {"paperId": "3fbf6339273c50b04e886fa9bd4ad18c952a683d", "title": "Rethinking Attention with Performers"}, {"paperId": "097210dc65924f8ce59523faf444e635523dc714", "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"}, {"paperId": "0964490205fdc38c2f0980c9d778069089ca92e3", "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections"}, {"paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3", "title": "Big Bird: Transformers for Longer Sequences"}, {"paperId": "725264948d7b6946259af5b8d966e996b9570f99", "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"}, {"paperId": "1882f194cb43828852cc052887671e55a80f945a", "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"}, {"paperId": "6f68e1bb253925d8431588555d3010419f322e04", "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"}, {"paperId": "9bb5665fe48e7122beda73a53316de9f7f243b19", "title": "APQ: Joint Search for Network Architecture, Pruning and Quantization Policy"}, {"paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0", "title": "Language Models are Few-Shot Learners"}, {"paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e", "title": "End-to-End Object Detection with Transformers"}, {"paperId": "8747f028acccde9ee7c35c858da8091613d3e574", "title": "Faster Depth-Adaptive Transformers"}, {"paperId": "c5cc2340766d68ece08bb1520d357bcf8c03ad48", "title": "The Right Tool for the Job: Matching Model and Instance Complexities"}, {"paperId": "925ad2897d1b5decbea320d07e99afa9110e09b2", "title": "Longformer: The Long-Document Transformer"}, {"paperId": "2573af4e13d9a5dddb257d22cd38a600528d9a8b", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, {"paperId": "5d34881ff68bd203ff790187e7e5c9e034389cfa", "title": "FastBERT: a Self-distilling BERT with Adaptive Inference Time"}, {"paperId": "9c5a239b75bade55c830b164e2fadc424e879137", "title": "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models"}, {"paperId": "1c332cfa211400fc6f56983fb01a6692046116dd", "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"}, {"paperId": "3bcb17559ce96eb20fa79af8194f4af0380d194a", "title": "Pre-trained models for natural language processing: A survey"}, {"paperId": "850464c9006261bd632c4203f3e630db09a32faf", "title": "Comparing Rewinding and Fine-tuning in Neural Network Pruning"}, {"paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e", "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers"}, {"paperId": "43f2ad297941db230c089ba353efc3f281ab678c", "title": "5\u5206\u3067\u5206\u304b\u308b!? \u6709\u540d\u8ad6\u6587\u30ca\u30ca\u30e1\u8aad\u307f\uff1aJacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "bdbf780dfd6b3eb0c9e980887feae5f23af15bc4", "title": "GLU Variants Improve Transformer"}, {"paperId": "2e27f119e6fcc5477248eb0f4a6abe8d7cf4f6e7", "title": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing"}, {"paperId": "94f94e8892261d0377159379ca5a166ceae19a14", "title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination"}, {"paperId": "765866ecb5fe6a225d4e791498caf6a8351c16c7", "title": "Faster Transformer Decoding: N-gram Masked Self-Attention"}, {"paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500", "title": "Reformer: The Efficient Transformer"}, {"paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1", "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"}, {"paperId": "f51497f463566581874c941353dd9d80069c5b77", "title": "Compressive Transformers for Long-Range Sequence Modelling"}, {"paperId": "dc52b09089704ebd6f471177474bc29741c50023", "title": "Fast Transformer Decoding: One Write-Head is All You Need"}, {"paperId": "7be8c119dbe065c52125ee7716601751f3116844", "title": "Generalization through Memorization: Nearest Neighbor Language Models"}, {"paperId": "6c4b76232bb72897685d19b3d264c6ee3005bc2b", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, {"paperId": "4585611042d2be0d997ee135e3fe219d668db9ec", "title": "Depth-Adaptive Transformer"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "c95383f251a62c63217586059c67f63507c3e839", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"}, {"paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"}, {"paperId": "f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1", "title": "Reducing Transformer Depth on Demand with Structured Dropout"}, {"paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a", "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"}, {"paperId": "5b446648504afeecf7c73028aa02c2da16db6224", "title": "Hint-Based Training for Non-Autoregressive Machine Translation"}, {"paperId": "4fb8fd55b476909a26a8dc594e0ae98d4923ad4d", "title": "Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT"}, {"paperId": "bb3f24186972fbc6d8dcd3327dabe7da1e0e4ce8", "title": "Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks"}, {"paperId": "2f9d4887d0022400fc40c774c4c78350c3bc5390", "title": "Small and Practical BERT Models for Sequence Labeling"}, {"paperId": "80cf2a6af4200ecfca1c18fc89de16148f1cd4bf", "title": "Patient Knowledge Distillation for BERT Model Compression"}, {"paperId": "93ad19fbc85360043988fa9ea7932b7fdf1fa948", "title": "Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation"}, {"paperId": "07a64686ce8e43ac475a8d820a8a9f1d87989583", "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"}, {"paperId": "b03c7ff961822183bab66b2e594415e585d3fd09", "title": "Are Sixteen Heads Really Better than One?"}, {"paperId": "1a858b96d2fdfeadf8c0f7126cbd55825223fb9d", "title": "HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision"}, {"paperId": "21da617a0f79aabf94272107184606cefe90ab75", "title": "Generating Long Sequences with Sparse Transformers"}, {"paperId": "947928b4e7ff7ebb4a65d88d9c553a1fe5da7070", "title": "Data-Free Learning of Student Networks"}, {"paperId": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89", "title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks"}, {"paperId": "2a31319e73d4486716168b65cdf7559baeda18ce", "title": "Star-Transformer"}, {"paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6", "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"}, {"paperId": "cf440ccce4a7a8681e238b4f26d5b95109add55d", "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity"}, {"paperId": "8f880afbfea328c496d1ab96a123bb57d4b506b5", "title": "A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers"}, {"paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329", "title": "Image Transformer"}, {"paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f", "title": "Deep Contextualized Word Representations"}, {"paperId": "4feef0fd284feb1233399b400eb897f59ec92755", "title": "mixup: Beyond Empirical Risk Minimization"}, {"paperId": "3b4d671a8c7018c0b42673ba581e5ff3ae762d6c", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression"}, {"paperId": "ee53c9480132fc0d09b1192226cb2c460462fd6d", "title": "Channel Pruning for Accelerating Very Deep Neural Networks"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "896de8418884f4aab1ae4a60027500c9e8baffc3", "title": "BranchyNet: Fast inference via early exiting from deep neural networks"}, {"paperId": "57a10537978600fd33dcdd48922c791609a4851a", "title": "Sequence-Level Knowledge Distillation"}, {"paperId": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"}, {"paperId": "d9e022b5b10af145e728471e13aa07783b509e0e", "title": "Fast approximate computations with Cauchy matrices and polynomials"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "8604f376633af8b347e31d84c6150a93b11e34c2", "title": "FitNets: Hints for Thin Deep Nets"}, {"paperId": "eb42cf88027de515750f230b23b1a057dc782108", "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition"}, {"paperId": "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions"}, {"paperId": "d770060812fb646b3846a7d398a3066145b5e3c8", "title": "Do Deep Nets Really Need to be Deep?"}, {"paperId": "eff61216e0136886e1158625b1e5a88ed1a7cbce", "title": "Predicting Parameters in Deep Learning"}, {"paperId": "a9c1379fa08a07e28bb85343f7f7d4d09c302bac", "title": "Early exit optimizations for additive machine learned ranking systems"}, {"paperId": "30c9bb327b7f2b9f1d1e5b69b9d0c97b410948d9", "title": "Model compression"}, {"paperId": "2e9d221c206e9503ceb452302d68d10e293f2a10", "title": "Long Short-Term Memory"}, {"paperId": "35c0183e9940feb567b4417115be8460bc127cfa", "title": "Linear System Theory and Design"}, {"paperId": "052b1d8ce63b07fec3de9dbb583772d860b7c769", "title": "Learning representations by back-propagating errors"}, {"paperId": "8b16dc5b4c0728147eef1647a6ab7f786333b76c", "title": "Sparse Token Transformer with Attention Back Tracking"}, {"paperId": "72c03b873e8c5cd86b15bf73186df341da4731c9", "title": "Prune and Tune: Improving Efficient Pruning Techniques for Massive Language Models"}, {"paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af", "title": "Are Emergent Abilities of Large Language Models a Mirage?"}, {"paperId": "51cda783aa6a97e0b3b5915a2bb5a35f31f3c083", "title": "GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models"}, {"paperId": "0088b2b6f7983a9ac1b53e34a307d68a3383f42c", "title": "Structured Pruning for Efficient Generative Pre-trained Language Models"}, {"paperId": "e3dfeb8be76960036fbb4439e7cff4b9c7184998", "title": "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Theory of Mind"}, {"paperId": "81051b830a4f5606106765902a51ba281c9230f9", "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling"}, {"paperId": "104f7a96eba307056e1038e183ee8c24d009ba13", "title": "nuQmm: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models"}, {"paperId": "76579d7425474606583aa82e2b16702980b9a03a", "title": "SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization"}, {"paperId": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"}, {"paperId": null, "title": "Efficient content-based sparse attention with routing trans-formers"}, {"paperId": "9405cc0d6169988371b2755e573cc28650d14dfe", "title": "Language Models are Unsupervised Multitask Learners"}, {"paperId": null, "title": "Bert and pals: Projected attention layers for efficient adaptation in multi-task learning"}, {"paperId": null, "title": "Distilling bert for natural language understanding"}, {"paperId": null, "title": "Sheared"}, {"paperId": null, "title": "Accurate fully binarized bert"}, {"paperId": null, "title": "Vitas: Vision transformer architecture search"}, {"paperId": null, "title": "Program-aided distillation specializes large models in reasoning"}, {"paperId": null, "title": "int8 (): 8-bit matrix multiplication for trans-formers at scale"}]}