{"paperId": "716388872d0620617cf48e670c991a05b4518f9c", "title": "ALPINE: An adaptive language-agnostic pruning method for language models for code", "abstract": "Language models of code have demonstrated state-of-the-art performance across various software engineering and source code analysis tasks. However, their demanding computational resource requirements and consequential environmental footprint remain as significant challenges. This work introduces ALPINE, an adaptive programming language-agnostic pruning technique designed to substantially reduce these models' computational overhead. The proposed method offers a pluggable layer that can be integrated with all Transformer-based models. With ALPINE, input sequences undergo adaptive compression throughout the pipeline, reaching a size up to $\\times 3$ less their initial size, resulting in significantly reduced computational load. Our experiments on two software engineering tasks, defect prediction and code clone detection across three language models CodeBERT, GraphCodeBERT and UniXCoder show that ALPINE achieves up to a 50% reduction in FLOPs, a 58.1% decrease in memory footprint, and a 28.1% improvement in throughput on average. This led to a reduction in CO2 by up to $44.85$%. Importantly, it achieves the reduction in computation resources while maintaining up to 98.1% of the original predictive performance. These findings highlight the potential of ALPINE in making language models of code more resource-efficient and accessible while preserving their performance, contributing to the overall sustainability of adopting language models in software development. Also, it sheds light on redundant and noisy information in source code analysis corpora, as shown by the substantial sequence compression achieved by ALPINE.", "venue": "", "year": 2024, "citationCount": 0, "influentialCitationCount": 0, "openAccessPdf": null, "tldr": {"model": "tldr@v2.0.0", "text": "This work introduces ALPINE, an adaptive programming language-agnostic pruning technique designed to substantially reduce these models' computational overhead and sheds light on redundant and noisy information in source code analysis corpora, as shown by the substantial sequence compression achieved by ALPINE."}, "embedding": {"model": "specter_v2", "vector": [0.22146140038967133, 0.03990832343697548, -0.553743839263916, 0.5244480967521667, -0.17392900586128235, -0.18219512701034546, -0.039815548807382584, 0.09630245715379715, 0.1659301370382309, -0.4199761152267456, 0.11494147777557373, -0.4062636196613312, 0.1420747935771942, -0.2783443033695221, -0.34684616327285767, 0.13568133115768433, -0.5661793351173401, 0.007483838591724634, -0.09185333549976349, 0.12860597670078278, 0.36708903312683105, -0.45858657360076904, -1.267236590385437, 0.043231938034296036, 0.8911438584327698, 0.46030479669570923, 0.0771721825003624, 1.0484273433685303, -0.7713092565536499, 0.6656439900398254, 0.8387069702148438, -0.38298049569129944, 0.1972855180501938, -0.3392581641674042, -0.1394031047821045, 0.04826703667640686, 0.07824499160051346, 0.33338892459869385, -0.2955065667629242, 0.7581818103790283, -0.6055562496185303, -0.03106820210814476, 0.045566264539957047, -0.7683213353157043, -0.24022570252418518, 0.6889851093292236, 0.2442629635334015, 0.8054527640342712, -0.10129751265048981, -0.20067326724529266, 1.166088581085205, -1.3309149742126465, 0.09219949692487717, 0.970529317855835, 0.9883294701576233, 0.06425457447767258, -0.4320404529571533, -0.3919934630393982, -0.15395542979240417, -0.3804996907711029, -0.8851500749588013, -0.14254720509052277, -0.09435363858938217, -0.8313804268836975, 2.101017475128174, -0.44167956709861755, -0.05639790743589401, -0.05661281570792198, 0.3887239098548889, 0.8178943991661072, -0.15368229150772095, -1.0033727884292603, 0.18120525777339935, 0.05264243111014366, -0.12633143365383148, 1.2462446689605713, -0.05184624344110489, 0.12004044651985168, -1.0255707502365112, -0.5898598432540894, 0.2737460434436798, 0.23976650834083557, 0.15166139602661133, -0.5950828790664673, 0.112429179251194, 0.6616297364234924, 0.053222838789224625, 0.5727278590202332, -0.08879253268241882, 0.8784880042076111, 0.663581371307373, 0.35077694058418274, 0.31094861030578613, 0.7915083169937134, -0.11901694536209106, 0.11001234501600266, -0.7779869437217712, -0.10123512148857117, -0.032960280776023865, 0.9172036051750183, 0.10914459079504013, 0.7759082913398743, -0.30389320850372314, 0.4067058265209198, 1.1978901624679565, -0.00372500903904438, 0.5105762481689453, -0.5405030250549316, 0.2530002295970917, 0.2746850848197937, -0.13404366374015808, -0.449543833732605, 0.061497174203395844, -0.20342767238616943, -0.5774688720703125, -0.9832696914672852, -1.1268634796142578, 0.15864749252796173, -0.6803213357925415, 0.1702798455953598, -0.43862536549568176, 0.27404630184173584, 0.10910529643297195, -0.19364546239376068, 0.2678077220916748, 0.4484997093677521, 0.2820276618003845, -0.1522063910961151, 0.4440516531467438, -1.0943777561187744, -0.11398018896579742, -1.292081594467163, 0.7731516361236572, -0.43911001086235046, 0.1320418417453766, -0.3857939541339874, -1.2921600341796875, -0.7725008130073547, -0.6098663210868835, 0.2763969898223877, -0.2996896505355835, 0.4196717441082001, 1.027342677116394, 0.4421022832393646, -0.8604747653007507, 0.6226321458816528, -0.2861482799053192, -0.274795800447464, 0.2942471504211426, 0.05906633660197258, 0.49457883834838867, -0.2578293979167938, -0.567988395690918, 0.12697264552116394, 0.17595812678337097, -1.1498985290527344, -0.4351540207862854, -0.7210494875907898, -1.1045749187469482, 0.2355576902627945, 0.3734469413757324, -0.38279154896736145, 0.9620295763015747, -0.15421907603740692, -0.6953520178794861, 0.4691915214061737, -0.19713547825813293, -0.08811682462692261, 0.05818871408700943, -0.04480331018567085, -0.5439299941062927, -0.33024370670318604, -0.002816662425175309, -0.24487467110157013, 0.3791540563106537, -0.22324100136756897, -0.08942700177431107, 0.6962676048278809, -0.15158767998218536, -0.544010579586029, -0.3240542709827423, 1.3287551403045654, -0.3303174376487732, -0.30546149611473083, 0.2615146040916443, 0.5308965444564819, -0.5284762382507324, 0.12717413902282715, -0.11942722648382187, -0.34042811393737793, 0.38721969723701477, -0.2839631140232086, 1.5257517099380493, -0.9602082371711731, -0.6340101361274719, -0.17671328783035278, -0.231841579079628, -0.21283678710460663, -0.5215714573860168, 0.7185065746307373, -0.6574694514274597, 0.780634343624115, -0.8095038533210754, -0.512023389339447, -0.3047223687171936, -0.38419798016548157, -0.5478854775428772, -0.10058735311031342, 0.44007381796836853, 0.9445244073867798, -0.9501336216926575, 0.20572595298290253, 0.010041952133178711, -0.1281358152627945, -1.1791740655899048, 1.0978426933288574, -0.44655683636665344, -0.31705257296562195, 0.27306002378463745, 0.2618066668510437, 0.16868653893470764, -0.2956206798553467, 0.338222861289978, 0.3461184799671173, -0.4395207166671753, 0.43093034625053406, -0.10258550196886063, 1.440056324005127, -0.6885898113250732, 0.36042889952659607, 0.030561933293938637, -0.29931893944740295, 0.6411137580871582, 0.42318764328956604, 0.23837991058826447, -0.16785107553005219, 0.19362446665763855, 0.6546621322631836, -0.3691473603248596, -0.29358136653900146, 0.4926236867904663, 0.7586477398872375, -0.47565019130706787, 0.45224812626838684, 0.9150196313858032, -0.6988535523414612, 0.9954357743263245, 0.18165132403373718, 1.1109583377838135, 0.4449412524700165, 0.6293736100196838, -0.1847127079963684, 0.5680059790611267, -0.5172264575958252, 0.006460999604314566, 0.223258376121521, 0.6709092855453491, 0.8699110150337219, 0.5079788565635681, -0.7362925410270691, -0.4942460358142853, 0.17042812705039978, 1.1325011253356934, 1.3226584196090698, -0.2005690336227417, -0.5728400945663452, -0.5824462175369263, -0.517468273639679, -0.10454586893320084, 0.13262246549129486, 0.0629768967628479, -0.6683594584465027, -0.3911431133747101, -1.15433669090271, 0.9044170379638672, 0.3987591564655304, 0.805172860622406, -0.386991411447525, -0.5493202805519104, -0.26184478402137756, 0.4233713746070862, -0.22809858620166779, -0.4422486424446106, 0.71720951795578, -0.453487366437912, -0.5009250640869141, 0.48930829763412476, -0.14901413023471832, 0.2814541459083557, -0.30119770765304565, 1.1199289560317993, 0.09954188764095306, -0.9692103862762451, 0.39489156007766724, 0.35935619473457336, -0.6006291508674622, -1.3012075424194336, -0.06283854693174362, -0.11981060355901718, -0.5861713886260986, 0.5934723615646362, 0.20207007229328156, 0.47059547901153564, -0.10984104871749878, -0.34695589542388916, -0.27721643447875977, -0.01941862516105175, 0.332410603761673, 0.48940229415893555, 0.008983556181192398, -0.49509406089782715, -0.7074162364006042, 0.6984543800354004, -0.31637853384017944, -0.7550245523452759, 0.30946314334869385, -0.7792947292327881, -0.2993662357330322, 0.7689054608345032, -0.22773155570030212, -0.3191801607608795, -0.7622007131576538, 0.5833922028541565, -0.11981193721294403, -0.0897793099284172, 0.3851890563964844, 0.6725946068763733, -0.33829641342163086, 0.40938761830329895, 0.6500447392463684, 0.31625092029571533, -0.2604418694972992, 0.10567943751811981, -0.7520591616630554, 0.42019450664520264, 0.14377576112747192, 0.1694539487361908, -0.4165981411933899, -0.12575961649417877, -0.5403363704681396, 0.20228266716003418, -0.47246289253234863, -0.1078028604388237, -0.4335443377494812, 0.05729951336979866, -0.7996724247932434, -0.2548159658908844, -0.5038944482803345, -1.122654914855957, 0.04369174689054489, 0.01669919677078724, -0.3754902184009552, -0.16428445279598236, -0.8194757699966431, -1.0082610845565796, -0.9141618609428406, -0.6203500628471375, -1.2682167291641235, 0.3089163899421692, -0.18147850036621094, -0.4255989193916321, -0.3028426170349121, 0.1070510521531105, -0.4687877595424652, 0.681267261505127, -0.31509116291999817, 1.0672658681869507, 0.0070413206703960896, -0.15268106758594513, 0.03181497007608414, 0.27820780873298645, 0.48378199338912964, -0.3704817593097687, 1.0401039123535156, -0.2957404553890228, 0.2795529067516327, -0.033982910215854645, -0.23844076693058014, 0.013296215794980526, -0.09563075006008148, 0.8677054643630981, 0.1111021488904953, -0.47653326392173767, 0.1653308868408203, 1.6644796133041382, -0.5719791650772095, -0.13174252212047577, 0.2631247043609619, 0.7183728814125061, 0.07748117297887802, -0.24150784313678741, 0.831831693649292, -0.36263230443000793, 0.12860146164894104, 0.5018273591995239, -0.14716874063014984, 0.023173559457063675, -0.36198124289512634, 0.5938445329666138, 1.6369450092315674, 0.17929041385650635, -0.12307693064212799, -1.4470933675765991, 0.7847632765769958, -0.8443169593811035, -0.6098136305809021, -0.023763014003634453, 0.5848933458328247, 0.3193971514701843, -0.5150184035301208, -0.3967379331588745, 0.32249322533607483, 0.5568692088127136, 0.07568291574716568, -0.23408818244934082, -0.9949045181274414, 0.2690560817718506, 0.3315138518810272, 0.3465326428413391, 0.10094423592090607, -0.4080352485179901, -0.006714604329317808, 15.342426300048828, 0.7169163227081299, 0.009194654412567616, 0.7736263871192932, 0.34716296195983887, 0.2875073552131653, -0.3594345450401306, 0.08310838788747787, -0.6690242290496826, 0.09387712180614471, 1.4065879583358765, -0.3212212324142456, 0.700296938419342, 0.9901157021522522, -0.10098155587911606, -0.036362841725349426, -0.10258171707391739, 0.3460470139980316, 0.6341122388839722, -1.2533111572265625, 0.25804612040519714, 0.018641836941242218, 0.7247106432914734, 0.5507574677467346, 0.4481590986251831, 0.7771995663642883, 0.38783812522888184, -0.46840646862983704, 0.5033174753189087, -0.24834471940994263, 0.9378739595413208, -0.3596836030483246, 0.5184719562530518, 0.35632193088531494, -1.2113643884658813, -0.729194700717926, -0.8990011215209961, -1.4081770181655884, 0.14804445207118988, 0.33615535497665405, -0.7612056732177734, -0.23960906267166138, -0.07980848848819733, 0.7531133890151978, 0.013326723128557205, 0.6412778496742249, -0.27576592564582825, 0.4151805341243744, 0.20295965671539307, 0.3583677411079407, -0.2649022340774536, 0.4573211073875427, 0.058256570249795914, 0.13905055820941925, 0.4905538558959961, -0.3273884356021881, 0.3136511743068695, 0.5812251567840576, -0.5961588621139526, -0.06182094290852547, -0.7021915912628174, -0.70405513048172, -0.14798052608966827, 0.5151264667510986, 0.04145515710115433, 0.47003471851348877, -0.9251222014427185, -0.08992766588926315, 0.4283246397972107, -0.18904449045658112, -0.7845576405525208, -0.15684443712234497, 0.357058048248291, -0.16259750723838806, 0.0015480940928682685, 0.45366352796554565, -0.6413236260414124, -0.4941443204879761, -0.7466837167739868, -0.3965529203414917, 0.28955623507499695, -0.7438586354255676, -0.5452268123626709, 0.7612963318824768, -0.36615851521492004, -0.632075309753418, 0.4574585556983948, -0.9449467658996582, -0.4620435833930969, 0.20144987106323242, -0.8751899600028992, -0.3137134611606598, 0.12519298493862152, -0.6970559358596802, -0.10826461762189865, -0.3246636986732483, 1.1357214450836182, 0.11377883702516556, -0.18055035173892975, -0.329023152589798, 0.19273096323013306, -0.3532229959964752, -0.4577590823173523, -0.6903824210166931, 0.8575140237808228, 0.8886575698852539, -0.09711670875549316, 0.4343431890010834, 0.05890916660428047, -0.30576494336128235, -0.6711678504943848, -0.4136744439601898, 0.7307604551315308, -0.5850746631622314, 0.05523229017853737, -1.0526670217514038, -0.76864093542099, 0.24928835034370422, 0.31658005714416504, -0.16727672517299652, 0.5033531785011292, -0.31610220670700073, -0.6733351945877075, 0.22258618474006653, -0.9921295046806335, -0.2463502585887909, 0.725581169128418, -1.1102473735809326, -0.14802856743335724, 0.25731217861175537, 0.4634530246257782, -0.6934434175491333, -0.6205347776412964, 0.07125911861658096, -0.23853799700737, -0.7431140542030334, 0.7689322233200073, -0.1326923370361328, 1.3653632402420044, 0.5662180185317993, 0.0856713056564331, -0.6646286249160767, -0.19428908824920654, -0.7769138813018799, -0.002065100008621812, 0.057163581252098083, 0.8258582353591919, -0.18833214044570923, 0.15652158856391907, 0.6958063840866089, -0.06326957046985626, -0.28257715702056885, -0.7979023456573486, -0.054627805948257446, -0.0896785631775856, -0.5268622040748596, 0.36535412073135376, -0.18970352411270142, 0.18622688949108124, -0.6818804144859314, 0.41113707423210144, 0.5763542652130127, -0.9800010919570923, -0.38969695568084717, 0.4647862911224365, 0.0031889271922409534, -0.21595260500907898, -0.42390406131744385, -0.482264906167984, -0.6260251998901367, 0.09994646161794662, -1.1307159662246704, 0.3602002263069153, -0.3029601573944092, -0.1023898720741272, 0.4748598635196686, -0.20013754069805145, 0.09227490425109863, 0.5319927334785461, -0.2686257064342499, -0.6823062300682068, -0.5742449760437012, -0.5461856126785278, 0.8395517468452454, 0.1655016541481018, -0.9951067566871643, 0.18715697526931763, -0.3159852623939514, 0.38486406207084656, 0.403178870677948, 0.40157005190849304, -0.8153229355812073, -0.6783744692802429, -1.2090120315551758, 0.5420433878898621, -0.5839716792106628, -0.05752329155802727, -0.926364541053772, 0.4906420111656189, 0.12306755036115646, -0.6066546440124512, 0.7138476371765137, -0.3034008741378784, -0.8583226799964905, 0.0488898791372776, 0.7923198938369751, -0.5677682757377625, 0.5388430953025818, 0.4162496328353882, -0.6795839071273804, -0.5210737586021423, 0.2776591181755066, -0.002438741037622094, -1.2730190753936768, -0.507681131362915, 0.1598784327507019, -0.7727997899055481, -0.008469914086163044, 0.21581675112247467, 0.1114649847149849, -0.9632336497306824, 0.19151782989501953, 0.3199312686920166, 0.13373607397079468, 0.27913177013397217, 1.136163353919983, 0.19452504813671112, -0.8674973845481873, 0.2662559747695923, 0.4566599130630493, 0.10524701327085495, -0.2559977173805237, 0.2728114128112793, 0.4537386894226074, -0.8925835490226746, 0.3869835138320923, 0.28719910979270935, 0.2106003314256668, -0.7720442414283752, 0.3851855993270874, 0.5088544487953186, -0.3978163003921509, 0.2757743000984192, 1.1616730690002441, -0.34788641333580017, -1.055497646331787, -0.36958831548690796, -1.0516612529754639, -0.04951895773410797, -0.7907666563987732, 0.5687116384506226, 0.6023184061050415, 0.10482820123434067, -0.24272532761096954, -0.47122105956077576, 0.05617441609501839, 0.5297529101371765, 0.13749246299266815, 0.8039923906326294, -0.1951945722103119, -0.893121063709259, 0.6539494395256042, 0.2662368416786194, -0.21443413197994232, -0.27528131008148193, -0.09911714494228363, -0.16791662573814392, -0.5098180174827576, 0.01872517541050911, -0.3020378649234772, -0.705988883972168, 0.6124221682548523, 0.35324451327323914, 0.39257776737213135, 0.2015000730752945, -0.4259154498577118, -0.28321000933647156, 0.24558213353157043, 0.3496362566947937, -0.5851133465766907, -0.49671676754951477, 0.9737586975097656, 1.0966861248016357, -0.7912926077842712, 0.7468909025192261, -0.5230177640914917, -0.5754777789115906, 1.1764930486679077, 0.6504043340682983, -0.020686138421297073, 0.6077480912208557, 0.11848808825016022, -0.25040024518966675, 0.0990566685795784, -0.9541123509407043, 0.053703274577856064, 0.2374814748764038, 0.8632840514183044, 1.058521032333374, -0.012147968634963036, -0.14163722097873688, 0.825078547000885, 0.4881132245063782, 0.3223675787448883, 1.0836825370788574, 0.876743495464325, -0.22257696092128754, 0.060047607868909836, -0.242560014128685, 0.7661416530609131, -0.6002374291419983, -0.6802647113800049, 0.047443825751543045, 0.5464306473731995, 0.7155001759529114, 0.7018818855285645, 0.4408396780490875, -0.09180372953414917, 0.22973638772964478, 0.5497808456420898, -0.08492940664291382, -0.9760438799858093, -0.4381284713745117, -0.20560038089752197, -0.5662645101547241, -0.06191587075591087, -0.1745750904083252, -0.18235903978347778, -0.33918824791908264, -0.052857182919979095, -0.010210654698312283, 0.3436051607131958, 0.46861547231674194, 0.6214360594749451, 1.0050512552261353, 0.5862076282501221, -0.31678712368011475, -0.4072244167327881, -0.11319710314273834, -0.660304069519043, -0.09622209519147873, -0.7448873519897461, -0.5135130882263184, -0.47592785954475403, -0.3861808180809021, -0.059575002640485764]}, "authors": [{"authorId": "2211603201", "name": "M. Saad"}, {"authorId": "2279620921", "name": "Jos'e Antonio Hern'andez L'opez"}, {"authorId": "2237865625", "name": "Boqi Chen"}, {"authorId": "2279547379", "name": "D'aniel Varr'o"}, {"authorId": "2279544583", "name": "Tushar Sharma"}], "references": [{"paperId": "18e7ab056c16928d8f9539509a4b366889106d97", "title": "StarCoder 2 and The Stack v2: The Next Generation"}, {"paperId": "77b0e94d7820c5db65814fc65e524ba8f87d543e", "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models"}, {"paperId": "445c71912a718b6add8b79d01ee02aaadb51a5cb", "title": "A survey on machine learning techniques applied to source code"}, {"paperId": "9eb476cd15becf02163d6f3dab75d207eed52214", "title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?"}, {"paperId": "0fa5f7a6769eed965bbfd6610b2068fedaff74e7", "title": "Naturalness of Attention: Revisiting Attention in Code Language Models"}, {"paperId": "3ba0b9250d5e91c01eb7cca8269afec23f24c515", "title": "Energy and Carbon Considerations of Fine-Tuning BERT"}, {"paperId": "945db0077b6d19b720f5998b3f61300013c4f885", "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"}, {"paperId": "000f964393dafe113a8e66734d63b2a145844159", "title": "Large Language Models for Software Engineering: A Systematic Literature Review"}, {"paperId": "338d8f3b199abcebc85f34016b0162ab3a9d5310", "title": "A Survey on Model Compression for Large Language Models"}, {"paperId": "7bebb8743f1f0eddfa41dd0179a2e65d3dfb6352", "title": "TABASCO: A transformer based contextualization toolkit"}, {"paperId": "3b7ef6f9f27e33e6a4e3bfac90dcb01ab09718bc", "title": "SqueezeLLM: Dense-and-Sparse Quantization"}, {"paperId": "51db4c39dc0bdf5c95c8bbe89bf4211b48d0b4df", "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"}, {"paperId": "be3ce39e7840db0ab4dcae8742dc51a701398340", "title": "Too Few Bug Reports? Exploring Data Augmentation for Improved Changeset-based Bug Localization"}, {"paperId": "32ac52069e562d4f900afee70bdca63f53461481", "title": "QLoRA: Efficient Finetuning of Quantized LLMs"}, {"paperId": "9ada8fa11b1cdece31f253acae50b62df8d5f823", "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation"}, {"paperId": "886e0962479ec6dac563666399ca4c96a468fcaa", "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages"}, {"paperId": "c61d54644e9aedcfc756e5d6fe4cc8b78c87755d", "title": "A Survey of Large Language Models"}, {"paperId": "f6beeaf7ca72e4ef32a8a8110d4c7e30b18b0231", "title": "Invalidator: Automated Patch Correctness Assessment Via Semantic and Syntactic Reasoning"}, {"paperId": "2aa32aa6dd4ad3798941c98762167999e5511eac", "title": "CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection"}, {"paperId": "8a4fc5f00cd4aca61e148e46a2125c3a406719f1", "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"}, {"paperId": "c95f4d9606e3413694ad2252668e38be494c0cf2", "title": "Aligning Offline Metrics and Human Judgments of Value for Code Generation Models"}, {"paperId": "eed359a8a3ffdc45253e698b352443f0271a9666", "title": "Compressing Pre-trained Models of Code into 3 MB"}, {"paperId": "e9fc39f56abbc6b8aed1e05496d985e70345a95a", "title": "An extensive study on pre-trained models for program understanding and generation"}, {"paperId": "f0631f7928b99ee51f8164acd04889219b2bcdbb", "title": "Diet code is healthy: simplifying programs for pre-trained models of code"}, {"paperId": "e03609f2587f690867e7ea0bedaf0db25282c548", "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers"}, {"paperId": "45bd8878895bc29c7248e20fb75c763be265188b", "title": "Low Level Source Code Vulnerability Detection Using Advanced BERT Language Model"}, {"paperId": "f8a90067f2e3359c61c87e2a28738cfd01bbd6a5", "title": "Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-solution Study"}, {"paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da", "title": "InCoder: A Generative Model for Code Infilling and Synthesis"}, {"paperId": "9a6730534295335247eebdec59b7decdeb83d59a", "title": "On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages"}, {"paperId": "1cd7165a7393ba1e4234513a19cfea4a90ffcea4", "title": "The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches"}, {"paperId": "4b27f18bff43d605805c92696a979714ced0b805", "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation"}, {"paperId": "2c3111cbd1327e9b8616082eef39dc0a3a516b2a", "title": "Natural Attack for Pre-trained Models of Code"}, {"paperId": "7006c481e07cf0c71d93d82f8cfbafc5ffac8913", "title": "Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests"}, {"paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d", "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"}, {"paperId": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896", "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"}, {"paperId": "c156b1b30e3dd9284615e5304f2fb2826c09d0ff", "title": "Learned Token Pruning for Transformers"}, {"paperId": "8a0a7170977cf5c94d9079b351562077b78df87a", "title": "A White Paper on Neural Network Quantization"}, {"paperId": "0646bb09db4d1ba24150e69b71edcd4aff691b3c", "title": "Unified Pre-training for Program Understanding and Generation"}, {"paperId": "69a72ff5b30642d11c96635e99aadad3140d33a7", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"}, {"paperId": "949c0941d4c57482318afa28f2c8eb82569fb401", "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey"}, {"paperId": "4083958684292f6fa2f5c7fd4f9be975e80145b6", "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow"}, {"paperId": "1728cb805a9573b59330890ba9723e73d6c3c974", "title": "Knowledge Distillation: A Survey"}, {"paperId": "9a21740d87976bf76f4a9668a9da631035302fb2", "title": "Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms"}, {"paperId": "738215a396f6eee1709c6b521a6199769f0ce674", "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"}, {"paperId": "0fe2636446cd686830da3d971b31a004d6094b3c", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"}, {"paperId": "92515b7ed018194e340f9edefeb52d9b19f679ef", "title": "Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree"}, {"paperId": "b3ea2d9c8e5ea3b87ace121f0bece71565abc187", "title": "Quantifying the Carbon Emissions of Machine Learning"}, {"paperId": "ce106590145e89ea4b621c99665862967ccf5dac", "title": "Q8BERT: Quantized 8Bit BERT"}, {"paperId": "fbe25e4f069a19dc63daca27b7c98cff338663b9", "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"}, {"paperId": "00549af4bc3270e0f688acbf694f912d7ee39cad", "title": "Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"}, {"paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, {"paperId": "95a251513853c6032bdecebd4b74e15795662986", "title": "What Does BERT Look at? An Analysis of BERT\u2019s Attention"}, {"paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need"}, {"paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d", "title": "Gaussian Error Linear Units (GELUs)"}, {"paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19", "title": "Distilling the Knowledge in a Neural Network"}, {"paperId": "9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c", "title": "Towards a Big Data Curated Benchmark of Inter-project Code Clones"}, {"paperId": "77bdb4c1117d72ad863845f35ba50929cb623776", "title": "On the naturalness of software"}, {"paperId": "01530a6d54ed5c881c57c761ffdbb979751753e4", "title": "Boosting Automated Patch Correctness Prediction via Pre-trained Language Model"}, {"paperId": "df41c5e9c62371a2091f5e209e30cbb50c4acf3b", "title": "Automated Program Repair Based on Code Review: How do Pre-trained Transformer Models Perform?"}, {"paperId": "cbde5598c1a78285adfcfd77fb3636f5498987a0", "title": "EBERT: Efficient BERT Inference with Dynamic Structured Pruning"}, {"paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}, {"paperId": "3b5b72dc39b7365357cc25597fcaeea9a692d5c6", "title": "Floating Point Operations in Matrix-Vector Calculus"}, {"paperId": null, "title": ". What is the impact of the pruning technique on the performance of language models for code on various SE tasks? Naturally, pruning tokens would result in some information loss"}, {"paperId": null, "title": "\u201cAlpine replication package"}]}