acronym,title,id
transformer,Attention is All you Need,204e3073870fae3d05bcbc2f6a8e263d9b72e776
gpt,Improving Language Understanding by Generative Pre-Training,cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
gpt2,Language Models are Unsupervised Multitask Learners,9405cc0d6169988371b2755e573cc28650d14dfe
gpt3,Language Models are Few-Shot Learners,90abbc2cf38462b954ae1b772fac9532e2ccd8b0
bert,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,df2b0e26d0599ce3e70df8a9da02e51594e0e992
ttt,Learning to (Learn at Test Time): RNNs with Expressive Hidden States,35ab93f41115e860bee5e202b71061addfd0fd5d
xlstm,xLSTM: Extended Long Short-Term Memory,98372f2e164a4ae44c390a72a39bd6d7675cae89
griffin,Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models,d53fe76bd2795a19ddf52d012917782f6f6f2c1e
hyena,Hyena Hierarchy: Towards Larger Convolutional Language Models,998ac3e945857cf2676ee7efdbaf443a0c6f820a
m2,Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture,c85268696fe1435605ae66a18653cfdcf8153753
spikegpt,SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks,9f52317ea9c5a6804b978987ff2a6557f98b5b2c
mamba2,Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality,ca9f5b3bf0f54ad97513e6175b30497873670fed
s4,Efficiently Modeling Long Sequences with Structured State Spaces,ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51
hippo,HiPPO: Recurrent Memory with Optimal Polynomial Projections,0964490205fdc38c2f0980c9d778069089ca92e3
lssl,"Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers",ca9047c78d48b606c4e4f0c456b1dda550de28b2
httyh,How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections,a30ac45ac5b7bd2148d3fb80ee7f3c29724e3170
s4d,On the Parameterization and Initialization of Diagonal State Space Models,ca444821352a4bd91884413d8070446e2960715a
mamba,Mamba: Linear-Time Sequence Modeling with Selective State Spaces,7bbc7595196a0606a07506c4fb1473e5e87f6082
samba,Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling,28eb18717cfa257f0fc49fb9512c48279cafa031
retnet,Retentive network: a successor to transformer for large language models,240103933ffe3dac2179cc160a2bd91299357a53
gla,Gated Linear Attention Transformers with Hardware-Efficient Training,62b18cc55dcc7ffe52c28e1086aee893b7bc4334
rebased,Linear Transformers with Learnable Kernel Functions are Better In-Context Models,4300adeecf60f0ed1c0ccb9c4b62c0904227bca4
deltanet,Linear Transformers Are Secretly Fast Weight Programmers,1a703f08da01cf737cce3fb9064259b3f4b44e9c
hedgehog,The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry,af8a43c72e833add4cb3cff5e5427e1dbb0bacc1
polysketchformer,Fast Transformers via Sketching Polynomial Kernels,d9a2f89dc65b2cb7cd03cf2d37e2f67c6f72359c
transnormerllm,A Faster and Better Large Language Model with Improved TransNormer,2a38daf98d506477f8180806f503409d5036eaf4
rwkv4,Reinventing RNNs for the Transformer Era,026b3396a63ed5772329708b7580d633bb86bec9
gateloop,Fully Data-Controlled Linear Recurrence for Sequence Modeling,d7f64f2bdd80ea15f21ef7d867e102ac9ecdc797
abc,Attention with Bounded-memory Control,e0cbbca02b332f398c6639b3bea0613f79166220
vqtransformer,Linear-Time Transformers via Vector Quantization,9c464f92cb3ab18a7c09f5bcee8e6e80bdec3b3b
hgrn,Hierarchically Gated Recurrent Neural Network for Sequence Modeling,434d751d355d7a7c20efa570e785c76286245e77
hgrn2,HGRN2: Gated Linear RNNs with State Expansion,46732358e98ce6be0c564ae11f71d556a64b4c35
rwkv6,Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence,157ed5647da39a7f5d33a84a90414b2a9e97e301
mega,Mega: Moving Average Equipped Gated Attention,70e91e16eb321067d9402710e14a40cf28311f73
dcmha,Improving Transformers with Dynamically Composable Multi-Head Attention,f4a25d45bb381b3f6ab08e84c9a65bff90e3a104
lllm,When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models,b72a8884a7a2f93d60d44930dd77d0af85dd32b8
ring,Ring Attention with Blockwise Transformers for Near-Infinite Context,02ad9f3fefe33cb9ca546591bec65dbdf7766c80
bpt,Blockwise Parallel Transformer for Large Context Models,86b6e42e2ce957f6497d4aa578c9bb4d2b4e4ba3
eva,Efficient Attention via Control Variates,ac608a4a6b19b3208e560eee5daadb3cc18638a2
dijiang,DiJiang: Efficient Large Language Models through Compact Kernelization,ef8846c0b9eb9e915d44e18bf06fda51f9b5e2fa
tnn,Toeplitz Neural Network for Sequence Modeling,f35f5aedc30e2c5ded210d9c91ba6e84bd029425
cope,Contextual Position Encoding: Learning to Count What's Important,d7ee15521fcfd8704c8422997614b2b22f5e1148
infiniti,Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention,3fd5bc3077d04965eaa3498372c39bbdd09d55e4
peer,Mixture of A Million Experts,ebe6b817ba8a44d0481598575904231c8324932e
colt5,CoLT5: Faster Long-Range Transformers with Conditional Computation,27d391d65ab42c30dc35595213ba6585633afa5d
locater,Self-attention Networks Localize When QK-eigenspectrum Concentrates,b6dd8671099ed834850290530a6dc347303adfbf
prefix,Just read twice: closing the recall gap for recurrent language models,0671179a1bc13419d725b15af49934a99e10f4b8
lasp,Linear Attention Sequence Parallelism,660d80773f55c5dc889de3ce93b71672133a91bc
synthesizer,Synthesizer: Rethinking Self-Attention in Transformer Models,a238109c3969ae681eee0d4f1bf2012f28850593
cosformer,cosFormer: Rethinking Softmax in Attention,c49ac1f916d6d2edeb187e6619c8d23acd95eb21
lara,Linear complexity randomized self-attention mechanism,1944cebf4e41a10ea7bd02ce30404c18c9c4e04f
luna,Luna: Linear unified nested attention,af679d69fcc1d0fcf0f039aba937853bcb50a8de
coneheads,Coneheads: Hierarchy Aware Attention,ff9451b34b1959ded1d870131d0d70095bd69686
loma,LoMA: Lossless Compressed Memory Attention,abd848eb0a37a1625165aa087afe6c148f4f1230
mobilellm,MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,f7310dac21abc6ba357bcd5e75fb2e6957a97303
gqa,GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints,5ae6fb6b5a3c7df515ff4a82ac9673bae6a8e200
mqa,Fast Transformer Decoding: One Write-Head is All You Need,dc52b09089704ebd6f471177474bc29741c50023
nystromformer,Nyströmformer: A nyström-based algorithm for approximating self-attention,6fa1cfc4f97f03a8485692418c7aa1a06c574a85
funneltransformer,Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing,4ca3b0ea12f02e2dea01a4aa505956bae5500a09
settransformer,Set Transformer,e9adc3f0c2f1fb7035fa23c01ae54fcd3ab759c9
loki,Loki: Low-Rank Keys for Efficient Sparse Attention,f4c07dc79976a4e3a558bb6fcd0d615673e8ecef
sumformer,Sumformer: Universal Approximation for Efficient Transformers,f2f68ed280d27bd25d61782224f8a465db8f43bd
flurka,FLuRKA: Fast and accurate unified Low-Rank&Kernel Attention,2fd0aa038cf1009e265e9cbddab8ea6a8e03016a
scatterbrain,Scatterbrain: Unifying Sparse and Low-rank Attention Approximation,5f895e84c1fea75de07b4f90da518273c2e57291
performer,Rethinking Attention with Performers,3fbf6339273c50b04e886fa9bd4ad18c952a683d
rfa,Random Feature Attention,9ed25f101f19ea735ca300848948ed64064b97ca
linformer,Linformer: Self-Attention with Linear Complexity,c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87
lrt,Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer,49e5b09480189fc9b2316a54f9d1e55cf0097c8b
lineartransformer,Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention,6f68e1bb253925d8431588555d3010419f322e04
based,Simple linear attention language models balance the recall-throughput tradeoff,cde66097f4123a62bf3e28d48c764648e8c69f72
lightningattn2,Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models,7294c426b8a95975ca932eaf8f700acdd3d950b2
sfa,Faster Causal Attention Over Large Sequences Through Sparse Flash Attention,d203c764fb5dec2b053be667c8b06e516ea6ef10
poolingformer,Poolingformer: Long Document Modeling with Pooling Attention,e32a12b14e212506115cc6804667b3d8297917e1
bigbird,Big Bird: Transformers for Longer Sequences,044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3
longformer,Longformer: The Long-Document Transformer,925ad2897d1b5decbea320d07e99afa9110e09b2
blockbert,Blockwise Self-Attention for Long Document Understanding,2cf3bd0cc1382f35384e259d99e4f9744eeaed28
sparsetransformer,Generating Long Sequences with Sparse Transformers,21da617a0f79aabf94272107184606cefe90ab75
moa,MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression,952fb6413499bc377faa51bf71e4d558ac6f6387
hyperattention,HyperAttention: Long-context Attention in Near-Linear Time,93e58491830abe1eb965ab37ec64fa97263f6048
clusterformer,ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer,1f53e69f94a1020b48ea7d282221c576555c38a3
reformer,Reformer: The Efficient Transformer,055fd6a9f7293269f1b22c1470e63bd02d8d9500
sinkhorn,Sparse Sinkhorn Attention,34a4e6818d680875ff0bef9a76de0376118446d1
clusteredattn,Fast Transformers with Clustered Attention,cd4ffe5e014601a3d6b64121355d29a730591490
routingtransformer,Efficient Content-Based Sparse Attention with Routing Transformers,657329c633709dd1ac34a30d57341b186b1a47c2
bipe,Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation,9d932de1d2f51067b6481745f28a2db345293d48
infinibench,∞-Bench: Extending Long Context Evaluation Beyond 100K Tokens,f05e84702562cb693dd68d3d1c88072519a7bd71
resonance,Resonance RoPE: Improving Context Length Generalization of Large Language Models,f016f079ee63a0487756f895c1d93ff0110d3ecd
longrope,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,c9603ec967879c24973b5bd48861df2e5555932e
e2,E^2-LLM:Efficient and Extreme Length Extension of Large Language Models,33230a9d2e4d0ef9c923813a07107b2e3bc56605
scalelawrope,Scaling Laws of RoPE-based Extrapolation,539fadfb615ef84c240f4741061c44eeda540091
lex,A Length-Extrapolatable Transformer,9575afb5702bc33d7df14c48feeee5901ea00369
pi,Extending Context Window of Large Language Models via Positional Interpolation,f5afaccfe90268485a9961c5771ec5e71e9b806c
ntk,"NTK Interpolation, Blog Post ",
yarn,YaRN: Efficient Context Window Extension of Large Language Models,819bbdc2dac9e13d9ca3e2508a6e063186ce5e40
clex,CLEX: Continuous Length Extrapolation for Large Language Models,a54761081c2b001c057fb6e1ea9a48058d5aa5e0
pose,PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training,73290ecbec2f38d1d647ddef1ada69cee41725b3
fire,Functional Interpolation for Relative Positions Improves Long Context Transformers,dc48bc1a4d81e0f37603013fd2a95644dc233bd0
alibi,"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",9ca329408813d209b1dcb36936f7f9cba82506bd
elgllm,Exploring Length Generalization in Large Language Models,f843233f76a5dff07bfa93a71a1cf13d8aa6a94a
rmt,Recurrent Memory Transformer,a8cf0f7a20f886acfb332071c2daaf58ba86a5ca
brt,Block-Recurrent Transformers,736eb449526fe7128917954ec5532b59e318ec78
infiniteformer,∞-former: Infinite Memory Transformer,74adfb26adfd9d7aac3818a9ebb1371e174ed207
memformer,Memformer: A Memory-Augmented Transformer for Sequence Modeling,67ee20536c30a225b86902af2f091e28e5e19b40
transformerxl,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,c4744a7c2bb298e4a52289a1e085c71cc3d37bc6
xl3m,XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference,884ca2feb4ffb14390e98daf95d1efbdca7a9b11
transformerfam,TransformerFAM: Feedback attention is working memory,89d786457591d39091cf6ef4831f2bbd72698caf
nbce,Naive Bayes-based Context Extension for Large Language Models,c79fe572b44b0ad904bd30bdfd78d5d3c591e342
neuralcompress,Training LLMs over Neurally Compressed Text,e27c9c39ecb67e8e7708d8d53bec2f891cfa7a40
lminfinite,LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models,a7fc585cc4c2b6822646b2c410e0c427a20798f2
dca,Training-Free Long-Context Scaling of Large Language Models,cf7ab5df804575bad88a9fcf0fbf7707bf500944
cepe,Long-Context Language Modeling with Parallel Context Encoding,2330035c7586a0dc0b1f09e9c00106b295acf543
flagembedding,Soaring from 4K to 400K: Extending LLM’s Context with Activation Beacon,2b8439f319dfa73df62ca8957ff6d0c1f3c7a73c
selfextend,LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning,a9468d8bfa6bd016dfd3128c4e8408e30eb8549b
semanticcompress,Extending Context Window of Large Language Models via Semantic Compression,a3dd3adc37cdb8991936f3feb109c20b6f892f3d
streamingllm,Efficient Streaming Language Models with Attention Sinks,fdc53c2c10742464087c0525f77e32604827a21d
pcw,Parallel Context Windows for Large Language Models,980e55d9226cac302d0fae7732da4e67b8bc952c
longnet,"LongNet: Scaling Transformers to 1,000,000,000 Tokens",c12db2c60e8989f646a29ad4f4d24475e860ad91
stm,Efficient Long-Text Understanding with Short-Text Models,732e3faec4e5be4d144256f2c379b9dc49f0b227
densemamba,DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models,7351898febca53d01453283c9b1a541b662e1ed3
mambabyte,MambaByte: Token-free Selective State Space Model,a6e2dca754f3dc625a9da5f10f9b7a57079bfd27
seqboat,Sparse Modular Activation for Efficient Sequence Modeling,d2d0371158803df93a249c9f7237ffd79b875816
gssm,Long Range Language Modeling via Gated State Spaces,eaef083b9d661f42cc0d89d9d8156218f33a91d9
bst,Block-State Transformers,0a067fab18c67d4a386efa846c080f8afff5e8f3
dssm,Diagonal State Spaces are as Effective as Structured State Spaces,71e15a9a52dcafca57bff5f310b95e2c7d0cfc87
h3,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,5a77b508302771fc083bf24e0bcda8553c9b5421
matmulfree,Scalable MatMul-free Language Modeling,401c4147375b016d4758cf2dd859232a8271fdcd
megalodon,MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length,e9484cf633985ccef70b0ce46866ce232a81ca4b
pangupi,PanGu-π: Enhancing Language Model Architectures via Nonlinearity Compensation,7d011d6a9e1704acc29bab88d616089089ea1006
compressivetransformer,Compressive Transformers for Long-Range Sequence Modelling,f51497f463566581874c941353dd9d80069c5b77
productkeymem,Large Memory Layers with Product Keys,bf442ab269074665a68e4dbbe19e4efc97862541
flashattn,FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,87c5b281fa43e6f27191b20a8dd694eda1126336
metaformer,MetaFormer Is Actually What You Need for Vision,57150ca7d793d6f784cf82da1c349edf7beb6bc2
vntransformer,VN-Transformer: Rotation-Equivariant Attention for Vector Neurons,9628221b1fa271d93968e97a0a5868b657e0a5be
compositionalattn,Compositional Attention: Disentangling Search and Retrieval,b8b813111c411ae61881ab9cd25707d9de6444ec
flash,Transformer Quality in Linear Time,dc0102a51a9d33e104a4a3808a18cf17f057228c
mea,Self-attention Does Not Need $O(n^2)$ Memory,53c3940f35b8b45d55ed49056282e1961954513d
rela,Sparse Attention with Linear Units,a7721b6523971394a8bd4bfda139122ef59b22cd
ngrammer,N-grammer: Augmenting Transformers with latent n-grams,6ac1fccf1e04487d439ee598f51c03ddac5144ca
remixer,REMIXERS: A Mixer-Transformer Architecture with Compositional Operators for Natural Language Understanding,f37e00c1a3919d1a2c2b550c9acc1612f9319f35
mogrifier,Mogrifier LSTM,3c615eb1bfbd7c25202fa3e1b851973608ae2a73
htransformer1d,H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences,dbf53ece1a6a8860e41ff5f721c72ceb0fb18dd6
lstransformer,Long-Short Transformer: Efficient Transformers for Language and Vision,1a883522f3c0051d70be1f8cbdb8989a77395006
roformer,RoFormer: Enhanced Transformer with Rotary Position Embedding,66c10bf1f11bc1b2d92204d8f8391d087f6de1c4
gmlp,Pay Attention to MLPs,e3a3e85c5a32af29e13b3561f6cf070de70651de
feedbackmem,Addressing Some Limitations of Transformers with Feedback Memory,
kroneckerattn,Kronecker Attention Networks,57ae9f6566f7a9ea849ac63a5d8316855d9c69c8
memcompress,Generating Wikipedia by Summarizing Long Sequences,8691706ad0cf5e83969658b2e6bfffdc379440c9
normalizedattn,Normalized Attention Without Probability Cage,71a72da632d55b4a9e12ca0b9e35bafe0466e318
aft,An Attention Free Transformer,d5e999aae76d5270ef272076979c809817458212
srt,Self Reasoning Tokens,
cvt,Building Blocks for a Complex-Valued Transformer Architecture,d6e09d3274df2c717e47fa15575f493422309a10
kangpt,KAN-GPT,
hierarchitrans,Hierarchical Transformers Are More Efficient Language Models,231e768f0cd280faa0f725bb353262cb4fed08d1
effibeam,Efficient Beam Tree Recursion,9e3e56957e249cdebdd8673fd1174980ed694560
chordmixer,ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths,2fc384087f55111a196c96d88029aab324aa543e
fastr2d2,Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation,dc85180153687539724d20a5927b2fbdf5f8e2a4
templatent,Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning,d6a0dfd5f39222d8924b7727a0a49f81fa247d71
bptrans,BP-Transformer: Modelling Long-Range Context via Binary Partitioning,2e14e84ccec924ed770b58108ad1d9de6f0ca295
lkconv,Time-aware Large Kernel Convolutions,af34ea4242ca8725ea739ec1bef674bec10c1fa9
s5,Simplified State Space Layers for Sequence Modeling,6d7d141c75af752ffc0d8a6184cca3f9323d6c74
staircaseattn,Staircase Attention for Recurrent Processing of Sequences,b50815251c948f00baedccaf5f56c281ffa7650f
etc,ETC: Encoding Long and Structured Inputs in Transformers,d27669c82faf78ea08cceaa0a171b540cccc304d
scattn,Sparse and continuous attention mechanisms,09e69bf0926e55cd277a3ef5b1450ba083719cb9
longt5,LongT5: Efficient text-to-text transformer for long sequences,3dfb1f50f2a34a699c339dabaa6f9b3a977973de
unlimiformer,Unlimiformer: Long-Range Transformers with Unlimited Length Input,dbc368bc8b49347dd27679894524fa62f88492c9
landmarkattn,Landmark Attention: Random-Access Infinite Context Length for Transformers,60b35c6d68acced19b0c66edcfc0ee0a2c11efed
compresscontext,Adapting Language Models to Compress Contexts,2f7364d8e5cf94315bf8905f57de9c5543e9a4bf
megabyte,MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,412e266cddfd87c79087a88ba1e4d11b89a45a13
dcpruning,Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers,c193eb176985a81ae64f63c5e50b2f11cfb7c4e6
selfretrieval,Long-range Language Modeling with Self-retrieval,cbbc2cc774c50b0b19922185b80e9ce90b7cd2f6
maxmargin,Max-Margin Token Selection in Attention Mechanism,a87f40a49da377c0d00bebe711e417fc3b1d8969
castrans,"Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers",658869b820bfaa919d10378d0a019bff7e56db6c
sparsetokentrans,Sparse Token Transformer with Attention Back Tracking,8b16dc5b4c0728147eef1647a6ab7f786333b76c
lbcontextcompress,Empower Your Model with Longer and Better Context Comprehension,58d1a002a0ff0aa40b6633f0a7073d48f1cdff53
longheads,LongHeads: Multi-Head Attention is Secretly a Long Context Processor,f6440a16ccc5c13d2a86af91b76e078685abfd16
zebra,Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention,36697944858ab17ca23b23ae2043aa6c0b2e3d5d
corm,CORM: Cache Optimization with Recent Message for Large Language Model Inference,39835c99ded942f08dd6e8355f1a8681a58b0be6
sinklora,SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models,7777c897ea1abd046dd30a5d13504ef0afa02fca
hipattn,HiP Attention: Sparse Sub-Quadratic Attention with Hierarchical Attention Pruning,45294f7227d0228f3d26b2796824f46524ad7a76
sentineltokens,Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens,0a52a97ec49da2560d96d431f15831bc7e1389e1
sparseless,Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers,ee2b3f7703b553b487428862b83995ea3e8c0c3a
assparseattn,Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention,5b2c04e082a56c0eb70ed62bc36148919f665e1c
neurocache,Neurocache: Efficient Vector Retrieval for Long-range Language Modeling,0ea22780c42536f488c097431c037aa45aac0b2d
wgqa,Weighted Grouped Query Attention in Transformers,61203ec44f69a1bb6838f772940ea468b2b6f96e
butterfly,Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations,a6e92f6fa9e91b7e869562a63b30a9a56cf14582
fnet,Fnet: Mixing tokens with fourier transforms,1f133158a8973fb33fea188f20517cd7e69bfe7f
latentattn,Latent Attention for Linear Time Transformers,2adc906e5d96f79316a53333595eaeb9dc6b3ac3
softmaxattn,Softmax Attention with Constant Cost per Token,723aa15a72f08a4f8264f6301bc932bf40e723f3
lightningattn,"Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention",f19f3869dabb6e4019b91b65b82a442373cd40aa
secretlinseqmod,Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective,dc0005e3fb9ad04e43dc60b12df8e27f29dd04c3
rnnattn,Attention as an RNN,caeb1e2d285c3e68ea88fd31ab4a3770aa55649b
lightnet,You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet,7f505660c595d6547e258de0a5978fc2d5fad925
neurallegal,Neural Legal Judgment Prediction in English,aca16f64ddbf187f8944118c8f72777c3d682521
hierarchinn,Hierarchical Neural Network Approaches for Long Document Classification,320584ca7e9fce221ad3ca480efc8c66d31ef80e
hitrans,Hi-transformer: Hierarchical interactive transformer for efficient and effective long document modeling,84daddd294fa3cc12596b5785f81c2a153d2fb1d
erniesparse,Erniesparse: Learning hierarchical efficient transformer through regularized self-attention,94e46e18d2628343a926acf6c3d0817e11d35d58
memorizingtrans,Memorizing Transformers,0e802c0739771acf70e60d59c2df51cd7e8c50c0
recurattnnet,Recurrent Attention Networks for Long-text Modeling,b1fc02780a21bc0f61d8d8e70a8e8f6987c06bb0
segrecurtrans,Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model,b7a4a942502a7b34baa498cc34fe6c138779d7a8
trams,TRAMS: Training-free Memory Selection for Long-range Language Modeling,482ffdfc1e3594f5a22f51e3b9ee31e1c74f85de
extensibleembed,Extensible Embedding: A Flexible Multipler For LLM's Context Length,c8a092916d325b398e2962ae1dd4d56664fd6c08
linllms,Linearizing Large Language Models,862479f7bd78a69c52a0691766848caa8eb4660f
assorecurmemtrans,Associative Recurrent Memory Transformer,1085ddc5028be0a6f517bde6c44029abc208c63f
locost,LOCOST: State-Space Models for Long Document Abstractive Summarization,99621f3ce8caf5d99f2b350d53ec8e6c57695bc2
ssmfundmodel,State Space Models as Foundation Models: A Control Theoretic Overview,2a53d07a399c47151a7b440dacfb3673b9c4e753
approxdiag,Robustifying State-space Models for Long Sequences via Approximate Diagonalization,db916693bc6f0b858b290b5963c8f012f8e8bccd
zamba,Zamba: A Compact 7B SSM Hybrid Model,3dd6bc488283f1e4cc967d98a6a6c3d7f1a6cf76
mambalms,An Empirical Study of Mamba-based Language Models,d668f557b922d0cac746c8ec82f7975f06ac906b
bmojo,B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory,835dcf0ba3402d6a8a8f237ad2020ff3055c9b33
mamba4gcn,MambaForGCN: Enhancing Long-Range Dependency with State Space Model and Kolmogorov-Arnold Networks for Aspect-Based Sentiment Analysis,61223f041fda417c6d3e053c8151adf79b5b33e3
discdifflm,Discrete Diffusion Language Model for Long Text Summarization,e8d49f58030a861810eec227903f39031e7622e6
kerple,KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,d6c5aab433d9871cabc01ffb1e5e1ea89141155b
receptivefieldana,Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis,5735e49e501c8e51e9be4079592e46e047747b03
transextra,Exploring Transformer Extrapolation,2f0203386f3dcbffb47c9f7fe2d19d373d9dda2f
attnalignnfpe,Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation,889617f6dc6fe2de4f0fb004c3c827db1b4c771a
infinigram,Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens,1c143a753d4eb82da5526132149f6e4ed5271b63
lengenewope,Length Generalization of Causal Transformers without Position Encoding,37b1ce339678f63315c82841c6824dd739269636
cape,CAPE: Context-Adaptive Positional Encoding for Length Extrapolation,24313a18e1ba789fc72ef22489b9b9a630244aed
posicoupling,Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers,24fb3ae6352e4dad2a59b889ec4ac84c3045c3ec
explicitencoding,Explicitly Encoding Structural Symmetry is Key to Length Generalization in Arithmetic Tasks,58557a9654b9f1770667cb71219379f65f32ded9
3drpe,3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position Encoding,7d755929690577dbd4b8c187238ed44c3e3a247a
humanepisodic,Human-like Episodic Memory for Infinite Context LLMs,34c856ce5a539d46a996845b194be7ce42df82a2
granite,Scaling Granite Code Models to 128K Context,d772ff2f1c490a2d13d909df22b98c948d32a3dd
grootvl,GrootVL: Tree Topology is All You Need in State Space Model,8bcff5a73c2191084564e7d50ee954358ce12de4
glrsm,A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models ,4f02df5e50a50d593a3336bf9a566c30e6fef00d
ssmformal,The Expressive Capacity of State Space Models: A Formal Language Perspective ,e7f47e8393c697696a3fccd9ff906dfdb49fe736
kanmlp,KAN or MLP: A Fairer Comparison,3814fefc10766fc802656f91a249678dbe91a641
graphlm,Graph Language Models,e0d303b5c1334e96dd04b9c94658fe0dc8f568ac
mentaltimetravel,Towards mental time travel: a hierarchical memory for reinforcement learning agents,008cfd24dfdfb28fc6a89c32772c7ffe5cb0cf8a
multistreamtrans,Multi-Stream Transformers,177e21dbf45d01a0f585bb46e548d795b441aad9
perceiverio,Perceiver IO: A General Architecture for Structured Inputs & Outputs,9933a5af7895354087baf6c96b64dc8a8973eaed
axialattn,Axial Attention in Multidimensional Transformers,366244acdd930e488ae224ab6e2a92dc24aa7e06
hiddenattnmamba,The Hidden Attention of Mamba Models ,26e6cd121c5fdb147df83cb848e4813c926737c8
jamba,Jamba: A Hybrid Transformer-Mamba Language Model ,cbaf689fd9ea9bc939510019d90535d6249b3367
multiresconvmem,Sequence Modeling with Multiresolution Convolutional Memory ,6b655b74da908e2e0d7c37996d36d5ff4a28f030
resurrectrnn,Resurrecting Recurrent Neural Networks for Long Sequences ,f393aff1593c2d370ec0ae004910d18e40524967
hypermixer,HyperMixer: An MLP-based Low Cost Alternative to Transformers ,fb7e324729be2931dc463a572566283802d8aff2
lighdynconv,Pay Less Attention with Lightweight and Dynamic Convolutions ,fea820b7d953d32069e189af2961c28fd213470b
transnormer,The Devil in Linear Transformer ,e3fc46d5f4aae2c7a8a86b6bd21ca8db5d40fcbd
flowformer,Flowformer: Linearizing Transformers with Conservation Flows ,9b61adb6f0d1e8831ab2f5481a12e2125b13c50a
universaltrans,Universal Transformers ,ac4dafdef1d2b685b7f28a11837414573d39ff4e
neuraldatarouter,The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization ,e528466e2aff981511d4ca6e063211297c0b4175
orderedmem,Ordered Memory ,eb48aae0bd3a6311cdaf38b4d95aca41538a2d0c
contrnn,Modeling Hierarchical Structures with Continuous Recursive Neural Networks ,db016d2b6d2577c47d62f9de2a7d1ddbf226386a
soft,SOFT: Softmax-free Transformer with Linear Complexity ,2e644c67a697073d561da4f4dad35e5ad5316cfd
sedd,Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution ,ce806f8d32f6fb1eaa821248a1bc4fa2cd949fbb
ssdlm,Semi-autoregressive Simplex-based Diffusion Language Model ,0b9770a377b3f96cef9f268cee1791d39a0d4893
diffusionbert,DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models,a979742220a88b1d32e1fbe72c41e8ba3007053c
d3pms,Structured Denoising Diffusion Models in Discrete State-Spaces ,91b32fc0a23f0af53229fceaae9cce43a0406d2e
diffusionlm,Diffusion-LM Improves Controllable Text Generation ,1386b8a11929cf02da291c56aca353e33bbc22ed
likelihooddifflm,Likelihood-Based Diffusion Language Models ,d9ffb44ee3c8ec0b6692df8a90451384c1edd89b
tess,TESS: Text-to-Text Self-Conditioned Simplex Diffusion ,67cdecbcfed07b9a29d9e2a92da684604383afd7
diffuseq,DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models ,69144d537f90f214d5b07a7c79121d16afd7da16
contdiffu,Continuous diffusion for categorical data ,22775e58932cdfbd273a2a835a22c5d86800a458
classfreediffu,Classifier-Free Diffusion Guidance ,af9f365ed86614c800f082bd8eb14be76072ad16
selfcondembdiffu,Self-conditioned Embedding Diffusion for Text Generation ,2c6ac935c826002976722ca8d3319f691975687e
analogbits,Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning ,b64537bdf7a103aa01972ba06ea24a9c08f7cd74
reparamdiscdiffu,A Reparameterized Discrete Diffusion Model for Text Generation ,1f898d66acabff511a3871b82799aa73c0055402
derandddm,Fast Sampling via Discrete Non-Markov Diffusion Models,abca18159e0836406a648414cd4275715f3cc12e
dinoiser,DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises ,020a50f6a7154850ac81e3cde69ad8198ded6751
sgconv,What Makes Convolutional Models Great on Long Sequence Modeling?,240300b1da360f22bf0b82c6817eacebba6deed4
liquids4,Liquid Structural State-Space Models,b40f0b0465cdf4b487fb2ef85d4e2672c4b623cc
