{
    "log": "Checking the designed model...\nChecking code format...\nCode format is correct and reformatted.\n\n\nWarnings:\n\nThe super().__init__(embed_dim, block_loc) call in GAB is force overwritten by the reformatter. It may cause error if you modified this line.\n\nThe statement \"logger = logging.get_logger(__name__)\" is removed by the reformatter.\n\nThe statement \"CHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\" is removed by the reformatter.\n\nThe statement \"logger = logging.get_logger(__name__)\" is removed by the reformatter.\n\nThe statement \"CHILDREN_DECLARATIONS = []\" is removed by the reformatter.\n\nThe statement \"CHILDREN_DECLARATIONS = []\" is removed by the reformatter.\n\nThe statement \"logger = logging.get_logger(__name__)\" is removed by the reformatter.\n\nThe statement \"CHILDREN_DECLARATIONS = [UnitDecl(unitname='RotaryEmbedding', requirements=\n    '', inputs=['X'], outputs=['cos', 'sin'])]\" is removed by the reformatter.\n\nThe statement \"logger = logging.get_logger(__name__)\" is removed by the reformatter.\n\nThe statement \"CHILDREN_DECLARATIONS = []\" is removed by the reformatter.\n\nThe statement \"logger = logging.get_logger(__name__)\" is removed by the reformatter.\n\nThe statement \"CHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\" is removed by the reformatter.\n\nChecking the unit tests...\nReloading the model...\nTesting forward pass... Mock input shape: torch.Size([2, 2048]).\nForward check finished. Captured output during the test:\n\nBEGIN OF CAPTURED OUTPUT:\n\n - No captured output during the loading and initialization of the model.\n\n - No captured output or error during the forward pass of the model.\n\n\n\nEND OF CAPTURED OUTPUT.\n\n\nModel initialization succeeded.\n|------Model size------|\n Total params: 5.26M (tied)\n - GAM params: 5.26M\n   - Embedding: 4.10M\n   - Non-embedding: 1.17M\n     - Block: 194.71K x 6\n       - GAB: 194.71K\n - LM Head params: 4.10M\n|----------------------|\n\nChecking forward pass... Mock input shape: torch.Size([2, 2048]).\nForward pass test passed\nChecking causality... It checks the causality by changing all future steps X[t+delta] of X[t] and see if Y[t] or any previous outputs change.Mock input shape: torch.Size([2, 100, 128]).\nCausality test passed\nChecking differentiability... Mock input shape: torch.Size([2, 2048]).\nDifferentiability test passed\nChecking effectiveness...\nThe model is effective.\n\n\nWarnings:\n\nThe model is not efficient. The training time is long. Its 1.97 times of the benchmark.\nAll tests passed!\n\n",
    "effectiveness": {
        "run_time": 38.2132,
        "loss": 8.90625,
        "gradient_of_losses": -0.27343,
        "max_memory_allocated": 7763.63330078125,
        "total_flos": 2297372344320.0,
        "train_loss": 8.90625
    },
    "hints": []
}