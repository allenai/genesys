{
    "log": "Checking the designed model...\nChecking code format...\nCode format is correct and reformatted.\n\n\nWarnings:\n\nThe super().__init__(embed_dim, block_loc) call in GAB is force overwritten by the reformatter. It may cause error if you modified this line.\n\nChecking the unit tests...\nReloading the model...\nTesting forward pass... Mock input shape: torch.Size([2, 2048]).\nForward check finished. Captured output during the test:\n\nBEGIN OF CAPTURED OUTPUT:\n\n - No captured output during the loading and initialization of the model.\n\n - No captured output or error during the forward pass of the model.\n\n\n\nEND OF CAPTURED OUTPUT.\n\n\nModel initialization succeeded.\n|------Model size------|\n Total params: 5.61M (tied)\n - GAM params: 5.61M\n   - Embedding: 4.10M\n   - Non-embedding: 1.51M\n     - Block: 251.90K x 6\n       - GAB: 251.90K\n - LM Head params: 4.10M\n|----------------------|\n\nChecking forward pass... Mock input shape: torch.Size([2, 2048]).\nForward pass test passed\nChecking causality... It checks the causality by changing all future steps X[t+delta] of X[t] and see if Y[t] or any previous outputs change.Mock input shape: torch.Size([2, 100, 128]).\nCausality test passed\nChecking differentiability... Mock input shape: torch.Size([2, 2048]).\nDifferentiability test passed\nChecking effectiveness...\nThe model is effective.\n\n\nWarnings:\n\nThe model is not efficient. The memory usage is high. Its 1.65 times of the benchmark.\nAll tests passed!\n\n",
    "effectiveness": {
        "run_time": 19.4585,
        "loss": 8.415625,
        "gradient_of_losses": -0.32500999999999997,
        "max_memory_allocated": 11486.37939453125,
        "total_flos": 2972083814400.0,
        "train_loss": 8.415625
    },
    "hints": []
}