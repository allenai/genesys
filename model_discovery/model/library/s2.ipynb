{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "index_dir='library_index.csv'\n",
    "\n",
    "\n",
    "headers = {'x-api-key': 'api_key'}\n",
    "\n",
    "match_api='https://api.semanticscholar.org/graph/v1/paper/search/match?query={title}'\n",
    "\n",
    "index_csv=pd.read_csv(index_dir)\n",
    "index_dict=index_csv.to_dict(orient='records')\n",
    "index={}\n",
    "for i in index_dict:\n",
    "    index[i['acronym'].lower()]=i['title']\n",
    "print(len(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=pd.read_csv('library_index.csv')\n",
    "base=os.listdir('base')\n",
    "\n",
    "PASS=True\n",
    "for i in base:\n",
    "    if i not in list(index['acronym']):\n",
    "        print(i, 'not in index')\n",
    "        PASS=False\n",
    "    code=[i.split('.')[0] for i in os.listdir('base/'+i)]\n",
    "    if f'{i}_edu' not in code:\n",
    "        print(i, 'code not named correctly')\n",
    "        PASS=False\n",
    "if PASS:\n",
    "    print('All checks passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get paper id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:55<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "papers={}\n",
    "for i in tqdm(index):\n",
    "    papers[i]=requests.get(match_api.format(title=index[i]), headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_csv['acronym']=index_csv['acronym'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ntk\n",
      "Error: longnet\n",
      "Error: feedbackmem\n",
      "Error: srt\n",
      "Error: kangpt\n",
      "Error: s4pp\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "unmatched=[]\n",
    "\n",
    "for i in papers:\n",
    "    try:\n",
    "        id=papers[i]['data'][0]['paperId']\n",
    "        index_csv.loc[index_csv['acronym']==i,'id']=id\n",
    "    except:\n",
    "        unmatched.append(i)\n",
    "        print('Error:',i)\n",
    "\n",
    "print(len(unmatched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.15it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(unmatched):\n",
    "    papers[i]=requests.get(match_api.format(title=index[i])).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntk\n",
      "feedbackmem\n",
      "srt\n",
      "kangpt\n",
      "s4pp\n"
     ]
    }
   ],
   "source": [
    "# index_csv.to_csv('library_index.csv',index=False)\n",
    "for i in index_csv['acronym']:\n",
    "    if index_csv.loc[index_csv['acronym']==i,'id'].isnull().values.any():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct metadata & references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:00<00:00, 1041.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:00<00:00, 86839.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "meta={}\n",
    "meta_dir='library_meta.json'\n",
    "if os.path.exists(meta_dir):\n",
    "    with open(meta_dir,'r') as f:\n",
    "        meta=json.load(f)\n",
    "\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=year,authors,tldr,venue,abstract,citationCount,influentialCitationCount,references,embedding.specter_v2,openAccessPdf'\n",
    "\n",
    "for i in tqdm(index_csv.index):\n",
    "    acronym=index_csv.loc[i,'acronym']\n",
    "    title=index_csv.loc[i,'title']\n",
    "    if 'detail' in meta.get(acronym,{}):\n",
    "        if not 'message' in meta[acronym]['detail']:\n",
    "            continue\n",
    "    if not pd.isna(index_csv.loc[i,'id']):\n",
    "        paper_id=index_csv.loc[i,'id']\n",
    "        detail=requests.get(paper_detail.format(paper_id=paper_id)).json()#,headers=headers).json()\n",
    "        if 'message' in detail:\n",
    "            print('Error:',acronym,detail['message'])\n",
    "            continue\n",
    "        meta[acronym]={}\n",
    "        meta[acronym]['title']=title\n",
    "        meta[acronym]['id']=paper_id\n",
    "        meta[acronym]['detail']=detail\n",
    "    else:\n",
    "        meta[acronym]={}\n",
    "        meta[acronym]['title']=title\n",
    "        meta[acronym]['id']=paper_id\n",
    "        meta[acronym]['detail']={}\n",
    "\n",
    "print(len(meta))\n",
    "\n",
    "to_remove=[]\n",
    "for i in meta:\n",
    "    if i not in index:\n",
    "        to_remove.append(i)\n",
    "for i in to_remove:\n",
    "    meta.pop(i)\n",
    "    \n",
    "print(len(meta))\n",
    "with open(meta_dir,'w') as f:\n",
    "    json.dump(meta,f)\n",
    "        \n",
    "\n",
    "references={}\n",
    "ref_dir='library_ref.json'\n",
    "if os.path.exists(ref_dir):\n",
    "    with open(ref_dir,'r') as f:\n",
    "        references=json.load(f)\n",
    "\n",
    "references_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references?fields=contextsWithIntent,intents,isInfluential,title,influentialCitationCount'\n",
    "\n",
    "for i in tqdm(index_csv.index):\n",
    "    acronym=index_csv.loc[i,'acronym']\n",
    "    id=index_csv.loc[i,'id']\n",
    "    if acronym in references:\n",
    "        if 'message' not in references[acronym]:\n",
    "            continue\n",
    "    if pd.isna(id):\n",
    "        references[acronym]=[]\n",
    "    else:\n",
    "        ret=requests.get(references_detail.format(paper_id=id)).json()#,headers=headers).json()\n",
    "        if 'message' in ret:\n",
    "            print('Error:',acronym,ret['message'])\n",
    "            continue\n",
    "        references[acronym]=ret\n",
    "    \n",
    "print(len(references))\n",
    "to_remove=[]\n",
    "for i in references:\n",
    "    if i not in index:\n",
    "        to_remove.append(i)\n",
    "for i in to_remove:\n",
    "    references.pop(i)\n",
    "print(len(references))\n",
    "\n",
    "with open(ref_dir,'w') as f:\n",
    "    json.dump(references,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./library_index.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./library_meta.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     meta\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "index=pd.read_csv('./library_index.csv')\n",
    "with open('./library_meta.json','r') as f:\n",
    "    meta=json.load(f)\n",
    "with open('./library_ref.json','r') as f:\n",
    "    refs=json.load(f)\n",
    "print(len(index),len(meta),len(refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['hydra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "LIBRARY_DIR = './'\n",
    "\n",
    "pjoin=os.path.join\n",
    "pexists=os.path.exists\n",
    "\n",
    "@dataclass\n",
    "class NodeObject:\n",
    "    acronym: str\n",
    "    title: str\n",
    "    seed_ids: List[str]\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, dict: Dict):\n",
    "        return cls(**dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, save_dir: str, acronym:str):\n",
    "        with open(pjoin(save_dir,acronym+'.json'),'r') as f:\n",
    "            return cls.from_dict(json.load(f))\n",
    "\n",
    "    def save(self,save_dir: str):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        with open(pjoin(save_dir,self.acronym+'.json'),'w') as f:\n",
    "            json.dump(self.to_dict(),f,indent=4)\n",
    "\n",
    "    def to_desc(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "@dataclass\n",
    "class LibraryReference(NodeObject):\n",
    "    s2id: str = None\n",
    "    abstract: str = None\n",
    "    authors: List[str] = None\n",
    "    venue: str = None\n",
    "    year: int = None\n",
    "    tldr: str = None\n",
    "    # embedding: list\n",
    "    citationCount: int = None\n",
    "    influentialCitationCount: int = None\n",
    "    code: str = None\n",
    "    description: str = None\n",
    "    url: str = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        code_dir=pjoin(LIBRARY_DIR,'base',self.acronym,self.acronym+'_edu.py')\n",
    "        if pexists(code_dir):\n",
    "            self.code=open(code_dir,'r').read()\n",
    "        else:\n",
    "            self.code=None\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        if self.code is not None:\n",
    "            return 'ReferenceWithCode'\n",
    "        else:\n",
    "            return 'Reference'\n",
    "\n",
    "    def to_desc(self) -> str:\n",
    "        title=self.title.replace(':',' ')\n",
    "        mdtext=f'# {title}'\n",
    "        if self.s2id:\n",
    "            mdtext+=f'\\n* S2 ID {self.s2id} *'\n",
    "        if self.authors:\n",
    "            authors=', '.join(self.authors)\n",
    "            mdtext+=f'\\n* Authors: {authors} *'\n",
    "        if self.tldr:\n",
    "            tldr=self.tldr.replace(':',' ').replace(',',',\\n')\n",
    "            mdtext+=f'\\n\\n* TL;DR {tldr} *'\n",
    "        if self.abstract:\n",
    "            abstract=self.abstract.replace(':',' ').replace('.','.\\n')\n",
    "            mdtext+=f'\\n\\n## Abstract\\n{abstract}'\n",
    "        if self.venue:\n",
    "            venue=self.venue.replace(':',' ')\n",
    "            mdtext+=f'\\n\\n* Published at {venue} in {self.year} *'\n",
    "            mdtext+=f'\\n* Cited {self.citationCount} times *'\n",
    "            mdtext+=f'\\n* Impactful citations {self.influentialCitationCount} *'\n",
    "        if self.description:\n",
    "            description=self.description.replace(':',' ').replace('.','.\\n')\n",
    "            mdtext+=f'\\n\\n## Description\\n{description}'\n",
    "        return mdtext\n",
    "\n",
    "\n",
    "manual_input={\n",
    "    'srt': {\n",
    "        'title': 'Self Reasoning Tokens',\n",
    "        'authors': ['Felipe Sens Bonetto'],\n",
    "        'year': 2024,\n",
    "        'url': 'https://github.com/lucidrains/self-reasoning-tokens-pytorch',\n",
    "        'description': \"The project \\\"Reasoning Tokens\\\" by Felipe Sens Bonetto aims to enhance the reasoning abilities of language models like GPT by teaching them to plan ahead in a self-supervised way. The core idea is to introduce \\\"reasoning tokens,\\\" where for each token predicted, an additional token is generated that duplicates the input and doesn't receive a gradient from the next token but from future tokens. This approach encourages the model to pre-cache information useful for future predictions. Initial experiments showed a significant reduction in loss, indicating improved performance. The project plans to explore this method further, especially in fine-tuned instruction-following models, potentially replacing the need for step-by-step explanations during training. The ultimate goal is to create models that can reason internally, improving their performance and reducing the need for manually crafted training data.\",\n",
    "        'seed_ids': ['gpt3']\n",
    "    },\n",
    "    'ntk': {\n",
    "        'title': 'NTK-Aware Scaled RoPE',\n",
    "        'authors': ['bloc97'],\n",
    "        'year': 2023,\n",
    "        'url': 'https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have',\n",
    "        'description': \"The \\\"NTK-Aware Scaled RoPE\\\" project aims to extend the context size of LLaMA models beyond 8k tokens without fine-tuning and with minimal perplexity degradation. Traditional methods like RoPE interpolation often fail to distinguish between closely positioned tokens, leading to performance issues. By applying Neural Tangent Kernel (NTK) theory, this new method uses a nonlinear interpolation scheme that changes the RoPE's base rather than its scale, allowing for accurate distinction of token positions. This approach enables the LLaMA 7B model to handle longer contexts (up to 12k tokens) with minimal perplexity degradation, without fine-tuning. Initial tests show promising results, suggesting that further fine-tuning could enhance performance even more. The method provides a new way to extend the context window size efficiently, potentially benefiting tasks like long document summarization. The author encourages further experimentation and innovation in this area.\",\n",
    "        'seed_ids': ['roformer']\n",
    "    },\n",
    "    'feedbackmem': {\n",
    "        'title': 'Addressing Some Limitations of Transformers with Feedback Memory',\n",
    "        'authors': ['Angela Fan', 'Thibaut Lavril', 'Edouard Grave', 'Armand Joulin', 'Sainbayar Sukhbaatar'],\n",
    "        'venue': 'arXiv',\n",
    "        'year': 2020,\n",
    "        'abstract': \"Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.\",\n",
    "        'tldr': 'Transformers have shortcomings - limited memory and limited state update - but Feedback Memory is a straightforward way to resolve these. ',\n",
    "        'seed_ids': ['transformer','bert']\n",
    "    },\n",
    "    'kangpt': {\n",
    "        'title': 'Generative Pre-trained Transformers (GPTs) using Kolmogorov-Arnold Networks (KANs) for language modeling',\n",
    "        'authors': ['Aditya N Ganesh'],\n",
    "        'year': 2024,\n",
    "        'url': 'https://adityang.github.io/kan-gpt/',\n",
    "        'description': \"Kolmogorov-Arnold Networks (KANs) are promising alternatives of Multi-Layer Perceptrons (MLPs). KANs have strong mathematical foundations just like MLPs: MLPs are based on the universal approximation theorem, while KANs are based on Kolmogorov-Arnold representation theorem. KANs and MLPs are dual: KANs have activation functions on edges, while MLPs have activation functions on nodes. This simple change makes KANs better (sometimes much better!) than MLPs in terms of both model accuracy and interpretability. \",\n",
    "        'seed_ids': ['gpt3','transformer']\n",
    "    },\n",
    "    's4pp': {\n",
    "        'title': 'S4++: Elevating Long Sequence Modeling with State Memory Reply',\n",
    "        'authors': ['Biqing Qi', 'Junqi Gao', 'Dong Li', 'Kaiyan Zhang', 'Jianxing Liu', 'Ligang Wu', 'Bowen Zhou'],\n",
    "        'venue': 'ICLR 2024 Withdrawn Submission',\n",
    "        'year': 2024,\n",
    "        'url': 'https://openreview.net/forum?id=bdnw4qjfH9',\n",
    "        'abstract': \"Recently, state space models (SSMs) have shown significant performance advantages in modeling long sequences. However, in spite of their promising performance, there still exist limitations. 1. Non-Stable-States (NSS): Significant state variance discrepancies arise among discrete sampling steps, occasionally resulting in divergence. 2. Dependency Bias: The unidirectional state space dependency in SSM impedes the effective modeling of intricate dependencies. In this paper, we conduct theoretical analysis of SSM from the even-triggered control (ETC) theory perspective and first propose the presence of NSS Phenomenon. Our findings indicate that NSS primarily results from the sampling steps, and the integration of multi-state inputs into the current state significantly contributes to the mitigation of NSS. Building upon these theoretical analyses and findings, we propose a simple, yet effective, theoretically grounded State Memory Reply (SMR) mechanism that leverages learnable memories to incorporate multi-state information into the current state. This enables the precise modeling of finer state dependencies within the SSM, resulting in the introduction of S4+. Furthermore, we integrate the complex dependency bias into S4+ via interactive cross attentions mechanism, resulting in the development of S4++. Our extensive experiments in autoregressive language modeling and benchmarking against the Long Range Arena demonstrate superior performance in most post-processing tasks.\",\n",
    "        'seed_ids': ['s4']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for acronym in refs:\n",
    "    if refs[acronym]==[]:\n",
    "        obj=manual_input[acronym]\n",
    "        obj['acronym']=acronym\n",
    "        proj=LibraryReference.from_dict(obj)\n",
    "        proj.save('./tree')\n",
    "        continue   \n",
    "    refdata=refs[acronym]['data']\n",
    "    seed_ids=[]\n",
    "    for ref in refdata:\n",
    "        if 'methodology' in ref['intents']:\n",
    "            if ref['citedPaper']['paperId'] in index['id'].values:\n",
    "                ref_acronym=index.loc[index['id']==ref['citedPaper']['paperId'],'acronym'].values[0]\n",
    "                seed_ids.append(ref_acronym)\n",
    "    title=meta[acronym]['title']\n",
    "    s2id=meta[acronym]['id']\n",
    "    abstract=meta[acronym]['detail']['abstract']\n",
    "    authors=[author['name'] for author in meta[acronym]['detail']['authors']]\n",
    "    if abstract is None:\n",
    "        abstract='N/A'\n",
    "    venue=meta[acronym]['detail']['venue']\n",
    "    if venue is None:\n",
    "        venue='arXiv'\n",
    "    year=meta[acronym]['detail']['year']\n",
    "    if year is None:\n",
    "        year='N/A'\n",
    "    tldr=meta[acronym]['detail']['tldr']\n",
    "    if tldr is None:\n",
    "        tldr='N/A'\n",
    "    else:\n",
    "        tldr=tldr['text']\n",
    "        if tldr is None: tldr='N/A'\n",
    "    embedding=meta[acronym]['detail']['embedding']\n",
    "    if embedding is None:\n",
    "        embedding=[]\n",
    "    else:\n",
    "        embedding=embedding['vector']\n",
    "    citationCount=meta[acronym]['detail']['citationCount']\n",
    "    influentialCitationCount=meta[acronym]['detail']['influentialCitationCount']\n",
    "    paper=LibraryReference(title=title,acronym=acronym,seed_ids=seed_ids,s2id=s2id,abstract=abstract,authors=authors,venue=venue,year=year,tldr=tldr,citationCount=citationCount,influentialCitationCount=influentialCitationCount)\n",
    "    paper.save('./tree')\n",
    "\n",
    "print(len(os.listdir('./tree')))\n",
    "# for i in os.listdir('./tree'):\n",
    "#     if i.split('.')[0] not in refs:\n",
    "#         print(i)\n",
    "#         os.remove(pjoin('./tree',i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 1 hop impactful cites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 297\n"
     ]
    }
   ],
   "source": [
    "index=pd.read_csv('./library_index.csv')\n",
    "with open('./library_meta.json','r') as f:\n",
    "    meta=json.load(f)\n",
    "print(len(index),len(meta))\n",
    "dir_1hop='./tree_ext'\n",
    "get_cite='https://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations?fields=intents,contextsWithIntent,isInfluential,title&offset={offset}&limit=1000'\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=year,authors,tldr,venue,abstract,citationCount,influentialCitationCount,references,embedding.specter_v2,openAccessPdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:00<00:00, 5183.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cite_1hop_dir=dir_1hop+'/cite_1hop.json'\n",
    "# detail_1hop_dir=dir_1hop+'/detail_1hop.json'\n",
    "os.makedirs(dir_1hop,exist_ok=True)\n",
    "\n",
    "cite_1hop={}\n",
    "if os.path.exists(cite_1hop_dir):\n",
    "    with open(cite_1hop_dir,'r') as f:\n",
    "        cite_1hop=json.load(f)\n",
    "\n",
    "for acronym in tqdm(index['acronym']):\n",
    "    id=index.loc[index['acronym']==acronym,'id'].values[0]\n",
    "    if pd.isna(id):\n",
    "        cite_1hop[acronym]=[]\n",
    "        continue\n",
    "    citecount=meta[acronym]['detail']['citationCount']\n",
    "    if acronym in cite_1hop:\n",
    "        if 'message' not in cite_1hop[acronym]:\n",
    "            # print('Already done:',acronym,len(cite_1hop[acronym]),citecount)\n",
    "            continue\n",
    "    cite_1hop[acronym]=[]\n",
    "    maxoffset=min(citecount,9001)\n",
    "    for offset in range(0,maxoffset,1000):\n",
    "        if offset+1000>=10000:\n",
    "            offset=8999\n",
    "        print(acronym,offset,offset+1000)\n",
    "        cites=requests.get(get_cite.format(paper_id=id,offset=offset)).json()#,headers=headers).json()\n",
    "        if 'message' in cites:\n",
    "            print('Error:',id,cites['message'])\n",
    "            continue\n",
    "        if 'data' not in cites:\n",
    "            print('Error:',cites)\n",
    "            raise\n",
    "        for c in cites['data']:\n",
    "            paperid=c['citingPaper']['paperId']\n",
    "            if paperid in cite_1hop[acronym]:\n",
    "                continue\n",
    "            if 'methodology' in c['intents'] and c['isInfluential']:\n",
    "                cite_1hop[acronym].append(paperid)\n",
    "        time.sleep(0.1)\n",
    "    print('Done',acronym,len(cite_1hop[acronym]),citecount)\n",
    "    with open(cite_1hop_dir,'w') as f:\n",
    "        json.dump(cite_1hop,f)\n",
    "\n",
    "# to_remove=[]\n",
    "# for i in cite_1hop:\n",
    "#     if i not in index['acronym'].values:\n",
    "#         to_remove.append(i)\n",
    "# for i in to_remove:\n",
    "#     cite_1hop.pop(i)\n",
    "# with open(cite_1hop_dir,'w') as f:\n",
    "#     json.dump(cite_1hop,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load paper details for 1 hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6856/6856 [00:02<00:00, 2980.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "with open(cite_1hop_dir,'r') as f:\n",
    "    cite_1hop=json.load(f)\n",
    "\n",
    "index_1hop={}\n",
    "for i in cite_1hop:\n",
    "    for j in cite_1hop[i]:\n",
    "        index_1hop[j]={}\n",
    "\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=title,year,authors,tldr,venue,abstract,citationCount,influentialCitationCount,references,embedding.specter_v2,openAccessPdf'\n",
    "\n",
    "dir_meta='./tree_ext/meta'\n",
    "os.makedirs(dir_meta,exist_ok=True)\n",
    "for c in tqdm(index_1hop):\n",
    "    if c is None: continue\n",
    "    detail_1hop_dir=dir_meta+'/'+c+'.json'\n",
    "    if os.path.exists(detail_1hop_dir):\n",
    "        detail=json.load(open(detail_1hop_dir,'r'))\n",
    "        if 'title' in detail:\n",
    "            continue\n",
    "    detail=requests.get(paper_detail.format(paper_id=c),headers=headers).json()\n",
    "    time.sleep(0.2)\n",
    "    if 'message' in detail:\n",
    "        print('Error:',c,detail['message'])\n",
    "        continue\n",
    "    with open(detail_1hop_dir,'w') as f:\n",
    "        json.dump(detail,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 hoc tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6855\n"
     ]
    }
   ],
   "source": [
    "import json,os\n",
    "\n",
    "detail_1hop_dir='./tree_ext/meta'\n",
    "cite_1hop_dir='./tree_ext/cite_1hop.json' \n",
    "\n",
    "with open(cite_1hop_dir,'r') as f:\n",
    "    cite_1hop=json.load(f)\n",
    "\n",
    "details={}\n",
    "for i in os.listdir(detail_1hop_dir):\n",
    "    with open(os.path.join(detail_1hop_dir,i),'r') as f:\n",
    "        details[i.split('.')[0]]=json.load(f)\n",
    "print(len(details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paperId', 'title', 'abstract', 'venue', 'year', 'citationCount', 'influentialCitationCount', 'openAccessPdf', 'tldr', 'embedding', 'authors', 'references'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details['4064ebec539718b2d77607df8d86c6d436def83e'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "LIBRARY_DIR = './'\n",
    "\n",
    "pjoin=os.path.join\n",
    "pexists=os.path.exists\n",
    "\n",
    "@dataclass\n",
    "class NodeObject:\n",
    "    acronym: str\n",
    "    title: str\n",
    "    seed_ids: List[str]\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, dict: Dict):\n",
    "        return cls(**dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, save_dir: str, acronym:str):\n",
    "        with open(pjoin(save_dir,acronym+'.json'),'r') as f:\n",
    "            return cls.from_dict(json.load(f))\n",
    "\n",
    "    def save(self,save_dir: str):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        with open(pjoin(save_dir,self.acronym+'.json'),'w') as f:\n",
    "            json.dump(self.to_dict(),f,indent=4)\n",
    "\n",
    "    def to_desc(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "@dataclass\n",
    "class LibraryReference(NodeObject):\n",
    "    s2id: str = None\n",
    "    abstract: str = None\n",
    "    authors: List[str] = None\n",
    "    venue: str = None\n",
    "    year: int = None\n",
    "    tldr: str = None\n",
    "    # embedding: list\n",
    "    citationCount: int = None\n",
    "    influentialCitationCount: int = None\n",
    "    code: str = None\n",
    "    description: str = None\n",
    "    url: str = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        code_dir=pjoin(LIBRARY_DIR,'base',self.acronym,self.acronym+'_edu.py')\n",
    "        if pexists(code_dir):\n",
    "            self.code=open(code_dir,'r').read()\n",
    "        else:\n",
    "            self.code=None\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        if self.code is not None:\n",
    "            return 'ReferenceWithCode'\n",
    "        else:\n",
    "            return 'Reference'\n",
    "\n",
    "    def to_desc(self) -> str:\n",
    "        title=self.title.replace(':',' ')\n",
    "        mdtext=f'# {title}'\n",
    "        if self.s2id:\n",
    "            mdtext+=f'\\n* S2 ID {self.s2id} *'\n",
    "        if self.authors:\n",
    "            authors=', '.join(self.authors)\n",
    "            mdtext+=f'\\n* Authors: {authors} *'\n",
    "        if self.tldr:\n",
    "            tldr=self.tldr.replace(':',' ').replace(',',',\\n')\n",
    "            mdtext+=f'\\n\\n* TL;DR {tldr} *'\n",
    "        if self.abstract:\n",
    "            abstract=self.abstract.replace(':',' ').replace('.','.\\n')\n",
    "            mdtext+=f'\\n\\n## Abstract\\n{abstract}'\n",
    "        if self.venue:\n",
    "            venue=self.venue.replace(':',' ')\n",
    "            mdtext+=f'\\n\\n* Published at {venue} in {self.year} *'\n",
    "            mdtext+=f'\\n* Cited {self.citationCount} times *'\n",
    "            mdtext+=f'\\n* Impactful citations {self.influentialCitationCount} *'\n",
    "        if self.description:\n",
    "            description=self.description.replace(':',' ').replace('.','.\\n')\n",
    "            mdtext+=f'\\n\\n## Description\\n{description}'\n",
    "        return mdtext\n",
    "\n",
    "class LibraryReference1hop(LibraryReference):\n",
    "\n",
    "    def type(self):\n",
    "        if self.code is not None:\n",
    "            return 'Reference1hopWithCode'\n",
    "        else:\n",
    "            return 'Reference1hop'\n",
    "\n",
    "\n",
    "for id in details:\n",
    "    if os.path.exists(f'./tree_ext/1hop/{id}.json'):\n",
    "        continue\n",
    "    seed_ids=[]\n",
    "    detail=details[id]\n",
    "    references=[ref['paperId'] for ref in detail['references']]\n",
    "    for i in cite_1hop:\n",
    "        if id in cite_1hop[i]:\n",
    "            seed_ids.append(i)\n",
    "    for i in references:\n",
    "        if i in details:\n",
    "            seed_ids.append(i)\n",
    "    s2id=id\n",
    "    acronym = id\n",
    "    title = detail['title']\n",
    "    abstract = detail['abstract']\n",
    "    authors = [author['name'] for author in detail['authors']]\n",
    "    venue = detail['venue']\n",
    "    year = detail['year']\n",
    "    tldr = detail['tldr']\n",
    "    citationCount = detail['citationCount']\n",
    "    influentialCitationCount = detail['influentialCitationCount']\n",
    "    seed_ids: List[str]\n",
    "    paper=LibraryReference1hop(title=title,acronym=acronym,seed_ids=seed_ids,s2id=s2id,abstract=abstract,authors=authors,venue=venue,year=year,tldr=tldr,citationCount=citationCount,influentialCitationCount=influentialCitationCount)\n",
    "    paper.save('./tree_ext/1hop')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
