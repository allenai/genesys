{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "index_dir='library_index.csv'\n",
    "\n",
    "\n",
    "headers = {'x-api-key': 'api_key'}\n",
    "\n",
    "match_api='https://api.semanticscholar.org/graph/v1/paper/search/match?query={title}'\n",
    "\n",
    "index_csv=pd.read_csv(index_dir)\n",
    "index_dict=index_csv.to_dict(orient='records')\n",
    "index={}\n",
    "for i in index_dict:\n",
    "    index[i['acronym'].lower()]=i['title']\n",
    "print(len(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=pd.read_csv('library_index.csv')\n",
    "base=os.listdir('base')\n",
    "\n",
    "PASS=True\n",
    "for i in base:\n",
    "    if i not in list(index['acronym']):\n",
    "        print(i, 'not in index')\n",
    "        PASS=False\n",
    "    code=[i.split('.')[0] for i in os.listdir('base/'+i)]\n",
    "    if f'{i}_edu' not in code:\n",
    "        print(i, 'code not named correctly')\n",
    "        PASS=False\n",
    "if PASS:\n",
    "    print('All checks passed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get paper id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:55<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "papers={}\n",
    "for i in tqdm(index):\n",
    "    papers[i]=requests.get(match_api.format(title=index[i]), headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_csv['acronym']=index_csv['acronym'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ntk\n",
      "Error: longnet\n",
      "Error: feedbackmem\n",
      "Error: srt\n",
      "Error: kangpt\n",
      "Error: s4pp\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "unmatched=[]\n",
    "\n",
    "for i in papers:\n",
    "    try:\n",
    "        id=papers[i]['data'][0]['paperId']\n",
    "        index_csv.loc[index_csv['acronym']==i,'id']=id\n",
    "    except:\n",
    "        unmatched.append(i)\n",
    "        print('Error:',i)\n",
    "\n",
    "print(len(unmatched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.15it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in tqdm(unmatched):\n",
    "    papers[i]=requests.get(match_api.format(title=index[i])).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntk\n",
      "feedbackmem\n",
      "srt\n",
      "kangpt\n",
      "s4pp\n"
     ]
    }
   ],
   "source": [
    "# index_csv.to_csv('library_index.csv',index=False)\n",
    "for i in index_csv['acronym']:\n",
    "    if index_csv.loc[index_csv['acronym']==i,'id'].isnull().values.any():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct metadata & references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:00<00:00, 1041.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:00<00:00, 86839.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "meta={}\n",
    "meta_dir='library_meta.json'\n",
    "if os.path.exists(meta_dir):\n",
    "    with open(meta_dir,'r') as f:\n",
    "        meta=json.load(f)\n",
    "\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=year,authors,tldr,venue,abstract,citationCount,influentialCitationCount,references,embedding.specter_v2,openAccessPdf'\n",
    "\n",
    "for i in tqdm(index_csv.index):\n",
    "    acronym=index_csv.loc[i,'acronym']\n",
    "    title=index_csv.loc[i,'title']\n",
    "    if 'detail' in meta.get(acronym,{}):\n",
    "        if not 'message' in meta[acronym]['detail']:\n",
    "            continue\n",
    "    if not pd.isna(index_csv.loc[i,'id']):\n",
    "        paper_id=index_csv.loc[i,'id']\n",
    "        detail=requests.get(paper_detail.format(paper_id=paper_id)).json()#,headers=headers).json()\n",
    "        if 'message' in detail:\n",
    "            print('Error:',acronym,detail['message'])\n",
    "            continue\n",
    "        meta[acronym]={}\n",
    "        meta[acronym]['title']=title\n",
    "        meta[acronym]['id']=paper_id\n",
    "        meta[acronym]['detail']=detail\n",
    "    else:\n",
    "        meta[acronym]={}\n",
    "        meta[acronym]['title']=title\n",
    "        meta[acronym]['id']=paper_id\n",
    "        meta[acronym]['detail']={}\n",
    "\n",
    "print(len(meta))\n",
    "\n",
    "to_remove=[]\n",
    "for i in meta:\n",
    "    if i not in index:\n",
    "        to_remove.append(i)\n",
    "for i in to_remove:\n",
    "    meta.pop(i)\n",
    "    \n",
    "print(len(meta))\n",
    "with open(meta_dir,'w') as f:\n",
    "    json.dump(meta,f)\n",
    "        \n",
    "\n",
    "references={}\n",
    "ref_dir='library_ref.json'\n",
    "if os.path.exists(ref_dir):\n",
    "    with open(ref_dir,'r') as f:\n",
    "        references=json.load(f)\n",
    "\n",
    "references_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references?fields=contextsWithIntent,intents,isInfluential,title,influentialCitationCount'\n",
    "\n",
    "for i in tqdm(index_csv.index):\n",
    "    acronym=index_csv.loc[i,'acronym']\n",
    "    id=index_csv.loc[i,'id']\n",
    "    if acronym in references:\n",
    "        if 'message' not in references[acronym]:\n",
    "            continue\n",
    "    if pd.isna(id):\n",
    "        references[acronym]=[]\n",
    "    else:\n",
    "        ret=requests.get(references_detail.format(paper_id=id)).json()#,headers=headers).json()\n",
    "        if 'message' in ret:\n",
    "            print('Error:',acronym,ret['message'])\n",
    "            continue\n",
    "        references[acronym]=ret\n",
    "    \n",
    "print(len(references))\n",
    "to_remove=[]\n",
    "for i in references:\n",
    "    if i not in index:\n",
    "        to_remove.append(i)\n",
    "for i in to_remove:\n",
    "    references.pop(i)\n",
    "print(len(references))\n",
    "\n",
    "with open(ref_dir,'w') as f:\n",
    "    json.dump(references,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata to tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 297 297\n"
     ]
    }
   ],
   "source": [
    "index=pd.read_csv('./library_index.csv')\n",
    "with open('./library_meta.json','r') as f:\n",
    "    meta=json.load(f)\n",
    "with open('./library_ref.json','r') as f:\n",
    "    refs=json.load(f)\n",
    "print(len(index),len(meta),len(refs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta['hydra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "LIBRARY_DIR = './'\n",
    "\n",
    "pjoin=os.path.join\n",
    "pexists=os.path.exists\n",
    "\n",
    "@dataclass\n",
    "class NodeObject:\n",
    "    acronym: str\n",
    "    title: str\n",
    "    seed_ids: List[str]\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return asdict(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, dict: Dict):\n",
    "        return cls(**dict)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, save_dir: str, acronym:str):\n",
    "        with open(pjoin(save_dir,acronym+'.json'),'r') as f:\n",
    "            return cls.from_dict(json.load(f))\n",
    "\n",
    "    def save(self,save_dir: str):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        with open(pjoin(save_dir,self.acronym+'.json'),'w') as f:\n",
    "            json.dump(self.to_dict(),f,indent=4)\n",
    "\n",
    "    def to_desc(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "@dataclass\n",
    "class LibraryReference(NodeObject):\n",
    "    s2id: str = None\n",
    "    abstract: str = None\n",
    "    authors: List[str] = None\n",
    "    venue: str = None\n",
    "    year: int = None\n",
    "    tldr: str = None\n",
    "    # embedding: list\n",
    "    citationCount: int = None\n",
    "    influentialCitationCount: int = None\n",
    "    code: str = None\n",
    "    description: str = None\n",
    "    url: str = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        code_dir=pjoin(LIBRARY_DIR,'base',self.acronym,self.acronym+'_edu.py')\n",
    "        if pexists(code_dir):\n",
    "            self.code=open(code_dir,'r').read()\n",
    "        else:\n",
    "            self.code=None\n",
    "\n",
    "    @property\n",
    "    def type(self) -> str:\n",
    "        if self.code is not None:\n",
    "            return 'ReferenceWithCode'\n",
    "        else:\n",
    "            return 'Reference'\n",
    "\n",
    "    def to_desc(self) -> str:\n",
    "        title=self.title.replace(':',' ')\n",
    "        mdtext=f'# {title}'\n",
    "        if self.s2id:\n",
    "            mdtext+=f'\\n* S2 ID {self.s2id} *'\n",
    "        if self.authors:\n",
    "            authors=', '.join(self.authors)\n",
    "            mdtext+=f'\\n* Authors: {authors} *'\n",
    "        if self.tldr:\n",
    "            tldr=self.tldr.replace(':',' ').replace(',',',\\n')\n",
    "            mdtext+=f'\\n\\n* TL;DR {tldr} *'\n",
    "        if self.abstract:\n",
    "            abstract=self.abstract.replace(':',' ').replace('.','.\\n')\n",
    "            mdtext+=f'\\n\\n## Abstract\\n{abstract}'\n",
    "        if self.venue:\n",
    "            venue=self.venue.replace(':',' ')\n",
    "            mdtext+=f'\\n\\n* Published at {venue} in {self.year} *'\n",
    "            mdtext+=f'\\n* Cited {self.citationCount} times *'\n",
    "            mdtext+=f'\\n* Impactful citations {self.influentialCitationCount} *'\n",
    "        if self.description:\n",
    "            description=self.description.replace(':',' ').replace('.','.\\n')\n",
    "            mdtext+=f'\\n\\n## Description\\n{description}'\n",
    "        return mdtext\n",
    "\n",
    "\n",
    "manual_input={\n",
    "    'srt': {\n",
    "        'title': 'Self Reasoning Tokens',\n",
    "        'authors': ['Felipe Sens Bonetto'],\n",
    "        'year': 2024,\n",
    "        'url': 'https://github.com/lucidrains/self-reasoning-tokens-pytorch',\n",
    "        'description': \"The project \\\"Reasoning Tokens\\\" by Felipe Sens Bonetto aims to enhance the reasoning abilities of language models like GPT by teaching them to plan ahead in a self-supervised way. The core idea is to introduce \\\"reasoning tokens,\\\" where for each token predicted, an additional token is generated that duplicates the input and doesn't receive a gradient from the next token but from future tokens. This approach encourages the model to pre-cache information useful for future predictions. Initial experiments showed a significant reduction in loss, indicating improved performance. The project plans to explore this method further, especially in fine-tuned instruction-following models, potentially replacing the need for step-by-step explanations during training. The ultimate goal is to create models that can reason internally, improving their performance and reducing the need for manually crafted training data.\",\n",
    "        'seed_ids': ['gpt3']\n",
    "    },\n",
    "    'ntk': {\n",
    "        'title': 'NTK-Aware Scaled RoPE',\n",
    "        'authors': ['bloc97'],\n",
    "        'year': 2023,\n",
    "        'url': 'https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have',\n",
    "        'description': \"The \\\"NTK-Aware Scaled RoPE\\\" project aims to extend the context size of LLaMA models beyond 8k tokens without fine-tuning and with minimal perplexity degradation. Traditional methods like RoPE interpolation often fail to distinguish between closely positioned tokens, leading to performance issues. By applying Neural Tangent Kernel (NTK) theory, this new method uses a nonlinear interpolation scheme that changes the RoPE's base rather than its scale, allowing for accurate distinction of token positions. This approach enables the LLaMA 7B model to handle longer contexts (up to 12k tokens) with minimal perplexity degradation, without fine-tuning. Initial tests show promising results, suggesting that further fine-tuning could enhance performance even more. The method provides a new way to extend the context window size efficiently, potentially benefiting tasks like long document summarization. The author encourages further experimentation and innovation in this area.\",\n",
    "        'seed_ids': ['roformer']\n",
    "    },\n",
    "    'feedbackmem': {\n",
    "        'title': 'Addressing Some Limitations of Transformers with Feedback Memory',\n",
    "        'authors': ['Angela Fan', 'Thibaut Lavril', 'Edouard Grave', 'Armand Joulin', 'Sainbayar Sukhbaatar'],\n",
    "        'venue': 'arXiv',\n",
    "        'year': 2020,\n",
    "        'abstract': \"Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.\",\n",
    "        'tldr': 'Transformers have shortcomings - limited memory and limited state update - but Feedback Memory is a straightforward way to resolve these. ',\n",
    "        'seed_ids': ['transformer','bert']\n",
    "    },\n",
    "    'kangpt': {\n",
    "        'title': 'Generative Pre-trained Transformers (GPTs) using Kolmogorov-Arnold Networks (KANs) for language modeling',\n",
    "        'authors': ['Aditya N Ganesh'],\n",
    "        'year': 2024,\n",
    "        'url': 'https://adityang.github.io/kan-gpt/',\n",
    "        'description': \"Kolmogorov-Arnold Networks (KANs) are promising alternatives of Multi-Layer Perceptrons (MLPs). KANs have strong mathematical foundations just like MLPs: MLPs are based on the universal approximation theorem, while KANs are based on Kolmogorov-Arnold representation theorem. KANs and MLPs are dual: KANs have activation functions on edges, while MLPs have activation functions on nodes. This simple change makes KANs better (sometimes much better!) than MLPs in terms of both model accuracy and interpretability. \",\n",
    "        'seed_ids': ['gpt3','transformer']\n",
    "    },\n",
    "    's4pp': {\n",
    "        'title': 'S4++: Elevating Long Sequence Modeling with State Memory Reply',\n",
    "        'authors': ['Biqing Qi', 'Junqi Gao', 'Dong Li', 'Kaiyan Zhang', 'Jianxing Liu', 'Ligang Wu', 'Bowen Zhou'],\n",
    "        'venue': 'ICLR 2024 Withdrawn Submission',\n",
    "        'year': 2024,\n",
    "        'url': 'https://openreview.net/forum?id=bdnw4qjfH9',\n",
    "        'abstract': \"Recently, state space models (SSMs) have shown significant performance advantages in modeling long sequences. However, in spite of their promising performance, there still exist limitations. 1. Non-Stable-States (NSS): Significant state variance discrepancies arise among discrete sampling steps, occasionally resulting in divergence. 2. Dependency Bias: The unidirectional state space dependency in SSM impedes the effective modeling of intricate dependencies. In this paper, we conduct theoretical analysis of SSM from the even-triggered control (ETC) theory perspective and first propose the presence of NSS Phenomenon. Our findings indicate that NSS primarily results from the sampling steps, and the integration of multi-state inputs into the current state significantly contributes to the mitigation of NSS. Building upon these theoretical analyses and findings, we propose a simple, yet effective, theoretically grounded State Memory Reply (SMR) mechanism that leverages learnable memories to incorporate multi-state information into the current state. This enables the precise modeling of finer state dependencies within the SSM, resulting in the introduction of S4+. Furthermore, we integrate the complex dependency bias into S4+ via interactive cross attentions mechanism, resulting in the development of S4++. Our extensive experiments in autoregressive language modeling and benchmarking against the Long Range Arena demonstrate superior performance in most post-processing tasks.\",\n",
    "        'seed_ids': ['s4']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for acronym in refs:\n",
    "    if refs[acronym]==[]:\n",
    "        obj=manual_input[acronym]\n",
    "        obj['acronym']=acronym\n",
    "        proj=LibraryReference.from_dict(obj)\n",
    "        proj.save('./tree')\n",
    "        continue   \n",
    "    refdata=refs[acronym]['data']\n",
    "    seed_ids=[]\n",
    "    for ref in refdata:\n",
    "        if 'methodology' in ref['intents']:\n",
    "            if ref['citedPaper']['paperId'] in index['id'].values:\n",
    "                ref_acronym=index.loc[index['id']==ref['citedPaper']['paperId'],'acronym'].values[0]\n",
    "                seed_ids.append(ref_acronym)\n",
    "    title=meta[acronym]['title']\n",
    "    s2id=meta[acronym]['id']\n",
    "    abstract=meta[acronym]['detail']['abstract']\n",
    "    authors=[author['name'] for author in meta[acronym]['detail']['authors']]\n",
    "    if abstract is None:\n",
    "        abstract='N/A'\n",
    "    venue=meta[acronym]['detail']['venue']\n",
    "    if venue is None:\n",
    "        venue='arXiv'\n",
    "    year=meta[acronym]['detail']['year']\n",
    "    if year is None:\n",
    "        year='N/A'\n",
    "    tldr=meta[acronym]['detail']['tldr']\n",
    "    if tldr is None:\n",
    "        tldr='N/A'\n",
    "    else:\n",
    "        tldr=tldr['text']\n",
    "        if tldr is None: tldr='N/A'\n",
    "    embedding=meta[acronym]['detail']['embedding']\n",
    "    if embedding is None:\n",
    "        embedding=[]\n",
    "    else:\n",
    "        embedding=embedding['vector']\n",
    "    citationCount=meta[acronym]['detail']['citationCount']\n",
    "    influentialCitationCount=meta[acronym]['detail']['influentialCitationCount']\n",
    "    paper=LibraryReference(title=title,acronym=acronym,seed_ids=seed_ids,s2id=s2id,abstract=abstract,authors=authors,venue=venue,year=year,tldr=tldr,citationCount=citationCount,influentialCitationCount=influentialCitationCount)\n",
    "    paper.save('./tree')\n",
    "\n",
    "print(len(os.listdir('./tree')))\n",
    "# for i in os.listdir('./tree'):\n",
    "#     if i.split('.')[0] not in refs:\n",
    "#         print(i)\n",
    "#         os.remove(pjoin('./tree',i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 1 hoc impactful cites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 297\n"
     ]
    }
   ],
   "source": [
    "index=pd.read_csv('./library_index.csv')\n",
    "with open('./library_meta.json','r') as f:\n",
    "    meta=json.load(f)\n",
    "print(len(index),len(meta))\n",
    "dir_1hoc='./expanded_tree'\n",
    "get_cite='https://api.semanticscholar.org/graph/v1/paper/{paper_id}/citations?fields=intents,contextsWithIntent,isInfluential,title&offset={offset}&limit=1000'\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=year,authors,tldr,venue,abstract,citationCount,influentialCitationCount,references,embedding.specter_v2,openAccessPdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 297/297 [00:00<00:00, 5657.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cite_1hoc_dir=dir_1hoc+'/cite_1hoc.json'\n",
    "detail_1hoc_dir=dir_1hoc+'/detail_1hoc.json'\n",
    "os.makedirs(dir_1hoc,exist_ok=True)\n",
    "\n",
    "cite_1hoc={}\n",
    "if os.path.exists(cite_1hoc_dir):\n",
    "    with open(cite_1hoc_dir,'r') as f:\n",
    "        cite_1hoc=json.load(f)\n",
    "\n",
    "for acronym in tqdm(index['acronym']):\n",
    "    id=index.loc[index['acronym']==acronym,'id'].values[0]\n",
    "    if pd.isna(id):\n",
    "        cite_1hoc[acronym]=[]\n",
    "        continue\n",
    "    citecount=meta[acronym]['detail']['citationCount']\n",
    "    if acronym in cite_1hoc:\n",
    "        if 'message' not in cite_1hoc[acronym]:\n",
    "            # print('Already done:',acronym,len(cite_1hoc[acronym]),citecount)\n",
    "            continue\n",
    "    cite_1hoc[acronym]=[]\n",
    "    maxoffset=min(citecount,9001)\n",
    "    for offset in range(0,maxoffset,1000):\n",
    "        if offset+1000>=10000:\n",
    "            offset=8999\n",
    "        print(acronym,offset,offset+1000)\n",
    "        cites=requests.get(get_cite.format(paper_id=id,offset=offset)).json()#,headers=headers).json()\n",
    "        if 'message' in cites:\n",
    "            print('Error:',id,cites['message'])\n",
    "            continue\n",
    "        if 'data' not in cites:\n",
    "            print('Error:',cites)\n",
    "            raise\n",
    "        for c in cites['data']:\n",
    "            paperid=c['citingPaper']['paperId']\n",
    "            if paperid in cite_1hoc[acronym]:\n",
    "                continue\n",
    "            if 'methodology' in c['intents'] and c['isInfluential']:\n",
    "                cite_1hoc[acronym].append(paperid)\n",
    "        time.sleep(0.1)\n",
    "    print('Done',acronym,len(cite_1hoc[acronym]),citecount)\n",
    "    with open(cite_1hoc_dir,'w') as f:\n",
    "        json.dump(cite_1hoc,f)\n",
    "\n",
    "# to_remove=[]\n",
    "# for i in cite_1hoc:\n",
    "#     if i not in index['acronym'].values:\n",
    "#         to_remove.append(i)\n",
    "# for i in to_remove:\n",
    "#     cite_1hoc.pop(i)\n",
    "# with open(cite_1hoc_dir,'w') as f:\n",
    "#     json.dump(cite_1hoc,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6856 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/6856 [00:01<13:08,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 3c5ced872aa5d16bf3aa59c40931ed85b54cb3e5 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/6856 [00:02<13:06,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 7df2a6f12ecfee64aedb6236102e133b9f02264e Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 48/6856 [00:03<06:17, 18.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 5978b57d019f9054d11ec23e7c27364844eb2f31 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 50/6856 [00:04<09:46, 11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 241ce066dfe79b074912a423213310c811b6abe4 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 55/6856 [00:05<12:20,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 82f0116fe46dd427685c578d4ef91b49d6ee7cb1 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 58/6856 [00:06<16:02,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: c792c97fdb2d1b16846817b1a95fe2e0f4841586 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 60/6856 [00:07<20:58,  5.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 205da343bfbd5928b8155d05500589d6bce435d4 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 61/6856 [00:08<28:26,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: b5d26362c1c95c0fcc7d5060e27fbb2ce871e81f Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 104/6856 [00:09<07:09, 15.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: a79c67e5ecddc025ccf705c7b7a40e8844f83a6a Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 106/6856 [00:10<09:48, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: e308bd5b5fbc0d916bf53c1138acc5ae4e0837a0 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 108/6856 [00:11<13:12,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 735e95cf11d660ae099277f6e368431f38865911 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 111/6856 [00:12<16:33,  6.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 8114c2ececb5d6fd960bbb152eff9fed37e920a3 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 112/6856 [00:13<22:24,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 8444b6982b5fa799e016e20f833594bb2f9ab12e Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 120/6856 [00:14<19:12,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 54bc257c802847bbd8ecadc7fe50f883e193b9f2 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 140/6856 [00:15<11:19,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 98b264884e12e30b17285046f40224a427f75499 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 141/6856 [00:16<15:29,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4240fa51b58a07ce8ffd283298b57f171f1584fc Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 142/6856 [00:17<21:01,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 455f711555e066d405ac57245ae7e63a67548918 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 143/6856 [00:18<27:58,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 78dae567bd35ebc41faa75700f90e72e50eb615a Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 144/6856 [00:19<36:14,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 6c4e682f1f4c0da9d69ead2f21f2d475622949db Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 145/6856 [00:20<45:33,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1e5d04206ae3fc2ecdc7c0b00418fcbad3b8c898 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 146/6856 [00:21<55:31,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 5a934623068ebed6b72995d142d7dc96073e78fa Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 147/6856 [00:22<1:05:56,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 580aa5082617dde7bde2e0a9a9122badca77fee9 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 148/6856 [00:23<1:15:50,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 9668af6b4c770a84cfcc30b94ecd10fb9acda796 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 149/6856 [00:24<1:24:47,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 173bd3b71792bf891ab5af4a6454e6ba07576bcd Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 150/6856 [00:25<1:32:18,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 10d3e2b5d7b17f6eab363100ae33f62c7820ac34 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 152/6856 [00:26<1:18:12,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 50ae1d44ec10a8a981f5c6bc2c6a2674bf9a48d3 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 153/6856 [00:28<1:26:47,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1c76caee0fd659253a042e91f87ea85b26a29594 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 154/6856 [00:29<1:33:30,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 37735a8a1c0b60af4449e7367f92e9740a10f03d Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 156/6856 [00:30<1:19:03,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 8419b1865eeecb496b96d2ab8e8e39c813bb595a Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 157/6856 [00:31<1:27:17,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4853df65daa4c220a57b86960d04c2d1d0bc227a Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 158/6856 [00:32<1:34:21,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: a405b19f43f2e461e8e3b5681b3890b51ce1113f Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 159/6856 [00:33<1:39:59,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ddfc1bc4b69dc6de3d4aab10593825875135e029 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 160/6856 [00:34<1:44:14,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: f903b52dba21c112670ba5fbcb29005beb19bbff Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 161/6856 [00:35<1:47:02,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 59d69a8005a7474d55091bf384be8a9c9c339bdb Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 162/6856 [00:36<1:49:22,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: b525ca9f3c0c64b004774e81ad55d306dc4eda52 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 163/6856 [00:37<1:50:52,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: f7bc5be4cc305f70b36456a959d7b95669b48ac2 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 165/6856 [00:38<1:26:34,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 636e8a520d15e2c56bffd67ae4070db80df52bb3 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 166/6856 [00:39<1:33:18,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: aada3709412cfbc57839aac2121f3047ededcadc Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 167/6856 [00:40<1:38:51,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 9810718f3591a73589b3b74fa16ed04aaf83ae42 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 168/6856 [00:41<1:43:30,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 75c0019b1c4bec220f4750d7f6fc5da6030e1d5e Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 169/6856 [00:42<1:46:32,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1e8b76a27cc57793c09fff0be151532070f800e7 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 170/6856 [00:43<1:48:44,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 2bdd26006fbd9b78a5b988268513f07a00308aac Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 171/6856 [00:44<1:50:52,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: e6b7bb81c863235ffe22f25b316b6270080d28e4 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 172/6856 [00:45<1:52:45,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 6dc23db46d1f9f410c96812bed499f10ae19a926 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 173/6856 [00:46<1:53:14,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: b72a8884a7a2f93d60d44930dd77d0af85dd32b8 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 174/6856 [00:47<1:53:39,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 8075cefca86196a8ee561b1bcff277fb3dd2c4f8 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 175/6856 [00:48<1:54:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0430f7e17a51c43bd90822751ab32fb3fd542bd6 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 176/6856 [00:49<1:54:18,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 547fbeebdb4da444a1631fa26a4fcbefa506941d Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 177/6856 [00:50<1:54:32,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: ef8cfbfbcddeac0735c522de10cef769b783a0a6 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 178/6856 [00:51<1:54:36,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 9222daef0cf084f3d254f526c68253b9220e4ae8 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 179/6856 [00:52<1:55:28,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: abfab6468fc589ce65f5c065ed29d7a2da83cb73 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 180/6856 [00:53<1:55:13,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 4a0d00220659b0e569defb0d99f33a48b319bda8 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 181/6856 [00:54<1:54:56,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 11ac0b5634a282f1a0da204b98e7473d8b480dfb Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 182/6856 [00:55<1:54:47,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: c926accb88cce2ac64f9292c5a5bbcff26189bb6 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 183/6856 [00:56<1:54:38,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 31947145652dd384fbc8b2abef02779bb95c7a72 Too Many Requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 222/6856 [01:43<2:14:43,  1.22s/it]"
     ]
    }
   ],
   "source": [
    "detail_1hoc={}\n",
    "if os.path.exists(detail_1hoc_dir):\n",
    "    with open(detail_1hoc_dir,'r') as f:\n",
    "        detail_1hoc=json.load(f)\n",
    "\n",
    "index_1hoc={}\n",
    "for i in cite_1hoc:\n",
    "    for j in cite_1hoc[i]:\n",
    "        index_1hoc[j]={}\n",
    "\n",
    "paper_detail='https://api.semanticscholar.org/graph/v1/paper/{paper_id}?fields=year,authors,tldr,venue,abstract,citationCount,influentialCitationCount,references,embedding.specter_v2,openAccessPdf'\n",
    "\n",
    "cnt=0\n",
    "save_every=10\n",
    "for c in tqdm(index_1hoc):\n",
    "    if c in detail_1hoc:\n",
    "        if 'message' not in detail_1hoc[c]:\n",
    "            continue\n",
    "    detail=requests.get(paper_detail.format(paper_id=c)).json()#,headers=headers).json()\n",
    "    time.sleep(1)\n",
    "    if 'message' in detail:\n",
    "        print('Error:',c,detail['message'])\n",
    "        continue\n",
    "    detail_1hoc[c]=detail\n",
    "    if cnt%save_every==0:\n",
    "        with open(detail_1hoc_dir,'w') as f:\n",
    "            json.dump(detail_1hoc,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
