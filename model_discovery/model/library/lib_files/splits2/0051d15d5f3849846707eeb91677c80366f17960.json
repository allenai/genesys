{
    "0051d15d5f3849846707eeb91677c80366f17960-0": "Near Input Sparsity Time Kernel Embeddings via Adaptive Sampling\n\nDavid P. Woodruff Amir Zandieh\n\nAbstract\n\nTo accelerate kernel methods, we propose a near input sparsity time algorithm for sampling the high-dimensional feature space implicitly defined by a kernel transformation. Our main contribution is an importance sampling method for subsampling the feature space of a degree tensoring of data points in almost input sparsity time, improving the recent oblivious sketching method of (Ahle et al., 2020) by a factor of . This leads to a subspace embedding for the polynomial kernel, as well as the Gaussian kernel, with a target dimension that is only linearly dependent on the statistical dimension of the kernel and in time which is only linearly dependent on the sparsity of the input dataset. We show how our subspace embedding bounds imply new statistical guarantees for kernel ridge regression. Furthermore, we empirically show that in large-scale regression tasks, our algorithm outperforms state-of-the-art kernel approximation methods. Machine Learning, ICML\n\n1 Introduction\n\nKernel methods provide a simple, yet powerful framework for applying non-parametric modeling techniques to a number of important problems in statistics and machine learning, such as kernel ridge regression, SVM, PCA, CCA, etc. While kernel methods are statistically well understood and perform well empirically, they often pose scalability challenges as they operate on the kernel matrix (Gram matrix) of the data, whose size scales quadratically in the size of the training dataset. Primitives such as kernel PCA or kernel ridge regression generally take a prohibitively large quadratic amount of space and at least quadratic time. Thus, much work has focused on scaling up kernel methods by producing compressed and low-rank approximations to kernel matrices (Rahimi & Recht, 2008; Alaoui & Mahoney, 2015; Avron et al., 2017a; Musco & Musco, 2017; Avron et al., 2017b, 2014; Ahle et al., 2020; Zandieh et al., 2020). 1.1 Problem Definition\n\nFor a given kernel function and a dataset of -dimensional vectors , let be the kernel matrix corresponding to this dataset defined as for every . A classical solution for scaling up kernel methods is via kernel low-rank approximation, where one seeks to find a low-rank matrix such that can serve as a proxy to the kernel matrix . In order to obtain statistical and algorithmic guarantees for downstream kernel-based learning applications, such as kernel regression, PCR, CCA, etc., one needs to have spectral approximation bounds on the entire surrogate kernel matrix. Formally, for given we need to be an -spectral approximation to the kernel matrix , meaning that has to satisfy,\n\nK + \u03bb \u200b I 1 + \u03f5 \u2aaf Z \u22a4 \u200b Z + \u03bb \u200b I \u2aaf K + \u03bb \u200b I 1 \u2212 \u03f5 . precedes-or-equals \ud835\udc3e \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 superscript \ud835\udc4d top \ud835\udc4d \ud835\udf06 \ud835\udc3c precedes-or-equals \ud835\udc3e \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 \\frac{K+\\lambda I}{1+\\epsilon}\\preceq Z^{\\top}Z+\\lambda I\\preceq\\frac{K+\\lambda I}{1-\\epsilon}. (1)\n\nIntuitively, if is much larger than the operator norm of then is a good solution that satisfies (1). On the other hand if , then the target dimension has to be at least equal to the rank of . In general, the statistical dimension (or effective dimension) captures this tradeoff, defined as , where the are the eigenvalues of . The goal is to find a matrix with a target dimension which depends only linearly on , using a runtime that is nearly equal to the number of non-zero entries (i.e., the sparsity) of the input dataset, denoted by . The main motivation of this paper is the following:\n\nP : Given a dataset , and a kernel function , if is the kernel matrix corresponding to this dataset with statistical dimension , can we compute a matrix with , using runtime, such that is an -spectral approximation to as per (1)? The runtime that (P) is asking for requires the terms to be decoupled from the input sparsity, . Hence, up to low order terms, we aim for a runtime which only depends linearly on the sparsity of the input dataset. We address (P) for two important kernel classes: the degree- polynomial kernel for some , and the Gaussian kernel . We also remark that, as we will later discuss in Section 3.3, our method is very general and can be applied to the class of dot-product kernels. As we will discuss in the related work section, all prior methods for approximating the polynomial kernel achieve a runtime of either the form or , and similarly all prior results for the Gaussian kernel achieve a runtime of either or , where is the radius of the input dataset. These are strictly worse than the target runtime of (P). 1.2 Our Results\n\nWe answer problem (P) in the affirmative by designing near input sparsity time algorithms for embedding the polynomial and Gaussian kernels. Our main result for the polynomial kernel is given in the following theorem. Theorem 1. For any dataset , any and any positive integer , if is the degree- polynomial kernel matrix corresponding to this dataset () with statistical dimension and , then there exists an algorithm that computes a matrix , with target dimension such that, with high probability, is an -spectral approximation to as in (1) using time. We also address for approximating the Gaussian kernel by proving the following theorem. Theorem 2. For any dataset such that for all , any , if is the Gaussian kernel matrix corresponding to this dataset () with statistical dimension , then there exists an algorithm that computes a matrix , with target dimension such that, with high probability, is an -spectral approximation to as in (1) using time. Theorems 1 and 2 imply accelerated algorithms for kernel ridge regression (KRR) with improved statistical and algorithmic guarantees. We analyze the empirical risk of our sampling algorithm for the KRR problem in Appendix H. Furthermore, in the experiments section we evaluate our approximate KRR method on various standard large-scale regression datasets and empirically show that our method competes favorably with the state-of-the-art, including Nystrom (Musco & Musco, 2017) and Fourier features methods (Rahimi & Recht, 2008), as well as the oblivious sketching of (Ahle et al., 2020). We show that our method achieves better testing error and smaller runtime on large datasets with more than half a million training examples. Additional downstream learning applications: While we focus on KRR here, we remark that spectral approximation bounds form the basis of analyzing sketching methods for tasks including kernel low-rank approximation, PCA, CCA, k-means and many more. In the kernelized setting, such bounds have been analyzed, without regularization, for the polynomial kernel (Avron et al., 2014). It is shown in (Cohen et al., 2017) that (1) along with a trace condition on (which holds for the sampling approaches we consider) yields a so-called projection-cost preservation condition. With chosen appropriately, this condition ensures that can serve as a proxy for for approximately solving kernel k-means and for certain versions of kernel PCA and kernel CCA. See (Musco & Musco, 2017) for details, where this analysis is carried out for the Nystrom method. 1.3 Prior Work\n\nA popular approach for accelerating kernel methods is based on Nystrom sampling. We refer the reader to the work of (Musco & Musco, 2017) and the references therein. By recursively sampling Nystrom landmarks according to the so-called ridge leverage score distribution, Musco & Musco (2017) prove that for any kernel with statistical dimension , there exists an algorithm that outputs a matrix with which satisfies the spectral approximation guarantee of (1) with high probability, using runtime. However, the leading term in the time complexity of this method is , which unsatisfactorily depends on and also depends linearly on . Hence, for both the polynomial and Gaussian kernels our Theorems 1 and 2 improve on the runtime of this method by a factor of . Another popular line of work on kernel approximation problems is the Fourier features method of Rahimi & Recht (2008). It is proved in (Avron et al., 2017b) that this method can achieve spectral approximation guarantees for the Gaussian kernel using a sub-optimal number of samples and runtime. This sample complexity is substantially larger than our result in Theorem 2. Furthermore we improve the runtime of this method by a factor of . However, (Avron et al., 2017b) show that this method can be modified to achieve a sample complexity of using a runtime of . For constant dimensional datasets (constant ) the number of samples that (Avron et al., 2017b) achieve is comparable to our target dimension in Theorem 2 but it deteriorates exponentially with the dimension . Furthermore, the runtime of this method is substantially larger than our runtime by a factor of . In the linear sketching literature, (Avron et al., 2014) proposed an oblivious subspace embedding for the polynomial kernel based on the TensorSketch of (Pham & Pagh, 2013). They applied this method to a wide array of kernel problems, including PCA, PCR, and CCA.",
    "0051d15d5f3849846707eeb91677c80366f17960-1": "The runtime of this method, while nearly linear in , scales exponentially in the degree of the polynomial kernel. Their runtime for the degree- polynomial kernel is , which has an unsatisfactory term. Recently, (Ahle et al., 2020) proposed a new oblivious sketching solution for the polynomial kernel that improves the exponential dependence of TensorSketch on to polynomial. Ahle et al. (2020) gave an algorithm that outputs a matrix with which satisfies the spectral approximation guarantee of (1) with high probability. Their algorithm has runtime111 notation hides factors.. This runtime has an undesirable inverse polynomial dependence on and scales sub-optimally with the degree of the polynomial kernel as . Our Theorem 1 improves the runtime of (Ahle et al., 2020) by an factor. Moreover, they showed that their sketch for the polynomial kernel leads to an efficient oblivious sketch for the Gaussian kernel on bounded datasets. Ahle et al. (2020) gave an algorithm that for any dataset with radius , computes a matrix with which spectrally approximates the Gaussian kernel matrix corresponding to this dataset as in (1) with high probability. This was the first result that resolved the curse of dimensionality for embedding the high dimensional Gaussian kernel. The algorithm has runtime, which unsatisfactorily depends on and scales poorly as a function of the dataset\u2019s radius as . Our Theorem 2 improves this runtime by a factor of . 1.4 Our Techniques\n\nOur method relies on the fact that any kernel function defines a lifting such that the kernel function computes the inner product between the lifted data points, i.e., . Therefore, any kernel matrix can be decomposed as where is a matrix with columns whose columns are the lifted data points . Our approach is to design an importance sampling matrix such that satisfies the spectral approximation guarantee of (1). Our algorithm generates a sampling matrix that samples a small number of rows of using a recursive leverage score sampling technique, which has been extensively applied to various algorithmic problems in the literature (Kapralov et al., 2014; Alaoui & Mahoney, 2015; Cohen et al., 2016; Musco & Musco, 2017; Avron et al., 2017b; Cohen et al., 2017). Our main novelty is in generating a sample from the leverage score distribution without ever forming the entire distribution explicitly, as the support size of this distribution is equal to the number of rows of which is typically high (even infinite). For the polynomial kernel, the lifting matrix is , where is a matrix whose columns are obtained by a -fold self-tensoring of the columns of the dataset matrix (see Section 2 for notation). After multiple reductions, our importance sampling problem boils down to performing -sampling on a vector of the form , where is an arbitrary vector in . Here by -sampling of a vector, we mean sampling a coordinate proportional to its squared value. We design a primitive that can generate a sample with probability proportional to the squared value of the entry of the vector using roughly time. Our algorithm relies on the fact that, by reshaping, entries of the vector are in bijective correspondence with entries of the matrix , where is a diagonal matrix whose diagonal entries are the elements of . Therefore, our importance sampling amounts to sampling an element of with probability proportional to the square of its absolute value. We do this by first sampling a column of this matrix with probability proportional to its squared norm, and then sampling a row with probability proportional to the squares of the entries of the sampled column. After sampling a column of the matrix , we next perform -sampling on the column of the mentioned matrix, which is in the form of , where . One can see that we have made progress and now it is enough to iterate in this fashion by performing -sampling on . However, note that has rows, and hence, computing its column norms is prohibitively expensive. We tackle this issue by sketching the columns of using the sketch introduced in (Ahle et al., 2020), which is able to preserve the column norms up to a small error and with runtime roughly . Our algorithm is actually more involved and includes extra dimensionality reduction steps. In the paragraph above we explained how to generate a single sample with the right distribution, but in order to obtain the spectral approximation guarantee of (1) we need to generate such samples. It is crucial that our runtime does not lose a multiplicative factor of . We heavily exploit the structure of tensor products to reuse most computations and generate samples in time proportional to . Moreover, to spectrally approximate the Gaussian kernel, we adapt our sampling algorithm to a truncated Taylor expansion of the Gaussian kernel. Furthermore, in Section 3.3 we discuss how our method can be generalized to any dot-product kernel. 2 Preliminaries\n\nThroughout the paper, for any matrices and , denotes the vertical concatenation of and , i.e., . Moreover, denotes the vertical tensor product of and . The rows of are indexed by where and and for any , . We also use to denote, . For any matrix we use to denote its row and we use to refer to its column. Also for any set , denotes a sub-matrix of that includes rows of . 3 Algorithm and Analysis\n\nLet be the feature matrix whose columns are the projections of the data points in the feature space. We start by presenting a recursive importance sampling algorithm that efficiently computes a matrix which satisfies the spectral approximation guarantee of (1) for the kernel . Sampling rows of with probabilities proportional to the squared row norms of the matrix , which are known as the ridge leverage scores of , is an efficient sampling strategy for obtaining the spectral approximation guarantee of (1). In Algorithm 1, we give a generic recursive method for performing approximate leverage score sampling on any matrix . The recursive procedure works by generating samples from a crude approximation to the leverage scores and iteratively refining the sampling distribution. We first introduce the definition of a row norm sampler as follows,\n\nDefinition 3.1 (Row Norm Sampler). Let be a matrix with rows . For any probability distribution that satisfies for all , and any positive integer , a rank- row norm sampler for matrix is a random matrix which is constructed by generating i.i.d. samples with distribution and letting the row of be for every , where are the standard basis vectors in . Now we are ready to prove the correctness of Algorithm 1,\n\nLemma 3. Suppose that for any matrices and , any , and any positive integer , the primitive RowSampler returns a rank- row norm sampler for matrix as in Definition 3.1. Then for any matrix with statistical dimension , any , any , Algorithm 1 returns a sampling matrix with such that with probability ,\n\n\u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 + \u03f5 \u2aaf \u03a6 \u22a4 \u200b \u03a0 \u22a4 \u200b \u03a0 \u200b \u03a6 + \u03bb \u200b I \u2aaf \u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 \u2212 \u03f5 .",
    "0051d15d5f3849846707eeb91677c80366f17960-2": "precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 superscript \u03a6 top superscript \u03a0 top \u03a0 \u03a6 \ud835\udf06 \ud835\udc3c precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 \\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1+\\epsilon}\\preceq\\Phi^{\\top}\\Pi^{\\top}\\Pi\\Phi+\\lambda I\\preceq\\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1-\\epsilon}. The proof of this lemma is included in Appendix C. 3.1 Adaptive Sampling for the Polynomial Kernel\n\nThe polynomial kernel of degree is defined as . Using the definition of tensor products, one can see that , where and are -fold self tensor products of vectors and , respectively. Suppose is the dataset matrix. The polynomial kernel matrix can be decomposed as , where is a matrix whose columns are obtained by the -fold self tensoring of the columns of . The goal is to apply the iterative leverage score sampling of Algorithm 1 to the feature matrix in nearly time. Note that the matrix has a large number of rows so even assuming that an oracle gives us the leverage score distribution of for free, just reading this distribution takes time. We show how to generate samples from the right distribution quickly. Algorithm 1 crucially uses the primitive RowSampler, which carries out the main computations of our proposed algorithm. This primitive performs row norm sampling (see Definition 3.1) on a matrix of the form , for any matrix , very efficiently. 3.1.1 RowSampler for the Polynomial kernel\n\nAn important technical contribution of this work is an efficient algorithm that can perform row norm sampling (see Definition 3.1) on a matrix of the form using nearly runtime, where and . Our primitive uses the sketch which was proposed in (Ahle et al., 2020) to preserve the norm of vectors in and sketch vectors of the form quickly. The next lemma follows from Theorem 1.2 of (Ahle et al., 2020),\n\nLemma 4. For every positive integers , every , and every , there exists a distribution on random matrices with such that, for any . Moreover, for any , the total time to compute for all is , where is the standard basis vector along the first coordinate. We prove this lemma in Appendix D. Now we are ready to design the procedure RowSampler to perform row norm sampling on matrices of the form . Overview of Algorithm 2: The goal is to generate a sample with probability proportional to the squared norm of the row of the matrix . Because the matrix is of a large size, we seek to compress it without perturbing the row norm distribution of . This can be done by applying a JL-transformation to the rows of this matrix (see, e.g., (Dasgupta & Gupta, 2003; Kane & Nelson, 2014)). Let be a random matrix with i.i.d. normal entries with . Then with probability the norm of each row of the matrix will be preserved up to a factor and hence by a union bound, with high probability all row norms of are within a factor of the row norms of the original matrix. This is done in line 2 of the algorithm by computing the matrix , which can be done quickly since is a low rank matrix and has few columns. Now the problem is reduced to performing row norm sampling on . In order to generate a sample with distribution proportional to the squares of the row norms of we can first sample a column of this matrix with probability proportional to the squared column norms and then generate a row index with probability proportional to the squared values of the entries of the selected column. This process generates a random index with our desired distribution. Computing the exact column norms of is too expensive as this matrix has rows, but if we apply the sketch from Lemma 4, we can compress the rows while preserving the column norms, in near input sparsity time, up to small error. So, it is enough to sample a column with probability proportional to the squared column norms of , which is done in lines 3-7 of the algorithm. Given that the column of was sampled, all we need to do is sample an entry of with probability proportional to the squared values of its entries. Note that forming this vector is out of the question since it has coordinates. By basic properties of tensor products, the entries of are in bijective correspondence with the entries of the matrix , where entry of is equal to the entry at row and column of . Therefore, it is enough to sample an entry of the matrix with probability proportional to its squared value. To this end, we first sample a row of this matrix with probability proportional to the squared row norms, and then sample a column by performing -sampling on the sampled row. Since has a large number of columns, we first sketch the rows of this matrix, incurring only a factor perturbation to the row norms, and then perform row norm sampling on the sketched matrix. Now we have an index sampled from the right distribution and all that is left to do is to carry out -sampling on the vector . Note that we have made progress because this vector has size and we have reduced the size by a factor of . We recursively repeat this process of reshaping the tensor product to a matrix and sampling a row of the matrix times until having all indices . Algorithm 2 does this. Note that the actual procedure requires more work because we need to generate i.i.d. samples with the row norm distribution. To ensure that our runtime does not lose a multiplicative factor of , resulting in total time, we need to do extra sketching and a random partitioning of the rows of the matrix to buckets. The formal guarantee on Algorithm 2 is given in the following lemma. Lemma 5. For any matrices and , any and any positive integers , with high probability, Algorithm 2 outputs a ranks- row norm sampler for (Definition 3.1) in time . Proof. All rows of the sampling matrix (output of Algorithm 2) have independent and identical distributions because the algorithm generates i.i.d. samples in line 7 and then for each , the row of the matrix is constructed by sampling in line 18 from a distribution that is solely determined by and is independent of the values of for . Since every row of is identically distributed, let us consider the distribution of the row of for some arbitrary . Let be a random variable that takes values in with probability distribution defined in line 6 of Algorithm 2. The random index generated in line 7 of the algorithm is a copy of the random variable . For any , let be a vector-valued random variable that takes values in with the following conditional probability distribution for every ,\n\nPr \u2061 [ I a j = i | I 1 j = i 1 , I 2 j = i 2 , \u22ef \u200b I a \u2212 1 j = i a \u2212 1 ] Pr subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e conditional \ud835\udc56 subscript superscript \ud835\udc3c \ud835\udc57 1 subscript \ud835\udc56 1 subscript superscript \ud835\udc3c \ud835\udc57 2 subscript \ud835\udc56 2 \u22ef subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e 1 subscript \ud835\udc56 \ud835\udc4e 1 \\displaystyle\\Pr[I^{j}_{a}=i|I^{j}_{1}=i_{1},I^{j}_{2}=i_{2},\\cdots I^{j}_{a-1}=i_{a-1}] = \u2016 W h \u200b ( i ) \u22c5 D j , a \u2212 1 \u200b P a \u22a4 \u2016 F 2 \u2211 t = 1 s \u2032 \u2016 W t \u200b D j , a \u2212 1 \u200b P a \u22a4 \u2016 F 2 \u200b \u2016 X i , \u22c6 \u200b D j , a \u2212 1 \u200b P a \u22a4 \u2016 2 2 \u2016 X h \u2212 1 \u200b ( h \u200b ( i ) ) , \u22c6 \u200b D j , a \u2212 1 \u200b P a \u22a4 \u2016 F 2 , absent superscript subscript norm \u22c5 subscript \ud835\udc4a \u210e \ud835\udc56 superscript \ud835\udc37 \ud835\udc57 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e top \ud835\udc39 2 superscript subscript \ud835\udc61 1 superscript \ud835\udc60 \u2032 superscript subscript norm subscript \ud835\udc4a \ud835\udc61 superscript \ud835\udc37 \ud835\udc57 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e top \ud835\udc39 2 superscript subscript norm subscript \ud835\udc4b \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc57 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e top 2 2 superscript subscript norm subscript \ud835\udc4b superscript \u210e 1 \u210e \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc57 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e top \ud835\udc39 2 \\displaystyle=\\frac{\\|W_{h(i)}\\cdot D^{j,a-1}P_{a}^{\\top}\\|_{F}^{2}}{\\sum_{t=1}^{s^{\\prime}}\\|W_{t}D^{j,a-1}P_{a}^{\\top}\\|_{F}^{2}}\\frac{\\|X_{i,\\star}D^{j,a-1}P_{a}^{\\top}\\|_{2}^{2}}{\\|X_{h^{-1}(h(i)),\\star}D^{j,a-1}P_{a}^{\\top}\\|_{F}^{2}},\n\nwhere for every are the matrices defined in line 11 of the algorithm and is a diagonal matrix of size whose diagonal entries are , for every and . For ease of notation we drop the superscript and just write . One can verify that the vector random variable obtained by stitching together the random indices generated in line 18 of the algorithm, is a copy of the random variable . Let be the quantity that the for loop in lines 21-30 of the algorithm computes. If are the indices sampled in line 18 of the algorithm, then the value of can be computed as, , where and are the quantities computed in lines 25 and 26 of the algorithm. Hence, for any , the distribution of is,\n\nPr \u2061 [ S l , \u22c6 = \u03b2 \u2212 1 / 2 \u200b ( \ud835\udc1e i 1 \u2297 \ud835\udc1e i 2 \u2297 \u22ef \u200b \ud835\udc1e i q ) \u22a4 ] Pr subscript \ud835\udc46 \ud835\udc59 \u22c6 superscript \ud835\udefd 1 2 superscript tensor-product subscript \ud835\udc1e subscript \ud835\udc56 1 subscript \ud835\udc1e subscript \ud835\udc56 2 \u22ef subscript \ud835\udc1e subscript \ud835\udc56 \ud835\udc5e top \\displaystyle\\Pr\\left[S_{l,\\star}=\\beta^{-1/2}({\\bf e}_{i_{1}}\\otimes{\\bf e}_{i_{2}}\\otimes\\cdots{\\bf e}_{i_{q}})^{\\top}\\right] = \u2211 j = 1 d \u2032 Pr \u2061 [ S l , \u22c6 = \u03b2 \u2212 1 / 2 \u200b ( \ud835\udc1e i 1 \u2297 \u22ef \u200b \ud835\udc1e i q ) \u22a4 | J = j ] \u22c5 p j absent superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 \u22c5 Pr subscript \ud835\udc46 \ud835\udc59 \u22c6 conditional superscript \ud835\udefd 1 2 superscript tensor-product subscript \ud835\udc1e subscript \ud835\udc56 1 \u22ef subscript \ud835\udc1e subscript \ud835\udc56 \ud835\udc5e top \ud835\udc3d \ud835\udc57 subscript \ud835\udc5d \ud835\udc57 \\displaystyle=\\sum_{j=1}^{d^{\\prime}}\\Pr\\left[\\left.S_{l,\\star}=\\beta^{-1/2}({\\bf e}_{i_{1}}\\otimes\\cdots{\\bf e}_{i_{q}})^{\\top}\\right|J=j\\right]\\cdot p_{j} = \u2211 j = 1 d \u2032 Pr \u2061 [ I j = ( i 1 , i 2 , \u22ef \u200b i q ) ] \u22c5 p j . absent superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 \u22c5 Pr superscript \ud835\udc3c \ud835\udc57 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e subscript \ud835\udc5d \ud835\udc57 \\displaystyle=\\sum_{j=1}^{d^{\\prime}}\\Pr\\left[I^{j}=(i_{1},i_{2},\\cdots i_{q})\\right]\\cdot p_{j}. (2)\n\nBy the law of total probability, we have , and therefore, because , we find that\n\nPr \u2061 [ S l , \u22c6 = \u03b2 \u2212 1 / 2 \u200b ( \ud835\udc1e i 1 \u2297 \ud835\udc1e i 2 \u2297 \u22ef \u200b \ud835\udc1e i q ) \u22a4 ] = \u03b2 s . Pr subscript \ud835\udc46 \ud835\udc59 \u22c6 superscript \ud835\udefd 1 2 superscript tensor-product subscript \ud835\udc1e subscript \ud835\udc56 1 subscript \ud835\udc1e subscript \ud835\udc56 2 \u22ef subscript \ud835\udc1e subscript \ud835\udc56 \ud835\udc5e top \ud835\udefd \ud835\udc60 \\Pr\\left[S_{l,\\star}=\\beta^{-1/2}({\\bf e}_{i_{1}}\\otimes{\\bf e}_{i_{2}}\\otimes\\cdots{\\bf e}_{i_{q}})^{\\top}\\right]=\\frac{\\beta}{s}.",
    "0051d15d5f3849846707eeb91677c80366f17960-3": "Now note that for any , is defined as where is a matrix with i.i.d. normal entries with rows. Therefore, is a JL-transform and for every , with high probability, i.e.,\n\n\u2016 W r \u200b D a \u2212 1 \u200b P a \u22a4 \u2016 F 2 n \u2032 \u2208 \u2016 X h \u2212 1 \u200b ( r ) , \u22c6 \u200b D a \u2212 1 \u200b P a \u22a4 \u2016 F 2 1 \u00b1 1 / 10 \u200b q . superscript subscript norm subscript \ud835\udc4a \ud835\udc5f superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e top \ud835\udc39 2 superscript \ud835\udc5b \u2032 superscript subscript norm subscript \ud835\udc4b superscript \u210e 1 \ud835\udc5f \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e top \ud835\udc39 2 plus-or-minus 1 1 10 \ud835\udc5e \\frac{\\|W_{r}D^{a-1}P_{a}^{\\top}\\|_{F}^{2}}{n^{\\prime}}\\in\\frac{\\|X_{h^{-1}(r),\\star}D^{a-1}P_{a}^{\\top}\\|_{F}^{2}}{1\\pm{1}/{10q}}. (3)\n\nFor a simple proof of (3), see (Dasgupta & Gupta, 2003) (see also (Kane & Nelson, 2014) for a more efficient version). By union bounding over events, (3) holds simultaneously for all , , and with high probability. We condition on (3) holding in what follows. We can bound the conditional probability of as follows,\n\nPr \u2061 [ I a j = i | I 1 j = i 1 , I 2 j = i 2 , \u22ef \u200b I a \u2212 1 j = i a \u2212 1 ] Pr subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e conditional \ud835\udc56 subscript superscript \ud835\udc3c \ud835\udc57 1 subscript \ud835\udc56 1 subscript superscript \ud835\udc3c \ud835\udc57 2 subscript \ud835\udc56 2 \u22ef subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e 1 subscript \ud835\udc56 \ud835\udc4e 1 \\displaystyle\\Pr[I^{j}_{a}=i|I^{j}_{1}=i_{1},I^{j}_{2}=i_{2},\\cdots I^{j}_{a-1}=i_{a-1}] \u2265 ( 1 \u2212 1 / 5 \u200b q ) \u22c5 \u2016 X i , \u22c6 \u200b D a \u2212 1 \u200b P a \u22a4 \u2016 2 2 \u2016 X \u200b D a \u2212 1 \u200b P a \u22a4 \u2016 F 2 . absent \u22c5 1 1 5 \ud835\udc5e superscript subscript norm subscript \ud835\udc4b \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e top 2 2 superscript subscript norm \ud835\udc4b superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e top \ud835\udc39 2 \\displaystyle\\qquad\\geq\\left(1-{1}/{5q}\\right)\\cdot\\frac{\\|X_{i,\\star}D^{a-1}P_{a}^{\\top}\\|_{2}^{2}}{\\|XD^{a-1}P_{a}^{\\top}\\|_{F}^{2}}. (4)\n\nFor every , line 4 of the algorithm computes , where is the sketch from Lemma 4 with . By basic properties of tensor products, for every ,\n\nP a \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 subscript \ud835\udc43 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top \\displaystyle P_{a}D^{a-1}X_{i,\\star}^{\\top} = Q q \u200b ( X \u2297 ( q \u2212 a ) \u2297 E 1 \u2297 a ) \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 absent superscript \ud835\udc44 \ud835\udc5e tensor-product superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e superscript subscript \ud835\udc38 1 tensor-product absent \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top \\displaystyle=Q^{q}\\left(X^{\\otimes(q-a)}\\otimes E_{1}^{\\otimes a}\\right)D^{a-1}X_{i,\\star}^{\\top} = Q q \u200b ( ( X \u2297 ( q \u2212 a ) \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 ) \u2297 \ud835\udc1e 1 \u2297 a ) . absent superscript \ud835\udc44 \ud835\udc5e tensor-product superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top superscript subscript \ud835\udc1e 1 tensor-product absent \ud835\udc4e \\displaystyle=Q^{q}\\left(\\left(X^{\\otimes(q-a)}D^{a-1}X_{i,\\star}^{\\top}\\right)\\otimes{\\bf e}_{1}^{\\otimes a}\\right). Hence, by Lemma 4, for every and every , with high probability,\n\n\u2016 X i , \u22c6 \u200b D a \u2212 1 \u200b P a \u22a4 \u2016 2 2 \u2208 \u2016 X \u2297 ( q \u2212 a ) \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 \u2016 2 2 1 \u00b1 0.1 / q . superscript subscript norm subscript \ud835\udc4b \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e top 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top 2 2 plus-or-minus 1 0.1 \ud835\udc5e \\left\\|X_{i,\\star}D^{a-1}P_{a}^{\\top}\\right\\|_{2}^{2}\\in\\frac{\\left\\|X^{\\otimes(q-a)}D^{a-1}X_{i,\\star}^{\\top}\\right\\|_{2}^{2}}{1\\pm 0.1/q}. (5)\n\nBy union bounding over events, with high probability, (5) holds simultaneously for all , all , and all . Therefore, conditioning on (5) holding and using (4), the conditional probability of satisfies\n\nPr \u2061 [ I a j = i | I 1 j = i 1 , I 2 j = i 2 , \u22ef \u200b I a \u2212 1 j = i a \u2212 1 ] Pr subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e conditional \ud835\udc56 subscript superscript \ud835\udc3c \ud835\udc57 1 subscript \ud835\udc56 1 subscript superscript \ud835\udc3c \ud835\udc57 2 subscript \ud835\udc56 2 \u22ef subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e 1 subscript \ud835\udc56 \ud835\udc4e 1 \\displaystyle\\Pr[I^{j}_{a}=i|I^{j}_{1}=i_{1},I^{j}_{2}=i_{2},\\cdots I^{j}_{a-1}=i_{a-1}] \u2265 ( 1 \u2212 2 / 5 \u200b q ) \u200b \u2016 X \u2297 ( q \u2212 a ) \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 \u2016 2 2 \u2016 X \u2297 ( q \u2212 a ) \u200b D a \u2212 1 \u200b X \u22a4 \u2016 F 2 . absent 1 2 5 \ud835\udc5e superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\qquad\\geq\\left(1-{2}/{5q}\\right)\\frac{\\|X^{\\otimes(q-a)}D^{a-1}X_{i,\\star}^{\\top}\\|_{2}^{2}}{\\|X^{\\otimes(q-a)}D^{a-1}X^{\\top}\\|_{F}^{2}}. (6)\n\nIt follows from the definition of tensor products and definition of , that\n\n\u2016 X \u2297 ( q \u2212 a \u2212 1 ) \u200b D a \u200b X \u22a4 \u2016 F 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e 1 superscript \ud835\udc37 \ud835\udc4e superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\left\\|X^{\\otimes(q-a-1)}D^{a}X^{\\top}\\right\\|_{F}^{2} = \u2016 X \u2297 ( q \u2212 a \u2212 1 ) \u200b D a \u2212 1 \u22c5 diag \u200b ( X i a , \u22c6 ) \u200b X \u22a4 \u2016 F 2 absent superscript subscript norm \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e 1 superscript \ud835\udc37 \ud835\udc4e 1 diag subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc4e \u22c6 superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle=\\left\\|X^{\\otimes(q-a-1)}D^{a-1}\\cdot\\text{diag}(X_{i_{a},\\star})X^{\\top}\\right\\|_{F}^{2} = \u2016 X \u2297 ( q \u2212 a ) \u200b D a \u2212 1 \u200b X i a , \u22c6 \u22a4 \u2016 2 2 absent superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc4e \u22c6 top 2 2 \\displaystyle=\\left\\|X^{\\otimes(q-a)}D^{a-1}X_{i_{a},\\star}^{\\top}\\right\\|_{2}^{2}\n\nUsing this equality and inequality (6),\n\nPr \u2061 [ I j = ( i 1 , i 2 , \u22ef \u200b i q ) ] Pr superscript \ud835\udc3c \ud835\udc57 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e \\displaystyle\\Pr\\left[I^{j}=(i_{1},i_{2},\\cdots i_{q})\\right] = \u220f a = 1 q Pr \u2061 [ I a j = i a | I 1 j = i 1 , \u22ef \u200b I a \u2212 1 j = i a \u2212 1 ] absent superscript subscript product \ud835\udc4e 1 \ud835\udc5e Pr subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e conditional subscript \ud835\udc56 \ud835\udc4e subscript superscript \ud835\udc3c \ud835\udc57 1 subscript \ud835\udc56 1 \u22ef subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e 1 subscript \ud835\udc56 \ud835\udc4e 1 \\displaystyle=\\prod_{a=1}^{q}\\Pr\\left[I^{j}_{a}=i_{a}|I^{j}_{1}=i_{1},\\cdots I^{j}_{a-1}=i_{a-1}\\right] \u2265 \u220f a = 1 q ( 1 \u2212 2 5 \u200b q ) \u200b \u2016 X \u2297 ( q \u2212 a ) \u200b D a \u2212 1 \u200b X i a , \u22c6 \u22a4 \u2016 2 2 \u2016 X \u2297 ( q \u2212 a ) \u200b D a \u2212 1 \u200b X \u22a4 \u2016 F 2 absent superscript subscript product \ud835\udc4e 1 \ud835\udc5e 1 2 5 \ud835\udc5e superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc4e \u22c6 top 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\geq\\prod_{a=1}^{q}\\left(1-\\frac{2}{5q}\\right)\\frac{\\|X^{\\otimes(q-a)}D^{a-1}X_{i_{a},\\star}^{\\top}\\|_{2}^{2}}{\\|X^{\\otimes(q-a)}D^{a-1}X^{\\top}\\|_{F}^{2}} \u2265 1 2 \u200b \u2016 X \u2297 ( 0 ) \u200b D q \u2212 1 \u200b X i q , \u22c6 \u22a4 \u2016 2 2 \u2016 X \u2297 ( q \u2212 1 ) \u200b D 0 \u200b X \u22a4 \u2016 F 2 absent 1 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent 0 superscript \ud835\udc37 \ud835\udc5e 1 superscript subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc5e \u22c6 top 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e 1 superscript \ud835\udc37 0 superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\geq\\frac{1}{2}\\frac{\\|X^{\\otimes(0)}D^{q-1}X_{i_{q},\\star}^{\\top}\\|_{2}^{2}}{\\|X^{\\otimes(q-1)}D^{0}X^{\\top}\\|_{F}^{2}} = 1 2 \u200b | [ X \u2297 q \u22c5 M ] ( i 1 , i 2 , \u22ef \u200b i q ) , j | 2 \u2016 [ X \u2297 q \u22c5 M ] \u22c6 , j \u2016 2 2 absent 1 2 superscript subscript delimited-[] \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e \ud835\udc57 2 superscript subscript norm subscript delimited-[] \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 \u22c6 \ud835\udc57 2 2 \\displaystyle=\\frac{1}{2}\\frac{\\left|[X^{\\otimes q}\\cdot M]_{(i_{1},i_{2},\\cdots i_{q}),j}\\right|^{2}}{\\|[X^{\\otimes q}\\cdot M]_{\\star,j}\\|_{2}^{2}} (7)\n\nBy plugging (7) back in (2) we find that,\n\nPr \u2061 [ S l , \u22c6 = \u03b2 \u2212 1 / 2 \u200b ( \ud835\udc1e i 1 \u2297 \ud835\udc1e i 2 \u2297 \u22ef \u200b \ud835\udc1e i q ) \u22a4 ] Pr subscript \ud835\udc46 \ud835\udc59 \u22c6 superscript \ud835\udefd 1 2 superscript tensor-product subscript \ud835\udc1e subscript \ud835\udc56 1 subscript \ud835\udc1e subscript \ud835\udc56 2 \u22ef subscript \ud835\udc1e subscript \ud835\udc56 \ud835\udc5e top \\displaystyle\\Pr\\left[S_{l,\\star}=\\beta^{-1/2}({\\bf e}_{i_{1}}\\otimes{\\bf e}_{i_{2}}\\otimes\\cdots{\\bf e}_{i_{q}})^{\\top}\\right] \u2265 \u2211 j = 1 d \u2032 1 2 \u22c5 | [ X \u2297 q \u22c5 M ] ( i 1 , i 2 , \u22ef \u200b i q ) , j | 2 \u2016 [ X \u2297 q \u22c5 M ] \u22c6 , j \u2016 2 2 \u22c5 p j absent superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 \u22c5 1 2 superscript subscript delimited-[] \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e \ud835\udc57 2 superscript subscript norm subscript delimited-[] \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 \u22c6 \ud835\udc57 2 2 subscript \ud835\udc5d \ud835\udc57 \\displaystyle\\qquad\\geq\\sum_{j=1}^{d^{\\prime}}\\frac{1}{2}\\cdot\\frac{\\left|[X^{\\otimes q}\\cdot M]_{(i_{1},i_{2},\\cdots i_{q}),j}\\right|^{2}}{\\|[X^{\\otimes q}\\cdot M]_{\\star,j}\\|_{2}^{2}}\\cdot p_{j} = 1 2 \u200b \u2211 j = 1 d \u2032 | [ X \u2297 q \u22c5 M ] ( i 1 , i 2 , \u22ef \u200b i q ) , j | 2 \u2016 X \u2297 q \u200b M \u22c6 , j \u2016 2 2 \u200b \u2016 Q q \u200b X \u2297 q \u200b M \u22c6 , j \u2016 2 2 \u2016 Q q \u200b X \u2297 q \u200b M \u2016 F 2 absent 1 2 superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 superscript subscript delimited-[] \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e \ud835\udc57 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e subscript \ud835\udc40 \u22c6 \ud835\udc57 2 2 superscript subscript norm superscript \ud835\udc44 \ud835\udc5e superscript \ud835\udc4b tensor-product absent \ud835\udc5e subscript \ud835\udc40 \u22c6 \ud835\udc57 2 2 superscript subscript norm superscript \ud835\udc44 \ud835\udc5e superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 \ud835\udc39 2 \\displaystyle\\qquad=\\frac{1}{2}\\sum_{j=1}^{d^{\\prime}}\\frac{\\left|[X^{\\otimes q}\\cdot M]_{(i_{1},i_{2},\\cdots i_{q}),j}\\right|^{2}}{\\|X^{\\otimes q}M_{\\star,j}\\|_{2}^{2}}\\frac{\\|Q^{q}X^{\\otimes q}M_{\\star,j}\\|_{2}^{2}}{\\|Q^{q}X^{\\otimes q}M\\|_{F}^{2}} \u2265 1 3 \u200b \u2211 j = 1 d \u2032 | [ X \u2297 q \u22c5 M ] ( i 1 , i 2 , \u22ef \u200b i q ) , j | 2 \u2016 X \u2297 q \u200b M \u22c6 , j \u2016 2 2 \u200b \u2016 X \u2297 q \u200b M \u22c6 , j \u2016 2 2 \u2016 X \u2297 q \u200b M \u2016 F 2 absent 1 3 superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 superscript subscript delimited-[] \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e \ud835\udc57 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e subscript \ud835\udc40 \u22c6 \ud835\udc57 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e subscript \ud835\udc40 \u22c6 \ud835\udc57 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 \ud835\udc39 2 \\displaystyle\\qquad\\geq\\frac{1}{3}\\sum_{j=1}^{d^{\\prime}}\\frac{\\left|[X^{\\otimes q}\\cdot M]_{(i_{1},i_{2},\\cdots i_{q}),j}\\right|^{2}}{\\|X^{\\otimes q}M_{\\star,j}\\|_{2}^{2}}\\frac{\\|X^{\\otimes q}M_{\\star,j}\\|_{2}^{2}}{\\|X^{\\otimes q}M\\|_{F}^{2}} = 1 3 \u200b \u2016 [ X \u2297 q \u22c5 M ] ( i 1 , i 2 , \u22ef \u200b i q ) , \u22c6 \u2016 2 \u2016 X \u2297 q \u200b M \u2016 F 2 absent 1 3 superscript norm subscript delimited-[] \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e \u22c6 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 \ud835\udc39 2 \\displaystyle\\qquad=\\frac{1}{3}\\frac{\\left\\|[X^{\\otimes q}\\cdot M]_{(i_{1},i_{2},\\cdots i_{q}),\\star}\\right\\|^{2}}{\\|X^{\\otimes q}M\\|_{F}^{2}}\n\nMatrix is defined as where is a random matrix with i.i.d. Gaussian entries with columns. Therefore, is a JL-transform, so for every , with probability ,\n\n\u2016 [ X \u2297 q \u22c5 M ] ( i 1 , i 2 , \u22ef \u200b i q ) , \u22c6 \u2016 2 2 d \u2032 superscript subscript norm subscript delimited-[] \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e \ud835\udc40 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e \u22c6 2 2 superscript \ud835\udc51 \u2032 \\displaystyle\\frac{\\left\\|[X^{\\otimes q}\\cdot M]_{(i_{1},i_{2},\\cdots i_{q}),\\star}\\right\\|_{2}^{2}}{d^{\\prime}} \u2208 ( 1 \u00b1 0.1 ) \u200b \u2016 [ X \u2297 q ] ( i 1 , i 2 , \u22ef \u200b i q ) , \u22c6 \u200b ( B \u22a4 \u200b B + \u03bb \u200b I ) \u2212 1 / 2 \u2016 2 2 . absent plus-or-minus 1 0.1 superscript subscript norm subscript delimited-[] superscript \ud835\udc4b tensor-product absent \ud835\udc5e subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e \u22c6 superscript superscript \ud835\udc35 top \ud835\udc35 \ud835\udf06 \ud835\udc3c 1 2 2 2 \\displaystyle\\qquad\\in\\left(1\\pm 0.1\\right)\\left\\|[X^{\\otimes q}]_{(i_{1},i_{2},\\cdots i_{q}),\\star}(B^{\\top}B+\\lambda I)^{-1/2}\\right\\|_{2}^{2}. Therefore, by union bounding over rows of , the above holds simultaneously for all with high probability. Therefore,\n\nPr \u2061 [ S l , \u22c6 = \u03b2 \u2212 1 / 2 \u200b ( \ud835\udc1e i 1 \u2297 \ud835\udc1e i 2 \u2297 \u22ef \u200b \ud835\udc1e i q ) \u22a4 ] Pr subscript \ud835\udc46 \ud835\udc59 \u22c6 superscript \ud835\udefd 1 2 superscript tensor-product subscript \ud835\udc1e subscript \ud835\udc56 1 subscript \ud835\udc1e subscript \ud835\udc56 2 \u22ef subscript \ud835\udc1e subscript \ud835\udc56 \ud835\udc5e top \\displaystyle\\Pr\\left[S_{l,\\star}=\\beta^{-1/2}({\\bf e}_{i_{1}}\\otimes{\\bf e}_{i_{2}}\\otimes\\cdots{\\bf e}_{i_{q}})^{\\top}\\right] \u2265 1 4 \u22c5 \u2016 [ X \u2297 q \u22c5 ( B \u22a4 \u200b B + \u03bb \u200b I ) \u2212 1 / 2 ] ( i 1 , i 2 , \u22ef \u200b i q ) , \u22c6 \u2016 2 2 \u2016 X \u2297 q \u200b ( B \u22a4 \u200b B + \u03bb \u200b I ) \u2212 1 / 2 \u2016 F 2 absent \u22c5 1 4 superscript subscript norm subscript delimited-[] \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc5e superscript superscript \ud835\udc35 top \ud835\udc35 \ud835\udf06 \ud835\udc3c 1 2 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc5e \u22c6 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc5e superscript superscript \ud835\udc35 top \ud835\udc35 \ud835\udf06 \ud835\udc3c 1 2 \ud835\udc39 2 \\displaystyle\\qquad\\geq\\frac{1}{4}\\cdot\\frac{\\left\\|\\left[X^{\\otimes q}\\cdot(B^{\\top}B+\\lambda I)^{-1/2}\\right]_{(i_{1},i_{2},\\cdots i_{q}),\\star}\\right\\|_{2}^{2}}{\\|X^{\\otimes q}(B^{\\top}B+\\lambda I)^{-1/2}\\|_{F}^{2}}\n\nBecause is the probability of sampling row of the matrix , the above inequality proves that with high probability, is a rank- row norm sampler for as in Definition 3.1.",
    "0051d15d5f3849846707eeb91677c80366f17960-4": "Runtime:\n\nOne of the expensive steps of this algorithm is the computation of in line 2 which takes operations since is rank . Another expensive step is the computation of for in line 4. By Lemma 4, this can be computed in time . Matrices for all in line 11 of the algorithm can be computed in time . Computing the distribution in line 15 takes time for a fixed and . Therefore, the total time to compute this distribution for all and is . The runtime to compute the distribution in line 17 depends on the sparsity of , i.e., . To bound the sparsity of , note that, . Let us introduce the random variables defined as for . Since the hash function is fully independent, the random variables are independent. Also, each of these random variables is zero mean and uniformly bounded, i.e., and for each . Therefore we can invoke Bernstein\u2019s inequality (Appendix A). Let . Then the variance of the sum is bounded as . By invoking Bernstein\u2019s inequality, for some constant , . Hence, for every , with high probability . By union bounding over events, with high probability, , simultaneously for all which implies that the distribution in line 17 of the algorithm can be computed in time for a fixed and a fixed . Therefore the total time to compute this distribution for all and all is . Finally the last expensive step is the computation of quantities and in lines 25 and 26 of the algorithm. Both of these quantities can be computed in time for a fixed and a fixed . Therefore the total time to compute these quantities for all , all , and all is . Therefore the total runtime of Algorithm 2 is . \u220e We prove Theorem 1 in Appendix E. 3.2 Adaptive Sampling for the Gaussian Kernel\n\nConsider the lifting corresponding to the Gaussian kernel, , that can be obtained through a Taylor expansion. This feature mapping was exploited in (Ahle et al., 2020) to obtain an efficient subspace embedding for the Gaussian kernel via sketching the polynomial terms in its Taylor expansion. For datasets with bounded radius, the Gaussian kernel can be well-approximated by a superposition of low-degree polynomial kernels. We formally define this approximate feature mapping (lifting) as follows. Definition 3.2 (Polynomial Lifting for Gaussian Kernel). For any integer the degree- polynomial lifting for Gaussian kernel is the mapping , defined as,\n\n\u03d5 q \u200b ( x ) = e \u2212 \u2016 x \u2016 2 2 / 2 \u200b ( x \u2297 0 0 !",
    "0051d15d5f3849846707eeb91677c80366f17960-5": "\u2295 x \u2297 1 1 ! \u2295 x \u2297 2 2 ! \u2295 \u22ef \u200b x \u2297 q q ! ) , subscript italic-\u03d5 \ud835\udc5e \ud835\udc65 superscript \ud835\udc52 superscript subscript norm \ud835\udc65 2 2 2 direct-sum superscript \ud835\udc65 tensor-product absent 0 0 superscript \ud835\udc65 tensor-product absent 1 1 superscript \ud835\udc65 tensor-product absent 2 2 \u22ef superscript \ud835\udc65 tensor-product absent \ud835\udc5e \ud835\udc5e \\phi_{q}(x)=e^{-\\|x\\|_{2}^{2}/2}\\left(\\frac{x^{\\otimes 0}}{\\sqrt{0!}}\\oplus\\frac{x^{\\otimes 1}}{\\sqrt{1!}}\\oplus\\frac{x^{\\otimes 2}}{\\sqrt{2!}}\\oplus\\cdots\\frac{x^{\\otimes q}}{\\sqrt{q!}}\\right),\n\nfor , where .",
    "0051d15d5f3849846707eeb91677c80366f17960-6": "Claim 6. Let be a dataset with bounded radius, i.e., for all . Suppose is the Gaussian kernel corresponding to this dataset (). Also suppose that is the degree- polynomial lifting for the Gaussian kernel as in Definition 3.2. If is a matrix with columns whose columns are obtained by applying the map on the data points, i.e., , then as long as , we have . Therefore, to find a spectral approximation to the Gaussian kernel for bounded datasets, it is enough to find a spectral approximation to , where is the matrix defined in the above claim. We have designed an efficient adaptive sampling method for tensor products of the form in the previous section. Since matrix is a concatenation of tensor products for , using our iterative leverage score sampling procedure for the polynomial kernel we can spectrally approximate in nearly time. We present a full algorithm which can perform recursive leverage score sampling on matrix and analyze it in Appendix F and prove Theorem 2 in Appendix G. 3.3 Generalization to dot-product Kernels\n\nAn important technical contribution of this paper is a sampling method that can embed the polynomial kernel using near-optimal runtime. Additionally, our method can be used for embedding a wide class of kernels that can be well-approximated by low-degree polynomials. In particular, our sampling method can be applied to any dot-product kernel with a rapidly convergent Taylor expansion. In this section, we argue how our method can be generalized to such kernels. The underlying observation that enables us to extend our subspace embedding to the class of dot-product kernels is a classical result in harmonic analysis due to Schoenberg (1988), that characterizes positive definite functions in a Hilbert space. This observation is simply the fact that any dot-product kernel defined as must have a Taylor expansion with only non-negative coefficients, i.e., is a kernel function if and only if , for all . As a result, truncating this sum at any point results in a valid kernel, that is is a valid positive definite kernel. For most dot-product kernels used in practice, the coefficients decay at least exponentially. If this is the case, then for any with and . Hence, in order to obtain a subspace embedding for kernel on any dataset with bounded radius, it is enough to find a subspace embedding for the truncated kernel . Since this kernel is a superposition of polynomial kernels, we can apply our subspace embedding for the polynomial kernel from Section 3.1 to each of the polynomial terms. This will result in a near input sparsity time subspace embedding for any dot-product kernel whose Taylor expansion decays at least exponentially. An example of a well known dot product kernel is the inverse polynomial kernel defined as . The Taylor expansion of this kernel is . Therefore, if we let then for any with , , where . Hence, we can obtain a subspace embedding for the inverse polynomial kernel in nearly time by applying our sampling method from Section 3.1 to polynomials of degree in this Taylor expansion. 4 Experiments\n\nIn this section we assess the performance of our result for embedding the Gaussian kernel (Theorem 2) against the Fourier features (FF) method (Rahimi & Recht, 2008), Nystrom method (Musco & Musco, 2017), as well as the Oblivious sketching method of (Ahle et al., 2020).",
    "0051d15d5f3849846707eeb91677c80366f17960-7": "The results are summarized in Table 1222We repeated the experiments with 5 different random seeds and reported the average RMSE and runtime in Table 1.. Our importance sampling algorithm is a recursive procedure given in Algorithm 1. In this set of experiments, we also consider a variant of our sampling algorithm that runs only a single round of the recursive sampling and hence is considerably faster. This variant is equivalent to sampling rows of the lifting matrix with probabilities proportional to the squared row norms. We denote this variant of our method by Row norm and denote the full recursive importance sampling algorithm by Adaptive. The target dimension of all methods is denoted by in Table 1. We base our comparison on the four standard large-scale regression datasets evaluated in (Le et al., 2013). The size of the data points is denoted by and the dimensionality is denoted by in Table 1. In all experiments, we first find a low-rank approximation to the kernel matrix using various feature sampling/sketching techniques. Then, using the kernel\u2019s proxy, we find an approximate regressor by solving an regularized least-squares problem. For all methods, Table 1 reports the total time to train the regressors, including the runtime of feature sampling and the runtime of linear regression. We use the same hyperparameters (kernel bandwidth and regularization parameter) across all kernel approximation methods which were selected via cross-validation on the Fourier features method, as our baseline method. For every method, we set the number of features to the smallest value such that increasing the number of features does not improve the error non-negligibly. The Row norm variant of our method is as fast as the FF method and runs significantly faster than the Nystrom and Oblivious methods while having superior testing RMSE. Our full algorithm, Adaptive, has even better performance than our single round variant Row norm in terms of RMSE on the test set and achieves a better RMSE while having a significantly smaller target dimension than all other methods. In terms of runtime, our full Adaptive method is no worse than Nystrom but is slower than our single round Row norm method. Our Adaptive method has a slightly better RMSE than the Oblivious method and runs slower, but it achieves a significantly smaller target dimension . However, our single round Row norm variant is significantly faster than Oblivious.",
    "0051d15d5f3849846707eeb91677c80366f17960-8": "Acknowledgements\n\nD. P. Woodruff was supported in part by Office of Naval Research (ONR) grant N00014-18-1-2562. References\n\nAhle et al. (2020) Ahle, T. D., Kapralov, M., Knudsen, J. B., Pagh, R., Velingker, A., Woodruff, D. P., and Zandieh, A. Oblivious sketching of high-degree polynomial kernels. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 141\u2013160. SIAM, 2020. Ailon & Chazelle (2006) Ailon, N. and Chazelle, B. Approximate nearest neighbors and the fast johnson-lindenstrauss transform. In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing, pp. 557\u2013563, 2006. Alaoui & Mahoney (2015) Alaoui, A.",
    "0051d15d5f3849846707eeb91677c80366f17960-9": "and Mahoney, M. W. Fast randomized kernel ridge regression with statistical guarantees.",
    "0051d15d5f3849846707eeb91677c80366f17960-10": "In Advances in Neural Information Processing Systems, pp. 775\u2013783, 2015. Avron et al. (2014) Avron, H., Nguyen, H., and Woodruff, D. Subspace embeddings for the polynomial kernel.",
    "0051d15d5f3849846707eeb91677c80366f17960-11": "In Advances in neural information processing systems, pp. 2258\u20132266, 2014. Avron et al. (2017a) Avron, H., Clarkson, K.",
    "0051d15d5f3849846707eeb91677c80366f17960-12": "L., and Woodruff, D. P. Faster kernel ridge regression using sketching and preconditioning. SIAM Journal on Matrix Analysis and Applications, 38(4):1116\u20131138, 2017a. Avron et al. (2017b) Avron, H., Kapralov, M., Musco, C., Musco, C., Velingker, A., and Zandieh, A. Random fourier features for kernel ridge regression: Approximation bounds and statistical guarantees.",
    "0051d15d5f3849846707eeb91677c80366f17960-13": "In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 253\u2013262. JMLR. org, 2017b. Bach (2013) Bach, F. Sharp analysis of low-rank kernel matrix approximations.",
    "0051d15d5f3849846707eeb91677c80366f17960-14": "In Conference on Learning Theory, pp. 185\u2013209, 2013. Boucheron et al. (2013) Boucheron, S., Lugosi, G., and Massart, P. Concentration inequalities: A nonasymptotic theory of independence.",
    "0051d15d5f3849846707eeb91677c80366f17960-15": "Oxford university press, 2013. Cohen et al. (2016) Cohen, M. B., Musco, C., and Pachocki, J. Online row sampling. arXiv preprint arXiv:1604.05448, 2016. Cohen et al. (2017) Cohen, M. B., Musco, C., and Musco, C. Input sparsity time low-rank approximation via ridge leverage score sampling.",
    "0051d15d5f3849846707eeb91677c80366f17960-16": "In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, pp.",
    "0051d15d5f3849846707eeb91677c80366f17960-17": "1758\u20131777. SIAM, 2017. Dasgupta & Gupta (2003) Dasgupta, S. and Gupta, A. An elementary proof of a theorem of johnson and lindenstrauss. Random Structures & Algorithms, 22(1):60\u201365, 2003. Kane & Nelson (2014) Kane, D.",
    "0051d15d5f3849846707eeb91677c80366f17960-18": "M. and Nelson, J. Sparser johnson-lindenstrauss transforms. Journal of the ACM (JACM), 61(1):4, 2014. Kapralov et al. (2014) Kapralov, M., Lee, Y. T., Musco, C., Musco, C., and Sidford, A. Single pass spectral sparsification in dynamic streams. In 2014 IEEE 55th Annual Symposium on Foundations of Computer Science, pp. 561\u2013570. IEEE, 2014. Le et al. (2013) Le, Q., Sarl\u00f3s, T., and Smola, A. Fastfood-approximating kernel expansions in loglinear time.",
    "0051d15d5f3849846707eeb91677c80366f17960-19": "In Proceedings of the international conference on machine learning, volume 85, 2013. Musco & Musco (2017) Musco, C. and Musco, C. Recursive sampling for the nystrom method.",
    "0051d15d5f3849846707eeb91677c80366f17960-20": "In Advances in Neural Information Processing Systems, pp. 3833\u20133845, 2017. Pham & Pagh (2013) Pham, N. and Pagh, R. Fast and scalable polynomial kernels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 239\u2013247, 2013. Rahimi & Recht (2008) Rahimi, A. and Recht, B. Random features for large-scale kernel machines. In Advances in neural information processing systems, pp. 1177\u20131184, 2008. Schoenberg (1988) Schoenberg, I. Positive definite functions on spheres. Duke Math. J, 1:172, 1988. Tropp (2011) Tropp, J. A. Improved analysis of the subsampled randomized hadamard transform. Advances in Adaptive Data Analysis, 3(01n02):115\u2013126, 2011.",
    "0051d15d5f3849846707eeb91677c80366f17960-21": "Zandieh et al. (2020) Zandieh, A., Nouri, N., Velingker, A., Kapralov, M., and Razenshteyn, I. Scaling up kernel ridge regression via locality sensitive hashing. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp.",
    "0051d15d5f3849846707eeb91677c80366f17960-22": "4088\u20134097, Online, 26\u201328 Aug 2020. PMLR. Appendix A Bernstein\u2019s Inequality\n\nWe use Bernstein\u2019s concentration inequality given in the following lemma. Lemma 7. Let be independent, mean-, real-valued random variables, and assume that each one is uniformly bounded:\n\n\ud835\udd3c \u200b [ S k ] = 0 \u200b and \u200b | S k | \u2264 L \u200b for each \u200b k = 1 , 2 , \u22ef \u200b n formulae-sequence \ud835\udd3c delimited-[] subscript \ud835\udc46 \ud835\udc58 0 and subscript \ud835\udc46 \ud835\udc58 \ud835\udc3f for each \ud835\udc58 1 2 \u22ef \ud835\udc5b \\mathbb{E}[S_{k}]=0\\text{ and }|S_{k}|\\leq L\\text{ for each }k=1,2,\\cdots n\n\nLet , and let denote the variance of the sum:\n\nv = \ud835\udd3c \u200b [ Z 2 ] = \u2211 k = 1 n \ud835\udd3c \u200b [ S k 2 ] . \ud835\udc63 \ud835\udd3c delimited-[] superscript \ud835\udc4d 2 superscript subscript \ud835\udc58 1 \ud835\udc5b \ud835\udd3c delimited-[] superscript subscript \ud835\udc46 \ud835\udc58 2 v=\\mathbb{E}[Z^{2}]=\\sum_{k=1}^{n}\\mathbb{E}[S_{k}^{2}]. Then,\n\nPr \u2061 [ | Z | \u2265 t ] \u2264 2 \u200b e \u2212 t 2 / 2 v + L \u200b t / 3 . Pr \ud835\udc4d \ud835\udc61 2 superscript \ud835\udc52 superscript \ud835\udc61 2 2 \ud835\udc63 \ud835\udc3f \ud835\udc61 3 \\Pr[|Z|\\geq t]\\leq 2e^{\\frac{-t^{2}/2}{v+Lt/3}}. See (Boucheron et al., 2013) for a proof of this result. Appendix B Properties of Leverage Scores\n\nIn this section we present the definition and basic properties of the ridge leverage scores of a matrix . For every regularization parameter and every the ridge leverage score of the row of is defined as,\n\nl i \u03bb \u2261 \u03d5 i \u22a4 \u200b ( \u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I ) \u2212 1 \u200b \u03d5 i , subscript superscript \ud835\udc59 \ud835\udf06 \ud835\udc56 superscript subscript italic-\u03d5 \ud835\udc56 top superscript superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 subscript italic-\u03d5 \ud835\udc56 l^{\\lambda}_{i}\\equiv\\phi_{i}^{\\top}(\\Phi^{\\top}\\Phi+\\lambda I)^{-1}\\phi_{i},\n\nwhere is the row of , treated as a column vector. There is a connection between the ridge leverage scores of and the statistical dimension of . The sum of the ridge leverage scores is equal to the statistical dimension of the kernel matrix ,\n\ns \u03bb \u2261 \ud835\udc2d\ud835\udc2b \u200b ( \u03a6 \u22a4 \u200b \u03a6 \u200b ( \u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I ) \u2212 1 ) = \u2211 i \u2208 [ d ] l i \u03bb . subscript \ud835\udc60 \ud835\udf06 \ud835\udc2d\ud835\udc2b superscript \u03a6 top \u03a6 superscript superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 subscript \ud835\udc56 delimited-[] \ud835\udc51 subscript superscript \ud835\udc59 \ud835\udf06 \ud835\udc56 s_{\\lambda}\\equiv{\\bf tr}\\left(\\Phi^{\\top}\\Phi(\\Phi^{\\top}\\Phi+\\lambda I)^{-1}\\right)=\\sum_{i\\in[d]}l^{\\lambda}_{i}. We next present a lemma which shows that ridge leverage score sampling is an optimal sampling strategy for achieving the spectral guarantee of (1) (up to an factor),\n\nLemma 8. Let be a matrix with rows and with ridge leverage scores for all . Let . Assume that we are given a probability distribution such that for every and some . Construct the sampling matrix by generating i.i.d. samples with distribution and letting the row of be for every , where are the standard basis vectors. If the number of rows of is at least , then with high probability,\n\n\u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 + \u03f5 \u2aaf \u03a6 \u22a4 \u200b \u03a0 \u22a4 \u200b \u03a0 \u200b \u03a6 + \u03bb \u200b I \u2aaf \u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 \u2212 \u03f5 . precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 superscript \u03a6 top superscript \u03a0 top \u03a0 \u03a6 \ud835\udf06 \ud835\udc3c precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 \\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1+\\epsilon}\\preceq\\Phi^{\\top}\\Pi^{\\top}\\Pi\\Phi+\\lambda I\\preceq\\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1-\\epsilon}.",
    "0051d15d5f3849846707eeb91677c80366f17960-23": "Proof. This guarantee for leverage score sampling is well-known. See, for example, (Cohen et al., 2016, 2017). \u220e\n\nAppendix C Proof of Lemma 3\n\nLet be the sampling matrix and let be the regularizing parameter in the iteration of Algorithm 1. The proof of the lemma proceeds by induction. We define the event as the set of all sampling matrices that satisfy the following condition,\n\n\u03a6 \u22a4 \u200b \u03a6 + \u03bb t \u200b I 1 + \u03f5 \u2aaf \u03a6 \u22a4 \u200b S t \u22a4 \u200b S t \u200b \u03a6 + \u03bb t \u200b I \u2aaf \u03a6 \u22a4 \u200b \u03a6 + \u03bb t \u200b I 1 \u2212 \u03f5 . precedes-or-equals superscript \u03a6 top \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c 1 italic-\u03f5 superscript \u03a6 top superscript subscript \ud835\udc46 \ud835\udc61 top subscript \ud835\udc46 \ud835\udc61 \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c precedes-or-equals superscript \u03a6 top \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c 1 italic-\u03f5 \\frac{\\Phi^{\\top}\\Phi+\\lambda_{t}I}{1+\\epsilon}\\preceq\\Phi^{\\top}S_{t}^{\\top}S_{t}\\Phi+\\lambda_{t}I\\preceq\\frac{\\Phi^{\\top}\\Phi+\\lambda_{t}I}{1-\\epsilon}. We show by induction that for all , the invariant conditionally holds with high probability, that is,\n\nPr \u2061 [ \u2130 t | \u2130 t \u2212 1 ] \u2265 1 \u2212 1 poly \u200b ( n ) . Pr conditional subscript \u2130 \ud835\udc61 subscript \u2130 \ud835\udc61 1 1 1 poly \ud835\udc5b \\Pr[\\mathcal{E}_{t}|\\mathcal{E}_{t-1}]\\geq 1-\\frac{1}{\\text{poly}(n)}. The base of the induction corresponds to . For we have that and , and therefore, , which implies that,\n\n\u03a6 \u22a4 \u200b \u03a6 + \u03bb 0 \u200b I 1 + \u03f5 \u2aaf \u03a6 \u22a4 \u200b S 0 \u22a4 \u200b S 0 \u200b \u03a6 + \u03bb 0 \u200b I \u2aaf \u03a6 \u22a4 \u200b \u03a6 + \u03bb 0 \u200b I . precedes-or-equals superscript \u03a6 top \u03a6 subscript \ud835\udf06 0 \ud835\udc3c 1 italic-\u03f5 superscript \u03a6 top superscript subscript \ud835\udc46 0 top subscript \ud835\udc46 0 \u03a6 subscript \ud835\udf06 0 \ud835\udc3c precedes-or-equals superscript \u03a6 top \u03a6 subscript \ud835\udf06 0 \ud835\udc3c \\frac{\\Phi^{\\top}\\Phi+\\lambda_{0}I}{1+\\epsilon}\\preceq\\Phi^{\\top}S_{0}^{\\top}S_{0}\\Phi+\\lambda_{0}I\\preceq\\Phi^{\\top}\\Phi+\\lambda_{0}I. Therefore, , which proves the base case of the induction. Now to prove the inductive step, note that conditioned on the event holding for some , we find that\n\n\u03a6 \u22a4 \u200b \u03a6 + \u03bb t \u200b I 1 + \u03f5 \u2aaf \u03a6 \u22a4 \u200b S t \u22a4 \u200b S t \u200b \u03a6 + \u03bb t \u200b I \u2aaf \u03a6 \u22a4 \u200b \u03a6 + \u03bb t \u200b I 1 \u2212 \u03f5 . precedes-or-equals superscript \u03a6 top \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c 1 italic-\u03f5 superscript \u03a6 top superscript subscript \ud835\udc46 \ud835\udc61 top subscript \ud835\udc46 \ud835\udc61 \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c precedes-or-equals superscript \u03a6 top \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c 1 italic-\u03f5 \\frac{\\Phi^{\\top}\\Phi+\\lambda_{t}I}{1+\\epsilon}\\preceq\\Phi^{\\top}S_{t}^{\\top}S_{t}\\Phi+\\lambda_{t}I\\preceq\\frac{\\Phi^{\\top}\\Phi+\\lambda_{t}I}{1-\\epsilon}. By definition of ridge leverage scores, , and noting that , we have\n\n\u03d5 i \u22a4 \u200b ( \u03a6 \u22a4 \u200b \u03a6 + \u03bb t \u200b I ) \u2212 1 \u200b \u03d5 i \u2264 l i \u03bb t + 1 \u2264 2 \u200b \u03d5 i \u22a4 \u200b ( \u03a6 \u22a4 \u200b \u03a6 + \u03bb t \u200b I ) \u2212 1 \u200b \u03d5 i . superscript subscript italic-\u03d5 \ud835\udc56 top superscript superscript \u03a6 top \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c 1 subscript italic-\u03d5 \ud835\udc56 superscript subscript \ud835\udc59 \ud835\udc56 subscript \ud835\udf06 \ud835\udc61 1 2 superscript subscript italic-\u03d5 \ud835\udc56 top superscript superscript \u03a6 top \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c 1 subscript italic-\u03d5 \ud835\udc56 \\phi_{i}^{\\top}(\\Phi^{\\top}\\Phi+\\lambda_{t}I)^{-1}\\phi_{i}\\leq l_{i}^{\\lambda_{t+1}}\\leq 2\\phi_{i}^{\\top}(\\Phi^{\\top}\\Phi+\\lambda_{t}I)^{-1}\\phi_{i}. By the inductive hypothesis, for , we have,\n\n1 3 \u22c5 l i \u03bb t + 1 \u2264 \u03d5 i \u22a4 \u200b ( \u03a6 \u22a4 \u200b S t \u22a4 \u200b S t \u200b \u03a6 + \u03bb t \u200b I ) \u2212 1 \u200b \u03d5 i \u2264 4 3 \u22c5 l i \u03bb t + 1 . \u22c5 1 3 superscript subscript \ud835\udc59 \ud835\udc56 subscript \ud835\udf06 \ud835\udc61 1 superscript subscript italic-\u03d5 \ud835\udc56 top superscript superscript \u03a6 top superscript subscript \ud835\udc46 \ud835\udc61 top subscript \ud835\udc46 \ud835\udc61 \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c 1 subscript italic-\u03d5 \ud835\udc56 \u22c5 4 3 superscript subscript \ud835\udc59 \ud835\udc56 subscript \ud835\udf06 \ud835\udc61 1 \\frac{1}{3}\\cdot l_{i}^{\\lambda_{t+1}}\\leq\\phi_{i}^{\\top}\\left(\\Phi^{\\top}S_{t}^{\\top}S_{t}\\Phi+\\lambda_{t}I\\right)^{-1}\\phi_{i}\\leq\\frac{4}{3}\\cdot l_{i}^{\\lambda_{t+1}}. Now note that by the assumption of the lemma, is a rank- row norm sampler for the matrix . Therefore there exists a probability distribution such that is the corresponding sampling matrix to this probability distribution constructed as in Definition 3.1. This probability distribution satisfies,\n\np i subscript \ud835\udc5d \ud835\udc56 \\displaystyle p_{i} \u2265 1 4 \u200b \u03d5 i \u22a4 \u200b ( \u03a6 \u22a4 \u200b S t \u22a4 \u200b S t \u200b \u03a6 + \u03bb t \u200b I ) \u2212 1 \u200b \u03d5 i \u2211 j \u2208 [ D ] \u03d5 j \u22a4 \u200b ( \u03a6 \u22a4 \u200b S t \u22a4 \u200b S t \u200b \u03a6 + \u03bb t \u200b I ) \u2212 1 \u200b \u03d5 j absent 1 4 superscript subscript italic-\u03d5 \ud835\udc56 top superscript superscript \u03a6 top superscript subscript \ud835\udc46 \ud835\udc61 top subscript \ud835\udc46 \ud835\udc61 \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c 1 subscript italic-\u03d5 \ud835\udc56 subscript \ud835\udc57 delimited-[] \ud835\udc37 superscript subscript italic-\u03d5 \ud835\udc57 top superscript superscript \u03a6 top superscript subscript \ud835\udc46 \ud835\udc61 top subscript \ud835\udc46 \ud835\udc61 \u03a6 subscript \ud835\udf06 \ud835\udc61 \ud835\udc3c 1 subscript italic-\u03d5 \ud835\udc57 \\displaystyle\\geq\\frac{1}{4}\\frac{\\phi_{i}^{\\top}\\left(\\Phi^{\\top}S_{t}^{\\top}S_{t}\\Phi+\\lambda_{t}I\\right)^{-1}\\phi_{i}}{\\sum_{j\\in[D]}\\phi_{j}^{\\top}\\left(\\Phi^{\\top}S_{t}^{\\top}S_{t}\\Phi+\\lambda_{t}I\\right)^{-1}\\phi_{j}} \u2265 1 16 \u22c5 l i \u03bb t + 1 \u2211 j \u2208 [ D ] l j \u03bb t + 1 . absent \u22c5 1 16 superscript subscript \ud835\udc59 \ud835\udc56 subscript \ud835\udf06 \ud835\udc61 1 subscript \ud835\udc57 delimited-[] \ud835\udc37 superscript subscript \ud835\udc59 \ud835\udc57 subscript \ud835\udf06 \ud835\udc61 1 \\displaystyle\\geq\\frac{1}{16}\\cdot\\frac{l_{i}^{\\lambda_{t+1}}}{\\sum_{j\\in[D]}l_{j}^{\\lambda_{t+1}}}. Therefore because , if is a large enough constant, by Lemma 8,\n\nPr \u2061 [ \u2130 t + 1 | \u2130 t ] \u2265 1 \u2212 1 poly \u200b ( n ) . Pr conditional subscript \u2130 \ud835\udc61 1 subscript \u2130 \ud835\udc61 1 1 poly \ud835\udc5b \\Pr[\\mathcal{E}_{t+1}|\\mathcal{E}_{t}]\\geq 1-\\frac{1}{\\text{poly}(n)}. This completes the inductive step. By union bounding over all , we get that,\n\nPr \u2061 [ \u2130 T ] \u2265 1 \u2212 1 poly \u200b ( n ) . Pr subscript \u2130 \ud835\udc47 1 1 poly \ud835\udc5b \\Pr[\\mathcal{E}_{T}]\\geq 1-\\frac{1}{\\text{poly}(n)}. Hence, since , with high probability the following holds for the sampling matrix ,\n\n\u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 + \u03f5 \u2aaf \u03a6 \u22a4 \u200b \u03a0 \u22a4 \u200b \u03a0 \u200b \u03a6 + \u03bb \u200b I \u2aaf \u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 \u2212 \u03f5 . precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 superscript \u03a6 top superscript \u03a0 top \u03a0 \u03a6 \ud835\udf06 \ud835\udc3c precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 \\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1+\\epsilon}\\preceq\\Phi^{\\top}\\Pi^{\\top}\\Pi\\Phi+\\lambda I\\preceq\\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1-\\epsilon}. This completes the proof of the lemma. Appendix D Proof of Lemma 4\n\nBy invoking Theorem 1.2 of (Ahle et al., 2020), there exists a sketch such that if then for any ,\n\nPr \u2061 [ \u2016 S q \u200b y \u2016 2 2 \u2208 ( 1 \u00b1 \u03f5 ) \u200b \u2016 y \u2016 2 2 ] \u2265 1 \u2212 \u03b4 / 2 . Pr superscript subscript norm superscript \ud835\udc46 \ud835\udc5e \ud835\udc66 2 2 plus-or-minus 1 italic-\u03f5 superscript subscript norm \ud835\udc66 2 2 1 \ud835\udeff 2 \\Pr\\left[\\|S^{q}y\\|_{2}^{2}\\in(1\\pm\\epsilon)\\|y\\|_{2}^{2}\\right]\\geq 1-\\delta/2.",
    "0051d15d5f3849846707eeb91677c80366f17960-24": "Let be a random matrix with i.i.d. normal entries. Thus, is a JL transform with high probability. By the analysis in (Kane & Nelson, 2014), if then for any ,\n\nPr \u2061 [ \u2016 G \u200b z \u2016 2 2 \u2208 ( 1 \u00b1 \u03f5 ) \u200b \u2016 z \u2016 2 2 ] \u2265 1 \u2212 \u03b4 / 2 . Pr superscript subscript norm \ud835\udc3a \ud835\udc67 2 2 plus-or-minus 1 italic-\u03f5 superscript subscript norm \ud835\udc67 2 2 1 \ud835\udeff 2 \\Pr\\left[\\|Gz\\|_{2}^{2}\\in(1\\pm\\epsilon)\\|z\\|_{2}^{2}\\right]\\geq 1-\\delta/2. Therefore if we let then we have that this matrix is of size and also by a union bound, for any ,\n\nPr \u2061 [ \u2016 Q q \u200b y \u2016 2 2 \u2208 ( 1 \u00b1 \u03f5 ) \u200b \u2016 y \u2016 2 2 ] \u2265 1 \u2212 \u03b4 . Pr superscript subscript norm superscript \ud835\udc44 \ud835\udc5e \ud835\udc66 2 2 plus-or-minus 1 italic-\u03f5 superscript subscript norm \ud835\udc66 2 2 1 \ud835\udeff \\Pr\\left[\\|Q^{q}y\\|_{2}^{2}\\in(1\\pm\\epsilon)\\|y\\|_{2}^{2}\\right]\\geq 1-\\delta. Runtime:\n\nAs shown in (Ahle et al., 2020), the sketch can be applied to by a recursive application of independent instances of OSNAP and SRHT sketches on the vectors and their sketched versions. The sketch in (Ahle et al., 2020) can be represented by a binary tree with leaves where the leaves are OSNAP sketches and the internal nodes are SRHT sketches. Therefore, by Theorem 1.2 of (Ahle et al., 2020), can be computed in time . From the binary tree structure of the sketch it follows that after computing , can be computed by updating the path from one of the leaves to the root of the binary tree which amounts to applying one OSNAP transform on and applying instances of SRHT on the intermediate vectors which can be computed in a total extra time of .",
    "0051d15d5f3849846707eeb91677c80366f17960-25": "By this argument, it follows that can be computed for all in total time . By choosing a large enough , this runtime will be . Also, the time to apply the Gaussian sketch , with large enough target dimension , to any -dimensional vector is . Hence the total time to compute for all is . Appendix E Proof of Theorem 1\n\nWe run the recursive leverage score sampling procedure of Algorithm 1 on the feature matrix with . Each time Algorithm 1 invokes the procedure RowSampler, we run Algorithm 2. By Lemma 5, for any , any integers , and any matrices , with high probability, the procedure RowSampler of Algorithm 2 outputs a rank- row norm sampler for matrix . Therefore, since the total number of times Algorithm 2 is invoked by Algorithm 1 is , by a union bound, with high probability the preconditions of Lemma 3 hold and hence we can invoke this lemma to conclude that the sampler that Algorithm 1 outputs satisfies the following with high probability,\n\n\u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 + \u03f5 \u2aaf \u03a6 \u22a4 \u200b \u03a0 \u22a4 \u200b \u03a0 \u200b \u03a6 + \u03bb \u200b I \u2aaf \u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 \u2212 \u03f5 . precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 superscript \u03a6 top superscript \u03a0 top \u03a0 \u03a6 \ud835\udf06 \ud835\udc3c precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 \\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1+\\epsilon}\\preceq\\Phi^{\\top}\\Pi^{\\top}\\Pi\\Phi+\\lambda I\\preceq\\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1-\\epsilon}. Therefore, if we let , the theorem follows because has rows. Moreover, the primitive RowSampler of Algorithm 2 is invoked times with inputs and a matrix with rows. Each invocation, by Lemma 5, takes operations. Hence the total runtime of the algorithm is . Appendix F RowSampler for the Gaussian Kernel\n\nWe design a procedure RowSampler that takes in the dataset matrix together with an matrix and performs row norm sampling (see Definition 3.1) on matrix , where is a matrix with columns which are obtained by applying the mapping of Definition 3.2 on each of the columns of , i.e., .",
    "0051d15d5f3849846707eeb91677c80366f17960-26": "Algorithm 3 performs this task. Lemma 9. For any matrix , let be a matrix with columns whose columns are obtained by applying the mapping as in Definition 3.2 to each column of , i.e., . For any matrix , any , and any integers and , Algorithm 3 outputs a rank- row norm sampler for matrix using runtime. Proof. Let be the matrix whose columns are for all , where is the degree- polynomial lifting for Gaussian kernel as in Definition 3.2. Algorithm 3 outputs a random sampling matrix with . First, we show that all rows of have independent and identical distributions. The reason is because the algorithm generates i.i.d. samples in line 7 and then for every , the row of the matrix is constructed by sampling in line 14 and then in line 20 from distributions that are solely determined by only, and is independent of the values of for . Let . Now let us partition the matrix as,\n\nA ~ = [ A ~ 0 A ~ 1 \u22ee A ~ q ] , ~ \ud835\udc34 matrix subscript ~ \ud835\udc34 0 subscript ~ \ud835\udc34 1 \u22ee subscript ~ \ud835\udc34 \ud835\udc5e \\widetilde{A}=\\begin{bmatrix}\\widetilde{A}_{0}\\\\\n\\widetilde{A}_{1}\\\\\n\\vdots\\\\\n\\widetilde{A}_{q}\\end{bmatrix},\n\nwhere is a matrix for every . Considering the action of the sampling matrix on matrix will ease notation, so we consider the matrix . Since every row of is identically distributed, let us consider the distribution of the row of for an arbitrary . Let be a random variable that takes values in with probability distribution , which is defined in line 6 of Algorithm 3. A random index generated in line 7 of the algorithm is a copy of the random variable . For any , let be a random variable that takes values in with probability distribution\n\nPr \u2061 [ T j = a ] = \u2016 P q \u2212 a \u22c5 M \u22c6 , j \u2016 F 2 / a ! \u2211 b = 0 q \u2016 P q \u2212 b \u22c5 M \u22c6 , j \u2016 F 2 / b ! , Pr superscript \ud835\udc47 \ud835\udc57 \ud835\udc4e superscript subscript norm \u22c5 subscript \ud835\udc43 \ud835\udc5e \ud835\udc4e subscript \ud835\udc40 \u22c6 \ud835\udc57 \ud835\udc39 2 \ud835\udc4e superscript subscript \ud835\udc4f 0 \ud835\udc5e superscript subscript norm \u22c5 subscript \ud835\udc43 \ud835\udc5e \ud835\udc4f subscript \ud835\udc40 \u22c6 \ud835\udc57 \ud835\udc39 2 \ud835\udc4f \\Pr[T^{j}=a]=\\frac{\\|P_{q-a}\\cdot M_{\\star,j}\\|_{F}^{2}/a!}{\\sum_{b=0}^{q}\\|P_{q-b}\\cdot M_{\\star,j}\\|_{F}^{2}/b!},\n\nwhere for are the matrices defined in line 4 and is the matrix defined in line 2 of the algorithm. The random sample generated in line 14 of the algorithm is a copy of the random variable . For any let be a vector random variable that takes values in with the following conditional probability distribution for every ,\n\nPr \u2061 [ I a j = i | I 1 j = i 1 , I 2 j = i 2 , \u22ef \u200b I a \u2212 1 j = i a \u2212 1 ] Pr subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e conditional \ud835\udc56 subscript superscript \ud835\udc3c \ud835\udc57 1 subscript \ud835\udc56 1 subscript superscript \ud835\udc3c \ud835\udc57 2 subscript \ud835\udc56 2 \u22ef subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e 1 subscript \ud835\udc56 \ud835\udc4e 1 \\displaystyle\\Pr[I^{j}_{a}=i|I^{j}_{1}=i_{1},I^{j}_{2}=i_{2},\\cdots I^{j}_{a-1}=i_{a-1}] = \u2016 W h \u200b ( i ) \u22c5 D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 \u2211 t = 1 s \u2032 \u2016 W t \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 \u200b \u2016 X i , \u22c6 \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 2 2 \u2016 X h \u2212 1 \u200b ( h \u200b ( i ) ) , \u22c6 \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 , absent superscript subscript norm \u22c5 subscript \ud835\udc4a \u210e \ud835\udc56 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 superscript subscript \ud835\udc61 1 superscript \ud835\udc60 \u2032 superscript subscript norm subscript \ud835\udc4a \ud835\udc61 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 superscript subscript norm subscript \ud835\udc4b \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top 2 2 superscript subscript norm subscript \ud835\udc4b superscript \u210e 1 \u210e \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 \\displaystyle=\\frac{\\|W_{h(i)}\\cdot D^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2}}{\\sum_{t=1}^{s^{\\prime}}\\|W_{t}D^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2}}\\frac{\\|X_{i,\\star}D^{a-1}P_{a+q-w}^{\\top}\\|_{2}^{2}}{\\|X_{h^{-1}(h(i)),\\star}D^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2}},\n\nwhere for are the matrices defined in line 11 of the algorithm and is a diagonal matrix of size whose diagonal entries are defined as,\n\nD r \u200b r a \u2212 1 = M r , j \u22c5 \u220f b = 1 a \u2212 1 X i b , r , subscript superscript \ud835\udc37 \ud835\udc4e 1 \ud835\udc5f \ud835\udc5f \u22c5 subscript \ud835\udc40 \ud835\udc5f \ud835\udc57 superscript subscript product \ud835\udc4f 1 \ud835\udc4e 1 subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc4f \ud835\udc5f D^{a-1}_{rr}=M_{r,j}\\cdot\\prod_{b=1}^{a-1}X_{i_{b},r},\n\nfor every and . For ease of notation we drop the superscript from and instead write . It follows that the vector random variable obtained by putting together the random indices generated in line 20 of the algorithm, is a copy of the random variable . Now we are ready to calculate the distribution of the row of , which we denote by . Let be the quantity that the for loop in lines 24-33 of the algorithm computes. If we let be the random number generated in line 14 and if we let be the indices sampled in line 20 of the algorithm, then we can compute the value of as follows,\n\n\u03b2 = s \u200b \u2211 j = 1 d \u2032 Pr \u2061 [ J = j ] \u200b Pr \u2061 [ T j = w ] \u22c5 \u220f b = 1 q p b \u2217 \u200b q b \u2217 , \ud835\udefd \ud835\udc60 superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 \u22c5 Pr \ud835\udc3d \ud835\udc57 Pr superscript \ud835\udc47 \ud835\udc57 \ud835\udc64 superscript subscript product \ud835\udc4f 1 \ud835\udc5e subscript superscript \ud835\udc5d \ud835\udc4f subscript superscript \ud835\udc5e \ud835\udc4f \\beta=s\\sum_{j=1}^{d^{\\prime}}\\Pr[J=j]\\Pr[T^{j}=w]\\cdot\\prod_{b=1}^{q}p^{*}_{b}q^{*}_{b},\n\nwhere the quantities and are computed in lines 28 and 29 of the algorithm. Hence, the distribution of is the following,\n\nPr \u2061 [ [ S \u200b A ~ ] l , \u22c6 = \u03b2 \u2212 1 / 2 \u22c5 [ A ~ w ] ( i 1 , i 2 , \u22ef \u200b i w ) , \u22c6 ] Pr subscript delimited-[] \ud835\udc46 ~ \ud835\udc34 \ud835\udc59 \u22c6 \u22c5 superscript \ud835\udefd 1 2 subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 \\displaystyle\\Pr\\left[[S\\widetilde{A}]_{l,\\star}=\\beta^{-1/2}\\cdot[\\widetilde{A}_{w}]_{(i_{1},i_{2},\\cdots i_{w}),\\star}\\right] = \u2211 j = 1 d \u2032 Pr \u2061 [ [ S \u200b A ~ ] l , \u22c6 = [ A ~ w ] ( i 1 , \u22ef \u200b i w ) , \u22c6 \u03b2 | w , j ] \u200b Pr \u2061 [ T j = w ] \u200b p j absent superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 Pr subscript delimited-[] \ud835\udc46 ~ \ud835\udc34 \ud835\udc59 \u22c6 conditional subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 subscript \ud835\udc56 1 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 \ud835\udefd \ud835\udc64 \ud835\udc57 Pr superscript \ud835\udc47 \ud835\udc57 \ud835\udc64 subscript \ud835\udc5d \ud835\udc57 \\displaystyle=\\sum_{j=1}^{d^{\\prime}}\\Pr\\left[\\left.[S\\widetilde{A}]_{l,\\star}=\\frac{[\\widetilde{A}_{w}]_{(i_{1},\\cdots i_{w}),\\star}}{\\sqrt{\\beta}}\\right|w,j\\right]\\Pr[T^{j}=w]p_{j} = \u2211 j = 1 d \u2032 Pr \u2061 [ I j = ( i 1 , i 2 , \u22ef \u200b i w ) ] \u200b Pr \u2061 [ T j = w ] \u22c5 p j absent superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 \u22c5 Pr superscript \ud835\udc3c \ud835\udc57 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 Pr superscript \ud835\udc47 \ud835\udc57 \ud835\udc64 subscript \ud835\udc5d \ud835\udc57 \\displaystyle=\\sum_{j=1}^{d^{\\prime}}\\Pr\\left[I^{j}=(i_{1},i_{2},\\cdots i_{w})\\right]\\Pr[T^{j}=w]\\cdot p_{j} = \u2211 j = 1 d \u2032 Pr \u2061 [ T j = w ] \u200b p j \u200b \u220f a = 1 w Pr \u2061 [ I a j = i a | i 1 \u200b i 2 \u200b \u22ef \u200b i a \u2212 1 ] absent superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 Pr superscript \ud835\udc47 \ud835\udc57 \ud835\udc64 subscript \ud835\udc5d \ud835\udc57 superscript subscript product \ud835\udc4e 1 \ud835\udc64 Pr subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e conditional subscript \ud835\udc56 \ud835\udc4e subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc4e 1 \\displaystyle=\\sum_{j=1}^{d^{\\prime}}\\Pr[T^{j}=w]p_{j}\\prod_{a=1}^{w}\\Pr\\left[I^{j}_{a}=i_{a}|i_{1}i_{2}\\cdots i_{a-1}\\right] (8)\n\nwhere .",
    "0051d15d5f3849846707eeb91677c80366f17960-27": "Therefore, . Now note that for any , the matrix is defined as where is a matrix with i.i.d. Gaussian entries with rows. Therefore is a Johnson-Lindenstrauss transform and hence for every and every , the following holds with high probability,\n\n\u2016 W r \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 superscript subscript norm subscript \ud835\udc4a \ud835\udc5f superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 \\displaystyle\\|W_{r}D^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2} (9) \u2208 ( 1 \u00b1 0.1 / q ) \u200b n \u2032 \u22c5 \u2016 X h \u2212 1 \u200b ( r ) , \u22c6 \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 . absent \u22c5 plus-or-minus 1 0.1 \ud835\udc5e superscript \ud835\udc5b \u2032 superscript subscript norm subscript \ud835\udc4b superscript \u210e 1 \ud835\udc5f \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 \\displaystyle\\qquad\\in\\left(1\\pm{0.1}/{q}\\right)n^{\\prime}\\cdot\\|X_{h^{-1}(r),\\star}D^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2}. By a union bound over events, (9) holds simultaneously for all , all , and all with high probability. We condition on (9) holding in what follows. Therefore, we can bound the conditional probability of as follows,\n\nPr \u2061 [ I a j = i | I 1 j = i 1 , I 2 j = i 2 , \u22ef \u200b I a \u2212 1 j = i a \u2212 1 ] Pr subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e conditional \ud835\udc56 subscript superscript \ud835\udc3c \ud835\udc57 1 subscript \ud835\udc56 1 subscript superscript \ud835\udc3c \ud835\udc57 2 subscript \ud835\udc56 2 \u22ef subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e 1 subscript \ud835\udc56 \ud835\udc4e 1 \\displaystyle\\Pr[I^{j}_{a}=i|I^{j}_{1}=i_{1},I^{j}_{2}=i_{2},\\cdots I^{j}_{a-1}=i_{a-1}] = \u2016 W h \u200b ( i ) \u22c5 D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 \u2211 t = 1 s \u2032 \u2016 W t \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 \u200b \u2016 X i , \u22c6 \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 2 2 \u2016 X h \u2212 1 \u200b ( h \u200b ( i ) ) , \u22c6 \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 absent superscript subscript norm \u22c5 subscript \ud835\udc4a \u210e \ud835\udc56 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 superscript subscript \ud835\udc61 1 superscript \ud835\udc60 \u2032 superscript subscript norm subscript \ud835\udc4a \ud835\udc61 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 superscript subscript norm subscript \ud835\udc4b \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top 2 2 superscript subscript norm subscript \ud835\udc4b superscript \u210e 1 \u210e \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 \\displaystyle=\\frac{\\|W_{h(i)}\\cdot D^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2}}{\\sum_{t=1}^{s^{\\prime}}\\|W_{t}D^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2}}\\frac{\\|X_{i,\\star}D^{a-1}P_{a+q-w}^{\\top}\\|_{2}^{2}}{\\|X_{h^{-1}(h(i)),\\star}D^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2}} \u2265 ( 1 \u2212 1 10 \u200b q ) \u200b \u2016 X i , \u22c6 \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 2 2 ( 1 + 1 10 \u200b q ) \u200b \u2016 X \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 absent 1 1 10 \ud835\udc5e superscript subscript norm subscript \ud835\udc4b \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top 2 2 1 1 10 \ud835\udc5e superscript subscript norm \ud835\udc4b superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 \\displaystyle\\geq\\frac{(1-\\frac{1}{10q})\\|X_{i,\\star}D^{a-1}P_{a+q-w}^{\\top}\\|_{2}^{2}}{(1+\\frac{1}{10q})\\|XD^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2}} \u2265 ( 1 \u2212 1 5 \u200b q ) \u200b \u2016 X i , \u22c6 \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 2 2 \u2016 X \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 F 2 . absent 1 1 5 \ud835\udc5e superscript subscript norm subscript \ud835\udc4b \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top 2 2 superscript subscript norm \ud835\udc4b superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top \ud835\udc39 2 \\displaystyle\\geq\\left(1-\\frac{1}{5q}\\right)\\frac{\\|X_{i,\\star}D^{a-1}P_{a+q-w}^{\\top}\\|_{2}^{2}}{\\|XD^{a-1}P_{a+q-w}^{\\top}\\|_{F}^{2}}. Now we invoke Lemma 4. For every , is defined as , where is the sketch from Lemma 4 with . We can write for every ,\n\nP a + q \u2212 w \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top \\displaystyle P_{a+q-w}D^{a-1}X_{i,\\star}^{\\top} = Q q \u200b ( X \u2297 ( w \u2212 a ) \u2297 E 1 \u2297 a + q \u2212 w ) \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 absent superscript \ud835\udc44 \ud835\udc5e tensor-product superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript subscript \ud835\udc38 1 tensor-product absent \ud835\udc4e \ud835\udc5e \ud835\udc64 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top \\displaystyle\\qquad=Q^{q}\\left(X^{\\otimes(w-a)}\\otimes E_{1}^{\\otimes a+q-w}\\right)D^{a-1}X_{i,\\star}^{\\top} = Q q \u200b ( ( X \u2297 ( w \u2212 a ) \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 ) \u2297 \ud835\udc1e 1 \u2297 a + q \u2212 w ) . absent superscript \ud835\udc44 \ud835\udc5e tensor-product superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top superscript subscript \ud835\udc1e 1 tensor-product absent \ud835\udc4e \ud835\udc5e \ud835\udc64 \\displaystyle\\qquad=Q^{q}\\left(\\left(X^{\\otimes(w-a)}D^{a-1}X_{i,\\star}^{\\top}\\right)\\otimes{\\bf e}_{1}^{\\otimes a+q-w}\\right). Hence, if we invoke Lemma 4 we get that for every and every , the following holds with high probability\n\n\u2016 X i , \u22c6 \u200b D a \u2212 1 \u200b P a + q \u2212 w \u22a4 \u2016 2 2 \u2208 \u2016 X \u2297 ( w \u2212 a ) \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 \u2016 2 2 1 \u00b1 0.1 / q . superscript subscript norm subscript \ud835\udc4b \ud835\udc56 \u22c6 superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc43 \ud835\udc4e \ud835\udc5e \ud835\udc64 top 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top 2 2 plus-or-minus 1 0.1 \ud835\udc5e \\left\\|X_{i,\\star}D^{a-1}P_{a+q-w}^{\\top}\\right\\|_{2}^{2}\\in\\frac{\\left\\|X^{\\otimes(w-a)}D^{a-1}X_{i,\\star}^{\\top}\\right\\|_{2}^{2}}{1\\pm 0.1/q}. (10)\n\nMoreover,\n\nP q \u2212 w \u22c5 M \u22c6 , j = Q q \u200b ( ( X \u2297 w \u200b M \u22c6 , j ) \u2297 \ud835\udc1e 1 \u2297 q \u2212 w ) , \u22c5 subscript \ud835\udc43 \ud835\udc5e \ud835\udc64 subscript \ud835\udc40 \u22c6 \ud835\udc57 superscript \ud835\udc44 \ud835\udc5e tensor-product superscript \ud835\udc4b tensor-product absent \ud835\udc64 subscript \ud835\udc40 \u22c6 \ud835\udc57 superscript subscript \ud835\udc1e 1 tensor-product absent \ud835\udc5e \ud835\udc64 P_{q-w}\\cdot M_{\\star,j}=Q^{q}\\left(\\left(X^{\\otimes w}M_{\\star,j}\\right)\\otimes{\\bf e}_{1}^{\\otimes q-w}\\right),\n\nand hence, by Lemma 4, for every and every , the following holds with high probability,\n\n\u2016 P q \u2212 w \u22c5 M \u22c6 , j \u2016 F 2 \u2208 ( 1 \u00b1 1 10 \u200b q ) \u200b \u2016 X \u2297 w \u22c5 M \u22c6 , j \u2016 F 2 . superscript subscript norm \u22c5 subscript \ud835\udc43 \ud835\udc5e \ud835\udc64 subscript \ud835\udc40 \u22c6 \ud835\udc57 \ud835\udc39 2 plus-or-minus 1 1 10 \ud835\udc5e superscript subscript norm \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc64 subscript \ud835\udc40 \u22c6 \ud835\udc57 \ud835\udc39 2 \\|P_{q-w}\\cdot M_{\\star,j}\\|_{F}^{2}\\in\\left(1\\pm\\frac{1}{10q}\\right){\\|X^{\\otimes w}\\cdot M_{\\star,j}\\|_{F}^{2}}. (11)\n\nBy union bounding over events we have that with high probability, both (10) and (11) hold simultaneously for all , all , all , and all . Therefore, conditioning on (10) and (11) holding, we have the following two bounds for the conditional probability of as well as the conditional probability ,\n\nPr \u2061 [ I a j = i | I 1 j = i 1 , I 2 j = i 2 , \u22ef \u200b I a \u2212 1 j = i a \u2212 1 ] Pr subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e conditional \ud835\udc56 subscript superscript \ud835\udc3c \ud835\udc57 1 subscript \ud835\udc56 1 subscript superscript \ud835\udc3c \ud835\udc57 2 subscript \ud835\udc56 2 \u22ef subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e 1 subscript \ud835\udc56 \ud835\udc4e 1 \\displaystyle\\Pr[I^{j}_{a}=i|I^{j}_{1}=i_{1},I^{j}_{2}=i_{2},\\cdots I^{j}_{a-1}=i_{a-1}] \u2265 ( 1 \u2212 2 5 \u200b q ) \u200b \u2016 X \u2297 ( w \u2212 a ) \u200b D a \u2212 1 \u200b X i , \u22c6 \u22a4 \u2016 2 2 \u2016 X \u2297 ( w \u2212 a ) \u200b D a \u2212 1 \u200b X \u22a4 \u2016 F 2 . absent 1 2 5 \ud835\udc5e superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b \ud835\udc56 \u22c6 top 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\qquad\\geq\\left(1-\\frac{2}{5q}\\right)\\frac{\\|X^{\\otimes(w-a)}D^{a-1}X_{i,\\star}^{\\top}\\|_{2}^{2}}{\\|X^{\\otimes(w-a)}D^{a-1}X^{\\top}\\|_{F}^{2}}. and,\n\nPr \u2061 [ T j = w ] Pr superscript \ud835\udc47 \ud835\udc57 \ud835\udc64 \\displaystyle\\Pr[T^{j}=w] \u2208 ( 1 \u00b1 1 / 5 \u200b q ) \u200b \u2016 X \u2297 w \u22c5 M \u22c6 , j \u2016 F 2 / w ! \u2211 b = 0 q \u2016 X \u2297 b \u22c5 M \u22c6 , j \u2016 F 2 / b ! absent plus-or-minus 1 1 5 \ud835\udc5e superscript subscript norm \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc64 subscript \ud835\udc40 \u22c6 \ud835\udc57 \ud835\udc39 2 \ud835\udc64 superscript subscript \ud835\udc4f 0 \ud835\udc5e superscript subscript norm \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc4f subscript \ud835\udc40 \u22c6 \ud835\udc57 \ud835\udc39 2 \ud835\udc4f \\displaystyle\\in\\frac{\\left(1\\pm{1}/{5q}\\right)\\|X^{\\otimes w}\\cdot M_{\\star,j}\\|_{F}^{2}/w!}{\\sum_{b=0}^{q}\\|X^{\\otimes b}\\cdot M_{\\star,j}\\|_{F}^{2}/b!} = ( 1 \u00b1 1 / 5 \u200b q ) \u200b \u2016 [ A ~ w \u200b H ] \u22c6 , j \u2016 2 2 \u2016 [ A ~ \u200b H ] \u22c6 , j \u2016 2 2 absent plus-or-minus 1 1 5 \ud835\udc5e superscript subscript norm subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b \u22c6 \ud835\udc57 2 2 superscript subscript norm subscript delimited-[] ~ \ud835\udc34 \ud835\udc3b \u22c6 \ud835\udc57 2 2 \\displaystyle=\\left(1\\pm{1}/{5q}\\right)\\frac{\\left\\|[\\widetilde{A}_{w}H]_{\\star,j}\\right\\|_{2}^{2}}{\\|[\\widetilde{A}H]_{\\star,j}\\|_{2}^{2}} (12)\n\nAlso we use the following fact that follows from the definition of tensor products and the definition of matrix ,\n\n\u2016 X \u2297 ( w \u2212 a \u2212 1 ) \u200b D a \u200b X \u22a4 \u2016 F 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e 1 superscript \ud835\udc37 \ud835\udc4e superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\left\\|X^{\\otimes(w-a-1)}D^{a}X^{\\top}\\right\\|_{F}^{2} = \u2016 X \u2297 ( w \u2212 a \u2212 1 ) \u200b D a \u2212 1 \u22c5 diag \u200b ( X i a , \u22c6 ) \u200b X \u22a4 \u2016 F 2 absent superscript subscript norm \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e 1 superscript \ud835\udc37 \ud835\udc4e 1 diag subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc4e \u22c6 superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\qquad=\\left\\|X^{\\otimes(w-a-1)}D^{a-1}\\cdot\\text{diag}(X_{i_{a},\\star})X^{\\top}\\right\\|_{F}^{2} = \u2016 X \u2297 ( w \u2212 a ) \u200b D a \u2212 1 \u200b X i a , \u22c6 \u22a4 \u2016 2 2 absent superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc4e \u22c6 top 2 2 \\displaystyle\\qquad=\\left\\|X^{\\otimes(w-a)}D^{a-1}X_{i_{a},\\star}^{\\top}\\right\\|_{2}^{2}\n\nNow we compute the following product of the conditional probabilities\n\n\u220f a = 1 w Pr \u2061 [ I a j = i a | I 1 j = i 1 , \u22ef \u200b I a \u2212 1 j = i a \u2212 1 ] superscript subscript product \ud835\udc4e 1 \ud835\udc64 Pr subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e conditional subscript \ud835\udc56 \ud835\udc4e subscript superscript \ud835\udc3c \ud835\udc57 1 subscript \ud835\udc56 1 \u22ef subscript superscript \ud835\udc3c \ud835\udc57 \ud835\udc4e 1 subscript \ud835\udc56 \ud835\udc4e 1 \\displaystyle\\prod_{a=1}^{w}\\Pr\\left[I^{j}_{a}=i_{a}|I^{j}_{1}=i_{1},\\cdots I^{j}_{a-1}=i_{a-1}\\right] \u2265 \u220f a = 1 w ( 1 \u2212 2 5 \u200b q ) \u200b \u2016 X \u2297 ( w \u2212 a ) \u200b D a \u2212 1 \u200b X i a , \u22c6 \u22a4 \u2016 2 2 \u2016 X \u2297 ( w \u2212 a ) \u200b D a \u2212 1 \u200b X \u22a4 \u2016 F 2 absent superscript subscript product \ud835\udc4e 1 \ud835\udc64 1 2 5 \ud835\udc5e superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc4e \u22c6 top 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\qquad\\geq\\prod_{a=1}^{w}\\left(1-\\frac{2}{5q}\\right)\\frac{\\|X^{\\otimes(w-a)}D^{a-1}X_{i_{a},\\star}^{\\top}\\|_{2}^{2}}{\\|X^{\\otimes(w-a)}D^{a-1}X^{\\top}\\|_{F}^{2}} \u2265 1 2 \u22c5 \u220f a = 1 w \u2016 X \u2297 ( w \u2212 a ) \u200b D a \u2212 1 \u200b X i a , \u22c6 \u22a4 \u2016 2 2 \u2016 X \u2297 ( w \u2212 a ) \u200b D a \u2212 1 \u200b X \u22a4 \u2016 F 2 absent \u22c5 1 2 superscript subscript product \ud835\udc4e 1 \ud835\udc64 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc4e \u22c6 top 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 \ud835\udc4e superscript \ud835\udc37 \ud835\udc4e 1 superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\qquad\\geq\\frac{1}{2}\\cdot\\prod_{a=1}^{w}\\frac{\\|X^{\\otimes(w-a)}D^{a-1}X_{i_{a},\\star}^{\\top}\\|_{2}^{2}}{\\|X^{\\otimes(w-a)}D^{a-1}X^{\\top}\\|_{F}^{2}} = 1 2 \u22c5 \u2016 X \u2297 ( 0 ) \u200b D q \u2212 1 \u200b X i q , \u22c6 \u22a4 \u2016 2 2 \u2016 X \u2297 ( w \u2212 1 ) \u200b D 0 \u200b X \u22a4 \u2016 F 2 absent \u22c5 1 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent 0 superscript \ud835\udc37 \ud835\udc5e 1 superscript subscript \ud835\udc4b subscript \ud835\udc56 \ud835\udc5e \u22c6 top 2 2 superscript subscript norm superscript \ud835\udc4b tensor-product absent \ud835\udc64 1 superscript \ud835\udc37 0 superscript \ud835\udc4b top \ud835\udc39 2 \\displaystyle\\qquad=\\frac{1}{2}\\cdot\\frac{\\|X^{\\otimes(0)}D^{q-1}X_{i_{q},\\star}^{\\top}\\|_{2}^{2}}{\\|X^{\\otimes(w-1)}D^{0}X^{\\top}\\|_{F}^{2}} = 1 2 \u22c5 | \u27e8 [ X \u2297 w ] ( i 1 , i 2 , \u22ef \u200b i w ) , \u22c6 , M \u22c6 , j \u27e9 | 2 \u2016 X \u2297 w \u22c5 M \u22c6 , j \u2016 2 2 absent \u22c5 1 2 superscript subscript delimited-[] superscript \ud835\udc4b tensor-product absent \ud835\udc64 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 subscript \ud835\udc40 \u22c6 \ud835\udc57 2 superscript subscript norm \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc64 subscript \ud835\udc40 \u22c6 \ud835\udc57 2 2 \\displaystyle\\qquad=\\frac{1}{2}\\cdot\\frac{\\left|\\langle[X^{\\otimes w}]_{(i_{1},i_{2},\\cdots i_{w}),\\star},M_{\\star,j}\\rangle\\right|^{2}}{\\|X^{\\otimes w}\\cdot M_{\\star,j}\\|_{2}^{2}} = 1 2 \u22c5 | [ A ~ w \u200b H ] ( i 1 , i 2 , \u22ef \u200b i w ) , j | 2 \u2016 [ A ~ w \u200b H ] \u22c6 , j \u2016 2 2 absent \u22c5 1 2 superscript subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \ud835\udc57 2 superscript subscript norm subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b \u22c6 \ud835\udc57 2 2 \\displaystyle\\qquad=\\frac{1}{2}\\cdot\\frac{\\left|[\\widetilde{A}_{w}H]_{(i_{1},i_{2},\\cdots i_{w}),j}\\right|^{2}}{\\|[\\widetilde{A}_{w}H]_{\\star,j}\\|_{2}^{2}} (13)\n\nBy plugging (12) and (13) back in (8) we get that,\n\nPr \u2061 [ [ S \u200b A ~ ] l , \u22c6 = \u03b2 \u2212 1 / 2 \u22c5 [ A ~ w ] ( i 1 , i 2 , \u22ef \u200b i w ) , \u22c6 ] Pr subscript delimited-[] \ud835\udc46 ~ \ud835\udc34 \ud835\udc59 \u22c6 \u22c5 superscript \ud835\udefd 1 2 subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 \\displaystyle\\Pr\\left[[S\\widetilde{A}]_{l,\\star}=\\beta^{-1/2}\\cdot[\\widetilde{A}_{w}]_{(i_{1},i_{2},\\cdots i_{w}),\\star}\\right] \u2265 \u2211 j = 1 d \u2032 1 \u2212 1 5 \u200b q 2 \u22c5 \u2016 [ A ~ w \u200b H ] \u22c6 , j \u2016 2 2 \u2016 [ A ~ \u200b H ] \u22c6 , j \u2016 2 2 \u200b | [ A ~ w \u200b H ] ( i 1 , i 2 , \u22ef \u200b i w ) , j | 2 \u2016 [ A ~ w \u200b H ] \u22c6 , j \u2016 2 2 \u200b p j absent superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 \u22c5 1 1 5 \ud835\udc5e 2 superscript subscript norm subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b \u22c6 \ud835\udc57 2 2 superscript subscript norm subscript delimited-[] ~ \ud835\udc34 \ud835\udc3b \u22c6 \ud835\udc57 2 2 superscript subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \ud835\udc57 2 superscript subscript norm subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b \u22c6 \ud835\udc57 2 2 subscript \ud835\udc5d \ud835\udc57 \\displaystyle\\qquad\\geq\\sum_{j=1}^{d^{\\prime}}\\frac{1-\\frac{1}{5q}}{2}\\cdot\\frac{\\|[\\widetilde{A}_{w}H]_{\\star,j}\\|_{2}^{2}}{\\left\\|[\\widetilde{A}H]_{\\star,j}\\right\\|_{2}^{2}}\\frac{\\left|[\\widetilde{A}_{w}H]_{(i_{1},i_{2},\\cdots i_{w}),j}\\right|^{2}}{\\left\\|[\\widetilde{A}_{w}H]_{\\star,j}\\right\\|_{2}^{2}}p_{j} = \u2211 j = 1 d \u2032 1 \u2212 1 5 \u200b q 2 \u22c5 | [ A ~ w \u200b H ] ( i 1 , i 2 , \u22ef \u200b i w ) , j | 2 \u2016 [ A ~ \u200b H ] \u22c6 , j \u2016 2 2 \u200b p j absent superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 \u22c5 1 1 5 \ud835\udc5e 2 superscript subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \ud835\udc57 2 superscript subscript norm subscript delimited-[] ~ \ud835\udc34 \ud835\udc3b \u22c6 \ud835\udc57 2 2 subscript \ud835\udc5d \ud835\udc57 \\displaystyle\\qquad=\\sum_{j=1}^{d^{\\prime}}\\frac{1-\\frac{1}{5q}}{2}\\cdot\\frac{\\left|[\\widetilde{A}_{w}H]_{(i_{1},i_{2},\\cdots i_{w}),j}\\right|^{2}}{\\left\\|[\\widetilde{A}H]_{\\star,j}\\right\\|_{2}^{2}}p_{j} (14)\n\nNow we bound , which is defined in line 6 of the algorithm as follows,\n\np j subscript \ud835\udc5d \ud835\udc57 \\displaystyle p_{j} = \u2016 Z \u22c6 , j \u2016 2 2 \u2016 Z \u2016 F 2 absent superscript subscript norm subscript \ud835\udc4d \u22c6 \ud835\udc57 2 2 superscript subscript norm \ud835\udc4d \ud835\udc39 2 \\displaystyle=\\frac{\\|Z_{\\star,j}\\|_{2}^{2}}{\\|Z\\|_{F}^{2}} = \u2211 b = 0 q \u2016 P q \u2212 b \u22c5 M \u22c6 , j \u2016 2 2 / b !",
    "0051d15d5f3849846707eeb91677c80366f17960-28": "\u2211 b = 0 q \u2016 P q \u2212 b \u22c5 M \u2016 F 2 / b ! absent superscript subscript \ud835\udc4f 0 \ud835\udc5e superscript subscript norm \u22c5 subscript \ud835\udc43 \ud835\udc5e \ud835\udc4f subscript \ud835\udc40 \u22c6 \ud835\udc57 2 2 \ud835\udc4f superscript subscript \ud835\udc4f 0 \ud835\udc5e superscript subscript norm \u22c5 subscript \ud835\udc43 \ud835\udc5e \ud835\udc4f \ud835\udc40 \ud835\udc39 2 \ud835\udc4f \\displaystyle=\\frac{\\sum_{b=0}^{q}\\|P_{q-b}\\cdot M_{\\star,j}\\|_{2}^{2}/b!}{\\sum_{b=0}^{q}\\|P_{q-b}\\cdot M\\|_{F}^{2}/b!} \u2265 ( 1 \u2212 1 / 5 \u200b q ) \u200b \u2211 b = 0 q \u2016 X \u2297 b \u22c5 M \u22c6 , j \u2016 2 2 / b ! \u2211 b = 0 q \u2016 X \u2297 b \u22c5 M \u2016 F 2 / b ! absent 1 1 5 \ud835\udc5e superscript subscript \ud835\udc4f 0 \ud835\udc5e superscript subscript norm \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc4f subscript \ud835\udc40 \u22c6 \ud835\udc57 2 2 \ud835\udc4f superscript subscript \ud835\udc4f 0 \ud835\udc5e superscript subscript norm \u22c5 superscript \ud835\udc4b tensor-product absent \ud835\udc4f \ud835\udc40 \ud835\udc39 2 \ud835\udc4f \\displaystyle\\geq(1-1/5q)\\frac{\\sum_{b=0}^{q}\\|X^{\\otimes b}\\cdot M_{\\star,j}\\|_{2}^{2}/b!}{\\sum_{b=0}^{q}\\|X^{\\otimes b}\\cdot M\\|_{F}^{2}/b!} = ( 1 \u2212 1 / 5 \u200b q ) \u200b \u2016 [ A ~ \u200b H ] \u22c6 , j \u2016 2 2 \u2016 A ~ \u200b H \u2016 F 2 , absent 1 1 5 \ud835\udc5e superscript subscript norm subscript delimited-[] ~ \ud835\udc34 \ud835\udc3b \u22c6 \ud835\udc57 2 2 superscript subscript norm ~ \ud835\udc34 \ud835\udc3b \ud835\udc39 2 \\displaystyle=(1-1/5q)\\frac{\\left\\|[\\widetilde{A}H]_{\\star,j}\\right\\|_{2}^{2}}{\\left\\|\\widetilde{A}H\\right\\|_{F}^{2}},\n\nwhere the inequality above follows from (11). Plugging the above into (14), we get that,\n\nPr \u2061 [ [ S \u200b A ~ ] l , \u22c6 = \u03b2 \u2212 1 / 2 \u22c5 [ A ~ w ] ( i 1 , i 2 , \u22ef \u200b i w ) , \u22c6 ] Pr subscript delimited-[] \ud835\udc46 ~ \ud835\udc34 \ud835\udc59 \u22c6 \u22c5 superscript \ud835\udefd 1 2 subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 \\displaystyle\\Pr\\left[[S\\widetilde{A}]_{l,\\star}=\\beta^{-1/2}\\cdot[\\widetilde{A}_{w}]_{(i_{1},i_{2},\\cdots i_{w}),\\star}\\right] \u2265 \u2211 j = 1 d \u2032 1 \u2212 2 5 \u200b q 2 \u22c5 | [ A ~ w \u200b H ] ( i 1 , i 2 , \u22ef \u200b i w ) , j | 2 \u2016 [ A ~ \u200b H ] \u22c6 , j \u2016 2 2 \u200b \u2016 [ A ~ \u200b H ] \u22c6 , j \u2016 2 2 \u2016 A ~ \u200b H \u2016 F 2 absent superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 \u22c5 1 2 5 \ud835\udc5e 2 superscript subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \ud835\udc57 2 superscript subscript norm subscript delimited-[] ~ \ud835\udc34 \ud835\udc3b \u22c6 \ud835\udc57 2 2 superscript subscript norm subscript delimited-[] ~ \ud835\udc34 \ud835\udc3b \u22c6 \ud835\udc57 2 2 superscript subscript norm ~ \ud835\udc34 \ud835\udc3b \ud835\udc39 2 \\displaystyle\\geq\\sum_{j=1}^{d^{\\prime}}\\frac{1-\\frac{2}{5q}}{2}\\cdot\\frac{\\left|[\\widetilde{A}_{w}H]_{(i_{1},i_{2},\\cdots i_{w}),j}\\right|^{2}}{\\left\\|[\\widetilde{A}H]_{\\star,j}\\right\\|_{2}^{2}}\\frac{\\left\\|[\\widetilde{A}H]_{\\star,j}\\right\\|_{2}^{2}}{\\left\\|\\widetilde{A}H\\right\\|_{F}^{2}} \u2265 1 3 \u22c5 \u2211 j = 1 d \u2032 | [ A ~ w \u200b H ] ( i 1 , i 2 , \u22ef \u200b i w ) , j | 2 \u2016 A ~ \u200b H \u2016 F 2 absent \u22c5 1 3 superscript subscript \ud835\udc57 1 superscript \ud835\udc51 \u2032 superscript subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \ud835\udc57 2 superscript subscript norm ~ \ud835\udc34 \ud835\udc3b \ud835\udc39 2 \\displaystyle\\geq\\frac{1}{3}\\cdot\\sum_{j=1}^{d^{\\prime}}\\frac{\\left|[\\widetilde{A}_{w}H]_{(i_{1},i_{2},\\cdots i_{w}),j}\\right|^{2}}{\\left\\|\\widetilde{A}H\\right\\|_{F}^{2}} = 1 3 \u22c5 \u2016 [ A ~ w \u200b H ] ( i 1 , i 2 , \u22ef \u200b i w ) , \u22c6 \u2016 2 2 \u2016 A ~ \u200b H \u2016 F 2 absent \u22c5 1 3 superscript subscript norm subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 2 2 superscript subscript norm ~ \ud835\udc34 \ud835\udc3b \ud835\udc39 2 \\displaystyle=\\frac{1}{3}\\cdot\\frac{\\left\\|[\\widetilde{A}_{w}H]_{(i_{1},i_{2},\\cdots i_{w}),\\star}\\right\\|_{2}^{2}}{\\left\\|\\widetilde{A}H\\right\\|_{F}^{2}}\n\nNow note that is a matrix with i.i.d. Gaussian entries with columns, and therefore is a Johnson-Lindenstrauss transform, and hence for every and every , with probability ,\n\n\u2016 [ A ~ w \u200b H ] ( i 1 , i 2 , \u22ef \u200b i w ) , \u22c6 \u2016 2 2 \u2208 d \u2032 \u200b ( 1 \u00b1 0.1 ) \u200b \u2016 [ A ~ w ] ( i 1 , i 2 , \u22ef \u200b i w ) , \u22c6 \u2016 2 2 . superscript subscript norm subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 \ud835\udc3b subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 2 2 superscript \ud835\udc51 \u2032 plus-or-minus 1 0.1 superscript subscript norm subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 2 2 \\left\\|[\\widetilde{A}_{w}H]_{(i_{1},i_{2},\\cdots i_{w}),\\star}\\right\\|_{2}^{2}\\in d^{\\prime}\\left(1\\pm 0.1\\right)\\left\\|[\\widetilde{A}_{w}]_{(i_{1},i_{2},\\cdots i_{w}),\\star}\\right\\|_{2}^{2}. Therefore, by union bounding over events, the above holds simultaneously for all and all with high probability. Therefore,\n\nPr \u2061 [ [ S \u200b A ~ ] l , \u22c6 = \u03b2 \u2212 1 / 2 \u22c5 [ A ~ w ] ( i 1 , i 2 , \u22ef \u200b i w ) , \u22c6 ] Pr subscript delimited-[] \ud835\udc46 ~ \ud835\udc34 \ud835\udc59 \u22c6 \u22c5 superscript \ud835\udefd 1 2 subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 \\displaystyle\\Pr\\left[[S\\widetilde{A}]_{l,\\star}=\\beta^{-1/2}\\cdot[\\widetilde{A}_{w}]_{(i_{1},i_{2},\\cdots i_{w}),\\star}\\right] \u2265 1 4 \u22c5 \u2016 [ A ~ w ] ( i 1 , i 2 , \u22ef \u200b i w ) , \u22c6 \u2016 2 2 \u2016 A \u2016 F 2 . absent \u22c5 1 4 superscript subscript norm subscript delimited-[] subscript ~ \ud835\udc34 \ud835\udc64 subscript \ud835\udc56 1 subscript \ud835\udc56 2 \u22ef subscript \ud835\udc56 \ud835\udc64 \u22c6 2 2 superscript subscript norm \ud835\udc34 \ud835\udc39 2 \\displaystyle\\qquad\\geq\\frac{1}{4}\\cdot\\frac{\\left\\|[\\widetilde{A}_{w}]_{(i_{1},i_{2},\\cdots i_{w}),\\star}\\right\\|_{2}^{2}}{\\|A\\|_{F}^{2}}. Because is the probability of sampling row of the block of the matrix , the above inequality proves that with high probability, is a rank- row norm sampler for as in Definition 3.1. Runtime:\n\nThe operations that this algorithm perform largely overlap with that of Algorithm 2 with a few additional operations. One of the additional computations in this algorithm is the computation of the matrix in line 5 of the algorithm, which takes operations. Another additional computational part of the algorithm is the computation of , for , in line 13 of the algorithm, that can be computed in time time for a fixed . Therefore, the total time to compute this distribution for all is . Finally the last additional computation is the computation of the quantity in line 25 of the algorithm which takes time for a fixed and a fixed . Hence the total time of this operation for all and is . The total runtime of Algorithm 3 is the sum of these terms and the runtime of Algorithm 2, which results in runtime. \u220e\n\nAppendix G Proof of Theorem 2\n\nLet for a large enough constant . Let be the degree- polynomial lifting for the Gaussian kernel as in Definition 3.2. Let be the matrix with columns whose columns are obtained by applying the lifting on the data points, i.e., for all . First of all, note that by Claim 6, since we assumed ,\n\n\u2016 \u03a6 \u22a4 \u200b \u03a6 \u2212 K \u2016 o \u200b p \u2264 \u03f5 2 \u200b \u03bb . subscript norm superscript \u03a6 top \u03a6 \ud835\udc3e \ud835\udc5c \ud835\udc5d italic-\u03f5 2 \ud835\udf06 \\|\\Phi^{\\top}\\Phi-K\\|_{op}\\leq\\frac{\\epsilon}{2}\\lambda. The algorithm finds a spectrally close surrogate for the Gaussian kernel matrix by invoking the recursive leverage score sampling procedure of Algorithm 1 with inputs , , , and . For every invocation of the primitive RowNormSampler by Algorithm 1, we run Algorithm 3, which is especially designed to perform row norm sampling on the Gaussian kernel\u2019s polynomial lifting matrix . By Lemma 9, for any , any integers and any matrices , with high probability, the procedure RowNormSampler of Algorithm 3 outputs a rank- row norm sampler for matrix . Therefore, because the total number of times Algorithm 3 is invoked by the recursive leverage score sampling procedure is bounded by , by a union bound, with high probability the preconditions of Lemma 9 hold and hence we can invoke this lemma to prove that the sampler returned by Algorithm 1 satisfies the following with high probability:\n\n\u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 + \u03f5 / 2 \u2aaf \u03a6 \u22a4 \u200b \u03a0 \u22a4 \u200b \u03a0 \u200b \u03a6 + \u03bb \u200b I \u2aaf \u03a6 \u22a4 \u200b \u03a6 + \u03bb \u200b I 1 \u2212 \u03f5 / 2 . precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 2 superscript \u03a6 top superscript \u03a0 top \u03a0 \u03a6 \ud835\udf06 \ud835\udc3c precedes-or-equals superscript \u03a6 top \u03a6 \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 2 \\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1+\\epsilon/2}\\preceq\\Phi^{\\top}\\Pi^{\\top}\\Pi\\Phi+\\lambda I\\preceq\\frac{\\Phi^{\\top}\\Phi+\\lambda I}{1-\\epsilon/2}. Therefore, since , this implies,\n\nK + \u03bb \u200b I 1 + \u03f5 \u2aaf \u03a6 \u22a4 \u200b \u03a0 \u22a4 \u200b \u03a0 \u200b \u03a6 + \u03bb \u200b I \u2aaf K + \u03bb \u200b I 1 \u2212 \u03f5 . precedes-or-equals \ud835\udc3e \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 superscript \u03a6 top superscript \u03a0 top \u03a0 \u03a6 \ud835\udf06 \ud835\udc3c precedes-or-equals \ud835\udc3e \ud835\udf06 \ud835\udc3c 1 italic-\u03f5 \\frac{K+\\lambda I}{1+\\epsilon}\\preceq\\Phi^{\\top}\\Pi^{\\top}\\Pi\\Phi+\\lambda I\\preceq\\frac{K+\\lambda I}{1-\\epsilon}. Therefore, if we let , the theorem follows because has rows. Also because Algorithm 1 calls the primitive RowSampler of Algorithm 2, times with inputs and matrix , which has rows, each call, by Lemma 9, takes operations. Hence, since , the total runtime of the algorithm is . Appendix H Kernel Ridge Regression\n\nOne of the most elementary and yet powerful kernel methods is Kernel Ridge Regression (KRR). Given training data , a kernel function , and a regularization parameter , the KRR estimator for a given input is:\n\nf \u00af \u200b ( x ) \u2261 \u2211 j = 1 n k \u200b ( x j , x ) \u200b \u03b1 j \u00af \ud835\udc53 \ud835\udc65 subscript superscript \ud835\udc5b \ud835\udc57 1 \ud835\udc58 subscript \ud835\udc65 \ud835\udc57 \ud835\udc65 subscript \ud835\udefc \ud835\udc57 \\bar{f}(x)\\equiv\\sum^{n}_{j=1}k(x_{j},x)\\alpha_{j}\n\nwhere is the solution to the equation,\n\n( K + \u03bb \u200b I ) \u200b \ud835\udf36 = \ud835\udc32 . \ud835\udc3e \ud835\udf06 \ud835\udc3c \ud835\udf36 \ud835\udc32 (K+\\lambda I){\\bm{\\alpha}}={\\bf y}. (15)\n\nIn (15), is the kernel matrix defined by and is the vector of responses. The KRR estimator can be derived by minimizing a regularized squared loss objective function over a hypothesis space defined by the reproducing kernel Hilbert space associated with . However, the details are not important here. Suppose that is the lifting corresponding to the kernel function, i.e., . Let be the matrix with columns which is obtained by applying the lifting on the dataset, i.e., . Then, Theorems 1 and 2 approximate the kernel matrix by finding a sampling matrix such that . This corresponds to approximating the kernel function by . Therefore, the approximate KRR estimator for a given input is,\n\nf ~ \u200b ( x ) \u2261 \u2211 j = 1 n k ~ \u200b ( x j , x ) \u200b \u03b1 ~ j = \u27e8 \ud835\udc30 , \u03a0 \u200b \u03d5 \u200b ( x ) \u27e9 , ~ \ud835\udc53 \ud835\udc65 subscript superscript \ud835\udc5b \ud835\udc57 1 ~ \ud835\udc58 subscript \ud835\udc65 \ud835\udc57 \ud835\udc65 subscript ~ \ud835\udefc \ud835\udc57 \ud835\udc30 \u03a0 italic-\u03d5 \ud835\udc65 \\tilde{f}(x)\\equiv\\sum^{n}_{j=1}\\tilde{k}(x_{j},x)\\tilde{\\alpha}_{j}=\\langle{\\bf w},\\Pi\\phi(x)\\rangle,\n\nwhere the vector is obtained by solving the equation,\n\n( \u03a0 \u200b \u03a6 \u200b \u03a6 \u22a4 \u200b \u03a0 \u22a4 + \u03bb \u200b I ) \u200b \ud835\udc30 = \u03a0 \u200b \u03a6 \u200b \ud835\udc32 . \u03a0 \u03a6 superscript \u03a6 top superscript \u03a0 top \ud835\udf06 \ud835\udc3c \ud835\udc30 \u03a0 \u03a6 \ud835\udc32 (\\Pi\\Phi\\Phi^{\\top}\\Pi^{\\top}+\\lambda I){\\bf w}=\\Pi\\Phi{\\bf y}. The above equation can be solved much faster than (15) since the sampling matrix has a small number of rows. H.1 Risk Bounds\n\nOne way to analyze our approximate KRR estimator is via risk bounds. Several recent papers on approximate KRR use such analysis (Bach, 2013; Alaoui & Mahoney, 2015; Musco & Musco, 2017; Avron et al., 2017b). In particular, these papers consider the fixed design setting and upper bound the expected in-sample predication error of the KRR estimator , considering it as an empirical estimate of the statistical risk. More precisely, the underlying assumption is that satisfies\n\ny i = f \u2217 \u200b ( x i ) + \u03bd i subscript \ud835\udc66 \ud835\udc56 superscript \ud835\udc53 subscript \ud835\udc65 \ud835\udc56 subscript \ud835\udf08 \ud835\udc56 y_{i}=f^{*}(x_{i})+\\nu_{i} (16)\n\nfor some . The \u2019s are i.i.d noise terms, distributed as normal variables with variance . The empirical risk of an estimator , which measures the quality of the estimator, is defined as\n\n\u211b \u200b ( f ) \u2261 \ud835\udd3c { \u03bd i } \u200b [ 1 n \u200b \u2211 j = 1 n | f \u200b ( x i ) \u2212 f \u2217 \u200b ( x i ) | 2 ] . \u211b \ud835\udc53 subscript \ud835\udd3c subscript \ud835\udf08 \ud835\udc56 delimited-[] 1 \ud835\udc5b subscript superscript \ud835\udc5b \ud835\udc57 1 superscript \ud835\udc53 subscript \ud835\udc65 \ud835\udc56 superscript \ud835\udc53 subscript \ud835\udc65 \ud835\udc56 2 \\mathcal{R}(f)\\equiv\\mathbb{E}_{\\{\\nu_{i}\\}}\\left[{\\frac{1}{n}\\sum^{n}_{j=1}\\left|f(x_{i})-f^{*}(x_{i})\\right|^{2}}\\right]. Let be the vector whose entry is . It is straightforward to show that for the KRR estimator we have (Bach, 2013; Alaoui & Mahoney, 2015; Avron et al., 2017b):\n\n\u211b \u200b ( f \u00af ) \u211b \u00af \ud835\udc53 \\displaystyle\\mathcal{R}(\\bar{f}) = n \u2212 1 \u200b \u03bb 2 \u200b \ud835\udc1f \u22a4 \u200b ( K + \u03bb \u200b I ) \u2212 2 \u200b \ud835\udc1f absent superscript \ud835\udc5b 1 superscript \ud835\udf06 2 superscript \ud835\udc1f top superscript \ud835\udc3e \ud835\udf06 \ud835\udc3c 2 \ud835\udc1f \\displaystyle=n^{-1}\\lambda^{2}{\\bf f}^{\\top}(K+\\lambda I)^{-2}{\\bf f} + n \u2212 1 \u200b \u03c3 \u03bd 2 \u22c5 \ud835\udc2d\ud835\udc2b \u200b ( K 2 \u200b ( K + \u03bb \u200b I ) \u2212 2 ) . \u22c5 superscript \ud835\udc5b 1 superscript subscript \ud835\udf0e \ud835\udf08 2 \ud835\udc2d\ud835\udc2b superscript \ud835\udc3e 2 superscript \ud835\udc3e \ud835\udf06 \ud835\udc3c 2 \\displaystyle\\quad+n^{-1}\\sigma_{\\nu}^{2}\\cdot{\\bf tr}\\left({K^{2}(K+\\lambda I)^{-2}}\\right). Since and , where is the statistical dimension of the kernel matrix . We define,\n\n\u211b ^ K \u200b ( \ud835\udc1f ) \u2261 n \u2212 1 \u200b \u03bb \u200b \ud835\udc1f \u22a4 \u200b ( K + \u03bb \u200b I ) \u2212 1 \u200b \ud835\udc1f + n \u2212 1 \u200b \u03c3 \u03bd 2 \u22c5 s \u03bb subscript ^ \u211b \ud835\udc3e \ud835\udc1f superscript \ud835\udc5b 1 \ud835\udf06 superscript \ud835\udc1f top superscript \ud835\udc3e \ud835\udf06 \ud835\udc3c 1 \ud835\udc1f \u22c5 superscript \ud835\udc5b 1 superscript subscript \ud835\udf0e \ud835\udf08 2 subscript \ud835\udc60 \ud835\udf06 \\widehat{\\mathcal{R}}_{K}({\\bf f})\\equiv n^{-1}\\lambda{\\bf f}^{\\top}(K+\\lambda I)^{-1}{\\bf f}+n^{-1}\\sigma_{\\nu}^{2}\\cdot s_{\\lambda}\n\nand note that . The first term in the above expressions for and is frequently referred to as bias, while the second term is the variance.",
    "0051d15d5f3849846707eeb91677c80366f17960-29": "Lemma 10. Suppose that (16) holds, and let be the vector whose entry is . Let be the KRR estimator, and let be the KRR estimator obtained using some other kernel , whose kernel matrix is . Suppose that is an -spectral approximation to as in (1) for some , and that . The following bound holds:\n\n\u211b \u200b ( f ~ ) \u2264 ( 1 \u2212 \u03f5 ) \u2212 1 \u200b \u211b ^ K \u200b ( \ud835\udc1f ) + \u03f5 1 + \u03f5 \u22c5 rank \u200b ( K ~ ) n \u22c5 \u03c3 \u03bd 2 \u211b ~ \ud835\udc53 superscript 1 italic-\u03f5 1 subscript ^ \u211b \ud835\udc3e \ud835\udc1f \u22c5 italic-\u03f5 1 italic-\u03f5 rank ~ \ud835\udc3e \ud835\udc5b subscript superscript \ud835\udf0e 2 \ud835\udf08 \\displaystyle\\mathcal{R}(\\tilde{f})\\leq(1-\\epsilon)^{-1}\\widehat{\\mathcal{R}}_{K}({\\bf f})+\\frac{\\epsilon}{1+\\epsilon}\\cdot\\frac{\\text{rank}{(\\widetilde{K})}}{n}\\cdot\\sigma^{2}_{\\nu} (17)\n\nProof. For the bias term we have:\n\n\ud835\udc1f \u22a4 \u200b ( K ~ + \u03bb \u200b I ) \u2212 1 \u200b \ud835\udc1f \u2264 ( 1 \u2212 \u03f5 ) \u2212 1 \u200b \ud835\udc1f \u22a4 \u200b ( K + \u03bb \u200b I ) \u2212 1 \u200b \ud835\udc1f . superscript \ud835\udc1f top superscript ~ \ud835\udc3e \ud835\udf06 \ud835\udc3c 1 \ud835\udc1f superscript 1 italic-\u03f5 1 superscript \ud835\udc1f top superscript \ud835\udc3e \ud835\udf06 \ud835\udc3c 1 \ud835\udc1f \\displaystyle{\\bf f}^{\\top}(\\widetilde{K}+\\lambda I)^{-1}{\\bf f}\\leq(1-\\epsilon)^{-1}{\\bf f}^{\\top}(K+\\lambda I)^{-1}{\\bf f}. (18)\n\nWe now consider the variance term. Denote , and let denote the eigenvalues of a matrix . We have:\n\ns \u03bb \u200b ( K ~ ) subscript \ud835\udc60 \ud835\udf06 ~ \ud835\udc3e \\displaystyle s_{\\lambda}(\\widetilde{K}) = \ud835\udc2d\ud835\udc2b \u200b ( ( K ~ + \u03bb \u200b I ) \u2212 1 \u200b K ~ ) absent \ud835\udc2d\ud835\udc2b superscript ~ \ud835\udc3e \ud835\udf06 \ud835\udc3c 1 ~ \ud835\udc3e \\displaystyle={\\bf tr}\\left({(\\widetilde{K}+\\lambda I)^{-1}\\widetilde{K}}\\right) = \u2211 i = 1 s \u03bb i \u200b ( K ~ ) \u03bb i \u200b ( K ~ ) + \u03bb absent subscript superscript \ud835\udc60 \ud835\udc56 1 subscript \ud835\udf06 \ud835\udc56 ~ \ud835\udc3e subscript \ud835\udf06 \ud835\udc56 ~ \ud835\udc3e \ud835\udf06 \\displaystyle=\\sum^{s}_{i=1}\\frac{\\lambda_{i}(\\widetilde{K})}{\\lambda_{i}(\\widetilde{K})+\\lambda} = s \u2212 \u2211 i = 1 s \u03bb \u03bb i \u200b ( K ~ ) + \u03bb absent \ud835\udc60 subscript superscript \ud835\udc60 \ud835\udc56 1 \ud835\udf06 subscript \ud835\udf06 \ud835\udc56 ~ \ud835\udc3e \ud835\udf06 \\displaystyle=s-\\sum^{s}_{i=1}\\frac{\\lambda}{\\lambda_{i}(\\widetilde{K})+\\lambda} \u2264 s \u2212 ( 1 + \u03f5 ) \u2212 1 \u200b \u2211 i = 1 s \u03bb \u03bb i \u200b ( K ) + \u03bb absent \ud835\udc60 superscript 1 italic-\u03f5 1 subscript superscript \ud835\udc60 \ud835\udc56 1 \ud835\udf06 subscript \ud835\udf06 \ud835\udc56 \ud835\udc3e \ud835\udf06 \\displaystyle\\leq s-(1+\\epsilon)^{-1}\\sum^{s}_{i=1}\\frac{\\lambda}{\\lambda_{i}(K)+\\lambda} = s \u2212 \u2211 i = 1 s \u03bb \u03bb i \u200b ( K ) + \u03bb + \u03f5 1 + \u03f5 \u200b \u2211 i = 1 s \u03bb \u03bb i \u200b ( K ) + \u03bb absent \ud835\udc60 subscript superscript \ud835\udc60 \ud835\udc56 1 \ud835\udf06 subscript \ud835\udf06 \ud835\udc56 \ud835\udc3e \ud835\udf06 italic-\u03f5 1 italic-\u03f5 subscript superscript \ud835\udc60 \ud835\udc56 1 \ud835\udf06 subscript \ud835\udf06 \ud835\udc56 \ud835\udc3e \ud835\udf06 \\displaystyle=s-\\sum^{s}_{i=1}\\frac{\\lambda}{\\lambda_{i}(K)+\\lambda}+\\frac{\\epsilon}{1+\\epsilon}\\sum^{s}_{i=1}\\frac{\\lambda}{\\lambda_{i}(K)+\\lambda} \u2264 n \u2212 \u2211 i = 1 n \u03bb \u03bb i \u200b ( K ) + \u03bb + \u03f5 \u22c5 s 1 + \u03f5 absent \ud835\udc5b subscript superscript \ud835\udc5b \ud835\udc56 1 \ud835\udf06 subscript \ud835\udf06 \ud835\udc56 \ud835\udc3e \ud835\udf06 \u22c5 italic-\u03f5 \ud835\udc60 1 italic-\u03f5 \\displaystyle\\leq n-\\sum^{n}_{i=1}\\frac{\\lambda}{\\lambda_{i}(K)+\\lambda}+\\frac{\\epsilon\\cdot s}{1+\\epsilon} = s \u03bb \u200b ( K ) + \u03f5 \u22c5 s 1 + \u03f5 absent subscript \ud835\udc60 \ud835\udf06 \ud835\udc3e \u22c5 italic-\u03f5 \ud835\udc60 1 italic-\u03f5 \\displaystyle=s_{\\lambda}(K)+\\frac{\\epsilon\\cdot s}{1+\\epsilon} \u2264 ( 1 \u2212 \u03f5 ) \u2212 1 \u200b s \u03bb \u200b ( K ) + \u03f5 \u22c5 s 1 + \u03f5 absent superscript 1 italic-\u03f5 1 subscript \ud835\udc60 \ud835\udf06 \ud835\udc3e \u22c5 italic-\u03f5 \ud835\udc60 1 italic-\u03f5 \\displaystyle\\leq(1-\\epsilon)^{-1}s_{\\lambda}(K)+\\frac{\\epsilon\\cdot s}{1+\\epsilon}\n\nwhere we use the fact that implies that (this is a simple consequence of the Courant-Fischer minimax theorem). Combining the above variance bound with the bias bound in (18) yields:\n\n\u211b ^ K ~ \u200b ( \ud835\udc1f ) \u2264 ( 1 \u2212 \u03f5 ) \u2212 1 \u200b \u211b ^ K \u200b ( \ud835\udc1f ) + \u03f5 ( 1 + \u03f5 ) \u22c5 rank \u200b ( K ~ ) n \u22c5 \u03c3 \u03bd 2 subscript ^ \u211b ~ \ud835\udc3e \ud835\udc1f superscript 1 italic-\u03f5 1 subscript ^ \u211b \ud835\udc3e \ud835\udc1f \u22c5 italic-\u03f5 1 italic-\u03f5 rank ~ \ud835\udc3e \ud835\udc5b subscript superscript \ud835\udf0e 2 \ud835\udf08 \\widehat{\\mathcal{R}}_{\\tilde{K}}({\\bf f})\\leq(1-\\epsilon)^{-1}\\widehat{\\mathcal{R}}_{K}({\\bf f})+\\frac{\\epsilon}{(1+\\epsilon)}\\cdot\\frac{\\text{rank}({\\widetilde{K}})}{n}\\cdot\\sigma^{2}_{\\nu}\n\nand the bound completes the proof. In short, Lemma 10 bounds the risk of the approximate KRR estimator as a function of both the risk upper bound in (17) and an additive term which is small if the rank of and/or is small.",
    "0051d15d5f3849846707eeb91677c80366f17960-30": "In particular, it is instructive to compare the additive term to the variance term . Since the approximation is only useful computationally if , we should expect the additive term in (17) to also approach and generally be small when is large. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Thu Mar 14 13:04:28 2024 by LaTeXML"
}