{
    "001534ec4e6d656722fddc81ba532779cea76875-0": "Noise Estimation for Generative Diffusion Models\n\nRobin San Roman*1, Eliya Nachmani* 2,3, Lior Wolf 2\n\nNoise Estimation for Generative Diffusion Models\n\nRobin San Roman*1, Eliya Nachmani* 2,3, Lior Wolf 2\n\nAbstract\n\nGenerative diffusion models have emerged as leading models in speech and image generation.",
    "001534ec4e6d656722fddc81ba532779cea76875-1": "However, in order to perform well with a small number of denoising steps, a costly tuning of the set of noise parameters is needed. In this work, we present a simple and versatile learning scheme that can step-by-step adjust those noise parameters, for any given number of steps, while the previous work needs to retune for each number separately. Furthermore, without modifying the weights of the diffusion model, we are able to significantly improve the synthesis results, for a small number of steps. Our approach comes at a negligible computation cost. \u2020\u2020*Equal contribution\n\nIntroduction\n\nDeep generative models have seen a tremendous advancement in the past few years. The main successful architectures can be divided into two categories: (i) autoregessive models, such as VQ-VAE for images (Razavi, Oord, and Vinyals 2019) and Wavenet for speech (Oord et al. 2016). (ii) non-autoregessive models, for example, StyleGAN (Karras et al. 2020) for vision application and WaveGAN (Donahue, McAuley, and Puckette 2018) for audio synthesis. An emerging class of non-autoregessive models is the one of Denoising Diffusion Probabilistic Models (DDPM). Such methods use diffusion models and denoising score matching in order to generate images (Ho, Jain, and Abbeel 2020) and speech (Chen et al. 2020). The DDPM model learns to perform a diffusion process on a Markov chain of latent variables. The diffusion process transforms a data sample into Gaussian noise. During inference the reverse process is used, which is called the denoisng process. The inference procedure starts from Gaussian noise and iteratively refines the signal. This process is often conditioned on the class and attributes that one wishes to generate. In order to get high quality synthesis, a large number of denosing steps are used (i.e. steps). To allow the process to converge to a high quality output with only a small number of denosing steps, a costly grid search is required in order to find a noise schedule that would produce high-fidelity results. In this paper, we propose a novel method for obtaining the noise schedule based on the conditioning data. The noise schedule that the proposed method produces leads to a high fidelity synthesis of samples, even for a small number of denosing steps. Moreover, for a given amount of denosing steps, the proposed method obtains better results than the previous models, when their noise schedule is determined by a costly per-sample grid search for the optimal parameters. Our method introduces a novel neural network that is able to monitor and control the denoising process. Instead of fixing in advance the steps of the reverse process that will be skipped, this network is able, by estimating the amount of noise in the data, to schedule the subsequent steps of the denoising process. Our results are demonstrated on two major domains: vision and audio. In the first domain, the proposed method is shown to provide a better FID score for generated images, when the number of steps is restricted. For speech data, we show that the proposed method improves various measures, such as Perceptual Evaluation of Speech Quality (PESQ) and short-time objective intelligibility (STOI). Related Work\n\nDiffusion Probabilistic Models were first introduced in the seminal work of Sohl-Dickstein et al. (Sohl-Dickstein et al. 2015), who presented the idea of using an iterative neural diffusion process for destroying the structure of a given distribution, while learning the reverse neural diffusion process for restoring the structure in the data. It was shown that the proposed neural diffusion process can learn the data distribution in domains, such as images and time series. The main issue with the proposed neural diffusion process is that during training it requires up to thousands of iterative steps in order to learn the target distribution. In (Song and Ermon 2019), a new generative model based on the score matching method (Hyv\u00e4rinen and Dayan 2005) and Langevin dynamics was introduced. The proposed model estimates and samples the logarithm of the data density, which is the Stein score function (Liu, Lee, and Jordan 2016). The proposed method achieves state of the art results for modeling the CIFAR-10 dataset. The two ideas of (i) neural Diffusion Probabilistic Models and (ii) generative models based on score matching, were combined by the DDPM method of Ho et al. (Ho, Jain, and Abbeel 2020). DDPM presents a generative model based on the neural diffusion process and applies score matching for image generation. Subsequently, in (Chen et al. 2020) a generative neural diffusion process based on score matching was applied to speech generation, obtaining state of the art results in comparison to well-established methods, such as Wavenet (Oord et al. 2016), Wavernn (Kalchbrenner et al. 2018) and GAN-TTS (Bi\u0144kowski et al. 2019). A parallel contribution presented high fidelity speech generation results using a different neural diffusion process (Kong et al. 2020). One major limitation of the generative neural diffusion process is that in order to generate a high quality sample, one should use a large number of diffusion steps, e.g., a thousand steps are often used. Denoising Diffusion Implicit Models (DDIMs) (Song, Meng, and Ermon 2021) is an acceleration for the denoising process. It employs non-Markovian diffusion processes, which leads to a faster sample procedure than other diffusion models. A further development in the score based generative models is to consider this process as a solution to a stochastic differential equation (Song et al. 2020). This method achieves state of the art performance for unconditional image generation on CIFAR-10. An alternative approach trains an energy-based generative model using a diffusion process that is applied to increasingly noisy versions of a dataset (Gao et al. 2020), also presenting results on CIFAR-10. The recent TimeGrad model (Rasul et al. 2021) is a diffusion process for probabilistic time series forecasting, which was shown empirically to outperform Transformers (Vaswani et al. 2017) and LSTMs (Hochreiter and Schmidhuber 1997) on some datasets. In another concurrent work, a multinomial diffusion process is learned by adding categorical noise to the process (Hoogeboom et al.",
    "001534ec4e6d656722fddc81ba532779cea76875-2": "2021). Competitive results are presented for image segmentation and language modeling. Background\n\nDenoising Diffusion Probabilistic Model (DDPM) are neural network that learn the gradients of the data log density :\n\ns \u200b ( y ) = \u2207 y log \u2061 p \u200b ( y ) \ud835\udc60 \ud835\udc66 subscript \u2207 \ud835\udc66 \ud835\udc5d \ud835\udc66 s(y)=\\nabla_{y}\\log p(y) (1)\n\nGiven those gradients, one can then use Langevin Dynamics to sample from the probability iteratively\n\ny ~ i + 1 = y ~ i + \u03b7 2 \u200b s \u200b ( y ~ i ) + \u03b7 \u200b z i subscript ~ \ud835\udc66 \ud835\udc56 1 subscript ~ \ud835\udc66 \ud835\udc56 \ud835\udf02 2 \ud835\udc60 subscript ~ \ud835\udc66 \ud835\udc56 \ud835\udf02 subscript \ud835\udc67 \ud835\udc56 \\tilde{y}_{i+1}=\\tilde{y}_{i}+\\frac{\\eta}{2}s(\\tilde{y}_{i})+\\sqrt{\\eta}z_{i} (2)\n\nWhere is the step size and . The formalization of Denoising Diffusion Probabilistic Models (DDPM) by Ho et al (Ho, Jain, and Abbeel 2020) employs a parameterized Markov chain trained using variational inference, in order to produce samples matching the data after finite time. The transitions of this chain are learned to reverse a diffusion process. This diffusion process is defined by a Markov chain that gradually adds noise in the data with a noise schedule and is defined as:\n\nq \u200b ( y 1 : N | y 0 ) = \u220f n = 1 N q \u200b ( y n | y n \u2212 1 ) , \ud835\udc5e conditional subscript \ud835\udc66 : 1 \ud835\udc41 subscript \ud835\udc66 0 superscript subscript product \ud835\udc5b 1 \ud835\udc41 \ud835\udc5e conditional subscript \ud835\udc66 \ud835\udc5b subscript \ud835\udc66 \ud835\udc5b 1 q(y_{1:N}|y_{0})=\\prod_{n=1}^{N}q(y_{n}|y_{n-1})\\,, (3)\n\nwhere N is the length of the diffusion process, and is a sequence of latent variables with the same size as the clean sample . At each iteration, the diffusion process adds Gaussian noise, according to the noise schedule:\n\nq \u200b ( y n | y n \u2212 1 ) := \ud835\udca9 \u200b ( y n ; 1 \u2212 \u03b2 n \u200b y n \u2212 1 , \u03b2 n \u200b \ud835\udc08 ) , assign \ud835\udc5e conditional subscript \ud835\udc66 \ud835\udc5b subscript \ud835\udc66 \ud835\udc5b 1 \ud835\udca9 subscript \ud835\udc66 \ud835\udc5b 1 subscript \ud835\udefd \ud835\udc5b subscript \ud835\udc66 \ud835\udc5b 1 subscript \ud835\udefd \ud835\udc5b \ud835\udc08 q(y_{n}|y_{n-1}):=\\mathcal{N}(y_{n};\\sqrt{1-\\beta_{n}}y_{n-1},\\beta_{n}\\mathbf{I})\\,, (4)\n\nwhere is the noise schedule as defined above. The diffusion process can be simulated for any number of steps with the closed formula:\n\ny n = \u03b1 \u00af \u200b y 0 + 1 \u2212 \u03b1 \u00af \u200b \u03b5 , subscript \ud835\udc66 \ud835\udc5b \u00af \ud835\udefc subscript \ud835\udc66 0 1 \u00af \ud835\udefc \ud835\udf00 y_{n}=\\sqrt{\\bar{\\alpha}}y_{0}+\\sqrt{1-\\bar{\\alpha}}\\varepsilon\\,, (5)\n\nwhere , and . One can use this to implement the DDPM training algorithm (Alg.1) which is defined in (Chen et al. 2020). The input to the training algorithm is the dataset . The algorithm samples , and . The noisy latent variable is calculated and fed to the DDPM neural network . A gradient descent step is taken in order to estimate the noise with the DDPM network. By the end of the algorithm, the DDPM network can estimate the noise added during the diffusion process. When is close to , the diffusion process adds a small amount of noise and when is close to , there are large amounts of noise that are added to the generation process. As mentioned in (Chen et al. 2020), sampling the noise level in the uniform distribution gives poor empirical results. This is due to the fact that the network would rarely be trained to fine tune good examples ( close to ). Instead, one can sample such that the distribution of the training examples match the forward process, i.e., there are an equal amount of training samples that correspond to every step of the diffusion process. The first step is to sample a state ( line 3) in the forward process and then sample the noise level using:\n\n\u03b1 \u00af \u223c \ud835\udcb0 \u200b [ l n , l n \u2212 1 ] similar-to \u00af \ud835\udefc \ud835\udcb0 subscript \ud835\udc59 \ud835\udc5b subscript \ud835\udc59 \ud835\udc5b 1 \\sqrt{\\bar{\\alpha}}\\sim\\mathcal{U}[l_{n},l_{n-1}] (6)\n\nWhere:\n\nl 0 = 1 , l n = \u220f i = 0 n 1 \u2212 \u03b2 i formulae-sequence subscript \ud835\udc59 0 1 subscript \ud835\udc59 \ud835\udc5b superscript subscript product \ud835\udc56 0 \ud835\udc5b 1 subscript \ud835\udefd \ud835\udc56 l_{0}=1,\\hskip 2.84526ptl_{n}=\\sqrt{\\prod_{i=0}^{n}1-\\beta_{i}} (7)\n\nIn Algorithm 1 (line 7) the DDPM is trained to learn the noise directly, instead of learning the Markov chain gradients. In (Ho, Jain, and Abbeel 2020) the authors show that the following reparametrization leads to better empirical results:\n\n\u03b5 = \u2212 1 \u2212 \u03b1 \u00af n \u200b \u2207 y n log \u2061 q \u200b ( y n | y 0 ) \ud835\udf00 1 subscript \u00af \ud835\udefc \ud835\udc5b subscript \u2207 subscript \ud835\udc66 \ud835\udc5b \ud835\udc5e conditional subscript \ud835\udc66 \ud835\udc5b subscript \ud835\udc66 0 \\varepsilon=-\\sqrt{1-\\bar{\\alpha}_{n}}\\nabla_{y_{n}}\\log q(y_{n}|y_{0}) (8)\n\nThe trained model can be used to perform inference using a variation of Langevin dynamics (Eq. 2). The following update from (Song, Meng, and Ermon 2021) is used to reverse a step of the diffusion process:\n\ny n \u2212 1 = y n \u2212 1 \u2212 \u03b1 n 1 \u2212 \u03b1 \u00af n \u200b \u03b5 \u03b8 \u200b ( y n , x , \u03b1 \u00af n ) \u03b1 \u00af n + \u03c3 n \u200b \u03b5 , subscript \ud835\udc66 \ud835\udc5b 1 subscript \ud835\udc66 \ud835\udc5b 1 subscript \ud835\udefc \ud835\udc5b 1 subscript \u00af \ud835\udefc \ud835\udc5b subscript \ud835\udf00 \ud835\udf03 subscript \ud835\udc66 \ud835\udc5b \ud835\udc65 subscript \u00af \ud835\udefc \ud835\udc5b subscript \u00af \ud835\udefc \ud835\udc5b subscript \ud835\udf0e \ud835\udc5b \ud835\udf00 y_{n-1}=\\dfrac{y_{n}-\\frac{1-\\alpha_{n}}{\\sqrt{1-\\bar{\\alpha}_{n}}}\\varepsilon_{\\theta}(y_{n},x,\\sqrt{\\bar{\\alpha}_{n}})}{\\sqrt{\\bar{\\alpha}_{n}}}+\\sigma_{n}\\varepsilon\\,, (9)\n\nwhere is white noise. Ho et al. (Ho, Jain, and Abbeel 2020) showed that adding white noise of variance is optimal, when the inference procedure is initialized with gaussian noise (). One can use this update rule in Eq. 9 to sample from the data distribution, by starting from a Gaussian noise and then step-by-step reversing the diffusion process. Algorithm 2 is used to sample with the network . Since our experiments on images are unconditional, the network no longer needs the input . The update equation that we use is defined in (Song, Meng, and Ermon 2021):\n\ny n \u2212 1 = \u03b1 \u00af n \u2212 1 \u200b y ^ 0 , n + 1 \u2212 \u03b1 \u00af n \u2212 1 \u2212 \u03c3 ~ n 2 \u200b \u03b5 \u03b8 \u200b ( y n , \u03b1 \u00af n ) + \u03c3 ~ n \u200b \u03b5 , subscript \ud835\udc66 \ud835\udc5b 1 subscript \u00af \ud835\udefc \ud835\udc5b 1 subscript ^ \ud835\udc66 0 \ud835\udc5b 1 subscript \u00af \ud835\udefc \ud835\udc5b 1 superscript subscript ~ \ud835\udf0e \ud835\udc5b 2 subscript \ud835\udf00 \ud835\udf03 subscript \ud835\udc66 \ud835\udc5b subscript \u00af \ud835\udefc \ud835\udc5b subscript ~ \ud835\udf0e \ud835\udc5b \ud835\udf00 y_{n-1}=\\sqrt{\\bar{\\alpha}_{n-1}}\\hat{y}_{0,n}+\\sqrt{1-\\bar{\\alpha}_{n-1}-\\tilde{\\sigma}_{n}^{2}}\\varepsilon_{\\theta}(y_{n},\\bar{\\alpha}_{n})+\\tilde{\\sigma}_{n}\\varepsilon\\,, (10)\n\nwhere is the prediction of , is white noise, and is a new parameter of the generative process. One can apply the rule of Eq.10 with , in which case no random noise is added. This makes the process deterministic and leads to the best results in most of the scenarios (Song, Meng, and Ermon 2021). Method\n\nWe note that at training time the data is constructed in such a way (cf. Eq. 5) that we can feed the network with the noisy data and with ground truth noise level . However, at a given inference step, the amount of noise in the data is unknown. In most methods, the conditioning used is a predefined one. As a result, the input conditioning given to the network is not exploited at its full potential. This inexact analysis of the noise is especially detrimental when the number of steps in the generation process is small. In this case, the quality of the samples at intermediate states () varies widely. To solve this problem, we introduce a novel neural network that estimates the value of , thus providing better conditioning for the diffusion network. The input to the neural network is the data generated at step and its output is the estimated noise level . This network provides a continuous control signal to the generation process, by providing the DDPM network with a conditioning signal that relates to the actual quality of the generation. Figure 1 depicts the generation process used. Similarly to (Song, Meng, and Ermon 2021), the idea is to use a pretrained DDPM () from (Ho, Jain, and Abbeel 2020) and skip some of the states in the process to shorten the generation time. However, our model includes a Noise estimation model () to calculate, between denoising steps, adjustments of the noise schedule. Noise Level Estimation\n\nThe network is trained with a similar procedure as Alg. 1. The sampling of the noise level is done using the distribution described in Eq. 6. Given the input , the network estimates the noise level (). Empirically, we found that that the performance of the network in low noise situations is critical to the performance of our method. Indeed, the last steps of the generation process are responsible for the quality of the final sample.",
    "001534ec4e6d656722fddc81ba532779cea76875-3": "At those stages, the amount of noise is very small, and approaches . (See experiments Fig. 9(b)) We, therefore, design our regression loss on :\n\n\u2112 \u200b ( \u03b1 \u00af , \u03b1 ^ ) = \u2016 log \u2061 ( 1 \u2212 \u03b1 \u00af ) \u2212 log \u2061 ( 1 \u2212 \u03b1 ^ ) \u2016 2 \u2112 \u00af \ud835\udefc ^ \ud835\udefc subscript norm 1 \u00af \ud835\udefc 1 ^ \ud835\udefc 2 \\mathcal{L}(\\bar{\\alpha},\\hat{\\alpha})=\\|\\log(1-\\bar{\\alpha})-\\log(1-\\hat{\\alpha})\\|_{2} (11)\n\nThis loss penalizes with higher cost the errors when close to 1 resulting in better network performances in this region. Noise Schedule Adjustments\n\nWe wish to use , the output of the , in order to adjust the noise schedule parameters. In the following, we present how obtain those parameters, assuming they follow either a linear or a Fibonacci distribution. This allows us to define the following function:\n\n\u03b1 , \u03b2 = updateNoiseSchedule \u200b ( \u03b1 ^ , n ) \ud835\udefc \ud835\udefd updateNoiseSchedule ^ \ud835\udefc \ud835\udc5b \\alpha,\\beta=\\text{updateNoiseSchedule}(\\hat{\\alpha},n) (12)\n\nWhere are the set of noise parameters and is the number of remaining denoising steps. As we show, in order to define this function, it is sufficient to estimate the first parameters and the type of distribution. Linear Schedule\n\nIn the general case, is given by:\n\n\u03b1 \u00af = \u220f i = 0 n \u2212 1 ( 1 \u2212 \u03b2 i ) \u00af \ud835\udefc superscript subscript product \ud835\udc56 0 \ud835\udc5b 1 1 subscript \ud835\udefd \ud835\udc56 \\bar{\\alpha}=\\prod_{i=0}^{n-1}(1-\\beta_{i}) (13)\n\nSince the values of the are typically between and we can use the Taylor approximation . We can derive from Eq. 13:\n\nlog \u2061 ( \u03b1 \u00af ) = \u2211 i = 0 n \u2212 1 log \u2061 ( 1 \u2212 \u03b2 i ) \u2248 \u2212 \u2211 i = 0 n \u2212 1 \u03b2 i \u00af \ud835\udefc superscript subscript \ud835\udc56 0 \ud835\udc5b 1 1 subscript \ud835\udefd \ud835\udc56 superscript subscript \ud835\udc56 0 \ud835\udc5b 1 subscript \ud835\udefd \ud835\udc56 \\log(\\bar{\\alpha})=\\sum_{i=0}^{n-1}\\log(1-\\beta_{i})\\approx-\\sum_{i=0}^{n-1}\\beta_{i} (14)\n\nAssuming the linear schedule, the expression of with respect to in the range is:\n\n\u03b2 i = \u03b2 0 + i \u200b x , subscript \ud835\udefd \ud835\udc56 subscript \ud835\udefd 0 \ud835\udc56 \ud835\udc65 \\beta_{i}=\\beta_{0}+ix\\,, (15)\n\nwhere is the step size. Therefore, we have:\n\nlog \u2061 ( \u03b1 \u00af ) = \u2212 ( \u2211 i = 0 n \u2212 1 \u03b2 0 + i \u200b x ) \u00af \ud835\udefc superscript subscript \ud835\udc56 0 \ud835\udc5b 1 subscript \ud835\udefd 0 \ud835\udc56 \ud835\udc65 \\log(\\bar{\\alpha})=-\\left(\\sum_{i=0}^{n-1}\\beta_{0}+ix\\right) (16)\n\nand\n\nx = \u2212 2 \u200b ( log \u2061 ( \u03b1 \u00af ) + n \u200b \u03b2 0 ) n \u200b ( n \u2212 1 ) \ud835\udc65 2 \u00af \ud835\udefc \ud835\udc5b subscript \ud835\udefd 0 \ud835\udc5b \ud835\udc5b 1 x=-2\\frac{(\\log(\\bar{\\alpha})+n\\beta_{0})}{n(n-1)} (17)\n\nOnce is recovered, Eq.15 provides us with the noise parameters required to perform the remaining denoising steps. Fibonacci Schedule\n\nChen et al. (Chen et al. 2020) employ a Fibonacci schedule for a low number of denoising steps:\n\n\u03b2 i + 2 = \u03b2 i + \u03b2 i + 1 subscript \ud835\udefd \ud835\udc56 2 subscript \ud835\udefd \ud835\udc56 subscript \ud835\udefd \ud835\udc56 1 \\beta_{i+2}=\\beta_{i}+\\beta_{i+1} (18)\n\nWe can find a closed form for this series given and , which will allow us to compute all the terms. The homogenuous recurrent equation is:\n\n\u03b2 i + 2 \u2212 \u03b2 i + 1 \u2212 \u03b2 i = 0 subscript \ud835\udefd \ud835\udc56 2 subscript \ud835\udefd \ud835\udc56 1 subscript \ud835\udefd \ud835\udc56 0 \\beta_{i+2}-\\beta_{i+1}-\\beta_{i}=0 (19)\n\nThus, the series () is of the form:\n\n\u03b2 i = A \u200b \u03c6 i + B \u200b \u03c6 \u2032 \u2063 i , subscript \ud835\udefd \ud835\udc56 \ud835\udc34 superscript \ud835\udf11 \ud835\udc56 \ud835\udc35 superscript \ud835\udf11 \u2032 \ud835\udc56 \\beta_{i}=A\\varphi^{i}+B\\varphi^{\\prime i}\\,, (20)\n\nwhere and are the solutions of\n\nWith a straightforward induction on , one can show that:\n\n\u2200 n > 2 , \u03b2 1 + \u2211 i = 0 n \u2212 1 \u03b2 i = \u03b2 n + 1 formulae-sequence for-all \ud835\udc5b 2 subscript \ud835\udefd 1 superscript subscript \ud835\udc56 0 \ud835\udc5b 1 subscript \ud835\udefd \ud835\udc56 subscript \ud835\udefd \ud835\udc5b 1 \\forall n>2,\\beta_{1}+\\sum_{i=0}^{n-1}\\beta_{i}=\\beta_{n+1} (21)\n\nCombining Eq. 21 with the approximation of Eq. 14, we obtain that is the solution of the following system of linear equations:\n\n{ A \u200b \u03c6 0 + B \u200b \u03c6 \u2032 \u2063 0 = \u03b2 0 A \u200b \u03c6 1 + B \u200b \u03c6 \u2032 \u2063 1 \u2212 log \u2061 ( \u03b1 \u00af ) = A \u200b \u03c6 n + 1 + B \u200b \u03c6 \u2032 \u2063 n + 1 cases \ud835\udc34 superscript \ud835\udf11 0 \ud835\udc35 superscript \ud835\udf11 \u2032 0 subscript \ud835\udefd 0 otherwise \ud835\udc34 superscript \ud835\udf11 1 \ud835\udc35 superscript \ud835\udf11 \u2032 1 \u00af \ud835\udefc \ud835\udc34 superscript \ud835\udf11 \ud835\udc5b 1 \ud835\udc35 superscript \ud835\udf11 \u2032 \ud835\udc5b 1 otherwise \\begin{cases}A\\varphi^{0}+B\\varphi^{\\prime 0}=\\beta_{0}\\\\\nA\\varphi^{1}+B\\varphi^{\\prime 1}-\\log(\\bar{\\alpha})=A\\varphi^{n+1}+B\\varphi^{\\prime n+1}\\end{cases} (22)\n\nThe unique solution of these equations is:\n\nA = \u03b2 0 \u2212 log \u2061 ( \u03b1 \u00af ) \u2212 \u03b2 0 \u200b ( \u03c6 \u2212 \u03c6 n + 1 ) \u03c6 \u2032 \u2212 \u03c6 \u2212 ( \u03c6 \u2032 \u2063 n + 1 \u2212 \u03c6 n + 1 ) B = log \u2061 ( \u03b1 \u00af ) \u2212 \u03b2 0 \u200b ( \u03c6 \u2212 \u03c6 n + 1 ) \u03c6 \u2032 \u2212 \u03c6 \u2212 ( \u03c6 \u2032 \u2063 n + 1 \u2212 \u03c6 n + 1 ) \ud835\udc34 subscript \ud835\udefd 0 \u00af \ud835\udefc subscript \ud835\udefd 0 \ud835\udf11 superscript \ud835\udf11 \ud835\udc5b 1 superscript \ud835\udf11 \u2032 \ud835\udf11 superscript \ud835\udf11 \u2032 \ud835\udc5b 1 superscript \ud835\udf11 \ud835\udc5b 1 \ud835\udc35 \u00af \ud835\udefc subscript \ud835\udefd 0 \ud835\udf11 superscript \ud835\udf11 \ud835\udc5b 1 superscript \ud835\udf11 \u2032 \ud835\udf11 superscript \ud835\udf11 \u2032 \ud835\udc5b 1 superscript \ud835\udf11 \ud835\udc5b 1 \\begin{split}A&=\\beta_{0}-\\frac{\\log(\\bar{\\alpha})-\\beta_{0}(\\varphi-\\varphi^{n+1})}{\\varphi^{\\prime}-\\varphi-(\\varphi^{\\prime n+1}-\\varphi^{n+1})}\\\\\nB&=\\frac{\\log(\\bar{\\alpha})-\\beta_{0}(\\varphi-\\varphi^{n+1})}{\\varphi^{\\prime}-\\varphi-(\\varphi^{\\prime n+1}-\\varphi^{n+1})}\\end{split} (23)\n\nThus a closed form solution is obtained that allows to compute the Fibonacci noise schedule (), given the noise level and . Conditioning on Discrete Indexes\n\nMost DDPMs are not conditioned on the noise level but instead use a discrete integer index that represents steps in the reverse process.",
    "001534ec4e6d656722fddc81ba532779cea76875-4": "The two types of conditioning are related. Instead of giving the network with the exact value , the network is given the integer of the interval (defined in Eq. 7), which contains the estimated value. To enable our method to work with this type of DDPM, we estimate the noise level and feed the network with the integer encoding corresponding interval. Inference procedure\n\nOur inference procedure is introduce in algorithm 4. The idea is to have a set of step indexes for which, we readjust the noise schedule. For simplicity, in all of our experiments, for a given number of steps . The adjustment is done using the neural network , which estimates the noise level . Given this estimation, we deduce the sets of parameters for the remaining denoising steps, as shown in the Noise Schedule adjustments Section. The noise schedule (vectors and ) is initialised with a set of predefined values (function initialNoiseSchedule()). The rest of the algorithm (lines 3-15) is very similar to the algorithm 2. The only difference (lines 7-9) is that we use a set of iteration indexes for which we will adjust the noise schedule vectors. For the iteration , we estimate the noise level using the model , then we follow the deterministic method described previously (function updateNoiseSchedule()) to compute the adjustment of the noise schedule. Experiments\n\nTo demonstrate the wide applicability of our approach, we perform experiments in both speech synthesis and image generation. In both cases, we determine the optimal few-step scheduling for state of the art models. Speech Generation\n\nFor speech generation, we used a WaveGrad implementation that came with a grid searched noise schedule for iterations (Vovk 2020). We trained a small model based on a the version of ConvTASNet (Luo and Mesgarani 2019) architecture. The model is composed of the encoding and masking module of ConvTASNet. Its decoding part is not utilized. The ConvTASNet architecture cuts, using a separator network, the signal in chunks and processes each chunk individually. Our network further applies a fully connected layer with an output dimension of , followed by a sigmoid activation function to each chunk. The final output is the average of the outputs of all the chunks. The encoder has filters of size (the parameter names follow (Luo and Mesgarani 2019)). The separator uses stacks of convolutional blocks of kernel size with repeats. It has channels in the convolutional blocks and in the bottleneck and residual paths. To train our model , we use the same split of the LJ-Speech dataset (Ito and Johnson 2017) used by (Vovk 2020) for training the WaveGrad model. Our model was trained with the Adam optimizer (Kingma and Ba 2014) and a learning rate of . Each batch includes samples from an audio duration of seconds with a sampling rate of Hz. Next, we evaluate the synthesis quality. The DDPM employed in our experiments was trained using Mel Spectrogram as the conditioning signal . Following (Chen et al. 2020), in our experiments, the ground truth Mel-Spectrogram is given as an input in order to allow the computation of the relative speech metrics (MCD(Kubichek 1993), PESQ(Rix et al.",
    "001534ec4e6d656722fddc81ba532779cea76875-5": "2001), STOI(Taal et al. 2011)). In our experiments, we performed inference for six iterations with an adjustment of the noise schedule at every step i.e. . This noise schedule adjustment uses the Fibonacci method. Sample results can be found under the following link. As can be heard from the samples that are provided, for few iterations, our method obtains a much better improvement than the baseline method, after applying a costly grid search for the optimal parameters of the baseline method. This is also captured by the qualitative results, presented in Tab.",
    "001534ec4e6d656722fddc81ba532779cea76875-6": "1. Even though our method results in a small decrease in the MCD, we demonstrate a large improvement in both PESQ and STOI. Image Generation\n\nFor image generation, we use denoising diffusion models trained in (Jonathan Ho 2020), relying on the implementation available in (Jiaming Song and Ermon 2020).",
    "001534ec4e6d656722fddc81ba532779cea76875-7": "We trained our model on three image datasets (i) CelebA 64x64 (Liu et al. 2015), (ii) LSUN Bedroom 256x256, and (iii) LSUN Church 256x256 (Yu et al. 2015). The model used for the noise estimation employed a VGG11 (Simonyan and Zisserman 2014) backbone pretrained on ImageNet (Deng et al. 2009). We added ReLU activations, followed with a fully connected layer with an output dimension and a final sigmoid activation to the backbone. An Adam optimizer(Kingma and Ba 2014) with a learning rate of was used, with a batch size of size 64. The Fr\u00e9chet Inception Distance (FID) (Heusel et al. 2017) is used as the benchmark metric. For all experiments, similarly to the DDIM paper, we compute the FID score with generated images using the torch-fidelity implementation (Obukhov et al. 2020). Figure 4 depicts the FID score of our method using the update in Eq. 9 is comparison to the baseline DDIM method. Both methods use the same DDIM diffusion model provided in (Jiaming Song and Ermon 2020) for CelebA. Our method uses the adjusted noise schedule at every step and generates a linear noise schedule. Different plots are given for different values of , which is the parameter that control in Eq. 10:\n\n\u03c3 ~ = \u03b7 \u200b \u03b2 n \u200b ( 1 \u2212 \u03b1 \u00af n \u2212 1 ) \u200b ( 1 \u2212 \u03b1 \u00af n ) ~ \ud835\udf0e \ud835\udf02 subscript \ud835\udefd \ud835\udc5b 1 subscript \u00af \ud835\udefc \ud835\udc5b 1 1 subscript \u00af \ud835\udefc \ud835\udc5b \\tilde{\\sigma}=\\eta\\sqrt{\\beta_{n}(1-\\bar{\\alpha}_{n-1})(1-\\bar{\\alpha}_{n})} (24)\n\nAs can be seen, our method improves by a large gap the FID score the DDIM method. For example, for three iteration with our method improve the FID score by . The gap in performance is maintained for up to 10 iterations. In Figure 5, we demonstrate the progression over a six iteration denoising process for both our method and the DDIM method that we use as a baseline. Evidently, our generated images are more coherent and sharper than the DDIM method. Figure 6 presents samples for celebA 64x64 generation, given a different number of target steps. Our inference procedure is able to generate decent images for as little as denoising steps. We also show convincing generations results for and steps generation processes. This results demonstrate that our method helps toward finding the best non-Markovian denoising process, especially when generating with very few iterations. Figures 7 and 8 depicts comparison of generative processes and generated samples between our method and the DDIM baseline. Our method overall clearly improves sharpness and contrast over the baseline. In Figure 3, we compare our method with DDPM for the CelebA dataset. As can be seen, our method improves, by a large margin, the results of the DDPM method up to 100 iterations. For example, for iteration, we improve the FID score by . Similarly, in Figures 3 we provide the FID score for LSUN Church and Bedroom 256x256 datasets, using the published DDIM models.",
    "001534ec4e6d656722fddc81ba532779cea76875-8": "As can be seen for a small number of iteration, i.e. less then , our method greatly improves the results of the baseline method DDIM. In (Luhman and Luhman 2021), authors propose a distilation framework that can sample in one iteration. The method performs FID scores of on LSUN Church and on Bedroom. As can be seen our method gets better results for as few as 6 iterations. Additional Experiments\n\nWe perform ablation experiments to justify some claims and choice in our method. In Figure 9(b) we computed the optimal noise schedule for a data sample. We perturbed the values of for the different by . One can clearly see that perturbations on , , results in huge drop of the performance whereas the others do not change the results much. It shows that the performance of the sampling algorithm is mostly dependent on the last steps, where the alpha values are small. This is why we use the loss 11 and want our model to perform his best when is close to . In order evaluate the accuracy of in recovering , noisy speech signals are generated according to Eq. 5 with values between 0 and 1. The noise level is then estimated by the trained network . The results of this experiment are presented in Figure 9(a). The Mean Square Error (MSE) between the ground truth (used to generate the noisy speech) and the network estimation is depicted for between 0 and 1. Each point is computed with 16 audio files of 3 secs from the validation set. As can be seen, our model is able to estimate the noise level within an error of at most and with even better performance when alpha is close to , which is where the precision is critical to the performance of our method. In Table 2 we provide the PESQ score for denoising steps, with various number of adjustments. As can been seen, the best results are when readjusting every steps, yet our method already outperforms the grid search for two adjustments. Table 3 depicts the mean run-time to generate an image on an Nvidia Titan X over 1000 generations. It compares our method that adjusts the noise schedule at every steps with a fully predefined schedule. This table shows that even though we use VGG11 which is not a cost efficient method to estimate the noise, the generation time increases by a moderate amount. Conclusions\n\nWhen employing diffusion models with a limited number of steps, it is required to carefully choose the schedule of the synthesis process.",
    "001534ec4e6d656722fddc81ba532779cea76875-9": "Some of the previous methods perform a grid search to find the optimal global noise parameters per each number of steps. However, this fixed selection does not adjust according to the specific sample that is being generated. Our method adjusts on-the-fly the noise parameters and thus alters the subsequent states of the process. Our solution is based on estimating, during inference, the current noise level. It is, therefore, generic and independent, given the current sample, from the conditioning parameters. It remains for future work to check whether the same network can be used across multiple datasets. References\n\nBi\u0144kowski et al. (2019) Bi\u0144kowski, M.; Donahue, J.; Dieleman, S.; Clark, A.; Elsen, E.; Casagrande, N.; Cobo, L. C.; and Simonyan, K. 2019. High fidelity speech synthesis with adversarial networks. arXiv preprint arXiv:1909.11646. Chen et al. (2020) Chen, N.; Zhang, Y.; Zen, H.; Weiss, R. J.; Norouzi, M.; and Chan, W. 2020. WaveGrad: Estimating gradients for waveform generation.",
    "001534ec4e6d656722fddc81ba532779cea76875-10": "arXiv preprint arXiv:2009.00713. Deng et al. (2009) Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 248\u2013255. Ieee. Donahue, McAuley, and Puckette (2018) Donahue, C.; McAuley, J.; and Puckette, M. 2018. Adversarial audio synthesis. arXiv preprint arXiv:1802.04208. Gao et al. (2020) Gao, R.; Song, Y.; Poole, B.; Wu, Y. N.; and Kingma, D.",
    "001534ec4e6d656722fddc81ba532779cea76875-11": "P. 2020. Learning Energy-Based Models by Diffusion Recovery Likelihood.",
    "001534ec4e6d656722fddc81ba532779cea76875-12": "arXiv preprint arXiv:2012.08125. Heusel et al. (2017) Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium.",
    "001534ec4e6d656722fddc81ba532779cea76875-13": "arXiv preprint arXiv:1706.08500. Ho, Jain, and Abbeel (2020) Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239. Hochreiter and Schmidhuber (1997) Hochreiter, S.; and Schmidhuber, J. 1997. Long short-term memory. Neural computation, 9(8): 1735\u20131780. Hoogeboom et al. (2021) Hoogeboom, E.; Nielsen, D.; Jaini, P.; Forr\u00e9, P.; and Welling, M. 2021. Argmax Flows and Multinomial Diffusion: Towards Non-Autoregressive Language Models. arXiv preprint arXiv:2102.05379. Hyv\u00e4rinen and Dayan (2005) Hyv\u00e4rinen, A.; and Dayan, P. 2005. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4). Ito and Johnson (2017) Ito, K.; and Johnson, L. 2017. The LJ Speech Dataset. https://keithito.com/LJ-Speech-Dataset/. Jiaming Song and Ermon (2020) Jiaming Song, C.",
    "001534ec4e6d656722fddc81ba532779cea76875-14": "M.; and Ermon, S. 2020. Denoising Diffusion Implicit Models. https://github.com/ermongroup/ddim.",
    "001534ec4e6d656722fddc81ba532779cea76875-15": "Jonathan Ho (2020) Jonathan Ho, P. A., Ajay Jain. 2020. Denoising Diffusion Probabilistic Models. https://github.com/hojonathanho/diffusion. Kalchbrenner et al. (2018) Kalchbrenner, N.; Elsen, E.; Simonyan, K.; Noury, S.; Casagrande, N.; Lockhart, E.; Stimberg, F.; Oord, A.; Dieleman, S.; and Kavukcuoglu, K. 2018. Efficient neural audio synthesis. In International Conference on Machine Learning, 2410\u20132419. PMLR. Karras et al. (2020) Karras, T.; Laine, S.; Aittala, M.; Hellsten, J.; Lehtinen, J.; and Aila, T. 2020. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8110\u20138119. Kingma and Ba (2014) Kingma, D.; and Ba, J. 2014. Adam: A Method for Stochastic Optimization. International Conference on Learning Representations. Kong et al. (2020) Kong, Z.; Ping, W.; Huang, J.; Zhao, K.; and Catanzaro, B. 2020. Diffwave: A versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761.",
    "001534ec4e6d656722fddc81ba532779cea76875-16": "Kubichek (1993) Kubichek, R. 1993. Mel-cepstral distance measure for objective speech quality assessment. In Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing, volume 1, 125\u2013128 vol.1. Liu, Lee, and Jordan (2016) Liu, Q.; Lee, J.; and Jordan, M. 2016. A kernelized Stein discrepancy for goodness-of-fit tests. In International conference on machine learning, 276\u2013284. PMLR. Liu et al. (2015) Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep Learning Face Attributes in the Wild. In Proceedings of International Conference on Computer Vision (ICCV). Luhman and Luhman (2021) Luhman, E.; and Luhman, T. 2021. Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed. CoRR, abs/2101.02388. Luo and Mesgarani (2019) Luo, Y.; and Mesgarani, N. 2019. Conv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for speech separation. IEEE/ACM transactions on audio, speech, and language processing, 27(8): 1256\u20131266. Obukhov et al. (2020) Obukhov, A.; Seitzer, M.; Wu, P.-W.; Zhydenko, S.; Kyl, J.; and Lin, E. Y.-J. 2020. High-fidelity performance metrics for generative models in PyTorch. Version: 0.2.0, DOI: 10.5281/zenodo.3786540. Oord et al. (2016) Oord, A. v. d.; Dieleman, S.; Zen, H.; Simonyan, K.; Vinyals, O.; Graves, A.; Kalchbrenner, N.; Senior, A.; and Kavukcuoglu, K. 2016. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499. Rasul et al. (2021) Rasul, K.; Seward, C.; Schuster, I.; and Vollgraf, R. 2021. Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting. arXiv preprint arXiv:2101.12072. Razavi, Oord, and Vinyals (2019) Razavi, A.; Oord, A. v. d.; and Vinyals, O. 2019. Generating diverse high-fidelity images with vq-vae-2.",
    "001534ec4e6d656722fddc81ba532779cea76875-17": "arXiv preprint arXiv:1906.00446. Rix et al. (2001) Rix, A. W.; Beerends, J. G.; Hollier, M. P.; and Hekstra, A.",
    "001534ec4e6d656722fddc81ba532779cea76875-18": "P. 2001. Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs.",
    "001534ec4e6d656722fddc81ba532779cea76875-19": "In 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing.",
    "001534ec4e6d656722fddc81ba532779cea76875-20": "Proceedings (Cat. No.01CH37221), volume 2, 749\u2013752 vol.2. Simonyan and Zisserman (2014) Simonyan, K.; and Zisserman, A. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556. Sohl-Dickstein et al. (2015) Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and Ganguli, S. 2015. Deep unsupervised learning using nonequilibrium thermodynamics.",
    "001534ec4e6d656722fddc81ba532779cea76875-21": "In International Conference on Machine Learning, 2256\u20132265. PMLR. Song, Meng, and Ermon (2021) Song, J.; Meng, C.; and Ermon, S. 2021. Denoising Diffusion Implicit Models. In International Conference on Learning Representations. Song and Ermon (2019) Song, Y.; and Ermon, S. 2019. Generative modeling by estimating gradients of the data distribution. arXiv preprint arXiv:1907.05600. Song et al. (2020) Song, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Ermon, S.; and Poole, B. 2020. Score-Based Generative Modeling through Stochastic Differential Equations. arXiv preprint arXiv:2011.13456. Taal et al. (2011) Taal, C. H.; Hendriks, R. C.; Heusdens, R.; and Jensen, J. 2011. An Algorithm for Intelligibility Prediction of Time\u2013Frequency Weighted Noisy Speech. IEEE Transactions on Audio, Speech, and Language Processing, 19(7): 2125\u20132136. Vaswani et al. (2017) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762. Vovk (2020) Vovk, I. 2020. WaveGrad. https://github.com/ivanvovk/WaveGrad. Yu et al. (2015) Yu, F.; Zhang, Y.; Song, S.; Seff, A.; and Xiao, J. 2015. LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop.",
    "001534ec4e6d656722fddc81ba532779cea76875-22": "arXiv preprint arXiv:1506.03365. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Thu Mar 7 00:48:22 2024 by LaTeXML"
}