{
    "003326a15fc4a8833785a47a741d7712474fa256-0": "Supplementary material for LeViT: a Vision Transformer in ConvNet\u2019s Clothing for Faster Inference\n\nIn this supplementary material, we report details and results that did not fit in the main paper: Appendix A details the timings of constituent block and provides more details about our ablation. We provide visualizations of the attention bias in Appendix B. Appendix A Detailed analysis\n\nA.1 Block timings\n\nIn this section we compare the differences in design between DeiT and LeViT blocks from the perspective of a detailed runtime analysis. We measure the runtime of their constituent parts side-by-side in the supplementary Table 1. For DeiT-Tiny, we replace the GELU activation with Hardswish, as otherwise it dominates the runtime. For DeiT, we consider a block from DeiT-tiny. For LeViT, we consider a block from the first stage of LeViT-256. Both operate at resolution and have comparable run times, although LeViT is 33% wider ( vs ). Note that stage 1 is the most expensive part of LeViT-256. In stages 2 and 3, the cost is lower due to the reduction in resolution (see Figure 4 of the main paper). LeViT spends less time calculating the attention , but more time on the subsequent matrix product . Despite having the larger block width , LeViT spends less time on the MLP component as the expansion factor is halved from four to two. A.2 More details on our ablation\n\nHere we give additional details of the ablation experiments in Section 5.6 and Table 4 of the main paper. A1 \u2013 without pyramid shape. We test the effect of the LeViT pyramid structure, we replace the three stages with a single stage of depth 11 at resolution . To preserve the FLOP count, we take , and . A6 \u2013 without wider blocks. Compared to DeiT, LeViT blocks are relatively wide given the number of FLOPs, with smaller keys and MLP expansion factors. To test this change we modify LeViT-128S to have more traditional blocks while preserving the number of FLOPs.",
    "003326a15fc4a8833785a47a741d7712474fa256-1": "We therefore take to all have dimension , and for the three stages. As in DeiT, the MLP expansion ratio is 4. In the subsampling layers we use , respectively. Appendix B Visualizations: attention bias\n\nThe attention bias maps from Eqn. 1 in the main paper are just two-dimensional maps. Therefore we can vizualize them, see Figure 1. They can be read as the amount of attention between two pixels that are at a certain relative position. The lowest values of the bias are low enough (-20) to suppress the attention between the two pixels, since they are input to a softmax. We can observe that some heads are quite uniform, while other heads specialize in nearby pixels (\\egmost heads of the shrinking attention). Some are clearly directional, \\egheads 1 and 4 of Stage 2/block 1 handle the pixels adjacent vertically and horizontally (respectively). Head 1 of stage 2, block 4 has a specific period-2 pattern that may be due to the fact that its output is fed to a sub-sampling filter in the next shrinking attention block.",
    "003326a15fc4a8833785a47a741d7712474fa256-2": "\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Sat Mar 16 23:54:56 2024 by LaTeXML"
}