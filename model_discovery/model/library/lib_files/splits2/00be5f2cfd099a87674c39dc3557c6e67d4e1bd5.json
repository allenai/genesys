{
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-0": "BiViT: Extremely Compressed Binary Vision Transformers\n\nYefei He1 Zhenyu Lou1 Luoming Zhang1 Jing Liu2 Weijia Wu1 Hong Zhou1 Bohan Zhuang2 1Zhejiang University, China 2ZIP Lab, Monash University, Australia H.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-1": "Zhou and B. Zhuang are corresponding authors. Abstract\n\nModel binarization can significantly compress model size, reduce energy consumption, and accelerate inference through efficient bit-wise operations. Although binarizing convolutional neural networks have been extensively studied, there is little work on exploring binarization of vision Transformers which underpin most recent breakthroughs in visual recognition. To this end, we propose to solve two fundamental challenges to push the horizon of Binary Vision Transformers (BiViT). First, the traditional binary method does not take the long-tailed distribution of softmax attention into consideration, bringing large binarization errors in the attention module. To solve this, we propose Softmax-aware Binarization, which dynamically adapts to the data distribution and reduces the error caused by binarization. Second, to better preserve the information of the pretrained model and restore accuracy, we propose a Cross-layer Binarization scheme that decouples the binarization of self-attention and multi-layer perceptrons (MLPs), and Parameterized Weight Scales which introduce learnable scaling factors for weight binarization. Overall, our method performs favorably against state-of-the-arts by 19.8% on the TinyImageNet dataset. On ImageNet, our BiViT achieves a competitive 75.6% Top-1 accuracy over Swin-S model. Additionally, on COCO object detection, our method achieves an mAP of 40.8 with a Swin-T backbone over Cascade Mask R-CNN framework. 1 Introduction\n\nVision Transformer (ViT) [21] and its variants have achieved great success in a variety of computer vision tasks, such as image classification [21, 49, 24], object detection [38, 22, 11], semantic segmentation [82, 64, 12], etc. However, massive parameters and calculations of the Transformer models hinder their applications on portable devices such as mobile phones. To tackle the efficiency bottlenecks, various model compression algorithms have been widely studied, such as distillation [65, 67, 33], pruning [58, 84, 75] and quantization [43, 45, 42]. Among them, binary neural networks (BNN) [15, 62, 8] aggressively compress weights and activations to a single bit, which delivers savings on memory consumption, and enables efficient - bit-wise operations to greatly accelerate model inference and reduce energy consumption. However, the performance degradation restricts the wide application of BNNs, which is mainly caused by the limited representational ability and difficulty in optimization. To improve the performance of BNNs, binarized convolutional neural networks (CNNs) literature has been extensively studied to design accurate binarization functions [62, 83, 8], enhance the representation ability [54, 52, 86] and relieve the gradient approximation error in optimization [61, 30, 4]. Also, many attempts have been made in previous studies to binarize BERT [19] for natural language processing (NLP) tasks, such as calibrating the attention value range mismatch [60], customizing knowledge distillation and techniques in binary CNNs to Transformers [50]. However, there are few studies on the binarization of ViTs so far. Therefore, it is highly critical and imperative to explore BiViT for diverse edge devices to infer ViT at low-latency and low-power for real-world flexibility. In addition to the common challenges mentioned above, binarizing Transformers presents two new exclusive challenges. Firstly, it lacks effective methods for accurately binarizing softmax attention. Self-attention module aims to encode pairwise similarity between all the tokens [69], which is very different from convolutional or fully-connected layers. Specifically, the values of attention scores are all positive values between (0, 1) and exhibit long-tailed distributions after operation (See Figures 1 (b) and 3). In contrast, the ordinary weights have both positive and negative values and follow a bell-shaped distribution. Moreover, attention scores are generated during inference while the ordinary weights are fixed after finishing training. Consequently, the functionality and data distribution of softmax attention differ significantly from ordinary weights. As to this problem, the recent study BiBERT [60] proposes to maximize the information entropy of binary attention scores by applying function on pre-softmax attentions, resulting in the balanced number of zeros and ones, as shown in Figure 1 (a). However, the softmax attention scores are actually dominated by few elements, thus the number of ones in binary attentions should be much less than the number of zeros to ensure a low quantization error (see Figure 1 (b)). In other words, BiBERT follows a softmax-agnostic approach and overlooks the effect of on the distribution, resulting in mismatched attention score distributions before and after binarization and leading to significant quantization errors (See Table 1). Another study BiT [50] proposes to learn both scales and thresholds for weight and attention binarization, making them fixed during inference. While this method works well for weight binarization, it neglects the dynamics of attention scores and cannot adapt well to the changing distribution of attentions, as this approach remains softmax-agnostic at inference time as well. Secondly, how to preserve the information in the pretrained ViTs during binarization is under explored. Unlike binary CNNs that perform well when training from scratch [61, 68, 52], we observe that BiViTs heavily rely on pretrained models and are sensitive to quantization, as shown in Figure 2. Even if the initial weights are derived from the pretrained model, directly binarizing all parameters still causes a huge loss of pretrained information, which then leads to a severe performance drop. Also, the loss of pretrained information is difficult for Transformers to recover through quantization-aware training (QAT). In particular, MLP modules account for nearly half of the computations and parameters within a Transformer [47]. They are mainly composed of convolutions, which are widely recognized to be difficult to binarize due to the limited representational capability [86, 23, 7]. Therefore, the effective binarization of softmax attention and the retention of information from pretrained models remain open questions. To reduce the quantization error in binarizing attentions, we first analyze the long-tailed distribution of softmax attention scores and discover their differing patterns across different attention vectors. To adaptively search the optimal thresholds for binarization, we propose an optimization algorithm based on sparse coding and coordinate descent, and further propose an efficient approximation called Softmax-aware Binarization (SAB) to avoid conducting the optimization on each forward pass. Moreover, to retain pretrained information and further enhance the model representational capability, we then propose Cross-layer Binarization (CLB) to decouple the quantization of self-attention and MLPs to avoid mutual interference and introduce Parameterized Weight Scales (PWS) for weights binarization. To our best knowledge, we are the pioneering work to probe binarizing Transformers for vision tasks. In summary, our contributions are as follows:\n\n\u2022\n\nWe are the pioneering work that explores binary vision Transformers, a demanding recipe for efficient ViT inference. \u2022\n\nWe design a Softmax-aware Binarization scheme for the self-attention module, which adapts to the long-tailed attention scores distribution and greatly reduces the quantization error. \u2022\n\nWe propose Cross-layer Binarization and Parameterized Weight Scales to retain pretrained information and further enhance the representational ability of BiViTs, improving convergence and accuracy. \u2022\n\nExperiments on TinyImageNet and ImageNet for image classification, and COCO for object detection, demonstrate that it consistently outperforms current state-of-the-arts by large margins, serving as a strong baseline for future research. 2 Related Work\n\n2.1 Vision Transformers\n\nTransformer [69] is initially proposed to process long sequences in NLP tasks. ViT [21] first adapts Transformers to vision tasks by splitting images into patches and processing them as token sequences. DeiT [66] further improves the data efficiency of vision Transformers. Benefiting from the global receptive field and the powerful long-range modeling capabilities of self-attention, ViT demonstrates promising performance against CNN counterparts. Many follow-up works are proposed to explore hierarchical structures [49, 70, 81], insert the convolutional inductive bias [41, 25, 17] and apply ViTs to various vision tasks [80, 77, 22]. However, the inference speed of ViTs is generally slower than that of CNNs in practical applications [40]. The reasons mainly include the lack of special optimization (such as Winograd [48] for convolutional layers) and the quadratic computational complexity of the self-attention module. To reduce the computational complexity of ViTs, many methods have been proposed, including linear complexity attention [63, 9, 71], network pruning [31, 74, 13] and quantization [43, 45, 53]. However, current Transformer quantization literature mainly focuses on fixed-point quantization, either through Quantization-Aware Training (QAT) [39, 43] or Post-Training Quantization (PTQ) [53, 45, 76]. Research on ternary or binary quantization remains to be studied. 2.2 Binary Neural Networks\n\nBNNs seek to quantize both weights and activations to 1-bit, which greatly reduces the complexity of the model. The binarization of models usually requires QAT to restore accuracy. To overcome the non-differentiability of quantizer during training and the limited representational capacity, many methods have been proposed to help binarize CNNs, such as binary-friendly model structures [54, 46, 85, 57, 7, 6], knowledge distillation [52, 56, 55], gradient approximation [28, 61, 79, 20], optimizer selection [51, 1, 16], Although some of them are also effective for Transformer models, as analyzed in BiT [50], methods targeting on binary Transformers still need to be developed to relieve accuracy degradation. The literature closely related to our work includes BinaryBERT [3], BiBERT [60] and BiT [50]. They propose techniques to improve the binarization of BERT [19] variants, such as customized binarization functions and model distillation, and evaluate them on NLP tasks. However, none of these methods are evaluated on vision tasks. In the following sections, we will migrate these methods to DeiT [66], Swin [49] and NesT [81] as our baselines to test their performance and analyze the key challenges. Then we propose to improve BiViT\u2019s performance with our SAB, CLB and PWS. To the best of our knowledge, we are the pioneering work studying BiViTs. 3 Method\n\n3.1 Preliminaries\n\nGenerally, standard BNNs follow [62] to use function to binarize weights and activations to {-1, +1}, and exploit Straight-Through Estimator (STE) [5] to overcome the non-differentiability of the function, as follows:\n\nx ^ = Sign ( x ) = { + 1 , if x \u2265 0 \u2212 1 , otherwise , \\displaystyle{\\hat{x}=\\mathrm{Sign}}(x)=\\left\\{\\begin{aligned} +1&,\\mathrm{if}\\ x\\geq 0\\\\\n-1&,\\mathrm{otherwise},\\end{aligned}\\right.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-2": "(1)\n\n\u2202 \u2112 \u2202 x \u2248 { \u2202 \u2112 \u2202 x ^ , if | x | \u2264 1 0 , otherwise . \\displaystyle\\frac{\\partial\\mathcal{L}}{\\partial x}\\approx\\left\\{\\begin{aligned} \\frac{\\partial\\mathcal{L}}{\\partial\\hat{x}}&,\\mathrm{if}\\ \\lvert x\\rvert\\leq 1\\\\\n\\noindent 0&,\\mathrm{otherwise}.\\end{aligned}\\right. (2)\n\nTo approximate the full-precision , BNNs further introduce a scaling factor to reduce the quantization error:\n\n\u03b1 = \u2225 \ud835\udc31 \u2225 \u2113 1 n , \ud835\udc31 \u2248 \u03b1 \u200b \ud835\udc31 ^ . formulae-sequence \ud835\udefc subscript delimited-\u2225\u2225 \ud835\udc31 subscript \u2113 1 \ud835\udc5b \ud835\udc31 \ud835\udefc ^ \ud835\udc31 \\alpha=\\frac{{\\lVert\\mathbf{x}\\rVert}_{\\ell_{1}}}{n},\\quad\\mathbf{x}\\approx\\alpha\\hat{\\mathbf{x}}. (3)\n\nHowever, binarization using can be problematic in Transformers and be very different from CNNs. Specifically, in the self-attention mechanism [69], attention is calculated as\n\nAttention \u200b ( \ud835\udc10 , \ud835\udc0a , \ud835\udc15 ) = Softmax \u200b ( \ud835\udc10\ud835\udc0a T d k ) \u200b \ud835\udc15 , Attention \ud835\udc10 \ud835\udc0a \ud835\udc15 Softmax superscript \ud835\udc10\ud835\udc0a \ud835\udc47 subscript \ud835\udc51 \ud835\udc58 \ud835\udc15 \\displaystyle\\text{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})=\\mathrm{Softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{T}}{\\sqrt{d_{k}}}\\right)\\mathbf{V}, (4)\n\nwhere , , are query, key and value matrics respectively, and is the dimension of the key. In the following, we will denote softmax attention vector (i.e., one row of the softmax attention matrix) as and pre-softmax attention vector as . According to the definition of , the results are non-negative and they will all be after function. In order to solve the problem of value range mismatch, BiBERT [60] proposes to use the function to binarize pre-softmax attention scores to {0, 1}, which is defined as:\n\nBool ( a p ) = { 1 , if a p \u2265 0 0 , otherwise . \\displaystyle\\mathrm{Bool}(a_{p})=\\left\\{\\begin{aligned} 1&,\\mathrm{if}\\ a_{p}\\geq 0\\\\\n0&,\\mathrm{otherwise}.\\end{aligned}\\right. (5)\n\nHowever, this will lead to huge quantization errors since is totally discarded, as we will analyze in Section 3.2. 3.2 Softmax-aware Binarization\n\nSelf-attention is designed to model global relationships among different patches (tokens) and focuses on important token pairs. Figure 3 presents the distribution of attention scores in the pretrained NesT-T [81] model. We observe that after operation, attention scores follow a long-tailed distribution and more than of them are less than , which is highly sparse. To further investigate this distribution, we take a deep look at an actual attention vector from the NesT-T pretrained model. As shown in Figure 1 (a), if we follow BiBERT [60] and use function to binarize pre-softmax attentions, nearly half of the attention scores are set to , indicating they have the same contribution to binarization, which is inconsistent with the actual distribution of softmax attention scores where only few values dominate (see Figure 1 (b)). BiT [50] proposes learnable thresholds for binarization but it is fixed after training, while distributions of attention scores can vary with different input images and may differ between each attention vector, leading to a suboptimal solution. In order to reduce the quantization error while binarizing softmax attention scores, we argue that the ideal binarization method should satisfy the following two properties: 1) The proportion of activated scores (set to ) in binary attentions should be smaller compared to directly using the function. As shown in Figure 3, most values are around 0, which barely contributes to the result during calculation, and only a few significant values are considered. 2) The activation thresholds should not be a fixed value. is operated on every row-wise attention vector and different softmax attention vectors follow distinct distributions. For example, the maximum value of some of them can reach , while the others are only about . Empirically, even though most softmax attentions are dominated by only a few elements, the thresholds to activate should be different across all attention vectors. To achieve this, the key is to find the optimal threshold for binarizing each softmax attention vector (i.e., is different for each row). Inspired by sparse coding [36] and LQ-Nets [78], we formulate the quantized attention vector by the inner product between a basis vector and the binary encoding vector :\n\n\ud835\udc1a q = \ud835\udc2f T \u200b \ud835\udc1b , subscript \ud835\udc1a \ud835\udc5e superscript \ud835\udc2f \ud835\udc47 \ud835\udc1b \\displaystyle\\mathbf{a}_{q}=\\mathbf{v}^{T}\\mathbf{b}, (6)\n\nwhere is the target bitwidth.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-3": "Then the optimization problem can be formulated as:\n\n\ud835\udc2f \u2217 , \ud835\udc1b \u2217 = arg \u2061 min \ud835\udc2f , \ud835\udc1b \u2225 \ud835\udc2f T \u200b \ud835\udc1b \u2212 \ud835\udc1a s \u2225 2 2 , s . t . \ud835\udc1b \u2208 { 0 , 1 } k \u00d7 n . formulae-sequence superscript \ud835\udc2f superscript \ud835\udc1b subscript \ud835\udc2f \ud835\udc1b superscript subscript delimited-\u2225\u2225 superscript \ud835\udc2f \ud835\udc47 \ud835\udc1b subscript \ud835\udc1a \ud835\udc60 2 2 \ud835\udc60 \ud835\udc61 \ud835\udc1b superscript 0 1 \ud835\udc58 \ud835\udc5b \\displaystyle\\mathbf{v^{*}},\\mathbf{b^{*}}=\\mathop{\\arg\\!\\min}\\limits_{\\mathbf{v},\\mathbf{b}}{\\lVert{\\mathbf{v}^{T}\\mathbf{b}-\\mathbf{a}_{s}}\\rVert}_{2}^{2},\\quad s.t.\\ \\mathbf{b}\\in\\mathbb{\\{}0,1\\}^{k\\times n}. (7)\n\nIn this paper, the bitwidth is set to 1, thus the basis vector becomes a scalar . However, with both and to be solved, brute-force search can be computationally expensive. Instead, the optimization problem can be efficiently solved in a coordinate descent approach. Specifically, we alternatively optimize the basis and binary encoding vector while keeping another fixed: Update : With the fixed binary encoding vector , the optimization problem will degenerate to a special case of linear regression. Therefore, the optimal can be derived by:\n\nv \u2217 = \ud835\udc1a s \u22c5 \ud835\udc1b \u2225 \ud835\udc1b \u2225 2 2 , superscript \ud835\udc63 \u22c5 subscript \ud835\udc1a \ud835\udc60 \ud835\udc1b superscript subscript delimited-\u2225\u2225 \ud835\udc1b 2 2 v^{*}=\\frac{\\mathbf{a}_{s}\\cdot\\mathbf{b}}{{\\lVert\\mathbf{b}\\rVert}_{2}^{2}}, (8)\n\nwhere represents the dot product between two vectors. Update : Since we get the optimal with Eq. (8), the two values for binarization becomes . Then the optimal threshold can be simply calculated as:\n\nT \u2217 = 0 + v \u2217 2 . superscript \ud835\udc47 0 superscript \ud835\udc63 2 T^{*}=\\frac{0+v^{*}}{2}. (9)\n\nThen we binarize the full-precision softmax attention vector with the threshold to update the binary encoding :\n\n\ud835\udc1b \u2217 = Bool \u200b ( \ud835\udc1a s \u2212 T \u2217 ) . superscript \ud835\udc1b Bool subscript \ud835\udc1a \ud835\udc60 superscript \ud835\udc47 \\mathbf{b^{*}}=\\mathrm{Bool}(\\mathbf{a}_{s}-T^{*}). (10)\n\nThe coordinate descent optimization process is summarized in Algorithm 1. After iterations, the quantization error between binary attentions and full-precision softmax attentions decreases significantly, as shown in the second row of Table 1. Although this optimization strategy minimizes the quantization error, it is not practical to optimize each generated attention vector during inference. Besides, we calculate an optimal for each attention vector, which introduces an extra computational burden. To simplify the optimization process and reduce computational complexity, we seek to establish a relationship between the optimal thresholds (calculated by Eq. (9)) and the distribution of the softmax attention scores. In practice, we sample images from ImageNet [18] dataset, obtain attention vectors and calculate the corresponding optimal thresholds with the proposed method. As illustrated in Figure 4, we observe that the optimal thresholds show a strong correlation with the maximum values of the softmax attention vectors. Therefore, we estimate a coefficient via linear regression with these sampled data to approximate thresholds for each attention vector and thus accelerate the optimization:\n\nT = \u03b2 \u200b Max \u200b ( \ud835\udc1a s ) . \ud835\udc47 \ud835\udefd Max subscript \ud835\udc1a \ud835\udc60 T=\\beta\\mathrm{Max}(\\mathbf{a}_{s}). (11)\n\nExperimental result demonstrates that this approximation barely increases the quantization error, as shown in the third row of Table 1. However, compared with the previous methods [60, 3, 50], multiplying the basis scalar by the binary encoding vector to get (as defined in Eq. (6)) for each attention vector still introduces extra computation. To keep the same computational complexity as previous methods, we make a second approximation and further discard the basis scalar . In this case, the quantization error is shown in the fourth row of Table 1. It should be noted that this step is simply a trade-off between accuracy and complexity. By default, we use the algorithm with two approximations for experiments. Note that there are also many intuitive schemes for approximating the operation, such as Top- algorithm with a learnable parameter and learnable thresholds proposed in BiT [50]. For Top- algorithm, it can take heavy computations to obtain and needs to find maximum values during inference. Instead, we only need to find the maximum value through the proposed twice approximation algorithm, which is faster and barely increases error. Compared with BiT [50], our method calculates the thresholds dynamically and can adapt well to the attention distributions during inference. In the backward pass, BiBERT employs the STE to propagate gradients from binary attentions to pre-softmax attentions without considering the operation:\n\n\u2202 \u2112 \u2202 \ud835\udc1a p \u2248 \u2202 \u2112 \u2202 \ud835\udc1a q . \u2112 subscript \ud835\udc1a \ud835\udc5d \u2112 subscript \ud835\udc1a \ud835\udc5e \\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{a}_{p}}\\approx\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{a}_{q}}. (12)\n\nIn contrast, our proposed method employs the STE to propagate gradients from binary attentions to softmax attentions , making it Softmax-aware during back-propagation:\n\n\u2202 \u2112 \u2202 \ud835\udc1a p = \u2202 \u2112 \u2202 \ud835\udc1a s \u200b \u2202 \ud835\udc1a s \u2202 \ud835\udc1a p \u2248 \u2202 \u2112 \u2202 \ud835\udc1a q \u200b \u2202 \ud835\udc1a s \u2202 \ud835\udc1a p . \u2112 subscript \ud835\udc1a \ud835\udc5d \u2112 subscript \ud835\udc1a \ud835\udc60 subscript \ud835\udc1a \ud835\udc60 subscript \ud835\udc1a \ud835\udc5d \u2112 subscript \ud835\udc1a \ud835\udc5e subscript \ud835\udc1a \ud835\udc60 subscript \ud835\udc1a \ud835\udc5d \\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{a}_{p}}=\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{a}_{s}}\\frac{\\partial\\mathbf{a}_{s}}{\\partial\\mathbf{a}_{p}}\\approx\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{a}_{q}}\\frac{\\partial\\mathbf{a}_{s}}{\\partial\\mathbf{a}_{p}}. (13)\n\nWe will further show in Section 4.4.1 that this greatly helps the training of BiViTs. Overall, the training process of the proposed SAB is summarized in Algorithm 2. 3.3 Information Preservation\n\n3.3.1 Cross-layer Binarization\n\nCompared with binary BERT [60, 50, 3], we find that BiViTs are more difficult to optimize. To justify this, we directly migrate BiBERT [60] to Swin-T [49] and NesT-T [81] to evaluate its performance on image classification tasks. The results are shown in Table 2. We observe that its accuracy degradation in image classification tasks can reach on TinyImageNet dataset, which indicates that vanilla BiViTs cannot make good use of the pretrained information and is difficult to optimize on vision tasks. To explore the reasons, we present the architecture and parameters of Swin-T as an example in Figure 6. As analyzed in Section 1, the MLP modules are hard to quantize due to the limited representational capability of convolutions and have more parameters than the self-attention module. To tackle this problem, we propose Cross-layer Binarization (CLB), which is analogous to the previous two-step training scheme [87, 55], to decouple the quantization of self-attention module and MLP module to reduce mutual interference. Specifically, in the first stage, we keep MLPs to full precision and binarize all the self-attention modules with our SAB scheme. Then in the second stage, we binarize MLPs to get the final model. Compared with previous two-step training schemes that first binarize activations and then weights [55, 52, 2], CLB is designed for Transformers to relieve the mutual interference and mitigate information loss mainly caused by binarizing MLPs. The experimental results demonstrate that using CLB brings more accuracy improvement than the traditional two-step training scheme, as we will show in Section 4.4.2. 3.3.2 Parameterized Weight Scales\n\nTo further narrow the performance gap between the binarized model and the full-precision counterpart, an intuitive idea is to increase the representation ability of the binarized model. However, this usually results in increased computational complexity. Motivated by SE-Net [32], channel-wise scaling factors can be regarded as the importance of each channel, rather than an approximation of its distribution. In order to preserve the model structure and complexity while enhancing its representation ability, we directly replace ordinary scaling factors (as defined in Eq. (3)) with learnable parameters. The parameterized scaling factors could be optimized in conjunction with other network parameters via backward propagation during training. As shown in Figure 6, the deviation of ordinary scaling factors across channels is small, indicating that the weight distribution of each channel is similar. In contrast, the parameterized scaling factors vary greatly from channel to channel, showing that it learns to pay more attention to specific channels and thus enhancing the representational capacity of the model. 4 Experiments on Image Classification\n\n4.1 Implementation Details\n\nDatasets and architectures. We conduct image classification experiments on two standard benchmarks: TinyImageNet [73] and ImageNet (ILSVRC12) [18]. The input resolution is . For data augmentation, we follow the settings in DeiT [66], which are common practices in ViTs. To demonstrate the versatility of our method, we adopt three widely-used efficient architectures: DeiT [66], Swin [49] and NesT [81]. All the blocks in Transformer models are binarized without exception. For binary attention modules, all weights and intermediate results including , , and projection layers, are binarized. For binary MLP modules, weights are binarized in all experiments. We leave the input embedding layer and output layer unbinarized as it is the common practice of BNNs [62]. The binary operations (BOPs) and floating-point operations (FLOPs) are counted separately and the reported total operations (OPs) are calculated by , following [62, 52]. Training setup. All experiments are implemented with PyTorch [59] and Timm [72] library. The iteration number for coordinate descent is . For both datasets, we employ Adam [34] optimizers without weight decay and train models for 300 epochs using a cosine annealing schedule with 5 epochs of warm-up. The initial learning rate is set to 5e-4. When training is split into two stages, we train 150 epochs at each stage to keep the number of total iterations the same. Knowledge distillation (KD) [29] is used in all experiments. Specifically, we use the distribution loss proposed in [52] for optimization. Before training, all parameters are initialized with the pretrained model. 4.2 Comparison with SOTA methods\n\n4.2.1 Evaluation on TinyImgnet\n\nWe begin our evaluation on the TinyImageNet, and the results are presented in Table 2. Previous Transformer binarization methods [50, 60] exhibit significant accuracy degradation, which limits their practicality. Remarkably, in some cases, the performance of BiT [50] can be worse than BiBERT [60], indicating that the learned thresholds may not be suitable for softmax attentions during inference or can even be detrimental. By contrast, our proposed SAB method dynamically determines the thresholds according to the distribution of softmax attention. For models with all weights and activations binarized, our approach can boost the Top-1 accuracy by almost ( vs. for NesT-T), greatly narrowing the performance gap between binary and full-precision models. However, we still observed a large accuracy drop, which can be attributed to the significant contribution of MLP modules to the parameters and their limited representational capability. To enhance practicality, we keep the activations in MLP modules as full-precision. The experimental results demonstrate that preserving activations as full-precision can significantly mitigate the performance degradation caused by fully-binarized MLPs ( vs. for NesT-T), achieving a better trade-off between accuracy and complexity. In this case, the model size can still be compressed by 32, and the costly multiply-accumulate operations can be replaced by cheap accumulations. With this configuration, our method also outperforms previous methods BiBERT and BiT by large margins, with improvements of up to for Nest-T. We also found that binary Swin-T outperforms binary NesT-T, while their full-precision models perform similarly. This difference may be attributed to the mask mechanism in the local window attention of Swin, which limits the number of elements greater than zero in the pre-softmax attention, thereby aiding in model binarization. Nevertheless, our method provides substantial accuracy gains for most Transformers with standard self-attention (like NesT). 4.2.2 Evaluation on ImageNet\n\nWe further evaluate the effectiveness of our method on the ImageNet, and the results are shown in Table 3. To preserve the accuracy, we binarize all weights while leaving the activations in MLP modules full-precision. In all cases, our method obtains the best performance. For full-attention models (like DeiT), the accuracy is significantly lower than local-attention models, especially when the model capacity is insufficient (such as DeiT-T). We speculate that the inductive bias of local information is very important for highly compressed BiViTs. For Swin-T, our method outperforms previous SOTA by a margin of . For NesT-T, where the previous method even fails to converge, our method obtains a Top-1 accuracy. We also conduct experiments on larger ViTs like DeiT-B and Swin-S, achieving a highly competitive accuracy of up to over Swin-S. These results demonstrate the feasibility of BiViTs in visual tasks for the first time. 4.3 Comparison with Binary CNNs\n\nWe present a comparison of the model size, total operations, and accuracy between binary ResNet [27] and BiViTs, as shown in Table 3. CNNs with their convolutional inductive bias and fewer convolution layers can achieve good performance with minimal parameters and operations, surpassing similarly sized BiViTs such as DeiT-T. However, as model capacity increases, the advantages of Transformers become more apparent. The global receptive field and attention mechanism in Transformers provide strong representational capability, resulting in higher accuracy for full-precision ViT models than ResNet models with similar parameters. As the information of the pretrained model is inherited, a stronger teacher model can significantly improve BiViT\u2019s training. For instance, binary Swin-S has similar OPs as full-precision ResNet-34, yet it achieves better accuracy ( vs. ) and reduces model size by . Moreover, we measure the latency of matrix multiplication operations in BiViTs and full-precision models using an RTX3090 GPU, as shown in Table 4. For floating-point (FP) operations, we utilize cuDNN [14], while binary operations are implemented using BTC-BNN [37]. In our default setting where attention modules are binarized while the MLP modules retain FP activations, we observe a reduction in latency compared to its full-precision counterpart on Swin-T. Moreover, when we fully binarize the MLP modules as well, the speedup further increases to . However, due to the long-standing optimization of convolution implementations, such as Winograd [35], the current latency of BiViTs is slightly higher than that of FP CNNs with similar OPs, such as binary Swin-S model and FP ResNet-34. 4.4 Ablation Studies\n\n4.4.1 Effect of Softmax-aware Binarization\n\nFirst, we conducted ablation experiments over Swin-T and NesT-T models on TinyImageNet to prove the effectiveness of the proposed SAB scheme. To eliminate the impact of MLPs, we keep them full-precision and only binarize attention modules. As shown in Table 5, our SAB consistently narrows the accuracy gap between the full-precision teacher and the binary student network, indicating successful suppression of the quantization error in the self-attention module. Incorporating for gradient approximation brings an accuracy improvement of 2.02%, because it mitigates the mismatch issue between backpropagation using STE and the operation in the forward pass.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-4": "Also, we conduct experiments to verify the impact of estimation (see Eq. (11)) on the accuracy of the model. The results in Table 6 show that the model is not sensitive to within a reasonable interval (about to ). By default, is set to to enable efficient bit-shift operation in Eq.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-5": "(11). However, the accuracy of the model decreases when is too small (less than ). This indicates that as the thresholds become too small, our SAB is less effective and too many attention scores are activated. 4.4.2 Effect of Information Preservation\n\nTo demonstrate the effectiveness of our proposed CLB training scheme, we conducted experiments comparing the accuracy of binary Nest-T trained with CLB to those trained using traditional two-step schemes [55], as well as those trained directly with one-step training scheme on TinyImageNet. The activations in MLP modules remain full-precision. The results, shown in Figure 7 and Table 7, demonstrate that using CLB can accelerate the convergence process of the BiViTs and significantly improve the accuracy by compared to one-step training. Furthermore, CLB outperforms the conventional two-step training scheme by , highlighting its strong ability to retain pretrained information. We expect that our CLB and the conventional two-step training methods can be combined to further improve the accuracy since they are orthogonal, but will leave it for future work. For PWS, it can be applied to both self-attention and MLP modules. To show the improvement brought by PWS, we conduct experiments starting with a BiBERT-based baseline and then changing scaling factors to learnable parameters in self-attention and MLP modules separately. As shown in Table 8, PWS is effective in both modules, resulting in an accuracy improvement of 4.2% and 1.3%, respectively. Therefore, we use PWS in both modules by default if they are binarized. 5 Experiments on Object Detection\n\nIn this section, we present the results of object detection and instance segmentation experiments over Swin-T on COCO 2017 [44] validation set. Currently, we only binarize the self-attention modules in the backbone. The experiments are implemented with classic object detection frameworks Mask R-CNN [26] and Cascade Mask R-CNN [10]. For training strategy and hyper-parameters, we follow the implementation in Swin [49], which takes ImageNet pretrained model as initialization and only trains for epochs. Table 9 compares the results of different binarization methods on both frameworks. When evaluated with the Mask R-CNN framework, the proposed method improves the performance over BiBERT by and mAP on the object detection and instance segmentation tasks, respectively. For the Cascade Mask R-CNN framework, the performance improvement brought by our method is even greater, achieving a competitive mAP on object detection. More results on COCO are available in the supplementary material. 6 Conclusion\n\nIn this paper, we have proposed to tackle two fundamental challenges with customized solutions for BiViTs, and have successfully applied BiViTs to visual tasks for the first time. To deal with the long-tailed distribution of softmax attention, we have proposed the Softmax-aware Binarization for self-attention, the core module of Transformers, which greatly reduces the quantization error. To preserve information from pretrained model, we have proposed the Cross-layer Binarization scheme that decouples the quantization of self-attention and MLPs. Moreover, we have introduced Parameterized Weight Scales to enhance representational ability of BiViTs. Our BiViT has achieved significant accuracy improvement over SOTA on the image classification task, with up to 75.6% Top-1 accuracy on ImageNet over Swin-S model. We have also conducted extensive experiments on COCO object detection and instance segmentation, demonstrating that BiViTs can be extended to downstream tasks. In the future, we will further investigate methods to narrow the gap between BiViTs and their full-precision counterparts. Acknowledgement This work was supported by the National Key Research and Development Program of China (2022YFC3602601) and Key Research and Development Program of Zhejiang Province of China (2021C02037). References\n\n[1] Milad Alizadeh, Javier Fern\u00e1ndez-Marqu\u00e9s, Nicholas D Lane, and Yarin Gal. A systematic study of binary neural networks\u2019 optimisation. In International Conference on Learning Representations, volume 63, page 81, 2019. [2] Mehdi Bahri, Ga\u00e9tan Bahl, and Stefanos Zafeiriou. Binary graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9492\u20139501, 2021. [3] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael R Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. In ACL/IJCNLP (1), 2021. [4] Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal operators. In International Conference on Learning Representations, 2018. [5] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-6": "arXiv preprint arXiv:1308.3432, 2013. [6] Joseph Bethge, Christian Bartz, Haojin Yang, Ying Chen, and Christoph Meinel. Meliusnet: An improved network architecture for binary neural networks. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1439\u20131448, 2021. [7] Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. High-capacity expert binary networks. In International Conference on Learning Representations, 2020. [8] Adrian Bulat and Georgios Tzimiropoulos. Xnor-net++: Improved binary neural networks.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-7": "arXiv preprint arXiv:1909.13863, 2019. [9] Han Cai, Chuang Gan, and Song Han. Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition. arXiv preprint arXiv:2205.14756, 2022. [10] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6154\u20136162, 2018. [11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020. [12] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-8": "arXiv preprint arXiv:2102.04306, 2021. [13] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing sparsity in vision transformers: An end-to-end exploration.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-9": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 19974\u201319988. Curran Associates, Inc., 2021. [14] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. cudnn: Efficient primitives for deep learning. arXiv preprint arXiv:1410.0759, 2014. [15] Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1. CoRR, abs/1602.02830, 2016. [16] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-10": "arXiv preprint arXiv:1602.02830, 2016. [17] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. Advances in Neural Information Processing Systems, 34:3965\u20133977, 2021. [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-11": "In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009. [19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-12": "In NAACL-HLT (1), 2019. [20] Rui Ding, Haijun Liu, and Xichuan Zhou. Ie-net: Information-enhanced binary neural networks for accurate classification. Electronics, 11(6):937, 2022. [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. [22] Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, and Wenyu Liu. You only look at one sequence: Rethinking transformer in vision through object detection.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-13": "Advances in Neural Information Processing Systems, 34:26183\u201326197, 2021. [23] Sahaj Garg, Anirudh Jain, Joe Lou, and Mitchell Nahmias. Confounding tradeoffs for neural network quantization. arXiv preprint arXiv:2102.06366, 2021. [24] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv\u00e9 J\u00e9gou, and Matthijs Douze. Levit: a vision transformer in convnet\u2019s clothing for faster inference. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12259\u201312269, 2021. [25] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12175\u201312185, 2022. [26] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017. [27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. [28] Yefei He, Luoming Zhang, Weijia Wu, and Hong Zhou. Binarizing by classification: Is soft function really necessary? arXiv preprint arXiv:2205.07433, 2022. [29] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. [30] Lu Hou, Quanming Yao, and James Tin Yau Kwok. Loss-aware binarization of deep networks.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-14": "In 5th International Conference on Learning Representations, ICLR 2017-Conference Track Proceedings, page 000, 2017. [31] Zejiang Hou and Sun-Yuan Kung. Multi-dimensional vision transformer compression via dependency guided gaussian process search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 3669\u20133678, June 2022. [32] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132\u20137141, 2018. [33] Ding Jia, Kai Han, Yunhe Wang, Yehui Tang, Jianyuan Guo, Chao Zhang, and Dacheng Tao. Efficient vision transformers via fine-grained manifold distillation. arXiv preprint arXiv:2107.01378, 2021. [34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [35] Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4013\u20134021, 2016. [36] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efficient sparse coding algorithms. Advances in neural information processing systems, 19, 2006. [37] Ang Li and Simon Su. Accelerating binarized neural networks via bit-tensor-cores in turing gpus. IEEE Transactions on Parallel and Distributed Systems, 32(7):1878\u20131891, 2021. [38] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527, 2022. [39] Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao, Peng Gao, and Guodong Guo. Q-vit: Accurate and fully quantized low-bit vision transformer. arXiv preprint arXiv:2210.06707, 2022. [40] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. arXiv preprint arXiv:2206.01191, 2022. [41] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to vision transformers. arXiv preprint arXiv:2104.05707, 2021. [42] Zhikai Li and Qingyi Gu. I-vit: integer-only quantization for efficient vision transformer inference. arXiv preprint arXiv:2207.01405, 2022. [43] Zhexin Li, Tong Yang, Peisong Wang, and Jian Cheng. Q-vit: Fully differentiable quantization for vision transformer.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-15": "arXiv preprint arXiv:2201.07703, 2022. [44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014. [45] Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and Shuchang Zhou. Fq-vit: Post-training quantization for fully quantized vision transformer.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-16": "In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 1173\u20131179, 2022. [46] Chunlei Liu, Wenrui Ding, Xin Xia, Baochang Zhang, Jiaxin Gu, Jianzhuang Liu, Rongrong Ji, and David Doermann. Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-17": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2691\u20132699, 2019. [47] Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, and Bohan Zhuang. Ecoformer: Energy-saving attention with linear complexity. In NeurIPS, 2022. [48] Xingyu Liu, Jeff Pool, Song Han, and William J Dally. Efficient sparse-winograd convolutional neural networks. arXiv preprint arXiv:1802.06367, 2018. [49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021. [50] Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, and Yashar Mehdad. Bit: Robustly binarized multi-distilled transformer.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-18": "arXiv preprint arXiv:2205.13016, 2022. [51] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, and Kwang-Ting Cheng. How do adam and training strategies help bnns optimization. In International Conference on Machine Learning, pages 6936\u20136946. PMLR, 2021. [52] Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng. Reactnet: Towards precise binary neural network with generalized activation functions.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-19": "In European conference on computer vision, pages 143\u2013159. Springer, 2020. [53] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization for vision transformer.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-20": "Advances in Neural Information Processing Systems, 34:28092\u201328103, 2021. [54] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In Proceedings of the European conference on computer vision (ECCV), pages 722\u2013737, 2018. [55] Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos. Training binary neural networks with real-to-binary convolutions. In International Conference on Learning Representations, 2019. [56] Asit Mishra and Debbie Marr. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. In International Conference on Learning Representations, 2018. [57] Asit Mishra, Eriko Nurvitadhi, Jeffrey J Cook, and Debbie Marr. Wrpn: Wide reduced-precision networks. In International Conference on Learning Representations, 2018. [58] Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang Wang, Rogerio Feris, and Aude Oliva. Ia-red: Interpretability-aware redundancy reduction for vision transformers.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-21": "Advances in Neural Information Processing Systems, 34:24898\u201324911, 2021. [59] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [60] Haotong Qin, Yifu Ding, Mingyuan Zhang, YAN Qinghua, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xianglong Liu. Bibert: Accurate fully binarized bert. In International Conference on Learning Representations, 2021. [61] Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward and backward information retention for accurate binary neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250\u20132259, 2020. [62] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-22": "In European conference on computer vision, pages 525\u2013542. Springer, 2016. [63] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 3531\u20133539, 2021. [64] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7262\u20137272, 2021. [65] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, pages 10347\u201310357. PMLR, 2021. [66] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, volume 139, pages 10347\u201310357, July 2021. [67] Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-23": "arXiv preprint arXiv:2204.07118, 2022. [68] Zhijun Tu, Xinghao Chen, Pengju Ren, and Yunhe Wang. Adabin: Improving binary neural networks with adaptive binary sets. In European Conference on Computer Vision, pages 379\u2013395. Springer, 2022. [69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 568\u2013578, 2021. [71] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415\u2013424, 2022. [72] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019. [73] Jiayu Wu, Qixiang Zhang, and Guoxi Xu. Tiny imagenet challenge. Technical report, 2017. [74] Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li, and Jan Kautz. Nvit: Vision transformer compression and parameter redistribution. arXiv preprint arXiv:2110.04869, 2021. [75] Hao Yu and Jianxin Wu. A unified pruning framework for vision transformers. arXiv preprint arXiv:2111.15127, 2021. [76] Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. Ptq4vit: Post-training quantization framework for vision transformers.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-24": "arXiv preprint arXiv:2111.12293, 2021. [77] Yanhong Zeng, Huan Yang, Hongyang Chao, Jianbo Wang, and Jianlong Fu. Improving visual quality of image synthesis by a token-based generator with transformers.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-25": "Advances in Neural Information Processing Systems, 34:21125\u201321137, 2021. [78] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for highly accurate and compact deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 365\u2013382, 2018. [79] Luoming Zhang, Yefei He, Zhenyu Lou, Xin Ye, Yuxing Wang, and Hong Zhou. Root quantization: a self-adaptive supplement ste. Applied Intelligence, pages 1\u201310, 2022. [80] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu, Gang Yu, and Chunhua Shen. Topformer: Token pyramid transformer for mobile semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12083\u201312093, 2022. [81] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, Sercan \u00d6 Arik, and Tomas Pfister. Nested hierarchical transformer: Towards accurate, data-efficient and interpretable visual understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 3417\u20133425, 2022. [82] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-26": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6881\u20136890, 2021. [83] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016. [84] Mingjian Zhu, Yehui Tang, and Kai Han. Vision transformer pruning.",
    "00be5f2cfd099a87674c39dc3557c6e67d4e1bd5-27": "arXiv preprint arXiv:2104.08500, 2021. [85] Shilin Zhu, Xin Dong, and Hao Su. Binary ensemble neural network: More bits per network or more networks per bit? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4923\u20134932, 2019. [86] Bohan Zhuang, Chunhua Shen, Mingkui Tan, Peng Chen, Lingqiao Liu, and Ian Reid. Structured binary neural networks for image recognition. International Journal of Computer Vision, 130(9):2081\u20132102, 2022. [87] Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Towards effective low-bitwidth convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7920\u20137928, 2018. Appendix\n\nAppendix A More experimental results on COCO\n\nIn this section, we present additional experimental results of BiViT on the COCO 2017 validation set. As shown in Table A, our method outperforms the competitive BiBERT by a large margin in all metrics. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Thu Mar 14 07:28:10 2024 by LaTeXML"
}