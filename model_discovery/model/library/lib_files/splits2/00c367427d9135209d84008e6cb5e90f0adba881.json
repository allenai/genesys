{
    "00c367427d9135209d84008e6cb5e90f0adba881-0": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers\n\nKai Shen, Zeqian Ju11footnotemark: 1, Xu Tan11footnotemark: 1, Yanqing Liu, Yichong Leng, Lei He Tao Qin, Sheng Zhao, Jiang Bian Microsoft Research Asia & Microsoft Azure Speech https://aka.ms/speechresearch The first three authors contributed equally to this work, and their names are listed in random order.",
    "00c367427d9135209d84008e6cb5e90f0adba881-1": "Corresponding author: Xu Tan, xuta@microsoft.com\n\nAbstract\n\nScaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2. 1 Introduction\n\nHuman speech is full of diversity, with different speaker identities (e.g., gender, accent, timbre), prosodies, styles (e.g., speaking, singing), etc. Text-to-speech (TTS) [1, 2] aims to synthesize natural and human-like speech with both good quality and diversity. With the development of neural networks and deep learning, TTS systems [3, 4, 5, 6, 7, 8, 9, 10, 11] have achieved good voice quality in terms of intelligibility and naturalness, and some systems (e.g., NaturalSpeech [11]) even achieves human-level voice quality on single-speaker recording-studio benchmarking datasets (e.g., LJSpeech [12]). Given the great achievements in speech intelligibility and naturalness made by the whole TTS community, now we enter a new era of TTS where speech diversity becomes more and more important in order to synthesize natural and human-like speech. Previous speaker-limited recording-studio datasets are not enough to capture the diverse speaker identities, prosodies, and styles in human speech due to limited data diversity. Instead, we can train TTS models on a large-scale corpus to learn these diversities, and as a by-product, these trained models can generalize to the unlimited unseen scenarios with few-shot or zero-shot technologies. Current large-scale TTS systems [13, 14, 15] usually quantize the continuous speech waveform into discrete tokens and model these tokens with autoregressive language models. This pipeline suffers from several limitations: 1) The speech (discrete token) sequence is usually very long (a 10s speech usually has thousands of discrete tokens) and the autoregressive models suffer from error propagation and thus unstable speech outputs. 2) There is a dilemma between the codec and language model: on the one hand, the codec with token quantization (VQ-VAE [16, 17] or VQ-GAN [18]) usually has a low bitrate token sequence, which, although eases the language model generation, incurs information loss on the high-frequency fine-grained acoustic details; on the other hand, some improving methods [19, 20] use multiple residual discrete tokens to represent a speech frame, which increases the length of the token sequence multiple times if flattened and incurs difficulty in language modeling. In this paper, we propose NaturalSpeech 2, a TTS system with latent diffusion models to achieve expressive prosody, good robustness, and most importantly strong zero-shot ability for speech synthesis. As shown in Figure 1, we first train a neural audio codec that converts a speech waveform into a sequence of latent vectors with a codec encoder, and reconstructs the speech waveform from these latent vectors with a codec decoder. After training the audio codec, we use the codec encoder to extract the latent vectors from the speech in the training set and use them as the target of the latent diffusion model, which is conditioned on prior vectors obtained from a phoneme encoder, a duration predictor, and a pitch predictor. During inference, we first generate the latent vectors from the text/phoneme sequence using the latent diffusion model and then generate the speech waveform from these latent vectors using the codec decoder. We elaborate on some design choices in NaturalSpeech 2 (shown in Table 1) as follows. \u2022\n\nContinuous vectors instead of discrete tokens. To ensure the speech reconstruction quality of the neural codec, previous works usually quantize speech with multiple residual quantizers. As a result, the obtained discrete token sequence is very long (e.g., if using 8 residual quantizers for each speech frame, the resulting flattened token sequence will be 8 times longer), and puts much pressure on the acoustic model (autoregressive language model). Therefore, we use continuous vectors instead of discrete tokens, which can reduce the sequence length and increase the amount of information for fine-grained speech reconstruction (see Section 3.1). \u2022\n\nDiffusion models instead of autoregressive models. We leverage diffusion models to learn the complex distributions of continuous vectors in a non-autoregressive manner and avoid error propagation in autoregressive models (see Section 3.2). \u2022\n\nSpeech prompting mechanisms for in-context learning. To encourage the diffusion models to follow the characteristics in the speech prompt and enhance the zero-shot capability, we design speech prompting mechanisms to facilitate in-context learning in the diffusion model and pitch/duration predictors (see Section 3.3). Benefiting from these designs, NaturalSpeech 2 is more stable and robust than previous autoregressive models, and only needs one acoustic model (the diffusion model) instead of two-stage token prediction as in [21, 13], and can extend the styles beyond speech (e.g., singing voice) due to the duration/pitch prediction and non-autoregressive generation. We scale NaturalSpeech 2 to 400M model parameters and 44K hours of speech data, and generate speech with diverse speaker identities, prosody, and styles (e.g., singing) in zero-shot scenarios (given only a few seconds of speech prompt). Experiment results show that NaturalSpeech 2 can generate natural speech in zero-shot scenarios and outperform the previous strong TTS systems. Specifically, 1) it achieves more similar prosody with both the speech prompt and ground-truth speech; 2) it achieves comparable or better naturalness (in terms of CMOS) than the ground-truth speech on LibriSpeech and VCTK test sets; 3) it can generate singing voices in a novel timbre either with a short singing prompt, or interestingly with only a speech prompt, which unlocks the truly zero-shot singing synthesis (without a singing prompt). Audio samples can be found in https://speechresearch.github.io/naturalspeech2. 2 Background\n\nWe introduce some background of NaturalSpeech 2, including the journey of text-to-speech synthesis on pursuing natural voice with high quality and diversity, neural audio codec models, and generative models for audio synthesis. 2.1 TTS for Natural Voice: Quality and Diversity\n\nText-to-speech systems [2, 3, 4, 5, 6, 8, 9, 22, 10, 11] aim to generate natural voice with both high quality and diversity. While previous neural TTS systems can synthesize high-quality voice on single-speaker recording-studio datasets (e.g., LJSpeech [12]) and even achieve human-level quality (e.g., NaturalSpeech [11]), they cannot generate diverse speech with different speaker identities, prosodies, and styles, which are critical to ensure the naturalness of the synthesized speech. Thus, some recent works [13, 14, 15] attempt to scale the TTS systems to large-scale, multi-speaker, and in-the-wild datasets to pursue diversity. These systems usually leverage a neural codec to convert speech waveform into discrete token sequence and an autoregressive language model to generate discrete tokens from text, which suffers from a dilemma as shown in Table 2: 1) If the audio codec quantizes each speech frame into a single token with vector-quantizer (VQ) [16, 17, 18], this could ease the token generation in the language model due to short sequence length, but will affect the waveform reconstruction quality due to large compression rate or low bitrate. 2) If the audio codec quantizes each speech frame into multiple tokens with residual vector-quantizer (RVQ) [19, 20], this will ensure high-fidelity waveform reconstruction, but will cause difficulty in autoregressive model generation (error propagation and robust issues) due to the increased length in the token sequence. Thus, previous works such as AudioLM [21] leverage two-stage language models to first generate some coarse-grained tokens in each frame and then generate the remaining fine-grained tokens, which are complicated and incur cascaded errors. To avoid the above dilemma, we leverage a neural codec with continuous vectors and a latent diffusion model with non-autoregressive generation. 2.2 Neural Audio Codec\n\nNeural audio codec [23, 24, 19, 20] refers to a kind of neural network model that converts audio waveform into compact representations with a codec encoder and reconstructs audio waveform from these representations with a codec decoder. Since audio codec is traditionally used for audio compression and transmission, the compression rate is a critical metric and thus discrete tokens with low bitrate are usually chosen as the compact representations. For example, SoundStream [19] and Encodec [20] leverage vector-quantized variational auto-encoders (VQ-VAE) with multiple residual vector-quantizers to compress speech into multiple tokens, and have been used as the intermediate representations for speech/audio generation [21, 25, 13, 14, 15]. Although good reconstruction quality and low bitrate can be achieved by residual vector quantizers, they are mainly designed for compression and transmission purposes and may not be suitable to serve as the intermediate representation for speech/audio generation. The discrete token sequence generated by residual quantizers is usually very long ( times longer if residual quantizers are used), which is difficult for the language models to predict. Inaccurate predictions of discrete tokens will cause word skipping, word repeating, or speech collapse issues when reconstructing speech waveforms from these tokens. In this paper, we design a neural audio codec to convert speech waveform into continuous vectors instead of discrete tokens, which can maintain enough fine-grained details for precise waveform reconstruction without increasing the length of the sequence. 2.3 Generative Models for Speech Synthesis\n\nDifferent generative models have been applied to speech or audio synthesis, and among these, autoregressive models and diffusion models are the two most prominent methods. Autoregressive models have long been used in speech synthesis for waveform generation [23] or acoustic feature generation [3]. Inspired by the success of autoregressive models in language generation [26, 27, 28], autoregressive models have been applied in speech and audio generation [21, 25, 13, 14, 15]. Meanwhile, diffusion models have also been widely used in speech synthesis for waveform generation [29, 30] and acoustic feature generation [31, 32]. Although both models are based on iterative computation (following the left-to-right process or the denoising process), autoregressive models are more sensitive to sequence length and error propagation, which cause unstable prosody and robustness issues (e.g., word skipping, repeating, and collapse). Considering text-to-speech has a strict monotonic alignment and strong source-target dependency, we leverage diffusion models enhanced with duration prediction and length expansion, which are free from robust issues. 3 NaturalSpeech 2\n\nIn this section, we introduce NaturalSpeech 2, a TTS system for natural and zero-shot voice synthesis with high fidelity/expressiveness/robustness on diverse scenarios (various speaker identities, prosodies, and styles). As shown in Figure 1, NaturalSpeech 2 consists of a neural audio codec (an encoder and a decoder) and a diffusion model with a prior (a phoneme encoder and a duration/pitch predictor). Since speech waveform is complex and high-dimensional, following the paradigm of regeneration learning [33], we first convert speech waveform into latent vectors using the audio codec encoder and reconstruct speech waveform from the latent vectors using the audio codec decoder. Next, we use a diffusion model to predict the latent vectors conditioned on text/phoneme input. We introduce the detailed designs of neural audio codec in Section 3.1 and the latent diffusion model in Section 3.2, as well as the speech prompting mechanism for in-context learning in Section 3.3. 3.1 Neural Audio Codec with Continuous Vectors\n\nWe use a neural audio codec to convert speech waveform into continuous vectors instead of discrete tokens, as analyzed in Section 2.1 and 2.2. Audio codec with continuous vectors enjoys several benefits: 1) Continuous vectors have a lower compression rate and higher bitrate than discrete tokens111Since our task is not speech compression but speech synthesis, we do not need a high compression rate or a low bitrate., which can ensure high-quality audio reconstruction. 2) Each audio frame only has one vector instead of multiple tokens as in discrete quantization, which will not increase the length of the hidden sequence. As shown in Figure 2, our neural audio codec consists of an audio encoder, a residual vector-quantizer (RVQ), and an audio decoder: 1) The audio encoder consists of several convolutional blocks with a total downsampling rate of for KHz audio, i.e., each frame corresponds to a ms speech segment. 2) The residual vector-quantizer converts the output of the audio encoder into multiple residual vectors following [19]. The sum of these residual vectors is taken as the quantized vectors, which are used as the training target of the diffusion model. 3) The audio decoder mirrors the structure of the audio encoder, which generates the audio waveform from the quantized vectors. The working flow of the neural audio codec is as follows. Audio \u200b Encoder : h = f enc \u200b ( x ) , : Audio Encoder \u210e subscript \ud835\udc53 enc \ud835\udc65 \\displaystyle{\\rm Audio~{}Encoder}:h=f_{\\rm enc}(x), (1) Residual \u200b Vector \u200b Quantizer : { e j i } j = 1 R = f rvq \u200b ( h i ) , z i = \u2211 j = 1 R e j i , z = { z i } i = 1 n , : Residual Vector Quantizer formulae-sequence subscript superscript subscript superscript \ud835\udc52 \ud835\udc56 \ud835\udc57 \ud835\udc45 \ud835\udc57 1 subscript \ud835\udc53 rvq superscript \u210e \ud835\udc56 formulae-sequence superscript \ud835\udc67 \ud835\udc56 subscript superscript \ud835\udc45 \ud835\udc57 1 subscript superscript \ud835\udc52 \ud835\udc56 \ud835\udc57 \ud835\udc67 subscript superscript superscript \ud835\udc67 \ud835\udc56 \ud835\udc5b \ud835\udc56 1 \\displaystyle{\\rm Residual~{}Vector~{}Quantizer}:\\{e^{i}_{j}\\}^{R}_{j=1}=f_{\\rm rvq}(h^{i}),~{}~{}z^{i}=\\sum\\limits^{R}_{j=1}e^{i}_{j},~{}~{}z=\\{z^{i}\\}^{n}_{i=1}, Audio \u200b Decoder : x = f dec \u200b ( z ) , : Audio Decoder \ud835\udc65 subscript \ud835\udc53 dec \ud835\udc67 \\displaystyle{\\rm Audio~{}Decoder}:x=f_{\\rm dec}(z),\n\nwhere , , and denote the audio encoder, residual vector quantizer, and audio decoder. is the speech waveform, is the hidden sequence obtained by the audio encoder with a frame length of , and is the quantized vector sequence with the same length as . is the index of the speech frame, is the index of the residual quantizer and is the total number of residual quantizers, and is the embedding vector of the codebook ID obtained by the -th residual quantizer on the -th hidden frame (i.e., ). The training of the neural codec follows the loss function in [19]. Actually, to obtain continuous vectors, we do not need vector quantizers, but just an autoencoder or variational autoencoder. However, for regularization and efficiency purposes, we use residual vector quantizers with a very large number of quantizers ( in Figure 2) and codebook tokens ( in Figure 2) to approximate the continuous vectors. By doing this, we have two benefits: 1) When training latent diffusion models, we do not need to store continuous vectors which are memory-cost. Instead, we just store the codebook embeddings and the quantized token IDs, which are used to derive the continuous vectors using Equation LABEL:eq_codec. 2) When predicting the continuous vectors, we can add an additional regularization loss on discrete classification based on these quantized token IDs (see in Section 3.2). 3.2 Latent Diffusion Model with Non-Autoregressive Generation\n\nWe leverage a diffusion model to predict the quantized latent vector conditioned on the text sequence . We leverage a prior model that consists of a phoneme encoder, a duration predictor, and a pitch predictor to process the text input and provide a more informative hidden vector as the condition of the diffusion model. Diffusion Formulation\n\nWe formulate the diffusion (forward) process and denoising (reverse) process as a stochastic differential equation (SDE) [34], respectively. The forward SDE transforms the latent vectors obtained by the neural codec (i.e., ) into Gaussian noises:\n\nd \u200b z t = \u2212 1 2 \u200b \u03b2 t \u200b z t \u200b d \u200b t + \u03b2 t \u200b d \u200b w t , t \u2208 [ 0 , 1 ] , formulae-sequence d subscript \ud835\udc67 \ud835\udc61 1 2 subscript \ud835\udefd \ud835\udc61 subscript \ud835\udc67 \ud835\udc61 d \ud835\udc61 subscript \ud835\udefd \ud835\udc61 d subscript \ud835\udc64 \ud835\udc61 \ud835\udc61 0 1 \\mathrm{d}z_{t}=-\\frac{1}{2}\\beta_{t}z_{t}~{}\\mathrm{d}t+\\sqrt{\\beta_{t}}~{}\\mathrm{d}w_{t},\\quad t\\in[0,1], (2)\n\nwhere is the standard Brownian motion, , and is a non-negative noise schedule function. Then the solution is given by:\n\nz t = e \u2212 1 2 \u200b \u222b 0 t \u03b2 s \u200b \ud835\udc51 s \u200b z 0 + \u222b 0 t \u03b2 s \u200b e \u2212 1 2 \u200b \u222b 0 t \u03b2 u \u200b \ud835\udc51 u \u200b d w s . subscript \ud835\udc67 \ud835\udc61 superscript \ud835\udc52 1 2 superscript subscript 0 \ud835\udc61 subscript \ud835\udefd \ud835\udc60 differential-d \ud835\udc60 subscript \ud835\udc67 0 superscript subscript 0 \ud835\udc61 subscript \ud835\udefd \ud835\udc60 superscript \ud835\udc52 1 2 superscript subscript 0 \ud835\udc61 subscript \ud835\udefd \ud835\udc62 differential-d \ud835\udc62 differential-d subscript \ud835\udc64 \ud835\udc60 z_{t}=e^{-\\frac{1}{2}\\int_{0}^{t}\\beta_{s}ds}z_{0}+\\int_{0}^{t}\\sqrt{\\beta_{s}}e^{-\\frac{1}{2}\\int_{0}^{t}\\beta_{u}du}\\mathrm{d}w_{s}. (3)\n\nBy properties of Ito\u2019s integral, the conditional distribution of given is Gaussian: , where and . The reverse SDE transforms the Gaussian noise back to data with the following process:\n\nd \u200b z t = \u2212 ( 1 2 \u200b z t + \u2207 log \u2061 p t \u200b ( z t ) ) \u200b \u03b2 t \u200b d \u200b t + \u03b2 t \u200b d \u200b w ~ t , t \u2208 [ 0 , 1 ] , formulae-sequence d subscript \ud835\udc67 \ud835\udc61 1 2 subscript \ud835\udc67 \ud835\udc61 \u2207 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc67 \ud835\udc61 subscript \ud835\udefd \ud835\udc61 d \ud835\udc61 subscript \ud835\udefd \ud835\udc61 d subscript ~ \ud835\udc64 \ud835\udc61 \ud835\udc61 0 1 \\mathrm{d}z_{t}=-(\\frac{1}{2}z_{t}+\\nabla\\log p_{t}(z_{t}))\\beta_{t}~{}\\mathrm{d}t+\\sqrt{\\beta_{t}}~{}\\mathrm{d}\\tilde{w}_{t},\\quad t\\in[0,1], (4)\n\nwhere is the reverse-time Brownian motion. Moreover, we can consider an ordinary differential equation (ODE) [34] in the reverse process:\n\nd \u200b z t = \u2212 1 2 \u200b ( z t + \u2207 log \u2061 p t \u200b ( z t ) ) \u200b \u03b2 t \u200b d \u200b t , t \u2208 [ 0 , 1 ] . formulae-sequence d subscript \ud835\udc67 \ud835\udc61 1 2 subscript \ud835\udc67 \ud835\udc61 \u2207 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc67 \ud835\udc61 subscript \ud835\udefd \ud835\udc61 d \ud835\udc61 \ud835\udc61 0 1 \\mathrm{d}z_{t}=-\\frac{1}{2}(z_{t}+\\nabla\\log p_{t}(z_{t}))\\beta_{t}~{}\\mathrm{d}t,\\quad t\\in[0,1]. (5)\n\nWe can train a neural network to estimate the score (the gradient of the log-density of noisy data), and then we can sample data by starting from Gaussian noise and numerically solving the SDE in Equation 4 or ODE in Equation 5. In our formulation, the neural network is based on WaveNet [23], which takes the current noisy vector , the time step , and the condition information as input, and predicts the data instead of the score, which we found results in better speech quality. Thus, . The loss function to train the diffusion model is as follows. \u2112 diff subscript \u2112 diff \\displaystyle\\mathcal{L}_{\\rm diff} = \ud835\udd3c z 0 , t \u200b [ \u2016 z ^ 0 \u2212 z 0 \u2016 2 2 + \u2016 \u03a3 t \u2212 1 \u200b ( \u03c1 \u200b ( z ^ 0 , t ) \u2212 z t ) \u2212 \u2207 log \u2061 p t \u200b ( z t ) \u2016 2 2 + \u03bb c \u200b e \u2212 r \u200b v \u200b q \u200b \u2112 ce \u2212 rvq ] , absent subscript \ud835\udd3c subscript \ud835\udc67 0 \ud835\udc61 delimited-[] superscript subscript norm subscript ^ \ud835\udc67 0 subscript \ud835\udc67 0 2 2 superscript subscript norm superscript subscript \u03a3 \ud835\udc61 1 \ud835\udf0c subscript ^ \ud835\udc67 0 \ud835\udc61 subscript \ud835\udc67 \ud835\udc61 \u2207 subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc67 \ud835\udc61 2 2 subscript \ud835\udf06 \ud835\udc50 \ud835\udc52 \ud835\udc5f \ud835\udc63 \ud835\udc5e subscript \u2112 ce rvq \\displaystyle=\\mathbb{E}_{z_{0},t}[||\\hat{z}_{0}-z_{0}||_{2}^{2}+||\\Sigma_{t}^{-1}(\\rho(\\hat{z}_{0},t)-z_{t})-\\nabla\\log p_{t}(z_{t})||_{2}^{2}+\\lambda_{ce-rvq}\\mathcal{L}_{\\rm ce-rvq}], (6)\n\nwhere the first term is the data loss, the second term is the score loss, and the predicted score is calculated by , which is also used for reverse sampling based on Equation 4 or 5 in inference. The third term is a novel cross-entropy (CE) loss based on residual vector-quantizer (RVQ). Specifically, for each residual quantizer , we first get the residual vector , where is the ground-truth quantized embedding in the -th residual quantizer ( is also introduced in Equation LABEL:eq_codec). Then we calculate the L2 distance between the residual vector with each codebook embedding in quantizer and get a probability distribution with a softmax function, and then calculate the cross-entropy loss between the ID of the ground-truth quantized embedding and this probability distribution. is the mean of the cross-entropy loss in all residual quantizers, and is set to 0.1 during training. Prior Model: Phoneme Encoder and Duration/Pitch Predictor\n\nThe phoneme encoder consists of several Transformer blocks [35, 6], where the standard feed-forward network is modified as a convolutional network to capture the local dependency in phoneme sequence. Both the duration and pitch predictors share the same model structure with several convolutional blocks but with different model parameters. The ground-truth duration and pitch information is used as the learning target to train the duration and pitch predictors, with an L1 duration loss and pitch loss . During training, the ground-truth duration is used to expand the hidden sequence from the phoneme encoder to obtain the frame-level hidden sequence, and then the ground-truth pitch information is added to the frame-level hidden sequence to get the final condition information . During inference, the corresponding predicted duration and pitch are used. The total loss function for the diffusion model is as follows:\n\n\u2112 = \u2112 diff + \u2112 dur + \u2112 pitch . \u2112 subscript \u2112 diff subscript \u2112 dur subscript \u2112 pitch \\mathcal{L}=\\mathcal{L}_{\\rm diff}+\\mathcal{L}_{\\rm dur}+\\mathcal{L}_{\\rm pitch}. (7)\n\n3.3 Speech Prompting for In-Context Learning\n\nTo facilitate in-context learning for better zero-shot generation, we design a speech prompting mechanism to encourage the duration/pitch predictor and the diffusion model to follow the diverse information (e.g., speaker identities) in the speech prompt. For a speech latent sequence , we randomly cut off a segment with frame index from to as the speech prompt, and concatenate the remaining speech segments and to form a new sequence as the learning target of the diffusion model. As shown in Figure 3, we use a Transformer-based prompt encoder to process the speech prompt ( in the figure) to get a hidden sequence. To leverage this hidden sequence as the prompt, we have two different strategies for the duration/pitch predictor and the diffusion model: 1) For the duration and pitch predictors, we insert a Q-K-V attention layer in the convolution layer, where the query is the hidden sequence of the convolution layer, and the key and value is the hidden sequence from the prompt encoder. 2) For the diffusion model, instead of directly attending to the hidden sequence from the prompt encoder that exposes too many details to the diffusion model and may harm the generation, we design two attention blocks: in the first attention block, we use randomly initialized embeddings as the query sequence to attend to the prompt hidden sequence, and get a hidden sequence with a length of as the attention results [36, 37, 38]; in the second attention block, we leverage the hidden sequence in the WaveNet layer as the query and the -length attention results as the key and value. We use the attention results of the second attention block as the conditional information of a FiLM layer [39] to perform affine transform on the hidden sequence of the WaveNet in the diffusion model. Please refer to Appendix B for the details of WaveNet architecture used in the diffusion model. 3.4 Connection to NaturalSpeech\n\nNaturalSpeech 2 is an advanced edition of the NaturalSpeech Series [11, 40]. Compared to its previous version NaturalSpeech [11], NaturalSpeech 2 has the following connections and distinctions. First, goal. While both NaturalSpeech 1 and 2 aim at synthesizing natural voices (with good speech quality and diversity), their focuses are different. NaturalSpeech focuses on speech quality by synthesizing voices that are on par with human recordings and only tackling single-speaker recording-studio datasets (e.g., LJSpeech). NaturalSpeech 2 focuses on speech diversity by exploring the zero-shot synthesis ability based on large-scale, multi-speaker, and in-the-wild datasets. Second, architecture. NaturalSpeech 2 keeps the basic components in NaturalSpeech, such as the encoder and decoder for waveform reconstruction, and the prior module (phoneme encoder, duration/pitch predictor). However, it leverages 1) a diffusion model to increase the modeling power to capture the complicated and diverse data distribution in large-scale speech datasets, 2) a residual vector quantizer to regularize the latent vectors to trade off the reconstruction quality and prediction difficulty, and 3) a speech prompting mechanism to enable zero-shot ability that is not covered in single-speaker synthesis system. 4 Experimental Settings\n\nIn this section, we introduce the experimental settings to train and evaluate NaturalSpeech 2, including the dataset, model configuration, baselines for comparison, training and inference, and evaluation metrics. 4.1 Datasets\n\nTraining Dataset\n\nTo train the neural audio codec and the diffusion model, we use the English subset of Multilingual LibriSpeech (MLS) [41] as the training data, which contains K hours of transcribed speech data derived from LibriVox audiobooks.",
    "00c367427d9135209d84008e6cb5e90f0adba881-2": "The number of distinct speakers is for males and for females respectively. The sample rate is KHz for all speech data. The input text sequence is first converted into a phoneme sequence using grapheme-to-phoneme conversion [42] and then aligned with speech using our internal alignment tool to obtain the phoneme-level duration. The frame-level pitch sequence is extracted from the speech using PyWorld222https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder. Evaluation Dataset\n\nWe employ two benchmark datasets for evaluation: 1) LibriSpeech [43] test-clean, which contains distinct speakers and hours of annotated speech data. 2) VCTK dataset [44], which contains distinct speakers. For LibriSpeech test-clean, we randomly sample utterances for each speaker and form a subset of 600 utterances for evaluation. For VCTK, we randomly sample utterances for each speaker, resulting in a subset of utterances for evaluation. Specifically, to synthesize each sample, we randomly select a different utterance of the same speaker and crop it into a -second audio segment to form a -second prompt. Note that both the speakers in LibriSpeech test-clean and VCTK are not seen during training. Thus, we aim to conduct zero-shot speech synthesis. The singing datasets follow a similar process in the speech dataset, and the details are shown in Section 5.6. 4.2 Model Configuration and Comparison\n\nModel Configuration\n\nThe phoneme encoder is a 6-layer Transformer [35] with 8 attention heads, 512 embedding dimensions, 2048 1D convolution filter size, 9 convolution 1D kernel size, and 0.1 dropout rate. The pitch and duration predictor share the same architecture of 30-layer 1D convolution with ReLU activation and layer normalization, 10 Q-K-V attention layers for in-context learning, which have 512 hidden dimensions and 8 attention heads and are placed every 3 1D convolution layers. We set the dropout to 0.5 in both duration and pitch predictors. For the speech prompt encoder, we use a 6-layer Transformer with 512 hidden size, which has the same architecture as the phoneme encoder. As for the query tokens in the first Q-K-V attention in the prompting mechanism in the diffusion model (as shown in Figure 3), we set the token number to 32 and the hidden dimension to 512. The diffusion model contains 40 WaveNet layers [23], which consist of 1D dilated convolution layers with 3 kernel size, 1024 filter size, and 2 dilation size. Specifically, we use a FiLM layer [39] at every 3 WaveNet layers to fuse the condition information processed by the second Q-K-V attention in the prompting mechanism in the diffusion model. The hidden size in WaveNet is 512, and the dropout rate is 0.2. More details of the model configurations are shown in Appendix A. Model Comparison\n\nWe choose the previous zero-shot TTS model YourTTS [45] as the baseline, with the official code and pre-trained checkpoint333https://github.com/Edresson/YourTTS, which is trained on VCTK [44], LibriTTS [46] and TTS-Portuguese [47]. We also choose VALL-E [13] that is based on discrete audio codec and autoregressive language model for comparison, which can help demonstrate the advantages of the designs in NaturalSpeech 2. We directly collect some audio samples from its demo page for comparison. 4.3 Model Training and Inference\n\nWe first train the audio codec using NVIDIA TESLA V100 16GB GPUs with a batch size of 200 audios per GPU for K steps. We follow the implementation and experimental setting of SoundStream [19] and adopt Adam optimizer with learning rate. Then we use the trained codec to extract the quantized latent vectors for each audio to train the diffusion model in NaturalSpeech 2. The diffusion model in NaturalSpeech 2 is trained using NVIDIA TESLA V100 32GB GPUs with a batch size of K frames of latent vectors per GPU for K steps (our model is still underfitting and longer training will result in better performance). We optimize the models with the AdamW optimizer with learning rate, k warmup steps following the inverse square root learning schedule. During inference, for the diffusion model, we find it beneficial to use a temperature and sample the terminal condition from [32]. We set to . To balance the generation quality and latency, we adopt the Euler ODE solver and set the diffusion steps to . 4.4 Evaluation Metrics\n\nWe use both objective and subjective metrics to evaluate the zero-shot synthesis ability of NaturalSpeech 2 and compare it with baselines. Objective Metrics\n\nWe evaluate the TTS systems with the following objective metrics:\n\n\u2022\n\nProsody Similarity with Prompt. We evaluate the prosody similarity (in terms of pitch and duration) between the generated speech and the prompt speech, which measures how well the TTS model follows the prosody in speech prompt in zero-shot synthesis. We calculate the prosody similarity with the following steps: 1) we extract phoneme-level duration and pitch from the prompt and the synthesized speech; 2) we calculate the mean, standard variation, skewness, and kurtosis [7] of the pitch and duration in each speech sequence; 3) we calculate the difference of the mean, standard variation, skewness, and kurtosis between each paired prompt and synthesized speech and average the differences among the whole test set. \u2022\n\nProsody Similarity with Ground Truth. We evaluate the prosody similarity (in terms of pitch and duration) between the generated speech and the ground-truth speech, which measures how well the TTS model matches the prosody in the ground truth. Since there is correspondence between two speech sequences, we calculate the Pearson correlation and RMSE of the pitch/duration between the generated and ground-truth speech, and average them on the whole test set. \u2022\n\nWord Error Rate. We employ an ASR model to transcribe the generated speech and calculate the word error rate (WER). The ASR model is a CTC-based HuBERT [48] pre-trained on Librilight [49] and fine-tuned on the 960 hours training set of LibriSpeech. We use the official code and checkpoint444https://huggingface.co/facebook/hubert-large-ls960-ft. Subjective Metrics\n\nWe conduct human evaluation and use the intelligibility score and mean opinion score as the subjective metrics:\n\n\u2022\n\nIntelligibility Score. Neural TTS models often suffer from the robustness issues such as word skipping, repeating, and collapse issues, especially for autoregressive models. To demonstrate the robustness of NaturalSpeech 2, following the practice in [6], we use the 50 particularly hard sentences (see Appendix C) and conduct an intelligibility test. We measure the number of repeating words, skipping words, and error sentences as the intelligibility score. \u2022\n\nCMOS and SMOS. Since synthesizing natural voices is one of the main goals of NaturalSpeech 2, we measure naturalness using comparative mean option score (CMOS) with 12 native speakers as the judges. We also use the similarity mean option score (SMOS) between the synthesized and prompt speech to measure the speaker similarity, with 6 native speakers as the judges. 5 Results on Natural and Zero-Shot Synthesis\n\nIn this section, we conduct a series of experiments to compare the NaturalSpeech 2 with the baselines from the following aspects: 1) Generation Quality, by evaluating the naturalness of the synthesized audio; 2) Generation Similarity, by evaluating how well the TTS system follows prompts; 3) Robustness, by calculating the WER and an additional intelligibility test. 5.1 Generation Quality\n\nWe conduct CMOS test to evaluate the generation quality (i.e., naturalness). We randomly select 20 utterances from the LibriSpeech and VCTK tests and crop the prompt speech to 3s. To ensure high-quality generation, we use a speech scoring model [50] to filter the multiple samples generated by the diffusion model with different starting Gaussian noises . Table 3 shows a comparison of NaturalSpeech 2 against baseline YourTTS and the ground truth. We have several observations: 1) NaturalSpeech 2 is comparable to the ground-truth recording in LibriSpeech ( is regarded as on par) and achieves much better quality on VCTK datasets ( is a large gap), which demonstrates the naturalness of the speech generated by NaturalSpeech 2 is high enough. 2) NaturalSpeech shows and CMOS gain over YourTTS in LibriSpeech and VCTK, respectively, which shows the superiority of NaturalSpeech 2 over this baseline. 5.2 Generation Similarity\n\nWe use two metrics to evaluate the speech similarity: 1) prosody similarity between the synthesized and prompt speech. 2) SMOS test. To evaluate the prosody similarity, we randomly sample one sentence for each speaker for both LibriSpeech test-clean and VCTK dataset to form the test sets. Specifically, to synthesize each sample, we randomly and independently sample the prompt speech with seconds. Note that YourTTS has seen speakers in VCTK in training, but we still compare NaturalSpeech 2 with YourTTS on all the speakers in VCTK (i.e., the 97 speakers are seen to YourTTS but unseen to NaturalSpeech 2). We apply the alignment tool to obtain phoneme-level duration and pitch and calculate the prosody similarity metrics between the synthesized speech and the prompt speech as described in Section 4.4. The results are shown in Table 4. We have the following observations: 1) NaturalSpeech 2 consistently outperforms the baseline YourTTS in both LibriSpeech and VCTK on all metrics, which demonstrates that our proposed NaturalSpeech 2 can mimic the prosody of prompt speech much better. 2) Although YourTTS has seen 97 from 108 speakers in VCTK dataset, our model can still outperform it by a large margin, which demonstrates the advantages of NaturalSpeech 2. Furthermore, we also compare prosody similarity between synthesized and ground-truth speech in Appendix D. We further evaluate the speaker similarity using SMOS test. We randomly select 10 utterances from LibriSpeech and VCTK datasets respectively, following the setting in the CMOS test. The length of the prompt speech is set to 3s. The results are shown in Table 5. NaturalSpeech 2 outperforms YourTTS by and SMOS scores for LibriSpeech and VCTK, respectively, which shows that NaturalSpeech 2 is significantly better in speaker similarity. 5.3 Robustness\n\nWe use the full test set of LibriSpeech and VCTK as described in Section 4.1 to synthesize the speech and compute the word error rate (WER) between the transcribed text and ground-truth text.",
    "00c367427d9135209d84008e6cb5e90f0adba881-3": "To synthesize each sample, we use a 3-second prompt by randomly cropping the whole prompt speech. The results are shown in Table 6. We observe that: 1) NaturalSpeech 2 significantly outperforms YourTTS in LibriSpeech and VCTK, indicating better synthesis of high-quality and robust speech. 2) Our synthesized speech is comparable to the ground-truth speech in LibriSpeech and surpasses that in VCTK. The higher WER results in VCTK may stem from a noisy environment and the lack of ASR model fine-tuning in that dataset. Autoregressive TTS models often suffer from alignment mismatch between phoneme and speech, resulting in severe word repeating and skipping. To further evaluate the robustness of the diffusion-based TTS model, we adopt the 50 particularly hard sentences in FastSpeech [6] to evaluate the robustness of the TTS systems. We can find that the non-autoregressive models such as FastSpeech [6], NaturalSpeech [11], and also NaturalSpeech 2 are robust for the 50 hard cases, without any intelligibility issues. As a comparison, the autoregressive models such as Tacotron [3], Transformer TTS [5], and VALL-E [13] will have a high error rate on these hard sentences. The comparison results are provided in Table 7. 5.4 Comparison with Other TTS Systems\n\nIn this section, we compare NaturalSpeech 2 with the zero-shot TTS model VALL-E [13]. We directly download the first 16 utterances from VALL-E demo page555https://valle-demo.github.io/, which consists of 8 samples from LibriSpeech and 8 samples from VCTK. We evaluate the CMOS and SMOS in Table 8. From the results, we find that NaturalSpeech 2 outperforms VALL-E by 0.3 in SMOS and 0.31 in CMOS, respectively. The SMOS results show that NaturalSpeech 2 is significantly better in speaker similarity. The CMOS results demonstrate that the speech generated by NaturalSpeech 2 is much more natural and of higher quality. 5.5 Ablation Study\n\nIn this section, we perform ablation experiments. 1) To study the effect of the speech prompt, we remove the Q-K-V attention layers in the diffusion (abbr. w/o. diff prompt), and the duration and pitch predictors (abbr. w/o. dur/pitch prompt), respectively. 2) To study the effect of the cross-entropy (CE) loss based on RVQ, we disable the CE loss by setting to (abbr.",
    "00c367427d9135209d84008e6cb5e90f0adba881-4": "w/o. CE loss). 3) To study the effectiveness of two Q-K-V attention in speech prompting for diffusion in Section 3.3, we remove the first attention that adopts randomly initialized query sequence to attend to the prompt hidden and directly use one Q-K-V attention to attend to the prompt hidden (abbr.",
    "00c367427d9135209d84008e6cb5e90f0adba881-5": "w/o. query attn). We report the prosody similarity metric between synthesized and prompt speech in Table 9. More ablation results between synthesized and ground-truth speech are included in Appendix E. We have the following observations: 1) Disabling speech prompt in the diffusion model significantly degrades prosody similarity (e.g., from to for the mean of the pitch or even can not converge), highlighting its importance for high-quality TTS synthesis. 2) Disabling cross-entropy loss worsens performance, as the residual vector quantizer\u2019s layer-wise cross entropy provides regularization for precise latent representations. 3) Disabling query attention strategy also degrades prosody similarity. In practice, we find that applying cross-attention to prompt hidden will leak details and thus mislead generation. In addition, since the prompt length is an important hyper-parameter for zero-shot TTS, we would like to investigate the effect of the prompt length. We follow the setting of prosody similarity between synthesized and prompt speech in Section 5.2. Specifically, we vary the prompt length by seconds and report the prosody similarity metrics of NaturalSpeech 2. The results are shown in Table 10. We observe that when the prompt is longer, the similarity between the generated speech and the prompt is higher for NaturalSpeech 2. It shows that the longer prompt reveals more details of the prosody, which help the TTS model to generate more similar speech. 5.6 Zero-Shot Singing Synthesis\n\nIn this section, we explore NaturalSpeech 2 to synthesize singing voice in a zero-shot setting, either given a singing prompt or only a speech prompt. For singing data collection, we crawl a number of singing voices and their paired lyrics from the Web. For singing data preprocessing, we utilize a speech processing model to remove the backing vocal and accompaniment in the song, and an ASR model to filter out samples with misalignments. The dataset is then constructed using the same process as speech data, ultimately containing around hours of singing data. The dataset is upsampled and mixed with speech data for singing experiments. We use speech and singing data together to train NaturalSpeech 2 with a learning rate. In inference, we set the diffusion steps to for better performance. To synthesize a singing voice, we use the ground-truth pitch and duration from another singing voice, and use different singing prompts to generate singing voices with different singer timbres. Interestingly, we find that NaturalSpeech 2 can generate a novel singing voice using speech as the prompt. See the demo page666https://speechresearch.github.io/naturalspeech2 for zero-shot singing synthesis with either singing or speech as the prompt. 5.7 Extension to Voice Conversion and Speech Enhancement\n\nIn this section, we extend NaturalSpeech 2 to another two speech synthesis tasks: 1) voice conversion and 2) speech enhancement. See the demo page777https://speechresearch.github.io/naturalspeech2 for zero-shot voice conversion and speech enhancement examples. 5.7.1 Voice Conversion\n\nBesides zero-shot text-to-speech and singing synthesis, NaturalSpeech 2 also supports zero-shot voice conversion, which aims to convert the source audio into the target audio using the voice of the prompt audio . Technically, we first convert the source audio into an informative Gaussian noise using a source-aware diffusion process and generate the target audio using a target-aware denoising process, shown as follows. Source-Aware Diffusion Process. In voice conversion, it is helpful to provide some necessary information from source audio for target audio in order to ease the generation process. Thus, instead of directly diffusing the source audio with some Gaussian noise, we diffuse the source audio into a starting point that still maintains some information in the source audio. Specifically, inspired by the stochastic encoding process in Diffusion Autoencoder [51], we obtain the starting point from as follows:\n\nz 1 = z 0 + \u222b 0 1 \u2212 1 2 \u200b ( z t + \u03a3 t \u2212 1 \u200b ( \u03c1 \u200b ( s ^ \u03b8 \u200b ( z t , t , c ) , t ) \u2212 z t ) ) \u200b \u03b2 t \u200b d \u200b t , subscript \ud835\udc67 1 subscript \ud835\udc67 0 superscript subscript 0 1 1 2 subscript \ud835\udc67 \ud835\udc61 superscript subscript \u03a3 \ud835\udc61 1 \ud835\udf0c subscript ^ \ud835\udc60 \ud835\udf03 subscript \ud835\udc67 \ud835\udc61 \ud835\udc61 \ud835\udc50 \ud835\udc61 subscript \ud835\udc67 \ud835\udc61 subscript \ud835\udefd \ud835\udc61 d \ud835\udc61 \\begin{split}z_{1}&=z_{0}+\\int_{0}^{1}-\\frac{1}{2}(z_{t}+\\Sigma_{t}^{-1}(\\rho(\\hat{s}_{\\theta}(z_{t},t,c),t)-z_{t}))\\beta_{t}~{}\\mathrm{d}t,\\end{split} (8)\n\nwhere is the predicted score at .",
    "00c367427d9135209d84008e6cb5e90f0adba881-6": "We can think of this process as the reverse of ODE (Equation 5) in the denoising process. Target-Aware Denoising Process. Different from the TTS which starts from the random Gaussian noise, the denoising process of voice conversion starts from the obtained from the source-aware diffusion process. We run the standard denoising process as in the TTS setting to obtain the final target audio , conditioned on and the prompt audio , where is obtained from the phoneme and the duration sequence of the source audio and the predicted pitch sequence. As a consequence, we observe that NaturalSpeech 2 is capable of producing speech that exhibits similar prosody to the source speech, while also replicating the timbre specified by the prompt. 5.7.2 Speech Enhancement\n\nNaturalSpeech 2 can be extended to speech enhancement, which is similar to the extension of voice conversion. In this setting, we assume that we have the source audio which contains background noise ( denotes the audio with background noise), the prompt with background noise for the source-aware diffusion process, and the prompt without background noise for target-aware denoising process. Note that and have the same background noise. To remove the background noise, firstly, we apply the source-aware diffusion process by and and obtain the as in Equation 8. The source audio\u2019s duration and pitch are utilized in this procedure. Secondly, we run the target-aware denoising process to obtain the clean audio by and the clean prompt . Specifically, we use the phoneme sequence, duration sequence, and pitch sequence of the source audio in this procedure. As a result, we find that NaturalSpeech 2 can effectively eliminate background noise while simultaneously preserving crucial aspects such as prosody and timbre. 6 Conclusion and Future Work\n\nIn this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with continuous latent vectors and a latent diffusion model with non-autoregressive generation to enable natural and zero-shot text-to-speech synthesis. To facilitate in-context learning for zero-shot synthesis, we design a speech prompting mechanism in the duration/pitch predictor and the diffusion model. By scaling NaturalSpeech 2 to 400M model parameters, 44K hours of speech, and 5K speakers, it can synthesize speech with high expressiveness, robustness, fidelity, and strong zero-shot ability, outperforming previous TTS systems. For future work, we will explore efficient strategies such as consistency models [52, 53] to speed up the diffusion model and explore large-scale speaking and singing voice training to enable more powerful mixed speaking/singing capability. Broader Impacts: Since NaturalSpeech 2 could synthesize speech that maintains speaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agree to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. References\n\n[1] Paul Taylor. Text-to-speech synthesis. Cambridge university press, 2009. [2] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. A survey on neural speech synthesis. arXiv preprint arXiv:2106.15561, 2021. [3] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, et al. Tacotron: Towards end-to-end speech synthesis. Proc. Interspeech 2017, pages 4006\u20134010, 2017. [4] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, et al. Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions.",
    "00c367427d9135209d84008e6cb5e90f0adba881-7": "In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4779\u20134783. IEEE, 2018. [5] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with Transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6706\u20136713, 2019. [6] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. FastSpeech: Fast, robust and controllable text to speech. In NeurIPS, 2019. [7] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. FastSpeech 2: Fast and high-quality end-to-end text to speech. In International Conference on Learning Representations, 2021. [8] Yanqing Liu, Zhihang Xu, Gang Wang, Kuan Chen, Bohan Li, Xu Tan, Jinzhu Li, Lei He, and Sheng Zhao. DelightfulTTS: The Microsoft speech synthesis system for Blizzard challenge 2021. arXiv preprint arXiv:2110.12612, 2021. [9] Yanqing Liu, Ruiqing Xue, Lei He, Xu Tan, and Sheng Zhao. DelightfulTTS 2: End-to-end speech synthesis with adversarial vector-quantized auto-encoders. arXiv preprint arXiv:2207.04646, 2022. [10] Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. arXiv preprint arXiv:2106.06103, 2021. [11] Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, Yichong Leng, Yuanhao Yi, Lei He, et al. NaturalSpeech: End-to-end text to speech synthesis with human-level quality. arXiv preprint arXiv:2205.04421, 2022. [12] Keith Ito. The LJ speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017. [13] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. [14] Eugene Kharitonov, Damien Vincent, Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. Speak, read and prompt: High-fidelity text-to-speech with minimal supervision. arXiv preprint arXiv:2302.03540, 2023. [15] Ruiqing Xue, Yanqing Liu, Lei He, Xu Tan, Linquan Liu, Edward Lin, and Sheng Zhao. Foundationtts: Text-to-speech for asr customization with generative language model.",
    "00c367427d9135209d84008e6cb5e90f0adba881-8": "arXiv preprint arXiv:2303.02939, 2023. [16] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6309\u20136318, 2017. [17] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In Advances in neural information processing systems, pages 14866\u201314876, 2019. [18] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021. [19] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. SoundStream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021. [20] Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. [21] Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a language modeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022. [22] Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. DiffSinger: Singing voice synthesis via shallow diffusion mechanism. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11020\u201311028, 2022. [23] A\u00e4ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. [24] Jean-Marc Valin and Jan Skoglund. LPCNet: Improving neural speech synthesis through linear prediction. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5891\u20135895. IEEE, 2019. [25] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv preprint arXiv:2209.15352, 2022. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. [27] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [28] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.",
    "00c367427d9135209d84008e6cb5e90f0adba881-9": "Advances in neural information processing systems, 33:1877\u20131901, 2020. [29] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A versatile diffusion model for audio synthesis. In ICLR, 2021. [30] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. WaveGrad: Estimating gradients for waveform generation.",
    "00c367427d9135209d84008e6cb5e90f0adba881-10": "In ICLR, 2021. [31] Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diff-TTS: A denoising diffusion model for text-to-speech. arXiv preprint arXiv:2104.01409, 2021. [32] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-TTS: A diffusion probabilistic model for text-to-speech.",
    "00c367427d9135209d84008e6cb5e90f0adba881-11": "arXiv preprint arXiv:2105.06337, 2021. [33] Xu Tan, Tao Qin, Jiang Bian, Tie-Yan Liu, and Yoshua Bengio. Regeneration learning: A learning paradigm for data generation. arXiv preprint arXiv:2301.08846, 2023. [34] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. [35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008, 2017. [36] Yequan Wang, Minlie Huang, Xiaoyan Zhu, and Li Zhao. Attention-based lstm for aspect-level sentiment classification. In Proceedings of the 2016 conference on empirical methods in natural language processing, pages 606\u2013615, 2016. [37] Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-Ryan, Eric Battenberg, Joel Shor, Ying Xiao, Ye Jia, Fei Ren, and Rif A Saurous. Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis.",
    "00c367427d9135209d84008e6cb5e90f0adba881-12": "In International Conference on Machine Learning, pages 5180\u20135189. PMLR, 2018. [38] Dacheng Yin, Chuanxin Tang, Yanqing Liu, Xiaoqiang Wang, Zhiyuan Zhao, Yucheng Zhao, Zhiwei Xiong, Sheng Zhao, and Chong Luo. Retrievertts: Modeling decomposed factors for text-based speech insertion.",
    "00c367427d9135209d84008e6cb5e90f0adba881-13": "arXiv preprint arXiv:2206.13865, 2022. [39] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with a general conditioning layer.",
    "00c367427d9135209d84008e6cb5e90f0adba881-14": "In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. [40] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. arXiv preprint arXiv:2304.09116, 2023. [41] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: A large-scale multilingual dataset for speech research.",
    "00c367427d9135209d84008e6cb5e90f0adba881-15": "Proc. Interspeech 2020, pages 2757\u20132761, 2020. [42] Hao Sun, Xu Tan, Jun-Wei Gan, Hongzhi Liu, Sheng Zhao, Tao Qin, and Tie-Yan Liu. Token-level ensemble distillation for grapheme-to-phoneme conversion.",
    "00c367427d9135209d84008e6cb5e90f0adba881-16": "In INTERSPEECH, 2019. [43] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. LibriSpeech: an ASR corpus based on public domain audio books.",
    "00c367427d9135209d84008e6cb5e90f0adba881-17": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206\u20135210. IEEE, 2015. [44] Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. Superseded-CSTK VCTK corpus: English multi-speaker corpus for CSTK voice cloning toolkit.",
    "00c367427d9135209d84008e6cb5e90f0adba881-18": "2016. [45] Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren G\u00f6lge, and Moacir A Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone. In International Conference on Machine Learning, pages 2709\u20132720. PMLR, 2022. [46] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. LibriTTS: A corpus derived from librispeech for text-to-speech. Proc. Interspeech 2019, pages 1526\u20131530, 2019. [47] Edresson Casanova, Arnaldo Candido Junior, Christopher Shulby, Frederico Santos de Oliveira, Jo\u00e3o Paulo Teixeira, Moacir Antonelli Ponti, and Sandra Alu\u00edsio. Tts-portuguese corpus: a corpus for speech synthesis in brazilian portuguese. Language Resources and Evaluation, 56(3):1043\u20131055, 2022. [48] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units.",
    "00c367427d9135209d84008e6cb5e90f0adba881-19": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451\u20133460, 2021. [49] Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar\u00e9, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light: A benchmark for asr with limited or no supervision.",
    "00c367427d9135209d84008e6cb5e90f0adba881-20": "In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7669\u20137673. IEEE, 2020. [50] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing.",
    "00c367427d9135209d84008e6cb5e90f0adba881-21": "IEEE Journal of Selected Topics in Signal Processing, 16(6):1505\u20131518, 2022. [51] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward a meaningful and decodable representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10619\u201310629, 2022. [52] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. [53] Zhen Ye, Wei Xue, Xu Tan, Jie Chen, Qifeng Liu, and Yike Guo. Comospeech: One-step speech and singing voice synthesis via consistency model. arXiv preprint arXiv:2305.06908, 2023. Appendix A Model Details\n\nAppendix B The Details of WaveNet Architecture in the Diffusion Model\n\nAs shown in Figure 4, the WaveNet consists of 40 blocks. Each block consists of 1) a dilated CNN with kernel size 3 and dilation 2, 2) a Q-K-V attention, and 3) a FiLM layer. In detail, we use Q-K-V attention to attend to the key/value obtained from the first Q-K-V attention module (from the speech prompt encoder) as shown in Figure 3. Then, we use the attention results to generate the scale and bias terms, which are used as the conditional information of the FiLM layer. Finally, we average the skip output results of each layer and calculate the final WaveNet output. Appendix C The 50 Particularly Hard Sentences\n\nThe 50 particularly hard sentences used in Section 5.3 are listed below:\n\n{hangparas}\n\n1.6em1 01.",
    "00c367427d9135209d84008e6cb5e90f0adba881-22": "a\n\n02. b\n\n03. c\n\n04. H\n\n05. I\n\n06. J\n\n07. K\n\n08. L\n\n09. 22222222 hello 22222222\n\n10. S D S D Pass zero - zero Fail - zero to zero - zero - zero Cancelled - fifty nine to three - two - sixty four Total - fifty nine to three - two -\n\n11. S D S D Pass - zero - zero - zero - zero Fail - zero - zero - zero - zero Cancelled - four hundred and sixteen - seventy six -\n\n12. zero - one - one - two Cancelled - zero - zero - zero - zero Total - two hundred and eighty six - nineteen - seven -\n\n13. forty one to five three hundred and eleven Fail - one - one to zero two Cancelled - zero - zero to zero zero Total -\n\n14. zero zero one , MS03 - zero twenty five , MS03 - zero thirty two , MS03 - zero thirty nine ,\n\n15. 1b204928 zero zero zero zero zero zero zero zero zero zero zero zero zero zero one seven ole32\n\n16. zero zero zero zero zero zero zero zero two seven nine eight F three forty zero zero zero zero zero six four two eight zero one eight\n\n17. c five eight zero three three nine a zero bf eight FALSE zero zero zero bba3add2 - c229 - 4cdb -\n\n18. Calendaring agent failed with error code 0x80070005 while saving appointment .",
    "00c367427d9135209d84008e6cb5e90f0adba881-23": "19. Exit process - break ld - Load module - output ud - Unload module - ignore ser - System error - ignore ibp - Initial breakpoint -\n\n20. Common DB connectors include the DB - nine , DB - fifteen , DB - nineteen , DB - twenty five , DB - thirty seven , and DB - fifty connectors .",
    "00c367427d9135209d84008e6cb5e90f0adba881-24": "21. To deliver interfaces that are significantly better suited to create and process RFC eight twenty one , RFC eight twenty two , RFC nine seventy seven , and MIME content . 22. int1 , int2 , int3 , int4 , int5 , int6 , int7 , int8 , int9 ,\n\n23. seven _ ctl00 ctl04 ctl01 ctl00 ctl00\n\n24. Http0XX , Http1XX , Http2XX , Http3XX ,\n\n25. config file must contain A , B , C , D , E , F , and G . 26. mondo - debug mondo - ship motif - debug motif - ship sts - debug sts - ship Comparing local files to checkpoint files \u2026\n\n27. Rusbvts . dll Dsaccessbvts . dll Exchmembvt . dll Draino . dll Im trying to deploy a new topology , and I keep getting this error . 28. You can call me directly at four two five seven zero three seven three four four or my cell four two five four four four seven four seven four or send me a meeting request with all the appropriate information . 29. Failed zero point zero zero percent < one zero zero one zero zero zero zero Internal . Exchange . ContentFilter . BVT ContentFilter . BVT_log . xml Error ! Filename not specified . 30. C colon backslash o one two f c p a r t y backslash d e v one two backslash oasys backslash legacy backslash web backslash HELP\n\n31. src backslash mapi backslash t n e f d e c dot c dot o l d backslash backslash m o z a r t f one backslash e x five\n\n32. copy backslash backslash j o h n f a n four backslash scratch backslash M i c r o s o f t dot S h a r e P o i n t dot\n\n33. Take a look at h t t p colon slash slash w w w dot granite dot a b dot c a slash access slash email dot\n\n34. backslash bin backslash premium backslash forms backslash r e g i o n a l o p t i o n s dot a s p x dot c s Raj , DJ ,\n\n35. Anuraag backslash backslash r a d u r five backslash d e b u g dot one eight zero nine underscore P R two h dot s t s contains\n\n36. p l a t f o r m right bracket backslash left bracket f l a v o r right bracket backslash s e t u p dot e x e\n\n37. backslash x eight six backslash Ship backslash zero backslash A d d r e s s B o o k dot C o n t a c t s A d d r e s\n\n38. Mine is here backslash backslash g a b e h a l l hyphen m o t h r a backslash S v r underscore O f f i c e s v r\n\n39. h t t p colon slash slash teams slash sites slash T A G slash default dot aspx As always , any feedback , comments ,\n\n40. two thousand and five h t t p colon slash slash news dot com dot com slash i slash n e slash f d slash two zero zero three slash f d\n\n41. backslash i n t e r n a l dot e x c h a n g e dot m a n a g e m e n t dot s y s t e m m a n a g e\n\n42. I think Rich\u2019s post highlights that we could have been more strategic about how the sum total of XBOX three hundred and sixtys were distributed .",
    "00c367427d9135209d84008e6cb5e90f0adba881-25": "43. 64X64 , 8K , one hundred and eighty four ASSEMBLY , DIGITAL VIDEO DISK DRIVE , INTERNAL , 8X ,\n\n44. So we are back to Extended MAPI and C++ because . Extended MAPI does not have a dual interface VB or VB .Net can read . 45. Thanks , Borge Trongmo Hi gurus , Could you help us E2K ASP guys with the following issue ? 46. Thanks J RGR Are you using the LDDM driver for this system or the in the build XDDM driver ? 47. Btw , you might remember me from our discussion about OWA automation and OWA readiness day a year ago . 48. empidtool . exe creates HKEY_CURRENT_USER Software Microsoft Office Common QMPersNum in the registry , queries AD , and the populate the registry with MS employment ID if available else an error code is logged .",
    "00c367427d9135209d84008e6cb5e90f0adba881-26": "49. Thursday, via a joint press release and Microsoft AI Blog, we will announce Microsoft\u2019s continued partnership with Shell leveraging cloud, AI, and collaboration technology to drive industry innovation and transformation. 50. Actress Fan Bingbing attends the screening of \u2019Ash Is Purest White (Jiang Hu Er Nv)\u2019 during the 71st annual Cannes Film Festival\n\nAppendix D Prosody Similarity with Ground Truth\n\nTo further investigate the quality of prosody, we follow the generation quality evaluation of prosody similarity between synthesized and prompt speech in Section 5.2 and compare the generated speech with the ground-truth speech. We use Pearson correlation and RMSE to measure the prosody matching between generated and ground-truth speech. The results are shown in Table 12. We observe that NaturalSpeech 2 outperforms the baseline YourTTS by a large margin, which shows that our NaturalSpeech 2 is much better in prosody similarity. Appendix E Ablation Study\n\nIn this section, we also compare the prosody similarity between audio generated by the ablation model and the ground-truth speech in Table 13. Similar to the results of comparing the audio generated by the ablation model and prompt speech, we also have the following observations. 1) The speech prompt is most important to the generation quality. 2) The cross-entropy and the query attention strategy are also helpful in high-quality speech synthesis. \u25c4 Feeling lucky?",
    "00c367427d9135209d84008e6cb5e90f0adba881-27": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Thu Feb 29 14:22:48 2024 by LaTeXML"
}