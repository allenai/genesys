{
    "0033cbb16ea8996520f2e239298070975b3f31ae-0": "On the Verification of Neural ODEs with Stochastic Guarantees\n\nSophie Gruenbacher1, Ramin Hasani1,2, Mathias Lechner3, Jacek Cyranka4, Scott A. Smolka5, Radu Grosu1\n\nAbstract\n\nWe show that Neural ODEs, an emerging class of time-continuous neural networks, can be verified by solving a set of global-optimization problems. For this purpose, we introduce Stochastic Lagrangian Reachability (SLR), an abstraction-based technique for constructing a tight Reachtube (an over-approximation of the set of reachable states over a given time-horizon), and provide stochastic guarantees in the form of confidence intervals for the Reachtube bounds. SLR inherently avoids the infamous wrapping effect (accumulation of over-approximation errors) by performing local optimization steps to expand safe regions instead of repeatedly forward-propagating them as is done by deterministic reachability methods. To enable fast local optimizations, we introduce a novel forward-mode adjoint sensitivity method to compute gradients without the need for backpropagation. Finally, we establish asymptotic and non-asymptotic convergence rates for SLR. 1 Introduction\n\nNeural ordinary differential equations (Neural ODEs) (Chen et al. 2018), which are analogous to a continuous-depth version of deep residual networks (He et al. 2016), exhibit considerable computational efficiency on time-series modeling tasks. Although Neural ODEs do not necessarily improve the performance of contemporary deep models, they enable the rich theory and tools from the field of differential equations to be applied to deep models. Examples include a better characterization of Neural ODEs (Rubanova, Chen, and Duvenaud 2019; Dupont, Doucet, and Teh 2019; Durkan et al. 2019; Jia and Benson 2019), and a better understanding of their robustness (Yan et al. 2020), stability (Yang et al. 2020), and controllability (Quaglino et al. 2019; Holl, Koltun, and Thuerey 2020; Kidger et al. 2020). As the use of Neural ODEs on real-world applications increases (Finlay et al.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-1": "2020; Lechner et al. 2020; Erichson et al. 2020; Lechner and Hasani 2020; Hasani et al. 2020b), so does the importance of ensuring their safety through the use of verification techniques. In this paper, we establish a theoretical foundation for the verification of Neural ODE networks. In particular, we introduce Stochastic Lagrangian Reachability (SLR), a new analysis technique with provable convergence and conservativeness guarantees for Neural ODEs , with field , hidden states , and parameters . (SLR works in fact for any nonlinear system defined by a set of nonlinear differential equations.)\n\nAt the core of SLR is the translation of the reachability problem to a global optimization problem, at every time step . The latter is solved globally, by uniformly sampling states from an initial ball , and locally, by computing a local minimum via gradient descent from . SLR avoids gradient descent if is within a spherical-cap around a previously sampled state or its corresponding local minimum. The radius of the cap is derived from the interval computation of the local Lipschitz constant of the objective function within the cap. The minimum computed by SLR at time stochastically defines an as-tight-as-possible ellipsoid covering all states reached at by the solution starting in , with tolerance and confidence , for given values of and . See Figure 1. Since SLR employs interval arithmetic only locally to compute the spherical-caps (also called safety or tabu regions), it avoids the infamous wrapping effect (Lohner 1992) of deterministic reachability methods (see Table 1), which prevents them from being deployed in practice. Consequently, our approach scales up to large-scale, real-life Neural ODEs. To the best of our knowledge, none of the available tools has been successfully applied to Neural ODEs. We also introduce a novel forward formulation of the adjoint sensitivity method (Pontryagin 2018) to compute the loss gradients in the optimization flow. This enables us to improve the time complexity of the optimization process compared to similar methods (Chen et al. 2018; Zhuang et al. 2020). Summary of results. In this work, we present a thorough theoretical approach to the problem of providing safety guarantees for the class of time-continuous neural networks formulated as Neural ODEs. As the main result, we develop SLR, a differentiable stochastic Lagrangian reachability framework, formulated as a global optimization problem. In particular, we prove that SLR converges (Theorem 2) to tight ellipsoidal safe regions (Theorem 1), within number of iterations (Theorem 3). This implies that for a given confidence level , our algorithm terminates according to the proposed rate, which leads to the important conclusion that the problem of constructing an ellipsoid abstraction of the true reachsets with probabilistic guarantees for Neural ODEs is decidable (the computed abstraction is conservative with confidence ). We summarize our key contributions as follows:\n\n\u2022\n\nWe introduce a theoretical framework for the verification of Neural ODEs by restating the reachability problem as a set of global-optimization problems. \u2022\n\nWe solve each optimization problem globally, via uniform sampling, and locally, through gradient descent (GD), thereby avoiding costly Hessian computations in the process. \u2022\n\nGD is avoided in spherical-caps around the start/end states of previous searches. The cap radius is derived from its local Lipschitz constant, computed via interval arithmetic. \u2022\n\nWe design a forward-mode GD algorithm based on the adjoint sensitivity method for (Neural) ODEs. \u2022\n\nWe prove convergence properties of SLR, its safety guarantees, and discuss its time and space complexity. 2 Related Work\n\nGlobal optimization. The literature on global optimization for continuous problems is vast and includes many different approaches depending on the smoothness assumptions made about the objective function. Evolutionary strategies like those based on the covariance matrix (Hansen and Ostermeier 2001; Igel, Hansen, and Roth 2007) work for general continuous objectives. Deterministic interval-based branch-and-bound methods (Neumaier 2004; Hansen 1980) work for differentiable objectives, and Lipschitz global optimization (Piyavskii 1972; Shubert 1972; Malherbe and Vayatis 2017) for objectives satisfying the Lipschitz condition. Our work is closest to the BRST algorithm (Boender et al. 1982; Rinnooy Kan and Timmer 1987a, b) which for smooth objectives uses Hessians to compute the basins of attraction for local minima as ellipsoidal bounds. Such basins define tabu regions. The final estimate for the global minimum and reasonable confidence bounds are provided. Stochastic reachability. Existing work is mainly concerned with the verification of safety guarantees for stochastic hybrid systems with continuous dynamics (ODEs) in each mode. Stochasticity is introduced in several ways: uncertainty in the model parameters (Wang et al. 2015; Fr\u00e4nzle, Teige, and Eggers 2010; Shmarov and Zuliani 2015b), uncertainty in the discrete jumps between modes (Fr\u00e4nzle et al. 2011), and uncertainty in the initial state (Huang et al. 2017). The work of (Enszer and Stadtherr 2011) focuses on the probabilistic verification of continuous-time ODEs with uncertainty in parameters and initial states. Reachability for continuous dynamical systems. Most of the relevant techniques are deterministic and based on interval arithmetic. We provide a qualitative summary of existing reachability methods for continuous-time systems in Table 1. 3 Setup\n\nIn this section, we introduce our notation, preliminary concepts, and definitions required to construct our theoretical setup for the verification of Neural ODEs. Neural ODE. The derivative of the hidden states is computed by a neural network parameterized by as follows (Chen et al. 2018):\n\n\u2202 t x = f \u200b ( x , x \u200b ( 0 ) , t , \u03b8 ) , x 0 \u2208 \u212c 0 formulae-sequence subscript \ud835\udc61 \ud835\udc65 \ud835\udc53 \ud835\udc65 \ud835\udc65 0 \ud835\udc61 \ud835\udf03 subscript \ud835\udc65 0 subscript \u212c 0 \\partial_{t}x=f(x,x(0),t,\\theta),x_{0}\\in\\mathcal{B}_{0} (1)\n\nWe require that the Neural ODE is Lipschitz-continuous and forward-complete. The solution to this initial-value problem can be computed by numerical ODE solvers, from any initial system state . Consequently, the numerical solution can be trained by reverse-mode automatic differentiation (Rumelhart, Hinton, and Williams 1986), either through the solver, by a vanilla backpropagation algorithm (Hasani et al. 2020a), or by treating the solver as a blackbox and using the adjoint sensitivity method (Pontryagin 2018). Geometrical deformation in time by a flow . To describe the optimization problem, we use Eulerian and Lagrangian coordinates from classical continuum mechanics. We regard the set of initial states, which is the ball , as a body that is being deformed in time by a flow . Given a point in Eulerian coordinates (the undeformed configuration), there is at every time the representation of that point in Lagrangian coordinates (the configuration deformed by ). The deformation of in time is related to the Neural ODE, where is defined as the solution flow of Eq. (1). Reachset. A reachset is the set of all states reached at a target time , given the initial states and a flow. More formally:\n\nDefinition 1. Given a set of initial states at time , the target time , and the flow of the Neural ODE (1), we call a conservative reachset enclosure if , for all ; i.e., the reachset bounds all state-trajectories of the Neural ODE. Whenever the initial set is known from the context, we simply refer to the reachset as the Reachset at time , or . Reachtube. A reachtube is a series of reachsets within a determined time-horizon. Formally:\n\nDefinition 2. Given a set of initial states at time , and a time horizon , we use to denote a sequence of time-stamped reachsets , , with . Whenever the initial set, time horizon, and flow are known from the context, we use the term reachtube over-approximation or , for that sequence of reachsets. Definition 3 (Ellipsoid). Given , with and , we call a ball in metric (or an ellipsoid) with center and radius if for all . Reachability as an optimization problem. Given a time horizon , an initial ball with center and radius , and Euclidean metric , our goal is to find a tight reachtube , bounding all state-trajectories of the Neural ODE (1). We capture the reachsets of by ellipsoids with center , radius , and metric . At every time , we use as the center , the numerical integration of , and as the metric , the optimal metric in minimizing the volume of the ellipsoid, as proposed in (Gruenbacher et al. 2020). Thus, our goal is to find at every time step , a radius which (stochastically) guarantees that is a conservative reachset. I.e., at each , we want to find the maximal distance of all to center in metric for , and define as this distance. Thus the optimization problem can be defined as follows:\n\n\u03b4 j subscript \ud835\udeff \ud835\udc57 \\displaystyle\\delta_{j} \u2265 max x \u2208 \u212c 0 \u2225 \u03c7 t 0 t j ( x ) \u2212 \u03c7 t 0 t j ( x 0 ) \u2225 M j = max x \u2208 \u212c 0 dist ( \u03c7 t 0 t j ( x ) ) \\displaystyle\\geq\\max_{x\\in\\mathcal{B}_{0}}\\left\\lVert\\chi_{t_{0}}^{t_{j}}(x)-\\chi_{t_{0}}^{t_{j}}(x_{0})\\right\\rVert_{M_{j}}=\\max_{x\\in\\mathcal{B}_{0}}\\operatorname{dist}\\left(\\chi_{t_{0}}^{t_{j}}(x)\\right) (2)\n\nwhere we use to describe the distance in Eq. (2) when metric and starting point are known. As we require Lipschitz-continuity and forward-completeness, the map is a homeomorphism and commutes with closure and interior operators. In particular, the image of the boundary of the set is equal to the boundary of the image . Thus, Eq. (2) has its optimum on the surface of the initial ball , and we will only consider points on the surface. In order to be able to optimize this problem, we describe the points on the surface with (n-dimensional) polar coordinates such that every point is represented by a tuple , with angles and center , having a conversion function from polar to Cartesian coordinates, defined as follows:\n\nx \u200b ( ( \u03b4 0 , \u03c6 ) , x 0 ) = \ud835\udc65 subscript \ud835\udeff 0 \ud835\udf11 subscript \ud835\udc65 0 absent \\displaystyle x((\\delta_{0},\\varphi),x_{0})= (3) ( x 0 , 1 + \u03b4 0 \u200b cos \u2061 ( \u03c6 1 ) \u22ee x 0 , n \u2212 1 + \u03b4 0 \u200b sin \u2061 ( \u03c6 1 ) \u22c5 \u2026 \u22c5 sin \u2061 ( \u03c6 n \u2212 2 ) \u200b cos \u2061 ( \u03c6 n \u2212 1 ) x 0 , n + \u03b4 0 \u200b sin \u2061 ( \u03c6 1 ) \u22c5 \u2026 \u22c5 sin \u2061 ( \u03c6 n \u2212 2 ) \u200b sin \u2061 ( \u03c6 n \u2212 1 ) ) matrix subscript \ud835\udc65 0 1 subscript \ud835\udeff 0 subscript \ud835\udf11 1 \u22ee subscript \ud835\udc65 0 \ud835\udc5b 1 \u22c5 subscript \ud835\udeff 0 subscript \ud835\udf11 1 \u2026 subscript \ud835\udf11 \ud835\udc5b 2 subscript \ud835\udf11 \ud835\udc5b 1 subscript \ud835\udc65 0 \ud835\udc5b \u22c5 subscript \ud835\udeff 0 subscript \ud835\udf11 1 \u2026 subscript \ud835\udf11 \ud835\udc5b 2 subscript \ud835\udf11 \ud835\udc5b 1 \\displaystyle\\begin{pmatrix}x_{0,1}+\\delta_{0}\\cos(\\varphi_{1})\\\\\n\\vdots\\\\\nx_{0,n-1}+\\delta_{0}\\sin(\\varphi_{1})\\cdot\\ldots\\cdot\\sin(\\varphi_{n-2})\\cos(\\varphi_{n-1})\\\\\nx_{0,n}+\\delta_{0}\\sin(\\varphi_{1})\\cdot\\ldots\\cdot\\sin(\\varphi_{n-2})\\sin(\\varphi_{n-1})\\\\\n\\end{pmatrix}\n\nWhenever the center and the radius of the initial ball are known from the context, we will use the following notation: for the conversion from polar to Cartesian coordinates and for Cartesian to polar. Using polar coordinates, we restate the optimization problem (2) as follows:\n\n\u03b4 j subscript \ud835\udeff \ud835\udc57 \\displaystyle\\delta_{j} = max x \u2208 \u212c 0 \u2225 \u03c7 t 0 t j ( x ) \u2212 \u03c7 t 0 t j ( x 0 ) \u2225 M j \\displaystyle=\\max_{x\\in\\mathcal{B}_{0}}\\left\\lVert\\chi_{t_{0}}^{t_{j}}(x)-\\chi_{t_{0}}^{t_{j}}(x_{0})\\right\\rVert_{M_{j}} = max \u03c6 \u2208 \u211d n \u2212 1 \u2061 \u2225 \u03c7 t 0 t j \u200b ( x \u200b ( \u03c6 ) ) \u2212 \u03c7 t 0 t j \u200b ( x 0 ) \u2225 M j \u23df = \u2212 L \u200b ( \u03c6 ) absent subscript \ud835\udf11 superscript \u211d \ud835\udc5b 1 subscript \u23df subscript delimited-\u2225\u2225 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 \ud835\udf11 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 subscript \ud835\udc65 0 subscript \ud835\udc40 \ud835\udc57 absent \ud835\udc3f \ud835\udf11 \\displaystyle=\\max_{\\varphi\\in\\mathbb{R}^{n-1}}\\underbrace{\\left\\lVert\\chi_{t_{0}}^{t_{j}}(x(\\varphi))-\\chi_{t_{0}}^{t_{j}}(x_{0})\\right\\rVert_{M_{j}}}_{=-L(\\varphi)} = min \u03c6 \u2208 \u211d n \u2212 1 \u2061 L \u200b ( \u03c6 ) = m \u22c6 , absent subscript \ud835\udf11 superscript \u211d \ud835\udc5b 1 \ud835\udc3f \ud835\udf11 superscript \ud835\udc5a \u22c6 \\displaystyle=\\min_{\\varphi\\in\\mathbb{R}^{n-1}}L(\\varphi)=m^{\\star}, (4)\n\nWe call the loss function in polar coordinates at time that we would like to minimize. Note that also depends on the initial radius and initial center ; as these are fixed inputs, we do not consider them in the notation. 4 Main Results\n\nIn this section, we present our verification framework for Neural ODEs, which we call Stochastic Lagrangian Reachability (SLR). As the main results of this paper, we show that the algorithm guarantees safety and converges to the tightest ellipsoid, almost surely, in the limit of the number of samples. We then compute the convergence rate and discuss space and time complexities. 4.1 Gradient Computation\n\nOur algorithm uses gradient descent locally when solving the global optimization problem of Eq. (3). Gradient descent is started from uniformly sampled points, which are not contained in already constructed safety regions. Uniform sampling is used to repeatedly select an initial point from the surface of the ball . Gradient descent is used from this point to find a local minimum. SLR is inspired by the gradient-only tabu-search (GOTS) proposed in (Stepanenko 2009). Instead of tabu regions, we use safety radii to construct an area around already visited points , where we know for sure what the minimum value inside that region is. In the following, we describe the computational steps of the loss\u2019s gradient for the main SLR algorithm in greater detail. Given the target time , termination tolerance , learning rate , initial guess , and loss function , we seek to compute the gradient of loss . We introduce a new framework to compute the loss\u2019s gradient which is needed in Line 3 of Algorithm 1 to find the local minimum. Using the chain rule, we can express the gradient as follows:\n\n\u2202 L \u200b ( \u22c5 ) \u2202 \u03c6 \u200b ( \u03c6 ) = \u2212 \u2202 dist \u2218 \u03c7 t 0 t j \u2218 x \u200b ( \u22c5 ) \u2202 \u03c6 = \u2212 \u2202 dist \u2202 y | y = \u03c7 t 0 t j \u200b ( x \u200b ( \u03c6 ) ) \u23df ( a ) \u22c5 \u2202 \u03c7 t 0 t j \u2202 x | x = x \u200b ( \u03c6 ) \u23df ( c ) \u22c5 \u2202 x \u200b ( \u22c5 ) \u2202 \u03c6 \u23df ( b ) \ud835\udc3f \u22c5 \ud835\udf11 \ud835\udf11 dist superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 \u22c5 \ud835\udf11 \u22c5 subscript \u23df evaluated-at dist \ud835\udc66 \ud835\udc66 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 \ud835\udf11 \ud835\udc4e subscript \u23df evaluated-at superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 \ud835\udc65 \ud835\udc65 \ud835\udf11 \ud835\udc50 subscript \u23df \ud835\udc65 \u22c5 \ud835\udf11 \ud835\udc4f \\begin{split}&\\frac{\\partial L(\\cdot)}{\\partial\\varphi}(\\varphi)=-\\left.\\frac{\\partial\\operatorname{dist}\\circ\\chi_{t_{0}}^{t_{j}}\\circ x(\\cdot)}{\\partial\\varphi}\\right.\\\\\n&=-\\underbrace{\\left.\\frac{\\partial\\operatorname{dist}}{\\partial y}\\right|_{y=\\chi_{t_{0}}^{t_{j}}(x(\\varphi))}}_{(a)}\\cdot\\underbrace{\\left.\\frac{\\partial\\chi_{t_{0}}^{t_{j}}}{\\partial x}\\right|_{x=x(\\varphi)}}_{(c)}\\cdot\\underbrace{\\frac{\\partial x(\\cdot)}{\\partial\\varphi}}_{(b)}\\end{split} (5)\n\nPart (a) - loss gradient wrt : The differentiation of the loss function defined in Eq. (2) can be expressed as\n\n\u2202 y dist ( y ) = A j ( y \u2212 \u03c7 t 0 t j ( x 0 ) ) dist ( y ) \u2212 1 A j , \\displaystyle\\partial_{y}\\operatorname{dist}(y)=A_{j}(y-\\chi_{t_{0}}^{t_{j}}(x_{0}))\\operatorname{dist}(y)^{-1}A_{j}, (6)\n\nwith from Def. 3 and as the metric in minimizing the volume of the ellipsoid (Gruenbacher et al. 2020). Part (b) - polar gradient: describes the transformation from polar coordinates to Cartesian coordinates, as given in Eq. (3). The differentiation with respect to is straightforward to obtain using the product rule and the derivatives of and :\n\n\u2202 \u03c6 x \u200b ( \u03c6 ) = subscript \ud835\udf11 \ud835\udc65 \ud835\udf11 absent \\displaystyle\\partial_{\\varphi}x(\\varphi)= (7) ( \u2212 \u03b4 0 \u200b sin \u2061 ( \u03c6 1 ) \u03b4 0 \u200b ( cos \u2061 ( \u03c6 1 ) \u200b cos \u2061 ( \u03c6 2 ) \u2212 sin \u2061 ( \u03c6 1 ) \u200b sin \u2061 ( \u03c6 2 ) ) \u22ee ) matrix subscript \ud835\udeff 0 subscript \ud835\udf11 1 subscript \ud835\udeff 0 subscript \ud835\udf11 1 subscript \ud835\udf11 2 subscript \ud835\udf11 1 subscript \ud835\udf11 2 \u22ee \\displaystyle\\begin{pmatrix}-\\delta_{0}\\sin(\\varphi_{1})\\\\\n\\delta_{0}\\left(\\cos(\\varphi_{1})\\cos(\\varphi_{2})-\\sin(\\varphi_{1})\\sin(\\varphi_{2})\\right)\\\\\n\\vdots\\\\\n\\end{pmatrix}\n\nPart (c) - gradient of the flow: The partial derivative in of the Neural ODE solution flow with respect to the initial condition is called the gradient of the flow or deformation gradient in (Slaughter 2002; Abeyaratne 1998), and the sensitivity matrix in (Donz\u00e9 2010; Donz\u00e9 and Maler 2007). Let be the identity matrix in . As we now show, the sensitivity matrix is a solution of the variational equations associated with (1):\n\n\u2202 x \u03c7 t 0 t j \u200b ( x ) = F \u200b ( t j , x ) subscript \ud835\udc65 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 \ud835\udc39 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 \\displaystyle\\partial_{x}\\chi_{t_{0}}^{t_{j}}(x)=F(t_{j},x) (8) \u2202 t F \u200b ( t , x ) = ( \u2202 x f ) \u200b ( \u03c7 t 0 t \u200b ( x ) ) \u200b F \u200b ( t , x ) , F \u200b ( t 0 , x ) = I formulae-sequence subscript \ud835\udc61 \ud835\udc39 \ud835\udc61 \ud835\udc65 subscript \ud835\udc65 \ud835\udc53 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 \ud835\udc61 \ud835\udc65 \ud835\udc39 \ud835\udc61 \ud835\udc65 \ud835\udc39 subscript \ud835\udc61 0 \ud835\udc65 \ud835\udc3c \\displaystyle\\partial_{t}F(t,x)=(\\partial_{x}f)(\\chi_{t_{0}}^{t}(x))F(t,x),\\quad F(t_{0},x)=I\n\nProof sketch: By interchanging the differentiation order, we obtain . Since is a solution of Eq.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-2": "(1), . By the chain rule, we get . Forward-mode use of adjoint sensitivity method. The integral of Eq. (8) has the same form of the auxiliary ODE used for reverse-mode automatic differentiation of Neural ODEs, when optimized by the adjoint sensitivity method (Chen et al. 2018) with one exception. In contrast to (Chen et al. 2018), which requires one to run the adjoint equation backward and have access to the termination time of the flow, our approach enjoys a simultaneous forward-mode use of the adjoint equation. This is due to the way we determine the loss function in the ODE space. In retrospect, this enables us to obtain the gradients of the loss at the current state-computation step. This property enables us to improve the optimization runtime by 50%, compared to the optimization scheme used in (Chen et al. 2018): we save half of the time because we do not have to go backward to compute the loss. More precisely, solving Eq. (8) until target time requires knowledge of for all . This ensures that we already know the value of when needed to compute the right side of Eq. (8) during integration of . Algorithm 2 demonstrates the computation of the gradient of the loss function. 4.2 Safety-Region Computation\n\nWith our global search strategy, we are covering the feasible region with already visited points . Consequently, we have access to the global minimum in all of those regions:\n\nm \u00af = min \u03c6 \u2208 \ud835\udcb1 \u2061 L \u200b ( \u03c6 ) \u00af \ud835\udc5a subscript \ud835\udf11 \ud835\udcb1 \ud835\udc3f \ud835\udf11 \\displaystyle\\bar{m}=\\min_{\\varphi\\in\\mathcal{V}}L(\\varphi) (9)\n\nwith , where is the global minimum of Eq. (3). We now identify safety regions for a Neural ODE flow and describe how this is incorporated in the SLR algorithm. Definition 4 (Safety Region). Let be an already-visited point. A safety-radius defines a safe spherical-cap , because for all s.t. . Our objective is to use the local Lipschitz constants to define a radius around an already visited point s.t. we can guarantee that is a safety region. Definition 5 (Lipschitz). The local Lipschitz constant (LLC) of a function in a region is defined as a with\n\n\u2016 L \u200b ( x ) \u2212 L \u200b ( y ) \u2016 \u2264 \u03bb A \u200b \u2016 x \u2212 y \u2016 \u2200 x , y \u2208 A . formulae-sequence norm \ud835\udc3f \ud835\udc65 \ud835\udc3f \ud835\udc66 subscript \ud835\udf06 \ud835\udc34 norm \ud835\udc65 \ud835\udc66 for-all \ud835\udc65 \ud835\udc66 \ud835\udc34 \\displaystyle\\|L(x)-L(y)\\|\\leq\\lambda_{A}\\|x-y\\|\\quad\\forall x,y\\in A. In the following theorem, we use the LLC to define the radius of the safety (or tabu) region around an already-visited point . Theorem 1 (Radius of Safety Region). At target time , let be the current global minimum, as in Eq. (9). Let be an already-visited point with value (), and let and be defined as follows with :\n\nr \u03c6 = \u03bb \u03a3 \u03c6 \u2212 1 \u200b ( L \u200b ( \u03c6 ) \u2212 \u03bc \u22c5 m \u00af ) subscript \ud835\udc5f \ud835\udf11 superscript subscript \ud835\udf06 subscript \u03a3 \ud835\udf11 1 \ud835\udc3f \ud835\udf11 \u22c5 \ud835\udf07 \u00af \ud835\udc5a \\displaystyle r_{\\varphi}=\\lambda_{\\Sigma_{\\varphi}}^{-1}\\left(L(\\varphi)-\\mu\\cdot\\bar{m}\\right) (10)\n\nwith . If is chosen s.t. , then it holds that:\n\nL \u200b ( \u03c8 ) \u2265 \u03bc \u22c5 m \u00af \u2200 x \u200b ( \u03c8 ) \u2208 B \u200b ( \u03c6 , r \u03c6 ) S formulae-sequence \ud835\udc3f \ud835\udf13 \u22c5 \ud835\udf07 \u00af \ud835\udc5a for-all \ud835\udc65 \ud835\udf13 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle L(\\psi)\\geq\\mu\\cdot\\bar{m}\\quad\\forall x(\\psi)\\in B(\\varphi,r_{\\varphi})^{S} (11)\n\nThe full proof is provided in the Appendix. Proof sketch: The Lipschitz constant defines a relation between the values in the domain and the ones in the range of the function. Theorem 1 says that areas around already-visited samples are safe. The size of the safety areas increases if we have a better current global minimum. Therefore, the theorem demonstrates that we can improve the convergence rate if we optimize the loss, by possibly finding a better current global minimum. This justifies the use of gradient descent together with a more global search strategy. Algorithm 3 computes the radius in Eq. (10) as a fixpoint of the choice of . For an over-approximation of the LLC in Line 2, we use the triangle inequality and the mean value inequality with a change in metric (Cyranka et al. 2017, Lemma 2). We then solve Eq. 8 using interval arithmetic to obtain an interval gradient matrix , and take the maximum singular value of , as proposed in (Gruenbacher et al. 2019). Depending on the Neural ODE, it is presumably faster to pick , and to always use the LLC of the entire initial ball. As a result of the way we select in Theorem 1, we are able to increase the radii as soon as a new region with a smaller local minimum than the previous ones is discovered. Thus: . 4.3 Stochastic Lagrangian Reachability\n\nBy using local gradient computation, global uniform sampling, and safety regions as in Algorithm 3, we present our SLR verification technique, as outlined in Algorithm 4. Given a Neural ODE as in Eq. (1) and a set of initial states , we start by specifying a confidence level and a tolerance for the entire Reachtube. The algorithm returns radii , , and the stochastic guarantee stating that overestimates by the true conservative Reachsets with a probability higher than . This holds also for the whole Reachtube, as it is defined by a series of Reachsets (Def. 2). As we reinitialize the variables at the beginning of every new timestep , and apply gradient descent to the loss function of the initial polar coordinates at time , we do not accumulate errors from one timestep to the next one. This is a prominent advantage compared to methods using interval arithmetic, and thus accumulating the wrapping effect, e.g.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-3": "(Zgliczynski 2002; Cyranka et al. 2018; Fan et al. 2017). Another advantage is that we can compute the for-loop in line 1 of Algorithm 4 (thus the Reachsets of the Reachtube) in parallel. At every timestep , we sample random points and construct safety regions around them until we reach the desired probability of being inside the tolerance region defined by . After sampling a new point, we check if this point is already in the covered area. If not, then we apply gradient descent to find a local minimum and compare this local minimum to the smallest value . Otherwise, if the sampled point is already in the covered area and thus in at least one safety region, we already know the lower bounds for that region and do not look for the local minimum again. This approach is similar to using baisins of attraction, but is more scalable because we do not require Hessian computation. In line 20, we recompute the radii of the safety regions when we find a new smallest value . By computing the current probability of having reached the desired confidence level, we check whether we have to resample more points or whether we are able to finish that timestep and save the radius of the stochastic Reachset at time . 4.4 Stochastic Guarantees of Reachsets\n\nIn this section, we derive the stochastic convergence guarantees and convergence bounds for finding the global minimum of Eq. (3) using SLR at every timestep . Let be defined as in Eq. (9), and let be the global minimum and an argument s.t. . We start by defining the probability of covering :\n\nPr \u2061 ( B \u200b ( \u03c6 , r \u03c6 ) S \u220b x \u200b ( \u03c6 \u22c6 ) ) Pr \ud835\udc65 superscript \ud835\udf11 \u22c6 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle\\Pr(B({\\varphi},r_{\\varphi})^{S}\\owns x(\\varphi^{\\star})) = Pr ( \u2225 x ( \u03c6 \u22c6 ) \u2212 x ( \u03c6 ) \u2225 2 ) \u2264 r \u03c6 ) \\displaystyle=\\Pr\\left(\\|x(\\varphi^{\\star})-x(\\varphi)\\|_{2})\\leq r_{{\\varphi}}\\right) = Pr \u2061 ( x \u200b ( \u03c6 ) \u2208 \ud835\udc9e \u200b ( r \u03c6 ) ) = Pr \u2061 ( \ud835\udc9e \u200b ( r \u03c6 ) ) absent Pr \ud835\udc65 \ud835\udf11 \ud835\udc9e subscript \ud835\udc5f \ud835\udf11 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udf11 \\displaystyle=\\Pr(x(\\varphi)\\in\\mathcal{C}(r_{{\\varphi}}))=\\Pr(\\mathcal{C}(r_{{\\varphi}})) (12)\n\nwith as defined in Eq.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-4": "(10) and being the spherical-cap in Fig. 2. By using the area of the spherical cap and the area of the initial ball\u2019s surface , the probability defined by Eq. (12) can be described as follows:\n\nPr \u2061 ( \ud835\udc9e \u200b ( r \u03c6 ) ) = Area \u2061 ( \ud835\udc9e \u200b ( r \u03c6 ) ) Area \u2061 ( \u212c 0 S ) Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udf11 Area \ud835\udc9e subscript \ud835\udc5f \ud835\udf11 Area superscript subscript \u212c 0 \ud835\udc46 \\displaystyle\\Pr(\\mathcal{C}(r_{{\\varphi}}))=\\frac{\\operatorname{Area}(\\mathcal{C}(r_{{\\varphi}}))}{\\operatorname{Area}(\\mathcal{B}_{0}^{S})} (13)\n\nThe area of can be computed using the formulas in (Shengqiao 2011). Next we derive some probabilities:\n\nPr \u2061 ( B \u200b ( \u03c6 j , r \u03c6 j ) S \u220b\u0338 \u03c6 \u22c6 ) Pr superscript \ud835\udf11 \u22c6 \ud835\udc35 superscript subscript \ud835\udf11 \ud835\udc57 subscript \ud835\udc5f subscript \ud835\udf11 \ud835\udc57 \ud835\udc46 \\displaystyle\\Pr(B({\\varphi_{j}},r_{\\varphi_{j}})^{S}\\not\\owns\\varphi^{\\star}) = 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r \u03c6 j ) ) absent 1 Pr \ud835\udc9e subscript \ud835\udc5f subscript \ud835\udf11 \ud835\udc57 \\displaystyle=1-\\Pr(\\mathcal{C}(r_{{\\varphi_{j}}})) Pr \u2061 ( \u2200 \u03c6 \u2208 \ud835\udcb0 : B \u200b ( \u03c6 , r \u03c6 ) S \u220b\u0338 \u03c6 \u22c6 ) Pr : for-all \ud835\udf11 \ud835\udcb0 superscript \ud835\udf11 \u22c6 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle\\Pr(\\forall\\varphi\\in\\mathcal{U}\\colon B({\\varphi},r_{\\varphi})^{S}\\not\\owns\\varphi^{\\star}) = \u220f \u03c6 \u2208 \ud835\udcb0 ( 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r \u03c6 ) ) ) absent subscript product \ud835\udf11 \ud835\udcb0 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udf11 \\displaystyle=\\prod_{\\varphi\\in\\mathcal{U}}\\left(1-\\Pr(\\mathcal{C}(r_{{\\varphi}}))\\right) Pr \u2061 ( \u2203 \u03c6 \u2208 \ud835\udcb0 : B \u200b ( \u03c6 , r \u03c6 ) S \u220b \u03c6 \u22c6 ) Pr : \ud835\udf11 \ud835\udcb0 superscript \ud835\udf11 \u22c6 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle\\Pr(\\exists\\varphi\\in\\mathcal{U}\\colon B({\\varphi},r_{\\varphi})^{S}\\owns\\varphi^{\\star}) = 1 \u2212 \u220f \u03c6 \u2208 \ud835\udcb0 ( 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r \u03c6 ) ) ) absent 1 subscript product \ud835\udf11 \ud835\udcb0 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udf11 \\displaystyle=1-\\prod_{\\varphi\\in\\mathcal{U}}\\left(1-\\Pr(\\mathcal{C}(r_{{\\varphi}}))\\right) (14)\n\nUsing Theorem 1, if for some , then holds, and thus:\n\nPr \u2061 ( \u03bc \u22c5 m \u00af \u2264 m \u22c6 ) \u2265 Pr \u2061 ( \u2203 \u03c6 \u2208 \ud835\udcb0 : B \u200b ( \u03c6 , r \u03c6 ) S \u220b \u03c6 \u22c6 ) Pr \u22c5 \ud835\udf07 \u00af \ud835\udc5a superscript \ud835\udc5a \u22c6 Pr : \ud835\udf11 \ud835\udcb0 superscript \ud835\udf11 \u22c6 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle\\begin{split}&\\Pr(\\mu\\cdot\\bar{m}\\leq m^{\\star})\\geq\\\\\n&\\Pr(\\exists\\varphi\\in\\mathcal{U}\\colon B({\\varphi},r_{\\varphi})^{S}\\owns\\varphi^{\\star})\\end{split} (15)\n\nTheorem 2 (Convergence Guarantees). Given , , local Lipschitz constant and , where N is the number of uniform-randomly generated points during global search process. Let as defined in Eq. (9), the global minimum, and an argument s.t. . Then:\n\nlim N \u2192 \u221e Pr \u2061 ( \u03bc \u22c5 m \u00af N \u2264 m \u22c6 ) = 1 subscript \u2192 \ud835\udc41 Pr \u22c5 \ud835\udf07 subscript \u00af \ud835\udc5a \ud835\udc41 superscript \ud835\udc5a \u22c6 1 \\displaystyle\\lim_{N\\rightarrow\\infty}\\Pr(\\mu\\cdot\\bar{m}_{N}\\leq m^{\\star})=1 (16)\n\nand thus\n\n\u2200 \u03b3 \u2208 ( 0 , 1 ) , \u2203 N \u2208 \u2115 \u200b s.t. \u200b Pr \u2061 ( \u03bc \u22c5 m \u00af N \u2264 m \u22c6 ) \u2265 1 \u2212 \u03b3 formulae-sequence for-all \ud835\udefe 0 1 \ud835\udc41 \u2115 s.t. Pr \u22c5 \ud835\udf07 subscript \u00af \ud835\udc5a \ud835\udc41 superscript \ud835\udc5a \u22c6 1 \ud835\udefe \\displaystyle\\forall\\gamma\\in(0,1),\\exists N\\in\\mathbb{N}\\textrm{ s.t. }\\Pr(\\mu\\cdot\\bar{m}_{N}\\leq m^{\\star})\\geq 1-\\gamma (17)\n\nThe full proof is provided in the Appendix. Proof sketch: By creating a lower bound for all , s.t. , we underestimate Eq. (14) by . Using this bound and Eq. (15), we show that the convergence guarantee holds. Theorem 2 shows that in the limit of the number of samples, the reachset constructed by Algorithm 4 converges with probability 1 to the smallest ellipsoid that encloses the true reachable set. Note that the algorithm cannot converge to the true reachable set because we approximate the reachset by ellipsoids, while the true reachset might be of arbitrary geometrical shape. Nonetheless, we proved that it provides the smallest possible ellipsoid that contains a true reachset. Moreover, although Theorem 2 shows that we achieve the tightest elliptical reachsets, it does not determine whether the algorithm can terminate or not, as the theorem is proven in the case of infinite samples. We now prove that SLR indeed converges at a reasonable rate. 4.5 Convergence Rate for SLR\n\nTheorem 3 computes a convergence rate for Algorithm 4. Theorem 3 (Convergence Rate). Given , , local Lipschitz constant , and dimension , let be the first random sample point. We can guarantee that if we perform at most iterations of the SLR Algorithm 4, with\n\nN m \u200b a \u200b x = ln \u2061 \u03b3 / ln \u2061 ( 1 \u2212 1 2 \u200b \u03c0 \u200b \u0393 \u200b ( n / 2 ) \u0393 \u200b ( ( n + 1 ) / 2 ) \u200b ( \u03c1 \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) \u03b4 0 ) n \u2212 1 ) subscript \ud835\udc41 \ud835\udc5a \ud835\udc4e \ud835\udc65 / \ud835\udefe 1 1 2 \ud835\udf0b \u0393 \ud835\udc5b 2 \u0393 \ud835\udc5b 1 2 superscript \ud835\udf0c subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 subscript \ud835\udeff 0 \ud835\udc5b 1 \\begin{split}&N_{max}=\\\\\n&\\ln{\\gamma}\\left/\\ln\\left(1-\\frac{1}{2\\sqrt{\\pi}}\\frac{\\Gamma(n/2)}{\\Gamma((n+1)/2)}\\left(\\frac{\\rho(r_{bound})}{\\delta_{0}}\\right)^{n-1}\\right)\\right.\\end{split} (18)\n\nand asymptotically it holds that\n\nN m \u200b a \u200b x = \ud835\udcaa \u200b ( \u2212 ln \u2061 \u03b3 \u200b ( \u03b4 0 r b \u200b o \u200b u \u200b n \u200b d ) 2 \u200b n ) , subscript \ud835\udc41 \ud835\udc5a \ud835\udc4e \ud835\udc65 \ud835\udcaa \ud835\udefe superscript subscript \ud835\udeff 0 subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 2 \ud835\udc5b \\displaystyle N_{max}=\\mathcal{O}\\left(-\\ln\\gamma\\left(\\frac{\\delta_{0}}{r_{bound}}\\right)^{2n}\\right), (19)\n\nwith and .",
    "0033cbb16ea8996520f2e239298070975b3f31ae-5": "The full proof is provided in the Appendix. Proof sketch: As the radius of the spherical cap is very small, we underestimate the area of the cap by removing the curvature and using the volume of an dimensional ball with radius as shown in Fig. 2. Thus, after finishing our global search strategy for timestep , we have the stochastic guarantee that the functional values of every are greater or equal to . This implies that we should initiate the search with a relatively large , obtaining for every a relatively large value of and therefore obtain a faster coverage of the search space. Subsequently, we can investigate whether the reachset with radius intersects with a region of bad (unsafe) states. If this is not the case, we can proceed to the next timestep . Otherwise, we reduce to , which reduces the safety regions and thus the already-covered-set . This means that we continue with our search strategy until the desired probability is reached again for a smaller radius . Accordingly, we can find a first radius for faster and refine it as long as intersects with the region of bad states. Theorem 3 guarantees convergence of the algorithm. It shows that for a given confidence level , our algorithm terminates after at most steps. Essentially, the theorem leads us to the significant result that the problem of constructing an ellipsoid abstraction of the true reachset with probabilistic guarantees for a Neural ODE is able to terminate. Additionally, the theorem assumes that we know the local Lipschitz constant, which is a reasonable assumption for proving convergence. In practice, one can safely replace the true Lipschitz constant by an upper-bound. 4.6 Computational Complexity\n\nThe complexity of Algorithm 1 depends on the geometry of the loss surface. In particular, Algorithm 1 may terminate after one iteration in case of a flat surface, whereas an exponential number may be needed for ill-posed problems, as is common practice when deriving convergence rates for gradient descent (Nagy and Palmer 2003; Drori 2017)\n\nThe runtime of Algorithm 2 is determined by the complexity of the ODE solver for simulating the given differential equation. For example, given the number of integration steps (implicit interpretation of the number of layers in a deep model) , and the time horizon of the simulation , Algorithm 2 runs in time and constant memory cost for each layer of a neural network . The complexity of Algorithm 3 depends on the local Lipschitz constant and the smoothness of the flow. Computing the true Lipschitz constant of a neural network is known to be NP-complete (Virmaux and Scaman 2018). However, Algorithm 3 operates correctly when we replace the true Lipschitz constant by an easier-to-compute upper bound, obtained for instance by means of interval arithmetic. Algorithm 4 implements the main routine of our framework. Its complexity for a given confidence score equals the convergence rate proven in Theorem 3, Eq. (19) for every Reachset. In particular, the runtime of Algorithm 4 depends exponentially on the dimension of the given Neural ODE and logarithmically on the confidence score. 5 Conclusions and Future Work\n\nIn this paper, we considered the verification problem for Neural ODEs. We introduced the SLR verification scheme, which is based on solving a global optimization problem. We designed a forward formulation of the adjoint method for the gradient descent algorithm. We also established strong convergence guarantees for SLR, showing that it can establish tight ellipsoidal bounds for the Neural ODE under consideration, at an arbitrary time horizon. An important future direction will be to improve the current convergence rate, which is exponential in the dimensionality of the Neural ODE network. Existing statistical verification methods are mostly concerned with the verification of (hybrid) dynamical systems having various uncertainties in model parameters, discrete jumps between modes, and/or initial states. We emphasize that reachability computation for Neural ODEs developed at scale will require dedicated methods tailored for that specific purpose. Acknowledgements\n\nThe authors would like to thank the reviewers for their insightful comments. RH and RG were partially supported by Horizon-2020 ECSEL Project grant No. 783163 (iDev40). RH was partially supported by Boeing. ML was supported in part by the Austrian Science Fund (FWF) under grant Z211-N23 (Wittgenstein Award). SG was funded by FWF project W1255-N23. JC was partially supported by NAWA Polish Returns grant PPN/PPO/2018/1/00029. SS was supported by NSF awards DCL-2040599, CCF-1918225, and CPS-1446832.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-6": "References\n\nAbeyaratne (1998) Abeyaratne, R. 1998. Continuum Mechanics. Lecture Notes on The Mechanics of Elastic Solids. Boender et al. (1982) Boender, C.; Rinnooy Kan, A.; Timmer, G.; and Stougie, L. 1982. A stochastic method for global optimization. Mathematical Programming 22: 125\u2013140. Bortolussi and Sanguinetti (2014) Bortolussi, L.; and Sanguinetti, G. 2014. A Statistical Approach for Computing Reachability of Non-linear and Stochastic Dynamical Systems. In Norman, G.; and Sanders, W., eds., Quantitative Evaluation of Systems, 41\u201356.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-7": "Cham: Springer International Publishing. Chen et al. (2018) Chen, T. Q.; Rubanova, Y.; Bettencourt, J.; and Duvenaud, D.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-8": "K. 2018. Neural Ordinary Differential Equations.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-9": "In Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing Systems 31, 6571\u20136583. Curran Associates, Inc. Chen, \u00c1brah\u00e1m, and Sankaranarayanan (2013) Chen, X.; \u00c1brah\u00e1m, E.; and Sankaranarayanan, S. 2013. Flow*: an Analyzer for Non-linear Hybrid Systems. In CAV, 258\u2013263. Cyranka et al. (2017) Cyranka, J.; Islam, M. A.; Byrne, G.; Jones, P.; Smolka, S. A.; and Grosu, R. 2017. Lagrangian Reachabililty. In Majumdar, R.; and Kun\u010dak, V., eds., CAV\u201917, the 29th International Conference on Computer-Aided Verification, 379\u2013400.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-10": "Heidelberg, Germany: Springer. Cyranka et al. (2018) Cyranka, J.; Islam, M. A.; Smolka, S. A.; Gao, S.; and Grosu, R. 2018. Tight Continuous-Time Reachtubes for Lagrangian Reachability. In CDC\u201918, the 57th IEEE Conference on Decision and Control, 6854\u20136861. Miami Beach, FL, USA: IEEE. Devonport et al. (2020) Devonport, A.; Khaled, M.; Arcak, M.; and Zamani, M. 2020. PIRK: Scalable Interval Reachability Analysis for High-Dimensional Nonlinear Systems. In Lahiri, S. K.; and Wang, C., eds., Computer Aided Verification, 556\u2013568. Cham: Springer International Publishing. Donz\u00e9 (2010) Donz\u00e9, A. 2010. Breach, a toolbox for verification and parameter synthesis of hybrid systems. In CAV\u201910, the 22nd International Conference on Computer Aided Verification, 167\u2013170. Edinburgh, UK: Springer. Donz\u00e9 and Maler (2007) Donz\u00e9, A.; and Maler, O. 2007. Systematic simulation using sensitivity analysis. In International Workshop on Hybrid Systems: Computation and Control, 174\u2013189. Springer. Drori (2017) Drori, Y. 2017. The exact information-based complexity of smooth convex minimization. Journal of Complexity 39: 1\u201316. Duggirala et al. (2015) Duggirala, P. S.; Mitra, S.; Viswanathan, M.; and Potok, M. 2015. C2E2: A Verification Tool for Stateflow Models. In Baier, C.; and Tinelli, C., eds., Tools and Algorithms for the Construction and Analysis of Systems, 68\u201382. Berlin, Heidelberg: Springer Berlin Heidelberg. Dupont, Doucet, and Teh (2019) Dupont, E.; Doucet, A.; and Teh, Y. W. 2019. Augmented neural odes. In Advances in Neural Information Processing Systems, 3140\u20133150. Durkan et al. (2019) Durkan, C.; Bekasov, A.; Murray, I.; and Papamakarios, G. 2019. Neural spline flows. In Advances in Neural Information Processing Systems, 7511\u20137522. Enszer and Stadtherr (2011) Enszer, J. A.; and Stadtherr, M. A. 2011. Verified Solution and Propagation of Uncertainty in Physiological Models.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-11": "Reliab. Comput. 15(3): 168\u2013178. URL http://interval.louisiana.edu/reliable-computing-journal/volume-15/no-3/reliable-computing-15-pp-168-178.pdf. Erichson et al. (2020) Erichson, N. B.; Azencot, O.; Queiruga, A.; and Mahoney, M.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-12": "W. 2020. Lipschitz recurrent neural networks. arXiv preprint arXiv:2006.12070 . Fan et al. (2017) Fan, C.; Kapinski, J.; Jin, X.; and Mitra, S. 2017. Simulation-Driven Reachability Using Matrix Measures. ACM Trans. Embed. Comput. Syst. 17(1). Finlay et al. (2020) Finlay, C.; Jacobsen, J.-H.; Nurbekyan, L.; and Oberman, A. 2020. How to train your neural ODE: the world of Jacobian and kinetic regularization.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-13": "In International Conference on Machine Learning, 3154\u20133164. PMLR. Fr\u00e4nzle et al. (2011) Fr\u00e4nzle, M.; Hahn, E.; Hermanns, H.; Wolovick, N.; and Zhang, L. 2011. Measurability and safety verification for stochastic hybrid systems. In Proceedings of the 14th ACM International Conference on Hybrid Systems: Computation and Control, HSCC 2011, Chicago, IL, USA, April 12-14, 2011, 43\u201352. Fr\u00e4nzle, Teige, and Eggers (2010) Fr\u00e4nzle, M.; Teige, T.; and Eggers, A. 2010. Engineering constraint solvers for automatic analysis of probabilistic hybrid automata. The Journal of Logic and Algebraic Programming 79(7): 436 \u2013 466. The 20th Nordic Workshop on Programming Theory (NWPT 2008). Gao, Kong, and Clarke (2013) Gao, S.; Kong, S.; and Clarke, E.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-14": "M. 2013. Satisfiability modulo ODEs. In 2013 Formal Methods in Computer-Aided Design, 105\u2013112. Gruenbacher et al. (2019) Gruenbacher, S.; Cyranka, J.; Islam, M. A.; Tschaikowski, M.; Smolka, S.; and Grosu, R. 2019. Under the Hood of a Stand-Alone Lagrangian Reachability Tool. EPiC Series in Computing 61. Gruenbacher et al. (2020) Gruenbacher, S.; Cyranka, J.; Lechner, M.; Islam, M. A.; Smolka, S. A.; and Grosu, R. 2020. Lagrangian Reachtubes: The Next Generation. arXiv preprint arXiv:2012.07458 . Hansen (1980) Hansen, E. 1980. Global Optimization Using Interval Analysis \u2013 the Multi-Dimensional Case. Numer.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-15": "Math. 34(3): 247\u2013270. Hansen and Ostermeier (2001) Hansen, N.; and Ostermeier, A. 2001. Completely Derandomized Self-Adaptation in Evolution Strategies. Evolutionary Computation 9(2): 159\u2013195. Hasani et al. (2020a) Hasani, R.; Lechner, M.; Amini, A.; Rus, D.; and Grosu, R. 2020a. Liquid Time-constant Networks. arXiv preprint arXiv:2006.04439 . Hasani et al. (2020b) Hasani, R.; Lechner, M.; Amini, A.; Rus, D.; and Grosu, R. 2020b. A Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits. In International Conference on Machine Learning, 4082\u20134093. PMLR. He et al. (2016) He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770\u2013778. Holl, Koltun, and Thuerey (2020) Holl, P.; Koltun, V.; and Thuerey, N. 2020. Learning to control pdes with differentiable physics. arXiv preprint arXiv:2001.07457 . Huang et al. (2017) Huang, C.; Chen, X.; Lin, W.; Yang, Z.; and Li, X. 2017. Probabilistic Safety Verification of Stochastic Hybrid Systems Using Barrier Certificates.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-16": "ACM Trans. Embed. Comput. Syst. 16(5s). ISSN 1539-9087. Igel, Hansen, and Roth (2007) Igel, C.; Hansen, N.; and Roth, S. 2007. Covariance Matrix Adaptation for Multi-objective Optimization. Evolutionary Computation 15(1): 1\u201328. Immler (2015) Immler, F. 2015. Verified Reachability Analysis of Continuous Systems. In Baier, C.; and Tinelli, C., eds., Tools and Algorithms for the Construction and Analysis of Systems, 37\u201351. Berlin, Heidelberg: Springer Berlin Heidelberg. Jia and Benson (2019) Jia, J.; and Benson, A. R. 2019. Neural jump stochastic differential equations. In Advances in Neural Information Processing Systems, 9847\u20139858. Kapela et al. (2020) Kapela, T.; Mrozek, M.; Wilczak, D.; and Zgliczynski, P. 2020. CAPD:: DynSys: a flexible C++ toolbox for rigorous numerical analysis of dynamical systems.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-17": "Pre-Print - ww2.ii.uj.edu.pl . Kidger et al. (2020) Kidger, P.; Morrill, J.; Foster, J.; and Lyons, T. 2020. Neural controlled differential equations for irregular time series. arXiv preprint arXiv:2005.08926 . Lechner and Hasani (2020) Lechner, M.; and Hasani, R. 2020. Learning Long-Term Dependencies in Irregularly-Sampled Time Series. arXiv preprint arXiv:2006.04418 . Lechner et al. (2020) Lechner, M.; Hasani, R.; Amini, A.; Henzinger, T. A.; Rus, D.; and Grosu, R. 2020. Neural circuit policies enabling auditable autonomy. Nature Machine Intelligence 2(10): 642\u2013652. Li, Bak, and Bogomolov (2020) Li, D.; Bak, S.; and Bogomolov, S. 2020. Reachability Analysis of Nonlinear Systems Using Hybridization and Dynamics Scaling. In Bertrand, N.; and Jansen, N., eds., Formal Modeling and Analysis of Timed Systems, 265\u2013282. Cham: Springer International Publishing. Lohner (1992) Lohner, R. 1992. Computation of Guaranteed Enclosures for the Solutions of Ordinary Initial and Boundary Value Problems. In J.R. Cash, I. G., ed., Computational Ordinary Differential Equations. Clarendon Press, Oxford. Malherbe and Vayatis (2017) Malherbe, C.; and Vayatis, N. 2017. Global Optimization of Lipschitz Functions. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, 2314\u20132323. JMLR.org. Meyer, Devonport, and Arcak (2019) Meyer, P.-J.; Devonport, A.; and Arcak, M. 2019. TIRA: Toolbox for Interval Reachability Analysis. In Association for Computing Machinery, HSCC \u201919, 224\u2013229.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-18": "New York, NY, USA. Nagy and Palmer (2003) Nagy, J. G.; and Palmer, K.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-19": "M. 2003. Steepest descent, CG, and iterative regularization of ill-posed problems. BIT Numerical Mathematics 43(5): 1003\u20131017. Neumaier (2004) Neumaier, A. 2004. Complete search in continuous global optimization and constraint satisfaction. Acta Numerica 13: 271\u2013369. Piyavskii (1972) Piyavskii, S. 1972. An algorithm for finding the absolute extremum of a function. USSR Computational Mathematics and Mathematical Physics 12(4): 57 \u2013 67. Pontryagin (2018) Pontryagin, L. S. 2018. Mathematical theory of optimal processes. Routledge. Quaglino et al. (2019) Quaglino, A.; Gallieri, M.; Masci, J.; and Koutn\u00edk, J. 2019. Snode: Spectral discretization of neural odes for system identification. arXiv preprint arXiv:1906.07038 . Rinnooy Kan and Timmer (1987a) Rinnooy Kan, A.; and Timmer, G. 1987a. Stochastic global optimization methods part I: Clustering methods. Mathematical Programming 39: 27\u2013\u201356. Rinnooy Kan and Timmer (1987b) Rinnooy Kan, A.; and Timmer, G. 1987b. Stochastic global optimization methods part II: Multi level methods. Mathematical Programming 39: 57\u2013\u201378. Rubanova, Chen, and Duvenaud (2019) Rubanova, Y.; Chen, R. T.; and Duvenaud, D.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-20": "K. 2019. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems, 5320\u20135330. Rumelhart, Hinton, and Williams (1986) Rumelhart, D. E.; Hinton, G. E.; and Williams, R.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-21": "J. 1986. Learning representations by back-propagating errors.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-22": "nature 323(6088): 533\u2013536.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-23": "Shengqiao (2011) Shengqiao, L. 2011. Concise Formulas for the Area and Volume of a Hyperspherical Cap. Asian Journal of Mathematics and Statistics 4. Shmarov and Zuliani (2015a) Shmarov, F.; and Zuliani, P. 2015a. ProbReach: A Tool for Guaranteed Reachability Analysis of Stochastic Hybrid Systems. In Bogomolov, S.; and Tiwari, A., eds., 1st International Workshop on Symbolic and Numerical Methods for Reachability Analysis, SNR@CAV 2015, San Francisco, CA, USA, July 19, 2015, volume 37 of EPiC Series in Computing, 40\u201348.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-24": "EasyChair. URL https://easychair.org/publications/paper/z1f. Shmarov and Zuliani (2015b) Shmarov, F.; and Zuliani, P. 2015b. ProbReach: verified probabilistic delta-reachability for stochastic hybrid systems. In Girard, A.; and Sankaranarayanan, S., eds., Proceedings of the 18th International Conference on Hybrid Systems: Computation and Control, HSCC\u201915, Seattle, WA, USA, April 14-16, 2015, 134\u2013139.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-25": "ACM. Shubert (1972) Shubert, B. O. 1972. A Sequential Method Seeking the Global Maximum of a Function. SIAM Journal on Numerical Analysis 9(3): 379\u2013388.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-26": "ISSN 00361429. URL http://www.jstor.org/stable/2156138. Slaughter (2002) Slaughter, W. 2002. The Linearized Theory of Elasticity. Springer Science and Business Media, LLC. Stepanenko (2009) Stepanenko, S. 2009. Global Optimization Methods based on Tabu Search : GTS, GOTS, TSPA.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-27": "Application for conformation searches. Bod. Virmaux and Scaman (2018) Virmaux, A.; and Scaman, K. 2018. Lipschitz regularity of deep neural networks: analysis and efficient estimation. In Advances in Neural Information Processing Systems, 3835\u20133844. Wang et al. (2015) Wang, Q.; Zuliani, P.; Kong, S.; Gao, S.; and Clarke, E.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-28": "M. 2015. SReach: A Probabilistic Bounded Delta-Reachability Analyzer for Stochastic Hybrid Systems.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-29": "In Roux, O.; and Bourdon, J., eds., Computational Methods in Systems Biology, 15\u201327.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-30": "Cham: Springer International Publishing. Yan et al. (2020) Yan, H.; Du, J.; Tan, V. Y.; and Feng, J. 2020. On robustness of neural ordinary differential equations. International Conference on Learning Representations . Yang et al. (2020) Yang, Y.; Wu, J.; Li, H.; Li, X.; Shen, T.; and Lin, Z. 2020. Dynamical System Inspired Adaptive Time Stepping Controller for Residual Network Families. Thirty-Fourht AAAI Conference on Artificial Intelligence . Zgliczynski (2002) Zgliczynski, P. 2002. C1 Lohner Algorithm. Foundations of Computational Mathematics 429\u2013465. Zhigljavsky and Zilinskas (2008) Zhigljavsky, A.; and Zilinskas, A. 2008. Stochastic Global Optimization, volume 9 of Springer Optimization and Its Applications. Springer US. Zhuang et al. (2020) Zhuang, J.; Dvornek, N.; Li, X.; Tatikonda, S.; Papademetris, X.; and Duncan, J. 2020. Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE. arXiv preprint arXiv:2006.02493 . Appendix A Appendix\n\nTheorem 4 (Radius of Safety Region). At target time , let be the current global minimum . Let be an already visited point with value () and let and be defined as follows with :\n\nr \u03c6 = \u03bb \u03a3 \u03c6 \u2212 1 \u200b ( L \u200b ( \u03c6 ) \u2212 \u03bc \u22c5 m \u00af ) subscript \ud835\udc5f \ud835\udf11 superscript subscript \ud835\udf06 subscript \u03a3 \ud835\udf11 1 \ud835\udc3f \ud835\udf11 \u22c5 \ud835\udf07 \u00af \ud835\udc5a \\displaystyle\\begin{split}r_{\\varphi}&=\\lambda_{\\Sigma_{\\varphi}}^{-1}\\left(L(\\varphi)-\\mu\\cdot\\bar{m}\\right)\\end{split} (20)\n\nwith . If is chosen s.t. , then it holds that\n\nL \u200b ( \u03c8 ) \u2265 \u03bc \u22c5 m \u00af \u2200 x \u200b ( \u03c8 ) \u2208 B \u200b ( \u03c6 , r \u03c6 ) S formulae-sequence \ud835\udc3f \ud835\udf13 \u22c5 \ud835\udf07 \u00af \ud835\udc5a for-all \ud835\udc65 \ud835\udf13 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle L(\\psi)\\geq\\mu\\cdot\\bar{m}\\quad\\forall x(\\psi)\\in B(\\varphi,r_{\\varphi})^{S} (21)\n\nProof. Given and as defined in the above theorem. Using the mean value inequality for vector valued functions, the triangle inequality, and considering the change of metric (Cyranka et al. 2017, Lemma 2) it holds that:\n\n| L \u200b ( \u03c6 1 ) \u2212 L \u200b ( \u03c6 2 ) | = \ud835\udc3f subscript \ud835\udf11 1 \ud835\udc3f subscript \ud835\udf11 2 absent \\displaystyle|L(\\varphi_{1})-L(\\varphi_{2})|= | \u2225 \u03c7 t 0 t j \u200b ( x \u200b ( \u03c6 1 ) ) \u2212 \u03c7 t 0 t j \u200b ( x 0 ) \u2225 M j \u2212 \u2225 \u03c7 t 0 t j \u200b ( x \u200b ( \u03c6 2 ) ) \u2212 \u03c7 t 0 t j \u200b ( x 0 ) \u2225 M j | subscript delimited-\u2225\u2225 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 subscript \ud835\udf11 1 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 subscript \ud835\udc65 0 subscript \ud835\udc40 \ud835\udc57 subscript delimited-\u2225\u2225 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 subscript \ud835\udf11 2 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 subscript \ud835\udc65 0 subscript \ud835\udc40 \ud835\udc57 \\displaystyle\\left|\\left\\lVert\\chi_{t_{0}}^{t_{j}}(x(\\varphi_{1}))-\\chi_{t_{0}}^{t_{j}}(x_{0})\\right\\rVert_{M_{j}}-\\left\\lVert\\chi_{t_{0}}^{t_{j}}(x(\\varphi_{2}))-\\chi_{t_{0}}^{t_{j}}(x_{0})\\right\\rVert_{M_{j}}\\right| \u2264 \u2225 \u03c7 t 0 t j \u200b ( x \u200b ( \u03c6 1 ) ) \u2212 \u03c7 t 0 t j \u200b ( x \u200b ( \u03c6 2 ) ) \u2225 M j absent subscript delimited-\u2225\u2225 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 subscript \ud835\udf11 1 superscript subscript \ud835\udf12 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc57 \ud835\udc65 subscript \ud835\udf11 2 subscript \ud835\udc40 \ud835\udc57 \\displaystyle\\leq\\left\\lVert\\chi_{t_{0}}^{t_{j}}(x(\\varphi_{1}))-\\chi_{t_{0}}^{t_{j}}(x(\\varphi_{2}))\\right\\rVert_{M_{j}} \u2264 \u03bb \u03a3 \u03c6 \u200b \u2225 x \u200b ( \u03c6 1 ) \u2212 x \u200b ( \u03c6 2 ) \u2225 I absent subscript \ud835\udf06 subscript \u03a3 \ud835\udf11 subscript delimited-\u2225\u2225 \ud835\udc65 subscript \ud835\udf11 1 \ud835\udc65 subscript \ud835\udf11 2 \ud835\udc3c \\displaystyle\\leq\\lambda_{\\Sigma_{\\varphi}}\\left\\lVert x(\\varphi_{1})-x(\\varphi_{2})\\right\\rVert_{I}\\quad \u2200 x \u200b ( \u03c6 1 ) , x \u200b ( \u03c6 2 ) \u2208 \u03a3 \u03c6 \u2287 B \u200b ( \u03c6 , r \u03c6 ) S for-all \ud835\udc65 subscript \ud835\udf11 1 \ud835\udc65 subscript \ud835\udf11 2 subscript \u03a3 \ud835\udf11 superset-of-or-equals \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle\\forall x(\\varphi_{1}),x(\\varphi_{2})\\in\\Sigma_{\\varphi}\\supseteq B(\\varphi,r_{\\varphi})^{S}\n\nThus is a local Lipschitz constant in the safety region . By definition for . Hence:\n\n| L \u200b ( \u03c8 ) \u2212 L \u200b ( \u03c6 ) | \ud835\udc3f \ud835\udf13 \ud835\udc3f \ud835\udf11 \\displaystyle|L(\\psi)-L(\\varphi)| (22) \u2264 \u03bb \u03a3 \u03c6 \u200b \u2016 x \u200b ( \u03c8 ) \u2212 x \u200b ( \u03c6 ) \u2016 absent subscript \ud835\udf06 subscript \u03a3 \ud835\udf11 norm \ud835\udc65 \ud835\udf13 \ud835\udc65 \ud835\udf11 \\displaystyle\\leq\\lambda_{\\Sigma_{\\varphi}}\\|x(\\psi)-x(\\varphi)\\| (23) \u2264 \u03bb \u03a3 \u03c6 \u200b r \u03c6 = L \u200b ( \u03c6 ) \u2212 \u03bc \u22c5 m \u00af \u2200 x \u200b ( \u03c8 ) \u2208 B \u200b ( \u03c6 , r \u03c6 ) S formulae-sequence absent subscript \ud835\udf06 subscript \u03a3 \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc3f \ud835\udf11 \u22c5 \ud835\udf07 \u00af \ud835\udc5a for-all \ud835\udc65 \ud835\udf13 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle\\leq\\lambda_{\\Sigma_{\\varphi}}r_{\\varphi}=L(\\varphi)-\\mu\\cdot\\bar{m}\\quad\\forall x(\\psi)\\in B(\\varphi,r_{\\varphi})^{S} (24)\n\nTo prove that Eq. (21) holds, we distinguish between two cases for : (1) and (2) .",
    "0033cbb16ea8996520f2e239298070975b3f31ae-31": "Case (1) it is straightforward: . In case (2), we use Eq. (22) and thus:\n\n| L \u200b ( \u03c8 ) \u2212 L \u200b ( \u03c6 ) | \ud835\udc3f \ud835\udf13 \ud835\udc3f \ud835\udf11 \\displaystyle|L(\\psi)-L(\\varphi)| = L \u200b ( \u03c6 ) \u2212 L \u200b ( \u03c8 ) \u2264 L \u200b ( \u03c6 ) \u2212 \u03bc \u22c5 m \u00af absent \ud835\udc3f \ud835\udf11 \ud835\udc3f \ud835\udf13 \ud835\udc3f \ud835\udf11 \u22c5 \ud835\udf07 \u00af \ud835\udc5a \\displaystyle=L(\\varphi)-L(\\psi)\\leq L(\\varphi)-\\mu\\cdot\\bar{m} \u27f9 L \u200b ( \u03c8 ) \u2265 \u03bc \u22c5 m \u00af , \u27f9 absent \ud835\udc3f \ud835\udf13 \u22c5 \ud835\udf07 \u00af \ud835\udc5a \\displaystyle\\Longrightarrow L(\\psi)\\geq\\mu\\cdot\\bar{m},\n\nproving that Eq. (21) holds no matter if or if , for all . \u220e\n\nTheorem 5 (Convergence Guarantees). Given , , local Lipschitz constant and , where N is the number of uniform-randomly generated points during global search process. Let be the current minimum, the global minimum, and an argument s.t. . It holds that\n\nlim N \u2192 \u221e Pr \u2061 ( \u03bc \u22c5 m \u00af N \u2264 m \u22c6 ) = 1 subscript \u2192 \ud835\udc41 Pr \u22c5 \ud835\udf07 subscript \u00af \ud835\udc5a \ud835\udc41 superscript \ud835\udc5a \u22c6 1 \\displaystyle\\lim_{N\\rightarrow\\infty}\\Pr(\\mu\\cdot\\bar{m}_{N}\\leq m^{\\star})=1 (25)\n\nand thus\n\n\u2200 \u03b3 \u2208 ( 0 , 1 ) \u200b \u2203 N \u2208 \u2115 , s.t. \u200b Pr \u2061 ( \u03bc \u22c5 m \u00af N \u2264 m \u22c6 ) \u2265 1 \u2212 \u03b3 formulae-sequence for-all \ud835\udefe 0 1 \ud835\udc41 \u2115 s.t. Pr \u22c5 \ud835\udf07 subscript \u00af \ud835\udc5a \ud835\udc41 superscript \ud835\udc5a \u22c6 1 \ud835\udefe \\displaystyle\\forall\\gamma\\in(0,1)\\exists N\\in\\mathbb{N},\\textrm{ s.t. }\\Pr(\\mu\\cdot\\bar{m}_{N}\\leq m^{\\star})\\geq 1-\\gamma (26)\n\nProof. Next we derive some probabilities:\n\nPr \u2061 ( B \u200b ( \u03c6 j , r \u03c6 j ) S \u220b\u0338 \u03c6 \u22c6 ) Pr superscript \ud835\udf11 \u22c6 \ud835\udc35 superscript subscript \ud835\udf11 \ud835\udc57 subscript \ud835\udc5f subscript \ud835\udf11 \ud835\udc57 \ud835\udc46 \\displaystyle\\Pr(B({\\varphi_{j}},r_{\\varphi_{j}})^{S}\\not\\owns\\varphi^{\\star}) = 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r \u03c6 j ) ) absent 1 Pr \ud835\udc9e subscript \ud835\udc5f subscript \ud835\udf11 \ud835\udc57 \\displaystyle=1-\\Pr(\\mathcal{C}(r_{{\\varphi_{j}}})) Pr \u2061 ( \u2200 \u03c6 \u2208 \ud835\udcb0 : B \u200b ( \u03c6 , r \u03c6 ) S \u220b\u0338 \u03c6 \u22c6 ) Pr : for-all \ud835\udf11 \ud835\udcb0 superscript \ud835\udf11 \u22c6 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle\\Pr(\\forall\\varphi\\in\\mathcal{U}\\colon B({\\varphi},r_{\\varphi})^{S}\\not\\owns\\varphi^{\\star}) = \u220f \u03c6 \u2208 \ud835\udcb0 ( 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r \u03c6 ) ) ) absent subscript product \ud835\udf11 \ud835\udcb0 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udf11 \\displaystyle=\\prod_{\\varphi\\in\\mathcal{U}}\\left(1-\\Pr(\\mathcal{C}(r_{{\\varphi}}))\\right) Pr \u2061 ( \u2203 \u03c6 \u2208 \ud835\udcb0 : B \u200b ( \u03c6 , r \u03c6 ) S \u220b \u03c6 \u22c6 ) Pr : \ud835\udf11 \ud835\udcb0 superscript \ud835\udf11 \u22c6 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle\\Pr(\\exists\\varphi\\in\\mathcal{U}\\colon B({\\varphi},r_{\\varphi})^{S}\\owns\\varphi^{\\star}) = 1 \u2212 \u220f \u03c6 \u2208 \ud835\udcb0 ( 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r \u03c6 ) ) ) absent 1 subscript product \ud835\udf11 \ud835\udcb0 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udf11 \\displaystyle=1-\\prod_{\\varphi\\in\\mathcal{U}}\\left(1-\\Pr(\\mathcal{C}(r_{{\\varphi}}))\\right) (27)\n\nUsing Theorem 4, if for some , then holds, thus:\n\nPr \u2061 ( \u03bc \u22c5 m \u00af \u2264 m \u22c6 ) \u2265 Pr \u2061 ( \u2203 \u03c6 \u2208 \ud835\udcb0 : B \u200b ( \u03c6 , r \u03c6 ) S \u220b \u03c6 \u22c6 ) Pr \u22c5 \ud835\udf07 \u00af \ud835\udc5a superscript \ud835\udc5a \u22c6 Pr : \ud835\udf11 \ud835\udcb0 superscript \ud835\udf11 \u22c6 \ud835\udc35 superscript \ud835\udf11 subscript \ud835\udc5f \ud835\udf11 \ud835\udc46 \\displaystyle\\begin{split}&\\Pr(\\mu\\cdot\\bar{m}\\leq m^{\\star})\\geq\\\\\n&\\Pr(\\exists\\varphi\\in\\mathcal{U}\\colon B({\\varphi},r_{\\varphi})^{S}\\owns\\varphi^{\\star})\\end{split} (28)\n\nThus, it holds that , with as defined in Eq. (20). r \u03c6 subscript \ud835\udc5f \ud835\udf11 \\displaystyle r_{{\\varphi}} \u2265 \u03bb \u212c 0 S \u2212 1 \u200b ( min \u03c6 \u2208 \ud835\udcb0 \u2061 L \u200b ( \u03c6 ) \u2212 \u03bc \u22c5 m \u00af ) absent superscript subscript \ud835\udf06 superscript subscript \u212c 0 \ud835\udc46 1 subscript \ud835\udf11 \ud835\udcb0 \ud835\udc3f \ud835\udf11 \u22c5 \ud835\udf07 \u00af \ud835\udc5a \\displaystyle\\geq\\lambda_{\\mathcal{B}_{0}^{S}}^{-1}(\\min_{\\varphi\\in\\mathcal{U}}L(\\varphi)-\\mu\\cdot\\bar{m}) (29) \u2265 \u03bb \u212c 0 S \u2212 1 \u200b ( 1 \u2212 \u03bc ) \u200b m \u00af absent superscript subscript \ud835\udf06 superscript subscript \u212c 0 \ud835\udc46 1 1 \ud835\udf07 \u00af \ud835\udc5a \\displaystyle\\geq\\lambda_{\\mathcal{B}_{0}^{S}}^{-1}(1-\\mu)\\bar{m} (30) \u2265 \u03bb \u212c 0 S \u2212 1 \u200b ( 1 \u2212 \u03bc ) \u200b L \u200b ( \u03c6 1 ) = r b \u200b o \u200b u \u200b n \u200b d \u2200 \u03c6 \u2208 \ud835\udcb0 , formulae-sequence absent superscript subscript \ud835\udf06 superscript subscript \u212c 0 \ud835\udc46 1 1 \ud835\udf07 \ud835\udc3f subscript \ud835\udf11 1 subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 for-all \ud835\udf11 \ud835\udcb0 \\displaystyle\\geq\\lambda_{\\mathcal{B}_{0}^{S}}^{-1}(1-\\mu)L(\\varphi_{1})=r_{bound}\\quad\\forall\\varphi\\in\\mathcal{U}, (31)\n\nwith being the first random sampled point. Hence:\n\nPr \u2061 ( \u03bc \u22c5 m \u00af \u2264 m \u22c6 ) Pr \u22c5 \ud835\udf07 \u00af \ud835\udc5a superscript \ud835\udc5a \u22c6 \\displaystyle\\Pr(\\mu\\cdot\\bar{m}\\leq m^{\\star}) \u2265 1 \u2212 \u220f \u03c6 \u2208 \ud835\udcb0 ( 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) ) ) absent 1 subscript product \ud835\udf11 \ud835\udcb0 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 \\displaystyle\\geq 1-\\prod_{\\varphi\\in\\mathcal{U}}\\left(1-\\Pr(\\mathcal{C}(r_{{bound}}))\\right) (32) \u2265 1 \u2212 ( 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) ) ) N absent 1 superscript 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 \ud835\udc41 \\displaystyle\\geq 1-\\left(1-\\Pr(\\mathcal{C}(r_{{bound}}))\\right)^{N} (33)\n\nAs , it follows that Eq. (25) holds and thus we are able to guarantee the convergence of our global search strategy. \u220e\n\nTheorem 6 (Convergence Rate). Given , , local Lipschitz constant and dimension . Let be the first random-samples point. We can guarantee that if we perform at most iterations of the SLR Algorithm, with\n\nN m \u200b a \u200b x = ln \u2061 \u03b3 / ln \u2061 ( 1 \u2212 1 2 \u200b \u03c0 \u200b \u0393 \u200b ( n / 2 ) \u0393 \u200b ( ( n + 1 ) / 2 ) \u200b ( \u03c1 \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) \u03b4 0 ) n \u2212 1 ) subscript \ud835\udc41 \ud835\udc5a \ud835\udc4e \ud835\udc65 / \ud835\udefe 1 1 2 \ud835\udf0b \u0393 \ud835\udc5b 2 \u0393 \ud835\udc5b 1 2 superscript \ud835\udf0c subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 subscript \ud835\udeff 0 \ud835\udc5b 1 \\begin{split}&N_{max}=\\\\\n&\\ln{\\gamma}\\left/\\ln\\left(1-\\frac{1}{2\\sqrt{\\pi}}\\frac{\\Gamma(n/2)}{\\Gamma((n+1)/2)}\\left(\\frac{\\rho(r_{bound})}{\\delta_{0}}\\right)^{n-1}\\right)\\right.\\end{split} (34)\n\nand asymptotically it holds that\n\nN m \u200b a \u200b x = \ud835\udcaa \u200b ( \u2212 ln \u2061 \u03b3 \u200b ( \u03b4 0 r b \u200b o \u200b u \u200b n \u200b d ) 2 \u200b n ) , subscript \ud835\udc41 \ud835\udc5a \ud835\udc4e \ud835\udc65 \ud835\udcaa \ud835\udefe superscript subscript \ud835\udeff 0 subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 2 \ud835\udc5b \\displaystyle N_{max}=\\mathcal{O}\\left(-\\ln\\gamma\\left(\\frac{\\delta_{0}}{r_{bound}}\\right)^{2n}\\right), (35)\n\nwith (as defined in Eq.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-32": "(31)) and . Proof. If the right-hand side of Eq. (33) equals , it holds that . Thus reformulating that equation, we get an upper bound :\n\n1 \u2212 ( 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) ) ) N 1 superscript 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 \ud835\udc41 \\displaystyle 1-\\left(1-\\Pr(\\mathcal{C}(r_{{bound}}))\\right)^{N} = 1 \u2212 \u03b3 absent 1 \ud835\udefe \\displaystyle=1-\\gamma \u21d4 \u21d4 \\displaystyle\\Leftrightarrow (36) ( 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) ) ) N superscript 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 \ud835\udc41 \\displaystyle\\left(1-\\Pr(\\mathcal{C}(r_{{bound}}))\\right)^{N} = \u03b3 absent \ud835\udefe \\displaystyle=\\gamma \u21d4 \u21d4 \\displaystyle\\Leftrightarrow (37) ln \u2061 ( \u03b3 ) ln \u2061 ( 1 \u2212 Pr \u2061 ( \ud835\udc9e \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) ) ) \ud835\udefe 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 \\displaystyle\\frac{\\ln(\\gamma)}{\\ln(1-\\Pr(\\mathcal{C}(r_{{bound}})))} = N absent \ud835\udc41 \\displaystyle=N (38)\n\nWe use the following to overestimate of : Using the results of (Shengqiao 2011) it holds that , thus:\n\nPr \u2061 ( \ud835\udc9e \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) ) \u2265 Vol n \u2212 1 \u200b ( \u03c1 \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) ) Area \u2061 ( \u212c 0 ) = \u03c0 ( n \u2212 1 ) / 2 \u0393 \u200b ( ( n + 1 ) / 2 ) \u200b \u03c1 \u200b ( r b \u200b o \u200b u \u200b n \u200b d ) n \u2212 1 \u200b \u0393 \u200b ( n / 2 ) 2 \u200b \u03c0 n / 2 \u200b 1 \u03b4 0 n \u2212 1 Pr \ud835\udc9e subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 subscript Vol \ud835\udc5b 1 \ud835\udf0c subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 Area subscript \u212c 0 superscript \ud835\udf0b \ud835\udc5b 1 2 \u0393 \ud835\udc5b 1 2 \ud835\udf0c superscript subscript \ud835\udc5f \ud835\udc4f \ud835\udc5c \ud835\udc62 \ud835\udc5b \ud835\udc51 \ud835\udc5b 1 \u0393 \ud835\udc5b 2 2 superscript \ud835\udf0b \ud835\udc5b 2 1 superscript subscript \ud835\udeff 0 \ud835\udc5b 1 \\displaystyle\\begin{split}&\\Pr(\\mathcal{C}(r_{{bound}}))\\geq\\frac{\\textrm{Vol}_{n-1}(\\rho(r_{bound}))}{\\operatorname{Area}(\\mathcal{B}_{0})}=\\\\\n&\\frac{\\pi^{(n-1)/2}}{\\Gamma((n+1)/2)}\\rho(r_{bound})^{n-1}\\frac{\\Gamma(n/2)}{2\\pi^{n/2}}\\frac{1}{\\delta_{0}^{n-1}}\\end{split} (39)\n\nwith being the gamma function.",
    "0033cbb16ea8996520f2e239298070975b3f31ae-33": "Putting Eq. (39) and Eq. (38) we obviously get to equation (34). Using the information that and using the results of (Zhigljavsky and Zilinskas 2008)[Section 2.2], the asymptotical value of equals to Eq. (35) \u220e\n\n\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Fri Mar 1 13:24:21 2024 by LaTeXML"
}