{
    "00c04118a0d0f1ff855ad4597025448c5daf873d-0": "Learning Unitary Operators with Help From\n\nStephanie L. Hyland1, 2, Gunnar R\u00e4tsch1 1Department of Computer Science, ETH Zurich, Switzerland 2Tri-Institutional Training Program in Computational Biology and Medicine, Weill Cornell Medical, New York {hyland, raetsch}@inf.ethz.ch\n\nLearning Unitary Operators with Help From\n\nStephanie L. Hyland1, 2, Gunnar R\u00e4tsch1 1Department of Computer Science, ETH Zurich, Switzerland 2Tri-Institutional Training Program in Computational Biology and Medicine, Weill Cornell Medical, New York {hyland, raetsch}@inf.ethz.ch\n\nAbstract\n\nA major challenge in the training of recurrent neural networks is the so-called vanishing or exploding gradient problem. The use of a norm-preserving transition operator can address this issue, but parametrization is challenging. In this work we focus on unitary operators and describe a parametrization using the Lie algebra associated with the Lie group of unitary matrices. The exponential map provides a correspondence between these spaces, and allows us to define a unitary matrix using real coefficients relative to a basis of the Lie algebra. The parametrization is closed under additive updates of these coefficients, and thus provides a simple space in which to do gradient descent. We demonstrate the effectiveness of this parametrization on the problem of learning arbitrary unitary operators, comparing to several baselines and outperforming a recently-proposed lower-dimensional parametrization. We additionally use our parametrization to generalize a recently-proposed unitary recurrent neural network to arbitrary unitary matrices, using it to solve standard long-memory tasks. Introduction\n\nWhile recurrent neural networks (RNNs) are seeing widespread success across many tasks, the fundamental architecture presents challenges to typical training algorithms. In particular, the problem of \u2018vanishing/exploding gradients\u2019 (?) in gradient-based optimization persists, where gradients either vanish or diverge as one goes deeper into the network, resulting in slow training or numerical instability. The long short-term memory (LSTM) network (?) was designed to overcome this issue. Recently, the use of norm-preserving operators in the transition matrix - the matrix of weights connecting subsequent internal states - of the RNN have been explored (?; ?; ?). Using operators with bounded eigenvalue spectrum should, as demonstrated by ? (?), bound the norms of the gradients in the network, assuming an appropriate non-linearity is applied. Unitary matrices satisfy this requirement and are the focus of this work. Imposing unitarity (or orthogonality) on this transition matrix is however challenging for gradient-based optimization methods, as additive updates typically do not preserve unitarity. Solutions include re-unitarizing after each batch, or using a parametrization of unitary matrices closed under addition. In this work we propose a solution in the second category, using results from the theory of Lie algebras and Lie groups to define a general parametrization of unitary matrices in terms of skew-Hermitian matrices (elements of the Lie algebra associated to the Lie group of unitary matrices). As explained in more detail below, elements of this Lie algebra can be identified with unitary matrices, while the algebra is closed under addition, forming a vector space over real numbers. While we are motivated by the issues of RNNs, and we consider an application in RNNs, our primary focus here is on a core question: how can unitary matrices be learned? Assuming the choice of a unitary transition matrix is an appropriate modelling choice, the gradients on this operator should ultimately guide it towards unitarity, if it is possible under the parametrization or learning scheme used. It is therefore useful to know which approach is best. We distil the problem into its simplest form (a learning task described in detail later), so that our findings cannot be confounded by other factors specific to the RNN long-term memory task, before demonstrating our parametrization in that setting. Related work\n\nWe draw most inspiration from the recent work of ? (?), who proposed a specific parametrization of unitary matrices and demonstrated its utility in standard long-term memory tasks for RNNs. We describe their parametrization here, as we use it later. Citing the difficulty of obtaining a general and efficient parametrization of unitary matrices, they use the fact that the unitary group is closed under matrix multiplication to form a composite operator:\n\nU = \ud835\udc03 3 \u200b \ud835\udc11 2 \u200b \u2131 \u2212 1 \u200b \ud835\udc03 2 \u200b \u03a0 \u200b \ud835\udc11 1 \u200b \u2131 \u200b \ud835\udc03 1 \ud835\udc48 subscript \ud835\udc03 3 subscript \ud835\udc11 2 superscript \u2131 1 subscript \ud835\udc03 2 \u03a0 subscript \ud835\udc11 1 \u2131 subscript \ud835\udc03 1 U=\\mathbf{D}_{3}\\mathbf{R}_{2}\\mathcal{F}^{-1}\\mathbf{D}_{2}\\Pi\\mathbf{R}_{1}\\mathcal{F}\\mathbf{D}_{1} (1)\n\nwhere each component is unitary and easily parametrized:\n\n\u2022\n\nis a diagonal matrix with entries of the form ,\n\n\u2022\n\nis a complex reflection operator; ( denotes Hermitian conjugate)\n\n\u2022\n\nand are the Fourier and inverse Fourier transforms (or, in practice, their discrete matrix representations)\n\n\u2022\n\nis a fixed permutation matrix\n\nIn total, this parametrization has real learnable parameters ( for each reflection and for each diagonal operator), so describes a subspace of unitary matrices (which have real parameters). Nonetheless, they find that an RNN using this operator as its transition matrix outperforms LSTMs on the adding and memory tasks described first in ?",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-1": "(?). This prompted us to consider other parametrizations of unitary matrices which might be more expressive or interpretable. ? (?) constrain a part of the transition matrix to be close to the identity, acting as a form of long-term memory store, while ? (?) initialize it to the identity, and then use ReLUs as non-linearities. ? (?) study analytic solutions to the long-term memory task, supporting observations and intuitions that orthogonal (or unitary) matrices would be appropriate as transition matrices for this task. They also study initializations to orthogonal and identity matrices, and consider experiments where an additional term in the loss function encourages an orthogonal solution to the transition matrix, without using an explicit parametrization. ? (?) study exact solutions to learning dynamics in deep networks and find that orthogonal weight initializations at each layer lead to depth-independent learning (thus escaping the vanishing/exploding gradient problem). Interestingly, they attribute this to the eigenvalue spectrum of orthogonal matrices lying on the unit circle. They compare with weights initialized to random, scaled Gaussian values, which preserve norms in expectation (over values of the random matrix) and find orthogonal matrices superior. It therefore appears that preserving norms is not sufficient to stabilize gradients over network depth, but that the eigenvalue spectrum must also be strictly controlled. In a related but separate vein, ? (?) penalize the difference of difference of norms between subsequent hidden states in the network. This is not equivalent to imposing orthogonality of the transition matrix, as the norm of the hidden state may be influenced by the inputs and non-linearities, and their method directly addresses this norm. The theory of Lie groups and Lie algebras has seen most application in machine learning for its use in capturing notions of invariance. For example, ? (?), learn infinitesimal Lie group generators (elements of the Lie algebra) associated with affine transformations of images, corresponding to visual perceptual invariances. This is different to our setting as our generators are already known (we assume the Lie group ) and wish to learn the coefficients of a given transformation relative to that basis set of generators.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-2": "However, our approach could be extended to the case where the basis of is unknown, and must be learned. As we find later (appendix B), the choice of basis can impact performance, and so may be an important consideration. ? (?) learn commutative subgroups of (known as toroidal subgroups), motivated by learning the irreducible representations of the symmetry group corresponding to invariant properties of images. Their choice of group parametrization is equivalent to selecting a particular basis of the corresponding Lie algebra, as they describe, but primarily exploit the algebra to understand properties of toroidal subgroups. ? (?) perform motion estimation by defining a regression function in terms of a function on the Lie algebra of affine transformations, and then learning this. This is similar to our approach in the sense that they do optimization in the Lie algebra, although as they consider two-dimensional affine transformations only, their parametrization of the Lie algebra is straight forward. Finally, ? (?) describe an online learning algorithm for orthogonal matrices \u2013 which are the real-valued equivalent to unitary matrices. They also claim that the approach is extends easily to unitary matrices. Structure of this paper\n\nWe begin with an introduction to the relevant facts and definitions from the theory of Lie groups and Lie algebras, to properly situate this work in its mathematical context. Further exposition is beyond the scope of this paper, and we refer the interested reader to any of the comprehensive introductory texts on the matter. We explain our parametrization in detail and describe a method to calculate the derivative of the matrix exponential - a quantity otherwise computationally intractable. Then, we describe a simple but clear experiment designed to test our core question of learning unitary matrices. We compare to an approach using the parametrization of ? (?) and one using polar decomposition to \u2018back-project\u2019 to the closest unitary matrix. We use this experimental set-up to probe aspects of our model, studying the importance of the choice of basis (appendix B), and the impact of the restricted parameter set used by one of the alternate approaches. We additionally implement our parametrization in a recurrent neural network as a \u2018general unitary RNN\u2019, and evaluate its performance on standard long-memory tasks. The Lie algebra\n\nBasics of Lie groups and Lie algebras\n\nA Lie group is a group which is also a differentiable manifold, with elements of the group corresponding to points on the manifold. The group operations (multiplication and inversion) must be smooth maps (infinitely differentiable) back to the group. In this work we consider the group : the set of unitary matrices, with matrix multiplication. These are the complex-valued analogue to orthogonal matrices, satisfying the property\n\nU \u2020 \u200b U = U \u200b U \u2020 = \ud835\udd40 superscript \ud835\udc48 \u2020 \ud835\udc48 \ud835\udc48 superscript \ud835\udc48 \u2020 \ud835\udd40 U^{\\dagger}U=UU^{\\dagger}=\\mathbb{I} (2)\n\nwhere denotes the conjugate transpose (or Hermitian conjugate). Unitary matrices preserve matrix norms, and have eigenvalues lying on the (complex) unit circle, which is the desired property of the transition matrix in a RNN. The differentiable manifold property of Lie groups opens the door for the study of the Lie algebra. This object is the tangent space to the Lie group at the identity (the group must have an identity element). Consider a curve through the Lie group - a one-dimensional subspace parametrized by a variable , where (this is a matrix in parametrised by , not a group). Consider the defining property of unitary matrices (Equation 2), and take the derivative along this curve:\n\nU \u200b ( t ) \u2020 \u200b U \u200b ( t ) = \ud835\udd40 \u2192 U \u02d9 \u200b ( t ) \u2020 \u200b U \u200b ( t ) + U \u2020 \u200b ( t ) \u200b U \u02d9 \u200b ( t ) = 0 \ud835\udc48 superscript \ud835\udc61 \u2020 \ud835\udc48 \ud835\udc61 \ud835\udd40 \u2192 \u02d9 \ud835\udc48 superscript \ud835\udc61 \u2020 \ud835\udc48 \ud835\udc61 superscript \ud835\udc48 \u2020 \ud835\udc61 \u02d9 \ud835\udc48 \ud835\udc61 0 U(t)^{\\dagger}U(t)=\\mathbb{I}\\rightarrow\\dot{U}(t)^{\\dagger}U(t)+U^{\\dagger}(t)\\dot{U}(t)=0 (3)\n\nTaking , , we have\n\nU \u02d9 \u200b ( 0 ) \u2020 \u200b \ud835\udd40 + \ud835\udd40 \u2020 \u200b U \u02d9 \u200b ( 0 ) = 0 \u21d2 U \u02d9 \u200b ( 0 ) \u2020 = \u2212 U \u02d9 \u200b ( 0 ) \u02d9 \ud835\udc48 superscript 0 \u2020 \ud835\udd40 superscript \ud835\udd40 \u2020 \u02d9 \ud835\udc48 0 0 \u21d2 \u02d9 \ud835\udc48 superscript 0 \u2020 \u02d9 \ud835\udc48 0 \\dot{U}(0)^{\\dagger}\\mathbb{I}+\\mathbb{I}^{\\dagger}\\dot{U}(0)=0\\Rightarrow\\dot{U}(0)^{\\dagger}=-\\dot{U}(0) (4)\n\nThe elements belong to the Lie algebra. We refer to this Lie algebra as , and an arbitrary element as . Then Equation 4 defines the properites of these Lie algebra elements; they are skew-Hermitian matrices: . As vector spaces, Lie algebras are closed under addition. In particular is a vector space over , so a real linear combination of its elements is once again in (this is also clear from the definition of skew-Hermitian). We exploit this fact later. Lie algebras are also endowed with an operation known as the Lie bracket, which has many interesting properties, but is beyond the scope of this work. Lie algebras are interesting algebraic objects and have been studied deeply, but in this work we use because of the exponential map. Above, it was shown that elements of the algebra can be derived from the group (considering infinitesimal steps away from the identity). There is a reverse operation, allowing elements of the group to be recovered from the algebra: this is the exponential map. In the case of matrix groups, the exponential map is simply the matrix exponential:\n\nexp \u2061 ( L ) = \u2211 j = 0 \u221e L j j ! \ud835\udc3f superscript subscript \ud835\udc57 0 superscript \ud835\udc3f \ud835\udc57 \ud835\udc57 \\exp(L)=\\sum_{j=0}^{\\infty}\\frac{L^{j}}{j!} (5)\n\nVery simply, , then . While this map is not in general surjective, it so happens that is a compact, connected group and so is indeed surjective (?). That is, for any , there exists some such that . Notably, while orthogonal matrices also form a Lie group , with associated Lie algebra consisting of skew-symmetric matrices, is not connected, and so the exponential map can only produce special orthogonal matrices - those with determinant one - being the component of containing the identity. Parametrization of in terms of\n\nThe dimension of as a real vector space is . This is readily derived from noting that an arbitrary complex matrix has free real parameters, and the requirement of imposes constraints. So, a set of linearly-independent skew-Hermitian matrices defines a basis for the space; . Then any element can be written as\n\nL = \u2211 j = 1 n 2 \u03bb j \u200b T j \ud835\udc3f superscript subscript \ud835\udc57 1 superscript \ud835\udc5b 2 subscript \ud835\udf06 \ud835\udc57 subscript \ud835\udc47 \ud835\udc57 L=\\sum_{j=1}^{n^{2}}\\lambda_{j}T_{j} (6)\n\nwhere are real numbers; the coefficients of with respect to the basis. Using the exponential map,\n\nU = exp \u2061 ( L ) = exp \u2061 ( \u2211 j = 1 n 2 \u03bb j \u200b T j ) \ud835\udc48 \ud835\udc3f superscript subscript \ud835\udc57 1 superscript \ud835\udc5b 2 subscript \ud835\udf06 \ud835\udc57 subscript \ud835\udc47 \ud835\udc57 U=\\exp(L)=\\exp\\left(\\sum_{j=1}^{n^{2}}\\lambda_{j}T_{j}\\right) (7)\n\nwe see that these suffice as parameters of (given the basis ). This is the parametrization we propose. It has two attractive properties:\n\n1. It is a fully general parametrization, as the exponential map is surjective\n\n2. Gradient updates on preserve unitarity automatically, as the algebra is closed under addition\n\nThis parametrization means gradient steps are taken in the vector space of , rather than the manifold of , which may provide a flatter cost landscape - although confirming this intuition would require further analysis.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-3": "This work is intended to explore the use of this parametrization for learning arbitrary unitary matrices. There are many possible choices of basis for . We went for the following set of sparse matrices:\n\n1. diagonal, imaginary matrices: is on the -th diagonal, else zero. 2. symmetric, imaginary matrices with two non-zero elements, e.g., for ,\n\n3. anti-symmetric, real matrices with two non-zero elements, e.g., for ,\n\nWe explore the effects of choice of basis in appendix B. Derivatives of the matrix exponential\n\nThe matrix exponential appearing in Equation 7 poses an issue for gradient calculations. In general, the derivative of the matrix exponential does not have a closed-form expression, so computing gradients is intractable. In early stages of this work, we used the method of finite differences to approximate gradients, which would prohibit its use in larger-scale applications (such as RNNs). In the appendix we describe an investigation into using random projections to overcome this limitation, which while promising turned out to yield minimal benefit. We therefore sought mathematical solutions to this complexity issue, which we describe here and in further detail in the appendix. Exploiting the fact that is skew-Hermitian, we can derive an analytical expression for the derivative of with respect to each of its parameters, negating the need for finite differences. This expression takes the form:\n\n\u2202 U \u2202 \u03bb a = W \u200b V a \u200b W \u2020 \ud835\udc48 subscript \ud835\udf06 \ud835\udc4e \ud835\udc4a subscript \ud835\udc49 \ud835\udc4e superscript \ud835\udc4a \u2020 \\frac{\\partial U}{\\partial\\lambda_{a}}=WV_{a}W^{\\dagger} (8)\n\nwhere is a unitary matrix of eigenvectors obtained in the eigenvalue decomposition of ; , ( = diag(, , ); are the eigenvalues of ). Each is a matrix defined component-wise\n\ni = j : : \ud835\udc56 \ud835\udc57 absent \\displaystyle i=j: V i \u200b i = ( W \u2020 \u200b T a \u200b W ) i \u200b i \u200b e d i subscript \ud835\udc49 \ud835\udc56 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 subscript \ud835\udc47 \ud835\udc4e \ud835\udc4a \ud835\udc56 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udc51 \ud835\udc56 \\displaystyle V_{ii}=(W^{\\dagger}T_{a}W)_{ii}e^{d_{i}} (9) i \u2260 j : : \ud835\udc56 \ud835\udc57 absent \\displaystyle i\\neq j: V i \u200b j = ( W \u2020 \u200b T a \u200b W ) i \u200b j \u200b ( e d i \u2212 e d j d i \u2212 d j ) subscript \ud835\udc49 \ud835\udc56 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 subscript \ud835\udc47 \ud835\udc4e \ud835\udc4a \ud835\udc56 \ud835\udc57 superscript \ud835\udc52 subscript \ud835\udc51 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udc51 \ud835\udc57 subscript \ud835\udc51 \ud835\udc56 subscript \ud835\udc51 \ud835\udc57 \\displaystyle V_{ij}=(W^{\\dagger}T_{a}W)_{ij}\\left(\\frac{e^{d_{i}}-e^{d_{j}}}{d_{i}-d_{j}}\\right) (10)\n\nWhere is the basis matrix of the Lie algebra in the -th direction.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-4": "We provide the derivation, based on work from ? (?) and ? (?) in Appendix A. We can simplify the expression for each , depending on the type of basis element. In these expressions, refers to the -th row of W. 1. purely imaginary;\n\n2. symmetric imaginary, nonzero in positions and :\n\n3. antisymmetric real, nonzero in positions and :\n\nThese expressions follow from the sparsity of the basis and are derived in appendix A. Thus, we reduce the calculation of from two matrix multplications to at most two vector outer products. Overall, we have reduced the cost of calculating gradients to a single eigenvalue decomposition, and for each parameter two matrix multiplications (equation 8), one or two vector outer products, and element-wise multiplication of two matrices (equations 9, 10). As we see in the RNN experiments, this actually makes our approach faster than the (restricted)uRNN of (?) for roughly equivalent numbers of parameters. Supervised Learning of Unitary Operators\n\nWe consider the supervised learning problem of learning the unitary matrix that generated a from ; , given examples of such s and s. This is the core learning problem that needs to be solved for the state-transformation matrix in RNNs. It is similar to the setting considered in ? (?) (they consider an online learning problem). We compare a number of methods for learning at different values of . We further consider the case where we have artificially restricted the number of learnable variables in our parametrization (for the sake of comparison), and generate a pathological change of basis to demonstrate the relevance of selecting a good basis (appendix B). Task\n\nThe experimental setup is as follows: we create a unitary matrix (the next section describes how this is done), then sample vectors with normally-distributed coefficients. We create where . The objective is to recover from the pairs by minimizing the squared Euclidean distance between predicted and true values;\n\nU = argmin U 1 N \u200b \u2211 j N \u2016 \ud835\udc32 \ud835\udc23 ^ \u2212 \ud835\udc32 \ud835\udc23 \u2016 2 = argmin U 1 N \u200b \u2211 j N \u2016 U \u200b \ud835\udc31 j \u2212 \ud835\udc32 \ud835\udc23 \u2016 2 \ud835\udc48 subscript argmin \ud835\udc48 1 \ud835\udc41 superscript subscript \ud835\udc57 \ud835\udc41 superscript norm ^ subscript \ud835\udc32 \ud835\udc23 subscript \ud835\udc32 \ud835\udc23 2 subscript argmin \ud835\udc48 1 \ud835\udc41 superscript subscript \ud835\udc57 \ud835\udc41 superscript norm \ud835\udc48 subscript \ud835\udc31 \ud835\udc57 subscript \ud835\udc32 \ud835\udc23 2 U=\\mathop{\\mathrm{argmin}}_{U}\\frac{1}{N}\\sum_{j}^{N}\\|\\hat{\\mathbf{y_{j}}}-\\mathbf{y_{j}}\\|^{2}=\\mathop{\\mathrm{argmin}}_{U}\\frac{1}{N}\\sum_{j}^{N}\\|U\\mathbf{x}_{j}-\\mathbf{y_{j}}\\|^{2} (11)\n\nWhile this problem is easily solved in the batch setting using least-squares, we wish to learn through mini-batch stochastic gradient descent, to emulate a deep learning scenario.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-5": "For each experimental run (a single ), we generate one million training pairs, divided into batches of size 20. The test and validation sets both contain examples. In practice we set and use a fixed learning rate of . For larger dimensions, we run the model through the data for multiple epochs, shuffling and re-batching each time. All experiments were implemented in Python. The code is available here: https://github.com/ratschlab/uRNN. For the matrix exponential, we use the scipy builtin expm, which uses Pade approximation (?). We make use of the fact that is Hermitian to use eigh (also in scipy) to perform eigenvalue decompositions. Generating the ground-truth unitary matrix\n\nThe we wish to recover is generated by one of three methods:\n\n1. QR decomposition: we create a complex matrix with normally-distributed entries and then perform a QR decomposition, producing a unitary matrix and an upper triangular matrix (which is discarded). This approach is also used to sample orthogonal matrices in ? (?), noting a result from ? (?) demonstrating that this is equivalent to sampling from the appropriate Haar measure. 2. Lie algebra: given the standard basis of , we sample normally-distributed real to produce\n\n3. Unitary composition: we compose parametrized unitary operators as in ?",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-6": "(?) (Equation 1). The parameters are sampled as follows: angles in come from . The complex reflection vectors in come from where . We study the effects of this generation method on test-set loss in a later section. While we find no significant association between generation method and learning approach, in our experiments we nonetheless average over an equal number of experiments using each method, to compensate for possible unseen bias. Approaches\n\nWe compare the following approaches for learning :\n\n1. projection: is represented as an unconstrained complex matrix, but after each gradient update we project it to the closest unitary matrix, using polar decomposition (?).",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-7": "This amounts to real parameters. 2. arjovsky: is parametrized as in Equation 1, which comes to real parameters. 3. lie_algebra: (we refer to this as ) is parametrized by its real coefficients in the Lie algebra, as in Equation 7. As baselines we use the true matrix , and a random unitary matrix generated by the same method as (in that experimental run). We also implemented the algorithm described in ? (?) and considered both unitary and orthogonal learning tasks (our parametrization contains orthogonal matrices as a special case) but found it too numerically unstable and therefore excluded it from our analyses. Comparison of Approaches\n\nTable 1 shows the test-set loss for different values of and different approaches for learning .",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-8": "We performed between 6 and 18 replicates of each experiment, and show bootstrap estimates of means and standard errors over these replicates. As we can see, the learning task becomes more challenging as increases, but our parametrization () consistently outperforms the other approaches. Restricting to parameters\n\nAs mentioned, arjovsky uses only parameters. To check if this difference accounts for the differences in loss observed in Table 1, we ran experiments where we fixed all but (selected randomly) of the in the lie_algebra parametrization. The fixed parameters retained their initial values throughout the experiment. We observe that, as suspected, restricting to parameters results in a performance degradation equivalent to that of arjovsky. Table 2 shows the results for . The fact that the restricted case is consistently within error of the arjovsky model supports our hypothesis that the difference in learnable parameters accounts for the difference in performance. This suggests that generalising the model of ? to allow for parameters may result in performance similar to our approach. However, how to go about such a generalisation is unclear, as a naive approach would simply use a composition of operators, and this would likely become computationally intractable. Method of generating\n\nAs described, we used three methods to generate the true . One of these produces in the subspace available to the composition parametrization (Equation 1), so we were curious to see if this parametrization performed better on experiments using that method. We were also concerned that generating using the Lie algebra parametrization might make the task too \u2018easy\u2019 for our approach, as its random initialization could lie close to the true solution. Figure 1 shows box-plots of the distribution of test losses from these approaches for the three methods, comparing our approach () with that of ? (?), denoted arjovsky. To combine results from experiments using different values of , we first scaled test-set losses by the performance of rand (the random unitary matrix), so the y-axis ranges from 0 (perfect) to 1 (random performance). The dotted line denotes the average (over methods) of the test-set loss for true, similarly scaled. The right panel in Figure 1 shows a zoomed-in version of the result where the comparison with true is more meaningful, and a comparison with the case where we have restricted to learnable parameters (see earlier). We do not observe a difference (within error) between the methods, which is consistent between and arjovsky. Our concern that using the Lie algebra to generate would make the task \u2018too easy\u2019 for was seemingly unfounded. Unitary Recurrent Neural Network for Long Memory Tasks\n\nTo demonstrate that our approach is practical for use in deep learning, we incorporate it into a recurrent neural network to solve standard long-memory tasks. Specifically, we define a general unitary RNN with recurrence relation\n\n\ud835\udc21 t = f \u200b ( \u03b2 \u200b U \u200b \ud835\udc21 t \u2212 1 + V \u200b \ud835\udc31 t + \ud835\udc1b ) subscript \ud835\udc21 \ud835\udc61 \ud835\udc53 \ud835\udefd \ud835\udc48 subscript \ud835\udc21 \ud835\udc61 1 \ud835\udc49 subscript \ud835\udc31 \ud835\udc61 \ud835\udc1b \\mathbf{h}_{t}=f\\left(\\beta U\\mathbf{h}_{t-1}+V\\mathbf{x}_{t}+\\mathbf{b}\\right) (12)\n\nwhere is a nonlinearity, is a free scaling factor, is our unitary matrix parametrised as in equation 7, is the hidden state of the RNN and is the input data at \u2018time-point\u2019 . We refer to this as a \u2018general unitary RNN\u2019 (guRNN), to distinguish it from the restricted uRNN of ? (?). We use the guRNN on two tasks: the \u2018adding problem\u2019 and the \u2018memory problem\u2019, first described in (?). For the sake of brevity we refer to (?) for specific experimental details, as we use an identical experimental setup (reproduced in TensorFlow; see above github link for code). We compare our model (guRNN) with the restricted uRNN (ruRNN) parametrised as in equation 1, a LSTM (?), and the IRNN of ? (?). Figure 2 shows the results for each task where the sequence length or the memory duration is . While our model guarantees unitarity of , this is not sufficient to prevent gradients from vanishing. Consider the norm of the gradient of the cost with respect to the data at time , and use submultiplicativity of the norm to write;\n\n\u2016 \u2202 C \u2202 \ud835\udc31 \u03c4 \u2016 \u2264 \u2016 \u2202 C \u2202 \ud835\udc31 T \u2016 \u200b ( \u220f t = \u03c4 T \u2212 1 \u2016 f \u2032 \u200b ( U \u200b \ud835\udc21 t + V \u200b \ud835\udc31 t + \ud835\udc1b ) \u2016 \u200b \u2016 U \u2016 ) \u200b \u2016 \u2202 \ud835\udc21 \u03c4 \ud835\udc31 \u03c4 \u2016 norm \ud835\udc36 subscript \ud835\udc31 \ud835\udf0f norm \ud835\udc36 subscript \ud835\udc31 \ud835\udc47 superscript subscript product \ud835\udc61 \ud835\udf0f \ud835\udc47 1 norm superscript \ud835\udc53 \u2032 \ud835\udc48 subscript \ud835\udc21 \ud835\udc61 \ud835\udc49 subscript \ud835\udc31 \ud835\udc61 \ud835\udc1b norm \ud835\udc48 norm subscript \ud835\udc21 \ud835\udf0f subscript \ud835\udc31 \ud835\udf0f \\left\\|\\frac{\\partial C}{\\partial\\mathbf{x}_{\\tau}}\\right\\|\\leq\\left\\|\\frac{\\partial C}{\\partial\\mathbf{x}_{T}}\\right\\|\\left(\\prod_{t=\\tau}^{T-1}\\|f^{\\prime}\\left(U\\mathbf{h}_{t}+V\\mathbf{x}_{t}+\\mathbf{b}\\right)\\|\\|U\\|\\right)\\left\\|\\frac{\\partial\\mathbf{h}_{\\tau}}{\\mathbf{x}_{\\tau}}\\right\\|\n\nwhere is a diagonal matrix giving the derivatives of the nonlinearity. Using a unitary matrix fixes , but beyond further restrictions (on and ) does nothing to control the norm of , which is at most 1 for common nonlinearities. Designing a nonlinearity to better preserve gradient norms is beyond the scope of this work, so we simply scaled by a constant multiplicative factor to counteract the tendency of the nonlinearity to shrink gradients. In Figure 2 we denote this setup by guRNN\u03b2. Confirming our intuition, this simple modification greatly improves performance on both tasks. Perhaps owing to our efficient gradient calculation (appendix A) and simpler recurrence relation, our model runs faster than that of (?) (in our implementation), by a factor of 4.8 and 2.6 in the adding and memory tasks shown in Figure 2 respectively. This amounts to the guRNN processing 61.2 and 37.0 examples per second in the two tasks, on a GeForce GTX 1080 GPU. Discussion\n\nDrawing from the rich theory of Lie groups and Lie algebras, we have described a parametrization of unitary matrices appropriate for use in deep learning. This parametrization exploits the Lie group-Lie algebra correspondence through the exponential map to represent unitary matrices in terms of real coefficients relative to a given basis of the Lie algebra . As this map from to is surjective, the parametrization can describe any unitary matrix. We have demonstrated that unitary matrices can be learned with high accuracy using simple gradient descent, and that this approach outperforms a recently-proposed parametrization (from ? (?)) and significantly outperforms the approach of \u2018re-unitarizing\u2019 after gradient updates. This experimental design is quite simple, designed to probe a core problem, before considering the broader setting of RNNs. Our experiments with general unitary RNNs using this parametrization showed that this approach is practical for deep learning. With a fraction of the parameters, our model outperforms LSTMs on the standard \u2018memory problem\u2019 and attains comparable (although inferior) performance on the adding problem (?). Further work is required to understand the difference in performance between our approach and the ruRNN of (?) - perhaps the -dimensional subspace captured by their parametrization is serendipitously beneficial for these RNN tasks - although we note that the results presented here are not the fruit of exhaustive hyperparameter exploration. Of particular interest is the impressive performance of both uRNNs on the memory task, where the LSTM and IRNN appear to fail to learn. While our RNN experiments have demonstrated the utility of using a unitary operator for these tasks, we believe that the role of the nonlinearity in the vanishing and exploding gradient problem must not be discounted. We have shown that a simple scaling factor can help reduce the vanishing gradient problem induced by the choice of nonlinearity. More analysis considering the combination of nonlinearity and transition operator must be performed to better tackle this problem. The success of our parametrization for unitary operator learning suggests that the approach of performing gradient updates in the Lie algebra is particularly effective. As Lie groups describe many naturally-occuring symmetries, the Lie group-Lie algebra correspondence could be rich for further exploitation to enhance performance in tasks beyond our initial motivation of recurrent neural networks. Appendix A: Derivation of derivative of the matrix exponential\n\nThis derivation draws elements from ?",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-9": "(?) and ? (?). We have , and seek . For what follows, we simply require that be normal, so the results are more general than the unitary case. In this case, is skew-Hermitian, which is normal and therefore diagonalisable by unitary matrices. Thus, there exist and such that , and therefore\n\nU = W \u200b D ~ \u200b W \u2020 \ud835\udc48 \ud835\udc4a ~ \ud835\udc37 superscript \ud835\udc4a \u2020 U=W\\tilde{D}W^{\\dagger} (13)\n\nwhere . We assume we can calculate: , , and and seek an expression for . Then using 13:\n\nd \u200b U = d \u200b ( W \u200b D ~ \u200b W \u2020 ) = d \u200b W \u200b D ~ \u200b W \u2020 + W \u200b d \u200b D ~ \u200b W \u2020 + W \u200b D ~ \u200b d \u200b W \u2020 \ud835\udc51 \ud835\udc48 \ud835\udc51 \ud835\udc4a ~ \ud835\udc37 superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a ~ \ud835\udc37 superscript \ud835\udc4a \u2020 \ud835\udc4a \ud835\udc51 ~ \ud835\udc37 superscript \ud835\udc4a \u2020 \ud835\udc4a ~ \ud835\udc37 \ud835\udc51 superscript \ud835\udc4a \u2020 \\begin{split}dU&=d(W\\tilde{D}W^{\\dagger})\\\\\n&=dW\\tilde{D}W^{\\dagger}+Wd\\tilde{D}W^{\\dagger}+W\\tilde{D}dW^{\\dagger}\\end{split} (14)\n\nPre-multiplying with and post-multiplying with :\n\nW \u2020 \u200b d \u200b U \u200b W = W \u2020 \u200b d \u200b W \u200b D ~ + d \u200b D ~ + D ~ \u200b d \u200b W \u2020 \u200b W superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc48 \ud835\udc4a superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a ~ \ud835\udc37 \ud835\udc51 ~ \ud835\udc37 ~ \ud835\udc37 \ud835\udc51 superscript \ud835\udc4a \u2020 \ud835\udc4a W^{\\dagger}dUW=W^{\\dagger}dW\\tilde{D}+d\\tilde{D}+\\tilde{D}dW^{\\dagger}W (15)\n\nThe last term can be simplified by differentiating both sides of (this follows from unitarity of );\n\nW \u2020 \u200b W + W \u2020 \u200b d \u200b W = 0 \u21d2 d \u200b W \u2020 \u200b W = \u2212 W \u2020 \u200b d \u200b W superscript \ud835\udc4a \u2020 \ud835\udc4a superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a 0 \u21d2 \ud835\udc51 superscript \ud835\udc4a \u2020 \ud835\udc4a superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a W^{\\dagger}W+W^{\\dagger}dW=0\\Rightarrow dW^{\\dagger}W=-W^{\\dagger}dW (16)\n\nand substituting back into 15 to get:\n\nW \u2020 \u200b d \u200b U \u200b W = W \u2020 \u200b d \u200b W \u200b D ~ \u2212 D ~ \u200b W \u2020 \u200b d \u200b W + d \u200b D ~ superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc48 \ud835\udc4a superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a ~ \ud835\udc37 ~ \ud835\udc37 superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a \ud835\udc51 ~ \ud835\udc37 W^{\\dagger}dUW=W^{\\dagger}dW\\tilde{D}-\\tilde{D}W^{\\dagger}dW+d\\tilde{D} (17)\n\nWe can then say that where\n\nV = W \u2020 \u200b d \u200b W \u200b D ~ \u2212 D ~ \u200b W \u2020 \u200b d \u200b W + d \u200b D ~ \ud835\udc49 superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a ~ \ud835\udc37 ~ \ud835\udc37 superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a \ud835\udc51 ~ \ud835\udc37 V=W^{\\dagger}dW\\tilde{D}-\\tilde{D}W^{\\dagger}dW+d\\tilde{D} (18)\n\nSimilarly, where (replacing with )\n\nA = W \u2020 \u200b d \u200b W \u200b D \u2212 D \u200b W \u2020 \u200b d \u200b W + d \u200b D \ud835\udc34 superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a \ud835\udc37 \ud835\udc37 superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a \ud835\udc51 \ud835\udc37 A=W^{\\dagger}dWD-DW^{\\dagger}dW+dD (19)\n\nand also . Calculating\n\nWe use the convention that repeated indices denote summation over that index, unless otherwise stated. Looking at the components of ;\n\nV i \u200b j = ( W \u2020 \u200b d \u200b W \u200b D ~ ) i \u200b j \u2212 ( D ~ \u200b W \u2020 \u200b d \u200b W ) i \u200b j + d \u200b D ~ i \u200b j subscript \ud835\udc49 \ud835\udc56 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a ~ \ud835\udc37 \ud835\udc56 \ud835\udc57 subscript ~ \ud835\udc37 superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a \ud835\udc56 \ud835\udc57 \ud835\udc51 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc57 V_{ij}=(W^{\\dagger}dW\\tilde{D})_{ij}-(\\tilde{D}W^{\\dagger}dW)_{ij}+d\\tilde{D}_{ij} (20)\n\nDiagonal case (): (no summation over )\n\nV i \u200b i = W i \u200b a \u2020 \u200b d \u200b W a \u200b b \u200b D ~ b \u200b i \u2212 D ~ i \u200b a \u200b W a \u200b b \u2020 \u200b d \u200b W b \u200b i + d \u200b D ~ i \u200b i subscript \ud835\udc49 \ud835\udc56 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc4e \ud835\udc51 subscript \ud835\udc4a \ud835\udc4e \ud835\udc4f subscript ~ \ud835\udc37 \ud835\udc4f \ud835\udc56 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc4e subscript superscript \ud835\udc4a \u2020 \ud835\udc4e \ud835\udc4f \ud835\udc51 subscript \ud835\udc4a \ud835\udc4f \ud835\udc56 \ud835\udc51 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc56 V_{ii}=W^{\\dagger}_{ia}dW_{ab}\\tilde{D}_{bi}-\\tilde{D}_{ia}W^{\\dagger}_{ab}dW_{bi}+d\\tilde{D}_{ii} (21)\n\nSince , the first two terms cancel:\n\nV i \u200b i = W i \u200b a \u2020 \u200b d \u200b W a \u200b b \u200b \u03b4 b \u200b i \u200b d ~ i \u2212 \u03b4 a \u200b i \u200b d ~ i \u200b W a \u200b b \u2020 \u200b d \u200b W b \u200b i + d \u200b D ~ i \u200b i = W i \u200b a \u2020 \u200b d \u200b W a \u200b i \u200b d ~ i \u2212 d ~ i \u200b W i \u200b b \u2020 \u200b d \u200b W b \u200b i + d \u200b D ~ i \u200b i = d \u200b D ~ i \u200b i subscript \ud835\udc49 \ud835\udc56 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc4e \ud835\udc51 subscript \ud835\udc4a \ud835\udc4e \ud835\udc4f subscript \ud835\udeff \ud835\udc4f \ud835\udc56 subscript ~ \ud835\udc51 \ud835\udc56 subscript \ud835\udeff \ud835\udc4e \ud835\udc56 subscript ~ \ud835\udc51 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc4e \ud835\udc4f \ud835\udc51 subscript \ud835\udc4a \ud835\udc4f \ud835\udc56 \ud835\udc51 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc4e \ud835\udc51 subscript \ud835\udc4a \ud835\udc4e \ud835\udc56 subscript ~ \ud835\udc51 \ud835\udc56 subscript ~ \ud835\udc51 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc4f \ud835\udc51 subscript \ud835\udc4a \ud835\udc4f \ud835\udc56 \ud835\udc51 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc56 \ud835\udc51 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc56 \\begin{split}V_{ii}=&W^{\\dagger}_{ia}dW_{ab}\\delta_{bi}\\tilde{d}_{i}-\\delta_{ai}\\tilde{d}_{i}W^{\\dagger}_{ab}dW_{bi}+d\\tilde{D}_{ii}\\\\\n=&W^{\\dagger}_{ia}dW_{ai}\\tilde{d}_{i}-\\tilde{d}_{i}W^{\\dagger}_{ib}dW_{bi}+d\\tilde{D}_{ii}\\\\\n=&d\\tilde{D}_{ii}\\end{split} (22)\n\nUsing 19 we get\n\nRecall that the diagonal elements of are the exponentiated versions of the diagonal elements of , so . Then\n\nd \u200b D ~ i \u200b i = d \u200b ( d i ) \u200b e d i = d \u200b D i \u200b i \u200b D ~ i \u200b i \ud835\udc51 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc56 \ud835\udc51 subscript \ud835\udc51 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udc51 \ud835\udc56 \ud835\udc51 subscript \ud835\udc37 \ud835\udc56 \ud835\udc56 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc56 d\\tilde{D}_{ii}=d(d_{i})e^{d_{i}}=dD_{ii}\\tilde{D}_{ii} (23)\n\nInserting that into Equation 22:\n\nV i \u200b i = d \u200b D i \u200b i \u200b D ~ i \u200b i = ( W \u2020 \u200b d \u200b L \u200b W ) i \u200b i \u200b D ~ i \u200b i = ( W \u2020 \u200b d \u200b L \u200b W ) i \u200b i \u200b e d i subscript \ud835\udc49 \ud835\udc56 \ud835\udc56 \ud835\udc51 subscript \ud835\udc37 \ud835\udc56 \ud835\udc56 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc3f \ud835\udc4a \ud835\udc56 \ud835\udc56 subscript ~ \ud835\udc37 \ud835\udc56 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc3f \ud835\udc4a \ud835\udc56 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udc51 \ud835\udc56 V_{ii}=dD_{ii}\\tilde{D}_{ii}=(W^{\\dagger}dLW)_{ii}\\tilde{D}_{ii}=(W^{\\dagger}dLW)_{ii}e^{d_{i}} (24)\n\nThis produces Equation 9 in the main paper. Off-diagonal case (): (no summation over )\n\nIn this case, the purely diagonal part vanishes. We get:\n\nV i \u200b j = W i \u200b a \u2020 \u200b d \u200b W a \u200b b \u200b \u03b4 b \u200b j \u200b d ~ j \u2212 \u03b4 a \u200b i \u200b d ~ i \u200b W a \u200b b \u2020 \u200b d \u200b W b \u200b j = W i \u200b a \u2020 \u200b d \u200b W a \u200b j \u200b d ~ j \u2212 W i \u200b b \u2020 \u200b d \u200b W b \u200b j \u200b d ~ i = ( W \u2020 \u200b d \u200b W ) i \u200b j \u200b ( d ~ j \u2212 d ~ i ) subscript \ud835\udc49 \ud835\udc56 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc4e \ud835\udc51 subscript \ud835\udc4a \ud835\udc4e \ud835\udc4f subscript \ud835\udeff \ud835\udc4f \ud835\udc57 subscript ~ \ud835\udc51 \ud835\udc57 subscript \ud835\udeff \ud835\udc4e \ud835\udc56 subscript ~ \ud835\udc51 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc4e \ud835\udc4f \ud835\udc51 subscript \ud835\udc4a \ud835\udc4f \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc4e \ud835\udc51 subscript \ud835\udc4a \ud835\udc4e \ud835\udc57 subscript ~ \ud835\udc51 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc4f \ud835\udc51 subscript \ud835\udc4a \ud835\udc4f \ud835\udc57 subscript ~ \ud835\udc51 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a \ud835\udc56 \ud835\udc57 subscript ~ \ud835\udc51 \ud835\udc57 subscript ~ \ud835\udc51 \ud835\udc56 \\begin{split}V_{ij}=&W^{\\dagger}_{ia}dW_{ab}\\delta_{bj}\\tilde{d}_{j}-\\delta_{ai}\\tilde{d}_{i}W^{\\dagger}_{ab}dW_{bj}\\\\\n=&W^{\\dagger}_{ia}dW_{aj}\\tilde{d}_{j}-W^{\\dagger}_{ib}dW_{bj}\\tilde{d}_{i}\\\\\n&=(W^{\\dagger}dW)_{ij}(\\tilde{d}_{j}-\\tilde{d}_{i})\\end{split} (25)\n\nSimilarly,\n\nA i \u200b j = ( W \u2020 \u200b d \u200b W ) i \u200b j \u200b ( d j \u2212 d i ) subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a \ud835\udc56 \ud835\udc57 subscript \ud835\udc51 \ud835\udc57 subscript \ud835\udc51 \ud835\udc56 A_{ij}=(W^{\\dagger}dW)_{ij}(d_{j}-d_{i}) (26)\n\nRemembering that this is all component-wise multiplication (no summation over and ), we can rearrange expressions to get:\n\n( W \u2020 \u200b d \u200b W ) i \u200b j = A i \u200b j d j \u2212 d i = ( W \u2020 \u200b d \u200b L \u200b W ) i \u200b j d j \u2212 d i subscript superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc4a \ud835\udc56 \ud835\udc57 subscript \ud835\udc34 \ud835\udc56 \ud835\udc57 subscript \ud835\udc51 \ud835\udc57 subscript \ud835\udc51 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc3f \ud835\udc4a \ud835\udc56 \ud835\udc57 subscript \ud835\udc51 \ud835\udc57 subscript \ud835\udc51 \ud835\udc56 (W^{\\dagger}dW)_{ij}=\\frac{A_{ij}}{d_{j}-d_{i}}=\\frac{(W^{\\dagger}dLW)_{ij}}{d_{j}-d_{i}} (27)\n\nCombining this with 25 and remembering , we have, for :\n\nV i \u200b j = ( W \u2020 \u200b d \u200b L \u200b W ) i \u200b j \u200b ( e d i \u2212 e d j d i \u2212 d j ) subscript \ud835\udc49 \ud835\udc56 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc51 \ud835\udc3f \ud835\udc4a \ud835\udc56 \ud835\udc57 superscript \ud835\udc52 subscript \ud835\udc51 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udc51 \ud835\udc57 subscript \ud835\udc51 \ud835\udc56 subscript \ud835\udc51 \ud835\udc57 V_{ij}=(W^{\\dagger}dLW)_{ij}\\left(\\frac{e^{d_{i}}-e^{d_{j}}}{d_{i}-d_{j}}\\right) (28)\n\nThis is Equation 10 in the main paper. Efficiently calculating\n\nThis section is specific to our work, as it relies on the choice of basis for . In our case, is simple. is a linear combination of the parameters ;\n\nL = \u2211 i n 2 \u03bb i \u200b T i \ud835\udc3f superscript subscript \ud835\udc56 superscript \ud835\udc5b 2 subscript \ud835\udf06 \ud835\udc56 subscript \ud835\udc47 \ud835\udc56 L=\\sum_{i}^{n^{2}}\\lambda_{i}T_{i} (29)\n\nWhere are the basis matrices of . Then\n\nd \u200b L a = \u2202 L \u2202 \u03bb a = T a \ud835\udc51 subscript \ud835\udc3f \ud835\udc4e \ud835\udc3f subscript \ud835\udf06 \ud835\udc4e subscript \ud835\udc47 \ud835\udc4e dL_{a}=\\frac{\\partial L}{\\partial\\lambda_{a}}=T_{a} (30)\n\nWe need for all . Since the s are sparse, this is cheaper than performing full matrix multiplications, as we demonstrate now. In components;\n\n( W \u2020 \u200b T a \u200b W ) i \u200b j = W i \u200b k \u2020 \u200b T a k \u200b l \u200b W l \u200b j subscript superscript \ud835\udc4a \u2020 subscript \ud835\udc47 \ud835\udc4e \ud835\udc4a \ud835\udc56 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc58 subscript subscript \ud835\udc47 \ud835\udc4e \ud835\udc58 \ud835\udc59 subscript \ud835\udc4a \ud835\udc59 \ud835\udc57 (W^{\\dagger}T_{a}W)_{ij}=W^{\\dagger}_{ik}{T_{a}}_{kl}W_{lj} (31)\n\nCases:\n\ndiagonal, purely imaginary\n\nis zero except for a in the -th position on the diagonal. ( W \u2020 \u200b T a \u200b W ) i \u200b j = i \u200b W i \u200b a \u2020 \u200b W a \u200b j = i \u200b W a \u200b i \u2217 \u200b W a \u200b j \u21d2 W \u2020 \u200b T a \u200b W = i \u22c5 outer \u200b ( \ud835\udc30 a \u2217 , \ud835\udc30 a ) subscript superscript \ud835\udc4a \u2020 subscript \ud835\udc47 \ud835\udc4e \ud835\udc4a \ud835\udc56 \ud835\udc57 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc4e subscript \ud835\udc4a \ud835\udc4e \ud835\udc57 \ud835\udc56 subscript superscript \ud835\udc4a \ud835\udc4e \ud835\udc56 subscript \ud835\udc4a \ud835\udc4e \ud835\udc57 \u21d2 superscript \ud835\udc4a \u2020 subscript \ud835\udc47 \ud835\udc4e \ud835\udc4a \u22c5 \ud835\udc56 outer subscript superscript \ud835\udc30 \ud835\udc4e subscript \ud835\udc30 \ud835\udc4e \\begin{split}(W^{\\dagger}T_{a}W)_{ij}&=iW^{\\dagger}_{ia}W_{aj}=iW^{*}_{ai}W_{aj}\\\\\n\\Rightarrow W^{\\dagger}T_{a}W&=i\\cdot\\mathrm{outer}(\\mathbf{w}^{*}_{a},\\mathbf{w}_{a})\\end{split} (32)\n\nwhere is the -th row of . symmetric, purely imaginary\n\nis zero except for in position and . ( W \u2020 \u200b T r \u200b s \u200b W ) i \u200b j = i \u200b W i \u200b k \u2020 \u200b ( \u03b4 k \u200b s , l \u200b r + \u03b4 k \u200b r , l \u200b s ) \u200b W l \u200b j = i \u200b ( W i \u200b s \u2020 \u200b W r \u200b j + W i \u200b r \u2020 \u200b W s \u200b j ) = i \u200b ( W s \u200b i \u2217 \u200b W r \u200b j + W r \u200b i \u2217 \u200b W s \u200b j ) \u21d2 W \u2020 \u200b T r \u200b s \u200b W = i \u22c5 ( outer \u200b ( \ud835\udc30 s \u2217 , \ud835\udc30 r ) + outer \u200b ( \ud835\udc30 r \u2217 , \ud835\udc30 s ) ) subscript superscript \ud835\udc4a \u2020 subscript \ud835\udc47 \ud835\udc5f \ud835\udc60 \ud835\udc4a \ud835\udc56 \ud835\udc57 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc58 subscript \ud835\udeff \ud835\udc58 \ud835\udc60 \ud835\udc59 \ud835\udc5f subscript \ud835\udeff \ud835\udc58 \ud835\udc5f \ud835\udc59 \ud835\udc60 subscript \ud835\udc4a \ud835\udc59 \ud835\udc57 \ud835\udc56 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc60 subscript \ud835\udc4a \ud835\udc5f \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc5f subscript \ud835\udc4a \ud835\udc60 \ud835\udc57 \ud835\udc56 subscript superscript \ud835\udc4a \ud835\udc60 \ud835\udc56 subscript \ud835\udc4a \ud835\udc5f \ud835\udc57 subscript superscript \ud835\udc4a \ud835\udc5f \ud835\udc56 subscript \ud835\udc4a \ud835\udc60 \ud835\udc57 \u21d2 superscript \ud835\udc4a \u2020 subscript \ud835\udc47 \ud835\udc5f \ud835\udc60 \ud835\udc4a \u22c5 \ud835\udc56 outer subscript superscript \ud835\udc30 \ud835\udc60 subscript \ud835\udc30 \ud835\udc5f outer subscript superscript \ud835\udc30 \ud835\udc5f subscript \ud835\udc30 \ud835\udc60 \\begin{split}(W^{\\dagger}T_{rs}W)_{ij}&=iW^{\\dagger}_{ik}(\\delta_{ks,lr}+\\delta_{kr,ls})W_{lj}\\\\\n&=i(W^{\\dagger}_{is}W_{rj}+W^{\\dagger}_{ir}W_{sj})=i(W^{*}_{si}W_{rj}+W^{*}_{ri}W_{sj})\\\\\n\\Rightarrow W^{\\dagger}T_{rs}W&=i\\cdot(\\mathrm{outer}(\\mathbf{w}^{*}_{s},\\mathbf{w}_{r})+\\mathrm{outer}(\\mathbf{w}^{*}_{r},\\mathbf{w}_{s}))\\end{split} (33)\n\nantisymmetric, purely real\n\nis zero except for in position and in position . ( W \u2020 \u200b T r \u200b s \u200b W ) i \u200b j = W i \u200b k \u2020 \u200b ( \u03b4 k \u200b r , s \u200b l \u2212 \u03b4 k \u200b s , r \u200b l ) \u200b W l \u200b j = W i \u200b r \u2020 \u200b W s \u200b j \u2212 W i \u200b s \u2020 \u200b W r \u200b j = W r \u200b i \u2217 \u200b W s \u200b j \u2212 W s \u200b i \u2217 \u200b W r \u200b j \u21d2 W \u2020 \u200b T r \u200b s \u200b W = outer \u200b ( \ud835\udc30 r \u2217 , \ud835\udc30 s ) \u2212 outer \u200b ( \ud835\udc30 s \u2217 , \ud835\udc30 r ) subscript superscript \ud835\udc4a \u2020 subscript \ud835\udc47 \ud835\udc5f \ud835\udc60 \ud835\udc4a \ud835\udc56 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc58 subscript \ud835\udeff \ud835\udc58 \ud835\udc5f \ud835\udc60 \ud835\udc59 subscript \ud835\udeff \ud835\udc58 \ud835\udc60 \ud835\udc5f \ud835\udc59 subscript \ud835\udc4a \ud835\udc59 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc5f subscript \ud835\udc4a \ud835\udc60 \ud835\udc57 subscript superscript \ud835\udc4a \u2020 \ud835\udc56 \ud835\udc60 subscript \ud835\udc4a \ud835\udc5f \ud835\udc57 subscript superscript \ud835\udc4a \ud835\udc5f \ud835\udc56 subscript \ud835\udc4a \ud835\udc60 \ud835\udc57 subscript superscript \ud835\udc4a \ud835\udc60 \ud835\udc56 subscript \ud835\udc4a \ud835\udc5f \ud835\udc57 \u21d2 superscript \ud835\udc4a \u2020 subscript \ud835\udc47 \ud835\udc5f \ud835\udc60 \ud835\udc4a outer subscript superscript \ud835\udc30 \ud835\udc5f subscript \ud835\udc30 \ud835\udc60 outer subscript superscript \ud835\udc30 \ud835\udc60 subscript \ud835\udc30 \ud835\udc5f \\begin{split}(W^{\\dagger}T_{rs}W)_{ij}&=W^{\\dagger}_{ik}(\\delta_{kr,sl}-\\delta_{ks,rl})W_{lj}\\\\\n&=W^{\\dagger}_{ir}W_{sj}-W^{\\dagger}_{is}W_{rj}=W^{*}_{ri}W_{sj}-W^{*}_{si}W_{rj}\\\\\n\\Rightarrow W^{\\dagger}T_{rs}W&=\\mathrm{outer}(\\mathbf{w}^{*}_{r},\\mathbf{w}_{s})-\\mathrm{outer}(\\mathbf{w}^{*}_{s},\\mathbf{w}_{r})\\end{split} (34)\n\nThese reproduce the expressions in the main paper. The outer product of two -dimensional vectors is an operation, and so this provides a (up to) factor speed-up on matrix multiplication. Appendix B: Changing the basis of\n\nThe Lie group parametrization assumes a fixed basis of . Our intuition is that this makes some regions of more \u2018accessible\u2019 to the optimization procedure, elements whose coefficients are small given this basis. Learning a matrix which came from elsewhere in may therefore be more challenging. We emulated this \u2018change of basis\u2018 without needing to explicitly construct a new basis by generating a change of basis matrix, . That is, if is the -th element of the new basis, it is given by\n\nV j = \u2211 k M j \u200b k \u200b T k subscript \ud835\udc49 \ud835\udc57 subscript \ud835\udc58 subscript \ud835\udc40 \ud835\udc57 \ud835\udc58 subscript \ud835\udc47 \ud835\udc58 V_{j}=\\sum_{k}M_{jk}T_{k} (35)\n\nIf are the coefficients of relative to the basis , the coefficients relative to the old basis are given by:\n\n\u03bb b = \u2211 k \u03bb ~ k \u200b M k \u200b b = \u03bb ~ T \u22c5 M subscript \ud835\udf06 \ud835\udc4f subscript \ud835\udc58 subscript ~ \ud835\udf06 \ud835\udc58 subscript \ud835\udc40 \ud835\udc58 \ud835\udc4f \u22c5 superscript ~ \ud835\udf06 \ud835\udc47 \ud835\udc40 \\lambda_{b}=\\sum_{k}\\tilde{\\lambda}_{k}M_{kb}=\\tilde{\\lambda}^{T}\\cdot M (36)\n\nA change of basis matrix must be full-rank. We generate one by sampling a square, matrix from a continuous uniform distribution ( is a constant we vary in experiments, see Figure 3). This is very unlikely to be singular. We choose the range of the distribution such that will have \u2018large\u2019 values relative to the true matrix , whose parameters (relative to ) are drawn from . Preliminary experiments suggested that the learning rate must be adjusted to compensate for the change of scale - evidence for this is visible in the first column of Figure 3, where changing the basis without changing the learning rate results in an unstable validation set trace. Poor performance resulting from an inappropriate learning rate is not our focus here, so we performed experiments for different values of the learning rate. Figure 3 shows a grid of validation set losses as we vary the learning rate (columns) and the value of (rows). Our intuition is that if the performance under the change of basis is purely driven by the difference in scale, using an appropriately-scaled learning rate should negate its affect. Each parameter is scaled by a variable uniformly distributed between . The expectation value of the absolute value of this quantity is , so we consider learning rates normalised by this factor. As seen in Figure 3, the graphs on the diagonal are not identical, suggesting that merely scaling the learning rate does not account for the change of learning behavior given a new basis - at least in expectation. Nonetheless, it is reassuring to observe that for all choices of explored, there exists a learning rate which facilitates learning, even if it markedly slower than the \u2018ideal\u2019 case. While having a \u2018misspecified\u2019 basis does appear to negatively impact learning, it can be largely overcome with choice of learning rate. References\n\n[2009] Al-Mohy, A. H., and Higham, N. J. 2009. A new scaling and squaring algorithm for the matrix exponential. SIAM Journal on Matrix Analysis and Applications 31(3):970\u2013989. [2016] Arjovsky, M.; Shah, A.; and Bengio, Y. 2016. Unitary evolution recurrent neural networks. International Conference on Learning Representations (ICLR) - Workshop track. [2014] Cohen, T., and Welling, M. 2014. Learning the irreducible representations of commutative lie groups. International Conference on Machine Learning (ICML). [2016] Hazan, E.; Kale, S.; and Warmuth, M. K. 2016. Learning rotations with little regret. Machine Learning 1\u201320. [2016] Henaff, M.; Szlam, A.; and LeCun, Y. 2016. Orthogonal rnns and long-memory tasks. arXiv preprint arXiv:1602.06662. [1997] Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural computation 9(8):1735\u20131780. [1991] Hochreiter, S. 1991. Untersuchungen zu dynamischen neuronalen netzen. Master\u2019s thesis, Institut fur Informatik, Technische Universitat, Munchen. [1976] Jennrich, R. I., and Bright, P.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-10": "B. 1976. Fitting systems of linear differential equations using computer generated exact derivatives. Technometrics 18(4):385\u2013392. [1985] Kalbfleisch, J., and Lawless, J. F. 1985. The analysis of panel data under a markov assumption. Journal of the American Statistical Association 80(392):863\u2013871. [1975] Keller, J.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-11": "B. 1975. Closest unitary, orthogonal and hermitian operators to a given operator.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-12": "Mathematics Magazine 48(4):192\u2013197. [2016] Krueger, D., and Memisevic, R. 2016. Regularizing rnns by stabilizing activations. International Conference on Learning Representations (ICLR). [2015] Le, Q. V.; Jaitly, N.; and Hinton, G.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-13": "E. 2015. A simple way to initialize recurrent networks of rectified linear units.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-14": "arXiv preprint arXiv:1504.00941. [2007] Miao, X., and Rao, R. P. 2007. Learning the lie groups of visual invariance. Neural computation 19(10):2665\u20132693. [2015] Mikolov, T.; Joulin, A.; Chopra, S.; Mathieu, M.; and Ranzato, M. 2015. Learning longer memory in recurrent neural networks. International Conference on Learning Representations (ICLR) - Workshop track. [2014] Saxe, A. M.; McClelland, J. L.; and Ganguli, S. 2014. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. International Conference on Learning Representations (ICLR).",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-15": "[1980] Stewart, G. 1980. The efficient generation of random orthogonal matrices with an application to condition estimators. SIAM Journal on Numerical Analysis 17(3):403\u2013409. [2011] Tao, T. 2011. Two small facts about lie groups. https://terrytao.wordpress.com/2011/06/25/two-small-facts-about-lie-groups/.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-16": "[2012] Tieleman, T., and Hinton, G. 2012. Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-17": "COURSERA: Neural Networks for Machine Learning.",
    "00c04118a0d0f1ff855ad4597025448c5daf873d-18": "[2008] Tuzel, O.; Porikli, F.; and Meer, P. 2008. Learning on lie groups for invariant detection and tracking. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 1\u20138. IEEE. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Fri Mar 15 05:20:56 2024 by LaTeXML"
}