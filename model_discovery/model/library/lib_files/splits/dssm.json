{
    "dssm-0": "# Diagonal State Spaces are as Effective as Structured State Spaces \n\nAnkit Gupta*<br>IBM Research<br>ankitgupta.iitkanpur@gmail.com\n\nAlbert Gu<br>Stanford University<br>albertgu@stanford.edu\n\nJonathan Berant<br>Tel Aviv University<br>joberant@cs.tau.ac.il\n\n\n#### Abstract\n\nModeling long range dependencies in sequential data is a fundamental step towards attaining human-level performance in many modalities such as text, vision, audio and video.",
    "dssm-1": "While attention-based models are a popular and effective choice in modeling short-range interactions, their performance on tasks requiring long range reasoning has been largely inadequate. In an exciting result, Gu et al. [GGR22] proposed the Structured State Space (S4) architecture delivering large gains over state-of-the-art models on several long-range tasks across various modalities. The core proposition of S4 is the parameterization of state matrices via a diagonal plus low rank structure, allowing efficient computation. In this work, we show that one can match the performance of S4 even without the low rank correction and thus assuming the state matrices to be diagonal. Our Diagonal State Space (DSS) model matches the performance of S4 on Long Range Arena tasks, speech classification on Speech Commands dataset, while being conceptually simpler and straightforward to implement. ## 1 Introduction\n\nThe Transformer architecture [ $\\mathrm{VSP}^{+}$17] has been successful across many areas of machine learning. Transformers pre-trained on large amounts of unlabelled text via a denoising objective have become the standard in natural language processing, exhibiting impressive amounts of linguistic and world knowledge $\\left[\\mathrm{BMR}^{+} 20, \\mathrm{BMH}^{+} 21\\right]$. This recipe has also led to remarkable developments in the areas of vision $\\left[\\mathrm{RPG}^{+} 21, \\mathrm{RKH}^{+} 21\\right]$ and speech [DXX18, BZMA20]. The contextualizing component of the Transformer is the multi-head attention layer which, for inputs of length $L$, has an expensive $\\Omega\\left(L^{2}\\right)$ complexity. This becomes prohibitive on tasks where the model is required to capture long-range interactions of various parts of a long input. To alleviate this issue, several Transformer variants have been proposed with reduced compute and memory requirements [QML ${ }^{+}$20, KKL20, RSVG21, BPC20, GB20, KVPF20, WLK ${ }^{+}$20, $\\mathrm{CLD}^{+}$21, ZS21, GDG $\\left.{ }^{+} 21\\right]$ (cf. [TDBM20] for a survey). Despite this effort, all these models have reported inadequate performance on benchmarks created to formally evaluate and quantify a model's ability to perform long-range reasoning (such as Long Range Arena $\\left[\\mathrm{TDA}^{+} 21\\right]$ and SCROLLS $\\left[\\mathrm{SSI}^{+} 22\\right]$ ). In a recent breakthrough result, Gu et al. [GGR22] proposed S 4 , a sequence-to-sequence model that uses linear state spaces for contextualization instead of attention. It has shown remarkable performance on tasks requiring long-range reasoning in domains such as text, images and audio. For instance, on Long Range Arena it advances the state-of-the-art by 19 accuracy points over the best performing Transformer variant. Its remarkable abilities are not limited to text and images and carry over to tasks such as time-series forecasting, speech recognition and audio generation [GGDR22]. Despite S4's achievements, its design is complex and is centered around the HiPPO theory, which is a mathematical framework for long-range modeling [VKE19, $\\mathrm{GDE}^{+} 20, \\mathrm{GJG}^{+}$21]. [GGR22]\n\n[^0]showed that state space models with various alternative initializations perform poorly in comparison to initializing the state-space parameters with a particular HiPPO matrix. In order to leverage this matrix, they parameterize the learned state spaces using a Diagonal Plus Low Rank (DLPR) structure and, as a result, need to employ several reduction steps and linear algebraic techniques to be able to compute the state space output efficiently, making S4 difficult to understand, implement and analyze. In this work, we show that it is possible to match S4's performance while using a much simpler, fullydiagonal parameterization of state spaces. While we confirm that random diagonal state spaces are less effective, we observe that there do in fact exist effective diagonal state matrices: simply removing the low-rank component of the DPLR HiPPO matrix still preserves its performance. Leveraging this idea, our proposed Diagonal State Space (DSS) model enforces state matrices to be diagonal, making it significantly simpler to formulate, implement and analyze, while being provably as expressive as general state spaces. In contrast to S4, DSS does not assume any specialized background beyond basic linear algebra and can be implemented in just a few lines of code. Our implementation fits in a single page and is provided in $\\S$ A. 5 (Figure 6). We evaluate the performance of DSS on Long Range Arena (LRA) which is a suite of sequencelevel classification tasks with diverse input lengths ( $1 K-16 K)$ requiring similarity, structural, and visual-spatial reasoning over a wide range of modalities such as text, natural/synthetic images, and mathematical expressions. Despite its simplicity, DSS delivers an average accuracy of 81.88 across the 6 tasks of LRA, comparable to the state-of-the-art performance of S4 (80.21). In addition, DSS maintains a comfortable 20 point lead over the best Transformer variant ( 81.88 vs 61.41 ). In the audio domain, we evaluate the performance of DSS on raw speech classification. On the Speech Commands dataset [War18], which consists of raw audio samples of length $16 K$, we again found the performance of DSS to be comparable to that of S4 ( 98.2 vs 98.1 ). To summarize, our results demonstrate that DSS is a simple and effective method for modeling long-range interactions in modalities such as text, images and audio. We believe that the effectiveness, efficiency and transparency of DSS can significantly contribute to the adoption of state space models over their attention-based peers. Our code is available at https://github.com/ag1988/dss. ## 2 Background\n\nWe start by reviewing the basics of time-invariant linear state spaces. State Spaces A continuous-time state space model (SSM) parameterized by the state matrix $A \\in \\mathbb{R}^{N \\times N}$ and vectors $B \\in \\mathbb{R}^{N \\times 1}, C \\in \\mathbb{R}^{1 \\times N}$ is given by the differential equation:\n\n$$\n\\frac{d x}{d t}(t)=A x(t)+B u(t), y(t)=C x(t)\n$$\n\nwhich defines a function-to-function map $u(t) \\mapsto y(t)$. For a given value of time $t \\in \\mathbb{R}, u(t) \\in \\mathbb{R}$ denotes the value of the input signal $u, x(t) \\in \\mathbb{R}^{N \\times 1}$ denotes the state vector and $y(t) \\in \\mathbb{R}$ denotes the value of the output signal $y$. We call a state space diagonal if it has a diagonal state matrix. Discretization For a given sample time $\\Delta \\in \\mathbb{R}_{>0}$, the discretization of a continuous state space (Equation 1) assuming zero-order hold ${ }^{2}$ on $u$ is defined as a sequence-to-sequence map from $\\left(u_{0}, \\ldots, u_{L-1}\\right)=u \\in \\mathbb{R}^{L}$ to $\\left(y_{0}, \\ldots, y_{L-1}\\right)=y \\in \\mathbb{R}^{L}$ via the recurrence,\n\n$$\n\\begin{aligned}\n& x_{k}=\\bar{A} x_{k-1}+\\bar{B} u_{k} \\quad, \\quad y_{k}=\\bar{C} x_{k} \\\\\n& \\bar{A}=e^{A \\Delta}, \\bar{B}=(\\bar{A}-I) A^{-1} B, \\bar{C}=C\n\\end{aligned}\n$$\n\nAssuming $x_{-1}=0$ for simplicity, this recurrence can be explicitly unrolled as\n\n$$\ny_{k}=\\sum_{j=0}^{k} \\overline{C A}^{j} \\bar{B} \\cdot u_{k-j}\n$$\n\nFor convenience, the scalars $\\overline{C A}^{k} \\bar{B}$ are gathered to define the SSM kernel $\\bar{K} \\in \\mathbb{R}^{L}$ as\n\n$$\n\\bar{K}=\\left(\\overline{C B}, \\overline{C A B}, \\ldots, \\overline{C A}^{L-1} \\bar{B}\\right)=\\left(C e^{A \\cdot k \\Delta}\\left(e^{A \\Delta}-I\\right) A^{-1} B\\right)_{0 \\leqslant k<L}\n$$\n\n[^1]where the last equality follows by substituting the values of $\\bar{A}, \\bar{B}, \\bar{C}$ from Equation 2 .",
    "dssm-2": "Hence,\n$$\ny_{k}=\\sum_{j=0}^{k} \\bar{K}_{j} \\cdot u_{k-j}\n$$\n\nGiven an input sequence $u \\in \\mathbb{R}^{L}$, it is possible to compute the output $y \\in \\mathbb{R}^{L}$ sequentially via the recurrence in Equation 2. Unfortunately, sequential computation is prohibitively slow on long inputs and, instead, Equation 5 can be used to compute all elements of $y$ in parallel, provided we have already computed $\\bar{K}$. Computing $y$ from $u$ and $\\bar{K}$ is easy. Given an input sequence $u \\in \\mathbb{R}^{L}$ and the SSM kernel $\\bar{K} \\in \\mathbb{R}^{L}$, naively using Equation 5 for computing $y$ would require $O\\left(L^{2}\\right)$ multiplications. Fortunately, this can be done much more efficiently by observing that for the univariate polynomials\n\n$$\n\\bar{K}(z)=\\sum_{i=0}^{L-1} \\bar{K}_{i} z^{i} \\text { and } u(z)=\\sum_{i=0}^{L-1} u_{i} z^{i}\n$$\n\n$y_{k}$ is the coefficient of $z^{k}$ in the polynomial $\\bar{K}(z) \\cdot u(z)$, i.e. all $y_{k}$ 's can be computed simultaneously by multiplying two degree $L-1$ polynomials. It is well-known that this can be done in $O(L \\log (L))$ time via Fast Fourier Transform (FFT) [CLRS09]. We denote this fast computation of Equation 5 via the discrete convolution as\n\n$$\ny=\\bar{K} * u\n$$\n\nHence, given the SSM kernel $\\bar{K} \\in \\mathbb{R}^{L}$, the output of a discretized state space can be computed efficiently from the input. The challenging part is computing $\\bar{K}$ itself as it involves computing $L$ distinct matrix powers (Equation 4). Instead of directly using $A, B, C$, our idea is to use an alternate parameterization of state spaces for which it would be easier to compute $\\bar{K}$. ## 3 Method\n\nHaving stated the necessary background, we now turn to the main contribution of our work. ### 3.1 Diagonal State Spaces\n\nOur model is based on the following proposition which asserts that, under mild technical assumptions, diagonal state spaces are as expressive as general state spaces. We use the operator $\\bar{K}_{\\Delta, L}(A, B, C) \\in$ $\\mathbb{R}^{1 \\times L}$ to denote the kernel (Equation 4) of length $L$ for state space $(A, B, C)$ and sample time $\\Delta$. Proposition 1. Let $K \\in \\mathbb{R}^{1 \\times L}$ be the kernel of length $L$ of a given state space $(A, B, C)$ and sample time $\\Delta>0$, where $A \\in \\mathbb{C}^{N \\times N}$ is diagonalizable over $\\mathbb{C}$ with eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{N}$ and $\\forall i, \\lambda_{i} \\neq 0$ and $e^{L \\lambda_{i} \\Delta} \\neq 1$. Let $P \\in \\mathbb{C}^{N \\times L}$ be $P_{i, k}=\\lambda_{i} k \\Delta$ and $\\Lambda$ be the diagonal matrix with $\\lambda_{1}, \\ldots, \\lambda_{N}$. Then there exist $\\widetilde{w}, w \\in \\mathbb{C}^{1 \\times N}$ such that\n(a) $K=\\bar{K}_{\\Delta, L}\\left(\\Lambda,(1)_{1 \\leqslant i \\leqslant N}, \\widetilde{w}\\right)=\\widetilde{w} \\cdot \\Lambda^{-1}\\left(e^{\\Lambda \\Delta}-I\\right) \\cdot \\operatorname{elementwise-exp}(P)$,\n(b) $K=\\bar{K}_{\\Delta, L}\\left(\\Lambda,\\left(\\left(e^{L \\lambda_{i} \\Delta}-1\\right)^{-1}\\right)_{1 \\leqslant i \\leqslant N}, w\\right)=w \\cdot \\Lambda^{-1} \\cdot \\operatorname{row}-\\operatorname{softmax}(P)$. The proof of Proposition 1 is elementary and is provided in $\\S$ A.1. In the above equations, the last equality follows by using Equation 4 to explicitly compute the expression for the kernel of the corresponding diagonal state space. Hence, for any given state space with a well-behaved state matrix there exists a diagonal state space computing the same kernel. ${ }^{3}$ More importantly, the expressions for the kernels of the said diagonal state spaces no longer involve matrix powers but only a structured matrix-vector product. Proposition 1(a) suggests that we can parameterize state spaces via $\\Lambda, \\widetilde{w} \\in \\mathbb{C}^{N}$ and simply compute the kernel as shown in Proposition 1(a). Unfortunately, in practice, the real part of the elements of $\\Lambda$ can become positive during training making the training unstable on long inputs. This is because the matrix elementwise- $\\exp (P)$ contains terms as large as $\\exp \\left(\\lambda_{i} \\Delta(L-1)\\right)$ which even for a modest value of $L$ can be very large. To address this, we propose two methods to model diagonal state spaces. [^2]```\nInput: parameters \\(\\Lambda_{\\mathrm{re}}, \\Lambda_{\\mathrm{im}} \\in \\mathbb{R}^{N}, w \\in \\mathbb{C}^{N}\\), sample time \\(\\Delta_{\\log } \\in \\mathbb{R}\\)\nOutput: SSM kernel \\(\\bar{K} \\in \\mathbb{R}^{L}\\) (Proposition 1(b))\n    \\(: \\Lambda \\leftarrow \\Lambda_{\\mathrm{re}}+i \\cdot \\Lambda_{\\mathrm{im}}, \\Delta \\leftarrow \\exp \\left(\\Delta_{\\mathrm{log}}\\right) \\quad \\triangleright \\Delta\\) is positive real\n\\(P_{N \\times L} \\leftarrow(\\Delta * \\Lambda)_{N \\times 1} *[0,1, \\ldots L-1]_{1 \\times L} \\quad \\triangleright\\) outer product\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-04.jpg?height=42&width=1348&top_left_y=407&top_left_x=402)\n\n```\n\\(\\bar{K} \\leftarrow \\operatorname{Re}\\left((w / \\Lambda)_{1 \\times N} \\cdot S\\right) \\quad \\triangleright\\) real part\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-04.jpg?height=44&width=603&top_left_y=496&top_left_x=758)\n\nDSS $_{\\text {ExP }}$ In this variant, we use Proposition 1(a) to model our state space but restrict the real part of elements of $\\Lambda$ to be negative. $\\operatorname{DSS}_{\\mathrm{EXP}}$ has parameters $\\Lambda_{\\mathrm{re}}, \\Lambda_{\\mathrm{im}} \\in \\mathbb{R}^{N}, \\widetilde{w} \\in \\mathbb{C}^{N}$ and $\\Delta_{\\log } \\in \\mathbb{R} . \\Lambda$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-04.jpg?height=44&width=1387&top_left_y=669&top_left_x=369) $\\exp \\left(\\Delta_{\\log }\\right) \\in \\mathbb{R}_{>0}$ and the kernel is then computed via Proposition 1(a). $\\mathrm{DSS}_{\\text {EXP }}$ provides a remarkably simple computation of state space kernels but restricts the space of the learned $\\Lambda$ (the real part must be negative). It is not clear if such a restriction could be detrimental for some tasks, and we now present an alternate method that provides the simplicity of Proposition 1(a) while being provably as expressive as general state spaces. DSS $_{\\text {Softmax }}$ Instead of restricting the elements of $\\Lambda$, another option for bounding the elements of elementwise- $\\exp (P)$ is to normalize each row by the sum of its elements, which leads us to\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-04.jpg?height=47&width=1387&top_left_y=1034&top_left_x=369) as $\\Lambda_{\\mathrm{re}}+i \\cdot \\Lambda_{\\mathrm{im}}, \\Delta$ is computed as $\\exp \\left(\\Delta_{\\mathrm{log}}\\right) \\in \\mathbb{R}_{>0}$, and the kernel is then computed via Proposition 1(b). A sketch of this computation is presented in Algorithm 1. We note that, unlike over $\\mathbb{R}$, softmax can have singularities over $\\mathbb{C}$ and slight care must be taken while computing it (e.g., softmax $(0, i \\pi)$ is not defined). We instead use a corrected version $\\operatorname{softmax}_{\\epsilon}$ and detail it in \u00a7A.2. ### 3.2 Diagonal State Space (DSS) Layer\n\nWe are now ready to describe the DSS layer. We retain the skeletal structure of S4 and simply replace the parameterization and computation of the SSM kernel by one of the methods described in $\\S 3.1$. Each DSS layer receives a length- $L$ sequence $u$ of $H$-dimensional vectors as input, i.e., $u \\in \\mathbb{R}^{H \\times L}$, and produces an output $y \\in \\mathbb{R}^{H \\times L}$. The parameters of the layer are $\\Lambda_{\\mathrm{re}}, \\Lambda_{\\mathrm{im}} \\in \\mathbb{R}^{N}, \\Delta_{\\mathrm{log}} \\in \\mathbb{R}^{H}$ and $W \\in \\mathbb{C}^{H \\times N}$. For each coordinate $h=1, \\ldots, H$, a state space kernel $\\bar{K}_{h} \\in \\mathbb{R}^{L}$ is computed using one of methods described in $\\S 3.1$. For example, in case of $\\mathrm{DSS}_{\\text {softmax }}$, Algorithm 1 is used with parameters $\\Lambda_{\\mathrm{re}}, \\Lambda_{\\mathrm{im}} \\in \\mathbb{R}^{N}, W_{h} \\in \\mathbb{C}^{N}$ and $\\left(\\Delta_{\\mathrm{log}}\\right)_{h} \\in \\mathbb{R}$. The output $y_{h} \\in \\mathbb{R}^{L}$ for coordinate $h$ is computed from $u_{h} \\in \\mathbb{R}^{L}$ and $\\bar{K}_{h}$ using Equation 6. Following S4, we also add a residual connection from $u$ to $y$ followed by a GELU non-linearity [HG16]. This is followed by a position-wise linear projection $W_{\\text {out }} \\in \\mathbb{R}^{H \\times H}$ to enable information exchange among the $H$ coordinates. The DSS layer can be implemented in just a few lines of code and our PyTorch implementation of $\\mathrm{DSS}_{\\text {Softmax }}$ layer is provided in $\\S \\mathrm{A} .5$ (Figure 6). The implementation of $\\mathrm{DSS}_{\\text {EXP }}$ layer is even simpler and is omitted. Complexity For batch size $B$, sequence length $L$ and hidden size $H$, the DSS layer requires $O(N H L)$ time and space to compute the kernels, $O(B H L \\log (L))$ time for the discrete convolution and $O\\left(B H^{2} L\\right)$ time for the output projection. For small batch size $B$, the time taken to compute the kernels becomes important whereas for large batches more compute is spent on the convolution and the linear projection. The kernel part of DSS layer has $2 N+H+2 H N$ real-valued parameters. ### 3.3 Initialization of DSS layer\n\nThe performance of state spaces models is known to be highly sensitive to initialization [GGR22]. In line with the past work, we found that carefully initializing the parameters of the DSS layer is crucial to obtain state-of-the-art performance (\u00a74). The real and imaginary parts of each element of $W$ are initialized from $\\mathcal{N}(0,1)$. Each element of $\\Delta_{\\log }$ is initialized as $e^{r}$ where $r \\sim \\mathcal{U}(\\log (.001), \\log (.1)) . \\Lambda \\in \\mathbb{C}^{N}$ is initialized using eigenvalues of\nthe normal part of normal plus low-rank form of HiPPO matrix [GGR22]. Concretely, $\\Lambda_{\\mathrm{re}}, \\Lambda_{\\mathrm{im}}$ are initialized such that the resulting $\\Lambda$ is the vector of those $N$ eigenvalues of the following $2 N \\times 2 N$ matrix which have a positive imaginary part. $$\n\\begin{cases}(2 i+1)^{1 / 2}(2 j+1)^{1 / 2} / 2 & i<j \\\\ -1 / 2 & i=j \\\\ -(2 i+1)^{1 / 2}(2 j+1)^{1 / 2} / 2 & i>j\\end{cases}\n$$\n\nHenceforth, we would refer to the above initialization of $\\Lambda$ as Skew-Hippo initialization. In all our experiments, we used the above initialization with $N=64$. The initial learning rate of all DSS parameters was $10^{-3}$ and weight decay was not applied to them. Exceptions to these settings are noted in \u00a7A.3. ### 3.4 States of DSS via the Recurrent View\n\nIn $\\S 3$, we argued that it can be slow to compute the output of state spaces via Equation 2 and instead leveraged Equation 5 for fast computation. But in situations such as autoregressive decoding during inference, it is more efficient to explicitly compute the states of a state space model similar to a linear RNN (Equation 2). We now show how to compute the states of the DSS models described in \u00a73.1. Henceforth, we assume that we have already computed $\\Lambda$ and $\\Delta$. $\\mathbf{D S S}_{\\text {EXP }} \\quad$ As stated in Proposition 1(a), $\\mathrm{DSS}_{\\text {EXP }}$ computes $\\bar{K}_{\\Delta, L}\\left(\\Lambda,(1)_{1 \\leqslant i \\leqslant N}, \\widetilde{w}\\right)$. For this state space and sample time $\\Delta$, we use Equation 2 to obtain its discretization\n\n$$\n\\bar{A}=\\operatorname{diag}\\left(e^{\\lambda_{1} \\Delta}, \\ldots, e^{\\lambda_{N} \\Delta}\\right) \\quad, \\quad \\bar{B}=\\left(\\lambda_{i}^{-1}\\left(e^{\\lambda_{i} \\Delta}-1\\right)\\right)_{1 \\leqslant i \\leqslant N}\n$$\n\nwhere diag creates a diagonal matrix of the scalars. We can now compute the states using the SSM recurrence $x_{k}=\\bar{A} x_{k-1}+\\bar{B} u_{k}$ (Equation 2). As $\\bar{A}$ is diagonal, the $N$ coordinates do not interact and hence can be computed independently. Let us assume $x_{-1}=0$ and say we have already computed $x_{k-1}$. Then, for the $i$ 'th coordinate independently compute\n\n$$\nx_{i, k}=e^{\\lambda_{i} \\Delta} x_{i, k-1}+\\lambda_{i}^{-1}\\left(e^{\\lambda_{i} \\Delta}-1\\right) u_{k}\n$$\n\nNote that in $\\operatorname{DSS}_{\\text {ExP }}, \\operatorname{Re}\\left(\\lambda_{i}\\right)<0$ and hence $\\left|e^{\\lambda_{i} \\Delta}\\right|=\\left|e^{\\operatorname{Re}\\left(\\lambda_{i}\\right) \\Delta}\\right|<1$. Intuitively, if $\\left|\\lambda_{i}\\right| \\Delta \\approx 0$, we would have $x_{i, k} \\approx x_{i, k-1}$ and be able to copy history over many timesteps. On the other hand if $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\Delta \\ll 0$ then $x_{i, k} \\approx-\\lambda_{i}^{-1} u_{k}$ and hence the information from the previous timesteps would be forgotten similar to a \"forget\" gate in LSTMs. DSS $_{\\text {Softmax }}$ As stated in Proposition 1(b), $\\operatorname{DSS}_{\\text {Softmax }}$ computes $\\bar{K}_{\\Delta, L}\\left(\\Lambda, \\quad\\left(\\left(e^{L \\lambda_{i} \\Delta}-\\right.\\right.\\right.$ $\\left.\\left.1)^{-1}\\right)_{1 \\leqslant i \\leqslant N}, w\\right)$. For this state space and sample time $\\Delta$, we obtain the discretization\n\n$$\n\\bar{A}=\\operatorname{diag}\\left(e^{\\lambda_{1} \\Delta}, \\ldots, e^{\\lambda_{N} \\Delta}\\right), \\quad \\bar{B}=\\left(\\frac{e^{\\lambda_{i} \\Delta}-1}{\\lambda_{i}\\left(e^{\\lambda_{i} \\Delta L}-1\\right)}\\right)_{1 \\leqslant i \\leqslant N}\n$$\n\nFor the $i$ 'th coordinate we can independently compute\n\n$$\nx_{i, k}=e^{\\lambda_{i} \\Delta} x_{i, k-1}+\\frac{u_{k}\\left(e^{\\lambda_{i} \\Delta}-1\\right)}{\\lambda_{i}\\left(e^{\\lambda_{i} \\Delta L}-1\\right)}\n$$\n\nLet us drop the coordinate index $i$ for clarity to obtain\n\n$$\nx_{k}=e^{\\lambda \\Delta} x_{k-1}+\\frac{u_{k}\\left(e^{\\lambda \\Delta}-1\\right)}{\\lambda\\left(e^{\\lambda \\Delta L}-1\\right)}\n$$\n\nwhere $x_{k-1}$ is a scalar.",
    "dssm-3": "As the expression involves the term $e^{\\lambda \\Delta L}$, where $L$ can be large, directly computing such terms can result in numerical instability and we must avoid exponentiating scalars with a positive real part. We make two cases depending on the sign of $\\operatorname{Re}(\\lambda)$ and compute $x_{k}$ via an intermediate state $\\widetilde{x}_{k}$ as follows. Let $p=I[\\operatorname{Re}(\\lambda)>0] \\in\\{0,1\\}$. Then,\n\n$$\n\\widetilde{x}_{k}=e^{\\lambda \\Delta(1-p)} \\cdot \\widetilde{x}_{k-1}+e^{-k \\lambda \\Delta p} \\cdot u_{k} \\quad, \\quad x_{k}=\\widetilde{x}_{k} \\cdot \\frac{e^{\\lambda \\Delta p(k-(L-1))}}{\\lambda} \\cdot \\frac{e^{\\lambda \\Delta(1-2 p)}-1}{e^{\\lambda \\Delta(1-2 p) L}-1}\n$$\n\n| MODEL | LISTOPS | TEXT | RETRIEVAL | IMAGE | PATHFINDER | PATH-X | AVG |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| (input length) | 2000 | 2048 | 4000 | 1024 | 1024 | 16384 |  |\n| Transformer | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | $\\mathbf{x}$ | 53.66 |\n| Reformer | 37.27 | 56.10 | 53.40 | 38.07 | 68.50 | $\\mathbf{x}$ | 50.56 |\n| BigBird | 36.05 | 64.02 | 59.29 | 40.83 | 74.87 | $\\mathbf{x}$ | 54.17 |\n| Linear Trans. | 16.13 | 65.90 | 53.09 | 42.34 | 75.30 | $\\mathbf{x}$ | 50.46 |\n| Performer | 18.01 | 65.40 | 53.82 | 42.77 | 77.05 | $\\mathbf{x}$ | 51.18 |\n| FNet | 35.33 | 65.11 | 59.61 | 38.67 | 77.80 | $\\mathbf{x}$ | 54.42 |\n| Nystr\u00f6mformer | 37.15 | 65.52 | 79.56 | 41.58 | 70.94 | $\\mathbf{x}$ | 57.46 |\n| Luna-256 | 37.25 | 64.57 | 79.29 | 47.38 | 77.72 | $\\mathbf{x}$ | 59.37 |\n| H-Transformer-1D | 49.53 | 78.69 | 63.99 | 46.05 | 68.78 | $\\mathbf{x}$ | 61.41 |\n| S4 (as in [GGR22]) | 58.35 | 76.02 | 87.09 | $\\mathbf{8 7 . 2 6}$ | 86.05 | $\\mathbf{8 8 . 1 0}$ | 80.48 |\n| S4 (our run) | 57.6 | 75.4 | 87.6 | 86.5 | $\\mathbf{8 6 . 2}$ | 88.0 | 80.21 |\n| DSS $_{\\text {SoftMax }}($ ours) | $\\mathbf{6 0 .",
    "dssm-4": "6}$ | $\\mathbf{8 4 .",
    "dssm-5": "8}$ | $\\mathbf{8 7 . 8}$ | 85.7 | 84.6 | 87.8 | $\\mathbf{8 1 . 8 8}$ |\n| DSS $_{\\text {ExP }}($ ours) | 59.7 | 84.6 | 87.6 | 84.9 | 84.7 | 85.6 | 81.18 |\n| DSS $_{\\text {ExP-No-scale }}$ (ours) | 59.3 | 82.4 | 86.0 | 81.2 | 81.3 | $\\mathbf{X}$ | 65.03 |\n\nTable 1: (Long Range Arena) Accuracy on the full suite of LRA tasks.",
    "dssm-6": "(Top) Transformer variants reported in LRA, (Middle) other long-range models reported in the literature, (Bottom) state space models. The above equation can be parsed as follows. If $\\operatorname{Re}(\\lambda) \\leqslant 0$ then we have\n\n$$\n\\widetilde{x}_{k}=e^{\\lambda \\Delta} \\cdot \\widetilde{x}_{k-1}+u_{k} \\quad, \\quad x_{k}=\\widetilde{x}_{k} \\cdot \\frac{\\left(e^{\\lambda \\Delta}-1\\right)}{\\lambda\\left(e^{\\lambda \\Delta L}-1\\right)}\n$$\n\nand hence if $\\operatorname{Re}(\\lambda) \\ll 0$ then $\\widetilde{x}_{k} \\approx u_{k}$ and $x_{k} \\approx u_{k} / \\lambda$. In this case, information from the previous timesteps would be ignored and the information used would be local. On the other hand if $\\operatorname{Re}(\\lambda)>0$ then we would have\n\n$$\n\\widetilde{x}_{k}=\\widetilde{x}_{k-1}+e^{-k \\lambda \\Delta} \\cdot u_{k} \\quad, \\quad x_{k}=\\widetilde{x}_{k} \\cdot \\frac{e^{\\lambda \\Delta(k-(L-1))}}{\\lambda} \\cdot \\frac{e^{-\\lambda \\Delta}-1}{e^{-\\lambda \\Delta L}-1}\n$$\n\nand hence if $\\operatorname{Re}(\\lambda) \\gg 0$ then $\\widetilde{x}_{0} \\approx u_{0}$ and $\\widetilde{x}_{k} \\approx \\widetilde{x}_{k-1} \\approx u_{0}, x_{k<L-1} \\approx 0$ and $x_{L-1} \\approx u_{0} / \\lambda$. Hence, the model would be able to copy information even from extremely distant positions, allowing it to capture long-range dependencies. ## 4 Experiments\n\nWe evaluate the performance of DSS on sequence-level classification tasks over text, images, audio. Overall, we find its performance is comparable to S 4 . Long Range Arena (LRA) LRA [TDA $\\left.{ }^{+} 21\\right]$ is a standard benchmark for assessing the ability of models to process long sequences. LRA contains 6 tasks with diverse input lengths $1 K-16 K$, encompassing modalities such as text and images. Several Transformer variants have been benchmarked on LRA but all have underperformed due to factors such as their high compute-memory requirements, implicit locality bias and inability to capture long-range dependencies. Table 1 compares DSS against S 4 , the Transformer variants reported in [TDA ${ }^{+}$21], as well as followup work. State space models (S4, DSS) shown in Table 1 are left-to-right unidirectional whereas other models could be bidirectional. Despite its simplicity, DSS delivers state-of-the-art performance on LRA. Its performance is comparable to that of S4, with a modest improvement in test accuracy averaged across the 6 tasks ( 81.88 vs 80.21). ${ }^{4}$ In addition, DSS maintains a comfortable 20 point lead over the best performing Transformer variant ( 81.88 vs 61.41 ). Interestingly, despite being less expressive than $\\mathrm{DSS}_{\\text {SOFTMAX }}, \\mathrm{DSS}_{\\text {EXP }}$ also reports an impressive performance which makes it specially appealing given its simplicity during training and inference. We investigate an even simpler version of $\\mathrm{DSS}_{\\text {EXP }}$, denoted as $\\mathrm{DSS}_{\\text {EXP-NO-SCALE }}$, which is same\n\n[^3]| MODEL | MFCC | RaW | MODEL | MFCC | RaW |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer | 90.75 | $x$ | WaveGAN-D | $x$ | 96.25 |\n| Performer | 80.85 | 30.77 |  | 93.96 |  |\n| ODE-RNN | 65.9 | $x$ | S4 (as in [GGR22]) | 93.96 | 98.32 |\n| NRDE | 89.8 | 16.49 | S4 (our run) |  |  |\n| ExpRNN | 82.13 | 11.6 | ![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-07.jpg?height=38&width=257&top_left_y=451&top_left_x=1016) |  | 97.7 |\n| LipschitzRNN | 88.38 | $x$ | DSS $_{\\text {EXP }}$ (ours) |  | 98.2 |\n| CKConv | 95.3 | 71.66 | DSS $_{\\text {EXP-No-SCale }}$ (ours) |  | 97.7 |\n\nTable 2: (Speech Commands (SC)) Transformer, CTM, RNN, CNN, and SSM models.",
    "dssm-7": "(MFCC) Standard pre-processed MFCC features (length-161). (Raw) Unprocessed signals (length-16000). $\\boldsymbol{X}$ denotes not applicable or computationally infeasible on a single GPU. All results except ours are reproduced from [GGR22]. as $\\operatorname{DSS}_{\\text {EXP }}$ except that we omit the term $\\Lambda^{-1}\\left(e^{\\Lambda \\Delta}-I\\right)$ in the expression of the kernel shown in Proposition 1(a) and instead compute it as $\\widetilde{w} \\cdot$ elementwise-exp $(P)$. As shown in Table 1, the performance of $\\mathrm{DSS}_{\\mathrm{EXP}-\\mathrm{NO}-\\mathrm{SCALE}}$ is generally inferior to that of $\\mathrm{DSS}_{\\mathrm{EXP}}$ with the model failing on the challenging PATH-X task which requires the model to capture extremely long-range interactions. Raw Speech Classification Audio is typically digitized using a high sampling rate resulting in very long sequences. This provides an interesting domain for investigating the abilities of long-range models. We evaluate the performance of DSS on the Speech Commands (SC) dataset [War18], consisting of raw audio samples of length 16000 , modeled as a 10 -way classification task. As shown in Table 2, the performance of all DSS variants is comparable to that of S4 ( 98.2 vs 98.1 ). In all experiments and ablations, S4 and DSS use identical model hyperparameters such as hidden size, number of layers, etc. Our experimental setup was built on top of the training framework provided by the S4 authors and for our S4 runs we followed their official instructions. ${ }^{5}$ Details about model initialization, and hyperparameters are provided in \u00a7A.3. ### 4.1 Analyzing the Performance of DSS\n\nWhile the experimental results presented above are encouraging, and clearly demonstrate the effectiveness of DSS at modeling long-range dependencies, it is not clear what exactly are the main factors contributing to its performance. To investigate this further, we performed an ablation analysis aimed at answering the following questions:\n\n- How significant is the Skew-Hippo initialization (\u00a73.3) to the model performance? Would initializing $\\Lambda$ randomly work just as well? - Is the main source of superior performance of state space models (S4, DSS), compared to previous models, their ability to model long-range dependencies? Would restricting DSS to only model local interactions hurt its performance on the above tasks? Random Initialization To answer the first question, we repeated the experiments after initializing each element of $\\Lambda_{\\text {re }}$ and $\\Lambda_{\\text {im }}$ in DSS $_{\\text {softmax }}$ by randomly sampling from $\\mathcal{N}(0,1)$. Truncated Kernels To answer the second question, instead of constructing a kernel of length equal to the length $L$ of the input, we restricted the length of the kernel constructed in $\\mathrm{DSS}_{\\text {softmax }}$ (Algorithm 1) to 128, significantly shorter than the length of the input. To understand the implication of this restriction recall Equation 5 which states that $y_{k}=\\sum_{j=0}^{k} \\bar{K}_{j} \\cdot u_{k-j}$. For a given context size $c=128$, restricting $\\bar{K}_{\\geqslant c}=0$ would imply\n\n$$\ny_{k}=\\sum_{j=0}^{\\min (k, c-1)} \\bar{K}_{j} \\cdot u_{k-j}\n$$\n\nand hence the output $y_{k}$ at position $k$ would only depend on $u_{k}, \\ldots, u_{k-c+1}$. This would restrict each $\\mathrm{DSS}_{\\text {Softmax }}$ layer to only model local interactions and the model would require several layers to have a broader receptive field and capture long-range interactions. [^4]| MODEL | ListOPS | TEXT | IMAGE | PATHFINDER | Path-X | SC |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Non-State-Space best | 49.5 | 78.7 | 47.4 | 77.8 | $x$ | 96.5 |\n| DSS $_{\\text {softmax }}$ | 60.6 | 84.8 | 85.7 | 84.6 | 87.8 | 97.7 |\n| DSS $_{\\text {softmax }}$ (random init) | 57.6 | 80.54 | 72.13 | 77.5 | $x$ | 96.8 |\n| DSS $_{\\text {softmax }}$ (kernel length 128) | 51.6 | 75.41 | 83.8 | 65.1 | $x$ | 96.7 |\n| DSS $_{\\text {sOftmax }}\\left(\\operatorname{argmax}_{k<L}\\left\\|K_{k}\\right\\|-95-\\right.$ percentile $)$ | 89 | 124 | 66 | 87 | 1269 | 81 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-08.jpg?height=42&width=1387&top_left_y=548&top_left_x=369) in top and middle sections of Table 1. (Bottom) \"argmax ${ }_{k<L}\\left|K_{k}\\right|-95$-percentile\" is the 95 th percentile of $\\left\\{\\operatorname{argmax}_{0 \\leqslant k<L}\\left|K_{k}\\right|: K\\right.$ is a kernel in $\\left.\\operatorname{DSS}_{\\text {softmax }}\\right\\}$ and is explained in \u00a74.2. As shown in Table 3, randomly initializing the $\\Lambda$ parameters of DSS leads to significant performance degradation on the majority of tasks, with the model failing to perform on PATH-X. This is inline with the findings of [GGR22] who also reported the initialization of $S 4$ to be critical to its performance. Interestingly, despite this performance reduction, DSS manages to outperform all non state-spacebased models on every task. Truncating the length of the kernel also leads to a significant reduction in performance across most tasks (Table 3), suggesting that the superior performance of state-space models on these tasks can indeed be partly attributed to their ability to capture long-range dependencies. Moreover, on some tasks such as ListOPS and ImAGE, using a truncated kernel still manages to outperform all Transformer variants, which is surprising as Transformer layers are known to be effective at capturing interactions at such short ranges. ### 4.2 Analysis of Learned DSS Parameters\n\nTo further explore the inner workings of DSS, we visually inspected the trained parameters and\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-08.jpg?height=44&width=346&top_left_y=1276&top_left_x=369)\nThe kernels of all layers of the trained DSS $_{\\text {softmax }}$ are shown in Figure 2 and reveal a stark contrast between the tasks. On the tasks ImAGE and SC, for almost all kernels, the absolute values of the first 128 positions are significantly higher than the later positions indicating that these kernels are mostly local. On the other hand for PATH-X, for a significant proportion of kernels the opposite is true, indicating that these kernels are modeling long-range interactions. This partially explains why the performance of $\\mathrm{DSS}_{\\text {Softmax }}$ on IMAGE and SC does not decrease significantly even after limiting the kernel length to 128 , whereas Pathfinder and Path-X report significant degradation (Table 3). The last row of Table 3 shows the 95 th percentile of the set $\\left\\{\\operatorname{argmax}_{0 \\leqslant k<L}\\left|K_{k}\\right|: K\\right.$ is a kernel in $\\left.\\operatorname{DSS}_{\\text {softmax }}\\right\\}$, i.e., the set of positions over all kernels, at which for a given kernel its absolute value is maximized. As expected, most kernels of IMAGE and SC are local whereas for PATH-X they are mostly long range. The values of the real and imaginary parts of $\\Lambda$ are shown in Figure 3 (and \u00a7A.4, Figure 4). We observe that, although all elements of $\\Lambda_{\\mathrm{re}}$ are initialized as -0.5 , their values can change significantly during training and can become positive (see plots for LiSTOPS, SC). It can also be observed that a $\\lambda$ with a more negative $\\operatorname{Re}(\\lambda)$ generally tends to have a larger $\\operatorname{Im}(\\lambda)$, which interestingly is a property not satisfied by Skew-Hippo initialization. The values of the trained $\\Delta_{\\text {log }}$ corresponding to the real and imaginary parts of $\\Lambda$ are shown in $\\S$ A. 4 (Figure 5) and, in this case as well, change significantly during training. The values of $\\Delta_{\\mathrm{log}}$ on short-range tasks such as IMAGE and SC generally tend to be larger compared to long-range tasks such as PATHFINDER and PATH-X, inline with the intuition that, when $\\operatorname{Re}(\\lambda)<0$, larger values of $\\Delta$ correspond to modeling long-range dependence (\u00a73.4).",
    "dssm-8": "We note that the plot for LISTOPS reveals an outlier with a value of 22 which after exponentiation in Algorithm 1 would result in an extreme large $\\Delta$. This can potentially lead to training instabilities and we plan to address this issue in future work. ## 5 Discussion\n\nRelated work In a long line of work, several variants of the Transformer have been proposed to address its quadratic complexity (cf.",
    "dssm-9": "[GB21] and references therein). Recently, Gu et al. [GGR22]\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-09.jpg?height=591&width=1383&top_left_y=242&top_left_x=355)\n\nFigure 2: Kernels of trained DSS $_{\\text {Softmax }}$. Each row of pixels corresponds to one of the (number-of-layers $\\times H$ ) kernels. For a row (kernel) $K \\in \\mathbb{R}^{L}$ the element $K_{k} \\in \\mathbb{R}$ is shown as $\\left|K_{k}\\right| /|| K \\|_{\\infty}$ to enable visualization. To enable comparison across tasks, only the first 1024 positions are shown. ![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-09.jpg?height=640&width=1430&top_left_y=1058&top_left_x=335)\n\nFigure 3: Values of $\\Lambda$ in trained $\\operatorname{DSS}_{\\text {Softmax }}$ for tasks described in $\\S 4$. For $\\lambda=x+i y$, we show $y$ on $\\log$-scale for better visualization by plotting $\\lambda$ as $(x, \\operatorname{arcsinh}(y / 2))=\\left(x, \\log \\left(y / 2+\\sqrt{(y / 2)^{2}+1}\\right)\\right)$. Black dots correspond to Skew-Hippo initialization (\u00a73.3). introduced S4, a new type of model that leverages linear state spaces for contextualization instead of attention. Our work is inspired from S 4 but uses a diagonal parameterization of state spaces. As a result, our method is significantly simpler compared to $S 4$ and we do not require (1) Pad\u00e9 approximations to $\\bar{A}=e^{A \\Delta}$ (Euler, Bilinear, etc), (2) Woodbury Identity reductions to compute matrix inverse, and (3) fourier analysis for computing the SSM kernel efficiently via truncated generating functions. Limitations and future work In this work, we evaluated DSS on sequence-level classification tasks. In future work, we plan to include token-level generation tasks such as language modeling, forecasting, etc. Another natural and important future direction is to pretrain models based on DSS over large amounts of raw data. Lastly, we found that the initialization and learning rates of DSS parameters $\\Lambda_{\\mathrm{re}}, \\Lambda_{\\mathrm{im}}, \\Delta_{\\mathrm{log}}$ (\u00a73.3) play an important role in the performance and convergence of the model. Informally, for tasks such as PATH-X that require very long-range interactions, a smaller initialization of $\\Delta_{\\text {log }}$ was beneficial. An in-depth analysis of this phenomenon could be helpful and remains for future work. ## Acknowledgments and Disclosure of Funding\n\nWe thank Ramon Fernandez Astudillo for carefully reviewing the preliminary draft and suggesting several helpful edits. We thank Omer Levy, Achille Fokoue and Luis Lastras for their support. Our experiments were conducted on IBM's Cognitive Computing Cluster, with additional resources from Tel Aviv University. This research was supported by (1) IBM AI Residency program and (2) Defense Advanced Research Projects Agency (DARPA) through Cooperative Agreement D20AC00004 awarded by the U.S. Department of the Interior (DOI), Interior Business Center. ## References\n\n$\\left[\\mathrm{BMH}^{+}\\right.$21] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. W. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. ArXiv, abs/2112.04426, 2021. $\\left[\\mathrm{BMR}^{+}\\right.$20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc' Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [BPC20] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. ArXiv preprint, abs/2004.05150, 2020. [BZMA20] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [CLD ${ }^{+}$21] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [CLRS09] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms. The MIT Press, 3rd edition, 2009. [DXX18] Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2018, Calgary, AB, Canada, April 15-20, 2018, pages 5884-5888. IEEE, 2018. [GB20] Ankit Gupta and Jonathan Berant. GMAT: Global memory augmentation for transformers. ArXiv preprint, abs/2006.03274, 2020. [GB21] Ankit Gupta and Jonathan Berant. Value-aware approximate attention.",
    "dssm-10": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9567-9574, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. $\\left[\\mathrm{GDE}^{+} 20\\right]$ Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. $\\left[\\mathrm{GDG}^{+}\\right.$21] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. In Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing, pages 39-52, Virtual, November 2021. Association for Computational Linguistics. [GGDR22] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. ArXiv preprint, abs/2202.09729, 2022. [GGR22] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [GJG ${ }^{+}$21] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state-space layers. Advances in neural information processing systems, 34, 2021. [HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus).",
    "dssm-11": "ArXiv preprint, abs/1606.08415, 2016. [KKL20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "dssm-12": "In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 5156-5165. PMLR, 2020. $\\left[\\mathrm{QML}^{+}{ }^{20]}\\right.$ Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise selfattention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2555-2565, Online, 2020. Association for Computational Linguistics. [RKH ${ }^{+}$21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages $8748-8763$. PMLR, 2021. $\\left[\\mathrm{RPG}^{+}\\right.$21] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8821-8831. PMLR, 2021. [RSVG21] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.",
    "dssm-13": "Transactions of the Association for Computational Linguistics, 9:53-68, 2021. [SSI ${ }^{+}$22] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. Scrolls: Standardized comparison over long language sequences, 2022. $\\left[\\mathrm{TDA}^{+}\\right.$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [TDBM20] Yi Tay, M. Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ArXiv, abs/2009.06732, 2020. [VKE19] Aaron Voelker, Ivana Kajic, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks.",
    "dssm-14": "In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 15544-15553, 2019. [VSP ${ }^{+}$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.",
    "dssm-15": "Wallach, Rob Fergus, S.",
    "dssm-16": "V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008, 2017. [War18] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. ArXiv, abs/1804.03209, 2018. $\\left[W^{+}{ }^{+}\\right.$20] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. $A r X i v$, abs/2006.04768, 2020. [ZS21] Zhenhai Zhu and Radu Soricut. H-transformer-1D: Fast one-dimensional hierarchical attention for sequences.",
    "dssm-17": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3801-3815, Online, 2021.",
    "dssm-18": "Association for Computational Linguistics. ## A Supplemental Material\n\n## A. 1 Diagonal State Spaces\n\nWe restate Proposition 1 for convenience. Proposition. Let $K \\in \\mathbb{R}^{1 \\times L}$ be the kernel of length $L$ of a given state space $(A, B, C)$ and sample time $\\Delta>0$, where $A \\in \\mathbb{C}^{N \\times N}$ is diagonalizable over $\\mathbb{C}$ with eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{N}$ and $\\forall i, \\lambda_{i} \\neq 0$ and $e^{L \\lambda_{i} \\Delta} \\neq 1$. Let $P \\in \\mathbb{C}^{N \\times L}$ be $P_{i, k}=\\lambda_{i} k \\Delta$ and $\\Lambda$ be the diagonal matrix with $\\lambda_{1}, \\ldots, \\lambda_{N}$. Then there exist $\\widetilde{w}, w \\in \\mathbb{C}^{1 \\times N}$ such that\n(a) $K=\\bar{K}_{\\Delta, L}\\left(\\Lambda,(1)_{1 \\leqslant i \\leqslant N}, \\widetilde{w}\\right)=\\widetilde{w} \\cdot \\Lambda^{-1}\\left(e^{\\Lambda \\Delta}-I\\right) \\cdot \\operatorname{elementwise-exp}(P)$,\n(b) $K=\\bar{K}_{\\Delta, L}\\left(\\Lambda,\\left(\\left(e^{L \\lambda_{i} \\Delta}-1\\right)^{-1}\\right)_{1 \\leqslant i \\leqslant N}, w\\right)=w \\cdot \\Lambda^{-1} \\cdot \\operatorname{row}-\\operatorname{softmax}(P)$. Proof. Let $A$ be diagonalizable over $\\mathbb{C}$ as $A=V \\Lambda V^{-1}$ with eigenvalues $\\lambda_{1}, \\ldots, \\lambda_{N} \\in \\mathbb{C}$. From Equation 4 we have\n\n$$\nK=\\left(C e^{A \\cdot k \\Delta}\\left(e^{A \\Delta}-I\\right) A^{-1} B\\right)_{0 \\leqslant k<L}\n$$\n\nwhere\n\n$$\nK_{k}=C e^{A \\cdot k \\Delta}\\left(e^{A \\Delta}-I\\right) A^{-1} B=(C V) e^{\\Lambda k \\Delta}\\left(e^{\\Lambda \\Delta}-I\\right) \\Lambda^{-1}\\left(V^{-1} B\\right)\n$$\n\nFor $C V \\in \\mathbb{C}^{1 \\times N}$ and $V^{-1} B \\in \\mathbb{C}^{N \\times 1}$ let $(C V)^{\\top} *\\left(V^{-1} B\\right)=\\widetilde{w} \\in \\mathbb{C}^{N}$ be the element-wise product of $C V$ and $V^{-1} B$. Then,\n\n$$\n\\begin{aligned}\nK_{k} & =\\sum_{i=1}^{N} \\frac{e^{\\lambda_{i} k \\Delta}\\left(e^{\\lambda_{i} \\Delta}-1\\right)}{\\lambda_{i}} \\cdot \\widetilde{w}_{i} \\\\\n& =\\sum_{i=1}^{N} \\frac{e^{\\lambda_{i} k \\Delta}\\left(e^{\\lambda_{i} \\Delta}-1\\right)}{\\lambda_{i}\\left(e^{L \\lambda_{i} \\Delta}-1\\right)} \\cdot\\left(\\left(e^{L \\lambda_{i} \\Delta}-1\\right) \\widetilde{w}_{i}\\right) \\\\\n& =\\sum_{i=1}^{N}\\left(\\widetilde{w}_{i} \\cdot\\left(e^{L \\lambda_{i} \\Delta}-1\\right)\\right) \\cdot \\frac{1}{\\lambda_{i}} \\cdot \\frac{e^{\\lambda_{i} k \\Delta}}{\\left(\\sum_{r=0}^{L-1} e^{r \\lambda_{i} \\Delta}\\right)}\n\\end{aligned}\n$$\n\nwhere the last equality follows from $\\left(z^{L}-1\\right)=(z-1)\\left(z^{0}+\\ldots+z^{L-1}\\right)$ and using $z^{L} \\neq 1$. Let $P \\in \\mathbb{C}^{N \\times L}$ be the matrix $P_{i, k}=\\lambda_{i} \\cdot k \\Delta$ and let $E=$ elementwise- $\\exp (P)$. It is easy to verify that Equation 7 can be re-written as a vector-matrix product as\n\n$$\nK=\\widetilde{w} \\cdot \\Lambda^{-1}\\left(e^{\\Lambda \\Delta}-I\\right) \\cdot E\n$$\n\nSimilarly, for the state space $\\left.\\left(\\Lambda,(1)_{1 \\leqslant i \\leqslant N}, \\widetilde{w}\\right)\\right)$ and sample time $\\Delta$ its kernel $\\widetilde{K}$ can be obtained from Equation 4 as\n\n$$\n\\begin{aligned}\n\\widetilde{K}_{k} & =\\widetilde{w} \\cdot e^{\\Lambda \\cdot k \\Delta}\\left(e^{\\Lambda \\Delta}-I\\right) \\Lambda^{-1} \\cdot[1, \\ldots, 1]_{N \\times 1} \\\\\n& =\\sum_{i=1}^{N} \\widetilde{w}_{i} \\cdot \\frac{e^{\\lambda_{i} k \\Delta}\\left(e^{\\lambda_{i} \\Delta}-1\\right)}{\\lambda_{i}}\n\\end{aligned}\n$$\n\nwhich is also the expression for $K_{k}$ (Equation 7). This proves part (a) and we now consider part (b). Let $w \\in \\mathbb{C}^{N}$ be defined as\n\n$$\nw_{i}=\\widetilde{w}_{i} \\cdot\\left(e^{L \\lambda_{i} \\Delta}-1\\right)\n$$\n\nThen from Equation 9,\n\n$$\nK_{k}=\\sum_{i=1}^{N} w \\cdot \\frac{1}{\\lambda_{i}} \\cdot \\frac{e^{\\lambda_{i} k \\Delta}}{\\left(\\sum_{r=0}^{L-1} e^{r \\lambda_{i} \\Delta}\\right)}\n$$\n\nLet $S=$ row-softmax $(P)$ denote the matrix obtained after applying softmax on the rows of $P$, i.e. $$\nS_{i, k}=\\frac{e^{\\lambda_{i} k \\Delta}}{\\sum_{r=0}^{L-1} e^{r \\lambda_{i} \\Delta}}\n$$\n\nIt is easy to verify that Equation 10 can be expressed as a vector-matrix product\n\n$$\nK=w \\cdot \\Lambda^{-1} \\cdot S\n$$\n\nSimilarly, for the state space $\\left.\\left(\\Lambda,\\left(\\left(e^{L \\lambda_{i} \\Delta}-1\\right)^{-1}\\right)_{1 \\leqslant i \\leqslant N}, w\\right)\\right)$ and sample time $\\Delta$ its kernel $\\hat{K}$ can be obtained from Equation 4 as\n\n$$\n\\begin{aligned}\n\\widehat{K}_{k} & =w \\cdot e^{\\Lambda \\cdot k \\Delta}\\left(e^{\\Lambda \\Delta}-I\\right) \\Lambda^{-1} \\cdot\\left[\\ldots,\\left(e^{L \\lambda_{i} \\Delta}-1\\right)^{-1}, \\ldots\\right]_{N \\times 1} \\\\\n& =\\sum_{i=1}^{N} w_{i} \\cdot \\frac{e^{\\lambda_{i} k \\Delta}\\left(e^{\\lambda_{i} \\Delta}-1\\right)}{\\lambda_{i}\\left(e^{L \\lambda_{i} \\Delta}-1\\right)} \\\\\n& =\\sum_{i=1}^{N} \\widetilde{w}_{i} \\cdot \\frac{e^{\\lambda_{i} k \\Delta}\\left(e^{\\lambda_{i} \\Delta}-1\\right)}{\\lambda_{i}}\n\\end{aligned}\n$$\n\nwhich is also the expression for $K_{k}$ (Equation 7). ## A. 2 Numerically Stable softmax\n\nAs noted in $\\S 3.1$, softmax can have singularities over $\\mathbb{C}$. To address this issue, we use a simple correction to make it well-defined over the entire domain:\n\n- $\\operatorname{softmax}: \\operatorname{Given}\\left(x_{0}, \\ldots, x_{L-1}\\right)=x \\in \\mathbb{C}^{L}$, let $\\operatorname{softmax}(x) \\in \\mathbb{C}^{L}$ be defined as $(\\operatorname{softmax}(x))_{k}=$ $e^{x_{k}}\\left(e^{x_{0}}+\\ldots+e^{x_{L-1}}\\right)^{-1}$. Note that for any $c \\in \\mathbb{C}$, softmax $\\left(x_{0}, \\ldots, x_{L-1}\\right)=\\operatorname{softmax}\\left(x_{0}-\\right.$ $\\left.c, \\ldots, x_{L-1}-c\\right)$. Unlike over $\\mathbb{R}$, softmax can have singularities over $\\mathbb{C}$ as sum of exponentials can vanish. E.g. $e^{0}+e^{i \\pi}=0$ and hence $\\operatorname{softmax}(0, i \\pi)$ is not defined. - $\\max$ : Given $\\left(x_{0}, \\ldots, x_{L-1}\\right)=x \\in \\mathbb{C}^{L}$, let $\\max (x)$ be the $x_{i}$ with the maximum real part, i.e. $x_{\\operatorname{argmax}_{i} \\operatorname{Re}\\left(x_{i}\\right)}$. - $\\operatorname{reciprocal}_{\\epsilon}$ : Given $x \\in \\mathbb{C}$ and $\\epsilon \\in \\mathbb{R}_{>0}$, let $\\operatorname{reciprocal}_{\\epsilon}(x)=\\frac{\\bar{x}}{x \\cdot \\bar{x}+\\epsilon}$ where $\\bar{x}$ is the complex conjugate of $x$. The denominator is always in $\\mathbb{R}_{\\geqslant \\epsilon}$ and $\\mid$ reciprocal ${ }_{\\epsilon} \\mid \\leqslant(2 \\sqrt{\\epsilon})^{-1}$. - $\\operatorname{softmax}_{\\epsilon}$ : Given $\\left(x_{0}, \\ldots, x_{L-1}\\right)=x \\in \\mathbb{C}^{L}$ let $m=\\max (x)$ and $\\tilde{x}_{i}=x_{i}-m$. Note that $\\left|e^{\\widetilde{x}_{i}}\\right| \\leqslant 1$. Given $\\epsilon \\in \\mathbb{R}_{>0}$, let $\\operatorname{softmax}_{\\epsilon}(x) \\in \\mathbb{C}^{L}$ be\n\n$$\n\\left(\\operatorname{softmax}_{\\epsilon}(x)\\right)_{k}=e^{\\widetilde{x}_{k}} \\cdot \\operatorname{reciprocal}_{\\epsilon}\\left(\\sum_{r=0}^{L-1} e^{\\widetilde{x}_{r}}\\right)\n$$\n\n$\\operatorname{softmax}_{\\epsilon}$ is always bounded and differentiable. In our implementation, we use softmax ${ }_{\\epsilon}$ with $\\epsilon=10^{-7}$. SSM Softmax In our current implementation of $\\operatorname{softmax}(x)$ (Figure 6), we exploit the specific structure of $x$ that arises in Algorithm 1. We now describe an alternate method based on FFT which also uses this specific structure and might lead to a faster implementation in the future. Claim 1 (SSM Softmax). Given $c \\in \\mathbb{C}$, let $p=I[\\operatorname{Re}(c)>0], n=1-p, e=\\exp (c \\cdot(n-p))$ and $r=(n-p e) /(p-n e)$. Let $\\omega=\\exp (-2 \\pi i / L)$ where $i=\\sqrt{-1}$. Then,\n\n$$\n\\begin{aligned}\n\\operatorname{softmax}(c \\cdot 0, \\ldots, c \\cdot(L-1)) & =\\text { inverseFFT }\\left(\\frac{1-e}{n-p e+(p-n e) \\omega^{k}}\\right)_{0 \\leqslant k<L} \\\\\n& =\\text { inverseFFT }\\left(\\frac{r+1}{r+\\omega^{k}}\\right)_{0 \\leqslant k<L}\n\\end{aligned}\n$$\n\nProof. There are 2 cases depending on sign of $\\operatorname{Re}(c)$. Case $1(p=0, n=1)$ : In this case we have $e=\\exp (c)$. For the map\n\n$$\nF(z)=\\frac{1-e}{1-e^{L}} \\sum_{k=0}^{L-1}(e z)^{k}=\\frac{(1-e)\\left(1-(e z)^{L}\\right)}{\\left(1-e^{L}\\right)(1-e z)}\n$$\n\nwe get the coefficients of $F(z)$ as\n\n$$\n\\operatorname{invFFT}\\left(F\\left(\\omega^{k}\\right)_{0 \\leqslant k<L}\\right)=\\left(\\frac{(1-e)}{\\left(1-e^{L}\\right)} e^{k}\\right)_{0 \\leqslant k<L}=\\operatorname{softmax}(c \\cdot 0, \\ldots, c \\cdot(L-1))\n$$\n\nWe have,\n\n$$\nF\\left(\\omega^{k}\\right)=\\frac{(1-e)\\left(1-\\left(e \\omega^{k}\\right)^{L}\\right)}{\\left(1-e^{L}\\right)\\left(1-e \\cdot \\omega^{k}\\right)}=\\frac{1-e}{1-e \\cdot \\omega^{k}}\n$$\n\nwhere last equality follows from $\\omega^{L}=1$. Case $2(p=1, n=0)$ : In this case we have $e=\\exp (-c)$. For the map\n\n$$\n\\begin{aligned}\nF(z) & =\\frac{1-e}{1-e^{L}} \\sum_{k=0}^{L-1} e^{k} z^{L-1-k}=\\frac{(1-e) z^{L-1}}{1-e^{L}} \\sum_{k=0}^{L-1}\\left(\\frac{e}{z}\\right)^{k} \\\\\n& =\\frac{(1-e) z^{L-1}}{1-e^{L}} \\frac{1-\\left(\\frac{e}{z}\\right)^{L}}{1-\\frac{e}{z}}=\\frac{(1-e)}{\\left(1-e^{L}\\right)} \\frac{\\left(z^{L}-e^{L}\\right)}{(z-e)}\n\\end{aligned}\n$$\n\nwe get the coefficients of $F(z)$ as\n\n$$\n\\begin{aligned}\n& \\operatorname{invFFT}\\left(F\\left(\\omega^{k}\\right)_{0 \\leqslant k<L}\\right)=\\left(\\frac{(1-e)}{1-e^{L}} e^{L-1-k}\\right)_{k} \\\\\n& =\\left(\\frac{\\left(e^{-1}-1\\right)}{\\left(e^{-1}\\right)^{L}-1}\\left(e^{-1}\\right)^{k}\\right)_{0 \\leqslant k<L}=\\operatorname{softmax}(c \\cdot 0, \\ldots, c \\cdot(L-1))\n\\end{aligned}\n$$\n\nas $e^{-1}=\\exp (c)$. Moreover, we have\n\n$$\nF\\left(\\omega^{k}\\right)=\\frac{(1-e)\\left(\\omega^{k \\cdot L}-e^{L}\\right)}{\\left(1-e^{L}\\right)\\left(\\omega^{k}-e\\right)}=\\frac{1-e}{-e+\\omega^{k}}\n$$\n\nwhere last equality follows from $\\omega^{L}=1$. Finally, the second equality of the main Claim follows from $1-e=n-p e+p-n e$. The computation of softmax in Claim 1 is numerically stable and we always exponentiate scalars with a negative real part.",
    "dssm-19": "The computed function has singularities at $c \\in\\{-2 \\pi i k / L, 0 \\leqslant k<L\\}$. ## A. 3 Experimental Setup\n\nWe now describe the training details for DSS and S4 on LRA and Speech Commands (\u00a74). Sequence Classification Head: Both LRA and Speech Commands are sequence classification tasks. The final layer of the DSS stack outputs a sequence which is aggregated into a single vector via mean pooling along the length dimension. Exceptions to this were Text and Pathfinder tasks where the rightmost token was used as the aggregate. For all datasets, we used AdamW optimizer with a constant learning rate schedule with decay on validation plateau. However, for the DSS parameters (\u00a73.2) initial learning rate was $10^{-3}$ and weight decay was not used, with a few exceptions noted below. We used hyperparameters such as model sizes, number of update steps, etc as recommended by the S4 authors on their official repository and are listed in Table 4. We made the following exceptions for DSS trainings:\n\n- ListOpS: learning rate of $\\Delta_{\\text {log }}$ was 0.02 instead of $10^{-3}$. - TEXT: learning rate of $\\Delta_{\\log } 0.02$ instead of $10^{-3}$. - ImAGE: we used seed 0 and trained for 200 epochs instead of 100. - Pathfinder: we used Patience $=13$. - Path-X: we used batch size 16 and trained for 35 epochs. $\\Delta_{\\text {log }}$ was initialized as $e^{r}$ where $r \\sim$ $\\mathcal{U}(\\log (.0001), \\log (.01))$ and its learning rate was $10^{-4}$. This was beneficial in early convergence of the model. For our experiments, the test accuracy that we report in $\\S 4$ was measured at the checkpoint with the highest validation accuracy. All our experiments were conducted on a single A100 GPU (40GiB). |  | Depth | Features $H$ | Norm | Pre-norm | Dropout | LR | Batch Size | Epochs | WD | Patience |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| ListOps | 6 | 128 | BN | False | 0 | 0.01 | 50 | 50 | 0.01 | 5 |\n| Text | 4 | 128 | BN | True | 0 | 0.01 | 50 | 40 | 0 | 10 |\n| Retrieval | 6 | 256 | BN | True | 0 | 0.002 | 64 | 25 | 0 | 20 |\n| Image | 6 | 512 | LN | False | 0.2 | 0.004 | 50 | 200 | 0.01 | 10 |\n| Pathfinder | 6 | 256 | BN | True | 0.1 | 0.004 | 100 | 200 | 0 | 10 |\n| Path-X | 6 | 256 | BN | True | 0.0 | 0.0005 | 32 | 100 | 0 | 40 |\n| Speech Commands (Raw) | 6 | 128 | BN | True | 0.1 | 0.01 | 20 | 200 | 0 | 20 |\n\nTable 4: Hyperparameters for the S4 and DSS models. Exceptions for DSS are detailed in \u00a7A.3. (Top) LRA and (Bottom) Speech Commands. LR is initial learning rate and WD is weight decay. BN and LN refer to Batch Normalization and Layer Normalization. ## A. 4 Learned Parameters of DSS $_{\\text {Softmax }}$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-16.jpg?height=643&width=1421&top_left_y=993&top_left_x=341)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-16.jpg?height=41&width=776&top_left_y=1660&top_left_x=672)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-16.jpg?height=641&width=1427&top_left_y=1769&top_left_x=338)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-16.jpg?height=44&width=823&top_left_y=2428&top_left_x=651)\n\n## A. 5 Implementation of DSS $_{\\text {Softmax }}$\n\n```\ndef reciprocal(x, epsilon=1e-7):\n    x_conj = x.conj() # conjugate\n    return x_conj / (x*x_conj + epsilon)\ndef dss_kernel(L):\n    # L: kernel length\n    # Lambda: [N 2], log_dt: [H], W: [H N 2] (floats)\n    Lambda, log_dt, W = get_layer_parameters()\n    # complex parameter stored as 2 floats denoting real,\n    # imaginary parts as ADAM moments are non-linear\n    # convert reals to complex\n    Lambda, W = map(torch.view_as_complex, (Lambda, W)) # [N], [H N]\n    dt_Lambda = log_dt.exp().unsqueeze(-1) * Lambda # [H L]\n    pos = torch.arange(L, device=W.device) # [L]\n    P = dt_Lambda.unsqueeze(-1)* pos # [H N L]\n    # fast softmax using structure of P\n    Lambda_gt_0 = Lambda.real > 0 # [N]\n    if Lambda_gt_0.any():\n        with torch.no_grad():\n            P_max = dt_Lambda * (Lambda_gt_0 * (L-1)) # [H N]\n        P = P - P_max.unsqueeze(-1)\n    S = P.exp() # [H N L]\n    dt_Lambda_neg = dt_Lambda * (1 - 2*Lambda_gt_0) # [H N]\n    # 1 / S.sum(-1) == num / den\n    num = dt_Lambda_neg.exp() - 1 # [H N]\n    den = (dt_Lambda_neg * L).exp() - 1 # [H N]\n    W = W * num * reciprocal(den * Lambda) # [H N]\n    # mixture of softmaxes\n    return torch.einsum('hn,hnl->hl', W, S).real # [H L]\ndef state_space(u):\n    # u: batch of input sequences\n    # B: batch size, H: hidden size, L: sequence length\n    B, H, L = u.shape\n    # compute state space kernel for each of H coordinates\n    K = dss_kernel(L) # [H L]\n    # multiply two degree L-1 polynomials\n    #(u0 + u1*z ...uL-1*z^^-1)(K0 + K1*z ...",
    "dssm-20": "KL-1*z^^L-1)\n    # zero-pad them to degree 2L-1 to avoid wrap-around\n    K_f = torch.fft.rfft(K, n=2*L) # [H L+1]\n    u_f = torch.fft.rfft(u, n=2*L) # [B H L+1]\n    y_f = K_f * u_f # [B H L+1]\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-17.jpg?height=42&width=1088&top_left_y=2077&top_left_x=437)\n\n```\n    # yi = ui*KO + ...u0*Ki\n    # residual connection, non-linearity, output projection not shown\n    return y\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_26743ec392255ebc6d87g-17.jpg?height=42&width=907&top_left_y=2245&top_left_x=609)\n\n\n[^0]:    * work done while author was part of IBM AI Residency program.",
    "dssm-21": "Preprint. Under review. [^1]:    ${ }^{2}$ assumes the value of a sample of $u$ is held constant for a duration of one sample interval $\\Delta$. [^2]:    ${ }^{3}$ It is possible for norms of the parameters of the resulting diagonal state spaces to be much larger than that of the original state space. For example, this occurs for the HiPPO matrix [GGR22]. [^3]:    ${ }^{4}$ The large gap between S4 and DSS on TEXT is due to the use of a larger learning rate for $\\Delta_{\\log }$ in DSS. For our S 4 runs, we decided to use the official hyperparameters as provided by [GGR22]. [^4]:    ${ }^{5}$ https://github.com/HazyResearch/state-spaces\n\n"
}