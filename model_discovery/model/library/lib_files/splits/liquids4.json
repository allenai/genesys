{
    "liquids4-0": "# Liquid Structural State-Space Models \n\nRamin Hasani * *, Mathias Lechner *, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini,<br>Daniela Rus\n\nComputer Science and Artificial Intelligence Lab (CSAIL)\nMassachusetts Institute of Technology (MIT)\nCambridge, 02139, MA\n\n* indicates authors with equal contributions\n\nCorrespondence to rhasani@mit.edu\n\n\n#### Abstract\n\nA proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-ofthe-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S 4 is given by a linear liquid time-constant (LTC) statespace model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S 4 , and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical timeseries, with an average performance of $87.32 \\%$ on the Long-Range Arena benchmark. On the full raw Speech Command recognition dataset Liquid-S4 achieves $96.78 \\%$ accuracy with $30 \\%$ reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference. ## 1. Introduction\n\nLearning representations from sequences of data requires expressive temporal and structural credit assignment. In this space, the continuous-time neural network class of liquid time-constant networks (LTC) (Hasani et al., 2021b) has shown theoretical and empirical evidence for their expressivity and their ability to capture the cause and effect of a given task from high-dimensional sequential demonstrations (Lechner et al., 2020a; Vorbach et al., 2021). Liquid networks are nonlinear state-space models (SSMs) with an input-dependent state transition module that enables them to learn to adapt the dynamics of the model to incoming inputs, at inference, as they are dynamic causal models (Friston et al., 2003). Their complexity, however, is bottlenecked by their differential equation numerical solver that limits their scalability to longer-term sequences. How can we take advantage of LTC's generalization and causality capabilities and scale them to competitively learn long-range sequences without gradient issues, compared to advanced recurrent neural networks (RNNs) (Erichson et al., 2021; Gu et al., 2020a; Rusch and Mishra, 2021), convolutional neural networks (CNNs) (Cheng et al., 2022; Lea et al., 2016; Romero et al., 2021b), and attention-based models (Vaswani et al., 2017)? In this work, we set out to leverage the elegant formulation of structural state-space models (S4) (Gu et al., 2022a) to obtain linear liquid network instances that possess the approximation capabilities of both S4 and LTCs. This is because structural SSMs are shown to largely dominate advanced RNNs, CNNs, and\n\n[^0]Transformers across many data modalities such as text, sequence of pixels, audio, and time series (Gu et al., 2021, 2022a,b; Gupta, 2022). Structural SSMs achieve such impressive performance by using three main mechanisms: 1) High-order polynomial projection operators (HiPPO) (Gu et al., 2020a) that are applied to state and input transition matrices to memorize signals' history, 2) diagonal plus low-rank parametrization of the obtained HiPPO (Gu et al., 2022a), and 3) an efficient (convolution) kernel computation of an SSM's transition matrices in the frequency domain, transformed back in time via an inverse Fourier transformation (Gu et al., 2022a). To combine S4 and LTCs, instead of modeling sequences by linear state-space models of the form $\\dot{x}=$ A $x+\\mathbf{B} u, y=\\mathbf{C} x$, (as done in structural and diagonal SSMs ( Gu et al., 2022a,b), we propose to use a linearized LTC state-space model (Hasani et al., 2021b), given by the following dynamics: $\\dot{x}=(\\mathbf{A}+\\mathbf{B} u) x+$ B $u, y=\\mathbf{C} x$. We show that this dynamical system can also be efficiently solved via the same parametrization of S4, giving rise to an additional convolutional Kernel that accounts for the similarities of lagged signals. We call the obtained model Liquid-S4. Through extensive empirical evaluation, we show that Liquid-S4 consistently leads to better generalization performance compared to all variants of S4, CNNs, RNNs, and Transformers across many time-series modeling tasks. In particular, we achieve SOTA performance on the Long Range Arena benchmark (Tay et al., 2020b) with an average of $87.32 \\%$. To sum up, we make the following contributions:\n\n1. We introduce Liquid-S4, a new state-space model that encapsulates the generalization and causality capabilities of liquid networks as well as the memorization, efficiency and scalability of S4. 2. We achieve State-of-the-art performance on pixel-level sequence classification, text, speech recognition and all six tasks of the long-range arena benchmark with an average accuracy of $87.32 \\%$. On the full raw Speech Command recognition dataset Liquid-S4 achieves $96.78 \\%$ accuracy with $30 \\%$ reduction in parameter. Finally on the BIDMC vital signs dataset Liquid-S4 achieves SOTA in all modes. ## 2. Related Works\n\nLearning Long-Range Dependencies with RNNs. Sequence modeling can be performed autoregressively with RNNs which possess persistent states (Little, 1974) originated from Ising (Brush, 1967) and Hopfield networks (Hopfield, 1982; Ramsauer et al., 2020). Discrete RNNs approximate continuous dynamics stepby-steps via dependencies on the history of their hidden states, and continuous-time (CT) RNNs use ordinary differential equation (ODE) solvers to unroll their dynamics with more elaborate temporal steps (Funahashi and Nakamura, 1993). CT-RNNs can perform remarkable credit assignment in sequence modeling problems both on regularly sampled, irregularly-sampled data (Amig\u00f3 et al., 2012; Belletti et al., 2016; Foster, 1996; Kowal et al., 2019; Li and Marlin, 2016; Pearson et al., 2003; Roy and Yan, 2020), by turning the spatiotemproal dependencies into vector fields (Chen et al., 2018), enabling better generalization and expressivity (Hasani et al., 2021b; Massaroli et al., 2020). Numerous works have studied their characteristics to understand their applicability and limitations in learning sequential data and flows (Dupont et al., 2019; Durkan et al., 2019; Gruenbacher et al., 2022; Grunbacher et al., 2021; Hanshu et al., 2020; Hasani et al., 2020; Holl et al., 2020; Jia and Benson, 2019; Kidger et al., 2020; Lechner et al., 2019; Liebenwein et al., 2021; Quaglino et al., 2020). However, when these RNNs are trained by gradient descent (Allen-Zhu and Li, 2019; Rumelhart et al., 1986; Sherstinsky, 2020), they suffer from the vanishing/exploding gradients problem, which makes difficult the learning of long-term dependencies in sequences (Bengio et al., 1994; Hochreiter, 1991). This issue happens in both discrete RNNs such as GRU-D with its continuous delay mechanism (Che et al., 2018) and PhasedLSTMs (Neil et al., 2016), and continuous RNNs such as ODE-RNNs (Rubanova et al., 2019), GRU-ODE (De Brouwer et al., 2019), Log-ODE methods (Morrill et al., 2020) which compresses the input time-series by\ntime-continuous path signatures (Friz and Victoir, 2010), and neural controlled differential equations (Kidger et al., 2020), and liquid time-constant networks (LTCs) (Hasani et al., 2021b). Numerous solutions have been proposed to resolve these gradient issues to enable long-range dependency learning. Examples include discrete gating mechanisms in LSTMs (Greff et al., 2016; Hasani et al., 2019; Hochreiter and Schmidhuber, 1997), GRUs (Chung et al., 2014), continuous gating mechanisms such as CfCs (Hasani et al., 2021a), hawks LSTMs (Mei and Eisner, 2017), IndRNNs (Li et al., 2018), state regularization (Wang and Niepert, 2019), unitary RNNs (Jing et al., 2019), dilated RNNs (Chang et al., 2017), long memory stochastic processes (Greaves-Tunnell and Harchaoui, 2019), recurrent kernel networks (Chen et al., 2019), Lipschitz RNNs (Erichson et al., 2021), symmetric skew decomposition (Wisdom et al., 2016), infinitely many updates in iRNNs (Kag et al., 2019), coupled oscillatory RNNs (coRNNs) (Rusch and Mishra, 2021), mixedmemory RNNs (Lechner and Hasani, 2021), and Legendre Memory Units (Voelker et al., 2019). Learning Long-range Dependencies with CNNs and Transformers. RNNs are not the only solution to learning long-range dependencies. Continuous convolutional kernels such as CKConv (Romero et al., 2021b) and (Romero et al., 2021a), and circular dilated CNNs (Cheng et al., 2022) have shown to be efficient in modeling long sequences faster than RNNs. There has also been a large series of works showing the effectiveness of attention-based methods for modeling spatiotemporal data. A large list of these models is listed in Table $\\mathbf{1}$. These baselines have recently been largely outperformed by the structural state-space models (Gu et al., 2022a). State-Space Models. SSMs are well-established frameworks to study deterministic and stochastic dynamical systems (KALMAN, 1960). Their state and input transition matrices can be directly learned by gradient descent to model sequences of observations (Gu et al., 2021; Hasani et al., 2021b; Lechner et al., 2020b). In a seminal work, Gu et al. (2022a) showed that with a couple of fundamental algorithmic methods on memorization and computation of input sequences, SSMs can turn into the most powerful sequence modeling framework to-date, outperforming advanced RNNs, temporal and continuous CNNs (Cheng et al., 2022; Romero et al., 2021a,b) and a wide variety of Transformers (Vaswani et al., 2017), available in Table $\\mathbf{1}$ by a significant margin. The key to their success is their diagonal plus-low rank parameterization of the transition matrix of SSMs via higher-order polynomial projection (HiPPO) matrix (Gu et al., 2020a) obtained by a scaled Legendre measure (LegS) inspired by the Legendre Memory Units (Voelker et al., 2019) to memorize input sequences, a learnable input transition matrix, and an efficient Cauchy Kernel algorithm, results in obtaining structural SSMs named S4. It was also shown recently that diagonal SSMs (S4D) (Gupta, 2022) could be as performant as S4 in learning long sequences when parametrized and initialized properly (Gu et al., 2022b,c). There was also a new variant of S4 introduced as simplified-S4 (S5) Smith et al. (2022) that tensorizes the 1-D operations of S4 to gain a more straightforward realization of SSMs. Here, we introduce Liquid-S4, which is obtained by a more expressive SSM, namely liquid time-constant (LTC) representation (Hasani et al., 2021b) which achieves SOTA performance across many benchmarks. ## 3. Setup and Methodology\n\nIn this section, we first revisit the necessary background to formulate our Liquid Structural State-Space Models. We then set up and sketch our technical contributions. ### 3.1. Background\n\nWe aim to design an end-to-end sequence modeling framework built by SSMs. A continuous-time SSM representation of a linear dynamical system is given by the following set of equations:\n\n$$\n\\dot{x}(t)=\\mathbf{A} x(t)+\\mathbf{B} u(t), \\quad y(t)=\\mathbf{C} x(t)+\\mathbf{D} u(t) . $$\n\nHere, $x(t)$ is an $N$-dimensional latent state, receiving a 1-dimensional input signal $u(t)$, and computing a 1-dimensional output signal $y(t) . \\mathbf{A}^{(N \\times N)}, \\mathbf{B}^{(N \\times 1)}, \\mathbf{C}^{(1 \\times N)}$ and $\\mathbf{D}^{(1 \\times 1)}$ are system's parameters. For the sake of brevity, throughout our analysis, we set $\\mathbf{D}=0$ as it can be added eventually after construction of our main results in the form of a skip connection (Gu et al., 2022a). Discretization of SSMs. In order to create a sequence-to-sequence model similar to a recurrent neural network (RNN), we discretize the continuous-time representation of SSMs by the trapezoidal rule (bilinear transform) ${ }^{\\dagger}$ as follows (sampling step $=\\delta t$ ) (Gu et al., 2022a):\n\n$$\nx_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} u_{k}, \\quad y_{k}=\\overline{\\mathbf{C}} x_{k}\n$$\n\nThis is obtained via the following modifications to the transition matrices:\n\n$$\n\\overline{\\mathbf{A}}=\\left(\\mathbf{I}-\\frac{\\delta t}{2} \\mathbf{A}\\right)^{-1}\\left(\\mathbf{I}+\\frac{\\delta t}{2} \\mathbf{A}\\right), \\quad \\overline{\\mathbf{B}}=\\left(\\mathbf{I}-\\frac{\\delta t}{2} \\mathbf{A}\\right)^{-1} \\delta t \\mathbf{B}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}\n$$\n\nWith this transformation, we constructed a discretized seq-2-seq model that can map the input $u_{k}$ to output $y_{k}$, via the hidden state $x_{k} \\in \\mathbb{R}^{N}$. $\\overline{\\mathbf{A}}$ is the hidden transition matrix, $\\overline{\\mathbf{B}}$ and $\\overline{\\mathbf{C}}$ are input and output transition matrices, respectively. Creating a Convolutional Representation of SSMs. The system described by (2) and (3), can be trained via gradient descent to learn to model sequences, in a sequential manner which is not scalable. To improve this, we can write the discretized SSM in (2) as a discrete convolutional kernel. To construct the convolutional kernel, let us unroll the system (2) in time as follows, assuming a zero initial hidden states $x_{-1}=0$ :\n\n$$\n\\begin{aligned}\n& x_{0}=\\overline{\\mathbf{B}} u_{0}, \\quad x_{1}=\\overline{\\mathbf{A}} \\overline{\\mathbf{B}} u_{0}+\\overline{\\mathbf{B}} u_{1}, \\quad x_{2}=\\overline{\\mathbf{A}} \\overline{\\mathbf{B}} u_{0}+\\overline{\\mathbf{A} \\overline{\\mathbf{B}}} u_{1}+\\overline{\\mathbf{B}} u_{2}, \\quad \\ldots \\\\\n& y_{0}=\\overline{\\mathbf{C}} \\mathbf{B} u_{0}, \\quad y_{1}=\\overline{\\mathbf{C A}} \\mathbf{B} u_{0}+\\overline{\\mathbf{C B}} u_{1}, \\quad y_{2}=\\overline{\\mathbf{C A}}^{2} \\overline{\\mathbf{B}} u_{0}+\\overline{\\mathbf{C A}} \\mathbf{B} u_{1}+\\overline{\\mathbf{C B}} u_{2}, \\quad \\ldots\n\\end{aligned}\n$$\n\nThe mapping $u_{k} \\rightarrow y_{k}$ can now can be formulated into a convolutional kernel explicitly:\n\n$$\n\\begin{gathered}\ny_{k}=\\overline{\\mathbf{C A}}^{k} \\overline{\\mathbf{B}} u_{0}+\\overline{\\mathbf{C A}}^{k-1} \\overline{\\mathbf{B}} u_{1}+\\ldots \\overline{\\mathbf{C A B}} u_{k-1}+\\overline{\\mathbf{C}} u_{k,} \\quad y=\\overline{\\mathbf{K}} * u \\\\\n\\overline{\\mathbf{K}} \\in \\mathbb{R}^{L}:=\\mathcal{K}_{L}(\\overline{\\mathbf{C}}, \\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}):=(\\overline{\\mathbf{C A}} \\overline{\\mathbf{B}})_{i \\in[L]}=\\left(\\overline{\\mathbf{C B}}, \\overline{\\mathbf{C A B}}, \\ldots, \\overline{\\mathbf{C A}}{ }^{L-1} \\overline{\\mathbf{B}}\\right)\n\\end{gathered}\n$$\n\nEquation (5) is a non-circular convolutional kernel. Gu et al. (2022a) showed that under the condition that $\\overline{\\mathbf{K}}$ is known, it could be solved very efficiently by a black-box Cauchy kernel computation pipeline. ### 3.2. Liquid Structural State-Space Models\n\nIn this work, we construct a convolutional kernel corresponding to a linearized version of LTCs (Hasani et al., 2021b); an expressive class of continuous-time neural networks that demonstrate attractive generalizability outof-distribution and are dynamic causal models (Friston et al., 2003; Hasani et al., 2020; Vorbach et al., 2021). [^1]In their general form, the state of a liquid time-constant network at each time-step is given by the set of ODEs described below (Hasani et al., 2021b):\n\n$$\n\\frac{d \\mathbf{x}(t)}{d t}=-\\underbrace{[\\boldsymbol{A}+\\boldsymbol{B} \\odot f(\\mathbf{x}(t), \\mathbf{u}(t), t, \\theta)]}_{\\text {Liquid time-constant }} \\odot \\mathbf{x}(t)+\\boldsymbol{B} \\odot f(\\mathbf{x}(t), \\mathbf{u}(t), t, \\theta)\n$$\n\nIn this expression, $\\mathbf{x}^{(N \\times 1)}(t)$ is the vector of hidden state of size $N, \\mathbf{u}^{(m \\times 1)}(t)$ is an input signal with $m$ features, $\\boldsymbol{A}^{(N \\times 1)}$ is a time-constant state-transition mechanism, $\\boldsymbol{B}^{(N \\times 1)}$ is a bias vector, and $\\odot$ represents the Hadamard product. $f($.$) is a bounded nonlinearity parametrized by \\theta$. Our objective is to show how the liquid time-constant (i.e., an input-dependent state transition mechanism in state-space models can enhance its generalization capabilities by accounting for the covariance of the input samples. To do this, we linearize the LTC formulation of Eq. 7 in the following to better connect the model to SSMs. Let's dive in:\nLinear Liquid Time-Constant State-Space Model. A Linear LTC SSM can be presented by the following coupled bilinear (first order bilinear Taylor approximation (Penny et al., 2005)) equation:\n\n$$\n\\dot{x}(t)=[\\mathbf{A}+\\mathbf{B} u(t)] x(t)+\\mathbf{B} u(t), \\quad y(t)=\\mathbf{C} x(t)\n$$\n\nSimilar to (1), $x(t)$ is an $N$-dimensional latent state, receiving a 1 -dimensional input signal $u(t)$, and computing a 1-dimensional output signal $y(t) .",
    "liquids4-1": "\\mathbf{A}^{(N \\times N)}, \\mathbf{B}^{(N \\times 1)}$, and $\\mathbf{C}^{(1 \\times N)}$. Note that $\\mathbf{D}$ is set to zero for simplicity. In (8), the first $\\mathbf{B} u(t)$ is added element-wise to $\\mathbf{A}$. This dynamical system allows the coefficient (state transition compartment) of state vector $x(t)$ to be input dependent which, as a result, allows us to realize more complex dynamics. Discretization of Liquid-SSMs. Similar to SSMs, Liquid-SSMs can also be discretized by a bilinear transform (trapezoidal rule) to construct a sequence-to-sequence model as follows:\n\n$$\nx_{k}=\\left(\\overline{\\mathbf{A}}+\\overline{\\mathbf{B}} u_{k}\\right) x_{k-1}+\\overline{\\mathbf{B}} u_{k}, \\quad y_{k}=\\overline{\\mathbf{C}} x_{k}\n$$\n\nThe discretized parameters $\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}$, and $\\overline{\\mathbf{C}}$ are identical to that of (3), which are function of the continuous-time coefficients $\\mathbf{A}, \\mathbf{B}$, and $\\mathbf{C}$, and the discretization step $\\delta t$. Creating a Convolutional Representation of Liquid-SSMs. Similar to (4), we first unroll the Liquid-SSM in time to construct a convolutional kernel of it. By assuming $x_{-1}=0$, we have:\n\n$$\n\\begin{aligned}\n& x_{0}=\\overline{\\mathbf{B}} u_{0}, \\quad y_{0}=\\overline{\\mathbf{C}} u_{0} \\\\\n& x_{1}=\\overline{\\mathbf{A B}} u_{0}+\\overline{\\mathbf{B}} u_{1}+\\overline{\\mathbf{B}}^{2} u_{0} u_{1}, \\quad y_{1}=\\overline{\\mathbf{C A B}} u_{0}+\\overline{\\mathbf{C B}} u_{1}+\\overline{\\mathbf{C B}}^{2} u_{0} u_{1} \\\\\n& x_{2}=\\overline{\\mathbf{A}} \\overline{\\mathbf{B}} u_{0}+\\overline{\\mathbf{A B}} u_{1}+\\overline{\\mathbf{B}} u_{2}+\\overline{\\mathbf{A B}}^{2} u_{0} u_{1}+\\overline{\\mathbf{A B}}^{2} u_{0} u_{2}+\\overline{\\mathbf{B}}^{2} u_{1} u_{2}+\\overline{\\mathbf{B}}^{3} u_{0} u_{1} u_{2} \\\\\n& y_{2}=\\overline{\\mathbf{C A}}^{2} \\overline{\\mathbf{B}} u_{0}+\\overline{\\mathbf{C A}} \\overline{\\mathbf{B}} u_{1}+\\overline{\\mathbf{C}} \\overline{\\mathbf{B}} u_{2}+\\overline{\\mathbf{C A}}^{2} \\overline{\\mathbf{B}}_{0} u_{1}+\\overline{\\mathbf{C A}}^{2} u_{0} u_{2}+\\overline{\\mathbf{C B}}^{2} u_{1} u_{2}+\\overline{\\mathbf{C B}}^{3} u_{0} u_{1} u_{2}, \\ldots\n\\end{aligned}\n$$\n\nThe resulting expressions of the Liquid-SSM at each time step consist of two types of weight configurations: 1. Weights corresponding to the mapping of individual time instances of inputs independently, shown in black in (10), and 2. Weights associated with all orders of auto-correlation of the input signal, shown in violet in (10). The first set of weights corresponds to the convolutional kernel of the simple SSM, shown by Eq. 5 and Eq. 6 , whereas the second set leads to the design of an additional input correlation kernel, which we call the liquid\nkernel. These kernels generate the following input-output mapping:\n\n$$\n\\begin{aligned}\n& y_{k}=\\overline{\\mathbf{C A}}^{k} \\overline{\\mathbf{B}} u_{0}+\\overline{\\mathbf{C A}}{ }^{k-1} \\overline{\\mathbf{B}} u_{1}+\\ldots \\overline{\\mathbf{C A}} \\overline{\\mathbf{B}} u_{k-1}+\\overline{\\mathbf{C}} u_{k}+ \\\\\n& \\sum_{p=2}^{\\mathcal{P}} \\forall\\binom{k+1}{p} \\text { of } u_{i} u_{i+1} \\ldots u_{p} \\overline{\\mathbf{C A}}^{(k+1-p-i)} \\overline{\\mathbf{B}}^{p} u_{i} u_{i+1} \\ldots u_{p} \\\\\n& \\text { for } i \\in \\mathbb{Z} \\text { and } i \\geq 0, \\quad \\rightarrow \\quad y=\\overline{\\mathbf{K}} * u+\\overline{\\mathbf{K}}_{\\text {liquid }} * u_{\\text {correlations }}\n\\end{aligned}\n$$\n\nFor instance, let us assume we have a 1-dimensional input signal $u(t)$ of length $L=100$ on which we run the liquid-SSM kernel.",
    "liquids4-2": "We set the hyperparameters $\\mathcal{P}=4$. This value represents the maximum order of the correlation terms we would want to take into account to output a decision. This means that the signal $u_{\\text {correlations }}$ in (11) will contain all combinations of 2 order correlation signals $\\binom{L+1}{2}, u_{i} u_{j}, 3$ order $\\binom{L+1}{3}, u_{i} u_{j} u_{k}$ and 4 order signals $\\binom{L+1}{4}, u_{i} u_{j} u_{k} u_{l}$. The kernel weights corresponding to this auto-correlation signal would be:\n\n$$\n\\begin{aligned}\n& \\overline{\\mathbf{K}}_{\\text {liquid }} * u_{\\text {correlations }}=\\left[\\overline{\\mathbf{C A}}^{(k-1)} \\overline{\\mathbf{B}}^{2}, \\ldots, \\overline{\\mathbf{C B}}^{2}, \\ldots, \\overline{\\mathbf{C A}}^{(k-2)} \\overline{\\mathbf{B}}^{3}, \\ldots, \\overline{\\mathbf{C B}}^{3}, \\ldots, \\overline{\\mathbf{C A}}^{(k-3)} \\overline{\\mathbf{B}}^{4}, \\ldots, \\overline{\\mathbf{C B}}^{4}\\right] * \\\\\n& {\\left[u_{0} u_{1}, \\ldots, u_{k-1} u_{k}, \\ldots, u_{0} u_{1} u_{2}, \\ldots, u_{k-2} u_{k-1} u_{k}, \\ldots, u_{0} u_{1} u_{2} u_{3}, \\ldots, u_{k-3} u_{k-2} u_{k-1} u_{k}\\right]^{T}}\n\\end{aligned}\n$$\n\nHere, $u_{\\text {correlations }}$ is a vector of length $\\binom{k+1}{2}+\\binom{k+1}{3}+\\binom{k+1}{4}$, and the kernel $\\overline{\\mathbf{K}}_{\\text {liquid }} \\in \\mathbb{R}^{\\binom{k+1}{2}+\\binom{k+1}{3}+\\binom{k+1}{4}}$. This additional kernel takes the temporal correlation of incoming input samples into consideration. This way Liquid-SSM give rise to a more general sequence modeling framework. The liquid convolutional kernel, $\\overline{\\mathbf{K}}_{\\text {liquid }}$ is as follows:\n\n$$\n\\overline{\\mathbf{K}}_{\\text {liquid }} \\in \\mathbb{R}^{\\tilde{L}}:=\\mathcal{K}_{L}(\\overline{\\mathbf{C}}, \\overline{\\mathbf{A}}, \\overline{\\mathbf{B}}):=\\left(\\overline{\\mathbf{C A}}^{(\\tilde{L}-i-p)} \\overline{\\mathbf{B}}^{p}\\right)_{i \\in[\\tilde{L}], p \\in[\\mathcal{P}]}=\\left(\\overline{\\mathbf{C A}}^{\\tilde{L}-2} \\overline{\\mathbf{B}}^{2}, \\ldots, \\overline{\\mathbf{C B}}^{p}\\right)\n$$\n\nHow to compute Liquid-S4 kernel efficiently? Gu et al. (2022a) showed that the S4 convolution kernel could be computed efficiently using the following elegant parameterization tricks:\n\n- To obtain better representations in sequence modeling schemes by SSMs, instead of randomly initializing the transition matrix A, we can use the Normal Plus Low-Rank (NPLR) matrix below, called the Hippo Matrix (Gu et al., 2020a) which is obtained by the Scaled Legendre Measure (LegS) (Gu et al., 2021, 2022a):\n\n$$\n\\text { (HiPPO Matrix) } \\quad \\boldsymbol{A}_{n k}=- \\begin{cases}(2 n+1)^{1 / 2}(2 k+1)^{1 / 2} & \\text { if } n>k \\\\ n+1 & \\text { if } n=k \\\\ 0 & \\text { if } n<k\\end{cases}\n$$\n\n- The NPLR representation of this matrix is the following (Gu et al., 2022a):\n\n$$\n\\boldsymbol{A}=\\boldsymbol{V} \\boldsymbol{\\Lambda} \\boldsymbol{V}^{*}-\\boldsymbol{P} \\boldsymbol{Q}^{\\top}=\\boldsymbol{V}\\left(\\boldsymbol{\\Lambda}-\\left(\\boldsymbol{V}^{*} \\boldsymbol{P}\\right)\\left(\\boldsymbol{V}^{*} \\boldsymbol{Q}\\right)^{*}\\right) \\boldsymbol{V}^{*}\n$$\n\nHere, $\\boldsymbol{V} \\in \\mathbb{C}^{N \\times N}$ is a unitary matrix, $\\boldsymbol{\\Lambda}$ is diagonal, and $\\boldsymbol{P}, \\boldsymbol{Q} \\in \\mathbb{R}^{N \\times r}$ are the low-rank factorization. Eq. 14 is Normal plus low rank with $\\mathrm{r}=1$ ( Gu et al., 2022a). With the decomposition 15, we can obtain A over complex numbers in the form of Diagonal plus low-rank (DPLR) (Gu et al., 2022a). - Vectors $B_{n}$ and $P_{n}$ are initialized by $\\boldsymbol{B}_{n}=(2 n+1)^{\\frac{1}{2}}$ and $\\boldsymbol{P}_{\\boldsymbol{n}}=(n+1 / 2)^{\\frac{1}{2}}$ (Gu et al., 2022b). Both vectors are trainable. - Furthermore, it was shown in Gu et al. (2022b) that with Decomposition 15, the eigenvalues of A might be on the right half of the complex plane, thus, result in numerical instability. To resolve this, Gu et al. (2022b) recently proposed to use the parametrization $\\boldsymbol{\\Lambda}-\\boldsymbol{P} \\boldsymbol{P}^{*}$ instead of $\\boldsymbol{\\Lambda}-\\boldsymbol{P} \\boldsymbol{P}^{*}$. ```\nAlgorithm 1 LiquID-S4 KerneL - The S4 convolution kernel (highlighted in black) is used from Gu et al.",
    "liquids4-3": "(2022a) and Gu et al. (2022b). Liquid kernel computation is highlighted in purple. Input: S4 parameters \\(\\boldsymbol{\\Lambda}, \\boldsymbol{P}, \\boldsymbol{B}, \\boldsymbol{C} \\in \\mathbb{C}^{N}\\), step size \\(\\Delta\\), liquid kernel order \\(\\mathcal{P}\\), inputs seq length \\(L\\), liquid kernel\n    sequence length \\(\\tilde{L}\\)\nOutput: SSM convolution kernel \\(\\overline{\\boldsymbol{K}}=\\mathcal{K}_{L}(\\overline{\\boldsymbol{A}}, \\overline{\\boldsymbol{B}}, \\overline{\\boldsymbol{C}})\\) and SSM liquid kernel \\(\\overline{\\boldsymbol{K}}_{\\text {liquid }}=\\mathcal{K}_{\\tilde{L}}(\\overline{\\boldsymbol{A}}, \\overline{\\boldsymbol{B}}, \\overline{\\bar{C}})\\) for \\(\\boldsymbol{A}=\\)\n    \\(\\Lambda-P P^{*}(\\) Eq. 6)\n    \\(\\widetilde{\\boldsymbol{C}} \\leftarrow\\left(I-\\bar{A}^{L}\\right)^{*} \\overline{\\boldsymbol{C}} \\quad \\triangleright\\) Truncate SSM generating function (SSMGF) to length \\(L\\)\n    \\(\\left[\\begin{array}{ll}k_{00}(\\omega) & k_{01}(\\omega) \\\\ k_{10}(\\omega) & k_{11}(\\omega)\\end{array}\\right] \\leftarrow[\\widetilde{\\boldsymbol{C}} \\boldsymbol{P}]^{*}\\left(\\frac{2}{\\Delta} \\frac{1-\\omega}{1+\\omega}-\\boldsymbol{\\Lambda}\\right)^{-1}[\\boldsymbol{B} \\boldsymbol{P}] \\quad \\triangleright\\) Black-box Cauchy kernel\n    \\(\\hat{\\mathbf{K}}(\\omega) \\leftarrow \\frac{2}{1+\\omega}\\left[k_{00}(\\omega)-k_{01}(\\omega)\\left(1+k_{11}(\\omega)\\right)^{-1} k_{10}(\\omega)\\right] \\quad \\triangleright\\) Woodbury Identity\n    \\(\\hat{\\mathbf{K}}=\\left\\{\\hat{\\mathbf{K}}(\\omega): \\omega=\\exp \\left(2 \\pi i \\frac{k}{L}\\right)\\right\\} \\quad \\triangleright\\) Evaluate SSMGF at all roots of unity \\(\\omega \\in \\Omega_{L}\\)\n    \\(\\overline{\\mathbf{K}} \\leftarrow \\mathrm{iFFT}(\\hat{\\mathbf{K}}) \\quad \\triangleright\\) Inverse Fourier Transform\n    if Mode \\(=\\mathrm{KB}\\) then \\(\\quad \\triangleright\\) Liquid-S4 Kernel as shown in Eq. 13\n        for \\(p\\) in \\(\\{2, \\ldots, \\mathcal{P}\\}\\) do\n            \\(\\overline{\\mathbf{K}}_{\\text {liquid }=p}=\\left[\\overline{\\mathbf{K}}_{(L-\\tilde{L}, L)} \\odot \\overline{\\mathbf{B}}_{(L-\\tilde{L}, L)}^{p-1}\\right] * \\mathbf{J}_{\\tilde{L}} \\quad \\triangleright \\mathbf{J}_{\\tilde{L}}\\) is a backward identity matrix\n            \\(\\overline{\\mathbf{K}}_{\\text {liquid }}\\).append \\(\\left(\\overline{\\mathbf{K}}_{\\text {liquid=p }}\\right)\\)\n        end for\n    else if Mode \\(=\\) PB then \\(\\triangleright\\) Liquid-S4 Kernel of Eq. 13 with \\(\\bar{A}\\) reduced to Identity. for \\(p\\) in \\(\\{2, \\ldots, \\mathcal{P}\\}\\) do\n            \\(\\overline{\\boldsymbol{K}}_{\\text {liquid }=p}=\\overline{\\boldsymbol{C}} \\odot \\overline{\\mathbf{B}}_{(L-\\tilde{L}, L)}^{p-1}\\)\n            \\(\\overline{\\mathbf{K}}_{\\text {liquid }}\\).append \\(\\left(\\overline{\\mathbf{K}}_{\\text {liquid=p }}\\right)\\)\n        end for\n    end if\n```\n\n- Computing the powers of $A$ in direct calculation of the S 4 kernel $\\overline{\\mathbf{K}}$ is computationally expensive. S4 computes the spectrum of $\\overline{\\boldsymbol{K}}$ instead of direct computations, which reduces the problem of matrix powers to matrix inverse computation Gu et al. (2022a). S4 then computes this convolution kernel via a black-box Cauchy Kernel efficiently, and recovers $\\overline{\\boldsymbol{K}}$ by an inverse Fourier Transform (iFFT) (Gu et al., 2022a). $\\overline{\\mathbf{K}}_{\\text {liquid }}$ possess similar structure to the S 4 kernel. In particular, we have:\nProposition 1. The liquid-S4 kernel for each order $p \\in \\mathcal{P}, \\overline{\\boldsymbol{K}}_{\\text {liquid }}$, can be computed by the anti-diagonal transformation (flip operation) of the product of the S4 convolution kernel, $\\overline{\\boldsymbol{K}}=\\left(\\overline{\\boldsymbol{C}}, \\overline{\\boldsymbol{C A}} \\overline{\\boldsymbol{B}}, \\ldots, \\overline{\\boldsymbol{C A}}{ }^{L-1} \\overline{\\boldsymbol{B}}\\right)$, and a vector $\\overline{\\boldsymbol{B}}^{p-1} \\in \\mathbb{R}^{N}$. The proof is given in Appendix. Proposition 1 indicates that the liquid-s4 kernel can be obtained from the precomputed S4 kernel and a Hadamard product of that kernel with the transition vector $\\overline{\\boldsymbol{B}}$ powered by the chosen liquid order. This is illustrated in Algorithm 1, lines 6 to 10, corresponding to a mode we call KB, which stands for Kernel $\\times$ B. Additionally, we introduce a simplified Liquid-S4 kernel that is easier to compute while being as expressive as or even better performing than the KB kernel. To obtain this, we set the transition matrix $\\bar{A}$ in Liquid-S4 of Eq. 13, with an identity matrix, only for the input correlation terms. This way, the liquid-s4 Kernel for a given liquid order $p \\in \\mathcal{P}$ reduces to the following expression:\n\nTable 1: Performance on Long Range Arena Tasks.",
    "liquids4-4": "Numbers indicate validation accuracy (standard deviation). The accuracy of models denoted by $*$ are reported from (Tay et al., 2020b). Methods denoted by $* *$ are reported from (Gu et al., 2022a). The rest of the models' performance results are reported from the cited paper. Liquid-S4 is used with its PB kernel. | Model <br> (input length) | ListOps <br> 2048 | IMDB <br> 2048 | AAN <br> 4000 | CIFAR <br> 1024 | Pathfinder <br> 1024 | Path-X <br> 16384 | Avg. |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Random* | 10.00 | 50.00 | 50.00 | 10.00 | 50.00 | 50.00 | 36.67 |\n| Transformer* (Vaswani et al., 2017) | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | x | 54.39 |\n| Local Att.* (Tay et al., 2020b) | 15.82 | 52.98 | 53.39 | 41.46 | 66.63 | $x$ | 46.06 |\n| Sparse Transformer* (Child et al., 2019) | 17.07 | 63.58 | 59.59 | 44.24 | 71.71 | x | 51.24 |\n| Longformer* (Beltagy et al., 2020) | 35.63 | 62.85 | 56.89 | 42.22 | 69.71 | x | 53.46 |\n| Linformer* (Wang et al., 2020) | 16.13 | 65.90 | 53.09 | 42.34 | 75.30 | x | 50.55 |\n| Reformer* (Kitaev et al., 2019) | 37.27 | 56.10 | 53.40 | 38.07 | 68.50 | x | 50.56 |\n| Sinkhorn Trans.* (Tay et al., 2020a) | 33.67 | 61.20 | 53.83 | 41.23 | 67.45 | $x$ | 51.23 |\n| BigBird* (Zaheer et al., 2020) | 36.05 | 64.02 | 59.29 | 40.83 | 74.87 | x | 55.01 |\n| Linear Trans.* (Katharopoulos et al., 2020) | 16.13 | 65.90 | 53.09 | 42.34 | 75.30 | x | 50.46 |\n| Performer* (Choromanski et al., 2020) | 18.01 | 65.40 | 53.82 | 42.77 | 77.05 | x | 51.18 |\n| FNet** (Lee-Thorp et al., 2021) | 35.33 | 65.11 | 59.61 | 38.67 | 77.80 | x | 54.42 |\n| Nystr\u00f6mformer** (Xiong et al., 2021) | 37.15 | 65.52 | 79.56 | 41.58 | 70.94 | x | 57.46 |\n| Luna-256** Ma et al. (2021) | 37.25 | 64.57 | 79.29 | 47.38 | 77.72 | x | 59.37 |\n| H-Transformer-1D** (Zhu and Soricut, 2021) | 49.53 | 78.69 | 63.99 | 46.05 | 68.78 | x | 61.41 |\n| CDIL (Cheng et al., 2022) | 44.05 | 86.78 | 85.36 | 66.91 | 91.70 | x | 74.96 |\n| DSS (Gupta, 2022) | 57.6 | 76.6 | 87.6 | 85.8 | 84.1 | 85.0 | 79.45 |\n| S4 (original) ${ }^{* *}$ (Gu et al., 2022a) | 58.35 | 76.02 | 87.09 | 87.26 | 86.05 | 88.10 | 80.48 |\n| S4-LegS (Gu et al., 2022b) | $59.60(0.07)$ | $86.82(0.13)$ | $90.90(0.15)$ | $88.65(0.23)$ | $94.20(0.25)$ | 96.35 | 86.09 |\n| S4-FouT (Gu et al., 2022b) | $57.88(1.90)$ | $86.34(0.31)$ | $89.66(0.88)$ | $89.07(0.19)$ | $94.46(0.26)$ | x | 77.90 |\n| S4-LegS/FouT (Gu et al., 2022c) | $60.45(0.75)$ | $86.78(0.26)$ | $90.30(0.28)$ | $89.00(0.26)$ | $\\underline{94.44}(0.08)$ | X | 78.50 |\n| S4D-LegS (Gu et al., 2022b) | $60.47(0.34)$ | $86.18(0.43)$ | $89.46(0.14)$ | $88.19(0.26)$ | $93.06(1.24)$ | 91.95 | 84.89 |\n| S4D-Inv (Gu et al., 2022b) | $60.18(0.35)$ | $87.34(0.20)$ | $\\underline{91.09}(0.01)$ | $87.83(0.37)$ | $93.78(0.25)$ | 92.80 | 85.50 |\n| S4D-Lin (Gu et al., 2022b) | $60.52(0.51)$ | $86.97(0.23)$ | $90.96(0.09)$ | $87.93(0.34)$ | $93.96(0.60)$ | x | 78.39 |\n| S5 (Smith et al., 2022) | 61.00 | 86.51 | 88.26 | 86.14 | 87.57 | 85.25 | 82.46 |\n| Liquid-S4 (ours) | $62.75(0.2)$ | 89.02 (0.04) | $91.20(0.01)$ | $89.50(0.4)$ | $94.8(0.2)$ | 96.66(0.001) | 87.32 |\n|  | $p=5$ | $\\mathrm{p}=6$ | $\\mathrm{p}=2$ | $\\mathrm{p}=3$ | $\\mathrm{p}=2$ | $\\mathrm{p}=2$ |  |\n\n$($ Liquid-S4 - PB $) \\quad \\overline{\\mathbf{K}}_{\\text {liquid }=p} \\in \\mathbb{R}^{\\tilde{L}}:=\\mathcal{K}_{L}(\\overline{\\mathbf{C}}, \\overline{\\mathbf{B}}):=\\left(\\overline{\\mathbf{C B}}^{p}\\right)_{i \\in[\\tilde{L}], p \\in[\\mathcal{P}]}$\nWe call this kernel Liquid-S4 - PB, as it is obtained by powers of the vector $\\overline{\\boldsymbol{B}}$. The computational steps to get this kernel is outlined in Algorithm 1 lines 11 to 15. Computational Complexity of the Liquid-S4 Kernel. The computational complexity of the S4-Legs Convolutional kernel solved via the Cauchy Kernel is $\\tilde{\\mathcal{O}}(N+L)$, where N is the state-size, and L is the sequence length [Gu et al. (2022a), Theorem 3]. Liquid-S4 both in KB and PB modes can be computed in $\\tilde{\\mathcal{O}}\\left(N+L+p_{\\max } \\tilde{L}\\right)$. The added time complexity in practice is tractable. This is because we usually select the liquid orders, $p$, to be less than 10 (typically $p_{\\max }=3$, and $\\tilde{L}$ which is the number of terms we use to compute the input correlation vector, $u_{\\text {correlation }}$, is typically two orders of magnitude smaller than the seq length. ## 4. Experiments with Liquid-S4\n\nIn this section, we present an extensive evaluation of Liquid-S4 on sequence modeling tasks with very long-term dependencies and compare its performance to a large series of baselines ranging from advanced Transformers\n\nTable 2: Performance on BIDMC Vital Signs dataset. Numbers indicate RMSE on the test set. The accuracy of models denoted by $*$ is reported from (Gu et al., 2022b). The rest of the models' performance results are reported from the cited paper. |  | BIDMC |  |  |\n| :--- | :---: | :---: | :---: |\n| Model | HR | RR | SPO2 |\n| UnICORNN (Rusch and Mishra, 2021) | 1.39 | 1.06 | 0.869 |\n| coRNN (Rusch and Mishra, 2021) | 1.81 | 1.45 | - |\n| CKConv* | 2.05 | 1.214 | 1.051 |\n| NRDE (Morrill et al., 2021) | 2.97 | 1.49 | 1.29 |\n| LSTM* $^{*}$ | 10.7 | 2.28 | - |\n| Transformer* | 12.2 | 2.61 | 3.02 |\n| XGBoost (Tan et al., 2021) | 4.72 | 1.67 | 1.52 |\n| Random Forest (Tan et al., 2021) | 5.69 | 1.85 | 1.74 |\n| Ridge Regress. (Tan et al., 2021) | 17.3 | 3.86 | 4.16 |\n| S4-LegS* (Gu et al., 2022b) | $0.332(0.013)$ | $0.247(0.062)$ | $0.090(0.006)$ |\n| S4-FouT* (Gu et al., 2022b) | $0.339(0.020)$ | $0.301(0.030)$ | $\\underline{0.068}(0.003)$ |\n| S4D-LegS* (Gu et al., 2022b) | $0.367(0.001)$ | $0.248(0.036)$ | $0.102(0.001)$ |\n| S4-(LegSS/FouT) ${ }^{*}$ (Gu et al., 2022b) | $0.344(0.032)$ | $\\underline{0.163}(0.008)$ | $0.080(0.007)$ |\n| S4D-Inv* (Gu et al., 2022b) | $0.373(0.024)$ | $0.254(0.022)$ | $0.110(0.001)$ |\n| S4D-Lin* (Gu et al., 2022b) | $0.379(0.006)$ | $0.226(0.008)$ | $0.114(0.003)$ |\n| Liquid-S4 (ours) | $\\mathbf{0 .",
    "liquids4-5": "3 0 3}(0.002)$ | $\\mathbf{0 . 1 5 8}(0.001)$ | $\\mathbf{0 . 0 6 6}(0.002)$ |\n|  | $\\mathrm{p}=3$ | $\\mathrm{p}=2$ | $\\mathrm{p}=4$ |\n\nand Convolutional networks to many variants of State-space models. In the following, we first outline the baseline models we compare against. We then list the datasets we evaluated these models on and finally present results and discussions. Baselines. We consider a broad range of advanced models to compare liquid-S4 with. These baselines include transformer variants such as vanilla Transformer (Vaswani et al., 2017), Sparse Transformers (Child et al., 2019), a Transformer model with local attention (Tay et al., 2020b), Longformer (Beltagy et al., 2020), Linformer (Wang et al., 2020), Reformer (Kitaev et al., 2019), Sinkhorn Transformer (Tay et al., 2020a), BigBird (Zaheer et al., 2020), Linear Transformer (Katharopoulos et al., 2020), and Performer (Choromanski et al., 2020). We also include architectures such as FNets (Lee-Thorp et al., 2021), Nystrom\u0308former (Xiong et al., 2021), Luna-256 (Ma et al., 2021), H-Transformer-1D (Zhu and Soricut, 2021), and Circular Diluted Convolutional neural networks (CDIL) (Cheng et al., 2022). We then include a full series of state-space models and their variants such as diagonal SSMs (DSS) (Gupta, 2022), S4 (Gu et al., 2022a), S4-legS, S4-FouT, S4-LegS/FouT (Gu et al., 2022c), S4D-LegS (Gu et al., 2022b), S4D-Inv, S4D-Lin and the Simplified Structural State-space models (S5) (Smith et al., 2022). Datasets. We first evaluate Liquid-S4's performance on the well-studied Long Range Arena (LRA) benchmark (Tay et al., 2020b), where Liquid-S4 outperforms other S4 and S4D variants in every task pushing the state-of-the-art further with an average accuracy of $\\mathbf{8 7 . 3 2 \\%}$. LRA dataset includes six tasks with sequence lengths ranging from 1 k to 16 k . We then report Liquid-S4's performance compared to other S4, and S4D variants as well as other models, on the BIDMC Vital Signals dataset (Goldberger et al., 2000; Pimentel et al., 2016). BIDMC uses bio-marker signals of length 4000 to predict Heart rate (HR), respiratory rate (RR), and blood oxygen saturation (SpO2). We also experiment with the sCIFAR dataset that consists of the classification of flattened images in the form of 1024-long sequences into 10 classes. Finally, we perform RAW Speech Command (SC) recognition with FULL 35 LABELS as conducted\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_a8d8c5746fd07c9f7452g-10.jpg?height=471&width=1119&top_left_y=292&top_left_x=508)\n\nFig. 1: Performance vs Liquid Order in Liquid-S4 for A) ListOps, and B) IMDB datasets. ( $\\mathrm{n}=3$ )\nvery recently in the updated S 4 article (Gu et al., 2022a). ${ }^{\\ddagger}$ SC dataset contains sequences of length 16 k to be classified into 35 commands. Gu et al. (2022a) introduced a new test case setting to assess the performance of models (trained on 16 kHz sequences) on sequences of length 8 kHz . S4 and S4D perform exceptionally well in this zero-shot test scenario. ### 4.1. Results on Long Range Arena\n\nTable $\\mathbf{1}$ depicts a comprehensive list of baselines benchmarked against each other on six long-range sequence modeling tasks in LRA. We observe that Liquid-S4 instances (all use the PB kernel with a scaled Legendre (LegS) configuration) with a small liquid order, $p$, ranging from 2 to 6 , consistently outperform all baselines in all six tasks, establishing the new SOTA on LRA with an average performance of $\\mathbf{8 7 . 3 2 \\%}$. In particular, on ListOps, Liquid-S4 improves S4-LegS performance by more than $3 \\%$, on character-level IMDB by $2.2 \\%$, and on 1-D pixel-level classification (CIFAR) by $0.65 \\%$, while establishing the-state-of the-art on the hardest LRA task by gaining $\\mathbf{9 6 . 5 4 \\%}$ accuracy. Liquid-S4 performs on par with improved S4 and S4D instances on both AAN and Pathfinder tasks. The performance of SSM models is generally well-beyond what advanced Transformers, RNNs, and Convolutional networks achieve on LRA tasks, with the Liquid-S4 variants standing on top. It is worth noting that Liquid-S4 kernels perform better with smaller kernel sizes (See more details on this in Appendix); For instance, on ListOps and IMDB, their individual liquid-S4 kernel state-size could be as small as seven units. This significantly reduces the parameter count in Liquid-S4 in comparison to other variants. The impact of increasing Liquid Order $p$. Figure $\\mathbf{1}$ illustrates how increasing the liquid order, $p$, can consistently improve performance on ListOps and IMDB tasks from LRA. ### 4.2. Results on BIDMC Vital Signs\n\nTable $\\mathbf{2}$ demonstrates the performance of a variety of classical and advanced baseline models on the BIDMC dataset for all three heart rate $(\\mathrm{HR})$, respiratory rate $(\\mathrm{RR})$, and blood oxygen saturation $(\\mathrm{SpO} 2)$ level prediction tasks. We observe that Liquid-s4 with a PB kernel of order $p=3, p=2$, and $p=4$, perform better than all S 4 and S4D variants. It is worth denoting that Liquid-S4 is built by the same parametrization as S4-LegS (which is the official S 4 model reported in the updated S 4 report ( Gu et al., 2022a)). In RR, Liquid-S4 outperforms S4-LegS by a significant margin of $36 \\%$. On SpO2, Liquid-S4 performs $26.67 \\%$ better than S4-Legs. On\n\n[^2]Table 3: Performance on sCIFAR dataset. Numbers indicate Accuracy (standard deviation). The accuracy of baseline models is reported from Table 9 of Gu et al. (2022b). | Model | Accuracy |\n| :--- | :---: |\n| Transformer (Trinh et al., 2018) | 62.2 |\n| FlexConv (Romero et al., 2021a) | 80.82 |\n| TrellisNet (Bai et al., 2018) | 73.42 |\n| LSTM (Hochreiter and Schmidhuber, 1997) | 63.01 |\n| r-LSTM (Trinh et al., 2018) | 72.2 |\n| UR-GRU (Gu et al., 2020b) | 74.4 |\n| HiPPO-RNN (Gu et al., 2020a) | 61.1 |\n| LipschitzRNN (Erichson et al., 2021) | 64.2 |\n| S4-LegS (Gu et al., 2022b) | $91.80(0.43)$ |\n| S4-FouT (Gu et al., 2022b) | $91.22(0.25)$ |\n| S4-(LegS/FouT) (Gu et al., 2022b) | $91.58(0.17)$ |\n| S4D-LegS (Gu et al., 2022b) | $89.92(1.69)$ |\n| S4D-Inv (Gu et al., 2022b) | $90.69(0.06)$ |\n| S4D-Lin (Gu et al., 2022b) | $90.42(0.03)$ |\n| S5 Smith et al. (2022) | 89.66 |\n| Liquid-S4 (ours) | $92.02(0.14)$ |\n|  | $\\mathrm{p}=3$ |\n\nHR, Liquid-S4 outperforms S4-Legs by $8.7 \\%$ improvement in performance.",
    "liquids4-6": "The hyperparameters are given in Appendix. ### 4.3. Results on 1-D Pixel-level Image Classification\n\nSimilar to the previous tasks, a Liquid-S4 network with PB kernel of order $p=3$ outperforms all variants of S4 and S4D while being significantly better than Transformer and RNN baselines as summarized in Table 3. The hyperparameters are given in Appendix. ### 4.4. Results on Speech Commands\n\nTable 4 demonstrates that Liquid-S4 with liquid order $p=2$ achieves the best performance amongst all benchmarks on the 16 KHz testbed with full dataset. Liquid-S4 also performs competitively on the half-frequency zero-shot experiment, while it does not realize the best performance. Although the task is solved to a great degree, the reason could be that liquid kernel accounts for covariance terms. This might influence the learned representations in a way that hurts performance by a small margin in this zero-shot experiment. The hyperparameters are given in Appendix. It is essential to denote that there is a modified speech command dataset that restricts the dataset to only ten output classes, namely SC10, and is used in a couple of works (see for example ( Gu et al., 2021; Kidger et al., 2020; Romero et al., 2021a,b)). Aligned with the updated results reported in ( Gu et al., 2022a) and (Gu et al., 2022b), we choose not to break down this dataset and report the full-sized benchmark in the main paper. Nevertheless, we conducted an experiment with SC10 and showed that even on the reduced dataset, with the same hyperparameters, we solved the task with a SOTA accuracy of $98.51 \\%$. The results are presented in Table S2. Table 4: Performance on RAW Speech Command dataset with FULL 35 Labels and with the reduced ten classes.Numbers indicate validation accuracy. The accuracy of baseline models is reported from Table 11 of (Gu et al., 2022b). |  |  | SC FULL Labels |  |\n| :--- | :---: | :---: | :---: |\n| Model | Parameters | 16 kHz | 8 kHz |\n| InceptionNet (Nonaka and Seita, 2021) | 481 K | $61.24(0.69)$ | $05.18(0.07)$ |\n| ResNet-18 | 216 K | $77.86(0.24)$ | $08.74(0.57)$ |\n| XResNet-50 | 904 K | $83.01(0.48)$ | $07.72(0.39)$ |\n| ConvNet | 26.2 M | $95.51(0.18)$ | $07.26(0.79)$ |\n| S4-LegS (Gu et al., 2022b) | 307 K | $96.08(0.15)$ | $91.32(0.17)$ |\n| S4-FouT (Gu et al., 2022b) | 307 K | $95.27(0.20)$ | $91.59(0.23)$ |\n| S4-(LegS/FouT) (Gu et al., 2022b) | 307 K | $95.32(0.10)$ | $90.72(0.68)$ |\n| S4D-LegS (Gu et al., 2022b) | 306 K | $95.83(0.14)$ | $91.08(0.16)$ |\n| S4D-Inv (Gu et al., 2022b) | 306 K | $\\underline{96.18}(0.27)$ | $\\mathbf{9 1 . 8 0}(0.24)$ |\n| S4D-Lin (Gu et al., 2022b) | 306 K | $\\underline{96.25}(0.03)$ | $\\underline{91.58}(0.33)$ |\n| Liquid-S4 (ours) | $\\mathbf{2 2 4 K}$ | $\\mathbf{9 6 . 7 8 ( 0 . 0 5 )}$ | $90.00(0.25)$ |\n|  |  | $\\mathrm{p}=2$ | $\\mathrm{p}=2$ |\n\n## 5. Conclusions\n\nWe showed that structural state-space models could be considerably improved in performance if they are formulated by a linear liquid time-constant kernel, namely Liquid-S4. Liquid-S4 kernels are obtainable with minimal effort with their kernel computing the similarities between time-lags of the input signals in addition to the main S4 diagonal plus low-rank parametrization. Liquid-S4 kernels with Smaller parameter counts achieve SOTA performance on all six tasks of the Long-range arena dataset, on BIDMC heart rate, respiratory rate, and blood oxygen saturation, on sequential 1-D pixel-level image classification, and on Speech command recognition. ## Acknowledgments\n\nThis work is supported by The Boeing Company and the Office of Naval Research (ONR) Grant N00014-18$1-2830$. ## References\n\nZ. Allen-Zhu and Y. Li. Can sgd learn recurrent neural networks with provable generalization? In Advances in Neural Information Processing Systems, pages 10331-10341, 2019.",
    "liquids4-7": "J. M. Amig\u00f3, R. Monetti, T. Aschenbrenner, and W. Bunk. Transcripts: An algebraic approach to coupled time series. Chaos: An Interdisciplinary Journal of Nonlinear Science, 22(1):013105, 2012. S. Bai, J. Z. Kolter, and V. Koltun. Trellis networks for sequence modeling. In International Conference on Learning Representations, 2018.",
    "liquids4-8": "F. W. Belletti, E. R. Sparks, M. J. Franklin, A. M. Bayen, and J. E. Gonzalez. Scalable linear causal inference for irregularly sampled time series with long range dependencies.",
    "liquids4-9": "arXiv preprint arXiv:1603.03336, 2016. I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994.",
    "liquids4-10": "S. G. Brush. History of the lenz-ising model. Reviews of modern physics, 39(4):883, 1967. S. Chang, Y. Zhang, W. Han, M. Yu, X. Guo, W. Tan, X. Cui, M. Witbrock, M. A. Hasegawa-Johnson, and T. S. Huang. Dilated recurrent neural networks. Advances in neural information processing systems, 30, 2017. B. Charlier, J. Feydy, J. A. Glaun\u00e8s, F.-D. Collin, and G. Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1-6, 2021. URL http:// jmlr.org/papers/v22/20-275.html. Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu. Recurrent neural networks for multivariate time series with missing values. Scientific reports, 8(1):1-12, 2018. D. Chen, L. Jacob, and J. Mairal. Recurrent kernel networks. In Advances in Neural Information Processing Systems, pages 13431-13442, 2019.",
    "liquids4-11": "T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. In Advances in neural information processing systems, pages 6571-6583, 2018. L. Cheng, R. Khalitov, T.",
    "liquids4-12": "Yu, and Z. Yang. Classification of long sequential data using circular dilated convolutional neural networks. arXiv preprint arXiv:2201.02143, 2022. R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.",
    "liquids4-13": "K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020. J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. E. De Brouwer, J. Simm, A. Arany, and Y. Moreau. Gru-ode-bayes: Continuous modeling of sporadicallyobserved time series. In Advances in Neural Information Processing Systems, pages 7377-7388, 2019. E. Dupont, A. Doucet, and Y.",
    "liquids4-14": "W. Teh. Augmented neural odes. In Advances in Neural Information Processing Systems, pages 3134-3144, 2019. C. Durkan, A. Bekasov, I. Murray, and G. Papamakarios. Neural spline flows. In Advances in Neural Information Processing Systems, pages 7509-7520, 2019.",
    "liquids4-15": "N. B. Erichson, O. Azencot, A. Queiruga, L. Hodgkinson, and M. W. Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=-N7PBXqOUJZ. G. Foster. Wavelets for period analysis of unevenly sampled time series. The Astronomical Journal, 112:1709, 1996.",
    "liquids4-16": "K. J. Friston, L. Harrison, and W. Penny. Dynamic causal modelling. Neuroimage, 19(4):1273-1302, 2003.",
    "liquids4-17": "P. K. Friz and N.",
    "liquids4-18": "B. Victoir. Multidimensional stochastic processes as rough paths: theory and applications, volume 120. Cambridge University Press, 2010. K.-i. Funahashi and Y. Nakamura. Approximation of dynamical systems by continuous time recurrent neural networks. Neural networks, 6(6):801-806, 1993.",
    "liquids4-19": "A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H.",
    "liquids4-20": "E. Stanley. Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals.",
    "liquids4-21": "circulation, 101(23):e215-e220, 2000.",
    "liquids4-22": "A. Greaves-Tunnell and Z. Harchaoui. A statistical investigation of long memory in language and music. In International Conference on Machine Learning, pages 2394-2403, 2019. K. Greff, R. K. Srivastava, J. Koutn\u00edk, B. R. Steunebrink, and J. Schmidhuber. Lstm: A search space odyssey. IEEE transactions on neural networks and learning systems, 28(10):2222-2232, 2016.",
    "liquids4-23": "S. A. Gruenbacher, M. Lechner, R. Hasani, D. Rus, T. A. Henzinger, S. A. Smolka, and R. Grosu. Gotube: Scalable statistical verification of continuous-depth models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, No 6, pages 6755-6764, 2022. S. Grunbacher, R. Hasani, M. Lechner, J. Cyranka, S. A. Smolka, and R. Grosu. On the verification of neural odes with stochastic guarantees. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, No 13, pages 11525-11535, 2021. A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Re. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33, 2020a. A. Gu, C. Gulcehre, T. Paine, M. Hoffman, and R. Pascanu. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pages 3800-3809. PMLR, 2020b. A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34,2021 .",
    "liquids4-24": "A. Gu, K. Goel, and C. Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022a. URL https://openreview. net/forum? id=uYLFOz1v1AC. A. Gu, A. Gupta, K. Goel, and C. R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022b. A. Gu, I. Johnson, A. Timalsina, A. Rudra, and C. R\u00e9. How to train your hippo: State space models with generalized orthogonal basis projections. arXiv preprint arXiv:2206.12037, 2022c.",
    "liquids4-25": "A. Gupta. Diagonal state spaces are as effective as structured state spaces. arXiv preprint arXiv:2203.14343, 2022. Y. Hanshu, D. Jiawei, T.",
    "liquids4-26": "Vincent, and F. Jiashi. On robustness of neural ordinary differential equations. In International Conference on Learning Representations, 2020. R. Hasani, A. Amini, M. Lechner, F. Naser, R. Grosu, and D. Rus. Response characterization for auditing cell dynamics in long short-term memory networks. In 2019 International Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE, 2019. R. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu. The natural lottery ticket winner: Reinforcement learning with ordinary neural circuits. In Proceedings of the 2020 International Conference on Machine Learning. JMLR. org, 2020. R. Hasani, M. Lechner, A. Amini, L. Liebenwein, M. Tschaikowski, G. Teschl, and D. Rus. Closed-form continuous-depth models. arXiv preprint arXiv:2106.13898, 2021a. R. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu. Liquid time-constant networks. Proceedings of the AAAI Conference on Artificial Intelligence, 35(9):7657-7666, May 2021b. S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen [in german] diploma thesis. TU M\u00fcnich, 1991. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. P. Holl, V. Koltun, and N. Thuerey. Learning to control pdes with differentiable physics. arXiv preprint arXiv:2001.07457, 2020.",
    "liquids4-27": "J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554-2558, 1982. J. Jia and A. R. Benson. Neural jump stochastic differential equations. In Advances in Neural Information Processing Systems, pages 9843-9854, 2019. L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark, M. Soljacic, and Y. Bengio. Gated orthogonal recurrent units: On learning to forget. Neural computation, 31(4):765-783, 2019. A. Kag, Z. Zhang, and V. Saligrama. Rnns incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients? In International Conference on Learning Representations, 2019. R. KALMAN. A new approach to linear filtering and prediction problems.",
    "liquids4-28": "J. Basic Eng., Trans. ASME, D, 82: $35-45,1960$.",
    "liquids4-29": "A. Katharopoulos, A. Vyas, N.",
    "liquids4-30": "Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156-5165. PMLR, 2020. P. Kidger, J. Morrill, J. Foster, and T. Lyons. Neural controlled differential equations for irregular time series. Advances in Neural Information Processing Systems, 33:6696-6707, 2020. N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019.",
    "liquids4-31": "D. R. Kowal, D. S. Matteson, and D. Ruppert. Functional autoregression for sparsely sampled data. Journal of Business \\& Economic Statistics, 37(1):97-109, 2019. C. Lea, R. Vidal, A. Reiter, and G.",
    "liquids4-32": "D. Hager. Temporal convolutional networks: A unified approach to action segmentation. In European Conference on Computer Vision, pages 47-54. Springer, 2016. M. Lechner and R. Hasani. Mixed-memory rnns for learning long-term dependencies in irregularly sampled time series. OpenReview, 2021. M. Lechner, R. Hasani, M. Zimmer, T. A. Henzinger, and R. Grosu. Designing worm-inspired neural networks for interpretable robotic control. In 2019 International Conference on Robotics and Automation (ICRA), pages $87-94$. IEEE, 2019. M. Lechner, R. Hasani, A. Amini, T. A. Henzinger, D. Rus, and R. Grosu. Neural circuit policies enabling auditable autonomy. Nature Machine Intelligence, 2(10):642-652, 2020a. M. Lechner, R. Hasani, D. Rus, and R. Grosu. Gershgorin loss stabilizes the recurrent neural network compartment of an end-to-end robot learning scheme. In 2020 International Conference on Robotics and Automation (ICRA). IEEE, 2020b. J. Lee-Thorp, J. Ainslie, I. Eckstein, and S. Ontanon. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824, 2021. S. Li, W. Li, C. Cook, C.",
    "liquids4-33": "Zhu, and Y. Gao. Independently recurrent neural network (indrnn): Building a longer and deeper rnn. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $5457-5466,2018$.",
    "liquids4-34": "S. C.-X. Li and B.",
    "liquids4-35": "M. Marlin. A scalable end-to-end gaussian process adapter for irregularly sampled time series classification. In Advances in neural information processing systems, pages 1804-1812, 2016. L. Liebenwein, R. Hasani, A. Amini, and D. Rus. Sparse flows: Pruning continuous-depth models. Advances in Neural Information Processing Systems, 34:22628-22642, 2021.",
    "liquids4-36": "W. A. Little. The existence of persistent states in the brain. Mathematical biosciences, 19(1-2):101-120, 1974. X. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441-2453, 2021. S. Massaroli, M. Poli, J. Park, A. Yamashita, and H. Asama. Dissecting neural odes. Advances in Neural Information Processing Systems, 33:3952-3963, 2020. H. Mei and J. M. Eisner. The neural hawkes process: A neurally self-modulating multivariate point process. In Advances in Neural Information Processing Systems, pages 6754-6764, 2017. J. Morrill, P. Kidger, C. Salvi, J. Foster, and T. Lyons. Neural cdes for long time series via the log-ode method. arXiv preprint arXiv:2009.08295, 2020. J. Morrill, C. Salvi, P. Kidger, and J. Foster. Neural rough differential equations for long time series. In International Conference on Machine Learning, pages 7829-7838. PMLR, 2021. D. Neil, M. Pfeiffer, and S.-C. Liu. Phased lstm: Accelerating recurrent network training for long or event-based sequences.",
    "liquids4-37": "In Advances in neural information processing systems, pages 3882-3890, 2016.",
    "liquids4-38": "N. Nonaka and J. Seita. In-depth benchmarking of deep neural network architectures for ecg diagnosis. In Machine Learning for Healthcare Conference, pages 414-439. PMLR, 2021. R. Pearson, G. Goney, and J. Shwaber. Imbalanced clustering for microarray time-series. In Proceedings of the ICML, volume 3, 2003. W. Penny, Z. Ghahramani, and K. Friston. Bilinear dynamical systems. Philosophical Transactions of the Royal Society B: Biological Sciences, 360(1457):983-993, 2005.",
    "liquids4-39": "M. A. Pimentel, A. E. Johnson, P. H. Charlton, D. Birrenkott, P.",
    "liquids4-40": "J. Watkinson, L. Tarassenko, and D.",
    "liquids4-41": "A. Clifton. Toward a robust estimation of respiratory rate from pulse oximeters. IEEE Transactions on Biomedical Engineering, 64(8):1914-1923, 2016. A. Quaglino, M. Gallieri, J. Masci, and J. Koutn\u00c3k. Snode: Spectral discretization of neural odes for system identification. In International Conference on Learning Representations, 2020. H. Ramsauer, B. Sch\u00e4fl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, T. Adler, D. Kreil, M. K. Kopp, et al. Hopfield networks is all you need. In International Conference on Learning Representations, 2020 .",
    "liquids4-42": "D. W. Romero, R.-J. Bruintjes, J. M. Tomczak, E. J. Bekkers, M. Hoogendoorn, and J. van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. In International Conference on Learning Representations, 2021a.",
    "liquids4-43": "D. W. Romero, A. Kuzina, E. J. Bekkers, J. M. Tomczak, and M. Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data. In International Conference on Learning Representations, 2021b. D. Roy and L. Yan. Robust landsat-based crop time series modelling. Remote Sensing of Environment, 238: 110810, 2020. Y. Rubanova, T. Q. Chen, and D. K. Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems, pages 5321-5331, 2019.",
    "liquids4-44": "D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors.",
    "liquids4-45": "nature, 323(6088):533-536, 1986.",
    "liquids4-46": "T. K. Rusch and S. Mishra. Coupled oscillatory recurrent neural network ( $c o\\{\\mathrm{rnn}\\}$ ): An accurate and (gradient) stable architecture for learning long time dependencies. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=F3s69XzWOia. A. Sherstinsky. Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network. Physica D: Nonlinear Phenomena, 404:132306, 2020.",
    "liquids4-47": "J. T. Smith, A. Warrington, and S.",
    "liquids4-48": "W. Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. C. W. Tan, C. Bergmeir, F. Petitjean, and G. I. Webb. Time series extrinsic regression. Data Mining and Knowledge Discovery, 35(3):1032-1060, 2021. Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438-9447. PMLR, 2020a. Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020b. T. Trinh, A. Dai, T. Luong, and Q. Le. Learning longer-term dependencies in rnns with auxiliary losses. In International Conference on Machine Learning, pages 4965-4974. PMLR, 2018. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. A. Voelker, I. Kaji\u0107, and C. Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. C. Vorbach, R. Hasani, A. Amini, M. Lechner, and D. Rus. Causal navigation by continuous-time neural networks. Advances in Neural Information Processing Systems, 34, 2021. C. Wang and M. Niepert. State-regularized recurrent neural networks. In International Conference on Machine Learning, pages 6596-6606, 2019. S. Wang, B. Z. Li, M. Khabsa, H.",
    "liquids4-49": "Fang, and H. Ma. Linformer: Self-attention with linear complexity.",
    "liquids4-50": "arXiv preprint arXiv:2006.04768, 2020. S. Wisdom, T. Powers, J. Hershey, J. Le Roux, and L. Atlas. Full-capacity unitary recurrent neural networks. Advances in neural information processing systems, 29:4880-4888, 2016. Y. Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y. Li, and V. Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, No 16, pages 14138-14148, 2021.",
    "liquids4-51": "M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283-17297, 2020. Z. Zhu and R. Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3801-3815, 2021. ## Supplementary Materials\n\n## S1. Proof of Proposition 1\n\nProof. This can be shown by unrolling the S 4 convolution kernel and multiplying its components with $\\overline{\\mathbf{B}}^{p-1}$, performing an anti-diagonal transformation to obtain the corresponding liquid S 4 kernel :\n\n$$\n\\overline{\\mathbf{K}}=\\left(\\overline{\\mathbf{C B}}, \\overline{\\mathbf{C A B}}, \\overline{\\mathbf{C A}}^{2} \\overline{\\mathbf{B}}, \\ldots, \\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}\\right)\n$$\n\nFor $p=2$ (correlations of order 2 ), S 4 kernel should be multiplied by $\\overline{\\boldsymbol{B}}$. The resulting kernel would be:\n\n$$\n\\left(\\overline{\\mathbf{C B}}^{2}, \\overline{\\mathbf{C A}}^{2}, \\overline{\\mathbf{C A}}^{2} \\overline{\\mathbf{B}}^{2}, \\ldots, \\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}^{2}\\right)\n$$\n\nWe obtain the liquid kernel by flipping the above kernel to be convolved with the 2-term correlation terms $(\\mathrm{p}=2)$ :\n\n$$\n\\overline{\\mathbf{K}}_{\\text {liquid }=2}=\\left(\\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}^{2}, \\ldots, \\overline{\\mathbf{C A}}^{2} \\overline{\\mathbf{B}}^{2}, \\overline{\\mathbf{C A B}}^{2}, \\overline{\\mathbf{C B}}^{2}\\right)\n$$\n\nSimilarly, we can obtain liquid kernels for higher liquid orders and obtain the statement of the proposition. ## S2. Hyperparameters\n\nLearning Rate. Liquid-S4 generally requires a smaller learning rate compared to S 4 and S 4 D blocks. Setting $\\Delta t_{\\max }$ and $\\Delta t_{\\min }$ We set $\\Delta t_{\\max }$ for all experiments to 0.2 , while the $\\Delta t_{\\min }$ was set based on the recommendations provided in (Gu et al., 2022c) to be proportional to $\\propto \\frac{1}{\\text { seq length }}$. Causal Modeling vs. Bidirectional Modeling Liquid-S4 works better when it is used as a causal model, i.e., with no bidirectional configuration. $d_{s}$ tate We observed that liquid-S4 PB kernel performs best with smaller individual state sizes $d_{s} t a t e$. For instance, we achieve SOTA results in ListOps, IMDB, and Speech Commands by a state size set to 7, significantly reducing the number of required parameters to solve these tasks. Choice of Liquid-S4 Kernel In all experiments, we choose our simplified PB kernel over the KB kernel due to the computational costs and performance. We recommend the use of PB kernel. Choice of parameter $p$ in liquid kernel. In all experiments, start off by setting $p$ or the liquidity order to 2 . This means that the liquid kernel is going to be computed only for correlation terms of order 2 . In principle, we observe that higher $p$ values consistently enhance the representation learning capacity of liquid-S4 modules, as we showed in all experiments. We recommend $p=3$ as a norm to perform experiments with Liquid-S4. The kernel computation pipeline uses the PyKeops package (Charlier et al., 2021) for large tensor computations without memory overflow. All reported results are validation accuracy (similar to Gu et al. (2022a)) performed with 2 to 3 different random seeds, except for the BIDMC dataset, which reports accuracy on the test set. Table S1: Hyperparameters for obtaining best performing models. $\\mathrm{BN}=$ Batch Normalization, $\\mathrm{LN}=$ Layer normalization, $\\mathrm{WD}=$ Weight decay. |  | Depth | Features $H$ | State Size | Norm | Pre-norm | Dropout | LR | Batch Size | Epochs | WD |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| ListOps | 9 | 128 | 7 | BN | True | 0.01 | 0.002 | 12 | 30 | 0.03 |\n| Text (IMDB) | 4 | 128 | 7 | BN | True | 0.1 | 0.003 | 8 | 50 | 0.01 |\n| Retrieval (AAN) | 6 | 256 | 64 | BN | False | 0.2 | 0.005 | 16 | 20 | 0.05 |\n| Image (CIFAR) | 6 | 512 | 512 | LN | False | 0.1 | 0.01 | 16 | 200 | 0.03 |\n| Pathfinder | 6 | 256 | 64 | BN | True | 0.0 | 0.0004 | 4 | 200 | 0.03 |\n| Path-X | 6 | 320 | 64 | BN | True | 0.0 | 0.001 | 8 | 60 | 0.05 |\n| Speech Commands | 6 | 128 | 7 | BN | True | 0.0 | 0.008 | 10 | 50 | 0.05 |\n| BICMD (HR) | 6 | 128 | 256 | LN | True | 0.0 | 0.005 | 32 | 500 | 0.01 |\n| BICMD (RR) | 6 | 128 | 256 | LN | True | 0.0 | 0.01 | 32 | 500 | 0.01 |\n| BICMD (SpO2) | 6 | 128 | 256 | LN | True | 0.0 | 0.01 | 32 | 500 | 0.01 |\n| sCIFAR | 6 | 512 | 512 | LN | False | 0.1 | 0.01 | 50 | 200 | 0.03 |\n\nTable S2: Performance on RAW Speech Command dataset with the reduced ten classes (SC10) dataset.Numbers indicate validation accuracy. The accuracy of baseline models is reported from Table 5 of ( Gu et al., 2022a). x stands for infeasible computation on a single GPU or not applicable as stated in Table 10 of (Gu et al., 2022a). The hyperparameters for LiquidS4 are the same as the ones reported for Speech Commands Full Dataset reported in Table S1. |  | SC10 |  |\n| :--- | :--- | :--- |\n| Model | 16 kHz | 8 kHz |\n| Transformer | x | x |\n| Performer | 30.77 | 30.68 |\n| ODE-RNN | x | x |\n| NRDE | 16.49 | 15.12 |\n| ExpRNN | 11.6 | 10.8 |\n| LipschitzRNN | x | x |\n| CKConv | 71.66 | 65.96 |\n| WaveGAN-D | 96.25 | x |\n| LSSL (Gu et al., 2021) | x | x |\n| S4-LegS (Gu et al., 2022a) | 98.32 | $\\mathbf{9 6 . 3 0}$ |\n| Liquid-S4 (ours) | $\\mathbf{9 8 . 5 1}$ | 95.9 |\n|  | $\\mathrm{p}=2$ | $\\mathrm{p}=2$ |\n\n\n[^0]:    *Code Repository: https://github.com/raminmh/liquid-s4\n\n[^1]:    ${ }^{\\dagger} S \\leftarrow \\frac{2}{\\delta t} \\frac{1-z^{-1}}{1+z^{-1}}$\n\n[^2]:    ${ }^{\\ddagger}$ It is essential to denote that there is a modified speech command dataset that restricted the dataset to only 10 output classes and is used in a couple of works (see for example (Gu et al., 2021; Kidger et al., 2020; Romero et al., 2021a,b)).",
    "liquids4-52": "Aligned with the updated results reported in (Gu et al., 2022a) and (Gu et al., 2022b), we choose not to break down this dataset and use the full-sized benchmark.",
    "liquids4-53": ""
}