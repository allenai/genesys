{
    "based-0": "# Simple linear attention language models balance the recall-throughput tradeoff \n\nSimran Arora* ${ }^{* \\dagger}$, Sabri Eyuboglu* ${ }^{* \\dagger}$ Michael Zhang* ${ }^{* \\dagger}$ Aman Timalsina ${ }^{\\Delta}$, Silas Alberti ${ }^{\\dagger}$,<br>Dylan Zinsley ${ }^{\\ddagger}$, James Zou ${ }^{\\dagger}$, Atri Rudra ${ }^{\\ddagger}$, and Christopher R\u00e9 ${ }^{\\dagger}$<br>${ }^{\\dagger}$ Stanford University<br>${ }^{\\ddagger}$ University at Buffalo<br>${ }^{\\triangle}$ Purdue University<br>$\\dagger$ \\{simran, eyuboglu, mzhang, alberti, jamesz, chrismre\\}@cs.stanford.edu<br>$\\ddagger$ dylanzin, atri\\}@buffalo.edu<br>$\\triangle$ atimalsi\\}@purdue.edu\n\nMarch 1, 2024\n\n\n#### Abstract\n\nRecent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context.",
    "based-1": "However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3 b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make Based competitive, we develop IO-aware algorithms that enable $24 \\times$ higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. ## 1 Introduction\n\nThe choice of sequence mixer (e.g. attention, convolution) in a language model affects both its quality and efficiency [1, 2. Prior work shows that attention excels at recall, the ability to ground generations in previously seen tokens [1, 3]. On the other hand, the throughput of attention-based models is bottle-necked during training by quadratic compute complexity and during inference by aggressive memory consumption. The natural question is: can we improve the real-world speed and memory-use of language models without comprising on quality? Recently, a number of architectures have been proposed that enable substantially higher throughput while matching attention in perplexity [4, 5, 6, 7, 8. However, coarse metrics like overall perplexity can obscure important differences in model quality. For example, recent work shows that a specific class of architectures, gated-convolutions, despite complexity scaling sub-quadratically in sequence length, are less efficient than\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-02.jpg?height=420&width=1620&top_left_y=253&top_left_x=247)\n\nFigure 1: Based overview. Combining linear attention with tiny sliding window softmax attention (e.g., 64 or 128 tokens in width) enables improved recall accuracy with limited efficiency overhead vs. smaller tile sizes. (Left) Time to execute Cutlass GEMMs $(y)$ vs. sliding window attention size $(x)$, with batch size 512 on tensor cores. (Center) Model recall accuracy ( $y$ ) vs. sliding window attention size $(x)$. We compare linear attention alone (dark blue), sliding window attention alone (light blue), and their combination (BASED, orange). (Right) Schematic diagram of BASED illustrating how the two components complement each other. attention at performing recall 1]. Building on this analysis, we evaluate a broader class of architectures across real-world recall-intensive tasks and show attention improves over the best attention-free alternative, Mamba, by 46.7 accuracy points (Table 1). ${ }^{1}$\n\nMotivated by these observations, we explore the pareto frontier of the tradeoff between high-recall and high-throughput models. We evaluate a range of architectures (e.g. attention, SSMs, and convolutions) on a popular synthetic associative recall task [1, 3, 9, Since generation throughput is bottle-necked by memory consumption, we vary hyperparameters (e.g. model dimension) that affect the size of the recurrent state during generation and demonstrate a fundamental recall-memory tradeoff that holds across architecture classes (Figure 2). Attention performs associative recall perfectly, but the recurrent state (i.e. the KV-cache) grows linearly with the sequence length. Sliding window attention can cap the size of the recurrent state at the cost of worse long-range recall 10 . However, Mamba, a recently proposed SSM architecture expands the Pareto frontier beyond sliding window. This begs the question: are there other, perhaps simpler, models that can also expand the pareto frontier? To reduce the memory consumption, we consider using two simple techniques: sliding window attention and softmax-approximating linear attention. Our results on language modeling (Table 1) and synthetic recall experiments (Figure 1, center) suggest neither primitive alone suffices to navigate the Pareto frontier. 1. We find that linear attention alone struggles to solve associative recall (Figure 1, center). We hypothesize that this is because linear attention lacks the precision to perform local token shifts and comparisons [1, 9. 2. In sliding window attention, associative recall range is limited by the width of the windows (Figure 1, center). As we increase the window size, the recurrent state grows linearly and has a non-linear affect on speed during parallel training and inference (Figure 1 , left). We combine these two techniques into a single architecture, which we call BASED (Figure 1, right). We find that sliding window attention and linear attention complement each other, enabling BASED to expand the pareto frontier of the recall-memory tradeoff (Figure 22. We suspect that (1) the large recurrent memory of linear attention could help model long-range token interactions in the sequence and (2) sliding window attention handles the precise local shifts needed to perform associative recall. To make BASED competitive with SoTA attention 11] and recurrent [5 models under wall-clock and throughput metrics, we introduce several IO-aware optimizations. 1. Despite the theoretically improved complexity, linear attention implementations are often slower than well-optimized attention implementations 12 . In BASED, we use the 2nd-order Taylor approximation of softmax as the linear attention feature map With sequence length $N$ and head dimension $d$, this\n\n[^1]na\u00efvely requires $\\mathcal{O}\\left(N d^{3}\\right)$ time and space complexity [13, 14].",
    "based-2": "To make our attention competitive in real-world wall-clock time and memory usage, we provide hardware-efficient algorithms and custom CUDA implementations. Relative to the baseline, our algorithm reduces data movement from HBM (slowerto-acccess memory) to SRAM (faster-to-access memory) by $\\mathcal{O}\\left(N d^{2}\\right)$ bytes and from SRAM to register (fastest memory) by $O\\left(N d^{3}\\right)$ bytes (Section 5 ). 2. Sliding window attention exploits tensor cores, specialized units on modern GPUs for performing matrix multiplications (GEMMs). While popular architectures use long window sizes (e.g. 4096 for Mistral-7B [10]), we choose fixed size 64 windows, guided by hardware properties. In particular, we use just enough occupancy to hide the tensor core kernel launch latency. Although tensor cores operate on $16 \\times 16$ tiles, in Figure 1 (left), we see that the latencies for performing $16 \\times 16 \\mathrm{vs} .64 \\times 64$ (or even $128 \\times 128$ ) dimension matrix multiplications on NVIDIA H100 tensor cores are similar, informing our window size. In experiments, we show that BASED competes in quality with strong Transformer ++15 and SoTA sub-quadratic baselines in models up to the 1.3 Bn parameters across language modeling on the Pile language, DNA modeling, and the LM Eval Harness [16. Beyond this, BASED outperforms prior sub-quadratic architectures on the associative recall slice of the Pile and in downstream recall-intensive tasks by 0.14 perplexity points and 6.22 accuracy points, respectively. In efficiency, BASED enables up to $24 \\times$ higher throughput than the strong FlashAttention-2 implementation on generation. Code for this work is provided at: https://github.com/HazyResearch/based. ## 2 Preliminaries and Related Work\n\nWe discuss the key relevant work in this section and provide an extended discussion in Appendix A\nAttention Popularized by Transformers [2] as the de facto language modeling primitive, softmax attention takes inputs $\\boldsymbol{x} \\in \\mathbb{R}^{N \\times d}$ of length $N$ and head dimension $d$, and computes outputs $\\boldsymbol{y} \\in \\mathbb{R}^{N \\times d}$ via the softmax over projections $\\boldsymbol{q}, \\boldsymbol{k}, \\boldsymbol{v}=\\boldsymbol{x} \\boldsymbol{W}_{q}, \\boldsymbol{x} \\boldsymbol{W}_{k}, \\boldsymbol{x} \\boldsymbol{W}_{v}$, i.e.,\n\n$$\n\\boldsymbol{y}_{i}=\\sum_{j=1}^{i} \\frac{\\exp \\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j} / \\sqrt{d}\\right) \\boldsymbol{v}_{j}}{\\sum_{m=1}^{i} \\exp \\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{m} / \\sqrt{d}\\right)}\n$$\n\nin the causal case where $\\boldsymbol{W}_{q}, \\boldsymbol{W}_{k}, \\boldsymbol{W}_{v} \\in \\mathbb{R}^{d \\times d}$ are learnable matrices. While effective at recall 1 and efficient to train (Eq 1 is parallelizable on GPUs and $\\mathcal{O}(N)$ in memory with recent advances 12), attention remains expensive for generation. For every new output $\\boldsymbol{y}_{n}$, we require $n d$ operations over a growing $K V$-cache of prior $\\left\\{\\boldsymbol{k}_{i}, \\boldsymbol{v}_{i}\\right\\}_{i=1}^{n-1}$. This results in larger memory consumption and lower-throughput for longer sequences. Efficient attentions Various works thus try to improve on attention's efficiency without sacrificing quality. Sparse attentions reduce attention's time and memory requirements by only attending over specific strided patterns or local sliding windows [17, 18, 19. While further popularized in large language models (Mistral, Jiang et al. [10]), prior works either underperform full attention with sparse patterns that fail to capture dense interactions, or use large window sizes that still permit large KV-caches and subsequent inefficiency. Meanwhile, linear attentions replace the softmax in standard attention with alternative kernel functions 14 , 20, 21, 22, 23. By removing the $\\exp \\left(\\boldsymbol{q}^{\\top} \\boldsymbol{k}\\right)$ in favor of feature map dot-products $\\phi(\\boldsymbol{q})^{\\top} \\phi(\\boldsymbol{k})$, these methods use matrix product associativity to compute attention in $\\mathcal{O}\\left(N d^{2}\\right)$ time and space 24. Furthermore, they permit a recurrent view for constant memory and $\\mathcal{O}(1)$ time per-token generation [25, 26. However, present linear attention feature maps either fail to match standard attention on recall or remain expensive to compute 13. Linear attentions also do not achieve faster wall-clock time or lower memory versus modern standard attention in practice 12 . Attention alternatives Finally, various models use attention-free sequence mixers such as state-space models (SSMs) [27, 28, gated convolutions [7, 9] and input-dependent recurrences [5, 8, to rival attention performance while improving its efficiency. However, while recent such models can match attention in overall perplexity, further study suggests they may underperform Transformers on tasks such as recall and in-context learning [1, 29]. ## 3 No Free Lunch: The Memory-Recall Tradeoff\n\nIn this section, we demonstrate a fundamental tradeoff between a model's memory consumption during inference (i.e., the size of its recurrent state) and its capacity to perform recall. We use a combination of experiments on synthetic data and theoretical analysis. - Empirical study of memory-recall tradeoff : In Section 3.1, we evaluate a number of popular architecture classes (e.g. Mamba, Hyena) on a synthetic associative recall task, varying hyperparameters that affect the model's recurrent state size (Figure 2). Within each architecture class, we observe a clear tradeoff: the larger the recurrent state size, the better recall. However, for a fixed recurrent state size, performance is not consistent across architectures. We observe that some sequence mixers fall well-below the pareto-frontier. This motivates the design of sequence mixers that can expand the pareto frontier. - Lower bounds on memory required for recall: In Section 3.2, we lower bound the recurrent state size required to perform exact recall with any recurrent model Theorem F.1. This analysis reinforces our empirical observations on the throughput-recall tradeoff. ### 3.1 Empirical study of memory-recall tradeoff\n\nSetup. We use a synthetic AR task called Multi-Query Associative Recall (MQAR) [1 to demonstrate the tradeoff.",
    "based-3": "In this task, input sequences consist of a number of key-value pairs followed by queries. For a given query, the model must recall the corresponding key-value pair from earlier in the sequence in order to predict the next token. For example, the correct output for input below would be $4,6,1,2,3$ :\n\n$$\n\\text { A } 4 \\text { B } 3 \\text { C } 6 \\underbrace{\\text { F } 1}_{\\text {Key-Value }} \\mathrm{E} 2 \\rightarrow \\text { A? C ? } \\underbrace{\\mathrm{F} ?}_{\\text {Query }} \\mathrm{E} ? \\mathrm{~B} ? $$\n\nWe train on sequences of length 256 tokens containing between 4 and 64 key-value pairs. During evaluation, we measure accuracy on sequences of length 1,024 tokens containing between 4 and 256 key-value pairs. We train and evaluate six sequence mixers: attention [2, sliding window attention 19], Mamba [5], H3 9], Hyena [7, and BASED. For each, we vary hyperparameters that affect the memory consumption during inference. For example, in sliding window attention we vary the window width. We compare how MQAR accuracy varies with the size of the recurrent state. Appendix E. 1 contains details on how the state size is calculated for each architecture. Figures 2 and 3 can be reproduced or extended to new architectures using the scripts provided at https: //github.com/HazyResearch/zoology\n\nResults In Figure 2, we demonstrate a fundamental tradeoff between recurrent state size and accuracy on MQAR that holds within and across architecture classes. Within each architecture class (e.g. H3 models), increasing the recurrent state size almost always leads to an improvement in accuracy. Across architecture classes, we see a tradeoff as well. Attention achieves perfect recall accuracy, but its recurrent state size grows with the length of the sequence. Other architecture classes like Mamba and H3 admit models with much smaller recurrent states, but these models have limited recall capacity. | Architecture | Params | Efficiency |  | Language Modeling (Pile) |  |  | Info. Extraction |  | Question Answering |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | Prefill | Generate | All | AR | Other | SWDE | FDA | SQUAD | Common |\n|  |  | Tok. $/ \\mathrm{ms} \\uparrow$ | Tok. $/ \\mathrm{ms} \\uparrow$ | Ppl. $\\downarrow$ | Ppl. $\\downarrow$ | Ppl. $\\downarrow$ | Acc $\\uparrow$ | Acc $\\uparrow$ | F1 \u4e2a | Avg. Acc. $\\uparrow$ |\n| Transformer++ | $1.33 b$ | 103.50 | 0.99 | 7.26 | 1.74 | 8.10 | 41.97 | 73.23 | 36.19 | 47.64 |\n| BASED | $1.35 b$ | 161.71 | 24.28 | 7.43 | 1.87 | $\\underline{8.26}$ | 30.83 | 24.41 | 30.46 | 46.68 |\n| Mamba | $1.32 b$ | 112.22 | 25.69 | 7.48 | 1.96 | 8.29 | 25.93 | 12.89 | 28.20 | 46.84 |\n| Transformer++ | 360 m | 207.77 | 23.82 | 8.39 | 1.87 | 9.42 | 37.62 | 58.00 | 27.18 | 44.08 |\n| BASED | 363 m | 514.57 | 47.23 | 8.65 | 2.07 | 9.64 | 22.81 | 11.71 | $\\underline{25.07}$ | 43.03 |\n| Mamba | 358 m | 267.09 | $\\underline{39.95}$ | 8.64 | 2.21 | 9.59 | $\\underline{25.61}$ | 6.53 | 24.06 | $\\underline{43.51}$ |\n| GLA | 362 m | - | - | 9.12 | 2.36 | 10.68 | - | - | - | - |\n| RWKV v5 | 362 m | \u2014 | \u2014 | 9.79 | 2.40 | 10.90 | \u2014 | - | - | \u2014 |\n| H3 | 362 m | \u2014 | \u2014 | 10.60 | 4.88 | 11.23 | 17.59 | 0.64 | 7.87 | 39.35 |\n\nTable 1: Evaluation of pre-trained language models. All models were trained on the same set of 10 billion tokens drawn from the Pile 30 . We report inference throughput on 4,096 tokens $(16,384$ for 360 m param.) of pre-fill and 2,048 tokens of recurrent generation for a subset of architectures. We report language model perplexity on the overall Pile test set as well as perplexity on two slices of the test set: associative recall tokens and other tokens (see Section 6.1, 11). We report zero-shot performance on three recall-intensive tasks: information retrieval on SWDE and FDA as well as question answering on SQUAD. Finally, we report average performance on the set of LM Eval Harness [16] common sense reasoning tasks used in Gu and Dao 5], details in Appendix D. These tasks do not require significant recall capacity because the input text is typically very short. See Section 6.1. Some proposed architectures that do not implement recurrent views for generation are marked with a -\n\nGiven a fixed recurrent state, not all architectures have the same recall capacity. Among architectures proposed in prior work, Mamba makes the best use of a limited memory budget. Notably, architectures with a convolutional view (e.g. Hyena and H 3 ) fall well below the pareto frontier. Our proposed architecture, BASEd (introduced in Section 44, expands the pareto-frontier beyond Mamba. By varying hyper-parameters that determine its state size (e.g. feature dimension and model dimension), we can smoothly navigate the tradeoff between efficient models and memory-hungry models with high recall capacity. ### 3.2 Theoretical Analysis\n\nOur theoretical analysis provides further insight into the empirical observations described above. First, using results from communication complexity theory, we show that the recall capacity of any causal model (e.g. Mamba, Attention) is bounded by the size of its recurrent state (Theorem F.",
    "based-4": "3 in Appendix F. Theorem 3.1. Any recurrent mode depending causally on input $\\boldsymbol{u} \\in\\{0,1\\}^{N \\times d}$ requires $\\Omega(N)$-bit $\\}^{3}$ in state size to solve MQAR.",
    "based-5": "This result suggests that the tradeoff observed in Figure 2 is fundamental, not an artifact of architectural quirks. Next, we focus on gated-convolutions, a broad class of architectures built from gating and convolutions (e.g. H3, Hyena, RWKV v4). To make progress in theoretically analyzing the broad set of gated convolution proposals, prior work develops a canonical gated-convolution, referred to as BaseConv which can provably simulate any architecture built from gating and convolution primitives. Building on this work, we show that BaseConv cannot solve MQAR in constant-many layers (Theorem F.",
    "based-6": "5 and Theorem F.",
    "based-7": "6 in Appendix F. Theorem 3.2. Given an input sequence $\\boldsymbol{u} \\in\\{0,1\\}^{3 N \\times d}$, where $N$ and denote the sequence length and head dimension, respectively, a data-independent BaseConv model needs $\\log (2 d)$-layers to solve MQAR for $d=\\log _{2}(c)$, where $c$ denotes the vocabulary siz ${ }_{4}^{4}$. In contrast, Arora et al. [1] show that attention solves MQAR in constant-many layers. This result helps to explain why the gated-convolution architectures ( H 3 and Hyena) in Figure 2 lie below the pareto frontier established by newer architectures. [^2]Remark 3.1. For a class of input encodings that generalizes one-hot encodings termed as $p$-hot encodings (Definition F.7), input-dependent BaseConv needs at least $\\lfloor\\log (2 p)\\rfloor$-layers to solve MQAR where $d=p \\cdot \\sqrt[p]{c}$. Finally, we show that we can simulate linear attention [20, the foundation of BASED, using BaseConv [1] with a poly-log blowup in the number of layers (Proposition F. 1 in Appendix F), pointing to the relative efficiency of linear attention over gated-convolution architectures. ## 4 The Based Architecture\n\nIn this section, we introduce BASED. Our objective in designing this architecture is to demonstrate how we can navigate the pareto-frontier of the memory-recall tradeoff using well-known architectural building blocks. Softmax attention excels at recall, but since its recurrent state, the KV-cache, grows unconstrained with the length of sequence, it is stuck in the upper right quadrant of Figure 2. We study two simple approaches for constraining the size of attention's recurrent state: linear attention and sliding window attention. The recurrent state size of linear attention (i.e. attention without softmax) does not grow with the sequence length and can be modulated by changing simple hyperparameters 20]. With sliding window attention, we cap the recurrent state size to be the width of the window. However, our experiments on real-world language modeling (Table 4) and synthetic associative recall (Figure 1 middle) suggest that neither primitive alone suffices to navigate the pareto frontier. Linear attention lacks the precision to perform local token shifts and comparisons 1, 31. In sliding window attention, associative recall range is limited by the width of the windows (Figure 2, center). As we increase the window size, the recurrent state grows linearly and has a non-linear effect on speed during parallel training and inference (Figure 2, left). BASED models simply combine (1) softmax-approximating linear attention applied globally and (2) exact softmax attention applied locally in small sliding windows (Figure 1, right). This combination allows us to use softmax attention in surprisingly small sliding windows (e.g., 64 tokens) that recover $90.8 \\%$ of full softmax attention's recall accuracy at $1 \\mathrm{e}-5 \\times$ its latency. Sliding window and linear attention alone fail (Figure 1 left). ### 4.1 Taylor Linear Attention\n\nBy approximating softmax attention using linear feature maps, we can constrain the size of the recurrent state while maintaining global token interactions (i.e.",
    "based-8": "each token depends on every token before it in the sequence). Katharopoulos et al. [20, Choromanski et al. 21, Tsai et al. 32] show that we can select a feature map $\\phi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{\\tilde{d}}$ such that $\\phi\\left(\\boldsymbol{q}_{i}\\right)^{\\top} \\phi\\left(\\boldsymbol{k}_{j}\\right) \\approx \\exp \\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j} / \\sqrt{d}\\right)$. We can then rewrite the formula for softmax attention in Equation (1) as\n\n$$\n\\sum_{j=1}^{i} \\frac{\\phi\\left(\\boldsymbol{q}_{i}\\right)^{\\top} \\phi\\left(\\boldsymbol{k}_{j}\\right) \\boldsymbol{v}_{j}}{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)}=\\frac{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\sum_{j=1}^{i}\\left(\\phi\\left(\\boldsymbol{k}_{j}\\right)^{\\top} \\boldsymbol{v}_{j}\\right)}{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)}\n$$\n\nwhere every query attends to every past key in $\\mathcal{O}\\left(N d^{2}\\right)$ time and space complexity. Furthermore, Katharopoulos et al. 24] show that linear attention has a fixed size recurrent state during generation. Letting $\\boldsymbol{s}_{i}=\\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)^{\\top} \\boldsymbol{v}_{j}$ and $\\boldsymbol{z}_{i}=\\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)^{\\top}$ be a \"KV-state\" and \"K-state\" respectively, we can compute Equation (2) as\n\n$$\n\\begin{gathered}\n\\boldsymbol{s}_{i}=\\boldsymbol{s}_{i-1}+\\phi\\left(\\boldsymbol{k}_{i}\\right)^{\\top} \\boldsymbol{v}_{i}, \\quad \\boldsymbol{z}_{i}=\\boldsymbol{z}_{i-1}+\\phi\\left(\\boldsymbol{k}_{i}\\right)^{\\top} \\\\\n\\boldsymbol{y}_{i}=\\frac{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\boldsymbol{s}_{i}}{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\boldsymbol{z}_{i}}\n\\end{gathered}\n$$\n\nwhere $\\boldsymbol{s}_{i} \\in \\mathbb{R}^{d \\times \\tilde{d}}$ and $\\boldsymbol{z}_{i} \\in \\mathbb{R}^{\\tilde{d}}$. Feature map. To approximate $\\exp \\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j} / \\sqrt{d}\\right)$, we use the $2^{\\text {nd }}$-order Taylor series feature map, picking $\\phi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d^{2}}$ such that\n\n$$\n\\phi\\left(\\boldsymbol{q}_{i}\\right)^{\\top} \\phi\\left(\\boldsymbol{k}_{j}\\right)=1+\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j}+\\frac{\\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j}\\right)^{2}}{2}\n$$\n\nWhile Zhang et al. 13 note that picking a feature map with $\\tilde{d}=d^{2}$ results in linear attention with $\\mathcal{O}\\left(N d^{3}\\right)$ time and space complexity and large recurrent state of size $O\\left(d^{3}\\right)$, we can tradeoff efficiency for recall capacity by projecting queries and keys to smaller dimensions i.e., $\\boldsymbol{W}_{q}, \\boldsymbol{W}_{k} \\in \\mathbb{R}^{d \\times d^{\\prime}}$ with $d^{\\prime}=16$. By changing $d^{\\prime}$ we modulate the size of the recurrent state. How does the choice of feature map affect the memory-recall tradeoff? Prior work demonstrates the strong performance of the Taylor feature map on associative recall [13]. Building on this analysis, we evaluate a broad set of feature maps $\\left(\\phi_{\\operatorname{ReLU}}(x)=\\max (x, 0)\\right.$, $\\phi_{\\text {PosELU }}(x)=\\operatorname{ELU}(x)+1, \\phi_{\\text {Square }}(x)=x^{2}, \\phi_{\\text {Identity }}(x)=x$, $\\phi_{\\text {CosFormer }}$ as defined in [23], and $\\phi_{\\text {Performer }}$ as defined in 21]) using the experimental setup described in Section 3.1. In Figure 3 (top), we plot the memory-recall tradeoff curves for these feature maps. The Taylor series feature map, along with the simple $\\phi_{\\text {PosELU }}$ and $\\phi_{\\text {ReLU }}$ feature maps, sits at the pareto frontier. One advantage of the Taylor feature map over these alternatives is that it expands the recurrent state size (improving recall capacity) without changing the number of parameters. As shown in Figure 3 (bottom), the Taylor series feature map requires fewer parameters than alternatives to achieve high recall capacity. This analysis and the ablations in Table 4 informed our decision to use the Taylor approximation, though other simple feature maps may be effective as well. ### 4.2 Local Exact Attention with Tensor Core Sliding Windows\n\nTo efficiently model fine-grained local interactions, BASED uses sliding window attention with window sizes set at small multiples of 16 (up to 64 tokens). Similar to past (causal) implementations 18 , 19], for window size $w$ each query $\\boldsymbol{q}_{i}$ only attends to past keys $\\left\\{\\boldsymbol{k}_{i-w+1}, \\ldots, \\boldsymbol{k}_{i}\\right\\}$. This enables $\\mathcal{O}(N w)$ time and space complexity for linear scaling in sequence length $N$, with a $w$-sized KV-cache for constant-memory generation. ![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-07.jpg?height=909&width=472&top_left_y=630&top_left_x=1355)\n\nFigure 3: Linear attention feature maps on AR. $x$ : state size (bytes) during generation or param. count; $y$ : MQAR accuracy. This setting is harder than fig. 2 (256 key-value pairs). sizes 256 [17] to 4096 [10, BASED uses only $w=16,32$, or 64 to best exploit modern GPUs. In Section 5, we discuss how this \"Tensor core-aware\" window (TCWINDOW) achieves $1 \\mathrm{e}-5 \\times$ the latency than the $w=4096$ windows in modern LLMs (e.g., Mistral 7B [10]). While the small $w$ in TCWindOW enable fast local and exact attention, it presents a challenge for long range modeling. With just $w=64$, for every layer of $w=4096$ Mistral sliding window attention we would require 64 layers of BASED to achieve the same receptive field. Controlling for model depth and sequence length, Figure 2 indeed shows smaller $w$ linearly decreasing in associative recall accuracy. BASED's global linear attention described above overcomes the lack of long-range modeling presented with low $w$. Additional architectural details for BASED are discussed in Appendix C and the hybridization of layers used in experiments are provided in Table 7. We include ablations of architectural choices in Table 4 and evaluate the overall quality and efficiency of BASED in Section 6 . ## 5 Efficient Implementation\n\nIn this section we focus on the efficiency of BASED. A na\u00efve implementation is slower than the most efficient standard attention implementations (shown in Figure 4) as it requires large amounts of high latency memory movement. We first describe preliminaries of the GPU execution model and memory hierarchy. We next present the baseline and our hardware-aware algorithms for linear attention in Section 5.1 and for sliding window attention in Section 5.2 .",
    "based-9": "Preliminaries GPU operations, or kernels, are executed by thousands of parallel threads. In NVIDIA terms, GPU streaming multiprocessors launch thread blocks at the software level. These blocks are divided into warps (e.g. 32 threads) that are assigned to cores at the hardware level. Threads need to read inputs into their registers to perform computations and write the outputs. The time taken to read and write is referred to as the IO cost. Operations could either be memory or compute bound, depending on the time to load data vs. perform computations on loaded data. In designing our IO-aware algorithms, we would like to exploit two key properties of modern GPUs. First, tensor core units (fast matrix multiply units) achieve $312 \\mathrm{TFLOP} / \\mathrm{s}$ speeds relative to $19 \\mathrm{TFLOP} / \\mathrm{s}$ for the non-matrix multiply cores. Second, GPUs face a memory hierarchy with large amounts of slow-to-access memory and smaller amounts of fast-to-access memory. The access speed is governed by the proximitiy of the processor to the data address. For instance, the hierarchy on a modern NVIDIA 80GB A100 GPU is: 80 GB of HBM with $2 \\mathrm{~TB} / \\mathrm{s}$ bandwidth, 80 MB of L 2 cache, 192 KB of L1 cache / shared memory (implemented via SRAM) with $19 \\mathrm{~TB} / \\mathrm{s}$ bandwidth per SM, and 256 KB of register file per SM [33].",
    "based-10": "Note that register memory is private to an executing thread, so threads need to write to shared memory to communicate data to other threads in the block. To reduce the time required for reads and writes, a key principle is to fuse multiple operations on the same data slice while it's in fast memory before writing it back to slow memory. ### 5.1 Taylor Exponential Linear Attention\n\nDespite the theoretically improved complexity, the linear attention methods demonstrated in prior work are often less efficient than highly-optimized softmax attention implementations (Flash Attention [12]) when measured in real-world wall-clock time and memory usage. We next present hardware-aware algorithms to make Taylor linear attention efficient. We focus on two operations: (1) prefill, corresponding to processing the prompt during generation or the forward pass during training, and (2) next token prediction during generation, which also requires updating the recurrent hidden state state. In this section, we refer to the batch size as $B$, number of heads as $H$, head dimension as $d$, sequence length as $N$ and feature dimension as $d^{\\prime}$, following Section 4 . For ease of notation, let $D=1+d^{\\prime}+d^{\\prime 2}$ in this section. Additional details for the IO-Aware algorithms are included in Appendix B. ### 5.1.1 Forward Pass / Generation Prefill\n\nBaseline Implementation The na\u00efve implementation detailed in Appendix B (1) produces the feature maps $Q, K,(2)$ computes and materializes the large hidden state $K V \\in \\mathbb{R}^{H \\times d \\times D}$, then (3) computes the causal dot product between $Q$ and $K V$. Prior work has released popular CUDA kernels for linear attention to efficiently perform the causal dot product / step (3) 34. The kernel parallelizes computation across the heads and batches, loads tiles of $V$ and $K$ to SRAM, updates the running $K V$ state in SRAM, loads tiles of $Q$ to SRAM, produces the final output in SRAM, and writes the result to HBM. Analysis In overall IO cost, ignoring the input and output projections in the linear attention layer, this procedure requires $2 B H N D$ bytes for writing featurized $Q, K$ to HBM. During the causal dot product, this requires $2 B H N D+B H N d$ bytes to read $Q, K, V$ tiles and $B H N d$ bytes to write the result. Throughout the computation, $\\mathcal{O}(B H N D d)$ bytes (note this is the shape $K V$ state during the forward pass) are read in and out of thread registers to SRAM to update the running output and $K V$ state at $19 \\mathrm{~TB} / \\mathrm{s}$ bandwidth. Algorithm To improve efficiency, our algorithm computes both the feature map and the causal dot product in fast memory. Our overall algorithm is detailed in Algorithm 1 and we discuss the overall process here. The algorithm first parallelizes over the batch $B$ and head $H$ dimensions, since computation is independent for each head in each batch. We consider the three terms $T_{0}, T_{1}, T_{2} \\in \\mathbb{R}^{N \\times d}$ in the linear attention output, corresponding to the three terms in the $2^{\\text {nd }}$-order Taylor polynomial for the exponential. We also consider the corresponding three terms, in the cumulative $K V$ state as we process a sequence. For a given tile (i.e. sub-matrix) of $Q, K, V$, we load the tiles (e.g., tile of $Q, K \\in \\mathbb{R}^{16 \\times 16}$ and $V \\in \\mathbb{R}^{16 \\times 64}$ given 16 tokens, 16 feature dimension, and 64 head dimension) into fast memory and update the running $K V$ state and output for each of the three terms. We load the tiles into the register file and fuse-operations to compute the causal dot product in register, only writing to SRAM to synchronize the cumulative $K V$ state across parallel warps. We use explicit writes to SRAM for this synchronization because register memory is warp-specific (threads in one warp do not access the registers for a thread in a different warp). Analysis In IO cost, again ignoring the input and output projections in the linear attention layer, our procedure requires $2 B H N d^{\\prime}$ bytes for reading $q, k$ and $2 B H N d$ bytes for reading $v$ and writing output $y$ between HBM and SRAM. Overall, our algorithm avoids in HBM $\\mathcal{O}(2 B H N D)$ bytes in HBM to SRAM data movement. We additionally improve upon the baseline by performing computation in-register to avoid the $\\mathcal{O}(B H N D d)$ bytes in SRAM to register data movement. ### 5.1.2 Next token prediction\n\nDuring next token prediction, an important consideration is how to efficiently update the recurrent state $K V_{t} \\in \\mathbb{R}^{B H D d}$ at timestep $t$. The expensive operation during next token prediction is computing the outer product between projected hidden states $k_{t+1} \\in \\mathbb{R}^{B H D}$ and $v_{t+1} \\in \\mathbb{R}^{B H d}$. The outer product requires $\\mathcal{O}(B H D d)$ computation and space, and the result is summed with $K V_{t}$ to produce $K V_{t+1}$. We provide an IO-aware algorithm for the state updates in Algorithm 2. This algorithm incurs $\\mathcal{O}(B H D+B H d)$ bytes of HBM to SRAM data movement (to load the $q, k, v$ projections). ## 5.2 tcWindow\n\nNext we motivate the choice of window size for TCWINDOW. In contrast to sliding-window style models such as the popular Mistral models, which use large window sizes $w=4096$ [10], BASED chooses a window size based on hardware specifications. GPU tensor cores operate on $16 \\times 16$ tiles. Large GEMMs are compute bound (for e.g. in long-context attention). But, we need sufficient occupancy to hide the latency of the tensor core units. Figure 1 (Right) shows $64 \\times 64$ dimension matrix multiplications are approximately the same latency as $16 \\times 16$. BASED sets $w$ to use $64 \\times 64$ tiles (Figure 1). To distinguish from prior sliding windows, we refer to this approach as TCWindow. We use the Flash Attention sliding window implementation during training [11] and in Appendix B Algorithm 3, we provide an IO-aware algorithm of TCWindow for next token prediction. End-to-end benchmarks for BASED implemented with these IO-aware algorithms are provided in Section 6 . Micro-benchmarks for each kernel against the baseline implementations are provided in Appendix B. ## 6 Results\n\nIn this section, we present results for the following claims:\n\n1. Language modeling overall. We evaluate architectures in pretraining from scratch on the Pile 330] and on standard benchmarks from the LM Eval Harness [16. We find BASED matches or outperforms the strongest sub-quadratic architectures (e.g.",
    "based-11": "Mamba [5]) across these settings. 2. Language modeling recall. BASED closes the gap to attention on the challenging associative recall slice of the real-world Pile language modeling corpus (see Table1), outperforming prior sub-quadratic architectures. We apply these pretrained models zero-shot to a suite of recall-intensive tasks (e.g. information extraction, QA), showing that BASED systematically outperforms other efficient architectures with comparable throughput. ![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-10.jpg?height=510&width=1616&top_left_y=249&top_left_x=249)\n\nFigure 4: (Left) Throughput numbers for the varied prefill sequence lengths at a fixed batch size of 2. Right generation throughput at varied batch sizes at a fixed generation length of 1024 tokens. The $y$-axis shows the in latency (ms). Lines are cutoff when the model runs out of memory. We show results for both 360 M and 1.3 Bn , and all numbers are computed on a single NVIDIA H100 GPU. 3. Generation throughput. Our IO-aware implementation of recurrent generation in Based enables $40-60 \\%$ speedups relative to FlashAttention-2 and Mamba for prefill at $4 k$ sequence length and up to $24 \\times$ higher throughput over FlashAttention-2 in generating 1024 tokens at batch size 128 (see Figure 4 ).",
    "based-12": "4. Ablations of key design choices. We ablate four key design choices and hyperparameters: (1) the feature map, (2) the feature dimension, (3) use of local sliding-window softmax, and (4) the use of short gated-convolutions. The results of these ablations are shown in Table 4\n\nBaselines We compare to key baselines in the literature. We compare to Transformer (GPT architecture) and Transformer++ (Llama architecture [15]), which adds rotary encodings [35] and gated linear units. We compare an early class of efficient architectures built from gating and long-convolution primitives including Hyena [7, RWKV 8], and H3 [9. We finally compare to recent state-of-the-art architectures, that use input-dependent sequence aggregation to improve in quality upon the long-convolution models, including Mamba [5] and Gated Linear Attention 6. ### 6.1 Language Modeling Evaluations\n\nLanguage Modeling Benchmarks We pretrain language models from scratch at two parameter scales (355M and 1.3Bn parameters) on the Pile 30. Each model sees the same 10 billion tokens of pretraining data in the same order. The Pile data is tokenized using the GPT-2 BPE tokenizer 36. We measure perplexity on the Pile and report results in Table 1 and further experimental details are provided in Appendix E. 1 . We additionally evaluate the pretrained models on the standard LM Eval Harness [16], as is standard protocol for our baselines [5, 6. A detailed breakdown of tasks and metrics can be found in Appendix D. In both pretraining and on the LM Eval Harness, BASED consistently competes with the strongest Transformer++ and Mamba baselines. While these overall metrics are helpful, we next turn to a fine-grained analysis of recall and in-context learning ability on real-world data. Recall and In-Context Learning Evaluations We evaluate our pretrained models on a suite of incontext learning tasks selected to test the downstream recall capacity in Table 1. These tasks fall into three categories: (1) Real-world AR Beyond perplexity scores, we slice the next token predictions on the Pile to understand each architecture's AR quality ( Appendix E.1). (2) Information extraction (IE) SWDE and FDA are popular semi-structured and unstructured document IE benchmarks respectively [37, 38, 39]. SWDE has raw HTML for 8 Movie and 5 University websites (e.g. IMDB, US News) and annotations for 8-274 attributes per website (e.g., Movie runtime), and (3) Question answering from in-context passages. We find BASED outperforms the baseline sub-quadratic architectures across these evaluations, closing the gap to Transformer++. These trends tracks the MQAR synthetic results from Section 3.1 . | Model | Shots | BoolQ <br> Acc. $\\uparrow$ | CB |  | COPA <br> Acc. $\\uparrow$ | MultiRC <br> Acc. $\\uparrow$ | ReCoRD |  | RTE <br> Acc. $\\uparrow$ | WiC <br> Acc. $\\uparrow$ | WSC <br> Acc. $\\uparrow$ | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  |  | Acc. $\\uparrow$ | F1 $\\uparrow$ |  |  | F1 $\\uparrow$ | EM $\\uparrow$ |  |  |  |  |\n| Based <br> (363m) | 0 | 59.0 | 41.1 | 19.4 | 69.0 | 54.9 | 14.5 | 14.0 | 52.0 | 50.0 | 36.5 | 45.7 |\n|  | 1 | 57.5 | 37.5 | 26.8 | 68.0 | 52.5 | 19.9 | 19.2 | 47.7 | 50.9 | 49.0 | 47.2 |\n|  | 5 | 56.6 | 44.6 | 28.9 | 73.0 | 53.6 | 24.9 | 24.1 | 48.7 | 51.1 | 39.4 | 48.0 |\n| Transformer++ <br> (360m) | 0 | 57.3 | 41.1 | 21.3 | 67.0 | 57.0 | 16.6 | 16.1 | 53.8 | 50.0 | 37.5 | 46.3 |\n|  | 1 | 54.2 | 39.3 | 25.3 | 69.0 | 51.5 | 22.2 | 21.6 | 50.9 | 47.0 | 55.8 | 47.8 |\n|  | 5 | 50.7 | 58.9 | 49.9 | 64.0 | 46.9 | 24.2 | 23.6 | 47.3 | 52.2 | 51.9 | 48.9 |\n| Mamba <br> $(358 \\mathrm{~m})$ | 0 | 57.5 | 35.7 | 24.4 | 71.0 | 57.2 | 18.8 | 18.3 | 52.4 | 50.0 | 36.5 | 46.6 |\n|  | 1 | 51.1 | 39.3 | 27.4 | 71.0 | 52.9 | 21.6 | 21.0 | 46.6 | 46.2 | 52.9 | 46.9 |\n|  | 5 | 41.1 | 37.5 | 23.6 | 69.0 | 49.2 | 20.4 | 19.9 | 48.4 | 51.7 | 51.9 | 45.2 |\n\nTable 2: Few-shot downstream evaluation on SuperGLUE of pre-trained language models. The same set of models as in table 1, all were trained on the same 10 billion tokens drawn from the Pile 30, evaluated on the SuperGLUE benchmark [40] using the LM eval harness by EleutherAI [16]. When computing the average, we first average the metrics by task and then average across tasks. |  |  | HG38 PPL $\\downarrow$ |  |  |\n| :--- | :---: | :---: | :---: | :---: |\n| Model | Params | $N=1024$ | $N=4096$ | $N=8192$ |\n| Transformer++ | 46.2 | 2.52 | 2.50 | 2.51 |\n| Mamba | 46.1 | $\\mathbf{2 .",
    "based-13": "5 1}$ | $\\mathbf{2 . 4 9}$ | $\\mathbf{2 . 4 9}$ |\n| Based | 48.8 | $\\mathbf{2 . 5 1}$ | 2.50 | $\\mathbf{2 . 4 9}$ |\n\nTable 3: DNA modeling performance on the HG38 dataset. All models are pretrained from scratch for 10 Bn tokens at $N=1 \\mathrm{k}, 4 \\mathrm{k}$, and 8 k sequence lengths respectively. We report results after hyperparameter sweeping the learning rate for each architecture. In order to understand in-context-learning performance, we next perform few-shot evaluations on the SuperGLUE benchmark 40 for BASEd, Mamba and Transformer++ in Table 2. Each model was evaluated on all tasks using under 0 shot (i.e., number of in-context examples), 1 shot and 5 shot prompting, respectively. Transformer ++ and BASED both see monotonic improvement from increasing the number of shots. For Mamba, however, albeit getting a slight improvement from 0 -shot to 1 -shot, it performs worse on 5 -shot than even on 0 -shot. This result suggests that the limited recall ability observed in Mamba could also impact few-shot abilities. DNA modeling Towards understanding the capability of BASED beyond natural English language, we next evaluate each architecture on its ability to perform DNA next token prediction (Table 3). We evaluate architectures on the HG38 (human genome) benchmark at $1 k, 4 k$, and $8 k$ sequence lengths used in prior architecture evaluations [5, 41. The DNA tasks uses a byte-level tokenizer wherein the vocabulary consists of characters corresponding to the nucleotide bases. We find BASED is competitive with state-of-the-art architectures across evaluated sequence lengths. In Appendix D, we evaluate these checkpoints on downstream DNA classification tasks. ### 6.2 Efficiency Benchmarks\n\nWe benchmark the throughput of BASED, with and without our proposed IO-Aware algorithms (Section 5). We consider both the forward pass / generation prefill and next token prediction stages. Experiments were run using an H100 NVIDIA GPU and averaged over 20 repetitions. Results are shown in Figure 4. End-to-end benchmarks Using our efficient implementation (Section 5), BASED achieves $56 \\%$ faster prefill than FlashAttention-2 [11] and $44 \\%$ faster than Mamba at $4 k$ sequence length and 1.3Bn parameters ( $28 \\%$ faster than FlashAttention-2 and $76 \\%$ faster than Mamba at 360M parameters). We find that next token generation, with no prefill, provides $24 \\times$ higher throughput (tokens/second) over the highly optimized FlashAttention-2 implementation and achieves $95 \\%$ and the throughput of the recurrent Mamba architecture at batch size 128 and 1.3Bn parameters ( $98 \\%$ higher throughput vs. FlashAttention-2 and 118\\% higher throughput vs. Mamba at 360M parameters). All benchmarks is on a single NVIDIA H100 GPU, using CUDA cache graphs during next token prediction 42 . In Figure 4, we also include results for the baseline implementation of BASED that uses the popular Fast\n\n| Hyperparameters |  |  |  |  | Language Modeling (Pile) |  |  | Info.",
    "based-14": "Extraction | QA |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Feat. Map | Feat. Dim. | Sliding | Convs. | Decay | All <br> Ppl. $\\downarrow$ | AR <br> Ppl. $\\downarrow$ | Other <br> Ppl. $\\downarrow$ | FDA <br> Acc. $\\uparrow$ | SQUAD <br> Acc. $\\uparrow$ |\n| Taylor Exp. (2nd) | 16 (153) | $\\checkmark$ (64) | $\\checkmark$ | $\\checkmark$ | 8.65 | 2.07 | 9.64 | 11.71 | 25.07 |\n| Performer | 16 (16) | $\\checkmark(64)$ | $\\checkmark$ | $\\checkmark$ | 9.08 | 8.53 | 11.62 | 0.36 | 7.47 |\n| CosFormer | 16 (32) | $\\checkmark(64)$ | $\\checkmark$ | $\\checkmark$ | 9.03 | 2.42 | 9.98 | 7.71 | 24.63 |\n| CosFormer | 64 (128) | $\\checkmark(64)$ | $\\checkmark$ | $\\checkmark$ | 8.82 | 2.18 | 9.80 | 9.07 | 27.85 |\n| Taylor Exp. (2nd) | 32 (561) | $\\checkmark(64)$ | $\\checkmark$ | $\\checkmark$ | 8.56 | 2.00 | 9.57 | 12.89 | 26.74 |\n| Taylor Exp. (2nd) | 24 (325) | $\\checkmark(64)$ | $\\checkmark$ | $\\checkmark$ | 8.58 | 2.02 | 9.58 | 20.87 | 24.77 |\n| Taylor Exp. (2nd) | 16 (153) | $\\checkmark(64)$ | $\\checkmark$ | $\\checkmark$ | 8.65 | 2.07 | 9.64 | 11.71 | 25.07 |\n| Taylor Exp. (2nd) | 8 (45) | $\\checkmark(64)$ | $\\checkmark$ | $\\checkmark$ | 8.77 | 2.18 | 9.75 | 12.79 | 22.35 |\n| Taylor Exp. (2nd) | 16 (153) | $\\checkmark$ (64) | $\\checkmark$ | $\\checkmark$ | 8.65 | 2.07 | 9.64 | 11.71 | 25.07 |\n| Taylor Exp. (2nd) | 16 (153) | $\\checkmark(64)$ | $\\checkmark$ | $x$ | 8.65 | 2.04 | 9.66 | 1.72 | 10.80 |\n| Taylor Exp. (2nd) | 16 (153) | $x$ | $\\checkmark$ | $\\checkmark$ | 8.91 | 2.11 | 9.94 | 10.16 | 24.5 |\n| Taylor Exp. (2nd) | 16 (153) | $\\checkmark(64)$ | $x$ | $\\checkmark$ | 8.74 | 2.09 | 9.74 | 2.36 | 18.87 |\n| Taylor Exp. (2nd) | 24 (325) | $x$ | $x$ | $\\checkmark$ | 9.49 | 2.29 | 10.58 | 8.71 | 11.33 |\n| Taylor Exp. (2nd) | 16 (153) | $\\checkmark(128)$ | $\\checkmark$ | $\\checkmark$ | 8.61 | 2.06 | 9.60 | 14.39 | 31.84 |\n| Taylor Exp. (2nd) | 16 (153) | $\\checkmark(64)$ | $\\checkmark$ | $\\checkmark$ | 8.65 | 2.07 | 9.64 | 11.71 | 25.07 |\n\nTable 4: Ablations of design choices and hyperparameters in Based. All models are 362M param variants of the BASED architecture described in Section 4 , trained to 10 billion tokens on the Pile. We ablate the hyperparameters central to the design of BASED: (1) the choice of feature map $\\phi$ (see Section 4.1), (2) the size of the feature dim $d^{\\prime}$ (we show the effective size of the feature after applying the feature map in parantheses, see Section 4.1), (3) the use of local sequence mixers (sliding window attention and short convolutions), and (4) the data-dependent decay defined in Section 4\n\nTransformers CUDA kernel in the linear attention literature to compute the causal dot product 34 (discussed in Section 5). The custom kernel introduced in our work unlocks the efficiency of BASED. Micro benchmarks As the end-to-end BASED architecture is a hybrid architecture, we provide micro benchmarks of the individual IO-aware kernels against the key baseline implementations in Appendix B. ### 6.3 Quality Ablations\n\nOur objective with BASED is to measure the throughput and recall of the simplest possible linear attention model that achieves strong performance. Therefore, we ablate the key design decisions - choice of feature map, feature dimension for the Taylor map, use of sliding window and convolutions - to understand their contributions to the quality of BASED. We perform these ablations on the Pile corpus [30] using the same number of tokens and data ordering as the prior experiments. In feature map ablations, we consider the CosFormer [23] and Performers 21 feature maps, which have been demonstrated as strong linear attentions in prior work 13. We also include a baseline that expands the state size using learned projections and applies CosFormer towards comparing to the larger state size of the Taylor map. For these baselines, we keep the rest of the BASED architecture the same (i.e. in the number of linear attention layers and hybridization with sliding window and gated convolution layers). We observe that with the larger state size, CosFormer quality is increasingly competitive with the Taylor map. We note that expanding the state size requires increasing the model's overall parameter count (due to the learned projections) in the CosFormer baseline, in contrast to the Taylor map. Next, we ablate the feature dimension, holding the feature map fixed to the Taylor map. We find larger feature dimension improves quality, with diminishing returns going from 24 to 32 dimension. Note that feature dimension $\\sqrt{1024}=32$, where 1024 is the attention model dimension at the 360 parameter scale in our experiments. Finally, the ablations show that eliminating the convolutions and/or the sliding window attention degrades quality. We observe that adding either convolutions or sliding window helps on the associative recall slice relative to neither (e.g. 2.29 AR Ppl. on the Pile with neither vs. 2.09 or 2.11 with sliding window or convolutions.). Further, increasing the window size from 0 to 64 vs. 64 to 128 (also an efficient design point in Figure 1, left) continues to help quality, but with marginal improvements. ## 7 Conclusion\n\nThis work studies the properties of high-quality and efficient sequence mixers. We identify a fundamental tradeoff between recall, key to in-context learning, and throughput through theory and experiments. Attention performs recall perfectly, but requires retaining a KV cache that grows with the sequence length. As an alternative, BASED combines two simple techniques - local fine-grained attention and long-range linear attention via a Taylor approximation of the softmax exponential function - that are sub-quadratic during training and permit an efficient recurrent inference view. To enable wall clock efficiency, we introduce IO-aware algorithms of the Taylor linear attention computation that leads BASED to perform generation up to $24 \\times$ faster than FlashAttention-2 at the 1.3Bn parameter scale, generating 1024 tokens. Beyond competing in overall perplexity, BASED outperforms prior sub-quadratic architectures in recall quality by up to 6.2 accuracy points, pointing to BASED's promise as a language modeling architecture. ## Acknowledgments\n\nWe thank Tri Dao, Daniel Fu, Songlin Yang, Jessica Grogan, Albert Gu, Eric Nguyen, Michael Wornow, Alyssa Unell, and Gautam Machiraju for their helpful feedback and discussion during this work. We thank the Hazy Research lab and Together AI for supporting this work. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF2247015 (Hardware-Aware), CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under Nos. W911NF-23-2-0184 (Long-context) and W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under Nos. N000142312633 (Deep Signal Processing), N000141712266 (Unifying Weak Supervision), N000142012480 (Non-Euclidean Geometry), and N000142012275 (NEPTUNE); Stanford HAI under No. 247183; NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare.",
    "based-15": "The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. AR's research is supported by NSF grant CCF\\#2247014. ## References\n\n[1] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. Zoology: Measuring and improving recall in efficient language models. International Conference on Learning Representations, 2023. [2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. volume 30, 2017. [3] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. [4] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention.",
    "based-16": "arXiv preprint arXiv:2212.10544, 2022. [5] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.",
    "based-17": "arXiv preprint arXiv:2312.00752, 2023. [6] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training.",
    "based-18": "arXiv preprint arXiv:2312.06635, 2023. [7] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [8] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, and Jiaming et al. Kong. Rwkv: Reinventing rnns for the transformer era. arXiv:2305.13048, 2023. [9] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations, 2023. [10] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.",
    "based-19": "arXiv preprint arXiv:2310.06825, 2023. [11] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning.",
    "based-20": "2023. [12] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [13] Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog \\& the porcupine: Expressive linear attentions with softmax mimicry. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=4g0212N2Nx. [14] Feyza Duman Keles, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. On the computational complexity of self-attention. In 34th International Conference on Algorithmic Learning Theory, volume 201, page $1-23,2023$. [15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. [16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836\n[17] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pages 4055-4064. PMLR, 2018. [18] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [19] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "based-21": "In International conference on machine learning, pages 5156-5165. PMLR, 2020. [21] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.",
    "based-22": "arXiv preprint arXiv:2009.14794, 2020. [22] Krzysztof Choromanski, Haoxian Chen, Han Lin, Yuanzhe Ma, Arijit Sehanobish, Deepali Jain, Michael S Ryoo, Jake Varley, Andy Zeng, Valerii Likhosherstov, et al. Hybrid random features.",
    "based-23": "arXiv preprint arXiv:2110.04367, 2021. [23] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention.",
    "based-24": "arXiv preprint arXiv:2202.08791, 2022. [24] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the International Conference on Machine Learning (ICML), 2020. URL https://arxiv.org/abs/2006.16236. [25] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning pretrained transformers into RNNs. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10630-10643, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.830. URL https://aclanthology.org/2021.emnlp-main.",
    "based-25": "830. [26] Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers.",
    "based-26": "In International Conference on Machine Learning, pages 9355-9366. PMLR, 2021. [27] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [28] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023. [29] Ekin Aky\u00fcrek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-context language learning: Architectures and algorithms. 2024. URL https://arxiv.org/abs/2401.12973. [30] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling.",
    "based-27": "arXiv preprint arXiv:2101.00027, 2020. [31] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling. arXiv preprint arXiv:2302.06646, 2023. [32] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: a unified understanding of transformer's attention via the lens of kernel.",
    "based-28": "arXiv preprint arXiv:1908.11775, 2019. [33] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.",
    "based-29": "[34] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. 2020. [35] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. [36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [37] Eric Wu, Kevin Wu, Roxana Daneshjou, David Ouyang, Daniel Ho, and James Zou. How medical ai devices are evaluated: limitations and recommendations from an analysis of fda approvals.",
    "based-30": "Nature Medicine, 27:1-3, 042021. [38] Xiang Deng, Prashant Shiralkar, Colin Lockard, Binxuan Huang, and Huan Sun. Dom-lm: Learning generalizable representations for html documents. 2022. [39] Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher R\u00e9. Language models enable simple systems for generating structured views of heterogeneous data lakes. arXiv:2304.09433, 2023. [40] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: a stickier benchmark for general-purpose language understanding systems. Curran Associates Inc., Red Hook, NY, USA, 2019. [41] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, Stefano Ermon, Stephen A. Baccus, and Chris R\u00e9. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution, 2023. [42] NVIDIA. Getting started with cuda graphs, 2019. URL https://developer.nvidia.com/blog/ cuda-graphs/. [43] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. [44] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and et al. Big bird: Transformers for longer sequences.",
    "based-31": "Proceedings of NeurIPS, 2020. [45] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. [46] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. Advances in neural information processing systems, 34:17723-17736, 2021. [47] Silas Alberti, Niclas Dern, Laura Thesing, and Gitta Kutyniok. Sumformer: Universal approximation for efficient transformers. arXiv preprint arXiv:2307.02301, 2023. [48] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [49] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14138-14148, 2021. [50] Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr \\\" om method.",
    "based-32": "In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id= pZCYG7gjkKz. [51] Alexandre De Brebisson and Pascal Vincent. An exploration of softmax alternatives belonging to the spherical loss family.",
    "based-33": "arXiv preprint arXiv:1511.05042, 2015. [52] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00e9. Scatterbrain: Unifying sparse and low-rank attention approximation.",
    "based-34": "arXiv preprint arXiv:2110.15343, 2021. [53] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025-7041, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: $10.18653 / v 1 / 2022 . e m n l p-m a i n .473$. URL https: //aclanthology.org/2022.emnlp-main. 473 . [54] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [55] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [56] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of computation, 19(90):297-301, 1965. [57] David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub M. Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data.",
    "based-35": "2022. [58] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [59] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models, 2022. [60] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces, 2022. [61] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Zettlemoyer Luke. Mega: Moving average equipped gated attention.",
    "based-36": "arXiv preprint arXiv:2009.10655, 2022 . [62] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1, 2021. [63] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. [64] Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse modular activation for efficient sequence modeling. arXiv preprint arXiv:2306.11197, 2023. [65] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context.",
    "based-37": "arXiv preprint arXiv:2310.01889, 2023. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages $611-626,2023$. [67] Daniel Y Fu, Hermann Kumbong, Eric Nguyen, and Christopher R\u00e9. Flashfftconv: Efficient convolutions for long sequences with tensor cores.",
    "based-38": "arXiv preprint arXiv:2311.05908, 2023. [68] Markus N Rabe and Charles Staats. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint $\\operatorname{arXiv:2112.05682,2021.}$\n[69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46 th Annual International Symposium on Computer Architecture (ISCA), pages 250-263, 2019. [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint arXiv:2305.19370, 2023. [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models.",
    "based-39": "122023. doi: $10.57967 / h f / 1595$. URL https://github.com/togethercomputer/stripedhyena\n[73] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2023. [74] Katarina Gresova, Vlastimil Martinek, David Cechak, Petr Simecek, and Panagiotis Alexiou. Genomic benchmarks: A collection of datasets for genomic sequence classification. bioRxiv, 2022. doi: 10.1101/2022. 06.08.495248. URL https://www.biorxiv.org/content/early/2022/06/10/2022.06.08.495248\n[75] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context, 2016. [76] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019. [77] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. [78] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. [79] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. [80] Colin Lockard, Prashant Shiralkar, and Xin Luna Dong. OpenCeres: When open information extraction meets the semi-structured web. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3047-3056, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1309. URL https://aclanthology.org/N19-1309. [81] Peter B\u00fcrgisser, Michael Clausen, and Mohammad A Shokrollahi. Algebraic complexity theory, volume 315. Springer Science \\& Business Media, 2013. [82] Thathachar S Jayram, Ravi Kumar, and Dandapani Sivakumar. The one-way communication complexity of hamming distance. Theory of Computing, 4(1):129-135, 2008. [83] Swastik Kopparty. Topics in algorithms and complexity theory: Spring 2020. 2020. The appendix is organized as follows:\n\n1. Appendix A includes an extended related works discussion. 2. Appendix B includes details on the IO-aware implementation and benchmarking for BASED. 3. Appendix Cincludes additional discussion of BASED architectural details. 4. Appendix D provides additional experimental results. 5. Appendix E provides experimental details. 6. Appendix Fincludes theoretical results and proofs. ## A Extended Related Work\n\nOur work relates broadly to various developments in efficient sequence modeling. In this section, we organize these related works into (1) model-based or algorithmic contributions (appendix A.1) and (2) implementation or systems-based contributions (appendix A.2). ## A. 1 Efficient Language Modeling Architectures\n\nWhile Transformers often achieve state-of-the-art language modeling quality, their design motivates various efficiency improvements when both processing input sequences and generating outputs. In particular, various works try to retain their modeling quality, while improving on their quadratic scaling $\\left(\\mathcal{O}\\left(N^{2}\\right)\\right.$ in input sequence length $N$ ) when processing inputs and $\\mathcal{O}(N M)$ time and space when decoding outputs for outputs of length $M$ (when caching prior keys and values in the attention mechanism). We note that most related lines of work build on one of two primitives: attention approximations (e.g., linear attentions, sparse attentions, sparse and low-rank attentions), or state-space models (SSMs) (which have alternative parameterizations as either \"long\" convolutional models or recurrent neueral networks). Both model classes achieve subquadratic time and space complexity when processing inputs, while linear attentions and SSMs also enable better than $\\mathcal{O}(N M)$ decoding via their ability to process inputs recurrently like a recurrent neural network (RNN). We describe each of these model classes next. ## A.1.1 Efficient Attentions\n\nWe focus on two of the most related paradigms for efficiently computing attention here, structured sparse attentions and linear attentions. We acknowledge a great deal of prior work to compute attention more efficiently, such as via locality-sensitive hashing [43, random sparse attentions [44, and sequence compression 45, 46, 47. Please see 48 for a comprehensive survey. Structured sparse attentions Structured parse attentions reduce attention's time and memory requirements by only attending over specific strided patterns or local sliding windows [17, 18, 19. For example, 17] propose computing attention only over a local window of the past $w$ tokens, such that processing sequences $N$ tokens long only takes $\\mathcal{O} N w$ time and space. [18] note that this window alone may not all capture all desired dependencies (such as long-term interactions), and propose two strided patterns to compute dot products between queries and keys further away. [19] further propose allowing specific tokens to attend to all other tokens in a dense manner. While further popularized in recent large language models (Mistral, Jiang et al. 10]), we note that these implementations use large window sizes that still leave room for improving efficiency. In Based, we introduce a hardware-guided design (using small windows) and sliding window implementation that allows us to capitalize on sparse attention's efficiency. Linear attentions Linear attentions preserve the same \"sequence-mixing\" operations as standard attention, computing dot products between queries and keys to weight corresponding values. However, their key insight is to replace the softmax in standard attention with alternative kernel functions [20]. Mechanically, by removing the $\\exp \\left(\\boldsymbol{q}^{\\top} \\boldsymbol{k}\\right)$ in favor of feature map dot-products $\\phi(\\boldsymbol{q})^{\\top} \\phi(\\boldsymbol{k})$, these methods use matrix product associativity to compute attention in $\\mathcal{O}\\left(N d^{2}\\right)$ time and space 24] (Equation 22). Furthermore, they permit a recurrent view for constant memory and $\\mathcal{O}(1)$ time per-token generation [25, 26] (Equation (3)). Prior works propose different feature maps $\\phi$ to improve linear attention modeling quality. 20 originally use the positive elu function $1+$ elu such that $\\phi(\\boldsymbol{q})^{\\top} \\phi(\\boldsymbol{k})$ remains positive and attention weights remain affine. 23] instead use the ReLU function combined with a cosine-based reweighting function to add a locality bias. Other approaches propose feature maps that aim to approximate the Softmax, such as Random Fourier Features [21, 22] the Nystrom method 49, 50, or deterministic low-degree polynomial approximations [13, 14, 51]. Finally, recent works treat the feature map as a learnable function [25], and optionally train the feature map explicitly to recover the softmax kernel 13. Combining sparse and linear attentions Finally, another line of works similar in spirit to BASED combine sparse and linear attentions. Scatterbrain [52] use a sparse and low-rank decomposition to unify past sparse and low-rank approximations. They theoretically and empirically show that any low rank approximation of attention's $\\exp \\left(Q K^{T}\\right)$ will have a much larger approximation error than a sparse plus low rank approximation, echoed in our observations. TransNormer 53] apply normalizations such as LayerNorm 554 or RMSNorm 55] to linear attention outputs in certain layers, and apply softmax attention in local chunks in other layers. ## A.1.2 Attention Alternatives\n\nWe now review other attention alternatives, which focus on improving upon the quadratic scaling of attention. Initial work in this vein uses linear time invariant state space models (SSMs) or long convolutions, which can efficiently process sequences of length $N$ in $O(N \\log N)$ time invoking the FFT-convolution theorem [56, as the sequence mixer [4, 27, 31, 57, 58, 59, 60, 61. SSMs can also be rewritten as recurrences to permit fast $O(1)$ inference. Subsequent work identified that the long convolution alone is not expressive enough to perform particular sub-tasks in language modeling. Prior work shows pure linear SSMs cannot perform associative recall, a skill that is correlated with a model's in-context learning capability [3, 62, and introduces multiplicative interactions (via gating or Hadamard product [63]) between tokens to allow the model to compare tokens in the sequence [7, 8, 9]. However, Arora et al. 1] show empirically and theoretically the class of gated convolution architectures, any architectures built from the two gating and convolution primitives, struggles to learn associative recall (on synthetic and real language data) as efficiently as attention. They show that while attention solves AR in constant many layers / with model dimension that is independent of sequence length, any gated convolution architecture uses dimensionality that scales with the sequence length - we build upon their upper bound theoretical results with a lower bound argument in Section 3.2. We also study a broader set of architectures in this work beyond gated convolutions. Arora et al. [1], Gu and Dao [5], Yang et al. [6] identify that the use of input-dependent sequence mixers is important for an architecture to perform AR as efficiently as attention. AR requires shifting information that appears prior in a sequence to interact with the current (last) tokens in the sequence, in order to predict the next token [9. While gating is one way to introduce data-dependence [7], allowing comparing tokens in two (e.g. a shifted and unshifted) sequences, it is difficult to select which information from the prefix of the sequence to shift forwards in the first place, using gating alone. Intuitively, the information to shift depends on the input's properties. Thus, several subquadratic architectures consider alternate strategies to introduce input-dependence [5, 6, 24, 61, 64. We present another strategy for efficient input-dependent sequence mixing in our work. ## A. 2 Efficient Implementations\n\nBeyond designing new model architectures, various works introduce systems-level innovations to improve training and inference efficiency. These include alternative implementations of architecture primitives such as attention [11, 65, 66, long convolutions [31, 67], and linear attention [6, 20]. They frequently achieve both\nreduced memory and increased computational speed on modern GPUs by \"fusing\" operations such as matrix multiplications into a single CUDA kernel, and designing \"IO-aware\" ways to distribute and compute the results of various read and write operations between different levels of GPU memory. ## A.2.1 Efficient Attention Implementations\n\n$[12]$ introduce FlashAttention, an alternative yet exact implementation of softmax attention that improves memory and speed by both fusing attention operations into a single CUDA kernel and distributing the attention operations to better exploit High Bandwidth Memory (HBM) and Static Random Access Memory (SRAM). They first compute attention's query-key-value dot-products, masking, and softmax, together as a single kernel. By doing so after a single load to SRAM before moving the output back to HRAM, they exploit SRAM's fast compute and reduce the total number of read-write operations. To get around SRAM's small memory size and avoid attention's quadratic memory size over input sequence length, they use tiling to split up the query, key, and value inputs into smaller \"blocks\", compute the attention operations for each block, and adjust the outputs after computing all blocks to properly normalize the softmax 68, 69]. To perform backpropagation fast on SRAM, they get around SRAM's limited storage by recomputing the gradients rather than storing them. Despite the extra operations, this IO-aware implementation still significantly improves wall-clock time during training. Similarly making use of block-wise computation, 65 instead compute attention blocks across different devices in RingAttention, enabling training and inference over much larger context lengths that scale with device count. They distribute and compute the attention operations in each block across multiple hosts in parallel, likewise keeping track of summary statistics to gather results correctly into exact attention. However, they introduce an \"overlapping\" mechanism to coordinate communication of blocks to reduce overhead. They further make use of Blockwise Parallel Transformers 70 to reduce memory, which similar to FlashAttention removes the quadratic in memory scaling of attention by dividing the attention operation into separate blocks before gathering back the adjusted softmax output with block-wise normalization statistics. As a complement to attention training and inference, 66 improve attention generation with PagedAttention. PagedAttention similarly uses block-wise computation to address memory utilization issues during generation, where the KV cache can grow an undetermined amount. Existing systems may na\u00efvely handle this by preallocating large amounts of contiguous memory. However, this can result in low utilization and computational bottlenecks. Accordingly, PagedAttention divides attention's growing KV cache into KV blocks that can be stored separately on physical memory. This enables more flexible memory management, where smaller chunks can be allocated in different locations when needed to reduce memory-based bottlenecks. In Based, we use similar blocking strategies to more efficiently compute both the second-order Taylor series linear attention and the sliding window softmax attention, and for both training and inference. ## A.2.2 Efficient Attention-Alternative Implementations\n\nBeyond optimizations for attention, various works also introduce similar \"IO-aware\" implementations to improve memory usage and speed for convolutional and recurrent operations. We overview the most relevant works to Based, which make use of similar techniques such as fusing operations and blocking (tiling) to compute results in SRAM. Long convolutions 67] improve the efficiency of long convolutions on modern GPUs. They build on using the Fast Fourier Transform (FFT), which enables computing convolutions with filter sizes equal to input sequence length from $\\mathcal{O}\\left(N^{2}\\right)$ (if $N$ is filter size and sequence length) to $\\mathcal{O}(N \\log N)$. However, to compute this algorithm efficiently on GPUs, they break down the convolution into separate matrix multiply operations via a Monarch decomposition of the FFT, which allows both (1) fusing multiple steps of the FFT together (for reduced read-write operations) and (2) scheduling these operations for fast computation in SRAM while remaining under the smaller SRAM memory constraints. Recurrence [5 improve the efficiency of recent neural state-space models (SSMs) 27] using several similar techniques to FlashAttention, specifically with regard the recurrent view. They load the SSM parameters into SRAM for computation before saving results back in HBM, and also use recomputation where during\nbackpropagation the intermediate states are not saved but rather recomputed when inputs are loaded from HBM to SRAM. They finally improve wall-clock time by parallelizing the recurrent view of the SSM as a parallel scan. Linear Attention Finally, several works propose techniques to improve the real-world wall-clock time and memory-usage of linear attention. 20] fuse several operations in the causal dot product of linear attention. 6] use blocking to divide the linear attention matrices into SRAM-computable chunks in FlashLinearAttention. As a trade-off between the slow yet memory-efficient RNN view of linear attention and faster but memoryintensive parallel \"standard attention\" view, they further optimize a \"chunk-wise\" implementation of linear attention [71. When processing input sequences, the input is first divided into several non-overlapping chunks, where we save memory by computing \"kv states\" at the end of each chunk, and save time by computing the tokens in a given chunk in parallel. ## B IO Aware Implementations\n\nIn this section, we provide additional details pertaining to the benchmarking experiments and we provide microbenchmarking results for the individual kernels we contribute, to complement the end-to-end benchmarking results in the Section 6\n\n## B. 1 Forward / Generation Prefill\n\nBaselines In Figure 4 we implement Based using our IO-aware Taylor linear attention Algorithm 1 The baseline approach presented in 13, prior to our kernel, uses the popular linear attention CUDA kernel from Fast Transformers for computing the causal dot product 20, 34. ${ }^{5}$. The listing below shows the baseline implementation for reference (where line 76-77 can be computed using pure PyTorch or the Fast Transformers kernel). Micro Benchmark To complement the end-to-end architecture benchmarks in Section 6 we provide micro benchmark results for only the linear attention forward pass in Figure 5. ![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-23.jpg?height=589&width=1539&top_left_y=949&top_left_x=289)\n\nFigure 5: Time (ms) for different ways of computing the Taylor linear attention forward pass - using Pure PyTorch (shown in the Listing and introduced in 13), Fast Transformers kernel (as indicated in the listing) [24, 34], or our BASEd kernel (Algorithm 1). (Left) Varying the batch size at fixed sequence length 1024. (Right) Varying the sequence length at fixed batch size 4. (All) Benchmarking uses 16 feature dimension, 16 heads, 64 head dimension, and focuses on the numerator of the linear attention.",
    "based-40": "Each point represents the median across 10 iterations is measured on a single NVIDIA H100 GPU. Lines terminate on out-of-memory errors. ```\nfrom einops import rearrange\nimport torch\nfrom torch import nn\nclass TaylorExp(nn.Module):\n    \"\" \"\n    Feature map to compute 2nd-order Taylor approx. of exp(q^ T k / sqrt(d))\n    \" \" \"\n    def __init__(self, input_dim, head_dim_idx, temp=None, eps=1e-12):\n        super().__init__()\n        self.input_dim = input_dim\n        self.head_dim_idx = head_dim_idx\n        self.temp = 1.0 if temp is None else temp\n    5}\\mathrm{ https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/causal_linear_attention. py\n```\n\n```\n    self.eps = eps\n    self.r2 = math.sqrt(2)\n    self.rd = math.sqrt(self.input_dim)\n    self.rrd = math.sqrt(self.rd)\n    def forward(self, x: torch.Tensor):\n    # Get 2nd-order terms (rearrange(x * x), '... m n -> ... (m n)')\n    x2 = (x.unsqueeze(-1) * x.unsqueeze(-2)).flatten(start_dim=-2) / self.r2\n    term1 = torch.ones(x[..., :1].shape).to(x.device)\n    term2 = x / self.rrd\n    term3 = x2 / self.rd\n    terms = [term1, term2, term3]\n    return torch.cat(t for t in terms), dim=self.head_dim_idx)\nclass TaylorLinAttn(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.d_model = d_model\n        self.feature_dim = 16\n        self.num_heads = 16\n        self.num_key_value_heads = 16\n        self.head_dim = self.d_model // self.num_key_value_heads\n        self.eps = 1e-12\n        feature_map_kwargs = {\n            \"input_dim\": self.feature_dim,\n            \"head_dim_idx\": -1,\n            \"eps\": 1e-12,\n        }\n        self.feature_map = TaylorExp(**feature_map_kwargs)\n        self.proj_q = nn.Linear(\n            self.d_model, self.feature_dim * self.num_heads, bias=False\n    )\n    self.proj_k = nn.Linear(\n        self.d_model, self.feature_dim * self.num_heads, bias=False\n    )\n    self.proj_v = nn.Linear(\n                self.d_model, self.num_key_value_heads * self.head_dim, bias=False\n    )\n    self.proj_o = nn.Linear(\n        self.num_heads * self.head_dim, self.d_model, bias=False\n    )\n    def forward(self, hidden_states: torch.Tensor, *args, **kwargs):\n    b, l, _ = hidden_states.size()\n    q = self.proj_q(hidden_states)\n    k = self.proj_k(hidden_states)\n    v = self.proj_v(hidden_states)\n    q = q.view(b, l, self.num_heads, self.feature_dim).transpose(1, 2)\n    k = k.view(b, l, self.num_key_value_heads, self.feature_dim).transpose(1, 2)\n    v = v.view(b, l, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n    # Linear attention\n    q, k = self.feature_map(q), self.feature_map(k)\n    q, k, v = q.unsqueeze(-2), k.unsqueeze(-2), v.unsqueeze(-1)\n    # Compute attention causal (alternatively use the Fast Transformers kernel)\n    num = (q* (k * v).cumsum(dim=2)).sum(dim=-1)\n    denom = (q* k.cumsum(dim=2)).sum(dim=-1) + self.eps\n    y = (num / denom)\n    y = rearrange(y, \"b h l d -> b l (h d)\")\n    y = self.proj_o(y)\n    return y\n```\n\nListing 1: PyTorch implementation of Taylor linear attention. ```\nAlgorithm 1 Computing the \\(0^{t h}(T 0), 1^{\\text {st }}(T 1), 2^{\\text {nd }}(T 2)\\) Order Taylor Linear Attention Terms\nInput: Input projected hidden states \\(q, k, v \\in \\mathbb{R}^{N \\times d}\\). Output: Output \\(y=T 0+T 1+T 2 \\in \\mathbb{R}^{N \\times d}\\)\n    Parallelize into batch \\(\\times\\) heads parallel computations, with \\(\\mathrm{n}_{\\text {warps }}=8\\) warps per block. Within a block:\n    Define tile size \\(T \\quad \\triangleright T=16\\) in BASED\n    Define \\(\\mathrm{n}_{\\text {tiles }}=\\frac{N}{T} \\quad \\triangleright\\) Block along the sequence dimension\n    Define \\(\\mathrm{n}_{\\text {blocks }}=\\mathrm{n}_{\\text {tiles }} / \\mathrm{n}_{\\text {warps }} \\quad \\triangleright\\) Block along the number of warps\n    Define tic \\(=0\\), toc \\(=1\\)\n\\(\\triangleright\\) Flags for asynchronous data loading\n```\n\nCreate SRAM buffers $B_{q}, B_{k}\\left(\\right.$ Size $\\left.2 \\times \\mathrm{n}_{\\text {warps }} \\times T \\times T\\right)$ and $B_{v}\\left(\\right.$ Size $\\left.2 \\times \\mathrm{n}_{\\text {warps }} \\times T \\times 4 T\\right)$\nCreate SRAM buffers A0, A1, A2 (Size $\\mathrm{n}_{\\text {warps }} \\times T \\times 4 T$ ) for storing interim. results for $T 0, T 1, T 2$ as warps process the sequence\nCreate SRAM buffers total $_{A 0}$ and total $_{A 1}$ to hold cumulative (\"KV\") state corresponding to $T 0, T 1$\nCreate SRAM buffers $y$ of (Size $\\mathrm{n}_{\\text {warps }} \\times T \\times 4 T$ ) for storing the final output\nCreate register fragments $\\mathrm{q}_{\\mathrm{a}}, \\mathrm{q}_{\\mathrm{b}}, \\mathrm{k}_{\\mathrm{a}}, \\mathrm{k}_{\\mathrm{b}}, \\mathrm{q}_{\\text {frag }}, \\mathrm{k}_{\\text {frag }}, \\mathrm{qk}_{\\mathrm{accum}}$ of size $16 \\times 16$. We create register fragments $\\mathrm{v}_{\\text {frag }}, \\mathrm{a}_{\\text {frag }}$, $\\mathrm{a} 1_{\\text {accum }}, A 2_{0}, A 2_{1}, \\mathrm{qA} 2_{\\text {accum }}$, oaccum of size $16 \\times 64$. These fragments are for holding data during in-register computation. Initialize the fragments to 0 . Each warp loads initial tiles $B_{q}[$ tic $][$ warpid $] \\leftarrow Q_{t}, B_{k}[$ tic $][$ warpid $] \\leftarrow K_{t}$ and $B_{v}[$ tic $][$ warpid $] \\leftarrow V_{t} \\triangleright$ HBM into SRAM\nfor cur $_{\\text {block }} \\in\\left[0\\right.$. n $\\left._{\\text {blocks }}-1\\right]$; tic $=0 \\oplus=1$, toc $\\oplus 1$ do $\\triangleright$ XORs tic and toc to toggle. Warp loads $B_{q}[$ toc $][$ warpid $] \\leftarrow Q_{t}$ for cur $_{\\text {block }}+1 \\quad \\triangleright$ HBM to SRAM\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-25.jpg?height=47&width=700&top_left_y=1145&top_left_x=325)\nWarp loads $B_{v}[$ toc $][$ warpid $] \\leftarrow V_{t}$ for cur $_{\\text {block }}+1$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-25.jpg?height=47&width=1560&top_left_y=1261&top_left_x=326)\nWarp loads $\\mathrm{k}_{\\text {frag }} \\leftarrow k[$ tic $][$ warpid]\nWarp loads $\\mathrm{v}_{\\text {frag }} \\leftarrow v[$ tic $][$ warpid $]$\nCompute the warp-local cumulative sum on $\\mathrm{v}_{\\text {frag }} \\rightarrow \\mathrm{a}_{\\text {frag }} . \\quad \\triangleright \\mathrm{T} 0$ computation\nAdd the running $A 0$ to the current a $0_{\\text {frag }}$\nCompute $\\mathrm{q}_{\\text {frag }} \\mathrm{k}_{\\text {frag }}{ }^{T}$ (attention) and make it causal and store in a $\\mathrm{qk}_{\\text {accum }} \\triangleright \\mathrm{T} 1$ computation Compute $\\mathrm{qk}_{\\text {accum }} \\mathrm{V}_{\\text {frag }} \\rightarrow \\mathrm{O}_{\\text {accum }} \\quad \\triangleright$ Store causal $q k^{T} v$\nWarps store $\\mathrm{k}_{\\text {frag }}^{\\mathrm{T}} \\mathrm{v}_{\\text {frag }} \\rightarrow \\mathrm{a} 1_{\\text {accum }}$ and write $\\mathrm{a}_{\\text {accum }} \\rightarrow A 1[$ warpid $] \\quad \\triangleright$ Register to SRAM\nCompute cumulative sum over $A 1$ in SRAM, updating $A 1$ entries\nWarps read $A 1$ tiles back to registers $\\triangleright$ Each warp now contains its preceeding $A 1$\nWarps multiply the values in register with $\\mathrm{q}_{\\text {frag }}$ to update $\\rightarrow \\mathrm{o}_{\\text {accum }} \\quad \\triangleright$ Add in T 1 to the running result\nUpdate $\\mathrm{a}_{\\text {frag }} \\rightarrow \\mathrm{O}_{\\text {accum }}$\n$\\triangleright$ Add in T0 to the running result\nSquare $\\mathrm{qk}_{\\text {accum }}$, multiply with $\\mathrm{v}_{\\text {frag }}$ and add $\\rightarrow \\mathrm{o}_{\\text {accum }} \\quad \\triangleright$ Add in diagonal T 2 to the running result\nSum the values of oaccum into $y[$ warpid $]$\nfor block in $\\mathrm{n}_{\\text {warps }}$ iterations do $\\triangleright$ Remaining T2 computation; Assumes feature dimension 16 Each of 8 warps copies the same slice of $q\\left[\\right.$ tic] [warpid] to 2 registers $q_{a}, q_{b}$\nEach thread $j$ in the warp computes $q_{a}[:, 2 j] q_{a}$ for dimension $2 j$, and for $2 j+1$ (and for $q_{b}$ ). Together the threads compute the 256 elements resulting from the second order outer product in the feature map. Each warp stores two slices of $A 2: A 2_{0}$ and $A 2_{1} \\quad \\triangleright$ Partitioning the large $A 2$ across warp registers\nAccumulate both $q_{a} A 2_{0}$ and $q_{b} A 2_{1} \\rightarrow \\mathrm{qA} 2_{\\text {accum }}$\nWarp writes qA2 accum $\\rightarrow A 2$ [warpid] $\\triangleright$ Register to SRAM\nSum results across all in $A 2$ [warpid] and store the sum in $y[$ block] $\\triangleright$ Add in T2\nEach of 8 warps copies the same slice of $k[$ tic $][$ block $]$ to 2 registers $k_{a}, k_{b} \\triangleright$ KV state update Square $k_{a}$ and $k_{b}$\nEach of the 8 warps loads $v[$ tic $][$ block $]$ to $\\mathrm{v}_{\\text {frag }}$ in register\nMultiply $k_{a}$ and $\\mathrm{v}_{\\text {frag }}, k_{b}$ and $\\mathrm{v}_{\\text {frag }}$ and accumulate the results into $A 2_{0}$ and $A 2_{1}$, the two in-register slices of $A 2$ for the warp, respectively\nEnd. Store $y$. Optionally store $A 0, A 1, A 2$ (comprising the \"KV state\") for generation. $\\quad \\triangle$ SRAM to HBM\n\nAlgorithm Here we revisit the key equations we aim to compute and then describe Algorithm 1 in detail. Objective First recall from Section 4 :\n\n$$\n\\boldsymbol{o}_{i}=\\sum_{j=1}^{i} \\frac{\\phi\\left(\\boldsymbol{q}_{i}\\right)^{\\top} \\phi\\left(\\boldsymbol{k}_{j}\\right) \\boldsymbol{v}_{j}}{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)}=\\frac{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\sum_{j=1}^{i}\\left(\\phi\\left(\\boldsymbol{k}_{j}\\right)^{\\top} \\boldsymbol{v}_{j}\\right)}{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)}\n$$\n\nwhere $q_{i}$ reflects the $i^{\\text {th }}$ of $N$ total tokens in the sequence and every query attends to every past key in $\\mathcal{O}\\left(N d^{2}\\right)$ time and space complexity for embedding dimension $d$. To approximate $\\exp \\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j} / \\sqrt{d}\\right)$, we use the $2^{\\text {nd }}$-order Taylor series feature map, picking $\\phi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d^{2}}$ such that\n\n$$\n\\phi\\left(\\boldsymbol{q}_{i}\\right)^{\\top} \\phi\\left(\\boldsymbol{k}_{j}\\right)=1+\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j}+\\frac{\\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j}\\right)^{2}}{2}\n$$\n\nIn this section, we will refer to $q_{i}$ as a tile of data (e.g. of 16 tokens) instead of as a single token since the hardware operates on chunks of data in parallel. Algorithm description In Algorithm 1. we allow each thread block to compute the result for a particular (batch, head) input. Within the thread block, we use 8 warps / workers to produce the result. We initialize data structures $B_{q}, B_{k}, B_{v}$ in SRAM and $\\mathrm{q}_{\\mathrm{a}}, \\mathrm{q}_{\\mathrm{b}}, \\mathrm{k}_{\\mathrm{a}}, \\mathrm{k}_{\\mathrm{b}}, \\mathrm{q}_{\\mathrm{frag}}, \\mathrm{k}_{\\mathrm{frag}}, \\mathrm{v}_{\\text {frag }}$ in register to hold chunks or tiles of the $q, k, v$ inputs. We initialize data structures $A 0, A 1, A 2$ in SRAM and a $0_{\\text {frag }}, \\mathrm{al}_{\\text {accum }}, \\mathrm{qA} 2_{\\text {accum }}$ in register to hold computation for the running $K V$ state for the $0^{t h}, 1^{\\text {st }}, 2^{\\text {nd }}$ order Taylor polynomial terms. We partition the computation along the sequence dimension into $n_{\\text {blocks }}$, where in each loop from 1 to $\\mathrm{n}_{\\text {blocks }}$, the warps load the next 8 chunks into fast memory. Note that for 2048 sequence length and 8 warps, 16 tile size, we end up with $\\mathrm{n}_{\\text {tiles }}=128$ and $n_{\\text {blocks }}=16$. In each iteration, each warp loads in $16 \\times 16$ tiles of $q, k$ and $16 \\times 64$ tiles of $v$, where 16 indicates a chunk of 16 tokens along the sequence dimension and 16,64 are the feature and head dimensions respectively. Once tiles are streamed in, we do not need to reuse them, which is key to the efficiency of linear attention. Zeroeth order Taylor terms: During the computation, for the $0^{t h}$ term in the Taylor polynomial, $q, k$ are 1 after we apply the feature map (Equation (6). Therefore, computing a cumulative sum over $q\\left(k^{T} v\\right)$ reduces to maintaining a cumulative sum of $v$ as we iterate across the sequence. First order Taylor terms: Next we consider the $1^{\\text {st }}$ order terms. On-diagonal: First consider the on-diagonal blocks, e.g. with respect to tiles $q_{i}, k_{i}, v_{i}$. For these, we simply multiply $q^{T} k$, masking (making it causal), and then multiplying with $v$, following the order of operations in standard attention. This makes it easy to apply the masking ( 0 out non-causal elements). Now each warp contains a local result for its set of on-diagonal tiles of $q_{i}, k_{i}, v_{i}$. Off-diagonal: However, we need to obtain a global cumulative sum where $\\left(q_{i}^{T} k_{j}\\right) v_{j}$ depends on all $j \\in[1 .",
    "based-41": ". i]$ (Equation (5)). Each warp is therefore missing values for tiles $j \\in[1 .",
    "based-42": ". i-1]$. To incorporate this computation, we will now compute the cumulative $K V$ hidden state for the warp up until $i-1$ and multiply this with the local tile of $q$ (i.e. $q_{f r a g}$ ). To accomplish this, note in Algorithm 1 we multiply $\\mathrm{k}_{\\text {frag }}{ }^{T}$ and $\\mathrm{v}_{\\text {frag }}$ to compute local tiles of the hidden state, local to each warp, in thread register. To perform the global cumulative sum across the 8 warps' local results, we write from registers (thread specific) to $A 1$ in SRAM (shared across warp threads). After computing the global cumulative sum in shared memory, each warp loads back the $K V$ state (in $A 1$ ) into its registers such that it contains all the preceeding $K V$ (history) for tiles $[1 . . i-1]$. We then multiply the local $\\mathrm{q}_{\\mathrm{frag}}$ in register with this $K V$ state to update the final output for the $1^{\\text {st }}$ up until the current $\\mathrm{n}_{\\text {blocks. }}$ Note that we maintain the running $K V$ state corresponding to the $1^{\\text {st }}$ order term in $A 1$ shared memory for the next iteration along $\\mathrm{n}_{\\text {blocks }}$. Second order Taylor terms: We finally need to compute the $2^{\\text {nd }}$ order term. Similar to the $1^{\\text {st }}$ order term, we'll consider On-diagonal: We can leverage the computation from above. We'll square the causal $\\left(q k^{T}\\right)^{2}$ from above and multiply with $\\mathrm{v}_{\\text {frag }}$ to obtain the portion of the $2^{\\text {nd }}$ order term corresponding to the on-diagonal tiles $q_{i}, k_{i}, v_{i}$. Off-diagonal: Again, we also need to compute the result with respect to tiles $[1 .",
    "based-43": ". i-1]$. - Partitioning KV hidden state for $2^{\\text {nd }}$ order Because the hidden state for the second order term is large $\\left(\\mathcal{O}\\left(d^{2} D\\right)\\right.$ in feature dimension $d$ and head dimension $\\left.D\\right)$ and warps have a limited number of registers, we slice its storage across the registers of the 8 warps. Considering the the $16^{2} \\times 64\\left(d^{2} \\times D\\right)$\nhidden state (stored in $A 2$ SRAM in Algorithm 1), we divide this into 16 slices along the sequence dimension and let each of the 8 warps handle 2 of the $16 \\times 64$ slices (stored in $A 2_{0}, A 2_{1}$ fragments in thread registers in Algorithm(1). Warp $i$ will maintain slices $2 i$ and $2 i+1$ in two registers per thread. - Computing output for $2^{\\text {nd }}$ order Each warp $i$ loads in one tile of $q_{i}$ into 2 registers. We will use the 32 threads in the warp to compute the 256 outer product terms for each token computed by the Taylor $2^{\\text {nd }}$ order term (for feature dimension 16). Next, the threads multiply these 256 terms with the running $A 2_{0}$ and $A 2_{1}$ slices. The results for the two slices are summed in register and then stored in SRAM ( $A 2$ [warpid]). Since $o_{i}$ is ultimately the sum of $q_{i}$ terms multiplied with all slices of $A 2$ (Equation (5)), we then sum the results from all the warps together (which hold the remaining slices of $A 2$ ) and store the result in $y[\\mathrm{block}]$. We can think of $y$ [block] as holding the result up until the ( $8 \\times$ cur $_{\\text {block }}+$ block $)$ tile of tokens (note 8 is because in each\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-27.jpg?height=44&width=1148&top_left_y=756&top_left_x=326)\n\n- Updating the $K V$ state: For block $=i$, we load in $k[i], v[i]$ tiles of size $16 \\times 16$ and $16 \\times 64$ respectively to registers $k_{a}, k_{b}, \\mathrm{v}_{\\text {frag }}$. We compute the 256 outer product terms on $k[i]$ using the 32 threads, multiply with $\\mathrm{v}_{\\text {frag }}$, and store the result in the $A 2_{0}, A 2_{1}$ running state. The final result in $y$ is summed into the output to complete the $2^{n d}$ order computation. ## B. 2 Next Token Prediction\n\nDuring next token prediction in generation, we use IO-aware algorithms for the expensive $K V$ state update in Taylor linear attention and for the sliding window attention computation. ## B.2.1 Taylor linear attention\n\nThe KV update in PyTorch is provided in the following listing. In Figure 6we benchmark the speed of the PyTorch implementation against our kernel. ![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-28.jpg?height=733&width=969&top_left_y=631&top_left_x=575)\n\nFigure 6: Time (ms) for computing the Taylor linear attention recurrent update using Pure PyTorch (shown in the Listing and introduced in [13]) vs. our BASED kernel (Algorithm 24. Benchmarking uses 16 feature dimension, 16 heads, 64 head dimension, and focuses on the numerator of the linear attention. Each point represents the median across 10 iterations is measured on a single NVIDIA H100 GPU. ```\nfrom einops import rearrange\nimport torch\nfrom torch import nn\ndef step(self, kv_state: torch.Tensor, k_state: torch.Tensor, q: torch.Tensor, k: torch. Tensor, v: torch.Tensor):\n        \"\"\"\n        Compute linear attention with recurrent view\n        -> Assume q.shape is (b, h, 1, D); k and v.shape are (b, h, l, d), where D is the\n    dimension after applying the feature map and d is the head dimension. \"\"\"\n        b, h, l, d = q.shape\n        assert l == 1, f'q.shape is {q.shape} but should be ({b}, {h}, 1, {d})'\n        # Expand dims for broadcasting to compute linear attention\n        q, k, v = q.unsqueeze(-2), k.unsqueeze(-2), v.unsqueeze(-1)\n        kv_state += k[:, :, -1:] * v[:, :, -1:]\n        k_state += k[:, :, -1:]\n        # Compute linear attention\n        num = (q * kv_state).sum(dim=-1)\n        y = num / ((q** k_state).sum(dim=-1) + self.eps)\n        y = rearrange(y, 'b h l d -> b l (h d)').to(q.dtype)\n        return self.dropout(self.out_proj(y))\n```\n\nListing 2: PyTorch implementation of Taylor linear attention KV update\n\n```\nAlgorithm 2 Computing \\(K V\\) State Updates\nInput: \\(K V_{t-1}\\) state \\(\\in \\mathbb{R}^{H d^{\\prime 2} d}\\), at time \\(t\\).",
    "based-44": "Featurized \\(q, k \\in \\mathbb{R}^{B \\times H \\times 1 \\times D}\\) and \\(V \\in \\mathbb{R}^{B \\times H \\times 1 \\times d}\\), for \\(d\\) as the head\n    dimension (e.g. 64) and \\(D\\) as the expanded feature map dimension (e.g. \\(273=1+16+16^{2}\\) for feature dim 16). To\n    be hardware-friendly, we let \\(D=320\\) (s.t. \\(320 \\bmod 64=0\\) ) via padding. Output: Updated \\(K V_{t}\\) state. Parallelize into batch \\(\\times\\) heads parallel computations, with \\(\\mathrm{n}_{\\text {warps }}=8\\) warps per block. Within a block:\n    Define \\(\\mathrm{n}_{\\text {threads }}=\\mathrm{n}_{\\text {warps }} \\times 32 \\quad \\triangleright\\) Assuming 32 threads per warp\n    Define buffer \\({ }_{\\text {size }}=\\mathrm{n}_{\\text {warps }} \\times 8 \\times d\\)\n    Define total \\(_{\\text {batches }}=\\frac{D}{\\mathrm{n}_{\\text {warps }} \\times 8} \\quad \\triangleright\\) E.g. total \\(_{\\text {batches }}=5\\) if \\(D=320\\); For \\(k, \\frac{320}{5}=64\\) values per batch\n    Define tic \\(=0\\), toc \\(=1\\)\n    Create SRAM buffer \\(B_{q}\\) (Size \\(D\\) ) for \\(q\\)\n    Create SRAM buffer \\(B_{k}\\) (Size \\(D\\) ) for \\(k\\)\n    Create SRAM buffer \\(B_{v}\\) (Size \\(d\\) ) for \\(V\\)\n    Create SRAM buffer \\(B_{k v s}\\) (Size \\(2 \\times\\) buffer \\(_{\\text {size }}\\) ) for storing blocks of \\(\\mathrm{kv}_{\\text {state }}\\)\n    Create SRAM buffer o (Size \\(d\\) ) for output. Create SRAM buffer A (Size \\(\\mathrm{n}_{\\text {warps }} \\times d\\) ) for intermediate computation\n    Create register buffer \\(\\mathrm{v}_{\\text {reg }}\\) (Size 2) to store \\(V\\) data\n    Create register \\(\\mathrm{A}_{\\text {reg }}\\) (Size 2) for intermediate computation\n    Warps load \\(B_{q} \\leftarrow q \\quad \\triangleright\\) HBM to SRAM; Load all \\(D=320\\) elements of \\(q\\)\n    Warps load \\(B_{k} \\leftarrow k\\)\n    Warps load \\(B_{v} \\leftarrow V\\)\n    Warps load chunk \\(B_{\\text {kvs }}[\\mathrm{tic}] \\leftarrow \\mathrm{kv}_{\\text {state }} \\quad \\triangleright\\) Load \\((1 \\times 64) \\times 64\\) of the \\(\\left(\\right.\\) total \\(\\left.{ }_{\\text {batches }} \\times 64\\right) \\times 64\\) elements in \\(K V_{t-1}\\)\n    Initialize \\(m=0\\)\n    for Threads \\(j \\in[0 . .31] ; j<d ; j+=32, m+=1\\) do \\(\\quad \\triangleright\\) Each thread holds 2 values \\((~ d=64 ; 32\\) threads \\()\\)\n        Load \\(\\mathrm{v}_{\\text {reg }}[m] \\leftarrow v[j] \\quad \\triangleright\\) SRAM to Register; Now \\(v[j]\\) is stored in thread \\(j\\) mod 32\n    for \\(i \\in\\left[0\\right.\\). .total \\(\\left._{\\text {batches }}\\right] ; i=i+1\\), tic \\(\\oplus 1\\), toc \\(\\oplus 1\\) do\n```\n\n```\n        Loads \\(B_{k v s}[\\) toc \\(] \\leftarrow\\) next batch of \\(\\mathrm{kv}_{\\text {state }} \\quad \\triangleright\\) Asynchronous loads of next batch\n    for \\(j=\\) warpid; \\(j<d ; j+=\\mathrm{n}_{\\text {warps }}\\) do \\(\\quad \\triangleright\\) Each of the 8 warps loads 8 of the 64 rows of \\(k, q\\) in the batch\n        \\(\\mathrm{k}_{\\mathrm{val}} \\leftarrow B_{k}[i * d+j] \\quad \\triangleright\\) Grab single rows \\(q[i]\\) and \\(k[i]\\), Broadcast to all threads\n        \\(\\mathrm{q}_{\\text {val }} \\leftarrow B_{q}[i * d+j]\\)\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-29.jpg?height=58&width=1517&top_left_y=1727&top_left_x=369)\n\n```\n            Initialize \\(m=0\\)\n            for Thread \\(k \\in[0 . .31] ; k<d ; k+=32, m+=1\\) do\n                \\(p[k]+=\\mathrm{k}_{\\text {val }} * \\mathrm{v}_{\\mathrm{reg}}[m] \\quad \\triangleright\\) Update running state by multiplying broadcasted \\(\\mathrm{k}_{\\mathrm{val}}\\) with the full \\(\\mathrm{v}_{\\mathrm{reg}}\\)\n                        \\(\\triangleright\\) This updates a \\(1 \\times d\\) strip of the \\(d \\times D\\) full \\(K V_{t}\\) outer product\n                \\(\\mathrm{A}_{\\mathrm{reg}}[m]+=\\mathrm{q}_{\\text {val }} * p[k] \\quad \\triangleright\\) Multiply \\(\\mathrm{q}_{\\text {val }}\\) with the running state, updating all values in the \\(1 \\times d\\) output\n                Write out new \\(K V_{t}\\) state for this batch: \\(B_{k v s}[\\) tic \\(][k]\\)\n                    \\(\\triangleright\\) SRAM to HBM\n    Initialize \\(m=0\\)\n    for Threads \\(j \\in[0 . .31] ; j<d ; j+=32, m+=1\\) do \\(\\quad \\triangleright\\) Each thread holds info for 2 of the 64 output values\n        Store \\(A[\\) warpid \\(][j] \\leftarrow \\mathrm{A}_{\\text {reg }}[m] \\quad \\triangleright\\) Register to SRAM\n    for Thread \\(j ; j<d ; j+=\\mathrm{n}_{\\text {threads }}\\) do \\(\\quad \\triangleright d=64\\) threads put values from first warp in \\(n_{j}\\)\n        \\(n_{j}=A[0][j] \\quad \\triangleright\\) Each warp had only computed output values for a subset of (e.g. 8) rows of \\(k\\) and \\(q\\)\n        for \\(w \\in\\left[0 . . \\mathrm{n}_{\\text {warps }}\\right]\\) do\n            Sum the \\(n_{j}+=A[w][j]\\) across \\(\\quad \\triangleright\\) Need to combine results across warps\n        Store \\(o[j] \\leftarrow n_{j}\\)\n    Write output o\n                            \\(\\triangle\\) SRAM to HBM\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-30.jpg?height=784&width=1053&top_left_y=253&top_left_x=533)\n\nFigure 7: Time (ms) for different ways of computing sliding window attention next token prediction \u2014 using PyTorch, Flash Attention (which supports a sliding window function), or our inference kernel. Each point represents the median across query tokens at different token positions in the generation $\\in\\{100,250,500,750\\}$. ## B.2.2 Sliding window attention\n\nBaselines During training / prefill, we use the Flash Attention sliding window implementation 11]. Our IO-aware implementation focuses on next token prediction. In the listing below, we include a Torch reference. Our IO-aware sliding window attention algorithm is provided in 3 . The key insight is to fuse operations in thread registers to minimize slower SRAM to register data movement. Micro Benchmark We benchmark key baselines (Torch, Flash Attention-2 [11], and the Based kernel on an NVIDIA H100 GPU in Figure 7. The benchmark uses window size 64, head dimension 64, and number of heads 16. We vary the batch size on the $x$ axis and repeat the median timing across iterations on the $y$ axis. Note that these timings include only the attention computation and not the time for updating the $K V$-cache state. These timings also do not include any processing for Rotary encodings (as shown below). ```\nimport torch\nfrom torch import nn\n\" \" \"\nb: batch size\nh: number of heads\nn: sequence length\nd: head dimension\nw: window size\nqw: b x h x 1 x d\nkw: b x h x w x d\nvw: b x h x w x d\n\" \" \"\nw = torch.einsum(\"bhod, bhnd-> bhn\", qw, kw)\na = torch.nn.functional.softmax(w, dim=-1)\nresult = torch.einsum(\"bhn,bhnd->bhd\", a, vw)\n```\n\nListing 3: PyTorch implementation of Sliding Window\n\n```\nAlgorithm 3 Sliding window generation\nInput: \\(K V_{t-1}\\) state \\(\\in \\mathbb{R}^{H w d}\\), at time \\(t\\) and projected hidden states \\(q, k, v \\in \\mathbb{R}^{B \\times H \\times 1 \\times d}\\), for \\(H\\) heads, head dimension\n    \\(d\\), sliding window size \\(w\\), and batch size \\(B\\).",
    "based-45": "Output: Updated \\(K V_{t}\\) state. Parallelize into batch \\(\\times\\) heads parallel computations, with \\(\\mathrm{n}_{\\text {warps }}=4\\) warps per block. Within a block:\n    Define tile size \\(T \\quad \\triangleright T=16\\) in BASED\n    Define \\(\\mathrm{n}_{\\text {threads }}=\\mathrm{n}_{\\text {warps }} \\times 32 \\quad \\triangleright\\) Assuming 32 threads per warp\n    Create SRAM buffers \\(B_{k}\\) and \\(B_{v}\\) (Each of size \\(4 T \\times 4 T\\) ) to hold \\(k, v . \\quad \\triangleright\\) Assumes \\(4 T=64\\) is the \\(w, d\\)\n    Create SRAM vector \\(B_{q}\\) (Size \\(\\left.1 \\times 4 T\\right)\\) to hold \\(q\\) during the kernel execution. \\(\\quad \\triangleright\\) Single query, assume \\(d=64\\)\n    Create SRAM vector \\(B_{w}(\\) Size \\(1 \\times 4 T)\\) of type float for intermediate attention computation. Create SRAM vector \\(B_{o}(\\) Size \\(1 \\times 4 T)\\) to hold the output. \\(\\quad \\triangleright\\) Single output, assume \\(d=64\\)\n    Create SRAM buffers max and sum (Each of workers by float size). Create register fragments \\(\\mathrm{q}_{\\text {reg }}, \\mathrm{k}_{\\text {reg }}, \\mathrm{v}_{\\text {reg }}\\) to hold data during fused computation in-register. Create register fragments \\(\\mathrm{w}_{\\text {reg }}(\\) size \\(1 \\times 4 T)\\) and \\(\\mathrm{wv}_{\\text {reg }}(\\) size \\(4 T \\times 1)\\) to store intermediate computation in-register. Create register fragment \\(o_{\\text {reg }}\\) (size \\(4 T \\times 1\\) ) to store output in-register. Loads \\(B_{k} \\leftarrow k\\) using \\(\\mathrm{n}_{\\text {threads }} ; B_{v} \\leftarrow v\\) using \\(\\mathrm{n}_{\\text {threads }} ; B_{q} \\leftarrow q\\) using one warp. \\(\\triangleright\\) HBM to SRAM\n    Loads \\(\\mathrm{q}_{\\mathrm{reg}} \\leftarrow B_{q} . q\\) gets broadcasted to all warps. \\(\\triangleright\\) SRAM to Register\n    Loads \\(\\mathrm{k}_{\\text {reg }} \\leftarrow B_{k}\\) [warpid]. Each warp gets \\(T \\times 4 T\\) of the \\(4 T \\times 4 T\\) in \\(B_{k}\\) (i.e. a column). Loads \\(\\mathrm{v}_{\\text {reg }} \\leftarrow B_{v}[\\) warpid \\(]\\).",
    "based-46": "Each warp gets \\(T \\times 4 T\\) of the \\(4 T \\times 4 T\\) in \\(B_{v}\\) (i.e. a column). Initialize \\(\\mathrm{w}_{\\text {reg }}\\) to zero\n    \\(\\mathrm{w}_{\\text {reg }} \\leftarrow \\mathrm{q}_{\\text {reg }} \\mathrm{k}_{\\mathrm{reg}} \\quad \\triangleright\\) Matrix-vector (GEMV) multiplication\n    Initialize float \\(m=-\\infty\\) for the max \\(\\quad \\triangleright\\) Obtain the max across tiles for Softmax\nUpdate \\(m \\leftarrow \\max \\left(\\mathrm{w}_{\\text {reg }}\\right)\\) with the max from the local data\n\\(\\max [\\) warpid \\(] \\leftarrow m\\) for all warps to access\nIterate over \\(\\mathrm{n}_{\\text {warps }}\\) entries in max buffer to compute the global max of \\(\\mathrm{w}_{\\text {reg }}\\)\nPut global max back into each warp's \\(m\\) float\n    Initialize float \\(s=0\\) for the sum \\(\\triangleright\\) Obtain the sum across tiles for Softmax\n    Update \\(s \\leftarrow \\operatorname{sum}\\left(\\mathrm{w}_{\\text {reg }}\\right)\\) with the sum from the local data\n    \\(\\operatorname{sum}[\\) warpid \\(] \\leftarrow s\\) for all warps to access\n    Iterate over \\(\\mathrm{n}_{\\text {warps }}\\) entries in sum buffer to compute the global sum of \\(\\mathrm{w}_{\\text {reg }}\\)\n    Put global sum back into each warp's \\(s\\) float\n    \\(\\mathrm{w}_{\\text {reg }} \\leftarrow \\mathrm{w}_{\\text {reg }}-m \\quad \\triangleright\\) Start attention computation in register\n    \\(\\mathrm{w}_{\\text {reg }} \\leftarrow \\exp \\left(\\mathrm{w}_{\\text {reg }}\\right)\\)\n    \\(\\mathrm{W}_{\\text {reg }} \\leftarrow \\frac{\\mathrm{w}_{\\mathrm{reg}}}{s}\\)\n    \\(B_{w}[\\) warpid \\(] \\leftarrow \\mathrm{w}_{\\text {reg }} \\quad \\triangleright\\) Register to SRAM; storing for the slice of \\(k\\)\n    \\(\\mathrm{wv}_{\\mathrm{reg}} \\leftarrow B_{w} \\quad \\triangle\\) SRAM to Register. Warp loads entirety of \\(B_{w}\\); all slices\n    Initialize oreg to zero. \\(\\mathrm{O}_{\\mathrm{reg}} \\leftarrow \\mathrm{wv}_{\\text {reg }} \\mathrm{V}_{\\mathrm{reg}} \\quad \\triangleright\\) Matrix-vector (GEMV) multiplication\n    Write o oreg to global memory\n                            \\(\\triangleright\\) Register to SRAM, SRAM to HBM\n```\n\n\n## C Extended Architecture Details\n\nIn this section, we describe two additional architectural details that can enable small improvements in language model perplexity. We emphasize, however, that the combination of Taylor linear attention and TCWINDOW layers alone is sufficient to come within 0.1 perplexity points of our best models using these additional components (Table 4). Convolution. We find that replacing some of the linear attention and TCWINDOW layers with gated convolutions [1, 9, 72 enables small improvements in language modeling performance. We use BaseConv layers [1] with short convolutions and a SilU non-linearity [73. By keeping the convolutions short (e.g. width 3), we keep the recurrent state size low and improve throughput. The projections expand the dimensionality by a factor $c=4$. Note, this layer is nearly identical to a Mamba block without the SSM [5]. $$\n\\boldsymbol{y}:=(\\underbrace{\\left(\\boldsymbol{u} \\cdot \\boldsymbol{W}_{1}+\\boldsymbol{b}_{1}\\right)}_{\\text {Linear Projection }} \\odot \\sigma \\underbrace{\\left(\\boldsymbol{h} * \\boldsymbol{u} \\cdot \\boldsymbol{W}_{2}+\\boldsymbol{b}_{2}\\right)}_{\\text {Convolution }}) \\cdot \\boldsymbol{W}_{3}+\\boldsymbol{b}_{3}\n$$\n\nwhere $\\boldsymbol{u} \\in \\mathbb{R}^{N \\times d}$ is a projected input, $h \\in \\mathbb{R}^{N \\times c d}$ is a learned filter, $\\odot$ is the Hadamard product, and $\\boldsymbol{W}_{1}, \\boldsymbol{W}_{2} \\in \\mathbb{R}^{d \\times c d}, \\boldsymbol{W}_{3} \\in \\mathbb{R}^{c d \\times d}, \\boldsymbol{b}_{1}, \\boldsymbol{b}_{2} \\in \\mathbb{R}^{c d}$, and $\\boldsymbol{b}_{3}, \\in \\mathbb{R}^{d}$ define weights and biases of three linear projections. Decay. Recent recurrent architectures include the use of decay terms, implemented in a variety of ways [5, 6, 27, 28. As intuition, decay terms control how much a token should attend to \"recent\" tokens vs. \"early\" tokens in the sequence. Prior work falls in two categories: using input-independent [27, 28, inter alia.] or input-dependent [5, 6] decay rates. The latter offers improved quality, but requires the use of a parallel scan during sequence processing [5]. Instead, we introduce a coarser input-dependent decay technique for the linear attention layer, avoiding the parallel scan. We start with decay matrices that use a fixed decay rate as in prior work and let each of the $h$ heads in the linear attention apply a unique decay schedule to the causal dot product. We then introduce a learned projection $\\boldsymbol{W} \\in \\mathbb{R}^{d \\times h}$ that effectively allows each input to weigh the combination of heads (decay rates) when producing the output representation. | Architecture | Params | LAMBADA |  | HellaSwag <br> Acc. Norm. $\\uparrow$ | PIQA <br> Acc $\\uparrow$ | Arc-E <br> Acc $\\uparrow$ | Arc-C <br> Acc. Norm. $\\uparrow$ | WinoGrande <br> Acc. $\\uparrow$ | Average <br> Acc. $\\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | Ppl. $\\downarrow$ | Acc. $\\uparrow$ |  |  |  |  |  |  |\n| Transformer++ (LLaMa) | 360 m | 18.39 | 42.52 | 33.48 | 63.98 | 46.04 | 24.49 | 53.99 | 44.08 |\n| Transformer (Pythia) | 356 m | 25.17 | 37.16 | 31.32 | 63.76 | 44.82 | 23.8 | 51.54 | 42.08 |\n| Based | 363 m | 21.80 | 38.66 | 33.43 | 64.42 | 45.79 | 24.66 | 51.22 | 43.03 |\n| Mamba | 358 m | 20.23 | 39.65 | 33.63 | 65.02 | 47.01 | 25.00 | 50.75 | 43.51 |\n| H3 | 362 m | 57.59 | 23.58 | 30.62 | 63.11 | 45.20 | 23.29 | 50.28 | 39.35 |\n| Transformer++ (LLaMa) | 1.33 b | 11.12 | 49.10 | 39.29 | 66.16 | 51.68 | 26.19 | 53.43 | 47.64 |\n| Based | 1.35b | 12.35 | 46.96 | 39.11 | 66.32 | 50.72 | 26.54 | 50.43 | 46.68 |\n| Mamba | $1.32 b$ | 13.11 | 46.13 | 39.41 | 66.38 | 52.36 | 25.94 | 50.83 | 46.84 |\n\nTable 5: Downstream evaluation of pre-trained language models. The same set of models as in table 1. all were trained on the same 10 billion tokens drawn from the Pile [30, evaluated using the LM eval harness by EleutherAI 16\n\n| Dataset | Enhancer Cohn | Enhancer Ens | Human Reg. | Non-TATA Promoters | Human OCR Ens. |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| CNN | 69.5 | 68.9 | 93.3 | 84.6 | 68.0 |\n| DNABERT | 74.0 | 85.7 | 88.1 | 85.6 | 75.1 |\n| GPT | 70.5 | 83.5 | 91.5 | 96.6 | 73.0 |\n| HyenaDNA | 74.2 | 89.2 | $\\mathbf{9 3 . 8}$ | 94.4 | $\\mathbf{8 0 . 9}$ |\n| Transformer++ | 73.4 | $\\mathbf{8 9 . 5}$ | 89.9 | 96.6 | 79.5 |\n| Mamba | 73.0 | - | - | $\\mathbf{9 6 . 8}$ | 79.0 |\n| Based | $\\mathbf{7 4 . 6}$ | $\\mathbf{8 9 . 5}$ | 89.5 |  |  |\n\nTable 6: Downstream evaluation of pre-trained DNA models on GenomicsBenchmarks [74]. We report top-1 classification accuracy (\\%) with pretrained models (Transformer++, Mamba, BASED) along with prior reported results in [41. We find the similar quality-matching in pretraining transfers to downstream tasks. Modern architectures are also able to achieve state-of-the-art results on the classification tasks. ## D Extended Results\n\nDownstream Language Results To further evaluate BASED's performance in language modeling, we evaluate the PILE-pretrained models on several downstream tasks. We use the same protocol as 5, utilizing the LM evaluation harness by EleutherAI [16. In particular, we use the following set of metrics and tasks:\n\n- LAMBADA (perplexity and accuracy) 75\n- HellaSwag (normalized accuracy) 76\n- PIQA (accuracy) 77\n- ARC-challenge (normalized accuracy) and, separately, the easy subset ARC-easy (accuracy) 78]\n- WinoGrande (accuracy) 79\n\nNormalized accuracy refers to accuracy normalized by sequence length and is used to maintain the equivalent setting to [5. We report results in Table 5. For both 360 million and 1.3 billion parameter models, BASED performs competitively with recent and state-of-the art architectures, including Mamba and modern Transformers (Transformer++ (LLaMa)). Downstream DNA Classification We further evaluate how different architectures compare for DNA modeling. We use the pretrained models described in Section 6.1, and test how well they transfer to DNA sequence classification on a popular benchmark (GenomicBenchmarks) 74 used to evaluate recent DNA language models 41 . We compare our HG38-pretrained Transformer++ and BASEd models in Table 6. We find similar performance across tasks, indicating that prior matching in quality during pretraining transfers to downstream classification. For reference, we also include results from 41]. Although not directly comparable to due differences in tokenization, the evaluations suggest BASED can perform strongly on different modalities, and that recent sequence modeling architectures are also able to outpeform or compete with prior state-of-the-art on evaluated DNA tasks. ## E Experimental Details\n\n## E. 1 Language Model Pretraining\n\nWe use A100 80GB Nvidia GPUs to run all experiments. We use training infrastructure closely adapted from the FlashAttention code base: https://github.com/Dao-AILab/flash-attention/tree/main for all pretraining runs [11]. The Pile data is tokenized using the GPT2BPETokenizer and all models see the data in the same order. Here we provide details on the hyperaparamters and configurations used for training each architecture. We also provide details on the FLOPs computation. - Based We train using the specifications in Table 7. Our implementation is provided here: https: //github.com/HazyResearch/based. The initial models were trained and evaluated using the Fast Transformer CUDA kernels discussed in Appendix B 24, 34 . - Transformer++ [15] We refer to the modern Llama architecture with Rotary encodings, RMSNorm and SwiGLU as Transformer++, following prior work [5, 6]. We train using the the specifications in Table 8 using the Flash Attention training code provided here: https://github.com/Dao-AILab/ flash-attention/tree/main 11. - Mamba [5] We train using the specifications in Table 9 where the parameters are sourced from the Appendix of [5]. The implementation is sourced from the provided reference at https://github.com/ state-spaces $/ \\mathrm{mamba}$\n- Hyena [7] We train using the specifications in Table 10, where the parameters are sourced from the Appendix of [7]. The implementation is sourced from the provided reference at https://github.com/ HazyResearch/safari. - H3 [9] We train using the specifications in Table 11. The implementation is sourced from the provided reference at https://github.com/HazyResearch/safari. - RWKV [8] We train using the specifications in Table 12 and use the reference implementation at https://github.com/BlinkDL/RWKV-LM. We specifically evaluate RWKV-V5. - Gated Linear Attention (GLA) We train using the specifications in Table 13. We train following the reference implementation at https://github.com/berlino/gated_linear_attention. We give all models the improved Transformer++ recipe (e.g., SwiGLU) as relevant. ## E. 2 Computing Recurrent State Size\n\nIn this section, we provide details on how we compute the size of the recurrent hidden state for the results described in Section 3.1. We train and evaluate six sequence mixers on a synthetic associative recall task: attention [2, sliding window attention [19, Mamba [5, H3 [9], Hyena [7, and BASED.",
    "based-47": "For each, we vary hyperparameters that affect the memory consumption during inference. We compare how MQAR accuracy varies with the size of the recurrent hidden state. Based. The recurrent state size in BASED is determined by the model dimension $d$ and the size of the hidden dimension after applying the feature map $\\tilde{d}$. The +1 accounts for the K-state required for computing the denominator. For more details on the recurrent view of BASED, see 4. $$\n\\operatorname{sizeof}\\left(s_{i}\\right)=(d+1) \\times \\tilde{d}\n$$\n\nIn Based, we use the Taylor Exponential feature map after projecting $d$ down to a smaller dimension $d^{\\prime}$. With this approach, recurrent state size is given by:\n\n$$\n\\operatorname{sizeof}\\left(\\boldsymbol{s}_{i}\\right)=(d+1) \\times\\left(1+\\frac{3 d^{\\prime}}{2}+\\frac{d^{\\prime 2}}{2}\\right)\n$$\n\nIn our synthetic experiments, we run BASED with $d \\in\\{48,64,128\\}$ and $d^{\\prime} \\in\\{8,16,24\\}$. Attention. The recurrent state size (i.e. KV-cache size) in attention depends on two parameters: the model dimension $d$ and the sequence length $N$. The 2 in the expression below accounts for the separate storage for keys and values in the KV-cache. $$\n\\operatorname{sizeof}\\left(s_{i}\\right)=2 \\times d \\times N\n$$\n\nIn our synthetic experiments we run attention with $d \\in\\{64,128\\}$. The sequence length $N$ is determined by the task, not the model architecture. Sliding window attention. The recurrent state size in sliding window attention is given by the model dimension $d$ and the width of the sliding window $k_{\\text {sliding }}$. The 2 in the expression below accounts for the separate storage for keys and values in the KV-cache. $$\n\\operatorname{sizeof}\\left(s_{i}\\right)=2 \\times d \\times \\min \\left(N, k_{\\text {sliding }}\\right)\n$$\n\nIn our synthetic experiment we run sliding window attention with $d \\in\\{128\\}$ and $k_{\\text {sliding }} \\in\\{8,16,32,64,128,256,512,1024\\}$. Mamba. The recurrent state size in Mamba is determined by the model dimension $d$ and the number of heads $h$. The 2 in the expression below accounts for the expansion in the Mamba block. $$\n\\operatorname{sizeof}\\left(\\boldsymbol{s}_{i}\\right)=2 \\times d \\times d_{\\text {state }}\n$$\n\nIn our synthetic experiments, we run Mamba with $d \\in\\{64,128,256\\}$ and $d_{\\text {state }} \\in\\{8,16,24\\}$. H3. The recurrent state size in H3 is determined by the model dimension $d$ and the number of heads $d_{\\text {state }}$. $$\n\\operatorname{sizeof}\\left(\\boldsymbol{s}_{i}\\right)=d \\times d_{\\text {state }}\n$$\n\nIn our synthetic experiments, we run H 3 with $d \\in\\{64,128,256\\}$ and $d_{\\text {state }}=\\frac{d}{4}$. Hyena. The recurrent state size in Hyena is determined by the model dimension $d$ and the number of heads $h$. The 2 in the expression below accounts for the separate storage for keys and values in the KV-cache. $$\n\\operatorname{sizeof}\\left(\\boldsymbol{s}_{i}\\right)=d \\times N\n$$\n\nIn our synthetic experiments, we run Hyena with $d \\in\\{64,128,256\\}$. ## E. 3 Language Model Evaluation\n\nIn this section, we provide details on each of the evaluations (columns) reported in tables 1 and 4\nPile (Language Modeling). First, we report overall perplexity on the Pile test set 30. Then, to understand how much of the perplexity gap is due to recall capacity, we also evaluate perplexity on two slices (i.e. subsets) of the test set:\n\n1. Associative recall $(A R)$ tokens. Tokens in the final position of a bigram which previously occured in context, but $\\leq 1250$ times in the training data. 2. Other tokens. All other tokens. To construct these slices, we exactly follow the protocol in Arora et al. 1] and refer the reader to that work for more details. We compute these slices on the first 16 million tokens in the test set. SWDE (Information Extraction). The task in the SWDE benchmark is to extract semi-structured relations from raw HTML websites. For example, given an IMBD page for a movie (e.g. Harry Potter and the Sorcerer's Stone) and a relation key (e.g. release date), the model must extract the correct relation value (e.g. 2001). The SWDE benchmark was originally curated by Lockard et al. 80 for the task of open information extraction from the semi-structured web. Because we are evaluating the zero-shot capabilities of relatively small language models, we adapt the task to make it slightly easier. Our task setup is similar after to that used in Arora et al. 39 . FDA (Information Extraction). The task is to extract key-value pairs from a set of PDFs scraped from the FDA website.",
    "based-48": "We use the dataset and labels collected in 39. We break apart the documents into chunks of 1,920 tokens. For every key-value pair that appears in the chunk, we create a zero-shot prompt using the simple prompt template: \\{chunk\\} $\\backslash \\mathrm{n}\\{$ key $\\}:$\nWe allow the model to generate a fixed number of tokens after the prompt and check (with case insensitivity) if the value is contained within the generation. We report accuracy, the fraction of prompts for which the generation contains the value. Below we include one example of a zero-shot prompt for the key-value pair \"Type of Test: Quantitative, colorometric, pyranose oxidase ( $P R O D)$ \". The actual chunk is substantially longer in the dataset (note the ellipsis). 510(k) SUBSTANTIAL EQUIVALENCE DETERMINATION DECISION SUMMARY ASSAY ONLY\nTEMPLATE A. $510(\\mathrm{k})$ Number: k180209 B. Purpose for Submission: New Device C. Measurand:\n1,5-Anhydroglucitol (1,5-AG) D. Type of Test: Quantitative, colorometric, pyranose oxidase\n(PROD) E. Applicant: Diazyme Laboratories Inc. F. Proprietary and Established Names: Diazyme\n1,5-AG Assay G. Regulatory Information: 1. Regulation section: 21 CFR 864.7470 ; Glycosylated\nhemoglobin assay 2. Classification: Class II ... [1,920 tokens of context from the PDF] ... Diazyme's\n1,5-AG assay uses the enzyme pyranose oxidase (PROD) to oxidize the 2nd position hydroxyl group of\n1,5-AG and to detect the generated hydrogen peroxide by colorimetry using peroxidase (POD).",
    "based-49": "Type of\nTest:\n510(k) SUBSTANTIAL EQUIVALENCE DETERMINATION DECISION SUMMARY ASSAY ONLY TEMPLATE A. $510(\\mathrm{k})$ Number: k180209 B. Purpose for Submission: New Device C. Measurand: 1,5-Anhydroglucitol (1,5-AG) D. Type of Test: Quantitative, colorometric, pyranose oxidase (PROD) E. Applicant: Diazyme Laboratories Inc. F. Proprietary and Established Names: Diazyme 1,5-AG Assay G. Regulatory Information: 1. Regulation section: 21 CFR 864.7470; Glycosylated hemoglobin assay 2. Classification: Class II ... [1,920 tokens of context from the PDF] ... Diazyme's 1,5 -AG assay uses the enzyme pyranose oxidase (PROD) to oxidize the 2nd position hydroxyl group of $1,5-\\mathrm{AG}$ and to detect the generated hydrogen peroxide by colorimetry using peroxidase (POD). Type of Test:\n\nSQUAD (Question Answering). The Stanford Question Answering Dataset (SQUAD) can be used to evaluate the reading comprehension of language models. The model is given a passage of text and a question whose answer is contained in the passage. Because the models trained in this work are relatively small-scale (up to 1.3 billion parameters trained on 10 billion tokens) and not instruction fine-tuned, they struggle to answer questions when asked directly. To make the task more amenable to these raw language models, we first use GPT-4 to reformat the questions to more closely resemble the next-token-prediction task the models were trained on:\nCan you rewrite this question and answer as a statement. Ensure that the answer is the last part of the statement. $\\backslash \\mathrm{n} \\backslash \\mathrm{n}$ Question: \\{question\\} $\\backslash \\mathrm{n} \\backslash \\mathrm{n}$ Answer: \\{answer\\} $\\backslash \\mathrm{n} \\backslash \\mathrm{n}$ Rewrite:\n\nFor example, the question and answer \"Question: Which NFL team represented the AFC at Super Bowl 50? Answer: Denver Broncos\" was rewritten by GPT-4 as \"The NFL team that represented the AFC at Super Bowl 50 was the Denver Broncos.\" We verify that the rewritten sentence does indeed end with the answer, discarding any sentences where it does not ( $40 \\%$ of questions). We run the reformatting on 5,000 squad questions from the validation set, yielding a final dataset of $\\mathbf{2 , 9 8 4}$ questions formatted as next token predictions.",
    "based-50": "Below we include one example of a zero-shot prompt. The reformatted question is in bold. For the third straight season, the number one seeds from both conferences met in the Super Bowl. The Carolina Panthers became one of only ten teams to have completed a regular season with only one loss, and one of only six teams to have acquired a $15-1$ record, while the Denver Broncos became one of\nfour teams to have made eight appearances in the Super Bowl. The Broncos made their second Super Bowl appearance in three years, having reached Super Bowl XLVIII, while the Panthers made their second Super Bowl appearance in franchise history, their other appearance being Super Bowl XXXVIII. Coincidentally, both teams were coached by John Fox in their last Super Bowl appearance prior to Super Bowl 50. The team in Super Bowl 50 that had a $\\mathbf{1 5 - 1}$ record was the\n\n## F Theoretical Results\n\n## F. 1 Introduction\n\nOur focus in this section will be on the theoretical results of the paper. Specifically, we will show the equivalence of models Based and Mamba [5 with BaseConv, a minimal gated-convolution operator [1, Definition 4.1], and prove lower bounds for the MQAR problem [1, Section H.7.1] in various settings. We begin by setting notation and introducing the theoretical formulations of the models. Notation. We will be denoting the all 1 row vector of size $k$, given by $\\left[\\begin{array}{lllll}1 & 1 & \\ldots & 1 & 1\\end{array}\\right]$, and the all 0 row vector of size $k$, given by $\\left[\\begin{array}{lllll}0 & 0 & \\ldots & 0 & 0\\end{array}\\right]$, as $\\mathbf{1}^{k}$ and $\\mathbf{0}^{k}$, respectively. We will also construe the standard basis vector $\\mathbf{e}_{i}$ as a column vector in these notes, and adhere to the following matrix indexing convention: $\\mathbf{M}[i, j]$ is the entry in the $i$ th row and the $j$ th column, $\\mathbf{M}[i,:] \\in \\mathbb{F}^{1 \\times n}$ denotes the $i$ th row, and $\\mathbf{M}[:, j] \\in \\mathbb{F}^{m \\times 1}$ denotes the $j$ th column of $\\mathbf{M} \\in \\mathbb{F}^{m \\times n}$, where $\\mathbb{F}$ is a field and the reader can substitute $\\mathbb{F}$ for $\\mathbb{R}$ for convenience. For a matrix $\\mathbf{M} \\in \\mathbb{R}^{n \\times m}$, we define the pair-wise Hadamard product of columns of $M$ as $M \\circ M \\in \\mathbb{R}^{n \\times m^{2}}$, where\n\n$$\n\\begin{array}{r}\n(\\boldsymbol{M} \\circ \\boldsymbol{M})[:, i]:=\\mathbf{M}[:, j] \\odot \\mathbf{M}[:, k] \\quad \\text { for } \\quad i \\in\\left[m^{2}\\right] \\\\\nj=\\left\\lfloor\\frac{i-1}{m}\\right\\rfloor+1, \\quad k=(i-1) \\quad \\bmod m+1 . \\end{array}\n$$\n\nMoreover, we define the element-wise exponentiation of a matrix $\\mathbf{M}$ as $\\exp [\\mathbf{M}]$ where $\\exp [\\mathbf{M}]_{i j}=\\exp \\left(\\mathbf{M}_{i j}\\right)$. Next, we denote the Hadamard product of vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{F}^{n}$ as $\\mathbf{u} \\odot \\mathbf{v}$; the operation can be extended to matrices accordingly, and for vectors $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{F}^{n}$, we denote their linear (or acyclic) convolution as $\\mathbf{u} * \\mathbf{v}$\n\nArithmetic Circuit Notation. We briefly introduce the notation of arithmetic circuits 81]. An arithmetic circuit $\\mathcal{C}$ with variables $X \\triangleq\\left\\{x_{1}, x_{2}, \\ldots, x_{n}\\right\\}$ over a field $\\mathbb{F}$ is interpreted as a directed acyclic graph, where the input nodes are labelled by either the variables from $X$ or constants from $\\mathbb{F}$ and the internal nodes are labelled by + or $\\times$ with the output being the polynomial computed at the output node. We shall also refer to the size of the circuit as the number of nodes, the depth of the circuit as the length of the longest path between an input node and the output node, and the width of the circuit as the number of parallel operations in the circuit, or 'wires' which will be intersected by a horizontal 'cut' through the circuit. Moreover, the degree of a circuit is defined as the degree of the polynomial computed by the circuit. We summarize this with the following definition:\n\nDefinition F.1. An arithmetic circuit $\\mathcal{C}$ is an $(n, s, \\Delta, w)$-circuit if $\\mathcal{C}$ is an $n$-variate arithmetic circuit of size $s$ and of depth at most $\\Delta$, and width $w$.",
    "based-51": "## F. 2 The Models\n\nWe now introduce the definitions of the models Based and Mamba for the reader's convenience. Note that we have redefined these models to ensure consistency with the notation presented above. ## F.2.1 Based\n\nThe Based model combines two layer types: BaseConv and LinearAttention defined below. Definition F. 2 (BaseConv (1). Given an input sequence $\\boldsymbol{u} \\in \\mathbb{R}^{N \\times d}$, where $N$ is the sequence length and $d$ is the model dimension, a learned weight matrix $\\boldsymbol{W}^{B} \\in \\mathbb{R}^{d \\times d}$ and biases $\\boldsymbol{B}^{B}, \\boldsymbol{B}^{K} \\in \\mathbb{R}^{N \\times d}$ and a matrix of convolution filters $\\boldsymbol{K} \\in \\mathbb{R}^{N \\times d}$, a BaseConvlayer computes the following:\n\n$$\n\\boldsymbol{z}^{\\text {Baseconv }}:=\\left(\\boldsymbol{u} \\boldsymbol{W}^{B}+\\boldsymbol{B}^{B}\\right) \\odot\\left(\\boldsymbol{K} * \\boldsymbol{u}+\\boldsymbol{B}^{K}\\right) \\in \\mathbb{R}^{N \\times d}\n$$\n\nwhere the convolutions are applied across the input length $N$.",
    "based-52": "Definition F. 3 (LinearAttention [24]). Given an input sequence $\\boldsymbol{u} \\in \\mathbb{R}^{N \\times d}$, where $N$ is the sequence length and $d$ is the model dimension, a set of linear projections ${ }^{6}$ Projection $_{q}$, Projection $_{k} \\in \\mathbb{R}^{d \\times d^{\\prime}}$, Projection $_{v} \\in$ $\\mathbb{R}^{d \\times d}$, where $d^{\\prime}$ is the feature dimension, the LinearAttention layer computes the following:\n\n$$\n\\boldsymbol{z}^{\\text {LinearAttention }}:=\\left(\\overline{\\boldsymbol{Q}} \\overline{\\boldsymbol{K}}^{\\top}\\right) \\boldsymbol{V} \\in \\mathbb{R}^{N \\times d}\n$$\n\nwhere $\\boldsymbol{Q}:=\\operatorname{Projection}_{q}(\\boldsymbol{u}), \\boldsymbol{K}:=\\operatorname{Projection}_{k}(\\boldsymbol{u}), \\boldsymbol{V}:=\\operatorname{Projection}_{v}(\\boldsymbol{u})$, and we have\n\n$$\n\\begin{aligned}\n\\overline{\\boldsymbol{Q}} & =[\\mathbf{1}, \\boldsymbol{Q}, \\boldsymbol{Q} \\circ \\boldsymbol{Q}] \\in \\mathbb{R}^{N \\times\\left(1+d^{\\prime}+d^{\\prime 2}\\right)} \\\\\n\\overline{\\boldsymbol{K}} & =[\\mathbf{1}, \\boldsymbol{Q}, \\boldsymbol{K} \\circ \\boldsymbol{K}] \\in \\mathbb{R}^{N \\times\\left(1+d^{\\prime}+d^{\\prime 2}\\right)}\n\\end{aligned}\n$$\n\n## F. 2.2 Mamba\n\nWe now introduce the Mamba model from [5].",
    "based-53": "Definition F. 4 (Mamba [5]). Given an input sequence $\\boldsymbol{u} \\in \\mathbb{R}^{N \\times d}$, where $N$ is the sequence length and $d$ is the model dimension, the Mamba layer computes the following:\n\n$$\nz^{\\text {Mamba }}:=\\operatorname{SSM}(\\overline{\\boldsymbol{A}}, \\overline{\\boldsymbol{B}}, \\boldsymbol{C})(\\boldsymbol{u}) \\in \\mathbb{R}^{N \\times d}\n$$\n\nwith the parameters, $\\overline{\\boldsymbol{A}} \\in \\mathbb{R}^{\\bar{d} \\times \\bar{d}}, \\overline{\\boldsymbol{B}} \\in \\mathbb{R}^{\\bar{d}}$, defined as\n\n$$\n\\begin{aligned}\n\\overline{\\boldsymbol{A}} & :=\\exp (\\Delta \\boldsymbol{A}) \\\\\n\\overline{\\boldsymbol{B}} & :=(\\Delta \\boldsymbol{A})^{-1}(\\exp (\\Delta \\boldsymbol{A})-\\boldsymbol{I}) \\cdot \\Delta \\boldsymbol{B} \\\\\n& =\\boldsymbol{A}^{-1}(\\exp (\\Delta \\boldsymbol{A})-\\boldsymbol{I}) \\cdot \\boldsymbol{B}\n\\end{aligned}\n$$\n\nwhere $\\bar{d}$, the state dimension, and $\\boldsymbol{A} \\in \\mathbb{R}^{\\bar{d} \\times \\bar{d}}$ are parameters of the model and do not depend on the input $\\boldsymbol{u}$, along with the following input-dependent parameters $\\boldsymbol{B}, \\boldsymbol{C} \\in \\mathbb{R}^{N \\times \\bar{d}}, \\Delta \\in \\mathbb{R}^{N \\times d}$ defined as\n\n$$\n\\begin{aligned}\n& \\boldsymbol{B}:=\\text { Linear }_{N \\times \\bar{d}}(\\boldsymbol{u}) \\in \\mathbb{R}^{\\bar{d}}, \\\\\n& \\boldsymbol{C}:=\\text { Linear }_{N \\times \\bar{d}}(\\boldsymbol{u}) \\in \\mathbb{R}^{\\bar{d}}, \\\\\n& \\Delta:=\\text { Linear }_{N \\times d}(\\boldsymbol{u}) \\in \\mathbb{R}\n\\end{aligned}\n$$\n\nfor $i \\in[N]$.",
    "based-54": "It is important to note here that the parameters $\\overline{\\boldsymbol{B}}, \\boldsymbol{C}, \\Delta$ are causa ${ }^{7}$ and we denote the dependence on upto the $i$ th row of the input $\\boldsymbol{u}$ for $i \\in[N]$ by adding a subscript $i$ where the dependence for $\\overline{\\boldsymbol{A}}_{i} \\in \\mathbb{R}^{\\overline{\\boldsymbol{d}} \\times \\bar{d}}$ is inherited from $\\Delta_{i}$ in equation 19 and we denote $\\overline{\\boldsymbol{B}}[i,:]=: \\boldsymbol{B}_{i}, \\overline{\\boldsymbol{C}}[i,:]=: \\boldsymbol{C}_{i}$. Finally, the SSM in equation 18 is realized as a linear recurrence. That is, for every $(i, j) \\in[N] \\times[d]$, we have\n\n$$\n\\begin{aligned}\n\\boldsymbol{h}[i, j] & =\\overline{\\boldsymbol{A}}_{i} \\boldsymbol{h}[i-1, j]+\\overline{\\boldsymbol{B}}_{i} \\boldsymbol{u}[i, j] \\\\\n\\boldsymbol{z}[i, j] & =\\boldsymbol{C}_{i}^{\\top} \\boldsymbol{h}[i, j]\n\\end{aligned}\n$$\n\nwhere $\\boldsymbol{h}[i, j] \\in \\mathbb{R}^{\\bar{d}}, \\boldsymbol{z}[i, j] \\in \\mathbb{R}$ denote the latent state and the output of the SSM in eq.",
    "based-55": "18, respectively. ## F. 3 Equivalency to BaseConv\n\nFor a polynomial with variables $X$ over a field $\\mathbb{F}$, there exists a corresponding arithmetic circuit $\\mathcal{C}$ over $X$ that computes the output of the polynomial at its terminating node when interpreted as a directed acyclic graph. For any such arithmetic circuit $\\mathcal{C}$ of size $s$ and depth $\\Delta$, [1, Theorem 4.2] showed the existence of an equivalent BaseConv operator that uses $\\tilde{\\mathcal{O}}(s \\Delta)$ parameters and $\\tilde{\\mathcal{O}}(\\Delta)$ layers. In the sequel, we use this result by expressing the model outputs computed in equation 17 and equation 18 as polynomials in $\\boldsymbol{u}$ and $\\exp (\\boldsymbol{u})$ to show the equivalency between these disparate models.",
    "based-56": "We would now like to recall [1, Theorem 4.2]. Before doing so, we first establish the following definitions from [1]. [^3]Definition F.5. An $(N, L, d, \\tilde{N}, \\tilde{d})$ - Gated Convolution Model is a stacked sequence to sequence model with $L$ layers such that:\n\n1. input and output are $N \\times d$ matrices,\n2. each layer's operations consist of element-wise gating, convolution, linear projection, and\n3. all the individual gated convolution layers take in $\\tilde{N} \\times \\tilde{d}$ matrices and output $\\tilde{N} \\times \\tilde{d}$ matrices. We refer to the tuple $(\\tilde{N}, \\tilde{d})$ as the inner dimension of the model. We also assume that the input $\\boldsymbol{u} \\in \\mathbb{R}^{N \\times d}$ is embedded into $\\boldsymbol{u}^{\\prime} \\in \\mathbb{R}^{\\tilde{N} \\times d}$ such that\n\n$$\n\\boldsymbol{u}^{\\prime}[n, t]=\\left\\{\\begin{array}{l}\n\\boldsymbol{u}[n, t] \\quad \\text { if } n<N, t<d \\\\\n0 \\quad \\text { otherwise }\n\\end{array}\\right. $$\n\nThe output from the last layer $\\boldsymbol{z} \\in \\mathbb{R}^{\\tilde{N} \\times \\tilde{d}}$ is transformed into output $\\boldsymbol{y} \\in R^{N \\times d}$ by extracting the top left $N \\times d$ entries in $\\boldsymbol{z}$.",
    "based-57": "Theorem F. 1 ([1], Theorem 4.2). For any $(n d, s, \\Delta, w)$-arithmetic circuit $\\mathcal{C}$, there exists an equivalent $\\left(N, \\Delta^{\\prime}, d, \\tilde{N}, \\tilde{d}\\right)-$ BaseConv with $N=n, \\Delta^{\\prime}=\\mathcal{O}(\\Delta \\log w), \\tilde{N}=\\mathcal{O}(w), \\tilde{d}=d$ that simulates $\\mathcal{C}$. Remark F.1. For notational simplicity, we will use $\\boldsymbol{u}_{i, j}$ as the symbol for the variable in the polynomial in $\\boldsymbol{u}$ representing the entry $\\boldsymbol{u}[i, j]$\n\nWe now present the results showing equivalency between the models in appendix F. 2 and the BaseConv layer in equation 16 using theorem F.1. Proposition F.1. Given an input $\\boldsymbol{u} \\in \\mathbb{R}^{N \\times d}$, there exists an equivalent $\\left(N, O\\left(\\log ^{2}(N d)\\right), d, O\\left(N\\left(d+d^{\\prime 2}\\right), O\\left(\\max \\left(d, d^{\\prime 2}\\right)\\right)\\right)-\\right.$ BaseConv that computes the output of the LinearAttention layer with feature dimension d', cf.",
    "based-58": "eq.",
    "based-59": "17). Proof. For the matrices $\\boldsymbol{Q}, \\boldsymbol{K} \\in \\mathbb{R}^{N \\times d^{\\prime}}, \\boldsymbol{V} \\in \\mathbb{R}^{N \\times d}$ with the corresponding projection matrices $\\boldsymbol{W}^{Q}, \\boldsymbol{W}^{k} \\in$ $\\mathbb{R}^{d \\times d^{\\prime}}, \\boldsymbol{W}^{V} \\in \\mathbb{R}^{d \\times d}$, a single BaseConv layer that computes each of these matrices by simply taking identical projection and $\\boldsymbol{h}^{s}, \\boldsymbol{h}^{l}, \\boldsymbol{B}^{s} \\equiv 0$ and $\\boldsymbol{B}^{\\ell} \\equiv \\mathbb{1}^{N \\times d}$, the all 1 matrix. Using the remembering primitive 1, Proposition H.10], we can compute each of these in turn while remembering others using $O(1)$ layers and $N d$ parameters. Next, we derive an expression for each entry $(i, j) \\in[N] \\times\\left[d^{\\prime 2}\\right]$ of $\\boldsymbol{Q} \\circ \\boldsymbol{Q}, \\boldsymbol{K} \\circ \\boldsymbol{K} \\in \\mathbb{R}^{N \\times d^{\\prime 2}}$. From equation 15 . observe that each entry of $\\boldsymbol{M} \\circ \\boldsymbol{M}$ can be written as the product of entries from $\\boldsymbol{M}$. Hence we have\n\n$$\n\\begin{aligned}\n(\\boldsymbol{Q} \\circ \\boldsymbol{Q})[i, j] & \\equiv \\boldsymbol{Q}[i, k] \\cdot \\boldsymbol{Q}[i, \\ell] \\\\\n(\\boldsymbol{K} \\circ \\boldsymbol{K})[i, j] & \\equiv \\boldsymbol{K}[i, k] \\cdot \\boldsymbol{K}[i, \\ell]\n\\end{aligned}\n$$\n\nfor $k=\\left\\lfloor\\frac{j-1}{d^{\\prime}}\\right\\rfloor+1, \\quad \\ell=(j-1) \\bmod d^{\\prime}+1$. Note, however, that we can simulate the above by first increasing the inner dimension and copying over columns of $\\boldsymbol{Q}$ to get $\\boldsymbol{Q}_{1}, \\boldsymbol{Q}_{2} \\in \\mathbb{R}^{N \\times d}$ defined as $\\boldsymbol{Q}_{1}[i, j]:=$ $\\boldsymbol{Q}[i, k]$ and $\\boldsymbol{Q}_{2}[i, j]:=\\boldsymbol{Q}[i, \\ell]$ for $k=\\left\\lfloor\\frac{j-1}{d^{\\prime}}\\right\\rfloor+1, \\quad \\ell=(j-1) \\bmod d^{\\prime}+1$ so that $(\\boldsymbol{Q} \\circ \\boldsymbol{Q})=\\boldsymbol{Q}_{1} \\odot \\boldsymbol{Q}_{2}$, which, mutatis mutandis, also applies to $(\\boldsymbol{K} \\circ \\boldsymbol{K})$ We can achieve the copying of the columns by simply using the projection matrix $\\boldsymbol{W}^{B}$ and another permutation matrix $\\boldsymbol{P}$. Apart from the multiplication by $\\boldsymbol{P}$, we only need to use $O(1)$ layers, and moreover, since the circuit that computes $\\boldsymbol{P} \\boldsymbol{u}$ simply rearranges the input, there exists a single BaseConv layer that computes $\\boldsymbol{P u}$ [1, Corollary H.20]. By the stacking lemma [1, Lemma H.11], we can stack these layers to get a composition of the outputs so far to get a $\\left(N, O(1), d, O\\left(N\\left(d+d^{\\prime 2}\\right), O\\left(\\max \\left(d, d^{2}\\right)\\right)\\right)\\right.$ - BaseConv model. Moreover, the concatenated matrices $\\overline{\\boldsymbol{Q}}, \\overline{\\boldsymbol{K}}$ $\\in \\mathbb{R}^{N \\times\\left(1+d^{\\prime}+d^{\\prime 2}\\right)}$ then take the addition of the computed components so far which again takes $O(1)$ layers of BaseConv. Finally, we can express each entry $(i, j) \\in[N] \\times[d]$ of the output of LinearAttention as a polynomial as follows:\n\n$$\n\\boldsymbol{z}_{i, j}(\\boldsymbol{u}) \\equiv \\sum_{m \\in\\left[1+d^{\\prime}+d^{\\prime 2}\\right], n \\in[N]} \\overline{\\boldsymbol{Q}}[i, m] \\cdot \\overline{\\boldsymbol{K}}[n, m] \\cdot \\boldsymbol{V}[n, j]\n$$\n\nThus, we can derive the arithmetic circuit that computes $\\boldsymbol{z}_{i, j}(\\boldsymbol{u})$ by taking in the outputs of the BaseConv layers so far as input and compute each of the terms inside the sum by multiplying the outputs from all three and compute the sum using additional $\\log \\lceil N d\\rceil$ depth.",
    "based-60": "Each term inside the sum requires two multiplication gates with depth 2 , each of which serve as inputs to the circuit with size $N d$ computing the sum. Moreover, there are $N \\cdot d$ such output gates each of which is computed in parallel resulting in a circuit of size $O(N \\cdot d)$, depth $O(\\log (N d))$ and width $O(N d)$. O Overall, applying theorem F. 1 then results in an equivalent $\\left(N, O\\left(\\log ^{2}(N d)\\right), d, O\\left(N\\left(d+d^{\\prime 2}\\right), O\\left(\\max \\left(d, d^{2}\\right)\\right)\\right)\\right.$ - BaseConv model that computes $\\boldsymbol{z}$. ## F. 4 The Lower Bounds\n\nIn the sequel, we consider the multiple-query associative recall problem (MQAR) as defined in [1, Section H.7.1]. We briefly recall the definition here. Suppose we are given an input sequence $\\boldsymbol{u}[0 \\cdots 3 N-1] \\triangleq\\left\\{\\left(\\boldsymbol{k}_{0}, \\boldsymbol{v}_{0}, \\boldsymbol{q}_{0}\\right), \\ldots,\\left(\\boldsymbol{k}_{N-1}, \\boldsymbol{v}_{N-1}, \\boldsymbol{q}_{N-1}\\right)\\right\\}$ with each $\\boldsymbol{k}_{i}, \\boldsymbol{v}_{i}, \\boldsymbol{q}_{i} \\in C$ is a token drawn from a vocabulary of size $c=|C|$. Our goal is then to check, for each $1 \\leq i \\leq N-1$, whether there exists $0 \\leq j<i$ such that $\\boldsymbol{q}_{i} \\equiv \\boldsymbol{k}_{j}$, and if so, output $\\boldsymbol{v}_{j}$. ## F.4.1 The Space Complexity of AR\n\nWe will start by providing a lower bound on the space complexity of solving the standard associative recall (AR) problem. As AR is a subclass of MQAR, this naturally provides a lower bound on the space complexity of MQAR as well. Here, we formally recall the associative recall problem. The AR problem takes key-value pairs $\\left\\{\\boldsymbol{k}_{i}, \\boldsymbol{v}_{i}\\right\\}_{i=0}^{n-1}$ along with a query $\\boldsymbol{q}$ appended at the end as input and the goal is to output $\\boldsymbol{v}_{i}$ if $\\boldsymbol{q}=\\boldsymbol{k}_{i}$ for some $i \\in[0, N-1]$. We now require a randomized communication complexity lower bound result for the index problem:\nThe index problem has two agents, Alice and Bob, where Alice has a string $\\boldsymbol{x} \\in\\{0,1\\}^{n}$ and Bob has an index $i \\in[n]$, and the goal for the players is to output the $i$-th entry $\\boldsymbol{x}_{i}$. Moreover, we also require the communication to be one-way: only Alice is allowed to send a single message to Bob and Bob needs to output the answer. We will make use of the following lower-bound result. Theorem F. 2 ([82]). The one-way randomized communication complexity of the index problem for sending an $n$-length bit string is $\\Omega(n)$. ## F.4.2 Lower Bound for Recurrent Models\n\nWe now use theorem F. 2 to first provide a lower bound on the number of bits required by the following class of models to solve AR. Definition F. 6 (Recurrent Models). A model $\\mathcal{M}$ taking an input $\\boldsymbol{u} \\in \\mathbb{R}^{N \\times d}$, where $N$ is the input length and $d$ is the model dimension, is termed a recurrent model if its $i$-th state, representing the output at location $i, \\boldsymbol{Z}_{\\mathcal{M}}^{i} \\in \\mathbb{R}^{\\tilde{d}}$, with $\\tilde{d}$ denoting the state size, is determined exclusively by the preceding elements of the input $\\boldsymbol{u}[0 \\ldots i-1]$. The state $\\boldsymbol{Z}_{\\mathcal{M}}^{i}$ represents the accumulated information of the model depending on the inputs up to the $i$-th element, and is distinct from learned parameters that are static with respect to the input sequence. Specifically, $\\boldsymbol{Z}_{\\mathcal{M}}^{i}(\\boldsymbol{u})=\\phi(\\boldsymbol{u}[0 \\ldots i-1])$, indicating that the state is a function of the input history but not of the entire input sequence simultaneously. Moreover, we can express this as:\n\n$$\n\\boldsymbol{Z}_{\\mathcal{M}}^{i}(\\boldsymbol{u})=f_{\\mathcal{M}}^{i}\\left(\\boldsymbol{Z}_{\\mathcal{M}}^{i-1}, \\boldsymbol{u}[i]\\right)\n$$\n\nfor a sequence of functions $\\left\\{f_{\\mathcal{M}}^{i}\\right\\}_{i \\in[N]}$, where each function is tailored to evolve the state based on the immediate past state and the current input. [^4]Remark F.2. Note that definition F.6 excludes models that inherently require the entire input sequence for computation at any state, such as those based on non-causal convolutional operations over the full input. Theorem F.3. Any recurrent model $\\mathcal{M}$ (definition F.6) that solves $A R$ requires $\\max _{i}\\left|\\boldsymbol{Z}_{\\mathcal{M}}^{i}\\right|$ to be at least $\\Omega(N)$-bits. Proof. Consider an instance $(\\boldsymbol{x}, i)$ of the index problem with $\\boldsymbol{x} \\in\\{0,1\\}^{N}$. We now describe the corresponding instance of the AR problem:\n\n$$\n\\left\\{j, \\boldsymbol{x}_{j}\\right\\}_{j=0}^{N-1}, i\n$$\n\nNext, consider the following one-way protocol for solving the index problem using the regressive model $\\mathcal{M}$. Alice with their access of $\\boldsymbol{x} \\in\\{0,1\\}^{N}$ generate an input for AR (without the query) as in equation 22. Alice then runs the model $\\mathcal{M}$ on $\\left\\{i, \\boldsymbol{x}_{j}\\right\\}_{j=0}^{N-1}$ and sends the memory content of running the model $\\mathcal{M}$ to Bob. This should include the state $\\boldsymbol{Z}_{\\mathcal{M}}^{N-1}$ of size $\\tilde{d}$ as we can reasonably assume that both have access to the set of functions $\\left\\{f_{\\mathcal{M}}^{j}\\right\\}_{j \\in[N]}$. Since we assume that this model solves AR , the output Out $[N,:]=\\boldsymbol{x}_{i}$ should contain the associated value of $i$. Here, Bob can compute Out $[N,:]$ by using the memory content sent by Alice and applying the function $f^{N}$ as follows. $$\n\\boldsymbol{x}_{i}=\\operatorname{Out}[N,:]=f^{N}\\left(\\boldsymbol{Z}^{N-1}, \\boldsymbol{u}[N]\\right)\n$$\n\nThat is, the total number of bits that are communicated in this protocol is $\\left|\\boldsymbol{Z}_{\\mathcal{M}}^{N-1}\\right|$. Now, if $\\max _{j}\\left|\\boldsymbol{Z}_{\\mathcal{M}}^{j}\\right|$ is $o(N)$ bits, we have shown that a one-way communication protocol exists for solving the index problem exists that uses $o(N)$ communication complexity. This contradicts theorem F. 2 and hence, we conclude that the model $\\mathcal{M}$ solving AR also needs $\\Omega(N)$ bits. Corollary F.1. Given an input $\\boldsymbol{u} \\in \\mathbb{R}^{N \\times d}$ to the AR problem, a causal Mamba model with all entries in its computation taking $O(1)$ bits needs $d+\\bar{d} \\geq \\Omega(N)$ to solve $A R$. Proof. We will first show that causal Mamba is a recurrent model. To see this, first observe equation 21 and note the fact that the input-dependent parameters $\\overline{\\boldsymbol{A}}, \\overline{\\boldsymbol{B}}, \\boldsymbol{C}, \\Delta$ are causal as mentioned in definition F. 4 . Next, due to equation 21 , in order to compute $\\boldsymbol{z}_{N,:} \\in \\mathbb{R}^{d}$, we need $\\boldsymbol{C}_{N} \\in \\mathbb{R}^{\\bar{d}}, \\overline{\\boldsymbol{B}}_{N} \\in \\mathbb{R}^{\\bar{d}}$ and $\\Delta_{N} \\in \\mathbb{R}^{d}$ along with $\\boldsymbol{h}[N-1,:] \\in \\mathbb{R}^{\\bar{d}}$. Here, we have the $(N-1)$-st state $\\boldsymbol{Z}_{\\text {Mamba }}^{N-1} \\in \\mathbb{R}^{3 \\bar{d}+d}$ given by\n\n$$\n\\boldsymbol{Z}_{\\text {Mamba }}^{N-1}:=\\left\\{\\boldsymbol{h}[i-1,:], \\Delta_{N}^{1}, \\overline{\\boldsymbol{B}}_{N}^{1}, \\boldsymbol{C}_{N}^{1}\\right\\}\n$$\n\nwhere $\\Delta_{N}^{1}, \\overline{\\boldsymbol{B}}_{N}^{1}, \\boldsymbol{C}_{N}^{1}$ are all linear functions of $\\boldsymbol{u}[0 \\cdots N-1]$ that we receive from the $(N-1)$-st state and we compute $\\Delta_{N}^{2}, \\overline{\\boldsymbol{B}}_{N}^{2}, \\boldsymbol{C}_{N}^{2}$ as linear functions of $\\boldsymbol{u}[N]$ so that we have $\\Delta_{N}=\\Delta_{N}^{1}+\\Delta_{N}^{1}, \\overline{\\boldsymbol{B}}_{N}=\\overline{\\boldsymbol{B}}_{N}^{1}+\\overline{\\boldsymbol{B}}_{N}^{2}, \\boldsymbol{C}_{N}=$ $\\boldsymbol{C}_{N}^{1}+\\boldsymbol{C}_{N}^{2}$. We can then define the function $f^{N}$ as follows:\n\n$$\n\\begin{aligned}\n\\boldsymbol{Z}_{\\text {Mamba }}^{N}[j] & =\\exp \\left(\\Delta_{N}[j] \\boldsymbol{A}\\right) \\boldsymbol{h}[N-1, j]+\\overline{\\boldsymbol{B}}_{N} \\boldsymbol{u}[N, j] \\\\\n& =\\overline{\\boldsymbol{A}}_{N} \\boldsymbol{h}[N-1, j]+\\overline{\\boldsymbol{B}}_{N} \\boldsymbol{u}[N, j] \\\\\n\\text { Out }[N, j] & =f^{N}\\left(\\boldsymbol{Z}_{\\text {Mamba }}^{N-1}\\right)[j]=\\boldsymbol{C}_{N}^{\\top} \\boldsymbol{Z}_{\\text {Mamba }}^{N}[j]\n\\end{aligned}\n$$\n\nThus, due to theorem F.3 we can conclude that $\\left|\\boldsymbol{Z}_{\\text {Mamba }}^{N-1}\\right|$ does require $\\Omega(N)$-bits to solve AR. Finally, assuming each entry of $\\boldsymbol{Z}_{\\text {Mamba }}^{N-1}$ needs $O(1)$ bits to represent, the overall state $\\boldsymbol{Z}_{\\text {Mamba }}^{N-1}$ needs $O(d+\\bar{d})$ to represent, which completes the proof of the claim. ## F.4.3 Lower Bound on the Number of Layers for AR\n\nNext, we will again use theorem F. 2 to provide a better bound on the number of layers required to solve AR. Theorem F.4. Given an input $\\boldsymbol{u} \\in\\{0,1\\}^{N \\times d}$ to the $A R$ problem with any encoding such that $\\log c \\leq d \\leq$ $2^{(\\log N)^{1-\\epsilon}}$ for $\\epsilon>0$, and c possible tokens from the vocabulary with $c \\leq N$, a data-independent BaseConv model with model parameters taking $O(\\log N)$ bits needs $\\Omega(\\epsilon \\log \\log N)$ layers to solve $A R$. Proof. For a BaseConv model that solves AR using $L$ layers, by definition, there exists a polynomial $P(\\boldsymbol{u})$ of degree at most $2^{L}$ that solves AR for any $\\boldsymbol{u} \\in\\{0,1\\}^{N \\times Q 9}$. This is because for the output of the $i$ th layer of BaseConv, given by $\\boldsymbol{Z}_{\\text {BaseConv }}^{i}$, we have\n\n$$\n\\boldsymbol{Z}_{\\text {BaseConv }}^{i}\\left(\\boldsymbol{Y}_{\\mathcal{M}}^{i-1}\\right) \\equiv P^{i}\\left(\\boldsymbol{Z}_{\\text {BaseConv }}^{i-1}\\right), \\quad \\operatorname{deg}\\left(P^{i}\\right) \\leq 2\n$$\n\nfor some polynomial $P^{i}$ of degree 2 which simply takes the inner products allowing the model to solve AR, where $\\boldsymbol{Z}_{\\mathrm{BaseConv}}^{0}:=\\boldsymbol{u}$. Further, for such a model with $L$ layers, by composition, the output of the $i$-th layer for $i \\in[L]$ is also a polynomial over the input $\\boldsymbol{u}$ and has degree at most $2^{i}$.",
    "based-61": "At the end, we have a polynomial $P(\\boldsymbol{u})$ of degree $\\leq 2^{L}$ for $\\boldsymbol{u} \\in\\{0,1\\}^{N \\times d}$. As in the proof of theorem F.3. again take the instance instance ( $\\left.\\boldsymbol{x}, i\\right)$ of the index problem with $\\boldsymbol{x} \\in\\{0,1\\}^{N}$ and the corresponding instance of the AR problem as before\n\n$$\n\\boldsymbol{u}:=\\left\\{j, \\boldsymbol{x}_{j}\\right\\}_{j=0}^{N-1}, i\n$$\n\nNext, we build the following one-way protocol for solving the index problem using the BaseConv model from the hypothesis that it solves AR . Alice with their access of $\\boldsymbol{x} \\in\\{0,1\\}^{N}$ will again generate an input $\\boldsymbol{u}$ for AR (without the query) as in equation 26\n\nAlice first takes the values $\\boldsymbol{a}:=\\boldsymbol{u}[0: N-2,:] \\in\\{0,1\\}^{(N-1) \\times d}$ and substitutes these known $(N-1) d$ values to define the following polynomial:\n\n$$\nQ\\left(\\boldsymbol{u}_{N-1,0}, \\ldots, \\boldsymbol{u}_{N-1, d-1}\\right)=P\\left(\\boldsymbol{a}, \\boldsymbol{u}_{N-1,0}, \\ldots, \\boldsymbol{u}_{N-1, d-1}\\right)\n$$\n\nHere, note that $Q$ is a polynomial in $d$ variables that correspond to the values $\\boldsymbol{u}[N-1,:]$ that Bob has and trivially has degree $D \\leq 2^{L}$. Now, Alice can run the model $\\mathcal{M}$, retrieve the coefficients of $Q$, and send it to Bob. Since we assume that $P$ solves AR, Bob can take the coefficients of $Q$ and substitute $\\boldsymbol{u}[N-1,:]$ to $Q$ to compute $P(\\boldsymbol{u})$ which is the associated value of $i$. Here, the polynomial $Q$ that Alice sends has at most $d^{2^{L}}$ coefficients as each term in $Q$ can have degree at most $2^{L}$. If each such coefficient has $B$ bits, then using theorem F.2, the total number of bits being communicated must satisfy $B \\cdot d^{2^{L}} \\geq \\Omega(N)$. This follows from the fact that if $B \\cdot d^{2^{L}} \\leq o(N)$, then since the associated value of $i$ in equation 26 is the answer to the indexing problem, we have shown that a one-way communication protocol for solving the index problem uses $o(N)$ communication complexity, which then contradicts theorem F.2. Thus, we must have\n\n$$\nB \\cdot d^{2^{L}} \\geq \\Omega(N) \\Longrightarrow 2^{L} \\log (d) \\geq \\log \\left(\\frac{N}{B}\\right)-O(1)\n$$\n\nTaking logarithm of both sides then yields\n\n$$\n\\begin{aligned}\nL & \\geq \\log \\left(\\frac{\\log \\left(\\frac{N}{B}\\right)}{\\log (d)}\\right)-O(1) \\geq \\log \\left(\\frac{\\log N-\\log B}{\\log (d)}\\right)-O(1) \\\\\n& \\geq \\log \\left(\\frac{\\log N-\\log B}{(\\log N)^{1-\\epsilon}}\\right)\n\\end{aligned}\n$$\n\nwhere we use the fact that $d \\leq 2^{(\\log N)^{1-\\epsilon}}$ for any $\\epsilon>0$ in equation 28 . Moreover, as the model parameters are assumed to be $O(\\log N)$ bits, any coefficient in $Q$ should have absolute value at most $\\left(2^{O(\\log N)} \\cdot N d\\right)^{2^{L}}$ as each coefficient can be a product of at most $N d$ variables. That is, for some $\\alpha>0$, we have the following bound on each coefficient:\n\n$$\n2^{B} \\leq\\left(N^{\\alpha+1} d\\right)^{2^{L}} \\leq\\left(N^{(\\alpha+2) 2^{L}}\\right)\n$$\n\nwhere the last equality uses the fact that $d \\leq N$. We thus have\n\n$$\n\\log (B) \\leq \\log (\\alpha+2)+L+\\log \\log N\n$$\n\n[^5]Substituting equation 29 to equation 28 , we get\n\n$$\nL \\geq \\log \\left(\\frac{\\log N-\\log (\\alpha+2)-L-\\log \\log N}{(\\log N)^{1-\\epsilon}}\\right)\n$$\n\nNow, if $L>\\log \\log N$, we are done. Otherwise, if $L \\leq \\log \\log N$, then we can substitute this to equation 30 to get\n\n$$\n\\begin{aligned}\nL & \\geq \\log \\left(\\frac{\\log N-\\log (\\alpha+2)-2 \\log \\log N}{(\\log N)^{1-\\epsilon}}\\right) \\\\\n& =\\log (\\log N-\\log (\\alpha+2)-2 \\log \\log N)-(1-\\epsilon) \\log \\log N\n\\end{aligned}\n$$\n\nWe now claim that first term in equation 31 satisfies the following:\n\n$$\n\\log (\\log N-\\log (\\alpha+2)-2 \\log \\log N) \\geq\\left(1-\\frac{\\epsilon}{2}\\right) \\log \\log N\n$$\n\nTo see this, note that, for sufficiently large enough $N$, the following holds:\n\n$$\n\\frac{\\log N}{2} \\geq \\log (\\alpha+2)+2 \\log \\log N\n$$\n\nhence, we get\n\n$$\n\\log (\\log N-\\log (\\alpha+2)-2 \\log \\log N) \\geq \\log \\left(\\frac{\\log N}{2}\\right) \\geq \\log \\log N-1 \\geq\\left(1-\\frac{\\epsilon}{2}\\right) \\log \\log N\n$$\n\nThis proves the claim in equation 32 . Finally, using equation 32 equation 31 leads to the following:\n\n$$\nL \\geq\\left(1-\\frac{\\epsilon}{2}\\right) \\log \\log N-(1-\\epsilon) \\log \\log N=\\frac{\\epsilon}{2} \\log \\log N\n$$\n\nwhich still provides the lower bound $L=\\Omega(\\epsilon \\log \\log N)$, as desired. Remark F.3. We remark that it is possible to extend theorem F.4 to any model whose output from each layer is a polynomial of some degree $\\Delta \\geq 2$ ot get a lower bound of $\\Omega(\\epsilon \\log \\log N / \\log \\Delta)$. ## F.4.4 Lower Bound on the Number of Layers for MQAR with $d=\\log _{2} c$\n\nSetup. We take $d=\\log _{2} c$ to encode all $c$ possible tokens from $C$. That is, all the $2^{d}$ possible $d$-bit vectors can appear as a token in the input for MQAR. We will show that data-independent BaseConv needs $\\Omega(\\log d)$ $=\\Omega(\\log \\log c)$-layers to solve this setting of MQAR, while Attention $(+\\operatorname{ReLU})$ can solve this in $O(1)$ layers. We first provide the trivial solution using Attention ( + ReLU). Proposition F.2. Attention (with linear biases and ReLU) followed by two layers of MLPs can solve MQAR for an input sequence $\\boldsymbol{u} \\in\\{0,1\\}^{3 N \\times d}$ such that $d=\\log _{2}(c)$ in $O(1)$ layers. Proof. Given a row $\\boldsymbol{u}[i,:] \\in\\{0,1\\}^{d}$, we express each row as $\\boldsymbol{w}[i,:] \\in\\{-1,1\\}^{d}$ by applying the projection $\\boldsymbol{u} \\boldsymbol{W}+\\boldsymbol{B}$, where $\\boldsymbol{W}:=\\operatorname{diag}(2, \\ldots, 2) \\in \\mathbb{R}^{d \\times d}$ and the bias matrix $\\boldsymbol{B}$ is the matrix of all -1 's so that $\\boldsymbol{w}[i, j]=2 \\boldsymbol{u}[i, j]-1$. Then, we can specify the query and key projection matrices $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{3 N \\times d}$ as follows:\n\n$$\n\\begin{aligned}\n& \\mathbf{K}[i,:] \\equiv \\begin{cases}\\boldsymbol{w}[i,:]=\\boldsymbol{k}_{\\lfloor i / 3\\rfloor} & \\text { if } i \\equiv 0 \\quad \\bmod 3 \\\\\n\\mathbf{0} & \\text { otherwise }\\end{cases} \\\\\n& \\mathbf{Q}[i,:] \\equiv \\begin{cases}\\boldsymbol{w}[i,:]=\\boldsymbol{q}_{\\lfloor i / 3\\rfloor} & \\text { if } i \\equiv 2 \\bmod 3 \\\\\n\\mathbf{0} & \\text { otherwise }\\end{cases} \\\\\n& \\mathbf{V}[i,:] \\equiv \\begin{cases}\\boldsymbol{w}[i+1,:]=\\boldsymbol{v}_{\\lfloor i / 3\\rfloor} & \\text { if } i \\equiv 0 \\bmod 3 \\\\\n\\mathbf{0} & \\text { otherwise }\\end{cases}\n\\end{aligned}\n$$\n\nwhere the values are shifted to the corresponding key index. Computing the pair-wise inner products then yields\n\n$$\n\\mathbf{Q K}^{\\top}[i, j] \\equiv \\begin{cases}\\left\\langle\\boldsymbol{q}_{\\lfloor i / 3\\rfloor}, \\boldsymbol{k}_{\\lfloor j / 3\\rfloor}\\right\\rangle & \\text { if } i \\equiv 2 \\bmod 3 \\text { and } j \\equiv 0 \\bmod 3 \\\\ \\mathbf{0} & \\text { otherwise }\\end{cases}\n$$\n\nHowever, since both $\\boldsymbol{q}_{\\lfloor i / 3\\rfloor}, \\boldsymbol{k}_{\\lfloor j / 3\\rfloor} \\in\\{-1,1\\}^{d}$, we have $\\left\\langle\\boldsymbol{q}_{\\lfloor i / 3\\rfloor}, \\boldsymbol{k}_{\\lfloor j / 3\\rfloor}\\right\\rangle \\leq d$ with equality iff $\\boldsymbol{q}_{\\lfloor i / 3\\rfloor} \\equiv \\boldsymbol{k}_{\\lfloor j / 3\\rfloor}$. We then subtract off $d-1$ from each of the $3 N \\times 3 N$ entries by taking the bias $\\mathbf{B} \\in \\mathbb{R}^{3 N \\times 3 N}$ as the matrix with each entry $-d+1$. Let $\\mathbf{Z}:=\\operatorname{RELU}\\left(\\mathbf{Q K}^{\\top}+\\mathbf{B}\\right)$ so that we have\n\n$$\n\\mathbf{Z}[i, j]=\\mathbb{1}\\left\\{\\boldsymbol{q}_{\\lfloor i / 3\\rfloor} \\equiv \\boldsymbol{k}_{\\lfloor j / 3\\rfloor}\\right\\}\n$$\n\nNext, as we may have multiple matches and we only need to return 1, we modify $\\boldsymbol{Z}$ by multiplying with the matrices $\\boldsymbol{W}_{1}, \\boldsymbol{W}_{2} \\in \\mathbb{R}^{d \\times d}$ and adding the bias $\\boldsymbol{B} \\in \\mathbb{R}^{d \\times d}$ defined as follows:\n\n$$\n\\boldsymbol{W}_{1}[k, j]:=\\left\\{\\begin{array}{ll}\n1 & \\text { if } k \\geq j \\\\\n0 & \\text { otherwise }\n\\end{array}, \\quad \\boldsymbol{W}_{2}[\\ell, k]:= \\begin{cases}-1 & \\text { if } k=0 \\\\\n1 & \\text { if } k=\\ell, \\ell \\neq 0, \\quad \\boldsymbol{B}[i, j]=1 \\\\\n0 & \\text { otherwise }\\end{cases}\\right. $$\n\nFor $\\boldsymbol{Z}_{1}:=\\boldsymbol{Z} \\boldsymbol{W}_{1}$ and $\\boldsymbol{Z}_{2}:=\\boldsymbol{Z} \\boldsymbol{W}_{1} \\boldsymbol{W}_{2}$, we have:\n\n$$\n\\begin{aligned}\n& \\boldsymbol{Z}_{1}[i, j]=\\sum_{k} \\boldsymbol{Z}[i, k] \\boldsymbol{W}_{1}[k, j]=\\sum_{k \\geq j} \\boldsymbol{Z}[i, k] \\\\\n& \\boldsymbol{Z}_{2}[i, j]=\\sum_{k} \\boldsymbol{Z}_{1}[i, k] \\boldsymbol{W}_{2}[k, j]=\\boldsymbol{Z}_{1}[i, j]-\\boldsymbol{Z}_{1}[i, 0]\n\\end{aligned}\n$$\n\nThat is, each entry in $\\boldsymbol{Z}_{1}$ sums the entries in the row that are at the same or higher column index while each column in $\\boldsymbol{Z}_{2}$ subtracts the first entry-the sum of all entries in the row-from each entry in the row. Semantically, for each row in $\\boldsymbol{Z}_{1}$, the entries from 0 to the index of the first match must have the same value, and thus, are the only non-negative entries in $\\boldsymbol{Z}_{2}$. Next, we add the bias and activate under RELU to get $\\boldsymbol{Z}^{\\prime} \\in \\mathbb{R}^{3 N \\times d}$ :\n\n$$\n\\boldsymbol{Z}^{\\prime}[i, k]:=\\operatorname{RELU}\\left(\\boldsymbol{Z}_{2}+\\boldsymbol{B}\\right)[i, k]= \\begin{cases}1 & \\text { if } k \\leq \\min \\left\\{j \\mid \\boldsymbol{q}_{\\lfloor i / 3\\rfloor} \\equiv \\boldsymbol{k}_{\\lfloor j / 3\\rfloor}\\right\\} \\\\ 0 & \\text { otherwise. }\\end{cases}\n$$\n\nNow, we multiply by the weight matrix $\\boldsymbol{W}_{3} \\in \\mathbb{R}^{3 N \\times d}$ defined as\n\n$$\n\\boldsymbol{W}_{3}[k, j]:= \\begin{cases}-1 & \\text { if } k=j+1 \\\\ 1 & \\text { if } k=j \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nThis yields the retriever $\\overline{\\boldsymbol{Z}}=\\boldsymbol{Z}^{\\prime} \\boldsymbol{W}_{3} \\in \\mathbb{R}^{3 N \\times d}$ given by\n\n$$\n\\overline{\\boldsymbol{Z}}[i, k]:=\\sum_{\\ell} \\boldsymbol{Z}^{\\prime}[i, \\ell] \\boldsymbol{W}_{3}[\\ell, k]=\\boldsymbol{Z}^{\\prime}[i, k]-\\boldsymbol{Z}^{\\prime}[i, k+1]=\\mathbb{1}\\left\\{k=\\min \\left\\{j \\mid \\boldsymbol{q}_{\\lfloor i / 3\\rfloor} \\equiv \\boldsymbol{k}_{\\lfloor j / 3\\rfloor}\\right\\}\\right\\}\n$$\n\nFinally, we multiply with the values $\\mathbf{V}$ to get\n\n$$\n(\\overline{\\boldsymbol{Z}} \\mathbf{V})[i,:] \\equiv \\overline{\\boldsymbol{Z}}[i,:] \\mathbf{V} \\equiv \\overline{\\boldsymbol{Z}}\\left[i, j^{*}\\right] \\cdot \\mathbf{V}\\left[j^{*},:\\right] \\equiv \\begin{cases}\\boldsymbol{v}_{j^{*}} & \\text { if } \\boldsymbol{q}_{\\lfloor i / 3\\rfloor} \\equiv \\boldsymbol{k}_{\\left\\lfloor j^{*} / 3\\right\\rfloor}, j^{*}=\\min \\left\\{j \\mid \\boldsymbol{q}_{\\lfloor i / 3\\rfloor} \\equiv \\boldsymbol{k}_{\\lfloor j / 3\\rfloor}\\right\\} \\\\ \\mathbf{0} & \\text { if no such } j^{*} \\text { exists. }\\end{cases}\n$$\n\nThat is, the row corresponding to the query returns the value associated to the first matching key. Thus, the model with Attention (computing $\\boldsymbol{Z}$ ) followed by two MLPs computing $\\boldsymbol{Z}^{\\prime}$ and $\\overline{\\boldsymbol{Z}}$, respectively, solves the MQAR problem. Next, we relate the output of $L$ layers of BaseConv to the degree of the polynomial that it computes. Lemma F.1. For any input sequence $\\boldsymbol{u}$, there exists a multilinear polynomial equivalent (over Boolean inputs) to the polynomial computed by L layers of BaseConv with degree at most $2^{L}$. Proof. Let $P(\\boldsymbol{u})$ be the polynomial computed by $L$ layers of BaseConv. Since the output of a single layer of BaseConv is equivalent to a polynomial over the input variables with degree at most 2, composing $L$ such layers yields a polynomial of degree at most $2^{L}$. However, $P(\\boldsymbol{u})$ need not be multi linear, but the polynomial defined as\n\n$$\nQ(\\boldsymbol{u}):=\\left(\\cdots\\left(\\left(P(\\boldsymbol{u}) \\bmod \\left(u_{1}^{2}-u_{1}\\right)\\right) \\quad \\bmod \\left(u_{2}^{2}-u_{2}\\right)\\right) \\cdots\\right) \\quad \\bmod \\left(u_{3 N d}^{2}-u_{3 N d}\\right)\n$$\n\nis equivalent to $P(\\boldsymbol{u})$ as $\\left(u_{i}^{2}-u_{i}\\right)$ evaluates to 0 for each input var $u_{i} \\in\\{0,1\\}$. However, $\\operatorname{deg}(Q(\\boldsymbol{u})) \\leq$ $\\operatorname{deg}(P(\\boldsymbol{u}))$, and thus, the claim holds. We now relate the MQAR (in the above setting) to the degree of the polynomial that it computes. Lemma F.2. The $M Q A R$ problem with $d=\\log _{2}(c)$ is represented by a multi-linear polynomial of degree $2 d+1$\n\nProof. We will start by specifying the obvious Boolean circuit that solves MQAR. First, we take the XNOR of keys and queries bitwise as follows. $$\n\\boldsymbol{x}^{i j}=\\boldsymbol{q}_{i} \\text { xnor } \\boldsymbol{k}_{j}:=\\left(\\boldsymbol{q}_{i} \\wedge \\boldsymbol{k}_{j}\\right) \\vee\\left(\\neg \\boldsymbol{q}_{i} \\wedge \\neg \\boldsymbol{k}_{j}\\right) \\text { for } i>j\n$$\n\nwhere, for $\\boldsymbol{x}, \\boldsymbol{y} \\in\\{0,1\\}^{d}$, we have\n\n$$\n[\\boldsymbol{x} \\text { xnor } \\boldsymbol{y}][k]:= \\begin{cases}1 & \\text { if } \\boldsymbol{x}[k]=\\boldsymbol{y}[k] \\\\ 0 & \\text { othwerise }\\end{cases}\n$$\n\nThat is, each bit from $\\boldsymbol{x}^{i j}$ is set to 1 iff the corresponding bits from $\\boldsymbol{q}_{i}$ and $\\boldsymbol{k}_{j}$ match. Next, we take the AND of the $d$-bits to get\n\n$$\n\\boldsymbol{y}^{i j}:=\\bigwedge_{k \\in[d]} \\boldsymbol{x}_{k}^{i j}, i>j\n$$\n\nThus, $\\boldsymbol{y}^{i j}$ is set to 1 iff the query $\\boldsymbol{q}_{i}$ matches with the key $\\boldsymbol{k}_{j}$. Finally, we AND with each bit of the values to get the output $\\boldsymbol{z}^{i j}$ with the $k$ th bit for $k \\in[d]$ given by\n\n$$\n\\boldsymbol{z}_{k}^{i j}:=\\boldsymbol{y}_{i j} \\wedge\\left[\\boldsymbol{v}_{j}\\right]_{k}\n$$\n\nThus, the output of the circuit can be represented as\n\n$$\n\\boldsymbol{z}^{i j}= \\begin{cases}\\boldsymbol{v}_{i} & \\text { if } \\boldsymbol{q}_{i} \\equiv \\boldsymbol{k}_{j}, i>j \\\\ \\mathbf{0} & \\text { otherwise }\\end{cases}\n$$\n\nWe can now directly translate the above circuit into a multi-linear polynomial. With slight abuse of notation, we have the following correspondence for equation 34 , where $\\boldsymbol{u}_{i} \\equiv \\boldsymbol{q}_{i}, \\boldsymbol{u}_{j} \\equiv \\boldsymbol{k}_{j}, i>j$ and we use $\\boldsymbol{u}_{i j}$ to represent the variable corresponding to the entry $\\boldsymbol{u}[i, j]$. $$\n\\boldsymbol{x}_{k}^{i j}(\\boldsymbol{u}):=\\boldsymbol{u}_{i k} \\boldsymbol{u}_{j k}+\\left(1-\\boldsymbol{u}_{i k}\\right)\\left(1-\\boldsymbol{u}_{j k}\\right) \\quad \\text { for each } k \\in[d], i>j\n$$\n\nNext, we translate equation 34 as follows. $$\n\\boldsymbol{y}^{i j}(\\boldsymbol{u}):=\\prod_{k \\in[d]}\\left(\\boldsymbol{u}_{i k} \\boldsymbol{u}_{j k}+\\left(1-\\boldsymbol{u}_{i k}\\right)\\left(1-\\boldsymbol{u}_{j k}\\right)\\right)\n$$\n\nFinally, we can write the polynomial that computes MQAR as follows. $$\n\\boldsymbol{z}^{i j}(\\boldsymbol{u}):=\\left(\\prod_{k \\in[d]} \\boldsymbol{u}_{i k} \\boldsymbol{u}_{j k}+\\left(1-\\boldsymbol{u}_{i k}\\right)\\left(1-\\boldsymbol{u}_{j k}\\right)\\right) \\boldsymbol{u}_{(i+1) k} \\quad \\text { for each } k \\in[d], i>j\n$$\n\nwhere $\\boldsymbol{u}[i+1,:] \\equiv \\boldsymbol{v}_{j}$. It is then easy to observe that equation 36 is multi-linear and has degree $2 d+1$. We are now ready to provide the lower bound. Theorem F.5. A data-independent BaseConv model needs $\\log (2 d)$-layers to solve MQAR for an input sequence $\\boldsymbol{u} \\in\\{0,1\\}^{3 N \\times d}$ with $d=\\log _{2}(c)$. Proof. Due to Lemma F.2, we know there exists a multi-linear polynomial that solves MQAR, and due to [83, Lecture 3, Proposition 4], it is unique. Specifically we cannot solve MQAR with a multi-linear polynomial of degree $\\leq 2 d$. Now, assume that there is a BaseConv model with $L$ layers that exactly solves MQAR. Then, due to Lemma F.1, this yields a multilinear polynomial $P(\\boldsymbol{u})$ of degree at most $2^{L}$. Here, if $L \\leq \\log (2 d)$, then the resulting BaseConv with $L$ layers results in a multilinear polynomial of degree $\\leq 2 d$. This contradicts the above claim that we cannot have a multi linear polynomial of degree $<2 d+1$ that exactly represents MQAR. Consequently, a data-independent BaseConv model needs $\\geq \\log (2 d)$-layers to solve MQAR. ## F. 5 Lower Bound on the Number of Layers for $d \\geq \\log _{2} c$ with Specific Encodings\n\n## F.5.1 The Equality Problem\n\nFor an input pair $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2}$ where each $\\boldsymbol{u}_{i}$ is a token drawn from a vocabulary of size $c=|C|$ and embedded in $\\{0,1\\}^{d}$, we define the equality problem (EQ) as checking whether the two encodings are equal: $\\boldsymbol{u}_{1} \\equiv \\boldsymbol{u}_{2}$. We first note that any model that solves MQAR also solves EQ via the following proposition. Proposition F.3. Any model $M_{\\mathrm{MQAR}}$ that solves $M Q A R$ also solves $E Q$ using the same number of layers. Proof. If there exists a model $\\mathrm{M}_{\\mathrm{MQAR}}$ that solves MQAR using $L$ layers, then for an arbitrary input instance for EQ given by $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\in \\mathbb{R}^{2 \\times d}$, we can produce the following input instance for MQAR: $\\boldsymbol{u}:=$ $\\left\\{\\left(\\boldsymbol{u}_{1}, \\mathbb{1}, \\boldsymbol{u}_{1}\\right),\\left(\\boldsymbol{u}_{2}, \\mathbb{1}, \\boldsymbol{u}_{2}\\right)\\right\\}$ and solve EQ using $L$ layers with $\\mathrm{M}_{\\text {MQAR }}$ returning $\\mathbb{1}$ iff there is a match.",
    "based-62": "Due to Proposition F.3.",
    "based-63": "we obtain the following corollary. Corollary F.2. Any lower bound $\\bar{L}$ on the number of layers $L$ of BaseConv to solving $E Q$ is also a lower bound on the number of layers required for solving MQAR. We now try to prove a lower bound for the case of $d \\geq \\log _{2} c$. First, note that there are embeddings here where the lower bound from F. 5 holds: consider the embedding where the first $\\log _{2} c$ has the compact binary embedding as before but the last $d-\\log _{2} c$ bits are the same for all the tokens. We will instead prove a lower bound for a more interesting set of embeddings. ## F.5.2 The $p$-Hot Encoding for $p \\geq 1$\n\nDefinition F. 7 ((Almost) $p$-Hot Encoding). We define the $p$-hot encoding to be the collection of embeddings for a token $\\boldsymbol{x}_{t}$ with $0 \\leq t<c$ such that we express $t$ in base $\\sqrt[p]{c}:\\left(t_{0}, . ., t_{p-1}\\right) \\in[0, \\sqrt[p]{c})^{p}$ and represent each $t_{i}$ as one hot encoding in $\\{0,1\\} \\sqrt[p]{c}$. That is, we take $d=p \\cdot \\sqrt[p]{c}$. Moreover, we define the almost p-hot encoding to be the collection of embeddings where each $t_{i}$ is mapped in $\\{0,1\\} \\sum^{\\sqrt[p]{c}-1}$ obtained by dropping the last bit of its one-hot encoding in $\\{0,1\\} \\sqrt[p]{C}$. Note that both of the encodings have $p$-many blocks derived from each of the one-hot encodings. Definition F. 8 (Block-Exclusive). We say that a polynomial $P$ with variables in $\\boldsymbol{u}:=\\left(\\boldsymbol{u}_{0}, \\ldots, \\boldsymbol{u}_{p-1}\\right)$ is block-exclusive if each non-zero monomial in $P$ given by the product\n\n$$\n\\prod_{i \\in[p], j \\in[\\sqrt[p]{c}]} u_{i, j}\n$$\n\ndoes not contain any product of the form $\\boldsymbol{u}_{i, j} \\boldsymbol{u}_{i, j^{\\prime}}$ for $i \\in[p], j, j^{\\prime} \\in[\\sqrt[p]{c}]$. Remark F.4. The condition specified in Definition F.8 ensures that a block-exclusive polynomial is necessarily multilinear, as it disallows the term $\\boldsymbol{u}_{i, j} \\boldsymbol{u}_{i, j^{\\prime}}$ for $j=j^{\\prime}$ in any non-zero monomial. Lemma F.3. For any Boolean function $f:\\{0,1\\} \\rightarrow\\{0,1\\}$ with inputs from the almost p-hot encoding or the $p$-hot encoding setting, there exists a block-exclusive polynomial equivalent to $f$. Proof. Given an input $\\boldsymbol{u}$ to $f$ from the almost $p$-hot encoding or the $p$-hot encoding such that $\\boldsymbol{u}:=$ $\\left(\\boldsymbol{u}_{0}, \\ldots, \\boldsymbol{u}_{p-1}\\right)$, we first observe that the polynomial $P(\\boldsymbol{u})$ representing $f(\\boldsymbol{u})$ cannot have a non-zero monomial with variables from the same block. Specifically, for $0 \\leq j<p$, any non-zero monomial in $P$ cannot have a product of the form $\\boldsymbol{u}_{j, k} \\boldsymbol{u}_{j, k^{\\prime}}$ for $k \\neq k^{\\prime}$. To see this, assume that there exists a non-zero monomial in $P$ with at least two terms $\\boldsymbol{u}_{j, k} \\boldsymbol{u}_{j, k^{\\prime}}$ from the same $j$ th block in $\\boldsymbol{u}$, then monomial always evaluates to 0 as the $j$ th block is derived from the one-hot encoding in $\\{0,1\\} \\sqrt[p]{c}$ or the almost one-hot encoding in $\\{0,1\\} \\sqrt[p]{c}-1$, and hence, cannot have more than one bit set to 1 . Next, if a non-zero monomial in $P$ does contain a product of the form $\\boldsymbol{u}_{j, k} \\boldsymbol{u}_{j, k^{\\prime}}$ for $k, k^{\\prime} \\in[\\sqrt[p]{c}]$, we can define the polynomial\n\n$$\nQ(\\boldsymbol{u}):=\\left(\\cdots\\left(\\left(P(\\boldsymbol{u}) \\bmod \\left(u_{0,0}^{2}-u_{0,0}\\right)\\right) \\quad \\bmod \\left(u_{0,1}^{2}-u_{0,1}\\right)\\right) \\cdots\\right) \\bmod \\left(u_{p-1, \\sqrt[p]{c}-1}^{2}-u_{p-1, \\sqrt[p]{c}-1}\\right)\n$$\n\nSince each entry is Boolean, $Q$ is equivalent to $P$ over Boolean inputs, and thus, $Q$ is the block-exclusive polynomial equivalent to $f$. Proposition F.4. Any Boolean function $f:\\{0,1\\} \\rightarrow\\{0,1\\}$ with inputs from the almost $p$-hot encoding setting has a unique representation as a block-exclusive polynomial. Proof. Due to [83, Proposition 4], we know that every Boolean function $f$ is represented by a multilinear polynomial. Moreover, from Lemma F.3, we know that the polynomial $P(\\boldsymbol{u})$ representing $f(\\boldsymbol{u})$ is blockexclusive for $\\boldsymbol{u}$ with the almost $p$-hot encoding. To show uniqueness, we replicate the argument from [83, Lecture 3, Proposition 4]: Given two blockexclusive polynomials $P$ and $P^{\\prime}$ equivalent to $f$ with inputs from the almost $p$-hot encoding, we have $\\left(P-P^{\\prime}\\right)(\\boldsymbol{u}) \\equiv 0$. Now, assume, for the sake of contradiction, that $P-P^{\\prime} \\not \\equiv 0$. Here, note that as $P-P^{\\prime}$ is not identically zero and we have a non-zero monomial, and since the inputs are from the almost $p$-hot encoding, we know that this monomial cannot contain any product of the form $\\boldsymbol{u}_{j, k} \\boldsymbol{u}_{j, k^{\\prime}}$. Let $S \\subseteq[p] \\times[\\sqrt[p]{c}-1]$ be a minimal set of indices such that the monomial $\\prod_{(j, k) \\in S} \\boldsymbol{u}_{j, k}$ appears in $P-P^{\\prime}$ with non-zero coefficient. Note that $\\chi_{S}$ forms a valid input to $f$ as each block in $S$ can be assigned at most one non-zero entry. Then, since $\\left(P-P^{\\prime}\\right)\\left(\\chi_{S}\\right) \\neq 0$ as every other monomial will get at least one variable that is assigned to 0 for $\\chi_{S}$ , we achieve a contradiction, and thus, $P-P^{\\prime}$ must be identically zero on inputs from the almost $p$-hot encoding. Lemma F.4. The EQ problem in the almost p-hot encoding setting is represented by a block-exclusive polynomial of degree $2 p$. Proof. Each input pair $\\boldsymbol{u}^{1}, \\boldsymbol{u}^{2}$ to the EQ problem can be represented as $\\boldsymbol{u}^{i}:=\\left(\\boldsymbol{u}_{0}^{i}, \\ldots, \\boldsymbol{u}_{p-1}^{i}\\right)$ for $i \\in\\{1,2\\}$, where for each $0<j<p$ such that we have\n\n$$\n\\boldsymbol{u}_{j}^{i}:=\\left(\\boldsymbol{u}_{j, 0}^{i}, \\ldots, \\boldsymbol{u}_{j, \\sqrt[p]{c}-2}^{i}\\right) \\in\\{0,1\\} \\sqrt[p]{c}-1\n$$\n\nThe following polynomial takes the inner product of each of these one-hot encodings:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-48.jpg?height=133&width=889&top_left_y=1885&top_left_x=615)\nfor $0<j<p$. Here, note that there can be only be at most 1 in both $\\boldsymbol{u}_{j}^{1}$ and $\\boldsymbol{u}_{j}^{2}$, and thus, $P^{j}(\\boldsymbol{u})=1$ iff the $j$ th block agree. Next, the following polynomial is equivalent to the Boolean function that solves the EQ problem:\n\n$$\nP(\\boldsymbol{u}):=\\prod_{j=0}^{p-1} P^{j}(\\boldsymbol{u})\n$$\n\nand we have $P(\\boldsymbol{u})=\\mathbb{1}\\left\\{\\boldsymbol{u}^{1} \\equiv \\boldsymbol{u}^{2}\\right\\}$. Here, note that $P$ is multi-linear and has degree $2 p$ as each $P^{j}$ is a degree- 2 polynomial. Moreover, $P$ is block-exclusive as each $P^{j}$ is block-exclusive and we only multiply monomials from different blocks in $P$. Proposition F.5. Let $P$ be the block-exclusive polynomial that solves the EQ problem in the $p$-hot encoding. Then, $\\operatorname{deg}(P) \\geq 2 p$. Proof. For the sake of contradiction, assume that there exists a block-exclusive polynomial $P$ that solves EQ in the $p$-hot encoding setting with degree $\\leq 2 p-1$. Then, given an input $\\boldsymbol{u}:=\\left(\\boldsymbol{u}_{0}, \\ldots, \\boldsymbol{u}_{p-1}\\right)$ from the almost $p$-hot encoding, where each block $\\boldsymbol{u}_{i}$ corresponds to the truncated bit string from the one-hot encoding in $\\{0,1\\} \\sqrt[p]{c}-1$, we can convert this input to the $p$-hot encoding $\\boldsymbol{v}:=\\left(\\boldsymbol{v}_{0}, \\ldots, \\boldsymbol{v}_{p-1}\\right)$ as follows:\n\n$$\n\\boldsymbol{v}_{i}:=\\left(\\boldsymbol{u}_{i, 0}, \\ldots, \\boldsymbol{u}_{i, \\sqrt[p]{c}-2}, 1-\\sum_{j=0}^{\\sqrt[p]{c}-2} \\boldsymbol{u}_{i, j}\\right)\n$$\n\nThen, the block-wise multilinear polynomial $Q(\\boldsymbol{u})=P(\\boldsymbol{v})$ solves the EQ problem in the almost one-hot encoding setting and has $\\operatorname{deg}(Q) \\leq \\operatorname{deg}(P) \\leq 2 p-1$ which contradicts the combination of Proposition F.",
    "based-64": "4 and Lemma F.",
    "based-65": "4 . Theorem F.6. A data-independent BaseConv model needs at least $\\lfloor\\log (2 p)\\rfloor$-layers to solve MQAR for an input sequence $\\boldsymbol{u} \\in\\{0,1\\}^{3 N \\times d}$ in the $p$-hot encoding setting, where $d=p \\cdot \\sqrt[p]{c}$.",
    "based-66": "Proof. We know from Corollary F. 2 that it suffices to show a lower bound for the EQ problem. Moreover, we know from Proposition F. 5 that we cannot solve the EQ problem in the $p$-hot encoding setting with a block-exclusive polynomial of degree $\\leq 2 p-1$. Now, assume that there is a BaseConv model with $L$ layers that exactly solves EQ in the $p$-hot encoding setting. Then, due to Lemma F.1 and Proposition F.4, this yields a block-exclusive polynomial $P(\\boldsymbol{u})$ of degree at most $2^{L}$. Here, if $L<\\lfloor\\log (2 p)\\rfloor$ which, then the resulting BaseConv with $L$ layers results in a block-exclusive polynomial of degree $\\leq 2 p-1$. This contradicts the above claim that we cannot have a block-exclusive polynomial of degree $<2 p$ that exactly represents EQ. Consequently, a data-independent BaseConv model needs $\\geq\\lfloor\\log (2 p)\\rfloor$-layers to solve EQ. Table 7: BASED Training Settings\n\n|  | 355 M | 1.4 B |\n| ---: | :---: | :---: |\n| Optimizer | Adam |  |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.95$ |  |\n| Optimizer eps | $1 e-8$ |  |\n| Precision | BFloat16 |  |\n| Warmup | $1 \\%$ |  |\n| Learning rate decay | Cosine |  |\n| Learning rate (min, base) | $8 \\mathrm{e}-5,8 \\mathrm{e}-4$ |  |\n| Global batch size | 256 |  |\n| Weight decay | 0.1 |  |\n| Num Layers | 27 | 36 |\n| Hidden Size | 1024 | 1792 |\n| MLP Activation | SwiGLU |  |\n| MLP Width | 2 |  |\n| Num.",
    "based-67": "Linear Attn Layers | 5 | 7 |\n| Num. Linear Attn Heads | 16 |  |\n| Taylor Feature Dimension | 16 |  |\n| Linear Attn Positional Encodings | None |  |\n| Num. Sliding Window Layers | 5 | 7 |\n| Sliding Window Size | 64 | 16 |\n| Sliding Window Heads | 16 |  |\n| Num. BaseConv Layers | Rotary |  |\n| Sliding Window Positional Encodings | 22 |  |\n| BaseConv Projection Expansion Factor | 4 |  |\n| BaseConv Filter Size | 3 |  |\n| BaseConv Activation | SiLU |  |\n\nTable 8: Attention Training Settings\n\n|  | $355 \\mathrm{M} \\quad 1.4 \\mathrm{~B}$ |\n| :---: | :---: |\n| Optimizer | Adam |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.95$ |\n| Optimizer eps | $1 e-8$ |\n| Precision | BFloat16 |\n| Warmup | $1 \\%$ |\n| Learning rate decay | Cosine |\n| Learning rate (min, base) | $8 \\mathrm{e}-5,8 \\mathrm{e}-4$ |\n| Global batch size | 256 |\n| Weight decay | 0.1 |\n| Num Layers | 2436 |\n| Hidden Size | 10241680 |\n| Num Heads | 1624 |\n| RMSNorm | True |\n| MLP Bias | False |\n| Flash Attn | True |\n| Rotary Emb. Fraction | 0.5 |\n| MLP Activation | SwiGLU |\n| MLP Width | 4 |\n\nTable 9: Mamba Training Settings\n\n|  | 355 M |\n| ---: | :---: |\n| Optimizer | Adam |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.95$ |\n| Optimizer eps | $1 e-8$ |\n| Precision | BFloat16 |\n| Warmup | $1 \\%$ |\n| Learning rate decay | Cosine |\n| Learning rate (min, base) | $8 \\mathrm{e}-5,8 \\mathrm{e}-4$ |\n| Global batch size | 256 |\n| Weight decay | 0.1 |\n| Num Layers | 46 |\n| Hidden Size | 1024 |\n| RMSNorm | 2048 |\n| Norm Epsilon | $1 e-5$ |\n| Dt State | 16 |\n| Dt (Min, Max) | $(0.001,0.1)$ |\n| Dt Init. Strategy | Random |\n| Dt Init. Floor | $1 e-4$ |\n| Dt Scale | 1.0 |\n| Dt Softplus | True |\n| Projection Expansion Factor | 2 |\n| Short Conv Filter Size | 4 |\n\nTable 10: Hyena Training Settings\n\n|  | 355 M |\n| ---: | :---: |\n| Optimizer | Adam |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.95$ |\n| Optimizer eps | $1 e-8$ |\n| Precision | BFloat16 |\n| Warmup | $1 \\%$ |\n| Learning rate decay | Cosine |\n| Learning rate (min, base) | $8 \\mathrm{e}-5,8 \\mathrm{e}-4$ |\n| Global batch size | 256 |\n| Weight decay | 0.1 |\n| Num Layers | 29 |\n| Hidden Size | 1024 |\n| Num Heads | 1 |\n| MLP Width | 2 |\n| 3 |  |\n| Exp.",
    "based-68": "Mod.",
    "based-69": "Decay. (Falter Size | $0.3,1.2$ |\n| Filter Sine Freq. (w) | 14 |\n| Filter Order | 64 |\n| Filter Inner MLP | 2 |\n\nTable 11: Hyena Training Settings\n\n|  | 355 M |\n| ---: | :---: |\n| Optimizer | Adam |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.99$ |\n| Optimizer eps | $1 e-8$ |\n| Precision | BFloat16 |\n| Warmup | $1 \\%$ |\n| Learning rate (min, base) | Cosine |\n| Global batch size | $25,8 \\mathrm{e}-4$ |\n| Weight decay | 0.1 |\n| Num Layers | 24 (No Attention Layers) |\n| Hidden Size | 1024 |\n| Num Heads | 16 |\n| MLP Width | 4 |\n\nTable 12: Hyena Training Settings\n\n|  | 355 M |\n| ---: | :---: |\n| Optimizer | Adam |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.99$ |\n| Optimizer eps | $1 e-8$ |\n| Precision | BFloat16 |\n| Warmup | $1 \\%$ |\n| Learning rate decay | Cosine |\n| Learning rate (min, base) | $8 \\mathrm{e}-5,8 \\mathrm{e}-4$ |\n| Global batch size | 256 |\n| Weight decay | 0.1 |\n| Num Layers | 19 |\n| Hidden Size | 1024 |\n| MLP Width | 3.5 |\n\nTable 13: Gated Linear Attention (GLA) Training Settings\n\n|  | 355 M |\n| ---: | :---: |\n| Optimizer | Adam |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.95$ |\n| Optimizer eps | $1 e-8$ |\n| Precision | BFloat16 |\n| Warmup | $1 \\%$ |\n| Learning rate decay | Cosine |\n| Learning rate (min, base) | $8 \\mathrm{e}-5,8 \\mathrm{e}-4$ |\n| Global batch size | 256 |\n| Weight decay | 0.1 |\n| Num Layers | 24 |\n| Hidden Size | 1024 |\n| Num Heads | 4 |\n| MLP Width | 2 |\n\n\n[^0]:    *Corresponding authors; equal contribution and random ordering for SA, SE, MZ (SSM).",
    "based-70": "[^1]:    ${ }^{1}$ Examples of recall-intensive tasks include information extraction, reading comprehension, and code generation using custom variable and function names. These require using in context (contrasting memorized) information during generation. [^2]:    ${ }^{2}$ In particular, for Mamba 5, see Corollary F. 1\n    ${ }^{3}$ Here, we need the entries of the state to be bounded. ${ }^{4}$ That is, each token from the vocabulary has the natural binary encoding in $\\{0,1\\}{ }^{\\log _{2}(c)}$\n\n[^3]:    ${ }^{6}$ By linear projections of a matrix $\\boldsymbol{u} \\in \\mathbb{R}^{m \\times n}$, we mean $\\boldsymbol{u} \\boldsymbol{W}+\\boldsymbol{B}$ for some weight matrix $\\boldsymbol{W} \\in \\mathbb{R}^{n \\times n}$ and bias $\\boldsymbol{B} \\in \\mathbb{R}^{m \\times n}$. ${ }^{7}$ That is, $\\boldsymbol{B}[i,:], C[i,:]$ and $\\Delta[i,:]$ depend only on $\\boldsymbol{u}[0 \\cdots i-1]$. [^4]:    ${ }^{8}$ The randomized communication complexity of function $f$ is defined as $\\min _{\\pi}\\|\\pi\\|$, where $\\pi$ ranges over all randomized protocols that can solve $f$ with probability of success at least $2 / 3$. [^5]:    ${ }^{9}$ Since BaseConv is data independent, note that the polynomial $P(\\cdot)$ is defined once we fix $N$ and $d$.",
    "based-71": ""
}