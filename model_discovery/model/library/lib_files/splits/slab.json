{
    "slab-0": "SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization\n\nJialong Guo Xinghao Chen Yehui Tang Yunhe Wang\n\nAbstract\n\nTransformers have become foundational architectures for both natural language and computer vision tasks.",
    "slab-1": "However, the high computational cost makes it quite challenging to deploy on resource-constraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains top-1 accuracy on ImageNet-1K with ms latency, which is ms less than that of Flatten-Swin with higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency. Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/. Machine Learning, ICML\n\n1 Introduction\n\nIntroduced initially for tasks in natural language processing (Vaswani et al., 2017), transformer architecture has rapidly emerged as a preeminent model in the landscape of language models. Its influence has significantly expanded with the introduction of Vision Transformer (ViT) (Dosovitskiy et al., 2020), illustrating the efficacy and versatility of transformer-based architectures. These architectures have demonstrated their capability to achieve competitive performance benchmarks in comparison to convolutional neural networks (CNNs) across diverse vision tasks (Han et al., 2022; Wang et al., 2022; Zheng et al., 2023; Tang et al., 2023a; Carion et al., 2020; Xu et al., 2023). Due to its powerful performance, transformer has become the mainstream architecture in deep learning. However, the computational demands of transformer architecture pose a significant challenge, which is predominantly due to the quadratic computational complexity of its attention mechanism and the necessity for online statistic computation of LayerNorm component. Numerous efforts have been directed towards enhancing the efficiency of transformer architecture (Tang et al., 2024; Wu et al., 2023; Tang et al., 2023b). Several approaches have sought to mitigate computational complexity by limiting the scope of token interactions within self-attention mechanisms, such as downsampling the key and value matrices (Wang et al., 2021), implementing sparse global attention patterns (Child et al., 2019), and computing self-attention within smaller windows (Tu et al., 2022; Liu et al., 2021; Dong et al., 2022). Meanwhile, linear attention emerges as an alternative strategy to enhance computational efficiency by breaking down the attention mechanism into linear computational cost (Cai et al., 2022; Han et al., 2023), yet it is still a challenging task to obtain a good balance between efficiency and accuracy. Moreover, there are some explorations into substituting LayerNorm (LN) with BatchNorm (BN) within transformers, motivated by the additional computational overhead LayerNorm incurs during inference. Yang et al. (2022) propose to add a BatchNorm layer in-between the two linear layers in the feed forward network to stabilize the training. However, there still exists a performance gap between the LayerNorm-based and BatchNorm-based transformers. In this paper, we focus on obtaining efficient transformer architectures by digging deep into the computational inefficient modules, i.e., normalization layers and attention modules. We first explore to replace LayerNorm with BatchNorm to accelerate inference for transformer. BatchNorm leads to lower inference latency but may cause training collapse and inferior performance, while LayerNorm could stabilize the training yet has extra computational cost during inference. To this end, we first propose a progressive strategy to gradually replace LayerNorm with BatchNorm by using a hyper-parameter to control the proportion of both normalization layers. Initially the transformer architecture is dominated by the LayerNorm and gradually transits to pure BatchNorm at the end of training. This strategy effectively mitigates the risk of training collapse and also eliminating the need for calculating statistics during inference. In addition to the progressive strategy, we also propose a novel re-parameterization formula for BatchNorm (RepBN), to enhance training stability and overall performance. Furthermore, the computational cost of attention is critical for efficient transformer and prior methods struggle to obtain good balance of efficiency and accuracy. To this end, we propose a simplified linear attention (SLA) module which utilizes ReLU as the kernel function and incorporate a depth-wise convolution to perform local feature enhancement. The proposed attention mechanism is more efficient than prior linear attention but still attains comparable performance. We extensively evaluate our proposed method for various architectures on various benchmarks. Our progressive re-parameterized BatchNorm shows strong performance for image classification and object detection tasks, obtaining similar accuracy with lower inference latency. Moreover, coupled with the progressive RepBN and simplified linear attention module, our SLAB transformer achieves competitive accuracy compared to Flatten transformer with improved computational efficiency. For example, SLAB-Swin-S achieves 83.6% Top-1 accuracy on ImageNet-1K with ms latency, which is ms less than that of Flatten-Swin-S with higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower inference latency. 2 Related Work\n\n2.1 Efficient Architecture for Transformers\n\nWith the advent of the pioneering Vision Transformer (ViT) (Dosovitskiy et al., 2020), the potential of the transformer architecture for computer vision tasks has been greatly explored. Various researchers are devoted to this field to make transformer-based architecture more efficient and powerful. Touvron et al. (2021) propose DeiT which utilizes distillation to achieve strong performance with training only on ImageNet1K. Liu et al. (Liu et al., 2021) propose Swin Transformer, which introduces shifted windowing scheme and brings greater efficiency. As the self-attention computation is limited to a small window, this transformer has linear computational complexity. Several works improve the design of sparse pattern to enhance the interaction of each token, such as CSwin (Dong et al., 2022). Besides, the dynamic attention mechanism tries to control the key/value interact with query adaptive to data, such as DAT++ (Xia et al., 2023) and BiFormer (Zhu et al., 2023). Apart from the above methods, linear attention is a popular research direction to reduce the computational complexity for transformer. Many effective mechanisms have been proposed to replace the softmax function. For example, Performers (Choromanski et al., 2020) uses positive orthogonal random features approach to approximate softmax. Hydra attention (Bolya et al., 2022) selects the cosine similarity as kernel function. Flatten Transformer (Han et al., 2023) designs a focused function to improve the focus ability of linear attention. 2.2 Normalization for Transformers\n\nNormalization is known as an useful method to make training stable and boost performance. Nowadays, a variety of normalization methods have been proposed, such as BatchNorm (Ioffe & Szegedy, 2015), LayerNorm (Ba et al., 2016), InstanceNorm (Ulyanov et al., 2016), GroupNorm (Wu & He, 2018), MABN (Yan et al., 2020) and UN (Yang et al., 2022). BatchNorm is widely used in convolutional networks and LayerNorm is commonly utilized for networks such as transformer and LSTM. Normalization could be categorized into offline methods and online methods according to whether the mean and variance need to be computed at inference time (Yang et al., 2022). Online methods are usually batch-irrelevant like LayerNorm, InstanceNorm and GroupNorm. These methods compute the statistics in both training and inference. LayerNorm is a commonly used in transformer architecture. Offline methods are batch-related like BatchNorm and UN, in which the batch dimension is concluded in the calculations of both mean and variance (Yao et al., 2021). As the mean and variance are pre-computed in inference, offline normalization can be fused into adjacent linear operations. During inference, there will be no offline normalization operations and the inference time will be reduced. However, offline methods usually face the problem of performance degradation and training collapse while using in transformer. To address this problem, Yao et al. (2021) proposes to add a BatchNorm layer in-between the two linear layers in the MLP block that makes training statistics stable. Yang et al. (2022) finds that the issue is caused by abnormal behaviors of activation statistics, and proposes a tailored fluctuation smoothing strategy and an adaptive outlier filtration strategy to boost performance and stable training. 3 Preliminaries\n\nGiven the input tokens , where is the feature dimension, the general architecture of transformer block can be written as:\n\nX = X + Attn \u200b ( Norm \u200b ( X ) ) , X = X + MLP \u200b ( Norm \u200b ( X ) ) , formulae-sequence \ud835\udc4b \ud835\udc4b Attn Norm \ud835\udc4b \ud835\udc4b \ud835\udc4b MLP Norm \ud835\udc4b \\begin{split}X=X+\\mathrm{Attn}(\\mathrm{Norm}(X)),\\\\\nX=X+\\mathrm{MLP}(\\mathrm{Norm}(X)),\\end{split} (1)\n\nwhere calculates the attention scores, denotes multilayer perceptron and is the normalization function. In the default configuration of transformer block, is usually a LayerNorm operation and is the softmax-based attention mechanism (Vaswani et al., 2017). Attention plays an important role in Transformer. Denote query, key and value matrix as , softmax attention computes the pairwise similarity between queries and keys firstly, and leads to the quadratic computation complexity in relation to the number of queries and keys N. This makes transformer computationally expensive especially in dealing with tasks that have a long sequence input. Linear attention aims to decouple the softmax function with proper approximation or instead it with other kernel function to compute first. With this change in computation order, the computation complexity becomes , which is linearly related to the number of queries and keys N. However, LayerNorm occupies unnegligible portion of latency since it requires statistic calculation during inference. Therefore, in this paper we explore to leverage BatchNorm for building efficient transformers, which only exists in training and could be merged with preceding or sequential linear layers. Moreover, the attention module plays the most important part for transformers and the softmax-based attention mechanism is computational inefficient due to its quadratic computation complexity. In this paper, we propose a simple yet efficient form of attention, which greatly reduce the latency but also remains strong performance on various vision tasks. 4 Methods\n\nIn this paper, we focus on building efficient transformers and propose a series of strategies, including a progressive strategy to replace the LayerNorm (LN) with the re-parameterized BatchNorm (BN) and the simplified linear attention (SLA) module. The proposed SLAB transformers obtains strong performance compared with prior methods while enjoying more computational efficacy. 4.1 Progressive Re-parameterized BatchNorm\n\nLayerNorm requires statistic calculations in both training and inference, thus significantly hinders the running speed of transformers. On contrast, BatchNorm could be simply merged with linear layers during inference and is more suitable for efficient architectures. However, directly leveraging BatchNorm for transformers brings unsatisfactory performance (Yao et al., 2021). To this end, we propose to progressively replace LayerNorm with BatchNorm during training, and also propose a new re-parameterization formula of BatchNorm to further improve the performance, as shown in Figure 2. Re-parameterized BatchNorm. The proposed RepBN is formulated as:\n\nRepBN \u200b ( X ) = BN \u200b ( X ) + \u03b7 \u200b X , RepBN \ud835\udc4b BN \ud835\udc4b \ud835\udf02 \ud835\udc4b \\mathrm{RepBN}(X)=\\mathrm{BN}(X)+\\eta X, (2)\n\nwhere is a learnable parameter that is jointly trained in an end-to-end manner. Once the training is done, the RepBN could be re-parameterized as a norm form of BN, as shown in Lemma 4.1. Lemma 4.1. Denote a BN layer with mean , standard deviation , rescale and shift parameters and as . We can re-parameterize the RepBN in Eq. 2 as:\n\nRepBN \u200b ( X ; \u03bc , \u03c3 , \u03b1 , \u03b2 ) = BN \u200b ( X ; \u03bc , \u03c3 , \u03b1 + \u03b7 \u200b \u03c3 , \u03b2 + \u03b7 \u200b \u03bc ) . RepBN \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udefc \ud835\udefd BN \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udefc \ud835\udf02 \ud835\udf0e \ud835\udefd \ud835\udf02 \ud835\udf07 \\small\\mathrm{RepBN}(X;\\mu,\\sigma,\\alpha,\\beta)=\\mathrm{BN}(X;\\mu,\\sigma,\\alpha+\\eta\\sigma,\\beta+\\eta\\mu). (3)\n\nProof. RepBN \u200b ( X ; \u03bc , \u03c3 , \u03b1 , \u03b2 ) = BN \u200b ( X ; \u03bc , \u03c3 , \u03b1 , \u03b2 ) + \u03b7 \u200b X = X \u2212 \u03bc \u03c3 \u200b \u03b1 + \u03b2 + \u03b7 \u200b X = X \u2212 \u03bc \u03c3 \u200b \u03b1 + \u03b2 + X \u03c3 \u200b \u03c3 \u200b \u03b7 = X \u2212 \u03bc \u03c3 \u200b \u03b1 + \u03b2 + X \u2212 \u03bc \u03c3 \u200b \u03c3 \u200b \u03b7 + \u03bc \u200b \u03b7 = X \u2212 \u03bc \u03c3 \u200b ( \u03b1 + \u03b7 \u200b \u03c3 ) + ( \u03b2 + \u03b7 \u200b \u03bc ) = BN \u200b ( X ; \u03bc , \u03c3 , \u03b1 + \u03b7 \u200b \u03c3 , \u03b2 + \u03b7 \u200b \u03bc ) . RepBN \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udefc \ud835\udefd BN \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udefc \ud835\udefd \ud835\udf02 \ud835\udc4b \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udefc \ud835\udefd \ud835\udf02 \ud835\udc4b \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udefc \ud835\udefd \ud835\udc4b \ud835\udf0e \ud835\udf0e \ud835\udf02 \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udefc \ud835\udefd \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udf0e \ud835\udf02 \ud835\udf07 \ud835\udf02 \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udefc \ud835\udf02 \ud835\udf0e \ud835\udefd \ud835\udf02 \ud835\udf07 BN \ud835\udc4b \ud835\udf07 \ud835\udf0e \ud835\udefc \ud835\udf02 \ud835\udf0e \ud835\udefd \ud835\udf02 \ud835\udf07 \\begin{split}&\\mathrm{RepBN}(X;\\mu,\\sigma,\\alpha,\\beta)=\\mathrm{BN}(X;\\mu,\\sigma,\\alpha,\\beta)+\\eta X\\\\\n&=\\frac{X-\\mu}{\\sigma}\\alpha+\\beta+\\eta X=\\frac{X-\\mu}{\\sigma}\\alpha+\\beta+\\frac{X}{\\sigma}\\sigma\\eta\\\\\n&=\\frac{X-\\mu}{\\sigma}\\alpha+\\beta+\\frac{X-\\mu}{\\sigma}\\sigma\\eta+\\mu\\eta\\\\\n&=\\frac{X-\\mu}{\\sigma}(\\alpha+\\eta\\sigma)+(\\beta+\\eta\\mu)\\\\\n&=\\mathrm{BN}(X;\\mu,\\sigma,\\alpha+\\eta\\sigma,\\beta+\\eta\\mu).\\end{split} (4)\n\nBased on Lemma 4.1, the distribution of RepBN\u2019s output is control by and , which is corresponds to the variance and mean. RepBN can recover the distribution with the help of and . Meanwhile, when , it is equivalent to BatchNorm being skipped. When , RepBN is converted into pure BatchNorm. Progressive LN RepBN. To facilitate the training of a pure BN-based transformers, we propose to progressively transit the LN to RepBN during training, i.e.,\n\nPRepBN \u200b ( X ) = \u03b3 \u200b LN \u200b ( X ) + ( 1 \u2212 \u03b3 ) \u200b RepBN \u200b ( X ) , PRepBN \ud835\udc4b \ud835\udefe LN \ud835\udc4b 1 \ud835\udefe RepBN \ud835\udc4b \\mathrm{PRepBN}(X)=\\gamma\\mathrm{LN}(X)+(1-\\gamma)\\mathrm{RepBN}(X), (5)\n\nwhere is a hyper-parameter to control the output of different normalization layers. Generally at the begin of training when the LN dominates the architecture, and at the end of training to make sure it transits to a pure BN-based transformer. We utilize a simple yet effective decay strategy for :\n\n\u03b3 = T \u2212 T c \u200b u \u200b r T , \u03b3 \u2208 [ 0 , 1 ] , formulae-sequence \ud835\udefe \ud835\udc47 subscript \ud835\udc47 \ud835\udc50 \ud835\udc62 \ud835\udc5f \ud835\udc47 \ud835\udefe 0 1 \\gamma=\\dfrac{T-T_{cur}}{T},\\gamma\\in[0,1], (6)\n\nwhere is the total steps of training with LayerNorm and is the current step. This progressive strategy eases the difficulty of training a pure BN-based transformer and thus leads to strong performance on various tasks.",
    "slab-2": "There are some other decay strategies for attenuating the value of gradually, such as cosine decay and step decay. Empirically, we find that the linear strategy is one of the more effective and simpler. 4.2 Simplified Linear Attention\n\nAttention module is the most import part in a transformer network, which is generally formulated as:\n\nQ = X \u200b W Q , K = X \u200b W K , V = X \u200b W V , O i = \u2211 j = 1 N Sim \u200b ( Q i , K j ) \u2211 j Sim \u200b ( Q i , K j ) \u200b V j , formulae-sequence \ud835\udc44 \ud835\udc4b subscript \ud835\udc4a \ud835\udc44 formulae-sequence \ud835\udc3e \ud835\udc4b subscript \ud835\udc4a \ud835\udc3e formulae-sequence \ud835\udc49 \ud835\udc4b subscript \ud835\udc4a \ud835\udc49 subscript \ud835\udc42 \ud835\udc56 superscript subscript \ud835\udc57 1 \ud835\udc41 Sim subscript \ud835\udc44 \ud835\udc56 subscript \ud835\udc3e \ud835\udc57 subscript \ud835\udc57 Sim subscript \ud835\udc44 \ud835\udc56 subscript \ud835\udc3e \ud835\udc57 subscript \ud835\udc49 \ud835\udc57 \\small\\begin{split}&Q=XW_{Q},K=XW_{K},V=XW_{V},\\\\\n&O_{i}=\\sum_{j=1}^{N}\\dfrac{\\mathrm{Sim}(Q_{i},K_{j})}{\\sum_{j}\\mathrm{Sim}(Q_{i},K_{j})}V_{j},\\end{split} (7)\n\nwhere project the input tokens to query, key and value tensors, respectively. denotes the similarity function. For the original form of attention, the similarity function is:\n\nSim softmax \u200b ( Q i , K j ) = exp \u2061 ( Q i \u200b K j T C ) , subscript Sim softmax subscript \ud835\udc44 \ud835\udc56 subscript \ud835\udc3e \ud835\udc57 subscript \ud835\udc44 \ud835\udc56 superscript subscript \ud835\udc3e \ud835\udc57 \ud835\udc47 \ud835\udc36 \\small\\mathrm{Sim_{softmax}}(Q_{i},K_{j})=\\exp(\\frac{Q_{i}K_{j}^{T}}{\\sqrt{C}}), (8)\n\nthis softmax-based attention leads to high computational complexity. Several recent methods investigate the usage of linear attention to remove the softmax calculation thus improve the efficacy of transformers (Han et al., 2023). However, these methods still suffer quite complex design and are not computation efficient enough. In this paper, we propose a simplified linear attention (SLA) which is formulated as follow:\n\nSim S \u200b L \u200b A \u200b ( Q i , K j ) = ReLU \u200b ( Q i ) \u200b ReLU \u200b ( K j ) T , O ~ i = \u2211 j = 1 N Sim S \u200b L \u200b A \u200b ( Q i , K j ) \u2211 j Sim S \u200b L \u200b A \u200b ( Q i , K j ) \u200b V j , O S \u200b L \u200b A = O ~ + DWC \u200b ( V ) , formulae-sequence subscript Sim \ud835\udc46 \ud835\udc3f \ud835\udc34 subscript \ud835\udc44 \ud835\udc56 subscript \ud835\udc3e \ud835\udc57 ReLU subscript \ud835\udc44 \ud835\udc56 ReLU superscript subscript \ud835\udc3e \ud835\udc57 \ud835\udc47 formulae-sequence subscript ~ O \ud835\udc56 superscript subscript \ud835\udc57 1 \ud835\udc41 subscript Sim \ud835\udc46 \ud835\udc3f \ud835\udc34 subscript \ud835\udc44 \ud835\udc56 subscript \ud835\udc3e \ud835\udc57 subscript \ud835\udc57 subscript Sim \ud835\udc46 \ud835\udc3f \ud835\udc34 subscript \ud835\udc44 \ud835\udc56 subscript \ud835\udc3e \ud835\udc57 subscript \ud835\udc49 \ud835\udc57 subscript O \ud835\udc46 \ud835\udc3f \ud835\udc34 ~ O DWC \ud835\udc49 \\small\\begin{split}&{\\rm Sim}_{SLA}\\left(Q_{i},K_{j}\\right)=\\mathrm{ReLU}\\left(Q_{i}\\right){\\mathrm{ReLU}\\left(K_{j}\\right)}^{T},\\\\\n&\\tilde{\\rm O}_{i}=\\sum_{j=1}^{N}\\dfrac{\\mathrm{Sim}_{SLA}(Q_{i},K_{j})}{\\sum_{j}\\mathrm{Sim}_{SLA}(Q_{i},K_{j})}V_{j},\\\\\n&\\!{\\rm O}_{SLA}\\!=\\tilde{\\rm O}+\\!{\\rm DWC}(V),\\end{split} (9)\n\nwhere denotes a depth-wise convolution. It is a simple yet efficient linear attention since it also enjoys the decoupling computation order by computing first, and leads to great complexity reduction. Moreover, only ReLU function and depth-wise convolution are explored and both operations are computation friendly in most hardware. To demonstrate that our method still maintains feature diversity, we visualize the effect of attention map on DeiT-T that applied the strategy of progressive re-parameterized BatchNorm and simplified linear attention (SLAB), as shown in Figure 3. It can be find that a high rank is still kept for our proposed method, demonstrating its good capacity for capturing attention information. 5 Experiments\n\nIn this section, we evaluate our method on various computer vision tasks including image classification, object detection and instance segmentation and also on language modeling task. We conduct extensive experiments for various backbones with our proposed progressive re-parameterized BatchNorm and simplified linear attention module. We train our model on ImageNet-1K (Deng et al., 2009) for image classification, and evaluate the effect of object detection and instance segmentation tasks on COCO dataset (Lin et al., 2014). At last, we ablate the important design elements of our proposed method on classification task. 5.1 Image Classification\n\nSettings. For image classification, we adhere to the configuration outlined in (Touvron et al., 2021). We train all models for 300 epochs with AdamW optimizer, incorporating a cosine decay learning rate scheduler with 20 epochs of linear warm-up. The batch size is set to 1024, the initial learning rate is 0.001, and the weight decay value is 0.05. In the case of models utilizing the proposed progressive re-parameterized BatchNorm, a reduced droppath (Larsson et al., 2016) rate is applied. The linear decay steps for PRepBN slightly varies across different backbones. Due to the variance shift induced by droppath, we freeze the model parameters and exclusively update the statistics of re-parameterized BatchNorm for 10 epochs at the end of training. We also demonstrate the effectiveness of our method with both progressive re-parameterized BatchNorm and simplified linear attention. We follow the setting of (Han et al., 2023) on macro architecture design and training. All reported results of throughput/latency are obtained on a single V100 GPU. For classification task, we measure FLOPs as well as the throughput/latency for the image resolution of 224224. Results on image classification task. Table 1 presents the results of different backbones with our PRepBN normalization. Our proposed PRepBN demonstrates comparable or even superior performance when compared with LayerNorm. More specifically, the models using our PRepBN as normalization layer exhibit performance improvements ranging from 0.1% to 1.4%. Notably, PRepBN is amenable to fusion with other linear operations, allowing it to obtain more efficient inference. We further compare our method with BN+FFNBN (Yao et al., 2021) which also aims to train transformer model with BatchNorm. It can be seen that our PRepBN achieves consistent improvements on different backbones. For example, our proposed PRepBN achieves 80.2% top-1 accuracy on DeiT-S model, which is 1.4% better than the BN+FFNBN method. For swin transformer, our PRepBN brings +0.5%, +0.4% and +0.5% accuracy gain than BN+FFNBN on Swin-T, Swin-S and Swin-B models. As shown in Figure 4, we present a comparative analysis of our method across DeiT-based, PVT-based, and Swin-based models. It is evident that transformers equipped with our PRepBN achieve higher throughput while maintaining similar accuracy levels. Table 2 presents the performance of our SLAB transformer, which is powered by our proposed progressive re-parameterized BatchNorm and simplified linear attention module. We compare our model with Flatten transformer, which utilizes focused linear attention for higher efficiency. Experiments on various architectures including DeiT (Touvron et al., 2021), PVT (Wang et al., 2021), CSwin (Dong et al., 2022) and Swin (Liu et al., 2021) demonstrate than our SLAB transformer obtains better performance than Flatten transformer. More specifically, our SLAB-Swin-T model obtains top-1 accuracy on ImageNet-1K with ms latency, which is ms less than that of Flatten-Swin with higher accuracy. Our models are more computational efficient mainly due to more hardware friendly normalization layers as well as the simplified linear attention module. Figure 1 also shows the trade-off between accuracy and latency of our SLAB transformer, Flatten tranformer (Han et al., 2023) and the original Swin transformer, which demonstrate better performance of our model. 5.2 Object Detection\n\nSettings. We use Mask R-CNN (He et al., 2017) to evaluate the effectiveness of our method on COCO dataset for object detection and instance segmentation tasks. The backbones used in Mask R-CNN are pretrained on ImageNet-1K. All models are trained for schedule, i.e., 12 epochs. The latency is measured with a batch size of 1 on V100 GPU for a average value of 100 rounds. Results on object detection task. We compare our proposed PRepBN with standard LayerNorm for various backbones including Swin and PVT for object detection task. The results are shown in Table 3. It reveals that our method achieves quite comparable performance with original models equipped with LayerNorm. Taking advantages of offline normalization, the models with our proposed PRepBN obtain lower inference latency. For example, the latency of Mask R-CNN exhibits a reduction from 64ms to 59.6ms when PVT-S backbone is equipped with our PRepBN and the accuracy for object detection and instance segmentation is similar. A more clear visualization for the trade-off between mAP and latency is also presented in Figure 5, which demonstrates that our proposed method achieves better overall performance on object detection.",
    "slab-3": "5.3 language modeling\n\nSettings. We also evaluate our proposed method on language modeling task based on Adaptive Inputs (Baevski & Auli, 2018). We train our models on the Wikitext-103 (Merity et al., 2016) dataset, which contains over 100 million tokens. We set the number of tokens per GPU to 4096 and train on 8 GPUs. The number of tokens per sample is limit to 512. We also apply our method on LLaMA-350M model, following the similar architecture and training settings as prior work (Yang et al., 2023; He et al., 2024). Results on language modeling task. As shown in Table 4, our PRepBN achieves similar perplexity with the model equipped with LayerNorm, while the latency reduces from 13.9 ms to 12.9 ms per token. Besides, We apply our PRepBN on more modern large language models such as LLaMA, which adopts the variant of LayerNorm that removes the computation of mean values, i.e., RMSNorm. As shown in Table 5, our method successfully boost the throughput from 44.0 to 50.4 tokens per second on V100 GPU, and obtains even slightly better average accuracy. These results demonstrate the effectiveness of our proposed PRepBN on language modeling task. 5.4 Ablation Studies\n\nIn this section, we conduct extensive ablation studies to demonstrate the impact of our key designs. The impact of SLA and PRepBN. We first explore the impact of the simplified linear attention (SLA) module and progressive re-parameterized BatchNorm (PRepBN) on different backbones. As shown in Table 6, utilizing our simplified linear attention (SLA) brings consistent improvement for efficiency. For DeiT and PVT, our SLA obtains significant latency reduction and a few accuracy drop. Moreover, Swin transformers equipped with our SLA achieve quite comparable accuracy with that of original ones but with lower latency. In addition, the latency could be further reduced by replacing LayerNorm by our proposed progressive re-parameterized BatchNorm (PRepBN). This strategy hardly affects the accuracy and even recover the accuracy of model like DeiT and PVT. Combining these two strategies, the latency is reduced by 5.6 ms when the accuracy is improved by 0.2% for DeiT-T. Moreover, our method obtains similar accuracy and harvests 2.2 ms and 2.4 ms latency reduction for Swin-T and Swin-S models. Ablation study for PRepBN. We investigate key components of our proposed PRepBN, i.e., the progressive strategy and re-parameterized BatchNorm (RepBN). Directly training a BatchNorm-based transformer leads to quite unstable training, either obtaining inferior performance or collapse in training (e.g., DeiT-S and Flatten-Swin-T). To avoid the variance shift (Li et al., 2019) caused by droppath, which will influence the performance of BatchNorm, we simply set the droppath rate to 0 on DeiT-T model. As shown in Table 7, applying progressive strategy on a BatchNorm-based DeiT-T model brings 1.2% accuracy gain. We further utilize our RepBN in the model and the accuracy increases to 73.6%. These results demonstrate that both our proposed progressive strategy and re-parameterized BatchNorm (RepBN) are beneficial for training a pure BatchNorm-based transformer. 6 Conclusion\n\nIn this paper, we investigates the computational bottleneck modules of transformer and propose novel strategies including progressive Re-parameterized BatchNorm and simplified linear attention to obtain efficient transformer architectures. Our method progressively replace LayerNorm with re-parameterized BatchNorm during training to obtain lossless accuracy, while leveraging the efficiency advantages of BatchNorm during inference. Additionally, we devise a simplified linear attention mechanism that attains comparable performance with other linear attention methods but with less computational cost. Through extensive experiments for both computer vision and language modeling tasks, we showcase that our method achieves stronger performance with respect to accuracy and efficiency than prior methods and sheds light into the design of efficient transformer. Acknowledgements. We gratefully acknowledge the support of MindSpore (Huawei, 2020), CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research. Impact Statements\n\nThis paper presents work whose goal is to advance the field of Deep Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References\n\nBa et al. (2016) Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Baevski & Auli (2018) Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018. Bolya et al. (2022) Bolya, D., Fu, C.-Y., Dai, X., Zhang, P., and Hoffman, J. Hydra attention: Efficient attention with many heads.",
    "slab-4": "In European Conference on Computer Vision, pp. 35\u201349. Springer, 2022. Cai et al. (2022) Cai, H., Gan, C., and Han, S. Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition. arXiv preprint arXiv:2205.14756, 2022. Carion et al. (2020) Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. End-to-end object detection with transformers. In European conference on computer vision, pp. 213\u2013229. Springer, 2020. Child et al. (2019) Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.",
    "slab-5": "arXiv preprint arXiv:1904.10509, 2019. Choromanski et al. (2020) Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers.",
    "slab-6": "arXiv preprint arXiv:2009.14794, 2020. Deng et al. (2009) Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009. Dong et al. (2022) Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., and Guo, B. Cswin transformer: A general vision transformer backbone with cross-shaped windows.",
    "slab-7": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12124\u201312134, 2022. Dosovitskiy et al. (2020) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale.",
    "slab-8": "arXiv preprint arXiv:2010.11929, 2020. Han et al. (2023) Han, D., Pan, X., Han, Y., Song, S., and Huang, G. Flatten transformer: Vision transformer using focused linear attention.",
    "slab-9": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5961\u20135971, 2023. Han et al. (2022) Han, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., Tang, Y., Xiao, A., Xu, C., Xu, Y., et al. A survey on vision transformer. IEEE transactions on pattern analysis and machine intelligence, 45(1):87\u2013110, 2022. He et al. (2017) He, K., Gkioxari, G., Doll\u00e1r, P., and Girshick, R. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pp. 2961\u20132969, 2017. He et al. (2024) He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., and Wang, Y. Densemamba: State space models with dense hidden connection for efficient large language models.",
    "slab-10": "arXiv preprint arXiv:2403.00818, 2024. Huawei (2020) Huawei. Mindspore. https://www.mindspore.cn/, 2020. Ioffe & Szegedy (2015) Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift.",
    "slab-11": "In International conference on machine learning, pp. 448\u2013456. pmlr, 2015. Larsson et al. (2016) Larsson, G., Maire, M., and Shakhnarovich, G. Fractalnet: Ultra-deep neural networks without residuals.",
    "slab-12": "arXiv preprint arXiv:1605.07648, 2016. Li et al. (2019) Li, X., Chen, S., Hu, X., and Yang, J. Understanding the disharmony between dropout and batch normalization by variance shift.",
    "slab-13": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2682\u20132690, 2019. Li et al. (2023) Li, Z., Xiao, J., Yang, L., and Gu, Q. Repq-vit: Scale reparameterization for post-training quantization of vision transformers.",
    "slab-14": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 17227\u201317236, 2023. Lin et al. (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp.",
    "slab-15": "740\u2013755. Springer, 2014. Liu et al. (2021) Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows.",
    "slab-16": "In Proceedings of the IEEE/CVF international conference on computer vision, pp. 10012\u201310022, 2021. Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models.",
    "slab-17": "arXiv preprint arXiv:1609.07843, 2016. Tang et al. (2023a) Tang, Q., Liu, C., Liu, F., Liu, Y., Jiang, J., Zhang, B., Han, K., and Wang, Y. Category feature transformer for semantic segmentation.",
    "slab-18": "arXiv preprint arXiv:2308.05581, 2023a. Tang et al. (2023b) Tang, Q., Zhang, B., Liu, J., Liu, F., and Liu, Y. Dynamic token pruning in plain vision transformers for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 777\u2013786, 2023b. Tang et al. (2024) Tang, Y., Wang, Y., Guo, J., Tu, Z., Han, K., Hu, H., and Tao, D. A survey on transformer compression. arXiv preprint arXiv:2402.05964, 2024. Touvron et al. (2021) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pp. 10347\u201310357. PMLR, 2021. Tu et al. (2022) Tu, Z., Talebi, H., Zhang, H., Yang, F., Milanfar, P., Bovik, A., and Li, Y. Maxvit: Multi-axis vision transformer. In European conference on computer vision, pp. 459\u2013479. Springer, 2022. Ulyanov et al. (2016) Ulyanov, D., Vedaldi, A., and Lempitsky, V. Instance normalization: The missing ingredient for fast stylization.",
    "slab-19": "arXiv preprint arXiv:1607.08022, 2016. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang et al. (2021) Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.",
    "slab-20": "In Proceedings of the IEEE/CVF international conference on computer vision, pp. 568\u2013578, 2021. Wang et al. (2022) Wang, Y., Chen, X., Cao, L., Huang, W., Sun, F., and Wang, Y. Multimodal token fusion for vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12186\u201312195, 2022. Wu et al. (2023) Wu, X., Zeng, F., Wang, X., and Chen, X. PPT: Token pruning and pooling for efficient vision transformers. arXiv preprint arXiv:2310.01812, 2023. Wu & He (2018) Wu, Y. and He, K. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3\u201319, 2018. Xia et al. (2023) Xia, Z., Pan, X., Song, S., Li, L. E., and Huang, G. Dat++: Spatially dynamic vision transformer with deformable attention. arXiv preprint arXiv:2309.01430, 2023. Xu et al. (2023) Xu, Y., Li, C., Li, D., Sheng, X., Jiang, F., Tian, L., and Sirasao, A. Fdvit: Improve the hierarchical architecture of vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5950\u20135960, 2023. Yan et al. (2020) Yan, J., Wan, R., Zhang, X., Zhang, W., Wei, Y., and Sun, J. Towards stabilizing batch statistics in backward propagation of batch normalization.",
    "slab-21": "arXiv preprint arXiv:2001.06838, 2020. Yang et al. (2022) Yang, Q., Zhang, K., Lan, C., Yang, Z., Li, Z., Tan, W., Xiao, J., and Pu, S. Unified normalization for accelerating and stabilizing transformers. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 4445\u20134455, 2022. Yang et al. (2023) Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Yao et al. (2021) Yao, Z., Cao, Y., Lin, Y., Liu, Z., Zhang, Z., and Hu, H. Leveraging batch normalization for vision transformers. In 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pp. 413\u2013422. IEEE, 2021. Zheng et al. (2023) Zheng, D., Dong, W., Hu, H., Chen, X., and Wang, Y. Less is more: Focus attention for efficient detr. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6674\u20136683, 2023. Zhu et al. (2023) Zhu, L., Wang, X., Ke, Z., Zhang, W., and Lau, R. W. Biformer: Vision transformer with bi-level routing attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10323\u201310333, 2023. Appendix A Detailed hyper-parameter settings. The detailed hyper-parameter settings are provided in Table A1. For image classification, we follow the setting of DeiT (Touvron et al., 2021). We use AdamW as our default optimizer and train all of model for 300 epochs with a cosine decay learning rate scheduler and 20 epochs of linear warm-up. The batch size is set to 1024, an initial learning rate is 0.001, and the value of weight decay is 0.05. For model used proposed progressive re-parameterized batchnorm, we employ an smaller rate of droppath (Larsson et al., 2016). Owing to the variance shift caused by droppath, We freeze the model parameters and only update the statistics of re-parameterized BatchNorm for 10 epochs at the end of training. Appendix B Combination with post-quantization method. Our method focuses on replacing LayerNorm with BatchNorm to obtain inference speed-up without performance degradation. It is a complementary strategy with common model compression methods like weight quantization, pruning or distillation and these methods can be combined to achieve a better performance. As a proof of concept, we conduct post-quantization using RepQ-ViT (Li et al., 2023) on DeiT-Tiny model that is trained with our PRepBN strategy. As shown in the below table, applying W8A8 quantization on top of our method still achieves the accuracy of 73.6%, while further reducing the computational cost to only 0.33 GFLOPs. This demonstrates that our method can be effectively combined with other compression methods such as quantification. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Jun 5 14:05:59 2024 by LaTeXML"
}