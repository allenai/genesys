{
    "s5-0": "Simplified State Space Layers for Sequence Modeling\n\nJimmy T.H.",
    "s5-1": "Smith*, 1, 2, Andrew Warrington*, 2, 3, Scott W. Linderman2, 3 *Equal contribution. 1Institute for Computational and Mathematical Engineering, Stanford University. 2Wu Tsai Neurosciences Institute, Stanford University. 3Department of Statistics, Stanford University. {jsmith14,awarring,scott.linderman}@stanford.edu\n\nAbstract\n\nModels using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages on the long range arena benchmark, and on the most difficult Path-X task.",
    "s5-2": "1 Introduction\n\nEfficiently modeling long sequences is a challenging problem in machine learning. Information crucial to solving tasks may be encoded jointly between observations that are thousands of timesteps apart. Specialized variants of recurrent neural networks (RNNs) (Arjovsky et al., 2016; Erichson et al., 2021; Rusch & Mishra, 2021; Chang et al., 2019), convolutional neural networks (CNNs) (Bai et al., 2018; Oord et al., 2016; Romero et al., 2022b), and transformers (Vaswani et al., 2017) have been developed to try to address this problem. In particular, many efficient transformer methods have been introduced (Choromanski et al., 2021; Katharopoulos et al., 2020; Kitaev et al., 2020; Beltagy et al., 2020; Gupta & Berant, 2020; Wang et al., 2020) to address the standard transformer\u2019s quadratic complexity in the sequence length. However, these more efficient transformers still perform poorly on very long-range sequence tasks (Tay et al., 2021). Gu et al. (2021a) presented an alternative approach using structured state space sequence (S4) layers. An S4 layer defines a nonlinear sequence-to-sequence transformation via a bank of many independent single-input, single-output (SISO) linear state space models (SSMs) (Gu et al., 2021b), coupled together with nonlinear mixing layers. Each SSM leverages the HiPPO framework (Gu et al., 2020a) by initializing with specially constructed state matrices. Since the SSMs are linear, each layer can be equivalently implemented as a convolution, which can then be applied efficiently by parallelizing across the sequence length. Multiple S4 layers can be stacked to create a deep sequence model. Such models have achieved significant improvements over previous methods, including on the long range arena (LRA) (Tay et al., 2021) benchmarks specifically designed to stress test long-range sequence models. Extensions have shown good performance on raw audio generation (Goel et al., 2022) and classification of long movie clips (Islam & Bertasius, 2022). We introduce a new state space layer that builds on the S4 layer, the S5 layer, illustrated in Figure 1. S5 streamlines the S4 layer in two main ways. First, S5 uses one multi-input, multi-output (MIMO) SSM in place of the bank of many independent SISO SSMs in S4. Second, S5 uses an efficient and widely implemented parallel scan. This removes the need for the convolutional and frequency-domain approach used by S4, which requires a non-trivial computation of the convolution kernel. The resulting state space layer has the same computational complexity as S4, but operates purely recurrently and in the time domain. We then establish a mathematical relationship between S4 and S5. This connection allows us to inherit the HiPPO initialization schemes that are key to the success of S4. Unfortunately, the specific HiPPO matrix that S4 uses for initialization cannot be diagonalized in a numerically stable manner for use in S5. However, in line with recent work on the DSS (Gupta et al., 2022) and S4D (Gu et al., 2022) layers, we found that a diagonal approximation to the HiPPO matrix achieves comparable performance. We extend a result from Gu et al. (2022) to the MIMO setting, which justifies the diagonal approximation for use in S5. We leverage the mathematical relationship between S4 and S5 to inform several other aspects of parameterization and initialization, and we perform thorough ablation studies to explore these design choices. The final S5 layer has many desirable properties. It is straightforward to implement (see Appendix A),111The full S5 implementation is available at: https://github.com/lindermanlab/S5. enjoys linear complexity in the sequence length, and can efficiently handle time-varying SSMs and irregularly sampled observations (which is intractable with the convolution implementation of S4). S5 achieves state-of-the-art performance on a variety of long-range sequence modeling tasks, with an LRA average of , and accuracy on the most difficult Path-X task. 2 Background\n\nWe provide the necessary background in this section prior to introducing the S5 layer in Section 3. 2.1 Linear State Space Models\n\nContinuous-time linear SSMs are the core component of both the S4 layer and the S5 layer. Given an input signal , a latent state and an output signal , a linear continuous-time SSM is defined by the differential equation:\n\nd \u200b \ud835\udc31 \u200b ( t ) d \u200b t = \ud835\udc00\ud835\udc31 \u200b ( t ) + \ud835\udc01\ud835\udc2e \u200b ( t ) , \ud835\udc32 \u200b ( t ) = \ud835\udc02\ud835\udc31 \u200b ( t ) + \ud835\udc03\ud835\udc2e \u200b ( t ) , formulae-sequence d \ud835\udc31 \ud835\udc61 d \ud835\udc61 \ud835\udc00\ud835\udc31 \ud835\udc61 \ud835\udc01\ud835\udc2e \ud835\udc61 \ud835\udc32 \ud835\udc61 \ud835\udc02\ud835\udc31 \ud835\udc61 \ud835\udc03\ud835\udc2e \ud835\udc61 \\displaystyle\\dfrac{\\mathrm{d}\\mathbf{x}(t)}{\\mathrm{d}t}=\\mathbf{A}\\mathbf{x}(t)+\\mathbf{B}\\mathbf{u}(t),\\quad\\quad\\mathbf{y}(t)=\\mathbf{C}\\mathbf{x}(t)+\\mathbf{D}\\mathbf{u}(t), (1)\n\nand is parameterized by a state matrix , an input matrix , an output matrix and a feedthrough matrix . For a constant step size, , the SSM can be discretized using, e.g. Euler, bilinear or zero-order hold (ZOH) methods to define the linear recurrence\n\n\ud835\udc31 k = \ud835\udc00 \u00af \u200b \ud835\udc31 k \u2212 1 + \ud835\udc01 \u00af \u200b \ud835\udc2e k , \ud835\udc32 k = \ud835\udc02 \u00af \u200b \ud835\udc31 k + \ud835\udc03 \u00af \u200b \ud835\udc2e k , formulae-sequence subscript \ud835\udc31 \ud835\udc58 \u00af \ud835\udc00 subscript \ud835\udc31 \ud835\udc58 1 \u00af \ud835\udc01 subscript \ud835\udc2e \ud835\udc58 subscript \ud835\udc32 \ud835\udc58 \u00af \ud835\udc02 subscript \ud835\udc31 \ud835\udc58 \u00af \ud835\udc03 subscript \ud835\udc2e \ud835\udc58 \\displaystyle\\mathbf{x}_{k}=\\overline{\\mathbf{A}}\\mathbf{x}_{k-1}+\\overline{\\mathbf{B}}\\mathbf{u}_{k},\\quad\\quad\\mathbf{y}_{k}=\\overline{\\mathbf{C}}\\mathbf{x}_{k}+\\overline{\\mathbf{D}}\\mathbf{u}_{k}, (2)\n\nwhere the discrete-time parameters are each a function, specified by the discretization method, of the continuous-time parameters. See Iserles (2009) for more information on discretization methods. 2.2 Parallelizing Linear State Space Models With Scans\n\nWe use parallel scans to efficiently compute the states of a discretized linear SSM. Given a binary associative operator (i.e. ) and a sequence of elements , the scan operation (sometimes referred to as all-prefix-sum) returns the sequence\n\n[ a 1 , ( a 1 \u2219 a 2 ) , \u2026 , ( a 1 \u2219 a 2 \u2219 \u2026 \u2219 a L ) ] . subscript \ud835\udc4e 1 \u2219 subscript \ud835\udc4e 1 subscript \ud835\udc4e 2 \u2026 \u2219 subscript \ud835\udc4e 1 subscript \ud835\udc4e 2 \u2026 subscript \ud835\udc4e \ud835\udc3f \\displaystyle[a_{1},\\ (a_{1}\\bullet a_{2}),\\ ...,\\ (a_{1}\\bullet a_{2}\\bullet...\\bullet a_{L})]. (3)\n\nComputing a length linear recurrence of a discretized SSM, as in (2), is a specific example of a scan operation. As discussed in Section 1.4 of Blelloch (1990), parallelizing the linear recurrence of the latent transitions in the discretized SSM above can be computed in a parallel time of , assuming processors, where represents the cost of matrix-matrix multiplication. For a general matrix , is . This can be prohibitively expensive in deep learning settings. However, if is a diagonal matrix, the parallel time becomes with processors and only requires space. Finally, we note that efficient parallel scans are implemented in a work-efficient manner, thus the total computational cost of the parallel scan with a diagonal matrix is operations. See Appendix H for more information on parallel scans. 2.3 S4: Structured State Space Sequence Layers\n\nThe S4 layer (Gu et al., 2021a) defines a nonlinear sequence-to-sequence transformation, mapping from an input sequence to an output sequence . An S4 layer contains a bank of independent single-input, single-output (SISO) SSMs with -dimensional states. Each S4 SSM is applied to one dimension of the input sequence. This results in an independent linear transformation from each input channel to each preactivation channel. A nonlinear activation function is then applied to the preactivations. Finally, a position-wise linear mixing layer is applied to combine the independent features and produce the output sequence . Figure 4(a) in the appendix illustrates the view of the S4 layer as a bank of independent SSMs. Figure 2(a) shows an alternative view of S4 as one large SSM with state size and block-diagonal state, input and output matrices. Each S4 SSM leverages the HiPPO framework for online function approximation (Gu et al., 2020a) by initializing the state matrices with a HiPPO matrix (most often the HiPPO-LegS matrix). This was demonstrated empirically to lead to strong performance (Gu et al., 2021b; a), and can be shown as approximating long-range dependencies with respect to an infinitely long, exponentially-decaying measure (Gu et al., 2023). While the HiPPO-LegS matrix is not stably diagonalizable (Gu et al., 2021a), it can be represented as a normal plus low-rank (NPLR) matrix. The normal component, referred to as HiPPO-N and denoted , can be diagonalized. Thus, the HiPPO-LegS can be conjugated into a diagonal plus low-rank (DPLR) form, which S4 then utilizes to derive an efficient form of the convolution kernel. This motivates S4\u2019s DPLR parameterization. Efficiently applying the S4 layer requires two separate implementations depending on context: a recurrent mode and a convolution mode. For online generation, the SSM is iterated recurrently, much like other RNNs. However, when the entire sequence is available and the observations are evenly spaced, a more efficient convolution mode is used. This takes advantage of the ability to represent the linear recurrence as a one-dimensional convolution between the inputs and a convolution kernel for each of the SSMs. Fast Fourier transforms (FFTs) can then be applied to efficiently parallelize this application. Figure 4(a) in the appendix illustrates the convolution approach of the S4 layer for offline processing. We note that while parallel scans could, in principle, allow a recurrent approach to be used in offline scenarios, applying the parallel scan to all of the -dimensional SSMs would in general be much more expensive than the convolution approach S4 actually uses. The trainable parameters of each S4 layer are the independent copies of the learnable SSM parameters and the parameters of the mixing layer. For each of the S4 SSMs, given a scalar input signal , an S4 SSM uses an input matrix , a DPLR parameterized transition matrix , an output matrix , and feedthrough matrix , to produce a signal . To apply the S4 SSMs to discrete sequences, each continuous-time SSM is discretized using a constant timescale parameter . The learnable parameters of each SSM are the timescale parameter , the continuous-time parameters , , , and the DPLR matrix, parameterized by vectors and representing the diagonal matrix and low-rank terms respectively. For notational compactness we denote the concatenation of the S4 SSM states at discrete time index as , and the SSM outputs as . 3 The S5 Layer\n\nIn this section we present the S5 layer. We describe its structure, parameterization and computation, particularly focusing on how each of these differ from S4. 3.1 S5 Structure: From SISO to MIMO\n\nThe S5 layer replaces the bank of SISO SSMs (or large block-diagonal system) in S4 with a multi-input, multi-output (MIMO) SSM, as in (1), with a latent state size , and input and output dimension . The discretized version of this MIMO SSM can be applied to a vector-valued input sequence , to produce a vector-valued sequence of SSM outputs (or preactivations) , using latent states . A nonlinear activation function is then applied to produce a sequence of layer outputs . See Figure 2(b) for an illustration. Unlike S4, we do not require an additional position-wise linear layer, since these features are already mixed. We note here that compared to the latent size of the block-diagonal SSM in the S4 layer, S5\u2019s latent size can be significantly smaller, allowing for the use of efficient parallel scans, as we discuss in Section 3.3. 3.2 S5 Parameterization: Diagonalized Dynamics\n\nThe parameterization of the S5 layer\u2019s MIMO SSM is motivated by the desire to use efficient parallel scans. As discussed in Section 2.2, a diagonal state matrix is required to efficiently compute the linear recurrence using a parallel scan. Thus, we diagonalize the system, writing the continuous-time state matrix as , where denotes the diagonal matrix containing the eigenvalues and corresponds to the eigenvectors. Therefore, we can diagonalize the continuous-time latent dynamics from (1) as\n\nd \u200b \ud835\udc15 \u2212 1 \u200b \ud835\udc31 \u200b ( t ) d \u200b t = \ud835\udeb2 \u200b \ud835\udc15 \u2212 1 \u200b \ud835\udc31 \u200b ( t ) + \ud835\udc15 \u2212 1 \u200b \ud835\udc01\ud835\udc2e \u200b ( t ) . d superscript \ud835\udc15 1 \ud835\udc31 \ud835\udc61 d \ud835\udc61 \ud835\udeb2 superscript \ud835\udc15 1 \ud835\udc31 \ud835\udc61 superscript \ud835\udc15 1 \ud835\udc01\ud835\udc2e \ud835\udc61 \\displaystyle\\dfrac{\\mathrm{d}\\mathbf{V}^{-1}\\mathbf{x}(t)}{\\mathrm{d}t}=\\boldsymbol{\\Lambda}\\mathbf{V}^{-1}\\mathbf{x}(t)+\\mathbf{V}^{-1}\\mathbf{B}\\mathbf{u}(t). (4)\n\nDefining , , and gives a reparameterized system,\n\nd \u200b \ud835\udc31 ~ \u200b ( t ) d \u200b t = \ud835\udeb2 \u200b \ud835\udc31 ~ \u200b ( t ) + \ud835\udc01 ~ \u200b \ud835\udc2e \u200b ( t ) , \ud835\udc32 \u200b ( t ) d ~ \ud835\udc31 \ud835\udc61 d \ud835\udc61 \ud835\udeb2 ~ \ud835\udc31 \ud835\udc61 ~ \ud835\udc01 \ud835\udc2e \ud835\udc61 \ud835\udc32 \ud835\udc61 \\displaystyle\\dfrac{\\mathrm{d}\\tilde{\\mathbf{x}}(t)}{\\mathrm{d}t}=\\mathbf{\\Lambda}\\tilde{\\mathbf{x}}(t)+\\tilde{\\mathbf{B}}\\mathbf{u}(t),\\quad\\quad\\mathbf{y}(t) = \ud835\udc02 ~ \u200b \ud835\udc31 ~ \u200b ( t ) + \ud835\udc03\ud835\udc2e \u200b ( t ) . absent ~ \ud835\udc02 ~ \ud835\udc31 \ud835\udc61 \ud835\udc03\ud835\udc2e \ud835\udc61 \\displaystyle=\\tilde{\\mathbf{C}}\\tilde{\\mathbf{x}}(t)+\\mathbf{D}\\mathbf{u}(t). (5)\n\nThis is a linear SSM with a diagonal state matrix. This diagonalized system can be discretized with a timescale parameter using the ZOH method to give another diagonalized system with parameters\n\n\ud835\udeb2 \u00af = e \ud835\udeb2 \u200b \u0394 , \ud835\udc01 \u00af = \ud835\udeb2 \u2212 1 \u200b ( \ud835\udeb2 \u00af \u2212 \ud835\udc08 ) \u200b \ud835\udc01 ~ , \ud835\udc02 \u00af = \ud835\udc02 ~ , \ud835\udc03 \u00af = \ud835\udc03 . formulae-sequence \u00af \ud835\udeb2 superscript \ud835\udc52 \ud835\udeb2 \u0394 formulae-sequence \u00af \ud835\udc01 superscript \ud835\udeb2 1 \u00af \ud835\udeb2 \ud835\udc08 ~ \ud835\udc01 formulae-sequence \u00af \ud835\udc02 ~ \ud835\udc02 \u00af \ud835\udc03 \ud835\udc03 \\displaystyle\\overline{\\mathbf{\\Lambda}}=e^{\\mathbf{\\Lambda}\\Delta},\\quad\\overline{\\mathbf{B}}=\\mathbf{\\Lambda}^{-1}(\\overline{\\mathbf{\\Lambda}}-\\mathbf{I})\\tilde{\\mathbf{B}},\\quad\\overline{\\mathbf{C}}=\\tilde{\\mathbf{C}},\\quad\\overline{\\mathbf{D}}=\\mathbf{D}. (6)\n\nIn practice, we use a vector of learnable timescale parameters (see Section 4.3) and restrict the feedthrough matrix to be diagonal. The S5 layer therefore has the learnable parameters: , , , , and . Initialization\n\nPrior work showed that the performance of deep state space models are sensitive to the initialization of the state matrix (Gu et al., 2021b; a). We discussed in Section 2.2 that state matrices must be diagonal for efficient application of parallel scans. We also discussed in Section 2.3 that the HiPPO-LegS matrix cannot be diagonalized stably, but that the HiPPO-N matrix can be. In Section 4 we connect the dynamics of S5 to S4 to suggest why initializing with HiPPO-like matrices may also work well in the MIMO setting. We support this empirically, finding that diagonalizing the HiPPO-N matrix leads to good performance, and perform ablations in Appendix E to compare to other initializations. We note that DSS (Gupta et al., 2022) and S4D (Gu et al., 2022) layers also found strong performance in the SISO setting by using a diagonalization of the HiPPO-N matrix. Conjugate Symmetry\n\nThe complex eigenvalues of a diagonalizable matrix with real entries always occur in conjugate pairs. We enforce this conjugate symmetry by using half the number of eigenvalues and latent states. This ensures real outputs and reduces the runtime and memory usage of the parallel scan by a factor of two.",
    "s5-3": "This idea is also discussed in Gu et al. (2022). 3.3 S5 Computation: Fully Recurrent\n\nCompared to the large effective latent size of the block-diagonal S4 layer, the smaller latent dimension of the S5 layer () allows the use of efficient parallel scans when the entire sequence is available. The S5 layer can therefore be efficiently used as a recurrence in the time domain for both online generation and offline processing. Parallel scans and the continuous-time parameterization also allow for efficient handling of irregularly sampled time series and other time-varying SSMs, by simply supplying a different matrix at each step. We leverage this feature and apply S5 to irregularly sampled data in Section 6.3. In contrast, the convolution of the S4 layer requires a time invariant system and regularly spaced observations. 3.4 Matching the Computational Efficiency of S4 and S5\n\nA key design desiderata for S5 was matching the computational complexity of S4 for both online generation and offline recurrence. The following proposition guarantees that their complexities are of the same order if S5\u2019s latent size . Proposition 1. Given an S4 layer with input/output features, an S5 layer with input/output features and a latent size has the same order of magnitude complexity as an S4 layer in terms of both runtime and memory usage.",
    "s5-4": "Proof. See Appendix C.1. \u220e\n\nWe also support this proposition with empirical comparisons in Appendix C.2. 4 Relationship Between S4 and S5\n\nWe now establish a relationship between the dynamics of S5 and S4. In Section 4.1 we show that, under certain conditions, the outputs of the S5 SSM can be interpreted as a projection of the latent states computed by a particular S4 system. This interpretation motivates using HiPPO initializations for S5, which we discuss in more detail in Section 4.2. In Section 4.3 we discuss how the conditions required to relate the dynamics further motivate initialization and parameterization choices. 4.1 Different Output Projections of Equivalent Dynamics\n\nWe compare the dynamics of S4 and S5 under some simplifying assumptions:\n\nAssumption 1. We consider only -dimensional to -dimensional sequence maps. Assumption 2. We assume the state matrix of each S4 SSM is identical, . Assumption 3. We assume the timescales of each S4 SSM are identical,\n\nAssumption 4. We assume that the same state matrix is used in S5 as in S4 (also cf. Assumption 2). Note this also specifies the S5 latent size . We also assume the S5 input matrix is the horizontal concatenation of the column input vectors used by S4: . We will discuss relaxing these assumptions shortly, but under these conditions it is straightforward to derive a relationship between the dynamics of S4 and S5:\n\nProposition 2. Consider an S5 layer, with state matrix , input matrix and some output matrix (cf. Assumption 1); and an S4 layer, where each of the S4 SSMs has state matrix (cf. Assumption 2, 4) and input vector (cf. Assumption 4). If the S4 and S5 layers are discretized with the same timescales (cf. Assumption 3), then the S5 SSM produces outputs, , equivalent to a linear combination of the latent states of the S4 SSMs, , where . Proof. See Appendix D.2. \u220e\n\nImportantly, the S5 SSM outputs are not equal to the outputs of the block-diagonal S4 SSM. Instead they are equivalent to the outputs of the block-diagonal S4 SSM with modified output matrix . Under the assumptions, however, the underlying state dynamics are equivalent. Recalling that initializing the S4 dynamics with HiPPO was key to performance (Gu et al., 2021a), the relationship established in Proposition 2 motivates using HiPPO initializations for S5, as we now discuss. 4.2 Diagonalizable Initialization\n\nIdeally, given the interpretation above, we would initialize S5 with the exact HiPPO-LegS matrix. Unfortunately, as discussed in Section 2.3, this matrix is not stably diagonalizable, as is required for the efficient parallel scans used for S5.",
    "s5-5": "However, Gupta et al. (2022) and Gu et al. (2022) showed empirically that removing the low rank terms and initializing with the diagonalized HiPPO-N matrix still performed well. Gu et al. (2022) offered a theoretical justification for the use of this normal approximation for single-input systems: in the limit of infinite state dimension, the linear ODE with HiPPO-N state matrix produces the same dynamics as an ODE with the HiPPO-LegS matrix. Using linearity, it is straightforward to extend this result to the multi-input system that S5 uses:\n\nCorollary 1 (Extension of Theorem 3 in Gu et al.",
    "s5-6": "(2022)). Consider , , as defined in Appendix B.1.1. Given vector-valued inputs , the ordinary differential equation converges to as . We include a simple proof of this extension in Appendix D.3. This extension motivates the use of HiPPO-N to initialize S5\u2019s MIMO SSM. Note that S4D (the diagonal extension of S4) uses the same HiPPO-N matrix. Thus, when under the assumptions in Proposition 2, an S5 SSM in fact produces outputs that are equivalent to a linear combination of the latent states produced by S4D\u2019s SSMs. Our empirical results in Section 6 suggest that S5 initialized with the HiPPO-N matrix performs just as well as S4 initialized with the HiPPO-LegS matrix. 4.3 Relaxing the Assumptions\n\nWe now revisit the assumptions required for Proposition 2, since they only relate a constrained version of S5 to a constrained version of S4. Regarding Assumption 2, Gu et al. (2021a) report that S4 models with tied state matrices can still perform well, though allowing different state matrices often yields higher performance. Likewise, requiring a single scalar timescale across all of the S4 SSMs, per Assumption 3, is restrictive. S4 typically learns different timescale parameters for each SSM (Gu et al., 2023) to capture different timescales in the data. To relax these assumptions, note that Assumption 4 constrains S5 to have dimension , and is typically much smaller than the dimensionality of the inputs, . Proposition 1 established that S5 can match S4\u2019s complexity with . By allowing for larger latent state sizes, Assumptions 2 and 3 can be relaxed, as discussed in Appendix D.4. We also discuss how this relaxation motivates a block-diagonal initialization with HiPPO-N matrices on the diagonal. Finally, to further relax the tied timescale assumptions, we note that in practice, we find improved performance by learning different timescales (one per state). See Appendix D.5 for further discussion of this empirical finding and the ablations in Appendix E.1. 5 Related work\n\nS5 is most directly related to S4 and its other extensions, which we have discussed thoroughly. However, there is prior literature that uses similar ideas to those developed here. For example, prior work studied approximating nonlinear RNNs with stacks of linear RNNs connected by nonlinear layers, while also using parallel scans (Martin & Cundy, 2018). Martin & Cundy (2018) showed that several efficient RNNs, such as QRNNs (Bradbury et al., 2017) and SRUs (Lei et al., 2018), fall into a class of linear surrogate RNNs that can leverage parallel scans. Kaul (2020) also used parallel scans for an approach that approximates RNNs with stacks of discrete-time single-input, multi-output (SIMO) SSMs. However, S4 and S5 are the only methods to significantly outperform other comparable state-of-the-art nonlinear RNNs, transformers and convolution approaches. Our ablation study in Appendix E.2 suggests that this performance gain over prior attempts at parallelized linear RNNs is likely due to the continuous-time parameterization and the HiPPO initialization. 6 Experiments\n\nWe now compare empirically the performance of the S5 layer to the S4 layer and other baseline methods. We use the S5 layer as a drop-in replacement for the S4 layer. The architecture consists of a linear input encoder, stacks of S5 layers, and a linear output decoder (Gu et al., 2021a). For all experiments we choose the S5 dimensions to ensure similar computational complexities as S4, following the conditions discussed in Section 3.3, as well as comparable parameter counts. The results we present show that the S5 layer matches the performance and efficiency of the S4 layer. We include in the appendix further ablations, baselines and runtime comparisons. 6.1 Long Range Arena\n\nThe long range arena (LRA) benchmark (Tay et al., 2021) is a suite of six sequence modeling tasks, with sequence lengths from 1,024 to over 16,000. The suite was specifically developed to benchmark the performance of architectures on long-range modeling tasks (see Appendix G for more details). Table 1 presents S5\u2019s LRA performance in comparison to other methods. S5 achieves the highest average score among methods that have linear complexity in sequence length (most notably S4, S4D, and the concurrent works: Liquid-S4 (Hasani et al., 2023) and Mega-chunk (Ma et al., 2023)). Most significantly, S5 achieves the highest score among all models (including Mega (Ma et al., 2023)) on the Path-X task, which has by far the longest sequence length of the tasks in the benchmark. 6.2 Raw Speech Classification\n\nThe Speech Commands dataset (Warden, 2018) contains high-fidelity sound recordings of different human readers reciting a word from a vocabulary of 35 words. The task is to classify which word was spoken. We show in Table 2 that S5 outperforms the baselines, outperforms previous S4 methods and performs similarly to the concurrent Liquid-S4 method (Hasani et al., 2023). As S4 and S5 methods are parameterized in continuous-time, these models can be applied to datasets with different sampling rates without the need for re-training, simply by globally re-scaling the timescale parameter by the ratio between the new and old sampling rates. The result of applying the best S5 model trained on 16kHz data, to the speech data sampled (via decimation) at 8kHz, without any additional fine-tuning, is also presented in Table 2. S5 also improves this metric over the baseline methods. 6.3 Variable Observation Interval\n\nThe final application we study here highlights how S5 can naturally handle observations received at irregular intervals. S5 does so by supplying a different value to the discretization at each step. We use the pendulum regression example presented by Becker et al. (2019) and Schirmer et al. (2022), illustrated in Figure 3. The input sequence is a sequence of images, each pixels in size, that has been corrupted with a correlated noise process and sampled at irregular intervals from a continuous trajectory of duration . The targets are the sine and cosine of the angle of the pendulum, which follows a nonlinear dynamical system. The velocity is unobserved. We match the architecture, parameter count and training procedure of Schirmer et al. (2022). Table LABEL:tab:results:pendulum_mse summarizes the results of this experiment. S5 outperforms CRU on the regression task, recovering a lower mean error. Furthermore, S5 is markedly faster than CRU on the same hardware. 6.4 Pixel-level 1-D Image Classification\n\nTable 10 in Appendix F.4 shows results of S5 on other common benchmarks including sequential MNIST, permuted sequential MNIST and sequential CIFAR (color). We see that S5 broadly matches the performance of S4, and outperforms a range of state-of-the-art RNN-based methods. 7 Conclusion\n\nWe introduce the S5 layer for long-range sequence modeling. The S5 layer modifies the internal structure of the S4 layer, and replaces the frequency-domain approach used by S4 with a purely recurrent, time-domain approach leveraging parallel scans. S5 achieves high performance while retaining the computational efficiency of S4. S5 also provides further opportunities. For instance, unlike the convolutional S4 methods, the parallel scan unlocks the ability to efficiently and easily process time-varying SSMs whose parameters can vary with time. Section 6.3 illustrated an example of this for sequences sampled at variable sampling rates. The concurrently developed method, Liquid-S4 (Hasani et al., 2023), uses an input-dependent bilinear dynamical system and highlights further opportunities for time-varying SSMs. The more general MIMO SSM design will also enable connections to be made with classical probabilistic state space modeling as well as more recent work on parallelizing filtering and smoothing operations (S\u00e4rkk\u00e4 & Garc\u00eda-Fern\u00e1ndez, 2020). More broadly, we hope the simplicity and generality of the S5 layer can expand the use of state space layers in deep sequence modeling and lead to new formulations and extensions. Acknowledgements and Disclosure of Funding\n\nWe thank Albert Gu for his thorough and insightful feedback. We also acknowledge The Annotated S4 Blog (Rush & Karamcheti, 2022) which inspired our JAX implementation. This work was supported by grants from the Simons Collaboration on the Global Brain (SCGB 697092), the NIH BRAIN Initiative (U19NS113201 and R01NS113119), and the Sloan Foundation. Some of the computation for this work was made possible by Stanford Data Science Microsoft Education Azure cloud credits. References\n\nArjovsky et al. (2016) Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pp. 1120\u20131128. PMLR, 2016. Bai et al. (2018) Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. Bai et al. (2019) Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In International Conference on Learning Representations, 2019. Becker et al. (2019) Philipp Becker, Harit Pandya, Gregor Gebhardt, Cheng Zhao, C James Taylor, and Gerhard Neumann. Recurrent kalman networks: Factorized inference in high-dimensional deep feature spaces.",
    "s5-7": "In International Conference on Machine Learning, pp. 544\u2013552. PMLR, 2019. Beltagy et al. (2020) Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Blelloch (1990) Guy Blelloch. Prefix sums and their applications.",
    "s5-8": "Technical report, Tech. rept. CMU-CS-90-190. School of Computer Science, Carnegie Mellon, 1990. Bradbury et al. (2017) James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2017. Chang et al. (2019) Bo Chang, Minmin Chen, Eldad Haber, and Ed Chi. AntisymmetricRNN: A dynamical system view on recurrent neural networks.",
    "s5-9": "In International Conference on Learning Representations, 2019. Chang et al. (2017) Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks. Advances in Neural Information Processing Systems, 30, 2017. Chen et al. (2018) Ricky Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. Chilkuri & Eliasmith (2021) Narsimha Reddy Chilkuri and Chris Eliasmith. Parallelizing Legendre memory unit training.",
    "s5-10": "In International Conference on Machine Learning, pp. 1898\u20131907. PMLR, 2021. Cho et al. (2014) Kyunghyun Cho, Bart van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder\u2013decoder approaches. Syntax, Semantics and Structure in Statistical Translation, pp. 103, 2014. Choromanski et al. (2021) Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. Dauphin et al. (2017) Yann Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International Conference on Machine Learning, pp. 933\u2013941. PMLR, 2017. De Brouwer et al. (2019) Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series.",
    "s5-11": "Advances in neural information processing systems, 32, 2019. Erichson et al. (2021) N. Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021. Goel et al. (2022) Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It\u2019s raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 7616\u20137633. PMLR, 17\u201323 Jul 2022. Gu et al. (2020a) Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent memory with optimal polynomial projections.",
    "s5-12": "Advances in Neural Information Processing Systems, 33:1474\u20131487, 2020a. Gu et al. (2020b) Albert Gu, Caglar Gulcehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pp. 3800\u20133809. PMLR, 2020b. Gu et al. (2021a) Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021a. Gu et al. (2021b) Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021b. Gu et al. (2022) Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. Gu et al. (2023) Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your HIPPO: State space models with generalized orthogonal basis projections.",
    "s5-13": "In International Conference on Learning Representations, 2023. Gupta & Berant (2020) Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for transformers, 2020.",
    "s5-14": "Gupta et al. (2022) Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces.",
    "s5-15": "In Advances in Neural Information Processing Systems, 2022. Hasani et al. (2023) Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. Hochreiter & Schmidhuber (1997) Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780, 1997. Iserles (2009) Arieh Iserles. A first course in the numerical analysis of differential equations.",
    "s5-16": "44. Cambridge university press, 2009. Islam & Bertasius (2022) Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXV, pp. 87\u2013104, 2022. Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156\u20135165. PMLR, 2020. Kaul (2020) Shiva Kaul. Linear dynamical systems as a core computational primitive. Advances in Neural Information Processing Systems, 33:16808\u201316820, 2020. Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. Master\u2019s thesis, University of Toronto, 2009. Ladner & Fischer (1980) Richard Ladner and Michael Fischer. Parallel prefix computation. Journal of the ACM (JACM), 27(4):831\u2013838, 1980. Lakshmivarahan & Dhall (1994) Sivaramakrishnan Lakshmivarahan and Sudarshan Dhall. Parallel computing using the prefix problem.",
    "s5-17": "Oxford University Press, 1994.",
    "s5-18": "Lee-Thorp et al. (2022) James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4296\u20134313, 2022. Lei et al. (2018) Tao Lei, Yu Zhang, Sida Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly parallelizable recurrence.",
    "s5-19": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4470\u20134481, 2018. Lezcano-Casado & Mart\u0131nez-Rubio (2019) Mario Lezcano-Casado and David Mart\u0131nez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group.",
    "s5-20": "In International Conference on Machine Learning, pp. 3794\u20133803. PMLR, 2019. Li et al. (2018) Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network (INDRNN): Building a longer and deeper RNN.",
    "s5-21": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5457\u20135466, 2018. Linsley et al. (2018) Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning long-range spatial dependencies with horizontal gated recurrent units.",
    "s5-22": "Advances in Neural Information Processing Systems, 31, 2018. Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Ma et al. (2021) Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention.",
    "s5-23": "Advances in Neural Information Processing Systems, 34, 2021. Ma et al. (2023) Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In International Conference on Learning Representations, 2023. Maas et al. (2011) Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, and Christopher Potts. Learning word vectors for sentiment analysis.",
    "s5-24": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142\u2013150, 2011. Martin & Cundy (2018) Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length.",
    "s5-25": "In International Conference on Learning Representations, 2018. Nangia & Bowman (2018) Nikita Nangia and Samuel Bowman. ListOps: A diagnostic dataset for latent tree learning.",
    "s5-26": "NAACL HLT 2018, pp. 92, 2018. Nonaka & Seita (2021) Naoki Nonaka and Jun Seita. In-depth benchmarking of deep neural network architectures for ecg diagnosis. In Machine Learning for Healthcare Conference, pp. 414\u2013439. PMLR, 2021. Oord et al. (2016) Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.",
    "s5-27": "arXiv preprint arXiv:1609.03499, 2016. Radev et al. (2009) Dragomir Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. The ACL anthology network corpus. ACL-IJCNLP 2009, pp. 54, 2009. Romero et al. (2021) David Romero, Robert-Jan Bruintjes, Jakub Mikolaj Tomczak, Erik Bekkers, Mark Hoogendoorn, and Jan van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes.",
    "s5-28": "In International Conference on Learning Representations, 2021. Romero et al. (2022a) David Romero, David Knigge, Albert Gu, Erik Bekkers, Efstratios Gavves, Jakub Tomczak, and Mark Hoogendoorn. Towards a general purpose CNN for long range dependencies in . arXiv preprint arXiv:2206.03398, 2022a. Romero et al. (2022b) David Romero, Anna Kuzina, Erik Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogendoorn. CKConv: Continuous kernel convolution for sequential data. In International Conference on Learning Representations, 2022b. Rubanova et al. (2019) Yulia Rubanova, Ricky Chen, and David Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. Advances in Neural Information Processing Systems, 32, 2019. Rusch & Mishra (2021) T. Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long time dependencies. In International Conference on Machine Learning, pp. 9168\u20139178. PMLR, 2021. Rush & Karamcheti (2022) Sasha Rush and Sidd Karamcheti. The Annotated S4. In Blog Track at ICLR 2022, 2022. URL https://srush.github.io/annotated-s4/. S\u00e4rkk\u00e4 & Garc\u00eda-Fern\u00e1ndez (2020) Simo S\u00e4rkk\u00e4 and \u00c1ngel F Garc\u00eda-Fern\u00e1ndez. Temporal parallelization of Bayesian smoothers. IEEE Transactions on Automatic Control, 66(1):299\u2013306, 2020. Schirmer et al. (2022) Mona Schirmer, Mazin Eltayeb, Stefan Lessmann, and Maja Rudolph. Modeling irregular time series with continuous recurrent units. In International Conference on Machine Learning, pp. 19388\u201319405. PMLR, 2022. Shukla & Marlin (2021) Satya Narayan Shukla and Benjamin Marlin. Multi-time attention networks for irregularly sampled time series.",
    "s5-29": "In International Conference on Learning Representations, 2021. Tanaka (2020) Masayuki Tanaka. Weighted sigmoid gate unit for an activation function of deep neural network. Pattern Recognition Letters, 135:354\u2013359, 2020. Tay et al. (2021) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient transformers.",
    "s5-30": "In International Conference on Learning Representations, 2021. Trinh et al. (2018) Trieu Trinh, Andrew Dai, Thang Luong, and Quoc Le. Learning longer-term dependencies in RNNs with auxiliary losses. In International Conference on Machine Learning, pp. 4965\u20134974. PMLR, 2018. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. Voelker et al. (2019) Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre Memory Units: Continuous-time representation in recurrent neural networks.",
    "s5-31": "Advances in Neural Information Processing Systems, 32, 2019. Wang et al. (2020) Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.",
    "s5-32": "arXiv preprint arXiv:2006.04768, 2020. Warden (2018) Pete Warden. Speech Commands: A dataset for limited-vocabulary speech recognition.",
    "s5-33": "arXiv preprint arXiv:1804.03209, 2018. Xiong et al. (2021) Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A Nystr\u00f6m-based algorithm for approximating self-attention.",
    "s5-34": "In Proceedings of the AAAI Conference on Artificial Intelligence., volume 35, pp.",
    "s5-35": "14138. NIH Public Access, 2021. Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283\u201317297, 2020. Zhu & Soricut (2021) Zhenhai Zhu and Radu Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for sequences.",
    "s5-36": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3801\u20133815, 2021. Appendix for: Simplified State Space Layers for Sequence Modeling\n\nContents:\n\n\u2022\n\nAppendix A: JAX Implementation of S5 Layer. \u2022\n\nAppendix B: S5 Layer Details. \u2022\n\nAppendix C: Computational Efficiency of S5. \u2022\n\nAppendix D: Relationship Between S4 and S5. \u2022\n\nAppendix E: Ablations. \u2022\n\nAppendix F: Supplementary Results. \u2022\n\nAppendix G: Experiment Configurations. \u2022\n\nAppendix H: Background on Parallel Scans for Linear Recurrences. Appendix A JAX Implementation of S5 Layer\n\nAppendix B S5 Layer Details\n\nB.1 Initialization Details\n\nB.1.1 Initialization of the State Matrix\n\nHere we provide additional details to supplement the discussion of initialization in Section 3.2.",
    "s5-37": "Gu et al. (2023) explains the ability of S4 to capture long-range dependencies when using the HiPPO-LegS matrix via decomposing the input with respect to an infinitely long, exponentially decaying measure. The HiPPO-LegS matrix and corresponding SISO input vector are defined as\n\n( \ud835\udc00 LegS ) n \u200b k subscript subscript \ud835\udc00 LegS \ud835\udc5b \ud835\udc58 \\displaystyle\\left(\\mathbf{A_{\\mathrm{LegS}}}\\right)_{nk} = \u2212 { ( 2 \u200b n + 1 ) 1 / 2 \u200b ( 2 \u200b k + 1 ) 1 / 2 , n > k n + 1 , n = k 0 , n < k . absent cases superscript 2 \ud835\udc5b 1 1 2 superscript 2 \ud835\udc58 1 1 2 \ud835\udc5b \ud835\udc58 \ud835\udc5b 1 \ud835\udc5b \ud835\udc58 0 \ud835\udc5b \ud835\udc58 \\displaystyle=-\\begin{cases}(2n+1)^{1/2}(2k+1)^{1/2},&n>k\\\\\nn+1,&n=k\\\\\n0,&n<k\\end{cases}. (7) ( \ud835\udc1b LegS ) n subscript subscript \ud835\udc1b LegS \ud835\udc5b \\displaystyle\\left(\\mathbf{b_{\\mathrm{LegS}}}\\right)_{n} = ( 2 \u200b n + 1 ) 1 2 . absent superscript 2 \ud835\udc5b 1 1 2 \\displaystyle=(2n+1)^{\\frac{1}{2}}. (8)\n\nNote that in Section 4.2, the input matrix used in Corollary 1 is formed by concatenating copies of . Theorem 1 of Gu et al. (2021a) then shows that the HiPPO matrices in Gu et al. (2020a), can be represented with a normal plus low-rank (NPLR) form consisting of a normal matrix, , and a low-rank term\n\n\ud835\udc00 HiPPO = \ud835\udc00 HiPPO Normal \u2212 \ud835\udc0f\ud835\udc10 \u22a4 = \ud835\udc15 \u200b ( \ud835\udeb2 \u2212 ( \ud835\udc15 \u2217 \u200b \ud835\udc0f ) \u200b ( \ud835\udc15 \u2217 \u200b \ud835\udc10 ) \u2217 ) \u200b \ud835\udc15 \u2217 subscript \ud835\udc00 HiPPO superscript subscript \ud835\udc00 HiPPO Normal superscript \ud835\udc0f\ud835\udc10 top \ud835\udc15 \ud835\udeb2 superscript \ud835\udc15 \ud835\udc0f superscript superscript \ud835\udc15 \ud835\udc10 superscript \ud835\udc15 \\displaystyle\\mathbf{A_{\\mathrm{HiPPO}}}=\\mathbf{A}_{\\mathrm{HiPPO}}^{\\mathrm{Normal}}-\\mathbf{P}\\mathbf{Q}^{\\top}=\\mathbf{V}\\left(\\mathbf{\\Lambda}-(\\mathbf{V}^{*}\\mathbf{P})(\\mathbf{V}^{*}\\mathbf{Q})^{*}\\right)\\mathbf{V}^{*} (9)\n\nfor unitary , diagonal , and low-rank factorization . The right hand side of this equation shows HiPPO matrices can be conjugated into a diagonal plus low-rank (DPLR) form. The HiPPO-LegS matrix can therefore be written in terms of the normal HiPPO-N matrix and low-rank term (Goel et al., 2022) as\n\n\ud835\udc00 LegS subscript \ud835\udc00 LegS \\displaystyle\\mathbf{A_{\\mathrm{LegS}}} = \ud835\udc00 LegS Normal \u2212 \ud835\udc0f Legs \u200b \ud835\udc0f Legs \u22a4 absent superscript subscript \ud835\udc00 LegS Normal subscript \ud835\udc0f Legs superscript subscript \ud835\udc0f Legs top \\displaystyle=\\mathbf{A}_{\\mathrm{LegS}}^{\\mathrm{Normal}}-\\mathbf{P}_{\\mathrm{Legs}}\\mathbf{P}_{\\mathrm{Legs}}^{\\top} (10)\n\nwhere\n\n\ud835\udc00 LegS n \u200b k Normal superscript subscript \ud835\udc00 subscript LegS \ud835\udc5b \ud835\udc58 Normal \\displaystyle\\mathbf{A}_{\\mathrm{LegS}_{nk}}^{\\mathrm{Normal}} = \u2212 { ( n + 1 2 ) 1 / 2 \u200b ( k + 1 2 ) 1 / 2 , n > k 1 2 , n = k ( n + 1 2 ) 1 / 2 \u200b ( k + 1 2 ) 1 / 2 , n < k . absent cases superscript \ud835\udc5b 1 2 1 2 superscript \ud835\udc58 1 2 1 2 \ud835\udc5b \ud835\udc58 1 2 \ud835\udc5b \ud835\udc58 superscript \ud835\udc5b 1 2 1 2 superscript \ud835\udc58 1 2 1 2 \ud835\udc5b \ud835\udc58 \\displaystyle=-\\begin{cases}(n+\\frac{1}{2})^{1/2}(k+\\frac{1}{2})^{1/2},&n>k\\\\\n\\frac{1}{2},&n=k\\\\\n(n+\\frac{1}{2})^{1/2}(k+\\frac{1}{2})^{1/2},&n<k\\end{cases}. (11) \ud835\udc0f Legs n subscript subscript \ud835\udc0f Legs \ud835\udc5b \\displaystyle\\mathbf{P_{\\mathrm{Legs}}}_{n} = ( n + 1 2 ) 1 2 absent superscript \ud835\udc5b 1 2 1 2 \\displaystyle=(n+\\frac{1}{2})^{\\frac{1}{2}} (12)\n\nOur default is to set the S5 layer state matrix , and take the eigendecomposition of this matrix to recover the initial . We often find it beneficial to also use and to initialize and , as described below. As mentioned in Section 4.3, we also found that performance on many tasks benefited from initializing the S5 state matrix as block-diagonal, with each block on the diagonal equal to , where here is less than the state dimension , e.g. when 4 blocks are used on the diagonal. We then take the eigendecomposition of this matrix to initialize , as well as and . We note that even in this case, and are still initialized in dense form and there is no constraint that requires to remain block-diagonal during learning. In the hyperparameter table in Appendix G, the hyperparameter indicates the number of these HiPPO-N blocks used on the diagonal for initialization, where indicates we used the default case of initializing with a single HiPPO-N matrix. We discuss the motivation for this block-diagonal initialization further in Appendix D.4. B.1.2 Initialization of Input, Output and Feed-through Matrices\n\nIn general, we explicitly initialize the input matrix and output matrix using the eigenvectors from the diagonalization of the initial state matrix. Specifically, we sample and and then initialize the (complex) learnable parameters as and . We initialize by independently sampling each element from a standard normal distribution. B.1.3 Initialization of the Timescales\n\nPrior work (Gupta et al., 2022; Gu et al., 2023) found the initialization of this timescale parameter to be important.",
    "s5-38": "This is studied in detail in Gu et al. (2023). We sample these parameters in line with S4 and sample each element of from a uniform distribution on the interval , where the default range is and . The only exception is the Path-X experiment, where we initialize from and to account for the longer timescales as discussed in Gu et al. (2023). B.2 Comparison of S4 and S5 Computational Elements\n\nIn Figure 4 we illustrate a comparison of the computational details of the S4 and S5 layers for efficient, parallelized offline processing. Appendix C Computational Efficiency of S5\n\nC.1 Theoretical Computational Efficiency\n\nProposition 1. Given an S4 layer with input/output features, an S5 layer with input/output features and a latent size has the same order of magnitude complexity as an S4 layer in terms of both runtime and memory usage. Proof. We first consider the case where the entire sequence is available and compare the S4 layer\u2019s convolution mode to the S5 layer\u2019s use of a parallel scan. We then consider the online generation case where each method operates recurrently. Parallelized offline processing\n\nWe consider the application of both the S4 and S5 layer to a vector-valued sequence . Note that there are multiple ways to compute the convolution kernel for S4/S4D and the exact computational complexity depends on the implementation (Gu et al., 2021a; 2022). However, the overall complexity of computing the S4 SSM outputs given the inputs is lower bounded by the FFTs used to convert into the frequency domain to apply the convolutions. Therefore, we can lower bound the cost of applying a single SISO S4 SSM to a scalar input sequence as operations and space. The S4 layer then consists of different S4 SSMs, and therefore requires operations and space. Finally, mixing the activations at each timestep requires independent matrix-vector multiplications. Thus, the S4 layer sequence-to-sequence transformation requires a total of operations when run in the efficient convolution mode. Given processors, these operations can be parallelized to a minimum of parallel time. We now consider the S5 layer. As discussed in Section 2.2, the parallel scan requires operations and space to perform the linear recurrence that computes the discretized states . In addition, operations are required for the independent matrix-vector multiplications that compute and the SSM outputs . Therefore, the S5 layer requires operations. Given processors, these operations can be parallelized to a minimum of parallel time. Thus, we see that when the S5 state dimension , the S5 layer requires operations compared to the S4 layer\u2019s operations. Crucially, when , S4 and S5 each have parallel complexity of (when processors are available). In addition, when , the space complexity for the parallel scan is which matches the space complexity of S4\u2019s FFTs. Both methods then also perform identical broadcasted matrix vector multiplications, and hence have the same space complexity. Online generation\n\nFor online generation, both the S4 and S5 layers are run recurrently. The S4 layer requires operations per step due to its DPLR-matrix-vector multiplication (Gu et al., 2021a) and the matrix-vector multiplication of its mixing layer. The S5 layer requires operations per step due to its matrix-vector multiplication with its diagonal matrix and its matrix-vector multiplications to compute and . Thus, we see the two approaches have the same per step complexity of when and the individual S4 SSM state sizes are . Thus, S4 and S5 have the same order computational complexity and memory requirements in both cases. \u220e\n\nC.2 Empirical Runtime Comparison\n\nTable 4 provides an empirical evaluation of the runtime performance, in terms of speed and memory, between S4, S4D and S5 across a range of sequence lengths from the LRA tasks. We compared the JAX implementation of S5 to a JAX implementation of S4 and S4D, based on the JAX implementation from Rush & Karamcheti (2022). For a fair comparison, we modified these existing JAX implementations of S4 and S4D to allow them both to enforce conjugate symmetry and use bidirectionality. For each task, models use bidirectionality and conjugate symmetry as reported in Gu et al. (2022). All models, except for the italicised S5 row, use the same input/output features and number of layers as reported in Gu et al. (2022). The S4 and S4D layers also use the same S4 SSM latent size as reported in Gu et al. (2022). All methods used the same batch size and all comparisons were made using a 16GB NVIDIA V100 GPU. Note we observed the JAX S4D implementation to in general be faster than the JAX S4 implementation (possibly due to this specific S4 implementation\u2019s use of the naive Cauchy kernel computation (Gu et al., 2021a)). For this reason, we consider S4D as the baseline. We consider three configurations of S5 for comparison. The first two configurations, corresponding to lines 3 and 4 for each metric in Table 4, show how the runtime metrics vary as S5\u2019s latent size is adjusted, with all other architecture choices equal to those of S4. In line 3 of each metric, we denote the S5 \u201cArchitecture\u201d as \u201c(PH) Matched to Gu et al. (2022)\u201d to indicate that this configuration of S5 sets the latent size equal to the number of input/output features, . This line empirically supports the complexity argument presented in Appendix C.1. In line 4 of each metric, we denote the S5 \u201cArchitecture\u201d as \u201c (PN) Matched to Gu et al. (2022)\u201d to indicate that this configuration of S5 sets the latent size equal to the latent size S4 uses for each of its SISO SSMs. This line also corresponds to the constrained version of S5 that performs similarly to S4/S4D as presented in the ablation study in Table 5. The runtime results of both of these configurations supports the claim in Section 4.3 that the latent size of S5 can be increased while maintaining S4\u2019s computational efficiency. Finally, we include a third configuration of S5, presented in the fifth line of each metric and italicized. This configuration of S5 uses the best architectural dimensions from Table 11 and was used for the corresponding LRA results in Table 1. Importantly, the broad takeaway from this empirical study is that the runtime and memory usage of S5 and S4/S4D are broadly similar, as suggested by the complexity analysis in the main text. Appendix D Relationship Between S4 and S5\n\nWe now describe in more detail the connection between the S4 and S5 architectures. This connection allowed us to develop more performant architectures and extend theoretical results from existing work. We break this analysis down into three parts:\n\n1. In Section D.2 we prove Proposition 2. We exploit the linearity of the systems to identify that the latent states computed by the S5 SSM are equivalent to a linear combination of latent states computed by the SISO S4 SSMs, and that the outputs of the S5 SSM are a further linear transformation of these states. We then highlight how S4 and S5 effectively define different output matrices in the block-diagonal perspective shown in Figure 2. 2. In Section D.3 we provide a simple extension of the proof provided by Gu et al. (2022). The original proof shows that in the SISO case, in the limit of large , the dynamics arising from a (non-diagonalizable) HiPPO-LegS matrix, are faithfully approximated by the (diagonalizable) normal component of the HiPPO-LegS matrix. We extend this proof to apply to the MIMO setting. This motivates initialization with the HiPPO-N matrix, which in-turn allows us to use parallel scans efficiently. 3. In Section D.4 we conclude by showing that, by judicious choice of initialization of the S5 state matrix, S5 can implement multiple independent S4 systems and relax the assumptions made. We also discuss the vector of timescale parameters, which we found to improve performance. We note that many of these results follow straightforwardly from the linearity of the recurrence. D.1 Assumptions\n\nFor these following sections we will use the following assumptions, until otherwise stated:\n\nAssumption 1. We consider only -dimensional to -dimensional sequence maps. Assumption 2. We assume the state matrix of each S4 SSM is identical, . Assumption 3. We assume the timescales of each S4 SSM are identical,\n\nAssumption 4. We assume that the same state matrix is used in S5 as in S4 (also cf. Assumption 2). Note this also specifies the S5 latent size . We also assume the S5 input matrix is the horizontal concatenation of the column input vectors used by S4, . D.2 Different Output Projections of Equivalent Dynamics\n\nWe provide a proof of Proposition 2. Proposition 2. Consider an S5 layer, with state matrix , input matrix and some output matrix (cf. Assumption 1); and an S4 layer, where each of the S4 SSMs has state matrix (cf. Assumption 2, 4) and input vector (cf. Assumption 4). If the S4 and S5 layers are discretized with the same timescales (cf. Assumption 3), then the S5 SSM produces outputs, , equivalent to a linear combination of the latent states of the S4 SSMs, , where . Proof. For a single S4 SSM, the discretized latent states can be expressed as a function of the input sequence\n\n\ud835\udc31 k ( h ) = \u2211 i = 1 k \ud835\udc00 \u00af k \u2212 i \u200b \ud835\udc01 \u00af ( h ) \u200b u i ( h ) . superscript subscript \ud835\udc31 \ud835\udc58 \u210e superscript subscript \ud835\udc56 1 \ud835\udc58 superscript \u00af \ud835\udc00 \ud835\udc58 \ud835\udc56 superscript \u00af \ud835\udc01 \u210e superscript subscript \ud835\udc62 \ud835\udc56 \u210e \\mathbf{x}_{k}^{(h)}=\\sum\\nolimits_{i=1}^{k}\\overline{\\mathbf{A}}^{k-i}\\overline{\\mathbf{B}}^{(h)}u_{i}^{(h)}. (13)\n\nFor an S5 layer, the latent states are expressible as\n\n\ud835\udc31 k = \u2211 i = 1 k \ud835\udc00 \u00af k \u2212 i \u200b \ud835\udc01 \u00af \u200b \ud835\udc2e i , subscript \ud835\udc31 \ud835\udc58 superscript subscript \ud835\udc56 1 \ud835\udc58 superscript \u00af \ud835\udc00 \ud835\udc58 \ud835\udc56 \u00af \ud835\udc01 subscript \ud835\udc2e \ud835\udc56 \\mathbf{x}_{k}=\\sum\\nolimits_{i=1}^{k}\\overline{\\mathbf{A}}^{k-i}\\overline{\\mathbf{B}}\\mathbf{u}_{i}, (14)\n\nwhere we index as and\n\nHere we make the observation:\n\n\ud835\udc31 k = \u2211 h = 1 H \ud835\udc31 k ( h ) , subscript \ud835\udc31 \ud835\udc58 superscript subscript \u210e 1 \ud835\udc3b superscript subscript \ud835\udc31 \ud835\udc58 \u210e \\mathbf{x}_{k}=\\sum\\nolimits_{h=1}^{H}\\mathbf{x}_{k}^{(h)}, (15)\n\nwhere this result follows directly from the linearity of (13) and (14). This shows that (under the assumptions outlined above) the states of the MIMO S5 SSM are equivalent to the summation of the states across the different SISO S4 SSMs. We can then consider the effect of the output matrix . For S5, the output matrix is a single dense matrix\n\n\ud835\udc32 k = \ud835\udc02\ud835\udc31 k . subscript \ud835\udc32 \ud835\udc58 subscript \ud835\udc02\ud835\udc31 \ud835\udc58 \\mathbf{y}_{k}=\\mathbf{C}\\mathbf{x}_{k}. (16)\n\nWe can substitute the relationship in (15) into (16) to cast the outputs of the MIMO S5 SSM in terms of the state of the SISO S4 SSMs:\n\n\ud835\udc32 k subscript \ud835\udc32 \ud835\udc58 \\displaystyle\\mathbf{y}_{k} = \ud835\udc02 \u200b \u2211 h = 1 H \ud835\udc31 k ( h ) , absent \ud835\udc02 superscript subscript \u210e 1 \ud835\udc3b subscript superscript \ud835\udc31 \u210e \ud835\udc58 \\displaystyle=\\mathbf{C}\\sum\\nolimits_{h=1}^{H}\\mathbf{x}^{(h)}_{k}, (17) = \u2211 h = 1 H \ud835\udc02\ud835\udc31 k ( h ) . absent superscript subscript \u210e 1 \ud835\udc3b subscript superscript \ud835\udc02\ud835\udc31 \u210e \ud835\udc58 \\displaystyle=\\sum\\nolimits_{h=1}^{H}\\mathbf{C}\\mathbf{x}^{(h)}_{k}. (18)\n\nDenoting the vertical concatenation of the S4 SSM state vectors , we see that the outputs of the S5 SSM are expressible as:\n\n\ud835\udc32 k = \ud835\udc02 equiv \u200b \ud835\udc31 k ( 1 : H ) , where \ud835\udc02 equiv = [ \ud835\udc02 \u200b \u2223 \u22ef \u2223 \u200b \ud835\udc02 ] , formulae-sequence subscript \ud835\udc32 \ud835\udc58 superscript \ud835\udc02 equiv superscript subscript \ud835\udc31 \ud835\udc58 : 1 \ud835\udc3b where superscript \ud835\udc02 equiv delimited-[] \ud835\udc02 delimited-\u2223\u2223 \u22ef \ud835\udc02 \\mathbf{y}_{k}=\\mathbf{C}^{\\mathrm{equiv}}\\mathbf{x}_{k}^{(1:H)},\\quad\\mathrm{where}\\quad\\mathbf{C}^{\\mathrm{equiv}}=\\left[\\ \\mathbf{C}\\mid\\cdots\\mid\\mathbf{C}\\ \\right], (19)\n\nand hence are equivalent to a linear combination of the states computed by the S4 SSMs. \u220e\n\nThis shows the outputs of the constrained S5 SSM under consideration (cf. Assumption 4) can be interpreted as a linear combination of the latent states computed by constrained S4 SSMs with the same state matrices and timescale parameters. Note however, it does not show that the outputs of the S5 SSM directly equal the outputs of the effective block-diagonal S4 SSM. Indeed, they are not equal, and we can repeat this analysis for the S4 layer to concretely identify the difference. For comparison we assume that the output vector for each S4 SSM is given as a row in the S5 output matrix, i.e. . We can express the output of each S4 SSM as\n\ny k ( h ) = \ud835\udc02 ( h ) \u200b \ud835\udc31 k ( h ) , superscript subscript \ud835\udc66 \ud835\udc58 \u210e superscript \ud835\udc02 \u210e superscript subscript \ud835\udc31 \ud835\udc58 \u210e \\displaystyle y_{k}^{(h)}=\\mathbf{C}^{(h)}\\mathbf{x}_{k}^{(h)}, (20)\n\nwhere . We can then define the effective output matrix that operates on the entire latent space (the dashed box labelled in Figure 2(a)) in S4 as\n\ny k ( h ) = ( \ud835\udc02 S4 \u200b \ud835\udc31 k ) ( h ) superscript subscript \ud835\udc66 \ud835\udc58 \u210e superscript superscript \ud835\udc02 S4 subscript \ud835\udc31 \ud835\udc58 \u210e y_{k}^{(h)}=\\left(\\mathbf{C}^{\\mathrm{S4}}\\mathbf{x}_{k}\\right)^{(h)} (21)\n\nBy inspecting (19) and (21), we can concretely express the difference in the equivalent output matrix used by both layers\n\n\ud835\udc02 S4 superscript \ud835\udc02 S4 \\displaystyle\\mathbf{C}^{\\mathrm{S4}} = [ \ud835\udc02 ( 1 ) \u22ef \ud835\udfce \u22ee \u22f1 \u22ee \ud835\udfce \u22ef \ud835\udc02 ( H ) ] , absent delimited-[] superscript \ud835\udc02 1 \u22ef 0 \u22ee \u22f1 \u22ee 0 \u22ef superscript \ud835\udc02 \ud835\udc3b \\displaystyle=\\left[\\begin{array}[]{ccc}\\mathbf{C}^{(1)}&\\cdots&\\mathbf{0}\\\\\n\\vdots&\\ddots&\\vdots\\\\\n\\mathbf{0}&\\cdots&\\mathbf{C}^{(H)}\\end{array}\\right], \ud835\udc02 equiv = [ \ud835\udc02 ( 1 ) \u22ef \ud835\udc02 ( 1 ) \u22ee \u22f1 \u22ee \ud835\udc02 ( H ) \u22ef \ud835\udc02 ( H ) ] = [ \ud835\udc02 \u200b \u2223 \u22ef \u2223 \u200b \ud835\udc02 ] . superscript \ud835\udc02 equiv delimited-[] superscript \ud835\udc02 1 \u22ef superscript \ud835\udc02 1 \u22ee \u22f1 \u22ee superscript \ud835\udc02 \ud835\udc3b \u22ef superscript \ud835\udc02 \ud835\udc3b delimited-[] \ud835\udc02 delimited-\u2223\u2223 \u22ef \ud835\udc02 \\displaystyle\\mathbf{C}^{\\mathrm{equiv}}=\\left[\\begin{array}[]{ccc}\\mathbf{C}^{(1)}&\\cdots&\\mathbf{C}^{(1)}\\\\\n\\vdots&\\ddots&\\vdots\\\\\n\\mathbf{C}^{(H)}&\\cdots&\\mathbf{C}^{(H)}\\end{array}\\right]=\\left[\\ \\mathbf{C}\\mid\\cdots\\mid\\mathbf{C}\\ \\right]. (28)\n\nIn S4, the effective output matrix consists of independent vectors on the leading diagonal (as is pictured in Figure 2(a)). In contrast, the effective output matrix used by S5 instead ties dense output matrices across the S4 SSMs. As such, S5 can be interpreted as simply defining a different projection of the independent SISO SSMs than is used by S4. Note that both projection matrices have the same number of parameters. Although the projection is different, the fact that the latent dynamics can still be interpreted as a linear projection of the same underlying S4 latent dynamics suggests that initializing the state dynamics in S5 with the HiPPO-LegS matrix may lead to good performance, similarly to what was observed in S4. We discuss this in the next section. We note that it is not obvious whether tying the dense output matrices is any more or less expressive than S4\u2019s use of a single untied output vector for each SSM, and it is unlikely that one approach is universally better than the other. We also stress that one would never implement S4 using the block diagonal matrix in (28), or, implement S5 using the repeated matrix in (28). These matrices are simply constructs for understanding the equivalence between S4 and S5. Finally, we note an alternative view: the block-diagonal S4 with output matrix of Proposition 2 is equivalent to a version of S4 that uses a bank of single-input, multi-output (SIMO) SSMs with tied state matrices, timescales and multi-channel output matrices and sums the individual SSM outputs. See appendix of Gu et al. (2021b) for a discussion of how the linear state space layer (LSSL), a predecessor of S4, can have multiple output channels. D.3 Diagonalizable Initialization\n\nProposition 2 suggests that initializing with the HiPPO-LegS matrix may yield good performance in S5, just as it does in S4 (because the constrained version of S5 under consideration is effectively a different linear projection of the same latent dynamics). However, the HiPPO-LegS matrix is not stably diagonalizable. Corollary 1 allows us to initialize MIMO SSMs with the diagonalizable HiPPO-N matrix to approximate the HiPPO-LegS matrix and expect the performance to be comparable. Corollary 1 (Extension of Theorem 3 in Gu et al.",
    "s5-39": "(2022)). Consider , , as defined in Appendix B.1.1. Given vector-valued inputs , the ordinary differential equation converges to as .",
    "s5-40": "Proof. Theorem 3 in Gu et al. (2022) shows the following relationship for scalar input signals as :\n\nd \u200b \ud835\udc31 ( h ) \u200b ( t ) d \u200b t = \ud835\udc00 LegS Normal \u200b \ud835\udc31 ( h ) \u200b ( t ) + 1 2 \u200b \ud835\udc01 LegS ( h ) \u200b u ( h ) \u200b ( t ) , d superscript \ud835\udc31 \u210e \ud835\udc61 d \ud835\udc61 superscript subscript \ud835\udc00 LegS Normal superscript \ud835\udc31 \u210e \ud835\udc61 1 2 subscript superscript \ud835\udc01 \u210e LegS superscript \ud835\udc62 \u210e \ud835\udc61 \\frac{\\mathrm{d}\\mathbf{x}^{(h)}(t)}{\\mathrm{d}t}=\\mathbf{A}_{\\mathrm{LegS}}^{\\mathrm{Normal}}\\mathbf{x}^{(h)}(t)+\\frac{1}{2}\\mathbf{B}^{(h)}_{\\mathrm{LegS}}u^{(h)}(t), (29)\n\nwhere the only modification we have made is introducing the superscript to allow us to explicitly index dimensions later.",
    "s5-41": "We define . We wish to extend this to the case of vector-valued input signals. We first recall (15), which shows that the latent states of the MIMO S5 SSM are the summation of the latent states of the SISO S4 SSMs (to which Theorem 3 from Gu et al. (2021a) applies). Although we derived (15) in discrete time, it applies equally in continuous time:\n\n\ud835\udc31 \u200b ( t ) = \u2211 h = 1 H \ud835\udc31 ( h ) \u200b ( t ) . \ud835\udc31 \ud835\udc61 superscript subscript \u210e 1 \ud835\udc3b superscript \ud835\udc31 \u210e \ud835\udc61 \\mathbf{x}(t)=\\sum\\nolimits_{h=1}^{H}\\mathbf{x}^{(h)}(t). (30)\n\nWe can therefore define the derivative of the S5 state as:\n\nd \u200b \ud835\udc31 \u200b ( t ) d \u200b t = \u2211 h = 1 H d \u200b \ud835\udc31 ( h ) \u200b ( t ) d \u200b t . d \ud835\udc31 \ud835\udc61 d \ud835\udc61 superscript subscript \u210e 1 \ud835\udc3b d superscript \ud835\udc31 \u210e \ud835\udc61 d \ud835\udc61 \\frac{\\mathrm{d}\\mathbf{x}(t)}{\\mathrm{d}t}=\\sum\\nolimits_{h=1}^{H}\\frac{\\mathrm{d}\\mathbf{x}^{(h)}(t)}{\\mathrm{d}t}. (31)\n\nSubstituting (29) into this then yields:\n\nd \u200b \ud835\udc31 \u200b ( t ) d \u200b t d \ud835\udc31 \ud835\udc61 d \ud835\udc61 \\displaystyle\\frac{\\mathrm{d}\\mathbf{x}(t)}{\\mathrm{d}t} = \u2211 h = 1 H [ \ud835\udc00 LegS Normal \u200b \ud835\udc31 ( h ) \u200b ( t ) + 1 2 \u200b \ud835\udc01 LegS ( h ) \u200b u ( h ) \u200b ( t ) ] , absent superscript subscript \u210e 1 \ud835\udc3b delimited-[] superscript subscript \ud835\udc00 LegS Normal superscript \ud835\udc31 \u210e \ud835\udc61 1 2 superscript subscript \ud835\udc01 LegS \u210e superscript \ud835\udc62 \u210e \ud835\udc61 \\displaystyle=\\sum\\nolimits_{h=1}^{H}\\left[\\mathbf{A}_{\\mathrm{LegS}}^{\\mathrm{Normal}}\\mathbf{x}^{(h)}(t)+\\frac{1}{2}\\mathbf{B}_{\\mathrm{LegS}}^{(h)}u^{(h)}(t)\\right], (32) = \ud835\udc00 LegS Normal \u200b \u2211 h = 1 H \ud835\udc31 ( h ) \u200b ( t ) + 1 2 \u200b \u2211 h = 1 H \ud835\udc01 LegS ( h ) \u200b u ( h ) \u200b ( t ) , absent superscript subscript \ud835\udc00 LegS Normal superscript subscript \u210e 1 \ud835\udc3b superscript \ud835\udc31 \u210e \ud835\udc61 1 2 superscript subscript \u210e 1 \ud835\udc3b superscript subscript \ud835\udc01 LegS \u210e superscript \ud835\udc62 \u210e \ud835\udc61 \\displaystyle=\\mathbf{A}_{\\mathrm{LegS}}^{\\mathrm{Normal}}\\sum\\nolimits_{h=1}^{H}\\mathbf{x}^{(h)}(t)+\\frac{1}{2}\\sum\\nolimits_{h=1}^{H}\\mathbf{B}_{\\mathrm{LegS}}^{(h)}u^{(h)}(t), (33) = \ud835\udc00 LegS Normal \u200b \ud835\udc31 \u200b ( t ) + 1 2 \u200b \ud835\udc01 LegS \u200b \ud835\udc2e \u200b ( t ) . absent superscript subscript \ud835\udc00 LegS Normal \ud835\udc31 \ud835\udc61 1 2 subscript \ud835\udc01 LegS \ud835\udc2e \ud835\udc61 \\displaystyle=\\mathbf{A}_{\\mathrm{LegS}}^{\\mathrm{Normal}}\\mathbf{x}(t)+\\frac{1}{2}\\mathbf{B}_{\\mathrm{LegS}}\\mathbf{u}(t). (34)\n\nThis equivalence motivates initializing S5 state matrices with the diagonalizable HiPPO-N matrix and suggests that we can expect to see similar performance gains. D.4 Relaxing the Assumptions\n\nHere we discuss how relaxing the constraint on S5\u2019s latent size from Assumption 4 helps to relax the assumptions on the tied S4 SSM state matrices (Assumption 2) and timescales (Assumption 3) as well as the tied output matrices that result from Proposition 2. We start by considering the case when the S5 SSM state matrix is block-diagonal. Consider an S5 SSM with latent size and block-diagonal , dense , dense , and different timescale parameters . As a result of the block-diagonal state matrix, this system has a latent state that can be partitioned into different states . We can then partition this system into different subsystems and discretize each subsystem with one of the to get the following discretized system:\n\n\ud835\udc00 \u00af = [ \ud835\udc00 \u00af ( 1 ) \u22f1 \ud835\udc00 \u00af ( J ) ] , \ud835\udc01 \u00af = [ \ud835\udc01 \u00af ( 1 ) \u22ee \ud835\udc01 \u00af ( J ) ] , \ud835\udc02 = [ \ud835\udc02 ( 1 ) \u200b \u2223 \u22ef \u2223 \u200b \ud835\udc02 ( J ) ] , formulae-sequence \u00af \ud835\udc00 delimited-[] superscript \u00af \ud835\udc00 1 missing-subexpression missing-subexpression missing-subexpression \u22f1 missing-subexpression missing-subexpression missing-subexpression superscript \u00af \ud835\udc00 \ud835\udc3d formulae-sequence \u00af \ud835\udc01 delimited-[] superscript \u00af \ud835\udc01 1 missing-subexpression missing-subexpression \u22ee missing-subexpression missing-subexpression superscript \u00af \ud835\udc01 \ud835\udc3d missing-subexpression missing-subexpression \ud835\udc02 delimited-[] superscript \ud835\udc02 1 delimited-\u2223\u2223 \u22ef superscript \ud835\udc02 \ud835\udc3d missing-subexpression missing-subexpression \\overline{\\mathbf{A}}=\\left[\\begin{array}[]{ccc}\\overline{\\mathbf{A}}^{(1)}&&\\\\\n&\\ddots&\\\\\n&&\\overline{\\mathbf{A}}^{(J)}\\end{array}\\right],\\quad\\overline{\\mathbf{B}}=\\left[\\begin{array}[]{ccc}\\overline{\\mathbf{B}}^{(1)}\\\\\n\\vdots\\\\\n\\overline{\\mathbf{B}}^{(J)}\\end{array}\\right],\\quad\\mathbf{C}=\\left[\\begin{array}[]{ccc}\\mathbf{C}^{(1)}\\mid\\cdots\\mid\\mathbf{C}^{(J)}\\end{array}\\right], (35)\n\nwhere , and . It follows that this partitioned system can be also be viewed as independent -dimensional S5 SSM subsystems and the output of the overall system is the sum of the output of the subsystems\n\n\ud835\udc32 k subscript \ud835\udc32 \ud835\udc58 \\displaystyle\\mathbf{y}_{k} = \ud835\udc02\ud835\udc31 k absent subscript \ud835\udc02\ud835\udc31 \ud835\udc58 \\displaystyle=\\mathbf{C}\\mathbf{x}_{k} (36) = \u2211 j = 1 J \ud835\udc02 ( j ) \u200b \ud835\udc31 k ( j ) . absent superscript subscript \ud835\udc57 1 \ud835\udc3d superscript \ud835\udc02 \ud835\udc57 superscript subscript \ud835\udc31 \ud835\udc58 \ud835\udc57 \\displaystyle=\\sum_{j=1}^{J}\\mathbf{C}^{(j)}\\mathbf{x}_{k}^{(j)}. (37)\n\nIt follows from Proposition 2 that the dynamics of each of these S5 SSM subsystems can be related to the dynamics of a different S4 system from Proposition 2. Each of these S4 systems has its own bank of tied S4 SSMs (cf. Assumptions 2, 3). Importantly, each of the S4 systems can have its own state matrix, timescale parameter and output matrix shared across its S4 SSMs. Thus, the outputs of a dimensional S5 SSM can be equivalent to the linear combination of the latent states of different S4 systems from Proposition 2. This fact motivates the option to initialize a block-diagonal S5 state matrix with several HiPPO-N matrices on the blocks, rather than just initializing with one larger HiPPO-N matrix. In practice we found the block-diagonal initialization to improve performance on many tasks, see Appendix E. D.5 Timescale Parameterization\n\nFinally, we take a closer look at the parameterization of the timescale parameters . As discussed in Section 4.3, S4 can learn a different timescale parameter for each S4 SSM, potentially allowing it to capture different timescales of the data. Further, the initialization of the timescales can be important (Gu et al., 2023; Gupta et al., 2022), and limiting to sampling a single initial parameter may lead to poor initialization. The discussion in the previous section motivates potentially learning different timescale parameters, one for each of the subsystems. However, in practice, we found better performance when using different timescale parameters, one for each of the states. On the one hand, this can be viewed simply as learning a different scaling for each of the eigenvalues in the diagonalized system (see Eq. (6)). On the other hand, this could be viewed as increasing the number of timescale parameters sampled at initialization, helping to combat the possibility of poor initialization. Of course, the system could learn to use just a single timescale by setting all of the timescales to be the same. See further discussion in the ablation study in Appendix E. Appendix E Ablations\n\nWe perform several ablations to empirically explore different aspects of S5. E.1 S5 latent size, block-diagonal initialization, and timescale parameterization\n\nThe discussion in Section 4 and Appendix D raises several interesting questions: How does S5 perform when the latent size is restricted to be equal to the latent size used by each of S4\u2019s SSMs? How important is the timescale parameterization discussed in Appendix D.5? How important is the block-diagonal initialization? Table 5 displays the results of an ablation study performed on the LRA tasks to get a better sense of this. We consider 3 versions of S5. The first version uses the same general architecture (e.g. number of input/output features , number of layers, etc) as reported for the S4/S4D variants in Gu et al. (2022), sets the S5 SSM latent size to be equal to the latent size used by each of the S4 SSMs, and uses only a single scalar timescale parameter . This is essentially the version of S5 we consider in Proposition 2. We observe that this constrained version of S5 actually performs well on most tasks, though struggles to perform comparably to the S4 baselines on Image and ListOps. The second version of S5 is exactly the same as the first except we parameterize the timescale parameter as a vector . We observe uniform improvements over the scalar timescale parameterization and this reflects our general findings when training S5. Finally, the complexity analysis and runtime comparison in Appendix C.2 suggests the latent size of S5 can be increased while maintaining similar complexity and practical runtimes as S4. We include the unconstrained version of S5 reported in our main results that uses the settings reported in the hyperparameter Table 11. These models were allowed to be parameterized with fewer input/output features (to ensure similar parameter counts to the S4 baselines) and generally used larger latent sizes . Further, we swept over the use of a block-diagonal initialization or not and the number of blocks to use (where indicates no block-diagonal initialization was used). All models benefited from the block-diagonal initialization for the LRA tasks (See Table 11 ). E.2 Importance of HiPPO-N and continuous-time parameterization\n\nWe perform a further ablation study to gain insight into the differences between S5 and prior attempts at parallelized linear RNNs (discussed in Section 5) focusing on what appears to be the distinguishing features: continuous-time parameterizations and HiPPO initializations. We compare different initializations of the state matrix: random Gaussian, random antisymmetric, and HiPPO-N. The antisymmetric initialization is interesting because prior work considered these matrices in RNNs for long-range dependencies (Chang et al., 2019), and because the HiPPO-LegS matrix can be parameterized in a way related to antisymmetric matrices (Gu et al., 2021a). Moreover, to compare to a setup more akin to the previous parallelized linear RNN work, we also consider a direct discrete-time parameterization of S5 that does not perform repeated discretization during training or learn the timescale parameter . We present the results of this ablation study in Table 6 (along with S5). We consider three of the LRA tasks that vary in length and difficulty. The main takeaway is that the only approach that consistently performs well on all tasks, including the ability to solve Path-X, is the S5 approach that uses the continuous-time parameterization and HiPPO initialization. We also note that we observed the discrete time/HiPPO-N matrix configuration to be difficult to train due to stability issues, typically requiring a much lower learning rate. E.3 S4D Initialization ablations\n\nFinally, Gu et al. (2022) propose several alternative diagonal matrices to the diagonalized HiPPO-N matrix, including the S4D-Inv and S4D-Lin matrices. They perform an ablation on the LRA tasks where they simply replace the diagonalized HiPPO-N matrix with the S4D-Inv and S4D-Lin matrices while keeping all other factors the same. We include these results in Table 7. In Table 7, we also include results for a similar ablation in S5 by using these matrices to initialize S5 in place of the HiPPO-N matrix while keeping all other factors constant. Both matrices perform well on most tasks with the exception of the S4D-Lin matrix on Path-X. Interestingly, one of these runs reached , however the other runs did not exceed random guessing on this task. Future exploration of these and other matrices are an interesting direction for future work. Appendix F Supplementary Results\n\nWe include further experimental results to supplement the results presented in the main text. F.1 Extended LRA Results\n\nF.2 Extended Speech Results\n\nF.3 Pendulum Extended Results\n\nWe also evaluate two ablations: S5-drop uses the same S5 architecture, but drops the dependence on the inter-sample interval, i.e.",
    "s5-42": ". We expect this network to perform poorly as it has no knowledge of how long has elapsed between observations. S5-append uses the same S5 architecture, but appends the integration timestep to the thirty-dimensional image encoding, prior to being input into the dense S5 input layer.",
    "s5-43": "Hypothetically, we expect this network to perform as well as S5. However, to do so, requires the S5 network to learn to process time, which may be difficult, especially in more complex domains. We include these ablations in the bottom partition of Table LABEL:app:tab:results:pendulum_mse. Note that the runtimes quoted for the baseline methods (runtimes marked with a *) are as reported by Schirmer et al. (2022). These times are the total time for a training epoch, and hence include any time spent batching data. We re-ran the CRU using the original PyTorch code on the same hardware as we run our JAX S5 experiments on (labelled CRU (our run)). For these experiments we used a single NVIDIA GeForce RTX 2080 Ti. For these runs (CRU (our run), S5, S5-drop and S5-append) we exclude the time spent batching the data to more faithfully compare the runtimes for the models themselves. Also note that our S5 experiments will benefit from JAX compilation, but that this is not sufficient to explain the difference in runtime. F.4 Pixel-level 1-D Image Classification Results\n\nTable 10 presents results and citations of the pixel-level 1-D image classification. Appendix G Experiment Configurations\n\nIn this section we describe the experimental details. This includes the model architecture, general hyperparameters, specifics for each task, and information about the datasets. G.1 Deep Sequence Model Architecture\n\nFor the experiments, we use the S5 layer as a drop-in replacement for the S4 layer used in the sequence model architecture of (Gu et al., 2021a). On a high level, this architecture consists of a linear encoder (to encode the input at each time step into features), multiple S5 layers, a mean pooling layer, a linear decoder, and a Softmax operation for the classification tasks. The mean pooling layer compresses the output of the last S5 layer, of shape [batch size, sequence length, number of features ()], across the sequence length dimension, so that a single -dimensional encoding is available for softmax classification. The baseline S4 models from Gu et al. (2022; 2023) apply a GLU activation (Dauphin et al., 2017) function to the S4 SSM outputs. To take advantage of the fact that the S5 SSM outputs have already been mixed throughout the MIMO SSM we use what is essentially a weighted sigmoid gated unit (Tanaka, 2020) (a GLU activation without an additional linear transform). Specifically, given an S5 SSM output and a dense matrix , the layer output of the activation function we apply is . Hyperparameter options such as dropout rate, using either layer normalization or batch normalization, and using either pre-norm or post-norm are applied between the layers. Exceptions to the basic architecture described here are mentioned in the individual experiment sections below. G.2 Default Hyperparameters\n\nTable 11 presents the main hyperparameters used for each experiment. For all experiments we ensure the number of layers and layer input/output features are less than or equal to the number of layers and layer input/output features reported in Gu et al. (2022) as well as ensuring comparable parameter counts. In general, the models for most tasks used batch normalization and pre-norm. Exceptions are noted in the individual experiment sections below. G.2.1 Optimizers and learning rates\n\nWe follow the general optimization approach used by S4/S4D in Gu et al. (2022). We use the AdamW optimizer (Loshchilov & Hutter, 2019) with a global learning rate. However, in general, no weight decay and a smaller learning rate (the SSM learning rate) is applied to , , .",
    "s5-44": "All experiments used cosine annealing. Exceptions to these points are noted in the individual experiment sections below. G.2.2 Bidirectionality\n\nWe follow Gu et al. (2022) and use bidirectional models for the LRA and speech tasks. Unidirectional (causal) models were used for the pendulum, sequential and permuted MNIST for fair comparison with prior methods that used unidirectional models. G.3 Task Specific Hyperparameters\n\nHere we specify any task-specific details, hyperparameter or architectural differences from the defaults outlined above. G.3.1 Listops\n\nWeight decay and the global learning rate were applied to . G.3.2 Text\n\nNo exceptions to the defaults for this run. G.3.3 Retrieval\n\nThis document matching task requires a slightly different architecture from the other experiments, as discussed in Tay et al. (2021). We use the same configuration as S4 (Gu et al., 2021a). Each string is passed through the input encoder, S5 layers, and mean pooling layers. Denoting as the output for the first document and as the output for the second document, four features are created and concatenated together (Tay et al., 2021) as\n\nX = [ X 1 , X 2 , X 1 \u2217 X 2 , X 1 \u2212 X 2 ] . \ud835\udc4b subscript \ud835\udc4b 1 subscript \ud835\udc4b 2 subscript \ud835\udc4b 1 subscript \ud835\udc4b 2 subscript \ud835\udc4b 1 subscript \ud835\udc4b 2 \\displaystyle X=[X_{1},X_{2},X_{1}*X_{2},X_{1}-X_{2}]. (38)\n\nThis concatenated feature is then fed to a linear decoder and softmax function as normal. G.3.4 Image\n\nWeight decay and the global learning rate were applied to . G.3.5 Pathfinder\n\nNo exceptions to the defaults for this run. G.3.6 Path-X\n\nWeight decay was applied to . G.3.7 Speech Commands\n\nNo weight decay and the SSM learning rate were applied to for this run. G.3.8 Pendulum Regression\n\nWe use the same encoder-decoder architecture as Schirmer et al. (2022). The encoder has layers: convolution, ReLU, max pool, convolution, ReLU, max pool, dense, ReLU, dense. The first convolution layer has twelve features, a kernel, and a padding of . The second convolution layer has twelve features, a kernel, a stride of , and a padding of . Both max pools use a window size of and a stride of 2. The dense layer has thirty hidden units. The linear readout layer outputs features. This is chosen to match the encoding size in Schirmer et al. (2022), and is used for all layers. Separate mean and unconstrained variance decoders are used, defined as a one-layer MLP with a hidden size of thirty. An elu+1 activation function is used to constrain the variance to be positive. Layer normalization and post-norm were used for this task. For the timings presented in Table LABEL:tab:results:pendulum_mse and LABEL:app:tab:results:pendulum_mse we use a batch size of , instead of the batch size of used during training, to match the batch sizes reported by the baselines. G.3.9 Sequential MNIST\n\nNo exceptions to the defaults for this run. G.3.10 Permuted Sequential MNIST\n\nWeight decay and the global learning rate were applied to . Post-norm was used for this task. G.3.11 Sequential CIFAR\n\nWe trained a model with the exact hyperparameter settings as used for the LRA-IMAGE (grayscale sequential CIFAR) task with no further tuning. G.4 Dataset Details\n\nWe provide more context and details for each of the LRA (Tay et al., 2021) and Speech Commands (Warden, 2018) datasets we consider.",
    "s5-45": "Note that we follow the same data pre-processing steps as Gu et al. (2021a), which we also include here for completeness. \u2022\n\nListOps: A lengthened version of the dataset presented by Nangia & Bowman (2018). Given a nested set of mathematical operations (such as min and max) and integer operands in the range zero to nine, expressed in prefix notation with brackets, compute the integer result of the mathematical expression, e.g.",
    "s5-46": ". Characters are encoded as one-hot vectors, with unique values possible (opening brackets and operators are grouped into a single token). The sequences are of unequal length, and hence the end of shorter sequences is padded with a fixed indicator value, padded to a maximum length of . A reserved end-of-sequence token is appended. There are different classes, representing the integer result of the expression. There are training sequences, validation sequences, and test sequences. No normalization is applied. \u2022\n\nText: Based off of the iMDB sentiment dataset presented by Maas et al. (2011). Given a movie review, where characters are encoded as a sequence of integer tokens, classify whether the movie review is positive or negative. Characters are encoded as one-hot vectors, with unique values possible. Sequences are of unequal length, and are padded to a maximum length of . There are two different classes, representing positive and negative sentiment. There are training examples and test examples. No validation set is provided. No normalization is applied. \u2022\n\nRetrieval: Based off of the ACL Anthology network corpus presented by Radev et al. (2009). Given two textual citations, where characters are encoded as a sequence of integer tokens, classify whether the two citations are equivalent. The citations must be compressed separately, before being passed into a final classifier layer. This is to evaluate how effectively the network can represent the text. The decoder head then uses the encoded representation to complete the task. Characters are encoded into a one-hot vector with unique values. Two paired sequences may be of unequal length, with a maximum sequence length of . There are two different classes, representing whether the citations are equivalent or not. There are training pairs, validation pairs, and test pairs. No normalization is applied. \u2022\n\nImage: Uses the CIFAR-10 dataset presented by Krizhevsky (2009). Given a grayscale CIFAR-10 image as a one-dimensional raster scan, classify the image into one of ten classes.",
    "s5-47": "Sequences are of equal length (). There are ten different classes. There are training examples, validation examples, and test examples. RGB pixel values are converted to a grayscale intensities, which are then normalized to have zero mean and unit variance (across the entire dataset). \u2022\n\nPathfinder: Based off of the Pathfinder challenge introduced by Linsley et al. (2018). A grayscale image image shows a start and an end point as a small circle. There are a number of dashed lines on the image. The task is to classify whether there is a dashed line (or path) joining the start and end point. There are two different classes, indicating whether there is a valid path or not. Sequences are all of the same length (). There are training examples, validation examples, and test examples. The data is normalized to be in the range . \u2022\n\nPath-X: An \u201cextreme\u201d version of the Pathfinder challenge. Instead, the images are pixels, resulting in sequences that are a factor of sixteen times longer. Otherwise identical to the Pathfinder challenge. \u2022\n\nSpeech Commands: Based on the dataset released by Warden (2018). Readers recite one of 35 words. The task is then to classify which of the 35 words was spoken from a one-dimensional audio recording. There are 35 different classes, each representing one of the words in the vocabulary. Sequences are all of the same length (). There are training examples, validation examples, and test examples. Data is normalized to be zero mean and have a standard deviation of . \u2022\n\nSpeech Commands 0.5: Temporally sub-sampled version of Speech Commands, where the validation and test datasets only are sub-sampled by a factor of , and are therefore shortened to length .",
    "s5-48": "No subsequent padding is applied. The training dataset is not subsampled. \u2022\n\nSequential MNIST: (sMNIST) 10-way digit classification from a grayscale image of a handwritten digit, where the input image is flattened into a -length scalar sequence. \u2022\n\nPermuted Sequential MNIST: (psMNIST) 10-way digit classification from a grayscale image of a handwritten digit, where the input image is flattened into a -length scalar sequence. This sequence is then permuted using a fixed order. \u2022\n\nSequential CIFAR: (sCIFAR): 10-way image classification using the CIFAR-10 dataset. Identical to image, except that full colour images are input as a -length input sequence, where each input is an (R,G,B) triple. \u2022\n\nPendulum Regression: Reproduced from Becker et al. (2019) and Schirmer et al. (2022). The input sequence is a grayscale rendering of a pendulum, driven by a random torque process. The images pixels are corrupted by a noise process that is correlated in time. The pendulum is simulated for timesteps, and frames are irregularly sampled without replacement from the simulation. The objective is to estimate the sine and cosine of the angle of the pendulum. A train/validation/test split of is used. Appendix H Background on Parallel Scans for Linear Recurrences\n\nFor the interested reader, this section provides more background on using a parallel scan for a linear recurrence, as well as a simple example to illustrate how it can compute the recurrence in parallel. The parallelization of scan operations has been well studied (Ladner & Fischer, 1980; Lakshmivarahan & Dhall, 1994; Blelloch, 1990), and many standard scientific computing libraries contain efficient implementations. We note the linear recurrence we consider here is a specific instance of the more general setting discussed in Section 1.4 of Blelloch (1990). Computing a general parallel scan requires defining two objects:\n\n\u2022\n\nThe initial elements the scan will operate on. \u2022\n\nA binary associative operator used to combine the elements. To compute a length linear recurrence, , we will define the initial elements, , such that each element is the tuple\n\nc k = ( c k , a , c k , b ) := ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u k ) . subscript \ud835\udc50 \ud835\udc58 subscript \ud835\udc50 \ud835\udc58 \ud835\udc4e subscript \ud835\udc50 \ud835\udc58 \ud835\udc4f assign \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 \ud835\udc58 \\displaystyle c_{k}=(c_{k,a},c_{k,b}):=(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{k}). (39)\n\nThese will be precomputed prior to the scan. Having created the list of elements for the scan to operate on, we define the binary operator for the scan to use on this linear recurrence as\n\nq i \u2219 q j := ( q j , a \u2299 q i , a , q j , a \u2297 q i , b + q j , b ) , assign \u2219 subscript \ud835\udc5e \ud835\udc56 subscript \ud835\udc5e \ud835\udc57 direct-product subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4e tensor-product subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4f subscript \ud835\udc5e \ud835\udc57 \ud835\udc4f \\displaystyle q_{i}\\bullet q_{j}:=\\left(q_{j,a}\\odot q_{i,a},\\enspace q_{j,a}\\otimes q_{i,b}+q_{j,b}\\right), (40)\n\nwhere denotes an input element to the operator that could be the initial elements or some intermediate result, denotes matrix-matrix multiplication, denotes matrix-vector multiplication and denotes elementwise addition. We show that this operator is associative at the end of this section. Simple example using binary operator\n\nWe can illustrate how can be used to compute a linear recurrence in parallel with a simple example. Consider the system , and a length sequence of inputs . Assuming , the desired latent states from this recurrence are:\n\nx 1 subscript \ud835\udc65 1 \\displaystyle x_{1} = \ud835\udc01 \u00af \u200b u 1 absent \u00af \ud835\udc01 subscript \ud835\udc62 1 \\displaystyle=\\overline{\\mathbf{B}}u_{1} (41) x 2 subscript \ud835\udc65 2 \\displaystyle x_{2} = \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc01 \u00af \u200b u 2 absent \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc01 subscript \ud835\udc62 2 \\displaystyle=\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{B}}u_{2} (42) x 3 subscript \ud835\udc65 3 \\displaystyle x_{3} = \ud835\udc00 \u00af 2 \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 2 + \ud835\udc01 \u00af \u200b u 3 absent superscript \u00af \ud835\udc00 2 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 2 \u00af \ud835\udc01 subscript \ud835\udc62 3 \\displaystyle=\\overline{\\mathbf{A}}^{2}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{2}+\\overline{\\mathbf{B}}u_{3} (43) x 4 subscript \ud835\udc65 4 \\displaystyle x_{4} = \ud835\udc00 \u00af 3 \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc00 \u00af 2 \u200b \ud835\udc01 \u00af \u200b u 2 + \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 3 + \ud835\udc01 \u00af \u200b u 4 absent superscript \u00af \ud835\udc00 3 \u00af \ud835\udc01 subscript \ud835\udc62 1 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc01 subscript \ud835\udc62 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 3 \u00af \ud835\udc01 subscript \ud835\udc62 4 \\displaystyle=\\overline{\\mathbf{A}}^{3}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{A}}^{2}\\overline{\\mathbf{B}}u_{2}+\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{3}+\\overline{\\mathbf{B}}u_{4} (44)\n\nWe first note that can be used to compute this recurrence sequentially. We can initialize the scan elements as in (39), and then sequentially scan over these elements to compute the output elements . Defining where is the identity matrix, we have for our example:\n\ns 1 = s 0 \u2219 c 1 subscript \ud835\udc60 1 \u2219 subscript \ud835\udc60 0 subscript \ud835\udc50 1 \\displaystyle s_{1}=s_{0}\\bullet c_{1} = ( \ud835\udc08 , 0 ) \u2219 ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 1 ) = ( \ud835\udc00 \u00af \u200b \ud835\udc08 , \ud835\udc00 \u00af \u200b 0 + \ud835\udc01 \u00af \u200b u 1 ) = ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 1 ) absent \u2219 \ud835\udc08 0 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc00 \ud835\udc08 \u00af \ud835\udc00 0 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \\displaystyle=(\\mathbf{I},\\enspace 0)\\bullet(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{1})=(\\overline{\\mathbf{A}}\\mathbf{I},\\enspace\\overline{\\mathbf{A}}0+\\overline{\\mathbf{B}}u_{1})=(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{1}) (45) s 2 = s 1 \u2219 c 2 subscript \ud835\udc60 2 \u2219 subscript \ud835\udc60 1 subscript \ud835\udc50 2 \\displaystyle s_{2}=s_{1}\\bullet c_{2} = ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 1 ) \u2219 ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 2 ) = ( \ud835\udc00 \u00af 2 , \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc01 \u00af \u200b u 2 ) absent \u2219 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 2 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc01 subscript \ud835\udc62 2 \\displaystyle=(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{1})\\bullet(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{2})=(\\overline{\\mathbf{A}}^{2},\\enspace\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{B}}u_{2}) (46) s 3 = s 2 \u2219 c 3 subscript \ud835\udc60 3 \u2219 subscript \ud835\udc60 2 subscript \ud835\udc50 3 \\displaystyle s_{3}=s_{2}\\bullet c_{3} = ( \ud835\udc00 \u00af 2 , \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc01 \u00af \u200b u 2 ) \u2219 ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 3 ) = ( \ud835\udc00 \u00af 3 , \ud835\udc00 \u00af 2 \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 2 + \ud835\udc01 \u00af \u200b u 3 ) absent \u2219 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc01 subscript \ud835\udc62 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 3 superscript \u00af \ud835\udc00 3 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 2 \u00af \ud835\udc01 subscript \ud835\udc62 3 \\displaystyle=(\\overline{\\mathbf{A}}^{2},\\enspace\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{B}}u_{2})\\bullet(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{3})=(\\overline{\\mathbf{A}}^{3},\\enspace\\overline{\\mathbf{A}}^{2}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{2}+\\overline{\\mathbf{B}}u_{3}) (47) s 4 = s 3 \u2219 c 4 subscript \ud835\udc60 4 \u2219 subscript \ud835\udc60 3 subscript \ud835\udc50 4 \\displaystyle s_{4}=s_{3}\\bullet c_{4} = ( \ud835\udc00 \u00af 3 , \ud835\udc00 \u00af 2 \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 2 + \ud835\udc01 \u00af \u200b u 3 ) \u2219 ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 4 ) absent \u2219 superscript \u00af \ud835\udc00 3 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 2 \u00af \ud835\udc01 subscript \ud835\udc62 3 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 4 \\displaystyle=(\\overline{\\mathbf{A}}^{3},\\enspace\\overline{\\mathbf{A}}^{2}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{2}+\\overline{\\mathbf{B}}u_{3})\\bullet(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{4}) (48) = ( \ud835\udc00 \u00af 4 , \ud835\udc00 \u00af 3 \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc00 \u00af 2 \u200b \ud835\udc01 \u00af \u200b u 2 + \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 3 + \ud835\udc01 \u00af \u200b u 4 ) . absent superscript \u00af \ud835\udc00 4 superscript \u00af \ud835\udc00 3 \u00af \ud835\udc01 subscript \ud835\udc62 1 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc01 subscript \ud835\udc62 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 3 \u00af \ud835\udc01 subscript \ud835\udc62 4 \\displaystyle=(\\overline{\\mathbf{A}}^{4},\\enspace\\overline{\\mathbf{A}}^{3}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{A}}^{2}\\overline{\\mathbf{B}}u_{2}+\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{3}+\\overline{\\mathbf{B}}u_{4}). (49)\n\nNote that the second entry of each of the output tuples, , contains the desired computed above. Computing the scan in this way requires four sequential steps since each depends on . Now consider how we can use this binary operator to compute the recurrence in parallel. We will label the output elements of the parallel scan as and define . We will first compute the even indexed elements and , and then compute the odd indexed elements and . We start by applying the binary operator to adjacent pairs of our initial elements to compute and the intermediate result , and we then repeat this process to compute by applying to and :\n\nr 2 = c 1 \u2219 c 2 subscript \ud835\udc5f 2 \u2219 subscript \ud835\udc50 1 subscript \ud835\udc50 2 \\displaystyle r_{2}=c_{1}\\bullet c_{2} = ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 1 ) \u2219 ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 2 ) = ( \ud835\udc00 \u00af 2 , \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc01 \u00af \u200b u 2 ) absent \u2219 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 2 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc01 subscript \ud835\udc62 2 \\displaystyle=(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{1})\\bullet(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{2})=(\\overline{\\mathbf{A}}^{2},\\enspace\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{B}}u_{2}) (50) q 4 = c 3 \u2219 c 4 subscript \ud835\udc5e 4 \u2219 subscript \ud835\udc50 3 subscript \ud835\udc50 4 \\displaystyle q_{4}=c_{3}\\bullet c_{4} = ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 3 ) \u2219 ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 4 ) = ( \ud835\udc00 \u00af 2 , \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 3 + \ud835\udc01 \u00af \u200b u 4 ) absent \u2219 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 3 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 4 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 3 \u00af \ud835\udc01 subscript \ud835\udc62 4 \\displaystyle=(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{3})\\bullet(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{4})=(\\overline{\\mathbf{A}}^{2},\\enspace\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{3}+\\overline{\\mathbf{B}}u_{4}) (51) r 4 = r 2 \u2219 q 4 subscript \ud835\udc5f 4 \u2219 subscript \ud835\udc5f 2 subscript \ud835\udc5e 4 \\displaystyle r_{4}=r_{2}\\bullet q_{4} = ( \ud835\udc00 \u00af 2 , \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc01 \u00af \u200b u 2 ) \u2219 ( \ud835\udc00 \u00af 2 , \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 3 + \ud835\udc01 \u00af \u200b u 4 ) absent \u2219 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc01 subscript \ud835\udc62 2 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 3 \u00af \ud835\udc01 subscript \ud835\udc62 4 \\displaystyle=(\\overline{\\mathbf{A}}^{2},\\enspace\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{B}}u_{2})\\bullet(\\overline{\\mathbf{A}}^{2},\\enspace\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{3}+\\overline{\\mathbf{B}}u_{4}) (52) = ( \ud835\udc00 \u00af 4 , \ud835\udc00 \u00af 3 \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc00 \u00af 2 \u200b \ud835\udc01 \u00af \u200b u 2 + \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 3 + \ud835\udc01 \u00af \u200b u 4 ) . absent superscript \u00af \ud835\udc00 4 superscript \u00af \ud835\udc00 3 \u00af \ud835\udc01 subscript \ud835\udc62 1 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc01 subscript \ud835\udc62 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 3 \u00af \ud835\udc01 subscript \ud835\udc62 4 \\displaystyle=(\\overline{\\mathbf{A}}^{4},\\enspace\\overline{\\mathbf{A}}^{3}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{A}}^{2}\\overline{\\mathbf{B}}u_{2}+\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{3}+\\overline{\\mathbf{B}}u_{4}). (53)\n\nNow we will compute the odd indexed elements and , using the even indexed and , as :\n\nr 1 = r 0 \u2219 c 1 subscript \ud835\udc5f 1 \u2219 subscript \ud835\udc5f 0 subscript \ud835\udc50 1 \\displaystyle r_{1}=r_{0}\\bullet c_{1} = ( I , 0 ) \u2219 ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 1 ) = ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 1 ) absent \u2219 \ud835\udc3c 0 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \\displaystyle=(I,\\enspace 0)\\bullet(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{1})=(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{1}) (54) r 3 = r 2 \u2219 c 3 subscript \ud835\udc5f 3 \u2219 subscript \ud835\udc5f 2 subscript \ud835\udc50 3 \\displaystyle r_{3}=r_{2}\\bullet c_{3} = ( \ud835\udc00 \u00af 2 , \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc01 \u00af \u200b u 2 ) \u2219 ( \ud835\udc00 \u00af , \ud835\udc01 \u00af \u200b u 3 ) = ( \ud835\udc00 \u00af 3 , \ud835\udc00 \u00af 2 \u200b \ud835\udc01 \u00af \u200b u 1 + \ud835\udc00 \u00af \u200b \ud835\udc01 \u00af \u200b u 2 + \ud835\udc01 \u00af \u200b u 3 ) . absent \u2219 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc01 subscript \ud835\udc62 2 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 3 superscript \u00af \ud835\udc00 3 superscript \u00af \ud835\udc00 2 \u00af \ud835\udc01 subscript \ud835\udc62 1 \u00af \ud835\udc00 \u00af \ud835\udc01 subscript \ud835\udc62 2 \u00af \ud835\udc01 subscript \ud835\udc62 3 \\displaystyle=(\\overline{\\mathbf{A}}^{2},\\enspace\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{B}}u_{2})\\bullet(\\overline{\\mathbf{A}},\\enspace\\overline{\\mathbf{B}}u_{3})=(\\overline{\\mathbf{A}}^{3},\\enspace\\overline{\\mathbf{A}}^{2}\\overline{\\mathbf{B}}u_{1}+\\overline{\\mathbf{A}}\\overline{\\mathbf{B}}u_{2}+\\overline{\\mathbf{B}}u_{3}). (55)\n\nNote that the second entry of each of the output tuples, , corresponds to the desired . Inspecting the required dependencies for each application of , we see that and the intermediate result can be computed in parallel. Once and are computed, , and can all be computed in parallel. We have therefore reduced the number of sequential steps required from four in the sequential scan version to two in the parallel scan version. This reduction in sequential steps becomes important when the sequence length is large since, given sufficient processors, the parallel time scales logarithmically with the sequence length. Associativity of binary operator\n\nFinally, for completeness, we show that the binary operator is associative:\n\n( q i \u2219 q j ) \u2219 q k \u2219 \u2219 subscript \ud835\udc5e \ud835\udc56 subscript \ud835\udc5e \ud835\udc57 subscript \ud835\udc5e \ud835\udc58 \\displaystyle(q_{i}\\bullet q_{j})\\bullet q_{k} = ( q j , a \u2299 q i , a , q j , a \u2297 q i , b + q j , b ) \u2219 q k absent \u2219 direct-product subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4e tensor-product subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4f subscript \ud835\udc5e \ud835\udc57 \ud835\udc4f subscript \ud835\udc5e \ud835\udc58 \\displaystyle=\\left(q_{j,a}\\odot q_{i,a},\\enspace q_{j,a}\\otimes q_{i,b}+q_{j,b}\\right)\\bullet q_{k} (56) = ( q k , a \u2299 ( q j , a \u2299 q i , a ) , q k , a \u2297 ( q j , a \u2297 q i , b + q j , b ) + q k , b ) absent direct-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e direct-product subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4e tensor-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e tensor-product subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4f subscript \ud835\udc5e \ud835\udc57 \ud835\udc4f subscript \ud835\udc5e \ud835\udc58 \ud835\udc4f \\displaystyle=\\left(q_{k,a}\\odot(q_{j,a}\\odot q_{i,a}),\\enspace q_{k,a}\\otimes(q_{j,a}\\otimes q_{i,b}+q_{j,b})+q_{k,b}\\right) (57) = ( ( q k , a \u2299 q j , a ) \u2299 q i , a , q k , a \u2297 ( q j , a \u2297 q i , b ) + q k , a \u2297 q j , b + q k , b ) absent direct-product direct-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4e tensor-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e tensor-product subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4f tensor-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e subscript \ud835\udc5e \ud835\udc57 \ud835\udc4f subscript \ud835\udc5e \ud835\udc58 \ud835\udc4f \\displaystyle=\\left((q_{k,a}\\odot q_{j,a})\\odot q_{i,a},\\enspace q_{k,a}\\otimes(q_{j,a}\\otimes q_{i,b})+q_{k,a}\\otimes q_{j,b}+q_{k,b}\\right) (58) = ( ( q k , a \u2299 q j , a ) \u2299 q i , a , ( q k , a \u2299 q j , a ) \u2297 q i , b + q k , a \u2297 q j , b + q k , b ) absent direct-product direct-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4e tensor-product direct-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e subscript \ud835\udc5e \ud835\udc56 \ud835\udc4f tensor-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e subscript \ud835\udc5e \ud835\udc57 \ud835\udc4f subscript \ud835\udc5e \ud835\udc58 \ud835\udc4f \\displaystyle=\\left((q_{k,a}\\odot q_{j,a})\\odot q_{i,a},\\enspace(q_{k,a}\\odot q_{j,a})\\otimes q_{i,b}+q_{k,a}\\otimes q_{j,b}+q_{k,b}\\right) (59) = q i \u2219 ( q k , a \u2299 q j , a , q k , a \u2297 q j , b + q k , b ) absent \u2219 subscript \ud835\udc5e \ud835\udc56 direct-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e subscript \ud835\udc5e \ud835\udc57 \ud835\udc4e tensor-product subscript \ud835\udc5e \ud835\udc58 \ud835\udc4e subscript \ud835\udc5e \ud835\udc57 \ud835\udc4f subscript \ud835\udc5e \ud835\udc58 \ud835\udc4f \\displaystyle=q_{i}\\bullet\\left(q_{k,a}\\odot q_{j,a},\\enspace q_{k,a}\\otimes q_{j,b}+q_{k,b}\\right) (60) = q i \u2219 ( q j \u2219 q k ) absent \u2219 subscript \ud835\udc5e \ud835\udc56 \u2219 subscript \ud835\udc5e \ud835\udc57 subscript \ud835\udc5e \ud835\udc58 \\displaystyle=q_{i}\\bullet(q_{j}\\bullet q_{k}) (61)\n\n\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Mar 13 16:47:57 2024 by LaTeXML"
}