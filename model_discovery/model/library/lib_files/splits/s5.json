{
    "s5-0": "# Simplified State Space LayERS FOR SEQUENCE MODELING \n\nJimmy T.H. Smith ${ }^{*}$, 1,2 , Andrew Warrington ${ }^{*, 2,3}$, Scott W. Linderman ${ }^{2,3}$<br>*Equal contribution.<br>${ }^{1}$ Institute for Computational and Mathematical Engineering, Stanford University.<br>${ }^{2} \\mathrm{Wu}$ Tsai Neurosciences Institute, Stanford University.<br>${ }^{3}$ Department of Statistics, Stanford University.<br>\\{jsmith14, awarring, scott.linderman\\}@stanford.edu. #### Abstract\n\nModels using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the $S 5$ layer. Whereas an $S 4$ layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S 5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages $87.4 \\%$ on the long range arena benchmark, and $98.5 \\%$ on the most difficult Path-X task.",
    "s5-1": "## 1 INTRODUCTION\n\nEfficiently modeling long sequences is a challenging problem in machine learning. Information crucial to solving tasks may be encoded jointly between observations that are thousands of timesteps apart. Specialized variants of recurrent neural networks (RNNs) (Arjovsky et al., 2016; Erichson et al., 2021; Rusch \\& Mishra, 2021; Chang et al., 2019), convolutional neural networks (CNNs) (Bai et al., 2018; Oord et al., 2016; Romero et al., 2022b), and transformers (Vaswani et al., 2017) have been developed to try to address this problem. In particular, many efficient transformer methods have been introduced (Choromanski et al., 2021; Katharopoulos et al., 2020; Kitaev et al., 2020; Beltagy et al., 2020; Gupta \\& Berant, 2020; Wang et al., 2020) to address the standard transformer's quadratic complexity in the sequence length. However, these more efficient transformers still perform poorly on very long-range sequence tasks (Tay et al., 2021). Gu et al. (2021a) presented an alternative approach using structured state space sequence (S4) layers. An S4 layer defines a nonlinear sequence-to-sequence transformation via a bank of many independent single-input, single-output (SISO) linear state space models (SSMs) (Gu et al., 2021b), coupled together with nonlinear mixing layers. Each SSM leverages the HiPPO framework (Gu et al., 2020a) by initializing with specially constructed state matrices. Since the SSMs are linear, each layer can be equivalently implemented as a convolution, which can then be applied efficiently by parallelizing across the sequence length. Multiple S4 layers can be stacked to create a deep sequence model. Such models have achieved significant improvements over previous methods, including on the long range arena (LRA) (Tay et al., 2021) benchmarks specifically designed to stress test long-range sequence models. Extensions have shown good performance on raw audio generation (Goel et al., 2022) and classification of long movie clips (Islam \\& Bertasius, 2022). We introduce a new state space layer that builds on the S 4 layer, the $S 5$ layer, illustrated in Figure 1. S5 streamlines the S4 layer in two main ways. First, S5 uses one multi-input, multi-output (MIMO) SSM in place of the bank of many independent SISO SSMs in S4. Second, S5 uses an efficient and widely implemented parallel scan. This removes the need for the convolutional and frequencydomain approach used by S 4 , which requires a non-trivial computation of the convolution kernel. ![](https://cdn.mathpix.com/cropped/2024_09_17_1f0dc00258b87a7224b1g-02.jpg?height=358&width=1397&top_left_y=279&top_left_x=364)\n\nFigure 1: The computational components of an S5 layer for offline application to a sequence. The S5 layer uses a parallel scan on a diagonalized linear SSM to compute the SSM outputs $\\mathbf{y}_{1: L} \\in \\mathbb{R}^{L \\times H}$. A nonlinear activation function is applied to the SSM outputs to produce the layer outputs. A similar diagram for S 4 is included in Appendix B. The resulting state space layer has the same computational complexity as $S 4$, but operates purely recurrently and in the time domain. We then establish a mathematical relationship between S4 and S5. This connection allows us to inherit the HiPPO initialization schemes that are key to the success of S4. Unfortunately, the specific HiPPO matrix that S 4 uses for initialization cannot be diagonalized in a numerically stable manner for use in S5. However, in line with recent work on the DSS (Gupta et al., 2022) and S4D (Gu et al., 2022) layers, we found that a diagonal approximation to the HiPPO matrix achieves comparable performance. We extend a result from Gu et al. (2022) to the MIMO setting, which justifies the diagonal approximation for use in S5. We leverage the mathematical relationship between S4 and S5 to inform several other aspects of parameterization and initialization, and we perform thorough ablation studies to explore these design choices. The final S5 layer has many desirable properties. It is straightforward to implement (see Appendix A), ${ }^{1}$ enjoys linear complexity in the sequence length, and can efficiently handle time-varying SSMs and irregularly sampled observations (which is intractable with the convolution implementation of S4). S5 achieves state-of-the-art performance on a variety of long-range sequence modeling tasks, with an LRA average of $87.4 \\%$, and $98.5 \\%$ accuracy on the most difficult Path-X task. ## 2 BACKGROUND\n\nWe provide the necessary background in this section prior to introducing the S5 layer in Section 3. ### 2.1 Linear State Space Models\n\nContinuous-time linear SSMs are the core component of both the S4 layer and the S5 layer. Given an input signal $\\mathbf{u}(t) \\in \\mathbb{R}^{U}$, a latent state $\\mathbf{x}(t) \\in \\mathbb{R}^{P}$ and an output signal $\\mathbf{y}(t) \\in \\mathbb{R}^{M}$, a linear continuous-time SSM is defined by the differential equation:\n\n$$\n\\frac{\\mathrm{d} \\mathbf{x}(t)}{\\mathrm{d} t}=\\mathbf{A} \\mathbf{x}(t)+\\mathbf{B u}(t), \\quad \\mathbf{y}(t)=\\mathbf{C x}(t)+\\mathbf{D} \\mathbf{u}(t)\n$$\n\nand is parameterized by a state matrix $\\mathbf{A} \\in \\mathbb{R}^{P \\times P}$, an input matrix $\\mathbf{B} \\in \\mathbb{R}^{P \\times U}$, an output matrix $\\mathbf{C} \\in \\mathbb{R}^{M \\times P}$ and a feedthrough matrix $\\mathbf{D} \\in \\mathbb{R}^{M \\times U}$. For a constant step size, $\\Delta$, the SSM can be discretized using, e.g. Euler, bilinear or zero-order hold $(\\mathrm{ZOH})$ methods to define the linear recurrence\n\n$$\n\\mathbf{x}_{k}=\\overline{\\mathbf{A}} \\mathbf{x}_{k-1}+\\overline{\\mathbf{B}} \\mathbf{u}_{k}, \\quad \\mathbf{y}_{k}=\\overline{\\mathbf{C}} \\mathbf{x}_{k}+\\overline{\\mathbf{D}} \\mathbf{u}_{k}\n$$\n\nwhere the discrete-time parameters are each a function, specified by the discretization method, of the continuous-time parameters. See Iserles (2009) for more information on discretization methods. ### 2.2 Parallelizing Linear State Space Models With Scans\n\nWe use parallel scans to efficiently compute the states of a discretized linear SSM. Given a binary associative operator $\\bullet($ i.e. $(a \\bullet b) \\bullet c=a \\bullet(b \\bullet c))$ and a sequence of $L$ elements $\\left[a_{1}, a_{2}, \\ldots, a_{L}\\right]$, the\n\n[^0]scan operation (sometimes referred to as all-prefix-sum) returns the sequence\n$$\n\\left[a_{1},\\left(a_{1} \\bullet a_{2}\\right), \\ldots,\\left(a_{1} \\bullet a_{2} \\bullet \\ldots \\bullet a_{L}\\right)\\right]\n$$\n\nComputing a length $L$ linear recurrence of a discretized SSM, $\\mathbf{x}_{k}=\\overline{\\mathbf{A}} \\mathbf{x}_{k-1}+\\overline{\\mathbf{B}} \\mathbf{u}_{k}$ as in (2), is a specific example of a scan operation. As discussed in Section 1.4 of Blelloch (1990), parallelizing the linear recurrence of the latent transitions in the discretized SSM above can be computed in a parallel time of $\\mathcal{O}\\left(T_{\\odot} \\log L\\right)$, assuming $L$ processors, where $T_{\\odot}$ represents the cost of matrix-matrix multiplication. For a general matrix $\\overline{\\mathbf{A}} \\in \\mathbb{R}^{P \\times P}, T_{\\odot}$ is $\\mathcal{O}\\left(P^{3}\\right)$. This can be prohibitively expensive in deep learning settings. However, if $\\overline{\\mathbf{A}}$ is a diagonal matrix, the parallel time becomes $\\mathcal{O}(P \\log L)$ with $L$ processors and only requires $\\mathcal{O}(P L)$ space. Finally, we note that efficient parallel scans are implemented in a work-efficient manner, thus the total computational cost of the parallel scan with a diagonal matrix is $\\mathcal{O}(P L)$ operations. See Appendix H for more information on parallel scans. ### 2.3 S4: Structured State Space SeQuence Layers\n\nThe S4 layer (Gu et al., 2021a) defines a nonlinear sequence-to-sequence transformation, mapping from an input sequence $\\mathbf{u}_{1: L} \\in \\mathbb{R}^{L \\times H}$ to an output sequence $\\mathbf{u}_{1: L}^{\\prime} \\in \\mathbb{R}^{L \\times H}$. An S4 layer contains a bank of $H$ independent single-input, single-output (SISO) SSMs with $N$-dimensional states. Each S4 SSM is applied to one dimension of the input sequence. This results in an independent linear transformation from each input channel to each preactivation channel. A nonlinear activation function is then applied to the preactivations. Finally, a position-wise linear mixing layer is applied to combine the independent features and produce the output sequence $\\mathbf{u}_{1: L}^{\\prime}$. Figure 4 a in the appendix illustrates the view of the S4 layer as a bank of independent SSMs. Figure 2a shows an alternative view of S4 as one large SSM with state size $H N$ and block-diagonal state, input and output matrices. Each S4 SSM leverages the HiPPO framework for online function approximation (Gu et al., 2020a) by initializing the state matrices with a HiPPO matrix (most often the HiPPO-LegS matrix). This was demonstrated empirically to lead to strong performance (Gu et al., 2021b;a), and can be shown as approximating long-range dependencies with respect to an infinitely long, exponentially-decaying measure (Gu et al., 2023). While the HiPPO-LegS matrix is not stably diagonalizable (Gu et al., 2021a), it can be represented as a normal plus low-rank (NPLR) matrix. The normal component, referred to as HiPPO-N and denoted $\\mathbf{A}_{\\mathrm{LegS}}^{\\text {Normal }}$, can be diagonalized. Thus, the HiPPO-LegS can be conjugated into a diagonal plus low-rank (DPLR) form, which S4 then utilizes to derive an efficient form of the convolution kernel. This motivates S4's DPLR parameterization. Efficiently applying the S4 layer requires two separate implementations depending on context: a recurrent mode and a convolution mode. For online generation, the SSM is iterated recurrently, much like other RNNs. However, when the entire sequence is available and the observations are evenly spaced, a more efficient convolution mode is used. This takes advantage of the ability to represent the linear recurrence as a one-dimensional convolution between the inputs and a convolution kernel for each of the SSMs. Fast Fourier transforms (FFTs) can then be applied to efficiently parallelize this application. Figure 4a in the appendix illustrates the convolution approach of the S 4 layer for offline processing. We note that while parallel scans could, in principle, allow a recurrent approach to be used in offline scenarios, applying the parallel scan to all $H$ of the $N$-dimensional SSMs would in general be much more expensive than the convolution approach S 4 actually uses. The trainable parameters of each S 4 layer are the $H$ independent copies of the learnable SSM parameters and the $\\mathcal{O}\\left(H^{2}\\right)$ parameters of the mixing layer. For each of the $h \\in\\{1, \\ldots, H\\} \\mathrm{S} 4 \\mathrm{SSMs}$, given a scalar input signal $u^{(h)}(t) \\in \\mathbb{R}$, an S4 SSM uses an input matrix $\\mathbf{B}^{(h)} \\in \\mathbb{C}^{N \\times 1}$, a DPLR parameterized transition matrix $\\mathbf{A}^{(h)} \\in \\mathbb{C}^{N \\times N}$, an output matrix $\\mathbf{C}^{(h)} \\in \\mathbb{C}^{1 \\times N}$, and feedthrough matrix $\\mathbf{D}^{(h)} \\in \\mathbb{R}^{1 \\times 1}$, to produce a signal $y^{(h)}(t) \\in \\mathbb{R}$. To apply the S4 SSMs to discrete sequences, each continuous-time SSM is discretized using a constant timescale parameter $\\Delta^{(h)} \\in \\mathbb{R}_{+}$. The learnable parameters of each SSM are the timescale parameter $\\Delta^{(h)} \\in \\mathbb{R}_{+}$, the continuous-time parameters $\\mathbf{B}^{(h)}, \\mathbf{C}^{(h)}, \\mathbf{D}^{(h)}$, and the DPLR matrix, parameterized by vectors $\\boldsymbol{\\Lambda}^{(h)} \\in \\mathbb{C}^{N}$ and $\\mathbf{p}^{(h)}, \\mathbf{q}^{(h)} \\in \\mathbb{C}^{N}$ representing the diagonal matrix and low-rank terms respectively. For notational compactness we denote the concatenation of the $H$ S4 SSM states at discrete time index $k$ as $\\mathbf{x}_{k}^{(1: H)}=\\left[\\left(\\mathbf{x}_{k}^{(1)}\\right)^{\\top}, \\ldots,\\left(\\mathbf{x}_{k}^{(H)}\\right)^{\\top}\\right]^{\\top}$, and the $H$ SSM outputs as $\\mathbf{y}_{k}=\\left[\\mathbf{y}_{k}^{(1)}, \\ldots, \\mathbf{y}_{k}^{(H)}\\right]^{\\top}$. ![](https://cdn.mathpix.com/cropped/2024_09_17_1f0dc00258b87a7224b1g-04.jpg?height=556&width=1391&top_left_y=275&top_left_x=367)\n(a) Internal structure of a single S4 layer (Gu et al., 2021a) when viewed as a block-diagonal system. ![](https://cdn.mathpix.com/cropped/2024_09_17_1f0dc00258b87a7224b1g-04.jpg?height=467&width=1387&top_left_y=889&top_left_x=369)\n(b) Internal structure of a single S5 layer. Figure 2: Schematic of the internal structure of a discretized S4 layer (Gu et al., 2021a) (top) and S5 layer (bottom). Note $\\mathbf{D}$ is omitted for simplicity. We view an S4 layer as a single block-diagonal SSM with a latent state of size $H N$, followed by a nonlinearity and mixing layer to mix the independent features. (b) In contrast, the S 5 layer uses a dense, MIMO linear SSM with latent size $P \\ll H N$. ## 3 THE S5 LAYER\n\nIn this section we present the S 5 layer. We describe its structure, parameterization and computation, particularly focusing on how each of these differ from S 4 . ### 3.1 S5 Structure: From SISO to MIMO\n\nThe S5 layer replaces the bank of SISO SSMs (or large block-diagonal system) in S4 with a multiinput, multi-output (MIMO) SSM, as in (1), with a latent state size $P$, and input and output dimension $H$. The discretized version of this MIMO SSM can be applied to a vector-valued input sequence $\\mathbf{u}_{1: L} \\in \\mathbb{R}^{L \\times H}$, to produce a vector-valued sequence of SSM outputs (or preactivations) $\\mathbf{y}_{1: L} \\in \\mathbb{R}^{L \\times H}$, using latent states $\\mathbf{x}_{k} \\in \\mathbb{R}^{P}$. A nonlinear activation function is then applied to produce a sequence of layer outputs $\\mathbf{u}_{1: L}^{\\prime} \\in \\mathbb{R}^{\\dot{L} \\times H}$. See Figure 2 b for an illustration. Unlike $S 4$, we do not require an additional position-wise linear layer, since these features are already mixed. We note here that compared to the $H N$ latent size of the block-diagonal SSM in the S4 layer, S5's latent size $P$ can be significantly smaller, allowing for the use of efficient parallel scans, as we discuss in Section 3.3. ### 3.2 S5 PARAMETERIZATION: DIAGONALIZED DYNAMICS\n\nThe parameterization of the S5 layer's MIMO SSM is motivated by the desire to use efficient parallel scans. As discussed in Section 2.2, a diagonal state matrix is required to efficiently compute the linear recurrence using a parallel scan. Thus, we diagonalize the system, writing the continuous-time state matrix as $\\mathbf{A}=\\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{-1}$, where $\\boldsymbol{\\Lambda} \\in \\mathbb{C}^{P \\times P}$ denotes the diagonal matrix containing the eigenvalues\nand $\\mathbf{V} \\in \\mathbb{C}^{P \\times P}$ corresponds to the eigenvectors. Therefore, we can diagonalize the continuous-time latent dynamics from (1) as\n\n$$\n\\frac{\\mathrm{d} \\mathbf{V}^{-1} \\mathbf{x}(t)}{\\mathrm{d} t}=\\mathbf{\\Lambda} \\mathbf{V}^{-1} \\mathbf{x}(t)+\\mathbf{V}^{-1} \\mathbf{B u}(t)\n$$\n\nDefining $\\tilde{\\mathbf{x}}(t)=\\mathbf{V}^{-1} \\mathbf{x}(t), \\tilde{\\mathbf{B}}=\\mathbf{V}^{-1} \\mathbf{B}$, and $\\tilde{\\mathbf{C}}=\\mathbf{C V}$ gives a reparameterized system,\n\n$$\n\\frac{\\mathrm{d} \\tilde{\\mathbf{x}}(t)}{\\mathrm{d} t}=\\boldsymbol{\\Lambda} \\tilde{\\mathbf{x}}(t)+\\tilde{\\mathbf{B}} \\mathbf{u}(t), \\quad \\mathbf{y}(t)=\\tilde{\\mathbf{C}} \\tilde{\\mathbf{x}}(t)+\\mathbf{D} \\mathbf{u}(t)\n$$\n\nThis is a linear SSM with a diagonal state matrix. This diagonalized system can be discretized with a timescale parameter $\\Delta \\in \\mathbb{R}_{+}$using the ZOH method to give another diagonalized system with parameters\n\n$$\n\\overline{\\boldsymbol{\\Lambda}}=e^{\\boldsymbol{\\Lambda} \\Delta}, \\quad \\overline{\\mathbf{B}}=\\boldsymbol{\\Lambda}^{-1}(\\overline{\\boldsymbol{\\Lambda}}-\\mathbf{I}) \\tilde{\\mathbf{B}}, \\quad \\overline{\\mathbf{C}}=\\tilde{\\mathbf{C}}, \\quad \\overline{\\mathbf{D}}=\\mathbf{D}\n$$\n\nIn practice, we use a vector of learnable timescale parameters $\\boldsymbol{\\Delta} \\in \\mathbb{R}^{P}$ (see Section 4.3) and restrict the feedthrough matrix $\\mathbf{D}$ to be diagonal. The S 5 layer therefore has the learnable parameters: $\\tilde{\\mathbf{B}} \\in \\mathbb{C}^{P \\times H}, \\tilde{\\mathbf{C}} \\in \\mathbb{C}^{H \\times P}, \\operatorname{diag}(\\mathbf{D}) \\in \\mathbb{R}^{H}, \\operatorname{diag}(\\boldsymbol{\\Lambda}) \\in \\mathbb{C}^{P}$, and $\\boldsymbol{\\Delta} \\in \\mathbb{R}^{P}$. Initialization Prior work showed that the performance of deep state space models are sensitive to the initialization of the state matrix (Gu et al., 2021b;a). We discussed in Section 2.2 that state matrices must be diagonal for efficient application of parallel scans. We also discussed in Section 2.3 that the HiPPO-LegS matrix cannot be diagonalized stably, but that the HiPPO-N matrix can be. In Section 4 we connect the dynamics of S 5 to S 4 to suggest why initializing with HiPPO-like matrices may also work well in the MIMO setting. We support this empirically, finding that diagonalizing the HiPPO-N matrix leads to good performance, and perform ablations in Appendix E to compare to other initializations. We note that DSS (Gupta et al., 2022) and S4D (Gu et al., 2022) layers also found strong performance in the SISO setting by using a diagonalization of the HiPPO-N matrix. Conjugate Symmetry The complex eigenvalues of a diagonalizable matrix with real entries always occur in conjugate pairs. We enforce this conjugate symmetry by using half the number of eigenvalues and latent states. This ensures real outputs and reduces the runtime and memory usage of the parallel scan by a factor of two.",
    "s5-2": "This idea is also discussed in Gu et al. (2022). ### 3.3 S5 Computation: Fully RECurRent\n\nCompared to the large $H N$ effective latent size of the block-diagonal S4 layer, the smaller latent dimension of the S 5 layer $(P)$ allows the use of efficient parallel scans when the entire sequence is available. The S 5 layer can therefore be efficiently used as a recurrence in the time domain for both online generation and offline processing. Parallel scans and the continuous-time parameterization also allow for efficient handling of irregularly sampled time series and other time-varying SSMs, by simply supplying a different $\\overline{\\mathbf{A}}_{k}$ matrix at each step. We leverage this feature and apply S5 to irregularly sampled data in Section 6.3. In contrast, the convolution of the $S 4$ layer requires a time invariant system and regularly spaced observations. ### 3.4 Matching THE COMPUTATIONAL EFFICIENCY OF S4 ANd S5\n\nA key design desiderata for S 5 was matching the computational complexity of S4 for both online generation and offline recurrence. The following proposition guarantees that their complexities are of the same order if S5's latent size $P=\\mathcal{O}(H)$. Proposition 1. Given an S4 layer with H input/output features, an S5 layer with H input/output features and a latent size $P=\\mathcal{O}(H)$ has the same order of magnitude complexity as an S4 layer in terms of both runtime and memory usage.",
    "s5-3": "Proof. See Appendix C.1. We also support this proposition with empirical comparisons in Appendix C.2. ## 4 RELATIONSHIP BETWEEN S4 ANd S5\n\nWe now establish a relationship between the dynamics of S5 and S4. In Section 4.1 we show that, under certain conditions, the outputs of the S5 SSM can be interpreted as a projection of the latent states computed by a particular S4 system. This interpretation motivates using HiPPO initializations for S5, which we discuss in more detail in Section 4.2. In Section 4.3 we discuss how the conditions required to relate the dynamics further motivate initialization and parameterization choices. ### 4.1 Different Output Projections of EQuivalent DynAmiCs\n\nWe compare the dynamics of S4 and S5 under some simplifying assumptions:\nAssumption 1. We consider only $H$-dimensional to $H$-dimensional sequence maps. Assumption 2. We assume the state matrix of each S4 SSM is identical, $\\mathbf{A}^{(h)}=\\mathbf{A} \\in \\mathbb{C}^{N \\times N}$. Assumption 3. We assume the timescales of each S4 SSM are identical, $\\Delta^{(h)}=\\Delta \\in \\mathbb{R}_{+}$\nAssumption 4. We assume that the same state matrix $\\mathbf{A}$ is used in $S 5$ as in $S 4$ (also cf. Assumption 2). Note this also specifies the $S 5$ latent size $P=N$. We also assume the $S 5$ input matrix is the horizontal concatenation of the column input vectors used by S4: $\\mathbf{B} \\triangleq\\left[\\mathbf{B}^{(1)}|\\ldots| \\mathbf{B}^{(H)}\\right]$. We will discuss relaxing these assumptions shortly, but under these conditions it is straightforward to derive a relationship between the dynamics of S4 and S5:\nProposition 2. Consider an S5 layer, with state matrix A, input matrix $\\mathbf{B}$ and some output matrix $\\mathbf{C}$ (cf. Assumption 1); and an S4 layer, where each of the H S4 SSMs has state matrix A (cf. Assumption 2, 4) and input vector $\\mathbf{B}^{(h)}$ (cf. Assumption 4). If the $S 4$ and $S 5$ layers are discretized with the same timescales (cf. Assumption 3), then the S5 SSM produces outputs, $\\mathbf{y}_{k}$, equivalent to a linear combination of the latent states of the H S4 SSMs, $\\mathbf{y}_{k}=\\mathbf{C}^{\\text {equiv }} \\mathbf{x}_{k}^{(1: H)}$, where $\\mathbf{C}^{\\text {equiv }}=[\\mathbf{C} \\cdots \\mathbf{C}]$. ## Proof. See Appendix D.2. Importantly, the S5 SSM outputs are not equal to the outputs of the block-diagonal S4 SSM. Instead they are equivalent to the outputs of the block-diagonal S4 SSM with modified output matrix $\\mathbf{C}^{\\text {equiv }}$. Under the assumptions, however, the underlying state dynamics are equivalent. Recalling that initializing the S4 dynamics with HiPPO was key to performance (Gu et al., 2021a), the relationship established in Proposition 2 motivates using HiPPO initializations for S5, as we now discuss. ### 4.2 DIAGONALIZABLE INITIALIZATION\n\nIdeally, given the interpretation above, we would initialize S 5 with the exact HiPPO-LegS matrix. Unfortunately, as discussed in Section 2.3, this matrix is not stably diagonalizable, as is required for the efficient parallel scans used for S5.",
    "s5-4": "However, Gupta et al. (2022) and Gu et al. (2022) showed empirically that removing the low rank terms and initializing with the diagonalized HiPPO-N matrix still performed well. Gu et al. (2022) offered a theoretical justification for the use of this normal approximation for single-input systems: in the limit of infinite state dimension, the linear ODE with HiPPO-N state matrix produces the same dynamics as an ODE with the HiPPO-LegS matrix. Using linearity, it is straightforward to extend this result to the multi-input system that S5 uses:\nCorollary 1 (Extension of Theorem 3 in Gu et al. (2022)). Consider $\\mathbf{A}_{\\text {LegS }} \\in \\mathbb{R}^{N \\times N}$, $\\mathbf{A}_{\\text {LegS }}^{\\text {Normal }} \\in$ $\\mathbb{R}^{N \\times N}, \\mathbf{B}_{\\mathrm{LegS}} \\in \\mathbb{R}^{N \\times H}, \\mathbf{P}_{\\mathrm{LegS}} \\in \\mathbb{R}^{N}$ as defined in Appendix B.1.1. Given vector-valued inputs $\\mathbf{u}(t) \\in \\mathbb{R}^{H}$, the ordinary differential equation $\\frac{\\mathrm{d} \\mathbf{x}^{\\prime}(t)}{\\mathrm{d} t}=\\mathbf{A}_{\\mathrm{LegS}}^{\\mathrm{Normal}} \\mathbf{x}^{\\prime}(t)+\\frac{1}{2} \\mathbf{B}_{\\mathrm{LegS}} \\mathbf{u}(t)$ converges to $\\frac{\\mathrm{d} \\mathbf{x}(t)}{\\mathrm{d} t}=\\mathbf{A}_{\\mathrm{LegS}} \\mathbf{x}(t)+\\mathbf{B}_{\\mathrm{LegS}} \\mathbf{u}(t)$ as $N \\rightarrow \\infty$\n\nWe include a simple proof of this extension in Appendix D.3. This extension motivates the use of HiPPO-N to initialize S5's MIMO SSM. Note that S4D (the diagonal extension of S4) uses the same HiPPO-N matrix. Thus, when under the assumptions in Proposition 2, an S5 SSM in fact produces outputs that are equivalent to a linear combination of the latent states produced by S4D's SSMs. Our empirical results in Section 6 suggest that S5 initialized with the HiPPO-N matrix performs just as well as S 4 initialized with the HiPPO-LegS matrix. ### 4.3 RELAXING THE ASSUMPTIONS\n\nWe now revisit the assumptions required for Proposition 2, since they only relate a constrained version of S5 to a constrained version of S4. Regarding Assumption 2, Gu et al. (2021a) report that S4 models with tied state matrices can still perform well, though allowing different state matrices often yields higher performance. Likewise, requiring a single scalar timescale across all of the S4 SSMs, per Assumption 3, is restrictive. S4 typically learns different timescale parameters for each SSM (Gu et al., 2023) to capture different timescales in the data. To relax these assumptions, note that Assumption 4 constrains S 5 to have dimension $P=N$, and $N$ is typically much smaller than the dimensionality of the inputs, $H$. Proposition 1 established that S5 can match S4's complexity with $P=\\mathcal{O}(H)$. By allowing for larger latent state sizes, Assumptions 2 and 3 can be relaxed, as discussed in Appendix D.4. We also discuss how this relaxation motivates a block-diagonal initialization with HiPPO-N matrices on the diagonal. Finally, to further relax the tied timescale assumptions, we note that in practice, we find improved performance by learning $P$ different timescales (one per state).",
    "s5-5": "See Appendix D. 5 for further discussion of this empirical finding and the ablations in Appendix E.1. ## 5 RELATED WORK\n\nS5 is most directly related to S4 and its other extensions, which we have discussed thoroughly. However, there is prior literature that uses similar ideas to those developed here. For example, prior work studied approximating nonlinear RNNs with stacks of linear RNNs connected by nonlinear layers, while also using parallel scans (Martin \\& Cundy, 2018). Martin \\& Cundy (2018) showed that several efficient RNNs, such as QRNNs (Bradbury et al., 2017) and SRUs (Lei et al., 2018), fall into a class of linear surrogate RNNs that can leverage parallel scans. Kaul (2020) also used parallel scans for an approach that approximates RNNs with stacks of discrete-time single-input, multi-output (SIMO) SSMs. However, S4 and S5 are the only methods to significantly outperform other comparable state-of-the-art nonlinear RNNs, transformers and convolution approaches. Our ablation study in Appendix E. 2 suggests that this performance gain over prior attempts at parallelized linear RNNs is likely due to the continuous-time parameterization and the HiPPO initialization. ## 6 EXPERIMENTS\n\nWe now compare empirically the performance of the S5 layer to the S4 layer and other baseline methods. We use the S 5 layer as a drop-in replacement for the S 4 layer. The architecture consists of a linear input encoder, stacks of S5 layers, and a linear output decoder (Gu et al., 2021a). For all experiments we choose the S 5 dimensions to ensure similar computational complexities as S 4 , following the conditions discussed in Section 3.3, as well as comparable parameter counts. The results we present show that the S5 layer matches the performance and efficiency of the S4 layer. We include in the appendix further ablations, baselines and runtime comparisons. ### 6.1 LONG RANGE ARENA\n\nThe long range arena (LRA) benchmark (Tay et al., 2021) is a suite of six sequence modeling tasks, with sequence lengths from 1,024 to over 16,000 . The suite was specifically developed to benchmark the performance of architectures on long-range modeling tasks (see Appendix G for more details). Table 1 presents S5's LRA performance in comparison to other methods. S5 achieves the highest average score among methods that have linear complexity in sequence length (most notably S4, S4D, and the concurrent works: Liquid-S4 (Hasani et al., 2023) and Mega-chunk (Ma et al., 2023)). Most significantly, S5 achieves the highest score among all models (including Mega (Ma et al., 2023)) on the Path-X task, which has by far the longest sequence length of the tasks in the benchmark. ### 6.2 RAW SPEECH ClaSSIFICATION\n\nThe Speech Commands dataset (Warden, 2018) contains high-fidelity sound recordings of different human readers reciting a word from a vocabulary of 35 words. The task is to classify which word was spoken. We show in Table 2 that S5 outperforms the baselines, outperforms previous S4 methods and performs similarly to the concurrent Liquid-S4 method (Hasani et al., 2023). As S4 and S5 methods\n\nTable 1: Test accuracy on the LRA benchmark tasks (Tay et al., 2021). $\\boldsymbol{X}$ indicates the model did not exceed random guessing. We include an expanded table, Table 7 , with full citations and error bars in the appendix. We follow the procedure reported in Gu et al. (2021a; 2022) and report means across three seeds for S4, S4D (as reported by Gu et al. (2021a; 2022)) and S5. Bold scores indicate highest performance, underlined scores indicate second placed performance. We also include the results for the concurrent methods Liquid-S4 (Hasani et al., 2023) and Mega (Ma et al., 2023). Unlike S4 methods and S5, the best Mega model retains the transformer's $\\mathcal{O}\\left(L^{2}\\right)$ complexity. | Model <br> (Input length) | ListOps <br> $(2,048)$ | Text <br> $(4,096)$ | Retrieval <br> $(4,000)$ | Image <br> $(1,024)$ | Pathfinder <br> $(1,024)$ | Path-X <br> $(16,384)$ | Avg. |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | $\\boldsymbol{x}$ | 53.66 |\n| Luna-256 | 37.25 | 64.57 | 79.29 | 47.38 | 77.72 | $\\boldsymbol{x}$ | 59.37 |\n| H-Trans.-1D | 49.53 | 78.69 | 63.99 | 46.05 | 68.78 | $\\boldsymbol{x}$ | 61.41 |\n| CCNN | 43.60 | 84.08 | $\\boldsymbol{x}$ | 88.90 | 91.51 | $\\boldsymbol{x}$ | 68.02 |\n| Mega $\\left(\\mathcal{O}\\left(L^{2}\\right)\\right)$ | $\\mathbf{6 3 .",
    "s5-6": "1 4}$ | $\\mathbf{9 0 . 4 3}$ | $\\underline{91.25}$ | $\\mathbf{9 0 . 4 4}$ | $\\mathbf{9 6 . 0 1}$ | $\\underline{97.98}$ | $\\mathbf{8 8 . 2 1}$ |\n| Mega-chunk $(\\mathcal{O}(L))$ | 58.76 | $\\underline{90.19}$ | 90.97 | 85.80 | 94.41 | 93.81 | 85.66 |\n| S4D-LegS | 60.47 | 86.18 | 89.46 | 88.19 | 93.06 | 91.95 | 84.89 |\n| S4-LegS | 59.60 | 86.82 | 90.90 | 88.65 | 94.20 | 96.35 | 86.09 |\n| Liquid-S4 | $\\underline{62.75}$ | 89.02 | 91.20 | $\\underline{89.50}$ | 94.8 | 96.66 | 87.32 |\n| S5 | 62.15 | 89.31 | $\\mathbf{9 1 . 4 0}$ | 88.00 | $\\underline{95.33}$ | $\\mathbf{9 8 . 5 8}$ | $\\underline{87.46}$ |\n\nTable 2: Test accuracy on 35-way Speech Commands classification task (Warden, 2018). We include an expanded table, Table 8, with error bars in the appendix. Training examples are one-second 16 kHz audio waveforms. Last column indicates 0 -shot testing at 8 kHz (constructed by naive decimation). As in Gu et al. (2022), the mean across three random seeds is reported. Performance for the baselines InceptionNet through to S4D-Lin are reported from Gu et al. (2022). | Model <br> (Input length) | Parameters | 16 kHz <br> $(16,000)$ | 8 kHz <br> $(8,000)$ |\n| :--- | :---: | :---: | :---: |\n| InceptionNet (Nonaka \\& Seita, 2021) | 481 K | 61.24 | 05.18 |\n| ResNet-1 (Nonaka \\& Seita, 2021) | 216 K | 77.86 | 08.74 |\n| XResNet-50 (Nonaka \\& Seita, 2021) | 904 K | 83.01 | 07.72 |\n| ConvNet (Nonaka \\& Seita, 2021) | 26.2 M | 95.51 | 07.26 |\n| S4-LegS (Gu et al., 2021a) | 307 K | 96.08 | $\\underline{91.32}$ |\n| S4D-LegS (Gu et al., 2022) | 306 K | 95.83 | 91.08 |\n| Liquid-S4 (Hasani et al., 2023) | 224 K | $\\mathbf{9 6 . 7 8}$ | 90.00 |\n| S5 | 280 K | $\\underline{96.52}$ | $\\mathbf{9 4 . 5 3}$ |\n\nare parameterized in continuous-time, these models can be applied to datasets with different sampling rates without the need for re-training, simply by globally re-scaling the timescale parameter $\\Delta$ by the ratio between the new and old sampling rates. The result of applying the best S 5 model trained on 16 kHz data, to the speech data sampled (via decimation) at 8 kHz , without any additional fine-tuning, is also presented in Table 2. S5 also improves this metric over the baseline methods. ### 6.3 VARIABLE OBSERVATION INTERVAL\n\nThe final application we study here highlights how S5 can naturally handle observations received at irregular intervals. S5 does so by supplying a different $\\Delta_{t}$ value to the discretization at each step. We use the pendulum regression example presented by Becker et al.",
    "s5-7": "(2019) and Schirmer et al. (2022), illustrated in Figure 3. The input sequence is a sequence of $L=50$ images, each $24 \\times 24$ pixels in size, that has been corrupted with a correlated noise process and sampled at irregular intervals from a continuous trajectory of duration $T=100$. The targets are the sine and cosine of the angle of the pendulum, which follows a nonlinear dynamical system. The velocity is unobserved. We match the\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_1f0dc00258b87a7224b1g-09.jpg?height=399&width=1361&top_left_y=272&top_left_x=382)\n\nFigure 3: Illustration of the pendulum regression example. Shown in the top row are the images used as input at the time points indicated. Shown on the bottom are the values of $\\sin \\left(\\theta_{t}\\right)$ and $\\cos \\left(\\theta_{t}\\right)$, where $\\theta_{t}$ is the angle of the pendulum at time $t$, that are used as the regression targets. Table 3: Regression MSE $\\times 10^{-3}$ (mean $\\pm \\mathrm{std}$ ) and relative application speed on the pendulum regression task on a held-out test set. Performance for the baselines, mTAND through to CRU, are reported from Schirmer et al. (2022). We include an expanded table, Table 9, and further details in the appendix. Results for $C R U$ (our run) and $S 5$ are across twenty seeds. | Model | Relative speed | Regression MSE $\\left(\\times 10^{-3}\\right)$ |\n| :--- | :---: | :---: |\n| mTAND (Shukla \\& Marlin, 2021) | $12.2 \\times$ | $65.64(4.05)$ |\n| RKN (Becker et al., 2019) | $1.9 \\times$ | $8.43(0.61)$ |\n| RKN- $\\Delta_{t}$ (Becker et al., 2019) | $1.9 \\times$ | $5.09(0.40)$ |\n| ODE-RNN (Rubanova et al., 2019) | $1.0 \\times$ | $7.26(0.41)$ |\n| CRU (Schirmer et al., 2022) | $1.0 \\times$ | $4.63(1.07)$ |\n| CRU (our run) | $1.0 \\times$ | $\\underline{3.94}(0.21)$ |\n| S5 | $\\mathbf{8 6} \\times$ | $\\mathbf{3 . 4 1}(0.27)$ |\n\narchitecture, parameter count and training procedure of Schirmer et al.",
    "s5-8": "(2022). Table 3 summarizes the results of this experiment. S5 outperforms CRU on the regression task, recovering a lower mean error. Furthermore, S5 is markedly faster than CRU on the same hardware. ### 6.4 PiXEl-LEVEL 1-D IMAGE ClasSIfiCation\n\nTable 10 in Appendix F. 4 shows results of S5 on other common benchmarks including sequential MNIST, permuted sequential MNIST and sequential CIFAR (color). We see that S5 broadly matches the performance of S4, and outperforms a range of state-of-the-art RNN-based methods. ## 7 CONCLUSION\n\nWe introduce the S5 layer for long-range sequence modeling. The S5 layer modifies the internal structure of the S 4 layer, and replaces the frequency-domain approach used by S 4 with a purely recurrent, time-domain approach leveraging parallel scans. S5 achieves high performance while retaining the computational efficiency of S4. S5 also provides further opportunities. For instance, unlike the convolutional S4 methods, the parallel scan unlocks the ability to efficiently and easily process time-varying SSMs whose parameters can vary with time. Section 6.3 illustrated an example of this for sequences sampled at variable sampling rates. The concurrently developed method, Liquid-S4 (Hasani et al., 2023), uses an input-dependent bilinear dynamical system and highlights further opportunities for time-varying SSMs. The more general MIMO SSM design will also enable connections to be made with classical probabilistic state space modeling as well as more recent work on parallelizing filtering and smoothing operations (S\u00e4rkk\u00e4 \\& Garc\u00eda-Fern\u00e1ndez, 2020). More broadly, we hope the simplicity and generality of the S5 layer can expand the use of state space layers in deep sequence modeling and lead to new formulations and extensions. ## AcKnOWLEDGEMENTS AND DisCLOSURE OF Funding\n\nWe thank Albert Gu for his thorough and insightful feedback. We also acknowledge The Annotated S4 Blog (Rush \\& Karamcheti, 2022) which inspired our JAX implementation. This work was supported by grants from the Simons Collaboration on the Global Brain (SCGB 697092), the NIH BRAIN Initiative (U19NS113201 and R01NS113119), and the Sloan Foundation. Some of the computation for this work was made possible by Stanford Data Science Microsoft Education Azure cloud credits. ## REFERENCES\n\nMartin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International Conference on Machine Learning, pp. 1120-1128. PMLR, 2016. Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In International Conference on Learning Representations, 2019. Philipp Becker, Harit Pandya, Gregor Gebhardt, Cheng Zhao, C James Taylor, and Gerhard Neumann. Recurrent kalman networks: Factorized inference in high-dimensional deep feature spaces.",
    "s5-9": "In International Conference on Machine Learning, pp. 544-552. PMLR, 2019. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer.",
    "s5-10": "arXiv preprint arXiv:2004.05150, 2020. Guy Blelloch. Prefix sums and their applications. Technical report, Tech. rept. CMU-CS-90-190. School of Computer Science, Carnegie Mellon, 1990. James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2017. Bo Chang, Minmin Chen, Eldad Haber, and Ed Chi. AntisymmetricRNN: A dynamical system view on recurrent neural networks. In International Conference on Learning Representations, 2019. Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks. Advances in Neural Information Processing Systems, 30, 2017. Ricky Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. Narsimha Reddy Chilkuri and Chris Eliasmith. Parallelizing Legendre memory unit training.",
    "s5-11": "In International Conference on Machine Learning, pp. 1898-1907. PMLR, 2021. Kyunghyun Cho, Bart van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. Syntax, Semantics and Structure in Statistical Translation, pp.",
    "s5-12": "103, 2014. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. Yann Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International Conference on Machine Learning, pp. 933-941. PMLR, 2017. Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. GRU-ODE-Bayes: Continuous modeling of sporadically-observed time series. Advances in neural information processing systems, 32, 2019. N. Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021. Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 7616-7633. PMLR, 17-23 Jul 2022. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent memory with optimal polynomial projections.",
    "s5-13": "Advances in Neural Information Processing Systems, 33: $1474-1487,2020 \\mathrm{a}$. Albert Gu, Caglar Gulcehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pp. 3800-3809. PMLR, 2020b. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021a. Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021b. Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations, 2023. Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for transformers, 2020. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): $1735-1780,1997$. Arieh Iserles. A first course in the numerical analysis of differential equations. 44. Cambridge university press, 2009. Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pp. 87-104, 2022. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Shiva Kaul. Linear dynamical systems as a core computational primitive. Advances in Neural Information Processing Systems, 33:16808-16820, 2020. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. Alex Krizhevsky. Learning multiple layers of features from tiny images. Master's thesis, University of Toronto, 2009. Richard Ladner and Michael Fischer. Parallel prefix computation. Journal of the ACM (JACM), 27 (4):831-838, 1980. Sivaramakrishnan Lakshmivarahan and Sudarshan Dhall. Parallel computing using the prefix problem.",
    "s5-14": "Oxford University Press, 1994. James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4296-4313, 2022. Tao Lei, Yu Zhang, Sida Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly parallelizable recurrence.",
    "s5-15": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4470-4481, 2018. Mario Lezcano-Casado and David Mart\u0131nez-Rubio. Cheap orthogonal constraints in neural networks: A simple parametrization of the orthogonal and unitary group.",
    "s5-16": "In International Conference on Machine Learning, pp. 3794-3803. PMLR, 2019. Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network (INDRNN): Building a longer and deeper RNN.",
    "s5-17": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5457-5466, 2018. Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning longrange spatial dependencies with horizontal gated recurrent units.",
    "s5-18": "Advances in Neural Information Processing Systems, 31, 2018. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34, 2021. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention.",
    "s5-19": "In International Conference on Learning Representations, 2023. Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142-150, 2011. Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018. Nikita Nangia and Samuel Bowman. ListOps: A diagnostic dataset for latent tree learning.",
    "s5-20": "NAACL HLT 2018, pp. 92, 2018. Naoki Nonaka and Jun Seita. In-depth benchmarking of deep neural network architectures for ecg diagnosis. In Machine Learning for Healthcare Conference, pp. 414-439. PMLR, 2021. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.",
    "s5-21": "arXiv preprint arXiv:1609.03499, 2016. Dragomir Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. The ACL anthology network corpus. ACL-IJCNLP 2009, pp. 54, 2009. David Romero, Robert-Jan Bruintjes, Jakub Mikolaj Tomczak, Erik Bekkers, Mark Hoogendoorn, and Jan van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. In International Conference on Learning Representations, 2021. David Romero, David Knigge, Albert Gu, Erik Bekkers, Efstratios Gavves, Jakub Tomczak, and Mark Hoogendoorn. Towards a general purpose CNN for long range dependencies in ND. arXiv preprint arXiv:2206.03398, 2022a. David Romero, Anna Kuzina, Erik Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogendoorn. CKConv: Continuous kernel convolution for sequential data. In International Conference on Learning Representations, 2022b. Yulia Rubanova, Ricky Chen, and David Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. Advances in Neural Information Processing Systems, 32, 2019. T. Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long time dependencies.",
    "s5-22": "In International Conference on Machine Learning, pp. 9168-9178. PMLR, 2021. Sasha Rush and Sidd Karamcheti. The Annotated S4. In Blog Track at ICLR 2022, 2022. URL https://srush.github.io/annotated-s $4 /$. Simo S\u00e4rkk\u00e4 and \u00c1ngel F Garc\u00eda-Fern\u00e1ndez. Temporal parallelization of Bayesian smoothers. IEEE Transactions on Automatic Control, 66(1):299-306, 2020. Mona Schirmer, Mazin Eltayeb, Stefan Lessmann, and Maja Rudolph. Modeling irregular time series with continuous recurrent units. In International Conference on Machine Learning, pp. 19388-19405. PMLR, 2022. Satya Narayan Shukla and Benjamin Marlin. Multi-time attention networks for irregularly sampled time series.",
    "s5-23": "In International Conference on Learning Representations, 2021. Masayuki Tanaka. Weighted sigmoid gate unit for an activation function of deep neural network.",
    "s5-24": "Pattern Recognition Letters, 135:354-359, 2020. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. Trieu Trinh, Andrew Dai, Thang Luong, and Quoc Le. Learning longer-term dependencies in RNNs with auxiliary losses. In International Conference on Machine Learning, pp. 4965-4974. PMLR, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre Memory Units: Continuous-time representation in recurrent neural networks.",
    "s5-25": "Advances in Neural Information Processing Systems, 32, 2019 . Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.",
    "s5-26": "arXiv preprint arXiv:2006.04768, 2020. Pete Warden. Speech Commands: A dataset for limited-vocabulary speech recognition.",
    "s5-27": "arXiv preprint arXiv:1804.03209, 2018. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A Nystr\u00f6m-based algorithm for approximating self-attention.",
    "s5-28": "In Proceedings of the AAAI Conference on Artificial Intelligence., volume 35, pp. 14138. NIH Public Access, 2021. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33:17283-17297, 2020. Zhenhai Zhu and Radu Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for sequences.",
    "s5-29": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3801-3815, 2021. ## APPEndiX For: Simplified State Space Layers for SEQUENCe Modeling\n\n## CONTENTS:\n\n- Appendix A: JAX Implementation of S5 Layer. - Appendix B: S5 Layer Details. - Appendix C: Computational Efficiency of S5. - Appendix D: Relationship Between S4 and S5. - Appendix E: Ablations. - Appendix F: Supplementary Results.",
    "s5-30": "- Appendix G: Experiment Configurations. - Appendix H: Background on Parallel Scans for Linear Recurrences. ## A JAX IMPLEMENTATION OF S5 LAYER\n\n```\nimport jax\nimport jax.numpy as np\nparallel_scan = jax.lax.associative_scan\ndef discretize(Lambda, B_tilde, Delta):\n    \"\"\" Discretize a diagonalized, continuous-time linear SSM\n        Args:\n            Lambda (complex64): diagonal state matrix (P,)\n            B_tilde (complex64): input matrix (P, H)\n            Delta (float32): discretization step sizes (P,)\n        Returns:\n            discretized Lambda_bar (complex64), B_bar (complex64) (P,), (P,H)\"\"\"\n    Identity = np.ones(Lambda.shape[0])\n    Lambda_bar = np.exp(Lambda * Delta)\n    B_bar = (1 / Lambda * (Lambda_bar - Identity))[..., None] * B_tilde\n    return Lambda_bar, B_bar\ndef binary_operator(element_i, element_j):\n    \"\"\" Binary operator for parallel scan of linear recurrence. Assumes a diagonal matrix A. Args:\n            element_i: tuple containing A_i and Bu_i at position i (P,), (P,)\n            element_j: tuple containing A_j and Bu_j at position j (P,), (P,)\n        Returns:\n            new element ( A_out, Bu_out ) \"\"\"\n    A_i, Bu_i = element_i\n    A_j, Bu_j = element_j\n    return A_j * A_i, A_j * Bu_i + Bu_j\ndef apply_ssm(Lambda_bar, B_bar, C_tilde, D, input_sequence):\n    \"\"\" Compute the LxH output of discretized SSM given an LxH input. Args:\n            Lambda_bar (complex64): discretized diagonal state matrix\n            B_bar (complex64): discretized input matrix\n            C tilde (complex64): output matrix\n            D (float32): feedthrough matrix\n            input_sequence (float32): input sequence of features (L,H)\n        Returns:\n            ys (float32): the SSM outputs (S5 layer preactivations)\n                        (L, H) \"\"\"\n    # Prepare elements required to initialize parallel scan\n    Lambda_elements = np.repeat(Lambda_bar[None, ...], input_sequence.shape[0], axis=0)\n    Bu_elements = jax.vmap(lambda u: B_bar @ u)(input_sequence)\n    elements = (Lambda_elements, Bu_elements) # (L, P), (L, P)\n    # Compute latent state sequence given input sequence using parallel scan\n    _, xs = parallel_scan(binary_operator, elements)\n                            # (L, P)\n    # Compute SSM output sequence\n    ys = jax.vmap(lambda x, u: (C_tilde @ x + D * u).real)(xs, input_sequence)\n    return ys\ndef apply_S5_layer(params, input_sequence):\n    \"\"\" Computes LxH output sequence of an S5 layer given LxH input sequence. Args:\n            params: tuple of the continuous time SSM parameters\n            input_sequence: input sequence of features (L, H)\n        Returns:\n            The S5 layer output sequence\n                        (L,H) \"\"\"\n    Lambda, B_tilde, C_tilde, D, log_Delta = params\n    Lambda_bar, B_bar = discretize(Lambda, B_tilde, np.exp(log_Delta))\n    preactivations = apply_ssm(Lambda_bar, B_bar, C_tilde, D, input_sequence)\n    return jax.nn.gelu(preactivations)\ndef batch_apply_S5_layer(params, input_sequences):\n    \"\"\" Computes BxLxH output sequence of an S5 layer given BxLxH input sequence. Args:\n        params: tuple of the continuous time SSM parameters\n            input_sequences: batch of input feature sequences (B, L ,H)\n        Returns:\n            Batch of S5 layer output sequences\n                        (B, L, H) \"\"\"\n```\n\nreturn jax.vmap(apply_S5_layer, in_axes=(None, 0)) (params, input_sequences)\nListing 1: JAX implementation to apply a single S 5 layer to a batch of input sequences.",
    "s5-31": "## B S5 LAYER DETAILS\n\n## B. 1 InitialiZation DETAILS\n\n## B.1.1 InItIALIZATION OF THE State MATRIX\n\nHere we provide additional details to supplement the discussion of initialization in Section 3.2. Gu et al. (2023) explains the ability of S4 to capture long-range dependencies when using the HiPPOLegS matrix via decomposing the input with respect to an infinitely long, exponentially decaying measure. The HiPPO-LegS matrix and corresponding SISO input vector are defined as\n\n$$\n\\begin{aligned}\n\\left(\\mathbf{A}_{\\mathrm{LegS}}\\right)_{n k} & =- \\begin{cases}(2 n+1)^{1 / 2}(2 k+1)^{1 / 2}, & n>k \\\\\nn+1, & n=k \\\\\n0, & n<k\\end{cases} \\\\\n\\left(\\mathbf{b}_{\\mathrm{LegS}}\\right)_{n} & =(2 n+1)^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nNote that in Section 4.2, the input matrix $\\mathbf{B}_{\\mathrm{LegS}} \\in \\mathbb{R}^{N \\times H}$ used in Corollary 1 is formed by concatenating $H$ copies of $\\mathbf{b}_{\\mathrm{LegS}} \\in \\mathbb{R}^{N}$.",
    "s5-32": "Theorem 1 of Gu et al. (2021a) then shows that the HiPPO matrices in Gu et al. (2020a), $\\mathbf{A}_{\\text {HiPPO }} \\in$ $\\mathbb{R}^{N \\times N}$ can be represented with a normal plus low-rank (NPLR) form consisting of a normal matrix, $\\mathbf{A}_{\\mathrm{HiPPO}}^{\\mathrm{Normal}}=\\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^{*} \\in \\mathbb{R}^{N \\times N}$, and a low-rank term\n\n$$\n\\mathbf{A}_{\\mathrm{HiPPO}}=\\mathbf{A}_{\\mathrm{HiPPO}}^{\\text {Normal }}-\\mathbf{P Q}^{\\top}=\\mathbf{V}\\left(\\boldsymbol{\\Lambda}-\\left(\\mathbf{V}^{*} \\mathbf{P}\\right)\\left(\\mathbf{V}^{*} \\mathbf{Q}\\right)^{*}\\right) \\mathbf{V}^{*}\n$$\n\nfor unitary $\\mathbf{V} \\in \\mathbb{C}^{N \\times N}$, diagonal $\\boldsymbol{\\Lambda} \\in \\mathbb{C}^{N \\times N}$, and low-rank factorization $\\mathbf{P}, \\mathbf{Q} \\in \\mathbb{R}^{N \\times r}$. The right hand side of this equation shows HiPPO matrices can be conjugated into a diagonal plus low-rank (DPLR) form. The HiPPO-LegS matrix can therefore be written in terms of the normal HiPPO-N matrix and low-rank term $\\mathbf{P}_{\\text {LegS }} \\in \\mathbb{R}^{N}$ (Goel et al., 2022) as\n\n$$\n\\mathbf{A}_{\\text {LegS }}=\\mathbf{A}_{\\text {LegS }}^{\\text {Normal }}-\\mathbf{P}_{\\text {Legs }} \\mathbf{P}_{\\text {Legs }}^{\\top}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\mathbf{A}_{\\text {LegS }_{n k}}^{\\text {Normal }} & =- \\begin{cases}\\left(n+\\frac{1}{2}\\right)^{1 / 2}\\left(k+\\frac{1}{2}\\right)^{1 / 2}, & n>k \\\\\n\\frac{1}{2}, & n=k \\\\\n\\left(n+\\frac{1}{2}\\right)^{1 / 2}\\left(k+\\frac{1}{2}\\right)^{1 / 2}, & n<k\\end{cases} \\\\\n\\mathbf{P}_{\\text {Legs }_{n}} & =\\left(n+\\frac{1}{2}\\right)^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\nOur default is to set the S5 layer state matrix $\\mathbf{A}=\\mathbf{A}_{\\text {LegS }}^{\\text {Normal }} \\in \\mathbb{R}^{P \\times P}$, and take the eigendecomposition of this matrix to recover the initial $\\boldsymbol{\\Lambda}$. We often find it beneficial to also use $\\mathbf{V}$ and $\\mathbf{V}^{-1}=\\mathbf{V}^{*}$ to initialize $\\tilde{\\mathbf{B}}$ and $\\tilde{\\mathbf{C}}$, as described below. As mentioned in Section 4.3, we also found that performance on many tasks benefited from initializing the S5 state matrix as block-diagonal, with each block on the diagonal equal to $\\mathbf{A}_{\\text {LegS }}^{\\text {Normal }} \\in \\mathbb{R}^{R \\times R}$, where $R$ here is less than the state dimension $P$, e.g. $R=\\frac{P}{4}$ when 4 blocks are used on the diagonal. We then take the eigendecomposition of this matrix to initialize $\\boldsymbol{\\Lambda}$, as well as $\\tilde{\\mathbf{B}}$ and $\\tilde{\\mathbf{C}}$. We note that even in this case, $\\tilde{\\mathbf{B}}$ and $\\tilde{\\mathbf{C}}$ are still initialized in dense form and there is no constraint that requires A to remain block-diagonal during learning. In the hyperparameter table in Appendix G, the $J$ hyperparameter indicates the number of these HiPPO-N blocks used on the diagonal for initialization, where $J=1$ indicates we used the default case of initializing with a single HiPPO-N matrix. We discuss the motivation for this block-diagonal initialization further in Appendix D.4. ## B.1.2 InItIALIZATION OF InPut, OUTPut and FEED-THROUGH Matrices\n\nIn general, we explicitly initialize the input matrix $\\tilde{\\mathbf{B}}$ and output matrix $\\tilde{\\mathbf{C}}$ using the eigenvectors from the diagonalization of the initial state matrix. Specifically, we sample $\\mathbf{B}$ and $\\mathbf{C}$ and then initialize the (complex) learnable parameters $\\tilde{\\mathbf{B}}$ as $\\tilde{\\mathbf{B}}=\\mathbf{V}^{-1} \\mathbf{B}$ and $\\tilde{\\mathbf{C}}=\\mathbf{C V}$. We initialize $\\mathbf{D} \\in \\mathbb{R}^{H}$ by independently sampling each element from a standard normal distribution. ![](https://cdn.mathpix.com/cropped/2024_09_17_1f0dc00258b87a7224b1g-17.jpg?height=684&width=1378&top_left_y=271&top_left_x=363)\n(a) S4 layer (Gu et al., 2021a) offline processing. ![](https://cdn.mathpix.com/cropped/2024_09_17_1f0dc00258b87a7224b1g-17.jpg?height=367&width=1397&top_left_y=998&top_left_x=364)\n(b) S5 layer offline processing. Duplicated from the main text. Figure 4: The computational components of the S4 layer (Gu et al., 2021a) (top) and the S5 layer (bottom) for offline application to a sequence. (a) The S 4 layer applies an independent SSM to each dimension of the input sequence $\\mathbf{u}_{1: L} \\in \\mathbb{R}^{L \\times H}$. This requires a Cauchy kernel computation to compute the convolution kernel coefficients in the frequency domain. Convolutions are computed using FFTs to produce the independent SSM outputs $\\mathbf{y}_{1: L} \\in \\mathbb{R}^{L \\times H}$. A nonlinear activation function that includes a mixing layer is applied to the SSM outputs to produce the layer outputs. (b) (Reproduced from Figure 1) The S5 layer uses a parallel scan on a diagonalized linear SSM to compute the SSM outputs $\\mathbf{y}_{1: L} \\in \\mathbb{R}^{L \\times H}$. A nonlinear activation function is applied to the SSM outputs to produce the layer outputs. ## B.1.3 InITIALIZATION OF THE TIMESCALES\n\nPrior work (Gupta et al., 2022; Gu et al., 2023) found the initialization of this timescale parameter to be important.",
    "s5-33": "This is studied in detail in Gu et al. (2023). We sample these parameters in line with S4 and sample each element of $\\log \\Delta \\in \\mathbb{R}^{P}$ from a uniform distribution on the interval $\\left[\\log \\delta_{\\min }, \\log \\delta_{\\max }\\right)$, where the default range is $\\delta_{\\min }=0.001$ and $\\delta_{\\max }=0.1$. The only exception is the Path-X experiment, where we initialize from $\\delta_{\\min }=0.0001$ and $\\delta_{\\max }=0.1$ to account for the longer timescales as discussed in Gu et al.",
    "s5-34": "(2023). ## B. 2 COMPARISON OF S4 AND S5 COMPUTATIONAL ELEMENTS\n\nIn Figure 4 we illustrate a comparison of the computational details of the S4 and S5 layers for efficient, parallelized offline processing. ## C COMPUtATIONAL EFFICIENCY OF S5\n\n## C. 1 ThEORETICAL COMPUTATIONAL EFFICIENCY\n\nProposition 1. Given an S4 layer with H input/output features, an $S 5$ layer with H input/output features and a latent size $P=\\mathcal{O}(H)$ has the same order of magnitude complexity as an S4 layer in terms of both runtime and memory usage. Proof. We first consider the case where the entire sequence is available and compare the S 4 layer's convolution mode to the S5 layer's use of a parallel scan. We then consider the online generation case where each method operates recurrently. Parallelized offline processing We consider the application of both the S4 and S5 layer to a vector-valued sequence $\\mathbf{u}_{1: L} \\in \\mathbb{R}^{L \\times H}$. Note that there are multiple ways to compute the convolution kernel for S4/S4D and the exact computational complexity depends on the implementation ( Gu et al., 2021a; 2022). However, the overall complexity of computing the S4 SSM outputs given the inputs is lower bounded by the FFTs used to convert into the frequency domain to apply the convolutions. Therefore, we can lower bound the cost of applying a single SISO S4 SSM to a scalar input sequence as $\\mathcal{O}(L \\log L)$ operations and $\\mathcal{O}(L)$ space. The S 4 layer then consists of $H$ different S4 SSMs, and therefore requires $\\mathcal{O}(H L \\log L)$ operations and $\\mathcal{O}(H L)$ space. Finally, mixing the $H$ activations at each timestep requires $L$ independent matrix-vector multiplications. Thus, the S 4 layer sequence-to-sequence transformation requires a total of $\\mathcal{O}\\left(H^{2} L+H L \\log L\\right)$ operations when run in the efficient convolution mode. Given $H^{2} L$ processors, these operations can be parallelized to a minimum of $\\mathcal{O}(\\log H+\\log L)$ parallel time. We now consider the S 5 layer. As discussed in Section 2.2, the parallel scan requires $\\mathcal{O}(P L)$ operations and $\\mathcal{O}(P L)$ space to perform the linear recurrence that computes the discretized states $\\mathbf{x}_{1: L} \\in \\mathbb{R}^{L \\times P}$. In addition, $\\mathcal{O}(P H L)$ operations are required for the independent matrix-vector multiplications that compute $\\overline{\\mathbf{B}} \\mathbf{u}_{1: L} \\in \\mathbb{R}^{L \\times P}$ and the SSM outputs $\\tilde{\\mathbf{C}} \\mathbf{x}_{1: L} \\in \\mathbb{R}^{L \\times H}$. Therefore, the S5 layer requires $\\mathcal{O}(P H L+P L)$ operations. Given $P H L$ processors, these operations can be parallelized to a minimum of $\\mathcal{O}(\\log P+\\log L)$ parallel time. Thus, we see that when the S 5 state dimension $P=\\mathcal{O}(H)$, the S5 layer requires $\\mathcal{O}\\left(H^{2} L+H L\\right)$ operations compared to the S4 layer's $\\mathcal{O}\\left(H^{2} L+H L \\log L\\right)$ operations. Crucially, when $P=\\mathcal{O}(H)$, S4 and S5 each have parallel complexity of $\\mathcal{O}(\\log H+\\log L)$ (when $H^{2} L$ processors are available). In addition, when $P=\\mathcal{O}(H)$, the space complexity for the parallel scan is $\\mathcal{O}(H L)$ which matches the space complexity of S4's FFTs. Both methods then also perform identical broadcasted matrix vector multiplications, and hence have the same space complexity. Online generation For online generation, both the S 4 and S 5 layers are run recurrently. The S4 layer requires $\\mathcal{O}\\left(H^{2}+H N\\right)$ operations per step due to its $\\mathcal{O}(H N)$ DPLR-matrix-vector multiplication (Gu et al., 2021a) and the $\\mathcal{O}\\left(H^{2}\\right)$ matrix-vector multiplication of its mixing layer. The S5 layer requires $\\mathcal{O}(P H+P)$ operations per step due to its $\\mathcal{O}(P)$ matrix-vector multiplication with its diagonal matrix and its $\\mathcal{O}(P H)$ matrix-vector multiplications to compute $\\overline{\\mathbf{B}} \\mathbf{u}_{k} \\in \\mathbb{R}^{P}$ and $\\tilde{\\mathbf{C}} \\mathbf{x}_{k} \\in \\mathbb{R}^{H}$. Thus, we see the two approaches have the same per step complexity of $\\mathcal{O}\\left(H^{2}\\right)$ when $P=\\mathcal{O}(H)$ and the individual S4 SSM state sizes $N$ are $\\mathcal{O}(H)$. Thus, S4 and S5 have the same order computational complexity and memory requirements in both cases. ## C. 2 EMPIRICAL RUNTIME COMPARISON\n\nTable 4 provides an empirical evaluation of the runtime performance, in terms of speed and memory, between S4, S4D and S5 across a range of sequence lengths from the LRA tasks. We compared the JAX implementation of S5 to a JAX implementation of S4 and S4D, based on the JAX implementation from Rush \\& Karamcheti (2022). For a fair comparison, we modified these existing JAX implementations of S4 and S4D to allow them both to enforce conjugate symmetry and use bidirectionality. For each task, models use bidirectionality and conjugate symmetry as reported in Gu et al. (2022). All models, except for the italicised S5 row, use the same input/output features $H$ and\n\nTable 4: Benchmarking the runtime performance of S4, S4D and S5 on three LRA tasks of varied sequence lengths using parameterizations described in Section C.2. For speeds, $>1 \\times$ indicates faster than the S4D baseline. For memory, $<1 \\times$ indicates less memory was used than the S4D baseline. The fifth line of each metric shows the performance of the actual S5 model used for the LRA result in Table 1 for each task using the architecture reported in Table 11. | Model | Architecture | Dataset (Input Length) |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  |  | ListOps (2,048) | Text $(4,096)$ | Path-X (16,384) |\n|  |  | Train step speed ( $\\uparrow$ ) |  |  |\n| S4 | From Gu et al. (2021a) | $0.6 \\times$ | $0.4 \\times$ | $0.4 \\times$ |\n| S4D | From Gu et al. (2022) | $\\mathbf{1 . 0} \\times$ | $1.0 \\times$ | $1.0 \\times$ |\n| S5 | (P=H) Matched to Gu et al. (2022) | $0.7 \\times$ | $0.9 \\times$ | $1.9 \\times$ |\n| S5 | $(\\mathrm{P}=\\mathrm{N})$ Matched to Gu et al. (2022) | $0.9 \\times$ | $\\mathbf{1 . 5} \\times$ | $\\mathbf{2 . 9} \\times$ |\n| S5 | As in Table 11 | $1.1 \\times$ | $1.0 \\times$ | $4.7 \\times$ |\n|  |  | Evaluation step speed $(\\uparrow)$ |  |  |\n| S4 | From Gu et al. (2021a) | $0.6 \\times$ | $0.5 \\times$ | $0.4 \\times$ |\n| S4D | From Gu et al. (2022) | $\\mathbf{1 . 0} \\times$ | $1.0 \\times$ | $1.0 \\times$ |\n| S5 | $(\\mathrm{P}=\\mathrm{H})$ Matched to Gu et al. (2022) | $0.5 \\times$ | $0.9 \\times$ | $1.7 \\times$ |\n| S5 | $(\\mathrm{P}=\\mathrm{N})$ Matched to Gu et al. (2022) | $0.6 \\times$ | $\\mathbf{1 . 2} \\times$ | $\\mathbf{1 . 8} \\times$ |\n| S5 | As in Table 11 | $0.7 \\times$ | $1.0 \\times$ | $4.1 \\times$ |\n|  |  |  | Memory allocation $(\\downarrow)$ |  |\n| S4 | From Gu et al. (2021a) | $1.1 \\times$ | $1.2 \\times$ | $1.2 \\times$ |\n| S4D | From Gu et al. (2022) | $1.0 \\times$ | $1.0 \\times$ | $1.0 \\times$ |\n| S5 | $(\\mathrm{P}=\\mathrm{H})$ Matched to Gu et al. (2022) | $0.9 \\times$ | $1.2 \\times$ | $1.1 \\times$ |\n| S5 | $(\\mathrm{P}=\\mathrm{N})$ Matched to Gu et al. (2022) | $\\mathbf{0 .",
    "s5-35": "7} \\times$ | $\\mathbf{0 . 7} \\times$ | $\\mathbf{0 . 7} \\times$ |\n| S5 | As in Table 11 | $0.7 \\times$ | $1.0 \\times$ | $0.9 \\times$ |\n\nnumber of layers as reported in Gu et al. (2022). The S4 and S4D layers also use the same S4 SSM latent size as reported in Gu et al. (2022). All methods used the same batch size and all comparisons were made using a 16GB NVIDIA V100 GPU. Note we observed the JAX S4D implementation to in general be faster than the JAX S4 implementation (possibly due to this specific S4 implementation's use of the naive Cauchy kernel computation (Gu et al., 2021a)). For this reason, we consider S4D as the baseline. We consider three configurations of S5 for comparison. The first two configurations, corresponding to lines 3 and 4 for each metric in Table 4, show how the runtime metrics vary as S5's latent size is adjusted, with all other architecture choices equal to those of S4. In line 3 of each metric, we denote the S5 \"Architecture\" as \" $(\\mathrm{P}=\\mathrm{H})$ Matched to Gu et al. (2022)\" to indicate that this configuration of S5 sets the latent size equal to the number of input/output features, $P=H$. This line empirically supports the complexity argument presented in Appendix C.1. In line 4 of each metric, we denote the S5 \"Architecture\" as \" $(\\mathrm{P}=\\mathrm{N})$ Matched to Gu et al. (2022)\" to indicate that this configuration of S5 sets the latent size $P$ equal to the latent size $N$ S4 uses for each of its SISO SSMs. This line also corresponds to the constrained version of S5 that performs similarly to S4/S4D as presented in the ablation study in Table 5. The runtime results of both of these configurations supports the claim in Section 4.3 that the latent size of S5 can be increased while maintaining S4's computational efficiency. Finally, we include a third configuration of S5, presented in the fifth line of each metric and italicized. This configuration of S5 uses the best architectural dimensions from Table 11 and was used for the corresponding LRA results in Table 1. Importantly, the broad takeaway from this empirical study is that the runtime and memory usage of S5 and S4/S4D are broadly similar, as suggested by the complexity analysis in the main text. ## D RELATIONSHIP BETWEEN S4 AND S5\n\nWe now describe in more detail the connection between the S4 and S5 architectures. This connection allowed us to develop more performant architectures and extend theoretical results from existing work. We break this analysis down into three parts:\n\n1. In Section D. 2 we prove Proposition 2. We exploit the linearity of the systems to identify that the latent states computed by the S5 SSM are equivalent to a linear combination of latent states computed by the $H$ SISO S4 SSMs, and that the outputs of the S5 SSM are a further linear transformation of these states.",
    "s5-36": "We then highlight how S4 and S5 effectively define different output matrices in the block-diagonal perspective shown in Figure 2.",
    "s5-37": "2. In Section D. 3 we provide a simple extension of the proof provided by Gu et al. (2022). The original proof shows that in the SISO case, in the limit of large $N$, the dynamics arising from a (non-diagonalizable) HiPPO-LegS matrix, are faithfully approximated by the (diagonalizable) normal component of the HiPPO-LegS matrix. We extend this proof to apply to the MIMO setting. This motivates initialization with the HiPPO-N matrix, which in-turn allows us to use parallel scans efficiently.",
    "s5-38": "3. In Section D. 4 we conclude by showing that, by judicious choice of initialization of the $S 5$ state matrix, S5 can implement multiple independent S 4 systems and relax the assumptions made.",
    "s5-39": "We also discuss the vector of timescale parameters, which we found to improve performance.",
    "s5-40": "We note that many of these results follow straightforwardly from the linearity of the recurrence. ## D. 1 AsSUMPTIONS\n\nFor these following sections we will use the following assumptions, until otherwise stated:\nAssumption 1. We consider only $H$-dimensional to $H$-dimensional sequence maps. Assumption 2. We assume the state matrix of each S4 SSM is identical, $\\mathbf{A}^{(h)}=\\mathbf{A} \\in \\mathbb{C}^{N \\times N}$. Assumption 3. We assume the timescales of each S4 SSM are identical, $\\Delta^{(h)}=\\Delta \\in \\mathbb{R}_{+}$\nAssumption 4. We assume that the same state matrix A is used in S5 as in S4 (also cf. Assumption 2). Note this also specifies the $S 5$ latent size $P=N$. We also assume the $S 5$ input matrix is the horizontal concatenation of the column input vectors used by $S 4, \\mathbf{B} \\triangleq\\left[\\mathbf{B}^{(1)}|\\ldots| \\mathbf{B}^{(H)}\\right]$. ## D. 2 DifFERENT Output Projections of EQUIVALENT DynAmics\n\nWe provide a proof of Proposition 2. Proposition 2. Consider an S5 layer, with state matrix $\\mathbf{A}$, input matrix $\\mathbf{B}$ and some output matrix $\\mathbf{C}$ (cf. Assumption 1); and an S4 layer, where each of the H S4 SSMs has state matrix A (cf. Assumption 2, 4) and input vector $\\mathbf{B}^{(h)}$ (cf. Assumption 4). If the $S 4$ and $S 5$ layers are discretized with the same timescales (cf. Assumption 3), then the S5 SSM produces outputs, $\\mathbf{y}_{k}$, equivalent to a linear combination of the latent states of the HS4SSMs, $\\mathbf{y}_{k}=\\mathbf{C}^{\\text {equiv }} \\mathbf{x}_{k}^{(1: H)}$, where $\\mathbf{C}^{\\text {equiv }}=[\\mathbf{C} \\cdots \\mathbf{C}]$. Proof. For a single S4 SSM, the discretized latent states can be expressed as a function of the input sequence $\\mathbf{u}_{1: L} \\in \\mathbb{R}^{L \\times H}$\n\n$$\n\\mathbf{x}_{k}^{(h)}=\\sum_{i=1}^{k} \\overline{\\mathbf{A}}^{k-i} \\overline{\\mathbf{B}}^{(h)} u_{i}^{(h)}\n$$\n\nFor an S5 layer, the latent states are expressible as\n\n$$\n\\mathbf{x}_{k}=\\sum_{i=1}^{k} \\overline{\\mathbf{A}}^{k-i} \\overline{\\mathbf{B}} \\mathbf{u}_{i}\n$$\n\nwhere we index as $\\overline{\\mathbf{B}} \\triangleq\\left[\\overline{\\mathbf{B}}^{(1)}|\\ldots| \\overline{\\mathbf{B}}^{(H)}\\right]$ and $\\mathbf{u}_{i} \\triangleq\\left[u_{i}^{(1)}, \\ldots, u_{i}^{(H)}\\right]^{\\top}$\n\nHere we make the observation:\n\n$$\n\\mathbf{x}_{k}=\\sum_{h=1}^{H} \\mathbf{x}_{k}^{(h)}\n$$\n\nwhere this result follows directly from the linearity of (13) and (14). This shows that (under the assumptions outlined above) the states of the MIMO S5 SSM are equivalent to the summation of the states across the $H$ different SISO S4 SSMs. We can then consider the effect of the output matrix C. For S5, the output matrix is a single dense matrix\n\n$$\n\\mathbf{y}_{k}=\\mathbf{C x}_{k}\n$$\n\nWe can substitute the relationship in (15) into (16) to cast the outputs of the MIMO S5 SSM in terms of the state of the $H$ SISO S4 SSMs:\n\n$$\n\\begin{aligned}\n\\mathbf{y}_{k} & =\\mathbf{C} \\sum_{h=1}^{H} \\mathbf{x}_{k}^{(h)} \\\\\n& =\\sum_{h=1}^{H} \\mathbf{C} \\mathbf{x}_{k}^{(h)}\n\\end{aligned}\n$$\n\nDenoting the vertical concatenation of the $H$ S4 SSM state vectors $\\mathbf{x}_{k}^{(1: H)}=\\left[\\mathbf{x}_{k}^{(1)^{\\top}}, \\ldots, \\mathbf{x}_{k}^{(H)^{\\top}}\\right]^{\\top}$, we see that the outputs of the S5 SSM are expressible as:\n\n$$\n\\mathbf{y}_{k}=\\mathbf{C}^{\\text {equiv }} \\mathbf{x}_{k}^{(1: H)}, \\quad \\text { where } \\quad \\mathbf{C}^{\\text {equiv }}=[\\mathbf{C}|\\cdots| \\mathbf{C}]\n$$\n\nand hence are equivalent to a linear combination of the $H N$ states computed by the $H \\mathrm{~S} 4 \\mathrm{SSMs}$. This shows the outputs of the constrained S5 SSM under consideration (cf. Assumption 4) can be interpreted as a linear combination of the latent states computed by $H$ constrained S 4 SSMs with the same state matrices and timescale parameters. Note however, it does not show that the outputs of the S5 SSM directly equal the outputs of the effective block-diagonal S4 SSM. Indeed, they are not equal, and we can repeat this analysis for the S 4 layer to concretely identify the difference. For comparison we assume that the output vector for each S4 SSM is given as a row in the S5 output matrix, i.e. $\\mathbf{C}=\\left[\\mathbf{C}^{(1)^{\\top}}|\\ldots| \\mathbf{C}^{(H)^{\\top}}\\right]^{\\top}$. We can express the output of each S4 SSM as\n\n$$\ny_{k}^{(h)}=\\mathbf{C}^{(h)} \\mathbf{x}_{k}^{(h)}\n$$\n\nwhere $y_{k}^{(h)} \\in \\mathbb{R}$. We can then define the effective output matrix that operates on the entire latent space (the dashed box labelled $\\mathbf{C}$ in Figure 2a) in S 4 as\n\n$$\ny_{k}^{(h)}=\\left(\\mathbf{C}^{\\mathrm{S} 4} \\mathbf{x}_{k}\\right)^{(h)}\n$$\n\nBy inspecting (19) and (21), we can concretely express the difference in the equivalent output matrix used by both layers\n\n$$\n\\mathbf{C}^{\\mathrm{S} 4}=\\left[\\begin{array}{ccc}\n\\mathbf{C}^{(1)} & \\cdots & \\mathbf{0} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathbf{0} & \\cdots & \\mathbf{C}^{(H)}\n\\end{array}\\right], \\quad \\mathbf{C}^{\\text {equiv }}=\\left[\\begin{array}{ccc}\n\\mathbf{C}^{(1)} & \\cdots & \\mathbf{C}^{(1)} \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathbf{C}^{(H)} & \\cdots & \\mathbf{C}^{(H)}\n\\end{array}\\right]=[\\mathbf{C}|\\cdots| \\mathbf{C}]\n$$\n\nIn S4, the effective output matrix consists of independent vectors on the leading diagonal (as is pictured in Figure 2a). In contrast, the effective output matrix used by S5 instead ties dense output matrices across the $H \\mathrm{~S} 4 \\mathrm{SSMs}$. As such, S 5 can be interpreted as simply defining a different projection of the $H$ independent SISO SSMs than is used by S4. Note that both projection matrices have the same number of parameters. Although the projection is different, the fact that the latent dynamics can still be interpreted as a linear projection of the same underlying S4 latent dynamics suggests that initializing the state dynamics in S5 with the HiPPO-LegS matrix may lead to good performance, similarly to what was observed in S4. We discuss this in the next section. We note that it is not obvious whether tying the dense output matrices is any more or less expressive than S4's use of a single untied output vector for each SSM, and it is unlikely that one approach is universally better than the other. We also stress that one would never implement S4 using the block diagonal matrix in (22), or, implement S5 using the repeated\nmatrix in (22). These matrices are simply constructs for understanding the equivalence between S 4 and S5. Finally, we note an alternative view: the block-diagonal S4 with output matrix $\\mathrm{C}^{\\text {equiv }}$ of Proposition 2 is equivalent to a version of $S 4$ that uses a bank of single-input, multi-output (SIMO) SSMs with tied state matrices, timescales and multi-channel output matrices $\\mathbf{C}^{(h)}=\\mathbf{C} \\in \\mathbb{R}^{H \\times N}$ and sums the individual SSM outputs. See appendix of Gu et al. (2021b) for a discussion of how the linear state space layer (LSSL), a predecessor of S4, can have multiple output channels. ## D. 3 DIAGONALIZABLE INITIALIZATION\n\nProposition 2 suggests that initializing with the HiPPO-LegS matrix may yield good performance in S5, just as it does in S4 (because the constrained version of S5 under consideration is effectively a different linear projection of the same latent dynamics). However, the HiPPO-LegS matrix is not stably diagonalizable. Corollary 1 allows us to initialize MIMO SSMs with the diagonalizable HiPPON matrix to approximate the HiPPO-LegS matrix and expect the performance to be comparable. Corollary 1 (Extension of Theorem 3 in Gu et al. (2022)). Consider $\\mathbf{A}_{\\mathrm{LegS}} \\in \\mathbb{R}^{N \\times N}$, $\\mathbf{A}_{\\mathrm{LegS}}^{\\text {Normal }} \\in$ $\\mathbb{R}^{N \\times N}, \\mathbf{B}_{\\text {LegS }} \\in \\mathbb{R}^{N \\times H}, \\mathbf{P}_{\\text {LegS }} \\in \\mathbb{R}^{N}$ as defined in Appendix B.1.1. Given vector-valued inputs $\\mathbf{u}(t) \\in \\mathbb{R}^{H}$, the ordinary differential equation $\\frac{\\mathrm{d} \\mathbf{x}^{\\prime}(t)}{\\mathrm{d} t}=\\mathbf{A}_{\\mathrm{LegS}}^{\\mathrm{Normal}} \\mathbf{x}^{\\prime}(t)+\\frac{1}{2} \\mathbf{B}_{\\mathrm{LegS}} \\mathbf{u}(t)$ converges to $\\frac{\\mathrm{d} \\mathbf{x}(t)}{\\mathrm{d} t}=\\mathbf{A}_{\\mathrm{Leg} \\mathrm{S}} \\mathbf{x}(t)+\\mathbf{B}_{\\mathrm{Leg} S} \\mathbf{u}(t)$ as $N \\rightarrow \\infty$.",
    "s5-41": "Proof. Theorem 3 in Gu et al. (2022) shows the following relationship for scalar input signals as $N \\rightarrow \\infty$ :\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_1f0dc00258b87a7224b1g-22.jpg?height=111&width=689&top_left_y=1300&top_left_x=710)\nwhere the only modification we have made is introducing the $(h)$ superscript to allow us to explicitly index dimensions later. We define $\\mathbf{B}=\\left[\\mathbf{B}_{\\mathrm{LegS}}^{(1)}|\\ldots| \\mathbf{B}_{\\mathrm{LegS}}^{(H)}\\right]$. We wish to extend this to the case of vector-valued input signals. We first recall (15), which shows that the latent states of the MIMO S5 SSM are the summation of the latent states of the $H$ SISO S4 SSMs (to which Theorem 3 from Gu et al. (2021a) applies). Although we derived (15) in discrete time, it applies equally in continuous time:\n\n$$\n\\mathbf{x}(t)=\\sum_{h=1}^{H} \\mathbf{x}^{(h)}(t)\n$$\n\nWe can therefore define the derivative of the S 5 state as:\n\n$$\n\\frac{\\mathrm{d} \\mathbf{x}(t)}{\\mathrm{d} t}=\\sum_{h=1}^{H} \\frac{\\mathrm{d} \\mathbf{x}^{(h)}(t)}{\\mathrm{d} t}\n$$\n\nSubstituting (23) into this then yields:\n\n$$\n\\begin{aligned}\n\\frac{\\mathrm{d} \\mathbf{x}(t)}{\\mathrm{d} t} & =\\sum_{h=1}^{H}\\left[\\mathbf{A}_{\\mathrm{LegS}}^{\\text {Normal }} \\mathbf{x}^{(h)}(t)+\\frac{1}{2} \\mathbf{B}_{\\mathrm{LegS}}^{(h)} u^{(h)}(t)\\right] \\\\\n& =\\mathbf{A}_{\\mathrm{LegS}}^{\\text {Normal }} \\sum_{h=1}^{H} \\mathbf{x}^{(h)}(t)+\\frac{1}{2} \\sum_{h=1}^{H} \\mathbf{B}_{\\mathrm{LegS}}^{(h)} u^{(h)}(t) \\\\\n& =\\mathbf{A}_{\\text {LegS }}^{\\text {Normal }} \\mathbf{x}(t)+\\frac{1}{2} \\mathbf{B}_{\\text {LegS }} \\mathbf{u}(t)\n\\end{aligned}\n$$\n\nThis equivalence motivates initializing S5 state matrices with the diagonalizable HiPPO-N matrix and suggests that we can expect to see similar performance gains. ## D. 4 RELAXING THE ASSUMPtions\n\nHere we discuss how relaxing the constraint on S5's latent size from Assumption 4 helps to relax the assumptions on the tied S4 SSM state matrices (Assumption 2) and timescales (Assumption 3) as well as the tied output matrices that result from Proposition 2. We start by considering the case when the S5 SSM state matrix is block-diagonal. Consider an S5 SSM with latent size $J N=\\mathcal{O}(H)$ and block-diagonal $\\mathbf{A} \\in \\mathbb{R}^{J N \\times J N}$, dense $\\mathbf{B} \\in \\mathbb{R}^{J N \\times H}$, dense $\\mathbf{C} \\in \\mathbb{R}^{H \\times J N}$, and $J$ different timescale parameters $\\boldsymbol{\\Delta} \\in \\mathbb{R}^{J}$. As a result of the block-diagonal state matrix, this system has a latent state $\\mathbf{x}_{k} \\in \\mathbb{R}^{J N}$ that can be partitioned into $J$ different states $\\mathbf{x}_{k}^{(j)} \\in \\mathbb{R}^{N}$. We can then partition this system into $J$ different subsystems and discretize each subsystem with one of the $\\Delta^{(j)}$ to get the following discretized system:\n\n$$\n\\overline{\\mathbf{A}}=\\left[\\begin{array}{ccc}\n\\overline{\\mathbf{A}}^{(1)} & & \\\\\n& \\ddots & \\\\\n& & \\overline{\\mathbf{A}}^{(J)}\n\\end{array}\\right], \\quad \\overline{\\mathbf{B}}=\\left[\\begin{array}{c}\n\\overline{\\mathbf{B}}^{(1)} \\\\\n\\vdots \\\\\n\\overline{\\mathbf{B}}^{(J)}\n\\end{array}\\right], \\quad \\mathbf{C}=\\left[\\mathbf{C}^{(1)}|\\cdots| \\mathbf{C}^{(J)}\\right]\n$$\n\nwhere $\\overline{\\mathbf{A}}^{(j)} \\in \\mathbb{R}^{N \\times N}, \\overline{\\mathbf{B}}^{(j)} \\in \\mathbb{R}^{N \\times H}$ and $\\mathbf{C}^{(j)} \\in \\mathbb{R}^{H \\times N}$. It follows that this partitioned system can be also be viewed as $J$ independent $N$-dimensional S5 SSM subsystems and the output of the overall system is the sum of the output of the $J$ subsystems\n\n$$\n\\begin{aligned}\n\\mathbf{y}_{k} & =\\mathbf{C} \\mathbf{x}_{k} \\\\\n& =\\sum_{j=1}^{J} \\mathbf{C}^{(j)} \\mathbf{x}_{k}^{(j)}\n\\end{aligned}\n$$\n\nIt follows from Proposition 2 that the dynamics of each of these $J$ S5 SSM subsystems can be related to the dynamics of a different S 4 system from Proposition 2. Each of these S 4 systems has its own bank of tied S4 SSMs (cf. Assumptions 2, 3). Importantly, each of the $J \\mathrm{~S} 4$ systems can have its own state matrix, timescale parameter and output matrix shared across its $H \\mathrm{~S} 4 \\mathrm{SSMs}$. Thus, the outputs of a $J N$ dimensional S5 SSM can be equivalent to the linear combination of the latent states of $J$ different S 4 systems from Proposition 2. This fact motivates the option to initialize a block-diagonal S5 state matrix with several HiPPO-N matrices on the blocks, rather than just initializing with one larger HiPPO-N matrix. In practice we found the block-diagonal initialization to improve performance on many tasks, see Appendix E. ## D. 5 TIMESCALE PARAMETERIZATION\n\nFinally, we take a closer look at the parameterization of the timescale parameters $\\boldsymbol{\\Delta}$. As discussed in Section 4.3, S4 can learn a different timescale parameter for each S4 SSM, potentially allowing it to capture different timescales of the data. Further, the initialization of the timescales can be important (Gu et al., 2023; Gupta et al., 2022), and limiting to sampling a single initial parameter may lead to poor initialization. The discussion in the previous section motivates potentially learning $J$ different timescale parameters, one for each of the $J$ subsystems. However, in practice, we found better performance when using $P$ different timescale parameters, one for each of the states. On the one hand, this can be viewed simply as learning a different scaling for each of the eigenvalues in the diagonalized system (see Eq. (6)). On the other hand, this could be viewed as increasing the number of timescale parameters sampled at initialization, helping to combat the possibility of poor initialization. Of course, the system could learn to use just a single timescale by setting all of the timescales to be the same. See further discussion in the ablation study in Appendix E. ## E Ablations\n\nWe perform several ablations to empirically explore different aspects of S5. ## E. 1 S5 LATENT SIZE, BLOCK-DIAGONAL INITIALIZATION, AND TIMESCALE PARAMETERIZATION\n\nThe discussion in Section 4 and Appendix D raises several interesting questions: How does S5 perform when the latent size $P$ is restricted to be equal to the latent size $N$ used by each of S4's SSMs? How important is the timescale parameterization discussed in Appendix D.5? How important is the block-diagonal initialization? Table 5 displays the results of an ablation study performed on the LRA tasks to get a better sense of this. We consider 3 versions of S5. The first version uses the same general architecture (e.g. number of input/output features $H$, number of layers, etc) as reported for the S4/S4D variants in Gu et al. (2022), sets the S5 SSM latent size $P$ to be equal to the latent size $N=64$ used by each of the S4 SSMs, and uses only a single scalar timescale parameter $\\Delta \\in \\mathbb{R}$. This is essentially the version of S 5 we consider in Proposition 2 . We observe that this constrained version of S5 actually performs well on most tasks, though struggles to perform comparably to the S 4 baselines on Image and ListOps. The second version of S 5 is exactly the same as the first except we parameterize the timescale parameter as a vector $\\Delta \\in \\mathbb{R}^{N}$. We observe uniform improvements over the scalar timescale parameterization and this reflects our general findings when training S5. Finally, the complexity analysis and runtime comparison in Appendix C. 2 suggests the latent size of S5 can be increased while maintaining similar complexity and practical runtimes as S4. We include the unconstrained version of S 5 reported in our main results that uses the settings reported in the hyperparameter Table 11. These models were allowed to be parameterized with fewer input/output features $H$ (to ensure similar parameter counts to the S 4 baselines) and generally used larger latent sizes $P>N$. Further, we swept over the use of a block-diagonal initialization or not and the number of blocks to use (where $J=1$ indicates no block-diagonal initialization was used). All models benefited from the block-diagonal initialization for the LRA tasks (See Table 11 ). Table 5: Ablations on the LRA benchmark tasks (Tay et al., 2021). S4 results were taken from Gu et al. (2022; 2021a). Note that the total parameter count for all models in this table are\ncommensurate, and hence variations in performance cannot be attributed to a model having drastically more parameters. | Model <br> (Input length) | ListOps <br> $(2,048)$ | Text <br> $(4,096)$ | Retrieval <br> $(4,000)$ | Image <br> $(1,024)$ | Pathfinder <br> $(1,024)$ | Path-X <br> $(16,384)$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| S4D-LegS | $\\underline{60.47}$ | 86.18 | 89.46 | $\\underline{88.19}$ | 93.06 | 91.95 |\n| S4-LegS | 59.60 | 86.82 | $\\underline{90.90}$ | $\\mathbf{8 8 . 6 5}$ | 94.20 | 96.35 |\n| S5 $(\\mathrm{P}=\\mathrm{N}, J=1, \\Delta \\in \\mathbb{R})$ | 57.20 | 87.60 | 90.53 | 82.01 | 94.14 | 96.59 |\n| S5 $\\left(\\mathrm{P}=\\mathrm{N}, J=1, \\boldsymbol{\\Delta} \\in \\mathbb{R}^{N}\\right)$ | 58.65 | $\\underline{88.12}$ | 90.76 | 85.04 | $\\underline{94.53}$ | $\\underline{97.49}$ |\n| S5 $\\left(\\mathrm{P}:\\right.$ free, $\\mathrm{J} \\geq 1, \\boldsymbol{\\Delta} \\in \\mathbb{R}^{P}$, see Table 11) | $\\mathbf{6 2 .",
    "s5-42": "1 5}$ | $\\mathbf{8 9 . 3 1}$ | $\\mathbf{9 1 . 4 0}$ | 88.00 | $\\mathbf{9 5 .",
    "s5-43": "3 3}$ | $\\mathbf{9 8 . 5 8}$ |\n\n## E. 2 IMPORTANCE OF HIPPO-N AND CONTINUOUS-TIME PARAMETERIZATION\n\nWe perform a further ablation study to gain insight into the differences between S 5 and prior attempts at parallelized linear RNNs (discussed in Section 5) focusing on what appears to be the distinguishing features: continuous-time parameterizations and HiPPO initializations. We compare different initializations of the state matrix: random Gaussian, random antisymmetric, and HiPPO-N. The antisymmetric initialization is interesting because prior work considered these matrices in RNNs for long-range dependencies (Chang et al., 2019), and because the HiPPO-LegS matrix can be parameterized in a way related to antisymmetric matrices (Gu et al., 2021a). Moreover, to compare to a setup more akin to the previous parallelized linear RNN work, we also consider a direct discretetime parameterization of S 5 that does not perform repeated discretization during training or learn the\n\nTable 6: S5 Initialization and Parameterization Ablation Study. $\\boldsymbol{X}$ indicates the model did not improve over random guessing. | Model <br> (Input length) | Parameterization | Initialization | ListOps <br> $(2,048)$ | Text <br> $(4,096)$ | Path-X <br> $(16,384)$ |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| S5 (ablation) | Discrete | Gaussian | 41.50 | 81.09 | $\\boldsymbol{x}$ |\n| S5 (ablation) | Discrete | Antisymmetric | 49.10 | $\\underline{86.42}$ | $\\boldsymbol{x}$ |\n| S5 (ablation) | Discrete | HiPPO-N | 58.15 | 61.93 | $\\boldsymbol{x}$ |\n| S5 (ablation) | Continuous | Gaussian | 58.50 | 69.03 | $\\boldsymbol{x}$ |\n| S5 (ablation) | Continuous | Antisymmetric | $\\mathbf{5 9 . 3 5}$ | 82.83 | $\\boldsymbol{x}$ |\n| $\\quad$ S5 | Continuous | HiPPO-N | $\\mathbf{6 2 .",
    "s5-44": "1 5}$ | $\\mathbf{8 9 . 3 1}$ | $\\mathbf{9 8 . 5 8}$ |\n\ntimescale parameter $\\Delta$. We present the results of this ablation study in Table 6 (along with S5). We consider three of the LRA tasks that vary in length and difficulty. The main takeaway is that the only approach that consistently performs well on all tasks, including the ability to solve Path-X, is the S5 approach that uses the continuous-time parameterization and HiPPO initialization. We also note that we observed the discrete time/HiPPO-N matrix configuration to be difficult to train due to stability issues, typically requiring a much lower learning rate. ## E. 3 S4D INITIALIZATION ABLATIONS\n\nFinally, Gu et al. (2022) propose several alternative diagonal matrices to the diagonalized HiPPO-N matrix, including the S4D-Inv and S4D-Lin matrices. They perform an ablation on the LRA tasks where they simply replace the diagonalized HiPPO-N matrix with the S4D-Inv and S4D-Lin matrices while keeping all other factors the same. We include these results in Table 7. In Table 7, we also include results for a similar ablation in S 5 by using these matrices to initialize S 5 in place of the HiPPO-N matrix while keeping all other factors constant. Both matrices perform well on most tasks with the exception of the S4D-Lin matrix on Path-X. Interestingly, one of these runs reached $96.79 \\%$, however the other runs did not exceed random guessing on this task. Future exploration of these and other matrices are an interesting direction for future work. ## F SUPPLEMENTARY RESULTS\n\nWe include further experimental results to supplement the results presented in the main text. ## F. 1 EXTENDED LRA RESULTS\n\nTable 7: Test accuracy on the LRA benchmark tasks (Tay et al., 2021). $\\boldsymbol{X}$ indicates the model did not exceed random guessing. Citations refer to the original model. The results for Transformer through Performer are from (Tay et al., 2021). We follow the procedure reported in Gu et al. (2021a; 2022) and report means across three seeds for S4, S4D (as reported by Gu et al. (2021a; 2022)) and S5, with the standard deviation in parenthesis. | Model <br> (Input length) | ListOps <br> $(2,048)$ | Text <br> $(4,096)$ | Retrieval <br> $(4,000)$ | Image <br> $(1,024)$ | Pathfinder <br> $(1,024)$ | Path-X <br> $(16,384)$ | Avg. |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer (Vaswani et al., 2017) | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | $x$ | 53.66 |\n| Reformer (Kitaev et al., 2020) | 37.27 | 56.10 | 53.40 | 38.07 | 68.50 | $x$ | 50.56 |\n| BigBird (Zaheer et al., 2020) | 36.05 | 64.02 | 59.29 | 40.83 | 74.87 | $x$ | 54.17 |\n| Linear Trans. (Katharopoulos et al., 2020) | 16.13 | 65.90 | 53.09 | 42.34 | 75.30 | $x$ | 50.46 |\n| Performer (Choromanski et al., 2021) | 18.01 | 65.40 | 53.82 | 42.77 | 77.05 | $x$ | 51.18 |\n| FNet (Lee-Thorp et al., 2022) | 35.33 | 65.11 | 59.61 | 38.67 | 77.80 | $x$ | 54.42 |\n| Nystr\u00f6mformer (Xiong et al., 2021) | 37.15 | 65.52 | 79.56 | 41.58 | 70.94 | $x$ | 57.46 |\n| Luna-256 (Ma et al., 2021) | 37.25 | 64.57 | 79.29 | 47.38 | 77.72 | $x$ | 59.37 |\n| H-Transformer-1D (Zhu \\& Soricut, 2021) | 49.53 | 78.69 | 63.99 | 46.05 | 68.78 | $x$ | 61.41 |\n| CCNN (Romero et al., 2022a) | 43.60 | 84.08 | $x$ | 88.90 | 91.51 | $x$ | 68.02 |\n| Mega $\\left(\\mathcal{O}\\left(L^{2}\\right)\\right.$ ) (Ma et al., 2023) | 63.14 | 90.43 | 91.25 | 90.44 | 96.01 | 97.98 | 88.21 |\n| Mega-chunk $(\\mathcal{O}(L)$ ) (Ma et al., 2023) | 58.76 | 90.19 | 90.97 | 85.80 | 94.41 | 93.81 | 85.66 |\n| ![](https://cdn.mathpix.com/cropped/2024_09_17_1f0dc00258b87a7224b1g-26.jpg?height=42&width=473&top_left_y=1386&top_left_x=32) | 60.60 | 84.80 | 87.80 | 85.70 | 84.60 | 87.80 | 81.88 |\n| S4D-LegS (Gu et al., 2022) | $60.47(0.34)$ | $86.18(0.43)$ | $89.46(0.14)$ | $88.19(0.26)$ | $93.06(1.24)$ | 91.95 | 84.89 |\n| S4D-Inv (Gu et al., 2022) | $60.18(0.35)$ | $87.34(0.20)$ | $91.09(0.01)$ | $87.83(0.37)$ | $93.78(0.25)$ | 92.80 | 85.50 |\n| S4D-Lin (Gu et al., 2022) | $60.52(0.51)$ | $86.97(0.23)$ | $90.96(0.09)$ | $87.93(0.34)$ | $93.96(0.60)$ | $x$ | 78.39 |\n| S4-FouT (Gu et al., 2023) | 57.88 (1.90) | $86.34(0.31)$ | $89.66(0.88)$ | $89.07(0.19)$ | $94.46(0.24)$ | $x$ | 77.90 |\n| S4-LegS/FouT (Gu et al., 2023) | $60.45(0.75)$ | $86.78(0.26)$ | $90.30(0.28)$ | $89.00(0.26)$ | $94.44(0.08)$ | $x$ | 78.50 |\n| S4-LegS (Gu et al., 2021a) | $59.60(0.07)$ | $86.82(0.13)$ | $90.90(0.15)$ | $88.65(0.23)$ | $94.20(0.25)$ | 96.35 | 86.09 |\n| Liquid-S4 (Hasani et al., 2023) | $\\underline{62.75}(0.20)$ | $89.02(0.04)$ | $91.20(0.01)$ | $\\underline{89.50}(0.40)$ | $94.80(0.2)$ | $96.66(0.001)$ | 87.32 |\n| S5-Inv (See Appendix E.3) | $60.07(0.26)$ | $87.77(0.29)$ | $91.26(0.12)$ | $86.41(0.17)$ | $93.42(0.42)$ | $97.54(0.74)$ | 86.08 |\n| S5-Lin (See Appendix E.3) | $59.98(0.53)$ | $88.15(0.24)$ | $91.31(0.24)$ | $86.05(0.96)$ | $94.31(0.36)$ | $65.60(27.00)$ | 80.90 |\n| S5 | $62.15(0.23)$ | $89.31(0.15)$ | $91.40(0.05)$ | $88.00(0.22)$ | $\\underline{95.33}(0.26)$ | $98.58(0.17)$ | $\\underline{87.46}$ |\n\n## F. 2 Extended SpeEch Results\n\nTable 8: Speech Commands classification task (Warden, 2018). Test accuracy on 35-way keyword spotting. Training examples are one-second audio waveforms sampled at 16 kHz , or a one-dimensional sequence of length 16000 . Last column indicates zero-shot testing at 8 kHz where examples are constructed by naive decimation. The mean across three random seeds is reported, with the standard deviation in parenthesis. Performance for the baselines InceptionNet through to S4D-Lin are reported from Gu et al. (2022). | Model | Parameters | 16000 Hz | 8000 Hz |\n| :--- | :---: | :---: | :---: |\n| InceptionNet (Nonaka \\& Seita, 2021) | 481 K | $61.24(0.69)$ | $05.18(0.07)$ |\n| ResNet-18 (Nonaka \\& Seita, 2021) | 216 K | $77.86(0.24)$ | $08.74(0.57)$ |\n| XResNet-50 (Nonaka \\& Seita, 2021) | 904 K | $83.01(0.48)$ | $07.72(0.39)$ |\n| ConvNet (Nonaka \\& Seita, 2021) | 26.2 M | $95.51(0.18)$ | $07.26(0.79)$ |\n| S4-LegS (Gu et al., 2021a) | 307 K | $96.08(0.15)$ | $91.32(0.17)$ |\n| S4-FouT (Gu et al., 2023) | 307 K | $95.27(0.20)$ | $91.59(0.23)$ |\n| S4-(LegS/FouT) (Gu et al., 2023) | 307 K | $95.32(0.10)$ | $90.72(0.68)$ |\n| S4D-LegS (Gu et al., 2022) | 306 K | $95.83(0.14)$ | $91.08(0.16)$ |\n| S4D-Inv (Gu et al., 2022) | 306 K | $96.18(0.27)$ | $\\underline{91.80}(0.24)$ |\n| S4D-Lin (Gu et al., 2022) | 306 K | $96.25(0.03)$ | $91.58(0.33)$ |\n| Liquid-S4 (Hasani et al., 2023) | 224 K | $\\mathbf{9 6 .",
    "s5-45": "7 8 ( 0 . 0 5 )}$ | $90.00(0.25)$ |\n| S5 | 280 K | $\\underline{96.52(0.16)}$ | $\\mathbf{9 4 . 5 3 ( 0 . 1 0 )}$ |\n\n## F. 3 PENDULum EXtEnded Results\n\nWe also evaluate two ablations: $S 5$-drop uses the same S5 architecture, but drops the dependence on the inter-sample interval, i.e. $\\Delta_{t} \\triangleq 1.0$. We expect this network to perform poorly as it has no knowledge of how long has elapsed between observations. S5-append uses the same S 5 architecture, but appends the integration timestep to the thirty-dimensional image encoding, prior to being input into the dense S5 input layer. Hypothetically, we expect this network to perform as well as S5. However, to do so, requires the S 5 network to learn to process time, which may be difficult, especially in more complex domains. We include these ablations in the bottom partition of Table 9. Note that the runtimes quoted for the baseline methods (runtimes marked with a $*$ ) are as reported by Schirmer et al. (2022). These times are the total time for a training epoch, and hence include any time spent batching data. We re-ran the CRU using the original PyTorch code on the same hardware as we run our JAX S5 experiments on (labelled CRU (our run)). For these experiments we used a single NVIDIA GeForce RTX 2080 Ti. For these runs (CRU (our run), S5, S5-drop and S5-append) we exclude the time spent batching the data to more faithfully compare the runtimes for the models themselves. Also note that our S5 experiments will benefit from JAX compilation, but that this is not sufficient to explain the difference in runtime. Table 9: Test MSE $\\times 10^{-3}$ and runtimes on the pendulum regression task. Performance for the baselines, mTAND through to CRU, are reported from Schirmer et al. (2022), with mean and standard deviations across five random seeds (standard deviation in parenthesis). Accompanying citation indicates the original citation for the method. We re-ran the CRU (labelled CRU (our run)) and ran our S5 methods across twenty random seeds. We report mean and variances of the MSE error on the held-out test set, using a model selected using the validation set MSE. We refer the reader to Schirmer et al. (2022) for full description of the baselines. | Model | Runtime (sec/train epoch) | Regression MSE $\\left(\\times 10^{-3}\\right)$ |\n| :--- | :---: | :---: |\n| mTAND (Shukla \\& Marlin, 2021) | $3^{*}$ | $65.64(4.05)$ |\n| RKN (Becker et al., 2019) | $20^{*}$ | $8.43(0.61)$ |\n| RKN- $\\Delta_{t}$ (Becker et al., 2019) | $20^{*}$ | $5.09(0.40)$ |\n| GRU (Cho et al., 2014) | $12^{*}$ | $9.44(1.00)$ |\n| GRU- $\\Delta_{t}$ (Cho et al., 2014) | $12^{*}$ | $5.44(0.99)$ |\n| Latent ODE (Chen et al., 2018) | $52^{*}$ | $15.70(2.85)$ |\n| ODE-RNN (Rubanova et al., 2019) | $37^{*}$ | $7.26(0.41)$ |\n| GRU-ODE-B (De Brouwer et al., 2019) | $60^{*}$ | $9.78(3.40)$ |\n| f-CRU (Schirmer et al., 2022) | $29^{*}$ | $6.16(0.88)$ |\n| CRU (Schirmer et al., 2022) | $36^{*}$ | $4.63(1.07)$ |\n| CRU (our run) | 22 | $3.94(0.21)$ |\n| S5 | 0.25 | $\\mathbf{3 .",
    "s5-46": "4 1}(0.27)$ |\n| S5-drop (ablation) | $\\mathbf{0 . 2 5}$ | $6.68(0.38)$ |\n| S5-append (ablation) | $\\underline{0.25}$ | $4.13(0.43)$ |\n\n## F. 4 PIXEL-LEVEL 1-D IMAGE CLASSIFICATION RESUlts\n\nTable 10 presents results and citations of the pixel-level 1-D image classification. Table 10: Test accuracy on Pixel-level 1-D Image classification. Citations refer to the original model; additional citation indicates work from which this baseline is reported. | Model | sMNIST <br> $(784)$ | psMNIST <br> (Input length) | sCIFAR <br> $(1024)$ |\n| :--- | :---: | :---: | :---: |\n| Transformer (Trinh et al., 2018; Vaswani et al., 2017) | 98.9 | 97.9 | 62.2 |\n| CCNN (Romero et al., 2022a) | $\\mathbf{9 9 . 7 2}$ | $\\mathbf{9 8 . 8 4}$ | $\\mathbf{9 3 . 0 8}$ |\n| FlexTCN (Romero et al., 2021) | 99.62 | 98.63 | 80.82 |\n| CKConv (Romero et al., 2022b) | 99.32 | 98.54 | 63.74 |\n| TrellisNet (Bai et al., 2019) | 99.20 | 98.13 | 73.42 |\n| TCN (Bai et al., 2018) | 99.0 | 97.2 | - |\n| LSTM (Gu et al., 2020b; Hochreiter \\& Schmidhuber, 1997) | 98.9 | 95.11 | 63.01 |\n| r-LSTM (Trinh et al., 2018) | 98.4 | 95.2 | 72.2 |\n| Dilated GRU (Chang et al., 2017) | 99.0 | 94.6 | - |\n| Dilated RNN (Chang et al., 2017) | 98.0 | 96.1 | - |\n| IndRNN (Li et al., 2018) | 99.0 | 96.0 | - |\n| expRNN (Lezcano-Casado \\& Mart\u0131nez-Rubio, 2019) | 98.7 | 96.6 | - |\n| UR-LSTM (Gu et al., 2020b) | 99.28 | 96.96 | 71.00 |\n| UR-GRU (Gu et al., 2020b) | 99.27 | 96.51 | 74.4 |\n| LMU (Voelker et al., 2019) | - | 97.15 | - |\n| HiPPO-RNN (Gu et al., 2020a) | 98.9 | 98.3 | 61.1 |\n| UNIcoRNN (Rusch \\& Mishra, 2021) | - | 98.4 | - |\n| LMU-FFT (Chilkuri \\& Eliasmith, 2021) | - | 98.49 | - |\n| LipschitzRNN (Erichson et al., 2021) | 99.4 | 96.3 | 64.2 |\n| LSSL (Gu et al., 2021b) | 99.53 | 98.76 | 84.65 |\n| S4 (Gu et al., 2022; 2021a) | 99.63 | 98.70 | 91.80 |\n| S4D (Gu et al., 2022) | - | - | 89.92 |\n| Liquid-S4 (Hasani et al., 2023). | - | - | 92.02 |\n| S5 | 99.65 | 98.67 | 90.10 |\n\n## G EXPERIMENT CONFIGURATIONS\n\nIn this section we describe the experimental details. This includes the model architecture, general hyperparameters, specifics for each task, and information about the datasets. ## G. 1 Deep Sequence Model Architecture\n\nFor the experiments, we use the S 5 layer as a drop-in replacement for the S 4 layer used in the sequence model architecture of ( Gu et al., 2021a). On a high level, this architecture consists of a linear encoder (to encode the input at each time step into $H$ features), multiple S 5 layers, a mean pooling layer, a linear decoder, and a Softmax operation for the classification tasks. The mean pooling layer compresses the output of the last S5 layer, of shape [batch size, sequence length, number of features $(H)]$, across the sequence length dimension, so that a single $H$-dimensional encoding is available for softmax classification. The baseline S4 models from Gu et al. (2022; 2023) apply a GLU activation (Dauphin et al., 2017) function to the S4 SSM outputs. To take advantage of the fact that the S5 SSM outputs have already been mixed throughout the MIMO SSM we use what is essentially a weighted sigmoid gated unit (Tanaka, 2020) (a GLU activation without an additional linear transform). Specifically, given an S5 SSM output $\\mathbf{y}_{k} \\in \\mathbb{R}^{H}$ and a dense matrix $\\mathbf{W} \\in \\mathbb{R}^{H \\times H}$, the layer output of the activation function we apply is $\\mathbf{u}_{k}^{\\prime}=\\operatorname{GELU}\\left(\\mathbf{y}_{k}\\right) \\odot \\sigma\\left(\\mathbf{W} * \\operatorname{GELU}\\left(\\mathbf{y}_{k}\\right)\\right)$. Hyperparameter options such as dropout rate, using either layer normalization or batch normalization, and using either pre-norm or post-norm are applied between the layers. Exceptions to the basic architecture described here are mentioned in the individual experiment sections below. ## G. 2 DEFAULT HYperparamETERS\n\nTable 11 presents the main hyperparameters used for each experiment. For all experiments we ensure the number of layers and layer input/output features $H$ are less than or equal to the number of layers and layer input/output features reported in Gu et al. (2022) as well as ensuring comparable parameter counts. In general, the models for most tasks used batch normalization and pre-norm. Exceptions are noted in the individual experiment sections below. ## G.2.1 OPTIMIZERS AND LEARNING RATES\n\nWe follow the general optimization approach used by S4/S4D in Gu et al. (2022). We use the AdamW optimizer (Loshchilov \\& Hutter, 2019) with a global learning rate. However, in general, no weight decay and a smaller learning rate (the SSM learning rate) is applied to $\\boldsymbol{\\Lambda}, \\tilde{\\mathbf{B}}, \\Delta$.",
    "s5-47": "All experiments used cosine annealing. Exceptions to these points are noted in the individual experiment sections below. ## G.2.2 BIDIRECTIONALITY\n\nWe follow Gu et al. (2022) and use bidirectional models for the LRA and speech tasks. Unidirectional (causal) models were used for the pendulum, sequential and permuted MNIST for fair comparison with prior methods that used unidirectional models.",
    "s5-48": "## G. 3 Task Specific Hyperparameters\n\nHere we specify any task-specific details, hyperparameter or architectural differences from the defaults outlined above. ## G.3.1 LISTOPS\n\nWeight decay and the global learning rate were applied to $\\tilde{\\mathbf{B}}$. Table 11: Hyperparameters used for the reported results. Depth: number of layers. H: number of input/output features. P: Latent size. J: number of blocks used for the initialization of A (see Section B.1.1). Dropout: dropout rate. LR: global learning rate. SSM LR: the SSM learning rate. B: batch size. Epochs: max epochs set for the run. WD: weight decay. |  | Depth | H | P | J | Dropout | LR | SSM LR | B | Epochs | WD |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ListOps | 8 | 128 | 16 | 8 | 0.0 | 0.003 | 0.001 | 50 | 40 | 0.04 |\n| Text | 6 | 256 | 192 | 12 | 0.1 | 0.004 | 0.001 | 50 | 35 | 0.07 |\n| Retrieval | 6 | 128 | 256 | 16 | 0.0 | 0.002 | 0.001 | 32 | 20 | 0.05 |\n| Image | 6 | 512 | 384 | 3 | 0.1 | 0.005 | 0.001 | 50 | 250 | 0.07 |\n| Pathfinder | 6 | 192 | 256 | 8 | 0.05 | 0.005 | 0.0009 | 64 | 200 | 0.03 |\n| Path-X | 6 | 128 | 256 | 16 | 0.0 | 0.002 | 0.0006 | 32 | 75 | 0.06 |\n| Speech | 6 | 96 | 128 | 16 | 0.1 | 0.008 | 0.002 | 16 | 40 | 0.04 |\n| Pendulum | 4 | 30 | 16 | 8 | 0.0 | 0.012 | 0.003 | 32 | 100 | 0.0 |\n| sMNIST | 4 | 96 | 128 | 1 | 0.1 | 0.008 | 0.002 | 50 | 150 | 0.01 |\n| psMNIST | 4 | 128 | 128 | 2 | 0.15 | 0.004 | 0.001 | 50 | 150 | 0.01 |\n| sCIFAR | 6 | 512 | 384 | 3 | 0.1 | 0.0045 | 0.001 | 50 | 250 | 0.07 |\n\n## G.3.2 TEXT\n\nNo exceptions to the defaults for this run. ## G.3.3 RETRIEVAL\n\nThis document matching task requires a slightly different architecture from the other experiments, as discussed in Tay et al.",
    "s5-49": "(2021). We use the same configuration as S4 (Gu et al., 2021a). Each string is passed through the input encoder, S5 layers, and mean pooling layers. Denoting $X_{1}$ as the output for the first document and $X_{2}$ as the output for the second document, four features are created and concatenated together (Tay et al., 2021) as\n\n$$\nX=\\left[X_{1}, X_{2}, X_{1} * X_{2}, X_{1}-X_{2}\\right]\n$$\n\nThis concatenated feature is then fed to a linear decoder and softmax function as normal. ## G.3.4 ImAGE\n\nWeight decay and the global learning rate were applied to $\\tilde{\\mathbf{B}}$. ## G.3.5 PATHFINDER\n\nNo exceptions to the defaults for this run. ## G.3.6 PATH-X\n\nWeight decay was applied to $\\tilde{\\mathbf{B}}$. ## G.3.7 SpeECh COMmAndS\n\nNo weight decay and the SSM learning rate were applied to $\\tilde{\\mathbf{C}}$ for this run. ## G.3.8 PENDULUM REGRESSION\n\nWe use the same encoder-decoder architecture as Schirmer et al. (2022). The encoder has layers: convolution, ReLU, max pool, convolution, ReLU, max pool, dense, ReLU, dense. The first convolution layer has twelve features, a $5 \\times 5$ kernel, and a padding of $(2,2)$. The second convolution layer has twelve features, a $3 \\times 3$ kernel, a stride of 2 , and a padding of $(1,1)$. Both max pools use a window size of $2 \\times 2$ and a stride of 2 . The dense layer has thirty hidden units. The linear readout layer outputs $H=30$ features. This is chosen to match the encoding size in Schirmer et al. (2022),\nand is used for all layers. Separate mean and unconstrained variance decoders are used, defined as a one-layer MLP with a hidden size of thirty. An elu+1 activation function is used to constrain the variance to be positive. Layer normalization and post-norm were used for this task. For the timings presented in Table 3 and 9 we use a batch size of 50 , instead of the batch size of 32 used during training, to match the batch sizes reported by the baselines. ## G.3.9 SEQUENTIAL MNIST\n\nNo exceptions to the defaults for this run. ## G.3.10 Permuted SeQuential MNIST\n\nWeight decay and the global learning rate were applied to $\\tilde{\\mathbf{B}}$. Post-norm was used for this task. ## G.3.11 SEQUENTIAL CIFAR\n\nWe trained a model with the exact hyperparameter settings as used for the LRA-IMAGE (grayscale sequential CIFAR) task with no further tuning. ## G. 4 DATASET DETAILS\n\nWe provide more context and details for each of the LRA (Tay et al., 2021) and Speech Commands (Warden, 2018) datasets we consider. Note that we follow the same data pre-processing steps as Gu et al. (2021a), which we also include here for completeness. - ListOps: A lengthened version of the dataset presented by Nangia \\& Bowman (2018). Given a nested set of mathematical operations (such as min and max) and integer operands in the range zero to nine, expressed in prefix notation with brackets, compute the integer result of the mathematical expression, e.g. [MAX29[MIN47]0] $\\rightarrow$ 9. Characters are encoded as one-hot vectors, with 17 unique values possible (opening brackets and operators are grouped into a single token). The sequences are of unequal length, and hence the end of shorter sequences is padded with a fixed indicator value, padded to a maximum length of 2,000 . A reserved end-of-sequence token is appended. There are 10 different classes, representing the integer result of the expression. There are 96,000 training sequences, 2,000 validation sequences, and 2,000 test sequences. No normalization is applied. - Text: Based off of the iMDB sentiment dataset presented by Maas et al. (2011). Given a movie review, where characters are encoded as a sequence of integer tokens, classify whether the movie review is positive or negative. Characters are encoded as one-hot vectors, with 129 unique values possible. Sequences are of unequal length, and are padded to a maximum length of 4,096 . There are two different classes, representing positive and negative sentiment. There are 25,000 training examples and 25,000 test examples. No validation set is provided. No normalization is applied. - Retrieval: Based off of the ACL Anthology network corpus presented by Radev et al. (2009). Given two textual citations, where characters are encoded as a sequence of integer tokens, classify whether the two citations are equivalent. The citations must be compressed separately, before being passed into a final classifier layer. This is to evaluate how effectively the network can represent the text. The decoder head then uses the encoded representation to complete the task. Characters are encoded into a one-hot vector with 97 unique values. Two paired sequences may be of unequal length, with a maximum sequence length of 4,000 . There are two different classes, representing whether the citations are equivalent or not. There are 147, 086 training pairs, 18, 090 validation pairs, and 17,437 test pairs. No normalization is applied. - Image: Uses the CIFAR-10 dataset presented by Krizhevsky (2009). Given a $32 \\times 32$ grayscale CIFAR-10 image as a one-dimensional raster scan, classify the image into one of ten classes. Sequences are of equal length $(1,024)$. There are ten different classes. There are 45,000 training examples, 5,000 validation examples, and 10,000 test examples. RGB\npixel values are converted to a grayscale intensities, which are then normalized to have zero mean and unit variance (across the entire dataset).",
    "s5-50": "- Pathfinder: Based off of the Pathfinder challenge introduced by Linsley et al. (2018). A $32 \\times 32$ grayscale image image shows a start and an end point as a small circle. There are a number of dashed lines on the image. The task is to classify whether there is a dashed line (or path) joining the start and end point. There are two different classes, indicating whether there is a valid path or not. Sequences are all of the same length $(1,024)$. There are 160,000 training examples, 20,000 validation examples, and 20,000 test examples. The data is normalized to be in the range $[-1,1]$. - Path-X: An \"extreme\" version of the Pathfinder challenge. Instead, the images are $128 \\times 128$ pixels, resulting in sequences that are a factor of sixteen times longer. Otherwise identical to the Pathfinder challenge. - Speech Commands: Based on the dataset released by Warden (2018). Readers recite one of 35 words. The task is then to classify which of the 35 words was spoken from a 16 kHz one-dimensional audio recording. There are 35 different classes, each representing one of the words in the vocabulary. Sequences are all of the same length $(16,000)$. There are 24,482 training examples, 5,246 validation examples, and 5,247 test examples. Data is normalized to be zero mean and have a standard deviation of 0.2 . - Speech Commands $\\times$ 0.5: Temporally sub-sampled version of Speech Commands, where the validation and test datasets only are sub-sampled by a factor of $1 / 0.5$, and are therefore shortened to length 8,000 .",
    "s5-51": "No subsequent padding is applied. The training dataset is not subsampled. - Sequential MNIST: (sMNIST) 10-way digit classification from a $28 \\times 28$ grayscale image of a handwritten digit, where the input image is flattened into a 784 -length scalar sequence. - Permuted Sequential MNIST: (psMNIST) 10-way digit classification from a $28 \\times 28$ grayscale image of a handwritten digit, where the input image is flattened into a 784 -length scalar sequence. This sequence is then permuted using a fixed order. - Sequential CIFAR: (sCIFAR): 10-way image classification using the CIFAR-10 dataset. Identical to image, except that full colour images are input as a 1, 024 -length input sequence, where each input is an ( $R, G, B$ ) triple.",
    "s5-52": "- Pendulum Regression: Reproduced from Becker et al. (2019) and Schirmer et al. (2022). The input sequence is a $24 \\times 24$ grayscale rendering of a pendulum, driven by a random torque process. The images pixels are corrupted by a noise process that is correlated in time. The pendulum is simulated for 100 timesteps, and 50 frames are irregularly sampled without replacement from the simulation. The objective is to estimate the sine and cosine of the angle of the pendulum. A train/validation/test split of $2,000 / 1,000 / 1,000$ is used. ## H Background on Parallel Scans for LinEar RECurRences\n\nFor the interested reader, this section provides more background on using a parallel scan for a linear recurrence, as well as a simple example to illustrate how it can compute the recurrence in parallel. The parallelization of scan operations has been well studied (Ladner \\& Fischer, 1980; Lakshmivarahan \\& Dhall, 1994; Blelloch, 1990), and many standard scientific computing libraries contain efficient implementations. We note the linear recurrence we consider here is a specific instance of the more general setting discussed in Section 1.4 of Blelloch (1990). Computing a general parallel scan requires defining two objects:\n\n- The initial elements the scan will operate on. - A binary associative operator $\\bullet$ used to combine the elements. To compute a length $L$ linear recurrence, $x_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} x_{k}$, we will define the $L$ initial elements, $c_{1: L}$, such that each element $c_{k}$ is the tuple\n\n$$\nc_{k}=\\left(c_{k, a}, c_{k, b}\\right):=\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{k}\\right)\n$$\n\nThese $c_{1: L}$ will be precomputed prior to the scan. Having created the list of elements for the scan to operate on, we define the binary operator $\\bullet$ for the scan to use on this linear recurrence as\n\n$$\nq_{i} \\bullet q_{j}:=\\left(q_{j, a} \\odot q_{i, a}, \\quad q_{j, a} \\otimes q_{i, b}+q_{j, b}\\right)\n$$\n\nwhere $q_{k}$ denotes an input element to the operator that could be the initial elements $c_{k}$ or some intermediate result, $\\odot$ denotes matrix-matrix multiplication, $\\otimes$ denotes matrix-vector multiplication and + denotes elementwise addition. We show that this operator is associative at the end of this section. Simple example using binary operator We can illustrate how $\\bullet$ can be used to compute a linear recurrence in parallel with a simple example. Consider the system $x_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} u_{k}$, and a length $L=4$ sequence of inputs $u_{1: 4}$. Assuming $x_{0}=0$, the desired latent states from this recurrence are:\n\n$$\n\\begin{aligned}\n& x_{1}=\\overline{\\mathbf{B}} u_{1} \\\\\n& x_{2}=\\overline{\\mathbf{A B}} u_{1}+\\overline{\\mathbf{B}} u_{2} \\\\\n& x_{3}=\\overline{\\mathbf{A}}^{2} \\overline{\\mathbf{B}} u_{1}+\\overline{\\mathbf{A B}} u_{2}+\\overline{\\mathbf{B}} u_{3} \\\\\n& x_{4}=\\overline{\\mathbf{A}}^{3} \\overline{\\mathbf{B}} u_{1}+\\overline{\\mathbf{A}}^{2} \\overline{\\mathbf{B}} u_{2}+\\overline{\\mathbf{A B}} u_{3}+\\overline{\\mathbf{B}} u_{4}\n\\end{aligned}\n$$\n\nWe first note that $\\bullet$ can be used to compute this recurrence sequentially. We can initialize the scan elements $c_{1: 4}$ as in (33), and then sequentially scan over these elements to compute the output elements $s_{i}=s_{i-1} \\bullet c_{i}$. Defining $s_{0}:=(\\mathbf{I}, 0)$ where $\\mathbf{I}$ is the identity matrix, we have for our example:\n\n$$\n\\begin{aligned}\n& s_{1}=s_{0} \\bullet c_{1}=(\\mathbf{I}, 0) \\bullet\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{1}\\right)=\\left(\\overline{\\mathbf{A}} \\mathbf{I}, \\overline{\\mathbf{A}} 0+\\overline{\\mathbf{B}} u_{1}\\right)=\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{1}\\right) \\\\\n& s_{2}=s_{1} \\bullet c_{2}=\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{1}\\right) \\bullet\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{2}\\right)=\\left(\\overline{\\mathbf{A}}^{2}, \\overline{\\mathbf{A B}} u_{1}+\\overline{\\mathbf{B}} u_{2}\\right) \\\\\n& s_{3}=s_{2} \\bullet c_{3}=\\left(\\overline{\\mathbf{A}}^{2}, \\overline{\\mathbf{A B}} u_{1}+\\overline{\\mathbf{B}} u_{2}\\right) \\bullet\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{3}\\right)=\\left(\\overline{\\mathbf{A}}^{3}, \\overline{\\mathbf{A}}^{2} \\overline{\\mathbf{B}} u_{1}+\\overline{\\mathbf{A B}} u_{2}+\\overline{\\mathbf{B}} u_{3}\\right) \\\\\n& s_{4}=s_{3} \\bullet c_{4}=\\left(\\overline{\\mathbf{A}}^{3}, \\overline{\\mathbf{A}}^{2} \\overline{\\mathbf{B}} u_{1}+\\overline{\\mathbf{A B}} u_{2}+\\overline{\\mathbf{B}} u_{3}\\right) \\bullet\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{4}\\right) \\\\\n& =\\left(\\overline{\\mathbf{A}}^{4}, \\quad \\overline{\\mathbf{A}}^{3} \\overline{\\mathbf{B}} u_{1}+\\overline{\\mathbf{A}}^{2} \\overline{\\mathbf{B}} u_{2}+\\overline{\\mathbf{A B}} u_{3}+\\overline{\\mathbf{B}} u_{4}\\right) \\text {. }\n\\end{aligned}\n$$\n\nNote that the second entry of each of the output tuples, $s_{i, b}$, contains the desired $x_{i}$ computed above. Computing the scan in this way requires four sequential steps since each $s_{i}$ depends on $s_{i-1}$. Now consider how we can use this binary operator to compute the recurrence in parallel. We will label the output elements of the parallel scan as $r_{1: 4}$ and define $r_{0}=(\\mathbf{I}, 0)$. We will first compute the even indexed elements $r_{2}$ and $r_{4}$, and then compute the odd indexed elements $r_{1}$ and $r_{3}$. We start by applying the binary operator $\\bullet$ to adjacent pairs of our initial elements $c_{1: 4}$ to compute $r_{2}$ and the\nintermediate result $q_{4}$, and we then repeat this process to compute $r_{4}$ by applying $\\bullet$ to $r_{2}$ and $q_{4}$ :\n\n$$\n\\begin{aligned}\nr_{2}=c_{1} \\bullet c_{2} & =\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{1}\\right) \\bullet\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{2}\\right)=\\left(\\overline{\\mathbf{A}}^{2}, \\overline{\\mathbf{A B}} u_{1}+\\overline{\\mathbf{B}} u_{2}\\right) \\\\\nq_{4}=c_{3} \\bullet c_{4} & =\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{3}\\right) \\bullet\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{4}\\right)=\\left(\\overline{\\mathbf{A}}^{2}, \\overline{\\mathbf{A B}} u_{3}+\\overline{\\mathbf{B}} u_{4}\\right) \\\\\nr_{4}=r_{2} \\bullet q_{4} & =\\left(\\overline{\\mathbf{A}}^{2}, \\overline{\\mathbf{A B}} u_{1}+\\overline{\\mathbf{B}} u_{2}\\right) \\bullet\\left(\\overline{\\mathbf{A}}^{2}, \\overline{\\mathbf{A B}} u_{3}+\\overline{\\mathbf{B}} u_{4}\\right) \\\\\n& =\\left(\\overline{\\mathbf{A}}^{4}, \\overline{\\mathbf{A}}^{3} \\overline{\\mathbf{B}} u_{1}+\\overline{\\mathbf{A}}^{2} \\overline{\\mathbf{B}} u_{2}+\\overline{\\mathbf{A B}} u_{3}+\\overline{\\mathbf{B}} u_{4}\\right)\n\\end{aligned}\n$$\n\nNow we will compute the odd indexed elements $r_{1}$ and $r_{3}$, using the even indexed $r_{0}$ and $r_{2}$, as $r_{k}=r_{k-1} \\bullet c_{k}$ :\n\n$$\n\\begin{aligned}\n& r_{1}=r_{0} \\bullet c_{1}=(I, 0) \\bullet\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{1}\\right)=\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{1}\\right) \\\\\n& r_{3}=r_{2} \\bullet c_{3}=\\left(\\overline{\\mathbf{A}}^{2}, \\overline{\\mathbf{A B}} u_{1}+\\overline{\\mathbf{B}} u_{2}\\right) \\bullet\\left(\\overline{\\mathbf{A}}, \\overline{\\mathbf{B}} u_{3}\\right)=\\left(\\overline{\\mathbf{A}}^{3}, \\overline{\\mathbf{A}}^{2} \\overline{\\mathbf{B}} u_{1}+\\overline{\\mathbf{A B}} u_{2}+\\overline{\\mathbf{B}} u_{3}\\right)\n\\end{aligned}\n$$\n\nNote that the second entry of each of the output tuples, $r_{k, b}$, corresponds to the desired $x_{k}$. Inspecting the required dependencies for each application of $\\bullet$, we see that $r_{2}$ and the intermediate result $q_{4}$ can be computed in parallel. Once $r_{2}$ and $q_{4}$ are computed, $r_{1}, r_{3}$ and $r_{4}$ can all be computed in parallel. We have therefore reduced the number of sequential steps required from four in the sequential scan version to two in the parallel scan version. This reduction in sequential steps becomes important when the sequence length is large since, given sufficient processors, the parallel time scales logarithmically with the sequence length. Associativity of binary operator Finally, for completeness, we show that the binary operator $\\bullet$ is associative:\n\n$$\n\\begin{aligned}\n\\left(q_{i} \\bullet q_{j}\\right) \\bullet q_{k} & =\\left(q_{j, a} \\odot q_{i, a}, q_{j, a} \\otimes q_{i, b}+q_{j, b}\\right) \\bullet q_{k} \\\\\n& =\\left(q_{k, a} \\odot\\left(q_{j, a} \\odot q_{i, a}\\right), q_{k, a} \\otimes\\left(q_{j, a} \\otimes q_{i, b}+q_{j, b}\\right)+q_{k, b}\\right) \\\\\n& =\\left(\\left(q_{k, a} \\odot q_{j, a}\\right) \\odot q_{i, a}, q_{k, a} \\otimes\\left(q_{j, a} \\otimes q_{i, b}\\right)+q_{k, a} \\otimes q_{j, b}+q_{k, b}\\right) \\\\\n& =\\left(\\left(q_{k, a} \\odot q_{j, a}\\right) \\odot q_{i, a}, \\quad\\left(q_{k, a} \\odot q_{j, a}\\right) \\otimes q_{i, b}+q_{k, a} \\otimes q_{j, b}+q_{k, b}\\right) \\\\\n& =q_{i} \\bullet\\left(q_{k, a} \\odot q_{j, a}, q_{k, a} \\otimes q_{j, b}+q_{k, b}\\right) \\\\\n& =q_{i} \\bullet\\left(q_{j} \\bullet q_{k}\\right)\n\\end{aligned}\n$$\n\n\n[^0]:    ${ }^{1}$ The full S5 implementation is available at: https: / / github.com/lindermanlab/S5. "
}