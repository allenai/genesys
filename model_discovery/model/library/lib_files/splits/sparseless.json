{
    "sparseless-0": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers\n\nChao Lou1 Zixia Jia2 Zilong Zheng2,\u2217 Kewei Tu1, 1 ShanghaiTech University 2 National Key Laboratory of General Artificial Intelligence, BIGAI {louchao,tukw}@shanghaitech.edu.cn {jiazixia,zlzheng}@bigai.ai Corresponding Author\n\nAbstract\n\nAccommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms.",
    "sparseless-1": "In this work, we introduce SparseK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SparseK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SparseK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SparseK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications. Our code will be publicly available. 1 Introduction\n\nTransformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8, 21], and more recently, constructing large language models (LLMs) [9, 69]. Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges [1, 20, 19], owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices. Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths. However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs. Clustering-based methods [39, 61] allow queries to attend to different sets of KV pairs. In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81, 42, 78]. (ii) Previous learnable sparse attention often has super-linear complexity, especially during training.",
    "sparseless-2": "For example, clustering-based methods usually cost to maintain clusters. Ainslie et al. [1] incorporates a SoftTopK operator [41] to compute soft masks in Transformer encoders. Meanwhile, migrating SoftTopK to Transformer decoders is less advantageous because solving SoftTopK for variable-length context associated with different queries requires quadratic time in total. To tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1. Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top- mask operator SparseK, which normalizes scores to a soft mask (or gates) in linear time. It is worth noting that our method draws inspiration from the concept of top- attention [32, 1]. Unfortunately, conventional top- attention is non-differentiable and therefore cannot be used to train the scoring network. With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SparseK attention as follows. Incremental KV Selection. The SparseK operator ( 3.3) supports incremental evaluation and thus has a linear complexity in the decoder. Besides, compared with SoftTopK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results. Computational and Memory Efficiency. SparseK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65, 32, 2, 47] to linear time and achieves constant memory cost in inference. This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference. Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs. This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately ( 3.2). Extension with IO-awareness. FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness. However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application. To address this, we develop a Triton kernel that fuses the computation of attention and the selection of proper key-value pairs. Our implementation exhibits linear complexity and surpasses FlashAttention in performance when handling 4096 input tokens, of which 1024 key-value pairs are selected for each query. Additionally, we offer a kernel for the backward pass, which fuses the computation of the gradient of SparseK and others, resulting in increased speed and improved memory efficiency. We verify the advantages of SparseK attention by replacing full attention in various models (such as GPT2 [57] and Pythia [6]) with it and other efficient attention methods. We consider a wide range of settings, including training from scratch and fine-tuning pretrained models. Experiments on language modeling and downstream tasks demonstrate that, when matching the context size, our method outperforms other efficient attention methods consistently while providing promising speed-up at training compared to full attention. 2 Related Work\n\nLong-range Transformers\n\nSelf-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context. Numerous efficient approaches have emerged, spanning state-space models [30, 62], recurrent neural networks [45, 52, 49], linear attention [55, 38] and low-rank approximations of self-attention [75, 14, 53], which replace the self-attention with novel linear blocks for long-context modeling. Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29, 77]. Besides, a few studies combine the Transformer with block-wise recurrence [17, 35, 36, 12] or key-value compression [60, 59, 18]. In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix. This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions [15, 27, 42]. Sparse attention\n\nSome sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56, 51], dilated sliding windows [4, 22], combination of patterns [34, 13], or domain-specific patterns [31]. Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81, 42, 27].",
    "sparseless-3": "However, these static methods often prove suboptimal in various scenarios [66, 2]. Alternatively, sparse patterns can be learned in a data-driven manner. For example, Reformer [39] employs locality-sensitive hashing for token clustering and do attention within a cluster, while Routing Transformers [61], Cluster-Former [74] and Clustered Attention [73] use K-Means clustering on tokens. Besides, Sparse Sinkhorn Attention [68] establishes sparsity by sorting blocks of inputs. Despite achieving sub-quadratic complexity, these methods still remain above linear complexity and face challenges when handling extremely long sequences or failing to offer constant memory cost during inference. A recent approach by Anagnostidis et al. [2] introduces a learnable, irreversible key-value pair pruning for inference-time memory efficiency with the concept of relaxing pruning actions to accumulated gating. However, this method still suffers from quadratic complexity during training, hindering its ability to expedite the training process. In this paper, we present a novel, efficient sparse attention mechanism with learnable patterns, addressing all the aforementioned challenges. 3 SparseK Attention\n\n3.1 Background\n\nSelf-Attention\n\nGiven a sequence of vectors where is the sequence length and is the hidden dimension, an attention head first projects into query, key and value vectors with where and is the number of attention heads:\n\n\ud835\udc78 = \ud835\udc7f \u200b \ud835\udc7e Q \ud835\udc78 \ud835\udc7f subscript \ud835\udc7e \ud835\udc44 \\displaystyle\\bm{Q}=\\bm{X}\\bm{W}_{Q} \ud835\udc72 = \ud835\udc7f \u200b \ud835\udc7e K \ud835\udc72 \ud835\udc7f subscript \ud835\udc7e \ud835\udc3e \\displaystyle\\bm{K}=\\bm{X}\\bm{W}_{K} \ud835\udc7d = \ud835\udc7f \u200b \ud835\udc7e V , \ud835\udc7d \ud835\udc7f subscript \ud835\udc7e \ud835\udc49 \\displaystyle\\bm{V}=\\bm{X}\\bm{W}_{V}, (1)\n\nIn the decoder-only architecture [72], a causal attention mask guarantees each query only attends to positions . Consequently, the output of single-head dot-product attention is defined as\n\n\ud835\udc7a = \ud835\udc78 \u200b \ud835\udc72 \u22a4 \ud835\udc77 = SoftMax \u200b ( \ud835\udc7a + \ud835\udc74 ) \ud835\udc76 = \ud835\udc77 \u200b \ud835\udc7d formulae-sequence \ud835\udc7a \ud835\udc78 superscript \ud835\udc72 top formulae-sequence \ud835\udc77 SoftMax \ud835\udc7a \ud835\udc74 \ud835\udc76 \ud835\udc77 \ud835\udc7d \\displaystyle\\bm{S}=\\bm{Q}\\bm{K}^{\\top}\\qquad\\bm{P}=\\textsc{SoftMax}(\\bm{S}+\\bm{M})\\qquad\\bm{O}=\\bm{P}\\bm{V} (2)\n\nThe multi-head self-attention concatenates the outputs of multiple heads (indexed by subscripts) and applies a linear projection with :\n\nMHA \u200b ( \ud835\udc7f ) = Concatenate \u200b ( \ud835\udc76 1 , \ud835\udc76 2 , \u2026 , \ud835\udc76 h ) \u200b \ud835\udc7e O MHA \ud835\udc7f Concatenate subscript \ud835\udc76 1 subscript \ud835\udc76 2 \u2026 subscript \ud835\udc76 \u210e subscript \ud835\udc7e \ud835\udc42 \\displaystyle\\text{MHA}(\\bm{X})=\\text{Concatenate}(\\bm{O}_{1},\\bm{O}_{2},\\dots,\\bm{O}_{h})\\bm{W}_{O} (3)\n\nThe quadratic complexity of self-attention is contributed by the quadratically sized attention weight . Inspired by Ainslie et al. [1], we propose to select a constant number of key-value pairs for each query in an irreversible way (defined formally in the following subsections 3.2 and 3.3), leading to linear training complexity and a constant inference-time memory cost. For simplicity, here we omit the RoPE position embedding [64] and focus on single-head attention to illustrate our methodology. The multi-head case is briefly discussed in Appendix C.7. SparseMax operator\n\nThere are many popular technical choices that relax ArgMax operation, such as SoftMax and SparseMax [46]. Especially, SparseMax uses the Euclidean projection onto the probabilistic simplex and tends to yield sparse solutions:\n\nSparseMax \u200b ( \ud835\udc9b ) SparseMax \ud835\udc9b \\displaystyle\\textsc{SparseMax}({\\bm{z}}) \u2254 arg \u200b min \ud835\udc91 \u2208 \u25b3 m \u2212 1 \u200b \u2016 \ud835\udc91 \u2212 \ud835\udc9b \u2016 2 , \u2254 absent subscript arg min \ud835\udc91 superscript \u25b3 \ud835\udc5a 1 superscript norm \ud835\udc91 \ud835\udc9b 2 \\displaystyle\\coloneqq\\operatorname*{arg\\,min}_{{\\bm{p}}\\in\\triangle^{m-1}}||{\\bm{p}}-{\\bm{z}}||^{2}, (4)\n\nwhere . Building on this, we introduce SparseK, an extension of SparseMax for the case where . 3.2 Learnable Key-Value Pair Selection\n\nKey-value pair selection\n\nWe use to represent the selection of key-value pairs out of entries, where indicates that the -th key-value pair is the -th selected entry ( i.e., the -th key-value pair is positioned in the -th slot after sorting), and otherwise.",
    "sparseless-4": "It is noteworthy that each column vector and row vector exhibit one-hot characteristics. We use subscripts to distinguish selection corresponding to different queries, i.e., for . The causality of Transformer decoders puts a natural constraint: if . Then, the self-attention with query and its selected contexts are defined as\n\n\ud835\udc92 i = \ud835\udc7e Q \u200b \ud835\udc99 i \ud835\udc72 ^ i = \u0394 i \u200b \ud835\udc72 \ud835\udc7d ^ i = \u0394 i \u200b \ud835\udc7d \ud835\udc94 ^ i = \ud835\udc92 i \u22a4 \u200b \ud835\udc72 ^ i \ud835\udc90 i = \ud835\udc91 ^ i \u22a4 \u200b \ud835\udc7d ^ i \ud835\udc91 ^ i = SoftMax \u200b ( \ud835\udc94 ^ i ) , missing-subexpression formulae-sequence subscript \ud835\udc92 \ud835\udc56 subscript \ud835\udc7e \ud835\udc44 subscript \ud835\udc99 \ud835\udc56 formulae-sequence subscript ^ \ud835\udc72 \ud835\udc56 subscript \u0394 \ud835\udc56 \ud835\udc72 subscript ^ \ud835\udc7d \ud835\udc56 subscript \u0394 \ud835\udc56 \ud835\udc7d missing-subexpression missing-subexpression formulae-sequence subscript ^ \ud835\udc94 \ud835\udc56 superscript subscript \ud835\udc92 \ud835\udc56 top subscript ^ \ud835\udc72 \ud835\udc56 formulae-sequence subscript \ud835\udc90 \ud835\udc56 superscript subscript ^ \ud835\udc91 \ud835\udc56 top subscript ^ \ud835\udc7d \ud835\udc56 subscript ^ \ud835\udc91 \ud835\udc56 SoftMax subscript ^ \ud835\udc94 \ud835\udc56 missing-subexpression \\displaystyle\\begin{aligned} &{\\bm{q}}_{i}=\\bm{W}_{Q}{\\bm{x}}_{i}\\qquad\\hat{\\bm{K}}_{i}=\\Delta_{i}\\bm{K}\\qquad\\hat{\\bm{V}}_{i}=\\Delta_{i}\\bm{V}&\\\\\n&\\hat{{\\bm{s}}}_{i}={\\bm{q}}_{i}^{\\top}\\hat{\\bm{K}}_{i}\\quad{\\bm{o}}_{i}=\\hat{{\\bm{p}}}_{i}^{\\top}\\hat{\\bm{V}}_{i}\\quad\\hat{{\\bm{p}}}_{i}=\\textsc{SoftMax}(\\hat{{\\bm{s}}}_{i}),&\\end{aligned} (5)\n\nIrreversibile selection\n\nIn a series of decoder steps, the selection of key-value pairs for queries is performed over incremental candidate sets (e.g., ). Irreversible selection is a strategy that, at step , () can be selected only if it has been selected in all preceding steps from to , Irreversible selection is memory-efficient because it guarantees the feasibility of pruning a key-value pair once it is not selected at a step. Many previous studies, such as clustering-based methods [61, 39], put no constraint on interactions between queries and key-value pairs and thus are not irreversible, resulting in a memory cost equivalent to that of pure self-attention. We employ a simple selection strategy in this paper: we score each key-value pair and choose the key-value pairs with the top- scores. This strategy is irreversible because of the monotonic increase of the -th largest value in incremental sets. Note that we do not use attention scores as the selection criteria [32] to avoid the quadratic cost of computing . Learnable selection\n\nFormally, we use a scoring network, a linear projection with parameter augmented with a position slope , to get the importance scores of the key-value pairs computed from the input vectors :\n\n\ud835\udc96 = \ud835\udc7f \u200b \ud835\udc98 s \u200b c \u200b o \u200b r \u200b e + d \u200b ( \ud835\udc96 ) , where d \u200b ( \ud835\udc96 ) i = i \u200b \u03f5 . formulae-sequence \ud835\udc96 \ud835\udc7f subscript \ud835\udc98 \ud835\udc60 \ud835\udc50 \ud835\udc5c \ud835\udc5f \ud835\udc52 \ud835\udc51 \ud835\udc96 where \ud835\udc51 subscript \ud835\udc96 \ud835\udc56 \ud835\udc56 italic-\u03f5 \\displaystyle{\\bm{u}}=\\bm{X}{\\bm{w}}_{score}+d({\\bm{u}}),\\qquad\\text{where}\\quad d({\\bm{u}})_{i}=i\\epsilon. (6)\n\nfor some small positive scalar . The introduction of distance slope is to avoid numerical explosion of : if without it, the scoring network would be compelled to predict increasingly larger scores to retain new tokens and discard old ones. This hurts training stabilitiy and generalization ability to larger context length. Then, the selection matrix for query can be defined to select top- entries,\n\n\ud835\udc8e t \u200b o \u200b p \u200b k = TopK \u200b ( \ud835\udc96 1 : i , k ) \u0394 t h \u200b a \u200b r \u200b d = MaskSelect \u200b ( Diag \u200b ( \ud835\udc8e t \u200b o \u200b p \u200b k ) , \ud835\udc8e t \u200b o \u200b p \u200b k ) , missing-subexpression subscript \ud835\udc8e \ud835\udc61 \ud835\udc5c \ud835\udc5d \ud835\udc58 TopK subscript \ud835\udc96 : 1 \ud835\udc56 \ud835\udc58 missing-subexpression superscript subscript \u0394 \ud835\udc61 \u210e \ud835\udc4e \ud835\udc5f \ud835\udc51 MaskSelect Diag subscript \ud835\udc8e \ud835\udc61 \ud835\udc5c \ud835\udc5d \ud835\udc58 subscript \ud835\udc8e \ud835\udc61 \ud835\udc5c \ud835\udc5d \ud835\udc58 \\displaystyle\\begin{aligned} &{\\bm{m}}_{topk}=\\textsc{TopK}({\\bm{u}}_{1:i},k)\\\\\n&\\Delta_{t}^{hard}=\\text{MaskSelect}(\\text{Diag}({\\bm{m}}_{topk}),{\\bm{m}}_{topk}),\\end{aligned} (7)\n\nwhere is an indicator vector, i.e., if ranks within the top- of and otherwise, and is a matrix with in the main diagonal. The function selects rows of according to the mask . TopK is not differentiable with respect to its input, preventing us from updating with gradient-based methods. Therefore, we propose to use SparseK, a differentiable relaxation of TopK, in Equation (7),\n\n\ud835\udc8e s \u200b p \u200b a \u200b r \u200b s \u200b e \u200b k = SparseK \u200b ( \ud835\udc96 1 : i , k ) \u0394 t s \u200b o \u200b f \u200b t = MaskSelect \u200b ( Diag \u200b ( \ud835\udc8e s \u200b p \u200b a \u200b r \u200b s \u200b e \u200b k ) , \ud835\udc8e t \u200b o \u200b p \u200b k ) , missing-subexpression subscript \ud835\udc8e \ud835\udc60 \ud835\udc5d \ud835\udc4e \ud835\udc5f \ud835\udc60 \ud835\udc52 \ud835\udc58 SparseK subscript \ud835\udc96 : 1 \ud835\udc56 \ud835\udc58 missing-subexpression superscript subscript \u0394 \ud835\udc61 \ud835\udc60 \ud835\udc5c \ud835\udc53 \ud835\udc61 MaskSelect Diag subscript \ud835\udc8e \ud835\udc60 \ud835\udc5d \ud835\udc4e \ud835\udc5f \ud835\udc60 \ud835\udc52 \ud835\udc58 subscript \ud835\udc8e \ud835\udc61 \ud835\udc5c \ud835\udc5d \ud835\udc58 \\displaystyle\\begin{aligned} &{\\bm{m}}_{sparsek}=\\textsc{SparseK}({\\bm{u}}_{1:i},k)\\\\\n&\\Delta_{t}^{soft}=\\text{MaskSelect}(\\text{Diag}({\\bm{m}}_{sparsek}),{\\bm{m}}_{topk}),\\end{aligned} (8)\n\n3.3 The Differentiable SparseK Operator\n\nDefinition\n\nWe relax the constraint in Equation (4) from a probablistic simplex to a k-sum constraint and define SparseK as follows,\n\nSparseK \u200b ( \ud835\udc9b , k ) SparseK \ud835\udc9b \ud835\udc58 \\displaystyle\\textsc{SparseK}({\\bm{z}},k) \u2254 arg \u200b min \ud835\udc91 \u2208 \u2102 \u200b \u2016 \ud835\udc91 \u2212 \ud835\udc9b \u2016 2 \u2254 absent subscript arg min \ud835\udc91 \u2102 superscript norm \ud835\udc91 \ud835\udc9b 2 \\displaystyle\\coloneqq\\operatorname*{arg\\,min}_{{\\bm{p}}\\in\\mathbb{C}}||{\\bm{p}}-{\\bm{z}}||^{2} (9) = arg \u200b max \ud835\udc91 \u2208 \u2102 \u2061 \ud835\udc91 \u22a4 \u200b \ud835\udc9b + H G \u200b ( \ud835\udc91 ) , absent subscript arg max \ud835\udc91 \u2102 superscript \ud835\udc91 top \ud835\udc9b superscript \ud835\udc3b \ud835\udc3a \ud835\udc91 \\displaystyle=\\operatorname*{arg\\,max}_{{\\bm{p}}\\in\\mathbb{C}}{\\bm{p}}^{\\top}{\\bm{z}}+H^{G}({\\bm{p}}), (10)\n\nwhere is the generalized Gini entropy for instead of a distribution. SparseK is related to SoftTopK as explained below. The generalized Gini entropy is a special case of the generalized -Tsallis entropy for [71], where for . Then we can define the generalized operator as\n\n\u03b1 \u200b -EntTopK = arg \u200b max \ud835\udc91 \u2208 \u2102 \u2061 \ud835\udc91 \u22a4 \u200b \ud835\udc9b + H T \u200b ( \ud835\udc91 ) . \ud835\udefc -EntTopK subscript arg max \ud835\udc91 \u2102 superscript \ud835\udc91 top \ud835\udc9b superscript \ud835\udc3b \ud835\udc47 \ud835\udc91 \\displaystyle\\alpha\\textsc{-EntTopK}=\\operatorname*{arg\\,max}_{{\\bm{p}}\\in\\mathbb{C}}{\\bm{p}}^{\\top}{\\bm{z}}+H^{T}({\\bm{p}}). (11)\n\nThe entropic index controls the smoothness of the solution. Taking the limit of , we get the TopK operator. Taking the limit of , the generalized -Tsallis entropy becomes the generalized Gibbs-Boltzmann-Shannon entropy and we obtain the SoftTopK operator [41]. Solution\n\nSimilar to SparseMax, SparseK is a soft-thresholding operation. Its solution is expressed as follows:\n\n\ud835\udc91 \u2217 superscript \ud835\udc91 \\displaystyle{\\bm{p}}^{*} = max \u2061 ( min \u2061 ( \ud835\udc9b \u2212 \u03c4 \u200b ( \ud835\udc9b ) , \ud835\udfcf ) , \ud835\udfce ) , absent \ud835\udc9b \ud835\udf0f \ud835\udc9b 1 0 \\displaystyle=\\max(\\min({\\bm{z}}-\\tau({\\bm{z}}),\\bm{1}),\\bm{0}), (12) \u03c4 \u200b ( \ud835\udc9b ) \ud835\udf0f \ud835\udc9b \\displaystyle\\tau({\\bm{z}}) = \u2211 u \u2217 < j \u2264 w \u2217 z ( j ) + u \u2217 \u2212 k w \u2217 \u2212 u \u2217 , absent subscript superscript \ud835\udc62 \ud835\udc57 superscript \ud835\udc64 subscript \ud835\udc67 \ud835\udc57 superscript \ud835\udc62 \ud835\udc58 superscript \ud835\udc64 superscript \ud835\udc62 \\displaystyle=\\frac{\\sum_{u^{*}<j\\leq w^{*}}z_{(j)}+u^{*}-k}{w^{*}-u^{*}}, (13)\n\nwhere is the threshold function that satisfies , is the sorted coordinates of , is the number of entries with value 1 in , and is the number of entries with nonzero values. Algorithm 1 illustrates an algorithm for evaluating SparseK. With the pre-computed cumulative sum of , line 5 can be evaluated in , and thus the overall complexity is primarily due to sorting. We give the proof in Appendix A. Algorithm 1 can be extended to evaluate SparseK on incremental sets. The idea is that we can compute step based on the results from step instead of starting from scratch. Algorithm 2 illustrates the algorithm, where highlighted lines are the main difference from Algorithm 1. We introduce two min-heaps (and maintain the sum of elements for each heap) for tracking the search progress of and achieving evaluation of line 9 in Algorithm 2. Note that each insertion into a min-heap costs logarithmic time in the heap size and each introduces at most two more possible pairs (lines 2-6 in Algorithm 2). Therefore, executing Algorithm 2 over incremental sets (i.e., steps) costs in total. As Peters et al. [54] have noted, the solution tends to contain only a few nonzeros, leading to small and . Therefore, in practice, we can use partial sort on the largest values instead of full sort in Algorithm 1, thereby achieving a complexity of . With respect to Algorithm 2, this change is equivalent to restricting the size of the min-heap to an upper bound for achieving the same reduction in complexity. 3.4 Extensions\n\nTraining with fixed-size truncation-free cache\n\nOur selection method enables training on extremely long documents that need to be segmented into smaller chunks for recurrent processing. Algorithm 3 illustrates the process. Without introducing any additional truncation strategies or parameters, the algorithm maintains a fixed-size cache benefit by recurrent calculations and produces exactly the same results as calculating without chunking, which is guaranteed by the irreversibility of our selection method. To minimize the memory footprint, we stop the gradients of the cache, thereby pruning the computation graph, as in Transformer-XL [17]. With this algorithm, we can extend the training context length to hundreds of thousands of tokens. Combine with other efficient attention mechanism\n\nOur SparseK attention can be combined with other sparse attention as long as they have irreversible selection patterns. In this work, we integrate SparseK attention with sliding window (SW) attention by default, motivated by the well-known experience that sliding windows are simple yet incredibly strong for language modeling [58, 37]. Specifically, given a sliding window size , we replace in (5) with\n\n\ud835\udc72 ^ i = [ \u0394 i \u2212 w \u200b \ud835\udc72 \ud835\udc72 i \u2212 w + 1 : i ] \ud835\udc7d ^ i = [ \u0394 i \u2212 w \u200b \ud835\udc7d \ud835\udc7d i \u2212 w + 1 : i ] , formulae-sequence subscript ^ \ud835\udc72 \ud835\udc56 delimited-[] matrix subscript \u0394 \ud835\udc56 \ud835\udc64 \ud835\udc72 subscript \ud835\udc72 : \ud835\udc56 \ud835\udc64 1 \ud835\udc56 subscript ^ \ud835\udc7d \ud835\udc56 delimited-[] matrix subscript \u0394 \ud835\udc56 \ud835\udc64 \ud835\udc7d subscript \ud835\udc7d : \ud835\udc56 \ud835\udc64 1 \ud835\udc56 \\displaystyle\\hat{\\bm{K}}_{i}=\\left[\\begin{matrix}\\Delta_{i-w}\\bm{K}\\\\\n\\bm{K}_{i-w+1:i}\\end{matrix}\\right]\\quad\\hat{\\bm{V}}_{i}=\\left[\\begin{matrix}\\Delta_{i-w}\\bm{V}\\\\\n\\bm{V}_{i-w+1:i}\\end{matrix}\\right], (14)\n\nThis combination does not introduce any overhead thanks to our fused Triton kernel. In this combination, SparseK attention attention aims at efficiently global (long-range) dependencies modeling, while SW attention is used for modeling local dependencies. Besides, SparseK attention can also be combined with linear attention methods, which hypothesize the existence of low-rank structures in attention scores rather than sparsity. From a theoretical perspective, Chen et al. [10] reveal that linear attention and sparse attention capture different attention patterns, and their combination provides a closer approximation to full attention. In this work, we extend their results to SparseK attention and recent attention optimizations [19]. For technical details, please refer to Appendix B.1. Straight-through estimator\n\nFrom TopK to SparseK, we employ relaxation techniques to facilitate gradient-based training. Alternatively, the straight-through estimator (ST) [5] can be utilized, i.e., , allowing the model to perform true selection. By utilizing the ST method, the model achieves slightly improved efficiency since it bypasses the multiplication of selection scores during the forward pass. Our experimental results indicate that employing ST results in negligible performance degradation. Consequently, this technique shows promise in balancing performance and computational efficiency. 3.5 Techniques for Faster and More Stable Training\n\nWe introduce three beneficial modeling tricks discovered in our experiments. We also develop an optimized implementation based on FlashAttention-2111https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py for obtaining practically efficient sparse attention. Please refer to Appendix B.2 for details. Score normalization\n\nWe add timestep normalization [44] on the time dimension: , which computes cumulative mean and variance for each timestep. Note that the gradient of SparseK operation is sparse due to thresholding. We hypothesize that the additional normalization enables gradients in every position, thus resulting in better training. Moreover, timestep normalization is necessary to avoid numerical explosion when combining SparseK attention with linear attention. More discussion is in Appendix B.1. Hard selection for keys and soft selection for values\n\nRecall that the SparseK attention is given by . We have found it generally beneficial to use instead, especially when fine-tuning pretrained models. This preference arises because the gradient of via can be problematic given that, in pretrained models, some entries of can be very close to the limitations of the bfloat16 data type, whereas and the output of SoftMax are generally stable. Initialization to mimic attention score\n\nWhen integrating our SparseK attention into pre-trained models, we hypothesize that an effective initialization for should ensure that the importance scores correspond with cumulative attention scores, preventing key-value pairs that receive heavy attention from being pruned. Inspired by the selection metric proposed in Yu et al. [78], whose ranking has been demonstrated to correlate strongly with the rankings of attention weights, we use within the pretrained model for initialization: where . 4 Experiments\n\nTo evaluate the efficiency, scalability, and compatibility of SparseK attention, we tested it across various model architectures and scales. Our experiments focus on language modeling tasks using the OpenWebText corpus [28] and the SlimPajama corpus [63], and downstream tasks in LongBench [3]. Empirically, the SparseK attention mechanism outperforms all previous efficient attention methods. More results, such as speed benchmark, ablation study and visualization, can be found in Appendix C. 4.1 Language Modeling from Scratch\n\nWe adopt the GPT-2 small architecture, comprising 124 million parameters [57]. Notably, we substitute the original absolute position embeddings with rotary position embeddings as proposed in Su et al. [64]. We alter the standard full attention in this architecture with our SparseK attention and several other efficient attention methods for comparison. For sparse attention, we include sliding window attention (SW) [56, 51], fixed sparse attention (Fixed) [13], randomized sparse attention (Random) [50] and hash attention (Hash) [39, 50]. We adjust sparsity configurations to restrict the context window size to about 256 when the context length is 1024 or 4096 and about 512 when 8192. For linear attention, we utilize the kernelization proposed by Katharopoulos et al. [38]. Additionally, we compare our methods to recent linear attention works that employ their own architectures rather than the GPT-2 architecture: GLA [77] and RetNet [67]. As the smallest GLA and RetNet is 340M, We modify their hyperparameters to align with our setting. Detailed hyperparameters and results of other configurations can be found in Appendix C.1. We trained all models on the OpenWebText222https://huggingface.co/datasets/Skylion007/openwebtext corpus for 10,000 steps, varying the context length. The results are presented in Table 1. Our SparseK+SW method consistently outperforms all previously established efficient attention methods. Particularly, SparseK+SW offers superior performance and has lower time complexity compared to previous learnable sparse attention methods, such as hash attention. Furthermore, linear attention methods, such as Linear+SW, GLA, and RetNet, demonstrate limitations, particularly in modeling long contexts. However, when combining linear attention with SparseK attention, we observed additional performance gains over SparseK+SW, even surpassing full attention. This suggests the potential of exploring a mixture of different attention methods for more efficient modeling. 4.2 Fine-tuning Existing Models\n\nWe replace the standard full attention in Pythia 160M, Pythia 410M [6] and TinyLlama 1.1B [80] with our SparseK attention and sliding window attention. The models are then fine-tuned over a few steps to ensure compatibility with the modified attention modules. Here we only consider sliding window attention because other efficient attention methods often require additional changes of the model architecture and sliding window attention is reported to be efficient in Chen et al. [11]. In fine-tuning, the NTK-aware interpolation [7] is adopted to extend the limit of pretrained positional encodings. For the Pythia models, we utilize a 1% sampled subset of the SlimPajama dataset333https://huggingface.co/datasets/DKYoon/SlimPajama-6B [63] to perform fine-tuning on moderate-length settings (i.e., 4k and 8k). In contrast, we use an upsampled dataset comprising long documents [25] to fine-tune the TinyLlama models on long-length settings (i.e., 8k and 16k). Training hyperparameters are listed in Appendix C.2. In Figure 2, we report the perplexity on the held-out set across various levels of sparsity and training context lengths. Extending the training context length and increasing the context size generally benefit all types of attention mechanisms. When matching the KV size, our SparseK+SW attention consistently outperforms sliding window attention. For the TinyLlama models, SparseK+SW attention achieves comparable perplexity using only half the KV size required by sliding window attention. These results underscore the advantages of a more adaptable context as implemented in SparseK+SW. We further evaluate TinyLlama 1.1B, fine-tuned with an 8k context window, across additional tasks as presented in the following sections. 4.3 Retrieval-based Evaluation and Length Extrapolation\n\nA common concern with sparse attention is its potential to neglect informative history. To investigate this, we evaluated our fine-tuned models on the passkey retrieval task [47], along with two baseline methods that require no training: dynamic NTK [7, 23] and LM-Infinite [33]. The results are presented in Figure 3(a). It is evident that the sliding window approach fails even within the trained context length. Furthermore, among the training-free methods, NTK utilizes full attention and extends the context length by a factor of four, whereas the memory-efficient method LM-Infinite fails in extrapolation. In contrast, SparseK+SW is memory-efficient while maintaining performance for context lengths well beyond four times longer. We also analyze the perplexity of tokens in various positional buckets within a long context, as depicted in Figure 3(b). In the language modeling task, SW demonstrates the ability to effectively manage contexts four times longer than standard models, although it is less competitive in relatively short contexts. While SparseK+SW fails at contexts extending to 26k tokens, it outperforms both NTK and fine-tuned full attention models. 4.4 Downstream Task\n\nWe evaluated our method on the English subsets of LongBench [3] using the OpenCompass package [16], which encompasses a wide range of long-context downstream tasks. The choice of language is based on the fact that the training corpus of TinyLlama is primarily in English. We test all models using greedy decoding, with the evaluation context size set to 8192. All results are presented in Table 2. Full attention offers the best performance but incurs the highest memory cost. Sliding window attention is memory-efficient; however, it results in significant performance degradation. In contrast, our SparseK+SW attention not only demonstrates strong performance but also achieves high memory efficiency. Notably, SparseK+SW outperforms the training-free method, NTK, and the inference-time KV cache compression method, H2O [81]. This suggests the benefits of maintaining consistency between training and inference. However, SparseK+SW underperforms fine-tuned full attention, representing a trade-off between efficiency and performance. 5 Conclusion\n\nWe propose SparseK attention, a new approach to sparse attention that achieves both computational and memory efficiency. Within self-attention, we use an additional scoring network evaluating the importance of each key-value pair and select the top- pairs. We propose the differentiable SparseK operator, a relaxation of TopK, to enable gradient-based optimization. Experiments on language modeling and downstream tasks demonstrate consistent improvements compared to previous efficient attention methods. References\n\nAinslie et al. [2023] Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontan\u2019on, Siddhartha Brahma, Yury Zemlyanskiy, David C. Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit K. Sanghai. Colt5: Faster long-range transformers with conditional computation. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:257622671. Anagnostidis et al. [2023] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. ArXiv, abs/2305.15805, 2023. URL https://api.semanticscholar.org/CorpusID:258888224. Bai et al. [2023] Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding. ArXiv, abs/2308.14508, 2023. URL https://api.semanticscholar.org/CorpusID:261245264. Beltagy et al. [2020] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. ArXiv, abs/2004.05150, 2020. URL https://api.semanticscholar.org/CorpusID:215737171. Bengio et al. [2013] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. ArXiv, abs/1308.3432, 2013. URL https://api.semanticscholar.org/CorpusID:18406556. Biderman et al. [2023] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling.",
    "sparseless-5": "In International Conference on Machine Learning, pages 2397\u20132430. PMLR, 2023. bloc97 [2023] bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/.",
    "sparseless-6": "Bommasani et al. [2021] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Chen et al. [2021] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Chris R\u00e9. Scatterbrain: Unifying sparse and low-rank attention approximation.",
    "sparseless-7": "ArXiv, abs/2110.15343, 2021. URL https://api.semanticscholar.org/CorpusID:248498407. Chen et al. [2023] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models.",
    "sparseless-8": "ArXiv, abs/2309.12307, 2023. URL https://api.semanticscholar.org/CorpusID:262084134. Chevalier et al. [2023] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. ArXiv, abs/2305.14788, 2023. URL https://api.semanticscholar.org/CorpusID:258865249. Child et al. [2019] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.",
    "sparseless-9": "ArXiv, abs/1904.10509, 2019. URL https://api.semanticscholar.org/CorpusID:129945531. Choromanski et al. [2020] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. ArXiv, abs/2009.14794, 2020. URL https://api.semanticscholar.org/CorpusID:222067132. Clark et al. [2019] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does bert look at? an analysis of bert\u2019s attention. In BlackboxNLP@ACL, 2019. URL https://api.semanticscholar.org/CorpusID:184486746. Contributors [2023] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. Dai et al. [2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. ArXiv, abs/1901.02860, 2019. URL https://api.semanticscholar.org/CorpusID:57759363. Dai et al. [2020] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V. Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. ArXiv, abs/2006.03236, 2020. URL https://api.semanticscholar.org/CorpusID:219401850. Dao [2023] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. ArXiv, abs/2307.08691, 2023. URL https://api.semanticscholar.org/CorpusID:259936734. Dao et al. [2022] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u2019e. Flashattention: Fast and memory-efficient exact attention with io-awareness. ArXiv, abs/2205.14135, 2022. URL https://api.semanticscholar.org/CorpusID:249151871. Devlin et al. [2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Ding et al. [2023] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens.",
    "sparseless-10": "ArXiv, abs/2307.02486, 2023. URL https://api.semanticscholar.org/CorpusID:259341682. emozilla [2023] emozilla. Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. [24] Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org. Fu et al. [2024] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hanna Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. ArXiv, abs/2402.10171, 2024. URL https://api.semanticscholar.org/CorpusID:267682361. Gao et al. [2023] Leo Gao, Jonathan Tow, Baber Abbasi, et al. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. Ge et al. [2023] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms.",
    "sparseless-11": "ArXiv, abs/2310.01801, 2023. URL https://api.semanticscholar.org/CorpusID:263609075. Gokaslan and Cohen [2019] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019. Gu and Dao [2023] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. ArXiv, abs/2312.00752, 2023. URL https://api.semanticscholar.org/CorpusID:265551773. Gu et al. [2021] Albert Gu, Karan Goel, and Christopher R\u2019e. Efficiently modeling long sequences with structured state spaces. ArXiv, abs/2111.00396, 2021. URL https://api.semanticscholar.org/CorpusID:240354066. Guo et al. [2023] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder: A long-range pre-trained language model for code completion. In International Conference on Machine Learning, 2023. URL https://api.semanticscholar.org/CorpusID:259262301. Gupta et al. [2021] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention.",
    "sparseless-12": "In SUSTAINLP, 2021. URL https://api.semanticscholar.org/CorpusID:235422257. Han et al. [2023] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Zero-shot extreme length generalization for large language models.",
    "sparseless-13": "arXiv preprint arXiv:2308.16137, 2023. Ho et al. [2019] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. ArXiv, abs/1912.12180, 2019. URL https://api.semanticscholar.org/CorpusID:209323787. Hua et al. [2022] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. In International Conference on Machine Learning, 2022. URL https://api.semanticscholar.org/CorpusID:247011581. Hutchins et al. [2022] DeLesley S. Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. ArXiv, abs/2203.07852, 2022. URL https://api.semanticscholar.org/CorpusID:247451135. Jiang et al. [2023] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u2019elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023. URL https://api.semanticscholar.org/CorpusID:263830494. Katharopoulos et al. [2020] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franccois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "sparseless-14": "In International Conference on Machine Learning, 2020. URL https://api.semanticscholar.org/CorpusID:220250819. Kitaev et al. [2020] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.",
    "sparseless-15": "ArXiv, abs/2001.04451, 2020. URL https://api.semanticscholar.org/CorpusID:209315300. Lefaudeux et al. [2022] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hackable transformer modelling library. https://github.com/facebookresearch/xformers, 2022. Lei et al. [2023] Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Zhao, Yuexin Wu, Bo Li, Yu Zhang, and Ming-Wei Chang. Conditional adapters: Parameter-efficient transfer learning with fast inference.",
    "sparseless-16": "ArXiv, abs/2304.04947, 2023. URL https://api.semanticscholar.org/CorpusID:258060039. Liu et al. [2023] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.",
    "sparseless-17": "ArXiv, abs/2305.17118, 2023. URL https://api.semanticscholar.org/CorpusID:258947558. Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101, 2017. URL https://api.semanticscholar.org/CorpusID:3312944. Ma et al. [2024] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length, 2024. Martin and Cundy [2017] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. ArXiv, abs/1709.04057, 2017. URL https://api.semanticscholar.org/CorpusID:3497822. Martins and Astudillo [2016] Andr\u00e9 F. T. Martins and Ram\u00f3n Fern\u00e1ndez Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification.",
    "sparseless-18": "ArXiv, abs/1602.02068, 2016. URL https://api.semanticscholar.org/CorpusID:16432551. Mohtashami and Jaggi [2023] Amirkeivan Mohtashami and Martin Jaggi. Landmark Attention: Random-Access Infinite Context Length for Transformers. ArXiv, abs/2305.16300, 2023. URL https://api.semanticscholar.org/CorpusID:258887482. Munkhdalai et al. [2024] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention.",
    "sparseless-19": "ArXiv, abs/2404.07143, 2024. URL https://api.semanticscholar.org/CorpusID:269033427. Orvieto et al. [2023] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. ArXiv, abs/2303.06349, 2023. URL https://api.semanticscholar.org/CorpusID:257496654. Pagliardini et al. [2023] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and Franccois Fleuret. Faster causal attention over large sequences through sparse flash attention.",
    "sparseless-20": "ArXiv, abs/2306.01160, 2023. URL https://api.semanticscholar.org/CorpusID:259063695. Parmar et al. [2018] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam M. Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, 2018. URL https://api.semanticscholar.org/CorpusID:3353110. Peng et al. [2023] Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Koco\u0144, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan Sokrates Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui Zhu. Rwkv: Reinventing rnns for the transformer era. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:258832459. Peng et al. [2021] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. Random feature attention. ArXiv, abs/2103.02143, 2021. URL https://api.semanticscholar.org/CorpusID:232105052. Peters et al. [2019] Ben Peters, Vlad Niculae, and Andr\u00e9 F. T. Martins. Sparse sequence-to-sequence models. In Anna Korhonen, David Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1504\u20131519, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1146. URL https://aclanthology.org/P19-1146. Qin et al. [2024] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, and Yiran Zhong. Transnormerllm: A faster and better large language model with improved transnormer, 2024.",
    "sparseless-21": "Qiu et al. [2019] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. ArXiv, abs/1911.02972, 2019. URL https://api.semanticscholar.org/CorpusID:207847640. Radford et al. [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Rae and Razavi [2020] Jack Rae and Ali Razavi. Do transformers need deep long-range memory? In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7524\u20137529, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.672. URL https://aclanthology.org/2020.acl-main.672. Rae et al. [2019] Jack W. Rae, Anna Potapenko, Siddhant M.",
    "sparseless-22": "Jayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. ArXiv, abs/1911.05507, 2019. URL https://api.semanticscholar.org/CorpusID:207930593. Ren et al. [2021] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost.",
    "sparseless-23": "ArXiv, abs/2107.05768, 2021. URL https://api.semanticscholar.org/CorpusID:235829099. Roy et al. [2020] Aurko Roy, Mohammad Taghi Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.",
    "sparseless-24": "Transactions of the Association for Computational Linguistics, 9:53\u201368, 2020. URL https://api.semanticscholar.org/CorpusID:212718077. Smith et al. [2022] Jimmy Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling. ArXiv, abs/2208.04933, 2022. URL https://api.semanticscholar.org/CorpusID:251442769. Soboleva et al. [2023] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama, 6 2023.",
    "sparseless-25": "Su et al. [2021] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.",
    "sparseless-26": "ArXiv, abs/2104.09864, 2021. URL https://api.semanticscholar.org/CorpusID:233307138. Sukhbaatar et al. [2019] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers.",
    "sparseless-27": "ArXiv, abs/1905.07799, 2019. URL https://api.semanticscholar.org/CorpusID:159041867. Sun et al. [2021] Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? ArXiv, abs/2109.09115, 2021. URL https://api.semanticscholar.org/CorpusID:237572264. Sun et al. [2023] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models.",
    "sparseless-28": "ArXiv, abs/2307.08621, 2023. URL https://api.semanticscholar.org/CorpusID:259937453. Tay et al. [2020] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In International Conference on Machine Learning, 2020. URL https://api.semanticscholar.org/CorpusID:211505992. Team [2023] OpenAI Team. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models.",
    "sparseless-29": "ArXiv, abs/2302.13971, 2023. URL https://api.semanticscholar.org/CorpusID:257219404. Tsallis [1988] Constantino Tsallis. Possible generalization of Boltzmann-Gibbs statistics. Journal of Statistical Physics, 52(1-2):479\u2013487, July 1988. ISSN 0022-4715, 1572-9613. doi: 10.1007/BF01016429. URL http://link.springer.com/10.1007/BF01016429. Vaswani et al. [2017] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. URL https://api.semanticscholar.org/CorpusID:13756489. Vyas et al. [2020] Apoorv Vyas, Angelos Katharopoulos, and Franccois Fleuret. Fast transformers with clustered attention. ArXiv, abs/2007.04825, 2020. URL https://api.semanticscholar.org/CorpusID:220424511. Wang et al. [2020a] Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu. Cluster-former: Clustering-based sparse transformer for long-range dependency encoding.",
    "sparseless-30": "ArXiv, abs/2009.06097, 2020a. URL https://api.semanticscholar.org/CorpusID:260424300. Wang et al. [2020b] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. ArXiv, abs/2006.04768, 2020b. URL https://api.semanticscholar.org/CorpusID:219530577. Yang and Zhang [2024] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/flash-linear-attention. Yang et al. [2023] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training.",
    "sparseless-31": "ArXiv, abs/2312.06635, 2023. URL https://api.semanticscholar.org/CorpusID:266162792. Yu et al. [2023] Haofei Yu, Cunxiang Wang, Yue Zhang, and Wei Bi. Trams: Training-free memory selection for long-range language modeling. In Conference on Empirical Methods in Natural Language Processing, 2023. URL https://api.semanticscholar.org/CorpusID:264439578. Zhang et al. [2024a] Michael Zhang, Kush S. Bhatia, Hermann Kumbong, and Christopher R\u2019e. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. ArXiv, abs/2402.04347, 2024a. URL https://api.semanticscholar.org/CorpusID:267523164. Zhang et al. [2024b] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024b.",
    "sparseless-32": "Zhang et al. [2023] Zhenyu (Allen) Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R\u00e9, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. ArXiv, abs/2306.14048, 2023. URL https://api.semanticscholar.org/CorpusID:259263947. Zhong et al. [2024] Zexuan Zhong, Mengzhou Xia, Danqi Chen, and Mike Lewis. Lory: Fully differentiable mixture-of-experts for autoregressive language model pre-training.",
    "sparseless-33": "2024. URL https://api.semanticscholar.org/CorpusID:268891288. Zhu et al. [2015] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015. Appendix A Derivation of SparseK\n\nA.1 The solution of SparseK\n\nProposition A.1.",
    "sparseless-34": "The solution of is in the form of , where . Define and . Then,\n\n\u03c4 \u200b ( \ud835\udc9b ) \ud835\udf0f \ud835\udc9b \\displaystyle\\tau({\\bm{z}}) = \u2211 j \u2208 S \u2217 \u200b ( \ud835\udc9b ) z j + | F \u2217 \u200b ( \ud835\udc9b ) | \u2212 k | S \u2217 \u200b ( \ud835\udc9b ) | absent subscript \ud835\udc57 superscript \ud835\udc46 \ud835\udc9b subscript \ud835\udc67 \ud835\udc57 superscript \ud835\udc39 \ud835\udc9b \ud835\udc58 superscript \ud835\udc46 \ud835\udc9b \\displaystyle=\\frac{\\sum_{j\\in S^{*}({\\bm{z}})}z_{j}+|F^{*}({\\bm{z}})|-k}{|S^{*}({\\bm{z}})|} (15)\n\nProof. Consider the Lagrangian problem of SparseK:\n\n\u2112 \u200b ( \ud835\udc9b , \ud835\udf41 , \ud835\udf42 , \u03c4 ) = 1 2 \u200b \u2016 \ud835\udc91 \u2212 \ud835\udc9b \u2016 2 \u2212 \ud835\udf41 \u22a4 \u200b \ud835\udc91 + \ud835\udf42 \u22a4 \u200b ( \ud835\udc91 \u2212 \ud835\udfcf ) + \u03c4 \u200b ( \ud835\udfcf \u22a4 \u200b \ud835\udc91 \u2212 k ) \u2112 \ud835\udc9b \ud835\udf41 \ud835\udf42 \ud835\udf0f 1 2 superscript norm \ud835\udc91 \ud835\udc9b 2 superscript \ud835\udf41 top \ud835\udc91 superscript \ud835\udf42 top \ud835\udc91 1 \ud835\udf0f superscript 1 top \ud835\udc91 \ud835\udc58 \\mathcal{L}({\\bm{z}},\\bm{\\mu},\\bm{\\nu},\\tau)=\\frac{1}{2}||{\\bm{p}}-{\\bm{z}}||^{2}-\\bm{\\mu}^{\\top}{\\bm{p}}+\\bm{\\nu}^{\\top}({\\bm{p}}-\\bm{1})+\\tau(\\bm{1}^{\\top}{\\bm{p}}-k) (16)\n\nThe optimal () must satisfy the following Karush-Kuhn-Tucker conditions:\n\n\ud835\udc91 \u2217 \u2212 \ud835\udc9b \u2212 \ud835\udf41 \u2217 + \ud835\udf42 \u2217 + \u03c4 \u2217 \u200b \ud835\udfcf = \ud835\udfce , superscript \ud835\udc91 \ud835\udc9b superscript \ud835\udf41 superscript \ud835\udf42 superscript \ud835\udf0f 1 0 \\displaystyle{\\bm{p}}^{*}-{\\bm{z}}-\\bm{\\mu}^{*}+\\bm{\\nu}^{*}+\\tau^{*}\\bm{1}=\\bm{0}, (17) \ud835\udfce \u2264 \ud835\udc91 \u2217 \u2264 \ud835\udfcf , \ud835\udfcf \u22a4 \u200b \ud835\udc91 \u2217 = k , \ud835\udf41 \u2217 \u2265 \ud835\udfce , \ud835\udf42 \u2217 \u2265 \ud835\udfce , formulae-sequence 0 superscript \ud835\udc91 1 formulae-sequence superscript 1 top superscript \ud835\udc91 \ud835\udc58 formulae-sequence superscript \ud835\udf41 0 superscript \ud835\udf42 0 \\displaystyle\\bm{0}\\leq{\\bm{p}}^{*}\\leq\\bm{1},\\quad\\bm{1}^{\\top}{\\bm{p}}^{*}=k,\\quad\\bm{\\mu}^{*}\\geq\\bm{0},\\quad\\bm{\\nu}^{*}\\geq\\bm{0}, (18) \u03bc i \u2217 \u200b p i \u2217 = 0 , v i \u2217 \u200b ( p i \u2217 \u2212 1 ) = 0 , \u2200 i \u2208 [ K ] .",
    "sparseless-35": "formulae-sequence subscript superscript \ud835\udf07 \ud835\udc56 subscript superscript \ud835\udc5d \ud835\udc56 0 formulae-sequence subscript superscript \ud835\udc63 \ud835\udc56 subscript superscript \ud835\udc5d \ud835\udc56 1 0 for-all \ud835\udc56 delimited-[] \ud835\udc3e \\displaystyle\\mu^{*}_{i}p^{*}_{i}=0,\\quad v^{*}_{i}(p^{*}_{i}-1)=0,\\quad\\forall i\\in[K]. (19)\n\nCase I\n\nIf for we have , then from (19) we must have and , which from (17) we must have , i.e., . Case II\n\nIf for we have , then from (19) we must have and , which from (17) we must have , i.e., . Case III\n\nIf for we have , then from (19) we must have and , which from (17) we must have , i.e., . From Case I, II, III and (18) we obtain\n\n\ud835\udc91 \u2217 = max \u2061 ( min \u2061 ( \ud835\udc9b \u2212 \u03c4 \u2217 , 1 ) , 0 ) , superscript \ud835\udc91 \ud835\udc9b superscript \ud835\udf0f 1 0 \\displaystyle{\\bm{p}}^{*}=\\max(\\min({\\bm{z}}-\\tau^{*},1),0), (20) and \u2211 j \u2208 S \u200b ( \ud835\udc9b ) ( z j \u2212 \u03c4 \u2217 ) + | F \u200b ( \ud835\udc9b ) | = k . subscript \ud835\udc57 \ud835\udc46 \ud835\udc9b subscript \ud835\udc67 \ud835\udc57 superscript \ud835\udf0f \ud835\udc39 \ud835\udc9b \ud835\udc58 \\displaystyle\\sum_{j\\in S({\\bm{z}})}(z_{j}-\\tau^{*})+|F({\\bm{z}})|=k. (21)\n\nRearrange (21) then we will get (15). Let be the sorted coordinates of . We can define and , so we have\n\nz ( u \u2217 ) \u2265 \u03c4 \u200b ( \ud835\udc9b ) + 1 > z ( u \u2217 + 1 ) subscript \ud835\udc67 superscript \ud835\udc62 \ud835\udf0f \ud835\udc9b 1 subscript \ud835\udc67 superscript \ud835\udc62 1 \\displaystyle z_{(u^{*})}\\geq\\tau({\\bm{z}})+1>z_{(u^{*}+1)} z ( w \u2217 ) > \u03c4 \u200b ( \ud835\udc9b ) \u2265 z ( w \u2217 + 1 ) subscript \ud835\udc67 superscript \ud835\udc64 \ud835\udf0f \ud835\udc9b subscript \ud835\udc67 superscript \ud835\udc64 1 \\displaystyle z_{(w^{*})}>\\tau({\\bm{z}})\\geq z_{(w^{*}+1)} (22)\n\nConsequently, (15) can be rewritten as\n\n\u03c4 \u200b ( \ud835\udc9b ) = \u2211 u \u2217 < j \u2264 w \u2217 z ( j ) + u \u2217 \u2212 k w \u2217 \u2212 u \u2217 , \ud835\udf0f \ud835\udc9b subscript superscript \ud835\udc62 \ud835\udc57 superscript \ud835\udc64 subscript \ud835\udc67 \ud835\udc57 superscript \ud835\udc62 \ud835\udc58 superscript \ud835\udc64 superscript \ud835\udc62 \\displaystyle\\tau({\\bm{z}})=\\frac{\\sum_{u^{*}<j\\leq w^{*}}z_{(j)}+u^{*}-k}{w^{*}-u^{*}}, (23)\n\nA.2 Get Algorithm 5\n\nThe exact solution of (23) can be evaluated by searching in the descending order that satisfies (22). The descending order is given by the step functions with respect to an incremental threshold ,\n\nu \u200b ( \u03b2 ) = max \u2061 { j | z ( j ) \u2265 \u03b2 + 1 } \ud835\udc62 \ud835\udefd conditional \ud835\udc57 subscript \ud835\udc67 \ud835\udc57 \ud835\udefd 1 \\displaystyle u(\\beta)=\\max\\{j|z_{(j)}\\geq\\beta+1\\} w \u200b ( \u03b2 ) = max \u2061 { j \u200b | z ( j ) > \u200b \u03b2 } . \ud835\udc64 \ud835\udefd \ud835\udc57 ket subscript \ud835\udc67 \ud835\udc57 \ud835\udefd \\displaystyle w(\\beta)=\\max\\{j|z_{(j)}>\\beta\\}. (24)\n\nNote that there are at most distinct pairs, corresponding to the values of that may trigger change of either or , i.e., . A.3 Gradient\n\nThe SparseK operator is differentiable everywhere except at splitting points causing changes to set or . Note that we have . Then, from Equation (12) and Equation (15), we have\n\n\u2202 SparseK i \u200b ( \ud835\udc9b ) \u2202 z j = { \u03b4 i \u200b j \u2212 1 | S \u2217 \u200b ( \ud835\udc9b ) | if \u200b z i \u2208 S \u2217 \u200b ( \ud835\udc9b ) , 0 otherwise, \\displaystyle\\frac{\\partial\\textsc{SparseK}_{i}({\\bm{z}})}{\\partial z_{j}}=\\left\\{\\begin{aligned} &\\delta_{ij}-\\frac{1}{|S^{*}({\\bm{z}})|}&&\\text{if }z_{i}\\in S^{*}({\\bm{z}}),\\\\\n&0&&\\text{otherwise,}\\end{aligned}\\right. (25)\n\nwhere if and 0 otherwise. Then, the Jacobian matrix and Jacobian-vector product (JVP) for a given vector are given by\n\n\ud835\udc71 \u200b ( \ud835\udc9b ) = Diag \u200b ( \ud835\udc94 ) \u2212 \ud835\udc94 \u200b \ud835\udc94 \u22a4 / | S \u2217 \u200b ( \ud835\udc9b ) | \ud835\udc71 \ud835\udc9b Diag \ud835\udc94 \ud835\udc94 superscript \ud835\udc94 top superscript \ud835\udc46 \ud835\udc9b \\displaystyle\\bm{J}({\\bm{z}})=\\text{Diag}({\\bm{s}})-{\\bm{s}}{\\bm{s}}^{\\top}/|S^{*}({\\bm{z}})| (26) \ud835\udc71 \u200b ( \ud835\udc9b ) \u22c5 \ud835\udc97 = \ud835\udc94 \u2299 ( \ud835\udc97 \u2212 v ^ \u200b \ud835\udfcf ) , with \u200b v ^ = \u2211 j \u2208 S \u2217 \u200b ( \ud835\udc9b ) v j | S \u2217 \u200b ( \ud835\udc9b ) | formulae-sequence \u22c5 \ud835\udc71 \ud835\udc9b \ud835\udc97 direct-product \ud835\udc94 \ud835\udc97 ^ \ud835\udc63 1 with ^ \ud835\udc63 subscript \ud835\udc57 superscript \ud835\udc46 \ud835\udc9b subscript \ud835\udc63 \ud835\udc57 superscript \ud835\udc46 \ud835\udc9b \\displaystyle\\bm{J}({\\bm{z}})\\cdot{\\bm{v}}={\\bm{s}}\\odot({\\bm{v}}-\\hat{v}\\bm{1}),\\text{with }\\hat{v}=\\frac{\\sum_{j\\in S^{*}({\\bm{z}})}v_{j}}{|S^{*}({\\bm{z}})|} (27)\n\nwhere is an indicator vector taking if and otherwise, and is the Hadamard product. Note that SparseK has the same form of gradients, Jacobian and jacobian-vector product (JVP) as SparseMax [46] but with a different definition of . Appendix B Other Technical Details\n\nB.1 Combine SparseK with Low-rank Linear Attention\n\nFor simplicity, we consider the attention of one query, . In Chen et al. [10], each attendable position () is attended to via either sparse attention or linear attention. This hard strategy resembles top- attention. In our SparseK attention, we employ soft gating, denoted as . Consequently, the combination of SparseK and linear attention forms an interpolation between them, modulated by :\n\n\ud835\udc90 i = \u2211 j ( \ud835\udc8e s \u200b p \u200b a \u200b r \u200b s \u200b e \u200b k , j \u200b exp \u2061 ( \ud835\udc92 i \u22a4 \u200b \ud835\udc8c j ) + ( 1 \u2212 \ud835\udc8e s \u200b p \u200b a \u200b r \u200b s \u200b e \u200b k , j ) \u200b \u03d5 \u200b ( \ud835\udc92 i ) \u22a4 \u200b \u03d5 \u200b ( \ud835\udc8c j \u2032 ) ) \u200b \ud835\udc97 j \u2211 j \u2032 \ud835\udc8e s \u200b p \u200b a \u200b r \u200b s \u200b e \u200b k , j \u200b exp \u2061 ( \ud835\udc92 i \u22a4 \u200b \ud835\udc8c j \u2032 ) + ( 1 \u2212 \ud835\udc8e s \u200b p \u200b a \u200b r \u200b s \u200b e \u200b k , j ) \u200b \u03d5 \u200b ( \ud835\udc92 i ) \u22a4 \u200b \u03d5 \u200b ( \ud835\udc8c j \u2032 ) subscript \ud835\udc90 \ud835\udc56 subscript \ud835\udc57 subscript \ud835\udc8e \ud835\udc60 \ud835\udc5d \ud835\udc4e \ud835\udc5f \ud835\udc60 \ud835\udc52 \ud835\udc58 \ud835\udc57 superscript subscript \ud835\udc92 \ud835\udc56 top subscript \ud835\udc8c \ud835\udc57 1 subscript \ud835\udc8e \ud835\udc60 \ud835\udc5d \ud835\udc4e \ud835\udc5f \ud835\udc60 \ud835\udc52 \ud835\udc58 \ud835\udc57 italic-\u03d5 superscript subscript \ud835\udc92 \ud835\udc56 top italic-\u03d5 subscript \ud835\udc8c superscript \ud835\udc57 \u2032 subscript \ud835\udc97 \ud835\udc57 subscript superscript \ud835\udc57 \u2032 subscript \ud835\udc8e \ud835\udc60 \ud835\udc5d \ud835\udc4e \ud835\udc5f \ud835\udc60 \ud835\udc52 \ud835\udc58 \ud835\udc57 superscript subscript \ud835\udc92 \ud835\udc56 top subscript \ud835\udc8c superscript \ud835\udc57 \u2032 1 subscript \ud835\udc8e \ud835\udc60 \ud835\udc5d \ud835\udc4e \ud835\udc5f \ud835\udc60 \ud835\udc52 \ud835\udc58 \ud835\udc57 italic-\u03d5 superscript subscript \ud835\udc92 \ud835\udc56 top italic-\u03d5 subscript \ud835\udc8c superscript \ud835\udc57 \u2032 \\displaystyle{\\bm{o}}_{i}=\\sum_{j}\\frac{\\left({\\bm{m}}_{sparsek,j}\\exp\\left({\\bm{q}}_{i}^{\\top}{\\bm{k}}_{j}\\right)+\\left(1-{\\bm{m}}_{sparsek,j}\\right)\\phi({\\bm{q}}_{i})^{\\top}\\phi({\\bm{k}}_{j^{\\prime}})\\right){\\bm{v}}_{j}}{\\sum_{j^{\\prime}}{\\bm{m}}_{sparsek,j}\\exp({\\bm{q}}_{i}^{\\top}{\\bm{k}}_{j^{\\prime}})+(1-{\\bm{m}}_{sparsek,j})\\phi({\\bm{q}}_{i})^{\\top}\\phi({\\bm{k}}_{j^{\\prime}})} (28)\n\nWe can use this notation to express other kinds of combination: (i) Linear+SW means to use a local gating where if and 0 otherwise. (ii) SparseK+Linear+SW means using a concatenation of and as mentioned in Section 3.4. In this work, we adopt the feature map based on the methodology proposed by Katharopoulos et al. [38]. The additional head-wise linear transformation is introduced to mitigate the dilemma of and having to perform two types of attention. This adjustment introduces less than 1% additional parameters. However, using linear attention can introduce significant overhead during training because consumes substantial memory that needs to be stored in the computation graph. In contrast, the combination of SparseK and sliding window (SW) techniques introduces minimal overhead since the same , , and are employed in both kinds of attention. In our early experiments, we also considered the more recent Hedgehog feature map [79]. We observed slightly better performance compared to the aforementioned feature map. However, the Hedgehog feature map requires significantly more computational resources due to doubling the hidden dimension. While it is possible to fuse the feature map and linear attention within a single kernel, we leave this for future work. Recently, Infini-attention [48] was proposed to integrate local attention and linear attention through an input-independent gating mechanism. We contend that our methods exhibit a superior expressive capability because our gating mechanism is data-dependent. The numerical explosion issue\n\nWithout normalization, we have observed that the gate is trained to put all credits away from linear attention part (i.e., set to zero) because standard dot-product attention is generally better than linear attention, and in contrast, the linear attention part is trained to increase significantly to survive. The score normalization breaks the competition and makes the training feasible. B.2 Efficient GPU Kernel of SparseK Attention\n\nIn this section, we discuss optimizations for obtaining practically efficient sparse attention. We discuss a few challenges here and present our solution briefly. We measure time and memory cost on one NVIDIA A6000 GPU when , and to show the effectiveness of each optimization. High memory cost\n\nA naive implementation is to select contexts before doing attention. However, each selection produces , resulting in a memory cost of for all selections. The cost is significant considering a reasonable size of selected contexts, such as . Solution We implement a fused Triton kernel to load needed key-value pairs on demand within FlashAttention [20]. By doing so, the memory cost is reduced from 8192 MB to 4.125 MB. Underuse of hardware\n\nOur method requires matrix-vector multiplications (MVM) of instead of one matrix-matrix multiplication (MMM) of in pure self-attention. This fails to capitalize on the hardware-optimized matrix-matrix multiplication in modern GPUs. Solution We group successive queries to do attention jointly. This is efficient because the majority of selections remain unchanged in successive steps. With grouping, MVM are transformed into MMM and hence hardware can be fully utilized. By doing so, the running time is reduced from 3.09 ms to 195 us, where about 2 ms is due to the faster matrix-matrix multiplications, and the reset is credited to less IO. IO-inefficiency\n\nStoring and using selection scores has an IO complexity of . Besides, As SparseMax, jacobian-vector product (JVP) computation of SparseK requires multiple read/write operations. Solution Our approach involves storing only the unnormalized scores and the threshold . Then, is computed dynamically as needed, akin to the re-computation technique in FlashAttention, which trades slow IO with fast computation. Regarding gradients, we extend the backward kernel of FlashAttention to include IO-aware JVP computation. Furthermore, we introduce additional IO optimizations for grouped queries, achieved by reducing intermediate results in a group instead of using a computing-then-reducing pipeline. By doing so, the running time is reduced from 12.07 ms to 1.46ms and the memory cost of backward pass is reduced from 192.13 MB to 13.53 MB. B.3 Practical Implementation of Algorithm 2\n\nAlgorithm 2 scans the sequence of scores and incorporates heaps, making it unsuitable for GPUs. Therefore, we implement the algorithm on the CPU, where it performs efficiently, even with millions of input elements. However, this raises the concern that frequent data transfers between the CPU and GPU might degrade efficiency. To address this, we leverage the asynchronous execution property of GPU kernels to conceal the data transfer overhead behind other large GPU kernels. Appendix C Additional Experimental Results\n\nC.1 Language Modeling from Scratch\n\nFor all methods, we use the AdamW optimizer [43] with a learning rate of and a weight decay of 0.1.",
    "sparseless-36": "We apply gradient clipping with a threshold of 1.0. Our learning rate follows a cosine schedule, incorporating 100 warm-up steps from an initial learning rate of . The batch size is set to 512K tokens. We utilize only those documents whose lengths are equal to or exceed the training context length, ensuring that no data packing is applied. This approach simplifies data preprocessing because the incremental selection of SparseK on two unrelated sentences is not meaningful. There are more sophisticated strategies available, such as the similarity-based batching [82], which we plan to explore in future work. For evaluation, we compute perplexity on a held-out set using a sliding window approach without overlap. For GLA, we adjusted the settings of its 340M architecture as follows: the hidden dimension was decreased from 1024 to 768, the number of layers was reduced from 24 to 12, and the number of heads was decreased from 4 to 2. For RetNet, the modifications were similar to those of the GLA, except that the number of heads was reduced to 3. For GLA and RetNet, we use the same training hyperparameters (such as learning rate scheduler and weight decay) as in our experiments of Transformers rather than the official configuration [77]. We found that our configuration leads to slightly better results. For Hash attention, we normalize as suggested in Pagliardini et al. [50]. Table 3 lists all used sparsity configurations, and figure 4 plots perplexity achieved by each model relative to the average (or maximum, if applicable) number of attended KV pairs per query. We use the flash-linear-attention package [76] for our linear attention related experiments, including GLA, RetNet, Linear+SW and SparseK+Linear+SW. We use the xformers [40] package for our Fixed and BigBird experiments. We use the open-sourced code of Pagliardini et al. [50] for our Hash and Random experiments. C.2 Fine-tuning Existing Models\n\nFor all methods, we use the AdamW optimizer with a weight decay of 0.1. We apply gradient clipping with a threshold of 1.0. Our learning rate follows a cosine schedule, incorporating 100 warm-up steps from an initial learning rate of of the peak learning rate, which is for Pythia 160M, for Pythia 410M and for TinyLlama 1.1B. The batch size is set to 1M tokens. For Pythia models, we use packing to generate training samples of the target length from a uniformly sampled subset of SlimPajama. For evaluation, we compute the perplexity on a held-out set using a sliding window approach with a step size of 512. For TinyLlama models, we utilize only those documents whose lengths are equal to or exceed the training context length in the up-sampled SlimPajama dataset Fu et al. [25]. For evaluation, we compute perplexity on a held-out set using a sliding window approach without overlap. C.3 More Downstream Task Evaluation on Fine-tuned Models\n\nWe present the results of tasks that do not require long-context modeling in Table 4. These results were generated using the lm-eval-harness package [26]. Although SparseK+SW still outperforms , a comparison with the original model indicates that fine-tuning on long data somewhat diminishes performance on short data [25]. This trade-off has been observed in other studies, prompting further research to address this issue. C.4 Compare with Memory-efficient Attention Method\n\nWe compare our method with Anagnostidis et al. [2] in Table 5, which uses a pairwise criterion to prune the KV cache based on the current query. Following Anagnostidis et al. [2], we use the GPT2 small architecture and train models with Wikipedia 20220301.en [24] and BookCorpus [83] datasets. DCP achieves a slightly better perplexity than ours, matching the stronger expressiveness of their method than our query-independent selection. However, their method costs a much longer training time due to the heavy pairwise criterion, making it hard to adopt in training on long documents. Meanwhile, our method remains efficient in this short context length experiments and achieves similar performance gains. C.5 Ablation Study\n\nPreviously, we choose SparseK+SW as the default model. In this section, we conduct ablation study to investigate the performance of SparseK-only model and effectiveness of learnable selection. We use the Pythia-160M architecture. The context length is 8192. All models are set to have a sparsity ratio similar to SW with a window size of 1024. We randomly initialize all parameters. All models are trained with 5000 steps. Table 6 shows the results. We first study attention types. SparseK-only outperforms sliding window attention. A combination of these two attention leads to better performance. Then, we study how to select. We consider two baselines: random selection and unlearnable selection which is parameterized as in Section 3.5 initialization to mimic attention score. The two baseline obviously underperforms SparseK-only, validating the effectiveness of our learnable selection. C.6 Balance Between Selection Size and Sliding Window Size\n\nIn Table 7, we present a comparison of various ratios of and . Pythia 410M is utilized as the base model, with a training context length of 16,384.",
    "sparseless-37": "The results indicate an optimal performance at approximately . Consequently, we set in all subsequent experiments for consistency and simplicity. C.7 Multi-group Selection\n\nIt is straightforward to use head-specific selection or grouped selection; the only modification required is to employ separate scoring networks for each head or group. In Table 8, we compare the performance of multi-group selection against single-group selection. We use Pythia 410M as the base model. The training context length is 4096. is set to 256. When utilizing two groups with , the worst-case memory budget amounts to . Our findings indicate that even with a 50% increase in the budget, amounting to 768 (where ), the multi-group selection fails to achieve the performance of single-group attention. Consequently, we have opted to use single-group selection in our subsequent experiments. C.8 Speed Benchmark\n\nFigure 5 presents an sole comparison of the wall-clock time between our developed Triton kernel (with and ) and the Triton implementation of FlashAttention-2 [19], across different input lengths. While our kernel initially shows slower performance than FlashAttention-2 on short inputs due to overhead associated with selecting contexts, it outpaces FlashAttention-2 as input lengths surpass 4096 and 8192 for forward only and forward+backward, respectively. This performance gain is attributed to our method\u2019s linear complexity. Additionally, using a straight-through estimator can reduce the IO operations required for reading , thereby providing further improvements. Figure 6 presents the times and steps required to achieve a specific perplexity. We use a pretrained Pythia-160m model with learning rate decay disabled. The results indicate that SparseK+SW typically learns more quickly than the sliding window attention, achieving a better pareto-optimum in performance vs. efficiency. In Table 9, we compare the training time when using different sparse attention. We use the Pythia-160M architecture. The context length is 8192. All models are set to have a sparsity ratio similar to SW with a window size of 1024. All models are trained with 5000 steps. C.9 Visualization\n\nAppendix D Limitations\n\nThis paper presents SparseK Attention, whose goal is to address both computational and memory efficiency challenges in long-range Transformer computing. We list limitations of our work below:\n\n\u2022\n\nLimited model size and context length In this study, we validate the advantages of SparseK in settings involving models up to 1.1 billion parameters and context lengths up to 16,000 tokens. The primary reason for this limitation is our restricted computational resources. Despite these constraints, we consider a broad range of model sizes and context lengths to demonstrate the general applicability of our method. Nevertheless, with the advent of recent parameter-efficient fine-tuning techniques, it may be possible to experiment with our method on more limited devices. \u2022\n\nDecoder-only We restrict our discussion to applying SparseK within Transformer decoders. the incremental evaluation capability of the SparseK operation demonstrates superior complexity compared to previous approaches. Nonetheless, the SparseK operation is also applicable in various other contexts, such as Transformer encoders and routing in mixture-of-expert models, where a top-k operation might be employed. The sparse output produced by the SparseK operation offers a notable advantage in these scenarios because it is close to the top- selection performed somewhere. \u2022\n\nText-only We focus exclusively on text tasks.",
    "sparseless-38": "However, our method is not dependent on the input modality. Future research involving vision or speech could further substantiate the robustness of our method. Appendix E Impact Statement\n\nThis paper presents SparseK Attention, whose goal is to address both computational and memory efficiency challenges in long-range Transformer computing. We believe our innovative attention mechanism can benefit both NLP and machine learning communities in constructing long-range foundation models. Specifically, we highlight the potential impacts of SparseK as follows:\n\n\u2022\n\nEfficient Long-Range Modeling. First and foremost, the SparseK attention mechanism significantly reduces computational requirements compared to traditional self-attention mechanisms. By prioritizing a subset of key-value pairs, SparseK attention effectively reduces the memory footprint without sacrificing model performance.",
    "sparseless-39": "This is particularly advantageous for resource-constrained environments and edge computing devices. Moreover, this innovation enhances training speed and convergence, contributing to more efficient model development and experimentation. \u2022\n\nMore powerful long-range pretrained models. The differentiable SparseK operator facilitates gradient-based optimization, contributing to accelerated training speed. This is particularly advantageous for pretrained language models, where training on massive datasets can be time-consuming. Faster convergence and training speed enable researchers and practitioners to experiment with and refine models more efficiently. The efficiency gains provided by SparseK attention make it more scalable to work with massive long-sequence datasets, enabling researchers to harness the wealth of information available on the internet and other sources for training robust and contextually aware language models. \u2022\n\nGeneral Application to downstream tasks. The proposed efficient self-attention mechanism can benefit a spectrum of NLP and machine learning downstream tasks, such as long-context document analysis and generation, Transformer-based long-form video analysis, etc. Moreover, the SparseK attention mechanism is not limited to specific domains or tasks.",
    "sparseless-40": "Its adaptability makes it applicable across a wide range of natural language processing and machine learning applications, offering a versatile solution to improve efficiency and performance in diverse scenarios. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Fri Jul 5 21:23:54 2024 by LaTeXML"
}