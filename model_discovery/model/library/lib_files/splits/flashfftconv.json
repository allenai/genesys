{
    "flashfftconv-0": "# FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores \n\nDaniel Y. Fu*, ${ }^{* 1}$, Hermann Kumbong*, ${ }^{*, 1}$ Eric Nguyen ${ }^{2}$, Christopher R\u00e9 ${ }^{1}$<br>*Equal contribution. ${ }^{1}$ Department of Computer Science, Stanford University.<br>${ }^{2}$ Department of Biongineering, Stanford University.<br>\\{danfu,kumboh, etnguyen, chrismre\\}@stanford.edu\n\nNovember 13, 2023\n\n\n#### Abstract\n\nConvolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)\u2014which allows long convolutions to run in $O(N \\log N)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms-1) partial convolutions and 2) frequency-sparse convolutions-which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to $7.93 \\times$ over PyTorch and achieves up to $4.4 \\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score-matching models with twice the parameter count. FlashFFTCONV also achieves $96.1 \\%$ accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50\\%. Furthermore, partial convolutions enable longer-sequence models-yielding the first DNA model that can process the longest human genes ( 2.3 M base pairs) - and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality. ## 1 Introduction\n\nA key challenge in machine learning is to efficiently reason over long sequences. Recently, convolutions have emerged as a key primitive for sequence modeling, underpinning state-of-the-art performance in language modeling [42, 76, 94, 110], time-series analysis [36, 46, 103, 115], computer vision [74, 81, 109], DNA modeling [82], and more [27, 55, 61, 71, 77, 80]. Despite these strong quality results-and other benefits ranging from better scaling in sequence length [46] to greater stability [9, 106]-convolutional sequence models still lag behind Transformers in wall-clock time. A major reason is poor hardware support. Unlike classical convolutions used in vision applications, which often have short filters (e.g., $3 \\times 3$ or $7 \\times 7[53,63]$ ), convolutions for sequence modeling often use filters as long as the input sequence [71, 97]. Such long filters necessitate the use of the FFT convolution algorithm, which computes the convolution between an input $u$ and convolution kernel $k$ via a conversion to frequency space:\n\n$$\n(u * k)[i]=\\sum_{j}^{i} u[i] k[j-i] \\cong u * k=\\mathcal{F}^{-1}(\\mathcal{F} u \\odot \\mathcal{F} k)\n$$\n\nwhere $\\mathcal{F}$ is the FFT, which can be computed in $O(N \\log N)$ time in sequence length $N$, and $\\odot$ is elementwise multiplication. Despite its asymptotic efficiency, the FFT convolution algorithm has poor wall-clock time\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2e48bf63902e7cf6031fg-02.jpg?height=389&width=1635&top_left_y=239&top_left_x=234)\n\nFigure 1: Left: GPU memory hierarchy. Middle left: Order- $p$ Monarch decomposition of FFT convolution, with $p=2$. Middle right: Kernel fusion for end-to-end speedup. Right: FlashFFTConv introduces analogues of sparsity for convolutions. on modern accelerators. In contrast, systems advances have pushed Transformers to the limits of modern accelerators-achieving more than $72 \\%$ FLOP utilization end-to-end with FlashAttention-v2 [22, 24]. In this paper, we study how to optimize the FFT convolution algorithm on modern accelerators, to enable longer-context abilities. Just as systems advances such as FlashAttention yielded improvements in modeling quality $[1,70]$ and the development of new attention algorithms [2, 66, 73, 92], we hope that understanding how to optimize the FFT convolution can also inspire algorithmic innovation, thus improving the quality of convolutional sequence models. For short sequences, the FFT convolution is relatively easy to optimize. Kernel filters are often shared across many batches, which allows pre-computing the FFT of the filter $k_{f}=\\mathcal{F} k$ and re-using it in a batch: $(u * k)=\\mathcal{F}^{-1}\\left(\\mathcal{F} u \\odot k_{f}\\right)$. Thus the FFT convolution is pleasantly parallel across batches and filters, and intermediate outputs of the convolution can be cached in SRAM or registers via kernel fusion. However, as sequence length increases, we find that two key bottlenecks emerge. First, FFT convolutions do not effectively use the specialized matrix-matrix multiply units available on modern accelerators-e.g., the H100 can use tensor cores to compute matrix-matrix multiply at 1.0 PetaFLOP/s compared to $67 \\mathrm{TeraFLOP} / \\mathrm{s}$ for general arithmetic. Second, sequences become too large to fit in SRAM, and kernel fusion fails, resulting in expensive I/O costs (Figure 1 middle right). These I/O costs can be exacerbated by padding operations for causality, and conversions from real-valued inputs/outputs to complex-valued FFT intermediates. In response, we propose FlashFFTCONV, a new system that optimizes the FFT convolution for long sequences using a Monarch decomposition of the FFT. An order-p Monarch decomposition rewrites the FFT as a series of $p$ matrix-matrix multiply operations (Figure 1 middle left), which can be efficiently mapped onto hardware [23]. The order $p$ controls the number of matrix multiply operations and introduces a tradeoff: higher values of $p$ incur lower FLOP cost via smaller matrices, but require more I/O to communicate intermediate results. Using a simple GPU cost model, we show how to adjust $p$ based on the sequence length to balance the FLOP cost and I/O cost. This decomposition introduces a second benefit: a reduction in the amount of the sequence that needs to be kept in SRAM, which makes kernel fusion viable at longer sequence length. As a result, FlashFFTConv scales across four orders of magnitude in sequence length, from 256 to 4 million. FlashFFTCONv also exploits a real-valued FFT algorithm to cut the length of the FFT operation in half [102], and selectively skips portions of the matrix-multiply operations when the input is zero-padded. Finally, the matrix view of the FFT convolution presents a natural interface to implement two architectural modifications: partial convolutions, which learn with $k$ that is shorter than the input sequence, and frequencysparse convolutions, which zero out portions of the kernel $k_{f}$ in frequency space. These can be viewed as convolutional analogues to sparse/approximate attention in Transformers [8, 50, 51, 62, 92], and map naturally on to FlashFFTCONV: both algorithms can be implemented simply by skipping portions of the matrix decomposition, thus reducing memory footprint and wall-clock runtime. Evaluation We show that FlashFFTConv speeds up the FFT convolution, yielding higher-quality, more efficient, and longer-sequence models. - Quality FlashFFTCONv improves the quality of convolutional sequence models via better efficiency: for the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity [94],\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2e48bf63902e7cf6031fg-03.jpg?height=285&width=1497&top_left_y=237&top_left_x=325)\n\nFigure 2: Illustration of Monarch FFT decomposition. and allows M2-BERT-base [42] to achieve up to 3.3 higher average GLUE score-a gain in performance equivalent to doubling the parameters of the model. - Efficiency FlashFFTCONv makes convolutions more efficient across four orders of magnitude in sequence length, yielding speedups of up to $7.93 \\times$ and memory savings of up to $5.60 \\times$ over PyTorch. FlashFFTCONV achieves up to $62.3 \\%$ end-to-end FLOP utilization-only $10 \\%$ less than FlashAttention-v2 - and is faster in wall-clock time than FlashAttention-v2 end-to-end at sequence lengths 2 K and longer due to lower FLOP costs. - Longer Sequence Models FlashFFTConv enables longer-sequence models. In high-resolution image classification, FlashFFTConv yields the first model that can solve the challenging Path- 512 task (sequence length 256 K ) from the long range arena benchmark [104]. In DNA modeling, FlashFFTConv uses partial convolutions to extend HyenaDNA [82] to 4 M sequence length-yielding the first model that can embed the longest human genes (up to 2.3 M base pairs) at single nucleotide resolution. Overall, we hope that FlashFFTConv enables further adoption of convolutional sequence models and that the insights from our work helps inform the design of better hardware-efficient architectures. ## 2 Background\n\nWe provide some background on the FFT convolution and the Monarch FFT decomposition, and discuss the performance characteristics of GPUs. ### 2.1 FFT Convolution\n\nRecall the definition of a convolution operation: $(u * k)[i]=\\sum_{j}^{i} u_{j} k_{i-j}$. Computing this formula directly incurs $O\\left(N N_{k}\\right)$ FLOPs in sequence length $N$ and kernel length $N_{k}$. For long convolutions, where $N_{k}=N$, a popular strategy is to use the Fourier transform to convert the signal $u$ and kernel $k$ to the frequency domain, and compute the convolution using pointwise multiplication in frequency domain, using Equation 1. Critically, a Fourier transform $\\mathcal{F}_{N}$ over an input of length $N$ can be computed in $O(N \\log N)$ time using the FFT-bringing the overall cost of the long convolution from $O\\left(N^{2}\\right)$ to $O(N \\log N)$. We note that the FFT convolution technically computes a circular convolution $\\sum_{j}^{N} u_{j} k_{i-j}$, where $i-j<0$ loops back to the end of $k$. For this reason, $u$ and $k$ are often padded with zeros to compute a causal convolution. Monarch FFT Decomposition Figure 2 shows a demonstration of the order-2 Monarch FFT decomposition. For $N=N_{1} N_{2}$, an order-2 Monarch FFT decomposition rewrites $\\mathcal{F}_{N}=\\mathbf{P}\\left(\\mathbf{I}_{N_{2}} \\otimes \\mathcal{F}_{N_{1}}\\right) \\mathbf{D} \\mathbf{P}^{-1}\\left(\\mathbf{I}_{N_{1}} \\otimes \\mathcal{F}_{N_{2}}\\right) \\mathbf{P}$, where $\\otimes$ denotes the Kronecker product, $\\mathcal{F}_{N}$ is the $N \\times N$ discrete Fourier matrix, $\\mathbf{P}$ is a permutation matrix that reshapes the input to $N_{1} \\times N_{2}$, transposes it to $N_{2} \\times N_{1}$, and then reshapes it back to $N$, and $\\mathbf{D} \\in \\mathbb{C}^{N \\times N}$ is a diagonal matrix containing correctional values called Twiddle factors [6]. Higher-order Monarch decompositions recursively apply the order-2 decomposition to $\\mathcal{F}_{N_{1}}$ or $\\mathcal{F}_{N_{2}}$, which reduces FLOP costs but increases the number of permutation operations, increasing I/O cost. ### 2.2 GPU Performance Characteristics\n\nWe provide some background on the GPU memory hierarchy and available compute units, as well as computebound vs. memory-bound operations. We focus on GPU programming in this paper, but the general principles\nextend to most modern hardware accelerators $[35,57,68,114]$. GPU Compute Model and Memory Hierarchy GPUs have a memory hierarchy consisting of global memory (HBM), shared memory (SRAM), and registers, as shown in Figure 1 Left. Lower/larger levels of the memory hierarchy have more space but are much slower, whereas higher/smaller levels of the memory hierarchy have less space but are much faster [83-85]. The memory hierarchy is closely tied to the GPU compute model. A GPU is composed of many independent streaming multiprocessors (SMs), each of which is composed of independent threads. HBM is shared among all SMs, but each SM has an independent SRAM. The SRAM is shared among all the threads in the SM. Each thread has access to its own registers, but cannot access the registers of other threads. Thus, performing global operations between SMs requires moving data to and from HBM, whereas independent work in each SM can remain local to SRAM. GPU Compute Units Modern GPUs (since the V100 [83]) have specialized matrix multiply units called tensor cores, which can compute matrix-matrix multiply operations with much higher TFLOPs than the general-purpose compute units. For example, the H 100 tensor core can compute matrix multiplication between $16 \\times 16$ matrices at 1.0 PFLOPs, whereas the general-purpose compute units can only compute at 67 TFLOPs [85]. Memory-Bound vs. Compute-Bound Operations GPU operations can be memory-bound or computebound. Memory-bound operations are bottlenecked by the amount of I/O between HBM and registers they need to perform, and are limited by the bandwidth of the memory hierarchy. Examples include simple pointwise operations such as addition or multiplication, as well as most traditional FFT implementations. Compute-bound operations are bottlenecked by the amount of FLOPs they need to execute, and are limited by the speed of the compute units. Examples include large matrix multiply operations. Kernel Fusion A popular method for reducing I/O costs is kernel fusion-loading data for multiple operations into SRAM, computing them independently in each SM, and then writing the final results back to HBM. Kernel fusion is common (and can be automated) for pointwise operations [93], but is more challenging for complex operations that require referencing multiple pieces of data. For example, fusing the operations in attention was not common until the development of FlashAttention [24]. ## 3 FlashFFTConv\n\nSection 3.1 provides a broad overview of FlashFFTCONV and shows how to adapt the Monarch FFT decomposition to convolutions, which involves broadcasting the matrix multiply in parallel across the input sequence. We also describe our kernel fusion strategy and how we exploit domain-specific properties of the convolution in ML for further optimization. Section 3.2 presents a cost model characterizing the relative cost of different order- $p$ decompositions of the FFT as sequence length changes, along with a simple heuristic for selecting $p$ given hardware characteristics. Finally, Section 3.3 discusses architectural extensions by presenting analogues to sparsity in convolutional kernels. ### 3.1 FlashFFTConv Algorithm\n\nWe describe the core FlashFFTConv algorithm. Algorithm 1 provides an overview. We first describe how we adapt the Monarch FFT decomposition for convolutions. Then, we discuss how the Monarch decomposition enables kernel fusion for\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2e48bf63902e7cf6031fg-04.jpg?height=389&width=559&top_left_y=1708&top_left_x=1268)\n\nFigure 3: Top: FlashFFTConv adapts the Monarch FFT decomposition to broadcast matrix multiply operations over the sequence instead of over the batch and hidden dimensions. Bottom: This converts HBM permutations simple matrix transpose operations in SRAM. ```\nAlgorithm 1 FlashFFTConv core algorithm, with order-2 Monarch decomposition. We assume \\(N=N_{1}^{2}\\)\nfor simplicity here. Input: Input \\(u \\in \\mathbb{R}^{B \\times H \\times N}\\), convolution kernel \\(k_{f} \\in \\mathbb{C}^{H \\times N}\\), FFT matrices \\(\\mathbf{F} \\in \\mathbb{C}^{N_{1} \\times N_{1}}, \\mathbf{F}^{-\\mathbf{1}} \\in \\mathbb{C}^{N_{1} \\times N_{1}}\\),\n    Twiddle factors \\(t \\in \\mathbb{C}^{N}, t_{i n v} \\in \\mathbb{C}^{N}, B\\) tile size \\(B_{\\text {tile }}, H\\) tile size \\(H_{\\text {tile }}\\). Output: Output \\(y \\in \\mathbb{R}^{B \\times H \\times N}\\). for SMs in parallel across \\(B / B_{\\text {tile }} \\times H / H_{\\text {tile }}\\) do\n        Load \\(\\mathbf{F}, \\mathbf{F}^{-1}, t, t_{i n v}\\) from HBM. for \\(h \\leftarrow 1\\) to \\(H_{\\text {tile }}\\) do\n            Load \\(\\mathbf{K}_{\\mathbf{f}} \\leftarrow k_{f}[h]\\) from HBM, reshaped to \\(N_{1} \\times N_{1}\\). for \\(b \\leftarrow 1\\) to \\(B_{\\text {tile }}\\) do\n            Load \\(\\mathbf{X} \\leftarrow u[b, h]\\) from \\(H B M\\), reshaped to \\(N_{1} \\times N_{1}\\). \\(\\mathbf{X} \\leftarrow\\left(\\left(\\mathbf{F}^{\\top} \\mathbf{X}\\right) * t\\right) \\mathbf{F} \\quad \\triangleright\\) FFT, decomposed into two steps\n            \\(\\mathbf{X} \\leftarrow \\mathbf{X} * \\mathbf{K}_{\\mathbf{f}}{ }^{\\top} \\quad \\triangleright\\) Elementwise multiply with \\(k_{f}\\)\n            \\(\\mathbf{Y} \\leftarrow\\left(\\left(\\mathbf{X F}^{-1}\\right)^{\\top} * t_{i n v}\\right) \\mathbf{F}^{-1} \\triangleright\\) Inverse FFT, decomposed into two steps\n            Write \\(\\mathbf{Y}^{\\top}\\) to HBM. ```\n\nlong sequences. We conclude by presenting domain-specific optimizations. Adapting Monarch for Fusion The Monarch FFT decomposition, as well as classical algorithms such as Bailey's FFT algorithm [6], traditionally broadcasts the matrix operation against the batch dimension and the hidden dimension, as shown in Figure 3 top left. This allows each $\\mathcal{F}_{N_{1}}$ operation in the $\\mathbf{I}_{N_{2}} \\otimes \\mathcal{F}_{N_{1}}$ matrix to run independently. However, it also makes kernel fusion difficult; fusing across the matrix multiply and permutation operations requires loading at least 16 sequences at once into SRAM to fill out the matrix multiply unit-limiting sequence length to around $2 K$ on A100 and H100. Instead, we broadcast the matrix operation across the entire sequence, as shown in Figure 3 top right, and run the algorithm in parallel across the batch and hidden dimensions. This reduces the SRAM requirements for kernel fusion, since we only need to load a single sequence into SRAM at a time - allowing us to fuse the entire kernel for sequences up to 32 K on A 100 and H100. Broadcasting along the sequence has an added benefit: the permutations simply become matrix transposes (Figure 3 bottom), which can be done quickly using well-established routines on-chip [84]. Finally, we also tile the computation across the $B$ and $H$ dimensions to reduce the cost of loading $k_{f}, \\mathcal{F}$, and the twiddle factors from HBM. The core algorithm is shown in Algorithm 1 for a two-way decomposition. Higher-order decompositions and more details are given in Appendix A. Kernel Fusion and Recomputation The Monarch decomposition allows kernel fusion for long sequences. Inner layers of the decomposition do not require the entire sequence, which reduces the SRAM requirements for fusion. Thus, for long sequences, we can fuse the innermost matrix operations and elementwise multiplications, and take an I/O each for the outermost matrix operations. We use also use recomputation in the backward pass to reduce the memory footprint and I/O cost. Instead of storing intermediate results on HBM for the backward pass (e.g., the intermediate result of $\\mathcal{F}_{N} u$ ), we simply recompute them in the backward pass. Domain-Specific Optimizations Finally, we use a few domain-specific optimizations to adapt the convolution specifically for the sequence learning workload. First, since the convolutions used in sequence learning are real-to-real convolutions (with real kernel weights), we can use a classic algorithm called one-stage decimation in time to compute the FFT of a sequence of length $N$ using a complex FFT of length $N / 2$ (see Appendix A) - cutting the FFT cost in half. Second, inputs and outputs are often padded with zeros in the convolution to compute a causal convolution [42, 46, 94]. We special-case this padding, and use it to eliminate half of the outermost matrix multiply operations in the FFT and iFFT. We also fuse in additional operations around the convolution, such as elementwise-gating, to further reduce I/O. ![](https://cdn.mathpix.com/cropped/2024_09_12_2e48bf63902e7cf6031fg-06.jpg?height=540&width=1571&top_left_y=245&top_left_x=277)\n\nFigure 4: Compute costs of different order-p Monarch decompositions as sequence length increases on A100. Tradeoff points correspond to when the matrices in the Monarch decomposition reach the size of tensor cores on A100 and when the sequence becomes too long for SRAM. ### 3.2 Cost Model of order- $p$ Monarch Decomposition\n\nWe present a formal cost model for an order- $p$ Monarch decomposition of the convolution based on sequence length. The cost model accounts for both the cost of compute and I/O, similar to a roofline analysis [54]. Let $B$ and $H$ be the batch size and model hidden dimension, respectively, and assume that we compute the convolution in half precision. Let $N$ be the sequence length, and let $N=\\prod_{i=1}^{p} N_{i}$ be the product of $p$ factors. For simplicity, we will assume that $N$ is a power of 2 . Let $\\mu$ be the size of the matrix-matrix multiply unit on the GPU (e.g., 16 for A100 [84] and H100 [85]). Let $\\tau_{G}$ and $\\tau_{M}$ be the empirically-achievable FLOPs on the GPU for general-purpose arithmetic, and matrix-matrix multiply arithmetic, respectively. For convenience, define $\\gamma\\left(N_{i}\\right)$ as a helper function that returns $\\tau_{G}$ if $N_{i}<\\mu$, and $\\tau_{M}$ if $N_{i} \\geq \\mu$. Finally, let $\\sigma_{H}$ and $\\sigma_{S}$ be empirically-achievable bandwidth for HBM and SRAM, respectively. Sample values for these constants are given in Appendix C. Now, we can present the cost of an FFT convolution with an order-p Monarch decomposition. Let $\\omega(i)$ be a helper function that returns the bandwidth of the memory where the intermediate results of decomposition step $i$ is stored. The overall cost of the convolution using an order- $p$ Monarch decomposition is given by the following:\n\n$$\nC=B H \\sum_{i=1}^{p} \\frac{16 N N_{i}}{\\gamma\\left(N_{i}\\right)}+\\frac{4 N}{\\omega(i)}\n$$\n\nFigure 4 graphs Equation 2 for different order-p decompositions on different sequence lengths for A100, for $p \\in\\{2,3,4\\}$. For cases where $N_{1}=\\cdots=N_{p}$, the total FLOP cost of an order- $p$ decomposition grows with $O\\left(N^{(p+1) / p}\\right)$. However, for shorter sequences, higher-order decompositions are actually more expensive, since they decompose to matrices that are smaller than the matrix-matrix multiply unit (corresponding to the early bumps). Note also the bump in cost for $p=3$ between 32 K and 64 K , which is a result of running out of SRAM but which is mediated by an extra decomposition for $p=4$. ### 3.3 Architectural Extensions: Sparsity in Convolutions\n\nWe present 2 architectural extensions to FlashFFTCONv: partial convolutions and frequency-sparse convolutions. These can be thought of as convolutional analogues to sparse attention and present opportunities for further optimization. Partial Convolutions In partial convolutions, we zero out later portions of the convolution kernel, analogous to local attention. This has two benefits. First, it reduces the memory footprint, since it requires fewer elements to be held in GPU memory at once. Second, it allows for natural extensions of a pretrained convolutional model to longer sequences (i.e., via a sliding window approach). Table 1: Improvement in quality given a fixed compute budget. | Model (Metric) | PyTorch | FlashFFTCONV |\n| ---: | :---: | :---: |\n| M2-BERT-base-110M (GLUE Score $\\uparrow$ ) | 77.6 | $\\mathbf{8 0 . 9}$ |\n| Hyena-s-155M (PPL $\\downarrow$ ) | 13.4 | $\\mathbf{1 1 . 1}$ |\n\nTable 2: Classification accuracy ( $\\uparrow$ ) on Path-X and Path-512 from the long range arena benchmark [104]. FlashFFTCONV allows for higher-resolution classification. $\\boldsymbol{X}$ indicates out of memory. | Task (seq. len.) | PyTorch | FlashFFTConV |\n| ---: | :---: | :---: |\n| Path-X (16K) | $\\mathbf{9 6 . 9}$ | $\\mathbf{9 6 . 9}$ |\n| Path-512 $(256 \\mathrm{~K})$ | $\\boldsymbol{x}$ | $\\mathbf{9 6 . 1}$ |\n\nFrequency-Sparse Convolutions In frequency-sparse convolutions, we zero out portions of the convolution kernel in frequency space, i.e. zeroing out portions of $k_{f}$. This can be thought of as a variant of partial convolutions in frequency space. Here, the specific sparsity pattern can yield computational benefits. Zeroing out the right portions of the kernel can obviate the need to compute portions of the matrix-matrix multiplies in the Monarch decomposition. We present examples of such sparsity patterns in Appendix A. ## 4 Experiments\n\nIn this section, we evaluate FlashFFTCONV in terms of quality and efficiency. First (Section 4.1), we show that FlashFFTCONV allows models to achieve better quality for the same compute budget in language modeling-matching the performance of models with twice the parameters for free. FlashFFTCONV also enables higher quality via higher resolution in image classification-solving the challenging Path-512 task for the first time simply via increased sequence length. Next (Section 4.2), we demonstrate FlashFFTConV's speedup over other implementations of convolutions, evaluate its efficiency gains when used in convolutional models, and compare a convolutional model using FlashFFTConv to Transformers using FlashAttention-v2. Finally (Section 4.3), we evaluate partial and frequency-sparse convolutions. Partial convolutions yield the first DNA model that can embed the longest genes at single nucleotide resolution ( 2.3 M base pairs), and frequency-sparse convolutions yield speedup while maintaining-or improving-quality. ### 4.1 Impact of Efficiency on Quality\n\nWe study how FlashFFTCOnV impacts downstream quality. First, given two implementations with the same compute budget, FlashFFTConv achieves higher quality due to higher training throughput. Second, we show that improved efficiency can lead to higher quality via longer sequence length. Improvement in Quality with Fixed Compute Budget To evaluate the impacts of efficiency on downstream quality, we train two popular convolutional language models, M2-BERT-base [42] and Hyenas [94], from scratch. These models are trained BERT-style (masked language modeling) and GPT-style (next token prediction), respectively. We compare the quality of models trained with the same compute budget but different implementations of the convolution - either FlashFFTCONv or a PyTorch implementation of the FFT convolution. FlashFFTConv achieves higher pretraining throughput, which allows the models to see more data during pretraining. These efficiency gains improve average GLUE score by up to 3.4 points for M2-BERT-base and perplexity by 2.3 points for Hyena-s.",
    "flashfftconv-1": "For context, these improvements in quality are similar in magnitude to the effect of doubling the number of parameters in the model (see Appendix B for reference results). Longer Sequence Models Next, we show how increased efficiency can lead to higher quality via longer sequence lengths. We evaluate long convolution models on Path-X and Path-512, high-resolution imaging\n\nTable 3: Time $(\\downarrow)$ to compute the forward pass of a convolution with FlashFFTCONV in milliseconds on one H100-SXM, as well as ablations removing specific optimizations.",
    "flashfftconv-2": "We also show memory savings. All results scaled to batch size 64, hidden dimension 768. $p$ indicates the order of the Monarch decomposition. | Sequence Length | $p=2$ |  | $p=3$ |  |  |  | $p=4$ |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 256 | 1 K | 4 K | 8 K | 16 K | 32 K | 1M | 2 M | 4 M |\n| PyTorch | 0.43 | 1.57 | 6.65 | 13.7 | 28.6 | 62.1 | 2,346.3 | $4,892.1$ | $10,127.6$ |\n| FlashFFTConv | 0.09 | 0.24 | 1.37 | 3.19 | 9.27 | 21.8 | $1,492.8$ | 2,695.1 | 7,587.0 |\n| Fusion-Only/cuFFTdx | 0.21 | 0.67 | 3.51 | 7.71 | 21.4 | 45.5 | - | - | - |\n| Speedup over PyTorch | $4.78 \\times$ | $6.54 \\times$ | $4.85 \\times$ | $4.29 \\times$ | $3.09 \\times$ | $2.85 \\times$ | $1.57 \\times$ | $1.82 \\times$ | $1.33 \\times$ |\n| Memory Savings | $8.21 \\times$ | $7.73 \\times$ | $7.61 \\times$ | $7.59 \\times$ | $7.21 \\times$ | $6.57 \\times$ | $2.64 \\times$ | $2.63 \\times$ | $2.63 \\times$ |\n\ntasks from the long range arena (LRA) benchmark [104]. ${ }^{1}$ These tasks take an image ( $128 \\times 128$ for Path-X and $512 \\times 512$ for Path-512), flatten it out, and require a sequence model to classify whether two dots in the image are connected by a path. Existing PyTorch implementations of convolutional sequence models (or even prior optimized implementations [43]) fail to achieve better-than-random ( $50 \\%$ ) accuracy on Path- 512 due to out of memory errors and a lack of support for such long sequences. However, Table 2 shows that FlashFFTConv allows a convolutional sequence model to solve Path- 512 for the first time simply by increasing the available sequence length and reducing the memory footprint of the model through fusion. ### 4.2 Efficiency\n\nWe evaluate FlashFFTConv on how fast it computes convolutions compared to a PyTorch baseline, and how much speedup it yields for convolutional sequence models end-to-end. We also evaluate memory savings compared to PyTorch and compare end-to-end efficiency against highly-optimized Transformers using FlashAttention-v2 [22]. FlashFFTConv Speeds up Convolutions We benchmark the speed of the convolution compared against an FFT convolution implemented in PyTorch. We also benchmark ablations evaluating kernel fusion without using tensor cores-which recovers the strong baseline of using Nvidia's cuFFTdx kernel fusion library [87]-and FlashFFTConv without its domain-specific optimizations. Table 3 shows that FlashFFTConv outperforms PyTorch FFT convolution across all sequence lengths, by up to $6.54 \\times$. Speedups are greatest for short sequences, where the PyTorch FFT convolution is dominated by I/O costs. Speedup is more modest for longer sequences, which incur additional I/O costs (between registers and SRAM for the $p=3$ and between SRAM and HBM for $p=4$ ). Without using the Monarch decomposition for tensor cores (fusion-only), FlashFFTCONV becomes bottlenecked by the speed of general arithmetic operations on GPUs, and does not support sequences longer than 32 K due to a lack of SRAM space. Further benchmarks are given in Appendix B. Domain-Specific Optimizations Provide Further Speedup We also benchmark domain-specific optimizations in FlashFFTConv. Table 4 shows the performance of a gated convolution $y=v \\odot((u \\odot w) * k)$, where $v$ and $w$ are linear projections of the input $u$. This pattern is common in convolutional and SSM-based architectures for language modeling [43, 44, 78, 94]. A PyTorch implementation of a gated convolution incurs additional I/O overhead from the gating operations, whereas FlashFFTConv fuses the gating operations into the convolution. This fusion results in further speedup over PyTorch, up to $7.93 \\times$. Benchmarks of further domain-specific optimizations such as implicit padding (i.e., padding the input to ensure causality, without running an extra padding operation) are given in Appendix B. [^0]Table 4: Time ( $\\downarrow$ ) to compute the forward pass of a gated convolution with FlashFFTConv in milliseconds on one H100-SXM.",
    "flashfftconv-3": "We also show memory savings. All results scaled to batch size 64 , hidden dimension 768 . $p$ indicates the order of the Monarch decomposition. | Sequence Length | $p=2$ |  | $p=3$ |  |  |  | $p=4$ |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 256 | 1 K | 4K | 8 K | 16 K | 32 K | 1M | 2M | 4 M |\n| PyTorch | 0.62 | 2.30 | 9.49 | 19.4 | 29.9 | 84.8 | 3,071.4 | $6,342.6$ | 13,031.2 |\n| FlashFFTConv | 0.11 | 0.29 | 1.43 | 3.58 | 12.2 | 26.3 | $1,768.9$ | 4,623.5 | $10,049.4$ |\n| Speedup | $5.64 \\times$ | $7.93 \\times$ | $6.64 \\times$ | $5.42 \\times$ | $2.45 \\times$ | $3.22 \\times$ | $1.74 \\times$ | $1.37 \\times$ | $1.30 \\times$ |\n| Memory Savings | $6.65 \\times$ | $6.40 \\times$ | $6.35 \\times$ | $6.34 \\times$ | $6.17 \\times$ | 5.87\u00d7 | $2.82 \\times$ | 2.81\u00d7 | $2.81 \\times$ |\n\nTable 5: End-to-end throughput $(\\uparrow)$ of convolutional sequence models against PyTorch. | Model (size, seqlen, unit) | PyTorch | FlashFFTConv | Speedup |\n| ---: | :---: | :---: | :---: |\n| M2-BERT-base (110M, 128, seqs/s) | 4,480 | $\\mathbf{8 , 5 8 0}$ | $1.9 \\times$ |\n| Hyena-s-4K (155M, 4K, seqs/s) | 84.1 | $\\mathbf{1 4 7}$ | $1.7 \\times$ |\n| Long convs, Path-X (102M, 16K, images/s) | 126 | $\\mathbf{3 0 8}$ | $2.4 \\times$ |\n| SaShiMi (5.4M, 64K, audio clips/s) | 38.7 | $\\mathbf{5 0 . 3}$ | $1.3 \\times$ |\n| HyenaDNA (1M, seqs/s) | 0.69 | $\\mathbf{3 . 0 3}$ | $4.4 \\times$ |\n\nFlashFFTConv Provides Memory Savings Tables 3 and 4 also show the memory savings from FlashFFTConv compared to PyTorch. FlashFFTConv reduces the memory footprint of convolutions and gated convolutions by using recomputation in the backward pass and kernel fusion. The absolute memory savings for gated convolutions is greater, since FlashFFTConv does not need to store intermediate activations from the gating operations (see Appendix B), but the relative memory savings is smaller since gated convolutions take more memory. FlashFFTConv Speeds Up Convolutional Sequence Models We benchmark end-to-end throughput of convolutional sequence models across various modalities and sequence lengths spanning four orders of magnitude. We benchmark M2-BERT-base [42], a BERT-style language model that has sequence length 128; Hyena-s-4K [94], a GPT-style language model with sequence length 4K; a long-convolutional model [44] trained on Path-X with sequence length 16K [104]; SaShiMi [45], an audio generation model trained on 1-second audio clips sampled at 64 KHz ; and HyenaDNA-1M [82], a DNA modeling model trained on 1 M sequence length. Details of the architectures and architecture-specific optimizations (such as fusing multiplicative gating for M2 and Hyena models) are given in Appendix C. Table 5 shows that FlashFFTConv speeds up these models end-to-end. Speedup varies vary by the size of the models and the relative amount of time spent computing the convolution compared to other parts of the models. For example, FlashFFTConv only speeds up the SaShiMi model by $1.3 \\times$, since the model interleaves convolutions with SSM-based filter generation, pooling layers, and MLPs, which reduces the relative amount of time spent computing the convolution itself. Speedup is greatest for HyenaDNA, where PyTorch is bottlenecked by small batch size. The PyTorch implementation only allows batch size 1 on an 80GB GPU, whereas FlashFFTCONV allows batch size 4 -yielding significant speedup. FlashFFTConv is Faster than FlashAttention-v2 We compare end-to-end efficiency of a 2.7Bparameter Hyena model using FlashFFTConv against a 2.7B-parameter GPT model using FlashAttentionv2 [22] at three sequence lengths. Table 6 shows throughput, end-to-end FLOP utilization, and speedup. FlashFFTCONV achieves lower end-to-end FLOP utilization than FlashAttention-v2 but achieves higher throughput, since convolutions incur fewer overall FLOPs. Table 6: End-to-end throughput ( $\\uparrow$ ) in thousands of tokens per second, FLOP utilization, and speedup of Hyena against GPT running FlashAttention-v2 [22] across sequence lengths for A100. | Model | $\\mathbf{2 K}$ | $\\mathbf{8 K}$ | $\\mathbf{1 6 K}$ |\n| ---: | :---: | :---: | :---: |\n| GPT-2.7B, FA-v2 [22] | 33.8 | 27.8 | 21.6 |\n| Hyena-2.7B, FLASHFFTCoNV | $\\mathbf{3 5 .",
    "flashfftconv-4": "2}$ | $\\mathbf{3 5 . 2}$ | $\\mathbf{3 2 . 3}$ |\n| FA-v2 FLOP Utilization | 65.7 | 72.1 | 78.5 |\n| FlashFFTConV FLOP Utilization | 62.3 | 61.9 | 56.5 |\n| FlashFFTCONV Speedup | $1.1 \\times$ | $1.3 \\times$ | $1.5 \\times$ |\n\nTable 7: Quality and memory footprint of partial convolutions during training across sequence lengths. | Hyena-s-8K | $\\mathbf{8 K}$ | $\\mathbf{4 K}$ | $\\mathbf{2 K}$ | $\\mathbf{1 K}$ | $\\mathbf{5 1 2}$ | $\\mathbf{2 5 6}$ |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| PPL $(\\downarrow)$ | 13.8 | 13.8 | 13.8 | 13.9 | 14.0 | 14.2 |\n| Memory Footprint $(\\downarrow)$ | 32.5 G | 15.3 G | 11.8 G | 8.4 G | 6.1 G | 5.8 G |\n\n### 4.3 Partial and Frequency-Sparse Convolutions\n\nWe evaluate the impact of partial convolutions on downstream quality and memory footprint and on how well they can extend the sequence length of existing models. We evaluate the impact of frequency-sparse convolutions on downstream quality, and we show that frequency-sparse convolutions can yield up to $1.4 \\times$ additional speedup in the convolution without impacting quality. Partial Convolutions Reduce Memory Footprint and Increase Sequence Length Partial convolutions reduce the memory footprint of models, in both language modeling and DNA modeling. A large proportion of the convolution filters can be pruned without impacting downstream quality. Table 7 shows that a Hyena-s-8K model can be pretrained with a much shorter convolution kernel-as short as 2 K \u2014without negatively impacting quality. Partial convolutions yield another benefit: we can naturally extend the sequence length of existing pretrained models. We extend a pretrained HyenaDNA-1M model to 4 M sequence length with promising PPL results (Table 8) -yielding the first model that can embed the longest human genes at single-nucleotide resolution (2.3M base pairs) (See Appendix B for a visualization of gene embeddings). Frequency-Sparse Convolutions Increase Throughput Frequency-sparse convolutions can increase the speed of convolutions - and may also have positive effects on quality. Table 9 shows that we can set up to $79 \\%$ of the entries of the kernel $k_{f}$ to zero without losing quality. Sparsification in frequency space may even improve the quality of pretrained models slightly; the PPL of a pretrained HyenaDNA-1M model improves by 0.01 points after its kernels are $75 \\%$ sparsified in frequency space-potentially as a result of removing high-frequency noise. Sparsification also yields up to $1.4 \\times$ speedup in the convolution via skipping entire blocks of the matrix-matrix multiplies in the Monarch decomposition. Appendix C provides more details about the sparsity patterns used in Table 9 . ## 5 Related Work\n\nLong Convolutions in Sequence Modeling Long convolutional models have emerged as a promising alternative to Transformers for sequence modeling [42-44, 46-48, 52, 76, 82, 94, 96, 97, 101]. These methods differ in how they generate the convolutional kernels; for example, the S 4 line of work uses learned state space models [46, 49, 76, 78], while other works [94, 96, 97] parameterize the convolution using an MLP from positional encodings. However, all the models operate by taking a convolution over the input sequence with a kernel as long as the input: $y=u * k$, where $u \\in \\mathbb{R}^{B \\times H \\times N}, k \\in \\mathbb{R}^{H \\times N}$, and the kernel $k$ is broadcast along the $B$ dimension. When used for language modeling, these models often incorporate elementwise\n\nTable 8: PPL $(\\downarrow)$ from using partial convolutions to extend the sequence length of HyenaDNA to longer sequences. At 4 M sequence length, the models are able to embed the longest human genes. | Base Filter Length | $\\mathbf{1 M}$ | $\\mathbf{2 M}$ | $\\mathbf{4 M}$ |\n| ---: | :---: | :---: | :---: |\n| HyenaDNA-450K | 2.91 | 2.91 | 2.91 |\n| HyenaDNA-1M | 2.91 | 2.91 | 2.90 |\n\nTable 9: Applying frequency-sparsity to the filters of a pretrained HyenaDNA-1M model. | Sparsity Fraction | $\\mathbf{0 \\%}$ | $\\mathbf{5 0 \\%}$ | $\\mathbf{7 5 \\%}$ | $\\mathbf{7 9 \\%}$ | $\\mathbf{8 4 \\%}$ | $\\mathbf{9 1 \\%}$ |\n| ---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| PPL $(\\downarrow)$ | 2.91 | 2.91 | 2.90 | 2.91 | 2.93 | 2.98 |\n| Convolution Speedup $(\\uparrow)$ | $1.0 \\times$ | $1.2 \\times$ | $1.3 \\times$ | $1.4 \\times$ | $1.5 \\times$ | $1.8 \\times$ |\n\nmultiplicative gating as well: $y=f(u) \\odot((g(u) \\odot h(u)) * k$, where $f, g$, and $h$ are linear maps along the $H$ dimension $[42,43,78,94,110]$. Long-Context Applications Long convolutional models have especially been helpful for long-context applications, such as DNA modeling and speech synthesis. In DNA modeling, most longer-context genomic models have relied on either tokenization [56, 107, 113] or downsampling [3, 38]. However, recent work has suggested that modeling DNA directly from base pairs can yield downstream improvements in quality, which requires long sequence lengths [82]. Like DNA modeling, speech synthesis has also benefited from long-context modeling. While traditional speech synthesis pipelines use intermediate representations such as spectrograms [64, 95, 99], linguistic features $[10,59,89]$, or discrete audio codes $[30,31,67,108]$, recent work has shown that modeling the speech directly from the raw waveform can yield downstream improvements in quality [45]. Again, such models require long sequences to model audio at the rate at which it is naturally sampled, necessitating long-sequence modeling. FFT Algorithms There is a long history of efficient FFT algorithms, ranging from the Cooley-Tukey FFT algorithm published in 1965 [19] to parallel FFT algorithms [4] and more [5, 6, 18]. These algorithms have enabled fundamental progress in a range of disciplines, from control theory [7, 12] to signal processing [90, 91]. As FFTs prove more useful for modern deep learning applications, such as long convolutions, new techniques are required to run them efficiently on modern accelerators. Our work continues a line of work exploring how to use tensor cores for the FFT convolution [43, 44, 69], and extends the algorithmic capabilities to much longer sequences. Sparsity in Deep Learning As deep learning models have grown larger and deeper [11, 13, 17], there is increasing interest in reducing the cost of training and running models. Sparsity in particular has received a great deal of attention, and has a long history in machine learning, including work in pruning neural networks $[32,50,51,72,98]$ and finding lottery tickets [39-41]. Our work in partial convolutions and frequencysparse convolutions relates to this line of work, as an analogue of sparsity in convolutional filters. The Monarch decomposition is also closely related to structured matrices. Structured matrices have subquadratic $\\left(o\\left(n^{2}\\right)\\right.$ for dimension $\\left.n \\times n\\right)$ parameters and runtime, such as sparse and low-rank matrices, and fast transforms (Fourier, Chebyshev, sine/cosine, orthogonal polynomials) [23]. Structured matrices can often be computed with simple divide-and-conquer schemes, and can be used to represent many fast transforms [28, 34, 58, 100]. Optimization of deep learning primitives There is a rich history of optimizing deep learning primitives. Many techniques, such as kernel fusion, aim to reduce data movement. Recently, libraries such as PyTorch 2.0 [93] have added kernel fusion automatically. Other techniques include checkpointing, wherein one stores fewer intermediate results and recomputes the others on-the-fly where they are needed, trading additional compute for memory [65, 111]. Many algorithms also have hand-optimizations that can remove unnecessary computation or memory accesses [79]. Another line of optimization techniques aims to reduce FLOPs. MLPs and attention are particularly popular targets of FLOP reduction, via sparse factorizations of weights [14, 19, 23, 25, 26, 29, 39, 116], or sparse/low-rank approximations of attention $[8,16,21,33,37,60,62,75,112,116]$ and their combinations [15, 105]. ## 6 Conclusion\n\nWe present FlashFFTConv, a new system for optimizing FFT convolutions for long sequences. We show that FlashFFTCONV improves quality under a fixed compute budget, enables longer-sequence models, and improves the efficiency of long convolutions. We also show that analogues of sparsity in convolution filters map naturally on to FlashFFTConv's compute model, and can reduce memory footprint and runtime. We hope that our work will help support further adoption of convolutional sequence models, and that our insights can help inform the design of future architectures. ## Acknowledgments\n\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Microsoft Azure, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.",
    "flashfftconv-5": "The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S.",
    "flashfftconv-6": "Government. ## References\n\n[1] Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan, Qinghui Xia, William Gerecke, Timothy J O'Donnell, Daniel Berenberg, Ian Fisk, Niccol\u00f2 Zanichelli, Bo Zhang, et al. Openfold: Retraining alphafold2 yields new insights into its learning mechanisms and capacity for generalization.",
    "flashfftconv-7": "bioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.",
    "flashfftconv-8": "arXiv preprint arXiv:2004.05150, 2020. [9] Alberto Bietti and Julien Mairal. Invariance and stability of deep convolutional representations. Advances in neural information processing systems, 30, 2017. [10] Miko\u0142aj Bi\u0144kowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande, Luis C Cobo, and Karen Simonyan. High fidelity speech synthesis with adversarial networks.",
    "flashfftconv-9": "In International Conference on Learning Representations, 2019. [11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models.",
    "flashfftconv-10": "arXiv preprint arXiv:2108.07258, 2021. [12] E Oran Brigham. The fast Fourier transform and its applications. Prentice-Hall, Inc., 1988. [13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [14] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R\u00e9. Pixelated butterfly: Simple and efficient sparse training for neural network models.",
    "flashfftconv-11": "2021. [15] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00e9. Scatterbrain: Unifying sparse and low-rank attention. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [16] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.",
    "flashfftconv-12": "arXiv preprint arXiv:2009.14794, 2020. [17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.",
    "flashfftconv-13": "arXiv preprint arXiv:2204.02311, 2022. [18] Eleanor Chu and Alan George. Inside the FFT black box: serial and parallel fast Fourier transform algorithms. CRC press, 1999. [19] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of computation, 19(90):297-301, 1965. [20] Fiona Cunningham, James E Allen, Jamie Allen, Jorge Alvarez-Jarreta, M Ridwan Amode, Irina M Armean, Olanrewaju Austine-Orimoloye, Andrey G Azov, If Barnes, Ruth Bennett, et al. Ensembl 2022. Nucleic acids research, 50(D1):D988-D995, 2022. [21] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, $33: 4271-4282,2020$\n[22] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [23] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. Monarch: Expressive structured matrices for efficient and accurate training.",
    "flashfftconv-14": "In International Conference on Machine Learning. PMLR, 2022. [24] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness.",
    "flashfftconv-15": "In Advances in Neural Information Processing Systems, 2022. [25] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations.",
    "flashfftconv-16": "arXiv preprint arXiv:1903.05895, 2020. [26] Tri Dao, Nimit S. Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher R\u00e9. Kaleidoscope: An efficient, learnable representation for all structured linear maps.",
    "flashfftconv-17": "arXiv preprint arXiv:2012.14966, 2021. [27] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision s4: Efficient sequencebased rl via state spaces layers. In The Eleventh International Conference on Learning Representations, 2022 . [28] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00e9, and Atri Rudra. A two-pronged progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1060-1079. SIAM, 2018. [29] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance.",
    "flashfftconv-18": "arXiv preprint arXiv:1907.04840, 2019. [30] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020. [31] Sander Dieleman, Aaron van den Oord, and Karen Simonyan. The challenge of realistic music generation: modelling raw audio at scale.",
    "flashfftconv-19": "Advances in neural information processing systems, 31, 2018. [32] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon.",
    "flashfftconv-20": "Advances in Neural Information Processing Systems, 30, 2017. [33] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts.",
    "flashfftconv-21": "In International Conference on Machine Learning, pages 5547-5569. PMLR, 2022 . [34] Yuli Eidelman and Israel Gohberg. On a new class of structured matrices. Integral Equations and Operator Theory, 34(3):293-324, 1999. [35] Murali Emani, Venkatram Vishwanath, Corey Adams, Michael E Papka, Rick Stevens, Laura Florescu, Sumti Jairath, William Liu, Tejas Nama, and Arvind Sujeeth. Accelerating scientific applications with sambanova reconfigurable dataflow architecture. Computing in Science 8 Engineering, 23(2):114-119, 2021 . [36] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. Multi-head state space model for speech recognition. arXiv preprint arXiv:2305.12498, 2023. [37] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232-5270, 2022 . [38] Quentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter transformers. ACM Computing Surveys, 2021. [39] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [40] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis.",
    "flashfftconv-22": "In International Conference on Machine Learning, pages 3259-3269. PMLR, 2020. [41] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Stabilizing the lottery ticket hypothesis.",
    "flashfftconv-23": "arXiv preprint arXiv:1903.01611, 2019. [42] Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin F. Spector, Michael Poli, Atri Rudra, and Christopher R\u00e9. Monarch Mixer: A simple sub-quadratic GEMM-based architecture.",
    "flashfftconv-24": "In Advances in Neural Information Processing Systems, 2023. [43] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations, 2023. [44] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning, 2023. [45] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. [46] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. [47] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [48] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. How to train your hippo: State space models with generalized orthogonal basis projections.",
    "flashfftconv-25": "arXiv preprint arXiv:2206.12037, 2022 . [49] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces.",
    "flashfftconv-26": "In Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.",
    "flashfftconv-27": "Elsevier, 2011 . [55] Md Mohaiminul Islam, Mahmudul Hasan, Kishan Shamsundar Athrey, Tony Braskich, and Gedas Bertasius. Efficient movie scene detection using state-space transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18749-18758, 2023. [56] Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome. Bioinformatics, $37(15): 2112-2120,2021$\n[57] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pages 1-14, 2023. [58] Thomas Kailath, Sun-Yuan Kung, and Martin Morf. Displacement ranks of matrices and linear equations. Journal of Mathematical Analysis and Applications, 68(2):395-407, 1979. [59] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In International Conference on Machine Learning, pages 2410-2419. PMLR, 2018. [60] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "flashfftconv-28": "In International Conference on Machine Learning, pages 5156-5165. PMLR, 2020. [61] Sanghyeon Kim and Eunbyung Park. Smpconv: Self-moving point representations for continuous convolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10289-10299, 2023. [62] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. [63] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [64] Kundan Kumar, Rithesh Kumar, Thibault De Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre De Brebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial networks for conditional waveform synthesis.",
    "flashfftconv-29": "Advances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.",
    "flashfftconv-30": "arXiv preprint arXiv:2104.11471, 2021. [70] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you!",
    "flashfftconv-31": "arXiv preprint arXiv:2305.06161, 2023. [71] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling?",
    "flashfftconv-32": "arXiv preprint arXiv:2210.09298, 2022. [72] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. Advances in neural information processing systems, 30, 2017. [73] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2e48bf63902e7cf6031fg-17.jpg?height=47&width=378&top_left_y=291&top_left_x=326)\n[74] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976-11986, 2022. [75] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441$2453,2021$. [76] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention.",
    "flashfftconv-33": "arXiv preprint arXiv:2209.10655, 2022 . [77] Temesgen Mehari and Nils Strodthoff. Towards quantitative precision for ecg analysis: Leveraging state space models, self-supervision and patient metadata. IEEE Journal of Biomedical and Health Informatics, 2023. [78] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. [79] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax.",
    "flashfftconv-34": "arXiv preprint arXiv:1805.02867, 2018. [80] Koichi Miyazaki, Masato Murata, and Tomoki Koriyama. Structured state space decoder for speech recognition and synthesis. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023. [81] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4nd: Modeling images and videos as multidimensional signals with state spaces.",
    "flashfftconv-35": "In Advances in neural information processing systems, 2022. [82] Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution.",
    "flashfftconv-36": "In Advances in Neural Information Processing Systems, 2023. [83] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017. [84] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020. [85] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022. [86] NVIDIA. Cuda c++ programming guide, 2023. https://docs.nvidia.com/cuda/cuda-c-programmingguide/index.html. [87] NVIDIA. cufftdx v1.1.0 documentation, 2023. https://docs.nvidia.com/cuda/cufftdx/index.html. [88] NVIDIA. Cutlass 3.2, 2023. https://github.com/NVIDIA/cutlass. [89] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. [90] Alan V Oppenheim. Applications of digital signal processing. Englewood Cliffs, 1978. [91] Alan V Oppenheim, John R Buck, and Ronald W Schafer. Discrete-time signal processing. Vol. 2. Upper Saddle River, NJ: Prentice Hall, 2001. [92] Daniele Paliotta, Matteo Pagliardini, Martin Jaggi, and Fran\u00e7ois Fleuret. Fast causal attention with dynamic sparsity. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [93] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019. [94] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. Proceedings of the 40th International Conference on Machine Learning (ICML 2023), 2023. [95] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3617-3621. IEEE, 2019. [96] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, and Jan C van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. arXiv preprint arXiv:2110.08059, 2021. [97] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data.",
    "flashfftconv-37": "In International Conference on Learning Representations, 2021. [98] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.",
    "flashfftconv-38": "Advances in Neural Information Processing Systems, 33:20378-20389, 2020. [99] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions.",
    "flashfftconv-39": "In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 4779-4783. IEEE, 2018. [100] Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep learning. Advances in Neural Information Processing Systems, 28, 2015. [101] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [102] H V Sorensen, D Jones, Michael Heideman, and C Burrus. Real-valued fast fourier transform algorithms. IEEE Transactions on acoustics, speech, and signal processing, 35(6):849-863, 1987. [103] Siyi Tang, Jared A Dunnmon, Liangqiong Qu, Khaled K Saab, Christopher Lee-Messer, and Daniel L Rubin. Spatiotemporal modeling of multivariate signals with graph neural networks and structured state space models. arXiv preprint arXiv:2211.11176, 2022. [104] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020. [105] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [106] Yi Tay, Mostafa Dehghani, Jai Prakash Gupta, Vamsi Aribandi, Dara Bahri, Zhen Qin, and Donald Metzler. Are pretrained convolutions better than pretrained transformers? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4349-4359, 2021. [107] Yi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via gradient-based subword tokenization.",
    "flashfftconv-40": "arXiv preprint arXiv:2106.12672, 2021. [108] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [109] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective structured state-spaces for long-form video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6387-6397, 2023. [110] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention.",
    "flashfftconv-41": "arXiv preprint arXiv:2212.10544, 2022. [111] Qipeng Wang, Mengwei Xu, Chao Jin, Xinran Dong, Jinliang Yuan, Xin Jin, Gang Huang, Yunxin Liu, and Xuanzhe Liu. Melon: Breaking the memory wall for resource-efficient on-device machine learning.",
    "flashfftconv-42": "In Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services, pages 450-463, 2022. [112] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.",
    "flashfftconv-43": "arXiv preprint arXiv:2006.04768, 2020. [113] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020. [114] Dan Zhang, Safeen Huda, Ebrahim Songhori, Kartik Prabhu, Quoc Le, Anna Goldie, and Azalia Mirhoseini. A full-stack search technique for domain optimized deep learning accelerators. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, pages 27-42, 2022. [115] Michael Zhang, Khaled Kamal Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher Re. Effectively modeling time series with simple discrete state spaces. In International Conference on Learning Representations, 2022. [116] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Information Processing Systems, 34:17723-17736, 2021. ## Appendix\n\nWe present additional algorithmic details (Appendix A), additional experimental results (Appendix B), and experimental details (Appendix C). ## A Algorithm Details\n\n## A. 1 Domain-Specific Optimizations\n\nWe review the details of how to compute a real-to-real FFT of size $N$ using a complex FFT of size $N / 2$, following a tutorial by [102]. For this section, we adopt notation common in describing FFT algorithms. Let $x(n)$ be an input sequence of length $N$, and let $X(k)$ be the result of its discrete Fourier transform. Recall that:\n\n$$\nX(k)=\\sum_{n=0}^{N-1} x(n) W_{N}^{n k}\n$$\n\nfor $k=0,1, \\ldots, N-1$, where $W_{N}=e^{-2 \\pi i / N}$ is the $N$ th root of unity. First, if $x(n)$ is real, then symmetries emerge in $X(k)$. In particular, we have $X(k)=X^{*}(-k)=X^{*}(N-k)$, where * denotes complex conjugation. These symmetries allow us to have an algorithm for computing $X(k)$ using a single complex DFT of size $N / 2$. In particular:\n\n$$\n\\begin{aligned}\nX(k) & =\\sum_{n=0}^{N-1} x(n) W_{N}^{n k} \\\\\n& =\\sum_{n=0}^{N / 2-1} x(2 n) W_{N / 2}^{n k}+W_{N}^{k} \\sum_{n=0}^{N / 2-1} x(2 n+1) W_{N / 2}^{n k}\n\\end{aligned}\n$$\n\nfor $k=0,1, \\ldots, N-1$. The DFT is now decomposed into two parts: a DFT over the even-indexed elements of $x(n)$, and over the odd-indexed elements of $x(n)$. We can now create a third complex sequence, of length $N / 2$, and put the even-indexed elements of $x(n)$ in the real part, and the odd-indexed elements of $x(n)$ in the imaginary part. Let:\n\n$$\nz(n)=x(2 n)+i x(2 n+1)\n$$\n\nfor $n=0,1, \\ldots, N / 2-1$. Then, we compute the $N / 2$-sized DFT $Z(k)$, and we can recover the DFT over the even and odd parts of $x(n)\\left(X_{e}[k]\\right.$ and $X_{o}[k]$, respectively $)$ :\n\n$$\n\\begin{aligned}\n& X_{e}[k]=\\frac{Z[k]+Z^{*}[N / 2-k]}{2} \\\\\n& X_{o}[k]=-i \\frac{Z[k]-Z^{*}[N / 2-k]}{2 i}\n\\end{aligned}\n$$\n\nWe can now recover $X[k], k=0 \\ldots, N-1$ using:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2e48bf63902e7cf6031fg-20.jpg?height=71&width=789&top_left_y=2076&top_left_x=668)\n\nThe inverse FFT proceeds similarly. The goal is to recover $x(n)$ given an input $X[k]$, using a simple complex inverse DFT of length $N / 2$. First, we recover $X_{e}[k]$ and $X_{o}[k]$ :\n\n$$\n\\begin{aligned}\n& X_{e}[k]=\\frac{X[k]+X^{*}[N / 2-k]}{2} \\\\\n& X_{o}[k]=\\frac{X[k]-X^{*}[N / 2-k]}{2} W_{N}^{k}\n\\end{aligned}\n$$\n\nfor $k=0, \\ldots, N / 2-1$. Then, we construct $Z[k]$ :\n\n$$\nZ[k]=X_{e}[k]+i X_{o}[k], k=0 \\ldots, N / 2-1\n$$\n\nWe use the inverse DFT to recover $z(n)$, and then recover $x(n)$ from the real and imaginary parts of $z(n)$ :\n\n$$\n\\begin{aligned}\nx(2 n) & =\\operatorname{Re}\\left(z_{n}\\right) \\\\\nx(2 n+1) & =\\operatorname{Im}\\left(z_{n}\\right)\n\\end{aligned}\n$$\n\nfor $n=0, \\ldots, N / 2-1$. To implement these in our kernels, we perform the bookkeeping after reading the inputs or before writing the output, and then use the FFT/iFFT implementations as detailed in Algorithm 1 and others. ## A. 2 Low-level CUDA details\n\nTo ensure high performance, we implement CUDA kernels for each specific sequence length, allowing us to cater to specific performance nuances that arise from the decomposition at that sequence length. In this section, we dive into some of the low-level implementation details for FlashFFTCONV. Matrix Multiplication Using CUDA Tensor cores CUDA Tensor cores can perform the multiplication of two $m \\times k$ and $k \\times n$ matrices for bfloat16 or float16 elements, using around the same number of cycles as is required for the multiplication of two scalars. $m \\times k \\times n$ must be of one of the following: $16 \\times 16 \\times 16$, $32 \\times 8 \\times 16, \\quad 8 \\times 32 \\times 16$. This informs our choice of radix for decomposition when performing the FFT and iFFT. In particular our implementation breaks down matrix-matrix multiplications into blocked matrix-matrix multiplications where $m \\times k \\times n=16 \\times 16 \\times 16$. We note the following about matrix-matrix multiplication on tensor cores [86]:\n\n- Tensor cores are utilized at the level of the warp and programmatic access of the tensor cores is via the Warp Level Matrix Multiply Accumulate (WMMA) API. - Tensor core operands are held in register fragments (wmma :: matrix_a, and wmma :: matrix_b) and results are written to a register fragment (wmma :: accumulator). - The operand fragments can hold data in row-major or column-major format and data in the wmma :: accumulator fragment can be written to memory in row-major or column-major format. - The specific mapping of items in a fragment to threads in warp is unspecified, however, the mapping of items to threads in the wmma :: accumulator fragment exactly matches that for the wmma :: matrix_a fragment read row-major, allowing us to directly copy the results of a matrix-matrix multiplication and use as the operand for another matrix-matrix multiply. To perform a matrix-matrix multiplication $C=A \\times B$ using the tensor cores, a warp loads the contents of $A$ and $B$ into registers (WMMA fragments in CUDA parlance), performs the matrix-matrix multiplication, and writes the results which are stored in an accumulator fragment back to memory. Register Reuse A key part of ensuring high performance is minimizing I/O across different levels of the memory hierarchy: from HBM to SRAM and from SRAM to registers. To ensure this, we move the output from the accumulator fragment directly into matrix_a fragment for use in subsequent matrix multiplications, avoiding an extra trip to SRAM. However, this is only possible if the output from the previous matrix-matrix multiply does not need to be transposed before using it as an operand for the next one. When this is not the case, we need to make a trip to SRAM and back. In Algorithm 2 we detail I/O from SRAM to registers. Locality and Tiling The algorithm is trivially parallelizable across $B$ and $H$, allowing us to tile in both dimensions at the threadblock level. In Algorithm 3, all loops from $i \\leftarrow 1$ to $N_{1}$ are warp-tiled. ```\nAlgorithm 2 Detailed Annotation of FlashFFTConv core algorithm showing I/O from SRAM to register\nfragments, with two-way Monarch decomposition. We assume \\(N=N_{1}^{2}\\) for simplicity here. Input: Input \\(u \\in \\mathbb{R}^{B \\times H \\times N}\\), convolution kernel \\(k_{f} \\in \\mathbb{C}^{H \\times N}\\), FFT matrices \\(\\mathbf{F} \\in \\mathbb{C}^{N_{1} \\times N_{1}}, \\mathbf{F}^{-\\mathbf{1}} \\in \\mathbb{C}^{N_{1} \\times N_{1}}\\),\n    Twiddle factors \\(t \\in \\mathbb{C}^{N}, t_{\\text {inv }} \\in \\mathbb{C}^{N}, B\\) tile size \\(B_{\\text {tile }}, H\\) tile size \\(H_{\\text {tile }}\\). Output: Output \\(y \\in \\mathbb{R}^{B \\times H \\times N}\\). for SMs in parallel across \\(B / B_{\\text {tile }} \\times H / H_{\\text {tile }}\\) do\n        Load \\(\\mathbf{F}, \\mathbf{F}^{-1}, t, t_{i n v}\\) from HBM. for \\(h \\leftarrow 1\\) to \\(H_{\\text {tile }}\\) do\n            Load \\(\\mathbf{K}_{\\mathbf{f}} \\leftarrow k_{f}[h]\\) from HBM, reshaped to \\(N_{1} \\times N_{1}\\). for \\(b \\leftarrow 1\\) to \\(B_{\\text {tile }}\\) do\n            Load \\(\\mathbf{X} \\leftarrow u[b, h]\\) from \\(H B M\\), reshaped to \\(N_{1} \\times N_{1}\\). \\(\\mathbf{X} \\leftarrow \\mathbf{F}^{\\top} \\mathbf{X} \\quad \\triangleright \\mathbf{F}^{\\top}(\\) matrix_a \\(), \\mathbf{X}\\left(\\right.\\) matrix_b \\(^{\\top}\\) ) output to accumulator\n            Load \\(\\mathbf{X}\\) from accumulator to matrix_a\n            \\(\\mathbf{X} \\leftarrow \\mathbf{X} * t \\quad \\triangleright\\) Elementwise multiply directly in matrix_a\n            \\(\\mathbf{X} \\leftarrow \\mathbf{X F} \\quad \\triangleright \\mathbf{X}(\\) matrix_a \\(), \\mathbf{F}(\\) matrix_b) output to accumulator\n            Load \\(\\mathbf{X}\\) from accumulator to matrix_a\n            \\(\\mathbf{X} \\leftarrow \\mathbf{X} * \\mathbf{K}_{\\mathbf{f}}{ }^{\\top} \\triangleright\\) Elementwise multiply with \\(k_{f}\\) directly in matrix_a\n            \\(\\mathbf{X} \\leftarrow \\mathbf{X F}^{-1} \\quad \\triangleright \\mathbf{X}(\\) matrix_a \\(), \\mathbf{F}^{-1}\\) (matrix_b) output to accumulator\n            Write \\(\\mathbf{X}\\) from accumulator fragment to SRAM\n            Load \\(\\mathbf{X}^{\\top}\\) from SRAM to matrix_a fragment\n            \\(\\mathbf{X} \\leftarrow \\mathbf{X}^{\\top} * t_{i n v} \\quad \\triangleright\\) Elementwise multiply with \\(t_{i n v}\\) directly in matrix_a\n            \\(\\mathbf{Y} \\leftarrow \\mathbf{X F}^{-1} \\quad \\triangleright \\mathbf{X}\\left(\\right.\\) matrix_a \\(^{-1}, \\mathbf{F}^{-1}\\) (matrix_b) output to accumulator\n                Write \\(\\mathbf{Y}^{\\top}\\) to HBM\n```\n\nMiscellaneous optimizations In addition to the above optimizations, we also perform some other optimizations that provide marginal speedup. These include: utilizing vector intrinsics/types for performing memory reads/writes and arithmetic for 16-bit floating point (fp16) and brain float point (bf16), allowing non-tensor core operations on these types to be performed at around twice the normal speed. Furthermore, we double buffer I/O movements across all levels of the memory hierarchy, reducing warp stalls. We also aggressively tune our kernel hyperparameters such as block and tile dimensions, and loop unrolling factors for the best performance on the specific underlying hardware. ## A. 3 Generalization to 3-way and 4-way Monarch Decompositions\n\nWe provide algorithm listings for 3 -way and 4 -way Monarch Decompositions. 3-Way Decomposition Algorithm 3 shows the algorithm for a 3-way Monarch decomposition. It involves one extra matrix multiply operation on either side of the FFT and iFFT, and proceeds over the algorithm in Algorithm 1 in an inner loop. 4-way Decomposition For the 4-way decomposition, we assume that we need to write intermediate outputs to HBM. Here, we treat the 3-way decomposition as a sub-routine, and assume it has a fused kernel (i.e., Algorithm 3). We compute one matrix multiply for the FFT and one for the iFFT, and then call the kernel for the 3 -way decomposition over the rows of the output. The algorithm is listed in Algorithm 4. ## A. 4 Frequency-Sparse Patterns\n\nWe describe frequency-sparse patterns and the matmul savings in more detail here. We use the full 4-way decomposition case, since the algorithms generalize to lower-order decompositions. Let $N=N_{1}^{4}$, and consider a kernel $k_{f} \\in \\mathbf{C}^{N}$. Consider the matrix multiply and looping operations that occur when computing the FFT portions of FlashFFTConv $\\left(u, k_{f}\\right)$ (the iFFT portions are the same, in the opposite order):\n\n```\nAlgorithm 3 FlashFFTConv algorithm for 3-way decomposition. We assume \\(N=N_{1}^{3}\\) for simplicity here. Input: Input \\(u \\in \\mathbb{R}^{B \\times H \\times N}\\), convolution kernel \\(k_{f} \\in \\mathbb{C}^{H \\times N}\\), FFT matrices \\(\\mathbf{F} \\in \\mathbb{C}^{N_{1} \\times N_{1}}, \\mathbf{F}^{-\\mathbf{1}} \\in \\mathbb{C}^{N_{1} \\times N_{1}}\\),\n    Twiddle factors \\(t_{1} \\in \\mathbb{C}^{N_{1}^{2}}, t_{1, i n v} \\in \\mathbb{C}_{\\nVdash} N_{1}^{2}, t_{2} \\in \\mathbb{C}^{N}, t_{2, i n v} \\in \\mathbb{C}^{N}, B\\) tile size \\(B_{\\text {tile }}, H\\) tile size \\(H_{\\text {tile }}\\). Output: Output \\(y \\in \\mathbb{R}^{B \\times H \\times N}\\). for SMs in parallel across \\(B / B_{\\text {tile }} \\times H / H_{\\text {tile }}\\) do\n        Load \\(\\mathbf{F}, \\mathbf{F}^{-1}, t, t_{i n v}\\) from HBM. for \\(h \\leftarrow 1\\) to \\(H_{\\text {tile }}\\) do\n            Load \\(\\mathbf{K}_{\\mathbf{f}} \\leftarrow k_{f}[h]\\) from HBM, reshaped to \\(N_{1}^{2} \\times N_{1}\\). \\(\\mathbf{K}_{\\mathbf{f}} \\leftarrow K_{f}^{T}\\). \\(\\triangleright\\) Transpose last two dimensions. Reshape \\(\\mathbf{K}_{\\mathbf{f}}\\) to \\(N_{1} \\times N_{1}^{2}\\)\n            for \\(b \\leftarrow 1\\) to \\(B_{\\text {tile }}\\) do\n            Load \\(\\mathbf{X} \\leftarrow u[b, h]\\) from HBM , reshaped to \\(N_{1} \\times N_{1} \\times N_{1}\\). for \\(i \\leftarrow 1\\) to \\(N_{1}\\) do\n                \\(\\mathbf{X}^{\\prime} \\leftarrow \\mathbf{F X}\\left[:, i * N_{1}:(i+1) * N_{1}\\right]\\)\n                \\(\\mathbf{X}\\left[:, \\mathbf{i} * \\mathbf{N}_{\\mathbf{1}}:(\\mathbf{i}+\\mathbf{1}) * \\mathbf{N}_{\\mathbf{1}}\\right] \\leftarrow \\mathbf{X}^{\\prime} \\quad \\triangleright\\) Transpose, matmul, transpose. \\(\\mathbf{X} \\leftarrow \\mathbf{X} * t_{2}\\)\n            for \\(i \\leftarrow 1\\) to \\(N_{1}\\) do \\(\\triangleright\\) Loop over rows\n                \\(\\mathbf{X}^{\\prime} \\leftarrow \\mathbf{F X}[i]\\)\n                Reshape \\(\\mathbf{X}^{\\prime}\\) to \\(N_{1} \\times N_{1}\\)\n                \\(\\mathbf{X}^{\\prime} \\leftarrow\\left(\\left(\\mathbf{F}^{\\top} \\mathbf{X}^{\\prime}\\right) * t\\right) \\mathbf{F} \\quad \\triangleright\\) FFT, decomposed into two steps\n                \\(\\mathbf{X}^{\\prime} \\leftarrow \\mathbf{X}^{\\prime} * \\mathbf{K}_{\\mathbf{f}}[i]^{\\top} \\quad \\triangleright\\) Elementwise multiply with \\(k_{f}\\)\n                \\(\\mathbf{Y}^{\\prime} \\leftarrow\\left(\\left(\\mathbf{X}^{\\prime} \\mathbf{F}^{-1}\\right)^{\\top} * t_{\\text {inv }}\\right) \\mathbf{F}^{-1} \\quad \\triangleright\\) Inverse FFT, decomposed into two steps\n                \\(\\mathbf{Y}^{\\prime} \\leftarrow \\mathbf{Y}^{\\prime}{ }^{\\top}\\)\n                \\(\\mathbf{Y}[i] \\leftarrow \\mathbf{Y}^{\\prime} \\quad \\triangleright\\) Finish inner loop\n            \\(\\mathbf{Y} \\leftarrow \\mathbf{Y} * t_{2, i n v}\\)\n            for \\(i \\leftarrow 1\\) to \\(N_{1}\\) do\n                \\(\\mathbf{Y}^{\\prime} \\leftarrow \\mathbf{F Y}\\left[:, i * N_{1}:(i+1) * N_{1}\\right]\\)\n                \\(\\mathbf{Y}\\left[:, \\mathbf{i} * \\mathbf{N}_{\\mathbf{1}}:(\\mathbf{i}+\\mathbf{1}) * \\mathbf{N}_{\\mathbf{1}}\\right] \\leftarrow \\mathbf{Y}^{\\prime} \\quad \\triangleright\\) Transpose, matmul, transpose. Write \\(\\mathbf{Y}\\) to HBM. ```\n\n1. In Algorithm 4, there is one FFT operation over the columns of $u$, reshaped to $N_{1} \\times N / N_{1}$, and a Twiddle correction.. 2. Then, Algorithm 3 iterates over the rows of $u$ for $\\alpha:=N_{1}$ steps. 3. Let $u^{\\prime}$ be the row in a specific iteration. In Algorithm 3, there is an FFT over the columns of $u^{\\prime}$, reshaped to $N_{1} \\times N_{1}^{2}$, and a Twiddle correction. 4. Then, the inner loop iterates over the rows of $u^{\\prime}$ for $\\beta:=N_{1}$ steps. 5. In each loop, $u^{\\prime}$ has one FFT operation with a twiddle factor correction. Let the matrix of this FFT operation be denoted $\\mathbf{A}$. 6. Then there is a second FFT operation. Let the matrix of this FFT operation be denoted B. Now, reshape $k_{f}$ to $N_{1} \\times N_{1} \\times N_{1} \\times N_{1}$. Let us consider how sparsity along the each of the four dimensions of $k_{f}$ lets us skip operations in the above steps. - Sparsity in the first dimension allows us to skip computation in $\\mathbf{B}$, exactly in proportion to how much of the first dimension we eliminate. This can result in cost savings, as long as $\\mathbf{B}$ can still be expressed using the tensor cores on-chip after skipping the computation. For example, if $\\mathbf{B}$ is $32 \\times 32$, then $N_{1}=32$, and it does not make sense to eliminate more than half of the first dimension. - Sparsity in the second dimension works exactly the same way, except it allows us to skip computation in A. ```\nAlgorithm 4 FlashFFTConv algorithm for 4-way decomposition. We assume \\(N=N_{1}^{4}\\) for simplicity here. Input: Input \\(u \\in \\mathbb{R}^{B \\times H \\times N}\\), convolution kernel \\(k_{f} \\in \\mathbb{C}^{H \\times N}\\), FFT matrices \\(\\mathbf{F} \\in \\mathbb{C}^{N_{1} \\times N_{1}}, \\mathbf{F}^{-\\mathbf{1}} \\in \\mathbb{C}^{N_{1} \\times N_{1}}\\),\n    Twiddle factors \\(t \\in \\mathbb{C}^{N}, t_{i n v} \\in \\mathbb{C}_{\\nVdash}{ }^{N}, t_{2} \\in \\mathbb{C}^{N}, t_{2, i n v} \\in \\mathbb{C}^{N}\\). Output: Output \\(y \\in \\mathbb{R}^{B \\times H \\times N}\\). Reshape \\(u\\) to \\(B \\times H \\times N_{1} \\times\\left(N / N_{1}\\right)\\). Reshape \\(k_{f}\\) to \\(H \\times N_{1} \\times\\left(N / N_{1}\\right)\\). \\(k_{f} \\leftarrow k_{f}^{\\top}\\). \\(\\triangleright\\) Transpose last two dimensions. Reshape \\(k_{f}\\) to \\(H N_{1} \\times N / N_{1}\\). \\(u \\leftarrow \\mathbf{F} u \\quad \\triangleright\\) Computes the FFT over the columns of \\(u\\). Reshape \\(u\\) to \\(B \\times\\left(H N_{1}\\right) \\times\\left(N / N_{1}\\right)\\). \\(\\quad \\triangleright\\) Move \\(N_{1}\\) into \\(H\\) dimension. Reshape \\(k_{f}\\) to \\(\\left(H N_{1}\\right) \\times\\left(N / N_{1}\\right)\\). Call FlashFFTConv \\(\\left(u, k_{f}\\right)\\). \\(\\triangleright\\) Call 3-way FlashFFTConv. Reshape \\(u\\) to \\(B \\times H \\times N_{1} \\times\\left(N / N_{1}\\right)\\). \\(y \\leftarrow \\mathbf{F}^{-1} u \\quad \\triangleright\\) Computes the iFFT over the columns of \\(u\\). Return \\(y\\). ```\n\nTable 10: Sparsity patterns for $k_{f}$ and sparsity fraction for the frequency-sparse convolution experiment in Table 9 . | Sparsity Pattern | S |\n| ---: | :---: |\n| $\\mathrm{a}=0, \\mathrm{~b}=0, \\mathrm{c}=0, \\mathrm{~d}=0$ | 0 |\n| $\\mathrm{a}=16, \\mathrm{~b}=0, \\mathrm{c}=0, \\mathrm{~d}=0$ | 50 |\n| $\\mathrm{a}=16, \\mathrm{~b}=1, \\mathrm{c}=0, \\mathrm{~d}=0$ | 75 |\n| $\\mathrm{a}=16, \\mathrm{~b}=16, \\mathrm{c}=4, \\mathrm{~d}=4$ | 79 |\n| $\\mathrm{a}=16, \\mathrm{~b}=16, \\mathrm{c}=8, \\mathrm{~d}=8$ | 84 |\n| $\\mathrm{a}=16, \\mathrm{~b}=16, \\mathrm{c}=16, \\mathrm{~d}=16$ | 91 |\n\n- Sparsity in the third dimension lets us reduce $\\beta$. Each row of the third dimension that we remove lets us skip one iteration of the inner loop in step 4 above. - Sparsity in the fourth dimension lets us reduce $\\alpha$. Each row of the fourth dimension that we remove lets us skip one iteration of the outer loop in step 2 above. As an example, we reveal the sparsity dimensions that we applied in the experiment detailed in Table 9 in the main paper. Conceptually, we use the full 2-million length kernel $k_{f}$, and reshape it to $32 \\times 32 \\times 32 \\times 64$. Let $a, b, c$, and $d$ be variables describing how much of each dimension we set to zero. Specifically, we set $k_{f}[a:,:,:,:]=0, k_{f}[:, b:,:,:]=0, k_{f}[:,:, c:,:]=0$, and $k_{f}[:,:,:, d:]=0$ sequentially. The formula the sparsity fraction $S$ given $a, b, c, d$ in this case is given by:\n\n$$\nS=1-(32-a)(32-b)(32-c)(64-d)\n$$\n\nor more generally, 1 minus the product of the fraction of each dimension that is removed. Table 10 lists the configurations of the sparsity patterns and the sparsity fractions used for the experiment in Table 9. ## A. 5 Hardware Support\n\nFlashFFTConv was developed on A100 GPUs, and tested on A100 and H100 GPUs. Older generations of GPU such as V100 are not supported, since the sizes of the tensor cores are different. We look forward to integrating more general libraries such as Cutlass [88] to support a wider range of GPUs, and developing support for non-GPU accelerators. Table 11: Full results for the forward pass of a convolution with FlashFFTConv compared to PyTorch in milliseconds on one H100-SXM. Batch size 64, hidden dimension 768. | Seq Len | PyTorch | FlashFFTConv | Speedup |\n| ---: | :---: | :---: | :---: |\n| $\\mathbf{2 5 6}$ | 0.43 | 0.09 | 4.69 |\n| $\\mathbf{5 1 2}$ | 0.81 | 0.15 | 5.34 |\n| $\\mathbf{1 0 2 4}$ | 1.57 | 0.24 | 6.61 |\n| $\\mathbf{2 0 4 8}$ | 3.27 | 0.55 | 5.95 |\n| $\\mathbf{4 0 9 6}$ | 6.65 | 1.37 | 4.87 |\n| $\\mathbf{8 1 9 2}$ | 13.72 | 3.19 | 4.30 |\n| $\\mathbf{1 6 3 8 4}$ | 28.58 | 9.27 | 3.09 |\n| $\\mathbf{3 2 7 6 8}$ | 62.09 | 21.84 | 2.84 |\n| $\\mathbf{6 5 5 3 6}$ | 141.15 | 67.96 | 2.08 |\n| $\\mathbf{1 3 1 0 7 2}$ | 292.26 | 147.26 | 1.98 |\n| $\\mathbf{2 6 2 1 4 4}$ | 582.76 | 308.48 | 1.89 |\n| $\\mathbf{5 2 4 2 8 8}$ | $1,167.28$ | 742.26 | 1.57 |\n| $\\mathbf{1 0 4 8 5 7 6}$ | $2,346.26$ | $1,492.84$ | 1.57 |\n| $\\mathbf{2 0 9 7 1 5 2}$ | $4,892.09$ | $2,695.51$ | 1.81 |\n| $\\mathbf{4 1 9 4 3 0 4}$ | $10,127.56$ | $7,586.96$ | 1.33 |\n\n## B Additional Results\n\n## B. 1 Full Results for All Sequence Lengths\n\nWe report full results for all sequence lengths in powers of two between 256 and 4 M . We report full results for five cases:\n\n- Table 11: Standard forward pass, where the FFT size is the same as the input size. This is equivalent to a circular convolution. - Table 12: Gated forward pass, where the FFT size is the same as the input size. - Table 13: Forward pass, where the input size is half the FFT size. This is equivalent to a causal convolution. - Table 14: Gated forward pass, where the input size is half the FFT size. - Table 15 Standard backward pass, where the FFT size is the same as the input size. - Table 16 Memory use for FlashFFTConv compared to PyTorch for a convolution, scaled to batch size 64 , hidden dimension 768 . - Table 17 Memory use for a gated convolution using FlashFFTConv compared to PyTorch for a convolution, scaled to batch size 64 , hidden dimension 768 . Speedups vary, but generally follow the trend from the results in the body of the paper. FlashFFTCONV achieves significant memory savings over PyTorch due to recomputation in the backward pass and kernel fusion. To measure memory savings, we measure the relative additional memory from calling the convolution operations (we do not measure the footprint of hte original inputs). ## B. 2 Reference Larger Models\n\nTable 18 gives performance numbers for larger models trained for the same number of tokens and steps as the reference PyTorch models in Table 1 in the main paper. The GPT-style PyTorch models are trained for 5B tokens, with batch size 512 K tokens. The BERT-style PyTorch models are trained for 16000 steps, with batch size 64 K tokens. In contrast, the FlashFFTConV models, with higher training throughput, are trained for 15 B tokens and 70000 steps in the same compute budget, respectively. Table 12: Full results for the forward pass of a gated convolution with FlashFFTCONV compared to PyTorch in milliseconds on one H100-SXM. Batch size 64, hidden dimension 768. | Seq Len | PyTorch | FlashFFTConv | Speedup |\n| ---: | :---: | :---: | :---: |\n| $\\mathbf{2 5 6}$ | 0.62 | 0.11 | 5.76 |\n| $\\mathbf{5 1 2}$ | 1.18 | 0.19 | 6.14 |\n| $\\mathbf{1 0 2 4}$ | 2.30 | 0.29 | 7.81 |\n| $\\mathbf{2 0 4 8}$ | 4.70 | 0.67 | 7.05 |\n| $\\mathbf{4 0 9 6}$ | 9.49 | 1.43 | 6.65 |\n| $\\mathbf{8 1 9 2}$ | 19.38 | 3.58 | 5.42 |\n| $\\mathbf{1 6 3 8 4}$ | 39.91 | 12.18 | 3.28 |\n| $\\mathbf{3 2 7 6 8}$ | 84.79 | 26.32 | 3.22 |\n| $\\mathbf{6 5 5 3 6}$ | 186.69 | 79.84 | 2.34 |\n| $\\mathbf{1 3 1 0 7 2}$ | 382.98 | 181.51 | 2.11 |\n| $\\mathbf{2 6 2 1 4 4}$ | 764.08 | 376.96 | 2.03 |\n| $\\mathbf{5 2 4 2 8 8}$ | $1,530.34$ | 878.93 | 1.74 |\n| $\\mathbf{1 0 4 8 5 7 6}$ | $3,071.37$ | $1,768.94$ | 1.74 |\n| $\\mathbf{2 0 9 7 1 5 2}$ | $6,342.58$ | $4,623.46$ | 1.37 |\n| $\\mathbf{4 1 9 4 3 0 4}$ | $13,031.21$ | $10,049.42$ | 1.30 |\n\nTable 13: Full results for the forward pass of a convolution where the input is half the length of the convolution size with FlashFFTConv compared to PyTorch in milliseconds on one H100-SXM. Batch size 64, hidden dimension 768 . | Seq Len | PyTorch | FlashFFTConv | Speedup |\n| ---: | :---: | :---: | :---: |\n| $\\mathbf{2 5 6}$ | 0.44 | 0.09 | 4.64 |\n| $\\mathbf{5 1 2}$ | 0.82 | 0.16 | 5.03 |\n| $\\mathbf{1 0 2 4}$ | 1.57 | 0.24 | 6.45 |\n| $\\mathbf{2 0 4 8}$ | 3.25 | 0.53 | 6.08 |\n| $\\mathbf{4 0 9 6}$ | 6.59 | 1.37 | 4.83 |\n| $\\mathbf{8 1 9 2}$ | 13.60 | 3.13 | 4.34 |\n| $\\mathbf{1 6 3 8 4}$ | 28.37 | 8.82 | 3.22 |\n| $\\mathbf{3 2 7 6 8}$ | 61.87 | 21.34 | 2.90 |\n| $\\mathbf{6 5 5 3 6}$ | 141.42 | 77.32 | 1.83 |\n| $\\mathbf{1 3 1 0 7 2}$ | 292.26 | 151.28 | 1.93 |\n| $\\mathbf{2 6 2 1 4 4}$ | 582.82 | 315.99 | 1.84 |\n| $\\mathbf{5 2 4 2 8 8}$ | $1,167.21$ | 757.33 | 1.54 |\n| $\\mathbf{1 0 4 8 5 7 6}$ | $2,343.55$ | $1,525.13$ | 1.54 |\n| $\\mathbf{2 0 9 7 1 5 2}$ | $4,922.63$ | $3,321.71$ | 1.48 |\n| $\\mathbf{4 1 9 4 3 0 4}$ | $10,179.86$ | $7,305.61$ | 1.39 |\n\nTable 14: Full results for the forward pass of a gated convolution where the input is half the length of the convolution size with FlashFFTConv compared to PyTorch in milliseconds on one H100-SXM. Batch size 64, hidden dimension 768 . | Seq Len | PyTorch | FlashFFTConv | Speedup |\n| ---: | :---: | :---: | :---: |\n| $\\mathbf{2 5 6}$ | 0.54 | 0.11 | 4.71 |\n| $\\mathbf{5 1 2}$ | 1.01 | 0.19 | 5.27 |\n| $\\mathbf{1 0 2 4}$ | 1.94 | 0.29 | 6.75 |\n| $\\mathbf{2 0 4 8}$ | 3.97 | 0.59 | 6.69 |\n| $\\mathbf{4 0 9 6}$ | 8.01 | 1.41 | 5.68 |\n| $\\mathbf{8 1 9 2}$ | 16.42 | 3.46 | 4.75 |\n| $\\mathbf{1 6 3 8 4}$ | 34.04 | 10.62 | 3.21 |\n| $\\mathbf{3 2 7 6 8}$ | 73.15 | 25.03 | 2.92 |\n| $\\mathbf{6 5 5 3 6}$ | 163.75 | 78.88 | 2.08 |\n| $\\mathbf{1 3 1 0 7 2}$ | 337.37 | 153.13 | 2.20 |\n| $\\mathbf{2 6 2 1 4 4}$ | 672.48 | 319.47 | 2.10 |\n| $\\mathbf{5 2 4 2 8 8}$ | $1,346.99$ | 763.97 | 1.76 |\n| $\\mathbf{1 0 4 8 5 7 6}$ | $2,704.91$ | $1,538.89$ | 1.76 |\n| $\\mathbf{2 0 9 7 1 5 2}$ | $5,644.20$ | $3,545.79$ | 1.59 |\n| $\\mathbf{4 1 9 4 3 0 4}$ | $11,625.79$ | $8,132.32$ | 1.43 |\n\nTable 15: Full results for the backward pass of a convolution with FlashFFTConv compared to PyTorch in milliseconds on one H100-SXM. Batch size 64, hidden dimension 768. | Seq Len | PyTorch | FlashFFTConv | Speedup |\n| ---: | :---: | :---: | :---: |\n| $\\mathbf{2 5 6}$ | 0.76 | 0.24 | 3.24 |\n| $\\mathbf{5 1 2}$ | 1.45 | 0.22 | 6.43 |\n| $\\mathbf{1 0 2 4}$ | 2.83 | 0.65 | 4.37 |\n| $\\mathbf{2 0 4 8}$ | 5.76 | 1.48 | 3.90 |\n| $\\mathbf{4 0 9 6}$ | 11.56 | 2.86 | 4.05 |\n| $\\mathbf{8 1 9 2}$ | 23.11 | 6.16 | 3.75 |\n| $\\mathbf{1 6 3 8 4}$ | 46.85 | 18.57 | 2.52 |\n| $\\mathbf{3 2 7 6 8}$ | 103.85 | 57.68 | 1.80 |\n| $\\mathbf{6 5 5 3 6}$ | 241.81 | 111.76 | 2.16 |\n| $\\mathbf{1 3 1 0 7 2}$ | 489.38 | 239.32 | 2.04 |\n| $\\mathbf{2 6 2 1 4 4}$ | 976.24 | 519.49 | 1.88 |\n| $\\mathbf{5 2 4 2 8 8}$ | $1,960.31$ | $1,240.95$ | 1.58 |\n| $\\mathbf{1 0 4 8 5 7 6}$ | $3,938.92$ | $2,708.36$ | 1.45 |\n| $\\mathbf{2 0 9 7 1 5 2}$ | $7,909.27$ | $4,977.93$ | 1.59 |\n| $\\mathbf{4 1 9 4 3 0 4}$ | $16,552.21$ | $12,932.02$ | 1.28 |\n\nTable 16: Memory usage in GB for FlashFFTConv compared to PyTorch. Scaled up to batch size 64, hidden dimension 768 . | Seq Len | PyTorch | FlashFFTConv | Memory Reduction |\n| ---: | :---: | :---: | :---: |\n| $\\mathbf{2 5 6}$ | 0.42 | 0.05 | $8.21 \\times$ |\n| $\\mathbf{5 1 2}$ | 0.80 | 0.10 | $8.19 \\times$ |\n| $\\mathbf{1 0 2 4}$ | 1.58 | 0.20 | $7.73 \\times$ |\n| $\\mathbf{2 0 4 8}$ | 3.12 | 0.39 | $7.94 \\times$ |\n| $\\mathbf{4 0 9 6}$ | 6.21 | 0.82 | $7.61 \\times$ |\n| $\\mathbf{8 1 9 2}$ | 12.39 | 1.63 | $7.59 \\times$ |\n| $\\mathbf{1 6 3 8 4}$ | 24.93 | 3.46 | $7.21 \\times$ |\n| $\\mathbf{3 2 7 6 8}$ | 50.43 | 7.68 | $6.57 \\times$ |\n| $\\mathbf{6 5 5 3 6}$ | 121.60 | 46.08 | $2.64 \\times$ |\n| $\\mathbf{1 3 1 0 7 2}$ | 243.21 | 92.18 | $2.64 \\times$ |\n| $\\mathbf{2 6 2 1 4 4}$ | 486.41 | 184.39 | $2.64 \\times$ |\n| $\\mathbf{5 2 4 2 8 8}$ | 972.83 | 368.91 | $2.64 \\times$ |\n| $\\mathbf{1 0 4 8 5 7 6}$ | 1945.65 | 738.34 | $2.64 \\times$ |\n| $\\mathbf{2 0 9 7 1 5 2}$ | 3889.23 | 1477.69 | $2.63 \\times$ |\n| $\\mathbf{4 1 9 4 3 0 4}$ | 7778.45 | 2961.56 | $2.63 \\times$ |\n\nTable 17: Memory usage in GB for FlashFFTConv for a gated convolution compared to PyTorch. Scaled up to batch size 64, hidden dimension 768 . | Seq Len | PyTorch | FlashFFTConv | Memory Reduction |\n| ---: | :---: | :---: | :---: |\n| $\\mathbf{2 5 6}$ | 0.66 | 0.10 | $6.65 \\times$ |\n| $\\mathbf{5 1 2}$ | 1.28 | 0.19 | $6.61 \\times$ |\n| $\\mathbf{1 0 2 4}$ | 2.54 | 0.40 | $6.40 \\times$ |\n| $\\mathbf{2 0 4 8}$ | 5.04 | 0.78 | $6.49 \\times$ |\n| $\\mathbf{4 0 9 6}$ | 10.05 | 1.58 | $6.35 \\times$ |\n| $\\mathbf{8 1 9 2}$ | 20.07 | 3.17 | $6.34 \\times$ |\n| $\\mathbf{1 6 3 8 4}$ | 40.29 | 6.53 | $6.17 \\times$ |\n| $\\mathbf{3 2 7 6 8}$ | 81.15 | 13.83 | $5.87 \\times$ |\n| $\\mathbf{6 5 5 3 6}$ | 164.61 | 58.37 | $2.82 \\times$ |\n| $\\mathbf{1 3 1 0 7 2}$ | 329.22 | 116.75 | $2.82 \\times$ |\n| $\\mathbf{2 6 2 1 4 4}$ | 658.44 | 233.54 | $2.82 \\times$ |\n| $\\mathbf{5 2 4 2 8 8}$ | 1316.89 | 467.21 | $2.82 \\times$ |\n| $\\mathbf{1 0 4 8 5 7 6}$ | 2633.78 | 934.95 | $2.82 \\times$ |\n| $\\mathbf{2 0 9 7 1 5 2}$ | 5265.48 | 1870.90 | $2.81 \\times$ |\n| $\\mathbf{4 1 9 4 3 0 4}$ | 10530.97 | 3747.99 | $2.81 \\times$ |\n\nTable 18: Reference quality numbers for models when trained for the same number of steps and training data. | Model (Metric) |  |\n| ---: | ---: |\n| M2-BERT-base-110M (GLUE Score $\\uparrow$ ) | 77.6 |\n| M2-BERT-large-260M (GLUE Score $\\uparrow$ ) | 81.0 |\n| Hyena-s-155M (PPL $\\downarrow$ ) | 13.4 |\n| Hyena-m-355M (PPL $\\downarrow$ ) | 11.1 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2e48bf63902e7cf6031fg-29.jpg?height=863&width=1449&top_left_y=666&top_left_x=292)\n\nFigure 5: t-SNE visualization of various genes and DNA segments using our new HyenaDNA-4M. The longest human gene, Dystrophin, is annotated. ## B. 3 DNA Embeddings\n\nWe use our 4M-sequence length HyenaDNA model to generate embeddings for various DNA segments following the procedure from [82]. The DNA classes include human genes corresponding to different biological function annotations from the Ensembl genome dataset known as biotypes [20]. The longest human gene, the dystrophin gene, is annotated. ## C Experiment Details\n\n## C. 1 Compute\n\nAll experiments were conducted on a box with 8 xA100-40GB GPUs or a box with 8 xH100-SXM GPUs. ## C. 2 Fixed Compute Budget Experiment\n\nFor the experiment in Table 1, we train an M2-BERT-base model from scratch, and a Hyena-s-155M model from scratch. We train the M2-BERT-base model using masked language modeling of $30 \\%$ on the C4 dataset, and finetune it on GLUE using the protocol from [42]. The FlashFFTCONV model has higher training throughput, so it trains for more tokens; we train the FlashFFTCONV model for 70,000 steps with a batch size of 64 K tokens. The PyTorch model, with lower training throughput, only trains for 16,000 steps, with the same batch size. The M2-BERT-base model we use is parameter-matched with a Transformer BERT-base. It has 12 hidden layers, with a model dimension of 960 , and an expansion factor of four. It also uses a block-diagonal MLP with four blocks. The M2 Hyena filter has embedding dimension 5, filter order 128, and initial sine activation factor of 10 . We train with learning rate $8 \\mathrm{e}-4$, weight decay $1 \\mathrm{e}-5$, and $6 \\%$ warmup with a cosine decay. We train the Hyena-s-155M model using a causal language modeling objective on the Pile. We train the FlashFFTConv model for 15 M tokens, and the PyTorch model for 5 M tokens. The Hyena-s-155M model matches the configuration from [94] and has 18 layers, with a hidden dimension of 864, and an expansion factor of 4. The Hyena filter has embedding dimension 33, filter order 64, and initial sine activation factor of 14. We train with learning rate $6 \\mathrm{e}-4$, with $1 \\%$ warmup time and a cosine decay. ## C. 3 Path-X and Path-512 Experiments\n\nFor the experiment in Table 2, we use simple convolutional language models, as in [44]. For Path-X, we use the same model and hyperparameters as the convolutional model from [44]. We use a convolutional model with 6 layers, prenorm batch norm, and hidden dimension of 256 . For the convolution filter parameters, we use kernel dropout 0.3 , kernel learning rate $0.0005, \\lambda$ factor 0.001 , and two channels on the filter. We use an overall learning rate of 0.0005 and weight decay 0.05 . We train for 500000 steps, with 10000 steps of warmup with a cosine decay, and global batch size 16. For Path-512, we scale up the resolution of Path-256. We train for 200000 steps, with 10000 steps warmup, learning rate 0.0005 , and weight decay 0.05 . For the model, we train with 4 layers, and hidden dimension 256 . We use kernel dropout 0.1 , kernel learning rate $0.0005, \\lambda$ factor 0.001 , and two channels on the filter. We keep the filter length to be 65536 . ## C. 4 Convolution Benchmarks\n\nFor the experiments in Table 3, we time the forward pass of a convolution with batch size 64, hidden dimension 768 , and varying sequence length. If we run out of memory for a sequence length, we split the batch and hidden dimension and call the forward pass multiple times. We time each call 30 times and take the average of the runs. We use the same protocol for the backward pass in Table ??. ## C. 5 End-to-End Modeling Details\n\nFor the experiments in Table 5, we run forward pass of each model, and use it to compute throughput. Batch sizes vary by model, and we check throughput calculations with a few batch sizes to make sure the result is consistent. For the M2-BERT-base model, we use a 110M model from Monarch Mixer [42]. For the Hyena-s-4K model, we use an identical model to the one in Table 1, but with a filter length of 4K. For the long convs Path-X model, we use the same model as in Table 2. For the SaShiMi model, we use the standalone SaShiMi model from the official implementation [45], and we use 8 layers with hidden dimension 64, and 4 up pool and down pool layers. For the HyenaDNA model, we use the official 1M-sequence length checkpoint from [82]. For M2-BERT-base, Hyena-s-4K, and HyenaDNA, we additionally use a fast depthwise convolution kernel for short kernels. For M2-BERT-base, Hyena-s-4K, and HyenaDNA, we report results benchmarked on one H100-SXM. For the others, we report performance on one A100-40GB. ## C. 6 Comparison to Transformers\n\nFor the comparison against Transformers in Table 6, we use the official implementations with the FlashAttentionv2 release [22]. We use a Hyena model, and match the number of layers, hidden dimension, and expansion factor to the 2.7B Transformer model. To compute the FLOP usage, we take the formula:\n\n$$\n2 * \\text { num tokens } * \\text { num parameters }\n$$\n\nTable 19: Measured Constants for Cost Model for A100-40GB. | Constant | A100-40GB |\n| ---: | :---: |\n| $\\sigma_{H}$ | $1.35 \\mathrm{~TB} / \\mathrm{s}$ |\n| $\\sigma_{S}$ | $9.5 \\mathrm{~TB} / \\mathrm{s}$ |\n| $\\tau_{M}$ | 234 TFLOPs |\n| $\\tau_{G}$ | 17.6 TFLOPs |\n\nfor the parametric FLOPs. For the non-parameter FLOPs, we add the raw FLOP count from our cost model in Equation 2 (without the adjustment for speed of tensor core FLOPs). ## C. 7 Partial Convolutions for Hyena\n\nFor the measurement of memory footprint reduction in Table 7, we use the same Hyena-s model as in Tables 1 and 5 , except we cut the filter short. This lets us offload parts of the input, which reduces the memory footprint. ## C. 8 Extending HyenaDNA-1M\n\nIn Table 8, we use a sliding window approach to extend the HyenaDNA-1M and HyenaDNA-450K models to longer sequences. This mimics training a 4 M -sequence HyenaDNA with a short filter. ## C. 9 Frequency-Sparse Convolutions\n\nTo evaluate frequency-sparse convolutions, we take the pretrained HyenaDNA-1M model, and sparsify $k_{f}$ using the strategy described in Appendix A.4.",
    "flashfftconv-44": "We then run standard validation using the validation set from $[82]$. ## C. 10 Empirical GPU Profiling\n\nTable 19 gives empirically-measured GPU stats for an A100-40GB, which we used to generate Figure 4. The statistics are specialized to the Monarch decomposition workload. To measure the achievable tensor core FLOPs, we measured the utilization of real fp16 matrix multiply. To measure achievable general arithmetic FLOPs, we measured the utilization of continuously applying Twiddle factors. To measure the achievable HBM bandwidth, we measured the speed of torch. clone of a tensor. To measure the achievable SRAM bandwidth, we measured the slow down from writing intermediate results to SRAM between matrix multiply instructions. [^0]:    ${ }^{1}$ We refer to Path-512 as a scaled-up version of Path-256.",
    "flashfftconv-45": ""
}