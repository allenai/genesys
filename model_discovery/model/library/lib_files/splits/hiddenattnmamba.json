{
    "hiddenattnmamba-0": "# The Hidden Attention of Mamba Models \n\nAmeen Ali*, Itamar Zimerman*, and Lior Wolf<br>School of Computer Science, Tel Aviv University\n\n\n#### Abstract\n\nThe Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains, including NLP, long-range sequence processing, and computer vision.",
    "hiddenattnmamba-1": "Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via an IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available ${ }^{1}$. ## 1 Introduction\n\nRecently, Selective State Space Layers [30], also known as Mamba models, have shown remarkable performance in diverse applications including language modeling [5,30,58,73], image processing [44,82], video processing [80], medical imaging [28,43,48,62,75,76,78], tabular data [2], point-cloud analysis [42], graphs [10, 71], N-dimensional sequence modeling [41] and more. Characterized by their linear complexity in sequence length during training and fast RNN-like computation during inference (left and middle panels of Fig. 1), Mamba models offer a 5 x increase in the throughput of Transformers for auto-regressive generation and the ability to efficiently handle long-range dependencies. Despite their growing success, the information-flow dynamics between tokens in Mamba models and the way they learn remain largely unexplored. Critical questions about their learning mechanisms, particularly how they capture dependencies and their resemblance to other established layers, such as RNNs, CNNs, or attention mechanisms, remain unanswered. Additionally, the lack of interoperability methods for these models may pose a significant hurdle to debugging them and may also reduce their applicability in socially sensitive domains in which explainability is required. Motivated by these gaps, our research aims to provide insights into the dynamics of Mamba models and develop methodologies for their interpretation. While the traditional views of state-space models are through the lens of convolutional or recurrent layers [32], we show that selective state-space layers are a form\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_d5bc810f0d7b57c18473g-02.jpg?height=500&width=1128&top_left_y=387&top_left_x=496)\n\nFig. 1: Three Perspectives of the Selective State-Space Layer:(Left) Selective StateSpace Models (SSMs) can be efficiently computed with linear complexity using parallel scans, allowing for effective parallelization on modern hardware, such as GPUs. (Middle) Similar to SSMs, the selective state-space layer can be computed via a time-variant recurrent rule. (Right) A new view of the selective SSM layer, showing that it uses attention similarly to transformers (see Eq. 11). Our view enables the generation of attention maps, offering valuable applications in areas such as XAI. of attention models. This is achieved through a novel reformulation of Mamba computation using a data-control linear operator, unveiling hidden attention matrices within the Mamba layer. This enables us to employ well-established interpretability and explainability techniques, commonly used in transformer realms, to devise the first set of tools for interpreting Mamba models. Furthermore, our analysis of implicit attention matrices offers a direct framework for comparing the properties and inner representations of transformers [70] and Mamba models. Our main contributions encompass the following main aspects: (i) We shed light on the fundamental nature of Mamba models, by showing that they rely on implicit attention, which is implemented by a unique data-control linear operator, as illustrated in Fig. 1 (right). (ii) Our analysis reveals that Mamba models give rise to three orders of magnitude more attention matrices than transformers. (iii) We provide a set of explainability and interpretability tools based on these hidden attention matrices. (iv) For comparable model sizes, Mamba model-based attention shows comparable explainability metrics results to that of transformers. (v) We present a theoretical analysis of the evolution of attention capabilities in state-space models and their expressiveness, offering a deeper understanding of the factors that contribute to Mamba's effectiveness. ## 2 Background\n\nTransformers The Transformer architecture [70] is the dominant architecture in the recent NLP and Computer Vision literature. It relies on self-attention to capture dependencies between different tokens. Self-attention allows these models to dynamically focus on different parts of the input sequence, calculating\nthe relevance of each part to others. It can be computed as follows:\n\n$$\n\\text { Self }-\\operatorname{Attention}(Q, K, V)=\\alpha V, \\quad \\alpha=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)\n$$\n\nwhere $Q, K$, and $V$ represent queries, keys, and values, respectively, and $d_{k}$ is the dimension of the keys. Additionally, the Transformer utilizes $H$ attention heads to process information in parallel, allowing the model to capture various dependencies. The attention matrix $\\alpha$ enables the models to weigh the importance of tokens based on their contribution to the context, and they can also used for interpretability [8], explainability [15], and improved classification [16, 69]. State-Space Layers State-Space Layers were first introduced in [32] and have seen significant improvements through the seminal work in [31]. These layers have demonstrated promising results across several domains, including NLP $[24,51]$, audio generation [26], image processing [9,54,79], long video understanding [72], RL [18, 45], speech recognition [63], and more. Given one channel of the input sequence $x:=\\left(x_{1}, \\cdots, x_{L}\\right)$ such that $x_{i} \\in \\mathbb{R}$, these layers can be implemented using either recurrence or convolution. The recurrent formulation, which relies on the recurrent state $h_{t} \\in \\mathbb{R}^{N}$ where $N$ is the state size, is defined as follows: given the discretization functions $f_{A}, f_{B}$, and parameters $A, B, C$ and $\\Delta$, the recurrent rule for the SSM is:\n\n$$\n\\bar{A}=f_{A}(A, \\Delta), \\quad \\bar{B}=f_{B}(A, B, \\Delta), \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t}, \\quad y_{t}=C h_{t}\n$$\n\nThis recurrent rule can be expanded as:\n$h_{t}=\\bar{A}^{t} \\bar{B} x_{0}+\\bar{A}^{t-1} \\bar{B} x_{1}+\\cdots+\\bar{B} x_{t}, \\quad y_{t}=C \\bar{A}^{t} \\bar{B} x_{0}+C \\bar{A}^{t-1} \\bar{B} x_{1}+\\cdots+C \\bar{B} x_{t}$\nSince the recurrence is linear, Eq. 3 can also be expressed as a convolution, via a convolution kernel $K:=\\left(k_{1}, \\cdots, k_{L}\\right)$, where $k_{i}=C \\bar{A}^{i-1} \\bar{B}$, thus allowing sub-quadratic complexity in sequence length. The equivalence between the recurrence and the convolution provides a versatile framework that enables parallel and efficient training with sub-quadratic complexity with the convolution view, alongside a faster recurrent view, facilitating the acceleration of autoregressive generation by decoupling step complexity from sequence length. As the layer defined as a map from $\\mathbb{R}^{\\mathbb{L}}$ to $\\mathbb{R}^{\\mathbb{L}}$, to process $D$ channels the layer employs $D$ independent copies of itself. S6 Layers A recent development in state space layers is selective SSMs [30] (S6), which show outstanding performance in NLP [5,58,73], vision [44,82], graph classification [10,71], and more. These models rely on time-variant SSMs, namely, the discrete matrices $\\bar{A}, \\bar{B}$, and $C$ of each channel are modified over the $L$ time steps depending on the input sequence. As opposed to traditional state-space layers, which operate individually on each channel, selective state-space layers compute the SSM matrices $\\bar{A}_{i}, \\bar{B}_{i}, C_{i}$ for all $i \\leq L$ based on all the channels, and then apply the time-variant recurrent rule individually for each channel. Hence, we denote the entire input sequence by $\\hat{x}:=\\left(\\hat{x}_{1}, \\cdots, \\hat{x}_{L}\\right) \\in \\mathbb{R}^{L \\times D}$ where $\\hat{x}_{i} \\in \\mathbb{R}^{D}$. The per-time discrete matrices $\\bar{A}_{i}, \\bar{B}_{i}$, and $C_{i}$ are defined as follows:\n\n$$\n\\begin{gathered}\nB_{i}=S_{B}\\left(\\hat{x}_{i}\\right), \\quad C_{i}=S_{C}\\left(\\hat{x}_{i}\\right), \\quad \\Delta_{i}=\\operatorname{softplus}\\left(S_{\\Delta}\\left(\\hat{x}_{i}\\right)\\right) \\\\\nf_{A}\\left(\\Delta_{i}, A\\right)=\\exp \\left(\\Delta_{i} A\\right), \\quad f_{B}\\left(\\Delta_{i}, A, B_{i}\\right)=\\Delta_{i} B_{i} \\\\\n\\bar{A}_{i}=f_{A}\\left(\\Delta_{i}, A\\right), \\quad \\bar{B}_{i}=f_{B}\\left(\\Delta_{i}, A, B_{i}\\right)\n\\end{gathered}\n$$\n\nwhere $f_{A}, f_{B}$ represents the discretization rule, $S_{B}, S_{C}, S_{\\Delta}$ are linear projection layers, and SoftPlus is an elementwise function that is a smooth approximation of ReLU. While previous state-space layers employ complex-valued SSMs and non-diagonal matrices, Mamba employs real-diagonal parametrization. The motivation for input-dependent time-variant layers is to make those recurrent layers more expressive and flexible, allowing them to capture more complex dependencies. While other input-dependent time-variant mechanisms have been proposed in previous works through gated RNNs, the S5 layer [66], or adaptive filtering via input-dependent IIR filters [47], Mamba significantly improves on these layers by presenting a flexible, yet still efficient, approach. This efficiency was achieved via the IO-aware implementation of associative scans, which can be parallelized on modern hardware via work-efficient parallel scanners [12, 50]. Mamba The Mamba block is built on top of the selective state-space layer, Conv1D and other elementwise operators. Inspired by the architecure of Gated MLP and H3 [24], and given an input $\\hat{x}^{\\prime}:=\\left(\\hat{x}_{1}^{\\prime}, \\cdots \\hat{x}_{L}^{\\prime}\\right)$ it is defined as follows:\n\n$$\n\\hat{x}=\\operatorname{SiLU}\\left(\\operatorname{Conv} 1 D\\left(\\operatorname{Linear}\\left(\\hat{x}^{\\prime}\\right)\\right)\\right), \\quad \\hat{z}=\\operatorname{SiLU}\\left(\\operatorname{Linear}\\left(\\hat{x}^{\\prime}\\right)\\right)\n$$\n\n$$\n\\left.\\hat{y}^{\\prime}=\\operatorname{Linear}(\\operatorname{Selective} \\operatorname{SSM}(\\hat{x}) \\otimes \\hat{z})\\right), \\quad \\hat{y}=\\operatorname{LayerNorm}\\left(\\hat{y}^{\\prime}+\\hat{x}^{\\prime}\\right)\n$$\n\nwhere $\\otimes$ is elementwise multiplication. Mamba models contain $\\Lambda$ stacked mamba blocks and $D$ channels per block, and we denote the tensors in the i-th block and j-th channel with a superscript, where the first index refers to the block number. Inspired by the vision transformer ViT [19], both [44,82] replace the standard self-attention mechanism by two Mamba layers, where each layer is applied in a bidirectional manner. The resulting model achieves favorable results compared to the standard ViT in terms of both accuracy and efficiency, when comparing models with the same number of parameters. Explainability Explainability methods have been extensively explored in the context of deep neural networks, particularly in domains such as natural language processing (NLP) [1,4,6,14,15, 81], computer vision [7,36,53,64,68], and attention-based models $[4,14,15,81]$. The contributions most closely aligned with ours are those specifically tailored for transformer explainability. Abnar and Zuidema [1] introduce the AttentionRollout method, which aggregates the attention matrices across different layers by analyzing the paths in the inter-layer pairwise attention graph. Chefer et al. $[14,15]$ combine LRP scores [7] with the attention gradients to obtain classspecific relevance scores. Ali et al. [4] enhanced attributions by treating the\nnon-linear Softmax and LayerNorm operators as a constant, thereby attributing relevance exclusively through the value path, disregarding these operators. Yuan et al. [81] treats the output token representations as states in a Markov chain in which the transition matrix is built using attention weights. Our work performs similar attention-based analysis for Mamba and we derive versions of $[1,15]$ that are suitable for such SSM models. ## 3 Method\n\nIn this section, we detail our methodology. First, in section 3.1, we reformulate selective state-space (S6) layers as self-attention, enabling the extraction of attention matrices from S6 layers. Subsequently, in sections 3.2 and 3.3, we demonstrate how these hidden attention matrices can be leveraged to develop class-agnostic and class-specific tools for explainable AI of Mamba models. ### 3.1 Hidden Attention Matrices In Selective State Spaces Layers\n\nGiven the per-channel time-variant system matrices $\\bar{A}_{1}, \\cdots, \\bar{A}_{L}, \\bar{B}_{1}, \\cdots, \\bar{B}_{L}$, and $C_{1}, \\cdots, C_{L}$ from Eq. 4 and 6 , each channel within the selective state-space layers can be processed independently. Thus, for simplicity, the formulation presented in this section will proceed under the assumption that the input sequence $x$ consists of a single channel. By considering the initial conditions $h_{0}=0$, unrolling Eq. 2 yields:\n$h_{1}=\\bar{B}_{1} x_{1}, \\quad y_{1}=C_{1} \\bar{B}_{1} x_{1}, \\quad h_{2}=\\bar{A}_{2} \\bar{B}_{1} x_{1}+\\bar{B}_{2} x_{2}, \\quad y_{2}=C_{2} \\bar{A}_{2} \\bar{B}_{1} x_{1}+C_{2} \\bar{B}_{2} x_{2}$\nand in general:\n\n$$\nh_{t}=\\sum_{j=1}^{t}\\left(\\Pi_{k=j+1}^{t} \\bar{A}_{k}\\right) \\bar{B}_{j} x_{j}, \\quad y_{t}=C_{t} \\sum_{j=1}^{t}\\left(\\Pi_{k=j+1}^{t} \\bar{A}_{k}\\right) \\bar{B}_{j} x_{j}\n$$\n\nBy converting Eq. 10 into a matrix form we get:\n\n$$\ny=\\tilde{\\alpha} x, \\quad\\left[\\begin{array}{c}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{L}\n\\end{array}\\right]=\\left[\\begin{array}{cccc}\nC_{1} \\bar{B}_{1} & 0 & \\cdots & 0 \\\\\nC_{2} \\bar{A}_{2} \\bar{B}_{1} & C_{2} \\bar{B}_{2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\nC_{L} \\Pi_{k=2}^{L} \\bar{A}_{k} \\bar{B}_{1} & C_{L} \\Pi_{k=3}^{L} \\bar{A}_{k} \\bar{B}_{2} & \\cdots & C_{L} \\bar{B}_{L}\n\\end{array}\\right]\\left[\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{L}\n\\end{array}\\right]\n$$\n\nHence, the S6 layer can be viewed as a data-controlled linear operator [59], where the matrix $\\tilde{\\alpha} \\in \\mathbb{R}^{L \\times L}$ is a function of the input and the parameters $A, S_{B}, S_{C}, S_{\\Delta}$. The element at row $i$ and column $j$ captures how $x_{j}$ influences $y_{i}$, and is computed by:\n\n$$\n\\tilde{\\alpha}_{i, j}=C_{i}\\left(\\Pi_{k=j+1}^{i} \\bar{A}_{k}\\right) \\bar{B}_{j}\n$$\n\nEq. 11 and 12 link $\\tilde{\\alpha}$ to the conventional standard attention matrix (Eq. 1), and highlight that S 6 can be considered a variant of causal self-attention. Simplifying and Interpreting the Hidden Matrices Since $\\bar{A}_{t}$ is a diagonal matrix, the different $N$ coordinates of the state $h_{t}$ in Eq. 10 do not interact when computing $h_{t+1}$. Thus, Eq. 10 (left) can be computed independently for each coordinate $m \\in\\{1,2, \\ldots, N\\}$ :\n\n$$\nh_{t}[m]=\\sum_{j=1}^{t}\\left(\\Pi_{k=j+1}^{t} \\bar{A}_{k}[m, m]\\right) \\bar{B}_{j}[m] x_{j}, \\quad y_{t}=\\sum_{m=1}^{N} C_{t}[m] h_{t}[m]\n$$\n\nwhere $C_{i}[m], A_{k}[m, m], B_{j}[m] \\in \\mathbb{R}$, plugging it into Eq. 12 yields:\n\n$$\n\\tilde{\\alpha}_{i, j}=C_{i}\\left(\\Pi_{k=j+1}^{i} \\bar{A}_{k}\\right) \\bar{B}_{j}=\\sum_{m=1}^{N} C_{i}[m]\\left(\\Pi_{k=j+1}^{i} \\bar{A}_{k}[m, m]\\right) \\bar{B}_{j}[m]\n$$\n\nNote that while equations 10 and 12 contain matrix multiplication, Eq. 13 relies on elementwise multiplication. An interesting observation arising from Eq. 14 is that a single channel of S6 produces $N$ inner attention matrices $C_{i}[m]\\left(\\prod_{k=j+1}^{i} \\bar{A}_{k}[m, m]\\right) \\bar{B}_{j}[m]$, which are summed up over $m$ to obtain $\\tilde{\\alpha}$. In contrast, in the Transformer, a single attention matrix is produced by each of the $H$ attention heads. Given that the number of channels in Mamba models $D$ is typically a hundred times greater than the number of heads in a transformer (for example, Vision-Mamba-Tiny has $D=384$ channels, compared to $H=3$ heads in DeiT-Tiny), the Mamba layer generates approximately $\\frac{D N}{H} \\approx 100 N$ more attention matrices than the original self-attention layer. To further understand the structure and characterization of these attention matrices, we will express the hidden attention matrices $\\tilde{\\alpha}$ for each channel $d$ as a direct function of the input $\\hat{x}$.",
    "hiddenattnmamba-2": "To do so, we first substitute Eq. 4,5 and Eq. 6 into Eq. 12, and obtain:\n\n$$\n\\begin{gathered}\n\\tilde{\\alpha}_{i, j}=S_{C}\\left(\\hat{x}_{i}\\right)\\left(\\prod_{k=j+1}^{i} \\exp \\left(\\operatorname{softplus}\\left(S_{\\Delta}\\left(\\hat{x}_{k}\\right)\\right) A\\right)\\right) \\operatorname{softplus}\\left(S_{\\Delta}\\left(\\hat{x}_{j}\\right)\\right) S_{B}\\left(\\hat{x}_{j}\\right)= \\\\\n\\quad S_{C}\\left(\\hat{x}_{i}\\right)\\left(\\exp \\left(\\sum_{k=j+1}^{i} \\operatorname{softplus}\\left(S_{\\Delta}\\left(\\hat{x}_{k}\\right)\\right)\\right) A\\right) \\operatorname{softplus}\\left(S_{\\Delta}\\left(\\hat{x}_{j}\\right)\\right) S_{B}\\left(\\hat{x}_{j}\\right)\n\\end{gathered}\n$$\n\nFor simplicitly, we propose a simplification of Eq. 16 by substituting the softplus function with the ReLU function, and summing only over positive elements:\n\n$$\n\\tilde{\\alpha}_{i, j} \\approx S_{C}\\left(\\hat{x}_{i}\\right)\\left(\\exp \\left(\\sum_{\\substack{k=j+1 \\\\ S_{\\Delta}\\left(\\hat{x}_{k}\\right)>0}}^{i} S_{\\Delta}\\left(\\hat{x}_{k}\\right)\\right) A\\right) \\operatorname{ReLU}\\left(S_{\\Delta}\\left(\\hat{x}_{j}\\right)\\right) S_{B}\\left(\\hat{x}_{j}\\right)\n$$\n\nConsider the following query/key/value notation:\n\n$$\n\\tilde{Q}_{i}:=S_{C}\\left(\\hat{x}_{i}\\right), \\quad \\tilde{K}_{j}:=\\operatorname{ReLU}\\left(S_{\\Delta}\\left(\\hat{x}_{j}\\right) S_{B}\\left(\\hat{x}_{j}\\right), \\quad \\tilde{H}_{i, j}:=\\exp \\left(\\sum_{\\substack{k=j+1 \\\\ S_{\\Delta}\\left(\\hat{x}_{k}\\right)>0}}^{i} S_{\\Delta}\\left(\\hat{x}_{k}\\right)\\right) A\\right. $$\n\nEq. 17 can be further simplified to:\n\n$$\n\\tilde{\\alpha}_{i, j} \\approx \\tilde{Q}_{i} \\tilde{H}_{i, j} \\tilde{K}_{j}\n$$\n\nThis formulation enhances our understanding of the Mamba's attention mechanism. Whereas traditional self-attention captures the influence of $x_{j}$ on $x_{i}$ through the dot products between $Q_{i}$ and $K_{j}$, Mamba's approach correlates this influence with $\\tilde{Q}_{i}$ and $\\tilde{K}_{j}$, respectively. Additionally, $\\tilde{H}_{i, j}$ controls the significance of the recent $i-j$ tokens, encapsulating the continuous aggregated historical context spanning from $x_{j}$ to $x_{i}$. This distinction between self-attention and Mamba, captured by $\\tilde{H}_{i, j}$ could be a key factor in enabling Mamba models to understand and utilize continuous historical context within sequences more efficiently than attention. Furthermore, Eq. 19, and 18 offer further insights into the characterization of the hidden attention matrices by demonstrating that the only terms modified across channels are $A$ and $\\Delta_{i}$, which influence the values of $\\tilde{H}_{i, j}$ and $\\tilde{K}_{j}$ through the discretization rule in Eq. 5. Hence, all the hidden attention matrices follow a common pattern, distinguished by the keys $\\tilde{K}_{j}$ via $\\Delta_{i}$ and the significance of the history $\\tilde{H}_{i, j}$ via $A$ and $\\Delta_{i}$. A distinct divergence between Mamba's attention mechanism and traditional self-attention lies in the latter's utilization of a per-row softmax function. It is essential to recognize that various attention models have either omitted the softmax [46] or substituted it with elementwise neural activations [38,49, 77, 83], achieving comparable outcomes to the original framework. The softmax operator is known to lead to oversmoothing [3, 74]. As we show in the Appendix A, Mamba attention layers have lower inter-token smoothing than Transformer attention. ### 3.2 Application to Attention Rollout\n\nAs our class-agnostic explainability teqchniqe for Mamba models, we built our method on top of the Attention-Rollout [1] method. For simplicity, we assume that we are dealing with a vision mamba model, which operates on sequences of size $L+1$, where $L$ is the sequence length obtained from the $\\sqrt{L} \\times \\sqrt{L}$ image patches, with a classification (CLS) token appended to the end of the sequence. To do so, for each sample, we first extract the hidden attention matrix $\\tilde{\\alpha}^{\\lambda, d}$ for any channel $d \\in[D]$ and layer $\\lambda \\in[\\Lambda]$ according to the formulation in section 3.1 (Eq. 11), such that $\\tilde{\\alpha}^{\\lambda, d} \\in \\mathbb{R}^{(L+1) \\times(L+1)}$\n\nAttention-Rollout is then applied as follows:\n\n$$\n\\forall \\lambda \\in[\\Lambda]: \\quad \\tilde{\\alpha}^{\\lambda}=\\mathbb{I}_{L+1}+\\underset{d \\in[D]}{\\mathbb{E}}\\left(\\tilde{\\alpha}^{\\lambda, d}\\right), \\quad \\tilde{\\alpha}^{\\lambda} \\in \\mathbb{R}^{(L+1) \\times(L+1)}\n$$\n\nwhere $\\mathbb{I}_{L+1} \\in \\mathbb{R}^{(L+1) \\times(L+1)}$ is an identity matrix utilized to incorporate the influence of skip connections along the layers. Now, the per-layer global attention matrices $\\tilde{\\alpha}^{\\lambda}$ for all $\\lambda \\in[\\Lambda]$ are aggregated into the final map $\\rho$ by:\n\n$$\n\\rho=\\Pi_{\\lambda=1}^{\\Lambda} \\tilde{\\alpha}^{\\lambda}, \\quad \\rho \\in \\mathbb{R}^{(L+1) \\times(L+1)}\n$$\n\nNote that each row of $\\rho$ corresponds to a relevance map for each token, given the other tokens. In the context of this study, which concentrates on classification models, our attention analysis directs attention exclusively to the CLS token. Thus, we derive the final relevance map from the row associated with the CLS token in the output matrix, denoted by $\\rho_{\\mathrm{CLS}} \\in \\mathbb{R}^{L}$, which contains the relevance scores evaluating each token's influence on the classification token. Finally, to obtain the final explanation heatmap we reshape $\\rho_{\\mathrm{CLS}} \\in \\mathbb{R}^{L}$ to $\\sqrt{L} \\times \\sqrt{L}$ and upsample it back to the size of the original image using bilinear interpolation. Although Mamba models are causal by definition, resulting in causal hidden attention matrices, our method can be extended to a bidirectional setting in a straightforward manner. This adaptation involves modifying Eq. 20 so that $\\tilde{\\alpha}^{\\lambda, d}$ becomes the outcome of summing the (two) per-direction matrices of the $\\lambda$-layer and the $d$-channel. ### 3.3 Application to Attention-based Attribution\n\nAs our class-specific explainability technique for Mamba models, we have tailored the Transformer-Attribution [15] explainability method, which is specifically designed for transformers, to suit Mamba models. This method relies on a combination of LRP scores and attention gradients to generate the relevance scores. Since each Mamba block includes several peripheral layers that are not included in transformers, such as Conv1D, additional gating mechanisms, and multiple linear projection layers, a robust mechanism must be designed carefully. For simplicity, we focus on vision Mamba, with a grid of $\\sqrt{L}$ patches in each row and column, as in Sec. 3.2. The Transformer-Attribution method encompasses two stages: (i) generating a relevance map for each attention layer, followed by (ii) the aggregation of these relevance maps across all layers, using the aggregation rule specified in 21 , to produce the final map $\\rho$. The difference from the attention rollout method therefore lies in how step (i) is applied to each Mamba layer $\\lambda \\in[\\Lambda]$. For the $\\hat{h} \\in[H]$ attention head at layer $\\lambda$, the transformer method [15] computes the following two maps: (1) LRP [7] relevance scores map $R^{\\lambda, \\hat{h}}$, and (2) the gradients $\\nabla \\tilde{\\alpha}^{\\lambda, \\hat{h}}$ with respect to a target class of interest. Then, these two are fused by a Hadamard product:\n\n$$\n\\beta^{\\lambda}=\\mathbb{I}_{L}+\\underset{\\hat{h} \\in[\\hat{H}]}{\\mathbb{E}}\\left(\\nabla \\alpha^{\\lambda, \\hat{h}} \\odot R^{\\lambda, \\hat{h}}\\right)^{+}, \\quad \\mathbb{I}_{L+1} \\in \\mathbb{R}^{(L+1) \\times(L+1)}\n$$\n\nOur method, Mamba-Attribution, depicted in Fig. 2, deviates from this method by modifying Eq. 22 in the following aspects: (i) Instead of computing the\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d5bc810f0d7b57c18473g-09.jpg?height=356&width=657&top_left_y=386&top_left_x=474)\n\nFig. 2: Comperative Visualization of Transformer-Attribution and our MambaAttribution, both class specific methods. ![](https://cdn.mathpix.com/cropped/2024_09_12_d5bc810f0d7b57c18473g-09.jpg?height=376&width=490&top_left_y=387&top_left_x=1186)\n\nFig. 3: Average attention maps for CLS token in the middle $(a, b, c)$ and as the first $(d, e, f)$. gradients on the per-head attention matrices $\\nabla \\alpha^{\\lambda, \\hat{h}}$, we compute the gradients of $\\nabla \\hat{y}^{\\prime \\lambda, d}$. The motivation for these modifications is to exploit the gradients of both the S6 mixer and the gating mechanism in Eq. 8 (left), to obtain strong classspecific maps. (ii) We simply replace $R^{\\lambda, \\hat{h}}$ with the attention matrices $\\tilde{\\alpha}^{\\lambda, d}$ at layer $\\lambda$ and channel $d$, since we empirically observe that those attention matrices produce better relevance maps. Both of these modifications are manifested by the following form, which defines our method:\n\n$$\n\\tilde{\\beta}^{\\lambda}=\\mathbb{I}_{L}+\\left(\\underset{d \\in D}{\\mathbb{E}}\\left(\\nabla \\hat{y}^{\\prime \\lambda, d}\\right) \\odot \\underset{d \\in D}{\\mathbb{E}}\\left(\\tilde{\\alpha}^{\\lambda, d}\\right)\\right)^{+}\n$$\n\n## 4 Experiments\n\nIn this section, we present an in-depth analysis of the hidden attention mechanism embedded within Mamba models, focusing on its semantic diversity and applicability in explainable AI frameworks. We start by visualizing the hidden attention matrices for both NLP and vision models in Sec. 4.1, followed by assessing our explainable AI techniques empirically, via perturbation and segmentation tests in Sec. 4.2. ### 4.1 Visualization of Attention Matrices\n\nThe Visual Mamba (ViM) comes in two versions: in one, the CLS token is last and in the other, the CLS token is placed in the middle. Fig. 3 shows how this positioning influences the impact of the patches on the CLS, by averaging over the entire test set. Evidently, the patches near the CLS token are more influential. This phenomenon may suggest that a better strategy is to have a non-spatial/global CLS token [22,37]. Fig. 4 compares the attention matrices in Mamba and Transformer on both vision and NLP tasks. For clearer visualization, we apply the Softmax function to each row of the attention matrices obtained from transformers and perform minmax normalization on the absolute values of the Mamba matrices. In all cases, we\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d5bc810f0d7b57c18473g-10.jpg?height=813&width=1217&top_left_y=388&top_left_x=483)\n\nFig. 4: Hidden Attention Matrices: Attention matrices in vision and NLP Models.Each row represents a different layer within the models, showcasing the evolution of the attention matrices at $25 \\%$ (top), $50 \\%$, and $75 \\%$ (bottom) of the layer depth. limit our focus to the first 64 tokens. In vision, we compare Vision-Mamba (ViM) and ViT (DeiT), for models of a tiny size, trained on ImageNet-1K. The attention maps are extracted using examples from the test set. Each Mamba attention matrix is obtained by combining the two maps of the bidirectional channel. In NLP, we compare attention matrices extracted from Mamba (130m) and Transformer (Pythia-160m [11]) language models, trained on the Pile [25] dataset for next token prediction. The attention maps are extracted using examples from the Lambada dataset (preprocessed by OpenAI). As can be seen, the hidden attention matrices of Mamba appear to be similar to the attention matrices extracted from transformers In both models, the dependencies between distant tokens are captured in the deeper layers of the model, as depicted in the lower rows. Some of the attention matrices demonstrate the ability of selective SSM models and transformers to focus on parts of the input. In those cases, instead of the diagonal patterns, some columns seem to miss the diagonal element and the attention is more diffused (recall that we normalized the Mamba attention maps for visualization purposes. In practice, these columns have little activity). Evidently, both the Mamba attention matrices and the transformer attention matrices possess similar properties and depict the two-dimensional structure within the data as bands with an offset of $\\sqrt{L}$. ![](https://cdn.mathpix.com/cropped/2024_09_12_d5bc810f0d7b57c18473g-11.jpg?height=898&width=1275&top_left_y=385&top_left_x=442)\nFig. 5: Qualitative results for the different explanation methods for the ViT-small and the Mamba-small models. (a) the original image, (b) the aggregated Raw-Attention of ViT-Small, (c) Attention Rollout for ViT-Small, (d) Transformer-Attribution for ViTSmall, (e) the Raw-Attention of Mamba-Small, (f) Attention-Rollout of Mamba-Small and $(\\mathrm{g})$ the Mamba-Attribution method for the Mamba-Small model. ### 4.2 Explainability Metrics\n\nThe explainable AI experiments include three types of explainability methods: (1) Raw-Attention, which employs raw attention scores as relevancies. Our findings indicate that averaging the attention maps across layers yields optimal results. (2) Attn-Rollout [1] for Transformers, and its Mamba version, as depicted in Sec. 3.2. Finally, (3) The Transformer Attribution of Chefer et al. [14] and its Mamba Attribution counterpart, detailed in Sec.",
    "hiddenattnmamba-3": "3.3. Fig. 5 depicts the results of the six attribution methods on typical samples from the ImageNet test set. As can be seen, the Mamba-based heatmaps are often more complete than their transformer-based counterparts. The raw attention of Mamba stands out from the other five heatmaps, since it depicts activity across the entire image. However, the relevant object is highlighted. Next, we apply explainability evaluation metrics. These metrics allow one to compare different explainability methods that are applied to the same model. Applying them to compare different models is not meant to say that model X is more explainable than model Y. The main purpose is to show that the attention maps of Mamba we introduce are as useful as the attention maps of Transformers\nin terms of providing explainability. A secondary aim is to validate the feasibility of potential use for weakly supervised downstream tasks that require spatial location. Perturbation\n\nPerturbation Tests In this evaluation framework, we employ an input perturbation scheme to assess the efficacy of various explanation methods, following the approach outlined by $[14,15]$. These experiments are conducted under two distinct settings. In the positive perturbation scenario, a quality explanation involves an ordered list of pixels, arranged most-to-least relevant. Consequently, when gradually masking out the pixels of the input image, starting from the highest relevance to the lowest, and measuring the mean top-1 accuracy of the network, one anticipates a notable decrease in performance. Conversely, in the negative perturbation setup, a robust explanation is expected to uphold the accuracy of the model while systematically removing pixels, starting from the lowest relevance to the highest. In both cases, the evaluation metrics consider the area-under-curve (AUC) focusing on the erasure of $10 \\%$ to $90 \\%$ of the pixels. The results of the perturbations are presented in Tab. 1, depicting the performance of different explanation methods under both positive and negative perturbation scenarios across the two models. In the positive perturbation scenario, where lower AUC values are indicative of better performance, we notice that for Raw-Attention, Mamba shows a better AUC compared to the Vision Transformer (ViT). For the Attn-Rollout method, Mamba outperforms the ViT, while the latter shows a better AUC under the Attribution method. In the negative perturbation scenario, where higher AUC values are better, the Transformerbased methods consistently outperform Mammba across all three methods. The tendency for lower AUC in both positive (where it is desirable) and negative perturbation (where it is undesirable) may indicate that the Mamba model is more sensitive to blacking out patches, and it would be interesting to add experiments in which the patches are blurred instead [23]. For perturbation tests in the NLP domain, please refer to the Appendices B and C. Segmentation Tests It is expected that an effective explainability method would produce reasonable foreground segmentation maps. This is assessed for ImageNet classifiers by comparing the obtained heatmap against the ground truth segmentation maps available in the ImageNet-Segmentation dataset [33]. Evaluation is conducted based on pixel accuracy, mean-intersection-overunion ( mIoU ) and mean average precision ( mAP ) metrics, aligning with established benchmarks in the literature for explainability [14, 15, 36, 53]. The results are outlined in Tab. 2. For Raw-Attention, Mamba demonstrates significantly higher pixel accuracy and mean Intersection over Union compared to Vision Transformer, while the latter performs better in mean Average Precision. Under the Attn-Rollout method, Mamba outperforms Vision Transformer in mean Average Precision, pixel accuracy and mean Intersection over Union. Finally, Transformer-Attribution consistently surpasses Mamba-Attribution, achiev-\n\nTable 1: Positive and Negative perturbation AUC results (percentages) for the predicted class on the ImageNet validation set. For positive perturbation lower is better, and for negative perturbation higher is better. |  | Positive Perturbation |  |  | Negative Perturbation |  |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n|  | Mamba | Transformer |  | Mamba | Transformer |\n| Raw-Attention | 17.268 | 20.687 |  | 34.025 | 40.766 |\n| Attn-Rollout | 18.806 | 20.594 |  | 41.864 | 43.525 |\n| Attribution | 16.619 | $\\mathbf{1 5 . 3 5 1}$ |  | 39.632 | $\\mathbf{4 8 . 0 8 9}$ |\n\nTable 2: Segmentation performance on the ImageNet-Segmentation [33] dataset (percent). Higher is better. The upper part depicts the results for Vision Mamba-small while the lower part contains the results for Vision Transformer-Small\n\n| Model | Method | pixel accuracy | mAP | mIoU |\n| :--- | :--- | :---: | :---: | :---: |\n| Transformer | Raw-Attention | 59.69 | $\\mathbf{7 7 . 2 5}$ | 36.94 |\n| Mamba | Raw-Attention | $\\mathbf{6 7 . 6 4}$ | 74.88 | $\\mathbf{4 5 . 0 9}$ |\n| Transformer | Attn-Rollout [1] | 66.84 | 80.34 | 47.85 |\n| Mamba | Attn-Rollout (Sec.",
    "hiddenattnmamba-4": "3.2) | $\\mathbf{7 1 . 0 1}$ | $\\mathbf{8 0 . 7 8}$ | $\\mathbf{5 1 . 5 1}$ |\n| Transformer | Transformer-Attr [15] | $\\mathbf{7 9 .",
    "hiddenattnmamba-5": "2 6}$ | $\\mathbf{8 4 . 8 5}$ | $\\mathbf{6 0 . 6 3}$ |\n| Mamba | Mamba-Attr (Sec. 3.3) | 74.72 | 81.70 | 54.24 |\n\ning the highest scores in pixel accuracy, mean Average Precision, and mean Intersection over Union, respectively. These results underscore the potential of Mamba's attention mechanism as approaching and sometimes surpassing the interoperability level of Transformer models, especially when the attention maps are taken as is. It also highlights the applicability of Mamba models for downstream tasks such as weakly supervised segmentation. It seems, however, that the Mamba-based attribution model, which is modeled closely after the transformer method of Chefer et al. [15] may benefit from further adjustments. ## 5 Discussion: The Evolution of Attention in SSMs\n\nA natural question to ask is whether the attention perspective we exposed is unique to Selective SSM (the building block of Mamba), separating it from other SSMs. The answer is that Selective SSM, similar to transformers, contains a type of layer we call data-dependent non-diagonal mixer, which previous layers do not. In their seminal work, Poli et al. [59] claim that a crucial aspect of transformers is the existence of an expressive, data-controlled linear operator. Here, we focus on a more specific component, which is an expressive data-controlled linear non-diagonal mixer operator. This distinguishes between elementwise operators that act on the data associated with specific tokens (including MLP,\ngating mechanisms, and GLU activations [65]) and mixer operations that pool information from multiple tokens. The mixer components can further be divided into fixed, e.g., using pooling operators with fixed structure and coefficients, or data-dependent, in which the interactions between tokens are controlled by their input-dependent representations, e.g., self-attention. In appendix E we prove the following result, which sheds light on the gradual evolution of attention in SSM models. Theorem 1. (i) S4 [31], DSS [34], S5 [66] have fixed mixing elements. (ii) GSS [52], and Hyena [59] have fixed mixing elements with diagonal data-control mechanism. (iii) Selective SSM have data-controlled non-diagonal mixers. Transformers are recognized for their superior in-context learning (ICL) capabilities, where the model adapts its function according to the input provided [13]. Empirical evidence has demonstrated that Mamba models are the first SSMs to exhibit ICL capabilities on par with those of transformers [29,56]. Based on the intuition that the capacity to focus on specific inputs is required for ICL, we hypothesize that the presence of data-controlled non-diagonal mixers in both transformers and Mamba models is crucial for achieving a high level of ICL. A question then arises: which model is more expressive, transformers or selective SSM? While previous work has shown that Transformers are more expressive than traditional state-space layers [85], we show in Appendix D that the situation is reversed for selective SSMs, as follows:\nTheorem 2. One channel of the selective state-space layer can express all functions that a single transformer head can express. Conversely, a single Transformer layer cannot express all functions that a single selective SSM layer can. ## 6 Conclusions\n\nIn this work, we have established a significant link between Mamba and selfattention layers, illustrating that the Mamba layer can be reformulated as an implicit form of causal self-attention mechanism. This links the highly effective Mamba layers directly with the transformer layers. The parallel perspective plays a crucial role in efficient training and the recurrent perspective is essential for effective causal generation. The attention perspective plays a role in understanding the inner representation of the Mamba model. While \"Attention is not Explanation\" [39], attention layers have been widely used for transformer explainability. By leveraging the obtained attention matrices, we introduce the first (as far as we can ascertain) explainability techniques for Mamba models, for both task-specific and task-agnostic regimes. This contribution equips the research community with novel tools for examining the performance, fairness, robustness, and weaknesses of Mamba models, thereby paving the way for future improvements, and it also enables weakly supervised downstream tasks. Looking ahead, we plan to delve into the relationships between Mamba, Self-Attention and other recent layers, such as RWKV [57], Retention [67] and Hyena [59], and develop XAI methods for LLMs relying on these layers and their corresponding vision variants $[21,84]$. ## 7 Acknowledgments\n\nThis work was supported by a grant from the Tel Aviv University Center for AI and Data Science (TAD). This research was also supported by the Ministry of Innovation, Science \\& Technology ,Israel (1001576154) and the Michael J. Fox Foundation (MJFF-022407). The contribution of the first author is part of a PhD thesis research conducted at Tel Aviv University. ## References\n\n1. Abnar, S., Zuidema, W.: Quantifying attention flow in transformers. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4190-4197 (2020) 4, 5, 7, 11, 13\n2. Ahamed, M.A., Cheng, Q.: Mambatab: A simple yet effective approach for handling tabular data. arXiv preprint arXiv:2401.08867 (2024) 1\n3. Ali, A., Galanti, T., Wolf, L.: Centered self-attention layers. arXiv preprint arXiv:2306.01610 (2023) 7, 20, 21\n4. Ali, A., Schnake, T., Eberle, O., Montavon, G., M\u00fcller, K.R., Wolf, L.: Xai for transformers: Better explanations through conservative propagation.",
    "hiddenattnmamba-6": "In: International Conference on Machine Learning. pp. 435-451. PMLR (2022) 4, 21\n5. Anthony, Q., Tokpanov, Y., Glorioso, P., Millidge, B.: Blackmamba: Mixture of experts for state-space models. arXiv preprint arXiv:2402.01771 (2024) 1, 3\n6. Arras, L., Montavon, G., M\u00fcller, K.R., Samek, W.: Explaining recurrent neural network predictions in sentiment analysis. In: Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis.",
    "hiddenattnmamba-7": "pp. 159-168 (2017) 4\n7. Bach, S., Binder, A., Montavon, G., Klauschen, F., M\u00fcller, K.R., Samek, W.: On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one $\\mathbf{1 0}(7)$, e0130140 (2015) 4,8\n8. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014) 3\n9. Baron, E., Zimerman, I., Wolf, L.: 2-d ssm: A general spatial layer for visual transformers. arXiv preprint arXiv:2306.06635 (2023) 3\n10. Behrouz, A., Hashemi, F.: Graph mamba: Towards learning on graphs with state space models. arXiv preprint arXiv:2402.08678 (2024) 1, 3\n11. Biderman, S., Schoelkopf, H., Anthony, Q.G., Bradley, H., O\u2019Brien, K., Hallahan, E., Khan, M.A., Purohit, S., Prashanth, U.S., Raff, E., et al.: Pythia: A suite for analyzing large language models across training and scaling.",
    "hiddenattnmamba-8": "In: International Conference on Machine Learning. pp. 2397-2430. PMLR (2023) 10\n12. Blelloch, G.E.: Prefix sums and their applications. Technical Report (1990) 4\n13. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877-1901 (2020) 14\n14. Chefer, H., Gur, S., Wolf, L.: Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 397-406 (2021) 4, 11, 12\n15. Chefer, H., Gur, S., Wolf, L.: Transformer interpretability beyond attention visualization.",
    "hiddenattnmamba-9": "In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 782-791 (2021) $3,4,5,8,12,13,21$\n16. Chefer, H., Schwartz, I., Wolf, L.: Optimizing relevance maps of vision transformers improves robustness. Advances in Neural Information Processing Systems 35, $33618-33632(2022) 3$\n17. Chen, N., Shou, L., Pei, J., Gong, M., Cao, B., Chang, J., Li, J., Jiang, D.: Alleviating over-smoothing for unsupervised sentence representation. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 3552-3566 (Jul 2023) 20\n18. David, S.B., Zimerman, I., Nachmani, E., Wolf, L.: Decision s4: Efficient sequencebased rl via state spaces layers. In: The Eleventh International Conference on Learning Representations (2022) 3\n19. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth $16 \\times 16$ words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020) 4\n20. Dovonon, G.J., Bronstein, M.M., Kusner, M.J.: Setting the record straight on transformer oversmoothing. arXiv preprint arXiv:2401.04301 (2024) 20\n21. Fan, Q., Huang, H., Chen, M., Liu, H., He, R.: Rmt: Retentive networks meet vision transformers. arXiv preprint arXiv:2309.11523 (2023) 14\n22. Farooq, A., Awais, M., Ahmed, S., Kittler, J.: Global interaction modelling in vision transformer via super tokens. arXiv preprint arXiv:2111.13156 (2021) 9\n23. Fong, R.C., Vedaldi, A.: Interpretable explanations of black boxes by meaningful perturbation.",
    "hiddenattnmamba-10": "In: Proceedings of the IEEE international conference on computer vision.",
    "hiddenattnmamba-11": "pp. 3429-3437 (2017) 12\n24. Fu, D.Y., Dao, T., Saab, K.K., Thomas, A.W., Rudra, A., R\u00e9, C.: Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052 (2022) 3, 4\n25. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al.: The pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020) 10\n26. Goel, K., Gu, A., Donahue, C., R\u00e9, C.: It's raw! audio generation with state-space models. In: International Conference on Machine Learning. pp. 7616-7633. PMLR (2022) 3\n27. Gong, C., Wang, D., Li, M., Chandra, V., Liu, Q.: Vision transformers with patch diversification. arXiv preprint arXiv:2104.12753 (2021) 20\n28. Gong, H., Kang, L., Wang, Y., Wan, X., Li, H.: nnmamba: 3d biomedical image segmentation, classification and landmark detection with state space model. arXiv preprint arXiv:2402.03526 (2024) 1\n29. Grazzi, R., Siems, J., Schrodi, S., Brox, T., Hutter, F.: Is mamba capable of incontext learning? arXiv preprint arXiv:2402.03170 (2024) 14\n30. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023) 1, 3\n31. Gu, A., Goel, K., R\u00e9, C.: Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396 (2021) 3, 14, 27\n32. Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., R\u00e9, C.: Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems 34, 572-585 (2021) 1, 3\n33. Guillaumin, M., K\u00fcttel, D., Ferrari, V.: Imagenet auto-annotation with segmentation propagation. International Journal of Computer Vision 110, 328-348 (2014) 12,13\n34. Gupta, A., Gu, A., Berant, J.: Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems 35, 22982-22994 (2022) 14,27\n35. Gupta, A., Mehta, H., Berant, J.: Simplifying and understanding state space models with diagonal linear rnns. arXiv preprint arXiv:2212.00768 (2022) 25\n36. Gur, S., Ali, A., Wolf, L.: Visualization of supervised and self-supervised neural networks via attribution guided factorization.",
    "hiddenattnmamba-12": "In: Proceedings of the AAAI conference on artificial intelligence.",
    "hiddenattnmamba-13": "vol. 35, pp. 11545-11554 (2021) 4, 12\n37. Hatamizadeh, A., Yin, H., Heinrich, G., Kautz, J., Molchanov, P.: Global context vision transformers.",
    "hiddenattnmamba-14": "In: International Conference on Machine Learning. pp. 1263312646. PMLR (2023) 9\n38. Hua, W., Dai, Z., Liu, H., Le, Q.: Transformer quality in linear time.",
    "hiddenattnmamba-15": "In: International Conference on Machine Learning. pp. 9099-9117. PMLR (2022) 7\n39. Jain, S., Wallace, B.C.: Attention is not explanation.",
    "hiddenattnmamba-16": "In: Proceedings of NAACLHLT. pp. 3543-3556 (2019) 14\n40. Kulikov, I., Eremeev, M., Cho, K.: Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling. In: Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 1115-1124. Association for Computational Linguistics, Online only (Nov 2022) 20\n41. Li, S., Singh, H., Grover, A.: Mamba-nd: Selective state space modeling for multidimensional data. arXiv preprint arXiv:2402.05892 (2024) 1\n42. Liang, D., Zhou, X., Wang, X., Zhu, X., Xu, W., Zou, Z., Ye, X., Bai, X.: Pointmamba: A simple state space model for point cloud analysis. arXiv preprint arXiv:2402.10739 (2024) 1\n43. Liu, J., Yang, H., Zhou, H.Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang, S., Zheng, H., et al.: Swin-umamba: Mamba-based unet with imagenet-based pretraining. arXiv preprint arXiv:2402.03302 (2024) 1\n44. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024) 1, 3, 4\n45. Lu, C., Schroecker, Y., Gu, A., Parisotto, E., Foerster, J., Singh, S., Behbahani, F.: Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems 36 (2024) 3\n46. Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., Xu, C., Xiang, T., Zhang, L.: Soft: Softmax-free transformer with linear complexity. Advances in Neural Information Processing Systems 34, 21297-21309 (2021) 7\n47. Lutati, S., Zimerman, I., Wolf, L.: Focus your attention (with adaptive iir filters). arXiv preprint arXiv:2305.14952 (2023) 4\n48. Ma, J., Li, F., Wang, B.: U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722 (2024) 1\n49. Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., Zettlemoyer, L.: Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655 (2022) 7\n50. Martin, E., Cundy, C.: Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057 (2017) 4\n51. Mehta, H., Gupta, A., Cutkosky, A., Neyshabur, B.: Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947 (2022) 3\n52. Mehta, H., Gupta, A., Cutkosky, A., Neyshabur, B.: Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947 (2022) 14, 27\n53. Nam, W.J., Gur, S., Choi, J., Wolf, L., Lee, S.W.: Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks.",
    "hiddenattnmamba-17": "In: Proceedings of the AAAI conference on artificial intelligence.",
    "hiddenattnmamba-18": "vol. 34, pp. 2501-2508 (2020) 4, 12\n54. Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., R\u00e9, C.: S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in neural information processing systems 35, 2846-2861 (2022) 3\n55. Nguyen, T., Nguyen, T., Baraniuk, R.: Mitigating over-smoothing in transformers via regularized nonlocal functionals. Advances in Neural Information Processing Systems 36 (2024) 20\n56. Park, J., Park, J., Xiong, Z., Lee, N., Cho, J., Oymak, S., Lee, K., Papailiopoulos, D.: Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248 (2024) 14\n57. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K.K., et al.: Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048 (2023) 14\n58. Pi\u00f3ro, M., Ciebiera, K., Kr\u00f3l, K., Ludziejewski, J., Jaszczur, S.: Moe-mamba: Efficient selective state space models with mixture of experts. arXiv preprint arXiv:2401.04081 (2024) 1,3\n59. Poli, M., Massaroli, S., Nguyen, E., Fu, D.Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., R\u00e9, C.: Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866 (2023) 5, 13, 14, 27\n60. Romero, D.W., Kuzina, A., Bekkers, E.J., Tomczak, J.M., Hoogendoorn, M.: Ckconv: Continuous kernel convolution for sequential data. arXiv preprint arXiv:2102.02611 (2021) 27\n61. Ru, L., Zheng, H., Zhan, Y., Du, B.: Token contrast for weakly-supervised semantic segmentation.",
    "hiddenattnmamba-19": "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3093-3102 (2023) 20\n62. Ruan, J., Xiang, S.: Vm-unet: Vision mamba unet for medical image segmentation. arXiv preprint arXiv:2402.02491 (2024) 1\n63. Saon, G., Gupta, A., Cui, X.: Diagonal state space augmented transformers for speech recognition. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).",
    "hiddenattnmamba-20": "pp. 1-5. IEEE (2023) 3\n64. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Gradcam: Visual explanations from deep networks via gradient-based localization.",
    "hiddenattnmamba-21": "In: Proceedings of the IEEE international conference on computer vision. pp. 618-626 (2017) 4\n65. Shazeer, N.: Glu variants improve transformer. arXiv preprint arXiv:2002.05202 (2020) 14\n66. Smith, J.T., Warrington, A., Linderman, S.W.: Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933 (2022) 4, 14, 27\n67. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., Wei, F.: Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621 (2023) 14\n68. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks.",
    "hiddenattnmamba-22": "In: International conference on machine learning. pp. 3319-3328. PMLR (2017) 4\n69. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers \\& distillation through attention.",
    "hiddenattnmamba-23": "In: International conference on machine learning. pp. 10347-10357. PMLR (2021) 3\n70. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need.",
    "hiddenattnmamba-24": "In: Advances in Neural Information Processing Systems. vol. 30 (2017) 2\n71. Wang, C., Tsepa, O., Ma, J., Wang, B.: Graph-mamba: Towards long-range graph sequence modeling with selective state spaces. arXiv preprint arXiv:2402.00789 (2024) 1,3\n72. Wang, J., Zhu, W., Wang, P., Yu, X., Liu, L., Omar, M., Hamid, R.: Selective structured state-spaces for long-form video understanding.",
    "hiddenattnmamba-25": "In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.",
    "hiddenattnmamba-26": "pp. 63876397 (2023) 3\n73. Wang, J., Gangavarapu, T., Yan, J.N., Rush, A.M.: Mambabyte: Token-free selective state space model. arXiv preprint arXiv:2401.13660 (2024) 1, 3\n74. Wang, P., Zheng, W., Chen, T., Wang, Z.: Anti-oversmoothing in deep vision transformers via the fourier domain analysis: From theory to practice. arXiv preprint arXiv:2203.05962 (2022) 7, 20, 21\n75. Wang, Z., Ma, C.: Semi-mamba-unet: Pixel-level contrastive cross-supervised visual mamba-based unet for semi-supervised medical image segmentation. arXiv preprint arXiv:2402.07245 (2024) 1\n76. Wang, Z., Zheng, J.Q., Zhang, Y., Cui, G., Li, L.: Mamba-unet: Unet-like pure visual mamba for medical image segmentation. arXiv preprint arXiv:2402.05079 (2024) 1\n77. Wortsman, M., Lee, J., Gilmer, J., Kornblith, S.: Replacing softmax with relu in vision transformers. arXiv preprint arXiv:2309.08586 (2023) 7\n78. Xing, Z., Ye, T., Yang, Y., Liu, G., Zhu, L.: Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation. arXiv preprint arXiv:2401.13560 (2024) 1\n79. Yan, J.N., Gu, J., Rush, A.M.: Diffusion models without attention. arXiv preprint arXiv:2311.18257 (2023) 3\n80. Yang, Y., Xing, Z., Zhu, L.: Vivim: a video vision mamba for medical video object segmentation. arXiv preprint arXiv:2401.14168 (2024) 1\n81. Yuan, T., Li, X., Xiong, H., Cao, H., Dou, D.: Explaining information flow inside vision transformers using markov chain. In: eXplainable AI approaches for debugging and diagnosis. (2021) 4, 5\n82. Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., Wang, X.: Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417 (2024) $1,3,4$\n83. Zimerman, I., Baruch, M., Drucker, N., Ezov, G., Soceanu, O., Wolf, L.: Converting transformers to polynomial form for secure inference over homomorphic encryption. arXiv preprint arXiv: 2311.08610 (2023) 7\n84. Zimerman, I., Wolf, L.: Multi-dimensional hyena for spatial inductive bias. arXiv preprint arXiv:2309.13600 (2023) 14\n85. Zimerman, I., Wolf, L.: On the long range abilities of transformers. arXiv preprint arXiv:2311.16620 (2023) 14\n\n## A Oversmoothing\n\nOversmoothing poses a significant challenge for Transformer-based models [3,3, $20,27,55,61,74]$, affecting their ability to accurately capture and represent intricate features and relationships. In Transformer-based models, oversmoothing impacts tasks such as natural language processing $[17,40]$ and vision $[3,27,61]$, where it blurs fine-grained details essential for understanding and localization [3, $61]$. ![](https://cdn.mathpix.com/cropped/2024_09_12_d5bc810f0d7b57c18473g-20.jpg?height=641&width=1044&top_left_y=883&top_left_x=519)\n\nFig. 6: The inter cosine-similarity across the tokens for both DeiT and Vision-Mamba small models across the layers, results are averaged across the whole validation set of ImageNet. Recent transformer-based contributions [3, 74] on over-smoothing have developed various tools for measuring it, which we utilize when comparing transformers to Mamba-based architectures. Following Wang et al [74] we employ the cosine similarity metric, which operates on a layer index $l$ and its corresponding feature map $\\boldsymbol{X}^{l} \\in \\mathbb{R}^{L+1 \\times d}$, such that $L$ is the sequence length excluding the CLS token and $d$ is the feature dimensionality. The cosine similarity for $\\boldsymbol{X}^{l}$ is then defined as:\n\n$$\n\\boldsymbol{M}_{\\text {feat }}^{l}=\\frac{2}{n(n-1)} \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\frac{\\boldsymbol{X}_{i,:}^{(l) T} \\boldsymbol{X}_{j,}^{(l)}}{\\left\\|\\boldsymbol{X}_{i,:}^{(l)}\\right\\|_{2}\\left\\|\\boldsymbol{X}_{j,:}^{(l)}\\right\\|_{2}}\n$$\n\nwhere $\\boldsymbol{X}_{i,:}^{(l)}$ represents the $i$-th row (token) of $\\boldsymbol{X}^{(l)}$. In words, this measure calculates the average similarity between all token pairs except for self-similarity. The results are averaged across the entire ImageNet validation set. For further details, refer to [74]. Fig. 6 compares $\\boldsymbol{M}_{\\text {feat }}^{l}$ between transformers' attention and Mamba attention for different layers, where the layer index is normalized by the total number of layers. Evidently, there is a clear reduction in the inter-cosine similarity among tokens for all layers in the Mamba model (with 24 layers) compared to the Vision Transformer (12 layers). This variation can be attributed to the use of the softmax operator within the self-attention layer, a factor recognized as contributing to the oversmoothing problem $[3,74]$. ## B NLP Experiments\n\nIn this experiment, our aim is to extend the utilization of the proposed methods to the domain of Natural Language Processing (NLP). To achieve this, we conduct a comparative analysis between the Mamba-160M model and BERT-large, drawing upon established literature in the field $[4,15]$. Two settings are considered : (1) activation task, in this task, a good explanation involves listing tokens in order of their relevance, from most to least. When these tokens are added to an initially empty sentence, they should activate the network output as much and as quickly as possible. We evaluate the quality of explanations by observing the output probability $p_{c}(x)$ for the ground-truth class $c$. (2) pruning task, the pruning task involves removing tokens from the original sentence, starting with those deemed least relevant and progressing to the most relevant. We assess the impact of this pruning, by measuring the difference between the unpruned model's output logits $y_{0}$ and $y_{m t}$ of the pruned output. In the activation task, we begin with a sentence containing \" $<\\mathrm{UNK}>$ \" tokens and gradually replace them with the original tokens in order of highest to lowest relevance. Conversely, in the pruning task, we remove tokens from lowest to highest relevance by replacing them with \" $<\\mathrm{UNK}>$ \" tokens. The dataset employed in our study is the IMDb movie review sentiment classification dataset, consisting of 25,000 samples for training and an equal number for testing, with binary labels indicating sentiment polarity. We utilize the Mamba-130M ${ }^{2}$ and BERT $^{3}$ models fine-tuned on the IMDB dataset for classification. BERT stands out as our baseline choice, benefiting from a readily available implementation of the Transformer-Attr method ${ }^{4}$. Notably, both models exhibit comparable accuracy levels on the downstream task of IMDB movie review sentiment classification. The results, depicted in Fig. 7, illustrate that in both the pruning and activation tasks, Mamba-Attr exhibits comparable or occasionally superior performance to the Transformer-Attr method. We present the results of each method in separate graphs, as the two models are not directly comparable due to differences in the logit scale and the behavior on random changes to the prompt. [^1]![](https://cdn.mathpix.com/cropped/2024_09_12_d5bc810f0d7b57c18473g-22.jpg?height=1339&width=1181&top_left_y=417&top_left_x=476)\n\nFig. 7: Evaluation of explanations using input perturbations for the IMDb dataset, top row shows the results for the pruning task in which the words of least absolute relevance are replaced with $<\\mathrm{UNK}>$ first and the bottom row shows the results for the activation task in which the most relevant words are added first, in both tasks we show the results for Mamba-Attr and Transformer-Attr separately. In Sec C we provide qualitative results for the different explanation methods (Mamba-Attr and Transformer-Attr) on the IMDb dataset, for both positive (green) and negative (red) sentiments. Evidently, Mamba-Attr tends to generate more sparse explanations in comparison to its Transformer-Attr counterpart. For instance, in the analysis of the first negative sample, our method emphasizes the rating of \"1\" as the most salient feature along with other negative terms. Conversely, the transformer attribution method yields a less sparse explanation, focusing primarily on the relevant word while also encompassing other nonrelevant terms. Similarly, in the assessment of the third negative example, our method exhibits a comparable behavior, placing emphasis on the ratings alongside other relevant negative terms. Conversely, while the salient words identified by the transformer attribution method remain valid, its explanation is comparatively less sparse. We observe a similar trend across positive sentiments as well (depicted in green). For instance, in the final positive review, Mamba-Attr distinctly highlights the phrase \"Greatest Movie which ever made, \" serving as clear evidence of a positive sentiment. In contrast, the explanation provided by Trans-Attr appears more broad and encompassing. ## C NLP Qualitative Results\n\n## Mamba-Attr\n\nthis movie is so bad, I knew how it ends right\nafter this little girl killed the first person. Very bad\nacting very bad plot very bad movie do yourself a favour\nand DONT watch it $1 / 10$\nthis is definitely the worst movie Adam's ever done but at this point in his life, he was just happy to have a movie.",
    "hiddenattnmamba-27": "There are 3 or 4 laughs in\nit but I used the fast forward button through some of it but I used the fast forward butto\nit. Dont waste your time. I only saw it because I wanted to see all of his movies, but it sucked. Transformer-Attr\nthis movie is so bad, i knew how it ends right\nafter this little girl killed the first person. very bad acting very bad plot very bad movie do yourself a favour\nand don \\#\\# watch it $1 / 10$\nthis is definitely the worst movie adam 's ever done but at this point in his life, he was just happy in have a movie. there are 3 or 4 laughs of it. don \\#\\#t waste your time. i only saw it because i wanted to see all of his movies , but it sucked\nthis was an incredibly stupid movie. It was possibly the worst movie Ive ever had the displeasure of sitting through - I cannot fathom how it ranks a rating of 5 or $6 \\ldots \\ldots \\ldots . .$. ![](https://cdn.mathpix.com/cropped/2024_09_12_d5bc810f0d7b57c18473g-24.jpg?height=164&width=603&top_left_y=1067&top_left_x=468)\nfor long time I haven't seen such a good fantasy movie , magic fights here are even better than in LoTR\neven considering that it's a 1987 movie and haven\n't computer special effects. This movie have good plot good acting and interesting ideas. Recommend everybody to see it\nthere's never a dull moment in this movie. Wonderful visuals, good actors, and a classical story of the fight of good and evil. Mostly very funny , sometimes even scary. A true classic, a movie everybody should see. What can I say, it's a damn good movie. See it if you still haven't. Great camera works\nand les is ins certainly take the place of 'Citizen Kane'. i think it's one of the greatest movies which are ever made, and I've seen many... The book is\nbetter, but it's still a very good movie! this was an incredibly stupid movie. it was possibly the worst movie iv \\#\\#e ever had the displeasure of sitting through $i$ cannot fat \\#\\#hom how it ranks a rating of 5 or 6 .......... ... $i^{\\prime}$ ' ve seen this film because i had do ( my\njob includes seeing movies of all kinds). i couldn 't stop thinking \" who gave money to make such an awful film and also submit it to cannes festival!",
    "hiddenattnmamba-28": "\" it wasn ' t only boring, the actors were awful as well. it was one of the worst movies $i$ ' ve ever seen. for long time $i$ haven ' $t$ seen such a good fantasy movie, magic fights here are even better than in lo $\\# \\# r$, even considering that it 's a 1987 movie and haven ' $t$ computer special effects. this movie have good plot, good acting and interesting ideas. recommend everybody\nthere 's never a dull moment in this movie. wonderful visuals, good actors, and a classical story of the fight of good and evil. mostly very funny, sometimes even scary. a true classic, a movie everybody should see\nwhat can i say, it ' s a damn good movie\nsee it if you still haven ' $t$. great\ncamera works and lighting techniques.",
    "hiddenattnmamba-29": "awesome, just awesome. orson welles is incredible' the lady from shanghai ' can certainly take the place of 'citizen kane '\ni think it 's one of the greatest movies which are ever made, and $i$ ' ve seen many.. - the book is better, but it ' s still\na very good movie\n\n\n## D Expressiveness of Mamba Models\n\nTheorem 3. One channel of the selective state-space layer can express all functions that a single transformer head can express. Conversely, a single Transformer layer cannot express all functions that a single selective SSM layer can. Motivation and Intuition: The motivation for this proof relies on $\\tilde{H}_{i, j}$ in Eq. 19, which enables Mamba to utilize continuous historical context within\nsequences more efficiently than traditional attention mechanisms. To exploit this capability, we focus on a problem involving input-dependent control over the entire input, a task that cannot be captured by relying solely on pairwise interactions at single layer, which constitute the foundation of self-attention. ## Assumptions:\n\n1. For simplicity, we will disregard the discretization, as it has been shown to be unnecessary in previous work [35]. 2. As our regime focuses on real elements $\\left(x_{i} \\in \\mathbb{R}\\right)$, the hidden dimension of the transformer is 1 . Hence, the parameters of both the self-attention mechanism and the Mamba are scalars, namely $A_{i}, B_{i}, C_{i}, W^{Q}, W^{V}, W^{K} \\in \\mathbb{R}$\n\nProof.",
    "hiddenattnmamba-30": "Based on the definition of the count in row function, our proof straightforwardly arises from the following three lemmas:\n\nDefinition 1. The count in row problem: Given a binary sequence $x_{1}, x_{2}, \\ldots, x_{L}$ such that $x_{i} \\in\\{0,1\\}$ for all $i \\leq L$, the \"count in row\" function $f$ is defined to produce an output sequence $y_{1}, y_{2}, \\ldots, y_{L}$, where each $y_{i}$ is determined based on the contiguous subsequence of $1 s$ to which $x_{i}$ belongs. Formally:\n\n$$\ny_{i}=f\\left(x_{1}, \\ldots, x_{i}\\right)=\\max _{0 \\leq j \\leq i}\\left(\\left\\{i-j+1 \\mid \\prod_{k=j}^{i}\\left[x_{k}>0\\right]=1\\right\\} \\cup\\{0\\}\\right)\n$$\n\nwhere $\\left[x_{k}>0\\right]$ is the Iverson bracket, equaling 1 if $x_{k}>0$ and 0 otherwise. Lemma 1. One channel of Mamba can express the count in row function for sequences of any length. Proof. Assumption 1 defines the following recurrence rule:\n\n$$\n\\begin{gathered}\n\\bar{B}_{i}=S_{B}\\left(\\hat{x}_{i}\\right), \\quad C_{i}=S_{C}\\left(\\hat{x}_{i}\\right), \\quad \\bar{A}_{i}=S_{A}\\left(\\hat{x}_{i}\\right)+A \\\\\nh_{t}=\\bar{A}_{t} h_{t-1}+\\bar{B}_{t} x_{t}, \\quad y_{t}=C_{t} h_{t}\n\\end{gathered}\n$$\n\nBy substituting $S_{B}, S_{C}, S_{A}=1, A=0$ into Eq. 27 , we obtain the following results:\n\n$$\nh_{t}=h_{t-1}+x_{t}, \\quad y_{t}=h_{t}\n$$\n\nNow, there are two cases: (i) If $x_{i}=0$, it's clear that both the state $h_{t}$ and the output $y_{t}$ receive zero values. (ii) Otherwise (if $x_{i}=1$ ), we see that both $h_{t}$ and $y_{t}$ increase by one, clearly demonstrating that the entire mechanism exactly solves the count in row problem. Lemma 2. One transformer head cannot express the count in row function for sequences with more than two elements. Proof. The self-attention mechanism computes the output as follows\n\n$$\nO=\\operatorname{softmax}\\left(\\frac{\\left(X W^{Q}\\right)\\left(X W^{K}\\right)^{T}}{\\sqrt{d_{k}}}\\right) \\cdot\\left(X W^{V}\\right)\n$$\n\nConsider the count in row problem for a binary sequence of length 3 , the i-th coordinate in the output can be computed by:\n\n$$\nO_{i}=\\sum_{j=1}^{3}\\left(\\frac{\\exp \\left(\\left(W^{Q} \\cdot x_{i}\\right) \\cdot\\left(W^{K} \\cdot x_{j}\\right)\\right)}{\\sum_{k=1}^{3} \\exp \\left(\\left(W^{Q} \\cdot x_{i}\\right) \\cdot\\left(W^{K} \\cdot x_{k}\\right)\\right)}\\right) \\cdot\\left(W^{V} \\cdot x_{j}\\right)\n$$\n\nwhere we omitted the scale factor $\\sqrt{d_{k}}$ (which can be incorporated into the $W^{Q}$ matrix). For the sake of contradiction, we will assume that there are weights for the key, query, and value matrices that solve this problem. Furthermore, recall that $W^{Q}, W^{K}, W^{V} \\in \\mathbb{R}$, according to Assumption 2. Hence:\n\n1. For $\\left(x_{1}, x_{2}, x_{3}\\right)=(0,1,1)$, the output $y_{3}=2$. Plugging it into Eq. 30 yields:\n\n$$\nO_{3}=W^{V}\\left(\\frac{2 \\exp \\left(W^{Q} W^{K}\\right)}{1+2 \\exp \\left(W^{Q} W^{K}\\right)}\\right)=2\n$$\n\n2. For $\\left(x_{1}, x_{2}, x_{3}\\right)=(0,0,1)$, the output $y_{3}=1$. Plugging it into Eq. 30 yields:\n\n$$\nO_{3}=W^{V}\\left(\\frac{\\exp \\left(W^{Q} W^{K}\\right)}{2+\\exp \\left(W^{Q} W^{K}\\right)}\\right)=1\n$$\n\nDividing Eq. 31 by Eq. 32 results in the following:\n\n$$\n2 \\frac{2+\\exp \\left(W^{Q} W^{K}\\right)}{1+2 \\exp \\left(W^{Q} W^{K}\\right)}=2 \\quad \\rightarrow \\quad \\exp \\left(W^{Q} W^{K}\\right)=1\n$$\n\nUpon plugging it into the eq. 31, we obtained:\n\n$$\nO_{3}=W^{V} \\frac{2}{3}=2 \\quad \\rightarrow \\quad W^{V}=3\n$$\n\nHowever, for $\\left(x_{1}, x_{2}, x_{3}\\right)=(1,0,1)$, the output $y_{3}$ is 1 , by plugging it to eq. 30, and substituting the values of $W^{V}$ and $\\exp \\left(W^{Q} W^{K}\\right)$, we obtain:\n\n$$\nO_{3}=3 \\frac{2 \\exp \\left(W^{Q} W^{K}\\right)}{1+2 \\exp \\left(W^{Q} W^{K}\\right)}=2 \\neq 1\n$$\n\nAs requested.",
    "hiddenattnmamba-31": "Please note that the same technique also works when omitting the softmax function. Lemma 3. One channel of the selective state-space layer can express all functions that a single transformer head can express. Proof. For simplicity, we consider a causal attention variant without softmax, as the softmax is designed to normalize values rather than improve expressiveness. According to Assumption 1, we omit the discretization. Thus, we can simply set the value of $A_{i}$ to $\\mathbb{I}$ which is the identity, by substitute $A=\\mathbb{I}$ and $S_{A}=0$. Hence, it is clear that Eq. 11 and Eq. 12 become identical to causal attention, except for the softmax function. ## E Expressiveness of SSMs and Long-Convolution Layers\n\nTheorem 4. (i) S4 [31], DSS [34], S5 [66] have fixed mixing elements. (ii) GSS [52], and Hyena [59] have fixed mixing elements with diagonal data-control mechanism. (iii) Selective SSM have data-controlled non-diagonal mixers. Proof. We will prove this theorem separately per each layer:\nS4, DSS: Both layers implicitly parametrize a convolution kernel $\\bar{K}$ via the $A, \\bar{B}$ and $\\bar{C}$ matrices as follows:\n\n$$\n\\bar{K}=\\left(C \\bar{B}, C \\bar{A} \\bar{B}, \\cdots, C \\bar{A}^{L-1} \\bar{B}\\right)\n$$\n\nThis kernel does not depend on the input, and it is the only operation that captures interactions between tokens. Therefore, both layers have fixed elements. S5: The S5 layer extend S4 such that it map multi-input to multi-output rather than mapping single-input to single-output. It use the following recurrent rule:\n$h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t}, \\quad y_{t}=C h_{t}, \\quad \\bar{A} \\in \\mathbb{R}^{P \\times P}, \\quad \\bar{B} \\in \\mathbb{R}^{P \\times H}, C \\in \\mathbb{R}^{H \\times P}, x_{t}, y_{t} \\in \\mathbb{R}^{H}$\nwhich can be computed by\n\n$$\ny_{t}=C \\sum_{i=1}^{t} \\bar{A}^{t-i} \\bar{B} x_{t}\n$$\n\nHowever, in contrast to S4 and DSS, now $C \\bar{A}^{i} \\bar{B}$ in $\\mathbb{R}^{H \\times H}$ instead of in $\\mathbb{R}$. Hence, we can conclude that the mechanism mixes tokens in a fixed pattern, which is captured by $C \\sum_{i=1}^{t} \\bar{A}^{t-i} \\bar{B} x_{t}$. GSS: GSS enhances the DSS framework, which utilizes fixed mixing elements, by incorporating an elementwise gating mechanism. Hence, the entire layer can be viewed as a composition of two operators, a mixer that isn't data-dependent (DSS), and an elementwise data-dependent gating, which is equivalent to a diagonal data-control linear operator. Hyena: The Hyena layer is defined by the recurrence of two components: long implicit convolution and elementwise gating. For simplicity, we consider single recurrence steps to constitute the entire layer, since any layer can benefit from such a recurrent-based extension. Additionally, single recurrence is the most common application of the Hyena layer. Hence, similar to GSS, the layer can be viewed as a composition of a mixer that isn't data-dependent (based on CKConv [60]) and a diagonal data-control operator, which is implemented through elementwise data-dependent gating. Selective SSM: As can be seen in Eq. 10 and 19, the selective SSM can be represented by:\n\n$$\ny=\\tilde{\\alpha} x, \\quad \\tilde{\\alpha}_{i, j}=\\tilde{Q}_{i} \\tilde{H}_{i, j} \\tilde{K}_{j}\n$$\n\nThus, it's clear that the linear operator, which relies on $\\tilde{\\alpha}$, is a data-controlled, non-diagonal mixer. [^0]:    * These authors contributed equally to this work.",
    "hiddenattnmamba-32": "${ }^{1}$ https://github.com/AmeenAli/HiddenMambaAttn\n\n[^1]:    ${ }^{2}$ https://huggingface.co/trinhxuankhai/mamba_text_classification\n    ${ }^{3}$ https://huggingface.co/textattack/bert-base-uncased-imdb\n    ${ }^{4}$ https://github.com/hila-chefer/Transformer-Explainability\n\n"
}