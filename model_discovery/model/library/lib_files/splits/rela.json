{
    "rela-0": "# Sparse Attention with Linear Units \n\nBiao Zhang ${ }^{1} \\quad$ Ivan Titov $^{1,2} \\quad$ Rico Sennrich ${ }^{3,1}$<br>${ }^{1}$ School of Informatics, University of Edinburgh<br>${ }^{2}$ ILLC, University of Amsterdam<br>${ }^{3}$ Department of Computational Linguistics, University of Zurich<br>B.Zhang@ed.ac.uk, ititov@inf.ed.ac.uk, sennrich@cl.uzh.ch\n\n\n#### Abstract\n\nRecently, it has been argued that encoderdecoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants.",
    "rela-1": "In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off') for some queries, which is not possible with sparsified softmax alternatives. ${ }^{1}$\n\n\n## 1 Introduction\n\nAttention models (Bahdanau et al., 2015) have been hugely successful recently, with Transformer (Vaswani et al., 2017) in particular, advancing state of the art on various tasks, such as machine translation (Bojar et al., 2018), document summarization (Liu and Lapata, 2019) and speech processing (Chiu et al., 2018), and delivering a large impact on a broad range of NLP tasks via large-scale self-supervised pretraining (Devlin et al., 2019). [^0]![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-01.jpg?height=381&width=389&top_left_y=803&top_left_x=1073)\n(a) Attention with Softmax\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-01.jpg?height=430&width=375&top_left_y=753&top_left_x=1457)\n(b) Rectified Linear Attention\nFigure 1: Overview of the vanilla dot-product attention with softmax and the proposed Rectified Linear Attention (ReLA). Major differences highlighted in red. At the core of attention is a mechanism that dynamically highlights relevant context features for a given query input. In the vanilla softmax-based attention model (Vaswani et al., 2017, SMATt), this is achieved by imposing a categorical distribution constraint on the query-context relevance (i.e. attention) scores, implemented with the softmax activation (see Figure 1(a)). SMATT produces dense distributions, assigning some small amounts of attention even to irrelevant features. This complicates the analysis of the information flow in the model, and has led researchers to study sparse alternatives, which often lead to improved model performance and/or interpretability (Correia et al., 2019). Efforts in this category include designing fixed sparsity patterns (Raganato et al., 2020; Child et al., 2019) and creating sparsified softmax variants (Martins and Astudillo, 2016; Peters et al., 2019). However, these methods also have drawbacks. Fixed sparsity patterns lack flexibility and generalize poorly across tasks. Sparsified softmax variants often depend on complex inference algorithms (e.g., requiring the sorting operation), which reduces their efficiency. In this paper, we propose rectified linear attention (ReLA) to alleviate the above problems. ReLA\nuses ReLU rather than softmax as an activation function for attention scores, abandoning the probabilistic constraint. ${ }^{2}$ ReLU is inherently sparse since negative activations are dropped, and we will show that such sparse behaviour indeed emerges during training. In contrast to softmax activations, the output of ReLU can be any non-negative value, providing extra flexibility. To stabilize gradients and ease model convergence, we apply layer normalization together with a specialized initialization or a gating mechanism. Figure 1(b) shows ReLA, and also contrasts it with SMATT. ReLA is an easy-to-implement drop-in replacement for SMATT that requires no specialized operations or inference processes. Note that the behaviour of ReLA is data-driven, and it does not enforce a constant attention mass or sparsity level across queries, even allowing for null attention (all attention scores are zero) for some queries. We provide experimental results for ReLA with Transformer on five machine translation tasks, along with an in-depth analysis on WMT14 English-German task. Our contributions are summarized below:\n\n- We propose ReLA, a drop-in SMATT alternative, that learns sparse attention automatically with high flexibility and efficiency. - Experiments on five translation tasks show that ReLA achieves comparable translation performance, with similar training/decoding speed to SMATT, but is substantially faster than sparsified softmax baselines. - Our analysis shows that ReLA delivers high sparsity rate, high head diversity, and better accuracy than all baselines with respect to source-target word alignment. We also observe the emergence of attention heads with a high rate of null attention, only activating for certain queries. For some heads, this null rate can also indicate the quality of sentence pairs. ## 2 Related Work\n\nReLA ensures sparsity in attention. An alternative solution in this direction is to develop sparsified softmax alternatives, such as sparsemax (Martins and Astudillo, 2016; Malaviya et al., 2018), entmax (Peters et al., 2019; Correia et al., 2019), fusedmax (Niculae and Blondel, 2017), and\n\n[^1]hashing/clustering-based variants (Roy et al., 2020; Kitaev et al., 2020). These models often require dedicated algorithms for forward and backward propagation, at the cost of a significant computational overhead. Another strategy is to manually define sparse patterns inspired by task-specific attention analysis. Raganato et al. (2020) corroborated the feasibility of fixed patterns for Transformer encoder in translation. Child et al. (2019) introduced local and strided patterns to scale SMATT up to very long inputs. Unlike data-driven approaches, whether these patterns could generalize to different tasks and settings is still an open question. In contrast, ReLA is both data-driven and efficient. In this respect, our work shares similarity with the explicit sparse Transformer (Zhao et al., 2019) which also delivers faster speed but still depends on top- $k$ sorting as in sparsemax and entmax with $k$, a tunable hyperparameter. Note that all the above mentioned methods follow the categorical distribution constraint on attentions, while ReLA goes beyond. Thus, unlike ReLA, none of them enables null attentions. A different type of linear attention model is proposed by Katharopoulos et al. (2020) and Choromanski et al. (2020), who aim at reducing the $\\mathcal{O}\\left(n^{2}\\right)$ complexity in SMAtT. These models behave fundamentally differently from ReLA, because they eliminate the token-wise modeling rather than introducing sparsity. The explanatory power of standard attention weights is hotly debated (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019). Much of the criticism stems from the observation that low attention scores do not always imply irrelevance of the corresponding feature, as the information can still flow and its influence can be large (e.g., due to the large magnitude of the corresponding features). In contrast, sparse variants, including ReLA, assign exact zeroes, ensuring that the information flow from the corresponding features within the attention component is cut completely. Even with standard attention, prior studies show some evidence that attention partially reflects linguistic properties. In machine translation, the encoder-decoder attention captures the source-target word alignment to a certain degree (Ghader and Monz, 2017), with recent work further strengthening this via specific induction methods (Ding et al., 2019; Kobayashi et al., 2020; Chen et al., 2020). We apply analysis techniques from previous work to analyze our\nmodels. ## 3 Background: Attention in Transformer\n\nMany variants of attention mechanism have been developed since its first proposal (Bahdanau et al., 2015; Luong et al., 2015). In this paper, we focus on the one used by Transformer, namely multihead scaled dot-product attention (MHATT), in an encoder-decoder setup. Given query inputs $\\mathbf{X} \\in$ $\\mathbb{R}^{n \\times d}$ and a sequence of context items $\\mathbf{Y} \\in \\mathbb{R}^{m \\times d}$, each head in MHATT summarizes query-relevant context information as follows:\n\n$$\n\\begin{aligned}\n\\operatorname{SMATT}(\\mathbf{X}, \\mathbf{Y}) & =\\boldsymbol{\\alpha} \\mathbf{V}, \\\\\n\\text { with } \\quad \\boldsymbol{\\alpha} & =\\operatorname{Softmax}\\left(f\\left(\\mathbf{Q}, \\mathbf{K}^{T}\\right)\\right)\n\\end{aligned}\n$$\n\nwith $\\mathbf{Q}=\\mathbf{X W}_{q} ; \\mathbf{K}, \\mathbf{V}=\\mathbf{Y W}_{k}, \\mathbf{Y} \\mathbf{W}_{v}$, where $n$ and $m$ are the query and context length, respectively; $d$ and $d_{h}$ are the model and head dimension, respectively; $\\mathbf{W}_{*} \\in \\mathbb{R}^{d \\times d_{h}}$ denotes trainable model parameters. $\\alpha \\in \\mathbb{R}^{n \\times m}$ is the attention weight, which estimates the degree of relevance between one query input and each context. The softmax normalizes the scores and ensures that the attention weights $\\boldsymbol{\\alpha}$ define a categorical distribution. $f(\\cdot)$ is a scoring function. Different attention mechanisms make different choices for $f(\\cdot)$, but the use of softmax, or its sparsified variants, is universal. SMATt in Transformer adopts the scaled dot product for $f(\\cdot)$, which is further extended by MHATT to allow for parallel attentions in different sub-spaces over the same inputs:\n\n$$\n\\operatorname{MHATT}(\\mathbf{X}, \\mathbf{Y})=\\left[\\operatorname{SMATT}^{1}, \\ldots, \\operatorname{SMATT}^{H}\\right] \\mathbf{W}_{o}\n$$\n\nwhere $[\\cdot, \\cdot]$ denotes the concatenation operation, $H$ is the number of heads, $\\mathbf{W}_{o} \\in \\mathbb{R}^{H d_{h} \\times d}$ are output transformation parameters, and $d=H d_{h}$. In the encoder-decoder framework, MHATt is used in three different ways: Encoder Attention, Decoder Attention and Cross Attention, modeling intra-source, intra-target, and source-target dependencies, respectively. Transformer performs layered MHATT with residual connection and layer normalization (Ba et al., 2016) to handle variations of token-wise dependencies. The learning of MHATT is guided by the training objective, often without direct supervision. ## 4 Rectified Linear Attention\n\nWe argue that the use of the softmax function in SMATT (Eq. 1) has two undesirable consequences:\n\n- The attention mass is densely distributed over all context items, even the ones that are intuitively irrelevant. - The attention mass for each query is constant, although the relevance of context may vary. Both potentially hamper interpretability and even performance. ${ }^{3}$\n\nAs an alternative to sparsified softmax variants (Peters et al., 2019; Correia et al., 2019), we go one step further and consider whether the softmax, or broadly the categorical distribution, could be avoided completely. Model Structure We offer an answer to the question by proposing rectified linear attention (ReLA). ReLA abandons the distribution assumption and adopts linear activation instead. It is formulated as follows (see Figure 1(b) for illustration):\n\n$$\n\\begin{aligned}\n\\operatorname{ReLA}(\\mathbf{X}, \\mathbf{Y}) & =\\operatorname{LN}(\\boldsymbol{\\alpha} \\mathbf{V}) \\\\\n\\text { with } \\quad \\boldsymbol{\\alpha} & =\\operatorname{ReLU}\\left(f\\left(\\mathbf{Q}, \\mathbf{K}^{T}\\right)\\right)\n\\end{aligned}\n$$\n\nwhere $f(\\cdot)$ denotes any scoring function as in Eq. 1, $\\mathrm{LN}(\\cdot)$ denotes variants of layer normalization (Ba et al., 2016; Zhang and Sennrich, 2019), and $\\operatorname{ReLU}(\\cdot)=\\max (0, \\cdot)$ is the rectified linear unit. Note here, we describe our model by assuming only one attention head for clarity. In the multihead ReLA, we impose the normalization $\\operatorname{LN}(\\cdot)$ on the concatenated head representation rather than each single head separately. Unlike SMATT, ReLA prunes out all negative scores of low query-context relevance, automatically ensuring the sparse property of the attention weight $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{n \\times m}$. Besides, ReLA allows for null attention, where it assigns zero scores to all context items (i.e. some rows of $\\alpha$ are zero vectors), effectively switching off the corresponding attention head for certain queries. Nevertheless, the outputs of ReLU in Eq. 3 are often of different scales and varied variance, causing gradient instability and also optimization failure. Stabilization with Normalization A common strategy in deep learning to stabilize neuron activations is to apply layer normalization $\\mathrm{LN}(\\cdot)(\\mathrm{Ba}$\n\n[^2]et al., 2016). We follow this strategy and normalize each representation $\\mathbf{z} \\in \\mathbb{R}^{d_{h}}$ in the attention outputs $(\\boldsymbol{\\alpha} \\mathbf{V})$ with root mean square layer normalization (Zhang and Sennrich, 2019, RMSNorm):\n$$\n\\operatorname{LN}(\\mathbf{z})=\\operatorname{RMSNorm}(\\mathbf{z})=\\frac{\\mathbf{z}}{\\operatorname{RMS}(\\mathbf{z})} \\odot \\mathbf{g}\n$$\nwhere $\\odot$ denotes the element-wise multiplication, $\\mathrm{RMS}(\\cdot)$ calculates the root mean square statistic, and $\\mathrm{g} \\in \\mathbb{R}^{d_{h}}$ is the gain parameter, usually initialized at 1. We adopt RMSNorm rather than the vanilla LayerNorm (Ba et al., 2016) for ReLA because it avoids the re-centering constraint, being more flexible and computationally simpler. Although RMSNorm largely smooths gradients, our preliminary experiments show that ReLA still suffers from unstable gradients during early training, delivering suboptimal convergence. We propose two solutions, corresponding to two variants of ReLA, to solve this problem by down-scaling ReLA's activations. ReLA- $i$ changes the initialization of the gain parameter $g$ in RMSNorm with a uniform xavier initializer: $\\mathbf{g} \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{3}{d_{h}}}, \\sqrt{\\frac{3}{d_{h}}}\\right) .{ }^{4}$\n\nReLA- $g$ adds a simple gating function to the normalization:\n\n$$\n\\operatorname{LN}(\\mathbf{z})=\\sigma(\\mathbf{w} \\odot \\mathbf{z}) \\odot \\operatorname{RMSNorm}(\\mathbf{z})\n$$\n\nwhere $\\sigma(\\cdot)$ denotes the sigmoid function, and $\\mathbf{w} \\in \\mathbb{R}^{d_{h}}$ is a trainable parameter.",
    "rela-2": "We compare their performance in our experiments. The only overhead due to ReLA, compared to SMATT, is the added normalization layer and it is marginal. ReLA is a drop-in replacement of SMATT, and we apply it to Transformer for all three types of attention. ## 5 Experiments\n\nSettings We take machine translation as the testbed. We conduct experiments on five datasets of varied language pairs and training data sizes, including WMT14 English-German (Bojar et al., 2014, En-De, 4.5M training instances), WMT14 English-French (Bojar et al., 2014, En-Fr, 36M), WMT18 English-Finnish (Bojar et al., 2018, EnFi, 3.3M), WMT18 Chinese-English (Bojar et al.,\n\n[^3]| ID | Model | BLEU |\n| :--- | :--- | :---: |\n| 1 | Baseline (softmax) | $26.9(27.59)$ |\n| 2 | 1 + sparsemax | $26.4(27.02)$ |\n| 3 | $1+1.5$-entmax | $26.7(27.39)$ |\n| 4 | $1+$ ReLU alone | - |\n| 5 | $4+$ RMSNorm | $26.0(26.60)$ |\n| 6 | $1+$ ReLA- | $26.5(27.16)$ |\n| 7 | $1+$ ReLA- $g$ | $26.6(27.31)$ |\n| 8 | $7+$ LayerNorm | $26.6(27.18)$ |\n| 9 | $7+$ GeLU | $26.5(27.14)$ |\n| 10 | $7+$ Leaky ReLU | $26.5(27.13)$ |\n| 11 | $7+$ Encoder Attention Only | $26.3(27.00)$ |\n| 12 | $7+$ Decoder Attention Only | $27.0(27.70)$ |\n| 13 | $7+$ Cross Attention Only | $27.0(27.69)$ |\n\nTable 1: SacreBLEU (tokenized BLEU in brackets) for different models and settings on WMT14 En-De. GeLU: Gaussian error linear unit (Hendrycks and Gimpel, 2016); Leaky ReLU: leaky rectified linear unit (Xu et al., 2015).",
    "rela-3": "Baseline: Transformer. \"-\": optimization failed, where training loss didn't decrease. Higher BLEU indicates better result. 2018, Zh-En, 25M), and WMT16 RomanianEnglish (Bojar et al., 2016, Ro-En, 608K). We evaluate on the official test set from the corresponding year (e.g. newstest2014 for WMT14), and regard the previous year's test set as the development set (e.g. newstest2013 for WMT14). We preprocess all datasets using the byte pair encoding algorithm (Sennrich et al., 2016) with 32 K merging operations. We report detokenized case-sensitive BLEU (Papineni et al., 2002) implemented by SacreBLEU (Post, 2018), ${ }^{5}$ and also show tokenized case-sensitive BLEU with multi-bleu.perl for ablation studies. Model Configuration We use the Transformer base setting for experiments: model dimension $d=512$, head number $H=8$, head dimension $d_{h}=64,6$ layers and FFN size of 2048 (Vaswani et al., 2017). We apply dropout to the residual connections and attention weights, with a rate of 0.1 . We tune model parameters using Adam (Kingma and $\\mathrm{Ba}, 2015, \\beta_{1}=0.9, \\beta_{2}=0.98$ ) with label smoothing of 0.1 . We schedule the learning rate following Vaswani et al. (2017) with a warmup step of 4 K . Each training batch contains around 25K target tokens. For decoding, we average the last 5 checkpoints and adopt beam search with a beam size of 4 and length penalty of 0.6 . Apart from softmax-based SMATt, we consider two additional baselines: sparsemax (Martins and Astudillo, 2016) and 1.5-entmax (Peters\n\n[^4]| Model | \\#Params | $\\Delta$ Train | $\\Delta$ Decode |\n| :--- | :---: | :---: | :---: |\n| softmax | 72.31 M | $1.00 \\times$ | $1.00 \\times$ |\n| sparsemax | 72.31 M | $0.26 \\times$ | $0.54 \\times$ |\n| 1.5 -entmax | 72.31 M | $0.27 \\times$ | $0.49 \\times$ |\n| ReLA- $g$ | 72.34 M | $0.93 \\times$ | $0.98 \\times$ |\n\nTable 2: Number of parameters (\\#Params) and running efficiency for different models on WMT14 En-De.",
    "rela-4": "DTrain: speedup per training step measured on 500 steps with about 25 K target tokens per batch. $\\Delta$ Decode: translation speedup on newstest 2014 with a batch size of 1 . We perform 3 runs on a single GeForce GTX 1080 Ti and report average results for the speedups. Higher speedup indicates better efficiency. et al., 2019; Correia et al., 2019). ${ }^{6}$ We implement all models with Tensorflow (version 1.13.1). ### 5.1 Translation Results\n\nWe start with an ablation study for ReLA on WMT14 En-De. Results are given in Table 1. Ablation on ReLA's Architecture At the heart of ReLA is its replacement of softmax with ReLU. But simply applying ReLU to SMATt increases gradient instability, resulting in training failure (4). Applying layer normalization to the outputs of the attention model alleviates this problem, albeit sacrificing 0.9 detokenized BLEU ((1) $\\rightarrow$ (5)). By contrast, the proposed solutions, ReLA- $i$ and ReLA- $g$, yield a detokenized BLEU score of 26.5 (6)) and 26.6 ((7)) respectively, narrowing the quality gap against the baseline. ReLA- $g$ performs slightly better than ReLA- $i(+0.1$ detokenized BLEU) and on par with 1.5-entmax (-0.1 detokenized BLEU), partially due to the increased gating parameters. In the following experiments and analysis, we mainly report results with ReLA-g (i.e.",
    "rela-5": "(7)). ${ }^{7}$\n\nRMSNorm vs. LayerNorm Results show that replacing RMSNorm with LayerNorm $($ (7) $\\rightarrow$ (8) leads to no quality improvement ( -0.13 tokenized BLEU). We adopt RMSNorm for ReLA due to its efficiency. ReLU vs. its Variants We also attempted some smoothed variants of ReLU, such as GeLU (Hendrycks and Gimpel, 2016) and Leaky ReLU (Xu et al., 2015). Results show that these variants (9), (10) yield worse performance than ReLU ( -0.1 detokenized BLEU). Dropping those\n\n[^5]![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-05.jpg?height=372&width=475&top_left_y=248&top_left_x=1199)\n\nFigure 2: Decoding speedup as source length increases on WMT14 En-De.",
    "rela-6": "We divide the newstest2014 testset into 10 disjoint groups uniformly according to source length. Results are averaged over 3 runs. low-relevance attention scores, as ReLA does, benefits translation. ReLA for Different Attention Types By default, we apply ReLA to all attention sublayers. As shown in Section 3, Transformer includes three types of attentions with different functionalities. We study next how ReLA performs when applied to each attention separately. Results show that incorporating ReLA into the decoder self-attention (12) or encoder-decoder cross attention (13) yields quality gains over Baseline ( +0.1 detokenized BLEU). By contrast, only sparsifying encoder-side attentions with ReLA leads to a big quality reduction ( -0.6 detokenized BLEU). We argue that the encoder self-attention requires denser token-wise modeling to induce informative features of the source input for translation, compared to the other two attention types, echoing with the findings of Correia et al.",
    "rela-7": "(2019). Efficiency Analysis Table 2 shows the results. Sparsemax and 1.5-entmax run more than 3 and 1.8 times slower than Baseline (softmax) at training and decoding, respectively. We ascribe this to the dedicated inference procedure (such as sorting) both methods require in order to discover the best sparsity patterns (Peters et al., 2019), which reduces efficiency. By contrast, the computation in ReLA- $g$ is much simpler, and training and decoding speed is comparable to the baseline. Besides, we offer an analysis about the impact of source length on decoding speed. Curves in Figure 2 show consistent efficiency trend across different lengths: ReLA translates slightly slower than Baseline but at least 1.8 times faster than sparsemax and ci.s-entmax.",
    "rela-8": "We also notice that Correia et al. (2019) and Zhao et al. (2019) reported better training efficiency\n\n| Model | WMT14 En-Fr | WMT18 En-Fi | WMT18 Zh-En | WMT16 Ro-En |\n| :--- | :---: | :---: | :---: | :---: |\n| softmax | 37.2 | $\\mathbf{1 5 .",
    "rela-9": "5}$ | $\\mathbf{2 1 . 1}$ | 32.7 |\n| sparsemax | 37.3 | 15.1 | 19.2 | $\\mathbf{3 3 . 5}$ |\n| 1.5-entmax | $\\mathbf{3 7 .",
    "rela-10": "9}$ | $\\mathbf{1 5 . 5}$ | 20.8 | 33.2 |\n| ReLA- $g$ | $\\mathbf{3 7 . 9}$ | 15.4 | 20.8 | 32.9 |\n\nTable 3: SacreBLEU scores for different models on more WMT translation tasks. Best scores are highlighted in bold. ![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-06.jpg?height=340&width=1056&top_left_y=571&top_left_x=499)\n\nFigure 3: Average sparsity rate over heads at each layer for different attention models and types on the WMT14 En-De test set. Larger sparsity rate indicates that more attention scores are exactly zero. | Model | FLOPs |\n| :--- | :---: |\n| softmax | $3 H T^{2}-H T$ |\n| ReLA- $g$ | $H T^{2}+10 T d+T$ |\n\nTable 4: Comparison of FLOPs between softmax-based SMATT and ReLA- $g$.",
    "rela-11": "for sparsemax and 1.5-entmax than our results in Table 2. This is due to implementation difference. We re-tested the efficiency of different approaches using Pytorch with an in-house Transformer codebase, and worked with the official entmax implementation ${ }^{8}$. We observe that the training efficiency gap becomes much narrower, where sparsemax, 1.5-entmax and ReLA yield a speedup of $0.87 \\times$, $0.90 \\times$ and $0.95 \\times$, respectively. Although speedups vary across implementations, ReLA shows consistently higher computational efficiency than these sparsified softmax variants. Why ReLA- $g$ Is Slower Than Softmax\uff1fTable 2 and Figure 2 show that ReLA- $g$ runs slower than Baseline. This is because ReLA- $g$ is not just an activation function as in softmax. Apart from ReLU, ReLA- $g$ also includes a gated RMSNorm layer which brings in extra computational overhead. This becomes clearer as we show their FLOPs in Table 4, where $T$ denotes the sequence length. Take Transformer base $(H=8, d=512)$ as an example. For translation tasks where sequences often contain fewer than 100 tokens, the FLOPs of softmax is lower than that of ReLA-\n\n[^6]$g(239 K<592 K$, at $T=100)$. But ReLA$g$ has better scalability with respect to sequence length and would benefit long-sequence modeling (23.99M>13.12M, at $T=1000)$. Results for More Language Pairs Table 3 summarizes the results. Overall, performance of ReLA$g$ is competitive to the baseline, with BLEU differences ranging from -0.3 detokenized BLEU ( Zh En) to +0.7 detokenized BLEU (En-Fr), suggesting that ReLA generalizes to different (similar or distant) language pairs. Average performance is 0.5 detokenized BLEU higher than that of sparsemax, and 0.1 detokenized BLEU below that of 1.5-entmax. ### 5.2 Attention Analysis\n\nAlthough different sparsified SMATt models achieve comparable translation performance, their learned attention weights $\\boldsymbol{\\alpha}$ often have different characteristics. In this section, we quantitatively analyze these weights on WMT14 En-De. We first define layer attention, the weights averaged over the heads in one layer, to ease our following study. Besides, we obtain the word-level attention weights by merging its subword-level counterparts following Zenkel et al. (2019). We train each model three times with different random seeds on WMT14 EnDe and report the average results. Attention Sparsity The ability to automatically induce sparse attention is one of the key characteristics of ReLA. We next report the sparsity rates, i.e. the fraction of attention weights exactly equal\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-07.jpg?height=572&width=699&top_left_y=245&top_left_x=268)\n\nFigure 4: AER scores for different models with normal attention (left, $\\boldsymbol{\\alpha}$ ) and shifted attention (right, $\\boldsymbol{\\alpha}[1:]$ ). Solid curves correspond to the best head per layer; dashed curves are for layer attention. Lower AER score indicates better alignment quality. to 0 . We calculate the average sparsity rate over heads for each layer. Results are shown in Figure 3. We observe that the cross attention has the highest sparsity rate on average, resonating with the fact that word alignment is sparse. Self-attention at lower encoder/decoder layers often has a higher sparsity rate, particularly for sparsemax and 1.5-entmax. In ReLA- $g$, we find that its sparsity rate for the decoder self- and cross-attention tends to increase with layer depth, while that of the encoder selfattention fluctuates. Overall, ReLA- $g$ produces attentions of similar but slightly higher (lower) sparsity rate than 1.5-entmax (sparsemax), learned automatically without any constraint. Note softmaxbased SMATT only produces dense attentions, i.e., a sparsity rate of 0 . Cross Attention vs. Word Alignment We experiment with the publicly available De-En evaluation set $^{9}$ and evaluate the alignment quality with alignment error rate (Och and Ney, 2000, AER). We study normal attention and shifted attention following previous work (Chen et al., 2020; Kobayashi et al., 2020). The former explores attention weights corresponding to decoder outputs (i.e. $\\boldsymbol{\\alpha}$ in Eq. 1 and 3); the latter, by contrast, skips the weights at the first decoding step, i.e. $\\boldsymbol{\\alpha}[1:]$, to offset the left padding to the decoder inputs made for auto-regressive generation in Transformer. Figure 4 shows the results. Regardless of the attention type (normal or shifted), attention re-\n\n[^7]| Model | Normal Attention |  | Shifted Attention |  |\n| :--- | :---: | :---: | :---: | :---: |\n|  | AoL | AoH | AoL | AoH |\n| softmax | 75.95 | 67.51 | 79.38 | 54.31 |\n| sparsemax | 78.17 | 67.88 | 82.32 | 54.95 |\n| 1.5-entmax | 78.82 | 67.69 | 79.84 | 58.98 |\n| ReLA- $g$ | $\\mathbf{6 4 .",
    "rela-12": "4 6}$ | $\\mathbf{6 1 .",
    "rela-13": "6 4}$ | $\\mathbf{5 9 . 2 4}$ | $\\mathbf{5 2 . 4 1}$ |\n\nTable 5: Average AER scores over layers for different models. AoL: average for layer attention; AoH : average for the best head. Best scores are highlighted in bold. | Model | Enc | Dec | Cross |\n| :--- | :---: | :---: | :---: |\n| softmax | 0.56 | 0.31 | 0.45 |\n| 1.5 -entmax | 0.84 | 0.39 | 0.52 |\n| ReLA- $g(\\tau=1.00)$ | 1.26 | 0.81 | 0.78 |\n| ReLA- $g(\\tau=0.50)$ | 0.97 | 0.65 | 0.69 |\n| ReLA- $g(\\tau=0.25)$ | 0.92 | 0.65 | 0.71 |\n| ReLA- $g(\\tau=0.10)$ | 0.92 | 0.66 | 0.72 |\n\nTable 6: Average head diversity score over layers for different models.",
    "rela-14": "Enc: encoder attention; dec: decoder attention; cross: cross attention. $\\tau$ : temperature used for the re-normalization. sembles alignments more at some middle layer of Transformer; and the shifted attention overall performs better than the normal attention, echoing previous findings (Chen et al., 2020; Kobayashi et al., 2020). When considering the best AER head per layer, we observe that ReLA- $g$ generally obtains the (near-)best AER at each layer index for both normal and shifted attention. This becomes more obvious for the layer attention (bottom figures). Results in Table 5 further show that the behaviour of $\\mathrm{ReLA}-g$ is more alignment-like than the baselines we consider. Head Diversity We evaluate head diversity with a generalization of Jensen-Shannon divergence following (Correia et al., 2019) to reflect disagreements between heads. For ReLA- $g$, we renormalize its attention scores via softmax, and regard the null attention as a special one-hot distribution putting all probability mass to a dummy zero vector, i.e.",
    "rela-15": "entropy of 0. Figure 5 shows the results. We observe that the heads of the encoder self-attention exhibit much higher disagreement than those of the other two attention types for all sparsified attention models. Overall, heads in ReLA- $g$ are in less agreement than with the sparsified softmax alternatives, in most cases across different attention types. This indicates that ReLA- $g$ is capable of inducing heads with different roles (Voita et al., 2019). Note we convert the attention scores of ReLA- $g$ into categorical distribution via softmax for diver-\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-08.jpg?height=348&width=1056&top_left_y=245&top_left_x=498)\n\nFigure 5: Jensen-Shannon (JS) Divergence over heads at each layer for different attention models and types on the WMT14 En-De test set. Higher JS Divergence indicates higher head diversity. ![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-08.jpg?height=367&width=472&top_left_y=776&top_left_x=381)\n\nFigure 6: Average null rate over heads at each layer for different attention types in ReLA- $g$ on the WMT14 En-De test set. Null attention corresponds to attention of all zero scores. Dashed curves stand for the results on the hallucinated test set where target sentences are randomly shuffled. Hallucination pairs are assigned with significantly higher null rate for the cross-attention across different layers.",
    "rela-16": "sity evaluation. Such re-normalization might have a large impact on the final diversity results. We next explore this impact by adding some temperatures $(\\tau)$ to $\\boldsymbol{\\alpha}$, i.e. $\\boldsymbol{\\alpha}^{\\tau}$ (in Eq. 3) before applying softmax. Smaller temperatures will enforce smoothness into the output distribution, so alleviating the emergence of peaked distributions. Table 6 shows the results. The temperature indeed affects the diversity results but does not eliminate the diversity gap, and the diversity of ReLA gradually converges as $\\tau$ goes smaller. Null Attention One important feature distinguishing ReLA from (sparsified) softmax is that ReLA allows for null attention where all keys are assigned an attention weight of 0 , effectively deactivating the attention head for this query. Figure 6 analyzes the null rate of different attention types, i.e. the fraction of query tokens associated with all zero attention scores. Note all softmax-based variants have a null rate of about or exactly 0. We find that the encoder self-attention has few null attentions, suggesting that encoder prefers denser connections and also compact information flow. The decoder self-attention yields more null attentions for deeper layers. Together with the findings from Figure 3, it shows that the lower decoder self-attention layers model dense dependency with previous target tokens, while the dependencies in the upper ones become sparser. The cross-attention shows the most interesting phenomenon: an obvious peak at the middle layer. Attention at this layer shows the highest sparsity (Figure 3) with a large null rate variance of 0.256 (over heads), low head disagreement (Figure 5), but best AER score (right, bottom figure in Figure 4). Notice that attentions in ReLA- $g$ are of high diversity. The layer attention at each layer has almost no null attentions. Diving deeper into these null attentions as shown in Figure 7, we observe diverse specializations: head 0 and 7 capture source-target alignments with varying degrees of sparsity; head 2 has a null rate of $83 \\%$, and tends to fire after producing a verb (null rate $0 \\%$ after AUX, $23 \\%$ after VERB), attending tokens in the corresponding clause; head 3 has a null rate of $95 \\%$, but regularly fires after comma (null rate $0.03 \\%$ ), attending to the relevant source context (the clause boundary has said that in this example). Extra attention matrices are shown in Appendix A. Is Null Attention Meaningful? Apart from heads that have learned some sparse specialization, we also find that null attention can be informative for some cross-attention heads which learn an alignment. Specifically, the null rate increases for sentence pairs of low quality where many target tokens lack relevant source translations (see Appendix B). In order to quantify this effect, we create a hallucinated test set with target references randomly shuffled for comparison. The dashed curves in Figure 6 show that ReLA- $g$ associates such hallucination pairs with clearly higher null rate for the cross-attention across different layers. We next average the null rate of the crossattention over layers and utilize this metric to rank\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-09.jpg?height=361&width=1335&top_left_y=242&top_left_x=361)\n\nFigure 7: Null-attention examples for head $(0,2,3,7)$ at the 3rd cross attention layer.",
    "rela-17": "This example comes from the WMT14 En-De test set. Different heads show different linguistic patterns. the WMT14 En-De training corpus (top ranked samples have lower null rate). We randomly sample 100 cases from the top 10 K candidates, and another 100 from the bottom 10 K for manual analysis. We observe clear quality difference between these two groups: sentence pairs with a low null rate are predominantly good translations ( $\\sim 95 \\%$ correct), whereas sentence pairs with a high null rate are predominantly mistranslations ( $\\sim 1 \\%$ correct). Bad translations include sentence pairs with the wrong output language and semantically mismatched sentence pairs. Most interestingly, this null rate metric is sensitive to insertion errors, which are difficult to detect via traditional corpus filtering methods. We note previous work that used attention statistics to identify mistranslations (Rikters and Fishel, 2017), but find null attention more easily interpretable than more subtle changes in attention distribution. ## 6 Conclusion and Future Work\n\nIn this paper, we have presented rectified linear attention (ReLA), a novel softmax-free sparse attention model. ReLA avoids the categorical distribution assumption for attention, and, due to using ReLU as activation function, prunes out all negative attention scores and produces sparse attention. To stabilize model training, we add a normalization layer to attention outputs with a specialized initialization or gating structure. ReLA is datadriven, computationally efficient and is a drop-in replacement of SMATT. Experiments on five machine translation tasks with Transformer demonstrate ReLA's effectiveness in delivering comparable translation quality to softmax-based baselines. Results also show that ReLA is substantially faster than sparsemax and 1.5-entmax at training and decoding. The attentions learned by ReLA correspond better to word alignment, with high head diversity and sparsity rate. Also, in contrast to alternative sparse attention approaches, ReLA pro- duces null attentions, i.e. a head can assign a total attention of zero for some queries, leading to highly specialized attention heads and showing potential to indicate translation quality. In the future, we will apply ReLA to other neural models and tasks. We are interested in scaling ReLA to very long inputs (Child et al., 2019; Kitaev et al., 2020), or multi-source architectures where the relevance of each source may vary. In its current formulation, the sparsity level of ReLA emerges from the threshold in ReLU which prunes negative scores. We are interested in ways to manipulate the level of sparsity, or make the threshold differentiable. ## Acknowledgments\n\nWe thank the reviewers for their insightful feedback. Rico Sennrich acknowledges support of the Swiss National Science Foundation (MUTAMUR; no. 176727). ## References\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale\u0161 Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12-58, Baltimore, Maryland, USA. Association for Computational Linguistics. Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aur\u00e9lie N\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131-198, Berlin, Germany. Association for Computational Linguistics. Ond\u0159ej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, and Christof Monz. 2018. Findings of the 2018 conference on machine translation (WMT18). In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 272-303, Belgium, Brussels. Association for Computational Linguistics. Yun Chen, Yang Liu, Guanhua Chen, Xin Jiang, and Qun Liu. 2020. Accurate word alignment induction from neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 566-576, Online. Association for Computational Linguistics. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509. Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, and Michiel Bacchiani. 2018. State-of-the-art speech recognition with sequence-to-sequence models. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4774-4778. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint arXiv:2009.14794. Gon\u00e7alo M. Correia, Vlad Niculae, and Andr\u00e9 F. T. Martins. 2019. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 21742184, Hong Kong, China. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics. Shuoyang Ding, Hainan Xu, and Philipp Koehn. 2019. Saliency-driven word alignment interpretation for neural machine translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 1-12, Florence, Italy. Association for Computational Linguistics. Hamidreza Ghader and Christof Monz. 2017. What does attention in neural machine translation pay attention to? In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 30-39, Taipei, Taiwan. Asian Federation of Natural Language Processing. Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus).",
    "rela-18": "arXiv preprint arXiv:1606.08415. Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3543-3556, Minneapolis, Minnesota. Association for Computational Linguistics. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning. Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International Conference on Learning Representations. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In International Conference on Learning Representations. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention is not only a weight: Analyzing transformers with vector norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7057-7075, Online. Association for Computational Linguistics. Yang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730-3740, Hong Kong, China. Association for Computational Linguistics. Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural Language Processing, pages 1412-1421, Lisbon, Portugal. Association for Computational Linguistics. Chaitanya Malaviya, Pedro Ferreira, and Andr\u00e9 F.",
    "rela-19": "T. Martins. 2018. Sparse and constrained attention for neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 370-376, Melbourne, Australia. Association for Computational Linguistics. Andre Martins and Ramon Astudillo. 2016. From softmax to sparsemax: A sparse model of attention and multi-label classification.",
    "rela-20": "In International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 16141623, New York, New York, USA. PMLR. Vlad Niculae and Mathieu Blondel. 2017. A regularized framework for sparse and structured neural attention.",
    "rela-21": "In Advances in Neural Information Processing Systems, volume 30, pages 3338-3348. Curran Associates, Inc. Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440-447, Hong Kong. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Ben Peters, Vlad Niculae, and Andr\u00e9 F.",
    "rela-22": "T. Martins. 2019. Sparse sequence-to-sequence models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 15041519, Florence, Italy. Association for Computational Linguistics. Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186191, Belgium, Brussels. Association for Computational Linguistics. Alessandro Raganato, Yves Scherrer, and J\u00f6rg Tiedemann. 2020. Fixed encoder self-attention patterns in transformer-based machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 556-568, Online. Association for Computational Linguistics. Mat\u012bss Rikters and Mark Fishel.",
    "rela-23": "2017. Confidence Through Attention. In Proceedings of the 16th Machine Translation Summit (MT Summit 2017), Nagoya, Japan.",
    "rela-24": "Aurko Roy, Mohammad Taghi Saffar, David Grangier, and Ashish Vaswani. 2020. Efficient content-based sparse attention with routing transformers. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin.",
    "rela-25": "2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R.",
    "rela-26": "Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998-6008. Curran Associates, Inc. Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. Context-aware neural machine translation learns anaphora resolution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1264-1274, Melbourne, Australia. Association for Computational Linguistics. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797-5808, Florence, Italy. Association for Computational Linguistics. Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP$I J C N L P)$, pages $11-20$, Hong Kong, China. Association for Computational Linguistics. Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. 2015. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853. Thomas Zenkel, Joern Wuebker, and John DeNero. 2019. Adding interpretable attention to neural translation models improves word alignment. arXiv preprint arXiv:1901.11359. Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. In Advances in Neural Information Processing Systems, volume 32, pages 12381-12392. Curran Associates, Inc. Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, and Xu Sun. 2019. Explicit sparse transformer: Concentrated attention through explicit selection. arXiv preprint arXiv:1912.11637. A Examples for Null Attention\nB Null Attention for Diverse-quality Examples\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-13.jpg?height=2088&width=1320&top_left_y=382&top_left_x=371)\n\nFigure 8: Null-attention examples at the 3rd cross attention layer. All examples come from the WMT14 En-De test set. ![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-14.jpg?height=1049&width=1595&top_left_y=307&top_left_x=240)\n\nFigure 9: Null-attention examples for multi-clause sentences at the 3rd cross attention layer. All examples come from the WMT14 En-De test set. ![](https://cdn.mathpix.com/cropped/2024_09_12_43b07a423ece2f5eeb81g-14.jpg?height=854&width=1663&top_left_y=1616&top_left_x=242)\n\nFigure 10: Null-attention at the 3rd cross attention layer for the 7 -th head. Example comes from the WMT14 En-De training corpus. Top row shows high-quality examples, while bottom row shows low-quality ones. Low-quality examples include insertion errors and mistranslations, which increase the null rate. [^0]:    ${ }^{1}$ Source code is available at https: / / github.com/ bzhangGo/zero. [^1]:    ${ }^{2}$ Note that sparsified softmax variants also use some form of ReLU to achieve sparsity, but they stick to the probabilistic constraint which demands extra complexity. [^2]:    ${ }^{3}$ As an anecdotal example, Voita et al. (2018) performed an analysis of attention to previous sentences in MT, and found that the model has learned to generally attend to the end-ofsentence symbol as a way to ignore context. While this might be an effective strategy for instances where context matters little, this reduces the interpretability of attention. [^3]:    ${ }^{4}$ Note here we set fan $_{\\text {in }}=$ fan $_{\\text {out }}=d_{h}$. [^4]:    ${ }^{5}$ Signature: BLEU+c.mixed+\\#.1+s.exp+tok.13a+v.1.4.2\n\n[^5]:    ${ }^{6}$ https://gist.github.com/ justheuristic/60167e77a95221586be315ae527c3\n    ${ }^{7}$ Note we apply ReLA- $g$ to all attention sublayers so as to avoid the interference of other attention variants. This allows us to fully examine the effectiveness of ReLA. [^6]:    ${ }^{8}$ Available at https://github.com/deep-spin/ entmax. [^7]:    ${ }^{9}$ https://www-i6.informatik.rwth-aachen. de/goldAlignment/\n\n"
}