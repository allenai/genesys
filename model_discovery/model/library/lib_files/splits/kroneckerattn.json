{
    "kroneckerattn-0": "# Kronecker Attention Networks \n\nHongyang Gao<br>Texas A\\&M University<br>College Station, TX<br>hongyang.gao@tamu.edu\n\nZhengyang Wang<br>Texas A\\&M University<br>College Station, TX<br>zhengyang.wang@tamu.edu\n\nShuiwang Ji<br>Texas A\\&M University<br>College Station, TX<br>sji@tamu.edu\n\n\n#### Abstract\n\nAttention operators have been applied on both 1-D data like texts and higher-order data such as images and videos.",
    "kroneckerattn-1": "Use of attention operators on high-order data requires flattening of the spatial or spatial-temporal dimensions into a vector, which is assumed to follow a multivariate normal distribution. This not only incurs excessive requirements on computational resources, but also fails to preserve structures in data. In this work, we propose to avoid flattening by assuming the data follow matrix-variate normal distributions. Based on this new view, we develop Kronecker attention operators (KAOs) that operate on high-order tensor data directly. More importantly, the proposed KAOs lead to dramatic reductions in computational resources. Experimental results show that our methods reduce the amount of required computational resources by a factor of hundreds, with larger factors for higher-dimensional and higher-order data. Results also show that networks with KAOs outperform models without attention, while achieving competitive performance as those with original attention operators. ## CCS CONCEPTS\n\n- Computing methodologies $\\rightarrow$ Artificial intelligence; Machine learning algorithms; Neural networks. ## KEYWORDS\n\nAttention, neural networks, Kronecker attention, image classification, image segmentation\n\n## ACM Reference Format:\n\nHongyang Gao, Zhengyang Wang, and Shuiwang Ji. 2020. Kronecker Attention Networks. In Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining USB Stick (KDD '20), August 2327, 2020, Virtual Event, USA. ACM, New York, NY, USA, 9 pages. https: //doi.org/10.1145/3394486.3403065\n\n## 1 INTRODUCTION\n\nDeep neural networks with attention operators have shown great capability of solving challenging tasks in various fields, such as natural language processing $[3,19,33]$, computer vision $[26,38]$, and network embedding $[9,34]$. Attention operators are able to capture long-range dependencies, resulting in significant performance boost [23, 27]. While attention operators were originally proposed\n\n[^0]for 1-D data, recent studies $[7,35,40]$ have attempted to apply them on high-order data, such as images and videos. However, a practical challenge of using attention operators on high-order data is the excessive requirement computational resources, including computational cost and memory usage. For example, for 2-D image tasks, the time and space complexities are both quadratic to the product of the height and width of the input feature maps. This bottleneck becomes increasingly severe as the spatial or spatialtemporal dimensions and the order of input data increase. Prior methods address this problem by either down-sampling data before attention operators [35] or limiting the path of attention [16]. In this work, we propose novel and efficient attention operators, known as Kronecker attention operators (KAOs), for highorder data. We investigate the above problem from a probabilistic perspective. Specifically, regular attention operators flatten the data and assume the flattened data follow multivariate normal distributions. This assumption not only results in high computational cost and memory usage, but also fails to preserve the spatial or spatial-temporal structures of data. We instead propose to use matrix-variate normal distributions to model the data, where the Kronecker covariance structure is able to capture relationships among spatial or spatial-temporal dimensions. Based on this new view, we propose our KAOs, which avoid flattening and operate on high-order data directly. Experimental results show that KAOs are as effective as original attention operators, while dramatically reducing the amount of required computational resources. In particular, we employ KAOs to design a family of efficient modules, leading to our compact deep models known as Kronecker attention networks (KANets). KANets significantly outperform prior compact models on the image classification task, with fewer parameters and less computational cost. Additionally, we perform experiments on image segmentation tasks to demonstrate the effectiveness of our methods in general application scenarios. ## 2 BACKGROUND AND RELATED WORK\n\nIn this section, we describe the attention and related non-local operators, which have been applied on various types of data such as texts, images and videos. ### 2.1 Attention Operator\n\nThe inputs to an attention operator include a query matrix $Q=$ $\\left[\\mathbf{q}_{1}, \\mathbf{q}_{2}, \\cdots, \\mathbf{q}_{m}\\right] \\in \\mathbb{R}^{d \\times m}$ with each $\\mathbf{q}_{i} \\in \\mathbb{R}^{d}$, a key matrix $K=$ $\\left[\\mathbf{k}_{1}, \\mathbf{k}_{2}, \\cdots, \\mathbf{k}_{n}\\right] \\in \\mathbb{R}^{d \\times n}$ with each $\\mathbf{k}_{i} \\in \\mathbb{R}^{d}$, and a value matrix $\\boldsymbol{V}=\\left[\\mathbf{v}_{1}, \\mathbf{v}_{2}, \\cdots, \\mathbf{v}_{n}\\right] \\in \\mathbb{R}^{p \\times n}$ with each $\\mathbf{v}_{i} \\in \\mathbb{R}^{p}$. The attention operation computes the responses of a query vector $\\mathbf{q}_{i}$ by attending it to all key vectors in $K$ and uses the results to take a weighted sum over value vectors in $V$. The layer-wise forward-propagation\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_db1abbc7002dd7093cd5g-2.jpg?height=530&width=873&top_left_y=277&top_left_x=168)\n\nFigure 1: An illustration of the attention operator.",
    "kroneckerattn-2": "Here, $\\times$ denotes matrix multiplication, and softmax($\\cdot$) is the columnwise softmax operator. $Q, K$, and $V$ are input matrices. A similarity score is computed between each query vector as a column of $Q$ and each key vector as a column in $K$. Softmax($\\cdot$) normalizes these scores and makes them sum to 1. Multiplication between normalized scores and the matrix $V$ yields the corresponding output vector. operation of an attention operator can be expressed as\n\n$$\nO=\\operatorname{attn}(Q, K, V)=V \\times \\operatorname{Softmax}\\left(K^{T} Q\\right)\n$$\n\nMatrix multiplication between $K^{T}$ and $Q$ results in a coefficient ma$\\operatorname{trix} E=K^{T} Q$, in which each element $e_{i j}$ is calculated by the inner product between $\\mathbf{k}_{i}^{T}$ and $\\mathbf{q}_{j}$. This coefficient matrix $\\boldsymbol{E}$ computes similarity scores between every query vector $\\mathbf{q}_{i}$, and every key vector $\\mathbf{k}_{j}$ and is normalized by a column-wise softmax operator to make every column sum to 1 . The output $O \\in \\mathbb{R}^{p \\times m}$ is obtained by multiplying $V$ with the normalized $E$. In self-attention operators [33], we have $Q=K=V$. Figure 1 provides an illustration of the attention operator. The computational cost in Eq. 1 is $O(m \\times n \\times(d+p))$. The memory required for storing the intermediate coefficient matrix $\\boldsymbol{E}$ is $O(m n)$. If $d=p$ and $m=n$, the time and space complexities become $O\\left(m^{2} \\times d\\right)$ and $O\\left(m^{2}\\right)$, respectively. There are several other ways to compute $E$ from $Q$ and $K$, including Gaussian function, dot product, concatenation, and embedded Gaussian function. It has been shown that dot product is the simplest but most effective one [35]. Therefore, we focus on the dot product similarity function in this work. In practice, we can first perform separate linear transformations on each input matrix, resulting in the following attention operator: $\\boldsymbol{O}=\\boldsymbol{W}^{V} \\boldsymbol{V} \\operatorname{Softmax}\\left(\\left(\\boldsymbol{W}^{K} \\boldsymbol{K}\\right)^{T} \\boldsymbol{W}^{Q} \\boldsymbol{Q}\\right)$, where $\\boldsymbol{W}^{V} \\in \\mathbb{R}^{\\boldsymbol{p}^{\\prime} \\times p}, \\boldsymbol{W}^{K} \\in$ $\\mathbb{R}^{d^{\\prime} \\times d}$, and $W^{Q} \\in \\mathbb{R}^{d^{\\prime} \\times d}$. For notational simplicity, we omit linear transformations in the following discussion. ### 2.2 Non-Local Operator\n\nNon-local operators, which is proposed in [35], apply self-attention operators on higher-order data such as images and videos. Taking 2-D data as an example, the input to the non-local operator is a third-order tensor $\\mathcal{X} \\in \\mathbb{R}^{h \\times w \\times c}$, where $h, w$, and $c$ denote the height, width, and number of channels, respectively. The tensor is first\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_db1abbc7002dd7093cd5g-2.jpg?height=227&width=857&top_left_y=276&top_left_x=1096)\nFigure 2: Conversion of a third-order tensor into a matrix by unfolding along mode -3 . In this example, a $h \\times w \\times c$ tensor is unfolded into a $c \\times h w$ matrix. converted into a matrix $X_{(3)} \\in \\mathbb{R}^{c \\times h w}$ by unfolding along mode3 [21], as illustrated in Figure 2. Then we perform the operation in Eq. 1 by setting $Q=K=V=X_{(3)}$. The output of the attention operator is converted back to a third-order tensor as the final output. One practical challenge of the non-local operator is that it consumes excessive computational resources. If $h=w$, the computational cost of a 2-D non-local operator is $O\\left(h^{4} \\times c\\right)$. The memory used to store the intermediate coefficient matrix incurs $O\\left(h^{4}\\right)$ space complexity. The time and space complexities are prohibitively high for high-dimensional and high-order data. ## 3 KRONECKER ATTENTION NETWORKS\n\nIn this section, we describe our proposed Kronecker attention operators, which are efficient and effective attention operators on high-order data. We also describe how to use these operators to build Kronecker attention networks. ### 3.1 From Multivariate to Matrix-Variate Distributions\n\nWe analyze the problem of attention operators on high-order data and propose solutions from a probabilistic perspective. To illustrate the idea, we take the non-local operator on 2-D data in Section 2.2 as an example. Formally, consider a self-attention operator with $Q=$ $K=V=X_{(3)}$, where $X_{(3)} \\in \\mathbb{R}^{c \\times h w}$ is the mode-3 unfolding of a third-order input tensor $\\boldsymbol{X} \\in \\mathbb{R}^{h \\times w \\times c}$, as illustrated in Figure 2. The $i$ th row of $X_{(3)}$ corresponds to vec $\\left(X_{:: i}\\right)^{T} \\in \\mathbb{R}^{1 \\times h w}$, where $X_{:: i} \\in$ $\\mathbb{R}^{h \\times w}$ denotes the $i$ th frontal slice of $\\mathcal{X}$ [21], and vec $(\\cdot)$ denotes the vectorization of a matrix by concatenating its columns [12]. The frontal slices $X_{:: 1}, X_{:: 2}, \\ldots, X_{:: c} \\in \\mathbb{R}^{h \\times w}$ of $X_{\\text {are }}$ asually known as $c$ feature maps. In this view, the mode-3 unfolding is equivalent to the vectorization of each feature map independently. It is worth noting that, in addition to vec $(\\cdot)$, any other operation that transforms each feature map into a vector leads to the same output from the non-local operator, as long as a corresponding reverse operation is performed to fold the output into a tensor. This fact indicates that unfolding of $\\mathcal{X}$ in local operators ignores the structural information within each feature map, i.e., the relationships among rows and columns. In addition, such unfolding results in excessive requirements on computational resources, as explained in Section 2.2. In the following discussions, we focus on one feature map $X \\in$ $\\left\\{\\boldsymbol{X}_{:: 1}, \\boldsymbol{X}_{:: 2}, \\ldots, \\boldsymbol{X}_{:: c}\\right\\}$ by assuming feature maps are conditionally independent of each other, given feature maps of previous layers. This assumption is shared by many deep learning techniques that\nprocess each feature map independently, including the unfolding mentioned above, batch normalization [18], instance normalization [31], and pooling operations [22]. To view the problem above from a probabilistic perspective [18, 31], the unfolding yields the assumption that $\\operatorname{vec}(\\boldsymbol{X})$ follows a multivariate normal distribution as $\\operatorname{vec}(X) \\sim \\mathcal{N}_{h w}(\\boldsymbol{\\mu}, \\Omega)$, where $\\boldsymbol{\\mu} \\in \\mathbb{R}^{h w}$ and $\\Omega \\in \\mathbb{R}^{h w \\times h w}$. Apparently, the multivariate normal distribution does not model relationships among rows and columns in $X$. To address this limitation, we propose to model $\\boldsymbol{X}$ using a matrix-variate normal distribution [12], defined as below. Definition 1. A random matrix $A \\in \\mathbb{R}^{m \\times n}$ is said to follow a matrix-variate normal distribution $\\mathcal{M} \\mathcal{N}_{m \\times n}(M, \\Omega \\otimes \\Psi)$ with mean matrix $\\boldsymbol{M} \\in \\mathbb{R}^{m \\times n}$ and covariance matrix $\\Omega \\otimes \\Psi$, where $\\Omega \\in \\mathbb{R}^{m \\times m}>$ 0 and $\\Psi \\in \\mathbb{R}^{n \\times n}>0$, if $\\operatorname{vec}\\left(A^{T}\\right) \\sim \\mathcal{N}_{m n}\\left(\\operatorname{vec}\\left(M^{T}\\right), \\Omega \\otimes \\Psi\\right)$. Here, $\\otimes$ denotes the Kronecker product $[11,32]$. The matrix-variate normal distribution has separate covariance matrices for rows and columns. They interact through the Kronecker product to produce the covariance matrix for the original distribution. Specifically, for two elements $X_{i j}$ and $X_{i^{\\prime} j^{\\prime}}$ from different rows and columns in $\\boldsymbol{X}$, the relationship between $X_{i j}$ and $X_{i^{\\prime} j^{\\prime}}$ is modeled by the interactions between the $i$ th and $i^{\\prime}$ th rows and the $j$ th and $j^{\\prime}$ th columns. Therefore, the matrix-variate normal distribution is able to incorporate relationships among rows and columns. ### 3.2 The Proposed Mean and Covariance Structures\n\nIn machine learning, Kalaitzis et al. [20] proposed to use the Kronecker sum to form covariance matrices, instead of the Kronecker product. Based on the above observations and studies, we propose to model $X$ as $X \\sim \\mathcal{M} \\mathcal{N}_{h \\times w}(M, \\Omega \\oplus \\Psi)$, where $M \\in \\mathbb{R}^{h \\times w}$, $\\Omega \\in \\mathbb{R}^{h \\times h}>0, \\Psi \\in \\mathbb{R}^{w \\times w}>0, \\oplus$ denotes the Kronecker sum [20], defined as $\\Omega \\oplus \\Psi=\\Omega \\otimes I_{[w]}+I_{[h]} \\otimes \\boldsymbol{\\Psi}$, and $\\boldsymbol{I}_{[n]}$ denotes an $n \\times n$ identity matrix. Covariance matrices following the Kronecker sum structure can still capture the relationships among rows and columns [20]. It also follows from [2,37] that constraining the mean matrix $M$ allows a more direct modeling of the structural information within a feature map. Following these studies, we assume $\\boldsymbol{X}$ follows a variant of the matrix-variate normal distribution as\n\n$$\nX \\sim \\mathcal{M} \\mathcal{N}_{h \\times w}(M, \\Omega \\oplus \\Psi)\n$$\n\nwhere the mean matrix $\\boldsymbol{M} \\in \\mathbb{R}^{h \\times w}$ is restricted to be the outer sum of two vectors, defined as\n\n$$\n\\boldsymbol{M}=\\boldsymbol{\\mu} \\oplus \\boldsymbol{v}=\\boldsymbol{\\mu} \\mathbf{1}_{[w]}^{T}+\\mathbf{1}_{[h]} \\boldsymbol{v}^{T}\n$$\n\nwhere $\\boldsymbol{\\mu} \\in \\mathbb{R}^{h}, \\boldsymbol{v} \\in \\mathbb{R}^{\\boldsymbol{w}}$, and $\\mathbf{1}_{[n]}$ denotes a vector of all ones of size $n$. Under this model, the marginal distributions of rows and columns are both multivariate normal [2]. Specifically, the $i$ th row vector $\\boldsymbol{X}_{i:} \\in \\mathbb{R}^{1 \\times w}$ follows $\\boldsymbol{X}_{i:}^{T} \\sim \\mathcal{N}_{w}\\left(\\mu_{i}+\\boldsymbol{v}^{T}, \\Omega_{i i}+\\Psi\\right)$, and the $j$ th column vector $X_{: j} \\in \\mathbb{R}^{h \\times 1}$ follows $X_{: j} \\sim \\mathcal{N}_{h}\\left(v_{i}+\\boldsymbol{\\mu}, \\Psi_{i i}+\\Omega\\right)$. In the following discussion, we assume that $\\Omega$ and $\\Psi$ are diagonal, implying that any pair of variables in $X$ are uncorrelated. Note that, although the variables in $X$ are independent, their covariance matrix still follows the Kronecker covariance structure, thus capturing the relationships among rows and columns [2,37]. ### 3.3 Main Technical Results\n\nLet $\\overline{\\boldsymbol{X}}_{\\text {row }}=\\left(\\sum_{i=1}^{h} \\boldsymbol{X}_{i:}^{T}\\right) / h \\in \\mathbb{R}^{w}$ and $\\overline{\\boldsymbol{X}}_{\\text {col }}=\\left(\\sum_{j=1}^{w} \\boldsymbol{X}_{: j}\\right) / w \\in \\mathbb{R}^{h}$ be the average of row and column vectors, respectively. Under the assumption above, $\\bar{X}_{\\text {row }}$ and $\\overline{\\boldsymbol{X}}_{\\text {col }}$ follow multivariate normal distributions as\n\n$$\n\\begin{aligned}\n& \\bar{X}_{\\text {row }} \\sim \\mathcal{N}_{w}\\left(\\bar{\\mu}+v, \\frac{\\bar{\\Omega}+\\Psi}{h}\\right) \\\\\n& \\bar{X}_{c o l} \\sim \\mathcal{N}_{h}\\left(\\bar{v}+\\mu, \\frac{\\bar{\\Psi}+\\Omega}{w}\\right)\n\\end{aligned}\n$$\n\nwhere $\\overline{\\boldsymbol{\\mu}}=\\left(\\sum_{i=1}^{h} \\mu_{i}\\right) / h, \\overline{\\boldsymbol{\\Omega}}=\\left(\\sum_{i=1}^{h} \\Omega_{i i}\\right) / h, \\overline{\\boldsymbol{v}}=\\left(\\sum_{j=1}^{w} v_{j}\\right) / w$, and $\\bar{\\Psi}=\\left(\\sum_{j=1}^{w} \\Psi_{j j}\\right) / w$. Our main technical results can be summarized in the following theorem. Theorem 1. Given the multivariate normal distributions in Eqs. (4) and (5) with diagonal $\\Omega$ and $\\Psi$, if (a) $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{h}$ are independent and identically distributed (i.i.d.) random vectors that follow the distribution in Eq. (4), (b) $\\mathbf{c}_{1}, \\mathbf{c}_{2}, \\ldots, \\mathbf{c}_{w}$ are i.i.d. random vectors that follow the distribution in Eq. (5), (c) $\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{h}$ and $\\mathbf{c}_{1}, \\mathbf{c}_{2}, \\ldots, \\mathbf{c}_{w}$ are independent, we have\n\n$$\n\\tilde{X} \\sim \\mathcal{M} \\mathcal{N}_{h \\times w}\\left(\\tilde{M}, \\frac{\\bar{\\Psi}+\\Omega}{w} \\oplus \\frac{\\bar{\\Omega}+\\Psi}{h}\\right)\n$$\n\nwhere $\\tilde{\\boldsymbol{X}}=\\left[\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{h}\\right]^{T}+\\left[\\mathbf{c}_{1}, \\mathbf{c}_{2}, \\ldots, \\mathbf{c}_{w}\\right], \\tilde{\\boldsymbol{M}}=(\\boldsymbol{\\mu} \\oplus \\boldsymbol{v})+(\\overline{\\boldsymbol{\\mu}}+\\overline{\\boldsymbol{v}})$. In particular, if $h=w$, the covariance matrix satisfies\n\n$$\n\\operatorname{tr}\\left(\\frac{\\bar{\\Psi}+\\Omega}{w} \\oplus \\frac{\\bar{\\Omega}+\\Psi}{h}\\right)=\\frac{2}{h} \\operatorname{tr}(\\Omega \\oplus \\Psi)\n$$\n\nwhere $\\operatorname{tr}(\\cdot)$ denotes matrix trace. Proof. The fact that $\\Omega$ and $\\Psi$ are diagonal implies independence in the case of multivariate normal distributions. Therefore, it follows from assumptions (a) and (b) that\n\n$$\n\\left[\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{h}\\right]^{T} \\sim \\mathcal{M} \\mathcal{N}_{h \\times w}\\left(M_{r}, \\boldsymbol{I}_{[h]} \\otimes \\frac{\\bar{\\Omega}+\\Psi}{h}\\right)\n$$\n\nwhere $\\boldsymbol{M}_{r}=\\overline{\\boldsymbol{\\mu}}+[\\boldsymbol{v}, \\boldsymbol{v}, \\ldots, \\boldsymbol{v}]^{T}=\\overline{\\boldsymbol{\\mu}}+\\mathbf{1}_{[h]} \\boldsymbol{v}^{T}$, and\n\n$$\n\\left[\\mathbf{c}_{1}, \\mathbf{c}_{2}, \\ldots, \\mathbf{c}_{w}\\right] \\sim \\mathcal{M N}_{h \\times w}\\left(\\boldsymbol{M}_{c}, \\frac{\\bar{\\Psi}+\\Omega}{w} \\otimes \\boldsymbol{I}_{[w]}\\right)\n$$\n\nwhere $\\boldsymbol{M}_{c}=\\overline{\\boldsymbol{v}}+[\\boldsymbol{\\mu}, \\boldsymbol{\\mu}, \\ldots, \\boldsymbol{\\mu}]=\\overline{\\boldsymbol{v}}+\\boldsymbol{\\mu} \\mathbf{1}_{[w]}^{T}$. Given assumption (c) and $\\tilde{X}=\\left[\\mathbf{r}_{1}, \\mathbf{r}_{2}, \\ldots, \\mathbf{r}_{h}\\right]^{T}+\\left[\\mathbf{c}_{1}, \\mathbf{c}_{2}, \\ldots, \\mathbf{c}_{w}\\right]$, we have\n\n$$\n\\tilde{X} \\sim \\mathcal{M} \\mathcal{N}_{h \\times w}\\left(\\tilde{M}, \\frac{\\bar{\\Psi}+\\Omega}{w} \\oplus \\frac{\\bar{\\Omega}+\\Psi}{h}\\right)\n$$\n\nwhere $\\tilde{\\boldsymbol{M}}=\\boldsymbol{M}_{r}+\\boldsymbol{M}_{c}=(\\boldsymbol{\\mu} \\oplus \\boldsymbol{v})+(\\overline{\\boldsymbol{\\mu}}+\\overline{\\boldsymbol{v}})$. If $h=w$, we have\n\n$$\n\\operatorname{tr}(\\Omega \\oplus \\Psi)=h\\left(\\sum \\Omega_{i i}+\\sum \\Psi_{j j}\\right)\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_db1abbc7002dd7093cd5g-4.jpg?height=1015&width=1798&top_left_y=267&top_left_x=162)\n\nFigure 3: Illustrations of regular attention operator (a), $\\mathrm{KAO}_{K V}$ (b) and $\\mathrm{KAO}_{Q K V}$ (c) on 2-D data. In the regular attention operator (a), the input tensor is unfolded into a mode-3 matrix and fed into the attention operator. The output of the attention operator is folded back to a tensor as the final output. In $\\mathrm{KAO}_{K V}$ (b), we juxtapose the horizontal and lateral average matrices derived from the input tensor as the key and value matrices. We keep the mode- 3 unfolding of input tensor as the query matrix. In $\\mathrm{KAO}_{Q K V}$ (c), all three input matrices use the juxtaposition of two average matrices. In contrast to $\\mathrm{KAO}_{K V}$, we use an outer-sum operation to generate the third-order tensor from the output of the attention operator. and\n\n$$\n\\begin{aligned}\n& \\operatorname{tr}\\left(\\frac{\\bar{\\Psi}+\\Omega}{w} \\oplus \\frac{\\bar{\\Omega}+\\Psi}{h}\\right) \\\\\n= & \\operatorname{tr}\\left(\\frac{1}{h}(\\Omega \\oplus \\Psi)+\\frac{1}{h}(\\bar{\\Psi}+\\bar{\\Omega})\\right) \\\\\n= & \\left(\\sum \\Omega_{i i}+\\sum \\Psi_{j j}\\right)+h(\\bar{\\Psi}+\\bar{\\Omega}) \\\\\n= & 2\\left(\\sum \\Omega_{i i}+\\sum \\Psi_{j j}\\right) \\\\\n= & \\frac{2}{h} \\cdot \\operatorname{tr}(\\Omega \\oplus \\Psi)\n\\end{aligned}\n$$\n\nThis completes the proof of the theorem. With certain normalization on $\\boldsymbol{X}$, we can have $\\overline{\\boldsymbol{\\mu}}+\\overline{\\boldsymbol{v}}=0$, resulting in\n\n$$\n\\tilde{M}=\\mu \\mapsto v\n$$\n\nAs the trace of a covariance matrix measures the total variation, Theorem 1 implies that $\\tilde{X}$ follows a matrix-variate normal distribution with the same mean and scaled covariance as the distribution of $\\boldsymbol{X}$ in Eq. (2). Given this conclusion and the process to obtain $\\tilde{X}$ from $X$, we propose our Kronecker attention operators in the following section. ### 3.4 Kronecker Attention Operators\n\nWe describe the Kronecker attention operators (KAO) in the context of self-attention on 2-D data, but they can be easily generalized to generic attentions.",
    "kroneckerattn-3": "In this case, the input to the $\\ell$ th layer is a third-order tensor $X^{(\\ell)} \\in \\mathbb{R}^{h \\times w \\times c}$. Motivated by the theoretical results of Sections 3.2 and 3.3, we propose to use horizontal and lateral average matrices to represent original mode-3 unfolding without much information loss. Based on Eq. (4) and Eq. (5), the horizontal average matrix $\\boldsymbol{H}$ and the lateral average matrix $\\boldsymbol{L}$ are computed as\n\n$$\n\\begin{aligned}\nH & =\\frac{1}{h} \\sum_{i=1}^{h} X_{i::}^{(\\ell)} \\in \\mathbb{R}^{w \\times c} \\\\\nL & =\\frac{1}{w} \\sum_{j=1}^{w} X_{: j:}^{(\\ell)} \\in \\mathbb{R}^{h \\times c}\n\\end{aligned}\n$$\n\nwhere $X_{i::}^{(\\ell)}$ and $X_{: j:}^{(\\ell)}$ are the horizontal and lateral slices [21] of tensor $\\mathcal{X}^{(\\ell)}$, respectively. We then form a matrix $\\boldsymbol{C}$ by juxtaposing $\\boldsymbol{H}^{T}$ and $\\boldsymbol{L}^{T}$ as\n\n$$\nC=\\left[\\boldsymbol{H}^{T}, \\boldsymbol{L}^{T}\\right] \\in \\mathbb{R}^{c \\times(h+w)} . $$\n\nBased on the horizontal and lateral average matrices contained in $\\boldsymbol{C}$, we propose two Kronecker attention operators (KAOs), i.e.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_db1abbc7002dd7093cd5g-5.jpg?height=803&width=1791&top_left_y=270&top_left_x=172)\n\nFigure 4: Architectures of the BaseModule (a), BaseSkipModule (b), AttnModule (c), and AttnSkipModule (d) as described in Section 3.5.",
    "kroneckerattn-4": "The skip connections indicated by single dashed paths are not used when $s>1$ or $c \\neq d$.",
    "kroneckerattn-5": "Those indicated by double dashed paths are not used when $s>1$. $\\mathrm{KAO}_{K V}$ and $\\mathrm{KAO}_{Q K V}$. In $\\mathrm{KAO}_{K V}$ as shown in Figure $3(\\mathrm{~b})$, we use $X_{(3)}^{(\\ell)}$ as the query matrix and $C$ as the key and value matrices as\n\n$$\nO=\\operatorname{attn}\\left(X_{(3)}^{(\\ell)}, C, C\\right) \\in \\mathbb{R}^{c \\times h w}\n$$\n\nNote that the number of columns in $\\boldsymbol{O}$ depends on the number of query vectors. Thus, we obtain $h w$ output vectors from the attention operation in Eq. (16). Similar to the regular attention operator, $\\boldsymbol{O}$ is folded back to a third-order tensor $\\mathbf{y}^{(\\ell)} \\in \\mathbb{R}^{h \\times w \\times c}$ by considering the column vectors in $\\boldsymbol{O}$ as mode-3 fibers of $\\boldsymbol{y}^{(\\ell)} . \\mathrm{KAO}_{K V}$ uses $y^{(\\ell)}$ as the output of layer $\\ell$. If $h=w$, the time and space complexities of $\\mathrm{KAO}_{K V}$ are $O(h w \\times$ $c \\times(h+w))=O\\left(h^{3} \\times c\\right)$ and $O(h w \\times(h+w))=O\\left(h^{3}\\right)$, respectively. Compared to the original local operator on 2-D data, $\\mathrm{KAO}_{K V}$ reduces time and space complexities by a factor of $h$. In order to reduce the time and space complexities further, we propose another operator known as $\\mathrm{KAO}_{Q K V}$. In $\\mathrm{KAO}_{Q K V}$ as shown in Figure 3(c), we use $C$ as the query, key, and value matrices as\n\n$$\n[\\underbrace{\\tilde{\\boldsymbol{H}}}_{h}, \\underbrace{\\tilde{L}}_{w}]=\\boldsymbol{O}=\\operatorname{attn}(C, C, C) \\in \\mathbb{R}^{c \\times(h+w)} \\text {. }\n$$\n\nThe final output tensor $y^{(\\ell)} \\in \\mathbb{R}^{h \\times w \\times c}$ is obtained as\n\n$$\nY_{:: i}^{(\\ell)}=\\tilde{\\boldsymbol{H}}_{i:}^{T} \\otimes \\tilde{\\boldsymbol{L}}_{i:}^{T}\n$$\n\nwhere $\\tilde{H}_{i}$ : and $\\tilde{L}_{i}$ : are the $i$ th rows of the corresponding matrices. That is, the $i$ th frontal slice of $\\boldsymbol{y}^{(\\ell)}$ is obtained by computing the outer sum of the $i$ th rows of $\\tilde{\\boldsymbol{H}}$ and $\\tilde{\\boldsymbol{L}}$. If $h=w$, the time and space complexities of $\\mathrm{KAO}_{Q K V}$ are $O((h+$ $w) \\times c \\times(h+w))=O\\left(h^{2} \\times c\\right)$ and $O((h+w) \\times(h+w))=O\\left(h^{2}\\right)$, respectively. Thus, the time and space complexities have been reduced by a factor of $h^{2}$ as compared to the original local operator, and by a factor of $h$ as compared to $\\mathrm{KAO}_{K V}$. Note that we do not consider linear transformations in our description, but these transformations can be applied to all three input matrices in $\\mathrm{KAO}_{K V}$ and $\\mathrm{KAO}_{Q K V}$ as shown in Figure 3. ### 3.5 Kronecker Attention Modules and Networks\n\nAttention models have not been used in compact deep models to date, primarily due to their high computational cost. Our efficient KAOs make it possible to use attention operators in compact convolutional neural networks (CNNs) like MobileNet [28]. In this section, we design a family of efficient Kronecker attention modules based on MobileNetV2 that can be used in compact CNNs. BaseModule: MobileNetV2 [28] is mainly composed of bottleneck blocks with inverted residuals. Each bottleneck block consists of three convolutional layers; those are, $1 \\times 1$ convolutional layer, $3 \\times 3$ depth-wise convolutional layer, and another $1 \\times 1$ convolutional layer. Suppose the expansion factor is $r$ and stride is $s$. Given input $\\mathcal{X}^{(\\ell)} \\in \\mathbb{R}^{h \\times w \\times c}$ for the $\\ell$ th block, the first $1 \\times 1$ convolutional layer outputs $r c$ feature maps $\\tilde{X}^{(\\ell)} \\in \\mathbb{R}^{h \\times w \\times r c}$. The depth-wise convolutional layer uses a stride of $s$ and outputs $r c$ feature maps $\\bar{X}^{(\\ell)} \\in \\mathbb{R}^{\\frac{h}{s} \\times \\frac{w}{s} \\times r c}$. The last $1 \\times 1$ convolutional layer produces $d$ feature maps $\\boldsymbol{y}^{(\\ell)} \\in \\mathbb{R}^{\\frac{h}{s} \\times \\frac{w}{s} \\times d}$. When $s=1$ and $c=d$, a skip connection is added between $\\mathcal{X}^{(\\ell)}$ and $\\boldsymbol{y}^{(\\ell)}$. The BaseModule is illustrated in Figure $4(a)$. BaseSkipModule: To facilitate feature reuse and gradient backpropagation in deep models, we improve the BaseModule by adding\n\nTable 1: Details of the KANets architecture. Each line describes a sequence of operators in the format of \"input size / operator name / expansion rate $r$ / number of output channels $c$ / number of operators in the sequence $n$ / stride $s$ \". \"Conv2D\" denotes the regular 2D convolutional layer. \"AvgPool\" and \"FC\" denote the global average pooling layer and the fully-connected layer, respectively. All depth-wise convolutions use the kernel size of $3 \\times 3$. For multiple operators in a sequence denoted in the same line, all operators produce $c$ output channels. And the first operator applies the stride of $s$ while the following operators applies the stride of $1 . k$ denotes the class number in the task. | Input | Operator | $\\boldsymbol{r}$ | $\\boldsymbol{c}$ | $\\boldsymbol{n}$ | $\\boldsymbol{s}$ |\n| :--- | :--- | :--- | :---: | :--- | :--- |\n| $224^{2} \\times 3$ | Conv2D $3 \\times 3$ | - | 32 | 1 | 2 |\n| $112^{2} \\times 32$ | BaseSkipModule | 1 | 16 | 1 | 1 |\n| $112^{2} \\times 16$ | BaseSkipModule | 6 | 24 | 2 | 2 |\n| $56^{2} \\times 24$ | BaseSkipModule | 6 | 32 | 2 | 2 |\n| $28^{2} \\times 32$ | AttnSkipModule | 6 | 32 | 1 | 1 |\n| $28^{2} \\times 32$ | BaseSkipModule | 6 | 64 | 1 | 2 |\n| $14^{2} \\times 64$ | AttnSkipModule | 6 | 64 | 3 | 1 |\n| $14^{2} \\times 64$ | AttnSkipModule | 6 | 96 | 3 | 1 |\n| $14^{2} \\times 96$ | BaseSkipModule | 6 | 160 | 1 | 2 |\n| $7^{2} \\times 160$ | AttnSkipModule | 6 | 160 | 2 | 1 |\n| $7^{2} \\times 160$ | AttnSkipModule | 6 | 320 | 1 | 1 |\n| $7^{2} \\times 320$ | Conv2D $1 \\times 1$ | - | 1280 | 1 | 1 |\n| $7^{2} \\times 1280$ | AvgPool + FC | - | $k$ | 1 | - |\n\na skip connection. Given input $X^{(\\ell)}$, we use an expansion factor of $r-1$ for the first $1 \\times 1$ convolutional layer, instead of $r$ as in BaseModule. We then concatenate the output with the original input, resulting in $\\tilde{X}^{(\\ell)} \\in \\mathbb{R}^{h \\times w \\times r c}$. The other parts of the BaseSkipModule are the same as those of the BaseModule as illustrated in Figure 4 (b). Compared to the BaseModule, the BaseSkipModule reduces the number of parameters by $c \\times c$ and computational cost by $h \\times w \\times c$. It achieves better feature reuse and gradient backpropagation. AttnModule: We propose to add an attention operator into the BaseModule to enable the capture of global features. We reduce the expansion factor of the BaseModule by 1 and add a new parallel path with an attention operator that outputs $c$ feature maps. Concretely, after the depth-wise convolutional layer, the original path outputs $\\bar{X}_{a}^{(\\ell)} \\in \\mathbb{R}^{\\frac{h}{s} \\times \\frac{w}{s} \\times(r-1) c}$. The attention operator, optionally followed by an average pooling of stride $s$ if $s>1$, produces $\\bar{X}_{b}^{(\\ell)} \\in \\mathbb{R}^{\\frac{h}{s} \\times \\frac{w}{s} \\times c}$. Concatenating them gives $\\bar{X}^{(\\ell)} \\in \\mathbb{R}^{\\frac{h}{s} \\times \\frac{w}{s} \\times r c}$. The final $1 \\times 1$ convolutional layer remains the same. Within the attention operator, we only apply the linear transformation on the value matrix $V$ to limit the number of parameters and required computational resources. We denote this module as the AttnModule as shown in Figure 4 (b). In this module, the original path acts as locality-based feature extractors, while the new parallel path with an attention operator computes global features. This enables the module to incorporate both local and global information. Note that we can use any attention operator in this module, including the regular attention operator and our KAOs. AttnSkipModule: We propose to add an additional skip connection in the AttnModule, as shown in Figure 4 (d). This skip connection can always be added unless $s>1$. The AttnSkipModule has the same amount of parameters and computational cost as the AttnModule. ## 4 EXPERIMENTAL STUDIES\n\nIn this section, we evaluate our proposed operators and networks on image classification and segmentation tasks. We first compare our proposed KAOs with regular attention operators in terms of computational cost and memory usage. Next, we design novel compact CNNs known as Kronecker attention networks (KANets) using our proposed operators and modules. We compare KANets with other compact CNNs on the ImageNet ILSVRC 2012 dataset [5].",
    "kroneckerattn-6": "Ablation studies are conducted to investigate how our KAOs benefit the entire networks. We also perform experiments on the PASCAL 2012 dataset [6] to show the effectiveness of our KAOs on general application scenarios. ### 4.1 Experimental Setup\n\nIn this section, we describe the experimental setups for both image classification tasks and image segmentation tasks. Experimental Setup for Image Classification As a common practice on this dataset, we use the same data augmentation scheme in He et al. [14]. Specifically, during training, we scale each image to $256 \\times 256$ and then randomly crop a $224 \\times 224$ patch. During inference, the center-cropped patches are used. We train our KANets using the same settings as MobileNetV2 [28] with minor changes. We perform batch normalization [18] on the coefficient matrices in KAOs to stabilize the training. All trainable parameters are initialized with the Xavier initialization [10]. We use the standard stochastic gradient descent optimizer with a momentum of 0.9 [30] to train models for 150 epochs in total. The initial learning rate is 0.1 and it decays by 0.1 at the 80th, 105th, and 120th epoch. Dropout [29] with a keep rate of 0.8 is applied after the global average pooling layer. We use 8 TITAN Xp GPUs and a batch size of 512 for training, which takes about 1.5 days. Since labels of the test dataset are not available, we train our networks on training dataset and report accuracies on the validation dataset. Experimental Setup for Image Segmentation We train all the models with randomly cropped patches of size $321 \\times 321$ and a batch size of 8 . Data augmentation by randomly scaling the inputs for training is employed. We adopt the \"poly\" learning rate policy [25] with power $=0.9$, and set the initial learning rate to 0.00025 . Following DeepLabV2, we use the ResNet-101 model pre-trained on ImageNet [5] and MS-COCO [24] for initialization. The models are then trained for 25,000 iterations with a momentum of 0.9 and a weight decay of 0.0005 .",
    "kroneckerattn-7": "We perform no post-processing such as conditional random fields and do not use multi-scale inputs due to limited GPU memory. All the models are trained on the training set and evaluated on the validation set. Table 2: Comparisons between the regular attention operator, the regular attention operator with a pooling operation [35], and our proposed $\\mathrm{KAO}_{K V}$ and $\\mathrm{KAO}_{Q K V}$ in terms of the number of parameters, number of MAdd, memory usage, and CPU inference time on simulated data of different sizes.",
    "kroneckerattn-8": "The input sizes are given in the format of \"batch size $\\times$ spatial sizes $\\times$ number of input channels\". \"Attn\" denotes the regular attention operator. \"Attn+Pool\" denotes the regular attention operator which employs a $2 \\times 2$ pooling operation on $K$ and $V$ input matrices to reduce required computational resources. | Input | Operator | MAdd | Cost Saving | Memory | Memory Saving | Time | Speedup |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $8 \\times 14^{2} \\times 8$ | Attn | 0.63 m | $0.00 \\%$ | 5.2MB | 0.00\\% | 5.8 ms | $1.0 \\times$ |\n|  | Attn+Pool | 0.16 m | $75.00 \\%$ | 1.5 MB | 71.65\\% | 2.0 ms | $3.0 \\times$ |\n|  | $\\mathrm{KAO}_{K V}$ | 0.09 m | 85.71\\% | 0.9 MB | 82.03\\% | 1.7 ms | $3.5 \\times$ |\n|  | $\\mathrm{KAO}_{Q K V}$ | 0.01m | $\\mathbf{9 7 . 7 1 \\%}$ | 0.3MB | 95.06\\% | 0.8 ms | $6.8 \\times$ |\n| $8 \\times 28^{2} \\times 8$ | Attn | 9.88 m | $0.00 \\%$ | 79.9MB | 0.00\\% | 72.4 ms | $1.0 \\times$ |\n|  | Attn+Pool | 2.47 m | 75.00\\% | 20.7MB | 74.13\\% | 20.9 ms | $3.5 \\times$ |\n|  | $\\mathrm{KAO}_{K V}$ | 0.71 m | 92.86\\% | 6.5MB | 91.88\\% | 7.1 ms | 10.1\u00d7 |\n|  | $\\mathrm{KAO}_{Q K V}$ | 0.05m | $\\mathbf{9 9 . 4 6 \\%}$ | 0.9MB | 98.85\\% | 1.7 ms | $40.9 \\times$ |\n| $8 \\times 56^{2} \\times 8$ | Attn | 157.55m | 0.00\\% | 1,262.6MB | 0.00\\% | 1,541.1ms | 1.0\u00d7 |\n|  | Attn+Pool | 39.39 m | 75.00\\% | 318.7MB | 74.76\\% | 396.9 ms | $3.9 \\times$ |\n|  | $\\mathrm{KAO}_{K V}$ | 5.62 m | 96.43\\% | 48.2 MB | 96.18\\% | 49.6 ms | $31.1 \\times$ |\n|  | $\\mathrm{KAO}_{\\text {QKV }}$ | 0.21m | 99.87\\% | 3.4MB | 99.73\\% | 5.1 ms | $305.8 \\times$ |\n\n### 4.2 Comparison of Computational Efficiency\n\nAccording to the theoretical analysis in Section 3.4, our KAOs have efficiency advantages over regular attention operators on highorder data, especially for inputs with large spatial sizes. We conduct simulated experiments to evaluate the theoretical results. To reduce the influence of external factors, we build networks composed of a single attention operator, and apply the TensorFlow profile tool [1] to report the multiply-adds (MAdd), required memory, and time consumed on 2-D simulated data. For the simulated input data, we set the batch size and number of channels both to 8 , and test three spatial sizes; those are, $56 \\times 56,28 \\times 28$, and $14 \\times 14$.",
    "kroneckerattn-9": "The number of output channels is also set to 8 . Table 2 summarizes the comparison results. On simulated data of spatial sizes $56 \\times 56$, our $\\mathrm{KAO}_{K V}$ and $\\mathrm{KAO}_{Q K V}$ achieve 31.1 and 305.8 times speedup, and $96.18 \\%$ and $99.73 \\%$ memory saving compared to the regular attention operator, respectively. Our proposed KAOs show significant improvements over regular attention operators in terms of computational resources, which is consistent with the theoretical analysis. In particular, the amount of improvement increases as the spatial sizes increase. These results show that the proposed KAOs are efficient attention operators on high-dimensional and high-order data. ### 4.3 Results on Image Classification\n\nWith the high efficiency of our KAOs, we have proposed several efficient Kronecker attention modules for compact CNNs in Section 3.5. To further show the effectiveness of KAOs and the modules, we build novel compact CNNs known as Kronecker attention networks (KANets). Following the practices in [35], we apply these modules on inputs of spatial sizes $28 \\times 28,14 \\times 14$, and $7 \\times 7$. The detailed network architecture is described in Table 1 in the Section 4.1 . We compare KANets with other CNNs on the ImageNet ILSVRC 2012 image classification dataset, which serves as the benchmark for compact CNNs [8, 15, 28, 39]. The dataset contains 1.2 million\nTable 3: Comparisons between KANets and other CNNs in terms of the top- 1 accuracy on the ImageNet validation set, the number of total parameters, and MAdd. We use KANet $_{{ }_{K V}}$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_db1abbc7002dd7093cd5g-7.jpg?height=44&width=852&top_left_y=1203&top_left_x=1097) respectively. | Model | Top-1 | Params | MAdd |\n| :---: | :---: | :---: | :---: |\n| GoogleNet | 0.698 | 6.8 m | 1550m |\n| VGG16 | 0.715 | 128 m | 15300m |\n| AlexNet | 0.572 | 60 m | 720m |\n| SqueezeNet | 0.575 | 1.3 m | 833m |\n| MobileNetV1 | 0.706 | 4.2 m | 569 m |\n| ShuffleNet 1.5x | 0.715 | 3.4 m | 292m |\n| ChannelNet-v1 | 0.705 | 3.7 m | 407 m |\n| MobileNetV2 | 0.720 | 3.47 m | 300 m |\n| KANet $_{K V}$ (ours) | 0.729 | 3.44 m | 288m |\n| KANet $_{\\text {QKV }}$ (ours) | 0.728 | 3.44 m | 281m |\n\ntraining, 50 thousand validation, and 50 thousand testing images. Each image is labeled with one of 1,000 classes. Details of the experimental setups are provided in the Section 4.1. The comparison results between our KANets and other CNNs in terms of the top-1 accuracy, number of parameters, and MAdd are reported in Table 3. SqueezeNet [17] has the least number of parameters, but uses the most MAdd and does not obtain competitive performance as compared to other compact CNNs. Among compact CNNs, MobileNetV2 [28] is the previous state-of-the-art model, which achieves the best trade-off between effectiveness and efficiency. According to the results, our KANets significantly outperform MobileNetV2 with 0.03 million fewer parameters. Specifically, our KANet $_{K V}$ and KANet $_{Q K V}$ outperform MobileNetV2 by margins of $0.9 \\%$ and $0.8 \\%$, respectively. More importantly, our KANets has the least computational cost. These results demonstrate the effectiveness and efficiency of our proposed KAOs. Table 4: Comparisons between KANets with regular attention operators (denoted as AttnNet), KANets with regular attention operators with a pooling operation (denoted as AttnNet + Pool) and KANets with KAOs in terms of the top-1 accuracy on the ImageNet validation set, the number of total parameters, and MAdd. | Model | Top-1 | Params | MAdd |\n| :---: | :---: | :---: | :---: |\n| AttnNet | 0.730 | 3.44 m | 365m |\n| AttnNet + Pool | 0.729 | 3.44 m | 300m |\n| KANet $_{K V}$ | 0.729 | 3.44 m | 288m |\n| KANet $_{Q K V}$ | 0.728 | 3.44 m | 281m |\n\nThe performance of KANets indicates that our proposed methods are promising, since we only make small modifications to the architecture of MobileNetV2 to include KAOs. Compared to modules with the regular convolutional layers only, our proposed modules with KAOs achieve better performance without using excessive computational resources. Thus, our methods can be used widely for designing compact deep models. Our KAOs successfully address the practical challenge of applying regular attention operators on highorder data. In the next experiments, we show that our proposed KAOs are as effective as regular attention operators. ### 4.4 Comparison with Regular Attention Operators\n\nWe perform experiments to compare our proposed KAOs with regular attention operators. We consider the regular attention operator and the one with a pooling operation in [35]. For the attention operator with pooling operation, the spatial sizes of the key matrix $\\boldsymbol{K}$ and value matrix $\\boldsymbol{V}$ are reduced by $2 \\times 2$ pooling operations to save computation cost. To compare these operators in fair settings, we replace all KAOs in KANets with regular attention operators and regular attention operators with a pooling operation, denoted as AttnNet and AttnNet + Pool, respectively. The comparison results are summarized in Table 4. Note that all these models have the same number of parameters. We can see that KANet $_{K V}$ and KANet $_{Q K V}$ achieve similar performance as AttnNet and $\\mathrm{AttnNet}+\\mathrm{Pool}$ with dramatic reductions of computational cost. The results indicate that our proposed KAOs are as effective as regular attention operators while being much more efficient. In addition, our KAOs are better than regular attention operators that uses a pooling operation to increase efficiency in [35]. ### 4.5 Ablation Studies\n\nTo show how our KAOs benefit entire networks in different settings, we conduct ablation studies on MobileNetV2 and KANet $_{K V}$. For MobileNetV2, we replace BaseModules with AttnModules as described in Section 3.5, resulting in a new model denoted as MobileNetV2+KAO. On the contrary, based on $\\mathrm{KANet}_{K V}$, we replace all AttnSkipModules by BaseModules. The resulting model is denoted as KANet w/o KAO. Table 5 reports the comparison results. By employing $\\mathrm{KAO}_{K V}$, MobileNetV2+KAO gains a performance boost of $0.6 \\%$ with fewer\nTable 5: Comparisons between MobileNetV2, MobileNetV2 with $\\mathrm{KAOs}_{K V}$ (denoted as MobileNetV2+KAO ${ }_{K V}$ ), KANet $_{K V}$, and KANet $_{K V}$ without KAO $_{K V}$ (denoted as KANet w/o KAO) in terms of the top- 1 accuracy on the ImageNet validation set, the number of total parameters, and MAdd. | Model | Top-1 | Params | MAdd |\n| :---: | :---: | :---: | :---: |\n| MobileNetV2 | 0.720 | 3.47 m | 300m |\n| MobileNetV2+KAO | 0.726 | 3.46 m | 298m |\n| KANet $_{K V}$ | 0.729 | 3.44 m | 288m |\n| KANet w/o KAO | 0.721 | 3.46 m | 298m |\n\nTable 6: Comparisons of DeepLabV2, DeepLabV2 with the regular attention operator (DeepLabV2+Attn), DeepLabV2 with our $\\mathrm{KAO}_{K V}$ (DeepLabV2+KAO ${ }_{K V}$ ), and DeepLabV2 with our $\\mathrm{KAO}_{Q K V}$ (DeepLabV2+KAO ${ }_{Q K V}$ ) in terms of the pixelwise accuracy, and mean IOU on the PASCAL VOC 2012 validation dataset. | Model | Accuracy | Mean IOU |\n| :---: | :---: | :---: |\n| DeepLabV2 | 0.944 | 75.1 |\n| DeepLabV2+Attn | 0.947 | 76.3 |\n| DeepLabV2+ $\\mathrm{KAO}_{K V}$ | 0.946 | 75.9 |\n| DeepLabV2+KAO ${ }_{Q K V}$ | 0.946 | 75.8 |\n\nparameters than MobileNetV2. On the other hand, KANet $_{K V}$ outperforms KANet w/o KAO by a margin of $0.8 \\%$, while KANet w/o KAO has more parameters than KANet $_{K V}$. KANet $_{K V}$ achieves the best performance while costing the least computational resources. The results indicate that our proposed KAOs are effective and efficient, which is independent of specific network architectures. ### 4.6 Results on Image Segmentation\n\nIn order to show the efficiency and effectiveness of our KAOs in broader application scenarios, we perform additional experiments on image segmentation tasks using the PASCAL 2012 dataset [6].",
    "kroneckerattn-10": "With the extra annotations provided by [13], the augmented dataset contains 10,582 training, 1,449 validation, and 1,456 testing images. Each pixel of the images is labeled by one of 21 classes with 20 foreground classes and 1 background class. We re-implement the DeepLabV2 model [4] as our baseline. Following [36], using attention operators as the output layer, instead of atrous spatial pyramid pooling (ASPP), results in a significant performance improvement. In our experiments, we replace ASPP with the regular attention operator and our proposed KAOs, respectively, and compare the results. For all attention operators, linear transformations are applied on $Q, K$, and $V$. Details of the experimental setups are provided in the Section 4.1. Table 6 shows the evaluation results in terms of pixel accuracy and mean intersection over union (IoU) on the PASCAL VOC 2012 validation set. Clearly, models with attention operators outperform the baseline model with ASPP. Compared with the regular attention operator, KAOs result in similar pixel-wise accuracy but slightly lower mean IoU. From the pixel-wise accuracy, results indicate that\n\nKAOs are as effective as the regular attention operator. The decrease in mean IoU may be caused by the strong structural assumption behind KAOs. Overall, the experimental results demonstrate the efficiency and effectiveness of our KAOs in broader application scenarios. ## 5 CONCLUSIONS\n\nIn this work, we propose Kronecker attention operators to address the practical challenge of applying attention operators on highorder data. We investigate the problem from a probabilistic perspective and use matrix-variate normal distributions with Kronecker covariance structure. Experimental results show that our KAOs reduce the amount of required computational resources by a factor of hundreds, with larger factors for higher-dimensional and higherorder data. We employ KAOs to design a family of efficient modules, leading to our KANets. KANets significantly outperform the previous state-of-the-art compact models on image classification tasks, with fewer parameters and less computational cost. Additionally, we perform experiments on the image segmentation task to show the effectiveness of our KAOs on general application scenarios. ## ACKNOWLEDGMENTS\n\nThis work was supported in part by National Science Foundation grants IIS-1908220 and DBI-1922969. ## REFERENCES\n\n[1] Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. Tensorflow: a system for large-scale machine learning.. In OSDI, Vol.",
    "kroneckerattn-11": "16. $265-283$. [2] Genevera I Allen and Robert Tibshirani. 2010. Transposable regularized covariance models with an application to missing data imputation. The Annals of Applied Statistics 4, 2 (2010), 764. [3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. International Conference on Learning Representations (2015). [4] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. 2018. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence 40, 4 (2018), 834-848.",
    "kroneckerattn-12": "[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. [6] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. 2010. The pascal visual object classes (voc) challenge. International journal of computer vision 88, 2 (2010), 303-338.",
    "kroneckerattn-13": "[7] Hongyang Gao and Shuiwang Ji. 2019. Graph representation learning via hard and channel-wise attention networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining. 741-749. [8] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. 2018. ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions.",
    "kroneckerattn-14": "In Advances in Neural Information Processing Systems. 5203-5211. [9] Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. 2018. Large-scale learnable graph convolutional networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining. 1416-1424. [10] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. 249-256. [11] Alexander Graham. 2018. Kronecker products and matrix calculus with applications. Courier Dover Publications.",
    "kroneckerattn-15": "[12] Arjun K Gupta and Daya K Nagar. 2018. Matrix variate distributions. Chapman and Hall/CRC. [13] Bharath Hariharan, Pablo Arbel\u00e1ez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. 2011. Semantic contours from inverse detectors. In Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 991-998. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. $770-778$. [15] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017). [16] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. 2018. CCNet: Criss-Cross Attention for Semantic Segmentation. arXiv preprint arXiv:1811.11721 (2018). [17] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. 2016. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5 MB model size. arXiv preprint arXiv:1602.07360 (2016). [18] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.",
    "kroneckerattn-16": "In International Conference on Machine Learning. 448-456. [19] Rie Johnson and Tong Zhang. 2015. Semi-supervised convolutional neural networks for text categorization via region embedding.",
    "kroneckerattn-17": "In Advances in neural information processing systems. 919-927. [20] Alfredo Kalaitzis, John Lafferty, Neil Lawrence, and Shuheng Zhou. 2013. The bigraphical lasso. In International Conference on Machine Learning.",
    "kroneckerattn-18": "1229-1237. [21] Tamara G. Kolda and Brett W.",
    "kroneckerattn-19": "Bader. 2009. Tensor Decompositions and Applications. SIAM Rev. 51, 3 (2009), 455-500. [22] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradientbased learning applied to document recognition. Proc. IEEE 86, 11 (1998), 22782324 . [23] Guanbin Li, Xiang He, Wei Zhang, Huiyou Chang, Le Dong, and Liang Lin. 2018. Non-locally Enhanced Encoder-Decoder Network for Single Image De-raining. In 2018 ACM Multimedia Conference on Multimedia Conference. ACM, 1056-1064. [24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision. Springer, 740-755. [25] Wei Liu, Andrew Rabinovich, and Alexander C Berg. 2015. Parsenet: Looking wider to see better. arXiv preprint arXiv:1506.04579 (2015). [26] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016. Hierarchical question-image co-attention for visual question answering. In Advances In Neural Information Processing Systems. 289-297. [27] Mateusz Malinowski, Carl Doersch, Adam Santoro, and Peter Battaglia. 2018. Learning Visual Question Answering by Bootstrapping Hard Attention.",
    "kroneckerattn-20": "In Proceedings of the European Conference on Computer Vision (ECCV). 3-20. [28] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and LiangChieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks.",
    "kroneckerattn-21": "In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, $4510-4520$. [29] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15, 1 (2014), 1929-1958. [30] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On the importance of initialization and momentum in deep learning.",
    "kroneckerattn-22": "In International conference on machine learning. 1139-1147. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. 2016. Instance Normalization: The Missing Ingredient for Fast Stylization. arXiv preprint arXiv:1607.08022 (2016). [32] Charles F Van Loan. 2000. The ubiquitous Kronecker product. fournal of computational and applied mathematics 123, 1-2 (2000), 85-100. [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. 5998-6008. [34] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. 2017. Graph Attention Networks. arXiv preprint arXiv:1710.10903 (2017). [35] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Nonlocal neural networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Vol. 1. 4. [36] Zhengyang Wang and Shuiwang Ji. 2018. Smoothed Dilated Convolutions for Improved Dense Prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining. ACM, 2486-2495. [37] Zhengyang Wang, Hao Yuan, and Shuiwang Ji. 2019. Spatial Variational AutoEncoding via Matrix-Variate Normal Distributions. In Proceedings of the 2019 SIAM International Conference on Data Mining. SIAM, 648-656. [38] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning. 2048-2057. [39] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. 2017. Shufflenet: An extremely efficient convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083 (2017).",
    "kroneckerattn-23": "[40] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. 2018. PSANet: Point-wise Spatial Attention Network for Scene Parsing.",
    "kroneckerattn-24": "In Proceedings of the European Conference on Computer Vision (ECCV).",
    "kroneckerattn-25": "$267-283$. [^0]:    Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '20, August 23-27, 2020, Virtual Event, USA\n    (c) 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7998-4/20/08...\\$15.00\n    https://doi.org/10.1145/3394486.3403065\n\n"
}