{
    "flurka-0": "# FLuRKA: Fast and accurate unified Low-Rank \\& Kernel Attention \n\n\\author{\nAhan Gupta <br> University of Illinois Urbana-Champaign <br> ag82@illinois.edu\n\nHao Guo<br>University of Illinois Urbana-Champaign<br>haoguo2@illinois.edu <br> Yueming Yuan <br> University of Illinois Urbana-Champaign <br> yy28@illinois.edu\n\n}\n\nYanqi Zhou<br>Google DeepMind<br>yanqiz@google.com\n\nCharith Mendis<br>University of Illinois Urbana-Champaign<br>charithm@illinois.edu\n\n\n#### Abstract\n\nMany efficient approximate self-attention techniques have become prevalent since the inception of the transformer architecture.",
    "flurka-1": "Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its strengths. We observe these strengths synergistically complement each other and exploit them to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA (Fast Low-Rank \\& Kernel Attention). FLuRKA are highly trainingefficient with faster model speeds and similar model qualities compared to constituent low-rank and kernel methods. We theoretically and empirically evaluate the speed and quality of FLuRKA. Our model speed analysis posits a variety of parameter configurations where FLuRKA exhibit speedups over low-rank and kernel approximations and our model quality analysis bounds the error of FLuRKA with respect to full-attention. Empirically, we instantiate three FLuRKA variants which experience speedups of up to 3.3 x and 1.7 x over low-rank and kernel methods respectively. This translates to speedups of up to 20x over models with flash-attention. Across a diverse set of tasks spanning language modeling, language understanding, long sequence modeling, machine translation, and image classification, FLuRKA achieve comparable accuracy with underlying low-rank and kernel approximations, occasionally surpassing both.",
    "flurka-2": "## 1 Introduction\n\nTransformers have been widely adopted across various domains, powering popular applications like ChatGPT, Gemini Pro, and Claude, which handle millions of queries per day Meyer et al. [2023]. To effectively train and deploy high-quality models at this scale, transformers must be training efficient. This entails balancing two crucial factors. (1) Model speed, which corresponds to models with low step-times and fewer FLOPs per step. (2) Model quality, which corresponds to expressive models that yield low losses with few training tokens. Highly training-efficient transformers are fast and of high quality. However, achieving both simultaneously is challenging, as higher quality transformers often require larger parameter counts and data-set sizes Li et al. [2020], Hoffmann et al. [2022], leading to slower model speeds. This trade-off is further exacerbated by the quadratic dependence in run-time\non increasing input sequence lengths ( 32 k for GPT-4, 100k for Claude, and 1.5 M for Gemini-pro OpenAI 2023], Anthropic [2023], Team et al. [2024]). To design training-efficient transformers, researchers have developed linear time self-attention approximations, falling into three main categories: sparse Child et al.",
    "flurka-3": "[2019], Kitaev et al. [2020], Zaheer et al. [2020], low-rank Wang et al. [2020], and kernel Choromanski et al. [2021], Katharopoulos et al. [2020], Peng et al.",
    "flurka-4": "[2021], Zheng et al. [2023, 2022] methods. Sparse methods compute a subset of the full-attention matrix, low-rank methods leverage the low-rank property of self-attention and kernel methods approximate the softmax kernel. While each category excels in specific tasks - sparse methods in document retrieval, low-rank methods in long-sequence modeling, and kernel methods in classification - their high model quality is limited to these narrow domains Tay et al.",
    "flurka-5": "[2020c]. To design training-efficient models across diverse tasks, researchers have experimented with unifying different linear time self-attention approximation techniques to combine their individual strengths. Scatterbrain (SB) Chen et al. [2021a] unifies kernel (K) and sparse (S) methods by computing an attention matrix whose values come from $S$ if the cosine similarity between a query and key is large, else comes from K. Though expressive, SB requires K and S to be entirely computed, resulting in $T(S B)>\\min (T(K), T(S))$ due to parallelism, where $T$ is the step-time (model speed) of a corresponding method. Long-short (LS) Zhu et al. [2021] unifies low-rank (LR) and sparse ( S ) methods by computing low-rank approximation and concatenating the attention matrix with a sparse local window of keys and values for each query. Though expressive, LS also requires the entire computation of LR and S to be computed, similarly resulting in $T(L S)>\\min (T(L R), T(S))$. Existing fusions are limited in their training-efficiencies due to model speeds lower-bounded by the faster of their constituent methods. Constructing a unification faster than both constituent models requires an additional approximation that partially computes at least one of them. However, naively doing so may reduce model expressivity and adversely impact quality. In this work, we bridge this gap by proposing a unification of low-rank (LR) and kernel (K) methods, FLuRKA (Fast and accurate Low-rank and Kernel Attention), a new class of high-quality unified transformers that is faster than its constituent models. We specifically construct a unification that exploits the orthogonal compute strengths of low-rank methods (efficient linear transformations) and kernel methods (efficient softmax kernel computations), resulting in model speeds faster than $\\min (T(L R), T(K))$. Additionally, we demonstrate our construction tightly approximates full attention and is of high-quality. Empirically, we validate FLuRKA and show they are up to 3.3 x and 1.7 x faster than constituent low-rank and kernel approximations with comparable model qualities across 6 diverse data-sets spanning image and text modalities. FLuRKA variants are highly training-efficient, requiring fewer FLOPs than full attention to achieve similar loss levels (see table 1). This significantly reduces the computational cost of training high-quality models for a diverse range of tasks. Specifically, we make the following contributions:\n\n- A technique to unify two classes of approximations: low-rank and kernel methods, to produce a new class of transformers, FLuRKA. FLuRKA are faster than constituent low-rank and kernel methods, incurring step-times (model speeds) faster than $\\min (T(L R), T(K))$ and are of high quality, resulting in high training-efficiencies. - A theoretical analysis of FLuRKA where we delineate precisely when FLuRKA are faster than low-rank and kernel methods and additionally show that FLuRKA have tight approximation error with respect to full-attention. - Empirical studies on three different instantiations of low-rank and kernel methods demonstrating the generality of our technique. Our studies show that FLuRKA are up to 1.7x and 3.3 x faster compared to kernel and low-rank methods, respectively.",
    "flurka-6": "Moreover on language modeling (on wikitext-103 Merity et al. [2016]), language understanding (on GLUE Wang et al. 2018]), long sequence modeling (on LRA Tay et al. 2020c|), machine translation (on\nenglish to german and english to french) and image classification (on imagenet Russakovsky et al. [2015]), FLuRKA are competitive with, and occasionally surpass, the low-rank and kernel methods that compose them. ## 2 Background and Related Work\n\nThe backbone of the transformer is multi-head-self-attention (MHSA) Vaswani et al. 2017. MHSA computes the following matrix: Concat $\\left(\\operatorname{Head}_{1}\\right.$, Head $_{2}, \\ldots$, Head $\\left._{h}\\right)$ where $\\mathrm{Head}_{i}$ is:\n\n$$\n\\underbrace{\\operatorname{softmax}\\left(Q W_{i}^{Q}\\left(K W_{i}^{K}\\right)^{T}\\right)}_{A_{i}} V W_{i}^{V}\n$$\n\nThe matrices $Q, K \\& V \\in \\mathbb{R}^{N \\times d_{m}}$ are the input matrices consisting of $N$ points embedded in $\\mathbb{R}^{d_{m}}$, where $d_{m}$ and $N$ are known as the embedding dimension and sequence length respectively. $W_{i}^{Q}$, $W_{i}^{K}$ and $W_{i}^{V} \\in \\mathbb{R}^{d_{m} \\times d_{h}}$ are linear transformations. The matrix $A_{i}$ is known as the attention matrix and the softmax is taken row-wise in the product $Q W_{i}^{Q}\\left(K W_{i}^{K}\\right)^{T}$. Self-attention is expensive due to the matrix $A_{i}$ being of size $O\\left(N^{2}\\right)$. ### 2.1 Efficient Approximations\n\nLow-rank Methods Low-rank methods exploit the observation that the matrix $A_{i}$ is of low-rank. Motivated by this, linformer [Wang et al., 2020], a SOTA low-rank technique, constructs a low-rank approximation of $\\mathrm{Head}_{i}$ via:\n\n$$\n\\operatorname{Softmax}\\left(Q W_{i}^{Q}\\left(E_{1} K W_{i}^{K}\\right)^{T}\\right) E_{2} V W_{i}^{V}\n$$\n\nWhere $E_{1}$ and $E_{2}$ are matrices in $\\mathbb{R}^{d_{k} \\times N}$ whose entries are sampled from $N\\left(0, \\frac{1}{d_{k}}\\right)$.",
    "flurka-7": "The resultant size of $A_{i}$ is $O\\left(N d_{k}\\right)$, where $d_{k}$ is the downsampling-factor: a hyperparameter that is set prior to training. In practice, $d_{k}$ is agnostic of $N$ and is much smaller, linearizing self-attention. Kernel Methods Kernel methods |Choromanski et al., 2021, Zheng et al., 2022, 2023, Katharopoulos et al. 2020] replace the softmax with a cheaper approximation. They map the input to a space where the dot-product approximates the softmax, computing the following for $\\mathrm{Head}_{i}$ :\n\n$$\n\\phi\\left(Q W_{i}^{Q}\\right)\\left(\\phi\\left(K W_{i}^{K}\\right)^{T} V W_{i}^{V}\\right)\n$$\n\nWhere $\\phi: \\mathbb{R}^{d_{h}} \\rightarrow \\mathbb{R}^{d_{p}}$ is a kernel with the property: $\\operatorname{softmax}(x, y) \\approx \\phi(x)^{T} \\cdot \\phi(y)$. Kernel methods do not materialize the matrix $A_{i}$ directly, instead by multiplying out the matrices in an optimal manner, they linearize self-attention to $O\\left(N d_{p}^{2}\\right)$. There are two categories of kernel methods: random feature and non-random-feature-mapped kernels. Random feature mapped kernels are parameterised by $m$ i.i.d random-variables $\\psi_{1}, \\psi_{2}, \\ldots \\psi_{m}$ with $\\psi_{i}: \\mathbb{R}^{d_{h}} \\rightarrow \\mathbb{R}$, where $\\mathbb{E}\\left[\\psi_{i}(x)^{T} \\cdot \\psi_{i}(y)\\right]=\\exp \\left(x^{T} \\cdot y\\right)$, and are defined as: $\\phi(x)=\\frac{1}{\\sqrt{m}}\\left[\\psi_{1}(x), \\psi_{2}(x), \\ldots \\psi_{m}(x)\\right]$. Non-random feature mapped kernels have no element of randomness and instead use kernels like the exponential linear unit Katharopoulos et al., 2020]. We fuse low-rank methods with both random (performer, EVA) and non-random (RNN) feature mapped kernel methods. ### 2.2 Unified Attention Mechanisms\n\nExisting unified attention methods exploit the synergies between disparate attention mechanisms for higher model quality. Scatterbrain [Chen et al., 2021a] unifies sparse with kernel methods whereas longshot Zhu et al. [2021] unifies sparse with low-rank methods. However, both these existing fusions have model speeds lower-bounded by the faster of their respective constituents, limiting their training-efficiencies. Although we also propose a unification of different transformers, to the best of our knowledge we are the first to explore a unification: (1) of low-rank and kernel methods, (2) that produces high quality transformers with faster model speeds than both constituent models. ## 3 FLuRKA: Fused Low-Rank and Kernel Attention\n\nConstructing unified high-quality transformers whose model speeds are faster than their constituents is challenging due to two reasons. (1) An additional approximation is required to partially compute\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_c1212708b0e2415c0653g-04.jpg?height=711&width=1392&top_left_y=238&top_left_x=364)\n\nFigure 2: The pipeline of operations involved in constructing FLuRKA, parameterized by a kernel $\\phi$, compared to low-rank and kernel methods.",
    "flurka-8": "$N, d_{m}, d_{k}$ are the sequence length, hidden dimension, and downsampling factors, respectively. $d_{p}$ is the dimension of the kernelized queries and keys. at least one constituent method to enhance model speed. (2) This approximation cannot adversely impact the unified model's quality. Nevertheless, we propose a unification of low-rank (LR) and kernel (K) methods that achieves both attributes. In doing so, we produce high-quality transformers whose model speeds are faster than $\\min (T(L R), T(K)$ ), resulting in high training-efficiencies. Moreover, our technique is general, enabling a unification of any low-rank and kernel method, producing a family of transformer models. Naive Unification. We observe that the attention matrix implicitly materialized through cheaper softmax kernel approximations: $\\phi\\left(Q W_{i}^{Q}\\right) \\phi\\left(K W_{i}^{K}\\right)$ is of low-rank. This is demonstrated in figure 1 , where we compute the number of singular values for every attention-head in every alternate layer of a pre-trained performer model. The size of the attention matrix is $128 \\times 128$, yet the attention-head of highest rank is 64 (the size of the head hidden dimension) with the rank of the attention-heads decreasing in later layers. Therefore, we can apply low-rank (LR) approximation over the kernelized $(\\mathrm{K})$ attention matrix to unify the two techniques. The full unification of $H e a d_{i}$ will look as follows:\n\n$$\n\\phi\\left(Q W_{i}^{Q}\\right)\\left(E_{1} \\phi\\left(K W_{i}^{K}\\right)\\left(E_{2} V W_{i}^{V}\\right)\\right)\n$$\n\nAlthough this maintains model expressivity, the resulting model is slow as the entire low-rank and kernel approximations need to be computed. Moreover, they cannot be parallelised, resulting in model speeds lower-bounded by $T(L R)+$ $T(K)$. Optimized Unification. To propose an optimized unification whose model speed is faster than equation 1 and is of comparable model quality, we observe that low-rank and kernel methods enhance the speed of self-attention in orthogonal ways. On the one hand, low-rank methods contract the keys and values through multiplication with the $E_{1} \\& E_{2}$ matrices, reducing the costs of linear-transformations and\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_c1212708b0e2415c0653g-04.jpg?height=404&width=659&top_left_y=1755&top_left_x=1080)\n\nFigure 1: We take 12-layer 12-head performer pretrained on wikitext-103 and plot the number of unique singular values in the SVD of the kernelized attention matrix: $\\phi\\left(Q W^{Q}\\right) \\phi\\left(K W^{K}\\right)$ for every alternate attention head in each layer.",
    "flurka-9": "$A_{i}\\left(E_{i} V W_{i}^{V}\\right)$ products. On the other hand, kernel methods cheaply approximate the softmax kernel through an inexpensive kernel mapping of the queries and keys, reducing the costs of softmax kernels. A faster unified model will therefore incur low linear-transformation and softmax approximation cost. This only occurs when the keys and values are contracted before the linear-transformations and softmax approximation, which requires low-rank (LR) approximation to be applied before kernel $(\\mathrm{K})$ approximation, in contrast to the naive unification in equation 1. This gives us the optimised construction of FLuRKA as follows:\n\n$$\n\\phi\\left(Q W^{Q}\\right)\\left(\\phi\\left(\\left(E_{1} K\\right) W^{K}\\right)\\left(\\left(E_{2} V\\right) W^{V}\\right)\\right)\n$$\n\nSuch a unification incurs model speeds faster than $\\min (T(L R), T(K))$ while retaining the quality of constituent models, resulting in highly training-efficient models. We theoretically analyze the speed and quality of equation 2. Claim 1 gives a tight bound delineating for what input sequence lengths FLuRKA incur speeds faster than $\\min (T(L R), T(K))$, and theorem 1 tightly bounds the approximation error with respect to full-attention. Model speed theoretical analysis. We theoretically analyze the model speed of FLuRKA and delineate precisely when FLuRKA are faster than their underlying low-rank and kernel approximations. Claim 1. For sequence length: $N$, hidden dimension: $d_{m}$, downsampling factor: $d_{k}$, head hidden dimension $d_{h}$, number of heads $H$, when:\n\n$$\nN>d_{k}(H+2)>d_{m}>d_{k}>d_{h}\n$$\n\nFLuRKA incur fewer FLOPs against both kernel and low-rank methods. We note that claim 1 is tight, resulting in faster models for input sequence lengths as short as several thousand. We further describe two additional scenarios where FLuRKA are faster than their constituents in Appendix B. Model quality theoretical analysis. The following theorem bounds the approximation error with respect to full-attention for a generic FLuRKA under reasonable assumptions. Theorem 1. Suppose we have a random feature map $\\phi$ defined as follows:\n\n$$\n\\phi(x)=\\frac{1}{\\sqrt{m}}\\left[\\psi_{1}(x), \\psi_{2}(x), \\ldots \\psi_{m}(x)\\right]\n$$\n\nsuch that:\n\n$$\n\\mathbb{E}\\left[\\psi_{i}(x)^{T} \\cdot \\psi_{i}(y)\\right]=\\exp \\left(x^{T} \\cdot y\\right)\n$$\n\nThen for any $Q_{i}, K_{i}, V_{i} \\in \\mathbb{R}^{n \\times d_{m}}$ and $W_{i}^{Q}, W_{i}^{K}, W_{i}^{V} \\in \\mathbb{R}^{d_{m} \\times d_{h}}$, and $k=5 \\log (d) /\\left(\\epsilon_{2}^{2}-\\epsilon_{3}^{2}\\right)$. We have, for the matrices $E_{i}=\\delta R, F_{i}=e^{-\\delta} R$ where $R \\in \\mathbb{R}^{n \\times k}$ whose entries are iid sampled from $N(0,1 / k)$ and a random feature based kernel method parameterised by $\\phi$, with $\\epsilon_{4}>0$ :\n\n$$\n\\left\\|\\phi\\left(Q W_{i}^{Q}\\right) \\phi\\left(E_{1} K W_{i}^{K}\\right)^{T} F_{i} V W_{i}^{V}-A_{i} V W_{i}^{V}\\right\\|_{\\infty}<\\epsilon_{4}\\left\\|F_{i} V W_{i}^{V}\\right\\|_{\\infty}+\\epsilon_{1}\\left\\|A_{i}\\right\\|_{2}\\left\\|V W_{i}^{V}\\right\\|_{2}\n$$\n\nOccurs with probability at least $1-o(1)$ for large enough $m$. Proof Sketch. Given $\\mathbb{E}\\left[\\psi_{i}(x)^{T} \\cdot \\psi_{i}(y)\\right]=\\exp \\left(x^{T} \\cdot y\\right)$. We can show, by the law of large numbers, that:\n\n$$\n\\lim _{m \\rightarrow \\infty} \\phi(x)^{T} \\cdot \\phi(y)=\\exp \\left(x^{T} \\cdot y\\right)\n$$\n\nWhich allows us to bound the error of a kernel method by stating that for an $\\epsilon_{4}>0 \\exists m \\in \\mathbb{Z}^{+}$:\n\n$$\n\\left\\|\\hat{A}_{i}-A_{i}\\right\\|_{\\infty}<\\epsilon_{4}\n$$\n\nwhere $\\hat{A}_{i}$ is the attention matrix materialised by a kernel method. We can then substitute $K=E_{1} K$ and right multiply 4 by $\\left\\|F_{i} V W_{i}^{V}\\right\\|_{\\infty}$ to bound the error of a FLuRKA with respect to low-rank:\n$\\left\\|\\phi\\left(Q W_{i}^{Q}\\right) \\phi\\left(E_{1} K W_{i}^{K}\\right)^{T} F_{i} V W_{i}^{V}-\\operatorname{softmax}\\left(Q W_{i}^{Q}\\left(E_{1} K W_{i}^{K}\\right)^{T}\\right) F_{i} V W_{i}^{V}\\right\\|_{\\infty}<\\epsilon_{4}\\left\\|F_{i} V W_{i}^{V}\\right\\|_{\\infty}$\nWe then apply the triangle inequality to the sum of 5 and linformer theorem 2 Wang et al., 2020] to recover our bound.",
    "flurka-10": "For a detailed proof of theorem 11 refer to the Appendix. ![](https://cdn.mathpix.com/cropped/2024_09_12_c1212708b0e2415c0653g-06.jpg?height=430&width=1392&top_left_y=243&top_left_x=365)\n\nFigure 3: Comparing inference times of all variants as sequence lengths increase. All the models have the same parameter count and were run for 100 iterations of inference. Our method (FLuRKA) are in green, low-rank is in blue \\& kernel is in orange. Lower is better. |  | Model | COLA <br> (m) | SST-2 <br> (a) | MRPC <br> (f1/a) | STS-B <br> $(\\mathrm{p} / \\mathrm{s})$ | QQP <br> (f1/a) | MNLI <br> (a) |  | QNLI <br> (a) | RTE <br> (a) | Average |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Full Attention | 48.9 | 90.7 | 89.1/84.8 | 85.5/85.6 | 86.4/90.0 | 81.3 | 81.8 | 88.2 | 65.7 | 81.5 |\n| \u0e01 | Linformer | 29.2 | 90.0 | 86.3/79.1 | 82.4/82.5 | 84.1/88.1 | 76.9 | 76.4 | 83.9 | 59.9 | 76.6 |\n| \uff01\uff01\uff09 | Performer | 39.0 | 90.3 | 85.1/76.9 | 81.2/81.1 | 84.7/88.6 | 78 | 77.5 | 84.5 | 62.0 | 77.4 |\n| \u7a74 | RNN | 39.9 | 89.2 | 84.3/76.4 | 81.3/81.4 | 84.4/88.3 | 77.4 | 76.6 | 83.4 | 60.6 | 76.9 |\n| x | EVA | 51.5 | 91.1 | 82.9/73.8 | 72.8/72.9 | 83.9/88.0 | 75.4 | 76.1 | 80.2 | 59.2 | 75.7 |\n| $\\mathbb{k}$ | Lin.-Perf. | 44.1 | 89.9 | 85.9/78.6 | 80.9/81.0 | 84.1/88.1 | 75.6 | 76.2 | 82.4 | 61.3 | 77.3 |\n| \u7eff | Lin.-RNN | 45.3 | 91.0 | 82.3/72.7 | 78.9/79.0 | 84.3/88.3 | 77.4 | 77.1 | 83.4 | 60.6 | 76.7 |\n| \u67b5 | Lin.-EVA | 38.7 | 89.1 | 83.2/75.4 | 79.2/79.2 | 83.7/87.9 | 75.7 | 75.6 | 82.2 | 64.6 | 76.2 |\n\nTable 2: A comparison of the GLUE scores in between the full-attention, low-rank (linformer), kernel methods and FLuRKA. MNLI has two tasks: matched ( $6^{\\text {th }}$ column) and mismatched ( $7^{\\text {th }}$ column). ## 4 Evaluation\n\nThe design of our experiments is motivated by the following question: are FLuRKA empirically more training-efficient compared to underlying low-rank and kernel methods? To answer this question, we instantiate three FLuRKA variants which unify linformer (low-rank approximation) with performer [Choromanski et al., 2021], RNN [Katharopoulos et al., 2020], and EVA [Zheng et al., 2023] (kernel approximation). We assess the model speed and quality of the three FLuRKA variants compared to underlying low-rank and kernel approximations. Section 4.1 details our experiments on model speed, and section 4.2 details our experiments on model quality. Section 4.3 details our ablation studies. Experimental Setup. All our experiments were done on a cluster of 4 A100GPUs with 80GB of memory that are pairwise NVLinked. We use Jax 0.4.4 with cuda 11.0, cudnn 8.2, and Pytorch 1.8.1 for data-loading.",
    "flurka-11": "For inference benchmarks (model speed), unless otherwise stated, our models are 6 layers with a FFN size of 512 with 4 attention heads. For model quality benchmarks, refer to the Appendix for a detailed configuration of the model sizes we use. ### 4.1 Model speed\n\nImpact of Increasing Sequence Length. We set $d_{m}(=2600)>d_{k}(=1500)>d_{h}(=325)$, with $H$ (number of heads) to 8 following claim 1 . We vary $N$ from 7.05 k to 55.5 k in increments of 3 k . These are realistic scenarios of parameter configurations. For example, GPT- 4 has a sequence length of $32 k$ [OpenAI 2023], and Claude has a sequence length of 100k [Anthropic, 2023]. Figure 3shows the result. As the sequence length increases, the speedups of FLuRKA over low-rank and kernel methods increase. This culminates in the linformer-performer variant experiencing a 1.86 x and 1.37 x , the linformer-RNN variant experiencing a 1.83 x and 1.37 x , and the linformer-EVA variant experiencing a 1.86 x and 1.31 x speedup over each of its respective constituent low-rank and kernel method. |  | Model | ListOps | Text | Retrieval | Image | Path | Average |\n| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Full Attention | 37.95 | 60.18 | 80.45 | 37.39 | 70.19 | 57.23 |\n|  | Linformer | 37.85 | 55.80 | 78.61 | 37.72 | 64.75 | 54.95 |\n|  | Performer | 38.51 | 59.62 | 80.77 | 38.14 | 71.24 | $\\underline{57.66}$ |\n| \u0e2d | RNN | 37.20 | 64.98 | 76.92 | 38.19 | 68.58 | 57.17 |\n|  | EVA | 38.61 | 65.16 | 81.00 | 41.60 | 69.27 | $\\mathbf{5 9 . 1 3}$ |\n| Lin.-Perf. | 37.55 | 57.61 | 65.82 | 39.30 | 66.68 | 53.40 |  |\n| Lin.-RNN | 36.84 | 58.46 | 75.72 | 37.60 | 66.27 | 54.98 |  |\n| Lan.-EVA | 37.30 | 60.15 | 74.35 | 42.29 | 68.48 | 56.51 |  |\n\nTable 3: A comparison of the LRA scores in between the full-attention, low-rank (linformer), kernel methods and FLuRKA. Speedups over full-attention and flash-attention. The speedups attained over low-rank and kernel methods are non-trivial and compound against full-attention. Figure 4 (top) and 4 (bottom) illustrate this compounding effect with FLuRKA variants experiencing speedups of up to 25.1x over fullattention. Moreover, FLuRKA experience speedups of up to 23 x over SOTA flash-attention Dao et al. [2022]. See Appendix C for our evaluation against flash-attention. Model speed discussion. Intuitively, these speedups occur because FLuRKA inherit the performance strengths of both underlying kernel and low-rank methods. Speedups against kernel methods occur because the hidden dimension is large enough such that the following operations: (1) linear transformations and (2) $K V$ products contribute significantly to run-time. FLuRKA, in applying low-rank approximation prior to these operations cut down the number of keys and values and accordingly massively reduce the cost of this product. Speedups against low-rank methods occur because the $d_{k} \\& N$ are large enough such that the following operations: (1) softmax on $A_{i}$ and (2) $Q K^{T} \\& A_{i} V$ products contribute significantly to runtime. FLuRKA, in cheaply approximating the softmax as well as re-ordering the matrix multiplications offset some of this cost. ### 4.2 Model quality\n\nWe train our models across a variety of tasks: language modeling (Wikitext-103), language understanding (GLUE), long sequence modeling (LRA), machine translation (English to German and English to French), and image classification (ImageNet). See Appendix for a detailed setup of the hyperparameters we use. Masked Language Modeling. We pre-train with the masked language modeling objective on wikitext-103 for 120k steps. Our results are in table 4 . The linformer-EVA variant outperforms linformer by $5.5 \\%$ and is within $13 \\%$ of EVA, while the linformer-RNN variant outperforms linformer by $5 \\%$ and is within $0.8 \\%$ of RNN. The linformer-performer variant is within $2 \\%$ of linformer \\& $9 \\%$ of performer's perplexity scores. Language Understanding. We fine-tune on GLUE after pre-training on wikitext-103. Our results are in table 4 The linformer-performer variant outperforms linformer by $0.9 \\%$ and is within $0.1 \\%$ of performer. The linformer-RNN variant outperforms linformer by $0.2 \\%$ and is within $0.2 \\%$ of RNN. The linformer-EVA variant surpasses EVA by $0.6 \\%$ and is within $0.5 \\%$ of linformer.",
    "flurka-12": "Long Sequence Modeling. We train on the LRA benchmark following the setup in Chen et al. [2021b]. Our results are in table 3. The linformer-performer variant is within 2.9\\% of linformer and $7.9 \\%$ of performer. The linformer-RNN variant outperforms linformer by $0.05 \\%$ and is within $3.9 \\%$ of RNN. The linformer-EVA variant outperforms linformer by $2.7 \\%$ and is within $4.6 \\%$ of EVA. Auto-regressive Machine Translation. We train on the english to german and english to french machine translation tasks for 128 k steps using the $t 5 x^{1}$ library. Our results are in table 4 We observe that the linformer-EVA variant outperforms linformer, performer, RNN \\& EVA by at least $1 \\%$. The\n\n[^0]| Model | NMT |  | ImageNet |  | Wikitext-103 |  |\n| :--- | :--- | :---: | :---: | :---: | :---: | :---: |\n|  | En $\\rightarrow$ De | En $\\rightarrow$ Fr | Top-1 | Top-5 | Perplexity |  |\n|  | Full Attention | 24.65 | 32.15 | 71.662 | 90.982 | 5.533 |\n|  | Linformer | $\\underline{24.37}$ | 31.82 | 70.272 | 89.802 | 7.696 |\n|  | 22.38 | 29.99 | 67.52 | 88.144 | $\\underline{7.206}$ |  |\n| RNN | 22.52 | 29.36 | 67.43 | 87.98 | 7.228 |  |\n| EVA | 24.16 | $\\underline{32.12}$ | $\\underline{70.322}$ | $\\underline{89.994}$ | $\\mathbf{6 . 4 3 2}$ |  |\n| Linformer-Performer | 24.30 | 31.21 | 67.592 | 88.23 | 7.867 |  |\n| Linformer-RNN | 22.72 | 29.73 | 67.416 | 88.106 | 7.293 |  |\n|  | Linformer-EVA | $\\mathbf{2 4 .",
    "flurka-13": "6 1}$ | $\\mathbf{3 2 .",
    "flurka-14": "3 7}$ | $\\mathbf{7 0 . 6 6 2}$ | $\\mathbf{9 0 . 3 6 6}$ | 7.271 |\n\nTable 4: A comparison between all the models on 3 tasks: machine translation, image classification and masked language modeling. En $\\rightarrow \\mathrm{De}$ and $\\mathrm{En} \\rightarrow \\mathrm{Fr}$ are the English to German and English to French translation tasks respectively. linformer-performer variant outperforms performer by $8.5 \\%$ and is within $0.3 \\%$ of linformer's BLUE. The linformer-RNN variant outperforms RNN by $0.9 \\%$ and is within $6 \\%$ of linformer's BLUE. Image Classification. We train a T2T-ViT Yuan et al. [2021] on imagenet for 350 epochs. Our results are in table 4 We observe that the linformer-EVA variant outperforms linformer, performer, RNN \\& EVA by at least $0.4 \\%$ across both top-1 and top-5 accuracy. The linformer-performer variant outperforms performer by $0.01 \\%$ (top-1) and $0.009 \\%$ (top-5) and is within 3\\% (top-1) and 2\\% (top-5) of linformer. The linformer-RNN variant is within $0.02 \\%$ (top-1) and $0.1 \\%$ (top-5) of RNN as well as within $3.8 \\%$ (top-1) and $1 \\%$ (top-5) of linformer. Model quality discussion. Low-rank and kernel methods exhibit high model quality across different tasks with FLuRKA unifying their strengths. Our results indicate that low-rank methods are good at image classification and machine-translation, while kernel methods are good at language understanding and masked language modeling. Nevertheless, across all 6 workloads (GLUE, wikitext-103, LRA, En $\\rightarrow \\mathrm{De}$, En $\\rightarrow \\mathrm{Fr}$ and imagenet) and 3 FLuRKA variants resulting in 18 data-points, 12 rank between their underlying low-rank and kernel approximations, 3 rank better than both, while 3 rank worse than both. Moreover, most of the 12 data points that rank between the quality of underlying low-rank and kernel approximations closely approach the better of the two. Despite theorem 1 indicating that FLuRKA's approximation error is the sum of underlying lowrank and kernel approximations, empirically FLuRKA's model quality approaches the better of its constituent models. In practice, the gap in theorem 1 is mitigated by up-training Komatsuzaki et al. [2023], Ainslie et al. [2023], where an $\\alpha$ fraction of the training steps trains either a low-rank or kernelized base model, thereafter training a FLuRKA variant for the remaining $1-\\alpha$ fraction of steps whose parameters are initialized with the base model. Our ablations in section 4.3 indicate that up-training averages the approximation error of underlying low-rank and kernel approximations. Moreover, tuning $\\alpha$ results in FLuRKA variants whose quality approaches the better of the two. ### 4.3 Ablations\n\nWe conduct two sets of ablations studies investigating the impact of hyperparameters on (1) model speed (see section 4.3.1), and (2) model quality (see section 4.3.2. Our model speed ablations ascertain the impact of the downsampling factor (a hyperparameter introduced by low-rank approximation) and hidden dimension on step-time. Our model quality ablations ascertain the impact of up-training. ### 4.3.1 Model speed\n\nImpact of Increasing Downsampling Factor.",
    "flurka-15": "We set $N(=20 k)>d_{k}>d_{h}(=128)$. We vary $d_{k}$ from 8 k to 20 k in increments of 3.2 k . The sequence lengths and head hidden dimension are similar to other SOTA models. Figure 4 (top) shows the result normalized to speedups over full-attention. As the downsampling factor increases, the speedup of all the FLuRKA over low-rank magnifies. This culminates in the linformer-performer variant incurring a 3.39 x , the linformer-RNN variant incurring a 3.38 x , and the linformer-EVA variant incurring a 3.12 x speedup over linformer (low-rank). ![](https://cdn.mathpix.com/cropped/2024_09_12_c1212708b0e2415c0653g-09.jpg?height=950&width=1321&top_left_y=249&top_left_x=402)\n\nFigure 4: The impact of the downsampling factor and hidden dimension on runtime performance (model speeds) normalized to speedups over full-attention.",
    "flurka-16": "The top figure compares FLuRKA to lowrank methods and the bottom figure compares FLuRKA to kernel methods. Our methods (FLuRKA) are highlighted in hashed bars, while low-rank and kernel methods are highlighted in solid bars. | Kernel |  | Low-rank | FLuRKA |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  |  | $12.5 \\%$ | $25 \\%$ | $37.5 \\%$ | 50\\% | $62.5 \\%$ | $75 \\%$ | 87.5\\% |\n| Performer | 27.97 | 28 | 27.98 | 27.68 | 28.00 | 27.86 | 27.86 | 27.56 | 27.66 |\n| RNN | 23.41 |  | 23.94 | 24.57 | 24.23 | 24.42 | 24.34 | 24.25 | 23.45 |\n| EVA | 28.54 |  | 28.70 | 28.72 | 28.87 | 28.65 | 28.73 | 28.66 | 28.64 |\n\nTable 5: We compare FLuRKA variants up-trained on different $\\alpha$ from $12.5 \\%$ to $87.5 \\%$ to constituent low-rank and kernel methods on the english to german machine-translation task.",
    "flurka-17": "The first, second, and third rows correspond to the linformer-performer, linformer-RNN, and linformer-EVA variants, respectively. We report the BLEU scores of each run. Impact of Increasing Hidden Dimension. We set $N(=20 k)>d_{k}(H+2)(=6 k), d_{m}>d_{k}(=1 k)$. We vary $d_{m}$ from 800 to 20 k in increments of 3.2 k . Figure 4 (bottom) shows the result normalized to speedups over full-attention. As the hidden dimension increases, the speedup of all the FLuRKA over their kernel methods intensifies. This culminates in the linformer-performer variant enjoying a 1.72 x , the linformer-RNN variant enjoying a 1.72 x and the linformer-EVA variant enjoying a 1.71 x speedup over each of its kernel methods respectively. ### 4.3.2 Model quality\n\nImpact of up-training on model quality. We up-train FLuRKA with different ratios of $\\alpha$ (percent of the training steps spent on the base model) and compare their quality to constituent kernel and low-rank methods. We use the english to german machine-translation task. Our results are in table 5 We note 3 observations. (1) Majority of the up-trained FLuRKA variants, 16 out of 21, rank between constituent low-rank and kernel methods. (2) The highest quality linformer-performer and linformerEVA variants outperform all the kernel and low-rank methods. (3) The highest quality up-trained FLuRKA variants have ratios of $\\alpha$ within the $[25 \\%, 37.5 \\%]$ range. These observations imply that\nup-training averages the approximation error of constituent low-rank and kernel methods, producing FLuRKA variants whose quality approaches the higher-quality constituent model. Moreover, the optimal $\\alpha$ ratios are low, resulting in a small fraction of time spent training the base-model. ## 5 Conclusion\n\nWe propose a new technique to unify low-rank and kernel methods, producing a family of transformers, FLuRKA. FLuRKA are fast, incurring end-to-end speedups of up to 1.7 x and 3.3 x over kernel and low-rank methods respectively. Moreover, they are of high quality across diverse tasks. Across 6 benchmarks ranging text and image modalities FLuRKA are competitive with, and occasionally surpass, the underlying low-rank and kernel methods that compose them. As a result of their trainingefficient nature, FLuRKA require fewer FLOPs to attain similar loss levels compared to full-attention, reducing the computational costs of training high-quality models for a diverse range of tasks. ## Acknowledgements\n\nWe would like to thank Hao Peng for introducing us to the EVA model, Krzysztof Choromanski for his help in how to train transformers to achieve SOTA results, Tianle Cai for his insight on how to produce a general theoretical claim, as well as Minjia Zhang and Wanyu Zhao for their feedback on the draft of the paper.",
    "flurka-18": "This work is in part supported by ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA and generous support from Google Cloud Credits.",
    "flurka-19": "## References\n\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr\u00f3n, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. Anthropic. Introducing 100k context windows, 2023. URLhttps://www.anthropic.com/index/ 100 k -context-windows.",
    "flurka-20": "I. Beltagy, M. E.",
    "flurka-21": "Peters, and A. Cohan. Longformer: The long-document transformer.",
    "flurka-22": "CoRR, abs/2004.05150, 2020.",
    "flurka-23": "URL https://arxiv.org/abs/2004.05150. J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URLhttp://github.com/google/jax.",
    "flurka-24": "T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners.",
    "flurka-25": "CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165\nB. Chen, T. Dao, E. Winsor, Z. Song, A. Rudra, and C. R\u00e9. Scatterbrain: Unifying sparse and low-rank attention approximation. CoRR, abs/2110.15343, 2021a.",
    "flurka-26": "URL https://arxiv.org/ abs/2110.15343. Y. Chen, Q. Zeng, H.",
    "flurka-27": "Ji, and Y. Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr\u00f6m method.",
    "flurka-28": "CoRR, abs/2111.00035, 2021b. URL https://arxiv.org/abs/2111.00035. R. Child, S. Gray, A.",
    "flurka-29": "Radford, and I. Sutskever. Generating long sequences with sparse transformers.",
    "flurka-30": "CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509. K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zukOWRH. A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways, 2022.",
    "flurka-31": "G. M. Correia, V. Niculae, and A.",
    "flurka-32": "F. T. Martins. Adaptively sparse transformers. CoRR, abs/1909.00015, 2019. URL http://arxiv.org/abs/1909.00015. T. Dao, D. Y. Fu, S. Ermon, A.",
    "flurka-33": "Rudra, and C. R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022.",
    "flurka-34": "J. Devlin, M. Chang, K.",
    "flurka-35": "Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding.",
    "flurka-36": "CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810 04805\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.",
    "flurka-37": "CoRR, abs/2101.03961, 2021. URL https://arxiv.org/abs/ 2101.03961\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L.",
    "flurka-38": "A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models, 2022. W. Hua, Z. Dai, H. Liu, and Q.",
    "flurka-39": "V. Le. Transformer quality in linear time, 2022.",
    "flurka-40": "A. Katharopoulos, A. Vyas, N.",
    "flurka-41": "Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "flurka-42": "CoRR, abs/2006.16236, 2020. URL https://arxiv.org/ abs/2006.16236\nN. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. CoRR, abs/2001.04451, 2020. URL https://arxiv.org/abs/2001.04451. A. Komatsuzaki, J. Puigcerver, J. Lee-Thorp, C. R. Ruiz, B. Mustafa, J. Ainslie, Y. Tay, M. Dehghani, and N. Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints, 2023.",
    "flurka-43": "Z. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D.",
    "flurka-44": "Klein, and J.",
    "flurka-45": "E. Gonzalez. Train large, then compress: Rethinking model size for efficient training and inference of transformers.",
    "flurka-46": "CoRR, abs/2002.11794, 2020. URL https://arxiv.org/abs/2002.11794. S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models.",
    "flurka-47": "CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843. J. G. Meyer, R. J. Urbanowicz, P. C. Martin, K. O\u2019Connor, R. Li, P.-C. Peng, T. J. Bright, N. Tatonetti, K.",
    "flurka-48": "J.",
    "flurka-49": "Won, G. Gonzalez-Hernandez, et al. Chatgpt and large language models in academia: opportunities and challenges. BioData Mining, 16(1):20, 2023. OpenAI. Gpt-4 technical report, 2023. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K\u00f6pf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. CoRR, abs/1912.01703, 2019. URLhttp://arxiv.org/abs/1912.01703. H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. Smith, and L. Kong. Random feature attention. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=QtTKTdVrFBB\n\nJ. Qiu, H. Ma, O. Levy, S. W. Yih, S. Wang, and J. Tang. Blockwise self-attention for long document understanding. CoRR, abs/1911.02972, 2019. URLhttp://arxiv.org/abs/1911.02972. C. Raffel, N. Shazeer, A.",
    "flurka-50": "Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P.",
    "flurka-51": "J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "flurka-52": "Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html\nA. Rahimi and B. Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL https://proceedings.neurips.cc/paper_ files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf\nA. Roy, M. Saffar, A. Vaswani, and D. Grangier. Efficient content-based sparse attention with routing transformers.",
    "flurka-53": "CoRR, abs/2003.05997, 2020.",
    "flurka-54": "URL https://arxiv.org/abs/2003.05997. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge, 2015.",
    "flurka-55": "Y. Tay, D. Bahri, D. Metzler, D. Juan, Z. Zhao, and C. Zheng. Synthesizer: Rethinking self-attention in transformer models.",
    "flurka-56": "CoRR, abs/2005.00743, 2020a. URL https://arxiv.org/abs/2005 00743\nY. Tay, D. Bahri, L. Yang, D. Metzler, and D. Juan. Sparse sinkhorn attention. CoRR, abs/2002.11296, 2020b. URL https://arxiv.org/abs/2002.11296. Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range arena: A benchmark for efficient transformers. CoRR, abs/2011.04006, 2020c. URL https://arxiv.org/abs/2011.04006\nY. Tay, M. Dehghani, D. Bahri, and D. Metzler. Efficient transformers: A survey. CoRR, abs/2009.06732, 2020d. URL/https://arxiv.org/abs/2009.06732\nG. Team, M. Reid, N. Savinov, D. Teplyashin, Dmitry, Lepikhin, T. Lillicrap, J. baptiste Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, I. Antonoglou, R. Anil, S. Borgeaud, A. Dai, K. Millican, E. Dyer, M. Glaese, T. Sottiaux, B. Lee, F. Viola, M. Reynolds, Y. Xu, J. Molloy, J. Chen, M. Isard, P. Barham, T. Hennigan, R. McIlroy, M. Johnson, J. Schalkwyk, E. Collins, E. Rutherford, E. Moreira, K. Ayoub, M. Goel, C. Meyer, G. Thornton, Z. Yang, H. Michalewski, Z. Abbas, N. Schucher, A. Anand, R. Ives, J. Keeling, K. Lenc, S. Haykal, S. Shakeri, P. Shyam, A. Chowdhery, R. Ring, S. Spencer, E. Sezener, L. Vilnis, O. Chang, N. Morioka, G. Tucker, C. Zheng, O. Woodman, N. Attaluri, T. Kocisky, E. Eltyshev, X. Chen, T. Chung, V. Selo, S. Brahma, P. Georgiev, A. Slone, Z. Zhu, J. Lottes, S. Qiao, B. Caine, S. Riedel, A. Tomala, M. Chadwick, J. Love, P. Choy, S. Mittal, N. Houlsby, Y. Tang, M. Lamm, L. Bai, Q. Zhang, L. He, Y. Cheng, P. Humphreys, Y. Li, S. Brin, A. Cassirer, Y. Miao, L. Zilka, T. Tobin, K. Xu, L. Proleev, D. Sohn, A. Magni, L. A. Hendricks, I. Gao, S. Ontanon, O. Bunyan, N. Byrd, A. Sharma, B. Zhang, M. Pinto, R. Sinha, H. Mehta, D. Jia, S. Caelles, A. Webson, A. Morris, B. Roelofs, Y. Ding, R. Strudel, X. Xiong, M. Ritter, M. Dehghani, R. Chaabouni, A. Karmarkar, G. Lai, F. Mentzer, B. Xu, Y. Li, Y. Zhang, T. L. Paine, A. Goldin, B. Neyshabur, K. Baumli, A. Levskaya, M. Laskin, W. Jia, J. W. Rae, K. Xiao, A. He, S. Giordano, L. Yagati, J.-B. Lespiau, P. Natsev, S. Ganapathy, F. Liu, D. Martins, N. Chen, Y. Xu, M. Barnes, R. May, A. Vezer, J. Oh, K. Franko, S. Bridgers, R. Zhao, B. Wu, B. Mustafa, S. Sechrist, E. Parisotto, T. S. Pillai, C. Larkin, C. Gu, C. Sorokin, M. Krikun, A. Guseynov, J. Landon, R. Datta, A. Pritzel, P. Thacker, F. Yang, K. Hui, A. Hauth, C.-K. Yeh, D. Barker, J. Mao-Jones, S. Austin, H. Sheahan, P. Schuh, J. Svensson, R. Jain, V. Ramasesh, A. Briukhov, D.-W. Chung, T. von Glehn, C. Butterfield, P. Jhakra, M. Wiethoff, J. Frye, J. Grimstad, B. Changpinyo, C. L. Lan, A. Bortsova, Y.",
    "flurka-57": "Wu, P. Voigtlaender, T. Sainath, S. Gu, C. Smith, W. Hawkins, K. Cao, J. Besley, S. Srinivasan, M. Omernick, C. Gaffney, G. Surita, R. Burnell, B. Damoc, J. Ahn, A. Brock, M. Pajarskas, A. Petrushkina, S. Noury, L. Blanco, K. Swersky, A. Ahuja, T. Avrahami, V. Misra, R. de Liedekerke, M. Iinuma, A. Polozov, S. York, G. van den Driessche, P. Michel, J. Chiu, R. Blevins, Z. Gleicher, A. Recasens, A. Rrustemi, E. Gribovskaya, A. Roy, W. Gworek, S.",
    "flurka-58": "M. R. Arnold, L. Lee, J. Lee-Thorp, M. Maggioni, E. Piqueras, K. Badola, S. Vikram, L. Gonzalez, A. Baddepudi, E. Senter, J. Devlin, J. Qin, M. Azzam, M. Trebacz, M. Polacek, K. Krishnakumar, S. yiin Chang, M. Tung, I. Penchev, R. Joshi, K. Olszewska, C. Muir, M. Wirth, A. J. Hartman, J. Newlan, S. Kashem, V. Bolina,\n\nE. Dabir, J. van Amersfoort, Z. Ahmed, J. Cobon-Kerr, A. Kamath, A. M. Hrafnkelsson, L. Hou, I. Mackinnon, A. Frechette, E. Noland, X. Si, E. Taropa, D. Li, P. Crone, A. Gulati, S. Cevey, J. Adler, A. Ma, D. Silver, S. Tokumine, R. Powell, S. Lee, K. Vodrahalli, S. Hassan, D. Mincu, A. Yang, N. Levine, J. Brennan, M. Wang, S. Hodkinson, J. Zhao, J. Lipschultz, A. Pope, M. B. Chang, C. Li, L. E. Shafey, M. Paganini, S. Douglas, B. Bohnet, F. Pardo, S. Odoom, M. Rosca, C. N. dos Santos, K. Soparkar, A. Guez, T. Hudson, S. Hansen, C. Asawaroengchai, R. Addanki, T. Yu, W. Stokowiec, M. Khan, J. Gilmer, J. Lee, C. G. Bostock, K. Rong, J. Caton, P. Pejman, F. Pavetic, G. Brown, V. Sharma, M. Lu\u010di\u0107, R.",
    "flurka-59": "Samuel, J. Djolonga, A. Mandhane, L. L. Sj\u00f6sund, E. Buchatskaya, E. White, N. Clay, J. Jiang, H. Lim, R. Hemsley, Z. Cankara, J. Labanowski, N. D. Cao, D. Steiner, S. H. Hashemi, J. Austin, A. Gergely, T. Blyth, J. Stanton, K. Shivakumar, A. Siddhant, A. Andreassen, C. Araya, N. Sethi, R. Shivanna, S. Hand, A. Bapna, A. Khodaei, A. Miech, G. Tanzer, A. Swing, S. Thakoor, L. Aroyo, Z. Pan, Z. Nado, J. Sygnowski, S. Winkler, D. Yu, M. Saleh, L. Maggiore, Y. Bansal, X. Garcia, M. Kazemi, P. Patil, I. Dasgupta, I. Barr, M. Giang, T. Kagohara, I. Danihelka, A. Marathe, V. Feinberg, M. Elhawaty, N. Ghelani, D. Horgan, H. Miller, L. Walker, R. Tanburn, M. Tariq, D. Shrivastava, F. Xia, Q. Wang, C.-C. Chiu, Z. Ashwood, K. Baatarsukh, S. Samangooei, R. L. Kaufman, F. Alcober, A. Stjerngren, P. Komarek, K. Tsihlas, A. Boral, R. Comanescu, J. Chen, R. Liu, C. Welty, D. Bloxwich, C. Chen, Y. Sun, F. Feng, M. Mauger, X. Dotiwalla, V. Hellendoorn, M. Sharman, I. Zheng, K. Haridasan, G. BarthMaron, C. Swanson, D. Rogozi\u0144ska, A. Andreev, P. K. Rubenstein, R. Sang, D. Hurt, G. Elsayed, R. Wang, D. Lacey, A. Ili\u0107, Y. Zhao, A. Iwanicki, A. Lince, A. Chen, C. Lyu, C. Lebsack, J. Griffith, M. Gaba, P. Sandhu, P. Chen, A. Koop, R. Rajwar, S. H. Yeganeh, S. Chang, R. Zhu, S. Radpour, E. Davoodi, V. I. Lei, Y. Xu, D. Toyama, C. Segal, M. Wicke, H. Lin, A. Bulanova, A. P. Badia, N. Raki\u0107evi\u0107, P. Sprechmann, A. Filos, S. Hou, V. Campos, N. Kassner, D. Sachan, M. Fortunato, C. Iwuanyanwu, V. Nikolaev, B. Lakshminarayanan, S. Jazayeri, M. Varadarajan, C. Tekur, D. Fritz, M. Khalman, D. Reitter, K. Dasgupta, S. Sarcar, T. Ornduff, J. Snaider, F. Huot, J. Jia, R. Kemp, N. Trdin, A. Vijayakumar, L. Kim, C. Angermueller, L. Lao, T. Liu, H. Zhang, D. Engel, S. Greene, A. White, J. Austin, L. Taylor, S. Ashraf, D. Liu, M. Georgaki, I. Cai, Y. Kulizhskaya, S. Goenka, B. Saeta, Y. Xu, C. Frank, D. de Cesare, B. Robenek, H. Richardson, M. Alnahlawi, C. Yew, P. Ponnapalli, M. Tagliasacchi, A. Korchemniy, Y. Kim, D. Li, B. Rosgen, K. Levin, J. Wiesner, P. Banzal, P. Srinivasan, H. Yu, \u00c7a\u011flar \u00dcnl\u00fc, D. Reid, Z. Tung, D. Finchelstein, R. Kumar, A. Elisseeff, J. Huang, M. Zhang, R. Aguilar, M. Gim\u00e9nez, J. Xia, O. Dousse, W. Gierke, D. Yates, K. Jalan, L. Li, E. Latorre-Chimoto, D. D. Nguyen, K. Durden, P. Kallakuri, Y. Liu, M. Johnson, T. Tsai, A. Talbert, J. Liu, A. Neitz, C. Elkind, M. Selvi, M. Jasarevic, L. B. Soares, A. Cui, P. Wang, A. W. Wang, X. Ye, K. Kallarackal, L. Loher, H. Lam, J. Broder, D. Holtmann-Rice, N. Martin, B. Ramadhana, M. Shukla, S. Basu, A. Mohan, N. Fernando, N. Fiedel, K. Paterson, H. Li, A. Garg, J. Park, D. Choi, D. Wu, S. Singh, Z. Zhang, A. Globerson, L. Yu, J. Carpenter, F. de Chaumont Quitry, C. Radebaugh, C.-C. Lin, A. Tudor, P. Shroff, D. Garmon, D. Du, N. Vats, H. Lu, S. Iqbal, A. Yakubovich, N. Tripuraneni, J. Manyika, H. Qureshi, N. Hua, C. Ngani, M. A. Raad, H. Forbes, J. Stanway, M. Sundararajan, V. Ungureanu, C. Bishop, Y. Li, B. Venkatraman, B. Li, C. Thornton, S. Scellato, N. Gupta, Y. Wang, I. Tenney, X. Wu, A. Shenoy, G.",
    "flurka-60": "Carvajal, D. G. Wright, B. Bariach, Z. Xiao, P. Hawkins, S. Dalmia, C. Farabet, P. Valenzuela, Q. Yuan, A. Agarwal, M. Chen, W. Kim, B. Hulse, N. Dukkipati, A. Paszke, A.",
    "flurka-61": "Bolt, K. Choo, J. Beattie, J. Prendki, H. Vashisht, R. Santamaria-Fernandez, L.",
    "flurka-62": "C. Cobo, J. Wilkiewicz, D. Madras, A. Elqursh, G. Uy, K. Ramirez, M. Harvey, T. Liechty, H. Zen, J. Seibert, C. H. Hu, A. Khorlin, M. Le, A. Aharoni, M. Li, L. Wang, S. Kumar, N. Casagrande, J. Hoover, D. E. Badawy, D. Soergel, D. Vnukov, M. Miecnikowski, J. Simsa, P. Kumar, T. Sellam, D. Vlasic, S. Daruki, N. Shabat, J. Zhang, G. Su, J. Zhang, J. Liu, Y. Sun, E. Palmer, A. Ghaffarkhah, X. Xiong, V. Cotruta, M. Fink, L. Dixon, A. Sreevatsa, A. Goedeckemeyer, A. Dimitriev, M. Jafari, R. Crocker, N. FitzGerald, A. Kumar, S. Ghemawat, I. Philips, F. Liu, Y. Liang, R. Sterneck, A. Repina, M. Wu, L. Knight, M. Georgiev, H. Lee, H. Askham, A. Chakladar, A. Louis, C. Crous, H. Cate, D. Petrova, M. Quinn, D. OwusuAfriyie, A. Singhal, N. Wei, S. Kim, D. Vincent, M. Nasr, C. A. Choquette-Choo, R. Tojo, S. Lu, D. de Las Casas, Y. Cheng, T. Bolukbasi, K. Lee, S. Fatehi, R. Ananthanarayanan, M. Patel, C. Kaed, J. Li, S. R. Belle, Z. Chen, J. Konzelmann, S. P\u00f5der, R. Garg, V. Koverkathu, A. Brown, C. Dyer, R. Liu, A. Nova, J. Xu, A. Walton, A. Parrish, M. Epstein, S. McCarthy, S. Petrov, D. Hassabis, K. Kavukcuoglu, J. Dean, and O. Vinyals. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024.",
    "flurka-63": "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R.",
    "flurka-64": "Fergus,\n\nS. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_ files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\nA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S.",
    "flurka-65": "R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding.",
    "flurka-66": "CoRR, abs/1804.07461, 2018. URL http://arxiv.org/abs/1804.07461\nS. Wang, B. Z. Li, M. Khabsa, H.",
    "flurka-67": "Fang, and H. Ma. Linformer: Self-attention with linear complexity.",
    "flurka-68": "CoRR, abs/2006.04768, 2020. URL https://arxiv.org/abs/2006.04768\nY. Xiong, Z. Zeng, R. Chakraborty, M. Tan, G. Fung, Y.",
    "flurka-69": "Li, and V. Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention.",
    "flurka-70": "CoRR, abs/2102.03902, 2021. URL https://arxiv.org/abs/2102.03902. L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F.",
    "flurka-71": "E. H. Tay, J.",
    "flurka-72": "Feng, and S. Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.",
    "flurka-73": "CoRR, abs/2101.11986, 2021. URL https://arxiv.org/abs/2101.11986\nM. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Onta\u00f1\u00f3n, P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed. Big bird: Transformers for longer sequences. CoRR, abs/2007.14062, 2020. URL https://arxiv.org/abs/2007.14062. L. Zheng, C. Wang, and L. Kong. Linear complexity randomized self-attention mechanism.",
    "flurka-74": "In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 27011-27041. PMLR, 17-23 Jul 2022. URL https://proceedings mlr.press/v162/zheng22b.html.",
    "flurka-75": "L. Zheng, J. Yuan, C.",
    "flurka-76": "Wang, and L. Kong. Efficient attention via control variates. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=G-uNfHKrj46\nC. Zhu, W. Ping, C. Xiao, M. Shoeybi, T. Goldstein, A. Anandkumar, and B. Catanzaro. Long-short transformer: Efficient transformers for language and vision. CoRR, abs/2107.02192, 2021. URL https://arxiv.org/abs/2107.02192\nY. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015. ## Appendix\n\n## A Training Setup\n\n## A. 1 Metrics\n\nWe measure the following for all our models:\n\n- Perplexity: we measure the perplexity across pre-training on Wikitext-103 and BookCorpus. - Accuracy: for certain tasks in GLUE. - F1: for the QQP and MRPC tasks in GLUE. - Matthews Correlation: for the COLA task in GLUE. - Pearson \\& Spearman correlation: for the STS-B task in GLUE.",
    "flurka-77": "- BLEU: for the neural machine translation tasks in WMT'14. ## A. 2 Datasets\n\nWikitext-103 Wikitext-103 is a collection of good and verified articles from Wikipedia. It consists of over 100 million tokens and is a popular dataset to pre-train self-attention models on. GLUE The General Language Understanding Evaluation (GLUE) is a collection of 9 tasks that evaluate natural language understanding systems. It is a popular benchmark to fine-tune self-attention models on. LRA Long Range Arena (LRA) is a task suite that evaluates transformer model qualities under long-context scenarios. It consists of 6 tasks that cover multiple modalities including text, and image. ImageNet ImageNet is a collection of over 14 million annotated images according to the WordNet hierarchy. It is a popular dataset for image and object classification. ## A. 3 Model Configurations\n\nIn MLM, NLU, LRA, and image classification, we use encoder-only architectures. In NMT and model quality ablations, we use encoder-decoder architectures with our techniques applied to the encoder only. A detailed setup of each model's configuration is shown in table 6\n\n## A. 4 Up-training Ratios\n\nFor NMT, the linformer-performer and linformer-EVA variants are up-trained with a ratio of $\\alpha=80 \\%$ with a low-rank base model. Linformer-RNN is up-trained with a ratio of $\\alpha=64 \\%$ with a kernel base model. For image classification, linfromer-performer and linformer-EVA are up-trained with a ratio of $\\alpha=16.7 \\%$ with a low-rank base model. Linformer-RNN is up-trained with a ratio of $\\alpha=33.3 \\%$ with a kernel base model.",
    "flurka-78": "No up-training is applied to any other experiment. ![](https://cdn.mathpix.com/cropped/2024_09_12_c1212708b0e2415c0653g-15.jpg?height=459&width=1452&top_left_y=1966&top_left_x=369)\n\nTable 6: Training setting and model size used in evaluation section. ## B Efficiency Analysis\n\nWe analyze where FLuRKA incurs fewer FLOPs compared to low-rank methods, kernel methods as well as both at the same time. Claim 2. For sequence length: $N$, downsampling factor: $d_{k}$, head hidden dimension: $d_{h}$ when:\n\n$$\nN-1>d_{k}>d_{h}\n$$\n\nFLuRKA incur lower FLOPs against low-rank methods. Claim 3. For sequence length: $N$, hidden dimension: $d_{m}$, downsampling factor: $d_{k}$, number of heads: $H$, when:\n\n$$\nN>d_{k}(H+2), d_{m}>d_{k}\n$$\n\nFLuRKA incur lower FLOPs against kernel methods.",
    "flurka-79": "Each of the above claims delineates a regime where FLuRKA exhibit competitive performance over low-rank and kernel techniques. Moreover, each bound is tight, requiring sequence lengths larger than 1 k to imply speedups. Our argument for all the claims is similar. We first compute the runtime of FLuRKA, kernel and low-rank methods by breaking up the steps of MHSA into the following:\n\n- The low-Rank downsampling products: $E_{1} K \\& E_{2} V$ (low-rank \\& FLuRKA only)\n- Linear Transformations: $Q^{\\prime}=Q W^{Q}, K^{\\prime}=K W^{K} \\& V^{\\prime}=V W^{V}$. - The product: $Q^{\\prime} K^{\\prime T}$ (low-rank only)\n- The application of the kernel: $\\phi\\left(Q^{\\prime}\\right) \\& \\phi\\left(K^{\\prime}\\right)$ (FLuRKA \\& kernel only)\n- The softmax (low-rank only)\n- The product $A V^{\\prime}$ (low-rank only). Or the product $Q^{\\prime}\\left(K^{\\prime T} V^{\\prime}\\right)$ (FLuRKA \\& kernel only). We can sum the time across all the steps of MHSA for each method and compare when one method's expression is greater than the others'. Throughout our arguments, we assume that the kernel $\\phi$ that parameterises a kernel method produces vectors whose dimensionality is on the same order of magnitude as the hidden dimension. We use the following notation for convenience: $H$ - number of heads, $d_{k}$ - downsampling factor, $d_{m}$ - hidden dimension, $N$ - sequence length, and $d_{h}$ - head hidden dimension. Cumulative runtime of low-rank MHSA. The time taken for the downsampling step:\n\n$$\n2 N d_{k} d_{m}\n$$\n\nThe time taken for the Linear transformation step:\n\n$$\nN d_{m}^{2}+2 d_{k} d_{m}^{2}\n$$\n\nThe time taken for the $Q K^{T}$ product:\n\n$$\nN d_{m} d_{k}\n$$\n\nThe time taken for the softmax:\n\n$$\nN H d_{k}\n$$\n\nThe time taken for the $A V$ product:\n\n$$\nN d_{k} d_{m}\n$$\n\nWith the total time taken as:\n\n$$\n2 N d_{k} d_{m}+N d_{m}^{2}+2 d_{k} d_{m}^{2}+N d_{m} d_{k}+N H d_{k}+N d_{k} d_{m}\n$$\n\nCumulative runtime of kernel MHSA. The time taken for the Linear Transformation step:\n\n$$\n3 N d_{m}^{2}\n$$\n\nThe time taken for the application of the kernel:\n\n$$\n2 N d_{m}\n$$\n\nThe time taken for the application of the $Q\\left(K^{T} V\\right)$ product:\n\n$$\n2 N d_{m} d_{h}\n$$\n\nWith the total time taken as:\n\n$$\n3 N d_{m}^{2}+2 N d_{m}+2 N d_{m} d_{h}\n$$\n\nCumulative runtime of FLuRKA' MHSA. The time taken for the downsampling step:\n\n$$\n2 N d_{k} d_{m}\n$$\n\nThe time taken for the linear transformation step:\n\n$$\nN d_{m}^{2}+2 d_{k} d_{m}^{2}\n$$\n\nThe time taken for the application of the kernel is:\n\n$$\nN d_{m}+d_{m} d_{k}\n$$\n\nThe time taken for the $Q K^{T} V$ product is:\n\n$$\nd_{k} d_{m} d_{h}+N d_{h} d_{m}\n$$\n\nWith the total time taken as:\n\n$$\n2 N d_{k} d_{m}+N d_{m}^{2}+2 d_{k} d_{m}^{2}+N d_{m}+d_{m} d_{k}+d_{k} d_{m} d_{h}+N d_{h} d_{m}\n$$\n\n## B. 1 Argument for Claim 1\n\nTo show claim 1 - when FLuRKA incur lower FLOPs over both low-rank and kernel methods, we have to that $N>d_{k}(H+2)>d_{m}>d_{k}>d_{h} \\rightarrow 6-8>0 \\wedge 7-8>0$. We first subtract the runtime of FLuRKA from the runtime of low-rank methods to produce the following expression:\n\n$$\n\\underbrace{N d_{m} d_{k}}_{a}+\\underbrace{N H d_{k}}_{b}+\\underbrace{N d_{k} d_{m}}_{c}-\\underbrace{N d_{m}}_{d}-\\underbrace{d_{m} d_{k}}_{e}-\\underbrace{d_{k} d_{m} d_{k}}_{f}-\\underbrace{N d_{h} d_{m}}_{g}\n$$\n\nWe see that: $d_{h}<d_{k} \\rightarrow b>d \\wedge c>g$ and $N>d_{m}>d_{k} \\rightarrow N>1+d_{k} \\rightarrow a>e+f$. We thus have that: when $N>1+d_{k}, d_{k}>d_{h}$ that $a+b+c>d+e+f+g$. Next, we subtract the runtime of FLuRKA from the runtime of kernel methods to get:\n\n$$\n\\underbrace{2 N d_{m}^{2}}_{i}+\\underbrace{N d_{m}}_{j}+\\underbrace{2 N d_{m} d_{h}}_{k}-\\underbrace{2 N d_{k} d_{m}}_{l}-\\underbrace{2 d_{k} d_{m}^{2}}_{m}-\\underbrace{d_{m} d_{k}}_{n}-\\underbrace{d_{k} d_{m} d_{h}}_{o}\n$$\n\nWe see that $d_{m}>d_{k} \\rightarrow i>l$. We next collectively consider when $j+k>m+n+o$. This occurs when:\n\n$$\n\\begin{array}{r}\nN d_{m}+2 N d_{m} d_{h}-2 d_{k} d_{m}^{2}-d_{m} d_{k}-d_{k} d_{m} d_{h}>0 \\\\\n\\rightarrow d_{m}\\left(N+2 N d_{h}-2 d_{k} d_{m}-d_{k}-d_{k} d_{h}\\right)>0 \\\\\n\\rightarrow N+2 N d_{h}-2 d_{k} d_{m}-d_{k}-d_{k} d_{h}>0 \\\\\n\\rightarrow N\\left(1+2 d_{h}\\right)-d_{k}\\left(2 d_{m}+1+d_{h}\\right)>0 \\\\\n\\rightarrow N>\\underbrace{\\frac{d_{k}\\left(2 d_{m}+1+d_{h}\\right)}{1+2 d_{h}}}_{z}\n\\end{array}\n$$\n\nNow, we can simplify $z$ as follows:\n\n$$\n\\begin{aligned}\n& \\frac{d_{k}\\left(2 d_{m}+1+d_{h}\\right)}{1+2 d_{h}} \\\\\n& =\\frac{d_{k}\\left(2 H d_{h}+1+d_{h}\\right)}{1+2 d_{h}} \\\\\n& =\\frac{d_{k}\\left(2(H+1) d_{h}+1\\right)}{1+2 d_{h}} \\\\\n& <\\frac{d_{k}\\left(2(H+1) d_{h}+1\\right)}{2 d_{h}} \\\\\n& <\\frac{d_{k}\\left(2(H+1) d_{h}+d_{h}\\right)}{2 d_{h}} \\\\\n& =\\frac{d_{k}(2(H+2))}{2}=d_{k}(H+2)\n\\end{aligned}\n$$\n\nTo finally attain: $N>d_{k}(H+2)>z \\rightarrow j+k>m+n+o$. We thus have when $N>$ $d_{k}(H+2), d_{m}>d_{k}$ that $i+j+k>l+m+n+o$. It follows that when $N>d_{k}(H+2)>d_{m}>d_{k}>d_{h}$ FLuRKA incur lower FLOPs over kernel and low-rank methods. ## B. 2 Argument for Claim 2\n\nClaim 2 - when FLuRKA incur lower FLOPs over low-rank - naturally follows from 9 since $N-1>$ $d_{k}>d_{h} \\rightarrow 6-8>0$\n\n## B. 3 Argument for Claim 3\n\nClaim 3 - when FLuRKA incur lower FLOPs over kernel methods - naturally follows from 10 since $N>d_{k}(H+2), d_{m}>d_{k} \\rightarrow 7-8>0$. ## C Comparison against flash-attention\n\nWe compare the step-times of FLuRKA variants against SOTA flash-attention's implementation.",
    "flurka-80": "We use a 6 layer transformer, with a FFN hidden dimension of 512 and 4 attention heads. We vary the sequence length from 7500 to 52500 in increments of 9000 . Our results are in figure 5 . Across all sequence lengths, FLuRKA yield considerable speedups over flash-attention, culminating in speedups of up to $23 x$. ![](https://cdn.mathpix.com/cropped/2024_09_12_c1212708b0e2415c0653g-18.jpg?height=440&width=1378&top_left_y=1189&top_left_x=365)\n\nFigure 5: Speedups FLuRKA attain over Flash-Attention.",
    "flurka-81": "## D Accuracy Analysis\n\n## D.",
    "flurka-82": "1 Proof of Theorem 1\n\nWe first define important structures to be used throughout this proof. Definition. In computing canonical multi-head self-attention, suppose we have the queries, keys and values $Q, K, V \\in \\mathbb{R}^{n \\times d_{m}}$ respectively. As well as the learnt weights: $W_{i}^{Q}, W_{i}^{K}$ and $W_{i}^{V} \\in \\mathbb{R}^{n \\times d_{h}}$. We have that:\n\n$$\nA_{i}=\\operatorname{softmax}\\left(\\frac{Q W_{i}^{Q}\\left(K W_{i}^{K}\\right)^{T}}{\\sqrt{d_{k}}}\\right) V W_{i}^{V}\n$$\n\nFurthermore, we denote:\n\n$$\nA_{\\text {softin }}=\\frac{Q W_{i}^{Q}\\left(K W_{i}^{K}\\right)^{T}}{\\sqrt{d_{k}}}\n$$\n\nWe also cite the following theorem from Linformer, which will form the basis for our proof:\nLinformer's Theorem. For any $Q_{i}, K_{i}, V_{i} \\in \\mathbb{R}^{n \\times d_{m}}$ and $W_{i}^{Q}, W_{i}^{K}, W_{i}^{V} \\in \\mathbb{R}^{d_{m} \\times d_{h}}$. If $k=$ $5 \\log (d) /\\left(\\epsilon_{2}^{2}-\\epsilon_{3}^{2}\\right)$ where $\\epsilon_{2}$ and $\\epsilon_{3}$ are defined as per Linformer Theorem 2, and we further define\nthe matrices $E_{i}=\\delta R, F_{i}=e^{-\\delta} R$ where $R \\in \\mathbb{R}^{n \\times k}$ whose entries are iid sampled from $N(0,1 / k)$. We have that, for any row-vector $w \\in A_{\\text {softin }}$ :\n$\\operatorname{Pr}\\left(\\left\\|\\operatorname{softmax}\\left(w E_{i}^{T}\\right) F_{i} V W_{i}^{V}-\\operatorname{softmax}(w) V W_{i}^{V}\\right\\|_{2}<\\epsilon_{1}\\|\\operatorname{softmax}(w)\\|_{2}\\left\\|V W_{i}^{V}\\right\\|_{2}\\right)>1-o(1)$\n\nWe re-write the above into a form that simplifies our proof:\n\n$$\n\\operatorname{Pr}\\left(\\left\\|\\operatorname{softmax}\\left(A_{\\text {softin }} E_{i}^{T}\\right) F_{i} V W_{i}^{V}-A_{i} V W_{i}^{V}\\right\\|_{\\infty}<\\epsilon_{1}\\left\\|A_{i}\\right\\|_{2}\\left\\|V W_{i}^{V}\\right\\|_{2}\\right)>1-o(1)\n$$\n\nNow, let us assume we have a random-feature based kernel method parameterised by $\\phi(x)=$ $\\frac{1}{\\sqrt{m}}\\left[\\psi_{1}(x), \\ldots \\psi_{m}(x)\\right]$ such that:\n\n$$\n\\mathbb{E}\\left[\\psi_{i}(x)^{T} \\cdot \\psi_{i}(y)\\right]=\\exp \\left(x^{T} \\cdot y\\right)\n$$\n\nWe have the following:\n\n$$\n\\phi(x)^{T} \\cdot \\phi(y)=\\frac{1}{m} \\sum_{i=1}^{m} \\psi_{i}(x)^{T} \\cdot \\psi_{i}(y)\n$$\n\nWe then have, by the law of large numbers:\n\n$$\n\\begin{aligned}\n& \\lim _{m \\rightarrow \\infty} \\phi(x)^{T} \\cdot \\phi(y) \\\\\n& =\\lim _{m \\rightarrow \\infty} \\frac{1}{m} \\sum_{i=1}^{m} \\psi_{i}(x)^{T} \\cdot \\psi_{i}(y) \\\\\n& =\\mathbb{E}\\left[\\psi_{i}(x)^{T} \\cdot \\psi_{i}(y)\\right] \\\\\n& =\\exp \\left(x^{T} \\cdot y\\right)\n\\end{aligned}\n$$\n\nEstablishing that $\\lim _{m \\rightarrow \\infty} \\phi(x)^{T} \\cdot \\phi(y)=\\exp \\left(x^{T} \\cdot y\\right)$. Therefore, by the definition of a limit, for $\\epsilon_{4}>0$, $\\epsilon_{5}=\\left(\\epsilon_{4} F\\right) / 2 N^{2}, F=\\min _{1 \\leq i \\leq N}\\left|\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right)\\right|, \\exists m \\in \\mathbb{Z}^{+}$such that:\n\n$$\n\\left|\\phi(x)^{T} \\cdot \\phi(y)-\\exp \\left(x^{T} \\cdot y\\right)\\right|<\\epsilon_{5}\n$$\n\nWith $x_{i} \\in Q W_{i}^{Q}$ and $y_{k} \\in K W_{i}^{K}$. Next, we bound $\\left\\|\\hat{A}_{i}-A_{i}\\right\\|_{\\infty}$, where $\\hat{A}$ is the attention matrix materialised by the random-feature based kernel method parameterised by $\\phi$ :\n\n$$\n\\begin{aligned}\n& \\left\\|\\hat{A}_{i}-A_{i}\\right\\|_{\\infty} \\\\\n& =\\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N}\\left|a_{i j}\\right| \\\\\n& =\\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N}\\left|\\frac{\\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{j}\\right)}{\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right)}-\\frac{\\exp \\left(x_{i}^{T} \\cdot y_{j}\\right)}{\\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)}\\right| \\\\\n& =\\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N}\\left|\\frac{\\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{j}\\right) \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)-\\exp \\left(x_{i}^{T} \\cdot y_{j}\\right) \\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right)}{\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right) \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)}\\right| \\\\\n& \\leq \\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N}\\left|\\frac{\\left.\\left[\\exp \\left(x_{i}^{T} \\cdot y_{j}\\right)+\\epsilon_{5}\\right] \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)-\\exp \\left(x_{i}^{T} \\cdot y_{j}\\right) \\sum_{k=1}^{N}\\left[\\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)+\\epsilon_{5}\\right]\\right]}{\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right) \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)}\\right| \\\\\n& =\\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N}\\left|\\frac{\\epsilon_{5} \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)-N \\epsilon_{5} \\exp \\left(x_{i}^{T} \\cdot y_{j}\\right)}{\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right) \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)}\\right| \\\\\n& \\leq \\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N}\\left|\\frac{\\epsilon_{5} \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)+N \\epsilon_{5} \\exp \\left(x_{i}^{T} \\cdot y_{j}\\right)}{\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right) \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)}\\right| \\\\\n& \\leq \\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N}\\left|\\frac{\\epsilon_{5} \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)+N \\epsilon_{5} \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)}{\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right) \\sum_{k=1}^{N} \\exp \\left(x_{i}^{T} \\cdot y_{k}\\right)}\\right| \\\\\n& =\\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N}\\left|\\frac{\\epsilon_{5}+N \\epsilon_{5}}{\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right)}\\right| \\\\\n& =\\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N} \\frac{\\epsilon_{5}+N \\epsilon_{5}}{\\left|\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right)\\right|} \\\\\n& \\leq \\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N} \\frac{\\epsilon_{5}+N \\epsilon_{5}}{\\min _{i \\leq I \\leq N}\\left|\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right)\\right|} \\\\\n& =\\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N} \\frac{\\epsilon_{4} F / 2 N^{2}+\\epsilon_{4} F / 2 N}{\\min _{i \\leq I \\leq N}\\left|\\sum_{k=1}^{N} \\phi\\left(x_{i}\\right)^{T} \\cdot \\phi\\left(y_{k}\\right)\\right|} \\\\\n& =\\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N} \\frac{\\epsilon_{4}}{2 N^{2}}+\\frac{\\epsilon_{4}}{2 N} \\\\\n& \\leq \\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N} \\frac{\\epsilon_{4}}{2 N}+\\frac{\\epsilon_{4}}{2 N} \\\\\n& \\leq \\max _{1 \\leq i \\leq N} \\sum_{j=1}^{N} \\frac{\\epsilon_{4}}{N} \\\\\n& =\\epsilon_{4}\n\\end{aligned}\n$$\n\nTo get the final bound:\n\n$$\n\\left\\|\\hat{A}_{i}-A_{i}\\right\\|_{\\infty}<\\epsilon_{4}\n$$\n\nSubstitute $K=E_{1} K$ in 13 and we get:\n\n$$\n\\left\\|\\phi\\left(Q W_{i}^{Q}\\right) \\phi\\left(E_{1} K W_{i}^{K}\\right)^{T}-\\operatorname{softmax}\\left(Q W_{i}^{Q}\\left(E_{1} K W_{i}^{K}\\right)^{T}\\right)\\right\\|_{\\infty}<\\epsilon_{4}\n$$\n\nWe can recover the full Linformer style attention computation by right multiplying by the norm $\\left\\|F_{i} V W_{i}^{V}\\right\\|_{\\infty}$ to obtain:\n\n$$\n\\left\\|\\phi\\left(Q W_{i}^{Q}\\right) \\phi\\left(E_{1} K W_{i}^{K}\\right)^{T}-\\operatorname{softmax}\\left(Q W_{i}^{Q}\\left(E_{1} K W_{i}^{K}\\right)^{T}\\right)\\right\\|_{\\infty}\\left\\|F_{i} V W_{i}^{V}\\right\\|_{\\infty}<\\epsilon_{4}\\left\\|F_{i} V W_{i}^{V}\\right\\|_{\\infty}\n$$\n\nSince $\\|A B\\| \\leq\\|A\\|\\|B\\|, 15$ simplifies to:\n\n$$\n\\left\\|\\phi\\left(Q W_{i}^{Q}\\right) \\phi\\left(E_{1} K W_{i}^{K}\\right)^{T} F_{i} V W_{i}^{V}-\\operatorname{softmax}\\left(Q W_{i}^{Q}\\left(E_{1} K W_{i}^{K}\\right)^{T}\\right) F_{i} V W_{i}^{V}\\right\\|_{\\infty}<\\epsilon_{4}\\left\\|F_{i} V W_{i}^{V}\\right\\|_{\\infty}\n$$\n\nNow, we add 16 to 11 to get:\n\n$$\n\\begin{aligned}\n& \\left\\|\\phi\\left(Q W_{i}^{Q}\\right) \\phi\\left(E_{1} K W_{i}^{K}\\right)^{T} F_{i} V W_{i}^{V}-\\operatorname{softmax}\\left(Q W_{i}^{Q}\\left(E_{1} K W_{i}^{K}\\right)^{T}\\right) F_{i} V W_{i}^{V}\\right\\|_{\\infty}+ \\\\\n& \\quad\\left\\|\\operatorname{softmax}\\left(A_{\\text {softin }} E_{i}\\right) F_{i} V W_{i}^{V}-A_{i} V W_{i}^{V}\\right\\|_{\\infty}<\\epsilon_{4}\\left\\|F_{i} V W_{i}^{V}\\right\\|_{\\infty}+\\epsilon_{1}\\left\\|A_{i}\\right\\|_{2}\\left\\|V W_{i}^{V}\\right\\|_{2}\n\\end{aligned}\n$$\n\nWe apply the triangle inequality to the LHS, to simplify the above expression to finally yield:\n\n$$\n\\left\\|\\phi\\left(Q W_{i}^{Q}\\right) \\phi\\left(E_{1} K W_{i}^{K}\\right)^{T} F_{i} V W_{i}^{V}-A_{i} V W_{i}^{V}\\right\\|_{\\infty}<\\epsilon_{4}\\left\\|F_{i} V W_{i}^{V}\\right\\|_{\\infty}+\\epsilon_{1}\\left\\|A_{i}\\right\\|_{2}\\left\\|V W_{i}^{V}\\right\\|_{2}\n$$\n\nWith high probability for a carefully chosen $E_{i}, F_{i}$ and $\\phi$. [^0]:    ${ }^{1}$ https://github.com/google-research/t5x\n\n"
}