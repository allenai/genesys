{
    "effibeam-0": "# Efficient Beam Tree Recursion \n\nJishnu Ray Chowdhury Cornelia Caragea<br>Computer Science<br>University of Illinois Chicago<br>jraych2@uic.edu cornelia@uic.edu\n\n\n#### Abstract\n\nBeam Tree Recursive Neural Network (BT-RvNN) was recently proposed as an extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks.",
    "effibeam-1": "However, although better than previous approaches in terms of memory usage, BT-RvNN can be still exorbitantly expensive. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10-16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$ into a token contextualizer of the form $f: \\mathbb{R}^{n \\times d} \\rightarrow$ $\\mathbb{R}^{n \\times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models. Our code is available at the link: https://github.com/JRC1995/BeamRecursionFamily\n\n\n## 1 Introduction\n\nRecursive Neural Networks (RvNNs) [63] in their most general form can be thought of as a repeated application of some arbitrary neural function (the recursive cell) combined with some arbitrary halting criterion. The halting criterion can be dynamic (dependent on input) or static (independent of the input). From this viewpoint, nearly any neural network encoder in the deep learning family can be seen as a special instance of an RvNN. For example, Universal Transformers [13] repeat a Transformer [85] layer block as a recursive cell and adaptively halt by tracking the halting probability in each layer using some neural function [25, 3]. Deep Equilibrium Models (DEQ) [2] implicitly \"repeat\" a recursive cell function using some root-finding method which is equivalent to using the convergence of hidden states dynamics as the halting criterion. As Bai et al. [2] also showed, any traditional Transformer - i.e. stacked layer blocks of Transformers with non-shared weights can be also equivalently reformulated as a recursive repetition of a big sparse Transformer block with the halting criterion being some preset static upperbound (some layer depth set as a hyperparameter). A broad class of RvNNs can also be viewed as a repeated application of a Graph Neural Network (GNN) [68, 91] - allowing iterative message passing for some arbitrary depth (determined by the halting criterion). Transformer layers can be seen as implementing a fully connected (all-to-all) graph with sequence tokens as nodes and attention-based edge weights. In natural language processing, we often encounter the use of stacks of GNNs (weight shared or not) to encourage message-passing through underlying linguistic structures such as dependency parses, constituency parses, discourse structures, abstract meaning representations, and the like [80, 39, 47, 59, 11, 90, 94]. In many cases, such models are implemented with some static fixed number of layers or iterations. However, learning certain tasks in a length-generalizing fashion without prior knowledge of the sequence length distribution should require a dynamic halting function. For example, consider a simple arithmetic task: $7 \\times(8+3)$. It is first necessary to process $8+3$ before doing the multiplication in the next step. A task like this can require structure-sensitive sequential processing where the sequential steps depend on the input length and nested operators. Although the above example is simple enough, we can have examples with arbitrary nested operators within arbitrarily complex parenthetical structures. Datasets like ListOps [57] and logical inference [7], among others, serve as a sanity check for any model's ability to perform similar structure-sensitive tasks. Natural language tasks, on the other hand, can sometimes hide systematic failures because of the distributional dominance of confounding statistical factors [54]. While, in some algorithmic tasks, Universal Transformer [13] can perform better than vanilla Transformers, both can still struggle in length generalization in logical inference or ListOps [71, 84]. This brings us to explore another alternative family of models-Tree-RvNNs with a stronger inductive bias than Universal Transformer but still possessing a form of dynamic halt that we discuss below. Tree Recursive Neural Networks: Tree-RvNNs [79, 63, 77] (more particularly, constituency Tree RvNNs ) treat an input sequence of vectors as the terminal nodes of some underlying latent tree. From this perspective, Tree-RvNNs sequentially build up the tree nodes in a bottom-up manner. Each node (terminal or non-terminal) in the tree will represent some span within the sequence (for text sequences, these would be words, phrases, clauses, and the like). The root node will represent the whole input sequence and thus can be used as an encoded representation of the whole input (for sentences, it can be treated as a sentence encoding). The nodes are generally in the form of vectors. The non-terminal nodes are built bottom-up through the repeated use of some recursive cell composition function, say $r e c$, starting from height 1 (height 0 being the terminal nodes) all the way up to the root. When building the representation of some node $p$ at height $t$, the Tree-RvNN uses the rec function to take all the immediate child nodes of $p$ (from the previous heights 0 to $t-1$ ) as input arguments and outputs the representation of $p$. Typically, we only consider binarized trees so the number of children as arguments will be a constant (two) for rec. Thus, it can be represented in the form rec : $\\mathbb{R}^{d} \\times \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$. Generally, we also constrain our consideration to only projective tree structures ${ }^{1}$ This implies that the model enforces only temporally contiguous node pairs be considered as candidate siblings for any parent node ${ }^{2}$ Finally, when the root representation is built the computation terminates. Therefore, reaching the root is the halting criterion. Since the tree structure is dependent on the input and the halting is dependent on the tree structure, the halting mechanism here is dynamic and thus can adapt with input complexity. This approach has the potential to model mereological (part-whole) structures at different hierarchical scales in a systematic and dynamic manner ${ }^{3}$ As an example, let us consider that we have an input such as $(4 \\times 3)+(7+(8 \\times 9))$. Then, in the ideal case, the Tree-RvNN order of operation would potentially yield: $\\operatorname{rec}(\\operatorname{rec}(\\operatorname{rec}(\\operatorname{rec}(4, \\times) 3),+) \\operatorname{rec}(\\operatorname{rec}(7,+) \\operatorname{rec}(\\operatorname{rec}(8, \\times), 9)))$. As we can see, Tree-RvNNs have the potential to model sensitive order of operations considering input hierarchies which can be botched by unconstrained attention without careful inductive biases [12, 10, 29] or extensive pre-training ${ }^{4}$ We present an extended related work review in Appendix D\nMotivation for Tree-RvNNs: Although, at first glance Tree-RvNNs as formalized above may appear to have too strong of an inductive bias, there are a few reasons to be motivated to empirically explore the vicinity of these models:\n\n1. Familiar Recurrent Neural Networks (RNNs) [19, 33] are special cases of Tree-RvNNs that follow an enforced chain-structured tree. So at the very least Tree-RvNNs have more flexibility than RNNs which have served as a fairly strong general model in the pre-Transformer era; and even now, optimized linear variants of RNNs [53] are making a comeback out-competing Transformers in long range datasets [26, 27, 60]. [^0]2. Often Tree-RvNNs are the core modules behind models that have been shown to productively length-generalize or compositionally generalize [71, 10, 46, 32, 5, 45, 52] in settings where other family of models struggle (at least without extensive pre-training). 3. The recursive cell function of Tree-RvNNs can still allow them to go outside their explicit structural constraints by effectively organizing information in their hidden states if necessary. The projective tree-structure provides only a rough contour for information flow through Tree-RvNNs which Tree-RvNNs can learn to go around just as an RNN can [7]. So in practice some of these constraints can be less concerning than one may think. Issues: Unfortunately, the flexibility of Tree-RvNNs over RNNs comes at a cost. While in RNNs we could just follow on the same chain-structure tree (left-to-right or right-to-left) for any input, in Tree-RvNNs we have to also consider some way to dynamically induce a tree-structure to follow. This can be done externally-that is, we can rely on human inputs or some external parser. However, neither of them is always ideal. Here, we are focusing on complete Tree-RvNN setups that do their own parsing without any ground-truth tree information anywhere in the pipeline. Going this direction makes the implementation of Tree-RvNNs more challenging because it is hard to induce discrete tree structures through backpropagation. Several methods have been proposed to either induce discrete structures [31,9,66,61], or approximate them through some continuous mechanism [10, 95, 72, 71]. Nevertheless, all these methods have their trade offs - some do not actually work in structured-sensitive tasks [57, 9, 72], others need to resort to reinforcement learning and an array of optimizing techniques [31] or instead use highly sophisticated architectures that can become practically too expensive in space or time or both [51, 71, 10]. Moreover, many of the above models [10, 31, 9] have been framed and used mainly as a sentence encoder of the form $f: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}(n$ being the sequence size). This formulation as sentence encoder limits their applicability as a deep learning building block - for example, we cannot use them as an intermediate block and send their output to another module of their kind or some other kind like Transformers because the output of a sentence encoder will just be a single vector. Our Contributions: Believing that simplicity is a virtue, we direct our attention to Gumbel-Tree (GT) RvNN [9] which uses a simple easy-first parsing technique [23] to automatically greedily parse tree structures and compose sentence representations according to them (we provide a more extended discussion on related works in Appendix). Despite its simplicity, GT-RvNN relies on StraightThrough Estimation (STE) [4] which induces biased gradient, and has been shown empirically to fail in structure-sensitive tasks [57]. Yet, a recent approach - Beam Tree RvNN (BT-RvNN) [66] - promisingly shows that simply using beam search instead of the greedy approach succeeds quite well in structure-sensitive tasks like ListOps [57] and Logical Inference [7] without needing STE. Nevertheless, BT-RvNN can still have exhorbitant memory usage. Furthermore, so far it has been only tested as a sentence encoder. We take a step towards addressing these issues in this paper:\n\n1. We identify a critical memory bottleneck in both Gumbel-Tree RvNN and Beam-Tree RvNN and propose strategies to fix this bottleneck (see $\\$ 3$ ). Our strategies reduce the peak memory usage of Beam-Tree RvNNs by 10-16 times in certain stress tests (see Table 4. 2. We propose a strategy to utilize the intermediate tree nodes (the span representations) to provide top-down signals to the original terminal representations using a parent attention mechanism. This allows a way to contextualize token representations using RvNNs enabling us to go beyond sentence-encoding (see 4 ). 3. We show that the proposed efficient variant of BT-RvNN incurs marginal accuracy loss if at all compared to the original-and in some cases even outperforms the original by a large margin (in ListOps). ## 2 Existing Framework\n\nHere we first describe the existing framework used in Choi et al.",
    "effibeam-2": "[9] and Ray Chowdhury and Caragea [66]. In the next section, we identify its weakness in terms of memory consumption and resolve it. Task Structure: As in related prior works [9, 10, 71], we start with our focus (although we will expand - see $\\$ 4$ on exploring the use of RvNNs as sentence encoders of the form: $f: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$. Given a sequence of $n$ vectors of size $d$ as input (of the form $\\mathbb{R}^{n \\times d}$ ), $f$ compresses it into a single vector (of the form $\\mathbb{R}^{d}$ ). Sentence encoders can be used for sentence-pair comparison or classification. Components: The core components of this framework are: a scoring function scorer : $\\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ and a recursive cell rec $: \\mathbb{R}^{d} \\times \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d}$. The scorer is implemented as $\\operatorname{scorer}(v)=W_{v} v$ where $W_{v} \\in \\mathbb{R}^{1 \\times d}$. The $\\operatorname{rec}\\left(\\right.$ child $_{l}$, child $\\left._{r}\\right)$ function is implemented as below:\n\n$$\n\\begin{aligned}\n& {\\left[\\begin{array}{l}\nl, \\\\\nr \\\\\ng, \\\\\nh\n\\end{array}\\right]=\\operatorname{GeLU}\\left(\\left[\\begin{array}{c}\n\\text { child }_{l} ; \\\\\n\\text { child }_{r}\n\\end{array}\\right] W_{1}^{r e c}+b_{1}\\right) W_{2}^{r e c}+b_{2}} \\\\\n& p=L N\\left(\\sigma(l) \\odot \\text { child }_{l}+\\sigma(r) \\odot \\operatorname{child}_{r}+\\sigma(g) \\odot h\\right)\n\\end{aligned}\n$$\n\nHere $-\\sigma$ is sigmoid; [;] represents concatenation; $p$ is the parent node representation built from the children child $_{l}$ and child $d_{r}, W_{1}^{\\text {rec }} \\in \\mathbb{R}^{2 \\cdot d \\times d_{\\text {cell }}} ; b_{1} \\in \\mathbb{R}^{d_{\\text {cell }}} ; W_{2}^{\\text {rec }} \\in \\mathbb{R}^{d_{\\text {cell }} \\times 4 \\cdot d} ; b_{2} \\in \\mathbb{R}^{d}$, and $l, r, g, h \\in \\mathbb{R}^{d} .",
    "effibeam-3": "L N$ is layer normalization. $d_{\\text {cell }}$ is generally set as $4 \\times d$. Overall, this is the Gated Recursive Cell (GRC) that was originally introduced by Shen et al. [71] and has been consistently shown to be superior [71, 10, 66] to earlier RNN cells like Long Short Term Memory Networks (LSTM) [33, 80]. Note that these models generally also apply an initial transformation layer to the terminal nodes before starting up the RvNN. Similar to [71, 10, 66], we apply a single linear transform followed by a layer normalization as the initial transformation. Greedy Search Tree Recursive Neural Networks: Here we describe the implementation of GumbelTree RvNN [9]. Assume that we have a sequence of hidden states of the form $\\left(h_{1}^{t}, h_{2}^{t}, \\ldots h_{n}^{t}\\right)$ in some intermediate recursion layer $t$. For the recursive step in that layer, first all possible parent node candidates are computed as:\n\n$$\np_{1}^{t}=\\operatorname{rec}\\left(h_{1}^{t}, h_{2}^{t}\\right), p_{2}^{t}=\\operatorname{rec}\\left(h_{2}^{t}, h_{3}^{t}\\right), \\ldots, p_{n-1}^{t}=\\operatorname{rec}\\left(h_{n-1}^{t}, h_{n}^{t}\\right)\n$$\n\nSecond, each parent candidate node $p_{i}^{t}$ is scored as $e_{i}^{t}=\\operatorname{scorer}\\left(p_{i}^{t}\\right)$. Next, the index of the top score is selected as $j=\\operatorname{argmax}\\left(e_{1: n-1}^{t}\\right)$. Finally, now the update rule for the next recursion can be expressed as:\n\n$$\nh_{i}^{t+1}= \\begin{cases}h_{i}^{t} & i<j \\\\ \\operatorname{rec}\\left(h_{i}^{t}, h_{i+1}^{t}\\right) & i=j \\\\ h_{i+1}^{t} & i>j\\end{cases}\n$$\n\nNote that this is essentially a greedy tree search process where in each turn all the locally available choices (parent candidates) are scored and the maximum scoring choice is selected. In each iteration, the sequence size is decreased by 1 . In the final step only one representation will be remaining (the root node). At this point, however, the tree parsing procedure is not differentiable because it purely relies on an argmax. In practice, reinforcement learning [31], STE with gumbel softmax [9], or techniques like SPIGOT 61] have been used to replace argmax. Below we discuss another alternative and our main focus. Beam Search Tree Recursive Neural Networks (BT-RvNN): Seeing the above algorithm as a greedy process provides a natural extension through beam search as done in Ray Chowdhury and Caragea [66]. BT-RvNN replaces the argmax in Gumbel-Tree RvNN with a stochastic Top-K [42] that stochastically extracts both the $K$ highest scoring parent candidates and the $K$ corresponding log-softmaxed scores.",
    "effibeam-4": "The process collects all parent node compositions and also accumulates (by addition) log-softmaxed scores for each selected choices in corresponding beams. With this, the end result is $K$ beams of accumulated scores and $K$ beams of root node representations. The final representation is a softmax-based marginalization of the $K$ root nodes: $\\sum_{i} \\frac{\\exp \\left(s_{i}\\right)}{\\sum_{j} \\exp \\left(s_{j}\\right)} \\cdot b_{i}$ where $b_{i}$ is the root node representation of beam $i$ and $s_{i}$ is the accumulated (added) log-softmaxed scores at every iteration for beam $i$. Doing this enabled BT-RvNN to improve greatly [66] over greedy Gumbel-Tree recursion [9]. However, BT-RvNN can also substantially increase the memory usage, which makes it inconvenient to use. ## 3 Bottleneck and Solution\n\nIn this section, we identify a major bottleneck in the memory usage that exists in the above framework (both for greedy-search and beam-search) that can be adjusted for. ![](https://cdn.mathpix.com/cropped/2024_09_12_a99d960147e1b11454d2g-05.jpg?height=471&width=1369&top_left_y=255&top_left_x=375)\n\nFigure 1: Visualization of the contrast between the existing framework (left) and the proposed one (right). $H_{1}, H_{2}, H_{3}$ are the input representations in the iteration. The possible contiguous pairs of them are candidate child pairs for nodes to be built in this iteration. On the left side, we see each pair is in parallel fed to the recursive cells to create their corresponding candidate parent representations. Then they are scored and one parent $\\left(P_{1}\\right)$ is selected. On the right side (our approach), each child pair candidate is directly scored. The faded colored bars in $H_{1}, H_{2}, H_{3}$ represent sliced away vector values. The scoring function then selects one child pair. Then only that specific selected child pair is composed using the recursive cell to create the parent representation $\\left(P_{1}\\right)$ not wasting unnecessary compute by applying the recursive cell for other non-selected child pairs. Bottleneck: The main bottleneck in the above existing framework is Eqn. 3 That is, the framework runs a rather heavy recursive cell (concretely, the GRC function in Eqn. 2) in parallel for every item in the sequence and for every iteration. In contrast, RNNs could use the same GRC function but for one position at a time sequentially - taking very little memory. While Transformers also use similarly big feedforward networks like GRC in parallel for all hidden states - they have fixed a number of layers - whereas BT-RvNNs may have to recurse for hundreds of layers depending on the input making this bottleneck more worrisome. However, we think this bottleneck can be highly mitigated. Below, we present our proposals to fix this bottleneck. ### 3.1 Efficient Beam Tree Recursive Neural Network (EBT-RvNN)\n\nHere, we describe our new model EBT-RvNN.",
    "effibeam-5": "It extends BT-RvNN by incorporating the fixes below. We present the contrast between the previous method and our current method visually in Figure 1 . Fix 1 (new scorer): At any iteration $t$, we start only with some sequence $\\left(h_{1}^{t}, \\ldots, h_{n}^{t}\\right)$. In the existing framework, starting from this the function to compute the score for any child-pair will look like:\n\n$$\ne_{i}=\\operatorname{scorer} \\circ \\operatorname{rec}\\left(h_{i}, h_{i+1}\\right)\n$$\n\nThis is, in fact, the only reason for which we need to apply rec to all positions (in Eqn. 3) at this iteration because that is the only way to get the corresponding score; that is, currently the score computation entangles the recursive cell rec and the scorer. However, there is no clear reason to do this. Instead we can just replace scorer $\\circ$ rec (in Eqn. 5) with a single new scorer function (scorer $_{n e w}: \\mathbb{R}^{2 \\times d} \\rightarrow \\mathbb{R}$ ) that directly interacts with the concatenation of $\\left(h_{i}, h_{i+1}\\right)$ without the $r e c$ as an intermediate step - and thus disentangling it from the scorer. We use a parameter-light 2-layered MLP to replace scorer $\\circ$ rec:\n\n$$\ne_{i}=\\operatorname{scorer}_{\\text {new }}\\left(h_{i}, h_{i+1}\\right)=\\operatorname{GeLU}\\left(\\left[h_{i} ; h_{i+1}\\right] W_{1}^{s}+b_{1}^{s}\\right) W_{2}^{s}+b_{2}^{s}\n$$\n\nHere, $W_{1}^{s} \\in \\mathbb{R}^{2 \\cdot d \\times d_{s}}, W_{1}^{s} 2 \\in \\mathbb{R}^{d_{s} \\times 1}, b_{1}^{s} \\in \\mathbb{R}^{d_{s}}, b_{2}^{s} \\in \\mathbb{R}$. Since lightness of the scorer $_{\\text {new }}$ function is critical for lower memory usage (this has to be run in parallel for all contiguous pairs) we set $d_{s}$ to be small $\\left(d_{s}=64\\right)$. In this formulation, the rec function will be called only for the selected contiguous pairs (siblings) in Eqn. 4\n\nFix 2 (slicing): While we already took a major step above in making BT-RvNN more efficient, we can still go further. It is unclear whether the full hidden state vector size $d$ is necessary for parsing decisions. Parsing typically hangs on more coarse-grained abstract information - for example, when doing arithmetic while precise numerical information needs to be stored in the hidden states for future\ncomputation, the exact numerical information is not relevant for parsing - only the class of being a number should suffice. Thus, we assume that we can project the inputs into a low-dimensional space for scoring. One way to do that is to use a linear layer. However, parallel matrix multiplications on the full hidden state can be still costly when done for each hidden state in the sequence in every recursion. So, instead, we can allow the initial transformation or the RvNN itself to implicitly learn to organize parsing-related information in some sub-region of the vector. We can treat only the first $\\min \\left(d_{s}, d\\right.$ ) (out of $d$ ) as relevant for parsing decisions. Then we can simply slice the first $\\min \\left(d_{s}, d\\right)$ out before sending the candidate child pairs to the scoring function. Thus, the score computation can be presented as below:\n\n$$\ne_{i}=\\operatorname{scorer}_{n e w}\\left(h_{i}\\left[0: \\min \\left(d_{s}, d\\right)\\right], h_{i+1}\\left[0: \\min \\left(d_{s}, d\\right)\\right]\\right)\n$$\n\nSo, now $W_{1}^{s} \\in \\mathbb{R}^{2 \\cdot \\min \\left(d_{s}, d\\right)} \\times d_{s}$. As before we keep $d_{s}$ small $\\left(d_{s}=64\\right)$. Now, the total hidden state size (d) can be kept as high as needed to preserve overall representation capacity without making the computation scale as much with increasing $d$. The model can now just learn through gradient descent to organize parsing relevant features in the first $\\min \\left(d_{s}, d\\right)$ values of the hidden states because only through them will gradient signals related to parsing scores propagate. Note that unlike [31], we are not running a different $r e c$ function for the parsing decisions. The parsing decision in our case still depends on the output of the same single recursive cell but from the previous iterations (if any). No OneSoft: Ray Chowdhury and Caragea [66] also introduced a form of soft top- $k$ function (OneSoft) for better gradient propagation in BT-RvNN. While we still use that as a baseline model, we do not include OneSoft in EBT-RvNN. This is because OneSoft generally doubles the memory usage and EBT-RvNN already runs well without it. The combination of EBT-RvNN with OneSoft can be studied more in the future, but it is a variable that we do not focus on in this study. None of the fixes here makes any strict asymptotic difference in terms of sequence length but it does lift a large overhead that can be empirically demonstrated (see Table 1). ## 4 Beyond Sentence Encoding\n\nAs discussed before many of the previous models [9, 10, 31, 66] in this sphere that focus on competency on structure-sensitive tasks have been framed to work only as a sentence encoder of the form $f: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$. Taking a step further, we also explore a way to use bottom-up Tree-RvNNs for token-level contextualization, i.e., to make it serve as a function of the form $f: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. This allows Tree-RvNNs to be stackable with other deep learning modules like Transformers. Here, we consider whether we can re-formalize EBT-RvNNs for token contextualization. In EBT$\\mathrm{RvNN}^{5}$, strictly speaking, the output is not just the final sentence encoding (the root encoding), but also the intermediate non-terminal tree nodes. Previously, we ignored them after we got the root encoding. However, using them can be the key to creating a token contextualization out of EBT-RvNNs. Essentially, what EBT-RvNN will build is a tree structure with node representations the terminal nodes being the initial token vectors, the root node being the overall sentence encoding vector, and the non-terminal nodes representing different scales of hierarchies as previously discussed. Under this light, one way to create a token contextualization is to contextualize the terminal representations based on higher-level composite representations at different scales or hierarchies of which the terminal representation is a part of. In other words, while we use a bottom-up process of building wholes from parts during sentence encoding, for token contextualization, we can implement a top-down process of contextualizing parts from the wholes that it compose. A similar idea is used by Teng et al. [82] to recursively contextualize child node representations based on their immediate parent node using another recursive cell starting from the root and ending up at the terminal node representations. The contextualized terminal node representations can then become the contextualized token representations. But this idea requires costly sequential operations. An alternative - that we propose - is to allow the terminal nodes to attend [1] to the non-terminal nodes to retrieve relevant information to different scales of hierarchies. More precisely, if we want the terminal nodes to be contextualized by the wholes that they compose then we want to restrict the attention to only the parents (direct or indirect). This can be done by creating an attention mask based\n\n[^1]on the induced tree structures. In practice, we allow every terminal node as queries to attend to every node as keys and values but use a mask to allow attention only if the key represents the same node as that represented by the query or the key represents some parent (direct or indirect) of the node represented by the query. We also implement a relative positional encoding to bias attention [70] using the difference in heights of the nodes as the relative distance. In essence, we are proposing the use of a form of graph attention network [86]. This attention mechanism can be repeated for iterative refinement of the token representations through multiple layers of message-passing. In the case of EBT-RvNNs, we can create separate token representations for each beam and then marginalize them based on beam scores. We describe our concrete setup briefly below but more details are presented in Appendix F\nStep 1: First, we begin with beams of representations before they were marginalized. This allows us to access discrete edges connecting parents for every beam. As a setup, we have some $b$ beams of tree node representations and their structural information (edge connections). Step 2: We use Gated Attention Unit (GAU) [36], a modern Transformer variant, as the attention mechanism block. We use the terminal node representations as queries $(Q)$ and all the non-terminal nodes as keys $(K)$ and values $(V)$. We use GAU, like a graph attention network [86], by using an adjacency matrix $A$ as an attention mask. $A_{i j}$ is set as 1 if and only if $Q_{i}$ is a child of $K_{j}$ based on our tree extraction. Otherwise $A_{i j}$ is set as 0 . Thus attention is only allowed from parents to their terminal children (direct or indirect). Step 3: We implement a basic relative positional encoding - similar to that of Raffel et al. [64]. The only difference is that for us, the relative distances are the relative height distances. Step 4: The GAU layer is repeated for iterative refinement of terminal node representations. We repeat for two iterations since this is an expensive step. Step 5: As before, we marginalize the beams based on the accumulated log-softmax scores after the terminal node representations are contextualized. Use Case: In theory, the contextualized terminal node representations that are built up in Step 5 can be used for any task like sequence labelling or masked language modeling. At this point, we explore one specific use case - sentence-pair matching tasks (natural language inference and paraphrase detection). For these tasks we have two input sequences that we need to compare. Previously we only created sentence encoding for each sequences and made the vectors interact, but now we can make the whole of two sequences of contextualized terminal-node embeddings interact with each other through a stack of GAU-based self-attention. This is an approach that we use for some of the sentence-matching tasks in Natural Language Processing (Table 3).",
    "effibeam-6": "The models are trained end to end. We discuss the technical details about these architectural setups more explicitly in Appendix $F$. ## 5 Experiments and Results\n\n### 5.1 Model Nomenclature\n\nSentence Encoder models: Transformer refers to Transformers [85]; UT refers to Universal Transformers [13]; CRvNN refers to Continuous Recursive Neural Network [10]; OM refers to Ordered Memory [71]; BT-GRC refers to BT-RvNN implemented with GRC [66]; BT-GRC OS refers to BT-GRC combined with OneSoft (OS) Top- $K$ function [66]; EBT-GRC refers to our proposed EBT-RvNN model with GRC; GT-GRC refers to Gumbel-Tree RvNN [9] but with GRC as the recursive cell; EGT-GRC refers to GT-GRC plus the fixes that we propose. Sentence Interaction Models: Sequence interaction models refer to the models in the style described in Section 4 These models use some deeper interaction between contextualized token representations from both sequences without bottlenecking the interactions through a pair of vectors. We use EBTGAU to refer to the approach described in Section 4 EGT-GAU refers to a new baseline which uses the same framework as EBT-GAU except it replaces the Beam-Tree-Search with greedy STE gumbel-softmax based selection as in [9]. GAU refers to a plain stack of Gated Attention Units [36] (made to approximate the parameters of EBT-GAU) that do not use any Tree-RvNNs and is trained directly on $<\\mathrm{SEP}>$ concatenated sequence pairs. Table 1: Empirical time and (peak) memory consumption for various models on an RTX A6000. Ran on 100 ListOps data with batch size 1 and the same hyperparameters as used on ListOps on various sequence lengths. (-slice) indicates EBT-GRC without slicing from Fix 2, (512) indicates EBT-GRC with the hidden state dimension (d) set as 512 (instead of 128).(512,-slice) represents EBT-GRC with 512 dimensional hidden state size and without slicing. |  | Sequence Lengths |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Model | $200-250$ |  | $500-600$ |  | 900 - 1000 |  | 1500 - 2000 |  |\n|  | Time <br> (min) | Memory <br> (GB) | Time <br> (min) | Memory <br> (GB) | Time <br> (min) | Memory <br> (GB) | Time <br> (min) | Memory <br> $(\\mathrm{GB})$ |\n| OM | 8.0 | 0.09 | 20.6 | 0.21 | 38.2 | 0.35 | 76.6 | 0.68 |\n| CRvNN | 1.5 | 1.57 | 4.3 | 12.2 | 8.0 | 42.79 | OOM | OOM |\n| GT-GRC | 0.5 | 0.35 | 2.1 | 1.95 | 3.5 | 5.45 | 7.1 | 21.76 |\n| EGT-GRC | 1 | 0.07 | 2.5 | 0.3 | 4.3 | 0.81 | 8.5 | 3.15 |\n| BT-GRC | 1.1 | 1.71 | 2.6 | 9.82 | 5.1 | 27.27 | OOM | OOM |\n| BT-GRC OS | 1.4 | 2.74 | 4.0 | 15.5 | 7.1 | 42.95 | OOM | OOM |\n| EBT-GRC | 1.2 | 0.19 | 3.2 | 1.01 | 5.5 | 2.78 | 10.5 | 10.97 |\n| - slice | 1.2 | 0.35 | 3.2 | 1.95 | 5.4 | 5.4 | 10.3 | 21.12 |\n| (512) | 1.2 | 0.41 | 3.3 | 1.29 | 5.6 | 3.13 | 12.1 | 11.41 |\n| (512,- slice) | 1.2 | 1.55 | 3.3 | 7.77 | 5.5 | 21.02 | OOM | OOM |\n\n### 5.2 Efficiency Analysis\n\nIn Table 1, we compare the empirical time-memory trade offs of the most relevant Tree-RvNN models (particularly those that are competent in ListOps and logical inference).",
    "effibeam-7": "We use CRvNN in the no halting mode as [66] because otherwise it can start to halt trivially because of limited training data.",
    "effibeam-8": "For the splits of lengths $200-1000$ we use the data shared by Havrylov et al. [31]; for the $1500-2000$ split we sample from the training set of LRA listops [81]. We can observe from the table that EBT-GRC achieves better memory efficiency among all the strong RvNN contenders (GT-GRC and EGT-GRC fail on ListOps/Logical inference) except for OM. However, OM's memory efficiency comes with a massive cost in time, being nearly 8 times slower than EBT-GRC. Compared to BT-GRC OS's 43GB peak memory consumption in 900-1000 sequence length from [66], the memory consumption of EBT-GRC is reduced to only 2.8 GB . Even compared to BT-GRC, the reduction is near ten times. EBT-GRC even outperforms the original greedy GT-GRC used in Choi et al. [9]. Removing the slicing from the full model EBT-GRC (i.e., -slice) can substantially increase the memory cost. This becomes most apparent when training with higher hidden state size (compare (512) vs.",
    "effibeam-9": "(512,-slice)). This shows the effectiveness of slicing. ### 5.3 Results\n\nHyperparameters are presented in Appendix $G$ architecture details are presented in Appendix $F$ task details are provided in Appendix B and additional results (besides what is presented below) in logical inference and text classification are provided in Appendix C. List Operations (ListOps): The task of ListOps consist of hierarchical nested operations that neural networks generally struggle to solve particularly in length-generalizable settings.",
    "effibeam-10": "There are only a few known contenders that achieve decent performance in the task [31, 10, 71, 66]. For this task we use the original training set [57] with the length generalization splits from Havrylov et al. [31], the argument generalization splits from Ray Chowdhury and Caragea [66], and the LRA test set from Tay et al. [81]. The different splits test the model in different out-of-distribution settings (one in unseen lengths, another in an unseen number of arguments, and another in both unseen lengths and arguments). Remarkably, as can be seen from Table 2, EBT-GRC outperforms most of the previous models in accuracy - only being slightly behind OM for some argument generalization splits. EBT-GRC - Slice represents the performance of EBT-GRC without slicing. It shows that slicing in fact improves the accuracy as well in this context but even without slicing the model is better than BT-GRC or BT-GRC OS. Table 2: Accuracy on ListOps. For our models, we report the median of 3 runs. Our models were trained on lengths $\\leq 100$, depth $\\leq 20$, and arguments $\\leq 5$. * represents results copied from [71]. We bold the best results that do not use gold trees. Subscript represents standard deviation. As an example, $90_{1}=90 \\pm 0.1$\n\n| Model | near-IID | Length Gen. |  |  | Argument Gen. |  | LRA |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| (Lengths) | $\\leq 1000$ | 200-300 | 500-600 | 900-1000 | 100-1000 | 100-1000 | 2000 |\n| (Arguments) | $\\leq 5$ | $\\leq 5$ | $\\leq 5$ | $\\leq 5$ | 10 | 15 | 10 |\n| With gold trees |  |  |  |  |  |  |  |\n| GoldTreeGRC | 99.9 .2 | 99.9.9 | $99.8^{1}$ | 100.5 | $81.2_{28}$ | $79.5^{14}$ | $78.5_{29}$ |\n| Baselines without gold trees |  |  |  |  |  |  |  |\n| Transformer * | $57.4_{4}$ | - | - | - | - | \u2014 | \u2014 |\n| UT* | $71.5_{78}$ | \u2014 | \u2014 | \u2014 | \u2014 | - | \u2014 |\n| GT-GRC | 754.6 | 47.78 .4 | $42.7_{2.8}$ | $37.53_{37}$ | $50.9_{15}$ | $51.4_{16}$ | $45.3_{12}$ |\n| EGT-GRC | $84.2_{19}$ | $51.3_{37}$ | $42.9_{35}$ | $34.4_{35}$ | $44.7_{17}$ | $40.8_{16}$ | $34.4_{14}$ |\n| OM | 99.9.3 | 99.6.7 | $92.4_{13}$ | $76.3_{13}$ | 83.2 ${ }_{24}$ | $76.3_{38}$ | $79.3_{18}$ |\n| CRvNN | $99.7_{2.8}$ | $98.8_{11}$ | $97.2_{23}$ | $94.9_{49}$ | $66.6_{40}$ | $43.7_{38}$ | $55.38_{44}$ |\n| BT-GRC | $99.4_{2.7}$ | $96.8_{10}$ | $93.6_{22}$ | $88.4_{27}$ | $75.2_{28}$ | $59.1_{79}$ | $63.4_{57}$ |\n| BT-GRC OS | 99.65.4 | $97.2_{35}$ | $94.8_{65}$ | $92.2_{86}$ | $73.3_{64}$ | $63.1_{92}$ | $66.1_{101}$ |\n| EBT-GRC | $\\mathbf{9 9 .",
    "effibeam-11": "9}_{\\text {o. } 3}$ | $\\mathbf{9 9 . 7}_{\\text {2.4 }}$ | $99.5_{1}$ | $99.3_{5}$ | $82.5_{13}$ | $79.6_{8.7}$ | $79.3_{6.5}$ |\n| EBT-GRC - Slice | $99.7_{3}$ | $98.6_{12}$ | $98.4_{17}$ | $98.6_{14}$ | $79.3_{20}$ | $74.4_{37}$ | $75.5_{25}$ |\n\nLogical Inference and Sentiment Classification: We show the results of our models in formal logical inference (another dataset where only RvNN-like models have shown some success) and sentiment classification in Appendix C. There we show that our EBT-GRC can easily keep up on these tasks with BT-GRC despite being much more efficient. NLI and Paraphrase Detection: As we can see from Table 3, although EBT-GRC does not strictly outperform BT-GRC or BT-GRC OS, it remains in the same ballpark performance. The sentence interaction models, unsurprisingly, tend to have higher scores that sentence encoder modes because of their more parameters and more interaction space. We do not treat them commensurate here. Among the sequence interaction models, our EBT-GAU generally outperforms both baseline models in its vicinity - GAU and EGT-GAU. Even when used in conjunction with a Transformer, beam search still maintains some usefulness over simpler STE-based greedy search (EGT-GAU) and it shows some potential against pure Transformer stacks as well (GAU). ## 6 Conclusion\n\nWe identify a memory bottleneck in a popular RvNN framework [9] which has caused BT-RvNN [66] to require more memory than it needs to. Mitigating this bottleneck allows us to reduce the memory consumption of EBT-RvNN (an efficient variant of BT-RvNN) by 10-16 times without much other cost and while preserving similar task performances (and sometimes even beating the original BT-RvNN). The fixes also equally apply to any model using the framework including the original Gumbel Tree model [9]. We believe our work can serve as a basic baseline and a bridge for the development of more scalable models in the RvNN family and beyond. ## 7 Limitations\n\nAlthough our proposal improves upon the computational trade-offs over some of the prior works [10, 9, 66, 71], it can be still more expensive than standard RNNs although we address this limitation, to an extent, in our concurrent work [65]. Moreover, our investigation of utilizing bottom-up TreeRvNNs for top-down signal (without using expensive CYK models [15, 16]) is rather preliminary (efficiency being our main focus). This area of investigation needs to be focused more in the future. Moreover, although our proposal reduces memory usage it does not help much on accuracy scores compared with other competitive RvNNs. Table 3: Mean accuracy and standard deviaton on SNLI [6], QQP, and MNLI [89]. Hard represents the SNLI test set from Gururangan et al. [28], Break represents the SNLI test set from Glockner et al. [22]. Count. represents the counterfactual test set from Kaushik et al. [38]. PAWS ${ }_{Q Q P}$ and PAWS ${ }_{W I K I}$ are adversarial test sets from Zhang et al. [97], ConjNLI is the dev set from Saha et al. [67], NegM,NegMM,LenM,LenMM are Negation Match, Negation Mismatch, Length Match, Length Mismatch stress test sets from Naik et al. [56] respectively. Our models were run 3 times on different seeds. Subscript represents standard deviation. As an example, $90_{1}=90 \\pm 0.1$\n\n| Models | SNLI Training |  |  |  | QQP Training |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | IID | Hard | Break | Count. | IID | PAWS $_{Q Q P}$ | PAWS $_{W i k i}$ |\n| (Sequence Encoder Models) |  |  |  |  |  |  |  |\n| CRvNN | $85.3_{2}$ | $70.6_{4}$ | $55.3_{17}$ | $59.8_{6}$ | $84.8_{3}$ | $34.8_{7}$ | $46.6_{6}$ |\n| OM | $85.5_{2}$ | $70.6_{3}$ | $67.4_{9}$ | $59.9_{2}$ | $84.6_{0}$ | $38.1_{7}$ | $45.6_{8}$ |\n| BT-GRC | $84.9_{1}$ | $70_{5}$ | $51_{14}$ | $59_{4}$ | $84.7_{5}$ | $36.9_{17}$ | $46.4_{12}$ |\n| BT-GRC OS | $84.9_{1}$ | $70.3_{6}$ | $53.29_{10}$ | $58.6_{3}$ | $84.2_{2}$ | $37.1_{8}$ | $46.3_{6}$ |\n| EBT-GRC | $84.7_{4}$ | $69.9_{8}$ | $55.6_{20}$ | $58.1_{1}$ | $84.3_{2}$ | $36.9_{5}$ | $47.5_{5}$ |\n| (Sequence Interaction Models) |  |  |  |  |  |  |  |\n| GAU | $87_{2}$ | $74.2_{4}$ | $69.40_{34}$ | $66.4_{2.5}$ | $83.6_{1}$ | $38.6_{14}$ | $47.3_{1}$ |\n| EGT-GAU | $87.1_{0}$ | $74.8^{4}$ | $66.1_{12}$ | $66.2^{2}$ | $83.5_{4}$ | $39.4_{31}$ | $49.3_{13}$ |\n| EBT-GAU | $87.5_{2}$ | $75.7_{3}$ | $70_{26}$ | $67.6_{4}$ | $83.9_{2}$ | $42.3_{35}$ | $47.2_{8}$ |\n|  | MNLI Training |  |  |  |  |  |  |\n| Models | Match | MM | ConjNLI | NegM | NegMM | LenM | LenMM |\n| (Sequence Encoder Models) |  |  |  |  |  |  |  |\n| CRvNN | $72.2_{4}$ | $72.6_{5}$ | $41.7_{10}$ | $52.8^{6}$ | $53.8_{4.2}$ | $62_{44}$ | $63.3_{47}$ |\n| OM | $7^{72.5} 3$ | $7^{72}$ | $41.7_{4}$ | $50.9_{7}$ | $51.7_{13}$ | $56.5_{33}$ | $57.06_{31}$ |\n| BT-GRC | $71.6_{2}$ | $72.3_{1}$ | $40.7_{6}$ | $53.7_{37}$ | $54.8_{43}$ | $64.7_{6}$ | $66.4_{5}$ |\n| BT-GRC OS | $71.7_{1}$ | $71.9_{2}$ | $41.2^{9}$ | $53.2_{2}$ | $54.2_{5}$ | $65.6_{1} 3$ | $66.7_{9}$ |\n| EBT-GRC | $72.1_{2}$ | $72_{1}$ | $40.93_{0}$ | $52.33_{23}$ | $53.28_{22}$ | $64.92_{10}$ | $66.4_{10}$ |\n| (Sequence Interaction Models) |  |  |  |  |  |  |  |\n| GAU | $76.4_{3}$ | $76.5_{2}$ | $53.5_{12}$ | $48.2_{11}$ | $48.24_{11}$ | $69.6_{20}$ | $70.6_{22}$ |\n| EGT-GAU | $75.1_{2}$ | $75.5_{3}$ | $53.1_{4}$ | $48.7_{14}$ | $48.6_{14}$ | $69.8_{13}$ | $70.6_{11}$ |\n| EBT-GAU | $76.5_{1}$ | $76.9_{2}$ | $53.3_{18}$ | $49.2_{4}$ | $49_{3}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_a99d960147e1b11454d2g-10.jpg?height=53&width=113&top_left_y=1610&top_left_x=1357) | 72.5 ${ }_{19}$ |\n\n## 8 Acknowledgments\n\nThis research is supported in part by NSF CAREER award \\#1802358, NSF IIS award \\#2107518, and UIC Discovery Partners Institute (DPI) award.",
    "effibeam-12": "Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of NSF or DPI. We thank our anonymous reviewers for their constructive feedback. ## References\n\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473\n[2] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf\n[3] Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. ICML Workshop, abs/2107.05407, 2021. URL https://arxiv.org/abs/2107.05407. [4] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL http://arxiv.org/abs/1308.3432\n[5] Ben Bogin, Sanjay Subramanian, Matt Gardner, and Jonathan Berant. Latent compositional representations improve systematic generalization in grounded question answering. Transactions of the Association for Computational Linguistics, 9:195-210, 2021. doi: 10.1162/tacl_a_00361. URL https://aclanthology.org/2021.tacl-1.12. [6] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https://aclanthology.org/D15-1075. [7] Samuel R. Bowman, Christopher D. Manning, and Christopher Potts. Tree-structured composition in neural networks without tree-structured architectures. In Proceedings of the 2015th International Conference on Cognitive Computation: Integrating Neural and Symbolic Approaches - Volume 1583, COCO'15, page 37-42, Aachen, DEU, 2015. CEUR-WS.org. [8] Samuel R. Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D. Manning, and Christopher Potts. A fast unified model for parsing and sentence understanding. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1466-1477, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1139. URL https://aclanthology.org/ P16-1139. [9] Jihun Choi, Kang Min Yoo, and Sang-goo Lee. Learning to compose task-specific tree structures.",
    "effibeam-13": "In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5094-5101. AAAI Press, 2018. URL https://www. aaai.org/ocs/index.php/AAAI/AAAI18/paper/ view/16682. [10] Jishnu Ray Chowdhury and Cornelia Caragea. Modeling hierarchical structures with continuous recursive neural networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1975-1988. PMLR, 18-24 Jul 2021. URL https://proceedings mlr.press/v139/chowdhury21a.html. [11] Caio Corro and Ivan Titov. Learning latent trees with stochastic perturbations and differentiable dynamic programming. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5508-5521, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1551. URL https://aclanthology.org/ P19-1551. [12] R\u00f3bert Csord\u00e1s, Kazuki Irie, and J\u00fcrgen Schmidhuber. The neural data router: Adaptive control flow in transformers improves systematic generalization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=KBQP4A_J1K\n[13] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HyzdRiR9Y7. [14] Gr'egoire Del'etang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Marcus Hutter, Shane Legg, and Pedro A. Ortega. Neural networks and the chomsky hierarchy. ArXiv, abs/2207.02098, 2022. [15] Andrew Drozdov, Patrick Verga, Mohit Yadav, Mohit Iyyer, and Andrew McCallum. Unsupervised latent tree induction with deep inside-outside recursive auto-encoders. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 11291141, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1116. URL https://aclanthology.org/N19-1116\n[16] Andrew Drozdov, Subendhu Rongali, Yi-Pei Chen, Tim O'Gorman, Mohit Iyyer, and Andrew McCallum. Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside recursive autoencoders.",
    "effibeam-14": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4832-4845, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.392. URL https:// aclanthology.org/2020.emnlp-main.",
    "effibeam-15": "392 . [17] Brian DuSell and David Chiang. Learning context-free languages with nondeterministic stack RNNs. In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 507-519, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.conll-1.41. URL https://aclanthology.org/2020.conll-1.41. [18] Brian DuSell and David Chiang. Learning hierarchical structures with differentiable nondeterministic stacks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=5LXw_QplBiF\n[19] Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179-211, 1990. ISSN 0364-0213. doi: https://doi.org/10.1016/0364-0213(90)90002-E. URL https://wWw sciencedirect.com/science/article/pii/036402139090002E\n[20] Hao Fei, Yafeng Ren, and Donghong Ji. Retrofitting structure-aware transformer language model for end tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2151-2161, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.168. URL https:// aclanthology.org/2020.emnlp-main. 168\n[21] Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F.",
    "effibeam-16": "Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307-1323, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. findings-emnlp.117. URL https://aclanthology.org/2020.findings-emnlp.117. [22] Max Glockner, Vered Shwartz, and Yoav Goldberg. Breaking NLI systems with sentences that require simple lexical inferences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 650-655, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2103. URL https://aclanthology.org/P18-2103. [23] Yoav Goldberg and Michael Elhadad. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 742750, Los Angeles, California, June 2010. Association for Computational Linguistics. URL https://aclanthology.org/N10-1115. [24] C. Goller and A. Kuchler. Learning task-dependent distributed representations by backpropagation through structure.",
    "effibeam-17": "In Proceedings of International Conference on Neural Networks (ICNN'96), volume 1, pages 347-352 vol.1, 1996. doi: 10.1109/ICNN.1996.548916. [25] Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv, abs/1603.08983, 2016. URL http://arxiv.org/abs/1603.08983\n[26] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [27] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982-22994, 2022. [28] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/ N18-2017. URL https://aclanthology.org/N18-2017. [29] Michael Hahn. Theoretical limitations of self-attention in neural sequence models.",
    "effibeam-18": "Transactions of the Association for Computational Linguistics, 8:156-171, 2020. doi: 10.1162/tacl_a_00306. URL https://aclanthology.org/2020.tacl-1.11\n[30] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908-15919, 2021. [31] Serhii Havrylov, Germ\u00e1n Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax and semantics. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1118-1128, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1115. URLhttps://aclanthology org/N19-1115\n[32] Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generalization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 908-921, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.74. URL https://aclanthology.org/2021 acl-long. 74\n[33] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9 (8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735\n[34] Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, and Gerard de Melo. R2D2: Recursive transformer based on differentiable tree for interpretable hierarchical language modeling.",
    "effibeam-19": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4897-4908, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.379. URL https: //aclanthology.org/2021.acl-long. 379\n[35] Xiang Hu, Haitao Mi, Liang Li, and Gerard de Melo. Fast-R2D2: A pretrained recursive neural network based on pruned CKY for grammar induction and text representation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2809-2821, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main. 181\n[36] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 9099-9117. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/hua22a.html. [37] Shankar Iyer, Nikhil Dandekar, and Korn\u00e9l Csernai. First quora dataset release: Question pairs. In Quora, 2017. [38] Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. Learning the difference that makes a difference with counterfactually-augmented data. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SklgsONFvr\n[39] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. International Conference on Learning Representations, 2017. [40] Donald E. Knuth. On the translation of languages from left to right. Information and Control, 8 (6):607 - 639, 1965. ISSN 0019-9958. [41] Lingpeng Kong, Alexander M. Rush, and Noah A. Smith. Transforming dependencies into phrase structures. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 788798, Denver, Colorado, May-June 2015. Association for Computational Linguistics. doi: 10.3115/v1/N15-1080. URLhttps://aclanthology.org/N15-1080\n[42] Wouter Kool, Herke Van Hoof, and Max Welling. Stochastic beams and where to find them: The Gumbel-top-k trick for sampling sequences without replacement.",
    "effibeam-20": "In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 3499-3508. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/kool19a.html. [43] Phong Le and Willem Zuidema. Compositional distributional semantics with long short term memory. In Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics, pages 10-19, Denver, Colorado, June 2015. Association for Computational Linguistics. doi: 10.18653/v1/S15-1002. URL https://aclanthology.org/S15-1002\n[44] Phong Le and Willem Zuidema. The forest convolutional network: Compositional distributional semantics with a neural chart and without binarization.",
    "effibeam-21": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1155-1164, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1137. URL https://aclanthology.org/D15-1137. [45] Chenyao Liu, Shengnan An, Zeqi Lin, Qian Liu, Bei Chen, Jian-Guang Lou, Lijie Wen, Nanning Zheng, and Dongmei Zhang. Learning algebraic recombination for compositional generalization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 11291144, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. findings-acl.97. URL https://aclanthology.org/2021.findings-acl.97. [46] Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. Compositional generalization by learning analytical expressions. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020.",
    "effibeam-22": "Curran Associates Inc. ISBN 9781713829546. [47] Yang Liu and Mirella Lapata. Learning structured text representations. Transactions of the Association for Computational Linguistics, 6:63-75, 2018. doi: 10.1162/tacl_a_00005. [48] Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang. Easy-first POS tagging and dependency parsing with beam search. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 110-114, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology org/P13-2020. [49] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https://aclanthology.org/P11-1015. [50] Jean Maillard and Stephen Clark. Latent tree learning with differentiable parsers: Shift-reduce parsing and chart parsing. In Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP, pages 13-18, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2903. URL https:// aclanthology.org/W18-2903\n[51] Jean Maillard, Stephen Clark, and Dani Yogatama. Jointly learning sentence embeddings and syntax with unsupervised tree-lstms.",
    "effibeam-23": "Natural Language Engineering, 25(4):433-449, 2019. doi: 10.1017/S1351324919000184. [52] Jiayuan Mao, Freda H. Shi, Jiajun Wu, Roger P. Levy, and Joshua B. Tenenbaum. Grammarbased grounded lexicon learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https: //openreview.net/forum?id=iI6nkEZkO1\n[53] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length.",
    "effibeam-24": "In International Conference on Learning Representations, 2018. URL https://openreview net/forum?id=HyUNwulC-. [54] Tom McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1334. URL https://aclanthology.org/P19-1334. [55] Tsendsuren Munkhdalai and Hong Yu. Neural tree indexers for text understanding. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 11-21, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://aclanthology.org/E17-1002. [56] Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2340-2353, Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics. URL https://www. aclweb.org/ anthology/C18-1198\n[57] Nikita Nangia and Samuel Bowman. ListOps: A diagnostic dataset for latent tree learning.",
    "effibeam-25": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 92-99, New Orleans, Louisiana, USA, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-4013. URL https://aclanthology.org/N18-4013. [58] Xuan-Phi Nguyen, Shafiq Joty, Steven Hoi, and Richard Socher. Tree-structured attention with hierarchical accumulation. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HJxK5pEYvr\n[59] Vlad Niculae, Andre Martins, Mathieu Blondel, and Claire Cardie. SparseMAP: Differentiable sparse structured inference. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3799-3808, Stockholmsm\u00e4ssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. [60] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [61] Hao Peng, Sam Thomson, and Noah A. Smith. Backpropagating through structured argmax using a SPIGOT. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1863-1873, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1173. URL https://aclanthology.org/P18-1173. [62] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532-1543, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL https://aclanthology.org/ D14-1162. [63] Jordan B. Pollack. Recursive distributed representations. Artificial Intelligence, 46(1):77 105, 1990. ISSN 0004-3702. doi: https://doi.org/10.1016/0004-3702(90)90005-K. URL http://www.sciencedirect.com/science/article/pii/000437029090005K. [64] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html. [65] Jishnu Ray Chowdhury and Cornelia Caragea. Recursion in recursion: Two-level nested recursion for length generalization with scalabiliy. In Proceedings of the Neural Information Processing Systems, 2023. [66] Jishnu Ray Chowdhury and Cornelia Caragea. Beam tree recursive cells. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 28768-28791. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/ray-chowdhury23a.html. [67] Swarnadeep Saha, Yixin Nie, and Mohit Bansal. ConjNLI: Natural language inference over conjunctive sentences. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8240-8252, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.661. URL https:// aclanthology.org/2020.emnlp-main. 661. [68] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model.",
    "effibeam-26": "Trans. Neur. Netw., 20(1):61-80, jan 2009. ISSN 1045-9227. doi: 10.1109/TNN.2008.2005605. URL https://doi.org/10.1109/TNN.2008.2005605\n[69] M.P. Sch\u00fctzenberger. On context-free languages and push-down automata. Information and Control, 6(3):246 - 264, 1963.",
    "effibeam-27": "ISSN 0019-9958. [70] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464-468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. URL https://aclanthology.org/N18-2074. [71] Yikang Shen, Shawn Tan, Arian Hosseini, Zhouhan Lin, Alessandro Sordoni, and Aaron C Courville. Ordered memory. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 5037-5048. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/ 8748-ordered-memory.pdf. [72] Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. Ordered neurons: Integrating tree structures into recurrent neural networks. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=B116qiR5F7\n[73] Yikang Shen, Yi Tay, Che Zheng, Dara Bahri, Donald Metzler, and Aaron Courville. StructFormer: Joint unsupervised induction of dependency and constituency structure from masked language modeling. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7196-7209, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.559. URL https: //aclanthology.org/2021.acl-long. 559\n[74] Zhiqiang Shen, Zechun Liu, and Eric Xing. Sliced recursive transformer. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXIV, pages 727-744.",
    "effibeam-28": "Springer, 2022. [75] Haoyue Shi, Hao Zhou, Jiaze Chen, and Lei Li. On tree-based neural sentence modeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4631-4641, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1492. URL https://aclanthology.org/D18-1492. [76] Bing Shuai, Zhen Zuo, Bing Wang, and Gang Wang. Dag-recurrent neural networks for scene labeling. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $3620-3629$, 2016. [77] Richard Socher, Christopher D. Manning, and Andrew Y. Ng. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, 2010. [78] Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151161, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL https://aclanthology.org/D11-1014. [79] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170. [80] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1556-1566, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1150. URL https://aclanthology.org/P15-1150. [81] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers.",
    "effibeam-29": "In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k\n[82] Zhiyang Teng and Yue Zhang. Head-lexicalized bidirectional tree LSTMs. Transactions of the Association for Computational Linguistics, 5:163-177, 2017. doi: 10.1162/tacl_a_00053. URL https://aclanthology.org/Q17-1012. [83] Veronika Thost and Jie Chen. Directed acyclic graph neural networks. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=JbuYF437WB6. [84] Ke Tran, Arianna Bisazza, and Christof Monz. The importance of being recurrent for modeling hierarchical structure. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4731-4736, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1503. URL https://aclanthology.org/D18-1503. [85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin.",
    "effibeam-30": "Attention is all you need.",
    "effibeam-31": "In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[86] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rJXMpikCZ\n[87] Yaushian Wang, Hung-Yi Lee, and Yun-Nung Chen. Tree transformer: Integrating tree structures into self-attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1061-1070, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1098. URL https://aclanthology.org/D19-1098. [88] Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural language sentences. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, IJCAI'17, page 4144-4150. AAAI Press, 2017. ISBN 9780999241103. [89] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https: //aclanthology.org/N18-1101. [90] Zhaofeng Wu. Learning with latent structures in natural language processing: A survey. arXiv preprint arXiv:2201.00490, 2022. [91] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32(1):4-24, 2021. doi: 10.1109/TNNLS.2020.2978386. [92] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. Bp-transformer: Modelling long-range context via binary partitioning.",
    "effibeam-32": "arXiv preprint arXiv:1911.04070, 2019. [93] Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. Learning to compose words into sentences with reinforcement learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [94] Fabio Massimo Zanzotto, Andrea Santilli, Leonardo Ranaldi, Dario Onorati, Pierfrancesco Tommasino, and Francesca Fallucchi. KERMIT: Complementing transformer architectures with encoders of explicit syntactic interpretations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 256-267, Online, November 2020. Association for Computational Linguistics. doi: $10.18653 / v 1 / 2020$.emnlp-main.18. URL https://aclanthology.org/2020.emnlp-main. 18\n[95] Aston Zhang, Yi Tay, Yikang Shen, Alvin Chan, and SHUAI ZHANG. Self-instantiated recurrent units with dynamic soft recursion.",
    "effibeam-33": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 6503-6514. Curran Associates, Inc., 2021. URLhttps://proceedings neurips.cc/paper/2021/file/3341f6f048384ec73a7ba2e77d2db48b-Paper.pdf\n[96] Yu Zhang, Qingrong Xia, Shilin Zhou, Yong Jiang, Guohong Fu, and Min Zhang. Semantic role labeling as dependency parsing: Exploring latent tree structures inside arguments. In Proceedings of the 29th International Conference on Computational Linguistics, pages 42124227, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/2022.coling-1.370. [97] Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298-1308, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1131. URL https://aclanthology org/N19-1131\n[98] Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo. Long short-term memory over recursive structures. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1604-1612, Lille, France, 07-09 Jul 2015. PMLR. URL https://proceedings.mlr press/v37/zhub15.html. [99] Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. DAG-structured long short-term memory for semantic compositionality. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 917-926, San Diego, California, June 2016.",
    "effibeam-34": "Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / \\mathrm{N} 16-1106$. URL https://aclanthology.org/N16-1106\n\n## A Appendix Organization\n\nIn Section we describe the settings of all the tasks and datasets that we have tested our models on. In Section C, we provide additional results on logical inference and sentiment classification. Then, in Section D, we present an extended survey of related works. In Section E, we present the pseudocode for EBT-RvNN. In Section F we detail our architecture setup including the sequence-interaction models. In Section G, we provide our hyperparameters. ## B Task Details\n\nListOps: ListOps was introduced by Nangia and Bowman [57] and is a task for solving nested lists of mathematical operations. It is a 10 -way classification task. Similar to Chowdhury and Caragea [10], we train our models on the original training set with all samples $\\geq 100$ sequence lengths filtered out. We use the original development set for validation. We test on the following sets: the original test set (near-IID split); the length generalization splits from Havrylov et al. [31] that include samples of much higher lengths; the argument generalization splits from Ray Chowdhury and Caragea [66] that involve an unseen number of maximum arguments for each operator; and the LRA split (which has both higher sequence length and higher argument number) from Tay et al. [81]. Logical Inference: Logical Inference was introduced by Bowman et al. [7] and is a task that involves classifying fine-grained inferential relations between two given sequences in a form similar to that of formal sentences of propositional logic. Similar to Tran et al. [84], our models were trained on splits with logical connectives $\\leq 6$. We show the results in OOD test sets with logical connections 10-12.",
    "effibeam-35": "We use the same splits as Shen et al. [71], Tran et al. [84], Chowdhury and Caragea [10]. SST5: SST5 is a fine-grained 5 -way sentiment classification task introduced by Socher et al.",
    "effibeam-36": "[79]. We use the original splits. IMDB: IMDB is a binary sentiment classification task from Maas et al.",
    "effibeam-37": "[49]. We use the same train, validation, and IID test sets as created in Ray Chowdhury and Caragea [66]. We also use the contrast set Gardner et al.",
    "effibeam-38": "[21] and counterfactual set Kaushik et al. [38] as additional test splits. QQP: QQP ${ }^{[6]}$ [37] is a task of classifying whether two given sequences in a pair are paraphrases of each other or not. Following prior works Wang et al. [88], we randomly sample 10,000 samples for validation and IID test set such that for each split 5,000 samples are maintained to be paraphrases and the other 5,000 are maintained to be not paraphrases. We also use the adversarial test sets PAWS $_{Q Q P}$ and PAWS ${ }_{W I K I}$ form Zhang et al.",
    "effibeam-39": "[97]. SNLI: SNLI [6] is a natural language inference (NLI) task. It is a 3 -way classification task to classify the inferential relation between two given sequences. We use the same train, development, and IID test set splits as in Chowdhury and Caragea [10]. Any data with a sequence of length $\\geq 150$ is filtered out from the training set for efficiency. We use also additional test set splits for stress tests. We use the hard test set split from Gururangan et al. [28], the break test set from Glockner et al. [22], and the counterfactual test set from Kaushik et al. [38]. MNLI: MNLI [89] is another NLI dataset, which is similar to SNLI in format.",
    "effibeam-40": "We use the original development sets (match and mismatch) as test sets. We filter out all data with any sequence length $\\geq 150$ from the training set. Our actual development set is a random sample of 10,000 data-points from the filtered training set. As additional testing sets, we use the development set of Conjunctive NLI (ConjNLI) [67] and a few of the stress sets from Naik et al. [56]. These stress test sets include - Negation Match (NegM), Negation Mismatch (NegMM), Length Match (LenM), and Length Mismatch (LenMM). NegM and NegMM add tautologies containing \"not\" terms - this can bias the models to classify contradiction as the inferential relation because the training set contains spurious correlations between existence of \"not\" related terms and the class of contradiction. LenM and LenMM add tautologies to artificially increase the lengths of the samples without changing the inferential relation class. [^2]Table 4: Mean accuracy and standard deviation on the Logical Inference [7] for $\\geq 10$ number of operations after training on samples with $\\leq 6$ operations, and on SST5 [79] and IMDB [49].",
    "effibeam-41": "Count. represents counterfactual test split from Kaushik et al. [38] and Cont. represents contrast test split from Gardner et al. [21] The best results are shown in bold. Our models were run 3 times on different seeds. Subscript represents standard deviation. As an example, $90_{1}=90 \\pm 0.1$\n\n| Model | Logical Inference <br> Number of Operations |  |  | SST5 <br> IID | IMDB |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 10 | 11 | 12 |  | IID | Cont. | Count. |\n| GT-GRC | $90.33_{22}$ | $88.43_{18}$ | $85.70_{24}$ | $51.67_{8.8}$ | $85.11_{1} 0$ | $70.63_{21}$ | $81.97_{5}$ |\n| EGT-GRC | $75.79_{61}$ | $73.38{ }_{68}$ | $69.68^{7.8}$ | $51.63_{14}$ | $86.58_{2.7}$ | 729.2 | $81.76_{14}$ |\n| CRvNN | $94.51_{2.9}$ | $94.48_{5.6}$ | $92.73_{15}$ | $51.75_{11}$ | $91.47_{1.2}$ | $77.80_{15}$ | $85.38_{3.5}$ |\n| OM | $94.95_{2}$ | $93.9_{2.2}$ | $93.36_{6.2}$ | $52.30_{2.7}$ | 91.69 $_{\\text {0.5 }}$ | $76.98_{5.8}$ | $83.68_{7.8}$ |\n| BT-GRC | $95.04_{2.3}$ | $94.29_{3.8}$ | $93.36_{2.4}$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_a99d960147e1b11454d2g-20.jpg?height=37&width=142&top_left_y=755&top_left_x=1090) | $91.29_{1.2}$ | $75.07_{29}$ | $82.86_{23}$ |\n| BT-GRC OS | $\\mathbf{9 5 . 4 3}_{4.5}$ | $94.21_{6.6}$ | $\\mathbf{9 3 . 3 9}^{\\text {1.5 }}$ | $51.92_{7.2}$ | 90.869.3 | $75.68_{21}$ | $84.77_{11}$ |\n| EBT-GRC | $94.95_{1.5}$ | $93.87_{7.4}$ | $93.04_{6.7}$ | $52.22_{1}$ | $91.47_{1.2}$ | $76.16_{17}$ | $84.29_{12}$ |\n\n## C Additional Results\n\nIn Table 4, we show that our EBT-GRC model can keep up fairly well with BT-GRC and BT-GRC OS on logical inference [7] and sentiment classification tasks like SST5 [79], and IMDB [21] while being much more computationally efficient as demonstrated in the main paper.",
    "effibeam-42": "Additional comparisons with other models like Transformers and Universal Transformer in logical inference can be found in prior works Shen et al.",
    "effibeam-43": "[71], Tran et al. [84]. They underperform RNNs and RvNNs in logical inference. ## D Extended Related Works\n\nRvNN History: Recursive Neural Networks (RvNNs) in the more specified sense of building representations through trees and directed acyclic graphs were proposed in [63, 24]. Socher et al. [77, 78, 79] extended the use of RvNNs in Natural Language Processing (NLP) by considering constituency trees and dependency trees. A few works [98, 80, 43, 99] started adapting Long Shot-term Memory Networks [33] as a cell function for recursive processing. Le and Zuidema [44], Maillard et al. [51] proposed a chart-based method for simulating bottom-up Recursive Neural Networks through dynamic programming. Shi et al. [75], Munkhdalai and Yu [55] explored heuristicsbased tree-structured RvNNs. RvNNs can also be simulated by stack-augmented recurrent neural networks (RNNs) to an extent (similar to how pushdown automata can model context-free grammar [69, 40]). There are multiple works on stack-augmented RNNs [8, 93, 50]. Ordered Memory [71] is one of the more modern such examples. More recently, DuSell and Chiang [17, 18] explored non-deterministic stack augmented RNNs and Del'etang et al. [14] explored other expressive models. Wu [90] presented a survey of latent structure models. Choi et al. [9] proposed a greedy search strategy based on easy-first algorithm [23, 48] for auto-parsing structures for recursion utilizing STE gumbel softmax for gradient signals. Peng et al. [61] extended the framework with SPIGOT and Havrylov et al. [31] extended it with reinforcement learning (RL). Ray Chowdhury and Caragea [66] extended it with beam search and soft top-k. Chowdhury and Caragea [10], Zhang et al. [95] introduced different forms of soft-recursion. Top-down Signal: Similar to us, Teng and Zhang [82] explored bidirectional signal propagation (bottom-up and top-down). However, they sent top-down signal in a sequential manner which can be expensive - either it can get slow without parallelization or memory-wise expensive with parallelization of contextualization of nodes in the same height. Our approach in EBT-GAU also has some kinship with BP-Transformer [92]. BP-Transformer allows message passing between a fixed subset of parent nodes and terminal nodes created using a heuristics-based balanced binary tree. Chart-based models can also create sequence contextualized representations [15, 16] but they can be quite expensive by default [66] needing their own separate techniques [34, 35]. ```\nAlgorithm 1 Efficient Beam Tree Cell (without slicing)\n    Input: data \\(X=\\left[x_{1}, x_{2}, \\ldots x_{n}\\right], k\\) (beam size)\n    Beam \\(X \\leftarrow[X]\\)\n    BeamScores \\(\\leftarrow[0]\\)\n    while True do\n        if \\(\\operatorname{len}(\\operatorname{BeamX}[0])==1\\) then\n            \\(\\operatorname{BeamX} \\leftarrow[\\) beam \\([0]\\) for beam in BeamX]\n            break\n        end if\n        if \\(\\operatorname{len}(\\operatorname{BeamX}[0])==2\\) then\n            BeamX \\(\\leftarrow[\\) cell(beam[0], beam[1]) for beam in BeamX]\n            break\n        end if\n        NewBeamX \\(\\leftarrow[]\\)\n        NewBeamScores \\(\\leftarrow[]\\)\n        for Beam,BeamScore in \\(z i p(B e a m X\\), BeamScores) do\n            Scores \\(\\leftarrow \\log \\circ\\) softmax \\(([\\operatorname{scorer}(\\) beam \\([i]\\), beam \\([i+1])\\) for parent in Parents \\(])\\)\n            Indices \\(\\leftarrow \\operatorname{topk}(\\) Scores,\\(k)\\)\n            for \\(i\\) in \\(\\operatorname{range}(K)\\) do\n                newBeam \\(\\leftarrow\\) deepcopy \\((\\) Beam \\()\\)\n                newBeam \\([\\) Indices \\([i]] \\leftarrow \\operatorname{cell}(\\) Beam \\([\\operatorname{Indices}[i]]\\), Beam \\([\\) Indices \\([i]+1])\\)\n                    Delete newBeam \\([\\) Indices \\([i]+1]\\)\n                NewBeamX.append(newBeam)\n                newScore \\(\\leftarrow\\) BeamScore + Scores \\([\\) indices \\([i]]\\)\n                newBeamScores.append(newScore)\n            end for\n        end for\n        Indices \\(\\leftarrow\\) topk(newBeamScores, \\(k)\\)\n        BeamScores \\(\\leftarrow[\\) newBeamScores \\([i]\\) for \\(i\\) in Indices \\(]\\)\n        Beam \\(X \\leftarrow[\\) newBeam \\(X[i]\\) for \\(i\\) in Indices \\(]\\)\n    end while\n    BeamScores \\(\\leftarrow\\) Softmax(BeamScores)\n    Return \\(\\operatorname{sum}([\\operatorname{score} * X\\) for score, \\(X\\) in \\(z \\operatorname{ip}(\\) BeanScores, BeamX \\()])\\)\n```\n\nTransformers + RvNNs: There have been several approaches to incorporating RvNN-like inductive biases to Transformers.",
    "effibeam-44": "For instance, Universal Transformer [13] introduced weight-sharing and dynamic halt to Transformers. Csord\u00e1s et al. [12] extended on universal transformer with geometric attention for locality bias and gating. Shen et al. [74] built on weight-shared transformers with high layer depth and group self-attention. Wang et al. [87], Nguyen et al. [58], Shen et al. [73] added hierarchical structural biases to self-attention. Fei et al. [20] biased pre-trained Transformers to have constituent information in intermediate representations. Hu et al. [34] used Transformer as binary recursive cells in chart-based encoders. ## E Pseudocode\n\nWe present the pseudocode of EBT-RvNN in Algorithm 1.",
    "effibeam-45": "Note that the algorithms are written as they are for the sake of illustration: in practice, many of the nested loops are made parallel through batched operations. ## F Architecture details\n\n## F. 1 Sentence Encoder Models\n\nFor the sentence encoder models the architectural framework we use is the same siamese dual-encoder setup as Ray Chowdhury and Caragea [66].",
    "effibeam-46": "## F. 2 Sentence Interaction Models\n\nGAU-Block: Our specific implementation of a GAU-block [36] is detailed below. Our GAUBlock can be defined as GAUBlock $(x, p, G)$. The function arguments are of the following forms: $x \\in \\mathbb{R}^{n \\times d}, p \\in \\mathbb{R}^{l \\times d}$ and $G \\in\\{0,1\\}^{n \\times l}$. $x$ accepts the main sequence of vectors that is to serve as attention queries; $p$ accepts either the sequence of intermediate node representations created from our RvNN (for parent attention) or it accepts the same input as $x$ (for usual cases); $p$ serves as keys and values for attention; $G$ accepts either the adjacency matrix in case of parent attention (where $G_{i j}=1$ iff $p_{j}$ is a parent of $x_{i}$ else $G_{i j}=0$ ), otherwise, it accepts just the usual attention mask; either way, $G$ serves as an attention mask. $$\n\\begin{gathered}\nx^{\\prime}=L N\\left(x W_{\\text {init }}+b_{\\text {init }}\\right) ; \\quad p^{\\prime}=L N\\left(p W_{\\text {init }}+b_{\\text {init }}\\right) \\\\\nu=\\operatorname{SiLU}\\left(x^{\\prime} W_{u}+b_{u}\\right) ; \\quad v=\\operatorname{SiLU}\\left(p^{\\prime} W_{v}+b_{v}\\right) \\\\\nq=z_{q} \\odot \\operatorname{SiLU}\\left(x^{\\prime} W_{z}+b_{z}\\right)+z b_{q} ; \\quad k=z_{k} \\odot \\operatorname{SiLU}\\left(p^{\\prime} W_{z}+b_{z}\\right)+z b_{k} \\\\\nA=\\operatorname{Softmax}\\left(\\frac{q k^{T}+p o s}{\\sqrt{2 d}}, \\operatorname{mas} k=G\\right) \\\\\nv^{\\prime}=A v \\\\\no=\\left(u \\odot v^{\\prime}\\right) W_{o}+b_{o} \\\\\ng=\\operatorname{Sigmoid}\\left([o ; x] W_{\\text {gate }}+b_{\\text {gate }}\\right) \\\\\n\\text { out }=g \\odot o+(1-g) \\odot x\n\\end{gathered}\n$$\n\nHere, $W_{\\text {init }} \\in \\mathbb{R}^{d \\times d} ; W_{z} \\in \\mathbb{R}^{d \\times d_{h}}, W_{u}, W_{v} \\in \\mathbb{R}^{d \\times 2 d}, b_{i n i t}, b_{z}, b_{o} \\in \\mathbb{R}^{d} ; z_{q}, z b_{q}, z_{k}, z b_{k} \\in$ $\\mathbb{R}^{d_{h}} ; b_{u}, b_{v} \\in \\mathbb{R}^{2 d}, W_{o}, W_{\\text {gate }} \\in \\mathbb{R}^{2 d \\times d}$.",
    "effibeam-47": "[;] represents concatenation. $L N$ is layer normalization. pos is calculated using the technique of Raffel et al. [64] using relative tree height distance for parent attention, or relative positional distance for usual cases. GAU Sequence Interaction Setup: Let GAUStack represent some arbitrary number of compositions of GAUBlocks (multilayered GAU block). GAUStack has the same function arguments as GAUBlock. Given two sequences $\\left(x_{1}, x_{2}\\right)$ and their corresponding attention masks $\\left(M_{1}, M_{2}\\right)$ as inputs where $x_{1} \\in \\mathbb{R}^{n_{1} \\times d}, x_{2} \\in \\mathbb{R}^{n_{2} \\times d}, M_{1} \\in\\{0,1\\}^{n_{1} \\times n_{1}}, M_{1} \\in\\{0,1\\}^{n_{2} \\times n_{2}}$, the GAU setup can be expressed as:\n\n$$\n\\begin{gathered}\n\\text { inp }=\\left[C L S+\\operatorname{seg}_{1} ; x_{1}+\\text { seg }_{1} ; S E P ; C L S+\\text { seg }_{2}, x_{2}+\\text { seg }_{2}\\right] \\\\\nr=\\operatorname{GAUStack}\\left(x=\\text { inp }, p=i n p, G=f\\left(M_{1} ; M_{2}\\right)\\right) \\\\\n\\alpha=\\operatorname{Softmax}\\left(\\operatorname{GELU}\\left(r W_{1}+b_{1}\\right) W_{2}+b_{2}\\right) \\\\\nc l s^{\\prime}=\\sum_{i} \\alpha_{i} r \\\\\n\\text { logits }=\\operatorname{GELU}\\left(c l s^{\\prime} W_{1}^{\\text {logits }}+b_{1}^{\\text {logits }}\\right) W_{2}^{\\text {logits }}+b_{2}^{\\text {logits }}\n\\end{gathered}\n$$\n\nHere, $C L S, S E P, \\operatorname{seg}_{1}, s e g_{2} \\in \\mathbb{R}^{1 \\times d}$ are randomly initialized trainable vectors; $\\operatorname{seg}_{1}$, seg $_{2}$ are segment embeddings.",
    "effibeam-48": "$W_{1} \\in \\mathbb{R}^{d \\times d}, W_{2} \\in \\mathbb{R}^{d \\times 1} ; b_{1}, b_{2}, b_{1}^{\\text {logits }} \\in \\mathbb{R}^{d} ; b_{2}^{\\text {logits }} \\in \\mathbb{R}^{c} ; W_{1}^{\\text {logits }} \\in$ $\\mathbb{R}^{d \\times d}, W_{2}^{\\text {logits }} \\in \\mathbb{R}^{d \\times c} . c$ is the number of classes for the task. $f$ is a function that takes the attention masks as input and concatenates them while adjusting for the special tokens (CLS, SEP). EGT-GAU Sequence Interaction Setup: EGT-GAU starts from the same input as above. Let us also assume we have the $\\operatorname{EGT}-\\operatorname{GRC}(x)$ module which takes a sequence of vectors $x \\in \\mathbb{R}^{n \\times d}$ as the input to recursively process and outputs $(c l s, p, G)$ where $c l s \\in \\mathbb{R}^{1 \\times d}$ is the root representation, $p \\in \\mathbb{R}^{l \\times d}$ is the sequence of non-terminal representations from the tree, and $G \\in\\{0,1\\}^{n \\times l}$ is the\nadjacency matrix for parent attention (i.e., $G_{i j}=1$ iff $p_{j}$ is a parent of $x_{i}$, else $G_{i j}=0$ ). Technically, tree height information is also extracted for relative position but we do not express that explicitly for the sake of brevity. With these elements, EGT-GAU can be expressed as below:\n\n$$\n\\begin{gathered}\nc l s_{1}, p_{1}, G_{1}=\\operatorname{EGT}-\\operatorname{GRC}\\left(x=x_{1}\\right) ; \\quad c l s_{2}, p_{2}, G_{2}=\\operatorname{EGT}-\\operatorname{GRC}\\left(x=x_{2}\\right) \\\\\nx_{1}^{\\prime}=\\operatorname{GAUStack}_{1}\\left(x=x_{1}, p=p_{1}, G=G_{1}\\right) ; \\quad x_{2}^{\\prime}=\\operatorname{GAUStack}{ }_{1}\\left(x=x_{2}, p=p_{2}, G=G_{2}\\right) \\\\\nc l s_{1}^{\\prime}=\\operatorname{GELU}\\left(c l s_{1} W_{1}^{c l s}+b_{1}^{c l s}\\right) W_{2}^{c l s}+b_{2}^{c l s} ; \\quad c l s_{2}^{\\prime}=\\operatorname{GELU}\\left(c l s_{2} W_{1}^{c l s}+b_{1}^{c l s}\\right) W_{2}^{c l s}+b_{2}^{c l s} \\\\\ni n p=\\left[c l s_{1}^{\\prime}+\\operatorname{seg}_{1} ; x_{1}^{\\prime}+\\operatorname{seg}_{1} ; S E P ; c l s_{2}^{\\prime}+\\operatorname{seg}_{2}, x_{2}^{\\prime}+s e g_{2}\\right] \\\\\nr=\\operatorname{GAUStack}_{2}\\left(x=i n p, p=i n p, G=f\\left(M_{1}, M_{2}\\right)\\right)\n\\end{gathered}\n$$\n\nEverything else after eqn.",
    "effibeam-49": "25 is the same as eqn. 18 to $20 . S E P, \\operatorname{seg}_{1}, s e g_{2} \\in R^{1 \\times d} ; \\operatorname{seg}_{1}$, seg $_{2}$ are segment embeddings as before. $W_{1}^{c l s}, W_{2}^{c l s} \\in \\mathbb{R}^{d \\times d} ; b_{1}^{c l s}, b_{2}^{c l s} \\in \\mathbb{R}^{d}$. EBT-GAU Sequence Interaction Setup: This setup is similar to that of EGT-GAU but with a few changes. EBT-GAU uses EBT-GRC as a module instead of EGT-GRC. EBT-GAU returns outputs of the form $(c l s, b p, b G, s)$ where $c l s \\in \\mathbb{R}^{1 \\times d}$ is the beam-score-weighted-averaged root representation, $b p \\in \\mathbb{R}^{b \\times l \\times d}$ are the beams (beam size $b$ ) of sequences of non-terminal representations from the tree, $b G \\in\\{0,1\\}^{b \\times n \\times l}$ are the beams of adjacency matrices for parent attention, and $s \\in \\mathbb{R}^{b}$ are the softmax-normalized beam scores. Let NGAUStack represent the same function as GAUStack but formalized for batched processing of multiple beams of sequences. With these elements, EBT-GAU can be expressed as:\n\n$$\n\\begin{aligned}\nc l s_{1}, b p_{1}, b G_{1}, s_{1}=\\operatorname{EBT}-\\operatorname{GRC}\\left(x=x_{1}\\right) ; & c l s_{2}, b p_{2}, b G_{2}, s_{2}=\\operatorname{EBT}-\\operatorname{GRC}\\left(x=x_{2}\\right) \\\\\nb x_{1}=\\operatorname{repeat}\\left(x_{1}, b\\right) ; & b x_{2}=\\operatorname{repeat}\\left(x_{2}, b\\right) \\\\\nb x_{1}^{\\prime}=\\operatorname{NGAUStack}_{1}\\left(b x_{1}, b p_{1}, b G_{1}\\right) ; & b x_{2}^{\\prime}=\\operatorname{NGAUStack}_{1}\\left(b x_{2}, b p_{2}, b G_{2}\\right) \\\\\nx_{1}^{\\prime}=\\sum_{i} s[i] \\cdot b x_{1}^{\\prime}[i] ; & x_{2}^{\\prime}=\\sum_{i} s[i] \\cdot b x_{2}^{\\prime}[i]\n\\end{aligned}\n$$\n\nEverything else after eqn. 29 is the same as the equations 23 . 25 followed by the equations 18 to 20 repeat $(x, b)$ changes $x \\in \\mathbb{R}^{n \\times d}$ to $b x \\in \\mathbb{R}^{b \\times n \\times d}$ by batching the same $x$ for $b$ times. ## G Hyperparameter details\n\nFor sentence encoder models, we use the same hyperparameters as [66] (the preprint of the paper is available in the supplementary in anonymized form) for all the datasets. The only new hyperparameter for EBT-GRC is $d_{s}$ which we set as 64 ; otherwise the hyperparameters are the same as that of BT-GRC or BT-GRC OS. We discuss the hyperparameters of the sequence interaction models next. For EBT-GAU/EGT-GAU, we used a two-layered weight-shared GAU-Blocks for NGAUStack $_{1} /$ GAUStack $_{1}$ and a three-layered weight-shared GAU-Blocks for GAUStack ${ }_{2}$ (for parameter efficiency and regularization). GAU uses a five-layered GAU-Blocks (weights unshared) for GAUStack so that the parameters are similar to that of EBT-GAU or EGT-GAU. We use a dropout of 0.1 after the multiplation with $W_{o}$ in each GAUBlock layer and a head size $d_{h}$ of 128 (similar to Hua et al. [36]). For relative position, we set $k=5$ ( $k$ here corresponds the receptive field for relative attention in Shaw et al. [70]) for normal GAUBlocks and $k=10$ for parent attention (since parent attention is only applied to higher heights, we do not need to initialize weights for negative relative distances). Other hyperparameters are kept same as the sentence encoder models. The hyperparameters of MNLI, SNLI, and QQP are shared. Note that all the natural language tasks are trained with fixed 840B Glove Embeddings [62] as in Ray Chowdhury and Caragea [66]. All models were trained in a single Nvidia RTX A6000. The code is available in the supplementary. [^0]:    ${ }^{1}$ This is done mainly to simplify the search space but assuming projective structures is also standard fare in Natural Language Processing (NLP) [41, 96]. 2\"temporal\" refers to the original sequential order. The original order is preserved in every iteration. ${ }^{3}$ Similar inductive biases can be favorable in computer vision as well 763074\n    ${ }^{4}$ Overall, Tree-RvNNs, as described above, can be also considered as a special case of DAG-GNN [83] where similar principles can apply. Exploring bottom-up tree node building through a more flexible space of DAG-structures can be a direction to consider in the future. [^1]:    ${ }^{5}$ The principles discussed here also apply to them Gumbel Tree RvNNs, BT-RvNNs, and the like. [^2]:    ${ }^{6}$ https://data.quora.com/First-Quora-Dataset-Release-QuestionPairs\n\n"
}