{
    "contdiffu-0": "# Continuous diffusion for categorical data \n\nSander Dieleman ${ }^{1}$, Laurent Sartran ${ }^{1}$, Arman Roshannai ${ }^{2 *}$, Nikolay Savinov ${ }^{1}$, Yaroslav Ganin ${ }^{1}$, Pierre H.<br>Richemond ${ }^{1}$, Arnaud Doucet ${ }^{1}$, Robin Strudel ${ }^{3 *}$, Chris Dyer ${ }^{1}$, Conor Durkan ${ }^{1}$, Curtis Hawthorne ${ }^{4}$, R\u00e9mi<br>Leblond ${ }^{1}$, Will Grathwoh ${ }^{1}$ and Jonas Adler ${ }^{1}$<br>${ }^{1}$ DeepMind, ${ }^{2}$ University of Southern California, ${ }^{3}$ INRIA, Ecole Normale Sup\u00e9rieure, ${ }^{4}$ Google Research, Brain Team, *Work done<br>while at DeepMind\n\nDiffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement.",
    "contdiffu-1": "Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space.",
    "contdiffu-2": "We demonstrate its efficacy on several language modelling tasks. ## 1. Introduction\n\nGenerative models have seen a rapid increase in scale and capabilities over the past few years, across many modalities, including images, audio signals, video and text (Borsos et al., 2022; Brown et al., 2020; Dhariwal et al., 2020; Ho et al., 2022a; Ramesh et al., 2022; Saharia et al., 2022b). In language modelling, the focus has been on scaling up and expanding the capabilities of autoregressive models, instigated by the development of the Transformer architecture (Vaswani et al., 2017). This has resulted in general-purpose language models that are suitable for practical use. Until recently, work on visual modalities lagged behind in terms of scale and practicability, but the development of diffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2015; Song and Ermon, 2019) has resulted in a noticeable step change in capabilities. Whereas previous generative models of images were relatively inflexible and tended to produce lowresolution outputs, modern text-conditional image generators such as DALL-E 2 (Ramesh et al., 2022) and Imagen (Saharia et al., 2022b) are able to produce high-resolution outputs for any conceivable textual prompt. While this trend cannot be attributed exclusively to the advent of diffusion models (models with similar capabilities that are not based on diffusion do exist, e.g. Parti (Yu et al., 2022)), this new paradigm for generative modelling through iterative refinement has indisputably played a key role in the 'mainstreaming' of generative models of images. Diffusion-based language models have seen relatively little success so far. This is in part due to the discrete categorical nature of textual representations of language, which standard diffusion models are illequipped to deal with. As a result, several diffusion-\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_11c9bceac3b6de560e63g-01.jpg?height=710&width=784&top_left_y=1227&top_left_x=1067)\n\nFigure 1 | Overview of the CDCD framework.",
    "contdiffu-3": "Components with learnable parameters are marked with $\\nabla$. Novel components are bolded. The denoising model is a Transformer (without attention masking) which predicts tokens from noisy embeddings (\u00a73.2) and is trained with the cross-entropy loss (\u00a73.1). The input noise is time-dependent, with timesteps $t$ sampled non-uniformly during training via time warping (\u00a73.3). With the predicted logits, score function estimates can be obtained through interpolation, which are used for sampling with an ODE solver. See Figure 3 for a more detailed diagram. inspired approaches to language modelling have recently been proposed (Austin et al., 2021; Hoogeboom et al., 2021a,b; Reid et al., 2022; Savinov et al., 2021), but these depart from the diffusion modelling framework used for perceptual data in several important ways (with a few exceptions, e.g. Li et al.",
    "contdiffu-4": "(2022); Strudel et al. (2022)). This usually implies having to give up some of the unique capabilities of this model class, such as the ability to use classifier-free guidance to enhance conditional generation (Ho and Salimans, 2022), which has been instrumental to the success of diffusion-based text-conditional image generators. In this paper, we study the suitability of continuous diffusion as a generative modelling paradigm for discrete categorical data, and for textual representations of language in particular. We develop a framework, Continuous diffusion for categorical data (CDCD), based on the diffusion framework proposed by Karras et al. (2022), which enables efficient and straightforward training of diffusion-based language models that are continuous in both time and input space, by embedding discrete tokens in Euclidean space. Our approach very closely mirrors the training procedure for masked language models such as BERT (Devlin et al., 2018, which are non-autoregressive and non-generative ${ }^{1}$ ), and hence should appear familiar to language modelling practitioners. We hope that this will help lower the barrier to entry, and encourage researchers to explore continuous diffusion models for other domains for which categorical representations are best suited. Our contributions are as follows:\n\n- We propose score interpolation as an alternative to score matching for diffusion model training. This allows us to use the familiar cross-entropy loss function for training, which in turn enables end-to-end training of the diffusion model and the Euclidean embeddings with a single loss function;\n- We introduce time warping, an active learning strategy which automatically adapts the distribution of noise levels sampled during training to maximise efficiency;\n- We describe CDCD, a framework for continuous diffusion models of categorical data (see Figure 1), and explore its application to language modelling and machine translation. [^0]\n## 2. Diffusion models\n\nDiffusion models enable generative modelling via iterative denoising. Given a gradual corruption process which turns the data distribution into a simple distribution that is easy to sample from (usually an isotropic Gaussian distribution), we can train a model that learns to revert this process step by step. Each step in the reverse direction attempts to reconstruct a small amount of information that the corruption process removed. This is a much easier task than learning to generate data in a single forward pass through a model, as variational autoencoders (VAEs, Kingma and Welling, 2013; Rezende et al., 2014) and generative adversarial networks (GANs, Goodfellow et al., 2014) do. Autoregressive models similarly enable decomposition of the generative modelling problem into smaller subproblems that are easier to solve. Both of these approaches to iterative refinement are compared in \u00a72.3. ### 2.1. Formalism\n\nMany different formalisms have been proposed for diffusion models, e.g. based on score matching (Song and Ermon, 2019) or latent variable models (Ho et al., 2020). In this work, we will follow Song et al. (2020) and use differential equations to describe the corruption process, as well as the reverse process. We believe that all these different perspectives are largely interchangeable and complementary to some degree. Song et al. (2020) suggest modelling a diffusion process with the following stochastic differential equation $(\\mathrm{SDE}):$\n\n$$\n\\mathrm{d} \\mathbf{x}=\\mathbf{f}(\\mathbf{x}, t) \\mathrm{d} t+g(t) \\mathrm{d} \\mathbf{w}\n$$\n\nwhere $\\mathbf{w}$ is the standard Wiener process, $\\mathbf{f}$ is the (vector-valued) drift coefficient, $g$ is the diffusion coefficient and time $t$ ranges from 0 (clean data) to $T$ (fully corrupted). The reverse process can then be described by the following SDE:\n\n$$\n\\mathrm{d} \\mathbf{x}=\\left(\\mathbf{f}(\\mathbf{x}, t)-g(t)^{2} \\nabla_{\\mathbf{x}} \\log p_{t}(\\mathbf{x})\\right) \\mathrm{d} t+g(t) \\mathrm{d} \\overline{\\mathbf{w}}\n$$\n\nwhere $\\overline{\\mathbf{w}}$ is the standard Wiener process in reversed time. $\\mathbf{s}(\\mathbf{x}, t):=\\nabla_{\\mathbf{x}} \\log p_{t}(\\mathbf{x})$ is the so-called score function, i.e. the gradient of the density of $\\mathbf{x}$ at time $t$. We can train a model to predict this quantity given $\\mathbf{x}$ and $t$ using score matching (Hyv\u00e4rinen and Dayan, 2005):\n\n$$\n\\min \\left(\\hat{\\mathbf{s}}(\\mathbf{x}, t)-\\nabla_{\\mathbf{x}} \\log p_{t}(\\mathbf{x})\\right)^{2}\n$$\n\nThe estimate $\\hat{\\mathbf{s}}(\\mathbf{x}, t)$ can then be plugged into this SDE to produce samples ${ }^{2}$. It turns out that we can instead describe the evolution of $\\mathbf{x}$ over time deterministically with an ordinary differential equation (ODE):\n\n$$\n\\mathrm{d} \\mathbf{x}=\\left(\\mathbf{f}(\\mathbf{x}, t)-\\frac{1}{2} g(t)^{2} \\nabla_{\\mathbf{x}} \\log p_{t}(\\mathbf{x})\\right) \\mathrm{d} t\n$$\n\nThis is the probability flow ODE, which has the same marginals $p_{t}(\\mathbf{x})$ as the forward SDE at all timesteps $t$. This equivalence is quite powerful, because it enables us to deterministically map data examples $\\mathbf{x}_{0}$ to latent representations $\\mathbf{x}_{T}$, and vice versa (with $\\mathbf{x}_{T}$ approximately following a Gaussian distribution). Karras et al. (2022) thoroughly explored the design space of diffusion models based on the probability flow ODE formulation, and we will largely follow their recommendations here. Concretely, we will choose $\\mathbf{f}(\\mathbf{x}, t)=0$ and $g(t)=\\sqrt{2 t}$, which yields:\n\n$$\n\\mathrm{d} \\mathbf{x}=-t \\nabla_{\\mathbf{x}} \\log p_{t}(\\mathbf{x}) \\mathrm{d} t\n$$\n\nIn this formulation, $t$ corresponds directly to the standard deviation of the Gaussian noise that is added to $\\mathbf{x}_{0}$ to simulate samples from $p_{t}(\\mathbf{x})$ (and therefore they refer to $t$ as $\\sigma$ instead $^{3}$ ). ### 2.2. Diffusion for discrete data\n\nWhen $\\mathbf{x}$ is discrete, the score function is undefined. This can be worked around in two ways: we can try to define a similar iterative refinement procedure through denoising for discrete data, or we can embed $\\mathbf{x}$ into a continuous space and apply continuous diffusion to the embeddings. While most of the literature has focused on the former approach (see $\\S 5.1$ for an overview), in this work we will explore the latter abandoning continuity of the input usually means that we have to forgo a lot of useful capabilities, such as classifier-free guidance, which we would like to keep. Another potential advantage of lifting the discrete input into a continuous space is that it becomes possible to represent superpositions of possible outcomes at intermediate timesteps of the sampling process. In language modelling, this means that we can represent uncertainty at the individual token level: the sampling procedure only commits to specific tokens at the very end. Denoising models that operate directly in the discrete input space do not have this ability: they are only able to represent specific tokens, or the absence\n\n[^1]of a decision (through use of a 'mask' token). This requirement to commit early to some subset of tokens can lead to inconsistencies in the resulting samples, which are difficult to correct retroactively ${ }^{4}$. Changing the input representation to be continuous enables us to use the standard diffusion framework that has been exceptionally successful for perceptual modalities, but we should not necessarily expect it to work as well for language modelling out of the box. In fact, the most commonly used modelling setup for images implicitly reduces the loss weighting of high frequency content relative to likelihood-based models, allowing for a more efficient use of model capacity which is well aligned with human perception (Song et al., 2021a). Furthermore, the underlying physical phenomena that are being modelled (e.g. light intensity, air pressure) are inherently continuous. The relative ease with which diffusion models of images have been scaled to high resolution inputs can at least partially be attributed to these facts. We cannot expect to benefit from this for language modelling, as the notion of 'high frequency content' is not meaningful in this setting ${ }^{5}$. Finally, we also need to consider the impact of the choice of embedding procedure on generative modelling performance. ### 2.3. Diffusion and autoregression\n\nAutoregressive (AR) models currently dominate language modelling research at scale. They factorise the joint distribution over a token sequence $p\\left(x_{1}, x_{2}, \\ldots, x_{N}\\right)$ into sequential conditionals $p\\left(x_{k} \\mid x_{1}, \\ldots, x_{k-1}\\right)$ and model each of them separately (with shared parameters). This means sampling always proceeds along the direction of the sequence (i.e. from left to right, when modelling text in English), and in this case, sampling an additional token constitutes an 'iterative refinement' step. Autoregression is a very natural fit for language, because it is best represented as a one-dimensional sequence of tokens.",
    "contdiffu-5": "That said, the way humans tend to produce language, especially in written form, is far from linear. For many tasks, the ability to go back and refine earlier parts of the sequence, or to construct it hierarchically, is useful. Changing the modelling paradigm to a more flexible form of iterative refinement (such as diffusion) is desirable, because the increased flexibility would facilitate new applications and could potentially re-\n\n[^2]duce the computational cost of sampling. However, this is a challenging prospect because of the statistical efficiency of AR model training. Because the same parameters can be used to model all sequential conditionals, each training example provides a useful gradient signal for every step of the iterative refinement procedure. This is not the case for diffusion models, where we can only train on a single noise level for each training example. As a result, diffusion models are likely to be less data-efficient, and will converge more slowly. AR models are also able to benefit from caching of previous model activations at sampling time, which significantly reduces the computational cost of each step. Diffusion models require a full forward pass across the entire sequence at each step, which can be much more costly. Nonetheless, this apparent efficiency benefit of AR models does impose a rather strict constraint on the connectivity patterns within these models - specifically, causality with respect to the input sequence. This constraint is usually implemented using some form of masking, which implies that a significant amount of computation is wasted during training. It also complicates the use of multiresolution architectures, which is very common in other domains of machine learning (e.g. computer vision). Diffusion models on the other hand are completely architecturally unrestricted, so the use of multiresolution architectures (or more exotic variants) is straightforward. This architectural flexibility compounds with the adaptivity of the denoising procedure, which enables trading off the computational cost and sample quality at sampling time by choosing the appropriate number of iterative refinement steps, without requiring retraining or finetuning. Conversely, for AR models, the number of steps is necessarily the same as the length of the sequence to be generated ${ }^{6}$. More sophisticated sampling procedures for diffusion models are also being developed on a regular basis, which can be applied to existing models without any changes. Therefore, we believe that diffusion models for language are a worthwhile pursuit, despite their relatively reduced data efficiency. ## 3. The CDCD framework\n\nWe will first describe how we can train diffusion models using the familiar categorical cross-entropy loss with score interpolation. We then show how to map\n\n[^3]categorical inputs to continuous embeddings in a way that is amenable to diffusion, which we achieve by jointly learning the embeddings and the diffusion model itself, allowing them to co-adapt. Finally, we will discuss time warping, an active learning strategy which automatically adapts the distribution of noise levels sampled during training. Together, these components constitute a framework for continuous diffusion of categorical data, or CDCD, which is summarised in a diagram in Figure 1. ### 3.1. Score interpolation\n\nDiffusion models are typically trained by minimising the score matching objective (Equation 3), where the model learns to approximate the score function $\\mathbf{s}(\\mathbf{x}, t)$ in the least-squares sense. The model predictions can then be substituted directly into Equations 2 or 4 for sampling. We observe that when the data is discrete and categorical, with tokens taken from a vocabulary of size $V$, the conditional score function $\\mathbf{s}\\left(\\mathbf{x}, t \\mid \\mathbf{x}_{0}\\right)$ can only assume $V$ possible values. Therefore, if we have a probabilistic prediction of $\\mathbf{x}_{0}$, we can use it to linearly interpolate the $V$ possible values to obtain a score function estimate:\n\n$$\n\\hat{\\mathbf{s}}(\\mathbf{x}, t)=\\sum_{i=1}^{V} p\\left(\\mathbf{x}_{0}=\\mathbf{e}_{i} \\mid \\mathbf{x}, t\\right) \\mathbf{s}\\left(\\mathbf{x}, t \\mid \\mathbf{x}_{0}=\\mathbf{e}_{i}\\right)\n$$\n\nwhere we have used $\\mathbf{e}_{i}$ to represent the embedding corresponding to token $i$ in the vocabulary. Note that this is also equivalent to the expectation $\\mathbb{E}_{p\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}, t\\right)}\\left[\\mathbf{s}\\left(\\mathbf{x}, t \\mid \\mathbf{x}_{0}\\right)\\right]$, which helps explain why this approach works: this expectation is also the minimiser of the score matching loss, so the global optimum is the same for score matching and score interpolation. To obtain an estimate of $p\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}, t\\right)$, we can make our model predict $V$ logits and apply a softmax nonlinearity, and minimise the categorical cross-entropy loss. This is also the standard setup used to train autoregressive language models (as well as classifiers in general), so it is well-studied and understood, and it ensures stability during training. Compared to the score matching loss, the cross-entropy loss will of course weight errors in the score function estimates differently relative to each other. This difference is important in practice, because we are only able to optimise the loss approximately (i.e. the global optimum is unlikely to be reached), and the relative weighting of the noise levels will also be different. The conditional score function corresponding to the ODE in Equation 5 is given by:\n\n$$\n\\mathbf{s}\\left(\\mathbf{x}, t \\mid \\mathbf{x}_{0}\\right)=\\frac{\\mathbf{x}_{0}-\\mathbf{x}}{t^{2}}\n$$\n\nwhich is affine in $\\mathbf{x}_{0}$ (Karras et al., 2022). Therefore, the expectation $\\mathbb{E}_{p\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}, t\\right)}\\left[\\mathbf{s}\\left(\\mathbf{x}, t \\mid \\mathbf{x}_{0}\\right)\\right]$ can be written as an affine function of the expectation of $\\mathbf{x}_{0}$ itself:\n\n$$\n\\hat{\\mathbf{s}}(\\mathbf{x}, t)=\\frac{\\mathbb{E}_{p\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}, t\\right)}\\left[\\mathbf{x}_{0}\\right]-\\mathbf{x}}{t^{2}}\n$$\n\nIn other words, we can first obtain an estimate of the ground truth embedding vector $\\hat{\\mathbf{x}}_{0}:=\\mathbb{E}_{p\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}, t\\right)}\\left[\\mathbf{x}_{0}\\right]$, and then use this to obtain a score estimate. This perspective facilitates further modifications to the score estimate, which we will discuss in $\\S 3.2$. ### 3.2. Diffusion on embeddings\n\nTo embed the input in a continuous space, we could arbitrarily assign embeddings to different tokens, or use a representation learning technique to obtain embeddings (Strudel et al., 2022). However, since we are able to backpropagate gradients from the diffusion model into the embeddings using the reparameterisation trick (Kingma and Welling, 2013; Rezende et al., 2014), we explore learning the embeddings and the diffusion model jointly. This yields a simpler setup, with a single shared loss function for all model parameters. If we were to train our diffusion model with score matching, joint training would result in collapse of the embedding space. Since the model is effectively predicting the noise that is added to the embeddings, this task becomes trivial when all embeddings correspond to the same vector. This minimises the loss function, but it does not yield a useful model. Therefore, additional loss terms are necessary to prevent collapse ( Li et al., 2022). Using score interpolation, we can train the diffusion model with the cross-entropy loss instead. Since the objective is now to distinguish the true embedding from all other embeddings, given a noisy embedding as input, the model is encouraged to push the embeddings as far apart as possible (as this minimises the confounding impact of the noise). We now have the opposite problem, where joint training leads to uncontrollable growth of the embedding parameters, unless they are constrained in some way. We could again implement such a constraint using additional loss terms, but a simpler alternative is to explicitly normalise the embedding vectors. We find that this approach is very effective, and it yields a model that is trainable end-to-end with the cross-entropy loss, without requiring any additional terms. Concretely, we always L2-normalise the embedding vectors before they are used, but we allow the underlying parameters to vary freely. We backpropagate through the normalisation operation as needed. We can also apply L2-normalisation to the embedding estimate $\\hat{\\mathbf{x}}_{0}$ (see \u00a73.1) before calculating the score estimate, which we refer to as renormalisation. This means that the score estimate no longer corresponds directly to the expectation $\\mathbb{E}_{p\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}, t\\right)}\\left[\\mathbf{s}\\left(\\mathbf{x}, t \\mid \\mathbf{x}_{0}\\right)\\right]$, but rather a rescaled version of it $^{7}$. Another possible manipulation is to clamp $\\hat{\\mathbf{x}}_{0}$ to the nearest embedding vector in the vocabulary (in the Euclidean sense). In practice, we found both of these manipulations to work less well (see \u00a76.3) - unlike Li et al. (2022), who found that clamping improved their results. ### 3.3. Time warping\n\nDiffusion models are essentially denoisers that can operate at many different noise levels with a single set of shared parameters. Therefore, the degree to which the model dedicates capacity to different noise levels has a significant impact on the perceived quality of the resulting samples. We can control this by appropriately weighting the noise levels during training. The impact of different weightings has been studied extensively for diffusion models of images (Karras et al., 2022; Kingma et al., 2021; Nichol and Dhariwal, 2021; Song et al., 2021a). We note that our use of the cross-entropy loss, while conveying many benefits (such as stability and end-toend training), also changes the relative weighting of the noise levels corresponding to different timesteps $t$. Because the effects of this change are difficult to quantify, we seek to determine a time reweighting strategy that maximises sample quality. In practice, this is best implemented through importance sampling: rather than explicitly multiplying loss contributions from different noise levels with a weighting function $\\lambda(t)$, we will instead sample $t$ from a non-uniform distribution, whose density directly corresponds to the desired relative weighting. This way, we avoid introducing significant variance in the loss estimates for timesteps $t$ for which $\\lambda(t)$ is particularly large. To sample $t$ non-uniformly in practice, we can use inverse transform sampling: we first sample uniform samples $u \\in[0,1]$ and then warp them using the inverse cumulative distribution function (CDF) of the distribution which corresponds to the desired weighting: $t=F^{-1}(u)$. This time warping procedure is equivalent to time reweighting in expectation, but more statistically efficient. To estimate the CDF $F(t)$ in question, we propose\n\n[^4]to use the following heuristic:\n\nThe entropy of the model predictions should increase linearly as a function of $F(t)$. This heuristic has an intuitive interpretation: it implies that the uncertainty of the model predictions (measured in bits or nats) increases at a constant rate as a function of 'uniform time' $u$. Therefore, the capacity of the model should be evenly distributed across the information content of the input sequences. Furthermore, when sampling from the model, using equally spaced timesteps in uniform time will result in a gradual decrease of uncertainty with an approximately constant rate. This ensures that all sampling steps do an equal amount of work in terms of resolving uncertainty. In practice, we can use the cross-entropy loss values already computed during training as a stand-in for the prediction entropy ${ }^{8}$. This means we can fit an unnormalised monotonic function $\\tilde{F}(t)$ to the observed cross-entropy loss values $\\mathcal{L}(t)$ (using the mean squared error loss), which we then need to normalise and invert to implement time warping:\n\n$$\n\\min (\\tilde{F}(t)-\\mathcal{L}(t))^{2}\n$$\n\nThis yields an active learning strategy where noise levels are initially sampled uniformly, but as training progresses, the noise level distribution changes to focus attention towards those levels for which training is the most useful.",
    "contdiffu-6": "Inspired by Durkan et al. (2019); M\u00fcller et al. (2019) we parameterise $\\tilde{F}(t)$ as a monotonic piecewise linear function, which is very straightforward to normalise and invert. We describe the fitting procedure in detail in Appendix A. Figure 2 shows the cross-entropy loss as a function of $t$ for a fully trained model, along with the monotonic piecewise linear approximation $\\tilde{F}(t)$ (top). It also shows the implied relative weighting of timesteps (middle) and the cross-entropy loss as a function of uniform time $u$ (bottom), which is approximately linear as a result of time warping. Typically, we find that time warping puts most of the weight on intermediate noise levels. At very low noise levels, the denoising classification problem becomes trivial, because the embeddings corresponding to each token are easy to identify. At very high noise levels, the optimal strategy is to predict the marginal\n\n[^5]distribution of tokens (given the conditioning), which is also relatively straightforward to learn. ## 4. Diffusion language models\n\nUsing the CDCD framework described in the previous section, we can now construct language models for different tasks. We will describe a general Transformer (Vaswani et al., 2017) architecture which can be used for prompt completion and infilling, using a boolean conditioning mask to indicate which tokens in the sequence are to be sampled ('noisy'), and which tokens are given as conditioning ('clean'). The full model setup is visualised in a diagram in Figure 3. We will also describe an encoder-decoder model architecture for machine translation. ### 4.1. Mask-conditional Transformer\n\nSince CDCD enables us to reduce the language modelling problem to a denoising classification task, without imposing any restrictions on the model architecture (such as causality, see \u00a72.3), we are able to use the Transformer architecture without any form of attention masking. However, practical applications of language modelling require the ability to fix some subset of the tokens in a sequence, while generating the rest, conditioned on these given tokens. One way to achieve this would be to 'clamp' certain token positions throughout the sampling procedure, by reinjecting these tokens at each step, corrupted by the appropriate level of noise. This method was proposed by Song et al. (2020) and referred to as 'the replacement method' by Ho et al. (2022b). It seems attractive, because it would allow us to treat the model as fully unconditional during training. Nonetheless, it is not as effective as training the model to specifically support conditional sampling out of the box. Indeed, diffusion models for image inpainting are also more effective when trained specifically for that task (Saharia et al., 2022a). Therefore, we construct the input to the model by stacking three sequences:\n\n- $\\mathbf{x}$ : the embeddings corresponding to the noisy input sequence, with embeddings for conditioning tokens set to the zero vector;\n- $\\mathbf{c}$ : the embeddings corresponding to the conditioning tokens, with embeddings for the tokens to be sampled set to the zero vector;\n- $m$ : the boolean conditioning mask, indicating which tokens are given ('clean', $m_{i}=0$ ) and which are to be generated ('noisy', $m_{i}=1$ ). ![](https://cdn.mathpix.com/cropped/2024_09_12_11c9bceac3b6de560e63g-07.jpg?height=854&width=1478&top_left_y=291&top_left_x=291)\n\nFigure 2 | Time warping. Top: cross-entropy loss $\\mathcal{L}(t)$ and learnt unnormalised $\\operatorname{CDF} \\tilde{F}(t)$ for a fully trained model with $t_{\\min }=1$ and $t_{\\max }=100$. Dotted vertical lines indicate bin edges for the piecewise linear approximation (see Appendix A). Middle: implied relative weighting of timesteps $\\lambda(t)$ (time derivative of $F(t)$ ). Bottom: cross-entropy loss as a function of uniform time $u=F^{-1}(t)$. As discussed before in \u00a73.1, the output of the model consists of a sequence of logit vectors, which correspond to the predicted probabilities of each token in the vocabulary occurring at each sequence position. When calculating the training loss, we zero out the positions for given tokens. Prefix masking Autoregressive language models naturally allow prefix conditioning, where the start of a sequence is given and the model generates a completion. This is not as straightforward with diffusion models. Since prefix conditioning is a very general procedure for interacting with language models, we would like our models to support it. To achieve this, we randomly sample mask sequences $m$ during training which correspond to prefix conditioning. Such masks consist of a sequence of zeros of a certain length, followed by a sequence of ones, where the length of the prefix is sampled uniformly at random. Fully random masking Since diffusion models are able to iteratively refine all tokens in a sequence in parallel, we are not restricted to prefix conditioning. To enable conditioning on an arbitrary subset of sequence positions, we can sample masks $m$ fully randomly during training. Rather than sampling a mask value independently for each sequence position, we first sample a clean position count uniformly at random, and then randomly select a subset of that size from the sequence. This ensures that the model is able to support conditioning on any number of tokens. Mixed masking While fully random masking yields the most general model, supporting conditioning on any arbitrary subset of tokens in a sequence, prefix masking is sufficient to support the most common use cases for language models. Somewhat surprisingly, we find that training on an equal mixture of prefix masks and fully random masks actually slightly improves prefix completion performance (see \u00a76.3).",
    "contdiffu-7": "### 4.2. Noise level conditioning\n\nDiffusion models operate on inputs corrupted with varying levels of noise. These levels correspond directly to timesteps in the diffusion process. We provide the timestep as an additional input, which is incorporated into the model using conditional normalisation: each layer normalisation operation in the model is followed by shifting and scaling the activations, with the shift and scale parameters depending on the timestep (Perez et al., 2018). ![](https://cdn.mathpix.com/cropped/2024_09_12_11c9bceac3b6de560e63g-08.jpg?height=2046&width=960&top_left_y=279&top_left_x=548)\n\nFigure 3 | Transformer language model using the CDCD framework. A conditioning mask $m$ indicates which sequence positions are given ('clean') and which are to be generated ('noisy'). During training, Gaussian noise is added to the embeddings for noisy positions. The Transformer input consists of a timestep $(t)$ embedding, along with the concatenation of the mask $m$, token embeddings $\\mathbf{c}$ for clean positions, and noisy embeddings $\\mathbf{x}$ and previous predictions $\\mathbf{p}$ for noisy positions. The output logits are used to interpolate the vocabulary embeddings (to produce new predictions) and the corresponding score functions (to produce score estimates). The score estimates are used to partially denoise the noisy embeddings $\\mathbf{x}$. ### 4.3. Self-conditioning\n\nChen et al. (2022) introduced self-conditioning, which significantly improves the performance of diffusion models in certain contexts. They noted that the predictions produced by diffusion models are only used to determine the direction in which to update the noisy input, and are then discarded, which is wasteful. Giving the model direct access to the predictions it produced at the previous sampling step enables a more efficient use of model capacity, by allowing it to refine previous predictions, rather than constructing them from scratch at each step. This approach bears a strong resemblance to the unrolled denoising strategy proposed by Savinov et al. (2021). To enable the model to make use of this additional input without requiring unrolling across multiple sampling steps during training, they propose a training procedure which only requires an additional forward pass on half of the batch at each training step. Following Strudel et al. (2022), we use this procedure and find that it only increases training time by 10-15\\% for our models in practice, while yielding significant performance gains. To use self-conditioning with CDCD, the input to the model now consists of four stacked sequences. In addition to $\\mathbf{x}, \\mathbf{c}$ and $m$ (see \u00a74.1), we add $\\mathbf{p}$, which is a sequence of embeddings found by interpolating the vocabulary embeddings using the token probabilities predicted in the previous sampling step (see \u00a73.1; embeddings for conditioning tokens are set to the zero vector). ### 4.4. Machine translation model\n\nFor machine translation, we use an encoder-decoder architecture with two separate Transformer stacks. Since the conditioning (source) sequence and target sequence are separate, there is no need for masking. We adopt an architecture very similar to the original Transformer (Vaswani et al., 2017), with absolute positional embeddings (sinusoidal on the source side, learned on the target side) instead of RoPE, and ReLU as activation function. Generating samples of the correct length is an important concern for translation. As we do not use any causal mask on the decoder side, we cannot simply disregard the contribution to the loss of the positions corresponding to padding tokens during training. Instead, we simply predict the whole sequence of tokens, including beginning-of-sentence (BOS), end-ofsentence (EOS), and padding tokens, up to a constant length. At sampling time, tokens past the first EOS are discarded. In order to provide a strong conditioning signal to the decoder, we extend the conditioning described in $\\S 4.2$ to additionally use the length of the source sequence. ### 4.5. Comparison to BERT\n\nThe model architecture and training procedure we have described so far is very similar to BERT (Devlin et al., 2018). Given the widespread use of that model, and its popularity among language modelling practitioners, we provide a side-by-side comparison. Both models are sequence denoisers, but the nature of the noise differs. While BERT is trained on sequences corrupted by masking noise, which randomly removes a subset of tokens altogether, inputs to our model are corrupted by Gaussian noise which is added directly to the token embeddings. The input consists of a stack of multiple sequences (see $\\S 4.1$ and \u00a74.3), rather than a single sequence where some of the tokens have been masked. Because the intensity of the noise varies according to the timesteps of the diffusion process, we also provide the timestep as an additional input, which is not required for BERT. Finally, to avoid uncontrollable growth of the embedding parameters, we force the embeddings to be normalised. Other than that, the model architectures are essentially identical during training, and the loss functions are the same. ## 5. Related work\n\n### 5.1. Discrete diffusion\n\nSeveral diffusion-based and diffusion-inspired approaches have been proposed for non-autoregressive iterative refinement of discrete data, and especially for language in particular (Austin et al., 2021; Chang et al., 2022; Ghazvininejad et al., 2019; Hoogeboom et al., 2021a,b; Reid et al., 2022; Savinov et al., 2021). Replacing continuous diffusion with a discrete corruption process affords some flexibility, but it also requires forgoing several capabilities associated with the continuous paradigm, such as efficient sampling algorithms based on advanced ODE solvers, or classifier-free guidance. More recently, several papers have proposed approaches to apply the continuous diffusion framework to discrete data. It is important to distinguish continuity in the input space from continuity of the time variable of the corruption process; for CDCD, both are continuous.",
    "contdiffu-8": "Li et al. (2022), Strudel et al. (2022) and Han et al. (2022) all target language modelling as the primary application and use an embedding-based strategy in combination with discrete-time diffusion. Campbell et al. (2022) and Sun et al. (2022) propose\ncontinuous-time models for discrete input, though they do not explore the application to language modelling. Meng et al. (2022) propose concrete score matching, which can be applied to both discrete and continuous inputs. Chen et al. (2022) use continuoustime diffusion applied to continuous relaxations of binary representations of the input. Chen et al. (2022); Li et al. (2022); Strudel et al. (2022) also suggested using the cross-entropy loss (in combination with other loss terms). ### 5.2. Iterative refinement for machine translation\n\nThere have been considerable efforts to apply nonautoregressive iterative refinement models to the task of machine translation. The first attempts by Gu et al. (2017) already uncovered the issue of 'multi-modality': since non-AR models usually predict all tokens in parallel and independently of each other, uncoordinated sampling decisions might lead to incoherencies like repeated tokens. Earlier advances in sequence-level distillation (Kim and Rush, 2016) allowed to alleviate those issues, albeit at the cost of expensive training dataset creation. Another line of work introduced a latent transformer (Kaiser et al., 2018): discrete latents are first sampled autoregressively, and then decoded non-autoregressively. LVM-DAE (Lee et al., 2018) applied a nonautoregressive decoder multiple times to alleviate multi-modality. Insertion (Stern et al., 2019) and Levenshtein (Gu et al., 2019) transformers demonstrated good parallel decoding results on machine translation along with editing capabilities. The LVM-DAE line of work was later significantly improved upon by the CMLM (Ghazvininejad et al., 2019) and DisCo (Kasai et al., 2020a) methods via novel training and decoding procedures. A later work by Kong et al. (2020) combined CMLM with local autoregression. Promising advances were achieved by another follow-up of CMLM called SMART (Ghazvininejad et al., 2020), further closing the gap between AR and non-AR methods. More recently, Imputer (Chan et al., 2020; Saharia et al., 2020) achieved good results by optimizing alignment between source and target. Kasai et al. (2020b) questioned the speed advantage of non-AR models in machine translation by comparing them to a shallow-decoder AR baseline. SUNDAE (Savinov et al., 2021) introduced step-unrolls and obtained excellent results both in machine translation and unconditional generation without relying on sequence-level distillation. Huang et al. (2022) also eliminated distillation with an unroll-like technique. Aggressive decoding (Xia et al., 2022) generated tokens in parallel while using AR models to verify generation and re-generate after the first devi- ation. DiffusER (Reid et al., 2022) used edit-based reconstructions and 2D beam search to almost completely close the gap between AR and non-AR models in machine translation.",
    "contdiffu-9": "### 5.3. Other related work\n\nKingma et al. (2021) suggest parameterising and optimising the noise schedule during training in a similar fashion to time warping (\u00a73.3), though the objective is different: the goal is to minimise the variance of the diffusion loss, whereas our goal is to linearise the entropy of the model predictions. ## 6. Experiments\n\nWe study the effect of various design choices, and compare language models based on CDCD with the standard autoregressive approach. We train maskconditional models for tasks such as prompt completion and infilling, and encoder-decoder models for machine translation. ### 6.1. Architecture and hyperparameters\n\nFor the mask-conditional models, we use a SentencePiece tokenizer (Kudo and Richardson, 2018) with a vocabulary size of 32000 . We use a standard pre-LN Transformer architecture (Xiong et al., 2020) with 8 blocks, 1024 units and rotary positional encodings (Su et al., 2021, RoPE), trained for 1 million steps with batch size 1024 and sequence length 64 . We use Random Fourier projections and an MLP with two layers of 128 units to produce timestep embeddings (see Figure 3). We also use self-conditioning (Chen et al., 2022) and time warping. We use 100 bins for the piecewise linear unnormalised $\\operatorname{CDF} \\tilde{F}(t)$ (see $\\S 3.3$ and Appendix A). We set the embedding dimensionality $d=256$. We L2-normalise the embeddings and scale them by $\\sqrt{d}$, so that each component has a standard deviation of 1 . We choose $t_{\\min }=1.0$ and $t_{\\max }=300.0$. While these values are quite different from the ones suggested by Karras et al. (2022) for image diffusion, we note that the discrete underlying nature of the input data makes it possible to predict the original tokens with $100 \\%$ accuracy even when noise with $\\sigma=1.0$ is added (recall that $\\sigma=t$, see \u00a72.1). Therefore, it is not useful to consider lower noise levels, except when the embedding dimensionality $d$ is reduced. We scale the noisy embeddings by $\\frac{1}{\\sqrt{\\sigma^{2}+\\sigma_{\\text {data }}^{2}}}=\\frac{1}{\\sqrt{t^{2}+1}}$ before passing them into the model, so that the components again have a standard deviation of 1 . We drop out the conditioning by zeroing out the corresponding embeddings for 10\\% of training examples, in order to be able to support sampling with classifier-free guidance (Ho and Salimans, 2022). We use low-discrepancy sampling (Kingma et al., 2021) to sample uniform timesteps $u$ before applying time warping, to reduce the variance of the loss estimates. We use a mixed masking strategy (see \u00a74.1): $50 \\%$ of the masks sampled during training are prefix masks (with the length of the prefix uniformly sampled), and the other $50 \\%$ are fully random masks (with the number of clean token positions again uniformly sampled). We use the Adam optimiser (Kingma and Ba, 2015) with a learning rate of $10^{-4}, \\beta_{1}=0.9$ and $\\beta_{2}=0.99$. We use 200 Euler steps for sampling, and do not tune any sampling parameters (such as temperatures or guidance scales). While this is a large number of steps relative to the sequence length, we wanted to ensure that our measurements would not be negatively affected by discretisation errors introduced by the ODE solver. ### 6.2. Evaluation\n\nWe train mask-conditional models on the MassiveText dataset (Rae et al., 2021), except for the larger model used in $\\S 6.5$, which is trained on the publicly available C4 dataset (Raffel et al., 2019). Both datasets contain web-crawled content, and are very diverse as a result. For machine translation, we train models on the WMT2014 German-English / English-German and WMT2020 Chinese-English datasets. To evaluate our machine translation models, we follow the literature and use the BLEU score (Papineni et al., 2002). For mask-conditional models, quantitative evaluation is more challenging. Following Savinov et al. (2021); Strudel et al. (2022), we measure the likelihood of generated samples under a 1.3B parameter autoregressive language model (AR-NLL), as well as the unigram (per-token) entropy of the samples. As long as the entropy remains high enough, we found this negative log-likelihood to be strongly correlated with subjectively assessed model quality, and we made extensive use of this metric for hyperparameter exploration. To ensure a fair comparison, we always calculate these metrics using a fixed prefix mask with a prefix length of half the sequence length, regardless of the masking strategy used during training. For experiments with a larger mask-conditional model, we also report the MAUVE metric (Pillutla et al., 2021), which is specifically designed for openended text generation and has been shown to correlate with human judgement (see \u00a76.5). ### 6.3. Design decisions\n\nWe conduct various ablation experiments to justify our architecture and hyperparameter choices, reporting both the autoregressive negative log likelihood (ARNLL ) and the unigram entropy at the token level (H) in each case. The results are summarised in Table 1 and visualised in Figure 4. Renormalisation and clamping Both manipulations of the score estimate (see \u00a73.2) significantly reduce the AR-NLL, but also reduce the entropy quite a lot. Anecdotally, we also find that models using renormalisation are less amenable to improvements from sampling hyperparameter tuning, and models without renormalisation produce better samples when tuned. Because we also make use of self-conditioning (see below), it is important to treat renormalisation and clamping as training-time hyperparameters, because they will affect the previous predictions $\\mathbf{p}$ that the model receives as input (see \u00a74.3). When not using self-conditioning, renormalisation and clamping could instead be treated as sampling hyperparameters (see \u00a76.4). Embedding dimensionality We reduced $t_{\\text {min }}$ from 1.0 to 0.1 for this experiment, because at lower embedding dimensionalities, the same amount of noise erases more information. We verified that the crossentropy is nearly zero at $t=0.1$ for all values of $w$ we consider ${ }^{9}$. AR-NLLs initially improve with increasing dimensionality, but stabilise beyond $w=64$, thanks to time warping automatically shifting focus to the relevant noise levels (see Figure 2). We used $w=256$ and $t_{\\min }=1.0$ for all other experiments, including the base model, but the result obtained with $t_{\\min }=0.1$ is very similar. Embedding initialisation scale Results degrade when the scale of the initial embedding parameters is too high. We use $\\sigma=0.001$ throughout to avoid this pitfall. Given its impact on performance, tuning this parameter is important, which is worth noting because weight initialisation scales are usually chosen heuristically, and generally are not treated as important hyperparameters to tune. We normalise the embeddings whenever they are used (see \u00a73.2), so the scale of the underlying parameters is not relevant for inference, but clearly it significantly affects optimisation. [^6]| Ablation |  | AR-NLL | Entropy |\n| :---: | :---: | :---: | :---: |\n| Base model |  | $4.392_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |\n| Data |  | 3.389 | 7.179 |\n| Prediction renormalisation and clamping | neither <br> renormalisation only <br> clamping only <br> both | $4.392_{ \\pm 0.004}$ <br> $3.762_{ \\pm 0.003}$ <br> $3.881_{ \\pm 0.009}$ <br> $3.895_{ \\pm 0.005}$ | $7.237_{ \\pm 0.009}$ <br> $6.946_{ \\pm 0.007}$ <br> $6.770_{ \\pm 0.020}$ <br> $6.803_{ \\pm 0.011}$ |\n| Embedding dimensionality $\\left(t_{\\min }=0.1\\right)$ | $w=16$ <br> $w=32$ <br> $w=64$ <br> $w=128$ <br> $w=256$ <br> $w=512$ <br> $w=1024$ | $4.493_{ \\pm 0.006}$ <br> $4.423_{ \\pm 0.008}$ <br> $4.392_{ \\pm 0.003}$ <br> $4.391_{ \\pm 0.003}$ <br> $4.398_{ \\pm 0.009}$ <br> $4.389_{ \\pm 0.007}$ <br> $4.386_{ \\pm 0.004}$ | $7.268_{ \\pm 0.005}$ <br> $7.237_{ \\pm 0.006}$ <br> $7.222_{ \\pm 0.008}$ <br> $7.235_{ \\pm 0.007}$ <br> $7.248_{ \\pm 0.008}$ <br> $7.232_{ \\pm 0.009}$ <br> $7.222_{ \\pm 0.003}$ |\n| Embedding initialisation scale | $\\sigma=0.0001$ <br> $\\sigma=0.001$ <br> $\\sigma=0.01$ <br> $\\sigma=0.1$ <br> $\\sigma=1.0$ | $4.376_{ \\pm 0.004}$ <br> $4.392_{ \\pm 0.004}$ <br> $4.386_{ \\pm 0.006}$ <br> $4.437_{ \\pm 0.003}$ <br> $4.617_{ \\pm 0.002}$ | $7.226_{ \\pm 0.006}$ <br> $7.237_{ \\pm 0.009}$ <br> $7.230_{ \\pm 0.008}$ <br> $7.257_{ \\pm 0.003}$ <br> $7.251_{ \\pm 0.003}$ |\n| Masking strategy | prefix, fixed length (32) <br> prefix, random length <br> fully random <br> $20 \\%$ prefix $+80 \\%$ fully random <br> $50 \\%$ prefix $+50 \\%$ fully random <br> $80 \\%$ prefix $+20 \\%$ fully random | $4.411_{ \\pm 0.006}$ <br> $4.417_{ \\pm 0.003}$ <br> $4.401_{ \\pm 0.011}$ <br> $4.402_{ \\pm 0.004}$ <br> $4.392_{ \\pm 0.004}$ <br> $4.394_{ \\pm 0.012}$ | $7.239_{ \\pm 0.007}$ <br> $7.233_{ \\pm 0.003}$ <br> $7.206_{ \\pm 0.004}$ <br> $7.228_{ \\pm 0.007}$ <br> $7.237_{ \\pm 0.009}$ <br> $7.222_{ \\pm 0.009}$ |\n| Self-conditioning (training and sampling) | neither <br> training only <br> both | $4.781_{ \\pm 0.010}$ <br> $4.892_{ \\pm 0.010}$ <br> $4.392_{ \\pm 0.004}$ | $7.224_{ \\pm 0.008}$ <br> $7.241_{ \\pm 0.008}$ <br> $7.237_{ \\pm 0.009}$ |\n| Time warping (training and sampling) | neither <br> training only <br> both | $5.091_{ \\pm 0.007}$ <br> $5.092_{ \\pm 0.013}$ <br> $4.392_{ \\pm 0.004}$ | $7.308_{ \\pm 0.006}$ <br> $7.311_{ \\pm 0.008}$ <br> $7.237_{ \\pm 0.009}$ |\n| Time warping: temperature | $T=0.5$ <br> $T=0.8$ <br> $T=0.9$ <br> $\\mathbf{T}=\\mathbf{1} . \\mathbf{0}$ <br> $T=1.2$ <br> $T=1.5$ <br> $T=2.0$ <br> $T=5.0$ <br> $T=10.0$ <br> $T=100.0$ | $4.445_{ \\pm 0.006}$ <br> $4.396_{ \\pm 0.008}$ <br> $4.393_{ \\pm 0.012}$ <br> $4.392_{ \\pm 0.004}$ <br> $4.383_{ \\pm 0.005}$ <br> $4.416_{ \\pm 0.006}$ <br> $4.442_{ \\pm 0.006}$ <br> $4.737_{ \\pm 0.006}$ <br> $4.871_{ \\pm 0.009}$ <br> $5.036_{ \\pm 0.007}$ | $7.244_{ \\pm 0.003}$ <br> $7.245_{ \\pm 0.005}$ <br> $7.235_{ \\pm 0.011}$ <br> $7.237_{ \\pm 0.009}$ <br> $7.226_{ \\pm 0.002}$ <br> $7.251_{ \\pm 0.007}$ <br> $7.227_{ \\pm 0.008}$ <br> $7.281_{ \\pm 0.005}$ <br> $7.275_{ \\pm 0.006}$ <br> $7.285_{ \\pm 0.006}$ |\n| Time warping: uniformity | $\\begin{aligned}\\(\\mu & =0.0 \\\\ \\mu & =0.01 \\\\ \\mu & =0.02 \\\\ \\mu & =0.05 \\\\ \\mu & =0.1 \\\\ \\mu & =0.2 \\\\ \\mu & =0.5 \\\\ \\mu & =1.0\\end{aligned}$\\) | $4.392_{ \\pm 0.004}$ <br> $4.380_{ \\pm 0.005}$ <br> $4.381_{ \\pm 0.006}$ <br> $4.380_{ \\pm 0.003}$ <br> $4.377_{ \\pm 0.011}$ <br> $4.419_{ \\pm 0.005}$ <br> $4.463_{ \\pm 0.010}$ <br> $5.074_{ \\pm 0.002}$ | $7.237_{ \\pm 0.009}$ <br> $7.221_{ \\pm 0.009}$ <br> $7.222_{ \\pm 0.005}$ <br> $7.235_{ \\pm 0.005}$ <br> $7.221_{ \\pm 0.010}$ <br> $7.253_{ \\pm 0.005}$ <br> $7.249_{ \\pm 0.006}$ <br> $7.305_{ \\pm 0.003}$ |\n| Time warping: Beta-CDF | $\\alpha=0.5, \\beta=0.5$ <br> $\\alpha=0.5, \\beta=1.0$ <br> $\\alpha=0.5, \\beta=2.0$ <br> $\\alpha=1.0, \\beta=0.5$ <br> $\\alpha=1.0, \\beta=1.0$ <br> $\\alpha=1.0, \\beta=2.0$ <br> $\\alpha=2.0, \\beta=0.5$ <br> $\\alpha=2.0, \\beta=1.0$ <br> $\\alpha=2.0, \\beta=2.0$ | $4.364_{ \\pm 0.005}$ <br> $4.362_{ \\pm 0.007}$ <br> $4.397_{ \\pm 0.002}$ <br> $4.449{ }_{ \\pm 0.007}$ <br> $4.392_{ \\pm 0.004}$ <br> $4.402_{ \\pm 0.007}$ <br> $4.544_{ \\pm 0.011}$ <br> $4.456_{ \\pm 0.008}$ <br> $4.408_{ \\pm 0.009}$ | $7.238_{ \\pm 0.005}$ <br> $7.224_{ \\pm 0.008}$ <br> $7.23{ }^{ \\pm 0.006}$ <br> $7.242_{ \\pm 0.009}$ <br> $7.237{ }_{ \\pm 0.009}$ <br> 7.236 $\\pm 0.004$ <br> $7.236_{ \\pm 0.014}$ <br> $7.238_{ \\pm 0.009}$ <br> $7.225_{ \\pm 0.007}$ |\n\nTable 1 | Design decision ablations. We measure the negative log likelihood under an autoregressive language model (AR-NLL) and the unigram entropy (H), averaged over 5 runs, along with the standard error.",
    "contdiffu-10": "Base model results are bolded. See $\\S 6.3$ for details and Figure 4 for a visualisation. ![](https://cdn.mathpix.com/cropped/2024_09_12_11c9bceac3b6de560e63g-13.jpg?height=814&width=1657&top_left_y=287&top_left_x=201)\n\nFigure $4 \\mid$ Design decision ablations. We report the negative log likelihood under an autoregressive language model (AR-NLL), averaged over 5 runs. See $\\S 6.3$ and Table 1 for details. Masking strategy We find that training with fully random masks improves results, even when evaluating using a prefix mask. We suspect that learning the embeddings becomes easier when bidirectional context is available. To ensure that the model uses enough capacity for prefix completion tasks, we use a 50-50 masking strategy for all other experiments. Self-conditioning Like Strudel et al. (2022), we find that self-conditioning (Chen et al., 2022) has a significant positive impact on model performance, by enabling reuse of computation from preceding sampling steps. The AR-NLL improves significantly, while the entropy stays roughly at the same level. Time warping During training, focusing on the right noise levels is clearly important, but we also find that spacing the sampling steps accordingly is essential to benefit from this improvement (see \u00a73.3). We experimented with several manipulations of the warping function to verify the quality of our proposed entropy linearisation heuristic. We find that changing the temperature or the uniformity of the weighting (see Appendix A and Figure 8) can sometimes yield improvements, but they are relatively minor. Targeting linearity of the loss w.r.t. uniform time $u$ implies an assumption that 'all bits are equal', i.e. all information is equally important. To ensure that this is sensible, we also experimented with different target shapes of the loss as a function of $u$. To target a functional shape $S(u):[0,1] \\mapsto[0,1]$, we minimise $\\left(S(F(t)) \\frac{\\tilde{F}(t)}{F(t)}-\\mathcal{L}(t)\\right)^{2}$ instead of $(\\tilde{F}(t)-\\mathcal{L}(t))^{2}$ to fit the unnormalised CDF ( $S$ must be differentiable). Using the CDF of the Beta distribution for $S(u)$, we can produce target shapes with different slopes. We again find only minor improvements by deviating from $\\alpha=1.0, \\beta=1.0$, which corresponds to the identity function (i.e. targeting a linear shape). ### 6.4. Sampling\n\nUsing the base model, we investigate the impact of various sampling hyperparameters when they are varied individually. Note that there are some significant interactions between sampling hyperparameters, which are not reflected in per-parameter experiments. We will also look at the interaction between the score temperature and the classifier-free guidance scale as an example of this. Algorithm and number of steps We compare the Euler and Heun samplers suggested by Karras et al. (2022), reducing the number of sampling steps by a factor of two for the Heun sampler, so that the total number of required function evaluations does not change. For self-conditioning, we always use the most recent model prediction. The Heun sampler seems to offer little benefit when\n\n| Hyperparameter |  | AR-NLL | Entropy | Hyperparameter |  | AR-NLL | Entropy |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Base model |  | $4.392_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ | Data |  | 3.389 | 7.179 |\n| \\# steps (Euler <br> sampler) | $N=10$ | $5.039_{ \\pm 0.009}$ | $7.281_{ \\pm 0.010}$ | \\# steps (Heun <br> sampler) | $N=5$ | $4.600_{ \\pm 0.006}$ | $6.955_{ \\pm 0.009}$ |\n|  | $N=20$ | $4.775_{ \\pm 0.009}$ | $7.270_{ \\pm 0.010}$ |  | $N=10$ | $4.350_{ \\pm 0.003}$ | $7.044_{ \\pm 0.007}$ |\n|  | $N=50$ | $4.546_{ \\pm 0.005}$ | $7.249_{ \\pm 0.008}$ |  | $N=25$ | $4.433_{ \\pm 0.007}$ | $7.196_{ \\pm 0.008}$ |\n|  | $N=100$ | $4.448_{ \\pm 0.004}$ | $7.241_{ \\pm 0.008}$ |  | $N=50$ | $4.414_{ \\pm 0.005}$ | $7.227_{ \\pm 0.008}$ |\n|  | $\\mathrm{N}=200$ | $4.392_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |  | $N=100$ | $4.380_{ \\pm 0.004}$ | $7.231_{ \\pm 0.008}$ |\n|  | $N=500$ | $4.353_{ \\pm 0.005}$ | $7.231_{ \\pm 0.009}$ |  | $N=250$ | $4.347_{ \\pm 0.004}$ | $7.229_{ \\pm 0.008}$ |\n| Score <br> temperature | $T=0.5$ | 2.830 $\\pm 0.101$ | $6.558_{ \\pm 0.102}$ | Softmax <br> temperature | $T=0.5$ | $3.743_{ \\pm 0.004}$ | $6.708_{ \\pm 0.006}$ |\n|  | $T=0.8$ | $3.379_{ \\pm 0.049}$ | $7.010_{ \\pm 0.029}$ |  | $T=0.8$ | $4.041_{ \\pm 0.005}$ | $6.999_{ \\pm 0.006}$ |\n|  | $T=0.9$ | $3.838_{ \\pm 0.010}$ | $7.028_{ \\pm 0.009}$ |  | $T=0.9$ | $4.201_{ \\pm 0.004}$ | $7.112_{ \\pm 0.007}$ |\n|  | $T=0.95$ | $4.073_{ \\pm 0.003}$ | $7.118_{ \\pm 0.008}$ |  | $T=0.95$ | $4.294_{ \\pm 0.003}$ | $7.174_{ \\pm 0.007}$ |\n|  | $T=0.98$ | $4.253_{ \\pm 0.003}$ | $7.184_{ \\pm 0.008}$ |  | $T=0.98$ | $4.351_{ \\pm 0.005}$ | $7.211_{ \\pm 0.008}$ |\n|  | $T=0.99$ | $4.324_{ \\pm 0.003}$ | $7.211_{ \\pm 0.008}$ |  | $T=0.99$ | $4.372_{ \\pm 0.003}$ | $7.224_{ \\pm 0.008}$ |\n|  | $\\mathrm{T}=1.0$ | $4.392+0.004$ | $7.237_{ \\pm 0.009}$ |  | $\\mathrm{~T}=1.0$ | $4.39{ }^{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |\n| Initial noise <br> scale | $\\sigma=0.5$ | ![](https://cdn.mathpix.com/cropped/2024_09_12_11c9bceac3b6de560e63g-14.jpg?height=40&width=161&top_left_y=1131&top_left_x=680) | $7.044_{ \\pm 0.011}$ | Softmax nucleus | $p=0.5$ | $3.955_{ \\pm 0.004}$ | $6.965_{ \\pm 0.005}$ |\n|  | $\\sigma=0.8$ | $4.134_{ \\pm 0.004}$ | $7.137_{ \\pm 0.008}$ |  | $p=0.8$ | $4.217_{ \\pm 0.005}$ | $7.137 \\pm 0.007$ |\n|  | $\\sigma=0.9$ | $4.256_{ \\pm 0.002}$ | $7.184_{ \\pm 0.008}$ |  | $p=0.9$ | $4.303_{ \\pm 0.003}$ | $7.188_{ \\pm 0.007}$ |\n|  | $\\sigma=0.95$ | $4.325_{ \\pm 0.003}$ | $7.210_{ \\pm 0.008}$ |  | $p=0.95$ | $4.348_{ \\pm 0.004}$ | $7.213_{ \\pm 0.007}$ |\n|  | $\\sigma=0.98$ | $4.364 \\pm 0.004$ | $7.226_{ \\pm 0.008}$ |  | $p=0.98$ | $4.374_{ \\pm 0.004}$ | $7.228_{ \\pm 0.008}$ |\n|  | $\\sigma=0.99$ | $4.379_{ \\pm 0.004}$ | $7.231_{ \\pm 0.008}$ |  | $p=0.99$ | $4.382_{ \\pm 0.003}$ | $7.233_{ \\pm 0.008}$ |\n|  | $\\sigma=1.0$ | $4.39{ }^{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |  | $p=1.0$ | $4.392_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |\n| Step spacing (no <br> warping) | $\\rho=1.0$ | $5.068_{ \\pm 0.016}$ | $7.294_{ \\pm 0.012}$ | Step spacing <br> (warping) | $\\rho=1.0$ | $4.392_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |\n|  | $\\rho=2.0$ | $4.631_{ \\pm 0.005}$ | $7.256_{ \\pm 0.009}$ |  | $\\rho=2.0$ | $4.405_{ \\pm 0.005}$ | $7.240_{ \\pm 0.009}$ |\n|  | $\\rho=4.0$ | $4.534_{ \\pm 0.004}$ | $7.248_{ \\pm 0.008}$ |  | $\\rho=4.0$ | $4.424_{ \\pm 0.005}$ | $7.243_{ \\pm 0.008}$ |\n|  | $\\rho=8.0$ | $4.509_{ \\pm 0.005}$ | $7.246_{ \\pm 0.009}$ |  | $\\rho=8.0$ | $4.443_{ \\pm 0.005}$ | $7.247_{ \\pm 0.009}$ |\n| Time warping: <br> temperature | $T=0.5$ | $4.444_{ \\pm 0.004}$ | $7.259_{ \\pm 0.008}$ | Time warping: <br> uniformity | $\\mu=0.0$ | $4.392_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |\n|  | $T=0.8$ | $4.396_{ \\pm 0.005}$ | $7.239_{ \\pm 0.009}$ |  | $\\mu=0.01$ | $4.392_{ \\pm 0.004}$ | $7.237 \\pm 0.009$ |\n|  | $T=0.9$ | $4.394_{ \\pm 0.004}$ | $7.238_{ \\pm 0.009}$ |  | $\\mu=0.02$ | $4.394_{ \\pm 0.004}$ | $7.236{ }_{ \\pm 0.008}$ |\n|  | $\\mathrm{T}=1.0$ | $4.392^{ \\pm 0.004}$ | $7.237{ }^{+0.009}$ |  | $\\mu=0.05$ | $4.394_{ \\pm 0.004}$ | $7.236_{ \\pm 0.008}$ |\n|  | $T=1.2$ | $4.396_{ \\pm 0.004}$ | $7.236_{ \\pm 0.008}$ |  | $\\mu=0.1$ | $4.398_{ \\pm 0.005}$ | $7.237_{ \\pm 0.008}$ |\n|  | $T=1.5$ | $4.410_{ \\pm 0.005}$ | $7.237_{ \\pm 0} .008$ |  | $\\mu=0.2$ | $4.402_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |\n|  | $T=2.0$ | $4.441_{ \\pm 0.004}$ | $7.238_{ \\pm 0.008}$ |  | $\\mu=0.5$ | $4.435_{ \\pm 0.005}$ | $7.238_{ \\pm 0.008}$ |\n|  | $T=5.0$ | $4.662_{ \\pm 0.006}$ | $7.255_{ \\pm 0.009}$ |  | $\\mu=1.0$ | $5.070_{ \\pm 0.017}$ | $7.295_{ \\pm 0.012}$ |\n|  | $T=10.0$ | $4.833_{ \\pm 0.009}$ | $7.269_{ \\pm 0.011}$ |  |  |  |  |\n|  | $T=100.0$ | $5.043_{ \\pm 0.014}$ | $7.291_{ \\pm 0.011}$ |  |  |  |  |\n| CF guidance <br> scale | $\\gamma=1.0$ | $4.392_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |  |  |  |  |\n|  | $\\gamma=2.0$ | 4.157 ${ }_{ \\pm 0.007}$ | $7.198_{ \\pm 0.007}$ |  |  |  |  |\n|  | $\\gamma=4.0$ | $3.927 \\pm 0.009$ | $7.157_{ \\pm 0.007}$ |  |  |  |  |\n|  | $\\gamma=8.0$ | $3.731_{ \\pm 0.005}$ | $7.118_{ \\pm 0.005}$ |  |  |  |  |\n|  | $\\gamma=16.0$ | $3.580_{ \\pm 0.007}$ | $7.100_{ \\pm 0.003}$ |  |  |  |  |\n| Final prediction | disabled | $4.394_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |  |  |  |  |\n|  | enabled | $4.392_{ \\pm 0.004}$ | $7.237_{ \\pm 0.009}$ |  |  |  |  |\n\nTable $2 \\mid$ Effect of sampling hyperparameters. We measure the negative log likelihood under an autoregressive language model (AR-NLL) and the unigram entropy at the token level (H), averaged over 5 runs, along with the standard error.",
    "contdiffu-11": "Base model results are bolded. Note that there are some significant interactions between sampling hyperparameters, which are not reflected in this per-parameter experiment. See $\\S 6.4$ for details and Figure 5 for a visualisation. ![](https://cdn.mathpix.com/cropped/2024_09_12_11c9bceac3b6de560e63g-15.jpg?height=814&width=1641&top_left_y=287&top_left_x=213)\n\nFigure $5 \\mid$ Effect of sampling hyperparameters. We report the negative log likelihood under an autoregressive language model (AR-NLL) and the unigram entropy at the token level (H), averaged over 5 runs. Marker size increases with hyperparameter values. The + sign corresponds to the default values. See $\\S 6.4$ and Table 2 for details. using a sufficient number of steps ${ }^{10}$, and entropy degrades when the number of steps is decreased, which is not the case for the Euler sampler. We use the Euler sampler with 200 steps for most experiments, but reasonable quality can be achieved with as few as 50 function evaluations. Based on results in the image domain, it may be possible to reduce this further with stochastic samplers ${ }^{11}$, or more advanced sampling algorithms (Lu et al., 2022a,b). Scaling hyperparameters Many classes of generative models offer some notion of 'temperature tuning'. The CDCD framework is particularly versatile in this regard:\n\n- scaling the score function estimate by a factor corresponds to changing the temperature of $p_{t}(\\mathbf{x})$;\n- the standard deviation of the initial noise can be reduced below 1.0, which is often done for flowbased models (Kingma and Dhariwal, 2018);\n- since the model produces a categorical probability distribution $p\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}, t\\right)$, we can also manipulate this using various truncation strategies,\n\n[^7]such as temperature tuning and nucleus sampling ${ }^{12}$ (Holtzman et al., 2019);\n\n- classifier-free guidance (Ho and Salimans, 2022) can be used to amplify the influence of the conditioning. Out of these, we find that scaling the score temperature or changing the initial noise scale offer a strictly better trade-off than manipulating $p\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}, t\\right)$. The effectiveness of changing the score temperature is surprising, because this tends to work very poorly for diffusion-based models in the visual domain. While the trade-off offered by classifier-free guidance seems even better at first glance, in practice we find that samples obtained with high guidance scales tend to contain a lot of repeated phrases. We also study the interaction between the score temperature and the guidance scale (see Figure 6), and find that their effects are complementary to a degree. Step spacing Karras et al. (2022) suggest that spacing the sampling steps non-uniformly significantly improves sample quality. For CDCD, our use of time warping at sampling time already results in nonuniform step spacing. We compare their heuristic\n\n[^8]![](https://cdn.mathpix.com/cropped/2024_09_12_11c9bceac3b6de560e63g-16.jpg?height=524&width=789&top_left_y=292&top_left_x=219)\n\nFigure 6 | Interaction of the score temperature and guidance scale sampling hyperparameters. Marker size increases with score temperature. The AR-NLL and entropy of the training data are indicated with $\\times$. for $\\rho=1,2,4$ or 8 with time warping, and we also investigate whether they compound, by applying time warping to non-uniformly spaced steps obtained using their heuristic. When not using time warping at sampling time, the heuristic is clearly helpful, but it does not reach the same performance. We also find that the effects do not compound, strengthening our intuition that spacing the steps using time warping is close to optimal, because it makes the rate of decrease of uncertainty approximately constant during sampling. Time warping In \u00a76.3, we established that manipulating the warping function at training time is not particularly helpful.",
    "contdiffu-12": "We can also choose to manipulate it only at sampling time, where it can still affect step spacing (but not model training). We again find that these manipulations do not have any meaningful positive effect on sample quality. Final prediction We run the model one additional time after all sampling steps are completed, and take the argmax of the predicted distributions at each sequence position to determine the sampled tokens. Instead, we could use the tokens whose embeddings are nearest to the predicted embeddings in the Euclidean sense.",
    "contdiffu-13": "This works equally well, so it can save some computation, though this only becomes significant if the number of sampling steps is greatly reduced. ### 6.5. Prompt completion and infilling\n\nWe compare a 1.3B parameter model based on the CDCD framework with a pre-trained autoregressive model with the same architecture (24 Transformer blocks, 2048 units, 16 attention heads). Both models are trained on the C4 dataset for 600,000 steps with 524,288 tokens per batch. Due to the reduced data efficiency of diffusion model training (see \u00a72.3), it is likely that the CDCD model would benefit more from further training. Relative to the autoregressive model, the CDCD model has some extra learnable parameters in the MLP for timestep embedding, and in an initial linear layer which maps the token embeddings to the Transformer hidden state. The token embeddings themselves on the other hand account for $8 \\times$ fewer parameters, because we use an embedding dimensionality of 256 instead of 2048. While the autoregressive model was trained with a batch size of 256 and a sequence length of 2048 , we trained the CDCD model with a batch size of 2048 and a sequence length of 256 instead. This is partly because at this point, we are not focusing our evaluation on long-range coherence, but also because diffusion model training benefits from larger batch sizes: since noise levels are sampled on a per-sequence basis, a larger batch size yields a lower variance loss estimate. Note that the number of sampling steps is still 200, which is now smaller than the sequence length. We take 5,000 token sequences of length 256 from the C 4 validation set (rejecting shorter sequences and cropping longer ones). For each sequence, we select a random-length prefix to use as the prompt, with prompt lengths varying between 0 and 128 (half the sequence length). We sample from the autoregressive model using these prompts with nucleus sampling, for various values of $p$. We prevent the end-of-sentence (EOS) token from being sampled, to ensure that a full-length sequence is produced every time. This is necessary because the MAUVE metric is calculated on the full sequences, including the prompts, so the presence of shorter sequences would bias the results. We also sample from the CDCD model using the same set of prompts, with different score temperatures ( $T$ ) and classifier-free guidance scales $(\\gamma)$. The resulting completions are evaluated by comparing them with the original sequences from the dataset using MAUVE, which we report in Table 3, alongside the AR-NLL (measured with the autoregressive model itself ${ }^{13}$ ) and the unigram entropy for the generated sequences. We get favourable MAUVE scores with the CDCD model for several settings of the score temperature $T$ and guidance scale $\\gamma$. Note however that the MAUVE scores for the autoregressive samples do not seem to be convex in $p$, and it is unclear to what extent\n\n[^9]any excessive repetition introduced by increasing the guidance scale is penalised, so these numbers should be interpreted with care. Nevertheless, they provide some evidence that the CDCD model is able to produce compelling samples. Selected prompt completion and infilling samples are shown in Figure 7. ### 6.6. Machine translation\n\nWe compare a Transformer autoregressive machine translation model to a CDCD model with an encoderdecoder of the same size, on three translation tasks - WMT 2014 English-German, WMT 2014 GermanEnglish, and WMT 2020 Chinese-English - and two model sizes - Transformer base and big. For each task, all models are trained on the same training set of the corresponding year, and evaluated on newstest2014 and newstest2020 accordingly. The standard validation sets are used for early stopping and for tuning the hyperparameters of CDCD. We train two shared SentencePiece (Kudo and Richardson, 2018) tokenizers (one for English/German and one for Chinese/English) of size 32,768 , using the unigram method (Kudo, 2018), with byte fallback. For the autoregressive baseline, we use the hyperparameters described by Vaswani et al. (2017), except for the following: we tie the source/target/output embeddings following Press and Wolf (2017), we use the fixed vocabulary size indicated above, and, for Chinese-English only, we use a beam size of 6, and a length penalty $\\alpha=0.6$. The CDCD models are trained for 2 M steps, with a batch size of 512 for base and 1024 for big, and a sequence length of 160 . We use the same dropout values as for the autoregressive baseline. At sampling time, swe set the score temperature $T=0.8$ and classifier-free guidance scales $\\gamma=4.0$ for English-German and German-English, and $\\gamma=8.0$ for Chinese-English, found using a manual hyperparameter search on the validation sets. Using a diffusion framework provides a natural way of approximating Minimum Bayes-Risk decoding (Eikema and Aziz, 2021; Kumar and Byrne, 2004) by sampling a number of hypotheses, each from a different initial noise vector, and selecting the one minimizing in expectation a metric of interest, in our case BLEU. We report BLEU scores computed on the test set for each model, year and language pair, using sacreBLEU ${ }^{14}$ (Post, 2018), in Table 4. Overall, CDCD models perform worse (between 3 and 7 BLEU points) than autoregressive counterparts of the same size. The dif-\n\n[^10]ference is largest on English-German, where manual examination of the samples reveals that hypotheses in English sometimes appear, even though German output is expected. This certainly contributes to the poor score that the CDCD model obtains on this pair. It might be explained by the noisy character of the training data itself (English appearing on the German side). CDCD does comparatively better on ChineseEnglish, especially at larger scale, which may enable the model to make use of the larger training set. Finally, sampling-based MBR decoding improves translation quality, and monotonically so as a function of the number of samples, providing a gain of 0.7 to 1.8 BLEU points when using 100 samples compared to a single one. Nonetheless, hypotheses produced by CDCD in our experiments tend to suffer from systematic defects, in particular repeated or missing tokens. We hypothesise that the model is not trained to recover from these errors, and that data augmentation, in the form of extra padding, or corruption, may prove beneficial. We leave exploring this direction to future work. ## 7. Discussion\n\nWe have proposed CDCD, a framework for diffusion of categorical data that is continuous both in time and input space, which enables training of nonautoregressive language models with a procedure reminiscent of BERT (see \u00a74.5). Our proposed approach has some advantages over autoregressive models, such as the ability to perform arbitrary infilling ${ }^{15}$ and a flexible sampling procedure which allows trading off sample quality and compute requirements. It also stands to benefit from diffusion model enhancements such as classifier-free guidance and improved sampling algorithms, and from the ability to deterministically map inputs to latents and vice versa (which we have not explored so far). Some limitations compared to autoregressive models were discussed in \u00a72.3. An important research question which we have not yet investigated, is how best to handle variable-length output. With autoregression, this is very naturally handled by introducing an endof-sentence (EOS) token, which the model can predict to indicate that sampling should be halted. Diffusion models on the other hand require a fixed-size 'canvas' which is iteratively refined during sampling, although inserting padding tokens at random during training (and collapsing them after sampling) could allow for some length variation (Strudel et al., 2022), and it may also reduce the incidence of repeated tokens in\n\n[^11]| Model |  |  | MAUVE | AR-NLL | Entropy |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| Data |  |  | 1.000 | 2.580 | 7.569 |\n| Autoregressive with nucleus sampling | $p=0.5$ |  | 0.545 | 1.469 | 7.095 |\n|  | $p=0.8$ |  | 0.900 | 1.887 | 7.204 |\n|  | $p=0.9$ |  | 0.940 | 1.988 | 7.235 |\n|  | $p=0.95$ |  | 0.917 | 2.016 | 7.244 |\n|  | $p=0.98$ |  | 0.928 | 2.025 | 7.241 |\n|  | $p=0.99$ |  | 0.931 | 2.027 | 7.242 |\n|  | $p=1.0$ |  | 0.931 | 2.029 | 7.242 |\n| CDCD with classifier-free guidance | $T=0.5$ | $\\gamma=1.0$ | 0.006 | 2.377 | 6.591 |\n|  |  | $\\gamma=2.0$ | 0.006 | 1.830 | 6.444 |\n|  |  | $\\gamma=4.0$ | 0.009 | 1.542 | 6.532 |\n|  |  | $\\gamma=8.0$ | 0.013 | 1.522 | 6.889 |\n|  | $T=0.8$ | $\\gamma=1.0$ | 0.215 | 3.070 | 6.726 |\n|  |  | $\\gamma=2.0$ | 0.523 | 2.399 | 7.246 |\n|  |  | $\\gamma=4.0$ | 0.574 | 2.073 | 7.488 |\n|  |  | $\\gamma=8.0$ | 0.555 | 1.894 | 7.597 |\n|  | $T=0.9$ | $\\gamma=1.0$ | 0.914 | 3.130 | 7.353 |\n|  |  | $\\gamma=2.0$ | 0.939 | 2.808 | 7.433 |\n|  |  | $\\gamma=4.0$ | 0.928 | 2.535 | 7.510 |\n|  |  | $\\gamma=8.0$ | 0.884 | 2.309 | 7.570 |\n|  | $T=0.95$ | $\\gamma=1.0$ | 0.927 | 3.466 | 7.491 |\n|  |  | $\\gamma=2.0$ | 0.951 | 3.173 | 7.517 |\n|  |  | $\\gamma=4.0$ | 0.952 | 2.874 | 7.555 |\n|  |  | $\\gamma=8.0$ | 0.911 | 2.614 | 7.594 |\n|  | $T=0.98$ | $\\gamma=1.0$ | 0.907 | 3.724 | 7.570 |\n|  |  | $\\gamma=2.0$ | 0.938 | 3.443 | 7.576 |\n|  |  | $\\gamma=4.0$ | 0.950 | 3.125 | 7.594 |\n|  |  | $\\gamma=8.0$ | 0.947 | 2.829 | 7.613 |\n|  | $T=0.99$ | $\\gamma=1.0$ | 0.913 | 3.821 | 7.595 |\n|  |  | $\\gamma=2.0$ | 0.925 | 3.539 | 7.597 |\n|  |  | $\\gamma=4.0$ | 0.953 | 3.217 | 7.607 |\n|  |  | $\\gamma=8.0$ | 0.943 | 2.906 | 7.621 |\n|  | $T=1.0$ | $\\gamma=1.0$ | 0.889 | 3.926 | 7.620 |\n|  |  | $\\gamma=2.0$ | 0.917 | 3.639 | 7.618 |\n|  |  | $\\gamma=4.0$ | 0.949 | 3.312 | 7.623 |\n|  |  | $\\gamma=8.0$ | 0.937 | 2.986 | 7.633 |\n\nTable 3 | Prompt completion evaluation. We report MAUVE, AR-NLL and unigram entropy across 5,000 prefix prompts of varying length (see \u00a76.5), for a 1.3B parameter autoregressive language model trained on the C4 dataset, and an equivalent CDCD model. MAUVE results that meet or exceed the best autoregressive result are bolded. | Size | Model | WMT 2014 EN-DE | WMT 2014 DE-EN | WMT 2020 ZH-EN |\n| :--- | :--- | ---: | ---: | ---: |\n| Transformer base | Autoregressive | 26.2 | 30.2 | 25.2 |\n|  | CDCD $(n=1)$ | 19.3 | 24.9 | 20.7 |\n|  | CDCD $(n=10)$ | 19.7 | 25.4 | 21.7 |\n|  | CDCD $(n=100)$ | 20.0 | 26.0 | 22.2 |\n| Transformer big | Autoregressive | 27.6 | 31.3 | 27.1 |\n|  | CDCD $(n=1)$ | 20.7 | 26.0 | 24.0 |\n|  | CDCD $(n=10)$ | 21.9 | 27.0 | 25.0 |\n|  | CDCD $(n=100)$ | 22.4 | 27.8 | 25.6 |\n\nTable 4 | Machine translation evaluation. $n$ indicates the number of samples used to approximate Minimum Bayes-Risk decoding. ![](https://cdn.mathpix.com/cropped/2024_09_12_11c9bceac3b6de560e63g-19.jpg?height=937&width=764&top_left_y=294&top_left_x=241)\n\nFigure 7 | Completion and infilling samples from a 1.3B CDCD language model with score temperature $T=0.9$ and guidance scale $\\gamma=1.0$ (see $\\S 6.5$ ). Prompts are bolded. The model always produces sequences of 256 tokens, so the samples are truncated. the samples. Model architecture We intentionally limited the degree of architectural exploration in this work, because the choice of architecture is largely orthogonal to the framework we have proposed. We focused on the Transformer as it is currently the canonical language model architecture. Nevertheless, we pointed out the absence of any architectural restrictions in this modelling paradigm (see \u00a72.3), which considerably simplifies the use of more intricate patterns, such as multi-resolution or Perceiver-based architectures (Jaegle et al., 2021). We would also like to explore recent innovations for sampling from diffusion models (e.g. Lu et al. (2022a,b)), which have so far been demonstrated chiefly in the image domain, and determine to what extent they improve sampling efficiency in the CDCD framework. Application domains While we concentrated our empirical evaluation of the proposed CDCD framework on language tasks, none of its components are specific to language. We expect it to be suitable for any\nThis easy chicken curry recipe is made with just a handful of ingredients, including a healthy combination of flavored rice that you can easily make at home. Preheat oven to $350^{\\circ} \\mathrm{F}\\left(175^{\\circ} \\mathrm{C}\\right)$. Line a pan with foil. Combine paprika, garlic, ginger, baking powder and $1 / 2$ cup ( 2 tsp ) salt. Set aside for 15 minutes. Prepare rice according to directions. Or, cook rice in boiling water for 1 minute and drain completely. Heat a heavy-bottomed saucepan over high heat. Once the oil has been heated, add the garlic and green onions; stir-fry for 30 seconds until fragrant. Add the chicken and stir-fry for about 2 minutes. Add ginger, honey, tomatoes, coconut milk and bay leaves; bring to a boil. Reduce heat to low to cover and cook for 10 minutes. stir-fry in the salt. Add the cooked rice and mix well. Transfer to oven and cook for about 20 minutes or until chicken is well cooked and rice is tender. A year ago in Paris, we spent three hours at the Eiffel Tower in the morning.",
    "contdiffu-14": "We enjoyed the show, visited the tourist sites, went to the ceremony, took photos, and visited the cafe. Wow, what a great day! Flower Shop is pleased to show you our wonderful collection of Gift and Fruit Baskets perfect for sending with New Bedfordshire Flowers.",
    "contdiffu-15": "You will be able to find gifts to suit every special occasion such as Valentine's Day, Mother's Day or Father's day. We also have a large selection of fresh cakes, gift baskets, balloons and many other gifts. Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals(such as images and sound) through iterative refinement. In this paper, we propose a straightforward computational approach that identifies the signals to model through refinement without relying solely on the extraction and storage model rather than an unsupervised model. The formulation is continuous, is able to quantify independent layer changes and is robust across a variety of datasets. The approach is also fully domain-independent and is easily compatible with any other diffusion model on the net. We demonstrate its efficacy on several language modelling tasks. generative modelling problem involving structured categorical data. As an example, we note that latent diffusion models such as Stable Diffusion (Rombach et al., 2022) first use VQ-VAE (Van Den Oord et al., 2017) or VQ-GAN (Esser et al., 2021) to learn a latent space in which the diffusion process is then applied, by mapping the categorical latents to the continuous embeddings from the vector quantisation bottleneck. While this works well, we hypothesise that fitting new embeddings jointly with the diffusion model could bring further improvements, especially in combination with time warping. Finally, our proposed time warping heuristic may also be useful beyond diffusion models of categorical data. ## Acknowledgements\n\nWe would like to thank Andy Brock, Bart Chrzaszcz, Noah Constant, Jeff Donahue, Douglas Eck, Dominik Grewe, Jordan Hoffmann, Patrick Kidger, Skanda Koppula, Lena Martens, Katie Millican, Ben Moran, Simon Osindero, Evan Shelhamer, Milo\u0161 Stanojevi\u0107, Federico Vaggi, Bj\u00f6rn Winckler, and the wider DeepMind team for their assistance and input. We are also thankful to the creators and main-\ntainers of the open source software used in this work, including Python (Van Rossum and Drake, 2009), NumPy (Harris et al., 2020), SciPy (Virtanen et al., 2020), JAX (Bradbury et al., 2018), TensorFlow (Abadi et al., 2016), the DeepMind JAX Ecosystem (Babuschkin et al., 2020), Diffrax (Kidger, 2021) and Matplotlib (Hunter, 2007).",
    "contdiffu-16": "## References\n\nM. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G.",
    "contdiffu-17": "Irving, M. Isard, et al. Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pages 265-283, 2016.",
    "contdiffu-18": "J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993, 2021.",
    "contdiffu-19": "I. Babuschkin, K. Baumli, A. Bell, S. Bhupatiraju, J. Bruce, P. Buchlovsky, D. Budden, T. Cai, A. Clark, I. Danihelka, C. Fantacci, J. Godwin, C. Jones, T. Hennigan, M. Hessel, S. Kapturowski, T. Keck, I. Kemaev, M. King, L. Martens, V. Mikulik, T. Norman, J. Quan, G. Papamakarios, R. Ring, F. Ruiz, A. Sanchez, R. Schneider, E. Sezener, S. Spencer, S. Srinivasan, W. Stokowiec, and F. Viola. The DeepMind JAX Ecosystem, 2020.",
    "contdiffu-20": "M. Bavarian, H. Jun, N. Tezak, J. Schulman, C. McLeavey, J. Tworek, and M. Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022. Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, O. Teboul, D. Grangier, M.",
    "contdiffu-21": "Tagliasacchi, and N. Zeghidour. AudioLM: a language modeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022. J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python + NumPy programs, 2018.",
    "contdiffu-22": "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.",
    "contdiffu-23": "A. Campbell, J. Benton, V. De Bortoli, T. Rainforth, G. Deligiannidis, and A. Doucet. A continuous time framework for discrete denoising models. arXiv preprint arXiv:2205.14987, 2022. W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly. Imputer: Sequence modelling via imputation and dynamic programming. In International Conference on Machine Learning, pages 1403-1413. PMLR, 2020. H. Chang, H. Zhang, L. Jiang, C. Liu, and W.",
    "contdiffu-24": "T. Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315-11325, 2022. T. Chen, R. Zhang, and G. Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022.",
    "contdiffu-25": "J. C. Cox, J. E. Ingersoll Jr, and S. A. Ross. A theory of the term structure of interest rates. Econometrica, 2:385-407, 1985. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020. S. Dieleman, C. Nash, J. Engel, and K. Simonyan. Variable-rate discrete representation learning. arXiv preprint arXiv:2103.06089, 2021. C. Donahue, I. Simon, and S. Dieleman. Piano genie. In Proceedings of the 24th International Conference on Intelligent User Interfaces, pages 160-164, 2019. C. Durkan, A. Bekasov, I. Murray, and G. Papamakarios. Neural spline flows. Advances in neural information processing systems, 32, 2019. B. Eikema and W. Aziz. Sampling-based approximations to minimum bayes risk decoding for neural machine translation. arXiv preprint arXiv:2108.04718, 2021. P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873-12883, 2021. M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural\n\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 6112-6121, 2019.",
    "contdiffu-26": "M. Ghazvininejad, O. Levy, and L. Zettlemoyer. Semiautoregressive training improves mask-predict decoding. arXiv preprint arXiv:2001.08785, 2020. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C.",
    "contdiffu-27": "Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27.",
    "contdiffu-28": "Curran Associates, Inc., 2014.",
    "contdiffu-29": "K. Goyal, C. Dyer, and T. Berg-Kirkpatrick. Exposing the implicit energy networks behind masked language models via metropolis-hastings. arXiv preprint arXiv:2106.02736, 2021.",
    "contdiffu-30": "J. Gu, J. Bradbury, C. Xiong, V. O. Li, and R. Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017. J. Gu, C. Wang, and J. Zhao. Levenshtein transformer. arXiv preprint arXiv:1905.11006, 2019. X. Han, S. Kumar, and Y. Tsvetkov. SSD-LM: Semiautoregressive simplex-based diffusion language model for text generation and modular control, 2022.",
    "contdiffu-31": "C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del R'1o, M. Wiebe, P. Peterson, P. G'erard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. Nature, 585(7825):357-362, Sept. 2020. doi: $10.1038 / \\mathrm{s} 41586-020-2649-2$. J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.",
    "contdiffu-32": "J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, et al. Imagen video: High definition video generation with diffusion models.",
    "contdiffu-33": "arXiv preprint arXiv:2210.02303, 2022a. J. Ho, T. Salimans, A. A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models. In ICLR Workshop on Deep Generative Models for Highly Structured Data, 2022b.",
    "contdiffu-34": "A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2019. E. Hoogeboom, A. A. Gritsenko, J. Bastings, B. Poole, R. v. d. Berg, and T. Salimans. Autoregressive diffusion models. arXiv preprint arXiv:2110.02037, 2021a. E. Hoogeboom, D. Nielsen, P. Jaini, P. Forr\u00e9, and M. Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454$12465,2021 b$.",
    "contdiffu-35": "X. S. Huang, F. Perez, and M. Volkovs. Improving non-autoregressive translation models without distillation. In International Conference on Learning Representations, 2022.",
    "contdiffu-36": "J. D. Hunter. Matplotlib: A 2d graphics environment. Computing in science \\& engineering, 9(3):90-95, 2007. A. Hyv\u00e4rinen and P. Dayan. Estimation of nonnormalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005. A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 4651-4664. PMLR, 2021. V. Jayaram and J. Thickstun. Parallel and flexible sampling from autoregressive models via langevin dynamics. In International Conference on Machine Learning, pages 4807-4818. PMLR, 2021. L. Kaiser, S. Bengio, A. Roy, A. Vaswani, N. Parmar, J. Uszkoreit, and N. Shazeer. Fast decoding in sequence models using discrete latent variables. In International Conference on Machine Learning, pages 2390-2399. PMLR, 2018. T. Karras, M. Aittala, T.",
    "contdiffu-37": "Aila, and S. Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022. J. Kasai, J. Cross, M. Ghazvininejad, and J. Gu. Nonautoregressive machine translation with disentangled context transformer. In International Conference on Machine Learning, pages 5144-5155. PMLR, 2020a. J. Kasai, N. Pappas, H. Peng, J. Cross, and N.",
    "contdiffu-38": "A. Smith. Deep encoder, shallow decoder: Reevaluating nonautoregressive machine translation.",
    "contdiffu-39": "arXiv preprint arXiv:2006.10369, 2020b. P. Kidger. On Neural Differential Equations. PhD thesis, University of Oxford, 2021. Y. Kim and A. M. Rush. Sequence-level knowledge distillation.",
    "contdiffu-40": "arXiv preprint arXiv:1606.07947, 2016. D. Kingma, T. Salimans, B. Poole, and J. Ho. Variational diffusion models. Advances in neural information processing systems, 34:21696-21707, 2021.",
    "contdiffu-41": "D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.",
    "contdiffu-42": "D. P. Kingma and P. Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018.",
    "contdiffu-43": "D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. X. Kong, Z. Zhang, and E. Hovy. Incorporating a local translation mechanism into non-autoregressive translation. arXiv preprint arXiv:2011.06132, 2020. T. Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66-75, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: $10.18653 / v 1 /$ P18-1007. T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.",
    "contdiffu-44": "arXiv preprint arXiv:1808.06226, 2018. S. Kumar and W. Byrne. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 169-176, Boston, Massachusetts, USA, May 2 - May 7 2004. Association for Computational Linguistics. J. Lee, E. Mansimov, and K. Cho. Deterministic nonautoregressive neural sequence modeling by iterative refinement. arXiv preprint arXiv:1802.06901, 2018.",
    "contdiffu-45": "X. L. Li, J. Thickstun, I. Gulrajani, P. Liang, and T. B. Hashimoto. Diffusion-lm improves controllable text generation. arXiv preprint arXiv:2205.14217, 2022. C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpmsolver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.",
    "contdiffu-46": "arXiv preprint arXiv:2206.00927, 2022a. C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b.",
    "contdiffu-47": "C. Meng, K. Choi, J. Song, and S. Ermon. Concrete score matching: Generalized score matching for discrete data. arXiv preprint arXiv:2211.00802, 2022. T. M\u00fcller, B. McWilliams, F. Rousselle, M. Gross, and J. Nov\u00e1k. Neural importance sampling. ACM Transactions on Graphics (TOG), 38(5):1-19, 2019.",
    "contdiffu-48": "A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162-8171. PMLR, 2021. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318, 2002.",
    "contdiffu-49": "E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville. Film: Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. K. Pillutla, S. Swayamdipta, R. Zellers, J. Thickstun, S. Welleck, Y. Choi, and Z. Harchaoui. Mauve: Measuring the gap between neural text and human text using divergence frontiers.",
    "contdiffu-50": "In NeurIPS, 2021.",
    "contdiffu-51": "M. Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191, Belgium, Brussels, Oct. 2018. Association for Computational Linguistics. O. Press and L. Wolf. Using the output embedding to improve language models. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 157-163, Valencia, Spain, Apr. 2017. Association for Computational Linguistics.",
    "contdiffu-52": "J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R.",
    "contdiffu-53": "Ring, S. Young, et al. Scaling language models: Methods, analysis \\& insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P.",
    "contdiffu-54": "J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer.",
    "contdiffu-55": "arXiv e-prints, 2019. A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image\ngeneration with clip latents.",
    "contdiffu-56": "arXiv preprint arXiv:2204.06125, 2022. M. Reid, V. J. Hellendoorn, and G. Neubig. Diffuser: Discrete diffusion via edit-based reconstruction, 2022.",
    "contdiffu-57": "D. J. Rezende and F. Viola. Generalized elbo with constrained optimization, geco. In Workshop on Bayesian Deep Learning, NeurIPS, 2018.",
    "contdiffu-58": "D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International conference on machine learning, pages 1278-1286. PMLR, 2014. P. H. Richemond, S. Dieleman, and A. Doucet. Categorical sdes with simplex diffusion. arXiv preprint arXiv:2210.14784, 2022. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022. C. Saharia, W. Chan, S. Saxena, and M. Norouzi. Non-autoregressive machine translation with latent alignments. arXiv preprint arXiv:2004.07437, 2020. C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet, and M. Norouzi. Palette: Image-toimage diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1-10, 2022a. C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S.",
    "contdiffu-59": "K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi, R.",
    "contdiffu-60": "G. Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b. N. Savinov, J. Chung, M. Binkowski, E. Elsen, and A. v. d. Oord. Step-unrolled denoising autoencoders for text generation. arXiv preprint arXiv:2112.06749, 2021. A. Shih, D. Sadigh, and S. Ermon. Training and inference on any-order autoregressive models the right way. arXiv preprint arXiv:2205.13554, 2022. J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256-2265. PMLR, 2015. Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. Y. Song, J. Sohl-Dickstein, D.",
    "contdiffu-61": "P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. Y. Song, C. Durkan, I. Murray, and S. Ermon. Maximum likelihood training of score-based diffusion models. Advances in Neural Information Processing Systems, 34:1415-1428, 2021a. Y. Song, C. Meng, R. Liao, and S. Ermon. Accelerating feedforward computation via parallel nonlinear equation solving. In International Conference on Machine Learning, pages 9791-9800. PMLR, 2021b. M. Stern, W. Chan, J. Kiros, and J. Uszkoreit. Insertion transformer: Flexible sequence generation via insertion operations. In International Conference on Machine Learning, pages 5976-5985. PMLR, 2019. R. Strudel, C. Tallec, F. Altch\u00e9, Y. Du, Y. Ganin, A. Mensch, W. Grathwohl, N. Savinov, S. Dieleman, L. Sifre, and R. Leblond. Self-conditioned embedding diffusion for text generation, 2022.",
    "contdiffu-62": "J. Su, Y. Lu, S. Pan, B.",
    "contdiffu-63": "Wen, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding.",
    "contdiffu-64": "arXiv preprint arXiv:2104.09864, 2021. H. Sun, L. Yu, B. Dai, D. Schuurmans, and H. Dai. Score-based continuous-time discrete diffusion models. arXiv preprint arXiv:2211.16750, 2022.",
    "contdiffu-65": "A. Tamkin, D. Jurafsky, and N. Goodman. Language through a prism: A spectral approach for multiscale language representations. Advances in Neural Information Processing Systems, 33:5492-5504, 2020. A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. A. van den Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals, K. Kavukcuoglu, G. Driessche, E. Lockhart, L. Cobo, F. Stimberg, et al. Parallel wavenet: Fast high-fidelity speech synthesis. In International conference on machine learning, pages 3918-3926. PMLR, 2018. G. Van Rossum and F. L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA, 2009. ISBN 1441412697. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. P. Vincent. A connection between score matching and denoising autoencoders.",
    "contdiffu-66": "Neural computation, 23 (7):1661-1674, 2011.",
    "contdiffu-67": "P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. Jarrod Millman, N. Mayorov, A.",
    "contdiffu-68": "R. J. Nelson, E. Jones, R. Kern, E. Larson, C. Carey, 1. Polat, Y. Feng, E. W. Moore, J. Vand erPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and S. . . Contributors. Scipy 1.0: Fundamental algorithms for scientific computing in python.",
    "contdiffu-69": "Nature Methods, 2020. A. Wang and K. Cho. Bert has a mouth, and it must speak: Bert as a markov random field language model. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 30-36, 2019.",
    "contdiffu-70": "H. Xia, T. Ge, F.",
    "contdiffu-71": "Wei, and Z. Sui. Lossless speedup of autoregressive translation with generalized aggressive decoding. arXiv preprint arXiv:2203.16487, 2022. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020. J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B.",
    "contdiffu-72": "K. Ayan, et al. Scaling autoregressive models for content-rich text-toimage generation.",
    "contdiffu-73": "arXiv preprint arXiv:2206.10789, 2022. ## A. Fitting the unnormalised CDF\n\n## A.1. Parameterisation\n\nTo implement time warping (see \u00a73.3), we fit a monotonically increasing function to the expected loss at each timestep. It is essential to parameterise this function in a way that is easy to normalise and invert, so we can use it for inverse transform sampling. We take inspiration from Durkan et al. (2019); M\u00fcller et al. (2019) and parameterise this function by dividing both the input and output range into bins. For convenience, we will assume that the normalised CDF maps the interval $[0,1]$ to $[0,1]$. In practice, timesteps $t$ range from $t_{\\min }$ to $t_{\\max }$ (with $t_{\\max }>t_{\\min }>0$ ), but we can simply shift and scale them:\n\n$$\nt^{\\prime}:=\\frac{t-t_{\\min }}{t_{\\max }-t_{\\min }}\n$$\n\nThe normalised CDF $u=F\\left(t^{\\prime}\\right)$ is then parameterised using two sets of logits $l_{n}^{t}$ and $l_{n}^{u}, n=1, \\ldots, N$. Applying the softmax nonlinearity to both sets yields two partitions of the unit interval, which we use to define $N$ input and output regions. The sizes of the input and output bins are:\n\n$$\nw_{n}^{t}=\\operatorname{softmax}_{n}\\left(l_{n}^{t}\\right), w_{n}^{u}=\\operatorname{softmax}_{n}\\left(l_{n}^{u}\\right)\n$$\n\nThe left edges of the input and output bins are found as the cumulative sum of the sizes of all preceding bins:\n\n$$\ne_{1}^{t}=0, e_{n}^{t}=\\sum_{m=1}^{n-1} w_{m}^{t}, \\forall n>1\n$$\n\nand similar for $e_{n}^{u}$. The right edges are found by adding the bin sizes to the left edges. Within each region, the CDF can be evaluated by linearly interpolating the bin edges. The result is a piecewise linear function which is guaranteed to be monotonically increasing. We find that enforcing the minimal bin size to be nonzero helps ensure numerical stability, which we implement by adding a small constant to all bin sizes $w_{n}^{t}$ and $w_{n}^{u}$, and renormalising. By initialising both sets of logits $l_{n}^{t}$ and $l_{n}^{u}$ to a constant value, $F\\left(t^{\\prime}\\right)$ initially represents the identity function, which is the CDF of the uniform distribution on the unit interval. We use the constant $-\\log N$, so that the unnormalised CDF $\\tilde{F}\\left(t^{\\prime}\\right)$ is in fact normalised at the start of training. ## A.2. Fitting without normalisation\n\nWe now wish to fit $u=F\\left(t^{\\prime}\\right)$ so that it has the same shape as the expected loss as a function of time $t^{\\prime}$. However, $F\\left(t^{\\prime}\\right)$ is normalised so that $u \\in[0,1]$, whereas the expected loss will vary between 0 and $H$, the unigram entropy of the data. For an unconditional diffusion model, we can estimate said entropy remarkably accurately from a single batch of training data, simply by counting the frequencies of all tokens in the vocabulary to estimate the marginal token distribution. We could then use this entropy estimate to scale the loss values to the unit interval. Unfortunately, this does not work for conditional models, because in that case, the unigram entropy varies with the conditioning. It turns out that we can easily define an unnormalised function $\\tilde{F}\\left(t^{\\prime}\\right)$ using the same parameterisation: instead of applying the softmax nonlinearity to\nthe output logits $l_{n}^{u}$, we can apply the exponential function to find the bin sizes:\n\n$$\nw_{n}^{t}=\\operatorname{softmax}_{n}\\left(l_{n}^{t}\\right), w_{n}^{u}=\\exp \\left(l_{n}^{u}\\right)\n$$\n\nThe bin edges are now free to assume any positive real value, rather than being constrained to the unit interval. $\\tilde{F}\\left(t^{\\prime}\\right)$ can therefore be fit to the observed loss values directly. The result of this is shown in Figure 2 (top) for a fully trained model. To ensure that we fit the expectation of the loss at each timestep, we minimise the mean squared error (MSE) loss with respect to the per-sequence loss values observed during training. Once fit, we can obtain the normalised function $F\\left(t^{\\prime}\\right) \\propto \\tilde{F}\\left(t^{\\prime}\\right)$ by applying the softmax nonlinearity to the output logits instead of the exponential function, as before. ## A.3. Inverse and derivative\n\nApart from being trivial to normalise, it is also very easy to invert the learnt CDF, simply by switching the roles of the input and output logits $l_{n}^{t}$ and $l_{n}^{u}$. We can also easily evaluate the derivative of the CDF (i.e. the probability density function), which is piecewise constant: within each bin, it is equal to the ratio of the output and input bin size, $w_{n}^{u} / w_{n}^{t}$ (Figure 2, middle). ## A.4. Importance weighting\n\nSince we are using this fitting mechanism to change the distribution of sampled timesteps over the course of training, we have created a feedback loop, because the training data for $F\\left(t^{\\prime}\\right)$ will itself become biased towards oversampled timesteps over the course of training. To compensate for this, we can use importance weights, which correspond to the reciprocal of the derivative of the CDF (or equivalently, the derivative of the inverse CDF). We take care not to backpropagate gradients through the time warping operation and the importance weights. For additional stability, we use an exponential moving average of the parameters $l_{n}^{t}$ and $l_{n}^{u}$ when performing the warping, to limit the rate of change of the training distribution, though empirically we have found that this is not strictly necessary. ## A.5. Warping sampling timesteps\n\nFor sampling, we find that using steps that are linearly spaced in uniform time, and subsequently warped, works considerably better than using uniformly spaced timesteps. Using warping for both training (to change the distribution of noise levels on the fly) and sampling (to space the timesteps nonlinearly) yields the best results.",
    "contdiffu-74": "We hypothesise that decreasing the entropy at a constant rate from step to step is also a useful heuristic for sampling. ## A.6. Temperature and uniformity\n\nThe piecewise linear parameterisation also enables some useful distribution manipulations: we can easily change the temperature $T$ of the distribution represented by the CDF, by changing the output bin sizes:\n\n$$\nw_{n}^{\\prime u} \\propto w_{n}^{u} \\cdot\\left(\\frac{w_{n}^{u}}{w_{n}^{t}}\\right)^{T^{-1}-1}\n$$\n\nTo see why this corresponds to a temperature change, recall that the PDF is piecewise constant, and the values it assumes (up to a normalisation constant) are given by:\n\n$$\n\\frac{w_{n}^{\\prime u}}{w_{n}^{t}} \\propto \\frac{w_{n}^{u}}{w_{n}^{t}} \\cdot\\left(\\frac{w_{n}^{u}}{w_{n}^{t}}\\right)^{T^{-1}-1}=\\left(\\frac{w_{n}^{u}}{w_{n}^{t}}\\right)^{T^{-1}}\n$$\n\nSimilarly, we can also derive the CDF of a mixture of the distribution with a uniform distribution, with mixture weight $\\mu$ :\n\n$$\nw_{n}^{\\prime \\prime u}=(1-\\mu) \\cdot w_{n}^{u}+\\mu \\cdot w_{n}^{t}\n$$\n\nNote that when the corresponding input and output bin sizes are all equal to each other (i.e. $w_{n}^{u}=w_{n}^{t}, \\forall n$ ), we always obtain the identity function, which corresponds to the CDF of the uniform distribution on the unit interval.",
    "contdiffu-75": "The effect of these manipulations is visualised in Figure 8. ## B. Negative results\n\nWe informally discuss some failed attempts to improve our results. This information is provided to help understand some of our design choices, and to aid researchers and practitioners who are interested in using these methods or investigating ways to improve them. Bearing in mind that context and details significantly impact experimental results, especially in machine learning research, we expressly do not wish to discourage anyone from pursuing the ideas described here. ## B.1. Constraining the embedding parameters\n\nInstead of normalisation, the embedding parameters can also be prevented from growing uncontrollably by adding a regularisation loss term. We experimented\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_11c9bceac3b6de560e63g-26.jpg?height=940&width=797&top_left_y=291&top_left_x=213)\n\nFigure $8 \\mid$ Effect of tuning the temperature (top) and the uniformity (bottom) of the time warping function from Figure 2. with L2 regularisation, margin penalties (Dieleman et al., 2021; Donahue et al., 2019), as well as automatic adaptation of the L2 penalty weight to target unit variance for the embedding vector components (Dieleman et al., 2021; Rezende and Viola, 2018), but found that normalisation works best. ## B.2. Removing the time dependency\n\nIn an effort to simplify the model architecture and make it resemble BERT even more strongly, we investigated models without time embedding (i.e. we removed the time embedding MLP in Figure 3). Since the model is solving a classification task, our hypothesis was that it could simply infer the noise level from the noisy input vectors, instead of requiring explicit knowledge of the timestep $t$. Unfortunately this significantly hurt performance. We suspect that this is a result of the relative scale of the embeddings and the noise (which varies greatly across noise levels), as well as the rescaling we apply to the noisy embeddings to ensure unit variance (see \u00a76.1). ## B.3. Simplex diffusion\n\nWe started out by lifting discrete token sequences into the space of categorical distributions over tokens, and performing diffusion in that space. Categorical distributions are nonnegative real-valued vectors whose components sum to 1 , so they live on the simplex. We explored a tractable formulation of diffusion on the simplex, based on the Cox-Ingersoll-Ross process (Cox et al., 1985), as described by Richemond et al. (2022). We found that language modelling with this process is impeded by the uneven nature of the corruption process, which is a consequence of the high dimensionality of the simplex (corresponding to the number of tokens in the vocabulary $V$ ). In practice, the noise distribution is heavy-tailed and introduces frequent outliers, which make the corrupted vectors look like they correspond to the wrong tokens, even at very low noise levels. We were not able to circumvent this issue even with powerful Transformer models. We hypothesise that mitigating this issue requires modifying the corruption process so that it does not operate independently on all components. Formulating such a correlated process with a tractable transition density is non-trivial however, and this would complicate the model to a certain degree. Both score interpolation (\u00a73.1) and time warping (\u00a73.3) were originally developed in the context of simplex diffusion, but we found these ideas to be more effective in combination with Gaussian diffusion in a Euclidean embedding space. [^0]:    ${ }^{1}$ Although several works have explored generative approaches based on masked language models (Ghazvininejad et al., 2019; Goyal et al., 2021; Shih et al., 2022; Wang and Cho, 2019), they were originally introduced and are still mainly used for representation learning. [^1]:    ${ }^{2}$ We use denoising score matching in practice (Vincent, 2011). ${ }^{3}$ See Appendix B. 1 of Karras et al. (2022). [^2]:    ${ }^{4}$ Some approaches attempt to mitigate this by allowing some proportion of tokens to be resampled multiple times (Savinov et al., 2021). ${ }^{5}$ At least, not in the traditional sense; Tamkin et al. (2020) suggest an approach to obtain and analyse multi-scale representations of language. [^3]:    ${ }^{6}$ Strictly speaking, it is possible to decouple the cost of sampling from the sequence length even for autoregressive models, using probability density distillation (van den Oord et al., 2018) or alternative sampling algorithms (Jayaram and Thickstun, 2021; Song et al., 2021b), but these approaches have not been used for language models, to the best of our knowledge. [^4]:    ${ }^{7}$ We note that L2-normalisation of the expectation resembles the calculation of the mean direction of a von Mises-Fisher distribution. [^5]:    ${ }^{8}$ In the limit of a model that perfectly approximates the data distribution, the prediction entropy and the cross-entropy should be the same in expectation. In practice, this is a good enough approximation even for undertrained models. [^6]:    ${ }^{9}$ The cross-entropy is significantly higher than zero at $t=1.0$ for $w=16$, for example. [^7]:    ${ }^{10}$ Note that the suitability of higher-order samplers is known to be highly dependent on other sampling hyperparameters such as the guidance scale (Lu et al., 2022b). ${ }^{11}$ Deterministic samplers have some important benefits over stochastic ones, such as support for manipulations in latent space. [^8]:    ${ }^{12}$ Nucleus sampling is a misnomer in this case, because we never actually sample from the categorical distribution, but we can still use the same strategy to shape the logits. [^9]:    ${ }^{13}$ These numbers are not directly comparable to those reported in Tables 1 and 2, which were obtained with a model trained on a different dataset. [^10]:    ${ }^{14}$ The signature is nrefs:1|case:mixed|eff:noltok:13a|smooth:exp|version:2.3.1 when English is the target language, and nrefs:1|case:mixed|eff:no|tok:intl|smooth:exp|version:2.3.1 for German. [^11]:    ${ }^{15}$ Infilling is also possible with autoregressive models if they are specifically trained for this task (Bavarian et al., 2022). "
}