{
    "mechdesignscaling-0": "# Mechanistic Design and Scaling of Hybrid Architectures \n\nMichael Poli ${ }^{*, 1,7}$, Armin W Thomas ${ }^{*, 2,7}$, Eric Nguyen*,2, ${ }^{*, 2}$<br>Pragaash Ponnusamy ${ }^{1}$, Bj\u00f6rn Deiseroth ${ }^{3}$, Kristian Kersting ${ }^{3}$, Taiji Suzuki ${ }^{4}$,<br>Brian Hie ${ }^{2,5}$, Stefano Ermon ${ }^{2,6}$, Christopher R\u00e9 ${ }^{2}$, Ce Zhang ${ }^{1}$, Stefano Massaroli ${ }^{4}{ }^{4}$<br>${ }^{1}$ Together AI, ${ }^{2}$ Stanford University, ${ }^{3}$ Hessian AI, ${ }^{4}$ RIKEN, ${ }^{5}$ Arc Institute, ${ }^{6} \\mathrm{CZ}$ Biohub, ${ }^{7}$ Liquid AI\n\n\n#### Abstract\n\nThe development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at computeoptimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology. ## 1 Introduction\n\nAlongside data quality, the effectiveness of large-scale training is determined by the quality of a model architecture [1, 2], which is defined by the set and arrangement of the computational primitives used to form layers and functional blocks, as well as their parametrization. Due to the combinatorial explosion of possible architecture designs and a lack of reliable prototyping pipelines - despite progress on automated neural architecture search methods [3] - architectural improvements are obtained through an opaque development process guided by heuristics and individual experience, rather than systematic procedures. Further adding to this issue are the large costs and long iteration times associated with training and testing new architectures, underscoring the need for principled and nimble design pipelines. In spite of the wealth of possible architecture designs, the majority of models rely on variations of the same uniform Transformer recipe, based on a regular interleaving of memory-based mixers (self-attention layers) with memoryless mixers (shallow FFNs) [4, 5]. This particular combination of computational primitives originating from the first Transformer design [6] - is known to improve quality, with empirical arguments supporting the notion that these primitives specialize in different sequence modeling sub-tasks e.g., incontext versus factual recall [7]. Beyond the Transformer architecture are a class of emerging computational primitives inspired by signal processing, based on gated convolutions and recurrences $[8,9,10,11,12$, 13], promising improved quality, cheaper scaling to long sequence length, and efficient inference. These new primitives expand the architecture design space, offering new opportunities to extend capabilities and specializations of models. In this work, we set out to explore key questions arising from these observations:\n\n1. Can the architecture design process be streamlined through a set of simple pretext token manipulation tasks, providing quick and cheap performance estimates predictive of scaling laws? 2. Is it possible to bring together the \"best of all worlds\" by arranging different computational primitives into hybrid architecures, leveraging their respective specialized capabilities? In an attempt to provide answers to these questions, we make the following core contributions:\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-02.jpg?height=918&width=1492&top_left_y=207&top_left_x=281)\n\nFigure 1.1: Mechanistic architecture design (MAD) is a framework to enable fast iterative improvement of architectures, including emerging approaches based on recurrences and convolutions. [A]: Design architectures via selection of computational primitives and topology. [B]: MAD involves an evaluation of architecture designs at small scale on a set of token manipulation synthetic tasks, curated to unit test a variety of model capabilities. The experimental setup promotes direct comparison via normalization of total state dimension for recurrent models. [C]: Validate scaling laws of top-performing models on MAD synthetics in compute-optimal and overtrained regimes. Results in B used to reduce the number of candidate architectures. [D]: Verify alignment of scaling properties and (MAD) results for each architecture e.g., correlation of compute-optimal scaling perplexity and aggregate (MAD) score (in the figure, compute-optimal perplexity at 2 e 19 FLOP budget is shown). If the scores between target quantity and (MAD) synthetics are correlated, iterate on a single target architecture. Mechanistic architecture design We introduce a methodology for the fast prototyping and testing of new architectures, mechanistic architecture design (MAD). MAD is a collection of synthetic tasks - such as recall, memorization, and compression - curated to serve as isolated unit tests for key capabilities of an architecture, requiring only minutes of training time. In particular, MAD tasks are inspired by progress on understanding the inner workings of Transformers and other sequence models via in-context learning, recall, and other sequence manipulation tasks $[14,15,16,17,18]$. We apply MAD to test architectures built with representative computational primitives such as gated convolutions [10], gated input-varying linear recurrences [12, 13], and other operators e.g., mixture of experts (MoEs) [19], as well as novel ones. With MAD, we are able to filter for promising architecture candidates (Fig, 1.1, [A,B]). By identifying which individual tasks computational primitives excel at, we find and validate several ways to improve designs, such as striping i.e., sequentially interleaving blocks composed of different computational primitives with a specified interconnection topology, resulting in hybrid architectures [20, 15, 21]. Scaling laws of emerging architectures To investigate the link between MAD synthetics and real-world scaling, we execute the largest scaling law analysis on emerging architectures to date, training over 500 language models between 70 million and 7 billion parameters with different architectures. Our protocol builds and expands on compute-optimal scaling laws for LSTMs and Transformers [1, 22, 2]. Our findings show that hybrid architectures improve on all scaling measures, resulting in lower pretraining losses at different floating point operation (FLOP) compute-budgets at the compute-optimal frontier ${ }^{1}$. We also verify new architectures to be more robust to large pretraining runs outside the efficient frontier e.g., smaller models trained for significantly more tokens, which make up a majority of training settings in practice due to inference cost considerations [23]. [^1]Hybridization insights at scale Building on our scaling law analysis, we investigate hybridization schedules and model topology. Our findings uncover optimal hybridization ratios for attention [6], Hyena [10], and Mamba [12] mixtures, as well as the respective placement of these layers in an architecture. State-optimal scaling laws The size of the state - the analog of $k v$-caches in standard Transformers [24] of emerging convolutional and recurrent primitives [10, 12] plays a central role in MAD and our scaling analysis, as it determines inference efficiency, memory cost, and provably has a direct effect on recall capabilities [17]. We introduce a state-optimal scaling analysis, with the objective of estimating how perplexity scales with the state dimension of different model architectures. We find hybrid architectures to balance the trade-off between compute requirements, state dimension, and perplexity. New state-of-the-art architectures Leveraging MAD and new computational primitives, derived from the insights developed in this work, we design new state-of-the-art hybrid architectures, outperforming the best Transformer, convolutional, and recurrent baselines (Transformer++ [4], Hyena, Mamba) with a reduction of up to $20 \\%$ in perplexity for the same compute budget. Correlation between synthetics and scaling performance Finally, we provide the first evidence that a curated selection of MAD synthetic tasks can be used to reliably predict scaling law performance, paving the way to faster, automated architecture design. In particular, MAD accuracy is rank-correlated with compute-optimal perplexity at scale (Fig. 1.1, $[\\mathrm{D}]$ ), with particularly strong correlation for models in the same architecture class (Fig 5.1). ## 2 Background: Architecture Design\n\nArchitecture design refers to the selection and optimization of (a) computational primitives and their composition into layers and blocks, and (b) topology i.e., the interconnection and placement of individual blocks in an architecture. In the following, we define the bounds of the architecture design search space explored in this work. In particular, we provide details on the emerging class of implicit subquadratic models, since their properties drive the design of the synthetic task and evaluation pipeline in MAD, and motivate the introduction of a state-optimal scaling law analysis. ### 2.1 Computational primitives\n\nArchitectures are compositions of linear and nonlinear functions with learnable parameters. Common choices for the former are parametric dense or structured layers $\\mathrm{L}: \\mathbb{R}^{T} \\rightarrow \\mathbb{R}^{T}, y=\\mathrm{L}(u)$. As an example,\n\n$$\n\\begin{array}{lll}\n\\text { dense } & y_{t}=\\sum_{t^{\\prime}=1}^{T} \\mathrm{~W}_{t t^{\\prime}} u_{t^{\\prime}}, & \\mathrm{W} \\in \\mathbb{R}^{T \\times T} \\\\\n\\text { (causal) conv. } & y_{t}=\\sum_{t^{\\prime}=1}^{t} \\mathrm{~W}_{t-t^{\\prime}} u_{t^{\\prime}}, & \\mathrm{W} \\in \\mathbb{R}^{T} . \\end{array}\n$$\n\nIt is often useful to differentiate between explicitly and implicitly parametrized layers, depending on whether the entries $\\mathrm{W}_{t t^{\\prime}}$ are the learnable parameters of the layer or are themself parametric functions of positional encodings or of the input, i.e. $\\left(t, t^{\\prime}, u\\right) \\mapsto \\mathrm{W}_{t t^{\\prime}}(u)$ [10]. Implicit parametrizations disentangle the number of model parameters and dimensionality $T$ of the inputs. Further, they can be leveraged to create complex dependencies on the inputs in the entries of $\\mathrm{W}(u)$ such as in self-attention, $\\mathrm{W}_{t t^{\\prime}}(u)=\\sigma\\left(\\left\\langle Q u_{t}, K u_{t^{\\prime}}\\right\\rangle\\right)$. This ensures the layer can be applied to inputs with large $T$ without a prohibitive parameter and memory cost. We often refer to the implicit parametrization for an implicit layer as its featurization path. On nonlinearities in architecture design Linear primitives are typically interconnected via nonlinearities and residuals. Common nonlinearities are applied elementwise or to some specific dimension (e.g., the softmax used in attention). [25, 6]. Another commonly employed nonlinearity is gating, resulting in a polynomial function of the input. While other lines of work investigate choice and placement of nonlinearities in a layer to optimize quality, efficiency, or to minimize the emergence of activation outliers [26], these quality\nimprovements are smaller compared to other layer and topology changes ${ }^{2}$ and are thus outside the scope of this work. Implicit primitives Implicitly parametrized computational primitives are the backbone of most model architectures of practical interest. An important class of implicit layers can be described starting from so-called linear attention $[8,27,28]^{3}$, in its simplest (single-channel, unnormalized ${ }^{4}$ ) form\n\n$$\n\\begin{array}{lrl}\n\\text { recurrence } & x_{t+1} & =x_{t}+k_{t}(u) v_{t}(u) \\\\\n\\text { readout } & y_{t} & =q_{t}(u) x_{t}\n\\end{array}\n$$\n\nwhere $q, k, v: \\mathbb{R}^{T} \\rightarrow \\mathbb{R}^{T}$ are the featurization path of the layer. Linear attention is a linear recurrent neural network (RNN) or state-space model (SSM) with constant identity state-to-state dynamics, and implicitlyparametrized input-to-state and state-to-output mappings. Linear attention can be evaluated in parallel during training or inference prefilling using its parallel form $y_{t}=q_{t} \\sum_{t^{\\prime}=1}^{t} k_{t^{\\prime}} v_{t^{\\prime}}$, without materializing the state $x$. Notably, the class of subquadratic implicit models [10, 12, 13] emerges as generalizations of (2.1) with a few key differences. ### 2.2 State, cache, and memory\n\nIn autoregressive tasks, such as text generation, recurrent models enable lower latency and constant memory generation, since the fixed state $x_{t}$ replaces the cache required in other generic nonlinear blocks such as attention e.g., the $k v$-cache. Indeed, $k v$-caches can be seen as a state of dynamic size, by reformulating attention as a recurrence with state size $T$, see [24]. For this reason, we use fixed states and dynamic states to refer to states and $k v$-caches in hybrid architectures. Nonparametric state expansion tricks The size of the state and its utilization play a central role in the taxonomy, analysis, and design of efficient architectures. State size, as well as the parametrization of a block, determine memorization and recall capabilities of a layer, as well as inference efficiency. For this reason, different approaches have been developed to expand the state dimension without prohibitive parameter cost. The main ones are the outer-product head trick:\n\n$$\n\\begin{aligned}\nx_{t+1} & =x_{t}+\\left(k_{t} \\otimes I_{M}\\right) v_{t}, & k_{t}, v_{t}, q_{t} & \\in \\mathbb{R}^{M} \\\\\ny_{t} & =\\left(I_{M} \\otimes q_{t}\\right) x_{t}, & x_{t} & \\in \\mathbb{R}^{M^{2}} . \\end{aligned}\n$$\n\nNote that we have used a vectorized notation instead of the commonly employed matrix notation for models using the state expansion trick. This configuration linearly increases the state size from a head dimension $M$ to a total of $M^{2}$, and is employed in most linear attention variants [8], Hyena and RWKV variants $[24,9]$ as well as GLA [13]. The second method to expand the total number of states per layer is achieved via the multi single-input single-output (mSISO) layer configuration, which is equivalent to applying multiple independent recurrences with $M$ states in parallel. Given the importance of the total state dimension in determining the capacity of a layer, we find model comparisons in an iso-state setting - normalizing for the total number of states regardless of the specifics of the layer - to be required to ensure architecture improvements measured on smaller scale synthetic tasks can transfer to pretraining results at scale. Manipulating the state Beyond state expansion techniques, efficient layers can be taxonomized based on their parametrization of state-to-state dynamics and their implicit parameters. For example, an inputvarying layer introduces additional featurization path to extend input-variance to state-to-state transitions e.g., $x_{t+1}=g_{t}(u) x_{t}+k_{t}(u) v_{t}(u)$. We choose three state-of-the-art approaches spanning different possible combinations:\n\n| Hyena [10] <br> Multi-Head Hyena [24] | weakly input-varying <br> weakly input-varying | mSISO <br> mSISO with heads |\n| :---: | :---: | :---: |\n| Gated Linear Attention [13] | input-varying | heads |\n| Mamba [12] | input-varying | mSISO and weight sharing ${ }^{6}$ |\n\n[^2]The layers also vary slightly in their featurization paths e.g., GLA uses a low-rank elementwise implicit state-to-state transition, whereas Mamba uses a different low-rank parametrization and weight-tying. ### 2.3 Topology\n\nBeyond the specifics of the layer itself, designing architectures involves arranging these computational primitives into blocks, interconnected with a particular topology, for example, sequential, parallel, or hybrid (as illustrated in Fig. 1.1). In this work, we explore sequential striped topologies i.e., where different computational primitives are applied sequentially, as well as sparse parallel topologies i.e., mixture of experts. ## 3 Mechanistic Architecture Design\n\nIn the ideal case, we would have access to an oracle capable of quantifying how changes in model design at the microscopic level - choice of computational primitives, parametrization, topology - propagate to the macroscopic scale i.e., scaling laws. Indeed, a key challenge in architecture design is predicting whether new designs will match or improve quality of existing baselines at scale. Our working hypothesis is that the performance of an architecture primarily stems from its efficiency in performing an array of smaller token manipulation tasks well. We show that by probing the performance of architectures in each of these individual tasks at a small scale, one can recover relative model rankings matching those obtained via scaling laws analysis in quantities of interest such as compute-optimal perplexity. We call this process of capability identification and evaluation, with the goal of architecture prototyping, mechanistic architecture design (in short \"MAD\"). Beyond approximating scaling performance, MAD provides a means to probe the compositionality of model skills. ### 3.1 Synthetic tasks to probe model skills\n\nMAD utilizes synthetic tasks to probe model skills and inform model design, building on recent works [15, $10,17]$ considering only a single or subset of these tasks. We provide a schematic for each task, with $x$ representing the input, $y$ the target sequence, and prompt the evaluation sequence. ### 3.1.1 In-context recall\n\nTo answer a prompt well, language models must be able to understand and learn from new information presented in the prompt (so-called in-context learning [29]). A wealth of empirical work has demonstrated that the associative recall task, as studied in $[15,10]$, is well-suited to test a specific subset of in-context learning ability: direct lookup, requiring little to no processing of token embeddings to be solved ${ }^{7}$. Here, we are using a multi-query variant of this task, as proposed by [17]: Given an input sequence of key-value pairs, models are tasked with retrieving all values from the input sequence associated\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-05.jpg?height=144&width=646&top_left_y=1593&top_left_x=1159)\n\nFigure 3.1: Schematic of in-context recall. White tokens are masked; $y$ represents target sequences during training. At test time, the model is evaluated on recall of all key-value pairs that were already presented in the sequence. with keys that were already shown in the input sequence. Note that while the mapping from keys to values is consistent within an input sequence, it is randomly shuffled between sequences. To solve this task, a model thereby does not need to learn any information external to the prompt it is provided with at test time. [^3]\n### 3.1.2 Fuzzy in-context recall\n\nIn language, semantic units are often spread out over multiple adjacent tokens (e.g., \"blue sky\" vs \"gray sky\"). To test how capable a model is of semantically grouping together adjacent tokens, we utilize a variant of in-context recall, in which keys and values are composed of a variable number of adjacent tokens. For each sequence, variable length keys and values are randomly drawn from the vocabulary and then assigned into pairs. Since the structure of key/value lengths in a sequence, as well as the mapping from keys to values, change between sequences, fuzzy recall can be regarded as a more challenging variant of in-context recall. ### 3.1.3 Noisy in-context recall\n\nTo answer a prompt well, language models must be able to ignore irrelevant information of the input. We test this ability with another modification to standard in-context recall. Here, irrelevant information, represented by noise tokens from a special subset of the vocabulary, is added in an arbitrary and variable pattern in between the key-value pairs. Since the noise tokens are sampled from a fixed dictionary, this task requires the model to implement a specific type of memory, in addition to the recall circuits required for in-context recall. In particular, the model needs to remember which tokens belong to the set of noise tokens, as these do not carry relevant information for the task. ### 3.1.4 Selective Copying\n\nIn addition to ignoring irrelevant information of an input, language models must be able to selectively remember relevant information of an input. In the selective copying task, models are tasked with copying tokens from one position of an input sequence to a later position of the sequence, while ignoring irrelevant noise tokens that are inserted into the sequence. Tokens are always copied in their order of occurrence. Models thereby need to not just remember the tokens that are to be copied but also their specific order of occurrence in the sequence. The copy positions are gleaned from the structure of each sample, while the contents change between samples and must be inferred in-context. ### 3.1.5 Compression\n\nRecent findings in the mechanistic interpretability literature [30] indicate that language models are often required to perform \"token concatenation\", where early sequencemixing layers (e.g., attention) assemble information that is spread across multiple tokens in an input onto another token so that the assembled information can then be decoded well by subsequent channel-mixing layers (e.g., MLPs). To test this capability we use a compression task, in which models are tasked with compressing a random sequence of input tokens into a single aggregation token, in a way that enables reconstruction via an MLP. In other words, the compression task tests the ability of a model to compress token embeddings into a single one with the least amount of information loss. ### 3.1.6 Memorization\n\nIn addition to manipulating and retrieving information from an input sequence, language modeling requires the memorization of factual knowledge. To test this skill, we utilize a memorization task, in which models are tasked with learning a fixed key-value mapping (resembling facts in language) from the training data. Unlike recall, the mapping requires no in-context computation as the ground-truth mapping is constant across samples. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-07.jpg?height=143&width=532&top_left_y=294&top_left_x=1207)\n\nFigure 3.6: Schematic of the memorization task. The model is tasked with learning a fixed map between tokens (i.e., a set of \"facts\"). ### 3.2 MAD Protocol\n\nMAD follows a two-step procedure, starting from the design of a new candidate architecture, followed by its systematic evaluation according to the following key principles:\ni. Each MAD score is obtained by averaging architecture performances across a range of task difficulty levels. To manipulate difficulty, we independently vary a set of relevant experimental variables: length of the input sequence, size of the vocabulary, and size of the training set. Some tasks have additional variables such as the ratio of noise tokens in the noisy recall and selective copying tasks (Appendix B. 1 and B.5). ii. Fixed-state architectures are normalized to an iso-state and iso-parameter setting, including models featuring sparsely activated layers such as mixtures of experts (MoEs) [19]. Here, we normalize all fixed-state architectures to a common total state dimension of 4096 to control for any differences in model performance driven primarily by mismatch in model state dimension (Appendix B.3). iii. To ensure that model performance estimates are not dependent on a specific training setting, we sweep each architecture in each task setting over a grid of learning rate and weight decay values. We only include the best runs in our final analysis (Appendix B.4). $i v$. Model performances are always evaluated in an independent evaluation dataset, specific to each task setting. An implementation of the MAD tasks are available at https:/github.com/athms/mad-lab. ### 3.3 Candidate architecture designs\n\nWe apply MAD to a set of small two-blocks architectures built from a collection of common primitives such as attention, SwiGLU [31], and variants of efficient implicit recurrent and convolutional layers described in Sec. 2.2. We build different types of architectures with these primitives: sequential, striped, and sparse parallel (mixtures). In total, we evaluate 21 distinct architectures, including combinations of the primitives described in Sec.",
    "mechdesignscaling-1": "2. Additional architecture details are provided in (Appendix B). Mixture of Sequence Experts We further introduce to our MAD analysis a layer inspired by sparsely gated channel mixers, the Hyena experts layer. In a Hyena experts layer with $E$ experts and $K$ active experts, a router selects from a set of smaller Hyena mixers, using a router $G(u): u \\mapsto s$ from input sequence $u \\in R^{T \\times D}$ to scores $s \\in R^{T \\times K}$, defined as\n\n$$\ns_{t}=\\operatorname{softmax}\\left(\\operatorname{top}_{K}\\left(u_{t} \\mathrm{~W}_{g}\\right)\\right), \\quad \\mathrm{W}_{g} \\in \\mathbb{R}^{D \\times E}, \\quad t=1, \\ldots, L\n$$\n\nresulting in\n\n$$\n\\text { HyenaExperts }(u)_{t}=\\sum_{k^{\\prime}=1}^{k} s_{t k^{\\prime}} \\text { Hyena }(u)_{t k^{\\prime}}\n$$\n\nAn advantage of the Hyena experts layer is that only a subset of the total state dimension is used to compose the output at each time step. We note that sparsely gated recurrences have also been explored for recurrences in [32], and that other similar schemes for sparse gating at the state level are also possible using input-varying recurrent primitives. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-08.jpg?height=766&width=1545&top_left_y=244&top_left_x=240)\n\nFigure 3.7: MAD analysis: An extensive evaluation of a suite of model architectures, built from common sequence- and channel-mixing layer types, across six synthetic tasks, each designed to probe a specific skill relevant for sequence modeling at scale. ### 3.4 Results\n\nWe test a suite of architectures in the MAD protocol. In addition to ranking overall model performances across the synthetic tasks (Fig. 3.7), we take a high-level view on general patterns in model performances related to their design, including the presence of specific computational primitives in an architecture and the architecture's topology. We indicate a model's performance by its accuracy in correctly predicting tokens in the synthetic tasks. Note that model performances in MAD can likewise be measured through their evaluation loss (see Appendix B.1). Both performance metrics yield similar model rankings. Hybridization to combine specialized layers Inspecting the performance on individual tasks via a stratified analysis (Appendix B.5) reveals specialization of architectures built with a single type of primitive, such as Mamba excelling at compression and Hyena at fuzzy recall. Finding 1: Striped architectures outperform all non-striped architectures on composite metrics, with an average gain in accuracy of $8.1 \\%$ across the MAD synthetic tasks (Fig. 3.7). We further find MAD performance to increases with models' total fixed state dimension, underscoring the importance of normalizing state dimensions when comparing model capabilities, further motivating a state-optimal scaling law analysis (Fig. 4.3). Head expansion trick It is beneficial to arrange the fixed state dimension into larger heads with fewer states instead of smaller heads with additional states (in the limit case, in a mSISO configuration). Finding 2: Architectures that expand their total state dimension through heads (see Sec. 2.2) outperform architectures without heads, with an average gain of $2.3 \\%$ in accuracy across the MAD synthetic tasks (Fig. 3.7). We note that the head expansion trick also linearly increases the computation in the layer, and for this reason it introduces a trade-off between compute-optimality and state-optimality. In Sec. 4, we will explore the trade-offs of this state configuration by comparing compute-optimal and state-optimal scaling of models with and without heads. Sparse layers We find sparsely gated layers to outperform dense layers in MAD synthetics, in line with the literature on mixture of experts and their benefits. Finding 3: MAD performance improves with the addition of sparsely activated mixture of expert channel-mixing layers, when compared to architectures using SwiGLU channel mixers, with an average gain in accuracy of $1.7 \\%$ across tasks (Fig. 3.7). In our later analyses, we will connect the performance of architectures on MAD to their performance at scale on The Pile [33] (Fig. 5.1). Additional MAD analysis results are provided in Appendix B.5. ## 4 Scaling Analysis\n\nWe seek to verify the connection between mechanistic design tasks and performance at scale. For this reason, we execute an extensive scaling law analysis on language pretraining, expanding on the framework of [1, 2]. We train more than 500 models of different architectures on The Pile [33]. Let $\\mathcal{M}_{w, \\xi}$ be a model with parameters $w$ and architecture $\\xi$. Denote with $N=|w|$ the number of parameters, with $D$ the total number of training tokens, and the training cost (in floating point operations, FLOPS) with $c_{\\xi}(N, D)$. Let $\\mathcal{A}_{\\xi}(C)$ be the set of tuples $(N, D)$ such that the training cost is exactly $C$, $\\mathcal{A}_{\\xi}(C):=\\left\\{(N, D) \\mid c_{\\xi}(N, D)=C\\right\\}$. Given a tuple $(N, D) \\in \\mathcal{A}_{\\xi}(C)$ one can evaluate $\\mathcal{L}_{\\xi}(N, D)$, the loss achievable for that combination of parameters/tokens. A point $(C, \\ell(C))$ in the locus of the compute-optimal frontier in the loss-compute plane is defined as\n\n$$\n(C, \\ell(C)): \\ell(C)=\\min _{(N, D) \\in \\mathcal{A}_{\\xi}(C)} \\mathcal{L}_{\\xi}(N, D)\n$$\n\nwith $\\ell(C)$ indicating the best loss achievable by training $\\mathcal{M}_{\\theta, \\xi}$ at compute budget $C$, optimizing the allocation of compute to model size $N$ and training tokens $D$, for architecture $\\xi$. Relatedly, one may seek the functional form of the compute-optimal frontier in the parameter-compute or token-compute planes, composed of tuples $\\left(C, N^{*}\\right)$ and $\\left(C, D^{*}\\right)$, where $D^{*}, N^{*}$ represent the optimal i.e., achieving lowest loss, allocation subject to the $\\left(N^{*}, D^{*}\\right) \\in \\mathcal{A}_{\\xi}(\\mathcal{C})$ constraint. A primary objective of scaling law analyses is to determine such optimal allocation of the computational budget. To estimate efficient frontiers, we use an IsoFLOP approach, which explores different allocation ratios of model parameters and number of tokens at each compute budget. The loss optimum is then estimated via a quadratic fit (see Fig 4.2 as an example). ### 4.1 Compute-optimal frontier for new architectures\n\nOur first set of findings is related to the efficient frontier of the baseline Transformer++ [4] in relation to other architectures. [2] finds that when $\\xi$ is a standard Transformer architecture (combining attention and MLP), the optimal ratios between the number or model parameters, training tokens, and compute budget, are explained by a linear relationship in $\\log$-log space, i.e., $\\log N^{*} \\propto a \\log C$ and $\\log D^{*} \\propto b \\log C$. Finding 5: Let $a_{\\mathrm{H}}, a_{\\mathrm{T}}, b_{\\mathrm{H}}, b_{\\mathrm{T}}$ be the parameter size and data allocation coefficients for striped and Transformer models, respectively. We estimate $a_{\\mathrm{T}}>a_{H}$ and $b_{\\mathrm{T}}<b_{H}$ (Fig. 4.1). Optimal allocation of tokens and parameters is relatively stable under striping, with marginal differences. One notable difference is that optimal compute allocation in emerging efficient architectures is skewed towards additional data i.e., training smaller models for longer. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-09.jpg?height=568&width=1676&top_left_y=1944&top_left_x=235)\n\nFigure 4.1: Compute optimal scaling. [Top:] For each architecture, we train models of different sizes for a constant number of FLOPs (so-called IsoFLOP groups). For each of these IsoFLOP groups, we determine an optimum model size based on a polynomial fit to the observed training perplexities. [Bottom:] Using these estimates, we predict optimal model sizes and number of training tokens for each architecture. Beyond the efficient frontier Next, we look at optimality gaps when training outside the efficient frontier. By optimality gap, we refer to the increase in loss by training outside the compute-optimal frontier i.e., $\\mathcal{L}(C(\\tilde{N}, \\tilde{D}, \\xi))$ where $\\tilde{N}=N^{*}+\\delta N^{*}$ and the number of tokens $\\tilde{D}$ is adjusted to preserve the total compute cost. Finding 6: The off compute-optimal perplexity gap is proportional to the hybridization ratio (Fig.4.2), for all IsoFLOP groups. Intuitively, models with \"flatter\" IsoFLOP perplexity curves are preferred for overtraining smaller models, a setting particularly common in practice, as it results in smaller models with faster inference. Interestingly, the suboptimality gap in hybrids is smaller than Transformers, meaning they are better suited to training outside the optimal frontier. Striping schedule and topology We study compute-optimal ratio and allocation of attention operators in striped architectures, as well as their overall topology (Fig. D.1). Finding 7: The compute-optimal hybridization ratio for striped models is $25 \\%$ across all IsoFLOP groups ${ }^{8}$ (Fig.4.2 and Table D.1). ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-10.jpg?height=374&width=1595&top_left_y=1023&top_left_x=237)\n\nFigure 4.2: Optimal striping ratio. We find that StripedHyena architectures outperform non-striped Hyena ( $0 \\%$ Attention) and Transformer++ ( $100 \\%$ Attention) architectures across all evaluated FLOPS groups. In particular, we find a ratio of $25 \\%$ to be optimal. Batch sizes and hyperparameters Batch size and learning rate are two high-impact hyperparameters for scaling laws, as they visibly shift the compute-efficient frontier. We find that scaling the batch size with FLOP budgets, thus keeping it fixed within each IsoFLOP group, to be a simple and robust approach.",
    "mechdesignscaling-2": "Fig. C. 1 provides an example of potential issues arising from incorrect batch scaling. These results are in line with recent findings [34]. ### 4.2 State-optimal scaling\n\nBeyond driving MAD synthetics performance, the total state size in a model is also an important factor in determining inference latency and memory cost. We explore state-optimal scaling, aiming to provide a coarse estimate of state utilization by measuring scaling in perplexity over state dimension (Fig. 4.3, right). Finding 8: There exists a relation of the type $P^{*} \\propto M^{c}$ between compute-optimal perplexity $P^{*}$ and total state size $M$, with $c \\approx-0.28$ in our scaling experimental setup, consistent across all model architectures. The model class determines the offset of the state-optimal curve. Concretely, state-optimal scaling indicates that one may reach any target perplexity (up to saturation of compute-optimal scaling laws i.e., approaching entropy of text) with fixed-state architectures, by paying a FLOP cost multiplier that depends on the model class - training longer to maximize state utilization. Input-varying recurrences, multihead and striped hybrid architectures achieve a favourable trade-off between metrics, with comparable or improved compute-optimal perplexity to Transformers++ and a reduced total state dimension. ### 4.3 Compute-optimal scaling at byte resolution\n\n[^4]![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-11.jpg?height=718&width=1634&top_left_y=201&top_left_x=217)\n\nFigure 4.3: Compute-optimal and state-optimal scaling on The Pile. We report total state dimension, fixed (recurrences) and dynamic (attention). All models are trained at sequence length 8k. We identify distinct regions in the state-optimal frontier, indicating that one may pay an additional FLOP cost to obtain the same perplexity with a state of smaller dimension, by using other classes of architectures. Scaling laws analysis primarily focus on sub-word level tokenization. With a new range of architectural options, we also explore computeoptimal scaling of a subset of architectures (Transformer++, Mamba, Hyena and StripedHyena) at byte resolution. We scale the models across FLOP budgets from 8 e 18 to 8 e 19 with model sizes from 6 M to 1 B parameters. The compute-optimal frontier is obtained using a similar protocol as outlined in Sec.",
    "mechdesignscaling-3": "C, with additional details and results shown in Sec. D.2. We find attention-based models to yield significantly higher perplexity at all IsoFLOP groups, with alternative architectures outperforming Transformer++, including non-striped variants (Figure 4.4). These results show that model ranking varies significantly across domains and tokenization strategies. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-11.jpg?height=467&width=589&top_left_y=1183&top_left_x=1262)\n\nFigure 4.4: Compute-optimal scaling at byte resolution. ## 5 Connecting MAD to scaling metrics\n\nThe goal of MAD is to provide a framework that can accelerate the architecture design process by using small synthetic tasks, which can be evaluated quickly and with little compute, to estimate whether improvements to an existing architecture, or a new candidate architecture, will perform well at scale. To gauge this hypothesis, we study the correlation between MAD scores and scaling properties of interest. Correlation to compute-optimal perplexity We start with a case study using the Hyena [10] architecture. MAD has indicated that the performance of Hyena can be cumulatively improved by i) adding heads to the Hyena sequence mixer, ii) interleaving Hyena and attention layers, iii) using a sparse MoE channel mixer instead of SwiGLU, and iv) integrating a sparse routing mechanism into the Hyena sequence mixer (Fig.",
    "mechdesignscaling-4": "3.7). Using the results of our scaling analysis (Sec. 4), we can investigate the correlation between the MAD scores of these architectures, as indicated by their average accuracy across the synthetic tasks, and their compute-optimal performance on The Pile (Fig. 5.1 left). We also consider perplexity on MAD tasks as an additional metric (Appendix B.5). Finding 9: Aggregate MAD scores are linearly correlated with compute-optimal perplexity at scale for all compute budgets (Fig. 5.1 left, Appendix B.8). This result suggests that smaller, shallower models unit tested on MAD synthetics can be used to predict compute-optimal scaling, as well as to iterate on improvements to a base architecture. To better understand the contribution of each MAD task to the predictive power of the scores, we also report correlation for single-\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-12.jpg?height=546&width=1591&top_left_y=180&top_left_x=238)\n\nFigure 5.1: Improved performance on MAD synthetics correlates with better compute-optimal perplexity on The Pile. We highlight progressively improved versions of Hyena that were designed with the MAD pipeline, which translated to improved perplexity on the Pile (shown for $2 e 19$ FLOPs; see Appendix B. 8 for an analysis across IsoFLOP groups). task performances and compute-optimal perplexity at scale (Fig. 5.1 right). We note that there is very little variation in the architectures' performances on the memorization task, which could explain why we did not find an association between their performances on this task and their performances at scale. Next, we replicate this analysis for the Mamba architecture [12], comparing the base architecture to a striped hybrid variant (Appendix B.9). Again, improved performances on MAD correlate with improved compute-optimal perplexity on The Pile, underlining the generalizability of MAD. Correlation across different architecture classes, although present (see Fig. 1.1), is subject to more noise. Improvements to the pipeline and selection of evaluation settings MAD may be required to minimize the impact of spurious hyperparameters. Extensions and limitations The MAD evaluation framework relies on extrapolating performance from smaller (e.g., 2-block) models to deeper models trained at scale. As such, the framework has not yet been applied to sophisticated topologies requiring small-scale testing with a larger number of blocks e.g., hybrid models with more than two sequence mixer primitives, or alternative interconnection topologies that span multiple layers. In principle, MAD can be used to design architectures to optimize other quantities of interest, beyond perplexity or downstream benchmarks e.g., throughput. In this work, we focus on investigating correlation with compute-optimal scaling metrics, and leave other analyses to future work. ## 6 Conclusion\n\nThis work explores architecture optimization, from synthetic tasks designed to probe specific model capabilities to scaling laws. We introduce mechanistic architecture design (MAD), a methodology for fast prototyping and verification of new deep learning architectures based on key token manipulation tasks such as recall and compression. With MAD, we identify hybridization and new configurations to improve compute-optimal scaling of new architectures. We carry out an extensive scaling law analysis of new architectures, training over 500 models between parameter sizes of 70 M to 7 B , verifying the improvements found via MAD, and derive a collection of novel insights on the optimal scaling of new architectures. We introduce state-optimal scaling as a measure of efficiency for blocks with a fixed-size state, with implications for inference memory and latency. Finally, we show how MAD results are correlated with perplexity in a compute-optimal regime, paving the way for faster and cheaper architecture prototyping. Overall, this work provides evidence of correlation between scaling and a selection of synthetic token manipulation tasks, as well as of the existence of a variety of hybrid architectures improving over Transformers at scale and on individual tasks. ## 7 Ethical Impact\n\nThis paper introduces mechanistic architecture design (MAD), a methodology for improving the scaling performance of deep learning models, and presents several improved architectures. As a consequence of this line of work, we expect training and inference of large models to become more efficient, less expensive, and thus more readily available. Societal consequences related to the existence of large foundation models based on Transformers also apply when discussing new improved architectures. ## 8 Acknowledgments\n\nWe are grateful to the Hessian.AISC Service Center, funded by the Federal Ministry of Education and Research (BMBF), for the collaboration and joint use of their supercomputer forty-two. ## References\n\n[1] Jared Kaplan et al. \"Scaling laws for neural language models\". In: arXiv preprint arXiv:2001.08361 (2020) (cit. on pp. 1, 2, 9, 16). [2] Jordan Hoffmann et al. \"Training compute-optimal large language models\". In: arXiv preprint arXiv:2203.15556 (2022) (cit.",
    "mechdesignscaling-5": "on pp. 1, 2, 9, 16). [3] Colin White et al. \"Neural architecture search: Insights from 1000 papers\". In: arXiv preprint arXiv:2301.08727 (2023) (cit.",
    "mechdesignscaling-6": "on pp. 1, 16). [4] Hugo Touvron et al. \"Llama 2: Open foundation and fine-tuned chat models\". In: arXiv preprint arXiv:2307.09288 (2023) (cit.",
    "mechdesignscaling-7": "on pp. 1, 3, 9). [5] Albert Q Jiang et al. \"Mistral 7B\". In: arXiv preprint arXiv:2310.06825 (2023) (cit. on p. 1). [6] Ashish Vaswani et al. \"Attention is all you need\". In: Advances in neural information processing systems 30 (2017) (cit. on pp. 1, 3, 19). [7] Mor Geva et al. \"Dissecting recall of factual associations in auto-regressive language models\". In: arXiv preprint arXiv:2304.14767 (2023) (cit.",
    "mechdesignscaling-8": "on p. 1). [8] Angelos Katharopoulos et al. \"Transformers are rnns: Fast autoregressive transformers with linear attention\".",
    "mechdesignscaling-9": "In: International conference on machine learning. PMLR. 2020, pp. 5156-5165 (cit. on pp. 1, 4). [9] Bo Peng et al. \"RWKV: Reinventing RNNs for the Transformer Era\". In: arXiv preprint arXiv:2305.13048 (2023) (cit.",
    "mechdesignscaling-10": "on pp. 1, 4). [10] Michael Poli et al. \"Hyena hierarchy: Towards larger convolutional language models\". In: arXiv preprint arXiv:2302.10866 (2023) (cit.",
    "mechdesignscaling-11": "on pp. 1-5, 11, 16, 19, 29). [11] Eric Nguyen et al. \"Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution\". In: arXiv preprint arXiv:2306.15794 (2023) (cit.",
    "mechdesignscaling-12": "on p. 1). [12] Albert Gu and Tri Dao. \"Mamba: Linear-time sequence modeling with selective state spaces\". In: arXiv preprint arXiv:2312.00752 (2023) (cit.",
    "mechdesignscaling-13": "on pp. 1-4, 12, 16, 19, 29, 30). [13] Songlin Yang et al. \"Gated Linear Attention Transformers with Hardware-Efficient Training\". In: arXiv preprint arXiv:2312.06635 (2023) (cit.",
    "mechdesignscaling-14": "on pp. 1, 2, 4, 19). [14] Catherine Olsson et al. \"In-context learning and induction heads\". In: arXiv preprint arXiv:2209.11895 (2022) (cit.",
    "mechdesignscaling-15": "on pp. 2, 16). [15] Daniel Y Fu et al. \"Hungry hungry hippos: Towards language modeling with state space models\". In: arXiv preprint arXiv:2212.14052 (2022) (cit.",
    "mechdesignscaling-16": "on pp. 2, 5, 16). [16] Satwik Bhattamishra et al. \"Understanding in-context learning in transformers and llms by learning to learn discrete functions\". In: arXiv preprint arXiv:2310.03016 (2023) (cit.",
    "mechdesignscaling-17": "on pp. 2, 16). [17] Simran Arora et al. \"Zoology: Measuring and Improving Recall in Efficient Language Models\". In: arXiv preprint arXiv:2312.04927 (2023) (cit.",
    "mechdesignscaling-18": "on pp. 2, 3, 5, 16). [18] Ekin Aky\u00fcrek et al. \"In-Context Language Learning: Architectures and Algorithms\". In: arXiv preprint arXiv:2401.12973 (2024) (cit. on pp. 2, 16). [19] Noam Shazeer et al. \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\". In: arXiv preprint arXiv:1701.06538 (2017) (cit. on pp. 2, 7). [20] Xuezhe Ma et al. \"Mega: moving average equipped gated attention\". In: arXiv preprint arXiv:2209.10655 (2022) (cit.",
    "mechdesignscaling-19": "on p. 2). [21] Mahan Fathi et al. \"Block-State Transformer\". In: arXiv preprint arXiv:2306.09539 (2023) (cit.",
    "mechdesignscaling-20": "on p. 2). [22] Aleksandar Stani\u0107 et al. \"The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute\". In: arXiv preprint arXiv:2309.11197 (2023) (cit. on p. 2). [23] Nikhil Sardana and Jonathan Frankle. \"Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws\". In: arXiv preprint arXiv:2401.00448 (2023) (cit.",
    "mechdesignscaling-21": "on pp. 2, 28). [24] Stefano Massaroli et al. \"Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions\". In: arXiv preprint arXiv:2310.18780 (2023) (cit. on pp. 3-5, 19, 29). [25] Zhouhan Lin et al. \"A structured self-attentive sentence embedding\". In: ar Xiv preprint arXiv:1703.03130 (2017) (cit. on p. 3). [26] David R So et al. \"Primer: Searching for efficient transformers for language modeling\". In: arXiv preprint arXiv:2109.08668 (2021) (cit.",
    "mechdesignscaling-22": "on p. 3). [27] Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. \"Linear transformers are secretly fast weight programmers\".",
    "mechdesignscaling-23": "In: International Conference on Machine Learning. PMLR. 2021, pp. 9355-9366 (cit. on p. 4). [28] Weizhe Hua et al. \"Transformer quality in linear time\". In: International Conference on Machine Learning. PMLR. 2022, pp. 9099-9117 (cit. on p. 4). [29] Nelson Elhage et al. \"A mathematical framework for transformer circuits\". In: Transformer Circuits Thread 1 (2021) (cit.",
    "mechdesignscaling-24": "on p. 5). [30] Neel Nanda et al. \"Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level\". In: Alignment Forum (2023) (cit.",
    "mechdesignscaling-25": "on p. 6). [31] Noam Shazeer. \"Glu variants improve transformer\". In: arXiv preprint arXiv:2002.05202 (2020) (cit. on pp. 7, 19). [32] Liliang Ren et al. \"Sparse modular activation for efficient sequence modeling\". In: Advances in Neural Information Processing Systems 36 (2024) (cit. on p. 7). [33] Leo Gao et al. \"The pile: An 800 gb dataset of diverse text for language modeling\".",
    "mechdesignscaling-26": "In: arXiv preprint arXiv:2101.00027 (2020) (cit.",
    "mechdesignscaling-27": "on p. 9). [34] Xiao Bi et al. \"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism\". In: arXiv preprint arXiv:2401.02954 (2024) (cit.",
    "mechdesignscaling-28": "on p. 10). [35] Gail Weiss, Yoav Goldberg, and Eran Yahav. \"On the practical computatifonal power of finite precision RNNs for language recognition\". In: arXiv preprint arXiv:1805.04908 (2018) (cit.",
    "mechdesignscaling-29": "on p. 16). [36] John Hewitt et al. \"RNNs can generate bounded hierarchical languages with optimal memory\". In: arXiv preprint arXiv:2010.07515 (2020) (cit. on p. 16). [37] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural odes\". In: Advances in neural information processing systems 32 (2019) (cit. on p. 16). [38] Stefano Massaroli et al. \"Dissecting neural odes\". In: Advances in Neural Information Processing Systems 33 (2020), pp.",
    "mechdesignscaling-30": "3952-3963 (cit. on p. 16). [39] Albert Gu, Karan Goel, and Christopher R\u00e9. \"Efficiently modeling long sequences with structured state spaces\". In: arXiv preprint arXiv:2111.00396 (2021) (cit.",
    "mechdesignscaling-31": "on p. 16). [40] Michael Zhang et al. \"Effectively modeling time series with simple discrete state spaces\". In: arXiv preprint arXiv:2303.09489 (2023) (cit.",
    "mechdesignscaling-32": "on p. 16). [41] Dmitry Lepikhin et al. \"Gshard: Scaling giant models with conditional computation and automatic sharding\". In: arXiv preprint arXiv:2006.16668 (2020) (cit.",
    "mechdesignscaling-33": "on p. 19). [42] Biao Zhang and Rico Sennrich. \"Root mean square layer normalization\". In: Advances in Neural Information Processing Systems 32 (2019) (cit. on p. 28). [43] Jianlin Su et al. \"Roformer: Enhanced transformer with rotary position embedding\". In: Neurocomputing 568 (2024), p.",
    "mechdesignscaling-34": "127063 (cit. on p. 29). ## MECHAnistic DESIGn And SCALing OF HYBRId ARCHITECTURES <br> Supplementary Material\n\n## Contents\n\n1 Introduction ..... 1\n2 Background: Architecture Design ..... 3\n2.1 Computational primitives ..... 3\n2.2 State, cache, and memory ..... 4\n2.3 Topology ..... 5\n3 Mechanistic Architecture Design ..... 5\n3.1 Synthetic tasks to probe model skills ..... 5\n3.1.1 In-context recall ..... 5\n3.1.2 Fuzzy in-context recall ..... 6\n3.1.3 Noisy in-context recall ..... 6\n3.1.4 Selective Copying ..... 6\n3.1.5 Compression ..... 6\n3.1.6 Memorization ..... 7\n3.2 MAD Protocol ..... 7\n3.3 Candidate architecture designs ..... 7\n3.4 Results ..... 8\n4 Scaling Analysis ..... 9\n4.1 Compute-optimal frontier for new architectures ..... 9\n4.2 State-optimal scaling ..... 10\n4.3 Compute-optimal scaling at byte resolution ..... 10\n5 Connecting MAD to scaling metrics ..... 11\n6 Conclusion ..... 12\n7 Ethical Impact ..... 12\n8 Acknowledgments ..... 13\nA Additional Related Work ..... 16\nB Mechanistic Architecture Design ..... 17\nB. 1 Tasks ..... 17\nB.1.1 In-Context Recall ..... 17\nB.1.2 Fuzzy In-Context Recall ..... 17\nB.1.3 Noisy In-Context Recall ..... 17\nB.1.4 Selective Copying ..... 17\nB.1.5 Compression ..... 18\nB.1.6 Memorization ..... 18\nB. 2 Manipulating Task Difficulty ..... 18\nB. 3 Architectures ..... 19\nB.3.1 Channel-mixing Layers ..... 19\nB.3.2 Sequence-mixing Layers ..... 19\nB. 4 Training ..... 19\nB. 5 Results ..... 19\nB.5.1 Task Performances ..... 19\nB.5.2 Performance on Individual Tasks ..... 21\nC Scaling Laws ..... 28\nC. 1 Training Details ..... 28\nC. 2 Model architectures ..... 28\nC. 3 Model sizes and training hyperparameters ..... 29\nC. 4 FLOP calculation ..... 29\nC.4.1 Transformer++ ..... 29\nC.4.2 Hyena ..... 30\nC.4.3 Multi-Head Hyena ..... 30\nC.4.4 StripedHyena ..... 31\nC.4.5 Mamba ..... 31\nC.4.6 StripedMamba ..... 31\nC.4.7 StripedHyena-MoE ..... 32\nC.4.8 StripedHyena Experts +MoE ..... 32\nD Extended Scaling Results ..... 33\nD. 1 Optimal hybridization topologies ..... 33\nD. 2 Byte-level scaling laws ..... 34\n\n## A Additional Related Work\n\nSynthetics for analysis and design The MAD framework builds on work on synthetic tasks for mechanistic interpretability of RNNs and Transformers, including associative recall, reasoning tasks, compression. [14] and a number of follow up in mechanistic interpretability use an induction task to probe into the internals of Transformer model. There is a large body of work [35,36] studying the expressivity of recurrent models, either theoretically or empirically, using formal languages and other token manipulation tasks. Smaller scale synthetics have been used during the iterative design procedure of new layers and primitives, particularly in the context of emerging deep signal processing architecture. [37, 38, 39, 15, 40, 10, 17]. Notably, [15] uses associative recall to identify a key capability gap in previous gated state-space models, and proposes a modification to the layer. [10] extend associative recall procedure to longer sequences, introducing new synthetic tasks such as counting. However, the pretraining results only involve smaller models, and are not obtained via compute-optimal scaling. There exists a long line of work on neural architecture search methods (see [3] for a review). MAD provides a different approach based on synthetic tasks. MAD metrics are in principle compatible with various search methods. Synthetics for evaluation Synthetics have also been leveraged to evaluate models and model classes [17, 16, 18]. [10] shows correlation between synthetics and pretraining results on The Pile. [17] maps associative recall accuracy gaps to a perplexity gap between pretrained models. A variety of other analyses on synthetics for emerging architectures finds certain classes of efficient architectures to be on par or outperform Transformers on most tasks, with gaps on tasks involving heavy recall or copying of tokens. With MAD, we aim to leverage tasks as unit tests with a quantitative connection to scaling properties, instead of using smaller-scale experiments to only build intuition on potential model differences. Scaling laws We extend the compute-optimal scaling law analysis protocol of [1, 2] performed on Transformers to deep signal processing architectures, including hybrids and sparsely gated architectures. We base the scaling analysis in this work on the compute-optimal protocol, in order to evaluate relative performance and to identify optimal hybridization ratios. Moreover, we consider extensions such as state-optimal scaling and performance in overtrained regimes (outside the compute-optimal frontier), both of which have implications for efficient inference. Other work on evaluation of new architectures experiments in parameter-matched and data-matched regimes, which can result in a mismatch with scaling results due to different FLOP costs per iteration. Other notable examples of compute-matched evaluations for new models are provided in [10, 12]. Previous evaluations are not carried out at compute-optimal model sizes which can vary significantly across architectures see e.g., Figures 4.1 and D.3a). ## B Mechanistic Architecture Design\n\n## B. 1 Tasks\n\n## B.1.1 In-Context Recall\n\nThe in-context recall task is comprised of sequences of key-value pairs (with separate vocabularies for keys and values). Models are tasked with predicting all values for those keys that were already presented in the sequence:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-17.jpg?height=52&width=573&top_left_y=585&top_left_x=750)\nIn this example, keys are drawn from the vocabulary $\\{a, d, f\\}$ and values from the $\\{b, e, g\\}$ vocabulary. Importantly, the mapping from keys to values is randomly shuffled between sequences. Models are tasked with autoregressively predicting all underlined value in this example. In the baseline setting of this task, we use a vocabulary of 16 tokens and 12,800 training sequences with a length of 128 tokens. The vocabulary is equally divided into keys and values. ## B.1.2 Fuzzy In-Context Recall\n\nThe fuzzy in-context recall tasks adapts the in-context recall task by representing keys and values by a variable number of adjacent tokens:\n\n```\ninput example: (a d) (b) (d a f) (e g) | (d a f) (e g)\n```\n\nIn this example, keys are drawn from the vocabulary $\\{\\mathrm{a}, \\mathrm{d}, \\mathrm{f}\\}$ and values are drawn from the vocabulary $\\{b, e, g\\}$. We use brackets for illustrative purposes to indicate adjacent tokens that together represent a key or value but they are not part of the actual input to the model. In sequential order, the presented keys are 'a d' and 'd a f', with associated values 'b' and 'e g'. For each sequence, keys and values are randomly drawn form the key and value dictionaries, with randomly drawn lengths (ranging from 1 to 3 tokens in our analyses). We always evaluate with keys of length 3 (the longest length used in our analyses), to disambiguate whenever a key token appears in two keys of different values. We pad sequences with a separate pad token if necessary to ensure that all sequences of a dataset are of the exact same length. As for the in-context recall task, models are tasked with autoregressively predicting all underlined values in this example. In the baseline setting of this task, we use a vocabulary of 16 tokens and 12,800 training sequences with a length of 128 tokens. The vocabulary is equally divided into key and value tokens. ## B.1.3 Noisy In-Context Recall\n\nThe noisy in-context recall task represents another variation of in-context recall, in which noise tokens, from a separate vocabulary, are randomly inserted into the input sequences:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-17.jpg?height=61&width=715&top_left_y=1877&top_left_x=676)\nIn this example, keys and values are respectively drawn from the vocabularies $\\{a, d, f\\}$ and $\\{b, e, g\\}$, while noise is drawn form the vocabulary $\\{\\mathrm{h}, \\mathrm{i}\\}$. As for in-context recall, models are tasked with autoregressively predicting the underlined values in this example. In the baseline setting of this task, we use a vocabulary of 16 tokens, which are equally divided into keys and values, 12,800 training sequences with a length of 128 tokens, and a share of $20 \\%$ noise tokens in the input from a separate noise vocabulary of size 16 . ## B.1.4 Selective Copying\n\nThe selective copying task comprises sequences of randomly sampled tokens, with randomly inserted \"blank\" and \"insert\" tokens:\n\n```\ninput example: a c [b] t [b] [i] [i] [b] [i] | a c [b] t [b] a c [b] t\n```\n\nIn this example, tokens are drawn from the vocabulary $\\{\\mathrm{a}, \\mathrm{c}, \\mathrm{t}\\}$, while $[\\mathrm{b}]$ and $[\\mathrm{i}]$ indicate the blank and insert token. Given this example, the task of the model is to copy all non-special tokens to the positions of the insert tokens, in the order they were presented in the sequence. The purpose of the randomly inserted blank tokens is to force models to learn to selectively memorize or ignore information from the input. In the baseline setting of this task, models are tasked with copying 16 randomly drawn tokens from a vocabulary of 16 tokens, and are provided with 12,800 training sequences with a length of 256 tokens. ## B.1.5 Compression\n\nThe compression task consists of random token sequences, each ending with a dedicated \"compression token\":\n\n```\ninput example: a e c b h g i [c] | [c] + [poso] -> a\n```\n\nIn this example, tokens are randomly drawn from the vocabulary $\\{\\mathrm{a}, \\mathrm{b}, \\mathrm{c}, \\mathrm{e}, \\mathrm{g}, \\mathrm{h}, \\mathrm{i}\\}$, while $[\\mathrm{c}]$ indicates the compression token. Given this input, models are tasked with compressing all relevant sequence information into the compression token [c], such that a subsequent two-layer MLP can fully recover each token of the input sequence, given the model's output for the compression token. To indicate the position $i$ that is to be recovered from the input, we add a non-learnable sin-cos position embedding (indicated by $\\left[p_{i}{ }_{i}\\right]$ ) to the models output for the compression token before feeding it to the MLP decoder. In the baseline setting of this task, we use a vocabulary of 16 tokens and 12,800 training sequences with a length of 32 tokens. ## B.1.6 Memorization\n\nThe memorization task uses a fixed key-value dictionary, representing the facts to be learned:\nkey-value dictionary example: $\\{\\mathrm{a}: \\mathrm{b}, \\mathrm{c}: \\mathrm{d}, \\mathrm{e}: \\mathrm{f}\\}$\nInput sequences comprise key-value pairs that are randomly sampled form this dictionary. Importantly, all values are masked out form the input sequences with a dedicated \"insert token\":\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-18.jpg?height=44&width=1004&top_left_y=1280&top_left_x=526)\nIn this example, the values that are to be inserted at the positions of the insert tokens are: 'b', 'd', 'f', and 'b'. Models are then tasked with correctly inserting the masked-out values at the positions of the insert tokens. As the values are never part of the input sequences, models need to learn the mapping from keys to values over the course of their training. In the baseline setting of this task, we use a vocabulary of 256 tokens, equally divided into keys and values, and 256 training sequences with a length of 32 tokens (such that each fact is on average presented 32 times in the training data). ## B. 2 Manipulating Task Difficulty\n\nFor each MAD task, we evaluate model performances across several levels of difficulty. We manipulate task difficulty by i) increasing the length of the input sequences, ii) reducing the training dataset size, and iii) increasing the vocabulary size. In addition, we increase the share of noise in the inputs for the noisy incontext recall task as well as the number of tokens that are to be copied in the selective copying task. Importantly, we only change one task variable at a time, while keeping all others at their baseline level. For all variants of in-context recall, we evaluate input sequence lengths of 128, 256,512, and 1024 tokens, training dataset sizes with $12,800,6,400,3,200,1,600$ and 800 samples, and vocabulary sizes, which are equally divided into keys and values, of $16,32,64$, and 128 tokens. For noisy in-context recall, we additionally evaluate shares of $20 \\%, 40 \\%, 60 \\%$, and $80 \\%$ noise tokens in the inputs. For the selective copying task, we evaluate sequence lengths of 256,512 , and 1024 tokens, training dataset sizes with $12,800,6,400,3,200,1,600$ and 800 samples, vocabulary sizes of $16,32,64$, and 128 tokens, and $16,32,64$, and 96 tokens of a the input that are to be copied. For the compression task, we evaluate input sequence lengths of $32,64,128$ and 256 tokens, vocabulary sizes of $16,32,64$, and 128 tokens, and training dataset sizes of $12,800,6,400,3,200,1,600$ and 800 samples. For the memorization task, we evaluate vocabulary sizes of $256,512,1,024,2,048,4,096$, and 8,192 tokens, while keeping the training dataset fixed at 256 samples with an input length of 32 (thereby effectively varying the rate at which each fact appears in the training data, with average rates of $32,16,8,4,2$, and 1).",
    "mechdesignscaling-35": "## B. 3 Architectures\n\nWe build architectures from a set of common channel- and sequence-mixing layer primitives. Each architecture is composed of 2 blocks with a total of 4 layers. In general, blocks combine a sequence mixing layer with a subsequent channel mixing layer, with the exception of Mamba layers, which combine sequence and channel mixing into a single layer [12]. All layers are set to a width of 128 for our main analysis (if not stated otherwise), with all other architecture settings given below. Common architecture primitives are composed of two identical blocks combining each sequence-mixing layer with each of the two channel-mixing layers. Striped hybrid architectures combine each unique block of the common architecture primitives with a second block composed of multi-headed attention and one of the two channel mixers. ## B.3.1 Channel-mixing Layers\n\n- SwiGLU MLP [31]: inner width: 512\n- Mixture of Experts MLP [41]: number of experts: 8, expert width: 16, number of active experts: 2\n\n\n## B.3.2 Sequence-mixing Layers\n\nWe normalize the (fixed) state dimension of all sequence mixers, before running the MAD pipeline. Whenever possible, we prioritize keeping the shape of the layer fixed, over the state dimension (e.g., reducing state dimension before expansion factors, or reducting state dimension before number of heads). - Hyena [10]: filter order: 2, short filter order: 3, filter featurization is implemented following [24]. - Mamba [12]: state dimension: 4, convolution dimension: 4, width expansion: 2, no bias for linear and convolution layers. - Multi-head Gated Linear Attention [13]: number of heads: 8, head dimension: 16\n- Multi-Head Attention [6]: number of heads: 16, head dimension: 8, no bias for linear layers\n- Multi-Head Hyena [24]: number of heads: 16, state dimension of heads: 2, filter order: 2, short filter order: 3 . - Hyena Experts: number of experts: 8, expert width: 16, number of active experts: 2. All other parameters are shared with standard Hyena. At these settings, all evaluated architectures that do not include attention layers are normalized to a total state dimension of 4,096 . ## B. 4 Training\n\nFor each MAD task, we train models according to the setting described in Table B.1, using a standard crossentropy loss objective. Note that we sweep all evaluated architectures over a $3 \\times 2$ grid of learning rate and weight decay values (see Table B.1) and only include the best runs in our final analysis (as determined by their evaluation accuracy). ## B. 5 Results\n\n## B.5.1 Task Performances\n\nTable B.1: MAD training setting. | Optimizer | AdamW |\n| :---: | :---: |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.98$ |\n| Dropout | None |\n| Batch Size | 128 |\n| Training epochs | 200 |\n| Learning Rate sCHEDule | COSINE DECAY |\n| NUMBER Of LAYERS | 4 |\n| Number of evaluation Samples | 1,280 |\n| Base learning Rate | $[0.0001,0.0005,0.001]$ |\n| Weight decay | $[0.0,0.1]$ |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-20.jpg?height=1593&width=1614&top_left_y=931&top_left_x=219)\n\nFigure B.1: Architecture performances within and across the MAD synthetic tasks, when using evaluation accuracy as a performance metric (left) or evaluation loss (right). ## B.5.2 Performance on Individual Tasks\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-21.jpg?height=2142&width=1217&top_left_y=305&top_left_x=407)\n\nFigure B.2: In-context recall task model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H,A,Alg\\}: multiheaded model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-22.jpg?height=2139&width=1215&top_left_y=207&top_left_x=407)\n\nFigure B.3: Fuzzy in-context recall task model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H, A, Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-23.jpg?height=2140&width=1615&top_left_y=205&top_left_x=220)\n\nFigure B.4: Noisy in-context recall task model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H,A,Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-24.jpg?height=2145&width=1616&top_left_y=205&top_left_x=220)\n\nFigure B.5: Selective Copying model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H, A, Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-25.jpg?height=2137&width=1197&top_left_y=202&top_left_x=424)\n\nFigure B.6: Compression model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H,A,Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-26.jpg?height=2346&width=426&top_left_y=209&top_left_x=809)\n\nFigure B.7: Memorization model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H,A,Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-27.jpg?height=535&width=1592&top_left_y=497&top_left_x=237)\n\nFigure B.8: Improved performance on MAD synthetics correlates with better compute-optimal perplexity on The Pile across IsoFLOP groups. We highlight progressively improved versions of Hyena that were designed with the MAD pipeline.",
    "mechdesignscaling-36": "![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-27.jpg?height=537&width=1578&top_left_y=1770&top_left_x=238)\n\nFigure B.9: Replication of Fig. B. 8 for the Mamba and Striped Mamba architectures and IsoFLOP groups 8 e 18 and 2 e 19 . ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-28.jpg?height=492&width=690&top_left_y=197&top_left_x=683)\n\nFigure C.1: Increasing batch size with compute FLOPS can shift the compute-efficient frontier. When increasing batch size after $10^{9}$ parameters (red), the IsoFLOP curve underestimates the performance of larger models, when compared to a fixed batch size (blue), shifting the optimum estimation towards smaller models. ## C Scaling Laws\n\nWe design our model topologies starting from previous compute-optimal scaling results for Transformers [23], and selecting the number of layers (depth) and width to cover a range of parameters from $8 e 6$ to $7 e 9$ parameters (see Table C.2). The depth and width are generally fixed across models, which result in minor parameter count differences (except for the mixture of experts models where a distinction between total and active parameters must be made, see Tables C.4 and C.3). To compare how each model scales, we control for several compute budgets (IsoFLOP groups): $4 \\mathrm{e} 18,8 \\mathrm{e} 18,2 \\mathrm{e} 19,4 \\mathrm{e} 19,8 \\mathrm{e} 19,2 \\mathrm{e} 20,5 \\mathrm{e} 20,2 \\mathrm{e} 21$. We linearly interpolate learning rates from common settings at $150 e 6,350 e 6,1.3 e 9,3 e 9$ and $7 e 9$ model sizes, obtaining a linearly inverse relationship with model size. Batch size is scaled (increased) in discrete steps, with larger training FLOPs using larger batch sizes. For state-optimal scaling results, we obtain the optimal model size from the compute-optimal frontier, then compute the dynamic and fixed state dimensions of the closest model size available in the set of results. ## C. 1 Training Details\n\nWe control for key hyperparameters across all models, including batch size (Table C.1), learning rate (Table C.2) and scheduler. Most models were trained on a single node. For larger IsoFLOP groups, we trained in a multinode distributed training with tensor parallelism. We used a cosine decay learning rate scheduler, with warm up using $1 \\%$ the number of training steps, and the minimum decay to reach $10 \\%$ of the max learning rate. Table C.1: Batch sizes by IsoFLOP group. For very small models ( $<54 \\mathrm{M}$ ) parameters, batch size 262 k is used. | IsOFLOP | BATCH SizE |\n| :---: | :---: |\n| $4.0 \\mathrm{E}+18$ | 524 K |\n| $8.0 \\mathrm{E}+18$ | 524 K |\n| $2.0 \\mathrm{E}+19$ | 524 K |\n| $4.0 \\mathrm{E}+19$ | 524 K |\n| $8.0 \\mathrm{E}+19$ | 524 K |\n| $2.0 \\mathrm{E}+20$ | 1 M |\n| $5.0 \\mathrm{E}+20$ | 1 M |\n| $2.0 \\mathrm{E}+21$ | 2 M |\n\n## C. 2 Model architectures\n\nWe describe shared architecture details first, followed by model specific designs below. All models use a modern SwiGLU unit as the channel mixer, except for Mamba and StripedMamba (which merges the GLU block with the sequence mixer layer, resulting in twice the number of sequence mixers). We use RMSNorm [42] for normalization. All models tie the embedding layers. All sparsely activated layers use learned argmax routing. Transformer++ Transformer++ is state-of-the-art Transformer model, with rotary positional embeddings [43], SwiGLU and RMSNorm. Hyena We use the original architecture [10] with some improvements. The channel mixer is replaced with SwiGLU, we use RMSNorm, set weight decay to 0 to all Hyena layer parameters. Multi-Head Hyena We use a Hyena layer with heads as described by [24]. We sweep across different head dimensions at the IsoFLOP group 2 e 19 to find an optimal head dimension (8), and use the same number for all other experiments. StripedHyena We use 3 striping schedule ratios: $1 \\mathrm{~A}: 1 \\mathrm{H}, 1 \\mathrm{~A}: 3 \\mathrm{H}, 1 \\mathrm{~A}: 11 \\mathrm{H}$, where $\\mathrm{A}=$ Attention and $\\mathrm{H}=$ Hyena along model depth. In instances where the number of layers is not a multiple of the schedule, the ratio is repeated until the target depth is reached. Mamba Mamba doubles the number of sequence mixers, replacing the dedicated channel mixer, and uses a custom input-varying recurrence. Hyperparameters (state dimension 16, expansion factor 2, conv projection length 4 and width of implicit network are sourced from the original implementation [12])\n\nStripedMamba Similar to StripedHyena, we use the 3 striping ratio schedules to interleave attention at specified intervals along model depth. StripedHyena-MoE The StripedHyena-MoE replaces SwiGLU with a total of 8 experts and 2 active experts. We keep the same depth and model width in the mixer layer as baseline models, and adjust the MoE widths to match active parameters. StripedHyena Experts-MoE This model introduces expert in the Hyena sequence mixer at the output level, as described in the main text. We use a StripedHyena with striping ratio 1:11, and the following expert counts: total experts $=8$, active experts $=2$, total mixer experts $=8$, active mixer experts $=2$.",
    "mechdesignscaling-37": "## C. 3 Model sizes and training hyperparameters\n\nWe show common model settings across all architectures by size in Table C.2. We use Adam optimizer betas [0.9, 0.95], weight decay 0.1 , and no dropout. All models are trained in mixed precision: bfloat16 with full precision for Hyena and Mamba convolution and recurrences. ## C. 4 FLOP calculation\n\nWe provide FLOP calculators for each model architecture explored in this study. Notation is provided in C.5. ## C.4.1 Transformer++\n\n- Embedding layers: $4 L D V$\n- MHA\n- projections: $6 L D^{2}$\n- attention: $4 L^{2} D+2 H L^{2}$\n- out layer: $2 L D^{2}$\n- GLU\n\n$$\n-6 L D D_{\\mathrm{g} 1 \\mathrm{u}}\n$$\n\nTable C.2: Common settings across all architectures. For Mamba, we use the layer structure of $\\mathrm{Mb}-\\mathrm{Mb}$ following [12]. Actual parameter counts vary slightly for each architecture.. | PARAMS $(\\mathrm{M})$ | D_MODEL | FFW_SIZE | KV_SIZE | N_HEADS | N_LAYERS | LEARNING RATE |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 8 | 128 | 336 | 64 | 2 | 4 | $9.77 \\mathrm{E}-04$ |\n| 22 | 320 | 848 | 64 | 5 | 5 | $9.57 \\mathrm{E}-04$ |\n| 38 | 448 | 1200 | 64 | 7 | 7 | $9.36 \\mathrm{E}-04$ |\n| 54 | 512 | 1360 | 64 | 8 | 9 | $9.15 \\mathrm{E}-04$ |\n| 70 | 576 | 1536 | 64 | 8 | 10 | $8.95 \\mathrm{E}-04$ |\n| 102 | 640 | 1712 | 64 | 10 | 14 | $8.56 \\mathrm{E}-04$ |\n| 118 | 704 | 1872 | 64 | 11 | 14 | $8.37 \\mathrm{E}-04$ |\n| 134 | 768 | 2048 | 64 | 12 | 14 | $8.18 \\mathrm{E}-04$ |\n| 150 | 768 | 2048 | 64 | 12 | 16 | $8.00 \\mathrm{E}-04$ |\n| 163 | 768 | 2048 | 64 | 12 | 17 | $7.75 \\mathrm{E}-04$ |\n| 175 | 768 | 2048 | 64 | 12 | 19 | $7.50 \\mathrm{E}-04$ |\n| 196 | 832 | 2224 | 64 | 13 | 19 | $7.25 \\mathrm{E}-04$ |\n| 217 | 832 | 2224 | 64 | 13 | 21 | $7.00 \\mathrm{E}-04$ |\n| 251 | 896 | 2384 | 64 | 14 | 21 | $6.75 \\mathrm{E}-04$ |\n| 278 | 896 | 2384 | 64 | 14 | 24 | $6.50 \\mathrm{E}-04$ |\n| 306 | 960 | 2560 | 64 | 15 | 24 | $6.25 \\mathrm{E}-04$ |\n| 350 | 1024 | 2736 | 64 | 16 | 24 | $6.00 \\mathrm{E}-04$ |\n| 440 | 1152 | 3072 | 64 | 18 | 24 | $5.66 \\mathrm{E}-04$ |\n| 536 | 1280 | 3408 | 64 | 20 | 24 | $5.33 \\mathrm{E}-04$ |\n| 641 | 1408 | 3760 | 128 | 11 | 24 | $5.00 \\mathrm{E}-04$ |\n| 756 | 1536 | 4096 | 128 | 12 | 24 | $4.75 \\mathrm{E}-04$ |\n| 881 | 1664 | 4432 | 128 | 13 | 24 | $4.55 \\mathrm{E}-04$ |\n| 1010 | 1792 | 4784 | 128 | 14 | 24 | $4.33 \\mathrm{E}-04$ |\n| 1160 | 1920 | 5120 | 128 | 15 | 24 | $4.15 \\mathrm{E}-04$ |\n| 1200 | 1920 | 5120 | 128 | 15 | 25 | $4.11 \\mathrm{E}-04$ |\n| 1300 | 2048 | 5456 | 128 | 16 | 24 | $4.00 \\mathrm{E}-04$ |\n| 1600 | 2176 | 5808 | 128 | 17 | 26 | $3.84 \\mathrm{E}-04$ |\n| 1900 | 2304 | 6144 | 128 | 18 | 28 | $3.67 \\mathrm{E}-04$ |\n| 2250 | 2432 | 6480 | 128 | 19 | 30 | $3.47 \\mathrm{E}-04$ |\n| 2400 | 2560 | 6832 | 128 | 20 | 29 | $3.39 \\mathrm{E}-04$ |\n| 2640 | 2560 | 6832 | 128 | 20 | 32 | $3.25 \\mathrm{E}-04$ |\n| 3100 | 2688 | 7168 | 128 | 21 | 34 | $3.00 \\mathrm{E}-04$ |\n| 4200 | 3072 | 8192 | 128 | 24 | 36 | $2.72 \\mathrm{E}-04$ |\n| 5200 | 3328 | 8880 | 128 | 26 | 38 | $2.46 \\mathrm{E}-04$ |\n| 7000 | 3712 | 9904 | 128 | 29 | 41 | $2.00 \\mathrm{E}-04$ |\n|  |  |  |  |  |  |  |\n|  |  |  | 28 | 24 |  |  |\n\n## C.4.2 Hyena\n\nGLU and embedding calculation is the same as Transformer++. - Sequence Mixer\n- projections: $6 L D^{2}$\n- convs on projections: $18 L D$\n- featurization: $S_{\\text {hyena }} L D^{9}$\n- convolution and gates: $10 L \\log _{2}(L) D+4 L D$\n- out layer: $2 L D^{2}$\n\n\n## C.4.3 Multi-Head Hyena\n\n- Sequence Mixer\n- projections: $6 L D^{2}$\n\n[^5]Table C.3: MoE model sizes for StripedHyena. All MoE models use 8 total experts and 2 active experts. Other model settings for corresponding active parameter counts follow Table C.2, including d_model, n_heads, n_layers, ffw_size, kv_size, and learning rate. | Total PARAMS (M) | ACTIVE PARAMS | MoE WIDTH |\n| :---: | :---: | :---: |\n| 194 | 102 | 1728 |\n| 228 | 118 | 1856 |\n| 270 | 134 | 2048 |\n| 303 | 150 | 2048 |\n| 319 | 163 | 2048 |\n| 352 | 175 | 2048 |\n| 404 | 196 | 2176 |\n| 452 | 217 | 2240 |\n| 512 | 251 | 2368 |\n| 580 | 278 | 2368 |\n| 667 | 306 | 2560 |\n| 761 | 350 | 2752 |\n| 950 | 440 | 2752 |\n| 1160 | 536 | 3392 |\n| 1390 | 641 | 3712 |\n| 1660 | 756 | 4096 |\n| 1940 | 881 | 4416 |\n| 2230 | 1010 | 4736 |\n| 2550 | 1160 | 5056 |\n| 2910 | 1300 | 5440 |\n\n```\n- convs on projections: \\(18 L D\\)\n- featurization: \\(S_{\\text {hyena }} L H\\)\n- convolution and gates: \\(10 L \\log _{2}(L) D^{2} / H+4 L D^{2} / H\\)\n- out layer: \\(2 L D^{2}\\)\n```\n\n\n## C.4.4 StripedHyena\n\nFLOPS of StripedHyena are determined by summing the FLOPS of Hyena-GLU and MHA-GLU, with the mixing ratios specified by the particular instance of the model. ## C.4.5 Mamba\n\n- Sequence Mixer\n- projections: $4 L D^{2} E$\n- short conv: $6 L D E$\n- featurization: $2 L D E\\left(D_{\\mathrm{dt}}+2 S_{\\text {mamba }}\\right)+2 L D E D_{\\mathrm{dt}}$\n- associative scan: $2 L D E S_{\\text {mamba }}{ }^{10}$\n- out layer: $2 L D^{2} E$\n- No separate GLU block ( 2 x the sequence mixers). ## C.4.6 StripedMamba\n\nFLOPS of StripedMamba are determined by summing the FLOPS of Mamba-Mamba and MHA-GLU, with the mixing ratios specified by the particular instance of the model. [^6]Table C.4: StripedHyena Expert model sizes, which all use 8 total experts and 2 active experts for both sequence mixing and GLU experts. Other model settings for corresponding active parameter counts follow Table C.2, including d_model, n_heads, n_layers, ffw_size, kv_size, and learning rate. | Total PARAMS (M) | ACTIVE ParamS | ExPert WidTH | ExPERT Total WidTH | MoE WIDTH |\n| :---: | :---: | :---: | :---: | :---: |\n| 241 | 101 | 80 | 640 | 2368 |\n| 290 | 119 | 88 | 704 | 2624 |\n| 337 | 137 | 96 | 768 | 2816 |\n| 386 | 153 | 96 | 768 | 2880 |\n| 408 | 160 | 96 | 768 | 2880 |\n| 452 | 174 | 96 | 768 | 2880 |\n| 520 | 199 | 104 | 832 | 3072 |\n| 570 | 215 | 104 | 832 | 3072 |\n| 661 | 248 | 112 | 896 | 3328 |\n| 749 | 277 | 112 | 896 | 3328 |\n| 860 | 315 | 120 | 960 | 3584 |\n| 965 | 352 | 128 | 1024 | 3776 |\n| 1220 | 441 | 144 | 1152 | 4288 |\n| 1500 | 535 | 160 | 1280 | 4736 |\n| 1810 | 641 | 176 | 1408 | 5216 |\n| 2140 | 757 | 192 | 1536 | 5696 |\n| 2510 | 882 | 208 | 1664 | 6176 |\n| 2880 | 1010 | 224 | 1792 | 6592 |\n| 3320 | 1160 | 240 | 1920 | 7104 |\n| 3790 | 1310 | 256 | 2048 | 7616 |\n\n## C.4.7 StripedHyena-MoE\n\n- Sequence mixer\n- Same as StripedHyena\n- SwiGLU MoE (replaces MLP block)\n- router: $L D A_{\\text {moe }}$\n- up projections $4 D D_{\\text {moe }} A_{\\text {moe }}$\n- down projection (sparse) $2 D D_{\\text {moe }} G_{\\text {moe }}$\n\n\n## C.4.8 StripedHyena Experts + MoE\n\nModel has experts in both sequence mixers (Hyena) and GLU layers. In attention layers, Transformer++ sequence mixer (MHA) FLOPS are used. The idea of Hyena experts is to select via a router (softmax $\\operatorname{argmax}$ selection) $G_{\\text {moh }}$ smaller Hyena experts, and run computation only on those dimensions. Equivalently, this can be seen as adaptively choosing a subset of states, using the input sequence. - Hyena experts\n- router: $L D A_{\\text {moh }}$\n- projections: $6 L D^{2}$\n- convs on projections: $18 L D$\n- featurization: $S_{\\text {hyena }} L D_{\\text {moh }} G_{\\text {moh }}$\n- convolution and gates: $10 L \\log _{2}(L) D_{\\text {moh }} G_{\\text {moh }}+4 L D_{\\text {moh }} G_{\\text {moh }}$\n- out layer: $2 L D_{\\text {moh }} D$\n\nTable C.5: Notation for FLOP calculation. | Notation | Description |\n| :---: | :---: |\n| $C$ | Model FLOP cost per token |\n| $N$ | Number of layers |\n| $L$ | Sequence length |\n| $D$ | Model width |\n| $V$ | Vocabulary size |\n| $H$ | Number of heads |\n| $D_{\\text {glu }}$ | Width of GLU (reverse bottleneck) |\n| $D_{\\text {moe }}$ | Width of MoE expert |\n| $D_{\\text {dt }}$ | Width of bottleneck in Mamba featurization |\n| $D_{\\text {moh }}$ | Width of Hyena expert |\n| $D_{\\text {glu }}$ | Width of GLU (reverse bottleneck) |\n| $A_{\\text {moe }}$ | Number of MoE experts |\n| $A_{\\text {moh }}$ | Number of Hyena experts |\n| $G_{\\text {moe }}$ | Number of active MoE experts |\n| $G_{\\text {moh }}$ | Number of active Hyena experts |\n| $S_{\\text {hyena }}$ | filter order |\n| $S_{\\text {mamba }}$ | state dimension |\n| $E$ | projection expansion factor |\n\n## D Extended Scaling Results\n\n## D. 1 Optimal hybridization topologies\n\nWe observe the topology of hybrid architectures to have significant effect on their downstream performance. In MAD tests, interleaving schedules for StripedHyena, with gated convolution followed by attention, outperform schedules attention followed by gated convolution. Table D. 1 provides ablations on the perplexity at larger scales. A variety of topologies achieve best perplexity, including chunked interleaving ( $6 \\mathrm{H}: 6 \\mathrm{~A}$ ) and an encoder-decoder topology ( $6 \\mathrm{H}: 12 \\mathrm{~A}: 6 \\mathrm{H}$ ), where Hyena layers surround a block of attention layers. For all other experiments in the paper, including scaling laws, we adopt a simple $1 \\mathrm{H}: 1 \\mathrm{~A}$ topology for simplicity, as that is already seen to outperform other architectures in compute-optimal and state-optimal scaling. Table D.1: Topology ablation for StripedHyena (750M at 2e19 FLOPS on The Pile). H and A indicate Hyena and MHA layers, respectively. | Topology | Perplexity |\n| :--- | :---: |\n| $(1 \\mathrm{H}: 1 \\mathrm{~A}) \\times 12$ | 9.52 |\n| $(2 \\mathrm{H}: 2 \\mathrm{~A}) \\times 6$ | 9.32 |\n| $(3 \\mathrm{H}: 3 \\mathrm{~A}) \\times 4$ | 9.33 |\n| $(4 \\mathrm{H}: 4 \\mathrm{~A}) \\times 3$ | 9.37 |\n| $(6 \\mathrm{H}: 6 \\mathrm{~A}) \\times 2$ | 9.28 |\n| 12H:12A | 9.41 |\n| $(2 \\mathrm{H}: 4 \\mathrm{~A}: 4 \\mathrm{H}: 2 \\mathrm{~A}) \\times 2$ | 9.25 |\n| $(\\mathrm{H}: 5 \\mathrm{~A}: 5 \\mathrm{H}: \\mathrm{A}) \\times 2$ | 9.31 |\n| $4 \\mathrm{H}: 10 \\mathrm{~A}: 8 \\mathrm{H}: 2 \\mathrm{~A}$ | 9.33 |\n| $4 \\mathrm{H}: 12 \\mathrm{~A}: 8 \\mathrm{H}$ | 9.31 |\n| 6H:12A:6H | 9.30 |\n| 8H:12A:4A | 9.35 |\n\n## D. 2 Byte-level scaling laws\n\nWe report additional results for scaling laws on DNA sequences. We trained all models on 8 k sequence length, using model hyperparameters detailed in C.2. The model rankings are different from subword tokenized language data. We also compare architecture performance outside the compute-optimal frontier, namely with allocations of the computational budget are suboptimal but common in practice, such as overtraining smaller models D.2. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-34.jpg?height=376&width=1598&top_left_y=538&top_left_x=236)\n\nFigure D.1: Pretraining compute-optimal scaling on DNA sequences, with byte-level tokenization (nucleotide resolution). ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-34.jpg?height=320&width=1612&top_left_y=1095&top_left_x=224)\n\nFigure D.2: Scaling off the compute-optimal frontier on DNA data. We verify the perplexity scaling at model sizes with a percentage offset from the optimal model size at each FLOP budget. In particular, we train a $\\%$ offset smaller model, for longer. Transformers do not scale well to the overtraining regime. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-34.jpg?height=635&width=1458&top_left_y=1680&top_left_x=309)\n\nFigure D.3: Comparison of optimal model size and number of tokens for each FLOP budget. [^0]:    *Equal contribution. [^1]:    ${ }^{1}$ Found via the optimal allocation of compute to tokens and model size. [^2]:    ${ }^{2}$ Many tweaks to activation choice, placement and presence of biases are carried out to improve numerical stability and reduce the presence of large outliers in activations, rather than improve scaling performance. ${ }^{3}$ We use $t$ for consistency, although in practice these layers can be applied to both \"sequence\" dimension, as well as \"width\" dimension. ${ }^{4}$ For simplicity we detail unnormalized layers, as normalization simply redefines the operator as the ratio of two recurrences. [^3]:    ${ }^{6}$ Only the input-to-state and output-to-state maps are input-varying. ${ }^{6}$ Input-to-state and state-to-output maps are shared across channels. ${ }^{7}$ Solutions to in-context recall for some architectures can be expressed precisely and even hardcoded in an architecture without training [24, 17]. Thus, in-context recall tasks also represents a useful, albeit limited, test case to guide theoretical analysis. [^4]:    ${ }^{8}$ Accounting for state-optimality shifts the optimal ratio to $10 \\%$. [^5]:    ${ }^{9}$ Other filter parametrizations e.g., canonical via rational functions, scale with the order $\\mathcal{O}\\left(S_{\\text {hyena }} D L \\log _{2}(L)\\right)$. [^6]:    ${ }^{10}$ Estimate assumes \"most efficient\" scan algorithm in terms of FLOPS (but not latency).",
    "mechdesignscaling-38": "In practice, the constant may be larger.",
    "mechdesignscaling-39": ""
}