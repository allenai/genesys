{
    "sinkhorn-0": "Sparse Sinkhorn Attention\n\nYi Tay Dara Bahri Liu Yang Donald Metzler Da-Cheng Juan\n\nAbstract\n\nWe propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend.",
    "sinkhorn-1": "Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers. Machine Learning, ICML\n\n1 Introduction\n\nLearning sparse and efficient attention mechanisms has recently garnered considerable interest (Child et al., 2019; Kitaev et al., 2020). While existing state-of-the-art attention models have typically relied on dense, fully-connected attention graphs (Vaswani et al., 2017), these methods are often sub-optimal for two key reasons. First, large memory costs are incurred due to the quadratic complexity at the attention layer. Second, soft dense attention may suffer when , the sequence length, is large and noisy. Hence, at times, sparse attentive outputs that are reminiscent of hard attention methods, may serve as a desirable inductive bias (Xu et al., 2015). This paper proposes a new method for (1) reducing the memory complexity of the dot-product attention mechanism and (2) learning sparse attention outputs. Our method is based on a novel idea of differentiable sorting of internal representations within the self-attention module. Our method, which we call Sparse Sinkhorn Attention, incorporates a meta sorting network that learns to re arrange and sort input sequences. With this new sorted sequence, attention computation is reduced substantially even when considering computation only within the local neighborhood, emulating a global effect even with solely local computation of context windows. Our method is comprised of (1) a parameterized meta sorting network for dynamically generating block-wise permutation matrices and (2) a standard local attention module that receives block-wise permuted input sequences for attention computation. Concretely, at the heart of our sorting network lives a differentiable Sinkhorn balancing mechanism (Adams & Zemel, 2011; Mena et al., 2018), which normalizes permutation matrices to belong to the Birkhoff polytope, the set of doubly stochastic matrices (Sinkhorn, 1964). As such, given the block-sorted input sequences, the local attention module is able to compute attention weights beyond the default local neighborhood without incurring additional computation costs. Extensive experimental results across a potpourri of language, vision and arithmetic tasks demonstrate that Sparse Sinkhorn Attention outperforms strong baselines such as standard local attention and sparse attention Transformers (Child et al., 2019). Notably, our proposed method is general purpose in nature and is applicable to sequence encoding, sequence decoding or seq2seq tasks (Sutskever et al., 2014). In order to adapt Sinkhorn balancing to decoding tasks, we propose a causal variant, i.e., Causal Sinkhorn Balancing. Moreover, for further improvement to encoding efficiency, we propose an additional SortCut variant of our proposed method, which dynamically truncates sequences in a data-driven manner based on a user-defined budget hyperparameter. Finally, we propose a Mixture model between the Sparse Sinkhorn Attention and standard vanilla attention, leading to further performance improvements. Our method reduces the memory complexity from to where . When is large, this factorization of sequence length brings about substantial savings in terms of memory complexity111As an illustration, when and , this results in a memory saving factor of 240 times.. Our SortCut variant further reduces complexity to linear-time, i.e., where is a user defined budget hyperparameter and . We also equip state-of-the-art Transformer models with our proposed Sparse Sinkhorn Attention, evaluating Sinkhorn Transformers on several large-scale sequence modeling tasks including language modeling on the One Billion Word Corpus (Chelba et al., 2013), pixel-wise image generation and document classification. Our proposed Sinkhorn attention remains competitive to the dense fully-connected attention while outperforming local attention and Sparse Transformers. While differentiable neural-based sorting has demonstrated some proof-of-concept promise (Mena et al., 2018), this work demonstrates the first successful application in real large-scale problems. To summarize, the contributions of this paper are as follows:\n\n\u2022\n\nWe propose Sparse Sinkhorn Attention, a new attention method based on dynamic, learnable sorting of internal representations. Our method is based on differentiable Sinkhorn balancing and is the first successful application of differentiable sorting on large-scale tasks. \u2022\n\nWe also propose (1) Causal Sinkhorn balancing for autoregressive sequence decoding and (2) a new SortCut encoding scheme that further improves encoding efficiency by dynamically truncating sequences during attention computation. \u2022\n\nOur proposed methods reduce the memory complexity of dot-product attention while remaining competitive with or outperforming dense vanilla attention. \u2022\n\nWe conduct extensive experiments on large-scale generative modeling tasks. On all tasks, Sinkhorn Transformers match and/or outperform vanilla Transformers while consistently outperforming Sparse Transformers (Child et al., 2019) and Local Attention Transformers. 2 Related Work\n\nA natural and intuitive yet naive method typically employed for efficiently learning attention involves using a fixed window size. This method, usually referred to as local attention (Luong et al., 2015), has served as a simple and quick fix to run attention models on long sequences. An obvious weakness is that tokens in a window do not have access to context outside the window, restricting the expressiveness and its capability to model long-term dependencies. The study of window (or block-based) local attention has also been an emerging field of research (Shen et al., 2018b; Tay et al., 2019; Qiu et al., 2020; Child et al., 2019; Parmar et al., 2018). Building upon the notion of local windows, Sparse Transformer (Child et al., 2019) proposed factorizing the attention computation into local and strided operations, delegating different heads to focus on different sparse patterns. They demonstrate promising results, establishing Sparse Transformer as one of the canonical methods222That said, Sparse Attention requires highly specialized GPU kernels for efficient computation. This generally makes the approach less appealing, e.g., for portability purposes such as running on TPU pods. for efficient attention computation. While our method also relies on sequence partitioning, we note that there have been several orthogonal but related efforts. Reformer (Kitaev et al., 2020), proposes locality sensitive hashing as a means to reduce the memory complexity of self-attention. Transformer-XL (Dai et al., 2019) adopts recurrence to cache hidden states across long sequences, which spurred further interest in modeling and compression of long term dependencies (Rae et al., 2020). Star Transformer (Guo et al., 2019) performs attention sparsification by converting the dense graph into a starshaped topology using a shared relay node. However, while this method enables linear-time complexity, its setup makes it difficult for causal masking, making the Star Transformer useful only for encoding. Learning sparse outputs in attention models has also garnered reasonable interest. The key idea behind sparse weights (i.e., hard attention) is that they enable the model to only focus on a limited number of items at a time (Xu et al., 2015; Shen et al., 2018a). This can be a useful inductive bias when the input sequence is long and/or noisy, serving as a denoising filter. Moreover, hard attention can also improve inference speeds, as demonstrated by methods such as Sparsemax (Martins & Astudillo, 2016). Along a similar vein, this is also reminiscent of Sparse Mixture of Experts (Shazeer et al., 2017), which performs a sparse selection of outputs (experts) for prediction tasks. Our proposed method is not only a new way of learning efficient attention but also a new way of sparsification. At the core of our approach lies a Sinkhorn ranking operation (Adams & Zemel, 2011) that is used for learning differentiable rankings over internal representations. Leveraging the Gumbel reparameterization trick (Jang et al., 2016), Gumbel Sinkhorn Networks (Mena et al., 2018) proposed stochastic maximization over the set of possible latent permutations. The core novelty of our work lies in the introduction of neural sorting as a means to sparsify and improve the efficiency of well-established attention networks. 3 Sparse Sinkhorn Attention\n\nIn this section, we introduce our proposed Sparse Sinkhorn Attention and provide a high-level overview. In our method, the input sequence of length is partitioned into blocks in which each block has a length of tokens. Notably, the original idea of block-based local attention is to allow tokens to only attend to tokens within the same block. However, this restricts the global receptive field and limits the ability for local attention models to model long term dependencies. Our proposed method mitigates this problem by neural sorting of blocks and receptive fields (neighborhoods). More concretely, instead of attending to tokens in the same block, each token attends to tokens in the newly sorted block, which may actually be far apart in the original unsorted sequence. Sorting blocks instead of individual tokens is also more intuitive, since we do not wish to break connections between nearby tokens, i.e., it would be reasonable for each token to still maintain an approximate neighborhood. 3.1 Learning to Sort\n\nIn order to learn to sort, we introduce a Sorting Network (SortNet) for learning relaxed permutation matrices. Since our sorting function is differentiable, the parameters of the SortNet are also trained together in an end-to-end fashion. The SortNet accepts an input sequence of vectors of dimensions and partitions them into blocks. X \u2032 = \u03c8 P \u200b ( X ) superscript \ud835\udc4b \u2032 subscript \ud835\udf13 \ud835\udc43 \ud835\udc4b X^{\\prime}=\\psi_{P}(X) (1)\n\nThe function is a blockwise pooling operation that maps and . In SortNet, we adopt:\n\n\u03c8 P \u200b ( X ) i = \u2211 j = i \u2217 \u2113 B ( i + 1 ) \u2217 \u2113 B ( X j ) subscript \ud835\udf13 \ud835\udc43 subscript \ud835\udc4b \ud835\udc56 subscript superscript \ud835\udc56 1 subscript \u2113 \ud835\udc35 \ud835\udc57 \ud835\udc56 subscript \u2113 \ud835\udc35 subscript \ud835\udc4b \ud835\udc57 \\psi_{P}(X)_{i}=\\sum^{(i+1)*\\ell_{B}}_{j=i*\\ell_{B}}(X_{j}) (2)\n\nwhich is equivalent to taking the sum of embeddings of all tokens belonging to the local window. Our trainable SortNet is defined as follows:\n\nR i = P \u200b ( X i \u2032 ) subscript \ud835\udc45 \ud835\udc56 \ud835\udc43 subscript superscript \ud835\udc4b \u2032 \ud835\udc56 R_{i}=P(X^{\\prime}_{i}) (3)\n\nwhere refers to the block index. is an arbitrary parameterized function which accepts an input vector of dimensions and returns a vector of dimensions. For example, we may parameterize using a two layered feed-forward network with ReLU activations. P ( X ) = \u03c3 ( W B \u03c3 ( W P ( X ) + b P ) + b B P(X)=\\sigma(W_{B}\\sigma(W_{P}(X)+b_{P})+b_{B} (4)\n\nwhere and . Essentially, the key idea is that each block learns a projection to other blocks, effectively learning the position that it is supposed to be shifted (or permuted) to. 3.1.1 Sinkhorn Normalization\n\nThe matrix becomes a sorting matrix (or permutation matrix) if it is doubly stochastic (matrix is nonnegative and both rows and columns all sum to 1). More specifically, a permutation matrix is special case of a doubly stochastic matrix (where rows and columns sum to 1 and all entries are either or ). Since every permutation matrix is a convex combination of doubly stochastic matrices, we consider learning doubly stochastic matrices as a a form of relaxed permutation matrix. We consecutively normalize the rows and columns of the sorting matrix , i.e., a process of Sinkhorn normalization (Adams & Zemel, 2011). Here, the number of iterations is a user defined hyperparameter. This procedure is described as follows:\n\nS 0 \u200b ( R ) superscript \ud835\udc46 0 \ud835\udc45 \\displaystyle S^{0}(R) = exp \u200b ( R ) absent exp \ud835\udc45 \\displaystyle=\\text{exp}(R) S k \u200b ( R ) superscript \ud835\udc46 \ud835\udc58 \ud835\udc45 \\displaystyle S^{k}(R) = F c \u200b ( F r \u200b ( S k \u2212 1 \u200b ( R ) ) ) absent subscript \ud835\udc39 \ud835\udc50 subscript \ud835\udc39 \ud835\udc5f superscript \ud835\udc46 \ud835\udc58 1 \ud835\udc45 \\displaystyle=F_{c}(F_{r}(S^{k-1}(R))) S \u200b ( R ) \ud835\udc46 \ud835\udc45 \\displaystyle S(R) = lim k \u2192 \u221e S K \u200b ( R ) absent subscript \u2192 \ud835\udc58 superscript \ud835\udc46 \ud835\udc3e \ud835\udc45 \\displaystyle=\\lim_{k\\rightarrow\\infty}S^{K}(R)\n\nwhere are the row and column wise normalization function defined as follows:\n\nF c k \u200b ( X ) subscript superscript \ud835\udc39 \ud835\udc58 \ud835\udc50 \ud835\udc4b \\displaystyle F^{k}_{c}(X) = F c k \u2212 1 \u200b ( X ) \u2298 ( X \u200b 1 \u2113 \u200b 1 N \u22a4 ) absent \u2298 subscript superscript \ud835\udc39 \ud835\udc58 1 \ud835\udc50 \ud835\udc4b \ud835\udc4b subscript 1 \u2113 subscript superscript 1 top \ud835\udc41 \\displaystyle=F^{k-1}_{c}(X)\\oslash(X1_{\\ell}1^{\\top}_{N}) F r k \u200b ( X ) subscript superscript \ud835\udc39 \ud835\udc58 \ud835\udc5f \ud835\udc4b \\displaystyle F^{k}_{r}(X) = F r k \u2212 1 \u200b ( X ) \u2298 ( 1 \u2113 \u200b 1 N \u22a4 \u200b X ) absent \u2298 subscript superscript \ud835\udc39 \ud835\udc58 1 \ud835\udc5f \ud835\udc4b subscript 1 \u2113 subscript superscript 1 top \ud835\udc41 \ud835\udc4b \\displaystyle=F^{k-1}_{r}(X)\\oslash(1_{\\ell}1^{\\top}_{N}X)\n\nwhere is the element-wise division operator, is the length of the input matrix and 1 is a vector of ones. In practice, we perform calculations in log domain for improved stability. F c k \u200b ( X ) subscript superscript \ud835\udc39 \ud835\udc58 \ud835\udc50 \ud835\udc4b \\displaystyle F^{k}_{c}(X) = F c k \u2212 1 \u200b ( X ) \u2212 log \u2061 ( exp \u2061 ( X \u200b 1 \u2113 ) \u200b 1 N \u22a4 ) absent subscript superscript \ud835\udc39 \ud835\udc58 1 \ud835\udc50 \ud835\udc4b \ud835\udc4b subscript 1 \u2113 subscript superscript 1 top \ud835\udc41 \\displaystyle=F^{k-1}_{c}(X)-\\log(\\exp(X1_{\\ell})1^{\\top}_{N}) F r k \u200b ( X ) subscript superscript \ud835\udc39 \ud835\udc58 \ud835\udc5f \ud835\udc4b \\displaystyle F^{k}_{r}(X) = F r k \u2212 1 \u200b ( X ) \u2212 log \u2061 ( 1 \u2113 \u200b 1 N \u22a4 \u200b exp \u2061 ( X ) ) absent subscript superscript \ud835\udc39 \ud835\udc58 1 \ud835\udc5f \ud835\udc4b subscript 1 \u2113 subscript superscript 1 top \ud835\udc41 \ud835\udc4b \\displaystyle=F^{k-1}_{r}(X)-\\log(1_{\\ell}1^{\\top}_{N}\\exp(X))\n\nTo this end, (Sinkhorn, 1964) shows that iterative normalization of converges to the doubly stochastic limit if has support, i.e., a nonnegative matrix with a positive diagonal. Note that since is nonnegative by design due to the usage of ReLU in . Gradients of the iterative Sinkhorn normalization can be computed, enabling end-to-end training. 3.1.2 Neural Sorting of Sequences\n\nThe generated permutation matrix is then used to sort the input sequence. This is described by a simple matrix multiplication of against the blocked input sequence :\n\nX S = U \u200b ( R \u200b B \u200b ( X ) ) subscript \ud835\udc4b \ud835\udc46 \ud835\udc48 \ud835\udc45 \ud835\udc35 \ud835\udc4b \\displaystyle X_{S}=U({R}B(X))\n\nwhere converts input sequence into block-wise representations, i.e., and converts the block-wise sequences back into token-wise sequences. and can be interpreted as block-wise reshaping operators. Since is doubly stochastic, multiplying a partitioned sequence by is equivalent to sorting it. 3.2 Sparse Sinkhorn Attention\n\nThe key idea of the Sparse Sinkhorn Attention is to operate on block sorted sequences. Hence, the revised computation for the attention mechanism can now be written as:\n\nA i \u200b j = { ( Q i \u03c8 S ( K ) j \u22a4 ) + Q i ( K ) j \u22a4 ) , if \u200b \u230a j / \u2113 \u230b = \u230a i / \u2113 \u230b 0 otherwise \\displaystyle A_{ij}=\\begin{cases}(Q_{i}\\psi_{S}(K)_{j}^{\\top})+Q_{i}(K)_{j}^{\\top}),&\\text{if }\\lfloor{{j}/{\\ell}}\\rfloor=\\lfloor{i/{\\ell}}\\rfloor\\\\\n0&\\text{otherwise}\\end{cases}\n\nis the neural sorting function. Intuitively, this is identical to only enabling attention without a certain local neighborhood, albeit with key values sorted in a block-wise fashion. Subsequently, to compute and soft-select from the value matrix, we compute:\n\nY = Softmax \u200b ( A ) \u200b \u03c8 S \u200b ( V ) \ud835\udc4c Softmax \ud835\udc34 subscript \ud835\udf13 \ud835\udc46 \ud835\udc49 \\displaystyle Y=\\text{Softmax}(A)\\psi_{S}(V)\n\nHere, the value matrix is also sorted accordingly. In practice, we share the sorting operator between the key and values. The secondary term is the standard local attention which is added to the mixture. In practice, attention weights are only computed when . 3.2.1 Gumbel Noise\n\nFor to approximate the doubly-stochastic permutation matrix, we leverage the Gumbel categorical reparameterization trick (Jang et al., 2016). Concretely, we inject Gumbel noise into our sorting operator:\n\nS \u200b ( X ) = S \u200b ( ( X + \u03f5 ) \u03c4 ) \ud835\udc46 \ud835\udc4b \ud835\udc46 \ud835\udc4b italic-\u03f5 \ud835\udf0f \\displaystyle S(X)=S(\\frac{(X+\\epsilon)}{\\tau})\n\nwhere is the injected standard i.i.d Gumbel noise and is the temperature hyperparameter. Intuitively, lowering the temperature brings to be closer to a permutation matrix with discrete s and s. 3.2.2 Multihead Sparse Sinkhorn Attention\n\nWe have previously described the computation of a single Sinkhorn attention head. Similar to dot product attention, utilizing the multi-headed variation is straightforward. Y G = F H \u200b ( [ Y 1 \u200b \u22ef \u200b Y N H ] ) subscript \ud835\udc4c \ud835\udc3a subscript \ud835\udc39 \ud835\udc3b delimited-[] subscript \ud835\udc4c 1 \u22ef subscript \ud835\udc4c subscript \ud835\udc41 \ud835\udc3b \\displaystyle Y_{G}=F_{H}([Y_{1}\\cdots Y_{N_{H}}])\n\nwhere is the output of the -th attention head. is a linear transform layer with kernels . Notably, our implementation learns a sorting network on a per head basis, i.e., we do not share the same permutation matrix across all heads. 3.2.3 Mixture Model\n\nFinally, we also consider a variant where the Sinkhorn Attention is used to model an alternate view of the input sequence. Concretely, we leverage the combination of the Sinkhorn attention by mixing it with the vanilla standard dot product attention. Y = Softmax \u200b ( A ) \u200b \u03c8 S \u200b ( V ) + Softmax \u200b ( Q \u200b K \u22a4 ) \u200b V \ud835\udc4c Softmax \ud835\udc34 subscript \ud835\udf13 \ud835\udc46 \ud835\udc49 Softmax \ud835\udc44 superscript \ud835\udc3e top \ud835\udc49 \\displaystyle Y=\\text{Softmax}(A)\\psi_{S}(V)+\\text{Softmax}(QK^{\\top})V\n\nNotably, the mixture mode regresses to the same quadratic complexity of vanilla self-attention. However, we hypothesize that the side network may provide an alternative and diverse view, ultimately improving performance. 3.3 Causal Sparse Sinkhorn Attention\n\nOur Sinkhorn Attention not only involves sorting sequences but also learning the sorting order in a content-based fashion. To this end, pertaining to learning causal attention (i.e., no information from the future should leak to the present) there are two cases that we have to be careful about. The first is that current time steps should never have access to future time steps. Hence, if block is sorted into a new position , then it is being masked out. This produces an inductive bias that favors sorting orders that produce sorting between nearby blocks. 3.3.1 Causal Sorting Networks\n\nThe second case is the content-based and dynamic learning of sorting networks.",
    "sinkhorn-2": "To maintain the causal property, it would not be plausible to generate permutation matrices based on global information such as the sum of tokens in a sequence. Hence, the matrix is generated using the cumulative sum of embeddings instead. This is described as follows:\n\n\u03c8 P \u200b ( X ) i = \u2211 j = 0 ( i \u2217 \u2113 B + 1 ) ( X j ) \u200b and \u200b R = P \u200b ( \u03c8 P \u200b ( X ) ) subscript \ud835\udf13 \ud835\udc43 subscript \ud835\udc4b \ud835\udc56 subscript superscript \ud835\udc56 subscript \u2113 \ud835\udc35 1 \ud835\udc57 0 subscript \ud835\udc4b \ud835\udc57 and \ud835\udc45 \ud835\udc43 subscript \ud835\udf13 \ud835\udc43 \ud835\udc4b \\psi_{P}(X)_{i}=\\sum^{(i*\\ell_{B}+1)}_{j=0}(X_{j})\\>\\>\\>\\text{and}\\>\\>\\>R=P(\\psi_{P}(X)) (5)\n\nSince our attention operates based on the idea of blocks, we use the first token in the block as its representative embedding. The cumulative sum operator allows the model to learn a permutation matrix conditioned on all previous context information leading up to the current block. 3.3.2 Causal Sinkhorn Balancing\n\nWe note that the original Sinkhorn balancing requires knowledge of the future tokens for normalization. For causal self-attention, this is undesirable and non-permissible. Hence, we develop a causal variation of the typical Sinkhorn Balancing method which performs masking of the future while performing iterative normalization. F c k \u200b ( X ) subscript superscript \ud835\udc39 \ud835\udc58 \ud835\udc50 \ud835\udc4b \\displaystyle F^{k}_{c}(X) = F c k \u2212 1 \u200b ( X ) \u2212 log \u2061 ( exp \u2061 ( M \u200b ( X ) \u200b 1 \u2113 ) \u200b 1 N \u22a4 ) absent subscript superscript \ud835\udc39 \ud835\udc58 1 \ud835\udc50 \ud835\udc4b \ud835\udc40 \ud835\udc4b subscript 1 \u2113 subscript superscript 1 top \ud835\udc41 \\displaystyle=F^{k-1}_{c}(X)-\\log(\\exp(M(X)1_{\\ell})1^{\\top}_{N}) F r k \u200b ( X ) subscript superscript \ud835\udc39 \ud835\udc58 \ud835\udc5f \ud835\udc4b \\displaystyle F^{k}_{r}(X) = F r k \u2212 1 \u200b ( X ) \u2212 log \u2061 ( 1 \u2113 \u200b 1 N \u22a4 \u200b M \u200b ( exp \u2061 ( X ) ) ) absent subscript superscript \ud835\udc39 \ud835\udc58 1 \ud835\udc5f \ud835\udc4b subscript 1 \u2113 subscript superscript 1 top \ud835\udc41 \ud835\udc40 \ud835\udc4b \\displaystyle=F^{k-1}_{r}(X)-\\log(1_{\\ell}1^{\\top}_{N}M(\\exp(X)))\n\nwhere is a masking function. M \u200b ( x ) = { 1 , if \u200b j \u2265 i 0 , otherwise \ud835\udc40 \ud835\udc65 cases 1 if \ud835\udc57 \ud835\udc56 0 otherwise M(x)=\\begin{cases}1,&\\text{if }j\\geq i\\\\\n0,&\\text{otherwise}\\end{cases} (6)\n\n3.3.3 Connections to Learnable Sparsity\n\nDue to the computation of causal Sinkhorn, it is expected that some blocks may be masked out, i.e., a block is masked out if it is sorted into an earlier position (i.e, ). Essentially, the Sorting Network is also learning which tokens to mask by determining the sorting sequence. 3.4 SortCut Sinkhorn Attention\n\nWe propose an additional variant of our Sparse Sinkhorn Attention which we call SortCut. In this method, we propose a post-sorting truncation of the input sequence, essentially performing a hard top-k operation on the input sequence blocks within the computational graph. While most attention models mainly re-weight or assign near-zero weights during training, our method enables us to explicitly and dynamically truncate the input sequence. Specifically,\n\nY = S \u200b o \u200b f \u200b t \u200b m \u200b a \u200b x \u200b ( Q \u200b \u03c8 S \u200b ( K ) [ : n ] \u22a4 ) \u200b \u03c8 S \u200b ( V ) [ : n ] \ud835\udc4c \ud835\udc46 \ud835\udc5c \ud835\udc53 \ud835\udc61 \ud835\udc5a \ud835\udc4e \ud835\udc65 \ud835\udc44 subscript \ud835\udf13 \ud835\udc46 superscript subscript \ud835\udc3e delimited-[] : absent \ud835\udc5b top subscript \ud835\udf13 \ud835\udc46 subscript \ud835\udc49 delimited-[] : absent \ud835\udc5b \\displaystyle Y=Softmax(Q\\psi_{S}(K)_{[:n]}^{\\top})\\psi_{S}(V)_{[:n]}\n\nwhere is the SortCut budget hyperparameter. A caveat is that this mode may only be performed on the the Transformer encoder unless self-attention is explicitly computed again for every time-step in autoregressive decoding. 4 Complexity Analysis\n\nThe vanilla Transformer has a self-attention memory complexity of where is the input sequence length. Our proposed Sinkhorn model reduces this to where . Essentially, this is equivalent to the memory complexity of local attention models. The Sortcut Sinkhorn encoder has a memory complexity of where is the budget hyperparameter. Since , the complexity of the SortCut encoder can be reduced to . 5 Experiments\n\nWe evaluate the effectiveness of our proposed method for five tasks, including algorithmic sorting, language modeling, pixel-wise image generation, document classification and natural language inference. All our experiments are run on the open source Tensor2Tensor framework (Vaswani et al., 2018). If not stated otherwise, our Sinkhorn Transformers adopt the following global hyperparameters - temperature tuned among , number of sort iterations tuned among . Block size is largely dependent on the maximum length of the problem domain. Our tasks are designed to capture a wide and diverse range of scenarios such as medium to long range (e.g., 256 to 2048) and also covering a range of encoding focused and/or decoding focused tasks. 5.1 Algorithmic Tasks\n\nWe first test our model on a toy algorithmic sorting task. The task is cast as a sequence transduction problem (seq2seq) where the model is tasked to output a sorted sequence of an integer sequence. Experimental Setup\n\nWe use the algorithmic_sort_problem task in Tensor2Tensor. In this task, we train our models to sort sequences of and evaluate on sequences of length (i.e., 512) to probe for generalization ability and ensure the models are not just simply memorizing. We evaluate based on exact match (EM) and edit distance (the lower the better). The exact match metric is defined by the number of test sequences that the model gets entirely correct. The dataset consists of train examples and test examples. For Sinkhorn Transformers, we utilize Sparse Sinkhorn Attention for both encoding and decoding. We train all models for k steps using the default Transformer base hyperparameter. We compare against vanilla Transformers, local attention Transformers and Sparse Transformers. Results on Sorting Task\n\nTable 1 reports our results on the algorithmic sorting task. Sinkhorn Transformers outperform all other Transformer variants, including the vanilla attention model. Sparse Transformer outperforms dense attention, which demonstrates the usefulness of sparse inductive biases. The local attention performs the worst, which demonstrates that some extent of global knowledge is required to solve this task. 5.2 Language Modeling\n\nWe evaluate on the LM1B (Language Modeling One Billion) dataset (Chelba et al., 2013), a large-scale language modeling benchmark. We evaluate on subword-level and character level language modeling on this task. Experimental Setup\n\nWe implement our model in the Tensor2Tensor framework, using the packed TPU setting. Tokens are split into k word pieces and sentences are shuffled. For word level language modeling, we use the default Tensor2Tensor hyperparameters333tensor2tensor/models/research/lm_experiments.py. Concretely, we evaluate two model sizes, base and big corresponding to lmx_base and lmx_h2k_f8k respectively. All models are trained for steps on 16 TPU V2 Chips. For Sinkhorn Transformers and the corresponding Local Attention baseline, we tune the block size . We compare against the vanilla Transformer baseline, Local Attention Transformers, and Sparse Transformers (Child et al., 2019). We implement Sparse Transformers in Tensor2Tensor by referencing the original open source code using the fixed attention scheme (Child et al., 2019). However, instead of integrating the specialized cuda kernels, we manually simulated masking to achieve an equivalent implementation. We use a block length of and fixed stride of . For character-level language modeling, owing to the overall larger sequence length, we use a maximum sequence length of and use a fixed block length of . Results on Subword level Language Modeling\n\nTable 2 reports results on subword level language modeling. On both parameter settings, Sinkhorn Transformers outperform all local attention models and Sparse Transformer. Pertaining to relative performance between Sinkhorn and local attention, the performance gain at each setting ranges from perplexity points. Notably, on the base setting, Sinkhorn Transformer outperforms the vanilla Transformer at and . At , Sinkhorn Transformers remain competitive to base Transformers. On the big setting, Sinkhorn Transformers fail to outperform vanilla Transformers, but still perform reasonably well despite being more memory efficient. Finally, the Sinkhorn Mixture model outperforms all models. Results on Character-level Language Modeling\n\nTable 4 reports our experimental results (bytes per char) on character level language modeling. On both settings (base/big), our proposed Sinkhorn Transformer outperforms both local attention and Sparse Transformer, which affirms its effectiveness as an efficient attention method. On the contrary, local attention performs substantially worse compared to its counterparts, likely due to not having much global context. From this set of experiments, the vanilla full attention Transformer outperforms all efficient attention methods. However, our Sinkhorn Mixture model outperforms the Transformer baseline, achieving the best performance for both parameterizations. Comparison with the State-of-the-art\n\nTable 3 reports our best scores relative to the state-of-the-art444To the best of our knowledge, (Shazeer et al., 2018) is the best performing model on per-word perplexity. (Baevski & Auli, 2018) and (Dai et al., 2019) report per-token perplexity. Notably, our best performing Sinkhorn Transformer remains competitive with the High Budget MoE (Shazeer et al., 2017) and Evolved Transformer (So et al., 2019) models. This demonstrates the overall competitiveness of Sinkhorn Transformers. Unfortunately, we were unable to outperform Mesh Tensorflow (Shazeer et al., 2018) on our setup, which consists of billion parameters. Nevertheless, we consider our results to be reasonable given the improved memory complexity. 5.3 Pixel-wise Image Generation\n\nThis section introduces and reports results on pixel-wise image generation task. This task models unconditional generative modeling of images by modeling images as flat sequences of pixels. Experimental Setup\n\nWe evaluate our model on pixel-by-pixel image generation using the Tensor2Tensor framework. We use the CIFAR-10 dataset. Similar to language modeling, we evaluate using bytes per dimension (Bpd), a common metric for evaluating generative modeling of images. In this task, images are flatted to sequences of bits which probes for long-term sequence modeling capabilities. We train all models using the base parameterization for steps with a batch size of . Results on Image Generation\n\nTable 5 reports our results on the pixel-wise image generation task. Our proposed Sinkhorn Transformer outperforms all baselines. The local attention model performs the worst, which can be intuitively attributed to lack of global knowledge. While keeping the local window identical, our model also outperforms Sparse Transformer which demonstrates its utility as an efficient attention method. Finally, the Sinkhorn Mixture performs worse than the ordinary Sinkhorn Transformer, suggesting that a restricted (and learned) global view may serve as a useful inductive bias. 5.4 Text Classification\n\nWe evaluate several text classification benchmarks from the Tensor2Tensor framework. These tasks are mainly encoding only tasks, which allows us to benchmark the Sortcut encoding scheme. Experimental Setup\n\nWe experiment on both sentiment analysis and natural language inference. For the former, we use the standard open source IMDb sentiment (Maas et al., 2011) and Sentiment Treebank (SST) dataset (Socher et al., 2013). For the latter, we use two natural language inference (NLI) datasets, i.e., Stanford NLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017). For sentiment analysis, we evaluate on both character and word level. We set the maximum length of tokens to be 512/2048 for word/character level tasks respectively. We implement our models using Tensor2Tensor using the tiny Transformer setting (2 layers). Hyperparameters between our Sinkhorn Transformer and the vanilla Transformer remains identical. Token embeddings are initialized randomly. We train our models for steps for IMDb and SST and steps for NLI tasks. For all experiments, we use a batch size of tokens per batch. Models are trained with a single V100 GPU. For natural language inference, experimental setup follows the Tensor2Tensor setup where premise and hypothesis are concatenated into one long input sequence. Word embeddings are also randomly initialized. We use the Transformer tiny hyperparameter for this task. We would like to emphasize that our experimental setup for these tasks differs from the standard usage of these datasets. Results on Sentiment Analysis\n\nTable 6 reports results on sentiment analysis. Sinkhorn Transformers demonstrate promising results on sentiment analysis datasets on both word and character level. Even with significant memory savings, Sinkhorn Transformers are able to outperform or remain competitive with the baseline Transformer model. We also take this chance to benchmark the Sortcut encoder, which further reduces memory complexity. On all settings, we find that the Sortcut variation can achieve similar performance to not only the standard Sinkhorn Transformer but also the vanilla Transformer. Results on Natural Language Inference\n\nTable 7 reports results on SNLI and MNLI tasks. We find that both Sinkhorn and Sortcut Sinkhorn are able to outperform the vanilla Transformer. This task demonstrates the effectiveness of the SortCut variant despite the improvement of memory complexity over the standard Sinkhorn Transformer. 6 Analysis\n\nIn this section, we study the effect of certain modeling choices. 6.1 Effect of Modeling Choices\n\nWe are mainly interested in the effects of varying the Sorting Network model. Table 8 reports ablation studies on various model configurations. In (1) to (4), we vary the sorting network model. In (5), we experiment with a scheme to tie the weights of and (this is because they share the same permutation matrix). From Table 8, the best model for learning the sorting matrix is a linear layer, which signifies that the sorting network can be a simple model.",
    "sinkhorn-3": "We also observed that, more often than not, sharing the key-values seem to hurt performance. Finally in (6), we set which is equivalent to not performing Sinkhorn normalization on . We observe that performance degrades substantially and performs the worse of all ablative variations. 6.2 Hard or Soft Sorting? Figure 4 reports the effect of varying Sinkhorn balancing temperatures. Keeping all other variables constant, we varied the temperature of the Gumbel Sinkhorn balancing mechanism. Overall, we find that maintaining a high temperature (inclined towards soft sorting) works better than a more discrete (hard) form of sorting. On this task, the optimal temperature is at . 6.3 Effect of Sorting Iterations\n\nFigure 4 reports the performance trend with respect to , the number of sorting iterations. Overall, a small number of sorting iterations is sufficient for good performance. No sorting at all performs extremely bad while the optimal number of sorting iterations seems to be . Conversely, increasing the number of sorting iterations (beyond ) seem to hurt perplexity scores. 7 Conclusion\n\nWe proposed Sparse Sinkhorn Attention, a new efficient and sparse method for attention computation. Our work demonstrates the utility of neural sorting of internal representations within the attention module on a multitude of large-scale generative modeling and classification tasks. On these benchamrks, our proposed Sinkhorn Transformer outperforms or remains competitive to vanilla Transformer and sparse Transformer models on a multitude of applications while being memory efficient.",
    "sinkhorn-4": "References\n\nAdams & Zemel (2011) Adams, R. P. and Zemel, R. S. Ranking via sinkhorn propagation. arXiv preprint arXiv:1106.1925, 2011. Baevski & Auli (2018) Baevski, A. and Auli, M. Adaptive input representations for neural language modeling. arXiv preprint arXiv:1809.10853, 2018. Bowman et al. (2015) Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015. Chelba et al. (2013) Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. One billion word benchmark for measuring progress in statistical language modeling.",
    "sinkhorn-5": "2013. Child et al. (2019) Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.",
    "sinkhorn-6": "arXiv preprint arXiv:1904.10509, 2019. Dai et al. (2019) Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Guo et al. (2019) Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., and Zhang, Z. Star-transformer. arXiv preprint arXiv:1902.09113, 2019. Jang et al. (2016) Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax.",
    "sinkhorn-7": "arXiv preprint arXiv:1611.01144, 2016. Kitaev et al. (2020) Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer.",
    "sinkhorn-8": "In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB. Luong et al. (2015) Luong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.",
    "sinkhorn-9": "Maas et al. (2011) Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pp. 142\u2013150. Association for Computational Linguistics, 2011. Martins & Astudillo (2016) Martins, A. and Astudillo, R. From softmax to sparsemax: A sparse model of attention and multi-label classification.",
    "sinkhorn-10": "In International Conference on Machine Learning, pp. 1614\u20131623, 2016. Mena et al. (2018) Mena, G., Belanger, D., Linderman, S., and Snoek, J. Learning latent permutations with gumbel-sinkhorn networks.",
    "sinkhorn-11": "arXiv preprint arXiv:1802.08665, 2018. Parmar et al. (2018) Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, \u0141., Shazeer, N., Ku, A., and Tran, D. Image transformer.",
    "sinkhorn-12": "arXiv preprint arXiv:1802.05751, 2018. Qiu et al. (2020) Qiu, J., Ma, H., Levy, O., tau Yih, S. W., Wang, S., and Tang, J. Blockwise self-attention for long document understanding, 2020. URL https://openreview.net/forum?id=H1gpET4YDB. Rae et al. (2020) Rae, J. W., Potapenko, A., Jayakumar, S.",
    "sinkhorn-13": "M., Hillier, C., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylKikSYDH. Shazeer et al. (2017) Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.",
    "sinkhorn-14": "arXiv preprint arXiv:1701.06538, 2017. Shazeer et al. (2018) Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., et al. Mesh-tensorflow: Deep learning for supercomputers.",
    "sinkhorn-15": "In Advances in Neural Information Processing Systems, pp. 10414\u201310423, 2018. Shen et al. (2018a) Shen, T., Zhou, T., Long, G., Jiang, J., Wang, S., and Zhang, C. Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling.",
    "sinkhorn-16": "arXiv preprint arXiv:1801.10296, 2018a. Shen et al. (2018b) Shen, T., Zhou, T., Long, G., Jiang, J., and Zhang, C. Bi-directional block self-attention for fast and memory-efficient sequence modeling. arXiv preprint arXiv:1804.00857, 2018b. Sinkhorn (1964) Sinkhorn, R. A relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics, 35(2):876\u2013879, 1964. So et al. (2019) So, D. R., Liang, C., and Le, Q. V. The evolved transformer. arXiv preprint arXiv:1901.11117, 2019. Socher et al. (2013) Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality over a sentiment treebank.",
    "sinkhorn-17": "In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 1631\u20131642, 2013. Sutskever et al. (2014) Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. arXiv preprint arXiv:1409.3215, 2014. Tay et al. (2019) Tay, Y., Wang, S., Tuan, L. A., Fu, J., Phan, M.",
    "sinkhorn-18": "C., Yuan, X., Rao, J., Hui, S. C., and Zhang, A. Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives. arXiv preprint arXiv:1905.10847, 2019. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998\u20136008, 2017. Vaswani et al. (2018) Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A. N., Gouws, S., Jones, L., Kaiser, L., Kalchbrenner, N., Parmar, N., Sepassi, R., Shazeer, N., and Uszkoreit, J. Tensor2tensor for neural machine translation.",
    "sinkhorn-19": "CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416. Williams et al. (2017) Williams, A., Nangia, N., and Bowman, S. R. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017. Xu et al. (2015) Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., and Bengio, Y. Show, attend and tell: Neural image caption generation with visual attention.",
    "sinkhorn-20": "In International conference on machine learning, pp. 2048\u20132057, 2015. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Thu Mar 7 08:13:30 2024 by LaTeXML"
}