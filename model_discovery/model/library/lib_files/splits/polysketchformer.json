{
    "polysketchformer-0": "# PolySketchFormer: Fast Transformers via Sketching Polynomial Kernels \n\nPraneeth Kacham ${ }^{* 1}$ Vahab Mirrokni*2 Peilin Zhong*2\n\n\n#### Abstract\n\nThe quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of largescale Transformer-based language models.",
    "polysketchformer-1": "Recent theoretical results indicate the intractability of sub-quadratic softmax attention approximation under reasonable complexity assumptions. This paper addresses this challenge by first demonstrating that polynomial attention with high degree can effectively replace softmax without sacrificing model quality. Next, we develop polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees. Crucially, our approach achieves this speedup without requiring the sparsification of attention matrices. We also present a block-based algorithm to apply causal masking efficiently. Combining these techniques, we provide PolySketchFormer, a practical lineartime Transformer architecture for language modeling that offers provable guarantees. We validate PolySketchFormer empirically by training language models capable of handling long contexts. These experiments utilize both synthetic and real-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32 k and GPT- 2 style models, our model achieves $2 x$ speedup in training compared to FlashAttention of the fastest configuration, with no observed degradation in quality across our experiments. ${ }^{1}$\n\n[^0]Train Step latency per token\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_778cffb27f9dc74e6671g-01.jpg?height=367&width=722&top_left_y=673&top_left_x=1103)\n\nFigure 1. Train step latency per token in $\\mu \\mathrm{s} /$ token of GPT-2 small style models with softmax attention (FlashAttention) v.s.",
    "polysketchformer-2": "ours. Each model is trained with 1M tokens batches. Vanilla softmax attention goes out-of-memory (OOM) for context lengths $>8 \\mathrm{k}$. ## 1. Introduction\n\nTransformer-based models (Vaswani et al., 2017) are stateof-the-art for many natural language tasks, leading to breakthroughs in machine translation, language understanding (Devlin et al., 2019), and language modeling (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023; Anil et al., 2023). However, the quadratic time and space complexity of the attention mechanism limits scalability for long context lengths. Numerous \"efficient transformers\" have been proposed to address this issue (Wang et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2020; Han et al., 2023). These variants approximate ${ }^{2}$ the standard attention mechanism. A survey by Tay et al. (2022) provides a broad overview of these techniques. While many efficient transformer constructions achieve linear theoretical training complexity, the survey observes that practical training speedups are often less significant, with potential losses in model quality. This explains the continued dominance of vanilla transformers. In this work, we focus on improving training latency for transformer models in decoding-only tasks, specifically language modeling trained via next-word prediction. Our techniques are generalizable to encoding-only and encoderdecoder transformers, with potential applications beyond language modeling. We will first briefly discuss existing approaches to make training of transformer models faster and then place our contributions in context. [^1]Memory efficient and I/O aware approach. Recent work by (Dao et al., 2022; Dao, 2023) on FlashAttention and FlashAttention-2 seeks to enable vanilla transformer training on long contexts. This is achieved through I/O-aware optimizations like blocking/tiling and rematerialization, significantly improving memory efficiency. While this reduces the $O\\left(n^{2}\\right)^{3}$ HBM (High-Bandwidth Memory) requirements of accelerators (GPUs/TPUs), enabling training on thousands of tokens, the computational cost per step remains $O\\left(n^{2}\\right)$ (see Figure 1), and this remains a barrier to further scaling the context length. Approximate softmax attention via sparsification. Another line of work tries to approximate softmax attention and avoid $n \\times n$ attention matrix computation by focusing on a smaller set of pairs of query and key vectors. Techniques include utilizing locality/positional information (Child et al., 2019; Beltagy et al., 2020; Xiao et al., 2023; Zaheer et al., 2020; Roy et al., 2021; Ding et al., 2023), hashing/bucketing (Kitaev et al., 2020; Sun et al., 2021; Han et al., 2023), low-rank projection (Wang et al., 2020), or other sparsification methods. In these cases, there is usually some trade-off between model quality and sparsity, i.e., denser attentions improve quality but decrease speed. Hence, an efficient high-quality $n \\times n$ attention mechanism may potentially improve on these sparsification-based techniques. Efficient $n \\times n$ attention by kernel-based methods. The kernel-based view of attention was taken by a series of earlier works (Tsai et al., 2019; Katharopoulos et al., 2020; Choromanski et al., 2020; Peng et al., 2021). In particular, let $\\left\\{\\mathbf{q}_{i} \\in \\mathbb{R}^{h}\\right\\}_{i \\in[n]},\\left\\{\\mathbf{k}_{i} \\in \\mathbb{R}^{h}\\right\\}_{i \\in[n]},\\left\\{\\mathbf{v}_{i} \\in \\mathbb{R}^{h}\\right\\}_{i \\in[n]}$ be sets of query, key and value vectors respectively, the output of the attention mechanism for query $\\mathbf{q}_{i}$ is computed as $\\operatorname{Attn}\\left(\\mathbf{q}_{i},\\left\\{\\mathbf{k}_{j}\\right\\},\\left\\{\\mathbf{v}_{j}\\right\\}\\right)=\\sum_{j \\in[n]} \\frac{\\sigma\\left(\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right)}{\\sum_{j^{\\prime} \\in[n]} \\sigma\\left(\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right)} \\cdot \\mathbf{v}_{j}^{\\top}$. When the similarity kernel function $\\sigma(\\mathbf{x}, \\mathbf{y}):=\\exp (\\langle\\mathbf{x}, \\mathbf{y}\\rangle)$, the above attention is exactly the softmax attention ${ }^{4}$. If there exists a feature map $\\phi$ such that $\\sigma(\\mathbf{x}, \\mathbf{y}) \\equiv\\langle\\phi(\\mathbf{x}), \\phi(\\mathbf{y})\\rangle$, the attention output for query $\\mathbf{q}_{i}$ can be rewritten as:\n\n$$\n\\begin{aligned}\n\\operatorname{Attn}\\left(\\mathbf{q}_{i},\\left\\{\\mathbf{k}_{j}\\right\\},\\left\\{\\mathbf{v}_{j}\\right\\}\\right) & =\\sum_{j \\in[n]} \\frac{\\phi\\left(\\mathbf{q}_{i}\\right)^{\\top} \\cdot \\phi\\left(\\mathbf{k}_{j}\\right)}{\\sum_{j^{\\prime} \\in[n]} \\phi\\left(\\mathbf{q}_{i}\\right)^{\\top} \\cdot \\phi\\left(\\mathbf{k}_{j^{\\prime}}\\right)} \\cdot \\mathbf{v}_{j}^{\\top} \\\\\n& =\\frac{\\phi\\left(\\mathbf{q}_{i}\\right)^{\\top} \\cdot \\sum_{j \\in[n]} \\phi\\left(\\mathbf{k}_{j}\\right) \\cdot \\mathbf{v}_{j}^{\\top}}{\\phi\\left(\\mathbf{q}_{i}\\right)^{\\top} \\cdot \\sum_{j^{\\prime} \\in[n]} \\phi\\left(\\mathbf{k}_{j^{\\prime}}\\right)}\n\\end{aligned}\n$$\n\nIf $\\phi(\\cdot)$ has a finite dimension $h^{\\prime}$, one can first compute $\\sum_{j^{\\prime} \\in[n]} \\phi\\left(\\mathbf{k}_{j^{\\prime}}\\right)$ and $\\sum_{j \\in[n]} \\phi\\left(\\mathbf{k}_{j}\\right) \\cdot \\mathbf{v}_{j}^{\\top}$ in $O\\left(n h h^{\\prime}\\right)$ time, and then compute $\\operatorname{Attn}\\left(\\mathbf{q}_{i},\\left\\{\\mathbf{k}_{j}\\right\\},\\left\\{\\mathbf{v}_{j}\\right\\}\\right)$ for all $i \\in[n]$ in another $O\\left(n h h^{\\prime}\\right)$ time, which is linear in the context length $n$. Most of existing works such as (Katharopoulos et al., 2020; Bolya et al., 2022; Tsai et al., 2019; Babiloni et al., 2023;\n\n[^2]Yang et al., 2023; Kasai et al., 2021) only considers similarity functions $\\sigma(\\mathbf{x}, \\mathbf{y})$ with low dimensional feature mapping (e.g., $\\sigma(\\mathbf{x}, \\mathbf{y})=\\langle\\mathbf{x}, \\mathbf{y}\\rangle,\\langle\\mathbf{x}, \\mathbf{y}\\rangle^{2},\\langle\\mathrm{elu}(\\mathrm{x})+\\mathbf{1}, \\mathrm{elu}(\\mathbf{y})+\\mathbf{1}\\rangle$, etc.). Hua et al. (2022) proposed to use a mixed strategy based on the positions of the tokens: If positions $i, j \\in[n]$ are close enough, they use $\\sigma\\left(\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right)=\\operatorname{relu}^{2}\\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle\\right)$. Otherwise, they use $\\sigma\\left(\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right)=\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle$, which again has a low dimensional feature mapping. These simple similarity kernel functions $\\sigma(\\cdot)$ either suffer from some loss of model quality (Katharopoulos et al., 2020) or require additional tweaks of network structures (e.g., significantly increasing the number of attention layers (Hua et al., 2022), introducing decay factors for earlier tokens (Yang et al., 2023)) to achieve comparable model quality as softmax attention. Some other previous works try to approximate the regular softmax attention via approximate feature mappings for the exponential similarity function. Random Feature Attention (Peng et al., 2021) uses random Fourier features to produce an approximate feature mapping but without provable approximation guarantees. Performer (Choromanski et al., 2020) provides a low dimensional approximate non-negative feature mapping $\\phi^{\\prime}(\\cdot)$ via positive orthogonal random features. It has provable approximation to the pairwise similarities, i.e., the maximum error $\\max _{i, j \\in[n]}\\left|\\left\\langle\\phi^{\\prime}\\left(\\mathbf{q}_{i}\\right), \\phi^{\\prime}\\left(\\mathbf{k}_{j}\\right)\\right\\rangle-\\exp \\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle\\right)\\right|$ is small. However, the dimension of their feature mapping has to grow exponentially in $\\left\\|\\mathbf{q}_{i}\\right\\|_{2}^{2}$ and $\\left\\|\\mathbf{k}_{j}\\right\\|_{2}^{2}$ to have a small error. In other words, consider a single query $\\mathbf{q}_{i}$ and two keys $\\mathbf{k}_{j}$ and $\\mathbf{k}_{j^{\\prime}}$ such that all $\\left\\|\\mathbf{q}_{i}\\right\\|_{2},\\left\\|\\mathbf{k}_{j}\\right\\|_{2},\\left\\|\\mathbf{k}_{j^{\\prime}}\\right\\|_{2} \\leq R$, then $\\exp \\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle\\right) / \\exp \\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle\\right) \\leq \\exp \\left(2 R^{2}\\right)$. Thus, the maximum relative probability masses that can be assigned while guaranteeing the approximation factor is limited by the dimension of the feature mapping used. In fact, a recent work (Alman \\& Song, 2023) implies that it is actually impossible to get above approximation for pairwise exponential similarity under Strong Exponential Time Hypothesis (SETH (Impagliazzo et al., 2001)) when the query and key vectors have large entries. Furthermore, it was observed empirically (Choromanski et al., 2020; Hua et al., 2022) (also see Figure 2) that there is a clear model quality drop in comparison with the exact softmax attention. Given barriers above, a natural question arises: Does there exist a similarity kernel function that achieves similar model quality as softmax attention while also admitting proper approximation by a low-dimensional feature mapping? ### 1.1. Our Contributions\n\nPolynomial similarity kernel function of high degree. To tackle the first part of the above question, we explore the power of the polynomial kernel function $\\sigma(\\mathbf{x}, \\mathbf{y})=\\langle\\mathbf{x}, \\mathbf{y}\\rangle^{p}$ for large even degrees $p \\geq 4$ empirically for language modelling tasks. In particular, we look at the standard GPT-\n2 (Radford et al., 2019) architecture (from the small size to the large size) and the strongest known Transformer recipe (a.k.a. Transformer++) which is a common baseline model studied in many previous works as well (Hua et al., 2022; Gu \\& Dao, 2023; Yang et al., 2023). We compare the models with vanilla softmax attention to the models that simply replace the attention mechanism with degree- $p$ polynomial attention. We consider context lengths ranging from 512 to 32k. As shown in Figure 2 and our other empirical studies (see Section 4 and Appendix), for all synthetic tasks (including tasks for measuring content aware reasoning and memorization capabilities, see Appendix F), autoregressive pre-training metrics (perplexity) and few-shot evaluations that we studied, the models with degree- $p$ polynomial attention $(p \\geq 4)$ achieve comparable performance as the models with the vanilla softmax attention. In addition, we discuss the behavioral similarities between softmax attention and polynomial attention in Section 2.1 to provide more intuitions why they had similar empirical outcomes. Approximate feature mapping for polynomial kernel. Unlike exponential kernel whose exact feature mapping has infinite dimension, the feature mapping of degree- $p$ polynomial kernel over $\\mathbb{R}^{h}$ has a finite feature mapping of dimension $h^{p}$ (see e.g., (Avron et al., 2014)). In practice, the head size $h$ is usually 64,128 or even 256 (Chowdhery et al., 2023). Therefore, computing the exact feature mapping for $p \\geq 4$ is still expensive. To address this issue, we apply the sketching technique from the numerical linear algebra literature to compute a low-dimensional approximate feature mapping $\\phi^{\\prime}$ such that $\\left\\langle\\phi^{\\prime}(\\mathbf{x}), \\phi^{\\prime}(\\mathbf{y})\\right\\rangle \\approx\\langle\\mathbf{x}, \\mathbf{y}\\rangle^{p}$. Sketching polynomial kernels (Avron et al., 2014; Ahle et al., 2020; Song et al., 2021; Meister et al., 2019) has been extensively studied in the literature, and the techniques are used in many applications such as kernel regression (Song et al., 2021), kernel PCA (Avron et al., 2014), evaluating elementwise matrix functions (Han et al., 2020), and etc. However, though $\\langle\\mathbf{x}, \\mathbf{y}\\rangle^{p}$ is guaranteed to be non-negative for even integer $p$, none of the approximate feature mappings provided by previous work guarantees $\\left\\langle\\phi^{\\prime}(\\mathbf{x}), \\phi^{\\prime}(\\mathbf{y})\\right\\rangle \\geq 0$. This is undesired in practice since the original normalized attention weights $\\frac{\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{1}\\right\\rangle^{p}}{\\sum_{j \\in[n]}\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle^{p}}, \\frac{\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{2}\\right\\rangle^{p}}{\\sum_{j \\in[n]}\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle^{p}}, \\cdots, \\frac{\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{n}\\right\\rangle^{p}}{\\sum_{j \\in[n]}\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle^{p}}$ naturally represent a probability distribution, but the property does not hold when there exists some negative attention weight $\\left\\langle\\phi^{\\prime}\\left(\\mathbf{q}_{i}\\right), \\phi^{\\prime}\\left(\\mathbf{k}_{j}\\right)\\right\\rangle$. More importantly, previous work (Choromanski et al., 2020; Katharopoulos et al., 2020) found that negative attention weights make the training process unstable, potentially causing non-convergence. To address this issue, we open the construction of (Ahle et al., 2020) and develop an approximate feature mapping with desired non-negativity property. Theorem 1.1. Let $p \\geq 2$ be an even integer, $\\varepsilon \\in(0,0.5)$ be an error parameter. Let $h$ be the dimension of the vectors to be mapped. There is a randomized feature mapping $\\phi^{\\prime}$ : $\\mathbb{R}^{h} \\rightarrow \\mathbb{R}^{r^{2}}$ for $r=\\Theta\\left(p \\varepsilon^{-2} \\log 1 / \\delta\\right)$, such that given any set of vectors $\\left\\{\\mathbf{q}_{i} \\in \\mathbb{R}^{h}\\right\\}_{i \\in[n]},\\left\\{\\mathbf{k}_{j} \\in \\mathbb{R}^{h}\\right\\}_{i \\in[n]}$ :\n\n1. $\\forall i, j \\in[n],\\left\\langle\\phi^{\\prime}\\left(\\mathbf{q}_{i}\\right), \\phi^{\\prime}\\left(\\mathbf{k}_{j}\\right)\\right\\rangle \\geq 0$. 2. $\\sum_{i, j}\\left|\\left\\langle\\phi^{\\prime}\\left(\\mathbf{q}_{i}\\right), \\phi^{\\prime}\\left(\\mathbf{k}_{j}\\right)\\right\\rangle-\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle^{p}\\right|^{2} \\leq \\varepsilon^{2} \\sum_{i, j}\\left\\|\\mathbf{q}_{i}\\right\\|_{2}^{2 p}\\left\\|\\mathbf{k}_{j}\\right\\|_{2}^{2 p}$ holds with probability $1-\\delta$. 3. Computing $\\phi^{\\prime}(\\mathbf{x})$ only requires $p / 2$ matrix-vector multiplications of matrix size $h \\times r,(p / 2-2)$ matrixvector multiplications of matrix size $r \\times r,(p / 2-1)$ Hadamard products of $r$-dimensional vectors, and 1 self-Kronecker product of an r-dimensional vector. The first property above is the desired non-negativity property that we discussed earlier. We achieve this property by providing a \"self-tensoring\" technique stated in Theorem 2.4. The second property states our error bound. Unlike the approximation guarantee of (Choromanski et al., 2020), though our error bound is still in terms of $\\ell_{2}$ norms query and key vectors, it allows a larger ratio between attention weights due to the difference between exponential kernel and polynomial kernel (See more discussions in Appendix B). The third property implies that the computation of $\\phi^{\\prime}(\\cdot)$ only requires a small number of standard matrix/vector operations which can be implemented to run quickly on accelerators (GPUs/TPUs). Inspired by the literature of learned sketches (Hsu et al., 2019; Aamand et al., 2019), we also propose a heuristic which replaces each random projection matrix used in $\\phi^{\\prime}(\\cdot)$ constructed in Theorem 1.1 with a comparable size learnable multi-layer dense network. Since each random matrix used in $\\phi^{\\prime}(\\cdot)$ has size only $h \\times r$ or $r \\times r$, the number of parameters that we add to the model is negligible in comparison with the model size. We observe significant model quality improvements (see Figure 2) by learning the sketches through training instead of using randomly sampled sketches. Block-based lower triangular multiplication for handling causal masks. Another bottleneck in applying attention linearization techniques in training transformer models with causal masking on long contexts is to handle a huge number of sequential gradients update due to RNN-style sequential state updates (Hua et al., 2022). When causal masking is applied, the attention between the query $\\mathbf{q}_{i}$ and the key $\\mathbf{k}_{j}$ is masked out when $j>i$ (i.e., the $j$-th token appears later). More precisely, $\\operatorname{Attn}\\left(\\mathbf{q}_{i},\\left\\{\\mathbf{k}_{j}\\right\\},\\left\\{\\mathbf{v}_{j}\\right\\}\\right)=$\n\n$$\n\\sum_{j \\in[i]} \\frac{\\sigma\\left(\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right)}{\\sum_{j^{\\prime} \\in[i]} \\sigma\\left(\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right)} \\cdot \\mathbf{v}_{j}^{\\top}=\\frac{\\phi\\left(\\mathbf{q}_{i}\\right)^{\\top} \\cdot \\sum_{j \\in[i]} \\phi\\left(\\mathbf{k}_{j}\\right) \\cdot \\mathbf{v}_{j}^{\\top}}{\\phi\\left(\\mathbf{q}_{i}\\right)^{\\top} \\cdot \\sum_{j^{\\prime} \\in[i]} \\phi\\left(\\mathbf{k}_{j^{\\prime}}\\right)}\n$$\n\nDuring the training, to compute the output of the attention mechanism in time linear in context length, one has to compute the prefix sums $\\sum_{j \\in[i]} \\phi\\left(\\mathbf{k}_{j}\\right) \\cdot \\mathbf{v}_{j}^{\\top}$ for all $i$ and then multiply the $i$-th prefix sum with the corresponding vector\n$\\phi\\left(\\mathbf{q}_{i}\\right)^{\\top}$. This RNN-style sequential state updates make the training process fail in fully utilizing the parallelism strength of modern accelerators. To resolve above issue, we propose a general block-based approach to compute $\\mathrm{It}_{\\triangle}\\left(\\mathbf{A} \\cdot \\mathbf{B}^{\\top}\\right) \\cdot \\mathbf{C}^{5}$ for arbitrary matrices $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$ without materializing $\\mathbf{A} \\cdot \\mathbf{B}^{\\top}$, and it only requires a small number of prefix updates. By working more carefully with our block-based approach, we observe that instead of using the approximate polynomial attention weight via approximate feature mapping, we are able to compute the exact polynomial attention weight between $\\mathbf{q}_{i}$ and $\\mathbf{k}_{j}$ if the $i$-th token and the $j$-th token are close in position. After applying exact polynomial attention weight for local tokens, we see improvements in model qualities (see Figure 2, Section 4 and other empirical results in Appendix). Empirical studies. We empirically evaluate all our approaches. The models equipped with high degree polynomial attention and sketched polynomial attention achieve comparable or better quality on all our evaluation metrics in comparison with models equipped with vanilla softmax attention, and achieve significantly better quality than models with approximate softmax attention provided by Performer (Choromanski et al., 2020). For GPT-2 style small size models, the models with sketched polynomial attention achieve 2x speedup in comparison with FlashAttention (Dao et al., 2022; Dao, 2023) of the fastest configuration for 32k context length.",
    "polysketchformer-3": "Notice that we achieve such speed-up without applying any advanced I/O aware optimization techniques. We believe that our running time can be further reduced by optimizing the implementation in a more careful manner. ### 1.2. Other Notation\n\n[ $n$ ] denotes the set $\\{1,2,3, \\cdots, n\\}$. Given a matrix $\\mathbf{M} \\in$ $\\mathbb{R}^{n \\times m}$, we use $\\mathbf{m}_{i} \\in \\mathbb{R}^{m}$ to denote the $i$-th row of $\\mathbf{M}$. We also abuse the notation to use $\\mathbf{M}$ to indicate the set of vectors $\\left\\{\\mathbf{m}_{1}, \\mathbf{m}_{2}, \\cdots, \\mathbf{m}_{n}\\right\\}$. We use $\\mathbf{M}_{i, j}$ to denote the entry at the $i$-th row and $j$-th column of $\\mathbf{M}$. We use $\\mathbf{M}^{p}$ to indicate raising each entry of $\\mathbf{M}$ to the power $p$. Let $f: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}$ be an arbitrary function over vectors, we use $f(\\mathbf{M}) \\in \\mathbb{R}^{n \\times k}$ to denote the matrix obtained by replacing the $i$-th row of $\\mathbf{M}$ with $f\\left(\\mathbf{m}_{i}\\right) .\\|\\mathbf{x}\\|_{2}$ denotes the $\\ell_{2}$ norm of $\\mathbf{x}$, i.e., $\\sqrt{\\langle\\mathbf{x}, \\mathbf{x}\\rangle} \\cdot\\|\\mathbf{M}\\|_{\\mathrm{F}}$ denotes the Frobenius norm of $\\mathbf{M}$, i.e., $\\sqrt{\\sum_{i, j} \\mathbf{M}_{i, j}^{2}}$. Given vectors $\\mathbf{a}=\\left(a_{1}, a_{2}, \\cdots, a_{m}\\right)$ and $\\mathbf{b}=\\left(b_{1}, b_{2}, \\cdots, b_{m}\\right), \\mathbf{a} * \\mathbf{b}=\\left(a_{1} b_{1}, a_{2} b_{2}, \\cdots, a_{m} b_{m}\\right)$ denotes the entrywise product (Hadamard product), and $\\mathbf{a} \\otimes$ $\\mathbf{b}=\\left(a_{1} b_{1}, a_{1} b_{2}, \\cdots, a_{1} b_{m}, a_{2} b_{1}, a_{2} b_{2}, \\cdots, a_{2} b_{m}, \\cdots, a_{m} b_{m}\\right)$ denotes the Kronecker product. $\\operatorname{diag}(\\mathbf{a})$ denotes a diagonal matrix where the $i$-th diagnoal entry is $a_{i} . \\mathbf{A} * \\mathbf{B}$ de-\n\n[^3]notes the entrywise product between matrices $\\mathbf{A}$ and $\\mathbf{B}$. We define \"self-tensoring\" $\\mathbf{a}^{\\otimes p}:=\\mathbf{a} \\otimes \\mathbf{a}^{\\otimes(p-1)} \\in \\mathbb{R}^{m^{p}}$ where $\\mathbf{a}^{\\otimes 1}:=\\mathbf{a} . \\mathbf{A}^{\\otimes p}$ indicates replacing each row $\\mathbf{a}_{i}$ of $\\mathbf{A}$ with $\\mathbf{a}_{i}^{\\otimes p}$. It $\\mathbf{t}_{\\triangle}(\\mathbf{M})$ denotes the matrix obtained by only keeping the lower-triangular entries of $\\mathbf{M}$ and zeroing the remaining.",
    "polysketchformer-4": "$\\mathbf{1}_{m} \\in \\mathbb{R}^{m}$ denotes an all-one vector. ## 2. Polynomial Attention and Approximation\n\nWe discuss the polynomial attention in more detail in Section 2.1 and introduce the sketching techniques (Section 2.2, 2.3) for efficiently approximating the polynomial attention. We ignore causal masking in this section, and present how to efficiently handle causal masking in Section 3. ### 2.1. Softmax versus Normalized Polynomial\n\nLet us revisit the softmax attention. Given sets of query, key vectors $\\mathbf{Q}=\\left\\{\\mathbf{q}_{i}\\right\\}_{i \\in[n]}, \\mathbf{K}=\\left\\{\\mathbf{k}_{i}\\right\\}_{i \\in[n]} \\subset \\mathbb{R}^{h}$, and scaling parameter $\\beta>0$, bias parameter $\\alpha \\in \\mathbb{R}$, the normalized softmax attention weight between $\\mathbf{q}_{i}$ and $\\mathbf{k}_{j}$ is:\n\n$$\n\\mathbf{A}_{i, j}=\\frac{\\exp \\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle / \\beta-\\alpha\\right)}{\\sum_{j^{\\prime} \\in[n]} \\exp \\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle / \\beta-\\alpha\\right)}\n$$\n\nNote $\\mathbf{A}_{i, j}$ is invariant in $\\alpha$.",
    "polysketchformer-5": "In practice, $\\alpha$ is usually chosen to be $\\max _{j^{\\prime} \\in[n]}\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle / \\beta$ to make the computation of both numerator and denominator numerically stable. $\\beta$ is a smoothness factor. When $\\beta \\rightarrow \\infty$, then $\\mathbf{A}_{i, j} \\rightarrow 1 / n$, i.e., the $i$-th row of $\\mathbf{A}$ indicates a uniform distribution over all $j \\in[n]$. When $\\beta \\rightarrow 0$, then $\\mathbf{A}_{i, j} \\rightarrow\\left\\{\\begin{array}{ll}0 & \\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle \\neq \\max _{j^{\\prime} \\in[n]}\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle \\\\ 1 / a & \\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle=\\max _{j^{\\prime} \\in[n]}\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle\\end{array} \\quad\\right.$ where $a$ is the number of $j$ satisfying $\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle=\\max _{j^{\\prime} \\in[n]}\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle$, i.e., the $i$-th row of $\\mathbf{A}$ indicates a uniform distribution only over $j$ that provides the maximum inner product. The standard $\\beta$ is chosen to be $\\sqrt{h}$ (Vaswani et al., 2017). Interestingly, normalized polynomial function has a similar behavior of the interpolation nature between the uniform distribution and the argmax distribution discussed above. In particular, let $p$ be an even integer and consider the following normalized weight computed between $\\mathbf{q}_{i}$ and $\\mathbf{k}_{j}$ :\n\n$$\n\\frac{\\left(\\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle+\\alpha\\right) / \\beta\\right)^{p}}{\\sum_{j^{\\prime} \\in[n]}\\left(\\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle+\\alpha\\right) / \\beta\\right)^{p}}\n$$\n\nIt is clear the weight is invariant for different $\\beta$. Choosing a proper $\\beta$ can make both numerator and denominator fall in a reasonable range and make the computation numerically stable. When $\\alpha \\rightarrow \\infty$, the weight is close to $1 / n$, i.e., these weights provide a uniform distribution. When $\\alpha \\geq-\\min _{j^{\\prime} \\in[n]}\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle$ and $p \\rightarrow \\infty$, the weight is close to 0 if $\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle$ does not provide the maximum inner product, and the weight is close to $1 / a$ otherwise, where $a$ is the number of $\\mathbf{k}_{j}$ that provides the maximum inner product. ![](https://cdn.mathpix.com/cropped/2024_09_12_778cffb27f9dc74e6671g-05.jpg?height=499&width=1578&top_left_y=243&top_left_x=293)\n\nFigure 2. Pre-training metric (perplexities). Lower is better. GPT-2 small style models with various attention mechanisms are trained on PG-19 and Wiki-40B datasets at different context lengths up to 32k. Each batch contains 1M tokens in total. Polynomial attention with $p \\geq 4$ has comparable model quality as softmax attention but OOM'ed when context length $>8 \\mathrm{k}$. Polysketch attention (equipped with learned sketches (Section 2.3) + local exact polynomial attention (Section 3.2)) consistently outperforms all other mechanisms. The parameter $r$ denotes the sketch size (see formal definition in Section 2.2). See Tables 2 and 3 in the Appendix for a full list of perplexity values. Observe that if $\\left\\langle\\mathbf{q}_{i}, \\mathbf{1}_{h}\\right\\rangle=\\left\\langle\\mathbf{k}_{j}, \\mathbf{1}_{h}\\right\\rangle=0$ for all $i, j \\in[n]$, i.e., entries of $\\mathbf{q}_{i}, \\mathbf{k}_{j}$ always have mean 0 , then we have $\\forall i, j \\in[n],\\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle+\\alpha\\right) / \\beta=\\left\\langle\\mathbf{q}_{i}^{\\prime}, \\mathbf{k}_{j}^{\\prime}\\right\\rangle$, where $\\mathbf{q}_{i}^{\\prime}=$ $\\mathbf{q}_{i} / \\sqrt{\\beta}+\\sqrt{\\alpha /(\\beta h)} \\cdot \\mathbf{1}_{h}$, and $\\mathbf{k}_{j}^{\\prime}=\\mathbf{k}_{j} / \\sqrt{\\beta}+\\sqrt{\\alpha /(\\beta h)} \\cdot \\mathbf{1}_{h}$, i.e., $\\mathbf{q}_{i}^{\\prime}$ and $\\mathbf{k}_{j}^{\\prime}$ are obtained by applying the same rescaling and bias to $\\mathbf{q}_{i}$ and $\\mathbf{k}_{j}$ respectively. Motivated by the above observation, we slightly tweak Equation 1 by applying an additional layer normalization ${ }^{6}$ (Ba et al., 2016) to $\\left\\{\\mathbf{q}_{i}\\right\\},\\left\\{\\mathbf{k}_{j}\\right\\}$, this gives the normalized degree- $p$ polynomial attention weight matrix $\\mathbf{A}^{(p)}$ considered in this paper:\n\n$$\n\\mathbf{A}_{i, j}^{(p)}=\\frac{\\left\\langle\\mathbf{q}_{i}^{\\prime}, \\mathbf{k}_{j}^{\\prime}\\right\\rangle^{p}}{1+\\sum_{j^{\\prime} \\in[n]}\\left\\langle\\mathbf{q}_{i}^{\\prime}, \\mathbf{k}_{j^{\\prime}}^{\\prime}\\right\\rangle^{p}}\n$$\n\nwhere $\\mathbf{q}_{i}^{\\prime}, \\mathbf{k}_{j}^{\\prime}$ are obtained by applying the layer normalization layer to $\\mathbf{q}_{i}, \\mathbf{k}_{j}$ respectively. Unlike softmax attention matrix, it is possible that the term $\\sum_{j^{\\prime} \\in[n]}\\left\\langle\\mathbf{q}_{i}^{\\prime}, \\mathbf{k}_{j^{\\prime}}^{\\prime}\\right\\rangle^{p}$ is (close to) 0 . We add 1 to the denominator to avoid dividing by zero. Given value vectors $\\mathbf{V}=\\left\\{\\mathbf{v}_{i}\\right\\}_{i \\in[n]} \\subset \\mathbb{R}^{h}$, the full degree$p$ polynomial attention $\\operatorname{Attn}^{(p)}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\mathbf{A}^{(p)} \\cdot \\mathbf{V}=$ $\\mathbf{D}^{-1} \\cdot\\left(\\mathbf{Q}^{\\prime} \\mathbf{K}^{\\prime \\top}\\right)^{p} \\cdot \\mathbf{V}$, where $\\mathbf{D}=\\operatorname{diag}\\left(\\mathbf{1}_{n}+\\left(\\mathbf{Q}^{\\prime} \\mathbf{K}^{\\prime \\top}\\right)^{p} \\mathbf{1}_{n}\\right)$. In the rest of the paper, we abuse notation between $\\mathbf{Q}, \\mathbf{K}$ and $\\mathbf{Q}^{\\prime}, \\mathbf{K}^{\\prime}$, and only consider $\\mathbf{Q}, \\mathbf{K}$ after layer normalization. As presented in Figure 2 and other experiments in Section 4 and Appendix, the models with the degree- $p$ polynomial attention described above achieve comparable model quality as vanilla softmax attention on all metrics as long as $p \\geq 4$. To test the long range learning capabilities and in-context learning capabilities of attention mechanisms, we study the synthetic tasks of Selective Copying (Gu \\& Dao, 2023) and Induction heads (Olsson et al., 2022). The models with\n\n[^4]polynomial attention for $p \\geq 4$ perform as well as models with softmax attention (see Appendix F for more details). ### 2.2. Random Sketches for Polynomial Attention with Theoretical Gaurantees\n\nTo compute $\\operatorname{Attn}^{(p)}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})$, we only need to compute $\\left(\\mathbf{Q K}^{\\top}\\right)^{p} \\cdot \\mathbf{V}$ and $\\left(\\mathbf{Q K}^{\\top}\\right)^{p} \\cdot \\mathbf{1}_{n}$. Let us only focus on computing $\\left(\\mathbf{Q K}^{\\top}\\right)^{p} \\cdot \\mathbf{V}$ since we can handle $\\left(\\mathbf{Q} \\mathbf{K}^{\\top}\\right)^{p} \\cdot \\mathbf{1}_{n}$ in the same way. Due to a well-known fact $\\forall \\mathbf{x}, \\mathbf{y},\\langle\\mathbf{x}, \\mathbf{y}\\rangle^{p}=$ $\\left\\langle\\mathbf{x}^{\\otimes p}, \\mathbf{y}^{\\otimes p}\\right\\rangle$, we have $\\left(\\mathbf{Q} \\mathbf{K}^{\\top}\\right)^{p} \\mathbf{V}=\\mathbf{Q}^{\\otimes p}\\left(\\mathbf{K}^{\\otimes p}\\right)^{\\top}$. V. If we reorder the computation and compute $\\left(\\mathbf{K}^{\\otimes p}\\right)^{\\top} \\cdot \\mathbf{V}$ first, we are able to compute $\\mathbf{Q}^{\\otimes p} \\cdot\\left(\\mathbf{K}^{\\otimes p}\\right)^{\\top} \\cdot \\mathbf{V}$ in $O\\left(n h^{p+1}\\right)$ time which is linear in the context length $n$. However $h^{p+1}$ dependence is still expensive as we explained in Section 1.1. Thus, we resort to approximating $\\mathbf{Q}^{\\otimes p}\\left(\\mathbf{K}^{\\otimes p}\\right)^{\\top}$ using sketching techniques, which we formally describe ahead. We first state the definition of a sketch that has the \"Approximate Matrix Multiplication (AMM)\" guarantee. Definition 2.1 (Approximate Matrix Multiplication (Woodruff et al., 2014)). Given parameters $n$, $h$ and $p$, a randomized sketching matrix $\\mathbf{S} \\in \\mathbb{R}^{h^{p} \\times r}$ has the $(\\varepsilon, p)$-AMM property if given any two $n \\times h$ matrices $\\mathbf{A}$ and $\\mathbf{B}$, with probability $\\geq 9 / 10$ over the randomized sketching matrix $S$, we have that $\\left\\|\\left(\\mathbf{A}^{\\otimes p} \\mathbf{S}\\right)\\left(\\mathbf{B}^{\\otimes p} \\mathbf{S}\\right)^{\\top}-\\mathbf{A}^{\\otimes p}\\left(\\mathbf{B}^{\\otimes p}\\right)^{\\mathrm{T}}\\right\\|_{\\mathrm{F}} \\leq \\varepsilon\\left\\|\\mathbf{A}^{\\otimes p}\\right\\|_{\\mathrm{F}}\\left\\|\\mathbf{B}^{\\otimes p}\\right\\|_{\\mathrm{F}}$. The parameter $r$ above is referred to as the sketch size. Two important properties of a sketching distribution are (i) the sketch size $r$ as a function of the accuracy parameter $\\varepsilon$ and (ii) the time required to compute $\\mathbf{A}^{\\otimes p} \\mathbf{S}$ given an arbitrary matrix $\\mathbf{A}$. Ideally, we want the matrix $\\mathbf{S}$ to have a structure such that $\\mathbf{A}^{\\otimes p} \\mathbf{S}$ can be computed without realizing the large matrix $\\mathbf{A}^{\\otimes p}$. Ahle et al. (2020) gave constructions of differ-\n\n```\nAlgorithm 1 Polynomial Sketches\n    function PolySKETCHWITHNEGATIVItY \\(\\left(\\mathbf{A} \\in \\mathbb{R}^{k \\times m}, r, p\\right)\\)\n        // Implementation of Theorem 2.2 (Ahle et al., 2020). // The output computes \\(\\mathbf{A}^{\\otimes p} \\mathbf{S}\\). If \\(p=1\\), return \\(\\mathbf{A}\\)\n        \\(\\mathbf{M}_{1}=\\operatorname{POLYSKETCHWithNEGATIVITY}(\\mathbf{A}, r, p / 2)\\)\n        \\(\\mathbf{M}_{2}=\\operatorname{POLYSKETCHWITHNEGATIVITY}(\\mathbf{A}, r, p / 2\\) )\n        Sample Gaussian matrices \\(\\mathbf{G}_{1}, \\mathbf{G}_{2}\\), each of \\(r\\) columns\n        Return \\(\\sqrt{1 / r} \\cdot\\left[\\left(\\mathbf{M}_{1} \\mathbf{G}_{1}\\right) *\\left(\\mathbf{M}_{2} \\mathbf{G}_{2}\\right)\\right] \\in \\mathbb{R}^{k \\times r}\\)\n    end function\n    function PolySKetchNonNEgative \\(\\left(\\mathbf{A} \\in \\mathbb{R}^{k \\times m}, r, p\\right)\\)\n        // Our approach based on Theorem 2.4. // The output computes \\(\\phi^{\\prime}(\\mathbf{A})=\\left(\\mathbf{A}^{\\otimes(p / 2)} \\mathbf{S}\\right)^{\\otimes 2}\\) where \\(\\phi^{\\prime}(\\cdot)\\)\n        is the same mapping as mentioned in Theorem 1.1. \\(\\mathbf{M}=\\operatorname{POLYSKETCHWithNEGATIVity}(\\mathbf{A}, r, p / 2)\\)\n        Return \\(\\mathbf{M}^{\\otimes 2} \\in \\mathbb{R}^{k \\times r^{2}}\\). end function\n```\n\nent sketches that have both the properties that the sketch size $r$ is small and the matrix $\\mathbf{A}^{\\otimes p} \\mathbf{S}$ can be computed quickly. We describe the main properties of one of their sketches below and explain how to compute $\\mathbf{A}^{\\otimes p} \\mathbf{S}$. Theorem 2.2 ((Ahle et al., 2020)). Given $p$ and $\\varepsilon$, there is a sketching matrix $\\mathbf{S}$ with $r=\\Theta\\left(p / \\varepsilon^{2}\\right)$ columns such that $\\mathbf{S}$ satisfies the $(\\varepsilon, p)$-AMM property (Definition 2.1). Given an arbitrary vector $\\mathbf{a} \\in \\mathbb{R}^{h}$, computing $\\left(\\mathbf{a}^{\\otimes p}\\right)^{\\top} \\mathbf{S}$ only requires p matrix-vector multiplications of matrix size $h \\times r,(p-2)$ matrix-vector multiplications of matrix size $r \\times r$, and $(p-1)$ Hadamard products of $r$-dimensional vectors. To compute $\\mathbf{A}^{\\otimes p} \\mathbf{S}$, we only need to compute $\\left(\\mathbf{a}_{i}^{\\otimes p}\\right)^{\\top} \\mathbf{S}$ for each row $\\mathbf{a}_{i}$ of $\\mathbf{A}$. The number of matrix-vector multiplications and Hadamard products scales linearly in $n$. Let us focus on the construction of the sketch described in Theorem 2.2. We now explain how the sketch computation works for $p=2$ and how it is extended to general values of $p$ that are powers of 2 . Let $\\mathbf{G}_{1} \\in \\mathbb{R}^{h \\times r}$ and $\\mathbf{G}_{2} \\in \\mathbb{R}^{h \\times r}$ denote two independently sampled random Gaussian matrices, i.e., each entry is drawn indepenently from a standard Gaussian distribution. Then the outcome of applying the sketch on $\\mathbf{A}^{\\otimes 2}$ is $\\mathbf{A}^{\\otimes 2} \\mathbf{S}=\\sqrt{1 / r} \\cdot\\left[\\left(\\mathbf{A G}_{1}\\right) *\\left(\\mathbf{A G}_{2}\\right)\\right]$. The construction extends to all $p$ that are powers of 2 in a recursive way. PolySkEtchWithNEGATivity (A, $r, p$ ) (Algorithm 1) shows how to compute $\\mathbf{A}^{\\otimes p} \\mathbf{S}$ in general. The polynomial sketch described above can be used to approximate the matrix $\\left(\\mathbf{Q K}^{\\boldsymbol{\\top}}\\right)^{p}=\\mathbf{Q}^{\\otimes p}\\left(\\mathbf{K}^{\\otimes p}\\right)^{\\top}$ with $\\left(\\mathbf{Q}^{\\otimes p} \\mathbf{S}\\right)\\left(\\mathbf{K}^{\\otimes p} \\mathbf{S}\\right)^{\\top}$. However one issue is that they do not preserve nonnegativity: while for even $p$, the entries of the matrix $\\left(\\mathbf{Q K}^{\\top}\\right)^{p}$ are nonnegative, the entries of the matrix $\\left(\\mathbf{Q}^{\\otimes p} \\mathbf{S}\\right)\\left(\\mathbf{K}^{\\otimes p} \\mathbf{S}\\right)^{\\top}$ can be negative. This is not desired as discussed in Section 1.1. In the following, we propose a novel but simple approach to address this negativity issue. Consider two arbitrary vectors $\\mathbf{a}, \\mathbf{b}$, we can see that the dot product $\\left\\langle\\mathbf{a}^{\\otimes 2}, \\mathbf{b}^{\\otimes 2}\\right\\rangle=\\langle\\mathbf{a}, \\mathbf{b}\\rangle^{2} \\geq 0$. Thus, given matrices $\\mathbf{Q}^{\\otimes(p / 2)} \\mathbf{S}$ and $\\mathbf{K}^{\\otimes(p / 2)} \\mathbf{S}$, consider the matrix $\\left(\\mathbf{Q}^{\\otimes(p / 2)} \\mathbf{S}\\right)^{\\otimes 2}\\left(\\left(\\mathbf{K}^{\\otimes(p / 2)} \\mathbf{S}\\right)^{\\otimes 2}\\right)^{\\top}$. Since all the entries of the matrix are of the form $\\left\\langle\\mathbf{a}^{\\otimes 2}, \\mathbf{b}^{\\otimes 2}\\right\\rangle$ for some vectors $\\mathbf{a}, \\mathbf{b}$, all the entries of the matrix $\\left(\\mathbf{Q}^{\\otimes(p / 2)} \\mathbf{S}\\right)^{\\otimes 2}\\left(\\left(\\mathbf{K}^{\\otimes(p / 2)} \\mathbf{S}\\right)^{\\otimes 2}\\right)^{\\top}$ are nonnegative as well. The \"self-tensoring\" trick ensures that all the entries in the approximate attention matrix are nonnegative at the cost of squaring the sketch size $r$. Although $\\left(\\mathbf{Q}^{\\otimes(p / 2)} \\mathbf{S}\\right)^{\\otimes 2}\\left(\\left(\\mathbf{K}^{\\otimes(p / 2)} \\mathbf{S}\\right)^{\\otimes 2}\\right)^{\\top}$ guarantees nonnegative property, it is not clear whether it is still a good approximation to $\\left(\\mathbf{Q} \\mathbf{K}^{\\top}\\right)^{p}$ given that $\\mathbf{S}$ is a polynomial sketch for degree $p / 2$. One of our technical contributions is to provide a non-trivial analysis to show that it still provides a good approximation when the sketching matrix $\\mathbf{S}$ is constructed as in (Ahle et al., 2020). The key is Theorem 2.4 which shows that a degree $p / 2$ polynomial sketch followed by \"self-tensoring\" gives a degree $p$ polynomial sketch. To state Theorem 2.4 properly, we need to briefly introduce following concepts. The $(\\varepsilon, \\delta, t)$-JL moment property is defined as follows. Given a scalar random variable $\\mathbf{X}$ and $t \\geq$ $1,\\|\\mathbf{X}\\|_{L^{t}}$ is defined to be $\\mathbf{E}\\left[|\\mathbf{X}|^{t}\\right]^{1 / t} \\cdot\\|\\cdot\\|_{L^{t}}$ defines a norm over random variables defined over the same sample space and in particular satisfies $\\|\\mathbf{X}+\\mathbf{Y}\\|_{L^{t}} \\leq\\|\\mathbf{X}\\|_{L^{t}}+\\|\\mathbf{Y}\\|_{L^{t}}$. Definition 2.3 (JL-moment property (Woodruff et al., 2014)). Given $\\varepsilon, \\delta \\geq 0, t \\geq 1$, a random matrix $\\mathbf{S}^{m \\times r}$ has the $(\\varepsilon, \\delta, t)$-JL moment property if for any $\\mathbf{x} \\in \\mathbb{R}^{m}$ with $\\|\\mathbf{x}\\|_{2}=1,\\|\\| \\mathbf{x}^{\\top} \\mathbf{S}\\left\\|_{2}^{2}-1\\right\\|_{L^{t}} \\leq \\varepsilon \\cdot \\delta^{1 / t}$. Theorem 2.4. Let $\\mathbf{S} \\in \\mathbb{R}^{h^{p / 2} \\times r}$ be a random sketch satisfying the $(\\varepsilon, \\delta, t)$-JL moment and $(\\varepsilon, \\delta, 2 t)$-JL moment properties for some even integer $t$. Given matrices $\\mathbf{C}, \\mathbf{D}$ with $h^{p / 2}$ columns, $\\left\\|(\\mathbf{C S})^{\\otimes 2}\\left((\\mathbf{D S})^{\\otimes 2}\\right)^{\\top}-\\mathbf{C}^{\\otimes 2}\\left(\\mathbf{D}^{\\otimes 2}\\right)^{\\top}\\right\\|_{\\mathrm{F}} \\leq$ $\\sqrt{5} \\varepsilon\\left\\|\\mathbf{C}^{\\otimes 2}\\right\\|_{\\mathrm{F}}\\left\\|\\mathbf{D}^{\\otimes 2}\\right\\|_{\\mathrm{F}}$ holds with probability $\\geq 1-\\delta$,\n\nDue to the page limit, we defer the proof to Appendix C. Proof of Theorem 1.1. Results from Section 4 of (Ahle et al., 2020) implies that the polynomial sketch $\\mathbf{S}$ as mentioned in Theorem 2.2 for degree $p / 2$ with sketch size $r=\\Theta\\left(p / \\varepsilon^{2}\\right)$ satisfies the requirements of Theorem 2.4. By plugging $\\mathbf{Q}^{\\otimes(p / 2)}, \\mathbf{K}^{\\otimes(p / 2)}$ into $\\mathbf{C}, \\mathbf{D}$ of Theorem 2.4 respectively and scaling $\\varepsilon$ properly, we obtain $\\left\\|\\left(\\mathbf{Q}^{\\otimes(p / 2)} \\mathbf{S}\\right)^{\\otimes 2}\\left(\\left(\\mathbf{K}^{\\otimes(p / 2)} \\mathbf{S}\\right)^{\\otimes 2}\\right)^{\\top}-\\left(\\mathbf{Q K}^{\\top}\\right)^{p}\\right\\|_{\\mathbf{F}} \\leq$ $\\varepsilon\\left\\|\\mathbf{Q}^{\\otimes p}\\right\\|_{\\mathbf{F}}\\left\\|\\mathbf{K}^{\\otimes p}\\right\\|_{\\mathrm{F}}$ which concludes Theorem 1.1, i.e., the approximate feature mapping $\\phi^{\\prime}(\\mathbf{x})=\\left(\\left(\\mathbf{x}^{\\otimes(p / 2)}\\right)^{\\top} \\mathbf{S}\\right)^{\\otimes 2} \\epsilon$ $\\mathbb{R}^{r^{2}}$ and $\\phi^{\\prime}(\\mathbf{Q}), \\phi^{\\prime}(\\mathbf{K})$ can be efficiently computed using POLYSKETCHNONNEGATIVE $(\\cdot, r, p)$ (see Algorithm 1). Using $\\phi^{\\prime}(\\cdot)$, we get the following approximate polynomial attention $\\widehat{\\mathbf{A t t n}}^{(p)}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\tilde{\\mathbf{D}}^{-1} \\phi^{\\prime}(\\mathbf{Q}) \\phi^{\\prime}(\\mathbf{K})^{\\top} \\mathbf{V}$, where $\\tilde{\\mathbf{D}}=\\operatorname{diag}\\left(\\mathbf{1}_{n}+\\phi^{\\prime}(\\mathbf{Q}) \\phi^{\\prime}(\\mathbf{K})^{\\top} \\mathbf{1}_{n}\\right)$. We call this attention mechanism Polysketch attention. ![](https://cdn.mathpix.com/cropped/2024_09_12_778cffb27f9dc74e6671g-07.jpg?height=478&width=811&top_left_y=219&top_left_x=207)\n\nFigure 3. Block wise Lower Triangular Multiplication. $\\mathbf{A}_{l}, \\mathbf{B}_{l}$, $\\mathbf{C}_{l}$ are blocks of $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$.",
    "polysketchformer-6": "Each block has $b=n / t$ rows. ### 2.3. Learnable Sketches for Polynomial Attention\n\nThere are only $(p-2)$ random projections where each is introduced by a matrix multiplication with a small Gaussian matrix $\\left(\\mathbf{G}_{1}, \\mathbf{G}_{2}\\right.$ in each recursion call in Algorithm 1) of size either $h \\times r$ or $r \\times r$ during the recursive computation of $\\phi^{\\prime}(\\mathbf{X})=\\operatorname{POLYSKETCHNONNEGATIVE}(\\mathbf{X}, r, p)$ (Algorithm 1) for $\\mathbf{X} \\in \\mathbb{R}^{n \\times h}$. Inspired by the literature of learned sketches (Hsu et al., 2019; Aamand et al., 2019), a natural idea is to replace each random matrix $\\mathbf{G}_{1}, \\mathbf{G}_{2}$ in Algorithm 1 with learnable parameters. In practice, we found that replacing each of these random projections with a learnable non-linear transformation introduced by a dense neural network with size comparable to $\\mathbf{G}_{1}, \\mathbf{G}_{2}$ achieves a better model quality. We describe more details of our network structure for the learnable non-linear transformation in Appendix D. We also evaluate models with Polysketch attention with learned sketches on induction heads and selective copying synthetic tasks. We find that the models perform as well as models with softmax attention (See Appendix F). ## 3. Dealing with Causal Masks\n\nWhen considering causal masks, the Polysketch attention with respect to $\\mathbf{q}_{i}$ is $\\sum_{j \\leq i} \\frac{\\left\\langle\\phi^{\\prime}\\left(\\mathbf{q}_{i}\\right), \\phi^{\\prime}\\left(\\mathbf{k}_{j}\\right)\\right\\rangle}{\\left.1+\\sum_{j^{\\prime} \\leq i} \\phi^{\\prime}\\left(\\mathbf{q}_{i}\\right), \\phi^{\\prime}\\left(\\mathbf{k}_{j^{\\prime}}\\right)\\right\\rangle} \\cdot \\mathbf{v}_{j}^{\\top}$. In this causal case, the full Polysketch attention can be written as $\\widetilde{\\mathbf{A t t n}}^{(p)}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\tilde{\\mathbf{D}}^{-1} \\cdot \\mathrm{lt}_{\\Delta}\\left(\\phi^{\\prime}(\\mathbf{Q}) \\phi^{\\prime}(\\mathbf{K})^{\\top}\\right) \\cdot \\mathbf{V}$ where $\\tilde{\\mathbf{D}}=\\operatorname{diag}\\left(\\mathbf{1}_{n}+\\operatorname{lt}_{\\Delta}\\left(\\phi^{\\prime}(\\mathbf{Q}) \\phi^{\\prime}(\\mathbf{K})^{\\top}\\right) \\cdot \\mathbf{1}_{n}\\right)$. Therefore, it is crucial to efficiently compute $\\mathrm{It}_{\\triangle}\\left(\\phi^{\\prime}(\\mathbf{Q}) \\phi^{\\prime}(\\mathbf{K})^{\\top}\\right) \\cdot \\mathbf{X}$ for $\\mathbf{X} \\in\\left\\{\\mathbf{1}_{n}, \\mathbf{V}\\right\\}$. In the next subsection, we present a block based algorithm to compute $\\mathrm{It}_{\\triangle}(\\mathbf{A} \\cdot \\mathbf{B}) \\cdot \\mathbf{C}$ for arbitrary matrices $\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{n \\times m}, \\mathbf{C} \\in \\mathbb{R}^{n \\times k}$ in time linear in $n$, while the number of sequentially dependent steps is small. ### 3.1. Fast Lower Triangular Multiplication\n\nLet $b$ be the block size and $t=n / b$ be the number of blocks where each block $B_{l}(l \\in[t])$ contains indices $\\{(l-1) b+$ $1,(l-1) b+2, \\cdots, l \\cdot b\\}$. Let $\\mathbf{a}_{i}, \\mathbf{b}_{i}, \\mathbf{c}_{i}$ denote the $i$-th row vector of $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$ respectively. For each $l \\in[t]$, let the rows of $\\mathbf{A}_{l} \\in \\mathbb{R}^{b \\times m}$ consist of $\\mathbf{a}_{i}$ where $i \\in B_{l}$. We define submatrices $\\mathbf{B}_{l}, \\mathbf{C}_{l}$ of $\\mathbf{B}, \\mathbf{C}$ respectively in a similar way. For $l \\in[t]$, let us compute $\\mathbf{H}_{l}=\\sum_{i \\in B_{l}} \\mathbf{b}_{i} \\mathbf{c}_{i}^{\\top}$. Let $\\mathbf{Z}_{l}$ indicates the prefix sum: $\\mathbf{Z}_{l}=\\sum_{j<l} \\mathbf{H}_{j}$. In addition, let us compute $\\mathbf{P}_{l}=\\mathrm{It}_{\\triangle}\\left(\\mathbf{A}_{l} \\mathbf{B}_{l}^{\\top}\\right) \\mathbf{C}_{l}$ for each $l \\in[t]$ in the direct way. For any $l \\in[t]$, and any $i \\in B_{l}$, if $i$ is the $i^{\\prime}$-th index within the block $B_{l}$, it is easy to verify that the $i$-th row of $\\mathrm{t}_{\\triangle}\\left(\\mathbf{A B}^{\\top}\\right) \\mathbf{C}$ can be obtained by $\\mathbf{p}+\\mathbf{a}_{i}^{\\top} \\mathbf{Z}_{l}$ where $\\mathbf{p}$ is the $i^{\\prime}$-th row of $\\mathbf{P}_{l}$. Figure 3 justifies the correctness of the above algorithm. Since the prefix $\\operatorname{sum} \\mathbf{Z}_{t}$ is over $t$ matrices, the number of sequentially dependent steps is $t$. We can further reduce the number of sequential steps by using a parallel prefix sum algorithm (Blelloch, 1990) to exploit the parallelism. In our implementation, we only use the sequential prefix sum algorithm. Computing all $\\mathbf{P}_{l}$ requires $O\\left(t \\cdot b^{2}(m+k)\\right)$ time. Computing all $\\mathbf{H}_{l}, \\mathbf{Z}_{l}$ requires $O(t \\cdot b m k)$ time. Computing all $\\mathbf{A}_{l} \\mathbf{Z}_{l}+\\mathbf{P}_{l}$ requires $O(t \\cdot b m k)$ time. Therefore, the overall running time is $O(n b(m+k))$. When we set $b$ as a constant, the running time is linear in $n$. If we directly plugging $\\phi^{\\prime}(\\mathbf{Q}), \\phi^{\\prime}(\\mathbf{K}), \\mathbf{V}$ (or $\\mathbf{1}_{n}$ ) into $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$ above respectively, we compute causal Polysketch attention in time linear in the context length, $n$. Let us take another look using the above process to compute Polysketch attention, the matrix $\\mathbf{P}_{l}$ actually corresponds to $\\mathrm{It}_{\\triangle}\\left(\\phi^{\\prime}(\\mathbf{Q})_{l} \\phi^{\\prime}(\\mathbf{K})_{l}^{\\top}\\right) \\mathbf{X}_{l}$ where $\\mathbf{X}_{l} \\in\\left\\{\\mathbf{V}_{l}, \\mathbf{1}_{b}\\right\\} . \\phi^{\\prime}(\\mathbf{Q})_{l}$, $\\phi^{\\prime}(\\mathbf{K})_{l}$ corresponds to approximate feature mapped query and key vectors within the block $B_{l}$, and $\\mathbf{V}_{l}$ corresponds to the value vectors in $B_{l}$. Let $\\mathbf{Q}_{l}, \\mathbf{K}_{l}$ be the corresponding original query and key vectors in $B_{l}$. One observation is that $\\phi^{\\prime}(\\mathbf{Q})_{l} \\phi^{\\prime}(\\mathbf{K})_{l}^{\\top}=\\mathbf{L}^{\\otimes 2}\\left(\\mathbf{R}^{\\otimes 2}\\right)^{\\top}=\\left(\\mathbf{L R}^{\\top}\\right)^{2}$ where $\\mathbf{L}=$ PolySketchWithNegativity $\\left(\\mathbf{Q}_{l}, r, p / 2\\right) \\in \\mathbb{R}^{b \\times r}$ and $\\mathbf{R}=\\operatorname{POLYSKETCHWITHNEGATIVITY}\\left(\\mathbf{K}_{l}, r, p / 2\\right) \\in \\mathbb{R}^{b \\times r}$ (recall Algorithm 1). Therefore $\\mathrm{It}_{\\Delta}\\left(\\phi^{\\prime}(\\mathbf{Q})_{l} \\phi^{\\prime}(\\mathbf{K})_{l}^{\\top}\\right.$ ) only takes $O\\left(b^{2} r\\right)$ time instead of $O\\left(b^{2} r^{2}\\right)$ time. The total time to compute Polysketch attention is $O\\left(n b(r+h)+n r^{2} h\\right)$. ### 3.2. Applying Exact Attention Locally\n\nWe further observe that $\\phi^{\\prime}(\\mathbf{Q})_{l} \\phi^{\\prime}(\\mathbf{K})_{l}^{\\top}$ is used to approximate $\\left(\\mathbf{Q}_{l} \\mathbf{K}_{l}^{\\top}\\right)^{p}$. We can actually compute $\\mathbf{P}_{l}$ as It $_{\\Delta}\\left(\\left(\\mathbf{Q}_{l} \\mathbf{K}_{l}^{\\top}\\right)^{p}\\right) \\mathbf{X}_{l}$. This means that when token $i$ and $j$ are within the same local block, we can use their exact polynomial attention weight instead of using the approximation. The time to compute It $\\triangle\\left(\\left(\\mathbf{Q}_{l} \\mathbf{K}_{l}^{\\top}\\right)^{p}\\right) \\mathbf{X}_{l}$ is at most $O\\left(b^{2} h\\right)$. In this case, the total time to compute our Polysketch attention is at most $O\\left(n h\\left(b+r^{2}\\right)\\right)$. When $b \\leq r^{2}$, the running time is $O\\left(n h r^{2}\\right)$. As observed by our empirical studies (see Figure 2, Section 4 and other experiments in the appendix), using exact polynomial attention weights inside each local block further improves the model quality. ## 4. Experiments\n\nTo evaluate the effectiveness of the polynomial attention and Polysketch attention mechanisms, we train language models of various sizes with different attention mechanisms and look at both pre-training metrics and the performances on downstream tasks. Our implementations of all models are written in JAX. In our experiments, we use a Pallas implementation (JAX authors, 2023) of FlashAttention and a JAX implementation of Performer open-sourced by the authors (Choromanski et al., 2020). All the experiments are conducted on 32 Google Cloud TPUs. Synthetic tasks. Selective Copying and Induction Heads are two well-known downstream synthetic tasks for measuring content aware reasoning capabilities and the memorization abilities of the models (see Gu \\& Dao (2023) for more discussions). We conduct both experiments and see both polynomial and Polysketch have similar performance as softmax attention.",
    "polysketchformer-7": "We include more details in Appendix F. Models. For real world datasets, we train decoder-only models (only contain causal masked attention layers) of three different scales, mirroring the GPT-2 family (Radford et al., 2019): Small, Medium and Large. For small scale models, we train models using context lengths from 512 to $32 k$. For medium scale models, we only train using context length $8 k$. For large scale models, we only train using context length 2 k . The reason that we did not train longer context length for medium and large scale models is that non-kernel based attention mechanisms (softmax, polynomial) are too slow or go out of memory (OOM). The detailed descriptions of model sizes can be found in Appendix H. We take the recipe of Transformer++ (see (Hua et al., 2022; Yang et al., 2023; Gu \\& Dao, 2023) as well). We refer readers to Appendix I for a detailed description of the Transformer++ used by us. If not specified otherwise, we use 10k warmup steps, 125 k total training steps and a linear learning rate schedule. Depending on the original model scale, we also train kernel based attention models (Polysketch and Performer) with $0-3$ additional layers, since these models are significantly faster than non-kernel based attention models so we can afford to train larger models compared to vanilla softmax. It only slightly increases model sizes. Attention Mechanisms. We train models with the following 4 categories of attention mechanisms: (i) Softmax, (ii) Polynomial ( $p=2,4,8$ ), (iii) Polysketch (approximating polynomial attention of $p=4$ ) with variants enabling learned sketches (Section 2.3) or local exact polynomial attention (Section 3.2) or both, and (iv) Performer equipped with our lower triangular multiplication approach (Section 3.1) for handling causal masks. For both Performer and Polysketch, all attention heads share the same $\\phi^{\\prime}$ within the same attention layer. Hyper-parameters. For FlashAttention, we try both block size 256 and $512^{7}$. For our fast lower triangular multiplication approach, we use $b=1024$ for both Polysketch and Performer. We test both sketch sizes $r=32$ and $r=64$ for our Polysketch attention. We use 2048 features for Performer ${ }^{8}$. Pre-training metrics measurements (perplexities) over different context lengths. We train GPT-2 style small scale models equipped with different attention mechanisms on the Wiki-40B (Guo et al., 2020) and PG-19 (Rae et al., 2019) datasets with context length from 512 to 32 k where each training batch contains 1 M tokens. For all kernel based attentions (Performer and Polysketch), we use 13 layers instead of 12. More training details are mentioned in Appendix E. The perplexity results are shown in Figure 2 and training latencies are shown in Figure 4. Due to the space limit, we put all exact numbers in Appendix E including a detailed discussion. We observe that in the setting of 32 k context length, Polysketch (learned + local, $\\mathrm{r}=32$ ) achieves $\\mathbf{2 x}$ speed-up in comparison with FlashAttention of the fastest setup. As shown in Table 2 and Table 3 in Appendix E, when we increase the sketch size $r$ from 32 to 64 , we further reduce the perplexities. In addition, as shown in Table 4, Polysketch (learned + local, $r=64$ ) still keeps $\\sim 10 \\%$ speed-up in comparison with FlashAttention of the fastest setup. In addition, we observed that every kernelbased attention approach (Performer and Polysketch) with fast lower triangular matrix multiplication method almost keeps the same speed across different context lengths given that we use same number of training tokens per step.",
    "polysketchformer-8": "See more discussions in Appendix E. Downstream tasks of language models. We train our models at different scales on the C 4 dataset where each training batch contains 0.5 M tokens. The training details can be found in Appendix G. In Table 1, we report the perplexity on the validation split of C 4 dataset and 0 -shot and 5 -shot accuracies on a random sample of 500 examples of HellaSwag (Zellers et al., 2019), 500 examples of PIQA (Bisk et al., 2020) and on the full Physics question answering dataset (Wang \\& Wang). In addition to training models using 125 k steps, we also train models with 30 k steps to observe how the performance of attention mechanisms evolve with increasing number of total tokens trained on. The results for 30k steps can be found in Appendix G. As observed from\n\n[^5]|  | $\\frac{\\mathrm{C} 4}{\\text { Perplexity } \\downarrow}$ | HellaSwag |  | PIQA |  | Physics |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 0 -shot $\\uparrow$ | 5-shot $\\uparrow$ | 0 -shot $\\uparrow$ | 5-shot $\\uparrow$ | 0 -shot $\\uparrow$ | 5-shot $\\uparrow$ |\n| GPT-2 Small style, 100M-scale, 12 layers default, Context Length 8192, 125k training steps |  |  |  |  |  |  |  |\n| Softmax | 17.81 | 30.2 | 27.8 | 64.6 | 63.2 | 27.5 | 27.5 |\n| Polynomial (degree 4) | 18.18 | 28.6 | 28.4 | 64.2 | 65.0 | 27.5 | $\\underline{31.0}$ |\n| Polynomial (degree 8) | 17.77 | 29.8 | 29.8 | 62.2 | 64.0 | 23.1 | 26.2 |\n| Polysketch (learned, $r=64$ ) | 18.79 | 29.6 | 28.6 | 60.0 | 60.0 | 24.8 | 30.5 |\n| Polysketch (learned, 13 layers, $r=64$ ) | 18.47 | 28.4 | 29.4 | 62.0 | 62.6 | 27.5 | 31.8 |\n| Polysketch (learned + local, $r=64$ ) | 17.98 | 29.8 | 3.3 | 62.4 | 63.6 | 30.1 | 32.3 |\n| Polysketch (learned + local, 13 layers, $r=64$ ) | 17.68 | 29.0 | 29.0 | 62.6 | $\\underline{64.2}$ | 20.5 | 27.0 |\n| Polysketch (learned, $r=32$ ) | $\\overline{19.09}$ | 28.0 | 28.4 | 60.6 | 62.0 | 28.3 | 27.5 |\n| Polysketch (learned, 13 layers, $\\mathrm{r}=32$ ) | 19.50 | 28.4 | 29.0 | 61.6 | 64.6 | 27.9 | 33.1 |\n| Polysketch (learned + local, $r=32$ ) | 18.04 | 29.0 | 29.2 | 63.4 | 62.8 | 26.6 | 35.8 |\n| Polysketch (learned + local, 13 layers, $r=32$ ) | 17.72 | 31.2 | 30.4 | 64.8 | 64.6 | $\\underline{27.9}$ | $\\underline{31.8}$ |\n| GPT-2 Medium style, 300M-scale, 24 layers default, Context Length 8192, 125k training steps |  |  |  |  |  |  |  |\n| Softmax | 13.98 | 35.8 | 36.6 | 67.0 | 67.2 | 30.5 | 25.7 |\n| Polynomial (degree 4) | 14.29 | 35.8 | 36.0 | 65.8 | 67.6 | 27.5 | 28.8 |\n| Polynomial (degree 8) | 14.14 | 37.0 | 36.6 | 65.4 | 65.6 | 33.1 | 27.5 |\n| Polysketch (learned, $r=64$ ) | 14.64 | 34.6 | 33.4 | 63.2 | 65.4 | 3.0 | 26.2 |\n| Polysketch (learned, 26 layers, $r=64$ ) | 14.49 | 34.8 | 34.4 | 65.2 | 66.6 | 28.4 | 24.9 |\n| Polysketch (learned + local, $r=64$ ) | 14.16 | 35.0 | 35.0 | 65.8 | 68.6 | 29.6 | 34.5 |\n| Polysketch (learned + local, 26 layers, $r=64$ ) | 13.98 | 35.8 | 35.4 | 66.4 | 68.6 | 27.0 | 33.6 |\n| Polysketch (learned, $r=32$ ) | 14.94 | 32.2 | 33.8 | 65.6 | 67.6 | 32.7 | 33.6 |\n| Polysketch (learned, 26 layers, $\\mathrm{r}=32$ ) | 14.73 | 32.8 | 35.2 | 65.0 | 65.2 | 28.3 | 3 |\n| Polysketch (learned + local, r = 32) | 14.15 | 36.0 | 35.8 | 65.2 | 67.6 | 27.5 | 27.9 |\n| Polysketch (learned + local, 26 layers, $r=32$ ) | 14.00 | 37.2 | 35.4 | 68.0 | $\\underline{67.6}$ | 23.1 | $\\underline{29.6}$ |\n| GPT-2 Large style, 700M-scale, 36 layers default, Context Length 2048, 125k training steps |  |  |  |  |  |  |  |\n| Softmax | 12.71 | 40.2 | 40.2 | 68.8 | 71.4 | 34.4 | 24.4 |\n| Polynomial (degree 4) | 12.82 | 40.0 | 40.6 | 67.8 | 66.6 | 31.8 | 31.4 |\n| Polynomial (degree 8) | 12.85 | 40.0 | $\\overline{39.8}$ | 66.8 | 70.4 | 34.4 | 29.6 |\n| Polysketch (learned, 39 layers, $r=64$ ) | 12.83 | 41.0 | 39.4 | 68.6 | 68.8 | 33.6 | 36.6 |\n| Polysketch (learned + local, 39 layers, $r=64$ ) | 12.70 | 40.6 | 40.0 | 69.0 | 69.0 | 38.4 | 37.1 |\n| Polysketch (learned, 39 layers, $r=32$ ) | 12.98 | 39.4 | 40.4 | 68.6 | 67.6 | 33.6 | 27.0 |\n| Polysketch (learned + local, 39 layers, $r=32$ ) | 12.74 | 39.6 | 40.6 | 66.8 | 69.4 | 35.3 | 31.8 |\n\nTable 1. We compare the accuracies(\\%, higher the better) of different models on three different Q/A tasks. HellaSwag and Physics tasks have 4 choices and PIQA task has 2 choices. We also report the perplexities (lower the better) on the validation split of C4 dataset. Bolding indicates the best model in the task, underlining indicates beating softmax attention. ![](https://cdn.mathpix.com/cropped/2024_09_12_778cffb27f9dc74e6671g-09.jpg?height=535&width=1156&top_left_y=1803&top_left_x=452)\n\nFigure 4. Training speed of models on PG-19 and Wiki-40B for different context lengths. Softmax and polynomial attentions OOM'ed when context length $>8 \\mathrm{k}$. Table 1, Polysketch attention has a comparable performance and sometimes outperforms softmax attention. In addition, the model quality improved with increasing model sizes. We leave more discussions in Appendix G. ## 5. Acknowledgements\n\nWe would like to thank Zeyuan Allen-Zhu, Krzysztof Choromanski, Insu Han, Yanping Huang, Weizhe Hua, Rajesh Jayaram, Zhipeng Jia, Amin Karbasi, Tamas Sarlos, and David P. Woodruff for helpful discussions and comments for improving the detailed implementations and presentations. We would like to also thank many other contributors in the JAX/Flax community for suggestions on additional implementation details. ## References\n\nAamand, A., Indyk, P., and Vakilian, A. (learned) frequency estimation algorithms under zipfian distribution. arXiv preprint arXiv:1908.05198, 2019. Ahle, T. D., Kapralov, M., Knudsen, J. B., Pagh, R., Velingker, A., Woodruff, D. P., and Zandieh, A. Oblivious sketching of high-degree polynomial kernels. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 141-160. SIAM, 2020. Alman, J. and Song, Z. Fast attention requires bounded entries. arXiv preprint arXiv:2302.13214, 2023. Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. Avron, H., Nguyen, H., and Woodruff, D. Subspace embeddings for the polynomial kernel. Advances in neural information processing systems, 27, 2014. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Babiloni, F., Marras, I., Deng, J., Kokkinos, F., Maggioni, M., Chrysos, G., Torr, P., and Zafeiriou, S. Linear complexity self-attention with 3rd order polynomials. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. PIQA: reasoning about physical commonsense in natural language. In AAAI, pp. 7432-7439. AAAI Press, 2020. Blelloch, G. E. Prefix sums and their applications.",
    "polysketchformer-9": "1990. Bolya, D., Fu, C.-Y., Dai, X., Zhang, P., and Hoffman, J. Hydra attention: Efficient attention with many heads.",
    "polysketchformer-10": "In European Conference on Computer Vision, pp.",
    "polysketchformer-11": "35-49. Springer, 2022. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.",
    "polysketchformer-12": "arXiv preprint arXiv:1904.10509, 2019. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. JAX implementation of Performer is available at https://github.com/google-research/ google-research/blob/master/performer/fast_ attention/jax/fast_attention.py.",
    "polysketchformer-13": "Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Dao, T., Fu, D., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. Dauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933941. PMLR, 2017. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019. Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N., and Wei, F. Longnet: Scaling transformers to 1,000,000,000 tokens.",
    "polysketchformer-14": "arXiv preprint arXiv:2307.02486, 2023. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces.",
    "polysketchformer-15": "arXiv preprint arXiv:2312.00752, 2023. Guo, M., Dai, Z., Vrande\u010di\u0107, D., and Al-Rfou, R. Wiki-40b: Multilingual language model dataset. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 2440-2452, 2020. Han, I., Avron, H., and Shin, J. Polynomial tensor sketch for element-wise function of low-rank matrix.",
    "polysketchformer-16": "In International Conference on Machine Learning, pp. 3984-3993. PMLR, 2020. Han, I., Jayaram, R., Karbasi, A., Mirrokni, V., Woodruff, D. P., and Zandieh, A. Hyperattention: Longcontext attention in near-linear time. arXiv preprint arXiv:2310.05869, 2023. Hsu, C.-Y., Indyk, P., Katabi, D., and Vakilian, A. Learningbased frequency estimation algorithms. In International Conference on Learning Representations, 2019. Hua, W., Dai, Z., Liu, H., and Le, Q. Transformer quality in linear time. In International Conference on Machine Learning, pp. 9099-9117. PMLR, 2022. Impagliazzo, R., Paturi, R., and Zane, F. Which problems have strongly exponential complexity?",
    "polysketchformer-17": "Journal of Computer and System Sciences, 63(4):512-530, 2001. JAX authors. Implementation of FlashAttention in Pallas. https://github.com/google/jax/blob/main/jax/ experimental/pallas/ops/attention.py, 2023.",
    "polysketchformer-18": "Kasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G., Pappas, N., Mao, Y., Chen, W., and Smith, N. A. Finetuning pretrained transformers into rnns. arXiv preprint arXiv:2103.13076, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "polysketchformer-19": "In International conference on machine learning, pp. 5156-5165. PMLR, 2020. Kitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The efficient transformer.",
    "polysketchformer-20": "arXiv preprint arXiv:2001.04451, 2020. Meister, M., Sarlos, T., and Woodruff, D. Tight dimensionality reduction for sketching low degree polynomial kernels.",
    "polysketchformer-21": "Advances in Neural Information Processing Systems, 32, 2019 . Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. OpenAI. Gpt-4 technical report, 2023.",
    "polysketchformer-22": "Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., and Kong, L. Random feature attention. $a r X i v$ preprint arXiv:2103.02143, 2021. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.",
    "polysketchformer-23": "Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive transformers for longrange sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507. Roy, A., Saffar, M., Vaswani, A., and Grangier, D. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021. Shazeer, N. Glu variants improve transformer.",
    "polysketchformer-24": "arXiv preprint arXiv:2002.05202, 2020. Song, Z., Woodruff, D., Yu, Z., and Zhang, L. Fast sketching of polynomial kernels of polynomial degree. In International Conference on Machine Learning, pp. 9812-9823. PMLR, 2021. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding.",
    "polysketchformer-25": "arXiv preprint arXiv:2104.09864, 2021. Sun, Z., Yang, Y., and Yoo, S. Sparse attention with learning to hash. In International Conference on Learning Representations, 2021. Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. arXiv preprint arXiv:2009.06732, 2022. Tsai, Y.-H. H., Bai, S., Yamada, M., Morency, L.-P., and Salakhutdinov, R. Transformer dissection: a unified understanding of transformer's attention via the lens of kernel. arXiv preprint arXiv:1908.11775, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.",
    "polysketchformer-26": "Wang, G. and Wang, Z. Physics Multiple Choice. URL https://github.com/google/BIG-bench/tree/ main/bigbench/benchmark_tasks/physics.",
    "polysketchformer-27": "Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Woodruff, D. P. et al. Sketching as a tool for numerical linear algebra.",
    "polysketchformer-28": "Foundations and Trends\u00ae in Theoretical Computer Science, 10(1-2):1-157, 2014. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?",
    "polysketchformer-29": "arXiv preprint arXiv:1905.07830, 2019. ## A. Conclusion and Future Work\n\nIn this work, we empirically studied the performance of using high degree polynomial attention instead of softmax attention in training decoder-only models for language modeling tasks. Our empirical study shows that the polynomial attention can achieve a similar model quality as the vanilla softmax attention when degree $p \\geq 4$. Then we developed an efficient approximate polynomial attention via polynomial sketching techniques which can be computed in linear time of context length with provable approximation guarantees. In addition, we presented a fast block based lower triangular matrix multiplication algorithm which can significantly boost the training time of any kernel based attention in the decoder based models. There are several potential directions for future works. (1) Although we only empirically studied the performance of decoder-only models with polynomial attention for language modeling tasks, it is interesting to explore the potentials of encoder models with polynomial attention, and to understand whether it can be used in other fields such as vision. (2) In this work, empirically we mainly focus on reducing the training latency. The benefits of linear transformers also transfer to inference as the KV cache sizes are independent of the context length. The exact inference improvements using linear transformers have to be explored more thoroughly. (3) Polysketch attention is a kernel based method which can compute dense attention in linear time. It is interesting to see whether it can be combined with sparsification based efficient attention techniques such as HyperAttention proposed by (Han et al., 2023) recently.",
    "polysketchformer-30": "## B. Discussion of the error bound of Theorem 1.1\n\nLet us look closely to the error bound stated in Theorem 1.1. Our error only has polynomial dependence in the $\\ell_{2}$ norm bounds of $\\left\\{\\mathbf{q}_{i}\\right\\}$ and $\\left\\{\\mathbf{k}_{j}\\right\\}$. In other word, to keep the same error, our sketching dimension $r$ only has polynomial dependence in the $\\ell_{2}$ norm bounds of $\\left\\{\\mathbf{q}_{i}\\right\\}$ and $\\left\\{\\mathbf{k}_{j}\\right\\}$. In contrast, to approximate the exponential kernel, the sketching dimension of Performer (Choromanski et al., 2020) grows exponentially in the $\\ell_{2}$ norm bounds of $\\left\\{\\mathbf{q}_{i}\\right\\}$ and $\\left\\{\\mathbf{k}_{j}\\right\\}$. Suppose the $\\ell_{2}$ norm of query and key vectors is bounded, i.e., $\\max _{i} \\max \\left(\\left\\|\\mathbf{q}_{i}\\right\\|_{2},\\left\\|\\mathbf{k}_{i}\\right\\|_{2}\\right) \\leq C$. In the softmax attention, the ratio between two attention weights can be at most $\\exp \\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle\\right) / \\exp \\left(\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle\\right) \\leq \\exp \\left(C^{2}\\right) / \\exp \\left(-C^{2}\\right)=\\exp \\left(2 C^{2}\\right)$ which is bounded. In contrast, $\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle^{p} /\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle^{p}$ can be arbitrarily large since $\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle$ can be close to 0 . Therefore, in this bounded norm situation, polynomial attention is more capable for the operation of \"taking the max\". Another difference between the approximation provided by Performer and ours is that Performer provides entry-wise approximation guarantee while we provide an approximation guarantee in average. Consider an example that all query and key vectors have $\\ell_{2}$ norm at most 1. By Markov inequality, we know $90 \\%$ of pairs $(i, j) \\in[n] \\times[n]$ satisfies $\\left|\\left\\langle\\phi^{\\prime}\\left(\\mathbf{q}_{i}\\right), \\phi^{\\prime}\\left(\\mathbf{k}_{j}\\right)\\right\\rangle-\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle^{2}\\right| \\leq \\varepsilon^{\\prime}$, where $\\varepsilon^{\\prime}=10 \\varepsilon$. As long as both $\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle^{p},\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle^{p} \\in\\left(\\varepsilon^{\\prime} / \\varepsilon^{\\prime \\prime}, 1\\right]$ for some arbitrary $\\varepsilon^{\\prime \\prime}>\\varepsilon^{\\prime},\\left\\langle\\phi^{\\prime}\\left(\\mathbf{q}_{i}\\right), \\phi^{\\prime}\\left(\\mathbf{k}_{j}\\right)\\right\\rangle /\\left\\langle\\phi^{\\prime}\\left(\\mathbf{q}_{i}\\right), \\phi^{\\prime}\\left(\\mathbf{k}_{j^{\\prime}}\\right)\\right\\rangle$ is a $\\left(1 \\pm O\\left(\\varepsilon^{\\prime \\prime}\\right)\\right)$-approximation to $\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j}\\right\\rangle^{p} /\\left\\langle\\mathbf{q}_{i}, \\mathbf{k}_{j^{\\prime}}\\right\\rangle^{p}$. ## C. Proof of Theorem 2.4\n\nWe first note the following fact: If $\\mathbf{S}$ has $(\\varepsilon, \\delta, t)$-JL moment property, then for any two arbitrary vectors $\\mathbf{x}$ and $\\mathbf{y}$, we have that $\\left\\|\\left\\langle\\mathbf{S}^{\\top} \\mathbf{x}, \\mathbf{S}^{\\top} \\mathbf{y}\\right\\rangle-\\langle\\mathbf{x}, \\mathbf{y}\\rangle\\right\\|_{L^{t}} \\leq \\varepsilon \\delta^{1 / t}\\|\\mathbf{x}\\|_{2}\\|\\mathbf{y}\\|_{2}$. For a proof see Lemma 9 from (Ahle et al., 2020). Proof of Theorem 2.4. Let $\\mathbf{c}_{i}$ denote the $i$-th row of $\\mathbf{C}$ and $\\mathbf{d}_{j}$ denote the $j$-th row of $\\mathbf{D}$. Then the $(i, j)$-th entry of the matrix $\\mathbf{C}^{\\otimes 2}\\left(\\mathbf{D}^{\\otimes 2}\\right)^{\\top}$ is equal to $\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}$. Similarly, the $(i, j)$-th coordinate of the matrix $(\\mathbf{C S})^{\\otimes 2}\\left((\\mathbf{D S})^{\\otimes 2}\\right)^{\\top}$ is equal to $\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle^{2}$ and therefore\n\n$$\n\\left\\|(\\mathbf{C S})^{\\otimes 2}\\left((\\mathbf{D S})^{\\otimes 2}\\right)^{\\top}-\\mathbf{C}^{\\otimes 2}\\left(\\mathbf{D}^{\\otimes 2}\\right)^{\\top}\\right\\|_{\\mathrm{F}}^{2}=\\sum_{i, j}\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle^{2}-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\right)^{2}\n$$\n\nRecall that given an integer $t \\geq 1$, for a random variable $\\mathbf{X}$, we define $\\|\\mathbf{X}\\|_{L^{t}}$ as $\\mathbf{E}\\left[|\\mathbf{X}|^{t}\\right]^{1 / t}$. Also note that $\\|\\mathbf{X}\\|_{L^{t}}$ is a norm over the random variables and in-particular satisfies the triangle inequality. Now,\n\n$$\n\\begin{aligned}\n\\|\\|(\\mathbf{C S})^{\\otimes 2}\\left((\\mathbf{D S})^{\\otimes 2}\\right)^{\\top}-\\mathbf{C}^{\\otimes 2}\\left(\\mathbf{D}^{\\otimes 2}\\right)^{\\top}\\left\\|_{\\mathbf{F}}\\right\\|_{L^{t}} & =\\|\\|(\\mathbf{C S})^{\\otimes 2}\\left((\\mathbf{D S})^{\\otimes 2}\\right)^{\\top}-\\mathbf{C}^{\\otimes 2}\\left(\\mathbf{D}^{\\otimes 2}\\right)^{\\top}\\left\\|_{\\mathbf{F}}^{2}\\right\\|_{L^{t / 2}}^{1 / 2} \\\\\n& =\\left\\|\\sum_{i, j}\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle^{2}-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\right)^{2}\\right\\|_{L^{t / 2}}^{1 / 2} \\\\\n& \\leq\\left(\\sum_{i, j}\\left\\|\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle^{2}-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\right)^{2}\\right\\|_{L^{t / 2}}\\right)^{1 / 2}\n\\end{aligned}\n$$\n\nwhere we used the triangle inequality of $\\|\\cdot\\|_{L^{t}}$ in the last inequality. Now consider a single term $\\|\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle^{2}-\\right.$ $\\left.\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\right)^{2} \\|_{L^{t / 2}}$. First, we have\n\n$$\n\\begin{aligned}\n& \\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle^{2}-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\right)^{2} \\\\\n& =\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle+\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{2}\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{2} \\\\\n& =\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle+2\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{2}\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{2} \\\\\n& \\leq(1+C)\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{4}+4(1+1 / C)\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{2}\n\\end{aligned}\n$$\n\nwith probability 1 for any $C \\geq 1$. Since both LHS and RHS are non-negative random variables, we obtain that\n\n$$\n\\begin{aligned}\n& \\left\\|\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle^{2}-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\right)^{2}\\right\\|_{L^{t / 2}} \\\\\n& \\leq(1+C)\\left\\|\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{4}\\right\\|_{L^{t / 2}}+4(1+1 / C)\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\left\\|\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{2}\\right\\|_{L^{t / 2}}\n\\end{aligned}\n$$\n\nNow,\n\n$$\n\\begin{aligned}\n\\left\\|\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{4}\\right\\|_{L^{t / 2}} & =\\left\\|\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right\\|_{L^{2 t}}^{4} \\\\\n& \\leq \\varepsilon^{4} \\delta^{2 / t}\\left\\|\\mathbf{c}_{i}\\right\\|_{2}^{4}\\left\\|\\mathbf{d}_{j}\\right\\|_{2}^{4}\n\\end{aligned}\n$$\n\nassuming that $S$ has $(\\varepsilon, \\delta, 2 t)$-JL moment property. We also have\n\n$$\n\\begin{aligned}\n\\left\\|\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right)^{2}\\right\\|_{L^{t / 2}} & =\\left\\|\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle\\right\\|_{L^{t}}^{2} \\\\\n& \\leq \\varepsilon^{2} \\delta^{2 / t}\\left\\|\\mathbf{c}_{i}\\right\\|_{2}^{2}\\left\\|\\mathbf{d}_{j}\\right\\|_{2}^{2}\n\\end{aligned}\n$$\n\nassuming that $\\mathbf{S}$ has $(\\varepsilon, \\delta, t)$-JL moment property. Overall, we get\n\n$$\n\\begin{aligned}\n& \\left\\|\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}, \\mathbf{S}^{\\top} \\mathbf{d}_{j}\\right\\rangle^{2}-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\right)^{2}\\right\\|_{L^{t / 2}} \\\\\n& \\leq(1+C) \\varepsilon^{4} \\delta^{2 / t}\\left\\|\\mathbf{c}_{i}\\right\\|_{2}^{4}\\left\\|\\mathbf{d}_{j}\\right\\|_{2}^{4}+4(1+1 / C)\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2} \\varepsilon^{2} \\delta^{2 / t}\\left\\|\\mathbf{c}_{i}\\right\\|_{2}^{2}\\left\\|\\mathbf{d}_{j}\\right\\|_{2}^{2}\n\\end{aligned}\n$$\n\nPicking $C=1 / \\varepsilon$ and assuming $\\varepsilon \\leq 1 / 5$, we get that\n\n$$\n\\left\\|\\left(\\left\\langle\\mathbf{S}^{\\top} \\mathbf{c}_{i}\\right\\rangle^{2}-\\left\\langle\\mathbf{c}_{i}, \\mathbf{d}_{j}\\right\\rangle^{2}\\right)^{2}\\right\\|_{L^{t / 2}} \\leq 5 \\varepsilon^{2} \\delta^{2 / t}\\left\\|\\mathbf{c}_{i}\\right\\|_{2}^{4}\\left\\|\\mathbf{d}_{j}\\right\\|_{2}^{4}\n$$\n\nThus, we have\n\n$$\n\\begin{aligned}\n&\\|\\|(\\mathbf{C S})^{\\otimes 2}\\left((\\mathbf{D S})^{\\otimes 2}\\right)^{\\top}-\\mathbf{C}^{\\otimes 2}\\left(\\mathbf{D}^{\\otimes 2}\\right)^{\\top}\\left\\|_{\\mathrm{F}}\\right\\|_{L^{t}} \\leq \\sqrt{5} \\varepsilon \\delta^{1 / t} \\sqrt{\\sum_{i, j}\\left\\|\\mathbf{c}_{i}\\right\\|_{2}^{4}\\left\\|\\mathbf{d}_{j}\\right\\|_{2}^{4}} \\\\\n& \\leq \\sqrt{5} \\varepsilon \\delta^{1 / t}\\left\\|\\mathbf{C}^{\\otimes 2}\\right\\|_{\\mathrm{F}}\\left\\|\\mathbf{D}^{\\otimes 2}\\right\\|_{\\mathrm{F}} . \\end{aligned}\n$$\n\nBy using Markov's inequality, we obtain that with probability $\\geq 1-\\delta$,\n\n$$\n\\left\\|(\\mathbf{C S})^{\\otimes 2}\\left((\\mathbf{D S})^{\\otimes 2}\\right)^{\\top}-\\mathbf{C}^{\\otimes 2}\\left(\\mathbf{D}^{\\otimes 2}\\right)^{\\top}\\right\\|_{F} \\leq \\sqrt{5} \\varepsilon\\left\\|\\mathbf{C}^{\\otimes 2}\\right\\|_{F}\\left\\|\\mathbf{D}^{\\otimes 2}\\right\\|_{F}\n$$\n\n## D. Replacing Random Projections with Learnable Transformations\n\nOur learnable polynomial sketch algorithm is stated in Algorithm 2. It has a similar structure as our randomized polynomial sketch stated in Algorithm 1. The only differences are (1) we replace random projections $\\mathbf{M}_{1} \\mathbf{G}_{1}$ and $\\mathbf{M}_{2} \\mathbf{G}_{2}$ with $f_{1}\\left(\\mathbf{M}_{1}\\right)$ and $f_{2}\\left(\\mathbf{M}_{2}\\right)$ respectively. (2) We apply a tanh $(\\cdot)$ trick to each entry of $\\sqrt{1 / r} \\cdot\\left[f_{1}\\left(\\mathbf{M}_{1}\\right) * f_{2}\\left(\\mathbf{M}_{2}\\right)\\right]$ to make the output within a reasonable range and thus make the optimization process stable and converge. Each $f_{1}(\\cdot), f_{2}(\\cdot)$ has the same dense network structure but different learnable parameters. The network has output dimension $r$ and 3 hidden layers with size $[8 r, r, 8 r]$. We apply an activation function gelu $(\\cdot)$ after the first and the third hidden layer. We apply an layer normalization before the input and the second hidden layer. Therefore, each network only has roughly $8 h r+24 r^{2}$ or $32 r^{2}$ number of parameters. The entire learnable polynomial sketch only contains $p-2$ learnable networks. Since all attention heads share the same learnable polynomial sketch within an attention layer, the number of increased learnable parameters is negeligible in comparison with the entire model.",
    "polysketchformer-31": "Note that we did not take much time to optimize the network structure.",
    "polysketchformer-32": "It is likely that better network structures exist. We leave the question of finding a better network structure as a future work. ```\nAlgorithm 2 Learnable Polynomial Sketches\n    function LEARNABLEPOLYSKETCHWITHNEGATIVITY \\(\\left(\\mathbf{A} \\in \\mathbb{R}^{k \\times m}, r, p\\right)\\)\n        // Analog of PolySKEtchWithNEGativity. If \\(p=1\\), return \\(\\mathbf{A}\\)\n        \\(\\mathbf{M}_{1}=\\) LEARNABLEPOLYSKETCHWITHNEGATIVITY (A \\(, r, p / 2\\) )\n        \\(\\mathbf{M}_{2}=\\) LEARNABLEPOLYSKETCHWITHNEGATIVITY \\((\\mathbf{A}, r, p / 2\\) )\n        Return \\(\\sqrt{r} \\cdot \\tanh \\left(\\sqrt{1 / r} \\cdot\\left[f_{1}\\left(\\mathbf{M}_{1}\\right) * f_{2}\\left(\\mathbf{M}_{2}\\right)\\right]\\right) \\in \\mathbb{R}^{k \\times r}\\)\n    end function\n    function LEARNABLEPOLYSKETCHNONNEGATIVE \\(\\left(\\mathbf{A} \\in \\mathbb{R}^{k \\times m}, r, p\\right)\\)\n        // Analog of PolysketchNonNegative. \\(\\mathbf{M}=\\) LEARNABLEPOLYSKETCHWITHNEGATIVITY(A, \\(r, p / 2)\\)\n        Return \\(\\mathbf{M}^{\\otimes 2} \\in \\mathbb{R}^{k \\times r^{2}}\\). end function\n```\n\n|  | 512 | 1 k | 2 k | 4 k | 8 k | 16 k | 32 k |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Non-kernel based methods, 12 layers |  |  |  |  |  |  |  |\n| Softmax (using FlashAttention) | 13.57 | 12.75 | 12.23 | 11.88 | 11.65 | 11.57 | 11.55 |\n| Polynomial (deg=2) | 13.84 | 13.10 | 12.75 | 12.61 | 12.72 | OOM | OOM |\n| Polynomial (deg=4) | 13.58 | 12.76 | 12.26 | 12.00 | 11.85 | OOM | OOM |\n| Polynomial (deg=8) | 13.56 | 12.71 | 12.16 | 11.86 | 11.64 | OOM | OOM |\n| Kernel based methods, 13 layers |  |  |  |  |  |  |  |\n| Polysketch (random, $\\mathrm{r}=32$ ) | 14.31 | 13.74 | 13.40 | 13.26 | 13.41 | 13.79 | 14.75 |\n| Polysketch (random, $\\mathrm{r}=64$ ) | 14.00 | 13.35 | 13.03 | 12.84 | 12.92 | 13.18 | 13.66 |\n| Polysketch (learned, $\\mathrm{r}=32)$ | 13.49 | 12.74 | 12.34 | 12.16 | 12.21 | 12.40 | 12.79 |\n| Polysketch (learned, $\\mathrm{r}=64$ ) | $\\mathbf{1 3 . 3 3}$ | 12.60 | 12.10 | 11.90 | 11.86 | 11.94 | 12.19 |\n| Polysketch (random + local, $\\mathrm{r}=32)$ | 13.37 | $\\mathbf{1 2 . 5 8}$ | 12.23 | 12.01 | 11.95 | 11.90 | 12.16 |\n| Polysketch (random + local, $\\mathrm{r}=64)$ | 13.37 | $\\mathbf{1 2 . 5 8}$ | 12.24 | 11.98 | 11.93 | 11.96 | 11.91 |\n| Polysketch (learned + local, $\\mathrm{r}=32)$ | 13.37 | $\\mathbf{1 2 . 5 8}$ | 12.09 | 11.75 | 11.55 | 11.46 | 11.47 |\n| Polysketch (learned + local, $\\mathrm{r}=64$ ) | 13.37 | $\\mathbf{1 2 .",
    "polysketchformer-33": "5 8}$ | $\\mathbf{1 2 . 0 3}$ | $\\mathbf{1 1 . 6 9}$ | $\\mathbf{1 1 .",
    "polysketchformer-34": "4 4}$ | $\\mathbf{1 1 . 3 8}$ | $\\mathbf{1 1 . 3 4}$ |\n| Performer (2048 features) | 14.30 | 13.68 | 13.56 | 13.50 | 13.49 | 13.73 | 14.17 |\n\nTable 2. Perplexities on the test split of PG19 when the models are trained on PG19 dataset\n\n## E. Perplexity Results on PG-19 and Wiki-40B\n\nWe train GPT-2 small scale models on PG-19 and Wiki-40B datasets at various context lengths. We use the same training recipe that we described in Section 4 and train the models for 125 k steps with a batch size of 1 M tokens. For each of PG-19 and Wiki-40B datasets, we obtain a SentencePiece vocabulary of size 32 k and train the models using the respective tokenizer. We measure test perplexities of each of the models in Tables 2 and 3. We can see that the Polysketch attention model (learned + local) equipped with one additional layer beats the softmax attention models at all context lengths. We also note that Polysketch attention models, even without local attention, also achieve perplexities close to that of softmax models at all context lengths. These experiments show that our attention mechanism can scale to large context lengths without significant model quality loss. The training latencies are shown in Table 4. As we observed that all kernel based approaches equipped with our fast lower triangular multiplication approach are significantly faster than non-kernel based methods. Notably, Polysketch (learned + local, $r=64$ ) achieves 1.1 x speed up in comparison with the FlashAttention (block size 512) on 32k context length, and Polysketch (learned + local, $\\mathrm{r}=32$ ) achieves $\\mathbf{2 x}$ speed up in comparison with the FlashAttention (block size 512) on 32 k context length. Both Polysketch (learned + local, $r=32$ ) and Polysketch (learned + local, $r=64$ ) have lower perplexity than the softmax attention. Polysketch (learned + local, $r=64$ ) has the lowest test perplexity. |  | 512 | 1 k | 2 k | 4 k | 8 k | 16 k | 32 k |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Non-kernel based methods, 12 layers |  |  |  |  |  |  |  |\n| Softmax (using FlashAttention) | 15.82 | 15.04 | 14.61 | 14.40 | 14.35 | 14.34 | 14.35 |\n| Polynomial (p=2) | 16.24 | 15.58 | 15.38 | 15.41 | 15.60 | OOM | OOM |\n| Polynomial (p=4) | 15.85 | 15.11 | 14.75 | 14.59 | 14.59 | OOM | OOM |\n| Polynomial (p=8) | 15.81 | 15.00 | 14.56 | 14.36 | 14.32 | OOM | OOM |\n| Kernel based methods, 13 layers |  |  |  |  |  |  |  |\n| Polysketch (random, $\\mathrm{r}=32$ ) | 16.84 | 16.35 | 16.20 | 16.28 | 16.52 | 17.05 | 17.84 |\n| Polysketch (random, $\\mathrm{r}=64$ ) | 16.32 | 15.73 | 15.72 | 15.88 | 16.01 | 16.44 | 17.45 |\n| Polysketch (learned, $\\mathrm{r}=32$ ) | 15.84 | 15.20 | 14.95 | 14.91 | 15.06 | 15.52 | 15.93 |\n| Polysketch (learned, $\\mathrm{r}=64$ ) | 15.65 | 14.96 | 14.62 | 14.58 | 14.70 | 14.95 | 15.35 |\n| Polysketch (random + local, $\\mathrm{r}=32$ ) | $\\mathbf{1 5 .",
    "polysketchformer-35": "6 3}$ | $\\mathbf{1 4 . 8 6}$ | 14.60 | 14.50 | 14.52 | 14.65 | 14.68 |\n| Polysketch (random + local, $\\mathrm{r}=64)$ | $\\mathbf{1 5 . 6 3}$ | $\\mathbf{1 4 . 8 6}$ | 14.58 | 14.52 | 14.43 | 14.62 | 14.54 |\n| Polysketch (learned + local, $\\mathrm{r}=32$ ) | $\\mathbf{1 5 . 6 3}$ | $\\mathbf{1 4 . 8 6}$ | 14.46 | 14.28 | 14.24 | $\\mathbf{1 4 . 2 3}$ | 14.32 |\n| Polysketch (learned + local, $\\mathrm{r}=64$ ) | $\\mathbf{1 5 .",
    "polysketchformer-36": "6 3}$ | $\\mathbf{1 4 . 8 6}$ | $\\mathbf{1 4 . 4 3}$ | $\\mathbf{1 4 .",
    "polysketchformer-37": "2 6}$ | $\\mathbf{1 4 . 1 8}$ | 14.24 | $\\mathbf{1 4 . 2 9}$ |\n| Performer (2048 features) | 16.75 | 16.18 | 16.14 | 16.37 | 16.64 | 17.16 | 18.40 |\n\nTable 3. Perplexities on the test split of Wiki-40B when the models are trained on Wiki-40B dataset\n\n## E.1. Training Latency Comparison\n\nThe main advantage of linear transformers is that their training latency remains the same across different context lengths given that we use the same \"batch size\" (tokens per training step) for all the context lengths. To show that it is the case, we report the training latencies (in terms of steps $/ \\mathrm{sec}$ ) of our models and other attention mechanisms in Table 4. Using the same batch size across different context lengths, we note that the steps/sec of linear transformers such as Polysketch and Performer remain almost constant whereas the steps/sec of quadratic-time transformers decreases with increasing context lengths. The results show that, depending on the model structure, models using our Polysketch attention mechanism are significantly faster to train than models using a quadratic attention mechanism such as softmax (implemented via FlashAttention) at long context lengths. ## F. Experiments with Synthetic Tasks\n\nFor both synthetic experiments, We train a small 2-layer transformer. Each attention layer contains 8 attention heads where each has head size 16. For Polysketch attention, we choose $\\mathrm{r}=32$ and the block size $b=1024$ for fast lower triangular multiplication (Section 3.1). ## F.1. Selective Copying\n\nRecently, Gu \\& Dao (2023) have used selective copying task as a yard stick for measuring content aware reasoning capabilities and the memorization abilities of the models. In this task, the model is required to memorize colored blocks that appear in the context and the model needs to output the colored blocks in the same order at the end. See (Gu \\& Dao, 2023) for a more detailed description of this task. We generate 64 k random examples used for training. Each batch has 64 examples. We train for 400 k steps in total without otherwise specified. Using a similar training recipe as in their paper, we train small two layer models using different attention mechanisms to solve these tasks at context lengths $4 \\mathrm{k}, 16 \\mathrm{k}$, and 32 k . We report our results in Table 5 . We see that polynomial and Polysketch attention manage to learn to solve the selective copying task though the accuracy of Polysketch is a bit worse at 16 k context length with the same training recipe as other models. We found that with a different learning rate schedule, Polysketch attention also manages to solve the selective copying task at a context length 16k with an accuracy of $99.44 \\%$ thus showing that there may not be any loss in matching the reasoning capabilities of the softmax attention mechanism. This suggests that Polysketch attention may require different learning rate schedules to obtain the optimal performance as compared to softmax transformers. We also find that at a context length of 32 k , Polysketch attention learns to solve $95.29 \\%$ of test examples after 800k steps of training. In all our experiments, we observe sudden spike in the accuracies of the models indicating the point where the\n\n|  | 512 | 1 k | 2 k | 4 k | 8 k | 16 k | 32 k |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Softmax | $\\mathbf{6 . 0 0}$ | 4.95 | 3.65 | 1.95 | 1.19 | OOM | OOM |\n| FlashAttention <br> (Block size 256 x 256) | 4.78 | 4.09 | 3.25 | 2.31 | 1.47 | 0.85 | 0.45 |\n| FlashAttention <br> (Block size 512 x 512) | 5.46 | $\\mathbf{5 . 0}$ | $\\mathbf{4 . 4}$ | $\\mathbf{3 . 6 5}$ | $\\mathbf{2 . 7}$ | 1.8 | 1.03 |\n| Polynomial <br> (p=2, 4, 8) | 5.74 | 4.74 | 3.55 | 2.16 | 1.20 | 00 OM | 0 OM |\n| Polysketch <br> (random, 13 layers, $\\mathrm{r}=32$ ) | 5.25 | 4.31 | 3.66 | 3.54 | 3.39 | 3.25 | 2.56 |\n| Polysketch <br> (random, 13 layers, $\\mathrm{r}=64$ ) | 5.06 | 4.20 | 2.50 | 2.23 | 2.06 | 1.95 | 1.55 |\n| Polysketch <br> (learned, 13 layers, $\\mathrm{r}=32$ ) | 3.16 | 2.82 | 2.45 | 2.40 | 2.33 | 2.26 | 2.00 |\n| Polysketch <br> (learned, 13 layers, $\\mathrm{r}=64$ ) | 2.17 | 1.97 | 1.37 | 1.35 | 1.33 | 1.29 | 1.13 |\n| Polysketch <br> (random + local, 13 layers, $\\mathrm{r}=32$ ) | 5.35 | 4.40 | 3.24 | 3.2 | 3.14 | 3.11 | 2.71 |\n| Polysketch <br> (random + local, 13 layers, $\\mathrm{r}=64$ ) | 5.35 | 4.40 | 2.09 | 2.00 | 1.91 | 1.82 | 1.60 |\n| Polysketch <br> (learned + local, 13 layers, $\\mathrm{r}=$ 32) | 5.35 | 4.40 | 2.27 | 2.27 | 2.20 | $\\mathbf{2 . 1 9}$ | $\\mathbf{1 . 9 8}$ |\n| Polysketch <br> (learned + local, 13 layers, $\\mathrm{r}=64$ ) | 5.35 | 4.40 | 1.31 | 1.31 | 1.30 | 1.26 | 1.12 |\n| Performer <br> (2k features <br> +Fast lower triangular multiplications) | 2.21 | 1.58 | 1.39 | 1.36 | 1.33 | 1.32 | 1.18 |\n| Performer <br> (256 features (default) <br> without Fast lower triangular multiplications) | 0.44 | 0.40 | 0.36 | 0.29 | 0.21 | 0.14 | 0.08 |\n\nTable 4. Training steps/sec of different attention mechanisms at various context lengths (higher is faster). For context lengths 512 and 1 k , we compute the full attention matrix in Polysketch and Performer attention without using the linearization technique. These models are all GPT-2-like small scale models. Each batch contains 1 M tokens in total. ![](https://cdn.mathpix.com/cropped/2024_09_12_778cffb27f9dc74e6671g-18.jpg?height=245&width=1372&top_left_y=233&top_left_x=344)\n\nFigure 5. The test accuracy during training Polysketch (learned + local) on the selective copying task of 32 k context length. The x -axis is the number of steps trained and the $y$-axis is the test accuracy. We observe that the model suddenly learns the pattern at some point during the training. model learns to solve the task, see e.g., Figure 5. |  | 4 k | 16 k | 32 k |\n| :--- | :---: | :---: | :---: |\n| Softmax | $99.73 \\%$ | $98.17 \\%$ | $0 \\%$ |\n| Polynomial (degree=4) | $99.90 \\%$ | $97.97 \\%$ | $0 \\%$ |\n| Polynomial (degree=8) | $99.90 \\%$ | $97.65 \\%$ | $0 \\%$ |\n| Polysketch (learned + local) | $99.16 \\%$ | $92.75 \\%$ | $0 \\%$ |\n| Polysketch (learned + local) | - | $99.44 \\%$ | $87.16 \\%$ |\n| (different learning rate schedule) | - |  |  |\n\nTable 5. \\% of 4096 examples on which the models succeeded to perfectly output the colored blocks in the context in the same order.",
    "polysketchformer-38": "## F.2. Induction heads\n\nOlsson et al. (2022) have proposed induction heads task as a way to identify and explain the in-context learning capabilities of language models. This task requires the model to output the token that appears immediately after a special token that appears in the context exactly once at an arbitrary position. We generate 64 k random examples used for training. Each batch contains 64 examples. Each position is a random token from a vocabulary of size 16 . We replace a random position except the last 3 tokens with a special token. We replace the second to the last token with a special token. We replace the last token with the token appeared directly after the first special token. The total number of training steps is 400k. We consider context lengths 128 and 256 . We observe that all the models (softmax, degree 4 polynomial, degree 8 polynomial and Polysketch with sketch size 16 and 32) are able to solve (accuracy $>99.95 \\%$ ) the task at a context length of 128 and all of them fail to solve (accuracy around $1 / 16$, i.e., random guessing) the task at a context length 256 under the same optimization configuration. ## G. Experiments on Downstream Tasks\n\nAdditional training details. We train all models from scratch on the C 4 dataset using a SentencePiece tokenizer trained on C 4 with a vocabulary size of 32,000 . We use a batch size of 0.5 M tokens per training step and a peak learning rate of $3 \\mathrm{e}-4$. We use a linear learning rate schedule to warmup the learning rate for the first $10 \\%$ of iterations and then again use a linear learning rate schedule to decay the learning rate. We use Adam optimizer with weight decay parameters $\\left(\\beta_{1}=0.95, \\beta_{2}=0.98\\right)$ in all our experiments. The training latencies of small scale models on 8 k context length are:\n\n1. Softmax (without FlashAttention): 2.40 step/sec. 2. Polynomial $(\\mathrm{p}=4,8): 2.65$ step/sec. 3. Polysketch (learned, $r=64,12$ layers): 2.71 step $/ \\mathrm{sec}$. 4. Polysketch (learned, $r=32$, 12 layers): 4.68 step/sec. 5. Polysketch (learned, $r=64,13$ layers): $2.49 \\mathrm{step} / \\mathrm{sec}$. |  | $\\frac{\\mathrm{C} 4}{\\text { Perplexity } \\downarrow}$ | HellaSwag |  | PIQA |  | Physics |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 0 -shot $\\uparrow$ | 5-shot $\\uparrow$ | 0 -shot $\\uparrow$ | 5-shot $\\uparrow$ | 0 -shot $\\uparrow$ | 5-shot $\\uparrow$ |\n| GPT-2 Small style, 100M-scale, 12 layers default, Context Length 8192, 30k training steps |  |  |  |  |  |  |  |\n| Softmax | 20.11 | 28.0 | 28.8 | 61.8 | 62.8 | 20.5 | 30.1 |\n| Polynomial (degree 4) | 20.66 | $\\underline{28.4}$ | 28.6 | 60.4 | 61.0 | 24.4 | $\\underline{30.1}$ |\n| Polynomial (degree 8) | 20.05 | 27.6 | 27.4 | 59.8 | 60.0 | 20.5 | 23.1 |\n| Polysketch (learned, $r=64$ ) | 21.16 | 27.8 | 28.0 | 60.4 | 61.2 | 29.6 | 33.6 |\n| Polysketch (learned, 13 layers, $r=64$ ) | 20.93 | $\\underline{28.6}$ | 28.2 | 62.0 | 61.8 | 30.1 | 35.8 |\n| Polysketch (learned + local, $r=64$ ) | 20.30 | $\\underline{28.4}$ | 27.8 | 61.6 | 62.8 | 27.9 | 34.9 |\n| Polysketch (learned + local, 13 layers, $r=64$ ) | 19.91 | $\\underline{28.4}$ | 27.6 | 61.2 | 61.0 | $\\underline{31.0}$ | 3 |\n| Polysketch (learned, $r=32$ ) | 22.31 | 27.0 | 26.6 | 59.8 | 61.8 | 35.3 | 34.4 |\n| Polysketch (learned, 13 layers, $\\mathrm{r}=32$ ) | 22.15 | $\\underline{28.8}$ | 28.2 | 59.8 | 60.0 | 32.3 | 31.0 |\n| Polysketch (learned + local, $\\mathrm{r}=32$ ) | 20.28 | $\\underline{28.4}$ | 28.4 | 59.6 | 61.4 | $\\underline{29.6}$ | 31.4 |\n| Polysketch (learned + local, 13 layers, $r=32$ ) | 19.94 | $\\underline{28.6}$ | 28.2 | 60.4 | 61.2 | $\\underline{29.2}$ | $\\underline{30.5}$ |\n| GPT-2 Medium style, 300M-scale, 24 layers default, Context Length 8192, 30k training steps |  |  |  |  |  |  |  |\n| Softmax | 15.97 | 32.0 | 31.6 | 61.8 | 63.4 | 25.3 | 29.2 |\n| Polynomial (degree 4) | 16.46 | 29.2 | 29.8 | 63.2 | 64.4 | 30.5 | 33.6 |\n| Polynomial (degree 8) | 16.12 | 31.4 | 31.4 | 64.2 | 64.0 | 27.0 | 28.3 |\n| Polysketch (learned) | 17.18 | 29.2 | 30.0 | 62.8 | 64.0 | $\\underline{27.0}$ | 31.0 |\n| Polysketch (learned, 26 layers, $r=64$ ) | 17.06 | 29.8 | 31.0 | 64.8 | 64.6 | $\\underline{28.3}$ | 30.1 |\n| Polysketch (learned + local, $r=64$ ) | 16.14 | 31.6 | $\\underline{32.6}$ | $\\underline{64.6}$ | 64.4 | $\\underline{27.9}$ | 31.4 |\n| Polysketch (learned + local, 26 layers, $r=64$ ) | 15.95 | 31.8 | 31.4 | 63.8 | 66.0 | 20.0 | 27.9 |\n| Polysketch (learned, $r=32$ ) | 17.81 | 30.4 | 30.4 | 60.2 | 61.6 | 34.4 | 34.0 |\n| Polysketch (learned, 26 layers, $r=32$ ) | 17.47 | 29.2 | 30.2 | 61.6 | 62.0 | $\\underline{28.3}$ | 31.4 |\n| Polysketch (learned + local, $r=32$ ) | 16.20 | 31.6 | $\\underline{33.0}$ | 65.8 | 65.8 | $\\underline{31.8}$ | 28.8 |\n| Polysketch (learned + local, 26 layers, $\\mathrm{r}=32$ ) | 16.02 | 31.8 | 31.8 | 64.4 | 65.0 | 27.5 | 31.4 |\n\nTable 6. We compare the accuracies(\\%, higher the better) of different models (all trained for 30 k training steps) on three different $\\mathrm{Q} / \\mathrm{A}$ tasks. HellaSwag and Physics tasks have 4 choices and PIQA task has 2 choices. We also report the perplexities (lower the better) on the validation split of C 4 dataset. Bolding indicates the best model in the task, underlining indicates beating softmax attention. 6. Polysketch (learned, $\\mathrm{r}=32$, 13 layers): $4.34 \\mathrm{step} / \\mathrm{sec}$. 7. Polysketch (learned + local, $r=64,12$ layers): 2.70 step/sec. 8. Polysketch (learned + local, $\\mathrm{r}=32$, 12 layers): $4.42 \\mathrm{step} / \\mathrm{sec}$. 9. Polysketch (learned + local, $\\mathrm{r}=64$, 13 layers): 2.48 step/sec. 10. Polysketch (learned + local, $\\mathrm{r}=32$, 13 layers): 4.12 step/sec. Note that the difference between above training latencies and those presented in Table 4 is due to the different number of tokens per batch $(0.5 \\mathrm{M}$ vs 1 M$)$. The training latencies of medium scale models on 8 k context length are reported as follows:\n\n1. Softmax (without FlashAttention): $0.87 \\mathrm{step} / \\mathrm{sec}$. 2. Polynomial $(p=4,8): 0.89 \\mathrm{step} / \\mathrm{sec}$. 3. Polysketch (learned, $\\mathrm{r}=64$, 24 layers): 0.99 step/sec. 4. Polysketch (learned, r=32, 24 layers): 1.62 step/sec. 5. Polysketch (learned, $\\mathrm{r}=64$, 26 layers): 0.92 step/sec. 6. Polysketch (learned, $\\mathrm{r}=32$, 26 layers): $1.52 \\mathrm{step} / \\mathrm{sec}$. 7. Polysketch (learned + local, r=64, 24 layers): 0.98 step/sec. 8. Polysketch (learned + local, $\\mathrm{r}=32$, 24 layers): $1.59 \\mathrm{step} / \\mathrm{sec}$. 9. Polysketch (learned + local, $\\mathrm{r}=64$, 26 layers): 0.91 step $/ \\mathrm{sec}$. 10. Polysketch (learned + local, $\\mathrm{r}=32$, 26 layers): 1.46 step/sec. For large scale models, since the context length is only 2 k (recall that as we mentioned earlier, non-kernel based methods are either too slow or facing OOM issues for longer context length for the large-scale), the running time of kernel based methods do not take advantage from linearization. The purpose is to compare the model quality only. So we omit the training latencies of large scale models here. Additional results. The results similar to Table 1 but for the models trained on only 30 k steps is shown in Table 6. ## G.1. Scaling with Model Sizes\n\nFrom the results in Table 1 and Table 6, we have the following main observations: (i) Polysketch attention (learned + local) closely matches the performance and sometimes outperforms models trained with softmax attention. (ii) Models trained with Polysketch attention improve with increasing model sizes showing promise to be a replacement for softmax even in the largest models to achieve lower training latencies without significant performance issues. (iii) Strong performance of learned Polysketch attention, without relying on local attention, shows the capability of our proposed attention mechanism and that the results that we obtain with learned+local Polysketch attention are not just due to using exact polynomial attention within the blocks. ## H. Model Sizes for Each Attention Mechanism\n\nIn this section, we include the sizes of all models that we trained. ## H.1. Small scale models\n\nThe default configuration has 12 layers, 12 attention heads per layer, each attention head has head size 64 . The number of parameters of each model is stated as the following. 12 layer models:\n\n1. Softmax, Polynomial (degree $=2,4 \\& 8): 110 \\mathrm{M}$\n2. Polysketch (learned, sketch size $=64,12$ layers), Polysketch (learned + local, sketch size $=64,12$ layers): 113M\n3. Polysketch (learned, sketch size $=32,12$ layers), Polysketch (learned + local, sketch size $=32,12$ layers): 111 M\n13 layer models:\n4. Polysketch (learned, sketch size $=64,13$ layers), Polysketch (learned + local, sketch size $=64,13$ layers): 120 M\n5. Polysketch (learned, sketch size $=32$, 13 layers), Polysketch (learned + local, sketch size $=32,13$ layers): 118 M\n6. Polysketch (random, sketch size $=32,64,13$ layers), Polysketch (random + local, sketch size $=32,64,13$ layers), Performer ( 2 k features): 117 M\n\n## H.2. Medium scale models\n\nThe default configuration has 24 layers, 16 attention heads per layer, each attention head has head size 64 . The number of parameters of each model is stated as the following. 24 layer models:\n\n1. Softmax, Polynomial (degree $=4,8$ ): 337 M\n2. Polysketch (learned, sketch size $=64,24$ layers), Polysketch (learned + local, sketch size $=64,24$ layers): 341 M\n3. Polysketch (learned, sketch size $=32$, 24 layers), Polysketch (learned + local, sketch size $=32,24$ layers): 337M\n26 layer models:\n4. Polysketch (learned, sketch size $=64,26$ layers), Polysketch (learned + local, sketch size $=64,26$ layers): 367 M\n5. Polysketch (learned, sketch size $=32$, 26 layers), Polysketch (learned + local, sketch size $=32,26$ layers): 362 M\n\n## H.3. Large scale models\n\nThe default configuration has 36 layers, 20 attention heads per layer, each attention head has head size 64 . The number of parameters of each model is stated as the following. 36 layer models:\n\n1. Softmax, Polynomial $($ degree $=4,8): 748 \\mathrm{M}$\n39 layer models:\n2. Polysketch (learned, sketch size $=64,39$ layers), Polysketch (learned + local, sketch size $=64,39$ layers): 817 M\n3. Polysketch (learned, sketch size $=32,39$ layers), Polysketch (learned + local, sketch size $=32,39$ layers): 811 M\n\n## I. Reciepe of Transformer++\n\nWe add sinusoidal position embeddings (Vaswani et al., 2017) to the input embeddings and use Rotary Position Embeddings (RoPE) (Su et al., 2021) at all attention heads.",
    "polysketchformer-39": "We use Gated Linear Units (Dauphin et al., 2017; Shazeer, 2020) with an expansion factor of 4 as the FeedForward layer in the network. We use GELU as the non-linearity. All models are trained using Adam optimizer with weight decay and a peak learning rate of $7 \\mathrm{e}-4$. [^0]:    ${ }^{1}$ Carnegie Mellon University ${ }^{2}$ Google Research. Correspondence to: Praneeth Kacham [pkacham@cs.cmu.edu](mailto:pkacham@cs.cmu.edu), Vahab Mirrokni [mirrokni@google.com](mailto:mirrokni@google.com), Peilin Zhong [peilinz@google.com](mailto:peilinz@google.com). ${ }^{1}$ Our implementation is available at https://github. com/google-research/google-research/tree/master/ polysketchformer\n\n[^1]:    ${ }^{2}$ \"Approximation\" is used informally here, since some \"efficient transformers\" deviate significantly from the vanilla model. [^2]:    ${ }^{3} n$ denotes the context length - the number of input tokens. ${ }^{4}$ In standard softmax attention, $\\sigma(\\mathbf{x}, \\mathbf{y}):=\\exp (\\langle\\mathbf{x}, \\mathbf{y}\\rangle / \\sqrt{h})$. We omit $\\sqrt{h}$ here for simplicity of the presentation. [^3]:    ${ }^{5} \\mathrm{It}_{\\triangle}(\\mathrm{M})$ denotes the matrix obtained by only keeping the lower triangular entries of $M$ and zeroing the rest of the entries. [^4]:    ${ }^{6}$ Layer normalization shifts the entries of the input vector to make them have mean 0 and learns a suitable bias during training. [^5]:    ${ }^{7}$ We find a speed-up increasing the default 128 block size to 256 and 512 under our experimental setting. When increasing the block size to 1024, FlashAttention ran out of memory under our empirical setup. ${ }^{8}$ When using 4096 features, Performer ran out of memory in our experiments. "
}