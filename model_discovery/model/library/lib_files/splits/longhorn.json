{
    "longhorn-0": "# LonGHORn: State Space MoDELS ARE AMORTIZED ONLINE LEARNERS \n\nBo Liu ${ }^{\\dagger}$, Rui Wang ${ }^{\\ddagger}$, Lemeng Wu ${ }^{\\dagger}$, Yihao Feng ${ }^{\\dagger}$, Peter Stone ${ }^{\\dagger}$, Qiang Liu ${ }^{\\dagger}$<br>${ }^{\\dagger}$ The University of Texas at Austin, ${ }^{\\ddagger}$ Helixon<br>$\\{b l i u, l m w u, y i h a o, p s t o n e, l q i a n g\\} @ c s . u t e x a s . e d u$\n\n\n#### Abstract\n\nThe most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as \"sequence modeling.\" Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks. ${ }^{1}$\n\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_29b32183bedbbd493455g-01.jpg?height=392&width=1102&top_left_y=1384&top_left_x=510)\n\nFigure 1: (left) The average perplexity across eight downstream datasets for GLA, Mamba and Longhorn (1.3B model) over seen tokens on SlimPajama. Longhorn leads to a 1.8 x speed up in sampling efficiency. (right) Longhorn, pretrained with 2048 context length, successfully extrapolates to longer context (up to 16x of the trained context). ## 1 INTRODUCTION\n\nThe Transformer model has become the go-to architecture for sequence modeling in deep learning (Vaswani et al., 2017). However, its utility is constrained by the quadratic growth in training and decoding costs with increasing sequence length. Despite various optimizations such as efficient decoding (Chen et al., 2023; Kuperman \\& Dyke, 2011), KV-cache compression (DeepSeek-AI \\& Dai, 2024), and memory efficient implementation (Dao et al., 2022), scaling Transformers for autonomous and continual use with an infinite (or very long) context window remains a challenge. Recent advancements in linear attention models (Katharopoulos et al., 2020) and state-space models (SSMs)(Gu et al., 2021) have demonstrated their potential. These models are specialized recurrent neural networks capable of efficiently computing outputs in parallel when input tokens are provided\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_29b32183bedbbd493455g-02.jpg?height=372&width=1395&top_left_y=280&top_left_x=364)\n\nFigure 2: (left) Most existing sequence models consist of channel and sequence mixing layers. The sequence mixing layers can be viewed as \"meta-modules\" that compress history into a state $s_{t}$, which is then passed to later layers for sequence modeling. (middle) One can view sequence mixing as addressing an online learning problem, where the state of the SSM follows certain dynamics that optimize an online learning objective. The recurrent update of $s$ can then be derived by solving the online objective in closed form or by conducting a gradient step. (right) Longhorn's dynamics is derived from solving an online regression problem, where the goal is to recover $x \\in \\mathbb{R}$ (if $x \\in \\mathbb{R}^{d}$ we can run $d$ SSMs in parallel) based on a hint $k \\in \\mathbb{R}^{m}$ from a state vector $s \\in \\mathbb{R}^{d}$. The closed-form solution to the online regression problem corresponds to the implicit online learning update, where $\\Delta_{t}=\\beta_{t} /\\left(1+\\beta_{t} k_{t}^{\\top} k_{t}\\right)$.",
    "longhorn-1": "Please see the details in Section 3. simultaneously during training, thus avoiding the inefficiencies of traditional backpropagation through time. During inference, the recurrent form is employed, resulting in linear decoding efficiency. Initially, these models underperformed compared to Transformers. However, recent SSMs (Gu \\& Dao, 2023; Yang et al., 2023; Peng et al., 2024; De et al., 2024; Beck et al., 2024) have achieved performance parity with Transformers in language modeling tasks. Despite extensive research into various design aspects of SSMs, a guiding principle for designing SSMs remains elusive. In this work, we propose one potential principle. We observe that one can view SSMs (or any sequence mixing layers) as \"meta modules\" that compress the history into a memory state which is then used by later layers in the network for sequence modeling. From this perspective:\n\nThe recurrent form of SSMs can be viewed as solving an online learning problem. As a result, we can draw inspiration from online learning and confine the design choices of SSMs to reflect those learning dynamics that solve specific online prediction problems. The hope is that by selecting the right objective, one can save parameters/computation without sacrificing performance. In addition, the proposed online learning perspective might lead to a better understanding of what SSM layers are doing in a large model. Specifically, we view the recurrent update (a.k.a., the statetransition dynamics) of an SSM model as either a gradient descent step or a closed-form solution to an online learning objective. We provide the objectives for some existing SSMs in Table 1. Based on this insight, we propose a simple yet effective architecture (Longhorn), derived from the implicit closed-form update of an online associative recall problem. The closed-form update naturally leads to a stable recurrent form without a manually designed gating mechanism. So Longhorn does not need a separately parameterized forget gate, which saves parameters when the state size is large. We demonstrate that Longhorn performs comparably to or better than state-of-the-art SSMs like Mamba (Gu \\& Dao, 2023) on both synthetic and large-scale sequence modeling tasks. In particular, Longhorn outperforms Mamba at the size of 1.3B-parameter when trained on 100B tokens from the SlimPajama dataset (Soboleva et al., 2023). To summarize, our contributions are:\n\n1) Theoretical Framework: We propose a novel framework that views SSM's recurrent update as solving online learning objectives. As a result, the design of SSMs reduces to the design of the online learning objectives. In particular, we introduce a novel, simple, and effective SSM, named Longhorn, that explicitly solves an online associative recall problem. Longhorn's recurrent update is obtained by the closed-form solution to the online learning objective. Consequently, Longhorn does not require a separately parameterized forget gate that appears in most existing SSMs. 2) Empirical Results: Longhorn demonstrates better performance than existing SSMs including Mamba, across both synthetic associative recall tasks and the large-scale language modeling task. Moreover, it achieves 1.8x enhancement in sampling efficiency compared to Mamba (See Figure 1 (left)). Longhorn's training speed is as fast as Mamba, as we only replace the SSM module in the Mamba architecture with Longhorn's recurrence. So it serves as a drop-in replacement for Mamba. ```\nAlgorithm 1 Longhorn's Single-layer SSM Recurrence (Inference Time)\n    Parameters: \\(W_{q} \\in \\mathbb{R}^{m \\times d}, W_{k} \\in \\mathbb{R}^{m \\times d}, W_{\\beta} \\in \\mathbb{R}^{d \\times d}\\), where \\(W_{\\beta}\\) can be low-rank, horizon \\(T\\). Initialize the memory state \\(S_{0} \\leftarrow 0^{d \\times m}\\). for \\(t \\in\\{1, \\ldots, T\\}\\) do\n        1) Receive input \\(x_{t} \\in \\mathbb{R}^{d}\\). 2) Compute the query \\(q_{t}\\), key \\(k_{t}\\) and \\(\\beta_{t}\\) (as in objective \\(\\left\\|s-s_{t-1}\\right\\|^{2}+\\beta_{t}\\left\\|s^{\\top} k_{t}-x_{t}\\right\\|^{2}\\) ):\n```\n\n$$\nq_{t}=W_{q} x_{t} \\in \\mathbb{R}^{m}, \\quad k_{t}=W_{k} x_{t} \\in \\mathbb{R}^{m}, \\quad \\beta_{t}=\\operatorname{Sigmoid}\\left(W_{\\beta} x_{t}\\right) \\in(0,1)^{d}\n$$\n\n6: 3) Update the memory state $S_{t} \\in \\mathbb{R}^{d \\times m}$ via\n\n$$\nS_{t}=\\left(1-\\Delta_{t} \\otimes k_{t}^{\\odot 2}\\right) \\odot S_{t-1}+\\left(\\Delta_{t} \\odot x_{t}\\right) \\otimes k_{t}\n$$\n\nwhere $\\Delta_{t}$ is the step size:\n\n$$\n\\Delta_{t}=\\beta_{t} /\\left(1+\\beta_{t} k_{t}^{\\top} k_{t}\\right) \\in(0,1)^{d}\n$$\n\n7: 4) Compute the output $o_{t}=S_{t} q_{t} \\in \\mathbb{R}^{d}$. : end for\n9: Note: $\\odot$ elementwise product and $\\otimes$ is outer product. $x_{t}$ in practice is preprocessed through a linear projection followed by a Conv1d operation as in Mamba (Gu \\& Dao, 2023). Lastly, Longhorn, trained with 2048 context length can extrapolate to 32 K context length at inference time without much perplexity drop (See Figure 1 (right)). Notation Throughout this work, we use $\\odot$ to denote the Hadamard (elementwise) product, and $\\otimes$ to denote the Kronecker (or outer) product between two tensors. Uppercase letters $A, B$, etc. denote matrices, while lowercase $k, v$ are in general vectors. $\\|\\cdot\\|$ by default refers to the $\\ell_{2}$ norm for vectors. ## 2 BACKGROUND\n\nIn this section, we provide a brief introduction to contemporary deep state space models (deep SSMs). Modern large language models are sequence-to-sequence models consisting of a stack of layers $\\boldsymbol{y}=\\Phi_{L} \\circ \\cdots \\circ \\Phi_{1}(\\boldsymbol{x})$ that sequentially processes an input sequence $\\boldsymbol{x}=\\left\\{x_{t}\\right\\}_{t=1}^{T}$, where $T$ is the context length. Specifically, transformers consist of alternative stacks of self-attention (SA) and multi-layer perceptron (MLP) layers that conduct mixing (i.e., information aggregation) on the sequence and channel dimensions, respectively. Deep SSMs replace the SA layers with SSM layers. Some variants of SSM models leave the MLP layers unchanged (Sun et al., 2023; Yang et al., 2023; De et al., 2024), while others fuse the SSM layer and the MLP layer into a single unified module (Gu \\& Dao, 2023). But in both cases, the sequence mixing is done by the SSM module, and the channel mixing is done by the channel-wise MLP. Taking Mamba as an example (Gu \\& Dao, 2023), a Mamba model consists of a stack of homogeneous modules named Mamba block (the $\\Phi(\\boldsymbol{x})$ ); we provide a visualization of a single Mamba block in Figure 3 (Gu \\& Dao, 2023), which consists of an SSM block for sequence mixing (red), and an MLP block for channel mixing (blue). SSM: General Form The SSM block (in red) plays the crucial role of sequence mixing. It works by iteratively updating a memory state matrix $S_{t} \\in \\mathbb{R}^{d \\times m}$ with a linear recurrence:\n\n$$\nS_{t}=A\\left(x_{t}\\right) * S_{t-1}+B\\left(x_{t}\\right), \\quad \\forall t \\in\\{1, \\ldots, T\\}, \\quad S_{0}=0\n$$\n\nwhere $x_{t}$ is the input at time $t, S_{t}$ is the model's state, $A_{t}, B_{t}: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d \\times m}$ are some functions and * is a multiplication operation of choice, such as Hadamard product or matrix product. Given the state $S_{t}$, SSMs often give the output token at the next layer via a gated linear unit (GLU) (Dauphin et al., 2017):\n\n$$\ny_{t}=\\operatorname{Readout}\\left(S_{t}, x_{t}\\right)=W_{1}\\left(o_{t} \\odot \\sigma\\left(W_{2} x_{t}\\right)\\right), \\quad o_{t}=C\\left(x_{t}\\right) S_{t}\n$$\n\nwhere we first get $o_{t}$ via a state-dependent linear projection on $S_{t}$, which is then fed into a subsequent channel mixing gated linear unit (blue in Figure 3), where $\\sigma(\\cdot)$ is a non-linear activation function. A key feature of this design in Equation 1 is that the update of $S_{t}$ is a linear recurrence relation. That is, $S_{t}$ is a linear function of $S_{t-1}$. Crucially, this allows us to express all $S_{t}$ in an explicit form that can be calculated in parallel: when all $\\boldsymbol{x}=\\left\\{x_{t}\\right\\}_{t}$ are available as in the training phase, $\\left\\{S_{t}\\right\\}_{t}$ can be written into\n\n$$\nS_{t}=\\sum_{t^{\\prime} \\leqslant t}\\left(\\bar{A}_{t^{\\prime} \\rightarrow t}\\right) B\\left(x_{t^{\\prime}}\\right), \\quad \\text { where } \\quad \\bar{A}_{t^{\\prime} \\rightarrow t}=\\prod_{t^{\\prime}<\\tau \\leqslant t} A\\left(x_{\\tau}\\right)\n$$\n\nHere $\\prod$ denotes the product induced by multiplication operator $*$. The resulting cumulative product $\\bar{A}_{t^{\\prime} \\rightarrow t}$ can be implemented efficiently in parallel with the prefix scan algorithm (Harris et al., 2007), which\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_29b32183bedbbd493455g-04.jpg?height=370&width=400&top_left_y=585&top_left_x=1356)\n\nFigure 3: Mamba Block only requires a sublinear complexity in terms of sequence length (e.g., $\\mathcal{O}(\\log t)$ ). From now on, we will abbreviate $A\\left(x_{t}\\right)$ and $B\\left(x_{t}\\right)$ as $A_{t}$ and $B_{t}$, respectively. Designs of $\\left(A_{t}, B_{t}, *\\right)$ Existing variants of SSMs mainly differs in the design choices of the networks $A_{t}, B_{t}$, and the associated operator $*$ in the linear recurrence. A core issue here is that the memory state $S_{t} \\in \\mathbb{R}^{d \\times m}$, designed be $m$ times of the input $x_{t}$ in size, must be as large as possible to maintain sufficient information during recurrence. This makes the architecture design of $A_{t}, B_{t}$, both mapping $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d \\times m}$ challenging. A naive linear lay would result in $d \\times d \\times m$ weights, which is prohibitively large. This makes it necessary to impose certain low-dimensional structures in $A_{t}, B_{t}$, which is what differentiates the existing designs of SSM. In the following, we provide some examples in the form of Equation 1 for some existing SSM models. Example 2.1 (Linear Attention Variants). Linear Attention (LA) (Katharopoulos et al., 2020), Retention Network (RetNet) (Sun et al., 2023), and Gated Linear Attention (GLA) (Yang et al., 2023) all assume $A_{t}, B_{t}$ yield rank-1 (or even constant) outputs:\n\n$$\nS_{t}=A_{t} \\odot S_{t-1}+v\\left(x_{t}\\right) \\otimes k\\left(x_{t}\\right), \\quad \\text { with } \\begin{cases}A_{t}=1 & (\\text { LA) } \\\\ A_{t}=c \\in[0,1] & (\\text { RetNet }) \\\\ A_{t}=1 \\otimes \\alpha\\left(x_{t}\\right) & (\\text { GLA })\\end{cases}\n$$\n\nwhere $S_{t} \\in \\mathbb{R}^{d \\times m}, v\\left(x_{t}\\right) \\in \\mathbb{R}^{d}, k\\left(x_{t}\\right) \\in \\mathbb{R}^{m}$ are linear mappings of $x_{t}$, and $\\otimes$ denote the outer product. In practice, one can use $h$ heads as in the multi-head attention to save some computation, where the $m$ and $d$ dimensions are divided into $h$ groups and each group performs its own LA variant. The outer product complexity reduces to $\\mathcal{O}(h * m / h * d / h=m d / h)$. But then the effective size of $S_{t}$ also shrinks to $m d / h$. Example 2.2 (Mamba (Gu \\& Dao, 2023)). The Mamba architecture is derived by discretizing a continuous linear dynamics. Its discretized update is:\n\n$$\n\\begin{aligned}\n& S_{t}=A_{t} \\odot S_{t-1}+B_{t}, \\quad \\text { where } \\\\\n& A_{t}=\\exp \\left(A \\odot\\left(\\Delta\\left(x_{t}\\right) \\otimes 1\\right)\\right), \\quad B_{t}=\\left(\\Delta\\left(x_{t}\\right) \\odot x_{t}\\right) \\otimes k\\left(x_{t}\\right)\n\\end{aligned}\n$$\n\nwhere $S_{t} \\in \\mathbb{R}^{d \\times m}$ with $m=16$ by default, $\\Delta\\left(x_{t}\\right) \\in \\mathbb{R}^{d}, k\\left(x_{t}\\right) \\in \\mathbb{R}^{m}$ linear mappings of $x_{t}$, and $A \\in \\mathbb{R}^{d \\times m}$ is a data independent (not depending on $x_{t}$ trainable weight matrix. In Mamba, both $A_{t}$ and $B_{t}$ depend on $\\Delta\\left(x_{t}\\right)$, which represents the step size for the SSM update. In practice, Mamba does not use multiple heads as in linear attention variants. Perhaps the main reason is that given a fixed $m$ and $d$, the largest memory state will be with $h=1$ (as the effective size of $S_{t}$ is $\\mathrm{md} / \\mathrm{h}$ ). In addition, Mamba's output is $o_{t}=C\\left(x_{t}\\right) S_{t}+D_{t} \\odot x_{t}$, which has an additional residual part $D_{t} \\odot x_{t}$. Example 2.3 (Griffin (De et al., 2024)). In Mamba and the linear attention variants, the outer product serves as a critical rule of lifting vectors to matrices. The recent Griffin architecture abandons the outer product and performs pure elementwise product:\n\n$$\ns_{t}=a\\left(x_{t}\\right) \\odot s_{t-1}+\\sqrt{1-a\\left(x_{t}\\right)} \\odot i\\left(x_{t}\\right) \\odot x_{t}\n$$\n\nwhere $s_{t}, a\\left(x_{t}\\right), i\\left(x_{t}\\right)$ are all $\\mathbb{R}^{d}$. This yields smaller memory states, but in practice, Griffin is combined with local-attention (i.e., the sliding-window self-attention) to strengthen its capability. Example 2.4 (RWKV (Peng et al., 2023)). The original RWKV also performs elementwise recurrence. It maintains a state of ratio form $s_{t}=u_{t} / z_{t}$, where $u_{t}, z_{t}$ are updated separately by two SSMs:\n\n$$\n\\begin{aligned}\n& s_{t}=u_{t} / z_{t} \\\\\n& u_{t}=\\exp (-w) \\cdot u_{t-1}+\\exp \\left(k\\left(x_{t}\\right)\\right) \\odot v\\left(x_{t}\\right), \\quad \\quad z_{t}=\\exp (-w) \\cdot z_{t-1}+\\exp \\left(k\\left(x_{t}\\right)\\right)\n\\end{aligned}\n$$\n\nwhere all the vectors are of size $\\mathbb{R}^{d}$, and $w>0$ is a trainable weight for controlling the forgetting. In the most recent RWKV version (Peng et al., 2024), the denominator $z_{t}$ is removed, and the elementwise product is replaced with the outer product, which makes it more similar to an LA variant. Example 2.5 (HGRN2 (Qin et al., 2024a)). The Gated Linear RNNs with State Expansion (HGRN2) model is represented with the following recurrence:\n\n$$\nS_{t}=\\left(1 \\otimes f\\left(x_{t}\\right)\\right) \\odot S_{t-1}+i\\left(x_{t}\\right) \\otimes\\left(1-f\\left(x_{t}\\right)\\right)\n$$\n\nHere, $f\\left(x_{t}\\right) \\in[0,1]$ is the forget gate, $\\left(1-f\\left(x_{t}\\right)\\right)$ is the input gate, and $i\\left(x_{t}\\right)$ is the input vector. HGRN2 thus resembles an RNN. ## 3 An Online Learning Perspective For SeQuence Mixing\n\nAs demonstrated in the previous section, designing a state-space model (SSM) depends on the specific selection of $\\left(A_{t}, B_{t}, *\\right)$, which is intricate and somewhat artisanal. In this section, we propose to streamline SSM design through an online learning perspective. The main idea is to treat the SSM layers as learning modules that learn to compress information along the sequence dimension. From this perspective, the SSM layers are learning to learn, such that during the inference time, these layers are still learning (compressing) new information online. We begin with an overview of online learning and subsequently demonstrate how SSM can be framed as an online learning problem. Finally, we present a straightforward architecture based on the closed-form solution of the implicit online learning algorithm. ### 3.1 SSM AS ONLINE LEARNING\n\nWe advocate viewing the recurrence of SSM as solving an online learning problem. In online learning, the agent (the algorithm) picks a state $s_{t}$ at time $t$ and then incurs a loss $\\ell_{t}\\left(s_{t}\\right)$. The goal is to minimize\n\n$$\n\\min _{\\left\\{s_{t}\\right\\}} \\sum_{t} \\ell_{t}\\left(s_{t}\\right)\n$$\n\nFor instance, consider online linear prediction, where at each step the agent is given an input-label pair $\\left(x_{t}, y_{t}\\right)$ and $\\ell_{t}\\left(s_{t}\\right)=\\frac{1}{2}\\left\\|s_{t}^{\\top} x_{t}-y_{t}\\right\\|^{2}$ is the $\\ell_{2}$ regression loss, then the problem becomes an online regression problem, and the goal is to successfully predict $y_{t}$ given $x_{t}$ at future time steps. Online convex optimization (OCP) yields a principled approach to solving Equation 4 when $\\ell_{t}$ are convex, by trading-off the \"stability\" and \"plasiticity\" (Mermillod et al., 2013). Formally, an online convex programming algorithm updates $s_{t}$ by solving a regularized cost function:\n\n$$\ns_{t}=\\underset{s}{\\arg \\min } L_{t}(s), \\quad L_{t}(s)=\\underbrace{D_{\\phi}\\left(s, s_{t-1}\\right)}_{\\text {stability }}+\\underbrace{\\beta_{t} \\ell_{t}(s)}_{\\text {plasticity }}\n$$\n\nwhere $\\beta_{t} \\in \\mathbb{R}^{+}$and $D_{\\phi}$ is a discrepancy measure, often a Bregman divergence induced by the convex function $\\phi$ (e.g., when $\\phi(x)=\\frac{1}{2}\\|x\\|^{2}, D_{\\phi}\\left(s, s_{t-1}\\right)=\\frac{1}{2}\\left\\|s-s_{t-1}\\right\\|^{2}$ ). Here the first term ensures the updated $s$ will be close to the previous $s_{t-1}$, so the agent suffers less from catastrophic forgetting, while the second term ensures the agent is incorporating new knowledge from minimizing the new loss $\\ell_{t}(s)$. Hence, $\\beta_{t}$ controls the trade-off between stability and plasticity. ### 3.2 SSM AS ONLINE LEARNING AND THE LONGHORN ARCHiteCTURE\n\nUnder the online learning framework, the design of SSM reduces to the design of $D_{\\phi}$ and $\\ell_{t}$ in Equation 5. This provides a unified framework for the existing SSM variants. We summarize in Table 1 the online learning interpretation of several existing SSM architectures. In this work, we explore a highly simplified and natural design called Longhorn guided by the online principle (see the last row of Table 1). In particular, we consider $\\left\\{x_{t}\\right\\}_{t}$ as the input stream, where $x_{t} \\in \\mathbb{R}$ is a scalar (as in Mamba, one can consider there are $d$ parallel SSMs if $x_{t} \\in \\mathbb{R}^{d}$ is a vector). We want to recurrently update hidden states $\\left\\{s_{t}\\right\\}_{t}$, where $s_{t} \\in \\mathbb{R}^{m}$ is a vector that aims to summarize the information up to time $t$. We posit the following OCP objective for updating $s_{t}$ :\n\n$$\ns_{t}=\\underset{s \\in \\mathbb{R}^{m}}{\\arg \\min }\\left\\|s-s_{t-1}\\right\\|^{2}+\\beta_{t}\\left(x_{t}\\right)\\left\\|s^{\\top} k_{t}\\left(x_{t}\\right)-x_{t}\\right\\|^{2}\n$$\n\nHere, $k_{t}\\left(x_{t}\\right) \\in \\mathbb{R}^{m}$ is the linear transformation of the input token $x_{t} \\in \\mathbb{R}$, which represents the key vector in a standard transformer. $\\beta_{t}\\left(x_{t}\\right)$ controls how much new information about $x_{t}$ we want the model to incorporate. For instance, $\\beta_{t}\\left(x_{t}\\right)=0$ implies $s_{t}=s_{t-1}$, while a large $\\beta_{t}\\left(x_{t}\\right)$ implies the model sacrifices stability for plasticity. For simplicity, we will write $k_{t}=k_{t}\\left(x_{t}\\right)$ and $\\beta_{t}=\\beta_{t}\\left(x_{t}\\right)$ from now on. From a high-level, Equation 6 is solving an online prediction problem of learning a weight $s$ to predict $x_{t}$ given $k_{t}$ with a linear model $x_{t} \\approx s^{\\top} k_{t}$. It is a supervised formulation of the associative memory problem of memorize $\\left(k_{t}, x_{t}\\right)$ pairs by learning a mapping from $k_{t}$ to $x_{t}$, such that given a key (input) $k_{t}$ the model can retrieve (predict) its corresponding value (label) $x_{t}$. Fortunately, this simple objective gives a closed-form solution for $s_{t}$, which coincides with the implicit online learning method (Kulis \\& Bartlett, 2010). Theorem 3.1. The closed form solution for $s_{t}$ for objective in Equation 6 is\n\n$$\ns_{t}=\\left(I-\\Delta_{t} k_{t} k_{t}^{\\top}\\right) s_{t-1}+\\Delta_{t} k_{t} x_{t}, \\quad \\text { where } \\Delta_{t}=\\frac{\\beta_{t}}{1+\\beta_{t} k_{t}^{\\top} k_{t}} \\in[0, \\infty)\n$$\n\nProof. As the objective in Equation 6 is in a quadratic form with respect to $s$, there is a unique minimum:\n\n$$\n\\begin{aligned}\n\\nabla L_{t}(s)=0 & \\Longleftrightarrow\\left(s-s_{t-1}\\right)+\\beta_{t}\\left(s^{\\top} k_{t}-x_{t}\\right) k_{t}=0 \\\\\n& \\Longleftrightarrow\\left(I+\\beta_{t} k_{t} k_{t}^{\\top}\\right) s=s_{t-1}+\\beta_{t} k_{t} x_{t} \\\\\n& \\Longleftrightarrow(3) s=\\left(I-\\frac{\\beta_{t}}{1+\\beta_{t} k_{t}^{\\top} k_{t}} k_{t} k_{t}^{\\top}\\right) s_{t-1}+\\left(I-\\frac{\\beta_{t}}{I+\\beta_{t} k_{t}^{\\top} k_{t}} k_{t} k_{t}^{\\top}\\right) \\beta_{t} k_{t} x_{t} \\\\\n& \\Longleftrightarrow\\left(I-\\frac{\\beta_{t}}{I+\\beta_{t} k_{t}^{\\top} k_{t}} k_{t} k_{t}^{\\top}\\right) s_{t-1}+\\frac{\\left(I+\\beta_{t} k_{t}^{\\top} k_{t}-\\beta_{t} k_{t} k_{t}^{\\top}\\right) \\beta_{t} k_{t} x_{t}}{I+\\beta_{t} k_{t}^{\\top} k_{t}} \\\\\n& \\underbrace{\\Longleftrightarrow}_{(5)}\\left(I-\\frac{\\beta_{t}}{I+\\beta_{t} k_{t}^{\\top} k_{t}} k_{t} k_{t}^{\\top}\\right) s_{t-1}+\\frac{\\beta_{t} k_{t} x_{t}}{I+\\beta_{t} k_{t}^{\\top} k_{t}}\n\\end{aligned}\n$$\n\n(3) is derived from the fact that $\\left(I+\\beta_{t} k_{t} k_{t}^{\\top}\\right)^{-1}=\\left(I-\\frac{\\beta_{t} k_{t} k_{t}^{\\top}}{I+\\beta k_{t}^{\\top} k_{t}}\\right)$ by the Sherman-Morrison formula.",
    "longhorn-2": "(5) is derived by noticing that $k_{t}^{\\top} k_{t} k_{t} x_{t}-k_{t} k_{t}^{\\top} k_{t} x_{t}=0$. As $k_{t} k_{t}^{\\top}$ is a matrix, it is hard to compute its cumulative product for conducting a parallel scan. As a result, in practice, we use the diagonal approximation $1_{m}-\\Delta_{t} k_{t}^{\\odot 2}$ in place of $I-\\Delta_{t} k_{t} k_{t}^{\\top}$, where $x^{\\odot 2}=x \\odot x$ and $1_{m}$ is the $m$-dimensional vector of all ones. The final SSM recurrence therefore becomes:\n\n$$\ns_{t}=\\underbrace{\\left(1-\\Delta_{t} k_{t}^{\\odot 2}\\right)}_{A_{t}\\left(x_{t}\\right)} \\odot s_{t-1}+\\underbrace{\\Delta_{t} k_{t} x_{t}}_{B_{t}\\left(x_{t}\\right)}\n$$\n\nFollowing the Mamba design, when $x_{t} \\in \\mathbb{R}^{d}$ is a vector, we use $d$ parallel SSMs. As a result, $x_{t}, \\beta_{t}, \\Delta_{t} \\in \\mathbb{R}^{d}, k_{t} \\in \\mathbb{R}^{m}, S_{t} \\in \\mathbb{R}^{d \\times m}$, Longhorn's update becomes:\n\n$$\nS_{t}=A_{t} \\odot S_{t-1}+B_{t}, \\quad \\text { where } A_{t}=\\left(1-\\Delta_{t} \\otimes k_{t}^{\\odot 2}\\right), \\quad B_{t}=\\left(\\Delta_{t} \\odot x_{t}\\right) \\otimes k_{t}\n$$\n\n| Method | Online Learning Objective $L_{t}(s)$ (assume $\\left.x_{t} \\in \\mathbb{R}\\right)$ | Online Update |\n| :--- | :--- | :--- |\n| LA | $\\left\\\\|s-s_{t-1}\\right\\\\|^{2}-2\\left\\langle s^{\\top} k_{t}, x_{t}\\right\\rangle$ | $s_{t}=s_{t-1}+k_{t} x_{t}$ |\n| RetNet | $\\gamma\\left\\\\|s-s_{t-1}\\right\\\\|^{2}+(1-\\gamma)\\\\|s\\\\|^{2}-2\\left\\langle s^{\\top} k_{t}, x_{t}\\right\\rangle$ | $s_{t}=\\gamma s_{t-1}+k_{t} x_{t}$ |\n| GLA | $\\left(s-s_{t-1}\\right)^{\\top} \\operatorname{diag}\\left(\\alpha_{t}\\right)\\left(s-s_{t-1}\\right)+s^{\\top} \\operatorname{diag}\\left(1-\\alpha_{t}\\right) s-2\\left\\langle s^{\\top} k_{t}, x_{t}\\right\\rangle$ | $s_{t}=\\operatorname{diag}\\left(\\alpha_{t}\\right) s_{t-1}+k_{t} x_{t}$ |\n| Griffin | $\\left\\\\|\\sqrt{\\alpha_{t}} \\odot\\left(s-s_{t-1}\\right)\\right\\\\|^{2}+\\left\\\\|\\sqrt{1-\\alpha_{t}} \\odot s\\right\\\\|^{2}-2 \\sqrt{1-\\alpha_{t}} \\odot s \\odot i_{t} \\odot x_{t}$ | $s_{t}=\\alpha_{t} \\odot s_{t-1}+\\sqrt{\\left(1-\\alpha_{t}\\right) \\odot i_{t} \\odot x_{t}}$ |\n| Longhorn | $\\left\\\\|s-s_{t-1}\\right\\\\|^{2}+\\beta_{t}\\left\\\\|s^{\\top} k_{t}-x_{t}\\right\\\\|^{2}$ | $s_{t}=\\left(1-\\Delta_{t} k_{t}^{\\odot 2}\\right) s_{t-1}+\\Delta_{t} k_{t} x_{t}$ <br> $\\Delta_{t}=\\beta_{t} /\\left(1+\\beta_{t} k_{t}^{\\top} k_{t}\\right)$ |\n\nTable 1: Some of the existing SSMs and their corresponding online learning objectives/updates. The final architecture of Longhorn follows Mamba strictly (Figure 3), and we only replace the SSM block with the Longhorn's recurrence.",
    "longhorn-3": "We also provide an efficient CUDA kernel for it. The full inference-time algorithm is provided in Algorithm 1. One can compare Equation 9 to Equation 3 and other SSM in Section 2. Longhorn does not introduce an extra \"forgetting\" gate (hence it has fewer parameters), because the forget gate is naturally derived from the key vector, i.e., $\\left(1-k^{\\odot 2} \\otimes \\Delta_{t}\\right)$. ## Advantages of Longhorn:\n\n1. While we can derive the learning objective for some of the existing SSMs, Longhorn is the first SSM designed for explicitly solving an online regression problem. 2. Longhorn does not require a specific forget gate (e.g., $\\alpha_{t}$ in GLA or $A$ matrix in Mamba). The forgetting is naturally linked to the key vector $k_{t}$ through the derivation. This saves about $\\mathcal{O}(d \\times m)$ parameters per SSM module, where $m$ is the dimension of $k_{t}$, and $d$ is the dimension of $x_{t}$. However, Longhorn demonstrates better performance even with fewer parameters than Mamba (See Figure 1 (left), Table 3, Table 4). 3. The closed-form solution in Equation 7 does not need any specific initialization. In contrast, Mamba requires careful initialization of the $A$ and $\\Delta_{t}$. Compared to DeltaNet (Schlag et al., 2021; Yang et al., 2024), we restrict the state-transition matrix to be diagonal, hence Longhorn can leverage both parallel scan (as in Mamba) and the chunk-wise parallelization (as in GLA). Therefore, it is as fast as existing SSMs.",
    "longhorn-4": "4. Unlike DeltaNet, which, as noted in Yang et al. (2024), cannot extrapolate beyond the training context, Longhorn demonstrates the capability to successfully extrapolate to contexts up to 16x longer at inference time (See Figure 1 (right)). ## 4 RELATED WORK\n\nThis section provides a brief summary of recent advances in linear attention (LA) and deep state space models (SSMs). Linear Attention Models Several methods have been developed to address the quadratic complexity of the Transformer by making attention linear with respect to context length. In particular, Linformer uses a low-rank approximation of the self-attention by projecting the keys and values into a constant size matrix (instead of scaling with the sequence length)(Wang et al., 2020). Realizing that the main bottleneck causing the quadratic cost in Transformers is the Softmax function, the Linear Transformer replaces the Softmax function with a decomposable similarity function analogous to kernel methods, thereby making the computation linear with respect to sequence length(Katharopoulos et al., 2020). Performer approximates the softmax attention using positive orthogonal random features (Choromanski et al., 2020). More recently, based on the Linear Transformer, the Retentive Network (RetNet) adds additional constant forgetting and rotation (Sun et al., 2023). Gated Linear Attention (GLA) further experiments with learnable forget gates (Yang et al., 2023). Notably, linear attention can be viewed as a fast weight network where a slow net (the model's parameters) learns to program a changing fast network (e.g., a linear predictor) by adapting its parameters (e.g., the $s_{t}$ ) online using inputs (Schlag et al., 2021). State Space Models Instead of trying to linearize transformers, state space models (SSMs) start with parallelizable linear recurrent networks directly. Initially, the state transition matrix $A$ is assumed to be constant so that the recurrence can be computed in parallel using a convolution (Li et al., 2022; Gu et al., 2021). Subsequent developments include the Diagonal State Space (DSS) model (Gupta et al., 2022), Gated State Space (GSS) models (Mehta et al., 2022), S5 model (Smith et al., 2022), Bidirectional Gated SSM (BiGS)(Wang et al., 2022), H3 model(Fu et al., 2022), and Mamba (Gu \\& Dao, 2023). In addition, there are also works directly trying to make recurrent networks efficient, which often results in a particular form of SSM as well. This includes Deep Linear Recurrent Units (LRUs)(Orvieto et al., 2023; De et al., 2024), Hierarchically Gated Linear RNN (HGRN)(Qin et al., 2024b;a), and RWKV (Peng et al., 2023; 2024). Fast Weight Programmer The concept of a network modifying its own weights in response to inputs is not novel and has historical roots in the Fast-weight Programmer (Schmidhuber, 1992; 1993; Schlag \\& Schmidhuber, 2017; Schlag et al., 2021). Specifically, these models propose updating a weight matrix $W \\in \\mathbb{R}^{d \\times m}$ using the outer product of two vectors, expressed as $\\Delta W=v\\left(x_{t}\\right) \\otimes k\\left(x_{t}\\right)$. This mechanism closely aligns with the principles underlying the Linear Attention model. Our framework builds upon the Fast Weight concept by tailoring the weight update process to address a specific online learning objective, thereby extending its applicability and effectiveness in dynamic learning environments. Concurrent Work There are two concurrent works that share similar ideas to ours. Yang et al. (2024) propose a chunk-wise parallel form for scaling the DeltaNet (Schlag et al., 2021) to large-scale language modeling tasks (Yang et al., 2024). The update rule in DeltaNet can be viewed as conducting the gradient step instead of solving for the closed form of the online regression objective, hence the state transition matrix is $A\\left(x_{t}\\right)=\\left(I-\\beta_{t} k_{t} k_{t}^{\\top}\\right)$, which is not guaranteed to have eigenvalues $\\leqslant 1$ (the forgetting can even be negative). Therefore, in practice Yang et al. (2024) normalize the key vector $k_{t}$ by its $\\ell_{2}$ norm (Schlag et al., 2021), which can be restrictive. In contrast, the closed-form update in Longhorn ensures that the state transition matrix is always stable. In addition, $A\\left(x_{t}\\right)$ is a matrix in DeltaNet while we apply a diagonal approximation $\\left(k_{t}^{\\odot 2}\\right)$. Therefore, Longhorn can leverage either parallel scan (as in Mamba) and chunk-wise parallel training (as in GLA), making it as fast as any existing SSM. In practice, we provide a parallel scan CUDA kernel, so Longhorn serves as a drop-in replacement for Mamba. On the other hand, Sun et al. (2024) propose the Test-Time Training framework, where the update of the state is derived from applying one gradient step of an online regression objective (Sun et al., 2024). To make the model parallelizable, the authors assume that each gradient step at $x_{t}(t \\geqslant 0)$ is conducted at state $s_{0}$, hence deriving a parallel form that utilizes matrix multiplication. In contrast, Longhorn solves for the closed-form solution on every token, which potentially leads to a more flexible model. ## 5 EXPERIMENTS\n\nWe conduct experiments on both synthetic benchmarks and the language modeling task to validate the performance of Longhorn:\n\n1. To test Longhorn's performance on memorizing associations, we compare it against other SSMs on the synthetic multi-query associative recall benchmark (Arora et al., 2023). We find that Longhorn is the only model that achieves near perfect recall rate when the sequence length scales up to 512 with a hidden dimension of 64). 2. To see how Longhorn performs on language modeling tasks, we first conduct the scaling law experiments using the OpenWebText dataset (Gokaslan \\& Cohen, 2019), with model sizes of 120 M and 350 M and context length 1024 or 4096 , and then compare the validation perplexity of Longhorn against that of the other SSM models. We find that Longhorn consistently outperforms baseline deep SSM models across context lengths and model sizes. 3. In the end, to see how Longhorn performs on downstream language evaluation tasks, we train a 1.3B language model on the SlimPajama dataset (Soboleva et al., 2023) with 100B tokens, and then compare its downstream evaluation performance across 5 standard benchmarks. We find that Longhorn achieves 1.8x speedup sampling efficiency compared to Mamba, and also outperforms GLA and Mamba on average in downstream evaluation tasks. ### 5.1 Multi-Query Associative RECALl\n\nWe first consider the synthetic benchmark Multi-Query Associative Recal (MQAR)l (Arora et al., 2023). The task is the agent will see a sequence of tokens $\\left\\{k_{1}, v_{1}, k_{2}, v_{2}, \\ldots, k_{T}, v_{T}\\right\\}$, where each consecutive two-tokens become a key-value pair. At test time, the agent is provided with multiple $k \\sim\\left\\{k_{1}, \\ldots k_{T}\\right\\}$, the goal is for it to \"retrieve\" the corresponding values, hence the name multi-query associative recall. Following the original benchmark, we consider the sequence length $T \\in\\{64,128,256,512\\}$ and model dimension (size of the latent embedding of a token) $d \\in\\{64,128,256,512\\}$. We compare against 1) Transformer model (Attention), 2) Based architecture, which combines an SSM with local-attention, where the SSM is derived from the Taylor approximation of the self-attention (Arora et al., 2024), 3) Hyena (Poli et al., 2023), which is a special SSM that adopts long convolution via fast fourier transform, 4) RWKV (Peng et al., 2023), which can be viewed as the division of two SSMs (i.e., $y=a / b$, where $a, b$ are outputs from two SSMs). The state-transition matrix is a scalar, 5) BaseConv (Arora et al., 2023), an SSM that combines linear projection with convolution, and 6) Mamba (Gu \\& Dao, 2023), the state-of-the-art SSM that has data-dependent $A$ and $B$ (See Equation 3).",
    "longhorn-5": "Each experiment individually searches for the best learning rate from $\\{1 e-4,4.6 e-4,2.2 e-3,1 e-02\\}$ (evenly separated in the log space). Results are summarized in Figure 4. ![](https://cdn.mathpix.com/cropped/2024_09_12_29b32183bedbbd493455g-09.jpg?height=372&width=1359&top_left_y=1012&top_left_x=383)\n\nFigure 4: Comparison of Longhorn against state-of-the-art SSM variants on the MQAR benchmark. The y-axis is the recall rate. Observation: From the figure, we can see that Longhorn, which is designed to perform the associative recall task by solving the online prediction objective, outperforms existing SSM variants even at the sequence length of 512 and a small model dimension of 64 . ### 5.2 Scaling LaW On OpenWebText\n\nIn this section we consider language modeling tasks on models with 120 M or 350 M parameters with 1024 or 4096 context length. We choose the OpenWebText dataset as it is small and serves as an easily accessible benchmark for quick benchmarks. ${ }^{2}$. Following the Mamba (Gu \\& Dao, 2023) experiment setting, the training details are provided in Table 2 :\n\n| Params | n_layers | d_model | n_heads / d_head | Training steps | Learning Rate | Batch Size | Tokens |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 125M | 12 | 768 | $12 / 64$ | 4800 | $6 \\mathrm{e}-4$ | 0.5 M tokens | 2.5B |\n| 350M | 24 | 1024 | $16 / 64$ | 13500 | $3 \\mathrm{e}-4$ | 0.5 M tokens | 7B |\n\nTable 2: Training details on OpenWebText. We consider the following baseline models: LLaMA (Touvron et al., 2023), RetNet (Sun et al., 2023), Mamba (Gu \\& Dao, 2023), RWKV (Peng et al., 2023), and GLA (Yang et al., 2023). Then we experiment with 1024 or 4096 context length $T$, which model sizes around 120M or 350 M . Results are summarized in Figure 5 and Table 3. [^1]![](https://cdn.mathpix.com/cropped/2024_09_12_29b32183bedbbd493455g-10.jpg?height=376&width=1387&top_left_y=278&top_left_x=362)\n\nFigure 5: Scaling law with 1024 and 4096 context length on OpenWebText with various SSM models and the LLaMA (strong Transformer) baseline.",
    "longhorn-6": "| Model | \\# Param. (M) | Val. Loss ( $\\downarrow$ ) |  | \\# Param. (M) | Val. Loss ( $\\downarrow$ ) |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | $T=1024$ | $T=4096$ |  | $T=1024$ | $T=4096$ |\n| RetNet | 129.1 | 3.569 | 3.492 | 373.2 | 3.362 | 3.227 |\n| GLA | 123.8 | 3.381 | 3.364 | 361.1 | 3.018 | 3.001 |\n| RWKV | 124.4 | 3.291 | 3.276 | 354.8 | 2.983 | 2.931 |\n| Mamba | 129.2 | 3.238 | 3.231 | 371.5 | 2.902 | 2.868 |\n| LLaMA | 124.4 | 3.247 | 3.273 | 357.7 | 2.891 | 2.883 |\n| Longhorn | 128.6 | $\\mathbf{3 .",
    "longhorn-7": "2 2 5}$ | $\\mathbf{3 .",
    "longhorn-8": "1 9 2}$ | 369.8 | $\\mathbf{2 . 8 8 8}$ | $\\mathbf{2 . 8 5 9}$ |\n\nTable 3: Language modeling scaling law against LLaMA (Touvron et al., 2023), RetNet (Sun et al., 2023), RWKV (Peng et al., 2023), and Mamba (Gu \\& Dao, 2023).",
    "longhorn-9": "All models are trained on the OpenWebText dataset (Gokaslan \\& Cohen, 2019). Models vary from around 120M and 350M parameters, with context length 1024 or 4096. Observation: From the figure and table, we can see that Longhorn consistently outperforms baseline SSMs up to 350 M and 4096 context length. ### 5.3 LARGE-SCALE LANGUAGE MODELING\n\nFor the large-scale language modeling task, we followed the experiment setting in GLA (Yang et al., 2023), where a model with around 1.3B parameters is trained on the SlimPajama (Soboleva et al., 2023) dataset over 100B tokens with a batch size of 2 M tokens. We use the AdamW optimizer (Loshchilov \\& Hutter, 2017) with weight decay of 0.01 , and apply cosine learning rate decay with a peak learning rate of $3 e-4$ and the final learning rate of $3 e-5$. In addition, we use gradient clipping of 1.0. Due to the large size of the experiment, we only compare against LLaMA, Mamba and GLA models, all with a context size of 2048. ${ }^{3}$\n\nWe evaluate the trained models on a wide range of downstream tasks covering both common-sense reasoning and question-answering. In particular, we consider PIQA (Bisk et al., 2020), HellaSwag (Hella) (Zellers et al., 2019), WinoGrande (Wino) (Sakaguchi et al., 2021), ARC-easy (ARC-e) and ARC-challenge (ARC-c) (Clark et al., 2018), OpenBookQA (OBQA) (Mihaylov et al., 2018), Social Interaction QA (SIQA) (Sap et al., 2019), and Boolean questions (BoolQ) (Clark et al., 2019). We report the average perplexity across the above eight datasets throughout training in Figure 1 (left). Then we summarize the downstream evaluation results in Table 4. Observation: From Figure 1 (left), it is evident that Longhorn not only achieves a lower average perplexity but also improves sampling efficiency by $\\mathbf{1 . 8 x}$ compared to Mamba. In other words, Longhorn reaches the same average perplexity with nearly half the training data required by Mamba. From the Table 4, we can see that up to a 1.3B model, Longhorn remains strong among all baseline models and achieves slightly better result than Mamba, even though it has a bit fewer parameters. [^2]| Model | State Size | PIQA | Hella | Wino. | ARC-e | ARC-c | OBQA | SIQA | BoolQ | Avg. |\n| :--- | ---: | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | acc $\\uparrow$ | acc_norm $\\uparrow$ | acc $\\uparrow$ | acc $\\uparrow$ | acc_norm $\\uparrow$ | acc $\\uparrow$ | acc_norm $\\uparrow$ | acc $\\uparrow$ |  |\n| LLaMA | 1024 K | 55.08 | 55.36 | 71.73 | 59.26 | 32.19 | 43.35 | 45.16 | 62.13 | 53.03 |\n| GLA | 1024 K | 55.55 | 49.10 | 71.12 | 58.86 | 28.11 | 41.67 | 44.91 | 59.21 | 51.07 |\n| Mamba | 32 K | 54.21 | 53.61 | 71.67 | 61.05 | 30.15 | 43.94 | 44.18 | 59.22 | 52.25 |\n| Longhorn | 32 K | 55.78 | 52.30 | 71.00 | 60.63 | 29.53 | 43.55 | 44.68 | 61.29 | $\\mathbf{5 2 . 3 5}$ |\n\nTable 4: Language modeling results against LLaMA (Touvron et al., 2023), RetNet (Sun et al., 2023), and Mamba (Gu \\& Dao, 2023). All models are trained on the same subset of the SlimPajama dataset with the Mistral tokenizer. The 340M/1.3B models are trained for 15B/100B tokens respectively. State Size is the effective state size of an SSM per layer. For instance, GLA's state size $(1024 \\mathrm{~K})$ is computed by $m d / h$, where the key and value dimensions are $m=1024$ and $d=2048$, and there are 4 heads $h=4$.",
    "longhorn-10": "The individual task performance is via zero-shot. The last column shows the average value over the results on all benchmarks. ### 5.4 Ablation On Length EXtrapolation\n\nMoreover, we evaluate how Longhorn extrapolates to a context length longer than 2048 (training context length) at inference time. In particular, we pick a disjoint validation set from SlimPajama dataset, rearrange it into batches of sequences of length $T \\in\\{2048,4096,8192,16384,32768\\}$, and then evaluate the pretrained model's perplexity on those sequences. The results are summarized in Figure 1 (right). Observation: From the figure, we observe that Longhorn successfully extrapolates to contexts up to 16x longer than those used during training, this contrasts with DeltaNet (Yang et al., 2024), which highlights a limitation in that the model cannot extrapolate to longer contexts. Moreover, it is expected that LLaMA, which is a Transformer-based model, fails to extrapolates beyond its training context length. ## 6 CONCLUSION AND FUTURE WORK\n\nIn this work, we introduce a novel approach to designing deep state-space models (SSMs) by conceptualizing the recurrence update as solving an online objective. We propose a straightforward online regression objective, adopting its implicit closed-form update to define our model, which we refer to as Longhorn. Notably, Longhorn is highly parallelizable and achieves state-of-the-art performance in both synthetic sequence modeling and language modeling tasks. For future research, an intriguing avenue would be exploring other online learning objectives that can be efficiently implemented on modern hardware. Additionally, while the current implementation of Longhorn closely aligns with Mamba, recent studies (Ren et al., 2024) suggest that incorporating sliding-window attention with Mamba improves performance. We anticipate similar benefits for Longhorn. ## REFERENCES\n\nSimran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. Zoology: Measuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927, 2023. Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher R\u00e9. Simple linear attention language models balance the recall-throughput tradeoff. arXiv preprint arXiv:2402.18668, 2024. Maximilian Beck, Korbinian Poppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. 2024. URL https://api.semanticscholar.org/ CorpusID:269614336. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432-7439, 2020. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.",
    "longhorn-11": "arXiv preprint arXiv:2009.14794, 2020. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai 2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R'e. Flashattention: Fast and memory-efficient exact attention with io-awareness.",
    "longhorn-12": "ArXiv, abs/2205.14135, 2022. URL https://api.semanticscholar.org/CorpusID:249151871. Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933-941. PMLR, 2017. Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. DeepSeek-AI and Damai Dai. Deepseek-v2: A strong, economical, and efficient mixture-ofexperts language model.",
    "longhorn-13": "2024. URL https: / / api.semanticscholar.org/corpusID: 269613809 . Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models.",
    "longhorn-14": "arXiv preprint arXiv:2212.14052, 2022. Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces.",
    "longhorn-15": "Advances in Neural Information Processing Systems, 35:22982-22994, 2022. Mark Harris, Shubhabrata Sengupta, and John D Owens. Parallel prefix sum (scan) with cuda.",
    "longhorn-16": "GPU gems, 3(39):851-876, 2007. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Brian Kulis and Peter L Bartlett. Implicit online learning. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 575-582, 2010. Victor Kuperman and Julie A. Van Dyke. Individual differences in visual comprehension of morphological complexity. Cognitive Science, 33, 2011. URL https: / /api. semanticscholar. org/CorpusID:5555496. Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling?",
    "longhorn-17": "arXiv preprint arXiv:2210.09298, 2022. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.",
    "longhorn-18": "arXiv preprint arXiv:1711.05101, 2017. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. Martial Mermillod, Aur\u00e9lia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects, 2013. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pp. 26670-26698. PMLR, 2023. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys\u0142aw Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence.",
    "longhorn-19": "arXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.",
    "longhorn-20": "arXiv preprint arXiv:2406.07522, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.",
    "longhorn-21": "Communications of the ACM, 64(9):99-106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions.",
    "longhorn-22": "arXiv preprint arXiv:1904.09728, 2019. Imanol Schlag and J\u00fcrgen Schmidhuber. Gated fast weights for on-the-fly neural program generation. In NIPS Metalearning Workshop, 2017. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355-9366. PMLR, 2021. J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131-139, 1992. J\u00fcrgen Schmidhuber. Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets. In ICANN'93: Proceedings of the International Conference on Artificial Neural Networks Amsterdam, The Netherlands 13-16 September 1993 3, pp. 460-463. Springer, 1993. Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states. 2024. URL https: / api. semanticscholar. org/CorpusID: 271039606. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length.",
    "longhorn-23": "arXiv preprint arXiv:2406.06484, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [^0]:    ${ }^{1}$ Code is available at https://github.com/Cranial-XIX/longhorn.git. [^1]:    ${ }^{2}$ We adapt the code from the nanoGPT repo, a minimal reproduction of the GPT-2 model. See https: //github.com/karpathy/nanoGPT/tree/master\n\n[^2]:    ${ }^{3}$ Our result is in general better than those reported in the GLA paper, this might due to the fact that we are using a different 100B subset of SlimPajama since the original split of the SlimPajama is not provided. "
}