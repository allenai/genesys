{
    "s4d-0": "On the Parameterization and Initialization of Diagonal State Space Models\n\nAlbert Gu Ankit Gupta Karan Goel Christopher R\u00e9\n\nAbstract\n\nState space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers.",
    "s4d-1": "The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4\u2019s matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4\u2019s matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85% on the Long Range Arena benchmark. 1 Introduction\n\nA core class of models in modern deep learning are sequence models, which are parameterized mappings operating on arbitrary sequences of inputs. Recent approaches based on state space models (SSMs) have outperformed traditional deep sequence models such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), and Transformers, in both computational efficiency and modeling ability. In particular, the S4 model displayed strong results on a range of sequence modeling tasks, especially on long sequences [9]. Its ability to model long-range dependencies arises from being defined with a particular state matrix called the \u201cHiPPO matrix\u201d [6], which allows S4 to be viewed as a convolutional model that decomposes an input onto an orthogonal system of smooth basis functions[10]. However, beyond its theoretical interpretation, actually computing S4 as a deep learning model requires a sophisticated algorithm with many linear algebraic techniques that are difficult to understand and implement. These techniques were necessitated by parameterizing its state matrix as a diagonal plus low-rank (DPLR) matrix, which is necessary to capture HiPPO matrices. A natural question is whether simplifications of this parameterization and algorithm are possible. In particular, removing the low-rank term would result in a diagonal state space model (diagonal SSM) that is dramatically simpler to implement and understand. Although it is known that almost all SSMs have an equivalent diagonal form\u2014and therefore (complex) diagonal SSMs are fully expressive algebraically\u2014they may not represent all SSMs numerically, and finding a good initialization is critical. Gu et al. [9] showed that it is difficult to find a performant diagonal SSM, and that many alternative parameterizations of the state matrix \u2013 including by random diagonal matrices \u2013 are much less effective empirically, which motivated the necessity of the more complicated HiPPO matrix. However, recently Gupta [11] made the empirical observation that a variant of S4 using a particular diagonal matrix is nearly as effective as the original S4 method. This matrix is based on the original HiPPO matrix and is defined by simply chopping off the low-rank term in the DPLR representation. The discovery of performant diagonal state matrices opens up new possibilities for simplifying deep state space models, and consolidating models such as S4 and DSS to understand and improve them. First, the strongest version of DSS computes the SSM with a complex-valued softmax that complicates the algorithm, and is actually less efficient than S4. Additionally, DSS and S4 differ in several auxiliary aspects of parameterizing SSMs that can conflate performance effects, making it more difficult to isolate the core effects of diagonal versus DPLR state matrices. Most importantly, DSS relies on initializing the state matrix to a particular approximation of S4\u2019s HiPPO matrix. While S4\u2019s matrix has a mathematical interpretation for addressing long-range dependencies, the efficacy of the diagonal approximation to it remains theoretically unexplained. In this work, we seek to systematically understand how to train diagonal SSMs. We introduce the S4D method, a diagonal SSM which combines the best of S4\u2019s computation and parameterization and DSS\u2019s initialization, resulting in a method that is extremely simple, theoretically princpled, and empirically effective. \u2022\n\nFirst, we describe S4D, a simple method outlined by S4 for computing diagonal instead of DPLR matrices, which is based on Vandermonde matrix multiplication and is even simpler and more efficient than the DSS. Outside of the core state matrix, we categorize different representations of the other components of SSMs, introducing flexible design choices that capture both S4 and DSS and allow different SSM parameterizations to be systematically compared (Section 3). \u2022\n\nWe provide a new mathematical analysis of DSS\u2019s initialization, showing that the diagonal approximation of the original HiPPO matrix surprisingly produces the same dynamics as S4 when the state size goes to infinity. We propose even simpler variants of diagonal SSMs using different initializations of the state matrix (Section 4). \u2022\n\nWe perform a controlled study of these various design choices across many domains, tasks, and sequence lengths, and additionally compare diagonal (S4D) versus DPLR (S4) variants. Our best S4D methods are competitive with S4 on almost all settings, with near state-of-the-art results on image, audio, and medical time series benchmarks, and achieving 85% on the Long Range Arena benchmark (Section 5). 2 Background\n\nContinuous State Spaces Models\n\nS4 investigated state space models (1) that are parameterized maps on signals . These SSMs are linear time-invariant systems that can be represented either as a linear ODE (equation (1)) or convolution (equation (2)). x \u2032 \u200b ( t ) superscript \ud835\udc65 \u2032 \ud835\udc61 \\displaystyle x^{\\prime}(t) = \ud835\udc68 \u200b x \u200b ( t ) + \ud835\udc69 \u200b u \u200b ( t ) absent \ud835\udc68 \ud835\udc65 \ud835\udc61 \ud835\udc69 \ud835\udc62 \ud835\udc61 \\displaystyle=\\bm{A}x(t)+\\bm{B}u(t) (1) y \u200b ( t ) \ud835\udc66 \ud835\udc61 \\displaystyle y(t) = \ud835\udc6a \u200b x \u200b ( t ) absent \ud835\udc6a \ud835\udc65 \ud835\udc61 \\displaystyle=\\bm{C}x(t)\n\nK \u200b ( t ) \ud835\udc3e \ud835\udc61 \\displaystyle K(t) = \ud835\udc6a \u200b e t \u200b \ud835\udc68 \u200b \ud835\udc69 absent \ud835\udc6a superscript \ud835\udc52 \ud835\udc61 \ud835\udc68 \ud835\udc69 \\displaystyle=\\bm{C}e^{t\\bm{A}}\\bm{B} (2) y \u200b ( t ) \ud835\udc66 \ud835\udc61 \\displaystyle y(t) = ( K \u2217 u ) \u200b ( t ) absent \u2217 \ud835\udc3e \ud835\udc62 \ud835\udc61 \\displaystyle=(K\\ast u)(t)\n\nHere the parameters are the state matrix and other matrices . In the case of diagonal SSMs, is diagonal and we will overload notation so that denotes the entries of the parameters. An intuitive way to view the convolution kernel (2) is to interpret it as a linear combination (controlled by ) of basis kernels (controlled by )\n\nK \u200b ( t ) = \u2211 n = 0 N \u2212 1 \ud835\udc6a n \u200b K n \u200b ( t ) K n \u200b ( t ) := \ud835\udc86 n \u22a4 \u200b e t \u200b \ud835\udc68 \u200b \ud835\udc69 formulae-sequence \ud835\udc3e \ud835\udc61 superscript subscript \ud835\udc5b 0 \ud835\udc41 1 subscript \ud835\udc6a \ud835\udc5b subscript \ud835\udc3e \ud835\udc5b \ud835\udc61 assign subscript \ud835\udc3e \ud835\udc5b \ud835\udc61 superscript subscript \ud835\udc86 \ud835\udc5b top superscript \ud835\udc52 \ud835\udc61 \ud835\udc68 \ud835\udc69 \\displaystyle K(t)=\\sum_{n=0}^{N-1}\\bm{C}_{n}K_{n}(t)\\qquad\\qquad K_{n}(t):=\\bm{e}_{n}^{\\top}e^{t\\bm{A}}\\bm{B} (3)\n\nWe denote this basis as if necessary to disambiguate; note that it is a vector of functions. In the case of diagonal SSMs, each function is just . S4: Structured State Spaces\n\nAs a deep learning model, SSMs have many elegant properties with concrete empirical and computational benefits [8]. For example, the convolutional form (2) can be converted into a temporal recurrence that is substantially faster for autoregressive applications [5]. However, making SSMs effective required overcoming two key challenges: choosing appropriate values for the matrices, and computing the kernel (2) efficiently. First, Gu et al. [8] showed that naive instantiations of the SSM do not perform well, and instead relied on a particular (real-valued) matrix called the HiPPO-LegS matrix (4).111HiPPO also specifies formulas for , but the state matrix is more important. There are many other HiPPO instantiations besides LegS, but HiPPO-LegS is the main one that S4 uses and the term \u201cHiPPO matrix\u201d without the suffix refers to this one. These matrices were derived so that the basis kernels have closed-form formulas , where are normalized Legendre polynomials. Consequently, the SSM has a mathematical interpretation of decomposing the input signal onto a set of infinitely-long basis functions that are orthogonal respect to an exponentially-decaying measure, giving it long-range modeling abilities [10]. Second, S4 introduced a particular parameterization that decomposed this matrix into the sum of a normal and rank-1 matrix (5), which can be unitarily conjugated into a (complex) diagonal plus rank-1 matrix. Leveraging this structured form, they then introduced a sophisticated algorithm for efficiently computing the convolution kernel (2) for state matrices that are diagonal plus low-rank (DPLR). \ud835\udc68 n \u200b k = \u2212 { ( 2 \u200b n + 1 ) 1 2 \u200b ( 2 \u200b k + 1 ) 1 2 n > k n + 1 n = k 0 n < k \ud835\udc69 n = ( 2 \u200b n + 1 ) 1 2 \u200b \ud835\udc77 n = ( n + 1 / 2 ) 1 2 ( HiPPO-LegS matrix used in S4 ) subscript \ud835\udc68 \ud835\udc5b \ud835\udc58 absent cases superscript 2 \ud835\udc5b 1 1 2 superscript 2 \ud835\udc58 1 1 2 \ud835\udc5b \ud835\udc58 \ud835\udc5b 1 \ud835\udc5b \ud835\udc58 0 \ud835\udc5b \ud835\udc58 subscript \ud835\udc69 \ud835\udc5b absent superscript 2 \ud835\udc5b 1 1 2 subscript \ud835\udc77 \ud835\udc5b superscript \ud835\udc5b 1 2 1 2 missing-subexpression HiPPO-LegS matrix used in S4 \\qquad\\begin{aligned} \\bm{A}_{nk}&=-\\begin{cases}(2n+1)^{\\frac{1}{2}}(2k+1)^{\\frac{1}{2}}&n>k\\\\\nn+1&n=k\\\\\n0&n<k\\end{cases}\\\\\n\\bm{B}_{n}&=(2n+1)^{\\frac{1}{2}}\\quad\\bm{P}_{n}=(n+1/2)^{\\frac{1}{2}}\\\\\n&(\\textbf{HiPPO-LegS matrix used in S4})\\end{aligned} (4)\n\n\ud835\udc68 n \u200b k ( N ) subscript superscript \ud835\udc68 \ud835\udc41 \ud835\udc5b \ud835\udc58 \\displaystyle\\bm{A}^{(N)}_{nk} = \u2212 { ( n + 1 2 ) 1 / 2 \u200b ( k + 1 2 ) 1 / 2 n > k 1 2 n = k ( n + 1 2 ) 1 / 2 \u200b ( k + 1 2 ) 1 / 2 n < k absent cases superscript \ud835\udc5b 1 2 1 2 superscript \ud835\udc58 1 2 1 2 \ud835\udc5b \ud835\udc58 1 2 \ud835\udc5b \ud835\udc58 superscript \ud835\udc5b 1 2 1 2 superscript \ud835\udc58 1 2 1 2 \ud835\udc5b \ud835\udc58 \\displaystyle=-\\begin{cases}(n+\\frac{1}{2})^{1/2}(k+\\frac{1}{2})^{1/2}&n>k\\\\\n\\frac{1}{2}&n=k\\\\\n(n+\\frac{1}{2})^{1/2}(k+\\frac{1}{2})^{1/2}&n<k\\end{cases} (5) \ud835\udc68 \ud835\udc68 \\displaystyle\\bm{A} = \ud835\udc68 ( N ) \u2212 \ud835\udc77 \u200b \ud835\udc77 \u22a4 , \ud835\udc68 ( D ) := eig ( \ud835\udc68 ( N ) ) formulae-sequence absent superscript \ud835\udc68 \ud835\udc41 \ud835\udc77 superscript \ud835\udc77 top assign superscript \ud835\udc68 \ud835\udc37 eig superscript \ud835\udc68 \ud835\udc41 \\displaystyle=\\bm{A}^{(N)}-\\bm{P}\\bm{P}^{\\top},\\qquad\\bm{A}^{(D)}:=\\operatorname*{eig}(\\bm{A}^{(N)}) ( Normal / diagonal plus low-rank form ) Normal / diagonal plus low-rank form \\displaystyle(\\textbf{Normal / diagonal plus low-rank form})\n\nDSS: Diagonal State Spaces\n\nS4 was originally motivated by searching for a diagonal state matrix, which would be even more structured and result in very simple computation of the SSM. However, the HiPPO-LegS matrix cannot be stably transformed into diagonal form [9, Lemma 3.2], and they were unable to find any diagonal matrices that performed well, resulting in the DPLR formulation. Gupta [11] made the surprising empirical observation that simply removing the low-rank portion of the DPLR form of the HiPPO-LegS matrix results in a diagonal matrix that performs comparably to the original S4 method. More precisely, their initialization is the diagonal matrix , or the diagonalization of in (5). They termed the skew-HiPPO matrix, which we will also call the normal-HiPPO matrix. To be more specific and disambiguate these variants, we may also call the HiPPO-LegS-N or HiPPO-N matrix and the HiPPO-LegS-D or HiPPO-D matrix. In addition to this initialization, they proposed a method for computing a diagonal SSM kernel. Beyond these two core differences, several other aspects of their parameterization differ from S4\u2019s. In Sections 3 and 4, we systematically study the components of DSS: we categorize different ways to parameterize and compute the diagonal state space, and explain the theoretical interpretion of this particular diagonal matrix. Because there are several different concrete matrices with different naming conventions, this table summarizes these special matrices and ways to refer to them. Matrix Full Name Alternate Names \ud835\udc68 \ud835\udc68 \\bm{A} HiPPO-LegS HiPPO matrix, LegS matrix \ud835\udc68 ( N ) superscript \ud835\udc68 \ud835\udc41 \\bm{A}^{(N)} HiPPO-LegS-N HiPPO-N, skew-HiPPO, normal-HiPPO \ud835\udc68 ( D ) superscript \ud835\udc68 \ud835\udc37 \\bm{A}^{(D)} HiPPO-LegS-D HiPPO-D, diagonal-HiPPO\n\n3 Parameterizing Diagonal State Spaces\n\nWe describe various choices for the computation and parameterization of diagonal state spaces.",
    "s4d-2": "Our categorization of these choices leads to simple variants of the core method. Both DSS and our proposed S4D can be described using a combination of these factors (Section 3.4). 3.1 Discretization\n\nThe true continuous-time SSM can be represented as a continuous convolution\n\nIn discrete time, we view an input sequence as uniformly-spaced samples from an underlying function and must approximate this integral.",
    "s4d-3": "Standard methods for doing so that preserve the convolutional structure of the model exist. The first step is to discretize the parameters. Two simple choices that have been used in prior work include\n\n( Bilinear ) \ud835\udc68 \u00af = ( \ud835\udc70 \u2212 \u0394 / 2 \u200b \ud835\udc68 ) \u2212 1 \u200b ( \ud835\udc70 + \u0394 / 2 \u200b \ud835\udc68 ) ( ZOH ) \ud835\udc68 \u00af = exp \u2061 ( \u0394 \u200b \ud835\udc68 ) \ud835\udc69 \u00af = ( \ud835\udc70 \u2212 \u0394 / 2 \u200b \ud835\udc68 ) \u2212 1 \u22c5 \u0394 \u200b \ud835\udc69 \ud835\udc69 \u00af = ( \u0394 \u200b \ud835\udc68 ) \u2212 1 \u200b ( exp \u2061 ( \u0394 \u22c5 \ud835\udc68 ) \u2212 \ud835\udc70 ) \u22c5 \u0394 \u200b \ud835\udc69 . Bilinear bold-\u00af \ud835\udc68 superscript \ud835\udc70 \u0394 2 \ud835\udc68 1 \ud835\udc70 \u0394 2 \ud835\udc68 ZOH bold-\u00af \ud835\udc68 \u0394 \ud835\udc68 missing-subexpression bold-\u00af \ud835\udc69 \u22c5 superscript \ud835\udc70 \u0394 2 \ud835\udc68 1 \u0394 \ud835\udc69 missing-subexpression bold-\u00af \ud835\udc69 \u22c5 superscript \u0394 \ud835\udc68 1 \u22c5 \u0394 \ud835\udc68 \ud835\udc70 \u0394 \ud835\udc69 \\displaystyle\\begin{aligned} (\\textbf{Bilinear})\\>\\>&\\bm{\\overline{A}}=(\\bm{I}-\\Delta/2\\bm{A})^{-1}(\\bm{I}+\\Delta/2\\bm{A})&\\qquad(\\textbf{ZOH})\\>\\>&\\bm{\\overline{A}}=\\exp(\\Delta\\bm{A})\\\\\n&\\bm{\\overline{B}}=(\\bm{I}-\\Delta/2\\bm{A})^{-1}\\cdot\\Delta\\bm{B}&\\qquad&\\bm{\\overline{B}}=(\\Delta\\bm{A})^{-1}(\\exp(\\Delta\\cdot\\bm{A})-\\bm{I})\\cdot\\Delta\\bm{B}.\\end{aligned}\n\nWith these methods, the discrete-time SSM output is just\n\ny = u \u2217 \ud835\udc72 \u00af where \u200b \ud835\udc72 \u00af = ( \ud835\udc6a \u200b \ud835\udc69 \u00af , \ud835\udc6a \u200b \ud835\udc68 \u00af \u200b \ud835\udc69 \u00af , \u2026 , \ud835\udc6a \u200b \ud835\udc68 \u00af L \u2212 1 \u200b \ud835\udc69 \u00af ) . missing-subexpression \ud835\udc66 \u2217 \ud835\udc62 bold-\u00af \ud835\udc72 where bold-\u00af \ud835\udc72 \ud835\udc6a bold-\u00af \ud835\udc69 \ud835\udc6a bold-\u00af \ud835\udc68 bold-\u00af \ud835\udc69 \u2026 \ud835\udc6a superscript bold-\u00af \ud835\udc68 \ud835\udc3f 1 bold-\u00af \ud835\udc69 \\begin{aligned} &y=u\\ast\\bm{\\overline{K}}&\\qquad\\text{where }\\bm{\\overline{K}}=(\\bm{C}\\bm{\\overline{B}},\\bm{C}\\bm{\\overline{A}}\\bm{\\overline{B}},\\dots,\\bm{C}\\bm{\\overline{A}}^{L-1}\\bm{\\overline{B}})\\end{aligned}. (6)\n\nThese integration rules have both been used in prior works (e.g. LMU and DSS use ZOH [26, 11] while S4 and its predecessors use bilinear [6, 8, 9]). In Section 5, we show that there is little empirical difference between them. However, we note that there is a curious phenomenon where the bilinear transform actually perfectly smooths out the kernel used in DSS to match the S4 kernel (Section 4 Fig. 3(d)). We additionally note that numerical integration is a rich and well-studied topic and more stable methods of approximating the convolutional integral may exist. For example, it is well-known that simple rules like the Trapezoid rule [18] can dramatically reduce numerical integration error when the function has bounded second derivative. 3.2 Convolution Kernel\n\nThe main computational difficulty of the original S4 model is computing the convolution kernel . This is extremely slow for general state matrices , and S4 introduced a complicated algorithm for DPLR state matrices. When is diagonal, the computation is nearly trivial. By (6),\n\n\ud835\udc72 \u00af \u2113 = \u2211 n = 0 N \u2212 1 \ud835\udc6a n \u200b \ud835\udc68 \u00af n \u2113 \u200b \ud835\udc69 \u00af n \u27f9 \ud835\udc72 \u00af = ( \ud835\udc69 \u00af \u22a4 \u2218 \ud835\udc6a ) \u22c5 \ud835\udcb1 L \u200b ( \ud835\udc68 \u00af ) where \u200b \ud835\udcb1 L \u200b ( \ud835\udc68 \u00af ) n , \u2113 = \ud835\udc68 \u00af n \u2113 formulae-sequence subscript bold-\u00af \ud835\udc72 \u2113 superscript subscript \ud835\udc5b 0 \ud835\udc41 1 subscript \ud835\udc6a \ud835\udc5b superscript subscript bold-\u00af \ud835\udc68 \ud835\udc5b \u2113 subscript bold-\u00af \ud835\udc69 \ud835\udc5b bold-\u00af \ud835\udc72 \u22c5 superscript bold-\u00af \ud835\udc69 top \ud835\udc6a subscript \ud835\udcb1 \ud835\udc3f bold-\u00af \ud835\udc68 where subscript \ud835\udcb1 \ud835\udc3f subscript bold-\u00af \ud835\udc68 \ud835\udc5b \u2113 superscript subscript bold-\u00af \ud835\udc68 \ud835\udc5b \u2113 \\bm{\\overline{K}}_{\\ell}=\\sum_{n=0}^{N-1}\\bm{C}_{n}\\bm{\\overline{A}}_{n}^{\\ell}\\bm{\\overline{B}}_{n}\\implies\\bm{\\overline{K}}=(\\bm{\\overline{B}}^{\\top}\\circ\\bm{C})\\cdot\\mathcal{V}_{L}(\\bm{\\overline{A}})\\qquad\\qquad\\text{where }\\mathcal{V}_{L}(\\bm{\\overline{A}})_{n,\\ell}=\\bm{\\overline{A}}_{n}^{\\ell} (7)\n\nwhere is Hadamard product, is matrix multiplication, and is known as a Vandermonde matrix. Unpacking this a little more, we can write as the following Vandermonde matrix-vector multiplication. \ud835\udc72 \u00af = [ \ud835\udc69 \u00af 0 \u200b \ud835\udc6a 0 \u2026 \ud835\udc69 \u00af N \u2212 1 \u200b \ud835\udc6a N \u2212 1 ] \u200b [ 1 \ud835\udc68 \u00af 0 \ud835\udc68 \u00af 0 2 \u2026 \ud835\udc68 \u00af 0 L \u2212 1 1 \ud835\udc68 \u00af 1 \ud835\udc68 \u00af 1 2 \u2026 \ud835\udc68 \u00af 1 L \u2212 1 \u22ee \u22ee \u22ee \u22f1 \u22ee 1 \ud835\udc68 \u00af N \u2212 1 \ud835\udc68 \u00af N \u2212 1 2 \u2026 \ud835\udc68 \u00af N \u2212 1 L \u2212 1 ] bold-\u00af \ud835\udc72 matrix subscript bold-\u00af \ud835\udc69 0 subscript \ud835\udc6a 0 \u2026 subscript bold-\u00af \ud835\udc69 \ud835\udc41 1 subscript \ud835\udc6a \ud835\udc41 1 matrix 1 subscript bold-\u00af \ud835\udc68 0 superscript subscript bold-\u00af \ud835\udc68 0 2 \u2026 superscript subscript bold-\u00af \ud835\udc68 0 \ud835\udc3f 1 1 subscript bold-\u00af \ud835\udc68 1 superscript subscript bold-\u00af \ud835\udc68 1 2 \u2026 superscript subscript bold-\u00af \ud835\udc68 1 \ud835\udc3f 1 \u22ee \u22ee \u22ee \u22f1 \u22ee 1 subscript bold-\u00af \ud835\udc68 \ud835\udc41 1 superscript subscript bold-\u00af \ud835\udc68 \ud835\udc41 1 2 \u2026 superscript subscript bold-\u00af \ud835\udc68 \ud835\udc41 1 \ud835\udc3f 1 \\displaystyle\\bm{\\overline{K}}=\\begin{bmatrix}\\bm{\\overline{B}}_{0}\\bm{C}_{0}&\\dots&\\bm{\\overline{B}}_{N-1}\\bm{C}_{N-1}\\end{bmatrix}\\begin{bmatrix}1&\\bm{\\overline{A}}_{0}&\\bm{\\overline{A}}_{0}^{2}&\\dots&\\bm{\\overline{A}}_{0}^{L-1}\\\\\n1&\\bm{\\overline{A}}_{1}&\\bm{\\overline{A}}_{1}^{2}&\\dots&\\bm{\\overline{A}}_{1}^{L-1}\\\\\n\\vdots&\\vdots&\\vdots&\\ddots&\\vdots\\\\\n1&\\bm{\\overline{A}}_{N-1}&\\bm{\\overline{A}}_{N-1}^{2}&\\dots&\\bm{\\overline{A}}_{N-1}^{L-1}\\\\\n\\end{bmatrix}\n\nTime and Space Complexity\n\nThe naive way to compute (7) is by materializing the Vandermonde matrix and performing a matrix multiplication, which requires time and space. However, Vandermonde matrices are well-studied and theoretically the multiplication can be computed in operations and space. In fact, Vandermonde matrices are closely related to Cauchy matrices, which are the computational core of S4\u2019s DPLR algorithm, and have identical complexity [17]. Proposition 1. The time and space complexity of computing the kernel of diagonal SSMs is equal to that of computing DPLR SSMs. We note that on modern parallelizable hardware such as GPUs, a simple fast algorithm is to compute (7) with naive summation (using operations), but without materializing the Vandermonde matrix (using space). Just as with S4, this may require implementing a custom kernel in some modern deep learning frameworks such as PyTorch to achieve the space savings. 3.3 Parameterization\n\nThe next question is how to represent the parameters . Parameterization of . Note that the kernel blows up to as if has any eigenvalues with positive real part. Goel et al. [5] found that this is a serious constraint that affects the stability of the model, especially when using the SSM as an autoregressive generative model. They propose to force the real part of to be negative, also known as the left-half plane condition in classical controls, by parameterizing the real part inside an exponential function\n\nWe note that instead of , any activation function can be used as long as its range is bounded on one side, such as ReLU, softplus, etc. The original DSS does not constrain the real part of , which is sufficient for simple tasks involving fixed-length sequences, but could become unstable in other settings.",
    "s4d-4": "Parameterization of . Another choice in the parameterization is how to represent and . Note that the computation of the final discrete convolution kernel depends only on the elementwise product (equation (7)). Therefore DSS chose to parameterize this product directly, which they call , instead of and individually. However, we observe that this is equivalent to keeping independent and , and simply freezing while training . Therefore, just as S4 has separate parameters , , and and uses a fixed initialization for and , S4D also proposes separate , and and uses fixed initializations for (discussed in Section 4) and (set to ). Then the difference between S4D and DSS is simply that DSS does not train . In our ablations, we show that training gives a minor but consistent improvement in performance. As described in [10], S4 initializes randomly with standard deviation (in contrast to standard deep learning initializations, which scale with the dimension e.g. ), which is variance-preserving for S4\u2019s as a consequence of the HiPPO theory. Because it turns out that the diagonal approximation to HiPPO has similar theoretical properties, we retain this initialization in the diagonal case. Conjugate Symmetry. Finally, we make note of a minor parameterization detail originally used in S4. Note that we ultimately care about sequence transformations over real numbers. For example, HiPPO defines real matrices, and the base definition of S4 is a real SSM that is a map on sequences of real numbers. In this case, note that the state at any time is a vector , and similarly and would consist of real parameters. However, if using complex numbers, this effectively doubles the state dimension and the number of parameters in . Furthermore, when using a complex SSM, the output of the SSM is not guaranteed to be real even if the input is real, and similarly the convolution kernel (7) will in general be complex. To resolve this discrepency, note that when diagonalizing a real SSM into a complex SSM (see Proposition 2), the resulting parameters always occur in conjugate pairs. Therefore we can throw out half of the parameters. In other words, to parameterize a real SSM of state size , we can instead parameterize a complex SSM of state size , and implicitly add back the conjugate pairs of the parameters. This ensures that the total state size and parameter count is actually the equivalent of real numbers, and also guarantees that the output of the kernel is real. The implementation of this is very simple; the sum in (7) will implicitly include the conjugate pairs of and therefore resolve to twice the real part of the original sum. 3.4 S4D: the Diagonal Version of S4\n\nA key component of our exposition is disentangling the various choices possible in representing and computing state space models. With this categorization, different choices can be mixed and matched to define variants of the core method. Table 1 compares S4, DSS, and S4D, which have a core structure and kernel computation, but have various choices of other aspects of the parameterization. Comparison to S4 and DSS. We will define the base version of S4D to match the parameterization of S4 (i.e. bilinear discretization, parameterized with , trainable , and HiPPO-D initialization), but many other variants are possible. Note that unlike DSS, the output of S4D would be exactly the same as masking out the low-rank component of S4\u2019s DPLR representation. Thus comparing S4D vs. S4 is a comparison of diagonal vs. DPLR representations of while controlling all other factors. In our empirical study in Section 5, we systematically ablate the effects of each of these components. We elaborate more on the comparisons between S4, DSS, and S4D below. Kernel computation. The original S4 work briefly considered the diagonal case as motivation [9, Section 3.1], and explicitly mentioned the connection to Vandermonde products and the computational complexity of diagonal SSMs. However, their focus was the more complex DPLR representation because it is difficult to find a performant diagonal state matrix. Compared to S4, we fleshed out details of the Vandermonde connection and its computational complexity, which matches that of S4. On the other hand, DSS empirically found an effective diagonal state matrix, but introduced a more complicated method based on a complex softmax for computing it. Compared to S4D, this softmax essentially normalizes by the row-sums of the Vandermonde matrix, so we may sometimes refer to this distinction as \u201csoftmax normalization\u201d. This makes the kernel more complicated than necessary, and has a few concrete drawbacks. First, the row-normalization effectively makes the model dependent on a particular sequence length , and special logic is required to handle different sequence lengths. Second, it does not expose the optimal computational complexity of the method, and the original version of DSS in fact uses more memory in the kernel construction than S4(D).222An early version of DSS claimed that it did not require a custom kernel while S4 does, but this is because of its extra memory usage. The PyTorch implementation of S4 has an optional custom CUDA kernel primarily to save this factor of in space. Discretization. S4D disentangles the discretization method from the kernel computation (equation (7)), so that any discretization can be used, whereas previous methods required a specific discretization. For example, DSS requires the zero-order hold (ZOH) discretization because the term in the ZOH formula lends itself to be computed with a softmax. On the other hand, when is not diagonal, ZOH involves a matrix exponential which can be slower to compute, so S4 uses the bilinear discretization which can be computed efficiently for DPLR matrices. Eigenvalue constraint. All methods can enforce any constraint on the eigenvalues of . While DSS found that letting them be unconstrained has slightly better performance, our experiments find that the difference is negligible and we recommend contraining negative real part of as is standard practice in control systems.",
    "s4d-5": "This ensures stability even in unbounded autoregressive settings. The full model. The entire S4D method is very straightforward to implement, requiring just a few lines of code each for the parameterization and initialization, kernel computation, and full forward pass (Fig. 2). This minimal model maps an input sequence of length to an output of the same length; given multiple input channels, independent S4D layers are broadcast over them. Other details such as the initialization of and other components of the overall neural network architecture are the same as in S4 and DSS. Finally, note that different combinations of parameterization choices can lead to slightly different implementations of the kernel. Fig. 1 illustrates the S4D kernel with ZOH discretization which can be simplified even further to just 2 lines of code. 4 Initialization of Diagonal State Matrices\n\nThe critical question remains: which diagonal state matrices are actually effective? We comment on the limitations of diagonal SSMs, and then provide three instantiations of S4D that perform well empirically. Expressivity and Limitations of Diagonal SSMs. We first present a simplified view on the expressivity of diagonal SSMs mentioned by [11]. First, it is well-known that almost all matrices diagonalize over the complex plane. Therefore it is critical to use complex-valued matrices in order to use diagonal SSMs. Proposition 2. The set of diagonalizable matrices is dense in , and has full measure (i.e. its complement has measure ). It is also well known that the state space is exactly equivalent to (i.e. expresses the same map ) the state space , known in the SSM literature as a state space transformation. Therefore Proposition 2 says that (almost) all SSMs are equivalent to a diagonal SSM. However, we emphasize that Proposition 2 is about expressivity which does not guarantee strong performance of a trained model after optimization. For example, Gu et al. [9] and Gupta [11] show that parameterizing as a dense real matrix or diagonal complex matrix, which are both fully expressive classes, performs poorly if randomly initialized. Second, Proposition 2 does not take into account numerical representations of data, which was the original reason S4 required a low-rank correction term instead of a pure diagonalization [9, Lemma 3.2]. In Section 5.2, we also show that two different initializations with the same spectrum (i.e., are equivalent to the same diagonal ) can have very different performance. S4D-LegS. The HiPPO-LegS matrix has DPLR representation , and Gupta [11] showed that simply approximating it with works quite well (5). Our first result is providing a clean mathematical interpretation of this method. Theorem 3 shows a surprising fact that does not hold in general for DPLR matrices (Section A.1), and arises out of the special structure of this particular matrix. Theorem 3. Let and be the HiPPO-LegS matrices, and be its basis. As the state size , the SSM basis limits to (Fig.",
    "s4d-6": "3). Note that is then unitarily equivalent to , which preserves the stability and timescale [10] of the system. We define S4D-LegS to be the S4D method for this choice of diagonal . Theorem 3 explains the empirical results in [11] whereby this system performed quite close to S4, but was usually slightly worse. This is because DSS is a variant of S4D-LegS, which by Theorem 3 is a noisy approximation to S4-LegS. Fig. 3 illustrates this result, and also shows a curious phenomenon involving different discretization rules that is open for future work. S4D-Inv. To further simplify S4D-LegS, we analyze the structure of in more detail. The real part is easy to understand, which follows from the analysis in [9]:\n\nProposition 4. Let the imaginary part be sorted, i.e. is the -th largest (positive) imaginary component. We empirically deduced the following conjecture for the asymptotics of the imaginary part. Conjecture 5. As , where is a constant. For a fixed , the other eigenvalues satisfy an inverse scaling in : . Fig. 4 empirically supports this conjecture. Based on 5, we propose the initialization S4D-Inv to use the following inverse-law diagonal matrix which closely approximates S4D-LegS. ( S4D-Inv ) \ud835\udc68 n = \u2212 1 2 + i \u200b N \u03c0 \u200b ( N 2 \u200b n + 1 \u2212 1 ) S4D-Inv subscript \ud835\udc68 \ud835\udc5b 1 2 \ud835\udc56 \ud835\udc41 \ud835\udf0b \ud835\udc41 2 \ud835\udc5b 1 1 (\\textbf{S4D-Inv})\\quad\\bm{A}_{n}=-\\frac{1}{2}+i\\frac{N}{\\pi}\\left(\\frac{N}{2n+1}-1\\right) (8)\n\n( S4D-Lin ) \ud835\udc68 n = \u2212 1 2 + i \u200b \u03c0 \u200b n S4D-Lin subscript \ud835\udc68 \ud835\udc5b 1 2 \ud835\udc56 \ud835\udf0b \ud835\udc5b (\\textbf{S4D-Lin})\\quad\\bm{A}_{n}=-\\frac{1}{2}+i\\pi n (9)\n\nS4D-Lin. While S4D-Inv can be seen as an approximation to the original S4-LegS, we propose an even simpler scaling law for the imaginary parts that can be seen as an approximation of S4-FouT ([10]), where the imaginary parts are simply the Fourier series frequencies (i.e. matches the diagonal part of the DPLR form of S4-FouT). Fig. 1 (Right) illustrates the S4D-Lin basis , which are simply damped Fourier basis functions. General Diagonal SSM Basis Functions. The empirical study in Section 5 performs many ablations of different diagonal initializations, showing that many natural variants of the proposed methods do not perform as well. The overall guiding principles for the diagonal state matrix are twofold, which can be seen from the closed form of the basis functions (Eq. 3). First, the real part of controls the decay rate of the function. is a good default that bounds the basis functions by the envelope , giving a constant timescale (Fig. 1 (Right)). Second, the imaginary part of controls the oscillating frequencies of the basis function. Critically, these should be spread out, which explains why random initializations of do not perform well. S4D-Inv and S4D-Lin use simple asymptotics for these imaginary components that provide interpretable bases. We believe that alternative initializations that have different mathematical interpretations may exist, which is an interesting question for future work. 5 Experiments\n\nOur experimental study shows that S4D has strong performance in a wide variety of domains and tasks, including the well-studied Long Range Arena (LRA) benchmark where the best S4D variant is competitive with S4 on all tasks and significantly outperforms all non-SSM baselines. We begin with controlled ablations of the various representations of diagonal state space models. \u2022\n\nIn Section 5.1, we compare the different methods of parameterizing and computing a diagonal state space model (Section 3). \u2022\n\nIn Section 5.2, we compare our proposed initializations of the critical matrix and perform several ablations showing that simple variants can substantially degrade performance, underscoring the importance of choosing carefully (Section 4). \u2022\n\nIn Section 5.3, we compare our proposed S4D methods against the original S4 method (and the variants proposed in [10]). Methodology and Datasets. In order to study the effects of different S4 and S4D variants in a controlled setting, we propose the following protocol. We focus on three datasets covering a varied range of data modalities (image pixels, biosignal time series, audio waveforms), sequence lengths (1K, 4K, 16K), and tasks (classification and regression with bidirectional and causal models). \u2022\n\nSequential CIFAR (sCIFAR). CIFAR-10 images are flattened into a sequence of length , and a bidirectional sequence model is used to perform 10-way classification. \u2022\n\nBIDMC Vital Signs. EKG and PPG signals of length are used to predict respiratory rate (RR), heart rate (HR), and blood oxygen saturation (SpO2). We focus on SpO2 in this study. \u2022\n\nSpeech Commands (SC).333We note that a line of prior work including S4 [14, 19, 9] all used a smaller 10-class subset of SC, so our results on the full dataset are not directly comparable. A 1-second raw audio waveform comprising samples is used for 35-way spoken word classification. We use an autoregressive (AR) model to vary the setting; this causal setting more closely imitates autoregressive speech generation, where SSMs have shown recent promise [5]. We fix a simple architecture and training protocol that works generically. The architecture has layers and hidden dimension , resulting in parameters. All results are averaged over multiple seeds (full protocol and results including std. reported in Appendix B). 5.1 Parameterization, Computation, Discretization\n\nGiven the same diagonal SSM matrices , there are many variants of how to parameterize the matrices and compute the SSM kernel described in Section 3. We ablate the different choices described in Table 1. Results are in Table 2, and show that:\n\n(i)\n\nComputing the model with a softmax instead of Vandermonde product does not make much difference\n\n(ii)\n\nTraining is consistently slightly better\n\n(iii)\n\nDifferent discretizations (Section 3.1) do not make a noticeable difference\n\n(iv)\n\nUnrestricting the real part of (Section 3.3) may be slightly better\n\nThese ablations show that for a fixed initialization , different aspects of parameterizing SSMs make little difference overall. This justifies the parameterization and algorithm S4D uses (Section 3.4), which preserves the choices of the original S4 model and is simpler than DSS. For the remaining of the experiments in Section 5.2 and Section 5.3, we fix the S4D parameterization and algorithm described in Section 3. Note that this computes exactly the same kernel as the original S4 algorithm when the low-rank portion is set to , allowing controlled comparisons of the critical state matrix for the remainder of this section. 5.2 S4D Initialization Ablations\n\nThe original S4 model proposed a specific formula for the matrix, and the first diagonal version [11] used a specific matrix based on it. Our new proposed variants S4D-Inv and S4D-Lin also define precise formulas for the initialization of the matrix (8). This raises the question of whether the initialization of the still needs to be so precise, despite the large simplifications from the original version. We perform several natural ablations on these initializations, showing that even simple variations of the precise formula can degrade performance. Imaginary part scaling factor. The scaling rules for the imaginary parts of S4D-Inv and S4D-Lin are simple polynomial laws, but how is the constant factor chosen and how important is it? These constants are based on approximations to HiPPO methods (e.g. 5). Note that the range of imaginary components for S4D-Inv and S4D-Lin are quite different (Fig. 4); the largest imaginary part is for S4D-Inv and for S4D-Lin. We consider scaling all imaginary parts by a constant factor of or to investigate whether the constant matters. Note that this preserves the overall shape of the basis functions (Fig. 1, dashed lines) and simply changes the frequencies, and it is not obvious that this should degrade performance. However, both changes substantially reduce the performance of S4D in all settings. Randomly initialized imaginary part. Next, we consider choosing the imaginary parts randomly. For S4D-Inv, we keep the real parts equal to and set each imaginary component to\n\n\ud835\udc68 n subscript \ud835\udc68 \ud835\udc5b \\displaystyle\\bm{A}_{n} = \u2212 1 2 + i \u200b N \u03c0 \u200b ( N 2 \u200b u + 1 \u2212 1 ) u \u223c N \u22c5 \ud835\udcb0 \u200b [ 0 , 1 ] formulae-sequence absent 1 2 \ud835\udc56 \ud835\udc41 \ud835\udf0b \ud835\udc41 2 \ud835\udc62 1 1 similar-to \ud835\udc62 \u22c5 \ud835\udc41 \ud835\udcb0 0 1 \\displaystyle=-\\frac{1}{2}+i\\frac{N}{\\pi}\\left(\\frac{N}{2u+1}-1\\right)\\qquad u\\sim N\\cdot\\mathcal{U}[0,1] (10)\n\nNote that when is equally spaced in instead of uniformly random, this exactly recovers S4D-Inv (8), so this is a sensible random approximation to it. Similarly, we consider a variant of S4D-Lin\n\n\ud835\udc68 n subscript \ud835\udc68 \ud835\udc5b \\displaystyle\\bm{A}_{n} = \u2212 1 2 + i \u200b \u03c0 \u200b u \u200b N u \u223c N \u22c5 \ud835\udcb0 \u200b [ 0 , 1 ] formulae-sequence absent 1 2 \ud835\udc56 \ud835\udf0b \ud835\udc62 \ud835\udc41 similar-to \ud835\udc62 \u22c5 \ud835\udc41 \ud835\udcb0 0 1 \\displaystyle=-\\frac{1}{2}+i\\pi uN\\qquad u\\sim N\\cdot\\mathcal{U}[0,1] (11)\n\nthat is equal to equation (9) when is equally spaced instead of random. Table 3(a) (Random Imag) shows that this small change causes minor degradation in performance. We additionally note that the randomly initialized imaginary ablation can be interpreted as follows. Fig. 4 shows the asymptotics of the imaginary parts of SSM matrices, where the imaginary parts of the eigenvalues correspond to y-values corresponding to uniformly spaced nodes on the x-axis. This ablation then replaces the uniform spacing on the x-axis with uniformly random x values. Randomly initialized real part. We considering initializing the real part of each eigenvalue as instead of fixing them to . Table 3(a)(Left, Random Real) shows that this also causes minor but consistent degradation in performance on the ablation datasets. Finally, we also consider randomizing both real and imaginary parts, which degrades performance even further. Ablation: Other S4D matrices. Other simple variants of initializations show that it is not just the range of the eigenvalues but the actual distribution that is important (Fig. 4). Both S4D-Inv2 and S4D-Quad have real part and imaginary part satisfying the same maximum value as 5. The S4D-Inv2 initialization uses the same formula as S4D-Inv, but replaces a in the denominator with . The S4D-Quad initialization uses a polynomial law with power instead of (S4D-Inv) or (S4D-Lin). (12) (13)\n\nWe include two additional methods here that are not based on the proposed S4D-Inv or S4D-Lin methods. First, S4D-Rand uses a randomly initialized diagonal , and validates that it performs poorly, in line with earlier findings [9, 11]. Second, S4D-Real uses a particular real initialization with . This is the exact same spectrum as the original S4(-LegS) method, which validates that it is not just the diagonalization that matters, highlighting the limitations of Proposition 2. 5.3 Full Comparisons of S4D and S4 Methods\n\nTrainable matrices. Table 3(b) shows the performance of all S4D and S4 variants [10] on the ablations datasets. We observe several interesting phenomena:\n\n(i)\n\nFreezing the matrices performs comparably to training them on sCIFAR and BIDMC, but is substantially worse on SC. We hypothesize that this results from being poorly initialized for SC, so that at initialization models do not have context over the entire sequence, and training and helps adjust for this. As further evidence, the finite window methods S4-LegT and S4-FouT (defined in [10]) have the most limited context and suffer the most when is frozen. (ii)\n\nThe full DPLR versions are often slightly better than the diagonal version throughout the entire training curve. We report the validation accuracy after 1 epoch of training on sCIFAR and SC to illustrate this phenomenon. Note that this is not a consequence of having more parameters (Appendix B). Large models on ablation datasets. Finally, we relax the strict requirements on model size and regularization for the ablation datasets, and show the performance of S4 and S4D variants on the test sets with a larger model (architecture and training details in Appendix B) when the model size and regularization is simply increased (Table 4). We note that results for each dataset are better than the original S4 model, which was already state-of-the-art on these datasets [8, 9]. Long Range Arena. We use the same hyperparameter setting for the state-of-the-art S4 model in [10] on the Long Range Arena benchmark for testing long dependencies in sequence models. S4D variants are highly competitive on all datasets except Path-X, and outperform the S4 variants on several of them. On Path-X using this hyperparameter setting with bidirectional models, only S4D-Inv, our simpler approximation to the original S4-LegS model, achieves above random chance, and has an average of 85% on the full LRA suite, more than 30 points better than the original Transformer [24]. Final parameterization ablations on Path-X. Finally, we return to the parameterization choices presented in Section 3 and ablated in Section 5.1, and ablate them once more on the difficult Path-X dataset. We use small models of between 150K and 200K parameters (differing only depending on whether is trained). We fix the S4D-LegS initialization (i.e., the diagonal HiPPO initialization (5)). We start from the base S4D parameterization based on S4: bilinear discretization, , trainable , and no softmax (Table 1). We ablate each of these choices one at a time for the discretization, constraint on , trainability of , and normalization. We also consider the combination that defines DSS: ZOH discretization, identity , frozen , softmax normalization. Table 6 shows that the default S4 parameterization choices are a strong baseline. As in Section 5.1, we find that most of the other choices do not make much difference:\n\n(i)\n\nletting be unconstrained has little benefit, and can theoretically cause instabilities, so we do not recommend it,\n\n(ii)\n\nthe bilinear vs. ZOH discretizations make no difference,\n\n(iii)\n\ntraining helps slightly, for a minor increase in parameter count and no change in speed. Finally, on this task \u2013 unlike the easier ablation datasets in Section 5.1 \u2013 the softmax normalization of DSS actually hurts performance, and we do not recommend it in general. 6 Conclusion\n\nState space models based on S4 are a promising family of models for modeling many types of sequential data, with particular strengths for continuous signals and long-range interactions. These models are a large departure from conventional sequence models such as RNNs, CNNs, and Transformers, with many new ideas and moving parts. This work provides a more in-depth exposition for all aspects of working with S4-style models, from their core structures and kernel computation algorithms, to miscellaneous choices in their parameterizations, to new theory and methods for their initialization. We systematically analyzed and ablated each of these components, and provide recommendations for building a state space model that is as simple as possible, while as theoretically principled and empirically effective as S4. We believe that S4D can be a strong generic sequence model for a variety of domains, that opens new directions for state space models theoretically, and is much more practical to understand and implement for practitioners.",
    "s4d-7": "Acknowledgments\n\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.",
    "s4d-8": "The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S.",
    "s4d-9": "Government. References\n\nBa et al. [2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Bai et al. [2019] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In The International Conference on Learning Representations (ICLR), 2019. Dauphin et al. [2017] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks.",
    "s4d-10": "In International conference on machine learning, pages 933\u2013941. PMLR, 2017. Erichson et al. [2021] N Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021. Goel et al. [2022] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It\u2019s raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Gu et al. [2020a] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections.",
    "s4d-11": "In Advances in Neural Information Processing Systems (NeurIPS), 2020a. Gu et al. [2020b] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In The International Conference on Machine Learning (ICML), 2020b. Gu et al. [2021] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer.",
    "s4d-12": "In Advances in Neural Information Processing Systems (NeurIPS), 2021. Gu et al. [2022a] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022a. Gu et al. [2022b] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. How to train your hippo: State space models with generalized basis projections.",
    "s4d-13": "arXiv preprint arXiv:2206.12037, 2022b. Gupta [2022] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.",
    "s4d-14": "arXiv preprint arXiv:2203.14343, 2022. Hochreiter and Schmidhuber [1997] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. Ioffe and Szegedy [2015] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift.",
    "s4d-15": "In International conference on machine learning, pages 448\u2013456. PMLR, 2015. Kidger et al. [2020] Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations for irregular time series. arXiv preprint arXiv:2005.08926, 2020. Morrill et al. [2021] James Morrill, Cristopher Salvi, Patrick Kidger, James Foster, and Terry Lyons. Neural rough differential equations for long time series. The International Conference on Machine Learning (ICML), 2021. Nonaka and Seita [2021] Naoki Nonaka and Jun Seita. In-depth benchmarking of deep neural network architectures for ecg diagnosis.",
    "s4d-16": "In Machine Learning for Healthcare Conference, pages 414\u2013439. PMLR, 2021. Pan [2001] Victor Pan. Structured matrices and polynomials: unified superfast algorithms. Springer Science & Business Media, 2001. Ralston and Rabinowitz [2001] Anthony Ralston and Philip Rabinowitz. A first course in numerical analysis.",
    "s4d-17": "Courier Corporation, 2001. Romero et al. [2021] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data.",
    "s4d-18": "arXiv preprint arXiv:2102.02611, 2021. Romero et al. [2022] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, and Jan C van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes.",
    "s4d-19": "In The International Conference on Learning Representations (ICLR), 2022. Rusch and Mishra [2021] T Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long time dependencies.",
    "s4d-20": "The International Conference on Machine Learning (ICML), 2021. Shazeer [2020] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Tan et al. [2021] Chang Wei Tan, Christoph Bergmeir, Francois Petitjean, and Geoffrey I Webb. Time series extrinsic regression.",
    "s4d-21": "Data Mining and Knowledge Discovery, pages 1\u201329, 2021. doi: https://doi.org/10.1007/s10618-021-00745-9. Tay et al. [2021] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers.",
    "s4d-22": "In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k. Trinh et al. [2018] Trieu H Trinh, Andrew M Dai, Minh-Thang Luong, and Quoc V Le. Learning longer-term dependencies in RNNs with auxiliary losses.",
    "s4d-23": "In The International Conference on Machine Learning (ICML), 2018. Voelker et al. [2019] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. In Advances in Neural Information Processing Systems, pages 15544\u201315553, 2019. Appendix A Method Details\n\nA.1 Proofs\n\nWe prove Theorem 3, and then show why this it is a surprising result that is not true in general to low-rank perturbations of SSMs. We start with the interpretation of the S4-LegS matrix shown in [10], which corresponds to Fig. 1 (Left). Theorem 6. Let be the matrices defined in equation (4). The SSM kernels have the closed form formula\n\nK n \u200b ( t ) = L n \u200b ( e \u2212 t ) \u200b e \u2212 t subscript \ud835\udc3e \ud835\udc5b \ud835\udc61 subscript \ud835\udc3f \ud835\udc5b superscript \ud835\udc52 \ud835\udc61 superscript \ud835\udc52 \ud835\udc61 \\displaystyle K_{n}(t)=L_{n}(e^{-t})e^{-t}\n\nwhere are the Legendre polynomials shifted and scaled to be orthonormal on the interval .",
    "s4d-24": "Lemma A.1. The functions are a complete orthonormal basis with respect to the measure . Proof. The polynomials are defined to be orthonormal on , i.e. \u222b 0 1 L n \u200b ( t ) \u200b L m \u200b ( t ) \u200b \ud835\udc51 t = \u03b4 n , m . superscript subscript 0 1 subscript \ud835\udc3f \ud835\udc5b \ud835\udc61 subscript \ud835\udc3f \ud835\udc5a \ud835\udc61 differential-d \ud835\udc61 subscript \ud835\udeff \ud835\udc5b \ud835\udc5a \\displaystyle\\int_{0}^{1}L_{n}(t)L_{m}(t)\\mathop{}\\!dt=\\delta_{n,m}. By the change of variables with ,\n\n\u2212 \u222b \u2212 \u221e 0 L n \u200b ( e \u2212 s ) \u200b L m \u200b ( e \u2212 s ) \u200b e \u2212 s \u200b \ud835\udc51 s = \u03b4 n , m = \u222b 0 \u221e L n \u200b ( e \u2212 s ) \u200b L m \u200b ( e \u2212 s ) \u200b e \u2212 s \u200b \ud835\udc51 s superscript subscript 0 subscript \ud835\udc3f \ud835\udc5b superscript \ud835\udc52 \ud835\udc60 subscript \ud835\udc3f \ud835\udc5a superscript \ud835\udc52 \ud835\udc60 superscript \ud835\udc52 \ud835\udc60 differential-d \ud835\udc60 subscript \ud835\udeff \ud835\udc5b \ud835\udc5a superscript subscript 0 subscript \ud835\udc3f \ud835\udc5b superscript \ud835\udc52 \ud835\udc60 subscript \ud835\udc3f \ud835\udc5a superscript \ud835\udc52 \ud835\udc60 superscript \ud835\udc52 \ud835\udc60 differential-d \ud835\udc60 \\displaystyle-\\int_{-\\infty}^{0}L_{n}(e^{-s})L_{m}(e^{-s})e^{-s}\\mathop{}\\!ds=\\delta_{n,m}=\\int_{0}^{\\infty}L_{n}(e^{-s})L_{m}(e^{-s})e^{-s}\\mathop{}\\!ds\n\nwhich shows the orthonormality. Completeness follows from the fact that polynomials are complete. \u220e\n\nProof of Theorem 3. We start with the standard interpretation of SSMs as convolutional systems. The SSM is equivalent to the convolution\n\nx n \u200b ( t ) = ( u \u2217 K n ) \u200b ( t ) = \u222b \u2212 \u221e t u \u200b ( s ) \u200b K n \u200b ( t \u2212 s ) \u200b \ud835\udc51 s = \u222b 0 \u221e u \u200b ( t \u2212 s ) \u200b K n \u200b ( s ) \u200b \ud835\udc51 s subscript \ud835\udc65 \ud835\udc5b \ud835\udc61 \u2217 \ud835\udc62 subscript \ud835\udc3e \ud835\udc5b \ud835\udc61 superscript subscript \ud835\udc61 \ud835\udc62 \ud835\udc60 subscript \ud835\udc3e \ud835\udc5b \ud835\udc61 \ud835\udc60 differential-d \ud835\udc60 superscript subscript 0 \ud835\udc62 \ud835\udc61 \ud835\udc60 subscript \ud835\udc3e \ud835\udc5b \ud835\udc60 differential-d \ud835\udc60 \\displaystyle x_{n}(t)=(u\\ast K_{n})(t)=\\int_{-\\infty}^{t}u(s)K_{n}(t-s)\\mathop{}\\!ds=\\int_{0}^{\\infty}u(t-s)K_{n}(s)\\mathop{}\\!ds\n\nfor the SSM kernels (equation (3)). Defining , we can write this as\n\nx n \u200b ( t ) = \u27e8 u ( t ) , K n \u27e9 \u03c9 subscript \ud835\udc65 \ud835\udc5b \ud835\udc61 subscript superscript \ud835\udc62 \ud835\udc61 subscript \ud835\udc3e \ud835\udc5b \ud835\udf14 \\displaystyle x_{n}(t)=\\langle u^{(t)},K_{n}\\rangle_{\\omega}\n\nwhere and is the inner product in the Hilbert space of L2 functions with respect to measure . By Theorem 6, the are a complete orthonormal basis in this Hilbert space. There represents a decomposition of the function with respect to this basis, and can be recovered as a linear combination of these projections\n\nu ( t ) = \u2211 n = 0 \u221e x n \u200b ( t ) \u200b K n . superscript \ud835\udc62 \ud835\udc61 superscript subscript \ud835\udc5b 0 subscript \ud835\udc65 \ud835\udc5b \ud835\udc61 subscript \ud835\udc3e \ud835\udc5b \\displaystyle u^{(t)}=\\sum_{n=0}^{\\infty}x_{n}(t)K_{n}. Pointwise over the inner times ,\n\nu ( t ) \u200b ( s ) = \u2211 n = 0 \u221e x n \u200b ( t ) \u200b K n \u200b ( s ) . superscript \ud835\udc62 \ud835\udc61 \ud835\udc60 superscript subscript \ud835\udc5b 0 subscript \ud835\udc65 \ud835\udc5b \ud835\udc61 subscript \ud835\udc3e \ud835\udc5b \ud835\udc60 \\displaystyle u^{(t)}(s)=\\sum_{n=0}^{\\infty}x_{n}(t)K_{n}(s). This implies that\n\nu \u200b ( t ) \ud835\udc62 \ud835\udc61 \\displaystyle u(t) = u ( t ) \u200b ( 0 ) = \u2211 n = 0 \u221e x n \u200b ( t ) \u200b K n \u200b ( 0 ) absent superscript \ud835\udc62 \ud835\udc61 0 superscript subscript \ud835\udc5b 0 subscript \ud835\udc65 \ud835\udc5b \ud835\udc61 subscript \ud835\udc3e \ud835\udc5b 0 \\displaystyle=u^{(t)}(0)=\\sum_{n=0}^{\\infty}x_{n}(t)K_{n}(0) = \u2211 n = 0 \u221e x n \u200b ( t ) \u200b L n \u200b ( 0 ) = \u2211 n = 0 \u221e x n \u200b ( t ) \u200b ( 2 \u200b n + 1 ) 1 2 absent superscript subscript \ud835\udc5b 0 subscript \ud835\udc65 \ud835\udc5b \ud835\udc61 subscript \ud835\udc3f \ud835\udc5b 0 superscript subscript \ud835\udc5b 0 subscript \ud835\udc65 \ud835\udc5b \ud835\udc61 superscript 2 \ud835\udc5b 1 1 2 \\displaystyle=\\sum_{n=0}^{\\infty}x_{n}(t)L_{n}(0)=\\sum_{n=0}^{\\infty}x_{n}(t)(2n+1)^{\\frac{1}{2}} = \ud835\udc69 \u22a4 \u200b x \u200b ( t ) absent superscript \ud835\udc69 top \ud835\udc65 \ud835\udc61 \\displaystyle=\\bm{B}^{\\top}x(t)\n\nIntuitively, due to the function reconstruction interpretation of HiPPO [10], we can approximate using knowledge in the current state . There in the limit , the original SSM is equivalent to\n\nx \u2032 \u200b ( t ) superscript \ud835\udc65 \u2032 \ud835\udc61 \\displaystyle x^{\\prime}(t) = \ud835\udc68 \u200b x \u200b ( t ) + \ud835\udc69 \u200b u \u200b ( t ) absent \ud835\udc68 \ud835\udc65 \ud835\udc61 \ud835\udc69 \ud835\udc62 \ud835\udc61 \\displaystyle=\\bm{A}x(t)+\\bm{B}u(t) = \ud835\udc68 \u200b x \u200b ( t ) + 1 2 \u200b \ud835\udc69 \u200b u \u200b ( t ) + 1 2 \u200b \ud835\udc69 \u200b u \u200b ( t ) absent \ud835\udc68 \ud835\udc65 \ud835\udc61 1 2 \ud835\udc69 \ud835\udc62 \ud835\udc61 1 2 \ud835\udc69 \ud835\udc62 \ud835\udc61 \\displaystyle=\\bm{A}x(t)+\\frac{1}{2}\\bm{B}u(t)+\\frac{1}{2}\\bm{B}u(t) = \ud835\udc68 \u200b x \u200b ( t ) + 1 2 \u200b \ud835\udc69 \u200b \ud835\udc69 \u22a4 \u200b x \u200b ( t ) + 1 2 \u200b \ud835\udc69 \u200b u \u200b ( t ) absent \ud835\udc68 \ud835\udc65 \ud835\udc61 1 2 \ud835\udc69 superscript \ud835\udc69 top \ud835\udc65 \ud835\udc61 1 2 \ud835\udc69 \ud835\udc62 \ud835\udc61 \\displaystyle=\\bm{A}x(t)+\\frac{1}{2}\\bm{B}\\bm{B}^{\\top}x(t)+\\frac{1}{2}\\bm{B}u(t) = \ud835\udc68 \u200b x \u200b ( t ) + \ud835\udc77 \u200b \ud835\udc77 \u22a4 \u200b x \u200b ( t ) + 1 2 \u200b \ud835\udc69 \u200b u \u200b ( t ) absent \ud835\udc68 \ud835\udc65 \ud835\udc61 \ud835\udc77 superscript \ud835\udc77 top \ud835\udc65 \ud835\udc61 1 2 \ud835\udc69 \ud835\udc62 \ud835\udc61 \\displaystyle=\\bm{A}x(t)+\\bm{P}\\bm{P}^{\\top}x(t)+\\frac{1}{2}\\bm{B}u(t) = \ud835\udc68 N \u200b x \u200b ( t ) + 1 2 \u200b \ud835\udc69 \u200b u \u200b ( t ) absent superscript \ud835\udc68 \ud835\udc41 \ud835\udc65 \ud835\udc61 1 2 \ud835\udc69 \ud835\udc62 \ud835\udc61 \\displaystyle=\\bm{A}^{N}x(t)+\\frac{1}{2}\\bm{B}u(t)\n\nGeneral low-rank perturbations. Finally, we remark that this phenomenon where removing the low-rank correction to a DPLR matrix approximates the original dynamics, is unique to this HiPPO-LegS matrix. We note that if instead of , a random rank-1 correction is added to the HiPPO-LegS matrix in Theorem 3, the resulting SSM kernels look completely different and in fact diverge rapidly as the magnitude of increases (Fig. 5). Similarly, Fig. 6(a) shows a new S4 variant called S4-FouT that is also DPLR [10], but removing the low-rank component dramatically changes the SSM kernels. Appendix B Experiment Details\n\nAblation datasets training protocol. The architecture has layers and hidden dimension , resulting in around 100K trainable parameters. The and parameters were tied across the SSM copies; therefore the S4 models have only more parameters than S4D models, arising from the tensor in the DPLR representation . This choice was made because it generally does not affect performance much, while reducing parameter count and ensuring that S4 vs. S4D models have very similar numbers of parameters. All results are averaged over 2 or 3 seeds. All models use learning rate , weight decay, and no other regularization or data augmentation. For the classification tasks (sCIFAR and SC). we use a cosine scheduler with 1 epoch warmup and decaying to . For the regression task (BIDMC), we use a multistep scheduler following [21, 8]. Reported results are all best validation accuracy, except for the large models in Table 4. Full results for parameterization ablations. Table 7 and Table 8 contain the raw results for Table 2 including standard deviations. Full results for large models on ablations datasets. Figs. 8, 8 and 9 show full results comparing our proposed methods against the best models from the literature; citations indicate numbers from prior work. Note that earlier works on the Speech Commands dataset typically use pre-processing such as MFCC features, or a 10-class subset of the full 35-class dataset [14, 19, 9]. As we are not aware of a collection of strong baselines for raw waveform classification using the full dataset, we trained several baselines from scratch for Table 9. The InceptionNet, ResNet-18, and XResNet-50 models are 1D adaptations from Nonaka and Seita [16] of popular CNN architectures for vision. The ConvNet architecture is a generic convolutional neural network that we tuned for strong performance, comprising:\n\n\u2022\n\nFour stages, each composed of three identical residual blocks. \u2022\n\nThe first stage has model dimension (i.e. channels, in CNN nomenclature) . Each stage doubles the dimension of the previous stage (with a position-wise linear layer) and ends in an average pooling layer of width . Thus, the first stage operates on inputs of length , dimension (the input is zero-padded from 16000 to 16384) and the last on length , dimension . \u2022\n\nEach residual block has a (pre-norm) BatchNorm layer followed by a convolution layer and GeLU activation. \u2022\n\nConvolution layers have a kernel size of . Long Range Arena. Our Long Range Arena experiments follow the same setup as the original S4 paper with some differences in model architecture and hyperparameters. The main global differences are as follows:\n\nExceptions to the above rules are described below. Full hyperparameters are in Table 10. sCIFAR / LRA Image. This dataset is grayscale sequential CIFAR-10, and the settings for this task were taken from S4\u2019s hyperparameters on the normal sequential CIFAR-10 task. In particular, this used LayerNorm [1] instead of BatchNorm [13], a larger number of hidden features , post-norm instead of pre-norm, and minor dropout. We note that the choice of normalization and increased do not make a significant difference on final performance, still attaining classification accuracy in the high 80\u2019s. Dropout does seem to make a difference. BIDMC. We used a larger state size of , since we hypothesized that picking up higher frequency features on this dataset would help. We also used a step scheduler that decayed the LR by every epochs, following prior work [21, 8]. ListOps. We hypothesized that this task benefits from deeper models, because of the explicit hierarchical nature of the task, so the architecture used here had 8 layers and hidden features. However, results are very close with much smaller models. We also found that post-norm generalized better than pre-norm, but results are again close (less than difference). PathX. As described in [10], the initialization range for PathX is decreased from to . \u25c4 Feeling lucky?",
    "s4d-25": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Mon Mar 11 19:34:55 2024 by LaTeXML"
}