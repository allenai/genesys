{
    "ssmfundmodel-0": "State Space Models as Foundation Models: A Control Theoretic Overview\n\nCarmen Amo Alonso\u2217, Jerome Sieber\u2217, and Melanie N.",
    "ssmfundmodel-1": "Zeilinger * These authors contributed equally; ordered alphabetically.All authors are with the Institute for Dynamic Systems and Control, ETH Zurich, 8092 Zurich, Switzerland {camoalonso,jsieber,mzeilinger}@ethz.ch. This work was partially supported by the ETH AI Center. Abstract\n\nIn recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model\u2019s efficiency at learning long sequences. Index Terms:\n\nMachine learning, Linear systems, Time-varying systems. I Introduction\n\nRecently, foundation models have become central to the field of artificial intelligence. These models are large-scale learning models that are initially pretrained on extensive datasets, and subsequently fine-tuned for specific tasks. The term foundation models highlights these models\u2019 capability to learn and effectively generalize across a wide array of modalities, encompassing language, audio, images, video, genomics, and more. At their core, the predominant architecture for foundation models is the Transformer [1]. This architecture, based on the attention mechanism, allows to efficiently process information and model global dependencies in complex data; but it suffers from two main limitations. One is computational complexity: it requires the complete sequence to be fed into the model every time an output is generated, which results in poor scalability with the time horizon window,111Referred to as input length in Transformer\u2019s literature. and therefore poor performance in long context tasks [2]. The other limitation is explainability: despite its simple mathematical representation, it is currently not possible to interpret or understand the choice of outputs made by the Transformer [3]. Efforts to address the scalability challenges of Transformers have led to various architectural variants that still leverage the merits of the attention mechanism. Examples of such variants are the Longformer [4], BigBird [5], the Reformer [6], the Performer [7], and approaches leveraging Axial Attention [8]. However, despite extensive research on these fronts, the proposed solutions often degrade the inherent merits of the architecture or fail to perform well in practice [2]. A recent and promising research avenue proposes to fully replace the attention mechanism with a different representation based on State Space Models (SSM). The advantage of the SSM representation lies in its recurrent nature, where only the latest input has to be passed to the model since the state is able to capture information about past inputs. Moreover, due to their mathematical structure, they are amenable to computationally efficient training and inference\u2013in contrast to their predecessors, recurrent neural networks (RNNs) [9]. This new family of SSM-based architectures has been shown to beat Transformers in long-context tasks such as the Long Range Arena (LRA) benchmark [2], and recent proposals such as Mamba [10] exhibit performance and computational efficiency superior to state-of-the-art Transformers on long-context tasks. These results highlight the potential of SSMs to overcome many of the current limitations of Transformers. Although SSMs show great promise to serve as foundation models, most of the existing literature on SSMs focuses on providing performant architectures and efficient implementations. Despite the clear connection with control theory, in particular linear systems theory, to date a principled understanding of these models is lacking, and most design choices are motivated from an empirical performance rather than a systematic system theoretical viewpoint. There is large potential in leveraging existing system theoretic results and analysis to complement current implementations and enhance explainability, design and performance. Towards this goal, the aim of this paper is to provide an overview of state-of-the-art SSMs from a control theoretical perspective. In Section II, we provide an overview of the essential components and considerations in SSMs. In Section III, we review the most relevant SSM proposals to date. Since these models were primarily motivated by their ability to handle long contexts, we present the first performance comparison to date on the LRA benchmark in Section IV. Lastly, we end in Section V with concluding remarks and open research questions that could help advance SSMs and cross-pollinate the fields of foundation models and systems and control theory. II State Space Models\n\nWe first present a generic language modelling task to define the learning goal of a foundation model. Then, we give an overview of the state space model architecture, mathematical structure, and computational considerations that guide the SSMs introduced in the literature. II-A Learning setup\n\nA foundation model, such as those used in language modeling, can be seen as a map between input and output signals, i.e.,\n\ny \u200b ( k ) = f \u200b ( u \u200b ( k ) , \u2026 , u \u200b ( k \u2212 T ) ; \u03b8 ) , \ud835\udc66 \ud835\udc58 \ud835\udc53 \ud835\udc62 \ud835\udc58 \u2026 \ud835\udc62 \ud835\udc58 \ud835\udc47 \ud835\udf03 y(k)=f(u(k),\\dots,u(k-T);\\theta), (1)\n\nwhere at each time , the output is produced after evaluating an input signal of length , i.e., , and a set of parameters . The parameters are task dependent, and can be fine-tuned accordingly. Since the search space of general is too broad, different parameterizations of can be used to render the problem tractable. For instance, the model can consist of multiple stacked models like e.g. the Transformer or more recently SSMs. The architectural choice of is a fundamental factor in determining the success of the model at effectively learning structure from data. The goal of a foundation model used as large language model is to learn a compressed representation of structure present in language in order to perform tasks like machine translation or human-level conversations (e.g. ChatGPT). To learn such a representation the parameterized model is presented with input-output pairs , where represents the parameters. The parameters are then iteratively updated to minimize a loss function , i.e., iteratively solving the following optimization problem\n\nmin \u03b8 \u2061 \u2112 \u200b ( y \u2212 f \u200b ( u ; \u03b8 ) ) . subscript \ud835\udf03 \u2112 \ud835\udc66 \ud835\udc53 \ud835\udc62 \ud835\udf03 \\min_{\\theta}\\;\\mathcal{L}(y-f(u;\\theta)). (2)\n\nFor a language model the inputs are tokenized222An input token is the unit that represent the smallest meaningful components of the input data, whether it\u2019s text, images, or any other form of information that the model processes. sentences and the outputs are a shifted version of the same inputs, i.e., an auto-regressive setup. II-B Parametrization\n\nLet us consider the following continuous-time linear system with dynamics\n\nx \u02d9 \u200b ( t ) \u02d9 \ud835\udc65 \ud835\udc61 \\displaystyle\\dot{x}(t) = A \u200b x \u200b ( t ) + B \u200b u \u200b ( t ) , absent \ud835\udc34 \ud835\udc65 \ud835\udc61 \ud835\udc35 \ud835\udc62 \ud835\udc61 \\displaystyle=Ax(t)+Bu(t), (3a) y \u200b ( t ) \ud835\udc66 \ud835\udc61 \\displaystyle y(t) = C \u200b x \u200b ( t ) + D \u200b u \u200b ( t ) , absent \ud835\udc36 \ud835\udc65 \ud835\udc61 \ud835\udc37 \ud835\udc62 \ud835\udc61 \\displaystyle=Cx(t)+Du(t), (3b)\n\nwhere represents the complex-valued state, are the input and the output, respectively, and denotes the continuous-time index. We note that the input fed into the system denoted as is not a control input; it is seen as an exogenous input exciting the system (3). This choice of notation is made to maintain consistency with the corresponding literature. are complex-valued matrices of appropriate dimensions and in representation (3), these matrices are assumed to be time-invariant. When considering their time-varying version, a time sub-index would be appended, i.e., . In the SSM literature, system (3) is used as a black-box representation in a foundation model. Here, the exogenous input represents a signal or input token fed into the model at a given time . The state represents the hidden state that stores the relevant information about the current and previous inputs up to time , and is the output of the model at time . In a learning setup, the matrices are parameters, which are commonly learned via stochastic gradient descent. Since computational efficiency and initialization are essential aspects in this framework, the dynamic matrix is often assumed to have a particular structure. As such, SSMs are often referred to as Structured SSMs. Assumption II.1\n\nThe dynamic matrix in dynamics (3) has a diagonal structure, i.e., with . Although initial proposals [11, 12] deviate slightly from Assumption II.1, most of the Structured SSMs literature assumes a diagonal matrix. Specific choices will be discussed in Section III. II-C Discretization\n\nIn order to implement a SSM, a discrete-time version of system (3) is used. Hence, the implementation of system (3) in discrete-time is\n\nx \u200b ( k + 1 ) \ud835\udc65 \ud835\udc58 1 \\displaystyle x(k+1) = A \u00af \u200b x \u200b ( k ) + B \u00af \u200b u \u200b ( k ) , absent \u00af \ud835\udc34 \ud835\udc65 \ud835\udc58 \u00af \ud835\udc35 \ud835\udc62 \ud835\udc58 \\displaystyle=\\bar{A}x(k)+\\bar{B}u(k), (4a) y \u200b ( k ) \ud835\udc66 \ud835\udc58 \\displaystyle y(k) = C \u00af \u200b x \u200b ( k ) + D \u00af \u200b u \u200b ( k ) , absent \u00af \ud835\udc36 \ud835\udc65 \ud835\udc58 \u00af \ud835\udc37 \ud835\udc62 \ud835\udc58 \\displaystyle=\\bar{C}x(k)+\\bar{D}u(k), (4b)\n\nwhere are the discrete-time dynamic matrices discretized with time-step , possibly with complex-valued components, and denotes the discrete-time index. The choice of discretization scheme chosen varies widely among the proposed models in the SSM literature, and an overview is presented in Section III. We note that it is also possible to directly start from a discrete-time model as in equation (4), oblivious to its continuous-time representation (3). However, in most of the SSM literature, a continuous-time view of the dynamics is preferred in order to better motivate the choice of initialization for the dynamical matrices[13]. II-D Structure and Initialization\n\nSince the dynamics (3) are being learned via gradient descent, initialization of the parameters was found to be of crucial importance. In particular, the initial values of matrix have a significant impact on the performance after training: on a simple classification task, performance increases from 67% when is randomly initialized, to 80% when is initialized using a principled strategy [12, Section 4.4]. Different strategies and parametrizations have been proposed in order to achieve a successful initialization, i.e. an initialization that results in the state being able to capture the recent history of the inputs for some time horizon . This property is referred to as memory in the standard SSM literature. As is well-known in control theory, the memory of system (4) is directly linked to the eigenvalues of matrix . Lemma II.2\n\n(Informal) A dynamical system with dynamics (4) has long-range memory, i.e., captures information from past inputs, if the eigenvalues of are inside the unit circle and very close to the unit circumference, i.e. and . Hence, the various initialization schemes presented in the SSM literature aim to ensure that the modulo of the eigenvalues of the learned matrix is approximately equal to (but not bigger than) . For the initialization of the other matrices, i.e., , and , standard initialization methods are used, e.g., Glorot [14] or LeCun [15], which essentially draw the initial values from a transformed uniform or normal distribution. Therefore, we omit the initialization details of , and in the following and refer the reader to the original papers [14, 15]. II-E Implementation\n\nOne of the major challenges addressed in the SSM literature is how to efficiently learn (training time) and deploy (inference time) the recurrence (4). At inference time, a causal representation is needed since the model does not have access to excitation inputs beyond the current time step. For this reason, the recurrent representation (4) is directly used starting with an initial excitation and zero initial state . In order to speed up this process, parallel scans algorithms [16] are used that efficiently compute the recurrence by computing each output component in parallel and caching intermediate results. During training, it is possible (and desirable) to use a non-causal representation since input-output pairs are available for all . Different techniques have been proposed in the literature. Some of the architectures can take advantage of parallel scan algorithms and use the recurrent representation from equation (4). Some other architectures rely on the convolutional representation of system (4), i.e.,\n\ny \u200b ( k ) = \u2211 \u03c4 = 0 k C \u00af \u200b A \u00af k \u2212 \u03c4 \u200b B \u00af \u200b u \u200b ( \u03c4 ) . \ud835\udc66 \ud835\udc58 superscript subscript \ud835\udf0f 0 \ud835\udc58 \u00af \ud835\udc36 superscript \u00af \ud835\udc34 \ud835\udc58 \ud835\udf0f \u00af \ud835\udc35 \ud835\udc62 \ud835\udf0f y(k)=\\sum_{\\tau=0}^{k}\\bar{C}\\bar{A}^{k-\\tau}\\bar{B}u(\\tau). (5)\n\nThis convolutional representation allows for faster learning because the complete input sequence can be passed through the model in one step. In terms of learning algorithms, SSM models are commonly trained using a standard stochastic gradient descent variation, i.e. Adam [17], and backpropagation [9]. Additionally, they can utilize the same heuristic methods to improve training as other deep-learning models, e.g., dropout or normalization [9]. II-F Scaffolding and Layers\n\nAlthough learning the dynamics in equation (3) is a major focus of SSMs, these dynamics are not simply implemented in isolation. In fact, pre-processing of the input and post-processing of the output is necessary to ensure good performance. In this paper, we refer to the algebraic operations of pre- and post-processing as the scaffolding surrounding the SSM computation in dynamics (4). A general overview of the architecture used in SSMs is provided in Figure 1. A collection of different scaffolding choices have been proposed in the literature, ranging from standard multilayer perceptron (MLP) choices to gating operations, as defined in Definition II.3. In general, a linear or nonlinear map is performed on the input before it is fed into system (4). Once the output has been computed, a gating operation is generally performed to control the flow of information from the input to the output . Intuitively, the gate controls which outputs are set to zero based on the inputs via the softmax operation. Definition II.3\n\nGiven two vectors , a gating operation is defined as , where , is the element-wise multiplication, and is the softmax operation.333In the SSM literature, sometimes other nonlinearities are used, such as ReLU, SiLU, etc. As is common practice in deep learning, several layers of SSMs (dynamics (3) and accompanying scaffolding) are stacked together, where each of them processes the output of the previous layer as its input, which is then fed into the next layer. This is possible since input and output are of the same dimension . For example on smaller tasks like e.g. the LRA benchmark [2], a SSM is composed of 6 structurally-identical layers (with different dynamic matrices), and the size of the systems ranges in . For language modelling the number of layers and system size can be significantly larger. It is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. III Review of existing methods\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section II. We also provide a summary of their main characteristics in Table I. III-A Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. Parametrization\n\nThe S4 model starts from a continuous time model (3), where the structure imposed on matrix is\n\nA = d \u200b i \u200b a \u200b g \u200b ( \u03bb 1 , \u2026 , \u03bb p ) + r \u200b s \u22c6 \ud835\udc34 \ud835\udc51 \ud835\udc56 \ud835\udc4e \ud835\udc54 subscript \ud835\udf06 1 \u2026 subscript \ud835\udf06 \ud835\udc5d \ud835\udc5f superscript \ud835\udc60 \u22c6 A=diag(\\lambda_{1},\\dots,\\lambda_{p})+rs^{\\star} (6)\n\nwith , and . This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix . Discretization\n\nThe discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step , i.e.,\n\nA \u00af = ( I \u2212 \u0394 2 \u200b A ) \u2212 1 \u200b ( I + \u0394 2 \u200b A ) , B \u00af = ( I \u2212 \u0394 2 \u200b A ) \u2212 1 \u200b \u0394 \u200b B , formulae-sequence \u00af \ud835\udc34 superscript \ud835\udc3c \u0394 2 \ud835\udc34 1 \ud835\udc3c \u0394 2 \ud835\udc34 \u00af \ud835\udc35 superscript \ud835\udc3c \u0394 2 \ud835\udc34 1 \u0394 \ud835\udc35 \\bar{A}=(I-\\frac{\\Delta}{2}A)^{-1}(I+\\frac{\\Delta}{2}A),\\qquad\\bar{B}=(I-\\frac{\\Delta}{2}A)^{-1}\\Delta B, (7)\n\nand . Note that this choice of discretization method couples the parameterizations of and via the discretization step , which is a common feature of most SSMs. Structure and Initialization\n\nThe model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) for is fed into a separate system (4), each producing a scalar output with . Each dynamics matrix for each of the SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma II.2), the follow up work SaShiMi [23] enforces to ensure stability. Implementation\n\nAt training time, a convolutional representation (5) is used. For efficient computation, the structure of (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model (4) is directly used. Scaffolding\n\nInitially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, H3 [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A. The post-processing remains a gating function. III-B Diagonal Structured State Space Sequence Model (S4D)\n\nThe initially proposed Diagonal State Space (DSS) [26] model and its enhancement S4D [18] build upon the S4 model. They simplify the structure of the dynamics matrices by introducing for the first time Assumption II.1, which results in computational improvements. Parametrization\n\nThe main contribution of the S4D paper is the introduction of a new, more efficient, structure of matrix consistent with Assumption II.1:\n\nA = d \u200b i \u200b a \u200b g \u200b ( \u03bb 1 , \u2026 , \u03bb p ) . \ud835\udc34 \ud835\udc51 \ud835\udc56 \ud835\udc4e \ud835\udc54 subscript \ud835\udf06 1 \u2026 subscript \ud835\udf06 \ud835\udc5d A=diag(\\lambda_{1},\\dots,\\lambda_{p}). (8)\n\nDiscretization\n\nThe discrete-time version (4) is computed by applying exact discretization to dynamics (3) with discretization step , i.e.,\n\nA \u00af = e \u0394 \u200b A , B \u00af = ( \u0394 \u200b A ) \u2212 1 \u200b ( A \u00af \u2212 I ) \u200b \u0394 \u200b B , formulae-sequence \u00af \ud835\udc34 superscript \ud835\udc52 \u0394 \ud835\udc34 \u00af \ud835\udc35 superscript \u0394 \ud835\udc34 1 \u00af \ud835\udc34 \ud835\udc3c \u0394 \ud835\udc35 \\bar{A}=e^{\\Delta A},\\qquad\\bar{B}=(\\Delta A)^{-1}(\\bar{A}-I)\\Delta B, (9)\n\nand . Structure and Initialization\n\nThe SISO structure used in S4D is the same one as in S4. Initalization of S4D is also done using HiPPO theory, with the added insight that the resulting matrix can be diagonalized for added computational efficiency. Similar to SaShiMi [23], the eigenvalues of used for initialization are constrained to lie in the negative halfplane. This initialization results in the eigenvalues shown in Figure 2. Implementation\n\nSimilar to S4, a convolutional representation (5) is used at training time, and a recurrent representation (4) at inference time. Given the diagonal structure of matrix , discretization (9) can be computed efficiently. Scaffolding\n\nThe scaffolding of S4D is identical to the one used in S4. III-C Simplified Structured State Space Sequence Model (S5)\n\nThe S5 parametrization [19] presents a simplification of the previously proposed S4D and leverages the concept of multiple input multiple output (MIMO) systems (as opposed to SISO) to simplify the architectural components and enhance computation. Parametrization\n\nThe parametrization used is identical to S4D. Discretization\n\nS5 is amenable to both discretizations proposed in S4 and S4D: bilinear (7) and exact (9). Structure and Initialization\n\nThe main contribution of the S5 model is the introduction of a MIMO interpretation of the previously proposed models, which leads to significant computational enhancements. In particular, the full input vector is fed into a single MIMO system (4) (of bigger dimension) as opposed to SISO scalar subsystems (of smaller dimension). This is achieved by stacking the subsystem matrices used in S4 and S4D. The matrix is again initialized using HiPPO theory and results in the same initial eigenvalues as S4D (Figure 2). Implementation\n\nThe MIMO structure together with the diagonal parameterization of allows for parallel computation of the individual output components from the input components via a parallel scan algorithm [16]. As a result, both the computation at training time and the computation at inference time can be computed efficiently in their recurrent representation (4). Scaffolding\n\nThe MIMO representation allows for a simplication of the scaffolding previously proposed for S4 and S4D. The reason for this is that, although the stacked dynamics matrix is diagonal, stacked matrices are dense and therefore couple the input and output components. This allows to remove a mixing layer present in the post-processing of the S4 and S4D output. III-D Linear Recurrent Unit (LRU)\n\nThe LRU model attempts to simplify previous SSM proposals by unveiling their essential components. One of the main contributions of LRU is to explicitly encode long-range memory through eigenvalues. This allows to move away from the HiPPO theory and directly use a discrete-time model together with concepts of marginal stability from control theory. Parameterization\n\nThe LRU model directly parameterizes the discrete-time dynamics (4), i.e.,\n\nA \u00af = e \u2212 e d \u200b i \u200b a \u200b g \u200b ( \u03bb 1 , \u2026 , \u03bb p ) + i \u200b d \u200b i \u200b a \u200b g \u200b ( \u03b8 1 , \u2026 , \u03b8 p ) , B \u00af = e \u03b3 \u200b \u0393 formulae-sequence \u00af \ud835\udc34 superscript \ud835\udc52 superscript \ud835\udc52 \ud835\udc51 \ud835\udc56 \ud835\udc4e \ud835\udc54 subscript \ud835\udf06 1 \u2026 subscript \ud835\udf06 \ud835\udc5d \ud835\udc56 \ud835\udc51 \ud835\udc56 \ud835\udc4e \ud835\udc54 subscript \ud835\udf03 1 \u2026 subscript \ud835\udf03 \ud835\udc5d \u00af \ud835\udc35 superscript \ud835\udc52 \ud835\udefe \u0393 \\bar{A}=e^{-e^{\\,diag(\\lambda_{1},\\dots,\\lambda_{p})}+i\\,diag(\\theta_{1},\\dots,\\theta_{p})},\\qquad\\bar{B}=e^{\\gamma}\\Gamma (10)\n\nwith the complex unit, , a dense complex-valued matrix, and . Notice that this parameterization directly represents the diagonal entries of , and therefore the eigenvalues in polar coordinates, i.e. where , is constrained to the interval by construction. This is also the first parameterization that does not have shared parameters between and . Discretization\n\nThe LRU model is the first of the SSMs that is not seen as a discretization of a continuous-time model. Instead, a discrete parametrization of is directly used. Structure and Initialization\n\nThe structure of the model is identical to S5, where a MIMO system \u2013as opposed to SISO subsystems\u2013 is considered. Given the parametrization (10), Lemma II.2 is automatically enforced by constraining the eigenvalues of to lie in the unit-disk. Hence, the initialization is directly performed in polar coordinates by defining a range for and in which and are uniformly sampled, resulting in the eigenvalues shown in Figure 2. Implementation\n\nSimilar to LRU, the model is implemented using a parallel scan algorithm [16] for both training and inference. Scaffolding\n\nThe scaffolding used in LRU is identical to the one used in S5. III-E Scan Selective Structured State Space Sequence Model (S6)\n\nThe S6 parametrization [10] introduces for the first time a linear time-varying representation of the dynamics (3). The time-varying nature of the system stems from the matrices and being functions of the input at every time-step , which the authors refer to as selectivity. Although more expressive, the time-varying representation presents computational challenges. The main contribution of this paper is to address those so the more expressive time-varying nature of the system can be exploited in practice. Parametrization\n\nSimilar to S4D, the S6 parametrization relies on a time-invariant diagonal matrix (8) as per Assumption II.1. The novelty of the S6 parametrization is that and are parameterized to be time-varying given their input-dependent nature:\n\nB k = W B \u200b u \u200b ( k ) C k = W C \u200b u \u200b ( k ) formulae-sequence subscript \ud835\udc35 \ud835\udc58 subscript \ud835\udc4a \ud835\udc35 \ud835\udc62 \ud835\udc58 subscript \ud835\udc36 \ud835\udc58 subscript \ud835\udc4a \ud835\udc36 \ud835\udc62 \ud835\udc58 B_{k}=W_{B}u(k)\\qquad C_{k}=W_{C}u(k) (11)\n\nwhere and are linear projection matrices of appropriate dimensions. Discretization\n\nSimilar to S4D, the S6 model also uses exact discretization to compute the discrete-time dynamics (4). However, in this case the time-step is itself time-varying since it is a function of the input\n\n\u0394 k = \u03c3 \u200b ( W \u0394 \u200b u \u200b ( k ) ) , A \u00af k = e \u0394 k \u200b A , B \u00af k = ( \u0394 k \u200b A ) \u2212 1 \u200b ( A \u00af k \u2212 I ) \u200b \u0394 k \u200b B k , formulae-sequence subscript \u0394 \ud835\udc58 \ud835\udf0e subscript \ud835\udc4a \u0394 \ud835\udc62 \ud835\udc58 formulae-sequence subscript \u00af \ud835\udc34 \ud835\udc58 superscript \ud835\udc52 subscript \u0394 \ud835\udc58 \ud835\udc34 subscript \u00af \ud835\udc35 \ud835\udc58 superscript subscript \u0394 \ud835\udc58 \ud835\udc34 1 subscript \u00af \ud835\udc34 \ud835\udc58 \ud835\udc3c subscript \u0394 \ud835\udc58 subscript \ud835\udc35 \ud835\udc58 \\Delta_{k}=\\sigma(W_{\\Delta}u(k)),\\quad\\bar{A}_{k}=e^{\\Delta_{k}A},\\quad\\bar{B}_{k}=(\\Delta_{k}A)^{-1}(\\bar{A}_{k}-I)\\Delta_{k}B_{k}, (12)\n\nand , with and the softplus function. Structure and Initialization\n\nSimilar to S5, the model is structured in a MIMO manner. In order to initialize the dynamic matrix , its diagonal parametrization is exploited: , ensuring that the eigenvalues lie in the negative halfplane. Due to the time-varying nature of the discretization step , the eigenvalues of the discrete-time matrices have an initialization that is input-dependent as depicted in Figure 2. However, in order to enforce Lemma II.2, the resulting eigenvalues are guaranteed to lie in the unit disk since and in (12) are positive and negative, respectively. Implementation\n\nOne of the main contributions of the work in [10] is to provide an efficient implementation of the time-varying dynamics (4) with matrices (11) and (12) both at inference and training time. In general, the time-varying nature of the S6 model renders the convolutional representation too computationally expensive for practical use. To overcome these limitations, the S6 paper presents a highly customized variation of the parallel scan algorithm [16] for both training and inference. Scaffolding\n\nAnother innovation of the work in [10] is the introduction of a new scaffolding: the Mamba scaffolding. Here, the pre-processing relies on both linear and nonlinear maps. The map of the upper signal (linear map) is a linear projection followed by a causal convolution, while the map of the lower signal (nonlinear map) is a linear projection followed by a SiLU nonlinearity. The post-processing is once again a gating function similar to previous scaffolding proposals. III-F Real-Gated Linear Recurrent Unit (RG-LRU)\n\nThe RG-LRU model is a derivative of the well-known long short-term memory (LSTM) model [27] and therefore offers a different perspective on SSM models. The RG-LRU model fuses ideas from LSTMs, LRU, and S6. Parametrization\n\nFollowing S6, RG-LRU also relies on a time-varying parametrization of the linear dynamics. However, while all previous SSM proposals rely on output feedback dynamics, the RG-LRU model introduces for the first time a state feedback model where and are not present. The and matrices are then parameterized as\n\nA \u00af k = e \u2212 c \u200b \u03d5 \u200b ( W A ) \u200b \u03c3 \u200b ( W \u0394 \u200b u \u200b ( k ) ) , B \u00af k = 1 \u2212 A k 2 \u200b \u03c3 \u200b ( W B \u200b u \u200b ( k ) ) formulae-sequence subscript \u00af \ud835\udc34 \ud835\udc58 superscript \ud835\udc52 \ud835\udc50 italic-\u03d5 subscript \ud835\udc4a \ud835\udc34 \ud835\udf0e subscript \ud835\udc4a \u0394 \ud835\udc62 \ud835\udc58 subscript \u00af \ud835\udc35 \ud835\udc58 1 superscript subscript \ud835\udc34 \ud835\udc58 2 \ud835\udf0e subscript \ud835\udc4a \ud835\udc35 \ud835\udc62 \ud835\udc58 \\bar{A}_{k}=e^{-c\\phi(W_{A})\\sigma(W_{\\Delta}u(k))},\\qquad\\bar{B}_{k}=\\sqrt{1-A_{k}^{2}}\\sigma(W_{B}u(k)) (13)\n\nwhere are linear projection matrices of appropriate dimensions, is a scalar constant,444The paper empirically found that works best for language modelling., is the softplus function, and is the sigmoid function. The operation is computed element-wise for each entry of . Discretization\n\nSimilar to the LRU model, the RG-LRU model does not rely on a continuous-time representation and instead directly parametrizes the discrete matrices . Structure and Initialization\n\nSimilar to LRU, the RG-LRU model is structured as a MIMO system. Taking inspiration from LSTMs, this models assumes the state dimension to be equal to the input dimension, i.e., . The linear projection matrices are initialized with standard initialization methods, e.g. Glorot [14], resulting in the eigenvalues shown in Figure 2. Given the parameterization of in (13), its eigenvalues are restricted to the unit disk by construction. Implementation\n\nDue to the time-varying nature of the RG-LRU model, it faces the same challenges as the S6 model. Therefore, it also uses a customized variation of the parallel scan algorithm [16] to compute the outputs at both training and inference time. Scaffolding\n\nThe RG-LRU model uses the same scaffolding as the S6 model, Mamba. However, this work also introduces two additional task-specific scaffoldings around the basic Mamba scaffolding that are tailored to language modelling: Hawk and Griffin [21, Section 2]. IV Performance in practice\n\nIn this section, we evaluate the SSM proposals reviewed in Section III on the long-range arena (LRA) benchmark [2]. This benchmark evaluates the models in terms of their reasoning ability and their handling of diverse data types, which is one of the strengths of SSMs. We first introduce the benchmark, before presenting the empirical evaluation. IV-A Long-Range Arena (LRA) Benchmark\n\nThe goal of the LRA benchmark is to evaluate the reasoning capabilities of sequence models in diverse ways. The benchmark consists of different tasks, which we summarize in the following. For more details on the benchmark and the individual tasks, we refer to [2]. List Operations (ListOps)\n\nThis task evaluates a model\u2019s ability to capture hierarchical dependencies over long contexts. The goal is to predict the result of a mathematical operation consisting of nested mean, median, max, and min operations,555For instance, The task is a ten-way classification task with maximal input lengths of 2k. Text Classification (Text)\n\nThis task evaluates a model\u2019s ability to capture the tone of long tokenized texts. The dataset consists of IMDb movie reviews, which need to be classified as negative or positive in tone. The task is a binary classification task with maximal input lengths of 4k. Document Retrieval (Retrieval)\n\nThis task evaluates a model\u2019s ability to compress long sequences into representations that are suitable for similarity matching. The dataset consists of tokenized papers published by the American Academy of Neurology (AAN), which need to be classified in having a citation link or not. The task is a binary classification task with maximal input lengths of 8k. Image Classification (Image)\n\nThis task evaluates a model\u2019s ability to learn 2D spatial relations from a 1D vector. The dataset consists of vectorized images, which depict one of ten possible classes, e.g. a horse or a car. The task is a ten-way classification task with maximal input lengths of 1k. Long-Range Spacial Dependency\n\nThis task evaluates a model\u2019s ability to learn spacial dependencies in a vectorized image. The dataset consists of images, which depict two circles and multiple dashed paths. The goal is to evaluate whether the two circles are connected by any of the present paths or not. The task is therefore a binary classification task and is divided into two subtasks, which only differ in the size of the image. The first subtask has inputs of length 2k and we will refer to it as Pathfinder; the second subtask has a maximal input length of 16k and we will refer to it as Path-X. IV-B Empirical Evaluation of SSM Proposals\n\nThe empirical performance of the reviewed SSM proposals, the Transformer [1], and random guessing are reported in Table II. We include the performance of the Transformer as a baseline, since they are the dominant architecture in large language models and sequence modelling. For S4, S4D, S5, and LRU we report the performance of the best variant from the original papers in order to present the most competitive results. Other variants of these models might perform better on tasks not included in the LRA benchmark; for more details on these variants we refer to the original papers. Since performance on the LRA benchmark of S6 and RG-LRU have not been reported in the literature, we provide the results of our own implementation of these architectures, which we make available here.666https://github.com/jsie7/ssm-benchmark The hyperparameters of the models and training details of our implementation are stated in the public code repository. On the LRA benchmark, the LTI-based models S4, S4D, S5, LRU outperform the LTV-based models S6, RG-LRU and the Transformer. From a control theoretic perspective this is surprising, since a general LTV definition encompasses LTI systems as a special case, i.e., a LTV system should perform at least as well as a LTI system. However, this is not the case for the particular time-varying parametrization of S6 or RG-LRU, since e.g. cannot be achieved. We attempted to improve the performance of the LTV-based models by changing the initialization of S6 and RG-LRU and forcing the input-dependent eigenvalues of closer to marginal stability according to Lemma II.2. However, this resulted in both models to perform considerably worse or fail to learn anything meaningful at all. While marginally stable eigenvalues appear to be important for the LTI-based models, the same is not true for LTV-based models. To date, this behavior is not well understood. Lastly, even though the LTV-based models are closely related to the Transformer [28], they generally perform better on the LRA benchmark. V Conclusion and Future Opportunities\n\nIn this paper, we have provided an overview of state-of-the-art state space models (SSM) and explored their features from a control theoretic perspective. In doing this, we highlighted the many connections to standard control theoretic concepts such as the connection between memory and marginal-stability. Additionally, we compared the reviewed SSMs on the long-range arena (LRA) benchmark, finding that the more recent LTV-based SSMs perform worse than their LTI-based counterparts. From a control theoretic perspective, this raises many interesting research questions concerning a LTV parametrization that attains the same performance as the LTI models, as well as a deeper understanding on the role of the eigenvalues in the LTV-based models. SSMs, particularly the LTV versions, rely on dynamics where the dynamic matrices depend on the input (excitation) to the system. However, in the SSM literature the theoretical properties arising from these dynamics remains poorly understood. The evident connections between SSMs and linear-system theory give rise to ample opportunities to provide explainability to large foundational models. Moreover, as seen with the LRU model, control theoretic insights have the potential to inform better designs for SSMs.",
    "ssmfundmodel-2": "References\n\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, \u201cAttention is All you Need,\u201d in Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-3": "30. Curran Associates, Inc., 2017. [2] Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler, \u201cLong Range Arena : A Benchmark for Efficient Transformers,\u201d in International Conference on Learning Representations (ICLR), 2021.",
    "ssmfundmodel-4": "[3] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, \u201cOn the dangers of stochastic parrots: Can language models be too big?\u201d in Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 2021, pp.",
    "ssmfundmodel-5": "610\u2013623. [4] I. Beltagy, M.",
    "ssmfundmodel-6": "E. Peters, and A. Cohan, \u201cLongformer: The Long-Document Transformer,\u201d 2020.",
    "ssmfundmodel-7": "[Online]. Available: https://arxiv.org/abs/2004.05150\n\n[5] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang et al., \u201cBig bird: Transformers for longer sequences,\u201d Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-8": "33, 2020. [6] N. Kitaev, L. Kaiser, and A. Levskaya, \u201cReformer: The Efficient Transformer,\u201d in International Conference on Learning Representations, 2020.",
    "ssmfundmodel-9": "[7] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller, \u201cRethinking Attention with Performers,\u201d in International Conference on Learning Representations, 2021.",
    "ssmfundmodel-10": "[8] J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans, \u201cAxial Attention in Multidimensional Transformers,\u201d 2019.",
    "ssmfundmodel-11": "[Online]. Available: https://arxiv.org/abs/1912.12180\n\n[9] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016. [10] A. Gu and T. Dao, \u201cMamba: Linear-Time Sequence Modeling with Selective State Spaces,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2312.00752\n\n[11] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. R\u00e9, \u201cCombining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers,\u201d in Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-12": "34. Curran Associates, Inc., 2021, pp. 572\u2013585. [12] A. Gu, K. Goel, and C. R\u00e9, \u201cEfficiently Modeling Long Sequences with Structured State Spaces,\u201d in The International Conference on Learning Representations (ICLR), 2022.",
    "ssmfundmodel-13": "[13] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. R\u00e9, \u201cHiPPO: Recurrent Memory with Optimal Polynomial Projections,\u201d in Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-14": "33. Curran Associates, Inc., 2020, pp. 1474\u20131487. [14] X. Glorot and Y. Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks,\u201d in Proceedings of the thirteenth international conference on artificial intelligence and statistics.",
    "ssmfundmodel-15": "JMLR Workshop and Conference Proceedings, 2010, pp. 249\u2013256. [15] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, \u201cSelf-normalizing neural networks,\u201d Advances in neural information processing systems, vol.",
    "ssmfundmodel-16": "30, 2017.",
    "ssmfundmodel-17": "[16] G. E. Blelloch, \u201cPrefix sums and their applications,\u201d 1990.",
    "ssmfundmodel-18": "[17] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d 2014. [Online]. Available: https://arxiv.org/abs/1412.6980\n\n[18] A. Gu, A. Gupta, K. Goel, and C. R\u00e9, \u201cOn the Parameterization and Initialization of Diagonal State Space Models,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2206.11893\n\n[19] J. T. Smith, A. Warrington, and S. Linderman, \u201cSimplified State Space Layers for Sequence Modeling,\u201d in The Eleventh International Conference on Learning Representations, 2023.",
    "ssmfundmodel-19": "[20] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De, \u201cResurrecting Recurrent Neural Networks for Long Sequences,\u201d in Proceedings of the 40th International Conference on Machine Learning, vol. 202. PMLR, 23\u201329 Jul 2023, pp. 26 670\u201326 698. [21] S. De, S. L. Smith, A. Fernando, A. Botev, G. Cristian-Muraru, A. Gu, R. Haroun, L. Berrada, Y. Chen, S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y. W. Teh, R. Pascanu, N.",
    "ssmfundmodel-20": "D. Freitas, and C. Gulcehre, \u201cGriffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models,\u201d 2024.",
    "ssmfundmodel-21": "[Online]. Available: https://arxiv.org/abs/2402.19427\n\n[22] H. Liu, Z. Dai, D. So, and Q. V. Le, \u201cPay Attention to MLPs,\u201d in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [23] K. Goel, A. Gu, C. Donahue, and C. R\u00e9, \u201cIt\u2019s Raw! Audio Generation with State-Space Models,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2202.09729\n\n[24] J. Sherman and W. J. Morrison, \u201cAdjustment of an Inverse Matrix Corresponding to a Change in One Element of a Given Matrix,\u201d The Annals of Mathematical Statistics, vol.",
    "ssmfundmodel-22": "21, no. 1, pp. 124 \u2013 127, 1950. [25] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. R\u00e9, \u201cHungry Hungry Hippos: Towards Language Modeling with State Space Models,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2212.14052\n\n[26] A. Gupta, A. Gu, and J. Berant, \u201cDiagonal state spaces are as effective as structured state spaces,\u201d in Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-23": "35. Curran Associates, Inc., 2022, pp. 22 982\u201322 994. [27] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol.",
    "ssmfundmodel-24": "9, no. 8, pp. 1735\u20131780, 1997. [28] A. Ali, I. Zimerman, and L. Wolf, \u201cThe Hidden Attention of Mamba Models,\u201d 2024. [Online]. Available: https://arxiv.org/abs/2403.01590\n\n\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Fri Apr 5 16:13:35 2024 by LaTeXML"
}