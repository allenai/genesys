{
    "ssmfundmodel-0": "# State Space Models as Foundation Models: A Control Theoretic Overview \n\nCarmen Amo Alonso*, Jerome Sieber*, and Melanie N. Zeilinger\n\n\n#### Abstract\n\nIn recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences. Index Terms-Machine learning, Linear systems, Timevarying systems. ## I. INTRODUCTION\n\nRecently, foundation models have become central to the field of artificial intelligence. These models are large-scale learning models that are initially pretrained on extensive datasets, and subsequently fine-tuned for specific tasks. The term foundation models highlights these models' capability to learn and effectively generalize across a wide array of modalities, encompassing language, audio, images, video, genomics, and more. At their core, the predominant architecture for foundation models is the Transformer [1]. This architecture, based on the attention mechanism, allows to efficiently process information and model global dependencies in complex data; but it suffers from two main limitations. One is computational complexity: it requires the complete sequence to be fed into the model every time an output is generated, which results in poor scalability with the time horizon window ${ }^{1}$ and therefore poor performance in long context tasks [2]. The other limitation is explainability: despite its simple mathematical representation, it is currently not possible to interpret or understand the choice of outputs made by the Transformer [3]. Efforts to address the scalability challenges of Transformers have led to various architectural variants that still leverage the merits of the\n\n[^0]attention mechanism. Examples of such variants are the Longformer [4], BigBird [5], the Reformer [6], the Performer [7], and approaches leveraging Axial Attention [8]. However, despite extensive research on these fronts, the proposed solutions often degrade the inherent merits of the architecture or fail to perform well in practice [2]. A recent and promising research avenue proposes to fully replace the attention mechanism with a different representation based on State Space Models (SSM). The advantage of the SSM representation lies in its recurrent nature, where only the latest input has to be passed to the model since the state is able to capture information about past inputs. Moreover, due to their mathematical structure, they are amenable to computationally efficient training and inferencein contrast to their predecessors, recurrent neural networks (RNNs) [9]. This new family of SSM-based architectures has been shown to beat Transformers in long-context tasks such as the Long Range Arena (LRA) benchmark [2], and recent proposals such as Mamba [10] exhibit performance and computational efficiency superior to state-of-the-art Transformers on long-context tasks. These results highlight the potential of SSMs to overcome many of the current limitations of Transformers. Although SSMs show great promise to serve as foundation models, most of the existing literature on SSMs focuses on providing performant architectures and efficient implementations. Despite the clear connection with control theory, in particular linear systems theory, to date a principled understanding of these models is lacking, and most design choices are motivated from an empirical performance rather than a systematic system theoretical viewpoint. There is large potential in leveraging existing system theoretic results and analysis to complement current implementations and enhance explainability, design and performance. Towards this goal, the aim of this paper is to provide an overview of state-of-the-art SSMs from a control theoretical perspective. In Section $\\Pi$, we provide an overview of the essential components and considerations in SSMs. In Section III we review the most relevant SSM proposals to date. Since these models were primarily motivated by their ability to handle long contexts, we present the first performance comparison to date on the LRA benchmark in Section IV. Lastly, we end in Section V with concluding remarks and open research questions that could help advance SSMs and cross-pollinate the fields of foundation models and systems and control theory. ## II. State Space Models\n\nWe first present a generic language modelling task to define the learning goal of a foundation model. Then, we give an overview of the state space model architecture, mathematical structure, and computational considerations that guide the SSMs introduced in the literature. ## A. Learning setup\n\nA foundation model, such as those used in language modeling, can be seen as a map between input and output signals, i.e.,\n\n$$\ny(k)=f(u(k), \\ldots, u(k-T) ; \\theta)\n$$\n\nwhere at each time $k$, the output $y(k)$ is produced after evaluating an input signal of length $k-T$, i.e., $u(k), \\ldots, u(k-$ $T)$, and a set of parameters $\\theta$. The parameters $\\theta$ are task dependent, and can be fine-tuned accordingly. Since the search space of general $f(\\cdot ; \\theta)$ is too broad, different parameterizations of $f(\\cdot ; \\theta)$ can be used to render the problem tractable. For instance, the model $f(\\cdot ; \\theta)$ can consist of multiple stacked models like e.g. the Transformer or more recently SSMs. The architectural choice of $f(\\cdot ; \\theta)$ is a fundamental factor in determining the success of the model at effectively learning structure from data. The goal of a foundation model used as large language model is to learn a compressed representation of structure present in language in order to perform tasks like machine translation or human-level conversations (e.g. ChatGPT). To learn such a representation the parameterized model $f(\\cdot ; \\theta)$ is presented with input-output pairs $(u(k), y(k)) \\forall k$, where $\\theta$ represents the parameters. The parameters $\\theta$ are then iteratively updated to minimize a loss function $\\mathscr{L}(\\cdot)$, i.e., iteratively solving the following optimization problem\n\n$$\n\\min _{\\theta} \\mathscr{L}(y-f(u ; \\theta))\n$$\n\nFor a language model the inputs $u$ are tokenized ${ }^{2}$ sentences and the outputs $y$ are a shifted version of the same inputs, i.e., an auto-regressive setup. ## B. Parametrization\n\nLet us consider the following continuous-time linear system with dynamics\n\n$$\n\\begin{aligned}\n& \\dot{x}(t)=A x(t)+B u(t) \\\\\n& y(t)=C x(t)+D u(t)\n\\end{aligned}\n$$\n\nwhere $x \\in \\mathbb{C}^{p}$ represents the complex-valued state, $u, y \\in \\mathbb{R}^{q}$ are the input and the output, respectively, and $t$ denotes the continuous-time index. We note that the input fed into the system denoted as $u$ is not a control input; it is seen as an exogenous input exciting the system (3). This choice of notation is made to maintain consistency with the corresponding literature. $A, B, C, D$ are complex-valued matrices of appropriate dimensions and in representation (3), these matrices\n\n[^1]are assumed to be time-invariant. When considering their time-varying version, a time sub-index would be appended, i.e., $A_{t}, B_{t}, C_{t}, D_{t}$. In the SSM literature, system (3) is used as a black-box representation in a foundation model. Here, the exogenous input $u(t)$ represents a signal or input token fed into the model at a given time $t$. The state $x(t)$ represents the hidden state that stores the relevant information about the current and previous inputs up to time $t$, and $y(t)$ is the output of the model at time $t$. In a learning setup, the matrices $A, B, C, D$ are parameters, which are commonly learned via stochastic gradient descent. Since computational efficiency and initialization are essential aspects in this framework, the dynamic matrix $A$ is often assumed to have a particular structure. As such, SSMs are often referred to as Structured SSMs. Assumption 2.1: The dynamic matrix in dynamics (3) has a diagonal structure, i.e., $A=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)$ with $\\lambda_{i} \\in \\mathbb{C} \\forall i$. Although initial proposals [11], [12] deviate slightly from Assumption 2.1, most of the Structured SSMs literature assumes a diagonal $A$ matrix. Specific choices will be discussed in Section III\n\n## C. Discretization\n\nIn order to implement a SSM, a discrete-time version of system (3) is used. Hence, the implementation of system (3) in discrete-time is\n\n$$\n\\begin{aligned}\nx(k+1) & =\\bar{A} x(k)+\\bar{B} u(k) \\\\\ny(k) & =\\bar{C} x(k)+\\bar{D} u(k)\n\\end{aligned}\n$$\n\nwhere $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ are the discrete-time dynamic matrices discretized with time-step $\\Delta \\in \\mathbb{R}$, possibly with complexvalued components, and $k$ denotes the discrete-time index. The choice of discretization scheme chosen varies widely among the proposed models in the SSM literature, and an overview is presented in Section III\nWe note that it is also possible to directly start from a discrete-time model as in equation (4), oblivious to its continuous-time representation (3). However, in most of the SSM literature, a continuous-time view of the dynamics is preferred in order to better motivate the choice of initialization for the dynamical matrices[13]. ## D. Structure and Initialization\n\nSince the dynamics (3) are being learned via gradient descent, initialization of the parameters was found to be of crucial importance. In particular, the initial values of matrix $A$ have a significant impact on the performance after training: on a simple classification task, performance increases from $67 \\%$ when $A$ is randomly initialized, to $80 \\%$ when $A$ is initialized using a principled strategy [12, Section 4.4]. Different strategies and parametrizations have been proposed in order to achieve a successful initialization, i.e. an initialization that results in the state $x(k)$ being able to capture the recent history of the inputs $u(k), \\ldots, u(k-T)$ for some time horizon $T$. This property is referred to as memory in the standard SSM literature. As is well-known in control\ntheory, the memory of system (4) is directly linked to the eigenvalues of matrix $A$. Lemma 2.2: (Informal) A dynamical system with dynamics (4) has long-range memory, i.e., captures information from past inputs, if the eigenvalues of $A$ are inside the unit circle and very close to the unit circumference, i.e. $|\\operatorname{eig}(A)| \\leq 1$ and $|\\operatorname{eig}(A)| \\approx 1 \\forall \\operatorname{eig}(A)$. Hence, the various initialization schemes presented in the SSM literature aim to ensure that the modulo of the eigenvalues of the learned $A$ matrix is approximately equal to (but not bigger than) 1 . For the initialization of the other matrices, i.e., $B, C$, and $D$, standard initialization methods are used, e.g., Glorot [14] or LeCun [15], which essentially draw the initial values from a transformed uniform or normal distribution. Therefore, we omit the initialization details of $B, C$, and $D$ in the following and refer the reader to the original papers [14], [15]. ## E. Implementation\n\nOne of the major challenges addressed in the SSM literature is how to efficiently learn (training time) and deploy (inference time) the recurrence (4). At inference time, a causal representation is needed since the model does not have access to excitation inputs beyond the current time step. For this reason, the recurrent representation (4) is directly used starting with an initial excitation $u(1)$ and zero initial state $x(1)=0$. In order to speed up this process, parallel scans algorithms [16] are used that efficiently compute the recurrence by computing each output component in parallel and caching intermediate results. During training, it is possible (and desirable) to use a noncausal representation since input-output pairs $(u(k), y(k))$ are available for all $k$. Different techniques have been proposed in the literature. Some of the architectures can take advantage of parallel scan algorithms and use the recurrent representation from equation (4). Some other architectures rely on the convolutional representation of system (4), i.e.,\n\n$$\ny(k)=\\sum_{\\tau=0}^{k} \\bar{C} \\bar{A}^{k-\\tau} \\bar{B} u(\\tau)\n$$\n\nThis convolutional representation allows for faster learning because the complete input sequence $u(k) \\forall k$ can be passed through the model in one step. In terms of learning algorithms, SSM models are commonly trained using a standard stochastic gradient descent variation, i.e. Adam [17], and backpropagation [9]. Additionally, they can utilize the same heuristic methods to improve training as other deep-learning models, e.g., dropout or normalization [9]. ## F. Scaffolding and Layers\n\nAlthough learning the dynamics in equation (3) is a major focus of SSMs, these dynamics are not simply implemented in isolation. In fact, pre-processing of the input $u$ and postprocessing of the output $y$ is necessary to ensure good performance. In this paper, we refer to the algebraic operations of pre- and post-processing as the scaffolding surrounding\nA\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-3.jpg?height=776&width=874&top_left_y=211&top_left_x=1078)\n\nFig. 1: A. General scaffolding of a SSM. The dynamical model (4) is represented in green. The input to the SSM is pre-processed and forked off in a skip connection (lower signal). The nature of the pre-processing map (linear or nonlinear) depends on the specific scaffolding. The output of the recursion is then post-processed with a nonlinear gate. B. Overall architecture of a SSM. Each of the SSMs including its scaffolding (Fig. 1.A.) is structured in a layered fashion, where the output from one layer is the input to the next. the SSM computation in dynamics (4). A general overview of the architecture used in SSMs is provided in Figure 1. A collection of different scaffolding choices have been proposed in the literature, ranging from standard multilayer perceptron (MLP) choices to gating operations, as defined in Definition 2.3. In general, a linear or nonlinear map is performed on the input $\\bar{u}$ before it is fed into system (4). Once the output $y$ has been computed, a gating operation is generally performed to control the flow of information from the input $\\tilde{u}$ to the output $\\tilde{y}$. Intuitively, the gate $g(\\tilde{y}, \\tilde{u})$ controls which outputs $\\tilde{y}$ are set to zero based on the inputs $\\tilde{u}$ via the softmax operation. Definition 2.3: Given two vectors $x_{1}, x_{2} \\in \\mathbb{R}^{p}$, a gating operation is defined as $g\\left(x_{1}, x_{2}\\right):=x_{1} \\odot \\sigma\\left(W x_{2}\\right)$, where $W \\in \\mathbb{R}^{p \\times p}$, $\\odot$ is the element-wise multiplication, and $\\sigma$ is the softmax operation ${ }^{3}$\n\nAs is common practice in deep learning, several layers of SSMs (dynamics (3) and accompanying scaffolding) are stacked together, where each of them processes the output of the previous layer as its input, which is then fed into the next layer. This is possible since input $y$ and output $u$ are of the same dimension $\\mathbb{R}^{q}$. For example on smaller tasks like e.g. the LRA benchmark [2], a SSM is composed of 6 structurally-identical layers (with different dynamic matri-\n\n[^2]ces), and the size of the systems ranges in $p \\in[64,512], q \\in$ [32, 1024]. For language modelling the number of layers and system size can be significantly larger.",
    "ssmfundmodel-1": "It is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. ## III. REVIEW OF EXISTING METHODS\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section $\\Pi$ We also provide a summary of their main characteristics in Table I. ## A. Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. a) Parametrization: The S4 model starts from a continuous time model (3), where the structure imposed on matrix $A$ is\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)+r s^{\\star}\n$$\n\nwith $\\lambda_{i} \\in \\mathbb{C} \\forall i$, and $r, s \\in \\mathbb{C}^{p}$. This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix $A_{C L}=A+B K$. b) Discretization: The discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1}\\left(I+\\frac{\\Delta}{2} A\\right), \\quad \\bar{B}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1} \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. Note that this choice of discretization method couples the parameterizations of $\\bar{A}$ and $\\bar{B}$ via the discretization step $\\Delta$, which is a common feature of most SSMs. c) Structure and Initialization: The model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) $u_{i}$ for $i=1, \\ldots, q$ is fed into a separate system (4), each producing a scalar output $y_{j}$ with $j=1, \\ldots, q$. Each dynamics matrix $A$ for each of the $q$ SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma 2.2), the follow up work SaShiMi [23] enforces $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\in \\mathbb{R}^{-} \\forall i$ to ensure stability. d) Implementation: At training time, a convolutional representation (5) is used. For efficient computation, the structure of $\\bar{A}$ (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model 4 is directly used. e) Scaffolding: Initially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, $H 3$ [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A.",
    "ssmfundmodel-2": "The post-processing remains a gating function. ## B. Diagonal Structured State Space Sequence Model (S4D)\n\nThe initially proposed Diagonal State Space (DSS) [26] model and its enhancement S4D [18] build upon the S4 model. They simplify the structure of the dynamics matrices by introducing for the first time Assumption 2.1, which results in computational improvements. a) Parametrization: The main contribution of the S4D paper is the introduction of a new, more efficient, structure of matrix $A$ consistent with Assumption 2.1\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)\n$$\n\nb) Discretization: The discrete-time version (4) is computed by applying exact discretization to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=e^{\\Delta A}, \\quad \\bar{B}=(\\Delta A)^{-1}(\\bar{A}-I) \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. c) Structure and Initialization: The SISO structure used in S4D is the same one as in S4. Initalization of S4D is also done using HiPPO theory, with the added insight that the resulting matrix can be diagonalized for added computational efficiency. Similar to SaShiMi [23], the eigenvalues of $A$ used for initialization are constrained to lie in the negative halfplane. This initialization results in the eigenvalues shown in Figure 2\nd) Implementation: Similar to S4, a convolutional representation (5) is used at training time, and a recurrent representation (4) at inference time. Given the diagonal structure of matrix $\\bar{A}$, discretization (9) can be computed efficiently. e) Scaffolding: The scaffolding of S4D is identical to the one used in S4. ## C. Simplified Structured State Space Sequence Model (S5)\n\nThe S5 parametrization [19] presents a simplification of the previously proposed S4D and leverages the concept of multiple input multiple output (MIMO) systems (as opposed to SISO) to simplify the architectural components and enhance computation. a) Parametrization: The parametrization used is identical to S4D. b) Discretization: S 5 is amenable to both discretizations proposed in S4 and S4D: bilinear (7) and exact 9 . | Model | Features |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n|  | Parametrization | Discretization | Structure | Implementation | Scaffolding |\n| S4 [12] | LTI | Bilinear | SISO | Convolution and Recurrence | MLP / H3 |\n| S4D [18] | LTI | Exact | SISO | Convolution and Recurrence | MLP / H3 |\n| S5 [19] | LTI | Exact / Bilinear | MIMO | Parallel Scan | MLP / H3 |\n| LRU [20] | LTI | None | MIMO | Parallel Scan | MLP / H3 |\n| S6 [10] | LTV | Exact | MIMO | Custom Parallel Scan | Mamba |\n| RG-LRU [21] | LTV | None | MIMO | Custom Parallel Scan | Mamba / Hawk / Griffin |\n\nTABLE I: Overview of the model features for the different SSM models considered. Accronyms used are as follows: Linear Time-Invariant (LTI), Linear Time-Varying (LTV), Single Input Single Output (SISO), Multiple Input Multiple Output (MIMO). Details on the scaffolding can be found in MLP [22], H3 [13], Mamba [10], Hawk and Griffin [21]. c) Structure and Initialization: The main contribution of the S5 model is the introduction of a MIMO interpretation of the previously proposed models, which leads to significant computational enhancements. In particular, the full input vector $u \\in \\mathbb{R}^{q}$ is fed into a single MIMO system (4) (of bigger dimension) as opposed to $q$ SISO scalar subsystems (of smaller dimension). This is achieved by stacking the subsystem matrices $\\bar{A}, \\bar{B}, \\bar{C}$ used in S 4 and S4D. The matrix $A$ is again initialized using HiPPO theory and results in the same initial eigenvalues as S4D (Figure 22. d) Implementation: The MIMO structure together with the diagonal parameterization of $\\bar{A}$ allows for parallel computation of the individual output components from the input components via a parallel scan algorithm [16]. As a result, both the computation at training time and the computation at inference time can be computed efficiently in their recurrent representation (4). e) Scaffolding: The MIMO representation allows for a simplication of the scaffolding previously proposed for S4 and S4D. The reason for this is that, although the stacked dynamics matrix $\\bar{A}$ is diagonal, stacked matrices $\\bar{B}, \\bar{C}$ are dense and therefore couple the input and output components.",
    "ssmfundmodel-3": "This allows to remove a mixing layer present in the postprocessing of the S4 and S4D output. ## D. Linear Recurrent Unit (LRU)\n\nThe LRU model attempts to simplify previous SSM proposals by unveiling their essential components. One of the main contributions of LRU is to explicitly encode long-range memory through eigenvalues. This allows to move away from the HiPPO theory and directly use a discrete-time model together with concepts of marginal stability from control theory. a) Parameterization: The LRU model directly parameterizes the discrete-time dynamics (4), i.e.,\n\n$$\n\\bar{A}=e^{-e^{\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)}+i \\operatorname{idiag}\\left(\\theta_{1}, \\ldots, \\theta_{p}\\right)}, \\quad \\bar{B}=e^{\\gamma} \\Gamma\n$$\n\nwith $i$ the complex unit, $\\lambda_{j}, \\theta_{j} \\in \\mathbb{R} \\forall j=1, \\ldots, p, \\Gamma \\in \\mathbb{C}^{p \\times q}$ a dense complex-valued matrix, and $\\gamma \\in \\mathbb{R}$. Notice that this parameterization directly represents the diagonal entries of $\\bar{A}$, and therefore the eigenvalues in polar coordinates, i.e. $a_{j}=$ $r_{j}+i \\theta_{j}$ where $r_{j}=e^{-e^{\\lambda_{j}}}$, is constrained to the interval $[0,1]$ by construction. This is also the first parameterization that does not have shared parameters between $\\bar{A}$ and $\\bar{B}$. b) Discretization: The LRU model is the first of the SSMs that is not seen as a discretization of a continuous-time model. Instead, a discrete parametrization of $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ is directly used. c) Structure and Initialization: The structure of the model is identical to S5, where a MIMO system -as opposed to $q$ SISO subsystems- is considered. Given the parametrization 10p, Lemma 2.2 is automatically enforced by constraining the eigenvalues of $\\bar{A}$ to lie in the unitdisk. Hence, the initialization is directly performed in polar coordinates by defining a range for $r$ and $\\theta$ in which $r$ and $\\theta$ are uniformly sampled, resulting in the eigenvalues shown in Figure 2\nd) Implementation: Similar to LRU, the model is implemented using a parallel scan algorithm [16] for both training and inference. e) Scaffolding: The scaffolding used in LRU is identical to the one used in S 5 . ## E. Scan Selective Structured State Space Sequence Model (S6)\n\nThe S6 parametrization [10] introduces for the first time a linear time-varying representation of the dynamics (3). The time-varying nature of the system stems from the matrices $\\bar{A}_{k}, \\bar{B}_{k}$ and $\\bar{C}_{k}$ being functions of the input $u(k)$ at every timestep $k$, which the authors refer to as selectivity. Although more expressive, the time-varying representation presents computational challenges. The main contribution of this paper is to address those so the more expressive time-varying nature of the system can be exploited in practice. a) Parametrization: Similar to S4D, the S6 parametrization relies on a time-invariant diagonal $A$ matrix (8) as per Assumption 2.1. The novelty of the S6 parametrization is that $B$ and $C$ are parameterized to be time-varying given their input-dependent nature:\n\n$$\nB_{k}=W_{B} u(k) \\quad C_{k}=W_{C} u(k)\n$$\n\nwhere $W_{B}$ and $W_{C}$ are linear projection matrices of appropriate dimensions. ![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-6.jpg?height=467&width=1781&top_left_y=162&top_left_x=172)\n\nFig. 2: Complex plane representation of the unit disk and the eigenvalues of discrete-time dynamics matrix $\\bar{A}$ (4) resulting from the initialization method in each of the models S4, S4D, S5, LRU, S6, and RG-LRU. Since the initialization of S6 and RG-LRU are input dependent, we plot the initialization for two sample inputs (blue and orange). b) Discretization: Similar to S4D, the S6 model also uses exact discretization to compute the discrete-time dynamics (4). However, in this case the time-step $\\Delta_{k}$ is itself time-varying since it is a function of the input\n$\\Delta_{k}=\\sigma\\left(W_{\\Delta} u(k)\\right), \\quad \\bar{A}_{k}=e^{\\Delta_{k} A}, \\quad \\bar{B}_{k}=\\left(\\Delta_{k} A\\right)^{-1}\\left(\\bar{A}_{k}-I\\right) \\Delta_{k} B_{k}$,\n$\\bar{C}_{k}=C_{k}$ and $\\bar{D}_{k}=D_{k}$, with $W_{\\Delta} \\in \\mathbb{R}^{1 \\times q}$ and $\\sigma(\\cdot)$ the softplus function. c) Structure and Initialization: Similar to S5, the model is structured in a MIMO manner. In order to initialize the dynamic matrix $A$, its diagonal parametrization is exploited: $\\lambda_{i}=-i \\forall i=1, \\ldots, p$, ensuring that the eigenvalues lie in the negative halfplane. Due to the time-varying nature of the discretization step $\\Delta_{k}$, the eigenvalues of the discrete-time matrices $\\bar{A}_{k}$ have an initialization that is input-dependent as depicted in Figure 2 However, in order to enforce Lemma 2.2 the resulting eigenvalues are guaranteed to lie in the unit disk since $\\Delta_{k}$ and $A$ in (12) are positive and negative, respectively. d) Implementation: One of the main contributions of the work in [10] is to provide an efficient implementation of the time-varying dynamics (4) with matrices (11) and 12 both at inference and training time. In general, the timevarying nature of the S 6 model renders the convolutional representation too computationally expensive for practical use. To overcome these limitations, the S6 paper presents a highly customized variation of the parallel scan algorithm [16] for both training and inference. e) Scaffolding: Another innovation of the work in [10] is the introduction of a new scaffolding: the Mamba scaffolding. Here, the pre-processing relies on both linear and nonlinear maps. The map of the upper signal (linear map) is a linear projection followed by a causal convolution, while the map of the lower signal (nonlinear map) is a linear projection followed by a SiLU nonlinearity. The post-processing is once again a gating function similar to previous scaffolding proposals. ## F. Real-Gated Linear Recurrent Unit (RG-LRU)\n\nThe RG-LRU model is a derivative of the well-known long short-term memory (LSTM) model [27] and therefore offers a different perspective on SSM models. The RG-LRU model fuses ideas from LSTMs, LRU, and S6. a) Parametrization: Following S6, RG-LRU also relies on a time-varying parametrization of the linear dynamics. However, while all previous SSM proposals rely on output feedback dynamics, the RG-LRU model introduces for the first time a state feedback model where $C$ and $D$ are not present. The $A$ and $B$ matrices are then parameterized as\n\n$$\n\\bar{A}_{k}=e^{-c \\phi\\left(W_{A}\\right) \\sigma\\left(W_{\\Delta} u(k)\\right)}, \\quad \\bar{B}_{k}=\\sqrt{1-A_{k}^{2}} \\sigma\\left(W_{B} u(k)\\right)\n$$\n\nwhere $W_{\\Delta}, W_{A}, W_{B}$ are linear projection matrices of appropriate dimensions, $c \\in \\mathbb{R}$ is a scalar constant ${ }_{4}^{4} \\phi(\\cdot)$ is the softplus function, and $\\sigma(\\cdot)$ is the sigmoid function. The operation $\\sqrt{1-A_{k}^{2}}$ is computed element-wise for each entry of $A_{k}$. b) Discretization: Similar to the LRU model, the RGLRU model does not rely on a continuous-time representation and instead directly parametrizes the discrete matrices $\\bar{A}_{k}, \\bar{B}_{k}$. c) Structure and Initialization: Similar to LRU, the RG-LRU model is structured as a MIMO system. Taking inspiration from LSTMs, this models assumes the state dimension to be equal to the input dimension, i.e., $p=q$. The linear projection matrices $W_{\\Delta}, W_{A}, W_{B}$ are initialized with standard initialization methods, e.g. Glorot [14], resulting in the eigenvalues shown in Figure 2 Given the parameterization of $\\bar{A}_{k}$ in 13), its eigenvalues are restricted to the unit disk by construction. d) Implementation: Due to the time-varying nature of the RG-LRU model, it faces the same challenges as the S6 model. Therefore, it also uses a customized variation of the parallel scan algorithm [16] to compute the outputs at both training and inference time. e) Scaffolding: The RG-LRU model uses the same scaffolding as the S6 model, Mamba. However, this work also introduces two additional task-specific scaffoldings around the basic Mamba scaffolding that are tailored to language modelling: Hawk and Griffin [21, Section 2]. [^3]| Model | LRA Task [\\%] |  |  |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | ListOps | Text | Retrieval | Image | Pathfinder | Path-X | avg. |\n| Random | 10.00 | 50.00 | 50.00 | 10.00 | 50.00 | 50.00 | 36.67 |\n| Transformer [2] (paper results) | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | FAIL | 53.66 |\n| S4 [12] (paper results) | 59.60 | 86.82 | 90.90 | 88.65 | 94.20 | 96.35 | 86.09 |\n| S4D [18] (paper results) | 60.52 | 87.34 | 91.09 | 88.19 | 93.96 | 92.80 | 85.65 |\n| S5 [19] (paper results) | $\\mathbf{6 2 . 1 5}$ | 89.31 | $\\mathbf{9 1 . 4 0}$ | 88.00 | $\\mathbf{9 5 . 3 3}$ | $\\mathbf{9 8 . 5 8}$ | $\\mathbf{8 7 . 4 6}$ |\n| LRU [20] (paper results) | 60.20 | $\\mathbf{8 9 . 4 0}$ | 89.90 | $\\mathbf{8 9 . 0 0}$ | 95.10 | 94.20 | 86.30 |\n| S6 [10] | 38.02 | 82.98 | 72.14 | 69.82 | 69.26 | 67.32 | 66.59 |\n| RG-LRU [21] | 32.34 | 71.75 | 66.58 | 61.15 | 73.38 | 69.53 | 62.45 |\n\nTABLE II: Model performance in terms of test accuracy on the LRA benchmark. The first entry (Random) represents the performance of random guessing on the task, i.e., indicating the baseline above which a model is considered to have learned a meaningful representation. Models failing to exceed this baseline on a task are marked as FAIL. The best model on each task is highlighted in bold. ## IV. Performance in Practice\n\nIn this section, we evaluate the SSM proposals reviewed in Section III on the long-range arena (LRA) benchmark [2]. This benchmark evaluates the models in terms of their reasoning ability and their handling of diverse data types, which is one of the strengths of SSMs. We first introduce the benchmark, before presenting the empirical evaluation. ## A. Long-Range Arena (LRA) Benchmark\n\nThe goal of the LRA benchmark is to evaluate the reasoning capabilities of sequence models in diverse ways. The benchmark consists of 5 different tasks, which we summarize in the following. For more details on the benchmark and the individual tasks, we refer to [2]. a) List Operations (ListOps): This task evaluates a model's ability to capture hierarchical dependencies over long contexts. The goal is to predict the result of a mathematical operation consisting of nested mean, median, max, and $\\min$ operations ${ }^{5}$ The task is a ten-way classification task with maximal input lengths of 2 k . b) Text Classification (Text): This task evaluates a model's ability to capture the tone of long tokenized texts. The dataset consists of IMDb movie reviews, which need to be classified as negative or positive in tone. The task is a binary classification task with maximal input lengths of 4 k . c) Document Retrieval (Retrieval): This task evaluates a model's ability to compress long sequences into representations that are suitable for similarity matching. The dataset consists of tokenized papers published by the American Academy of Neurology (AAN), which need to be classified in having a citation link or not. The task is a binary classification task with maximal input lengths of 8 k . d) Image Classification (Image): This task evaluates a model's ability to learn 2D spatial relations from a 1D vector. The dataset consists of vectorized images, which depict one of ten possible classes, e.g. a horse or a car. The task is a ten-way classification task with maximal input lengths of 1 k . [^4]e) Long-Range Spacial Dependency: This task evaluates a model's ability to learn spacial dependencies in a vectorized image. The dataset consists of images, which depict two circles and multiple dashed paths. The goal is to evaluate whether the two circles are connected by any of the present paths or not. The task is therefore a binary classification task and is divided into two subtasks, which only differ in the size of the image. The first subtask has inputs of length 2 k and we will refer to it as Pathfinder; the second subtask has a maximal input length of 16 k and we will refer to it as Path-X. ## B. Empirical Evaluation of SSM Proposals\n\nThe empirical performance of the reviewed SSM proposals, the Transformer [1], and random guessing are reported in Table II We include the performance of the Transformer as a baseline, since they are the dominant architecture in large language models and sequence modelling. For S4, S4D, S5, and LRU we report the performance of the best variant from the original papers in order to present the most competitive results. Other variants of these models might perform better on tasks not included in the LRA benchmark; for more details on these variants we refer to the original papers. Since performance on the LRA benchmark of S6 and RGLRU have not been reported in the literature, we provide the results of our own implementation of these architectures, which we make available here ${ }^{6}$ The hyperparameters of the models and training details of our implementation are stated in the public code repository. On the LRA benchmark, the LTI-based models S4, S4D, S5, LRU outperform the LTV-based models S6, RG-LRU and the Transformer. From a control theoretic perspective this is surprising, since a general LTV definition encompasses LTI systems as a special case, i.e., a LTV system should perform at least as well as a LTI system. However, this is not the case for the particular time-varying parametrization of S6 or RG-LRU, since e.g. $\\bar{A}=\\bar{A}_{k} \\forall k$ cannot be achieved. We attempted to improve the performance of the LTV-based\n\n[^5]models by changing the initialization of S6 and RG-LRU and forcing the input-dependent eigenvalues of $\\bar{A}_{k} \\forall k$ closer to marginal stability according to Lemma 2.2 However, this resulted in both models to perform considerably worse or fail to learn anything meaningful at all. While marginally stable eigenvalues appear to be important for the LTI-based models, the same is not true for LTV-based models. To date, this behavior is not well understood. Lastly, even though the LTVbased models are closely related to the Transformer [28], they generally perform better on the LRA benchmark. ## V. CONCLUSION AND Future Opportunities\n\nIn this paper, we have provided an overview of stateof-the-art state space models (SSM) and explored their features from a control theoretic perspective. In doing this, we highlighted the many connections to standard control theoretic concepts such as the connection between memory and marginal-stability. Additionally, we compared the reviewed SSMs on the long-range arena (LRA) benchmark, finding that the more recent LTV-based SSMs perform worse than their LTI-based counterparts. From a control theoretic perspective, this raises many interesting research questions concerning a LTV parametrization that attains the same performance as the LTI models, as well as a deeper understanding on the role of the eigenvalues in the LTVbased models. SSMs, particularly the LTV versions, rely on dynamics where the dynamic matrices depend on the input (excitation) to the system. However, in the SSM literature the theoretical properties arising from these dynamics remains poorly understood. The evident connections between SSMs and linearsystem theory give rise to ample opportunities to provide explainability to large foundational models. Moreover, as seen with the LRU model, control theoretic insights have the potential to inform better designs for SSMs.",
    "ssmfundmodel-4": "## REFERENCES\n\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, \"Attention is All you Need,\" in Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-5": "30. Curran Associates, Inc., 2017. [2] Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler, \"Long Range Arena : A Benchmark for Efficient Transformers,\" in International Conference on Learning Representations (ICLR), 2021.",
    "ssmfundmodel-6": "[3] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, \"On the dangers of stochastic parrots: Can language models be too big?\" in Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 2021, pp.",
    "ssmfundmodel-7": "610-623. [4] I. Beltagy, M.",
    "ssmfundmodel-8": "E. Peters, and A. Cohan, \"Longformer: The Long-Document Transformer,\" 2020.",
    "ssmfundmodel-9": "[Online]. Available: https: //arxiv.org/abs/2004.05150\n[5] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang et al., \"Big bird: Transformers for longer sequences,\" Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-10": "33, 2020. [6] N. Kitaev, L. Kaiser, and A. Levskaya, \"Reformer: The Efficient Transformer,\" in International Conference on Learning Representations, 2020.",
    "ssmfundmodel-11": "[7] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller, \"Rethinking Attention with Performers,\" in International Conference on Learning Representations, 2021.",
    "ssmfundmodel-12": "[8] J. Ho, N. Kalchbrenner, D. Weissenborn, and T. Salimans, \"Axial Attention in Multidimensional Transformers,\" 2019.",
    "ssmfundmodel-13": "[Online]. Available: https://arxiv.org/abs/1912.12180\n[9] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016. [10] A. Gu and T. Dao, \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces,\" 2023. [Online]. Available: https: //arxiv.org/abs/2312.00752\n[11] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. R\u00e9, \"Combining Recurrent, Convolutional, and Continuous-time Models with Linear State Space Layers,\" in Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-14": "34. Curran Associates, Inc., 2021, pp. 572585. [12] A. Gu, K. Goel, and C. R\u00e9, \"Efficiently Modeling Long Sequences with Structured State Spaces,\" in The International Conference on Learning Representations (ICLR), 2022.",
    "ssmfundmodel-15": "[13] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. R\u00e9, \"HiPPO: Recurrent Memory with Optimal Polynomial Projections,\" in Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-16": "33. Curran Associates, Inc., 2020, pp. $1474-1487$. [14] X. Glorot and Y. Bengio, \"Understanding the difficulty of training deep feedforward neural networks,\" in Proceedings of the thirteenth international conference on artificial intelligence and statistics.",
    "ssmfundmodel-17": "JMLR Workshop and Conference Proceedings, 2010, pp. 249-256. [15] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter, \"Selfnormalizing neural networks,\" Advances in neural information processing systems, vol.",
    "ssmfundmodel-18": "30, 2017.",
    "ssmfundmodel-19": "[16] G. E. Blelloch, \"Prefix sums and their applications,\" 1990.",
    "ssmfundmodel-20": "[17] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" 2014. [Online]. Available: https://arxiv.org/abs/1412. 6980\n[18] A. Gu, A. Gupta, K. Goel, and C. R\u00e9, \"On the Parameterization and Initialization of Diagonal State Space Models,\" 2022. [Online]. Available: https://arxiv.org/abs/2206.11893\n[19] J. T. Smith, A. Warrington, and S. Linderman, \"Simplified State Space Layers for Sequence Modeling,\" in The Eleventh International Conference on Learning Representations, 2023.",
    "ssmfundmodel-21": "$[20]$ A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, and S. De, \"Resurrecting Recurrent Neural Networks for Long Sequences,\" in Proceedings of the 40th International Conference on Machine Learning, vol. 202. PMLR, 23-29 Jul 2023, pp. 26670 26698. [21] S. De, S. L. Smith, A. Fernando, A. Botev, G. Cristian-Muraru, A. Gu, R. Haroun, L. Berrada, Y. Chen, S. Srinivasan, G. Desjardins, A. Doucet, D. Budden, Y. W. Teh, R. Pascanu, N.",
    "ssmfundmodel-22": "D. Freitas, and C. Gulcehre, \"Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models,\" 2024.",
    "ssmfundmodel-23": "[Online]. Available: https://arxiv.org/abs/2402.19427\n[22] H. Liu, Z. Dai, D. So, and Q. V. Le, \"Pay Attention to MLPs,\" in Advances in Neural Information Processing Systems, A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., 2021. [23] K. Goel, A. Gu, C. Donahue, and C. R\u00e9, \"It's Raw! Audio Generation with State-Space Models,\" 2022. [Online]. Available: https://arxiv.org/abs/2202.09729\n[24] J. Sherman and W. J. Morrison, \"Adjustment of an Inverse Matrix Corresponding to a Change in One Element of a Given Matrix,\" The Annals of Mathematical Statistics, vol.",
    "ssmfundmodel-24": "21, no. 1, pp. 124 - 127, 1950. [25] D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. R\u00e9, \"Hungry Hungry Hippos: Towards Language Modeling with State Space Models,\" 2023. [Online]. Available: https: //arxiv.org/abs/2212.14052\n[26] A. Gupta, A. Gu, and J. Berant, \"Diagonal state spaces are as effective as structured state spaces,\" in Advances in Neural Information Processing Systems, vol.",
    "ssmfundmodel-25": "35. Curran Associates, Inc., 2022, pp. $22982-22994$. [27] S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural computation, vol.",
    "ssmfundmodel-26": "9, no. 8, pp. 1735-1780, 1997. [28] A. Ali, I. Zimerman, and L. Wolf, \"The Hidden Attention of Mamba Models,\" 2024. [Online]. Available: https://arxiv.org/abs/2403.01590\n\n\n[^0]:    * These authors contributed equally; ordered alphabetically. All authors are with the Institute for Dynamic Systems and Control, ETH Zurich, 8092 Zurich, Switzerland \\{camoalonso,jsieber,mzeilinger\\}@ethz.ch. This work was partially supported by the ETH AI Center. ${ }^{1}$ Referred to as input length in Transformer's literature. [^1]:    ${ }^{2} \\mathrm{An}$ input token is the unit that represent the smallest meaningful components of the input data, whether it's text, images, or any other form of information that the model processes. [^2]:    ${ }^{3}$ In the SSM literature, sometimes other nonlinearities are used, such as ReLU, SiLU, etc. [^3]:    ${ }^{4}$ The paper empirically found that $c=8$ works best for language modelling. [^4]:    ${ }^{5}$ For instance, input: $\\max (4, \\min (5,6, \\operatorname{mean}(9,4,5)))$, output: 5 . [^5]:    ${ }^{6}$ https://github.com/jsie7/ssm-benchmark\n\n"
}