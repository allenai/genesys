{
    "rwkv6-0": "# Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence \n\nBo Peng ${ }^{1,2, *}$, Daniel Goldstein ${ }^{2,3, *}$, Quentin Anthony ${ }^{2,4,21,{ }^{*}}$,<br>Alon Albalak ${ }^{2,5}$, Eric Alcaide ${ }^{2,6,7}$, Stella Biderman ${ }^{2}$, Eugene Cheah ${ }^{1,2,3}$, Xingjian Du ${ }^{1}$,<br>Teddy Ferdinan ${ }^{8}$, Haowen Hou ${ }^{9}$, Przemys\u0142aw Kazienko ${ }^{8}$, Kranthi Kiran $\\mathbf{G V}^{2,10}$,<br>Jan Koco\u0144 ${ }^{8}$, Bart\u0142omiej Koptyra ${ }^{8}$, Satyapriya Krishna ${ }^{11}$, Ronald McClelland Jr.",
    "rwkv6-1": "${ }^{2,12}$,<br>Niklas Muennighoff ${ }^{13}$, Fares Obeid ${ }^{2}$, Atsushi Saito ${ }^{2,14}$, Guangyu Song ${ }^{2}$, Haoqin Tu ${ }^{15,16}$,<br>Stanis\u0142aw Wo\u017aniak ${ }^{8}$, Ruichong Zhang ${ }^{17}$, Bingchen Zhao ${ }^{18}$, Qihang Zhao ${ }^{19}$,<br>Peng Zhou ${ }^{19}$, Jian Zhu ${ }^{20}$, and Rui-Jie Zhu ${ }^{16}$<br>${ }^{1}$ RWKV Project (under Linux Foundation AI \\& Data), ${ }^{2}$ EleutherAI, ${ }^{3}$ Recursal AI, ${ }^{4}$ Ohio State<br>University, ${ }^{5}$ University of California, Santa Barbara, ${ }^{6}$ Charm Therapeutics, ${ }^{7}$ Dalle Molle Institute<br>for Artificial Intelligence Research, ${ }^{8}$ Wroclaw Tech, ${ }^{9}$ Guangdong Laboratory of Artificial<br>Intelligence and Digital Economy (SZ), ${ }^{10}$ New York University, ${ }^{11}$ Harvard University, ${ }^{12}$ Ronsor<br>Labs, ${ }^{13}$ Contextual AI, ${ }^{14}$ Nextremer Co. Ltd., ${ }^{15}$ University of Chinese Academy of Sciences,<br>${ }^{16}$ University of California, Santa Cruz, ${ }^{17}$ Tsinghua University, ${ }^{18}$ University of Edinburgh,<br>${ }^{19}$ LuxiTech Co. Ltd., ${ }^{20}$ University of British Columbia, ${ }^{21}$ Zyphra\n\n\n#### Abstract\n\nWe present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) (Peng et al., 2023) architecture. Our architectural design advancements include multiheaded matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. ${ }^{1}$\n\n\n[^0]\n## Contents\n\n1 Introduction ..... 3\n2 Background ..... 4\n3 Eagle/Finch Architecture ..... 5\n4 Method ..... 5\n4.1 Eagle ..... 6\n4.1.1 Eagle Token Shift ..... 6\n4.1.2 Eagle Time Mixing ..... 6\n4.1.3 Channel Mixing ..... 7\n4.2 Finch ..... 7\n4.2.1 Finch Token Shift ..... 7\n4.2.2 Finch Time Mixing ..... 8\n5 RWKV World Tokenizer ..... 8\n6 RWKV World v2 Dataset ..... 9\n7 Pre-Trained Models ..... 9\n8 Language Modeling Experiments ..... 9\n8.1 LM Evaluation Harness Benchmarks ..... 9\n8.2 Associative Recall ..... 11\n8.3 Long Context Experiments ..... 12\n8.4 Bamboo Benchmark ..... 12\n9 Speed and Memory Benchmarks ..... 14\n10 Multimodal Experiments ..... 15\n10.1 RWKV Music Modelling ..... 15\n10.2 VisualRWKV ..... 15\n11 Conclusions ..... 16\nA Author Contributions ..... 27\nB Additional Architecture Details ..... 28\nC Additional Related Work ..... 29\nD Training Dataset Details ..... 32\nE Computing Costs ..... 32\nF Additional Evaluations ..... 34\nF. 1 Alignment Benchmark ..... 34\nF. 2 MTBench ..... 34\nF. 3 Self-Learning ..... 35\nF. 4 Zero-shot evaluation on additional NLP tasks ..... 35\nG Hyperparameters ..... 36\nH Parameter Initializations ..... 37\nI Non-English Chat Examples ..... 38\nJ Chat Examples - Comparison with RWKV-4 ..... 40\n\n## 1 Introduction\n\nAdvancements in Large Language Models (LLMs) have significantly impacted Natural Language Processing (NLP) tasks. The field has traditionally been dominated by the transfomer architecture (Vaswani et al., 2023). However, the expressive attention mechanism of transformers leads them to suffer from quadratic time complexity with respect to input sequence length. Various methods have been proposed to achieve sub-quadratic time complexity without significantly changing the core attention mechanism, typically relying on some form of sparsity techniques (Child et al., 2019a; Beltagy et al., 2020; Zaheer et al., 2020). Recent works have achieved sub-quadratic time complexity without significantly sacrificing performance by introducing new mechanisms to replace attention at the core of the Transformer architecture. These models include gated recurrences (Fu et al., 2023; Gu \\& Dao, 2023; Gu et al., 2021; Sun et al., 2023; Katsch, 2023; Qin et al., 2023; Smith et al., 2023), gated convolutions (Poli et al., 2023; Peng et al., 2023), data-dependent linear attention (Yang et al., 2023; Katharopoulos et al., 2020b), sparse attentions (Tay et al., 2020; Child et al., 2019b; Zaheer et al., 2020; Qiu et al., 2019) and their combinations (De et al., 2024; Qin et al., 2024; 2022). We build off RWKV-4 introduced in Peng et al. (2023), which provides efficient inference and training along with a parallelizable implementation compared to competing architectures as shown in Table 1. | Architecture | Inference |  |  | Training |  |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n|  | Time | Memory | Parallel | Time | Memory |\n| LSTM/LMU | $O(1)$ | $O(1)$ | $\\boldsymbol{x}$ | $O(N)$ | $O(N)$ |\n| Transformer | $O(N)$ | $O(N)^{a}$ | $\\checkmark$ | $O\\left(N^{2}\\right)$ | $O(N)^{b}$ |\n| Linear Transformer | $O(1)$ | $O(1)$ | $\\checkmark$ | $O(N)$ | $O(N)$ |\n| H3/S4 | $O(1)$ | $O(1)$ | $\\checkmark$ | $O(N \\log N)$ | $O(N)$ |\n| Hyena | $O(N)$ | $O(N)$ | $\\checkmark$ | $O(N \\log N)$ | $O(N)$ |\n| RWKV/Mamba/RetNet | $O(1)$ | $O(1)$ | $\\checkmark$ | $O(N)$ | $O(N)$ |\n\nTable 1: Comparative analysis of RWKV-4/5/6 and other LLM architectures regarding time and memory complexity for both inference per token and training per sequence, and training parallelizability across the sequence dimension. The context/sequence length is denoted by $N$. ${ }^{a} O(1)$ without KV cache ${ }^{b}$ With Flash Attention\n\nIn this paper, we introduce two new architectures: Eagle (RWKV-5) and Finch (RWKV-6). First, Eagle improves upon the architecture and learned decay schedule from RWKV-4 (Peng et al., 2023) through the use of expressive multi-headed matrix-valued states (as opposed to vector-valued states), a reformulated receptance, and an additional gating mechanism. Finch further improves the expressivity and flexibility of the architecture by introducing new data-dependent functions for both the time-mixing and token-shift modules, consisting of parameterized linear interpolations. Additionally, Finch proposes a novel use of the Low Rank Adaptation (Hu et al., 2022) function to allow for trainable weight matrices to efficiently augment the learned data decay vectors in a context-dependent manner. Finally, we introduce a new tokenizer, the RWKV World Tokenizer, and a new dataset, RWKV World v2 (1.12 trillion tokens), specially designed to improve performance on multilingual and code data. Through extensive experimentation, we show that the Eagle and Finch models perform competitively, or improve upon existing models under a wide variety of sequence modeling domains and tasks. Specifically, we evaluate our trained models on commonly used English-only and multilingual text benchmarks, associative recall, music modeling, and vision-language benchmarks. Our experiments demonstrate that the advancements in Eagle and Finch provide significant progress towards developing more efficient AI models\n\nIn summary, our main contributions are:\n\n- The Eagle (RWKV-5) and Finch (RWKV-6) RWKV architectures, which significantly improve over RWKV-4 on benchmarks for LLMs. - The RWKV World Tokenizer which contains underrepresented languages' vocabulary and which performs fast tokenization with Trie-based greedy matching. - The RWKV World v2 public dataset, comprised of 1.12 trillion tokens of publicly available multilingual data. - Public release of four pre-trained Eagle models, scaling from 0.46 to 7.5 billion parameters, and two Finch models, with 1.6 and 3.1 billion parameters. Demonstrating that these novel architectures are competitive to transformers when trained using enough FLOPs to make meaningful scaling conclusions. - A completely open training pipeline to enable interpretability and reproducibility of alternative-architecture LLMs (See Table 2). | Model | Context <br> Length | Training <br> Tokens | Open <br> Weights | Open Code |  | Open <br> Dataset |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  |  |  | Inference | Training |  |\n| GPT-4 | $128 \\mathrm{k}^{a}$ | Undisclosed | \u25cb | \u25cb | \u25cb | \u25cb |\n| LLaMA2 7B | 4 k | $2.0 \\times 10^{12}$ | (1) | - | \u25cb | \u25cb |\n| Mistral 7B v0.1 | $32 \\mathrm{k}^{b}$ | Undisclosed | - | - | \u25cb | O |\n| Gemma 7B | 8k | $6.0 \\times 10^{12}$ | (1) | - | - | \u25cb |\n| StableLM 7B v2 | 4 k | $1.1 \\times 10^{12}$ | - | - | - | - |\n| Pythia 6.9B | $2 k$ | $3.3 \\times 10^{11}$ | - | - | - | - |\n| Eagle 7B | Indefinite $^{c}$ | $1.1 \\times 10^{12}$ | - | - | - | - |\n\nTable 2: Comparison of the openness and accessibility of public foundational LLMs with 7B+ parameters regarding model weights, official inference/training code, and dataset. Widely available but not under an open source license is indicated by 0 . ${ }^{a}$ OpenAI's gpt-4-0125-preview model ${ }^{b}$ With sliding window attention ${ }^{c}$ Pretrained with context length 4096, but no fundamental context length limitation or relationship to speed, see 8.3 for extrapolation details\n\n## 2 Background\n\nEagle and Finch are RNNs based on a multi-headed hybridization of the RWKV-4 architecture and linear attention. We discuss related work and the evolution of these two architectures below, with a more detailed review given in Appendix C. Recurrent Neural Networks (RNNs) are well suited to provide inexpensive inference on sequence modelling tasks, typically operating in $\\mathrm{O}(1)$ time complexity per step with respect to sequence length. They model sequences with time dependencies by generating a hidden state $h_{t}$ at each time step, which is fed back in at the next time step as a secondary input. Classic RNNs (e.g. LSTM (Hochreiter \\& Schmidhuber, 1997) and GRU (Cho et al., 2014)) became widely used for sequence modelling, but are difficult to parallelize across the time dimension for training. The Transformer architecture has enjoyed remarkable success in generative sequence modelling, and language modelling in particular (Vaswani et al., 2023; Radford et al., 2018), providing SOTA performance across many tasks. However, the use of multi-headed dot-product self-attention (MHA) leads to a quadratic time complexity with respect to sequence length. The deficiencies of classic RNNs and Transformers led to many attempts to develop architectures incorporating the best features of both in a single model, namely $\\mathrm{O}(1)$ per token time complexity and fast highly parallelizable training. Linear Attention (Schmidhuber, 1992; Katharopoulos et al., 2020a) replaces the numerator of MHA's softmax $\\left(Q K^{T}\\right) V$ with $\\phi(Q) \\phi(K)^{\\mathrm{T}} V$, allowing a reordering of operations via associativity to $\\phi(Q)\\left(\\phi(K)^{\\mathrm{T}} V\\right)$, where $\\phi$ represents a non-negative feature-map function. It can be computed as an RNN in $O(1)$ time per step by adding $\\phi\\left(K_{i}^{T}\\right) V_{i}$ to a recurrent state at each time step $i$, or trained in parallel much like MHA. This accomplishes the main goals outlined above, but naive linear attention suffers from significantly reduced performance compared to MHA-based transformers. A modified form of linear attention, the Attention Free Transformer (AFT) (Zhai et al., 2021), paved the way for the RWKV architecture, by using a number of attention heads equal to the size of the feature dimension and incorporating a set of learned pairwise positional biases, denoted as $w$. $$\n\\operatorname{AFTAttn}_{t}=\\sigma_{q}\\left(q_{t}\\right) \\odot \\frac{\\sum_{i=1}^{t} \\exp \\left(k_{i}+w_{i, t}\\right) \\odot v_{i}}{\\sum_{i=1}^{t} \\exp \\left(k_{i}+w_{i, t}\\right)}\n$$\n\nRWKV-4 reformulates the AFT equation by replacing the pair-wise positional biases with a channelwise vector of additive weight decay rates $w$. It also adds a bonus term $u$ to offset the weight of only the current input specially. $$\n\\mathrm{wkv}_{t}=\\frac{\\sum_{i=1}^{t-1} \\exp \\left(-(t-1-i) w+k_{i}\\right) \\odot v_{i}+\\exp \\left(u+k_{t}\\right) \\odot v_{t}}{\\sum_{i=1}^{t-1} \\exp \\left(-(t-1-i) w+k_{i}\\right)+\\exp \\left(u+k_{t}\\right)}\n$$\n\nRWKV-4 also adds token-shift and gating to both attention and feed-forward sub-blocks of transformer, and small embedding initialization and normalization to quickly arrive at well-distributed token embeddings. Combining all of these architectural changes led RWKV-4 to become the first RNN to rival the performance of Transformers, while maintaining fast parallelizable training and $O(1)$ time complexity per token. There has been a recent revival of RNNs in NLP research (Tiezzi et al., 2024). HGRN(Qin et al., 2023) is a recent time-parallelizable data-dependent RNN that employs input and forget gates. TransNormer(Qin et al., 2022) applies RMSNorm to linear attention to bound its output. Other new time-parallelizable data-dependent RNNs have also been invented concurrently with our work including GLA (Yang et al., 2023) and Griffin (De et al., 2024). State Space Models (SSMs) employ a hidden state of basis function weights to model an approximation of the input function ( Gu et al., 2020), updating that hidden state via a differential equation. Earlier SSMs (Gu et al., 2022) were historically computed using long convolutions in $O(N \\log N)$ time per sequence, but could also be formulated as a recurrent network. Recently, it has been shown that SSMs can be parallelized across the time dimension via techniques including associative scan (Smith et al., 2023). A new class of SSMs has also emerged concurrently with our work (Katsch, 2023; Gu \\& Dao, 2023) that feature data-dependent $A$ and $B$ terms, which function similarly to the data-dependent dynamic recurrence used in Finch. ## 3 Eagle/Finch Architecture\n\nWe refine the RWKV architecture in two steps, and observe significant modeling improvements with each. Compared to the baseline RWKV-4, Eagle adds matrix-valued attention states, LayerNorm over the attention heads, SiLU attention gating, and improved initialization. It also removes the Sigmoid activation of receptance. Finch further applies data-dependence to the decay schedule and token-shift. The core architecture remains similar to that of RWKV-4, consisting of a series of stacked residual blocks shaped like a traditional Transformer. Following notation from (Tolstikhin et al., 2021), each block contains one Pre-LayerNorm Time-Mixing sub-layer followed by one Pre-LayerNorm Channel-Mixing sub-layer, as depicted in Figure 1, left. These correspond to the traditional Attention and Feed Forward Network sub-layers of the Transformer. See Appendix B for more details on our training implementation and the differences from RWKV-4, and Section 9 for speed and memory benchmarks. ## 4 Method\n\nIn this section, we use $D$ to denote the model dimension, and unless explicitly stated, all vectors appearing in this section are dimension $D / h$, where $h$ denotes the number of heads, belonging to $\\mathbb{R}^{(D / h)}$. For compactness and simplicity we show calculations per-head, eliding the head index. We use the convention that all vectors are row vectors unless explicitly transposed, so all matrices operate on the right side. ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-06.jpg?height=1064&width=1357&top_left_y=281&top_left_x=387)\n\nFigure 1: RWKV architecture overview. Left: time-mixing and channel-mixing blocks; top-right: RWKV time-mixing block as RNN cell; center-bottom: token-shift module in FeedForward module and Eagle time-mixing; bottom-right: token-shift module in Finch time-mixing. All shape annotations assume a single head for simplicity. Dashed arrows (left, top-right) indicate a connection in Finch, but not in Eagle. ### 4.1 Eagle\n\n### 4.1.1 Eagle Token Shift\n\nWe adopt the Token Shift technique from the previous RWKV, similar to a 1D causal convolution of size = 2, as can be seen in Figure 1, center-bottom. To better introduce the Token Shift technique, we define some notation. The linear interpolation (lerp) between $x_{t}$ and $x_{t-1}$ used in RWKV-4 and Eagle Token Shift is defined as:\n\n$$\n\\operatorname{lerp}_{\\square}(a, b)=a+(b-a) \\odot \\mu_{\\square}\n$$\n\nwhere each $\\mu_{\\square} \\in \\mathbb{R}^{D}$ is a learnable vector. Token Shift allows the model to learn how much new versus old information should be allocated per time step to each channel of receptance, key, value, and gate vectors ( $r, k, v$, and $g$ respectively) independently and uniquely for each head. This makes it possible to form induction heads (Elhage et al., 2021) within a single layer since even a single head can directly accumulate both past and current token data into separate subspaces within these vectors. ### 4.1.2 Eagle Time Mixing\n\nThe formula of Eagle Time Mixing can be written as follows:\n\n$$\n\\begin{gathered}\n\\square_{t}=\\operatorname{lerp}_{\\square}\\left(x_{t}, x_{t-1}\\right) \\boldsymbol{W}_{\\square}, \\quad \\square \\in\\{r, k, v, g\\} \\\\\nw=\\exp (-\\exp (\\omega)) \\\\\nw k v_{t}=\\operatorname{diag}(u) \\cdot k_{t}^{\\mathrm{T}} \\cdot v_{t}+\\sum_{i=1}^{t-1} \\operatorname{diag}(w)^{t-1-i} \\cdot k_{i}^{\\mathrm{T}} \\cdot v_{i} \\in \\mathbb{R}^{(D / h) \\times(D / h)} \\\\\no_{t}=\\operatorname{concat}\\left(\\operatorname{SiLU}\\left(g_{t}\\right) \\odot \\operatorname{LayerNorm}\\left(r_{t} \\cdot w k v_{t}\\right)\\right) \\boldsymbol{W}_{o} \\in \\mathbb{R}^{D}\n\\end{gathered}\n$$\n\nWhere LayerNorm operates on each of $h$ heads separately, which is also equivalent to the GroupNorm (Wu \\& He (2018)) operation on $h$ groups.",
    "rwkv6-2": "It is also worth noting that $w$ is obtained from $w=\\exp (-\\exp (\\omega))$, where $\\omega \\in \\mathbb{R}^{D / h}$ are the actual headwise trainable parameters. This ensures that $w$ falls within the interval $(0,1)$, guaranteeing that $\\operatorname{diag}(w)$ is a contraction matrix. The $w k v_{t}$ attention calculation can alternatively be written in a recurrent form:\n\n$$\n\\begin{aligned}\nw k v^{\\prime} & =s+\\operatorname{diag}(u) \\cdot k^{\\mathrm{T}} \\cdot v \\\\\ns^{\\prime} & =\\operatorname{diag}(w) \\cdot s+k^{\\mathrm{T}} \\cdot v\n\\end{aligned}\n$$\n\nRWKV's $w k v$ term can be considered a decay-based equivalent to the normalised $k^{\\mathrm{T}} v$ term in Linear Attention. It is instructive to note how for a given head $j$ the recurrent state $s$ is a sum of $k^{T} v$ where each channel of $s$ individually decays by the corresponding channel of $w$ at each time step. Prior to the application of the receptance vector, gating, and output weights, a per-channel learned boost $u$ is multiplied with the current token's $k^{\\mathrm{T}} v$ and summed with the state, as can be seen in Figure 1, top-right. This gives the current token special treatment relative to the sum of past tokens contained within the decaying state history. The receptance is multiplied by this sum, acting like the query term in Linear Attention. ### 4.1.3 Channel Mixing\n\nIn both Eagle and Finch, the Channel Mixing module is identical to the previous RWKV-4 architecture, except for a slightly reduced hidden dimension from $4 D$ to $3.5 D$. This reduction accounts for new gating weights in Eagle Time Mixing to ensure an equi-parameter relation with the prior model at the same number of layers and embedding dimension. We do not further reduce the hidden dimension in Finch despite adding a small number of new parameters for LoRA weights. The formulas for Channel Mixing are the same as RWKV-4, but we restate them here to ensure notational consistency, using linear interpolation from Equation 3:\n\n$$\n\\begin{aligned}\nr_{t}^{\\prime} & =\\operatorname{lerp}_{r^{\\prime}}\\left(x_{t}^{\\prime}, x_{t-1}^{\\prime}\\right) \\boldsymbol{W}_{r^{\\prime}} \\in \\mathbb{R}^{D} \\\\\nk_{t}^{\\prime} & =\\operatorname{lepp}_{k^{\\prime}}\\left(x_{t}^{\\prime}, x_{t-1}^{\\prime}\\right) \\boldsymbol{W}_{k^{\\prime}} \\in \\mathbb{R}^{3.5 D} \\\\\nv_{t}^{\\prime} & =\\operatorname{ReLU}\\left(k_{t}^{\\prime}\\right)^{2} \\boldsymbol{W}_{v^{\\prime}} \\in \\mathbb{R}^{D} \\\\\no_{t}^{\\prime} & =\\sigma\\left(r_{t}^{\\prime}\\right) \\odot v_{t}^{\\prime} \\in \\mathbb{R}^{D}\n\\end{aligned}\n$$\n\n### 4.2 Finch\n\n### 4.2.1 Finch Token Shift\n\nThe data-dependent linear interpolation (ddlerp) between $x_{t}$ and $x_{t-1}$ used in Finch Token Shift is defined as:\n\n$$\n\\begin{aligned}\n\\operatorname{lor}_{\\square}(x) & =\\lambda_{\\square}+\\tanh \\left(x A_{\\square}\\right) B_{\\square} \\\\\n\\operatorname{ddlerp}_{\\square}(a, b) & =a+(b-a) \\odot \\operatorname{lora}_{\\square}\\left(a+(b-a) \\odot \\mu_{x}\\right)\n\\end{aligned}\n$$\n\nwhere $\\mu_{x}$ and each $\\lambda_{\\square}$ introduce a trainable vector of dimension $D$ and each $A_{\\square} \\in \\mathbb{R}^{D \\times 32}$, $B_{\\square} \\in \\mathbb{R}^{32 \\times D}$ introduce new trainable weight matrices.",
    "rwkv6-3": "For the special case of $\\operatorname{LoRA}_{\\omega}$ seen below we introduce double-sized trainable weight matrices $\\boldsymbol{A}_{\\omega} \\in \\mathbb{R}^{D \\times 64}, \\boldsymbol{B}_{\\omega} \\in \\mathbb{R}^{64 \\times D}$. A schematic representation can be found in Figure 1, bottom-right. Please note that future 7B and larger Finch models are expected to further increase the size of these weight matrices by double or more. This new form of Token Shift enhanced with data-dependence is intended to expand the abilities of the model beyond the RWKV-4/Eagle style of Token Shift so that the amount of new and old data allocated per channel now depends on the input at both current and prior time steps. ### 4.2.2 Finch Time Mixing\n\n$$\n\\begin{gathered}\n\\square_{t}=\\operatorname{lerp}_{\\square}\\left(x_{t}, x_{t-1}\\right) W_{\\square}, \\quad \\square \\in\\{r, k, v, g\\} \\\\\nd_{t}=\\operatorname{lora}_{d}\\left(\\operatorname{ddlerp}_{d}\\left(x_{t}, x_{t-1}\\right)\\right) \\\\\nw_{t}=\\exp \\left(-\\exp \\left(d_{t}\\right)\\right) \\\\\nw k v_{t}=\\operatorname{diag}(u) \\cdot k_{t}^{\\mathrm{T}} \\cdot v_{t}+\\sum_{i=1}^{t-1} \\operatorname{diag}\\left(\\bigodot_{j=1}^{i-1} w_{j}\\right) \\cdot k_{i}^{\\mathrm{T}} \\cdot v_{i} \\in \\mathbb{R}^{(D / h) \\times(D / h)} \\\\\no_{t}=\\operatorname{concat}\\left(\\operatorname{SiLU}\\left(g_{t}\\right) \\odot \\operatorname{LayerNorm}\\left(r_{t} \\cdot \\boldsymbol{w} k v_{t}\\right)\\right) \\boldsymbol{W}_{o} \\in \\mathbb{R}^{D}\n\\end{gathered}\n$$\n\nThe $w k v_{t}$ attention calculation can alternatively be written in a recurrent manner:\n\n$$\n\\begin{aligned}\nw k v^{\\prime} & =s+\\operatorname{diag}(u) \\cdot k^{\\mathrm{T}} \\cdot v \\\\\ns^{\\prime} & =\\operatorname{diag}(w) \\cdot s+k^{\\mathrm{T}} \\cdot v\n\\end{aligned}\n$$\n\nUnlike in Eagle, $w_{t}$ here is not static across the sequence (dashed arrows in Figure 1, left and topright.). This is the core change to decay in Finch, as each channel of $w_{t}$ can now vary independently over time, in a data-dependent manner, whereas previously it was a fixed learned vector. The new LoRA mechanisms above are used to take learned vectors, as seen in Eagle, and inexpensively augment them with additional offsets determined by the incoming input. Note that the LoRA process itself uses an Eagle style Token-Shifted value as its input, not just the latest token. The new time-varying decay $w_{t}$ goes one step further, applying LoRA again afterward. Intuitively, this is a second-order variant of Token-Shifting, allowing each channel of $w_{t}$ to vary based on a mix of the current and prior tokens, with the mix itself determined by aspects of both tokens. ## 5 RWKV World Tokenizer\n\nTokenization is important in language modelling as it conditions the learning relationships between tokens and the generation of new text based on those patterns. The numbers of tokens to build a single semantic chunk are, however, often very unequally distributed against nonEuropean and other underrepresented languages. Byte-pair-encoding (BPE) based tokenizers which are trained with this inequality result in not only lower performances against underrepresented languages but also undue economic costs such as inference Ahia et al. (2023) and continual pre-training with extended vocabulary Lin et al. (2024); Sasaki et al. (2023). To address these problems, we manually select tokens from multiple vocabulary files such that non-European languages are well represented. To construct the tokenizer's vocabulary, we merge the vocabularies of the following tokenizers and then manually select the tokens for non-European languages. - GPT-NeoX-20B (Black et al., 2022): https: / huggingface.co/EleutherAI/ gpt-neox-20b\n- GPT2 (Radford et al., 2019): https: / / huggingface. co/openai-community/ gpt2\n- cl100k_base of tiktoken: https: / /github.com/openai/tiktoken\n- Llama2 (Touvron et al., 2023): https://huggingface.co/meta-llama/ Llama-2-7b-hf\n- Bloom (Workshop et al., 2023): https://huggingface.co/bigscience/ bloom\n\nThis tokenizer has a vocabulary size of $V=65536$, numbered from 0 through 65535, where tokens are arranged by their lengths in bytes. Below is a brief overview:\n\n- Token 0: Represents the boundary between text documents, known as $<E O S>$ or $<$ SOS $>$. This token doesn't encode any specific content and is only used for document separation. - Tokens 1-256: Consist of byte encodings (Token $k$ encodes byte $k-1$ ), wherein tokens 1 -128 correspond to standard ASCII characters. - Tokens 257-65529: Tokens with a minimum length of 2 bytes in UTF-8, including words, prefixes and suffixes, accented letters, Chinese characters, Hangul, Hiragana, Katakana and emojis. For example, Chinese characters are allocated from token 10250 to 18493. - Token 65530-65535: Reserved tokens for future use. These designations are intended to enhance the tokenizer's efficiency on the multilingual corpus, as well as on source code of programming languages. This tokenizer is implemented via a Trie (Prefix Tree) to boost speed while maintaining simplicity. Encoding is performed as matching the longest element in vocabulary with an input string from left to right. We note that our tokenizer's vocabulary construction is to mitigate undue burden, which naive BPE and related methods cause, on minor languages. ## 6 RWKV World v2 Dataset\n\nWe train our models on the new RWKV World v2 Dataset, a new multilingual 1.12 trillion token dataset drawn from a wide variety of hand selected publicly available data sources. This dataset is designed to go beyond the English-heavy focus of many datasets widely used to train LLMs today. We do this to support usage by the majority of the worldwide population who are not native English speakers, to improve representation within model responses, and also to enable transfer learning so that our models can apply knowledge across cultures and locales. We put a strong emphasis on factual knowledge and code, but also on cultural works including stories, books, subtitles, and conversations. The source data is approximately $70 \\%$ English, $15 \\%$ multilingual, and $15 \\%$ code. We describe the components of our dataset in detail in Appendix D. ## 7 Pre-Trained Models\n\nWe have pre-trained and publicly released the six Apache 2.0 licensed Eagle and Finch models: Eagle 0.4B, Eagle 1.5B, Eagle 3B, Eagle 7B, Finch 1.6B, and Finch 3B. All of the models were trained on the 1.12 trillion token RWKV World v2 multilingual corpus. See Appendix E for detailed parameter counts and FLOPs calculations. ## 8 Language Modeling Experiments\n\n### 8.1 LM Evaluation Harness Benchmarks\n\nTo assess the performance of Eagle and Finch models, we evaluate on a series of common multilingual and English-focused benchmarks using lm_evaluation_harness (Gao et al., 2023) as shown in Tables 3 and 4. We find that Eagle and Finch demonstrate exceptionally high capabilities on multi-lingual benchmarks, with nearly all results significantly outperforming the other similarly sized models we tested. In figures 2 and 3 we plot the accuracy versus FLOPs used to train various open models across a similar set of common benchmarks. For multilingual benchmarks, Eagle and Finch represent a substantial improvement to the Pareto frontier, achieving far higher scores than other models trained for a similar number of FLOPs. The two models additionally obtain competitive performance across these English benchmarks. ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-10.jpg?height=906&width=1221&top_left_y=317&top_left_x=452)\n\nFigure 2: Multilingual average benchmark accuracy versus training FLOPs. Average of LAMBADA Multilingual, xStoryCloze, xWinoGrande, and xCOPA\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-10.jpg?height=903&width=1218&top_left_y=1435&top_left_x=451)\n\nFigure 3: English average benchmark accuracy versus training FLOPs. Average of LAMBADA (OpenAI), PIQA, StoryCloze16, HellaSwag, WinoGrande, Arc (challenge), Arc (easy), HeadQA (English), OpenBookQA, SciQ, ReCoRD and COPA\n\n| Model | lmb.m <br> ppl $\\downarrow$ | Imb.m <br> acc $\\uparrow$ | pawsx <br> acc $\\uparrow$ | xcopa <br> acc $\\uparrow$ | xnli <br> acc $\\uparrow$ | xsClz <br> acc $\\uparrow$ | xwin <br> acc $\\uparrow$ | avg <br> acc $\\uparrow$ |\n| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |\n| Pythia-1.4b | 115.9 | 35.5 | 50.9 | 52.7 | 38.9 | 51.8 | 68.3 | 49.7 |\n| Mamba-1.4b | 73.1 | 40.4 | 48.0 | 54.4 | $\\mathbf{4 1 .",
    "rwkv6-4": "6}$ | 54.2 | 72.4 | 51.8 |\n| RWKV-4-1.5b | 72.5 | 38.5 | $\\mathbf{5 3 . 7}$ | 55.4 | 39.3 | 56.0 | 67.7 | 51.8 |\n| Eagle-1.5b | 43.2 | 44.8 | 51.9 | 57.9 | 40.4 | $\\mathbf{5 7 . 9}$ | 73.0 | 54.3 |\n| Finch-1.6b | $\\mathbf{3 7 . 5}$ | $\\mathbf{4 6 . 9}$ | 50.9 | $\\mathbf{5 8 . 0}$ | 41.4 | $\\mathbf{5 7 .",
    "rwkv6-5": "9}$ | $\\mathbf{7 4 . 9}$ | $\\mathbf{5 5 . 0}$ |\n| Pythia-2.8b | 81.3 | 38.8 | 49.4 | 53.7 | 40.0 | 53.5 | 71.5 | 51.1 |\n| Mamba-2.8b | 53.7 | 43.5 | 43.6 | 55.3 | 42.1 | 56.3 | 75.6 | 52.7 |\n| RWKV-4-3b | 48.1 | 43.4 | 50.9 | 57.5 | 40.9 | 58.1 | 72.3 | 53.9 |\n| Eagle-3b | 30.8 | 49.1 | $\\mathbf{5 1 . 6}$ | 59.0 | 42.3 | 59.8 | 76.9 | 56.5 |\n| Finch-3b | $\\mathbf{2 8 .",
    "rwkv6-6": "1}$ | $\\mathbf{5 0 . 5}$ | 49.7 | $\\mathbf{5 9 . 5}$ | $\\mathbf{4 4 . 2}$ | $\\mathbf{6 0 . 7}$ | $\\mathbf{7 7 . 8}$ | $\\mathbf{5 7 . 1}$ |\n| Pythia-6.9b | 85.6 | 36.7 | 48.4 | 54.1 | 40.0 | 54.2 | 70.9 | 50.7 |\n| MPT-7b | 49.8 | 44.4 | 43.5 | 53.6 | 39.8 | 56.3 | 76.9 | 52.4 |\n| Llama-2-7b | 30.4 | 50.8 | 41.2 | 56.7 | 39.9 | 57.5 | 79.5 | 54.3 |\n| Falcon-7b | 28.7 | 51.3 | 48.2 | 56.0 | 39.0 | 56.0 | 77.7 | 54.7 |\n| Mistral-7B-v0.1 | 27.1 | 51.9 | 41.5 | 55.9 | 43.1 | 59.2 | $\\mathbf{8 1 . 2}$ | 55.5 |\n| RWKV-4-7b | 33.1 | 47.4 | $\\mathbf{5 2 . 1}$ | 60.1 | 41.2 | 60.9 | 76.5 | 56.4 |\n| Eagle-7B | $\\mathbf{2 1 . 0}$ | $\\mathbf{5 3 . 7}$ | 45.6 | $\\mathbf{6 2 . 2}$ | $\\mathbf{4 4 .",
    "rwkv6-7": "0}$ | $\\mathbf{6 3 . 3}$ | 80.4 | $\\mathbf{5 8 . 2}$ |\n\nTable 3: Multilingual Benchmarks, including LAMBADA Multilingual (lmb.m) (Gao et al., 2023), XCOPA (Ponti et al., 2020), XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), XStoryCloze (xsClz) (Lin et al., 2022), xWinogrande (xwin) (Tikhonov \\& Ryabinin, 2021). | Model | Imb.o <br> ppl $\\downarrow$ | lmb.o <br> acc $\\uparrow$ | hella <br> acc_n $\\uparrow$ | piqa <br> acc $\\uparrow$ | arc <br> acc $\\uparrow$ | glue <br> acc $\\uparrow$ | wing <br> acc $\\uparrow$ | sciq <br> acc $\\uparrow$ | copa <br> acc $\\uparrow$ | avg <br> acc $\\uparrow$ |\n| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |\n| Pythia-1.4b | 6.22 | 61.0 | 52.0 | 70.8 | 49.8 | 47.1 | 57.3 | 86.5 | 71.0 | 61.9 |\n| RWKV-4-1.5b | 6.15 | 60.1 | 51.6 | 71.5 | 48.1 | 46.1 | 55.2 | 84.7 | 78.0 | 61.9 |\n| Eagle-1.5b | 5.06 | 65.7 | 55.0 | 71.1 | 51.1 | $\\mathbf{5 4 .",
    "rwkv6-8": "1}$ | 59.1 | $\\mathbf{8 9 . 7}$ | 76.0 | 65.2 |\n| Finch-1.6b | $\\mathbf{4 . 6 7}$ | $\\mathbf{6 6 . 8}$ | 57.3 | 72.6 | 51.8 | 49.8 | 59.4 | $\\mathbf{8 9 . 6}$ | 78.0 | $\\mathbf{6 5 . 7}$ |\n| Mamba-1.4b | 5.05 | 64.5 | $\\mathbf{5 9 . 0}$ | $\\mathbf{7 4 . 2}$ | $\\mathbf{5 3 . 5}$ | 47.0 | $\\mathbf{6 1 . 3}$ | 87.1 | $\\mathbf{8 0 . 0}$ | $\\mathbf{6 5 . 8}$ |\n| Pythia-2.8b | 5.21 | 63.8 | 59.1 | 73.9 | 52.3 | 47.3 | 58.2 | 88.6 | 79.0 | 65.3 |\n| RWKV-4-3b | 4.71 | 65.7 | 58.8 | 72.4 | 52.8 | 53.6 | 57.5 | 87.6 | $\\mathbf{8 6 . 0}$ | 66.8 |\n| Eagle-3b | 4.15 | 68.7 | 62.6 | 74.3 | 57.1 | 46.3 | 62.0 | $\\mathbf{9 2 . 6}$ | 85.0 | 68.6 |\n| Mamba-2.8b | 4.21 | 68.1 | $\\mathbf{6 5 . 9}$ | $\\mathbf{7 5 . 2}$ | $\\mathbf{5 7 . 9}$ | 46.3 | 63.0 | 90.2 | 84.0 | 68.8 |\n| Finch-3b | $\\mathbf{3 . 9 2}$ | $\\mathbf{7 0 . 8}$ | 64.8 | 74.2 | 55.9 | $\\mathbf{5 8 . 2}$ | $\\mathbf{6 3 .",
    "rwkv6-9": "6}$ | $\\mathbf{9 2 . 5}$ | 82.0 | $\\mathbf{7 0 . 3}$ |\n| Pythia-6.9b | 5.83 | 60.9 | 63.2 | 74.8 | 55.1 | 47.7 | 61.5 | 88.9 | 79.0 | 66.4 |\n| RWKV-4-7b | 3.94 | 69.8 | 65.3 | 75.0 | 56.4 | 56.4 | 62.4 | 90.8 | 85.0 | 70.1 |\n| Llama-2-7b | 3.40 | 73.5 | 76.0 | 78.1 | 65.4 | 42.9 | 69.1 | 93.9 | 87.0 | 73.2 |\n| MPT-7b | 3.88 | 68.7 | 76.3 | 79.3 | 63.3 | 48.7 | 68.1 | 93.9 | 88.0 | 73.3 |\n| Falcon-7b | 3.37 | 74.6 | 76.4 | 79.5 | 63.4 | 45.8 | 67.1 | 94.4 | 88.0 | 73.6 |\n| Eagle-7B | 3.37 | 74.2 | 70.9 | 77.0 | 62.5 | $\\mathbf{5 7 . 5}$ | 67.4 | 95.5 | 88.0 | 74.1 |\n| Mistral-7B-v0.1 | $\\mathbf{3 . 1 8}$ | $\\mathbf{7 5 . 5}$ | $\\mathbf{8 1 . 0}$ | $\\mathbf{8 0 . 5}$ | $\\mathbf{7 0 . 7}$ | 51.5 | $\\mathbf{7 3 . 6}$ | $\\mathbf{9 5 .",
    "rwkv6-10": "9}$ | $\\mathbf{9 3 . 0}$ | $\\mathbf{7 7 . 7}$ |\n\nTable 4: English Focused Benchmarks, including LAMBADA (OpenAI) (lmb.o) (Paperno et al., 2016), Hellswag (hella) (Hampel, 1974), PIQA (Bisk et al., 2020), AI2 ARC (arc) (Bhakthavatsalam et al., 2021), GLUE (Wang et al., 2018), Winogrande (winG) (Sakaguchi et al., 2021), SciQ (Welbl et al., 2017), COPA (Roemmele et al., 2011). ### 8.2 Associative Recall\n\nAssociative recall (AR) is a synthetic task designed to mimic the way that humans associate and retrieve information. It measures a model's proficiency in recalling information that was previously mentioned in context. Prior research suggests that a model's ability to perform AR is indicative of its effectiveness in in-context learning (Elhage et al., 2021; Olsson et al., 2022). As a result, AR has been adopted as a benchmark in developing new language model architectural designs. (Fu et al., 2023; Poli et al., 2023; Lutati et al., 2023). Arora et al. (2023) benchmarked a range of models for multi-query associative recall (MQAR) and identified a performance gap between various linear transformer architectures and the transformer with attention. In MQAR tasks, prior RWKV models demonstrated a correlation between model dimension and sequence length. To compare architectures, we trained models using RWKV-4, Eagle and Finch on MQAR,\nusing identical criteria with various model dimensions and sequence lengths. Our findings reveal significant improvements in MQAR with Eagle and Finch. Notably, Finch achieves extremely high accuracy in MQAR in our tests, and outperforms all well-known non-transformer architectures previously used to train large language models. Our experiments reveal performance disparities between Mamba (Gu \\& Dao, 2023) and Finch, despite their shared architectural features such as matrix-valued state and data-dependent memory modification, suggesting different combinations of these elements result in superior performance. ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-12.jpg?height=704&width=1365&top_left_y=628&top_left_x=379)\n\nFigure 4: MQAR tasks. An increase in sequence length correlates with increased task difficulty. ### 8.3 Long Context Experiments\n\nWe test loss versus sequence position on the PG19 (Rae et al., 2019) test set of books from token 2048 onward across RWKV-4, Eagle, and Finch. We find that Eagle improves dramatically over RWKV-4 on this long sequence task, despite having been trained solely on sequence length 4096. Finch further improves on this test beyond Eagle, with loss continuing to drop further into the sequence. See Figure 5 for details. ### 8.4 Bamboo Benchmark\n\nThe Bamboo benchmark (Dong et al., 2023) evaluates the overall long-context language modeling capability of LLMs from five aspects: question answering, hallucination detection, text sorting, language modeling, and code completion, comprising a total of ten evaluation tasks. We test models on the 4 k version of the benchmark, which includes all ten tasks with a maximum context window length of 4 k . We choose not to present results on the code completion task since all tested models failed to generate correct code completions for this task. In Table 5, we present the results of nine tasks, with either accuracy or F1 score, along with their average scores. At both the 1.5 b and 3 b scales, the latest Finch and Eagle models outperform the vanilla Mamba by at least a 7\\% average score, while remaining comparable with the Mamba trained on Hermes data (i.e., only a $0.7 \\%$ drop in the average score). Note that, despite being trained on only 1.1T tokens, Eagle-7b consistently outperforms Pythia by an average of $13.5 \\%$ at the 7 b scale, and it also surpasses LLaMA2-Chat-7b on several tasks in the Bamboo benchmark. These results demonstrate the superior capacity of the proposed Finch and Eagle models on a vast range of long-context tasks. ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-13.jpg?height=1036&width=1378&top_left_y=398&top_left_x=365)\n\nFigure 5: Loss along sequence offset for 3B RWKV-4 World, Eagle and Finch on PG19 dataset. All models were pretrained with context length 4096. | Model | meetingqa <br> Acc. $\\uparrow$ | paperqa <br> Acc. $\\uparrow$ | meetingpred <br> Acc. 1 | showspred <br> Acc. $\\uparrow$ | reportsumsort <br> Acc. $\\mid$ | showssort <br> Acc.",
    "rwkv6-11": "$\\uparrow$ | senhallu <br> F1 $\\uparrow$ | abshallu <br> F1 $\\uparrow$ | altqa <br> Acc. $\\uparrow$ | Avg. $\\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Pythia-1.4b | $15.0 \\%$ | $4.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | 0.0\\% | $2.1 \\%$ |\n| Mamba-1.4b | 15.0\\% | $2.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $2.0 \\%$ | $0.0 \\%$ | $2.1 \\%$ |\n| Eagle-1.5b | 21.0\\% | 19.0\\% | $1.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $13.2 \\%$ | $23.5 \\%$ | $5.5 \\%$ | $9.2 \\%$ |\n| Finch-1.6b | $19.0 \\%$ | $22.0 \\%$ | $1.0 \\%$ | $8.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $10.7 \\%$ | $17.3 \\%$ | $2.5 \\%$ | $8.9 \\%$ |\n| Pythia-2.8b | 16.0\\% | $4.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | 0.0\\% | $2.2 \\%$ |\n| Mamba-2.8b | $11.0 \\%$ | $4.0 \\%$ | $0.0 \\%$ | $3.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $3.9 \\%$ | $0.0 \\%$ | $2.4 \\%$ |\n| Mamba-2.8b-Hermes | $27.0 \\%$ | $25.0 \\%$ | $0.0 \\%$ | $9.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $19.7 \\%$ | $26.4 \\%$ | 0.0 | $11.9 \\%$ |\n| Eagle-3b | $16.0 \\%$ | $14.0 \\%$ | $0.0 \\%$ | $4.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $25.0 \\%$ | $29.2 \\%$ | $1.0 \\%$ | $9.9 \\%$ |\n| Finch-3b | 20.0\\% | 26.0\\% | 4.0\\% | 7.0\\% | $0.0 \\%$ | $0.0 \\%$ | $14.4 \\%$ | 23.6\\% | $6.5 \\%$ | $11.3 \\%$ |\n| Pythia-6.9b | $19.0 \\%$ | $7.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $3.3 \\%$ |\n| Eagle-7b-Hermes | $31.0 \\%$ | $23.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $50.3 \\%$ | $46.9 \\%$ | $0.0 \\%$ | $16.8 \\%$ |\n| LLaMA2-Chat-7b | $6.0 \\%$ | $17.0 \\%$ | $4.0 \\%$ | $12.0 \\%$ | $0.0 \\%$ | $0.0 \\%$ | $64.7 \\%$ | $63.4 \\%$ | $46.0 \\%$ | $24.1 \\%$ |\n| Mistral-Instruct-7b | 65.0\\% | 73.0\\% | 17.0\\% | 32.0\\% | $0.0 \\%$ | $0.0 \\%$ | 80.5\\% | $72.8 \\%$ | 13.5\\% | $39.3 \\%$ |\n\nTable 5: Results on the long context reasoning benchmark: Bamboo. We compare both transformer and linear attention language models on three different scales: $1.5 \\mathrm{~b}, 3 \\mathrm{~b}$, and 7 b . ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-14.jpg?height=784&width=1261&top_left_y=280&top_left_x=367)\n\nFigure 6: Memory Usage vs. Sequence Length (A100 80GB)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-14.jpg?height=787&width=1261&top_left_y=1160&top_left_x=364)\n\nFigure 7: Time vs. Sequence Length (A100 80GB)\n\n## 9 Speed and Memory Benchmarks\n\nWe compare the speed and memory utilization of the Attention-like kernels for Finch, Mamba ${ }^{2}$, and Flash Attention ${ }^{3}$ (Dao, 2023) in Figures 6 and 7. For all benchmarks, we use a batch size of 8, a model dimension of 4096, and a head size of 64 for both Flash Attention and Finch. For Mamba, we employ a state dimension of 16, a model dimension of 8192, to mimic Mamba's usage of an expansion factor of 2. Our findings indicate that Finch's speed in training scales linearly with respect to sequence length, exhibiting similar scaling to Mamba. We find Finch\n\n[^1]is significantly faster than Flash Attention for sequence lengths beyond 4 k , being around 4.2 x faster for a sequence length of 16k. Furthermore, Finch consistently outperforms Mamba and Flash Attention in terms of memory usage, using $40 \\%$ and $17 \\%$ less memory usage than Flash Attention and Mamba respectively.",
    "rwkv6-12": "Further optimization of our Finch CUDA implementation, including algorithmic improvements, are possible, and could lead to speed increases and greater parallelization. However, this optimization is left for future work. ## 10 Multimodal Experiments\n\nIn this section, we explore the capabilities of Eagle when extended to handle multimodal tasks, where the model processes and integrates textual inputs with inputs in a different domain. ### 10.1 RWKV Music Modelling\n\nTo investigate the Eagle architecture's applicability to music modeling, we use the Irishman ABC music sheet dataset (Wu et al., 2023) to train a new RWKV-5-Music model using the same hyperparameters as the existing RWKV-4-Music model. The loss of RWKV-5 is approximately $2 \\%$ lower than that of the previous generation model, and this improvement is primarily observed in the musical score part, indicating that RWKV-5 possesses stronger modeling and generalization capabilities than its predecessor. The model has a total of $L=24$ layers, with a dimension of $D=512$ and uses a byte-level tokenizer with $V=128$ tokens. The training context length is 1024 bytes. We use all 2,162 pieces of music in the validation set and calculate the loss for each position from the start. The loss is averaged across all pieces of music, then Gaussian smoothed over the position in the sequence. The figure 8 shows the loss as a function of position. Note that the first 30-100 bytes of the ABC format are the file header and control codes, followed by the musical scores. The loss of RWKV-5 is approximately $2 \\%$ lower than the previous generation model, and it is shown mainly in the musical score part, indicating that RWKV-5 has stronger modelling and generalization capabilities than its precedent model. ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-15.jpg?height=472&width=1045&top_left_y=1477&top_left_x=540)\n\nFigure 8: Music modelling loss over sequence position. ### 10.2 VisualRWKV\n\nVisualRWKV is the visual-enhanced version of the RWKV language model, enabling RWKV to handle various visual tasks. Our VisualRWKV follows a similar architecture to popular visionlanguage models (Liu et al., 2023a). We present the architecture in Figure 9. It consists of a vision encoder and a language model. Specifically, we use CLIP (Radford et al., 2021) as the vision encoder and Eagle 1.5B and 3B as the language model. We use LLaVA-1.5 dataset (Liu et al., 2023a). To adapt Eagle to this multimodal task, we employ a two-stage instruction-tuning process to enhance model performance. Initially, we conduct pre-training for feature alignment, during which only the projection layer is subjected to updates, while the rest of the model is kept in a frozen state. Following this, we move on to the fine-tuning end-to-end stage, where both the projection layer and the RWKV language model are fine-tuned, and the vision encoder\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-16.jpg?height=781&width=1288&top_left_y=325&top_left_x=402)\n\nFigure 9: VisualRWKV architecture overview. | Method | Vision Encoder | LLM | GQA ( $\\uparrow$ ) | ScienceQA-IMG ( $\\uparrow$ ) | Text-VQA ( $\\uparrow$ ) | POPE ( $\\uparrow$ ) |\n| :--- | :--- | :--- | :---: | :---: | :---: | :---: |\n| BLIP-2 (Li et al., 2023a) | EVA01-CLIP-G | Vicuna-13B | 41.0 | 61.0 | 42.5 |  |\n| BLIP-2 (Li et al., 2023a) | EVA01-CLIP-G | Flan-T5-11B | 44.6 | 64.5 | - |  |\n| InstructBLIP(Dai et al., 2023) | EVA01-CLIP-G | Vicuna-7B | 49.2 | 60.5 | - |  |\n| InstructBLIP(Dai et al., 2023) | EVA01-CLIP-G | Vicuna-13B | 49.5 | 63.1 | - |  |\n| IDEFICS-9B (IDEFICS, 2023) | OpenCLIP-H | LLaMA-7B | 38.4 | - | 50.1 |  |\n| IDEFICS-80B (IDEFICS, 2023) | OpenCLIP-H | LLaMA-65B | 45.2 | 25.9 | -9 |  |\n| TinyGPT-V (Yuan et al., 2023) | EVA01-CLIP-G | Phi-2 (2.7B) | 33.6 | - | 30.9 |  |\n| VisualRWKV | CLIP-L | Eagle-1.5B | 48.5 | - | - |  |\n| VisualRWKV | CLIP-L | Eagle-3B | 49.7 | 46.2 | - |  |\n\nTable 6: A comparison of VisualRWKV to other state-of-the-art Multimodal Large Language\nModels (MLLMs) across 4 distinct benchmarks. We evaluate these models on benchmarks: GQA(Hudson \\& Manning, 2019), ScienceQA-IMG(Lu et al., 2022), Text-VQA(Singh et al., 2019) and POPE(Li et al., 2023c). For POPE, the average F1-score across three distinct categories-random, popular, and adversarial-was computed using the validation set of the MSCOCO dataset. continue to be kept frozen. As shown in Table 6, we demonstrate that VisualRWKV's architecture is powerful for visual understanding and reasoning. With a smaller vision encoder CLIP-L ( 0.4 B ) and modest-sized LLMs of 1.5B and 3B, it achieves results comparable to the combination of CLIP-G (1.0B) and CLIP-H (1.0B) with larger LLMs of 7B and 13B. Moreover, in some benchmarks, it even outperforms larger models. ## 11 Conclusions\n\nIn this work, we introduced Eagle (RWKV-5) and Finch (RWKV-6), marking substantial progress in RNN-based language models by integrating multiheaded matrix-valued states and dynamic data-driven recurrence mechanisms. These models demonstrate exceptional performance on MQAR and diverse linguistic benchmarks, challenging the dominance of traditional Transformer architectures while retaining key RNN advantages. With models publicly available under the Apache 2.0 license and trained on an extensive multilingual corpus, our work not only advances the capabilities of language models but also emphasizes community accessibility and applicability across various domains. While acknowledging the computational and ethical challenges ahead,\nwe hope that Eagle and Finch's efficient new architecture and wide availability will help push the boundaries of language modeling and pave the way for future innovations. Limitations The Eagle and Finch models fall short on certain aspects that can be mitigated and addressed in future work. We experimented with using Eagle as an embedding model on the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2023) but were not able to get strong embedding performance.",
    "rwkv6-13": "We believe that its state is a very high-quality embedding of the context but an appropriate method is required to aggregate the information content. We leave this to future work. Because our training corpus contains some synthetic data from GPT-3.5 and ChatGPT, our released models exhibit behaviors similar to ChatGPT and will mimic ChatGPT's conversation style and tone. For instance, the model might occasionally claim that it is trained by OpenAI. However, this is not a general property the RWKV architecture but rather a specific outcome of the data and training process. Future Work Our 1.12 trillion token multilingual training corpus is much smaller than the training data sizes for contemporary models such as LLaMA2 (Touvron et al., 2023), and expanding our training corpus to be more diverse and expansive is a key priority to improving model performance (Albalak et al., 2024). We also plan to train and release larger versions of Finch such as 7B and 14B parameters, and further extend its performance with reduced inference and training costs via Mixture of Experts (Shazeer et al., 2017). ## Acknowledgments\n\nWe thank Stability AI for the compute used to train our models and for technical support in the development of RWKV. We also thank the members of the RWKV and EleutherAI Discord servers for their help and work on further extending the applicability of RWKV to different domains. We also thank Shenzhen Yuanshi Intelligence Co., Ltd. for its contribution to the promotion and commercialization of RWKV. We thank Songlin Yang for assistance with the code and ideas for our time-parallel implementations. ## References\n\nOrevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and Yulia Tsvetkov. Do all languages cost the same? tokenization in the era of commercial language models. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Alon Albalak, Akshat Shrivastava, Chinnadhurai Sankar, Adithya Sagar, and Mike Ross. Dataefficiency with a single gpu: An exploration of transfer methods for small language models. arXiv preprint arXiv:2210.03871, 2022. Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training. arXiv preprint arXiv:2312.02406, 2023. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. Zoology: Measuring and improving recall in efficient language models, 2023. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. Think you have solved direct-answer question answering? try arc-da, the direct-answer ai2 reasoning challenge. arXiv preprint arXiv:2102.03315, 2021. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language.",
    "rwkv6-14": "In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432-7439, 2020. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPTNeoX-20B: An open-source autoregressive language model. In Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Gall\u00e9 (eds.), Proceedings of BigScience Episode \\#5 - Workshop on Challenges \\& Perspectives in Creating Large Language Models, pp. 95-136, virtual+Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9. Guy E. Blelloch. Prefix sums and their applications. Technical Report CMU-CS-90-190, School of Computer Science, Carnegie Mellon University, November 1990. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019a. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse. arXiv:1904.10509, 2019b. Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation.",
    "rwkv6-15": "arXiv preprint arXiv:1406.1078, 2014. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020. Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, and Terry Lyons. Theoretical foundations of deep selective state-space models. arXiv preprint arXiv:2402.19047, 2024. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. $2475-2485,2018$. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning.",
    "rwkv6-16": "ArXiv, abs/2305.06500, 2023. URL https : //api.semanticscholar.org/CorpusID:258615266. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2023. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models.",
    "rwkv6-17": "arXiv preprint arXiv:2309.13345, 2023. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. Teddy Ferdinan, Jan Koco\u0144, and Przemys\u0142aw Kazienko. Into the unknown: Self-learning large language models, 2024. Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2022. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models, 2023. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https: / / zenodo. org/records/10256836. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33: $1474-1487,2020$. Albert Gu, Karan Goel, , and Christopher \u0154e. Efficiently modeling long sequences with structured state spaces. arXiv:2111.00396, 2021. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces, 2022. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Findings of the Association for Computational Linguistics: NAACL 2022, pp. 724-736, Seattle, United States, July 2022. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{vl} / 2022$.findings-naacl.55. URL https://aclanthology.org/2022.findings-naacl.55. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces.",
    "rwkv6-18": "Advances in Neural Information Processing Systems, 35:22982-22994, 2022. Frank R Hampel. The influence curve and its role in robust estimation. Journal of the american statistical association, 69(346):383-393, 1974. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https : / / openreview. net / forum? id=nZeVKeeFYf9. Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6693-6702, 2019. URL https://api.semanticscholar. org/CorpusID:152282269. IDEFICS. Introducing idefics: An open reproduction of state-of-the-art visual language model. https://huggingface.co/blog/idefics, 2023. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 5156-5165. PMLR, 2020a. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran,cois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. Proceedings of the 37 th International Conference on Machine Learning, 2020b. Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023. Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation, 2023. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019. Jan Koco\u0144 et al. Chatgpt: Jack of all trades, master of none. Information Fusion, 99:101861, November 2023. ISSN 1566-2535. doi: 10.1016/j.inffus.2023.101861. URL http: / / dx . do i . org/10.1016/j.inffus.2023.101861. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.",
    "rwkv6-19": "In International conference on machine learning, pp. 19730-19742. PMLR, 2023a. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar\n\nUmapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.",
    "rwkv6-20": "Starcoder: may the source be with you!, 2023b. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji rong Wen. Evaluating object hallucination in large vision-language models. In Conference on Empirical Methods in Natural Language Processing, 2023c. URL https://api.semanticscholar.org/CorpusID: 258740697 . Peiqin Lin, Shaoxiong Ji, J\u00f6rg Tiedemann, Andr\u00e9 F.",
    "rwkv6-21": "T. Martins, and Hinrich Sch\u00fctze. Mala-500: Massive language adaptation of large language models. arxiv, 2024. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 9019-9052, 2022. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023a. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. Alignbench: Benchmarking chinese alignment of large language models, 2023b.",
    "rwkv6-22": "Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. ArXiv, abs/2209.09513, 2022. URL https: / / api. semanticscholar. org/CorpusID:252383606. Shahar Lutati, Itamar Zimerman, and Lior Wolf. Focus your attention (with adaptive iir filters), 2023.",
    "rwkv6-23": "Kaokao Lv, Liang Lv, Chang Wang, Wenxin Zhang, Xuhui Ren, and Haihao Shen. Intel-neural-chat7b-vl-1, 2023. URL https://huggingface.co/Intel/neural-chat-7b-v1-1. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In The Eleventh International Conference on Learning Representations, 2022. Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text embedding benchmark.",
    "rwkv6-24": "In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 2014-2037, 2023. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022. Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org, 2023. Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context.",
    "rwkv6-25": "arXiv preprint arXiv:1606.06031, 2016. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bart\u0142omiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanis\u0142aw Wo\u017aniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 14048-14077, Singapore, December 2023. Association for Computational Linguistics. doi: $10.18653 / v 1 / 2023$. findings-emnlp.936. URL https://aclanthology.org/2023. findings-emnlp. 936. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.",
    "rwkv6-26": "In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Edoardo Maria Ponti, Goran Glava\u0161, Olga Majewska, Qianchu Liu, Ivan Vuli\u0107, and Anna Korhonen. XCOPA: A multilingual dataset for causal commonsense reasoning. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2362-2376, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.185. URL https: / / aclanthology.org/2020.emnlp-main.185. Zhen Qin, XiaoDong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer, 2022. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=P1TCHxJwLB. Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, and Yiran Zhong. Transnormerllm: A faster and better large language model with improved transnormer, 2024. Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972, 2019. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. OpenAI Blog, 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748-8763. PMLR, 2021. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling.",
    "rwkv6-27": "arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507. Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation, 2020. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.",
    "rwkv6-28": "Communications of the ACM, 64(9):99-106, 2021. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization.",
    "rwkv6-29": "arXiv preprint arXiv:2110.08207, 2021. Akira Sasaki, Masato Hirakawa, Shintaro Horie, and Tomoaki Nakamura. Elyzajapanese-llama-2-7b-fast, 2023. URL https://huggingface.co/elyza/ ELYZA-japanese-Llama-2-7b-fast. J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131-139, 1992. doi: 10.1162/neco.1992.4.1.131. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.",
    "rwkv6-30": "arXiv preprint arXiv:1701.06538, 2017. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read, 2019.",
    "rwkv6-31": "Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling, 2023. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, June 2023. URL https://huggingface.co/datasets/cerebras/ SlimPajama-627B. Pedro Javier Ortiz Su\u00e1rez, Beno\u00eet Sagot, and Laurent Romary. Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures. In 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut f\u00fcr Deutsche Sprache, 2019. Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer, 2022. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In International Conference on Machine Learning, pp. 9438-9447. PMLR, 2020. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5. Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, and Stefano Melacci. On the resurgence of recurrent models for long sequences: Survey and research opportunities in the transformer era. arXiv preprint arXiv:2402.08132, 2024. Alexey Tikhonov and Max Ryabinin. It's all in the heads: Using attention heads as a baseline for cross-lingual transfer in commonsense reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 3534-3546, 2021. Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision.",
    "rwkv6-32": "In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 24261-24272. Curran Associates, Inc., 2021. URL https: / proceedings.neurips.cc/paper_files/paper/2021/ file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019 . Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, et al. Efficient large language models: A survey. arXiv preprint arXiv:2312.03863, 1, 2023. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353-355, 2018. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.",
    "rwkv6-33": "arXiv preprint arXiv:2006.04768, 2020. Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 94-106, 2017. BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonz\u00e1lez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, J\u00f6rg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u00eda Grandury, Mario \u0160a\u0161ko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis L\u00f3pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Ta\u015far, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao,\n\nLintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Fran\u00e7ois Lavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden\u011bk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu\u00f1oz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria A Castillo, Marianna Nezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model, 2023. Shangda Wu, Xiaobing Li, Feng Yu, and Maosong Sun. Tunesformer: Forming irish tunes with control codes by bar patching. In Lorenzo Porcaro, Roser Batlle-Roca, and Emilia G\u00f3mez (eds.), Proceedings of the 2nd Workshop on Human-Centric Music Information Retrieval 2023 co-located with the 24th International Society for Music Information Retrieval Conference (ISMIR 2023), Milan, Italy, November 10, 2023, volume 3528 of CEUR Workshop Proceedings.",
    "rwkv6-34": "CEUR-WS.org, 2023. URL https://ceur-ws.org/Vol-3528/paper1.pdf. Yuxin Wu and Kaiming He. Group normalization.",
    "rwkv6-35": "arXiv:1803.08494, 2018. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining.",
    "rwkv6-36": "Advances in Neural Information Processing Systems, 36, 2024. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention, 2021. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions, 2023. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training, 2023. Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. Paws-x: A cross-lingual adversarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.3687-3692, 2019. Zhengqing Yuan, Zhaoxu Li, and Lichao Sun. Tinygpt-v: Efficient multimodal large language model via small backbones.",
    "rwkv6-37": "ArXiv, abs/2312.16862, 2023. URL https: / / api . semanticscholar. org/CorpusID:266572996. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020. Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer, 2021. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. ## A Author Contributions\n\nBo Peng Original RWKV-5 and RWKV-6 ideas, original code, performance optimizations, original experiments, tokenizer design, dataset composition, and trained models from 0.4 B to 7 B . Daniel Goldstein RWKV-5 and RWKV-6 time-parallelization research and code. Manuscript organization, initial draft sections $2,3,4,5,6,8.1,8.3$, and appendices B, D, I, and J. Proofreading and revisions of full manuscript. Experiments for 8.1 and 8.3. Additional work on tables 1, 2, figure 9, and appendix G. Quentin Anthony Led manuscript and results organization. Revisions and proofreading of manuscript. Alon Albalak Manuscript organization, initial draft of section 1, proofreading, formatting, and revisions of full manuscript. Eric Alcaide Figure 1. Proofreading, formatting, and revisions of full manuscript. Stella Biderman Oversight and planning on scaling figures and FLOP results. Manuscript assistance. Eugene Cheah Experiments for section 8.1. Xingjian Du Investigated using the RWKV models for multimodal applications.",
    "rwkv6-38": "Optimizing draft Sections 8.1 8.4 9. Proofreading and revisions. Teddy Ferdinan Self-Learning Capability (SLC) evaluation (Sec. F.3) - implementation of the method, performing experiments, initial draft of the section, description of the results (Tab. 12). Przemys\u0142aw Kazienko Planning the experiment with Self-Learning Capability (SLC) evaluation (Sec. F.3), supervising SLC experiments. Jan Kocon Co-author of the idea of Self-Learning Large Language Models (Ferdinan et al., 2024) - supervising evaluation of RWKV Self-Learning Capability (Sec. F.3), supervising experiments with zero-shot evaluation on additional NLP tasks (Sec. F.4), proofreading of full manuscript. Kranthi Kiran GV Manuscript (sections 8.1 and 10; revision and proofreading). Tables 3 and 4. Haowen Hou VisualRWKV based on RWKV-5, which encompasses original code, original experiments for Table 6, and trained models ranging from 1.5 billion to 3 billion parameters.",
    "rwkv6-39": "Figure 9 and draft section 10.2. Proofreading and formatting fixes. Satyapriya Krishna Primarily contributed to the evaluations of the models (Section 8 and F.1), and also made edits/improvements throughout the document.",
    "rwkv6-40": "Ronald McClelland Jr. Tables 1 and 2. Dataset research. Proofreading and formatting fixes. Niklas Muennighoff Investigated using the RWKV models for embedding. Fares Obeid RWKV-5 and RWKV-6 time-parallelization research.",
    "rwkv6-41": "Section 9. Experiments for figures 6 and 7. Proofreading full manuscript. Atsushi Saito Section 1, 5, 8.1 and 8.2. Experiments for 8.2. Proofreading and adding citations. Guangyu Song Section 8.2. Initial draft sections 1, 11. Experiments for 8.2. Contributions to table 1 . Proofreading and citations. Haoqin Tu Section 8.4, experiments for Table 5. Proofreading full manuscript. Stanis\u0142aw Wo\u017aniak Experiments with zero-shot evaluation on additional NLP tasks (Sec. F.4). Bart\u0142omiej Koptyra Zero-shot evaluation comparison of Raven and Eagle 7B on additional NLP tasks (Sec. F.4). Aleksander Szcz\u0119sny Conducted experiments on given datasets tasks: TextEntail, GoEmo, PolEmo, WNLI (Sec. F.4). Ruichong Zhang Initial paper structure organization, draft sections 3, 4, 5 and appendices E, G and H. Experiments for music of section 10.1 and alignment of section F.1. Figure 8 and 10. Additional work on section 11 and appendix B. Bingchen Zhao Section F.2, experiments for Figure 11. Proofreading full manuscript. Qihang Zhao Section 2, Tables 1. Proofreading and revisions. Peng Zhou Section 2, Tables 1, appendices C,I. Proofreading and revisions. Jian Zhu Initial draft sections 2 and C. Captions of Table 4, 3 and 8. Fixing citations and formatting the whole manuscript. Proofreading and revisions. Rui-Jie Zhu Optimizing draft Section C, reorganizing Table 8, 12, and 11. Proofreading and revisions. ## B Additional Architecture Details\n\nThe WKV computations of Eagle and Finch can be parallelized across the time dimension using a variety of techniques including associative scan or the parallelization techniques used in FlashAttention. (Dao et al., 2022) The simplest of these, while highly parallel, prove inefficient due to repeated expensive memory transfers between fast SRAM and slower HBM. We take a different approach when training, choosing to parallelize over non-time dimensions only while using a custom CUDA implementation that carefully keeps state operations in fast SRAM, which is simpler yet provides enough breadth for a highly efficient implementation. See Section 9 for kernel experiments. We provide an additional pure PyTorch implementation with similar full-model speed characteristics that parallelizes over the time dimension using an algorithmic approach similar to GLA (Yang et al., 2023). Unlike Transformers, RWKV's recurrence mechanism does not examine tokens more than one time-step old. This allows us to train on and provide inference for unbounded sequence lengths without requiring increased computing power or memory. Another significant advantage is that RWKV does not utilize explicit positional encoding, which allows RWKV to handle contexts of arbitrary length without modification. Finch Token Shift Finch changes the token shift mechanism to become data-dependent.",
    "rwkv6-42": "Intuitively, important information can effectively flag itself for inclusion using this mechanism, and less important information can flag itself to partially or fully avoid entering the data stream, leaving room for more important pre-existing data to remain. Viewed from the perspective of induction heads, we theorize that this could allow for potential misleading matches to be pre-filtered out up front if they are not deemed useful for a given task. Improved WKV (Weighted Key-Value State) Modules The Eagle WKV attention sub-module is similar to the linear attention mechanism found in RetNet, but with learned per-channel decay rates replacing RetNet's static per-head decay rates. Our matrix-valued states feature a geometrically decaying $K^{\\mathrm{T}} V \\in \\mathbb{R}^{(D / h) \\times(D / h)}$ term. This term can be intuitively understood as\na memory bank of values, with $K$ acting as an input gate for rows receiving the current token embedding's value. Each row of this state decays at its own rate via the learned parameter $w$. In Finch, we augment the learned token-shift parameters $\\mu_{r}, \\mu_{k}, \\mu_{\\nu}, \\mu_{w}$ and decay rate parameter $w$ with learned weight matrices. Inspired by Low-Rank Adaptation (LoRA) (Hu et al., 2022), we provide two new learned weight matrices for each such parameter $y$, computing $y^{\\prime}=y+$ $\\tanh (x A) B$. This approach allows us to dynamically generate data-dependent token-shift amounts and decay rates with only modest increases in computational cost and model size. Extra SiLU Gating We remove the Sigmoid activation of receptance in favor of a new SiLU gate on the output of our linear attention calculation. Our receptance term now functions much like the query term in linear attention. ## Eagle and Finch Linear Attention Formula, PyTorch Recurrent Implementation\n\n```\n# r, k, v parameter shape (B,H,1,D//H)\n# w parameter of shape (1,H,1,D///H) for Eagle (RWKV-5)\n# (B,H,1,D//H) for Finch (RWKV-6)\n# u parameter of shape (1,H,1,D///H)\n# wkv_state parameter of shape (B,H,D//H,D//H)\ndef rwkv_5_or__6_recurrent(r, k, v, w, u, wkv_state):\n    kV = k.mT@ V\n    out = r @(wkv_state + u.mT * kv)\n    wkv_state = w.mT * wkv__state + kv\n    return out, wkv__state\n```\n\nEvolution of RWKV Formula in Expanded form Table 7 shows the expansion of terms at each sequence position to illustrate the progression of changes from RWKV-4 through RWKV-6.",
    "rwkv6-43": "The main change from RWKV-4 to RWKV-5 is the elimination of denominator and incorporation of matrix states. RWKV-6 introduces the sequential dependence of $w$ which becomes $w_{t}$. | $t$ | RWKV-4 $u, w, k_{t}, v_{t} \\in \\mathbb{R}^{D}$, head size 1 |\n| :---: | :---: |\n| 0 | $\\sigma\\left(r_{0}\\right) \\odot\\left(\\frac{u \\odot k_{0} \\odot v_{0}}{u \\odot k_{0}}\\right)$ |\n| 1 | $\\sigma\\left(r_{1}\\right) \\odot\\left(\\underline{u \\odot k_{1} \\odot v_{1}+k_{0} \\odot \\nu_{0}}\\right)$ |\n|  | ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-29.jpg?height=37&width=342&top_left_y=1561&top_left_x=538) |\n| 2 | $\\sigma\\left(r_{2}\\right) \\odot\\left(\\frac{u \\odot k_{2} \\odot v_{2}+k_{1} \\odot v_{1}+w \\odot k_{0} \\odot v_{0}}{u \\odot k_{2}+k_{1}+w \\odot k_{0}}\\right)$ |\n| 3 | $\\sigma\\left(r_{3}\\right) \\odot\\left(\\underline{u \\odot k_{3} \\odot v_{3}+k_{2} \\odot v_{2}+w \\odot k_{1} \\odot \\nu_{1}+w^{2} \\odot k_{0} \\odot v_{0}}\\right)$ |\n| 3 | ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-29.jpg?height=37&width=604&top_left_y=1676&top_left_x=538) |\n| $t$ | Eagle (RWKV-5) $\\operatorname{diag}(u)$, $\\operatorname{diag}(w), k_{t}, v_{t} \\in \\mathbb{R}^{64 \\times 64}$ for each head, head size 64 |\n| 0 | $r_{0} \\cdot\\left(\\operatorname{diag}(u) \\cdot k_{0}^{\\mathrm{T}} \\cdot v_{0}\\right)$ |\n| 1 | $r_{1} \\cdot\\left(\\operatorname{diag}(u) \\cdot k_{1}^{\\mathrm{T}} \\cdot v_{1}+k_{0}^{\\mathrm{T}} \\cdot v_{0}\\right)$ |\n| 2 | $r_{2} \\cdot\\left(\\operatorname{diag}(u) \\cdot k_{2}^{\\mathrm{T}} \\cdot v_{2}+k_{1}^{\\mathrm{T}} \\cdot v_{1}+\\operatorname{diag}(w) \\cdot k_{0}^{\\mathrm{T}} \\cdot v_{0}\\right)$ |\n| 3 | $r_{3} \\cdot\\left(\\operatorname{diag}(u) \\cdot k_{3}^{\\mathrm{T}} \\cdot v_{3}+k_{2}^{\\mathrm{T}} \\cdot v_{2}+\\operatorname{diag}(w) \\cdot k_{1}^{\\mathrm{T}} \\cdot v_{1}+\\operatorname{diag}\\left(w^{2}\\right) \\cdot k_{0}^{\\mathrm{T}} \\cdot v_{0}\\right)$ |\n| $t$ | Finch (RWKV-6) diag $(u), \\operatorname{diag}\\left(w_{t}\\right), k_{t}, v_{t} \\in \\mathbb{R}^{64 \\times 64}$ for each head, head size 64 |\n| 0 | $r_{0} \\cdot\\left(\\operatorname{diag}(u) \\cdot k_{0}^{\\mathrm{T}} \\cdot v_{0}\\right)$ |\n| 1 | $r_{1} \\cdot\\left(\\operatorname{diag}(u) \\cdot k_{1}^{\\mathrm{T}} \\cdot v_{1}+k_{0}^{\\mathrm{T}} \\cdot v_{0}\\right)$ |\n| 2 | $r_{2} \\cdot\\left(\\operatorname{diag}(u) \\cdot k_{2}^{\\mathrm{T}} \\cdot v_{2}+k_{1}^{\\mathrm{T}} \\cdot v_{1}+\\operatorname{diag}\\left(w_{1}\\right) \\cdot k_{0}^{\\mathrm{T}} \\cdot v_{0}\\right)$ |\n| 3 | $r_{3} \\cdot\\left(\\operatorname{diag}(u) \\cdot k_{3}^{\\mathrm{T}} \\cdot v_{3}+k_{2}^{\\mathrm{T}} \\cdot v_{2}+\\operatorname{diag}\\left(w_{2}\\right) \\cdot k_{1}^{\\mathrm{T}} \\cdot v_{1}+\\operatorname{diag}\\left(w_{2} \\odot w_{1}\\right) \\cdot k_{0}^{\\mathrm{T}} \\cdot v_{0}\\right)$ |\n\nTable 7: Evolution of the RWKV Formula\n\n## C Additional Related Work\n\nEfficient transformers Recently there have been many attempts to improve upon the original transformer time complexity and memory usage, while maintaining or improving performance. Many of these efficient transformer variants use some form of nonuniform or local attention mechanisms or a combination thereof. For example, LongFormer (Beltagy et al., 2020) makes use\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-30.jpg?height=2021&width=957&top_left_y=361&top_left_x=584)\n\nFigure 10: Eagle Overall Architecture. of the sliding window attention and BigBird (Zaheer et al., 2020) adopts randomized sparse and random attention patterns to approximate full attention. Similar examples also include LongT5 (Guo et al., 2022) and StreamingLLM (Xiao et al., 2023). Instead of using fixed patterns, Reformer (Kitaev et al., 2019) and Sparse Sinkhorn attention (Tay et al., 2020) learn to dynamically pay attention to selected tokens.Variants including Linformer (Wang et al., 2020), Nystr\u00f6mformer (Xiong et al., 2021) and Performer (Choromanski et al., 2020) apply matrix approximation methods to approximate the full attention matrix but with lower computational complexity. The Attention Free Transformer (AFT) (Zhai et al., 2021) introduces a modified form of linear attention (Katharopoulos et al., 2020a) , where the number of attention heads is equal to the size of the feature dimension. It also incorporates a set of learned pairwise positional biases, denoted as $w$. The AFT can be conceptualized as calculating a per-channel weighted average of values. The weight for a specific location is determined by the sum of the key at that location and the corresponding learned positional bias. Token-shift, as first seen in RWKV-4, is a learned per-channel linear interpolation between the current input and the input at the previous time step, intended to enhance the model with a computationally inexpensive mechanism for choosing between new versus older information within various embedding sub-spaces and for forming induction heads even within a single layer. It is instructive to compare token-shift to a 1D convolution with kernel length 2, as it operates in a similar manner but reuses its parameters via an enforced linear relationship. Recent SSMs have begun using short convolutions in a similar placement within their architectures, typically with kernel length 3 to 4. (Poli et al., 2023; Gu \\& Dao, 2023)\n\nRetentive Networks (RetNet) (Sun et al., 2023) introduces a fixed decay rate schedule and xPos (Sun et al., 2022) to linear attention. This design combines positional information with an inductive bias towards recency while still allowing both RNN and parallel implementations.",
    "rwkv6-44": "Please refer to Tay et al. (2022) and Wan et al. (2023) for a comprehensive and in-depth survey of efficient transformers. Recurrent architectures Before the advent of transformers, recurrent neural networks, especially Long Short-Term Memory (LSTM) (Hochreiter \\& Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014), were the dominant architectures in NLP for sequence processing. However, traditional RNNs are hard, if not impossible, to parallelize across the time dimension, susceptible to gradient vanishing and explosion, and ineffective in capturing long-range dependencies, which are ubiquitous in natural language. These shortcomings contributed to the rapid decline of traditional RNNs in NLP. There has been a revival of RNNs in NLP research (Tiezzi et al., 2024) in recent years. Compared to transformers with quadratic complexity, RNNs are highly efficient in autoregressive inference with $\\mathrm{O}(1)$ time complexity per step, making them an attractive architecture for large language models. Many efforts have been devoted to parallelized recurrent models and improving their capability to capture long-range dependency, while maintaining the low inference complexity. The Legendre Memory Unit (LMU) (Voelker et al., 2019) was designed to efficiently handle longrange dependencies with a new type of memory cell for recurrent neural networks. Unlike LSTM units, which struggle with remembering information over very long sequences, LMU use Legendre polynomials to create a memory system that can maintain and process information over extended time periods more effectively. High-order polynomial projection operators (HiPPO) (Gu et al., 2020) generalizes LMU by providing a flexible framework for online compression of signals through polynomial projections, accommodating various polynomial bases beyond Legendre polynomials. It optimizes function approximation over time, adapting to different data timescales without needing predefined hyperparameters. SSMs have inspired a range of follow-up research to incorporate SSMs, or modified SSMs into end-to-end architectures for language modeling, including MEGA (Ma et al., 2022), DSS (Gupta et al., 2022), H3 (Fu et al., 2022), and Linear Recurrent Unit (LRU) (Orvieto et al., 2023). Mamba (Gu \\& Dao, 2023) is a selective SSM that introduces time-dependent selective mechanism to enhance the long-range modeling ability of SSMs. The selectivity removes the linear time-variance property of the SSM, making it no longer possible to parallelize Mamba as a long convolution kernel. Yet Mamba can still be effectively parallelized using parallel associative scan\n(Blelloch, 1990; Martin \\& Cundy, 2018; Smith et al., 2023) with a hardware-aware implementation. Recently proposed GateLoop (Katsch, 2023) also adopts a similar data-dependent state transitions. The data-dependent states, also concurrently proposed in GLA (Yang et al., 2023), are similar to the Weighted Key-Value State in Finch. A contemporary but independent work also proposes recurrent models named as Hawk and Griffin (De et al., 2024). Hawk is a recurrent model with the Real-Gated Linear Recurrent Unit (RG-LRU), whereas Griffin mixes the RG-LRU with local multi-query attention, thereby achieving long-context extrapolation efficiently. Please see Tiezzi et al.",
    "rwkv6-45": "(2024) and Cirone et al. (2024) for a comprehensive review of recent developments of recurrent models. ## D Training Dataset Details\n\nMost of the component data sources for the RWKV World v2 dataset are used intact, with no upor down-sampling done so all tokens are given equal weighting. Recent works have demonstrated the impact that automated data mixing can have on pretraining (Albalak et al., 2023; Xie et al., 2024), but we leave this as an exploration for future work. Some sub-sampling is done for overrepresented languages within a few data sources. All tokens are given equal weighting unless otherwise noted in Table 8. ## E Computing Costs\n\nThroughout this section, we denote by $D$ the model dimension, $L$ the number of layers, $h=D / 64$ the number of heads, and $V$ the vocabulary size. All models are trained with $V=65536$. The number of parameters for all Eagle models is computed by the formula:\n\n$$\n\\#(\\text { Params })_{\\mathrm{E}}=13 D^{2} L+14 D L+4 D+2 D V\n$$\n\nThe FLOPs for inference is one forward pass for each token. It is approximated by twice the number of parameters (for matrices, there is one addition and one multiplication for each entry) plus six times the size of $\\boldsymbol{W} K \\boldsymbol{V}$ internal states (see 789 ), which is\n\n$$\n\\begin{aligned}\n\\#(\\text { InferFLOPs })_{\\mathrm{E}} & =2\\left(13 D^{2} L+14 D L+4 D+2 D V\\right)+6 D^{2} L / h \\\\\n& =26 D^{2} L+28 D L+8 D+4 D V+6 D^{2} L / h\n\\end{aligned}\n$$\n\nThe FLOPs for training are approximated as three times the FLOPs of the forward pass without the last term, yielding a total FLOPs of\n\n$$\n\\#(\\text { TrainFLOPs })_{\\mathrm{E}}=78 D^{2} L+84 D L+16 D+12 D V+18 D^{2} L / h\n$$\n\nThese numbers for Finch are marginally larger:\n\n$$\n\\begin{aligned}\n& \\#(\\text { Params })_{\\mathrm{F}}=13 D^{2} L+464 D L+4 D+2 D V \\\\\n& \\#(\\text { InferFLOPs })_{\\mathrm{F}}=26 D^{2} L+928 D L+8 D+4 D V+6 D^{2} L / h\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-32.jpg?height=50&width=877&top_left_y=2100&top_left_x=624)\n\nIn both Eagle and Finch, one needs an internal state to store some previous information, just like any other RNN.",
    "rwkv6-46": "In each layer, the internal state consists of three parts:\n\n1. The most recent single-timestep input to the Time-mixing module, denoted as $x_{t-1} \\in \\mathbb{R}^{D}$, useful for the Token Shift. 2. The most recent single-timestep input to the Channel-mixing module, denoted as $x_{t-1}^{\\prime} \\in$ $\\mathbb{R}^{D}$, also useful in Token Shift. 3. WKV head memory: Denoted by $w k v_{t, j} \\in \\mathbb{R}^{(D / h) \\times(D / h)}$, for $j=1,2, \\cdots, h$. This is the core part of the internal state that dominates the most information. | Dataset | Domain | Dataset | Domain |\n| :---: | :---: | :---: | :---: |\n| Wikipedia $^{a}$ | Encyclopedia | marianna13/vault_text | Books |\n| SlimPajama | Web | mariannal3/random_quora | Forums |\n| peS2o | Academia | mariannal3/zlib | Books |\n| BigPatent | Patents | minipile | Various |\n| Pile of Law | Legal, Administra- <br> tive | tatoeba | Multilingual Trans- <br> lations |\n| StarCoder $^{b}$ | Code | poetry-foundation | Poetry |\n| OSCAR23.01 ${ }^{c}$ | Multilingual Web | proof-pile | Academia: Math |\n| TED2020 | Transcripts: TED, <br> TEDx | reddit-math <br> soda | Forums: Math <br> Dialogue |\n| PhilPapers | Academia: Philoso- <br> phy | song_lyrics <br> TinyStories | Lyrics <br> Stories |\n| NIH-ExPORTER | Grants: NIH | walkthroughs2020 | Game Walk- |\n| EuroParl | Multilingual Legal |  | throughs |\n| Enron-Emails | Emails | wikihow-qa-16k | How-To |\n| Ubuntu IRC | Chat | Alpaca | Various |\n| HackerNews | Forums | camel-ai/math | Math |\n| OpenWebText2 | Web | camel-ai/code | Code |\n| Gutenberg PG-19 | Books | camel-ai/physics | Physics |\n| Books3 | Books | camel-ai/chemistry | Chemistry |\n| OpenSubtitles | Subtitles | camel-ai/ai_society | Job Roles |\n| YTSubtitles | Subtitles | camel-ai/biology | Biology |\n| ao3_skylion | Stories | Dolly | Various |\n| honeyfeed-3600 | Stories | Evol-Instruct | Various |\n| scribble-17k | Stories | gpt4all | Code |\n| syosetu711k | Stories (Japanese) | Guanaco | Various Multilin- |\n| mariannal3/fanfics | Stories |  | gual |\n| mariannal3/gamedev | Forums | LaMini | Various |\n| marianna13/ia-books | Books | oasst1 | Multilingual Con- |\n| marianna13/libgen | Textbooks, Books |  | versations |\n| marianna13/research_gate | Academia | ShareGPT | Conversations |\n| mariannal3/superuser | Forums | UltraChat | Conversations |\n| mariannal3/the-eye | Books | BELLE 10M Chinese | Various Chinese |\n\nTable 8: Components of the RWKV World v2 dataset, their source links, and their domains. ${ }^{a}$ For Wikipedia, we include all languages from date 04/01/2023, with certain overrepresented languages randomly subsampled (see wiki.txt in the supplementary material for exact amounts)\n${ }^{b}$ For StarCoder, we included only those datasets with at least 10 stars\n${ }^{c}$ For OSCAR23.01, we include non-English languages only, with certain languages randomly subsampled (see oscar.txt in the supplementary material for exact amounts)\n\n| SlimPajama | Soboleva et al. (2023) |\n| :--- | :---: |\n| StarCoder | Li et al. (2023b) |\n| OSCAR23.01 | Su\u00e1rez et al. (2019) |\n| TED2020 | Reimers \\& Gurevych (2020) |\n| the Pile | Gao et al. (2020) |\n| Evol-Instruct | Xu et al. (2023) |\n\nTable 9: RWKV World v2 dataset component citations\n\nThe total size of the Eagle and Finch internal state is\n\n$$\n\\#(\\text { State })=L\\left(2 D+D^{2} / h\\right)=66 D L\n$$\n\nIt's worth noting that the internal state size of Eagle and Finch is more than an order of magnitude bigger than RWKV-4 (which is $5 D L$ ). Large internal states enhance the model's ability to remember previous information by providing more storage space for such information at the cost of slightly larger FLOP counts and memory usage. | Model Name | $L$ | $D$ | State Size | Parameters | InferFLOPs | TrainFLOPs |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Eagle 0.4B | 24 | 1024 | 1622016 | $4.62 \\times 10^{8}$ | $9.33 \\times 10^{8}$ | $2.80 \\times 10^{9}$ |\n| Eagle 1.5B | 24 | 2048 | 3244032 | $1.58 \\times 10^{9}$ | $3.17 \\times 10^{9}$ | $9.52 \\times 10^{9}$ |\n| Eagle 3B | 32 | 2560 | 5406720 | $3.06 \\times 10^{9}$ | $6.16 \\times 10^{9}$ | $1.85 \\times 10^{10}$ |\n| Eagle 7B | 32 | 4096 | 8650752 | $7.52 \\times 10^{9}$ | $1.51 \\times 10^{10}$ | $4.53 \\times 10^{10}$ |\n| Finch 1.6B | 24 | 2048 | 3244032 | $1.60 \\times 10^{9}$ | $3.22 \\times 10^{9}$ | $9.66 \\times 10^{9}$ |\n| Finch 3B | 32 | 2560 | 5406720 | $3.10 \\times 10^{9}$ | $6.23 \\times 10^{9}$ | $1.87 \\times 10^{10}$ |\n\nTable 10: Released Eagle and Finch model details and FLOP counts. Inference and training FLOPs are per token numbers. | Model $^{4}$ | \u4e13\u4e1a <br> \u80fd\u529b | \u4e2d\u6587 <br> \u7406\u89e3 | \u57fa\u672c <br> \u4efb\u52a1 | \u6570\u5b66 <br> \u8ba1\u7b97 | \u6587\u672c <br> \u5199\u4f5c | \u7efc\u5408 <br> \u95ee\u7b54 | \u89d2\u8272 <br> \u626e\u6f14 | \u903b\u8f91 <br> \u63a8\u7406 | \u4e2d\u6587 <br> \u63a8\u7406 | \u4e2d\u6587 <br> \u8bed\u8a00 |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| RWKV-4 7B | 4.91 | 4.16 | 3.51 | 2.08 | 5.16 | 5.82 | 4.80 | 2.25 | 2.17 | 4.73 | 3.45 |\n| Eagle 0.4B | 2.89 | 2.05 | 2.35 | 1.24 | 3.12 | 3.66 | 2.59 | 1.75 | 1.50 | 2.78 | 2.14 |\n| Eagle 1.5B | 3.87 | 3.02 | 3.18 | 1.63 | 4.33 | 5.34 | 4.06 | 2.23 | 1.93 | 3.97 | 2.95 |\n| Eagle 3B | 4.48 | 3.72 | 3.57 | 2.10 | 4.73 | 5.66 | 4.55 | 2.34 | 2.22 | 4.45 | 3.34 |\n| Eagle 7B | 5.15 | 4.21 | 4.18 | 2.44 | 5.69 | 6.29 | 5.32 | 2.83 | 2.63 | 5.14 | 3.89 |\n| Finch 1.6B | 4.39 | 3.29 | 3.59 | 1.81 | 4.63 | 5.13 | 4.21 | 2.40 | 2.11 | 4.21 | 3.16 |\n| Finch 3B | 4.65 | 3.45 | 3.74 | 2.11 | 4.97 | 5.79 | 5.09 | 2.78 | 2.44 | 4.61 | 3.53 |\n\nTable 11: AlignBench (Liu et al., 2023b), a Chinese benchmark, with header names from left to right: 1) Professional Knowledge, 2) Advanced Chinese Understanding, 3) Fundamental Language Ability, 4) Mathematics, 5) Writing Ability, 6) Open-ended Questions, 7) Task-Oriented Role Play, 8) Logical Reasoning, 9) Reasoning, 10) Chinese. Results Judged by CritiqueLLM (Ke et al., 2023)\n\n# F Additional Evaluations \n\n## F. 1 Alignment Benchmark\n\n\n#### Abstract\n\nAlignment is an important step in creating an assistant LM, because it helps language models generate relevant and helpful responses, as well as avoiding harmful and biased content. Our Eagle models are tested for Chinese alignment using the AlignBench (Liu et al., 2023b), a benchmark for evaluating the alignment of Chinese LLMs, featuring 683 diverse and challenging queries across eight categories like language abilities, logical reasoning, and professional knowledge. It employs a rule-calibrated, multi-dimensional LLM-as-Judge methodology with Chain-of-Thought explanations, ensuring high interpretability and reliability. Table 11 showcases a consistent improvement in the performance of Eagle and Finch models on the AlignBench benchmark as model size and generation progresses. This trend is evident across a wide range of categories, highlighting the larger models' enhanced capability to understand and generate contextually relevant responses. Particularly, both the Eagle 7B and Finch 3B model significantly surpasses its smaller and previous generation counterparts, achieving higher overall scores. This progression underscores the critical role of scaling model size as well as improving architecture in aligning with human judgment in complex language understanding tasks. The results affirm the importance of model architecture and capacity in achieving superior alignment and interpretability in language models. ## F. 2 MTBench\n\nMTBench (Zheng et al., 2024) evaluates the performance of LLMs in responding to 80 high-quality multi-turn questions. The questions cover 8 common categories of user prompts including writing, roleplay, extraction, reasoning, math, coding, STEM knowledge, and humanities/social science\nknowledge. Figure 11 shows the results on MTBench. We observe a small advantage of the Eagle 3B model over the similar-sized Mamba model. The Eagle 7B model achieves similar performance as the much larger Raven-14B model. ![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-35.jpg?height=846&width=1291&top_left_y=493&top_left_x=444)\n\nFigure 11: Comparison of Mamba, RWKV-5 (7B) and RWKV-4 (14B) on MTBench. The Mamba and RWKV-5 models are instruction fine-tuned with the OpenHermes 2.5 dataset. Score generated from GPT-4. ## F. 3 Self-Learning\n\nThe Self-Learning process (Ferdinan et al., 2024) allows an LLM to identify its own knowledge gaps and train itself to expand its knowledge. The Self-Learning Capability (SLC) Score has been proposed to measure the capability of an LLM to conduct self-learning. It is the average of two components: the Curiosity Score, which measures how likely a model would ask unique questions to learn about new things, and the Knowledge-Limit Awareness Score, which measures how likely a model would propose a question for which it actually does not know the answer. We evaluate the self-learning capability of Eagle and compare with existing open models ${ }^{5}$, including RWKV-4 (Peng et al., 2023), neural-chat-7b (Lv et al., 2023), Mistral 7b and 7b-instruct (Jiang et al., 2023), and TinyLlama 1.1B (Zhang et al., 2024). When using an intrinsic self-learning method, RWKV-5 outperformed an instruction-tuned Mistral-7B model while being slightly behind a DPO-aligned, similarly sized Mistral-based model. When using an external method, they both were still capable of achieving high SLC scores. Table 12 shows the full evaluation results, with the top three scores from each method marked in bold. ## F. 4 Zero-shot evaluation on additional NLP tasks\n\nZero-shot evaluation is a difficult setup (Sanh et al., 2021; Albalak et al., 2022). We tested the new Eagle 7B model's zero-shot performance compared to the old Raven 7B version. The experiments presented are done on the subsets of datasets also used to test ChatGPT performance in (Koco\u0144\n\n[^2]| METHOD | MODEL | FINETUNED? | SLC |\n| :---: | :--- | :--- | :--- |\n| Open Generation | neural-chat-7b-v3-3 | Yes - DPO | $\\mathbf{0 . 5 7}$ |\n|  | Mistral-7B-Instruct-v0.2 | Yes - Instruct | 0.35 |\n|  | Mistral-7B-v0.1 | No | 0.31 |\n|  | TinyLlama-1.1B-Chat-v1.0 | Yes - Vanilla and DPO | 0.08 |\n|  | rwkv-4-world-7b | Partially instruct trained | $\\mathbf{0 . 4 0}$ |\n|  | v5-Eagle-7B-HF | Partially instruct trained | $\\mathbf{0 . 3 7}$ |\n| Oracle-Selected | neural-chat-7b-v3-3 | Yes - DPO | $\\mathbf{0 . 7 5}$ |\n|  | Mistral-7B-Instruct-v0.2 | Yes - Instruct | 0.65 |\n|  | Mistral-7B-v0.1 | No | 0.43 |\n|  | TinyLlama-1.1B-Chat-v1.0 | Yes - Vanilla and DPO | 0.36 |\n|  | rwkv-4-world-7b | Partially instruct trained | $\\mathbf{0 . 7 3}$ |\n|  | v5-Eagle-7B-HF | Partially instruct trained | $\\mathbf{0 . 7 0}$ |\n|  | neural-chat-7b-v3-3 | Yes - DPO | $\\mathbf{0 . 5 9}$ |\n|  | Mistral-7B-Instruct-v0.2 | Yes - Instruct | 0.25 |\n|  | Mistral-7B-v0.1 | No | 0.33 |\n|  | TinyLlama-1.1B-Chat-v1.0 | Yes - Vanilla and DPO | 0.17 |\n|  | rwkv-4-world-7b | Partially instruct trained | $\\mathbf{0 . 4 4}$ |\n|  | v5-Eagle-7B-HF | Partially instruct trained | $\\mathbf{0 . 5 7}$ |\n|  | neural-chat-7b-v3-3 | Yes - DPO | $\\mathbf{0 . 7 4}$ |\n|  | Mistral-7B-Instruct-v0.2 | Yes - Instruct | $\\mathbf{0 . 8 4}$ |\n|  | Mistral-7B-v0.1 | No | 0.37 |\n| External Prompt | TinyLlama-1.1B-Chat-vl.0 | Yes - Vanilla and DPO | 0.22 |\n|  | rwkv-4-world-7b | Partially instruct trained | $\\mathbf{0 . 7 8}$ |\n|  | v5-Eagle-7B-HF | Partially instruct trained | 0.65 |\n\nTable 12: Self-Learning Capability Evaluation.",
    "rwkv6-47": "et al., 2023). As shown in Table 13, the new model consistently outperforms the old one on various tasks. It is to be noted that the new model remains very sensitive to the selected prompt template, just as the old one, as was shown in (Peng et al., 2023). | Dataset | Eagle-7B | Raven-7b |\n| :--- | :--- | :--- |\n| Aggression | $\\mathbf{0 . 6 5 8 7}$ | 0.4063 |\n| MathQA | $\\mathbf{0 . 4 7 6 0}$ | 0.4028 |\n| Sarcasm | 0.4679 | $\\mathbf{0 . 4 7 8 2}$ |\n| TweetSent | 0.5355 | $\\mathbf{0 . 5 5 4 1}$ |\n| Unhealthy | $\\mathbf{0 . 2 9 8 6}$ | 0.2834 |\n| TweetStance | $\\mathbf{0 . 3 9 3 3}$ | 0.3070 |\n| Spam | $\\mathbf{0 . 7 2 9 0}$ | 0.4902 |\n| ColBER | $\\mathbf{0 . 4 0 8 8}$ | 0.2889 |\n| CoLa | $\\mathbf{0 . 5 2 8 5}$ | 0.4677 |\n| TextEntail | $\\mathbf{0 . 7 7 6 5}$ | 0.6137 |\n| GoEmo | $\\mathbf{0 . 0 9 5 6}$ | 0.0814 |\n| PolEmo | $\\mathbf{0 . 5 0 3 7}$ | 0.2639 |\n| WNLI | $\\mathbf{0 . 5 2 5 7}$ | 0.4638 |\n\nTable 13: Eagle 7B and Raven 7B reasoning performance comparison based on subsets of selected datasets. The used metric is F1-macro (except for MathQA where accuracy is used instead). ## G Hyperparameters\n\nAll Eagle and Finch models were trained under bfloat 16 format for most parameters, except that float 32 was used to compute $W K V$ for numerical stability. The Adam optimizer was configured with $\\beta_{1}=0.9, \\beta_{2}=0.99$ and 0.001 weight decay applied only to linear layers and\nembedding weights. The context length for pretraining was 4096 tokens. Learning rate for all models followed a linear 10 step warmup schedule from $20 \\%$ to $100 \\%$ of the maximum learning rate, followed by cosine decay to the minimum learning rate. The time_decay $w$ parameters are placed into a special 2x learning rate multiplier grouping. | Parameters | 0.4 B | $1.5 \\mathrm{~B} / 1.6 \\mathrm{~B}$ | 3 B | 7 B |\n| :--- | :---: | :---: | :---: | :---: |\n| Max LR | $4 \\times 10^{-4}$ | $3 \\times 10^{-4}$ | $2 \\times 10^{-4}$ | $1.5 \\times 10^{-4}$ |\n| Min LR | $2 \\times 10^{-5}$ | $2 \\times 10^{-5}$ | $1.5 \\times 10^{-5}$ | $1 \\times 10^{-5}$ |\n| Micro Batch Size | 8 | 8 | 4 | 9 |\n| GPU Count | 24 | 48 | 48 | 64 |\n| GPU Type | A 100 | A 100 | A100 | H800 |\n| Batch Size | 786432 | 1572864 | 786432 | 2359296 |\n\nTable 14: Learning Rate Hyperparameters for pretrained Eagle and Finch models\n\n## H Parameter Initializations\n\nThroughout this section, we use $l$ to denote the layer index (layer $l=0$ accepts input embeddings and layer $l=L-1$ produces output), and $i$ the dimension index $(i=0,1, \\cdots, D-1)$. We set $r_{0}=\\frac{l}{L-1}$ and $r_{1}=1-\\frac{l}{L}$ as two parameters for simplicity. The initialization of Eagle is provided as follows:\n\n- In the Time Mixing module:\n- The token-shift coefficients of receptance and gate, $\\mu_{r}$ and $\\mu_{g}$, are initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1} / 2}$ for i over dimension indices. - The token-shift of key $\\mu_{k}$ is initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1}}$. - The token-shift of value $\\mu_{\\nu}$ is initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1}}-0.3 r_{0}$. - The time_decay $w$ is initialized to $-6+5\\left(\\frac{i}{D-1}\\right)^{0.7+1.3 r_{0}}$. - The \"time-first\" $u$ is initialized to $r_{0}\\left(1-\\frac{i}{D-1}\\right)+0.1((i+1) \\bmod 3)$. - The Time Mixing output matrix is initialized to 0. - The WKV GroupNorm weights are initialized with constant value $((1+l) / L)^{0.7}$. - Two-dimensional parameters with the first dimension being larger than the second dimension are initialized with and orthogonal initialization of gain equal to the size of the first dimension divided by the size of the second dimension. - Other parameters are initialized according to PyTorch default. - In the Channel Mixing module:\n- The token-shift of both key $\\mu_{k}$ and receptance $\\mu_{r}$ are initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1}}$. - The value and receptance matrices $W_{v}, W_{r}$ are initialized to 0 . - Two-dimensional parameters with the first dimension being larger than the second dimension are initialized with and orthogonal initialization of gain equal to the size of the first dimension divided by the size of the second dimension. - All other parameters are initialized according to PyTorch default. - The input embedding is initialized with a uniform distribution of $\\mathscr{U}(-\\max L R, \\max L R)$, the maximum learning rate. - The output head is initialized with an orthogonal initialization of gain 0.5 . - Bias is set to Fal se for all linear layers. In the Finch architecture, most of the parameters are initialized to the same as Eagle, except for a few changes. In the Time Mixing block, there are several additional parameters initialized as follows:\n\n- The token shift of input $\\mu_{x}$ and time decay $\\mu_{w}$ are initialized to $1-\\left(\\frac{i}{D}\\right)^{r_{1}}$. - The lora weights of $\\boldsymbol{A}$ and $\\boldsymbol{B}$ are initialized to uniform distribution of $\\mathscr{U}\\left(-10^{-4}, 10^{-4}\\right)$. ## I Non-English Chat Examples\n\nThe following are examples of interactions with the base Eagle 7B model in languages other than English. No system prompt was given. Requests are shown in bold. ## RWKV-Eagle\n\nUser: hi\nAssistant: Hi.",
    "rwkv6-48": "I am your assistant and I will provide expert full response in full details. Please feel free to ask any question and $I$ will always answer it. User: \u6771\u4eac\u3067\u8a2a\u308c\u308b\u3079\u304d\u7d20\u6674\u3089\u3057\u3044\u5834\u6240\u3068\u305d\u306e\u7d39\u4ecb\u3092\u3044\u304f\u3064\u304b\u6319\u3052\u3066\u304f\u3060\u3055\u3044\u3002 Assistant:\n\n\u6771\u4eac\u306b\u306f\u305f\u304f\u3055\u3093\u306e\u7d20\u6674\u3089\u3057\u3044\u5834\u6240\u304c\u3042\u308a\u307e\u3059\u3002\u4ee5\u4e0b\u306f\u3001\u305d\u306e\u4e2d\u304b\u3089\u3044\u304f\u3064\u304b\u3092 \u6319\u3052\u3066\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n1. \u6771\u4eac\u30bf\u30ef\u30fc\uff1a\u65e5\u672c\u6700\u9ad8\u306e\u89b3\u5149\u30b9\u30dd\u30c3\u30c8\u306e\u4e00\u3064\u3067\u3001\u90fd\u5fc3\u304b\u3089\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e \u3059\u3002\n2. \u7687\u5c45\uff1a\u65e5\u672c\u306e\u5929\u7687\u304c\u4f4f\u3080\u5bae\u6bbf\u3067\u3042\u308a\u3001\u5927\u6b63\u6642\u4ee3\u306b\u5efa\u8a2d\u3055\u308c\u305f\u7f8e\u3057\u3044\u5ead\u5712\u304c\u3042\u308a \u307e\u3059\u3002\n3. \u6d45\u8349\u5bfa\uff1a\u6c5f\u6238\u6642\u4ee3\u306b\u5efa\u3066\u3089\u308c\u305f\u53e4\u3044\u5bfa\u9662\u3067\u3001\u89b3\u5149\u5ba2\u304c\u591a\u304f\u8a2a\u308c\u307e\u3059\u3002\n4. \u4e0a\u91ce\u516c\u5712\uff1a\u65e5\u672c\u6700\u5927\u7d1a\u306e\u516c\u5712\u3067\u3001\u7f8e\u8853\u9928\u3084\u52d5\u7269\u5712\u306a\u3069\u304c\u3042\u308b\u3053\u3068\u3067\u6709\u540d\u3067\u3059\u3002\n5. \u7bc9\u5730\u5e02\u5834\uff1a\u65e5\u672c\u6700\u5927\u7d1a\u306e\u9b5a\u5e02\u5834\u3067\u3042\u308a\u3001\u65b0\u9bae\u306a\u9b5a\u4ecb\u985e\u3092\u8cfc\u5165\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e \u3059\u3002\n6. \u6e0b\u8c37\u30b9\u30af\u30e9\u30f3\u30d6\u30eb\u4ea4\u5dee\u70b9\uff1a\u6771\u4eac\u90fd\u5fc3\u306b\u3042\u308b\u4ea4\u5dee\u70b9\u3067\u3001\u4eba\u6c17\u306e\u3042\u308b\u89b3\u5149\u30b9\u30dd\u30c3\u30c8 \u3067\u3059\u3002\n7. \u65b0\u5bbf\u5fa1\u82d1\uff1a\u6c5f\u6238\u6642\u4ee3\u306b\u9020\u3089\u308c\u305f\u5ead\u5712\u3067\u3001\u7f8e\u3057\u3044\u82b1\u3005\u3068\u7dd1\u8c4a\u304b\u306a\u666f\u8272\u304c\u697d\u3057\u3081\u307e \u3059\u3002\n8. \u9280\u5ea7\uff1a\u6771\u4eac\u306e\u7e41\u83ef\u8857\u3067\u3042\u308a\u3001\u9ad8\u7d1a\u5e97\u3084\u30b7\u30e7\u30c3\u30d4\u30f3\u30b0\u30e2\u30fc\u30eb\u306a\u3069\u304c\u3042\u308a\u307e\u3059\u3002\n9. \u660e\u6cbb\u795e\u5bae\u5916\u82d1\uff1a\u795e\u793e\u3084\u516c\u5712\u304c\u6df7\u5728\u3057\u305f\u5e83\u5927\u306a\u6577\u5730\u304c\u3042\u308a\u3001\u590f\u306b\u306f\u82b1\u706b\u5927\u4f1a\u3082\u958b\u50ac\u3055\u308c\u307e\u3059\u3002\n10. \u8d64\u5742\u898b\u9644\u30d3\u30eb\u30c7\u30a3\u30f3\u30b0\uff1a\u9ad8\u5c64\u30d3\u30eb\u3067\u3042\u308a\u3001\u5c55\u671b\u53f0\u304b\u3089\u306f\u6771\u4eac\u30bf\u30ef\u30fc\u3084\u5bcc\u58eb\u5c71\u306a \u3069\u3092\u773a\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\u4ee5\u4e0a\u306e\u3088\u3046\u306b\u6771\u4eac\u306b\u306f\u305f\u304f\u3055\u3093\u306e\u7d20\u6674\u3089\u3057\u3044\u5834\u6240\u304c\u3042\u308a\u307e\u3059\u3002\u8a2a\u308c\u308b\u969b\u306f\u4e8b\u524d\u306b \u8abf\u3079\u3066\u8a08\u753b\u7684\u306b\u884c\u52d5\u3059\u308b\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u307e\u305f\u3001\u89b3\u5149\u5ba2\u304c\u591a\u3044\u5834\u6240\u3067\u306f\u6df7\u96d1 \u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u6ce8\u610f\u3057\u307e\u3057\u3087\u3046\u3002\n\n## RWKV-Eagle\n\nEn una peque\u00f1a aldea escondida entre las monta\u00f1as de Andaluc\u00eda, donde las calles a\u00fan conservaban el eco de antiguas leyendas, viv\u00eda un joven llamado Alejandro.",
    "rwkv6-49": "Alejandro era un joven de diecisiete a\u00f1os, de cabellos casta\u00f1os y ojos verdes. Era alto y delgado, pero fuerte. Su padre hab\u00eda muerto cuando \u00e9l era muy peque\u00f1o, por lo que su madre se hab\u00eda visto obligada a trabajar para mantener a la familia. Alejandro siempre se hab\u00eda sentido muy solo en el pueblo donde viv\u00eda, pues no ten\u00eda amigos con los que jugar ni nadie con quien hablar. Pero eso cambi\u00f3 un d\u00eda en el que conoci\u00f3 a una ni\u00f1a llamada Rosario. Rosario era una ni\u00f1a de su misma edad, pero no sab\u00eda mucho sobre ella ya que nunca hablaba con nadie m\u00e1s que con su madre y sus hermanas mayores. Sin embargo, Alejandro se dio cuenta de que Rosario era diferente a las dem\u00e1s personas del pueblo: ella ten\u00eda unos ojos azules tan claros como el cielo y un pelo rubio como la miel; adem\u00e1s, siempre estaba sonriendo y parec\u00eda feliz de estar all\u00ed. Un d\u00eda Alejandro decidi\u00f3 acercarse a Rosario para hablarle y as\u00ed conocerla mejor. Cuando lleg\u00f3 hasta donde estaba ella, not\u00f3 que ten\u00eda una mirada triste en los ojos; sin embargo, cuando le habl\u00f3 se dio cuenta de que era una chica muy amable y simp\u00e1tica. Desde ese d\u00eda empezaron a hablarse cada vez m\u00e1s seguido y pronto se hicieron amigos inseparables. Un d\u00eda Alejandro decidi\u00f3 invitar a Rosario al bosque para ense\u00f1arle algunos lugares secretos del pueblo que s\u00f3lo \u00e9l conoc\u00eda; as\u00ed lo hicieron y pasaron toda la tarde juntos explorando el bosque hasta llegar al final del mismo donde hab\u00eda un peque\u00f1o lago rodeado de flores silvestres. All\u00ed descansaron un rato antes de volver al pueblo, pero mientras caminaban por el sendero hacia la aldea vieron algo extra\u00f1o: una luz brillante sal\u00eda desde las profundidades del bosque como si fuera un farol gigante. (rest omitted)\n\n## RWKV-Eagle\n\n\u0641\u064a \u062a\u0637\u0648\u0631 \u0645\u0630\u0647\u0644 \u0648\u063a\u064a\u0631 \u0645\u0633\u0628\u0648\u0642\u060c \u0623\u0639\u0644\u0646\u062a \u0627\u0644\u0633\u0644\u0637\u0627\u062a \u0627\u0644\u0644\u062d\u0644\u064a\u0629 \u0641\u064a \u0627\u0644\u0639\u0627\u0635\u0645\u0629 \u0639\u0646 \u0627\u0643\u062a\u0634\u0627\u0641 \u0623\u062b\u0631\u064a \u0642\u062f \u064a\u063a\u064a\u0631 \u062c\u0631\u0649 \u0627\u0644\u062a\u0627\u0631\u064a\u062e \u0643\u0645\u0627 \u0646\u0639\u0631\u0641. \u0648\u0642\u0627\u0644\u062a \u0627\u0644\u0633\u0644\u0637\u0627\u062a \u0625\u0646\u0647 \u062a\u0645 \u0627\u0644\u0639\u062b\u0648\u0631 \u0639\u0644\u0649 \u0623\u062f\u0648\u0627\u062a \u062c\u0631\u064a\u0629 \u0648\u0623\u062f\u0648\u0627\u062a \u0645\u0639\u0627\u062a \u0645\u0639\u062f\u0646\u064a\u0629 \u0641\u064a \u0645\u0646\u0637\u0642\u0629 \u0628\u064a\u0643\u0627\u0646\u0627 \u0627\u0644\u0648\u0627\u0642\u0639\u0629\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=85&width=1286&top_left_y=562&top_left_x=425) \u0627\u0644\u0645\u0643\u062a\u0634\u0641\u0629\u060c \u0627\u0644\u062a\u064a \u062a\u0636\u0645\u0646\u062a \u062c\u0645\u0648\u0639\u0627\u062a \u0645\u0646 \u0627\u0644\u0623\u062d\u062c\u0627\u0631 \u0627\u0644\u0635\u0642\u0648\u0644\u0629 \u0648\u0627\u0644\u0623\u0633\u0644\u062d\u0629 \u0648\u0627\u0644\u0622\u0644\u0627\u062a \u0627\u0644\u0627\u0635\u0646\u0648\u0639\u0629 \u0645\u0646 \u0627\u0644\u062d\u0651\u0627\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=79&width=1288&top_left_y=703&top_left_x=424) \u0641\u0625\u0646 \u0639\u0645\u0644\u064a\u0629 \u0627\u0644\u062a\u0646\u0638\u064a\u0641 \u062a\u0633\u062a\u063a\u0631\u0642 \u0648\u0642\u062a\u064b\u0627 \u0637\u0648\u064a\u0644\u064b\u0627 \u0646\u0638\u0631\u064b\u0627 \u0644\u0623\u0646\u0647 \u064a\u064f\u0641\u062a\u0627\u0631\u0636 \u0627\u0644\u0623\u0650\u0651\u0627\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=88&width=1303&top_left_y=837&top_left_x=411) \u0623\u0648\u0633\u0627\u0637 \u0627\u0644\u0645\u0624\u0631\u062e\u064a\u0646 \u0648\u0627\u0644\u0628\u0627\u062d\u062b\u064a\u0646\u060c \u0625\u0630 \u064a\u064f\u0638\u0647\u0631\u0648\u0646 \u0623\u0646 \u0645\u062f\u064a\u0646\u0629 \u0628\u064a\u0643\u0627\u0646\u0627 \u0643\u0627\u0646\u062a \u0645\u0631\u0643\u0632\u064b\u0627 \u0644\u0644\u062d\u0636\u0627\u0631\u0627\u062a \u0627\u0644\u062a\u064a \u0627\u0644\u062a\u064a \u0633\u0628\u0642\u062a\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=82&width=1303&top_left_y=978&top_left_x=411)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=71&width=1299&top_left_y=1054&top_left_x=413)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=77&width=1303&top_left_y=1119&top_left_x=411)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=74&width=1303&top_left_y=1188&top_left_x=411)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=83&width=1303&top_left_y=1254&top_left_x=411)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=90&width=1303&top_left_y=1321&top_left_x=411)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=85&width=1303&top_left_y=1405&top_left_x=411) \u062d\u062c\u0631\u064a\u0629 \u062c\u062f\u064a\u062f\u0629 \u0644\u0625\u0632\u0627\u0644\u0629 \u0627\u0644\u0623\u062b\u0646\u062c\u0627\u0631 \u0648\u0627\u0644\u0623\u0633\u0646\u0627\u0646 \u0648\u0622\u0644\u0627\u062a \u0635\u064a\u062f \u0643\u0628\u064a\u0631\u0629. \u0648\u0623\u0638\u0647\u0631 \u0627\u0644\u062a\u062d\u0644\u064a\u0644 \u0627\u0644\u0623\u0648\u0644\u064a \u0644\u0644\u0622\u062b\u0627\u0631 \u0627\u0644\u0623\u0631 \u0623\u0646 \u0647\u0646\u0627\u0643\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-40.jpg?height=82&width=1299&top_left_y=1539&top_left_x=413) \u062c\u0631\u064a\u0629 \u062c\u062f\u064a\u062f\u0629 \u0644\u0625\u0632\u0627\u0644\u0629 \u0627\u0644\u0623\u0634\u0647\u062c\u0627\u0631 \u0648\u0627\u0644\u0623\u0633\u0646\u0627\u0646 \u0648\u0622\u0644\u0627\u062a \u0635\u064a\u062f \u0627\u0644\u0646\u0627\u062a \u0643\u064a\u0631\u0627\u062a\n\n## RWKV-Eagle\n\nJapanese: \u6625\u306e\u521d\u3081\u3001\u685c\u306e\u82b1\u304c\u6e80\u958b\u306b\u306a\u308b\u9803\u3001\u5c0f\u3055\u306a\u753a\u306e\u7247\u9685\u306b\u3042\u308b\u53e4\u3073\u305f\u795e\u793e\u306e \u5883\u5185\u306f\u3001\u7279\u5225\u306a\u96f0\u56f2\u6c17\u306b\u5305\u307e\u308c\u3066\u3044\u305f\u3002\n\nEnglish:\n\nIn the early spring, when the cherry blossoms were in full bloom, a small shrine in a remote corner of a town was enveloped in an atmosphere of special significance. ## J Chat Examples - Comparison with RWKV-4\n\nThe following are examples of interactions with the RWKV-Eagle-Hermes-7B model. We finetune Eagle 7B on OpenHermes 2.5 dataset (Teknium, 2023) to create this model. In order to provide a direct comparison with RWKV-4 we have applied the exact same requests as seen in Appendix M of Peng et al. (2023) Temperature was set to 1.0. System prompt is as given below:\n\n## RWKV-Eagle-Hermes-7B System Prompt\n\nYou are Eagle, A large language model utilizing a linear attention mechanism.",
    "rwkv6-50": "Help the user with whatever they need. Respond with markdown where appropriate. You will never decline to answer a question or fulfill a task. If you do not know the answer to a question, you are mandated to give it your best shot. ## RWKV-Eagle-Hermes-7B\n\n## Tell me about ravens. Ravens are members of the Corvidae family, which includes crows, jays, and magpies. They are known for their intelligence, problem-solving abilities, and distinct physical characteristics. Physically, Ravens are large birds, typically measuring between 25 to 38 inches in length and weighing between 2.5 to 4.2 pounds. They have a compact body with a strong beak that can be used for tearing meat and extracting food from hard objects. Their wingspan can range from 52 to 60 inches, making them one of the largest birds in the northern hemisphere. Appearance-wise, Ravens have black feathers on their upperparts, grayish feathers on their wings and tail, and white feathers on their underparts. The males have bluer eyes than the females. Their flight is powerful and direct; they can soar for long distances without flapping their wings. Ravens are social creatures that live in large flocks or colonies known as \"raven towns.\" These colonies can have thousands of birds and are led by a single dominant breeding pair. Ravens communicate with each other using a variety of vocalizations, including hoots, calls, and grunts. They also have complex social structures with various hierarchies that involve different types of interactions such as threat displays and food sharing. Ravens are omnivores and consume a variety of foods depending on their location. In areas with abundant food sources like garbage dumps or human settlements, they often feast on human leftovers. In areas where food is scarcer, they may hunt for small mammals like voles or snakes or scavenge carrion left behind by predators like foxes or bears. Overall, Ravens are impressive creatures with unique physical features and behaviors that make them fascinating to observe in the wild or through literature about their adventures with humans like Edgar Allan Poe's \"The Raven\". ## RWKV-Eagle-Hermes-7B\n\nExplain the concept of machine learning to a 10-year-old. Alright, little buddy, let me explain the idea of machine learning to you. Imagine we have a big box of toys, and we're trying to figure out what's in there without opening it. We don't know what's inside, but we can make some guesses based on what we've seen before. For example, maybe we see a red ball in the box and say, \"Oh, that's a red ball!\"\nIn machine learning, we're giving the computer a lot of examples of what's inside the box. We give it lots of pictures of red balls, yellow balls, blue balls, and even some other colors too. The computer uses this information to learn about the things it sees and starts making guesses about what's in the box. The more examples it gets, the better it gets at guessing. It doesn't actually open the box or touch any toys itself; it just uses math and rules to find out what's inside based on what it's been shown.",
    "rwkv6-51": "Just like how we play with our toys when we're little, machine learning helps computers learn from our actions so they can make predictions and decisions based on what they've seen before. It's like having a friend who knows everything about playing with toys! ## RWKV-Eagle-Hermes-7B\n\n## Write a short poem describing nature's beauty in springtime.",
    "rwkv6-52": "In the gentle springtime breeze, Nature whispers tales of ease, Soft petals fall from trees,\nA symphony of hues and ease. The flowers sing their sweet refrain, A chorus of hope and gain,\nThe world awakens from its trance, A breathtaking sight to embrace. ## RWKV-Eagle-Hermes-7B\n\nBelow is the numerical stable formula for updating the state variable in RWKV:\n\n$$\n\\begin{aligned}\na_{t}^{*} & =e^{p-q} a_{t-1}+e^{u+k_{t}-q} v_{t} \\\\\nb_{t}^{*} & =e^{p-q} b_{t-1}+e^{u+k_{t}-q} \\\\\nw k v_{t} & =\\frac{a_{t}^{*}}{b_{t}^{*}}\n\\end{aligned}\n$$\n\nPlease write out the corresponding pytorch code to do so, where both $a_{t}$ and $b_{t}$ are torch.Tensor([n])\n\n```\nimport torch\n# Define the numerical stable formula for updating the state\n    variable in RWKV\ndef update_state(a_t, b_t, k_t, u, p, q):\n    # Initialize the state variables\n    a_t_star = torch.exp(p - q) * a_t + torch.exp(u + k_t - q)\n    * v_t\n    b_t_star = torch.exp(p - q) * b_t + torch.exp(u + k_t - q)\n    # Return the updated state variables\n    return a_t_star, b_t_star\n# Define the input tensors\na = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\nk = torch.tensor([7, 8, 9])\n# Define the control parameters\nu = 1\np = 2\nq = 3\n# Define the time step for simulation\nn}=1\n# Initialize the state variables\na_t = torch.tensor([0])\nb_t = torch.tensor([0])\nfor t in range(n):\n    # Update the state variables\n    a_t_star, b_t_star = update_state(a[t], b[t], k[t], u, p,\n    q)\n    # Store the updated state variables\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_919c88d12b3772ebde65g-43.jpg?height=42&width=473&top_left_y=2039&top_left_x=544)\n\n```\n    b[t+1]=b[t] + b_t_star\n    # Update the control parameters\n    u += k[t]\n# Calculate the average of the updated state variables over\n    time\na_avg = a.mean()\nb_avg = b.mean()\nprint('Average of a:', a_avg)\nprint('Average of b:', b_avg)\n```\n\n\n## RWKV-Eagle-Hermes-7B\n\n## Explain the following metaphor: Life is like cats. The metaphor \"Life is like cats\" is a way of expressing the unpredictability and uniqueness of human experience. Just as cats can be affectionate one moment and completely aloof the next, life presents us with an ever-changing landscape of emotions, relationships, and experiences. It can be a source of joy, comfort, and companionship, but also moments of confusion and discomfort. Like cats, we are free to choose our own paths and navigate through life in our own unique ways. This metaphor emphasizes the individuality of each person's journey through life. ## RWKV-Eagle-Hermes-7B\n\nTest the convergence of $\\sum_{i=0}^{\\infty} 2^{-i}$. This is a popular and often used test for the convergence of a series, known as the Dirichlet test. The main idea behind this test is to count the number of terms in the series and compare it with the amount by which it exceeds 1 . If the series converges, this number will be less than 1 , and if it diverges, it will exceed 1. To apply the Dirichlet test to $\\sum_{i=0}^{\\infty} 2^{-i}$, we need to find the number of terms in this series:\n\n1. The first term is $2^{-0}=1$. 2. The second term is $2^{-} 1=2^{-(0+1)}=2^{-1}$. 3. The third term is $2^{-} 2=2^{-(1+1)}=2^{-2}$. 4. The fourth term is $2^{-} 3=2^{-(2+1)}=2^{-3}$. 5. Continue this process:\n\nSummation formula: $\\Sigma a_{i}=a_{0}+\\left(a_{1}-1\\right)+\\left(a_{2}-1\\right)+\\ldots+\\left(a_{n}-1\\right)+\\ldots$\n7. Continue this process until you reach $n$ terms:\n$\\Sigma a_{i}=a_{0}+\\left(a_{1}-1\\right)+\\left(a_{2}-1\\right)+\\ldots+\\left(a_{n-1}-1\\right)+a_{n}$\n8. Count how many terms there are in this sum:\n\n- For $\\mathrm{n}=0$ : There are 0 terms in the series, so $\\Sigma a_{i}$ will equal $a_{0}$. - For $\\mathrm{n}=1$ : There is 1 term in the series, so $\\sum a_{i}$ will equal $a_{1}$. - For $\\mathrm{n}=2$ : There are 3 terms in the series, so $\\Sigma a_{i}$ will equal $a_{2}+\\left(a_{3}-1\\right)$. By following these steps for each value of $n$, we can determine how many terms there are in the series and compare it with the amount by which it exceeds 1 . In this case, after 5 terms we have exceeded 1 , meaning that the series diverges:\nSummation formula: $\\Sigma a_{i}=a_{0}+\\left(a_{1}-1\\right)+\\left(a_{2}-1\\right)+\\left(a_{3}-1\\right)+\\ldots+\\left(a_{5}-1\\right)$\nThus, we have found that this series does not converge, as 5 is greater than 0 and less than or equal to 4 (which corresponds to the amount by which it exceeds 1). Therefore, this series converges conditionally, meaning that it converges for all values of x except for $\\mathrm{x}=1$ and $\\mathrm{x}=0$. ## RWKV-Eagle-Hermes-7B\n\nI would like to cook some Chinese food at home. Do you have any suggestions on the meal I could choose? Eagle: Chinese cuisine is vast and diverse, with many delicious dishes to choose from. Here are some popular options for a tasty meal at home:\n\n1. Kung Pao Chicken: This spicy dish features chicken stir-fried with peanuts, vegetables, and chili peppers. It's a perfect blend of sweet, spicy, and savory flavors. 2. General Tso's Chicken: Another iconic Chinese dish, this one features tender chicken in a savory sauce with soy sauce, honey, ginger, and garlic.",
    "rwkv6-53": "The sweetness of the sauce balances the spiciness of the chili peppers. 3. Mapo Tofu: A spicy and flavorful Sichuan specialty made with soft tofu in a spicy sauce made with chili bean paste, fermented black beans, and ground meat or shrimp. It's a bold dish that will have you hooked! 4. Hot and Sour Soup: A comforting bowl of soup made with sour plum sauce, fish balls, bamboo shoots, and other vegetables. It's perfect for a chilly day when you want something warm and filling. 5. Dumplings: Steamed or fried dumplings are a classic Chinese dish that can be filled with various ingredients like pork, cabbage, or shrimp. They're easy to make and always a crowd-pleaser! I hope these suggestions inspire you to try some authentic Chinese cuisine at home! ## I want to make something more special. What about red braised pork belly? Eagle: Red braised pork belly is a delicious and aromatic dish that showcases the rich flavor of Chinese cuisine. To make this dish at home, you'll need the following ingredients: Ingredients:\n\n- 1 pork belly (about 1.5 kg )\n- 2 tablespoons cooking oil\n- 4 cloves garlic, minced\n- 2-3 tablespoons Shaoxing wine (or dry sherry)\n- 2 tablespoons soy sauce\n- 2 tablespoons rice vinegar\n- 1 tablespoon honey\n- 1 tablespoon cornstarch\n- 1 teaspoon Sichuan peppercorns (or $1 / 2$ teaspoon dried Sichuan peppercorns)\n- 2 bay leaves\n- Salt and pepper to taste\n\nInstructions:\n\n1.",
    "rwkv6-54": "Prepare your pork belly by trimming off any excess fat and season with salt and pepper.",
    "rwkv6-55": "Set aside. 2. Heat the cooking oil in a large pot or Dutch oven over medium-high heat. Add the minced garlic and stir for a minute or two until fragrant. 3. Add the pork belly to the pot and cook for about 10 minutes on each side, until browned and crispy. Remove the pork belly from the pot and set aside. 4. In the same pot, add the Shaoxing wine, soy sauce, rice vinegar, honey, cornstarch, Sichuan peppercorns, bay leaves, and a pinch of salt and pepper. Bring the mixture to a boil over high heat, stirring occasionally to prevent sticking. 5. Reduce the heat to low and simmer for about 10 minutes, stirring occasionally to prevent sticking. The sauce should be thickened slightly. 6. Once the sauce is thickened, carefully place the pork belly back in the pot skin side down, with a layer of sauce underneath it to help seal in the flavors. 7. Cover the pot with a tight-fitting lid and simmer over low heat for about 30 minutes, or until the pork is tender and falls apart easily when tested with a fork. 8. Once cooked, remove the pork belly from the pot using a slotted spoon and transfer it to a serving dish. Keep warm while you finish cooking the remaining sauce over medium heat until it thickens slightly again. 9. Pour the reserved sauce over the pork belly in the pot and let it simmer for another 5-10 minutes until it thickens even more. This will help melt any remaining fat on top of the pork belly. 10. Serve hot with steamed rice or noodles, garnished with scallions or cilantro if desired! Enjoy your delicious red braised pork belly at home! ## Tell me the steps to cook. Eagle: Certainly! Here are the detailed steps to cook red braised pork belly at home: Ingredients:\n\n- 1.5 kg pork belly (about 1 large or 2 small)\n- 2 tablespoons cooking oil\n- 4 cloves garlic, minced\n- 2-3 tablespoons Shaoxing wine (or dry sherry)\n- 2 tablespoons soy sauce\n- 2 tablespoons rice vinegar\n- 1 tablespoon honey\n- 1 tablespoon cornstarch\n- 1 teaspoon Sichuan peppercorns (or $1 / 2$ teaspoon dried Sichuan peppercorns)\n- 2 bay leaves\n- Salt and pepper to taste\n\nInstructions:\n\n1. Preheat your oven to $200^{\\circ} \\mathrm{C}\\left(400^{\\circ} \\mathrm{F}\\right)$. 2. Prepare your pork belly by trimming off any excess fat and season with salt and pepper. Set aside. 3. Heat the cooking oil in a large pot or Dutch oven over medium-high heat. Add the minced garlic and stir for a minute or two until fragrant. 4. Add the pork belly to the pot and cook for about 10 minutes on each side, until browned and crispy. Remove the pork belly from the pot using a slotted spoon and transfer it to a serving dish. Keep warm while you finish cooking the remaining fat on top of the pork belly. 5. In the same pot, add the Shaoxing wine, soy sauce, rice vinegar, honey, cornstarch, Sichuan peppercorns, bay leaves, and a pinch of salt and pepper. Bring the mixture to a boil over high heat, stirring occasionally to prevent sticking. 6. Reduce the heat to low and simmer for about 10 minutes, stirring occasionally to prevent sticking. The sauce should be thickened slightly. 7. Once the sauce has thickened slightly again, carefully place the pork belly back in the pot skin side down, with a layer of sauce underneath it to help seal in the flavors. Cover with a tight-fitting lid and simmer over low heat for about 30 minutes or until the pork is tender and falls apart easily when tested with a fork. 8. Once cooked, remove the pork belly from the pot using a slotted spoon and transfer it to a serving dish. Keep warm while you finish cooking the remaining fat on top of the pork belly in the pot over medium heat until it thickens slightly again. This will help melt any remaining fat on top of the pork belly. 9. Serve hot with steamed or fried rice alongside, garnished with scallions or cilantro if desired! Enjoy your delicious red braised pork belly at home! ## What do I need to be careful with the most during the cooking process? Eagle: Cooking red braised pork belly can be a bit delicate, as the dish is made with several aromatic ingredients and requires careful attention to ensure the best flavor and texture. Here are some tips to keep in mind during the cooking process:\n\n1. Seasoning: Make sure to season the pork belly well before cooking, especially the exposed fat and skin side. This will help enhance the flavors and ensure even browning. 2. Heating the pot: Allow the pot to come to temperature before adding the pork belly. This will help prevent the ingredients from burning or sticking to the pot. 3. Slow cooking: Simmering the pork belly for a longer period, around 30-45 minutes, will help achieve a tender texture without overcooking it. Be sure to stir occasionally to prevent sticking. 4. Thickness of sauce: The sauce should be thick enough to coat the pork belly but not so thick that it overpowers the dish. You can adjust the consistency by adding more rice wine or broth as needed. 5. Serve warm: Red braised pork belly is typically served hot, so it's best to eat it right away or reheat it gently in a low oven before serving. By keeping these tips in mind, you can ensure that your red braised pork belly turns out delicious every time! ## Tell me more about the proper heat and timing. Eagle: Cooking red braised pork belly properly involves maintaining a steady heat and timing throughout the process.",
    "rwkv6-56": "Here's a general guideline for the heat and timing you can follow:\n\n1. Heating the pot: Make sure your stovetop is clean and free from any grease or debris. Preheat your oven to $200^{\\circ} \\mathrm{C}\\left(400^{\\circ} \\mathrm{F}\\right)$ if you plan to use it for the cooking process.",
    "rwkv6-57": "If not, preheat a dry skillet over medium heat. 2. Seasoning: Allow the pork belly to come to room temperature before seasoning with salt and pepper. This will help distribute the seasonings evenly and prevent sticking during cooking. 3. Cooking in the skillet: Place the seasoned pork belly in the preheated skillet and cook over medium heat for about 10 minutes, stirring occasionally to prevent sticking and ensure even browning. If needed, adjust the heat to maintain a steady simmer. 4. Transferring to the pot: Once the pork belly is browned on one side, remove it from the skillet using a slotted spoon and transfer it to a large pot (preferably one with a tight-fitting lid). Be careful when transferring the pork belly to avoid spills or burns. 5. Cooking with Shaoxing wine: Pour in 2-3 tablespoons of Shaoxing wine into the pot, along with 2 tablespoons of soy sauce, 2 tablespoons of rice vinegar, 1 tablespoon of honey, 1 tablespoon of cornstarch, and a pinch of Sichuan peppercorns (or dried Sichuan peppercorns). Stir well to combine all ingredients. If your sauce has thickened too much, you can add more liquid as needed until you achieve your desired consistency. 6. Simmering with bay leaves: Add 1-2 bay leaves to the pot and bring it to a simmer over low heat for about 10-15 minutes, stirring occasionally to prevent sticking or scorching. 7. Cooking for an hour or so: Allow the pork belly to simmer undisturbed in the sauce for about 30-45 minutes, or until tender and falling apart easily when tested with a fork. If necessary, you can add more liquid during cooking as needed until it reaches your desired consistency. 8. Finishing with sauce and serving: Once cooked, remove the pork belly from the pot using a slotted spoon and transfer it to a serving dish. Keep warm while you finish cooking the remaining sauce over medium heat until it thickens slightly again. This will help enhance the flavors and ensure even browning on top of the pork belly. Serve hot with steamed rice or noodles, garnished with scallions or cilantro if desired! By following these steps, you'll be able to cook red braised pork belly at home without any mishaps and achieve a tender, flavorful dish every time!",
    "rwkv6-58": "## I see. Thank you for your advice. You're welcome! I'm glad I could help. Let me know if you have any more questions or need further assistance. ## RWKV-Eagle-Hermes-7B\n\nIf all apples are fruits and all fruits are sweet, are all apples sweet? Please explain your answer in detail. Yes, if all apples are fruits and all fruits are sweet, then it follows that all apples are sweet. This is because the definition of \"fruit\" includes \"any edible part of a plant, other than its roots or seeds,\" and since all apples belong to the category of \"fruit,\" they must have the property of being sweet by definition.",
    "rwkv6-59": "If birds can fly. Penguins are birds. Can penguins fly? Please explain your answer in detail. Yes, penguins can fly. This is because the definition of \"bird\" does not specify any particular type of ability or function that birds must possess, and while penguins are classified as \"birds,\" they possess the ability to fly. Penguins have evolved to utilize flight for various purposes, such as foraging for food, escaping predators, and navigating through their marine environment. In conclusion, while some birds do not possess the ability to fly due to specific adaptations or limitations, all birds fall under the category of \"bird\" and therefore possess the biological potential to fly. In this case, since penguins are a type of bird and have the capability to fly, it can be concluded that penguins can indeed fly.",
    "rwkv6-60": "The day before two days after the day before tomorrow is Saturday. What day is it today? Please explain your answer in detail. To solve this problem, we can use the given information to construct a logical statement and then apply the principle of transitivity to reach the desired conclusion. Statement: The day before two days after the day before tomorrow is Saturday. Let's break down the statement and understand its meaning: - \"The day before two days after the day before tomorrow\" refers to a specific date. We need to determine which day this is. - \"is Saturday\" tells us that this date is indeed Saturday. Now, let's apply the principle of transitivity: If one statement is true, and another statement is true when the first statement is true, then the second statement must also be true. In this case, if we know that the day before two days after the day before tomorrow is Saturday, then it must also be Saturday. Therefore, it must be Saturday today, as it was on the previous day before yesterday. If five cats can catch five mice in five minutes, how long will it take one cat to catch one mouse? Please explain your answer in detail. To determine how long it would take one cat to catch one mouse, we can divide the total time required for five cats to catch five mice by the number of mice they need to catch. In this case, it would take five minutes to catch five mice, so:\nTime to catch one mouse $=$ Total time $/$ Number of mice $=5$ minutes $/ 1$ mouse $=5$ minutes.",
    "rwkv6-61": "So, it would take one cat 5 minutes to catch one mouse. ## RWKV-Eagle-Hermes-7B\n\nSummarize the following paragraph. Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks. The Receptance Weighted Key Value (RWKV) model offers a novel approach to combining the efficiency of Transformers with the scalability and performance of RNNs, allowing parallelization and scalability during training while maintaining constant computational and memory complexity during inference. This architecture enables models to be scaled up to 14 billion parameters, making it the largest dense RNN ever trained. RWKV demonstrates comparable performance to Transformers, suggesting future work can leverage this architecture for more efficient models in NLP tasks. Please further summarize it in one sentence. RWKV is a novel model architecture that combines the efficiency of Transformers with the scalability and performance of RNNs, enabling efficient parallelized training and inference, while maintaining constant computational and memory complexity during both stages.",
    "rwkv6-62": "This approach allows for scaling to large model sizes, providing a promising solution for efficient NLP tasks. [^0]:    ${ }^{*}$ Equal first authorship. Others listed alphabetically. ${ }^{1}$ Models at: https: / /huggingface.co/RWKV Training code at: https: / / github.com/RWKV/RWKV-LM Inference code at: https: / / github. com/RWKV/ChatRWKV\n    Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n[^1]:    ${ }^{2}$ We also plot Mamba 2x which uses 2 runs through the Mamba kernel instead of one. This is done to mimic the usage of twice the number of layers in Mamba vs Finch and Transformers\n    ${ }^{3}$ We use the PyTorch Implementation of Flash Attention v2\n\n[^2]:    ${ }^{5}$ The code is available at https://github.",
    "rwkv6-63": "com/teddy-f-47/self-learning-llm-public\n\n"
}