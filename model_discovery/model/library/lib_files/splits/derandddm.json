{
    "derandddm-0": "# Fast Sampling via Discrete Non-Markov Diffusion Models \n\nZixiang Chen* and Huizhuo Yuan ${ }^{\\dagger}$ and Yongqian $\\mathrm{Li}^{\\ddagger}$<br>Yiwen $\\mathrm{Kou}^{8}$ and Junkai Zhang $\\mathbb{I}^{\\mathbb{I}}$ and Quanquan Gull\n\n\n#### Abstract\n\nDiscrete diffusion models have emerged as powerful tools for high-quality data generation.",
    "derandddm-1": "Despite their success in discrete spaces, such as text generation tasks, the acceleration of discrete diffusion models remains under explored. In this paper, we propose a discrete non-Markov diffusion model, which admits an accelerated reverse sampling for discrete data generation. Our method significantly reduces the number of function evaluations (i.e., calls to the neural network), making the sampling process much faster. Furthermore, we study the transition from finite to infinite step sampling, offering new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models. Extensive experiments on natural language generation and machine translation tasks demonstrate the superior performance of our method in terms of both generation speed and sample quality compared to existing methods for discrete diffusion models. ## 1 Introduction\n\nDiffusion-based generative models, as first introduced by Sohl-Dickstein et al. (2015), have shown remarkable capabilities in generating high-quality samples across various domains, including images (Ho et al., 2020; Song and Ermon, 2020), audio (Chen et al., 2020, Kong et al., 2020), and videos (Ho et al, 2022). The diffusion model utilizes an innovative approach comprising a forward process that gradually transforms training data into pure noise and a reverse process that reconstructs clean data from the noise. Throughout the training phase, the model optimizes a neural network by minimizing an objective derived from maximum likelihood estimation. Once trained, the model can generate samples using various decoding strategies, including implicit dynamics (Song et al., 2020a), analytical processes (Bao et al., 2022), or differential equation solvers Song et al., 2020b; Liu et al.,\n\n[^0]2022, Lu et al., 2022). In particular, Song et al. (2020a) introduced the denoising diffusion implicit model (DDIM), providing a non-Markov and de-randomized version of the Denoising Diffusion Probabilistic Model (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020), which enables faster generation of high-quality samples. Although diffusion models were initially introduced for both discrete and continuous-state spaces (Sohl-Dickstein et al., 2015), these studies have largely focused on Gaussian diffusion processes in continuous-state spaces. Recently, Discrete Denoising Diffusion Probabilistic Models (D3PMs) (Austin et al. 2021) working in discrete-state spaces have gained increasing interest due to their applications in diverse areas such as text generation (Hoogeboom et al., 2021b), medical record generation (Ceritli et al., 2023), and protein design (Gruver et al., 2024). These models, which are distinct from their Gaussian counterparts, employ discrete noises, such as the multinomial distribution, for diffusion processes. Very recently, Zheng et al. (2023) introduced a reparameterized diffusion model (RDM) that can improve sampling speed and sample quality in text generation tasks. However, their proposed algorithm is a training-based approach. Compared with diffusion models using Gaussian noise, discrete diffusion models remain under-studied, especially regarding training-free sampling acceleration. In this work, we introduce a training-free approach aiming at enhancing the sampling speed of discrete diffusion models. This approach stems from a unique characteristic of discrete diffusion models: unlike continuous diffusion models, which typically employ Gaussian noise for data corruption (Ho et al., 2020; Song and Ermon, 2020; Song et al., 2020b a), discrete diffusion models often use categorical white noises (Hoogeboom et al., 2021b; Austin et al., 2021; Zheng et al., 2023). By delving into this special property, we develop a discrete non-Markov diffusion model, together with a design of accelerated algorithm. Notably, this new sampling technique does not require any modifications to the training\n\nTable 1: Cross Comparison of Diffusion Models. |  | Continuous | Discrete |\n| :---: | :---: | :---: |\n| Markov | DDPM | D3PM |\n|  | (Sohl-Dickstein et al., 2015) | Austin et al. |\n| (2021) |  |  |\n\nobjective of diffusion models and is, therefore, training-free. Our contributions are summarized as follows:\n\n- We propose discrete non-Markov diffusion models (DNDM), which preserve two important properties of the original discrete Markov diffusion model. Firstly, for any diffusion trajectory $\\left\\{\\mathbf{x}_{t}\\right\\}$ starting from real data $\\mathbf{x}_{0}$, DNDM provably preserves the marginal distribution $q\\left(\\mathbf{x}_{t}\\right)$ and the conditional distribution $q\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{t}\\right)$. Secondly, the proposed DNDM model naturally induces a set of latent variables $\\mathcal{T}$, termed as the transition time set. A predetermined transition time set allows us to develop a training-free sampling algorithm that can accelerate a large family of discrete diffusion models, including the two most widely used discrete diffusion models: multinomial diffusion (Hoogeboom et al., 2021b) and absorbing diffusions Austin et al. 2021). DDIM is non-Markov and admits a de-randomized, faster sampling algorithm compared to DDPM. In this context, DNDM can be considered as the discrete counterpart of DDIM (See Table 1). - Based on the DNDM, we design an accelerated sampling algorithm that reduces the required number of neural network function evaluations. In a standard $T$ time-step discrete diffusion process,\nwhile D3PM, including Multinomial Ho et al. 2020) and absorbing state discrete sampling Austin et al. 2021), requires evaluating the neural network function $T$ times, our approach only requires $|\\mathcal{T}|$ function evaluations, where $|\\mathcal{T}|$ is the cardinality of the transition set $\\mathcal{T}$. Moreover, $|\\mathcal{T}|$ is provably less than $T$ and approaches $O(1)$ as $T$ goes to infinity. We provide both theoretical analysis and empirical experiments showing that the improvement in the number of function evaluations (NFE) is significant. Notably, our algorithm is about $3 \\times$ faster than baselines for $T=50$ and about $30 \\times$ faster for $T=1000$ while preserving the sample quality. - To further illustrate the effectiveness of DNDM, we explore the limit as $T \\rightarrow \\infty$ and introduce an infinite-step sampling algorithm. With a pretrained neural network, we can generate an initial noise $\\mathbf{x}_{T}$ and a transition time set $\\mathcal{T} \\subseteq[0,1]$ with infinitesimal spacing, such that $|\\mathcal{T}|=O(1)$. This enables the generation of the real data distribution with only $|\\mathcal{T}|$ neural network evaluations. This study offers new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models.",
    "derandddm-2": "Notation. We use $|\\mathcal{T}|$ to denote the cardinality of the set $\\mathcal{T}$ (excluding repeated elements). We use lowercase letters to denote scalars, boldface lowercase letters to denote vectors, and boldface uppercase letters to denote matrices. The notation $1: N$ indicates the sequence from 1 through $N$. The symbol $\\mathbf{q}$ designates the real distribution in a diffusion process, while $\\mathbf{p}$ represents the distribution during sampling. With its success probability inside the parentheses, the Bernoulli distribution is denoted by Bernoulli $(\\cdot)$. We further use $\\operatorname{Cat}(\\mathbf{x} ; \\mathbf{p})$ to denote a categorical distribution over a one-hot row vector $\\mathbf{x}$ with probabilities given by the row vector $\\mathbf{p}$. ## 2 Background\n\nIn this section, we provide the background of discrete diffusion models. We begin by introducing the discrete Markov diffusion model, designed for handling categorical random variables. Specifically, consider a diffusion model trying to generate distributions over a discrete random variable $\\mathbf{x} \\in \\mathbb{R}^{K}$ that is one-hot encoded with $K$ categories, i.e., $\\mathbf{x}$ can be chosen as one of $K$ categories, and for any $k \\in[K]$, $\\mathbf{x}$ is categorized as $k$ if $\\mathbf{x}$ aligns with the standard basis vector $\\mathbf{e}_{k}$. The sequence $\\left\\{\\mathbf{x}_{t}\\right\\}_{t=0}^{T}$ represents how this random variable changes over time $0 \\leq t \\leq T$, starting from an $\\mathbf{x}_{0} \\in \\mathbb{R}^{K}$ drawn from the real distribution $\\mathbf{q}_{\\text {data }}$. In this paper, we focus on the two most widely used D3PMs: multinomial diffusion (Hoogeboom et al., 2021b) and absorbing diffusions (Austin et al., 2021). Forward Process. During the forward process, the real distribution $\\mathbf{q}_{\\text {data }}$ is gradually transformed into a noise distribution named $\\mathbf{q}_{\\text {noise }}$. The transformation occurs through $T$ steps, with $T$ intermediate latent variables $\\mathbf{x}_{1}, \\ldots \\mathbf{x}_{T}$ and update rules given by:\n\n$$\n\\mathbf{x}_{t}=b_{t} \\mathbf{x}_{t-1}+\\left(1-b_{t}\\right) \\mathbf{w}_{t}, \\quad t=1, \\ldots, T\n$$\n\nHere $b_{t}$ is randomly drawn from a Bernoulli distribution with parameter $\\beta_{t}$, denoted by $b_{t} \\sim$ Bernoulli $\\left(\\beta_{t}\\right)$, and $\\mathbf{w}_{t}$ is randomly drawn from the noise distribution $\\mathbf{q}_{\\text {noise }}$, while for different $t$ the samples are independent. In this work, we focus on cases where the noise $\\mathbf{q}_{\\text {noise }}$ can be either a uniform distribution over the vocabulary $\\{1,2, \\ldots, K\\}$ (Hoogeboom et al., 2021b), or a point mass with all of the probability mass lying on an absorbing state (Austin et al. 2021). Following this notation, the process in (1) defines a Markov process characterized by the transition kernel\n\n$$\nq\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\beta_{t} \\mathbf{x}_{t-1}+\\left(1-\\beta_{t}\\right) \\mathbf{q}_{\\text {noise }}\\right)\n$$\n\nMoreover, the Markov chain property allows us to get samples $\\mathbf{x}_{0: t}$ from $\\mathbf{x}_{0}$ by multiplying the transition probabilities at each step as $p\\left(\\mathbf{x}_{1: t} \\mid \\mathbf{x}_{0}\\right)=\\prod_{i=1}^{t} q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)$. It further leads to the following\nmarginal distribution. $$\nq\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\alpha_{t} \\mathbf{x}_{0}+\\left(1-\\alpha_{t}\\right) \\mathbf{q}_{\\text {noise }}\\right)\n$$\n\nwhere $\\alpha_{t}:=\\Pi_{s=1}^{t} \\beta_{s}$ is determined by the sequence of $\\beta_{t}$ of our choice and decreases from 1 to 0 . Reverse Process. Given the forward Markov process, the reverse process can be derived by Bayes' rule (Hoogeboom et al., 2021b; Austin et al., 2021; Zheng et al., 2023). The conditional probability $q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{0}, \\mathbf{x}_{t}\\right)$ can be determined by $q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{0}, \\mathbf{x}_{t}\\right)=q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right) q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{0}\\right) / q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)$. The reverse process can be used for synthetic data generation by sampling from the noise distribution $q_{\\text {noise }}$ and repeatedly applying a learned predictor (neural network) $p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{t}\\right)$ parameterized by $\\boldsymbol{\\theta}$ :\n\n$$\np_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{T}\\right)=q_{\\text {noise }}\\left(\\mathbf{x}_{T}\\right), \\quad q_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)=\\int_{\\widehat{\\mathbf{x}}_{0}} q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\widehat{\\mathbf{x}}_{0}\\right) p_{\\boldsymbol{\\theta}}\\left(\\widehat{\\mathbf{x}}_{0} \\mid \\mathbf{x}_{t}\\right) d \\widehat{\\mathbf{x}}_{0}\n$$\n\nWe note that the reverse process $q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\widehat{\\mathbf{x}}_{0}\\right)$ is stochastic and thus requires function evaluation at every step. Training the Neural Network. The neural network $p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{t}\\right)$ that predicts $\\widehat{\\mathbf{x}}_{0}$ is trained by maximizing the evidence lower bound (ELBO) (Sohl-Dickstein et al., 2015),\n\n$$\n\\begin{aligned}\n\\log p_{\\theta}\\left(\\mathbf{x}_{0}\\right) \\geq & \\mathbb{E}_{q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}\\right)}\\left[\\log \\frac{p_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{0: T}\\right)}{q\\left(\\mathbf{x}_{1: T} \\mathbf{x}_{0}\\right)}\\right] d \\mathbf{x}_{1: T} \\\\\n= & \\mathbb{E}_{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}\\left[\\log p_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)\\right]-\\sum_{t=2}^{T} \\mathbb{E}_{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)}\\left[\\operatorname{KL}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right) \\| p_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)\\right)\\right. \\\\\n& \\quad-\\mathbb{E}_{q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right)} \\operatorname{KL}\\left(q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right) \\| p_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{T}\\right)\\right)\n\\end{aligned}\n$$\n\nHere KL denotes Kullback-Liebler divergence and the last term $\\mathbb{E}_{q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right)} \\operatorname{KL}\\left(q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right) \\| q_{\\text {noise }}\\left(\\mathbf{x}_{T}\\right)\\right)$ equals zero. Building on this foundation, Austin et al. (2021) introduced an auxiliary denoising objective, which refines the data predictions $\\mathbf{x}_{0}$ at each time step. Since this paper primarily focuses on reverse sampling, we leave detailed discussions of these losses to Appendix B. ## 3 Discrete Non-Markov Diffusion Models (DNDM)\n\n### 3.1 Forward and Reverse Process\n\nIn this section, we introduce a non-Markov process such that the joint distribution of $\\left(\\mathbf{x}_{0}, \\mathbf{x}_{t}\\right)$ remains the same as the one defined with Markov process in Section 2. The new process aims to gradually transform input data $\\mathbf{q}_{\\text {data }}$ to the noise distribution $\\mathbf{q}_{\\text {noise }}$ through $T$ intermediate latent variables $\\mathbf{x}_{1}, \\ldots \\mathbf{x}_{T}$ with the following process:\n\n$$\n\\mathbf{x}_{t}=b_{t} \\mathbf{x}_{t-1}+\\left(1-b_{t}\\right) \\mathbf{w}\n$$\n\nwhere $b_{t}$ is independently drawn from the Bernoulli distribution Bernoulli $\\left(\\beta_{t}\\right)$ and $\\mathbf{w}$ is drawn from the noise distribution $\\mathbf{q}_{\\text {noise }}$. The only difference between (6) and (1) is that we replace $\\mathbf{w}_{t}$ in (1) by $\\mathbf{w}$, which is time-invariant during the diffusion. Therefore, the process in (6) becomes non-Markov since $q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}, \\ldots, \\mathbf{x}_{0}\\right)$ doesn't necessarily equals $q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)$. The following theorem shows that the conditional distribution $q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)$ remains unchanged. Theorem 3.1. For the non-Markov process in (6), we have\n\n$$\nq\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\alpha_{t} \\mathbf{x}_{0}+\\left(1-\\alpha_{t}\\right) \\mathbf{q}_{\\text {noise }}\\right)\n$$\n\nwhere $\\alpha_{t}:=\\prod_{i=1}^{s} \\beta_{s}$ is specified to decrease from 1 to 0 . Using the Bayes' rule, we have $q\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{t}\\right) \\propto q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right) q\\left(\\mathbf{x}_{0}\\right)$. Consequently, the condtional distribution $q\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{t}\\right)$ remains consistent with the one induced by the process process in (1). Therefore, neural network $p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{t}\\right)$ trained by the Markov process in (1), remains applicable to our non-Markov process (6) (see Appendix B for detail). Based on the discrete non-Markov diffusion model, we can give a simple characterization of the reverse process by introducing the transition time. Definition 3.2. Transition time $\\tau$ is the time that the token $\\mathbf{x}_{t}$ transition from $\\mathbf{x}_{0}$ to noise, i.e., $\\tau:=\\min _{t}\\left\\{t \\mid b_{t}=0\\right\\}$. Remark 3.3. The concept of transition time has also been introduced in Hoogeboom et al. (2021a). However, Hoogeboom et al. (2021a) restricts the transition time to be the first time of entering the absorbing state, which is only applicable to absorbing diffusion. Our definition is more general and applicable to discrete diffusion with various noise including multinomial diffusion. Given the transition time $\\tau$, the forward process reduces to:\n\n$$\n\\mathbf{x}_{t}=\\mathbb{1}(\\tau>t) \\mathbf{x}_{0}+\\mathbb{1}(\\tau \\leq t) \\mathbf{w}\n$$\n\nwhich shows that the token will be a real token $\\mathbf{x}_{0}$ before the time $\\tau$ and will be the noise $\\mathbf{w}$ after the transition time. Since token only get changed at the transition time $\\tau$, we can derive a reverse process based on $(7)$,\n\n$$\n\\mathbf{x}_{t-1}=\\mathbb{1}(\\tau=t) \\mathbf{x}_{0}+\\mathbb{1}(\\tau \\neq t) \\mathbf{x}_{t}\n$$\n\nTherefore, the process in (8) is de-randomized given transition time $\\tau$. Specifically, after independently sampled transition times $\\tau, \\mathbf{x}_{t-1}$ becomes deterministically known and fixed if we observe $\\mathbf{x}_{0}$ and $\\mathbf{x}_{t}$. It is also worth noting that given $\\mathbf{x}_{0}$ and $\\tau$, the exact reverse process (8) is Markovian, since $\\mathbf{x}_{t-1}$ solely depends on $\\mathbf{x}_{0}, \\tau, \\mathbf{x}_{t}$. Plugging (8) into (4) gives the generation process. We can prove the ELBO of the DNDM is equivalent to the ELBO of the original process (5) up to some constant, which further supports the neural network $p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{t}\\right)$ trained by the Markov process in (1), remains applicable to DNDM.",
    "derandddm-3": "(See Appendix B.",
    "derandddm-4": "3 for details). Remark 3.4. (7) and (8) suggest that even though there are $T$ distinct time steps, not every time in the range $1: T$ is crucial for capturing the process. Therefore, our primary focus should be on the most significant time step, i.e., the transition time $\\tau$, enabling faster reverse sampling. We further note that although transition happens only at time $\\tau$, the transition time is random, differs across runs, and covers the full range from 1 to $T$ on average. ### 3.2 Accelerated Reverse Sampling\n\nIn this section, we demonstrate that sampling from DNDM can lead to accelerated reverse sampling. Although our algorithm is quite general, we focus on text generation in the presentation. In Section 3.1, we only consider the case of a single token $\\mathbf{x} \\in \\mathbb{R}^{K}$ being one hot encoding of $K$ categories. In real applications, we are interested in generating a sentence with multiple tokens. So, we extend the terminology in Section 3.1, and we denote the sequence of tokens at $t$-th time step to be $\\mathbf{x}_{t, 1: N}=\\left[\\mathbf{x}_{t, 1}, \\ldots, \\mathbf{x}_{t, N}\\right]$ where $\\mathbf{x}_{t, n}$ is the $n$-th token and $N$ is the sequence length. The noise will be added to each token in a sequence independently. Therefore, each token will have its own transition time defined in Definition 3.2. We denote the transition time for each token $\\mathbf{x}_{n}$ to be $\\tau_{n}$ and further denote the transition time set $\\mathcal{T}:=\\left\\{\\tau_{n}\\right\\}_{n=1}^{N}$. Given the transition times $\\tau_{n} \\in \\mathcal{T}$, our DNDM can now be extended to the sequence with multiple tokens\n\n$$\n\\mathbf{x}_{t-1, n}=\\mathbb{1}\\left(\\tau_{n}=t\\right) \\mathbf{x}_{0, n}+\\mathbb{1}\\left(\\tau_{n} \\neq t\\right) \\mathbf{x}_{t, n}, \\forall n \\in[N]\n$$\n\nLearning the Reverse Process. We first generate the transition times $\\tau_{n}$ for $n \\in[N]$, then we follow (9) to generate the learned reverse process. Since $\\mathbf{x}_{0, n}$ is unknown in the process, we use the neural network evaluation $p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{t}\\right)$ obtained in Section 3.1 to predict $\\mathbf{x}_{0, n}$. In detail, the noisy sequence $\\mathbf{x}_{t, 1: N}$ is fed into $p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{t, 1: N}\\right)$ and the prediction tokens $\\widehat{\\mathbf{x}}_{0,1: N} \\sim p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{t, 1: N}\\right)$ are collected. Transition time. Transition time, denoted by $\\tau$, is crucial in our reverse process. This is because the reverse sampling becomes deterministic upon using (9). Each instance of transition time $\\tau$ is a random variable within the set $\\{1,2, \\ldots, T\\}$. Let's assume it follows the distribution $\\mathcal{D}_{\\tau}$. Given the schedule $\\left\\{\\alpha_{t}\\right\\}_{t=0}^{T}$, we can derive the distribution for $\\mathcal{D}_{\\tau}$. Theorem 3.5. Each specific transition time $\\tau_{n}$ in Definition 3.2 is independent. Furthermore, they collectively adhere to the distribution $\\mathcal{D}_{\\tau}$, which obeys the rule $\\mathbb{P}\\left(\\tau_{n}=t\\right)=\\alpha_{t-1}-\\alpha_{t}$. From Theorem 3.5, we discern that the nature of the diffusion model scheduler, $\\alpha_{t}$, clarifies the distribution of $\\tau$. Take the linear schedule as an example, as given by Austin et al. (2021), the relationship is $\\alpha_{t}=1-t / T$. This translates to $\\mathbb{P}\\left(\\tau_{n}=t\\right)=1 / T$ for every $t$ in the range 1 to $T$. As a result, transition time distributes uniformly across each moment in the set $\\{1, \\ldots, T\\}$. Generally, if we express $\\alpha_{t}$ as $g(t / T)$, then we can simplify to $\\mathbb{P}\\left(\\tau_{n}=t\\right)=g((t-1) / T)-g(t / T)$, which further refines to $(1 / T)\\left|g^{\\prime}(t / T)\\right|+o(1 / T)$. This indicates that transitions are more likely where $\\left|g^{\\prime}\\right|$ is large. In practice, we observed that the shape of the transition time does not need to exactly match the theoretically predicted schedule $\\mathcal{D} \\tau$ in Theorem 3.5. Algorithm 1 works even if $\\mathcal{D} \\tau$ is unknown. In particular, we can approximate the schedule with a Beta distribution by first sampling a time $t \\in[0,1]$ from a Beta distribution, then adjusting these samples to fit by multiplying by $T$ and rounding the result to obtain an integer. Accelerated Sampling. According to (9), a token $\\mathbf{x}_{t-1, n}$ is updated only if step $t$ is the transition time for the $n$-th token. If step $t$ is not the transition time for any token, the sentence from the previous step can be directly copied: $\\mathbf{x}_{t-1,1: N}=\\mathbf{x}_{t, 1: N}$. As a result, there is no need to do a function evaluation for the current step. Our attention, therefore, can be solely centered on the transition set $\\mathcal{T}$, necessitating function evaluations only for $t$ within $\\mathcal{T}$. For our method, when $N$ is fixed while $T \\rightarrow \\infty$, the total NFE $|\\mathcal{T}|$ will reach $N$. On the other hand, when $T$ is fixed and $N \\rightarrow \\infty$, the NFE $\\mathcal{T}$ will reach $T$ (See Theorem D. 1 for detail). It is worth noting that the auto-regressive diffusion model (ARDM) (Hoogeboom et al., 2021a) can also achieve at most $N$ NFE when $T=\\infty$. However, ARDM only focuses on infinite time steps, while our method here is able to accelerate sampling for finite time steps. More detailed discussion and theoretical analysis can be found in Section D, where additional experiments also demonstrate that our DNDM achieves an NFE that is less than half of the original Markov sampling method for discrete diffusion. By incorporating the forward process with different noises, we can develop DNDM-Multi and DNDM-Absorb, which accelerate the Multinomial and Absorbing sampling methods respectively. Recent works have demonstrated that the quality of samples can be enhanced by utilizing supplementary information derived from the neural network, Ghazvininejad et al., 2019; Savinov et al., 2021; Chang et al., 2022; He et al., 2022; Zheng et al., 2023). Our DNDM can also be improved using this idea. We call it a discrete non-Markov Diffusion Model with Top-k Transition Time (DNDM- $k$ ). Due to the limit of the pages, we leave the detailed Algorithm and discussion to Appendix E. ### 3.3 Continous-time (Infinite Step) Reverse Sampling\n\n```\nAlgorithm 1 Sampling From DNDM\nRequire: Trained prediction function \\(p_{\\boldsymbol{\\theta}}, \\mathbf{q}_{\\text {noise }}, \\mathcal{D}_{\\tau}\\)\n    for \\(n=1 \\ldots N\\) do\n        Initiate each token \\(\\mathbf{x}_{T, n} \\sim \\mathbf{q}_{\\text {noise }}\\)\n        Initiate the transition time \\(\\tau_{n} \\sim \\mathcal{D}_{\\tau}\\)\n    end for\n    Collect transition time set \\(\\mathcal{T}=\\left\\{\\tau_{n}\\right\\}_{n=1}^{N}\\)\n    for \\(t=T \\ldots 1\\) do\n        if \\(t \\in \\mathcal{T}\\) then\n            Generate \\(\\widetilde{\\mathbf{x}}_{0,1: N}\\) from \\(p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{t, 1: N}\\right)\\)\n            for \\(n=1 \\ldots N\\) do\n                        Update \\(\\mathbf{x}_{t-1, n}\\) based on condition of \\(\\tau_{n}\\)\n            end for\n        else\n            Update \\(\\mathbf{x}_{t-1,1: N}=\\mathbf{x}_{t, 1: N}\\)\n        end if\n    end for\n    Return \\(\\mathbf{x}_{0,1: N}\\)\n```\n\nIn the context of continuous state spaces, continuous-time processes have been proposed to accommodate algorithms that offer faster sampling speeds and enhanced sample quality (Jolicoeur-Martineau et al., 2021, Zhang and Chen, 2022, Salimans and Ho, 2022; Chung et al., 2022; Song et al., 2020b; Dockhorn et al., 2021). However, the application of continuous-time schemes to discrete-state spaces remains largely unexplored. Campbell et al. (2022) first developed a continuous framework for discrete-time diffusion for the Markovian process and randomized sampling, but not in our non-Markovian setting. In this section, we investigate the transition from finite to infinite step sampling, providing new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models. Continuous-time Forward and Backward process. Recall that the forward process described in (6) can be sampled from $\\mathbf{x}_{0, n}$ through the following process:\n\n$$\n\\mathbf{x}_{t, n}=\\alpha_{t} \\mathbf{x}_{0, n}+\\left(1-\\alpha_{t}\\right) \\mathbf{q}_{\\text {noise }}, \\quad \\alpha_{t}=\\prod_{i=1}^{t} \\beta_{i}\n$$\n\nIn the previous section, we are constrained to discrete time steps, where we must define a maximum step, denoted by $T$. The values of $\\mathbf{x}_{t}$ are computed only for $t=1, \\ldots, T$. As a result, during the training process, it is only possible to predict $\\mathbf{x}_{0}$ at these predetermined time steps. This constraint confines the computation of our reverse process exclusively to these fixed time stamps. To derive the continuous limit of $(10)$, for each $T$ we rescale 10 to a diffusion process on $[0,1]$, e.g., $\\mathbf{x}_{T, n}=\\widehat{\\mathbf{x}}_{1, n}, \\mathbf{x}_{0, n}=\\widehat{\\mathbf{x}}_{0, n}$, and $\\mathbf{x}_{t, n}=\\widehat{\\mathbf{x}}_{t / T, n}$. Therefore, when $T \\rightarrow \\infty, \\widehat{\\mathbf{x}}_{t, n}$ represents the continuous process that has values at arbitrary $t \\in[0,1]$. If the choice of $\\alpha_{t}$ for each $T$ is scale-invariant, we can define a continuous function $\\alpha(t)$ as the continuous $\\alpha$ schedule of the discrete counterpart ${ }^{1}$. More\n\n[^1]specifically, we obtain\n$$\n\\widehat{\\mathbf{x}}_{t, n}=\\alpha(t) \\widehat{\\mathbf{x}}_{0, n}+(1-\\alpha(t)) \\mathbf{q}_{\\text {noise }}, \\quad t \\in[0,1]\n$$\n\nFor the reverse-time process, we define the transition time set $\\mathcal{T}:=\\left\\{\\tau_{n}\\right\\}_{n=1}^{N}$ consistent with Theorem 3.5 and sample it from $\\mathbb{P}\\left(\\tau_{n}=t\\right)=-\\alpha^{\\prime}(t)$ (we always use decreasing $\\alpha(t)$ ). With $\\mathcal{T}$ defined, the updates to $\\mathbf{x}_{t, n}$ only occur at $\\left\\{\\tau_{n}\\right\\}$. Consequently, we arrange $\\tau_{n}$ to obtain an ordered sequence $\\tau_{n_{k}}$, where $\\tau_{n_{1}}<\\tau_{n_{2}}<\\ldots<\\tau_{n_{N}}$. When omitting the infinitely many time steps between $\\tau_{n_{k}}$ and $\\tau_{n_{k-1}}$, the resulting reverse process is then given by:\n\n$$\n\\mathbf{x}_{\\tau_{n_{k-1}}, n}=\\mathbb{1}\\left(\\tau_{n}=\\tau_{n_{k-1}}\\right) \\mathbf{x}_{0, n}+\\mathbb{1}\\left(\\tau_{n} \\neq \\tau_{n_{k-1}}\\right) \\mathbf{x}_{\\tau_{n_{k}}, n}\n$$\n\nfor all $n \\in[N]$. The detailed algorithm named DNDM-C is shown in Algorithm 4 in Appendix G. 1 . ## 4 Experiments\n\nIn this section, we evaluate DNDM and demonstrate its superior performance on two types of tasks: conditional sequence-to-sequence text generation (i.e., machine translation) and unconditional text generation. For the fairness of comparison, all the experiments are conducted using a single NVIDIA RTX A6000 GPU with 48 GB memory. Additional experiment details are provided in Appendix F. ### 4.1 Conditional Text Generation\n\nWe first evaluate the ability of DNDM on conditional text generation by machine translation tasks.",
    "derandddm-5": "Following (Zheng et al., 2023), we process the raw text with the Byte Pair Encoder (BPE) (Sennrich et al. 2016) to construct the vocabulary, which consists of the words and subwords of both the source and the target languages. We conduct our experiments using the FairSeq toolkit Ott et al., 2019), which employs a model consisting of an encoder and a decoder. The encoder takes the source text as input, while the decoder generates the target text. Datasets. We use the following three datasets to compare with the baselines for machine translation tasks: (1) IWSLT14 DE-EN (Cettolo et al. 2014), a dataset with German as the source language and English as the target language. It consists of 174272 examples (sentence pairs), and each of the validation set and the testing set accounts for 7283 and 6750 of the dataset; (2) WMT14 EN-DE (Bojar et al. (2014), which is an English-to-German translation dataset consisting of 3967182 examples. Each of the validation set and the testing set accounts for 3000 and 3003 of the dataset; and (3) WMT16 EN-RO (Bojar et al., 2016), which is an English-to-Russian translation dataset consisting of 612317 examples. Each of the validation sets and the testing set accounts for 1999 and 1999 of the dataset. The train-validation-test split is fixed across all experiments for all machine translation datasets to ensure fair comparison. Performance Metrics. We use the BLEU score (Papineni et al. 2002) to evaluate the machine translation quality, where the BLEU score is calculated based on the similarity between the actual target sequence and the predicted target sequence. The sampling speed is measured by wall-clock time (in second). Baselines. The main baselines we are comparing with are RDM and RDM- $k$ from Zheng et al. (2023). Here, we use RDM and RDM- $k$ to denote the sampling method proposed in their paper with and without the usage of top- $k$ selection for the token generation technique (see Appendix E for more details), respectively. RDM and RDM- $k$ are applied to two previously proposed state-of-the-art\ndiscrete diffusion models: Multinomial Diffusion (Hoogeboom et al., 2021b) and Absorbing Diffusion (Austin et al., 2021). Results and Discussion. Tables 2 and 3 present the performance evaluations of our algorithms in machine translation tasks. Table 2 presents results for multinomial diffusion, while Table 3 displays results for absorbing diffusion. Our reported time and BLEU scores are averaged over 5 repeated experiments, except for the baseline RDM experiment ${ }^{2}$\n\nFrom Tables 2 and 3, we observe that methods based on DNDM significantly accelerate the sampling process compared to baseline diffusion models. This acceleration allows for greater flexibility in increasing the number of steps (up to infinity) without imposing a significant computational burden. In particular, more sampling steps lead to better generation quality (BLEU) at the expense of longer sampling time, as indicated in each column of Tables 2 and 3. For RDM-based methods, generation time increases linearly with the number of sampling steps. On the contrary, for our DNDM-based method, generation time only increases marginally (See Figure 4 in Section G). As a result of the difference in the growing speed of sampling time with respect to sampling steps, the more sampling steps, the more speedup DNDM can obtain. Continuous-time results, as the ultimate limit of increasing sampling steps, are presented in the last row of each dataset with the tag $\\infty$. Given that the results with 1000 steps consistently outperform those with 50 steps, we compare $\\infty$ with 1000 steps in Table 2 and 3. For IWSLT14 and WMT16, where the generation BLEU score is relatively high, we observe a consistent performance improvement of up to 0.3 in BLEU score when utilizing the DNDM-C algorithm, with the exception of a single case in the absorbing diffusion setting for WMT16 without the use of top- $k$ selection. The performance gain of the continuous-time method on WMT14 is less significant, with both drops and gains. However, WMT14 itself has not reached a high level of performance, with a BLEU score significantly lower than other datasets. In general, training WMT14 poses challenges across all diffusion models, including multinomial diffusion (Hoogeboom et al., 2021b), absorbing diffusion Austin et al., 2021), and RDM diffusion (Zheng et al., 2023), etc. We defer a more detailed discussion on WMT14 to Appendix F.1. Finally, when compared with the results obtained with 50 steps, the performance of DNDM-C demonstrates improvement consistently. Furthermore, we note that regardless of the dataset or the method (i.e., RDM or DNDM) employed, top- $k$ token generation consistently outperforms vanilla methods. This approach enhances the BLEU score by approximately 1-2 points without introducing significant increases in sampling time. Scaling Law in Sampling Speed. For illustrative purposes, we use the example of IWSLT14 to visualize how the sample quality scales regarding sampling speed for different methods. In Figure 1, we observe the trend of the BLEU score in relation to computational time. Each line in the legend represents a different sampling algorithm, and a steeper slope indicates a larger marginal gain when sampling for longer periods. Figure 1 demonstrates that our algorithm displays nearly linear growth in BLEU score over the log of time, which is remarkable in contrast with the flat curve of the baseline. Particularly, for multinomial diffusion, the BLEU score increases by 1 in less than 60 seconds of additional sampling time. For absorbing diffusion, DNDM outperforms RDM before RDM samples 50 steps. In Tables 6 and 7 in Appendix D, we further use the average number of function evaluations (NFE) to measure the improved speed within the specified number of sampling\n\n[^2]Table 2: BLEU score comparison of multinomial diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. Below the dataset, we present the amount of data used to run the evaluation (sentences). The blue background highlights our algorithms, and the bold number indicates the best performance within each row and each setting (i.e., with or without top-k). | Dataset | Steps | RDM-Multi |  | DNDM-Multi |  | RDM- $k$-Multi |  | DNDM- $k$-Multi |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | BLEU | Time (s) | BLEU | Time (s) | BLEU | Time(s) | BLEU | Time (s) |\n| IWSLT14 | 25 | $\\mathbf{3 1 .",
    "derandddm-6": "2 6}$ | 166.9 | 30.95 | $\\mathbf{5 2 . 9}$ | $\\mathbf{3 2 . 8 2}$ | 161.9 | 32.30 | $\\mathbf{5 2 . 6}$ |\n|  | 1000 | 31.69 | 6308.9 | $\\mathbf{3 1 . 8 2}$ | $\\mathbf{1 9 1 . 3}$ | 32.64 | 6321.3 | $\\mathbf{3 3 . 1 5}$ | $\\mathbf{1 9 1 . 5}$ |\n|  | $\\infty$ | - | - | $\\mathbf{3 1 . 8 9}$ | $\\mathbf{2 2 5 . 2}$ | - | - | $\\mathbf{3 3 . 4 4}$ | $\\mathbf{2 2 8 . 1}$ |\n|  | 25 | $\\mathbf{2 5 . 2 5}$ | 237.3 | 25.01 | $\\mathbf{9 0 . 7}$ | $\\mathbf{2 6 . 0 3}$ | 230.9 | 25.98 | $\\mathbf{9 0 . 5}$ |\n| WMT14 | 50 | $\\mathbf{2 5 . 7 5}$ | 466.1 | 25.33 | $\\mathbf{1 3 8 . 4}$ | 26.14 | 500.2 | $\\mathbf{2 6 . 3 7}$ | $\\mathbf{1 3 8 . 3}$ |\n| $(3 \\mathrm{k})$ | 1000 | 25.66 | 8996.7 | $\\mathbf{2 5 . 7 1}$ | $\\mathbf{2 6 5 . 4}$ | 25.82 | 8991.7 | $\\mathbf{2 6 . 8 8}$ | $\\mathbf{2 6 5 . 5}$ |\n|  | $\\infty$ | - | - | $\\mathbf{2 4 . 7 9}$ | $\\mathbf{3 0 7 . 5}$ | - | - | $\\mathbf{2 6 . 3 9}$ | $\\mathbf{3 0 7 . 3}$ |\n|  | 25 | $\\mathbf{3 2 . 2 9}$ | 145.2 | 31.97 | $\\mathbf{3 6 . 4}$ | $\\mathbf{3 3 . 1 2}$ | 143.5 | 32.94 | $\\mathbf{3 6 . 4}$ |\n| WMT16 | 50 | $\\mathbf{3 2 . 5 3}$ | 286.1 | 32.50 | $\\mathbf{6 3 . 2}$ | $\\mathbf{3 3 . 4 1}$ | 312.4 | 33.26 | $\\mathbf{6 2 . 7}$ |\n| $(2 \\mathrm{k})$ | 1000 | 32.63 | 5588.9 | $\\mathbf{3 2 . 8 6}$ | $\\mathbf{1 7 1 . 4}$ | 33.67 | 5601.0 | $\\mathbf{3 3 . 7 9}$ | $\\mathbf{1 7 1 . 2}$ |\n|  | $\\infty$ | - | - | $\\mathbf{3 2 . 9 1}$ | $\\mathbf{1 9 6 .",
    "derandddm-7": "4}$ | - | - | $\\mathbf{3 3 . 8 6}$ | $\\mathbf{1 9 6 . 3}$ |\n\nTable 3: BLEU score comparison of absorbing diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. Below the dataset, we present the amount of data used to run the evaluation (sentences). The blue background highlights our algorithms, and the bold number indicates the best performance within each row and each setting (i.e., with or without top-k). | Dataset | Steps | RDM-Absorb |  | DNDM-Absorb |  | RDM- $k$-Absorb |  | DNDM- $k$-Absorb |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | BLEU | Time (s) | BLEU | Time (s) | BLEU | Time(s) | BLEU | Time (s) |\n| IWSLT14 <br> $(6.75 \\mathrm{k})$ | 25 | 31.58 | 116.3 | 32.43 | 67.2 | 34.50 | 108.9 | 34.14 | 67.3 |\n|  | 50 | 31.80 | 227.2 | 32.63 | 95.9 | 34.58 | 213.9 | 34.34 | 96.2 |\n|  | 1000 | 31.91 | 4197.4 | 32.93 | 161.1 | 34.60 | 4205.9 | 34.56 | 162.3 |\n|  | $\\infty$ | - | - | 33.03 | 174.6 | - | - | 34.65 | 180.7 |\n| WMT14 <br> (3k) | 25 | 24.97 | 116.4 | 25.79 | 68.1 | 27.50 | 107.5 | 27.18 | 68.0 |\n|  | 50 | 24.95 | 231.1 | 26.10 | 102.0 | 27.73 | 255.2 | 27.66 | 102.5 |\n|  | 1000 | 25.22 | 4169.4 | 26.43 | 178.3 | 27.75 | 4167.4 | 27.82 | 179.1 |\n|  | $\\infty$ | - | - | 26.50 | 180.1 | - | - | 27.50 | 181.2 |\n| WMT16 <br> (2k) | 25 | 32.86 | 75.5 | 33.20 | 41.2 | 33.92 | 69.9 | 33.96 | 41.4 |\n|  | 50 | 32.93 | 148.4 | 33.30 | 62.5 | 34.10 | 166.1 | 34.20 | 62.7 |\n|  | 1000 | 33.25 | 2951.7 | 33.60 | 121.3 | 34.44 | 2718.7 | 34.38 | 122.7 |\n|  | $\\infty$ | - | - | 33.42 | 121.8 | - | - | 34.41 | 121.9 |\n\nsteps. Additionally, in Figure 2, we visualize how the BLEU score and the generated text change throughout the sampling process. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f009531c71804551fabg-11.jpg?height=553&width=1610&top_left_y=230&top_left_x=251)\n\nFigure 1: Generation quality to generation time comparison on IWSLT14. $x$-axis: computational time in seconds; $y$-axis: BLEU score. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f009531c71804551fabg-11.jpg?height=480&width=784&top_left_y=996&top_left_x=237)\n(a) The BLEU Score in the Generation Process\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4f009531c71804551fabg-11.jpg?height=47&width=711&top_left_y=925&top_left_x=1100)\n$\\mathbf{t}=\\mathbf{7 5}$ [noise] $\\cdots$ [noise] and we [noise] $\\cdots$ [noise] govern[noise] [noise] year [noise] $\\mathbf{t}=\\mathbf{6 7}$ we [noise] [noise] fello [noise] [noise] [noise] and we let them [noise] [noise] city govern[noise] every year.",
    "derandddm-8": "t $=39$ we choose some fellows every year and we let them work with city governance every year. $\\mathbf{t}=\\mathbf{0}$ we choose some fellows every year and we let them work with city governance every year. (b) Text in the Generation Process\n\nFigure 2: We demonstrate the 100-step generation process of DNDM- $k$-Multi as an example, where the left is the change of the BLEU score along the generation process, and the right is the text at different time steps. As the time goes from 100 to 0 , noise is gradually removed until the corresponding English text emerges. Since the transition time follows a Beta distribution as described in Section 3.2 , the majority of transitions occur near the starting time. ### 4.2 Unconditional Text Generation\n\nIn unconditional text generation, we focus on language modeling tasks, where the goal is to generate language data similar to the provided training dataset. In this task, no input text is given during sampling, and the neural network directly learns $q\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{t}\\right)$. Datasets. The natural language generation task is evaluated on two language datasets following Hoogeboom et al. (2021b): text8 and enwik8. Both datasets are from Wikipedia, but their contents are highly distinct. In text8, the plain text consists of English words (all the letters are in lower case) and spaces, and it is tokenized into 26 characters and one blank space, resulting in 27 categories. In contrast to the cleanness of text8, enwik8 preserves the original XML dump contents, and there exist various special symbols in its raw text, so its text is tokenized into 1 Byte, resulting in 256 categories. We utilize text8 dataset with sequence length 256 and enwik8 dataset with sequence\nlength 320. The train/val/test splits are 9e7/5e6/5e5 for both text8 and enwik8. Performance Metrics. Our evaluation of text generation quality relies on the perplexity score. When generating text8 data, we calculate perplexity scores using the GPT2 model, while for enwik8 data generation, we employ the GPT2-large model.",
    "derandddm-9": "The sampling speed is measured in seconds. Baselines. We compare our proposed DNDM on unconditional text generation task with the vanilla Multinomial Diffusion (Hoogeboom et al., 2021b). Results and Discussion. Table 4 displays the performance of our algorithms in text generation tasks. We run the multinomial diffusion model on the text8 dataset for 1000 diffusion steps and on the enwik8 dataset for 4000 diffusion steps. Our DNDM-based algorithms outperform the vanilla sampling algorithm used in Hoogeboom et al. (2021b) in terms of both sampling time and perplexity score. Specifically, for the text8 dataset, DNDM-based algo-\n\nTable 4: Comparison of different sampling methods for unconditional text generation (multinomial diffusion) on text8 and enwik8 benchmarks. Sampling time is computed by generating a single text sample of length 256 for text8 and length 320 for enwik8, averaged over 10 runs. The blue background represents our algorithms, and the bold number indicates the optimal value. |  |  | Vanilla | DNDM |\n| :---: | :---: | :---: | :---: |\n| text8 | Perplexity | $1,465.75$ | $\\mathbf{6 0 0 . 0 2}$ |\n|  | Time (s) | 135.9 | $\\mathbf{3 1 . 1}$ |\n| enwik8 | Perplexity | 801.78 | $\\mathbf{5 5 6 . 7 8}$ |\n|  | Time (s) | 602.8 | $\\mathbf{4 7 . 4}$ |\n\nrithms are 5 times faster than the vanilla algorithm. For the enwik8 dataset, DNDM-based algorithms are 14 times faster than the vanilla algorithm. ## 5 Conclusion and Future Work\n\nThis paper presents a novel discrete non-Markov diffusion model (DNDM) accompanied by an accelerated sampling algorithm designed to boost sampling speed in a discrete-state space. Our discrete diffusion model incorporates \"transition time set\" latent variables, establishing itself as an efficacious diffusion and data generation method. Thanks to our acceleration technique, we significantly decrease the number of neural network function evaluations without sacrificing sample quality. We also introduce an infinite-step sampling algorithm, DNDM-C, which provides new insights into bridging the gap between discrete and continuous-time processes for discrete diffusion models. While this study focuses on text generation using non-autoregressive models, a promising direction for future exploration is applying our method to other tasks, such as audio and image generation, as well as other architectures, such as the GPT model (an autoregressive model). ## A Related Work\n\nContinous Diffusion Models. Generative modeling via continuous-time stochastic process has been investigated thoroughly in a series of work Movellan, 2008; Lyu, 2012; Sohl-Dickstein et al., 2009; Bengio et al., 2014; Alain et al., 2016; ALIAS PARTH GOYAL et al., 2017; Bordes et al., 2017). The two lines of probabilistic modeling, denoising diffusion probabilistic model (Sohl-Dickstein et al., 2015; Ho et al., 2020) and score matching with Langevin dynamics (Song and Ermon, 2019) are unified by Song et al. (2020b) through introducing the SDE framework for SGM. Based on it, subsequent works (Dockhorn et al., 2021; Nachmani et al., 2021; Vahdat et al., 2021) introduced a more complex diffusion process to improve the generation speed and quality. On the other hand, the score-based sampling process is time-consuming and has attracted much attention for improvements\nin speed (San-Roman et al., 2021; Watson et al., 2021; Kong and Ping, 2021; Karras et al., 2022; Song et al. 2023). \"Gotta go fast\" (GGF), an SDE solver with adaptive step size tailored to SGM, is proposed in Jolicoeur-Martineau et al. (2021). Song et al. (2020a) introduced a non-Markov diffusion process that corresponds to a deterministic sampling process, enabling the generation of high-quality samples more rapidly.",
    "derandddm-10": "Dockhorn et al. (2022); Liu et al. (2022) proposed a high-order SDE/ODE solver to achieve lower discretization error. Lu et al. (2022); Zhang and Chen (2022) leveraged the semi-linear structure of reverse ODE to reduce the discretization error and achieve state-of-the-art sampling speed. Discrete Diffusion Models. Research on discrete diffusion models was initiated by Sohl-Dickstein et al. (2015), who investigated diffusion processes over binary random variables. The methodology was expanded upon by Ho et al. (2020), integrating categorical random variables through transition matrices with uniform probabilities. Though Song et al. (2020a) suggested a similar extension in their supplementary content, they abstained from experimenting with this model type. Later on, Austin et al. (2021) unveiled a more intricate framework for diffusion concerning categorical random variables, enhancing the discrete diffusion models by merging them with Masked language models (MLMs). Contemporary research has furthered this domain by introducing features like editing-based operations (Jolicoeur-Martineau et al, 2021, Reid et al., 2022), auto-regressive diffusion models (Hoogeboom et al., 2021a; Ye et al., 2023), the evolution of a continuous-time structure (Campbell et al., 2022), and the exploration of neural network analogs for learning (Sun et al., 2022).",
    "derandddm-11": "Additionally, Zheng et al. (2023) introduced a re-parameterized loss and an associated sampling technique, attaining commendable outcomes in fewer iterations. Our contributions run parallel to these aforementioned studies. ## B Additional details of Discrete Diffusion\n\nIn our paper, we treat all the $\\mathbf{x}, \\mathbf{q}_{\\text {noise }}$ as a row vector and treat $\\mathbb{1}$ as a column vector with all elements equal 1. ## B. 1 Comparison between D3PM and DNDM\n\nIn Section 3.1, we introduced two different diffusion processes, the Markov process in (1) and the non-Markov process in (6). In this section, we explain why they are different but result in the same joint distribution of $\\left(\\mathbf{x}_{0}, \\mathbf{x}_{t}\\right)$ for every time step $t$. Since $\\mathbf{q}\\left(\\mathbf{x}_{0}\\right)$ keeps the same, we only need to prove that the conditional distribution $\\mathbf{q}\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)$ is the same for the two processes. Markov Process. 1 is a Markov process since $\\mathbf{w}_{n}$ is independent with $\\mathbf{x}_{t-1}, \\ldots, \\mathbf{x}_{0}$, so $\\mathbf{x}_{t}$ is independent of all the past states given the present state. This can also be inferred from the following distribution, which does not depend on $\\mathbf{x}_{0}, \\ldots, \\mathbf{x}_{t-2}$,\n\n$$\nq\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\beta_{t} \\mathbf{x}_{t-1}+\\left(1-\\beta_{t}\\right) \\mathbf{q}_{\\text {noise }}\\right)\n$$\n\nDenote $\\mathbf{Q}_{t}:=\\beta_{t} \\mathbf{I}+\\left(1-\\beta_{t}\\right) \\mathbb{1} \\mathbf{q}_{\\text {noise }}$, then we have that\n\n$$\n\\mathbf{x}_{t-1} \\mathbf{Q}_{t}=\\beta_{t} \\mathbf{x}_{t-1}+\\left(1-\\beta_{t}\\right) \\mathbf{x}_{t-1} \\mathbb{1} \\mathbf{q}_{\\text {noise }}=\\beta_{t} \\mathbf{x}_{t-1}+\\left(1-\\beta_{t}\\right) \\mathbf{q}_{\\text {noise }}\n$$\n\nwhere the last equality holds due to the fact that $\\mathbf{x}_{t-1}$ is a one hot vector and thus $\\mathbf{x}_{t-1} \\mathbb{1}=1$. Therefore, we can rewrite 13) as $q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\mathbf{x}_{t-1} \\mathbf{Q}_{t}\\right)$. Then, it is a Markov process with transition kernel $\\mathbf{Q}_{t}$. So $q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\mathbf{x}_{0} \\mathbf{Q}_{0} \\ldots \\mathbf{Q}_{t}\\right)$ (Austin et al., 2021). We can then have that\n\n$$\n\\mathbf{Q}_{0} \\ldots \\mathbf{Q}_{t}=\\left[\\beta_{0} \\mathbf{I}+\\left(1-\\beta_{0}\\right) \\mathbb{1} \\mathbf{q}_{\\text {noise }}\\right] \\ldots\\left[\\beta_{t} \\mathbf{I}+\\left(1-\\beta_{t}\\right) \\mathbb{1} \\mathbf{q}_{\\text {noise }}\\right]\n$$\n\n$$\n=\\Pi_{s=0}^{t} \\beta_{s} \\mathbf{I}+\\left(1-\\Pi_{s=0}^{t} \\beta_{s}\\right) \\mathbb{1} \\mathbf{q}_{\\text {noise }}\n$$\n\nwhere the last equality holds since identity matrix I multiplying any vector equals the vector itself and $\\mathbb{1} \\mathbf{q}_{\\text {noise }} \\mathbb{1} \\mathbf{q}_{\\text {noise }}=\\mathbb{1}\\left(\\mathbf{q}_{\\text {noise }} \\mathbb{1}\\right) \\mathbf{q}_{\\text {noise }}=\\mathbb{1} \\mathbf{q}_{\\text {noise }}$. Therefore, we have that\n\n$$\nq\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\Pi_{s=0}^{t} \\beta_{s} \\mathbf{x}_{0}+\\left(1-\\Pi_{s=0}^{t} \\beta_{s}\\right) \\mathbf{q}_{\\text {noise }}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\alpha_{t} \\mathbf{x}_{0}+\\left(1-\\alpha_{t}\\right) \\mathbf{q}_{\\text {noise }}\\right)\n$$\n\nwhere the last equality holds due to the definition $\\alpha_{t}=\\prod_{s=0}^{t} \\beta_{s}$. This gives rise to why the Markov process (1) results in conditional distribution $q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\alpha_{t} \\mathbf{x}_{0}+\\left(1-\\alpha_{t}\\right) \\mathbf{q}_{\\text {noise }}\\right)$. Non-Markov Process. Recall that our DNDM is defined by\n\n$$\n\\mathbf{x}_{t}=b_{t} \\mathbf{x}_{t-1}+\\left(1-b_{t}\\right) \\mathbf{w}\n$$\n\nwhere $\\mathbf{w}$ is fixed for any time $t$. Therefore, $\\mathbf{w}$ is no longer independent with $\\mathbf{x}_{0}, \\ldots, \\mathbf{x}_{t-1}$. Therefore, we can't define the transition kernel and compute $\\mathbf{q}\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)$ by using the property of Markov. Therefore, we need to advance the technique to calculate the conditional distribution. Proof of Theorem 3.1. By (6), we can derive the following explicit expression for a recursive sequence,\n\n$$\n\\begin{aligned}\n\\mathbf{x}_{t} & =b_{1} \\ldots b_{t} \\mathbf{x}_{0, n}+\\sum_{s=1}^{t}\\left(1-b_{s}\\right) b_{s+1} \\ldots b_{t} \\mathbf{w} \\\\\n& =b_{1} \\ldots b_{t} \\mathbf{x}_{0}+\\left(1-b_{1} \\ldots b_{t}\\right) \\mathbf{w} \\\\\n& =a_{t} \\mathbf{x}_{0}+\\left(1-a_{t}\\right) \\mathbf{w}\n\\end{aligned}\n$$\n\nwhere second equality is by cancellation of terms, the last inequality holds by defining $a_{t}=b_{1} \\ldots b_{t}$. Since $a_{t}$ either equals to 1 or 0 . Besides, $a_{t}$ equals 1 if and only if $b_{1}=b_{2}=\\ldots=b_{t}=1$, so we have that $a_{t}$ follows Bernoulli distribution Bernoulli $\\left(\\beta_{1} \\ldots \\beta_{t}\\right)=\\operatorname{Bernoulli}\\left(\\alpha_{t}\\right)$ where $\\alpha_{t}=\\prod_{i=1}^{t} \\beta_{s}$. Therefore, we can conclude that $\\mathbf{q}\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)=\\operatorname{Cat}\\left(\\mathbf{x}_{t} ; \\mathbf{p}=\\alpha_{t} \\mathbf{x}_{0}+\\left(1-\\alpha_{t}\\right) \\mathbf{q}_{\\text {noise }}\\right)$, which completes the proof.",
    "derandddm-12": "## B. 2 Training Objective\n\nHoogeboom et al. (2021b) utilized $L_{t}$ derived from the negative variational bound. In detail,\n\n$$\nL_{t}=\\operatorname{KL}\\left(\\operatorname { C a t } \\left(\\mathbf{x} ; \\mathbf{p}=\\boldsymbol{\\theta}_{\\text {post }}\\left(\\mathbf{x}_{t}, \\mathbf{x}_{0}\\right) \\mid \\operatorname{Cat}\\left(\\mathbf{x} ; \\mathbf{p}=\\boldsymbol{\\theta}_{\\text {post }}\\left(\\mathbf{x}_{t}, \\widehat{\\mathbf{x}}_{0}\\right)\\right)\\right.\\right. $$\n\nwhere $\\widehat{\\mathbf{x}}_{0} \\sim p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{t}\\right), \\boldsymbol{\\theta}_{\\text {post }}=\\left(\\beta_{t} \\mathbf{x}_{t}+\\left(1-\\beta_{t}\\right) / K \\mathbb{1}^{\\top}\\right) \\odot\\left(\\alpha_{t-1} \\mathbf{x}_{0}+\\left(1-\\alpha_{t-1}\\right) / K \\mathbb{1}^{\\top}\\right)$ and $\\boldsymbol{\\theta}_{\\text {post }}=$ $\\left(\\beta_{t} \\mathbf{x}_{t}+\\left(1-\\beta_{t}\\right) / K \\mathbb{1}^{\\top}\\right) \\odot\\left(\\alpha_{t-1} \\widehat{\\mathbf{x}}_{0}+\\left(1-\\alpha_{t-1}\\right) / K \\mathbb{1}^{\\top}\\right)$. This loss evolves KL divergence between two categorical distributions. Building on this foundation, Austin et al. (2021) introduced an auxiliary denoising objective to strengthen the data predictions $\\mathbf{x}_{0}$ at each time step. In detail, the auxiliary objective is as follows,\n\n$$\n\\mathbb{E}_{q\\left(\\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)}\\left[-\\log p_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{t}\\right)\\right]\n$$\n\nwhere the auxiliary loss term is minimized exactly when $p_{\\theta}\\left(\\cdot \\mid \\mathbf{x}_{t}\\right)$ has all its mass on the data point $\\mathrm{x}_{0}$. Furthering the advancements, Zheng et al. (2023) put forth a reparametrized loss $L_{t}$ that incorporates a re-weighted parameter $\\lambda_{t}$. The detailed loss is\n\n$$\n\\bar{L}_{t}=\\lambda_{t-1} \\mathbb{E}_{\\mathbf{x}_{t-1}, \\mathbf{x}_{t} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0}\\right)} \\operatorname{KL}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right) \\mid p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)\\right)\n$$\n\nThis loss can be related to the standard multi-class cross-entropy loss function, which is also simple and powerful.",
    "derandddm-13": "That's why we consider Zheng et al. (2023) as the baseline model. In Section 3.3, we consider the continuous-time forward and backward process. Based on that, we were motivated to analyze the infinite limit of the average loss $\\lim _{t \\rightarrow \\infty} \\frac{1}{T} \\sum_{t=1}^{T} L_{t}$. We find that the new loss can provide a better checkpoint than the loss averaged on the finite step on some tasks. ## B. 3 Calculation of the Evidence Lower Bound\n\n## B.3.1 Finite Time DNDM\n\nIn this section, we derive the evidence lower bound (ELBO) for our model. The derivatives are inspired by the reasoning in DDIM (Song et al., 2020a). Specifically, We denote the generative process as $p_{\\theta}\\left(\\mathbf{x}_{0: T} \\mid \\tau\\right)=p_{\\theta}^{(T)}\\left(\\mathbf{x}_{T} \\mid \\tau\\right) \\prod_{t=1}^{T} p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\tau\\right)$. Here, $p_{\\theta}^{(T)}$ is the pure noise and $p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\tau\\right)=$ $q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\widehat{\\mathbf{x}}_{0}, \\tau\\right)$, where $\\widehat{\\mathbf{x}}_{0}$ is given by a neural network $p_{\\boldsymbol{\\theta}}$, i.e., $\\widehat{\\mathbf{x}}_{0}=p_{\\boldsymbol{\\theta}}\\left(\\mathbf{x}_{t}, t\\right)$. Notice that by Jensen's inequality,\n\n$$\n\\log p_{\\theta}\\left(\\mathbf{x}_{0}\\right)=\\log \\mathbb{E}_{\\tau \\sim \\mathcal{D}_{\\tau}}\\left[p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\tau\\right)\\right] \\geq \\mathbb{E}_{\\tau \\sim \\mathcal{D}_{\\tau}}\\left[\\log p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\tau\\right)\\right]\n$$\n\nThe evidence lower bound inequality gives\n\n$$\n\\log p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\tau\\right) \\geq \\mathbb{E}_{\\mathbf{x}_{1: T} \\sim q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}, \\tau\\right)} \\log \\frac{p_{\\theta}\\left(\\mathbf{x}_{0: T} \\mid \\tau\\right)}{q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}, \\tau\\right)}\n$$\n\nPlugging (16) into (15) gives the following ELBO,\n\n$$\n\\log p_{\\theta}\\left(\\mathbf{x}_{0}\\right) \\geq \\mathbb{E}_{\\tau \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{1: T} \\sim q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}, \\tau\\right)} \\log \\frac{p_{\\theta}\\left(\\mathbf{x}_{0: T} \\mid \\tau\\right)}{q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}, \\tau\\right)}:=\\text { ELBO. }\n$$\n\nWe factorize the $p_{\\theta}$ and $q$ by\n\n$$\n\\begin{gathered}\np_{\\theta}\\left(\\mathbf{x}_{0: T} \\mid \\tau\\right)=p_{\\theta}^{(T)}\\left(\\mathbf{x}_{T} \\mid \\tau\\right) \\prod_{t=1}^{T} p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\tau\\right) \\\\\nq\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}, \\tau\\right)=q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}, \\tau\\right) \\prod_{t=2}^{T} q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}, \\tau\\right)\n\\end{gathered}\n$$\n\nHere $q$ admits such a decomposition due to our definition of the diffusion process in (6), which introduce the following reverse process:\n\n$$\n\\mathbf{x}_{t-1}=\\mathbb{1}(\\tau=t) \\mathbf{x}_{0}+\\mathbb{1}(\\tau \\neq t) \\mathbf{x}_{t}\n$$\n\nTherefore, $\\mathbf{x}_{1: T}$ is Markovian when conditioned on $\\mathbf{x}_{0}$ and $\\tau$. Based on the factorization, we have\n\n$$\n\\operatorname{ELBO}=\\mathbb{E}_{\\tau \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{1: T} \\sim q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}, \\tau\\right)}\\left[\\log p_{\\theta}^{(T)}\\left(\\mathbf{x}_{T} \\mid \\tau\\right)+\\sum_{t=1}^{T} \\log p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\tau\\right)\\right. $$\n\n$$\n\\begin{aligned}\n& \\left.-\\log q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}, \\tau\\right)-\\sum_{t=2}^{T} \\log q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}, \\tau\\right)\\right] \\\\\n& =\\mathbb{E}_{\\tau \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{1: T} \\sim q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}, \\tau\\right)}\\left[\\log p_{\\theta}^{(1)}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}, \\tau\\right)+\\sum_{t=2}^{T} \\log \\frac{p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\tau\\right)}{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}, \\tau\\right)}\\right. \\\\\n& \\left.+\\log \\frac{p_{\\theta}^{(T)}\\left(\\mathbf{x}_{T} \\mid \\tau\\right)}{q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}, \\tau\\right)}\\right] \\\\\n& =\\mathbb{E}_{\\tau \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{1} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0}, \\tau\\right)} \\log p_{\\theta}^{(1)}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}, \\tau\\right) \\\\\n& +\\sum_{t=2}^{T} \\mathbb{E}_{\\mathbf{x}_{t-1}, \\mathbf{x}_{t} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0}, \\tau\\right)} \\log \\frac{p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\tau\\right)}{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}, \\tau\\right)}+\\text { const } \\\\\n& =\\mathbb{E}_{\\tau \\sim \\mathcal{D}_{\\tau}} \\underbrace{\\mathbb{E}_{\\mathbf{x}_{1} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0}, \\tau\\right)} \\log p_{\\theta}^{(1)}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}, \\tau\\right)}_{\\overline{\\mathcal{L}_{1}}} \\\\\n& -\\sum_{t=2}^{T} \\mathbb{E}_{\\tau \\sim \\mathcal{D}_{\\tau}} \\underbrace{\\mathbb{E}_{\\mathbf{x}_{t-1}, \\mathbf{x}_{t} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0}, \\tau\\right)} \\operatorname{KL}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}, \\tau\\right) \\mid p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\tau\\right)\\right)}_{\\mathcal{L}_{t}}+\\text { const. }\n\\end{aligned}\n$$\n\nBy a slight abuse of notations we use $q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right), p_{\\theta}^{(t)}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)$ to indicate the distribution of the diffusion process defined in Zheng et al. (2023), that is, the standard Markov discrete diffusion process. In particular, we have\n\n$$\n\\begin{aligned}\n& \\overline{\\mathcal{L}}_{1}= \\begin{cases}\\mathbb{E}_{\\mathbf{x}_{1} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0}\\right)} \\log p_{\\theta}^{(1)}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right), & \\tau=1, \\\\\n\\text { const, } & \\tau \\neq 1\\end{cases} \\\\\n& \\overline{\\mathcal{L}}_{t}= \\begin{cases}\\mathbb{E}_{\\mathbf{x}_{t-1}, \\mathbf{x}_{t} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0}\\right)} \\operatorname{KL}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right) \\mid p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)\\right), & \\tau=t \\\\\n0, & \\tau \\neq t\\end{cases}\n\\end{aligned}\n$$\n\nThus, we can obtain that\n\n$$\n\\begin{aligned}\n\\operatorname{ELBO}= & \\mathbb{P}(\\tau=1) \\cdot \\underbrace{\\mathbb{E}_{\\mathbf{x}_{1} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0}\\right)} \\log p_{\\theta}^{(1)}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)}_{\\mathcal{L}_{1}} \\\\\n& -\\sum_{t=2}^{T} \\mathbb{P}(\\tau=t) \\cdot \\underbrace{\\mathbb{E}_{\\mathbf{x}_{t-1}, \\mathbf{x}_{t} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0}\\right)} \\mathrm{KL}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right) \\mid p_{\\theta}^{(t)}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)\\right)}_{\\mathcal{L}_{t}}+\\text { const. }\n\\end{aligned}\n$$\n\nHere $\\mathcal{L}_{t}$ matches the loss terms in Zheng et al. (2023). In the practical training process, Zheng et al. (2023) samples $t$ from Unif $\\{1, \\cdots, T\\}$ in each iteration and optimizes $\\lambda_{t} \\cdot \\mathcal{L}_{t}$, where $\\lambda_{t}$ 's are weights. Thus, when we sample $\\tau$ and optimize $\\mathcal{L}_{\\tau}$, our ELBO indeed leads to the same training objective as Zheng et al.",
    "derandddm-14": "(2023) up to reweighting. Since Zheng et al. (2023) is a parametrization of existing works (Austin et al., 2021, Hoogeboom et al., 2021b), our training objective indeed aligns with previous discrete diffusion models. ## B.3.2 Continous Time DNDM\n\nIn Section B.3, we derived an ELBO for DNDM and its accelerated algorithm defined in Section 3.1 and 3.2. While for finite sampling steps, we can decompose the diffusion process via the sampling\nsteps $1, \\ldots, T$ in (16), it becomes intractable for continuous Time DNDM (Infinite steps $T \\rightarrow \\infty$ ). Therefore, we can formulate the ELBO of continuous time DNDM by decomposing the transition times. The idea of decomposition of transition times follows Hoogeboom et al. (2021a), but their proof is only applicable to absorbing discrete diffusion, while ours can deal with discrete diffusion with various noise $q_{\\text {noise }}$ including multinomial diffusion. In Section B.3. we only consider the case of a single token $\\mathbf{x} \\in \\mathbb{R}^{K}$ for simplicity as we decompose with the sampling steps $T$. In this section, we decompose over the transition time $\\tau$. Therefore, we need to consider a sentence with multiple tokens $\\mathbf{x}_{t, 1: N}=\\left[\\mathbf{x}_{t, 1}, \\ldots, \\mathbf{x}_{t, N}\\right]$ where $\\mathbf{x}_{t, n}$ is the $n$-th token and $N$ is the sequence length. Recall that we defined the transition time set $\\mathcal{T}=\\left\\{\\tau_{n}\\right\\}_{n=1}^{N}$ in Section 3.2. We arrange $\\tau_{n}$ to obtain an ordered sequence $\\tau_{n_{k}}$, where $0=\\tau_{n_{0}}<\\tau_{n_{1}}<\\tau_{n_{2}}<\\ldots<$ $\\tau_{n_{N}}=T$. Then conditioning on the transition time set $\\mathcal{T}=\\left\\{\\tau_{1}, \\ldots, \\tau_{N}\\right\\}$, we have that\n\n$$\np_{\\theta}\\left(\\mathbf{x}_{0: T, 1: N} \\mid \\mathcal{T}\\right)=p_{\\theta}\\left(\\mathbf{x}_{\\tau_{n_{N}}}, 1: N \\mid \\mathcal{T}\\right) \\prod_{s=N, \\ldots, 1} p_{\\theta}\\left(\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N} \\mid \\mathbf{x}_{\\tau_{n_{s}}, 1: N}, \\mathcal{T}\\right)\n$$\n\nwhere we omit the time superscript of $p$ for simplicity. Then, the evidence lower bound inequality gives\n\n$$\n\\log p_{\\theta}\\left(\\mathbf{x}_{0,1: N} \\mid \\mathcal{T}\\right) \\geq \\mathbb{E}_{\\mathbf{x}_{\\tau_{n_{1}}: T, 1: N} \\sim q\\left(\\mathbf{x}_{\\tau_{n_{1}}: T, 1: N} \\mid \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right)} \\log \\frac{p_{\\theta}\\left(\\mathbf{x}_{0: T, 1: N} \\mid \\mathcal{T}\\right)}{q\\left(\\mathbf{x}_{\\tau_{n_{1}}: T, 1: N} \\mid \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right)}\n$$\n\nBy Jensen's inequality, we have\n\n$$\n\\log p_{\\theta}\\left(\\mathbf{x}_{0,1: N}\\right)=\\log \\mathbb{E}_{\\tau_{1}, \\ldots, \\tau_{n} \\sim \\mathcal{D}_{\\tau}}\\left[p_{\\theta}\\left(\\mathbf{x}_{0,1: N} \\mid \\mathcal{T}\\right)\\right] \\geq \\mathbb{E}_{\\tau_{1}, \\ldots, \\tau_{n} \\sim \\mathcal{D}_{\\tau}}\\left[\\log p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathcal{T}\\right)\\right]\n$$\n\nPlugging (17) into (18) gives the following ELBO,\n\n$$\n\\log p_{\\theta}\\left(\\mathbf{x}_{0,1: N}\\right) \\geq \\mathbb{E}_{\\tau_{1}, \\ldots, \\tau_{n} \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{\\tau_{n_{1}}: T} \\sim q\\left(\\mathbf{x}_{\\tau_{n_{1}}: T} \\mid \\mathbf{x}_{0}, \\mathcal{T}\\right)} \\log \\frac{p_{\\theta}\\left(\\mathbf{x}_{0: T} \\mid \\mathcal{T}\\right)}{q\\left(\\mathbf{x}_{\\tau_{n_{1}}: T} \\mid \\mathbf{x}_{0}, \\mathcal{T}\\right)}:=\\text { ELBO }\n$$\n\nWe factorize the $p_{\\theta}$ and $q$ by\n\n$$\n\\begin{aligned}\np_{\\theta}\\left(\\mathbf{x}_{0: T, 1: N} \\mid \\mathcal{T}\\right) & =p_{\\theta}\\left(\\mathbf{x}_{T, 1: N} \\mid \\mathcal{T}\\right) \\prod_{s=N, \\ldots, 1} p_{\\theta}\\left(\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N} \\mid \\mathbf{x}_{\\tau_{n_{s}}, 1: N}, \\mathcal{T}\\right) \\\\\nq\\left(\\mathbf{x}_{\\tau_{n_{1}}: T, 1: N} \\mid \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right) & =q\\left(\\mathbf{x}_{T, 1: N} \\mid \\mathbf{x}_{0}, \\mathcal{T}\\right) \\prod_{s=N, \\ldots, 2} q\\left(\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N} \\mid \\mathbf{x}_{\\tau_{n_{s}}, 1: N}, \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right)\n\\end{aligned}\n$$\n\nTherefore, we have\n\n$$\n\\begin{aligned}\n& \\operatorname{ELBO}=\\mathbb{E}_{\\tau_{1}, \\ldots, \\tau_{n} \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{\\tau_{n_{1}}: T, 1: N} \\sim q\\left(\\mathbf{x}_{\\tau_{n_{1}}: T, 1: N} \\mid \\mathbf{x}_{0,1, N}, \\mathcal{T}\\right)}\\left[\\log p_{\\theta}\\left(\\mathbf{x}_{T, 1: N} \\mid \\mathcal{T}\\right)\\right. \\\\\n& +\\sum_{s=1}^{N} \\log p_{\\theta}\\left(\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N} \\mid \\mathbf{x}_{\\tau_{n s}, 1: N}, \\mathcal{T}\\right)-\\log q\\left(\\mathbf{x}_{T, 1: N} \\mid \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right) \\\\\n& \\left.-\\sum_{s=2}^{N} \\log q\\left(\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N} \\mid \\mathbf{x}_{\\tau_{n_{s}}, 1: N}, \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right)\\right] \\\\\n& =\\mathbb{E}_{\\tau_{1}, \\ldots, \\tau_{n} \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{\\tau_{n}}: T, 1: N \\sim q\\left(\\mathbf{x}_{\\tau_{n_{1}}: T, 1: N} \\mid \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right)}\\left[\\log p_{\\theta}\\left(\\mathbf{x}_{0,1: N} \\mid \\mathbf{x}_{1,1: N}, \\mathcal{T}\\right)\\right. \\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =\\mathbb{E}_{\\tau_{1}, \\ldots, \\tau_{n} \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{1,1: N} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right)} \\log p_{\\theta}\\left(\\mathbf{x}_{0,1: N} \\mid \\mathbf{x}_{1,1: N}, \\mathcal{T}\\right) \\\\\n& +\\sum_{s=2}^{N} \\mathbb{E}_{\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N}, \\mathbf{x}_{\\tau_{n_{s}}, 1: N} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right)} \\log \\frac{p_{\\theta}\\left(\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N} \\mid \\mathbf{x}_{\\tau_{n_{s}}, 1: N}, \\mathcal{T}\\right)}{q\\left(\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N} \\mid \\mathbf{x}_{\\tau_{n_{s}}, 1: N}, \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right)}+\\text { const } \\\\\n& =\\mathbb{E}_{\\tau_{1}, \\ldots, \\tau_{n} \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{1,1: N} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right)} \\log p_{\\theta}\\left(\\mathbf{x}_{0,1: N} \\mid \\mathbf{x}_{1,1: N}, \\mathcal{T}\\right) \\\\\n& -\\sum_{s=2}^{N} \\mathbb{E}_{\\tau_{1}, \\ldots, \\tau_{n} \\sim \\mathcal{D}_{\\tau}} \\mathbb{E}_{\\mathbf{x}_{\\tau_{n_{s}-1}}, 1: N}, \\mathbf{x}_{\\tau_{n s}, 1: N} \\sim q\\left(\\cdot \\mid \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right) \\\\\n& \\operatorname{KL}\\left(q\\left(\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N} \\mid \\mathbf{x}_{\\tau_{n_{s}}, 1: N}, \\mathbf{x}_{0,1: N}, \\mathcal{T}\\right) \\mid p_{\\theta}\\left(\\mathbf{x}_{\\tau_{n_{s-1}}, 1: N} \\mid \\mathbf{x}_{\\tau_{n_{s}}, 1: N}, \\mathcal{T}\\right)\\right)+\\text { const. }\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4f009531c71804551fabg-18.jpg?height=133&width=1131&top_left_y=237&top_left_x=497)\n\nRemark B.1. 19) represents the ELBO utilized by the DNDM-C architecture. As our transition times $\\tau_{n}$ are independently and identically drawn from the distribution $\\mathcal{D}_{\\tau}$, we are unable to further decompose (19) into a loss function related to the position information $1: N$, as was accomplished by Hoogeboom et al. (2021a). ## C Choice of the Transition Time\n\nTransition time $\\tau$ in Definition 3.2 plays an important role in DNDM. In this section, we provide a deeper discussion of the transition time.",
    "derandddm-15": "We first give a proof of the Theorem 3.5. Proof of Theorem 3.5. By the definition of $\\tau$, we know that $\\tau_{n}=t$ is equivalent to $b_{0, n}=1, \\ldots, b_{t-1, n}=$ 1 and $b_{t, n}=0$. Since $\\left\\{b_{t, n}\\right\\}_{t=0}^{T}$ is independent for different $n$ by definition, each $\\tau_{n}$ is also independent. Therefore, we drop the subscript $n$ for simplicity. On the other hand if $b_{0}=1, \\ldots, b_{t-1}=1$ and $b_{t}=0$ we can also conclude that $\\tau=t$. Therefore, we have that\n\n$$\n\\begin{aligned}\n\\mathbb{P}(\\tau=t) & =\\mathbb{P}\\left(b_{0}=1, \\ldots, b_{t-1}=1, b_{t}=0\\right) \\\\\n& =\\left[\\Pi_{s=1}^{t-1} \\beta_{s}\\right] \\cdot\\left(1-\\beta_{t}\\right) \\\\\n& =\\Pi_{s=1}^{t-1} \\beta_{s}-\\Pi_{s=1}^{t} \\beta_{s} \\\\\n& =\\alpha_{t-1}-\\alpha_{t}\n\\end{aligned}\n$$\n\nwhere the second equality is due to $b_{s}, s=1,2, \\ldots, t$ are independent random variable following $\\operatorname{Bernoulli}\\left(\\beta_{s}\\right)$ distribution and the last equality is by the definition of $\\alpha_{t}=\\Pi_{s=1}^{t} \\beta_{s}$. Notice that $\\alpha_{t}$ is a decreasing sequence in the 0 to 1 range. Therefore, $\\mathbb{P}(\\tau=t) \\in[0,1]$ for any $t \\in\\{1, \\ldots, T\\}$. Besides $\\sum \\mathbb{P}(\\tau=t)=\\sum_{t=1}^{T}\\left(\\alpha_{t-1}-\\alpha_{t}\\right)=\\alpha_{0}-\\alpha_{T}=1$. Therefore, the derived distribution is valid as long as the $\\alpha_{t}$ is decreasing from 1 to 0 . From Theorem 3.5, we discern that the nature of the diffusion model scheduler, $\\alpha_{t}$, clarifies the distribution of $\\tau$. Linear $\\alpha$ schedule. This is a schedule studied in Austin et al. (2021), where $\\alpha_{t}=1-t / T$. This will result in $\\mathbb{P}\\left(\\tau_{n}=t\\right)=1 / T$ for every $t$ in the range 1 to $T$. As a result, transition time distributes uniformly across each moment in the set $\\{1, \\ldots, T\\}$. This can be verified in a) of Figure 3 . Cosine $\\alpha$ schedule. This is a schedule studied in Hoogeboom et al. (2021b), where $\\alpha_{t}=\\cos (\\pi * t / 2 T)$. For numerical consideration of the noise, a small offset $s$ is added, i.e., $\\alpha_{t}=f(t) / f(0)$ where\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4f009531c71804551fabg-19.jpg?height=1075&width=1671&top_left_y=228&top_left_x=224)\n\nFigure 3: Different distribution of transition time for $T=50 . a), b), c$ ) The transition time sampled 1 K times under the different $\\alpha_{t}$ schedule. d) The approximated transition time for $t=1, \\ldots, T$ using different hypter-parameters. $f(t)=\\cos ((s+t / T) /(1+s) * \\pi / 2)$. As shown in b) of Figure 3, the transition time will concentrate more on the large $T$. Cosine square $\\alpha$ schedule. This is a schedule studied in Zheng et al. (2023), where $\\alpha_{t}=$ $\\cos ^{2}(\\pi * t / 2 T)$, which motivated by Nichol and Dhariwal 2021). Again, for numerical consideration of the noise, a small offset $s$ is added, i.e., $\\alpha_{t}=f(t) / f(0)$ where $\\left.f(t)=\\cos ^{(}(s+t / T) /(1+s) * \\pi / 2\\right)$. As shown in c) of Figure 3, the transition time will concentrate more on the middle of the range. Generally, if we express $\\alpha_{t}$ as $g(t / T)$, then we can simplify to $\\mathbb{P}(\\tau=t)=g((t-1) / T)-g(t / T)$, which further refines to $(1 / T)\\left|g^{\\prime}(t / T)\\right|+o(1 / T)$. This indicates that transitions are more likely where $\\left|g^{\\prime}\\right|$ is large. Such a mathematical finding can match our observation in Figure 3 . In practice, we find that the shape of the transition time doesn't need to match the theoretical prediction schedule exactly. As we can see from d) in Figure 3. A reshaped Beta distribution can approximate all the transition time distributions in a fixed range. We first extract a time $t \\in[0,1]$ from a Beta distribution, then adjust these samples to fit by multiplying $T$ and round them to acquire the integer. Our experiment finds that a properly chosen Beta distribution (tuned on the validation set) makes DNDM perform better on the translation tasks. Specifically, the chosen Beta distributions and the searching method are reported in Appendix F. The performance of the four transition time schedules mentioned above, including the reported Beta distributions for comparison, are listed in Table 5, where we find the other three schedules affect the performance, and most of\ntheir scores are lower than the scores of Beta distribution, but their scores are at least still close to the reported Beta distributions, especially for DNDM-k-absorb and DNDM-absorb. The efficiencies (measured by NFE) are also similar to one another. Additionally, the ablation study on a reasonable range of different Beta distributions with 50 and 1000 sampling steps are shown in Tables 9 and 8 , where the BLEU scores and NFE values on the test set of one of the three machine translation datasets, WMT16, are shown for demonstration. The range of Beta distributions covers our chosen Beta schedules based on validation sets and a variety of basic Beta distribution shapes. These results show that the different Beta distributions influence the performance, but most of these choices of parameters still achieve results close to the optimal. Since the Beta distributions of the reported results in Tables 2 and 3 are selected using the validation set, they do not always have the highest scores on the test set, but their scores still at least belong to the top tiers according to these tables. Another view of the transition time. In Algorithm 1, we only need to call the neural network when $t \\in \\mathcal{T}$, which can significantly speed up the sampling since we reduce the function call. Notice that after we get the $\\mathbf{x}_{0}$ prediction, we only update the $\\mathbf{x}_{t}$ for those tokens at the transition time. However, (7) implies that $\\mathbf{x}_{t}=\\mathbf{x}_{0}$ as long as $\\tau>t$. Therefore, instead of only updating the $\\mathbf{x}_{t}$ for those tokens at the transition time, i.e., $\\tau=t$, we can also update those tokens with transition time $\\tau>=t$. This motivates us to consider a variation presented as Algorithm 2, which keeps almost the same sampling time but will update the tokens several times rather than just once. Since the tokens now get the chance to be corrected over time. The new Algorithm 2 will be more robust than Algorithm 1\n\nTable 5: The BLEU scores and average number of function evaluations (NFE) values of different distributions of transition time for 1000 sampling steps with batch size 100. The parameters of the Beta distributions in this table are the same as in Tables 2 and 3 and are reported in Appendix F. | Datasets | Schedules | DNDM-multi |  | DNDM-absorb |  | DNDM-k-multi |  | DNDM-k-absorb |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | BLEU | Avg NFE | BLEU | Avg NFE | BLEU | Avg NFE | BLEU | Avg NFE |\n| IWSLT14 | Cosine | 31.72 | 31.71 | 32.71 | 31.21 | 32.91 | 31.71 | 34.50 | 31.21 |\n|  | Cosine ${ }^{2}$ | 31.78 | 31.74 | 32.93 | 31.21 | 32.78 | 31.74 | 34.53 | 31.21 |\n|  | Linear $\\alpha$ | 31.77 | 31.82 | 32.65 | 31.33 | 32.83 | 31.82 | 34.53 | 31.33 |\n|  | Beta (reported) | 31.82 | 30.33 | 32.93 | 31.08 | 33.15 | 30.33 | 34.56 | 31.08 |\n| WMT14 | Cosine | 25.80 | 39.61 | 26.54 | 39.18 | 26.63 | 39.61 | 27.81 | 39.18 |\n|  | Cosine ${ }^{2}$ | 25.52 | 39.48 | 26.53 | 39.18 | 25.01 | 39.48 | 27.95 | 39.18 |\n|  | Linear $\\alpha$ | 25.58 | 39.97 | 26.33 | 39.82 | 25.47 | 39.97 | 27.63 | 39.82 |\n|  | Beta (reported) | 25.71 | 38.94 | 26.43 | 38.76 | 26.88 | 38.94 | 27.82 | 38.76 |\n| WMT16 | Cosine | 32.71 | 40.50 | 33.56 | 40.45 | 33.46 | 40.50 | 34.37 | 40.45 |\n|  | Cosine ${ }^{2}$ | 32.73 | 40.50 | 33.51 | 40.45 | 33.44 | 40.50 | 34.24 | 40.45 |\n|  | Linear $\\alpha$ | 32.85 | 40.36 | 33.46 | 40.36 | 33.47 | 40.36 | 33.88 | 40.36 |\n|  | Beta (reported) | 32.86 | 38.46 | 33.60 | 38.27 | 33.79 | 38.45 | 34.38 | 38.27 |\n\n## D Discussion on the Number of Function Evaluations (NFE). In this section, we discuss the number of function evaluations (NFE) in DNDM. According to (9), the update of a token $\\mathbf{x}_{t-1, n}$ occurs solely at its designated transition time. Meanwhile, if step $t$ does not coincide with a transition time for any token, we maintain the sentence from the preceding\nstep unchanged: $\\mathbf{x}_{t, 1: N}=\\mathbf{x}_{t-1,1: N}$. Therefore, our algorithm removes the need of function evaluation for steps outside the set of transition times. Given this structure, our analytical emphasis is on the transition set $\\mathcal{T}$ since function evaluations are required only at times $t$ that are members of $\\mathcal{T}$. Consequently, the NFE is precisely the cardinality of the transition set, denoted by $|\\mathcal{T}|$. In our main paper, we propose a naive upper bound for $|\\mathcal{T}|$ as $\\min \\{N, T\\}$, which effectively demonstrates the speed of our method when $T>N$. Next, we demonstrate that DNDM also reduces the NFE when $T<N$, by providing a precise estimation of $|\\mathcal{T}|$\n\nTheorem D.1. Suppose transition time follows distribution $\\mathcal{D}_{\\tau}$, and consider a sequence of length $N$. Then, the cardinality of the transition set $\\mathcal{T}:=\\left\\{\\tau_{1}, \\ldots, \\tau_{N}\\right\\}$ satisfies:\n\n- $1 \\leq|\\mathcal{T}| \\leq \\min \\{N, T\\}$\n$\\bullet \\mathbb{E}[|\\mathcal{T}|]=\\left[1-C_{T, N, \\mathcal{D}_{\\tau}}\\right] \\cdot T$, where $C_{T, N, \\mathcal{D}_{\\tau}}$ is a constant in the range $(0,1)$. Furthermore,\n\n$$\nC_{T, N, \\mathcal{D}_{\\tau}}=\\left(\\sum_{i=1}^{T}\\left(1-p_{i}\\right)^{N}\\right) / T \\geq(1-1 / T)^{N}\n$$\n\nwhere $p_{i}=\\mathbb{P}(\\tau=i)$ for $\\tau \\sim \\mathcal{D}_{\\tau}$, and the equality holds if and only if $\\mathcal{D}_{\\tau}$ is a uniform distribution.",
    "derandddm-16": "Proof. The first statement is straightforward. For completeness, the proof is provided. Since there are only $N$ transition times (possibly repeated): $\\tau_{1}, \\ldots, \\tau_{N}$, the distinct transition times must satisfy $|\\mathcal{T}| \\leq N$. Additionally, since $\\mathcal{T} \\subseteq\\{1, \\ldots, T\\}$, we also have $|\\mathcal{T}| \\leq T$\n\nTo prove the second statement, we decompose $\\mathcal{T}$ and use the property of expectation. Note that $|\\mathcal{T}|=\\sum_{i=1}^{T} \\mathbb{1}\\{i \\in \\mathcal{T}\\}$. Thus,\n\n$$\n\\mathbb{E}[|\\mathcal{T}|]=\\mathbb{E}\\left[\\sum_{i=1}^{T} \\mathbb{1}\\{i \\in \\mathcal{T}\\}\\right]=\\sum_{i=1}^{T} \\mathbb{P}(i \\in \\mathcal{T})\n$$\n\nAssuming $\\mathbb{P}_{\\mathcal{D}_{\\tau}}(\\tau=i)=p_{i}$, and that $\\tau_{n}$ are i.i.d. draws from $\\mathcal{D}_{\\tau}$, we have\n\n$$\n\\mathbb{P}(i \\in \\mathcal{T})=1-\\mathbb{P}(i \\notin \\mathcal{T})=1-\\left(1-p_{i}\\right)^{N}\n$$\n\nSubstituting (21) into yields\n\n$$\n\\mathbb{E}[|\\mathcal{T}|]=\\sum_{i=1}^{T}\\left[1-\\left(1-p_{i}\\right)^{N}\\right]=\\left[1-\\frac{\\sum_{i=1}^{T}\\left(1-p_{i}\\right)^{N}}{T}\\right] \\cdot T=\\left[1-C_{T, N, \\mathcal{D}_{\\tau}}\\right] \\cdot T\n$$\n\nwhere $C_{T, N, \\mathcal{D}_{\\tau}}=\\left(\\sum_{i=1}^{T}\\left(1-p_{i}\\right)^{N}\\right) / T$. An upper bound for $C_{T, N, \\mathcal{D}_{\\tau}}$ is given as\n\n$$\nC_{T, N, \\mathcal{D}_{\\tau}}=\\left[1-\\frac{\\sum_{i=1}^{T}\\left(1-p_{i}\\right)^{N}}{T}\\right] \\cdot T \\leq\\left[1-\\left(1-\\frac{1}{T}\\right)^{N}\\right] \\cdot T\n$$\n\nwhere the inequality holds if and only if $p_{i}=1 / T$ for all $i \\in[T]$, i.e., $\\mathcal{D}_{\\tau}$ is a uniform distribution. Remark D.2. Theorem D.1 suggests that even when $T \\leq N$, our method still provides a significant improvement. Specifically, for $T=N \\geq 4$, we have $C_{T, N, \\mathcal{D}_{\\tau}}=(1-1 / N)^{N} \\geq 0.3$. This implies that our model requires at most $0.7 T$ even in the worst case. Moreover, if we consider a special scenario where the number of $p_{i}$ satisfying $p_{i}<\\epsilon$ is more than $M$, then we have $C_{T, N, \\mathcal{D}_{\\tau}}>M(1-\\epsilon)^{N} / T$, indicating that with $M$ sufficiently large and $\\epsilon$ sufficiently small, $C_{T, N, \\mathcal{D}_{\\tau}}$ can be pretty close to 1 . Remark D.3. In practical applications of our model, we employ a beta distribution for $\\mathcal{D}_{\\tau}$, which typically exhibits a right-heavy tail. Therefore $C_{T, N, \\mathcal{D}_{\\tau}}$ tends to be larger than that in the worst-case scenario. In Tables 6 and 7 , we list the average NFE for each experiment we run in 84 . These results demonstrate a significant reduction in NFE compared to the original counts: for $T=25$, the NFE is only about half of the original count; for $T=50$, it is approximately one-third; and for $T=1000$, it reduces to less than one-twentieth of the original count.",
    "derandddm-17": "Remark D.4. By Bernoulli's inequality, $(1-p)^{N}>1-N \\cdot p$ for $1>p>0$. Therefore, $C_{T, N, \\mathcal{D}_{\\tau}}>$ $1-N / T$, implying that $\\mathbb{E}[|\\mathcal{T}|]<N$. As $T \\rightarrow \\infty$, assuming the transition time does not concentrate at a single point, the probability that two transitions occur simultaneously is zero. Consequently, the generation process will sequentially go through each token. Thus, the expected number of function evaluations (NFE), $\\mathbb{E}[|\\mathcal{T}|]$, will be $N$. In contrast, when $T$ is finite, there is a non-zero probability that multiple transitions happen at the same time. Hence, in this case, the NFE, $|\\mathcal{T}|$, is strictly less than $N$\n\nTable 6: BLEU score and the average number of function evaluations (NFE) comparison of multinomial diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. The blue background highlights our algorithms. The average NFE values are calculated by dividing the number of times calling the denoising function (neural network) during generation by the number of batches, where the batch sizes of all experiments are 100 . | Dataset | Steps | RDM-Multi |  | DNDM-Multi |  | RDM- $k$-Multi |  | DNDM- $k$-Multi |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | BLEU | Avg NFE | BLEU | Avg NFE | BLEU | Avg NFE | BLEU | Avg NFE |\n| IWSLT14 | 25 | $\\mathbf{3 1 .",
    "derandddm-18": "2 6}$ | 25 | 30.95 | $\\mathbf{9 . 0 3}$ | $\\mathbf{3 2 . 8 2}$ | 25 | 32.30 | $\\mathbf{9 . 0 3}$ |\n|  | 50 | $\\mathbf{3 1 . 5 0}$ | 50 | 31.45 | $\\mathbf{1 4 . 0 7}$ | $\\mathbf{3 2 . 8 2}$ | 50 | 32.80 | $\\mathbf{1 4 . 0 7}$ |\n|  | 1000 | 31.69 | 1000 | $\\mathbf{3 1 . 8 2}$ | $\\mathbf{3 0 . 3 3}$ | 32.64 | 1000 | $\\mathbf{3 3 . 1 5}$ | $\\mathbf{3 0 . 3 3}$ |\n|  | $\\infty$ | - | - | $\\mathbf{3 1 . 8 9}$ | $\\mathbf{3 2 . 7 3}$ | - | - | $\\mathbf{3 3 . 4 4}$ | $\\mathbf{3 2 . 7 3}$ |\n| WMT14 | 25 | $\\mathbf{2 5 .",
    "derandddm-19": "2 5}$ | 25 | 25.01 | $\\mathbf{1 3 . 5 2}$ | $\\mathbf{2 6 . 0 3}$ | 25 | 25.98 | $\\mathbf{1 3 . 5 2}$ |\n|  | 50 | $\\mathbf{2 5 . 7 5}$ | 50 | 25.33 | $\\mathbf{2 0 . 5 8}$ | 26.14 | 50 | $\\mathbf{2 6 . 3 7}$ | $\\mathbf{2 0 . 5 8}$ |\n|  | 1000 | 25.66 | 1000 | $\\mathbf{2 5 . 7 1}$ | $\\mathbf{3 8 . 9 4}$ | 25.82 | 1000 | $\\mathbf{2 6 . 8 8}$ | $\\mathbf{3 8 . 9 4}$ |\n|  | $\\infty$ | - | - | $\\mathbf{2 4 . 7 9}$ | $\\mathbf{4 0 . 6 7}$ | - | - | $\\mathbf{2 6 . 3 9}$ | $\\mathbf{4 0 . 6 7}$ |\n| WMT16 | 25 | $\\mathbf{3 2 .",
    "derandddm-20": "2 9}$ | 25 | 31.97 | $\\mathbf{8 . 5}$ | $\\mathbf{3 3 . 1 2}$ | 25 | 32.94 | $\\mathbf{8 . 5}$ |\n|  | 50 | $\\mathbf{3 2 . 5 3}$ | 50 | 32.50 | $\\mathbf{1 4 . 7 3}$ | $\\mathbf{3 3 . 4 1}$ | 50 | 33.26 | $\\mathbf{1 4 . 7 3}$ |\n|  | 1000 | 32.63 | 1000 | $\\mathbf{3 2 . 8 6}$ | $\\mathbf{3 8 . 4 5}$ | 33.67 | 1000 | $\\mathbf{3 3 . 7 9}$ | $\\mathbf{3 8 . 4 5}$ |\n|  | $\\infty$ | - | - | $\\mathbf{3 2 . 9 1}$ | $\\mathbf{4 1 .",
    "derandddm-21": "6 4}$ | - | - | $\\mathbf{3 3 . 8 6}$ | $\\mathbf{4 1 . 6 4}$ |\n\n## E Discrete Non-Markov Diffusion Model with Top-k Transition Time (DNDM-K).",
    "derandddm-22": "Recent works have demonstrated that the quality of samples can be enhanced by utilizing supplementary information derived from the neural network (Ghazvininejad et al., 2019; Savinov et al.",
    "derandddm-23": "2021; Chang et al. 2022, He et al., 2022). Very recently, Zheng et al. (2023) applied this idea in their RDM framework and can achieve significant performance improvement. Specifically, after decoding $\\widehat{\\mathbf{x}}_{0,1: N}$ from transformer $p_{\\theta}\\left(\\cdot \\mid \\mathbf{x}_{t, 1: N}\\right)$, the score corresponding to this decoded token from the transformer's last layer, is also recorded and denote as $s_{t, n}$.",
    "derandddm-24": "Tokens with high scores are more likely to be selected for updates. Inspired by Zheng et al. (2023), we introduce the discrete non-Markov discrete diffusion Model\n\nTable 7: BLEU score and the average number of function evaluations (NFE) comparison of absorbing diffusion on machine translation benchmarks IWSLT14 DE-EN, WMT14 EN-DE, and WMT16 EN-RO. The blue background highlights our algorithms. The average NFE values are calculated by dividing the number of times calling the denoising function (neural network) during generation by the number of batches, where the batch sizes of all experiments are 100 . | Dataset | Steps | RDM-Absorb |  | DNDM-Absorb |  | RDM- $k$-Absorb |  | DNDM- $k$-Absorb |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | BLEU | Avg NFE | BLEU | Avg NFE | BLEU | Avg NFE | BLEU | Avg NFE |\n| IWSLT14 | 25 | 31.58 | 25 | $\\mathbf{3 2 .",
    "derandddm-25": "4 3}$ | $\\mathbf{1 3 . 8 1}$ | $\\mathbf{3 4 . 5 0}$ | 25 | 34.14 | $\\mathbf{1 3 . 8 1}$ |\n|  | 50 | 31.80 | 50 | $\\mathbf{3 2 . 6 3}$ | $\\mathbf{1 9 . 2 4}$ | $\\mathbf{3 4 . 5 8}$ | 50 | 34.34 | $\\mathbf{1 9 . 2 4}$ |\n|  | 1000 | 31.91 | 1000 | $\\mathbf{3 2 . 9 3}$ | $\\mathbf{3 1 . 0 8}$ | $\\mathbf{3 4 . 6 0}$ | 1000 | 34.56 | $\\mathbf{3 1 . 0 8}$ |\n|  | $\\infty$ | - | - | $\\mathbf{3 3 . 0 3}$ | $\\mathbf{3 2 . 0 7}$ | - | - | $\\mathbf{3 4 . 6 5}$ | $\\mathbf{3 2 . 0 7}$ |\n| WMT14 | 25 | 24.97 | 25 | $\\mathbf{2 5 .",
    "derandddm-26": "7 9}$ | $\\mathbf{1 5 . 0 9}$ | $\\mathbf{2 7 . 5 0}$ | 25 | 27.18 | $\\mathbf{1 5 . 0 9}$ |\n|  | 50 | 24.95 | 50 | $\\mathbf{2 6 . 1 0}$ | $\\mathbf{2 2 . 4 5}$ | $\\mathbf{2 7 . 7 3}$ | 50 | 27.66 | $\\mathbf{2 2 . 4 5}$ |\n|  | 1000 | 25.22 | 1000 | $\\mathbf{2 6 . 4 3}$ | $\\mathbf{3 8 . 7 6}$ | 27.75 | 1000 | $\\mathbf{2 7 . 8 2}$ | $\\mathbf{3 8 . 7 6}$ |\n|  | $\\infty$ | - | - | $\\mathbf{2 6 . 5 0}$ | $\\mathbf{4 0 . 3 9}$ | - | - | $\\mathbf{2 7 . 5 0}$ | $\\mathbf{4 0 . 3 9}$ |\n| WMT16 | 25 | 32.86 | 25 | $\\mathbf{3 3 . 2 0}$ | $\\mathbf{1 3 . 9 1}$ | 33.92 | 25 | $\\mathbf{3 3 . 9 6}$ | $\\mathbf{1 3 . 9 1}$ |\n|  | 50 | 32.93 | 50 | $\\mathbf{3 3 . 3 0}$ | $\\mathbf{2 0 . 9 5}$ | 34.10 | 50 | $\\mathbf{3 4 . 2 0}$ | $\\mathbf{2 0 . 9 5}$ |\n|  | 1000 | 33.25 | 1000 | $\\mathbf{3 3 . 6 0}$ | $\\mathbf{3 8 . 2 7}$ | $\\mathbf{3 4 . 4 4}$ | 1000 | 34.38 | $\\mathbf{3 8 . 2 7}$ |\n|  | $\\infty$ | - | - | $\\mathbf{3 3 . 4 2}$ | $\\mathbf{4 1 . 5 9}$ | - | - | $\\mathbf{3 4 . 4 1}$ | $\\mathbf{4 1 . 5 9}$ |\n\nwith top-K transition time (DNDM-K). Instead of directly determining which token gets updated at step $t$ by first drawing transition time $\\tau \\sim \\mathcal{D}_{\\tau}$, we employ a two-step process. 1. We first compute $K_{t}=\\sum_{n=1}^{N} \\mathbb{1}\\left(\\tau_{n} \\geq t\\right)$. $k_{t}$ represents how many tokens should be decoded at the current step.",
    "derandddm-27": "2. Compare $K_{t-1}$ and $K_{t}$, if $K_{t-1}=K_{t}$. There is no transition time at time $t$, we just update $\\mathbf{x}_{t-1,1: N}=\\mathbf{x}_{t, 1: N}$. If $K_{t-1}>K_{t}$, Then there exist transition time at time $t$, we calculate and select the indexes with top- $K_{t-1}$ scores. Then we update those tokens if it hasn't been updated yet. Subsequently, we will only update those tokens with the highest $K_{t}$ score that hasn't been changed yet. Since the function evaluation occurs only when $K_{t}$ changes, DNDM-K can give an accelerated sampling algorithm.",
    "derandddm-28": "The details are presented in Algorithm 3. ## F Experiment details\n\n## F. 1 Conditional Text Generation\n\nParameter choices. In all experiments, the batch size is chosen to be 100. For RDM and RDM- $k$, our hyperparameter settings follow the original paper (Zheng et al., 2023) except for the batch size. Before the sampling, we used the saved checkpoint of trained models provided by the authors for discrete sampling experiments, and we trained the corresponding models for continuous sampling experiments. For finite-step DNDM, the transition times are determined by the schedule, and we approximate the schedule with a Beta distribution $\\operatorname{Beta}(\\alpha, \\beta)$ (please refer to Section 3.2 for detailed explanation). The $\\alpha$ and $\\beta$ values are selected by applying grid search on the validation sets. Based on the BLEU scores on the validation sets, we have selected Beta( 15,7 ) for Multinormial Diffusion on IWSLT14, Beta $(3,3)$ for Absorbing Diffusion on both IWSLT14 and WMT14, Beta $(5,3)$ for Multinormial Diffusion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4f009531c71804551fabg-24.jpg?height=1034&width=1666&top_left_y=252&top_left_x=229)\non WMT14 and Absorbing Diffusion on WMT16, and Beta(20,7) for Multinormial Diffusion on WMT16. For infinite-steps (continuous-step) diffusion (DNDM-C), the transition timestamps are sampled from $\\operatorname{Beta}(\\alpha, \\beta)$, where the choice of $(\\alpha, \\beta)$ are chosen from $(100.0,4.0)$ or $(17.0,4.0)$, based on the performance comparison on the validation set. In the end we choose $\\operatorname{Beta}(17,4)$ for IWSLT14 and $\\operatorname{Beta}(100,4)$ for WMT14 and WMT16. We conduct a performance comparison based on varying configurations of the Beta and Alpha distributions. The results of these comparisons are presented in Tables 9 and 8 . Furthermore, to evaluate the efficacy of discrete versus continuous step schemes, we also conduct an ablation study under the same set of parameters $(100,4)$ in Table 10. Continuous time vs discrete time diffusions. To test our hypothesis that the continuous-time sampler will produce more accurate results in reverse sampling if our $\\mathbf{x}_{0}$ estimator consistently approximates the true $\\mathbf{x}_{0}$ over time, we conduct various sampling experiments using a shared pre-trained neural network. For discrete-time sampling, we consider three cases: $T=25,50,1000$. In each case, we rescale the interval $[0, T]$ to $[0,50]$ and divide it into $T$ fractions. In contrast, for continuous-time sampling, we directly sample from a continuous distribution over the interval $[0,50]$ without any partitioning. Training approach. In machine translation tasks, the neural network is designed to learn $q\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{t}, \\mathbf{z}\\right)$, where $\\mathbf{z}$ represents the embedding of the source text obtained using transformer encoder layers. For a fair comparison, we employ the same neural network structure as our baseline, with detailed architecture specifications available in Section E.",
    "derandddm-29": "2 of Zheng et al. (2023). Furthermore, given that the primary focus of this paper is the speed and effectiveness of our sampling algorithm, we omit the training procedure and instead use a state-of-the-art diffusion-based pretrained checkpoint\n\nTable 8: BLEU scores on dataset WMT16 from the ablation study of other different $\\operatorname{Beta}(\\alpha, \\beta)$ distributions of the transition time with 1000 sampling steps. | Model | Alpha | Beta |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 3 | 5 | 7 | 9 | 11 | 13 | 15 | 17 | 19 | 21 |\n| DNDM-k-Multi | 3 | 33.47 | 33.67 | 33.62 | 33.77 | 33.87 | 33.64 | 33.73 | 33.60 | 33.68 | 33.56 |\n|  | 5 | 33.18 | 33.47 | 33.68 | 33.53 | 33.71 | 33.69 | 33.73 | 33.72 | 33.74 | 33.82 |\n|  | 7 | 32.99 | 33.20 | 33.49 | 33.56 | 33.58 | 33.61 | 33.67 | 33.72 | 33.78 | 33.83 |\n| DNDM-Multi | 3 | 32.73 | 32.66 | 32.74 | 32.82 | 32.77 | 32.92 | 32.80 | 32.81 | 32.76 | 32.86 |\n|  | 5 | 32.32 | 32.62 | 32.70 | 32.80 | 32.83 | 32.83 | 32.90 | 32.95 | 32.91 | 32.87 |\n|  | 7 | 32.35 | 32.35 | 32.53 | 32.67 | 32.75 | 32.78 | 32.86 | 32.80 | 32.86 | 32.88 |\n| DNDM-k-Absorb | 3 | 34.19 | 34.38 | 34.34 | 34.22 | 34.21 | 34.24 | 34.07 | 34.31 | 34.42 | 34.36 |\n|  | 5 | 32.15 | 33.99 | 34.29 | 34.30 | 34.29 | 34.40 | 34.40 | 34.24 | 34.30 | 34.22 |\n|  | 7 | 27.67 | 32.87 | 33.94 | 34.28 | 34.27 | 34.38 | 34.31 | 34.29 | 34.38 | 34.40 |\n| DNDM-Absorb | 3 | 33.53 | 33.60 | 33.67 | 33.71 | 33.71 | 33.70 | 33.58 | 33.63 | 33.53 | 33.54 |\n|  | 5 | 32.70 | 33.33 | 33.52 | 33.60 | 33.66 | 33.73 | 33.70 | 33.74 | 33.72 | 33.74 |\n|  | 7 | 30.56 | 32.65 | 33.28 | 33.37 | 33.51 | 33.52 | 33.61 | 33.67 | 33.63 | 33.67 |\n\nTable 9: BLEU scores on dataset WMT16 from the ablation study of other different $\\operatorname{Beta}(\\alpha, \\beta)$ distributions of the transition time with 50 sampling steps. | Model | Alpha | Beta |  |  |  |  |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 3 | 5 | 7 | 9 | 11 | 13 | 15 | 17 | 19 | 21 |\n| DNDM-k-Multi | 3 | 33.31 | 33.47 | 33.39 | 33.48 | 33.29 | 33.23 | 33.25 | 33.27 | 33.11 | 33.17 |\n|  | 5 | 32.93 | 33.28 | 33.29 | 33.58 | 33.45 | 33.21 | 33.40 | 33.49 | 33.16 | 33.19 |\n|  | 7 | 32.61 | 32.98 | 33.31 | 33.20 | 33.27 | 33.41 | 33.39 | 33.53 | 33.35 | 33.08 |\n| DNDM-Multi | 3 | 32.63 | 32.46 | 32.44 | 32.56 | 32.59 | 32.55 | 32.37 | 32.33 | 32.22 | 32.23 |\n|  | 5 | 32.31 | 32.43 | 32.66 | 32.64 | 32.68 | 32.55 | 32.55 | 32.44 | 32.35 | 32.30 |\n|  | 7 | 31.95 | 32.11 | 32.22 | 32.26 | 32.54 | 32.52 | 32.50 | 32.58 | 32.48 | 32.41 |\n| DNDM-k-Absorb | 3 | 34.05 | 34.2 | 34.31 | 34.37 | 34.15 | 34.05 | 34.06 | 33.77 | 33.81 | 33.84 |\n|  | 5 | 32.30 | 34.08 | 34.30 | 34.38 | 34.26 | 34.23 | 34.09 | 34.06 | 34.02 | 34.13 |\n|  | 7 | 27.39 | 32.64 | 33.71 | 34.18 | 34.02 | 34.33 | 34.31 | 34.17 | 34.12 | 34.19 |\n| DNDM-Absorb | 3 | 33.26 | 33.30 | 33.29 | 33.24 | 33.23 | 32.97 | 33.06 | 32.85 | 32.89 | 32.63 |\n|  | 5 | 32.47 | 33.08 | 33.31 | 33.22 | 33.41 | 33.25 | 33.15 | 33.27 | 33.04 | 32.98 |\n|  | 7 | 30.34 | 32.27 | 33.27 | 33.03 | 33.16 | 33.14 | 33.27 | 33.11 | 33.11 | 33.07 |\n\nTable 10: The BLEU scores on dataset WMT16 with $\\operatorname{Beta}(100,4)$ as the transition time schedule for discrete sampling or the distribution to sample transition timestamps for continuous sampling. | Steps | DNDM-k-multi | DNDM-k-absorb | DNDM-multi | DNDM-absorb |\n| :---: | :--- | :--- | :--- | :--- |\n| 50 | 31.60 | 31.74 | 30.39 | 29.69 |\n| 1000 | 33.59 | 34.37 | 32.87 | 33.52 |\n| $\\infty$ | 33.86 | 34.41 | 32.91 | 33.42 |\n\nfrom Zheng et al. (2023). In the Appendix, we present additional results of continuous sampling based on a continuously trained checkpoint. In this setting, we rescale our network input to the interval $[0,1]$ and uniformly sample from this interval.",
    "derandddm-30": "The rest of the architecture follows that\nof Zheng et al. (2023). Performance on WMT14. Our work primarily focuses on the sampling process, and for the training, we utilized a pretrained checkpoint trained on 50 steps. In our sampling experiments we noticed that our method does not work ideally on WMT14, this could be possibly attributed to the fact that the training performance on WMT14 was not ideal. Specifically, when we performed sampling using 1000 steps, the network was trained with exposure to only 50 time steps, specifically at intervals of $20(0,20,40, \\ldots, 980,1000)$. As a result, when we apply our model to generation using 1000 steps, the checkpoint NN has only been explicitly trained on these intervals. While we generally assume that the network can still provide a good estimate for the untrained steps, this might not hold under some hard scenarios. Considering the longer training time and poorer performance of WMT14, it is likely that the training performance is insufficient for us to rely on those unseen steps. In a word, the model's trained checkpoint may not be robust enough to effectively handle unseen steps, especially for timesteps 1000 or infinite timesteps. ## F. 2 Unconditional Text Generation\n\nParameter choices. We recover the checkpoints of the multinomial diffusion model employing the provided code by Hoogeboom et al. (2021b). We train 12-layer Transformers for both text8 and enwik8 datasets for 500 epochs with the cosine schedule. For the text8 dataset, we utilize a training batch size of 256 , while for the enwik8 dataset, we use a batch size of 128 . During training, we employ a learning rate of 0.0001 , a weight decay parameter of 0.99 , and the Adam optimizer. ## G Additional Experiments\n\nIn this section, we present additional experimental results. We begin by plotting the relationship between computational time and the number of sampling steps, using the absorbing diffusion in IWSLT14 as an example. Figure 4 displays the growth of computational time for absorbing diffusion (yellow and orange lines), RDM-absorbing diffusion, and our model DNDM-Absorb and DNDM-TAbsorb (green and blue lines). We see from Figure 4 that previous algorithms, including absorbing diffusion and RDM-absorbing diffusion all suffer from linear growth of computational time. ## G. 1 Continuous Training\n\nComparison with ARDM (Hoogeboom et al., 2021a). Autoregressive Diffusion Model (ARDM) (Hoogeboom et al., 2021a) is a discrete diffusion model built upon the autoregressive nature of data. ARDM is shown to be equivalent to a continuous-time absorbing diffusion model and thus provides a unique perspective for discrete diffusion. For continuous-time $(T=\\infty)$ reverse sampling, both ARDM and our method achieve $N$ NFEs. Compared with ARDM, our method provides a unified framework including both absorbing and multinomial diffusions, applicable to both finite time and continuous time diffusions. For infinite timesteps, Hoogeboom et al. 2021a) also proposed an advanced parallelizing technique that can reduce NFE according to the log-likelihood, which we have not considered in DNDM-C. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f009531c71804551fabg-27.jpg?height=1009&width=1654&top_left_y=233&top_left_x=233)\n\nFigure 4: The growth of computational time with the increase of the sampling steps\n\n```\nAlgorithm 4 Sampling from DNDM-C\nRequire: Trained prediction function \\(p_{\\boldsymbol{\\theta}}, \\mathbf{q}_{\\text {noise }}, \\mathcal{D}_{\\tau}\\)\n    for \\(n=1 \\ldots N\\) do\n        Initiate each token \\(\\mathbf{x}_{T, n} \\sim \\mathbf{q}_{\\text {noise }}\\)\n        Initiate the transition time \\(\\tau_{n} \\sim \\mathcal{D}_{\\tau}\\) and order them as \\(\\tau_{n_{1}}<\\ldots<\\tau_{n_{N}}\\)\n    end for\n    for \\(k=N \\ldots 1\\) do\n        Generate \\(\\widetilde{\\mathbf{x}}_{0,1: N}\\) from \\(p_{\\boldsymbol{\\theta}}\\left(\\cdot \\mid \\mathbf{x}_{\\tau_{n_{k}}, 1: N}, \\tau_{n_{k}}\\right)\\)\n        for \\(n=1 \\ldots N\\) do\n            Update \\(\\mathbf{x}_{\\tau_{n_{k-1}}, n}\\) based on condition of \\(\\tau_{n}\\)\n        end for\n    end for\n    Return \\(\\mathbf{x}_{0,1: N}\\)\n```\n\nIn Section 4.1, we introduce the DNDM-C algorithm, designed for continuous-time, over discretetime algorithms.",
    "derandddm-31": "However, this algorithm assumes that we have learned a sufficiently accurate neural network at any timestamp $t \\in[0,1]$. Using the checkpoint trained with 50 discrete time partitions might not suffice for the purpose of continuous sampling. In this section, we investigate the performance of continuous sampling when training is also done continuously. In Table 11, we summarize the performance of DNDM-C based on a neural network estimated continuously during training time. This involves sampling time uniformly from $[0,1]$ during training, and the forward process follows (11) in Section 3.3. The training objective remains the same as\n\nTable 11: Continuous Training + Continuous Sampling\n\n| Dataset | Step scheme | C-DNDM-Multi |  | C-DNDM-Absorb |  |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | Default | Top-k | Default | Top-k |\n| IWSLT14 | Continuous | $\\mathbf{3 2 . 0 7}$ | $\\mathbf{3 3 . 5 7}$ | 32.80 | 34.52 |\n| WMT16 | Continuous | $\\mathbf{3 3 . 4 8}$ | 33.71 | $\\mathbf{3 3 . 5 0}$ | 34.36 |\n\nin discrete-time training. In Table 11 we list the result of IWSLT14 and WMT16 with continuous training followed by continuous sampling. In addition, we compare the value with the corresponding value during discrete training and continuous sampling in Section 4.1 and mark every item that improves in bold. As demonstrated in Table 11, there is room for enhancement in the overall sampling scores by training the neural network in a complete space of timestamps. ## G. 2 Comparison with more generative models\n\nIn our study, a key aspect of evaluating our fast discrete generative model involves comparisons with prior work known for speed in sampling with minimal steps. Specifically, we draw a direct comparison with the Mask-Predict (Ghazvininejad et al., 2019), which is notable for its ability to generate high-quality results within just 10 iterations. The results are shown in Table 12. All experiments were conducted on the same GPU and within the same machine setup. Table 12: The performance comparison on WMT16 of DNDM with Mask-Predict (Ghazvininejad et al. 2019). We align the number of sampling steps used in Mask-Predict with a similar number of function evaluations (NFE) in our DNDM algorithm. We see that our Algorithm runs faster, with better BLEU score. | Mask-Predict |  |  |  | DNDM-Absorb |  |  |  | DNDM-k-Absorb |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Steps | BLEU | Time | Steps | BLEU | Time | NFE | Steps | BLEU | Time | NFE |  |\n| 10 | 33.08 | 49.25 | 25 | 33.20 | 41.2 | 13.91 | 25 | 33.96 | 41.4 | 13.91 |  |\n| 15 | 33.06 | 67.94 | 50 | 33.30 | 62.5 | 20.95 | 50 | 34.20 | 62.7 | 20.95 |  |\n| 25 | 33.16 | 111.89 | 1000 | 33.60 | 121.3 | 38.27 | 1000 | 34.38 | 122.7 | 38.27 |  |\n| 40 | 33.10 | 169.95 | $\\infty$ | 33.42 | 121.8 | 41.59 | $\\infty$ | 34.41 | 121.9 | 41.59 |  |\n\n## G. 3 Samples from the multinomial text models\n\nConditional Generation. For DNDM-Multi trained on IWSLT14, we provide a full generation process with 100 steps in Figure 5. A token ending with @@ indicates it is an incomplete word; it will be concatenated with the following token to form a complete word. For example, \"fel@@ lo@@ ws\" means \"fellows\". We can see that after $t=39$, the generate sentence converges. ## References\n\nAlain, G., Bengio, Y., Yao, L., Yosinski, J., Thibodeau-Laufer, E., Zhang, S. and Vincent, P. (2016). Gsns: generative stochastic networks. Information and Inference: A Journal of the IMA $5210-249$. AliAS PARTH GOYAL, A. G., Ke, N. R., Ganguli, S. and Bengio, Y. (2017). Variational walkback: Learning a transition operator as a stochastic recurrent net.",
    "derandddm-32": "Advances in Neural Information Processing Systems 30.",
    "derandddm-33": "Austin, J., Johnson, D. D., Ho, J., Tarlow, D. and Van Den Berg, R. (2021). Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems $3417981-17993$. Bao, F., Li, C., Zhu, J. and Zhang, B. (2022). Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. arXiv preprint arXiv:2201.06503 . Bengio, Y., Laufer, E., Alain, G. and Yosinski, J. (2014). Deep generative stochastic networks trainable by backprop.",
    "derandddm-34": "In International Conference on Machine Learning. PMLR. Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L.",
    "derandddm-35": "and Tamchyna, A. (2014). Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation. Association for Computational Linguistics, Baltimore, Maryland, USA. Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huck, M., Jimeno Yepes, A., Koefn, P., Logacheva, V., Monz, C., Negri, M., N\u00e9v\u00e9ol, A., Neves, M., Popel, M., Post, M., Rubino, R., Scarton, C., Specia, L., Turchi, M., Verspoor, K. and Zampieri, M. (2016). Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers. Association for Computational Linguistics, Berlin, Germany. Bordes, F., Honari, S. and Vincent, P. (2017). Learning to generate samples from noise through infusion training. arXiv preprint arXiv:1703.06975 . Campbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G. and Doucet, A. (2022). A continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems 3528266 -28279.",
    "derandddm-36": "Ceritli, T., Ghosheh, G. O., Chauhan, V. K., Zhu, T., Creagh, A. P. and Clifton, D. A. (2023). Synthesizing mixed-type electronic health records using diffusion models. arXiv preprint arXiv:2302.14679 . Cettolo, M., Niehues, J., St\u00fcker, S., Bentivogli, L. and Federico, M. (2014). Report on the 11th IWSLT evaluation campaign. In Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign.",
    "derandddm-37": "Lake Tahoe, California. Chang, H., Zhang, H., Jiang, L., Liu, C. and Freeman, W. T. (2022). Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M. and Chan, W. (2020). Wavegrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713 . Chung, H., Sim, B. and Ye, J. C. (2022). Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Dockhorn, T., Vahdat, A. and Kreis, K. (2021). Score-based generative modeling with critically-damped langevin diffusion. arXiv preprint arXiv:2112.07068 . Dockhorn, T., Vahdat, A. and Kreis, K. (2022). Genie: Higher-order denoising diffusion solvers. Advances in Neural Information Processing Systems 35 30150-30166. Ghazvininejad, M., Levy, O., Liu, Y. and Zettlemoyer, L. (2019). Mask-predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324 . Gruver, N., Stanton, S., Frey, N., Rudner, T. G., Hotzel, I., Lafrance-Vanasse, J., Rajpal, A., Cho, K. and Wilson, A.",
    "derandddm-38": "G. (2024). Protein design with guided discrete diffusion. Advances in Neural Information Processing Systems 36. He, Z., Sun, T., Wang, K., Huang, X. and Qiu, X. (2022). Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029 . Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D.",
    "derandddm-39": "J. et al. (2022). Imagen video: High definition video generation with diffusion models.",
    "derandddm-40": "arXiv preprint arXiv:2210.02303. Ho, J., Jain, A. and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems $336840-6851$.",
    "derandddm-41": "Hoogeboom, E., Gritsenko, A. A., Bastings, J., Poole, B., Berg, R. v. D. and Salimans, T. (2021a). Autoregressive diffusion models. arXiv preprint arXiv:2110.02037 . Hoogeboom, E., Nielsen, D., Jaini, P., Forr\u00e9, P. and Welling, M. (2021b). Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems 34 12454-12465. Jolicoeur-Martineau, A., Li, K., Pich\u00e9-Taillefer, R., Kachman, T. and Mitliagkas, I. (2021). Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080. Karras, T., Aittala, M., Aila, T. and Laine, S. (2022). Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems 35 2656526577 . Kong, Z. and Ping, W. (2021). On fast sampling of diffusion probabilistic models. arXiv preprint arXiv:2106.00132 . Kong, Z., Ping, W., Huang, J., Zhao, K. and Catanzaro, B. (2020). Diffwave: A versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761 . Liu, L., Ren, Y., Lin, Z. and Zhao, Z. (2022). Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2002.09778 . Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C. and Zhu, J. (2022). Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.",
    "derandddm-42": "Advances in Neural Information Processing Systems 35 5775-5787. Lyu, S. (2012). Interpretation and generalization of score matching. arXiv preprint arXiv:1205.2629\n\nMovellan, J.",
    "derandddm-43": "R. (2008). Contrastive divergence in gaussian diffusions. Neural Computation 20 $2238-2252$.",
    "derandddm-44": "Nachmani, E., Roman, R. S. and Wolf, L. (2021). Non gaussian denoising diffusion models. arXiv preprint arXiv:2106.07582 .",
    "derandddm-45": "Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International Conference on Machine Learning. PMLR. Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D. and Auli, M. (2019). fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038\n\nPapineni, K., Roukos, S., Ward, T.",
    "derandddm-46": "and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation.",
    "derandddm-47": "In Proceedings of the 40 th annual meeting of the Association for Computational Linguistics. Reid, M., Hellendoorn, V. J. and Neubig, G. (2022). Diffuser: Discrete diffusion via edit-based reconstruction. arXiv preprint arXiv:2210.16886 . Salimans, T. and Ho, J. (2022). Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512 . San-Roman, R., Nachmani, E. and Wolf, L. (2021). Noise estimation for generative diffusion models. arXiv preprint arXiv:2104.02600 . Savinov, N., Chung, J., Binkowski, M., Elsen, E. and Oord, A.",
    "derandddm-48": "v. D. (2021). Step-unrolled denoising autoencoders for text generation. arXiv preprint arXiv:2112.06749 . Sennrich, R., Haddow, B. and Birch, A. (2016). Neural machine translation of rare words with subword units.",
    "derandddm-49": "Sohl-Dickstein, J., Battaglino, P.",
    "derandddm-50": "and DeWeese, M. R. (2009). Minimum probability flow learning. arXiv preprint arXiv:0906.4779 . Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N. and Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics.",
    "derandddm-51": "In International conference on machine learning. PMLR. Song, J., Meng, C. and Ermon, S. (2020a). Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 . Song, Y., Dhariwal, P., Chen, M. and Sutskever, I. (2023). Consistency models. arXiv preprint arXiv:2303.01469. Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems 32. Song, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models. Advances in neural information processing systems $3312438-12448$. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S. and Poole, B. (2020b). Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 . Sun, H., Yu, L., Dai, B., Schuurmans, D. and Dai, H. (2022). Score-based continuous-time discrete diffusion models. arXiv preprint arXiv:2211.16750 . Vahdat, A., Kreis, K. and Kautz, J. (2021). Score-based generative modeling in latent space. Advances in Neural Information Processing Systems 34 11287-11302. Watson, D., Ho, J., Norouzi, M. and Chan, W. (2021). Learning to efficiently sample from diffusion probabilistic models. arXiv preprint arXiv:2106.03802 . Ye, J., Zheng, Z., Bao, Y., Qian, L. and Gu, Q. (2023). Diffusion language models can perform many tasks with scaling and instruction-finetuning. arXiv preprint arXiv:2308.12219 . Zhang, Q. and Chen, Y. (2022). Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902 . Zheng, L., Yuan, J., Yu, L. and Kong, L. (2023). A reparameterized discrete diffusion model for text generation. arXiv preprint arXiv:2302.05737. $\\mathbf{t}=100$\n[noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise]\n$\\mathbf{t}=79$\n[noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] year [noise]\n$\\mathbf{t}=78$\n[noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] we [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] year [noise]\n$\\mathbf{t}=77$\n[noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] and we [noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] year [noise]\n$\\mathrm{t}=75$\n[noise] [noise] [noise] [noise] [noise] [noise] [noise] [noise] and we [noise] [noise] [noise] [noise] govern@@ [noise] [noise] year [noise]\n$\\mathbf{t}=74$\nwe [noise] [noise] [noise] lo@@ [noise] [noise] [noise] and we [noise] [noise] [noise] [noise] govern@@ [noise] [noise] year [noise]\n$\\mathbf{t}=73$\nwe [noise] [noise] fel@@ lo@@ [noise] [noise] [noise] and we let [noise] [noise] [noise] [noise] govern@@ [noise] [noise] year [noise]\n$\\mathrm{t}=71$\nwe [noise] [noise] fel@@ lo@@ [noise] [noise] [noise] and we let [noise] [noise] [noise] [noise] govern@@ [noise] every year [noise]\n$\\mathbf{t}=67$\nwe [noise] [noise] fel@@ lo@@ [noise] [noise] [noise] and we let them [noise] [noise] city govern@@ [noise] every year. $\\mathbf{t}=66$\nwe [noise] [noise] fel@@ lo@@ ws [noise] [noise] and we let them work [noise] city govern@@ [noise] every year. $\\mathrm{t}=64$\nwe [noise] [noise] fel@@ lo@@ ws [noise] [noise] and we let them work [noise] city govern@@ ance every year. $\\mathbf{t}=61$\nwe [noise] [noise] fel@@ lo@@ ws [noise] [noise] and we let them work with city govern@@ ance every year. $\\mathbf{t}=60$\nwe [noise] [noise] fel@@ lo@@ ws [noise] year and we let them work with city govern@@ ance every year. $\\mathbf{t}=58$\nwe [noise] [noise] fel@@ lo@@ ws every year and we let them work with city govern@@ ance every year. $\\mathbf{t}=52$\nwe [noise] some fel@@ lo@@ ws every year and we let them work with city govern@@ ance every year.",
    "derandddm-52": "$\\mathbf{t}=39$\nwe choose some fel@@ lo@@ ws every year and we let them work with city governance every year. $\\mathbf{t}=\\mathbf{0}$\nwe choose some fel@@ lo@@ ws every year and we let them work with city governance every year. [^0]:    ${ }^{*}$ Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: chenzx19@cs.ucla.edu\n    ${ }^{\\dagger}$ Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: hzyuan@cs.ucla.edu\n    ${ }^{\\ddagger}$ Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: yongqianl@cs.ucla.edu\n    ${ }^{\\S}$ Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: evankou@cs.ucla.edu\n    ${ }^{I}$ Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: jkzhang@cs.ucla.edu\n    ${ }^{\\|}$Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: qgu@cs.ucla.edu\n\n[^1]:    ${ }^{1}$ If we represent $\\alpha_{t}$ with maximum step $T$ as $\\alpha_{t}(T)$, the scale-invariant property states that $\\alpha_{c t}(c T)=\\alpha_{t}(T)$. The simplest example of such an $\\alpha_{t}$ schedule is $\\alpha_{t}(T)=1-t / T$, under which $\\alpha(t)=1-t$. [^2]:    ${ }^{2}$ Due to computational intensity, we did not repeat the 1000 -step sampling for the RDM baseline. However, reproducing it was deemed unnecessary as the sampling time is largely stable across repeated experiments, and the precise averaged timing is not critical for demonstrating the speed improvement of DNDM. "
}