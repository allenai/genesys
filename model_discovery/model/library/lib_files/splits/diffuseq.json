{
    "diffuseq-0": "# DIFFUSEQ: SEQUENCE TO SEQUENCE Text Generation with Diffusion Models \n\nShansan Gong ${ }^{1}$, Mukai Li ${ }^{1}$, Jiangtao Feng ${ }^{1}$, Zhiyong $\\mathbf{W u}^{1}$, Lingpeng Kong $^{2}$<br>${ }^{1}$ Shark-NLP, Shanghai AI Laboratory ${ }^{2}$ The University of Hong Kong<br>\\{gongshansan, limukai, fengjiangtao, wuzhiyong\\}@pjlab.org.cn<br>lpk@cs.hku.hk\n\n\n#### Abstract\n\nRecently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DIFFUSEQ: a diffusion model designed for sequenceto-sequence (SEQ2SEQ) text generation tasks. Upon extensive evaluation over a wide range of SEQ2SEQ tasks, we find DIFFUSEQ achieving comparable or even better performance than six established baselines, including a state-of-theart model that is based on pre-trained language models. Apart from quality, an intriguing property of DIFFUSEQ is its high diversity during generation, which is desired in many SEQ2SEQ tasks. We further include a theoretical analysis revealing the connection between DIFFUSEQ and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. ## 1 InTRODUCTION\n\nAmong existing generative models, GAN (Goodfellow et al., 2014) suffers from the instability issue (Salimans et al., 2016), subjecting to mode collapse (Metz et al., 2017); VAE (Kingma \\& Welling, 2014) has to rely on surrogate objectives to approximate maximum likelihood training and Flow-based models (Dinh et al., 2017) has to use specialized architectures to construct reversible transform. Diffusion models (Ho et al., 2020; Nichol \\& Dhariwal, 2021) have circumvented several of these limitations and emerged as a new paradigm for generative models, theoretically underpinned by non-equilibrium thermodynamics (Sohl-Dickstein et al., 2015) and score-matching network (Song \\& Ermon, 2019). To date, the major breakthroughs are in domains using continuous signals, such as vision (Saharia et al., 2022a;b; Ramesh et al., 2022) and audio (Kong et al., 2020). However, extending continuous diffusion models to natural language remains an open challenge due to the inherently discrete nature of texts. On the basis of unconditional generation in continuous space which is illustrated in Figure 1(a), existing efforts (Hoogeboom et al., 2021; Austin et al., 2021) start customizing diffusion models to text in discrete space on unconditional language modeling (i.e., free text generation). Diffusion-LM (Li et al., 2022), as in Figure 1(b), models texts in continuous space and proposes to use an extra-trained classifier as guidance (i.e., the condition signal $\\mathbf{x}$ ) to impose subtle changes (usually complex, finegrained constraints) on generated sentences. Nonetheless, these models do not naturally generalize to conditional language modeling (i.e., the model assigns probabilities $p(\\mathbf{w} \\mid \\mathbf{x})$ to sequences of words $\\mathbf{w}$ given $\\mathbf{x}$ ). In the more general sequence-to-sequence (SEQ2SEQ) setting where the condition $\\mathbf{x}$ is also a sequence of words, applying Diffusion-LM can be difficult. The reason is that classifiers are attributes-oriented, and we can not train hundreds-of-thousands classifiers to model the semantic meaning between conditions and generated sentences. SEQ2SEQ is an essential setting in NLP that covers a wide range of important tasks such as openended sentence generation, dialogue, paraphrasing, and text style transfer. In this paper, we propose\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_e929bd7e46305c313f33g-02.jpg?height=482&width=1364&top_left_y=278&top_left_x=380)\n\nFigure 1: The demonstration of unconditional, classifier-guided, and classifier-free diffusion models. DIFFUSEQ, depicted in Figure 1(c), a classifier-free diffusion model that supports SEQ2SEQ text generation tasks. By modeling the conditional probability of the target sentence $\\mathbf{w}$ given context $\\mathbf{x}$ using one single model, one advantage of DIFFUSEQ is that this paradigm allows a complete model to fit data distribution and utilize conditional guidance, rather than depending on a separate classifier. Different from canonical generation approaches in an autoregressive (AR) left-to-right manner (Radford et al., 2019), DIFFUSEQ generates text tokens parallelly in the non-autoregressive (NAR) way. To corroborate the effectiveness of our DIFFUSEQ, we conduct experiments on four SEQ2SEQ tasks. Compared to AR and NAR models, which suffer from the \"degeneration\" problem (Holtzman et al., 2019) and rely on decoding strategies, DIFFUSEQ can achieve considerable sentence-level diversity without sacrificing the quality (see $\\S 4.2$ ). To sum up, we make a series of technical and conceptual contributions: (a) we are the first to deploy the diffusion model on SEQ2SEQ text generation, and our proposed DIFFUSEQ as a conditional language model is trained end-to-end in a classifier-free manner; (b) we establish a theoretical connection among AR, NAR and DiffUSEQ models, and justify DIFFUSEQ as an extension of iterative-NAR models; (c) with strong empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.",
    "diffuseq-1": "## 2 Preliminary and Problem Statement\n\nPreliminary. A diffusion model typically contains forward and reverse processes. Given a data point sampled from a real-world data distribution $\\mathbf{z}_{0} \\sim q(\\mathbf{z})$, the forward process gradually corrupts $\\mathbf{z}_{0}$ into a standard Gaussian noise $\\mathbf{z}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$. For each forward step $t \\in[1,2, \\ldots, T]$, the perturbation is controlled by $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\sqrt{1-\\beta_{t}} \\mathbf{z}_{t-1}, \\beta_{t} \\mathbf{I}\\right)$, with $\\beta_{t} \\in(0,1)$ as different variance scales. Once the forward process is completed, the reverse denoising process tries to gradually reconstruct the original data $\\mathbf{z}_{0}$ via sampling from $\\mathbf{z}_{T}$ by learning a diffusion model $f_{\\theta}$.",
    "diffuseq-2": "Problem Statement. Many recent efforts have been devoted to adapting diffusion models to discrete texts (See \u00a75). However, they all focus on unconditional sequence modeling. In this paper, we target the sequence-to-sequence text generation tasks. In particular, given a $m$-length source sequence $\\mathbf{w}^{x}=\\left\\{w_{1}^{x}, \\ldots, w_{m}^{x}\\right\\}$, we aim to learn a diffusion model that can produce a $n$-length target sequence $\\mathbf{w}^{y}=\\left\\{w_{1}^{y}, \\ldots, w_{n}^{y}\\right\\}$ conditioning on the source sequence. ## 3 DIFFUSEQ\n\nWe propose DIFFUSEQ to extend vanilla diffusion models to learn conditional text generation (as shown in Figure 2), concerning the model architecture and the training objective. Forward Process with Partial Noising. In the beginning of forward process, we follow DiffusionLM (Li et al., 2022) to design an embedding function $\\operatorname{EMB}(\\mathbf{w})$ to map the discrete text $\\mathbf{w}$ into a continuous space. In particular, given a pair of sequence $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$, DIFFUSEQ learns a unified\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_e929bd7e46305c313f33g-03.jpg?height=323&width=1348&top_left_y=283&top_left_x=386)\n\nFigure 2: The diffusion process of our conditional diffusion language model DIFFUSEQ.",
    "diffuseq-3": "Given the source $\\mathbf{w}^{x}$ and the target $\\mathbf{w}^{y}$, we pair-wisely transform them into continuous space $\\mathbf{z}_{0}$. The partial Gaussian noise is iteratively added on the target space of $\\mathbf{z}_{t}$. feature space of $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$ by embedding transformation and concatenation as $\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)=$ $\\left[\\operatorname{Emb}\\left(w_{1}^{x}\\right), \\ldots, \\operatorname{Emb}\\left(w_{m}^{x}\\right), \\operatorname{EMB}\\left(w_{1}^{y}\\right), \\ldots, \\operatorname{Emb}\\left(w_{n}^{y}\\right)\\right] \\in \\mathbb{R}^{(m+n) \\times d}$. The transformation allows us to adapt discrete textual input into the standard forward process, by extending the original forward chain to a new Markov transition $q_{\\phi}\\left(\\mathbf{z}_{0} \\mid \\mathbf{w}^{x \\oplus y}\\right)=\\mathcal{N}\\left(\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right), \\beta_{0} \\mathbf{I}\\right)$. We denote $\\mathbf{z}_{t}=\\mathbf{x}_{t} \\oplus \\mathbf{y}_{t}$ to simplify the wordings, where $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ represent parts of $\\mathbf{z}_{t}$ that belong to $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$, respectively. For each forward step $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)$, we gradually inject noise into last step's hidden state $\\mathbf{z}_{t-1}$ to obtain $\\mathbf{z}_{t}$. Unlike conventional diffusion models that corrupt the whole $\\mathbf{z}_{t}$ (both $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$ ) without distinction, we only impose noising on $\\mathbf{y}_{t}$. This modification (termed partial noising) allows us to adapt diffusion models for conditional language modeling. Reverse Process with Conditional Denoising. The ultimate goal of the reverse process is to recover the original $\\mathbf{z}_{0}$ by denoising $\\mathbf{z}_{t}: p_{\\theta}\\left(\\mathbf{z}_{0: T}\\right):=p\\left(\\mathbf{z}_{T}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)$. We model the learning process $p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t-1} ; \\mu_{\\theta}\\left(\\mathbf{z}_{t}, t\\right), \\sigma_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right)$ using the proposed diffusion model DIFFUSEQ: $f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$, where the $\\mu_{\\theta}(\\cdot)$ and $\\sigma_{\\theta}(\\cdot)$ is the parameterization of the predicted mean and standard deviation of $q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)$ in forward process, derived using Bayes' rule.",
    "diffuseq-4": "The detailed derivations are in Appendix A. With the partial nosing strategy adopted in the forward process, we can impose the input as the condition when denoising as shown in Figure 1. The proposed conditional denoising is classifier-free by nature: we do not require extra-trained classifiers to control the denoising process. Specifically, we use a transformer architecture to model $f_{\\theta}$, which spontaneously models the semantic relation between $\\mathbf{x}_{t}$ and $\\mathbf{y}_{t}$. We compute the variational lower bound $\\left(\\mathcal{L}_{\\mathrm{VLB}}\\right)$ following the original diffusion process. $\\mathcal{L}_{\\text {round }}$ corresponds to rounding operation in Figure 2. $$\n\\begin{aligned}\n\\mathcal{L}_{\\mathrm{VLB}} & =\\mathbb{E}_{q\\left(\\mathbf{z}_{1: T} \\mid \\mathbf{z}_{0}\\right)}[\\underbrace{\\log \\frac{q\\left(\\mathbf{z}_{T} \\mid \\mathbf{z}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{T}\\right)}}_{\\mathcal{L}_{T}}+\\sum_{t=2}^{T} \\underbrace{\\log \\frac{q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{0}, \\mathbf{z}_{t}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)}}_{\\mathcal{L}_{t-1}} \\\\\n& +\\underbrace{\\log \\frac{q_{\\phi}\\left(\\mathbf{z}_{0} \\mid \\mathbf{w}^{x \\oplus y}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{0} \\mid \\mathbf{z}_{1}\\right)}}_{\\mathcal{L}_{0}}-\\underbrace{\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)}_{\\mathcal{L}_{\\text {round }}}]\n\\end{aligned}\n$$\n\nWe further simplify the training objective as follows (details in Appendix A):\n\n$$\n\\begin{aligned}\n\\min _{\\theta} \\mathcal{L}_{\\mathrm{VLB}} & =\\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)-f_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right] \\\\\n& \\rightarrow \\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{y}_{0}-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{y}\\right)-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}+\\mathcal{R}\\left(\\left\\|\\mathbf{z}_{0}\\right\\|^{2}\\right)\\right]\n\\end{aligned}\n$$\n\nhere we use $\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$ to denote the fractions of recovered $\\mathbf{z}_{0}$ corresponding to $\\mathbf{y}_{0}$. Note that although in the first term, we only compute the loss w.r.t $\\mathbf{y}_{0}$, due to the attention mechanism in the transformer, the reconstruction of $\\mathbf{y}_{0}$ also takes $\\mathbf{x}_{0}$ into account, thus the gradients from the first term\nwill also affect the learning of $\\mathbf{x}_{0}$. The mathematically equivalent regularization term $\\left.\\mathcal{R}\\left(\\left\\|\\mathbf{z}_{0}\\right\\|^{2}\\right)\\right)$ regularize the embedding learning. We further share the embedding function between source and target sequences, enabling the training of two different feature spaces jointly. This sets DIFFUSEQ away from existing solutions in vision such as GLIDE (Nichol et al., 2022). Training and Inference Methods. In our preliminary experiments, we find that the high diversity in NLP datasets and long diffusion steps often result in insufficient training. We hypothesize the reason is that sampling step $t$ uniformly causes unnecessary noise in the $\\mathcal{L}_{\\text {VLB }}$ objective. We hence employ importance sampling (Nichol \\& Dhariwal, 2021) to address this problem. $$\n\\mathcal{L}_{\\mathrm{VLB}}=\\mathbb{E}_{t \\sim p_{t}}\\left[\\frac{\\mathcal{L}_{t}}{p_{t}}\\right], p_{t} \\propto \\sqrt{\\mathbb{E}\\left[\\mathcal{L}_{t}^{2}\\right]}, \\sum_{t=0}^{T-1} p_{t}=1\n$$\n\nIntuitively, the importance-weighted sampling algorithm will spend more steps on diffusion steps with larger $\\mathcal{L}_{t}$, and vice versa. To conduct SEQ2SEQ generation given the condition $\\operatorname{EMB}\\left(\\mathbf{w}^{x}\\right)$, we randomly sample $\\mathbf{y}_{T} \\sim$ $\\mathcal{N}(0, I)$ and concatenate $\\mathbf{y}_{T}$ with $\\operatorname{EmB}\\left(\\mathbf{w}^{x}\\right)$ to obtain $\\mathbf{z}_{T}$. We can now repeat the reverse process until we arrive at $\\mathbf{z}_{0}$. At each sampling step, an anchoring function is executed towards reparameterized $\\mathbf{z}_{t}$. Specifically, the anchoring function: (a) operates rounding on $\\mathbf{z}_{t}$ to map it back to word embedding space following Li et al. (2022); (b) replaces the part of recovered $\\mathbf{z}_{t-1}$ that belongs to $\\mathbf{w}^{x}$ with the original $\\mathbf{x}_{0}$, considering that this part is recovered from corrupted $\\mathbf{z}_{t}$ via $f_{\\theta}$ and not strictly equals to $\\mathbf{x}_{0}$. Note that (b) is designed for DIFFUSEQ. To improve the quality of generation, we apply the widely used Minimum Bayes Risk (MBR) decoding strategy (Koehn, 2004). We first generate a set of candidate samples $\\mathcal{S}$ from different random seeds of DIFFUSEQ and select the best output sequence that achieves the minimum expected risk under a meaningful loss function (e.g.",
    "diffuseq-5": "BLEU or other cheaper metrics like precision). In practice, we use the negative BLEU score in our implementation. Connections to AR, Iter-NAR, and Fully-NAR Models. To better understand the behavior of DIFFUSEQ, we give the theoretical connection to autoregressive (AR), iterative non-autoregressive (iter-NAR), and fully non-autoregressive (fully-NAR) models. We argue that DIFFUSEQ can be seen as an extension of iter-NAR model. Detailed graphical learning discrepancies of these four cases are discussed in Appendix B for reference. AR models learn $p\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)$ by autoregressive decomposition based on left-context:\n\n$$\np_{\\mathrm{AR}}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\underbrace{p\\left(w_{1}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{i=1, \\ldots, n-1} p\\left(w_{i+1}^{y} \\mid \\mathbf{w}_{1: i}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive left-context prediction }},\n$$\n\nwhile fully-NAR models (Gu et al., 2018; Qian et al., 2021) learn the conditional probability given independent assumption for fast inference:\n\n$$\np_{\\text {fully-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\prod_{i=1, \\ldots, n} p\\left(w_{i}^{y} \\mid \\mathbf{w}^{x}\\right)\n$$\n\nTo make a better analogy to AR and NAR models, we use a lossless way to formulate iterative NAR models (Gu et al., 2019; Ghazvininejad et al., 2019) by introducing a series of intermediate sequences $\\mathbf{w}_{1: K-1}^{y}, \\mathbf{w}_{K}^{y}=\\mathbf{w}^{y}$ with $K$ editable iterations:\n\n$$\np_{\\text {iter-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}}^{\\prod_{i=1 \\ldots n} p\\left(w_{1, i}^{y} \\mid \\mathbf{w}^{x}\\right)} \\prod_{\\text {initial prediction }}^{\\prod_{k=1 \\ldots K-1} \\prod_{i=1 \\ldots n} p\\left(w_{k+1, i}^{y} \\mid \\mathbf{w}_{k, 1: n}^{y}, \\mathbf{w}^{x}\\right)}\n$$\n\nPrevious study (Huang et al., 2022) shows that there is a gap called conditional total correlation between AR Eq. (4) and fully-NAR Eq. (5) learning paradigms, because of lossy decomposition of NAR models. However, when comparing iter-NAR Eq. (6) with AR Eq. (4) models, they both can be factorized into an initial prediction term and a progressive prediction process based on different context (i.e. left-context in AR and full-context in iter-NAR), and the discrepancy pointed out by\n\nHuang et al. (2022) is therefore closed in iter-NAR assuming sufficient steps. By showing DIFFUSEQ is an extension of the iter-NAR model, we offer a justification that it will not suffer from the conditional total correlation for the same reason. A straight-forward way to formulate pure continuous diffusion models is to introduce a series of Gaussian noise-corrupted features along with diffusion steps: $\\mathbf{y}_{1: T-1}, \\mathbf{y}_{0}=\\mathbf{y}, \\mathbf{y}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$. $$\np_{\\text {diffusion }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right)=\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} \\underbrace{p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right)}_{\\text {final prediction }} \\underbrace{\\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)}_{\\text {progressive full-context diffusion }},\n$$\n\nwhere $p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)$ describes the diffusion step on continuous representations $\\mathbf{y}$. The rounding operation in DIFFUSEQ maps the continuous vectors $\\mathbf{y}$ to discrete $\\mathbf{w}^{y}$ for each time step $t$, we in addition introduce this into Eq. (7):\n\n$$\n\\begin{aligned}\np_{\\text {DIFFUSEQ }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nBy rearranging Eq. (8) into Eq. (9), we can see DIFFUSEQ can be seen as a more generalized form of iter-NAR Eq. (6) before marginalizing out $\\left\\{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}\\right\\}$, despite the different initialization of $\\mathbf{y}_{T}{ }^{2}$. A more detailed derivation is shown in Appendix C. ## 4 EXPERIMENTS\n\nWe conduct experiments to validate the effectiveness of DIFFUSEQ on four different tasks, against six strong AR/NAR baselines. ### 4.1 EXPERIMENTAL SETUP\n\nTasks and Datasets. SEQ2SEQ generation covers a wide range of tasks, among which we choose four typical and popular tasks. Open domain dialogue requires models to generate informative responses given a dialogue context. We use Commonsense Conversation Dataset (Zhou et al., 2018), which is extracted from Reddit single-round dialogs, with over 3 million conversational pairs. Question generation $(\\mathrm{QG})$ aims to generate questions given a context as input. To obtain sufficient training samples, we use the dataset Quasar-T (Dhingra et al., 2017) preprocessed by Lin et al. (2018), and then generate document-question pairs to obtain 119K training samples (details in Appendix D.1). Text simplification aims to revise the complex text into sequences with simplified grammar and word choice. Jiang et al. (2020) constructs a corpus consisting of 677 K complexsimple sentences with revision alignment. Paraphrase task generates an alternative surface form in the same language expressing the same semantic content. We adopt widely used $\\mathrm{QQP}^{3}$ sourced from the community question answering forum Quora, with 147 K positive pairs. Baselines. We consider three groups of models as baselines, covering both AR and NAR architectures. The first group of methods adopts encoder-decoder architecture (Cho et al., 2014) which is well-studied for SEQ2SEQ tasks, and we conduct experiments on two popular models: GRU with attention and Transformer (Vaswani et al., 2017). The second group is the finetuned large pre-trained language model (PLM), among which GPT2 (Radford et al., 2019) has demonstrated great success in almost all SEQ2SEQ tasks. We further compare to GPVAE (Du et al., 2022), which augments a pre-trained T5 (Raffel et al., 2020) with VAE to improve the generation diversity. For the last group of baselines, we consider LevT (Gu et al., 2019), a widely used, strong iterative NAR model. All baselines are trained following instructions in their papers, and details can be found in Appendix D.2. [^1]Table 1: The overall results of different methods on different SEQ2SEQ tasks. The first group $\\diamond$ of methods adopt autoregressive encoder-decoder architecture and the second group $\\bullet$ is the finetuned large pre-trained language model (also in autoregressive manner) while the last group $\\ddagger$ is non-autoregressive. The best results are bold, and the best results without PLMs are underlined. | Tasks | Methods | BLEU $\\uparrow$ | R-L $\\uparrow$ | Score $\\uparrow$ | dist- $1 \\uparrow$ | selfB $\\downarrow / \\operatorname{div}-4 \\uparrow$ | Len |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Open <br> Domain <br> Dialogue | GRU-attention ${ }^{\\diamond}$ | 0.0068 | 0.1054 | 0.4128 | 0.8998 | 0.8008/0.1824 | 4.46 |\n|  | Transformer-base ${ }^{\\diamond}$ | 0.0189 | 0.1039 | 0.4781 | 0.7493 | 0.3698/0.6472 | 19.5 |\n|  | GPT2-base FT* | 0.0108 | 0.1508 | 0.5279 | 0.9194 | 0.0182/0.9919 | 16.8 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.0125 | 0.1002 | 0.5293 | 0.9244 | 0.0213/0.9938 | 16.8 |\n|  | GPVAE-T5 ${ }^{\\bullet}$ | 0.0110 | 0.1009 | 0.4317 | 0.5625 | $0.3560 / 0.5551$ | 20.1 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.0158 | 0.0550 | 0.4760 | 0.9726 | 0.7103/0.1416 | 4.11 |\n|  | DiffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.0139 | $\\underline{0.1056}$ | 0.5131 | 0.9467 | 0.0144/0. ${ }^{9971}$ | 13.6 |\n| Question <br> Generation | GRU-attention ${ }^{\\diamond}$ | 0.0651 | 0.2617 | 0.5222 | 0.7930 | 0.9999/0.3178 | 10.1 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.1663 | 0.3441 | 0.6307 | 0.9309 | 0.3265/0.7720 | 10.3 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.0741 | 0.2714 | 0.6052 | 0.9602 | 0.1403/0.9216 | 10.0 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.1110 | 0.3215 | 0.6346 | 0.9670 | 0.2910/0.8062 | 9.96 |\n|  | GPVAE-T5 ${ }^{\\circ}$ | 0.1251 | 0.3390 | 0.6308 | 0.9381 | $0.3567 / 0.7282$ | 11.4 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.0930 | 0.2893 | 0.5491 | 0.8914 | 0.9830/0.4776 | 6.93 |\n|  | DIFfuSEQ (Ours) ${ }^{\\ddagger}$ | 0.1731 | 0.3665 | 0.6123 | 0.9056 | $\\underline{0.2789} / \\underline{0.8103}$ | 11.5 |\n| Text <br> Simpli- <br> fication | GRU-attention ${ }^{\\diamond}$ | 0.3256 | 0.5602 | 0.7871 | 0.8883 | 0.9998/0.3313 | 18.9 |\n|  | Transformer-base ${ }^{\\circ}$ | 0.2693 | 0.4907 | 0.7381 | 0.8886 | $0.6924 / 0.5095$ | 18.5 |\n|  | GPT2-base FT ${ }^{\\bullet}$ | 0.3083 | 0.5461 | 0.8021 | 0.9439 | 0.5444/0.6047 | 16.1 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.2693 | 0.5111 | 0.7882 | 0.9464 | 0.6042/0.5876 | 15.4 |\n|  | GPVAE-T5 ${ }^{\\circ}$ | 0.3392 | 0.5828 | 0.8166 | 0.9308 | 0.8147/0.4355 | 18.5 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.2052 | 0.4402 | 0.7254 | 0.9715 | 0.9907/0.3271 | 8.31 |\n|  | DIffuSEQ (Ours) ${ }^{\\ddagger}$ | 0.3622 | 0.5849 | $\\underline{0.8126}$ | 0.9264 | $\\underline{0.4642} / \\underline{\\mathbf{0} 6604}$ | 17.7 |\n| Paraphrase | GRU-attention ${ }^{\\diamond}$ | 0.1894 | 0.5129 | 0.7763 | 0.9423 | 0.9958/0.3287 | 8.30 |\n|  | Transformer-base ${ }^{\\nabla}$ | $\\underline{0.2722}$ | 0.5748 | $\\underline{0.8381}$ | 0.9748 | $0.4483 / 0.7345$ | 11.2 |\n|  | GPT2-base FT* | 0.1980 | 0.5212 | 0.8246 | 0.9798 | 0.5480/0.6245 | 9.67 |\n|  | GPT2-large FT ${ }^{\\bullet}$ | 0.2059 | 0.5415 | 0.8363 | 0.9819 | 0.7325/0.5020 | 9.53 |\n|  | GPVAE-T5 ${ }^{\\circ}$ | 0.2409 | 0.5886 | 0.8466 | 0.9688 | $0.5604 / 0.6169$ | 9.60 |\n|  | NAR-LevT ${ }^{\\ddagger}$ | 0.2268 | 0.5795 | 0.8344 | 0.9790 | 0.9995/0.3329 | 8.85 |\n|  | DiffuSeQ (Ours) ${ }^{\\ddagger}$ | 0.2413 | $\\underline{0.5880}$ | 0.8365 | $\\underline{0.9807}$ | $\\underline{0.2732} / \\underline{0.8641}$ | 11.2 |\n\nEvaluation. We evaluate the generated sequences from two aspects: quality and diversity. To evaluate the quality, we use the standard metric BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) score. Since string-similarity-based metrics can be unsatisfactory for open-ended generation, we also report BERTScore (Zhang et al., 2019) that assesses the semantic similarity between generated sentences and references. Details are in Appendix D.4. Higher scores of BLEU, ROUGE and BERTScore reflect better performance. As for diversity, we use distinct unigram (dist-1) to measure intra-diversity within each generated sentence, where the lower dist- 1 indicates that the generated sentence contains more repeated words. For sentence-level diversity evaluation, we consider sentence-level self-BLEU (Zhu et al., 2018) to measure the n-gram overlap between the set of outputs w.r.t one source sentence, and we additionally use diverse 4 -gram (div-4) (Deshpande et al., 2019) to measure the ratio of distinct 4 -grams in the set of outputs per source sentence. The lower self-BLEU and higher div-4 suggest higher diversity of generation. For each method including DIFFUSEQ, we generate 3 samples for each source sentence to compute the diversity metrics. Implementation Details. Our DIffuSEQ is based on the 12 layers of Transformer with 12 attention heads, where the time step embedding is plugged akin to the position embedding. The maximum sequence length is 128 , with embedding dimension $d=128$, diffusion steps $T=2,000$ and a square-root noise schedule.",
    "diffuseq-6": "To reduce the out-of-vocabulary generation, we apply Byte Pair Encoding (Sennrich et al., 2016) to construct the vocabulary. After conducting the diversity beam\n\nTable 2: Sample outputs in QQP test set, conditioned on the same $\\mathbf{x}$. | Original sentence: How do I | make friends. $\\quad$ Paraphrase reference: How to make friends ? |  |\n| :--- | :--- | :--- |\n| GPT2-large finetune | GPVAE-T5 | DIFFUSEQ |\n| How can I make friends? | How can I make friends? | How can I make friends better? |\n| How can I make friends? | How do I make friends? | How can I make friends? |\n| How can I make friends? | How can I make friends? | How do you make friends? |\n| How can I make friends? | How can I make friends? | What is the best way to make friends? |\n| How do I make friends and | What's the best way to make | How can I make friends and more |\n| keep them?",
    "diffuseq-7": "| friends and make make friends? | something? |\n\nsearch (DBS) (Vijayakumar et al., 2016) for the Transformer-base model and GPT model, we find that DBS does not always promote diversity over temperature sampling and therefore we list the best diversity results. We compute the accuracy metrics of DIFFUSEQ using MBR with the size of candidate samples $|\\mathcal{S}|=10$. The experiment is deployed on NVIDIA A100 Tensor Core GPUs, and we use 4 GPUs on training and single GPU on sampling. ### 4.2 MAIN RESULTS\n\nAs shown in Table 1, we conclude that DIFFUSEQ achieves comparable or even higher generation quality compared with strong baselines. At the same time, DIFFUSEQ consistently demonstrates its superiority in generating diverse outputs given the same input sequence. As we can see from Table 1, DIFFUSEQ wins competitions over at least one quality metric against 6 baselines $\\times 4$ tasks. Although NAR models such as LevT can also outperform AR baselines sometimes, they still lag well behind DIFFUSEQ by large margins (i.e., relative improvements over $50 \\%$ for BLEU in QG task and R-L in Dialogue task). Even compared with pre-trained then finetuned GPT2 models, DIFFUSEQ still delivers superior performance than the base variant, and is comparable with the large variant, which has 8.2 times more parameters than DIFFUSEQ. These empirical results amply support our findings in $\\S 3$, where we theoretically analyze the potential of diffusion models in modeling text sequences compared with AR models given sufficient diffusion steps. DIFFUSEQ, as a member of the deep generative model family, also exhibit the capacity to generate highly diverse sequences. As suggested by self-BLEU (lower is better) and div-4 (higher is better), in almost all cases, DIFFUSEQ significantly outperforms 4 AR baselines in terms of sentence-level diversity (i.e., producing diverse outputs given the same input). For diversity in word choice within one sentence, we consider dist-1: a higher dist-1 indicates less repetition within a sentence. As we can see from Table 1, DIFFUSEQ has less repetition compared with encoder-decoder methods, but still fall behind the pre-trained GPT2 models (the same situation with BERTScore). These results suggest there is still room for improvement (e.g., use pre-training techniques) in diffusion models' token-level choice. Different from NAR-LevT, DIFFUSEQ does not rely on an extra length prediction module but automatically decides by the padding token instead and is able to generate longer output sentences, indicated by the last column for average generation length. In Table 2, we provide examples to showcase DIFFUSEQ's ability to generate diverse samples. More examples can be found in Appendix D.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_e929bd7e46305c313f33g-07.jpg?height=307&width=661&top_left_y=2120&top_left_x=387)\n\nFigure 3: The increase of BLEU score with different candidate sizes $|\\mathcal{S}|$. ![](https://cdn.mathpix.com/cropped/2024_09_12_e929bd7e46305c313f33g-07.jpg?height=302&width=662&top_left_y=2120&top_left_x=1057)\n\nFigure 4: Trade-off between quality and diversity (details in Appendix D.3). ![](https://cdn.mathpix.com/cropped/2024_09_12_e929bd7e46305c313f33g-08.jpg?height=318&width=725&top_left_y=280&top_left_x=407)\n\nFigure 5: The curve of BLEU/div-4 score along with generation process (percentage of steps). ![](https://cdn.mathpix.com/cropped/2024_09_12_e929bd7e46305c313f33g-08.jpg?height=307&width=484&top_left_y=280&top_left_x=1211)\n\nFigure 6: The BLEU and inference speed of DIFFUSEQ and GPT2-large. ### 4.3 ANALYSIS\n\nWe conduct a series of analysis to investigate the effectiveness of different aspects in DIFFUSEQ. Diversity Ensures Quality. Generating high-quality texts with high diversity is an important requirement for many text generation applications and the trade-off between quality and diversity is always a critical concern in open-ended NLG tasks (Zhang et al., 2021). Different from AR models relying on the decoding strategy like temperature and nucleus sampling (Holtzman et al., 2019) and VAE models sampling latent variable from Gaussian Prior, the natural advantage of DIFFUSEQ is to generate different sentences along with a series of random Gaussian noise. In Figure 4, we elucidate that DIFFUSEQ have better trade-off between generation quality (BLEU) and sentence-level diversity (div-4). Here we further demonstrate that the high diversity provided by DIFFUSEQ can be turned into better quality. MBR is a common strategy to improve generation quality by aggregating and ranking candidate sequences, and we find that the upper bound of MBR is decided by a diversified candidate set. To valid this, we simultaneously apply MBR on both DIFFUSEQ and GPT2 with various candidate sizes $|\\mathcal{S}|$. The results are shown in Figure 3. As we can see, DIFFUSEQ lags behind GPT2 without using $\\operatorname{MBR}(|\\mathcal{S}|=1)$ or with a small candidate set $(|\\mathcal{S}|=3)$. However, as $|\\mathcal{S}|$ increases, DiffuSEQ starts to outperform GPT2 by an increasing margin. The reason is that autoregressive models like GPT2 tend to generate highly similar candidates (as discussed in $\\S 4.2$ ), which impedes the effectiveness of MBR. As $|\\mathcal{S}|$ increases to 20, DifFUSEQ still shows better rising trends than GPT2. Our findings also stress the importance of better ranking methods in diffusion research. Step-wise Analysis against Iterative NAR. Given the underlying theoretical connection between iterative NAR and DIfFUSEQ discussed in $\\S 3$, we empirically investigate the behavior of LevT and DIFFUSEQ by analyzing their step-wise quality (i.e. BLEU) and diversity (i.e. div-4) curves. As is suggested in Figure 5, LevT grows fiercely in quality at the very beginning of generation, and quickly slows down in the successive refinement process. But DIFFUSEQ behaves differently, with BLEU score growing slowly at first, increasing rapidly as the diffusion process progresses and finally surpassing LevT. It is also observed that the diversity of both LevT and DIFFUSEQ is determined at the very early stage regardless of future refinement or diffusion, where DIFFUSEQ consistently outperforms LevT on diversity at any stage of generation. We conjecture that DIFFUSEQ explores more possible results at the first half of generation process, and soon converges to several potential candidates when it is closed to the end of steps. In this case, DIFFUSEQ shows its capacity to take both generation quality and diversity into consideration, and this is the capacity that iterative-NAR and even AR models can not obtain, due to the different learning paradigms. Inference Speed. The slow sampling speed is one of the major concerns about diffusion models. Here we fix the number of diffusion steps during training for DIFFUSEQ while shrinking the inference steps following DDIM (Song et al., 2020). As we can see from Figure 6, when reducing the inference to 1,000 diffusion steps on single GPU, DIFFUSEQ achieves a higher BLEU score than GPT2-large yet registers a closer inference speed to GPT2-large. Effectiveness of Joint Training. In DiffuSeQ, the representations of $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$ are jointly trained using the same embedding function $\\operatorname{Emb}(\\cdot)$ (stated in $\\S 3$ ). To validate the effectiveness\n\nTable 3: Results with or without joint training for Question Generation task. | Setting | BLEU $\\uparrow$ | R-L $\\uparrow$ | Score $\\uparrow$ | selfB $\\downarrow /$ div-4 $\\uparrow$ |\n| :--- | :--- | :--- | :--- | :--- |\n| DIFFUSEQ (w/o reranking) | 0.1567 | 0.3484 | 0.5947 | $0.2789 / 0.8103$ |\n| Fix EMB $\\left(\\mathbf{w}^{x}\\right)$ as pre-trained | 0.0110 | 0.0687 | 0.3769 | $0.0174 / 0.9376$ |\n\nof this joint training strategy, we compared it with the training strategy commonly used in text-toimage diffusion models (Nichol et al., 2022; Ramesh et al., 2022). In particular, we decouple the training of $\\operatorname{Emb}\\left(\\mathbf{w}^{x}\\right)$ and $\\operatorname{Emb}\\left(\\mathbf{w}^{y}\\right)$ by replacing $\\operatorname{Emb}\\left(\\mathbf{w}^{x}\\right)$ with representations extracted from a pre-trained BERT-tiny model (Turc et al., 2019). From Table 3, we find that the decoupled training strategy results in poor performance. ## 5 RELATED WORK\n\nDiffusion Models for Text Modeling. Text-to-Image generation using diffusion models has developed many potential applications. Models such as Imagen (Saharia et al., 2022b) and DALLE (Ramesh et al., 2022) are usually two-staged relying on the pre-trained models, requiring the alignment between the embedding vectors from two sources. GLIDE (Nichol et al., 2022) explores diffusion model with classifier-free (Ho \\& Salimans, 2022) guidance by setting guidance scale during training. The target space of these models is not discrete text space but stable vectors of pixel values. There are other works of diffusion on text generation, but they stick to the original encoderdecoder architecture and the diffusion process is interspersed on the decoder (Savinov et al., 2021), or the latent space (Yu et al., 2022). For text generation using the diffusion models, Hoogeboom et al. (2021) introduce the multinomial diffusion for character-level text generation, the forward categorical noise is applied through the Markov transition matrix. Austin et al. (2021) generalize discrete text diffusion models by introducing the absorbing state ([MASK]). However, discrete diffusion models may suffer from the scaling of the one-hot row vectors, and they only generate text samples unconditionally in discrete space. Diffusion-LM (Li et al., 2022) and Analog Bits (Chen et al., 2022) propose a new language model diffused on the continuous latent representations, with different mapping functions that connect the discrete and continuous space of texts. Compared with our work, we focus on the SEQ2SEQ diffusion models for text generation in the continuous space and our work is the first to explore this setting to the best of our knowledge. Diffusion Models for Conditional Generation. Related to conditional-VAE (Zhao et al., 2017), we can consider the latent encoded input $\\mathbf{x}$ as a condition. Diffusion-LM (Li et al., 2022) adopts the plug-and-play approaches (Dathathri et al., 2020) to compose fine-grained constraints on the generated sentences, but it fails to condition on the whole source sentence in SEQ2SEQ tasks. Noted that this controllable generation method is orthogonal to our DIFFSEQ, in other words, we can further add classifier-guided constraints on the SEQ2SEQ output to further control the text generation. There are other conditional diffusion models on the time series prediction like CSDI (Tashiro et al., 2021) or audio generation like WaveGrad (Chen et al., 2021), but their class conditions are usually attributes that are easy to model, while the contextual texts as conditions are much more complex. ## 6 CONCLUSIONS\n\nWe propose DIFFUSEQ to tackle SEQ2SEQ tasks in a diffusion way, which contains the strong potential to achieve better generation quality and diversity trade-off. The capability enables favorable characteristics of DIFFUSEQ to further enhance the quality of final results, by leveraging a minimum Bayes risk decoding algorithm. Besides, we theoretically connect the AR and NAR models to DIFFUSEQ, and show that DIFFUSEQ is a powerful extension of iterative-NAR model. The empirical results demonstrate that DIFFUSEQ is also a powerful model for text generation, matching or even surpassing competitive AR, iterative NAR, and large-scale pre-trained models on quality and diversity. Given the limited progress of current diffusion models on text generation, our study addresses promising achievements by such a new sequence-to-sequence learning paradigm. ## ACKNOWLEDGMENTS\n\nWe would like to thank the anonymous reviewers and other peers for their valuable advice, and we also acknowledge Chenxin An's efforts to update the generation results for the Transformer-base model on QG and Paraphrasing tasks. This work is partially supported by the Shanghai Committee of Science and Technology (Grant No. 21DZ1100100) and the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N_HKU714/21. ## REFERENCES\n\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 2021. Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation.",
    "diffuseq-8": "In International Conference on Learning Representations, ICLR, 2021. Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning.",
    "diffuseq-9": "arXiv preprint arXiv:2208.04202, 2022. Kyunghyun Cho, Bart van Merrienboer, \u00c7aglar G\u00fcl\u00e7ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing, EMNLP, 2014. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. In 8th International Conference on Learning Representations, ICLR, 2020. Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander G Schwing, and David Forsyth. Fast, diverse and accurate image captioning guided by part-of-speech. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10695-10704, 2019. Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. Quasar: Datasets for question answering by search and reading.",
    "diffuseq-10": "arXiv preprint arXiv:1707.03904, 2017. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp.",
    "diffuseq-11": "In International Conference on Learning Representations, ICLR, 2017. Wanyu Du, Jianqiao Zhao, Liwei Wang, and Yangfeng Ji. Diverse text generation via variational encoder-decoder models with gaussian process priors. In Proceedings of 6th Workshop on Structured Prediction for NLP of the Association for Computational Linguistics, ACL, 2022. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 6112-6121. Association for Computational Linguistics, 2019. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014. Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. In International Conference on Learning Representations, ICLR, 2018. Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer.",
    "diffuseq-12": "Advances in Neural Information Processing Systems, 2019. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 2020. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, ICLR, 2019. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 2021. Fei Huang, Tianhua Tao, Hao Zhou, Lei Li, and Minlie Huang. On the learning of non-autoregressive transformers. In International Conference on Machine Learning, ICML, 2022. Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. Neural crf model for sentence alignment in text simplification.",
    "diffuseq-13": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL, 2020. Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on Learning Representations, ICLR, 2014. Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 conference on empirical methods in natural language processing, pp. 388-395, 2004. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations, $I C L R, 2020$. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation.",
    "diffuseq-14": "arXiv preprint arXiv:2205.14217, 2022. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, 2004. Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. Denoising distantly supervised opendomain question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL, 2018. Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. In International Conference on Learning Representations, ICLR, 2017. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, ICML, 2022. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, ICML, 2021. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL, 2002. Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, and Lei Li. Glancing transformer for non-autoregressive neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, 2022a. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, 2016. Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. Stepunrolled denoising autoencoders for text generation. In International Conference on Learning Representations, ICLR, 2021. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715-1725. Association for Computational Linguistics, 2016. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, ICML, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, ICLR, 2020. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diffusion models for probabilistic time series imputation.",
    "diffuseq-15": "Advances in Neural Information Processing Systems, 2021. Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: The impact of student initialization on knowledge distillation.",
    "diffuseq-16": "CoRR, abs/1908.08962, 2019. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017. Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models.",
    "diffuseq-17": "arXiv preprint arXiv:1610.02424, 2016. P Yu, S Xie, X Ma, B Jia, B Pang, R Gao, Y Zhu, S-C Zhu, and YN Wu. Latent diffusion energybased model for interpretable text modeling. In International Conference on Machine Learning, ICML, 2022. Hugh Zhang, Daniel Duckworth, Daphne Ippolito, and Arvind Neelakantan. Trading off diversity and quality in natural language generation. In Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pp. 25-33. Association for Computational Linguistics, 2021. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert.",
    "diffuseq-18": "In International Conference on Learning Representations, ICLR, 2019. Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 2017. Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. Commonsense knowledge aware conversation generation with graph attention. In International Joint Conference on Artificial Intelligence, IJCAI, 2018. Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: A benchmarking platform for text generation models.",
    "diffuseq-19": "In The 41st International ACM SIGIR Conference on Research \\& Development in Information Retrieval, pp. 1097-1100, 2018. ## A Objective Derivations Of DiffuSeQ\n\nThe diffusion model is well-known as its ability to achieve the trade-off between flexibility and tractability of the models' probability distributions, compared with GAN, VAE and Flow-based models.",
    "diffuseq-20": "Following Ho et al. (2020); Nichol \\& Dhariwal (2021); Song et al. (2020), we systematically define the forward noising process and reverse denoising process on latent continuous space $\\mathbf{z}$. The forward noising is to perturb the structure of data $\\mathbf{z}_{0} \\cdot \\mathbf{z}_{0}$ is finally changed into the partial Gaussian noise with $\\mathbf{y}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$ through $T$-step forward random disturbance\n\n$$\nq\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\sqrt{1-\\beta_{t}} \\mathbf{z}_{t-1}, \\beta_{t} \\mathbf{I}\\right)\n$$\n\nwith $t=1,2, \\ldots, T$ and $\\left\\{\\beta_{t} \\in(0,1)\\right\\}_{t=1}^{T}$ are the variance schedule. Let $\\alpha_{t}=1-\\beta_{t}$ and $\\bar{\\alpha}_{t}=$ $\\prod_{i=1}^{t} \\alpha_{i}$, we have:\n\n$$\n\\begin{aligned}\n\\mathbf{z}_{t} & =\\sqrt{\\alpha_{t}} \\mathbf{z}_{t-1}+\\sqrt{1-\\alpha_{t}} \\epsilon_{t-1}=\\sqrt{\\alpha_{t} \\alpha_{t-1}} \\mathbf{z}_{t-2}+\\sqrt{1-\\alpha_{t} \\alpha_{t-1}} \\bar{\\epsilon}_{t-2} \\\\\n& =\\ldots=\\sqrt{\\bar{\\alpha}_{t}} \\mathbf{z}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}} \\epsilon\n\\end{aligned}\n$$\n\nwhere $\\epsilon$ stands for Gaussian noises. In the end, $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\sqrt{\\bar{\\alpha}_{t}} \\mathbf{z}_{0},\\left(1-\\bar{\\alpha}_{t}\\right) \\mathbf{I}\\right)$. We use a sqrt noise schedule in Diffusion-LM (Li et al., 2022), that is, $\\bar{\\alpha}_{t}=1-\\sqrt{t / T+s}$ with $s$ as a small constant at the start of noise level. The reverse process then denoises $\\mathbf{z}_{t}$, aiming to recover original $\\mathbf{z}_{0}$, and is defined as:\n\n$$\np_{\\theta}\\left(\\mathbf{z}_{0: T}\\right):=p\\left(\\mathbf{z}_{T}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right), \\quad p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t-1} ; \\mu_{\\theta}\\left(\\mathbf{z}_{t}, t\\right), \\sigma_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right)\n$$\n\nThe learning of $p_{\\theta}$ is based on our diffusion model DIFFUSEQ: $f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$, where the $\\mu_{\\theta}(\\cdot)$ and $\\sigma_{\\theta}(\\cdot)$ is the predicted parameterization of the mean and standard variation of $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}\\right)$ in forward process. Using Bayes' rule:\n\n$$\nq\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)=q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t-1}, \\mathbf{z}_{0}\\right) \\frac{q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{0}\\right)}{q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}\\right)}\n$$\n\nSubstitute Eq. (11) to it and we can get the parameterized mean of $q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)$ :\n\n$$\n\\mu_{t}\\left(\\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)=\\frac{\\sqrt{\\alpha_{t}}\\left(1-\\bar{\\alpha}_{t-1}\\right)}{1-\\bar{\\alpha}_{t}} \\mathbf{z}_{t}+\\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_{t}}{1-\\bar{\\alpha}_{t}} \\mathbf{z}_{0}\n$$\n\nand for brevity, we short the coefficient of $\\mathbf{z}_{t}$ and $\\mathbf{z}_{0}$ as $\\mathcal{U}$ and $\\mathcal{E}$ respectively. We can use the variational lower bound to optimize the negative log-likelihood $\\mathbb{E}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{0}\\right)\\right] \\leq$ $\\mathcal{L}_{\\text {VLB }}$. The objective can be further rewritten to be a combination of several KL-divergence and entropy terms following Sohl-Dickstein et al. (2015). $$\n\\begin{aligned}\n\\mathcal{L}_{\\mathrm{VLB}}=\\mathcal{L}_{T}+\\mathcal{L}_{T-1}+\\cdots+\\mathcal{L}_{0} & =\\mathbb{E}_{q\\left(\\mathbf{z}_{1: T} \\mid \\mathbf{z}_{0}\\right)}\\left[\\log \\frac{q\\left(\\mathbf{z}_{T} \\mid \\mathbf{z}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{T}\\right)}+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{0}, \\mathbf{z}_{t}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t-1} \\mid \\mathbf{z}_{t}\\right)}\\right. \\\\\n& \\left.+\\log \\frac{q_{\\phi}\\left(\\mathbf{z}_{0} \\mid \\mathbf{w}^{x \\oplus y}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{0} \\mid \\mathbf{z}_{1}\\right)}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right]\n\\end{aligned}\n$$\n\nFor $1 \\leq t \\leq T-1$, we compute the parameterization of $\\mathcal{L}_{t}$ by substituting Eq. (14) to minimize the difference from $\\mu_{t}$ and $\\mu_{\\theta}$ following Ho et al. (2020):\n\n$$\n\\begin{aligned}\n\\mathcal{L}_{t} & =\\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\log \\frac{q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}, \\mathbf{z}_{t+1}\\right)}{p_{\\theta}\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{t+1}\\right)}\\right]=\\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\frac{1}{\\mathcal{C}}\\left\\|\\mu_{t}\\left(\\mathbf{z}_{t}, \\mathbf{z}_{0}\\right)-\\mu_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}\\right] \\\\\n& =\\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\frac{1}{\\mathcal{C}}\\left\\|\\mathcal{U} \\mathbf{z}_{t}+\\mathcal{E} \\mathbf{z}_{0}-\\left(\\mathcal{U} \\mathbf{z}_{t}+\\mathcal{E} f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right)\\right\\|^{2}\\right]=\\frac{\\mathcal{E}}{\\mathcal{C}} \\mathbb{E}_{\\mathbf{z}_{0}}\\left[\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}\\right]\n\\end{aligned}\n$$\n\nwhere $\\mathcal{C}=2\\left\\|\\sigma_{\\theta}\\right\\|^{2}$ is a loss independent constant. Then the optimization of training loss $\\min _{\\theta} \\mathcal{L}_{\\mathrm{VLB}}$ can be further simplified as:\n\n$$\n\\begin{aligned}\n& \\min _{\\theta}\\left[\\left\\|\\mu\\left(\\mathbf{z}_{T}\\right)\\right\\|^{2}+\\sum_{t=2}^{T}\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)-f_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right] \\\\\n\\rightarrow & \\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{z}_{0}-f_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{x \\oplus y}\\right)-f_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}-\\log p_{\\theta}\\left(\\mathbf{w}^{x \\oplus y} \\mid \\mathbf{z}_{0}\\right)\\right] \\\\\n\\rightarrow & \\min _{\\theta}\\left[\\sum_{t=2}^{T}\\left\\|\\mathbf{y}_{0}-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)\\right\\|^{2}+\\left\\|\\operatorname{EMB}\\left(\\mathbf{w}^{y}\\right)-\\tilde{f}_{\\theta}\\left(\\mathbf{z}_{1}, 1\\right)\\right\\|^{2}+\\mathcal{R}\\left(\\left\\|\\mathbf{z}_{0}\\right\\|^{2}\\right)\\right]\n\\end{aligned}\n$$\n\n## B Graphical Models of AR, Fully NAR, ITERATIVE NAR ANd DIFFUSEQ MODELS\n\nWe start from the conditional sequence generation problem, which aims to learn a conditional probability $p\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)$ with $\\mathbf{w}^{x}$ and $\\mathbf{w}^{y}$. AR models learn $p\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)$ by autoregressive decomposition based on left-context:\n\n$$\np_{\\mathrm{AR}}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\underbrace{p\\left(w_{1}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{i=1, \\ldots, n-1} p\\left(w_{i+1}^{y} \\mid \\mathbf{w}_{1: i}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive left-context prediction }}\n$$\n\nconsisting of an initial prediction and an autoregressive left-context prediction process, while fullyNAR models (Gu et al., 2018; Qian et al., 2021) learn the conditional probability given independent assumption for fast inference:\n\n$$\np_{\\text {fully-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right)=\\prod_{i=1, \\ldots, n} p\\left(w_{i}^{y} \\mid \\mathbf{w}^{x}\\right)\n$$\n\nTo make a better analogy to AR and NAR models, we use a lossless way to formulate iterative NAR models (Gu et al., 2019; Ghazvininejad et al., 2019) by introducing a series of intermediate sequences $\\mathbf{w}_{1: K-1}^{y}, \\mathbf{w}_{K}^{y}=\\mathbf{w}^{y}$ as:\n\n$$\n\\begin{aligned}\np_{\\text {iter-NAR }}\\left(\\mathbf{w}_{1: n}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} p\\left(\\mathbf{w}_{1}^{y} \\mid \\mathbf{w}^{x}\\right) \\prod_{k=1 \\ldots K-1} p\\left(\\mathbf{w}_{k+1}^{y} \\mid \\mathbf{w}_{k}^{y}, \\mathbf{w}^{x}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} p\\left(\\mathbf{w}_{1}^{y} \\mid \\mathbf{w}^{x}\\right) \\prod_{k=1 \\ldots K-1} p\\left(\\mathbf{w}_{k+1}^{y} \\mid \\mathbf{w}_{k}^{y}, \\mathbf{w}^{x}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} \\underbrace{\\prod_{i=1 \\ldots n} p\\left(w_{1, i}^{y} \\mid \\mathbf{w}^{x}\\right)}_{\\text {initial prediction }} \\underbrace{\\prod_{k=1 . . K-1} \\prod_{i=1 \\ldots n} p\\left(w_{k+1, i}^{y} \\mid \\mathbf{w}_{k, 1: n}^{y}, \\mathbf{w}^{x}\\right)}_{\\text {progressive full-context prediction }}\n\\end{aligned}\n$$\n\nPrevious study (Huang et al., 2022) shows that there is a gap called conditional total correlation between AR and fully-NAR learning paradigms, because of the lossy decomposition of NAR models. This gap is mainly responsible for the performance drop from AR to NAR models. However, when comparing iter-NAR, Eq. (20), with AR models, they both can be factorized into an initial prediction term and a progressive prediction process based on different context (i.e. left-context in AR and full-context in iter-NAR). The discrepancy as pointed out by Huang et al. (2022) is therefore closed in iter-NAR assuming sufficient steps. By showing DIFFUSEQ is an extension of the iter-NAR model, we offer a justification that it will not suffer from the conditional total correlation for the same reason. A straight-forward way to formulate naive diffusion models is to introduce a series of Gaussian noise-corrupted features $\\mathbf{y}_{1: T-1}, \\mathbf{y}_{0}=\\mathbf{y}, \\mathbf{y}_{T} \\sim \\mathcal{N}(0, \\mathbf{I})$ on continuous space as:\n\n$$\n\\begin{aligned}\np_{\\text {diffusion }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} \\underbrace{p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right)}_{\\text {final-step prediction }} \\underbrace{\\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)}_{\\text {progressive full-context diffusion }} \\\\\n& =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} \\prod_{i=1, \\ldots, n} p\\left(\\mathbf{w}_{i}^{y} \\mid \\mathbf{y}_{0, i}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} \\prod_{i=1, \\ldots, n} p\\left(\\mathbf{y}_{t-1, i} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nwhere $p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)$ describes the diffusion process on contiguous representations $\\mathbf{y}$. The total number of diffusion steps is denoted as $T$. Thereafter we omit the independent decomposition on $\\mathbf{w}^{y}$ and $\\mathbf{y}_{t}$. To apply diffusion models on discrete space, the rounding operation in DIFFUSEQ maps the continuous vectors $\\mathbf{y}$ to discrete $\\mathbf{w}^{y}$ for each time step $t$, we hence in addition introduce both contiguous feature $\\mathbf{y}$ and the discrete text $\\mathbf{w}^{y}$ to represent the discrete text into Eq. (21):\n\n$$\n\\begin{aligned}\np\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) \\\\\n& =\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n& =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n& =\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} \\sum_{\\mathbf{w}_{t}^{y}} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\nBy rearranging Eq. (23) and Eq. (24), we can see that DIFFUSEQ can be seen as a more generalized form of iter-NAR before marginalizing out $\\left\\{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}\\right\\}$, where Eq. (23) and Eq. (24) are equivalent with different computation order, despite the different initialization of $\\mathbf{y}_{T}$. For NAR models, $\\mathbf{y}_{T}$ is uniform copied from the source sentence or unk's token embedding (Gu et al., 2018); for diffusion models, $\\mathbf{y}_{T}$ is sampled from normal distribution $\\mathcal{N}(0, \\mathbf{I})$. It is notable that unlike AR and fully NAR models generating text all at once, iterative NAR and diffusion models feature a self-corrected text generation process. The graphical comparison is shown in Figure 7. ## C From DiffuSEQ To ItERative NAR and Diffusion MODELS\n\nFrom DiffuSEQ to Iterative NAR We show how to derive DiffuSEQ to iterative nonautoregressive model on discrete space. ![](https://cdn.mathpix.com/cropped/2024_09_12_e929bd7e46305c313f33g-17.jpg?height=983&width=920&top_left_y=281&top_left_x=581)\n\nFigure 7: Graphical Models of AR, Fully NAR, iterative NAR and DIFFUSEQ models.",
    "diffuseq-21": "For simplicity, we omit source node $\\mathbf{w}^{x}$. Gray nodes indicate dependency on the source node while white nodes are independent to the source node. $$\n\\begin{array}{rlr} \n& p_{\\text {DIFFUSEQ }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) & \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) & \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) & \\text { reorder computation } \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} \\int_{\\mathbf{y}_{t}} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) p\\left(\\mathbf{y}_{t} \\mid \\mathbf{w}_{t+1}^{y}\\right) & \\\\\n= & \\left.\\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} p\\left(\\mathbf{w}_{T}^{y} \\mid \\mathbf{y}_{T}, \\mathbf{w}^{x}\\right) \\prod_{t=T-1, \\ldots, 0} p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{w}_{t+1}^{y}, \\mathbf{w}^{x}\\right)\\right) & \\text { marginalize over } \\mathbf{y} \\\\\n= & \\sum_{\\mathbf{w}_{1}^{y}, \\ldots, \\mathbf{w}_{K-1}^{y}} p\\left(\\mathbf{w}_{1}^{y} \\mid \\mathbf{w}^{x}\\right) \\prod_{k=1 \\ldots K-1} p\\left(\\mathbf{w}_{k+1}^{y} \\mid \\mathbf{w}_{k}^{y}, \\mathbf{w}^{x}\\right) & \\\\\n= & p_{\\text {iter-NAR }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) &\n\\end{array}\n$$\n\nFrom DIFFUSEQ to diffusion model We show how to derive DIFFUSEQ to the straight-forward diffusion model on continuous space. $$\n\\begin{aligned}\n& p_{\\text {DIFFUSEQ }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right) \\\\\n= & \\sum_{\\mathbf{w}_{T}^{y}, \\ldots, \\mathbf{w}_{1}^{y}} \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} \\sum_{\\mathbf{w}_{t}^{y}} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{w}_{t}^{y}\\right) p\\left(\\mathbf{w}_{t}^{y} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & \\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\\\\n= & p_{\\text {diffusion }}\\left(\\mathbf{w}^{y} \\mid \\mathbf{w}^{x}\\right)\n\\end{aligned}\n$$\n\n$$\n=\\int_{\\mathbf{y}_{T}, \\ldots, \\mathbf{y}_{0}} p\\left(\\mathbf{w}^{y} \\mid \\mathbf{y}_{0}, \\mathbf{w}^{x}\\right) \\prod_{t=T, \\ldots, 1} p\\left(\\mathbf{y}_{t-1} \\mid \\mathbf{y}_{t}, \\mathbf{w}^{x}\\right) \\quad \\text { marginalize over } \\mathbf{w}^{y}\n$$\n\n## D DETAILS OF EXPERIMENTS\n\n## D. 1 Processing of Question Generation Dataset\n\nTo construct high-quality document-question pairs from the Quasar-T dataset, which consists of\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_e929bd7e46305c313f33g-18.jpg?height=53&width=1386&top_left_y=1085&top_left_x=367) actly matches document $_{i}$.",
    "diffuseq-22": "After pre-processing, we obtain 119 K document-question training pairs. ## D. 2 Settings OF BASELinEs\n\nWe compare the settings of different models, including the number of parameters and how to sample the different output sentences, as shown in Table 4. For plain GRU-based encoder-decoder methods, we do not implement diversity search algorithms on it, thus its sentence-level diversity could be very poor. For NAR-LevT, we set max iteration to 9 and follow the termination condition mentioned in the original paper. For GPVAE-T5, we tune the scalar to find the best trade-off between quality and diversity on the dev set. The scalars of all four tasks are set to 2 . We implement GPT2 baselines using HuggingFace Transformers and for the baseline Transformer-base, we use Fairseq. Table 4: The comparison for different models\n\n| Models | \\# Parameters | Learning Paradigm | Diversity Source |\n| :--- | :--- | :--- | :--- |\n| GRU-attention | 65 M | encoder-decoder | - |\n| Transformer-base | 80 M | encoder-decoder | Temperature/DBS |\n| GPT2-base FT | 117 M | pretrain-finetune | Hybrid strategy ${ }^{4}$ |\n| GPT2-large FT | 774 M | pretrain-finetune | Hybrid strategy |\n| GPVAE-T5 | 220 M | pretrain+VAE | Gaussian sampling |\n| NAR-LevT | 80 M | non-autoregressive | - |\n| DIFFUSEQ | 91 M | non-autoregressive | Gaussian sampling |\n\n## D. 3 Diversity and QUALItY Trade-OFf SEtTings\n\nWe list the details to obtain Figure 4. For GPVAE-T5, we set different scalars as 1, 2, 3, 4. For DIFFUSEQ, we choose trained models at different training steps to achieve different trade-off points. For other baselines, there is no explicit factor to control the diversity generation, so we leave them as single points in the figure. [^2]\n## D. 4 METRICS\n\nThe used BLEU score is sentence-level smoothed from BLEU-1 to 4, and used ROUGE-L score is longest common subsequence based statistics. The implementation is based on NLTK ${ }^{5}$ and torchmetrics. The n-gram based metrics may fail to capture the semantic meaning of sentences, so we consider using BERTScore ${ }^{6}$. Specifically, we use microsoft/deberta-xlarge-mnli to help BERTScore correlate better with human scores. ## D. 5 GENERATION RESULTS\n\nFor different tasks we list some generation examples. As we can see in Table 5, Table 6 and Table 7, DIFFUSEQ tends to generate diverse outputs, but sometimes the sentence is not as fluent as finetuned GPT2. Table 5: Sample outputs with different random seed in Dialogue test set. | Utterance: How long does the dye last? <br> Response: Just did this two days ago, not sure how it'll fade yet! |  |\n| :---: | :---: |\n| GPVAE-T5 | NAR-LevT |\n| * I'm not sure, I'm not sure. I've tested it a few <br> times, but I don't know for sure. I've | * half . |\n| * I'm not sure. I'm not sure how long it lasts, I'm <br> sure it 'll get better. It's been a while since | * half . |\n| * I've been using it for about a year and a half. <br> I've been using it for about a year and a half. | * half |\n| GPT2-large finetune | DIFFUSEQ |\n| * Two weeks in my case.",
    "diffuseq-23": "| * About an hour, 5 days or so. |\n| * I've had it for about a year. | $* 4$ days. |\n| * The dye can sit around for a month then you <br> can wash it.",
    "diffuseq-24": "| * I'm not sure about this, about the same <br> kind of time. |\n\nTable 6: Sample outputs with different random seed in Question Generation test set. | Statement: The Japanese yen is the official and only currency recognized in Japan. <br> Question: What is the Japanese currency? |  |\n| :---: | :---: |\n| GPVAE-T5 | NAR-LevT |\n| * What is the japanese currency | * What is the basic unit of currency for Japan? |\n| * What is the japanese currency | * What is the basic unit of currency for Japan? |\n| * What is the japanese currency | * What is the basic unit of currency for Japan? |\n| GPT2-large finetune | DIFFUSEQ |\n| * What is the basic unit of currency for Japan? | * What is the Japanese currency |\n| * What is the Japanese currency | * Which country uses the \"yen yen\" in currency |\n| * What is the basic unit of currency for Japan?",
    "diffuseq-25": "| * What is the basic unit of currency? |\n\n[^3]Table 7: Sample outputs with different random seed in Text Simplification test set. Complex sentence: People can experience loneliness for many reasons, and many life events may cause it, such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person. Simplified: One cause of loneliness is a lack of friends during childhood and teenage years. GPVAE-T5\nPeople can experience loneliness for many reasons, and many life events may cause it,\n\n* such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person\n\nPeople can experience loneliness for many reasons, and many life events may cause it,\n\n* such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person\n\nPeople can experience loneliness for many reasons, and many life events may cause it,\n\n* such as a lack of friendship relations during childhood and adolescence, or the physical absence of meaningful people around a person\n\n| GPT2-large finetune | DIFFUSEQ |\n| :--- | :--- |\n| * Loneliness can be caused by many things. | $*$ Many life events may cause of loneliness |\n| $*$ Loneliness can affect people in many ways. | People can also be very experience <br> loneliness for many reasons. |\n| $*$ Loneliness can be caused by many things. | {f06837a76-d85c-48c6-89f7-1650812eb538}People can experience loneliness for many <br> reasons, and many life events may, cause it. |\n\n\n[^0]:    ${ }^{1}$ Code is available at https://github.com/Shark-NLP/DiffuSeq\n\n[^1]:    ${ }^{2}$ For NAR models, $\\mathbf{y}_{T}$ is uniform copied from the source sentence or unk's token embedding ( Gu et al., 2018); for diffusion models, $\\mathbf{y}_{T}$ is sampled from normal distribution $\\mathcal{N}(0, \\mathbf{I})$. ${ }^{3}$ https://www.kaggle.com/c/quora-question-pairs\n\n[^2]:    ${ }^{4}$ Including top-p sampling, temperature, diversity beam search (DBS) and etc. Implement using HuggingFace Transformers https://github.com/huggingface/transformers\n\n[^3]:    5https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n    ${ }^{6}$ https://github.com/Tiiiger/bert_score\n\n"
}