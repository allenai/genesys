{
    "lbcontextcompress-0": "# EMPOWER YOUR MODEL WITH LONGER AND BETTER CONTEXT COMPREHENSION \n\nA PrEPRINT\n\nYifei Gao<br>University of Electronic Science and Technology of China<br>yilei.jin123@gmail.com<br>Lei Wang*<br>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences (CAS)<br>lei.wang1@siat.ac.cn<br>Jun Fang<br>University of Electronic Science and Technology of China ShenZhen University<br>fangjun@uestc.edu.cn hulonghua2021@email.szu.edu.cn<br>Jun Cheng<br>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences (CAS)<br>jun.cheng@siat.ac.cn\n\nJuly 28, 2023\n\n\n#### Abstract\n\nRecently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era.",
    "lbcontextcompress-1": "Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on \"why models are unable to compensate or strengthen their capabilities on their own\". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted on the challenging XSum dataset using LLaMa-7b model with context token length ranging from 800 to 1900 . Results demonstrate that we achieve substantial improvements compared with the original generation results evaluated by GPT4. Our codes are available at: https://github.com/yileijin/Attention-Transition/tree/main\n\n\n## 1 Introduction\n\nLarge Language Models have recently become the tidal current of the exploration field in AI for its good performance and huge potential. Specifically, the occurrence of GPT4[1], an exceptionally large multi-modal model, has revolutionized our understanding of model capacity. Notably, GPT-4's remarkable performance on zero-shot downstream tasks sets it apart from other LLMs. Moreover, its unprecedented deductive capabilities represent a substantial advancement in this field. While emergent performance is undoubtedly enticing, however, the cost of training and maintenance is discouraging. As a result, followers are then seeking emergent performance in LLMs without the need for such large-scale resources. First mentioned by DeepMind [2] that for a given compute budget, the best performance is not achieved by the largest models but by smaller models trained with more datasets. LLaMA [3], as a practitioner, has further substantiated the validity of this concept by training different foundation models on a massive corpus. Meanwhile, training models finetuned on LLaMA have also become a fad. Not only in LLMs, but the emergent performance transfer in multi-modal is also a popular topic. For example, Mini-GPT4 [4], explores the emergent performance on image-text alignment. Although the emergent performance observed in LLMs is a milestone for AI research, it can not compensate for a fatal defect, i.e., long context comprehension in relatively small LLMs. In this paper, after careful observation and investigation, we introduce a technique named Attention Transition, which addresses the problem to a considerable extent. Our contributions are as follows:\n\n1. We fully explored the impact of attention weights on text generation and the influence of Rotary Embedding on information transition within LLMs. 2. We propose a technique that effectively tackles the context comprehension problem, enabling longer and better context comprehension with almost no additional training or impairment on generation fluency. 3. Research studies demonstrate that our technique achieves significant success in improving long-context comprehension and exhibits strong generalization potential when applied to current LLMs. ## 2 Related Works\n\n### 2.1 Position Embedding\n\nPosition embedding in language models serves the purpose of incorporating the positional information of words or tokens within a sequence. In essence, position embedding enables the language model to differentiate between words based on their positions within the sequence, enhancing the model's ability to understand and generate coherent and contextually appropriate text. The representations of position embedding are diverse. Some of the works introduced the utilization of absolute position embedding [5, 6, 7], by adding a set of trainable parameters on the original token embedding. In specific, BERT[8] uses totally random vectors at the initiation point while Attention uses a sinusoidal pattern to give prior information. Relative position embedding [9,10,11] is an extension of absolute position embedding that takes into account the relative distances or positions between tokens within a sequence, which captures the relative distances between tokens dynamically. This modification arises from the assumption in [10] that the absolute position embedding will no longer be a reminder beyond a certain distance. While position embeddings detailed above are mostly based on the decomposition of adding position encoding to the context representations, rotary embedding [12], however, encodes relative position by multiplying the context representations with a rotation matrix and clear theoretical interpretation. Rotary embedding incorporates the geometric property of vectors on a 2D plane and its complex form, then extends the theory into n-dimension form. Rotary embedding maintains the inherent characteristic of long-term decay, where tokens that are farther apart tend to have fewer connections, which we called Long-Term Decay. This property aligns well with the preferences of language models. For short, given rotary embedding $E_{\\Theta}^{d}$ and input embedding $\\boldsymbol{x} \\in \\mathbb{R}^{d}$, for position $m, \\boldsymbol{E}_{\\Theta, m}^{d} \\boldsymbol{x}$ can be computed as:\n\n$$\n\\boldsymbol{E}_{\\Theta, m}^{d} \\boldsymbol{x}=\\left(\\begin{array}{c}\nx_{1} \\\\\nx_{2} \\\\\nx_{3} \\\\\nx_{4} \\\\\n\\vdots \\\\\nx_{d-1} \\\\\nx_{d}\n\\end{array}\\right) \\otimes\\left(\\begin{array}{c}\n\\cos m \\theta_{1} \\\\\n\\cos m \\theta_{1} \\\\\n\\cos m \\theta_{2} \\\\\n\\cos m \\theta_{2} \\\\\n\\vdots \\\\\n\\cos m \\theta_{d / 2} \\\\\n\\cos m \\theta_{d / 2}\n\\end{array}\\right)+\\left(\\begin{array}{c}\n-x_{2} \\\\\nx_{1} \\\\\n-x_{4} \\\\\nx_{3} \\\\\n\\vdots \\\\\n-x_{d-1} \\\\\nx_{d}\n\\end{array}\\right) \\otimes\\left(\\begin{array}{c}\n\\sin m \\theta_{1} \\\\\n\\sin m \\theta_{1} \\\\\n\\sin m \\theta_{2} \\\\\n\\sin m \\theta_{2} \\\\\n\\vdots \\\\\n\\sin m \\theta_{d / 2} \\\\\n\\sin m \\theta_{d / 2}\n\\end{array}\\right)\n$$\n\n### 2.2 Attention Mechanism\n\nFirst proposed and implemented in [5], the attention mechanism is a key component in AI and is particularly prominent in the field of deep learning. It allows models to focus on specific parts of input data or context while performing a task, enabling them to selectively process relevant information. Various attention mechanisms are developed for either computation efficiency $[13,14,15]$ or information handling improvement $[16,17]$. Self-Attention with Multi-Head The input sequences $\\boldsymbol{x} \\in \\mathbb{R}^{b * l}$, where $b$ is the batch size and $l$ is the sequence length, are transformed into three sets of vectors, queries $(\\boldsymbol{Q})$, keys $(\\boldsymbol{K})$, and values $(\\boldsymbol{V})$, where $\\boldsymbol{Q}=\\boldsymbol{x} \\boldsymbol{W}_{\\boldsymbol{q}}, K=\\boldsymbol{x} \\boldsymbol{W}_{\\boldsymbol{k}}$, $\\boldsymbol{V}=\\boldsymbol{x} \\boldsymbol{W}_{\\boldsymbol{v}}, \\boldsymbol{W}_{\\boldsymbol{q}}, \\boldsymbol{W}_{\\boldsymbol{q}}, \\boldsymbol{W}_{\\boldsymbol{q}} \\mathbb{R}^{l * d}, d$ is the hidden state size. The attention mechanism is computed as:\n\n$$\n\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\n$$\n\nMulti-head attention divides the hidden state dimension $d$ into $n * k$, then the self-attention becomes the concatenation form of attention heads:\n\n$$\n\\operatorname{MultiHead}(Q, K, V)=\\operatorname{Concat}\\left(\\text { head }_{1}, \\ldots, \\text { head }_{\\mathrm{h}}\\right) W^{O}\n$$\n\nFor each attention head, attention mechanism remains the same:\n\n$$\n\\operatorname{head}_{i}=\\operatorname{Attention}\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)=\\underbrace{\\operatorname{softmax}\\left[\\frac{Q W_{i}^{Q}\\left(K W_{i}^{K}\\right)^{T}}{\\sqrt{d_{k}}}\\right]}_{P} V W_{i}^{V}\n$$\n\nAttention uses part $\\boldsymbol{P}$ in equation 4 to capture the relation among input context tokens. Our attention transition technique is mainly focusing on the part of P , for the rest of the paper, we will dive into this part and find a way to longer and better context comprehension. ## 3 Exploration\n\nPreviously proposed by [13], which maps attention weights from $R * R$ into $R * N(N<<R)$, it reveals that several most important attention weights carry almost the whole information needed. Intuitively, when the same self-attention or cross-attention mechanism is utilized in current LLMs, for a single layer, they are expected to function the same. But with stacks of more layers in LLMs and more input-output tokens, attention distribution changes, and so does the conclusion. Our idea is to eliminate unimportant attention weights and dispense them to important ones. So it is clear for our intention to explore attention mechanism: (1) How self-attention works in LLMs; (2) What affects attention weights most; (3) How attention weight distribution is in LLMs; (4) How much disturbance it will be after our eliminating operation. For experiments and conclusions below, we only concentrate on LLaMA-7b model. ### 3.1 Self-Attention with Rotary Embedding\n\nAttention mechanisms along with rotary position embedding are now the most utilized structure for the attention layer. Here, we do not discuss the detail of them but directly investigate their influence on attention weights computation. As mentioned in [12], rotary embedding successfully inherits the intuition of \"Long-Term Decay\", with upper bounds decreasing gradually along with relative distance, shown in Figure 1, which can be seen as a guideline for models to obey. Then it comes to a question: how can a model get information that exists over relatively long distances? It is infeasible to directly compute the interactions between token pairs on different attention layers, but if we visualize every decoder layer's outputs by concatenating with LM-Head, we can find that the information transfer among layers has traces to follow. After we concatenate LM-Head on each decoder layer in the LLaMa model and output generation results, we find that although the outputs of incomplete logits are incoherent or even make no sense, however, many same tokens appeared during layer transfer with different positions. Comparing the positions of these tokens, we conclude that: the rotary embedding limits the understanding of long-distance information, but as compensation, important information will gradually be sent to the closer positions during layer computation. As a result, the logits of the last token (LLaMa uses it to compute the generation token) can focus on these information blocks and consider the global context. ### 3.2 Attention Weight Distribution\n\nIntuitively, when the token lengths increase, the attention weights tend to become sparser. To conduct the attention weight distribution experiments and identify patterns in common, we computed the last position's attention distribution in different layers with different token lengths ranging from 200 to 2000. ![](https://cdn.mathpix.com/cropped/2024_09_12_c0d507287836116e2f68g-04.jpg?height=839&width=1567&top_left_y=277&top_left_x=279)\n\nFigure 1: The upper bound of different relative distance, where x stands for relative distance, y strands for upper boundary. Generally speaking, except for the first layer, we observed a strong alignment between the mean distribution of higher order of magnitude attention weights and the Long-Term Decay nature of rotary embedding. Despite the presence of more intricate hierarchies, the lower order of magnitude attention weights distribute evenly among layers(we define attention weights lower than the order of magnitude of $1 / 1$ ( 1 is the sequence length) as lower ones). Additionally, beyond the first layer, attention weights among different attention heads within a single layer remain analogous. In the first layer, attention weights on odd attention heads exhibit different trends compared with attention weights on even attention heads, as shown in Figure 2. We also observed that attention weights that are extremely high (greater than 0.5 ) consistently occur on the first token, with other attention weights desolated. This finding suggests the reason why during the generation with long tokens, there may be instances where the model simply retells the context from the beginning. ### 3.3 Attention Weight Disturbance\n\nThe assumption is that small partial attention weights contribute significantly to the overall importance. It's reasonable to eliminate smaller attention weights and rescale them into 1, with the expectation that this adjustment will not result in significant changes. Through computation results conducted on tokens of more than 450 , we find that we can eliminate most of the attention weights and the outputs will not change at all even after the accumulation of adjustments. The first layer is not included because of its strange weights distribution. By eliminating no more than $10 \\%$ attention weight, which accounts for approximately $60 \\%$ of the attention, the absolute differences compared to the original absolute logits weights are beneath $1 \\%$. But if we try to eliminate more weights count for more than $20 \\%$, the square difference will nearly exponentially increase. Then we also compute differences among layers; and find that the last several layers' square differences are clearly higher than previous layers. Meanwhile, LLM itself is robust enough to persist the information with disturbance. We partially conduct attention elimination and do not rescale the sum of attention weights into 1 . The final outputs still do not change. Curious about this finding, we investigate the final logits after $\\mathbf{l m}$ head, and the difference is high enough that one can distinguish with each other easily. This finding also proves the discreteness trained for LLM models. Notice that this finding is very useful for further augmentation detailed below. ![](https://cdn.mathpix.com/cropped/2024_09_12_c0d507287836116e2f68g-05.jpg?height=734&width=1626&top_left_y=246&top_left_x=250)\n\nFigure 2: The 1st layer's attention distribution among different heads. Once we have these discoveries, a natural line of thought arises. To facilitate the transfer of important information to further distance, in essence, our objective is to surpass the limitations imposed by rotary embedding, allowing the attention weight assigned to previously important information to become substantial enough to be concentrated. Our method named Attention Transition emerges in response to demands. ## 4 Attention Transition\n\nOur main purpose is to transfer unimportant attention weights into needed ones, by eliminating smaller attention weights and dispensing them to others. Considering the sequential transition nature, the effect of the disturbance and rotary embedding: (1) It's hard to eliminate attention weights with one exact bound, because many important information blocks may present a small weight number due to the lengths of tokens that can be seen. (2) It's irrational to apply attention transition globally. For import information in the rear, the manual transfer is unnecessary. For important information in the front, obviously, we are supposed to wait for the model itself to transmit the information. Then what we can help is to accelerate or augment the information transfer nature of the model itself, without much disturbance. Our algorithm consists of two parts, a Decision Maker and a Dispenser. We use decision maker to decide where the token positions and model layers are to apply this algorithm, and use dispenser to implement. ### 4.1 Decision Maker\n\nAware that there are distributional and quantitative variances between layers detailed before, so we do not implement the algorithm on (1) the first layer, because of its special weight distribution among attention heads; (2) the last two layers, as we do not want much disturbance. To cater to the transitional nature of information and keep computation efficient, we divide the input sentence by a fixed interval. Within a single layer, we exclusively apply attention transition to one interval. Assuming that the last chunk of words' logits does not need to change, and commence our transition from the third interval onwards, the total intervals that can be divided are 29. For the concern that too delicate disturbance will result in high perplexity; and one augmentation for a single interval may not be sufficient, we then design our algorithm to repeat strengthening operation within layers, and in a single layer, we strengthen several intervals. The design of the decision maker is versatile, which implies high adaptability in different situations. The decision maker utilized in our experiments is to sequentially augment intervals. ![](https://cdn.mathpix.com/cropped/2024_09_12_c0d507287836116e2f68g-06.jpg?height=588&width=1635&top_left_y=245&top_left_x=243)\n\nFigure 3: Illustration of the Dispenser\n\n### 4.2 Dispenser\n\nDispenser is meant to control how to apply attention transition. Because of the Long-Term Decay nature of rotary embedding, the connection between information closed to the computing token is strong enough, so we are supposed to use our technique to augment information connection with further distance intervals. As the information transfer within layers is a stack of accumulation, it's unnecessary to accomplish the augmentation in one fell swoop. Dispenser has two aims to accomplish, attention weights elimination and attention weights dispensation. Our algorithm is depicted in Figure 3. In exploration chapter, we have found that the lack of some attention weights will not cause any output changes. Because of the robustness of LLM, we can sacrifice some perplexity to get better context comprehension by manually adding attention weights. The rationale behind attention transition is to make the best use of the attention weights, so there must be sacrifices of lower ones. But if we slightly expand the sum of attention weights to more than one, results will be much better with just a little higher perplexity, which is illustrated in Algorithm 1. Through experiments, we find that it has almost no impairments for final output perplexity that we eliminate around $15 \\%$ attention weights and manually double or triple even 10 times these weights before handing them out. We also mentioned that the first token always has unproportionally high attention weights, so during elimination, we manually half the first token's attention weight being distributed and add it to eliminating attention weights. ```\nAlgorithm 1 Attention Transition given an interval and a layer to use\n    \\(\\mathbf{s}, \\mathbf{e}, \\mathbf{i} \\quad\\{\\) the start position, end position, and interval size, \\(\\mathbf{i}=\\mathbf{e}-\\mathbf{s}\\) \\}\n    \\(\\alpha, \\beta \\quad\\) \\{parameters used to control dispensation procedure\\}\n    att \\(\\leftarrow \\mathbf{R}_{b \\times n \\times l \\times l} \\quad\\) \\{multi-head attention weights \\}\n    bound \\(=\\alpha / \\mathbf{e} \\quad\\) \\{the lower case boundary\n    att \\(_{:,,, s: e,: e}=\\) att where att \\(\\geq\\) bound else \\(0 \\quad\\) \\{attention elimination \\}\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_c0d507287836116e2f68g-06.jpg?height=41&width=933&top_left_y=1942&top_left_x=276)\n\n```\n    \\(\\operatorname{att}_{\\text {eliminated }}=1-\\) att \\(_{\\text {sum }} \\quad\\{\\) eliminated weights \\}\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_c0d507287836116e2f68g-06.jpg?height=39&width=722&top_left_y=2013&top_left_x=276)\n\n```\n    addmatrix \\(=\\left(\\boldsymbol{a t t}_{\\text {eliminated }} / \\operatorname{sum}(\\right.\\) mask, \\(\\left.\\operatorname{dim}=-1)\\right)[:, ;, ;\\) None \\(] * \\beta \\quad\\{\\beta\\) is a scaler \\(\\}\\)\n    \\(\\mathbf{a t t}_{:,,, s: e, s-i: e-i}=\\mathbf{a t t}_{:,:, s: e, s-i: e-i}+7^{*}\\) addmatrix \\(_{:,,,,,-i} \\quad\\) \\{attention dispensation\\}\n    att \\(_{,:,, s: e, s-2 * i: e-2 * i}=\\) att \\(_{:,:, s: e, s-2 * i: e-2 * i}+3^{*}\\) addmatrix \\(_{:,:,,:, i} \\quad\\) \\{attention dispensation \\(\\}\\)\n    return att\n```\n\n\n## 5 Experiment Results\n\n### 5.1 Experimental Setup\n\nWe use the LLaMa-7b model in reference with parameters unchanged, which means our method can be employed by other tasks. Our test datasets were obtained from xsum [18], wherein we selected 200 paragraphs for each token length:\n\n| Token Leghths | 800 | 1200 | 1600 | 1900 |\n| :--- | :---: | :---: | :---: | :---: |\n| LLaMa-7b | 56.68 | 46.428 | 51.245 | 46.425 |\n| LLaMa-7b after Attention Transition | $\\mathbf{8 0 .",
    "lbcontextcompress-2": "9 4}$ | $\\mathbf{7 4 . 6 2}$ | $\\mathbf{8 1 . 3 7 4}$ | $\\mathbf{7 5 . 0 7}$ |\n\nTable 1: Generation result comparisons between original outputs and outputs after attention transition. Because there are many generation results with variable controllers, $\\alpha, \\beta$, and so on, we select the best generation results in comparison. The results are marked by GPT4, and the prompt template is available in our codes, as well as all kinds of our generation results. 800,1200,1600, and 1900 from the original datasets. Additionally, the original summaries of these sentences were limited to 50 tokens, which means it's really a hard task for comprehension. During generation, we add a prompt for inputs to inform our intention: \"Please summarize the context above: \" and make the model generate 50 tokens for 800 and 1200 token lengths, 100 for 1600 and 1900 lengths. With no other modifications made, we conducted a direct comparison of output quality between the original model and the model after applying the attention transition technique. To compare the categorical superiority of our technique, the temperature is set to 0 with explicit outputs, so you can acquire our results easily without ambiguity. We use GPT4 to score the generation results range form 1 to 100. The evaluations of GPT4 show that after utilizing the attention transition technique, our generation's quality overwhelmingly outperforms the original ones, illustrated in Table 1. As the results are stunning enough, in the beginning, we thought there may be something wrong. Because our experiments are highly repeatable, the generation results can not be the problem, so we not only compute the scores for generation results but also make GPT4 compute the reasons why scores are scored during the generation of 800 tokens and 1200 tokens(each we select 60 sentences to compute reasons ). These reasons are also available in our code. | Parameters: $\\alpha, \\beta$, lay | $0.5,0.1,2$ | $1,0.1,2$ | $0.5,0.1,3$ | $1,0.1,3 \\mid$ |\n| :--- | :---: | :---: | :---: | :---: |\n| 800 tokens | 56.52 | 56.81 | 57.66 | 57.33 |\n| 1200 tokens | 46.10 | 46.92 | 48.07 | 47.66 |\n| 1600 tokens | 43.93 | 45.59 | 25.00 | 45.71 |\n| 1900 tokens | 36.37 | 39.40 | 50.0 | 39.18 |\n\nTable 2: Results without attention extension. We do not compute results on 1600 tokens and 1900 tokens using these 3 sets of parameters. lay refers to how many intervals to implement attention transition within 1 layer\n\n### 5.2 Ablation Study and Analysis\n\nDuring generation, we found that the parameters in attention transition have a substantial influence on generation results. From Table 2, we conclude that without extended attention weights, the results are highly susceptible to parameters. For bad generation results, we find that some sentences' summaries are largely about the first several sentences. The usage of basic attention transition without attention extension is not as powerful as the attention transition after attention extension, shown in Table 3, where we can see that the results are significantly better than the original results. The best results we obtained during generation are not anchored with 1 set of parameters, but the whole rounding sets of parameters, so the average results are not as stunning as the best results. Too much disturbance on attention weights may lead to bad generation results, with the existence of repeating, unreasoning, hallucinating, or even totally chaotic generation, as shown in Table 4. In conclusion, attention transition without attention weight extension is most helpful when the sequence is not too long or too short. The cutting off of attention weight on the first token is not always helpful, which depends mainly on the distribution of the context you want to focus on. Attention transition with attention extension is extremely helpful to strengthen models' comprehension ability, in exchange for their stability, effectively leveraging this characteristic is crucial to success. Our algorithm is not stable enough to fully achieve the potential of the best generation results; but with exhaustive experiments, some suitable sets of parameters are selected with at least more than 5 degrees of augmentation. | Token Lengths | Parameters: $\\alpha, \\beta$, lay | Average Results |\n| :--- | :---: | :---: |\n| 800 | $0.5,0.5,2$ | 61.79 |\n|  | $1,0.5,2$ | 65.42 |\n| 1200 | $1.5,0.5,2$ | 62.61 |\n|  | $1,0.5,2$ | 55.21 |\n| 1600 | $1,0.3,3$ | 56.47 |\n|  | $0.5,0.8,3$ | 56.48 |\n|  | $0.5,1,4$ | 60.0 |\n\nTable 3: Results after attention extension, where $\\beta$ is bigger than 0.1\n\n| Token Lengths | Parameters: $\\alpha, \\beta$, lay | Average Results |\n| :--- | :---: | :---: |\n| 1600 | $1,0.5,4$ | 28.33 |\n|  | $1,0.8,4$ | 0 |\n|  | $1.5,0.5,4$ | 0 |\n\nTable 4: Results of the generation where attention extension is excessively used\n\n### 5.3 Usage in Short or Very Long Context Comprehension\n\nExcept for token lengths we have already discussed, for short context comprehension like 400 or 600 tokens, we also see obvious augmentation among both token lengths. In shorter lengths, obvious augmentation mainly comes from the attention extension. Unlike longer token lengths, the excessive usage of attention extension in shorter will lead to semantic incompleteness easily, which is an issue that warrants attention. Long context like 2400 tokens is largely beyond the model's own capacity, with original outputs totally a mess. We find that attention transition is incompetent with that length although it achieves excellent results on token length 1900, our technique can not break through the inherent limitations of the model itself. ### 5.4 Accessibility and Generalization\n\nAs we have declared, our technique does not need any training procedure and at the same time, plug-and-play. And the design of our technique is concise and flexible, permitting freedom in different situations. Meanwhile, we conducted our technique in other models that use attention mechanisms and rotary embedding, where all models demonstrate substantial progress in context comprehension, like Falcon-7b and ChatGLM-6b [19, 20]. For models with attention mechanisms but not rotary embedding [21], changing the decision maker and the dispenser algorithms can also result in better performance with attention extension, since the robustness of these models is analogous. For LLMs with larger sizes, our technique is also useful, in that no matter the model size, long context comprehension is defined relatively, for example, 10k context with 30 billion parameters. The intuition behind our Attention Transition technique after attention weight extension is the exchange of stability and comprehension capacity, LLMs are trained discretely enough to encounter these disturbances caused by attention transition. Further, we suspect that this technique can also be used in CV models, with precise control of attention disturbance. ## 6 Conclusion\n\nAfter thorough investigations, we introduced a technique called attention transition, which has outstanding properties: (1) strengthens LLM's comprehension ability to a large extent; (2) without any additional training needed and plugand-play; (3) possesses a strong generalization property, making it adaptable to a wide variety of situations. Results marked by GPT4 demonstrate that our technique has huge potential to facilitate the implementation of small LLMs in the industrialization process as well as cheaper ultra-long text comprehension for existing large LLMs. While the results are stunning enough, there is still room for growth that remained unexplored. We sincerely aspire for our work to be an accelerator for the coming AI era. ## References\n\n[1] OpenAI, \"Gpt-4 technical report,\" 2023.",
    "lbcontextcompress-3": "[2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre, \"Training compute-optimal large language models,\" 2022. [3] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, \"Llama: Open and efficient foundation language models,\" 2023.",
    "lbcontextcompress-4": "[4] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, \"Minigpt-4: Enhancing vision-language understanding with advanced large language models,\" 2023. [5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" 2017.",
    "lbcontextcompress-5": "[6] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, \"Albert: A lite bert for self-supervised learning of language representations,\" 2020. [7] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, \"Electra: Pre-training text encoders as discriminators rather than generators,\" 2020. [8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training of deep bidirectional transformers for language understanding,\" 2019. [9] P.",
    "lbcontextcompress-6": "Shaw, J. Uszkoreit, and A. Vaswani, \"Self-attention with relative position representations,\" 2018. [10] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q.",
    "lbcontextcompress-7": "V. Le, and R. Salakhutdinov, \"Transformer-xl: Attentive language models beyond a fixed-length context,\" 2019.",
    "lbcontextcompress-8": "[11] P. He, X. Liu, J. Gao, and W. Chen, \"Deberta: Decoding-enhanced bert with disentangled attention,\" 2021.",
    "lbcontextcompress-9": "[12] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu, \"Roformer: Enhanced transformer with rotary position embedding,\" 2022.",
    "lbcontextcompress-10": "[13] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, \"Linformer: Self-attention with linear complexity,\" 2020.",
    "lbcontextcompress-11": "[14] H. Zhou, S. Zhang, J. Peng, S. Zhang, J.",
    "lbcontextcompress-12": "Li, H. Xiong, and W. Zhang, \"Informer: Beyond efficient transformer for long sequence time-series forecasting,\" 2021. [15] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R\u00e9, \"Flashattention: Fast and memory-efficient exact attention with io-awareness,\" 2022. [16] A. Mohtashami and M. Jaggi, \"Landmark attention: Random-access infinite context length for transformers,\" 2023.",
    "lbcontextcompress-13": "[17] S. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski, and P. Mi\u0142o\u015b, \"Focused transformer: Contrastive training for context scaling,\" 2023.",
    "lbcontextcompress-14": "[18] S. Narayan, S. B. Cohen, and M. Lapata, \"Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization,\" in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018.",
    "lbcontextcompress-15": "[19] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, \"The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only,\" 2023 .",
    "lbcontextcompress-16": "[20] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma, Y. Xue, J. Zhai, W. Chen, P.",
    "lbcontextcompress-17": "Zhang, Y. Dong, and J. Tang, \"Glm-130b: An open bilingual pre-trained model,\" 2022. [21] K. Lin, C.-C. Lin, L. Liang, Z. Liu, and L. Wang, \"Mpt: Mesh pre-training with transformers for human pose and mesh reconstruction,\" 2023. "
}