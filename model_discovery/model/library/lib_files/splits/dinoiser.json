{
    "dinoiser-0": "# DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises \n\nJiasheng Ye ${ }^{* \\varrho \\diamond}$ Zaixiang Zheng ${ }^{\\dagger \\ominus}$ Yu Bao ${ }^{\\ominus}$ Lihua Qian ${ }^{\\varnothing}$ and Mingxuan Wang ${ }^{\\ominus}$<br>${ }^{\\ominus}$ ByteDance Research ${ }^{\\diamond}$ Fudan University<br>jsye23@m.fudan.edu.cn, zhengzaixiang@bytedance.com<br>\\{baoyu.001, qianlihua, wangmingxuan. 89 \\} @bytedance.com<br>https://github.com/yegcjs/DINOISER\n\n\n#### Abstract\n\nWhile diffusion models have achieved great success in generating continuous signals such as images and audio, it remains elusive for them to learn discrete sequence data like natural languages. Although recent advances circumvent this challenge of discreteness by embedding discrete tokens as continuous surrogates, they still fall short of satisfactory generation quality. To understand this, we first dive deep into the denoised training protocol of diffusion-based sequence generative models and determine their three severe problems: (1) failing to learn; (2) lack of scalability; and (3) neglecting source conditions. We argue that these problems can be boiled down to the pitfall of the not completely eliminated discreteness in the embedding space, and the scale of noises is decisive herein. In this paper, we introduce DINOISER to facilitate diffusion models for sequence generation by manipulating noises. We propose to adaptively determine the range of sampled noise scales during training; and encourage the proposed diffused sequence learner to leverage source conditions with amplified noise scales during inference. Experiments show that DINOISER enables consistent improvement over the baselines of previous diffusion sequence generative models on several conditional sequence modeling benchmarks thanks to both effective training and inference strategies. Analyses further verify that DINOISER can make better use of source conditions to govern its generative process. ## 1 Introduction\n\nConditional sequence learning aims at generating a target sequence from given conditions, which is one of the important paradigms of natural language generation (Sutskever et al., 2014; Wiseman\n\n[^0]et al., 2017; Raffel et al., 2020), including machine translation (Bahdanau et al., 2014), summarization (Rush et al., 2015), and paraphrasing (Prakash et al., 2016). Recent advances in generative modeling introduce diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b), which achieve great success in generating continuous signals, including images (Rombach et al., 2021), video (Ho et al., 2022), and audio (Kong et al., 2020). With promising characteristics such as diversity and controllability demonstrated in these domains, diffusion models also garner growing interest for sequence learning in the research community (Li et al., 2022), which further gives the promise to a unified generative modeling paradigm across different modalities (Bao et al., 2023),\n\nHowever, the discrete nature of sequence data, constituted by a number of tokens in order, makes it non-trivial to apply diffusion models for conditional sequence learning. Typical diffusion models noise data with Gaussian permutation kernels (Ho et al., 2020) and learn to recover original data from their corrupted versions, which is not directly compatible with discrete tokens. To remedy this, DiffusionLM (Li et al., 2022) attempted to embed discrete tokens into continuous space and employ diffusion models to the embedding space. Although this kind of approach unlocks the possibility of applying diffusion models to discrete data, it still falls short of competitive performance for various conditional sequence generation tasks (Fig. 1A). We argue that embedding discrete tokens into continuous surrogates does not necessarily eliminate discreteness completely. To verify this, we conduct in-depth preliminary studies and highlight our findings along with their implications as follows. (1) On the pitfall of discreteness. Embeddings populate only finite clusters (up to the vocabulary size) in the continuous space, which results in the vastness of low-density regions especially when the models are learned with small-scale noises. We\nrefer to this as the pitfall of discreteness, which suggests that small noises hinder conditional sequence learning, and thus should be avoided during training. (2) On scalability. It becomes increasingly harder for the diffusion process to eliminate discreteness when the dimension of the embedding space gets scaled up, suggesting that to ensure scalability, an adaptable noise schedule is necessitated yet neglected. (3) On conditional learning. Enlarging noises in inference can calibrate diffusion models to take into account more source conditional information. Please refer to $\\S 3$ for more details. Motivated by these findings, we propose DINOISER to improve diffusion models by manipulating noises for conditional sequence learning. We propose a novel training strategy to eliminate training on small noise scales to avoid their negative influences, for which we introduce the noise scale clipping strategy to adaptively manipulate the noise scales. For inference, we propose to manipulate the model to be exposed to larger noise scales to encourage trained diffusion models to leverage source conditions. We summarize our contributions as well as our findings as follows:\n\n- By thorough and in-depth preliminary studies, we shed light on the pitfall of discreteness along with the critical role of noise scales in conditional sequence learning with diffusion models, thereby suggesting meliorated solutions in terms of both training and inference by manipulating noises. - We accordingly propose DINOISER to leverage large noise scales in both training and inference. Experiments show that DINOISER achieves strong performance on a variety of conditional sequence learning tasks, paving way for featuring diffusion models for various conditional sequence learning tasks. Our experiments comprehensively include several machine translation benchmarks (both bilingual and multilingual), as well as text simplification and paraphrasing, ranging from lowresource to high-resource scenarios. - Ablations show that both DINOISER 's improved training and inference approaches result in considerable performance gains. Further analysis verifies that our proposed posthoc inference strategy, i.e., the condition enhanced denoiser, can help make better use of source conditions for accurate predictions. ## 2 Background\n\nConditional Sequence Learning. Conditional sequence learning aims to yield target sequence $\\mathbf{y}=\\left[y_{1}, y_{2}, \\ldots, y_{n}\\right] \\in\\{0,1\\}^{n \\times|\\mathcal{V}|}$ within the vocabulary space $\\mathcal{V}$, given source conditions $\\mathbf{x}$, which can be another sequence $\\mathbf{x}=\\left[x_{1}, x_{2}, \\ldots, x_{m}\\right]$. The conventional modeling paradigm (Sutskever et al., 2014; Vaswani et al., 2017) generates target tokens in an autoregressive decomposition $p(\\mathbf{y} \\mid \\mathbf{x})=\\prod_{i=1}^{n} p\\left(y_{i} \\mid \\mathbf{y}_{<i}, \\mathbf{x}\\right)$. Gu et al. (2018) proposed an alternative way in a fully nonautoregressive (NAR) manner, where all the tokens are predicted in parallel by assuming conditional independence between the target tokens, i.e., $p(\\mathbf{y} \\mid \\mathbf{x})=\\prod_{i=1}^{n} p\\left(y_{i} \\mid \\mathbf{x}\\right)$. Later works alleviate this strong assumption by iterative refinement (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019), resulting in improved generation quality. These iterative refinement approaches generate target sequences with several cycles, in each of which the models generate sequence depending on both the source sequence and the intermediate prediction of the previous one, i.e., $p(\\mathbf{y} \\mid \\mathbf{x})=\\prod_{t=1}^{T} p\\left(\\mathbf{y}^{(t)} \\mid \\mathbf{y}^{(t-1)}, \\mathbf{x}\\right)$. Diffusion Probabilistic Models. Given a random variable $\\mathbf{z}_{0}$ from an underlying data distribution $q\\left(\\mathbf{z}_{0}\\right)$, diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) define a forward diffusion process $\\left\\{\\mathbf{z}_{t}\\right\\}_{t \\in[0,1]}$ perturbed with a Gaussian perturbation kernel, starting with $\\mathbf{z}_{0}$ and converging to its corresponding stationary distribution, such that for any $t \\in[0,1]$, the distribution of $\\mathbf{z}_{t}$ given $\\mathbf{z}_{0}$ satisfies $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}\\right)=\\mathcal{N}\\left(\\mathbf{z}_{t} ; \\alpha(t) \\mathbf{z}_{0}, \\sigma^{2}(t) \\mathbf{I}\\right)$, or with Gaussian reparameterization (Kingma and Welling, 2013; Ho et al., 2020),\n\n$$\n\\mathbf{z}_{t}=\\alpha(t) \\mathbf{z}_{0}+\\sigma(t) \\epsilon_{t}, \\quad \\epsilon_{t} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n$$\n\nwhere $\\sigma(t)$ is a monotonically increasing function, usually referred to as the noise schedule, satisfying $\\sigma(0)=0$ and $\\sigma(1) \\approx 1 ;$ and $\\alpha(t)=\\sqrt{1-\\sigma^{2}(t)}$. The noise schedule $\\sigma(t)$ controls the degree of corruption at different timestep $t$. As $t$ gets larger, the noise scale $\\sigma(t)$ gets larger whereas $\\alpha(t)$ gets smaller, hence the more corrupted data $\\mathbf{z}_{t}$ from the original $\\mathbf{z}_{0}$. At $t=1$, with $\\alpha(1) \\approx 0$ and $\\sigma(1) \\approx 1$, $\\mathbf{z}_{t}$ become pure noises as reaching the stationary distribution of a standard Gaussian. Song et al. (2020b) proves that such a Gaussian diffusion process has the same transition distribution $q\\left(\\mathbf{z}_{t} \\mid \\mathbf{z}_{0}\\right)$ as the stochastic differential equa-\ntion (SDE): $\\mathrm{d} \\mathbf{z}=-\\frac{1}{2} \\beta(t) \\mathbf{z} \\mathrm{d} t+\\sqrt{\\beta(t)} \\mathrm{d} \\omega$ where $\\beta(t)=-2 \\frac{\\mathrm{d} \\log \\alpha(t)}{\\mathrm{d} t} ; \\omega$ denotes the standard Wiener process. As such, the corresponding generative process can be achieved as its time reversal by solving the following ordinary differential equation (diffusion ODE):\n\n$$\n\\begin{aligned}\n\\mathrm{d} \\mathbf{z} & =\\left[-\\frac{1}{2} \\beta(t) \\mathbf{z}+\\frac{1}{2} \\beta(t) \\frac{\\epsilon_{t}}{\\sigma(t)}\\right] \\mathrm{d} t \\\\\n& =\\left[-\\frac{1}{2} \\beta(t) \\mathbf{z}+\\frac{\\beta(t)}{2 \\sigma^{2}(t)}\\left(\\mathbf{z}-\\alpha(t) \\mathbf{z}_{0}\\right)\\right] \\mathrm{d} t\n\\end{aligned}\n$$\n\nIn practice, we can then use a learned model $\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$ to estimate $\\mathbf{z}_{0}$ and plug into Eqn. 2, which can be learned by minimizing the discrepancy between training data and model estimation (Ho et al., 2020; Song et al., 2020b; Ramesh et al., 2022):\n\n$$\n\\mathcal{L}_{\\text {diffusion }}\\left(\\mathbf{z}_{0}\\right)=\\underset{\\substack{t \\sim \\mathcal{U}(0,1) \\\\ \\epsilon_{t} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})}}{\\mathbb{E}}\\left[\\left\\|\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)-\\mathbf{z}_{0}\\right\\|_{2}^{2}\\right]\n$$\n\nGiven Eqn. 2 with a trained model $\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$, we can use arbitrary ODE solvers to solve this diffusion ODE from $t=1$ to $t=0$ for sampling data. An effective and efficient solver to this end is the DDIM solver (Song et al., 2020a; Lu et al., 2022) and is widely adopted. It discretizes the ODE into $M+1$ timesteps $\\left\\{t_{i}\\right\\}_{i=0}^{M}$ decreasing from $t_{0}=1$ to $t_{M} \\approx 0$. Then, it samples $\\mathbf{z}_{t_{0}}$ from the standard Gaussian distribution and computes $\\left\\{\\mathbf{z}_{t_{i}}\\right\\}_{i=1}^{M}$ with $M$ iterations, in each of which $\\mathbf{z}_{t_{i}}$ is predicted from $\\mathbf{z}_{t_{i-1}}$ according to\n\n$$\n\\mathbf{z}_{t_{i}}=\\alpha\\left(t_{i}\\right) \\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t_{i-1}}, t_{i-1}\\right)+\\sigma\\left(t_{i}\\right) \\epsilon_{\\theta}\\left(\\mathbf{z}_{t_{i-1}}, t_{i-1}\\right)\n$$\n\nwhere $\\epsilon_{\\theta}\\left(\\mathbf{z}_{t_{i-1}}, t_{i-1}\\right)$ is the predicted noise, which can be directly induced according to Eqn. 1,\n\n$$\n\\epsilon_{\\theta}\\left(\\mathbf{z}_{t_{i-1}}, t_{i-1}\\right)=\\frac{\\mathbf{z}_{t_{i-1}}-\\alpha\\left(t_{i-1}\\right) \\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t_{i-1}}, t_{i-1}\\right)}{\\sigma\\left(t_{i-1}\\right)}\n$$\n\nAfter iterations, the last prediction $\\mathbf{z}_{t_{M}}$ is taken as the final generated result $\\hat{\\mathbf{z}}_{0}$ of the sampling. ## Diffusion Models for Conditional Sequence\n\nLearning. The denoising process of diffusion models matches an iterative refinement process (Gong et al., 2022). However, diffusion models are not directly applicable to sequence learning tasks since the original diffusion models operate in continuous space rather than sequences of discrete tokens. DiffusionLM (Li et al., 2022) tackles this by embedding the discrete tokens into continuous latent space and applying diffusion models therein. We can then train the models as variational autoencoders (Kingma and Welling, 2013), where a diffusion model serves as the prior, from a latent-variable model perspective, and derive the corresponding variational lower bound (Wehenkel and Louppe, 2021; Vahdat et al., 2021):\n\n$$\n\\mathcal{L}(\\mathbf{y})=\\mathbb{E}_{\\mathbf{z}_{0}}[\\underbrace{-\\log p_{\\theta}\\left(\\mathbf{y} \\mid \\mathbf{z}_{0}\\right)}_{\\mathcal{L}_{\\text {reconstruction }}}+\\mathcal{L}_{\\text {diffusion }}\\left(\\mathbf{z}_{0}\\right)]\n$$\n\nwhere $\\mathbf{y}$ is the original sequence with $\\mathbf{z}_{0}$ as its embeddings ${ }^{1} ; \\mathcal{L}_{\\text {diffusion }}$ denote the diffusion loss (Eqn. 3) which now operates on the embedding space, and $\\mathcal{L}_{\\text {reconstruction }}$ is the newly added reconstruction term. To further adapt the model for conditional sequence generation, a vanilla approach is to replace the unconditional model $\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t}, t\\right)$ with a conditional model $\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t}, \\mathbf{x}, t\\right)$, where $\\mathbf{x}$ is the source condition. Similar to the previous practice of using diffusion models for conditional generation in vision (Rombach et al., 2021), the diffusion process can be kept unchanged, the same as Eqn. 1. And the length of the target sequences is decided by predicting the length difference between the source and the target. ## 3 The Pitfall of Discreteness: The Noise Scale Matters\n\nIn this section, we dive deep into the current weaknesses of diffusion models for conditional sequence learning and find that the noise scale matters, which accordingly motivates our proposal for improved training and inference.",
    "dinoiser-1": "Settings. We begin with the vanilla conditional diffusion model modified from DiffusionLM ( Li et al., 2022) as described in $\\S 2$. We follow the original paper of DiffusionLM to apply the sqrt schedule (i.e., $\\sigma(t)=t^{0.25}$ ) to arrange noise scales for training and sampling. We use IWSLT14 $\\mathrm{DE} \\rightarrow \\mathrm{EN}$ (Cettolo et al., 2012) machine translation benchmark for evaluation. We also include CMLM (Ghazvininejad et al., 2019) as a baseline for comparison, which is a strong conditional sequence generative model that generates sequence by iterative refinement similar to diffusion models but in discrete tokens space. [^1]![](https://cdn.mathpix.com/cropped/2024_09_12_d4f1c588dac32912a6a4g-04.jpg?height=650&width=1590&top_left_y=218&top_left_x=242)\n\nFigure 1: Preliminary study. (A) The validation BLEU of different models on IWSLT14 DE $\\rightarrow$ EN at different training steps. (B) Diffusion loss of DiffusionLM on the validation set of IWSLT14 DE $\\rightarrow$ EN at different noise scales and the distribution of noise scale sampled during training. (C) The accuracy of predicting $\\mathbf{z}_{0}$ from $\\mathbf{z}_{t}$ by finding the nearest neighbor for $\\mathbf{z}_{t}$ with different noise scales, vocabulary sizes $|\\mathcal{V}|$, and dimensions $D$. (D) An illustrative example of the distributions of $\\mathbf{z}_{t}$ of three data points corrupted with different noise scales as in Eqn. 1, where for small noise scales, a large proportion of the embedding space between modes (associated with tokens) remains vacant. (E) The tendency of whether the model prediction is more influenced by the source or target side information when fed with timestep correspond to different noise scales. In addition to the source condition $\\mathbf{x}$, we feed the model with $\\mathbf{z}_{t}^{\\prime}=\\mathbf{z}_{t}\\left(\\mathbf{y}^{\\prime}, t\\right)$ that is corrupted with a timestep-dependent noise $\\sigma(t)$ from a negative $\\mathbf{y}^{\\prime}$, which is different from the original (positive sample of) target sequence $\\mathbf{y}$. We compare the similarity between the model prediction $\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t}^{\\prime}, \\mathbf{x}, \\tau\\right)$ to the embedding of ground-truth $\\mathbf{z}_{0}(\\mathbf{y})$, and study to what degree the model prediction is governed by the source condition $\\mathbf{x}$ (via the embedding of the ground-truth $\\mathbf{z}_{0}(\\mathbf{y})$ as the proxy), or the target information $\\mathbf{y}^{\\prime}\\left(\\right.$ via $\\left.\\mathbf{z}_{t}^{\\prime}\\right)$ otherwise.",
    "dinoiser-2": "Observations.",
    "dinoiser-3": "Here are our findings. O1. DiffusionLM still falls short of conditional sequence learning. Fig. $1(\\mathrm{~A})$ shows the validation performance of the two models at different training steps, in which the performance of DiffusionLM still lags behind CMLM by a large margin, even taking many more steps before convergence. This shows that the performance and training efficiency of the vanilla diffusion-based sequence learner remain unsatisfactory. O2. Diffusion losses at small noise scales are unexpectedly small. DiffusionLM uniformly samples timesteps hence the corresponding noise scales during training. As shown in Fig. 1(B), we find that the magnitudes of diffusion losses approach almost zero for small noise scales, indicating that it is quite trivial to recover the corrupted embeddings under such circumstances. We conjecture that, combined with the illustrated example in Fig. 1(D), this is because there remain highly discrete modes for the embedding density such that any corrupted embedding is very likely to lie in a region with a small radius around the original token embedding. As a consequence, the more the modes of embeddings separate from each other the smaller the diffusion loss, which adheres to the following observation. O3. It becomes increasingly harder for the diffusion process to eliminate discreteness while the dimension of the embedding space scales up. Fig. 1(C) shows a surprisingly high accuracy of recovering corrupted embeddings that can be easily achieved by simply seeking the nearest neighbor when embedding dimensions enlarge, even at considerably large noise scales. This reveals that scaling embedding space leads to more severe discreteness, namely a curse of dimensionality. O4. On condition learning: larger noise scales calibrate diffusion models in taking into account more source conditional information during inference. We have seen that recovering embeddings corrupted with small noise scales is easy (O2), and if modes distribute separately, even finding the nearest neighbor is enough (O3). In Fig. 1(C), as the noise scale\n\nTable 1: Illustration of hallucinations of vanilla DiffusionLM on IWSLT14 DE $\\rightarrow$ EN translation task, along with DiNoISER's predictions, where the vanilla DiffusionLM generates inexplicable expressions that are irrelevant to the source condition whose meaning dramatically differs from the groud-truth target. | Source | Mit welchen worten w\u00fcrden sie ban beschreiben? |\n| :--- | :--- |\n| Reference | What are the words you would use to describe ban? |\n| DiffusionLM | In which words would you save ban? |\n| DINoISER | In which words would you describe ban? |\n\ndecreases, the prediction accuracy by finding the nearest neighbor increases and achieves almost $100 \\%$ under a threshold, which can be learned trivially even with little to no source conditions. This results in the hallucination as shown in Tab. 1, which is an unexpected consequence for conditional sequence generative models to yield output loyal to the input condition. To mitigate this, we quantitatively study the influence of noise scales on conditional reliance. As shown in Fig 1(E), we find that as the noise scales are larger, the model can predict more faithfully to source conditions. Concluding remarks. We summarize conclusions from the aforementioned observations along with suggestions for more plausible diffused conditional sequence learning:\nC1. We should not train on too small noise scales to circumvent the pitfall of discreteness. Both O 2 and O 4 show the negative influences of small noise scales on training that it leads to a not smooth embedding space with vast regions of low density between modes associated with tokens (O2). These regions can inevitably be sampled during inference ${ }^{2}$, thereby giving rise to error accumulation. Besides, it also impedes conditional learning (O4). To remedy this, probably a simple way is to eliminate the chance of training with small noise scales. C 2 . We need to determine the noise schedule according to the dimensionality of the embedding space. Fitting more complex datasets usually requires larger embedding dimensions. O3 indicates the criterion to distinguish large\n\n[^2]and small noise scales depends on the embeddings hence the complexity of the datasets. However, existing methods employ a fixed noise schedule for all embedding dimensions, which lacks scalability. This, therefore, demands a task-specific noise schedule to accommodate diverse datasets. C3. We could expose the model to larger noise scales for better source conditions leverage. O 4 suggests that the more corrupted the embeddings, the more difficult for the model to recover, thereby necessitating more reliance on source conditions. Accordingly, we may encourage trained diffusion models to care more about source conditions for free by post$h o c$ manipulating the noise to large ones. ## 4 DINOISER\n\nProvided the observations and postulates we discussed in $\\S 3$, we accordingly propose DINOISER, a simple yet effective method that improves diffusion models by manipulating noises for conditional sequence learning. The general principle of DINOISER is to determine the best-suited noise scales for both training and inference for conditional sequence generation. In a nutshell, as for training, we propose to eliminate the chance of training diffused sequence learners with small-scale noises so as to circumvent the aforementioned pitfall of discreteness in embedding space (\u00a74.1). As for sampling, we propose a new effective sampler to amplify the impact of source conditions on the model prediction, where timesteps corresponding to large noise scales are always fed into the model (\u00a74.2). We now dive deep into the details of DINOISER. ### 4.1 Noise Scale Clipping: Manipulating Noises for Counter-Discreteness Training\n\nRecall that C 1 and C 2 in $\\S 3$ demonstrate that small noises can barely help \"discrete\" embeddings populate the entire continuous space, and also undermine conditional learning. A simple yet effective way to mitigate this is to encourage training diffusion models with sufficiently large noise scales. As such, we propose noise scale clipping, where we bound the minimum noise scale $\\sigma_{\\min }$ for training such that only timesteps satisfying $\\sigma(t) \\geq \\sigma_{\\text {min }}$ could be sampled, which is decided adaptively as the model learn progresses. To start with, we can eliminate the scaling effect of $\\alpha(t)$ in the forward diffusion process for each\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d4f1c588dac32912a6a4g-06.jpg?height=193&width=783&top_left_y=192&top_left_x=245)\n(B)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d4f1c588dac32912a6a4g-06.jpg?height=290&width=632&top_left_y=412&top_left_x=309)\n\nFigure 2: (A) Illustration of the proposed noise scale clipping. To remedy the pitfall of discreteness, we propose to ensure a sufficiently large minimum \"overlap\" between corrupted embeddings. As shown in this example, such a goal of counter-discreteness is achieved by bounding the standard deviation $\\tilde{\\sigma}(t)$ of $\\operatorname{EMB}\\left(y_{i}\\right)$ by $\\delta_{i j}$ the \"distance\" to its nearest neighbor (i.e., $y_{j}$ ). (B) Comparison between sqrt noise schedule and our noise schedule $\\sigma(t)=t$ manipulated with the proposed noise scale clipping. token embedding by rewriting Eqn. 1 into:\n\n$$\n\\begin{gathered}\n\\frac{\\mathbf{z}_{t}[i]}{\\alpha(t)}=\\mathbf{z}_{0}+\\frac{\\sigma(t)}{\\alpha(t)} \\epsilon_{t} \\Rightarrow \\frac{\\mathbf{z}_{t}[i]}{\\alpha(t)} \\sim \\mathcal{N}\\left(\\mathbf{z}_{0}[i], \\frac{\\sigma^{2}(t)}{1-\\sigma^{2}(t)} \\mathbf{I}\\right) \\\\\n\\Rightarrow \\frac{\\mathbf{z}_{t}[i]}{\\alpha(t)} \\sim \\mathcal{N}\\left(\\operatorname{EMB}\\left(y_{i}\\right), \\tilde{\\sigma}^{2}(t) \\mathbf{I}\\right)\n\\end{gathered}\n$$\n\nAs illustrated in Fig. 2(A), there, intuitively, should exist a sufficiently large number $\\delta^{*}$ measuring the minimum \"overlap\" between the distributions of two corrupted embeddings under the Gaussian perturbation kernel with a standard deviation of $\\tilde{\\sigma}(t)=\\frac{\\sigma(t)}{\\sqrt{1-\\sigma^{2}(t)}}$. To this end, we let $\\delta^{2}$ be the minimum amount of variation of added noise, defined as the average squared L2-distances between the embeddings and their nearest neighbor, normalized by the dimension of embeddings (according to C 2 in $\\S 3$ ):\n\n$$\n\\begin{aligned}\n\\left(\\delta^{*}\\right)^{2} & =\\frac{1}{|\\mathcal{V}|} \\sum_{i=1}^{|\\mathcal{V}|} \\min _{1 \\leq j \\neq i \\leq|\\mathcal{V}|} \\delta_{i j}^{2} \\\\\n& =\\frac{1}{|\\mathcal{V}|} \\sum_{i=1}^{|\\mathcal{V}|} \\min _{1 \\leq j \\neq i \\leq|\\mathcal{V}|} \\frac{1}{D}\\left\\|\\operatorname{EMB}\\left(y_{i}\\right)-\\operatorname{EMB}\\left(y_{j}\\right)\\right\\|_{2}^{2}\n\\end{aligned}\n$$\n\nWe now define the noise scale clipping ${ }^{3}$ as follows:\n\n[^3]Definition (The noise scale clipping). Let $\\mathcal{V}$ be the target vocabulary with corresponding embeddings in $D$-dimensional space $\\forall y_{i} \\in \\mathcal{V}: \\operatorname{EMB}\\left(y_{i}\\right) \\in \\mathbb{R}^{D}$, the noise scale clipping is performed so that the noise scale $\\sigma(t)$ always satisfies:\n\n$$\n\\tilde{\\sigma}^{2}(t)=\\frac{\\sigma^{2}(t)}{1-\\sigma^{2}(t)} \\geq\\left(\\delta^{*}\\right)^{2} \\Rightarrow \\frac{\\sigma_{\\min }^{2}}{1-\\sigma_{\\min }^{2}}=\\left(\\delta^{*}\\right)^{2}\n$$\n\nthe clipping threshold $\\sigma_{\\min }$ is whereby derived when the equality in Eqn.",
    "dinoiser-4": "9 holds, such that\n$\\sigma_{\\min }=\\left(\\frac{|\\mathcal{V}| \\cdot D}{\\sum_{i=1}^{|\\mathcal{V}|} \\min _{1 \\leq j \\neq i \\leq|\\mathcal{V}|}\\left\\|\\operatorname{EMB}\\left(y_{i}\\right)-\\operatorname{EMB}\\left(y_{j}\\right)\\right\\|_{2}^{2}}+1\\right)^{-\\frac{1}{2}}$\nwhich is obtained by substituting Eqn.",
    "dinoiser-5": "8 into the R.H.S of Eqn. 9. satisfying $\\tilde{\\sigma}(t)>=\\delta^{*}$, cannot be discriminated from those originate from different embeddings, otherwise a smaller $\\tilde{\\sigma}(t)$ will lead to trivial reconstruction to the original one as a consequence of the pitfall of discreteness ( $\\mathrm{O} 2 \\& \\mathrm{O} 3$ in \u00a73). As a result, $\\delta^{*}$ serves as a minimum clipping threshold of noise scale for effective training of sequence diffusion models. This can closely relate to the minimum Word Mover Distance, a Wasserstein metric introduced in Kusner et al. (2015):\n\n$$\n\\delta^{2}=\\min _{\\mathbf{T}} \\sum_{i=1}^{L} \\mathbf{T}_{i j} \\delta_{i j}^{2}, \\quad \\text { s.t. } \\sum_{j} \\mathbf{T}_{i j}=d_{i}\n$$\n\nwhere $\\mathbf{T} \\in \\mathbb{R}^{L \\times L}$ is a (sparse) stochastic matrix, where $\\mathbf{T}_{i j}$ denotes how much of a word $i$ travels to word $j$, subject to the flow consistency equality $\\sum_{j} \\mathbf{T}_{i j}=d_{i}$, with $d_{i}$ representing the \"amount\" of a word $i$ appearing in token embedding space (we treat $d_{i}=1$ ). According to the Eqn. (2) in Kusner et al. (2015), under mild conditions, the optimal solution $\\mathbf{T}^{*}$ is for each token $i$ to move all its probability mass to the most similar token $j$ w.r.t. a certain measure of their embedding distances, $\\delta_{i j}=\\left\\|\\frac{\\mathbf{z}_{t}[i]}{\\alpha(t)}-\\operatorname{EMB}\\left(y_{j}\\right)\\right\\|_{2}$ :\n\n$$\n\\mathbf{T}_{i j}^{*}= \\begin{cases}d_{i} & \\text { if } j=\\arg \\min _{1 \\leq j \\neq i \\leq L} \\delta_{i j}^{2} \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nAs a result, the final minimum transportation cost becomes:\n\n$$\n\\begin{aligned}\n\\left(\\delta^{*}\\right)^{2} & =\\sum_{i=1}^{L} \\mathbf{T}_{i j}^{*} \\delta_{i j}^{2}=\\sum_{i=1}^{L} d_{i} \\cdot\\left(\\delta_{i j}^{2}\\right)^{*}=\\sum_{i=1}^{L} \\min _{1 \\leq j \\neq i \\leq L} \\delta_{i j}^{2} \\\\\n& =\\sum_{i=1}^{L} \\min _{1 \\leq j \\neq i \\leq L}\\left[\\left\\|\\frac{\\mathbf{z}_{t}[i]}{\\alpha(t)}-\\operatorname{EMB}\\left(y_{j}\\right)\\right\\|_{2}\\right]^{2}\n\\end{aligned}\n$$\n\nA too-small noise scales result in that the nearest neighbors of the corrupted embeddings are exactly their origins, thus\n\n$$\n\\left(\\delta^{*}\\right)^{2}==\\sum_{i=1}^{L}\\left\\|\\frac{\\mathbf{z}_{t}[i]}{\\alpha(t)}-\\operatorname{EMB}\\left(y_{i}\\right)\\right\\|_{2}^{2}=\\sum_{i=1}^{L}\\left(\\tilde{\\sigma}(t) \\epsilon_{\\mathbf{i}}\\right)^{2}\n$$\n\nwhere $\\epsilon_{\\mathbf{i}}$ are standard Gaussian noises. The above results contain no model parameters, indicating that a diffusion model, which learns to minimize the Wasserstein distance between prediction and target distribution (Kwon et al., 2022), can not learn from those mildly perturbed samples. From this perspective, our noise clipping tries to avoid training on these unhelpful samples. ```\nAlgorithm 1 Training with DINOISER\nInput Training dataset \\(\\mathcal{D}=\\{(\\mathbf{x}, \\mathbf{y})\\}\\). Output Optimized parameters \\(\\theta\\). repeat\n        Sample \\(\\mathbf{x}, \\mathbf{y}\\) from the dataset \\(\\mathcal{D}\\) and embed \\(\\mathbf{y}\\)\n        into \\(\\mathbf{z}_{0}\\)\n        \\(t \\sim \\mathcal{U}\\left(\\sigma^{-1}\\left(\\sigma_{\\min }\\right), 1\\right)\\), where \\(\\sigma_{\\min }\\) is from\n        Eqn. 10\n        Sample \\(\\mathbf{z}_{t}\\) with Gaussian reparameterization\n        (Eqn. 1)\n        Take gradient descent step on\n            \\(\\nabla_{\\theta}\\left[-\\log p_{\\theta}\\left(\\mathbf{y} \\mid \\mathbf{z}_{0}\\right)+\\left\\|\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t}, \\mathbf{x}, t\\right)-\\mathbf{z}_{0}\\right\\|_{2}^{2}\\right]\\)\n    until converged\n```\n\nAs illustrated in Fig. 2(B), the clipping threshold $\\sigma_{\\min }$ is estimated adaptively during training, depending on how properly the model learns the embeddings up to the minimum pair-wise distances within the vocabulary. In each training step, we first estimate the clipping threshold $\\sigma_{\\text {min }}$ with Eqn. 10, then sample timesteps among $t$ that satisfies $\\sigma(t)>\\sigma_{\\min }$. In practice, one can first estimate the noise scale threshold $\\sigma_{\\min }$ and then turn it into the timestep threshold $t_{\\min }=\\sigma^{-1}\\left(\\sigma_{\\min }\\right)$ in general. In this work, we select $\\sigma(t)=t$ as the noise scheduler to simplify this procedure ${ }^{4}$. As a result, the updated diffusion loss with an enlarged minimum timestep threshold (thus an increased minimum noise scale) in the final training objective (modified from Eqn. 6) now becomes:\n\n$$\n\\mathcal{L}_{\\text {diffusion }}^{\\prime}(\\mathbf{y}) \\underset{t \\sim \\mathcal{U}\\left(t_{\\min }, 1\\right), \\epsilon_{t}}{\\mathbb{E}}\\left[\\left\\|\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t}, \\mathbf{x}, t\\right)-\\mathbf{z}_{0}\\right\\|_{2}^{2}\\right]\n$$\n\nWe provide pseudocodes regarding how to manipulate noises in training as such in Alg. 1. ### 4.2 CEDI: Manipulating Noises for Condition-Enhanced Sampling\n\nBased on C3 in \u00a73, we suppose the model relies more on the source conditions when the input noise scale is large. This implies that we may make the model generate prediction more faithful to source conditions by feeding timesteps corresponding to large noise scales to the model. Fig. 3 shows a synthesis experiment similar to Fig. 1(E), wherein the predictions using a large timestep 0.995 (namely, a larger noise scale) are closer to the embedding\n\n[^4]![](https://cdn.mathpix.com/cropped/2024_09_12_d4f1c588dac32912a6a4g-07.jpg?height=384&width=766&top_left_y=208&top_left_x=1062)\n\nFigure 3: A synthesis experiment where the model is asked to predict with current timestep $\\tau=t$ and an alternative larger timestep $\\tau=0.995$, respectively. We compare the MSE between the model prediction $\\mathbf{z}_{\\theta}\\left(\\mathbf{z}_{t}^{\\prime}, \\mathbf{x}, \\tau\\right)$ to the embedding of ground-truth $\\mathbf{z}_{0}(\\mathbf{y})$ (top) and negative sample $\\mathbf{z}_{0}\\left(\\mathbf{y}^{\\prime}\\right)$ (bottom) respectively, and study to which target the model prediction assimilates, the original $y$ or the negative one $\\mathbf{y}^{\\prime}$, hence should most likely be governed by the source or the target information. of the original target $y$, while more distant to that of the misleading $\\mathbf{y}^{\\prime}$, reiterating that the model relies more on the source condition $\\mathbf{x}$ when being exposed to a larger noise due to manipulation in inference. Accordingly, we propose a condition-enhanced denoiser (CEDI) for sampling. CEDI always feeds a large $t$ to the model $\\mathbf{z}_{\\theta}$ to encourage the model to make use of the source condition. In practice, we largely follow the framework of DDIM solver (Song et al., 2020a) but pick two sets of timesteps. In the first set $\\left\\{t_{i}\\right\\}_{i=0}^{M}$, timesteps decrease uniformly from $t_{0}=1$ to $t_{M} \\approx 0$ as normal. As for the other set $\\left\\{\\tau_{i}\\right\\}_{i=0}^{M}, \\tau_{i}$ s decrease uniformly from $\\tau_{0}=1$ to a large time $\\tau_{M} \\gg 0^{5}$. When making predictions, we assign timesteps from the second set to the model. By replacing corresponding timesteps in the framework of DDIM (Eqn. 4 and Eqn. 5) with $\\tau_{i}$, we generate our predictions by iteratively computing\n$\\hat{\\mathbf{z}}_{t_{i}}=\\alpha\\left(t_{i}\\right) \\mathbf{z}_{\\theta}\\left(\\hat{\\mathbf{z}}_{t_{i-1}}, \\mathbf{x}, \\tau_{i-1}\\right)+\\sigma\\left(t_{i}\\right) \\epsilon_{\\theta}\\left(\\hat{\\mathbf{z}}_{t_{i-1}}, \\mathbf{x}, \\tau_{i-1}\\right)$,\nwhere the predicted noise is also updated as\n$\\epsilon_{\\theta}\\left(\\hat{\\mathbf{z}}_{t_{i-1}}, \\mathbf{x}, \\tau_{i-1}\\right)=\\frac{\\hat{\\mathbf{z}}_{t_{i-1}}-\\alpha\\left(\\tau_{i-1}\\right) \\mathbf{z}_{\\theta}\\left(\\hat{\\mathbf{z}}_{t_{i-1}}, \\mathbf{x}, \\tau_{i-1}\\right)}{\\sigma\\left(\\tau_{i-1}\\right)}$.",
    "dinoiser-6": "We also demonstrate how CEDI works in Alg.",
    "dinoiser-7": "2. ## 5 Experiment\n\nWe conduct experiments to verify the effectiveness of the DINOISER and study its characteristics. [^5]```\nAlgorithm 2 Sampling with DINOISER\nInput Source condition \\(\\mathbf{x}\\); number of steps \\(M\\); model\nparameters \\(\\theta\\). Output Predicted target \\(\\hat{\\mathbf{y}}\\). Uniformly discretize \\([T, 1]\\) into \\(M+1\\) steps \\(\\left\\{t_{i}\\right\\}_{i=0}^{M}\\)\n        in descend order \\((T \\approx 0)\\)\n    Uniformly discretize \\([\\mathcal{T}, 1]\\) into \\(M+1\\) steps\n    \\(\\left\\{\\tau_{i}\\right\\}_{i=0}^{M}\\) in descend order \\((\\mathcal{T} \\gg 0)\\)\n    Sample \\(\\hat{\\mathbf{z}}_{t_{0}} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\)\n    for \\(i=1\\) to \\(M\\) do\n        \\(\\hat{\\mathbf{z}}_{0} \\leftarrow \\mathbf{z}_{\\theta}\\left(\\hat{\\mathbf{z}}_{t_{i-1}}, \\mathbf{x}, \\tau_{i-1}\\right)\\)\n        \\(\\hat{\\epsilon} \\leftarrow \\frac{\\hat{\\mathbf{z}}_{t_{i-1}}-\\alpha\\left(\\tau_{i-1}\\right) \\hat{\\mathbf{z}}_{0}}{\\sigma\\left(\\tau_{i-1}\\right)}\\)\n        \\(\\hat{\\mathbf{z}}_{t_{i}} \\leftarrow \\alpha\\left(t_{i}\\right) \\hat{\\mathbf{z}}_{0}+\\sigma\\left(t_{i}\\right) \\hat{\\epsilon}\\)\n    end for\n    Map \\(\\hat{\\mathbf{z}}_{t_{M}}\\) to \\(\\hat{\\mathbf{y}}\\) with the embeddings\n```\n\n\n### 5.1 Experimental Setup\n\nTasks and Datasets. We mainly experiment on machine translation, a well-established benchmark task for conditional sequence learning. We consider IWSLT14 DE $\\leftrightarrow$ EN (160K pairs), WMT14 EN $\\leftrightarrow$ DE (4.0M pairs), and WMT 14 EN $\\leftrightarrow$ Ro ( 610 K pairs), six translation tasks with variant sizes of training data. Additionally, we experiment on two of the datasets introduced by DiffuSeq (Gong et al., 2022), including Wiki (Jiang et al., 2020) for text simplification and $\\mathrm{QQP}^{6}$ for paraphrasing. Baselines. We include three groups of baselines for machine translation: (1) The autoregressive Transformer (Vaswani et al., 2017); (2) The CMLM (Ghazvininejad et al., 2019), an iterativebased non-autoregressive model for conditional sequence learning. (3) Previous diffusion-based sequence generative models, including the vanilla design that simply extends the original DiffusionLM (Li et al., 2022) with an additional condition encoder, and the other recently proposed improved methods CDCD (continuous diffusion for categorical data, Dieleman et al., 2022), DiffuSeq (Gong et al., 2022), SeqDiffuSeq (Yuan et al., 2022) and Difformer (Gao et al., 2022). For text simplification and paraphrasing, we compare our method with DiffuSeq (Gong et al., 2022). Metrics. We primarily report SacreBLEU ${ }^{7}$ (Post, 2018) for machine translation, following\n\n[^6]CDCD (Dieleman et al., 2022). For text simplification and paraphrasing, we follow DiffuSeq to employ sentence-level BLEU under the tokenizer of BERT-BASE-UNCASED. Implementation. All our implementations are based on Transformer-BASE (Vaswani et al., 2017) for all datasets except IWSLT14. For IWSLT14, we use a smaller architecture that has 4 attention heads and 1024-dimensional feedforward layers. The embedding dimension for the diffusion model is 16 on IWSLT14 and 64 on the others. In the implementation of our method, we follow recent advances and apply self-conditioning techniques (Dieleman et al., 2022; Chen et al., 2022; Strudel et al., 2022). Besides, following previous practice in non-autoregressive machine translation, we train our model both with and without knowledge distillation ${ }^{8}$ (KD, Kim and Rush, 2016; Zhou et al., 2020). During inference, for machine translation, we apply beam search in the autoregressive Transformer with beam size 5 . Correspondingly, we use length beam 5 in the non-autoregressive models, except for CDCD and DiffuSeq since they vary the target lengths by predicting paddings instead of length predictions. For text simplification and paraphrasing, we report results with various length beams as length prediction on these tasks is more challenging and less studied. For all the diffusion-based methods, we follow previous work (Li et al., 2022; Gong et al., 2022; Dieleman et al., 2022) and apply Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). For both DiffusionLM and our model, we perform sampling with 20 steps. We implement DiffusionLM and DiNOISER upon fairseq (Ott et al., 2019), and also train Transformer and CMLM baselines using fairseq. The training batch size is 128 K for WMT14/WMT16, and 32K for the others. For more details, please refer to $\\S \\mathrm{B}$. ### 5.2 Main Results\n\nThe results of machine translation and the other two tasks are in Tab. 2 and Tab. 3, respectively. [^7]Table 2: Comparison in SacreBLEU on machine translation tasks. \"LB\": the size of the length beam search. \"MBR\": the number of candidates for each length beam to apply Minimum Bayes-Risk decoding. \"KD\": results are obtained with knowledge distillation (KD, Kim and Rush, 2016; Zhou et al., 2020). Provided that KD is common and effective practice in non-autoregressive (NAR) machine translation, though not the focus of this study, we also provide further experiments with KD for reference. The best NAR results with and without KD are in bold and the second best ones are underlined. We report $95 \\%$ confidential interval for our method computed with compare-mt (Neubig et al., 2019). $\\dagger$ : how CMLM originally selects candidates with different lengths differs from the MBR decoding we used for diffusion models, and we thus include its results with MBR decoding for fair comparisons. $\\ddagger:$ the results are quoted from Dieleman et al. (2022). $\\ddagger$ : the results are quoted from Gao et al. (2022) while the results of the rest datasets are missing in the original paper, for which we obtain through their opensource code. Note that the results of DiffuSeq and SeqDiffuSeq are presented in tokenized BLEU as reported in Gao et al. (2022), and we encourage readers to check the original papers for more details. | Methods | IWSLT14 |  | WMT14 |  | WMT16 |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $\\mathrm{DE} \\rightarrow$ En | EN $\\rightarrow$ DE | $\\mathrm{DE} \\rightarrow$ EN | EN $\\rightarrow$ DE | $\\mathrm{Ro} \\rightarrow$ En | EN $\\rightarrow$ Ro |\n| Transformer (Vaswani et al., 2017) (AR, beam $=5)$ | 33.61 | 28.30 | 30.55 | 26.85 | 33.08 | 32.86 |\n| CMLM (Ghazvininejad et al., 2019) (NAR, LB $=5$ ) | 29.41 | 24.33 | 28.71 | 23.22 | 31.13 | 31.26 |\n| CMLM (Ghazvininejad et al., 2019) (NAR, $\\mathrm{LB}=5, \\mathrm{MBR}=1^{\\dagger}$ ) | 29.32 | 24.34 | 28.43 | 23.09 | 31.07 | 30.92 |\n| DiffusionLM (Li et al., 2022) $(\\mathrm{LB}=5, \\mathrm{MBR}=1)$ | 26.61 | 20.29 | 17.31 | 15.33 | 28.61 | 27.01 |\n| DiffusionLM (Li et al., 2022) $(\\mathrm{LB}=5, \\mathrm{MBR}=10)$ | 29.11 | 22.91 | 19.69 | 17.41 | 30.17 | 29.39 |\n| CDCD (Dieleman et al., 2022) $(\\mathrm{MBR}=10)$ | - | - | $25.40^{\\ddagger}$ | $19.70^{\\ddagger}$ | - | - |\n| CDCD (Dieleman et al., 2022) $\\quad(\\mathrm{MBR}=100)$ | - | - | $26.00^{\\ddagger}$ | $20.00^{\\ddagger}$ | - | - $\\square$ |\n| Difformer (Gao et al., 2022) $(\\mathrm{LB} \\times \\mathrm{MBR}=20)$ | 28.01 | 23.31 | 25.30 | $23.80^{\\text {\u5a04 }}$ | 29.37 | 29.20 |\n| DINOISER $(\\mathrm{LB}=5, \\mathrm{MBR}=1)$ | $31.29_{0.67}$ | $25.55_{0.65}$ | $28.83_{0.92}$ | $24.25_{0.86}$ | $31.14_{1.13}$ | $30.93_{1.12}$ |\n| DINOISER $(\\mathrm{LB}=5, \\mathrm{MBR}=10)$ | 31.61 ${ }^{3.67}$ | $\\underline{25.70 ~}^{25.62}$ | $\\mathbf{2 9 . 0 5}_{0.92}$ | $\\underline{24.26}^{24.26 .84}$ | $31.22_{1.15}$ | $\\underline{31.08}_{1.12}$ |\n| DiNOISER ( $\\mathrm{LB}=10, \\mathrm{MBR}=5$ ) | $31.44_{0.68}$ | $20^{26.14}{ }_{0.65}$ | $\\underline{29.01 ~}_{0.88}$ | $\\mathbf{2 4 . 6 2}_{0.88}$ | $3^{31.24}{ }_{1.12}$ | $31.03_{1.13}$ |\n| DiffuSeq (Gong et al., 2022) $\\quad($ KD, $\\mathrm{LB} \\times \\mathrm{MBR}=10)$ | - | - | - | $15.37^{\\text {\u7c73 }}$ | - | $25.45^{\\ddagger}$ |\n| SeqDiffuSeq* ${ }^{\\text {( }}$ (Yuan et al., 2022) $($ (KD, LB $\\times$ MBR $=10)$ | - | - | - | $17.14^{\\ddagger}$ | - | $26.17^{\\text {\u4e8b }}$ |\n| DINOISER (KD, $\\mathrm{LB}=10, \\mathrm{MBR}=5$ ) | - | - | $\\mathbf{3 0 .",
    "dinoiser-8": "3 0}_{0.94}$ | $\\mathbf{2 5 .",
    "dinoiser-9": "8 8}_{0.95}$ | $\\mathbf{3 3 . 1 3}_{1.20}$ | 32.84 ${ }_{1.16}$ |\n\nOverall performance. Our DINOISER demonstrates effectiveness on all selected conditional sequence learning tasks, which we summarize into the following three aspects:\n\n- DINOISER achieves state-of-the-art results among diffusion-based models on one of the representative conditional sequence generation tasks, i.e., machine translation, where DINOISER outperforms the vanilla design of DiffusionLM, as well as the previous strongest approaches such as CDCD (Dieleman et al., 2022) and Difformer (Gao et al., 2022) by a large margin (Tab. 2). For DiffuSeq and SeqDiffuSeq, although their reported tokenized BLEUs are not strictly comparable to our SacreBLEU results due to the difference in tokenizers, our performance is far above them by over 4 BLEU score and even more if we involve knowledge distillation, which supports our superiority over them. - DINOISER demonstrates strong competitiveness in conditional sequence learning. It even surpasses CMLM (Ghazvininejad et al., 2019) on almost all the experimented machine translation datasets (Tab. 2). Provided that CMLM is one of the leading approaches among NAR sequence learners, the performance DINOISER achieves can be considered quite competitive. - DINOISER is generic to various conditional sequence learning tasks. Results on Tab. 3 shows that DINOISER also works well in tasks other than machine translation, surpassing previously proposed DiffuSeq. In addition to the overall performance, DINOISER also demonstrates several nice properties.",
    "dinoiser-10": "We elaborate on them as follows:\n\nScalability. As shown in Tab. 2, DiffusionLM seems more challenging to accommodate larger datasets like WMT14 than smaller ones (e.g., IWSLT14). This verifies the curse of scalability problem discussed in \u00a73. In contrast, DINOISER surpasses CMLM on almost all large- and smallscale scenarios, which indicates that DINOISER indeed greatly improves the scalability of diffusionbased sequence learners. This advantage of DINOISER could help facilitate further research of large-scale real-world applications of diffused sequence generative models. Table 3: Sentence-level BLEU of our method and DiffuSeq on Wiki (text simplification) and QQP (paraphrasing). \"NFE\": number of function evaluations, measuring the total number of forward passes to the model for each prediction. The results of DiffuSeq are quoted from Gong et al. (2022). | Methods | Steps | LB | MBR | NFE | Wiki | QQP |\n| :--- | ---: | ---: | ---: | ---: | :---: | :---: |\n| DiffuSeq | 2000 | - | 10 | 20000 | 36.22 | 24.13 |\n| DINOISER | 20 | 10 | 1 | 200 | $35.36_{1.63}$ | $\\mathbf{2 6 . 0 7}_{1.24}$ |\n| DINOISER | 20 | 20 | 1 | 400 | $\\mathbf{3 6 . 9 4}_{1.95}$ | $25.42_{1.46}$ |\n| DINOISER | 20 | 20 | 5 | 2000 | $36.88_{1.77}$ | $25.57_{1.29}$ |\n\nSampling efficiency. Given the sizes of length beam (LB) and MBR decoding shown in Tab. 2, DINOISER surpasses or closely approaches CMLM even when MBR=1, while the vanilla DiffusionLM heavily relies on a large number of candidates for MBR decoding. Besides, DINOISER necessitates much fewer NFEs to achieve strong performance, e.g., only 20 steps, resulting in only $1 \\%$ to $10 \\%$ computational costs and latency compared to previous works (Gong et al., 2022; Li et al., 2022; Dieleman et al., 2022). This manifests that DINOISER is more accurate yet efficient compared to previous diffusion-based sequence learning models. ### 5.3 Effect of Noise Scale Clipping for Training\n\nAblation study on noise clipping. We compare models trained with different settings in Tab. 4 to study the effect of our training strategy. We find that the proposed noise scale clipping consistently helps improve the model performance. Replacing the noise schedule in DINOISER (i.e., $\\sigma(t)=t$ ) with the sqrt schedule proposed by Li et al. (2022) has negligible influence on the final performance. This is expected since we only set the noise schedule as $\\sigma(t)=t$ for convenience. The difference is that the improvement by noise clipping is relatively smaller when using the sqrt schedule. This is because the noise scale of the sqrt schedule increases quickly in small timesteps (Fig. 2B). When $t=0.2, \\sigma_{\\text {sqrt }}(t) \\approx 0.67$, which is close to our initial clipping threshold. This suggests the success of the sqrt schedule may also be partly explained as involving more large-scale noises. On the effect of different noise scale clipping thresholds. To understand how different noise clipping thresholds affect the model performance, we compare our model trained with adaptive noise threshold and different fixed noise thresholds in Fig. 4. Results show that the performance degrades if the clipping threshold is either too small or too large. Our proposed strategy adaptively finds the\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d4f1c588dac32912a6a4g-10.jpg?height=473&width=626&top_left_y=209&top_left_x=1115)\n\nFigure 4: SacreBLEU on IWSLT14 DE $\\rightarrow$ EN with our models trained with adaptive $v s$.",
    "dinoiser-11": "different fixed noise clipping thresholds. We sample results with $\\mathrm{MBR}=5$ and oracle length. The star marker ( $\\star$ ) stands for the clipping threshold of our final checkpoint trained with adaptive clipping threshold.",
    "dinoiser-12": "balance well. It finds the ideal noise clipping threshold and achieves strong performance. ### 5.4 Effect of Condition Enhancement for Sampling\n\nWe compare the performance between different denoisers, i.e., DDIM and the proposed CEDI, in Tab. 4. In a nutshell, CEDI impressively outperforms DDIM, especially for small MBR candidate sizes. We also notice that DDIM performs particularly unsatisfactorily when the model is trained without noise scale clipping. However, for these models, CEDI can still produce a relatively good performance (over 20.50 for $\\mathrm{MBR}=1$ ) that even surpasses well-designed CDCD (20.00 for MBR=100). What's more, we highlight two critical characteristics of CEDI as follows:\nCEDI indeed better leverage source conditions for inference. Recall that we propose the CEDI with the purpose of encouraging the model to make better use of source conditions for prediction (\u00a74.2). To investigate whether the denoiser achieves this, we apply Layer-wise Relevant Propagation (LRP, Bach et al., 2015; Voita et al., 2021) to measure the relative contribution of the source condition to the model prediction. As shown in Fig. 5(B), we compare the source contribution of our CEDI and the DDIM solver along the sampling iterations. CEDI maintains a high source contribution, while the source contribution of CEDI is unsatisfactory in the first few steps, which demonstrates that sampling with our CEDi does leverage the source condition more. Correspondingly, as shown in Fig. 5(A), the prediction accuracy of CEDI increases steadily, while the performance of DDIM fails to improve\n\nTable 4: Ablation Study on WMT14 En $\\rightarrow$ DE. All the results are in SacreBLEU scores with $L B=5$. | Training Settings | DDIM $(\\mathrm{MBR}=1)$ | CEDI $(\\mathrm{MBR}=1)$ | DDIM $(\\mathrm{MBR}=10)$ | CEDI $(\\mathrm{MBR}=10)$ |\n| :--- | :---: | :---: | :---: | :---: |\n| Ours [final] | 19.23 | 24.25 | 22.12 | 24.26 |\n| w/o self-conditioning | 20.37 | 23.03 | 22.58 | 23.14 |\n| w/o noise scale clipping | 7.95 | 21.16 | 11.51 | 21.40 |\n| w/o self-conditioning, w/o noise scale clipping | 11.30 | 20.86 | 14.84 | 21.47 |\n| w/ sqrt noise schedule | 20.11 | 24.13 | 22.83 | 24.07 |\n| w/ sqrt noise schedule, w/o noise scale clipping | 16.68 | 23.22 | 20.46 | 23.40 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d4f1c588dac32912a6a4g-11.jpg?height=512&width=738&top_left_y=589&top_left_x=248)\n\nFigure 5: The difference between CEDI and DDIM solver over steps.",
    "dinoiser-13": "(A) The prediction accuracy at each step, measured with SacreBLEU. (B) The proportion of source contribution to the prediction in Layer-wise Relevant Propagation (LRP) at each step. at the beginning of the iterations, suggesting correlations between higher source contribution and higher performance improvement. Among all iterations, the first few steps establish the foundation for the overall performance. Although DDIM improves its performance in later iterations, it still falls behind our CEDI. This suggests the effectiveness of increasing the source contribution, especially at the beginning of the sampling process. To further show the strength of CEDI in capturing source conditions, we explore more complex conditional sequence learning scenarios. We simulate this under two multilingual translation settings, i.e. many-to-one and one-to-many translation. In the many-to-one scenario, a unified model needs to translate source sentences in multiple different source languages to English counterparts, requiring the model to handle complicated source conditions. On the other hand, the one-to-many setting simulates a multiconditional scenario, requiring the model to recognize the target language as another crucial condition to capture the target distribution. To this end, we construct a dataset by combining four language pairs of IWSLT14 translation benchmark, i.e., $\\mathrm{EN} \\leftrightarrow \\mathrm{DE}, \\mathrm{EN} \\leftrightarrow \\mathrm{Ro}, \\mathrm{EN} \\leftrightarrow \\mathrm{NL}$, and $\\mathrm{EN} \\leftrightarrow \\mathrm{Pt}-$ BR. In the one-to-many translation, we append language tokens to the source sequences to incor-\nTable 5: Results of multilingual machine translation ( $\\{$ De,Ro,Pt-BR,NL $\\} \\leftrightarrow E N$ ).",
    "dinoiser-14": "\"BilinGUAL\": integrated results of separate models of every language pair. \"Multiling.\": results from a unified multilingual model. We employ langdetect (Shuyo, 2010) to infer the language of generated sequences for computing the language accuracy. | Settings | Methods | SacreBLEU (Lang Acc \\%) |  |\n| :---: | :---: | :---: | :---: |\n|  |  | BilinguAL | Multiling. |\n|  | CMLM | 33.85 | 35.23 |\n| many-to-one | DINOISER (DDIM, MBR=1) | 32.40 | 33.43 |\n| \\{De,Ro,Pt-br,Nl\\} | DINOISER (DDIM, MBR=10) | 33.73 | 35.48 |\n| $\\rightarrow$ EN | DINOISER $(\\mathrm{MBR}=1)$ | 34.57 | 35.26 |\n|  | DINOISER $(\\mathrm{MBR}=10)$ | 34.74 | 35.66 |\n| one-to-many | CMLM | 28.10 | $30.55(94.73)$ |\n|  | DINOISER (DDIM, MBR=1) | 27.44 | $17.95(89.77)$ |\n| $\\mathrm{EN} \\rightarrow$ <br> $\\{\\mathrm{DE}, \\mathrm{Ro}, \\mathrm{PT}-\\mathrm{Br}, \\mathrm{NL}\\}$ | DINOISER (DDIM, MBR=10) | 28.54 | $18.57(89.53)$ |\n|  | DINOISER $(\\mathrm{MBR}=1)$ | 28.72 | 30.52 (95.31) |\n|  | DINOISER $(\\mathrm{MBR}=10)$ | 28.81 | 30.67 (95.42) |\n\nporate the target language as a condition. We also include a baseline in which the models are trained separately for each language pair for comparison. CEDI can handle sequence generation from complex and multiple conditions. As shown in Tab. 5, DiNOISER works well in multilingual settings, showing its strong capability in modeling conditions, i.e. a complex multimodal condition (source sentences of four languages in many-to-one), and multiple conditions (source sentence in English as well as the identity of target languages). Particularly, CEDI shows huge advantages over DDIM in the multilingual setting of one-to-many translation. In this case, the language accuracy of DDIM is much lower than that of CEDI, suggesting DDIM has trouble capturing the given condition, namely the language identity in this one-to-many scenario. In contrast, DINOISER augmented with CEDI yields satisfactory predictions with high language accuracy, exhibiting superiority in working with multiple conditions. This is also consistent with our findings from the qualitative examples as shown in Tab. 6, where DDIM may fail to capture the language condition and generate non-sense articles shared across languages, while the full DINOISER produces fluent and satisfactory results. Table 6: A quanlitative example for one-to-many translation.",
    "dinoiser-15": "The source contains both the English sentence to be translated and the target language. We compare generation results of CMLM, DINOISER but sampling with DDIM instead of CEDI, and the complete DINOISER. | Source | (target language: ro) something as dramatic as our identity has now become a matter of choice, as this slide is meant to indicate. |\n| :--- | :--- |\n| Reference | ceva at\u00e2t de important ca identitatea noastr\u0103 a devenit acum o problem\u0103 de alegere, \u0219i aceast\u0103 tranzi\u021bie are rolul de a ar\u0103ta ast |\n| CMLM | ceva la fel de dramatic ca identitatea noastr\u0103 a devenit o problem\u0103 de alegere, cum acest slide este f\u0103cut s\u0103 arate. |\n| DINOISER (w/ DDIM) | ceva de de de de de de de de de a de de de de de de de de de de de de de de de de. |\n| DINOISER (w/ CEDI) | ceva at\u00e2t de dramatic ca identitatea noastr\u0103, a devenit acum o problem\u0103 de alegere, a\u015fa cum se \u00eenseamn\u0103 s\u0103 indice acest imagine |\n\n## 6 Related Work\n\nNon-autoregressive Sequence Generative Models.",
    "dinoiser-16": "Non-autoregressive sequence learning (NAR) was first proposed by Gu et al. (2018) as an alternative to its autoregressive counterpart. It generates target tokens in parallel, either fully NAR (Gu et al., 2018) or up to a mild number of iterations (Ghazvininejad et al., 2019), liberating sequence modeling from the constraint of a predefined order (Qian et al., 2022). With recent efforts, NAR shows great potential in the applications of various domains, including language ( Qian et al., 2021; Qi et al., 2021; Huang et al., 2022c; Qian et al., 2022), speech (Kim et al., 2021), proteins (Zheng et al., 2023a; Wang et al., 2024), and molecules (Hoogeboom et al., 2022). Different from more commonly-used autoregressive (AR) models (Sutskever et al., 2014), NAR models assume conditional independence among the output tokens. Such an assumption risks ignoring the target dependencies (Ren et al., 2020; Huang et al., 2022b) and leads to the multi-modality problem (Gu et al., 2018). As a result, the vanilla fully NAR model has inferior generation quality. Some of the later improvements alleviate the strong assumption by reformulating NAR formulation under iterative refinement (Lee et al., 2018; Gu et al., 2019; Ghazvininejad et al., 2019, 2020; Huang et al., 2022a,d; Zheng et al., 2023b; Ye et al., 2023), which iteratively takes as input the previously generated sequence, which serves as an intermediate random variable, to produce the tokens of its refined or denoised version in parallel until convergence or the budget of maximum iterations run out. Some recent advances herein follow the idea of discrete diffusion (Sohl-Dickstein et al., 2015; Austin et al., 2021) and formalize iterative refinement as Markov processes (Savinov et al., 2021; He et al., 2022; Reid et al., 2022). Although both are named after diffusion models, these works operate on discrete state space, whereas our focus, continuous diffusion models accommodate the continuous (embedding) space of discrete tokens. Diffusion Models for Sequence Learning. Continuous diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b) gained first success in generating high-quality images. Recently, DiffusionLM (Li et al., 2022) successfully adapted them to sequence learning and proposed the DiffusionLM, the first diffusion-based sequence generative model with a special focus on controllable text generation. Later improvements to the diffusion-based sequence generative models are mainly categorized threefold. The first line includes novel components for diffusion modeling, such as the partial diffusion process proposed by DiffuSeq (Gong et al., 2022), selfconditioning techniques introduced by Strudel et al. (2022), and the adaptive noise schedule of Yuan et al. (2022). The second line applies diffusion models to the latent space of specific pretrained language models (Lovelace et al., 2022). And the third tries to incorporate conventional practice in discrete token prediction. For instance, CDCD (Dieleman et al., 2022), Difformer (Gao et al., 2022) and SSD (Han et al., 2022) incorporate the cross-entropy objectives in training. For the application of diffusion-based models for sequence learning, previous work found their advantages in controllable generation (Yu et al., 2022; Liu et al., 2022; Li et al., 2022), and generating diverse sequences (Gong et al., 2022). GENIE (Lin et al., 2022) demonstrates that diffusion-based sequence generative models can benefit from largescale self-supervised pretraining. While almost all these works mainly focus on the training phrase of diffusion-based sequence generative models, our study emphasizes both training and inference. ## 7 Conclusion\n\nIn this paper, we shed light on the crucial role of noise schedules in diffusion models for conditional sequence learning by systematic empirical study. Motivated by our findings, we propose DiNOISER to determine the best-suited noise scales for both training and inference. As a result, DiNOISER\nmakes training more effective and also enables the model to better utilize source conditions for prediction, thereby leading to considerable performance improvements. We expect that our study can help facilitate further research on diffusion models to empower various applications in NLP. ## Acknowledgement\n\nWe would like to thank the anonymous reviewers and editors for their invaluable feedback. ## References\n\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2021. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981-17993. Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. 2015. On pixelwise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. 2023. One transformer fits all distributions in multi-modal diffusion at scale. Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit3: Web inventory of transcribed and translated talks. In Conference of european association for machine translation, pages $261-268$. Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. 2022. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202. Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. 2022. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089. Zhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang Zhang, Jiang Bian, and Linli Xu. 2022. Difformer: Empowering diffusion model on embedding space for text generation. arXiv preprint arXiv:2212.09412. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019. Mask-predict: Parallel decoding of conditional masked language models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 6112-6121. Marjan Ghazvininejad, Omer Levy, and Luke Zettlemoyer. 2020. Semi-autoregressive training improves mask-predict decoding. arXiv preprint arXiv:2001.08785.",
    "dinoiser-17": "Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu , and LingPeng Kong. 2022. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933. Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. 2018. Nonautoregressive neural machine translation. In International Conference on Learning Representations. Jiatao Gu and Xiang Kong. 2021. Fully non-autoregressive neural machine translation: Tricks of the trade. In Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 120-133.",
    "dinoiser-18": "Jiatao Gu, Changhan Wang, and Junbo Zhao. 2019. Levenshtein transformer. Advances in Neural Information Processing Systems, 32. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. 2022. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432.",
    "dinoiser-19": "Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. 2022. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,\n\nDiederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. 2022. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840-6851. Curran Associates, Inc. Emiel Hoogeboom, Victor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. 2022. Equivariant diffusion for molecule generation in 3d.",
    "dinoiser-20": "In International Conference on Machine Learning, pages $8867-8887$. PMLR. Chenyang Huang, Hao Zhou, Osmar R Za\u00efane, Lili Mou, and Lei Li. 2022a. Non-autoregressive translation with layer-wise prediction and deep supervision. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10776-10784. Fei Huang, Tianhua Tao, Hao Zhou, Lei Li, and Minlie Huang. 2022b. On the learning of nonautoregressive transformers. In International Conference on Machine Learning, pages 93569376. PMLR. Fei Huang, Hao Zhou, Yang Liu, Hang Li, and Minlie Huang. 2022c. Directed acyclic transformer for non-autoregressive machine translation. In International Conference on Machine Learning, pages $9410-9428$. PMLR. Xiao Shi Huang, Felipe Perez, and Maksims Volkovs. 2022d. Improving non-autoregressive translation models without distillation. In International Conference on Learning Representations. Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. 2020. Neural crf model for sentence alignment in text simplification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943-7960. Jaehyeon Kim, Jungil Kong, and Juhee Son. 2021. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech.",
    "dinoiser-21": "In International Conference on Machine Learning, pages 5530-5540. PMLR. Yoon Kim and Alexander M Rush. 2016. Sequence-level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317-1327. Diederik P Kingma and Max Welling. 2013. Autoencoding variational bayes. arXiv preprint arXiv:1312.6114. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. 2020. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations. Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. Technical report, JOHNS HOPKINS UNIV BALTIMORE MD CENTER FOR LANGUAGE AND SPEECH PROCESSING (CLSP). Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to document distances. In International conference on machine learning, pages 957-966. PMLR. Dohyun Kwon, Ying Fan, and Kangwook Lee. 2022. Score-based generative modeling secretly minimizes the wasserstein distance. $A d$ vances in Neural Information Processing Systems, 35:20205-20217. Jason Lee, Elman Mansimov, and Kyunghyun Cho. 2018. Deterministic non-autoregressive neural sequence modeling by iterative refinement. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1173-1182. Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. 2022. Diffusion-lm improves controllable text generation. ArXiv, abs/2205.14217. Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Weizhu Chen, and Nan Duan. 2022. Genie: Large scale pretraining for text generation with diffusion model. arXiv preprint arXiv:2212.11685. Guangyi Liu, Zeyu Feng, Yuan Gao, Zichao Yang, Xiaodan Liang, Junwei Bao, Xiaodong He, Shuguang Cui, Zhen Li, and Zhiting Hu. 2022. Composable text controls in latent space with odes. arXiv preprint arXiv:2208.00638.",
    "dinoiser-22": "Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, and Kilian Weinberger. 2022. Latent diffusion for language generation. arXiv preprint arXiv:2212.09462.",
    "dinoiser-23": "Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. 2022. Dpmsolver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.",
    "dinoiser-24": "arXiv preprint arXiv:2206.00927. Graham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel, Danish Pruthi, Xinyi Wang, and John Wieting. 2019. compare-mt: A tool for holistic comparison of language generation systems. CoRR, abs/1903.07926. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038.",
    "dinoiser-25": "Matt Post. 2018. A call for clarity in reporting bleu scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191.",
    "dinoiser-26": "Aaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek V. Datla, Ashequl Qadir, Joey Liu, and Oladimeji Farri. 2016. Neural paraphrase generation with stacked residual LSTM networks. CoRR, abs/1610.03098.",
    "dinoiser-27": "Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu Chen, Dayiheng Liu, Kewen Tang, Houqiang Li, Jiusheng Chen, Ruofei Zhang, Ming Zhou, and Nan Duan. 2021. Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8630-8639. PMLR. Lihua Qian, Mingxuan Wang, Yang Liu, and Hao Zhou. 2022. Diff-glat: Diffusion glancing transformer for parallel sequence to sequence learning. arXiv preprint arXiv:2212.10240.",
    "dinoiser-28": "Lihua Qian, Yi Zhou, Zaixiang Zheng, Yaoming Zhu, Zehui Lin, Jiangtao Feng, Shanbo Cheng,\nLei Li, Mingxuan Wang, and Hao Zhou. 2021. The volctrans glat system: Non-autoregressive translation meets wmt21. WMT 2021, page 187. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "dinoiser-29": "J. Mach. Learn. Res., 21(140):1-67. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125. Machel Reid, Vincent Josua Hellendoorn, and Graham Neubig. 2022. Diffuser: Diffusion via editbased reconstruction. In International Conference on Learning Representations. Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao, and Tie-Yan Liu. 2020. A study of nonautoregressive model for sequence generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages $149-159$. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2021. High-resolution image synthesis with latent diffusion models. Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379-389, Lisbon, Portugal. Association for Computational Linguistics. Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord. 2021. Step-unrolled denoising autoencoders for text generation. In International Conference on Learning Representations. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $1715-1725$.",
    "dinoiser-30": "Nakatani Shuyo. 2010. Language detection library for java.",
    "dinoiser-31": "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256-2265, Lille, France. PMLR. Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020a. Denoising diffusion implicit models. In International Conference on Learning Representations. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2020b. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations. Robin Strudel, Corentin Tallec, Florent Altch\u00e9, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, et al. 2022. Self-conditioned embedding diffusion for text generation. arXiv preprint arXiv:2211.04236. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27. Arash Vahdat, Karsten Kreis, and Jan Kautz. 2021. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34:11287-11302. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Elena Voita, Rico Sennrich, and Ivan Titov. 2021. Analyzing the source and target contributions to predictions in neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages $1126-1140$. Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. 2024. Diffusion language models are versatile protein learners. arXiv preprint arXiv:2402.18567. Antoine Wehenkel and Gilles Louppe. 2021. Diffusion priors in variational autoencoders. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models.",
    "dinoiser-32": "Sam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253-2263. Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Quanquan Gu. 2023. Diffusion language models can perform many tasks with scaling and instruction-finetuning. arXiv preprint arXiv:2308.12219. P Yu, S Xie, X Ma, B Jia, B Pang, R Gao, Y Zhu, S-C Zhu, and YN Wu. 2022. Latent diffusion energy-based model for interpretable text modeling. In International Conference on Machine Learning (ICML 2022).",
    "dinoiser-33": "Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, and Songfang Huang. 2022. Seqdiffuseq: Text diffusion with encoder-decoder transformers. arXiv preprint arXiv:2212.10325. Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei YE, and Quanquan Gu. 2023a. Structure-informed language models are protein designers. arXiv preprint arXiv:2302.01649. Zaixiang Zheng, Yi Zhou, and Hao Zhou. 2023b. Deep equilibrium non-autoregressive sequence learning. In Findings of the Association for Computational Linguistics: ACL 2023. Chunting Zhou, Graham Neubig, and Jiatao Gu. 2020. Understanding knowledge distillation in non-autoregressive machine translation. In International Conference on Learning Representations. ## A More Results on Machine Translation\n\nTo provide references for further study and comparisons, we report more results on machine translation. Knowledge distillation (KD). A common practice to improve the performance of non-autoregressive machine translation is knowledge distillation (Kim and Rush, 2016; Zhou et al., 2020). We report the performance of our method trained on distilled data of WMT14 and WMT16 on Tab. 7. The result shows that the performance gap between our method and the autoregressive transformer is small when knowledge distillation is used. This suggests that our method achieves performance that satisfies the needs of applications. Table 7: Model performances on machine translation with knowledge distillation. The results of transformer are from raw data, while DINOISER is trained on distilled data. The performances are measured with SacreBLEU. |  | WMT14 |  | WMT16 |  |\n| :--- | :---: | :---: | :---: | :---: |\n| Methods | DE $\\rightarrow$ EN | EN $\\rightarrow$ DE | Ro $\\rightarrow$ EN | EN $\\rightarrow$ Ro |\n| Transformer | 30.55 | 26.85 | 33.08 | 32.86 |\n| DINOISER (LB=5, MBR=1) | 30.13 | 25.70 | 32.96 | 32.58 |\n| DINOISER (LB=5, MBR=10) | 30.12 | 25.90 | 33.04 | 32.57 |\n| DINOISER (LB=10, MBR=5) | 30.30 | 25.88 | 33.13 | 32.84 |\n\nEvaluation with tokenized BLEU. Some of the previous studies in machine translation reported tokenized BLEU, despite inconsistent tokenizers (other than the standard Moses tokenizer) they might use. To help conveniently compare DINOISER to them, we also report the performance of DINOISER with tokenized BLEU ${ }^{9}$ in Tab. 8. Table 8: Tokenized BLEU of our method on machine translation datasets. We use the moses tokenizer for all the texts.",
    "dinoiser-34": "\"LB\": the size of length beam. \"MBR\": the number of candidates for each length beam to apply Minimum Bayes-Risk decoding. + KD means the results are obtained with knowledge distillation. |  | IWSLT14 |  | WMT14 |  | WMT16 |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Methods | DE $\\rightarrow$ EN | EN $\\rightarrow$ DE | DE $\\rightarrow$ EN | EN $\\rightarrow$ DE | Ro $\\rightarrow$ EN | EN $\\rightarrow$ Ro |\n| DINOISER (LB=5, MBR=1) | 32.23 | 25.54 | 29.35 | 24.43 | 31.21 | 31.18 |\n| DINOISER (LB=5, MBR=10) | 32.48 | 25.68 | 29.53 | 24.45 | 31.39 | 31.29 |\n| DINOISER (LB=10, MBR=5) | 32.25 | 25.99 | 29.40 | 24.48 | 31.50 | 31.27 |\n| DINOISER + KD (LB=5, MBR=1) | - | - | 30.64 | 26.08 | 33.21 | 32.57 |\n| DINOISER + KD (LB=5, MBR=10) | - | - | 30.62 | 26.29 | 33.29 | 32.59 |\n| DINOISER + KD (LB=10, MBR=5) | - | - | 30.76 | 26.04 | 33.40 | 32.89 |\n\n## B Implementation Details\n\nAll our implementations are based on Trans forme-base (Vaswani et al., 2017) for all datasets except IWSLT14. For IWSLT14, we use a smaller architecture that has 4 attention heads and 1024-dimensional feedforward layers. The embedding dimension for the diffusion model is 16 on IWSLT14 and 64 on the others. In the implementation of our method, we follow recent advances and apply self-conditioning techniques (Dieleman et al., 2022; Chen et al., 2022; Strudel et al., 2022). Besides, following previous practice in non-autoregressive machine translation, we train our model both with and without knowledge distillation (KD, Kim and Rush, 2016; Zhou et al., 2020). During inference, we apply beam search in the autoregressive Transformer with beam size 5 for machine translation. Correspondingly, we use length beam 5 in the non-autoregressive models, except for CDCD and DiffuSeq since they vary the target lengths by predicting paddings instead of length predictions. For text simplification and paraphrasing, we report results with various length beams as length prediction on\n\n[^8]these tasks is more challenging and less studied. For all the diffusion-based methods, we follow previous work (Li et al., 2022; Gong et al., 2022; Dieleman et al., 2022) and apply Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). For both DiffusionLM and our model, we sample with 20 steps. We implement DiffusionLM and DiNOISER upon fairseq (Ott et al., 2019), and also train Transformer and CMLM baselines using fairseq. For data preprocessing, we follow the instruction in fairseq for IWSLT14 ${ }^{10}$ and use the preprocessed data by (Gu and Kong, 2021) for WMT14 and WMT16 ${ }^{11}$. For Wiki and QQP, we use preprocessed data provided by DiffuSeq ${ }^{12}$ and tokenized them with byte-pair encoding (BPE, Sennrich et al., 2016). The training batch size is 128K for WMT14/WMT16, and 32 K for the others. We empirically find checkpoint averaging unnecessary for our method and have not applied it in all our implementations. ## C Relationship Between Different Noise Schedules and Time Samplers\n\nGenerally, the training objective of diffusion models can be expressed as\n\n$$\n\\mathbb{E}_{t \\sim r(t), \\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})}\\left[w(t)\\left\\|\\mathbf{z}_{\\theta}-\\mathbf{z}(0)\\right\\|_{2}^{2}\\right]\n$$\n\nwhere $\\mathbf{z}_{\\theta}$ is the model prediction $\\mathbf{z}_{\\theta}\\left(\\sqrt{1-\\sigma^{2}(t)} \\mathbf{z}(0)+\\sigma(t) \\epsilon, t\\right)$ for short. The above expectation over timesteps can be rewritten into the expectation of noise scales as follows. $$\n\\begin{aligned}\n& \\mathbb{E}_{t \\sim r(t), \\epsilon}\\left[w(t)\\left\\|\\mathbf{z}_{\\theta}-\\mathbf{z}(0)\\right\\|_{2}^{2}\\right] \\\\\n= & \\mathbb{E}_{\\epsilon}\\left[\\int_{0}^{1} r(t) w(t)\\left\\|\\mathbf{z}_{\\theta}-\\mathbf{z}(0)\\right\\|_{2}^{2} \\mathrm{~d} t\\right] \\\\\n= & \\mathbb{E}_{\\epsilon}\\left[\\int_{0}^{1} \\hat{r}(\\sigma) \\hat{w}(\\sigma)\\left\\|\\mathbf{z}_{\\theta}-\\mathbf{z}(0)\\right\\|_{2}^{2}\\right] \\frac{\\mathrm{d} t}{\\mathrm{~d} \\sigma} \\mathrm{d} \\sigma \\\\\n= & \\mathbb{E}_{\\sigma \\sim U(0,1), \\epsilon}\\left[w^{\\prime}(\\sigma)\\left\\|\\mathbf{z}_{\\theta}-\\mathbf{z}(0)\\right\\|_{2}^{2}\\right]\n\\end{aligned}\n$$\n\nwhere $w^{\\prime}(\\sigma)=w\\left(\\sigma^{-1}\\right) r\\left(\\sigma^{-1}\\right) \\frac{\\mathrm{d} t}{\\mathrm{~d} \\sigma}$. Therefore, training with different noise schedules and different time samplers is interchangeable by applying different weighting functions. ## D Effect of Sizes of Length Beam and MBR\n\nDINOISER can leverage both length beam search and MBR decoding to produce diverse candidates for selection.",
    "dinoiser-35": "The results on all of the evaluated datasets (Tab. 2 and Tab. 3) demonstrate that the method can gain its performance by properly adjusting the two hyperparameters for sampling. In particular, we search on various combinations of length beams and MBRs and evaluate the corresponding performance of DINOISER on the validation set of WMT14 EN $\\rightarrow$ DE, shown in Fig. 6. The model performance rises first and then drops down as the length beam increases. And for each length beam, we can further boost the performance of DINOISER with MBR $>1$, suggesting that the effects of the two factors are complementary. Using both length beam search and MBR decoding also brings benefits to DINOISER over those only involving one of them. Compared to CMLM which decodes deterministically, DINOISER is able to sample multiple sentences for each length beam, providing more diverse candidates. Compared to CDCD, which predicts paddings to generate sentences of various lengths and whose sampling efficiency is restricted by maximum target length, DINOISER's use of length beams allows more fine-grained control of the computational budget. [^9]![](https://cdn.mathpix.com/cropped/2024_09_12_d4f1c588dac32912a6a4g-19.jpg?height=592&width=790&top_left_y=1092&top_left_x=633)\n\nFigure 6: SacreBLEU on the validation set of WMT14 EN $\\rightarrow$ DE with different length beams and MBR sizes.",
    "dinoiser-36": "[^0]:    ${ }^{*}$ This work was done during Jiasheng's internship at ByteDance Research. ${ }^{\\dagger}$ Zaixiang Zheng is the corresponding author. [^1]:    ${ }^{1}$ DiffusionLM adds tiny noise to the embeddings to form $\\mathbf{z}_{0}\\left(\\right.$ i.e. $\\mathbf{z}_{0} \\sim \\mathcal{N}\\left(\\operatorname{EMB}(\\mathbf{y}), \\sigma_{0} \\mathbf{I}\\right)$ ). We empirically find this unnecessary and letting $\\mathbf{z}_{0}$ follow a Dirac distribution makes training more efficient. [^2]:    ${ }^{2}$ Consider that a token $\\alpha$ is translated into token $A$ or $a$ with $50 \\%$ each. Without extra information, the optimal prediction for translating $\\alpha$ is the center of the embedding of $A$ and $a$. This is because minimizing its training objective (i.e., $\\frac{1}{2}\\left\\|\\mathbf{z}_{\\theta}-\\mathbf{z}_{A}(0)\\right\\|_{2}^{2}+\\frac{1}{2}\\left\\|\\mathbf{z}_{\\theta}-\\mathbf{z}_{a}(0)\\right\\|_{2}^{2}$ ) results in $\\mathbf{z}_{\\theta}=$ $\\frac{\\mathbf{z}_{A}(0)+\\mathbf{z}_{a}(0)}{2}$. The prediction exactly falls in the blank area that lies between embeddings. [^3]:    ${ }^{3}$ Our goal can be motivated through the lens of optimal transport. That it to say, we aim to determine the minimum cumulative cost $\\delta^{2}=\\sum_{i=1}^{L} \\mathbf{T}_{i j} \\delta_{i j}^{2}$, where $L$ is the sequence length, by finding the optimal transportation $\\mathbf{T}$ of moving the perturbed embeddings at timestep $t$, i.e. $\\frac{\\mathbf{z}_{t}[i]}{\\alpha(t)} \\sim \\mathcal{N}\\left(\\operatorname{EMB}\\left(y_{i}\\right), \\tilde{\\sigma}^{2}(t) \\mathbf{I}\\right), \\quad i \\in \\mathcal{V}$, such that the corrupted embedding $\\frac{\\mathbf{z}_{t}[i]}{\\alpha(t)}$, if gets noised by a Gaussian deviation\n\n[^4]:    ${ }^{4}$ This can be done since the effects of different noise schedules are theoretically interchangeable up to different weight factors under the simplified training objective (Ho et al., 2020) we adopted (see Appendix C).",
    "dinoiser-37": "We also provide empirical comparisons between different schedules in Tab. 4. [^5]:    ${ }^{5}$ Empirically, we find that $\\tau_{M}$ satisfying $\\sigma\\left(\\tau_{M}\\right)=0.99$ (i.e., $\\tau_{M}=0.99$ for $\\sigma(t)=t$ and $\\tau_{M}=0.9606$ for Li et al. (2022)'s sqrt schedule $\\sigma(t)=t^{0.25}$ ) generally works well. [^6]:    ${ }^{6}$ https://www.kaggle.com/c/ quora-question-pairs\n    ${ }^{7}$ The signature is nrefs:1|case:mixed|eff:no| tok:intl|smooth:exp|version:2.3.1 if the target language is German, and nrefs: $1 \\mid$ case:mixed| eff:no|tok:13a|smooth:exp|version:2.3.1 for others. [^7]:    ${ }^{8}$ Non-autoregressive sequence learning models typically struggle with learning multimodal distributions ( Gu et al., 2018). For this reason, a common technique to improve their performance is to apply knowledge distillation, which simplifies the target distribution by replacing target samples with predictions from an autoregressive teacher model. [^8]:    ${ }^{9}$ https://github.com/alvations/sacremoses\n\n[^9]:    ${ }^{10}$ https://github.com/facebookresearch/fairseq/tree/main/examples/translation\n    ${ }^{11}$ https://github.com/shawnkx/Fully-NAT\n    ${ }^{12}$ https://github.com/Shark-NLP/DiffuSeq\n\n"
}