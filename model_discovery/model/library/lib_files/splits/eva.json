{
    "eva-0": "# EFFICIENT ATTENTION VIA Control VARIATES \n\nLin Zheng ${ }^{1 *}$ Jianbo Yuan ${ }^{2} \\quad$ Chong Wang ${ }^{3 *} \\quad$ Lingpeng Kong ${ }^{1}$<br>${ }^{\\mathbf{1}}$ The University of Hong Kong ${ }^{\\mathbf{2}}$ ByteDance Inc. ${ }^{3}$ Apple Inc.<br>$\\{$ lzheng2, lpk\\}@cs.hku.hk<br>jianbo.yuan@bytedance.com mr.chongwang@apple.com\n\n\n#### Abstract\n\nRandom-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks ${ }^{1}$\n\n\n## 1 INTRODUCTION\n\nRandom-feature-based attention (RFA, also known as Performer; Choromanski et al., 2021, Peng et al., 2021b) is an established fast approximation to the conventional softmax attention mechanism (Bahdanau et al., 2014, Vaswani et al., 2017), which successfully scales Transformer models to processing much longer sequences (Choromanski et al., 2021). At its core is the usage of random features (RF; Rahimi \\& Recht, 2008) to linearize the exponential kernel in softmax attention, which reduces the computational cost from quadratic to linear runtime and space complexity. Despite its efficiency, recent studies have pointed out that such approximation suffers from substantial performance degeneration (Xiong et al., 2021a; Zheng et al., 2022b). In this work, we generalize the formulation of RFA via control variates (Owen, 2013), which characterizes the approximation gap between RFA and softmax attention in theory. We first show that RFA can be decomposed from a global approximation over the whole sequence into a sum of local control variate estimators, each of which is applied to an individual element in the sequence. Under this formulation, RFA is equivalent to employing the same coefficient for all control variate estimators to scale their variance isotropically \\$3.1). Besides, we prove that if we optimize the coefficient of each control variate to minimize the estimation variance individually, RFA estimation becomes exact, that is, softmax attention is recovered with zero bias and zero variance $\\S 3.2$. Our key observation is that such formulation reveals a localized perspective of the RFA approximation. Instead of directly seeking a better estimate over the entire sequence, we can break down the problem into smaller problems that aim at improving the approximation for each subsequence $\\$ 4$. The control variate estimator for each subsequence can be tuned separately and combined to yield better estimation, which provably reduces approximation error in the global sense (\u00a74.1). Nevertheless, one caveat is that as the number of sub-problems increases, the approximation gap will be reduced but at the expense of higher computational complexity. For instance, if we optimize the control variate for every single element, softmax attention would be recovered as desired but with quadratic complexity. To attain a good trade-off between approximation quality and efficiency, we develop a new Efficient attention via control VAriates (EVA) that implements this divide-and-conquer strategy efficiently. In EVA, the sequence is partitioned into a fixed number of disjoint subsets. For the subset\n\n[^0]that might bear the highest correlations to the query, we explicitly optimize the control variate for each element, which recovers exact softmax attention probabilities; while for the others, the control variate coefficient is shared locally among all elements within the same subset. The resulting attention mechanism is not only highly effective but also runs with the same computational complexity as RFA \u00a74.2. Extensive experiments on both language and vision tasks demonstrate that EVA outperforms the state-of-the-art efficient attention methods (\u00a75]. ## 2 BACKGROUND\n\n### 2.1 Softmax AtTEntion MECHAnISM\n\nAssume there exist a set of $N$ queries $\\left\\{\\mathbf{q}_{n}\\right\\}_{n=1}^{N}$ and $M$ key-value pairs $\\mathbf{K}=\\left[\\mathbf{k}_{1}, \\ldots, \\mathbf{k}_{M}\\right]$ and $\\mathbf{V}=\\left[\\mathbf{v}_{1}, \\ldots, \\mathbf{v}_{M}\\right]$, where queries, keys and values are all $d$-dimensional vectors. The softmax attention mechanism (Bahdanau et al. 2014, Vaswani et al. 2017) is defined as an average over the value vectors weighted by the dot-product similarities of the queries and keys. For the $n$-th query, the attention mechanism outputs\n\n$$\n\\operatorname{Softmax} \\operatorname{Attn}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right):=\\sum_{m=1}^{M} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime}=1}^{M} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)} \\mathbf{v}_{m}\n$$\n\nIn the case of self-attention (Lin et al. 2017, Vaswani et al, 2017), we have $M=N$, which results in quadratic computational complexity since we have to compute the similarity for each query-key pair explicitly. ### 2.2 RANDOM-FEATURE-BASED AtTEntion with SELF-NORmALIZED IMPORTANCE SAMPLING\n\nRecently, Zheng et al. (2022b) identifies that softmax attention (Equation 1) can be written as an expectation over an attention-like aggregating function,\n\n$$\n\\operatorname{Softmax} \\operatorname{Attn}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right)=\\sum_{m=1}^{M} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime}=1}^{M} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)} \\mathbf{v}_{m}=\\mathbb{E}_{\\omega \\sim p_{n}(\\omega)}\\left[f_{n}(\\omega)\\right]\n$$\n\nwhere\n\n$$\nf_{n}(\\omega):=\\frac{\\sum_{m=1}^{M} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega\\right) \\mathbf{v}_{m}}{\\sum_{m^{\\prime}=1}^{M} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right) \\xi\\left(\\mathbf{k}_{m^{\\prime}}, \\omega\\right)}, \\quad p_{n}(\\omega):=\\frac{\\mathcal{N}(\\omega ; 0, \\mathbf{I}) \\sum_{m=1}^{M} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)}{Z}\n$$\n\nHere $\\xi(\\cdot, \\cdot)$ is the randomized mapping defined in such a way that $\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)=$ $\\mathbb{E}_{\\omega \\sim \\mathcal{N}(0, \\mathbf{I})}\\left[\\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)\\right]$, and $Z=\\sum_{m=1}^{M} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)$ denotes the normalizing constant of distribution $p_{n}$. Throughout this paper, we consider the positive randomized mapping $\\xi(\\mathbf{x}, \\omega)=\\exp \\left(\\omega^{\\top} \\mathbf{x}-\\frac{1}{2}\\|\\mathbf{x}\\|^{2}\\right)$ Choromanski et al. 2021) unless otherwise specified. Random-Feature-based Attention (RFA) methods (Choromanski et al., 2021, Peng et al., 2021b) can be interpreted as performing self-normalized importance sampling (SNIS; Hesterberg, 1995) to approximate Equation 2(Zheng et al. 2022b). In SNIS, one draws Monte Carlo samples from some proposal distribution $q(\\omega)$ instead of the true distribution $p_{n}(\\omega)$ and estimates the target expectation as $\\mathbb{E}_{\\omega \\sim p_{n}(\\omega)}\\left[f_{n}(\\omega)\\right]=\\mathbb{E}_{\\omega \\sim q(\\omega)}\\left[\\frac{p_{n}(\\omega)}{q(\\omega)} f_{n}(\\omega)\\right] \\approx \\frac{\\sum_{s=1}^{S} \\frac{p_{n}(\\omega)}{q(\\omega)} f_{n}\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}}$, where $\\omega_{1}, \\ldots, \\omega_{S} \\sim q(\\omega)$. Vanilla RFA amounts to constructing the SNIS estimation with $q(\\omega)=\\mathcal{N}(\\omega ; 0, \\mathbf{I})$. The SNIS representation also turns out equivalent to the more established form of RFA,\n\n$$\n\\operatorname{RFA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right):=\\frac{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}}=\\frac{\\sum_{m=1}^{M} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}}{\\sum_{m^{\\prime}=1}^{M} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m^{\\prime}}, \\boldsymbol{\\omega}\\right)}\n$$\n\nwhere the random feature, denoted by $\\phi(\\mathbf{x}, \\boldsymbol{\\omega}):=1 / \\sqrt{S}\\left[\\xi\\left(\\mathbf{x}, \\omega_{1}\\right), \\ldots, \\xi\\left(\\mathbf{x}, \\omega_{S}\\right)\\right]^{\\top}$, is proposed to approximate exponential kernels in its original motivation (see Appendix A for a detailed review). ### 2.3 CONTROL VARIATES\n\nControl variates aim to reduce the estimation variance of an expectation $\\mathbb{E}[g(\\boldsymbol{\\omega})]$. Assuming our original RFA estimation is $g(\\boldsymbol{\\omega}) \\in \\mathbb{R}^{d}$ and there is some control variate $h(\\boldsymbol{\\omega}) \\in \\mathbb{R}$ with a known expectation $\\mathbb{E}[h(\\boldsymbol{\\omega})]$, we can employ the control variate $h(\\boldsymbol{\\omega})$ with the coefficient $\\boldsymbol{\\beta} \\in \\mathbb{R}^{d}$ as follows,\n\n$$\n\\tilde{g}(\\boldsymbol{\\omega})=g(\\boldsymbol{\\omega})-\\boldsymbol{\\beta} h(\\boldsymbol{\\omega})+\\boldsymbol{\\beta} \\mathbb{E}[h(\\boldsymbol{\\omega})]\n$$\n\nNote that the resulting estimator remains unbiased since $\\mathbb{E}[\\widetilde{g}(\\boldsymbol{\\omega})]=\\mathbb{E}[g(\\boldsymbol{\\omega})]-\\boldsymbol{\\beta} \\mathbb{E}[h(\\boldsymbol{\\omega})]+$ $\\boldsymbol{\\beta} \\mathbb{E}[h(\\boldsymbol{\\omega})]=\\mathbb{E}[g(\\boldsymbol{\\omega})]$. However, the estimation variance can be largely reduced if $g(\\cdot)$ and the scaled control variate $\\boldsymbol{\\beta} h(\\boldsymbol{\\omega})$ are positively correlated (Owen, 2013). ## 3 DISSECTING RFA with CONTROL VARIATES\n\nIn this section, we first go through the connections among RFA, importance sampling, and control variates, revealing a decomposed formulation of RFA \\$3.1, and then quantify the approximation gap between RFA and softmax attention $\\S 3.2$ from these connections. ### 3.1 RFA AS A SUM OF LOCAL CONTROL VARIATE ESTIMATORS\n\nAs shown in Equation 4. RFA estimation considers all key-value pairs and produces a global approximation over the entire sequence. In contrast, our work develops a decomposed representation of RFA based on the recent advances in SNIS (Vlassis et al., 2021), which indicates that an SNIS estimate is asymptotically equivalent to a control variate estimate (the detailed derivations is deferred to Appendix B.2. In particular, we have\n\n$$\n\\begin{aligned}\n\\frac{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}} & =\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)-\\frac{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}}\\left(\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}-1\\right) \\\\\n& =g(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})(h(\\boldsymbol{\\omega})-\\mathbb{E}[h(\\boldsymbol{\\omega})]):=\\widetilde{g}(\\boldsymbol{\\omega})\n\\end{aligned}\n$$\n\nwhere $g(\\boldsymbol{\\omega}):=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)$ is our base estimate, $h(\\boldsymbol{\\omega}):=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}$ is the control variate with control coefficient $\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega}):=\\left(\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)\\right) /\\left(\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}\\right)=\\frac{g(\\boldsymbol{\\omega})}{h(\\boldsymbol{\\omega})}$. We now examine the formulation of $g(\\cdot)$ and $h(\\cdot)$ in the context of RFA. According to Equation 3 ,\n\n$$\n\\begin{aligned}\ng(\\boldsymbol{\\omega}) & =\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)=\\sum_{s=1}^{S} \\alpha\\left(\\omega_{s}\\right) \\sum_{m=1}^{M} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right) \\mathbf{v}_{m} \\\\\nh(\\boldsymbol{\\omega}) & =\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}=\\sum_{s=1}^{S} \\alpha\\left(\\omega_{s}\\right) \\sum_{m=1}^{M} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right)\n\\end{aligned}\n$$\n\nwhere $\\alpha\\left(\\omega_{s}\\right):=\\frac{1}{S} \\frac{\\mathcal{N}\\left(\\omega_{s} ; 0, \\mathbf{I}\\right)}{Z q\\left(\\omega_{s}\\right)}$ collects terms that is constant w.r.t. queries, keys, and values. Our key observation is that by changing the order of summations, both $g(\\cdot)$ and $h(\\cdot)$ can be decomposed as $g(\\boldsymbol{\\omega})=\\sum_{m=1}^{M} g_{m}(\\boldsymbol{\\omega})$ and $h(\\boldsymbol{\\omega})=\\sum_{m=1}^{M} h_{m}(\\boldsymbol{\\omega})$ respectively, where\n\n$$\ng_{m}(\\boldsymbol{\\omega})=\\sum_{s=1}^{S} \\alpha\\left(\\omega_{s}\\right) \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right) \\mathbf{v}_{m}, \\quad h_{m}(\\boldsymbol{\\omega})=\\sum_{s=1}^{S} \\alpha\\left(\\omega_{s}\\right) \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right)\n$$\n\nAs a result, we can decompose the entire RFA estimate in Equation 6 into a summation of $M$ control variate estimates following\n\n$$\n\\begin{aligned}\n\\widetilde{g}(\\boldsymbol{\\omega}) & =g(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})(h(\\boldsymbol{\\omega})-\\mathbb{E}[h(\\boldsymbol{\\omega})]) \\\\\n& =\\left(\\sum_{m=1}^{M} g_{m}(\\boldsymbol{\\omega})\\right)-\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})\\left(\\left(\\sum_{m=1}^{M} h_{m}(\\boldsymbol{\\omega})\\right)-\\mathbb{E}\\left[\\sum_{m=1}^{M} h_{m}(\\boldsymbol{\\omega})\\right]\\right) \\\\\n& =\\sum_{m=1}^{M} g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})\\left(h_{m}(\\boldsymbol{\\omega})-\\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right]\\right):=\\sum_{m=1}^{M} \\widetilde{g}_{m}(\\boldsymbol{\\omega})\n\\end{aligned}\n$$\n\nHere $\\widetilde{g}_{m}(\\boldsymbol{\\omega})=g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})\\left(h_{m}(\\boldsymbol{\\omega})-\\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right]\\right)$ denotes the corresponding control variate estimator of the $m$-th key-value pair $2^{2}$ and $\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})$ is the coefficient shared across the entire sequence. ### 3.2 Optimizing CoEfficients in RFA Locally Recovers Softmax AtTEntion\n\nBased on the decomposition of RFA in Equation 7 we have one local control variate attached to each key-value pair. To see the benefit of such decomposition, we demonstrate that softmax attention is equivalent to associating each control variate with a locally optimized coefficient $\\widehat{\\boldsymbol{\\beta}}_{m}$ in RFA.",
    "eva-1": "Proposition 1. Let $\\widetilde{g}_{m}(\\boldsymbol{\\omega})=g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}}_{m}\\left(h_{m}(\\boldsymbol{\\omega})-\\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right]\\right)$. We denote the variance of some estimator $g(\\boldsymbol{\\omega})$ as $\\operatorname{Var}[g(\\boldsymbol{\\omega})]:=\\operatorname{Cov}[g(\\boldsymbol{\\omega}), g(\\boldsymbol{\\omega})]$. Then the optimal $\\widehat{\\boldsymbol{\\beta}}_{m}$ that minimizes $\\operatorname{Tr}\\left(\\operatorname{Var}\\left[\\widetilde{g}_{m}(\\boldsymbol{\\omega})\\right]\\right)$ (i.e., the sum variance over all dimensions) is of the form\n\n$$\n\\boldsymbol{\\beta}_{m}^{*}:=\\arg \\min _{\\boldsymbol{\\beta}} \\operatorname{Tr}\\left(\\operatorname{Var}\\left[\\widetilde{g}_{m}(\\boldsymbol{\\omega})\\right]\\right)=\\mathbf{v}_{m}=\\frac{g_{m}(\\boldsymbol{\\omega})}{h_{m}(\\boldsymbol{\\omega})}\n$$\n\nFurthermore, by letting $\\widehat{\\boldsymbol{\\beta}}_{m}=\\boldsymbol{\\beta}_{m}^{*}$ for all $m=1,2, \\ldots, M$, we have $\\operatorname{Tr}\\left(\\operatorname{Var}\\left[\\widetilde{g}_{m}(\\boldsymbol{\\omega})\\right]\\right)=0$. As a result, $\\operatorname{Tr}(\\operatorname{Var}[\\widetilde{g}(\\boldsymbol{\\omega})])=0$ and thus $\\operatorname{RFA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right)=\\widetilde{g}(\\boldsymbol{\\omega})=\\operatorname{Softmax} \\operatorname{Attn}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right)$. The proof is deferred to Appendix B.4. This proposition implies optimizing $\\widehat{\\boldsymbol{\\beta}}_{m}$ for each keyvalue pair in the decomposed formulation of RFA recovers the exact softmax attention. It not only characterizes the theoretical gap introduced by RFA but also sheds light on how to improve RFA towards softmax attention from a localized perspective. Furthermore, it delineates the trade-off between estimation quality and computational costs. On the one hand, if we use a distinct $\\widehat{\\boldsymbol{\\beta}}_{m}$ for each estimator, we could achieve a perfect estimation, albeit at the expense of computing $\\exp \\mathbf{q}_{n}^{p} \\mathbf{k}_{m}$ for every query-key pair explicitly with quadratic time and space complexity. On the other hand, if a single shared coefficient is employed, it degrades to conventional RFA, where all the control variate estimators can be merged and computed together in linear complexity (Choromanski et al., 2021; Peng et al., 2021b; Zheng et al. 2022b). ## 4 EVA: Efficient AtTention VIA Control VARIATES\n\nIn this section, we demonstrate that the control variate formulation offers a natural way to improve RFA with a finer-grained treatment over control variates. We describe the improved efficient attention mechanism EVA in $\\S 4.1$ and its practical implementation in $\\S 4.2$\n\n### 4.1 CONTROL VARIATES WITH LOCALLY SHARED COEFFICIENTS\n\nWe denote $[M]:=\\{1,2, \\ldots, M\\}$ as the set of all key-value indices. Instead of employing the same coefficient for all control variates as in RFA, we propose to partition $[M]$ into $C$ subsets $\\mathcal{P}_{1}, \\mathcal{P}_{2}, \\ldots, \\mathcal{P}_{C}$ and allocate a locally shared $\\boldsymbol{\\beta}_{c}$ for each subset $\\mathcal{P}_{c}$. For all $\\boldsymbol{\\beta}_{c}$ and their optimum $\\boldsymbol{\\beta}_{m}^{*}$ for each token, define the weighted mean squared error (weighted MSE) as $\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left\\|\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2}$, where $\\alpha_{m}>0$ and $\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}=1$. To see the benefit of partitioning, we demonstrate that there always exists some $\\left\\{\\boldsymbol{\\beta}_{c}\\right\\}_{c=1}^{C}$ that achieves lower weighted MSE than any globally shared coefficient (see Appendix B. 5 for a formal argument). The next question is how to determine $\\left\\{\\boldsymbol{\\beta}_{c}\\right\\}_{c=1}^{C}$. According to Proposition 1, a natural choice is to adapt the optimal coefficients (Equation 8) to the case of partitioned subsets. We justify this choice by proving that it is also optimal in minimizing the MSE above weighted by the true attention probabilities. Proposition 2. Suppose $U$ is a set of key-value indices, $\\boldsymbol{\\beta}_{m}^{*}$ is the optimal coefficient for each $m \\in U$ as defined in Proposition 1] and $\\mathcal{P}_{1}, \\mathcal{P}_{2}, \\ldots, \\mathcal{P}_{C}$ are an arbitrary partition of $U$, where each subset $\\mathcal{P}_{c}$ is associated with a distinct $\\boldsymbol{\\beta}_{c}$. We consider the following weighted mean squared error,\n\n$$\nJ\\left(\\boldsymbol{\\beta}_{1}, \\ldots, \\boldsymbol{\\beta}_{C}\\right):=\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left\\|\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2}\n$$\n\n[^1]Then for each $c=1, \\ldots, C$ we have\n\n$$\n\\boldsymbol{\\beta}_{c}^{*}:=\\arg \\min _{\\boldsymbol{\\beta}_{c}} J\\left(\\boldsymbol{\\beta}_{1}, \\ldots, \\boldsymbol{\\beta}_{C}\\right)=\\frac{\\mathbb{E}\\left[\\sum_{m \\in \\mathcal{P}_{c}} g_{m}(\\boldsymbol{\\omega})\\right]}{\\mathbb{E}\\left[\\sum_{m \\in \\mathcal{P}_{c}} h_{m}(\\boldsymbol{\\omega})\\right]}\n$$\n\nAs a consequence, with $\\boldsymbol{\\beta}_{c}=\\boldsymbol{\\beta}_{c}^{*}$, the partition scheme must achieve lower weighted mean squared error than any globally shared $\\boldsymbol{\\beta}$, that is, $J\\left(\\boldsymbol{\\beta}_{1}=\\boldsymbol{\\beta}_{1}^{*}, \\ldots, \\boldsymbol{\\beta}_{C}=\\boldsymbol{\\beta}_{C}^{*}\\right) \\leq J\\left(\\boldsymbol{\\beta}_{1}=\\boldsymbol{\\beta}, \\ldots, \\boldsymbol{\\beta}_{C}=\\boldsymbol{\\beta}\\right)$. The proof can be found in Appendix B.6. Apart from measuring the squared errors for all coefficients, Equation 9 also governs the significance of each error by its corresponding softmax weights, which attains closer alignment with true softmax attention. Therefore, this proposition implies that it is much easier for the partitioned control variate estimators to obtain coefficients closer to their optimum while faithfully respecting softmax attention. The optimal coefficients $\\boldsymbol{\\beta}_{c}^{*}$ could be estimated via Monte Carlo samples as $\\boldsymbol{\\beta}_{c}^{*} \\approx \\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})=\\left(\\sum_{m \\in \\mathcal{P}} g_{m}(\\boldsymbol{\\omega})\\right) /\\left(\\sum_{m \\in \\mathcal{P}} h_{m}(\\boldsymbol{\\omega})\\right)$, which is a widely adopted strategy in the control variate literature (Wang et al., 2013, Owen, 2013). The resulting estimator for each subset $\\mathcal{P}_{c}$ takes the form\n\n$$\n\\sum_{m \\in \\mathcal{P}_{c}}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega}) h_{m}(\\boldsymbol{\\omega})+\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega}) \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z}\\right)=\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})\n$$\n\nPartially Optimized Coefficients. Given the optimality of using a separate coefficient for each key-value pair, we could further improve the estimation by selecting some subset $E \\subseteq[M]$ and employ $\\widehat{\\boldsymbol{\\beta}}_{m}=\\widehat{\\boldsymbol{\\beta}}_{m}^{*}=\\mathbf{v}_{m}$ for each $m \\in E$. Without loss of generality, we assume $E \\cap \\mathcal{P}_{c}=\\varnothing$ for all $c=1, \\ldots, C$ and $[M]=\\left(\\bigcup_{c=1}^{C} \\mathcal{P}_{c}\\right) \\cup E$. According to Proposition 1 , for each $m \\in E$ we have\n\n$$\n\\widetilde{g}_{m}(\\boldsymbol{\\omega})=g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}}_{m} h_{m}(\\boldsymbol{\\omega})+\\widehat{\\boldsymbol{\\beta}}_{m} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z}=\\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}\n$$\n\nWe choose $E$ by running an additional sparse attention mechanism (e.g., local window attention (Child et al., 2019) or Reformer (Kitaev et al., 2020)), which tend to select tokens that are more relevant to the query in sub-quadratic complexity. Since estimates on these critical tokens are exact, this strategy not only reduces the overall squared error (Equation 9), but also produces a more informative context for queries, which often translates into better empirical performance. Combining Equations 12 and 11 together, we obtain an improved Efficient attention via control VAriates (EVA),\n\n$$\n\\begin{aligned}\n\\operatorname{EVA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right) & :=\\widetilde{g}(\\boldsymbol{\\omega})=\\sum_{m \\in E} \\widetilde{g}_{m}(\\boldsymbol{\\omega})+\\sum_{m \\notin E} \\widetilde{g}_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\mathbf{v}_{m}+\\sum_{c=1}^{C} \\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})\n\\end{aligned}\n$$\n\nComparison with Vanilla RFA. EVA and vanilla RFA can be re-written in a similar way (see Appendix B. 7 for a detailed derivation),\n\n$$\n\\begin{aligned}\n& \\operatorname{RFA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right)=\\frac{\\sum_{m=1}^{M} g_{m}(\\boldsymbol{\\omega})}{\\sum_{m=1}^{M} h_{m}(\\boldsymbol{\\omega})} \\\\\n& \\operatorname{EVA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right)=\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\frac{g_{m}(\\boldsymbol{\\omega})}{h_{m}(\\boldsymbol{\\omega})}+\\sum_{c=1}^{C} \\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\frac{\\sum_{m \\in \\mathcal{P}_{c}} g_{m}(\\boldsymbol{\\omega})}{\\sum_{m \\in \\mathcal{P}_{c}} h_{m}(\\boldsymbol{\\omega})}\n\\end{aligned}\n$$\n\nIntuitively, we can think of EVA as a calibrated version of RFA. Instead of directly computing and aggregating the random feature approximation for all tokens as in RFA (Equation 14), EVA (Equation 15) first constructs local estimation for either a single token $(m \\in E)$ or a subset (e.g., $\\mathcal{P}_{c}$ ), and then corrects these approximations by their corresponding true attention scores (e.g., $\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)$ for $\\left.\\mathcal{P}_{c}\\right)$.",
    "eva-2": "These adjusted local estimates are finally aggregated and globally normalized. Thanks to the decomposed representation of RFA, we can realize this divide-and-conquer strategy in a principled manner, which imposes finer-grained control on the whole estimation accuracy and enjoys increased approximation fidelity. Table 1: Classification accuracy on ImageNet 1 k in comparison to different RF-based approximations. ${ }^{\\dagger}$ vanilla PVT-v2-b3 (Wang et al., 2021b) uses a convolutional kernel to downsample key and value vectors, resulting in fewer FLOPs but with significant performance degradation. | Model | DeiT-Tiny |  |  |  | DeiT-Small |  |  |  | PVT-v2-b3 |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | \\# Param. | FLOPs | Top-1 Acc. | \\# Param. | FLOPs | Top-1 Acc. | \\# Param. | FLOPs | Top-1 Acc. |  |  |\n| Local | 5.7 M | 1.1 G | 67.10 | 22.0 M | 4.3 G | 74.06 | 36.0 M | 7.2 G | 83.34 |  |  |\n| Performer | 5.7 M | 1.2 G | 65.92 | 22.0 M | 4.4 G | 74.29 | 36.0 M | 8.2 G | 82.40 |  |  |\n| LARA | 5.8 M | 1.2 G | 71.48 | 22.2 M | 4.5 G | 79.48 | 39.9 M | 7.7 G | 83.47 |  |  |\n| EVA (Ours) | 5.8 M | 1.2 G | $\\mathbf{7 3 .",
    "eva-3": "0 0}$ | 22.2 M | 4.4 G | $\\mathbf{8 0 . 6 5}$ | 36.1 M | 7.4 G | $\\mathbf{8 3 . 7 1}$ |  |  |\n| Softmax | 5.7 M | 1.3 G | $\\mathbf{7 2 . 9 8}$ | 22.0 M | 4.6 G | 80.36 | 45.2 M | $6.9 \\mathrm{G}^{\\dagger}$ | $83.14^{\\dagger}$ |  |  |\n\n### 4.2 PRACTICAL IMPLEMENTATION\n\nAccording to the formulation (Equation 13) of EVA, the terms within $E$ could be computed efficiently due to its limited size; however, the partitioning requires computing $\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)$ explicitly for each subset, which again builds up to quadratic computational complexity. As discussed above, $\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)$ serves as a weight to correct the contribution from each subset $\\mathcal{P}_{c}$. In this regard, we propose to approximate such control by $\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\approx \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right)$, where $\\widetilde{\\mathbf{k}}_{c}$ is an adaptive vector summarizing the information of all keys belonging to $\\mathcal{P}_{c}$ (see Appendix Cfor more details). Such heuristic not only avoids computing the exponential dot product of each query-key pair explicitly, but also induces a fast approximation of the normalizing constant,\n\n$$\nZ=\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\approx \\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{c=1}^{C} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right)\n$$\n\nEquipped with these results, our EVA estimator (Equation 13) can be reduced as follows,\n\n$$\n\\operatorname{EVA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right) \\approx \\frac{\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}+\\sum_{c=1}^{C} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right) \\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})}{\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{c=1}^{C} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right)}\n$$\n\nParameterization Details. We define $E$ in the same way as a simple block-wise local attention (Xiong et al. 2021a). The input sequence is first chunked into multiple blocks (or 2D windows for images), and each query $\\mathbf{q}_{n}$ is associated with a specific $E^{n}$ that only contains tokens within the same block as the query. For the remaining indices $[M] \\backslash E^{n}$, we evenly split it into $C$ contiguous chunks $\\left\\{\\mathcal{P}_{1}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}$. Note that we add the superscript $n$ here to denote the dependence on the query position; however, for notational brevity, we omit the notation when there is no ambiguity. The pseudo-code of EVA is provided in Algorithm 1 of Appendix. More implementation details, including the definition of $\\widetilde{\\mathbf{k}}_{c}$ and $\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})$ in Equation 16 are deferred to Appendix C\n\nExtension to Autoregressive Modeling. The decoder (or causal) self-attention, where each query can only attend to previous tokens, is the key ingredient in Transformer-based generative modeling (Vaswani et al.",
    "eva-4": "2017, Brown et al. 2020). We demonstrate that it is straightforward to extend EVA to support such auto-regressive modeling with few modifications. Thanks to the decomposed formulation of EVA, we only need to incorporate two triangular mask matrices into the computation, which eliminate the information from future singletons $m \\in E$ and entire future subsets $\\mathcal{P}_{c}$ respectively. Unlike previous RFA methods, which are slow during training due to their recurrent computation (Choromanski et al., 2021, Peng et al., 2021b), the resulting causal variant remains highly efficient.",
    "eva-5": "More details can be found in Appendix D. including a pseudo-code Algorithm 2. ## 5 EXPERIMENTAL RESULTS\n\nIn this section, we evaluate our proposed method on various tasks, including image classification (\\$5.1), language tasks \\$5.2), and Long Range Arena benchmark (Appendix F). Details of experimental protocols and baselines can be found in Appendix E\n\nTable 2: Image classification accuracy on ImageNet 1 k dataset with DeiT-Tiny-784.",
    "eva-6": "| Model | \\# Param. | FLOPs | Top-1 Acc. |\n| :---: | :---: | :---: | :---: |\n| Performer (Choromanski et al., 2021, | 5.7 M | 4.9 G | 67.19 |\n| Local attention Child et al., 2019 , | 5.7 M | 4.4 G | 70.62 |\n| Scatterbrain Chen et al., ZULIa. | 5.7 M | 5.2 G | 73.50 |\n| Nystr\u00f6mformer Xiong et al., Z021b, | 5.7 M | 4.8 G | 74.20 |\n| LARA Zheng et al., ZULZD | 5.8 M | 4.6 G | 75.02 |\n| Combiner Ren et al., 2021 | 5.7 M | 4.7 G | 75.56 |\n| Long-Short Zhu et al., 202 , | 6.1 M | 5.0 G | 76.41 |\n| EVA (Ours) | 5.8 M | 4.6 G | 76.67 |\n| Softmax | 5.7 M | 7.0 G | 77.16 |\n\nTable 4: BLEU scores on the test set of ${ }^{W M T 14}$ En-De.",
    "eva-7": "${ }^{\\dagger}$ numbers are taken from Zheng et al.",
    "eva-8": "(2022b). | Model | \\# Param. | BLEU |\n| :---: | :---: | :---: |\n| Performer-128 ${ }^{\\dagger}$ | 60.92 M | 23.5 |\n| LARA- $16^{\\dagger}$ | 60.96 M | 26.4 |\n| LARA- $32^{\\dagger}$ | 60.96 M | 26.8 |\n| LARA-64 ${ }^{\\dagger}$ | 60.96 M | 27.0 |\n| EVA-16 | 60.96 M | 27.2 |\n| EVA-32 | 60.96 M | 27.3 |\n| EVA-64 | 60.96 M | 27.5 |\n| Softmax | 60.92 M | 27.5 |\n\nTable 3: Masked Language Modeling Perplexity on the Books 3 validation dataset. | Model | \\# Param. | FLOPs | Perplexity |\n| :---: | :---: | :---: | :---: |\n| Performer Choromanski et al., 2021, | 126 M | 213 G | 8.61 |\n| Linformer wang et al., 2020 , | 129 M | 193G | 5.16 |\n| LARA Zhengerat., $20 \\angle 20$ | 126 M | 194G | 4.39 |\n| Reformer Kitaev et at., 2020 | 126 M | 205G | 4.28 |\n| Local attention CDita et al. 2019 , | 136 M | 183 G | 4.27 |\n| Combiner Ren etal., $\\angle O Z 1$ | 136 M | 187 G | 4.12 |\n| Long-Short Znu et al., 202) | 142 M | 218G | 4.01 |\n| eVA (Ours) | 136 M | 184G | 3.94 |\n| EVA-4096 (Ours) | 136M | 387G | 3.73 |\n| Softmax | 126M | 252G | 3.74 |\n\nTable 5: Validation (Val.) and Test perplexity (PPL) on Wikitext-103.",
    "eva-9": "256/480 indicate evaluation context window sizes. ${ }^{\\dagger}$ numbers are due to Kasai et al. (2021). | Model | \\# Params. | 256 |  | 480 |  |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | Val. | Test | Val. | Test |\n| Softmax ${ }^{\\dagger}$ | 449M | 17.9 | 18.5 | - | - |\n| ELU $^{\\dagger}$ | 449M | 22.0 | 22.8 | - | - |\n| RFA $^{\\dagger}$ | 449M | 20.4 | 21.3 | - | - |\n| T2R ${ }^{\\dagger}$ | 450M | 20.1 | 20.8 | - | - |\n| EVA (Ours) | 450M | 17.9 | 18.6 | 17.7 | 18.3 |\n| Softmax | 247M | 18.8 | 19.5 | 18.4 | 19.1 |\n| EVA (Ours) | 247M | 18.8 | 19.4 | 18.5 | 19.1 |\n\n### 5.1 IMAGE ClassifICATION\n\nWe explore the ability to learn visual representations for different attention mechanisms in vision transformers (ViTs; Dosovitskiy et al.",
    "eva-10": "2021). In particular, we replace softmax attention used in ViTs with its efficient variants and evaluate their performance on the ImageNet 1 k dataset (Deng et al. 2009), which contains over $1,280 \\mathrm{~K}$ and 50 K images of 1,000 classes for training and validation splits, respectively. For the transformer model, we consider both a plain ViT (DeiT; Dosovitskiy et al. 2020 Touvron et al., 2021) and a pyramidal ViT (PVT; Wang et al. 2021b) to test the performance. The former maintains the same sequence length (which is set to 196 by default) across all transformer layers, while the latter processes much longer sequences (up to 3136 tokens) at early layers and progressively reduces the sequence length to form a hierarchical structure. Detailed experimental settings could be found in Appendix E. 2\nResults. We first compare the performance of EVA against our main baselines on the standard ViT architectures. As shown in Table 1, EVA significantly improves the performance of previous RFA approaches (including Performer (Choromanski et al., 2021) and LARA (Zheng et al., 2022b) and local attention by a large margin, and even outperforms the conventional softmax attention. We then consider a more challenging setting, where the plain architecture DeiT-Tiny is used but the sequence length is scaled up to 784 (denoted as DeiT-Tiny-784). We compare EVA against other attention variants in this setting and report the classification results in Table 2. EVA outperforms most previous baselines and remains highly competitive with softmax attention, illustrating its effectiveness. ### 5.2 Machine Translation and LanguaGe Modeling\n\nWe further evaluate EVA on the natural language domain. Specifically, we consider three tasks:\n\n- Masked language modeling (MLM) on a pretraining-scale book corpus Books 3 in the Pile dataset suite (Presser, 2020; Gao et al., 2020), consisting of over 196,640 published books. - Machine translation (MT) on WMT1 4 En-De benchmark (Bojar et al., 2014). - Autoregressive language modeling (Autoregressive LM) on a large-scale token-level LM benchmark Wikitext-103 (Merity et al. 2016). Results. We report MLM validation perplexity in Table 3, where the sequence length is 2048 by default. EVA substantially improves previous methods based on random features (including Performer and LARA) and outperforms the other efficient attention mechanisms. Thanks to the linear\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5520e6e7b143a538a399g-08.jpg?height=383&width=1409&top_left_y=270&top_left_x=356)\n\nFigure 1: Left and middle: empirical memory consumption and running time comparison respectively of different attention mechanisms under various sequence lengths. Right: a snapshot of MLM validation loss curve versus actual elapsed time during training. complexity of EVA, it can be scaled further to much longer sequences. With input sequences of length increased to 4096, EVA (denoted as \"EVA-4096\") attains lower validation perplexity than exact softmax attention, which demonstrates its capability of scaling to much longer sequences. Besides, machine translation results are compared in Table 4 , where in this task $C=8$ by default and EVA- $m$ denotes EVA with $|E|=m$. EVA outperforms previous random feature methods by a large margin and achieves translation quality on par with full softmax attention even under the setting of small $|E|$ and $C$. For Autoregressive LM (Table 5), EVA achieves the same perplexity as softmax attention with much lower computational complexity. Comparing against various random feature methods reported by previous work Kasai et al. (2021), we observe a significant performance gain brought from EVA even under a Transformer with half parameters. When further increasing the transformer model size as the setting in Kasai et al. (2021), EVA still scales as effectively as softmax attention with a comparable perplexity while outperforming previous random feature methods by a larger margin. These results indicate the substantially enlarged capacity of EVA to approximate softmax attention. ### 5.3 ANALYSIS\n\nRunning Time \\& Memory Comparison. We conduct a simulation experiment to evaluate the empirical efficiency of various attention methods, which is measured by the running time per iteration and memory footprint under different sequence lengths.",
    "eva-11": "The setup can be found in Appendix E.",
    "eva-12": "4 . As illustrated in Figures 1a and 1b. EVA only incurs a little computational overhead compared to Performer and local attention and achieves much better running time speed-up than Long-Short (Zhu et al., 2021), a strong baseline across various tasks albeit with much longer running time and larger memory consumption. In Figure 1c, we further visualize the speed-up of EVA relative to conventional softmax attention by plotting the validation loss curve versus actual elapsed time during training transformers (equivalent to 32 GPU days). It can be seen that EVA can achieve a much lower loss after running for the same elapsed time; in contrast, conventional softmax attention needs to run almost $3 \\times$ longer to match the loss quantity. Overall, our method attains a good trade-off between quality and empirical efficiency. Table 6: Classification accuracy on ImageNet 1 k dataset. | Model | Mem.(G) | Time(ms/iter) | $\\|E\\|$ | $C$ | Top-1 Acc. |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| Performer | 8.1 | 87 | 0 | 1 | 67.19 |\n| Local | 7.8 | 65 | 49 | 0 | 70.62 |\n|  | 8.4 | 77 | 0 | 49 | 74.33 |\n|  | 9.1 | 87 | 49 | 1 | 74.10 |\n|  | 9.4 | 89 | 49 | 16 | 75.83 |\n| EVA | 9.9 | 94 | 49 | 49 | 76.67 |\n|  | 12.5 | 119 | 49 | 196 | 77.10 |\n|  | 11.9 | 108 | 196 | 49 | 77.36 |\n| Softmax | 17.7 | 99 | n.a. | n.a. | 77.16 |\n\nTable 7: MLM validation perplexity on Books 3. \"-\" indicates fail to converge. | Model | Mem.(G) | Time(ms/iter) | $\\|E\\|$ | $C$ | Perplexity |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| Performer-4096 | 4.8 | 39 | 0 | 1 | - |\n| Local-4096 | 4.4 | 29 | 256 | 0 | 4.34 |\n|  | 5.8 | 40 | 256 | 128 | 3.82 |\n| EVA-4096 | 6.4 | 41 | 256 | 256 | 3.73 |\n|  | 6.9 | 47 | 512 | 128 | 3.71 |\n| Softmax-4096 | 21.2 | 102 | n.a. | n.a. | 3.65 |\n\nAblation Study. In this section, we conduct an ablation study on image classification and MLM tasks to investigate the effects of main hyper-parameters in EVA (see Table 8 for more comprehensive analysis). In particular, we vary $|E|$ and the partition size $C$ and evaluate their performance on both image classification and masked language modeling. As presented in Table 6 and Table 7, increasing $|E|$ amounts to obtaining exact estimates for more key-value pairs, which greatly improves empirical performance; besides, increasing $C$ would process control variates at a finer scale, also translating into better modeling quality, consistent with our theoretical analysis \\$4.1). ## 6 RELATED WORK\n\nControl Variates. Control variates are a widely used variance reduction technique in reinforcement learning (Greensmith et al., 2004, Grathwohl et al., 2018; Vlassis et al., 2021), stochastic optimization (Wang et al., 2013), variational inference (Paisley et al., 2012; Ranganath et al., 2014; Geffner \\& Domke, 2018; Tucker et al., 2017; Grathwohl et al., 2018), Markov chain Monte Carlo (Baker et al., 2019) and many other topics. Our construction with control variates provides a new perspective on designing faster yet more accurate attention approximations. Efficient Attention Mechanisms. A lot of research work has put the focus on reducing the quadratic complexity of conventional softmax attention. A widely used approach is to define a sparse attention pattern so that each query is limited to only attending to a subset of tokens. The sparse pattern could be either learnable (Kitaev et al., 2020; Vyas et al., 2020, Tay et al, 2020, Roy et al., 2021; Madaan et al., 2022) or simply fixed (Liu et al., 2018;, Parmar et al., 2018; Child et al., 2019, Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Liu et al., 2021; Xiong et al. 2021a; Wang et al., 2022; Chen et al., 2022; Hutchins et al.| 2022). Another paradigm is to adopt low-rank approximations, including via the Nystr\u00f6m method (Xiong et al., 2021b), down-sampling with learnable projections (Wang et al., 2020, Peng et al., 2021a), or explicitly compressing sequences ( Rae et al., 2020, Dai et al., 2020, Ma et al., 2021; Jaegle et al., 2021). There are also studies improving both sparse and low-rank methods for better attention matrix approximation (Nguyen et al., 2021; Zhu et al., 2021; Chen et al., 2021a; Ren et al., 2021; Zhu \\& Soricut, 2021; Hua et al., 2022, Zeng et al., 2022). Instead of adopting approximate methods, a recent line of work (Rabe \\& Staats, 2021; Dao et al., 2022) proposes to compute the exact softmax attention in an online manner (Milakov \\& Gimelshein, 2018) without materializing the full attention matrix. In this way, softmax attention can be computed in linear memory complexity, and the runtime can also be greatly improved by further minimizing memory accesses (Dao et al., 2022). Random-Feature-based Attention. Random-feature-based methods are a popular alternative that uses random features (Rahimi \\& Recht, 2008) to linearize exponential kernels in softmax attention (Katharopoulos et al., 2020, Choromanski et al. 2021, Peng et al., 2021b). Recent work attempts to improve RFA approximation from several aspects, such as designing more accurate random feature maps (Choromanski et al. 2022; Likhosherstov et al., 2022; Chowdhury et al., 2022), incorporating relative positional or other task-specific biases (Liutkus et al.,|2021; Luo et al., 2021; Chen, 2021; Zheng et al., 2022a; Qin et al., 2022b; Wu et al., 2022, Qin et al., 2022a), or leveraging connections to fast weight programmers (Peng et al., 2021b; Schlag et al., 2021; Irie et al., 2021). Prior work closely related to ours includes Zheng et al. (2022b), which reinterprets RFA using self-normalized importance sampling (Hesterberg, 1995) and theoretically extends the random feature approximation from individual exponential kernels to the whole softmax attention. Our work further generalizes this result via control variates and characterizes the approximation gap caused by RFA. Scatterbrain (Chen et al., 2021a) is also similar to our work in that it also refines RF approximation on critical local regions. However, it is developed based on a different motivation that attempts to approximate the attention matrix with a combination of sparse and low-rank matrices. Interestingly, we find that Scatterbrain can be cast as a special case under our framework; see Appendix Gfor a detailed discussion about connections between EVA and previous attention mechanisms. ## 7 CONCLUSION AND LIMITATIONS\n\nIn this work, we develop an efficient attention mechanism EVA via control variates. Our framework reveals a localized perspective of RFA approximation, which not only bridges the gap between RFA and exact softmax attention but also attains a good trade-off between modeling quality and efficiency. We evaluate our method on both vision and language tasks and demonstrate substantial improvements over previous baselines. There are some limitations of our framework. For instance, the approximation in computing control variate estimation for each partitioned subset is crude and might limit the potential modeling capacity; in addition, we only explore the most straightforward partitioning strategy that evenly splits the sequence into multiple contiguous chunks; while in general, the partition could contain arbitrary subsequences or be adaptive to inputs via clustering methods, which can be guided by task-specific inductive biases. It is interesting to investigate these limitations to unleash the expressiveness of EVA further, which we leave for future work. ## ACKNOWLEDGMENTS\n\nWe would like to thank the HKU NLP group, the Shark-NLP group, and the anonymous reviewers for their valuable suggestions that greatly helped improve this work. This work is partially supported by the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N_HKU714/21. ## REFERENCES\n\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 268-284, 2020. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2019. URL https: / / openreview. net/forum?id=ByxZX20qFQ\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate.",
    "eva-13": "arXiv preprint arXiv:1409.0473, 2014. Jack Baker, Paul Fearnhead, Emily B Fox, and Christopher Nemeth. Control variates for stochastic gradient mcmc. Statistics and Computing, 29(3):599-615, 2019. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Ond\u0159ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pp. 12-58, 2014. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n\nBeidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00e9. Scatterbrain: Unifying sparse and low-rank attention. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021a. Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum? id=Nfl-iXa-y7R\n\nChun-Fu Chen, Rameswar Panda, and Quanfu Fan. Regionvit: Regional-to-local attention for vision transformers.",
    "eva-14": "arXiv preprint arXiv:2106.02689, 2021b. Peng Chen. PermuteFormer: Efficient relative position encoding for long sequences. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10606-10618, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.828. URL https://aclanthology.org/ 2021.emnlp-main. 828\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=Ua6zuk0WRH. Krzysztof Marcin Choromanski, Han Lin, Haoxian Chen, Arijit Sehanobish, Yuanzhe Ma, Deepali Jain, Jake Varley, Andy Zeng, Michael S Ryoo, Valerii Likhosherstov, Dmitry Kalashnikov, Vikas Sindhwani, and Adrian Weller. Hybrid random features. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=EMigfE6ZeS. Sankalan Pal Chowdhury, Adamos Solomou, Kumar Avinava Dubey, and Mrinmaya Sachan. Learning the transformer kernel. Transactions on Machine Learning Research, 2022. URL https: //openreview.net/forum?id=tLIBAEYjcv. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing.",
    "eva-15": "Advances in neural information processing systems, $33: 4271-4282,2020$. Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255. Ieee, 2009. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / \\mathrm{N} 19-1423$. URL https: //aclanthology.org/N19-1423\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https: / openreview. net/forum?id=YicbFdNTTy. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.",
    "eva-16": "arXiv preprint arXiv:2101.00027, 2020. Tomas Geffner and Justin Domke. Using large ensembles of control variates for variational inference. Advances in Neural Information Processing Systems, 31, 2018. Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum? id=SyzKd1bCW. Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning.",
    "eva-17": "Journal of Machine Learning Research, 5(9), 2004. Tim Hesterberg. Weighted average importance sampling and defensive mixture distributions. Technometrics, 37(2):185-194, 1995. ISSN 00401706. URL/http://www.jstor.org/stable/ 1269620\n\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition.",
    "eva-18": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129-8138, 2020. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. BlockRecurrent Transformers. arXiv preprint arXiv:2203.07852, 2022. Kazuki Irie, Imanol Schlag, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=ot2ORiBqTa1. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 4651-4664. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/jaegle21a.html\n\nJungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning pretrained transformers into RNNs. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10630-10643, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.830. URL https://aclanthology.org/ 2021.emnlp-main. 830 . Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "eva-19": "In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=rkgNKkHtvB. Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unifying convolution and self-attention for visual recognition.",
    "eva-20": "arXiv preprint arXiv:2201.09450, 2022. Valerii Likhosherstov, Krzysztof Choromanski, Avinava Dubey, Frederick Liu, Tamas Sarlos, and Adrian Weller. Chefs' random tables: Non-trigonometric random features.",
    "eva-21": "arXiv preprint arXiv:2205.15317, 2022. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URLhttps://openreview.net/forum?id= Hyg0vbWC-\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.",
    "eva-22": "arXiv preprint arXiv:1907.11692, 2019. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows.",
    "eva-23": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 10012-10022, October 2021. Antoine Liutkus, Ond\u0159ej C\u00edfka, Shih-Lun Wu, Umut Simsekli, Yi-Hsuan Yang, and Gael Richard. Relative positional encoding for transformers with linear complexity. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 7067-7079. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/liutkus21a.html\n\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts.",
    "eva-24": "arXiv preprint arXiv:1608.03983, 2016. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, and Li Zhang. Soft: Softmax-free transformer with linear complexity.",
    "eva-25": "In Thirty-Fifth Conference on Neural Information Processing Systems, 2021. Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding.",
    "eva-26": "In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.",
    "eva-27": "net/forum? id=X7XNPor93uG. Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. arXiv preprint arXiv:2106.01540, 2021. Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, and Prateek Jain. Treeformer: Dense gradient trees for efficient attention computation. arXiv preprint arXiv:2208.09015, 2022. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.",
    "eva-28": "arXiv preprint arXiv:1609.07843, 2016. Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. Tan Nguyen, Vai Suliafu, Stanley Osher, Long Chen, and Bao Wang. Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention. Advances in Neural Information Processing Systems, 34, 2021. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 1-9, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6301. URLhttps://aclanthology.org/W18-6301. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URLhttps: / aclanthology.org/N19-4009\n\nZijing Ou, Tingyang Xu, Qinliang Su, Yingzhen Li, Peilin Zhao, and Yatao Bian. Learning set functions under the optimal subset oracle via equivariant variational inference. arXiv preprint arXiv:2203.01693, 2022. Art B. Owen. Monte Carlo theory, methods and examples. 2013. John Paisley, David M. Blei, and Michael I. Jordan. Variational bayesian inference with stochastic search. In Proceedings of the 29th International Coference on International Conference on Machine Learning, ICML'12, pp. 1363-1370, Madison, WI, USA, 2012. Omnipress. ISBN 9781450312851. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, pp. 4055-4064. PMLR, 2018. Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, and Noah A Smith. Abc: Attention with bounded-memory control. arXiv preprint arXiv:2110.02488, 2021a. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In International Conference on Learning Representations, 2021b. Shawn Presser. Books3. 2020. URL https://twitter.com/theshawwn/status/ 1320282149329784833\n\nZhen Qin, XiaoDong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer.",
    "eva-29": "arXiv preprint arXiv:2210.10340, 2022a. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022b. URLhttps://openreview.net/forum?id= Bl8CQrx2Up4. Markus N Rabe and Charles Staats. Self-attention does not need $\\mathcal{O}\\left(n^{2}\\right)$ memory.",
    "eva-30": "arXiv preprint arXiv:2112.05682, 2021. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10428-10436, 2020. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URLhttps://openreview.net/forum?id=SylKikSYDH. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020a. URL http://jmlr.org/papers/v21/20-074.html. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020b. URL http://jmlr.org/papers/v21/20-074.html. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2008. URL https://proceedings.neurips.cc/ paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf. Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial intelligence and statistics, pp. 814-822. PMLR, 2014. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. Advances in Neural Information Processing Systems, 34, 2021. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021. doi: 10.1162/tacl_a_00353. URL https://aclanthology.org/2021. tacl-1.4\n\nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9355-9366. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/ schlag21a.html. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse Sinkhorn attention. In Hal Daum\u00e9 III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 9438-9447. PMLR, 13-18 Jul 2020. URLhttps://proceedings.mlr.press/v119/tay20a.html. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=qVyeW-grC2k. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers \\& distillation through attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 10347-10357. PMLR, 18-24 Jul 2021. URLhttps://proceedings.mlr.press/v139/touvron21a.html\n\nZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer.",
    "eva-31": "arXiv preprint arXiv:2204.01697, 2022. George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models.",
    "eva-32": "In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017. Nikos Vlassis, Ashok Chandrashekar, Fernando Amat, and Nathan Kallus. Control variates for slate off-policy evaluation.",
    "eva-33": "In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview. net/forum?id=e9_UPqMNfi. Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. Fast transformers with clustered attention. Advances in Neural Information Processing Systems, 33, 2020. Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gradient optimization.",
    "eva-34": "In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc., 2013. URL/https://proceedings.neurips.cc/paper/2013/file/ $9766527 f 2 b 5 d 3 e 95 d 4 a 733 f c f b 77 b d 7 e-P a p e r . p d f$. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.",
    "eva-35": "arXiv preprint arXiv:2006.04768, 2020. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021a. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer.",
    "eva-36": "arXiv preprint arXiv:2106.13797, 2021b. Yuxin Wang, Chu-Tak Lee, Qipeng Guo, Zhangyue Yin, Yunhua Zhou, Xuanjing Huang, and Xipeng Qiu. What dense graph do you need for self-attention? In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 22752-22768. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr. press/v162/wang22l.html\n\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021. Haixu Wu, Jialong Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Flowformer: Linearizing transformers with conservation flows. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2422624242. PMLR, 17-23 Jul 2022. URLhttps://proceedings.mlr.press/v162/wu22m. html\nTete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll\u00e1r, and Ross Girshick. Early convolutions help transformers see better. arXiv preprint arXiv:2106.14881, 2021. Wenhan Xiong, Barlas O\u011fuz, Anchit Gupta, Xilun Chen, Diana Liskovich, Omer Levy, Wen-tau Yih, and Yashar Mehdad. Simple local attentions remain competitive for long-context tasks. arXiv preprint arXiv:2112.07210, 2021a. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 14138-14148, 2021b. Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal attention for long-range interactions in vision transformers.",
    "eva-37": "Advances in Neural Information Processing Systems, 34, 2021. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. Advances in neural information processing systems, 30, 2017. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 17283-17297. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/ file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf\nZhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh. Multi resolution analysis (MRA) for approximate self-attention. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 25955-25972. PMLR, 17-23 Jul 2022. URLhttps://proceedings.mlr.press/v162/ zeng22a.html. Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision longformer: A new vision transformer for high-resolution image encoding.",
    "eva-38": "arXiv preprint arXiv:2103.15358, 2021. Lin Zheng, Huijie Pan, and Lingpeng Kong. Ripple attention for visual perception with sub-quadratic complexity. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 26993-27010. PMLR, 17-23 Jul 2022a. URLhttps://proceedings.mlr.press/v162/zheng22a.html\n\nLin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention mechanism. arXiv preprint arXiv:2204.04667, 2022b. Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Information Processing Systems, 34, 2021. Zhenhai Zhu and Radu Soricut. H-transformer-1D: Fast one-dimensional hierarchical attention for sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3801-3815, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.294. URL https://aclanthology.org/2021 acl-long. 294\n\n## Appendix\n\nA A Brief Review of Vanilla Random Feature Attention ..... 18\nB Proofs \\& Derivations ..... 18\nB. 1 An Extended Review of Control Variates ..... 18\nB. 2 Derivation of SNIS as Control Variate Estimation ..... 19\nB. 3 Derivation of the Expectation of Per-term Control Variates ..... 19\nB. 4 Proof of Proposition 1 . ..... 19\nB.5 A Formal Analysis of the Advantage of Partitioning ..... 20\nB. 6 Proof of Proposition 2 . ..... 21\nB. 7 Derivation of Equations 14 and 15 ..... 23\nC More Implementation Details for EVA ..... 23\nD A Causal Variant of EVA ..... 25\nE Experimental Details ..... 26\nE. 1 Efficient Attention Baselines ..... 26\nE. 2 Image Classification ..... 27\nE. 3 Machine Translation and Language Modeling ..... 29\nE. 4 Experimental Settings of Efficiency Comparison . ..... 31\nF Experiments on Long Range Arena ..... 32\nG Connections to Other Attention Mechanisms ..... 33\nG. 1 RFA, Softmax Attention, and EVA ..... 33\nG. 2 Connections to LARA ..... 33\nG. 3 Connections to Clustered Attention ..... 33\nG. 4 Connections to Combiner ..... 34\nG. 5 Connections to Scatterbrain ..... 34\n\n## A A Brief Review of Vanilla Random Feature Attention\n\nVanilla random feature attention methods, such as Performer (Choromanski et al., 2021, Peng et al. 2021b), seek to approximate the softmax attention mechanism through random features (Rahimi \\& Recht 2008) $\\boldsymbol{\\phi}(\\mathbf{x}, \\boldsymbol{\\omega}):=1 / \\sqrt{S}\\left[\\xi\\left(\\mathbf{x}, \\omega_{1}\\right), \\ldots, \\xi\\left(\\mathbf{x}, \\omega_{S}\\right)\\right]^{\\top}$. Here, $\\omega_{1}, \\ldots, \\omega_{S} \\sim \\mathcal{N}(0, \\mathbf{I})$, and $\\xi(\\mathbf{x}, \\omega)$ is the randomized mapping such that\n\n$$\n\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)=\\mathbb{E}_{\\omega_{s} \\sim \\mathcal{N}(0, \\mathbf{I})}\\left[\\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right)\\right]\n$$\n\nTherefore, we can draw multiple Monte Carlo samples to estimate the exponential kernel,\n\n$$\n\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\approx \\frac{1}{S} \\sum_{s=1}^{S} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right):=\\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right)\n$$\n\nand then approximate the attention mechanism as\n\n$$\n\\sum_{m=1}^{M} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime}=1}^{M} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)} \\mathbf{v}_{m} \\approx \\frac{\\sum_{m=1}^{M} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}}{\\sum_{m^{\\prime}=1}^{M} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m^{\\prime}}, \\boldsymbol{\\omega}\\right)}\n$$\n\nIt is recently generalized as a self-normalized importance sampling estimator to approximate softmax attention (Zheng et al., 2022b), as described in $\\$ 2.2$. We refer the generalized random feature based approximations as RFA. ## B PROOFS \\& DERIVATIONS\n\n## B. 1 An EXTENDED REVIEW OF CONTROL VARIATES\n\nThe control variate method takes the following form,\n\n$$\n\\widetilde{g}(\\boldsymbol{\\omega})=g(\\boldsymbol{\\omega})-\\boldsymbol{\\beta} h(\\boldsymbol{\\omega})+\\boldsymbol{\\beta} \\mathbb{E}[h(\\boldsymbol{\\omega})]\n$$\n\nGiven the particular forms of $g(\\cdot)$ and $h(\\cdot), \\boldsymbol{\\beta}$ can be optimized to minimize the estimation variance. For notational convenience, we denote the covariance between a scalar and a random vector as $\\operatorname{Cov}[h(\\boldsymbol{\\omega}), g(\\boldsymbol{\\omega})]:=\\mathbb{E}[(h(\\boldsymbol{\\omega})-\\mathbb{E}[h(\\boldsymbol{\\omega})])(g(\\boldsymbol{\\omega})-\\mathbb{E}[g(\\boldsymbol{\\omega})])]$, and the variance of a random vector as $\\operatorname{Var}[g(\\boldsymbol{\\omega})]:=\\operatorname{Cov}[g(\\boldsymbol{\\omega}), g(\\boldsymbol{\\omega})]$. In particular, we have\n\n$$\n\\begin{aligned}\n\\operatorname{Var}[\\widetilde{g}(\\boldsymbol{\\omega})] & =\\operatorname{Var}[g(\\boldsymbol{\\omega})-\\boldsymbol{\\beta} h(\\boldsymbol{\\omega})] \\\\\n& =\\operatorname{Var}[g(\\boldsymbol{\\omega})]-2 \\operatorname{Cov}[\\boldsymbol{\\beta} h(\\boldsymbol{\\omega}), g(\\boldsymbol{\\omega})]+\\operatorname{Var}[\\boldsymbol{\\beta} h(\\boldsymbol{\\omega})] \\\\\n& =\\operatorname{Var}[g(\\boldsymbol{\\omega})]-2 \\operatorname{Cov}[h(\\boldsymbol{\\omega}), g(\\boldsymbol{\\omega})] \\boldsymbol{\\beta}^{\\top}+\\operatorname{Var}[h(\\boldsymbol{\\omega})] \\boldsymbol{\\beta} \\boldsymbol{\\beta}^{\\top}\n\\end{aligned}\n$$\n\nWe hope an optimal $\\boldsymbol{\\beta}$ would minimize $\\operatorname{Tr}(\\operatorname{Var}[\\widetilde{g}(\\boldsymbol{\\omega})])$, that is, the sum of estimating variance for each dimension. By differentiating, we obtain\n\n$$\n\\boldsymbol{\\beta}^{*}=\\arg \\min _{\\boldsymbol{\\beta}} \\operatorname{Tr}(\\operatorname{Var}[\\widetilde{g}(\\boldsymbol{\\omega})])=\\frac{\\operatorname{Cov}[h(\\boldsymbol{\\omega}), g(\\boldsymbol{\\omega})]}{\\operatorname{Var}[h(\\boldsymbol{\\omega})]}\n$$\n\nSince both the covariance and the variance may be intractable to compute, the optimal $\\boldsymbol{\\beta}^{*}$ is generally not available in closed form. Nevertheless, with the optimal coefficient, the variance of such control variate estimate would never be larger than the plain estimator $g(\\cdot)$. ## B. 2 DERIVATION OF SNIS aS CONTrol VARIATE ESTIMATION\n\nFor notational convenience, we denote the importance weight as $W\\left(\\omega_{s}\\right):=p_{n}\\left(\\omega_{s}\\right) / q\\left(\\omega_{s}\\right)$. Then we have\n\n$$\n\\begin{aligned}\n\\widetilde{g}(\\boldsymbol{\\omega}) & =\\frac{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}}=\\frac{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)} \\\\\n& =\\frac{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)}-\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right)+\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right) \\\\\n& =\\frac{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)}-\\frac{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)} \\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right)+\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right) \\\\\n& =\\frac{1-\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right)+\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right) \\\\\n& =\\frac{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)}\\left(1-\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)\\right)+\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right) \\\\\n& =\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right)-\\frac{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right) f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)}\\left(\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)-1\\right) \\\\\n& =g(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})(h(\\boldsymbol{\\omega})-\\mathbb{E}[h(\\boldsymbol{\\omega})])\n\\end{aligned}\n$$\n\nNote that the expectation of importance weights equals 1, that is,\n\n$$\n\\mathbb{E}[h(\\boldsymbol{\\omega})]=\\mathbb{E}\\left[\\frac{1}{S} \\sum_{s=1}^{S} W\\left(\\omega_{s}\\right)\\right]=\\mathbb{E}_{\\omega_{1}, \\ldots, \\omega_{S} \\sim q(\\omega)}\\left[\\sum_{s=1}^{S} \\frac{1}{S} \\frac{p\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}\\right]=\\frac{1}{S} \\sum_{s=1}^{S} \\mathbb{E}_{\\omega_{s} \\sim q(\\omega)}\\left[\\frac{p\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}\\right]=1\n$$\n\nSame as SNIS, this estimator is still biased due to the dependence of $\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})$ on $\\boldsymbol{\\omega}$.",
    "eva-39": "However, it would asymptotically become unbiased since $\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})$ is consistent and converges to a constant $\\boldsymbol{\\beta}$ w.r.t. $\\omega$ given a large number of samples,\n\n$$\n\\widehat{\\boldsymbol{\\beta}}(\\boldsymbol{\\omega})=\\frac{g(\\boldsymbol{\\omega})}{h(\\boldsymbol{\\omega})} \\stackrel{p}{\\rightarrow} \\frac{\\mathbb{E}[g(\\boldsymbol{\\omega})]}{\\mathbb{E}[h(\\boldsymbol{\\omega})]}=\\underbrace{\\mathbb{E}_{p_{n}(\\omega)}[f(\\omega)]}_{\\text {constant }}:=\\boldsymbol{\\beta}\n$$\n\n## B. 3 DERIVATION of the Expectation of PER-TERm CONTROL VARIATES\n\nAccording to the definition of randomized mappings, we have\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right] & =\\mathbb{E}_{\\omega_{1}, \\ldots, \\omega_{S} \\sim q(\\omega)}\\left[\\frac{1}{S} \\sum_{s=1}^{S} \\frac{\\mathcal{N}\\left(\\omega_{s} ; 0, \\mathbf{I}\\right)}{Z q\\left(\\omega_{s}\\right)} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right)\\right] \\\\\n& =\\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{Z} \\int \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right) \\mathcal{N}\\left(\\omega_{s} ; 0, \\mathbf{I}\\right) d \\omega_{s} \\\\\n& =\\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z}\n\\end{aligned}\n$$\n\n## B. 4 Proof of Proposition 1\n\nProof. We start with the formulation of $g(\\cdot)$ and $h(\\cdot)$,\n\n$$\n\\frac{g_{m}(\\boldsymbol{\\omega})}{h_{m}(\\boldsymbol{\\omega})}=\\frac{\\sum_{s=1}^{S} \\frac{\\mathcal{N}\\left(\\omega_{s} ; 0, \\mathbf{I}\\right)}{Z q\\left(\\omega_{s}\\right)} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right) \\mathbf{v}_{m}}{\\sum_{s=1}^{S} \\frac{\\mathcal{N}\\left(\\omega_{s} ; 0, \\mathbf{I}\\right)}{Z q\\left(\\omega_{s}\\right)} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right)}=\\mathbf{v}_{m}\n$$\n\nAs a result, we have $g_{m}(\\boldsymbol{\\omega})=h_{m}(\\boldsymbol{\\omega}) \\mathbf{v}_{m}$ and $\\mathbb{E}\\left[g_{m}(\\boldsymbol{\\omega})\\right]=\\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right] \\mathbf{v}_{m}$. We now investigate the optimal $\\boldsymbol{\\beta}_{m}$ according to Equation 20 . $$\n\\begin{aligned}\n\\boldsymbol{\\beta}_{m}^{*} & =\\arg \\min _{\\boldsymbol{\\beta}} \\operatorname{Tr}\\left(\\operatorname{Var}\\left[\\widetilde{g}_{m}(\\boldsymbol{\\omega})\\right]\\right) \\\\\n& =\\frac{\\operatorname{Cov}\\left[h_{m}(\\boldsymbol{\\omega}), g_{m}(\\boldsymbol{\\omega})\\right]}{\\operatorname{Var}\\left[h_{m}(\\boldsymbol{\\omega})\\right]} \\\\\n& =\\frac{\\mathbb{E}[(h(\\boldsymbol{\\omega})-\\mathbb{E}[h(\\boldsymbol{\\omega})])(h(\\boldsymbol{\\omega})-\\mathbb{E}[h(\\boldsymbol{\\omega})])] \\mathbf{v}_{m}}{\\mathbb{E}[(h(\\boldsymbol{\\omega})-\\mathbb{E}[h(\\boldsymbol{\\omega})])(h(\\boldsymbol{\\omega})-\\mathbb{E}[h(\\boldsymbol{\\omega})])]} \\\\\n& =\\mathbf{v}_{m}=\\frac{g_{m}(\\boldsymbol{\\omega})}{h_{m}(\\boldsymbol{\\omega})}\n\\end{aligned}\n$$\n\nIn terms of the variance, we again use $g_{m}(\\boldsymbol{\\omega})=h_{m}(\\boldsymbol{\\omega}) \\mathbf{v}_{m}$ to obtain\n\n$$\n\\begin{aligned}\n\\widetilde{g}_{m}(\\boldsymbol{\\omega}) & =g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}}_{m}\\left(h_{m}(\\boldsymbol{\\omega})-\\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right]\\right) \\\\\n& =g_{m}(\\boldsymbol{\\omega})-\\mathbf{v}_{m} h_{m}(\\boldsymbol{\\omega})+\\mathbf{v}_{m} \\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right] \\\\\n& =\\mathbf{v}_{m} \\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right] \\\\\n& =\\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\mathbf{v}_{m}\n\\end{aligned}\n$$\n\nSince this holds true for every term $m=1, \\ldots, M$, our estimate becomes exactly softmax attention,\n\n$$\n\\widetilde{g}(\\boldsymbol{\\omega})=\\sum_{m=1}^{M} \\tilde{g}_{m}(\\boldsymbol{\\omega})=\\sum_{m=1}^{M} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\mathbf{v}_{m}=\\sum_{m=1}^{M} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime}=1}^{M} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)} \\mathbf{v}_{m}\n$$\n\nSince all randomness is eliminated, the estimate is exact with zero bias and variance. That is, $\\operatorname{RFA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right)=\\widetilde{g}(\\boldsymbol{\\omega})=\\operatorname{SoftmaxAttn}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right)$\n\n## B. 5 A Formal Analysis of the Advantage of PARTItioning\n\nIn this section, we demonstrate the advantage of partitioning by showing that there always exists some set $\\left\\{\\boldsymbol{\\beta}_{c}\\right\\}_{c=1}^{C}$ that achieves lower weighted MSE than any globally shared coefficient, as discussed in $\\S 4.1$\nLemma 3. Suppose $\\boldsymbol{\\beta}_{m}^{*}$ is the optimal coefficient for each $m \\in[M]$ as defined in Proposition 1 and $\\mathcal{P}_{1}, \\mathcal{P}_{2}, \\ldots, \\mathcal{P}_{C}$ are an arbitrary partition of $[M]$, where each subset $\\mathcal{P}_{c}$ is associated with a distinct $\\boldsymbol{\\beta}_{c}$. We consider the following weighted mean squared error,\n\n$$\nJ\\left(\\boldsymbol{\\beta}_{1}, \\ldots, \\boldsymbol{\\beta}_{C}\\right):=\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left\\|\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2}\n$$\n\nwhere $\\alpha_{m}>0$ for each $m \\in[M]$ and $\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}=1$. Then for any choice of $\\left\\{\\alpha_{m}\\right\\}_{m=1}^{M}$ and any globally shared coefficient $\\boldsymbol{\\beta}$, there exists some $\\left\\{\\boldsymbol{\\beta}_{c}^{*}\\right\\}_{c=1}^{C}$ so that\n\n$$\nJ\\left(\\boldsymbol{\\beta}_{1}=\\boldsymbol{\\beta}, \\ldots, \\boldsymbol{\\beta}_{C}=\\boldsymbol{\\beta}\\right) \\geq J\\left(\\boldsymbol{\\beta}_{1}=\\boldsymbol{\\beta}_{1}^{*}, \\ldots, \\boldsymbol{\\beta}_{C}=\\boldsymbol{\\beta}_{C}^{*}\\right)\n$$\n\nProof. Let $\\boldsymbol{\\beta}_{c}^{*}=\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m} \\boldsymbol{\\beta}_{m}^{*}}{\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}}$ for each $c=1, \\ldots, C$. Then we have\n\n$$\n\\begin{aligned}\n\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left(\\boldsymbol{\\beta}_{c}^{*}-\\boldsymbol{\\beta}_{m}^{*}\\right) & =\\boldsymbol{\\beta}_{c}^{*}\\left(\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\right)-\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m} \\boldsymbol{\\beta}_{m}^{*} \\\\\n& =\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m} \\boldsymbol{\\beta}_{m}^{*}}{\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}}\\left(\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\right)-\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m} \\boldsymbol{\\beta}_{m}^{*} \\\\\n& =\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m} \\boldsymbol{\\beta}_{m}^{*}-\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m} \\boldsymbol{\\beta}_{m}^{*}=0\n\\end{aligned}\n$$\n\nAccording to Equations 25 and 24 for any $\\boldsymbol{\\beta}$ we have the following inequality,\n\n$$\n\\begin{aligned}\n& J\\left(\\boldsymbol{\\beta}_{1}=\\boldsymbol{\\beta}, \\ldots, \\boldsymbol{\\beta}_{C}=\\boldsymbol{\\beta}\\right) \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left\\|\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left\\|\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_{c}^{*}+\\boldsymbol{\\beta}_{c}^{*}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left(\\left\\|\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_{c}^{*}\\right\\|^{2}+2\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_{c}^{*}\\right)^{\\top}\\left(\\boldsymbol{\\beta}_{c}^{*}-\\boldsymbol{\\beta}_{m}^{*}\\right)+\\left\\|\\boldsymbol{\\beta}_{c}^{*}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2}\\right) \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left\\|\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_{c}^{*}\\right\\|^{2}+2 \\underbrace{\\sum_{m \\in \\mathcal{P}_{c}}^{C} \\sum_{m}\\left(\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_{c}^{*}\\right)^{\\top}\\left(\\boldsymbol{\\beta}_{c}^{*}-\\boldsymbol{\\beta}_{m}^{*}\\right)}_{c=1}+\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left\\|\\boldsymbol{\\beta}_{c}^{*}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left\\|\\boldsymbol{\\beta}-\\boldsymbol{\\beta}_{c}^{*}\\right\\|^{2}+\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left\\|\\boldsymbol{\\beta}_{c}^{*}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& \\geq \\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}\\left\\|\\boldsymbol{\\beta}_{c}^{*}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& =J\\left(\\boldsymbol{\\beta}_{1}=\\boldsymbol{\\beta}_{1}^{*}, \\ldots, \\boldsymbol{\\beta}_{C}=\\boldsymbol{\\beta}_{C}^{*}\\right) . \\end{aligned}\n$$\n\nAs a result, for any choice of $\\left\\{\\alpha_{m}\\right\\}_{m=1}^{M}$ and any globally shared coefficient $\\boldsymbol{\\beta}$, there always exists some $\\left\\{\\boldsymbol{\\beta}_{c}\\right\\}_{C=1}^{C}$ that achieves lower (or equal) weighted MSE, and a solution can be simply $\\boldsymbol{\\beta}_{c}=$ $\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m} \\boldsymbol{\\beta}_{m}^{*}}{\\sum_{m \\in \\mathcal{P}_{c}} \\alpha_{m}}$\n\n## B. 6 Proof of Proposition 2\n\nProof. We first consider the case of partitioned indices, where each subset $\\mathcal{P}_{c}$ is associated with some specific $\\boldsymbol{\\beta}_{c}$. To see the global minimum of $J$, we differentiate on both sides and obtain\n\n$$\n\\begin{aligned}\n\\frac{\\partial J\\left(\\boldsymbol{\\beta}_{1}, \\ldots, \\boldsymbol{\\beta}_{C}\\right)}{\\partial \\boldsymbol{\\beta}_{c}} & =\\frac{\\partial}{\\partial \\boldsymbol{\\beta}_{c}} \\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left\\|\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& =\\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)} 2\\left(\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right)\n\\end{aligned}\n$$\n\nBy setting the partial derivative to zero, we obtain\n\n$$\n\\begin{aligned}\n\\boldsymbol{\\beta}_{c}^{*} & =\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\boldsymbol{\\beta}_{m}^{*}}{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}=\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)} \\\\\n& =\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\mathbb{E}\\left[g_{m}(\\boldsymbol{\\omega})\\right]}{\\sum_{m \\in \\mathcal{P}_{c}} \\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right]}=\\frac{\\mathbb{E}\\left[\\sum_{m \\in \\mathcal{P}_{c}} g_{m}(\\boldsymbol{\\omega})\\right]}{\\mathbb{E}\\left[\\sum_{m \\in \\mathcal{P}_{c}} h_{m}(\\boldsymbol{\\omega})\\right]}\n\\end{aligned}\n$$\n\nAs a consequence, with $\\boldsymbol{\\beta}_{c}=\\boldsymbol{\\beta}_{c}^{*}$, the partition scheme must achieve lower weighted mean squared error than any globally shared $\\widehat{\\boldsymbol{\\beta}}$, that is, $J\\left(\\boldsymbol{\\beta}_{1}=\\boldsymbol{\\beta}_{1}^{*}, \\ldots, \\boldsymbol{\\beta}_{C}=\\boldsymbol{\\beta}_{C}^{*}\\right) \\leq J\\left(\\boldsymbol{\\beta}_{1}=\\widehat{\\boldsymbol{\\beta}}, \\ldots, \\boldsymbol{\\beta}_{C}=\\widehat{\\boldsymbol{\\beta}}\\right)$. In fact, with $\\boldsymbol{\\beta}_{c}=\\boldsymbol{\\beta}_{c}^{*}$, the partition scheme usually enjoys much lower error than adopting a globally shared coefficient. To see the error reduction of using the partitioned strategy, we first have\n\n$$\n\\begin{aligned}\n& J\\left(\\boldsymbol{\\beta}_{1}=\\widehat{\\boldsymbol{\\beta}}, \\ldots, \\boldsymbol{\\beta}_{C}=\\widehat{\\boldsymbol{\\beta}}\\right) \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left\\|\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left\\|\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}+\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left\\|\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}+\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left(\\left\\|\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right\\|^{2}+\\left(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right)^{\\top}\\left(\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right)+\\left\\|\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2}\\right)\n\\end{aligned}\n$$\n\nSince\n\n$$\n\\begin{aligned}\n& \\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right)^{\\top}\\left(\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right) \\\\\n& =\\sum_{c=1}^{C}\\left(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right)^{\\top} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left(\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right) \\\\\n& =\\sum_{c=1}^{C}\\left(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right)^{\\top} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left(\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}-\\boldsymbol{\\beta}_{m}^{*}\\right) \\\\\n& =\\sum_{c=1}^{C}\\left(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right)^{\\top} \\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)\\left(\\mathbf{v}_{m}-\\boldsymbol{\\beta}_{m}^{*}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)} \\\\\n& =0\n\\end{aligned}\n$$\n\nplugging this result back we obtain\n\n$$\n\\begin{aligned}\n& J\\left(\\boldsymbol{\\beta}_{1}=\\widehat{\\boldsymbol{\\beta}}, \\ldots, \\boldsymbol{\\beta}_{C}=\\widehat{\\boldsymbol{\\beta}}\\right) \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left(\\left\\|\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right\\|^{2}+\\left(\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right)^{\\top}\\left(\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right)+\\left\\|\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2}\\right) \\\\\n& =\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left(\\left\\|\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right\\|^{2}+\\left\\|\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2}\\right) \\\\\n& =\\underbrace{\\sum_{c=1}^{C} \\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left\\|\\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}_{c}\\right\\|^{2}}_{\\geq=1}+\\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left\\|\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2} \\\\\n& \\geq \\sum_{c=1}^{C} \\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in U} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\\left\\|\\boldsymbol{\\beta}_{c}-\\boldsymbol{\\beta}_{m}^{*}\\right\\|^{2}\n\\end{aligned}\n$$\n\nThe last inequality holds since the first term is always non-negative. Note that the first term computes the squared error between $\\widehat{\\boldsymbol{\\beta}}$ and each $\\boldsymbol{\\beta}_{c}$, weighted by the sum of attention scores over the corresponding subset. As a result, it is usually positive and the error reduction is significant if each $\\boldsymbol{\\beta}_{c}$ deviates from $\\widehat{\\boldsymbol{\\beta}}$ a lot. However, although the optimal coefficient in the partitioning always leads to lower error to the optimal individual coefficient, note that it does not necessarily yield lower estimation variance. Table 8: Classification results on ImageNet 1 k dataset under different hyper-parameter configurations of EVA. By default, we set $|E|=49$ and $C=49$ across all variants below. | Component | Specification | Top-1 Acc. |\n| :---: | :---: | :---: |\n| Partition Scheme of $\\left\\{\\mathcal{P}_{1}, \\ldots, \\mathcal{P}_{C}\\right\\}$ | partition over $[M] \\backslash E$ <br> partition over $[M]$ | 76.53 <br> 76.39 |\n| Parameterization of $\\sigma(\\cdot)$ | $\\sigma(\\cdot)=\\operatorname{LN}(\\operatorname{Linear}(\\cdot))$ <br> $\\sigma(\\cdot)=\\operatorname{Identity}(\\cdot)$ | 76.67 <br> 75.95 |\n| Number of Groups $(C=1)$ | Number of Samples $=1$ <br> Number of Samples $=49$ | 74.10 <br> 76.39 |\n| Number of Groups ( $C=49$ ) | Number of Samples $=1$ <br> Number of Samples $=49$ | 76.67 <br> 76.75 |\n| Proposal Parameterization $q_{c}(\\omega):=\\mathcal{N}\\left(\\omega ; \\boldsymbol{\\mu}_{c}, \\mathbf{I}\\right)$ | $\\boldsymbol{\\mu}_{c}=\\widetilde{\\mathbf{q}}_{c}+\\widetilde{\\mathbf{k}}_{c}$ <br> $\\boldsymbol{\\mu}_{c}=\\widetilde{\\mathbf{q}}_{c}$ <br> $\\boldsymbol{\\mu}_{c}=\\mathbf{0}$ <br> $\\boldsymbol{\\mu}_{c}=$ Trainable parameters | 76.67 <br> 76.77 <br> 76.24 <br> 76.39 |\n| Softmax |  | 77.16 |\n\n## B. 7 DERIVATIon of EQUATIons 14 and 15\n\nAccording to the definition of $g_{m}(\\cdot)$ and $h_{m}(\\cdot)$ in $\\$ 3.1$ for the vanilla RFA (Equation 4 we have\n\n$$\n\\operatorname{RFA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right)=\\frac{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}}=\\frac{g(\\boldsymbol{\\omega})}{h(\\boldsymbol{\\omega})}=\\frac{\\sum_{m=1}^{M} g_{m}(\\boldsymbol{\\omega})}{\\sum_{m=1}^{M} h_{m}(\\boldsymbol{\\omega})}\n$$\n\nBesides, since $\\mathbf{v}_{m}=g_{m}(\\boldsymbol{\\omega}) / h_{m}(\\boldsymbol{\\omega})$ and $\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})=\\left(\\sum_{m \\in \\mathcal{P}_{c}} g_{m}(\\boldsymbol{\\omega})\\right) /\\left(\\sum_{m \\in \\mathcal{P}_{c}} h_{m}(\\boldsymbol{\\omega})\\right)$, we can re-write EVA as\n\n$$\n\\begin{aligned}\n\\operatorname{EVA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right) & :=\\widetilde{g}(\\boldsymbol{\\omega}) \\\\\n& =\\sum_{m \\in E} \\widetilde{g}_{m}(\\boldsymbol{\\omega})+\\sum_{m \\notin E} \\widetilde{g}_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\mathbf{v}_{m}+\\sum_{c=1}^{C} \\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega}) \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\frac{g_{m}(\\boldsymbol{\\omega})}{h_{m}(\\boldsymbol{\\omega})}+\\sum_{c=1}^{C} \\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\frac{\\sum_{m \\in \\mathcal{P}_{c}} g_{m}(\\boldsymbol{\\omega})}{\\sum_{m \\in \\mathcal{P}_{c}} h_{m}(\\boldsymbol{\\omega})}\n\\end{aligned}\n$$\n\n## C MORE IMPLEMENTATION DETAILS FOR EVA\n\nIn this section, we provide more details of EVA. We also conduct a comprehensive ablation study to test the effect of different components in our implementation and report the results in Table 8. The pseudo-code for EVA is listed in Algorithm 1\n\nApproximating $\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)$ and Parameterizing $\\widetilde{\\mathbf{k}}_{c}$. In our implementation, we approximate the sum of exponentials as $\\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\approx \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right)$. Here we provide an informal justification for this approximation. Our main motivation for such approximation is based on the simple intuition that the sum of exponentials grows as fast as the maximum exponential value, as reflected by the following inequality,\n\n$$\n\\max _{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\leq \\sum_{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\leq\\left|\\mathcal{P}_{c}\\right| \\max _{m \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)\n$$\n\nThis means we can approximate the sum of exponentials by first computing the group representative $\\widetilde{\\mathbf{k}}_{c}:=\\arg \\max _{\\mathbf{k}_{m} \\in\\left\\{\\mathbf{k}_{m} \\mid m \\in \\mathcal{P}_{c}\\right\\}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)$, evaluating the corresponding exponential $\\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right)$\nand then multiplying it by some scalar. Since computing the argmax operation still needs to compare each exponential dot-product, it will still incur quadratic computational costs. To circumvent this, we adopt a heuristic strategy that computes a learnable group representation, which attempts to compensate for the approximation error while only evaluating one exponential dot product. Through preliminary experiments, we try various choices to compute the representative vector of each subset, such as max and average pooling; however, we found these strategies produce almost equally good performance. As a result, we adopt the average pooling by default due to its simplicity. To be specific, we implement it as\n\n$$\n\\widetilde{\\mathbf{k}}_{c}=\\sigma\\left(\\frac{1}{\\left|\\mathcal{P}_{c}\\right|} \\sum_{m \\in \\mathcal{P}_{c}} \\mathbf{k}_{m}\\right)\n$$\n\nwhere $\\sigma(\\cdot)$ is a trainable linear projection with the same hidden dimension size as inputs, followed by a layer normalization operation ( Ba et al. 2016) to stabilize training. We leave further improving the approximation, such as deriving tighter error bounds or using more expressive pooling methods (Zaheer et al., 2017, Ou et al., 2022) as future work. Parameterizing $\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})$. As discussed in $\\S 4.1$ we have\n\n$$\n\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})=\\frac{\\sum_{m \\in \\mathcal{P}_{c}} g_{m}(\\boldsymbol{\\omega})}{\\sum_{m \\in \\mathcal{P}_{c}} h_{m}(\\boldsymbol{\\omega})}=\\frac{\\sum_{s=1}^{S} \\frac{\\mathcal{N}\\left(\\omega_{s} ; 0, \\mathbf{I}\\right)}{Z q\\left(\\omega_{s}\\right)} \\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right) \\mathbf{v}_{m}}{\\sum_{s=1}^{S} \\frac{\\mathcal{N}\\left(\\omega_{s} ; 0, \\mathbf{I}\\right)}{Z q\\left(\\omega_{s}\\right)} \\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right)}\n$$\n\nCompared to the SNIS formulation of vanilla RFA Equation 4 we can express it as\n\n$$\n\\operatorname{RFA}\\left(\\mathbf{q}_{n}, \\mathbf{K}, \\mathbf{V}\\right)=\\frac{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)}{\\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}}=\\frac{\\sum_{m=1}^{M} g_{m}(\\boldsymbol{\\omega})}{\\sum_{m=1}^{M} h_{m}(\\boldsymbol{\\omega})}\n$$\n\nWe can think of each coefficient $\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})$ as computing the output of a localized RFA for each group $\\mathcal{P}_{c}$. From this perspective, we can recast each coefficient $\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})$ as an SNIS estimator as well, which tries to estimate\n\n$$\n\\mathbb{E}_{\\omega \\sim p_{c}(\\omega)}\\left[f_{c}(\\omega)\\right]=\\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)} \\mathbf{v}_{m}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\nf_{c}(\\omega) & :=\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega\\right) \\mathbf{v}_{m}}{\\sum_{m^{\\prime} \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right) \\xi\\left(\\mathbf{k}_{m^{\\prime}}, \\omega\\right)} \\\\\np_{c}(\\omega) & :=\\frac{\\mathcal{N}(\\omega ; 0, \\mathbf{I}) \\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)}{\\sum_{m^{\\prime} \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)} \\\\\n& =\\sum_{m \\in \\mathcal{P}_{c}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{\\sum_{m^{\\prime} \\in \\mathcal{P}_{c}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)} \\mathcal{N}\\left(\\omega ; \\mathbf{q}_{n}+\\mathbf{k}_{m}, \\mathbf{I}\\right)\n\\end{aligned}\n$$\n\nThis interpretation indicates that a good proposal distribution $q_{c}(\\omega)$ should be specific to each subset $\\mathcal{P}_{c}$. To get close to the true distribution $p_{c}(\\omega)$ while keeping efficient computation, Zheng et al. (2022b) suggests parameterizing the proposal distribution as\n\n$$\nq_{c}(\\omega):=\\mathcal{N}\\left(\\omega ; \\mu_{c}, \\mathbf{I}\\right)=\\mathcal{N}\\left(\\omega ; \\widetilde{\\mathbf{q}}_{c}+\\widetilde{\\mathbf{k}}_{c}, \\mathbf{I}\\right)\n$$\n\nwhere $\\widetilde{\\mathbf{q}}_{c}$ is calculated similarly to Equation 26. We refer readers to Zheng et al. (2022b) for more discussions about the parameterization choice of proposal distributions. We conduct further ablation studies to test the effect of proposal parameterizations in our proposed model, as shown in Table 8 In particular, we found our model is robust to different parameterization approaches. The essence in making the algorithm memory-efficient is to use only one sample in calculating $\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})$. In this case, we have\n\n$$\n\\begin{aligned}\n\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega}) & =\\frac{\\sum_{m \\in \\mathcal{P}_{c}} g_{m}(\\boldsymbol{\\omega})}{\\sum_{m \\in \\mathcal{P}_{c}} h_{m}(\\boldsymbol{\\omega})} \\\\\n& =\\frac{\\frac{\\mathcal{N}\\left(\\omega^{c} ; 0, \\mathbf{I}\\right)}{Z q_{c}\\left(\\omega^{c}\\right)} \\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{q}_{n}, \\omega^{c}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega^{c}\\right) \\mathbf{v}_{m}}{\\frac{\\mathcal{N}\\left(\\omega^{c} ; 0, \\mathbf{I}\\right)}{Z q_{c}\\left(\\omega^{c}\\right)} \\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{q}_{n}, \\omega^{c}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega^{c}\\right)} \\\\\n& =\\frac{\\frac{\\mathcal{N}\\left(\\omega^{c} ; 0, \\mathbf{I}\\right)}{Z q_{c}\\left(\\omega^{c}\\right)} \\xi\\left(\\mathbf{q}_{n}, \\omega^{c}\\right) \\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{k}_{m}, \\omega^{c}\\right) \\mathbf{v}_{m}}{\\frac{\\mathcal{N}\\left(\\omega^{c} ; 0, \\mathbf{I}\\right)}{Z q_{c}\\left(\\omega^{c}\\right)} \\xi\\left(\\mathbf{q}_{n}, \\omega^{c}\\right) \\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{k}_{m}, \\omega^{c}\\right)} \\\\\n& =\\frac{\\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{k}_{m}, \\omega^{c}\\right) \\mathbf{v}_{m}}{\\sum_{m \\in \\mathcal{P}_{c}} \\xi\\left(\\mathbf{k}_{m}, \\omega^{c}\\right)}, \\quad w^{c} \\sim q_{c}(\\omega)\n\\end{aligned}\n$$\n\nSince this degenerated formulation eliminates the dependence on individual queries $\\mathbf{q}_{n}$, we could precompute $\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})$ for each $\\mathcal{P}_{c}$, and then re-uses them for each query, which takes up $\\mathcal{O}(C d)$ memory. If multiple samples are used instead, the influence of queries needs to be explicitly taken into account and thus we need to compute a distinct $\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})$ for each query, leading to $\\mathcal{O}(N C d)$ memory usage, which incurs a significant compute overhead. On the other hand, if we set $C=1$, that is, using a shared $\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})$ over all $m \\notin E$, our approach does not suffer from this issue, since the memory usage is at most $\\mathcal{O}(N d)$. To investigate the effect of using larger $C$ or increasing the number of samples, we conduct an ablative analysis as in Table 8 , and find that 1) when $C=1$, the performance degrades a lot when using one sample, which can be largely improved by adopting more samples; while when $C>1$, our partitioning strategy dominates and increasing the number of samples only improves performance marginally.",
    "eva-40": "This also validates the effectiveness of adopting a finer-grained treatment over control variates. Partitioning Strategy. EVA significantly improves random feature approximation by trying to locally estimate each subset of tokens, which is a much easier task than approximating the whole sequence as in previous RFA methods. To achieve this, EVA partitions the whole token sequence into multiple subsets according to the current query position $n$, which is denoted by $\\left\\{E^{n}, \\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}_{n=1}^{N} \\square^{3}$ For elements in subset $E^{n}$, we optimize the control variate coefficient to give an exact estimate for each single token $m \\in E^{n}$. In addition, we impose T5-style relative positional encoding (Raffel et al. 2020a) over elements in $E^{n}$. While for some other subset $\\mathcal{P}_{c}$, we employ the shared coefficient to approximate all tokens belonging to $\\mathcal{P}_{c}$. We assume all $E^{1}, \\ldots, E^{N}$ are of the same cardinality $K$, and $\\left|\\mathcal{P}_{c}^{n}\\right|$ is the same for any $c=1, \\ldots, C$ and $n=1, \\ldots, N$. The partition strategy $\\left\\{E^{n}, \\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}_{n=1}^{N}$ is decided based on a simple criterion:\n\n- for $E^{n}$, it contains $K$ local neighbors with respect to each query $n$. To further simplify implementation and reduce memory usage, we chunk the whole sequence into contiguous blocks of size $K$, and all adjacent queries belonging to the same block will share this block as the subset $E^{n}$;\n- as for $\\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}$, we follow a similar treatment by splitting the complement $[M] \\backslash E^{n}$ into $C$ contiguous chunks of the same size. For ease of implementation, we simply partition the whole index set $[M]$ into multiple groups instead of $[M] \\backslash E^{n}$, which circumvents the overload for explicitly performing set difference operations in practical implementation. Although this leads to extra approximation error, this amounts to putting more attention weights on tokens belonging to the subset $E$ and we found this approximation does not lead to performance degradation (Table 8). ## D A CAuSAl VARIANT OF EVA\n\nIn this section, we describe the causal variant of EVA, where each query can only attend to historical tokens. Thanks to the partitioning scheme, all future information with respect to the current query token can be masked conveniently. Following the formulation of EVA, we partition the whole sequence into $C+1$ subsets $\\left\\{E^{n}, \\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}$ with respect to each query $\\mathbf{q}_{n}$. To fulfill the\n\n[^2]```\nAlgorithm 1 Pseudo-code for EVA\n    Input: the randomized mapping \\(\\xi(\\cdot, \\cdot)\\), queries \\(\\mathbf{Q}:=\\left\\{\\mathbf{q}_{n}\\right\\}_{n=1}^{N}\\), keys \\(\\mathbf{K}:=\\left\\{\\mathbf{k}_{m}\\right\\}_{m=1}^{M}\\), values\n    \\(\\mathbf{V}:=\\left\\{\\mathbf{v}_{m}\\right\\}_{m=1}^{M}\\) and partitions of the sequence \\(\\left\\{E^{n}, \\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}_{n=1}^{N} ;\\)\n    Output: attention output \\(\\mathbf{Y}:=\\left\\{\\mathbf{y}_{n}\\right\\}_{n=1}^{N}\\);\n    for \\(c=1,2, \\ldots, C\\) do\n        Compute \\(\\widetilde{\\mathbf{k}}_{c}\\) according to Equation 26\n        Compute \\(q_{c}(\\omega)\\) according to Equation 28\n        Sample \\(\\omega_{c} \\sim q_{c}(\\omega) ; \\quad \\triangleright\\) During inference, simply set \\(\\omega_{c}=\\mathbb{E}_{q_{c}(\\omega)}[\\omega]\\)\n        Compute \\(\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})=\\sum_{m \\in \\mathcal{P}_{c}^{n}} \\frac{\\xi\\left(\\mathbf{k}_{m}, \\omega_{c}\\right)}{\\sum_{m \\in \\mathcal{P}_{c}^{n}} \\xi\\left(\\mathbf{k}_{m}, \\omega_{c}\\right)} \\mathbf{v}_{m}\\)\n    end for\n    for \\(n=1,2, \\ldots, N\\) do\n        Compute \\(\\mathcal{S}=\\sum_{m \\in E^{n}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m} ; \\triangleright\\) Compute attention scores in the selected subset \\(E\\)\n        Compute \\(\\mathcal{R}=\\sum_{c=1}^{C} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right) \\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega}) ; \\quad \\triangleright\\) Compute approx. expected control variates\n        Compute \\(Z=\\sum_{m \\in E^{n}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{c=1}^{C} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right)\\);\n        Compute \\(\\mathbf{y}_{n}=(\\mathcal{S}+\\mathcal{R}) / Z\\)\n    end for\n    Return \\(\\mathbf{Y}:=\\left[\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{N}\\right]\\)\n```\n\ncausal requirement, we design two different types of masking matrices to deal with both $E^{n}$ and $\\left\\{\\mathcal{P}_{c}^{n}\\right\\}_{c=1}^{C}$ respectively. - For $E^{n}$, we adopt a single lower-triangular matrix with shape $K \\times K$ (recall that each set $E^{n}$ is of size $K$ ) to mask future tokens locally, similar to the case of standard decoder softmax attention. Future tokens that do not belong to $E^{n}$ are handled by masking functions for $\\left\\{\\mathcal{P}_{c}^{n}\\right\\}_{c=1}^{C}$, as described below. - For $\\left\\{\\mathcal{P}_{c}^{n}\\right\\}_{c=1}^{C}$, we make use of the fact $n \\in E^{n}$. Since any $\\mathcal{P}_{c}^{n}$ and $E^{n}$ are disjoint, we only need to mask all subsets $\\mathcal{P}_{c}^{n}$ that appear after $E^{n}$. This amounts to first allocating a lower-triangular matrix with shape $C \\times C$, and then conducting future masking at a subset level. The pseudo-code for the causal variant of EVA is listed in Algorithm2. ## E EXPERIMENTAL DETAILS\n\nAll of our experiments are conducted with at most 16 NVIDIA V100 GPUs. ## E. 1 Efficient AtTENTION BASELINES\n\nWe compare our proposed attention mechanism EVA against various baselines:\n\n- Performer (Choromanski et al., 2021), which uses the plain random features to approximate softmax attention;\n- LARA (Zheng et al. 2022b), an advanced RF approximation that makes use of multiple adaptive proposals to construct the SNIS estimator;\n- Linformer (Wang et al.",
    "eva-41": "2020), a low-rank approximation that uses a learnable matrix to project the key-value sequence into a shorter one;\n- Nystr\u00f6mformer (Xiong et al. 2021b), a low-rank approximation that adopts the Nystr\u00f6m method to approximate softmax attention map with a sub-sampled matrix;\n- Local attention (Child et al. 2019), a simple sparse approximation that splits the whole sequence into multiple blocks and only allows the query to attend to tokens within the same block;\n- Reformer (Kitaev et al., 2020), a sparse approximation where hash functions are used to adaptively distribute sequence tokens into multiple buckets, and each token can only attend to tokens within the same bucket;\n\n```\nAlgorithm 2 Pseudo-code for Causal EVA\n    Input: the randomized mapping \\(\\xi(\\cdot, \\cdot)\\), queries \\(\\mathbf{Q}:=\\left\\{\\mathbf{q}_{n}\\right\\}_{n=1}^{N}\\), keys \\(\\mathbf{K}:=\\left\\{\\mathbf{k}_{m}\\right\\}_{m=1}^{M}\\), values\n    \\(\\mathbf{V}:=\\left\\{\\mathbf{v}_{m}\\right\\}_{m=1}^{M}\\), and partitions of the sequence \\(\\left\\{E^{n}, \\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}_{n=1}^{N} ;\\)\n    Output: attention output \\(\\mathbf{Y}:=\\left\\{\\mathbf{y}_{n}\\right\\}_{n=1}^{N}\\);\n    for \\(c=1,2, \\ldots, C\\) do\n        Compute \\(\\widetilde{\\mathbf{k}}_{c}\\) according to Equation 26\n        Compute \\(q_{c}(\\omega)\\) according to Equation 28\n        Sample \\(\\omega_{c} \\sim q_{c}(\\omega) ; \\quad \\triangleright\\) During inference, simply set \\(\\omega_{c}=\\mathbb{E}_{q_{c}(\\omega)}[\\omega]\\)\n        Compute \\(\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})=\\sum_{m \\in \\mathcal{P}_{c}^{n}} \\frac{\\xi\\left(\\mathbf{k}_{m}, \\omega_{c}\\right)}{\\sum_{m \\in \\mathcal{P}_{c}^{n}} \\xi\\left(\\mathbf{k}_{m}, \\omega_{c}\\right)} \\mathbf{v}_{m}\\)\n    end for\n    Let \\(K \\leftarrow\\left|E^{N}\\right| ; \\quad \\triangleright\\) we assume all \\(E^{n}\\) are the same in size\n    Initialize \\(\\mathbf{M}^{E} \\in\\{0,1\\}^{K \\times K}\\) such that \\(\\mathbf{M}_{i, j}^{E}=\\mathbb{1}_{i \\leq j} ; \\quad \\triangleright\\) Intra- \\(E\\) masking matrix\n    Initialize \\(\\mathbf{M}^{\\mathcal{P}} \\in\\{0,1\\}^{C \\times C}\\) such that \\(\\mathbf{M}_{c, t}^{\\mathcal{P}}=\\mathbb{1}_{c \\leq t} ; \\quad \\quad \\Delta\\) Inter- \\(\\mathcal{P}\\) masking matrix\n    for \\(n=1,2, \\ldots, N\\) do\n        Find index \\(t\\) such that \\(\\mathcal{P}_{t}^{n}\\) is the most recent chunk on the left of \\(E\\);\n        Let \\(b^{n} \\leftarrow \\min _{i}\\left\\{i: i \\in E^{n}\\right\\} ; \\triangleright\\) The least position within \\(E^{n}\\); used for shifting token indices. \\(\\triangleright\\) The same masking matrix \\(\\mathbf{M}^{E}\\) can be reused across \\(n\\) via shifting token positions by \\(b^{n}\\). Compute \\(\\mathcal{S}=\\sum_{m \\in E^{n}} \\mathbf{M}_{m-b^{n}, n-b^{n}}^{E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}\\);\n            Compute \\(\\mathcal{R}=\\sum_{c=1}^{C} \\mathbf{M}_{c, t}^{\\mathcal{P}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right) \\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})\\);\n            Compute \\(Z=\\sum_{m \\in E^{n}} \\mathbf{M}_{m-b^{n}, n-b^{n}}^{E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{c=1}^{C} \\mathbf{M}_{c, t}^{\\mathcal{P}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right)\\);\n            Compute \\(\\mathbf{y}_{n}=(\\mathcal{S}+\\mathcal{R}) / Z\\)\n    end for\n    Return \\(\\mathbf{Y}:=\\left[\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{N}\\right]\\)\n```\n\n- Scatterbrain (Chen et al, 2021a), an approach that combines Performer and sparse attention. The details can be found in Appendix G. Here we implement the sparse module as a simple local attention to ensure a fair comparison;\n- Combiner (Ren et al. 2021), a probabilistic approach that constructs a structured factorization over the softmax probability distribution via a sparse mechanism. Combiner allows both direct and indirect calculations of conditional probabilities, where the direct probability is implemented as the sparse mechanism while the indirect probability is implemented through a local abstraction over a group of tokens. Similarly, we implement the sparse mechanism as a simple local attention, which corresponds to the Combiner-Fixed variant (Ren et al., 2021);\n- Transformer-LS, or Long-Short (Zhu et al., 2021), which is proposed to model long-term and short-term dependencies via low-rank structures and local attention respectively. The low-rank structure is defined as an input-dependent weight matrix that compresses the sequence into a shorter one; while the local attention is defined similarly as above. Note that for all mechanisms that involve a local attention, we split the sequence into non-overlapping blocks (or 2D windows in terms of images) and each query can only attend to tokens within the same block. We also use the relative positional embedding (Raffel et al., 2020b; Liu et al., 2021) within the local attention computation. Unlike Transformer-LS (Zhu et al. 2021) that allows each query to attend to multiple blocks, we do not use this extension as we find greatly increases memory consumption, although it does improve the model performance. ## E. 2 Image ClasSIFICATION\n\nThrough the experiments on image classification, we consider four different vision transformer (ViT) architectures:\n\nTable 9: Our hyper-parameter configuration for different attention mechanisms on DeiT-Tiny-784. | Attention | Hyper-parameter configuration on image classification |  |\n| :---: | :---: | :---: |\n| Local attention | Window size | 49 |\n| Scatterbrain Kitaev et al., 2020, | umber of random feature samples <br> Local attention window size | 96 <br> 49 |\n| Nystr\u00f6mformer Xiong et al., 2021b, | Number of landmarks | 49 |\n| Performer Choromanski et al., 2021. | Number of random feature samples <br> Type of random feature | 128 <br> Positive |\n| Combiner Ren et al., 2021 | Mode <br> Span size <br> Conditional distribution parameterization | Fixed <br> 49 <br> DeepSets-Max |\n| Transformer-LS Zhu et al., 2021 | Dynamic projection dimension <br> Local window size | 16 <br> 49 |\n| EVA (Ours) | Number of partitioned groups (C) <br> Size of $E$ | 49 <br> 49 |\n\n- DeiT-Tiny (Touvron et al. 2021), which maintains the sequence length as 196 across all transformer layers. For the particular tiny variant, the number of transformer layers is set to 12 , the embedding dimension is set to 196 and the number of heads is 3 ;\n- DeiT-Small (Touvron et al., 2021), which scales the embedding dimension and number of attention heads in DeiT-Tiny up to 384 and 6, respectively;\n- DeiT-Tiny-784, where the architecture is the same as DeiT-Tiny but the patch size in the tokenization step is decreased from 16 to 8 . This effectively increases the sequence length from 196 to 784 , which we found consistently improves predictive accuracy at the cost of significantly increased time and memory consumption. Under this setting, we also see clearer differences among these attention variants and it helps better evaluate the ability of different attention models to learn visual representations;\n- PVT-v2-B3 (Wang et al., 2021b), a pyramidal transformer architecture that processes much longer token sequences at early layers and progressively reduces the sequence length to form a hierarchical structure. It patchifies input images into $3136(56 \\times 56)$ tokens, and then processes the sequence through 4 stages. Each stage contains several transformer layers and a down-sampling operation, which reduces the sequence length by a factor of 4 and increases the embedding dimension by $2 \\times$. Due to the prohibitively long sequences initially, PVT applies an additional down-sampling module on input sequences to obtain key and value vectors, which are then passed through a normal softmax attention mechanism. To evaluate different RF approximations, we remove the down-sampling operation and directly operate on the original sequence length, which results in much fewer model parameters than vanilla PVT-v2-B3.",
    "eva-42": "We refer readers to Wang et al. (2021b) for detailed architecture configurations. For training, we do not use the [CLS] token for classification (Touvron et al., 2021); instead, we pool over the output of the last transformer layer to extract features and feed them into the classifier head. We followed the same protocol to train all model variants. Closely following DeiT Touvron et al. (2021), we employ the AdamW (Loshchilov \\& Hutter, 2019) optimizer to train models for 300 epochs, where the number of warm-up epochs is 10 , the learning rate is 0.001 with cosine learning rate decay (Loshchilov \\& Hutter, 2016), and batch size is set to 1024. The adopted augmentation and regularization are the same as DeiT, except that we remove repeated augmentation Hoffer et al. 2020) in DeiT models as it often slows down convergence, as also observed in previous studies (Xiao et al. 2021) $4^{4}$ The specific configurations of each attention mechanism on DeiT-Tiny-784 are listed in Table|9]. The hyper-parameter setup for each attention variant follows previous practices (Wang et al. 2021a b; Zheng et al. 2022b) closely to ensure a similar computational cost. Comparison to State-of-the-Art Model Architectures. We also compare our model against recent state-of-the-art (SOTA) model architectures with similar parameter sizes on ImageNet 1 k benchmark. As reported in Table 10, we observe that PVT-v2 (Wang et al., 2021b) with EVA greatly\n\n[^3]Table 10: Results on ImageNet 1 k dataset compared with SOTA model architectures. | Model | \\# Param. | FLOPs | Top-1 Acc. |\n| :---: | :---: | :---: | :---: |\n| PVT-v1-M (Wang et al., 2021a | 44 M | 6.7 G | 81.2 |\n| RegNetY-8G (Radosavovic et al., 2020, | 39 M | 8.0 G | 81.7 |\n| CvT-21 (Wu et al., 2021 | 32 M | 7.1G | 82.5 |\n| SOFT-M Lu et al., 2021 | 45 M | 7.2 G | 82.9 |\n| RegNetY-16G Radosavovic et al., 2020) | 84 M | 16.0 G | 82.9 |\n| UniFormer-S (Li et al., 2022, | 22 M | 3.6 G | 82.9 |\n| Swin-S (Liu et al., 2021 | 50 M | 8.7 G | 83.0 |\n| Swin-B (Liu et al, 2021 | 88 M | 15.4 G | 83.3 |\n| RegionV1I-M (Chen et al., 2021b) | 42 M | 7.9 G | 83.4 |\n| ViL-M (Zhang et al., 2021 | 40 M | 9.1 G | 83.5 |\n| Focal-S Yang et al., 2021 | 51 M | 9.1 G | 83.5 |\n| PVT-v2-b3 + LARA (Zheng et al., 2022b, | 40 M | 7.7 G | 83.6 |\n| MaxViT-T ( Tu et al., 2022 | 31 M | 5.6 G | 83.6 |\n| UniFormer-B (Li et al., 2022, | 50 M | 8.3 G | 83.9 |\n| PVT-v2-b3 (Wang et al., 2021b, | 45 M | 6.9 G | 83.1 |\n| PVT-v2-b3 + EVA | 36 M | 7.4 G | 83.7 |\n\nimproves the predictive accuracy and performs competitively with recent SOTA architectures while using fewer parameters and FLOPs. ## E. 3 Machine Translation and LanguAGE ModELING\n\nOur implementation for all language tasks is based on FairSeq toolkit (Ott et al. 2019). To compare different methods, we report BLEU scores on the test set as the main metric for MT and perplexity for both Autoregressive LM and MLM tasks. For the hyper-parameters $|E|$ and $C$ in EVA, we set $|E|=2 C$ by default, as we find that this choice attains a good trade-off between performance and computational costs across various tasks; while for $C$, it is determined based on previous practice for each task.",
    "eva-43": "Here we provide the detailed experimental protocol for each task. Masked Language Modeling. Following the standard pretraining practice as in RoBERTa (Liu et al. 2019), in MLM, we aim to reconstruct a subset of tokens in the input sequence that are randomly masked out, which is the core element of BERT-style natural language pretraining (Devlin et al.",
    "eva-44": "2019). This setting allows us to investigate the generalization ability of our model on larger model sizes and much more data. The task performance is measured with validation perplexity, which reflects how well the model fits the pretraining corpus and also exhibits good correlations with downstream task metrics. For the used corpus Books3, we randomly select 100 books without replacement for the validation split, similar to the setup in C4 dataset Raffel et al., 2020b). For the model, we use the RoBERTa-base architecture (Liu et al., 2019), where all the layer normalization operations (Ba et al., 2016) are placed before attention and FFN blocks (i.e., we adopt the pre-norm architecture), which leads to much more stable training for efficient attention mechanisms. We replace all softmax attention with EVA to test its effectiveness. The training setting and attention-specific parameters, which follow previous studies (Xiong et al. 2021a) to ensure a similar computational cost, can be found in Table 11 and Table 12 respectively. Machine Translation. We follow Ott et al. (2018) to process WMT14 En-De dataset, resulting in around $4.5 \\mathrm{M} / 3 \\mathrm{~K} / 3 \\mathrm{~K}$ English-German sentence pairs for training/validation/testing splits, respectively, and a shared vocabulary is obtained between the source and target language of around 32 K BPE types.",
    "eva-45": "The architecture and training specifics closely follow Vaswani et al. (2017), as listed in Table 13. We follow the previous protocol Zheng et al. (2022b) by replacing all encoder self-attention blocks in the encoder-decoder Transformer with EVA. For EVA, we find it beneficial to introduce an overlapping variant of $E$, where we allow $E$ to be overlapped with each other. Following previous practice in the context of local attention (Xiong et al., 2021a), $E$ not only contains all elements within the designated chunk but also additionally includes half the tokens in its neighboring chunks. As a result, EVA- 32 corresponds to $|E|=32$ with a contiguous chunk size of 16 .",
    "eva-46": "During inference, we follow the same setup as Zheng et al. (2022b) and average the last 10 model checkpoints to obtain the final model parameters. We apply beam search with size 4 , length penalty 0.6 , and compound split\n\nTable 11: Our hyper-parameter configuration for Masked Language Modeling (MLM). | Hyper-parameter | MLM |\n| :--- | :---: |\n| Number of transformer encoder layers | 12 |\n| Hidden size | 768 |\n| hidden size in FFN | 3072 |\n| Number of attention heads | 12 |\n| Batch size | 256 |\n| Sequence length | $2048,4096\\}$ |\n| Number of training steps | 200 K |\n| Number of warm-up steps | 5 K |\n| Weight decay rate | 0.01 |\n| Peak Learning Rate | $1 \\mathrm{e}-4$ |\n| Learning rate decay | Linear |\n| Optimizer | Adam |\n| Adam $\\epsilon$ | $1 \\mathrm{e}-6$ |\n| Adam $\\left(\\beta_{1}, \\beta_{2}\\right)$ | $(0.9,0.98)$ |\n| Gradient Clipping | 0.0 |\n| Dropout | 0.1 |\n| Attention dropout (if applicable) | 0.0 |\n\nTable 12: Our hyper-parameter configuration for different attention mechanisms on MLM task.",
    "eva-47": "* We used the exact positive random feature map (Choromanski et al. 2021) in our preliminary experiments. However, it failed to converge and exhibited substantial training instability. Therefore, we replace the positive random feature with a simple ReLU kernel function for MLM experiments, which yields better training performance. | Attention | Hyper-parameter configuration on MLM |  |\n| :---: | :---: | :---: |\n| Local attention | Window size | 256 |\n| Linformer (Wang et al., 2020, | Projected dimension | 256 |\n| Reformer Kitaev et al., 2020, | Number of hashes <br> Chunk size | 4 <br> 64 |\n| Performer Choromanski et al., 2021, | Number of random feature samples <br> Type of random feature | 256 <br> ReLU* |\n| LARA Zheng et al., 2022b, | Number of landmarks | 256 |\n| Combiner Ren et al., 2021 | Mode <br> Span size <br> Conditional distribution parameterization | Fixed <br> 256 <br> DeepSets-Max |\n| Transformer-LS Zhu et al., 2021, | Dynamic projection dimension <br> Local window size | 128 <br> 256 |\n| EVA (Ours) | Number of partitioned groups ( $C$ ) <br> Size of $E$ | 128 <br> 256 |\n\npost-processing. Since the input sequences in WMT14 En-De benchmark are much shorter than the other tasks considered in this paper (with an average sequence length of around 25 tokens), we start with $C=8,|E|=16$ and gradually increase $|E|$ to test the translation performance, similar to the setup in Ma et al.",
    "eva-48": "(2021); Zheng et al. (2022b). Note that increasing $C$ also leads to better translation quality, although we found the performance gain is slightly less effective than that of increasing $|E|$ (c.f.",
    "eva-49": "Tables 6 and 7 ). Autoregressive Language Modeling. We consider Wikitext-103 benchmark in this task, which consists of around $103 \\mathrm{M} / 218 \\mathrm{~K} / 246 \\mathrm{~K}$ tokens for training/validation/testing splits, respectively. We adopt the vanilla transformer decoder architecture (Vaswani et al., 2017), replace all decoder self-attention modules in the Transformer with the causal EVA mechanism, and evaluate EVA under two different setups: 1) a standard 16-layer Transformer LM (with model sizes of around 247 M ) as in Baevski \\& Auli (2019), and 2) a larger 32-layer Transformer LM (with model sizes of around 450 M ) as in Kasai et al. (2021). We follow their hyper-parameter settings to train all models,\n\nTable 13: Our hyper-parameter configuration for machine translation. | Hyper-parameter | Machine Translation |\n| :--- | :---: |\n| Number of transformer encoder layers | 6 |\n| Number of transformer decoder layers | 6 |\n| Hidden size | 512 |\n| hidden size in FFN | 2048 |\n| Number of attention heads | 8 |\n| Maximum number of tokens in a batch | 32768 |\n| Number of training steps | 300 K |\n| Number of warm-up steps | 6 K |\n| Weight decay rate | 0.0 |\n| Peak Learning Rate | 0.0007 |\n| Label Smoothing | 0.1 |\n| Learning rate decay | Inverse square root |\n| Optimizer | Adam |\n| Adam $\\epsilon$ | $1 \\mathrm{e}-6$ |\n| Adam $\\left(\\beta_{1}, \\beta_{2}\\right)$ | $(0.9,0.98)$ |\n| Gradient Clipping | 5.0 |\n| Dropout | 0.1 |\n| Attention dropout (if applicable) | 0.1 |\n\nTable 14: Our hyper-parameter configuration for autoregressive language modeling. | Hyper-parameter | LM in Baevski \\& Auli (2019, | LM in Kasai et al. (2021, |\n| :---: | :---: | :---: |\n| Number of transformer decoder layers | 16 | 32 |\n| Hidden size | 1024 | 1024 |\n| hidden size in FFN | 4096 | 4096 |\n| Number of attention heads | 8 | 8 |\n| Number of tokens in a batch | 65536 | 65536 |\n| Number of training steps | 286 K | 286 K |\n| Number of warm-up steps | 16 K | 16 K |\n| Weight decay rate | 0.0 | 0.0 |\n| Peak Learning Rate | 1.0 | 1.0 |\n| Learning rate decay | cosine | cosine |\n| Optimizer | nag | nag |\n| Gradient Clipping | 0.1 | 0.1 |\n| Dropout | 0.3 | 0.3 |\n| LayerDrop | - | 0.2 |\n| Attention dropout | 0.1 | 0.1 |\n\nwhere the corresponding configurations are listed in Table 14. 5 The vocabulary size is 267,744 with adaptive input embeddings (Baevski \\& Auli, 2019). During training, we set the sequence length to 512 and evaluate the validation/test PPL with various context window sizes in $\\{256,480\\}$, aligning with previous work (Baevski \\& Auli, 2019, Kasai et al. 2021). For other random feature baselines, unfortunately, we failed to fully replicate their results as reported in Kasai et al. (2021), where RFA in our implementation achieved a test perplexity of 29.0 even under a 449M Transformer model. For EVA, we set $|E|=128$ and $C=64$ by default for both 16-layer and 32-layer settings, ensuring similar computational cost to previous work that also evaluates random feature methods (typically with 128 or 256 random-feature dimension size) on Wikitext-103 language modeling task (Schlag et al.",
    "eva-50": "2021; Kasai et al.",
    "eva-51": "2021). ## E. 4 EXPERIMENTAL SETTINGS OF EFFICIENCY COMPARISON\n\nFor the simulation experiment conducted in $\\S 5.3$ we adopt the same transformer architecture across all attention variants. In particular, it uses 8 transformer layers, 192 embedding dimensions, and 2 attention heads so that longer sequences can fit into our devices. The batch size is set to 64 across\n\n[^4]![](https://cdn.mathpix.com/cropped/2024_09_12_5520e6e7b143a538a399g-32.jpg?height=545&width=1414&top_left_y=266&top_left_x=353)\n\nFigure 2: Left and right: Additional empirical memory consumption and running time comparison for different attention mechanisms under various sequence lengths. 8 V100 GPUs, and the statistics are computed by averaging the results of 30 runs. Besides, in our ablation study, the efficiency metrics reported in Table 6 and Table 7 are evaluated under the same setup used during training. Remark on Modeling Short Sequences. Unfortunately, similar to most previous efficient attention baselines, EVA also runs slower than softmax attention under shorter sequences (e.g., length of 128 or 256), but it soon catches up in running speed, and the reduction of memory consumption is still significant. Besides, in short-sequence settings (such as the case of DeiT-Tiny/Small with sequences of 196 tokens), EVA often performs on par with or better than conventional softmax attention (see Table 1, whereas most previous attention variants usually perform much worse. This implies EVA can achieve a better trade-off between efficiency and quality: for short sequences, EVA is possible to achieve stronger performance competitive with softmax attention (despite in longer running time); while for long sequences, EVA can be run much faster with less memory. Comparison to Memory-efficient Attention Mechanisms. In this section, we conduct an empirical efficiency comparison between efficient approximate attention methods and FlashAttention, one of the memory-efficient attention mechanisms (Rabe \\& Staats, 2021, Dao et al., 2022) with optimized memory accesses. FlashAttention computes the exact softmax attention in an online manner without materializing the full attention matrix, achieving linear memory complexity with respect to sequence lengths; besides, both runtime and memory usage are further improved by minimizing IO accesses. We benchmark different attention modules on one NVIDIA GeForce RTX 3090 GPU, where we measure the memory usage and runtime of running a single attention block, consisting of 8 attention heads with 512 embedding dimension size, for both a forward and backward pass. As shown in Figure 2, we observe that FlashAttention achieves significant memory usage reduction for softmax attention approximation and even consumes much less memory than all considered approximate baselines under all sequence lengths. In terms of runtime, we notice that FlashAttention runs faster than most attention baselines under sequence lengths less than 2048 despite scaling quadratically, but EVA, along with other more efficient approximate variants, begin to catch up at longer sequence lengths. This implies that the quadratic computational costs of softmax attention still bottleneck its runtime performance, aligning with one of the main findings in Dao et al. (2022). According to this empirical study, we observe that FlashAttention offers a general and effective technique to speed up softmax attention; since many approximate variants (including EVA) exhibit a similar formulation to softmax attention (e.g., Equation 16), we expect they can also benefit from the optimized online softmax calculation technique and memory accesses of FlashAttention (Dao et al., 2022). ## F EXPERIMENTS on Long RANGE ArEnA\n\nLong Range Arena (LRA; Tay et al. 2021) is a lightweight benchmark that assesses the ability of efficient attention methods to model long sequences in diverse domains. We follow the same hyper-parameter setup as Xiong et al. (2021b) to re-evaluate all attention baselines and report the\n\nTable 15: Classification accuracy (\\%) on LRA benchmark with different efficient attention mechanisms. | Model | ListOps | Text | Retrieval | Image | Pathfinder | Avg. |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Softmax | $\\mathbf{3 8 . 6 6}$ | 64.91 | 80.70 | 40.61 | 68.29 | 58.63 |\n| Linformer | 38.21 | 53.91 | 77.66 | 39.40 | 66.44 | 55.12 |\n| Performer | 29.84 | $\\mathbf{6 5 . 3 0}$ | 77.70 | 38.29 | 66.39 | 55.50 |\n| Reformer | 27.12 | 63.90 | 78.08 | 42.40 | 51.90 | 52.69 |\n| Scatterbrain | 38.21 | 64.04 | 77.83 | 42.51 | 60.62 | 56.64 |\n| Combiner | 38.26 | 63.98 | 81.47 | 42.80 | 55.94 | 56.49 |\n| LARA | 37.10 | 64.62 | 80.82 | 38.99 | 68.96 | 58.10 |\n| Nystr\u00f6mformer | 38.46 | $\\mathbf{6 5 . 2 8}$ | 80.44 | 39.71 | 68.98 | 58.57 |\n| Local | 38.46 | 63.70 | 80.71 | 42.25 | 68.46 | 58.72 |\n| Long-short | 38.56 | 63.46 | $\\mathbf{8 1 . 7 3}$ | 40.54 | $\\mathbf{7 1 . 2 8}$ | 59.11 |\n| EVA | $\\mathbf{3 8 . 6 1}$ | 64.31 | 80.21 | $\\mathbf{4 3 . 2 4}$ | 70.90 | $\\mathbf{5 9 . 4 5}$ |\n\ncomparison in Table 15. We observe that EVA largely improves previous RFA methods such as Performer (Choromanski et al. 2021) and LARA (Zheng et al., 2022b), and performs competitively with full softmax attention. Notably, EVA even achieves better average results over all tasks, with higher accuracy on Image and Pathfinder benchmarks, suggesting its capability of capturing long-term dependencies. For LRA benchmark, we set all attention-specific hyper-parameters to 128 (e.g., the number of landmarks in Nystr\u00f6mformer (Xiong et al. 2021b) and LARA (Zheng et al., 2022b), the window size in local attention and Combiner (Ren et al., 2021), etc.). We set $|E|=128$ and $C=64$ by default for EVA without any further tuning and find this setup works well. ## G Connections to Other AtTEntion Mechanisms\n\n## G. 1 RFA, SoftmAx AtTEntion, And EVA\n\nAs mentioned in our main text, one of the main contributions of this work is to develop a more general framework that bridges RFA and conventional softmax attention. To see how EVA (Equation 13) achieves this goal formally, note that if either $|E|=M$ or $C=M$, EVA would be equivalent to standard softmax attention; while if we set $|E|=0$ and $C=1$, EVA would recover vanilla RFA. ## G. 2 CONNECTIONS TO LARA\n\nNotably, EVA and LARA (Zheng et al. 2022b) are two efficient attention mechanisms that are both built upon the self-normalized importance sampling (SNIS) formulation of RFAs. LARA (Zheng et al. 2022b) puts the main focus on the proposal distribution used in SNIS and tries to design importance sampling proposals that are closer to the true underlying distribution. The proposed usage of multiple proposals further improves the estimation quality of SNIS and achieves strong empirical performance while still keeping linear complexity. In contrast to LARA, in this work we do not focus on the design choice of proposals used in importance sampling but aim to generalize the SNIS formulation further via control variates. As demonstrated in \\$3.2 our theory clearly delineates how the gap between such SNIS estimation and softmax attention can be closed by manipulating control variates. Since LARA and RFA are both SNIS estimators (their main difference lies in the choice of proposal distributions), our generalization also applies to LARA. To summarize, compared with LARA, EVA is a more general framework and improves conventional RFA from an orthogonal perspective. ## G. 3 Connections to Clustered AtTEntion\n\nClustered attention (Vyas et al. 2020) is an efficient attention mechanism that first clusters the set of queries into multiple groups, computes the mean centroid of each group, and then performs attention between query centroids and original key-value pairs. This framework is fast and effective and enjoys well-bounded approximation error. Clustered attention and EVA share some similarities in two aspects. First, both of them adopt the partitioning technique to reduce the computational complexity while remaining effective; and secondly, both observe that the efficient attention mechanism can be improved by refining the approximation over specific elements. For instance, clustered attention can be improved (Vyas et al., 2020) by selecting top- $k$ key-value pairs that are most relevant to each centroid and then refining the approximation by recomputing attention weights over these keys using original queries; while EVA notices that we can directly employ the optimal control variate coefficient for a subset of key-value pairs $(m \\in E)$ while still remaining efficient, which yields a more accurate approximation. Nevertheless, our main technical contribution is to develop a control variate formulation in the context of RFA and demonstrate that how RFA can be further improved locally. On the other hand, while clustered attention (Vyas et al. 2020) clusters queries, EVA partitions key-value pairs. This property makes EVA more amenable to the case of autoregressive language modeling since we do not impose clustering structures over the query set, and thus the causal relation among queries can be well maintained. ## G. 4 CONNECTIONS TO COMBINER\n\nCombiner (Ren et al. 2021) is a recently proposed attention mechanism that also partitions the sequence into chunks combined with local attention. The key difference between EVA and Combiner is the motivation, where Combiner introduces a structured factorization over the attention probability distribution, while our approach is built from the control variate perspective. ## G. 5 CONNECTIONS TO SCATTERbRAIN\n\nIn this section, we show that Scatterbrain (Chen et al., 2021a) can be cast as a special case of our framework EVA, although they are proposed based on quite different motivations. A Brief Review of Scatterbrain. Scatterbrain (Chen et al, 2021a) notes that sparse attention and RFA can approximate sharp and flat regions of the softmax attention matrix well, respectively. Based on this insight, Scatterbrain is proposed to first compute a Performer approximation to softmax attention and then cancel out the approximation error on critical regions via a sparse mechanism. Specifically, Scatterbrain (Chen et al. 2021a) defines a sparse matrix $\\mathbf{S} \\in \\mathbb{R}^{N \\times M}$ ) so that for each $(n, m) \\in \\mathbf{S}$ that indexes a non-zero entry. For notational simplicity, we also denote $\\operatorname{Supp}(\\mathbf{S})=\\left\\{(i, j) \\mid S_{i j} \\neq 0\\right\\}$ and $\\operatorname{Supp}_{n}(\\mathbf{S})=\\left\\{m \\mid S_{n m} \\neq 0\\right\\}$. With random features $\\phi(\\cdot, \\cdot)$ defined in Appendix A we let\n\n$$\nS_{n m}=\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)-\\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right)\n$$\n\nWe then add it back to the approximate output:\n\n$$\n\\begin{aligned}\ny_{n}^{\\prime} & =\\sum_{m=1}^{M} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\mathbf{S V} \\\\\n& =\\sum_{m=1}^{M} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\sum_{m^{\\prime} \\in \\operatorname{Supp}_{n}(\\mathbf{S})} S_{n m^{\\prime}} \\mathbf{v}_{m^{\\prime}} \\\\\n& =\\sum_{m \\notin \\operatorname{Supp}_{n}(\\mathbf{S})} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\sum_{m^{\\prime} \\in \\operatorname{Supp}_{n}(\\mathbf{S})} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right) \\mathbf{v}_{m^{\\prime}}\n\\end{aligned}\n$$\n\nThe sparse mechanism can be thought of as modeling the error due to RFA and eliminating it on the support of $\\mathbf{S}$. After the correction step, Scatterbrain further adds a post-hoc normalization step to obtain a normalized attention output:\n\n$$\ny_{n}=\\frac{\\sum_{m \\notin \\operatorname{Supp}_{n}(\\mathbf{S})} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\sum_{m^{\\prime} \\in \\operatorname{Supp}_{n}(\\mathbf{S})} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right) \\mathbf{v}_{m^{\\prime}}}{\\sum_{m \\notin \\operatorname{Supp}_{n}(\\mathbf{S})} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right)+\\sum_{m^{\\prime} \\in \\operatorname{Supp}_{n}(\\mathbf{S})} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\n$$\n\nIntuitively, Scatterbrain (Chen et al. 2021a) produces accurate approximation in the support of the sparse matrix and remains the random feature approximation outside the support. Scatterbrain is a Special Case of EVA. For notational convenience, we denote $E:=\\operatorname{Supp}_{n}(\\mathbf{S})$. According to Proposition 1, suppose we employ optimal coefficients $\\widehat{\\boldsymbol{\\beta}}_{m}$ for all entries in $\\operatorname{Supp}_{n}(\\mathbf{S})$, and use the same coefficient $\\widehat{\\boldsymbol{\\beta}}$ for all the remaining entries (in other words, we let $C=1$ and the whole index set is only partitioned into two subsets $\\{E,[M] \\backslash E\\})$. Then we have\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5520e6e7b143a538a399g-35.jpg?height=138&width=1160&top_left_y=495&top_left_x=472)\n\nAnd the resulting estimator overall becomes\n\n$$\n\\begin{aligned}\n\\widetilde{g}(\\boldsymbol{\\omega}) & =\\sum_{m=1}^{M} \\widetilde{g}_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\sum_{m \\in E} \\widetilde{g}_{m}(\\boldsymbol{\\omega})+\\sum_{m \\notin E} \\widetilde{g}_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})+\\widehat{\\boldsymbol{\\beta}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z}\\right) \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}} \\sum_{m \\notin E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}}\\left(1-\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z}\\right)\n\\end{aligned}\n$$\n\nScatterbrain (Chen et al. 2021a) can be a special case of this estimation algorithm if we set the proposal distribution to $q(\\omega)=\\mathcal{N}(\\omega ; 0, \\mathbf{I})$, and estimate the normalizing constant as follows. $$\n\\begin{aligned}\nZ & =\\mathbb{E}_{\\omega \\sim q(\\omega)}\\left[\\frac{\\mathcal{N}(\\omega ; 0, \\mathbf{I})\\left(\\sum_{m \\in E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)+\\sum_{m \\notin E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)\\right)}{q(\\omega)}\\right] \\\\\n& =\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\mathbb{E}_{\\omega \\sim q(\\omega)}\\left[\\frac{\\mathcal{N}(\\omega ; 0, \\mathbf{I}) \\sum_{m \\notin E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)}{q(\\omega)}\\right] \\\\\n& \\approx \\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\frac{1}{S} \\sum_{s=1}^{S} \\frac{\\mathcal{N}(\\omega ; 0, \\mathbf{I}) \\sum_{m \\notin E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)}{q\\left(\\omega_{s}\\right)} \\\\\n& =\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\frac{1}{S} \\sum_{s=1}^{S} \\sum_{m \\notin E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right) \\\\\n& =\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{m \\notin E} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\\\\n& :=\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{m \\notin E} \\widetilde{h}_{m}(\\boldsymbol{\\omega})\n\\end{aligned}\n$$\n\nwhere we define $\\widetilde{h}_{m}(\\boldsymbol{\\omega})=Z h_{m}(\\boldsymbol{\\omega})$, as in this case\n\n$$\n\\begin{aligned}\n& g(\\boldsymbol{\\omega})=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{Z} \\sum_{m=1}^{M} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right) \\mathbf{v}_{m} \\\\\n& h(\\boldsymbol{\\omega})=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{Z} \\sum_{m=1}^{M} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right)\n\\end{aligned}\n$$\n\nWith these specifications, we obtain\n\n$$\n\\begin{aligned}\n\\widetilde{g}(\\boldsymbol{\\omega}) & =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}}\\left(1-\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z}\\right) \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}} \\frac{Z-\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\\\\n& \\approx \\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}} \\frac{\\sum_{m \\notin E} \\widetilde{h}_{m}(\\boldsymbol{\\omega})}{Z} \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}} \\sum_{m \\notin E} h_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\frac{\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E} g_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\frac{\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E} \\frac{\\frac{1}{S} \\sum_{s=1}^{S} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right) \\mathbf{v}_{m}}{Z} \\\\\n& =\\frac{\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E} \\frac{\\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}}{Z} \\\\\n& \\approx \\frac{\\sum_{m \\notin E} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\sum_{m^{\\prime} \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right) \\mathbf{v}_{m^{\\prime}}}{\\sum_{m \\notin E} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right)+\\sum_{m^{\\prime} \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\n\\end{aligned}\n$$\n\nwhich is equivalent to Scatterbrain (Equation 30). Note that this equivalence would hold irrespective of the choice of shared coefficients $\\widehat{\\boldsymbol{\\beta}}$, which possibly indicates that the formulation of Scatterbrain limits the potential benefit of optimizing control variates under our framework. [^0]:    ${ }^{*}$ The majority of this work was done while these authors were at Bytedance. ${ }^{1}$ Our code and models are available at this link\n\n[^1]:    ${ }^{2}$ Note that the expectation of individual control variates $h_{m}(\\cdot)$ is still in closed form as $\\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right]=$ $\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) / Z$. The derivation can be found in Appendix B. 3\n\n[^2]:    ${ }^{3}$ Here we add the superscript $n$ to reflect the dependence on query position $n$. [^3]:    ${ }^{4}$ we retain the repeated augmentation technique in training PVT to be consistent with the original training protocol in Wang et al. (2021b). [^4]:    ${ }^{5}$ The setup in Baevski \\& Auli (2019) can be found in the corresponding Fairseq training script: https://github.com/pytorch/fairseq/blob/master/examples/language_ model/README. adaptive_inputs.md\n\n"
}