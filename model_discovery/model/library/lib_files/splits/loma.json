{
    "loma-0": "# LoMA: Lossless Compressed Memory Attention \n\nYumeng Wang* ${ }^{1}$ Zhenyang Xiao ${ }^{* 12}$\n\n\n#### Abstract\n\nLarge Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $t c$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression. ## 1. Introduction\n\nIn the field of Natural Language Processing (NLP), understanding and managing long context represents one of the significant challenges for achieving in-depth language comprehension. Research into long context not only enhances the model's capabilities in processing lengthy dialogues, document comprehension, and information retrieval tasks but also aids in achieving more precise language inference and knowledge extraction, thereby facilitating progress in\n\n[^0]applications such as machine translation, summarization, and question-answering systems(Yang et al., 2023). In these tasks, users expect language models to access as much information as possible, necessitating a method that can effectively store and retrieve information. An essential direction for improving long-context processing involves information compression, encapsulating prior key-value (KV) information within a few specialized tokens. Previous efforts, such as (Mu et al., 2023), have achieved this goal with relative efficacy. However, a notable limitation of these methods is their lossy nature of compression, which inevitably leads to the loss of vital information during the process. We propose a novel approach, the Lossless Compressed Memory Attention (LoMA), which divides sequence into multiple chunks of equal length, each chunk structured to include a reading zone, a memory zone and a repetition zone. The latter two zones incorporate newly introduced special tokens: ' $<\\mathrm{m}>$ ' and ' $<\\mathrm{r}>$ '. We also designed a unique attention matrix mask: the reading zone employs a conventional autoregressive lower triangular mask; in order to facilitate better internal information transmission and communication, the memory zone employs a bidirectional attention mechanism and they can attend to reading zone; tokens in the repetition zone can only observe the memory zone directly preceding it, as well as the token itself. With this masking strategy, the ' $<\\mathrm{r}>$ ' token in the repetition zone needs to faithfully reproduce the text content of the reading zone, while only being able to attend to the $<\\mathrm{m}>$ tokens in the memory zone. This implies that the ' $<\\mathrm{m}>$ ' tokens quickly learn to compress the entire content of the reading zone into their own KV. We have also mathematically demonstrated that the loss function generated in the repetition zone can indirectly supervise the training of the model in the memory zone, obviating the need for constructing labels and computing loss for the tokens in the memory zone. Through the generative algorithm of LoMA, transformer models acquire the ability to compress memory losslessly within the memory zone, substantially extending the length of the long-context they are capable of handling and significantly reducing computational and memory costs. Our experiments show that the Llama-2-7B model(Touvron et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-02.jpg?height=519&width=1743&top_left_y=204&top_left_x=159)\n\nFigure 1: Comparison of the standard transformer model with the LoMA model in autoregressive generation: (a) In the standard transformer model's autoregressive generation, the input token and the previous context's KV cache are fed together into the attention module to compute and predict the next token. (b) In the LoMA model's autoregressive generation, the previous context's KV cache is first compressed, and the input token is processed with the compressed KV cache by the attention module. 2023), when fine-tuned with the LoMA training method, is capable of high-ratio lossless memory compression of its own KV cache. Importantly, our approach does not modify the model's architecture or rely on additional auxiliary models. Chapter 2 reviews several studies related to our methodology, Chapter 3 provides an in-depth explanation of the LoMA generation algorithm, Chapter 4 describes the training precedure for endowing the transformer model with memory compression capabilities, Chapter 5 discusses our experimental results, and Chapter 6 concludes with a summary of our work.",
    "loma-1": "## 2. Related Works\n\n### 2.1. Sparse Attention\n\nIn recent times, the computational burden of long contexts has been effectively alleviated with the introduction of various sparsified attention mechanisms. (Zaheer et al., 2021) integrating random attention, windowed attention, and global attention achieved commendable results. (Zhao et al., 2019), (Gupta et al., 2021) posits that the plethora of irrelevant information within the attention mechanism can be distracting for the model, and thus zeroes out the less significant positions within the attention matrix to focus the model's attention. Subsequently, (Zhang et al., 2023) proposed a method to filter tokens of importance by summing up attention scores. Going a step further, (Ribar et al., 2023) estimated attention scores in the embedding dimension using the top-r values to then select the top- k largest KV pairs. The recently prominent Mistral architecture(Jiang et al., 2023a), employs windowed attention akin to the receptive fields of CNNs(O'Shea \\& Nash, 2015), theoretically enabling the effortless handling of text sequences up to the length of $32 \\times 4096$. However, none of these works can achieve lossless compression of context.",
    "loma-2": "More or less, some important information will be lost. ### 2.2. Explicit Memory\n\nExplicit memory is the conscious, intentional recollection of factual information, previous experiences, and concepts. Some method for Explicit memory compression are proposed by (Lanchantin et al., 2023), (Jiang et al., 2023b). Those approach involves the generation of a summary of preceding text, which is then inserted into the generated text, allowing subsequent text generation to utilize this summary to produce more coherent text. The downsides of this method include: 1) the generated summary occupies a significant portion of the text length, resulting in shorter generated text; 2) the process of generating a summary is also autoregressive, leading to a substantial increase in generation time; 3) the generated summary may omit some critical information, compromising the accuracy of the resulting text; and 4) a considerable amount of annotated data is required to fine-tune the model, which is costly. In (Mu et al., 2023), a novel compression method was introduced. This method involves inserting a 'gist token' between the prompt and response and employing a specially designed mask to ensure that the response chunk can only extract information from the gist token. During generation, the prompt is compressed into a gist token and then the original prompt is discarded to save resources. This approach effectively reduces memory usage. However, it's important to note that this method is not lossless and results in a significant loss of information. In contrast, our method achieves lossless compression of information into a ' $<\\mathrm{m}>$ ' token, ensuring that no information is lost. ## 3. Method\n\nThe LoMA framework introduces an enhanced autoregressive generation algorithm that leverages a transformer model\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-03.jpg?height=475&width=809&top_left_y=234&top_left_x=192)\n\nFigure 2: This figure delineates the relationship between single inference latency and KV cache length across various input token sequence lengths. The findings indicate that the latency of a single inference grows linearly with the length of the KV cache, yet the augmentation of input token sequence length does not substantially affect the computation time. Notably, when the input sequence consists of 16 tokens, an increase in KV cache length from 0 to 240 does not incur additional inference time, which might be attributable to the computational capacity characteristics of the hardware. trained to compress the KV cache losslessly. We first detail this algorithm and then describe the training methodology necessary to imbue the model with this advanced capability. ### 3.1. LoMA Generation\n\nWithin the architecture of a transformer, the KV (key-value) cache stores information from the preceding context and integrates it into the computation of attention. As the generated sequence lengthens, the memory occupied by the KV cache increases proportionally, leading to greater computational costs. Our proposed method, Lossless Compressed Memory Attention (LoMA), introduces an efficient computation step within the generation process to execute highratio lossless compression on the KV-cache. This significantly curtails storage and computational resource usage. LoMA functions with a defined compression ratio $c$ and a target compressed length $t$. Within the enhanced autoregressive generation framework, once the model accumulates a KV cache spanning $t c$ tokens, LoMA model compresses it to a fixed length $t$, as illustrated in Fig1 (b). This compression is achieved through the following steps:\n\n1. The model employs a standard autoregressive generation process to produce a sequence of $t c$ tokens, yielding a KV cache of corresponding length. This particular subset of tokens forms the reading zone, which is denoted by $\\mathrm{KV}_{\\text {Read }}$. 2. A single inference pass is conducted on $t$ ' $<\\mathrm{m}>$ ' tokens with $\\mathrm{KV}_{\\text {Read }}$, which yields a condensed KV cache of length $t$. This subsequence is designated as the memory zone. ![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-03.jpg?height=253&width=830&top_left_y=220&top_left_x=1057)\n\nFigure 3: The top row represents the original training samples, while the bottom row shows the processed training samples used for training or fine-tuning the LoMA model. In the original training samples, we insert $t$ ' $<\\mathrm{m}>$ ' tokens and $t c$ ' $<\\mathrm{r}>$ ' tokens after every $t c$ tokens. 3. The reading zone's KV cache is discarded, and following autoregressive generation proceeds utilizing the compressed KV cache from the memory zone.",
    "loma-3": "A comprehensive code listing detailing the aforementioned steps is presented in Appendix A. ### 3.2. Performance analysis\n\nIn this analysis, we evaluated the extent to which LoMA reduces the computational and storage resource requirements. Without loss of generality, we compared the standard autoregressive generation algorithm with the LoMA generation in the absence of prompts. Let $T_{\\text {infer }}(l, k)$ denote the time it takes for the model to complete one inference on a token sequence of length $l$ with a key-value (KV) cache of length $k$. Assuming the total generation spans $m$ chunks, each consisting of $t c$ tokens, the generation time for a traditional transformer is given by:\n\n$$\n\\sum_{k=0}^{m t c-1} T_{\\text {infer }}(1, k)\n$$\n\nUnder a preset compression ratio $c$ and memory length $t$, LoMA performs one inference every $t c$ tokens with $t$ ' $<\\mathrm{m}>$ ' tokens, resulting in a total generation time of:\n\n$$\n\\sum_{y=0}^{m} \\sum_{k=y t}^{y t+t c-1} T_{\\text {infer }}(1, k)+m T_{\\text {infer }}(t c, t)\n$$\n\nTypically, $T_{\\text {infer }}(l, k)$ is much less than $l T_{\\text {infer }}(1, k)$. Our tests conducted on an A100 GPU demonstrate this point, see Fig. 2. Consequently, even though the additional term $m T_{\\text {infer }}(t c, t)$ of the LoMA generation process slightly increases the computation, the significant compression of the KV cache results in a notable reduction in both generation time and memory usage, as illustrated in the table 1. ## 4. Training\n\nTo equip the transformer model with the aforementioned memory compression capability, pre-training or fine-tuning procedures are essential. We have devised a training procedure that includes structured reorganization of input samples, a novel loss terms, a unique design attention mask, and a specialized pattern of PositionIDs. ### 4.1. Input Samples\n\nIn the training procedure of LoMA, the original sequence of tokens is segmented into multiple subsequences each of length $t c$. To each subsequence, $t$ ' $<\\mathrm{m}>$ ' tokens followed by $t c$ '<r>' tokens are appended, forming a training chunk. All training chunks are concatenated to form a new structured sequence as a training sample. see Fig 3. ### 4.2. Loss\n\nTo train a Transformer model using the structured input sequence mentioned above, it is necessary to extend the vanilla loss $\\mathcal{L}_{\\mathrm{LM}}$ with an additional term that endows the model with the capability to compress memory. Since the output of model on the memory zone is not of concern, with the KV-cache in this zone being utilized to store compressed information, there is no need to design labels or a loss function for the memory zone. Indirectly, the memory zone is supervised through the loss applied to the repetition zone. Consequently, the training loss for each chunk is calculated as the sum of these two components, and the total loss across all chunks is determined by:\n\n$$\n\\mathcal{L}=\\sum_{y=1}^{m}\\left(\\mathcal{L}_{\\text {Read }}^{y}+\\mathcal{L}_{\\text {Rep }}^{y}\\right)\n$$\n\nwhere $\\mathcal{L}_{\\text {Read }}^{y}$ is the loss generated by the reading zone of the $y$-th training chunk while $\\mathcal{L}_{\\text {Rep }}^{y}$ corresponds to the loss produced by the repetition zone. Let the token subsequence from the reading zone be denoted as $\\operatorname{READ}_{y}=\\left\\{x_{k}, x_{k+1}, \\ldots, x_{k+t c}\\right\\}$, we have:\n\n$$\n\\mathcal{L}_{\\text {Read }}^{y}=\\sum_{i=k}^{k+t c} \\mathrm{CE}\\left(M\\left(x_{i}\\right), x_{i+1}\\right)\n$$\n\nis same with the standard trainning loss and\n\n$$\n\\mathcal{L}_{\\mathrm{REP}}^{y}=\\sum_{i=k}^{k+t c} \\mathrm{CE}\\left(M\\left({ }^{\\prime}<\\mathrm{r}>^{\\prime}{ }_{i+t(c+1)}\\right), x_{i}\\right)\n$$\n\nwhere $\\mathrm{CE}($ logits, label) refers to the standard cross-entropy loss function, and the $M(x)$ is the set of logits produced\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-04.jpg?height=324&width=836&top_left_y=216&top_left_x=1054)\n\nFigure 4: This figure describes the correspondence between inputs and labels. In reading zone, the input and target exhibit a standard autoregressive relationship. No labels are set in the memory zone, while the labels in the repetition zone consist of content from the reading zone. We demonstrated in Section.4.4 that by backpropagating gradients through the repetition zone, a supervisory signal can be provided to the memory zone. This allows the ' $<\\mathrm{m}>$ ' token to learn to compress the content of the reading zone into its own KV . by the model $M$ for the token $x$. The term ' $<\\mathrm{r}>{ }^{\\prime}{ }_{k+t(c+1)}$ refers to the ' $<\\mathrm{r}>$ ' token at position $k+t(c+1)$, indicating that the model's prediction for each ' $<$ r>' token should be identical with the corresponding token in the reading zone.",
    "loma-4": "Refer to Fig.",
    "loma-5": "4 for visual clarification. ### 4.3. Mask\n\nGiven that the input token sequence is restructured into training chunks, we cannot employ the standard lower triangular masking matrix directly in training. Instead, we must redesign the masking matrix to meet the following criteria:\n\n1. To enable the model to extract information from the memory zones for generation, the mask for the reading zone is designed to attend to all preceding memory zones, while maintaining unidirectional attention within the reading zone of the current training chunk. 2. The attention mask designed for the memory zone requires bidirectional attention. This facilitates the interaction of the KV cache, with a span of $t$ in the memory zone, effectively condensing the storage requirements for the KV cache of length $t c$ in the reading zone. It is also critical to prevent the memory zone from attending to any tokens beyond the adjacent reading zone. 3. In the repetition zone, the mask is designed to ensure that each token is restricted to attending only to the preceding memory zone. This constraint is vital because the repetition zone must reconstruct content based solely on the memory zone's information, without depending on the previously 'recalled' tokens to 'generate' subsequent tokens. For a training chunk composed of a reading zone of length $t c$, a memory zone of length $t$, and a repetition zone of length $t c$, the attention mask is defined as:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-05.jpg?height=725&width=743&top_left_y=234&top_left_x=214)\n\nFigure 5: The figure presents an attention mask for an input sequence comprising 12 tokens, which includes the initial token ' $<\\mathrm{s}>$ '. In this configuration, with $t=2$ and $c=2$, the reading and repetition zones each span 4 tokens, and the recall zone encompasses 2 tokens. Accordingly, the sequence is segmented into three training chunks. Each chunk is prefixed with ' $<m>$ ' and suffixed with ' $<r>$ ' tokens, yielding a total chunk length of 10 tokens $(4+2+4)$. This results in an attention mask with a dimension of $30 \\times 30$. Within this matrix, grey squares indicate a value of 0 , which blocks attention, and blue squares represent a value of 1 , allowing attention to flow. $$\n\\mathbf{M}_{s \\times s}=\\left[\\begin{array}{ccc}\n\\mathbf{L}_{t c \\times t c} & \\mathbf{0}_{t c \\times t} & \\mathbf{0}_{t c \\times t c} \\\\\n\\mathbf{1}_{t \\times t c} & \\mathbf{1}_{t \\times t} & \\mathbf{0}_{t \\times t c} \\\\\n\\mathbf{0}_{t c \\times t c} & \\mathbf{1}_{t c \\times t} & \\mathbf{I}_{t c \\times t c}\n\\end{array}\\right]\n$$\n\nWhere $s=t(2 c+1)$. During training, all training chunks are concatenated to form a single training sample, and the attention masks for each chunk are likewise concatenated in the following manner to form the attention mask for the entire sample. $$\n\\mathbf{M}=\\left[\\begin{array}{ccccc}\n\\mathbf{M}_{s \\times s} & \\mathbf{0}_{s \\times s} & \\mathbf{0}_{s \\times s} & \\ldots & \\mathbf{0}_{s \\times s} \\\\\n\\mathbf{S}_{s \\times s} & \\mathbf{M}_{s \\times s} & \\mathbf{0}_{s \\times s} & \\ldots & \\mathbf{0}_{s \\times s} \\\\\n\\mathbf{S}_{s \\times s} & \\mathbf{S}_{s \\times s} & \\mathbf{M}_{s \\times s} & \\ldots & \\mathbf{0}_{s \\times s} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{S}_{s \\times s} & \\mathbf{S}_{s \\times s} & \\mathbf{S}_{s \\times s} & \\ldots & \\mathbf{M}_{s \\times s}\n\\end{array}\\right]\n$$\n\nWhere\n\n$$\n\\mathbf{S}_{s \\times s}=\\left[\\begin{array}{ccc}\n\\mathbf{0}_{t c \\times t c} & \\mathbf{1}_{t c \\times t} & \\mathbf{0}_{t c \\times t c} \\\\\n\\mathbf{0}_{t \\times t c} & \\mathbf{0}_{t \\times t} & \\mathbf{0}_{t \\times t c} \\\\\n\\mathbf{0}_{t c \\times t c} & \\mathbf{0}_{t c \\times t} & \\mathbf{0}_{t c \\times t c}\n\\end{array}\\right]\n$$\n\nIn this design, the 1s in $\\mathbf{S}$ allow the reading zone to attend to the KV caches of all preceding memory zones. Fig. 5 illustrates the attention mask $M$ for a complete sample. ### 4.4. Gradient\n\nIn this section, we will demonstrate that LoMA can still learn the ability to compress memory during training, even without supervising the output of ' $<\\mathrm{m}>$ ' tokens. Suppose the input sequence is organized into a reading zone $r$ of length $t c$, a memory zone $m$ of length $t$, and a repetition zone $p$ of the same length as the reading zone, making the total length $s=t(2 c+1)$. Therefore, $\\mathbf{Q}, \\mathrm{K}$, and V are each composed of three vectors concatenated together as Equ.7. $$\nQ=\\left(\\begin{array}{c}\nq_{r} \\\\\nq_{m} \\\\\nq_{p}\n\\end{array}\\right), K=\\left(\\begin{array}{c}\nk_{r} \\\\\nk_{m} \\\\\nk_{p}\n\\end{array}\\right), V=\\left(\\begin{array}{c}\nv_{r} \\\\\nv_{m} \\\\\nv_{p}\n\\end{array}\\right)\n$$\n\nBy substituting the above expressions into the formula for Attention, we get the following calculation as Equ.8. $$\n\\hat{\\mathbf{A}}=\\left[\\begin{array}{lll}\n\\left(q_{r} k_{r}^{T}\\right)_{t c \\times t c} & \\left(q_{r} k_{m}^{T}\\right)_{t c \\times t} & \\left(q_{r} k_{p}^{T}\\right)_{t c \\times t c} \\\\\n\\left(q_{m} k_{r}^{T}\\right)_{t \\times t c} & \\left(q_{m} k_{m}^{T}\\right)_{t \\times t} & \\left(q_{m} k_{p}^{T}\\right)_{t \\times t c} \\\\\n\\left(q_{p} k_{r}^{T}\\right)_{t c \\times t c} & \\left(q_{p} k_{m}^{T}\\right)_{t c \\times t} & \\left(q_{p} k_{p}^{T}\\right)_{t c \\times t c}\n\\end{array}\\right]\n$$\n\nDot product $\\hat{\\mathbf{A}}$ with the previously designed mask (Equ.4) as Equ.9. $$\n\\mathbf{A}=\\hat{\\mathbf{A}} \\cdot \\mathbf{M}_{s \\times s}=\\left[\\begin{array}{ccc}\nq_{r} k_{r}^{T} \\odot \\mathbf{L} & \\mathbf{0} & \\mathbf{0} \\\\\nq_{m} k_{r}^{T} & q_{m} k_{m}^{T} & \\mathbf{0} \\\\\n\\mathbf{0} & q_{p} k_{m}^{T} & q_{p} k_{p}^{T} \\odot \\mathbf{I}\n\\end{array}\\right]\n$$\n\nHere, $\\odot$ denotes element-wise multiplication, $\\mathbf{L}$ is a lower triangular mask matrix. Without loss of generality, if we ignore the scale factor $\\sqrt{d_{k}}$, then the output of the Attention Block can be expanded as Equ. 10. $$\n\\begin{aligned}\n\\mathbf{O} & =\\operatorname{Softmax}(\\mathbf{A}) V \\\\\n& =\\left(\\begin{array}{c}\nE_{1} \\exp \\left(\\mathbf{A}_{1}\\right) V \\\\\nE_{2} \\exp \\left(\\mathbf{A}_{2}\\right) V \\\\\n\\cdots \\\\\nE_{s} \\exp \\left(\\mathbf{A}_{s}\\right) V\n\\end{array}\\right)\n\\end{aligned}\n$$\n\nWhere $\\mathbf{A}_{i}$ represents the $i$-th row vector of $\\mathbf{A}$, and $E_{i}=$ $1 / \\sum \\exp \\left(\\mathbf{A}_{i}\\right)$. Since we are currently only studying the propagation of gradients and not concerned with the absolute magnitude of the gradients, we can ignore coefficients(i.e., $E_{i}$ ). Thus, the first $t c$ rows of $\\mathbf{O}$, which represent the output of the reading zone, can be expressed as Equ. 11. $$\n\\mathbf{O}_{r}=\\exp \\left(q_{r} k_{r}^{T} \\odot \\mathbf{L}\\right) v_{r}\n$$\n\nTable 1: Resource Savings in Computational and Memory Footprint at Various $t$ and $c$ Parameter Configurations\n\n|  | $\\mathrm{t}=4$ |  | $\\mathrm{t}=8$ |  | $\\mathrm{t}=16$ |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| ratio | comp. overhead | memory reduced | comp. overhead | memory reduced | comp. overhead | memory reduced |\n| 2 | $-43.82 \\%$ | $-200 \\%$ | $-45.35 \\%$ | $-200 \\%$ | $-47.79 \\%$ | $-200 \\%$ |\n| 4 | $-65.87 \\%$ | $-400 \\%$ | $-67.12 \\%$ | $-400 \\%$ | $-66.95 \\%$ | $-400 \\%$ |\n| 8 | $-75.19 \\%$ | $-800 \\%$ | $-74.24 \\%$ | $-800 \\%$ | $-71.41 \\%$ | $-800 \\%$ |\n\nThe rows from $s-t c$ to $s$ of $\\mathbf{O}$, which represent the output of the repetition zone, can be expressed as Equ. 12\n\n$$\n\\mathbf{O}_{p}=\\exp \\left(q_{p} k_{m}^{T}\\right) v_{m}+\\exp \\left(q_{p} k_{p}^{T} \\odot \\mathbf{I}\\right) v_{p}\n$$\n\nSince we do not supervise the output of ' $<m>$ ' tokens, the loss consists only of two parts: $\\mathcal{L}_{\\mathrm{LM}}$ for the reading zone and $\\mathcal{L}_{\\text {repeat }}$. Therefore, the gradient of the loss with respect to $x$ is as follows:\n\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x}=\\frac{\\partial \\mathcal{L}_{\\text {Read }}}{\\partial \\mathbf{O}_{r}} \\cdot \\frac{\\partial \\mathbf{O}_{r}}{\\partial x}+\\frac{\\partial \\mathcal{L}_{\\text {Rep }}}{\\partial \\mathbf{O}_{p}} \\cdot \\frac{\\partial \\mathbf{O}_{p}}{\\partial x}\n$$\n\nIt's important to note that: in Equ.13, $\\mathbf{O}_{p}$ is a function of $k_{m}$ and $v_{m}$, indicating that the model's memory capability can receive supervisory signals from the loss in the repetition zone. ### 4.5. Position IDs\n\nDue to the intrinsic structure of the training samples in the LoMA model, which significantly differ from the original plain-form samples, it is necessary to design a corresponding set of position IDs. Assuming the original input sequence is segmented into chunks of length $t$, the starting position ID for the $i$-th chunk is $i t$. After organizing this chunk into a training chunk of length $t(2 c+1)$, the position IDs for the memory zone are designed as:\n\n$$\ni t+c-1, i t+2 c-1, \\ldots, i t+t c-1\n$$\n\nThe purpose of this design is twofold: 1) to preserve the original position ids of the reading zone, thereby maintaining consistent position encodings and ensuring coherence in the KV cache, and 2) to make the position ids of the memory zone contiguous with those of the reading zone in the subsequent chunk, facilitating information extraction.",
    "loma-6": "## 5. Experiments\n\n### 5.1. Settings\n\nModel. We conduct experiments based on the pretrained Llama2 7B Chat models. The training framework is Megatron-LM(Shoeybi et al., 2020), using a machine equipped with 8 A100 GPUs. The Global batch-size are set to 32 . The learning rate adopts a cosine decay pattern, varying between $[5 e-5,5 e-6]$, and includes a warmup of 100 iterations. Special Tokens. The tokenizer of Llama2 has a vocabulary length of 32,000 . We added two special tokens with IDs ' $<\\mathrm{m}>$ : 32000' and '<r>: 32001'. Accordingly, modifications are needed in the model's embedding layer: two vectors of 4096 dimensions are added, changing the weight shape from $32000 \\times 4096$ to $32002 \\times 4096$. These two new vectors are initialized with a normal distribution, the mean and variance of which are derived from the statistics of the original weights. Framework. Since LoMA requires a custom attention mask and position id, we are unable to use Flash-Attention to accelerate the training and need to make certain modifications to Megatron(Shoeybi et al., 2020). Data. A small portion is extracted from the C 4 dataset(Raffel et al., 2020) or the GSM8K dataset(Cobbe et al., 2021) is used in a loop to train the model. The method of data preprocessing is as mentioned in the previous section.4. Assuming $\\hat{s}$ is the preset training sequence length, we impose the following requirement:\n\n$$\n\\hat{s} \\bmod (2 t c+t)=0\n$$\n\nWe divide the training space into $\\hat{s} /(2 t c+t)=\\bar{n}$ chunks, adding memory and repetition zones to each chunk. Assuming the sequence length input into the model is $s$, then it is required that:\n\n$$\n2 s+\\left\\lfloor\\frac{s}{c}\\right\\rfloor \\leq \\hat{s}\n$$\n\nwhere, $c$ is the compress ratio. If $s+\\bar{n} t(c+1)<\\hat{s}$, it needs to be padded to the length of $\\hat{s}$. ### 5.2. Main Results\n\nFirstly, we tested different compression ratios while keeping the length of memory zone constant at $t=8$. The training data for this experiment was the C4, and the LLM used was Llama-2-7b. During training, it was observed that $\\mathcal{L}_{\\text {Rep }}$ decreased rapidly and eventually approached zero when $c \\leq 8$. The model was able to nearly perfectly recapitulate the content of the reading zone relying solely on the significantly\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-07.jpg?height=403&width=1679&top_left_y=406&top_left_x=192)\n\nFigure 6: The comparison of different compression ratios on same length of memory zone. The orange lines represent $\\mathcal{L}_{\\text {Rep }}$ from Equ.1, the green lines represent $\\mathcal{L}_{\\text {Read }}$, and the blue lines represent the total loss $\\mathcal{L}$. ![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-07.jpg?height=411&width=1680&top_left_y=1272&top_left_x=192)\n\nFigure 7: The comparison of different lengths of memory zone on same compression ratio. Table 2: Repetition Accuracy\n\n| Loma Hyperparameters | $c=4, t=8$ | $c=4, t=16$ | $c=8, t=8$ | $c=8, t=16$ |\n| :---: | :---: | :---: | :---: | :---: |\n| Zone accuracy | $\\mathbf{7 1 . 5 6 \\%}$ | $54.90 \\%$ | $35.30 \\%$ | $40.19 \\%$ |\n| Token accuracy | $\\mathbf{9 9 . 8 4 \\%}$ | $99.68 \\%$ | $99.38 \\%$ | $99.40 \\%$ |\n\ncompressed memory zone's KV cache, demonstrating the model's capability for lossless compression of the KV cache. See Fig. 6\n\nAdditionally, under the same experimental conditions and a constant compression ratio of $c=4$, we compared the effects of varying the $t$ values. The experimental findings indicate that changes in the $t$ value (ranging from 4 to 32) have a minimal impact on the model's memory compression capabilities, as illustrated in the Fig. 7. Subsequently, across multiple different parameter settings, we trained on the C4 dataset for 5000 iterations and then conducted inference testing on GSM8K. We evaluated the generalization of the model's memory compression ability by calculating the accuracy within the repetition zone. In this experiment, we defined two distinct accuracy metrics: zone accuracy and token accuracy. Zone accuracy denotes the percentage of zones that were recapitulated entirely correctly, while token accuracy represents the percentage of tokens that were accurately recapitulated.",
    "loma-7": "See Tab.2. ## 6. Conclusion\n\nWe propose the Lossless Compressed Memory Attention (LoMA), aimed at losslessly compressing information to reduce computational consumption in long text contexts. The advantages of this approach are: 1) It does not alter the model structure, allowing for an expansion of the model's contextual length to $c$ times its original size for most models; 2) It does not require additional annotated data and can be fine-tuned directly on pre-trained models; 3) It allows for segmental compression, and each compression only adds one inference process, avoiding a significant increase in generation time. We fine-tuned the LLaMA 7B model with LoMA on the C4 and GSM8K datasets, achieving convergence within 2000 iterations. Moreover, we found that information compression has good generalizability; models trained on C4 can be seamlessly generalized to the GSM8K dataset. We suggest adopting LoMA in pretraining to address the increasingly important scenarios of long texts in the future. ## References\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110. 14168. arXiv:2110.14168 [cs]. Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J. Memory-efficient Transformers via Top-\\$k\\$ Attention, June 2021.",
    "loma-8": "URL http: / / arxiv.org/abs/2106. 06899. arXiv:2106.06899 [cs]. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7B, October 2023a. URL http: / /arxiv.org/abs/2310. 06825. arXiv:2310.06825 [cs]. Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L. LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models, December 2023b.",
    "loma-9": "URL http: / / arxiv.org/abs/2310. 05736. arXiv:2310.05736 [cs]. Lanchantin, J., Toshniwal, S., Weston, J., Szlam, A., and Sukhbaatar, S. Learning to Reason and Memorize with Self-Notes, October 2023. URL http: / / arxiv org/abs/2305.00833. arXiv:2305.00833 [cs]. Mu, J., Li, X. L., and Goodman, N. Learning to Compress Prompts with Gist Tokens, July 2023. URL http: / / arxiv.org/abs/2304.08467. arXiv:2304.08467 [cs]. O'Shea, K. and Nash, R. An Introduction to Convolutional Neural Networks, December 2015. URL http: / / arxiv.org/abs/1511.08458. arXiv:1511.08458 [cs]. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. ISSN 1533-7928. URL http: / jmlr.org/papers/v21/20-074.html. Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi, C., and Orr, D. SparQ Attention: Bandwidth-Efficient LLM Inference, December 2023. URL http://arxiv.org/abs/2312. 04985. arXiv:2312.04985 [cs]. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-LM: Training MultiBillion Parameter Language Models Using Model Parallelism, March 2020. URL http://arxiv.org/ $\\mathrm{abs} / 1909.08053$. arXiv:1909.08053 [cs]. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\n\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open Foundation and FineTuned Chat Models, July 2023.",
    "loma-10": "URL http: / / arxiv. org/abs/2307.09288. arXiv:2307.09288 [cs]. Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D., Wang, D., Yan, D., Yang, F., Deng, F., Wang, F., Liu, F., Ai, G., Dong, G., Zhao, H., Xu, H., Sun, H., Zhang, H., Liu, H., Ji, J., Xie, J., Dai, J., Fang, K., Su, L., Song, L., Liu, L., Ru, L., Ma, L., Wang, M., Liu, M., Lin, M., Nie, N., Guo, P., Sun, R., Zhang, T., Li, T., Li, T., Cheng, W., Chen, W., Zeng, X., Wang, X., Chen, X., Men, X., Yu, X., Pan, X., Shen, Y., Wang, Y., Li, Y., Jiang, Y., Gao, Y., Zhang, Y., Zhou, Z., and Wu, Z. Baichuan 2: Open Large-scale Language Models, September 2023.",
    "loma-11": "URL http://arxiv.org/abs/ 2309.10305. arXiv:2309.10305 [cs]. Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. Big Bird: Transformers for Longer Sequences, January 2021.",
    "loma-12": "URL http: / / arxiv. org/ abs/2007. 14062 arXiv:2007.14062 [cs, stat]. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., R\u00e9, C., Barrett, C., Wang, Z., and Chen, B. H\\$_2\\$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models, July 2023.",
    "loma-13": "URL http: / /arxiv.org/abs/2306. 14048. arXiv:2306.14048 [cs]. Zhao, G., Lin, J., Zhang, Z., Ren, X., Su, Q., and Sun, X. Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection, December 2019.",
    "loma-14": "URL http://arxiv.org/abs/1912. 11637. arXiv:1912.11637 [cs]. ## A. Loma generator\n\n```\nclass LoMAGenerator:\n    def __init__(self, model, max_len, position_type, compress_ratio, mem_len,\n        mem_token_id) -> None:\n        self.model = model\n        self.model_dtype = model.parameters().___next__().dtype\n        self.max_len = max_len\n        self.mem_len = mem_len\n        self.compress_ratio = compress_ratio\n        self.position_type = position_type\n        self.read_len = compress_ratio * mem_len\n        self.mem_token_id = mem_token_id\n        self.reset()\n        self.position_ids = None\n    def reset(self):\n        self.cursor = 0\n        self.kv_cache = None\n        self.generated = []\n        self.compressed_chunks = 0\n        self.input_buffer = []\n        self.position_ids = None\n    def __call__(self, input_ids, eos_token_id):\n        generated = []\n        generated.append(self.add_token_ids(input_ids))\n        while len(generated) < self.max_len and generated[-1] != eos_token_id:\n        generated.append(self.add_token_ids([generated[-1]]))\n    return generated\n    def add_token_ids(self, token_ids):\n    assert len(token_ids) > 0\n    self.input_buffer.extend(token_ids)\n    while self.mem_len > 0:\n        # WHILE loop iterates inference and compression\n        kv_cache_len = self.kv_cache[0][0].shape[2] if self.kv_cache else 0\n        uncomp_cache_len = kv_cache_len - self.compressed_chunks * self. mem_len\n            assert uncomp_cache_len < self.read_len\n            proc_len = self.read_len - uncomp_cache_len\n            if len(self.input_buffer) < proc_len:\n                break\n            input_ids = self.input_buffer[:proc_len]\n            self.input_buffer = self.input_buffer[proc_len:]\n            last_predict = self.inference_input(input_ids)\n            self.compress_last_chunk()\n    if self.input_buffer:\n        last_predict = self.inference_input(self.input_buffer)\n        self.input_buffer = []\n    return last_predict\n    def inference_input(self, input_ids):\n    input_ids = torch.LongTensor(input_ids)\n```\n\n```\n    position_ids = torch.LongTensor(range(self.cursor, self.cursor + len(\n        input_ids)))\n    kv_cache_len = 0 if self.kv_cache is None else self.kv_cache[0][0].shape\n        [2]\n    mask_rows = input_ids.shape[0]\n    mask_cols = mask_rows + kv_cache_len\n    attn_mask = torch.ones((1, 1, mask_rows, mask_cols)).cuda().to(self. model_dtype)\n    attn_mask = torch.tril(attn_mask, diagonal=kv_cache_len)\n    with torch.no_grad():\n        output = self.model(\n            input_ids = input_ids.cuda().unsqueeze(0),\n            past_key_values = self.kv_cache,\n            position_ids = position_ids.cuda().unsqueeze(0),\n                attention_mask = attn_mask,\n                return_dict = True,\n                )\n    self.kv_cache = output['past_key_values']\n    self.cursor += input_ids.shape[0]\nreturn output['logits'][:, -1, :].argmax(dim=-1).item()\ndef compress_last_chunk(self):\n    \"\"\"\n    Compress all the read KV cache into the KV of <m> tokens\n    \"\"\"\n    assert self.kv_cache is not None\n    mem_cursor = self.compressed_chunks * self.mem_len\n    cache_len = self.kv_cache[0][0].shape[2]\nassert mem_cursor == cache_len - self.read_len\ninput_ids = torch.LongTensor([self.mem_token_id] * self.mem_len)\nif self.position_type == 'intermittent':\n            position_ids = list (range(\n            self.cursor - self.read_len + self.compress_ratio - 1,\n            self.cursor,\n            self.compress_ratio\n            ))\nelse:\n        position_ids = list(range(mem_cursor, mem_cursor + self.mem_len))\nposition_ids = torch.LongTensor(position_ids)\nread_kv_cache = []\nfor i in range(len(self.kv_cache)):\n    cache_k, cache_v = self.kv_cache[i]\n    read_kv_cache.append((cache_k[:, :, -self.read_len:],\n                                    cache_v[:, :, -self.read_len:]))\nwith torch.no_grad():\n    mem_attn_mask = torch.ones(1, 1, self.mem_len, self.mem_len + self. read_len).to(self.model_dtype)\n        output = self.model(\n            input_ids = input_ids.cuda().unsqueeze(0),\n            past_key_values = read_kv_cache,\n            position_ids = position_ids.cuda().unsqueeze(0),\n            attention_mask = mem_attn_mask.cuda(),\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-12.jpg?height=731&width=1786&top_left_y=201&top_left_x=107)\n\n\n[^0]:    ${ }^{*}$ Equal contribution ${ }^{1}$ Deepglint Inc.",
    "loma-15": "${ }^{2}$ Peking University. Correspondence to: Yumeng Wang <yumengwang @ deepglint.com>. Proceedings of the $41^{\\text {st }}$ International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). "
}