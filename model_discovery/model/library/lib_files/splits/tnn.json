{
    "tnn-0": "Toeplitz Neural Network for Sequence Modeling\n\n2Zhen Qin 2Xiaodong Han 3Weixuan Sun 2Bowen He 1Dong Li 3Dongxu Li 4Yuchao Dai 5Lingpeng Kong 1Yiran Zhong 1Shanghai AI Laboratory 2SenseTime Research 3Australian National University 4Northwestern Polytechnical University 5The University of Hong Kong Indicates the corresponding author.",
    "tnn-1": "Email: zhongyiran@gmail.com\n\nAbstract\n\nSequence modeling has important applications in natural language processing and computer vision. Recently, the transformer-based models have shown strong performance on various sequence modeling tasks, which rely on attention to capture pairwise token relations, and position embedding to inject positional information. While showing good performance, the transformer models are inefficient to scale to long input sequences, mainly due to the quadratic space-time complexity of attention. To overcome this inefficiency, we propose to model sequences with a relative position encoded Toeplitz matrix and use a Toeplitz matrix-vector production trick to reduce the space-time complexity of the sequence modeling to log linear. A lightweight sub-network called relative position encoder is proposed to generate relative position coefficients with a fixed budget of parameters, enabling the proposed Toeplitz neural network to deal with varying sequence lengths. In addition, despite being trained on 512-token sequences, our model can extrapolate input sequence length up to 14K tokens in inference with consistent performance. Extensive experiments on autoregressive and bidirectional language modeling, image modeling, and the challenging Long-Range Arena benchmark show that our method achieves better performance than its competitors in most downstream tasks while being significantly faster. The code is available at https://github.com/OpenNLPLab/Tnn. 1 Introduction\n\nSequence modeling is a fundamental problem in natural language processing, speech processing, and computer vision. Various sequence modeling methods have been proposed in the literature, including recurrent (Hochreiter & Schmidhuber, 1997), convolutional architectures (LeCun et al., 1989), and transformers (Vaswani et al., 2017). These models utilize various properties of sequential data for their modeling. For example, recurrent models (Hochreiter & Schmidhuber, 1997) mimic the sequential property by sequentially processing the input while maintaining hidden states through steps. Convolutional models (LeCun et al., 1989) enforce the locality bias sequentially and only interact elements within local patches. Transformers use attention matrices to model pairwise relations regardless of the distance between them. Recently, Transformers (Vaswani et al., 2017; Dosovitskiy et al., 2021) show strong performance on a wide range of applications across domains and become arguably one of the most successful architectures for sequence modeling in general. There are two main components in transformers: the attention mechanism that learns pairwise correlations of tokens from data, and the position embedding to introduce positional inductive biases. The vanilla attention mechanism requires quadratic space-time complexity, which precludes Transformers from handling long sequences. Numerous attention variants have been proposed recently to reduce the complexity, including linear transformers (Katharopoulos et al., 2020), and Performer (Choromanski et al., 2021). Although the types of attention vary, the position embedding remains in every method, which indicates the importance of position information in sequence modeling. This motivates us to ask the following question: since position information is important, can we design a model that relies entirely on the position information of its elements regardless of their content, thus alleviating the quadratic computation cost of the vanilla attention mechanism? In this paper, we give an affirmative answer to this question by introducing Toeplitz neural network, a new efficient architecture that solely exploits relative position relations for sequence modeling. In specific, instead of attention matrices, the Toeplitz neural network uses Toeplitz matrices to capture relations between each token pair. There are two motivations for selecting the Toeplitz matrix. One is that it compactly represents relative positional relations between tokens with much fewer parameters, i.e., parameters for an Toeplitz matrix. The other is that the Toeplitz matrix-vector production can be efficiently processed in complexity, which is exactly what we used in our token mixing operation. In this way, we avoid computing content similarities between tokens and effectively reduce the quadratic computation complexity of transformers to log linear, rendering a more efficient sequence modeling architecture. We further propose relative position encoder, a lightweight module that generates relative position parameters to assemble the Toeplitz matrices, so that the number of the TNN\u2019s parameters will no longer depend on the sequence length. Moreover, it allows TNN to deal with varying sequence lengths without retraining. In addition, the input sequence length extrapolation becomes an important ability in sequence modeling as training on longer sequences can be prohibitively expensive (Press et al., 2022). We propose an exponential decay bias that directly applies to the Toeplitz matrix. Our model achieves a consistent performance to a sequence length of 14K tokens in inference when training on sequences of 512 tokens. We also show analytically that the Toeplitz neural network represents a general form of sequence modeling methods, and derives transformers, CNNs, and the recently proposed State-space-based methods (Gu et al., 2022) as its special forms. We validate our model on a wide range of sequence modeling tasks and benchmarks. These include auto-regressive language modeling, text classification, image classification, and the Long-Range Arena benchmark. As illustrated in Fig. 1, our model achieves state-of-the-art performance on most tasks at a favorable log linear space-time complexity. It also demonstrates superior extrapolation capabilities when training on shorter sequences and evaluating on longer ones off-the-shelf. 2 Preliminary\n\nIn this section, we introduce concepts used throughout the paper, including positional embedding, token and channel mixing, and the Toeplitz matrix. Notations used can be found in Appendix A. Positional embedding is introduced in transformers (Vaswani et al., 2017) to inject positional inductive bias. It often uses fixed or learned parameters to encode position-specific information, thus making the model position-aware. There are mainly two types of positional embeddings: the absolute positional embedding (Vaswani et al., 2017) and the relative position embedding (Shaw et al., 2018). In this work, we focus on the relative position embedding to emphasize pair-wise token relations. A typical relative positional embedding (Raffel et al., 2020) is formulated as:\n\ne i \u200b j = \ud835\udc2a i \u22a4 \u200b \ud835\udc24 j / d + w i \u2212 j , subscript \ud835\udc52 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc2a \ud835\udc56 top subscript \ud835\udc24 \ud835\udc57 \ud835\udc51 subscript \ud835\udc64 \ud835\udc56 \ud835\udc57 e_{ij}=\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}/\\sqrt{d}+w_{i-j}, (1)\n\nwhere are two positional indices, denotes the attention score before softmax.",
    "tnn-2": "The represents the queries and keys in the attention. The is a positional coefficient. In this case, the relative position information is added to the attention as a bias. Token and channel mixing are used by (Yu et al., 2022) to refer to the two main procedures in sequence modeling. The token mixing refers to the process of mixing information between token pairs and the channel mixing for those between feature channels. In the Transformers, given the attention matrix and token matrix , the attention operation can be regarded as a token mixing process and the FFN module is used for channel mixing. Researchers often classify various sequence modeling techniques based on the token mixing techniques used. MLP-based methods (Liu et al., 2021; Tolstikhin et al., 2021) use matrix multiplication on the sequence dimension for token mixing. FFT-based methods (Lee-Thorp et al., 2022) utilize the FFT on the sequence dimension to mix token-wise information. The State-space-based methods (Gu et al., 2022) leverage the state equations and hidden states to model sequences, as well as perform interactions between tokens. Toeplitz matrix is a special form of a matrix that has constant values along each diagonal running from left to right, i.e.,\n\n\ud835\udc13 i \u200b j = \ud835\udc13 i + 1 , j + 1 = t i \u2212 j , \ud835\udc13 \u2208 \u211d n \u00d7 n . formulae-sequence subscript \ud835\udc13 \ud835\udc56 \ud835\udc57 subscript \ud835\udc13 \ud835\udc56 1 \ud835\udc57 1 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 \ud835\udc13 superscript \u211d \ud835\udc5b \ud835\udc5b \\mathbf{T}_{ij}=\\mathbf{T}_{i+1,j+1}=t_{i-j},\\mathbf{T}\\in\\mathbb{R}^{n\\times n}. (2)\n\nThere are two nice properties of a Toeplitz matrix: 1). For an Toeplitz matrix, we can efficiently describe it with parameters. 2). The Toeplitz matrix-vector production is faster than standard matrix-vector production. In particular, we have:\n\nTheorem 2.1. For a Toeplitz matrix and any vector , the time complexity of is . We provide detailed proof in Appendix B. This property enables us to use the Toeplitz matrices to perform efficient token mixing. 3 Toeplitz neural network\n\nIn this section, we provide a detailed design and analysis of our proposed Toeplitz Neural Network (TNN) by giving a glance at the overall structure of our model first and then describing each of its components. We also discuss the connection between the TNN and other sequence modeling methods at the end of this section. 3.1 The overall architecture\n\nOur model consists of a stack of Gated Toeplitz Units (GTU) and GLU (Shazeer, 2020). GTU is a modified GLU layer injected with the proposed Toeplitz Neural Operator (TNO), as illustrated in Fig. 2. A TNO is used to perform token mixing with a Toeplitz matrix. To generate relative position coefficients for the Toeplitz matrix, we propose a Relative Position Encoder (RPE), a lightweight fully-connected sub-network to encode the relative position information. An exponential decay bias is also added to the Toeplitz matrix to enable extrapolation on longer inputs. 3.2 Toeplitz neural operator\n\nHere, we will show how to use a Toeplitz matrix to represent relative positional information. Let us consider to be two positions in a 1D sequence, by using the relative position embedding in Eq. 1, we can define a Toeplitz matrix , where . Specifically, given a sequence of tokens, , we use a scalar to represent the relative position coefficients between and . Then a Toeplitz matrix can be formed by gathering for every token pair:\n\n\ud835\udc13 = [ t 0 t \u2212 1 \u22ef t \u2212 n + 1 t 1 t 0 \u22ee \u22ee t 0 t \u2212 1 t n \u2212 1 \u2026 t 1 t 0 ] \u2208 \u211d n \u00d7 n . \ud835\udc13 delimited-[] subscript \ud835\udc61 0 subscript \ud835\udc61 1 \u22ef subscript \ud835\udc61 \ud835\udc5b 1 subscript \ud835\udc61 1 subscript \ud835\udc61 0 missing-subexpression \u22ee \u22ee missing-subexpression subscript \ud835\udc61 0 subscript \ud835\udc61 1 subscript \ud835\udc61 \ud835\udc5b 1 \u2026 subscript \ud835\udc61 1 subscript \ud835\udc61 0 superscript \u211d \ud835\udc5b \ud835\udc5b \\mathbf{T}=\\left[\\begin{array}[]{cccc}t_{0}&t_{-1}&\\cdots&t_{-n+1}\\\\\nt_{1}&t_{0}&&\\vdots\\\\\n\\vdots&&t_{0}&t_{-1}\\\\\nt_{n-1}&\\ldots&t_{1}&t_{0}\\end{array}\\right]\\in\\mathbb{R}^{n\\times n}. (3)\n\nLet us define a token mixing operation as:\n\n\ud835\udc32 = \ud835\udc13\ud835\udc31 \u2208 \u211d n , \ud835\udc32 \ud835\udc13\ud835\udc31 superscript \u211d \ud835\udc5b \\mathbf{y}=\\mathbf{T}\\mathbf{x}\\in\\mathbb{R}^{n}, (4)\n\nwhere is the token mixing result. For any -dimensional sequences, the token mixing is performed on each dimension individually. As aforementioned in Theorem 2.1, the computation complexity of Eq. 4 is . As we need to perform token mixing on dimensions, our TNO has a computation complexity of . One following question is how to calculate the relative position coefficients in . A naive solution is to make the coefficients learnable parameters, such that the model can directly learn them from training data. However, this solution has some drawbacks: 1). Parameter explosion. For a -dimensional sequence of tokens, there are a total of learnable parameters, which can be prohibitively large as increases.",
    "tnn-3": "It also shows an unsatisfactory performance in our ablation studies in Sec.",
    "tnn-4": "4.3. 2). Fixed input sequence length. Since the sequence length is fixed in training, we are unable to adjust the sequence length during inference, i.e., it will cause a crucial performance drop when the sequence length changes. To address these drawbacks, we propose a relative position encoder to generate the relative position coefficients. 3.3 Relative position encoder\n\nWe illustrate the network structure of our RPE in Fig. 2, which is a fully connected network with layers. The input of the network is a 1-dimensional scalar, i.e., the value of , and output a dimension vector, which is used to assemble the Toeplitz matrix. In this case, the number of the TNN\u2019s parameters will no longer depend on the input sequence length and the TNN will have the flexibility to deal with various sequence lengths in the inference stage. Note that recent literature (Mildenhall et al., 2021) claims that projecting the scalar input to a higher dimensional space with high frequency functions, i.e., and functions, before passing a network can lead to better performance. However, in our ablations, we find that using the original integer achieves better performance. Exponential decay bias Previous models (Vaswani et al., 2017; Qin et al., 2022) often use a fixed sequence length in both training and inference. If we need to infer a longer sequence, the model needs to be retrained on the longer sequence length to maintain the performance, which can be prohibitively expensive in the application. ALiBi (Press et al., 2022) shows that by applying a simple penalty to the query-key attention scores, the Transformer can handle longer sequence length in inference without compromising the performance. The penalty is a linear bias that is proportional to the distance between tokens. Inspired by this technique, we propose an exponential decay bias that directly applies to the Toeplitz matrix to achieve the same goal. In specific, let us define a decay rate of , and the new relative position coefficients in can be expressed as:\n\nt \u00af i \u2212 j = \u03bb | i \u2212 j | \u200b t i \u2212 j . subscript \u00af \ud835\udc61 \ud835\udc56 \ud835\udc57 superscript \ud835\udf06 \ud835\udc56 \ud835\udc57 subscript \ud835\udc61 \ud835\udc56 \ud835\udc57 \\bar{t}_{i-j}=\\lambda^{|i-j|}t_{i-j}. (5)\n\nALiBi can be seen as a special case of our method. Given the equation of ALiBi:\n\ns \u00af i \u200b j = \ud835\udc2a i \u22a4 \u200b \ud835\udc24 j / d + m \u200b | i \u2212 j | , exp \u2061 ( s \u00af i \u200b j ) = exp \u2061 ( \ud835\udc2a i \u22a4 \u200b \ud835\udc24 j / d ) \u200b exp \u2061 ( m \u200b | i \u2212 j | ) , formulae-sequence subscript \u00af \ud835\udc60 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc2a \ud835\udc56 top subscript \ud835\udc24 \ud835\udc57 \ud835\udc51 \ud835\udc5a \ud835\udc56 \ud835\udc57 subscript \u00af \ud835\udc60 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc2a \ud835\udc56 top subscript \ud835\udc24 \ud835\udc57 \ud835\udc51 \ud835\udc5a \ud835\udc56 \ud835\udc57 \\bar{s}_{ij}=\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}/\\sqrt{d}+m|i-j|,\\quad\\exp(\\bar{s}_{ij})=\\exp(\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}/\\sqrt{d})\\exp(m|i-j|), (6)\n\nand\n\ns i \u200b j = \ud835\udc2a i \u22a4 \u200b \ud835\udc24 j / d , \u03bb \u225c exp \u2061 ( m ) , formulae-sequence subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc2a \ud835\udc56 top subscript \ud835\udc24 \ud835\udc57 \ud835\udc51 \u225c \ud835\udf06 \ud835\udc5a s_{ij}=\\mathbf{q}_{i}^{\\top}\\mathbf{k}_{j}/\\sqrt{d},\\quad{\\lambda}\\triangleq\\exp(m), (7)\n\nwe have:\n\nexp \u2061 ( s \u00af i \u200b j ) = exp \u2061 ( s i \u200b j ) \u200b \u03bb | i \u2212 j | . subscript \u00af \ud835\udc60 \ud835\udc56 \ud835\udc57 subscript \ud835\udc60 \ud835\udc56 \ud835\udc57 superscript \ud835\udf06 \ud835\udc56 \ud835\udc57 \\exp(\\bar{s}_{ij})=\\exp(s_{ij})\\lambda^{|i-j|}. (8)\n\nIt means the ALiBi applies an exponential decay on the softmax attention matrices whereas ours applies it on the Toeplitz matrices. 3.4 Relation to other sequence modeling models\n\nIn this section, we will show the relationship between our model and other sequence modeling models such as the Transformers (Vaswani et al., 2017), CNNs (LeCun et al., 1989), and the State space (Gu et al., 2022). We also compare the theoretical space-time complexity of our model with previous sequence modeling models in Table. 1. Transformers A Transformer with relative position embedding can be expressed as:\n\n\ud835\udc0e = Softmax \u200b ( \ud835\udc10\ud835\udc0a \u22a4 / d + \ud835\udc13 ) \u200b \ud835\udc15 . \ud835\udc0e Softmax superscript \ud835\udc10\ud835\udc0a top \ud835\udc51 \ud835\udc13 \ud835\udc15 \\mathbf{O}=\\mathrm{Softmax}(\\mathbf{Q}\\mathbf{K}^{\\top}/\\sqrt{d}+\\mathbf{T})\\mathbf{V}. (9)\n\nComparing it with Eq. 4, the TNN can be regarded as an attention-free transformer, i.e., removing the , and the , while only keeping the relative position matrices . CNNs A convolutional layer can be viewed as a Toeplitz matrix of a special structure. Considering a 1D convolution:\n\n\ud835\udc32 = \ud835\udc21 \u2217 \ud835\udc31 , \ud835\udc32 i = \u2211 j = 0 i \ud835\udc21 i \u2212 j \u200b \ud835\udc31 j , \ud835\udc21 \u2208 \u211d m , \ud835\udc31 \u2208 \u211d n , \ud835\udc32 \u2208 \u211d n + m \u2212 1 . formulae-sequence \ud835\udc32 \ud835\udc21 \ud835\udc31 formulae-sequence subscript \ud835\udc32 \ud835\udc56 superscript subscript \ud835\udc57 0 \ud835\udc56 subscript \ud835\udc21 \ud835\udc56 \ud835\udc57 subscript \ud835\udc31 \ud835\udc57 formulae-sequence \ud835\udc21 superscript \u211d \ud835\udc5a formulae-sequence \ud835\udc31 superscript \u211d \ud835\udc5b \ud835\udc32 superscript \u211d \ud835\udc5b \ud835\udc5a 1 \\mathbf{y}=\\mathbf{h}*\\mathbf{x},\\mathbf{y}_{i}=\\sum_{j=0}^{i}\\mathbf{h}_{i-j}\\mathbf{x}_{j},\\mathbf{h}\\in\\mathbb{R}^{m},\\mathbf{x}\\in\\mathbb{R}^{n},\\mathbf{y}\\in\\mathbb{R}^{n+m-1}. (10)\n\nLet\u2019s define a Toeplitz matrix :\n\n\ud835\udc13 s \u200b t = { \ud835\udc21 t \u2212 s 0 \u2264 t \u2212 s \u2264 m \u2212 1 , 0 \u2264 t \u2264 n \u2212 1 0 others , , \ud835\udc33 = [ \ud835\udc31 \ud835\udfce m \u2212 1 ] \u2208 \u211d n + m \u2212 1 . formulae-sequence subscript \ud835\udc13 \ud835\udc60 \ud835\udc61 cases subscript \ud835\udc21 \ud835\udc61 \ud835\udc60 formulae-sequence 0 \ud835\udc61 \ud835\udc60 \ud835\udc5a 1 0 \ud835\udc61 \ud835\udc5b 1 0 others \ud835\udc33 delimited-[] \ud835\udc31 subscript 0 \ud835\udc5a 1 superscript \u211d \ud835\udc5b \ud835\udc5a 1 \\mathbf{T}_{st}=\\begin{cases}\\mathbf{h}_{t-s}&0\\leq t-s\\leq m-1,0\\leq t\\leq n-1\\\\\n0&\\mathrm{others},\\end{cases},\\mathbf{z}=\\left[\\begin{array}[]{c}\\mathbf{x}\\\\\n\\mathbf{0}_{m-1}\\end{array}\\right]\\in\\mathbb{R}^{n+m-1}. (11)\n\nThen:\n\n\ud835\udc32 = \ud835\udc13\ud835\udc33 \u2208 \u211d n + m \u2212 1 . \ud835\udc32 \ud835\udc13\ud835\udc33 superscript \u211d \ud835\udc5b \ud835\udc5a 1 \\mathbf{y}=\\mathbf{T}\\mathbf{z}\\in\\mathbb{R}^{n+m-1}. (12)\n\nTherefore, a 1D CNN can be viewed as a special case of the TNN with a zero-padded input. For better illustration, we provide a matrix form of CNN operation in Appendix C.1. State space The equation of the State space can be expressed as:\n\n\ud835\udc2e i = \ud835\udc00\ud835\udc2e i \u2212 1 + \ud835\udc01\ud835\udc31 i , \ud835\udc32 i = \ud835\udc02\ud835\udc2e i , \ud835\udc00 \u2208 \u211d h \u00d7 h , \ud835\udc01 \u2208 \u211d h \u00d7 1 , \ud835\udc02 \u2208 \u211d 1 \u00d7 h , i = 1 , \u2026 , n formulae-sequence subscript \ud835\udc2e \ud835\udc56 subscript \ud835\udc00\ud835\udc2e \ud835\udc56 1 subscript \ud835\udc01\ud835\udc31 \ud835\udc56 formulae-sequence subscript \ud835\udc32 \ud835\udc56 subscript \ud835\udc02\ud835\udc2e \ud835\udc56 formulae-sequence \ud835\udc00 superscript \u211d \u210e \u210e formulae-sequence \ud835\udc01 superscript \u211d \u210e 1 formulae-sequence \ud835\udc02 superscript \u211d 1 \u210e \ud835\udc56 1 \u2026 \ud835\udc5b \\begin{gathered}\\mathbf{u}_{i}=\\mathbf{A}\\mathbf{u}_{i-1}+\\mathbf{B}\\mathbf{x}_{i},\\mathbf{y}_{i}=\\mathbf{C}\\mathbf{u}_{i},\\mathbf{A}\\in\\mathbb{R}^{h\\times h},\\mathbf{B}\\in\\mathbb{R}^{h\\times 1},\\mathbf{C}\\in\\mathbb{R}^{1\\times h},i=1,\\ldots,n\\end{gathered} (13)\n\nwhere is the input, is the output, is the intermediate state. According to (Gu et al., 2022), the output of the State space is:\n\n\ud835\udc32 i = \u2211 j = 0 i \ud835\udc24 i \u2212 j \u200b \ud835\udc31 j , \ud835\udc24 = ( \ud835\udc02\ud835\udc01 , \ud835\udc02\ud835\udc00\ud835\udc01 , \u2026 , \ud835\udc02\ud835\udc00 n \u2212 1 \u200b \ud835\udc01 ) \u22a4 \u2208 \u211d n . formulae-sequence subscript \ud835\udc32 \ud835\udc56 superscript subscript \ud835\udc57 0 \ud835\udc56 subscript \ud835\udc24 \ud835\udc56 \ud835\udc57 subscript \ud835\udc31 \ud835\udc57 \ud835\udc24 superscript \ud835\udc02\ud835\udc01 \ud835\udc02\ud835\udc00\ud835\udc01 \u2026 superscript \ud835\udc02\ud835\udc00 \ud835\udc5b 1 \ud835\udc01 top superscript \u211d \ud835\udc5b \\begin{gathered}\\mathbf{y}_{i}=\\sum_{j=0}^{i}{\\mathbf{k}}_{i-j}\\mathbf{x}_{j},{\\mathbf{k}}=\\left(\\mathbf{CB},\\mathbf{CAB},\\ldots,\\mathbf{CA}^{n-1}\\mathbf{B}\\right)^{\\top}\\in\\mathbb{R}^{n}.\\\\\n\\end{gathered} (14)\n\nLet\u2019s define the Toeplitz matrix :\n\n\ud835\udc13 i \u2212 j = { \ud835\udc24 i \u2212 j , i \u2265 j 0 , i < j . subscript \ud835\udc13 \ud835\udc56 \ud835\udc57 cases subscript \ud835\udc24 \ud835\udc56 \ud835\udc57 \ud835\udc56 \ud835\udc57 otherwise 0 \ud835\udc56 \ud835\udc57 otherwise \\mathbf{T}_{i-j}=\\begin{cases}\\mathbf{k}_{i-j},i\\geq j\\\\\n0,i<j\\end{cases}. (15)\n\nThen:\n\n\ud835\udc32 = \ud835\udc13\ud835\udc31 , \ud835\udc31 \u2208 \u211d n , \ud835\udc32 \u2208 \u211d n . formulae-sequence \ud835\udc32 \ud835\udc13\ud835\udc31 formulae-sequence \ud835\udc31 superscript \u211d \ud835\udc5b \ud835\udc32 superscript \u211d \ud835\udc5b \\mathbf{y}=\\mathbf{T}\\mathbf{x},\\mathbf{x}\\in\\mathbb{R}^{n},\\mathbf{y}\\in\\mathbb{R}^{n}. (16)\n\nIn this case, the State space can be regarded as a special form of TNN with the coefficients that are calculated by the State space. We also provide the matrix form in Appendix C.2 for better illustration. 4 Experiment\n\nWe compare our method to four kinds of sequential modeling methods including attention-based methods, MLP-based methods, FFT-based methods, and State-space-based methods. In particular, we select the following methods:\n\n\u2022\n\nAttention-based: Vanilla transformer(Vaswani et al., 2017), Transformer-LS(Zhu et al., 2021), FLASH, (Hua et al., 2022), 1+elu (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020), cosFormer (Qin et al., 2022). \u2022\n\nMLP-based: gMLP(Liu et al., 2021), Synthesizer (Random), Synthesizer (Dense) (Tay et al., 2021). \u2022\n\nFFT-based: FNet(Lee-Thorp et al., 2022), GFNet (Rao et al., 2021), AFNO(Guibas et al., 2021). \u2022\n\nState-space-based: S4(Gu et al., 2022), DSS (Gupta et al., 2022), GSS(Mehta et al., 2022). We evaluate our methods on the WikiText-103 (Merity et al., 2017) for autoregressive language modeling and the input length extrapolation ability, and the GLUE benchmark (Wang et al., 2018) for bidirectional language modeling. We also validate the accuracy and efficiency of our methods in handling long-range dependencies on the Long-Range Arena benchmark (Tay et al., 2020). To demonstrate the robustness of our model, we implement our model in DeiT (Touvron et al., 2021) structure and compare its performance with the vanilla DeiT (Touvron et al., 2021) on the ImageNet-1K (Deng et al., 2009) for image classification. 4.1 Setting\n\nWe implement our models in Pytorch (Paszke et al., 2019) and train them on 8 V100 GPUs. We adopt the same training configuration for all competitors, including batch size, learning rate, training epochs/updates, etc. More detailed hyper-parameters are listed in Appendix D. For the autoregressive language modeling, all models are trained on the WikiText-103 dataset (Merity et al., 2017) for 50K steps with a learning rate of . We use perplexity (PPL) as the evaluation metric. For the bidirectional language modeling, we choose the Roberta (Liu et al., 2019) model as the base model structure for all methods. All models are pre-trained on the WikiText-103 (Merity et al., 2017) for 50K steps with lr=0.005 and fine-tuned on the GLUE dataset (Wang et al., 2018). We use different learning rates among 1e-5, 3e-5, 6e-5, 1e-4 and choose the best result after fine-tuning for 3 epochs. For the Long-Range Arena benchmark, we adopt the same experimental configurations from the Skyformer Chen et al.",
    "tnn-5": "(2021). We ensure that performances and efficiencies of all methods are obtained with a similar parameter size and the same training hyperparameters. For the image classification on the ImageNet-1k dataset, we adopt the Deit (Touvron et al., 2021) network structure and replace the transformer layers with our model. 4.2 Results\n\nAutoregressive language modeling Autoregressive language modeling is a crucial task that requires the models to estimate causal probability distribution given the previously seen tokens. In Table 2, we compare the proposed TNN with competing sequence modeling models. First, compared to existing Mlp-based methods, TNN shows better performances with a clear margin on both val set and test set. Transformer-based methods are currently dominant sequence modeling methods. As a strong baseline, Transformer adopts a standard self-attention module with quadratic complexity, TNN still outperforms it on both val and test sets. in addition, TNN achieves better results than most efficient transformers including FLASH, 1+elu, Performer, and cosFormer. Finally, compared with recent emerging State-space-based sequence modeling methods, TNN achieves superior performance to all competing methods. it proves the effectiveness of our method in causal models. Further, we also compared the extrapolation capabilities of each method. In Figure 1, we show that our method outperforms all other methods and is comparable to ALiBi (Press et al., 2022). Complete results can be found in Appendix 15. Bidirectional language modeling We benchmark bidirectional modeling methods on the GLUE datasets in Table. 3. TNN achieves competitive results across all tasks. Further, it is worth noting that TNN boosts the results of CoLA by a significant margin, showing the ability to reason logistic information from sequences. It demonstrates the effectiveness of TNN in bidirectional language modeling. Long-Range Arena benchmark As shown in Table 4, we compare TNN with competing methods across five tasks of the LRA benchmark. The results before the Transformer-LS are taken from Skyformer (Chen et al., 2021). As demonstrated, TNN achieves the best scores on three tasks and the second places on the left two tasks. In terms of overall results, TNN outperforms all other competing methods including S4 (Gu et al., 2022) 111We re-run the S4 experiments with the new configuration to match the number of parameters. For the sake of completeness, we also compare TNN with S4 in the original size of S4 using the suffix \u201d-Large\u201d in Table14, which validates our ability to encode long sequences. For speed comparison, we compare the training speed of the TNN with other methods in Table 4.3. For a fair and comprehensive comparison, we follow exactly the same configurations of the Skyformer Chen et al. (2021) and report step per second under different sequence lengths. Timing is conducted on an Nvidia A6000 GPU with 48G GPU memory. Image modeling We report classification results on the ImageNet-1k dataset in Table 4.3. As shown, under similar parameter sizes, TNN achieves better results than Deit-Tiny and comparable results with Deit-Small. It demonstrates the capability of our method in encoding visual signals. 4.3 Ablation study\n\nNetwork structure configuration We ablate different structure configurations on the autoregressive language modeling task in Table 4.3. We consider three options of configuration: the GTU+GLU, GTU only, and attention+GLU. We empirically find that the GTU+GLU one achieves better performance than other options and choose it as our structure in TNN. Input of relative position encoder In Table 4.3, we ablate different RPE inputs on language modeling. (-(n-1),\u2026,(n-1)) denotes that we feed constants into the RPE. (-(n-1),\u2026,(n-1))/n denotes normalized constants. The sin, cos denotes the absolute position embedding method used in (Vaswani et al., 2017). We empirically find that using the original integers as the input for the RPE leads to better performance. Relative position encoder There are two ways to generate relative position coefficients for the Toeplitz matrix. One is to set these coefficients as learnable parameters and allow TNN to learn them from data. The other is to use our proposed RPE network to generate these coefficients. We compare these two strategies in Table 4.3. The TNN with our RPE network achieves an improvement of 2.47 PPL in language modeling. Exponential decay rate We ablate different exponential decay rates in Table 10 on the language modeling. We train these model variants with a fixed sequence length of 512 and test them on a series of sequence lengths from 512 to 14336 and compute the average PPL. When there is no exponential decay, the model fails to extrapolate to a longer sequence length. We also test our model with a learnable decay rate, but it does not show better performance. We empirically select 0.99 as the exponential decay rate in our method. 5 Conclusion\n\nIn this paper, we propose Toeplitz neural network, a new efficient architecture that relies entirely on relative positional information for sequence modeling. The proposed model enjoys a favorable log linear space-time complexity. Thanks to the proposed relative position encoder and exponential decay techniques, Toeplitz neural network generalizes to long sequences with a fixed budget of parameters while obtaining consistently superior performance than competing methods across multiple challenging tasks, including language modeling, image modeling, and sequence modeling on long inputs, i.e., the Long-Range Arena benchmark. Toeplitz neural network is also a generic sequence modeling approach, which renders various popular architectures, such as Transformers, CNNs, and State-space-based methods, as its special forms, offering a unified view for sequence modeling.",
    "tnn-6": "References\n\nBracewell & Bracewell (1986) Ronald Newbold Bracewell and Ronald N Bracewell. The Fourier transform and its applications, volume 31999. McGraw-hill New York, 1986. Chen et al. (2021) Yifan Chen, Qi Zeng, Heng Ji, and Yun Yang. Skyformer: Remodel self-attention with gaussian kernel and nystr\u00f6m method. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 2021. Choromanski et al. (2020) Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.",
    "tnn-7": "arXiv preprint arXiv:2009.14794, 2020. Choromanski et al. (2021) Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH. Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009. Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy. Gray et al. (2006) Robert M Gray et al. Toeplitz and circulant matrices: A review.",
    "tnn-8": "Foundations and Trends\u00ae in Communications and Information Theory, 2(3):155\u2013239, 2006. Gu et al. (2022) Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. Guibas et al. (2021) John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro. Efficient token mixing for transformers via adaptive fourier neural operators.",
    "tnn-9": "In International Conference on Learning Representations, 2021. Gupta et al. (2022) Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. Hochreiter & Schmidhuber (1997) Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997. Hua et al. (2022) Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time.",
    "tnn-10": "arXiv preprint arXiv:2202.10447, 2022. Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "tnn-11": "In International Conference on Machine Learning, pp. 5156\u20135165. PMLR, 2020. LeCun et al. (1989) Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.",
    "tnn-12": "Neural computation, 1(4):541\u2013551, 1989. Lee-Thorp et al. (2022) James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. FNet: Mixing tokens with Fourier transforms. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4296\u20134313, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.319. URL https://aclanthology.org/2022.naacl-main.319. Liu et al. (2021) Hanxiao Liu, Zihang Dai, David So, and Quoc V Le. Pay attention to mlps.",
    "tnn-13": "Advances in Neural Information Processing Systems, 34:9204\u20139215, 2021. Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.",
    "tnn-14": "arXiv preprint arXiv:1907.11692, 2019. Mehta et al. (2022) Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. Merity et al. (2017) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.",
    "tnn-15": "5th International Conference on Learning Representations, ICLR, Toulon, France, 2017. Mildenhall et al. (2021) Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021. Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.",
    "tnn-16": "Advances in neural information processing systems, 32, 2019. Press et al. (2022) Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Qin et al. (2022) Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Bl8CQrx2Up4. Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer.",
    "tnn-17": "J. Mach. Learn. Res., 21(140):1\u201367, 2020. Rao et al. (2021) Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networks for image classification. In Advances in Neural Information Processing Systems (NeurIPS), 2021. Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 464\u2013468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. URL https://aclanthology.org/N18-2074. Shazeer (2020) Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Tay et al. (2020) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020. Tay et al. (2021) Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models.",
    "tnn-18": "In International conference on machine learning, pp. 10183\u201310192. PMLR, 2021. Tolstikhin et al. (2021) Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in Neural Information Processing Systems, 34:24261\u201324272, 2021. Touvron et al. (2021) Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers &amp; distillation through attention. In International Conference on Machine Learning, volume 139, pp. 10347\u201310357, July 2021. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353\u2013355, 2018. Yu et al. (2022) Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision.",
    "tnn-19": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10819\u201310829, 2022. Zhu et al. (2021) Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision.",
    "tnn-20": "In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=M_lkFOwVdYc. Appendix\n\nAppendix A Mathematical notations\n\nAppendix B Proof of theorem\n\nIn this section, we will prove Theorem 2.1. Before doing that, let\u2019s first introduce the circulant matrix and Toeplitz matrix:\n\nDefinition B.1. A matrix is a circulant matrix if and only if , i.e.,\n\n\ud835\udc02 = [ c 0 c n \u2212 1 c n \u2212 2 \u22ef \u22ef c 1 c 1 c 0 c n \u2212 1 \u22f1 \u22ee c 2 c 1 \u22f1 \u22f1 \u22f1 \u22ee \u22ee \u22f1 \u22f1 \u22f1 c n \u2212 1 c n \u2212 2 \u22ee \u22f1 c 1 c 0 c n \u2212 1 c n \u2212 1 \u2026 \u2026 c 2 c 1 c 0 ] \u2208 \u211d n \u00d7 n . \ud835\udc02 delimited-[] subscript \ud835\udc50 0 subscript \ud835\udc50 \ud835\udc5b 1 subscript \ud835\udc50 \ud835\udc5b 2 \u22ef \u22ef subscript \ud835\udc50 1 subscript \ud835\udc50 1 subscript \ud835\udc50 0 subscript \ud835\udc50 \ud835\udc5b 1 \u22f1 missing-subexpression \u22ee subscript \ud835\udc50 2 subscript \ud835\udc50 1 \u22f1 \u22f1 \u22f1 \u22ee \u22ee \u22f1 \u22f1 \u22f1 subscript \ud835\udc50 \ud835\udc5b 1 subscript \ud835\udc50 \ud835\udc5b 2 \u22ee missing-subexpression \u22f1 subscript \ud835\udc50 1 subscript \ud835\udc50 0 subscript \ud835\udc50 \ud835\udc5b 1 subscript \ud835\udc50 \ud835\udc5b 1 \u2026 \u2026 subscript \ud835\udc50 2 subscript \ud835\udc50 1 subscript \ud835\udc50 0 superscript \u211d \ud835\udc5b \ud835\udc5b \\mathbf{C}=\\left[\\begin{array}[]{cccccc}c_{0}&c_{n-1}&c_{n-2}&\\cdots&\\cdots&c_{1}\\\\\nc_{1}&c_{0}&c_{n-1}&\\ddots&&\\vdots\\\\\nc_{2}&c_{1}&\\ddots&\\ddots&\\ddots&\\vdots\\\\\n\\vdots&\\ddots&\\ddots&\\ddots&c_{n-1}&c_{n-2}\\\\\n\\vdots&&\\ddots&c_{1}&c_{0}&c_{n-1}\\\\\nc_{n-1}&\\ldots&\\ldots&c_{2}&c_{1}&c_{0}\\end{array}\\right]\\in\\mathbb{R}^{n\\times n}. (17)\n\nDefinition B.2. A matrix is a Toeplitz matrix if and only if , i.e.,\n\n\ud835\udc13 = [ t 0 t \u2212 1 t \u2212 2 \u22ef \u22ef t \u2212 n + 1 t 1 t 0 t \u2212 1 \u22f1 \u22ee t 2 t 1 \u22f1 \u22f1 \u22f1 \u22ee \u22ee \u22f1 \u22f1 \u22f1 t \u2212 1 t n \u2212 2 \u22ee \u22f1 t 1 t 0 t \u2212 1 t n \u2212 1 \u2026 \u2026 t 2 t 1 t 0 ] \u2208 \u211d n \u00d7 n . \ud835\udc13 delimited-[] subscript \ud835\udc61 0 subscript \ud835\udc61 1 subscript \ud835\udc61 2 \u22ef \u22ef subscript \ud835\udc61 \ud835\udc5b 1 subscript \ud835\udc61 1 subscript \ud835\udc61 0 subscript \ud835\udc61 1 \u22f1 missing-subexpression \u22ee subscript \ud835\udc61 2 subscript \ud835\udc61 1 \u22f1 \u22f1 \u22f1 \u22ee \u22ee \u22f1 \u22f1 \u22f1 subscript \ud835\udc61 1 subscript \ud835\udc61 \ud835\udc5b 2 \u22ee missing-subexpression \u22f1 subscript \ud835\udc61 1 subscript \ud835\udc61 0 subscript \ud835\udc61 1 subscript \ud835\udc61 \ud835\udc5b 1 \u2026 \u2026 subscript \ud835\udc61 2 subscript \ud835\udc61 1 subscript \ud835\udc61 0 superscript \u211d \ud835\udc5b \ud835\udc5b \\mathbf{T}=\\left[\\begin{array}[]{cccccc}t_{0}&t_{-1}&t_{-2}&\\cdots&\\cdots&t_{-n+1}\\\\\nt_{1}&t_{0}&t_{-1}&\\ddots&&\\vdots\\\\\nt_{2}&t_{1}&\\ddots&\\ddots&\\ddots&\\vdots\\\\\n\\vdots&\\ddots&\\ddots&\\ddots&t_{-1}&t_{n-2}\\\\\n\\vdots&&\\ddots&t_{1}&t_{0}&t_{-1}\\\\\nt_{n-1}&\\ldots&\\ldots&t_{2}&t_{1}&t_{0}\\end{array}\\right]\\in\\mathbb{R}^{n\\times n}. (18)\n\nBased on the definition, we can give a key lemma:\n\nLemma B.3. A circulant matrix is orthogonally equivalent to the diagonal matrix , in particular, the orthogonal matrix is a DFT matrix:\n\n\ud835\udc02 = \ud835\udc05 \u22a4 \u200b \u039b \u200b \ud835\udc05 , \u039b = diag \u200b { \ud835\udc05 \u200b [ a 0 , a 1 , \u2026 , a n \u2212 1 ] \u22a4 } \u2208 \u211d n \u00d7 n , \ud835\udc05 s \u200b t = exp \u2061 ( 2 \u200b \u03c0 \u200b s \u200b t \u200b i n ) , i 2 = \u2212 1 . formulae-sequence formulae-sequence \ud835\udc02 superscript \ud835\udc05 top \u039b \ud835\udc05 \u039b diag \ud835\udc05 superscript subscript \ud835\udc4e 0 subscript \ud835\udc4e 1 \u2026 subscript \ud835\udc4e \ud835\udc5b 1 top superscript \u211d \ud835\udc5b \ud835\udc5b formulae-sequence subscript \ud835\udc05 \ud835\udc60 \ud835\udc61 2 \ud835\udf0b \ud835\udc60 \ud835\udc61 \ud835\udc56 \ud835\udc5b superscript \ud835\udc56 2 1 \\begin{gathered}\\mathbf{C}=\\mathbf{F}^{\\top}\\Lambda\\mathbf{F},\\\\\n\\Lambda=\\mathrm{diag}\\{\\mathbf{F}[a_{0},a_{1},\\ldots,a_{n-1}]^{\\top}\\}\\in\\mathbb{R}^{n\\times n},{\\mathbf{F}}_{st}=\\exp\\left(\\frac{2\\pi sti}{n}\\right),i^{2}=-1.\\end{gathered} (19)\n\nThe proof can be found in (Gray et al., 2006). Based on this, we can prove a key lemma:\n\nLemma B.4. For a vector and a circulant matrix , matrix multiplication can be done in time. Proof of Lemma B.3. Because is a DFT matrix, so and can be done time (Bracewell & Bracewell, 1986). Since is a diagonal matrix, so can be done in time, note that its diagonal elements can also be computed in time complexity, therefore,\n\n\ud835\udc02\ud835\udc31 = \ud835\udc05 \u22a4 \u200b \ud835\udeb2 \u200b \ud835\udc05\ud835\udc31 = \ud835\udc05 \u22a4 \u200b ( \ud835\udeb2 \u200b ( \ud835\udc05\ud835\udc31 ) ) , \ud835\udc02\ud835\udc31 superscript \ud835\udc05 top \ud835\udeb2 \ud835\udc05\ud835\udc31 superscript \ud835\udc05 top \ud835\udeb2 \ud835\udc05\ud835\udc31 \\mathbf{C}\\mathbf{x}=\\mathbf{F}^{\\top}{\\mathbf{\\Lambda}}\\mathbf{F}\\mathbf{x}=\\mathbf{F}^{\\top}\\left(\\mathbf{\\Lambda}(\\mathbf{F}\\mathbf{x})\\right), (20)\n\ncan be done in . \u220e\n\nBased on this, we can prove Theorem 2.1:\n\nProof of Theorem 2.1. We first fill the Toeplitz matrix into a circulant matrix :\n\nc k = { t k , 0 \u2264 k \u2264 n \u2212 1 t 0 , k = n t k \u2212 2 \u200b n , n + 1 \u2264 k \u2264 2 \u200b n \u2212 1 , subscript \ud835\udc50 \ud835\udc58 cases subscript \ud835\udc61 \ud835\udc58 0 \ud835\udc58 \ud835\udc5b 1 otherwise subscript \ud835\udc61 0 \ud835\udc58 \ud835\udc5b otherwise subscript \ud835\udc61 \ud835\udc58 2 \ud835\udc5b \ud835\udc5b 1 \ud835\udc58 2 \ud835\udc5b 1 otherwise c_{k}=\\begin{cases}t_{k},0\\leq k\\leq n-1\\\\\nt_{0},k=n\\\\\nt_{k-2n},n+1\\leq k\\leq 2n-1\\end{cases}, (21)\n\ni.e.,\n\n\ud835\udc02 = [ t 0 t \u2212 1 \u2026 \u2026 t \u2212 n + 1 t 0 t n \u2212 1 \u2026 t 2 t 1 t 1 t 0 \u22f1 \u22ee t \u2212 n + 1 \u22f1 \u22f1 t 2 t 2 \u22f1 \u22f1 \u22f1 \u22ee \u22ee \u22f1 \u22f1 \u22ee \u22ee \u22f1 t 0 t \u2212 1 t \u2212 2 \u22f1 \u22f1 t n \u2212 1 t n \u2212 1 \u2026 \u2026 t 1 t 0 t \u2212 1 t \u2212 2 \u2026 t \u2212 n + 1 t 0 t 0 t n \u2212 1 \u2026 \u2026 t 1 t 0 t \u2212 1 \u2026 \u2026 t \u2212 n + 1 t \u2212 n + 1 \u22f1 \u22f1 t 2 t 1 t 0 \u22f1 \u22ee \u22ee \u22f1 \u22f1 \u22ee t 2 \u22f1 \u22f1 \u22f1 \u22ee t \u2212 2 \u22f1 \u22f1 t n \u2212 1 \u22ee \u22f1 t 0 t \u2212 1 t \u2212 1 t \u2212 2 \u2026 \u2026 t 0 t n \u2212 1 \u2026 \u2026 t 1 t 0 ] \u2208 \u211d 2 \u200b n \u00d7 2 \u200b n . \ud835\udc02 delimited-[] subscript \ud835\udc61 0 subscript \ud835\udc61 1 \u2026 \u2026 subscript \ud835\udc61 \ud835\udc5b 1 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc5b 1 \u2026 subscript \ud835\udc61 2 subscript \ud835\udc61 1 subscript \ud835\udc61 1 subscript \ud835\udc61 0 \u22f1 missing-subexpression \u22ee subscript \ud835\udc61 \ud835\udc5b 1 \u22f1 \u22f1 missing-subexpression subscript \ud835\udc61 2 subscript \ud835\udc61 2 \u22f1 \u22f1 \u22f1 \u22ee \u22ee \u22f1 missing-subexpression \u22f1 \u22ee \u22ee missing-subexpression \u22f1 subscript \ud835\udc61 0 subscript \ud835\udc61 1 subscript \ud835\udc61 2 missing-subexpression \u22f1 \u22f1 subscript \ud835\udc61 \ud835\udc5b 1 subscript \ud835\udc61 \ud835\udc5b 1 \u2026 \u2026 subscript \ud835\udc61 1 subscript \ud835\udc61 0 subscript \ud835\udc61 1 subscript \ud835\udc61 2 \u2026 subscript \ud835\udc61 \ud835\udc5b 1 subscript \ud835\udc61 0 missing-subexpression missing-subexpression missing-subexpression missing-subexpression missing-subexpression missing-subexpression missing-subexpression missing-subexpression missing-subexpression missing-subexpression subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc5b 1 \u2026 \u2026 subscript \ud835\udc61 1 subscript \ud835\udc61 0 subscript \ud835\udc61 1 \u2026 \u2026 subscript \ud835\udc61 \ud835\udc5b 1 subscript \ud835\udc61 \ud835\udc5b 1 \u22f1 \u22f1 missing-subexpression subscript \ud835\udc61 2 subscript \ud835\udc61 1 subscript \ud835\udc61 0 \u22f1 missing-subexpression \u22ee \u22ee \u22f1 missing-subexpression \u22f1 \u22ee subscript \ud835\udc61 2 \u22f1 \u22f1 \u22f1 \u22ee subscript \ud835\udc61 2 missing-subexpression \u22f1 \u22f1 subscript \ud835\udc61 \ud835\udc5b 1 \u22ee missing-subexpression \u22f1 subscript \ud835\udc61 0 subscript \ud835\udc61 1 subscript \ud835\udc61 1 subscript \ud835\udc61 2 \u2026 \u2026 subscript \ud835\udc61 0 subscript \ud835\udc61 \ud835\udc5b 1 \u2026 \u2026 subscript \ud835\udc61 1 subscript \ud835\udc61 0 superscript \u211d 2 \ud835\udc5b 2 \ud835\udc5b \\mathbf{C}=\\left[\\begin{array}[]{ccccc|ccccc}t_{0}&t_{-1}&\\ldots&\\ldots&t_{-n+1}&t_{0}&t_{n-1}&\\ldots&t_{2}&t_{1}\\\\\nt_{1}&t_{0}&\\ddots&&\\vdots&t_{-n+1}&\\ddots&\\ddots&&t_{2}\\\\\nt_{2}&\\ddots&\\ddots&\\ddots&\\vdots&\\vdots&\\ddots&&\\ddots&\\vdots\\\\\n\\vdots&&\\ddots&t_{0}&t_{-1}&t_{-2}&&\\ddots&\\ddots&t_{n-1}\\\\\nt_{n-1}&\\ldots&\\ldots&t_{1}&t_{0}&t_{-1}&t_{-2}&\\ldots&t_{-n+1}&t_{0}\\\\\n\\hline\\cr t_{0}&t_{n-1}&\\ldots&\\ldots&t_{1}&t_{0}&t_{-1}&\\ldots&\\ldots&t_{-n+1}\\\\\nt_{-n+1}&\\ddots&\\ddots&&t_{2}&t_{1}&t_{0}&\\ddots&&\\vdots\\\\\n\\vdots&\\ddots&&\\ddots&\\vdots&t_{2}&\\ddots&\\ddots&\\ddots&\\vdots\\\\\nt_{-2}&&\\ddots&\\ddots&t_{n-1}&\\vdots&&\\ddots&t_{0}&t_{-1}\\\\\nt_{-1}&t_{-2}&\\ldots&\\ldots&t_{0}&t_{n-1}&\\ldots&\\ldots&t_{1}&t_{0}\\end{array}\\right]\\in\\mathbb{R}^{2n\\times 2n}. (22)\n\nUsing the notation of block matrix, we can define:\n\n\ud835\udc02 = [ \ud835\udc02 1 \ud835\udc02 2 \ud835\udc02 3 \ud835\udc02 4 ] \u2208 \u211d 2 \u200b n \u00d7 2 \u200b n , \ud835\udc02 s \u2208 \u211d n \u00d7 n , s = 1 , 2 , 3 , 4 , \ud835\udc02 1 = \ud835\udc13 . formulae-sequence \ud835\udc02 delimited-[] subscript \ud835\udc02 1 subscript \ud835\udc02 2 subscript \ud835\udc02 3 subscript \ud835\udc02 4 superscript \u211d 2 \ud835\udc5b 2 \ud835\udc5b formulae-sequence subscript \ud835\udc02 \ud835\udc60 superscript \u211d \ud835\udc5b \ud835\udc5b formulae-sequence \ud835\udc60 1 2 3 4 subscript \ud835\udc02 1 \ud835\udc13 \\begin{gathered}\\mathbf{C}=\\left[\\begin{array}[]{cc}\\mathbf{C}_{1}&\\mathbf{C}_{2}\\\\\n\\mathbf{C}_{3}&\\mathbf{C}_{4}\\\\\n\\end{array}\\right]\\in\\mathbb{R}^{2n\\times 2n},\\mathbf{C}_{s}\\in\\mathbb{R}^{n\\times n},s=1,2,3,4,\\mathbf{C}_{1}=\\mathbf{T}\\end{gathered}. (23)\n\nFor the vector , let\u2019s define:\n\n\ud835\udc31 1 = [ \ud835\udc31 \ud835\udfce n ] \u2208 \u211d 2 \u200b n , subscript \ud835\udc31 1 delimited-[] \ud835\udc31 subscript 0 \ud835\udc5b superscript \u211d 2 \ud835\udc5b \\mathbf{x}_{1}=\\left[\\begin{array}[]{c}\\mathbf{x}\\\\\n\\mathbf{0}_{n}\\end{array}\\right]\\in\\mathbb{R}^{2n}, (24)\n\nso:\n\n\ud835\udc02\ud835\udc31 1 = [ \ud835\udc02 1 \ud835\udc02 2 \ud835\udc02 3 \ud835\udc02 4 ] \u200b [ \ud835\udc31 \ud835\udfce n ] = [ \ud835\udc02 1 \u200b \ud835\udc31 \ud835\udc02 3 \u200b \ud835\udc31 ] = [ \ud835\udc13\ud835\udc31 \ud835\udc02 3 \u200b \ud835\udc31 ] \u2208 \u211d 2 \u200b n , subscript \ud835\udc02\ud835\udc31 1 delimited-[] subscript \ud835\udc02 1 subscript \ud835\udc02 2 subscript \ud835\udc02 3 subscript \ud835\udc02 4 delimited-[] \ud835\udc31 subscript 0 \ud835\udc5b delimited-[] subscript \ud835\udc02 1 \ud835\udc31 subscript \ud835\udc02 3 \ud835\udc31 delimited-[] \ud835\udc13\ud835\udc31 subscript \ud835\udc02 3 \ud835\udc31 superscript \u211d 2 \ud835\udc5b \\mathbf{C}\\mathbf{x}_{1}=\\left[\\begin{array}[]{cc}\\mathbf{C}_{1}&\\mathbf{C}_{2}\\\\\n\\mathbf{C}_{3}&\\mathbf{C}_{4}\\\\\n\\end{array}\\right]\\left[\\begin{array}[]{c}\\mathbf{x}\\\\\n\\mathbf{0}_{n}\\end{array}\\right]=\\left[\\begin{array}[]{c}\\mathbf{C}_{1}\\mathbf{x}\\\\\n\\mathbf{C}_{3}\\mathbf{x}\\end{array}\\right]=\\left[\\begin{array}[]{c}\\mathbf{T}\\mathbf{x}\\\\\n\\mathbf{C}_{3}\\mathbf{x}\\end{array}\\right]\\in\\mathbb{R}^{2n}, (25)\n\ntherefore:\n\n[ \ud835\udc08 n \ud835\udfce n \u00d7 n ] \u200b \ud835\udc02\ud835\udc31 1 = [ \ud835\udc08 n \ud835\udfce n \u00d7 n ] \u200b [ \ud835\udc13\ud835\udc31 \ud835\udc02 3 \u200b \ud835\udc31 ] = \ud835\udc13\ud835\udc31 . delimited-[] matrix subscript \ud835\udc08 \ud835\udc5b subscript 0 \ud835\udc5b \ud835\udc5b subscript \ud835\udc02\ud835\udc31 1 delimited-[] matrix subscript \ud835\udc08 \ud835\udc5b subscript 0 \ud835\udc5b \ud835\udc5b delimited-[] \ud835\udc13\ud835\udc31 subscript \ud835\udc02 3 \ud835\udc31 \ud835\udc13\ud835\udc31 \\left[\\begin{matrix}{\\mathbf{I}}_{n}&{\\mathbf{0}}_{n\\times n}\\end{matrix}\\right]\\mathbf{C}\\mathbf{x}_{1}=\\left[\\begin{matrix}\\mathbf{I}_{n}&\\mathbf{0}_{n\\times n}\\end{matrix}\\right]\\left[\\begin{array}[]{c}\\mathbf{T}\\mathbf{x}\\\\\n\\mathbf{C}_{3}\\mathbf{x}\\end{array}\\right]=\\mathbf{T}\\mathbf{x}. (26)\n\nNote that:\n\n\u2022\n\nComputing has a time complexity of . \u2022\n\nis equivalent to selecting the first rows of , the time complexity is . So the total time complexity is . \u220e\n\nAppendix C Matrix form of sequential models\n\nIn this section, we give the matrix form of some sequence models mentioned in section 3.4. C.1 CNN\n\nThe matrix form of CNN mentioned in Eq. 10 is:\n\n[ \ud835\udc32 0 \ud835\udc32 1 \ud835\udc32 2 \u22ee \ud835\udc32 n + m \u2212 1 ] = [ \ud835\udc21 0 0 \u2026 0 0 \ud835\udc21 1 \ud835\udc21 0 \u2026 \u22ee \u22ee \ud835\udc21 2 \ud835\udc21 1 \u2026 0 0 \u22ee \ud835\udc21 2 \u2026 \ud835\udc21 0 0 \ud835\udc21 m \u2212 2 \u22ee \u2026 \ud835\udc21 1 \ud835\udc21 0 \ud835\udc21 m \u2212 1 \ud835\udc21 m \u2212 2 \u22ee \u22ee \ud835\udc21 1 0 \ud835\udc21 m \u2212 1 \u2026 \ud835\udc21 m \u2212 3 \u22ee 0 0 \u2026 \ud835\udc21 m \u2212 2 \ud835\udc21 m \u2212 3 \u22ee \u22ee \u22ee \ud835\udc21 m \u2212 1 \ud835\udc21 m \u2212 2 0 0 0 \u22ef \ud835\udc21 m \u2212 1 ] \u200b [ \ud835\udc31 0 \ud835\udc31 1 \ud835\udc31 2 \u22ee \ud835\udc31 n \u2212 1 ] \u2208 \u211d n + m \u2212 1 . delimited-[] subscript \ud835\udc32 0 subscript \ud835\udc32 1 subscript \ud835\udc32 2 \u22ee subscript \ud835\udc32 \ud835\udc5b \ud835\udc5a 1 delimited-[] subscript \ud835\udc21 0 0 \u2026 0 0 subscript \ud835\udc21 1 subscript \ud835\udc21 0 \u2026 \u22ee \u22ee subscript \ud835\udc21 2 subscript \ud835\udc21 1 \u2026 0 0 \u22ee subscript \ud835\udc21 2 \u2026 subscript \ud835\udc21 0 0 subscript \ud835\udc21 \ud835\udc5a 2 \u22ee \u2026 subscript \ud835\udc21 1 subscript \ud835\udc21 0 subscript \ud835\udc21 \ud835\udc5a 1 subscript \ud835\udc21 \ud835\udc5a 2 \u22ee \u22ee subscript \ud835\udc21 1 0 subscript \ud835\udc21 \ud835\udc5a 1 \u2026 subscript \ud835\udc21 \ud835\udc5a 3 \u22ee 0 0 \u2026 subscript \ud835\udc21 \ud835\udc5a 2 subscript \ud835\udc21 \ud835\udc5a 3 \u22ee \u22ee \u22ee subscript \ud835\udc21 \ud835\udc5a 1 subscript \ud835\udc21 \ud835\udc5a 2 0 0 0 \u22ef subscript \ud835\udc21 \ud835\udc5a 1 delimited-[] subscript \ud835\udc31 0 subscript \ud835\udc31 1 subscript \ud835\udc31 2 \u22ee subscript \ud835\udc31 \ud835\udc5b 1 superscript \u211d \ud835\udc5b \ud835\udc5a 1 \\left[\\begin{array}[]{c}\\mathbf{y}_{0}\\\\\n\\mathbf{y}_{1}\\\\\n\\mathbf{y}_{2}\\\\\n\\vdots\\\\\n\\mathbf{y}_{n+m-1}\\end{array}\\right]=\\left[\\begin{array}[]{ccccc}\\mathbf{h}_{0}&0&\\ldots&0&0\\\\\n\\mathbf{h}_{1}&\\mathbf{h}_{0}&\\ldots&\\vdots&\\vdots\\\\\n\\mathbf{h}_{2}&\\mathbf{h}_{1}&\\ldots&0&0\\\\\n\\vdots&\\mathbf{h}_{2}&\\ldots&\\mathbf{h}_{0}&0\\\\\n\\mathbf{h}_{m-2}&\\vdots&\\ldots&\\mathbf{h}_{1}&\\mathbf{h}_{0}\\\\\n\\mathbf{h}_{m-1}&\\mathbf{h}_{m-2}&\\vdots&\\vdots&\\mathbf{h}_{1}\\\\\n0&\\mathbf{h}_{m-1}&\\ldots&\\mathbf{h}_{m-3}&\\vdots\\\\\n0&0&\\ldots&\\mathbf{h}_{m-2}&\\mathbf{h}_{m-3}\\\\\n\\vdots&\\vdots&\\vdots&\\mathbf{h}_{m-1}&\\mathbf{h}_{m-2}\\\\\n0&0&0&\\cdots&\\mathbf{h}_{m-1}\\end{array}\\right]\\left[\\begin{array}[]{c}\\mathbf{x}_{0}\\\\\n\\mathbf{x}_{1}\\\\\n\\mathbf{x}_{2}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n-1}\\end{array}\\right]\\in\\mathbb{R}^{n+m-1}. (27)\n\nC.2 State Space\n\nThe Toeplitz matrix mentioned in Eq. 15 is:\n\n\ud835\udc13 = [ \ud835\udc24 0 0 0 \u22ef \u22ef 0 \ud835\udc24 1 \ud835\udc24 0 0 \u22f1 \u22ee \ud835\udc24 2 \ud835\udc24 1 \u22f1 \u22f1 \u22f1 \u22ee \u22ee \u22f1 \u22f1 \u22f1 0 0 \u22ee \u22f1 \ud835\udc24 1 \ud835\udc24 0 0 \ud835\udc24 s \u2212 1 \u22ef \u22ef \ud835\udc24 2 \ud835\udc24 1 \ud835\udc24 0 ] \u2208 \u211d n \u00d7 n . \ud835\udc13 delimited-[] matrix subscript \ud835\udc24 0 0 0 \u22ef \u22ef 0 subscript \ud835\udc24 1 subscript \ud835\udc24 0 0 \u22f1 missing-subexpression \u22ee subscript \ud835\udc24 2 subscript \ud835\udc24 1 \u22f1 \u22f1 \u22f1 \u22ee \u22ee \u22f1 \u22f1 \u22f1 0 0 \u22ee missing-subexpression \u22f1 subscript \ud835\udc24 1 subscript \ud835\udc24 0 0 subscript \ud835\udc24 \ud835\udc60 1 \u22ef \u22ef subscript \ud835\udc24 2 subscript \ud835\udc24 1 subscript \ud835\udc24 0 superscript \u211d \ud835\udc5b \ud835\udc5b \\begin{gathered}\\mathbf{T}=\\left[\\begin{matrix}\\mathbf{k}_{0}&0&0&\\cdots&\\cdots&0\\\\\n\\mathbf{k}_{1}&\\mathbf{k}_{0}&0&\\ddots&&\\vdots\\\\\n\\mathbf{k}_{2}&\\mathbf{k}_{1}&\\ddots&\\ddots&\\ddots&\\vdots\\\\\n\\vdots&\\ddots&\\ddots&\\ddots&0&0\\\\\n\\vdots&&\\ddots&\\mathbf{k}_{1}&\\mathbf{k}_{0}&0\\\\\n\\mathbf{k}_{s-1}&\\cdots&\\cdots&\\mathbf{k}_{2}&\\mathbf{k}_{1}&\\mathbf{k}_{0}\\end{matrix}\\right]\\in\\mathbb{R}^{n\\times n}\\end{gathered}. (28)\n\nAppendix D Configurations\n\nAppendix E Experiments\n\nAppendix F Extrapolation\n\nAppendix G Visualization\n\nIn this section, we visualize Tnn, in particular, we choose the Toeplitz matrix used in Roberta for visualization. \u25c4 Feeling lucky?",
    "tnn-21": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Thu Feb 29 08:43:10 2024 by LaTeXML"
}