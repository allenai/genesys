{
    "softmaxattn-0": "Softmax Attention with Constant Cost per Token\n\nFranz A. Heinsen franz@glassroom.com\n\n(March 28, 2024)\n\nAbstract\n\nWe propose a simple modification to the conventional attention mechanism applied by Transformers: Instead of quantifying pairwise query-key similarity with scaled dot-products, we quantify it with the logarithms of scaled dot-products of exponentials. Our modification linearizes attention with exponential kernel feature maps, whose corresponding feature function is infinite dimensional. We show that our modification is expressible as a composition of log-sums of exponentials, with a latent space of constant size, enabling application with constant time and space complexity per token. We implement our modification, verify that it works in practice, and conclude that it is a promising alternative to conventional attention.111Source code and instructions for replicating our results are online at https://github.com/glassroom/heinsen_attention. 1 Summary\n\nThe conventional attention mechanism of Transformers Vaswani et al. (2017) has become the predominant method for capturing sequential dependencies, but its cost is quadratic in sequence length , because it applies a Softmax function over the rows of an matrix of scaled dot-products. FlashAttention Dao et al. (2022) reduces memory use from quadratic to linear by computing, normalizing, and reducing the matrix in an incremental fashion, i.e., without ever storing it in full, but the compute cost remains quadratic. Numerous alternatives to conventional attention have been proposed to reduce its quadratic cost, including linearized, low-rank, and sparse approximations, mechanisms that slide context windows, and convolutions in the frequency domain.",
    "softmaxattn-1": "We cannot summarize all proposals fairly in the space of a paragraph. Notable examples include the methods proposed by Child et al. (2019), Wang et al. (2020), Kitaev et al. (2020), Katharopoulos et al. (2020), Zhai et al. (2021), Roy et al. (2020), Lee-Thorp et al.",
    "softmaxattn-2": "(2021), and Poli et al. (2023). More recently, generalized state space models that build on previous research Martin and Cundy (2017) Gu et al. (2021) have shown promise by incorporating data-driven mechanisms to control the evolution of a fixed-size latent state Peng et al. (2023) Gu and Dao (2023) Katsch (2023), but their performance is inferior on certain tasks (e.g., recalling arbitrary parts of the input context), motivating the hypothesis that methods with a fixed-size latent space cannot outperform conventional attention Jelassi et al. (2024). 1.1 Modifying Attention\n\nWe find that a simple modification to conventional attention linearizes it Katharopoulos et al. (2020) with exponential kernel feature maps, and we show that this modification renders attention expressible as a composition of log-sums of exponentials, with a fixed-size latent space, for sequential application with constant cost per token. We implement our modification, verify that it works, and conclude that it is a promising alternative. The modification we propose is:\n\nAttention modified \u200b ( Q , K , V ) := Softmax \u2061 ( log \u2061 exp ( Q ) exp ( K ) T exp \u2061 ( c ) ) \u200b V , \\begin{array}[]{c}\\overset{\\text{modified}}{\\operatorname{Attention}}(Q,K,V):=\\\\[3.00003pt]\n\\displaystyle\\operatorname{Softmax}\\left(\\log\\frac{\\exp(Q)\\exp(K)^{T}}{\\exp(c)}\\right)V,\\\\\n\\end{array} (1)\n\nwhere queries , keys and values have , , and elements, respectively, and is a scalar constant, all in . We compute all exponentials elementwise. 1.2 As Log-Sums of Exponentials\n\nIn Section 2, we prove that\n\nAttention modified \u200b ( Q , K , V ) = exp \u2061 ( log \u2061 S \u2212 log \u2061 Z ) , modified Attention \ud835\udc44 \ud835\udc3e \ud835\udc49 \ud835\udc46 \ud835\udc4d \\overset{\\text{modified}}{\\operatorname{Attention}}(Q,K,V)=\\exp(\\log S-\\log Z), (2)\n\nwhere\n\nlog \u2061 S \ud835\udc46 \\displaystyle\\log S = LSE [ d K ] \u2061 ( Q + LSE [ n K ] \u2061 ( K T + log \u2061 V ) \u23df d K \u00d7 d V \u200b elements ) absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e \ud835\udc44 subscript \u23df subscript LSE delimited-[] subscript \ud835\udc5b \ud835\udc3e superscript \ud835\udc3e \ud835\udc47 \ud835\udc49 subscript \ud835\udc51 \ud835\udc3e subscript \ud835\udc51 \ud835\udc49 elements \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}(Q+\\underbrace{\\operatorname{{\\scriptstyle LSE}}_{[n_{K}]}(K^{T}+\\log V)}_{d_{K}\\times d_{V}\\text{ elements}}) (3) log \u2061 Z \ud835\udc4d \\displaystyle\\log Z = LSE [ d K ] \u2061 ( Q + LSE [ n K ] \u2061 ( K T ) \u23df d K \u200b elements ) . absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e \ud835\udc44 subscript \u23df subscript LSE delimited-[] subscript \ud835\udc5b \ud835\udc3e superscript \ud835\udc3e \ud835\udc47 subscript \ud835\udc51 \ud835\udc3e elements \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}(Q+\\underbrace{\\operatorname{{\\scriptstyle LSE}}_{[n_{K}]}(K^{T})}_{d_{K}\\text{ elements}}).",
    "softmaxattn-3": "The elementwise sums are over compatible dimensions, broadcasting over all other dimensions, from left to right\u2014e.g., before reduction, the broadcasted elementwise sum has elements. The functions and compute log-sums of exponentials over the dimension indexed by and , respectively. If any of \u2019s elements are negative, is complex, and therefore so is , but all Softmax mixtures of remain over because they are a composition of operations under which is closed (1). 1.3 Autoregressive Case\n\nFor autoregressive attention, in which and for each query at step we compute attention only over trailing tokens, we note that in (3), all sequential dependencies are modeled by the log-sums computed with , so we can compute autoregressive and with:\n\nlog \u2061 S \ud835\udc46 \\displaystyle\\log S = LSE [ d K ] \u2061 ( Q + LCSE [ n K ] \u2061 ( K T + log \u2061 V ) \u23df d K \u00d7 n K \u00d7 d V \u200b elements ) absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e \ud835\udc44 subscript \u23df subscript LCSE delimited-[] subscript \ud835\udc5b \ud835\udc3e superscript \ud835\udc3e \ud835\udc47 \ud835\udc49 subscript \ud835\udc51 \ud835\udc3e subscript \ud835\udc5b \ud835\udc3e subscript \ud835\udc51 \ud835\udc49 elements \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}(Q+\\underbrace{\\operatorname{{\\scriptstyle LCSE}}_{[n_{K}]}(K^{T}+\\log V)}_{d_{K}\\times n_{K}\\times d_{V}\\text{ elements}}) (4) log \u2061 Z \ud835\udc4d \\displaystyle\\log Z = LSE [ d K ] \u2061 ( Q + LCSE [ n K ] \u2061 ( K T ) \u23df d K \u00d7 n K \u200b elements ) , absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e \ud835\udc44 subscript \u23df subscript LCSE delimited-[] subscript \ud835\udc5b \ud835\udc3e superscript \ud835\udc3e \ud835\udc47 subscript \ud835\udc51 \ud835\udc3e subscript \ud835\udc5b \ud835\udc3e elements \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}(Q+\\underbrace{\\operatorname{{\\scriptstyle LCSE}}_{[n_{K}]}(K^{T})}_{d_{K}\\times n_{K}\\text{ elements}}),\n\nwhere the function computes a log-cumulative-sum of exponentials over the dimension indexed by and . For sequential application, given a new query at step , we need only the end-states of the two log-cumulative-sums of exponentials:\n\nlog \u2061 S t subscript \ud835\udc46 \ud835\udc61 \\displaystyle\\log S_{t} = LSE [ d K ] \u2061 ( Q t + H t ( S ) \u23df d K \u00d7 d V ) absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e subscript \ud835\udc44 \ud835\udc61 subscript \u23df subscript superscript \ud835\udc3b \ud835\udc46 \ud835\udc61 subscript \ud835\udc51 \ud835\udc3e subscript \ud835\udc51 \ud835\udc49 \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}\\big{(}Q_{t}+\\!\\underbrace{H^{\\scriptscriptstyle(S)}_{t}}_{d_{K}\\times d_{V}}\\!\\!\\big{)} (5) log \u2061 Z t subscript \ud835\udc4d \ud835\udc61 \\displaystyle\\log Z_{t} = LSE [ d K ] \u2061 ( Q t + H t ( Z ) \u23df d K ) , absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e subscript \ud835\udc44 \ud835\udc61 subscript \u23df subscript superscript \ud835\udc3b \ud835\udc4d \ud835\udc61 subscript \ud835\udc51 \ud835\udc3e \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}\\big{(}Q_{t}+\\underbrace{H^{\\scriptscriptstyle(Z)}_{t}}_{d_{K}}\\big{)},\n\nwhere hidden states and are the states of the two log-cumulative-sums at step :\n\nH t ( S ) subscript superscript \ud835\udc3b \ud835\udc46 \ud835\udc61 \\displaystyle H^{\\scriptscriptstyle(S)}_{t} = log \u2061 ( exp \u2061 ( H t \u2212 1 ( S ) ) + exp \u2061 ( K t + log \u2061 V t ) ) absent subscript superscript \ud835\udc3b \ud835\udc46 \ud835\udc61 1 subscript \ud835\udc3e \ud835\udc61 subscript \ud835\udc49 \ud835\udc61 \\displaystyle=\\log\\left(\\exp\\left(H^{\\scriptscriptstyle(S)}_{t-1}\\right)+\\exp(K_{t}+\\log V_{t})\\right) (6) H t ( Z ) subscript superscript \ud835\udc3b \ud835\udc4d \ud835\udc61 \\displaystyle H^{\\scriptscriptstyle(Z)}_{t} = log \u2061 ( exp \u2061 ( H t \u2212 1 ( Z ) ) + exp \u2061 ( K t ) ) , absent subscript superscript \ud835\udc3b \ud835\udc4d \ud835\udc61 1 subscript \ud835\udc3e \ud835\udc61 \\displaystyle=\\log\\left(\\exp\\left(H^{\\scriptscriptstyle(Z)}_{t-1}\\right)+\\exp(K_{t})\\right),\n\nwith zeros as their initial condition:\n\nH 0 ( S ) subscript superscript \ud835\udc3b \ud835\udc46 0 \\displaystyle H^{\\scriptscriptstyle(S)}_{0} = { 0 } d K \u00d7 d V absent superscript 0 subscript \ud835\udc51 \ud835\udc3e subscript \ud835\udc51 \ud835\udc49 \\displaystyle=\\{0\\}^{d_{K}\\times d_{V}} (7) H 0 ( Z ) subscript superscript \ud835\udc3b \ud835\udc4d 0 \\displaystyle H^{\\scriptscriptstyle(Z)}_{0} = { 0 } d K .",
    "softmaxattn-4": "absent superscript 0 subscript \ud835\udc51 \ud835\udc3e \\displaystyle=\\{0\\}^{d_{K}}. Together, and hold the latent, or hidden, state of autoregressive attention\u2019s computation at step . They enable us to compute autoregressive attention sequentially with constant time and space complexity per token, . 1.4 Non-Autoregressive Case\n\nFor non-autoregressive attention, in which may differ from and for each query we compute attention over all tokens in the sequence, we compute and with (3). For sequential application, in which we add a new token to the input context at step , with key and value , we compute and for all queries from the updated hidden states:\n\nlog \u2061 S \ud835\udc46 \\displaystyle\\log S = LSE [ d K ] \u2061 ( Q + H t ( S ) ) absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e \ud835\udc44 subscript superscript \ud835\udc3b \ud835\udc46 \ud835\udc61 \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}\\big{(}Q+H^{\\scriptscriptstyle(S)}_{t}\\big{)} (8) log \u2061 Z \ud835\udc4d \\displaystyle\\log Z = LSE [ d K ] \u2061 ( Q + H t ( Z ) ) , absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e \ud835\udc44 subscript superscript \ud835\udc3b \ud835\udc4d \ud835\udc61 \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}\\big{(}Q+H^{\\scriptscriptstyle(Z)}_{t}\\big{)},\n\nwhere and are the hidden states at step (6), with zeros as their initial condition (7). 2 Proof\n\nGiven a query and a key in , the logarithm of the dot-product of their exponentials is , where denotes an elementwise product. Log-sums of exponentials are associative and commutative, making the proof fairly straightforward. For clarity\u2019s sake, we walk step-by-step through a sequence of algebraic manipulations. We start by expanding the Softmax function in (1) and simplifying the resulting expression. We obtain a form of linear attention Katharopoulos et al. (2020) with exponential kernel feature maps:\n\nSoftmax \u2061 ( log \u2061 exp ( Q ) exp ( K ) T exp \u2061 ( c ) ) \u200b V = [ exp ( Q ) exp ( K ) T \u2211 [ n K ] exp ( Q ) exp ( K ) T ] \u200b V , \\begin{array}[]{c}\\displaystyle\\operatorname{Softmax}\\left(\\!\\log\\frac{\\exp(Q)\\exp(K)^{T}}{\\exp(c)}\\right)V=\\\\\n\\\\\n\\begin{bmatrix}\\displaystyle\\frac{\\exp(Q)\\exp(K)^{T}}{\\sum_{[n_{K}]}\\exp(Q)\\exp(K)^{T}}\\end{bmatrix}V,\\\\\n\\end{array} (9)\n\nwhere normalizes each row to a probability distribution. The scaling constant disappears because it becomes a common divisor of numerator and denominator expressions. Note that the feature function corresponding to the exponential kernel is infinite dimensional. Substitute the dot-products of exponentiated queries and exponentiated keys with equivalent explicit summations over elementwise products:\n\n[ \u2211 [ d K ] exp ( Q ) \u2299 exp ( K ) T \u2211 [ n K ] \u2211 [ d K ] exp ( Q ) \u2299 exp ( K ) T ] \u200b V , \\begin{bmatrix}\\displaystyle\\frac{\\sum_{[d_{K}]}\\exp(Q)\\odot\\exp(K)^{T}}{\\sum_{[n_{K}]}\\sum_{[d_{K}]}\\exp(Q)\\odot\\exp(K)^{T}}\\end{bmatrix}V, (10)\n\nwhere the elementwise product is over compatible dimensions, broadcasting over any other dimensions, from left to right, such that the broadcasted elementwise product has elements.222 Properly, we are computing a tensor product over the non-compatible dimensions, but we describe it as a combination of elementwise multiplication and broadcasting because those operations will be more familiar to more readers. Express matrix multiplication with as a summation over broadcasted elementwise products:\n\n\u2211 [ n K ] \u2211 [ d K ] exp ( Q ) \u2299 exp ( K ) T \u2299 V \u2211 [ n K ] \u2211 [ d K ] exp ( Q ) \u2299 exp ( K ) T . \\frac{\\sum_{[n_{K}]}\\sum_{[d_{K}]}\\exp(Q)\\odot\\exp(K)^{T}\\odot V}{\\sum_{[n_{K}]}\\sum_{[d_{K}]}\\exp(Q)\\odot\\exp(K)^{T}}. (11)\n\nBoth and have a dimension indexed by , but does not, so we can sum over that dimension before broadcast-multiplying elementwise with :\n\n\u2211 [ d K ] exp ( Q ) \u2299 \u2211 [ n K ] exp ( K ) T \u2299 V \u2211 [ d K ] exp ( Q ) \u2299 \u2211 [ n K ] exp ( K ) T . \\frac{\\sum_{[d_{K}]}\\exp(Q)\\odot\\sum_{[n_{K}]}\\exp(K)^{T}\\odot V}{\\sum_{[d_{K}]}\\exp(Q)\\odot\\sum_{[n_{K}]}\\exp(K)^{T}}. (12)\n\nDefine and as the expressions that compute numerators and denominators, respectively,\n\nS \ud835\udc46 \\displaystyle S := \u2211 [ d K ] exp ( Q ) \u2299 \u2211 [ n K ] exp ( K ) T \u2299 V \\displaystyle:=\\textstyle\\sum_{[d_{K}]}\\exp(Q)\\odot\\sum_{[n_{K}]}\\exp(K)^{T}\\odot V (13) Z \ud835\udc4d \\displaystyle Z := \u2211 [ d K ] exp ( Q ) \u2299 \u2211 [ n K ] exp ( K ) T , \\displaystyle:=\\textstyle\\sum_{[d_{K}]}\\exp(Q)\\odot\\sum_{[n_{K}]}\\exp(K)^{T},\n\nand take their logarithms. We obtain:\n\nlog \u2061 S \ud835\udc46 \\displaystyle\\log S = LSE [ d K ] \u2061 ( Q + LSE [ n K ] \u2061 ( K T + log \u2061 V ) ) absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e \ud835\udc44 subscript LSE delimited-[] subscript \ud835\udc5b \ud835\udc3e superscript \ud835\udc3e \ud835\udc47 \ud835\udc49 \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}(Q+\\operatorname{{\\scriptstyle LSE}}_{[n_{K}]}(K^{T}+\\log V)) (14) log \u2061 Z \ud835\udc4d \\displaystyle\\log Z = LSE [ d K ] \u2061 ( Q + LSE [ n K ] \u2061 ( K T ) ) , absent subscript LSE delimited-[] subscript \ud835\udc51 \ud835\udc3e \ud835\udc44 subscript LSE delimited-[] subscript \ud835\udc5b \ud835\udc3e superscript \ud835\udc3e \ud835\udc47 \\displaystyle=\\operatorname{{\\scriptstyle LSE}}_{[d_{K}]}(Q+\\operatorname{{\\scriptstyle LSE}}_{[n_{K}]}(K^{T})),\n\nwhich is the same as (3). 3 Implementation\n\nAs proof of concept, we implement our attention mechanism for both autoregressive applications (e.g., generative language modeling) and non-autoregressive applications (e.g., masked language modeling). For simplicity and expediency, we limit our implementation in two significant ways: First, we restrict to elements to avoid dealing with complex floating-point numbers, which incur greater overhead and are more cumbersome to manipulate than real floating-point numbers with existing software infrastructure. Second, when computing autoregressive attention over tokens, we first compute all hidden states with a parallel scan, and then reduce them, which is space-inefficient but easier to implement with existing software infrastructure.333 It is possible to compute autoregressive attention over tokens in parallel more space-efficiently, by evaluating without simultaneously storing all intermediate values of the broadcasted sum in memory. We apply our implementation in a small generative language model (125M parameters, 50257 token ids, 768 embedding features). For numerical stability, in each layer we compute over directly, with a dense feed-forward transformation of token states, implicitly defining as \u2019s exponential but never actually computing it. To remain in , we use the logarithm of attention as input to subsequent transformations in the layer, i.e., the input to subsequent transformations is instead of . Please see our published code for all model details. We train the model on 300B tokens from The Pile Gao et al. (2020) with a conventional sequence length of 1024 tokens, and obtain a cross-entropy loss of 2.47, competitive with state-of-the-art generative language models of similar size. 4 Conclusions\n\nBy all indications, our attention mechanism is a promising alternative to the conventional one, but the evidence we have so far is too scant to be conclusive. An adequate comparison requires addressing our implementation\u2019s temporary limitations and evaluating models with one to several orders of magnitude more parameters on a diverse set of benchmarks and downstream tasks. References\n\nChild et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. CoRR abs/1904.10509. Dao et al. (2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.",
    "softmaxattn-5": "Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 . Gu and Dao (2023) Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Gu et al. (2021) Albert Gu, Karan Goel, and Christopher R\u00e9. 2021. Efficiently modeling long sequences with structured state spaces. CoRR abs/2111.00396. Jelassi et al. (2024) Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying. Katharopoulos et al. (2020) Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. CoRR abs/2006.16236. Katsch (2023) Tobias Katsch. 2023. Gateloop: Fully data-controlled linear recurrence for sequence modeling. Kitaev et al. (2020) Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. CoRR abs/2001.04451. Lee-Thorp et al. (2021) James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Onta\u00f1\u00f3n. 2021. Fnet: Mixing tokens with fourier transforms. CoRR abs/2105.03824. Martin and Cundy (2017) Eric Martin and Chris Cundy. 2017. Parallelizing linear recurrent neural nets over sequence length. CoRR abs/1709.04057. Peng et al. (2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S.",
    "softmaxattn-6": "Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. 2023. Rwkv: Reinventing rnns for the transformer era. Poli et al. (2023) Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. 2023. Hyena hierarchy: Towards larger convolutional language models. Roy et al. (2020) Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Efficient content-based sparse attention with routing transformers.",
    "softmaxattn-7": "CoRR abs/2003.05997. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. CoRR abs/1706.03762. Wang et al. (2020) Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity. CoRR abs/2006.04768. Zhai et al. (2021) Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh M.",
    "softmaxattn-8": "Susskind. 2021. An attention free transformer. CoRR abs/2105.14103. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Sun May 5 18:43:39 2024 by LaTeXML"
}