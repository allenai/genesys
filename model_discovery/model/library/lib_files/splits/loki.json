{
    "loki-0": "# Loki: Low-Rank Keys for Efficient Sparse Attention \n\nPrajwal Singhania<br>prajwal@umd.edu\n\nSiddharth Singh<br>ssingh37@umd.edu\n\nShwai He<br>shwaihe@umd.edu\n\nSoheil Feizi<br>sfeizi@cs.umd.edu\n\nAbhinav Bhatele<br>bhatele@cs.umd.edu\n\nDepartment of Computer Science\nUniversity of Maryland\n\n\n#### Abstract\n\nInference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used.",
    "loki-1": "In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs. ## 1 Introduction\n\nAs the sizes of large language models (LLMs) increase, deploying them for efficient inference remains a significant challenge, primarily due to the computation and memory access bottlenecks in the self-attention block [31]. These challenges arise due to the auto-regressive nature of attention, where the output is generated token by token. At each generation step, the entire preceding state, represented by the key-value (KV) cache [23], must be fetched from memory. This state can, at times, be larger than the model parameters themselves [17]. The cost of reading the KV cache from the GPU DRAM to the registers at each generation step is prohibitively expensive. During inference, this KV-cache read/write cost scales quadratically with the sequence length, as opposed to training where it scales linearly. Moreover, the computational cost of matrix multiplications in the attention layers also scales quadratically with the sequence length [36]. Several strategies [38, 25, 18] have been proposed to address this challenge by reducing the computational complexity and/or memory demands associated with the self-attention mechanism. One promising category of approaches focuses on approximating attention, employing techniques such as quantization (compression along the bit dimension) or using a subset of the the number of tokens in the KV cache [9] (sparse attention). While other sparse attention methods either permanently throwing away tokens [38] or have a fixed sparsity pattern [33], we focus on using the keys (in the feature dimension) to decide which tokens\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-02.jpg?height=457&width=1313&top_left_y=246&top_left_x=399)\n\nFigure 1: (Left) Rank at which $90 \\%$ of the variance is explained averaged across all layers and heads for different models. (Right) Overview of Loki . might be important. We hypothesize that the keys in the KV cache lie in a low-dimensional space. We begin with exploring the intrinsic dimensionality of the key vectors in the $K V$ cache. We perform principal component analysis (PCA) [15] on the key vectors within each layer and head using a calibration dataset to study their dimensionality. As shown in Figure 1 (left), we find that the key vectors indeed lie in a low-dimensional space for several popular and recent models like Llama-3 [2] and Mixtral [14]. Next, we exploit the low dimensionality of the key vectors to develop a sparse attention method that does not significantly sacrifice model quality and also reduces data movement and compute costs. We call our method 'Loki'. We store all the principal components of the PCA-transformed keys, but use the first $d$ to compute approximate attention scores for picking the top- $k$ tokens. We then use the dimensionality only for the selected keys to compute the final attention scores. Figure 1(right) provides an overview of Loki. Our theoretical complexity analysis shows that Loki can provide significant speedups in the attention step. However, empirically, this requires efficient implementation of Loki to minimize data movement in the additional operations introduced. Thus, we implement optimized matrix multiplication kernels for Loki, leading to a speedup of up to $40 \\%$ over base attention for the Llama2-13B model. For this speedup setting, the average degradation in model accuracy (measured across 6 different benchmarks and 8 different models) is only $6.8 \\%$ points. Our contributions can be summarized as follows:\n\n- Detailed analysis showing the intrinsic low-dimensionality of keys in self-attention, its variation across layers for different models, and consistency across different datasets. - Loki: a sparse attention method that exploits the aforementioned low dimensionality of keys to speedup attention computation without sacrificing model quality. - Optimized kernels for efficient implementation of Loki in PyTorch. - Evaluation of Loki on multiple LLMs and downstream tasks, showing that it can achieve significant speedups with minimal degradation in model quality. ## 2 Background and Related Work\n\nThe attention mechanism [31] is at the core of the transformer architecture. Consider a single attention query head with head dimension $D$, processing an input token sequence of length $S$. During auto-regressive generation, the output of the attention head is calculated as:\n\n$$\n\\boldsymbol{y}=\\operatorname{softmax}\\left(\\frac{\\mathbf{q} \\mathbf{K}^{\\top}}{\\sqrt{D}}\\right) \\cdot \\boldsymbol{V}\n$$\n\nwhere $\\mathbf{q} \\in \\mathbb{R}^{1 \\times D}$ is the query, and $\\mathbf{K} \\in \\mathbb{R}^{S \\times D}$ and $\\mathbf{V} \\in \\mathbb{R}^{S \\times D}$ are the key and value caches respectively. Additionally, newer transformer models add Rotational Positional Embeddings (RoPE) [28] to the keys and query, before computing the attention scores. Since every query attends to all past keys, the attention mechanism has a quadratic complexity $\\mathcal{O}\\left(S^{2}\\right)$ in the number of tokens. ### 2.1 Related Work\n\nNumerous studies have explored the low-rank structures in transformers for various purposes. Linformer [32] demonstrated that the attention score matrix is low-rank and proposed alternative attention formulations through low-rank factorizations during training for linear computational complexity. LoRA [11] showed that the weights of transformers and the updates to these weights during finetuning reside in a low-dimensional subspace. To the best of our knowledge, our work is the first to study the intrinsic low dimensionality of the attention keys themselves and demonstrate the generalizability of this low-dimensional structure in a post-training setting (for natural language data). Sparse-transformers [5] was one of the first works to introduce a sparse-attention method employing fixed or strided sparsity patterns in the attention mechanism. Reformer [16] used locally-sensitive hashing to compute attention scores in a sparse manner. Performer [6] used positive orthogonal random features to approximate the attention mechanism. Unlike these methods, which require training or fine-tuning, our approach operates entirely post-training without any fine-tuning. Another category of sparse attention methods employ token eviction policies to permanently delete tokens from the KV-cache based on some heuristic. StreamingLLM [33] introduced the concept of attention sink, using initial tokens and a rolling KV-cache for processing infinite-length sequences. Zhang et al. [38] retain only \"Heavy Hitters\" tokens in the KV-cache based on accumulated attention scores. Liu et al. [18] propose Scissorhands, which prioritizes important tokens based on the \"Persistence of Importance Hypothesis\". Ge et al. [8] propose an adaptive eviction policy for each transformer layer. These methods are effective in reducing the memory and compute footprint of the attention but suffer from permanent loss of information that can lead to generation a non-trivial degradation in model quality. Our method does not involve any permanent loss of information with the trade-off of not reducing the memory footprint. Quantization-based approximate approaches [12, 21] are complimentary to our work and can be applied in tandem. SPAR-Q Attention [25] is a recent work that inspires our approach. They use the maximum magnitude dimensions of the queries, and the corresponding dimensions of the keys to compute approximate attention scores, and then use the full attention scores for the top-k keys. However, the main drawback of their method is that it requires expensive non-contiguous indexing of columns of the keys matrix. Further, they store two copies of the past keys leading to a $50 \\%$ increase in memory usage. Loki does not require additional memory, and the natural ordering of the principal components allows one of the two indexing operations to be replaced with a more efficient slicing operation. ## 3 Dimensionality Analysis of Attention Keys\n\nRQ1: Do attention keys in transformer models lie in a significantly lower dimensional space? In this section, we analyze whether most of the variance in the keys can be explained using dimensions $d<D$, where $D \\in \\mathbb{Z}^{+}$is the full dimensionality of the keys. ### 3.1 Setup\n\nTo investigate the dimensionality of attention keys, we run 11 transformer-based models: Llama-2 7B/13B/70B [30], Llama-3 8B/70B [2], TinyLlama-1.1B [37], Pythia-6.9B [4], Mistral-7B [13], Mixtral-8x7B/8x22B [14], and Phi3-Mini-4K [20] on 3 popular English language datasets datasets: WikiText-2 [19] (Validation Split), C4 [24] (Custom Split), and BookCorpus [39] (Custom Split). Custom splits are used for datasets where the validation split is not available. We run perplexity evaluation on these datasets and save the generated attention keys, before and after the application of rotary embeddings [28], referred to as pre-rotary and post-rotary keys, respectively throughout the paper. We then perform PCA on all the keys generated for each layer and head individually. The metric we use in our analysis is the rank at which $v \\%$ of the variance is explained by the principal components. We calculate this metric for each layer and head of the models as follows:\n\n$$\n\\operatorname{Rank}_{l, h} @ v=\\min \\left\\{d \\in \\mathbb{Z}^{+}: \\sum_{j=1}^{d} \\lambda_{l, h}^{j} \\geq v / 100\\right\\}\n$$\n\nwhere, $\\lambda_{l, h}^{j}$ is the $j^{t h}$ normalized eigenvalue of the covariance matrix of the keys for the $l^{t h}$ layer and $h^{t h}$ head. We average this metric ranks across all heads of the $l^{t h}$ layer and refer to it as $R a n k_{l} @ v$. ### 3.2 Analysis and Discussion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-04.jpg?height=550&width=1375&top_left_y=520&top_left_x=366)\n\nFigure 2: Rank at which $90 \\%$ of the variance is explained for pre-rotary and post-rotary keys produced by each layer averaged across all heads ( $\\operatorname{Rank}_{l} @ 90$ ) for different models. We observe that all models exhibit significantly low rank (Full dimensionality is 128 or 96 ) consistently across all datasets. Figure 1 (left) shows the average $\\operatorname{Rank}_{l} @ 90$ averaged across all layers for models with full key dimensionality of 128 . We can see that the average rank is significantly lower than the full dimensionality of the keys for all models. Diving deeper, we present a layer-wise analysis for a few models: Llama2-7B, Llama3-70B, Mixtral-8x7B, and Phi3-Mini-4K in Figure2.",
    "loki-2": "The results for the other models are similar and can be found in the appendix A. 1 . We observe that the dimensionality of the keys (both pre-rotary and post-rotary) is significantly lower than the full dimensionality of the keys across all calibration datasets. Furthermore, the $R a n k_{l} @ 90$ for a particular layer is consistent across datasets, for all combinations of models and datasets. This indicates that the lower-dimensional structure of the keys is consistent when calculated using different calibration datasets. Another trend we observe is that the initial layers of most models have a very low rank, as compared to the later layers, and this trend is particularly prominent for the pre-rotary keys . Lastly, we also observe that for most models, the average of $R a n k_{l} @ 90$ across all layers is lower for pre-rotary keys as compared to post-rotary keys, indicating that the rotary embeddings increase the dimensionality of the keys. Further analysis on the variation of the rank across different heads within a layer and across different layers within a model can be found in the appendix. These results indicate the existence of the following properties: (1) The keys produced by the attention layers of transformer models lie in a significantly lower-dimensional space. (2) The lower-dimensional structure of the keys is consistent across different calibration datasets. (3) Rotary embeddings increase the dimensionality of the keys for most models. We now use the first two properties to propose Loki, an efficient sparse-attention method. ## 4 Method: Loki\n\nRQ2: Can the lower intrinsic dimensionality of the keys be used to optimize attention? In this section, we present Loki, a novel sparse attention method leveraging PCA for top- $k$ attention computation (i.e. computing attention with a $k$ sized subset of the KV-cache tokens). We first prove some theoretical properties of attention in the PCA-transformed space, and then present the Loki algorithm. We also present a memory-efficient kernel the top- $k$ attention computation, crucial for empirical speedups with our method. ### 4.1 Properties of Attention in the PCA Transformed Space\n\nFirst, we introduce some lemmas that provide the rationale for our approach to computing attention in the PCA-transformed space.",
    "loki-3": "Lemma 4.1. Let $\\mathbf{P} \\in \\mathbb{R}^{D \\times D}$ be the $P C A$ projection matrix calibrated offline on a dataset. Assuming we are generating the $S^{\\text {th }}$ token in the sequence, let $\\mathbf{q}_{S} \\in \\mathbb{R}^{1 \\times D}$ be the query vector for the $S^{\\text {th }}$ token, $\\mathbf{K}_{: S} \\in \\mathbb{R}^{S \\times D}$ be the key vectors, including the past $(S-1)$ keys and the current key. Then, the attention scores computed using the PCA-transformed query and keys are equivalent to the attention scores computed using the original query and keys. Proof. Let $\\mathbf{q}_{S}^{\\prime}=\\mathbf{q}_{S} \\mathbf{P}$ and $\\mathbf{K}_{: S}^{\\prime}=\\mathbf{K}_{: S} \\mathbf{P}$ be the PCA transformed query and key vectors. Focusing on the dot product term in the attention computation (Equation 1), we have:\n\n$$\n\\begin{aligned}\n\\mathbf{q}_{S} \\mathbf{K}_{: S}^{T} & =\\mathbf{q}_{S}\\left(\\mathbf{K}_{: S}^{\\prime} \\mathbf{P}^{T}\\right)^{T} \\text { [inverting the PCA transform] } \\\\\n& =\\mathbf{q}_{S}\\left(\\left(\\mathbf{P}^{T}\\right)^{T} \\mathbf{K}_{: S}^{\\prime T}\\right)=\\left(\\mathbf{q}_{S} \\mathbf{P}\\right) \\mathbf{K}_{: S}^{\\prime T}=\\mathbf{q}_{S}^{\\prime} \\mathbf{K}_{: S}^{\\prime T}\n\\end{aligned}\n$$\n\nIt is important to note here that Lemma 4.1 holds for any orthogonal $\\mathbf{P}$. Lemma 4.2. Let $\\mathbf{K}_{: S,: d}^{\\prime} \\in \\mathbb{R}^{S \\times d}(d<D)$ be the reduced dimension key vectors obtained by projecting the key vectors onto the first d principal components of $\\mathbf{P}$. Then, the attention scores computed using $\\mathbf{K}_{: S,: d}^{\\prime}$ are a good approximation of the the actual attention scores.",
    "loki-4": "Proof. Let $\\mathbf{R}_{: d} \\in \\mathbb{R}^{d \\times D}$ be an orthogonal transformation that transforms the keys into the reduced dimension space as $\\mathbf{L}_{: S,: d}=\\mathbf{K}_{: S} \\mathbf{R}_{: d}$. Our objective is to minimize the following reconstruction error:\n\n$$\n\\min _{\\mathbf{R}_{: d}}\\left\\|\\mathbf{q}_{S} \\mathbf{K}_{: S}^{T}-\\mathbf{q}_{S}\\left(\\mathbf{L}_{: S,: d} \\mathbf{R}_{: d}^{T}\\right)^{T}\\right\\|_{2}^{2}\n$$\n\nUsing Cauchy-Schwarz inequality, we have:\n\n$$\n\\left\\|\\mathbf{q}_{S} \\mathbf{K}_{: S}^{T}-\\mathbf{q}_{S}\\left(\\mathbf{L}_{: S,: d} \\mathbf{R}_{: d}^{T}\\right)^{T}\\right\\|_{2}^{2} \\leq\\left\\|\\mathbf{q}_{S}\\right\\|_{2}^{2}\\left\\|\\mathbf{K}_{: S}^{T}-\\left(\\mathbf{L}_{: S,: d} \\mathbf{R}_{: d}^{T}\\right)^{T}\\right\\|_{2}^{2}\n$$\n\nWe change our objective to minimize the upper bound on the RHS instead of the original objective. We know that PCA minimizes the reconstruction error (2nd term in the RHS) among all the orthogonal transformations. Thus, it follows that the optimal value of $\\mathbf{R}_{: d}^{*}=\\mathbf{P}: d$, and $\\mathbf{L}_{: S,: d}^{*}=\\mathbf{K}_{: S,: d}^{\\prime}$\n\nSince we minimize an upper bound when proving Lemma 4.2 it is possible that some other transformation might give a better approximation to the dot product. Thus, in our experiments, we use PCA transforms computed on both the pre-rotary and post-rotary keys as candidate transformations. Using the above lemmas and our dimensionality analysis showing that the key vectors have low intrinsic dimensionality, we are now ready to present the algorithm for Loki. ### 4.2 Algorithm and Complexity Analysis\n\nIn this section, we propose the algorithm for our PCA-based Top-K Attention approach (Loki). Previous works [34, 29] have shown that attention scores for a query are highly concentrated on a small subset of keys. This observation has led to several methods to compute attention using only the top- $k$ keys. However, these previous works either compute the exact attention scores and then select the top- $k$ keys [10] or compute non-exact scores but have significantly higher memory requirements [25]. Loki alleviates these issues by computing approximate attention scores (for ranking the keys) in the reduced lower-dimensional space, without any significant increase in memory requirements. Algorithm 1 shows our Loki method. Line 5 of the algorithm computes the approximate attention scores using $d$ principal dimensions of the query and key vectors. Lines 6-7 select the top- $k$ keys based on the approximate attention scores. Line 8 computes the exact attention scores using the selected top- $k$ keys, directly in the transformed space (Lemma 4.1. Compute and Memory Analysis: For vanilla attention, the complexity of computing $\\mathbf{q}_{S} \\mathbf{K}_{: S}^{T}$ is $\\mathcal{O}(D S)$ and the complexity of multiplying the values with the attention scores is $\\mathcal{O}(D S)$. For Loki, the complexity of calculating the approximate attention scores (Line 5) is $\\mathcal{O}(d S)$. The complexity of selecting the top-K keys (Lines 6-7) is approximately $\\mathcal{O}(\\operatorname{Slog}(S)+k)$. The complexity of calculating the exact attention scores and multiplying with the values (Line 8-9) is $\\mathcal{O}(2 D k)$. Additionally, the\n\n```\nAlgorithm 1 Loki\nRequire: At the \\(S^{t h}\\) step - Input: \\(\\mathbf{x}_{S} \\in \\mathbb{R}^{1 \\times D}\\), KV-Cache: \\(\\mathbf{K}_{: S-1}^{\\prime}, \\mathbf{V}: S-1 \\in \\mathbb{R}^{(S-1) \\times D}\\), Projection\n    Matrix: \\(\\mathbf{P} \\in \\mathbb{R}^{D \\times D}, d, k\\)\n    function Loki-ATTENTION \\(\\left(\\mathbf{x}_{S}, \\mathbf{K}_{: S-1}^{\\prime}, \\mathbf{V}_{: S-1}, \\mathbf{P}, d, k\\right)\\)\n        \\(\\mathbf{q}_{S}, \\mathbf{k}_{S}, \\mathbf{v}_{S} \\leftarrow\\) computeQKV \\(\\left(\\mathbf{x}_{S}\\right)\\)\n        \\(\\mathbf{q}_{S}^{\\prime} \\leftarrow \\mathbf{q}_{S} \\mathbf{P}, \\mathbf{k}_{S}^{\\prime} \\leftarrow \\mathbf{k}_{S} \\mathbf{P}\\)\n        \\(\\mathbf{K}_{: S}^{\\prime} \\leftarrow \\operatorname{concat}\\left(\\mathbf{K}_{: S-1}^{\\prime}, \\mathbf{k}_{S}^{\\prime}\\right), \\mathbf{V}_{: S} \\leftarrow \\operatorname{concat}\\left(\\mathbf{V}_{: S-1}, \\mathbf{v}_{S}\\right)\\)\n        \\(A_{\\text {approx }} \\leftarrow \\mathbf{q}_{S,: d}^{\\prime}\\left(\\mathbf{K}_{: S,: d}^{\\prime}\\right)^{T}\\)\n        indices \\(\\leftarrow\\) topk \\(\\left(A_{\\text {approx }}, k\\right)\\)\n        \\(\\mathbf{K}_{: S}^{\\prime \\prime} \\leftarrow \\mathbf{K}_{: S}^{\\prime}[\\) indices \\(], \\mathbf{V}_{: S}^{\\prime \\prime} \\leftarrow \\mathbf{V}_{: S}[\\) indices \\(]\\)\n        \\(A_{\\text {exact }} \\leftarrow \\operatorname{softmax}\\left(\\frac{\\mathbf{q}_{S}^{\\prime}\\left(\\mathbf{K}_{: S}^{\\prime \\prime}\\right)^{T}}{\\sqrt{D}}\\right)\\)\n        return \\(A_{\\text {exact }} \\mathbf{V}_{: S}^{\\prime \\prime}\\)\n    end function\n```\n\ncomplexity of projections into the PCA space (Line 3) is $\\mathcal{O}\\left(2 D^{2}\\right)$. Assuming the complexity of selecting the top- $k$ keys is small compared to the other operations, the overall complexity of the algorithm is $\\mathcal{O}\\left(d S+2 D k+2 D^{2}\\right)$. Then, we have:\n\n$$\n\\text { speedup }=\\frac{2 D S}{d S+2 D k+2 D^{2}}=\\frac{1}{d / 2 D+k / S+D / S} \\approx \\frac{1}{d_{f} / 2+k_{f}} \\quad(\\text { given } D<<S)\n$$\n\nwhere, $d_{f}=d / D$ and $k_{f}=k / S$. The memory requirement of the KV-Cache is the same as the original attention, with a small overhead of storing the PCA transformation matrix. ### 4.3 Implementation in Triton\n\nPerforming Loki efficiently involves complex indexing operations within the KV-Cache (lines 5 and 7 of Algorithm 11. Standard PyTorch operations create temporary, dense copies of the KV-Cache data in memory, leading to slowdowns due to expensive memory access. To alleviate this issue, we develop optimized kernels in Triton [1] for the three matrix multiplication operations in Loki. Our kernels can directly access relevant subsets of the KV-Cache (both feature and sequence dimensions) and perform computations within GPU registers. This eliminates the need for creating dense copies, significantly improving performance. Our approach builds on SPAR-Q [25], which introduced similar kernels for top-k attention calculations. However, we identified and addressed inefficiencies in the SPAR-Q kernels, which resulted in speedups of nearly $2-3 \\times$ in certain scenarios. (see Appendix C). ## 5 Evaluation\n\nIn this section, we present our experimental setup and evaluation of Loki via three studies: (1) Comparing Loki with baselines on common ML benchmarks, (2) Examining its generalizability across various calibration datasets, and (3) Benchmarking its computational efficiency against vanilla attention computation. ### 5.1 Experimental Setup\n\nFor our ML benchmarking, we evaluate our method using two approaches - perplexity evaluation on WikiText-2 [19] dataset (test split) and downstream task performance using the LLM-harness benchmark [7]. For the downstream tasks, we choose the same tasks as the HuggingFace OpenLLM leaderboard [3]: The metrics used for these tasks are identical to the leaderboard\nWe compare our method against 3 baselines - full attention without any approximations, the exact TopK approach which computes the exact attention scores and then uses the top-k tokens to compute the final output, and H 2 O [38] method which is a popular token-eviction method. For these comparisons, we show the results with a budget size of $k_{f}=0.25$ and 0.125 . For our method, we additionally use $d_{f}=0.25$ and 0.125 . This configuration of our represents a 2.6 x theoretical speedup. Table 1 provides an overview of the baseline methods and the associated budget terms. H2O's budget was split equally between the heavy hitter and recent tokens, as per the original paper. For H 2 O , we were\n\nTable 1: Explanation of key-budget and dimensionality (dim.) for baselines and our method, along with the speedup and memory savings. | Method | Budget | Dim. | Description | Speedup | Memory Savings |\n| :--- | :---: | :---: | :--- | :---: | :---: |\n| Exact Top-K | $k_{f}$ | Full | $k_{f}$ fraction of keys selected using exact attention scores | No | No |\n| H 2 O | $k_{f}$ | Full | $k_{f}$ fraction of keys \\& values selected using H2O policy | $\\frac{1}{k_{f}}$ | $\\frac{1}{k_{f}}$ |\n| Loki | $k_{f}$ | $d_{f}$ | $k_{f}$ fraction of keys \\& values selected using attention <br> scores computed with $d_{f}$ fraction of full dimensionality | $\\frac{1}{\\left(d_{f} / 2\\right)+k_{f}}$ | No |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-07.jpg?height=480&width=1311&top_left_y=586&top_left_x=406)\n\nFigure 3: Evaluation of Loki on perplexity (left) and downstream tasks (right) for different models.",
    "loki-5": "For downstream tasks, we show the average performance across all tasks mentioned in 5.1\nunable to run the GSM8K task as the the author's ML benchmarking code was too memory intensive to run for that task. For the aforementioned experiments, we generate PCA transforms using the WikiText-103 dataset. For the generalizability study, we compare the results of our method with PCA transforms from different calibration datasets: WikiText-103 [19], C4 [24], and BookCorpus [39]. Additionally, we also benchmark our triton based implementation of PCA for running attention in a Llama2-13B-like setup (same hidden size and number of heads) for various prompt and generation lengths, and demonstrate speedups over vanilla attention. All experiments are run on NVIDIA A100 GPUs with 40 and 80 GB of memory on the Perlmutter [22] supercomputer. For larger models, we use AxoNN [26, 27] to shard the model across multiple GPUs. ### 5.2 Results and Discussion\n\nLet us begin our discussion with Figure 3, showing the perplexity (left) and downstream task (right) evaluation results for Loki on Llama2-7B/13B, Llama3-8B/70B, Mistral-7B, and Mixtral-8x22B models. We'll focus on the Llama2-7B model, comparing pre-rotary (light green/purple) and postrotary (dark green/purple) PCA transforms for different $k_{f}$ and $d_{f}$ values. For Llama2-7B, we see that the performance of both candidate transforms is similar and close to the full attention model. This trend is consistent across all the models except for Llama3-8B/70B and Mistral-7B, where the post-rotary PCA transform performs significantly worse than the pre-rotary one. For Llama3-8B, perplexity jumps from about 5 to over 10 , a significant decline not seen with the pre-rotary transform. Mistral-7B shows a similar pattern. This is a surprising observation as one might expect the postrotary PCA transform to perform better. We do not have a clear explanation for this observation and further investigation into RoPE embeddings is warranted to understand this behavior. Nevertheless, we see that at least one of the PCA transforms performs well for any model, and this is an important hyperparameter to tune when using our method. For subsequent results, we only show results from the better-performing transformation for each model. Comparing different $\\left(k_{f}, d_{f}\\right)$ settings, we see that using $k_{f}=0.25$ and $d_{f}=0.25$ (green), is better than using $k_{f}=0.125$ and $d_{f}=0.5$ (purple) for all models. These two settings balance speed and performance well, with the first being superior. Next, we compare the performance of Loki with various baselines, using $k_{f}=0.25$ for all methods and $d_{f}=0.25$ for ours. Table 2 shows the perplexity results for Llama2-7B, Llama2-13B, Llama3-8B, and Mistral-7B. Loki's perplexity drop is within 0.1 of full attention across all models, a threshold considered acceptable for attention mechanism approximations [35]. In contrast, H2O's perplexity drop exceeds 0.1 , nearing 0.2 for all models. Figure 4 confirms this trend in downstream task\n\nTable 2: Perplexity (Lower is better) evaluation of Loki and baselines for different models\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-08.jpg?height=717&width=1403&top_left_y=306&top_left_x=360)\n\nFigure 4: Downstream Task Performance higher is better for Loki and baselines for different models. GSM8K is excluded, as we were unable to run H 2 O for this task. evaluation. Loki performs similarly to full attention for all models, except Llama3-8B, where the performance is notably worse, though still better than H2O. Importantly, on the challenging MMLU task, Loki degrades less than H2O. Comparing Loki with Exact-TopK, we find similar performance for Llama2-7B, Llama2-13B, and Mistral-7B. Exact-TopK represents the upper performance bound for Loki if it could perfectly select the top- $k$ tokens. To understand why Loki works well, we examined the top-k agreement between attention scores from low-dimensional keys and exact attention scores. Figure 5 shows a Jaccard similarity between the top- $k$ tokens selected by both methods across all layers and heads for Llama27B. For the settings: $\\left(k_{f}=0.25, d_{f}=0.25\\right)$ and $\\left(k_{f}=0.125, d_{f}=0.5\\right)$, evaluated in Figure 3, we see the Jaccard similarity is between 0.85 and 0.95 , validating that the Loki is able to select the top- $k$ tokens with high accuracy. ![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-08.jpg?height=414&width=1291&top_left_y=1663&top_left_x=408)\n\nFigure 5: (Left) Top- $k$ agreement between Loki and Exact-TopK methods for Llama2-7B. (Right) Performance of Loki using transformations derived from different calibration datasets. We now turn our attention to the generalizability of the PCA transformations used in our method. Figure 5 (right) shows the performance of Loki using PCA transformations derived from different calibration datasets $\\left(k_{f}=0.25, d_{f}=0.25\\right)$. For a given model, we consistently use the bestperforming candidate PCA transformation (pre or post), based on the perplexity evaluation in Figure 3. for all the calibration datasets. We see that the performance of Loki is consistent across different calibration datasets, indicating that the PCA transformations used in our method are generalizable. This is an important observation as it shows that the PCA keys can be generated using a variety of calibration datasets and still achieve good performance. ![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-09.jpg?height=387&width=1328&top_left_y=240&top_left_x=365)\n\nFigure 6: Benchmarking vanilla attention and Loki attention in Llama2-13B using huggingface transformers.",
    "loki-6": "We use a prompt length of 3072 and generate 1024 tokens for the two breakdown plots on the left. For all figures, we use a batch size of 16 . For each datapoint, we do 10 runs and report the average time. Across all runs, we found the std.",
    "loki-7": "dev. in times to be less than 0.05 percent of the mean. Analyzing Llama2-13B with Hugging Face Transformers exposed an interesting bottleneck (Figure 6, leftmost). Regardless of the attention type (vanilla or Loki), more than $80 \\%$ of the inference time is consumed within the Hugging Face framework for appending key-value pairs for the latest token to the KV-Cache. This shared bottleneck minimizes the overall performance improvement of our optimizations. We hypothesize that using a more advanced inference system like vLLM [17] with efficient KV-Cache management could significantly reduce this append time, but leave that exploration for future work. To isolate the impact of our optimizations, the remaining plots in Figure 6 focus solely on the attention computation time, excluding the KV-Cache append time. In the second from left plot of Figure 6. we observe that Loki speeds up the total compute time spent in attention (sans KV-Cache appends) by nearly $40 \\%$ ! This is despite the fact that Loki incurs an extra matrix multiplication operation (orange, line 5 of Algorithm 11 to compute attention scores. In the other two figures, we compare the performance of Loki and vanilla attention for two prompt lengths - 2048 and 3072 and various generation lengths. For the shorter prompt length of 2048 we observe a speedup of around $35 \\%$, whereas for the longer prompt length of 3072 , we observe a larger speedup of $40 \\%$. This trend is expected as larger prompts result in a bigger KV-Cache, amplifying the impact of our optimizations. Limitations and Future Work: Our compute benchmarking shows that the cost of selecting the top$k$ keys impacts overall speedup, and a custom kernel for this operation could enhance performance. Additionally, the memory cost of updating the KV-Cache is a bottleneck shared with HuggingFace's implementation of vanilla attention. Using a more efficient KV-Cache management system is left for future work. Our method does not focus on reducing memory usage, and a potential future direction could involve utilizing CPU memory to store all the keys, and only transferring the top- $k$ keys to the GPU. The surprising observation that the pre-rotary PCA transforms perform better than the post-rotary ones for some models also warrants further investigation. Our key observation of the low intrinsic dimensionality of the keys opens up several avenues for future research, and further fine-tuning of Loki (like using per layer $d_{f}$ values) could lead to better results. ## 6 Conclusion\n\nIn conclusion, this work aims to devise an efficient sparse attention method, Loki, that does not compromise the model quality while reducing the computational complexity of attention in transformer models. We make a key observation that the key vectors in attention lie in a low-dimensional space, across different models and datasets. This insight, by itself, is interesting and can be investigated further in future work. Leveraging this insight, Loki uses attention scores computed in a lower dimensional space to rank and to select the top- $k$ tokens. It then uses the full dimensionality only for the selected tokens to compute the final attention. Our theoretical analysis shows that Loki can provide significant speedups in the attention step. To implement this efficiently, we develop optimized kernels that reduce data movement between the GPU and registers during the additional operations introduced by Loki. Our empirical evaluation shows that Loki performs better than popular approximation methods on a variety of models and tasks, with respect to preserving model quality. Finally, we show that Loki can provide speedups of up to $40 \\%$ over the base attention empirically, making it a promising approach to address the computational challenges in transformer inference. ## References\n\n[1] Introducing triton: Open-source gpu programming for neural networks. https://openai com/index/triton/ 2021. [2] Introducing meta llama 3: The most capable openly available llm to date. https://ai.meta com/blog/meta-llama-3/, 2024. [3] Edward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https: //huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023. [4] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397-2430. PMLR, 2023. [5] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers.",
    "loki-8": "arXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.",
    "loki-9": "CoRR, abs/2106.09685, 2021 . [12] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference, 2017. [13] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [14] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. Mixtral of experts, 2024. [15] Ian T Jolliffe and Jorge Cadima. Principal component analysis: a review and recent developments. Philosophical transactions of the royal society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016. [16] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.",
    "loki-10": "arXiv preprint arXiv:2001.04451, 2020. [17] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [18] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.",
    "loki-11": "arXiv preprint arXiv:2305.17118, 2023. [19] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.",
    "loki-12": "CoRR, abs/1609.07843, 2016. [20] Microsoft. Introducing phi-3: Redefining what's possible with slms. https://azure.microsoft.com/en-us/blog/ introducing-phi-3-redefining-whats-possible-with-slms/, 2024. [21] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization, 2021. [22] NERSC. Perlmutter system architecture. https://docs.nersc.gov/systems/ perlmutter/architecture/\n[23] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022.",
    "loki-13": "[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023. [25] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference, 2023. [26] Siddharth Singh and Abhinav Bhatele. AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. In Proceedings of the IEEE International Parallel \\& Distributed Processing Symposium, IPDPS '22. IEEE Computer Society, May 2022. [27] Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, and Abhinav Bhatele. A 4d hybrid algorithm to scale parallel training to thousands of gpus, 2024. [28] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023.",
    "loki-14": "[29] Mingjie Sun, Xinlei Chen, J. Zico Kolter, and Zhuang Liu. Massive activations in large language models, 2024. [30] Hugo Touvron et al. Llama 2: Open foundation and fine-tuned chat models, 2023. [31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. [32] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity, 2020. [33] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [34] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024. [35] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. 2023. [36] Biao Zhang, Ivan Titov, and Rico Sennrich. Sparse attention with linear units, 2021. [37] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. [38] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R\u00e9, Clark Barrett, et al. H _2 o: Heavy-hitter oracle for efficient generative inference of large language models.",
    "loki-15": "arXiv preprint arXiv:2306.14048, 2023. [39] Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, 2015. ## A Comprehensive Dimensionality Analysis\n\n## A. 1 Ranks Analysis for All Models\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-13.jpg?height=1626&width=1362&top_left_y=408&top_left_x=379)\n\nFigure 7: Rank at which $90 \\%$ of the variance is explained for pre-rotary and post-rotary keys produced by each layer averaged across all heads ( $\\operatorname{Rank}_{l} @ 90$ ) for different models. We observe that all models exhibit significantly low rank consistently across all datasets. In this section, we present our dimensionality analysis results (from 3) for all the models we experimented with. Figure 7 shows the $\\operatorname{Rank}_{l} @ 90$ for all the models mentioned in Section 3 We observe that our findings around the low dimensionality of the keys are consistent across all models and datasets. The results for the other models are similar to the ones shown in Figure 2. The set of models we experimented with encompasses a wide range of model sizes, architecture classes (dense vs MoE models), older and newer models, and models trained on different datasets. Even with these variations, our key observation holds true. An interesting trend we observe is that the $R_{a n} k_{l} @ 90$ varies across layers for different models. This indicates that the intrinsic dimensionality of the keys is\nnot uniform across layers of a model. A possible future direction could be to investigate the reasons behind this variation, and whether having a per layer $d_{f}$ in Loki could lead to better results. Figure 8 shows the normalized eigenvalues of the covariance matrix of the keys for a few layers and heads of Llama2-7B, Mistral-7B, and Pythia-6.9B on the WikiText-2 dataset as an example. The results for the other models are similar to the ones shown here. ![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-14.jpg?height=718&width=1357&top_left_y=488&top_left_x=383)\n\nFigure 8: Normalized eigenvalues of the covariance matrix of the keys produced by Layer 1, Head 1 (top row), and Layer 28, Head 6 (bottom row) of Llama2-7B (left), Mistral-7B (middle), and Pythia-6.9B (right) on the WikiText-2 dataset.",
    "loki-16": "We observe that the explained variance significantly decreases after the initial principal dimensions.",
    "loki-17": "The dashed lines represent the rank at which $90 \\%$ of the variance is explained ( $\\operatorname{Rank}_{i, h} @ 90$ ). ## A. 2 Variation of Rank across Attention Heads\n\nIn this section, we discuss the variation of the rank at which $90 \\%$ of the variance is explained\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-14.jpg?height=45&width=1387&top_left_y=1601&top_left_x=369) 9 shows the heatmap of the $\\operatorname{Rank}_{l} @ 90$ for the pre-rotary (top) and post-rotary (bottom) keys across all layers and heads for Mistral-7B. We observe that the $R a n k_{l} @ 90$ is considerably lower for prerotary keys vs post-rotary keys. Focusing on the pre-rotary keys, we see that the initial layers have a lower rank compared to the later layers. In each layer, there are some heads heads with high-rank values even though the median rank is low. This might indicate that some head in that layer is more important and uses more complex information about the keys. Interestingly for post-rotary keys, we see a pattern where 4 out of the 8 heads in each layer have the same rank. This might have to do with how the rotary embeddings are applied to Mistral-7B as we do not see this pattern in Llama2-7B. Figure 10 shows the heatmap of the $R a n k_{l} @ 90$ for the pre-rotary (left) and post-rotary (right) keys across all layers and heads for Llama2-7B. We observe a similar trend as Mistral-7B where the initial layers have a lower rank compared to the later layers. However, we do not see the same pattern in the post-rotary keys as we saw in Mistral-7B. This might indicate that the rotary embeddings are applied differently in Llama2-7B compared to Mistral-7B. ![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-15.jpg?height=709&width=1321&top_left_y=392&top_left_x=402)\n\nFigure 9: Heatmap showing the rank at $90 \\%$ explained variance for the pre-rotary(top) and postrotary(bottom) key vectors across all layers and heads for Mistral-7B. We can see that in each layer, there are some heads with high-rank values. This would indicated when reducing the keys to a fixed lower dimensionality, some heads might perform better than others. ![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-15.jpg?height=571&width=1369&top_left_y=1601&top_left_x=375)\n\nFigure 10: Heatmap showing the rank at $90 \\%$ explained variance for the pre-rotary(top) and postrotary(bottom) key vectors across all layers and heads for Mistral-7B.",
    "loki-18": "We can see that in each layer, there are some heads with high-rank values. This would indicated when reducing the keys to a fixed lower dimensionality, some heads might perform better than others. ## B Comprehensive Evaluation Results\n\n## B. 1 Performance of Loki on Perplexity and Downstream Tasks\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-16.jpg?height=1240&width=1261&top_left_y=383&top_left_x=432)\n\nFigure 11: Performance of Loki on Perplexity (left) and Downstream Tasks (right) for different models using pre-rotary (top) and post-rotary (bottom) PCA transformation. For each model and each transform type, we run Loki with different values of $k$ and $d$. In this section, we present the detailed evaluation of our method on a wide range of models and tasks. Figure 11 shows the performance of Loki on perplexity and downstream tasks compared to the full attention baseline. We show the results for both pre-rotary (top) and post-rotary (bottom) PCA transformation. The models evaluated are Llama2-7B, Llama2-13B, Llama2-70B, Llama3-8B, Llama3-70B, TinyLlama-1.1B, Mistral-7B, and Mixtral-8x7B. We evaluate the models with different configurations of $k$ and $d$ for Loki. We can see that as $k_{f}$ and $d_{f}$ decrease, the performance of the model deteriorates. This is especially true when $k_{f}$ and $d_{f}$ are set to 0.125 . We notice that the impact of $k_{f}$ on performance is more significant than $d_{f}$. This is evident from the fact that $k_{f}=0.125, d_{f}=0.5$ performs significantly worse than $k_{f}=0.5, d_{f}=0.125$ for almost all the models. The two settings with $k_{f}=0.25, d_{f}=0.25$ and $k_{f}=0.125, d_{f}=0.5$ perform relatively well across all models. These settings provide a good trade-off between performance and model accuracy, with a theoretical speedup of 2.6 x for both settings. All settings with $k_{f}=0.5$ preserve model quality much better but do not provide a significant speedup empirically. Table 3 and Table 4 show the same results in tabular form. Table 3: Performance of different models compared to hugging face baseline with different configurations of $k$ and $d$ using pre-rotary PCA transformation. | Model | Method | k | d | PPL $\\downarrow$ | Hellaswag $\\uparrow$ | TQA\u4e2a $\\uparrow$ | Winogrande $\\uparrow$ | ARC $\\uparrow$ | GSM8K $\\uparrow$ | MMLU $\\uparrow$ | Avg $\\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Llama2-7B | Full Attention | - | - | 5.1101 | 75.99 | 38.96 | 69.06 | 46.33 | 13.87 | 41.84 | 47.67 |\n| Llama2-7B | Loki | 0.5 | 0.5 | 5.1195 | 75.96 | 38.85 | 69.22 | 46.16 | 13.19 | 41.34 | 47.45 |\n|  | Loki | 0.5 | 0.25 | 5.1223 | 75.84 | 39.05 | 68.82 | 45.82 | 12.36 | 40.95 | 47.14 |\n|  | Loki | 0.5 | 0.125 | 5.1250 | 75.09 | 38.51 | 69.53 | 44.28 | 10.77 | 39.07 | 46.21 |\n|  | Loki | 0.25 | 0.5 | 5.1881 | 75.73 | 38.04 | 67.25 | 44.20 | 11.30 | 39.74 | 46.04 |\n|  | Loki | 0.25 | 0.25 | 5.2185 | 73.43 | 38.35 | 63.61 | 41.21 | 7.96 | 36.43 | 43.50 |\n|  | Loki | 0.25 | 0.125 | 5.3044 | 53.23 | 40.08 | 59.35 | 36.09 | 2.81 | 30.99 | 37.09 |\n|  | Loki | 0.125 | 0.5 | 5.4980 | 70.42 | 39.40 | 52.49 | 35.92 | 7.13 | 33.22 | 39.76 |\n|  | Loki | 0.125 | 0.25 | 6.0729 | 56.04 | 42.76 | 49.57 | 31.91 | 2.27 | 27.15 | 34.95 |\n|  | Loki | 0.125 | 0.125 | 8.0514 | 31.06 | 44.46 | 49.01 | 25.34 | 0.38 | 23.64 | 28.98 |\n| Llama2-13B | Full Attention | - | - | 4.5680 | 79.38 | 36.90 | 72.22 | 49.15 | 22.97 | 52.06 | 52.11 |\n| Llama2-13B | Loki | 0.5 | 0.5 | 4.5701 | 79.34 | 37.06 | 73.09 | 48.81 | 23.20 | 52.19 | 52.28 |\n|  | Loki | 0.5 | 0.25 | 4.5708 | 79.27 | 37.14 | 72.14 | 49.40 | 22.44 | 52.03 | 52.07 |\n|  | Loki | 0.5 | 0.125 | 4.5737 | 78.45 | 37.39 | 70.09 | 47.95 | 19.86 | 50.98 | 50.79 |\n|  | Loki | 0.25 | 0.5 | 4.5979 | 79.19 | 37.35 | 71.90 | 47.87 | 22.14 | 52.02 | 51.74 |\n|  | Loki | 0.25 | 0.25 | 4.6110 | 77.39 | 36.89 | 68.90 | 46.16 | 19.86 | 48.80 | 49.67 |\n|  | Loki | 0.25 | 0.125 | 4.6829 | 71.17 | 37.21 | 58.17 | 36.26 | 7.88 | 41.30 | 42.00 |\n|  | Loki | 0.125 | 0.5 | 4.8153 | 77.38 | 38.45 | 56.27 | 41.64 | 14.94 | 48.63 | 46.22 |\n|  | Loki | 0.125 | 0.25 | 5.3912 | 61.85 | 36.79 | 52.09 | 32.08 | 2.96 | 36.40 | 37.03 |\n|  | Loki | 0.125 | 0.125 | 7.6573 | 38.67 | 43.00 | 50.20 | 24.32 | 0.68 | 23.63 | 30.08 |\n| Llama2-70B | Full Attention | - | - | 3.1205 | 83.82 | 44.81 | 77.90 | 57.34 | 53.15 | 65.41 | 63.74 |\n| Llama2-70B | Loki | 0.5 | 0.5 | 3.1319 | - | - | - | - | - | - | - |\n|  | Loki | 0.5 | 0.25 | 3.1293 | 83.65 | 39.78 | 76.95 | 56.91 | 41.93 | 63.32 | 60.42 |\n|  | Loki | 0.5 | 0.125 | 3.1316 | 82.38 | 39.33 | 72.85 | 54.61 | 37.45 | 60.85 | 57.91 |\n|  | Loki | 0.25 | 0.5 | 3.2986 | 80.54 | 42.46 | 75.85 | 57.08 | 22.21 | 57.14 | 55.88 |\n|  | Loki | 0.25 | 0.25 | 3.2830 | 76.05 | 44.88 | 63.54 | 50.26 | 15.92 | 51.79 | 50.41 |\n|  | Loki | 0.25 | 0.125 | 3.4571 | 52.25 | 44.73 | 50.36 | 25.09 | 2.35 | 29.37 | 34.02 |\n|  | Loki | 0.125 | 0.5 | 3.8327 | 68.06 | 39.43 | 58.80 | 46.93 | 10.31 | 44.82 | 44.72 |\n|  | Loki | 0.125 | 0.25 | 3.9259 | 46.59 | 45.88 | 46.96 | 28.67 | 2.35 | 28.90 | 33.22 |\n|  | Loki | 0.125 | 0.125 | 6.4963 | 30.07 | 49.19 | 51.30 | 22.78 | 1.14 | 24.75 | 29.87 |\n| Llama3-8B | Full Attention | - | - | 5.5696 | 79.17 | 43.89 | 72.93 | 53.24 | 50.11 | 62.19 | 60.26 |\n| Llama3-8B | Loki | 0.5 | 0.5 | 5.5703 | 78.84 | 44.21 | 73.64 | 54.01 | 48.90 | 61.47 | 60.18 |\n|  | Loki | 0.5 | 0.25 | 5.5746 | 77.44 | 43.68 | 68.27 | 49.15 | 47.16 | 60.58 | 57.71 |\n|  | Loki | 0.5 | 0.125 | 5.5876 | 74.83 | 44.23 | 65.43 | 43.94 | 40.41 | 56.97 | 54.30 |\n|  | Loki | 0.25 | 0.5 | 5.5944 | 76.54 | 44.32 | 60.93 | 43.43 | 44.66 | 58.33 | 54.70 |\n|  | Loki | 0.25 | 0.25 | 5.6648 | 69.42 | 41.50 | 50.36 | 34.64 | 33.06 | 44.50 | 45.58 |\n|  | Loki | 0.25 | 0.125 | 6.0558 | 56.11 | 42.14 | 50.36 | 27.13 | 9.17 | 30.46 | 35.90 |\n|  | Loki | 0.125 | 0.5 | 5.7356 | 66.13 | 44.00 | 50.04 | 28.33 | 31.77 | 40.61 | 43.48 |\n|  | Loki | 0.125 | 0.25 | 6.5780 | 45.14 | 41.00 | 49.33 | 23.89 | 3.18 | 26.05 | 31.43 |\n|  | Loki | 0.125 | 0.125 | 11.1097 | 32.70 | 44.31 | 47.04 | 23.29 | 0.68 | 23.80 | 28.64 |\n| Llama3-70B | Full Attention | - | - | 2.5653 | 84.89 | 45.57 | 80.43 | 64.33 | 80.67 | 75.03 | 71.82 |\n| Llama3-70B | Loki | 0.5 | 0.5 | 2.5656 | 85.17 | 45.66 | 79.95 | 63.99 | 79.91 | 74.90 | 71.60 |\n|  | Loki | 0.5 | 0.25 | 2.5665 | 84.22 | 45.78 | 75.06 | 59.81 | 78.77 | 73.68 | 69.55 |\n|  | Loki | 0.5 | 0.125 | 2.5712 | 82.21 | 45.53 | 69.61 | 54.78 | 74.98 | 70.28 | 66.23 |\n|  | Loki | 0.25 | 0.5 | 2.5727 | 84.09 | 45.64 | 71.35 | 57.51 | 79.76 | 73.12 | 68.58 |\n|  | Loki | 0.25 | 0.25 | 2.5942 | 79.06 | 45.09 | 59.27 | 43.26 | 72.78 | 62.47 | 60.32 |\n|  | Loki | 0.25 | 0.125 | 2.7577 | 67.59 | 45.46 | 50.67 | 31.48 | 45.56 | 42.21 | 47.16 |\n|  | Loki | 0.125 | 0.5 | 2.6285 | 78.96 | 46.48 | 51.14 | 40.70 | 74.53 | 62.19 | 59.00 |\n|  | Loki | 0.125 | 0.25 | 2.8796 | 63.93 | 41.69 | 46.33 | 27.65 | 50.19 | 36.08 | 44.31 |\n|  | Loki | 0.125 | 0.125 | 4.1495 | 39.07 | 41.09 | 49.88 | 23.38 | 3.03 | 25.73 | 30.36 |\n| TinyLlama-1.1B | Full Attention | - | - | 7.9671 | 60.45 | 37.88 | 60.22 | 32.85 | 1.90 | 24.86 | 36.36 |\n| TinyLlama-1.1B | Loki | 0.5 | 0.5 | 8.0040 | 60.39 | 38.19 | 59.98 | 32.08 | 1.90 | 24.62 | 36.19 |\n|  | Loki | 0.5 | 0.25 | 8.0342 | 59.96 | 38.80 | 59.27 | 32.85 | 2.20 | 24.33 | 36.23 |\n|  | Loki | 0.5 | 0.125 | 8.1057 | 57.93 | 39.10 | 57.14 | 31.91 | 1.52 | 24.98 | 35.43 |\n|  | Loki | 0.25 | 0.5 | 8.3475 | 58.06 | 40.05 | 58.17 | 31.06 | 1.52 | 24.83 | 35.62 |\n|  | Loki | 0.25 | 0.25 | 8.6352 | 52.69 | 42.96 | 52.01 | 29.18 | 1.29 | 24.76 | 33.82 |\n|  | Loki | 0.25 | 0.125 | 9.4947 | 44.43 | 44.21 | 50.75 | 23.89 | 1.44 | 24.34 | 31.51 |\n|  | Loki | 0.125 | 0.5 | 9.3280 | 51.29 | 42.27 | 53.91 | 27.82 | 0.83 | 24.17 | 33.38 |\n|  | Loki | 0.125 | 0.25 | 11.5887 | 37.32 | 47.04 | 47.51 | 25.00 | 1.52 | 23.49 | 30.31 |\n|  | Loki | 0.125 | 0.125 | 19.9290 | 30.13 | 48.50 | 51.30 | 24.66 | 1.06 | 24.11 | 29.96 |\n| Mistral-7B | Full Attention | - | - | 4.9140 | 81.07 | 42.62 | 73.95 | 53.92 | 38.59 | 59.65 | 58.30 |\n| Mistral-7B | Loki | 0.5 | 0.5 | 4.9147 | 80.84 | 42.99 | 74.27 | 53.58 | 38.06 | 59.83 | 58.26 |\n|  | Loki | 0.5 | 0.25 | 4.9152 | 80.55 | 43.11 | 72.69 | 53.41 | 36.69 | 59.14 | 57.60 |\n|  | Loki | 0.5 | 0.125 | 4.9193 | 79.38 | 42.29 | 70.40 | 51.28 | 33.59 | 57.29 | 55.71 |\n|  | Loki | 0.25 | 0.5 | 4.9185 | 79.00 | 43.41 | 70.17 | 49.23 | 36.16 | 58.25 | 56.04 |\n|  | Loki | 0.25 | 0.25 | 4.9233 | 77.65 | 42.18 | 62.98 | 46.59 | 32.68 | 53.70 | 52.63 |\n|  | Loki | 0.25 | 0.125 | 4.9986 | 66.95 | 39.58 | 52.64 | 36.35 | 14.86 | 38.20 | 41.43 |\n|  | Loki | 0.125 | 0.5 | 4.9311 | 72.66 | 43.89 | 52.25 | 35.58 | 33.36 | 50.01 | 47.96 |\n|  | Loki | 0.125 | 0.25 | 4.9636 | 65.93 | 41.12 | 51.78 | 29.18 | 18.42 | 38.14 | 40.76 |\n|  | Loki | 0.125 | 0.125 | 5.7404 | 36.32 | 43.14 | 52.17 | 23.98 | 0.53 | 24.60 | 30.12 |\n| Mixtral-8x7B | Full Attention | - | - | 3.5967 | 84.01 | 48.53 | 76.32 | 59.73 | 58.38 | 67.90 | 65.81 |\n| Mixtral-8x7B | Loki | 0.5 | 0.5 | 3.5979 | 83.86 | 46.86 | 75.53 | 60.15 | 57.32 | 67.83 | 65.26 |\n|  | Loki | 0.5 | 0.25 | 3.6047 | 83.70 | 46.70 | 76.24 | 59.73 | 57.01 | 67.21 | 65.10 |\n|  | Loki | 0.5 | 0.125 | 3.6201 | 82.91 | 42.27 | 73.48 | 57.42 | 43.44 | 65.71 | 60.87 |\n|  | Loki | 0.25 | 0.5 | 3.6076 | 82.58 | 48.16 | 71.43 | 58.28 | 56.18 | 66.72 | 63.89 |\n|  | Loki | 0.25 | 0.25 | 3.6584 | 81.32 | 43.49 | 62.83 | 51.79 | 42.76 | 60.82 | 57.17 |\n|  | Loki | 0.25 | 0.125 | 3.9252 | 73.16 | 39.49 | 56.04 | 44.80 | 4.85 | 45.55 | 43.98 |\n|  | Loki | 0.125 | 0.5 | 3.6417 | 76.93 | 48.21 | 50.91 | 41.72 | 50.87 | 58.30 | 54.49 |\n|  | Loki | 0.125 | 0.25 | 3.8467 | 70.07 | 37.88 | 49.17 | 32.68 | 11.52 | 39.23 | 40.09 |\n|  | Loki | 0.125 | 0.125 | 6.9799 | 42.34 | 43.80 | 54.38 | 24.66 | 0.45 | 24.99 | 31.77 |\n\nTable 4: Performance of different models compared to hugging face baseline with different configurations of $k$ and $d$ using post-rotary PCA transformation. | Model | Method | k | d | PPL $\\downarrow$ | Hellaswag $\\uparrow$ | TQA\u4e2a $\\uparrow$ | Winogrande $\\uparrow$ | ARC $\\uparrow$ | GSM8K $\\uparrow$ | MMLU $\\uparrow$ | Avg $\\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Llama2-7B | Full Attention | - | - | 5.1101 | 75.99 | 38.96 | 69.06 | 46.33 | 13.87 | 41.84 | 47.67 |\n| Llama2-7B | Loki | 0.5 | 0.5 | 5.1195 | 75.91 | 38.87 | 68.59 | 46.50 | 14.10 | 41.49 | 47.58 |\n|  | Loki | 0.5 | 0.25 | 5.1206 | 75.84 | 39.05 | 68.82 | 45.82 | 12.36 | 40.95 | 47.14 |\n|  | Loki | 0.5 | 0.125 | 5.1241 | 75.48 | 38.77 | 67.64 | 43.94 | 12.59 | 38.85 | 46.21 |\n|  | Loki | 0.25 | 0.5 | 5.1838 | 75.19 | 38.16 | 62.12 | 41.21 | 10.69 | 40.42 | 44.63 |\n|  | Loki | 0.25 | 0.25 | 5.2017 | 72.59 | 39.16 | 56.59 | 37.37 | 10.24 | 37.74 | 42.28 |\n|  | Loki | 0.25 | 0.125 | 5.4428 | 68.49 | 38.83 | 56.51 | 32.17 | 10.92 | 32.68 | 39.93 |\n|  | Loki | 0.125 | 0.5 | 5.3601 | 70.42 | 39.40 | 52.49 | 35.92 | 7.13 | 33.22 | 39.76 |\n|  | Loki | 0.125 | 0.25 | 5.5606 | 59.98 | 41.72 | 48.86 | 26.96 | 6.22 | 28.38 | 35.35 |\n|  | Loki | 0.125 | 0.125 | 7.4062 | 40.14 | 43.84 | 49.64 | 25.43 | 5.99 | 24.13 | 31.53 |\n| Llama2-13B | Full Attention | - | - | 4.5680 | 79.38 | 36.90 | 72.22 | 49.15 | 22.97 | 52.06 | 52.78 |\n| Llama2-13B | Loki | 0.5 | 0.5 | 4.5731 | 79.34 | 37.06 | 73.09 | 48.81 | 23.20 | 52.19 | 52.28 |\n|  | Loki | 0.5 | 0.25 | 4.5737 | 79.05 | 37.46 | 72.69 | 48.29 | 23.58 | 51.94 | 52.17 |\n|  | Loki | 0.5 | 0.125 | 4.5745 | 78.45 | 37.39 | 70.09 | 47.95 | 19.86 | 50.98 | 50.79 |\n|  | Loki | 0.25 | 0.5 | 4.5937 | 79.19 | 37.35 | 71.90 | 47.87 | 22.14 | 52.02 | 51.74 |\n|  | Loki | 0.25 | 0.25 | 4.6102 | 77.39 | 36.89 | 68.90 | 46.16 | 19.86 | 48.80 | 49.67 |\n|  | Loki | 0.25 | 0.125 | 4.8082 | 61.52 | 38.10 | 52.41 | 26.54 | 20.85 | 44.69 | 40.68 |\n|  | Loki | 0.125 | 0.5 | 4.7029 | 77.38 | 38.45 | 56.27 | 41.64 | 14.94 | 48.63 | 46.22 |\n|  | Loki | 0.125 | 0.25 | 4.9668 | 72.71 | 40.09 | 51.14 | 33.28 | 9.10 | 39.20 | 40.92 |\n|  | Loki | 0.125 | 0.125 | 6.1436 | 34.81 | 46.07 | 52.09 | 24.15 | 8.79 | 26.50 | 32.07 |\n| Llama2-70B | Full Attention | - | - | 3.1205 | 83.82 | 44.81 | 77.90 | 57.34 | 53.15 | 65.41 | 63.74 |\n| Llama2-70B | Loki | 0.5 | 0.5 | 3.1411 | 83.89 | 41.32 | 78.06 | 57.68 | 50.42 | 64.75 | 62.69 |\n|  | Loki | 0.5 | 0.25 | 3.1453 | 83.69 | 43.42 | 76.80 | 56.31 | 52.99 | 64.73 | 62.99 |\n|  | Loki | 0.5 | 0.125 | 3.1457 | 83.41 | 43.51 | 75.45 | 55.89 | 52.54 | 64.12 | 62.49 |\n|  | Loki | 0.25 | 0.5 | 3.4619 | 82.36 | 41.91 | 76.87 | 56.48 | 42.61 | 60.11 | 60.06 |\n|  | Loki | 0.25 | 0.25 | 3.5701 | 81.42 | 45.26 | 71.11 | 49.74 | 44.28 | 59.56 | 58.56 |\n|  | Loki | 0.25 | 0.125 | 3.5459 | 80.59 | 45.57 | 65.59 | 49.15 | 46.93 | 58.00 | 57.64 |\n|  | Loki | 0.125 | 0.5 | 4.1427 | 71.90 | 44.59 | 58.09 | 41.98 | 34.34 | 50.30 | 50.20 |\n|  | Loki | 0.125 | 0.25 | 4.7796 | 67.03 | 46.58 | 51.85 | 32.17 | 37.30 | 42.21 | 46.19 |\n|  | Loki | 0.125 | 0.125 | 4.6898 | 64.84 | 44.89 | 50.51 | 29.95 | 38.74 | 39.08 | 44.67 |\n| Llama3-8B | Full Attention | - | - | 5.5696 | 79.17 | 43.89 | 72.93 | 53.24 | 50.11 | 62.19 | 60.26 |\n| Llama3-8B | Loki | 0.5 | 0.5 | 5.5699 | 76.03 | 43.83 | 67.32 | 44.71 | 49.36 | 59.38 | 56.77 |\n|  | Loki | 0.5 | 0.25 | 5.9343 | 72.55 | 42.67 | 61.64 | 39.93 | 41.09 | 57.88 | 52.63 |\n|  | Loki | 0.5 | 0.125 | 5.7429 | 71.38 | 43.16 | 58.64 | 40.61 | 39.42 | 57.14 | 51.72 |\n|  | Loki | 0.25 | 0.5 | 5.6783 | 68.02 | 42.07 | 48.78 | 31.31 | 43.59 | 48.06 | 46.97 |\n|  | Loki | 0.25 | 0.25 | 11.4459 | 57.39 | 42.13 | 48.70 | 27.90 | 28.28 | 38.69 | 40.52 |\n|  | Loki | 0.25 | 0.125 | 13.2883 | 48.99 | 42.10 | 48.07 | 22.87 | 12.81 | 30.90 | 34.29 |\n|  | Loki | 0.125 | 0.5 | 6.8023 | 49.68 | 41.14 | 49.25 | 25.51 | 31.39 | 30.86 | 37.97 |\n|  | Loki | 0.125 | 0.25 | 16.3507 | 36.39 | 43.62 | 50.04 | 25.09 | 16.60 | 26.21 | 32.99 |\n|  | Loki | 0.125 | 0.125 | 22.6596 | 31.60 | 46.38 | 49.25 | 23.12 | 1.14 | 23.61 | 29.18 |\n| Llama3-70B | Full Attention | - | - | 2.5653 | 84.89 | 45.57 | 80.43 | 64.33 | 80.67 | 75.03 | 71.82 |\n| Llama3-70B | Loki | 0.5 | 0.5 | 2.5660 | 83.60 | 45.83 | 72.61 | 56.14 | 79.15 | 73.43 | 68.46 |\n|  | Loki | 0.5 | 0.25 | 2.5697 | 79.92 | 46.22 | 62.90 | 48.46 | 78.39 | 71.11 | 64.50 |\n|  | Loki | 0.5 | 0.125 | 2.7810 | 76.66 | 46.77 | 59.27 | 42.66 | 56.94 | 68.48 | 58.46 |\n|  | Loki | 0.25 | 0.5 | 2.5742 | 74.91 | 47.67 | 51.54 | 38.05 | 77.71 | 64.14 | 59.00 |\n|  | Loki | 0.25 | 0.25 | 2.8593 | 61.40 | 47.86 | 48.38 | 27.73 | 67.32 | 41.18 | 48.98 |\n|  | Loki | 0.25 | 0.125 | 5.6725 | 41.90 | 47.18 | 47.59 | 23.29 | 5.31 | 26.98 | 32.04 |\n|  | Loki | 0.125 | 0.5 | 2.6231 | 56.24 | 43.91 | 50.51 | 24.66 | 72.48 | 38.52 | 47.72 |\n|  | Loki | 0.125 | 0.25 | 4.2512 | 31.91 | 47.39 | 50.43 | 24.57 | 19.71 | 24.72 | 33.12 |\n|  | Loki | 0.125 | 0.125 | 57.6788 | 27.01 | 49.28 | 50.67 | 24.06 | 0.68 | 24.76 | 29.41 |\n| TinyLlama-1.1B | Full Attention | - | - | 7.9671 | 60.45 | 37.88 | 60.22 | 32.85 | 1.90 | 24.86 | 36.36 |\n| TinyLlama-1.1B | Loki | 0.5 | 0.5 | 7.9979 | 60.17 | 38.14 | 58.33 | 31.57 | 1.90 | 25.12 | 35.87 |\n|  | Loki | 0.5 | 0.25 | 8.0135 | 58.78 | 39.95 | 54.38 | 30.55 | 1.29 | 24.58 | 34.92 |\n|  | Loki | 0.5 | 0.125 | 8.0414 | 57.77 | 38.20 | 54.93 | 30.89 | 1.44 | 24.39 | 34.60 |\n|  | Loki | 0.25 | 0.5 | 8.3190 | 57.35 | 37.87 | 53.83 | 29.69 | 1.67 | 25.13 | 34.26 |\n|  | Loki | 0.25 | 0.25 | 8.5687 | 52.40 | 40.86 | 49.33 | 26.96 | 2.20 | 23.34 | 32.51 |\n|  | Loki | 0.25 | 0.125 | 8.8956 | 51.19 | 42.07 | 52.96 | 28.92 | 0.91 | 25.10 | 33.52 |\n|  | Loki | 0.125 | 0.5 | 8.9679 | 51.32 | 38.24 | 50.20 | 24.23 | 1.29 | 24.92 | 31.70 |\n|  | Loki | 0.125 | 0.25 | 10.2592 | 42.85 | 39.06 | 51.85 | 25.60 | 1.52 | 24.06 | 30.82 |\n|  | Loki | 0.125 | 0.125 | 11.3508 | 39.27 | 41.55 | 50.67 | 22.78 | 0.45 | 24.50 | 29.87 |\n| Mistral-7B | Full Attention | - | - | 4.9140 | 81.07 | 42.62 | 73.95 | 53.92 | 38.59 | 59.65 | 58.30 |\n| Mistral-7B | Loki | 0.5 | 0.5 | 4.9149 | 79.89 | 42.15 | 70.56 | 49.83 | 37.45 | 58.00 | 56.31 |\n|  | Loki | 0.5 | 0.25 | 4.9221 | 78.99 | 40.84 | 63.06 | 45.48 | 33.43 | 55.15 | 52.82 |\n|  | Loki | 0.5 | 0.125 | 4.9317 | 73.88 | 40.58 | 57.06 | 33.87 | 22.06 | 45.95 | 45.57 |\n|  | Loki | 0.25 | 0.5 | 5.2052 | 71.86 | 40.74 | 56.04 | 38.91 | 24.18 | 45.56 | 46.22 |\n|  | Loki | 0.25 | 0.25 | 6.5445 | 62.62 | 38.93 | 48.62 | 25.17 | 1.82 | 30.80 | 34.66 |\n|  | Loki | 0.25 | 0.125 | 7.7609 | 35.51 | 43.67 | 53.20 | 23.63 | 1.06 | 23.86 | 30.16 |\n|  | Loki | 0.125 | 0.5 | 9.5167 | 51.73 | 45.44 | 51.62 | 25.77 | 3.03 | 27.99 | 34.26 |\n|  | Loki | 0.125 | 0.25 | 13.5597 | 34.85 | 46.38 | 50.20 | 22.53 | 0.45 | 23.60 | 29.67 |\n|  | Loki | 0.125 | 0.125 | 20.5289 | 28.52 | 51.98 | 50.91 | 26.96 | 0.45 | 23.64 | 30.41 |\n| Mixtral-8x7B | Full Attention | - | - | 3.5967 | 84.01 | 48.53 | 76.32 | 59.73 | 58.38 | 67.90 | 65.81 |\n| Mixtral-8x7B | Loki | 0.5 | 0.5 | 3.5970 | 83.24 | 47.32 | 74.27 | 58.53 | 56.48 | 67.23 | 64.51 |\n|  | Loki | 0.5 | 0.25 | 3.6196 | 81.71 | 43.51 | 69.61 | 53.67 | 55.57 | 63.92 | 61.33 |\n|  | Loki | 0.5 | 0.125 | 3.6635 | 76.18 | 41.63 | 61.72 | 47.78 | 49.28 | 58.94 | 55.92 |\n|  | Loki | 0.25 | 0.5 | 3.6004 | 79.99 | 46.47 | 61.64 | 49.15 | 57.85 | 63.04 | 59.69 |\n|  | Loki | 0.25 | 0.25 | 3.7906 | 71.58 | 37.77 | 53.28 | 37.54 | 37.38 | 46.66 | 47.37 |\n|  | Loki | 0.25 | 0.125 | 4.2566 | 59.23 | 36.58 | 50.75 | 28.67 | 15.39 | 32.28 | 37.15 |\n|  | Loki | 0.125 | 0.5 | 3.6358 | 72.29 | 45.28 | 50.67 | 33.70 | 55.50 | 47.15 | 50.76 |\n|  | Loki | 0.125 | 0.25 | 4.5500 | 52.16 | 37.86 | 46.57 | 23.98 | 17.13 | 27.02 | 34.12 |\n|  | Loki | 0.125 | 0.125 | 5.5250 | 46.93 | 40.33 | 49.72 | 23.55 | 0.91 | 24.78 | 31.04 |\n\n## C Comparison of our top-k kernels with SparQ\n\nAs mentioned in Section 4.3 , we create optimized kernels in Triton to efficiently compute the three matrix multiplications in Loki (lines 5, 8, and 9 of Algorithm 1) without creating temporary dense copies of subsets of the KV-cache. Initially, we planned to use the implementations developed by the authors of SparQ [25]. However, we discovered two major issues with their kernels. Let's say you are multiplying two matrices of sizes $m \\times k$ and $k \\times n$, then SparQ kernels parallelize compute along only the m dimension. However, it is well known that one can parallelize matrix multiplications along the n dimension as well and gain more performance. Thus, we add this extra dimension of parallelism to their triton kernel. Second, their kernels cannot handle non-powers of 2 number of tokens in the KV-cache, a setting which is commonly encountered in inference since we generated keys and values one at a time. Therefore, we extend their kernels to handle non-powers of two number of tokens in the KV-cache successfully. In Figure 12, we compare the performance of our kernel with sparq and vanilla PyTorch based attention for an attention layer in Llama2-7B for various sizes of the KV-cache ranging from 512 to 4096 . We do this for the matmul operation of query and keys with top-k as 0.25 . ![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-19.jpg?height=977&width=1269&top_left_y=829&top_left_x=425)\n\nFigure 12: Comparing the performance of our proposed kernel for computing $Q . K^{T}$, with SparQ's [25] top-k kernel for various batch sizes and number of keys in the KV-cache. We see very high speedups over SparQ for small batch sizes. For instance, for a batch size of 1 with 4096 keys (bottom right), our kernel is faster than SparQ by nearly $2.8 \\times$ ! Infact, the SparQ kernel barely obtains any speedup over vanilla PyTorch even though it is only using $25 \\%$ of the keys (1024 out of 4096). This is because SparQ only parallelizes compute across the m-dimension as we discussed before. In attention the m-dimension is proportional to the batch size, and thus having a small batch size limits the amount of parallelism SparQ kernels can exploit. Whereas, given our 2D parallel approach to matrix multiplication, we can exploit parallelism along the sequence dimension and thus still obtain significant speedups over vanilla PyTorch.",
    "loki-19": "Note that smaller batch sizes are very common in inference and hence it is extremely important to design a kernel that can function efficiently even at smaller batch sizes. Our speedups over SparQ decrease with larger batch sizes, but overall we are always faster than both SparQ and vanilla PyTorch. ## D Generation Examples\n\nIn this section, we look at a generation example from the Llama2-7B (Chat) model given a custom prompt for different settings of $k$ and $d$.",
    "loki-20": "![](https://cdn.mathpix.com/cropped/2024_09_12_2d32a22bc62287f76fc1g-20.jpg?height=126&width=1302&top_left_y=441&top_left_x=409)\n\nIn a world where technology has advanced beyond imagination, society grapples with the consequences of its own creations. The integration of artificial intelligence into everyday life has revolutionized how we live, work, and interact. However, with great power comes great responsibility, and ethical dilemmas abound. Governments struggle to regulate the use of AI, while corporations push the boundaries of what is possible in the pursuit of profit. Meanwhile, individuals navigate a landscape where privacy is increasingly scarce, and the line between human and machine blurs. Against this backdrop, a new generation of thinkers emerges, questioning the very nature of consciousness and what it means to be human. As tensions rise and alliances shift, the fate of humanity hangs in the balance, waiting to be written by those bold enough to seize the pen of destiny. ## Llama2-7B Chat Full\n\nWill we create a future where AI and humanity coexist in harmony, or will the pursuit of power and profit lead to our downfall? The choice is ours, and the clock is ticking. Llama2-7B Chat $-k=50 \\%, d=25 \\%$\nWill we chart a path towards utopia or succumb to the dangers of unchecked progress? Can we find a balance between technological advancement and preserving the essence of our humanity? The future is ours to create, and the choice is ours to make. The clock ticks on, waiting for us to take the next step forward into an uncertain tomorrow, where the only constant is the endless possibilities of an age of AI\n\nLlama2-7B Chat - $k=25 \\%, d=25 \\%$\nWill you answer the call to shape the future of AI?",
    "loki-21": "Will you play a part in shaping the course of human history?",
    "loki-22": "The possibilities are endless, and the outcome uncertain. Are you ready to embark on a journey that could change the world forever? ## E PCAAttn: Using reduced-dimensional attention scores directly in transformer models\n\n```\nAlgorithm 2 PCAAttn\nRequire: At the \\(i^{t h}\\) step - Input: \\(\\mathbf{x}_{i} \\in \\mathbb{R}^{1 \\times D}\\), KV-Cache: \\(\\mathbf{K}_{i-1, d}^{\\prime}, \\mathbf{V}_{i-1} \\in \\mathbb{R}^{(S-1) \\times d}\\), Projection\n    Matrix: \\(\\mathbf{P}_{d} \\in \\mathbb{R}^{D \\times d}\\) (first \\(d\\) principal components)\n    function PCA-AtTEntion \\(\\left(\\mathbf{x}_{i}, \\mathbf{K}_{i-1, d}^{\\prime}, \\mathbf{V}_{i-1}, \\mathbf{P}_{d}\\right)\\)\n        \\(\\mathbf{q}_{i}, \\mathbf{k}_{i}, \\mathbf{v}_{i} \\leftarrow\\) computeQKV \\(\\left(\\mathbf{x}_{i}\\right)\\)\n        \\(\\mathbf{q}_{i, d}^{\\prime} \\leftarrow \\mathbf{q}_{i} \\mathbf{P}_{d}, \\mathbf{k}_{i, d}^{\\prime} \\leftarrow \\mathbf{k}_{i} \\mathbf{P}_{d}\\)\n        \\(\\mathbf{K}_{i, d}^{\\prime} \\leftarrow \\operatorname{concat}\\left(\\mathbf{K}_{i-1, d}^{\\prime}, \\mathbf{k}_{i}^{\\prime}\\right)\\)\n        \\(\\mathbf{V}_{i} \\leftarrow \\operatorname{concat}\\left(\\mathbf{V}_{i-1}, \\mathbf{v}_{i}\\right)\\)\n        \\(A_{\\text {exact }}=\\operatorname{softmax}\\left(\\frac{\\mathbf{q}_{i, d}^{\\prime}\\left(\\mathbf{K}_{i, d}^{\\prime}\\right)^{T}}{\\sqrt{D}}\\right)\\)\n    end function\n```\n\nOne other approach we tried is to directly use the formulation in 4.1 to compute the final attention scores. More specifically, we compute the PCA transformed query and key vectors, projected onto the first $d$ principal components, and then compute the attention scores. We only store the reduced dimension key vectors in the KV cache. We call this method PCAAttn (Algorithm2). Compute and Memory Analysis: When computing attention between a single query $\\mathbf{q}_{i} \\in \\mathbb{R}^{1 \\times D}$ and the key vectors $\\mathbf{K}_{i} \\in \\mathbb{R}^{S \\times D}$, the matrix multiplication $\\mathbf{q}_{i} \\mathbf{K}_{i}^{T}$ has a complexity of $\\mathcal{O}(D S)$. Using PCAAttn, the key and query vectors are reduced to $d$ dimensions and the complexity of the matrix multiplication is reduced to $\\mathcal{O}(d S)$. Thus, we can get a speedup of $D / d$ in the attention dot product computation. The PCA transformation of the query and key vector generated at each step has a complexity of $\\mathcal{O}\\left(D^{2}\\right)$, which is small when $S>>D$. The KV-Cache memory requirement is reduced by a factor of $0.5 * D / d$ because we only reduce the key vectors to $d$ dimensions and not the values. Additionally, the PCA adds a significantly small memory overhead of $\\mathcal{O}(D d)$. Table 5 shows the explanation of key-budget and dimensionality for PCAAttn, along with the speedup and memory savings. Table 5: Explanation of key-budget and dimensionality (dim.) for PCAAttn, along with the speedup and memory savings. | Method | Budget | Dim. | Description | Speedup | Memory <br> Savings |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| PCAAttn | Full | $d$ | $d \\%$ of full dimensionality used to store keys and <br> compute attention output | $\\frac{100}{d}$ | $\\frac{100}{2 d}$ |\n\n## Experimental Results:\n\nTable 6: Performance of PCAAttn with various cache configurations. Compare with Table ?? for baseline numbers\n\n| Model | Method | $k$ | d | Perplexity $\\downarrow$ | Hellaswag $\\uparrow$ | Winogrande $\\uparrow$ | MathQA $\\uparrow$ | OpenbookQA $\\uparrow$ | RTE $\\uparrow$ | COPA $\\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Llama2-7B | Full Attention | - | - | 5.1102 | 57.2 | 69.1 | 28.4 | 31.4 | 62.8 | 87.0 |\n| Llama2-7B | Exact TopK | $50 \\%$ | - | 5.1191 | 57.2 | 68.9 | 28.3 | 31.2 | 63.9 | 86.0 |\n|  | H2O | $50 \\%$ | - | 5.1456 | 55.5 | 61.8 | 24.4 | 27.4 | 62.8 | 77.0 |\n|  | PCAAttn | - | $50 \\%$ | 38.3997 | 33.3 | 53.2 | 21.7 | 14.2 | 50.5 | 73 |\n| Llama2-7B | Exact TopK | $25 \\%$ | - | 5.1799 | 56.9 | 68.6 | 29.4 | 29 | 66.4 | 76.0 |\n|  | H2O | $25 \\%$ | - | 5.2809 | 50.1 | 51.6 | 21.1 | 17.8 | 55.2 | 55.0 |\n|  | PCAAttn | - | $25 \\%$ | 243.2631 | 26.9 | 48.5 | 20.5 | 11.4 | 49.1 | 65.0 |\n| Mistral-7B | Full Attention | - | - | 4.9140 | 61.2 | 73.9 | 35.7 | 32.2 | 66.8 | 91.0 |\n| Mistral-7B | Exact TopK | $50 \\%$ | - | 4.9143 | 61.1 | 73.8 | 35.6 | 32.6 | 65.3 | 92.0 |\n|  | H2O | $50 \\%$ | - | 4.9560 | 59.4 | 58.6 | 26.4 | 23.0 | 62.4 | 71.0 |\n|  | PCAAttn | - | 50\\% | 396.8967 | 31.4 | 50.4 | 22.5 | 15.6 | 53.4 | 72.0 |\n| Mistral-7B | Exact TopK | $25 \\%$ | - | 4.9170 | 60.4 | 73.0 | 35.4 | 30.0 | 65.3 | 85.0 |\n|  | H2O |  | - | 5.0805 | 52.7 | 49.7 | 21.9 | 17.4 | 52.0 | 56.0 |\n|  | PCAAttn | - | $25 \\%$ | 933.6016 | 27.2 | 52.2 | 21.6 | 13.6 | 53.0 | 63.0 |\n\nTable 6 shows the performance of PCAAttn on Llama2-7B and Mistral-7B models. We can see that our PCAAttn method performs poorly compared to all the baselines and the H2O method for all cache configurations. We believe that this happens because the application of rotary embeddings increases the dimensionality of the key vectors and using reduced dimensionality to store the keys results in loss of information. To further investigate this, let us look at Figure 9 which shows the rank at $90 \\%$ explained variance for the key vectors across all layers and heads. Even though, the average rank per layer is around $50 \\%$ of the full dimensionality, the rank for some layers and especially some heads within each layer is much higher. Due to the poor performance of PCAAttn, we do not include it in the final results and decide to focus on Loki instead in the main paper. ## F Estimate of Compute Resources Required to Replicate our Experiments\n\nAs mentioned in Section 5, we conduct all of our experiments on Perlmutter, a multi-GPU cluster with 4 A100 GPUs per node. Since we do not do any training/fine-tuning, our experiments can be done on a very small number of GPUs. For instance, all of our runs involving models with 7B and 13B parameters were done on a single A100 GPU. For models larger than this (like LLama270B, Llama3-70B), we had to resort to running on four A100 GPUs (or a single node) with tensor parallelism using the AxoNN parallel deep learning framework. All results for 7B and 13B sized models can be compiled within 3 hours. For larger models like the 70B LLaMA-2 and 3 as well as Mixtral models, the total times for computing all results are in the ballpark of 10 hours. Our compute benchmarking runs of Llama-13B are very short and can be completed within 5 minutes.",
    "loki-23": ""
}