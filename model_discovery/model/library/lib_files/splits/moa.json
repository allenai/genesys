{
    "moa-0": "# MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression \n\nTianyu Fu ${ }^{1,2, *}$ Haofeng Huang ${ }^{1,2, *}$, Xuefei Ning ${ }^{1, *}$, Genghan Zhang ${ }^{3}$, Boju Chen ${ }^{1}$,<br>Tianqi Wu ${ }^{1,2}$, Hongyi Wang ${ }^{1,2}$, Zixiao Huang ${ }^{1,2}$, Shiyao $\\mathbf{L i}^{1,2}$,<br>Shengen Yan ${ }^{1,2}$, Guohao Dai ${ }^{2,4}$, Huazhong Yang ${ }^{1}$, Yu Wang ${ }^{1}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Infinigence-AI ${ }^{3}$ Stanford University ${ }^{4}$ Shanghai Jiao Tong University\n\n\n#### Abstract\n\nSparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts.",
    "moa-1": "Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence length. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9 \\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1 \\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9 \\%-36 \\%$ to within $5 \\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4 \\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance. ## 1 Introduction\n\nLarge Language Models (LLMs) exhibit remarkable versatility across numerous applications [6, 57, 63]. Central to LLM is the attention mechanism [62], which computes interactions among tokens within a certain span, thereby enabling context understanding. Scaling input length is crucial for enhancing LLM capabilities [7, 60], including fact retrieval, summarization, few-shot learning, question answering and so on [4, 70]. However, the ever-growing attention computation and KeyValue Cache (KV-Cache) pose significant efficiency challenges [54, 69, 26, 33]. Previous work proposes sparse attention methods to address the efficiency challenges of long contexts in generative LLMs. These methods typically employ a uniform, fixed-span sliding window mask across all heads and input lengths, limiting attention to local contexts only [69, 26]. This approach allows the LLM to take long inputs with a fixed attention span, keeping bounded attention computation and KV caching overhead. Following previous works [7, 60], we quantify the effective context length\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-02.jpg?height=402&width=1397&top_left_y=208&top_left_x=359)\n\nFigure 1: Retrieval accuracy of the Vicuna-7B model using different attention methods across varying input lengths and retrieval positions on the LongEval dataset. This \"Needle In a Haystack\" experiment [19] takes massive key-value pairs as inputs and tests the accuracy to retrieve values based on given keys from diverse positions. (a) Original model with a full attention span; (b) StreamingLLM with half the attention span, showing reduced effectiveness beyond the span; (c) MoA with half the attention span, maintaining effectiveness beyond the span. as the maximum input length where content retrieval accuracy exceeds a $90 \\%$ threshold. In principle, fixed-span local attention can gradually aggregate global information through multiple model layers, yielding a longer effective context length than each attention span [18]. Nonetheless, we reveal that uniform masks, like StreamingLLM [69], hardly extend effective context length beyond the span, as shown in Figure 6. Figure 1(b) further illustrates such limitation: with a $50 \\%$ attention span mask, StreamingLLM fails to accurately retrieve content from the earlier half of the input and performs even worse at longer input lengths. Figure 2 reveals one possible explanation for the problem: while some attention heads focus on local contexts, others encompass the broad span of the entire input sequence. Consequently, the uniform approach fails to achieve a long effective context length as it limits the attention span of the global-context heads, while excessively allocates compute and memory budget for local-context heads. Additionally, as the input length increases, some attention heads need a faster increase in attention span than others to avoid serious performance degradation, as shown in Table 1 Unfortunately, the uniform approaches do not include heterogeneous rules to scale the attention spans differently for various heads. Besides, existing model compression methods [44, 40, 68, 36, 31, 37] use general language modeling corpora to decide the compression plan, which cannot accurately profile the influence of compression on long-context tasks. In this work, we propose Mixture of Attention (MoA), a training-free sparse attention method. As illustrated in Figure 3 . MoA constructs the search space of heterogeneous elastic rules of attention spans. For automatic LLM compression, MoA first utilizes gradient-based profiling to inspect the influences of each attention position on the prediction loss. Based on the profiling results, MoA tailors heterogeneous sparse attention configurations for each model layer and attention head. During profiling, MoA employs a calibration dataset with long-range dependencies and uses the original dense model's response instead of the human-written response as the reference to calculate the loss.",
    "moa-2": "This ensures an accurate profiling of the attention influences to facilitate better compression results. Our contributions are summarized as follows. - Heterogeneous Elastic Rules. We propose heterogeneous elastic rules for masks of each attention head. We formulate MoA compression search space to include a diverse range of elastic rules that tailor the local attention span relative to the input length for each attention head. The heterogeneous elastic rules improve the fact retrieval accuracy of MoA from 25\\% to $98 \\%$ compared with masks with uniform span and scaling function for each head. - Calibration Dataset Construction We emphasize the importance of data engineering in LLM compression. Our findings demonstrate that using datasets with long-range dependencies and referencing the original LLM's responses are crucial for accurately profiling the influences of compression. - Automatic Optimization. We propose an automatic pipeline to find the optimal compression plan encompassing heterogeneous elastic rules for various attention heads. This pipeline can efficiently find the optimal plan within several hours, for example, two hours for compressing Vicuna-13B. Experiments show that MoA achieves $5.5 \\times$ to $6.7 \\times$ throughput improvements on 7 B and 13 B dense LLMs at a $50 \\%$ density (the average of KV-Cache length / input length), with only $1 \\%$ average relative degradation in retrieval accuracy. Additionally, MoA achieves over $90 \\%$ retrieval accuracy with just $25 \\%$ average density, far surpassing sparse attention baselines that need a density of $75 \\%$ to $100 \\%$ for similar performance. On long-context understanding benchmarks, MoA performs comparably to dense models, with a maximum relative performance drop of less than $5 \\%$, which is about one-sixth of that observed with the uniform sparse attention baseline. Our code is available at https://github.com/thu-nics/MoA\n\n## 2 Preliminary and Related work\n\n### 2.1 Attention mechanism\n\nThe Multi-Head Self Attention (MHA) mechanism [62] is crucial to the functionality of LLMs.",
    "moa-3": "It starts with an input sequence transformed into query $(\\mathrm{Q})$, key $(\\mathrm{K})$, and value $(\\mathrm{V})$ matrices through linear projections. These matrices, combined with the cached K and V (KV-Cache) from previous sequences, compute the attention matrix (A). This calculation is modified by a causal mask (M) to ensure autoregressive properties, resulting in the output (O), as depicted in Equation 1 . $$\n\\mathbf{S}=\\mathbf{Q K}^{T}, \\quad \\mathbf{A}=\\operatorname{softmax}(\\mathbf{S}+\\mathbf{M}), \\quad \\mathbf{O}=\\mathbf{A} \\mathbf{V}\n$$\n\nAutoregressive inference in LLMs involves two stages: prefill and decode. During prefill, the model processes the entire input sequence to generate the initial response token. In the subsequent decode stage, it uses the newly generated token and previously cached K and V matrices to produce subsequent tokens until the generation concludes. Although effective, this iterative process increases memory and computation demands due to the expanding KV-Cache. ### 2.2 Efficient Attention\n\nEfficient methods are proposed to mitigate the computation and memory costs associated with attention. One branch of work uses dynamic sparse attention masks to adaptively skip attention computations during prefill stage [46, 52, 53, 64, 43, 32] or drop KV-Cache during decode stage [3, 75, 20, 54, 41] based on the input sequences. However, due to the complex control and computation flow, dynamic prefill often requires specific hardware to achieve substantial wall-time speedup [52, 64, 43, 25, 24]. Additionally, dynamic KV-Cache pruning in the decode stage may require extensive retraining [3], or additional accumulated attention score computation [54, 75, 41, 20]. Another branch of work uses static sparse attention, where predefined masks are applied consistently across all processed sentences. Thanks to the fixed computation flow, static sparse attention is generally more efficient and GPU-friendly. For language understanding models such as BERT [16], various masks are used [73, 5, 9, 76, 69, 26]. But for generative LLMs, the predominant method is the fixed-span sliding window mask with global attention on a few initial tokens [69, 26]. With the local attention pattern, the KV-Cache beyond the current attention span can be dropped, saving much memory for long sequence scenarios. However, the uniform static masks across different attention heads and input lengths are model- and data-agnostic, which can compromise LLMs' effective context length and lead to suboptimal performance in long sequence scenarios. Our method falls within this category, benefiting from the efficiency and training-free advantages, while addressing the performance limitations encountered by previous methods. In addition to sparse attention, alternative mechanisms have been proposed to replace traditional attention for long-sequence modeling [21, 49, 56, 51, 39, 30, 50, 10, 65]. However, these new mechanisms often require different weights compared to vanilla transformers, imposing significant re-training overhead for LLMs. Previous works also propose LLM acceleration frameworks [22, 2, 54, 33], as well as kernel-level optimizations [13, 12]. These kernel and system optimizations are orthogonal to our work and can be integrated to further enhance efficiency. ![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-04.jpg?height=284&width=854&top_left_y=255&top_left_x=362)\n\nFigure 2: Examples of attention matrices from different attention heads of the Vicuna-7B model. Each attention matrix is averaged over 256 data items from the LongEval dataset. | Layers | Window/Input Len. |  |  |\n| :---: | :---: | :---: | :---: |\n|  | 2k/4k | 2k/8k | $4 \\mathrm{k} / 8 \\mathrm{k}$ |\n| $6,7,8$ | 0.83 | 0.29 | 0.61 |\n| $9,10,11$ | 0.99 | 0.81 | 0.96 |\n| $17,18,19$ | 0.97 | 0.94 | 0.97 |\n\nTable 1: Retrieval accuracy of Vicuna-7B model using slidingwindow sparse attention masks across various model layers, window spans, and input lengths. ## 3 Mixture of Attention (MoA)\n\nWe first illustrate the heterogeneity of the attention patterns in pre-trained LLMs in Section 3.1. Based on this insight, we define the search space for our Mixture-of-Attention (MoA) method in Section 3.2\n\n### 3.1 Mixture of Attention Patterns and Elastic Rules\n\nHeterogeneous Attention Patterns. Different attention heads in LLMs exhibit heterogeneous attention patterns, as shown in Figure 2. For example, the first head primarily focuses on local contexts with a narrow-span sliding window, while the third head covers nearly the entire input, indicating global attention. Table 1 demonstrates that applying the same sliding-window sparse attention mask across model layers can lead to a $65 \\%$ variance in retrieval accuracies. It conforms to the multi-head self-attention design principle of capturing varied information [62], as well as the findings from concurrent research that identifies specific attention heads for global text retrieval [67]. Heterogeneous Elastic Rules. In addition to heterogeneity at a certain length, different attention heads also exhibit varying elastic behaviors as the input length changes. Figure 2 illustrates this variability: for shorter inputs (the upper left part of the attention matrix), the second and third heads initially show global attention. However, as input length increases, the second head remains the medium-span local focus, while the third head continues to expand as global attention. Table 1 further evidences the diverse elastic rules. For example, at 4 k input length, a 2 k sliding-window sparse attention mask on layers 9 to 11 yields better retrieval accuracy than on layers 17 to 19. However, the opposite is true for an 8 k input length. This data supports the visual observations from Figure 2 highlighting that attention patterns respond to input length scaling differently. Leveraging these insights, MoA encompasses heterogeneous elastic rules as the search space. ### 3.2 Heterogeneous Elastic Rule Search Space\n\nIn designing the search space for the MoA mask, we consider the inherently heterogeneous and elastic nature of LLM attention patterns. As shown in Figure 3(a), we adopt a hardware-friendly sliding-window mask as our base sparse attention mask [5]. Following previous work [69, 26], the initial few tokens ( 64 tokens for MoA ) are not masked. The attention span equals the sliding-windowspan plus the number of initial unmasked tokens. For simplicity, we define the attention span $S$ of head $h$ at input length $N$ using a straightforward linear function:\n\n$$\nS_{h}=\\alpha_{h}+\\beta_{h} \\times N\n$$\n\nwhere $\\alpha_{h}$ and $\\beta_{h}$ are hyperparameters that control the base span and its expansion rate with input length. To ensure hardware-friendliness, we limit the number of distinct rules to at most two per model layer for MoA unless specified otherwise. The $\\alpha$ and $\\beta$ hyperparameters of each attention head can be chosen from multiple discrete options. For LLMs with many layers, the MoA search space can become quite large. For example, for a 7B model consisting of 32 attention heads and 32 layers, with 6 and 9 different $\\alpha$ and $\\beta$ options for each head, the potential search space expands to $54^{1024}$ configurations. Thus, we design the automatic pipeline to efficiently pinpoint the optimal $\\alpha \\mathrm{s}$ and $\\beta \\mathrm{s}$ for any LLM. ![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-05.jpg?height=352&width=1377&top_left_y=240&top_left_x=368)\n\nFigure 3: Overview of the MoA. (a) The sparse attention search space includes heterogeneous elastic rules of the attention span on sliding-window masks. (b) The automatic compression pipeline begins with a calibration dataset, which includes long-dependency contexts and supervision texts generated by the original dense LLM. MoA profiles each attention value's impact on model predictions within this dataset, revealing accuracy losses for different candidate elastic rules across various input lengths. The final optimization step selects elastic rules for each attention head to minimize the total prediction loss while adhering to specified density constraints. ## 4 Automatic Pipeline for MoA Compression\n\nThis section outlines the MoA automatic compression pipeline as shown in Figure 3 b). Starting with a trained LLM and a calibration dataset, MoA first profiles the influence of each attention value on the model's prediction loss for various input sequences from the calibration dataset. The masked sum of the influences represents the accuracy loss associated with each mask at different input lengths, showing the accuracy loss each candidate elastic rule could cause at that length. Then, MoA optimizes the compression plan by selecting the optimal elastic rule for each head, which minimizes the accuracy loss across various lengths while adhering to specified density constraints. The following sections provide detailed discussions of each step in this pipeline. ### 4.1 Attention Influence Profiling\n\nIn the profile step, MoA quantifies the impact of individual attention values on the final prediction loss of a pre-trained LLM. It informs the subsequent step about the influence of masking each attention value, revealing the accuracy trade-offs of the candidate elastic rules for each attention head. The influence of each attention value is derived from the attention matrix $\\mathbf{A}$ and its gradient $\\partial L / \\partial \\mathbf{A}$, computed over a calibration dataset. When applying sparse attention masks, we approximate the change in the model's prediction loss, $\\Delta L$, using a first-order Taylor expansion based on variations in the attention matrices $\\mathbf{A}: \\Delta L=\\sum_{h} \\sum_{i} \\sum_{j} \\partial L / \\partial A_{h, i, j} \\cdot \\Delta A_{h, i, j}$. Here, $h$ indexes the attention heads across all layers, and $i, j$ are the row and column indices within each attention matrix $\\mathbf{A}_{h}$. We define the attention influence matrix, $E_{h, i, j}$, as the estimated change in loss, $\\Delta L$, if the attention value $A_{h, i, j}$ is masked (i.e., set to zero). As shown in Equation 3, this measure considers both the direct and indirect effects of the mask. For notation simplicity, we omit the head index $h$ here. Initially, masking directly reduces the attention value to zero, represented by $\\Delta A_{i, j \\mid j}=-A_{i, j}$. Additionally, the softmax function in attention normalizes the sum of each row in the attention matrix to one. Thus, setting one attention value at column $j$ to zero causes an increase in the other attention values, $\\Delta A_{i, n \\mid j}, n \\neq j$, within the same row. These two effects are integrated into the following formulation, whose derivation is provided in Appendix D.1:\n\n$$\nE_{i, j}=\\sum_{n} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot \\Delta A_{i, n \\mid j}=\\frac{\\partial L}{\\partial A_{i, j}} \\cdot\\left(-A_{i, j}\\right)+\\sum_{n \\neq j} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot A_{i, n} \\cdot \\frac{A_{i, j}}{1-A_{i, j}}\n$$\n\nIn practice, we use backpropagation on a calibration dataset to calculate the average attention influence $\\overline{\\mathbf{E}}_{h}$ of each head across data items. The average attention influence is calculated respectively for different input lengths. The gradient $\\partial L / \\partial \\mathbf{A}_{h}$ is computed using chain derivative in deep learning frameworks like PyTorch [48]. The detailed calibration dataset setup is discussed in Section 5\nWith the average attention influence of each head, MoA can calculate the accuracy loss of applying a candidate elastic rule at a specific input length. The loss is calculated with the sum of masked\n\nTable 2: Calibration dataset design choices: dataset content, supervision, and response reference. Calibration dataset with long dependency and model alignment improves MoA performance on retrieval accuracy and perplexity. All tests are done at $25 \\%$ average density at 8 k input length. | Dataset | Supervision | Reference | Long Dep. | Align Model $\\mid$ | Retrieval Acc. $\\uparrow$ | PPL $\\downarrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| RedPajama | Context | - | $x$ | $x$ | 0.25 | 4.95 |\n| MultiNews | Context \\& Summary | Human | $X / \\checkmark$ | $x$ | 0.27 | 4.62 |\n| MultiNews | Summary | Human | $\\checkmark$ | $x$ | 0.87 | 3.97 |\n| MultiNews | Summary | Model | $\\checkmark$ | $\\checkmark$ | 0.95 | 3.96 |\n\naverage attention influence according to the rule. We denote $\\mathbf{M}_{r_{h}}$ as the binary mask at head $h$ that corresponds to rule $r$, with masked positions marked as 0 and others as 1 . We formalize accuracy loss $\\Delta L$ as follows:\n\n$$\n\\Delta L=\\sum_{h} \\Delta L_{h, r_{h}}=\\sum_{h} \\sum_{i} \\sum_{j} M_{r_{h}, i, j} \\bar{E}_{h, i, j}\n$$\n\nAfter the profile stage, MoA acquires the unique accuracy-density trade-offs of elastic rules. It informs the allocation of denser masks to more sensitive heads and lighter masks to less sensitive ones. Profiling at different input lengths enables the identification of the most effective elastic rules, even for unseen lengths. ### 4.2 Automatic Optimization\n\nMoA automatically selects the optimal elastic rule for each attention head to minimize accuracy losses across various sequence lengths under density budgets. Based on the profiling results, MoA first identifies Pareto front compression plans where any improvement in accuracy loss at one profile length would worsen another. To ensure the best generalization to lengths beyond those profiled, MoA then selects the plan that yields the minimum loss at an unseen length among the Pareto front solutions as the final plan. Specifically, we utilize multi-objective optimization to search for a set of Pareto optimal compression plans across the profiled lengths. The objective for each length is to minimize the total accuracy loss while conforming to any user-defined density constraints. The objective is formulated as follows:\n\n$$\n\\underset{r_{h} \\in \\mathbb{R}}{\\arg \\min } \\Delta L^{\\left(N_{i}\\right)}, N_{i} \\in \\mathbb{N}_{\\text {profile }} \\quad \\text { s.t. } \\frac{1}{H} \\sum_{h=1}^{H} d_{r_{h}}^{\\left(N_{i}\\right)} \\leq d_{\\text {constr }}^{\\left(N_{i}\\right)}, \\forall N_{i} \\in \\mathbb{N}_{\\text {constr }}\n$$\n\nHere, superscript ( $N$ ) denotes values at different lengths; $\\mathbb{N}_{\\text {profile }}$ and $\\mathbb{N}_{\\text {constr }}$ denote the sets of lengths for profiling and those subject to density constraints, respectively; $\\mathbb{R}$ denotes the set of candidate rules; $\\Delta L^{\\left(N_{i}\\right)}$ denotes the accuracy loss due to compression; $d_{r_{h}}^{\\left(N_{i}\\right)}$ denotes the density of rule $r_{h}$ at head $h ; d_{\\text {constr }}^{\\left(N_{i}\\right)}$ denotes the average density constraint; $H$ denotes the total number of attention heads. Such formulation corresponds to the classic multi-objective mixed-integer-planing problem, which can be effectively solved within minutes using existing linear solvers, like Gurobi [23]. The detailed problem formulation and solving strategies are discussed in Appendix D. 2\nAmong the Pareto optimal compression plans, we select the one with the minimum loss at the unseen validation length as the optimal solution. This approach allows us to avoid profiling at every possible length while increasing the likelihood that the plan will generalize effectively to unseen lengths. Thanks to this automatic pipeline, we efficiently get the elastic rules tailored for each attention head. With the pipeline, MoA minimizes the accuracy loss caused by attention sparsification, while conforming to user-defined density constraints. ## 5 Dataset and Supervision\n\nIn this section, we highlight the overlooked importance of calibration dataset design and its supervision objective in LLM compression. Calibration datasets are essential for sensitivity analysis across various compression techniques, including weight pruning [44, 34, 42] and quantization [40, 68, 36, 31]. In this work, MoA profiles the attention influence on the calibration dataset, which is crucial for subsequent automatic optimization. Current Approach. General language modeling datasets, such as human-written text corpus RedPajama [11], are commonly used as the calibration dataset. These datasets, supervised by nexttoken-prediction on the entire corpus, primarily capture attention patterns coherent with immediately preceding tokens. However, they lack long context dependencies, failing to address the global attention crucial for tasks like long-range retrieval. Moreover, a notable misalignment exists between the model response and the human-written supervision. Consequently, it leads to inaccuracies when using human responses to compute attention values and gradients during profiling. For example, given the same question, a human might answer 'Blue', while the model could generate 'The blue color'. Using the human answer for supervision, attention influence is inaccurately quantified based on probability shift for predicting 'Blue'; this diverges from the objective of maintaining crucial attention for the original model prediction, 'The'.",
    "moa-4": "These inconsistencies arise from various factors, including mismatched positions, tones, and synonyms. MoA's Approach. MoA enhances the calibration dataset by integrating long-range dependencies and model alignment. It utilizes the long-contextual MultiNews dataset [17], which includes summaries heavily dependent on long-range content. The summaries are generated by the original dense model and serve as supervision. Compared to current approaches that adopt human responses as the reference to calculate the loss, using the responses generated by the original model as the supervision can facilitate accurate influence profiling, thus benefiting the compression results. Approach Comparison. We evaluate our design's effectiveness by varying dataset choices, supervision types, and summary references, while standardizing data item count and length to 50 and 8 k words, respectively. Additional setups and evaluations are in Appendices A and B.3.1\nWe show the importance of long-range dependencies by comparing the MoA compression plan generated with different datasets and supervisory methods. In Table 2, RedPajama [11] represents the general language modeling dataset, while MultiNews [17] highlights long-range contexts by aggregating multiple documents on a single incident. Additionally, each MultiNews item includes a human-written summary, providing even stronger long-range dependencies and better performance. Calculating loss on the summary of MultiNews leads to significantly better performance, with a $60 \\%$ increase in retrieval accuracy and a 0.98 decrease in perplexity. Furthermore, using summaries generated by the original dense model as supervision promotes higher alignment between its own attention patterns and the text supervision. It improves performance compared to potentially inconsistent human summaries, as shown in the last two rows of Table 2\n\n## 6 Experiment\n\n### 6.1 Setups\n\nWe brief the experiment setups here, with more details in Appendix A\nBaselines. We compare MoA with state-of-the-art static and dynamic sparse attention baselines for LLMs: StreamingLLM [69] and H2O [75]. We define the density of an LLM as the average KV-Cache length divided by the sequence length during the sparse decode stage. Notably, in MoA and StreamingLLM, the KV-Cache length matches the attention span during their sparse prefill stages. In contrast, H2O employs a dense prefill and requires additional computations for dynamic mask generation based on accumulative attention scores. Models and Benchmarks. We evaluate on vicuna-7b-v1.5-16k, vicuna-13b-v1.5-16k [8] from LMSys, and Llama-3-8B-Instruct-262k [1] from Gradient AI. For effective context length evaluation, we use LongEval [35] to test key-value retrieval accuracy with 100 data items per length level. For comprehensive ability evaluation, we use LV-Eval [70] and LongBench [4], which include 11 and 13 sub-tasks, respectively. For coherence testing, we measure perplexity on four subsets of LongBench [15, 17, 38, 28, 45], representing different capabilities. MoA Settings. We profile MoA on MultiNews [17] with model summaries at $2 \\mathrm{k}, 4 \\mathrm{k}$, and 8 k lengths. The optimal compression plan is selected with the validation dataset at 12 k .",
    "moa-5": "Unless specified, the same plan is used across all benchmarks and lengths for each model. The models are not finetuned. Table 3: Comparative analysis of retrieval accuracy, LV-Eval scores, LongBench scores, and perplexity for various models with different attention methods. All methods employ 50\\% density in both prefill and decode stages, except for H 2 O , which uses dense prefill. | Model | Attention | Retrieve Acc. $\\uparrow$ |  |  | LV-Eval $\\uparrow$ <br> 16k | LongBench $\\uparrow$ |  |  | PPL $\\downarrow$ <br> $8-12 k$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | 4 k | 8k | 16 k |  | 0-4k | $4-8 \\mathrm{k}$ | 8-16k |  |\n| Vicuna-7B | Original | 1.00 | 0.98 | 0.62 | 5.93 | 37.91 | 33.82 | 32.54 | 3.79 |\n|  | H2O | 0.86 | 0.68 | 0.35 | 5.42 | 36.23 | 32.74 | 31.81 | 3.94 |\n|  | StreamingLLM | 0.43 | 0.16 | 0.08 | 4.72 | 30.53 | 33.28 | 31.70 | 4.48 |\n|  | MoA | 1.00 | 0.97 | 0.57 | 5.61 | 37.04 | 32.90 | 31.94 | 3.75 |\n| Vicuna-13B | Original | 0.99 | 0.98 | 0.44 | 5.83 | 42.25 | 39.52 | 35.93 | 3.62 |\n|  | H2O | 0.88 | 0.76 | 0.28 | 5.66 | 41.63 | 38.02 | 34.75 | 3.80 |\n|  | StreamingLLM | 0.65 | 0.49 | 0.33 | 5.43 | 30.65 | 33.07 | 32.68 | 4.10 |\n|  | MoA | 0.99 | 0.93 | 0.49 | 7.16 | 41.73 | 38.88 | 35.69 | 3.62 |\n| Llama3-8B | Original | 0.99 | 0.99 | 0.97 | 17.49 | 44.27 | 43.53 | 43.26 | 4.52 |\n|  | H 2 O | 0.94 | 0.89 | 0.88 | 16.03 | 43.46 | 43.01 | 42.50 | 4.63 |\n|  | StreamingLLM | 0.68 | 0.55 | 0.52 | 11.16 | 37.20 | 38.02 | 39.43 | 4.79 |\n|  | MoA | 0.99 | 1.00 | 1.00 | 17.46 | 43.07 | 42.75 | 43.09 | 4.49 |\n| {(a) Attention Span <br> (b) Density <br> (c) Input Length} |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |\n\nFigure 4: Retrieval accuracy tests on LongEval. With Vicuna-7B: (a) varies input lengths and densities to show effective context lengths across attention spans, (b) sets input length at 8 k and shows retrieval accuracy across different densities. With Llama3-8B: (c) sets density at 50\\% and shows retrieval accuracy across increasing input lengths. ### 6.2 Performance\n\nOverall Performance. As detailed in Table 3 MoA outperforms state-of-the-art sparse attention methods and achieves comparable performance to the original dense model at $50 \\%$ density. We calculate the relative performance drop of the compressed model over the original one. For retrieval tasks, MoA demonstrates a maximum of $8 \\%$ relative accuracy drop (calculated as $\\max \\left\\{1-\\right.$ Acc. $_{\\text {MoA }} /$ Acc. $^{\\text {Original }}$ $\\}$ across three lengths and LLMs), significantly less than the $87 \\%$ and $44 \\%$ for StreamingLLM and H 2 O . On average, the relative accuracy drop for MoA is under $1 \\%$, while it is much higher for StreamingLLM and H2O at $51 \\%$ and $20 \\%$, respectively. In benchmarks using LV-Eval and LongBench, MoA shows only a 5\\% and 3\\% maximum relative drop in scores, respectively, compared to steep declines for StreamingLLM at $36 \\%$ and $27 \\%$. H2O exhibits maximum drops of $9 \\%$ and $4 \\%$ with higher efficiency costs. Similar trends show in perplexity tests, where MoA maintains less than $1 \\%$ relative perplexity increase, while StreamingLLM and H2O exhibit $13 \\%$ and $4 \\%$ increases. This trend holds for other densities, as shown in Appendices B.1.1 and B.1.3\n\nLong-Context Retrieval. We investigate the ultimate effective context length with retrieval tests on LongEval [35]. As shown in Figure 4 (a), MoA expands its effective context length to approximately $3.9 \\times$ its attention span before reaching up to the 12 k limit of the original model. Detailed data for effective context length calculation are in Appendix B.1.2. Figure 4(b) shows that at a fixed input length of 8 k , MoA reaches over 0.9 retrieval accuracy with just $25 \\%$ density, whereas StreamingLLM and H 2 O require $100 \\%$ and $75 \\%$ density, respectively. Figure 4(c) further investigates MoA's ability to expand and generalize to extreme input lengths. By profiling within 8 k lengths, MoA extends its capability to 60 k inputs while maintaining over $90 \\%$ retrieval accuracy at $50 \\%$ density. ![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-09.jpg?height=354&width=1272&top_left_y=197&top_left_x=421)\n\nFigure 5: (a) LV-Eval and (b) LongBench scores for different attention methods at 50\\% density, tested on Vicuna-7B and 13B models. Scores normalized against the original dense model. Table 4: Efficiency analysis of different frameworks on 7 B and 13B models. H2O and MoA use $50 \\%$ density. GPU memory evaluated with batch sizes 8 ( 7 B model) and 4 (13B model). Decode throughput evaluated at the maximum batch capacity of an A100-80GB GPU. |  |  | Memory (GB) |  |  |  | Throughput $($ tok/s |  |  |  |\n| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Size | Framework | 4 k | 8 k | 16 k | 4 k | 8 k | 16 k |  |  |\n| 7B | FlashAttn2 | 28.5 | 44.4 | 76.3 | 134.6 | 66.9 | 32.9 |  |  |\n|  | H2O | 36.9 | OOM | OOM | 754.9 | 296.3 | 51.7 |  |  |\n|  | MoA | $\\mathbf{2 2 .",
    "moa-6": "7}$ | $\\mathbf{3 2 . 9}$ | $\\mathbf{5 3 . 5}$ | $\\mathbf{8 9 7 .",
    "moa-7": "7}$ | $\\mathbf{4 3 6 . 7}$ | $\\mathbf{2 0 6 . 4}$ |  |  |\n|  | FlashAttn2 | 36.8 | 49.2 | 74.0 | 81.3 | 40.8 | 19.8 |  |  |\n| 13B | H2O | 40.4 | 77.9 | OOM | 330.2 | 138.2 | 37.4 |  |  |\n|  | MoA | $\\mathbf{3 2 .",
    "moa-8": "0}$ | $\\mathbf{3 9 . 6}$ | $\\mathbf{5 5 . 0}$ | $\\mathbf{4 7 1 .",
    "moa-9": "5}$ | $\\mathbf{2 2 2 . 6}$ | $\\mathbf{1 0 8 . 3}$ |  |  |\n\nTable 5: Ablation study on search space with consistent $25 \\%$ density, progressively introducing heterogeneity in layers, heads, and elastic rules. Evaluations are done with retrieval accuracy and perplexity. |  | Retrieval Acc. |  | PPL |  |\n| :--- | :---: | :---: | :---: | :---: |\n| Mask Design | 8 k | 16 k | 8 k | 12 k |\n| Uniform | 0.25 | 0.15 | 4.89 | 5.19 |\n| +Hetero. Layers | 0.31 | 0.26 | 4.55 | 4.85 |\n| +Hetero. Heads | 0.95 | 0.41 | $\\mathbf{3 . 9 6}$ | 4.30 |\n| +Elastic | $\\mathbf{0 . 9 8}$ | $\\mathbf{0 . 4 3}$ | $\\mathbf{3 . 9 6}$ | $\\mathbf{4 . 2 9}$ |\n\nLong-Context Understanding. Figure 5illustrates the performance comparison across long-context benchmarks LV-Eval and LongBench. MoA achieves comprehensive performance comparable to the original dense model, as well as H 2 O that requires higher efficiency cost. In contrast, StreamingLLM displays inconsistent performance: although it occasionally surpasses the original model, it generally suffers noticeable degradation. Ablation Study. We evaluate the performance impact of different sparse mask search spaces in Table 5 Starting with a basic uniform mask, we observe significant enhancements by sequentially introducing heterogeneity: layers first, then heads, and finally elastic rules. ### 6.3 Efficiency\n\nWe use Huggingface [66] with FlashAttention [13] as the baseline and implement MoA based on it. As shown in Table 5 , with an average $50 \\%$ density, MoA reduces total GPU memory by $1.2 \\times$ to $1.4 \\times$ across various model sizes and input lengths. Moreover, MoA boosts decode throughput by $5.5 \\times$ to $6.7 \\times$, primarily attributed to a static-size KV-Cache $(\\approx 3.0 \\times$ ), reduced attention computations $(\\approx 1.5 \\times$ ) and increased batch sizes $(\\approx 1.4 \\times)$. Compared with H2O that uses dense prefill and requires additional cost for dynamic mask generation, MoA achieves 1.2 to 4.0 throughput at the same $50 \\%$ density. Compared to the highly optimized vLLM framework [33], MoA still achieves a $1.4 \\times$ to $1.5 \\times$ increase in throughput. More efficiency results are shown in Appendix B.2\n\n### 6.4 Rules Discovered by MoA\n\nWe investigate MoA's elastic rules for each head. As shown in Figure 8 masks in the initial and middle layers exhibit high density, aligning with the conclusions from previous research on LLM's intrinsic dimensions [61] and layer sensitivities [71]. Conversely, in the final layers, most heads require low density, while few need high density. Figure 9 shows that layers with lower average mask density typically display a wider range of densities among heads, confirming the need for heterogeneity within the same layer. Further details and quantified insights are presented in Appendix C. ## 7 Conclusion and Future Work\n\nMoA automates the selection of heterogeneous elastic masks for each attention head and input length, significantly extending the effective context length of LLMs by $3.9 \\times$. It enhances retrieval\naccuracy by $1.5 \\times$ to $7.1 \\times$ and increases throughput to over $5 \\times$ at $50 \\%$ average density, maintaining performance on par with dense models in rigorous benchmarks. Limitations and Future Work. Under an extremely low-density budget, MoA fails to maintain good performance. Designing a dynamic MoA method has the potential to address this issue, which we leave for future work. To further enhance MoA's efficiency, system, and kernel-level optimizations such as kernel fusion [27] and KV-Cache management [33] could be integrated. Using non-linear elastic rules with bounded attention spans is also worth exploring. Additionally, MoA's profiling method can be adapted to evaluate the influence of weights and other activations, facilitating other compression methods such as weight and activation quantization [40, 68, 37]. ## Acknowledgement\n\nThis work was supported by National Natural Science Foundation of China (No. 62325405, 62104128, U19B2019, U21B2031, 61832007, 62204164), Tsinghua EE Xilinx AI Research Fund, and Beijing National Research Center for Information Science and Technology (BNRist). ## References\n\n[1] Meta AI. Introducing llama 3: Meta's latest large language model. https://ai.meta.com/ blog/meta-llama-3/2024. Accessed: 2024-05-17. [2] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He. Deepspeed- inference: Enabling efficient inference of transformer models at unprecedented scale. SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1-15, 2022. [3] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. ArXiv, abs/2305.15805, 2023. [4] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding, 2023. [5] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [7] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. ArXiv, abs/2306.15595, 2023. [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \\% *$ chatgpt quality, March 2023. [9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [10] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers.",
    "moa-10": "In International Conference on Learning Representations, 2020. [11] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April 2023. [12] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. ArXiv, abs/2307.08691, 2023.",
    "moa-11": "[13] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness.",
    "moa-12": "In Advances in Neural Information Processing Systems, 2022. [14] Rocktim Jyoti Das, Liqu Ma, and Zhiqiang Shen. Beyond size: How gradients shape pruning decisions in large language models. arXiv preprint arXiv:2311.04902, 2023. [15] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. ArXiv, abs/2105.03011, 2021. [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.",
    "moa-13": "arXiv preprint arXiv:1810.04805, 2018. [17] Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Annual Meeting of the Association for Computational Linguistics, 2019. [18] Aosong Feng, Irene Li, Yuang Jiang, and Rex Ying. Diffuser: Efficient transformers with multi-hop attention diffusion for long sequences. arXiv preprint arXiv:2210.11794, 2022. [19] Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hanna Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. ArXiv, abs/2402.10171, 2024. [20] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms.",
    "moa-14": "ArXiv, abs/2310.01801, 2023. [21] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.",
    "moa-15": "ArXiv, abs/2312.00752, 2023. [22] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. [23] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023. [24] Tae Jun Ham et al. A^ 3: Accelerating attention mechanisms in neural networks with approximation. In HPCA, pages 328-341. IEEE, 2020. [25] Tae Jun Ham et al. Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks.",
    "moa-16": "In ISCA, pages 692-705. IEEE, 2021. [26] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. [27] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu Wang. Flashdecoding++: Faster large language model inference on gpus.",
    "moa-17": "ArXiv, $\\mathrm{abs} / 2311.01282,2023$. [28] Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Toward semantics-based answer pinpointing. In Proceedings of the First International Conference on Human Language Technology Research, 2001. [29] Cheng Jiang, Ranjun Li, Zhuoyi Zhang, and Yu Shen. Pushing gradient towards zero: A novel pruning method for large language models, 2023.",
    "moa-18": "[30] Praneeth Kacham, Vahab S. Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers via sketches for polynomial kernels. ArXiv, abs/2310.01655, 2023. [31] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization.",
    "moa-19": "arXiv preprint arXiv:2306.07629, 2023. [32] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.",
    "moa-20": "arXiv preprint arXiv:2001.04451, 2020. [33] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Haotong Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. Proceedings of the 29th Symposium on Operating Systems Principles, 2023. [34] Je-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, and Azalia Mirhoseini. Cats: Contextually-aware thresholding for sparsity in large language models. arXiv preprint arXiv:2404.08763, 2024. [35] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023. [36] Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li, Kai Zhong, Guohao Dai, Huazhong Yang, and Yu Wang. Llm-mq: Mixed-precision quantization for efficient llm deployment. NeurIPS Workshop, 2024. [37] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158, 2024. [38] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. [39] Yuhong Li, Tianle Cai, Yi Zhang, De huai Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? ArXiv, abs/2210.09298, 2022. [40] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration.",
    "moa-21": "arXiv preprint arXiv:2306.00978, 2023. [41] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. [42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023. [43] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21, page 977-991, New York, NY, USA, 2021. Association for Computing Machinery. [44] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect.",
    "moa-22": "ArXiv, abs/2403.03853, 2024. [45] Michael Mohler, Mary Brunson, Bryan Rink, and Marc Tomlinson. Introducing the LCC metaphor datasets. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 4221-4227, Portoro\u017e, Slovenia, May 2016. European Language Resources Association (ELRA). [46] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and Franccois Fleuret. Faster causal attention over large sequences through sparse flash attention.",
    "moa-23": "ArXiv, abs/2306.01160, 2023. [47] Biswajit Paria, Kirthevasan Kandasamy, and Barnab\u00e1s P\u00f3czos. A flexible framework for multiobjective bayesian optimization using random scalarizations. In Conference on Uncertainty in Artificial Intelligence, 2018. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [49] Bo Peng, Eric Alcaide, Quentin G. Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, G Kranthikiran, Xuming He, Haowen Hou, Przemyslaw Kazienko, Jan Koco\u0144, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan Sokrates Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui Zhu. Rwkv: Reinventing rnns for the transformer era. In Conference on Empirical Methods in Natural Language Processing, 2023. [50] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong. Random feature attention. ArXiv, abs/2103.02143, 2021. [51] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.",
    "moa-24": "arXiv preprint arXiv:2302.10866, 2023. [52] Zheng Qu, Liu Liu, Fengbin Tu, Zhaodong Chen, Yufei Ding, and Yuan Xie. Dota: Detect and omit weak attentions for scalable transformer acceleration. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS '22, page 14-26, New York, NY, USA, 2022.",
    "moa-25": "Association for Computing Machinery. [53] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021. [54] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark W. Barrett, Joseph Gonzalez, Percy Liang, Christopher R\u00e9, Ion Stoica, and Ce Zhang. High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, 2023. [55] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and James Tin-Yau Kwok. Sparsebert: Rethinking the importance analysis in self-attention.",
    "moa-26": "In International Conference on Machine Learning, pages 9547-9557. PMLR, 2021. [56] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. ArXiv, $\\mathrm{abs} / 2307.08621,2023$. [57] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. [58] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10-19, 2019 . [59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [60] Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Milo's. Focused transformer: Contrastive training for context scaling.",
    "moa-27": "ArXiv, abs/2307.03170, 2023. [61] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga. The geometry of hidden representations of large transformer models. ArXiv, abs/2302.00294, 2023. [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [63] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. Efficient large language models: A survey. ArXiv, abs/2312.03863, 2023. [64] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning.",
    "moa-28": "In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pages 97-110. IEEE, 2021. [65] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.",
    "moa-29": "arXiv preprint arXiv:2006.04768, 2020. [66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [67] Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality.",
    "moa-30": "arXiv preprint arXiv:2404.15574, 2024. [68] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models, 2024. [69] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [70] Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. Lv-eval: A balanced long-context benchmark with 5 length levels up to 256k, 2024. [71] Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-aware singular value decomposition for compressing large language models. ArXiv, $\\mathrm{abs} / 2312.05821,2023$. [72] Yv Haimes Yv, Leon S. Lasdon, and Dang Da. On a bicriterion formation of the problems of integrated system identification and system optimization. IEEE Transactions on Systems, Man, and Cybernetics, pages 296-297, 1971. [73] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020 . [74] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv, abs/2205.01068, 2022. [75] Zhenyu (Allen) Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R\u00e9, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. ArXiv, abs/2306.14048, 2023. [76] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, and Yu Wang. A survey on efficient inference for large language models. ArXiv, abs/2404.14294, 2024. ## A Detailed Experiment Setup\n\n## A. 1 Main Setup\n\nBaselines. In the setup for our experiment, we adhere to specific configurations outlined in the respective papers. In the case of StreamingLLM [69], the initial four tokens remain unmasked, serving as the attention sink. For H 2 O [75], we ensure the same number of heavy hitter tokens and recent tokens. Note that H 2 O uses dense prefill since it relies on the column sum of the attention matrix to calculate the importance of every token for KV-Cache eviction. StreamingLLM and MoA use sparse prefill. Models and Benchmarks. Since vicuna-7b-v1.5-16k and vicuna-13b-v1.5-16k [8] can only take in 16 k context length, we use the 16 k split of LV-Eval benchmark [70], truncating the input to 15500 for model input. For the LongBench benchmark [4], we use the LongBench-E split, which features a balanced number of data items at every length level. The LongBench dataset is segmented into ranges of $0-4 \\mathrm{k}, 4-8 \\mathrm{k}$, and $8 \\mathrm{k}+$ tokens. We test each split using the input length truncation thresholds of $3,500,7,500$, and 15,500 tokens, respectively. Perplexity Evaluation. We construct a comprehensive yet concise test set by sampling $50 \\times 4$ data items for each length level from the test split of four long-context understanding datasets: Qasper [15], MultiNew [17], TREC [38, 28] and LCC [45], representing the question answering, summarization, few-shot learning, and code completion abilities of the LLM.",
    "moa-31": "Following LongBench, the data items are organized as question-answer pairs. The questions and answers are written by humans and come with the dataset. The perplexity is calculated solely on the answer part of the data, demonstrating the model's coherence in responding to user requests. Validation Dataset. The validation dataset is used to select the optimal compression plan among the Pareto front solutions during the optimization step. The validation dataset is similarly constructed as the perplexity test dataset, but on the respective validation split of the datasets. $50 \\times 4$ data items are sampled from the same four long-context understanding datasets: Qasper [15], MultiNew [17], TREC [38, 28] and LCC [45]. The additional 50 data items from the LongEval [35] dataset are also added to validate the retrieval ability. For the datasets that do not contain the validation split, namely TREC, MultiNews and LCC, we sample from the test split and ensure different data items with the perplexity evaluation dataset. MoA Settings. MoA uses the block sparse attention pattern with a block size of 64, where each grid depicted in Figure 3 (a) represents a block. The first block of tokens is not masked as the attention sink. For the profile stage, we use the MultiNews [17] calibration dataset with model response as supervision, as described in Section 5 . We use $50 \\times 3$ data items at $2 \\mathrm{k}, 4 \\mathrm{k}, 8 \\mathrm{k}$ lengths. The data items are padded to their corresponding length level in order to ensure a unified shape of attention influence tensors for each length level. We adopt block granularity during the profiling stage, calculating the average attention influence within each block to represent the block's overall influence. The optimization is done with the multi-objective optimization at the same set of lengths. Among the Pareto front solutions, we select the one with the lowest perplexity on the validation dataset of length 12 k . ## A. 2 Efficiency Experiment Setup\n\nWe test the efficiency of different frameworks using a single NVIDIA A100-SXM4-80GB GPU. To improve the runtime profiling accuracy, we first run five forward passes as warmups.",
    "moa-32": "Then we use torch. CudaEvent to calculate the runtime for each method. Our experiments are structured around three scenarios: including prefilling 3 k tokens and decoding 1 k tokens; prefilling 6 k tokens and decoding 2 k tokens; prefilling 12 k tokens and decoding 4 k tokens. The labels are marked by the total sequence length, which equals prefill length plus decode length. For MoA, The implementation is based on Huggingface Transformers. During the prefill stage, we use the sparse triton [58] kernel designed by us with block size 64. During the decode stage, we modify the KV-Cache implementation to support our heterogeneous elastic rules. Thanks to our fixed sliding-window span during the decode stage, we simply replace the old KV-Cache that exceeds the span with the latest KV-Cache. Then, the dense FlashAttention [13] decoding is applied with the updated KV-Cache. For H 2 O , we use its official efficient implementation, which is based on Flexgen [54]. Note that H2O uses dense prefill since it relies on the column sum of the attention matrix to calculate the importance of every token for KV-Cache eviction, which requires the attention matrix to be explicitly calculated. It makes H2O's prefill stage currently incompatible with kernel optimizations like FlashAttention. Therefore, H 2 O is easy to get OOM (Out-Of-Memory) with large prefill length and increased batch size. In our efficiency tests across all frameworks, we implemented a simple optimization at the language modeling head ( 1 m head) during the prefill stage. Specifically, after the final layer of the transformers, we compute the logits-these are the raw outputs that are transformed into probabilities-for only the last token. This selective computation avoids generating these probabilities for preceding tokens, substantially reducing both computational overhead and memory usage. We also set the environment variable PYTORCH_CUDA_ALLOC_CONF to be expandable_segments:True for Hugginface and MoA to mitigate memory fragmentation, allowing larger inference batch size. Following the performance experiments, we use Vicuna-7B and Vicuna-13B for efficiency tests whenever possible. However, the official efficient implementation of H2O based on Flexgen only supports OPT [74]. Therefore, we use OPT-6.7b and OPT-13b models for H2O in Table 4 for comparison. ## A. 3 Ablation Study Setup\n\nIn the ablation study in Table 2 and Table 5, we use $25 \\%$ density instead of the $50 \\%$ used in the main experiment in Table 3 This decision is based on the observation that at a density of $50 \\%$, the performance of the various designs is quite similar, making it difficult to discern significant differences. In contrast, a lower density of $25 \\%$ reveals more pronounced disparities between the designs, providing a clearer basis for comparison. In the calibration dataset experiments in Table 2, we intentionally exclude the influence of the validation dataset. We avoid using the validation dataset by profile and optimize solely at 8 k length, reducing the multi-objective optimization problem to a single-objective one with only one optimal compression plan instead of a set of Pareto fronts. ## B Additional Experiment Results\n\n## B. 1 Performance\n\n## B.1. 1 Overall Performance\n\nTable 6: Comparative analysis of retrieval accuracy, LV-Eval scores, LongBench scores, and perplexity for various models with different attention methods. All methods employ 75\\% density in both prefill and decode stages. | Model |  | Retrieve Acc. $\\uparrow$ |  |  | LV-Eval $\\uparrow$ | LongBench $\\uparrow$ |  | PPL $\\downarrow$ |  |\n| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Attention | 4 k | 8 k | 16 k | 16 k | $0-4 \\mathrm{k}$ | $4-8 \\mathrm{k}$ | $8-16 \\mathrm{k}$ | $8-12 \\mathrm{k}$ |\n| Vicuna-7B | StreamingLLM | 0.91 | 0.35 | 0.09 | 4.30 | 36.39 | 32.44 | 31.04 | 3.92 |\n|  | MoA | $\\mathbf{1 .",
    "moa-33": "0 0}$ | $\\mathbf{0 . 9 7}$ | $\\mathbf{0 . 5 8}$ | $\\mathbf{5 . 6 7}$ | $\\mathbf{3 8 . 0 7}$ | $\\mathbf{3 3 . 8 0}$ | $\\mathbf{3 1 . 7 5}$ | $\\mathbf{3 . 7 8}$ |\n| Vicuna-13B | StreamingLLM | 0.73 | 0.81 | 0.37 | $\\mathbf{5 . 6 5}$ | 36.77 | 34.65 | 33.43 | 3.70 |\n|  | MoA | $\\mathbf{0 .",
    "moa-34": "9 9}$ | $\\mathbf{0 . 9 7}$ | $\\mathbf{0 . 4 2}$ | 5.57 | $\\mathbf{4 1 . 8 5}$ | $\\mathbf{3 9 . 7 6}$ | $\\mathbf{3 6 . 0 6}$ | $\\mathbf{3 . 6 2}$ |\n| Llama3-8B | StreamingLLM | $\\mathbf{1 . 0 0}$ | 0.83 | 0.76 | 14.89 | 42.45 | 40.62 | 42.51 | $\\mathbf{4 . 5 1}$ |\n|  | MoA | 0.99 | $\\mathbf{1 . 0 0}$ | $\\mathbf{0 . 9 3}$ | $\\mathbf{1 5 . 6 1}$ | $\\mathbf{4 3 .",
    "moa-35": "5 1}$ | $\\mathbf{4 3 . 1 6}$ | $\\mathbf{4 3 . 5 8}$ | 4.53 |\n\nTable 6 shows the overall performance of MoA at a higher density of $75 \\%$. MoA shows improved performance over the baseline with the uniform attention baseline. The progressive change of performance with respect to different densities is also shown in Figure 4b) and Figure 7\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-18.jpg?height=476&width=1356&top_left_y=236&top_left_x=365)\n\nFigure 6: Retrieval accuracy of Vicuna-7B model using different attention methods across varying attention spans and input lengths. The X -axis shows different attention spans; the Y-axis shows different input lengths for the retrieval task. Subfigure (a) shows results for StreamingLLM, and subfigure (b) for MoA. ![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-18.jpg?height=516&width=836&top_left_y=992&top_left_x=620)\n\nFigure 7: LV-Eval score of StreamingLLM and MoA at various densities on Vicuna-7B model. ## B.1.2 Long-Context Retrieval\n\nWe conduct a detailed experiment to test the retrieval ability of different attention methods across various attention spans and input lengths. Following previous work [7, 60], we quantify effective context length as the maximum input length where retrieval accuracy remains above a $90 \\%$ threshold. Figure 6 shows the detailed data for effective context length calculation. As shown in the figure, StreamingLLM can hardly maintain retrieval accuracy when the input length is beyond the attention span, while MoA can effectively extend the effective context length. ## B.1.3 Long-Context Understanding\n\nWe conduct experiments with various densities on the LV-Eval benchmark [70]. As shown in Figure 7 . MoA constantly outperforms the uniform static attention baseline StreamingLLM at various densities, demonstrating the effectiveness of our heterogeneous elastic rules. ## B. 2 Efficiency\n\n## B.2.1 Throughput Breakdown\n\nIn this section, we conduct a detailed analysis of the throughput breakdown for MoA, compared to the baseline comprising Huggingface with FlashAttention. The observed increase in throughput, as shown in Table 7 primarily stems from three aspects:\n\nTable 7: Improvements in throughput (tokens per second) across various configurations and input sizes for MoA over FlashAttn2 on models with 7 b and 13 b parameters. The batch sizes for FlashAttn2 are configured to the maximum capacity of an A100-80GB GPU. | Size | Configuration | 4 k |  | 8k |  | 16 k |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  | Batch | Throughput | Batch | Throughput | Batch | Throughput |\n| 7B | FlashAttn2 | 30 | 134.6 | 15 | 66.9 | 8 | 32.9 |\n|  | +Static KV-Cache | 30 | 496.1 | 15 | 219.5 | 8 | 91.6 |\n|  | + Reduced Attention | 30 | 722.5 | 15 | 369.9 | 8 | 178.3 |\n|  | +Increased Batch Size | 50 | 897.7 | 25 | 436.7 | 12 | 206.4 |\n| 13B | FlashAttn2 | 16 | 81.3 | 8 | 40.8 | 4 | 19.8 |\n|  | +Static KV-Cache | 16 | 264.6 | 8 | 111.3 | 4 | 62.2 |\n|  | +Reduced Attention | 16 | 329.6 | 8 | 156.4 | 4 | 87.3 |\n|  | + Increased Batch Size | 28 | 471.5 | 14 | 222.6 | 7 | 108.3 |\n\nStatic KV-Cache. MoA only maintains the tokens within the span of each head, thereby preventing growth in the KV-Cache size. This strategy eliminates the need for additional memory allocation. Reduced Attention Computation. MoA with features reduced density in attention span and KVCache. It decreases the computation and memory access required for attention computation. Increased Batch Size. With the reduced size of KV-Cache, MoA supports a larger batch size, contributing to the increase in throughput. ## B.2.2 Comparison with vLLM\n\nTable 8: Throughput comparison of MoA and vLLM on 7B and 13B models. | Size | Framework | Throughput (tok/s) |  |  |\n| :---: | :---: | :---: | :---: | :---: |\n|  |  | 4 k | 8 k | 16k |\n| 7B | vLLM | 628.8 | 323.0 | 145.5 |\n|  | MoA | 897.7 | 436.7 | 206.4 |\n| 13B | vLLM | 314.8 | 160.5 | 71.1 |\n|  | MoA | 471.5 | 222.6 | 108.3 |\n\nWe compare the throughput of MoA with the highly optimized inference framework vLLM [33] as shown in Table 8 . MoA still achieves a $1.4 \\times$ to $1.5 \\times$ increase in throughput. Notably, vLLM incorporates extensive system-level optimizations that are orthogonal to MoA, suggesting the potential for MoA to further improve. As vLLM constantly pre-allocates a 71 GB memory and automatically manages the actual batch size, we exclude memory metrics from the comparison. ## B.2.3 Automatic Optimization Overhead\n\nWe present a detailed breakdown of the time usage of MoA pipeline. Table 9 summarizes the time required for various crucial phases within the MoA framework, encompassing calibration dataset generation, profiling, optimization, and validation, on the Vicuna-13B model. Table 9: Time overhead for various stages of MoA. | Stage | Time |\n| :--- | :---: |\n| Calibration Dataset Generation | 15 min |\n| Profile | 25 min |\n| Optimize | 25 min |\n| Validate | 40 min |\n| Sum. | 105 min |\n\nTable 10: Performance comparison on various test sets, using different calibration sets. Tested on Vicuna-7B model. The result is tested with $50 \\%$ density MoA on LongBench [4] 0-4k split. | Dataset | Long Dep. \\& Align Model | Qasper | Test Score <br> MultiNews | TREC | Avg. Score |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| Original | NA | 28.6 | 28.2 | 56.0 | 37.6 |\n| RedPajama | $\\boldsymbol{X}$ | $20.6(-8.0)$ | $19.6(-8.6)$ | $\\mathbf{6 6 . 0}(+10.0)$ | $35.4(-2.2)$ |\n| Qasper | $\\checkmark$ | $25.6(-3.0)$ | $\\mathbf{2 7 . 8}(-0.4)$ | $55.0(-1.0)$ | $36.1(-1.5)$ |\n| MultiNews | $\\checkmark$ | $\\mathbf{2 9 . 0}(+0.4)$ | $27.5(-0.7)$ | $54.0(-2.0)$ | $\\mathbf{3 6 . 8}(-0.8)$ |\n| TREC | $\\checkmark$ | $27.3(-1.3)$ | $27.3(-0.9)$ | $55.0(-1.0)$ | $36.5(-1.1)$ |\n\nProfiling is the most resource-demanding part of our pipeline. For a 13 b model with an 8 k profile length, two A100 GPUs are required. In other cases, we only need one single GPU. Profiling on a 13 b model with an 8 k profile length and 50 data items takes 15 minutes. Profiling on 4 k and 2 k lengths takes less than 5 minutes each. On the $\\operatorname{Intel}(\\mathrm{R}) \\operatorname{Xeon}(\\mathrm{R})$ Platinum 83582.60 GHz CPU , the optimization concludes within approximately 25 minutes. Typically, this phase generates around 10 compression plans. Validating each one of the compression plans takes about 4 minutes, totaling around 40 minutes. ## B. 3 Ablation Study\n\n## B.3.1 Calibration Dataset\n\nIn this section, we validate the robustness of our calibration dataset design principles. We select three sub-tasks and respective datasets from the LongBench benchmark, including Qasper [15], MultiNews [17], and TREC [38, 28]. We use their training set to construct the calibration dataset, and use their respective test set in LongBench to calculate the score. Following Section 5. all calibration datasets are constructed using the original model's response to the context and questions as the supervision. As shown in Table 10, we find that as long as the calibration dataset conforms to the long-range dependency and model alignment highlighted in section 5, the specific choice of the dataset is less important. Calibration datasets with long dependency and model alignment show somewhat similar test results on various datasets. Additionally, they all show strong generalization power to test sets other than their respective calibration dataset. In contrast, the RedPajama dataset without long-range dependency and model alignment shows large variance on various test sets. It also differs from the performance of the original dense model, which may incur unexpected behaviors after compression. Note that though all datasets exhibit long dependency, the questions in the TREC dataset can be answered without long context. The context in the TREC dataset of LongBench is the many-shot examples, each showing a short sentence and its classification result, while the question is to classify a new short sentence. Although the context helps to determine the complete set of 50 classes, the model can also directly clarify the sentence without any context based on common knowledge. It may contribute to a high score on the TREC test set with the RedPajama calibration dataset. ## C Compression Plan Analysis\n\n## C. 1 Statistics on Rules Discovered by MoA\n\nThis subsection provides empirical evidence for rules discovered by MoA as mentioned in Section 6.4 The lines and spans in Figure 8 show that all heads at the first few layers generally need a high KV-Cache density.",
    "moa-36": "Following that, a few layers generally only require medium density. Then, in the final layers, most heads require low density, while some outlier heads need high density. This observation conforms to previous findings of the intrinsic dimension of LLM [61]. The geometry of density is similar to the intrinsic dimension of LLM, with two local minima. As observed in Figure 8 layers with lower average density (smaller values on the lines) typically display a wider range of\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-21.jpg?height=377&width=1309&top_left_y=261&top_left_x=387)\n\nFigure 8: The MoA mask density across layers for different LLMs. ![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-21.jpg?height=463&width=1400&top_left_y=728&top_left_x=361)\n\nFigure 9: The MoA mask's average density and the density range for each layer for different LLMs.",
    "moa-37": "density (wider shades). Figure 9 validates such observation. This observation confirms the need for heterogeneous attention rules within the same layer. ## C. 2 Connections Between MoA Rule and Semantic\n\nIn this section, we invest the masks acquired with MoA and show the interpretable semantics of the masks. Previous works manually restrict the attention pattern of the model, which may harm the semantics learned by the dense model. In contrast, MoA preserves the semantics with statistic analysis and optimization. We use visualization, human interpretation and quantitive methods to analyze the semantics of the original model and to verify whether MoA captures such semantics. ## C.2.1 Mask Visualization and Semantic Categorization\n\nGiven any token, two kinds of information are used as the model inputs: position encoding and token embedding. Position encoding indicates the absolute [74] or relative positions [59] of tokens in the sentence. Token embedding maps different tokens as different vectors. The attention head $h$ responds to both information and output the corresponding attention value $A_{h}$. As shown in equation 6e denote the influence of position and token of head $h$ as function $P_{h}$ and $T_{h}$, respectively. The attention value $A_{h, i, j}$ between the $i$ th and $j$ th token $t_{i}$ and $t_{j}$ is determined by the combination $f_{h}$ of position and token influence functions. $$\nA_{h, i, j}=\\mathbb{A}_{h}\\left(t_{i}, t_{j}, i, j\\right)=f_{h}\\left(P_{h}(i, j), T_{h}\\left(t_{i}, t_{j}\\right)\\right)\n$$\n\nFigure 2 visualizes two typical heads that are either dominated by position $P$ or token $T$ function. For the first attention head in Figure 2, the local positional attention is clearly observed. In this head, whatever sentences are given, each token pays major attention to the first token and the prior token. As a result, the mean attention matrix accumulates extremely large attention values at the first column and the sub-diagonal. In contrast, the second attention head in Figure 2 lays more emphasis on content-based attention. Since the position distribution of important tokens are generally random, the attention matrix can show large attention values at any position. It results in a mean attention matrix without extreme mean attention values. In conclusion, the mean attention matrix of different sentences provides a valuable insight of whether attention values of an attention head is more position-based or content-based. Intuitively, the more uneven the attention matrix value distribution is, the more position-based the head is. ## C.2.2 Quantitative Semantic Analysis\n\nWe quantify how much the attention head is position-based and analyze whether MoA successfully utilizes such semantics through the evaluate-generate-optimization pipeline. We model equation 6 with a linear approximation. $P_{h}$ and $T_{h}$ are random variables with the same expectation $\\mu$ and standard variance $\\delta$ for all heads. For attention head $h$, the weight factor $\\alpha_{h}$ evaluates the relatively influence of position and token to the final attention value. $$\nA_{h, i, j}=\\alpha_{h} P_{h}(i, j)+\\left(1-\\alpha_{h}\\right) T_{h}\\left(t_{i}, t_{j}\\right)\n$$\n\nGiven the randomness of token positions in long context, we assume that the token position and its content are irrelevant. For different sentences $s$, the expectation $\\mathbb{E}_{t}$ of the attention value between position $i$ and $j$ can be expressed as follows. Note that it excludes the matrix diagonal since $T_{h}\\left(t_{i}, t_{j}\\right), i \\neq j$ and $T_{h}\\left(t_{i}, t_{i}\\right)$ may follow different distributions. $$\n\\begin{aligned}\n\\mathbb{E}_{t}\\left[A_{h, i, j}\\right] & =\\frac{1}{S} \\sum_{s=1}^{S}\\left(\\alpha_{h} P_{h}(i, j)+\\left(1-\\alpha_{h}\\right) T_{h}\\left(t_{i}^{(s)}, t_{j}^{(s)}\\right)\\right) \\\\\n& =\\alpha_{h} P_{h}(i, j)+\\left(1-\\alpha_{h}\\right) \\frac{1}{S} \\sum_{s=1}^{S} T_{h}\\left(t_{i}^{(s)}, t_{j}^{(s)}\\right) \\\\\n& =\\alpha_{h} P_{h}(i, j)+\\left(1-\\alpha_{h}\\right) \\mu_{T}, \\forall i>j\n\\end{aligned}\n$$\n\nThe standard division $\\sigma_{p}$ of $\\mathbb{E}_{t}$ over different positions of the attention matrix is\n\n$$\n\\begin{aligned}\n\\sigma_{p}\\left(\\mathbb{E}_{t}\\left[A_{h, i, j}\\right)\\right. & =\\sqrt{\\frac{2}{(1+N) N} \\sum_{i, j \\in[1, N), i>j}\\left[\\left(\\alpha_{h} P_{h}(i, j)+\\left(1-\\alpha_{h}\\right) \\mu_{T}\\right)-\\left(\\alpha_{h} \\mu_{P}+\\left(1-\\alpha_{h}\\right) \\mu_{T}\\right)\\right]^{2}} \\\\\n& =\\alpha_{h} \\delta_{p}\n\\end{aligned}\n$$\n\nWe name $\\sigma_{p}\\left(\\mathbb{E}_{t}\\left[A_{h, i, j}\\right]\\right)$ the Standard division of Expectation (SoE) of head $h$. Note that the expectation is taken over different sentences, while the standard division is taken over different attention positions. Since $\\delta_{p}$ is the same for all heads, we derive that the position impact $\\alpha_{h}$ is proportional to the SoE of different heads. The conclusion quantifies the observation stated in Section C.2.1. Intuitively, SoE shows how uneven the mean attention matrix is, thus showing the influence of position to the attention values. MoA's generated mask density shows positive relation with SoE, suggesting that MoA successfully captures the semantic information of the dense language model as shown in Figure 10\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-22.jpg?height=413&width=440&top_left_y=1967&top_left_x=1211)\n\nFigure 10: Positive correlation between MoA's mask sparsity and head's dependency on position (SoE). ## D MoA Automatic Pipeline Details\n\n## D. 1 Derivation of Attention Influence\n\nWe use the first-order Taylor expansion to calculate the influence of each attention value. This approximation approach is supported by methodologies commonly employed in other LLM compression approaches [36, 55, 14, 29]. As discussed in Section 4.1, when masking out attention value $A_{h, i, j}$ at head $h$, row $i$, and column $j$, it also influences the attention values in the same row by $\\Delta A_{h, i, n \\mid j}$. $$\n\\begin{aligned}\nA_{h, i, n} & =\\frac{e^{S_{h, i, n}}}{\\sum_{j} e^{S_{h, i, j}}} \\\\\n\\Delta A_{h, i, n \\mid j} & = \\begin{cases}-A_{h, i, n}, & n=j \\\\\nA_{h, i, n}\\left(\\sum_{j} e^{S_{h, i, j}} / \\sum_{j \\neq n} e^{S_{h, i, j}}-1\\right), & n \\neq j\\end{cases}\n\\end{aligned}\n$$\n\nFollowing the definition, the attention influence $\\mathbf{E}_{h}$ is calculated as follows:\n\n$$\nE_{h, i, j}=\\sum_{n} \\frac{\\partial L}{\\partial A_{h, i, n}} \\cdot \\Delta A_{h, i, n \\mid j}\n$$\n\nGiven Equation 11 and 10, we derive Equation 3 as follows. For notation simplicity, we omit the head index $h$ here. $$\n\\begin{aligned}\n& E_{i, j}=\\sum_{n} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot \\Delta A_{i, n \\mid j} \\\\\n& =\\frac{\\partial L}{\\partial A_{i, j}} \\cdot\\left(-A_{i, j}\\right)+\\sum_{n \\neq j} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot A_{i, n} \\cdot\\left(\\frac{\\sum_{k} e^{S_{i, k}}}{\\sum_{k \\neq j} e^{S_{i, k}}}-1\\right) \\\\\n& =\\frac{\\partial L}{\\partial A_{i, j}} \\cdot\\left(-A_{i, j}\\right)+\\sum_{n \\neq j} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot A_{i, n} \\cdot \\frac{e^{S_{i, j}}}{\\sum_{k} e^{S_{i, k}}-e^{S_{i, j}}} \\\\\n& =\\frac{\\partial L}{\\partial A_{i, j}} \\cdot\\left(-A_{i, j}\\right)+\\sum_{n \\neq j} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot A_{i, n} \\cdot \\frac{e^{S_{i, j}} / \\sum_{k} e^{S_{i, k}}}{1-e^{S_{i, j}} / \\sum_{k} e^{S_{i, k}}} \\\\\n& =\\frac{\\partial L}{\\partial A_{i, j}} \\cdot\\left(-A_{i, j}\\right)+\\sum_{n \\neq j} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot A_{i, n} \\cdot \\frac{A_{i, j}}{1-A_{i, j}} \\\\\n& =\\frac{\\partial L}{\\partial A_{i, j}} \\cdot\\left(-A_{i, j}\\right)-\\frac{\\partial L}{\\partial A_{i, j}} \\cdot A_{i, j} \\cdot \\frac{A_{i, j}}{1-A_{i, j}}+\\sum_{n} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot A_{i, n} \\cdot \\frac{A_{i, j}}{1-A_{i, j}} \\\\\n& =\\frac{\\partial L}{\\partial A_{i, j}} \\cdot\\left(-\\frac{A_{i, j}}{1-A_{i, j}}\\right)+\\frac{A_{i, j}}{1-A_{i, j}} \\cdot \\sum_{n} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot A_{i, n} \\\\\n& =-\\frac{A_{i, j}}{1-A_{i, j}}\\left(\\frac{\\partial L}{\\partial A_{i, j}}-\\sum_{n} \\frac{\\partial L}{\\partial A_{i, n}} \\cdot A_{i, n}\\right)\n\\end{aligned}\n$$\n\nIt is worth noting to mention that it can also be formulated as matrix multiplications:\n\n$$\n\\mathbf{E}_{h}=\\frac{\\mathbf{A}_{h}}{1-\\mathbf{A}_{h}} \\cdot\\left(\\frac{\\partial L}{\\partial \\mathbf{A}_{h}}-\\left(\\frac{\\partial L}{\\partial \\mathbf{A}_{h}} \\cdot \\mathbf{A}_{h}\\right) \\mathbb{1}^{N \\times N}\\right)\n$$\n\n## D. 2 Optimization Details\n\n## D.2. 1 Optimizing at Single Length\n\nThe optimization problem is formulated as follows:\n\n$$\n\\arg \\min \\Delta L=\\sum_{h} \\Delta L_{h, r_{h}}, \\quad \\text { s.t. } \\frac{1}{H} \\sum_{h} d_{r_{h}} \\leq d_{\\text {constr }}\n$$\n\nTo transform the optimization problem into a standard Mixed-Integer Programming (MIP) framework, we introduce the binary variable $X_{h, r_{h}} \\in\\{0,1\\}$. It indicates whether to select rule $r_{h}$ for the attention head $h$. Assume the model has $H$ attention head, and head $h$ has $R_{h}$ elastic rules. $$\n\\begin{aligned}\n& \\arg \\min \\frac{1}{H} \\sum_{h=0}^{H-1} \\sum_{r_{h}=0}^{R_{h}-1} \\Delta L_{h, r_{h}} X_{h, r_{h}} \\quad \\text { s.t. } \\\\\n& \\sum_{r_{h}=0}^{R_{h}-1} X_{h, r_{h}}=1, \\quad h \\in\\{0, \\cdots, H-1\\} \\\\\n& \\frac{1}{H} \\sum_{h=0}^{H-1} \\sum_{r_{h}=0}^{R_{h}-1} d_{r_{h}} X_{h, r_{h}} \\leq d_{\\text {constr }} \\\\\n& \\quad 0 \\leq X_{h, r_{h}} \\leq 1, X_{h, r_{h}} \\in \\mathbb{Z}, \\quad \\forall h \\in\\{0, \\cdots, H-1\\}, \\forall r_{h} \\in \\mathbb{R}\n\\end{aligned}\n$$\n\nIn this formulation, 15a serves as the objective function to minimize the loss, subject to the constraints that each matrix selects exactly one compression plan 15 b , and the average density does not exceed $d_{\\text {constr }} 15 \\mathrm{c}$. Finally, 15d enforces that $X_{h, r_{h}}$ is a binary variable, indicating the selection of plans. Additionally, to enforce the restriction that each model layer only has a limited number of different plans, we bound the norm of element-wise multiplication of $\\mathbf{X}_{h}=\\left[\\begin{array}{llll}X_{h, 0} & X_{h, 1} & \\cdots & X_{h, R_{h}-1}\\end{array}\\right]^{\\top}$ in a single layer. ## D.2.2 Optimizing at Multiple Lengths\n\nWith the ability to optimize at a single length, we utilize the same framework for multi-objective MIP across various lengths. The key is to transform the multi-objective MIP problem into several single-objective MIP problems [47]. We utilize the idea of epsilon-constraint method [72]. We first select each input length as our primary objective to perform the single-objective optimization on it while simultaneously recording the outcomes of other objectives. Specifically, for $N$ distinct objectives, we do single-objective MIP optimization on the $i$-th objective, getting minimum loss $\\Delta L_{i}^{\\left(N_{i}\\right)}$, and we concurrently collect losses of other objectives $\\Delta L_{i}^{\\left(N_{j}\\right)}$ for $j \\neq i$. This process allows us to establish the range of loss $R^{\\left(N_{j}\\right)}=\\left[\\min _{i} \\Delta L_{i}^{\\left(N_{j}\\right)}, \\max _{i} \\Delta L_{i}^{\\left(N_{j}\\right)}\\right]$ for each objective. Then, we iterate through each objective again. Compared with the original multi-objective optimization in Equation 5, we now consider other objectives as constraints. To implement this, we partition each loss range $R^{\\left(N_{j}\\right)}$ of other objectives $j \\neq i$ into $M$ uniform intervals $S_{k}^{\\left(N_{j}\\right)}$, where $0 \\leq k<M$. We then solve the MIP problems for each objective $i$ and iterating through the constraint intervals:\n\n$$\n\\underset{r_{h} \\in \\mathbb{R}}{\\arg \\min } \\Delta L^{\\left(N_{i}\\right)} \\quad \\text { s.t. } \\frac{1}{H} \\sum_{h=1}^{H} d_{r_{h}}^{\\left(N_{i}\\right)} \\leq d_{\\text {constr }}^{\\left(N_{i}\\right)}, \\forall N_{i} \\in \\mathbb{N}_{\\text {constr }} ; \\quad \\Delta L^{\\left(N_{j}\\right)} \\in S_{k_{j}}^{\\left(N_{j}\\right)}, \\forall j \\neq i\n$$\n\nwhere this optimization is performed for each $i$ ranging from 0 to $N$. For each $j, k_{j}$ can vary independently from 0 to $M$. For efficiency consideration, we set the number of intervals as five. Finally, the results that do not conform to the Pareto front requirements are removed, resulting in the final Pareto front set of our multi-objective optimization problem. [^0]:    *Equal contribution. Genghan Zhang made contributions while at Tsinghua University\n    Corresponding authors: Yu Wang (yu-wang@tsinghua.edu.cn), Xuefei Ning (foxdoraame@ gmail.com)\n\n"
}