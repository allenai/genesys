{
    "theoryssms-0": "# Theoretical Foundations of Deep Selective State-Space Models \n\nNicola Muca Cirone ${ }^{1}$ Antonio Orvieto ${ }^{2}$ Benjamin Walker ${ }^{3}$ Cristopher Salvi ${ }^{1}$ Terry Lyons ${ }^{3}$\n\n\n#### Abstract\n\nStructured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data.",
    "theoryssms-1": "Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input - capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants. ## 1. Introduction\n\nSequence-to-sequence blocks are fundamental components of modern deep learning models for language, images, video, audio, time series, and genomics. While attention (Vaswani et al., 2017, Dosovitskiy et al., 2020) has been for the last five years the dominant mechanism powering these architectures, competitive results have been recently achieved by replacing attention with state-space models (SSMs): GPU-efficient linear recurrent sequence-tosequence blocks stemming from S4 (Gu et al., 2021). SSMs\n\n[^0]Preprint, under review. achieve state-of-the-art results on long-range-reasoning benchmarks (Tay et al. 2020) and show outstanding performance in various domain including vision (Nguyen et al. 2022), audio (Goel et al., 2022), biological signals (Gu et al. 2021), reinforcement learning (Lu et al, 2023) and online learning (Zucchet et al. 2023). SSMs recently have gained significant interest in the community since their computational complexity scales linearly in sequence length, while attention scales quadratically; moreover, unlike other recurrent mechanisms such as LSTMs Hochreiter \\& Schmidhuber, 1997) and GRUs (Cho et al., 2014), they can be efficiently parallelized on GPUs during training using parallel scans (Martin \\& Cundy, 2017; Smith et al., 2023). While standard SSMs were shown to be particularly powerful on signal processing tasks, their computation power is limited: the core sequential mechanism of S4 is equivalent to a convolution (filtering) (Li et al., 2022a). This represents a drawback in challenging domains such as text and genetics, where the ability to select data efficiently in an inputdependent manner (i.e., perform content-based reasoning) is crucial (see results in (Wang et al., 2022, Fu et al., 2022)). Towards reaching this goal with recurrent models, various adaptations of S 4 have been proposed in the last few months. Notably, Mamba (Gu \\& Dao, 2023) implements simple and efficient gating mechanisms on the $S 4$ recurrence, unlocking input selectivity in the memory update. Mamba achieved state-of-the-art performance of various language modeling tasks at a fraction of the compute needed to train a transformer model with an equal number of parameters, with inference time still scaling (like S 4 ) with $O$ (sequence length). Similar ideas can be found in recent S 4 -inspired recurrent linear attention models, such as RWKV (Peng et al., 2023), RetNet (Sun et al., 2023), Gateloop (Katsch, 2023) and Gated Linear Attention (GLA) Yang et al., 2023). Last, very recently, (De et al., 2024) surpassed the performance of Mamba with a gated RNN architecture - Griffin - based on an improved version of the LRU (Orvieto et al. 2023b), perhaps the simplest SSM variant. At the core of the models discussed above is a linear timevarying dynamical system, where content-based reasoning is performed along the input sequence through an efficient and parallelizable update on a hidden state. In this paper, we generalize the structure of such models, drawing a direct link to controlled differential equations (CDEs) (Kidger et al. 2020, Morrill et al., 2021; Arribas et al., 2020, Salvi et al. 2022; Hoglund et al., 2023) and using tools from rough path theory to study expressivity of deep selective SSMs. 1. We provide a framework for the analysis of (inputcontrolled) linear (in the hidden state) recurrences such as S4, Mamba and GLA. This framework allows the use of powerful tools and results in the Rough Path Theory literature to reason about the expressivity of modern SSM layers and to obtain generalized versions of the universality results of (Li et al., 2022b). This framework captures both S 4 and Mamba, as well as recently proposed linear-attention-powered recurrences. 2. We prove that wide enough randomly initialized dense input-controlled linear recurrences are fully expressive: with no need for a multi-layer-perceptron (MLP) block transforming pointwise the recurrence output (as instead is done in the transformers architecture or at the S 4 recurrence output), we show that the hidden state contains sufficient input statistics to approximate any continuous function from the input sequence to a target value. This is in clear contrast to $S 4$, where the hidden state is just a convolution of the input sequence with a fixed kernel. 3. We show that diagonal input-controlled linear recurrences (such as Mamba) provably collect input statistics more efficiently than S4. We additionally show that chaining such blocks, by simply placing linear pointwise maps in between, allows computation of higher order global statistics - matching the dense linear inputcontrolled setting discussed in point (2) above, in the depth limit. The main objective of our research is to outline the foundational mathematical tools for comprehensive theoretical investigations of modern recurrent blocks. In addition to our results and insights concerning recently proposed SSM models, our work provides a framework that can be useful for analyzing and comparing forthcoming architectural advances. ## 2. State-space Models\n\nWe describe here the structure of the main SSMs-based strategies for processing length- $L$ input sequences of $d$ dimensional tokens: $x \\in \\mathbb{R}^{d \\times L}$. We denote by $x_{\\ell}$ the $\\ell$-th column of $x$ (the $\\ell$-th token) and by $x^{i}$ the $i$-th row of $x$ (time series for the $i$-th channel). ### 2.1. Review of Modern SSMs\n\nWe start with a quick simplified recap of S4 (Gu et al., 2021) and then describe recently proposed variants such as Mamba (in particular, the S6 block) (Gu \\& Dao, 2023) and Gated Linear Attention (GLA) (Yang et al., 2023). We restrict our focus to the recurrent mechanism and invite the reader to refer to the original papers for a description of the token-wise operations following and preceding each block. SSM basics (S4). Mos ${ }^{1}$ SSMs (Gu et al., 2021, 2022) operate independently on input channels. The time-series for each channel $x^{i} \\in \\mathbb{R}^{L}$ is seen as the result of sampling a latent continuous-time signal $X^{i}:[0,1] \\rightarrow \\mathbb{R}$ at multiples of a channel-dependent stepsize $\\Delta_{i}>0$ :\n\n$$\nX_{\\Delta_{i} \\ell}^{i}:=X^{i}\\left(\\Delta_{i} \\ell\\right)=x_{\\ell}^{i}\n$$\n\nfor all $\\ell \\in[L], i \\in[d]$. Under S 4 , each input signal $X^{i}$ produces a complex-valued state signal $Z^{i}:[0,1] \\rightarrow \\mathbb{C}^{N}$ according to a linear differential equation:\n\n$$\n\\frac{d}{d t} Z_{t}^{i}=A_{i} Z_{t}^{i}+B X_{t}^{i}\n$$\n\nwhere $A_{i}=\\operatorname{diag}\\left(a_{i, 1}, a_{i, 2}, \\ldots a_{i, N}\\right)$ is channel-specific diagonal $N \\times N$ complex valued matrix: $a_{i, n} \\in \\mathbb{C}$ for all $(i, n) \\in[d] \\times[N] . B \\in \\mathbb{C}^{N}$ is an input projection shared across input components $i \\in[d]$. SSM models are based on stable discretizations of the continuous time system above: each input sequence channel $x^{i}$ produces a sequence of hidden states $z^{i}=\\left(z_{1}^{i}, z_{2}^{i}, \\ldots, z_{L}^{i}\\right) \\in \\mathbb{R}^{N \\times L}$ as follows:\n\n$$\nz_{\\ell}^{i}=\\bar{A}_{i} z_{\\ell-1}^{i}+\\bar{B}_{i} x_{\\ell}^{i}\n$$\n\nwith $\\bar{A}_{i}, \\bar{B}_{i}$ determined by the discretization technique and the channel-dependent stepsize $\\Delta_{i}$ : under the commonly used Zero-Order Hold discretization 2\n\n$$\n\\bar{A}_{i}=\\exp \\left(\\Delta_{i} A_{i}\\right), \\bar{B}_{i}=\\left(\\Delta_{i} A_{i}\\right)^{-1}\\left(\\exp \\left(\\Delta_{i} A_{i}\\right)-I\\right) \\Delta_{i} B\n$$\n\nNote from eq. 2 that SSMs at inference time are equivalent to linear recurrent neural networks (RNNs). Yet, learning with gradient descent is performed on the continuous-time variables, unlocking stable signal propagation and alleviating vanishing gradients (Orvieto et al., 2023b). Finally, at each dimension $i$, the sequence of hidden states is mapped back to real numbers, and a projection is performed to produce an output a sequence of tokens $y \\in \\mathbb{R}^{d \\times L}$ with the same dimensions as $x$. To conclude, we point out that the transition matrix $A$ is often (yet not crucially, as shown by Orvieto et al. (2023b)) structured, i.e. initialized deterministically through HiPPO theory (Gu et al., 2020) in diagonal form. Common choices (Gu et al., 2022) are $a_{\\cdot, n}=-\\frac{1}{2}+\\mathrm{i} \\pi n$ (S4D-Lin ${ }^{3}$ ) and $a_{\\cdot, n}=-\\frac{1}{2}$ (S4D-Real). [^1]Mamba. The Selective SSM (S6) powering the Mamba architecture (Gu \\& Dao, 2023) augments S4 with inputcontrolled transitions:\n\n$$\nz_{\\ell}^{i}=\\bar{A}_{i}\\left(x_{\\ell}^{i}\\right) z_{\\ell-1}^{i}+\\bar{B}\\left(x_{\\ell}^{i}\\right) x_{\\ell}^{i}\n$$\n\nwhere the most crucial component (see Table 7 in Gu \\& Dao (2023)) is the dependency of $\\bar{A}_{i}$ on the input - thus making the operation $\\bar{A}_{i}\\left(x_{\\ell}^{i}\\right) z_{\\ell-1}^{i}$ effectively a gate. This dependency of $\\bar{A}_{i}$ on the input is achieved efficiently through the $\\Delta_{i}$ parameter, which becomes iteration-dependent following the equation $\\sqrt{4}$\n\n$$\n\\Delta_{\\ell}^{i}=\\operatorname{softplus}\\left(\\alpha_{i} x_{\\ell}^{i}+\\beta_{i}\\right)\n$$\n\nwith $\\alpha_{i}, \\beta_{i} \\in \\mathbb{R}$. The RG-LRU recurrence in (De et al. 2024) works in a similar way, yet processing all input channels at once. GLA and other linear attention variants. Gated Linear attention ${ }^{5}$ Yang et al. 2023 is a recursion on a matrix hidden state $z \\in \\mathbb{R}^{N \\times N}$ with input-controlled transition:\n\n$$\nz_{\\ell}=\\bar{A}\\left(x_{\\ell}\\right) \\odot z_{\\ell-1}+\\bar{B}\\left(x_{\\ell}\\right)\n$$\n\nwhere $\\odot$ denotes the Hadamard product and all channels in the input sequence are fed into the transition through a linear attention mechanism, and $A \\in \\mathbb{R}^{N \\times N}$ is computed through the equation:\n\n$$\n\\begin{aligned}\n\\bar{B}\\left(x_{\\ell}\\right) & =W_{\\mathrm{key}}^{\\top} x_{\\ell} x_{\\ell}^{\\top} W_{\\mathrm{val}} \\in \\mathbb{R}^{d \\times d} \\\\\n\\bar{A}\\left(x_{\\ell}\\right) & =\\tau^{-2} \\sigma\\left(x_{\\ell}^{\\top} W_{\\alpha}+b_{\\alpha}\\right)^{\\top} \\sigma\\left(x_{\\ell}^{\\top} W_{\\beta}+b_{\\beta}\\right) \\in \\mathbb{R}^{d \\times d}\n\\end{aligned}\n$$\n\nwith $W_{\\text {key }}, W_{\\text {val }}, W_{\\alpha}, W_{\\beta} \\in \\mathbb{R}^{d \\times d}, b_{\\alpha}, b_{\\beta} \\in \\mathbb{R}^{d}, \\tau \\geq 0$ and $\\sigma$ is a sigmoid. RetNet (Sun et al. 2023) is similar on $\\bar{B}$ but does not have input-controlled $A$. GateLoop (Katsch, 2023) is similar to RetNet, with $\\bar{A}$ diagonal and complex-valued: phase and magnitude of each component is controlled via a linear map on the input, similarly to Mamba. ### 2.2. Known properties of (non-linear) recurrences\n\nExpressiveness of standard nonlinear RNNs of the form $z_{\\ell}=A \\sigma\\left(z_{\\ell-1}\\right)+B x_{\\ell}$, where $\\sigma$ is a nonlinearity, has been extensively studied since the seminal work of Siegelmann \\& Sontag (1992), with recent contributions such as (Korsky \\& Berwick, 2019, Hanson \\& Raginsky, 2020). In particular, Hanson \\& Raginsky (2020) proved in the continuous-time setting that wide enough non-linear RNNs can approximate up to vanishing precision non-linear timehomogeneous systems of differential equations, driven by input paths. The argument used here to show universality is based on the celebrated Barron's theorem (Barron, 1993) for\n\n[^2]approximation of continuous functions with neural networks with one hidden layer. Indeed, note that non-linear RNNs are recurrent perceptrons with one hidden layer, acting both on the state and the input (Tallec \\& Ollivier 2018). Instead, (selective) SSMs such as S4 and Mamba have transition map which is linear in the state - unlocking parallelization of the recurrent computation (Smith et al. 2023, Gu \\& Dao, 2023). In the context of linear RNNs and non-selective SSMs, many results (old and new) exist that characterize expressivity. Li et al. (2022b) showed that linear RNNs (i.e. S4-like recurrences) can approximate arbitrary convolution filters in the width limit. Further, Hanson \\& Raginsky (2020) proved that stacking exponentially (in the sequence length) many temporal convolution filters, chained together with ReLU activations, leads to approximation of arbitrary non-linear filters. Recent works (Orvieto et al., 2023a, Wang \\& Xue, 2023) prove universality of linear recurrences (one layer) when equipped with a fixed (timestamp independent) point-wise MLP acting across the recurrence output, with intriguing connections to Volterra series (Boyd \\& Chua, 1985). Mamba (alongside with gated linear attention variants e.g. Yang et al. (2023)) does not fall in the linear RNN or the nonlinear RNN setting: its recurrence is linear on the hidden state (hence it can be parallelized) but it is not linear time-invariant as S4. In this paper, we study this setting. ## 3. SSMs as Linear CDEs\n\nAs shown by Gu \\& Dao (2023), the crucial component that unlocks in-context learning and selectivity in modern SSMs is the input-dependent state-to-state transition matrix $A$, gating the hidden state and thus allowing the system to filter out irrelevant information and remember relevant information indefinitely. As we will shortly see, one can study the structure and features of such systems within a unified convenient continuous-time framework (Linear Controlled Differential equations). We define such framework (Def. 6) in full generality, and draw the connection to S 4 and Mamba. ### 3.1. Linear CDEs\n\nLet us consider the separable Banach space $C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$ of absolutely continuous $\\mathbb{R}^{d}$-valued paths on $[0,1]$ and starting at the origin $\\square^{6}$\n\n$$\n\\left\\{\\omega: t \\mapsto \\int_{0}^{t} \\dot{\\omega}_{s} d s \\mid \\dot{\\omega} \\in L^{1}\\left([0,1] ; \\mathbb{R}^{d}\\right)\\right\\}\n$$\n\nendowed with the 1-variation norm\n\n$$\n\\|\\omega\\|_{1 ;[0,1]}:=\\int_{0}^{1}\\left|\\dot{\\omega}_{s}\\right| d s\n$$\n\nLet $\\mathbb{X}$ of be some topological vector space where the input data lies. The prototypical example of $\\mathbb{X}$ considered in\n\n[^3]the SSM literature is the set of sequences of finite length. Modern selective (or gated) SSMs such as Mamba do not work directly with raw input data. Instead, they introduce continuous embeddings\n$$\n\\omega: \\mathbb{X} \\rightarrow C_{1,0}\\left([0,1] ; \\mathbb{R}^{d_{\\omega}}\\right), \\quad \\xi: \\mathbb{X} \\rightarrow C_{1,0}\\left([0,1] ; \\mathbb{R}^{d_{\\xi}}\\right)\n$$\nusually referred to as gates. After the data $X \\in \\mathbb{X}$ is embedded into continuous paths $\\omega^{\\mathrm{X}}, \\xi^{\\mathrm{X}}$ by means of gates $\\omega, \\xi$ respectively, we define our model as a linear controlled differential equation (CDE) driven by these embeddings: fix $N \\in \\mathbb{N}$, and matrices $A_{1}, \\ldots, A_{d_{\\omega}} \\in \\mathbb{R}^{N \\times N}$, and $B \\in \\mathbb{R}^{N \\times d_{\\xi}}$. The model we consider in this paper is the following Linear $\\mathrm{CDE}_{\\square}^{7}$\n\\[\n\n$$\n\\begin{gathered}\nZ: \\mathbb{X} \\rightarrow C_{1}\\left([0,1] ; \\mathbb{R}^{N}\\right) \\\\\nd Z_{t}^{\\mathrm{X}}=\\sum_{i=1}^{d_{\\omega}} A_{i} Z_{t}^{\\mathrm{X}} d \\omega_{t}^{\\mathrm{X}, i}+B d \\xi_{t}^{\\mathrm{X}}, \\quad Z_{0}^{\\mathrm{X}} \\in \\mathbb{R}^{N}\n\\end{gathered}\n$$\n\\]\n\nwhere $t \\mapsto \\omega_{t}^{\\mathrm{X}, i}$ is the $i$-th channel of the path $\\omega^{\\mathrm{X}}$. We show that both S4 and Mamba can be written in continuous-time as Linear CDEs, for difference choices of $\\omega$ and $\\xi$.",
    "theoryssms-2": "We describe this visually in Fig. 1. This motivates us to study the expressivity of the model in Eq. 6 and how it is affected by the choice of $\\omega$ and $\\xi$. S4 is a Linear CDE. The S 4 model (Gu et al., 2021) is a Linear CDE of the form (6) with \"gate\" functions\n\n$$\n\\omega_{t}^{\\mathrm{X}}=t, \\quad \\xi_{t}^{\\mathrm{X}}=\\int_{0}^{t} X_{s} d s\n$$\n\n### 3.2. Mamba is a Linear CDE\n\nLet us revisit Eqn.",
    "theoryssms-3": "(3), dropping the channel index $m$ for convenience. We introduce a new parameter $\\delta>0$ and consider the following reparametrization: $\\alpha \\leftarrow \\delta \\tilde{\\alpha}, \\beta \\leftarrow$ $\\delta \\tilde{\\beta}$. Let us further approximate the softplus function with a $\\operatorname{ReLU}: \\sigma=\\operatorname{ReLU}(x) \\simeq \\log \\left(1+e^{x}\\right)$.",
    "theoryssms-4": "Eqn. (4) becomes\n\n$$\n\\Delta_{\\ell}=\\sigma\\left(\\delta \\tilde{\\alpha} x_{\\ell}+\\delta \\tilde{\\beta}\\right)=\\sigma\\left(\\tilde{\\alpha} x_{\\ell}+\\tilde{\\beta}\\right) \\delta\n$$\n\nAs shown Appendix F, Mamba in Eqn. (3) is equivalent to the following Linear CDE\n\n$$\nd Z_{t}^{\\mathrm{X}}=A Z_{t}^{\\mathrm{x}} d \\omega_{t}^{\\mathrm{x}}+B d \\xi_{t}^{\\mathrm{X}}\n$$\n\nwhere we defined $X_{t}$ piecewise constant, with values the $x_{l}$, on a grid of stepsize $\\delta$ and\n\n$$\n\\begin{aligned}\n& d \\omega_{t}^{\\mathrm{X}}=\\sigma\\left(\\tilde{\\alpha} X_{t}+\\tilde{\\beta}\\right) d t \\\\\n& d \\xi_{t}^{\\mathrm{X}}=\\sigma\\left(\\tilde{\\alpha} X_{t}+\\tilde{\\beta}\\right) X_{t} d t\n\\end{aligned}\n$$\n\n[^4]As a quick validation, note that Eqn. (3) becomes\n\n$$\nz_{\\ell}=\\exp \\left(\\sigma\\left(\\tilde{\\alpha} x_{\\ell}+\\tilde{\\beta}\\right) \\delta A\\right) z_{\\ell-1}+\\sigma\\left(\\tilde{\\alpha} x_{\\ell}+\\tilde{\\beta}\\right) \\delta B x_{\\ell}\n$$\n\nAs $\\delta \\rightarrow 0$, the exponential becomes linear and one has\n\n$$\nz_{\\ell} \\stackrel{\\delta \\rightarrow 0}{=} z_{\\ell-1}+\\sigma\\left(\\tilde{\\alpha} x_{\\ell}+\\tilde{\\beta}\\right) \\delta A z_{\\ell-1}+\\sigma\\left(\\tilde{\\alpha} x_{\\ell}+\\tilde{\\beta}\\right) \\delta B x_{\\ell}\n$$\n\nWhich is the standard forward Euler discretization of the Mamba ODE in Eqn. 88. ![](https://cdn.mathpix.com/cropped/2024_09_17_84ab3f26c8b5e128d602g-04.jpg?height=865&width=801&top_left_y=670&top_left_x=1071)\n\nFigure 1. Example of the dynamics of the S 4 and Mamba CDEs derived in Sec. 3.1 The continuous-time approximation of these algorithms can be written as a Linear CDE. In $\\mathrm{S} 4, \\omega_{t_{\\tilde{\\beta}}}^{\\mathrm{X}}=t$, while $\\xi_{t}^{\\mathrm{X}}=\\int_{0}^{t} X_{s} d s$. In Mamba, $\\omega_{t}^{\\mathrm{X}}=\\int_{0}^{t} \\sigma\\left(\\tilde{\\alpha} X_{s}+\\tilde{\\beta}\\right) d s$, while $\\xi_{t}^{\\mathrm{X}}=\\int_{0}^{t} \\sigma\\left(\\tilde{\\alpha} X_{s}+\\tilde{\\beta}\\right) X_{s} d s$. As we will see in Sec. 4 the input dependency of $\\omega_{t}^{\\mathrm{X}}$ allows Mamba to provably go beyond input filtering (S4), achieving higher reasoning power compared to S4: here, the hidden state of Mamba (one-dimensional) is updated only if the input signal is bigger than one in value.",
    "theoryssms-5": "This is an example of input selectivity, see discussion in Gu \\& Dao 2023. ## 4. Expressivity of Linear CDEs\n\nHaving established the connection between SSMs and Linear CDEs, we now provide an explicit characterisation of their uniform closure, i.e. a description of all the functions from compact subsets of $\\mathbb{X}$ to $\\mathbb{R}$ that can be uniformly approximated at an arbitrary precision by a Linear CDE of the form given in Eqn.",
    "theoryssms-6": "(6). ### 4.1. Characterization of the closure\n\nFor any path $\\omega \\in C_{1,0}\\left([0,1], \\mathbb{R}^{d_{\\omega}}\\right)$ and any sub-interval $[s, t] \\subset[0,1]$, we denote by $\\omega_{[s, t]} \\in C_{1,0}\\left([0,1], \\mathbb{R}^{d_{\\omega}}\\right)$ the\nfollowing path\n\n$$\n\\omega_{[s, t]}(u)= \\begin{cases}0 & \\text { if } u<s \\\\ \\omega_{u}-\\omega_{s} & \\text { if } u \\in[s, t] \\\\ \\omega_{t}-\\omega_{s} & \\text { if } u>t\\end{cases}\n$$\n\nTheorem 4.1. Consider continuous gates $\\omega, \\xi$ such that $\\omega_{t}^{X, 1}=t$ and $\\omega_{t}^{X, 2}=t^{2}{ }^{8}$ Let $\\mathbb{K} \\subset \\mathbb{X}$ be a compact subset. Then, the uniform closure of the Linear CDE model in (6) over $\\mathbb{K} \\times[0,1]$ is given by the following set of functionals\n\n$$\n\\left\\{(X, t) \\mapsto \\Psi\\left(\\omega_{[0, t]}^{X}\\right)+\\int_{0}^{t} \\Phi\\left(\\omega_{[s, t]}^{X}\\right) \\cdot d \\xi_{s}^{X}\\right\\}\n$$\n\nwhere $\\cdot$ is the scalar product on $\\mathbb{R}^{d_{\\xi}}$, and where\n$\\Psi: C_{1,0}\\left([0,1], \\mathbb{R}^{d_{\\omega}}\\right) \\rightarrow \\mathbb{R}, \\quad \\Phi: C_{1,0}\\left([0,1], \\mathbb{R}^{d_{\\xi}}\\right) \\rightarrow \\mathbb{R}^{d_{\\xi}}$ are generic continuous functions. If the matrices $A_{1}, \\ldots, A_{d_{\\omega}}$ are constrained to be diagonal, as it is often the case in practice, the requirements $\\omega_{t}^{X, 1}=t$, $\\omega_{t}^{X, 2}=t^{2}$ can be dropped and the closure reduces to\n\n$$\n\\left\\{(X, t) \\mapsto \\psi\\left(\\omega_{t}^{X}\\right)+\\int_{0}^{t} \\phi\\left(\\omega_{t}^{X}-\\omega_{s}^{X}\\right) \\cdot d \\xi_{s}^{X}\\right\\}\n$$\n\nfor continuous $\\psi: \\mathbb{R}^{d_{\\omega}} \\rightarrow \\mathbb{R}$ and $\\phi: \\mathbb{R}^{d_{\\omega}} \\rightarrow \\mathbb{R}^{d_{\\xi}}$.",
    "theoryssms-7": "All proofs can be found in the Appendix. Remark 4.2. This result can be seen as a generalization of the Universal Approximation for Linear RNNs presented by Li et al. (2022b) [Thm. 7] for generic gates $\\omega, \\xi$. In fact in their setting $Z_{0}=0$ with gates set as in (7) family (9) reduces to linear input filterings given by convolutions\n\n$$\n\\left\\{(X, t) \\mapsto \\int_{0}^{t} \\rho(t-s) \\cdot X_{s} d s\\right\\}\n$$\n\nRemark 4.3. The specific restriction of $\\omega$ to subsets of $[0,1]$ is a crucial part of 9 ). The family of approximable maps does not include all path-to-path causa ${ }^{9}$ functions $t \\mapsto$ $Y_{t}^{\\mathrm{X}}$ but a subset of them, of type $t \\mapsto Y_{t}^{\\mathrm{X}}:=\\Psi\\left(\\omega_{[0, t]}^{\\mathrm{X}}\\right)$, satisfying the specific time-homogeneity specified by the form of the restriction, akin to that in (Li et al.",
    "theoryssms-8": "2022b). ### 4.2. Signature expansion\n\nTo study expressivity of these generalized SSMs (Linear CDEs), it will be convenient to introduce the so-called signature transform (Lyons et al., 2007; Kidger et al. 2019;\n\n[^5]Fermanian et al., 2023), a classical path-transform from stochastic analysis. The main reason for doing so is that, as a simple consequence of the Stone-Weirestrass theorem, linear functionals on the signature provide the essential building blocks (analogous to monomials on Euclidean spaces) to approximate continuous functions on path space. Consider a path $\\omega \\in C_{1,0}\\left([0,1] ; \\mathbb{R}^{d_{\\omega}}\\right)$ and define as $\\mathbb{W}_{d_{\\omega}}$ the set of words (i.e. ordered sequences) in the alphabet $\\left\\{1, \\ldots, d_{\\omega}\\right\\}^{10}$ The signature transform is the following infinite collection of scalar iterated integrals\n\n$$\n\\operatorname{Sig}(\\omega)_{s, t}:=\\left(\\operatorname{Sig}(\\omega)_{s, t}^{(I)}\\right)_{I \\in \\mathbb{W}_{d_{\\omega}}} \\in l^{2}\\left(\\mathbb{W}_{d_{\\omega}}\\right)\n$$\n\nwhere\n\n$$\n\\operatorname{Sig}(\\omega)_{s, t}^{(I)}:=\\int_{s<u_{1}<\\ldots<u_{n}<t} \\dot{\\omega}_{u_{1}}^{\\left(i_{1}\\right)} \\ldots \\dot{\\omega}_{u_{n}}^{\\left(i_{n}\\right)} d u_{1} \\ldots d u_{n}\n$$\n\nWe refer the interested reader to Appendix A for additional details and references on the signature. A classical result from rough path theory states that a Linear CDE can be expanded explicitly as an (infinite) linear combination of terms in the signature of the driving path. Next, we specialize this result to the case of Linear CDEs given by Eqn. 6. Proposition 4.4. For any choice of matrices $A_{1}, \\ldots, A_{d_{\\omega}}$ and $B$, the unique solution to\n\n$$\nd Z_{t}^{X}=\\sum_{i=1}^{d_{\\omega}} A_{i} Z_{t}^{X} d \\omega_{t}^{X, i}+B d \\xi_{t}^{X}, \\quad Z_{0}^{X}=Z_{0} \\in \\mathbb{R}^{N}\n$$\n\nis given by\n\n$$\n\\begin{aligned}\nZ_{t}^{X}= & \\sum_{I \\in \\mathbb{W}_{d_{\\omega}}} A_{I} Z_{0} \\operatorname{Sig}\\left(\\omega^{X}\\right)_{0, t}^{(I)} \\\\\n& +\\sum_{i=1}^{d_{\\xi}} \\sum_{I \\in \\mathbb{W}_{d_{\\omega}}} A_{I} B_{i} \\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{X}\\right)_{s, t}^{(I)} d \\xi_{s}^{X, i} \\in \\mathbb{R}^{N}\n\\end{aligned}\n$$\n\nwhere we used the notation $A_{I}:=A_{i_{n}} \\ldots A_{i_{1}}$. Notice that the previous result does not rely on any assumptions on the nature of $Z_{0}, A_{i}$, and $B$; for any such choice the result is a time-independent linear map on a feature vector\n\n$$\nT(X)_{0, t}:=\\left(\\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}^{(I)}, \\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{(I)} d \\xi_{s}^{\\mathrm{X}, i}\\right)_{(I, i)}\n$$\n\nwhich is a subsequence of $\\operatorname{Sig}\\left(\\left(\\omega^{\\mathrm{X}}, \\xi^{\\mathrm{X}}\\right)\\right)_{0, t} \\in l^{2}\\left(\\mathbb{W}_{d_{\\omega}+d_{\\xi}}\\right)$, and where the index $(I, i)$ runs over $\\mathbb{W}_{d_{\\omega}} \\times\\left\\{1, \\ldots, d_{\\xi}\\right\\}$. [^6]\n### 4.3. Randomised Linear CDEs\n\nIt turns out that the same space of functionals in the closure (9) of a Linear CDE can be induced, with high probability, by sampling the matrices $A_{1}, \\ldots, A_{d_{\\omega}}, B$ and $Z_{0}$ at random according to standard ML practices (LeCun et al., 2012), as stated in the following result:\n\nTheorem 4.5. For any compact subset $\\mathbb{K} \\subset \\mathbb{X}$ and for any functional $F: \\mathbb{K} \\times[0,1] \\rightarrow \\mathbb{R}$ of the form given in Eqn. (9), the following holds\n\n$$\n\\begin{aligned}\n& \\lim _{N \\rightarrow \\infty} \\mathbb{P}\\left[\\left\\{\\exists v \\in \\mathbb{R}^{N} \\quad\\right.\\right. \\text { such that } \\\\\n&\\left.\\left.\\sup _{(X, t) \\in \\mathbb{K} \\times[0,1]}\\left|F(X, t)-\\left\\langle v, Z_{t}^{X}\\right\\rangle\\right| \\leq \\epsilon\\right\\}\\right]=1\n\\end{aligned}\n$$\n\nwhere the matrices $A_{1}, \\ldots, A_{d_{\\omega}}$ and $B$ defining $Z^{X}$ are sampled at random as\n\n$$\n\\left[A_{j}\\right]_{n, n^{\\prime}} \\stackrel{i i d}{\\sim} \\mathcal{N}\\left(0, \\frac{1}{N}\\right) \\quad[Z]_{n},[B]_{n, j} \\stackrel{i i d}{\\sim} \\mathcal{N}(0,1)\n$$\n\nIn particular, Thm. 4.5 implies that only the final readout $v$ needs to be trained, while the randomly initialised matrices $A_{1}, \\ldots, A_{d_{\\omega}}$ and $B$ can be left untrained ( $c f$.",
    "theoryssms-9": "Sec. 6). This is a similar mechanism to the paradigm advocated in reservoir computing (Cuchiero et al., 2021b, Lukoveviius \\& Jaeger.",
    "theoryssms-10": "2009). Remark 4.6. It can be proved, using the results by Dubach \\& Peled (2021), that the sampling measure does not have to be Gaussian if it satisfies certain moment requirements. The core argument behind the proof of Thm. 4.5 consists of exhibiting almost-orthogonal vectors trough random linear maps, and then using these as a pseudo-basis of vectors on which to mimic the dynamics of the feature map $T(X)_{0, t}$. It then suffices to bound the error made by this approximate approach, which is proved to disappear as the dimension $N$ increases. ### 4.4. S 4 as a special case\n\nIn case $\\omega_{t}^{\\mathrm{X}}=t$ and $\\xi_{t}^{\\mathrm{X}}=\\int_{0}^{t} X_{s} d s$, like in S 4 , one has $d_{\\omega}=1$. Thus, $\\mathbb{W}_{d_{\\omega}}=\\mathbb{N}$ and the signature reduces to\n\n$$\n\\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}:=\\left(\\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{(k)}\\right)_{k \\in \\mathbb{N}}\n$$\n\nwhere\n\n$$\n\\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{(k)}=\\frac{(t-s)^{k}}{k!}\n$$\n\nThis in turns implies the following expression for $Z^{X}$\n\n$$\nZ_{t}^{\\mathrm{X}}=\\sum_{i=1}^{d_{\\xi}} \\int_{0}^{t} e^{A(t-s)} B_{i} X_{s}^{i} d s\n$$\n\nand the corresponding feature vector $T$ reads\n\n$$\nT(X)_{0, t}=\\left(\\int_{0}^{t} \\frac{(t-s)^{k}}{k!} X_{s}^{i} d s\\right)_{(k, i)}\n$$\n\nwhere the index runs over $\\mathbb{N} \\times\\left\\{1, \\ldots, d_{\\xi}\\right\\}$. As a result, linear maps on $T$ are of the type\n\n$$\n\\int_{0}^{t} p(t-s) \\cdot X_{s} d s\n$$\n\nfor polynomials $p^{1}, \\ldots, p^{d}$, which reconciles our framework with the results obtained by Li et al. (2022b). It is evident from this discussion that the classical choice of input-independent $\\omega$ (i.e. $\\omega_{t}^{\\mathrm{X}}=t$ ) precludes the use of higher order statistics of $X$. Architectural choices The matrices $A_{1}, \\ldots, A_{d_{\\omega}} \\in$ $\\mathbb{R}^{N \\times N}$ defining the Linear CDE (6) are generally dense matrices. However, at the core of the efficient GPU-efficient computation strategy of S4 and Mamba is the need for diagonal recurrences (see speed comparisons in (Orvieto et al., 2023b)). Indeed, dense matrix multiplication inevitably takes the computational cost of the forward pass to $O\\left(N^{2} L\\right)$ complexity, where $L$ is the sequence length. Instead, using diagonal recurrences results in a $\\mathcal{O}(L N)$ complexity. Motivated by the efficiency requirements in S4 and Mamba, in the next section, we discuss the question of whether this choice of diagonal restricts the family of functionals that one can learn. ### 4.5. The Diagonal Case\n\nIf $A_{i}:=\\operatorname{diag}\\left(v_{i}\\right)$, due to commutativity of diagonal matrices, one has that for any permutation $\\sigma(I)$ of a word $I$ the equality $A_{I}=A_{\\sigma(I)}$ holds. This means that a linear functional on $Z_{t}^{\\mathrm{X}}$ such as 11 ) can fundamentally only see the symmetric part of $\\operatorname{Sig}\\left(\\omega^{\\mathrm{x}}\\right)_{s, t}$, which is just the tensor exponential\n\n$$\ne^{\\otimes \\int_{s}^{t} d \\omega_{r}^{\\mathrm{X}}}:=\\left(\\prod_{i=1}^{d_{\\omega}} \\frac{\\left(\\omega_{t}^{\\mathrm{X}, i}-\\omega_{s}^{\\mathrm{X}, i}\\right)^{k_{i}}}{k_{i}!}\\right)_{k \\in \\mathbb{N}^{d} \\omega}\n$$\n\nand as such does not capture higher order channel combinations (cf.",
    "theoryssms-11": "See Sect. 6 for empirical validation). In fact the explicit solution a can be written as\n\n$$\nZ_{t}^{\\mathrm{X}}=e^{\\operatorname{diag}\\left(V \\omega_{t}^{\\mathrm{X}}\\right)} Z_{0}+\\int_{0}^{t} e^{\\operatorname{diag}\\left(V\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right)\\right)} B d \\xi_{s}^{\\mathrm{X}}\n$$\n\nwhere $V:=\\left[v_{1}|\\cdots| v_{d_{\\omega}}\\right] \\in \\mathbb{R}^{N \\times d_{\\omega}}$. The closure of linear functionals on solutions of (13) is characterized as the following set of functionals\n\n$$\n\\left\\{(X, t) \\mapsto \\psi\\left(\\omega_{t}^{\\mathrm{X}}\\right)+\\int_{0}^{t} \\phi\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right) \\cdot d \\xi_{s}^{\\mathrm{X}}\\right\\}\n$$\n\nfor $\\psi \\in C^{0}\\left(\\mathbb{R}^{d_{\\omega}} ; \\mathbb{R}\\right)$ and $\\phi \\in C^{0}\\left(\\mathbb{R}^{d_{\\omega}} ; \\mathbb{R}^{d_{\\xi}}\\right)$. Efficiency and Stability. On top of efficiency, an important feature of selective SSMs, crucial for long-range reasoning (see discussion in (Orvieto et al. 2023b) is their stability in a dynamical system sense: eigenvalues of $\\bar{A}$ for those discrete recurrences always have absolute value bounded by 1 . Both S 4 and Mamba achieve this by restricting the sign of the diagonal components of their recurrent matrix $A$ with positive nonlinearities such as ReLUs (see Sec.",
    "theoryssms-12": "22. Under ZOH numerical integration (see again Sec. 27, the resulting discrete-time $\\bar{A}=\\exp (\\Delta A)$ is guaranteed to be stable. We can apply the same idea in the general linear diagonal CDE setting (cf. Appendix C.1). Instead, if one works with dense matrices, ensuring stability in numerical integration during training requires heavy adaptations of the optimizer on top of higher overall complexity. Expressivity. Note that the main difference between (9) and 143 is the path-dependence: the latter take as input just a value or increment of $\\omega^{\\mathrm{x}}$ while the maps in (9) are functions on whole sections of it. Diagonal selective SSMs (i.e. Mambs ${ }^{11}$ are thus fundamentally weaker than their nondiagonal counterparts, as they can at most capture the symmetric part of $\\omega^{\\mathrm{X}}$,s signature and its integrals against $d \\xi^{\\mathrm{X}}$ (see Appendix for details). ### 4.6. Chaining Diagonal CDEs\n\nFortunately it is possible to re-gain expressivity without sacrificing the computational advantages of diagonal schemes through chaining. This means driving a new linear CDE by the solution of a previous linear CDE, and repeating this procedure $K$ times. With this procedure, one can recover signature entries up to depth $K$ with complexity $\\mathcal{O}(K L N)$. Proposition 4.7. Assume a compact input set $\\mathbb{X} \\subset$ $C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$. For any $I \\in \\mathbb{W}_{d}$ with $|I| \\geq 2$ and $\\epsilon>0$ there is a sequence of linear maps $W_{k} \\in \\mathbb{R}^{N_{k} \\times 1}$ and weights for the following family of chained linear CDEs\n\n$$\n\\begin{aligned}\nd Z_{t}^{1, X}= & \\sum_{i=1}^{d} A_{i}^{(1)} Z_{t}^{1, X} d X_{t}^{i}+B^{(1)} d X_{t} \\\\\nd Z_{t}^{k+1, X}= & \\sum_{i=1}^{d+1} A_{i}^{(k+1)} Z_{t}^{k+1, X} d\\left[\\begin{array}{c}\nW_{k} Z^{k, X} \\\\\nX\n\\end{array}\\right]_{t}^{i} \\\\\n& +B^{(k+1)} d X_{t}\n\\end{aligned}\n$$\n\nwith $Z^{k} \\in \\mathbb{R}^{N_{k}}$, such that for some $v \\in \\mathbb{R}^{N_{|I|-1}}$ one has\n\n$$\n\\sup _{t) \\in \\mathbb{X} \\times[0,1]}\\left|\\operatorname{Sig}(X)_{0, t}^{(I)}-\\left\\langle v, Z_{t}^{|I|-1, X}\\right\\rangle\\right| \\leq \\epsilon\n$$\n\nThe intuition behind this result is the following: with a\n\n[^7]single diagonal layer it's possible ${ }^{12}$ to recover the terms\n$$\n\\int_{0}^{t}\\left(\\omega_{s}^{\\mathrm{X}, i}-\\omega_{0}^{\\mathrm{X}, i}\\right) d \\xi_{s}^{\\mathrm{X}, j}\n$$\nwhich are just length 2 terms from $\\operatorname{Sig}\\left(\\left(\\omega^{X}, \\xi^{X}\\right)\\right)_{0, t}$.",
    "theoryssms-13": "If we pass these terms as $\\omega$ in another layer then we can obtain a length 3 term from $\\operatorname{Sig}\\left(\\left(\\omega^{\\mathrm{X}}, \\xi^{\\mathrm{X}}\\right)\\right)_{0, t}$, and so on.",
    "theoryssms-14": "Remark 4.8. The structure of the chaining in Prop. 4.7 seems quite restrictive, in particular it's not clear to which extent gates which are non-linear in the input $X$ allow for such a construction. In the Appendix we address this issue in the simple but important case $d \\omega_{t}^{\\mathrm{X}}=\\operatorname{ReLU}\\left(W X_{t}+B\\right) d t$, where we show that no information is lost and numerical stability can be enforced. ## 5. Path-to-Path Learning\n\nUp to now we have been dealing with a limited class of path-to-path maps essentially built by functionals of type $(X, t) \\mapsto\\left\\langle\\alpha, T(X)_{s, t}\\right\\rangle$ for a fixed linear map $\\alpha$. Crucial for modern machine learning are general continuous path-topath models of type $C_{0,1}\\left([0,1] ; \\mathbb{R}^{d}\\right) \\rightarrow C_{0,1}([0,1] ; \\mathbb{R})$. As shown in (Orvieto et al., 2023a), classical SSMs followed by a non-linearity and a feed-forward network are universal path-to-path when the inputs are piecewise linear paths on fixed discretizations (i.e. universal on sequences). Orvieto et al. (2023a) discuss an important feature of S4, which we already presented in previous sections: its action on the input is a simple convolution with fixed (independent of the input) parameters. For this exact reason, the construction of (Orvieto et al. 2023a) defers all non-linear computation (reasoning) to the multi-layer perceptron (MLP) acting on the linear recurrence output, which is simply providing in their setting a loss-less compression of the inputs - with no added reasoning. ![](https://cdn.mathpix.com/cropped/2024_09_17_84ab3f26c8b5e128d602g-07.jpg?height=357&width=795&top_left_y=1792&top_left_x=1077)\n\nFigure 2. Square: Orvieto et al. 2023a)'s approach for path-topath universality. Triangle: our approach. In our setting, we instead characterized the processing power of input-controlled (dense or diagonal) SSMs precisely,\n\n[^8]showing that it greatly surpasses linear filtering. For this reason, the computational burden for an MLP action on a general linear CDE, unlocking path-to-path learning, would be greatly diminished and its actual function reduced to a time-consistent interpolation of already good path-to-point approximations. This method is robust to different input discretization schemes and does not offload all the complexity to the network which follows the SSM (cf.",
    "theoryssms-15": "Figure 2). Proposition 5.1. Fix a compact set $\\mathbb{K} \\subseteq \\mathbb{X}$ and continuous $\\omega, \\xi$ with $\\omega_{t}^{X, 1} \\equiv t$. Then for all $\\epsilon>0$ and all causal continuous mapping $G: C_{1,0}\\left([0,1], \\mathbb{R}^{d_{\\omega}}\\right) \\times[0,1] \\rightarrow \\mathbb{R}$ there exist an integer $N \\geq 0$, some Feed Forward neural network $F: \\mathbb{R}^{N} \\rightarrow \\mathbb{R}$, and parameters $Z_{0} \\in \\mathbb{R}^{N}, A_{i} \\in$ $\\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\xi}}$ such that\n\n$$\n\\sup _{, t) \\in \\mathbb{K} \\times[0,1]}\\left|F\\left(Z_{t}^{X}\\right)-G\\left(\\omega^{X}, t\\right)\\right|<\\epsilon\n$$\n\nActually, $F$ in the above result does not have to be a neural network, it suffices for it to be able to interpolate linear maps in a time-consistent manner $c f$.",
    "theoryssms-16": "Appendix D. ## 6. Empirical Validation\n\nA variation on the toy dataset introduced in Walker et al. 2024 is used to empirically validate our theoretical results. We use two datasets of 100,000 sample paths each, with dimensions 2 and 3 respectively. In both datasets, paths are defined on 100 regularly spaced time-steps between $t=0$ and $t=1$. The change in each channel at each time point is an independent sample from a standard Normal distribution rounded to the nearest integer. The datasets are then normalised to range between -1 and 1 . The aim on both datasets is to predict specific terms in the anti-symmetric part of the signature. For the 2 dimensional dataset the prediction is an area integral and for the 3 dimensional a volume integral given, respectively, by\n\n$$\n\\int_{0}^{1} \\int_{0}^{v} d X_{u}^{1} d X_{v}^{2}, \\quad \\int_{0}^{1} \\int_{0}^{w} \\int_{0}^{v} d X_{u}^{1} d X_{v}^{2} d X_{w}^{3}\n$$\n\nWe consider five models:\n\n1. A S5 or Mamba recurrence with linear readout. 2. Two stacked S5 or Mamba recurrences with a linear mixing layer in-between, and the final state of the second layer fed into a linear readout. 3. A linear NCDE with gates $\\omega_{t}^{\\mathrm{X}}=\\xi_{t}^{\\mathrm{X}}=\\left(t, X_{t}\\right)$ followed by a linear readout. All state space models have trainable matrices in their recurrences, whereas the linear NCDE is using fixed matrices. All of the models use a hidden dimension of 256 , with the state space models using a state dimension of 256 . The state space models are trained using batch gradient descent with a batch size of 32 and Adam with a learning rate of $3 \\times 10^{-5}$.",
    "theoryssms-17": "Results are plotted in Fig. 3\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_84ab3f26c8b5e128d602g-08.jpg?height=495&width=814&top_left_y=403&top_left_x=1073)\n\nFigure 3. Comparison of Linear CDE, Mamba and S5 on antisymmetric signature prediction tasks. As suggested by the theory Mamba struggles to immediately capture these high order statistics, but performance improves with chaining. On the other hand S5 does not learn such non-linear features even after training and chaining. The output from the linear NCDE's recurrence is obtained using an adaptive ODE solver, Tsit5, with an absolute and relative tolerance of $1 \\times 10^{-2}$. The linear NCDEs linear layer is obtained by ordinary least squares. ## 7. Conclusions\n\nIn this paper we have considered a family of models, defined by gating functions as solutions to linear CDEs, which extends both classical and modern SSM architectures. Using analytical tools from Rough Paths theory we have characterized the uniform closure of these models generalizing the results of (Li et al. 2022b), shedding light on the expressive advantages of input-controlled transition dynamics, which allow to capture high order statistics of the input as opposed to just the linear ones extracted by convolutions. We point out how the full expressive range is already captured by generic models, and how more efficient computational structures, such as imposing diagonality, weaken these expressive capabilities. A remedy to this loss is presented in the chaining of these models. Finally we show how the substitution of the linear readout with an MLP allows to approximate general path-to-path functions without, in contrast to the S4 case, offloading all the complexity to the neural network. ## 8. Acknowledgements\n\nNicola Muca Cirone is supported by the EPSRC Centre for Doctoral Training in Mathematics of Random Systems:\n\nAnalysis, Modelling and Simulation (EP/S023925/1). Antonio Orvieto acknowledges the financial support of the Hector Foundation. Benjamin Walker was funded by the Hong Kong Innovation and Technology Commission (InnoHK Project CIMDA). Terry Lyons was funded in part by the EPSRC [grant number EP/S026347/1], in part by The Alan Turing Institute under the EPSRC grant EP/N510129/1, the Data Centric Engineering Programme (under the Lloyd's Register Foundation grant G0095), the Defence and Security Programme (funded by the UK Government) and the Office for National Statistics \\& The Alan Turing Institute (strategic partnership) and in part by the Hong Kong Innovation and Technology Commission (InnoHK Project CIMDA).",
    "theoryssms-18": "## References\n\nArribas, I. P., Salvi, C., and Szpruch, L. Sig-sdes model for quantitative finance. In Proceedings of the First ACM International Conference on AI in Finance, pp. 1-8, 2020. Barron, A. R. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3):930-945, 1993. Berlinet, A. and Thomas-Agnan, C. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Springer US, 2011. ISBN 9781441990969 . URL https://books. google.co.uk/books?id=bX3TBwAAQBAJ\n\nBoyd, S. and Chua, L. Fading memory and the problem of approximating nonlinear operators with volterra series. IEEE Transactions on circuits and systems, 32(11):1150$1161,1985$. Chen, K.-T. Integration of paths-a faithful representation of paths by noncommutative formal power series. Transactions of the American Mathematical Society, 89 (2):395-407, 1958.",
    "theoryssms-19": "ISSN 00029947. URL http: //www.jstor.org/stable/1993193. Chevyrev, I. and Kormilitzin, A. A primer on the signature method in machine learning. arXiv preprint arXiv:1603.03788, 2016. Cho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation.",
    "theoryssms-20": "arXiv preprint arXiv:1406.1078, 2014. Cirone, N. M., Lemercier, M., and Salvi, C. Neural signature kernels as infinite-width-depth-limits of controlled resnets, 2023. Cochrane, T., Foster, P., Chhabra, V., Lemercier, M., Lyons, T., and Salvi, C. Sk-tree: a systematic malware detection algorithm on streaming trees via the signature kernel. In\n2021 IEEE international conference on cyber security and resilience (CSR), pp.",
    "theoryssms-21": "35-40. IEEE, 2021. Cuchiero, C., Gonon, L., Grigoryeva, L., Ortega, J.-P., and Teichmann, J. Expressive power of randomized signature. In The Symbiosis of Deep Learning and Differential Equations, 2021a. Cuchiero, C., Gonon, L., Grigoryeva, L., Ortega, J.-P., and Teichmann, J. Discrete-time signatures and randomness in reservoir computing. IEEE Transactions on Neural Networks and Learning Systems, Forthcoming, 04 2021b. doi: 10.1109/TNNLS.2021.3076777. Dasgupta, S. and Gupta, A. An elementary proof of a theorem of johnson and lindenstrauss. Random Structures \\& Algorithms, 22, 2003. URL https://api.sema nticscholar.org/CorpusID:10327785. De, S., Smith, S. L., Fernando, A., Botev, A., CristianMuraru, G., Gu, A., Haroun, R., Berrada, L., Chen, Y., Srinivasan, S., Desjardins, G., Doucet, A., Budden, D., Teh, Y. W., Pascanu, R., Freitas, N. D., and Gulcehre, C. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Houlsby, N., Gelly, S., Zhang, X., and Uszkoreit, J. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.",
    "theoryssms-22": "Dubach, G. and Peled, Y. On words of non-Hermitian random matrices. The Annals of Probability, 49(4):1886 - 1916, 2021. doi: 10.1214/20-AOP1496. URL https: //doi.org/10.1214/20-AOP1496. Fermanian, A. Embedding and learning with signatures, 2020. Fermanian, A., Lyons, T., Morrill, J., and Salvi, C. New directions in the applications of rough path theory. IEEE BITS the Information Theory Magazine, 2023.",
    "theoryssms-23": "Friz, P. K. and Victoir, N. B. Multidimensional Stochastic Processes as Rough Paths: Theory and Applications. Cambridge Studies in Advanced Mathematics. Cambridge University Press, 2010. doi: 10.1017/CB O9780511845079. Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2022. Goel, K., Gu, A., Donahue, C., and R\u00e9, C. It's raw! audio generation with state-space models. International Conference on Machine Learning, 2022. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2023. Gu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33: 1474-1487, 2020. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. Gu, A., Gupta, A., Goel, K., and R\u00e9, C. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022.",
    "theoryssms-24": "Hambly, B. and Lyons, T. Uniqueness for the signature of a path of bounded variation and the reduced path group. Annals of Mathematics, pp. 109-167, 2010. Hanson, J. and Raginsky, M. Universal simulation of stable dynamical systems by recurrent neural nets. In Learning for Dynamics and Control, pp. 384-392. PMLR, 2020. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 1997. Hoglund, M., Ferrucci, E., Hernandez, C., Gonzalez, A. M., Salvi, C., Sanchez-Betancourt, L., and Zhang, Y. A neural rde approach for continuous-time non-markovian stochastic control problems. arXiv preprint arXiv:2306.14258, 2023. Issa, Z., Horvath, B., Lemercier, M., and Salvi, C. Nonadversarial training of neural sdes with signature kernel scores. Advances in Neural Information Processing Systems, 2023. Katsch, T. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023. Kidger, P. On neural differential equations, 2022. Kidger, P., Bonnier, P., Perez Arribas, I., Salvi, C., and Lyons, T. Deep signature transforms. Advances in Neural Information Processing Systems, 32, 2019. Kidger, P., Morrill, J., Foster, J., and Lyons, T. Neural controlled differential equations for irregular time series. Advances in Neural Information Processing Systems, 33: 6696-6707, 2020. Korsky, S. A. and Berwick, R. C. On the computational power of rnns. arXiv preprint arXiv:1906.06349, 2019. LeCun, Y. A., Bottou, L., Orr, G. B., and M\u00fcller, K.-R. Efficient BackProp, pp. 9-48. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. ISBN 978-3-642-35289-8. doi: 10.1007/978-3-642-35289-8_3. URL https://doi. org/10.1007/978-3-642-35289-8_3\nLemercier, M., Salvi, C., Damoulas, T., Bonilla, E. V., and Lyons, T. Distribution regression for sequential data, 2021. Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022a. Li, Z., Han, J., Weinan, E., and Li, Q. Approximation and optimization theory for linear continuous-time recurrent neural networks.",
    "theoryssms-25": "J. Mach. Learn. Res., 23:42-1, 2022b. Lu, C., Schroecker, Y., Gu, A., Parisotto, E., Foerster, J., Singh, S., and Behbahani, F. Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems, 2023. Lukoveviius, M. and Jaeger, H. Reservoir computing approaches to recurrent neural network training.",
    "theoryssms-26": "Comput. Sci. Rev., 3:127-149, 2009. URLhttps://api.se manticscholar.org/CorpusID:554006. Lyons, T. and McLeod, A. D. Signature methods in machine learning, 2024. Lyons, T. J., Caruana, M., and L\u00e9vy, T. Differential equations driven by rough paths. Springer, 2007.",
    "theoryssms-27": "Martin, E. and Cundy, C. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017. Morrill, J., Salvi, C., Kidger, P., and Foster, J. Neural rough differential equations for long time series. In International Conference on Machine Learning, pp. 7829-7838. PMLR, 2021. Nguyen, E., Goel, K., Gu, A., Downs, G. W., Shah, P., Dao, T., Baccus, S. A., and R\u00e9, C. S4nd: Modeling images and videos as multidimensional signals using state spaces. $A d$ vances in Neural Information Processing Systems, 2022. Orvieto, A., De, S., Gulcehre, C., Pascanu, R., and Smith, S. L. On the universality of linear recurrences followed by nonlinear projections. arXiv preprint arXiv:2307.11888, 2023a. Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023b. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Salvi, C., Cass, T., Foster, J., Lyons, T., and Yang, W. The signature kernel is the solution of a goursat pde. SIAM Journal on Mathematics of Data Science, 3(3):873-899,\n2021a. doi: 10.1137/20M1366794. URL https: //doi.org/10.1137/20M1366794. Salvi, C., Lemercier, M., Cass, T., Bonilla, E.",
    "theoryssms-28": "V., Damoulas, T., and Lyons, T. J. Siggpde: Scaling sparse gaussian processes on sequential data. In International Conference on Machine Learning, pp. 6233-6242. PMLR, 2021b. Salvi, C., Lemercier, M., Liu, C., Horvath, B., Damoulas, T., and Lyons, T. Higher order kernel mean embeddings to capture filtrations of stochastic processes. Advances in Neural Information Processing Systems, 34:1663516647, 2021c. Salvi, C., Lemercier, M., and Gerasimovics, A. Neural stochastic pdes: Resolution-invariant learning of continuous spatiotemporal dynamics. Advances in Neural Information Processing Systems, 35:1333-1344, 2022. Siegelmann, H. T. and Sontag, E. D. On the computational power of neural nets. In Proceedings of the fifth annual workshop on Computational learning theory, pp. 440449, 1992. Smith, J. T. H., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling, 2023. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A successor to transformer for large language models, 2023. Tallec, C. and Ollivier, Y. Can recurrent neural networks warp time?, 2018. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Walker, B., McLeod, A. D., Qin, T., Cheng, Y., Li, H., and Lyons, T. Log neural controlled differential equations: The lie brackets make a difference, 2024.",
    "theoryssms-29": "Wang, J., Yan, J.",
    "theoryssms-30": "N., Gu, A., and Rush, A. M. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022.",
    "theoryssms-31": "Wang, S. and Xue, B. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. arXiv preprint arXiv:2309.13414, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Zucchet, N., Meier, R., Schug, S., Mujika, A., and Sacramento, J. Online learning of long-range dependencies, 2023. ## A. Introduction to Signatures\n\nThis initial section of the Appendix is devoted to a brief introduction to the topic of Signature Transform.",
    "theoryssms-32": "For more in-depth surveys we suggest the reader to refer to (Fermanian, 2020; Chevyrev \\& Kormilitzin, 2016; Lyons \\& McLeod, 2024). ## A.1. Intuition - Controlled Differential Equations\n\nIn the simplest setting of smooth paths a CDE is a differential equation of form\n\n$$\n\\frac{d Z_{t}}{d t}=F\\left(\\frac{d X_{t}}{d t}, Z_{t}\\right), \\quad Z_{0} \\in \\mathbb{R}^{n}\n$$\n\nwhere $X:[0,1] \\rightarrow \\mathbb{R}^{d}$ is a known smooth path to which we refer as control, $Z_{0}$ the known initial condition and $Z:[0,1] \\rightarrow \\mathbb{R}^{n}$ the unknown solution. The natural generalization is the following: assume to have two spaces $\\mathbb{R}^{d_{x}}$ and $\\mathbb{R}^{d_{z}}, X \\in C_{1}\\left([0,1] ; \\mathbb{R}^{d_{x}}\\right), Z \\in$ $C_{1}\\left([0,1] ; \\mathbb{R}^{d_{z}}\\right), F: \\mathbb{R}^{d_{z}} \\rightarrow \\mathcal{L}\\left(\\mathbb{R}^{d_{x}}, \\mathbb{R}^{d_{z}}\\right)$ and $Z_{0} \\in \\mathbb{R}^{d_{z}}$. We say that $\\left(Z, X, F, Z_{0}\\right)$ satisfy the CDE\n\n$$\nd Z_{t}=F\\left(Z_{t}\\right) d X_{t}, \\quad Z_{0} \\in \\mathbb{R}^{d_{z}}\n$$\n\nwhenever\n\n$$\nZ_{t}=Z_{0}+\\int_{0}^{t} F\\left(Z_{s}\\right) d X_{s}\n$$\n\nThe theory of Rough Paths has its origins in the study of such types of differential equations and provides a theoretical framework to define and work in rough settings i.e. when $X$ is not kust BV but even $\\alpha$-H\u00f6lder for $\\alpha \\in(0,1) c f$. (Friz \\& Victoir, 2010). Assume thus to have the CDE\n\n$$\nd Z_{t}=\\sum_{i=1}^{d_{x}} V_{i}\\left(Z_{t}\\right) d X_{t}^{i}, \\quad Z_{0} \\in \\mathbb{R}^{d_{z}}\n$$\n\nfor sufficiently regular vector fields $V_{i}$ and $X \\in C_{1}\\left([0,1] ; \\mathbb{R}^{d_{x}}\\right)$. Given a smooth $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$, by the change of variable formula (i.e. fundamental theorem of calculus) we have\n\n$$\nf\\left(Z_{t}\\right)=f\\left(Z_{0}\\right)+\\sum_{i=1}^{d_{x}} \\int_{0}^{t} V_{i} f\\left(Z_{s}\\right) d X_{s}^{i}\n$$\n\nwhere $V_{i} f(z):=d f_{y}\\left[V_{i}(z)\\right]$. Iterating this procedure on the $V_{i} f$ s, i.e. substituting in the previous equation the analogously obtained equality\n\n$$\nV_{i} f\\left(Z_{s}\\right)=V_{i} f\\left(Z_{0}\\right)+\\sum_{j=1}^{d_{x}} \\int_{0}^{s} V_{j}\\left(V_{i} f\\right)\\left(Z_{u}\\right) d X_{u}^{j}\n$$\n\nwe get\n\n$$\nf\\left(Z_{t}\\right)=f\\left(Z_{0}\\right)+\\sum_{i=1}^{d} V_{i} f\\left(Z_{0}\\right) \\int_{0}^{t} d X_{s}^{i}+\\sum_{i, j=1}^{d} \\int_{0}^{t} \\int_{0}^{s} V_{j} V_{i} f\\left(Z_{u}\\right) d X_{u}^{j} d X_{s}^{i}\n$$\n\nKeeping with this procedure for $N$ steps we get\n\n$$\nf\\left(Z_{t}\\right)=f\\left(Z_{0}\\right)+\\sum_{k=1}^{N} \\sum_{|I|=k} V_{I} f\\left(Z_{0}\\right) \\int_{s<u_{1}<\\cdots<u_{k}<t} \\cdots \\int_{u_{1}} d X_{i_{1}}^{i_{1}} \\cdots d X_{u_{k}}^{i_{k}}+R_{N}(t)\n$$\n\nwhere $I=\\left(i_{1}, \\ldots, i_{k}\\right)$ runs trough the multi-indices, $V_{I} f:=V_{i_{1}} V_{i_{2}} \\ldots V_{i_{k}} f$ and\n\n$$\nR_{N}(t):=\\sum_{|J|=k+1} \\int_{s<u_{1}<\\cdots<u_{k+1}<t} \\cdots V_{J} f\\left(Z_{u_{1}}\\right) d X_{u_{1}}^{j_{1}} \\cdots d X_{u_{k+1}}^{j_{k+1}}\n$$\n\nAs one can imagine, under reasonable regularity assumptions, the remainder goes to 0 as $N \\rightarrow \\infty$ and at the limit\n\n$$\nf\\left(Z_{t}\\right)=f\\left(Z_{0}\\right)+\\sum_{k=1}^{\\infty} \\sum_{|I|=k} V_{I} f\\left(Z_{0}\\right) \\int_{s<u_{1}<\\cdots<u_{k}<t} \\cdots \\int_{u_{1}} d X_{u_{1}}^{i_{1}} \\cdots d X_{u_{k}}^{i_{k}}\n$$\n\nThis is a remarkable result: to know the solution $Z_{t}$ to the original CDE it suffices to know the quantities $V_{I} f$ for all multi-indices and $f$ in the coordinate maps, together with the iterated integrals\n\n$$\n\\operatorname{Sig}(X)_{s, t}^{I}:=\\int_{s<u_{1}<\\cdots<u_{k}<t} \\cdots \\int_{u_{1}} d X_{u_{1}}^{i_{1}} \\cdots d X_{u_{k}}^{i_{k}}\n$$\n\nThis observation is at the core of Rough Path Analysis, the theory can in a sense be considered an extreme development of it. The collection of iterated integrals, the Signature, will be the main for our analysis. In Appendix Ewe expand and make rigorous the arguments of this section in the case of affine vector fields. ## A.2. Basic Definitions\n\nDenote by $\\left(\\mathbb{R}^{d}\\right)^{\\otimes n}:=\\mathbb{R}^{d} \\otimes \\cdots \\otimes \\mathbb{R}^{d}$ the tensor product of $n$ copies $\\mathbb{R}^{d}$, set $\\left(\\mathbb{R}^{d}\\right)^{\\otimes 0}:=\\mathbb{R}$. Let $T\\left(\\mathbb{R}^{d}\\right):=\\bigoplus_{k=0}^{\\infty}\\left(\\mathbb{R}^{d}\\right)^{\\otimes k}$ be the tensor algebra equipped with sum and tensor product. Definition A.1. Let $\\left\\{e_{1}, \\ldots, e_{d}\\right\\}$ be the canonical basis of $\\mathbb{R}^{d}$, then\n\n$$\n\\left\\{e_{i_{1}} \\otimes \\cdots \\otimes e_{i_{k}}:\\left(i_{1}, \\ldots, i_{k}\\right) \\in[d]^{k}\\right\\}\n$$\n\nis a basis of $\\left(\\mathbb{R}^{d}\\right)^{\\otimes k}$. We equip $\\left(\\mathbb{R}^{d}\\right)^{\\otimes k}$ with the inner product $\\langle\\cdot, \\cdot\\rangle_{\\left(\\mathbb{R}^{d}\\right)^{\\otimes k}}$ defined on basis elements as\n\n$$\n\\left\\langle e_{i_{1}} \\otimes \\cdots \\otimes e_{i_{k}}, e_{j_{1}} \\otimes \\cdots \\otimes e_{j_{k}}\\right\\rangle_{\\left(\\mathbb{R}^{d}\\right)^{\\otimes k}}=\\delta_{\\left(i_{1}, \\ldots, i_{k}\\right)}^{\\left(j_{1}, \\ldots, j_{k}\\right)}\n$$\n\nWe extend this product to $T\\left(\\mathbb{R}^{d}\\right)$ by\n\n$$\n\\langle A, B\\rangle_{T\\left(\\mathbb{R}^{d}\\right)}:=\\sum_{k=0}^{\\infty}\\left\\langle a_{k}, b_{k}\\right\\rangle_{\\left(\\mathbb{R}^{d}\\right) \\otimes k}\n$$\n\nwhere $A=\\left(a_{0}, a_{1}, \\ldots\\right)$ and $B=\\left(b_{0}, b_{1}, \\ldots\\right)$\nNote how for any $A, B \\in T\\left(\\mathbb{R}^{d}\\right)$ we have $\\left\\langle A \\otimes e_{i}, B \\otimes e_{j}\\right\\rangle_{T\\left(\\mathbb{R}^{d}\\right)}=\\langle A, B\\rangle_{T\\left(\\mathbb{R}^{d}\\right)}\\left\\langle e_{i}, e_{j}\\right\\rangle_{\\mathbb{R}^{d}}$, we refer to this as to the coproduct property. Definition A. 2 (Infinite Tensor Algebra). The infinite tensor algebra is defined as the space $T\\left(\\left(\\mathbb{R}^{d}\\right)\\right):=\\prod_{k=0}^{\\infty}\\left(\\mathbb{R}^{d}\\right)^{\\otimes k}$ equipped with the operations + and $\\otimes$ which act in the natural algebraic way; its elements are called tensor series. It is easily seen that $\\left(T\\left(\\left(\\mathbb{R}^{d}\\right)\\right),+, \\otimes\\right)$ is an algebra with unit $\\mathbf{1}=(1,0,0, \\cdots)$ and we can endow it with a natural product which inherits the coproduct property. Another point of view could be taken on the definitions of these spaces, one that we will prefer later on. If we define $\\mathbb{W}_{d}$ to be the set of words in $d$ letters then $T\\left(\\left(\\mathbb{R}^{d}\\right)\\right) \\sim \\mathbb{R}^{W_{d}}, T\\left(\\mathbb{R}^{d}\\right)$ is the subset of such functions with finite support and\n\n$$\n\\langle A, B\\rangle_{T\\left(\\mathbb{R}^{d}\\right)}=\\sum_{I \\in \\mathbb{W}_{d}} A_{I} B_{I}=\\sum_{k=0}^{\\infty} \\sum_{|I|=k} A_{I} B_{I}\n$$\n\nwhere $|I|$ is the length of the word $I$. The empty word, the only one with length 0 , is denoted by () and corresponds to the basis element of $\\left(\\mathbb{R}^{d}\\right)^{\\otimes 0}$. In this view the tensor product coincides with concatenation of words accordingly distributed and the closure of $T\\left(\\mathbb{R}^{d}\\right)$ with respect to its product is just the $l^{2}$ space $l^{2}\\left(\\mathbb{W}_{d}\\right)$.",
    "theoryssms-33": "Definition A. 3 (Signature). Given $\\gamma \\in C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$ and $s, t \\in[0,1]$ s.t. $s \\leq t$, the signature $\\operatorname{Sig}(\\gamma)_{s, t} \\in T\\left(\\left(\\mathbb{R}^{d}\\right)\\right)$ of the path $\\gamma$ over $[s, t]$ is defined as\n\n$$\n\\operatorname{Sig}(\\gamma)_{s, t}:=\\left(1, \\int_{s<u_{1}<t} d \\gamma_{u_{1}}, \\cdots, \\int_{s<u_{1}<\\cdots<u_{k}<t} \\cdots \\int_{u_{1}} \\otimes \\cdots \\otimes d \\gamma_{u_{k}}, \\cdots\\right)^{13}\n$$\n\n[^9]Equivalently $\\operatorname{Sig}(\\gamma)_{s, t}$ is that element of $l^{2}\\left(\\mathbb{W}_{d}\\right)$ defined recursively on words as\n\n$$\n\\operatorname{Sig}(\\gamma)_{s, t}^{()}=1, \\quad \\operatorname{Sig}(\\gamma)_{s, t}^{I j}=\\int_{s}^{t} \\operatorname{Sig}(\\gamma)_{s, r}^{I} d \\gamma_{r}^{j}\n$$\n\n## A.3.",
    "theoryssms-34": "Notable Results\n\nHere we present some notable results of which we will make use through the paper. We omit the proofs if they can be easily found in the suggested references. The first result is about bounding the norm of Signature entries:\nProposition A. 4 (Factorial Decay Rate). Given $\\gamma \\in C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$, for all $k \\geq 1$ and $s, t \\in[0,1]$ s.t. $s \\leq t$ one has\n\n$$\n\\left\\|\\int_{s<u_{1}<\\cdots<u_{k}<t} \\cdots \\int_{u_{1}} \\otimes \\cdots \\otimes d \\gamma_{u_{k}}\\right\\|_{\\left(\\mathbb{R}^{d}\\right)^{\\otimes k}} \\leq \\frac{\\|\\gamma\\|_{1-v a r,[s, t]}^{k}}{k!}\n$$\n\nThe most important fact about Signature is that it acts as the basis for a Taylor expansion in path space. In fact just as finite linear combinations of monomials are dense in the continuous functions with a compact input set, finite linear combinations of Signature entries are dense in continuous functions from compact path-spaces:\n\nTheorem A. 5 (Universal Approximation Fermanian 2020). Fix $K \\subset C_{1,0}\\left([0,1] ; \\mathbb{R}^{d+1}\\right)$ compact such that for any $\\gamma \\in K$ it holds $\\gamma_{t}^{1}=t$. For any $F \\in C^{0}(K ; \\mathbb{R})$ and $\\epsilon>0$ there is an integer $N \\geq 0$ such that\n\n$$\n\\sup _{\\gamma \\in K}\\left|F(\\gamma)-\\sum_{|I| \\leq N} \\alpha_{I} \\operatorname{Sig}(\\gamma)_{0,1}^{I}\\right| \\leq \\epsilon\n$$\n\nfor some finite sequence $\\left(\\alpha_{I}\\right)_{|I| \\leq N}$ of real numbers. Remark A.6. There is no magic in this result, it is just an application of Stone-Weiestrass enabled by the rich algebraic structure of iterated integrals, studied originally in (Chen, 1958). We will need to restrict some paths to sub-intervals of $[0,1]$ in such a way to still be able to consider them meaningfully as elements of $C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$, this is done in the following way:\n\nDefinition A.7. Given any path $\\gamma \\in C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$ we define its restriction on a sub-interval $[s, t] \\subseteq[0,1]$ as the path $\\gamma_{[s, t]} \\in C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$ with values\n\n$$\n\\gamma_{[s, t]}(r):= \\begin{cases}0 & \\text { if } r<s \\\\ \\gamma_{r}-\\gamma_{s} & \\text { if } s \\leq r \\leq t \\\\ \\gamma_{t}-\\gamma_{s} & \\text { if } r>t\\end{cases}\n$$\n\nThis definition is such that the following important equation holds\n\n$$\n\\operatorname{Sig}(\\gamma)_{s, t}=\\operatorname{Sig}\\left(\\gamma_{[s, t]}\\right)_{0,1}\n$$\n\nWith the right augmentation of the paths one can see that the Signature distinguishes between different sections of paths, this will be crucial for some of the original results presented in this work. Lemma A.8. Assume $\\omega, \\gamma \\in C_{1,0}\\left([0,1] ; \\mathbb{R}^{d+2}\\right)$ with $\\omega_{t}^{1}=\\gamma_{t}^{1} \\equiv t$ and $\\omega_{t}^{2}=\\gamma_{t}^{2} \\equiv t^{2}$. Then\n\n$$\n\\operatorname{Sig}(\\omega)_{s, t}=\\operatorname{Sig}(\\gamma)_{s^{\\prime}, t^{\\prime}} \\Longleftrightarrow \\omega_{[s, t]}=\\gamma_{\\left[s^{\\prime}, t^{\\prime}\\right]}\n$$\n\nProof. The if part is follows from $\\operatorname{Sig}(\\gamma)_{s, t}=\\operatorname{Sig}\\left(\\gamma_{[s, t]}\\right)_{0,1}$. For the only if part, If $s=s^{\\prime}$ and $t=t^{\\prime}$ the statement holds; this is because if the signatures over the time interval $[s, t]$ of two time-augmented paths are equal, then the two paths must be equal on $[s, t]$. We now show that augmenting the path with $t^{2}$ and imposing equality of signatures, implies $s=s^{\\prime}$\nand $t=t^{\\prime}$, which will in turn allow us to conclude the proof by the previous remark. Assume $\\operatorname{Sig}(\\omega)_{s, t}=\\operatorname{Sig}(\\gamma)_{s^{\\prime}, t^{\\prime}}$, in particular we must have\n\n$$\n\\begin{gathered}\n\\int_{s}^{t} d\\left(r^{2}\\right)=t^{2}-s^{2}=\\left(t^{\\prime}\\right)^{2}-\\left(s^{\\prime}\\right)^{2}=\\int_{s^{\\prime}}^{t^{\\prime}} d\\left(r^{2}\\right) \\\\\n\\int_{s}^{t} d(r)=t-s=t^{\\prime}-s^{\\prime}=\\int_{s^{\\prime}}^{t^{\\prime}} d(r)\n\\end{gathered}\n$$\n\nwhich reduces to the system\n\n$$\n\\left\\{\\begin{array} { l } \n{ t ^ { 2 } - s ^ { 2 } = ( t ^ { \\prime } ) ^ { 2 } - ( s ^ { \\prime } ) ^ { 2 } } \\\\\n{ t - s = t ^ { \\prime } - s ^ { \\prime } }\n\\end{array} \\quad \\left\\{\\begin{array} { l } \n{ t + s = t ^ { \\prime } + s ^ { \\prime } } \\\\\n{ t - s = t ^ { \\prime } - s ^ { \\prime } }\n\\end{array} \\quad \\left\\{\\begin{array}{l}\n2 t=2 t^{\\prime} \\\\\n2 s=2 s^{\\prime}\n\\end{array}\\right.\\right.\\right. $$\n\nHence it must be true that $t=t^{\\prime}$ and $s=s^{\\prime}$. ## B. Expressivity\n\n## B.1. Model Recap\n\nIn the body of the paper we have presented the main results with the simplified assumption of $Z_{0}^{\\mathrm{X}}=0$ or at best $Z_{0}^{\\mathrm{X}}=Z_{0}$ i.e. with an initial value independent from the input. In this appendix we will carry on the proofs in a more general setting in which $Z_{0}^{\\mathrm{X}}$ is allowed to be input-dependent, as previously discussed the choice of initial value is, in contrast to the classical setting, meaningful inasmuch it allows to approximate linear maps on the signature of $\\omega_{[0,1]}^{\\mathrm{X}}$. In order to do so we have to introduce a new gate, the initial value gate, in the form of a map\n\n$$\n\\begin{gathered}\n(\\cdot)_{0}: \\mathbb{X} \\rightarrow \\mathbb{R}^{d_{0}} \\\\\nX \\mapsto X_{0}\n\\end{gathered}\n$$\n\nDespite the notation, there is no reason why $(X)_{0}$ should be the initial value of the path $X$, one should think of this map as the one summarizing the data which still matters for the task but which does not have a time-series nature. To recapitulate, the general setting of our models is the following: a topological input space space $\\mathbb{X}$,\n\n$$\n\\begin{aligned}\n& (\\cdot)_{0}: \\mathbb{X} \\rightarrow \\mathbb{R}^{d_{0}}, \\\\\n& \\omega: \\mathbb{X} \\rightarrow C_{1,0}\\left([0,1] ; \\mathbb{R}^{d_{\\omega}}\\right), \\\\\n& \\xi: \\mathbb{X} \\rightarrow C_{1,0}\\left([0,1] ; \\mathbb{R}^{d_{\\xi}}\\right)\n\\end{aligned}\n$$\n\n$(\\xi$-gate)\nwhere all the gates are continuous functions on $\\mathbb{X}$. The space $\\mathbb{X}$ does not have to be a space of paths, a topological structure suffices, as long as the gates $(\\cdot)_{0}, \\omega, \\xi$ are well defined and continuous. Remark B.1. Typical examples for the choice of gates are $\\mathbb{X}$ space of paths and\n\n$$\n\\begin{gathered}\n(X)_{0}=0 \\quad \\omega_{t}^{\\mathrm{X}}=t \\quad \\xi_{t}^{\\mathrm{X}}=\\int_{0}^{t} X_{s} d s \\\\\n(X)_{0}=0 \\quad \\omega_{t}^{\\mathrm{X}}=\\int_{0}^{t} \\operatorname{softplus}\\left(\\alpha X_{s}+\\beta\\right) d s \\quad \\xi_{t}^{\\mathrm{X}}=\\int_{0}^{t} \\operatorname{softplus}\\left(\\alpha X_{s}+\\beta\\right) X_{s} d s\n\\end{gathered}\n$$\n\n(Mamba)\n\nThen the main object of study, \"gated\" Linear CDEs, are defined as:\nDefinition B.2. Fix gates $(\\cdot)_{0}, \\omega, \\xi$ as above, $N \\in \\mathbb{N}$, matrices $\\left\\{A_{i}\\right\\}_{i=1, \\ldots, d_{\\omega}}\\left(A_{i} \\in \\mathbb{R}^{N \\times N}\\right), B \\in \\mathbb{R}^{N \\times d_{\\xi}}, C \\in \\mathbb{R}^{N \\times d_{0}}$. The corresponding Linear CDE is the functional\n\n$$\n\\begin{gathered}\nZ: \\mathbb{X} \\rightarrow C_{1}\\left([0,1] ; \\mathbb{R}^{N}\\right) \\\\\nZ_{t}^{\\mathrm{X}}=\\sum_{i=1}^{d_{\\omega}} A_{i} Z_{t}^{\\mathrm{X}} d \\omega_{t}^{\\mathrm{X}, i}+B d \\xi_{t}^{\\mathrm{X}} \\\\\nZ_{0}^{\\mathrm{X}}=C X_{0}\n\\end{gathered}\n$$\n\n## B.2. Main Result - Statement and Strategy\n\nHere we present the unified expressivity result in its most general form:\nTheorem B.3. For any compact set $\\mathbb{K} \\subseteq \\mathbb{X}$ and continuous gates $(\\cdot)_{0}, \\omega, \\xi$ with $\\omega_{t}^{X, 1} \\equiv t$ and $\\omega_{t}^{X, 2} \\equiv t^{2}$. For any $\\epsilon>0$ and any\n\n$$\nF \\in\\left\\{(X, t) \\mapsto \\Psi\\left(\\omega_{[0, t]}^{X}\\right) \\cdot X_{0}+\\int_{0}^{t} \\Phi\\left(\\omega_{[s, t]}^{X}\\right) \\cdot d \\xi_{s}^{X}\\right\\}\n$$\n\nwhere $\\Psi \\in C^{0}\\left(C_{1,0} ; \\mathbb{R}^{d_{0}}\\right)$ and $\\Phi \\in C^{0}\\left(C_{1,0} ; \\mathbb{R}^{d_{\\xi}}\\right)$, there exist a choice of hidden dimension $N \\geq 1$ and parameters $v \\in \\mathbb{R}^{N}, A_{i} \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\xi}}, C \\in \\mathbb{R}^{N \\times d_{0}}$ such that\n\n$$\n\\sup _{(X, t) \\in \\mathbb{K} \\times[0,1]}\\left|F(X, t)-\\left\\langle v, Z_{t}^{X}\\right\\rangle\\right| \\leq \\epsilon\n$$\n\nMoreover generic parameters suffice with high probability in the sense that under LeCun initialization\n\n$$\n\\left[A_{i}\\right]_{n, j} \\stackrel{\\text { iid }}{\\sim} \\mathcal{N}\\left(0, \\frac{1}{N}\\right) \\quad C_{n, j}, B_{n, j} \\stackrel{i i d}{\\sim} \\mathcal{N}(0,1)\n$$\n\nthe following holds:\n\n$$\n\\lim _{N \\rightarrow \\infty} \\mathbb{P}\\left[\\exists v \\in \\mathbb{R}^{N}: 37 \\text { holds }\\right]=1\n$$\n\nIf the $A_{i}$ s are constrained to be diagonal, as often is the case in practice, the requirements $\\omega_{t}^{X, 1} \\equiv t, \\omega_{t}^{X, 2} \\equiv t^{2}$ can be dropped and the existence result only holds with\n\n$$\nF \\in\\left\\{(X, t) \\mapsto \\psi\\left(\\omega_{t}^{X}\\right) \\cdot X_{0}+\\int_{0}^{t} \\phi\\left(\\omega_{t}^{X}-\\omega_{s}^{X}\\right) \\cdot d \\xi_{s}^{X}\\right\\}\n$$\n\nfor $\\psi \\in C^{0}\\left(\\mathbb{R}^{d_{\\omega}} ; \\mathbb{R}^{d_{0}}\\right)$ and $\\phi \\in C^{0}\\left(\\mathbb{R}^{d_{\\omega}} ; \\mathbb{R}^{d_{\\xi}}\\right)$. Moreover in both the dense and diagonal cases the \"reverse\" also holds in the sense that, given any choice of matrices $A_{i}, B, C$ there is an $\\epsilon$-close map $F$ in the corresponding family.",
    "theoryssms-35": "As one can see the theorem is composed of different sub-results, which we believe are better understood separately from each other. The proof will thus be split in the following steps:\n\n1. Using the theory developed in Appendix E we see how linear functions on the $Z_{t} \\mathrm{~s}$ can be seen as linear functions on certain terms of the Signature Transform. 2. Such terms define a feature map $T(X)_{0, t}$ which generates a Reproducing Kernel Hilbert Space $\\mathcal{H}_{t}^{(\\cdot)_{0}, \\omega, \\eta}$. This abstract space acts as an upper bound on expressivity: linear functions on $Z_{t}$ always belong to its closure (in uniform norm), independently of dimension and weights chosen, hence they cannot reach what functions in $\\mathcal{H}_{t}^{(\\cdot)_{0}, \\omega, \\eta}$ can't approximate. 3. The full expressive range of $\\mathcal{H}_{t}^{(\\cdot)_{0}, \\omega, \\eta}$ is shown to be captured by generic $Z_{t} \\mathrm{~s}$. 4. Diagonal systems are shown to be restricted to a subset of the $\\mathcal{H}_{t}^{(\\cdot)_{0}, \\omega, \\eta}$ of which they capture the full expressive range. ## B.3. Main Result - Proofs\n\n## B.3.1. AN EXPANSION FOR $Z_{t}^{X}$\n\nProposition B.4. For any choice of $A_{i} \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\xi}}$ and $C \\in \\mathbb{R}^{N \\times d_{0}}$, the unique solution to\n\n$$\n\\begin{aligned}\n& d Z_{t}^{X}=\\sum_{i=1}^{d_{\\omega}} A_{i} Z_{t}^{X} d \\omega_{t}^{X, i}+B d \\xi_{t}^{X} \\\\\n& Z_{0}^{X}=C X_{0} \\in \\mathbb{R}^{N}\n\\end{aligned}\n$$\n\nis given, using the notation $A_{I j}:=A_{j} A_{I}$, by\n\n$$\nZ_{t}^{X}=\\sum_{i=1}^{d_{0}} \\sum_{I \\in \\mathbb{W}_{d_{\\omega}}} A_{I} C_{i} X_{0}^{i} \\operatorname{Sig}\\left(\\omega^{X}\\right)_{0, t}^{I}+\\sum_{j=1}^{d_{\\xi}} \\sum_{I \\in \\mathbb{W}_{d_{\\omega}}} A_{I} B_{j} \\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{X}\\right)_{s, t}^{I} d \\xi_{s}^{X, j} \\in \\mathbb{R}^{N}\n$$\n\nNotice here $A_{I} C_{i}, A_{I} B_{j} \\in \\mathbb{R}^{N}$ and $X_{0}^{i} \\operatorname{Sig}\\left(\\omega^{X}\\right)_{0, t}^{I}, \\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{X}\\right)_{s, t}^{I} d \\xi_{s}^{X, j} \\in \\mathbb{R}$\n\nProof.",
    "theoryssms-36": "Just apply Theorems (E.2) and (E.6) of Appendix E\n\nRemark B.5. A property highlighted by the previous result is the interpretability of these models. After training the NCDEs one can compute the matrix multiplications and observe which entries of the signature the model chooses to take into consideration, to attend. ## B.3.2. THE FEATURE MAP $T$ AND ITS RKHS\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_84ab3f26c8b5e128d602g-18.jpg?height=44&width=1074&top_left_y=279&top_left_x=187)\n\n$$\nT(X)_{0, t}:=\\left(X_{0}^{i} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}^{I}, \\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I} d \\xi_{s}^{\\mathrm{X}, j}: i \\in\\left[d_{0}\\right], I \\in \\mathbb{W}_{d_{\\omega}}, j \\in\\left[d_{\\xi}\\right]\\right)\n$$\n\nThis feature vector can be understood as a tensor in the following way:\nDefinition B.6. Let $\\mathbb{W}_{d_{0}, d_{\\omega}, d_{\\xi}}$ be the set of words in the alphabet\n\n$$\n\\mathcal{A}_{d_{0}, d_{\\omega}, d_{\\xi}}:=\\left\\{\\boldsymbol{e}_{i}\\right\\}_{i=1, \\ldots, d_{0}} \\cup\\left\\{\\boldsymbol{\\epsilon}_{j}^{\\xi}\\right\\}_{j=1, \\ldots, d_{\\xi}} \\cup\\left\\{\\boldsymbol{\\epsilon}_{k}^{\\omega}\\right\\}_{k=1, \\ldots, d_{\\omega}}\n$$\n\nFixed the gates $(\\cdot)_{0}, \\omega, \\xi$ we define $T(X):[0,1] \\rightarrow l^{2}\\left(\\mathbb{W}_{d_{0}, d_{\\omega}, d_{\\xi}}\\right) \\subseteq T\\left(\\left(\\mathcal{A}_{d_{0}, d_{\\omega}, d_{\\xi}}\\right)\\right)$ as the unique solution to:\n\n$$\nT(X)_{0, t}=\\sum_{i=1}^{d} X_{0}^{i} \\boldsymbol{e}_{i}+\\sum_{j=1}^{d_{\\xi}} \\xi_{t}^{\\mathrm{X}, j} \\boldsymbol{\\epsilon}_{j}^{\\xi}+\\sum_{k=1}^{d_{\\omega}} \\int_{0}^{t} T(X)_{0, s} d \\omega_{s}^{\\mathrm{X}, k} \\otimes \\boldsymbol{\\epsilon}_{k}^{\\omega}\n$$\n\nIn fact one readily sees that the only non-zero terms of $T(X)_{0, t}$ defined as above are\n\n$$\n\\begin{aligned}\n& \\left\\langle T(X)_{0, t}, \\boldsymbol{e}_{i}\\right\\rangle=X_{0}^{i} \\\\\n& \\left\\langle T(X)_{0, t}, \\boldsymbol{e}_{i} \\otimes \\boldsymbol{\\epsilon}_{I k}^{\\omega}\\right\\rangle=\\int_{0}^{t}\\left\\langle T(X)_{0, s}, \\boldsymbol{e}_{i} \\otimes \\boldsymbol{\\epsilon}_{I}^{\\omega}\\right\\rangle d \\omega_{s}^{\\mathrm{X}, k}=X_{0}^{i} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}^{I k} \\\\\n& \\left\\langle T(X)_{0, t}, \\boldsymbol{\\epsilon}_{j}^{\\xi}\\right\\rangle=\\xi_{t}^{\\mathrm{X}, j}=\\int_{0}^{t} d \\xi_{s}^{\\mathrm{X}, j} \\\\\n& \\left\\langle T(X)_{0, t}, \\boldsymbol{\\epsilon}_{j}^{\\xi} \\otimes \\boldsymbol{\\epsilon}_{I k}^{\\omega}\\right\\rangle=\\int_{0}^{t}\\left\\langle T(X)_{0, s}, \\boldsymbol{\\epsilon}_{j}^{\\xi} \\otimes \\boldsymbol{\\epsilon}_{I}^{\\omega}\\right\\rangle d \\omega_{s}^{\\mathrm{X}, k}=\\int_{s=0}^{t} \\int_{r=0}^{s} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{r, s}^{I} d \\xi_{r}^{\\mathrm{X}, j} d \\omega_{s}^{\\mathrm{X}, k} \\\\\n& \\quad=\\int_{r=0}^{t} \\int_{s=r}^{t} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{r, s}^{I} d \\omega_{s}^{\\mathrm{X}, k} d \\xi_{r}^{\\mathrm{X}, j}=\\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{r, t}^{I} d \\xi_{r}^{\\mathrm{X}, j}\n\\end{aligned}\n$$\n\nThis is similar to the tensor-valued CDE defining the signature as a tensor i.e. Salvi et al. 2021a)\n\n$$\n\\operatorname{Sig}(\\omega)_{0, t}=()+\\int_{0}^{t} \\operatorname{Sig}(\\omega)_{0, s} \\otimes d \\omega_{s}\n$$\n\nwith the addition of two terms to track $X_{0}$ and $\\xi^{\\mathrm{X}}$. One could also understand $T(X)_{s, t}$ as a sub-tensor of\n\n$$\nX_{0} \\otimes \\operatorname{Sig}\\left(\\left(\\omega^{\\mathrm{x}}, \\xi^{\\mathrm{X}}\\right)\\right)_{s, t}\n$$\n\nbut in doing this one would have to explicitly ignore most of the terms of this vector; the CDE (36) does exactly this, but implicitly. In any case the subtensor view shows that $T: \\mathbb{X} \\times[0,1]^{2} \\rightarrow l^{2}\\left(\\mathbb{W}_{d_{0}, d_{\\omega}, d_{\\xi}}\\right)$ is well defined and continuous. To the feature map $T(\\cdot)_{0, t}$ with values in the Hilbert space $l^{2}\\left(\\mathbb{W}_{d_{0}, d_{\\omega}, d_{\\xi}}\\right)$ is then associated a Reproducing Kernel Hilbert Space Berlinet \\& Thomas-Agnan, 2011, where the Kernel is the one induced by the $l^{2}$ product, which we denote by\n\n$$\n\\mathcal{H}_{t}^{(\\cdot)_{0}, \\omega, \\eta} \\subseteq C^{0}(\\mathbb{X} ; \\mathbb{R})\n$$\n\nClassical RKHS theory tells us that we can characterize its elements as:\nProposition B.7. A map $F(\\cdot)_{t}: \\mathbb{X} \\rightarrow \\mathbb{R}$ is an element of $\\mathcal{H}_{t}^{(\\cdot)_{0}, \\omega, \\eta}$ if and only if it is of the form\n\n$$\nF(x)_{t}=\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{X}\\right)_{0, t}\\right\\rangle+\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\beta_{j}, \\operatorname{Sig}\\left(\\omega^{X}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{X, j}\n$$\n\nfor $\\alpha_{i}, \\beta_{j} \\in l^{2}\\left(\\mathbb{W}_{d_{\\omega}}\\right)$. Moreover $\\left\\|F(\\cdot)_{t}\\right\\|_{\\mathcal{H}_{t}^{(\\cdot)}, \\omega, \\eta}^{2}$ is equal to the minimal value of\n\n$$\n\\sum_{i=1}^{d_{0}}\\left\\|\\alpha_{i}\\right\\|_{l^{2}\\left(\\mathbb{W}_{d_{\\omega}}\\right)}^{2}+\\sum_{j=1}^{d_{\\xi}}\\left\\|\\beta_{j}\\right\\|_{l^{2}\\left(\\mathbb{W}_{d_{\\omega}}\\right)}^{2}\n$$\n\ntaken over those $\\gamma, \\beta$ for which the above equality holds. Signature kernels (Salvi et al, 2021a) are a class of universal kernels on sequential data which have received attention in recent years thanks to their efficiency in handling path-dependent problems (Lemercier et al., 2021; Salvi et al., 2021c, Cochrane et al., 2021; Salvi et al. 2021b, Cirone et al., 2023; Issa et al., 2023). Just as signature kernels, the kernel associated to $T(X)_{0, t}$ can be explicitly written as the solution of a two-parameter CDE:\nLemma B.8. Let $\\mathcal{K}^{X, Y}(s, t):=\\left\\langle T(X)_{0, s}, T(Y)_{0, t}\\right\\rangle_{l^{2}}$ then\n\n$$\n\\mathcal{K}^{X, Y}(s, t)=\\left\\langle X_{0}, Y_{0}\\right\\rangle+\\left\\langle\\xi_{s}^{X}, \\xi_{t}^{Y}\\right\\rangle+\\int_{\\eta=0}^{s} \\int_{\\tau=0}^{t} \\mathcal{K}^{X, Y}(\\eta, \\tau)\\left\\langle d \\omega_{\\eta}^{X}, d \\omega_{\\tau}^{Y}\\right\\rangle\n$$\n\nor, directly in terms of Signature, also\n\n$$\n\\mathcal{K}^{X, Y}(s, t)=\\left\\langle X_{0}, Y_{0}\\right\\rangle\\left\\langle\\operatorname{Sig}\\left(\\omega^{X}\\right)_{0, s}, \\operatorname{Sig}\\left(\\omega^{Y}\\right)_{0, t}\\right\\rangle+\\int_{\\eta=0}^{s} \\int_{\\tau=0}^{t}\\left\\langle\\operatorname{Sig}\\left(\\omega^{X}\\right)_{\\eta, s}, \\operatorname{Sig}\\left(\\omega^{Y}\\right)_{\\tau, t}\\right\\rangle\\left\\langle d \\xi_{\\eta}^{X}, d \\xi_{\\tau}^{Y}\\right\\rangle\n$$\n\nProof. The first expression follows immediately from Eqn. 36p the second one by summing the products of $T(X)_{0, s}$ 's and $T(Y)_{0, t}$ 's entries given above. Definition B.9. Define the space $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta} \\subseteq C^{0}(\\mathbb{X} \\times[0,1] ; \\mathbb{R})$ as the space of functions of form\n\n$$\n(X, t) \\mapsto \\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle+\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\beta_{j}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j}\n$$\n\nfor $\\alpha_{i}, \\beta_{j} \\in l^{2}\\left(\\mathbb{W}_{d_{\\omega}}\\right)$. Thus for all $t \\in[0,1]$ and $F \\in \\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$ it holds $F(\\cdot, t) \\in \\mathcal{H}_{t}^{(\\cdot)_{0}, \\omega, \\eta}$. ## B.3.3. LINEAR MAPS ON $Z_{t}^{\\mathrm{X}}$ ARE CLOSE TO THE RKHS\n\nThe following proposition will show how linear maps on $Z_{t}^{\\mathrm{X}}$ cannot be more expressive than elements of the RKHS $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$ since their closure is in the closure of $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$. In this precise sense these spaces act like upper bounds to expressiveness.",
    "theoryssms-37": "Proposition B.10. Assume $\\mathbb{X}$ compact. Consider fixed the gates and $A_{i} \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\xi}}$ and $C \\in \\mathbb{R}^{N \\times d_{0}}$. Consider a linear readout $v \\in \\mathbb{R}^{N}$. For any $\\epsilon>0$ there exist choices of $\\alpha_{i}, \\beta_{j} \\in l^{2}\\left(\\mathbb{W}_{d_{\\omega}}\\right)$ such that\n\n$$\n\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|\\left\\langle v, Z_{t}^{X}\\right\\rangle-F(X, t)\\right| \\leq \\epsilon\n$$\n\nwhere $F \\in \\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$. In other words, linear maps on the $Z_{t}^{X}$ are in the uniform closure of $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$. Proof. Using Eqn. 34p we see that $\\left\\langle v, Z_{t}^{X}\\right\\rangle$ is a linear map on $T(X)_{0, t}$ with coefficients\n\n$$\nv^{\\top} A_{I} B_{j} \\quad v^{\\top} A_{I} C_{i}\n$$\n\nusing Cauchy-Schwartz it's moreover easy to see the existence of a constant $\\lambda \\geq 0$ such that for all $I, i, j$ one has\n\n$$\n\\left|v^{\\top} A_{I} B_{j}\\right| \\leq \\lambda^{|I|} \\quad\\left|v^{\\top} A_{I} C_{i}\\right| \\leq \\lambda^{|I|}\n$$\n\nSince $\\left|\\operatorname{Sig}(\\omega)_{s, t}^{I}\\right| \\leq \\frac{1}{|I|!}\\|\\omega\\|_{1-v a r,[s, t]}^{|I|}$ we have that, given an integer $M \\geq 0$, the bound\n\n$$\n\\begin{aligned}\nR_{M}(t) & :=\\left|\\sum_{i=1}^{d_{0}} \\sum_{|I| \\geq M} v^{\\top} A_{I} C_{i} X_{0}^{i} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}^{I}+\\sum_{j=1}^{d_{\\xi}} \\sum_{|I| \\geq M} v^{\\top} A_{I} B_{j} \\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I} d \\xi_{s}^{\\mathrm{X}, j}\\right| \\\\\n& \\leq\\left(\\left\\|X_{0}\\right\\|_{1}+\\left\\|\\xi_{t}^{\\mathrm{X}}\\right\\|_{1}\\right) \\sum_{m=M}^{\\infty} \\frac{\\lambda^{m} d_{\\omega}^{m}\\left\\|\\omega^{\\mathrm{X}}\\right\\|_{1-v a r,[0, t]}^{m}}{m!} \\\\\n& \\leq\\left(\\left\\|X_{0}\\right\\|_{1}+\\left\\|\\xi_{t}^{\\mathrm{X}}\\right\\|_{1}\\right) \\sum_{m=M}^{\\infty} \\frac{\\lambda^{m} d_{\\omega}^{m}\\left\\|\\omega^{\\mathrm{X}}\\right\\|_{1-v a r,[0,1]}^{m}}{m!} \\leq K \\sum_{m=M}^{\\infty} \\frac{\\left(\\lambda d_{\\omega} K\\right)^{m}}{m!}\n\\end{aligned}\n$$\n\nwhere $K \\geq 0$ is a constant which must exist by compactness of $\\mathbb{X}$ and continuity of the gates. Since $K \\sum_{m=M}^{\\infty} \\frac{\\left(\\lambda d_{\\omega} K\\right)^{m}}{m!}$ is just the tail of the taylor expansion of $K e^{\\lambda d_{\\omega} K}$ there must be an $M$ such that $\\sup _{t \\in[0,1]} R_{M}(t) \\leq \\epsilon$. But then the choice\n\n$$\n\\alpha_{i}^{I}:=v^{\\top} A_{I} C_{i} \\mathbb{I}(|I|<M) \\quad \\beta_{j}^{I}:=v^{\\top} A_{I} B_{j} \\mathbb{I}(|I|<M)\n$$\n\nsuffices for the required bound. ## B.3.4. UNIFORM CLOSURE OF THE RKHS\n\nNow that we have established the theoretical interest of the $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$ we proceed to characterize which maps $(X, t) \\rightarrow \\mathbb{R}$ can be uniformly approximated trough them. Proposition B.11. Fix a compact input set $\\mathbb{X}$ and continuous gates $(\\cdot)_{0}, \\omega, \\xi$ with $\\omega_{t}^{X, 1} \\equiv t$ and $\\omega_{t}^{X, 2} \\equiv t^{2}$. For any $\\epsilon>0$ and any\n\n$$\nF \\in\\left\\{(X, t) \\mapsto \\Psi\\left(\\omega_{[0, t]}^{X}\\right) \\cdot X_{0}+\\int_{0}^{t} \\Phi\\left(\\omega_{[s, t]}^{X}\\right) \\cdot d \\xi_{s}^{X}\\right\\}\n$$\n\nwhere $\\Psi \\in C^{0}\\left(C_{1,0} ; \\mathbb{R}^{d_{0}}\\right)$ and $\\Phi \\in C^{0}\\left(C_{1,0} ; \\mathbb{R}^{d_{\\xi}}\\right)$, there exist a $G \\in \\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$ such that\n\n$$\n\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}|F(X, t)-G(X, t)| \\leq \\epsilon\n$$\n\nProof. Note first that the map\n\n$$\n\\mathbb{X} \\times[0,1] \\times[0,1] \\rightarrow C_{1,0}\\left([0,1] ; \\mathbb{R}^{d_{\\omega}}\\right) \\quad(X, s, t) \\mapsto \\omega_{[s, t]}^{\\mathrm{X}}\n$$\n\nis a continuous map from a compact space, thus the image must be compact too.",
    "theoryssms-38": "Moreover by Prop. A. 8 the Signature separates the points in this image. Since any $G$ as above has form\n\n$$\n\\begin{aligned}\nG(X, t) & =\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle+\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\beta_{j}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j} \\\\\n& =\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\alpha_{i}, \\operatorname{Sig}\\left(\\omega_{[0, t]}^{\\mathrm{X}}\\right)_{0,1}\\right\\rangle+\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\beta_{j}, \\operatorname{Sig}\\left(\\omega_{[s, t]}^{\\mathrm{X}}\\right)_{0,1}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j}\n\\end{aligned}\n$$\n\nthe proof follows from the uniform density on compact sets of linear functionals on the (truncated) Signature (Thm. A.5), by also uniformly bounding thanks to compactness and continuity the norms of $X_{0}$ and $\\xi_{1}^{\\mathrm{X}}$. ## B.3.5. GENERIC WeIGHTS ARE FULLY EXPRESSIVE\n\nWe have seen how linear maps on $Z_{t}^{\\mathrm{X}}$ are in the uniform closure of $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$, and we have explicitly characterized this closure. It is then natural to ask \"how much\" of this closure the $Z_{t}^{\\mathrm{x}}$ are able to \"explore\". The present section not only shows that the $Z_{t}^{\\mathrm{X}}$ \"explore\" all the closure, but also that a generic choice of weights is enough to eventually do this with high probability. The fact that these maps are \"universal\" in the above sense is not surprising, since it is well known that Linear CDEs are universal for path-to-point tasks $c f$. (Kidger, 2022), what is surprising is that this universality can be achieved probabilistically with one of the standard parametrizations used in ML practice (LeCun)\n\nTheorem B.12. Fix $\\mathbb{X}$ compact and $\\epsilon>0$. For all $F \\in \\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$ there exist a choice of hidden dimension $N \\geq 1$ and parameters $v \\in \\mathbb{R}^{N}, A_{i} \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\xi}}, C \\in \\mathbb{R}^{N \\times d_{0}}$ such that\n\n$$\n\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|F(X, t)-\\left\\langle v, Z_{t}^{X}\\right\\rangle\\right| \\leq \\epsilon\n$$\n\nMoreover generic weight choices suffice with high probability, in the sense that under LeCun initialization\n\n$$\n\\left[A_{j}\\right]_{n, n^{\\prime}} \\stackrel{i i d}{\\sim} \\mathcal{N}\\left(0, \\frac{1}{N}\\right) \\quad[C]_{n, i},[B]_{n, j} \\stackrel{i i d}{\\sim} \\mathcal{N}(0,1)\n$$\n\nthe following holds\n\n$$\n\\lim _{N \\rightarrow \\infty} \\mathbb{P}\\left[\\exists v \\in \\mathbb{R}^{N}: \\text { 45) holds }\\right]=1\n$$\n\nWe propose two proofs, the first one of a deterministic character concerns the first claim in the theorem, the second one is probabilistic and concerns the whole result. The deterministic proof follows the same arguments employed by (Kidger, 2022) and is included to highlight the main idea of the probabilistic result, which reduces to a \"spin\" on the central argument of this proof. Deterministic Proof. Any $F \\in \\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$ has form\n\n$$\nF(X, t)=\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle+\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\beta_{j}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{x}}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j}\n$$\n\nfor fixed $\\alpha_{i}, \\beta_{j} \\in l^{2}\\left(\\mathbb{W}_{d_{\\omega}}\\right)$. Consider an integer $M \\geq 0$ such that\n\n$$\n\\sup _{(x, t) \\in \\mathbb{X} \\times[0,1]}\\left|F(X, t)-\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\pi_{M} \\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle-\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\pi_{M} \\beta_{j}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j}\\right| \\leq \\epsilon\n$$\n\nwhere $\\pi_{M}$ is the truncation at length $M$. Fix $d=d_{0}+d_{\\omega}+d_{\\xi}$. Consider $\\mu(M, d) \\in \\mathbb{N}$ such that $\\mathbb{R}^{\\mu(M, d)} \\simeq T^{M}\\left(\\mathbb{R}^{d}\\right)$. We are going to write $e_{I} \\in \\mathbb{R}^{\\mu(M, d)}$ to mean the image of $e_{I} \\in T^{M}\\left(\\mathbb{R}^{d}\\right)$ trough this identification. Note that $(\\cdot) \\otimes_{M} e_{k}: T^{M}\\left(\\mathbb{R}^{d}\\right) \\rightarrow T^{M}\\left(\\mathbb{R}^{d}\\right)$ is a linear map, it does then correspond to a matrix $\\Lambda_{k} \\in \\mathbb{R}^{\\mu(M, d) \\times \\mu(M, d)}$. Write $\\varepsilon_{j}^{\\xi}:=e_{d_{0}+j}$ for $j=1, \\ldots, d_{\\xi}$ and $\\varepsilon_{k}^{\\omega}:=e_{d_{0}+d_{\\xi}+k}$ for $k=1, \\ldots, d_{\\omega}$. Then the solution to\n\n$$\n\\mathbb{R}^{\\mu(M, d)} \\ni \\tilde{Z}_{t}=\\sum_{i=1}^{d_{0}} X_{0}^{i} e_{i}+\\sum_{j=1}^{d_{\\xi}} \\xi_{t}^{\\mathrm{X}, j} \\varepsilon_{j}^{B}+\\sum_{k=1}^{d_{\\omega}} \\int_{0}^{t} \\Lambda_{d_{0}+d_{\\xi}+k} \\tilde{Z}_{s} d \\omega_{s}^{\\mathrm{X}, k}\n$$\n\nis the object in $\\mathbb{R}^{\\mu(M, d)}$ corresponding to the truncated tensor $\\pi_{M}\\left(T(X)_{0, t}\\right)$. This $\\tilde{Z}_{t}$ is of the form $Z_{t}^{\\mathrm{X}}$ with $N=\\mu(M, d), A_{k}=\\Lambda_{d_{0}+d_{\\xi}+k}, B=\\left[\\varepsilon_{1}^{\\xi}|\\cdots| \\varepsilon_{d_{\\xi}}^{\\xi}\\right]$ and $C=\\left[e_{1}|\\cdots| e_{d_{0}}\\right]$\nIn particular note how these matrices are such that\n\n$$\ne_{J}^{T} A_{I} C_{i}=\\mathbb{I}\\left(e_{J}=e_{i} \\otimes \\varepsilon_{I}^{\\omega}\\right), \\quad e_{J}^{T} A_{I} B_{j}=\\mathbb{I}\\left(e_{J}=\\varepsilon_{j}^{\\xi} \\otimes \\varepsilon_{I}^{\\omega}\\right)\n$$\n\nsince it holds that\n\n$$\nA_{I} C_{i}=e_{i} \\otimes \\varepsilon_{I}^{\\omega}, \\quad A_{I} B_{j}=\\varepsilon_{j}^{\\xi} \\otimes \\varepsilon_{I}^{\\omega}\n$$\n\n[^10]and for all $|I|>M$ one has necessarily $A_{I}=0$. Our strategy is that of using these equaities to create a vector $v \\in \\mathbb{R}^{N}$ corresponding to the $\\pi_{M} \\alpha_{i}$ and $\\pi_{M} \\beta_{j}$. Define the vector\n$$\nv:=\\sum_{i=1}^{d_{0}} \\sum_{|I| \\leq M} \\alpha_{i}^{I} e_{i} \\otimes \\varepsilon_{I}^{\\omega}+\\sum_{j=1}^{d_{\\omega}} \\sum_{|I| \\leq M} \\beta_{j}^{I} \\varepsilon_{j}^{\\xi} \\otimes \\varepsilon_{I}^{\\omega} \\in \\mathbb{R}^{\\mu(M, d)}\n$$\n\nThen expanding $Z_{t}^{\\mathrm{X}}$ as in 34 and using the equalities above one has\n\n$$\n\\begin{aligned}\n\\left\\langle v, Z_{t}^{\\mathrm{X}}\\right\\rangle & =\\sum_{i=1}^{d_{0}} \\sum_{I} v^{\\top} A_{I} C_{i} X_{0}^{i} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}+\\sum_{j=1}^{d_{\\omega}} \\sum_{I} v^{\\top} A_{I} B_{j} \\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t} d \\xi_{s}^{\\mathrm{X}, j} \\\\\n& =\\sum_{i=1}^{d_{0}} \\sum_{|I| \\leq M} \\alpha_{i}^{I} X_{0}^{i} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}+\\sum_{j=1}^{d_{\\xi}} \\sum_{|I| \\leq M} \\beta_{j}^{I} \\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t} d \\xi_{s}^{\\mathrm{X}, j} \\\\\n& =\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\pi_{M} \\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle+\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\pi_{M} \\beta_{j}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j}\n\\end{aligned}\n$$\n\nproving that for such $v$ and $Z^{\\mathrm{X}}$ it holds\n\n$$\n\\sup _{(x, t) \\in \\mathbb{X} \\times[0,1]}\\left|F(X, t)-\\left\\langle v, Z_{t}^{\\mathbb{X}}\\right\\rangle\\right| \\leq \\epsilon\n$$\n\nThe crucial ingredient for the success of this proof is the possibility to recreate the space $T^{M}\\left(\\mathbb{R}^{d_{0}+d_{\\omega}+d_{\\xi}}\\right)$ as an euclidean space. To do this one needs $\\mu\\left(M, d_{0}+d_{\\omega}+d_{\\xi}\\right) \\sim\\left(d_{0}+d_{\\omega}+d_{\\xi}\\right)^{M}$ orthogonal vectors and a way to express them using the matrices $A_{i}, B$ and $C$, the essential equations which capture this are given by (49). The core idea of the following probabilistic proof of this same result is that of allowing for some error in (49), so the idea is that of exhibiting only approximately orthogonal vectors. At the cost of losing exactness, one can leverage results of the Johnson-Lindenstrauss (Dasgupta \\& Gupta 2003) type to find on the order of $\\sim e^{\\varepsilon^{2} N}$ vectors in $\\mathbb{R}^{N}$ orthogonal up to an $\\varepsilon$ error, using random projections. This idea in the context of Signature goes back to (Cuchiero et al. 2021a), and allows for much smaller hidden dimensions. Proof. (Probabilistic Proof) Any $F \\in \\mathcal{H}_{[0,1]}^{(\\cdot) \\cdot, \\omega, \\eta}$ has form\n\n$$\nF(X, t)=\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle+\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\beta_{j}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j}\n$$\n\nfor fixed $\\alpha_{i}, \\beta_{j} \\in l^{2}\\left(\\mathbb{W}_{d_{\\omega}}\\right)$. Consider an integer $M \\geq 0$ such that\n\n$$\n\\sup _{(x, t) \\in \\mathbb{X} \\times[0,1]}\\left|F(X, t)-\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\pi_{M} \\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle-\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\pi_{M} \\beta_{j}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j}\\right| \\leq \\epsilon\n$$\n\nwhere $\\pi_{M}$ is the truncation at length $M$. From (Cirone et al. 2023) [Appendix C] we know that\n\n$$\n\\begin{aligned}\n\\left\\|\\frac{1}{N} C_{i}^{\\top} A_{I}^{\\top} A_{J} C_{j}-\\delta_{i I}^{j J}\\right\\|_{L^{2}} & =\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right) 2^{\\frac{|I I+| J \\mid}{2}}(|I|+|J|)!! \\\\\n\\left\\|\\frac{1}{N} C_{i}^{\\top} A_{I}^{\\top} A_{J} B_{j}\\right\\|_{L^{2}} & =\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right) 2^{\\frac{|I I+| J \\mid}{2}}(|I|+|J|)!! \\\\\n\\left\\|\\frac{1}{N} B_{i}^{\\top} A_{I}^{\\top} A_{J} B_{j}-\\delta_{i I}^{j J}\\right\\|_{L^{2}} & =\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right) 2^{\\frac{|I|+|J|}{2}}(|I|+|J|)!! \\end{aligned}\n$$\n\nOur strategy is that of using these bounds to create a vector $v \\in \\mathbb{R}^{N}$ \"acting\" like the $\\pi_{M} \\alpha_{i}$ and $\\pi_{M} \\beta_{j}$. Define, noting that the $A_{i}, C_{i}, B_{j}$ depend on $N$, the vector\n\n$$\nv^{\\mathrm{N}}:=\\frac{1}{N}\\left(\\sum_{i=1}^{d_{0}} \\sum_{|I| \\leq M} \\alpha_{i}^{I} A_{I} C_{i}+\\sum_{j=1}^{d_{\\omega}} \\sum_{|I| \\leq M} \\beta_{j}^{I} A_{I} B_{j}\\right)\n$$\n\nThen expanding $Z_{t}^{\\mathrm{x}}$ as in 34\n\n$$\n\\begin{aligned}\nR_{M}:= & \\left\\|\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|\\left\\langle v^{\\mathrm{N}}, Z_{t}^{\\mathrm{X}}\\right\\rangle-\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\pi_{M} \\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle-\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\pi_{M} \\beta_{j}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j}\\right|\\right\\|_{L^{2}} \\\\\n\\leq & \\sum_{i=1}^{d_{0}} \\sum_{|I| \\leq M}\\left\\|\\left(v^{\\mathrm{N}}\\right)^{\\top} A_{I} C_{i}-\\alpha_{i}^{I}\\right\\|_{L^{2}} \\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|X_{0}^{i} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}^{I}\\right| \\\\\n& ++\\sum_{i=1}^{d_{0}} \\sum_{|I|>M}\\left\\|\\left(v^{\\mathrm{N}}\\right)^{\\top} A_{I} C_{i}\\right\\|_{L^{2}} \\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|X_{0}^{i} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}^{I}\\right| \\\\\n& +\\sum_{j=1}^{d_{\\omega}} \\sum_{|I| \\leq M}\\left\\|\\left(v^{\\mathrm{N}}\\right)^{\\top} A_{I} B_{j}-\\beta_{j}^{I}\\right\\|_{L^{2}} \\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|\\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I} d \\xi_{s}^{\\mathrm{X}, j}\\right| \\\\\n& ++\\sum_{j=1}^{d_{\\omega}} \\sum_{|I|>M}\\left\\|\\left(v^{\\mathrm{N}}\\right)^{\\top} A_{I} B_{j}\\right\\|_{L^{2}} \\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|\\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I} d \\xi_{s}^{\\mathrm{X}, j}\\right|\n\\end{aligned}\n$$\n\nNote how for $|I| \\leq M$ one has\n\n$$\n\\left\\|\\left(v^{\\mathrm{N}}\\right)^{\\top} A_{I} C_{i}-\\alpha_{i}^{I}\\right\\|_{L^{2}} \\leq \\mathcal{O}_{M}\\left(\\frac{1}{\\sqrt{N}}\\right)\n$$\n\nand that similarly for $|I|>M$\n\n$$\n\\left\\|\\left(v^{\\mathrm{N}}\\right)^{\\top} A_{I} C_{i}\\right\\|_{L^{2}} \\leq \\mathcal{O}_{M}\\left(\\frac{1}{\\sqrt{N}}\\right) 2^{\\frac{|I|}{2}}(M+|I|)!! $$\n\nWhich leads, thanks to the same bounds of (Cirone et al, 2023) [Appendix C], to\n\n$$\nR_{M}=\\frac{1}{\\sqrt{N}} \\mathcal{O}_{M, \\mathbb{X}}(1)\n$$\n\nBut then by Markov's inequality it holds that\n\n$$\n\\mathbb{P}\\left[\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|\\left\\langle v^{\\mathrm{N}}, Z_{t}^{\\mathrm{X}}\\right\\rangle-\\sum_{i=1}^{d_{0}} X_{0}^{i}\\left\\langle\\pi_{M} \\alpha_{i}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle-\\sum_{j=1}^{d_{\\xi}} \\int_{0}^{t}\\left\\langle\\pi_{M} \\beta_{j}, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}\\right\\rangle d \\xi_{s}^{\\mathrm{X}, j}\\right| \\leq \\epsilon\\right] \\rightarrow 1\n$$\n\nand thus there must be a choice of $N,\\left\\{A_{i}\\right\\}, B, C$ such that the inequality holds, and we thus obtain using 54\n\n$$\n\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|F(X, t)-\\left\\langle v^{\\mathrm{N}}, Z_{t}^{\\mathrm{X}}\\right\\rangle\\right| \\leq 2 \\epsilon\n$$\n\nand we conclude by arbitrariness of $\\epsilon$. ## B.4. The Diagonal Case\n\nHere we study the particular, but empirically important, case where the matrices $A_{i}$ are taken to be diagonal\nWhat we'll discover is that the $Z_{t}^{\\mathrm{X}}$ cannot differentiate between $\\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I}$ and other $\\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{\\sigma(I)}$ for any permutation $\\sigma$ of the letters in the word $I$. [^11]\n## B.4.1. DiAGONAL EXPANSION FOR $Z_{t}^{X}$\n\nProposition B.13. For any choice of $V \\in \\mathbb{R}^{N \\times d_{\\omega}}, B \\in \\mathbb{R}^{N \\times d_{\\xi}}$ and $C \\in \\mathbb{R}^{N \\times d_{0}}$, writing $A_{i}:=\\operatorname{diag}\\left(V_{i}\\right)$, the unique solution to\n\n$$\n\\begin{aligned}\n& d Z_{t}^{X}=\\sum_{i=1}^{d_{\\omega}} A_{i} Z_{t}^{X} d \\omega_{t}^{X, i}+B d \\xi_{t}^{X} \\\\\n& Z_{0}^{X}=C X_{0} \\in \\mathbb{R}^{N}\n\\end{aligned}\n$$\n\nis given by\n\n$$\nZ_{t}^{X}=e^{\\operatorname{diag}\\left(V \\omega_{t}^{X}\\right)} C X_{0}+\\int_{0}^{t} e^{\\operatorname{diag}\\left(V\\left(\\omega_{t}^{X}-\\omega_{s}^{X}\\right)\\right)} B d \\xi_{s}^{X}\n$$\n\nwhich can be expanded as\n\n$$\nZ_{t}^{X}=\\sum_{i=1}^{d_{0}} \\sum_{I \\in \\mathbb{W}_{d_{\\omega}}} A_{I}^{s y m} C_{i} X_{0}^{i} S i g\\left(\\omega^{X}\\right)_{0, t}^{s y m, I}+\\sum_{j=1}^{d_{\\xi}} \\sum_{I \\in \\mathbb{W}_{d_{\\omega}}} A_{I}^{s y m} B_{j} \\int_{0}^{t} \\operatorname{Sig}\\left(\\omega^{X}\\right)_{s, t}^{s y m, I} d \\xi_{s}^{X, j} \\in \\mathbb{R}^{N}\n$$\n\nwhere\n\n$$\nA_{I}^{s y m}:=\\frac{1}{|I|!} \\sum_{\\sigma \\in S_{k}} A_{\\sigma(I)}=A_{I} \\quad \\operatorname{Sig}\\left(\\omega^{X}\\right)_{s, t}^{s y m, I}:=\\frac{1}{|I|!} \\sum_{\\sigma \\in S_{k}} \\operatorname{Sig}\\left(\\omega^{X}\\right)_{s, t}^{\\sigma(I)}\n$$\n\nProof. By Theorem E. 1 and Theorem E. 6 we know that the solution of\n\n$$\nZ_{t}^{\\mathrm{X}}=Z_{0}^{\\mathrm{X}}+\\sum_{i=1}^{d_{\\omega}} \\int_{0}^{t} A_{i} Z_{t}^{\\mathrm{X}} d \\omega_{t}^{\\mathrm{X}, i}+\\int_{0}^{t} B d \\xi_{t}^{\\mathrm{x}}\n$$\n\nis explicitly given by\n\n$$\nZ_{t}^{\\mathrm{x}}=W_{0, t}^{\\mathrm{x}} Z_{0}^{\\mathrm{x}}+\\int_{0}^{t} W_{s, t}^{\\mathrm{x}} B d \\xi_{s}^{\\mathrm{x}}\n$$\n\nwhere $W_{s, t}$ is the unique solution to\n\n$$\nW_{s, t}^{\\mathrm{X}}=I d+\\sum_{i=1}^{d_{\\omega}} \\int_{s}^{t} A_{i} W_{s, r}^{\\mathrm{X}} d \\omega_{r}^{\\mathrm{X}, i}\n$$\n\nIn case the $A_{i}$ are commuting matrices one can explicitly write the solution as\n\n$$\nW_{s, t}^{\\mathrm{X}}=\\exp \\left(\\sum_{i=1}^{d_{\\omega}} \\int_{s}^{t} A_{i} d \\omega_{r}^{\\mathrm{X}, i}\\right)=\\exp \\left(\\operatorname{diag}\\left(V\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right)\\right)\\right)\n$$\n\nsince for fixed $s$ one has, using commutativity, that\n\n$$\nd W_{s, t}^{\\mathrm{X}}=W_{s, t}^{\\mathrm{X}}\\left(\\sum_{i=1}^{d_{\\omega}} A_{i} d \\omega_{t}^{\\mathrm{X}, i}\\right)=\\sum_{i=1}^{d_{\\omega}} A_{i} W_{s, t}^{\\mathrm{X}} d \\omega_{t}^{\\mathrm{X}, i}\n$$\n\nOn the other hand we know, Theorem E.2, that\n\n$$\nW_{s, t}^{\\mathrm{x}}=\\sum_{I \\in \\mathbb{W}_{d_{\\omega}}} A_{I} \\operatorname{Sig}\\left(\\omega^{\\mathrm{x}}\\right)_{s, t}^{I}=\\sum_{k=0}^{\\infty} \\sum_{I \\in \\mathbb{W}_{d_{\\omega}}^{k}} A_{I} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I}\n$$\n\nThe two views are reconciled by noticing that the symmetric group $S_{k}$ acts on $\\mathbb{W}_{d_{\\omega}}^{k}$, the space of words of lenth $k$, by permuting the letters and, by commutativity,\n\n$$\n\\forall \\sigma \\in S_{k} \\cdot \\forall I \\in \\mathbb{W}_{d_{\\omega}}^{k} \\cdot A_{I}=A_{\\sigma(I)}\n$$\n\nThen we have\n\n$$\n\\sum_{I \\in \\mathbb{W}_{d_{\\omega}}^{k}} A_{I} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I}=\\sum_{I \\in \\mathbb{W}_{d_{\\omega}}^{k}} \\frac{1}{k!} \\sum_{\\sigma \\in S_{k}} A_{\\sigma(I)} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{\\sigma(I)}=\\sum_{I \\in \\mathbb{W}_{d_{\\omega}}^{k}} \\frac{A_{I}}{k!} \\sum_{\\sigma \\in S_{k}} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{\\sigma(I)}\n$$\n\nrecalling then how $e_{I_{1}} \\amalg \\cdots e_{I_{k}}=\\sum_{\\sigma \\in S_{k}} e_{\\sigma(I)}$ we get to\n\n$$\n\\begin{aligned}\n\\sum_{I \\in \\mathbb{W}_{d_{\\omega}}^{k}} A_{I} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I} & =\\sum_{I \\in \\mathbb{W}_{a_{\\omega}}^{K}} \\frac{A_{I}}{k!} \\sum_{\\sigma \\in S_{k}} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{\\sigma(I)}=\\sum_{I \\in \\mathbb{W}_{a_{\\omega}}^{K}} \\frac{A_{I}}{k!} \\prod_{i=1}^{k} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I_{i}} \\\\\n& =\\sum_{I \\in \\mathbb{W}_{d_{\\omega}}^{k}} \\frac{1}{k!} \\prod_{i=1}^{k} A_{I_{i}} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{I_{i}}=\\frac{1}{k!}\\left(\\sum_{i=1}^{d_{\\omega}} A_{i} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{i}\\right)^{k}=\\frac{1}{k!}\\left(\\sum_{i=1}^{d_{\\omega}} \\int_{s}^{t} A_{i} d \\omega_{r}^{\\mathrm{X}, i}\\right)^{k}\n\\end{aligned}\n$$\n\nIn particular we see how in the commuting case\n\n$$\nW_{s, t}^{\\mathrm{X}}=\\sum_{I \\in \\mathbb{W}_{d_{\\omega}}} A_{I}^{s y m} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{s y m}, I\n$$\n\nwhere\n\n$$\nA_{I}^{s y m}:=\\frac{1}{|I|!} \\sum_{\\sigma \\in S_{k}} A_{\\sigma(I)}=A_{I} \\quad \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{s y m, I}:=\\frac{1}{|I|!} \\sum_{\\sigma \\in S_{k}} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{\\sigma(I)}\n$$\n\n## B.4.2. DiagonAL EXPRESSIVENESS\n\nTheorem B.14. Fix a compact input set $\\mathbb{X}$ and continuous gates $(\\cdot)_{0}, \\omega, \\xi$. For any $\\epsilon>0$ and any\n\n$$\nF \\in\\left\\{(X, t) \\mapsto \\psi\\left(\\omega_{t}^{X}\\right) \\cdot X_{0}+\\int_{0}^{t} \\phi\\left(\\omega_{t}^{X}-\\omega_{s}^{X}\\right) \\cdot d \\xi_{s}^{X}\\right\\}\n$$\n\nfor $\\psi \\in C^{0}\\left(\\mathbb{R}^{d_{\\omega}} ; \\mathbb{R}^{d_{0}}\\right)$ and $\\phi \\in C^{0}\\left(\\mathbb{R}^{d_{\\omega}} ; \\mathbb{R}^{d_{\\xi}}\\right)$, there exist a choice of hidden dimension $N \\geq 1$ and parameters $v \\in \\mathbb{R}^{N}, B \\in \\mathbb{R}^{N \\times d_{\\xi}}, C \\in \\mathbb{R}^{N \\times d_{0}}$ and diagonal $A_{i} \\in \\mathbb{R}^{N \\times N}$ such that\n\n$$\n\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|F(X, t)-\\left\\langle v, Z_{t}^{X}\\right\\rangle\\right| \\leq \\epsilon\n$$\n\nMoreover the \"reverse\" also holds i.e. given any choice of matrices $A_{i}, B, C$ there is an $\\epsilon$-close map $F$ in the family. Proof. This is just a repetition of the arguments used for the dense case with little more care to get the uniformity in time. One defines the subset $\\operatorname{Sym}\\left(\\mathcal{H}_{[0,1]}^{(\\cdot), \\omega, \\eta}\\right) \\subset \\mathcal{H}_{[0,1]}^{(\\cdot), \\omega, \\eta}$ of those $F$ of type 41$]$ defined by $\\alpha_{i}, \\beta_{j} \\in l^{2}\\left(\\mathbb{W}_{d_{\\omega}}\\right)$ such that for any word $I$ and any permutation $\\sigma(I)$ of it\n\n$$\n\\alpha_{i}^{I}=\\alpha_{i}^{\\sigma(I)} \\quad \\beta_{j}^{I}=\\beta_{j}^{\\sigma(I)}\n$$\n\nThe same argument of Proposition B. 10 shows that the uniform closure of the space of linear maps on the $Z_{t}^{\\mathrm{x}}$ is contained in the uniform closure of $\\operatorname{Sym}\\left(\\mathcal{H}_{[0,1]}^{(\\cdot), \\omega, \\eta}\\right)$, and the same bounds show that this latter closure is the same as that of its subset composed of those $F \\in \\operatorname{Sym}\\left(\\mathcal{H}_{[0,1]}^{(\\cdot), \\omega, \\eta}\\right)$ having entries eventually equal to 0 . Since\n\n$$\n\\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{s y m, I}:=\\frac{1}{|I|!} \\sum_{\\sigma \\in S_{k}} \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{s, t}^{\\sigma(I)}=\\frac{1}{|I|!} \\prod_{i=1}^{|I|}\\left(\\omega_{t}^{\\mathrm{X}, I_{i}}-\\omega_{s}^{\\mathrm{X}, I_{i}}\\right)\n$$\n\nsuch maps can be expressed exactly in the form\n\n$$\nP\\left(\\omega_{t}^{\\mathrm{X}}\\right) \\cdot X_{0}+\\int_{0}^{t} Q\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right) \\cdot d \\xi_{s}^{\\mathrm{X}}\n$$\n\nfor polynomial maps $P, Q$ fixed in time. The usual compactness and continuity argument, together with an application of Stone-Weiestrass, thus proves that the uniform closure of $\\operatorname{Sym}\\left(\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}\\right)$ has the form needed. The final ingredient is the density of the space of linear maps on the $Z_{t}^{\\mathrm{x}}$ in $\\operatorname{Sym}\\left(\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}\\right)$; this is another consequence of Stone-Weiestrass as seen from Proposition B. 17\n\nRemark B.15. Notice how here there is no need to augment the paths in creative ways in order to ensure separability of the points. The map $(\\omega, s, t) \\mapsto \\omega_{[s, t]} \\in C_{1,0}\\left([0,1] ; \\mathbb{R}^{d_{\\omega}}\\right)$ is replaced by $(\\omega, s, t) \\mapsto \\omega_{t}-\\omega_{s} \\in \\mathbb{R}^{d_{\\omega}}$ and the space of polynomials always separates points in $\\mathbb{R}^{d_{\\omega}}$. Remark B.16. It is not necessary to pass trough $\\operatorname{Sym}\\left(\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}\\right)$ to prove the previous result, since it directly follows from Proposition B.17. This choice of presentation has been motivated by the conviction of the usefulness of drawing parallels and comparisons.",
    "theoryssms-39": "Proposition B.17. Fix a compact set $\\mathbb{K} \\subset \\mathbb{R}^{d}$ and a d-dimensional convex cone $C$ containing the origin. The space\n\n$$\n\\mathcal{E}:=\\operatorname{Span}\\left(\\mathbb{K} \\ni x \\mapsto e^{\\langle\\alpha, x\\rangle_{\\mathbb{R}^{d}}} \\in \\mathbb{R}: \\alpha \\in C\\right)\n$$\n\nis uniformly dense in $C^{0}(\\mathbb{K} ; \\mathbb{R})$. Proof. This is an application of Stone-Weiestrass: $\\mathcal{E}$ is a sub-algebra since\n\n$$\ne^{\\langle\\alpha, x\\rangle} e^{\\langle\\beta, x\\rangle_{\\mathbb{R}^{d}}}=e^{\\langle\\alpha+\\beta, x\\rangle_{\\mathbb{R}^{d}}}\n$$\n\nand $\\alpha, \\beta \\in C \\Longrightarrow \\alpha+\\beta \\in C$ by convexity of the cone; $\\mathcal{E}$ contains the constant function $e^{\\langle 0, x\\rangle}=1$ and is clearly point separating since the cone, being $d$-dimensional, it contains a basis of the whole space. Remark B.18. The usefulness of stating the previous result in such a general setting is the following: with this formalism we can, for example, restrict to $\\alpha \\leq 0$, in this way we would have a method to control the stability ( $c f$. Appendix C.1) of the linear CDEs by choosing the gate with a.s. $\\dot{\\omega}^{\\mathrm{X}} \\geq 0$. ## C. Stability and Chaining of Diagonal Systems\n\nFor this section consider, unless otherwise stated, a fixed $N \\geq 0$, compact $\\mathbb{X}$ and gates $(\\cdot)_{0}, \\omega, \\xi$. We will study the stability and chaining of diagonal systems defined by the choice of a matrix $V \\in \\mathbb{R}^{N \\times d_{\\omega}}$ such that $A_{i}:=\\operatorname{diag}\\left(V_{i}\\right)$, where $V=\\left[V_{1}|\\cdots| V_{d_{\\omega}}\\right]$. Note that the present discussion holds even for non-diagonal but commuting matrices, since these can be simultaneously diagonalized (at the cost of considering the complex plane). ## C.1. Stability\n\nHere we explore the stability of the dynamical system $Z^{\\mathrm{X}}$, thus we need to study the eigenvalues of the $W_{s, t}^{\\mathrm{X}}$. Recall how in this setting\n\n$$\nW_{s, t}^{\\mathrm{X}}=\\exp \\left(\\sum_{i=1}^{d_{\\omega}} \\int_{s}^{t} A_{i} d \\omega_{r}^{\\mathrm{X}, i}\\right)=\\exp \\left(\\operatorname{diag}\\left(\\int_{s}^{t} V d \\omega_{r}^{\\mathrm{X}}\\right)\\right)=\\exp \\left(\\operatorname{diag}\\left(V\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right)\\right)\\right)\n$$\n\nNote that because $\\omega^{\\mathrm{x}}$ is continuous and of bounded variation, it can be reparameterised to be Lipschitz continuous, hence absolutely continuous. Thus we can assume that $\\omega^{\\mathrm{x}}$ is almost everywhere differentiable and its derivative $\\dot{\\omega} \\in L^{1}$. The stability of the dynamical system then depends on the alignment between $\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}$ and the singular vectors of $V$. If $V \\dot{\\omega}_{t}^{\\mathrm{X}} \\leq 0$ for all times, where the inequality is coordinate-wise, then $W_{s, t}^{\\mathrm{X}}$ has eigenvalues all in $[0,1]$ thus the system is stable making training easier (Orvieto et al., 2023b). Consider the singular value decomposition (SVD) of the matrix $V$\n\n$$\nV=\\sum_{k=1}^{K} \\sigma_{k} v_{k} u_{k}^{\\top}\n$$\n\nThen, a sufficient condition for stability is that for any $k=1, \\ldots, K$\n\n$$\n0>\\sigma_{k} \\in \\mathbb{R}, \\quad 0 \\leq v_{k} \\in \\mathbb{R}^{N}, \\quad \\text { and } \\quad\\left\\langle u_{k}, \\dot{\\omega}_{t}^{\\mathrm{X}}\\right\\rangle \\geq 0 \\text { for any } t \\in[0, T]\n$$\n\n## C.1.1. THE CASE OF MAMBA\n\nIn the case of Mamba (Gu \\& Dao, 2023) the matrices are diagonal and\n\n$$\nd \\omega_{t}^{\\mathrm{X}}=\\operatorname{softplus}\\left(W x_{t}+\\lambda\\right) d t, \\quad d \\xi_{t}^{\\mathrm{X}}=x_{t} \\odot d \\omega_{t}^{\\mathrm{X}}\n$$\n\nmoreover the proposed choices of $V$ are all of type\n\n$$\nV=-\\mathbf{v} \\otimes \\mathbf{1}_{d_{\\omega}}\n$$\n\nfor some choice of $0 \\leq \\mathbf{v} \\in \\mathbb{R}^{N}$. Note that softmax is just a smooth approximation of $\\operatorname{Re} L U$ and that $\\operatorname{Im}(\\operatorname{Re} L U) \\subseteq$ $\\left\\{w \\in \\mathbb{R}^{d_{\\omega}}:\\left\\langle\\mathbf{1}_{d_{\\omega}}, w\\right\\rangle \\geq 0\\right\\}$ hence mamba is implicitly ensuring that the dynamical system is approximately always well-conditioned. ## C.2. Chaining\n\nThe diagonal case differs from the general one not only in the fact that the class of approximable functions is much weaker but also in the necessity for the presence of $\\xi^{\\mathrm{X}}$ in order to obtain any path-dependence. The term\n\n$$\n\\int_{0}^{t} \\phi\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right) \\cdot d \\xi_{s}^{\\mathrm{X}}\n$$\n\nbecomes then a crucial component. At first sight one might think that such a term allows to recover at least level two components of the Signature of $\\left(\\omega^{\\mathrm{X}}, \\xi^{\\mathrm{X}}\\right)$, unfortunately things are not as easy as they may seem. Notice how inside of the integral time is \"going backwards\" from the perspective of $\\omega^{\\mathrm{X}}$, thus we can in general approximate terms of type\n\n$$\n\\int_{0}^{t} \\int_{s}^{t} d \\omega_{r}^{\\mathrm{X}, i} d \\xi_{s}^{\\mathrm{X}, j}=\\int_{1-t}^{1} \\int_{1-t}^{r} d \\overleftarrow{\\omega}_{r}^{\\mathrm{x}, i} d \\overleftarrow{\\xi}_{s}^{\\mathrm{x}, j}=\\operatorname{Sig}\\left(\\left(\\overleftarrow{\\omega}^{\\mathrm{x}}, \\overleftarrow{\\xi}^{\\mathrm{x}}\\right)\\right)_{1-t, 1}^{i_{\\omega} j_{\\xi}}\n$$\n\nwhich are indeed terms of the Signature, but of the reverse paths $\\overleftarrow{\\omega}_{r}^{\\mathrm{X}}=\\omega_{1-r}^{\\mathrm{X}}$ and $\\overleftarrow{\\xi}_{s}=\\xi_{1-s}^{\\mathrm{X}}$ ! Proposition C.1. Fix a compact input set $\\mathbb{X}$, continuous gates $(\\cdot)_{0}, \\omega, \\xi$ and $X_{0}^{1}=1$. If the components of $\\xi^{X}$ are linear combinations of those of $\\omega^{X}$, with time-independent weights, then linear functionals on $Z_{t}^{X}$ can, uniformly in $\\mathbb{X} \\times[0,1]$, approximate arbitrarily well the following level 2 terms of $\\operatorname{Sig}\\left(\\left(\\omega^{X}, \\xi^{X}\\right)\\right)_{0, t}$ :\n\n$$\n\\int_{0}^{t} \\int_{0}^{s} d \\omega_{r}^{X, i} d \\xi_{s}^{X, j}=\\operatorname{Sig}\\left(\\left(\\omega^{X}, \\xi^{X}\\right)\\right)_{0, t}^{i_{\\omega} j_{\\xi}}\n$$\n\nProof. Under these hypotheses we know that linear functionals on $Z_{t}^{\\mathrm{X}}$ are uniformly dense, for continuous $\\psi, \\phi$, in\n\n$$\n\\left\\{(X, t) \\mapsto \\psi\\left(\\omega_{t}^{\\mathrm{X}}\\right) \\cdot X_{0}+\\int_{0}^{t} \\phi\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right) \\cdot d \\xi_{s}^{\\mathrm{X}}\\right\\}\n$$\n\nAssume $\\xi_{s}^{\\mathrm{X}, j}=\\left\\langle\\alpha_{j}, \\omega_{t}^{\\mathrm{X}}\\right\\rangle$ and consider the choices\n\n$$\n\\psi(x)=\\left(x^{i}\\left\\langle\\alpha_{j}, x\\right\\rangle, 0, \\cdots, 0\\right)^{\\top}, \\quad \\phi(x)=-\\left(0, \\cdots, 0, x^{i}, 0, \\cdots, 0\\right)\n$$\n\nso that\n\n$$\n\\psi\\left(\\omega_{t}^{\\mathrm{X}}\\right) \\cdot X_{0}=\\omega_{t}^{\\mathrm{X}, i} \\xi_{t}^{\\mathrm{X}, j} \\quad \\phi\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right) \\cdot d \\xi_{s}^{\\mathrm{X}}=-\\left(\\omega_{t}^{\\mathrm{X}, i}-\\omega_{s}^{\\mathrm{X}, i}\\right) d \\xi_{s}^{\\mathrm{X}, j}\n$$\n\nTo conclude note that\n\n$$\n\\begin{aligned}\n\\omega_{t}^{\\mathrm{X}, i} \\xi_{t}^{\\mathrm{X}, j} & =\\int_{s=0}^{t} \\int_{r=0}^{t} d \\omega_{r}^{\\mathrm{X}, i} d \\xi_{s}^{\\mathrm{X}, j}=\\int_{s=0}^{t} \\int_{r=0}^{s} d \\omega_{r}^{\\mathrm{X}, i} d \\xi_{s}^{\\mathrm{X}, j}+\\int_{s=0}^{t} \\int_{r=s}^{t} d \\omega_{r}^{\\mathrm{X}, i} d \\xi_{s}^{\\mathrm{X}, j} \\\\\n& =\\int_{0}^{t} \\int_{0}^{s} d \\omega_{r}^{\\mathrm{X}, i} d \\xi_{s}^{\\mathrm{X}, j}+\\int_{s=0}^{t}\\left(\\omega_{t}^{\\mathrm{X}, i}-\\omega_{s}^{\\mathrm{X}, i}\\right) d \\xi_{s}^{\\mathrm{X}, j}\n\\end{aligned}\n$$\n\nhence\n\n$$\n\\int_{0}^{t} \\int_{0}^{s} d \\omega_{r}^{\\mathrm{X}, i} d \\xi_{s}^{\\mathrm{X}, j}=\\omega_{t}^{\\mathrm{X}, i} \\xi_{t}^{\\mathrm{X}, j}-\\int_{s=0}^{t}\\left(\\omega_{t}^{\\mathrm{X}, i}-\\omega_{s}^{\\mathrm{X}, i}\\right) d \\xi_{s}^{\\mathrm{X}, j}=\\psi\\left(\\omega_{t}^{\\mathrm{X}}\\right) \\cdot X_{0}+\\int_{0}^{t} \\phi\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right) \\cdot d \\xi_{s}^{\\mathrm{X}}\n$$\n\nIf $X \\in C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$ we can use the previous result to compute its Signature entries by chaining diagonal linear CDEs. Theorem C.2. Assume a compact input set $\\mathbb{X} \\subset C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$. For any $I \\in \\mathbb{W}_{d}$ with $|I| \\geq 2$ and $\\epsilon>0$ there is a sequence of linear maps $W_{k} \\in \\mathbb{R}^{N_{k} \\times 1}$ and weights for the following family of chained linear CDEs\n\n$$\n\\begin{gathered}\nd Z_{t}^{1, X}=\\sum_{i=1}^{d} A_{i}^{(1)} Z_{t}^{1, X} d X_{t}^{i}+B^{(1)} d X_{t} \\in \\mathbb{R}^{N_{1}}, \\quad Z_{0}^{1, X}=Z_{0}^{1} \\\\\nd Z_{t}^{k+1, X}=\\sum_{i=1}^{d+1} A_{i}^{(k+1)} Z_{t}^{k+1, X} d\\left[\\begin{array}{c}\nW_{k} Z^{k, X} \\\\\nX\n\\end{array}\\right]_{t}^{i}+B^{(k+1)} d X_{t} \\in \\mathbb{R}^{N_{k+1}}, \\quad Z_{0}^{k+1, X}=Z_{0}^{k+1}\n\\end{gathered}\n$$\n\nsuch that for some $v \\in \\mathbb{R}^{N_{|I|-1}}$ one has\n\n$$\n\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|\\operatorname{Sig}(X)_{0, t}^{I}-\\left\\langle v, Z_{t}^{|I|-1, X}\\right\\rangle\\right| \\leq \\epsilon\n$$\n\nProof.",
    "theoryssms-40": "For $|I|=2$ we can apply Prop. C.1. Assume the theorem holds for $|I| \\leq k$ and let $M:=\\sup _{X \\in \\mathbb{X}}\\|X\\|_{1-v a r}$. Fix $|I j|=k+1$ and $W_{k-1} \\in \\mathbb{R}^{N_{k-1} \\times 1}$ such that\n\n$$\n\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|\\operatorname{Sig}(X)_{0, t}^{I}-W_{k-1} Z_{t}^{k-1, \\mathrm{x}}\\right| \\leq \\frac{\\epsilon}{M}\n$$\n\nAgain by Prop. C. 1 there are a $N_{k}$ and $v \\in \\mathbb{R}^{N_{k}}$ such that\n\n$$\n\\sup _{(X, t) \\in \\mathbb{X} \\times[0,1]}\\left|\\int_{0}^{t} W_{k-1} Z_{s}^{k-1, \\mathrm{x}} d X_{s}^{j}-\\left\\langle v, Z_{t}^{k, \\mathrm{X}}\\right\\rangle\\right| \\leq \\epsilon\n$$\n\nThen\n\n$$\n\\begin{aligned}\n\\left|\\operatorname{Sig}(X)_{0, t}^{I j}-\\left\\langle v, Z_{t}^{k, \\mathrm{x}}\\right\\rangle\\right| & \\leq\\left|\\int_{0}^{t} \\operatorname{Sig}(X)_{0, s}^{I} d X_{s}^{j}-\\int_{0}^{t} W_{k-1} Z_{s}^{k-1, \\mathrm{x}} d X_{s}^{j}\\right|+\\left|\\int_{0}^{t} W_{k-1} Z_{s}^{k-1, \\mathrm{x}} d X_{s}^{j}-\\left\\langle v, Z_{t}^{k, \\mathrm{x}}\\right\\rangle\\right| \\\\\n& \\leq \\int_{0}^{t}\\left|\\operatorname{Sig}(X)_{0, s}^{I}-W_{k-1} Z_{s}^{k-1, \\mathrm{x}}\\right|\\left|d X_{s}^{j}\\right|+\\epsilon \\\\\n& \\leq \\int_{0}^{t} \\frac{\\epsilon}{M}\\left|d X_{s}^{j}\\right|+\\epsilon \\leq 2 \\epsilon\n\\end{aligned}\n$$\n\nthus concluding the proof. ## C.2.1. ReLU ACTIVATION CHOICE\n\nModels like Mamba do not only use diagonal matrices but also consider controls of a specific kind:\n\n$$\n\\omega_{t}^{\\mathrm{x}}=\\int_{0}^{t} \\operatorname{Re} L U\\left(W X_{s}+b\\right) d s\n$$\n\nThe choice of $R e L U$ enforces $\\dot{\\omega}_{t} \\geq 0$ for all times as seen above, but could, a priori, destroy information about $X$ which allows for the recovery, after chaining, of its Signature. Does this choice keep some expressivity? Fortunately almost all of it: since\n\n$$\n\\operatorname{Re} L U(x)-\\operatorname{Re} L U(-x)=x\n$$\n\none can choose a linear map $W$ which allows to linearly recover\n\n$$\n\\tilde{\\omega}_{t}^{\\mathrm{X}}=\\int_{0}^{t} X_{s} d s\n$$\n\nfrom $\\omega_{t}^{\\mathrm{X}}$. By correspondingly modifying the form of $\\psi$ and $\\phi$ in Eqn. 777) such that\n\n$$\n\\psi\\left(\\omega_{t}^{\\mathrm{X}}\\right) \\cdot X_{0}=\\tilde{\\omega}_{t}^{\\mathrm{X}, i} \\xi_{t}^{\\mathrm{X}, j} \\quad \\phi\\left(\\omega_{t}^{\\mathrm{X}}-\\omega_{s}^{\\mathrm{X}}\\right) \\cdot d \\xi_{s}^{\\mathrm{X}}=-\\left(\\tilde{\\omega}_{t}^{\\mathrm{X}, i}-\\tilde{\\omega}_{s}^{\\mathrm{X}, i}\\right) d \\xi_{s}^{\\mathrm{X}, j}\n$$\n\none is able, through a similar chaining procedure, to recover arbitrarily deep entries of the Signature of $\\tilde{\\omega}_{t}^{\\mathrm{X}}=\\int_{0}^{t} X_{s} d s$.",
    "theoryssms-41": "## D. Path-to-Path\n\nDefinition D.1. A map $G \\in C^{0}\\left(C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right) \\times[0,1] ; \\mathbb{R}\\right)$ is causal iff for all $t \\in[0,1]$ and paths $\\omega, \\tilde{\\omega} \\in C_{1,0}\\left([0,1] ; \\mathbb{R}^{d}\\right)$ one has\n\n$$\n\\left.\\omega\\right|_{[0, t]}=\\left.\\tilde{\\omega}\\right|_{[0, t]} \\Longrightarrow G(\\omega, t)=G(\\tilde{\\omega}, t)\n$$\n\ni.e. G is causal if it does not look in the future. Proposition D.2. Assume a compact input set $\\mathbb{X}$, continuous $(\\cdot)_{0}, \\omega, \\xi, X_{0}^{1} \\equiv 1$ and $\\omega_{t}^{X, 1} \\equiv t$. Then for all $\\epsilon>0$ and all causal $G \\in C^{0}\\left(C_{1,0}\\left([0,1] ; \\mathbb{R}^{d_{\\omega}}\\right) \\times[0,1] ; \\mathbb{R}\\right)$ there exist an integer $N \\geq 0$, some Feed Forward neural network $F: \\mathbb{R}^{N} \\rightarrow \\mathbb{R}$, and parameters $C \\in \\mathbb{R}^{N \\times d_{0}}, A_{i} \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\xi}}$ such that\n\n$$\n\\sup _{X \\in \\mathbb{X}} \\sup _{t \\in[0,1]}\\left|F\\left(Z_{t}^{X}\\right)-G\\left(\\omega^{X}, t\\right)\\right|<\\epsilon\n$$\n\nProof. Fix $\\epsilon>0$. By B. 7 the space $\\mathcal{H}_{[0,1]}^{(\\cdot), \\omega, \\eta}$ contains all functionals of form\n\n$$\n(X, t) \\mapsto\\left\\langle\\alpha, \\operatorname{Sig}\\left(\\omega^{\\mathrm{X}}\\right)_{0, t}\\right\\rangle\n$$\n\nthus, by the properties of the signature and by compactness of $\\mathbb{X}$, for any fixed $s_{0} \\in[0,1]$ there is some $f \\in \\mathcal{H}_{[0,1]}^{(\\cdot), \\omega, \\eta}$ such that\n\n$$\n\\sup _{X \\in \\mathbb{X}}\\left|f\\left(X, s_{0}\\right)-G\\left(\\omega^{\\mathrm{X}}, s_{0}\\right)\\right|<\\epsilon\n$$\n\nUsing the fact that $G \\in C^{0}\\left([0,1] ; C^{0}\\left(C_{1,0}\\left([0,1] ; \\mathbb{R}^{d_{\\omega}}\\right) ; \\mathbb{R}\\right)\\right)$ and compactness of $[0,1]$, we find a finite set $\\left\\{0 \\leq s_{0} \\leq \\cdots \\leq\\right.$ $\\left.s_{M} \\leq 1\\right\\}$ of points and $f_{0}, \\ldots, f_{M} \\in \\mathcal{H}_{[0,1]}^{(\\cdot) 0, \\omega, \\eta}$ such that\n\n$$\n\\begin{aligned}\n& \\sup _{(X, s) \\in \\mathbb{X} \\times\\left[s_{i-1}, s_{i+1}\\right]}\\left|G\\left(\\omega^{\\mathrm{X}}, s\\right)-G\\left(\\omega^{\\mathrm{X}}, s_{i}\\right)\\right|<\\epsilon \\\\\n& \\sup _{X \\in \\mathbb{X}}\\left|f_{i}\\left(X, s_{i}\\right)-G\\left(\\omega^{\\mathrm{X}}, s_{i}\\right)\\right|<\\epsilon \\\\\n& \\sup _{(X, s) \\in \\mathbb{X} \\times\\left[s_{i-1}, s_{i+1}\\right]}\\left|f_{i}(X, s)-f_{i}\\left(X, s_{i}\\right)\\right|<\\epsilon\n\\end{aligned}\n$$\n\nfor $i=0, \\ldots, M-1$. Notice then how for all $X \\in \\mathbb{X}$ and $s \\in\\left[s_{i-1}, s_{i+1}\\right]$\n\n$$\n\\left|f_{i}(X, s)-G\\left(\\omega^{\\mathrm{X}}, s\\right)\\right| \\leq\\left|f_{i}(X, s)-f_{i}\\left(X, s_{i}\\right)\\right|+\\left|f_{i}\\left(X, s_{i}\\right)-G\\left(\\omega^{\\mathrm{X}}, s_{i}\\right)\\right|+\\left|G\\left(\\omega^{\\mathrm{X}}, s_{i}\\right)-G\\left(\\omega^{\\mathrm{x}}, s\\right)\\right| \\leq 3 \\epsilon\n$$\n\nIt follows that the map $F \\in C^{0}([0,1] \\times \\mathbb{X} ; \\mathbb{R})$ linearly interpolating the $f_{i}$ in time satisfies\n\n$$\n\\sup _{X \\in \\mathbb{X}} \\sup _{t \\in[0,1]}\\left|F(X)_{t}-G\\left(\\omega^{\\mathrm{x}}, t\\right)\\right|<6 \\epsilon\n$$\n\nTo conclude note that $\\mathbb{X}$ being compact, the $f_{i}$ take values in a common compact set $K \\subseteq \\mathbb{R}$. There exist then a neural network $\\Psi:[0,1] \\times K^{M} \\rightarrow \\mathbb{R}$ such that\n\n$$\n\\sup _{i \\in 0, \\ldots, M-1} \\sup _{s \\in\\left[s_{i}, s_{i+1}\\right]}\\left|\\Psi(t, z)-\\left(\\frac{s_{i+1}-s}{s_{i+1}-s_{i}} z_{i}+\\frac{s-s_{i}}{s_{i+1}-s_{i}} z_{i+1}\\right)\\right|<\\epsilon\n$$\n\nwhich means that\n\n$$\n\\sup _{X \\in \\mathbb{X}} \\sup _{t \\in[0,1]}\\left|\\Psi\\left(t, f_{0}(X, t), \\ldots, f_{M}(X, t)\\right)-F(X, t)\\right|<\\epsilon\n$$\n\nRecalling that $\\omega_{t}^{\\mathrm{X}, 1}=t$ we get that $X \\mapsto\\{t \\mapsto t\\} \\in \\mathcal{H}_{[0,1]}^{(\\cdot), \\omega, \\eta}$ so that, given density of linear maps on $Z^{\\mathrm{X}}$ in the space, $\\Psi\\left(t, f_{0}(X, t), \\ldots, f_{M}(X, t)\\right)$ can be uniformly approximated. Triangular inequality gives finally\n\n$$\n\\sup _{X \\in \\mathbb{X}} \\sup _{t \\in[0,1]}\\left|\\Psi\\left(t, f_{0}(X, t), \\ldots, f_{M}(X, t)\\right)-G\\left(\\omega^{\\mathrm{X}}, t\\right)\\right|<7 \\epsilon\n$$\n\nwhich, by arbitrariness of $\\epsilon$, gives the thesis. The non-linearity is crucial for the path-to-path result. A map of type $(\\omega, t) \\mapsto\\left\\langle t \\alpha, \\operatorname{Sig}(\\omega)_{0, t}\\right\\rangle$ cannot be approximated arbitrarily well by $(\\omega, t) \\mapsto\\left\\langle\\beta, \\operatorname{Sig}(\\omega)_{0, t}\\right\\rangle$. In any case, note that in the proof the role of the neural network is only that of interpolating the RKHS elements in the right order and at the right time. All the non-linear complexity of learning the particular $G$ is offloaded and taken care of by the RKHS elements. Remark D.3. In the proof we have only considered the part of $T(X)$ concerning $\\omega^{\\mathrm{x}}$, but $T(X)_{t}$ depends linearly on $X_{0}$ and $\\xi^{\\mathrm{X}}$ suggesting that neural networks on $\\mathcal{H}_{[0,1]}^{(\\cdot)_{0}, \\omega, \\eta}$ have stronger generalization properties. In fact one can prove that it is possible to approximate all continuous $G\\left(X_{0}, \\omega^{\\mathrm{x}}, \\xi^{\\mathrm{x}}, t\\right)$, this is done by reconstructing $X_{0}$ and $\\xi_{[0,1]}^{\\mathrm{X}}$ as in the classical SSM case $c f$.",
    "theoryssms-42": "Orvieto et al.",
    "theoryssms-43": "2023a). ## E. Wronskian Matrix Theory\n\nIn this section we obtain a unified theory studying the solutions to general linear CDEs. The results presented here are not new and can be found in different terms in the literature (Friz \\& Victoir, 2010), despite this we have decided to reproduce them from scratch for completeness, notational reasons and to present a self-contained theory. Theorem E.1. For any choice $\\left\\{A^{1}, \\ldots, A^{d}\\right\\} \\subseteq C^{0}\\left([0,1] ; \\mathbb{R}^{N \\times N}\\right)$ and $\\omega \\in C_{1}\\left([0,1] ; \\mathbb{R}^{d}\\right)$ there exist a unique map $W \\in C^{0}\\left([0,1] \\times[0,1] ; \\mathbb{R}^{N \\times N}\\right)$ solving the following $C D E$\n\n$$\nW_{s, t}=I d_{N}+\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} A_{\\tau}^{i} W_{s, \\tau} d \\omega_{\\tau}^{i}\n$$\n\nProof. We will use Banach fixed point theorem leveraging the completeness of the space $\\Omega:=C^{0}\\left([0,1] \\times[0,1] ; \\mathbb{R}^{N \\times N}\\right)$ with the uniform norm\n\n$$\n\\|X\\|_{\\infty}:=\\sup _{s, t \\in[0,1]}\\left\\|X_{s, t}\\right\\|_{o p}\n$$\n\nDefine the map $\\Gamma: \\Omega \\rightarrow \\Omega$ as\n\n$$\n\\Gamma(X)_{s, t}=I d_{N}+\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} A_{\\tau}^{i} X_{s, \\tau} d \\omega_{\\tau}^{i}\n$$\n\nOne has, for $X, Y \\in \\Omega$ and $k \\in \\mathbb{N}$ setting $\\Gamma^{0}=I d_{\\Omega}$, that\n\n$$\n\\Gamma^{k+1}(X)_{s, t}-\\Gamma^{k+1}(Y)_{s, t}=\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} A_{\\tau}^{i}\\left(\\Gamma^{k}(X)_{s, \\tau}-\\Gamma^{k}(Y)_{s, \\tau}\\right) d \\omega_{\\tau}^{i}\n$$\n\nwhich iterated gives\n\n$$\n\\begin{aligned}\n\\Gamma^{k+1}(X)_{s, t}-\\Gamma^{k+1}(Y)_{s, t} & =\\sum_{\\substack{I \\in \\mathcal{W}_{d} \\\\\n|I|=k+1}} \\int_{\\tau_{k+1}=s}^{t} \\cdots \\int_{\\tau_{1}=s}^{\\tau_{2}}\\left(\\prod_{j=k+1}^{1} A_{\\tau_{j}}^{I_{j}}\\right)\\left(X_{s, \\tau_{1}}-Z_{s, \\tau_{1}}\\right) \\prod_{j=1}^{k+1} d \\omega_{\\tau_{j}}^{I_{j}} \\\\\n& =\\sum_{\\substack{I \\in \\mathcal{W}_{d} \\\\\n|I|=k+1}} \\int_{\\tau \\in \\Delta_{[s, t]}^{k+1}} A_{\\tau}^{I}\\left(X_{s, \\tau_{1}}-Z_{s, \\tau_{1}}\\right) d \\omega_{\\tau}^{I}\n\\end{aligned}\n$$\n\nwhere $\\mathcal{W}_{d}$ is the set of words in the alphabet $\\{1, \\ldots, d\\}$ and\n\n$$\n\\begin{gathered}\n\\Delta_{[s, t]}^{k}:=\\left\\{\\left(\\tau_{1}, \\ldots, \\tau_{k}\\right) \\in[0,1]^{k}: \\forall j \\in 1, \\ldots, k-1 . \\tau_{j} \\leq \\tau_{j+1}\\right\\} \\\\\nA_{\\tau}^{I}:=\\prod_{j=k+1}^{1} A_{\\tau_{j}}^{I_{j}} d \\omega_{\\tau}^{I}:=\\prod_{j=1}^{k+1} d \\omega_{\\tau_{j}}^{I_{j}}\n\\end{gathered}\n$$\n\nBy defining $M=\\max \\left\\{\\left\\|A^{i}\\right\\|_{\\infty}: i \\in\\{1, \\ldots, d\\}\\right\\}$ then one clearly has\n\n$$\n\\left\\|\\Gamma^{k}(X)-\\Gamma^{k}(Y)\\right\\|_{\\infty} \\leq \\frac{\\left(d M\\|\\omega\\|_{1-v a r}\\right)^{k}}{k!}\\|X-Y\\|_{\\infty}\n$$\n\nthus definitely (in $k$ ) the map $\\Gamma^{k}$ is a contraction. By Banach fixed point there exist a unique fixed point $W \\in \\Omega$. Theorem E.2. Under the assumptions of the previous theorem one can write $W_{s, t}$ explicitly as\n\n$$\nW_{s, t}=\\sum_{I \\in \\mathcal{W}_{d}} \\int_{\\tau \\in \\Delta_{[s, t]}^{|I|}} A_{\\tau}^{I} d \\omega_{\\tau}^{I}\n$$\n\nmoreover if for all $i$ the matrix-valued maps are constant on all $[0,1]$ i.e. $A_{t}^{i} \\equiv A_{i}$ then\n\n$$\nW_{s, t}=\\sum_{I \\in \\mathcal{W}_{d}} A_{I} \\operatorname{Sig}(\\omega)_{s, t}^{I}\n$$\n\nwhere $\\operatorname{Sig}(\\omega)_{s, t}^{I}$ is the Signature of the path $\\omega$. Proof. The second assertion follows from the first by definition of the Signature of a path. Regarding the first notice how the series is absolutely convergent in $\\mathbb{R}^{N \\times N}$, uniformly in $s, t$ since\n\n$$\n\\begin{aligned}\n& \\sum_{I \\in \\mathcal{W}_{d}}\\left\\|\\int_{\\tau \\in \\Delta_{[s, t]}^{|I|}} A_{\\tau}^{I} d \\omega_{\\tau}^{I}\\right\\|_{o p} \\leq \\sum_{k=0}^{\\infty} d^{k} M^{k} \\frac{\\|\\omega\\|_{1-v a r,[s, t]}^{k}}{k!} \\\\\n& =e^{d M\\|\\omega\\|_{1-v a r,[s, t]}} \\leq e^{d M\\|\\omega\\|_{1-v a r,[0,1]}}\n\\end{aligned}\n$$\n\nthus for any $s, t \\in[0,1]$ the series defines an element of $\\tilde{W}_{s, t} \\in \\mathbb{R}^{N \\times N}$. Using the uniformity of this bound and the fact that for all $I \\in \\mathcal{W}_{d}$ one has\n\n$$\n\\tilde{W}_{s, t}^{I}:=\\int_{\\tau \\in \\Delta_{[s, t]}^{\\mid I I}} A_{\\tau}^{I} d \\omega_{\\tau}^{I} \\in \\Omega\n$$\n\nas a function of $(s, t)$, which moreover is uniformly continuous\n\n$$\n\\begin{aligned}\n& \\left\\|\\tilde{W}_{s_{1}, t_{1}}^{I}-\\tilde{W}_{s_{2}, t_{2}}^{I}\\right\\|_{o p}=\\left\\|\\int_{\\tau \\in \\Delta_{\\left[s_{1} \\wedge s_{2}, t_{1} \\vee t_{2}\\right]}^{[I \\mid}}\\left(\\delta_{\\tau \\in \\Delta_{\\left[s_{1}, t_{1}\\right]}^{[I]}}-\\delta_{\\left.\\tau \\in \\Delta_{\\left[s_{2}, t_{2}\\right]}^{|I|}\\right)}\\right) A_{\\tau}^{I} d \\omega_{\\tau}^{I}\\right\\|_{o p} \\\\\n& \\leq M^{|I|} \\int_{\\tau \\in \\Delta_{\\left[s_{1} \\wedge s_{2}, t_{1} \\vee t_{2}\\right]}^{|I|}}\\left|\\delta_{\\tau \\in \\Delta_{\\left[s_{1}, t_{1}\\right]}^{|I|}}-\\delta_{\\tau \\in \\Delta_{\\left[s_{2}, t_{2}\\right]}^{|I|}}\\right|\\left|d \\omega_{\\tau}^{I}\\right| \\\\\n& \\leq M^{|I|}\\|\\omega\\|_{\\left[s_{1} \\wedge s_{2}, s_{1} \\vee s_{2}\\right] \\cup\\left[t_{1} \\wedge t_{2}, t_{1} \\vee t_{2}\\right]}^{|I|},\n\\end{aligned}\n$$\n\none concludes that $\\tilde{W}_{s, t} \\in \\Omega$. Finally notice that $\\tilde{W}_{s, t}$ is a fixed point of $\\Gamma$\n\n$$\n\\begin{aligned}\n\\Gamma\\left(\\tilde{W}_{s, t}\\right) & =I d_{N}+\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} A_{\\tau}^{i}\\left(\\sum_{I \\in \\mathcal{W}_{d}} \\int_{\\tau \\in \\Delta_{[0,1]}^{|I|}} A_{\\tau}^{I} d \\omega_{\\tau}^{I}\\right) d \\omega_{\\tau}^{i} \\\\\n& =I d_{N}+\\sum_{\\substack{I \\in \\mathcal{W}_{d} \\\\\n|I| \\geq 1}} \\int_{\\tau \\in \\Delta_{[0,1]}^{[I \\mid}} A_{\\tau}^{I} d \\omega_{\\tau}^{I} d \\omega_{\\tau}^{i} \\\\\n& =\\sum_{I \\in \\mathcal{W}_{d}} \\int_{\\tau \\in \\Delta_{[0,1]}^{|I|}} A_{\\tau}^{I} d \\omega_{\\tau}^{I}=\\tilde{W}_{s, t}\n\\end{aligned}\n$$\n\nand conclude by uniqueness. Proposition E.3. Under the previous conditions, the unique solution of the $N$-dimensional CDE\n\n$$\nd X_{t}=X_{0}+\\sum_{i=1}^{d} \\int_{\\tau=0}^{t} A_{\\tau}^{i} X_{\\tau} d \\omega_{\\tau}^{i}\n$$\n\nis given by\n\n$$\nX_{t}=W_{0, t} X_{0}\n$$\n\nProof. The solutions are unique by standard results (Friz \\& Victoir, 2010)[Thm. 3.7], moreover\n\n$$\nW_{0, t} X_{0}=\\left(I d_{N}+\\sum_{i=1}^{d} \\int_{\\tau=0}^{t} A_{\\tau}^{i} W_{0, \\tau} d \\omega_{\\tau}^{i}\\right) X_{0}=X_{0}+\\sum_{i=1}^{d} \\int_{\\tau=0}^{t} A_{\\tau}^{i}\\left(W_{0, \\tau} X_{0}\\right) d \\omega_{\\tau}^{i}\n$$\n\n## Proposition E.4. The Wronskian matrix has the following properties:\n\n1. $\\forall r, s, t \\in[0,1] . \\quad W_{r, t}=W_{s, t} W_{r, s}$\n2. $\\forall s, t \\in[0,1] . \\quad W_{s, t}^{-1}=W_{t, s}$\n3. $\\forall s, t \\in[0,1] . \\quad W_{s, t}=I d_{N}+\\sum_{i=1}^{d} \\int_{\\sigma=s}^{t} W_{\\sigma, t} A_{\\sigma}^{i} d \\omega_{\\sigma}^{i}$\n\nProof. Regarding the first statement notice that for all $X_{0} \\in \\mathbb{R}^{N}$ one has\n\n$$\n\\begin{aligned}\n\\tilde{X}_{t}: & =W_{s, t} W_{r, s} X_{0}=\\left(I d_{N}+\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} A_{\\tau}^{i} W_{s, \\tau} d \\omega_{\\tau}^{i}\\right) W_{r, s} X_{0} \\\\\n& =W_{r, s} X_{0}+\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} A_{\\tau}^{i}\\left(W_{s, \\tau} W_{r, s} X_{0}\\right) d \\omega_{\\tau}^{i} \\\\\n& =W_{r, s} X_{0}+\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} A_{\\tau}^{i} \\tilde{X}_{\\tau} d \\omega_{\\tau}^{i}\n\\end{aligned}\n$$\n\nand by the previous proposition also\n\n$$\nX_{t}:=W_{r, t} X_{0}=W_{r, t} X_{0}+\\sum_{i=1}^{d} \\int_{\\tau=r}^{t} A_{\\tau}^{i} X_{\\tau} d \\omega_{\\tau}^{i}\n$$\n\nthus $X_{t}$ and $\\tilde{X}_{t}$ solve the same $C D E$ and coincide at time $t=s$. This means, by uniqueness, that $X_{t}$ and $\\tilde{X}_{t}$ coincide for all times; hence $W_{s, t} W_{r, s}$ and $W_{r, t}$ coincide for all times too since for any choice of $X_{0}$ one has $W_{s, t} W_{r, s} X_{0}=W_{r, t} X_{0}$. The second statement follows from the previous one setting first $r=t$ and subsequently exchanging $s$ and $t$. To prove the third equality note that\n\n$$\n0=d_{s}\\left(W_{s, t} W_{t, s}\\right)=\\left(d_{s} W_{s, t}\\right) W_{t, s}+W_{s, t}\\left(d_{s} W_{t, s}\\right)\n$$\n\nhence\n\n$$\nd_{s} W_{s, t}=-W_{s, t}\\left(d_{s} W_{t, s}\\right) W_{t, s}^{-1}=-W_{s, t}\\left(\\sum_{i=1}^{d} A_{s}^{i} W_{t, s} d \\omega_{s}^{i}\\right) W_{t, s}^{-1}=-\\sum_{i=1}^{d} W_{s, t} A_{s}^{i} d \\omega_{s}^{i}\n$$\n\nProposition E. 5 (Liouville's Formula). Under the assumptions of the previous theorems, if $\\omega \\in C^{1}\\left([0,1] ; \\mathbb{R}^{d}\\right)$ then\n\n$$\n\\operatorname{det}\\left(W_{s, t}\\right)=1+\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} \\operatorname{tr}\\left(A_{\\tau}^{i}\\right) \\operatorname{det}\\left(W_{s, t}\\right) d \\omega_{\\tau}^{i}=\\exp \\left(\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} \\operatorname{tr}\\left(A_{\\tau}^{i}\\right) d \\omega_{\\tau}^{i}\\right)\n$$\n\nProof. This just follows from the classical case since we can write\n\n$$\n\\sum_{i=1}^{d} \\int_{\\tau=s}^{t} A_{\\tau}^{i} W_{s, \\tau} d \\omega_{\\tau}^{i}=\\int_{\\tau=s}^{t}\\left(\\sum_{i=1}^{d} A_{\\tau}^{i} \\dot{\\omega}_{\\tau}^{i}\\right) W_{s, \\tau} d \\tau\n$$\n\nWe can now state the main result of the section:\nTheorem E.6. Under the assumptions of the previous theorems, given continuous functions $\\left\\{B^{1}, \\ldots, B^{t}\\right\\} \\in\\left(\\mathbb{R}^{d}\\right)^{[0,1]}$ the unique solution of the $N$-dimensional $C D E$\n\n$$\nX_{t}=X_{0}+\\sum_{i=1}^{d} \\int_{\\tau=0}^{t}\\left(A_{\\tau}^{i} X_{\\tau}+B_{\\tau}^{i}\\right) d \\omega_{\\tau}^{i}\n$$\n\nis given explicitly by\n\n$$\nX_{t}=W_{0, t} X_{0}+\\sum_{i=1}^{d} \\int_{0}^{t} W_{s, t} B_{s}^{i} d \\omega_{s}^{i}\n$$\n\nwhere $W_{s, t} \\in C^{0}\\left([0,1] \\times[0,1] ; \\mathbb{R}^{N \\times N}\\right)$ is the Wronskian matrix defined by\n\n$$\nW_{s, t}=\\sum_{I \\in \\mathcal{W}_{d}} \\int_{\\tau \\in \\Delta_{[s, t]}^{[I I}} A_{\\tau}^{I} d \\omega_{\\tau}^{I}\n$$\n\nProof. Given the unique solution $X_{t}$ one has\n\n$$\n\\begin{aligned}\nd_{s}\\left(W_{s, t} X_{s}\\right) & =d_{s}\\left(W_{s, t}\\right) X_{s}+W_{s, t} d_{s}\\left(X_{s}\\right) \\\\\n& =\\sum_{i=1}^{d}\\left(-W_{s, t} A_{s}^{i} X_{s}+W_{s, t} A_{s}^{i} X_{s}+W_{s, t} B_{s}^{i}\\right) d \\omega_{s}^{i} \\\\\n& =\\sum_{i=1}^{d} W_{s, t} B_{s}^{i} d \\omega_{s}^{i}\n\\end{aligned}\n$$\n\nhence\n\n$$\nX_{t}-W_{0, t} X_{0}=W_{t, t} X_{t}-W_{0, t} X_{0}=\\sum_{i=1}^{d} \\int_{s=0}^{t} W_{s, t} B_{s}^{i} d \\omega_{s}^{i}\n$$\n\n## F. ZOH and Exact Solutions\n\nConsider a linear CDE as the one of Eqn. (28)\n\n$$\nd Z_{t}=\\sum_{i=1}^{d_{\\omega}} A_{i} Z_{t} d \\omega_{t}^{i}+B d \\xi_{t}\n$$\n\nand recall how the solution can be explicitly written, for times $s<t$, as\n\n$$\nZ_{t}=W_{s, t} Z_{s}+\\int_{s}^{t} W_{r, t} B d \\xi_{r}\n$$\n\nAssume moreover that in the interval $[s, t]$ both drivers have constant derivative i.e. $$\n\\omega_{r}=\\omega_{s}+\\boldsymbol{w}(r-s) \\quad \\xi_{r}=\\xi_{s}+\\boldsymbol{v}(r-s)\n$$\n\nThen if $\\mathbb{A}_{\\boldsymbol{w}}:=\\sum_{i=1}^{d_{\\omega}} A_{i} \\boldsymbol{w}^{i}$ we get that $W_{r, t}=e^{\\mathbb{A}_{\\boldsymbol{w}}(t-r)}$ thus\n\n$$\nZ_{t}=e^{\\mathbb{A}_{\\boldsymbol{w}}(t-s)} Z_{s}+\\int_{s}^{t} e^{\\mathbb{A}_{\\boldsymbol{w}}(t-r)} B \\boldsymbol{v} d r=e^{\\mathbb{A}_{\\boldsymbol{w}}(t-s)} Z_{s}+\\left(\\int_{s}^{t} e^{\\mathbb{A}_{\\boldsymbol{w}}(t-r)} d r\\right) B \\boldsymbol{v}\n$$\n\nBut the integral can be explicitly solved as\n\n$$\n\\int_{s}^{t} e^{\\mathbb{A}_{\\boldsymbol{w}}(t-r)} d r=\\left(-\\left.\\mathbb{A}_{\\boldsymbol{w}}^{-1} e^{\\mathbb{A}_{\\boldsymbol{w}}(t-r)}\\right|_{r=s} ^{t}=\\mathbb{A}_{\\boldsymbol{w}}^{-1}\\left(e^{\\mathbb{A}_{\\boldsymbol{w}}(t-s)}-\\mathbb{I}\\right)\\right. $$\n\nleaving us with\n\n$$\nZ_{t}=e^{\\mathbb{A}_{\\boldsymbol{w}}(t-s)} Z_{s}+\\mathbb{A}_{\\boldsymbol{w}}^{-1}\\left(e^{\\mathbb{A}_{\\boldsymbol{w}}(t-s)}-\\mathbb{I}\\right) B \\boldsymbol{v}\n$$\n\nwhich, setting $\\Delta=t-s$, can be rewritten as\n\n$$\nZ_{t}=e^{\\mathbb{A}_{\\boldsymbol{w}} \\Delta} Z_{s}+\\left(\\mathbb{A}_{\\boldsymbol{w}} \\Delta\\right)^{-1}\\left(e^{\\mathbb{A}_{\\boldsymbol{w}} \\Delta}-\\mathbb{I}\\right)(B \\Delta) \\boldsymbol{v}\n$$\n\ni.e. exactly the ZOH scheme. [^0]:    ${ }^{1}$ Department of Mathematics, Imperial College London. ${ }^{2}$ MPI for Intelligent Systems, T\u00fcbingen AI Center, ELLIS Institute T\u00fcbingen. ${ }^{3}$ Department of Mathematics, University of Oxford. Correspondence to: Nicola Muca Cirone $<$ n.mucacirone22@imperial.ac.uk>. [^1]:    ${ }^{1}$ The LRU (Orvieto et al. 2023b) and S5 (Smith et al. 2023) instead build a single recurrence operating on multidimensional (\\# channels) inputs. ${ }^{2}$ This corresponds to: (i). considering the continuous underlying signal $X_{t}$ to be piecewise constant, (ii). solving exactly the ODE 1 and finally (iii). sampling $Z_{t}$ at the sample times of $X_{t}$. ${ }^{3}$ Reall: i denotes the imaginary unit $\\sqrt{-1}$, not to be confused with the index $i$. [^2]:    ${ }^{4}$ To be more precise, the scalar multiplier map $\\alpha_{i}$ is the result of a low-rank projection (timestamp independent) of the input (all channels). ${ }^{5}$ Discussed is only the single-head setting. [^3]:    ${ }^{6} \\mathrm{We}$ remove the subscript 0 if this constrain is removed. [^4]:    ${ }^{7}$ More generally, the initial condition $Z_{0}^{X}$ can be chosen to be the image of another embedding of the input data, as we discuss in the Appendix. [^5]:    ${ }^{8}$ These assumptions are of technical nature and can be relaxed at the cost of talking about tree-like equivalence of paths $c f$. (Hambly \\& Lyons 2010). $\\omega_{t}^{X, 2}=t^{2}$ not needed for path-to-point tasks. ${ }^{9}$ A causal map is one which does not \"look in the future\" $c f$. Appendix D\n\n[^6]:    ${ }^{10}$ So when we write $I \\in \\mathbb{W}_{d_{\\omega}}$ we mean $I=i_{1} i_{2} \\cdots i_{M}$ for some integer $M \\geq 0$ and letters $i_{m} \\in\\left\\{1, \\ldots, d_{\\omega}\\right\\}$, then $I i$ will be the word $i_{1} i_{2} \\cdots i_{M} i$ for $i \\in\\left\\{1, \\ldots, d_{\\omega}\\right\\}$. [^7]:    ${ }^{11}$ Instead, expressivity of S 4 is unaffected by diagonality in the recurrence, see results above and in (Li et al. 2022b). [^8]:    ${ }^{12}$ This is not straightforward since the time in the integral part of 14 is going \"backwards\"! [^9]:    ${ }^{13}$ Here the integral is intended in the Riemann-Stjeltes sense. [^10]:    ${ }^{14}$ It can be proved, using the results of Dubach \\& Peled 2021, that the sampling measure does not have to be Gaussian if it satisfies certain moment requirements. [^11]:    ${ }^{15}$ It is equivalent to ask for them to be commuting.",
    "theoryssms-44": ""
}