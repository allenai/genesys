{
    "normalizedattn-0": "# Normalized Attention Without Probability Cage \n\nOliver Richter and Roger Wattenhofer<br>Department of Electrical Engineering and Information Technology<br>ETH Zurich, Switzerland<br>\\{richtero,wattenhofer\\}@ethz.ch\n\n\n#### Abstract\n\nAttention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results.",
    "normalizedattn-1": "Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available $\\square^{1}$\n\n\n## 1 Introduction\n\nThe concept of neural attention [11, 3] has sparked a number of architectural breakthroughs. The Transformer architecture [32] successfully deploys multi-headed self-attention in several consecutive layers for natural language processing (NLP) - an architecture choice that has become popular [32, 26, 27, 9, 40, 28, 19, 7]. Apart from NLP, self-attention has shown success in applications ranging from image classification [23] to generative adversarial networks [43] to reinforcement learning [4, 22]. The attention architecture choice is thereby often based on one, if not both, of the following arguments: (1) Attention helps with credit assignment by providing more direct, dynamic links between inputs and outputs. (2) Attention is directly interpretable as one can investigate the percentages to which different inputs are \"attended\" to. However, this second argument has been challenged recently, as several works show that attention weights do not directly correlate with predictions [14, 34, 5, 24, in NLP models. With interpretability in dispute, we are left with an open question: Can we improve the credit assignment ability by removing the constraint on attention weights to represent a distribution? In this work, we show the theoretical implications of constraining the attention weights to the probability simplex, and propose an unconstrained alternative based on normalization. We show that the popular Transformer architecture has an innate bias towards token isolation at initialization and showcase implications thereof on biases in the data. Our experimental results demonstrate the advantage of unconstrained attention. In particular, we improve robustness to hyperparameters and show the general applicability of attention based architectures as compared to other architectures such as sum and max pooling. To summarize, our contributions include:\n\n- a theoretical investigation of the probability simplex constraint in self-attention\n- a robust, general purpose alternative based on normalization\n- a large scale experimental comparison of the performance implications that an architecture choice entails with respect to the task type, hyperparameters as well as biases in the data\n\n[^0]\n## 2 Background and Related Work\n\nMany data processing tasks can be addressed by representing the input as a set or sequence of discrete tokens, e.g., the words in a sentence or the frames in a video. As a general formulation, we represent each input token through a vector $\\boldsymbol{x}^{i} \\in \\mathbb{R}^{d}$ for $i \\in\\{1, \\ldots, N\\}$, where $N$ is the sequence length and $d$ is the dimensionality of each token. For ease of notation we use the word \"sequence\" throughout, but note that all architectures discussed are also applicable to unordered sequences, i.e., sets of tokens. Multi-headed dot-product self-attention is a fundamental building block of the Transformer architecture [32]. It allows for information exchange between different tokens of the input sequence. More formally, for each attention head $m$ the input vectors $\\boldsymbol{x}^{i}$ are projected through an affine transformation to a query $\\boldsymbol{q}_{m}^{i}$, key $\\boldsymbol{k}_{m}^{i}$ and value vector $\\boldsymbol{v}_{m}^{i}$. The dimensionality of these vectors is chosen as $d_{h}=\\frac{d}{M}$, where $M$ is the number of attention heads. The query and key vectors are used for a pairwise dot product, scaled by the square root of the head dimension $d_{h}$, to form the attention logits $l_{m}^{i, j}$ and attention vectors $\\boldsymbol{a}_{m}^{i}$ as\n\n$$\nl_{m}^{i, j}=\\frac{<\\boldsymbol{q}_{m}^{i}, \\boldsymbol{k}_{m}^{j}>}{\\sqrt{d_{h}}} \\quad \\boldsymbol{a}_{m}^{i}=\\operatorname{softmax}\\left(\\left[l_{m}^{i, 1}, \\ldots, l_{m}^{i, N}\\right]\\right)\n$$\n\nwhere softmax refers to the normalized exponential function $\\operatorname{softmax}(\\boldsymbol{x})^{j}=\\frac{\\exp \\left(x^{j}\\right)}{\\sum_{k} \\exp \\left(x^{k}\\right)}$ commonly used to project vectors to the probability simplex $\\mathcal{S}_{P}=\\left\\{\\boldsymbol{a}_{m}^{i} \\mid a_{m}^{i, j} \\geq 0 \\forall j\\right.$ and $\\left.\\sum_{j} a_{m}^{i, j}=1\\right\\}$. The output $\\boldsymbol{o}_{m}^{i}$ of each attention head $m$ is then given by a weighted sum of all value vectors $\\boldsymbol{o}_{m}^{i}=\\sum_{j} a_{m}^{i, j} \\cdot \\boldsymbol{v}_{m}^{j}$. These attention head outputs are concatenated and mixed trough an additional affine transformation to form the attention layer output in the Transformer architecture [32]. In this work, we investigate whether constraining the attention vectors $a_{m}^{i}$ into the probability simplex through the softmax function is the best we can do. We contrast the multi-head self-attention architecture to attention-inspired architectures without softmax (discussed in Section 4) as well as simpler aggregation methods commonly used. Specifically, while Yun et al. [41] show that Transformers are universal sequence-to-sequence function approximators, we question the practical necessity of an attention architecture, when sum pooling [42] already provides general function approximation capabilities [42, 38, 30]. Further, we compare to max pooling, a common aggregator choice that has shown good empirical success [20, 42, 33]. Several recent works have proposed architectural changes to the Transformer [41, 35, 16, 8, 10, 37, 25, 2]. However, to the best of our knowledge, we are the first to explicitly question the softmax in self-attention. ## 3 Limitations and Implications of Softmax Attention\n\nTo start our discussion, we highlight an observation that follows directly from attention vectors $\\boldsymbol{a}^{i}$ being constrained to the probability simplex $\\mathcal{S}_{P}$ :\nAttention head outputs $\\boldsymbol{o}_{m}^{i}$ are convex combinations of value vectors $\\boldsymbol{v}_{m}^{i}$\nThis in itself has drastic implications. First and foremost, we note that a convex combination of vectors $\\boldsymbol{v}_{m}^{i}$ cannot yield any vector outside the convex hull spanned by the value vectors $\\boldsymbol{v}_{m}^{i}$. An illustration of this output cage is given in Figure 1(left). We conjecture that this constraint limits flexibility - and thereby ease of adjustment - of the functions expressed by the neural network throughout the training process. This conjecture is supported by our experimental results showing an increased robustness to hyperparameter choices when the constraint is removed. Exploring the observation above further, we note the following from a theoretical perspective:\n\n## No convex combination can represent the binary exclusive OR (XOR) function\n\nA formal proof is given in Appendix A. Note that this implication highlights an inability to represent non-linearity. While XOR can be represented in architectures with multiple heads and layers, the insight further underlines our argument: An aggregation with weights constrained to the probability simplex is restrictive. Especially if we compare it to other aggregation methods that can represent XOR (cf. Section 4). Finally, we want to highlight an additional insight that, to the best of our knowledge, has not been discussed in the literature so far:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-03.jpg?height=303&width=1330&top_left_y=238&top_left_x=394)\n\nFigure 1: Left: Softmax attention outputs can only lie within the convex hull spanned by the value vectors $\\boldsymbol{v}^{i}$ (blue region). Middle/Right: The standard deviation $(\\sigma)$ and norm of a pooling output is dependent on the sequence length $N$ (x-axis) and the pooling method, if the output is not normalized. Softmax attention outputs scale similar to mean pooling at initialization, i.e., Transformers focus more on local information in longer sequences. Transformers have an aggregation size dependent focus on local information at initialization. To see this, consider the embeddings after the first residual connection, given by\n\n$$\n\\boldsymbol{e}^{i}=\\boldsymbol{x}^{i}+\\boldsymbol{W}\\left[\\boldsymbol{o}_{1}^{i}, \\ldots, \\boldsymbol{o}_{M}^{i}\\right]+\\boldsymbol{b}=\\boldsymbol{x}^{i}+\\boldsymbol{W}\\left[\\sum_{j} a_{1}^{i, j} \\cdot \\boldsymbol{v}_{1}^{j}, \\ldots, \\sum_{j} a_{M}^{i, j} \\cdot \\boldsymbol{v}_{M}^{j}\\right]+\\boldsymbol{b}\n$$\n\nwhere $[\\cdot]$ denotes concatenation and $\\boldsymbol{W}$ and $\\boldsymbol{b}$ represent the parameters of the affine transformation that mixes the attention $M$ head outputs. Our aim is to show how much this embedding $e^{i}$ is influenced by the local information $\\boldsymbol{x}^{i}$ relative to the context information $\\left\\{\\boldsymbol{x}^{j} \\mid j \\neq i\\right\\}$. We first note that the contribution of context information depends on the initialization of $\\boldsymbol{W}$ and $\\boldsymbol{b}$, where a typical initialization in language models favors the residual connection, i.e., local information ${ }^{2}$ However, even if we consider $\\boldsymbol{W}$ as scale preserving, we note that the magnitudes of the attention head outputs $\\boldsymbol{o}_{m}$ are upper bounded by the magnitudes of the value vectors $\\boldsymbol{v}_{m}$ as a result of the convex hull. Moreover, attention logits are normally close to 0 at initialization (to have the softmax in the unsaturated region). This yields attention to be close to mean aggregation as $\\sum_{j} a_{m}^{i, j} \\cdot \\boldsymbol{v}_{m}^{j} \\approx \\frac{1}{N} \\sum_{j} \\boldsymbol{v}_{m}^{j}$. We note that taking the mean effectively scales the standard deviation of a random variable by the square root of the aggregation size. This means that the fraction of context information in $e^{i}$ is dependent on the sequence length and is smaller for longer sequences! Specifically, at initialization, Transformers focus more on local information in longer sequence than in shorter sequences. For reference, we visualize the dependence of $\\boldsymbol{o}_{m}$ on aggregation size at initialization for different aggregators in Figure 1 (right). Details on the corresponding experiment can be found in Appendix B We note that while an architectural bias towards local information might be beneficial in some applications, the implicit dependence on aggregation size is questionable. ## 4 Normalized Attention Pooling\n\nGiven the implications that a self-attention based architecture brings along, a few natural questions to ask are: What happens if we remove the softmax? Is some form of online logit normalization necessary at all? And how do these architectures compare to simpler pooling methods like sum- or max-pooling? To investigate these, we contrast the following architectures in our experiments. We provide a schematic figure of each architecture in Appendix C. Transformer Encoder (BERT): As a starting point, we replicate the encoder architecture presented by [32] as described in the code release of [9] $]^{3}$ This architecture is among others also used by [26, 27, 40, 28, 19, 7]. Each Transformer-layer consists of two sub-modules: a multi-head selfattention \"layer\" and a feed forward network. Both modules have residual connections around them. The multi-head self-attention \"layer\" consists of a projection to queries, keys and values, the attention mechanism as well as a mixing layer as described in Section 2. The feed forward network consists of two layers with a GELU [12] non-linearity on the hidden layer. Layer normalization [1] is applied between incoming and outgoing residual connections. Note that this gives a crucial distinction of this\n\n[^1]architecture: Embeddings are normalized after they are summed with the residual connection. This yields the implicit dependence on the sequence length as discussed in the end of Section 3. Further, in this architecture training is done with learning rate warm-up and gradient norm clipping. Modified Transformer Encoder (MTE): To overcome the implicit dependence on sequence length, reduce training specific confounding factors and to make the two sub-modules more similar to each other, we introduce the following modifications: We remove learning rate warm-up and gradient clipping, but keep a linearly decreasing learning rate schedule, taking [17] as reference. Layer normalization is moved before the residual addition. Additionally, we add layer normalization on the hidden layers in the modules, i.e., before the mixing layer and before the GELU non-linearity in the feed forward network. These modifications remove the dependence on sequence length. Note that this is different from the recently studied PreNorm [22, 21, 18] that places the normalization before the attention mechanism. Finaly, we add an additional GELU non-linearity in the middle of the attention sub-module. We provide an ablation of all modifications in Appendix D. All following architectures apply the same modifications. The resulting $M T E$ architecture here still projects the attention weights to the probability simplex through the softmax in the multi-head attention. This architecture is thereby limited to convex combinations of value vectors. Normalized Attention Pooling (NAP): Given the success of online normalization during training be it through batch- [13], layer- [1], group- [36], instance- [31] or weight-normalization [29] - our main proposal is to simply replace the softmax through a normalization:\n\n$$\n\\boldsymbol{a}_{m}^{i}=\\operatorname{normalize}\\left(\\left[l_{m}^{i, 1}, \\ldots, l_{m}^{i, N}\\right]\\right) \\quad \\text { with normalize }(\\boldsymbol{x})^{j}=g \\cdot \\frac{x^{j}-\\mu_{\\boldsymbol{x}}}{\\sigma_{\\boldsymbol{x}}}+b\n$$\n\nwhere $\\mu_{\\boldsymbol{x}}=\\frac{1}{N} \\sum_{j} x^{j}$ and $\\sigma_{\\boldsymbol{x}}=\\frac{1}{N} \\sum_{j}\\left(x^{j}-\\mu_{\\boldsymbol{x}}\\right)^{2}$ are the mean and standard deviation of the corresponding input vector $\\boldsymbol{x}$, in our case the logit vector calculated through key-query dot products.",
    "normalizedattn-2": "Similar to layer normalization [1], we introduce trainable gain and bias parameters $g$ and $b$ initialized to 1 and 0 , respectively. However, while [1] introduce gain and bias vectors, we only introduce scalar parameters and broadcast these over the sequence/vector length, as we want the architecture to be independent of the sequence length $N$. Note that while no convex combination can represent the logical XOR, a normalized weighting can - see Appendix Afor the corresponding proof. No Online Logit Normalization (NON): To investigate whether a dynamic normalization of the attention logits is necessary, we also train a model where we use the logits $l_{m}^{i, j}$ directly as attention weights, i.e., $\\boldsymbol{o}_{m}^{i}=\\operatorname{GELU}\\left(\\frac{1}{\\sqrt{N}} \\sum_{j} l_{m}^{i, j} \\cdot \\boldsymbol{v}_{m}^{j}\\right)$. We also replaced the layer normalization after the attention weighting here through a simple scaling factor $\\frac{1}{\\sqrt{N}}$. Note that this also yields an in expectation constant contribution of context at initialization, independent of sequence length. However, the model can easily deviate from it during training. Simple Summation of Embeddings (sum): From a theoretical perspective summation is sufficient for general function approximation [42, 38, 30]. Therefore, we investigate to simply replace attention through a sum-reduce-broadcast operation. Max Pooling over Sequence Dimension (max): Similar to sum pooling, we can replace the attention sub-module through a simple max-reduce-broadcast operation over the sequence dimension. Note that max pooling over the sequence is a powerful operation, as the resulting embedding has a direct link to up to $d$ different tokens. If not varied in a corresponding experiment, we default architecture hyperparameters to $L=2$ Transformer-layers (consisting of an attention sub-module and feed forward sub-module each), $M=4$ heads to calculate the logits (if applicable), $d=128$ as model dimension and train on a total of 3200 batches of 32 example sequences each, using the Adam optimizer [15]. The hidden dimension of the feed forward sub-modules is $4 \\cdot d$ for the models $B E R T, M T E, N A P$ and $N O N$. For the models sum and max we increase the feed forward hidden dimension to approximately match the parameter counts of the other models. ## 5 Experiments and Results\n\nOur goal with this work is to provide an insight into the variety of performance implications that the architecture choices entail. We aim to provide these insights independent of any particular downstream application, as these architectures can be applied to a variety of tasks - from NLP to\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-05.jpg?height=289&width=1367&top_left_y=252&top_left_x=375)\n\nFigure 2: Left: Pseudo code for case distinction task data. The case distinction points 64 and 50 are chosen arbitrarily. Middle/Right: Task setup for outputs across all tokens (middle, cf. Section 5.1) and outputs from the first token (right, cf. Section 5.2). Green boxes represent the trainable network layers (shared across tokens) while red boxes represent the pooling across tokens, the focus of this work. The targets of the displayed examples would be $[0,0,0,1]$ and $[1,0,0,0]$, respectively. graph neural networks to reinforcement learning agents. We therefore focus on carefully crafted synthetic tasks that (1) are general enough in that we can expect the insights to generalize to a large set of downstream tasks and (2) let us modify key aspects that are hidden in real world data sets, such as a bias towards a certain sub-task. The focus on synthetic tasks also allows us to get a better grasp on the learning dynamics - the focus of this work - as we can train thousands of models in diverse hyperparameter combinations. To limit the influence of confounding variables, we generate new data points for every batch.",
    "normalizedattn-3": "This allows us to omit regularization. See Appendix Efor an in depth discussion of this setup. ### 5.1 Argmin-First-Argmax Case Distinction Task\n\nAs a first task, we consider an input pipeline where tokens from a fixed integer-vocabulary are translated to a randomly initialized embedding. To the embedded tokens, a (also randomly initialized) positional embedding is added to provide position-relative information. The sequence of tokens is then processed by several architecture dependent Transformer-layers (as described in Section 4). Finally, each contextualized embedding is projected to a single output. A softmax-crossentropy loss is applied over the sequence dimension to train the networks to pin-point a specific, input dependent token. See the example in the middle of Figure 2 for a visualization. Note that the ability to pin-point a specific token is an abstract task relevant to NLP (e.g., question answering or co-reference resolution), graph neural networks (e.g., finding the next hop in a shortest path) as well as reinforcement learning (e.g., action credit assignment). To make the task input dependent, we generate the data as given in the pseudo code in Figure 2 Note that the argmin and argmax make this task quite challenging from a learning perspective as the networks start from random embeddings which do not provide any ordering information. Which embeddings correspond to bigger integers and which to smaller integers has to be inferred during training. Further, the case distinction in this task lets us tweak the data bias towards each sub-task. Specifically, we consider a vocabulary size of $S=100$ integers (0-99) and uniformly random sampled sequences of $N=128$ tokens in length. This leads to a bias as $p_{\\text {argmin }}=1-\\left(1-\\frac{1}{S}\\right)^{N} \\approx 72.4 \\%$ of data points require the network to pin-point the minimum in the input sequence, $p_{\\text {first }} \\approx 20.1 \\%$ require the network to pin-point the first token of the sequence and the remaining $p_{\\text {argmax }} \\approx 7.5 \\%$ require the network to pin-point the maximum in the input. ### 5.1.1 Varying Model Dimension $d$\n\nAs a first investigation, we are interested in how varying the model dimension $d$ influences the architectures ability to learn the given task. For this, we train each of the architectures for each of the model dimensions $d \\in\\{8,16,32,64,128,256,512,1024\\}$ using 10 different learning rates and 5 random seeds for each hyperparameter combination. As we want to base our insights on as many results as possible, we derive a novel, human friendly visualization of results. Figure 3 (top row) shows the first results as follows: The outcome of each hyperparameter combination is reported as an RGB pixel in the plot, where the R (red) value corresponds to the accuracy of the worst performing random seed, the G (green) value corresponds to the average over the random seeds and the B (blue) value corresponds to the best performing random seed. For each value (R, G and $B$ ), the max over the course of training is taken. This assignment roughly translates as follows: The brighter, the better brighter pixels correspond to higher min-, mean- and max-accuracy. Blue/turquoise pixels highlight a\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-06.jpg?height=684&width=1359&top_left_y=245&top_left_x=383)\n\nFigure 3: Learning rate (y-axis) vs. model dimension $d$ (x-axis) on the argmin-first-argmax case distinction task (with output across all tokens). The pixels' R (red), G (green) and B (blue) values correspond to min-, mean- and max-accuracy, respectively, of the corresponding hyperparameter combination - see main text for details. Top row: Training accuracy (sequence length $N=128$ ). Bottom row: Validation accuracy when validating on sequences of half the length $(N=64)$. Crosses indicate the combination for best mean validation accuracy, which we report behind the model name. large performance variation across random seeds and black/grey pixels correspond to hyperparameter combinations where none of the random seeds could solve the task. These condensed results directly give rise to the following observations: (1) All models have some hyper-parameter combinations that learn the task well (white pixels). (2) The optimal learning rate depends on the model size, especially in the BERT architecture. This has profound implications for hyperparameter optimization: Tuning hyperparameters independent of each other might lead to sub-optimal results. (3) Models with probability simplex limitations ( $B E R T$ and $M T E$ ) work for a smaller range of hyperparameters. We provide case learning curves and additional results in Appendix F Next, given that all architectures are applicable to sequences of any length, we investigate how the architectures generalize to sequences of different length. Specifically, we validated each of the models trained above after every 100 batches on 32 batches with sequences of half the length $(N=64)$. We report the corresponding accuracies as before in Figure 3 (bottom row). Note that as we are taking the maximum over the course of training, we report optimal early stopping results. We observe: (1) The sum architecture does not generalize well in this task. (2) Our $N A P$ architecture seems to be the most robust to this generalization. ### 5.1.2 Case Accuracy under Varying Data Biases\n\nAs a next experiment we reset the model dimension to $d=128$ and vary the sequence length $N \\in\\{4,8,16,32,64,128,256,512\\}$. Note that this implicitly varies the biases $p_{\\text {argmin }}, p_{\\text {first }}$ and $p_{\\text {argmax }}$ in the data. We report the case specific accuracies in Figure 4 as follows: After every 100 batches, we validate the models on 1000 examples per case. Reported is the best accurracy over the course of training in form of pixel value with R (red) corresponding to the argmin-case accuracy, G (green) corresponding to the first-case accuracy and B (blue) corresponding to the argmax-case accuracy. As a consequence, white pixels correspond to all cases learned and yellow pixels correspond to the argmin- and first-case learned. We make the following observations: (1) If the learning rate is too low, models tend to focus on the majority case (indicated in a shift from blue to red as the bias shifts from the argmax- to the argmin-case with increasing sequence length $N$ ). (2) If the learning rate is too high, the $B E R T$ architecture tends to focus on the first-case. We believe this is due to the architectural bias towards local information as discussed in Section 3. Note that the first-case can be solved by relying on the local positional embedding. (3) Only the $N A P$ and max architecture manage to learn all three cases from the highly biased data when $N=256$.",
    "normalizedattn-4": "In Appendix G.1 we provide a further experiment investigating different batch sizes. The results are complementary. ![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-07.jpg?height=378&width=1357&top_left_y=245&top_left_x=384)\n\nFigure 4: Learning rate (y-axis) vs. sequence length $N$ (x-axis) on the case distinction task (with output across all tokens). RGB pixel values correspond to argmin-, first- and argmax-mean-caseaccuracies, respectively. ![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-07.jpg?height=389&width=1356&top_left_y=822&top_left_x=382)\n\nFigure 5: Learning rate (y-axis) vs. model dimension $d$ (x-axis) on the case distinction task with output from the first token. RGB pixel values correspond to argmin-, first- and argmax-mean-caseaccuracies. Crosses indicate the best mean accuracy, which we report behind the model name. ### 5.2 First Token Output\n\nThe task so far requires the architectures to learn an information flow between tokens to distinguish the case and decide per token, whether it is the token that is looked for or not. Now we investigate, whether all this information can also be aggregated into a single token. We therefore modify the architecture output slightly in that we only take the contextualized embedding of the first token and project from it to a vector of size $N$ (see example on the right in Figure 2). Note that this task set-up is harder and can highlight bottlenecks in the information flow across tokens. We fix the sequence lenght to $N=128$ and again vary the model dimension $d$. We report the the case specific mean accuracies in Figure 5, min-, mean- and max-overall-accuracies are given in Appendix G. 2 We observe: (1) All architectures learn for (almost) all combinations the now close to trivial first-case. (2) The sum pooling architecture does not learn any of the other cases. (3) Only $N A P$ and max learn all three cases in some hyperparameter combinations. The worse performance of NON highlights the advantage of online normalization of the logits. While the softmax provides some form of online normalization, we hypothesize that the worse performance of $M T E$ and $B E R T$ in this task stems from an information bottleneck induced by the probability simplex limitations. To test this hypothesis, we vary the number of attention heads $M$ with results in Figure 6. We observe that increasing the number of heads helps the MTE and BERT architecture, supporting our hypothesis. Note however, that MTE and BERT are still outperformed significantly by NAP. In Appendix G. 3 we provide a further experiment, varying the depth up to $L=64$. The results are complementary. ### 5.3 Mode Finding Task\n\nGiven the results so far, one could conclude that $\\max$ is the best choice due to its simplicity. Note however, that max has an architectural prior that is in line with the underlying task of finding the maximum or minimum of the sequence. To study the effect of architectural priors, we experiment on an additional task: Finding the mode/most common integer in the input sequence. Also this task\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-08.jpg?height=362&width=1351&top_left_y=250&top_left_x=390)\n\nFigure 6: Learning rate (y-axis) vs. attention heads $M$ (x-axis) on the case distinction task (output from the first token). RGB pixel values correspond to min, mean and max accuracy. Black crosses indicate the best mean accuracy, reported in the table to the right. Red crosses indicate that the best mean validation accuracy (when validating with $N=64$ ) was taken from a different combination. Bold numbers indicate a min-accuracy higher than the best max-accuracy of the other models. ![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-08.jpg?height=386&width=1354&top_left_y=905&top_left_x=383)\n\nFigure 7: Learning rate (y-axis) vs. model dimension $d$ (x-axis) on the mode finding task. RGB pixel values correspond to min, mean and max accuracy. Crosses indicate the reported best mean accuracy. has ties to NLP (e.g., sentiment analysis), graph neural networks (e.g., consensus/agreement) and reinforcement learning (e.g., count based exploration). Here we remove the positional embeddings, as this task can also be done on sets, and project from the contextualized embedding of the first token to a vector of dimension $S$ (the vocabulary size) over which we apply the softmax-cross-entropy loss.",
    "normalizedattn-5": "We keep $N=128$ but reduce $S$ to 10 to have meaningful modes. Ties are broken by taking the smallest integer of the ones with maximal occurrence. Results of varying the model dimension $d$ are reported in Figure 7 . We observe: (1) sum pooling works well on this task, as it has a suitable architecutral prior. (2) max pooling cannot learn the task, not even with a model dimension $d=1024=8 \\cdot N$. In Appendix H we provide an additional experiment, varying the vocabulary size.",
    "normalizedattn-6": "The results are complementary. We refer an interested reader to [39] for more on architecture-task alignment. ## 6 Conclusion\n\nTaking all observations together, we come to the following conclusions: Many recent works apply some sort of neural self-attention mechanism involving a softmax that projects the attention weights to the probability simplex. In this work we question the softmax in dot-product self-attention modules. Our theoretical investigation shows that softmax-attention outputs are constrained to the convex hull spanned by the value vectors. In our experiments we show that this can lead to an unwanted hyperparameter sensibility. We show that simpler architectures like max- and sum-pooling perform well when their architectural prior aligns with the underlying task. These architectures however fail in cases where the architectural prior is not suitable. As a solution, we propose to replace the softmax in attention through normalization. Our resulting normalized attention pooling (NAP) architecture is the only architecture of the 6 investigated that performs well in all tasks and setups, showing a broad applicability and better performance than the widely used BERT architecture.",
    "normalizedattn-7": "We hope that our work provides a stepping stone to examine architectures with respect to biases in the data. Further, we see a lot of potential for future work to investigate the correlated effects of hyperparameters. ## Broader Impact\n\nWe contrast different architectures on an abstract level in this work. Hence, there is no direct risk associated with system failure or an implication that would put some at a disadvantage. On the contrary: We see huge potential in our work to benefit (1) researchers and practitioners that do not have the computational resources to perform expensive hyperparameter optimizations and (2) minorities under-represented in data-sets, as our proposed architecture shows increased robustness to hyperparameter changes and biases in the data. ## Acknowledgments\n\nThe main author would like to thank his colleagues Dami\u00e1n Pascual, B\u00e9ni Egressy, Lukas Faber, Gino Brunner, Zhao Meng and Johannes Ackermann for the insightful discussions and helpful feedback on preliminary versions of this work. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth.",
    "normalizedattn-8": "arXiv preprint arXiv:2003.04887, 2020. [3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. [4] Timo Bram, Gino Brunner, Oliver Richter, and Roger Wattenhofer. Attentive multi-task deep reinforcement learning. 072019. [5] Gino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. On identifiability in transformers. In International Conference on Learning Representations, 2020. [6] Satrajit Chatterjee. Coherent gradients: An approach to understanding generalization in gradient descent-based optimization. In International Conference on Learning Representations, 2020. [7] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pretraining text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2020. [8] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2019. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics, 2019. [10] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In International Conference on Learning Representations, 2020. [11] Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. [12] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus).",
    "normalizedattn-9": "arXiv preprint arXiv:1606.08415, 2016. [13] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift.",
    "normalizedattn-10": "In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 448-456. JMLR.org, 2015.",
    "normalizedattn-11": "[14] Sarthak Jain and Byron C. Wallace. Attention is not explanation. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 3543-3556. Association for Computational Linguistics, 2019. [15] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [16] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations.",
    "normalizedattn-12": "In International Conference on Learning Representations, 2020. [17] Mengtian Li, Ersin Yumer, and Deva Ramanan. Budgeted training: Rethinking deep neural network training under resource constraints.",
    "normalizedattn-13": "In International Conference on Learning Representations, 2020. [18] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. arXiv preprint arXiv:2004.08249, 2020. [19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach.",
    "normalizedattn-14": "CoRR, abs/1907.11692, 2019. [20] Jawad Nagi, Frederick Ducatelle, Gianni A Di Caro, Dan Cire\u015fan, Ueli Meier, Alessandro Giusti, Farrukh Nagi, J\u00fcrgen Schmidhuber, and Luca Maria Gambardella. Max-pooling convolutional neural networks for vision-based hand gesture recognition.",
    "normalizedattn-15": "In 2011 IEEE International Conference on Signal and Image Processing Applications (ICSIPA), pages 342-347.",
    "normalizedattn-16": "IEEE, 2011. [21] Toan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention. arXiv preprint arXiv:1910.05895, 2019. [22] Emilio Parisotto, H Francis Song, Jack W Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning.",
    "normalizedattn-17": "arXiv preprint arXiv:1910.06764, 2019. [23] Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models.",
    "normalizedattn-18": "In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.",
    "normalizedattn-19": "Wallach, Rob Fergus, S.",
    "normalizedattn-20": "V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5998-6008, 2017. [33] Petar Veli\u010dkovi\u0107, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution of graph algorithms. In International Conference on Learning Representations, 2020. [34] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 11-20. Association for Computational Linguistics, 2019. [35] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In International Conference on Learning Representations, 2019. [36] Yuxin Wu and Kaiming He. Group normalization.",
    "normalizedattn-21": "Int. J. Comput. Vis., 128(3):742-755, 2020. [37] Zhanghao Wu*, Zhijian Liu*, Ji Lin, Yujun Lin, and Song Han. Lite transformer with long-short range attention. In International Conference on Learning Representations, 2020. [38] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [39] Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken ichi Kawarabayashi, and Stefanie Jegelka. What can neural networks reason about? In International Conference on Learning Representations, 2020. [40] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 5754-5764, 2019. [41] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? In International Conference on Learning Representations, 2020. [42] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab\u00e1s P\u00f3czos, Ruslan Salakhutdinov, and Alexander J. Smola. Deep sets. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.",
    "normalizedattn-22": "Wallach, Rob Fergus, S.",
    "normalizedattn-23": "V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 3391-3401, 2017. [43] Han Zhang, Ian J.",
    "normalizedattn-24": "Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention generative adversarial networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 7354-7363.",
    "normalizedattn-25": "PMLR, 2019. ## A Lemmas and Proofs\n\nLemma 1. No convex combination can represent the binary exclusive $O R(X O R)$ function defined on binary inputs $x_{1} \\in\\{0,1\\}$ and $x_{2} \\in\\{0,1\\}$ by the indicator function as $X O R\\left(x_{1}, x_{2}\\right)=\\mathbf{1}_{x_{1} \\neq x_{2}}$. Proof. Suppose there exist convex combination weights $a_{1}$ and $a_{2}$ with $a_{1}+a_{2}=1$, such that $a_{1} \\cdot x_{1}+a_{2} \\cdot x_{2}$ represents the XOR function. Plugging in $x_{1}=x_{2}=1$ yields $a_{1} \\cdot x_{1}+a_{2} \\cdot x_{2}=$ $a_{1}+a_{2}=1$, which gives the contradiction. Lemma 2. Given the two binary inputs $x_{1} \\in\\{0,1\\}$ and $x_{2} \\in\\{0,1\\}$, there exists an affine mapping $f:\\{0,1\\}^{2} \\rightarrow \\mathbb{R}^{2}$, such that\n\n$$\n\\text { normalized weighting }\\left(f, x_{1}, x_{2}\\right)=\\frac{f_{1}\\left(x_{1}, x_{2}\\right)-\\mu_{f\\left(x_{1}, x_{2}\\right)}}{\\sigma_{f\\left(x_{1}, x_{2}\\right)}} \\cdot x_{1}+\\frac{f_{2}\\left(x_{1}, x_{2}\\right)-\\mu_{f\\left(x_{1}, x_{2}\\right)}}{\\sigma_{f\\left(x_{1}, x_{2}\\right)}} \\cdot x_{2}\n$$\n\nis equivalent to the logical exclusive $O R$ given by the indicator function as $X O R\\left(x_{1}, x_{2}\\right)=\\mathbf{1}_{x_{1} \\neq x_{2}}$.",
    "normalizedattn-26": "Proof. For a vector $\\boldsymbol{l} \\in \\mathbb{R}^{2}$, the standard deviation $\\sigma_{l}$ can be simplified to\n\n$$\n\\sigma_{l}=\\sqrt{\\frac{1}{2} \\sum_{i \\in\\{1,2\\}}\\left(l_{i}-\\mu_{l}\\right)^{2}}=\\sqrt{\\frac{1}{2}\\left(\\left(l_{1}-\\frac{l_{1}+l_{2}}{2}\\right)^{2}+\\left(l_{2}-\\frac{l_{1}+l_{2}}{2}\\right)^{2}\\right)}=\\frac{1}{2}\\left|l_{1}-l_{2}\\right|\n$$\n\nand the normalization function reduces to\n\n$$\n\\operatorname{normalize}(\\boldsymbol{l})=\\left[\\frac{l_{1}-\\mu_{\\boldsymbol{l}}}{\\sigma_{\\boldsymbol{l}}}, \\frac{l_{2}-\\mu_{\\boldsymbol{l}}}{\\sigma_{\\boldsymbol{l}}}\\right]^{T}=\\left[\\frac{l_{1}-l_{2}}{\\left|l_{1}-l_{2}\\right|}, \\frac{l_{2}-l_{1}}{\\left|l_{1}-l_{2}\\right|}\\right]^{T}=\\left\\{\\begin{array}{lll}\n{[1,-1]^{T}} & \\text { if } & l_{1}>l_{2} \\\\\n{[-1,1]^{T}} & \\text { if } & l_{1}<l_{2} \\\\\n\\text { undef. } & \\text { if } & l_{1}=l_{2}\n\\end{array}\\right. $$\n\nAs an example, consider the affine mapping $f(\\boldsymbol{x})=\\boldsymbol{l}=\\left[3 x_{1}+1,2 x_{2}\\right]^{T}$, which for $x_{1} \\in\\{0,1\\}$ and $x_{2} \\in\\{0,1\\}$ results in the function\nnormalized weighting $\\left(f, x_{1}, x_{2}\\right)=\\frac{3 x_{1}+1-2 x_{2}}{\\left|3 x_{1}+1-2 x_{2}\\right|} \\cdot x_{1}+\\frac{-3 x_{1}-1+2 x_{2}}{\\left|3 x_{1}+1-2 x_{2}\\right|} \\cdot x_{2}= \\begin{cases}1 & \\text { if } x_{1} \\neq x_{2} \\\\ 0 & \\text { otherwise }\\end{cases}$\n\nWe note that for a realization of such an affine mapping across tokens given the weight sharing constraints of the discussed architectures we would need $x_{1}$ and $x_{2}$ to be distinguishable for the mapping to keys and queries, e.g., through positional embeddings. This however does not invalidate our conclusion that normalized weighting is more expressive than softmax weighting, as we do not require the inputs that are weighted to be distinguishable. ## B Sequence Length Dependent Local/Context-Focus\n\nFor the middle and right plot in Figure 1 we sample 16'384 value, key and query vectors of dimension $d_{h}=128$ per sequence length $N \\in\\{1,2,4,8,16,32,64,128,512,1024,2048\\}$ from a normal Gaussian $\\mathcal{N}\\left(\\mathbf{0}, \\boldsymbol{I}_{d_{h}}\\right)-\\boldsymbol{I}_{d_{h}}$ being the $d_{h}$-dimensional identity matrix. We split the samples to form the sequences and calculate the corresponding output vectors $\\boldsymbol{o}^{i}$ for $i \\in\\{1, \\ldots, N\\}$. Here, the softmax attention outputs are calculated as described in Section 2, while the mean-, sum- and max-outputs are calculated as mean-, sum- and max-reduce of the value vectors over the sequence dimension. For the normalized results we take the sum-output vectors and normalize them (over the $d_{h}$-dimensional vector dimension). Note that such a normalization can be applied to any of the aggregation methods to get qualitatively similar results. The plots in Figure 1 are generated by reporting the standard deviation over all output values and the mean norm of the output values, respectively. Given the numerous successes of Transformers in natural language processing, we conjecture that a bias towards local information might be beneficial in language modeling. However, the implicit dependence on sequence length in a model that should be oblivious to different input sequence lengths is questionable. We leave an in depth investigation to future work. ## C Architectures\n\nWe provide a schematic of 1 Transformer-layer of each architecture investigated in Figure 8. Our base architectures consist of 2 such layers followed by a projection to the output dependent on the task as described in the corresponding sections (cf. Section 5.1.5.2 and 5.3). ![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-13.jpg?height=1731&width=1288&top_left_y=473&top_left_x=416)\n\nFigure 8: Schematics of 1 Transformer-layer block of the different architectures investigated. Green layers correspond to the main weight matrices that are trained. Note that displayed dimensions are not to scale - the hidden dimension of the feed forward layer is larger than the model dimension and the hidden layer size in the feed forward network of \"max\" and \"sum\" are adjusted to approximately match the parameter count of the other architectures. ## D Architecture Modification Ablations\n\nAn empirical ablation of the modifications that lead from the $B E R T$ architecture to the $M T E$ architecture is given in Figure 9. The plots are generated as described in Sections 5.1.1 and 5.1.2\n![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-14.jpg?height=1449&width=1158&top_left_y=458&top_left_x=489)\n\nFigure 9: Learning rate (y-axis) vs. model dimension $d$ (x-axis) on the argmin-first-argmax case distinction task (with output across all tokens) - architecture modification ablation study. In the first two rows, RGB pixel values correspond to min-, mean- and max-accuracy. In the last two rows, RGB pixel values correspondto argmin-, first- and argmax-mean-case-accuracies. 1. row: Training accuracy (sequence length $N=128$ ). 2. row: Validation accuracy when validating on sequences of half the length $(N=64)$. 3. row: Training case accuracy (sequence length $N=128$ ). 4. row: Validation case accuracy when validating on sequences of half the length ( $N=64$ ). Crosses indicate the combination for best mean accuracy, the accuracies at these locations are reported in Table 1\n\nThe first column in Figure 9 corresponds to the original $B E R T$ architecture, trained with gradient norm clipping and learning rate warm up. The second column (- warm up) corresponds to the same architecture, but trained without learning rate warm up. Here we see that too high learning rates learn even less without learning rate warm up in the BERT architecture, hinting at a necessity for learning rate warm up for the original architecture. Table 1: Ablation study accuracy values taken from the hyper-parameter combination that led to the best mean overall accuracy, indicated by a cross in Figure 9\n\n|  |  | BERT | - warm up | - grad. clip | + normalize | + GELU |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Overall | min | $99.3 \\%$ | $99.4 \\%$ | $99.3 \\%$ | $99.2 \\%$ | $99.3 \\%$ |\n| Training | mean | $99.4 \\%$ | $99.5 \\%$ | $99.4 \\%$ | $99.3 \\%$ | $99.3 \\%$ |\n| Accuracy | max | $99.5 \\%$ | $99.6 \\%$ | $99.6 \\%$ | $99.5 \\%$ | $99.5 \\%$ |\n| Overall | min | $96.5 \\%$ | $96.9 \\%$ | $96.9 \\%$ | $96.3 \\%$ | $96.7 \\%$ |\n| Validation | mean | $97.2 \\%$ | $97.6 \\%$ | $97.2 \\%$ | $96.8 \\%$ | $97.3 \\%$ |\n| Accuracy | max | $98.2 \\%$ | $98.2 \\%$ | $98.2 \\%$ | $98.4 \\%$ | $98.2 \\%$ |\n| Mean Case | argmin | $99.3 \\%$ | $99.6 \\%$ | $99.5 \\%$ | $99.6 \\%$ | $99.5 \\%$ |\n| Accuracy | first | $100 \\%$ | $100 \\%$ | $100 \\%$ | $100 \\%$ | $100 \\%$ |\n| Training | argmax | $96.9 \\%$ | $98.1 \\%$ | $97.5 \\%$ | $96.5 \\%$ | $98.0 \\%$ |\n| Mean Case | argmin | $98.0 \\%$ | $98.0 \\%$ | $98.0 \\%$ | $98.1 \\%$ | $97.9 \\%$ |\n| Accuracy | first | $100 \\%$ | $100 \\%$ | $100 \\%$ | $100 \\%$ | $100 \\%$ |\n| Validation | argmax | $93.9 \\%$ | $93.8 \\%$ | $93.1 \\%$ | $91.2 \\%$ | $93.8 \\%$ |\n\nThe third column (- grad. clip) reports the results if we further remove gradient clipping from the training schedule. This does not seem to have a big impact in our setup. Next, we report in the forth column (+ normalize) the results of moving the layer normalization before the residual addition and introducing an additional layer normalization right after the attention mechanism as well as on the hidden layer of the feed forward network. Note that this change removes the bias towards local information discussed in the end of Section 3 . We see that this change leads to a profound shift in focus in regions where the learning rate is high: models with the original normalization focus the (local) first-case, while models with our normalization focus on the (majority) argmin-case. This is in line with the insights stated in Section 5.1.2. Finally, we report in the fifth column (+ GELU) the results of adding an additional GELU layer after the attention mechanism. These results correspond to the MTE architecture used throughout the paper. Apart from the performance landscape changes just mentioned, the best hyper-parameter accuracies remain similar throughout all modifications, cf. Table 1 . ## E Regularization Experiments\n\nTo limit the number of variables which are not accounted for in the experiments, we focus on the infinite data but limited training time regime. In this regime, every batch consists of new data points. We believe that this regime is of paramount interest in future research, as more devices create a constant stream of data and training is more limited by the available training time than the available data. This regime allows us to omit regularization in all architectures as over-fitting is not an issue. In fact, our supplementary experiments below as well as related work [16] show that regularization does not help in this regime. We leave a comparison of the architectures in the limited data regime to future work. Here, we show empirical results supporting the intuition that $L 2$ as well as dropout regularization does not help in our setup. For each of our tasks, we take our default hyper-parameters ( $d=$ 128, $L=2, M=4, N=128$ ) and train 5 random seeds per learning rate for models with regularization, varying the dropout rate in $\\{0.0625,0.125,0.25,0.5\\}$ and the $L 2$ regularization weighting in $\\{0.0001,0.001,0.01,0.1\\}$. Tables 2,3 and 4 report the best mean accuracy achieved with the small number behind the accuracies indicating the regularization used, 1 referring to the smallest, 4 to the largest. We underline the results where regularization did lead to an improvement in mean accuracy. Note however that these improvements should be taken with a grain of salt, as (1) none of these improvements is significant considering the performance variation across random seeds and (2) the regularized values are likely to be overestimated, as the max is taken over 40 averages ( 4 regularization values times 10 learning rates) as compared to 10 averages ( 10 learning rates) in the unregulated case. Table 2: Regularization results in the case distinction task with output taken across all tokens. The top three rows correspond to the best mean training accuracy, while the bottom three rows correspond to the best mean validation accuracy when validating on sequences of half the length. |  | BERT | MTE | NAP | NON | sum | max |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| unregularized | $99.3 \\%$ | $99.1 \\%$ | $99.3 \\%$ | $99.1 \\%$ | $98.9 \\%$ | $99.2 \\%$ |\n| with dropout | $98.1 \\%^{1}$ | $97.3 \\%^{1}$ | $97.8 \\%^{1}$ | $97.5 \\%^{1}$ | $97.3 \\%^{1}$ | $98.2 \\%^{1}$ |\n| with $L 2$-regularization | $99.3 \\%^{2}$ | $\\underline{99.2 \\%^{1}}$ | $99.2 \\%^{2}$ | $\\underline{99.2 \\%^{1}}$ | $98.9 \\%^{1}$ | $\\underline{99.4 \\%^{2}}$ |\n| unregularized | $95.5 \\%$ | $95.5 \\%$ | $97.0 \\%$ | $95.3 \\%$ | $75.0 \\%$ | $97.1 \\%$ |\n| with dropout | $94.4 \\%^{1}$ | $94.6 \\%^{1}$ | $96.8 \\%^{2}$ | $\\underline{96.0 \\%^{1}}$ | $\\underline{83.1 \\%^{1}}$ | $96.3 \\%^{1}$ |\n| with $L 2$-regularization | $\\underline{97.2 \\%^{2}}$ | $93.6 \\%^{2}$ | $\\underline{97.1 \\%{ }^{1}}$ | $\\underline{96.1 \\%^{2}}$ | $67.7 \\%^{2}$ | $\\underline{97.2 \\%^{2}}$ |\n\nTable 3: Regularization results in the case distinction task with output from the first token. The top three rows correspond to the best mean training accuracy, while the bottom three rows correspond to the best mean validation accuracy when validating on sequences of half the length. |  | BERT | MTE | NAP | NON | sum | max |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| unregularized | $36.6 \\%$ | $66.5 \\%$ | $94.5 \\%$ | $23.2 \\%$ | $22.8 \\%$ | $97.8 \\%$ |\n| with dropout | $\\underline{44.9 \\%^{1}}$ | $44.3 \\%^{1}$ | $85.0 \\%^{1}$ | $23.2 \\%^{1}$ | $22.6 \\%^{1}$ | $92.6 \\%^{1}$ |\n| with $L 2$-regularization | $36.0 \\%^{2}$ | $55.3 \\%^{1}$ | $93.8 \\%^{2}$ | $22.8 \\%^{1}$ | $22.8 \\%^{1}$ | $95.4 \\%^{1}$ |\n| unregularized | $36.7 \\%$ | $50.6 \\%$ | $83.9 \\%$ | $29.6 \\%$ | $28.5 \\%$ | $88.5 \\%$ |\n| with dropout | $\\underline{41.4 \\%^{2}}$ | $40.7 \\%^{1}$ | $74.6 \\%^{1}$ | $29.6 \\%^{3}$ | $\\underline{28.9 \\%^{4}}$ | $87.8 \\%^{1}$ |\n| with $L 2$-regularization | $\\underline{37.2 \\%^{2}}$ | $45.7 \\%^{1}$ | $82.5 \\%^{2}$ | $28.9 \\%^{1}$ | $\\underline{29.0 \\%^{1}}$ | $81.0 \\%^{1}$ |\n\nTable 4: Regularization results in the mode finding task. The top three rows correspond to the best mean training accuracy, while the bottom three rows correspond to the best mean validation accuracy when validating on sequences of twice the length. |  | BERT | MTE | NAP | NON | sum | max |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| unregularized | $99.6 \\%$ | $99.8 \\%$ | $99.6 \\%$ | $98.7 \\%$ | $99.8 \\%$ | $14.4 \\%$ |\n| with dropout | $93.9 \\%^{1}$ | $93.3 \\%^{1}$ | $94.3 \\%^{1}$ | $91.8 \\%^{1}$ | $93.3 \\%^{1}$ | $24.5 \\%^{1}$ |\n| with $L 2$-regularization | $99.5 \\%^{1}$ | $\\underline{99.9 \\%^{2}}$ | $\\underline{99.7 \\%^{1}}$ | $\\underline{98.8 \\%^{1}}$ | $\\underline{99.9 \\%^{4}}$ | $14.4 \\%^{2}$ |\n| unregularized | $95.3 \\%$ | $95.4 \\%$ | $94.9 \\%$ | $91.3 \\%$ | $95.8 \\%$ | $13.5 \\%$ |\n| with dropout | $94.8 \\%^{2}$ | $95.4 \\%^{2}$ | $93.8 \\%^{1}$ | $\\underline{92.6 \\%^{1}}$ | $95.7 \\%^{2}$ | $13.4 \\%^{4}$ |\n| with $L$ 2-regularization | $94.7 \\%^{1}$ | $\\underline{96.0 \\%^{1}}$ | $94.9 \\%^{1}$ | $\\underline{94.7 \\%^{2}}$ | $95.8 \\%^{1}$ | $\\underline{13.7 \\%^{1}}$ |\n\nOverall we note that none of the architectures consistently benefits from regularization in our setup and regularization often decreases mean performance. Further, we point out that the best performance with regularization is most of the times achieved with the smallest regularization. ## F Case Learning Curves\n\nFigures 10, 11 and 12 show the case accuracies over the course of training. The corresponding results in the main text are given in Figure 3 (top row). Besides the observations made in the main text, a few additional insights can be noted: (1) Cases are mostly learned in the order of their occurrences (recall that $72.37 \\%$ of the examples are from the argmin case, $20.09 \\%$ are from the first case and $7.53 \\%$ are from the argmax case). This is to be expected when training with gradient descent, cf. [6]. (2) This order is not always given in the BERT architecture. Besides the focus on the first case if the learning rate is too high - discussed in the main text - we also highlight a curiosity that occurs when the model dimension is too small (see plot highlighted in with red in Figure 10): The first case is learned and then unlearned in favor of the argmin case.",
    "normalizedattn-27": "Note that all 5 random seeds follow this pattern. Note also that for a different learning rate, the opposite holds as seen in the plot just below the highlighted plot. We highly encourage an interested reader to check out our code releas ${ }^{4}$. which includes all results as well as visualization scripts to inspect them further. [^2]![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-17.jpg?height=2040&width=1261&top_left_y=260&top_left_x=417)\n\nFigure 10: Case accuracies over the course of training on the argmin-first-argmax case distinction task with output across all tokens, cf. Section 5.1 Each small sub-plot shows the case accuracies ( y -axis, bottom is set to $0 \\%$, top to $100 \\%$ ) over the course of training ( $x$-axis). Solid lines represent the mean accuracy over the 5 random seeds while shaded areas fill the spread between min- and max-accuracy achieved. Models BERT and MTE are shown here, cf. Figures 11 and 12\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-18.jpg?height=2034&width=1262&top_left_y=259&top_left_x=416)\n\nFigure 11: Case accuracies over the course of training on the argmin-first-argmax case distinction task with output across all tokens, cf. Section 5.1 Each small sub-plot shows the case accuracies ( y -axis, bottom is set to $0 \\%$, top to $100 \\%$ ) over the course of training ( x -axis). Solid lines represent the mean accuracy over the 5 random seeds while shaded areas fill the spread between min- and max-accuracy achieved. Models NAP and NON are shown here, cf. Figures 10 and 12\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-19.jpg?height=2023&width=1269&top_left_y=274&top_left_x=406)\n\nFigure 12: Case accuracies over the course of training on the argmin-first-argmax case distinction task with output across all tokens, cf. Section 5.1 Each small sub-plot shows the case accuracies ( $y$-axis, bottom is set to $0 \\%$, top to $100 \\%$ ) over the course of training ( $x$-axis). Solid lines represent the mean accuracy over the 5 random seeds while shaded areas fill the spread between min- and max-accuracy achieved. Models sum and max are shown here, cf. Figures 10 and 11 . ## G Argmin-First-Argmax Case Distinction Task - Additional Results\n\n## G. 1 Varying Batch Size\n\nIn Figure 13 we provide the case accuracy results of an additional experiment, varying the batch size. In this experiment we train the models using different batch sizes, adjusting the number of training steps accordingly to keep the total number of training points seen constant. With this experiment we aim to show the training behaviour of the different architectures if we go from single example batches (many, potentially noisier updates) to batches of size 128 - a batch size in which each batch contains in expectation several examples per case, but fewer updates are made to the network parameters. Besides replicating several insights made in the main text, this experiment additionally shows: (1) smaller batches require a smaller learning rate, supporting our argument that hyper-parameters should not be optimized independent of each other. (2) The focus of BERT on the first-case when the learning rate is too high is amplified in smaller batches. ![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-20.jpg?height=682&width=1363&top_left_y=838&top_left_x=381)\n\nFigure 13: Learning rate (y-axis) vs. batch size ( x -axis) on the argmin-first-argmax case distinction task (with output across all tokens). RGB pixel values correspond to argmin-, first- and argmax-caseaccuracies, respectively. ## G. 2 First Token Output - Varying Model Dimension\n\nSection 5.2 discusses the case accuracies when training on the case distinction task with outputs taken from the first token. In Figure 14 we addtionally give best the min-, mean- and max-accuracies over the course of training. The top row corresponds to in-distribution/training accuracy ( $N=128$ ) while the bottom row corresponds to out-of-distribution generalization accrucay when validating on sequences of half the length $(N=64)$. Again we note a correlation between optimal learning rate and model dimension, especially in the BERT and MTE architecture. We also note that these probability simplex constrained architectures have a large performance variation across random seeds in this setup. ## G. 3 First Token output - Varying Depth\n\nIn this section we investigate whether our results are tied to the shallow architecture of $L=2$ Transformer layers. We therefore vary the number of Tranfromer layers $L$ and report the results on the case distinction task with outputs taken from the first token in Figure 15. The results lead us to the following observations: (1) The BERT architecture does seem to perform better when the number of Transformer layers is increased to $L=4$. However, the performance degrades if we further increase the depth. (2) The NAP architecture achieves a higher best mean accuracy and performs well on a\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-21.jpg?height=684&width=1363&top_left_y=250&top_left_x=382)\n\nFigure 14: Learning rate (y-axis) vs. model dimension $d$ (x-axis) on the case distinction task with output from the first token. RGB pixel values correspond to min, mean and max accuracy. Top row: Training accuracy (sequence length $N=128$ ). Bottom row: Validation accuracy when validating on sequences of half the length $(N=64)$. Crosses indicate the combination for best mean validation accuracy, which we report behind the model name. ![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-21.jpg?height=684&width=1364&top_left_y=1206&top_left_x=378)\n\nFigure 15: Learning rate (y-axis) vs. Transformer-layers $L$ (x-axis) on the case distinction task (output from the first token). RGB pixel values correspond to min, mean and max accuracy. Top row: Training accuracy (sequence length $N=128$ ). Bottom row: Validation accuracy when validating on sequences of half the length $(N=64)$. Crosses indicate the combination for best mean validation accuracy, which we report behind the model name. wide range of depths. (3) The max architecture performs well on the biggest range of hyperparameters. This is due to the beneficial architectural prior as discussed in the main text. ## H Mode Finding Task - Varying Vocabulary Size\n\nFigure 16 shows the results of an additional experiment, varying the vocabulary size $S$ while keeping the sequence length $N=128$ constant during training. For this experiment, we also vary the total\nnumber of training steps and set it to $400 \\cdot S$, to keep the number of examples seen per vocabulary token approximately constant. We also include zero-shot generalization results when testing on sequences of twice the length $(N=256)$. Compared to the case distinction task we can do such a generalization evaluation here as we do not learn any positional embeddings in this setup. We make the following observations: (1) max completely fails to learn in any of the vocabulary sizes. Note that the shading to the left merely corresponds to the majority class base rate. (2) NAP struggles when the vocabulary consists of only 2 tokens. This is expected, as the mean subtraction in the normalization effectively removes the task relevant information (the mode) in this case. Note however, that for a high enough learning rate, the model learns to use the bias parameter $b$ introduced in Equation 1effectively reverting to sum pooling. (3) While all models learn the task well on small vocabularies, $N A P$ outperforms all other approaches significantly when $S$ gets larger then the training sequence length, cf. Table 5 . ![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-22.jpg?height=668&width=1360&top_left_y=753&top_left_x=380)\n\nFigure 16: Learning rate (y-axis) vs. vocabulary size $S$ (x-axis) on the mode finding task. RGB pixel values correspond to min, mean and max accuracy. Top row: Training accuracy (sequence length $N=128$ ). Bottom row: Validation accuracy when validating on sequences of twice the length ( $N=256$ ). Crosses indicate the learning rate for best mean accuracy, which we report in Table 5 . Table 5: Best mean accuracy per vocabulary size, taken from the combinations indicated in Figure 16 First six rows correspond to training accuracies, bottom six rows correspond to validation accuracies. Bold numbers indicate a min-accuracy higher than the best max accuracy of all other models. |  | $S=2$ | $S=4$ | $S=8$ | $S=16$ | $S=32$ | $S=64$ | $S=128$ | $S=256$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| BERT | $100 \\%$ | $99.9 \\%$ | $99.9 \\%$ | $92.1 \\%$ | $72.5 \\%$ | $76.2 \\%$ | $77.4 \\%$ | $74.4 \\%$ |\n| MTE | $100 \\%$ | $100 \\%$ | $99.9 \\%$ | $99.8 \\%$ | $99.3 \\%$ | $97.3 \\%$ | $73.3 \\%$ | $64.9 \\%$ |\n| NAP | $100 \\%$ | $99.9 \\%$ | $99.8 \\%$ | $99.6 \\%$ | $99.7 \\%$ | $\\mathbf{9 9 . 6 \\%}$ | $97.4 \\%$ | $\\mathbf{8 4 . 6 \\%}$ |\n| NON | $100 \\%$ | $99.9 \\%$ | $99.2 \\%$ | $97.3 \\%$ | $74.7 \\%$ | $71.5 \\%$ | $65.2 \\%$ | $61.5 \\%$ |\n| sum | $100 \\%$ | $100 \\%$ | $99.9 \\%$ | $99.8 \\%$ | $99.7 \\%$ | $99.2 \\%$ | $97.5 \\%$ | $60.6 \\%$ |\n| max | $55.7 \\%$ | $30.1 \\%$ | $17.3 \\%$ | $10.4 \\%$ | $6.6 \\%$ | $4.6 \\%$ | $3.6 \\%$ | $3.1 \\%$ |\n| BERT | $100 \\%$ | $98.2 \\%$ | $95.8 \\%$ | $88.0 \\%$ | $65.7 \\%$ | $68.6 \\%$ | $68.0 \\%$ | $53.0 \\%$ |\n| MTE | $99.2 \\%$ | $98.4 \\%$ | $96.1 \\%$ | $93.6 \\%$ | $90.5 \\%$ | $85.4 \\%$ | $61.6 \\%$ | $38.9 \\%$ |\n| NAP | $99.6 \\%$ | $98.4 \\%$ | $95.8 \\%$ | $93.1 \\%$ | $90.6 \\%$ | $90.4 \\%$ | $84.3 \\%$ | $\\mathbf{6 4 . 3 \\%}$ |\n| NON | $100 \\%$ | $97.7 \\%$ | $93.1 \\%$ | $85.7 \\%$ | $66.4 \\%$ | $58.3 \\%$ | $50.2 \\%$ | $46.4 \\%$ |\n| sum | $99.0 \\%$ | $97.9 \\%$ | $96.7 \\%$ | $94.4 \\%$ | $91.6 \\%$ | $89.1 \\%$ | $85.9 \\%$ | $45.6 \\%$ |\n| max | $53.8 \\%$ | $29.3 \\%$ | $16.1 \\%$ | $9.5 \\%$ | $6.0 \\%$ | $4.2 \\%$ | $3.0 \\%$ | $2.1 \\%$ |\n\n\n[^0]:    ${ }^{1}$ https://github.com/OliverRichter/normalized-attention\n\n[^1]:    ${ }^{2}$ As an example, BERT [9] initializes $\\boldsymbol{W}$ with parameters drawn from a truncated normal distribution with standard deviation 0.02 and $\\boldsymbol{b}$ to $\\mathbf{0}$. ${ }^{3}$ https://github.com/google-research/bert\n\n[^2]:    ${ }^{4}$ https://github.com/OliverRichter/normalized-attention\n\n"
}