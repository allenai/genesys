{
    "httyh-0": "# How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections \n\n\\author{\nAlbert $\\mathrm{Gu}^{* \\dagger}$, Isys Johnson ${ }^{* \\ddagger}$, Aman Timalsina ${ }^{\\ddagger}$, Atri Rudra ${ }^{\\ddagger}$, and Christopher R\u00e9 ${ }^{\\dagger}$ <br> ${ }^{\\dagger}$ Department of Computer Science, Stanford University <br> ${ }^{\\dagger}$ albertgu@stanford.edu, chrismre@cs.stanford.edu <br> ${ }^{\\ddagger}$ Department of Computer Science and Engineering, University at Buffalo <br> $\\ddagger$ isysjohn, amantima, atri\\}@buffalo.edu\n\n}\n\n\n#### Abstract\n\nLinear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4).",
    "httyh-1": "A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to $86 \\%$ on the Long Range Arena benchmark, with $96 \\%$ on the most difficult Path-X task. ## 1 Introduction\n\nThe Structured State Space model (S4) is a recent deep learning model based on continuous-time dynamical systems that has shown promise on a wide variety of sequence modeling tasks [7]. It is defined as a linear time-invariant (LTI) state space model (SSM), which give it multiple properties [6]: as an SSM, S4 can be simulated as a discrete-time recurrence for efficiency in online or autoregressive settings, and as a LTI model, S4 can be converted into a convolution for parallelizability and computational efficiency at training time. These properties give S4 remarkable computational efficiency and performance, especially when modeling continuous signal data and long sequences. Despite its potential, several aspects of the S4 model remain poorly understood. Most notably, Gu et al. [7] claim that the long range effects of S 4 arise from instantiating it with a particular matrix they call the HiPPO matrix. However, this matrix was actually derived in prior work for a particular time-varying system [5], and the use of this matrix in a time-invariant SSM did not have a mathematical interpretation. Consequently, the mechanism by which S 4 truly models long-range dependencies is actually not known. Beyond this initialization, several other aspects of parameterizing and training S4 remain poorly understood. For example, S 4 involves an important timescale parameter $\\Delta$, and suggests a method for parameterizing and initializing this parameter, but does not discuss its meaning or provide a justification. [^0]![](https://cdn.mathpix.com/cropped/2024_09_12_37931de6e86a3ea35c37g-02.jpg?height=364&width=1621&top_left_y=247&top_left_x=251)\n\nFigure 1: In this work, we focus on a more intuitive interpretation of state space models as a convolutional model where the convolution kernel is a linear combination of basis functions. We introduce a generalized framework that allows deriving state spaces $x^{\\prime}=\\boldsymbol{A} x+\\boldsymbol{B} u$ that produce particular basis functions, leading to several generalizations and new methods. (Left: LegS) We prove that the particular $\\boldsymbol{A}$ matrix chosen in S4 produces Legendre polynomials under an exponential re-scaling, resulting in smooth basis functions with a closed form formula. This results in a simple mathematical interpretation of the method as orthogonalizing against an exponentially-decaying measure, granting the system better ability to model long-range dependencies. (Middle, Right: FouT) We derive a new SSM that produces approximations to the truncated Fourier basis, perhaps the most intuitive and ubiquitous set of basis functions. This method generalizes sliding Fourier Transforms and local convolutions (i.e. CNNs), and can also encode spike functions to solve classic memorization tasks. This work aims to provide a comprehensive theoretical exposition of several aspects of S4. The major contribution of this work is a cleaner, more intuitive, and much more general formulation of the HiPPO framework. This result directly generalizes all previous known results in this line of work [5, 6, 7, 13]. As immediate consequences of this framework:\n\n- We prove a theoretical interpretation of S4's state matrix $\\boldsymbol{A}$, explaining S4's ability to capture long-range dependencies via decomposing the input with respect to an infinitely long, exponentially-decaying measure (Fig. 1 (Left)). - We derive new HiPPO matrices and corresponding S4 variants that generalize other nice basis functions. For example, our new method S4-FouT produces truncated Fourier basis functions. This method thus automatically captures sliding Fourier transforms (e.g. the STFT and spectrograms) which are ubiquitous as a hand-crafted signal processing tool, and can also represent any local convolution, thus generalizing conventional CNNs (Fig. 1 (Middle)). - We provide an intuitive explanation of the timescale $\\Delta$, which has a precise interpretation as controlling the length of dependencies that the model captures. Our framework makes it transparent how to initialize $\\Delta$ for a given task, as well as how to initialize the other parameters (in particular, the last SSM parameter $\\boldsymbol{C}$ ) to make a deep SSM variance-preserving and stable. Empirically, we validate our theory on synthetic function reconstruction and memorization tasks, showing that empirical performance of state space models in several settings is predicted by the theory. For example, our new S4-FouT method, which can provably encode a spike function as its convolution kernel, performs best on a continuous memorization task compared to other SSMs and other models, when $\\Delta$ is initialized correctly. Finally, we show that the original S4 method is still best on very long range dependencies, achieving a new state of the art of $\\mathbf{8 6 \\%}$ average on Long Range Arena, with $\\mathbf{9 6 \\%}$ on the most difficult Path-X task that even the other S 4 variants struggle with. ## 2 Background\n\n### 2.1 State Space Models: A Continuous-time Latent State Model\n\nThe state space model (SSM) is defined by the simple differential equation (1) and (2). It maps a 1-D input signal $u(t)$ to an $N$-D latent state $x(t)$ before projecting to a 1-D output signal $y(t)$. $$\n\\begin{aligned}\nx^{\\prime}(t) & =\\boldsymbol{A}(t) x(t)+\\boldsymbol{B}(t) u(t) \\\\\ny(t) & =\\boldsymbol{C}(t) x(t)+\\boldsymbol{D}(t) u(t)\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\nK(t) & =\\boldsymbol{C} e^{t \\boldsymbol{A}} \\boldsymbol{B} \\\\\ny(t) & =(K * u)(t)\n\\end{aligned}\n$$\n\nFor the remainder of this paper, we will assume $\\boldsymbol{D}=0$ and omit it for simplicity, unless explicitly mentioned. SSMs can in general have dynamics that change over time, i.e. the matrices $\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C}, \\boldsymbol{D}$ are a function of $t$ in (1) and (2). However, when they are constant the system is linear time invariant (LTI), and is equivalent to a convolutional system (3). The function $K(t)$ is called the impulse response which can also be defined as the output of the system when the input $u(t)=\\delta(t)$ is the impulse or Dirac delta function. We will call these time-invariant state space models (TSSM). These are particularly important because the equivalence to a convolution makes TSSMs parallelizable and very fast to compute, which is critical for S4's efficiency. Our treatment of SSMs will consider the $(\\boldsymbol{A}, \\boldsymbol{B})$ parameters separately from $\\boldsymbol{C}$. We will refer to an SSM as either the tuple $(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C})$ (referring to (3)) or $(\\boldsymbol{A}, \\boldsymbol{B})$ (referring to Definition 1) when the context is unambiguous. We also drop the T in TSSM when the context is clearly time-invariant. Definition 1. Given a TSSM $(\\boldsymbol{A}, \\boldsymbol{B}), e^{t \\boldsymbol{A}} \\boldsymbol{B}$ is a vector of $N$ functions which we call the $\\boldsymbol{S S M}$ basis. The individual basis functions are denoted $K_{n}(t)=\\boldsymbol{e}_{n}^{\\top} e^{t \\boldsymbol{A}} \\boldsymbol{B}$, which satisfy $x_{n}(t)=\\left(u * K_{n}\\right)(t)=\\int_{-\\infty}^{t} K_{n}(t-$ s) $u(s) d s$. Here $\\boldsymbol{e}_{n}$ is the one-hot basis vector. This definition is motivated by noting that the SSM convolutional kernel is a linear combination of the SSM basis controlled by the vector of coefficients $\\boldsymbol{C}, K(t)=\\sum_{n=0}^{N-1} \\boldsymbol{C}_{n} K_{n}(t)$. Discrete SSM with Timescales. To be applied on a discrete input sequence $\\left(u_{0}, u_{1}, \\ldots\\right)$ instead of continuous function $u(t),(1)$ must be discretized by a step size $\\Delta$ that represents the resolution of the input. Conceptually, the inputs $u_{k}$ can be viewed as sampling an implicit underlying continuous signal $u(t)$, where $u_{k}=u(k \\Delta)$. Analogous to the fact that the SSM has equivalent forms either as an dynamical system (1) or a continuous convolution (3), the discrete-time SSM can be computed either as a recurrence or a discrete convolution. The mechanics to compute the discrete-time SSM has been discussed in previous works $[6,7]$. For our purposes, we only require the following fact: for standard discretization methods used in prior work, discretizing the state space $(\\boldsymbol{A}, \\boldsymbol{B})$ at a step size $\\Delta$ is exactly equivalent to discretizing the state space $(\\Delta \\boldsymbol{A}, \\Delta \\boldsymbol{B})$ at a step size 1 . This allows thinking of $\\Delta$ simply as modulating the SSM parameters $(\\boldsymbol{A}, \\boldsymbol{B})$ instead of representing a step size. A poorly understood question from prior work is how to interpret and choose this $\\Delta$ parameter, especially when the input $u_{k}$ does not actually arise from uniformly sampling an underlying continuous signal. S 4 specifies to log-uniformly initialize $\\Delta$ in the range $\\left(\\Delta_{\\min }, \\Delta_{\\max }\\right)=(0.001,0.1)$, but do not provide a concrete justification. In Section 3.3 we show a simpler interpretation of $\\Delta$ directly in terms of the length of dependencies in a discrete input sequence. ### 2.2 HiPPO: High-order Polynomial Projection Operators\n\nS4 is defined as a TSSM where $(\\boldsymbol{A}, \\boldsymbol{B})$ is initialized with a particular formula (4). This was called the HiPPO matrix in [7], but is actually just one of several such special matrices derived in [5]. To disambiguate other variants of S4, we refer to the full S4 method using this HiPPO SSM as S4-LegS. Other cases considered in this work include LegT from prior work (5) and FouT that we introduce (6). (HiPPO-LegS)\n\n$$\n\\begin{aligned}\n\\boldsymbol{A}_{n k}= & -(2 n+1)^{\\frac{1}{2}}(2 k+1)^{\\frac{1}{2}} \\\\\n& \\cdot \\begin{cases}1 & n>k \\\\\n\\frac{n+1}{2 n+1} & n=k \\\\\n0 & n<k\\end{cases} \\\\\n\\boldsymbol{B}_{n}= & (2 n+1)^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\n(HiPPO-LegT)\n\n$$\n\\begin{aligned}\n\\boldsymbol{A}_{n k}= & -(2 n+1)^{\\frac{1}{2}}(2 k+1)^{\\frac{1}{2}} \\\\\n& \\cdot \\begin{cases}1 & k \\leq n \\\\\n(-1)^{n-k} & k \\geq n\\end{cases} \\\\\n\\boldsymbol{B}_{n}= & (2 n+1)^{\\frac{1}{2}}\n\\end{aligned}\n$$\n\n(HiPPO-FouT)\n\nThese matrices were originally motivated by the question of 'online memorization' of an input signal. The key idea is that for a suitably chosen SSM basis $\\boldsymbol{A}, \\boldsymbol{B}$, then at any time $t$, the current state $x(t)$ can be used to approximately reconstruct the entire input $u$ up to time $t$ (Fig.",
    "httyh-2": "2). The main theoretical idea is as follows. Suppose that the basis functions satisfy Definition 2. Definition 2. We call an SSM $(\\boldsymbol{A}(t), \\boldsymbol{B}(t))$ an orthogonal SSM (OSSM) for the basis $p_{n}(t, s)$ and measure $\\omega(t, s) \\geq 0$ if the functions $K_{n}(t, s)=p_{n}(t, s) \\omega(t, s)$ satisfy, at all times $t$,\n\n$$\nx_{n}(t)=\\int_{-\\infty}^{t} K_{n}(t, s) u(s) d s \\quad \\int_{-\\infty}^{t} p_{n}(t, s) p_{m}(t, s) \\omega(t, s) d s=\\delta_{n, m}\n$$\n\nIn the case of a time-invariant OSSM (TOSSM), $K_{n}(t, s)=: K_{n}(t-s)$ (depends only on $t-s$ ) giving us Definition 1 with measure $\\omega(t-s):=\\omega(t, s)$ and basis $p_{n}(t-s):=p_{n}(t, s)$. To be more specific about terminology, $p_{n}(t)$ and $\\omega_{n}(t)$ are called the basis and measure for orthogonal SSMs (Definition 2), while $K_{n}(t)$ are called the SSM basis kernels which applies more generally to all SSMs (Definition 1). The distinction will be made clear from context, notation, and the word \"kernel\" referring to $K_{n}(t)$. For OSSMs, $(p, \\omega)$ and $K$ are uniquely determined by each other, so we can refer to an OSSM by either. One direction is obvious: $(p, \\omega)$ determine $K$ via $K_{n}(t, s)=p_{n}(t, s) \\omega(t, s)$. Proposition 1. If a set of kernel functions satisfies $K_{n}(t, s)=p_{n}(t, s) \\omega(t, s)$ where the functions $p_{n}$ are complete and orthogonal w.r.t. $\\omega$ (equation (7) right), $p$ and $\\omega$ are unique. Equation (7) is equivalent to saying that for every fixed $t,\\left\\langle p_{n}, p_{m}\\right\\rangle_{\\omega}=\\delta_{n, m}$, or that $p_{n}$ are an orthonormal basis with respect to measure $\\omega$. More formally, defining $p_{n}^{(t)}(s)=p_{n}(t, s)$ and $\\omega^{(t)}$ similarly, then $p_{n}^{(t)}$ are orthonormal in the Hilbert space with inner product $\\left.\\langle p, q\\rangle=\\int p(s) q(s) \\omega^{(t)}(s) d s\\right)$. By equation (7), $x_{n}(t)=\\int_{-\\infty}^{t} u(s) K_{n}(t, s) d s=\\left\\langle u, p_{n}^{(t)}\\right\\rangle_{\\omega^{(t)}}$ where $p_{n}^{(t)}(s)=p_{n}(t, s)$. Thus at all times $t$, the state vector $x(t)$ is simply the projections of $\\left.u\\right|_{\\leq t}$ onto a orthonormal basis, so that the history of $u$ can be reconstructed from $x(t)$. HiPPO called this the online function approximation problem [5]. Proposition 2. Consider an OSSM that satisfies (7) and fix a time t. Furthermore suppose that in the limit $N \\rightarrow \\infty$, the $p_{n}^{(t)}$ are a complete basis on the support of $\\omega$. Then $u(s)=\\sum_{n=0}^{\\infty} x_{n}(t) p_{n}(t, s)$ for all $s \\leq t$. The main barrier to using Proposition 2 for function reconstruction is that SSMs are in general not OSSMs. For example, even though we will show that (4) is an TOSSM, its diagonalization is not. Proposition 3. There is no TOSSM with the diagonal state matrix $\\boldsymbol{A}=\\operatorname{diag}\\{-1,-2, \\ldots\\}$. HiPPO can be viewed as a framework for deriving specific SSMs that do satisfy (7). The original HiPPO methods and its generalizations [5,6] primarily focused on the case when the $p_{n}$ are orthogonal polynomials,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_37931de6e86a3ea35c37g-05.jpg?height=427&width=1632&top_left_y=250&top_left_x=255)\n\nFigure 2: Given an input function $u(t)$ (black), HiPPO compresses it online into a state vector $x(t) \\in \\mathbb{R}^{N}$ via equation (1). Specific cases of HiPPO matrices $\\boldsymbol{A}, \\boldsymbol{B}$ are derived so that at every time $t$, the history of $u$ up to time $t$ can be reconstructed linearly from $x(t)$ (red), according to a measure (green). (Left) The HiPPO-LegT method orthogonalizes onto the Legendre polynomials against a time-invariant uniform measure, i.e. sliding windows. (Right) The original HiPPO-LegS method is not time-invariant system. When used as a time-varying ODE $x^{\\prime}=\\frac{1}{t} \\boldsymbol{A} x+\\frac{1}{t} \\boldsymbol{B} u$, $x(t)$ represents the projection of the entire history of $u$ onto the Legendre polynomials. It was previously unknown how to interpret the time-invariant version of this ODE using the same $(\\boldsymbol{A}, \\boldsymbol{B})$ matrices. ![](https://cdn.mathpix.com/cropped/2024_09_12_37931de6e86a3ea35c37g-05.jpg?height=546&width=1640&top_left_y=1017&top_left_x=251)\n\nFigure 3: (HiPPO-LegT.) (Left) First 4 basis functions $K_{n}(t)$ for state size $N=1024$ (Proposition 4). (Right) Choosing a particular $\\boldsymbol{C}$ produces a spike kernel or \"delay network\" (Theorem 9). and specifically looked for solutions to (7), which turn out to be SSMs. We have rephrased the HiPPO definition in Definition 2 to start directly from SSMs. We discuss the two most important cases previously introduced. HiPPO-LegT. (5) is a TOSSM that approximates the truncated Legendre polynomials (Fig.",
    "httyh-3": "3). Definition 3. Let $\\mathbb{I}(t)$ be the indicator function for the unit interval $[0,1]$. We denote the Legendre polynomials rescaled to be orthonormal on $[0,1]$ as $L_{n}(t)$, satisfying $\\int L_{n}(t) L_{m}(t) \\mathbb{I}(t) d t=\\delta_{n, m}$. Proposition 4. As $N \\rightarrow \\infty$, the SSM with $(\\boldsymbol{A}, \\boldsymbol{B})$ in (5) is a TOSSM with\n\n$$\n\\omega(t)=\\mathbb{I}(t) \\quad K_{n}(t)=L_{n}(t) \\mathbb{I}(t)\n$$\n\nThis particular system was the precursor to HiPPO and has also been variously called the Legendre Delay Network (LDN) or Legendre Memory Unit (LMU) [13, 14]. The original motivation of this system was not through the online function approximation formulation of HiPPO, but through finding an optimal SSM approximation to the delay network that has impulse response $K(t)=\\delta(t-1)$ representing a time-lagged output by 1 time unit (Fig.",
    "httyh-4": "3). We state and provide an alternate proof of this result in Section 3.2, Theorem 9. HiPPO-LegS. Unlike the HiPPO-LegT case which is an LTI system (1) (i.e. TOSSM), the HiPPO-LegS matrix (4) was meant to be used in a time-varying system $x^{\\prime}(t)=\\frac{1}{t} \\boldsymbol{A} x(t)+\\frac{1}{t} \\boldsymbol{B} u(t)$ [5]. In contrast to HiPPOLegT, which reconstructs onto the truncated Legendre polynomials in sliding windows $[t-1, t]$, HiPPO-LegS reconstructs onto Legendre polynomials on \"scaled\" windows $[0, t]$; since the window changes across time, the system is not time-invariant (Fig.",
    "httyh-5": "2). Theorem 5. The $S S M\\left(\\frac{1}{t} \\boldsymbol{A}, \\frac{1}{t} \\boldsymbol{B}\\right)$ for $(\\boldsymbol{A}, \\boldsymbol{B})$ in (4) is an OSSM with\n\n$$\n\\omega(t, s)=\\frac{1}{t} \\cdot \\mathbb{I}(s / t) \\quad p_{n}(t, s)=L_{n}(s / t)\n$$\n\nHowever, the S4 model applies the exact same formula (4) inside the time-invariant SSM (1), i.e. dropped the $\\frac{1}{t}$ term, which had no mathematical interpretation. In other words, while $\\left(\\frac{1}{t} \\boldsymbol{A}, \\frac{1}{t} \\boldsymbol{B}\\right)$ is an OSSM, it was not known whether the TSSM $(\\boldsymbol{A}, \\boldsymbol{B})$ is a TOSSM. Given that the performance of SSM models is very sensitive to these matries $\\boldsymbol{A}[7,9]$, it remained a mystery why this works. In Section 3 we will prove that (4) actually does correspond to a TOSSM. LSSL. While HiPPO originally showed just the above two cases involving Legendre polynomials (and another case called LagT for Laguerre polynomials, which will not be a focus of this work), follow-up work showed that there exist OSSMs corresponding to all families of orthogonal polynomials $\\left\\{p_{n}(t)\\right\\}$. Our more general framework will also subsume these results. Naming convention. We use HiPPO-[SSM] to refer to a fixed $\\operatorname{OSSM}(\\boldsymbol{A}, \\boldsymbol{B})$ suitable for online function approximation, where $[\\mathrm{SSM}]$ is a suffix (e.g.",
    "httyh-6": "LegS, LegT) that abbreviates the corresponding basis functions (e.g. scaled Legendre, truncated Legendre). S4-[SSM] refers to the corresponding trainable layer $(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C})$ with randomly initialized $\\boldsymbol{C}$, trained with S4's representation and computational algorithm [7]. ## 3 Generalized HiPPO: General Orthogonal Basis Projections\n\nIn Section 3.1, we prove that the LTI HiPPO-LegS is actually a TOSSM and show closed formulas for its basis functions. In Section 3.2, we include more specific results on finite-window SSMs, including introducing a new method HiPPO-FouT based on truncated Fourier functions, and proving previously established conjectures. Section 3.3 shows more general properties of TOSSMs, which establish guidelines for interpreting and initializing SSM parameters such as the timescale $\\Delta$. Our main, fully general, result is Theorem 12 in Appendix C.2, which describes a very general way to derive OSSMs for various SSM basis functions $K_{n}(t, s)$. This result can be instantiated in many ways to generalize all previous results in this line of work. ### 3.1 Explanation of S4-LegS\n\nWe show the matrices $(\\boldsymbol{A}, \\boldsymbol{B})$ in (4) are deeply related to the Legendre polynomials $L_{n}$ defined in Theorem 5 . Corollary 3.1. Define $\\sigma(t, s)=\\exp (a(s)-a(t))$ for any differentiable function a. The $S S M\\left(a^{\\prime}(t) \\boldsymbol{A}, a^{\\prime}(t) \\boldsymbol{B}\\right)$ is an OSSM with\n\n$$\n\\omega(t, s)=\\mathbb{I}(\\sigma(t, s)) a^{\\prime}(s) \\sigma(t, s) \\quad \\quad p_{n}(t, s)=L_{n}(\\sigma(t, s))\n$$\n\nAs more specific corollaries of Corollary 3.1, we recover both the original time-varying interpretation of the matrix in (4), as well as the instantiation of LegS as a time-invariant system. If we set $a^{\\prime}(t)=\\frac{1}{t}$, then we recover the scale-invariant HiPPO-LegS OSSM in Theorem 5,\nCorollary 3.2 (Scale-Invariant HiPPO-LegS, Theorem 5). The $\\operatorname{SSM}\\left(\\frac{1}{t} \\boldsymbol{A}, \\frac{1}{t} \\boldsymbol{B}\\right)$ is a TOSSM for basis functions $K_{n}(t)=\\frac{s}{t} L_{n}\\left(\\frac{s}{t}\\right)$ and measure $\\omega=\\frac{1}{t} \\mathbb{I}[0,1]$ where $\\boldsymbol{A}$ and $\\boldsymbol{B}$ are defined as in (4). And if $a^{\\prime}(t)=1$, this shows a new result for the time-invariant HiPPO-LegS TOSSM:\nCorollary 3.3 (Time-Invariant HiPPO-LegS). The SSM $(\\boldsymbol{A}, \\boldsymbol{B})$ is a TOSSM with\n\n$$\n\\omega(t)=e^{-t} \\quad p_{n}(t)=L_{n}\\left(e^{-t}\\right)\n$$\n\nThis explains why removing the $\\frac{1}{t}$ factor from HiPPO-LegS still works: it is orthogonalizing onto the Legendre polynomials with an exponential \"warping\" or change of basis on the time axis (Fig. 1). ### 3.2 Finite Window Time-Invariant Orthogonal SSMs\n\nFor the remainder of this section, we restrict to the time-invariant SSM setting (3). A second important instantiation of Theorem 12 covers cases with a discontinuity in the SSM basis functions $K_{n}(t)$, which requires infinite-dimensional SSMs to represent. The most important type of discontinuity occurs when $K_{n}(t)$ is supported on a finite window, so that these TSSMs represent sliding window transforms. We first derive a new sliding window transform based on the widely used Fourier basis (Section 3.2.1). We also prove results relating finite window methods to delay networks (Section 3.2.2)\n\n### 3.2.1 S4-FouT\n\nUsing the more general framework (Theorem 12) that does not necessarily require polynomials as basis functions, we derive a TOSSM that projects onto truncated Fourier functions. Theorem 6. As $N \\rightarrow \\infty$, the SSM for (6) is a TOSSM with $\\omega(t)=\\mathbb{I}(t)$, and $\\left\\{p_{n}\\right\\}_{n \\geq 1}$ are the truncated Fourier basis functions orthonormal on $[0,1]$, ordered in the form $\\left\\{p_{n}\\right\\}_{n \\geq 0}=\\left(1, c_{0}(t), s_{0}(t), \\ldots\\right)$, where $s_{m}(t)=\\sqrt{2} \\sin (2 \\pi m t)$ and $c_{m}(t)=\\sqrt{2} \\cos (2 \\pi m t)$ for $m=0, \\ldots, N / 2$. This SSM corresponds to Fourier series decompositions, a ubiquitous tool in signal processing, but represented as a state space model.",
    "httyh-7": "The basis is visualized in Fig. 1 (middle) for state size $N=1024$. A benefit of using these well-behaved basis functions is that we can leverage classic results from Fourier analysis. For example, it is clear that taking linear combinations of the truncated Fourier basis can represent any function on $[0,1]$, and thus $S 4$-FouT can represent any local convolution (i.e. the layers of modern convolutional neural networks). Theorem 7. Let $K(t)$ be a differentiable kernel on $[0,1]$, and let $\\hat{K}(t)$ be its representation by the FouT system (Theorem 6) with state size $N$. If $K$ is L-Lipschitz, then for $\\epsilon>0, N \\geq\\left(\\frac{L}{\\pi \\epsilon}\\right)^{2}+2$, we have $\\|K(t)-\\hat{K}(t)\\| \\leq \\epsilon$. If $K$ has $k$-derivatives bounded by $L$, then we can take $N \\geq\\left(\\frac{L}{\\pi^{k} \\epsilon}\\right)^{\\frac{2}{2 k-1}}+2$. ### 3.2.2 Approximating Delay Networks\n\nAn interesting property of these finite window TSSMs is that they can approximate delay functions. This is defined as a system with impulse response $K(t)=\\delta(t-1)$ : then $y(t)=(K * u)(t)=u(t-1)$, which means the SSM outputs a time-lagged version of the input. This capability is intuitively linked to HiPPO, since in order to do this, the system must be remembering the entire window $u([t-1, t])$ at all times $t$, in other words perform an approximate function reconstruction. Any HiPPO method involving finite windows should have this capability, in particular, the finite window methods LegT and FouT. Theorem 8. For the FouT system $\\boldsymbol{A}$ and $\\boldsymbol{B}$, let $\\boldsymbol{C}$ be (twice) the vector of evaluations of the basis functions $\\boldsymbol{C}_{n}=2 \\cdot p_{n}(1)$ and let $\\boldsymbol{D}=1$. For the LegT system $\\boldsymbol{A}$ and $\\boldsymbol{B}$, let $\\boldsymbol{C}$ be the vector of evaluations of the basis functions $\\boldsymbol{C}_{n}=p_{n}(1)=(1+2 n)^{\\frac{1}{2}}(-1)^{n}$ and let $\\boldsymbol{D}=0$. Then the SSM kernel $K(t)=\\boldsymbol{C} e^{t \\boldsymbol{A}} \\boldsymbol{B}+\\boldsymbol{D} \\delta(t)$ limits to $K(t) \\rightarrow \\delta(t-1)$ as $N \\rightarrow \\infty$.",
    "httyh-8": "Theorem 8 is visualized in Figs. 1 and 3 (right). Further, the result for LegT can be characterized even more tightly for finite $N$. In fact, this was the original motivation for the LDN/LMU [13, 14], which worked backward from the transfer function of the desired delay function impulse response $K(t)=\\delta(t-1)$, and noticed that the SSM for Pad\u00e9 approximations to this were linked to Legendre polynomials. This was not fully proven, and we state it here and provide a full proof in Appendix C.4. Theorem 9. For $\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C}, \\boldsymbol{D}$ in the LegT system described in Theorem 8, the transfer function $\\mathcal{L}\\{K(t)\\}(s)$ is the $[N-1 / N]$ Pad\u00e9 approximant to $e^{-s}=\\mathcal{L}\\{\\delta(t-1)\\}(s)$. We remark that although LegT (LMU) is designed to be an \"optimal\" approximation to the delay function via Pad\u00e9 approximants, it actually produces a weaker spike function than FouT (Fig.",
    "httyh-9": "3 vs. Fig. 1) and empirically performs slightly worse on synthetic tasks testing this ability (Section 4.3 ). This may be because Pad\u00e9 approximation in the Laplace domain does not necessarily translate to localization in the time domain. ### 3.3 Properties of Time-invariant Orthogonal SSMs: Timescales and Normalization\n\nWe describe several general properties of TOSSMs, which let us answer the following questions:\n\n- How should all parameters $(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C})$ be initialized for an SSM layer to be properly normalized?",
    "httyh-10": "- What does $\\Delta$ intuitively represent, and how should it be set in an SSM model? It turns out that for TOSSMs, these two questions are closely related and have intuitive interpretations. Closure properties. First, several basic transformations preserve the structure of TOSSMs. Consider a $\\operatorname{TOSSM}(\\boldsymbol{A}, \\boldsymbol{B})$ with basis functions $p_{n}(t)$ and measure $\\omega(t)$. Then, for any scalar $c$ and unitary matrix $\\boldsymbol{V}$, the following are also TOSSMs with the corresponding basis functions and measure (Appendix C.5, Proposition 13):\n\n| Transformation | Matrices | Interpretation | Basis | Measure |\n| :--- | :--- | :--- | :--- | :--- |\n| Scalar Scaling | $(c \\boldsymbol{A}, c \\boldsymbol{B})$ | Timescale change | $p(c t)$ | $\\omega(c t) c$ |\n| Identity Shift | $(\\boldsymbol{A}+\\boldsymbol{c}, \\boldsymbol{B})$ | Exponential tilting | $p(t) e^{-c t}$ | $\\omega(t) e^{2 c t}$ |\n| Unitary Transformation | $\\left(\\boldsymbol{V} \\boldsymbol{A} \\boldsymbol{V}^{*}, \\boldsymbol{V} \\boldsymbol{B}\\right)$ | Identity | $\\boldsymbol{V} p(t)$ | $\\omega(t)$ |\n\nNormalization. A standard aspect of training deep learning models, in general, concerns the scale or variance of activations. This has been the subject of much research on training deep learning models, touching on deep learning theory for the dynamics of training such as the exploding/vanishing gradient problem [11], and a large number of normalization methods to ensure properly normalized methods, from the simple Xavier/He initializations $[4,10]$ to BatchNorm and LayerNorm [1, 12] to many modern variants and analyses of these [3]. The following proposition follows because for a TOSSM, $x(t)$ can be interpreted as projecting onto orthonormal functions in a Hilbert space (Proposition 2). Proposition 10 (Normalization of TOSSM). Consider an (infinite-dimensional) TOSSM. For any input $u(t),\\|x(t)\\|_{2}^{2}=\\|u\\|_{\\omega}^{2}=\\int_{-\\infty}^{t} u(s)^{2} \\omega(t-s) d t$. Corollary 3.4. For a TOSSM with a probability measure (i.e. $\\int \\omega(t)=1$ ) and any constant input $u(t)=c$, the state has norm $\\|x(t)\\|^{2}=c^{2}$ and the output $y(t)$ has mean 0 , variance $c^{2}$ if the entries of $\\boldsymbol{C}$ are mean 0 and variance 1. Note that the probability measure requirement can be satisfied by simply rescaling $\\boldsymbol{B}$. Corollary 3.4 says the TOSSM preserves the variance of inputs, the critical condition for a properly normalized deep learning layer. Note that the initialization of $\\boldsymbol{C}$ is different than a standard Linear layer in deep neural networks, which usually rescale by factor depending on its dimensionality such as $N^{-\\frac{1}{2}}[4]$. Timescales. As discussed in Section 2, converting from continuous to discrete time involves a parameter $\\Delta$ that represents the step size of the discretization. This is an unintuitive quantity when working directly with discrete data, especially if it is not sampled from an underlying continuous process. We observe the following fact: for all standard discretization methods (e.g. Euler, backward Euler, generalized bilinear transform, zero-order hold [6]), the discretized system depends on $(\\boldsymbol{A}, \\boldsymbol{B})$, and $\\Delta$ only through their products $(\\Delta \\boldsymbol{A}, \\Delta \\boldsymbol{B})$. This implies that the $\\operatorname{SSM}(\\boldsymbol{A}, \\boldsymbol{B})$ discretized at step size $\\Delta$ is computationally equivalent to the $\\operatorname{SSM}(\\Delta \\boldsymbol{A}, \\Delta \\boldsymbol{B})$ discretized at step size 1. Therefore, $\\Delta$ can be viewed just as a scalar scaling of the base SSM instead of changing the rate of the input. In the context of TOSSMs, this just scales the underlying basis and measure (Scalar Scaling). More broadly, scaling a general SSM simply changes its timescale or rate of evolution. Proposition 11. The $O D E y^{\\prime}=c \\boldsymbol{A} y+c \\boldsymbol{B} u$ evolves at a rate $c$ times as fast as the $S S M x^{\\prime}=\\boldsymbol{A} x+\\boldsymbol{B} u$, in the sense that the former maps $u(c t) \\mapsto x(c t)$ if the latter maps $u(t) \\mapsto x(t)$. The most intuitive example of this is for a finite window TOSSM such as LegT or FouT. Discretizing this system with step size $\\Delta$ is equivalent to considering the system $(\\Delta \\boldsymbol{A}, \\Delta \\boldsymbol{B})$ with step size 1 , which produces basis functions supported exactly on $\\left[0, \\frac{1}{\\Delta}\\right]$. The interpretation of the timescale $\\Delta$ lends to simple discretetime corollaries of the previous continuous-time results. For example, LegT and FouT represent sliding windows of $1 / \\Delta$ elements in discrete time. Corollary 3.5. By Theorem 8 , as $N \\rightarrow \\infty$, the discrete convolutional kernel $\\overline{\\boldsymbol{K}} \\rightarrow \\boldsymbol{e}_{\\left\\lceil\\Delta^{-1}\\right\\rceil}$, i.e. the discrete delay network with lag $\\frac{1}{\\Delta}$. Corollary 3.6. For HiPPO-FouT matrices $(\\boldsymbol{A}, \\boldsymbol{B})$, by Theorem 6, as $N \\rightarrow \\infty$, the discrete convolutional kernel $\\overline{\\boldsymbol{K}}$ (over the choice of $\\boldsymbol{C}$ ) can represent any local convolution of length $\\left\\lfloor\\Delta^{-1}\\right\\rfloor$. This discussion motivates the following definition. Properly normalized TOSSMs $(\\boldsymbol{A}, \\boldsymbol{B})$ will model dependencies of expected length 1 , and $\\Delta$ modulates it to model dependencies of length $\\frac{1}{\\Delta}$, allowing fine-grained control of the context size of a TOSSM. Definition 4 (Timescale of TOSSM). Define $\\mathbb{E}[\\omega]=\\frac{\\int_{0}^{\\infty} t \\omega(t) d t}{\\int_{0}^{\\infty} \\omega(t) d t}$ to be the timescale of a TOSSM having measure $\\omega(t)$. A TOSSM is timescale normalized if has timescale 1. By this definition, HiPPO-LegS is timescale normalized. This motivates S4's initialization of $\\Delta$ log-uniformly in $(0.001,0.1)$, covering a geometric range of sensible timescales (expected length 10 to 1000). In Section 4 we show that the timescale can be chosen more precisely when lengths of dependencies are known. We finally remark that HiPPO-LegT and -FouT were derived with measures $\\mathbb{I}[0,1]$. However, to properly normalize them by Definition 4 , we choose to halve the matrices to make them orthogonal w.r.t. $\\omega=\\frac{1}{2} \\mathbb{I}[0,2]$. The S4-FouT and S4-LegT methods in our experiments use these halved versions. ### 3.4 Discussion\n\nTable 1 summarizes the results for TOSSMs presented in this section, including both original HiPPO methods defined in Gu et al. [5] as well as our new methods. HiPPO-LagT We note that the original HiPPO paper also included another method called LagT based on Laguerre polynomials. Because Laguerre polynomials are orthogonal with respect to $e^{-t}$, this system was supposed to represent an exponentially decaying measure.",
    "httyh-11": "However, this method was somewhat anomalous; it generally performed a little worse than the others, and it was also empirically found to need different hyperparameters. For example, Gu et al. [5] found that on the permuted MNIST dataset, setting $\\Delta$ to around $\\frac{1}{784}$ for most HiPPO methods was indeed optimal, as the theory predicts. However, HiPPO-LagT performed better when set much higher, up to $\\Delta=1.0$. It turns out that this method changes the basis in a way such\n\nTable 1: Summary of time-invariant orthogonal SSMs. Original HiPPO results (Bottom) and new results (Top). | Method | SSM Matrices | SSM kernels $e^{t \\boldsymbol{A}} \\boldsymbol{B}$ | Orthogonal basis $p_{n}(t)$ | Measure $\\omega(t)$ | Timescale |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| LegS | equation Eq. (4) | $L_{n}\\left(e^{-t}\\right) e^{-t}$ | $L_{n}\\left(e^{-t}\\right)$ | $e^{-t}$ | 1 |\n| FouT | equation Eq. (6) | $(\\cos , \\sin )(2 \\pi n t) \\mathbb{I}[0,1]$ | $(\\cos , \\sin )(2 \\pi n t)$ | $\\mathbb{I}[0,1]$ | $1 / 2$ |\n| LegT | equation Eq. (5) | $L_{n}(t) \\mathbb{I}[0,1]$ | $L_{n}(t)$ | $\\mathbb{I}[0,1]$ | $1 / 2$ |\n| LagT | see [5] | $\\operatorname{Lag}(t) e^{-t / 2}$ | $\\operatorname{Lag}(t) e^{-t / 2}$ | $\\mathbb{I}[0, \\infty)$ | $\\infty$ |\n\nthat it is not orthogonal with respect to an exponentially decaying measure, but rather the constant measure $\\mathbb{I}[0, \\infty)$, and has a timescale of $\\infty$; this explains why the hyperparameters for $\\Delta$ need to be much higher. In summary, we do not recommend using the original HiPPO-LagT, which despite the original motivation does not represent orthogonalizing against an exponentially decaying measure. Instead, HiPPO-LegS (as a time-invariant SSM) actually represents an exponentially decaying measure. Timescales For a timescale normalized orthogonal SSM (i.e. $\\int_{0}^{\\infty} \\omega(t)=1$ and $\\left.\\int_{0}^{\\infty} t \\omega(t)=1\\right)$ :\n\n- $\\frac{1}{\\Delta}$ exactly represents the range of dependencies it captures. For example, S4-FouT can represent any finite convolution kernel of length $\\frac{2}{\\Delta}$ (so the expected length of a random kernel is $\\frac{1}{\\Delta}$ ). - A random vector $\\boldsymbol{C}$ with independent mean 0 , variance 1 entries is a variance-preserving SSM, i.e. produces outputs matching the variance of the input. LSSL and General Polynomials The Linear State Space Layer [6] succeeded HiPPO by incorporating it into a full deep SSM model, and also generalized the HiPPO theory to show that all orthogonal polynomials can be defined as the SSM kernels for some $(\\boldsymbol{A}, \\boldsymbol{B})$. Our framework is even stronger and immediately produces the main result of LSSL as a corollary (Appendix), and can also work for non-polynomial methods (e.g. FouT). These results show that all orthogonal polynomial bases, including truncated and scaled variants, have corresponding OSSMs with polynomial kernels. If we define this special case as polynomial OSSMs (POSSMs), we have therefore deduced that all of the original HiPPOs are POSSMs. ## 4 Experiments\n\nWe study the empirical tradeoffs of our proposed S4 variants. We compare several S 4 variants based on the TOSSMs introduced in this work, as well as to simpler diagonal SSMs called S4D that are not orthogonal SSMs [8]. Corresponding to our main contributions, we hypothesize that\n\n- S4-LegS excels at sparse memorization tasks because it represents very smooth convolution kernels that memorize the input against an infinitely-long measure (Corollary 3.3, Fig. 1). Conversely, it is less appropriate at short-range tasks with dense information because it smooths out the signal. - S4-FouT excels at dense memorization tasks because it can represent spike functions that pick out past elements at particular ranges (Section 3.2.2).",
    "httyh-12": "However, it is less appropriate at very long range tasks because it represents a finite (local) window. - $\\Delta$ can be initialized precisely based on known time dependencies in a given task to improve performance. ### 4.1 Long Range Arena\n\nThe Long Range Arena (LRA) benchmark is a suite of sequence classification tasks designed to stress test sequence models on modeling long sequences. We improve S4's previous state of the art by another 6 points\n\nTable 2: (Long Range Arena) Accuracy (std.) on full suite of LRA tasks. Hyperparameters in Appendix B. | Model | ListOps | Text | Retrieval | IMAGE | Pathfinder | Path-X | AvG |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| S4-LegS | $59.60(0.07)$ | $86.82(0.13)$ | $90.90(0.15)$ | $\\underline{88.65}(0.23)$ | $\\underline{94.20}(0.25)$ | $\\mathbf{9 6 . 3 5}(-)$ | $\\mathbf{8 6 . 0 9}$ |\n| S4-FouT | $57.88(1.90)$ | $86.34(0.31)$ | $89.66(0.88)$ | $\\mathbf{8 9 . 0 7}(0.19)$ | $\\mathbf{9 4 . 4 6}(0.24)$ | $\\boldsymbol{x}$ | 77.90 |\n| S4-LegS/FouT | $60.45(0.75)$ | $86.78(0.26)$ | $90.30(0.28)$ | $89.00(0.26)$ | $94.44(0.08)$ | $\\boldsymbol{x}$ | 78.50 |\n| S4 (original) | 58.35 | 76.02 | 87.09 | 87.26 | 86.05 | 88.10 | 80.48 |\n| Transformer | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | $\\boldsymbol{x}$ | 53.66 |\n\n(Table 2). Validating our hypothesis, S4-LegS is extremely strong at the hardest long-range task (Path-X) involving sparse dependencies of length 16384, which FouT cannot solve because it is a finite window method. The Path-X task also serves as a validation of the theory of timescales in Section 3.3. To set these results, we lowered the initialization of $\\Delta$ in accordance with known length of dependencies in the task. Fig. 4 illustrates the importance of setting $\\Delta$ correctly. ![](https://cdn.mathpix.com/cropped/2024_09_12_37931de6e86a3ea35c37g-11.jpg?height=529&width=1622&top_left_y=948&top_left_x=253)\n\nFigure 4: (Validation curves on Path-X.) (Left) Setting $\\Delta_{\\text {min }}$ too small can solve the task, but is inconsistent. (Right) A good setting of $\\Delta_{\\text {min }}$ can consistently solve the task. Note that the timescale of each feature is up to $\\frac{1}{\\Delta_{\\min }}=10^{4}$, which is on the order of (but not exceeding) the length of the task $L=16384$. Empirically, performance is best when spreading out the range of $\\Delta$ with a larger $\\Delta_{\\max }$ that covers a wider range of timescales and can potentially learn features at different resolutions, which are combined by a multi-layer deep neural network. We also show a diagonal variant of S4-LegS called S4D-Inv introduced in [8] which approximates S4-LegS, but is still worse. ### 4.2 Theory: Function Reconstruction, Timescales, Normalization\n\nFig. 5 confirms the HiPPO theory of online function reconstruction (Proposition 2) for the proposed TOSSMs LegS and FouT. We additionally construct a synthetic Reconstruction Task (for a uniform measure) to test if S4 variants can learn to reconstruct. The input is a white noise sequence $u \\in \\mathbb{R}^{4000}$. We use a single layer linear S 4 model with state size $N=256$ and $H=256$ hidden units. Models are required to use their output at the last time step, a vector $y_{4000} \\in \\mathbb{R}^{256}$, to reconstruct the last 1000 elements of the input with a linear probe. Concretely, the loss function is to minimize $\\left\\|u_{3000: 4000}-\\boldsymbol{W} y_{4000}\\right\\|_{2}^{2}$, where $\\boldsymbol{W} \\in \\mathbb{R}^{1000 \\times 256}$ is a learned matrix. Fig. 6 shows that S4-LegT and S4-FouT, the methods that theoretically reconstruct against a uniform measure, are far better than other methods. We include the new diagonal variants (S4D) proposed in [8], which are simpler SSM methods that generally perform well but do not learn the right function on this task. We also include a method S4-(LegS/FouT) which combines both LegS and FouT measures by simply initializing half of the SSM kernels to each. Despite having fewer S4-FouT kernels, this still performs as well as the pure S4-FouT initialization. ![](https://cdn.mathpix.com/cropped/2024_09_12_37931de6e86a3ea35c37g-12.jpg?height=429&width=1640&top_left_y=249&top_left_x=250)\n\nFigure 5: Function reconstruction predicted by our general theory. An input signal of length 10000 is processed sequentially, maintaining a state vector of size only $x(t) \\in \\mathbb{R}^{64}$, which is then used to approximately reconstruct the entire history of the input. (Left) HiPPO-LegS (as an LTI system) orthogonalizes on the Legendre polynomials warped by an exponential change of basis, smoothening them out. This basis is orthogonal with respect to an exponentially decaying measure. Matching the intuition, the reconstruction is very accurate for the recent past and degrades further out, but still maintains information about the full history of the input, endowing it with long-range modeling capacity. This is the same as S4. (Right) HiPPO-FouT orthogonalizes on the truncated Fourier basis, similar to the original HiPPO-LegT or LMU. |  | $(\\Delta \\min , \\Delta \\max )$ |  |\n| :--- | ---: | ---: |\n|  | $(1 \\mathrm{e}-3,2 \\mathrm{e}-3)$ | $(2 \\mathrm{e}-3,2 \\mathrm{e}-3)$ |\n| S4-LegS | -6.2581 | -6.6328 |\n| S4-LegT | -7.4987 | -8.1056 |\n| S4-FouT | -7.4889 | -8.3296 |\n| S4-(LegS/FouT) | -7.4992 | -8.3162 |\n| S4D-LegS | -6.1528 | -6.184 |\n| S4D-Inv | -5.9362 | -6.0986 |\n| S4D-Lin | -7.1233 | -6.6483 |\n| S4D-(Inv/Lin) | -6.839 | -6.705 |\n\n\n| $(\\Delta \\min , \\Delta$ max $)$ |  |\n| ---: | ---: |\n| $(1 \\mathrm{e}-3,1 \\mathrm{e}-1)$ | $(2 \\mathrm{e}-3,1 \\mathrm{e}-1)$ |\n| -3.5505 | -3.3017 |\n| -2.9729 | -2.6557 |\n| -3.0062 | -2.6976 |\n| -3.4784 | -3.2628 |\n| -3.8773 | -3.6317 |\n| -4.1402 | -3.7912 |\n| -3.973 | -3.5991 |\n| -4.325 | -3.8389 |\n\nFigure 6: Log-MSE after training on the Reconstruction Task. (Left) When the timescales $\\Delta$ are set appropriately for this task, the methods that theoretically reconstruct against a uniform measure ( LegT and FouT) are much better than alternatives, achieving MSE more orders of magnitude lower than other SSM initializations. (Right) Interestingly, when the timescales $\\Delta$ are not set correctly, these methods (LegT and FouT) actually perform worst and the diagonal methods introduced in [8] perform best. Table 3: Ablation of the initialization standard deviation of $\\boldsymbol{C}$ for S4-LegS on classification datasets. | Init std. $\\sigma$ of $\\boldsymbol{C}$ | 0.01 | 0.1 | 1.0 | 10.0 | 100.0 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Sequential CIFAR | $85.91(0.41)$ | $86.33(0.01)$ | $\\mathbf{8 6 . 4 9}(0.51)$ | $84.40(0.16)$ | $82.05(0.61)$ |\n| Speech Commands | $90.27(0.31)$ | $90.00(0.11)$ | $\\mathbf{9 0 . 6 7}(0.19)$ | $90.30(0.36)$ | $89.98(0.51)$ |\n\nFinally, we validate the theory of normalization in Section 3.3, which predicts that for a properly normalized TOSSM, the projection parameter $C$ should be initialized with unit variance, in contrast to standard initializations for deep neural networks which normalize by a factor related to the size of $N$ (in this case $N=64$ ). Table 3 shows classification results on datasets Sequential CIFAR (sCIFAR) and Speech Commands (SC), using models of size at most 150 K parameters.",
    "httyh-13": "This replicates the setup of the \"ablation models\" of $[8$, Section 5]. Results show that using standard deviation 1.0 for $\\boldsymbol{C}$ is slightly better than alternatives, although the difference is usually minor. ### 4.3 Memorization: the Delay (continuous copying) Task\n\nNext, we study how the synthetic reconstruction ability transfers to other tasks. The Delay Task requires models to learn a sequence-to-sequence map whose output is the input lagged by a fixed time period (Fig. 7a). For recurrent models, this task can be interpreted as requiring models to maintain a memory buffer that continually remembers the latest elements it sees. This capability was the original motivation for the Legendre Memory Unit, the predecessor to HiPPO-LegT, which was explicitly designed to solve this task because it can encode a spike kernel (Fig.",
    "httyh-14": "3). In Fig. 7b, we see that our new S4-FouT actually outperforms S4-LegT, which both outperform all other methods when the timescale $\\Delta$ is set correctly. We note that this task with a lag of just 1000 time steps is too hard for baselines such as an LSTM and Transformer, which empirically did not learn better than random guessing (RMSE 0.43). ![](https://cdn.mathpix.com/cropped/2024_09_12_37931de6e86a3ea35c37g-13.jpg?height=470&width=1635&top_left_y=752&top_left_x=234)\n(a) Models perform a mapping from $\\mathbb{R}^{4000} \\rightarrow \\mathbb{R}^{4000}$ where the target output is lagged by 1000 steps, with error measured by RMSE. The input is a white noise signal bandlimited to $1000 H z$. We use single layer SSMs with state size $N=1024$. | $(\\Delta$ min, $\\Delta$ max) | Frozen (A, B) | Trainable (A, B) |  |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| $(5 \\mathrm{e}-4,5 \\mathrm{e}-4)$ | 0.2891 | 0.2832 |  | $(\\Delta$ min, $\\Delta$ max $)=(1 \\mathrm{e}-3,1 \\mathrm{e}-1)$ |  | $(\\Delta \\min , \\Delta$ max $)=(2 \\mathrm{e}-3,2 \\mathrm{e}-3)$ |  |\n| $(1 e-3,1 e-3)$ | 0.1471 | 0.1414 |  | Frozen (A, B) | Trainable (A, B) | Frozen (A, B) | Trainable (A, B) |\n| $(2 e-3,2 e-3)$ | 0.0584 | 0.0078 | S4-LegS | 0.3072 | 0.0379 | 0.2369 | 0.0130 |\n| $(4 e-3,4 e-3)$ | 0.4227 | 0.1262 | S4-LegT | 0.2599 | 0.1204 | 0.0226 | 0.0129 |\n| $(8 \\mathrm{e}-3,8 \\mathrm{e}-3)$ | 0.4330 | 0.2928 | S4-FouT | 0.2151 | 0.1474 | 0.0304 | 0.0078 |\n| $(5 \\mathrm{e}-4,1 \\mathrm{e}-1)$ | 0.2048 | 0.1537 | S4-LegS+FouT | 0.1804 | 0.0309 | 0.0250 | 0.0080 |\n| $(1 e-3,1 e-1)$ | 0.2017 | 0.1474 | S4D-LegS | 0.1378 | 0.0337 | 0.0466 | 0.0140 |\n| $(2 e-3,1 e-1)$ | 0.3234 | 0.2262 | S4D-Inv | 0.1489 | 0.0243 | 0.0605 | 0.0186 |\n| $(4 e-3,1 e-1)$ | 0.4313 | 0.3417 | S4D-Lin | 0.1876 | 0.1653 | 0.0421 | 0.0144 |\n| $(8 \\mathrm{e}-3,1 \\mathrm{e}-1)$ | 0.4330 | 0.4026 |  |  |  |  |  |\n\n(b) (Left) Setting $\\Delta$ appropriately makes a large difference. For FouT $(\\boldsymbol{A}, \\boldsymbol{B})$, which encode finite window basis functions (Fig. 1), the model can see a history of length up to $\\frac{2}{\\Delta}$. For example, setting $\\Delta$ too large means the model cannot see 1000 steps in the past, and performs at chance. Performance is best at the theoretically optimal value of $\\Delta=2 \\cdot 10^{-3}$ which can encode a spike kernel at distance exactly 1000 steps (Corollary 3.5). (Right) When $\\Delta$ is set optimally, the proposed S4-FouT method is the best SSM as the theory predicts. When $\\Delta$ is not set optimally, other methods perform better, including the simple diagonal methods proposed in [8]. Figure 7: (Delay Task.) A synthetic memorization task: definition (Fig.",
    "httyh-15": "7a) and results (Fig. 7b). ## 5 Summary: How to Train Your HiPPO\n\n- SSMs represent convolution kernels that are linear combinations (parameterized by $\\boldsymbol{C}$ ) of basis functions (parameterized by $\\boldsymbol{A}$ and $\\boldsymbol{B}$ ). - HiPPO is a general mathematical framework for producing matrices $\\boldsymbol{A}$ and $\\boldsymbol{B}$ corresponding to prescribed families of well-behaved basis functions. We derive HiPPO matrices corresponding to exponentiallyscaled Legendre families (LegS) and the truncated Fourier functions (FouT). - HiPPO-LegS corresponds to the original S4 method and produces a very smooth, long-range family of kernels (Fig. 1) that is still the best method for long-range dependencies among all S4 variants\n- HiPPO-FouT is a finite window method that subsumes local convolutions (e.g. generalizing vanilla CNNs, Corollary 3.6) and captures important transforms such as the sliding DFT or STFT\n- Independently of a notion of discretization, the timescale $\\Delta$ has a simple interpretation as controlling the length of dependencies or \"width\" of the SSM kernels. Most intuitively, for a finite window method such as FouT, the kernels have length exactly $\\frac{1}{\\Delta}$, and generalize standard local convolutions used in deep learning. A companion paper to this work builds on the theory introduced here to define a simpler version of S4 using diagonal state matrices (S4D), which are approximations to the orthogonal SSMs we introduce and can inherit S4's strong modeling abilities [8]. It also includes experiments on more datasets comparing various state space models, including the S4 variants (S4-LegS and S4-FouT) introduced here. ## Acknowledgments\n\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.",
    "httyh-16": "Atri Rudra and Isys Johnson are supported by NSF grant CCF-1763481.",
    "httyh-17": "The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S.",
    "httyh-18": "Government. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.",
    "httyh-19": "arXiv preprint arXiv:2203.14343, 2022. [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, 2015. [11] Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universit\u00e4t M\u00fcnchen, 91(1), 1991. [12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift.",
    "httyh-20": "In International conference on machine learning, pages 448-456. PMLR, 2015. [13] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. In Advances in Neural Information Processing Systems, pages 15544-15553, 2019 . [14] Aaron Russell Voelker. Dynamical systems in spiking neuromorphic hardware. PhD thesis, University of Waterloo, 2019. ## A Related Work\n\nWe discuss in more detail the differences between this work and the previous results in this line of work. Legendre Memory Unit (Legendre Delay Network). The HiPPO-LegT matrix (5) was first introduced as the LMU [13, 14]. The original motivation was to produce a state space model that approximates the Delay Network, which can be defined as the LTI system that transforms $u(t)$ into $u(t-1)$, i.e. lags the input by 1 time unit. This can also be defined as the system with impulse response $K(t)=\\delta(t-1)$, i.e. convolves by the convolutional kernel with a $\\delta$ spike at time 1 . The connection between the Delay Network and Legendre polynomials was made in two steps. First, the transfer function of the ideal system is $\\mathcal{L}[\\delta(t-1)](s)=e^{-s}$ and must be approximated by a proper rational function to be represented as an SSM. Taking Pad\u00e9 approximants of this function yields \"optimal\" approximations by rational functions, which can then be distilled into a $\\operatorname{SSM}(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C})$ whose transfer function $\\boldsymbol{C}(s \\boldsymbol{I}-\\boldsymbol{A})^{-1} \\boldsymbol{B}$ matches it. Second, the SSM basis $e^{t \\boldsymbol{A}} \\boldsymbol{B}$ for this system can be computed and found to match Legendre polynomials. However, despite making this connection and writing out formulas for this SSM, Voelker [14] did not provide a complete proof of either of these two connections. The preceding two steps that motivated the LDN can be informally written as the chain of transformations (i) transfer function $e^{-s} \\rightarrow$ (ii) $\\operatorname{SSM}(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C}) \\rightarrow$ (iii) Legendre polynomials $e^{t \\boldsymbol{A}} \\boldsymbol{B}$. The HiPPO framework in a sense proceeded in the opposite direction. Gu et al. [5] started by defining the system that convolves with truncated Legendre polynomials, and with a particular differentiation technique showed that it could be written as a particular SSM which they called HiPPO-LegT. This SSM turned out to be the same (up to a minor change in scaling) as the original $(\\boldsymbol{A}, \\boldsymbol{B})$ defined by the LMU, thus proving the second of the two steps relating this particular SSM to the Legendre polynomials. In this work, we show the final piece in this reverse chain of equivalences. In particular, we start from the LegT SSM $(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C})$ and directly prove that its transfer function produces Pad\u00e9 approximants of the exponential. Our proof introduces new techniques in an inductive argument that can be applied to HiPPO SSMs beyond the LegT case, and relates them to continued fraction expansions of the exponential. We comment on a minor difference between the parameterization of HiPPO-LegT and the LMU. The LMU is originally defined as\n\n$$\nx^{\\prime}(t)=\\frac{1}{\\theta} \\boldsymbol{A} x(t)+\\frac{1}{\\theta} \\boldsymbol{B} u(t)\n$$\n\nwhere $\\theta$ is a hyperparameter that controls the length of the window. However, we point out that such constant scaling of the SSM is also controlled by the step size $\\Delta$ as discussed in Section 3.3. Therefore $\\theta$ is redundant with $\\Delta$, so the LegT matrices defined in [5] and in this work do not have a concept of $\\theta$. Additionally, in this work we redefine the LegT matrices $(\\boldsymbol{A}, \\boldsymbol{B})$ to be scaled by a factor of 2 to make them properly timescale normalized, using the theory developed in Section 3.3. HiPPO and LSSL. As discussed in Section 2.2, HiPPO can be thought of as a framework for deriving state space models corresponding to specific polynomial bases. The original paper did not explicitly draw the connection to state space models, and also developed systems only for a few particular cases which were called LegS (a time-varying system involving Legendre polynomials), LegT (a time-invariant system with the truncated Legendre polynomials), and LagT (involving Laguerre polynomials). A follow-up paper on Linear State Space Layers (LSSL) generalized these results to all orthogonal polynomial families, and also generalized the flexibility of the time-varying component. They produced SSMs $x^{\\prime}(t)=$ $\\boldsymbol{A}(t) x(t)+\\boldsymbol{B}(t) u(t)$ where at all times $t, x(t)$ can be viewed as the projection of the history of $\\left.u(s)\\right|_{s \\leq t}$ onto orthogonal polynomials $p_{n}$ rescaled onto the interval $[t-\\theta(t), t]$, where $\\theta(t)$ is an arbitrary factor. This generalized all 3 cases of the original HiPPO paper. Compared to these works, our framework (Definition 2) simplifies and generalizes the concepts directly in terms of (time-varying) state space models. We define a more natural concept of orthogonal SSM, derive\n\nTable 4: The values of the best hyperparameters found for LRA. LR is learning rate and WD is weight decay. BN and LN refer to Batch Normalization and Layer Normalization. |  | Depth | Features $H$ | Norm | Pre-norm | Dropout | LR | Batch Size | Epochs | WD | $\\left(\\Delta_{\\min }, \\Delta_{\\max }\\right)$ |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| ListOps | 8 | 128 | BN | False | 0 | 0.01 | 50 | 40 | 0.05 | $(0.001,0.1)$ |\n| Text | 6 | 256 | BN | True | 0 | 0.01 | 16 | 32 | 0.05 | $(0.001,0.1)$ |\n| Retrieval | 6 | 256 | BN | True | 0 | 0.01 | 64 | 20 | 0.05 | $(0.001,0.1)$ |\n| Image | 6 | 512 | LN | False | 0.1 | 0.01 | 50 | 200 | 0.05 | $(0.001,0.1)$ |\n| Pathfinder | 6 | 256 | BN | True | 0 | 0.004 | 64 | 200 | 0.03 | $(0.001,0.1)$ |\n| Path-X | 6 | 256 | BN | True | 0 | 0.0005 | 32 | 50 | 0.05 | $(0.0001,0.01)$ |\n\nvery general instantiations of it (Section 3.1), and flesh out its properties (Section 3.3). Our general result subsumes all prior cases including all cases of the LSSL as a direct corollary. Some concrete advantages include:\n\n- It allows more flexible transformations of polynomial bases, such as including a change-of-basis inside the polynomials. The previously expained case of LegS is an instance of this, which has basis functions $L\\left(e^{-t}\\right)$ with an exponential change of basis, instead of vanilla polynomials. - It can be applied to non-polynomial bases, such as the truncated Fourier basis FouT. - It does not require considering multiple cases depending on where the basis functions are supported. Instead, we handle this by considering discontinuities in the basis functions. S4. While the preceding discussion covers theoretical interpretations of SSMs, S4 (and its predecessor LSSL) are the application of these SSMs to deep learning. In comparison to prior works such as the LMU and HiPPO which require a pre-determined system $(\\boldsymbol{A}, \\boldsymbol{B})$ and incorporate them naively into an RNN, LSSL and S4 use a full state space model $(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C})$ as a completely trainable deep learning layer. Doing this required resolving computational problems with the SSM, which was the main focus of S4. In this work, we make a distinction between HiPPO, which is the theoretical derivation and interpretation of particular $\\mathrm{SSMs}(\\boldsymbol{A}, \\boldsymbol{B})$, and S 4 , which is the incorporation of those SSMs as a trainable deep learning layer with a particular algorithm. ## B Experiment Details and Additional Experiments\n\n## B. 1 Delay (Continuous Copying) Task\n\nThe Delay Task consists of input-output pairs where the input is a white noise signal of length 4000 bandlimited to 1000 Hz . The output is the same signal shifted by 1000 steps (Fig. 7a). We use single layer linear SSMs with $H=4$ hidden units and state size $N=1024$. Models are trained with the Adam optimizer with learning rate 0.001 for 20 epochs. ## B. 2 Long Range Arena\n\nThe settings for LRA use the same hyperparameters in [8]. A more detailed protocol can be found in [8]. To be self-contained, we recreate the same table of parameters in Table 4. ## C Proof Details\n\nWe furnish the missing proofs from Section 2 in Appendix C.1. We will describe our general framework and results in Appendix C.2\uff0cand prove the results in Sections 3.1 to 3.3 in Appendices C.",
    "httyh-21": "3 to C. 5 respectively. ## C. 1 Proofs from Background\n\nThis corresponds to results from Section 2. Proof of Proposition 1. Suppose for the sake of contradiction that there is a second basis and measure $q_{n}, \\mu$ such that $q_{n}$ is complete and orthogonal w.r.t. $\\mu$, and $K_{n}=q_{n} \\mu$. By completeness, there are coefficients $c_{\\ell, k}$ such that\n\n$$\np_{\\ell}=\\sum_{k} c_{\\ell, k} q_{k}\n$$\n\nThen\n\n$$\n\\int p_{\\ell} q_{j} \\mu=\\int \\sum_{k} c_{\\ell, k} q_{k} q_{j} \\mu=\\sum_{k} c_{\\ell, k} \\delta_{k j}=c_{\\ell, j}\n$$\n\nBut $q_{j} \\mu=K_{j}=p_{j} \\omega$, so\n\n$$\n\\int p_{\\ell} q_{j} \\mu=\\int p_{\\ell} p_{j} \\omega=\\delta_{\\ell j}\n$$\n\nSo $c_{\\ell, j}=\\delta_{\\ell, j}$ which implies that $p_{\\ell}=q_{\\ell}$ for all $\\ell$, as desired. Proof of Proposition 3. The SSM kernels are $K_{n}(t)=e^{-t(n+1)} \\boldsymbol{B}_{n}$. Assume $\\boldsymbol{B}_{n} \\neq 0$ so that the kernels are not degenerate. Suppose for the sake of contradiction that this was a TOSSM with measure $\\omega(t)$. Then we must have\n\n$$\n\\int K_{n}(s) K_{m}(s) \\omega(t)^{-1} d s=\\delta_{n, m}\n$$\n\nPlugging in $n=1, m=1$ and $n=0, m=2$ gives\n\n$$\n\\begin{aligned}\n& 1=\\int e^{-2 t} \\boldsymbol{B}_{1} e^{-2 t} \\boldsymbol{B}_{1} \\omega(t)^{-1} d s=\\boldsymbol{B}_{1} \\boldsymbol{B}_{1} \\int e^{-4 t} \\omega(t)^{-1} d s \\\\\n& 0=\\int e^{-1 t} \\boldsymbol{B}_{0} e^{-3 t} \\boldsymbol{B}_{2} \\omega(t)^{-1} d s=\\boldsymbol{B}_{0} \\boldsymbol{B}_{2} \\int e^{-4 t} \\omega(t)^{-1} d s\n\\end{aligned}\n$$\n\nThis is clearly a contradiction. ## C. 2 General theory\n\nConsider a measure supported on $[0,1]$ with density $\\omega(t) \\mathbb{I}(t)$, where $\\mathbb{I}(t)$ is the indicator function for membership in the interval $[0,1]$. Let the measure be equipped with a set of orthonormal basis functions $p_{0}, \\ldots, p_{N-1}$, i.e. $$\n\\int p_{j}(s) p_{k}(s) \\omega(s) \\mathbb{I}(s) d s=\\delta_{j k}\n$$\n\nwhere the integrals in this paper are over the range $[-\\infty, \\infty]$, unless stated otherwise. This is sufficient to derive an OSSM based on the HiPPO technique. The generalized HiPPO framework demonstrates how to build (T)OSSMs utilizing time warping to shape the time interval and tilting to construct new sets of orthogonal basis functions. Given an general interval $[\\ell, r]$, we will use the notation $\\mathbb{I}[\\ell, r]$ to denote the indicator function for the interval $[\\ell, r]$ - we will drop the interval if $\\ell=0, r=1$. We will need the notion of a \"time warping\" function $\\bar{\\sigma}$ as follows:\n\nDefinition 5. A time warping function is defined as\n\n$$\n\\bar{\\sigma}(t, s):(-\\infty, t] \\rightarrow[0,1]\n$$\n\nsuch that $\\bar{\\sigma}(t, t)=1$. We will be using a special case of time-warping function, which we say has a discontinuity at $t_{0}$ for some $t_{0} \\in(-\\infty, t]:$\n\n$$\n\\bar{\\sigma}(t, s)=\\mathbb{I}\\left[t_{0}, t\\right] \\sigma(t, s)\n$$\n\nsuch that\n\n$$\n\\frac{\\partial}{\\partial t}\\left(\\frac{\\partial}{\\partial s} \\sigma_{s}(t, s)\\right)=c(t) \\frac{\\partial}{\\partial s} \\sigma(t, s)\n$$\n\nWe allow for $t_{0}=-\\infty$, in which case we think of the interval $\\left[t_{0}, t\\right]$ as $(-\\infty, t]$. Before proceeding, let us clarify our notation. We will use $\\sigma_{t}$ and $\\sigma_{s}$ to denote the partial derivatives $\\frac{\\partial}{\\partial t} \\sigma(t, s)$ and $\\frac{\\partial}{\\partial s} \\sigma(t, s)$ respectively. We will drop the parameters $(t, s)$ and use $f$ instead of $f(t, s)$ when it is clear from context to reduce notational clutter. Further, we will extend this notation to function composition, i.e. write $g \\circ f(t, s))$ as $g(f)$ and function product, i.e. use $f g h$ instead of $f(t, s) g(t, s) g(t, s)$. Finally, we'll shorten $f g h \\circ \\phi(t, s)$ as $f g h(\\phi)$. We also define the tilting $\\chi$ and show that regardless of warping, we can construct a new orthogonal basis (note that the result holds for warping functions as in (9) and not just those as in (10)). Lemma C.1. For the set of orthonormal functions $\\left\\{p_{n}\\right\\}_{n=0}^{N-1}$ orthogonal over measure $\\omega I$, the set of basis functions\n\n$$\nq_{k}^{t}(\\sigma(t, s))=\\chi(t, s) p_{k}(\\sigma(t, s))\n$$\n\nare orthogonal over the measure\n\n$$\n\\mu(t, s)=\\omega(\\sigma(t, s)) \\mathbb{I}\\left[t_{0}, t\\right](s) \\sigma_{s}(t, s) \\chi(t, s)^{-2}\n$$\n\nfor time-warping function $\\sigma$ satisfying (9) and any $\\chi(t, s)$ that is non-zero in its support. Proof. Consider the following sequence of equalities:\n\n$$\n\\begin{aligned}\n\\int p_{j}(\\sigma) p_{k}(\\sigma) \\omega(\\sigma) \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s} d s & =\\int_{t_{0}}^{t} p_{j}(\\sigma) p_{k}(\\sigma) \\omega(\\sigma) \\sigma_{s} d s \\\\\n& =\\int_{\\sigma\\left(t, t_{0}\\right)}^{\\sigma(t, t)} p_{j}(y) p_{k}(y) \\omega(y) d y \\\\\n& =\\int_{\\sigma\\left(t, t_{0}\\right)}^{\\sigma(t, t)} p_{j}(y) p_{k}(y) \\omega(y) d y \\\\\n& =\\int_{0}^{1} p_{j}(y) p_{k}(y) \\omega(y) d y \\\\\n& =\\int p_{j}(y) p_{k}(y) \\omega(y) \\mathbb{I}(y) d y \\\\\n& =\\delta_{j k}\n\\end{aligned}\n$$\n\nIn the above, the second equality follows from the substitution $y \\leftarrow \\sigma(t, s)$ and hence $d y=\\sigma_{s} d s$ and the final equality follows from (8). Then since $\\chi(t, s)$ is always non-zero, we have\n\n$$\n\\int\\left(\\chi p_{j}(\\sigma)\\right)\\left(\\chi p_{k}(\\sigma)\\right) \\omega(\\sigma) \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s} \\chi^{-2} d s=\\delta_{j k}\n$$\n\nas desired. Without loss of generality, we can split $\\chi$ into a product\n\n$$\n\\chi(t, s)=\\frac{1}{\\psi(\\sigma(t, s)) \\phi(t, s)}\n$$\n\nof one part that depends on $\\sigma$ and another arbitrary component. Time Warped HiPPO. Since we have an orthonormal basis and measure, we can try to derive the (T)OSSM. For a given input signal $u(t)$, the HiPPO coefficients are defined as the projections. $$\n\\begin{aligned}\nx_{n}(t) & =\\left\\langle u, \\chi p_{n}\\right\\rangle_{\\mu} \\\\\n& =\\int u(s) \\cdot \\chi \\cdot\\left(p_{n} \\omega\\right)(\\sigma) \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s} \\chi^{-2} d s\n\\end{aligned}\n$$\n\ndefined as inner product of $u(t)$ with the tilted basis functions $\\chi p_{n}$ with respect to the measure $\\mu$ as defined in Lemma C.1. For additional convenience, we use the decomposition $\\chi=\\psi^{-1} \\phi^{-1}$ from (11) to get:\n\n$$\nx_{n}(t)=\\int u(s) \\cdot\\left(p_{n} \\omega \\psi\\right)(\\sigma) \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s} \\phi d s\n$$\n\nThe HiPPO technique is to differentiate through this integral in a way such that it can be related back to $x_{n}(t)$ and other $x_{k}(t)$. We require for every $n$, we require that there are a set of coefficients $\\left\\{\\gamma_{n k}\\right\\}_{k=0}^{N-1}$ such that\n\n$$\n\\sigma_{t}\\left(p_{n} \\omega \\psi\\right)^{\\prime}(\\sigma)=\\sum_{k=0}^{N-1} \\gamma_{n k}\\left(p_{n} \\omega \\psi\\right)(\\sigma)\n$$\n\nand for tilting component $\\phi$\n\n$$\n\\frac{d}{d t} \\phi(t, s)=d(t) \\phi(t, s)\n$$\n\nTheorem 12. Consider a set of basis functions $p_{n}$ orthogonal over $\\omega$, time warping $\\bar{\\sigma}(t, s)$ as in (9), (10), and tilting $\\chi$ as in (11) and (14) with the functions $\\sigma, p_{n}, \\omega, \\psi$ obeying (13). If $\\frac{d t_{0}}{d t} \\neq 0$, further assume that for some vector $\\boldsymbol{A}^{\\prime}$, we have as $N \\rightarrow \\infty$,\n\n$$\nu\\left(t_{0}\\right)=\\bar{c} \\sum_{k=0}^{N-1} \\boldsymbol{A}_{k}^{\\prime} \\cdot x_{k}(t)+\\bar{d} u(t)\n$$\n\nThen $\\left(\\boldsymbol{A}^{0}+(c(t)+d(t)) \\boldsymbol{I}-\\bar{c} \\boldsymbol{D}\\left(\\boldsymbol{A}^{\\prime}\\right)^{\\top}, \\boldsymbol{B}-\\bar{d} \\boldsymbol{D}\\right)$ is an OSSM for basis functions $\\chi p_{n}(\\sigma)$ with measure $\\omega \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s} \\chi^{-2}$ where\n\n$$\n\\boldsymbol{A}_{n k}^{0}=\\gamma_{n k}\n$$\n\nwith $\\gamma_{n k}$ as in (13),\n\n$$\n\\boldsymbol{D}_{n}=\\left(p_{n} \\omega \\psi\\right)\\left(\\sigma\\left(t, t_{0}\\right)\\right)\\left(\\sigma_{s} \\phi\\right)\\left(t, t_{0}\\right) \\cdot \\frac{d t_{0}}{d t}\n$$\n\nand\n\n$$\n\\boldsymbol{B}_{n}=\\left(p_{n} \\omega \\psi\\right)(1)\\left(\\sigma_{s} \\phi\\right)(t, t)\n$$\n\nProof. Applying the Leibniz rule to (12), we get\n\n$$\nx_{n}^{\\prime}(t)=x_{n}^{(0)}(t)+x_{n}^{(1)}(t)+x_{n}^{(2)}(t)+x_{n}^{(3)}(t)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& x_{n}^{(0)}(t)=\\int u(s) \\cdot \\sigma_{t}\\left(p_{n} \\omega \\psi\\right)^{\\prime}(\\sigma) \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s} \\phi d s \\\\\n& x_{n}^{(1)}(t)=\\int u(s) \\cdot\\left(p_{n} \\omega \\psi\\right)(\\sigma) \\mathbb{I}\\left[t_{0}, t\\right]\\left[\\frac{\\partial}{\\partial t}\\left(\\sigma_{s} \\phi\\right)\\right] d s\n\\end{aligned}\n$$\n\nand the $x_{n}^{(2)}(t)+x_{n}^{(3)}(t)$ terms capture the term we get when differentiating $\\mathbb{I}\\left[t_{0}, t\\right]$\nLet us consider each term separately. The first term\n\n$$\nx_{n}^{(0)}(t)=\\int u(s) \\cdot \\sigma_{t}\\left(p_{n} \\omega \\psi\\right)^{\\prime}(\\sigma) \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s} \\phi d s\n$$\n\ncorresponds to the differentiation of the basis functions and measure. In order to relate this to $\\left\\{x_{k}(t)\\right\\}$, it suffices that $\\sigma_{t}\\left(p_{n} \\omega \\psi\\right)^{\\prime}(\\sigma)$ satisfies (13) which implies that when we vectorize this, we get $x^{(0)}(t)=\\boldsymbol{A}^{0} \\cdot x(t)$. For additional warping and tilting terms, we consider\n\n$$\nx_{n}^{(1)}(t)=\\int u(s) \\cdot\\left(p_{n} \\omega \\psi\\right)(\\sigma) \\mathbb{I}\\left[t_{0}, t\\right]\\left[\\frac{\\partial}{\\partial t}\\left(\\sigma_{s} \\phi\\right)\\right] d s\n$$\n\nTo reduce this term to $x_{n}(t)$, recall from (10) that\n\n$$\n\\partial_{t}\\left(\\sigma_{s}\\right)=c(t) \\sigma_{s}\n$$\n\nThen the above and (14) imply\n\n$$\n\\partial_{t}\\left(\\sigma_{s} \\phi\\right)=c(t)\\left(\\sigma_{s} \\phi\\right)+d(t)\\left(\\sigma_{s} \\phi\\right)\n$$\n\nwhere $c(t), d(t)$ are defined as in (10) and (14). We will end up with $x_{n}^{(1)}(t)=(c(t)+d(t)) x_{n}(t)$. This leads to the the vectorized form $x^{(1)}(t)=(c(t)+$ $d(t)) \\boldsymbol{I} x(t)$. We now need to handle\n\n$$\nx_{n}^{(2)}(t)+x_{n}^{(3)}(t)=\\int u(s) \\cdot\\left(p_{n} \\omega \\psi\\right)(\\sigma)\\left[\\frac{\\partial}{\\partial t} \\mathbb{I}\\left[t_{0}, t\\right]\\right]\\left(\\sigma_{s} \\phi\\right) d s\n$$\n\nFor the above note that\n\n$$\n\\mathbb{I}\\left[t_{0}, t\\right](s)=H\\left(s-t_{0}\\right)-H(s-t)\n$$\n\nwhere $H(x)$ is the \"heaviside step function.\" It is know that $H^{\\prime}(x)=\\delta(x)$, which implies\n\n$$\n\\frac{\\partial}{\\partial t} \\mathbb{I}\\left[t_{0}, t\\right]=\\delta(s-t)-\\frac{d t_{0}}{d t} \\delta\\left(s-t_{0}\\right)\n$$\n\nUsing the above in RHS of (17), we separate out $x_{n}^{(2)}(t)$ and $x_{n}^{(3)}(t)$ as follows. First, define\n\n$$\n\\begin{aligned}\nx_{n}^{(2)}(t) & =\\int u(s) \\cdot\\left(p_{n} \\omega \\psi\\right)(\\sigma) \\delta(s-t) \\sigma_{s} \\phi d s \\\\\n& =u(t) \\cdot\\left(p_{n} \\omega \\psi\\right)(\\sigma(t, t))\\left(\\sigma_{s} \\phi\\right)(t, t) \\\\\n& =u(t) \\cdot\\left(p_{n} \\omega \\psi\\right)(1)\\left(\\sigma_{s} \\phi\\right)(t, t)\n\\end{aligned}\n$$\n\nIn the last equality, we have used the fact that $\\sigma(t, t)=\\bar{\\sigma}(t, 1)=1$ by definition. It follows that in vectorized form we have $x^{(2)}(t)=\\boldsymbol{B} u(t)$. Finally, define\n\n$$\n\\begin{aligned}\nx_{n}^{(3)}(t) & =-\\int u(s) \\cdot\\left(p_{n} \\omega \\psi\\right)(\\sigma) \\delta\\left(s-t_{0}\\right) \\frac{d t_{0}}{d t} \\sigma_{s} \\phi d s \\\\\n& =-u\\left(t_{0}\\right) \\cdot\\left(p_{n} \\omega \\psi\\right)\\left(\\sigma\\left(t, t_{0}\\right)\\right)\\left(\\sigma_{s} \\phi\\right)\\left(t, t_{0}\\right) \\cdot \\frac{d t_{0}}{d t}\n\\end{aligned}\n$$\n\nIf $\\frac{d t_{0}}{d t}=0$, then we have $\\boldsymbol{D}=\\mathbf{0}$ and hence we have $x^{(3)}(t)=\\mathbf{0}=-\\bar{c} \\boldsymbol{D}\\left(\\boldsymbol{A}^{\\prime}\\right)^{\\top} x(t)-\\bar{d} \\boldsymbol{D} u(t)$\nIf $\\frac{d t_{0}}{d t} \\neq 0$, then as $N \\rightarrow \\infty$, from (15), the above comes out to\n\n$$\nx_{n}^{(3)}(t)=-\\left(\\bar{c} \\sum_{k=0}^{N-1} \\boldsymbol{A}_{k}^{\\prime} \\cdot x_{k}(t)+\\bar{d} u(t)\\right) \\cdot\\left(p_{n} \\omega \\psi\\right)\\left(\\sigma\\left(t, t_{0}\\right)\\right)\\left(\\sigma_{s} \\phi\\right)\\left(t, t_{0}\\right) \\cdot \\frac{d t_{0}}{d t}\n$$\n\nIt follows that in vectorized form we have $x^{(3)}(t)=-\\bar{c} \\boldsymbol{D}\\left(\\boldsymbol{A}^{\\prime}\\right)^{\\top} x(t)-\\bar{d} \\boldsymbol{D} u(t)$.",
    "httyh-22": "The result follows after combining the terms. We see that the behavior of is the model is dictated by $t_{0}$. In particular, in this paper, we will consider two special cases. Corollary C. $2\\left(t_{0}\\right.$ independent of $\\left.t\\right)$. The $S S M((\\boldsymbol{A}+c(t)+d(t) \\boldsymbol{I}), \\boldsymbol{B})$ satisfying conditions of Theorem 12 with $t_{0}$ independent of $t$, is an OSSM for basis functions $\\chi p_{n}(\\sigma)$ with measure $\\omega \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s} \\chi^{-2}$ where $\\boldsymbol{A}=\\gamma_{n k}$ as in (13) and $\\boldsymbol{B}_{n}=\\left(p_{n} \\omega \\psi\\right)(1)\\left(\\sigma_{s} \\phi\\right)(t, t)$.",
    "httyh-23": "Proof. Follows from Theorem 12. Since $t_{0}$ is independent of $t$, then $\\frac{d t_{0}}{d t}=0$, and $\\boldsymbol{D}=\\mathbf{0}$.",
    "httyh-24": "Corollary C. $3\\left(t_{0}=t-\\theta\\right)$. The SSM $\\left(\\boldsymbol{A}^{0}+(c(t)+d(t)) \\boldsymbol{I}-\\bar{c} \\boldsymbol{D} \\boldsymbol{A}^{\\prime}, \\boldsymbol{B}-\\bar{d} \\boldsymbol{D}\\right)$ satisfying conditions of Theorem 12 with $t_{0}=t-\\theta$ for a fixed $\\theta$, is an OSSM with basis functions $\\chi p_{n}(\\sigma)$ with measure $\\omega \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s} \\chi^{-2}$ where $\\boldsymbol{A}_{n k}^{0}=\\gamma_{n k}$ as in (13), $\\boldsymbol{D}_{n}=\\left(p_{n} \\omega \\psi\\right)(\\sigma(t, t-\\theta))\\left(\\sigma_{s} \\phi\\right)(t, t-\\theta)$, and $\\boldsymbol{B}_{n}=\\left(p_{n} \\omega \\psi\\right)(1)\\left(\\sigma_{s} \\phi(t, t)\\right.$.",
    "httyh-25": "Proof.",
    "httyh-26": "This follows directly from Theorem 12 by setting $t_{0}=t-\\theta$. ## C. 3 LegS (and LSSL?)\n\n## C.3.1 Explanation of S4-LegS\n\nConsider the case when\n\n$$\n\\sigma=\\omega^{-1}\n$$\n\ni.e. the measure is completely \"tilted\" away, and let\n\n$$\n\\frac{\\partial}{\\partial t} \\sigma(t, s)=a(t) \\sigma(t, s)+b(t)\n$$\n\nLet's consider the special case of (18) where $b(t)=0$. This is most generally satisfied by\n\n$$\n\\sigma(t, s)=\\exp (a(t)+z(s))\n$$\n\nNote that the condition $\\sigma(t, t)=1$ forces $z=-a$. Hence, we have\n\n$$\n\\sigma(t, s)=\\exp (a(s)-a(t))\n$$\n\nWe now consider the following special case of Corollary C.2:\nCorollary C.4. Let $\\eta \\geq 0$. The $S S M\\left(-a^{\\prime}(t)(\\boldsymbol{A}+(\\eta+1) \\boldsymbol{I}), a^{\\prime}(t) \\boldsymbol{B}\\right)$, where $t_{0}$ is independent of $t$, is an OSSM for basis functions and measure\n\n$$\n\\frac{\\omega(\\sigma)}{\\sigma^{\\eta}} p_{n}(\\sigma) \\quad \\omega(t, s)=\\mathbb{I}(\\sigma) \\frac{a^{\\prime}(s) \\sigma^{2 \\eta+1}}{\\omega(\\sigma)}\n$$\n\nwhere $\\sigma$ satisfies (19),\n\n$$\n\\phi(t, s)=\\exp (\\eta a(s)-\\eta a(t))=\\sigma^{\\eta}\n$$\n\n$\\boldsymbol{A}=\\alpha_{n k}$ such that\n\n$$\ny p_{n}^{\\prime}(y)=\\sum_{k=0}^{n-1} \\alpha_{n k} p_{k}(y)\n$$\n\nand\n\n$$\n\\boldsymbol{B}_{n}=p_{n}(1)\n$$\n\nProof. Given a orthonormal basis $p_{0}, p_{1}, \\ldots, p_{N-1}$ with respect to a measure $\\omega$. Note that time-warping function $\\sigma$ satisfying (19) implies that $\\sigma_{s}=a^{\\prime}(s) \\sigma$. We fix tilting $\\chi(t, s)=\\frac{\\omega(\\sigma)}{\\sigma^{\\eta}}$, which in turn follows by setting\n\n$$\n\\psi=\\omega^{-1}\n$$\n\nWe show shortly that we satisfy the pre-conditions of Corollary C.2, which implies (with our choice of $\\chi$ and $\\sigma)$ that we have an OSSM with basis functions $p_{n}(t, s)=\\frac{\\omega(\\sigma)}{\\sigma^{\\eta}} p_{n}(\\sigma)$ and measure\n\n$$\n\\begin{aligned}\n\\omega(t, s) & =\\omega(\\sigma(t, s)) \\mathbb{I}\\left[t_{0}, t\\right] \\sigma_{s}(t, s) \\chi(t, s)^{-2} \\\\\n& =\\mathbb{I}\\left[t_{0}, t\\right] \\frac{a^{\\prime}(s) \\sigma^{2 \\eta+1}}{\\omega(\\sigma)}\n\\end{aligned}\n$$\n\nTo complete the proof, we show that out choice of paramters above satisfies the conditions of Corollary C. 2 (by showing they satisfy the conditions of Theorem 12). We verify that $\\sigma$ and $\\phi$ satisfy (10) and (14), noting that\n\n$$\n\\partial_{t}\\left(\\sigma_{s}\\right)=-a^{\\prime}(t) \\sigma_{s}\n$$\n\nand\n\n$$\n\\partial_{t}(\\phi)=-\\eta a^{\\prime}(t) \\phi\n$$\n\nThis implies that setting $c(t)=-a^{\\prime}(t)$ and $d(t)=-\\eta a^{\\prime}(t)$ is enough to satisfy (10) and (14). Further, note that (19) and the fact that $\\psi=\\omega^{-1}$ imply that\n\n$$\n\\sigma_{t}\\left(p_{n} \\omega \\psi\\right)^{\\prime}(\\sigma)=-a^{\\prime}(t) \\sigma p_{n}^{\\prime}(\\sigma)\n$$\n\nIt follows that (13) is satisfied as long as\n\n$$\n\\sigma p_{n}^{\\prime}(\\sigma)=\\sum_{k=0}^{n-1} \\alpha_{n k} p_{k}(\\sigma)\n$$\n\nfor some set of coefficients $\\left\\{\\alpha_{n k}\\right\\}_{k=0}^{N-1}$, which is exactly (21). This implies the $\\gamma_{n k}$ in Corollary C. 2 satisfy. $$\n\\gamma_{n k}=-a^{\\prime}(t) \\alpha_{n k}\n$$\n\nLet $\\boldsymbol{A}$ be the matrix such that $\\boldsymbol{A}_{n k}=-\\alpha_{n k}$ and then note that $-a^{\\prime}(t)(\\boldsymbol{A}+(\\eta+1) \\boldsymbol{I})$ is exactly the first parameter of the SSM in Corollary C.2. Similarly, recall in Corollary C. 2\n\n$$\n\\begin{aligned}\n\\boldsymbol{B}_{n} & =p_{n}(1)\\left(\\sigma_{s} \\phi\\right)(t, t) \\\\\n& =p_{n}(1) a^{\\prime}(t)\n\\end{aligned}\n$$\n\nwhere the final equality follows since in our case, $\\sigma_{s}(t, t)=a^{\\prime}(t) \\exp (a(t)-a(t))=a^{\\prime}(t)$. Overloading notation and letting $\\boldsymbol{B}_{n}=p_{n}(1)$, all conditions of Corollary C. 2 hold, from which the claimed result follows. We are particularly interested in the following two special cases of Corollary C.4. Corollary C.5. The SSM $\\left(-\\frac{1}{t}(\\boldsymbol{A}+\\boldsymbol{I}), \\frac{1}{t} \\boldsymbol{B}\\right)$ is a OSSM for basis functions $p_{n}\\left(\\frac{s}{t}\\right) \\omega\\left(\\frac{s}{t}\\right)$ with measure $\\frac{1}{t} \\mathbb{I}\\left[t_{0}, t\\right]\\left(\\frac{s}{t}\\right) \\cdot \\omega\\left(\\frac{s}{t}\\right)$ where $\\boldsymbol{A}=\\alpha_{n k}$ as in (21) and $\\boldsymbol{B}_{n}=p_{n}(1)$.",
    "httyh-27": "Proof. Letting $a^{\\prime}(t)=\\frac{1}{t}$ implies that $a(t)=\\ln t$. Then we can observe that is a case of Corollary C. 4 with time warping\n\n$$\n\\begin{aligned}\n\\sigma(t, s) & =\\exp (-\\ln t+\\ln s) \\\\\n& =\\exp (\\ln (s / t)) \\\\\n& =\\frac{s}{t}\n\\end{aligned}\n$$\n\nWe set $\\eta=0$ in Corollary C.4, which in turn sets $\\phi=\\sigma^{0}=1$. This gives the tilting\n\n$$\n\\begin{aligned}\n\\chi & =\\phi^{-1} \\psi^{-1} \\\\\n& =\\omega\n\\end{aligned}\n$$\n\nThen by Corollary C.4, it follows that that we can use $\\sigma$ and $\\chi$ to build an OSSM with basis functions\n\n$$\n\\frac{\\omega(\\sigma)}{\\sigma^{\\eta}} p_{n}(\\sigma)=\\omega\\left(\\frac{s}{t}\\right) \\cdot p_{n}\\left(\\frac{s}{t}\\right)\n$$\n\nwith measure\n\n$$\n\\mathbb{I}(\\sigma) \\frac{a^{\\prime}(s) \\sigma^{2 \\eta+1}}{\\omega(\\sigma)}=\\frac{1}{t} \\mathbb{I}(\\sigma) \\frac{\\sigma}{\\omega(\\sigma)}\n$$\n\nThen the result follows. Corollary C.6. The $S S M(-(\\boldsymbol{A}+\\boldsymbol{I}), \\boldsymbol{B})$ is a OSSM for basis functions $p_{n}\\left(e^{s-t}\\right) \\omega\\left(e^{s-t}\\right)$ with measure $\\omega=\\mathbb{I}\\left[t_{0}, t\\right]\\left(e^{s-t}\\right) \\frac{e^{s-t}}{\\omega\\left(e^{s-t}\\right)}$ where $\\boldsymbol{A}=\\alpha_{n k}$ as in (21) and $\\boldsymbol{B}_{n}=p_{n}(1)$. Proof. This is a case of Corollary C. 4 where $a^{\\prime}(t)=1, \\sigma=\\exp (s-t)$, and we pick $\\eta=0$, implying that $\\phi=\\sigma^{0}=1$. It follows that\n\n$$\n\\begin{aligned}\n\\chi & =\\phi^{-1} \\psi^{-1} \\\\\n& =\\omega\n\\end{aligned}\n$$\n\nUtilizing Corollary C.4, we can use $\\sigma$ and $\\chi$ to build an OSSM with basis functions\n\n$$\n\\frac{\\omega(\\sigma)}{\\sigma^{\\eta}} p_{n}(\\sigma)=\\omega(\\exp (s-t)) \\cdot p_{n}(\\exp (s-t))\n$$\n\nwith measure\n\n$$\n\\mathbb{I}(\\sigma) \\frac{a^{\\prime}(s) \\sigma^{2 \\eta+1}}{\\omega(\\sigma)}=\\mathbb{I}(\\sigma) \\frac{\\exp (s-t)}{\\omega(\\exp (s-t))}\n$$\n\nThis gives us our final result.",
    "httyh-28": "Next we instantiate Corollary C. 4 to prove Corollary 3.1. (Even though strictly not needed, we instantiate Corollary C. 6 and Corollary C. 5 to prove Theorem 5 and Corollary 3.3.) To that end, we will need the following result:\n\nLemma C.7. Let the Legendre polynomials orthonormal over the interval $[0,1]$ be denoted as $L_{n}$. Then\n\n$$\n\\begin{aligned}\n& y L_{n}^{\\prime}(y)=n L_{n}(y)+\\sqrt{2 n+1}\\left(\\sum_{k=0}^{n-1} \\sqrt{2 k+1} L_{k}(y)\\right), \\\\\n& L_{n}^{\\prime}(y)=2 \\sqrt{2 n+1}\\left(\\sum_{0 \\leq k \\leq n-1, n-k} \\text { is odd } \\sqrt{2 k+1} L_{k}(y)\\right),\n\\end{aligned}\n$$\n\nand\n\n$$\nL_{n}(0)=(2 n+1)^{\\frac{1}{2}}(-1)^{n} \\text { and } L_{n}(1)=(2 n+1)^{\\frac{1}{2}}\n$$\n\nProof. The Legendre polynomials satisfy the following orthogonality condition over $[-1,1]$ :\n\n$$\n\\int_{-1}^{1} P_{m}(z) P_{n}(z) d z=\\frac{2}{2 n+1} \\delta_{m n}\n$$\n\nLet us denote the normalized Legendre polynomials orthogonal over $[-1,1]$ as $\\lambda_{n} P_{n}(z)$ where $\\lambda_{n}=\\sqrt{\\frac{2 n+1}{2}}$. To orthogonalize them over $[0,1]$, let $y=\\frac{1+z}{2}$. It follows that $z=2 y-1, d z=2 d y$. Note that we then have\n\n$$\n\\int_{-1}^{1} P_{m}(z) P_{n}(z) d z=\\int_{0}^{1} 2 P_{m}(2 y-1) P_{n}(2 y-1) d y\n$$\n\nThis implies that\n\n$$\n\\int_{-1}^{1} \\frac{2 n+1}{2} \\cdot 2 P_{m}(2 y-1) P_{n}(2 y-1) d y=\\delta_{m n}\n$$\n\nThen if we let\n\n$$\nL_{n}(y)=\\sqrt{2} \\lambda_{n} P_{n}(2 y-1)=\\sqrt{2 n+1} P_{n}(2 y-1)\n$$\n\nthen we have an a set of functions over $[0,1]$ such that\n\n$$\n\\int_{0}^{1} L_{m}(y) L_{n}(y) d y=\\delta_{m n}\n$$\n\nFrom $[2,(2.8),(2.9)]$, note that $P_{n}(-1)=(-1)^{n}$ and $P_{n}(1)=1$. This implies that\n\n$$\nL_{n}(0)=\\sqrt{2 n+1} P_{n}(-1), \\quad L_{n}(1)=\\sqrt{2 n+1} P_{n}(1)\n$$\n\nFinally note that (25) implies:\n\n$$\n\\begin{aligned}\nL_{n}^{\\prime}(y) & =2 \\sqrt{2 n+1} P_{n}^{\\prime}(2 y-1) \\\\\n& =2 \\sqrt{2 n+1} P_{n}^{\\prime}(z)\n\\end{aligned}\n$$\n\nFrom $[5,7]$, we get\n\n$$\nP_{n}^{\\prime}(z)=\\sum_{0 \\leq k \\leq n-1, n-k \\text { is odd }}(2 k+1) P_{k}(z)\n$$\n\nUsing (25) on the above, we get (23). We now consider\n\n$$\n\\begin{aligned}\ny L_{n}^{\\prime}(y) & =2 y \\sqrt{2 n+1} P_{n}^{\\prime}(z) \\\\\n& =(1+z) \\sqrt{2 n+1} P_{n}^{\\prime}(z)\n\\end{aligned}\n$$\n\nFrom $[5,8]$, we get\n\n$$\n(z+1) P_{n}^{\\prime}(z)=n P_{n}(z)+\\sum_{k=0}^{n-1}(2 k+1) P_{k}(z)\n$$\n\nThen the above becomes\n\n$$\ny L_{n}^{\\prime}(y)=\\sqrt{2 n+1}\\left(n P_{n}(z)+\\sum_{k=0}^{n-1}(2 k+1) P_{k}(z)\\right)\n$$\n\n(25) implies that $P_{n}(z)=\\frac{L_{n}(y)}{\\sqrt{2 n+1}}$, thus\n\n$$\ny L_{n}^{\\prime}(y)=n L_{n}(z)+\\sqrt{2 n+1}\\left(\\sum_{k=0}^{n-1} \\sqrt{2 k+1} L_{k}(z)\\right)\n$$\n\nWe now re-state and prove Corollary 3.1:\nCorollary C. 8 (Corollary 3.1, restated). Let $L_{n}$ be the Legendre polynomials orthonormal over the interval $[0,1]$. Define $\\sigma(t, s)=\\exp (a(s)-a(t))$. The SSM $\\left(a^{\\prime}(t) \\boldsymbol{A}, a^{\\prime}(t) \\boldsymbol{B}\\right)$ is an OSSM with\n\n$$\n\\omega(t, s)=\\mathbb{I}(\\sigma(t, s)) a^{\\prime}(s) \\sigma(t, s) \\quad p_{n}(t, s)=L_{n}(\\sigma(t, s))\n$$\n\nwhere $\\boldsymbol{A}$ and $\\boldsymbol{B}$ are defined as in (4).",
    "httyh-29": "Proof. We consider our basis functions, the Legendre polynomials, which are orthogonal with respect to unit measure. This allows us to invoke Corollary C. 4 with $\\omega=1$. Further, here we have $t_{0}=-\\infty$ and $\\eta=0$. Now we have an SSM:\n\n$$\n\\left(-a^{\\prime}(t)\\left(\\boldsymbol{A}^{0}+\\boldsymbol{I}\\right), a^{\\prime}(t) \\boldsymbol{B}\\right)\n$$\n\nwhere $\\boldsymbol{A}_{n k}^{0}=\\alpha_{n k}$ as in $(21)$ and $\\boldsymbol{B}_{n}=L_{n}(1)$. From (24) observe that $\\boldsymbol{B}_{n}=(2 n+1)^{\\frac{1}{2}}$. From (22), we have\n\n$$\n\\alpha_{n k}= \\begin{cases}(2 n+1)^{\\frac{1}{2}}(2 k+1)^{\\frac{1}{2}} & k<n \\\\ n & k=n \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nWe write that $\\boldsymbol{A}=-\\left(\\boldsymbol{A}^{0}+\\boldsymbol{I}\\right)$. Indeed,\n\n$$\n-\\left(\\boldsymbol{A}^{0}+\\boldsymbol{I}\\right)_{n k}=- \\begin{cases}(2 n+1)^{\\frac{1}{2}}(2 k+1)^{\\frac{1}{2}} & \\text { if } k<n \\\\ n+1 & \\text { if } k=n \\\\ 0 & \\text { if } k>n\\end{cases}\n$$\n\nThus the $\\boldsymbol{A}$ and $\\boldsymbol{B}$ match those in (4), which completes our claim. We now re-state and prove Theorem 5:\nCorollary C.",
    "httyh-30": "9 (Theorem 5, restated). Let $L_{n}$ be the Legendre polynomials orthonormal over the interval $[0,1]$. Then the $S S M\\left(\\frac{1}{t} \\boldsymbol{A}, \\frac{1}{t} \\boldsymbol{B}\\right)$ is a OSSM for basis functions $L_{n}\\left(\\frac{s}{t}\\right)$ and measure $\\frac{1}{t} \\mathbb{I}\\left[t_{0}, t\\right]$ where $\\boldsymbol{A}$ and $\\boldsymbol{B}$ are defined as in (4).",
    "httyh-31": "Proof. We consider our basis functions, the Legendre polynomials, which are orthogonal with respect to unit measure. This allows us to invoke Corollary C. 5 with $\\omega=1$. Now we have\n\n$$\nx^{\\prime}(t)=\\frac{1}{t}\\left[-\\left(\\boldsymbol{A}^{0}+\\boldsymbol{I}\\right) x(t)+\\boldsymbol{B} u(t)\\right]\n$$\n\nwhere $\\boldsymbol{A}_{n k}^{0}=\\alpha_{n k}$ as in (21) and $\\boldsymbol{B}_{n}=L_{n}(1)$. From (24) observe that $\\boldsymbol{B}_{n}=(2 n+1)^{\\frac{1}{2}}$. From (22), we have\n\n$$\n\\alpha_{n k}= \\begin{cases}(2 n+1)^{\\frac{1}{2}}(2 k+1)^{\\frac{1}{2}} & k<n \\\\ n & k=n \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nWe write that $\\boldsymbol{A}=-\\left(\\boldsymbol{A}^{0}+\\boldsymbol{I}\\right)$. Indeed,\n\n$$\n-\\left(\\boldsymbol{A}^{0}+\\boldsymbol{I}\\right)_{n k}=- \\begin{cases}(2 n+1)^{\\frac{1}{2}}(2 k+1)^{\\frac{1}{2}} & \\text { if } k<n \\\\ n+1 & \\text { if } k=n \\\\ 0 & \\text { if } k>n\\end{cases}\n$$\n\nwhich completes our claim.",
    "httyh-32": "We now restate and prove Corollary 3.3.",
    "httyh-33": "Corollary C. 10 (Corollary 3.3, restated). Let $L_{n}$ be the Legendre polynomials orthonormal over the interval $[0,1]$. Then the $\\operatorname{SSM}(\\boldsymbol{A}, \\boldsymbol{B})$ is a TOSSM for basis functions $L_{n}\\left(e^{-t}\\right)$ with measure $\\omega=\\mathbb{I}\\left[t_{0}, t\\right] e^{-t}$ where $\\boldsymbol{A}, \\boldsymbol{B}$ are defined as in (4). Proof. We consider our basis functions, the Legendre polynomials, which are orthogonal with respect to unit measure, warping function $\\sigma=\\exp (s-t)$, and with tilting $\\chi=\\omega$. We note that $\\sigma=\\exp (s-t)$ satisfies (19) with, $a^{\\prime}(t)=1$. This allows us to invoke Corollary C.5. Then $x^{\\prime}(t)=(\\boldsymbol{A}+\\boldsymbol{I}) x(t)+\\boldsymbol{B} u(t)$ orthogonalizes against the basis functions $L_{n}\\left(e^{s-t}\\right)$ with measure $\\mathbb{I}[-\\infty, t] e^{s-t}$ where $\\boldsymbol{A}=\\alpha_{n k}$ as in 21. Note that the $\\operatorname{SSM}$ basis functions $K_{n}(t, s)=K_{n}(s-t)$, hence we get the claimed SSM form utilizing the same argument for $\\boldsymbol{A}, \\boldsymbol{B}$ as in the proof of Corollary C. 9\n\nThis explains why removing the $\\frac{1}{t}$ factor from HiPPO-LegS still works: it is orthogonalizing onto the Legendre polynomials with an exponential \"warping\". ## C. 4 Finite Windows\n\n## C.4.1 LegT Derivation\n\nCorollary C.11. Let $L_{n}$ be the Legendre polynomials orthonormal over the interval $[0,1]$ and let $\\sigma=1-\\frac{t-s}{\\theta}$ for a constant $\\theta$. Then the $S S M\\left(\\frac{1}{\\theta} \\boldsymbol{A}, \\frac{1}{\\theta} \\boldsymbol{B}\\right)$ is a OSSM for basis functions $L_{n}(\\sigma)$ with measure $\\frac{1}{\\theta} \\mathbb{I}\\left[t_{0}, t\\right](\\sigma)$ where $\\boldsymbol{A}, \\boldsymbol{B}$ are defined as in (5). Proof. Out plan is to apply Corollary C.3, for which we must show that the basis functions $L_{n}(t, s)$, time warping $\\sigma(t, s)$, and tilting $\\chi(t, s)=\\psi^{-1} \\phi^{-1}(t, s)$ satisfy (13), (10), and (14), respectively. We first set some parameters- note that because $\\omega=1$ and set $\\psi=\\phi=1$. The above implies that we have\n\n$$\n\\sigma_{t}\\left(L_{n} \\omega \\psi\\right)^{\\prime}(\\sigma)=-\\frac{1}{\\theta} L_{n}^{\\prime}(\\sigma)\n$$\n\nThe above along with (23), we see that the Legendre polynomials satisfy (13) with\n\n$$\n\\gamma_{n k}=\\frac{1}{\\theta} \\cdot \\begin{cases}-2 \\cdot(2 n+1)^{\\frac{1}{2}}(2 k+1)^{\\frac{1}{2}} & k<n \\text { and } n-k \\text { is odd } \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nWe also note that $\\sigma_{s}=\\frac{1}{\\theta}$. It follows that\n\n$$\n\\frac{d}{d t} \\sigma_{s}=0\n$$\n\nsatisfying (10) trivially by setting $c(t)=0$. Similarly, since $\\phi=1$ (14) is also satisfied trivially by setting $d(t)=0$. Finally we note that the $L_{n}$ forms a complete basis over $[0,1]$, hence as $N \\rightarrow \\infty$, we have\n\n$$\nu(t-\\theta)=\\sum_{k=0}^{N-1} x_{k}(t) L_{n}(\\sigma(t, t-\\theta))=\\sum_{k=0}^{N-1} x_{k}(t) L_{n}(0)\n$$\n\nThe above defines $\\boldsymbol{A}^{\\prime}$ by setting $\\boldsymbol{A}_{n}^{\\prime}=L_{n}(0)$ (as well as $\\bar{c}=1$ and $\\bar{d}=0$.) Now by Corollary C.3, we have an SSM\n\n$$\n\\left(\\boldsymbol{A}^{0}-\\boldsymbol{D}\\left(\\boldsymbol{A}^{\\prime}\\right)^{\\top}, \\boldsymbol{B}^{\\prime}\\right)\n$$\n\nwhere $\\boldsymbol{D}_{n}=\\frac{1}{\\theta} L_{n}(0)$, and by (22) $\\boldsymbol{A}_{n k}^{0}=\\gamma_{n k}\\left(\\right.$ as in (26)) and $\\boldsymbol{B}_{n}^{\\prime}=\\frac{1}{\\theta} L_{n}(1)$. From (24), we have $\\boldsymbol{D}_{n}=\\frac{1}{\\theta}(2 n+1)^{\\frac{1}{2}}(-1)^{n}$ and $\\boldsymbol{B}_{n}=\\frac{1}{\\theta}(2 n+1)^{\\frac{1}{2}}$. Thus, we have\n\n$$\n\\left(\\boldsymbol{A}^{0}-\\boldsymbol{D}\\left(\\boldsymbol{A}^{\\prime}\\right)^{\\top}\\right)_{n k}=\\frac{1}{\\theta} \\cdot \\begin{cases}-(2 n+1)^{\\frac{1}{2}}(2 k+1)^{\\frac{1}{2}}\\left(2+(-1)^{n-k}\\right) & k<n \\text { and } n-k \\text { is odd } \\\\ -(2 n+1)^{\\frac{1}{2}}(2 k+1)^{\\frac{1}{2}}(-1)^{n-k} & \\text { otherwise }\\end{cases}\n$$\n\nThe proof is complete by noting that $\\boldsymbol{A}^{0}-\\boldsymbol{D}\\left(\\boldsymbol{A}^{\\prime}\\right)^{\\top}=\\frac{1}{\\theta} \\boldsymbol{A}$ and $\\boldsymbol{B}^{\\prime}=\\frac{1}{\\theta} \\boldsymbol{B}$. We note that Corollary C. 11 implies Proposition 4. More specifically, Proposition 4 follows by setting $\\theta=1$ in Corollary C. 11 and noticing that the OSSM there is actually a TOSSM. (Technically we get basis function $L_{n}(1-t)$ for measure $\\mathbb{I}(1-t)$ but this is OK since $\\int_{0}^{1} L_{k}(1-t) L_{j}(1-t) d t=\\int_{0}^{1} L_{k}(t) L_{j}(t) d t$. We first give a proof of Theorem 6. Then, we prove Theorem 7 as a function approximation result pertaining to S4-FouT. ## C.4.2 Explanation of S4-FouT\n\nProof of Theorem 6. We seek to derive $\\boldsymbol{A}$ and $\\boldsymbol{B}^{\\prime}$ from (6) using Corollary C.3:\nWe use the time-warping function $\\sigma(t, s)=1-(t-s)$, which implies that we have\n\n$$\n\\begin{aligned}\n& \\sigma_{s}(t, s)=1 \\\\\n& \\frac{\\partial}{\\partial t} \\sigma_{s}(t, s)=0\n\\end{aligned}\n$$\n\nThus, we can take\n\n$$\nc(t)=0 \\text { in } \\frac{\\partial}{\\partial t} \\sigma_{s}(t, s)=c(t) \\sigma_{s}(t, s)\n$$\n\nWe then have $\\chi(t, s)=1$ as we set\n\n$$\n\\begin{aligned}\n& \\psi(t, s)=\\phi(t, s)=1 \\\\\n& \\frac{d}{d t} \\phi(t, s)=0\n\\end{aligned}\n$$\n\nSo, we can take\n\n$$\nd(t)=0 \\text { in } \\frac{d}{d t} \\phi(t, s)=d(t) \\phi(t, s)\n$$\n\nWe also have $\\omega(\\sigma)=1$, and we order our bases in the form $p_{n}=\\left(1, c_{1}(t), s_{1}(t), c_{2}(t), s_{2}(t), \\ldots\\right)^{1}$, where the basis functions have derivatives:\n\n$$\n\\begin{aligned}\n(1)^{\\prime}(\\sigma) & =0 \\\\\n\\left(c_{n}\\right)^{\\prime}(\\sigma) & =-2 \\pi n s_{n}(\\sigma) \\\\\n\\left(s_{n}\\right)^{\\prime}(\\sigma) & =2 \\pi n c_{n}(\\sigma)\n\\end{aligned}\n$$\n\nConsequently, we can define $\\gamma_{n k}$ as follows:\n\n$$\n\\gamma_{n k}= \\begin{cases}2 \\pi n & n-k=1, k \\text { odd } \\\\ -2 \\pi k & k-n=1, n \\text { odd } \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nFurther, the discontinuity is at $t_{0}=t-\\theta, \\theta=1$ which implies that $\\frac{d t_{0}}{d t}=1$. We now seek to use the stored approximation to $u$ at time $t$ to compute $u(t-1)$. First, denote the latent state $x(t)$ with coefficients $x=\\left(x^{1}(t), x_{1}^{c}(t), x_{1}^{s}(t), x_{2}^{c}(t), x_{2}^{s}(t), \\ldots\\right)$ and define the functions $v(s)$ and $w(s)$ such that we have\n\n$$\nv(s)=u(2 t-s-1) \\quad \\text { and } \\quad w(s)=\\frac{u(s)+v(s)}{2} \\quad \\text { for } s \\in[t-1, t]\n$$\n\nNow, let $\\hat{u}, \\hat{v}$, and $\\hat{w}$ denote the reconstruction of $u, v$ and $w$, where we have\n\n$$\n\\begin{array}{r}\n\\hat{u}(t, s)=\\langle u(s), 1\\rangle+\\sum_{n}\\left\\langle u(s), s_{n}(\\sigma(t, s))\\right\\rangle s_{n}(\\sigma(t, s))+\\sum_{n}\\left\\langle u(s), c_{n}(\\sigma(t, s))\\right\\rangle c_{n}(\\sigma(t, s)) \\\\\n\\hat{v}(t, s)=\\langle v(s), 1\\rangle+\\sum_{n}\\left\\langle v(s), s_{n}(\\sigma(t, s))\\right\\rangle s_{n}(\\sigma(t, s))+\\sum_{n}\\left\\langle v(s), c_{n}(\\sigma(t, s))\\right\\rangle c_{n}(\\sigma(t, s)) \\\\\n\\hat{w}(t, s)=\\langle w(s), 1\\rangle+\\sum_{n}\\left\\langle w(s), s_{n}(\\sigma(t, s))\\right\\rangle s_{n}(\\sigma(t, s))+\\sum_{n}\\left\\langle w(s), c_{n}(\\sigma(t, s))\\right\\rangle c_{n}(\\sigma(t, s))\n\\end{array}\n$$\n\nThen, we claim that\n\n$$\n\\hat{u}(t, t)=\\frac{u(t)+v(t)}{2}=\\frac{u(t)+u(t-1)}{2}\n$$\n\nTowards that end, we examine the sine and cosine coefficients of $u$ and $v$ as follows:\n\n$$\n\\begin{aligned}\n\\left\\langle v, c_{n}\\right\\rangle & =\\int v(s) c_{n}(\\sigma(t, s)) \\mathbb{I}[t-1, t] d s=\\int u(2 t-s-1) c_{n}(\\sigma(t, s)) \\mathbb{I}[t-1, t] d s \\\\\n& =\\int u\\left(s^{\\prime}\\right) c_{n}\\left(1-\\sigma\\left(t, s^{\\prime}\\right)\\right) \\mathbb{I}[t-1, t] d s^{\\prime} \\\\\n& =\\int u\\left(s^{\\prime}\\right) c_{n}\\left(\\sigma\\left(t, s^{\\prime}\\right)\\right) \\mathbb{I}[t-1, t] d s^{\\prime}=\\left\\langle u, c_{n}\\right\\rangle \\\\\n\\left\\langle v, s_{n}\\right\\rangle & =\\int v(s) s_{n}(\\sigma(t, s)) \\mathbb{I}[t-1, t] d s=\\int u(2 t-s-1) s_{n}(\\sigma(t, s)) \\mathbb{I}[t-1, t] d s \\\\\n& =\\int u\\left(s^{\\prime}\\right) s_{n}\\left(1-\\sigma\\left(t, s^{\\prime}\\right)\\right) \\mathbb{I}[t-1, t] d s^{\\prime} \\\\\n& =-\\int u\\left(s^{\\prime}\\right) s_{n}\\left(\\sigma\\left(t, s^{\\prime}\\right)\\right) \\mathbb{I}[t-1, t] d s^{\\prime}=-\\left\\langle u, s_{n}\\right\\rangle\n\\end{aligned}\n$$\n\nHere, for (38) and (39), we use the change of variables $s^{\\prime} \\leftarrow 2 t-s-1$, which gives us\n\n$$\n\\sigma(t, s)=1-(t-s)=1-(1+t-s-1)=1-[1-(t-(2 t-s-1))]=1-\\left(1-\\left(t-s^{\\prime}\\right)\\right)=1-\\sigma\\left(t, s^{\\prime}\\right)\n$$\n\n[^1]Then, we use the fact that $c_{n}\\left(1-\\sigma\\left(t, s^{\\prime}\\right)\\right)=c_{n}\\left(\\sigma\\left(t, s^{\\prime}\\right)\\right)$ but $s_{n}\\left(1-\\sigma\\left(t, s^{\\prime}\\right)\\right)=-s_{n}\\left(\\sigma\\left(t, s^{\\prime}\\right)\\right)$. That is, both $u$ and $v$ have the same cosine coefficients but negated sine coefficients of each other. But, we know that both $s_{n}(\\sigma(t, t-1))=s_{n}(1-(t-(t-1)))=s_{n}(0)=0$ and $s_{n}(\\sigma(t, t))=s_{n}(1-(t-t))=s_{n}(1)=0$, and hence, the reconstruction of $\\hat{u}$ at the endpoints $\\sigma(t, t-1)=0$ and $\\sigma(t, t)=1$ depends only on the cosine coefficients, whence we assert that the reconstruction $\\hat{u}$ agrees with $\\hat{v}$ at both endpoints. Therefore, we have $\\hat{u}(t, t)=\\hat{v}(t, t)$ implying that $\\hat{w}(t, t)=\\hat{u}(t, t)$. Note that $w$ is continuous and periodic, for which the basis $\\left\\{1, c_{n}, s_{n}\\right\\}_{n}$ is complete, and hence, we know that as $N \\rightarrow \\infty, \\hat{w} \\rightarrow w$. Thus, at $s=t$, we have $\\hat{u}(t, t)=\\hat{w}(t, t)=w(t)=\\frac{u(t)+v(t)}{2}=\\frac{u(t)+u(t-1)}{2}$, which completes the proof of the claim in (37). Recall from (34) that we can express the stored approximation of $u(t)$, given by $\\hat{u}(t, s)$, as follows:\n\n$$\n\\hat{u}(t, s)=\\langle u(s), 1\\rangle+\\sum_{n}\\left\\langle u(s), s_{n}(\\sigma(t, s))\\right\\rangle s_{n}(\\sigma(t, s))+\\sum_{n}\\left\\langle u(s), c_{n}(\\sigma(t, s))\\right\\rangle c_{n}(\\sigma(t, s))\n$$\n\nFor the value at $t$, the approximation $\\hat{u}(t, t)$ is then given by\n\n$$\n\\hat{u}(t, t)=x^{1}(t)+\\sum_{k} x_{k}^{c}(t) c_{k}(1)+\\sum_{k} x_{k}^{s}(t) s_{k}(1)=x^{1}(t)+\\sum_{k} \\sqrt{2} x_{k}^{c}(t)\n$$\n\nDue to (37), we know $u(t-1)=2 \\hat{u}(t, t)-u(t)$, which combined with the above yields:\n\n$$\nu(t-1)=2 x^{1}(t)+2 \\sqrt{2} \\sum_{k} x_{k}^{c}(t)-u(t)\n$$\n\nFinally, with regards to Corollary C.3, for Theorem 12, (29) satisfies (10) and (32) satisfies (14) with (33) satisfying (13) for $\\boldsymbol{A}^{0}$. Moreover, from (40), we can take $\\bar{c}=1, \\bar{d}=-1$, and $\\boldsymbol{A}_{k}^{\\prime}:=\\left\\{\\begin{array}{ll}2 & k=0 \\\\ 2 \\sqrt{2} & k \\text { odd } \\\\ 0 & \\text { otherwise }\\end{array}\\right.$ to satisfy (15). Invoking Corollary C. 3 now yields the following OSSM: ${ }^{2}$\n\n$$\n\\left(\\boldsymbol{A}^{0}+(c(t)+d(t)) \\boldsymbol{I}-\\bar{c} \\boldsymbol{D}\\left(\\boldsymbol{A}^{\\prime}\\right)^{\\top}, \\boldsymbol{B}-\\bar{d} \\boldsymbol{D}\\right)\n$$\n\nwhere $\\boldsymbol{A}_{n k}^{0}=\\gamma_{n k}$ with $\\boldsymbol{D}_{n}$ and $\\boldsymbol{B}_{n}$ specified as follows:\n\n$$\n\\begin{aligned}\n& \\boldsymbol{D}_{n}= \\begin{cases}1 & n=0 \\\\\n\\sqrt{2} & n \\text { odd } \\\\\n0 & \\text { otherwise }\\end{cases} \\\\\n& \\boldsymbol{B}_{n}= \\begin{cases}1 & n=0 \\\\\n\\sqrt{2}, & n \\text { odd } \\\\\n0 & \\text { otherwise }\\end{cases}\n\\end{aligned}\n$$\n\nHere, the values are derived from the expressions of Corollary C.3:\n\n$$\n\\boldsymbol{D}_{n}=\\left(p_{n} \\omega \\psi\\right)(\\sigma(t, t-1))\\left(\\sigma_{s} \\phi\\right)(t, t-1) \\text { and } \\boldsymbol{B}_{n}=\\left(p_{n} \\omega \\psi\\right)(1)\\left(\\sigma_{s} \\phi\\right)(t, t)\n$$\n\nRecall that we have $p_{n} \\in\\left\\{1, c_{n}, s_{n}\\right\\}, \\omega(t, s)=1$, and from (27) and $(30), \\sigma_{s}(t, s)=1$ with $\\psi(t, s)=\\phi(t, s)=1$.",
    "httyh-34": "Thus, (41) is due to $1(0) \\cdot 1=1, s_{n}(0) \\cdot 1=0$ but $c_{n}(0) \\cdot 1=\\sqrt{2}$. Similarly, (42) is because $1(0) \\cdot 1=1, s_{n}(1) \\cdot 1=0$ but again $c_{n}(1) \\cdot 1=\\sqrt{2}$. [^2]Now, we have\n\n$$\n\\left[\\boldsymbol{D}\\left(\\boldsymbol{A}^{\\prime}\\right)^{\\top}\\right]_{n k}=\\left\\{\\begin{array}{ll}\n2 & n=k=0 \\\\\n2 \\sqrt{2} & n=0, k \\text { odd or } k=0, n \\text { odd } \\\\\n4 & n, k \\text { odd } \\\\\n0 & \\text { otherwise }\n\\end{array} \\quad[\\bar{d} \\boldsymbol{D}]_{n}= \\begin{cases}-1 & n=0 \\\\\n-\\sqrt{2} & n \\text { odd } \\\\\n0 & n \\text { otherwise }\\end{cases}\\right. $$\n\nAs $c(t)=d(t)=0$, we define $\\boldsymbol{A} \\leftarrow \\boldsymbol{A}^{0}-\\bar{c} \\boldsymbol{D}\\left(\\boldsymbol{A}^{\\prime}\\right)^{\\top}$ and $\\boldsymbol{B} \\leftarrow \\boldsymbol{B}-\\bar{d} \\boldsymbol{D}$, given by\n\n$$\n\\boldsymbol{A}_{n k}=\\left\\{\\begin{array}{ll}\n-2 & n=k=0 \\\\\n-2 \\sqrt{2} & n=0, k \\text { odd or } k=0, n \\text { odd } \\\\\n-4 & n, k \\text { odd } \\\\\n2 \\pi n & n-k=1, k \\text { odd } \\\\\n-2 \\pi k & k-n=1, n \\text { odd } \\\\\n0 & \\text { otherwise }\n\\end{array} \\quad \\boldsymbol{B}_{n}= \\begin{cases}2 & n=0 \\\\\n2 \\sqrt{2} & n \\text { odd } \\\\\n0 & \\text { otherwise }\\end{cases}\\right. $$\n\n## C.4.3 Function Approximation Error\n\nProof of Theorem 7. First, the state size being $N$ dictates that there are $\\lfloor N / 2\\rfloor s_{n}$ and $c_{n}$ basis functions each. We fix time $t$ and denote $x_{n}^{c}$ and $x_{n}^{s}$ to be the respective coefficients for $s_{n}$ and $c_{n}$ basis corresponding to S4-Fou. Since $\\left\\{s_{n}, c_{n}\\right\\}_{n \\geq 0}$ forms an orthonormal basis, by Parseval's identity, we have\n\n$$\n\\|K-\\hat{K}\\|_{2}^{2}=\\sum_{n=\\lfloor N / 2\\rfloor}^{\\infty} x_{n}^{c^{2}}(t)+x_{n}^{s} 2(t)\n$$\n\nThus, in order to bound the error, it suffices to bound the high-order coefficients by integration by parts as follows:\n\n$$\n\\begin{aligned}\nx_{n}^{c}(t) & =\\left\\langle K, c_{n}\\right\\rangle=\\int_{0}^{1} K(t) c_{n}(t) d t \\\\\n& =\\left[K(t) \\frac{1}{2 \\pi n} s_{n}(t)\\right]_{0}^{1}-\\frac{1}{2 \\pi n} \\int_{0}^{1} K^{\\prime}(t) s_{n}(t) d t \\\\\n& =-\\frac{1}{2 \\pi n} \\int_{0}^{1} K^{\\prime}(t) s_{n}(t) d t\n\\end{aligned}\n$$\n\nThe quantity in the bracket vanishes as $s_{n}$ is periodic. Therefore\n\n$$\n\\left|x_{n}^{c}\\right| \\leq\\left|\\frac{1}{2 \\pi n} \\int_{0}^{1} K^{\\prime}(t) s_{n}(t) d t\\right| \\leq \\frac{1}{2 \\pi n} \\int_{0}^{1}\\left|K^{\\prime}(t)\\right|\\left|s_{n}(t)\\right| d t \\leq \\frac{L}{2 \\pi n}\n$$\n\nwhere we use the fact that $K$ is $L$-Lipshitz. For $x_{n}^{s}$, a similar argument holds and we get:\n\n$$\n\\left|x_{n}^{s}\\right| \\leq\\left|\\frac{1}{2 \\pi} \\int_{0}^{1} K^{\\prime}(t) c_{n}(t) d t\\right| \\leq \\frac{1}{2 \\pi} \\int_{0}^{1}\\left|K^{\\prime}(t)\\right|\\left|c_{n}(t)\\right| d t \\leq \\frac{L}{2 \\pi n}\n$$\n\nDue to (43), this then implies that\n\n$$\n\\begin{aligned}\n\\|K-\\hat{K}\\|_{2}^{2} & =\\sum_{n=\\lfloor N / 2\\rfloor}^{\\infty} x_{n}^{c^{2}}(t)+x_{n}^{s}{ }^{2}(t)=\\sum_{n=\\lfloor N / 2\\rfloor}^{\\infty}\\left|x_{n}^{c}\\right|^{2}(t)+\\left|x_{n}^{s}\\right|^{2}(t) \\\\\n& \\leq \\sum_{n=\\lfloor N / 2\\rfloor}^{\\infty} \\frac{2 L^{2}}{(2 \\pi n)^{2}}=\\frac{2 L^{2}}{(2 \\pi)^{2}} \\sum_{n=\\lfloor N / 2\\rfloor}^{\\infty} \\frac{1}{n^{2}}=\\frac{2 L^{2}}{(2 \\pi)^{2}} \\frac{1}{\\lfloor N / 2\\rfloor} \\\\\n& \\leq \\frac{L^{2}}{\\pi^{2}(N-2)}\n\\end{aligned}\n$$\n\nWe use (44) to get the following estimate on $\\|K-\\hat{K}\\|$ :\n\n$$\n\\|K-\\hat{K}\\|_{2} \\leq \\frac{L}{\\pi \\sqrt{(N-2)}}\n$$\n\nThus, it suffices for $N$ to satisfy the following inequality:\n\n$$\n\\frac{L}{\\pi \\sqrt{(N-2)}} \\leq \\epsilon \\Longrightarrow \\sqrt{N-2} \\geq \\frac{L}{\\pi \\epsilon} \\Longrightarrow N \\geq\\left(\\frac{L}{\\pi \\epsilon}\\right)^{2}+2\n$$\n\nWe now use the same argument as above to the fact that $K$ has order- $k$ bounded derivative. By iteration, we get:\n\n$$\n\\left|x_{n}^{s}\\right|=\\left|x_{n}^{c}\\right| \\leq\\left|\\frac{1}{(2 \\pi n)^{k}} \\int_{0}^{1} K^{(k)}(t) s_{n}(t) d t\\right| \\leq \\frac{1}{(2 \\pi n)^{k}} \\int_{0}^{1}\\left|K^{(k)}\\right|\\left|s_{n}(t)\\right| d t \\leq \\frac{L}{(2 \\pi n)^{k}}\n$$\n\nAgain, due to (43), this then gives us the following estimate on the square error:\n\n$$\n\\begin{aligned}\n\\|K-\\hat{K}\\|_{2}^{2} & =\\sum_{n=\\lfloor N / 2\\rfloor}^{\\infty} x_{n}^{c 2}(t)+x_{n}^{s 2}(t)=\\sum_{n=\\lfloor N / 2\\rfloor}^{\\infty}\\left|x_{n}^{c}\\right|^{2}(t)+\\left|x_{n}^{s}\\right|^{2}(t) \\\\\n& \\leq \\sum_{n=\\lfloor N / 2\\rfloor}^{\\infty} \\frac{2 L^{2}}{(2 \\pi n)^{2 k}}=\\frac{2 L^{2}}{(2 \\pi)^{2 k}} \\sum_{n=\\lfloor N / 2\\rfloor}^{\\infty} \\frac{1}{n^{2 k}}=\\frac{2 L^{2}}{(2 \\pi)^{2 k}} \\frac{1}{(\\lfloor N / 2\\rfloor)^{2 k-1}} \\\\\n& \\leq \\frac{L^{2}}{\\pi^{2 k}(N-2)^{2 k-1}}\n\\end{aligned}\n$$\n\nIf $K$ has order $k$-bounded derivatives, then we use (45) to get the following estimate on $\\|K-\\hat{K}\\|$ :\n\n$$\n\\|K-\\hat{K}\\|_{2} \\leq \\frac{L}{\\pi^{k}(N-2)^{-k+1 / 2}}\n$$\n\nAgain, it suffices for $N$ to satisfy the following inequality:\n\n$$\n\\frac{L}{\\pi^{k}(N-2)^{-k+1 / 2}} \\leq \\epsilon \\Longrightarrow(N-2)^{k-1 / 2} \\geq \\frac{L}{\\pi^{k} \\epsilon} \\Longrightarrow N \\geq\\left(\\frac{L}{\\pi^{k} \\epsilon}\\right)^{\\frac{2}{2 k-1}}+2\n$$\n\n## C.4.4 Delay Network\n\nFinally, we prove Theorem 9. Note that this is a stronger version of the LegT portion of Theorem 8, while the FouT portion is a corollary of the proof of Theorem 6. We start by working out some calculations concretely to provide an example. The SSM corresponding to HiPPO-LegT is\n\n$$\n\\begin{aligned}\n\\boldsymbol{A} & =\\boldsymbol{P}^{\\frac{1}{2}}\\left[\\begin{array}{cccc}\n-1 & 1 & -1 & 1 \\\\\n-1 & -1 & 1 & -1 \\\\\n-1 & -1 & -1 & 1 \\\\\n-1 & -1 & -1 & -1\n\\end{array}\\right] \\boldsymbol{P}^{\\frac{1}{2}} \\\\\n\\boldsymbol{B} & =\\boldsymbol{P}^{\\frac{1}{2}} \\mathbf{1} \\\\\n\\boldsymbol{C} & =\\boldsymbol{Z}^{\\top} \\boldsymbol{P}^{\\frac{1}{2}} \\\\\n\\boldsymbol{P} & =\\operatorname{diag}\\{1+2 n\\} \\\\\n\\boldsymbol{Z}^{\\top} & =\\left[\\begin{array}{llll}\n1 & -1 & 1 & -1\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nThe transfer function is\n\n$$\n\\boldsymbol{C}(s \\boldsymbol{I}-\\boldsymbol{A})^{-1} \\boldsymbol{B}=\\boldsymbol{Z}\\left(s \\boldsymbol{P}^{-1}-\\boldsymbol{A}\\right)^{-1} \\mathbf{1}\n$$\n\n(In the RHS and for the rest of this part, we will redefine $\\boldsymbol{A}$ to be the $\\pm 1$ matrix found above for convenience.)\n\nCase $\\mathrm{N}=1$. We have $\\boldsymbol{A}=-1, \\boldsymbol{B}=\\boldsymbol{C}=1$, and the transfer function is $\\boldsymbol{C}(s \\boldsymbol{I}-\\boldsymbol{A})^{-1} \\boldsymbol{B}=\\frac{1}{1+s}$. Case $\\mathbf{N}=\\mathbf{2}$. The transfer function is\n\n$$\n\\begin{aligned}\n\\boldsymbol{C}(s \\boldsymbol{I}-\\boldsymbol{A})^{-1} \\boldsymbol{B} & =\\left[\\begin{array}{ll}\n1 & -1\n\\end{array}\\right]\\left(s \\boldsymbol{P}^{-1}-\\left[\\begin{array}{cc}\n-1 & 1 \\\\\n-1 & -1\n\\end{array}\\right]\\right)^{-1}\\left[\\begin{array}{l}\n1 \\\\\n1\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{ll}\n1 & -1\n\\end{array}\\right]\\left[\\begin{array}{cc}\ns+1 & -1 \\\\\n1 & \\frac{s}{3}+1\n\\end{array}\\right]^{-1}\\left[\\begin{array}{l}\n1 \\\\\n1\n\\end{array}\\right] \\\\\n& =\\frac{1}{\\frac{s^{2}}{3}+\\frac{4 s}{3}+2}\\left[\\begin{array}{ll}\n1 & -1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1+\\frac{s}{3} & 1 \\\\\n-1 & 1+s\n\\end{array}\\right]\\left[\\begin{array}{l}\n1 \\\\\n1\n\\end{array}\\right] \\\\\n& =\\frac{2-\\frac{2 s}{3}}{\\frac{s^{2}}{3}+\\frac{4 s}{3}+2} \\\\\n& =\\frac{1-\\frac{s}{3}}{1+\\frac{2 s}{3}+\\frac{s^{2}}{6}}\n\\end{aligned}\n$$\n\nIt can be verified that this is indeed $[1 / 2]_{\\exp }(-s)$.",
    "httyh-35": "A General Recursion. We will now sketch out a method to relate these transfer functions recursively. We will redefine $\\boldsymbol{Z}$ to be the vector that ENDS in +1 . The main idea is to write\n\n$$\n\\begin{aligned}\n\\boldsymbol{A}_{n} & =\\left[\\begin{array}{cc}\n\\boldsymbol{A}_{n-1} & \\boldsymbol{Z}_{n-1} \\\\\n-\\mathbf{1}_{n-1}^{\\top} & -1\n\\end{array}\\right] \\\\\n\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n}\\right)^{-1} & =\\left[\\begin{array}{cc}\ns \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1} & -\\boldsymbol{Z}_{n-1} \\\\\n\\mathbf{1}_{n-1}^{\\top} & 1+\\frac{s}{2 n+1}\n\\end{array}\\right]^{-1}\n\\end{aligned}\n$$\n\nNow we can use the block matrix inversion formula. ${ }^{3}$ Ideally, this will produce a recurrence where the desired transfer function $\\boldsymbol{Z}_{n}\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n}\\right)^{-1} \\mathbf{1}_{n}$ will depend on $\\boldsymbol{Z}_{n-1}\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} \\mathbf{1}_{n-1}$. However, looking at the block matrix inversion formula, it becomes clear that there are also dependencies on terms like $\\mathbf{1}_{n-1}^{\\top}\\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} \\mathbf{1}_{n-1}$ and $\\boldsymbol{Z}_{n-1}\\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} \\boldsymbol{Z}_{n-1}^{\\top}$. The solution is to track all of these terms simultaneously. We will compute the 4 transfer functions\n\n$$\n\\begin{aligned}\n& \\boldsymbol{H}_{n}(s):=\\left[\\begin{array}{ll}\n\\boldsymbol{H}_{n}^{1 z}(s) & \\boldsymbol{H}_{n}^{11}(s) \\\\\n\\boldsymbol{H}_{n}^{z z}(s) & \\boldsymbol{H}_{n}^{z 1}(s)\n\\end{array}\\right] \\\\\n& :=\\left[\\begin{array}{ll}\n\\mathbf{1}_{n}^{\\top}\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n}\\right)^{-1} \\boldsymbol{Z}_{n} & \\mathbf{1}_{n}^{\\top}\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n}\\right)^{-1} \\mathbf{1}_{n} \\\\\n\\boldsymbol{Z}_{n}^{\\top}\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n}\\right)^{-1} \\boldsymbol{Z}_{n} & \\boldsymbol{Z}_{n}^{\\top}\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n}\\right)^{-1} \\mathbf{1}_{n}\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{l}\n\\mathbf{1}_{n}^{\\top} \\\\\n\\boldsymbol{Z}_{n}^{\\top}\n\\end{array}\\right]\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n}\\right)^{-1}\\left[\\begin{array}{ll}\n\\boldsymbol{Z}_{n} & \\mathbf{1}_{n}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n[^3]Lemma C.12. Instead of using the explicit block matrix inversion formula, it will be easier to work with the following factorization used to derive it (block LDU decomposition ${ }^{4}$ ). $$\n\\begin{aligned}\n{\\left[\\begin{array}{ll}\nA & B \\\\\nC & D\n\\end{array}\\right] } & =\\left[\\begin{array}{cc}\nI & 0 \\\\\nC A^{-1} & I\n\\end{array}\\right]\\left[\\begin{array}{cc}\nA & 0 \\\\\n0 & D-C A^{-1} B\n\\end{array}\\right]\\left[\\begin{array}{cc}\nI & A^{-1} B \\\\\n0 & I\n\\end{array}\\right] \\\\\n{\\left[\\begin{array}{ll}\nA & B \\\\\nC & D\n\\end{array}\\right]^{-1} } & =\\left[\\begin{array}{cc}\nI & -A^{-1} B \\\\\n0 & I\n\\end{array}\\right]\\left[\\begin{array}{cc}\nA^{-1} & 0 \\\\\n0 & \\left(D-C A^{-1} B\\right)^{-1}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nI & 0 \\\\\n-C A^{-1} & I\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nUsing Lemma C.12, we can factor the inverse as\n\n$$\n\\begin{aligned}\n\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n}\\right)^{-1}= & {\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-1} & \\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} \\boldsymbol{Z}_{n-1} \\\\\n1\n\\end{array}\\right] } \\\\\n& {\\left[\\begin{array}{cc}\n\\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} \\\\\n& \\left(1+\\frac{s}{2 n+1}+(-1)^{n-1} \\boldsymbol{H}_{n-1}^{1 z}(s)\\right)^{-1}\n\\end{array}\\right] } \\\\\n& {\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-1} \\\\\n-\\mathbf{1}_{n-1}^{\\top}\\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} & 1\n\\end{array}\\right] }\n\\end{aligned}\n$$\n\nNow we compute\n\n$$\n\\begin{aligned}\n& {\\left[\\begin{array}{c}\n\\mathbf{1}_{n}^{\\top} \\\\\n\\boldsymbol{Z}_{n}^{\\top}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-1} & \\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} \\boldsymbol{Z}_{n-1} \\\\\n1\n\\end{array}\\right]} \\\\\n& =\\left[\\begin{array}{cc}\n\\mathbf{1}_{n-1}^{\\top} & 1 \\\\\n-\\boldsymbol{Z}_{n-1}^{\\top} & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-1} & \\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} \\boldsymbol{Z}_{n-1} \\\\\n1\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n\\mathbf{1}_{n-1}^{\\top} & 1+\\boldsymbol{H}_{n-1}^{1 z}(s) \\\\\n-\\boldsymbol{Z}_{n-1}^{\\top} & 1-\\boldsymbol{H}_{n-1}^{z z}(s)\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nand\n\n$$\n\\begin{aligned}\n& {\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-1} & \\\\\n-\\mathbf{1}_{n-1}^{\\top}\\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} & 1\n\\end{array}\\right]\\left[\\begin{array}{ll}\n\\boldsymbol{Z}_{n} & \\mathbf{1}_{n}\n\\end{array}\\right]} \\\\\n& =\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-1} \\\\\n-\\mathbf{1}_{n-1}^{\\top}\\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n-\\boldsymbol{Z}_{n-1} & \\mathbf{1}_{n-1} \\\\\n1 & 1\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{cc}\n-\\boldsymbol{Z}_{n-1} & \\mathbf{1}_{n-1} \\\\\n1+\\boldsymbol{H}_{n-1}^{1 z}(s) & 1-\\boldsymbol{H}_{n-1}^{11}(s)\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n[^4]Now we can derive the full recurrence for all these functions. $$\n\\begin{aligned}\n& \\boldsymbol{H}_{n}(s)=\\left[\\begin{array}{ll}\n\\boldsymbol{H}_{n}^{1 z}(s) & \\boldsymbol{H}_{n}^{11}(s) \\\\\n\\boldsymbol{H}_{n}^{z z}(s) & \\boldsymbol{H}_{n}^{z 1}(s)\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{l}\n\\mathbf{1}_{n}^{\\top} \\\\\n\\boldsymbol{Z}_{n}^{\\top}\n\\end{array}\\right]\\left(s \\boldsymbol{P}_{n}^{-1}-\\boldsymbol{A}_{n}\\right)^{-1}\\left[\\begin{array}{ll}\n\\boldsymbol{Z}_{n} & \\mathbf{1}_{n}\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{l}\n\\mathbf{1}_{n}^{\\top} \\\\\n\\boldsymbol{Z}_{n}^{\\top}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-1} & \\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} \\boldsymbol{Z}_{n-1} \\\\\n1\n\\end{array}\\right] \\\\\n& {\\left[\\begin{array}{ll}\n\\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1} & \\\\\n& \\left(1+\\frac{s}{2 n+1}+(-1)^{n-1} \\boldsymbol{H}_{n-1}^{1 z}(s)\\right)^{-1}\n\\end{array}\\right]}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =\\left[\\begin{array}{cc}\n\\mathbf{1}_{n-1}^{\\top} & 1+\\boldsymbol{H}_{n-1}^{1 z}(s) \\\\\n-\\boldsymbol{Z}_{n-1}^{\\top} & 1-\\boldsymbol{H}_{n-1}^{z z}(s)\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& \\cdot\\left[\\begin{array}{cc}\n-\\boldsymbol{Z}_{n-1} & \\mathbf{1}_{n-1} \\\\\n1+\\boldsymbol{H}_{n-1}^{1 z}(s) & 1-\\boldsymbol{H}_{n-1}^{11}(s)\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{c}\n\\mathbf{1}_{n-1}^{\\top} \\\\\n-\\boldsymbol{Z}_{n-1}^{\\top}\n\\end{array}\\right]\\left(s \\boldsymbol{P}_{n-1}^{-1}-\\boldsymbol{A}_{n-1}\\right)^{-1}\\left[-\\boldsymbol{Z}_{n-1} \\quad \\mathbf{1}_{n-1}\\right] \\\\\n& +\\left[\\begin{array}{l}\n1+\\boldsymbol{H}_{n-1}^{1 z}(s) \\\\\n1-\\boldsymbol{H}_{n-1}^{z z}(s)\n\\end{array}\\right]\\left(1+\\frac{s}{2 n+1}+\\boldsymbol{H}_{n-1}^{1 z}(s)\\right)^{-1}\\left[1+\\boldsymbol{H}_{n-1}^{1 z}(s) \\quad 1-\\boldsymbol{H}_{n-1}^{11}(s)\\right] \\\\\n& =\\left[\\begin{array}{cc}\n-\\boldsymbol{H}_{n-1}^{1 z}(s) & \\boldsymbol{H}_{n-1}^{11}(s) \\\\\n\\boldsymbol{H}_{n-1}^{z z}(s) & -\\boldsymbol{H}_{n-1}^{z 1}(s)\n\\end{array}\\right] \\\\\n& +\\left[\\begin{array}{l}\n1+\\boldsymbol{H}_{n-1}^{1 z}(s) \\\\\n1-\\boldsymbol{H}_{n-1}^{z z}(s)\n\\end{array}\\right]\\left(1+\\frac{s}{2 n+1}+\\boldsymbol{H}_{n-1}^{1 z}(s)\\right)^{-1}\\left[1+\\boldsymbol{H}_{n-1}^{1 z}(s) \\quad 1-\\boldsymbol{H}_{n-1}^{11}(s)\\right]\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_37931de6e86a3ea35c37g-36.jpg?height=109&width=641&top_left_y=715&top_left_x=517)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_37931de6e86a3ea35c37g-36.jpg?height=134&width=787&top_left_y=909&top_left_x=536)\n\nNow we'll define a few transformations which will simplfy the calculations. Define\n\n$$\n\\begin{aligned}\n& G_{n}^{1 z}(s)=\\frac{1}{2}\\left(1+\\boldsymbol{H}_{n}^{1 z}(s)\\right) \\\\\n& G_{n}^{11}(s)=1-\\boldsymbol{H}_{n}^{1 z}(s) \\\\\n& G_{n}^{z z}(s)=1-\\boldsymbol{H}_{n}^{1 z}(s) \\\\\n& G_{n}^{z 1}(s)=(-1)^{n} \\boldsymbol{H}_{n}^{z 1}(s)\n\\end{aligned}\n$$\n\nThese satisfy the following recurrences:\n\n$$\n\\begin{aligned}\nG_{n}^{1 z}(s) & =1-G_{n-1}^{1 z}(s)+\\frac{G_{n-1}^{1 z}(s) G_{n-1}^{1 z}(s)}{G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}} \\\\\nG_{n}^{11}(s) & =G_{n-1}^{11}(s)-\\frac{G_{n-1}^{11}(s) G_{n-1}^{1 z}(s)}{G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}} \\\\\nG_{n}^{z z}(s) & =G_{n-1}^{z z}(s)-\\frac{G_{n-1}^{z z}(s) G_{n-1}^{1 z}(s)}{G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}} \\\\\nG_{n}^{z 1}(s) & =G_{n-1}^{z 1}(s)-(-1)^{n-1} \\frac{G_{n-1}^{11}(s) G_{n-1}^{z z}(s)}{G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}}\n\\end{aligned}\n$$\n\nWe can analyze each term separately. ## Case $G_{n}^{1 z}(s)$. This will be the most important term, as it determines the denominator of the expressions. Simplifying the recurrence slightly gives\n\n$$\n\\begin{aligned}\nG_{n}^{1 z}(s) & =1-G_{n-1}^{1 z}(s)+\\frac{G_{n-1}^{1 z}(s) G_{n-1}^{1 z}(s)}{G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}} \\\\\n& =\\frac{\\left(G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}\\right)-\\frac{s}{2(2 n+1)} \\cdot G_{n-1}^{1 z}(s)}{G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}}\n\\end{aligned}\n$$\n\nNow let $G_{n}^{1 z}(s)=\\frac{P_{n}^{1 z}(s)}{Q_{n}^{1 z}(s)}$ where $P, Q$ are polynomials. Clearing the denominator $Q$ yields\n\n$$\n\\frac{P_{n}^{1 z}(s)}{Q_{n}^{1 z}(s)}=\\frac{\\left(P_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)} Q_{n-1}^{1 z}(s)\\right)-\\frac{s}{2(2 n+1)} \\cdot P_{n-1}^{1 z}(s)}{P_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)} \\cdot Q_{n-1}^{1 z}(s)}\n$$\n\nThis results in the recurrence\n\n$$\n\\begin{aligned}\n& Q_{n}^{1 z}(s)=P_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)} \\cdot Q_{n-1}^{1 z}(s) \\\\\n& P_{n}^{1 z}(s)=Q_{n-1}^{1 z}(s)-\\frac{s}{2(2 n+1)} \\cdot P_{n-1}^{1 z}(s)\n\\end{aligned}\n$$\n\nBut this is exactly the fundamental recurrence formula for continuants of the continued fraction\n\n$$\n\\begin{aligned}\n& e^{s}=1+\\frac{s}{1-\\frac{\\frac{1}{2} s}{1+\\frac{\\frac{1}{6} s}{1-\\frac{\\frac{1}{6} s}{1+\\frac{\\frac{1}{10} s}{1-\\frac{1}{10} s}}}}} \\\\\n& { }_{1+} \\cdot\n\\end{aligned}\n$$\n\nTherefore $Q_{n-1}^{1 z}(s)$ are the denominators of the Pade approximants. Note that by definition of $P, Q$,\n\n$$\nG_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}=\\frac{P_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)} \\cdot Q_{n-1}^{1 z}(s)}{Q_{n-1}^{1 z}(s)}=\\frac{Q_{n}^{1 z}(s)}{Q_{n-1}^{1 z}(s)}\n$$\n\nGoing forward we will also suppress the superscript of $Q, Q_{n-1}(s):=Q_{n-1}^{1 z}(s)$, as it will be evident that all terms have the same denominator $Q_{n}(s)$\nCase $G_{n}^{11}(s)$. First note that $G_{n}^{11}(s)=G_{n}^{z z}(s)$ is straightforward from the fact that their recurrences are identical. The recurrence is\n\n$$\n\\begin{aligned}\nG_{n}^{11}(s) & =G_{n-1}^{11}(s)-\\frac{G_{n-1}^{11}(s) G_{n-1}^{1 z}(s)}{G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}} \\\\\n& =\\frac{\\frac{s}{2(2 n+1)} \\cdot G_{n-1}^{11}(s)}{G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}} \\\\\n& =\\frac{\\frac{s}{2(2 n+1)} \\cdot G_{n-1}^{11}(s) Q_{n-1}(s)}{P_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)} \\cdot Q_{n-1}(s)} \\\\\n& =\\frac{\\frac{s}{2(2 n+1)} \\cdot G_{n-1}^{11}(s) Q_{n-1}(s)}{Q_{n}(s)}\n\\end{aligned}\n$$\n\nTherefore\n\n$$\n\\begin{aligned}\nG_{n}^{11}(s) Q_{n}(s) & =\\frac{s}{2(2 n+1)} \\cdot G_{n-1}^{11}(s) Q_{n-1}(s)=\\prod_{i=1}^{n} \\frac{s}{2(2 i+1)} \\\\\nG_{n}^{11}(s) & =\\frac{\\prod_{i=1}^{n} \\frac{s}{2(2 i+1)}}{Q_{n}(s)}\n\\end{aligned}\n$$\n\n## Case $G_{n}^{z 1}(s)$. Define\n\n$$\nG_{n}^{z 1}(s)=\\frac{P_{n}^{z 1}(s)}{Q_{n}(s)}\n$$\n\nThis term satisfies the formula\n\n$$\n\\begin{aligned}\nG_{n}^{z 1}(s) & =G_{n-1}^{z 1}(s)-(-1)^{n-1} \\frac{G_{n-1}^{11}(s) G_{n-1}^{z z}(s)}{G_{n-1}^{1 z}(s)+\\frac{s}{2(2 n+1)}} \\\\\n& =\\frac{P_{n-1}^{z 1}(s)}{Q_{n-1}(s)}-(-1)^{n-1} \\frac{\\left(\\prod_{i=0}^{n-1} \\frac{s}{2(2 i+1)}\\right)^{2} / Q_{n-1}(s)^{2}}{\\frac{Q_{n}(s)}{Q_{n-1}(s)}} \\\\\n& =\\frac{P_{n-1}^{z 1}(s) Q_{n}(s)}{Q_{n-1}(s) Q_{n}(s)}-(-1)^{n-1} \\frac{\\left(\\prod_{i=0}^{n-1} \\frac{s}{2(2 i+1)}\\right)^{2}}{Q_{n}(s) Q_{n-1}(s)}\n\\end{aligned}\n$$\n\nBy definition of $P^{z 1}$,\n\n$$\nP_{n-1}^{z 1}(s) Q_{n}(s)-(-1)^{n-1}\\left(\\prod_{i=0}^{n-1} \\frac{s}{2(2 i+1)}\\right)^{2}=P_{n}^{z 1}(s) Q_{n-1}(s)\n$$\n\nBut note that this is exactly satisfied by the Pad\u00e9 approximants, by the determinantal formula of continued fractions. This shows that $G_{n-1}^{1 z}(s)$ are the Pad\u00e9 approximants of $e^{-s}$, as desired. ## C. 5 Normalization and Timescales\n\nProposition 13 (Closure properties of TOSSMs). Consider a TOSSM $(\\boldsymbol{A}, \\boldsymbol{B})$ for basis functions $p_{n}(t)$ and measure $\\omega(t)$. Then, the following are also TOSSMs with the corresponding basis functions and measure:\n\n1. Constant scaling changes the timescale: $(c \\boldsymbol{A}, c \\boldsymbol{B})$ is a TOSSM with basis $p(c t)$ and measure $\\omega(c t) c$. 2. Identity shift tilts by exponential: $(\\boldsymbol{A}+c \\boldsymbol{I}, \\boldsymbol{B})$ is a TOSSM with basis $p(t) e^{-c t}$ and measure $\\omega(t) e^{2 c t}$. 3. Unitary change of basis preserves measure: $\\left(\\boldsymbol{V} \\boldsymbol{A} \\boldsymbol{V}^{*}, \\boldsymbol{V B}\\right)$ is a TOSSM with basis $\\boldsymbol{V} p(t)$ and measure $\\omega(t)$. Proof. We define $p(t)$ to be the vector of basis functions for the $\\operatorname{OSSM}(\\boldsymbol{A}, \\boldsymbol{B})$,\n\n$$\np(t)=\\left[\\begin{array}{c}\np_{0}(t) \\\\\n\\vdots \\\\\np_{N-1}(t)\n\\end{array}\\right]\n$$\n\nRecall that the SSM kernels are $K_{n}(t)=p_{n}(t) \\omega(t)$ so that $p(t) \\omega(t)=e^{t \\boldsymbol{A}} \\boldsymbol{B}$. 1. The SSM kernels are\n\n$$\ne^{t(c \\boldsymbol{A})}(c \\boldsymbol{B})=c e^{(c t) \\boldsymbol{A})} \\boldsymbol{B}=c p(c t) \\omega(c t)\n$$\n\nIt remains to show that the $p_{n}(c t)$ are orthonormal with respect to measure $c \\omega(c t)$ :\n\n$$\n\\int p_{j}(c t) p_{k}(c t) \\omega(c t) c=\\delta_{j k}\n$$\n\nwhich follows immediately from the change of variables formula. 2. Using the commutativity of $\\boldsymbol{A}$ and $\\boldsymbol{I}$, the SSM kernels are\n\n$$\ne^{t(\\boldsymbol{A}+c \\boldsymbol{I})} \\boldsymbol{B}=e^{t \\boldsymbol{A}} e^{c t \\boldsymbol{I}} \\boldsymbol{B}=e^{c t} p(t) \\omega(t)\n$$\n\nIt remains to show that $p_{n}(t) e^{-c t}$ are orthonormal with respect to measure $\\omega(t) e^{2 c t}$ :\n\n$$\n\\int p_{j}(t) e^{-c t} p_{k}(t) e^{-c t} \\omega(t) e^{2 c t}=\\int p_{j}(t) p_{k}(t) \\omega(t)=\\delta_{j k}\n$$\n\n3. The SSM basis is\n\n$$\ne^{t \\boldsymbol{V} \\boldsymbol{A} \\boldsymbol{V}^{*}} \\boldsymbol{V} \\boldsymbol{B}=\\boldsymbol{V} e^{t \\boldsymbol{A}} \\boldsymbol{B}=\\boldsymbol{V} p(t) \\omega(t)\n$$\n\nIt remains to show that the basis functions $\\boldsymbol{V} p(t)$ are orthonormal with respect to $\\omega(t)$. Note that orthonormality of a set of basis functions can be expressed as $\\int p(t) \\omega(t) p(t)^{\\top}=\\boldsymbol{I}$, so that\n\n$$\n\\begin{aligned}\n\\int(\\boldsymbol{V} p(t)) \\omega(t)(\\boldsymbol{V} p(t))^{*} & =\\boldsymbol{V}\\left[\\int p(t) \\omega(t) p(t)^{\\top}\\right] \\boldsymbol{V}^{*} \\\\\n& =\\boldsymbol{I}\n\\end{aligned}\n$$\n\n\n[^0]:    *Equal contribution.",
    "httyh-36": "[^1]:    ${ }^{1}$ Note that this is 0-indexed. [^2]:    ${ }^{2}$ Recall that, like the coefficients, the matrices are 0-indexed. [^3]:    ${ }^{3}$ https://en.wikipedia.org/wiki/Block_matrix\\#/Block_matrix_inversion\n\n[^4]:    ${ }^{4}$ https://en.wikipedia.org/wiki/Schur_complement\\#/Background\n\n"
}