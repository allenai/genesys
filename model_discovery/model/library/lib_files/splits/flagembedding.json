{
    "flagembedding-0": "# Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon \n\nPeitian Zhang ${ }^{1,2 *}$ Zheng Liu $^{1 \\dagger}$ Shitao Xiao $^{1}$ Ninglu Shao ${ }^{1,2}$ Qiwei Ye ${ }^{1}$ Zhicheng Dou ${ }^{2}$<br>1: Beijing Academy of Artificial Intelligence,<br>2: Gaoling School of Artificial Intelligence, Renmin University of China<br>\\{namespace.pt, zhengliu1026\\}@gmail.com\n\n\n#### Abstract\n\nThe utilization of long contexts poses a big challenge for LLMs due to their limited context window size.",
    "flagembedding-1": "Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small training cost. Our experiment verifies Activation Beacon's effectiveness of context extension: it can remarkably accomplish highquality extension of Llama-2-7B's context by $\\times 100$ times (from 4 K to 400 K ); meanwhile, it can also achieve superior performances across a variety of longcontext language modeling and understanding tasks. The source code and model checkpoint are available at https://github.com/FlagOpen/FlagEmbedding\n\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d51bda31a030ab1a9287g-01.jpg?height=403&width=1378&top_left_y=1734&top_left_x=364)\n\nFigure 1: Comparison of the sliding window perplexity [22] between Activation Beacon and other context extension methods, including 1) Position Interpolation [5], 2) NTK-Aware Scaled RoPE [1], 3) LongLlama [32]. Activation Beacon leads to better long-context generation quality with higher running efficiency (memory, time). [^0]![](https://cdn.mathpix.com/cropped/2024_09_12_d51bda31a030ab1a9287g-02.jpg?height=353&width=1409&top_left_y=231&top_left_x=356)\n\nFigure 2: (A) The beacon token ( $\\langle\\mathrm{bcn}\\rangle$ ) is appended to a context, which prompts the LLM to condense the raw activations into more compact forms; (B) The condensed activations are streamingly processed with the sliding window for auto-regression (AR). ## 1 Introduction\n\nLarge language models (LLMs) need to process long contexts to accomplish many important tasks, such as retrieval-augmented generation and in-context learning. However, existing LLMs are typically constrained by fixed context windows, e.g., 2 K for Llama-1 [29] and 4 K for Llama-2 [30], which is not enough to handle some real-world scenarios. Although LLMs can be fine-tuned or retrained to extend their context windows [16, 6; 5, 28, 20; 32; 18], it will result in considerable costs at both training and inference time due to the quadratic computing complexity of self attention. Besides, the continued training on long-sequence data may compromise the LLM's general capability in shorter contexts, which is unfavorable to their practical usage. In light of these challenges, it is desirable to explore new mechanisms, which can not only realize the cost-effective extension of context length, but also be compatible with the LLM's existing capabilities. In this work, we propose Activation Beacon (shown as Figure 2) as a new method for LLM's context extension. It condenses the LLM's raw activations (i.e. keys and values from the self-attention module) into highly compact forms such that the LLM can perceive the information from a vast scope of context even with a limited context window. The above idea shares the common philosophy as sparse attention [3, 8, 38] and context compression [4, 7, 19, 22, 14]. However, it enjoys substantial advantages over the previous methods in many perspectives, including the effectiveness of context extension (especially the quality of long-context generation and the flexibility of supporting diverse context lengths), inference and training efficiency, and the compatibility with the existing LLMs, thanks to a series of crucial technical designs. Instead of developing a new model from scratch, we argue that the LLM itself can work as a proficient activation condenser with proper adaptation given its strong and well-established context representation capability. Based on this argument, we introduce a simple but effective model architecture and running mechanism to facilitate the production and utilization of condensed activations. Particularly, we introduce special tokens, known as beacon tokens ( $\\langle\\mathrm{bcn}\\rangle$ ), which prompt the LLM to condense the contextual information into beacon tokens's activations (Figure 2). For a context of length $l$, a team of $k(k<l)$ beacon tokens are dispatched to the end of it, which leads to a condensing ratio of $\\alpha(\\alpha=l / k)$. We maintain another copy of the LLM's self-attention parameters, including $\\left\\{W_{Q}^{b}, W_{K}^{b}, W_{V}^{b}, W_{O}^{b}\\right\\}$. These new parameters are specialized to learn the activation condensing, while the original parameters in the LLM are fixed. Thus, Activation Beacon serves as a plug-in component for the LLM, introducing extended contextual information to the LLM without adversely affecting its existing capabilities in short contexts.",
    "flagembedding-2": "To efficiently handle long contexts, we propose stream processing with the sliding window. The long context is partitioned into multiple intervals of length $l$. A sliding window is employed to sequentially process one interval at a time. When dealing with the next interval, the raw activations of the previous interval are discarded while its condensed activations are accumulated. Therefore, the sliding window is formulated as $\\left[\\langle\\mathrm{bcn}\\rangle_{1}, \\ldots,\\langle\\mathrm{bcn}\\rangle_{m}, x_{m+1}, \\ldots, x_{n}\\right]$ where $\\langle\\mathrm{bcn}\\rangle_{*}$ stands for the beacon tokens from previous intervals and $x_{*}$ is normal tokens in the current interval. The size of the sliding window is upper-bounded by the maximum window size of the LLM, e.g. 4 K for Llama-2, which maintains a low memory consumption and a linear time complexity. Meanwhile, it also accumulatively gathers rich contextual information from the past $(\\alpha-1) \\times m+n$ tokens. The condensed activations are expected to fully encode the information within the raw activations, thereby assisting the LLM to accomplish high-quality generation of new content. With this consideration, we propose to learn Activation Beacon through the auto-regression task. In the sliding window, the generation likelihood of the normal token $x_{i}$ is maximized based on the beacon tokens and its preceding normal tokens, i.e., $\\max p\\left(x_{i} \\mid\\langle\\mathrm{bcn}\\rangle_{1}, \\ldots,\\langle\\mathrm{bcn}\\rangle_{m}, x_{m+1} \\ldots, x_{i-1}\\right)$. Considering that a dramatic extension of context calls for a large condensing ratio, while a moderate extension just needs a small condensing ratio, we perform a random sampling of $\\alpha$ during the stream processing. Consequently, the generation can be conditioned on a mixture of condensed activations with diversified condensing ratios, which substantially contributes to the Activation Beacon's generalization in handling the extension of different context lengths. Activation Beacon is applied to Llama-2-7B (chat), whose original context length is 4 K . The training data is sampled from RedPajama [10] and LongAlpaca [6], whose length are all less than 8 K . The training process merely takes 10 K steps, which can be accomplished within 9 hours on an $8 \\times \\mathrm{A} 800$ GPU machine. Notably, it leads to a superior quality of language modeling on the extended context lengths, like $8 \\mathrm{~K}, 16 \\mathrm{~K}$, and 32 K , whose result is even better than the fine-tuned full-attention baselines. It is equally competitive on long-context understanding tasks, such as question answering and fewshot learning. Activation Beacon also shows the potential to establish super long contexts: by learning to support the condensing factor of 128 , the context length of Llama-2 can be remarkably extended to 400K (Figure 1). As a compatible module, Activation Beacon can also work with other techniques, like position interpolation ( $\\S$ C) and retrieval ( $\\S$ D) for even longer and better context extension effect. To summarize, we propose Activation Beacon, which realizes dramatic extension of LLM's context based on the high-quality condensing of LLM's activations. It also enjoys a high running efficiency, a high compatibility with the existing LLM, and a small cost of training thanks to its optimized designs on architecture and running mechanism. In our experiment, the effectiveness of Activation Beacon is verified given its superior performances across a wide range of long-context processing tasks. ## 2 Activation Beacon\n\n### 2.1 Overview\n\nThe LLM exploits the contextual information while predicting the new content. The contextual information is represented by the activations, particularly the keys and values in the self-attention module. With a fixed size of context window $L$, a typical LLM can only query the recent $L$ activations for contextual information. However, we argue that the window size should simply be the upper bound of input units rather than context length. By condensing more information into each activation, i.e. the information from a larger scope rather a single token, the LLM will be able to perceive a longer context with its original context window. ### 2.2 Activation Condensing\n\nWe aim to adapt the LLM itself for activation condensing given its strong context representation capability. Particularly, we employ special tokens, called beacon tokens, which prompt the LLM to condense the contextual information into their activations. We also maintain another copy of the LLM's MHA (multi-head self-attention) parameters, denoted as MHA ${ }^{b}$, including the layerwise projection matrices for queries, keys, values, and outputs $\\left\\{\\boldsymbol{W}_{Q}^{b}, \\boldsymbol{W}_{K}^{b}, \\boldsymbol{W}_{V}^{b}, \\boldsymbol{W}_{O}^{b}\\right\\}$. These parameters are specifically learned for condensing the activations. Besides, they are lightweight, merely accounting for $1 / 3$ of the LLM's original parameters (e.g., 2B with the LLaMA-2 7B model). The activation condensing is performed with the following operations (Figure 3I). For the context of length $l, k$ beacon tokens are appended to the end of it. The LLM auto-regressively encodes the context as well as the beacon tokens, as a result, the raw activations of regular tokens are generated and then condensed into the beacon tokens' activations. Formally, let the input features of the beacon tokens as $\\boldsymbol{H}^{b} \\in \\mathbb{R}^{k \\times D}$, the projections for the beacon tokens' queries, keys, and values are performed in the first place:\n\n$$\n\\boldsymbol{Q}^{b} \\leftarrow \\boldsymbol{W}_{Q}^{b} \\boldsymbol{H}^{b}, \\quad \\boldsymbol{K}^{b} \\leftarrow \\boldsymbol{W}_{K}^{b} \\boldsymbol{H}^{b}, \\quad \\boldsymbol{V}^{b} \\leftarrow \\boldsymbol{W}_{V}^{b} \\boldsymbol{H}^{b}\n$$\n\nThen, the projection results query the keys $\\left(\\boldsymbol{K}^{r} \\in \\mathbb{R}^{l \\times D}\\right)$ and values $\\left(\\boldsymbol{V}^{r} \\in \\mathbb{R}^{l \\times D}\\right)$ of the raw activations from normal tokens to generate the condensed activations, leading to a condensing ratio\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d51bda31a030ab1a9287g-04.jpg?height=788&width=1172&top_left_y=294&top_left_x=482)\n\nFigure 3: (I) The raw activations of ordinal tokens (the blue square) are condensed into the compact activations of beacon tokens (the green squere). Future tokens are auto-regressively generated conditioned on the raw activations in the current interval and the condensed activations accumulated from previous intervals. (II) The attention schemes for activation condensing. $$\n\\alpha=l / k:\n$$\n\n$$\n\\begin{aligned}\n& \\boldsymbol{A} \\leftarrow \\operatorname{softmax}\\left(\\operatorname{mask}\\left(\\frac{\\boldsymbol{Q}^{b}\\left\\{\\boldsymbol{K}^{r} \\oplus \\boldsymbol{K}^{b}\\right\\}^{T}}{\\sqrt{D}}\\right)\\right), \\\\\n& \\boldsymbol{O}^{b} \\leftarrow \\boldsymbol{W}_{O}^{b} \\boldsymbol{A}\\left\\{\\boldsymbol{V}^{r} \\oplus \\boldsymbol{V}^{b}\\right\\} . \\end{aligned}\n$$\n\nThe final output of self-attention is produced by the concatenation of both raw activations from the normal tokens and the condensed activations from the beacon tokens. To optimize the quality of activation condensing, we explore three attention schemes for the beacon tokens, i.e. the mask $(\\cdot)$ operator, which are as shown in Figure 3II. 1) Segmentation, where each beacon can attend to an equally segmented span of the context. 2) Stepwise expansion, where each beacon can attend to one more span than its predecessor, and the last beacon can attend to the entire context. 3) Full coverage, where the entire context can be attended by all beacons. For all three options, we restrict the context length $l$ to be evenly divisible by the number of beacon tokens $k$. Besides, the beacon tokens are always positioned next to the last normal token it can attend to. Although the three options are of the same computation cost, it's empirically found that the second option, i.e. the stepwise expansion, leads to the optimal performance ( $\\$ 5$. ### 2.3 Stream Processing\n\nThe long context is partitioned into multiple intervals of length $l$. A sliding window is employed to sequentially process one interval at a time. When dealing with the next interval, the raw activations of the previous interval are discarded while its condensed activations are accumulated. Therefore, the sliding window consists of $m$ beacon tokens (i.e. $\\langle\\mathrm{bcn}\\rangle$ ) from the past intervals, and the normal tokens in the current interval. With the above formulation, the next token is predicted as:\n\n$$\np\\left(x_{n} \\mid\\langle\\mathrm{bcn}\\rangle_{1}, \\ldots,\\langle\\mathrm{bcn}\\rangle_{m}, x_{m+1}, \\ldots, x_{n-1} ; \\Theta, \\Theta^{b}\\right)\n$$\n\nwhere $\\Theta$ denotes the parameters of the LLM and $\\Theta^{b}$ denotes the introduced parameters for beacons. Crucially, both $\\langle\\mathrm{bcn}\\rangle_{*}$ and $x_{*}$, are encoded by their relative positions within the sliding window, regardless of their absolute positions in the entire context. The size of the sliding window is upbounded by the context window size of the LLM, which results in a competitive running efficiency for\nboth training and inference. Different from the typical stream processing where the context beyond the sliding window is discarded [36], our method can accumulatively cover the information from the past $(\\alpha-1) \\times m+n$ tokens. Note that the above working mechanism may also benefit from the increasing of window size, as more beacon tokens can be accumulated in the sliding window to cover an even longer context. Consequently, Activation Beacon can work with strategies like NTK [1], PI [5] for further extension of the context. Detailed collaboration effect is explored in Appendix C. ### 2.4 Learning Method\n\nPlug-in to LLM. As introduced, Activation Beacon introduces the following parameters $\\left(\\Theta_{b}\\right)$ : 1) the beacon token's embedding $\\left.e_{\\langle b c n\\rangle}, 2\\right)$ the linear projection matrices for $\\mathrm{MHA}^{b}$ : $\\left\\{\\boldsymbol{W}_{Q}^{b}, \\boldsymbol{W}_{K}^{b}, \\boldsymbol{W}_{V}^{b}, \\boldsymbol{W}_{O}^{b}\\right\\}$ in each transformer layer. Overall, it accounts for less than $1 / 3$ of the LLM's original size, e.g., 2B with the Llama-2-7B model. Activation Beacon reuses other transformer modules from the LLM (i.e., MLP and LayerNorm). This turns out to be the optimal trade-off between effectiveness and training cost. Activation Beacon is learned while all of the LLM's original parameters are frozen. Besides, it is only used to generate the condensed activations without interfering the inference process of normal tokens. Therefore, it serves as a plug-in module for the LLM, which introduces the long contextual information without affecting the LLM's existing capabilities in processing short contexts. Auto-Regression. We train Activation Beacon by auto-regression, where the next token is predicted based on the condensed activations from the beacon tokens and the raw activations from the ordinary tokens. As mentioned in $\\$ 2.2$ a training instance is partitioned into equal-sized intervals of length $l$ and streamingly processed. Afterwards, the following loss is minimized:\n\n$$\n\\min _{\\Theta_{b}} \\cdot \\sum_{j=1}^{\\lceil|X| / / l\\rceil} \\sum_{i=1}^{l}-\\log p\\left(x_{i}^{j} \\mid\\langle\\mathbf{b c n}\\rangle_{1}, \\ldots,\\langle\\mathbf{b c n}\\rangle_{m_{j}}, x_{1}^{j}, \\ldots, x_{i-1}^{j} ; \\Theta, \\Theta^{b}\\right)\n$$\n\nwhere $x_{i}^{j}$ is the $i$-th token in the $j$-th interval of $X, m_{j}$ stands for the number of beacon tokens accumulated before the $j$-th interval, whose value depends on the condensing ratio of each preceding interval $\\left(m_{j}=\\sum_{z=1}^{j-1}\\left(l / / \\alpha_{z}\\right)\\right.$ ). Step-wise randomized condensing ratio. The training is performed purely with short-sequence data, i.e. $1024<|X|<8192$, where the majority of training samples are less than 4 K (Table 6). Therefore, we are able to achieve superior training efficiency. To generalize Activation Beacon to support different context lengths, e.g., $16 \\mathrm{~K}, 32 \\mathrm{~K}, 100 \\mathrm{~K}$, and even longer, the auto-regression needs to be conditioned on different amounts of beacon tokens with diversified condensing ratios. For this purpose, we randomly sample the condensing ratio for each interval within a large candidate scope: $\\alpha_{j} \\sim\\{2,4,8, \\ldots 128\\}$, which will introduce dramatic diversity to the condensing ratios and amount of beacon tokens within the auto-regression process. ## 3 Experiment\n\nOur experiments are performed for the exploration of the following issues. 1) Activation Beacon's impact on the long-context generation capabilities (measured by Perplexity). 2) Activation Beacon's impact on the long-context utilization capability (reflected by tasks like long document QA and summarization). 3) Activation Beacon's impact on efficiency in terms of GPU memory and inference time.",
    "flagembedding-3": "4) The individual contribution of different technical factors. ### 3.1 Settings\n\nImplementation. Our method is applied to Llama-2-7B (chat) [30] for empirical studies. Our training data is a mixture of 80 K sampled data from RedPajama [10] and LongAlpaca [6] (70K from RedPajama and 10K from LongAlpaca, respectively). The sequence length of each sample is between 1024 and 8192. The statistics of our training data is reported in Table 6 We use a single $8 \\times$ A800 GPU machine for training. The training is performed for 10,000 steps (one epoch of the whole training data) with a batch size of 8 and a learning rate of $5 \\mathrm{e}-5$ using the linear scheduler. The length of the context interval is set to 1024 . The condensing ratio is sampled from $\\{2,4,8,16,32,64,128\\}$ during training. As introduced, Llama's own parameters are freezed throughout the training process. Table 1: Sliding window perplexity of different context window extension methods on PG19, ProofPile, and CodeParrot. Activation Beacon successfully extends the context window of Llama-2-7B model to sequences much longer than the ones seen during training. | Method | PG19 |  |  |  | Proof-Pile |  |  |  | CodeParrot |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 4K | 16 K | 32 K | 100K | 4K | 16 K | 32 K | 100K | 4K | 16 K | 32 K | 100K |\n| Llama-2-7B | 9.21 | $>10^{3}$ | $>10^{3}$ | OOM | 3.47 | $>10^{3}$ | $>10^{3}$ | OOM | 2.55 | $>10^{3}$ | $>10^{3}$ | OOM |\n| PI | 9.21 | 19.5 | $>10^{2}$ | OOM | 3.47 | 5.94 | 33.7 | OOM | 2.55 | 4.57 | 29.33 | OOM |\n| NTK | 9.21 | 11.5 | 37.8 | OOM | 3.47 | 3.65 | 7.67 | OOM | 2.55 | 2.86 | 7.68 | OOM |\n| StreamingLLM | 9.21 | 9.25 | 9.24 | 9.32 | 3.47 | 3.51 | 3.50 | 3.55 | 2.55 | 2.60 | 2.54 | 2.56 |\n| AutoCompre.-6K | 11.8 | $>10^{2}$ | $>10^{3}$ | OOM | 4.55 | $>10^{2}$ | $>10^{3}$ | OOM | 5.43 | $>10^{2}$ | $>10^{3}$ | OOM |\n| YaRN-128K | 6.68 | 6.44 | 6.38 | OOM | 2.70 | 2.47 | 2.41 | OOM | 2.17 | 2.04 | 2.00 | OOM |\n| LongChat-32K | 9.47 | 8.85 | 8.81 | OOM | 3.07 | 2.70 | 2.65 | OOM | 2.36 | 2.16 | 2.13 | OOM |\n| LongAlpaca-16K | 9.96 | 9.83 | $>10^{2}$ | OOM | 3.82 | 3.37 | $>10^{3}$ | OOM | 2.81 | 2.54 | $>10^{3}$ | OOM |\n| LongLlama | 9.06 | 8.83 | OOM | OOM | 2.61 | 2.41 | OOM | OOM | 1.95 | 1.90 | OOM | OOM |\n| Activation Beacon | 9.21 | 8.34 | 8.27 | 8.50 | 3.47 | 3.34 | 3.32 | 3.31 | 2.55 | 2.43 | 2.41 | 2.62 |\n\nBaselines. The following types of baselines are chosen for comparison (all based on the LLaMA-2-7B (chat) model unless otherwise specified). 1) The basic method, i.e. LLaMA-2-7B (chat) [29] with 4K context length. 2) The fine-tuning free methods, including Positional Interpolation (PI) [5], the NTKAware Scale ROPE (NTK) [1], and StreamingLLM [36]. 3) The fine-tuned full-attention methods, including LongChat-32K [16], LongAlpaca-16K [6], YaRN-128K [20]. 4) The fine-tuned methods with adapted architectures for long contexts, including AutoCompressor-6K [7] and LongLlama [32] (based on CodeLlama [24]). We enable FlashAttention-2 [11] to accelerate self-attention computation and save GPU usage for all the baselines. At present, Activation Beacon is incompatible with FlashAttention-2 due to its utilization of the customized attention scheme; thus, we use the scaled dot product attention (sdpa) from PyTorch [17] for acceleration. ### 3.2 Main Results\n\n### 3.2.1 Long-Context Language Modeling\n\nThe experiment on long-context language modeling is performed with three datasets: PG19 [22], Proof-Pile [40], and CodeParrot [31]. Specifically, for PG19, we use its entire test set with 100 books. For Proof-Pile, we extract the arxiv papers from the test set that are longer than 32 K , which are 79 papers in total. For CodeParrot, there is no pre-defined test set. Following previous studies [25, 39], we first concatenate code from the same repository to form long sequences, then we sample 100 sequences for evaluation. The perplexity is computed with a sliding window of size 2 K [21]. The evaluation results are reported in Table 1 , where Activation Beacon leads to a superior longcontext language modeling performance. First of all, it not only outperforms the Llama-2-7B baseline but also results in a notably improved performance than the fine-tuning free methods. It is worth noting that with the extension of context from 4 K to 32 K , the language modeling performance can be gradually improved by Activation Beacon, indicating that the expanded information from the longer context can be effectively utilized to facilitate the generation. By comparison, the language modeling performance is decreased with other fine-tuning-free methods. Most of them become ineffective after the context length goes beyond 32 K . Secondly, Activation Beacon's performance is comparable to or even better than the fine-tuned full-attention methods. This result is remarkable knowing that Activation Beacon runs with a much higher efficiency (to be analyzed in Section 3.3). Although there are cases where some of the fine-tuned full-attention baselines achieve better performances, their empirical advantages may not be fully resulted from the introduction of long contextual information. For example, YaRN-128K's performance has already been notably higher than Llama-2-7B at the context length of 4 K , and so is the case with LongChat-32K on Proof-Pile and CodeParrot. Note that the update of the LLM's original parameters is not always favorable because it may not be well generalized to many other scenarios. By comparison, our method is simply a plug-in module to introduce long contextual information without affecting the LLM's existing capabilities. Thirdly, Activation Beacon is able to achieve a much longer extension of the context than the rest of the methods. Particularly, it maintains a quality generation performance after the context length is\n\nTable 2: Evaluation of different methods on LongBench. Activation Beacon performs on par with the fine-tuned full-attention baselines. | Method | Single-Doc QA | Multi-Doc QA | Summarization | Few-Shot | Code |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| Llama-2-7B | 24.90 | 22.60 | 24.70 | 60.00 | 48.10 |\n| PI | 18.98 | 17.16 | 25.03 | 49.43 | 52.73 |\n| NTK | 23.21 | 23.34 | 24.40 | 59.29 | 49.28 |\n| StreamingLLM | 21.47 | 22.22 | 22.20 | 50.05 | 48.00 |\n| AutoCompressor-6K | 13.22 | 10.61 | 14.00 | 15.72 | 23.62 |\n| YaRN-128K | 24.03 | 24.11 | 19.82 | 60.00 | 62.73 |\n| LongChat-4K | 28.14 | 21.88 | 26.59 | 62.06 | 52.77 |\n| LongChat-32K | $\\mathbf{3 1 . 5 8}$ | 23.50 | 26.70 | $\\mathbf{6 4 . 0 2}$ | 54.10 |\n| LongAlpaca-4K | 26.81 | 24.44 | $\\underline{26.93}$ | 62.92 | 55.15 |\n| LongAlpaca-16K | 28.70 | $\\underline{28.10}$ | $\\mathbf{2 7 . 8 0}$ | $\\underline{63.70}$ | 56.00 |\n| LongLlama | $\\underline{30.12}$ | 16.37 | 24.19 | 60.31 | $\\mathbf{6 6 . 0 5}$ |\n| Activation Beacon | 28.27 | $\\mathbf{2 8 . 4 4}$ | 25.15 | 61.00 | 57.75 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d51bda31a030ab1a9287g-07.jpg?height=670&width=1045&top_left_y=928&top_left_x=540)\n\nFigure 4: The evaluation of topic retrieval accuracy at different context lengths. Activation Beacon is competitive against the fine-tuned methods, like LongChat-32K and LongAlpaca-16K. extended to 100 K , where most of the baselines become either ineffective or out-of-memory (OOM). In fact, Activation Beacon is still effective even after the context length is further extended to 400 K (see Figure 1), which means a $100 \\times$ extension of Llama-2-7B's maximum context length. Unlike many other methods like fine-tuning, Activation Beacon does not require any long-sequence training data to acquire such a super long-context capability, which contributes to its high usability in practice. ### 3.2.2 More Long-Context Tasks\n\nWe further study the five real-world tasks from LongBench [2], including single-doc QA, multi-doc QA, summarization, few-shot learning, and code completion, where the experiment result on each task is reported in Table 2.",
    "flagembedding-4": "We also evaluate the topic retrieval task [16], whose result is shown in Figure 4. In Appendix D, we evaluate the passkey retrieval task [35]. Similar to our previous observation on long-context language modeling, Activation Beacon leads to a notable improvement over Llama-2-7B and the fine-tuning-free baselines. Meanwhile, it reaches a comparable performance with the fine-tuned full-attention methods. Because a large portion of the evaluation samples can be (almost) covered by the 16 K or 32 K context window, the fine-tuned full-attention methods indeed set a high standard on LongBench. However, knowing that the fine-tuning operation will change the LLM's original parameters, it is still interesting to investigate where the empirical advantage of the finetuned methods comes from. To figure out this problem, we benchmark the performance of\n\nTable 3: Evaluation of inference time and GPU memory usage. Both metrics are measured by the average value of 100 forward passes (FlashAttention-2 is enabled for LongChat). | Method | 4 K | GPU Memory (GB) |  |  |  |  |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 4 K | 8 K | 16 K | 32 K | 100 K | 4 K | 8 K | 16 K | 32 K | 100 K |\n| LongChat-32K | 18.5 | 24.2 | 35.6 | 58.4 | OOM | 0.045 | 0.089 | 0.191 | 0.460 | OOM |\n| StreamingLLM | 19.9 | 19.9 | 19.9 | 19.9 | 19.9 | - | - | - | - | - |\n| AutoCompressor-6K | 17.7 | 22.6 | 32.3 | 51.7 | OOM | 0.087 | 0.134 | 0.224 | 0.478 | OOM |\n| LongLlama | 18.2 | 21.9 | 34.2 | OOM | OOM | 0.079 | 0.190 | 0.436 | OOM | OOM |\n| Activation Beacon | 21.7 | 21.3 | 21.4 | 21.6 | 21.6 | 0.071 | 0.121 | 0.237 | 0.473 | 1.494 |\n\nLongChat 32 K and LongAlpaca- 16 K at the context length of 4 K , where they use the same information as the Llama-2-7B baseline. Interestingly, both methods result in a substantial improvement over Llama-2-7B on every task. Especially for summarization, where both methods are already sufficiently strong at 4 K , yet little extra improvements are made with the further extended context window. By comparison, Activation Beacon inherits Llama-2-7B's performance at the context length of 4 K , where its performance gain over Llama-2-7B is introduced by the extended context. In this sense, its impact on utilizing the long contextual information can still be no inferior to the ones from the finetuned methods in the corresponding situations. ### 3.3 Efficiency Analysis\n\nWe evaluate the running efficiency at the inference time in terms of time cost and GPU memory usage, whose results are reported in Table 3 Compared with LongChat (full-attention) and LongLlama, Activation Beacon enjoys a much smaller GPU memory usage at the long context. Activation Beacon and StreamingLLM result in a similar memory cost because both methods\n\nTable 4: Comparison of training time and GPU memory cost between LongAlpaca-16K (8xA100 GPUs) and Activation Beacon (8xA800 GPUs). | Method | Time (Hour) | Memory (GB) |\n| :--- | :---: | :---: |\n| LongAlpaca-16K | 20.8 | 57.1 |\n| Activation Beacon | 9.0 | 55.9 |\n\nare based on sliding windows. As for the inference time, Activation Beacon is faster than LongLlama, but slower than LongChat when the context is short. This is because Activation Beacon is streamingly processed while LongChat is fully paralle ${ }^{3}$ However, Activation Beacon is able to gradually catch up when the context length gets longer, as its time complexity is linear to the context length. It will ultimately become much faster than the full-attention methods if the context length is extended long enough. Finally, we compare our training cost with LongAlpaca, which is featured for its high training efficiency (shown in Table 4). Under a similar hardware condition ( $8 \\times$ A800 GPUs vs. $8 \\times$ A100 GPUs), the training of Activation Beacon can be accomplished in just 9 hours, which is even faster than the reported time cost of LongAlpaca-16K with $S^{2}$-attr $\\sqrt[4]{4}(\\$ 2.4$. ### 3.4 Ablation Studies\n\nWe perform ablation studies to evaluate the impact from different technical factors, including the attention scheme of beacons ( $\\$ 2.2$, the sampling strategy of condensing ratio ( $\\$ 2.4$, the introduced parameters for beacons ( 2.4 , and the composition of training data ( 3.1 . The experiment results are shown in Table 5\n\nFirst of all, we can find that the attention scheme exerts a substantial impact on Activation Beacon's performances on both long-context language modeling (PG19) and long-context understanding (QA). The stepwise expansion works with the gradually expanded attention scope. Therefore, it enables the beacons to acquire different levels of local and global information of each context interval, which notably improves the performance over the other two options. [^1]Table 5: The impact of different technical factors: attention scheme of beacon token, condensing ratio, composition of training data. Performances are measured by PG19 with 32 K context and single-Doc QA on LongBench. Default settings are marked by *. | Factor | Setting | PG19 | QA |\n| :--- | :--- | :---: | :---: |\n| Attention | Segmentation | 8.39 | 26.05 |\n|  | Full coverage | 8.76 | 23.13 |\n|  | Stepwise expansion* | 8.27 | 28.27 |\n| Condensing | Monotonous $(\\alpha=4)$ | $>10^{2}$ | 26.48 |\n|  | Instance-wise randomized | 8.19 | 26.33 |\n|  | Step-wise randomized* | 8.27 | 28.27 |\n| Beacon | Q, K, V (1.5B) | 8.32 | 27.04 |\n|  | Q, K, V, O, MLP (5.5B) | 8.81 | 23.46 |\n|  | Q, K, V, O (2.0B)* | 8.27 | 28.27 |\n| Data | RedPajama only | 8.24 | 24.98 |\n| Composition | RedPajama+LongAlpaca* | 8.27 | 28.27 |\n\nSecondly, the sampling of the condensing ratio is another influential factor. In this place, we compare two alternative strategies. The instance-wise option samples one condensing ratio for all context intervals of each training instance $X$ (from the same scope as the step-wise method, i.e. $\\{2,4,8, \\ldots, 128\\}$ ). While the monotonous option makes use of one constant condensing ratio of 4 (which can support a up-to 16 K context length). We can observe that the step-wise sampling strategy, which introduces the most diversified condensing ratios when learning, results in competitive performance on perplexity while significantly outperforms the other two options on long-context understanding. Thirdly, we analyze the impact by introducing different amounts of learnable parameters to the beacon module. Specifically, when we remove the output projection matrix $\\boldsymbol{W}_{O}^{b}$ from the beacon parameters $\\mathrm{MHA}^{b}$ ( $\\$ 2.2$, the empirical performances on both tasks degrade. When we additionally include the MLP parameters of FFN, the model's performance does not improve. We conjecture that this is probably because the FFN layer is heavily loaded, which slows down the convergence of the training process. As a result, it suggests that our current formulation of the learnable parameters is a good trade-off between cost and effectiveness. Lastly, we can also observe that only using RedPajama as the training data already leads to a competitive performance on both evaluation tasks. The introduction of more training data from LongAlpaca contributes little to the language modeling task. However, it brings an additional improvement to the empirical performance on Single-Doc QA. ## 4 Related Works\n\nWe discuss the following works which are devoted to the extension of LLM's context. First of all, a large body of methods have been proposed to increase the size of context window. For example, $\\mathrm{ALiBi}[21]$ leverages linear-decaying attention biases to achieve the extrapolation of position encoding. Methods like Position Interpolation [5], NTK-Aware scaling [1] and ReRoPE [26] make progress on top of RoPE [27], which enable the LLM to handle unseen positions at the inference time. Although such methods can be directly applied to the well-trained LLM, they usually benefit from continual fine-tuning where the extended context can be better utilized [20]. The fine-tuning with long-sequence data is expensive. Thus, people investigate how to reduce the training cost. For example, LongLora [6] proposes $S^{2}$-Attn and leverages LoRA for cost-effective training; while PoSE [41] uses skip-wise position indices to train LLMs on 2 K context length as a simulation of 128 K . However, the fine-tuning operations are still prone to big costs if super long-sequence data is presented. Finally, the fine-tuning operation may impair the LLM's existing capabilities on short contexts [20]. By comparison, our method is trained with a small cost and enjoys a high efficiency in training and inference. Besides, it serves as a plug-in module that is fully compatible with the existing LLM. The quadratic complexity of transformer is a major bottleneck to achieve long contexts. Thus, many previous works aim to address this problem by using sparse attention [8; 3; 38; 12] or approximate attention computation [15; 33, 9, 23]. However, there are threefold challenges about these methods as analyzed in [36]: the requirement of customized GPU kernels for specific variants of matrix multiplication, the dependency on global attention patterns which are unsuitable for autoregressive language models, the incompatibility with the well-pretrained models. In contrast, our method is free from these constraints and preserves a high compatibility with the existing LLMs. It is also plausible to find ways to process long contexts with short context windows. One popular strategy is to use sliding windows. For example, StreamingLLM [36] and LM-Infinite [13] are able to achieve an infinite context by only maintaining the activations for the very first and the latest tokens. However, they are unable to leverage the rich information from the long context because the portion beyond the sliding window will be discarded. Besides, the long contexts can also be summarized and compressed into more compact forms [4, 7, 19, 22, 14], which follow the same spirit as our work. However, the previous methods call for major changes to the original model's architecture and working process, which brings in many problems. Notably, they are prone to substantial compression losses which prevent them from making extensions for long contexts. Besides, they lack the flexibility to support different context lengths, and suffer from the incompatibility with existing LLMs. Finally, it becomes popular to offload the long context into external memory and retrieve the useful part from it as the working context. The retrieved data can be either the chunked input [37, 39] or the cached KV activations, e.g., Memorizing Transformers [35] and LongMem [34]. This idea has been further extended by many recent works. For example, Landmark Attention [18] uses a special token to represent a chunk of activations, which enables more efficient computation of retrieval. Focused Transformers [32] proposes to use contrastive training which improves the discrimination of relevant keys from the cached data.",
    "flagembedding-5": "The retrieval-based methods can be limited due to the utilization of incoherent context. However, it tackles the the problem from a different perspective which can benefit from the collaboration with our method (explored in Appendix D. ## 5 Conclusion\n\nWe introduce Activation Beacon for the extension of LLM's context length. Activation Beacon condenses the LLM's raw activations into highly compact forms, enabling the LLM to perceive a long context with a limited context window. As a plug-in component for the LLM, it brings in long contextual information while fully preserving the LLM's existing capabilities in short contexts. When dealing with long-sequence data, it resorts to a sliding window for stream processing, which leads to a superior working efficiency for both training and inference. By using short-sequence data with diversely sampled condensing ratios, it can be effectively learned to support different context lengths with a small training cost. Our experiment verifies Activation Beacon as an effective, efficient, compatible, and low-cost method to extend the context length for LLMs. ## Broader Impact\n\nActivation Beacon establishes long-context capabilities for the large language model without affecting its original capabilities. This enhancement may benefit many long-context scenarios using LLMs, such as long document understanding/summarization, and lifelong chating with long-term memory. Therefore, it is particularly useful for AI applications like AI readers and lifelong AI chatbots. Activation Beacon is able to compress the raw activations of LLM into fewer yet more compact ones with minimal loss. As a result, it can reduce the Key-Value cache requirements for numerous AI applications, leading to significant resource savings. Moreover, compared to full attention mechanisms, Activation Beacon requires considerably fewer computational resources with competitive speed. This efficiency also contributes to environmental sustainability. As a downside, since Activation Beacon is based on the LLM, it inherits the internal biases of the LLM. Consequently, there is a risk of generating unreliable or harmful content, which underscores the need for careful monitoring the ethical usage of these AI systems. ## References\n\n[1] Ntk-aware scaled rope, 2023. URL/https://www.reddit.com/r/LocalLLaMA/comments/ 14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/. [2] Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [3] Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150. [4] Bulatov, A., Kuratov, Y., and Burtsev, M. S. Scaling transformer to 1m tokens and beyond with RMT. CoRR, abs/2304.11062, 2023.",
    "flagembedding-6": "doi: 10.48550/ARXIV.2304.11062. URL https: //doi.org/10.48550/arXiv.2304.11062\n[5] Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [6] Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [7] Chevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting language models to compress contexts. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 3829-3846. Association for Computational Linguistics, 2023. URL https: //aclanthology.org/2023.emnlp-main. 232 . [8] Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.",
    "flagembedding-7": "arXiv preprint arXiv:1904.10509, 2019. [9] Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D.",
    "flagembedding-8": "B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=Ua6zukOWRH\n[10] Computer, T. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data\n[11] Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning.",
    "flagembedding-9": "CoRR, abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691. URL https://doi.org/10 $48550 / \\mathrm{arXiv} .2307 .08691$\n[12] Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Zheng, N., and Wei, F. Longnet: Scaling transformers to $1,000,000,000$ tokens.",
    "flagembedding-10": "CoRR, abs/2307.02486, 2023. doi: $10.48550 /$ ARXIV.2307.02486. URL https://doi.org/10.48550/arXiv.2307.02486. [13] Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple onthe-fly length generalization for large language models.",
    "flagembedding-11": "$\\operatorname{CoRR}, \\mathrm{abs} / 2308.16137,2023$. doi: 10.48550/ARXIV.2308.16137. URL https://doi.org/10.48550/arXiv.2308.16137. [14] Huang, X. and Hollenstein, N. Long-range language modeling with selective cache. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 4838-4858. Association for Computational Linguistics, 2023. URL https://aclanthology.org/2023.findings-emnlp. 321\n[15] Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=rkgNKkHtvB. [16] Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise on context length?, June 2023. URL https://lmsys.org/blog/2023-06-29-longchat.",
    "flagembedding-12": "[17] Michael Gschwind, Driss Guessous, C. P. Accelerated pytorch 2 transformers. https:// pytorch.org/blog/accelerated-pytorch-2/.",
    "flagembedding-13": "2023.",
    "flagembedding-14": "[18] Mohtashami, A. and Jaggi, M. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023. [19] Mu, J., Li, X. L., and Goodman, N. D. Learning to compress prompts with gist tokens. CoRR, abs/2304.08467, 2023. doi: 10.48550/ARXIV.2304.08467. URL https://doi.org/10 $48550 / a r X i v .2304 .08467$\n[20] Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models.",
    "flagembedding-15": "arXiv preprint arXiv:2309.00071, 2023. [21] Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0\n[22] Rae, J.",
    "flagembedding-16": "W., Potapenko, A., Jayakumar, S.",
    "flagembedding-17": "M., Hillier, C., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SylKikSYDH\n[23] Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuurmans, D., and Dai, B. Combiner: Full attention transformer with sparse computation cost.",
    "flagembedding-18": "In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 22470-22482, 2021. URL https://proceedings.neurips cc/paper/2021/hash/bd4a6d0563e0604510989eb8f9ff71f5-Abstract.html.",
    "flagembedding-19": "[24] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. [25] Rubin, O. and Berant, J. Long-range language modeling with self-retrieval. CoRR, abs/2306.13421, 2023. doi: 10.48550/ARXIV.2306.13421. URL https://doi.org/10 48550/arXiv. 2306.13421\n[26] Su, J. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023. [27] Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding.",
    "flagembedding-20": "CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/2104.09864. [28] Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei, F. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022. [29] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [30] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models.",
    "flagembedding-21": "arXiv preprint arXiv:2307.09288, 2023. [31] Tunstall, L., Von Werra, L., and Wolf, T. Natural language processing with transformers, 2022. [32] Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023. [33] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768, 2020. URL https://arxiv.org/abs/2006.04768\n[34] Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F. Augmenting language models with long-term memory. CoRR, abs/2306.07174, 2023. doi: 10.48550/ARXIV.2306. 07174. URL https://doi.org/10.48550/arXiv.2306.07174. [35] Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=TrjbxzRcnf-. [36] Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [37] Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. CoRR, abs/2310.03025, 2023. doi: 10.48550/ARXIV.2310.03025. URL https://doi.org/10 48550/arXiv. 2310.03025\n[38] Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences.",
    "flagembedding-22": "Advances in neural information processing systems, 33:17283-17297, 2020. [39] Zhang, P., Xiao, S., Liu, Z., Dou, Z., and Nie, J. Retrieve anything to augment large language models.",
    "flagembedding-23": "CoRR, abs/2310.07554, 2023. doi: 10.48550/ARXIV.2310.07554. URL https: //doi.org/10.48550/arXiv. 2310.07554\n[40] Zhangir Azerbayev, Edward Ayers, B.",
    "flagembedding-24": "P. Proof-pile. https://huggingface.co/datasets/ hoskinson-center/proof-pile, 2022. [41] Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S. Pose: Efficient context window extension of llms via positional skip-wise training. CoRR, abs/2309.10400, 2023.",
    "flagembedding-25": "doi: 10.48550/ARXIV.2309.10400. URL https://doi.org/10.48550/arXiv.2309.10400. ## A Overall Algorithm of Activation Beacon\n\n```\nAlgorithm 1 Activation Beacon\nRequire: The LLM \\(\\Theta\\); the input context \\(X\\); the memory of condensed activations \\(\\boldsymbol{K}^{b} \\in\\)\n    \\(\\mathbb{R}^{* \\times L \\times D}, \\boldsymbol{V}^{b} \\in \\mathbb{R}^{* \\times L \\times D}\\); the memory of raw activations \\(\\boldsymbol{K}^{r} \\in \\mathbb{R}^{* \\times L \\times D}, \\boldsymbol{V}^{r} \\in \\mathbb{R}^{* \\times L \\times D}\\); the\n    interval length \\(l\\); and the starting index of the sliding window \\(i_{s}\\). repeat\n        Get the ending index of the sliding window: \\(i_{e} \\leftarrow i_{s}+l\\)\n        if \\(i_{e} \\leq|X|\\) then\n            \\(i s \\_f u l l \\_w i n d o w \\leftarrow\\) True\n            Set condensing ratio for this interval: \\(\\alpha \\leftarrow\\) set_ratio()\n            Set number of special tokens for this interval: \\(\\beta \\leftarrow l / / \\alpha\\)\n            The sliding window contains regular tokens in the context appended with beacon tokens:\n            \\(w \\leftarrow X_{i_{s}: i_{e}}+[\\langle\\mathrm{bcn}\\rangle] \\times \\beta\\)\n        else\n            is_full_window \\(\\leftarrow\\) False\n            The sliding window contains only the regular tokens in the context: \\(w \\leftarrow X_{i_{s}: i_{e}}\\)\n        end if\n        The memory of the LLM is the concatenation of the condensed activations and the raw\n        activations:\n```\n\n$$\n\\boldsymbol{K} \\leftarrow \\boldsymbol{K}^{b} \\oplus \\boldsymbol{K}^{r}, \\quad \\boldsymbol{V} \\leftarrow \\boldsymbol{V}^{b} \\oplus \\boldsymbol{V}^{r}\n$$\n\n13: The LLM auto-regressively encodes the regular tokens as well as the beacon tokens in the sliding window (note that activation condensing happens according to Equation 1). The logits of the next token and the newly generated activations are returned:\n\n$$\n\\boldsymbol{\\psi}, \\boldsymbol{K}^{\\prime}, \\boldsymbol{V}^{\\prime} \\leftarrow \\Theta(w, \\boldsymbol{K}, \\boldsymbol{V})\n$$\n\nif $i s \\_f u l l \\_w i n d o w$ then\nThe last $\\beta$ activations are condensed activations, which are accumulated:\n\n$$\n\\boldsymbol{K}^{b} \\leftarrow \\boldsymbol{K}^{b} \\oplus \\boldsymbol{K}_{-\\beta:}^{\\prime}, \\quad \\boldsymbol{V}^{b} \\leftarrow \\boldsymbol{V}^{b} \\oplus \\boldsymbol{V}_{-\\beta:}^{\\prime}\n$$\n\n16: The raw activations of previous intervals are emptied:\n\n$$\n\\boldsymbol{K}^{r} \\cdot \\operatorname{empty}(), \\quad \\boldsymbol{V}^{r} \\cdot \\operatorname{empty}()\n$$\n\nUpdate the starting index $i_{s} \\leftarrow i_{e}$ else\n\nThe raw activations from regular tokens are cached:\n\n$$\n\\boldsymbol{K}^{r} \\leftarrow \\boldsymbol{K}^{r} \\oplus \\boldsymbol{K}^{\\prime}, \\quad \\boldsymbol{V}^{r} \\leftarrow \\boldsymbol{V}^{r} \\oplus \\boldsymbol{V}^{\\prime}\n$$\n\nend if\nuntil $i_{e} \\geq|X|$\nOffset the starting index for future generation: $i_{s} \\leftarrow i_{s}-|X|$\nReturn $\\Theta, \\boldsymbol{\\psi}, \\boldsymbol{K}^{b}, \\boldsymbol{V}^{b}, \\boldsymbol{K}^{r}, \\boldsymbol{V}^{r}, i_{s}$\n\n## B Length Distribution of Training Data\n\n## C Combining Activation Beacon with Context Window Extension Techniques. Activation Beacon can be combined with context window extension techniques to further extend the context length. Specifically, Activation Beacon condenses the raw activations of LLM into more compact forms so that the LLM can perceive more information given its original context window size. It does not modify the position encoding scheme of the LLM within the context window. Therefore,\n\n| Length | $1 \\mathrm{~K} \\sim 2 \\mathrm{~K}$ | $2 \\mathrm{~K} \\sim 4 \\mathrm{~K}$ | $4 \\mathrm{~K} \\sim 6 \\mathrm{~K}$ | $6 \\mathrm{~K} \\sim 8 \\mathrm{~K}$ | Total |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| Count | 38 K | 23 K | 6 K | 13 K | 80 K |\n| Portion | $47 \\%$ | $29 \\%$ | $8 \\%$ | $16 \\%$ | $100 \\%$ |\n\nTable 6: The length distribution of training data. The average length of all training data is 3180 . ![](https://cdn.mathpix.com/cropped/2024_09_12_d51bda31a030ab1a9287g-15.jpg?height=679&width=1044&top_left_y=555&top_left_x=538)\n\nFigure 5: The perplexity evaluated on books longer than 400 K tokens in the PG19 test set. Activation beacon can work together with context window extension methods, further extending the context length to even 1 M tokens. we can directly employ the modern context window extension techniques, such as PI [5] and NTK [1], to expand the window size so that more condensed activations can be accomodated and hence further context extension effect. In Figure 5], we show Activation Beacon can extend the context length of Llama-2 to 1M when combined with PI and NTK, without any further fine-tuning. This result unveils the superior compatibility of Activation Beacon: it is not only compatible with existing capabilities of the LLM, but also compatible with any future advances of context window extension techniques. ![](https://cdn.mathpix.com/cropped/2024_09_12_d51bda31a030ab1a9287g-15.jpg?height=510&width=1406&top_left_y=1898&top_left_x=357)\n\nFigure 6: The accuracy and the fuzzy score on tha Passkey Retrieval task. Activation beacon can work together with retrieval techniques to accurately remember the passkey. ## D Combining Activation Beacon with Retrieval. Activation Beacon condenses the raw activations and accumulates them in the sliding window for later auto-regression. However, the size of the sliding window is up-bounded by the LLM's context window size, which limits the number of condensed activations that could be maintained. Consequently, Activation Beacon needs to trade-off between the memory accuracy and the context length: With a low condensing ratio, Activation Beacon can remember nearly all details about the context, while simultaneously producing more condensed activations for the context. Under this setting, Activation Beacon can only deal with a relatively short context (e.g. 8K). In contrast, with a high condensing ratio (e.g. 128), its memory is vague since 128 raw activations are compressed into one, yet it can easily process super long context (e.g. 100K). Therefore, its performance may degrade on tasks that require accurate memorization of super long context. However, we can largely mitigate this problem with retrieval. Specifically, we perform two types of condensation for each interval, one with a high condensing ratio and one with a low condensing ratio, and save the condensed activations from both of them. The model always conditions on the aggressively condensed activations, thus it can process super long context. Besides, we can obtain both types of condensed activations in one forward pass by adjusting the attention mask, which is efficient. When a query about the fine-grained memory is issued, we can leverage retrieval to locate $K$ relevant intervals. The information within these relevant intervals should be accurate to successfully answer the query, while other information can be vague. Therefore, for these intervals, we replace their aggressively condensed activations with those accurate ones resulted from the low condensing ratio, then proceed to generate the answer. Thanks to the step-wise randomized condensing ratio in training, Activation Beacon can robustly utilize the activations with different condensing ratios, thereby effectively utilizing the retrieved information.",
    "flagembedding-26": "The retrieval can be implemented in different ways. For example, the BM25 based on lexical matching, the dense retrieval based on semantic matching, and the hidden state retrieval. We adopt the simple BM25 retrieval and set $K=2$, which already yields satisfactory performance. We use the Passkey Retrieval task [35; 6, 32] to evaluate our approach. It constructs a 5-digit integer, namely passkey, and buries it at a random position in a synthetic document. The models are asked to exactly reproduce the passkey after reading the document. We repeat the experiment 5 times at each context length and report the average performance in Figure 6 The following observations are derived. Firstly, Activation Beacon cannot accurately remember the passkey hidden in the long context. Though Activation Beacon is accurate on the relatively short context ( 8 K ), it becomes incapable given 16 K context and beyond. This is as expected since longer context requires larger condensing ratio, which impairs the memory accuracy. However, this doen't mean the aggressively condensed activations are meaningless. In Figure 6 (B), we evaluate the fuzzy score between the model's prediction and the ground-truth passkey, which measures how many digits are overlapped between the prediction and the groud-truth. It can be noticed that Activation Beacon always yields positive fuzzy score. This indicates that Activation Beacon can restore several digits in the passkey, while fail to precisely remember it due to the vague memory. Secondly, Activation Beacon combined with BM25 significantly improves the memory accuracy, achieving $100 \\%$ accuracy at all context lengths. The reasons behind such a superior performance is two fold. On the one hand, the BM25 retriever successfully locates the interval where passkey resides. On the other hand, the model can effectively utilize the accurate memory of the retrieved intervals. In conclusion, we demonstrate that Activation Beacon's memory accuracy can significantly benefit from even the simplest form of retrieval. This motivates us to investigate more retrieval implementations and explore the potential of Activation Beacon on more real-world long-context tasks. ## E Impact of Different Condensing Ratios\n\nThe condensing ratio of Activation Beacon can be flexibly configured at inference time. One interesting question is how different condensing ratios affect the performance of Activation Beacon. We investigate this question on long-context generation and report the results in Figure 7\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d51bda31a030ab1a9287g-17.jpg?height=660&width=1044&top_left_y=253&top_left_x=538)\n\nFigure 7: The perplexity of Activation Beacon given different condensing ratios. The theoretical maximum context length is denoted in the brackets $(\\alpha \\times 3072+1024$ where $\\alpha$ is the condensing ratio). We can observe that lower condensing ratio leads to lower perplexity, which translates to higher generation long-context generation quality. However, since the window size of Activation Beacon is limited by that of the LLM, lower condensing ratio results in more activations to save, and hence shorter context it can process. ## F Limitations\n\nCurrently, the sliding window of Activation Beacon strides one interval at a time and only preserves the condensed activations in the previous interval. This means the tokens in the beginning of the next interval do not have any raw context: it can only attend to the condensed activations in previous intervals. This lack of raw context may cause the degradation of the generation performance especially when answering user instructions. In the future, we may adjust the stride of the sliding window to make it shorter than the interval length. Therefore, tokens in the beginning of any interval always attend to some raw activations as local context. [^0]:    ${ }^{*}$ Peitian Zhang and Zheng Liu are the co-first authors\n    ${ }^{\\dagger}$ Zheng Liu is the corresponding author\n\n[^1]:    ${ }^{3}$ StreamingLLM is slow due to its current step-wise realization, yet its theoretical speed should be comparable with our method. ${ }^{4}$ https://openreview.net/forum?id=6PmJoRfdaK\n\n"
}