{
    "xl3m-0": "XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference\n\nShengnan Wang Theory Lab, 2012 Labs Huawei Technologies Co., Ltd Youhui Bai Theory Lab, 2012 Labs Huawei Technologies Co., Ltd Lin Zhang Theory Lab, 2012 Labs Huawei Technologies Co., Ltd Pingyi Zhou Huawei Technologies Co., Ltd.",
    "xl3m-1": "Shixiong Zhao Theory Lab, 2012 Labs Huawei Technologies Co., Ltd Gong Zhang Theory Lab, 2012 Labs Huawei Technologies Co., Ltd Sen Wang Theory Lab, 2012 Labs Huawei Technologies Co., Ltd Renhai Chen Theory Lab, 2012 Labs Huawei Technologies Co., Ltd Hua Xu Huawei Technologies Co., Ltd Hongwei Sun Huawei Technologies Co., Ltd Correspondence to: wangshengnan12@huawei.com.",
    "xl3m-2": "Abstract\n\nLength generalization failure problem, namely the large language model (LLM) fails to generalize to texts longer than its maximum training length, greatly restricts the application of LLM in the scenarios with streaming long inputs. To address this problem, the existing methods either require substantial costs or introduce precision loss. In this paper, we empirically find that the accuracy of the LLM\u2019s prediction is highly correlated to its certainty. Based on this, we propose an efficient training-free framework, named XL3M (it means extra-long large language model), which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning. Under the XL3M framework, the input context will be firstly decomposed into multiple short sub-contexts, where each sub-context contains an independent segment and a common \u201cquestion\u201d which is a few tokens from the end of the original context. Then XL3M gives a method to measure the relevance between each segment and the \u201cquestion\u201d, and constructs a concise key context by splicing all the relevant segments in chronological order. The key context is further used instead of the original context to complete the inference task. Evaluations on comprehensive benchmarks show the superiority of XL3M. Using our framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card. 1 Introduction\n\nTransformer based large language models (LLMs) Vaswani et al.",
    "xl3m-3": "(2017); Brown et al. (2020); Touvron et al. (2023); Scao et al. (2022) have shown their impressive performance in many language tasks Zhao et al. (2023). However, due to the out-of-domain and distraction issues Xiao et al. (2024), the quality of the LLM\u2019s generation drops dramatically when the sequence length surpasses its context window size which is the largest training length. Such a drawback hinders the application of LLM in multi-round dialogue, conversation conduction, documents summarization, and other real tasks which often encounter very long sequences. Some pioneering works have been done for context length extrapolation. Most of them focused on optimizing the positional encoding (PE), since the PE of unseen length was identified as a major factor leading to length generalization failure. Compared with the vanilla absolute PE, the later proposed relative PE Raffel et al. (2020); Su et al. (2021), ALiBi Press et al. (2021), and NoPE Kazemnejad et al. (2023) were demonstrated to offer better generalization. However, all of them do not perform well when the sequence length is significantly longer than the largest training length. A more effective approach is to continually train or fine-tune the model on longer-length data Chen et al. (2023); Peng et al. (2023). Nevertheless, such a manner can only extend the context window to a limited length due to unacceptable training costs Xiong et al. (2023). Moreover, when the length is very long, even collecting the training data itself is a difficult task. Recently, some training-free length extension methods attracted widespread attention. LM-Infinite Han et al. (2023) and StreamLLM Xiao et al. (2023) extrapolated the length by discarding most contexts but only keeping the context at the end and the very beginning. Though these methods can efficiently deal with extremely long contexts, they lose a lot of long-distance dependencies, which leads to deviations or even errors in text understanding. PCW Ratner et al. (2023) designed chunked attention mask and reused the positional encoding for different chunks, which alleviated the restriction of the context window. However, PCW can only extend the context window to a very limited length, and the effectiveness of PCW needs to be further studied Ratner et al. (2023). In this work, we propose an efficient training-free inference framework named XL3M (it means extra-long large language model) which enables the LLM to break the length limit. The effectiveness of the XL3M framework is due to an important principle: the length of the sequence processed by the LLM at one time should not exceed the its context window size. The contributions of this paper are summarized as follows:\n\n\u2022\n\nWe empirically find that the accuracy of the LLM\u2019s prediction is highly correlated to its certainty measured by entropy. Based on this, we propose XL3M, a novel inference framework enabling any LLM to read and understand extremely long sequences. Inspired by the human\u2019s habit of reading in segments, under our framework, each input long context will be decomposed into multiple short sub-contexts with a common \u201cquestion\u201d to be answered. For each sub-context, we use the LLM to compute the local conditional probability distribution (cpd) as well as its corresponding entropy. Then the relevant sub-contexts with small entropy values are selected and reorganized into key context in chronological order. Since most irrelevant context is removed, LLM can generate high-quality results according to the extracted key context. \u2022\n\nWe evaluate the proposed framework on comprehensive LongBench tasks and the widely-used \u201cNeedle in a Haystack\u201d task. We compare the performance of XL3M with the state-of-the-art methods, including both fine-tuning and non-fine-tuning methods. The results demonstrate the superiority of the proposed framework. \u2022\n\nThe proposed XL3M framework does not modify the main the structure of the LLM, and it does not need any additional training or fine-tuning. It is both memory and time efficient. Under the XL3M framework, the LLM is able to reason sequences longer than 20M on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card. 2 Related work\n\nDue to the strong demand of long sequence inference, a lot of context window extension techniques have been proposed Naveed et al.",
    "xl3m-4": "(2023); Kaddour et al. (2023). These methods can be mainly divided into three categories: 1) Extension by fine-tuning; 2) Extension without fine-tuning; 3) Extension by external memory Bertsch et al.",
    "xl3m-5": "(2024); Wu et al.",
    "xl3m-6": "(2022); Xiao et al. (2024). 2.1 Extension by fine-tuning\n\nThe LLMs are generally trained on relatively short sequences, due to the expensive computational and memory requirements (quadratic with the sequence length) in the attention mechanism, and LLMs fail to generalize to unseen lengths at the inference phase Han et al. (2023); Chen et al. (2023). A straightforward idea for length generalization is to fine-tune the model on longer sequences. However, it was found that naively fine-tuning a pre-trained LLM for window extrapolation is less effective and inefficient Kaddour et al.",
    "xl3m-7": "(2023); Anil et al. (2022). Chen et al. (2023) showed that using position interpolation (PI) rather than extrapolation during fine-tuning can extend the context window of the pre-trained LLMs to 32k without performance loss. Further, Yarn Peng et al. (2023) proposed a novel NTK-aware interpolation method and achieved tens of times extension of the context window size. Other fine-tuning based methods include Giraffe Pal et al. (2023), FoT Tworkowski et al. (2023), and so on. However, this kind of method requires massive training resources since it needs to train LLMs on long-sequence data. In addition, collecting enough long-sequence data itself for fine-tuning is also a challenging work if one wants to extend the context window to extremely long. 2.2 Extension without fine-tuning\n\nTo save resources, some training-free context window extension methods are proposed. At the very beginning, most researchers focused on optimizing the positional encoding. The vanilla absolute position encoding strictly restricts the reasoning length of LLM. To tackle this issue, a lot of advanced position encoding schemes were proposed, such as RoPE Su et al. (2021), ALiBi Press et al. (2021), and the recently proposed NoPE Kazemnejad et al. (2023). However, all of them only make the model architecturally-able to deal with long inputs rather than actually perform well on long-sequence reasoning tasks Li et al. (2023); Kaddour et al. (2023). Instead of encoding the unseen length, StreamLLM Xiao et al. (2023) chose to discard most context but only keep the recent tokens (tokens at the end) and sink tokens (tokens at the very beginning), ensuring that the total length of the remaining context does not exceed the LLM\u2019s window size. Such a method not only enabled the LLM to deal with longer context, but also achieved a remarkable speedup. Similar idea was also adopted by LM-Infinite Han et al. (2023). However, both StreamLLM and LM-Infinite missed a lot of long-distance dependencies, leading to deviations or even errors in text understanding. The most related work should be parallel context windows (PCW) Ratner et al. (2023). By modifying position embedding and attention mask, PCW alleviated the context window restriction for any off-the-shelf LLM without further training. However, it was shown that PCW is only effective for a limited extension (about three times extension of its original context window size), and the performance degrades when extending to a much longer length. Moreover, the effectiveness of PCW was only demonstrated on tasks like multi-class tasks classification and information extraction. It remains an open question whether it is suitable for more general tasks. 2.3 Extension by external memory\n\nUnlike the previous approaches which keep the main architecture of the model unchanged, the methods in this category usually involve modifications to the model. Generally, this kind of method introduces the external memory to restore the information of the past context, and retrieves the relevant tokens from the memory for generation based on some search mechanism, such as KNN Bertsch et al.",
    "xl3m-8": "(2024); Wu et al. (2022). The main disadvantage is that these methods require additional memory overhead, and they usually need further training or fine-tuning to ensure the effectiveness.",
    "xl3m-9": "Xiao et al. (2024) and Munkhdalai et al. (2024) respectively proposed an offload mechanism and a compression mechanism to reduce memory pressure. In this paper, we aim to propose an effective training-free inference framework which enables any LLM to reason extremely long sequence. 3 XL3M: extra-long large language model\n\nLanguage model, including LLM, studies a conditional probability distribution (cpd) given string of texts Wei et al. (2023). In the inference phase, given an input sequence, LLM also first computes the cpd and then generates a token according to a predetermined generation mode, such as greedy search, top-k search, top-p search, etc. Since the length of the training data is limited within a context window size , LLM only studies the cpd of the cases when during training, so it fails to produce an effective cpd at the inference stage when the length of the input sequence is larger than . In other words, the context window size can be seen as an upper limit of the LLM\u2019s capacity for a single processing. Such a limit also exists in the human\u2019s reading comprehension. The human also can hardly understand a very long context by reading it from the beginning to the end at once. In fact, we humans almost never deal with the long context in such an one-shot way. On the contrary, given a long article and a question to be answered, we often use the following method to get the answer:\n\nContext segmentation and key information extraction\n\nSegment the long context first and read it segment by segment with the question. Then quickly determine which segments are relevant to the current task, and construct a short key context by reorganizing the relevant segments. Finally, answer the question based on the short key context. Inspired by the human\u2019s approach to reading and understanding long texts mentioned above, we propose a novel inference framework XL3M, which enables any LLM to reason extremely long sequence without any continual training or fine-tuning. The XL3M framework follows an important principle: the length of the sequence processed by the LLM at one time should not exceed its context window size. 3.1 Less uncertainty implies higher accuracy\n\nGenerally, LLM is trained by optimizing the cross-entropy loss, which forces the LLM\u2019s output cpd to gradually approach the ground-truth one-hot label vector during training. Note that the one-hot 0-1 distribution has the minimal uncertainty, so the procedure of training is also the procedure of reducing the uncertainty of LLM\u2019s prediction. Figure 1 shows the relationship between the cross-entropy loss and the uncertainty of LLM\u2019s output cpd defined by entropy. We can see that the cross-entropy loss and entropy value are highly positively correlated, which means that the certainty of LLM\u2019s prediction implies the accuracy to a large extent. 3.2 Main method\n\nIn this subsection, we introduce the detailed procedure of XL3M. XL3M follows the context segmentation and key information extraction path, that is, extract the relevant context first, and then reason based on the relevant context. Different from StreamLLM and other existing methods that manually discard most tokens and keep only a small part of context, XL3M lets the LLM itself decide what to keep. Decompose long context into short sub-contexts\n\nGiven an LLM and an input sequence whose length is much larger than the LLM\u2019s context window size , similar to Ratner et al. (2023), we first divide the whole sequence into a task sequence and a content sequence , as shown in Figure 2. The task sequence is a few tokens from the end of the original input sequence, and it acts as a \u201cquestion\" to be answered. The content sequence is further segmented into short sequences . To avoid cutting a complete sentence into two parts, we use an overlapped sliding window manner. Then by concatenating each short segment with the task sequence , we obtain sub-contexts , for . For convenience, we call and the head part and end part of the sub-context , respectively. Use the LLM to select relevant segments\n\nWe use the LLM model to compute the local cpd for each sub-context . For efficiency, all the sub-contexts can be processed in parallel. Recall that the certainty of LLM\u2019s prediction is highly correlated to the accuracy. Then we further compute the entropy for each , namely\n\ne \u200b n \u200b t \u200b r \u200b o \u200b p \u200b y \u200b ( p i ) = \u2211 j = 1 v \u2212 p i j \u200b log \u2061 p i j , \ud835\udc52 \ud835\udc5b \ud835\udc61 \ud835\udc5f \ud835\udc5c \ud835\udc5d \ud835\udc66 subscript \ud835\udc5d \ud835\udc56 superscript subscript \ud835\udc57 1 \ud835\udc63 superscript subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 superscript subscript \ud835\udc5d \ud835\udc56 \ud835\udc57 entropy(p_{i})=\\sum_{j=1}^{v}-p_{i}^{j}\\log p_{i}^{j}, (1)\n\nwhere is the dimension of and is the -th element in . A sub-context with small entropy value implies that the segment is relevant to the \u201cquestion\" . We select the sub-contexts with the top-k smallest entropy values and discard all the other noisy context. Then we construct a concise key context by splicing all the selected segments (the head part of the selected sub-contexts) as well as the task sequence in chronological order. By devising suitable segmentation and splicing strategies, we can ensure that the length of the key context is within the training context window. The constructed key context is used instead of the original long context to complete the inference task. Since most irrelevant content is removed and the length of the key context does not exceed the largest training length, both the out-of-domain and distraction issues are addressed. The whole process of XL3M is shown in Figure 2. 4 Evaluation\n\nIn this section, we evaluate the XL3M on the comprehensive benchmark LongBench Bai et al. (2023) and the widely-used long-sequence inference task \u201cNeedle in a Haystack\u201d 111https://github.com/gkamradt/LLMTestNeedleInAHaystack. All the experiments are conducted on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card. Baselines\n\nWe compare the proposed XL3M framework with the non-fine-tuning methods PCW Ratner et al. (2023), StreamLLM Xiao et al. (2023) and the fine-tuning models PI-7B-32k 222https://huggingface.co/togethercomputer/LLaMA-2-7B-32K and Yarn-7B-64k 333https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k. PI-7B-32k is obtained by fine-tuning the Llama2-7B-4k 444https://huggingface.co/meta-llama/Llama-2-7b model on sequences with length 32k, and Yarn-7B-64k is obtained by fine-tuning Mistral-7b-8k 555https://huggingface.co/mistralai/Mistral-7B-v0.1 on sequences with length 64k. How to fairly compare the fine-tuning and non-fine-tuning methods itself is also an open question. Note that the model after fine-tuning actually utilizes more training data, so the model should be much more powerful. Even for short sequence inference, the fine-tuned model still has better performance, compared with the same model before fine-tuning. For a fair comparison, the short sequence model chosen as the base model for the non-fine-tuning methods should have similar performance to the fine-tuned models on short sequence inference tasks. To achieve this goal, we construct a PI-7B-2k model by modifying the maxpositionembeddings hyper-parameter in PI-7B-32k to 2k. The modified PI-7B-2k model has the same performance to PI-7B-32k when the sequence length is not longer than 2k, but it is unable to address sequences longer than 2k. Namely, PI-7B-2k only inherits the capability of PI-7B-32k in short sequence reasoning. For convenience, we use PCW-7B-2k, StreamLLM-7B-2k, XL3M-7B-2k to represent the constructed PI-7B-2k models equipped with the corresponding non-fine-tuning extension methods. Setup\n\nFor XL3M, we use the last 128 tokens of a given sequence as the task sequence, and the rest as the content sequence. The content sequence is uniformly segmented by sliding window with overlap. The sliding window size is 512. The overlap size is 128 and for the last segment the overlap size is adjusted to ensure the consistent length of all the segments. The initial tokens are task prompts, so we further add the initial 128 tokens to the header of each sub-context. We select the sub-contexts with the top-k (k=3) smallest entropy values as relevant sub-contexts, and use them to construct key context. The total length of the key context is 1792, which is within 2k. For PCW, we set the context window to be 1792 and set the number of task tokens to be 128. For StreamLLM, we use the last 1792 tokens as recent tokens and the beginning 128 tokens as sink tokens to ensure that the task prompts are included and meanwhile the total length does not exceed 2k. 4.1 Evaluation on LongBench-E Bai et al. (2023): a multitask benchmark\n\nLongBench is a multitask benchmark which gives a comprehensive assessment of long context understanding capabilities of LLMs. LongBench supports a subset LongBench-E, which features more evenly distributed context lengths. LongBench-E contains six major categories and thirteen different tasks, covering key long-text application scenarios, such as single-document QA, multi-document QA, summarization, few-shot learning, synthetic tasks, and code completion. LongBench-E includes English, Chinese, and code languages. The brief overview of LongBench-E datasets is shown in Table 1. For detailed introduction of LongBench and LongBench-E, one can refer to Bai et al. (2023). In this section, we only evaluate on the English and code tasks in LongBench-E, so the MultiFieldQA-en dataset is removed, since it involves both English and Chinese. We show the performance of all the compared methods on the length of 0-4k, 4-8k, 8k+, respectively. The results are shown in Figure 3. For each major task, we report the average score of all the datasets belonging to it. From the figure, we see that XL3M outperform all the other non-fine-tuning methods in most cases. Meanwhile, XL3M achieves comparable and sometimes even better results compared with the fine-tuning models PI-7B-32k and Yarn-7B-64k. This is mainly because our method can filter out noisy tokens and allows the model to focus on the relevant information. Note that the performance of PCW-2k drops rapidly as the length increases. This is in line with the observation shown in Ratner et al. (2023) that PCW is only effective for a limited range of extension (about three times extension of its original context window size). Surprisingly, StreamLLM can also keep up with the performance of the other methods in some tasks, though it discards most tokens. This may be due to that the sequences in LongBench are relatively short and the answers usually appear near the end. We will evaluate all of these methods on longer sequences and more diverse scenarios in next subsection. 4.2 Evaluation on \u201cNeedle in a Haystack\u201d task\n\n\u201cNeedle in a Haystack\u201d is recently a widely-used task for testing the in-context retrieval ability of long context LLMs. The main procedure of \u201cNeedle in a Haystack\u201d is: 1. Place a random fact or statement (the \u201cneedle\u201d) somewhere in a long context (the \u201cHaystack\u201d); 2. Ask the model to retrieve this statement. We construct different lengths of contexts, varying from 16k to 128k, to measure the performance of all the above methods. Figure 4 shows the recall scores of all the compared methods with the \u201cneedle\u201d placed at ten different ranges of depth. For each range of depth, the result is averaged by ten independent runs with the \u201cneedle\u201d randomly placed in the corresponding range each time. We can see that XL3M exhibits strong performance for all the lengths, while the PI-7B-32k and Yarn-7B-64k models only perform well when the length is within their fine-tuning context window size, and their performance degrades rapidly when the length exceeds the fine-tuning length. For PCW, note that the length considered in this task is 8 to 64 times larger than the context window size of PCW-7B-2k, which is beyond the effective extension range of PCW, so it can hardly retrieval the right answers. StreamLLM does not perform well in this task either. Only when the \u201cneedle\u201d is placed right at the end of the sequence (shallow depth) is it possible to capture the relevant information. This is inline with its mechanism of mainly keeping the recent tokens (tokens near the end) and a few initial prompt tokens. Figure 5 further reports the performance of the proposed XL3M framework over a larger range of length. Meanwhile we investigate the impact of the context window size of the original base model on the performance of the proposed method. Using a similar manner, we construct a PI-7B-4k model by modifying the maxpositionembeddings hyper-parameter in PI-7B-32k to 4k. Compared with previously constructed PI-7B-2k, the only difference is that PI-7B-4k has a larger context window size. We use XL3M-7B-4k to represent the PI-7B-4k model equipped with XL3M. The sliding window size is enlarged to 1024. All the other settings remain unchanged, so the total length of the key context is 3328, less than 4k. From the figure, we see that, XL3M-7B-2k performs well in most lengths and depths, but in some cases, it does not get the accuracy, while XL3M-7B-4k can almost retrieve the right answers for all the cases. Though the proposed XL3M framework can enable any LLM to reason long sequences, the base model with a larger context window size allows a larger sliding window, which generally contributes to achieving better performance. Moreover, the XL3M framework is very memory efficient. The sequence length can be up to 20M or even larger with only 8 NPU cards. We also test our XL3M framework using a much larger-scale model Llama-65B 666https://huggingface.co/huggyllama/llama-65b. The pre-training contex window size of Llama-65B is 2k, so we follow the settings of XL3M-7B-2k to pre-process the input context. We use XL3M-65B-2k to represent the Llama-65B model applied with XL3M. The results are shown in Figure 6. We see that XL3M-65B-2k can recall the answer for all the cases. This is in line with our expectation. Since Llama-65B is much more powerful than Llama2-7B, it deserves to achieve better performance. 4.3 Evaluation on time efficiency\n\nWe compare the time efficiency of the proposed XL3M framework with the baselines, in terms of both prefill time (time consumption for generating the first token) and decoding time (time consumption for generating all the tokens except the first token). We evaluate all the methods on a 128k long \u201cNeedle in a Haystack\u201d task, and we set the decoding length to be 128. The compared results are shown in Table 2. The StreamLLM and XL3M methods are much more time efficient than the other competitors. However, StreamLLM will introduce severe precision loss, while XL3M can ensure both efficiency and effectiveness. 5 Conclusion\n\nWe empirically found that the accuracy of the LLM\u2019s prediction is highly correlated to its certainty measured by entropy. Based on this, we proposed a novel inference framework XL3M, which enables any LLM to break the length limit without any continual training or fine-tuning. In the XL3M framework, any input long context will be decomposed into multiple short sub-contexts containing a common \u201cquestion\u201d which is a few tokens from the end of the original input context. XL3M provides a method to extract the sub-contexts relevant to the current task and discard most irrelevant context. Then a concise key context is constructed by splicing the relevant sub-contexts in chronological order. The constructed key context is further used instead of the original context to complete the inference task. Experimental results demonstrated the effectiveness and efficiency of XL3M. We showed that equipped with XL3M framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card. Limitations of the proposed framework\n\nThe XL3M framework assumes that only a small portion of the original context is relevant to the given task or question, namely the length of the key context is less than the training context window size. Hence, when the LLM\u2019s training context window is very small and relevant context contains a significant number of tokens, some key tokens has to be discarded. Moreover, when the relevant content is widely distributed in different part of the original context, it is difficult to capture all of the key context by only selecting a few segments. For these cases, XL3M may not achieve satisfactory performance. References\n\nAnil et al. (2022) Anil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramasesh, V., Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546\u201338556, 2022. Bai et al. (2023) Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark for long context understanding.",
    "xl3m-10": "arXiv preprint arXiv:2308.14508, 2023. Bertsch et al. (2024) Bertsch, A., Alon, U., Neubig, G., and Gormley, M. Unlimiformer: Long-range transformers with unlimited length input.",
    "xl3m-11": "Advances in Neural Information Processing Systems, 36, 2024. Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. Chen et al. (2023) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Han et al. (2023) Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models.",
    "xl3m-12": "arXiv preprint arXiv:2308.16137, 2023. Kaddour et al. (2023) Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., and McHardy, R. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. Kazemnejad et al. (2023) Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., and Reddy, S. The impact of positional encoding on length generalization in transformers. arXiv preprint arXiv:2305.19466, 2023. Li et al. (2023) Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise on context length, 2023. Munkhdalai et al. (2024) Munkhdalai, T., Faruqui, M., and Gopal, S. Leave no context behind: Efficient infinite context transformers with infini-attention.",
    "xl3m-13": "arXiv preprint arXiv:2404.07143, 2024. Naveed et al. (2023) Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Barnes, N., and Mian, A. A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023. Pal et al. (2023) Pal, A., Karkhanis, D., Roberts, M., Dooley, S., Sundararajan, A., and Naidu, S. Giraffe: Adventures in expanding context lengths in llms.",
    "xl3m-14": "arXiv preprint arXiv:2308.10882, 2023. Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models.",
    "xl3m-15": "arXiv preprint arXiv:2309.00071, 2023. Press et al. (2021) Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. Ratner et al. (2023) Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Magar, I., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., and Shoham, Y. Parallel context windows for large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6383\u20136402, 2023. Scao et al. (2022) Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., Castagn\u00e9, R., Luccioni, A. S., Yvon, F., Gall\u00e9, M., et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. Su et al. (2021) Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding.",
    "xl3m-16": "arXiv preprint arXiv:2104.09864, 2021. Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models.",
    "xl3m-17": "arXiv preprint arXiv:2302.13971, 2023. Tworkowski et al. (2023) Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wei et al. (2023) Wei, C., Wang, Y.-C., Wang, B., and Kuo, C.-C. J. An overview on language models: Recent developments and outlook. arXiv preprint arXiv:2303.05759, 2023. Wu et al. (2022) Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Xiao et al. (2024) Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory.",
    "xl3m-18": "arXiv preprint arXiv:2402.04617, 2024. Xiao et al. (2023) Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks.",
    "xl3m-19": "arXiv preprint arXiv:2309.17453, 2023. Xiong et al. (2023) Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. Zhao et al. (2023) Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Jun 5 14:11:46 2024 by LaTeXML"
}