{
    "xl3m-0": "# XL ${ }^{3}$ M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference \n\n\\author{\nShengnan Wang * <br> Theory Lab, 2012 Labs <br> Huawei Technologies Co., Ltd <br> Youhui Bai <br> Theory Lab, 2012 Labs <br> Huawei Technologies Co., Ltd <br> Lin Zhang <br> Theory Lab, 2012 Labs <br> Huawei Technologies Co., Ltd\n\n## Pingyi Zhou\n\n <br> Huawei Technologies Co., Ltd.",
    "xl3m-1": "## Gong Zhang\n\n <br> Theory Lab, 2012 Labs <br> Huawei Technologies Co., Ltd <br> Renhai Chen <br> Theory Lab, 2012 Labs <br> Huawei Technologies Co., Ltd <br> Shixiong Zhao <br> Theory Lab, 2012 Labs <br> Huawei Technologies Co., Ltd\n## Sen Wang\n\n <br> Theory Lab, 2012 Labs <br> Huawei Technologies Co., Ltd\n## Hua Xu\n\n <br> Huawei Technologies Co., Ltd <br> Hongwei Sun <br> Huawei Technologies Co., Ltd}\n\n\n#### Abstract\n\nLength generalization failure problem, namely the large language model (LLM) fails to generalize to texts longer than its maximum training length, greatly restricts the application of LLM in the scenarios with streaming long inputs.",
    "xl3m-2": "To address this problem, the existing methods either require substantial costs or introduce precision loss. In this paper, we empirically find that the accuracy of the LLM's prediction is highly correlated to its certainty. Based on this, we propose an efficient trainingfree framework, named $\\mathrm{XL}^{3} \\mathrm{M}$ (it means extra-long large language model), which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning. Under the $\\mathrm{XL}^{3} \\mathrm{M}$ framework, the input context will be firstly decomposed into multiple short sub-contexts, where each sub-context contains an independent segment and a common \"question\" which is a few tokens from the end of the original context. Then $\\mathrm{XL}^{3} \\mathrm{M}$ gives a method to measure the relevance between each segment and the \"question\", and constructs a concise key context by splicing all the relevant segments in chronological order. The key context is further used instead of the original context to complete the inference task. Evaluations on comprehensive benchmarks show the superiority of $\\mathrm{XL}^{3} \\mathrm{M}$. Using our framework, a Llama2-7B model is able to reason 20 M long\n\n\n[^0]sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card. ## 1 Introduction\n\nTransformer based large language models (LLMs) Vaswani et al.",
    "xl3m-3": "(2017); Brown et al. (2020); Touvron et al. (2023); Scao et al. (2022) have shown their impressive performance in many language tasks Zhao et al. (2023). However, due to the out-of-domain and distraction issues Xiao et al. (2024), the quality of the LLM's generation drops dramatically when the sequence length surpasses its context window size which is the largest training length. Such a drawback hinders the application of LLM in multi-round dialogue, conversation conduction, documents summarization, and other real tasks which often encounter very long sequences. Some pioneering works have been done for context length extrapolation. Most of them focused on optimizing the positional encoding (PE), since the PE of unseen length was identified as a major factor leading to length generalization failure. Compared with the vanilla absolute PE, the later proposed relative PE Raffel et al. (2020); Su et al. (2021), ALiBi Press et al. (2021), and NoPE Kazemnejad et al. (2023) were demonstrated to offer better generalization. However, all of them do not perform well when the sequence length is significantly longer than the largest training length. A more effective approach is to continually train or fine-tune the model on longer-length data Chen et al. (2023); Peng et al. (2023). Nevertheless, such a manner can only extend the context window to a limited length due to unacceptable training costs Xiong et al. (2023). Moreover, when the length is very long, even collecting the training data itself is a difficult task. Recently, some training-free length extension methods attracted widespread attention. LM-Infinite Han et al. (2023) and StreamLLM Xiao et al. (2023) extrapolated the length by discarding most contexts but only keeping the context at the end and the very beginning. Though these methods can efficiently deal with extremely long contexts, they lose a lot of long-distance dependencies, which leads to deviations or even errors in text understanding. PCW Ratner et al. (2023) designed chunked attention mask and reused the positional encoding for different chunks, which alleviated the restriction of the context window.",
    "xl3m-4": "However, PCW can only extend the context window to a very limited length, and the effectiveness of PCW needs to be further studied Ratner et al. (2023). In this work, we propose an efficient training-free inference framework named $\\mathrm{XL}^{3} \\mathrm{M}$ (it means extra-long large language model) which enables the LLM to break the length limit. The effectiveness of the $\\mathrm{XL}^{3} \\mathrm{M}$ framework is due to an important principle: the length of the sequence processed by the LLM at one time should not exceed the its context window size. The contributions of this paper are summarized as follows:\n\n- We empirically find that the accuracy of the LLM's prediction is highly correlated to its certainty measured by entropy. Based on this, we propose $\\mathrm{XL}^{3} \\mathrm{M}$, a novel inference framework enabling any LLM to read and understand extremely long sequences. Inspired by the human's habit of reading in segments, under our framework, each input long context will be decomposed into multiple short sub-contexts with a common \"question\" to be answered. For each sub-context, we use the LLM to compute the local conditional probability distribution (cpd) as well as its corresponding entropy. Then the relevant sub-contexts with small entropy values are selected and reorganized into key context in chronological order. Since most irrelevant context is removed, LLM can generate high-quality results according to the extracted key context. - We evaluate the proposed framework on comprehensive LongBench tasks and the widelyused \"Needle in a Haystack\" task. We compare the performance of $\\mathrm{XL}^{3} \\mathrm{M}$ with the stateof-the-art methods, including both fine-tuning and non-fine-tuning methods. The results demonstrate the superiority of the proposed framework. - The proposed $\\mathrm{XL}^{3} \\mathrm{M}$ framework does not modify the main the structure of the LLM, and it does not need any additional training or fine-tuning. It is both memory and time efficient. Under the $\\mathrm{XL}^{3} \\mathrm{M}$ framework, the LLM is able to reason sequences longer than 20 M on an 8 -card Huawei Ascend 910B NPU machine with 64GB memory per card. ## 2 Related work\n\nDue to the strong demand of long sequence inference, a lot of context window extension techniques have been proposed Naveed et al.",
    "xl3m-5": "(2023); Kaddour et al. (2023). These methods can be mainly divided into three categories: 1) Extension by fine-tuning; 2) Extension without fine-tuning; 3) Extension by external memory Bertsch et al.",
    "xl3m-6": "(2024); Wu et al.",
    "xl3m-7": "(2022); Xiao et al. (2024). ### 2.1 Extension by fine-tuning\n\nThe LLMs are generally trained on relatively short sequences, due to the expensive computational and memory requirements (quadratic with the sequence length) in the attention mechanism, and LLMs fail to generalize to unseen lengths at the inference phase Han et al. (2023); Chen et al. (2023). A straightforward idea for length generalization is to fine-tune the model on longer sequences. However, it was found that naively fine-tuning a pre-trained LLM for window extrapolation is less effective and inefficient Kaddour et al.",
    "xl3m-8": "(2023); Anil et al. (2022). Chen et al. (2023) showed that using position interpolation (PI) rather than extrapolation during fine-tuning can extend the context window of the pre-trained LLMs to 32k without performance loss. Further, Yarn Peng et al. (2023) proposed a novel NTK-aware interpolation method and achieved tens of times extension of the context window size. Other fine-tuning based methods include Giraffe Pal et al. (2023), FoT Tworkowski et al. (2023), and so on. However, this kind of method requires massive training resources since it needs to train LLMs on long-sequence data. In addition, collecting enough long-sequence data itself for fine-tuning is also a challenging work if one wants to extend the context window to extremely long. ### 2.2 Extension without fine-tuning\n\nTo save resources, some training-free context window extension methods are proposed. At the very beginning, most researchers focused on optimizing the positional encoding. The vanilla absolute position encoding strictly restricts the reasoning length of LLM. To tackle this issue, a lot of advanced position encoding schemes were proposed, such as RoPE Su et al. (2021), ALiBi Press et al. (2021), and the recently proposed NoPE Kazemnejad et al. (2023). However, all of them only make the model architecturally-able to deal with long inputs rather than actually perform well on long-sequence reasoning tasks Li et al.",
    "xl3m-9": "(2023); Kaddour et al. (2023). Instead of encoding the unseen length, StreamLLM Xiao et al. (2023) chose to discard most context but only keep the recent tokens (tokens at the end) and sink tokens (tokens at the very beginning), ensuring that the total length of the remaining context does not exceed the LLM's window size. Such a method not only enabled the LLM to deal with longer context, but also achieved a remarkable speedup. Similar idea was also adopted by LM-Infinite Han et al. (2023). However, both StreamLLM and LM-Infinite missed a lot of long-distance dependencies, leading to deviations or even errors in text understanding. The most related work should be parallel context windows (PCW) Ratner et al. (2023). By modifying position embedding and attention mask, PCW alleviated the context window restriction for any off-the-shelf LLM without further training. However, it was shown that PCW is only effective for a limited extension (about three times extension of its original context window size), and the performance degrades when extending to a much longer length. Moreover, the effectiveness of PCW was only demonstrated on tasks like multi-class tasks classification and information extraction. It remains an open question whether it is suitable for more general tasks. ### 2.3 Extension by external memory\n\nUnlike the previous approaches which keep the main architecture of the model unchanged, the methods in this category usually involve modifications to the model. Generally, this kind of method introduces the external memory to restore the information of the past context, and retrieves the relevant tokens from the memory for generation based on some search mechanism, such as KNN Bertsch et al.",
    "xl3m-10": "(2024); Wu et al. (2022). The main disadvantage is that these methods require additional memory overhead, and they usually need further training or fine-tuning to ensure the effectiveness.",
    "xl3m-11": "Xiao et al. (2024) and Munkhdalai et al. (2024) respectively proposed an offload mechanism and a compression mechanism to reduce memory pressure. In this paper, we aim to propose an effective training-free inference framework which enables any LLM to reason extremely long sequence. ## $3 \\mathrm{XL}^{3} \\mathrm{M}$ : extra-long large language model\n\nLanguage model, including LLM, studies a conditional probability distribution (cpd) $p\\left(x_{t+1} \\mid X_{t}\\right)$ given string of texts $X_{t}=\\left[x_{1}, \\ldots, x_{t}\\right.$ Wei et al. (2023). In the inference phase, given an input sequence, LLM also first computes the cpd and then generates a token according to a predetermined generation mode, such as greedy search, top-k search, top-p search, etc. Since the length of the training data is limited within a context window size $C$, LLM only studies the cpd of the cases when $t \\leq C$ during training, so it fails to produce an effective cpd at the inference stage when the length of the input sequence is larger than $C$. In other words, the context window size $C$ can be seen as an upper limit of the LLM's capacity for a single processing. Such a limit also exists in the human's reading comprehension. The human also can hardly understand a very long context by reading it from the beginning to the end at once. In fact, we humans almost never deal with the long context in such an one-shot way. On the contrary, given a long article and a question to be answered, we often use the following method to get the answer:\n\nContext segmentation and key information extraction Segment the long context first and read it segment by segment with the question. Then quickly determine which segments are relevant to the current task, and construct a short key context by reorganizing the relevant segments. Finally, answer the question based on the short key context. Inspired by the human's approach to reading and understanding long texts mentioned above, we propose a novel inference framework $\\mathrm{XL}^{3} \\mathrm{M}$, which enables any LLM to reason extremely long sequence without any continual training or fine-tuning. The $\\mathrm{XL}^{3} \\mathrm{M}$ framework follows an important principle: the length of the sequence processed by the LLM at one time should not exceed its context window size. ### 3.1 Less uncertainty implies higher accuracy\n\nGenerally, LLM is trained by optimizing the cross-entropy loss, which forces the LLM's output cpd $p\\left(x_{t+1} \\mid X_{t}\\right)$ to gradually approach the ground-truth one-hot label vector during training. Note that the one-hot $0-1$ distribution has the minimal uncertainty, so the procedure of training is also the procedure of reducing the uncertainty of LLM's prediction. Figure 1 shows the relationship between the cross-entropy loss and the uncertainty of LLM's output cpd defined by entropy. We can see that the cross-entropy loss and entropy value are highly positively correlated, which means that the certainty of LLM's prediction implies the accuracy to a large extent. ![](https://cdn.mathpix.com/cropped/2024_09_17_52acad32720102f61d15g-04.jpg?height=456&width=616&top_left_y=1970&top_left_x=749)\n\nFigure 1: The relationship between the accuracy and certainty of LLM's prediction. ### 3.2 Main method\n\nIn this subsection, we introduce the detailed procedure of $\\mathrm{XL}^{3} \\mathrm{M} . \\mathrm{XL}^{3} \\mathrm{M}$ follows the context segmentation and key information extraction path, that is, extract the relevant context first, and then reason based on the relevant context. Different from StreamLLM and other existing methods that manually discard most tokens and keep only a small part of context, $\\mathrm{XL}^{3} \\mathrm{M}$ lets the LLM itself decide what to keep. Decompose long context into short sub-contexts Given an LLM $\\Phi(\\cdot \\mid \\theta)$ and an input sequence $X$ whose length is much larger than the LLM's context window size $C$, similar to Ratner et al. (2023), we first divide the whole sequence into a task sequence $X_{t}$ and a content sequence $X_{c}$, as shown in Figure 2. The task sequence is a few tokens from the end of the original input sequence, and it acts as a \"question\" to be answered. The content sequence is further segmented into $m$ short sequences $X_{c}^{1}, X_{c}^{2}, \\ldots, X_{c}^{m}$. To avoid cutting a complete sentence into two parts, we use an overlapped sliding window manner. Then by concatenating each short segment $X_{c}^{i}$ with the task sequence $X_{t}$, we obtain $m$ sub-contexts $X^{i}=\\left[X_{c}^{i}, X_{t}\\right]$, for $i=1,2, \\ldots, m$. For convenience, we call $X_{c}^{i}$ and $X_{t}$ the head part and end part of the sub-context $X^{i}$, respectively. Use the LLM to select relevant segments We use the LLM model $\\Phi(\\cdot \\mid \\theta)$ to compute the local cpd $p_{i}=\\Phi\\left(X^{i} \\mid \\theta\\right)$ for each sub-context $X^{i}$. For efficiency, all the sub-contexts can be processed in parallel. Recall that the certainty of LLM's prediction is highly correlated to the accuracy. Then we further compute the entropy for each $p_{i}$, namely\n\n$$\n\\operatorname{entropy}\\left(p_{i}\\right)=\\sum_{j=1}^{v}-p_{i}^{j} \\log p_{i}^{j}\n$$\n\nwhere $v$ is the dimension of $p_{i}$ and $p_{i}^{j}$ is the $j$-th element in $p_{i}$. A sub-context $X^{i}$ with small entropy value implies that the segment $X_{c}^{i}$ is relevant to the \"question\" $X_{t}$. We select the sub-contexts $X^{i}$ with the top-k smallest entropy values and discard all the other noisy context. Then we construct a concise key context by splicing all the selected segments $X_{c}^{i}$ (the head part of the selected sub-contexts) as well as the task sequence $X_{t}$ in chronological order. By devising suitable segmentation and splicing strategies, we can ensure that the length of the key context is within the training context window. The constructed key context is used instead of the original long context to complete the inference task. Since most irrelevant content is removed and the length of the key context does not exceed the largest training length, both the out-of-domain and distraction issues are addressed. The whole process of $\\mathrm{XL}^{3} \\mathrm{M}$ is shown in Figure 2\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_52acad32720102f61d15g-05.jpg?height=570&width=1047&top_left_y=1832&top_left_x=542)\n\nFigure 2: The main procedure of $\\mathrm{XL}^{3} \\mathrm{M}$. ## 4 Evaluation\n\nIn this section, we evaluate the $\\mathrm{XL}^{3} \\mathrm{M}$ on the comprehensive benchmark LongBench Bai et al. (2023) and the widely-used long-sequence inference task \"Needle in a Haystack\" ${ }^{2}$ All the experiments are conducted on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card. Baselines We compare the proposed $\\mathrm{XL}^{3} \\mathrm{M}$ framework with the non-fine-tuning methods PCW Ratner et al. (2023), StreamLLM Xiao et al. (2023) and the fine-tuning models PI-7B-32k 3 and Yarn-7B-64k ${ }_{4}^{4}$ PI-7B-32k is obtained by fine-tuning the Llama2-7B-4k ${ }^{5}$ model on sequences with length 32 k , and Yarn-7B-64k is obtained by fine-tuning Mistral-7b-8k ${ }^{6}$ on sequences with length 64 k . How to fairly compare the fine-tuning and non-fine-tuning methods itself is also an open question. Note that the model after fine-tuning actually utilizes more training data, so the model should be much more powerful. Even for short sequence inference, the fine-tuned model still has better performance, compared with the same model before fine-tuning. For a fair comparison, the short sequence model chosen as the base model for the non-fine-tuning methods should have similar performance to the fine-tuned models on short sequence inference tasks. To achieve this goal, we construct a PI-7B-2k model by modifying the max_position_embeddings hyper-parameter in PI-7B-32k to 2 k . The modified PI-7B-2k model has the same performance to PI-7B-32k when the sequence length is not longer than 2 k , but it is unable to address sequences longer than 2 k . Namely, PI-7B-2k only inherits the capability of PI-7B-32k in short sequence reasoning. For convenience, we use PCW-7B-2k, StreamLLM-7B-2k, XL ${ }^{3}$ M-7B-2k to represent the constructed PI-7B-2k models equipped with the corresponding non-fine-tuning extension methods. Setup For $\\mathrm{XL}^{3} \\mathrm{M}$, we use the last 128 tokens of a given sequence as the task sequence, and the rest as the content sequence.",
    "xl3m-12": "The content sequence is uniformly segmented by sliding window with overlap. The sliding window size is 512 . The overlap size is 128 and for the last segment the overlap size is adjusted to ensure the consistent length of all the segments. The initial tokens are task prompts, so we further add the initial 128 tokens to the header of each sub-context. We select the sub-contexts with the top- $\\mathrm{k}(\\mathrm{k}=3)$ smallest entropy values as relevant sub-contexts, and use them to construct key context. The total length of the key context is 1792 , which is within 2 k . For PCW, we set the context window $n_{\\max }$ to be 1792 and set the number of task tokens to be 128 . For StreamLLM, we use the last 1792 tokens as recent tokens and the beginning 128 tokens as sink tokens to ensure that the task prompts are included and meanwhile the total length does not exceed 2 k . ### 4.1 Evaluation on LongBench-E Bai et al. (2023): a multitask benchmark\n\nLongBench is a multitask benchmark which gives a comprehensive assessment of long context understanding capabilities of LLMs. LongBench supports a subset LongBench-E, which features more evenly distributed context lengths. LongBench-E contains six major categories and thirteen different tasks, covering key long-text application scenarios, such as single-document QA , multidocument QA, summarization, few-shot learning, synthetic tasks, and code completion. LongBench-E includes English, Chinese, and code languages. The brief overview of LongBench-E datasets is shown in Table 1 . For detailed introduction of LongBench and LongBench-E, one can refer to Bai et al. (2023). In this section, we only evaluate on the English and code tasks in LongBench-E, so the MultiFieldQA-en dataset is removed, since it involves both English and Chinese. We show the performance of all the compared methods on the length of $0-4 \\mathrm{k}, 4-8 \\mathrm{k}, 8 \\mathrm{k}+$, respectively. The results are shown in Figure 3 For each major task, we report the average score of all the datasets belonging to it. From the figure, we see that $\\mathrm{XL}^{3} \\mathrm{M}$ outperform all the other non-fine-tuning methods in most cases. Meanwhile, $\\mathrm{XL}^{3} \\mathrm{M}$ achieves comparable and sometimes even better results compared with the fine-tuning models PI-7B-32k and Yarn-7B-64k. This is mainly because our method can filter out noisy tokens and allows the model to focus on the relevant information. Note that the performance of PCW-2k drops rapidly as the length increases. This is in line with the observation shown in Ratner et al. (2023) that PCW is only effective for a limited range of extension (about three\n\n[^1]Table 1: Basic information of the dataset statistics in LongBench-E, including the data length distribution, metric, and language type. | Dataset | $0-4 \\mathrm{k}$ | 4-8k | $8 \\mathrm{k}+$ | Metric | Language |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| Single-Document QA |  |  |  |  |  |\n| Qasper | 100 | 100 | 24 | F1 | English |\n| MultiFieldQA-en | 67 | 70 | 13 | F1 | English/Chinese |\n| Multi-Document QA |  |  |  |  |  |\n| HotpotQA | 100 | 100 | 100 | F1 | English |\n| 2WikiMultihopQA | 100 | 100 | 100 | F1 | English |\n| Summarization |  |  |  |  |  |\n| GovReport | 100 | 100 | 100 | Rouge-L | English |\n| MultiNews | 100 | 100 | 94 | Rouge-L | English |\n| Few-shot Learning |  |  |  |  |  |\n| TREC | 100 | 100 | 100 | Accuracy (CLS) | English |\n| TriviaQA | 100 | 100 | 100 | F1 | English |\n| SAMSum | 100 | 100 | 100 | Rouge-L | English |\n| Synthetic Task |  |  |  |  |  |\n| PassageCount | 100 | 100 | 100 | Accuracy (EM) | English |\n| PassageRetrieval-en | 100 | 100 | 100 | Accuracy (EM) | English |\n| Code Completion |  |  |  |  |  |\n| LCC | 100 | 100 | 100 | Edit Sim | Python/C\\#/Java |\n| RepoBench-P | 100 | 100 | 100 | Edit Sim | Python/Java |\n\ntimes extension of its original context window size). Surprisingly, StreamLLM can also keep up with the performance of the other methods in some tasks, though it discards most tokens. This may be due to that the sequences in LongBench are relatively short and the answers usually appear near the end. We will evaluate all of these methods on longer sequences and more diverse scenarios in next subsection. ![](https://cdn.mathpix.com/cropped/2024_09_17_52acad32720102f61d15g-07.jpg?height=694&width=1202&top_left_y=1472&top_left_x=491)\n\nFigure 3: Average score (\\%) under different context length on LongBench-E. ### 4.2 Evaluation on \"Needle in a Haystack\" task\n\n\"Needle in a Haystack\" is recently a widely-used task for testing the in-context retrieval ability of long context LLMs. The main procedure of \"Needle in a Haystack\" is: 1. Place a random fact or\nstatement (the \"needle\") somewhere in a long context (the \"Haystack\"); 2. Ask the model to retrieve this statement. We construct different lengths of contexts, varying from 16 k to 128 k , to measure the performance of all the above methods. Figure 4 shows the recall scores of all the compared methods with the \"needle\" placed at ten different ranges of depth. For each range of depth, the result is averaged by ten independent runs with the \"needle\" randomly placed in the corresponding range each time. We can see that $\\mathrm{XL}^{3} \\mathrm{M}$ exhibits strong performance for all the lengths, while the PI-7B-32k and Yarn-7B-64k models only perform well when the length is within their fine-tuning context window size, and their performance degrades rapidly when the length exceeds the fine-tuning length. For PCW, note that the length considered in this task is 8 to 64 times larger than the context window size of PCW-7B-2k, which is beyond the effective extension range of PCW, so it can hardly retrieval the right answers. StreamLLM does not perform well in this task either. Only when the \"needle\" is placed right at the end of the sequence (shallow depth) is it possible to capture the relevant information. This is inline with its mechanism of mainly keeping the recent tokens (tokens near the end) and a few initial prompt tokens. ![](https://cdn.mathpix.com/cropped/2024_09_17_52acad32720102f61d15g-08.jpg?height=448&width=1357&top_left_y=918&top_left_x=383)\n\nFigure 4: Pressure test on \"Needle in a Haystack\". The test was run at 4 different lengths $(16 \\mathrm{k} \\rightarrow$ 128 k ) and 10 different ranges of document depth (buttom $\\rightarrow$ top). Each result is average by 10 independent runs. Figure 5 further reports the performance of the proposed $\\mathrm{XL}^{3} \\mathrm{M}$ framework over a larger range of length. Meanwhile we investigate the impact of the context window size of the original base model on the performance of the proposed method. Using a similar manner, we construct a PI-7B-4k model by modifying the max_position_embeddings hyper-parameter in PI-7B-32k to 4 k . Compared with previously constructed PI-7B-2k, the only difference is that PI-7B-4k has a larger context window size. We use $\\mathrm{XL}^{3} \\mathrm{M}-7 \\mathrm{~B}-4 \\mathrm{k}$ to represent the PI-7B-4k model equipped with $\\mathrm{XL}^{3} \\mathrm{M}$. The sliding window size is enlarged to 1024 . All the other settings remain unchanged, so the total length of the key context is 3328 , less than $4 k$. From the figure, we see that, $\\mathrm{XL}^{3} \\mathrm{M}-7 \\mathrm{~B}-2 \\mathrm{k}$ performs well in most lengths and depths, but in some cases, it does not get the $100 \\%$ accuracy, while $\\mathrm{XL}^{3} \\mathrm{M}-7 \\mathrm{~B}-4 \\mathrm{k}$ can almost retrieve the right answers for all the cases. Though the proposed $\\mathrm{XL}^{3} \\mathrm{M}$ framework can enable any LLM to reason long sequences, the base model with a larger context window size allows a larger sliding window, which generally contributes to achieving better performance. Moreover, the $\\mathrm{XL}^{3} \\mathrm{M}$ framework is very memory efficient. The sequence length can be up to 20 M or even larger with only 8 NPU cards. ![](https://cdn.mathpix.com/cropped/2024_09_17_52acad32720102f61d15g-08.jpg?height=307&width=1303&top_left_y=2123&top_left_x=454)\n\nFigure 5: Pressure test on \"Needle in a Haystack\" over a larger range of lengths. Left: recall accuracy of $\\mathrm{XL}^{3} \\mathrm{M}-7 \\mathrm{~B}-2 \\mathrm{k}$. Right: recall accuracy of $\\mathrm{XL}^{3} \\mathrm{M}-7 \\mathrm{~B}-4 \\mathrm{k}$. Table 2: Time efficiency of compared methods. | Models | Total time (s) | Prefill time (s) | Decoding time (s) |\n| :--- | :---: | :---: | :---: |\n| PI-7B-32k | 118.3 | 84.2 | 34.1 |\n| Yarn-7B-64k | 132.7 | 82.7 | 50.0 |\n| PCW-7B-2k | 51.2 | 17.8 | 33.4 |\n| StreamLLM-7B-2k | 17.4 | 4.3 | 13.1 |\n| XL $^{3}$ M-7B-2k | 35.3 | 23.1 | 12.2 |\n\nWe also test our $\\mathrm{XL}^{3} \\mathrm{M}$ framework using a much larger-scale model Llama-65B ${ }^{7}$ The pre-training contex window size of Llama-65B is 2 k , so we follow the settings of $\\mathrm{XL}^{3} \\mathrm{M}-7 \\mathrm{~B}-2 \\mathrm{k}$ to pre-process the input context. We use $\\mathrm{XL}^{3} \\mathrm{M}-65 \\mathrm{~B}-2 \\mathrm{k}$ to represent the Llama-65B model applied with $\\mathrm{XL}^{3} \\mathrm{M}$. The results are shown in Figure 6 We see that $\\mathrm{XL}^{3} \\mathrm{M}-65 \\mathrm{~B}-2 \\mathrm{k}$ can $100 \\%$ recall the answer for all the cases. This is in line with our expectation. Since Llama-65B is much more powerful than Llama2-7B, it deserves to achieve better performance. ![](https://cdn.mathpix.com/cropped/2024_09_17_52acad32720102f61d15g-09.jpg?height=544&width=1142&top_left_y=1029&top_left_x=513)\n\nFigure 6: Test the performance of the proposed methods on Llama-65B-2k. Our methods achieve $100 \\%$ recalls on the \"Needle in a Haystack\" task for all the cases. ### 4.3 Evaluation on time efficiency\n\nWe compare the time efficiency of the proposed $\\mathrm{XL}^{3} \\mathrm{M}$ framework with the baselines, in terms of both prefill time (time consumption for generating the first token) and decoding time (time consumption for generating all the tokens except the first token). We evaluate all the methods on a 128 k long \"Needle in a Haystack\" task, and we set the decoding length to be 128. The compared results are shown in Table 2. The StreamLLM and $\\mathrm{XL}^{3} \\mathrm{M}$ methods are much more time efficient than the other competitors. However, StreamLLM will introduce severe precision loss, while $\\mathrm{XL}^{3} \\mathrm{M}$ can ensure both efficiency and effectiveness. ## 5 Conclusion\n\nWe empirically found that the accuracy of the LLM's prediction is highly correlated to its certainty measured by entropy. Based on this, we proposed a novel inference framework $\\mathrm{XL}^{3} \\mathrm{M}$, which enables any LLM to break the length limit without any continual training or fine-tuning. In the $\\mathrm{XL}^{3} \\mathrm{M}$ framework, any input long context will be decomposed into multiple short sub-contexts containing a common \"question\" which is a few tokens from the end of the original input context. $\\mathrm{XL}^{3} \\mathrm{M}$ provides a method to extract the sub-contexts relevant to the current task and discard most irrelevant context. [^2]Then a concise key context is constructed by splicing the relevant sub-contexts in chronological order. The constructed key context is further used instead of the original context to complete the inference task. Experimental results demonstrated the effectiveness and efficiency of $\\mathrm{XL}^{3} \\mathrm{M}$. We showed that equipped with $\\mathrm{XL}^{3} \\mathrm{M}$ framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card. Limitations of the proposed framework The $\\mathrm{XL}^{3} \\mathrm{M}$ framework assumes that only a small portion of the original context is relevant to the given task or question, namely the length of the key context is less than the training context window size. Hence, when the LLM's training context window is very small and relevant context contains a significant number of tokens, some key tokens has to be discarded. Moreover, when the relevant content is widely distributed in different part of the original context, it is difficult to capture all of the key context by only selecting a few segments. For these cases, $\\mathrm{XL}^{3} \\mathrm{M}$ may not achieve satisfactory performance. ## References\n\nAnil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramasesh, V., Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B. Exploring length generalization in large language models. Advances in Neural Information Processing Systems, 35:38546-38556, 2022. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Bertsch, A., Alon, U., Neubig, G., and Gormley, M. Unlimiformer: Long-range transformers with unlimited length input.",
    "xl3m-13": "Advances in Neural Information Processing Systems, 36, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., and McHardy, R. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023. Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., and Reddy, S. The impact of positional encoding on length generalization in transformers. arXiv preprint arXiv:2305.19466, 2023. Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise on context length, 2023. Munkhdalai, T., Faruqui, M., and Gopal, S. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 2024. Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Barnes, N., and Mian, A. A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435, 2023. Pal, A., Karkhanis, D., Roberts, M., Dooley, S., Sundararajan, A., and Naidu, S. Giraffe: Adventures in expanding context lengths in llms. arXiv preprint arXiv:2308.10882, 2023. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models.",
    "xl3m-14": "arXiv preprint arXiv:2309.00071, 2023. Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020. Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Magar, I., Abend, O., Karpas, E., Shashua, A., LeytonBrown, K., and Shoham, Y. Parallel context windows for large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6383-6402, 2023. Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., Castagn\u00e9, R., Luccioni, A. S., Yvon, F., Gall\u00e9, M., et al. Bloom: A 176b-parameter open-access multilingual language model.",
    "xl3m-15": "arXiv preprint arXiv:2211.05100, 2022. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding.",
    "xl3m-16": "arXiv preprint arXiv:2104.09864, 2021. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models.",
    "xl3m-17": "arXiv preprint arXiv:2302.13971, 2023. Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wei, C., Wang, Y.-C., Wang, B., and Kuo, C.-C. J. An overview on language models: Recent developments and outlook. arXiv preprint arXiv:2303.05759, 2023. Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with trainingfree memory.",
    "xl3m-18": "arXiv preprint arXiv:2402.04617, 2024. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. [^0]:    *Correspondence to: wangshengnan12@huawei.com. [^1]:    ${ }^{2}$ https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n    ${ }^{3} \\mathrm{https}: / /$ huggingface.co/togethercomputer/LLaMA-2-7B-32K\n    ${ }^{4}$ https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k\n    ${ }^{5}$ https://huggingface.co/meta-llama/Llama-2-7b\n    ${ }^{6} \\mathrm{https}: / /$ huggingface.co/mistralai/Mistral-7B-v0.1\n\n[^2]:    ${ }^{7}$ https://huggingface.co/huggyllama/llama-65b\n\n"
}