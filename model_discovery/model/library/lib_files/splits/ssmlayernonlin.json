{
    "ssmlayernonlin-0": "State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\nShida Wang Department of Mathematics National University of Singapore e0622338@u.nus.edu Beichen Xue Department of Statistics and Data Science National University of Singapore e0773769@u.nus.edu\n\nAbstract\n\nState-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model\u2019s capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model\u2019s capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications. 1 Introduction\n\nState-space model [1, 2, 3, 4, 5] is an emerging family of neural networks specialized in learning long sequence relationships. It achieves significantly better performance compared with attention-based transformers in the long range arena (LRA) dataset [6, 7]. Despite its effectiveness, the state-space model is built on a relatively simple foundation of linear-RNN-like layers. One of the key advantages of state-space models is their simple recurrence, which enables efficient acceleration. In fact, this recurrence allows for an asymptotic computational complexity of only , which is significantly better than the complexity of traditional full-attention approaches [8]. A natural question would be whether SSM achieves this speedup with certain sacrifices in model capacity or memory property. It is currently unclear whether the state-space model\u2019s linear architecture with layerwise nonlinearity possesses sufficient expressive capacity to approximate any target sequence-to-sequence relationship. This knowledge would be important to answer pertinent questions regarding the model\u2019s ability to handle the complexity of real-world datasets characterized by diverse and intricate sequence relationships. In particular, considering the speed advantage of SSM over attention-based transformers, the universal approximation property impacts whether a state-space model could be a suitable replacement for a transformer. In this paper, we study the universality of state-space model. Furthermore, the memory property is investigated and we show that state-space models also have an asymptotically exponential decaying memory. The main contributions can be summarized as follow\n\n1. We give a constructive proof for the universal approximation property for multi-layer state-space models. The width dependency on sequence length is analyzed. 2. The state-space models are shown to have an exponentially decaying memory, which coincides with usual recurrent neural networks. 3. Numerical verifications for the memory property are given on synthetic datasets. 2 Background\n\nIn this section, we introduce the general form of the state-space models. The classical results on universal approximation for recurrent neural networks are summarized. Based on the approximation result, the well-documented challenge of learning long-term memory via recurrent networks is summarized. In particular, we give the definition of memory function which we shall use in the derivation of memory decay. 2.1 State-space models\n\nSingle-layer state-space model can be viewed as a recurrent neural network without nonlinear recurrent activation. As is shown in Figure 1, the discrete time version of SSM [9] is\n\nh k + 1 subscript \u210e \ud835\udc58 1 \\displaystyle h_{k+1} = W \u200b h k + U \u200b x k + 1 , h 0 = 0 formulae-sequence absent \ud835\udc4a subscript \u210e \ud835\udc58 \ud835\udc48 subscript \ud835\udc65 \ud835\udc58 1 subscript \u210e 0 0 \\displaystyle=Wh_{k}+Ux_{k+1},\\qquad h_{0}=0 (1) y k subscript \ud835\udc66 \ud835\udc58 \\displaystyle y_{k} = C \u200b h k + D \u200b x k = C \u200b W k \u200b h 0 + \u2211 i = 1 k C \u200b W k \u2212 i \u200b U \u200b x i + D \u200b x k . absent \ud835\udc36 subscript \u210e \ud835\udc58 \ud835\udc37 subscript \ud835\udc65 \ud835\udc58 \ud835\udc36 superscript \ud835\udc4a \ud835\udc58 subscript \u210e 0 superscript subscript \ud835\udc56 1 \ud835\udc58 \ud835\udc36 superscript \ud835\udc4a \ud835\udc58 \ud835\udc56 \ud835\udc48 subscript \ud835\udc65 \ud835\udc56 \ud835\udc37 subscript \ud835\udc65 \ud835\udc58 \\displaystyle=Ch_{k}+Dx_{k}=CW^{k}h_{0}+\\sum_{i=1}^{k}CW^{k-i}Ux_{i}+Dx_{k}. (2)\n\nwhere , , , , , , . Notice that we drop the bias term for simplicity. For multi-layer state-space model, the nonlinear activation is added layer-wise. The continuous time version of a single layer is\n\nd \u200b h t d \u200b t \ud835\udc51 subscript \u210e \ud835\udc61 \ud835\udc51 \ud835\udc61 \\displaystyle\\frac{dh_{t}}{dt} = W \u200b h t + U \u200b x t , h 0 = 0 formulae-sequence absent \ud835\udc4a subscript \u210e \ud835\udc61 \ud835\udc48 subscript \ud835\udc65 \ud835\udc61 subscript \u210e 0 0 \\displaystyle=Wh_{t}+Ux_{t},\\qquad h_{0}=0 (3) y t subscript \ud835\udc66 \ud835\udc61 \\displaystyle y_{t} = C \u200b h t + D \u200b x t = \u222b 0 T C \u200b e W \u200b ( t \u2212 s ) \u200b U \u200b x s \u200b \ud835\udc51 s + D \u200b x t . absent \ud835\udc36 subscript \u210e \ud835\udc61 \ud835\udc37 subscript \ud835\udc65 \ud835\udc61 superscript subscript 0 \ud835\udc47 \ud835\udc36 superscript \ud835\udc52 \ud835\udc4a \ud835\udc61 \ud835\udc60 \ud835\udc48 subscript \ud835\udc65 \ud835\udc60 differential-d \ud835\udc60 \ud835\udc37 subscript \ud835\udc65 \ud835\udc61 \\displaystyle=Ch_{t}+Dx_{t}=\\int_{0}^{T}Ce^{W(t-s)}Ux_{s}ds+Dx_{t}. (4)\n\nIt can be seen in Equation 4 that the first component of output is the convolution between kernel function and . In discrete case, the convolution operator is represented by\n\ny [ 0 , T ] = \u03c1 \u200b ( t ) \u200b \u2217 x [ 0 , T ] \u21d2 y k = \u2211 i = 0 k \u03c1 k \u2212 i \u200b x i . subscript \ud835\udc66 0 \ud835\udc47 \ud835\udf0c \ud835\udc61 \u2217 subscript \ud835\udc65 0 \ud835\udc47 \u21d2 subscript \ud835\udc66 \ud835\udc58 superscript subscript \ud835\udc56 0 \ud835\udc58 subscript \ud835\udf0c \ud835\udc58 \ud835\udc56 subscript \ud835\udc65 \ud835\udc56 y_{[0,T]}=\\rho(t)\\mathop{\\scalebox{1.7}{\\raisebox{-0.86108pt}{$\\ast$}}}x_{[0,T]}\\Rightarrow y_{k}=\\sum_{i=0}^{k}\\rho_{k-i}x_{i}. (5)\n\nCompared with temporal convolution network (TCN) [10], which has a finite kernel size, state-space models can be regarded as implementing global convolution () over the temporal axis. Numerically, since the kernel size is the same as the sequence length, the convolution implemented via fast Fourier transform can lead to significant accelerations [11]. Compared with attention-based transformer, it is shown that SSM can speed up the training 100x in learning sequences with length 64K [12]. The state-space model\u2019s temporal efficiency is achieved by eliminating the nonlinear activation function in its recurrent layers , resulting in faster processing of sequential data via parallel scan [11]. 2.2 Universal approximation in RNN\n\nIt is long-known that recurrent neural networks with nonlinear sigmoidal activations (such as tanh and sigmoid) are universal approximators. Theorem 2.1 (Simplified universality statement). For any sequence to sequence map: and tolerance , there exists a hidden dimension and weights such that the RNN with weights can approximate the target sequence to sequence map:\n\n\u2016 y \u2212 y ^ \u2016 \u2264 \u03f5 . norm \ud835\udc66 ^ \ud835\udc66 italic-\u03f5 \\|y-\\hat{y}\\|\\leq\\epsilon. (6)\n\nwhere prediction sequence is given by\n\nh k + 1 subscript \u210e \ud835\udc58 1 \\displaystyle h_{k+1} = \u03c3 \u200b ( W \u200b h k + U \u200b x k + b ) , absent \ud835\udf0e \ud835\udc4a subscript \u210e \ud835\udc58 \ud835\udc48 subscript \ud835\udc65 \ud835\udc58 \ud835\udc4f \\displaystyle=\\sigma(Wh_{k}+Ux_{k}+b), (7) y ^ k subscript ^ \ud835\udc66 \ud835\udc58 \\displaystyle\\hat{y}_{k} = C \u22a4 \u200b h k . absent superscript \ud835\udc36 top subscript \u210e \ud835\udc58 \\displaystyle=C^{\\top}h_{k}. (8)\n\nUniversal approximation establishes the feasibility of learning sequence to sequence relationships via recurrent neural networks. However, typical approximation rate results depend on the sequence length [13]. Sequence length dependent approximation rate does not generalize to the case of sequence of infinite length. In learning sequences with infinite length, Li et al. [14] shows that linear RNNs have difficulty in learning non-exponential decaying memory. Various numerical experiments [15] confirm that adding nonlinear recurrent activation does not fundamentally change the decay. In state-space models, the nonlinearity is included in a layer-wise approach. It is unknown whether such layer-wise nonlinearity alone is sufficient to approximate any sequence to sequence relationships. 2.3 Memory function and curse of memory\n\nIn this paper we study the memory property of state-space model. Before we introduce the main results, we present a simple memory function definition in sequence modelling. Li et al. [14] proves that a bounded causal continuous regular time-homogeneous linear functional has the following Riesz representation:\n\ny t = H t \u200b ( \ud835\udc31 ) = \u222b \u2212 \u221e t \u03c1 \u200b ( t \u2212 s ) \u200b x s \u200b \ud835\udc51 s . subscript \ud835\udc66 \ud835\udc61 subscript \ud835\udc3b \ud835\udc61 \ud835\udc31 superscript subscript \ud835\udc61 \ud835\udf0c \ud835\udc61 \ud835\udc60 subscript \ud835\udc65 \ud835\udc60 differential-d \ud835\udc60 y_{t}={H_{t}(\\mathbf{x})}=\\int_{-\\infty}^{t}\\rho(t-s)x_{s}ds.",
    "ssmlayernonlin-1": "(9)\n\nHere is an -integrable function. If rapidly decreases with , then the target sequence relationship has a short-term memory. Since fully captures the memory property of a linear functional [14], we call it the memory function (of the linear functional). In particular, when approximating the linear functional with linear RNNs, the model\u2019s memory function is . The curse of memory refers to the phenomenon that if a target linear functional can be approximated by a sequence of linear RNNs, the memory function decays exponentially. Take the test input to be Notice that the derivative of the linear functional at test input extracts the memory function . Therefore a natural extension of the memory function [16] to the bounded causal continuous regular time-homogeneous nonlinear functionals will be:\n\n\u03c1 ^ \u200b ( t ) = | d \u200b y ^ t d \u200b t | 2 , y ^ t = \ud835\udc07 ^ t \u200b ( \ud835\udc31 test ) . formulae-sequence ^ \ud835\udf0c \ud835\udc61 subscript \ud835\udc51 subscript ^ \ud835\udc66 \ud835\udc61 \ud835\udc51 \ud835\udc61 2 subscript ^ \ud835\udc66 \ud835\udc61 subscript ^ \ud835\udc07 \ud835\udc61 subscript \ud835\udc31 test \\hat{\\rho}(t)=\\left|\\frac{d\\hat{y}_{t}}{dt}\\right|_{2},\\quad\\hat{y}_{t}=\\widehat{\\mathbf{H}}_{t}(\\mathbf{x}_{\\textrm{test}}). (10)\n\nIt can be seen this definition is compatible with the memory function definition of linear functional. Also, this memory function can be evaluated by computing the models\u2019 derivative at the test input using finite difference method. 3 Main results\n\nIn this section, we first give a simple constructive proof to show that any element-wise function on input sequence can be approximated by a two-layer state-space model. Next, we show any temporal convolution can be approximated by a state-space model. The constructive proof for general nonlinear sequence to sequence functional is given based on the above propositions. Moreover, as the Kolmogorov-Arnold-representation-based construction has a weight number dependent on sequence length, it can be highly inefficient to construct a shallow wide network to learn long sequences. To reduce the widths dependency on the sequence length, a Volterra-series-based construction is demonstrated. 3.1 Two-layer SSM approximates element-wise function\n\nBy element-wise function, we mean learning sequence relationship with form\n\nProposition 3.1. For any given continuous function over a compact set , let the activation be sigmoidal function. lim z \u2192 \u221e \u03c3 \u200b ( z ) = 1 , lim z \u2192 \u2212 \u221e \u03c3 \u200b ( z ) = \u2212 1 . formulae-sequence subscript \u2192 \ud835\udc67 \ud835\udf0e \ud835\udc67 1 subscript \u2192 \ud835\udc67 \ud835\udf0e \ud835\udc67 1 \\lim_{z\\to\\infty}\\sigma(z)=1,\\quad\\lim_{z\\to-\\infty}\\sigma(z)=-1. (11)\n\nThere exists a sequence of two-layer state-space model with weights that can approximate sequence relationship . sup t sup \ud835\udc31 | f \u200b ( \ud835\udc31 ) \u2212 \ud835\udc32 ^ | \u2264 \u03f5 . subscript supremum \ud835\udc61 subscript supremum \ud835\udc31 \ud835\udc53 \ud835\udc31 ^ \ud835\udc32 italic-\u03f5 \\sup_{t}\\sup_{\\mathbf{x}}|f(\\mathbf{x})-\\hat{\\mathbf{y}}|\\leq\\epsilon. (12)\n\nHere the two-layer state-space model is constructed by\n\nh k + 1 ( 1 ) subscript superscript \u210e 1 \ud835\udc58 1 \\displaystyle h^{(1)}_{k+1} = W 1 \u200b h k ( 1 ) + U 1 \u200b x k + b 1 , absent subscript \ud835\udc4a 1 subscript superscript \u210e 1 \ud835\udc58 subscript \ud835\udc48 1 subscript \ud835\udc65 \ud835\udc58 subscript \ud835\udc4f 1 \\displaystyle=W_{1}h^{(1)}_{k}+U_{1}x_{k}+b_{1}, (13) h k + 1 ( 2 ) subscript superscript \u210e 2 \ud835\udc58 1 \\displaystyle h^{(2)}_{k+1} = W 2 \u200b h k ( 2 ) + U 2 \u200b \u03c3 \u200b ( h k + 1 ( 1 ) ) + b 2 absent subscript \ud835\udc4a 2 subscript superscript \u210e 2 \ud835\udc58 subscript \ud835\udc48 2 \ud835\udf0e subscript superscript \u210e 1 \ud835\udc58 1 subscript \ud835\udc4f 2 \\displaystyle=W_{2}h^{(2)}_{k}+U_{2}\\sigma(h^{(1)}_{k+1})+b_{2} (14) y k subscript \ud835\udc66 \ud835\udc58 \\displaystyle y_{k} = h k ( 2 ) .",
    "ssmlayernonlin-2": "absent subscript superscript \u210e 2 \ud835\udc58 \\displaystyle=h^{(2)}_{k}.",
    "ssmlayernonlin-3": "(15)\n\nThe proof is included in Section B.2. The main idea is to approximate element-wise function with A graphical demonstration is given in Figure 2. It can be seen the two-layer state-space model can approximate any element-wise function by setting\n\n3.2 SSM approximates temporal convolution\n\nIn this part we show that state-space models can approximate temporal convolution: According to Equation 3, the hidden state of SSM is the convolution between input sequence and exponentially decaying function . Therefore the approximation problem of temporal convolution by state-space models is reduced to the approximation problem of general convolution kernel by exponentially decaying convolution kernel . The optimal weights are defined by\n\nProposition 3.2. For any given convolution kernel , the single-layer state-space model is universal approximator for temporal convolution. In other words, for any , there exists a hidden dimension and corresponding weights such that satisfies\n\nsup k | \u03c1 k \u2212 \u03c1 ^ k | < \u03f5 . subscript supremum \ud835\udc58 subscript \ud835\udf0c \ud835\udc58 subscript ^ \ud835\udf0c \ud835\udc58 italic-\u03f5 \\sup_{k}|\\rho_{k}-\\hat{\\rho}_{k}|<\\epsilon. (16)\n\nSee Section B.3 for the proof. The main idea is to represent the single-layer state-space model output as a convolution between input and kernel function. The approximation of temporal convolution is then reduced to the approximation of general kernel with the SSM-induced kernels. Remark 3.3. Although the Proposition 3.2 indicates that we can use single-layer state-space model to approximate any convolution, it does not reveal the necessary hidden dimension for such approximation. 3.3 Universality of SSM\n\nNow we show that five-layer state-space model is universal.",
    "ssmlayernonlin-4": "Without loss of generality, assume the output is one-dimensional. The main proof is based on the famous Kolmogorov-Arnold representation theorem [17]. By Kolmogorov-Arnold representation theorem, we know any multivariate continuous function can be represented by\n\nf \u200b ( x 1 , \u2026 , x d ) = \u2211 q = 0 2 \u200b d \u03a6 q \u200b ( \u2211 p = 1 d \u03d5 q , p \u200b ( x p ) ) . \ud835\udc53 subscript \ud835\udc65 1 \u2026 subscript \ud835\udc65 \ud835\udc51 superscript subscript \ud835\udc5e 0 2 \ud835\udc51 subscript \u03a6 \ud835\udc5e superscript subscript \ud835\udc5d 1 \ud835\udc51 subscript italic-\u03d5 \ud835\udc5e \ud835\udc5d subscript \ud835\udc65 \ud835\udc5d f(x_{1},\\dots,x_{d})=\\sum_{q=0}^{2d}\\Phi_{q}\\left(\\sum_{p=1}^{d}\\phi_{q,p}(x_{p})\\right). (17)\n\nRemark 3.4. George Lorentz [18] shows that we can use the same : Sprecher [19] proves that it can be further reduced to the same : Braun and Griebel [20] gives the first constructive proof for the superposition. Moreover, the inner function is shown to be independent of target function . It means the learning of function can be approximated without retraining for different target functionals. We summarize the Kolmogorov-Arnold theorem as follow:\n\nTheorem 3.5 ([17, 20]). Fix dimension . There are real numbers and a continuous and monotone function , such that for any continuous function , there exists a continuous function with\n\nf \u200b ( x 1 , \u2026 , x d ) = \u2211 q = 0 2 \u200b d \u03a6 \u200b ( \u2211 p = 1 d b p \u200b \u03d5 \u200b ( x p + q \u200b a ) + c q ) . \ud835\udc53 subscript \ud835\udc65 1 \u2026 subscript \ud835\udc65 \ud835\udc51 superscript subscript \ud835\udc5e 0 2 \ud835\udc51 \u03a6 superscript subscript \ud835\udc5d 1 \ud835\udc51 subscript \ud835\udc4f \ud835\udc5d italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e subscript \ud835\udc50 \ud835\udc5e f(x_{1},\\dots,x_{d})=\\sum_{q=0}^{2d}\\Phi\\left(\\sum_{p=1}^{d}b_{p}\\phi(x_{p}+qa)+c_{q}\\right). (18)\n\nProposition 3.6. For any bounded causal continuous sequence to sequence relationship and tolerance , there exists a hidden dimension and corresponding state-space model (as constructed in Figure 3) such that the error of approximation\n\n| y k \u2212 y ^ k | \u2264 \u03f5 , k \u2208 { 1 , \u2026 , T } . formulae-sequence subscript \ud835\udc66 \ud835\udc58 subscript ^ \ud835\udc66 \ud835\udc58 italic-\u03f5 \ud835\udc58 1 \u2026 \ud835\udc47 |y_{k}-\\hat{y}_{k}|\\leq\\epsilon,\\quad k\\in\\{1,\\dots,T\\}. (19)\n\nSee the proof in Section B.4. The main idea is demonstrated in Figure 3, the nonlinear functions are separately approximated by two-layer state-space model. Remark 3.7. The Kolmogorov theorem provides a construction for achieving universality in a five-layer state-space model. However, the quantity of hidden neurons increases linearly with the sequence length. This can become exceedingly burdensome when the sequence length escalates. Another approach is from the perspective of Volterra Series, which features the sequence-length independent neurons. Theorem 3.8 ([21]). For any continuous time-invariant system with as input and as output can be expanded in the Volterra series as follow\n\ny \u200b ( t ) = h 0 + \u2211 n = 1 N \u222b 0 t \u22ef \u200b \u222b 0 t h n \u200b ( \u03c4 1 , \u2026 , \u03c4 n ) \u200b \u220f j = 1 n x \u200b ( t \u2212 \u03c4 j ) \u200b d \u200b \u03c4 j . \ud835\udc66 \ud835\udc61 subscript \u210e 0 superscript subscript \ud835\udc5b 1 \ud835\udc41 superscript subscript 0 \ud835\udc61 \u22ef superscript subscript 0 \ud835\udc61 subscript \u210e \ud835\udc5b subscript \ud835\udf0f 1 \u2026 subscript \ud835\udf0f \ud835\udc5b superscript subscript product \ud835\udc57 1 \ud835\udc5b \ud835\udc65 \ud835\udc61 subscript \ud835\udf0f \ud835\udc57 \ud835\udc51 subscript \ud835\udf0f \ud835\udc57 y(t)=h_{0}+\\sum_{n=1}^{N}\\int_{0}^{t}\\cdots\\int_{0}^{t}h_{n}(\\tau_{1},\\dots,\\tau_{n})\\prod_{j=1}^{n}x(t-\\tau_{j})d\\tau_{j}. (20)\n\nIn particular, we call the expansion order to be the series\u2019 order. A simplified interpretation of the -th Volterra Series expansion is the \u201c-th Taylor expansion in the sequence variable \u201d. Proposition 3.9. For any bounded causal continuous time-homgeneous sequence to sequence relationship and tolerance , there exists a hidden dimension and corresponding state-space model (as constructed in Figure 4) such that the error of approximation\n\n| y k \u2212 y ^ k | \u2264 \u03f5 , k \u2208 { 1 , \u2026 , T } . formulae-sequence subscript \ud835\udc66 \ud835\udc58 subscript ^ \ud835\udc66 \ud835\udc58 italic-\u03f5 \ud835\udc58 1 \u2026 \ud835\udc47 |y_{k}-\\hat{y}_{k}|\\leq\\epsilon,\\quad k\\in\\{1,\\dots,T\\}. (21)\n\nMoreover, the neurons of the state-space model do not explicitly depend on the sequence length . See the proof in Section B.5. The main idea is to approximate the convolution kernels by the low-rank tensor product of first-order convolution kernel. Remark 3.10. The advantage of Volterra-series-type construction is the approximation of the convolution kernel with SSM-induced kernel as well as the approximation of multivariable convolution kernel with tensor product of one-dimensional convolution kernel do not explicitly depend on the sequence length. Similar approaches have been adopted by idea of implicit convolution in CKConv [22]. 3.4 Memory decay of SSM\n\nIn the ensuing discourse, we turn our focus towards an examination of the memory property inherent in state-space models. It has been thoroughly studied in literature that recurrent neural networks exhibit a phenomenon of exponential memory decay [14]. An intriguing question that naturally arises in this context is whether state-space models are plagued by similar challenges. Upon careful investigation, it is concluded that state-space models, much like their neural network counterparts, do possess an asymptotically exponential decaying memory. Proposition 3.11. Assume there exists a constant such that\n\nlim t \u2192 \u221e e c 0 \u200b t \u200b \u2016 x t \u2212 x \u2217 \u2016 \u2192 0 , x \u2217 := lim t \u2192 \u221e x t . formulae-sequence \u2192 subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 norm subscript \ud835\udc65 \ud835\udc61 superscript \ud835\udc65 0 assign superscript \ud835\udc65 subscript \u2192 \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \\lim_{t\\to\\infty}e^{c_{0}t}\\|x_{t}-x^{*}\\|\\to 0,\\quad\\displaystyle x^{*}:=\\lim_{t\\to\\infty}x_{t}. (22)\n\nAssume the output is the output of single-layer state-space model with parameters . Then, for the same constant , if the output\u2019s derivative satisfies\n\nlim t \u2192 \u221e e c 0 \u200b t \u200b \u2016 y t \u2212 y \u2217 \u2016 \u2192 0 . \u2192 subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 norm subscript \ud835\udc66 \ud835\udc61 superscript \ud835\udc66 0 \\lim_{t\\to\\infty}e^{c_{0}t}\\|y_{t}-y^{*}\\|\\to 0. (23)\n\nAt the same time, general smooth nonlinear activation does not change the exponential decay property of a sequence:\n\nProposition 3.12. Assume there exists a constant such that\n\nlim t \u2192 \u221e e c 0 \u200b t \u200b \u2016 x t \u2212 x \u2217 \u2016 \u2192 0 , x \u2217 := lim t \u2192 \u221e x t . formulae-sequence \u2192 subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 norm subscript \ud835\udc65 \ud835\udc61 superscript \ud835\udc65 0 assign superscript \ud835\udc65 subscript \u2192 \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \\lim_{t\\to\\infty}e^{c_{0}t}\\|x_{t}-x^{*}\\|\\to 0,\\quad\\displaystyle x^{*}:=\\lim_{t\\to\\infty}x_{t}. (24)\n\nFor given Lipschitz continuous layer-wise activations , there exists a positive constant such that the output memory function\n\nlim t \u2192 \u221e e c 0 \u200b t \u200b \u2016 y t \u2212 y \u2217 \u2016 \u2192 0 . \u2192 subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 norm subscript \ud835\udc66 \ud835\udc61 superscript \ud835\udc66 0 \\lim_{t\\to\\infty}e^{c_{0}t}\\|y_{t}-y^{*}\\|\\to 0. (25)\n\nSee the proofs for above two propositions in Section B.6 and Section B.7. Based on the above two propositions, by induction we have the following theorem:\n\nTheorem 3.13. Assume is a multi-layer state-space model with Lipschitz continuous function as the layer-wise activations. Assume the state-space model is stable in the sense that matrix \u2019s eigenvalue are bounded by 1. There exists a positive constant such that the memory function (defined in Equation 10) of state-space model is decaying exponentially\n\nlim t \u2192 \u221e e c 0 \u200b t \u200b \u03c1 ^ \u200b ( t ) \u2192 0 . \u2192 subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 ^ \ud835\udf0c \ud835\udc61 0 \\lim_{t\\to\\infty}e^{c_{0}t}\\hat{\\rho}(t)\\to 0. (26)\n\n4 Numerical verifications\n\nBased on the generalization of memory function in linear functional, we verify the asymptotic memory decay of state-space models with simple randomly generated models.",
    "ssmlayernonlin-5": "The definition is given in one-dimensional case, but it can be generalized to multi-variable case by taking different unit inputs in various coordinates. The motivation for the above definition is based on the idea of measuring the earlier input at the later output. In our experiment, we construct various RNN models and SSM with random generated weights. It can be seen in Figure 5 that the memory of naive state-space models also has an exponentially decay memory. It is consistent with the previous theorem that state-space model has an asymptotic exponentially decaying memory. Notice that here the naive SSM is simply adding a tanh activation across layers without specially tuning the weights. Such random initialization can expose the memory issue more significantly as the S4 layer is constructed with several parameterization techniques. However, the manually constructed S4 still has an asymptotic exponential decaying memory as is shown in Figure 6. 5 Related Work\n\nIn this section, we introduce the previous works on state-space models. As the single-layer state-space model is a linear RNN, we summarize the related approximation work on RNN. In particular, the approximation result and memory result is emphasized as this paper works on the universal approximation property and memory decay property of SSM. State-space models\n\nState-space models originate from the HIPPO matrix which is optimal in the online function approximation sense [2, 3, 4]. The Hippo matrix initialization for recurrent matrix enables the state-space model to have a slow decaying memory.",
    "ssmlayernonlin-6": "The universal approximation idea is heuristically demonstrated in Orvieto et al. [23]. However, the proof from the Koopman theory perspective only guarantees the existence of universal approximation. Our proof is a constructive proof which can be further generalized to study the approximation rate with respect to the hidden dimensions and network depths. Recurrent neural networks\n\nRecurrent neural networks (RNNs) [24] are one of the most popular neural networks for sequence modelling. Various results have been established in RNNs approximation theory, see Sontag [25], Hanson et al. [13]. Apart from the universal approximation, the exponential decaying memory property is the notorious phenomenon in recurrent neural networks which prohibits the scale up of the models in terms of the sequence length [14, 26]. 6 Discussion\n\nIn Table 1 we compare the classical sequence models including RNN, TCN and attention-based transformer. The state-space model can be considered as an enhancement of Recurrent Neural Networks (RNNs) due to its superior optimization and inference speed. Despite maintaining a similar network topology, inference cost, and memory pattern, it provides a more efficient and streamlined approach. The effort to extend the long-memory learning is also carried out in convolutional networks and attention-based transformers. Romero et al. [22] proposes to parameterize the convolution kernel implicitly, which utilizes the power of spline function approximation . Similar idea has been adopted in Poli et al. [12]. To summarize, we regard SSM, CKConv and linear transformer as the sequence models\u2019 improvement in learning long-memory for long sequences. 7 Conclusion\n\nIn this paper, we give a constructive proof for the universal approximation property of multi-layer state-space models. It is shown that the nonlinear recurrent activations in classical recurrent neural networks are not necessary when there are nonlinear activations across different hidden layers. This result implies state-space model is as powerful as the classical recurrent neural networks in the approximation sense. Furthermore, we study the memory decay in multi-layer state-space models, which is a notorious issue in classical recurrent neural networks. While empirical evidence suggests that state-space models do not experience significant memory decay, they nonetheless exhibit a memory pattern that decays exponentially in the asymptotic sense. Our research has exciting implications for future work in state-space models. By extending our work to the approximation rate of state-space models, we can obtain better understanding of state-space models\u2019 hypothesis space. Such result is important to further optimize the architecture in various real-world applications. We aim to unlock the full potential of state-space models by identifying the ideal network structure (including depth and hidden dimension) for specific tasks and applications. References\n\nSiivola and Honkela [2003] V.",
    "ssmlayernonlin-7": "Siivola and A. Honkela. A state-space method for language modeling. In 2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721), pages 548\u2013553, St Thomas, VI, USA, 2003. IEEE. ISBN 978-0-7803-7980-0. doi: 10.1109/ASRU.2003.1318499. Gu et al. [2020] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections.",
    "ssmlayernonlin-8": "In Advances in Neural Information Processing Systems, volume 33, pages 1474\u20131487.",
    "ssmlayernonlin-9": "Curran Associates, Inc., 2020. Smith et al. [2023] Jimmy T. H. Smith, Andrew Warrington, and Scott Linderman. Simplified State Space Layers for Sequence Modeling. In International Conference on Learning Representations, February 2023. Fu et al. [2023] Daniel Y. Fu, Tri Dao, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher Re. Hungry Hungry Hippos: Towards Language Modeling with State Space Models. In The Eleventh International Conference on Learning Representations, February 2023. Wang et al. [2022] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. Tay et al. [2021] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena : A Benchmark for Efficient Transformers.",
    "ssmlayernonlin-10": "In International Conference on Learning Representations, January 2021. Gu et al. [2022] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models.",
    "ssmlayernonlin-11": "Advances in Neural Information Processing Systems, 35:35971\u201335983, December 2022. Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. Gu et al. [2023] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to Train your HIPPO: State Space Models with Generalized Orthogonal Basis Projections. In International Conference on Learning Representations, February 2023. Bai et al. [2018] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. Martin and Cundy [2018] Eric Martin and Chris Cundy. Parallelizing Linear Recurrent Neural Nets Over Sequence Length. In International Conference on Learning Representations, February 2018. Poli et al. [2023] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena Hierarchy: Towards Larger Convolutional Language Models. In International Conference on Machine Learning, June 2023. Hanson et al. [2021] Joshua Hanson, Maxim Raginsky, and Eduardo Sontag. Learning Recurrent Neural Net Models of Nonlinear Systems. In Proceedings of the 3rd Conference on Learning for Dynamics and Control, pages 425\u2013435. PMLR, May 2021. Li et al. [2022] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks. Journal of Machine Learning Research, 23(42):1\u201385, 2022. ISSN 1533-7928. Bengio et al. [1994] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, March 1994.",
    "ssmlayernonlin-12": "ISSN 1941-0093. doi: 10.1109/72.279181. Wang et al. [2023] Shida Wang, Zhong Li, and Qianxiao Li. Inverse approximation theory for nonlinear recurrent neural networks.",
    "ssmlayernonlin-13": "arXiv preprint arXiv:2305.19190, 2023. Kolmogorov [1963] A. N. Kolmogorov. On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition, volume 28, pages 55\u201359.",
    "ssmlayernonlin-14": "American Mathematical Society, Providence, Rhode Island, 1963. ISBN 978-0-8218-1728-5 978-1-4704-3239-3. doi: 10.1090/trans2/028/04. Collatz and Krabs [2013] Lothar Collatz and Werner Krabs. Approximationstheorie: Tschebyscheffsche Approximation mit Anwendungen.",
    "ssmlayernonlin-15": "Springer-Verlag, April 2013. ISBN 978-3-322-94885-4. Sprecher [1965] David A Sprecher. ON THE STRUCTURE OF REPRESENTATIONS OF CONTINUOUS FUNCTIONS OF SEVERAL VARIABLES AS FINITE SUMS OF CONTINUOUS FUNCTIONS OF ONE VARIABLE.",
    "ssmlayernonlin-16": "Proceedings of the American Mathematical Society, 1965. Braun and Griebel [2009] J\u00fcrgen Braun and Michael Griebel. On a Constructive Proof of Kolmogorov\u2019s Superposition Theorem. Constructive Approximation, 30(3):653\u2013675, December 2009. ISSN 0176-4276, 1432-0940. doi: 10.1007/s00365-009-9054-2. Boyd et al. [1984] Stephen Boyd, L. O. Chua, and C. A. Desoer. Analytical Foundations of Volterra Series. IMA Journal of Mathematical Control and Information, 1(3):243\u2013282, January 1984.",
    "ssmlayernonlin-17": "ISSN 0265-0754. doi: 10.1093/imamci/1.3.243. Romero et al. [2022] David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogendoorn. CKConv: Continuous Kernel Convolution For Sequential Data. In International Conference on Learning Representations, January 2022. Orvieto et al. [2023] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. Rumelhart et al. [1986] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533\u2013536, October 1986.",
    "ssmlayernonlin-18": "ISSN 1476-4687. doi: 10.1038/323533a0. Sontag [1998] Eduardo D. Sontag. A learning result for continuous-time recurrent neural networks. Systems & Control Letters, 34(3):151\u2013158, June 1998.",
    "ssmlayernonlin-19": "ISSN 01676911. doi: 10.1016/S0167-6911(98)00006-1. Jiang et al. [2023] Haotian Jiang, Qianxiao Li, Zhong Li, and Shida Wang. A Brief Survey on the Approximation Theory for Sequence Modelling. Journal of Machine Learning, 2(1):1\u201330, June 2023. ISSN 2790-203X, 2790-2048. doi: 10.4208/jml.221221. Barron [1994] Andrew R. Barron. Approximation and estimation bounds for artificial neural networks. Machine Learning, 14(1):115\u2013133, January 1994.",
    "ssmlayernonlin-20": "ISSN 1573-0565. doi: 10.1007/BF00993164. Cybenko [1989] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4):303\u2013314, December 1989.",
    "ssmlayernonlin-21": "ISSN 1435-568X. doi: 10.1007/BF02551274. Eldan and Shamir [2016] Ronen Eldan and Ohad Shamir. The Power of Depth for Feedforward Neural Networks.",
    "ssmlayernonlin-22": "In Conference on Learning Theory, pages 907\u2013940. PMLR, June 2016. Appendix A Memory of multi-layer linear RNNs\n\nIn this section, we study the memory function of linear RNN. The depth of linear RNN does not directly expand the memroy pattern as the memory are still decaying exponentially. Nonetheless, although the depth does not increase the approximation capacity, a deeper model is endowed with more structural property. Take the two-layer Linear RNN as an example:\n\ny ^ t subscript ^ \ud835\udc66 \ud835\udc61 \\displaystyle\\hat{y}_{t} = \u222b 0 t C \u200b e W 1 \u200b ( t \u2212 s ) \u200b U 1 \u200b \u222b 0 s e W 2 \u200b ( s \u2212 r ) \u200b U 2 \u200b x r \u200b \ud835\udc51 r \u200b \ud835\udc51 s absent superscript subscript 0 \ud835\udc61 \ud835\udc36 superscript \ud835\udc52 subscript \ud835\udc4a 1 \ud835\udc61 \ud835\udc60 subscript \ud835\udc48 1 superscript subscript 0 \ud835\udc60 superscript \ud835\udc52 subscript \ud835\udc4a 2 \ud835\udc60 \ud835\udc5f subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f differential-d \ud835\udc5f differential-d \ud835\udc60 \\displaystyle=\\int_{0}^{t}Ce^{W_{1}(t-s)}U_{1}\\int_{0}^{s}e^{W_{2}(s-r)}U_{2}x_{r}drds (27) = \u222b 0 t C \u200b P 1 \u200b e \u039b 1 \u200b ( t \u2212 s ) \u200b P 1 \u2212 1 \u200b U 1 \u200b \u222b 0 s P 2 \u200b e \u039b 2 \u200b ( s \u2212 r ) \u200b P 2 \u2212 1 \u200b U 2 \u200b x r \u200b \ud835\udc51 r \u200b \ud835\udc51 s absent superscript subscript 0 \ud835\udc61 \ud835\udc36 subscript \ud835\udc43 1 superscript \ud835\udc52 subscript \u039b 1 \ud835\udc61 \ud835\udc60 superscript subscript \ud835\udc43 1 1 subscript \ud835\udc48 1 superscript subscript 0 \ud835\udc60 subscript \ud835\udc43 2 superscript \ud835\udc52 subscript \u039b 2 \ud835\udc60 \ud835\udc5f superscript subscript \ud835\udc43 2 1 subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f differential-d \ud835\udc5f differential-d \ud835\udc60 \\displaystyle=\\int_{0}^{t}CP_{1}e^{\\Lambda_{1}(t-s)}P_{1}^{-1}U_{1}\\int_{0}^{s}P_{2}e^{\\Lambda_{2}(s-r)}P_{2}^{-1}U_{2}x_{r}drds (28) = \u222b 0 t C \u200b e \u039b 1 \u200b ( t \u2212 s ) \u200b U 1 \u200b \u222b 0 s e \u039b 2 \u200b ( s \u2212 r ) \u200b U 2 \u200b x r \u200b \ud835\udc51 r \u200b \ud835\udc51 s absent superscript subscript 0 \ud835\udc61 \ud835\udc36 superscript \ud835\udc52 subscript \u039b 1 \ud835\udc61 \ud835\udc60 subscript \ud835\udc48 1 superscript subscript 0 \ud835\udc60 superscript \ud835\udc52 subscript \u039b 2 \ud835\udc60 \ud835\udc5f subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f differential-d \ud835\udc5f differential-d \ud835\udc60 \\displaystyle=\\int_{0}^{t}Ce^{\\Lambda_{1}(t-s)}U_{1}\\int_{0}^{s}e^{\\Lambda_{2}(s-r)}U_{2}x_{r}drds (29) = \u222b 0 < r < s < t \ud835\udc51 r \u200b \ud835\udc51 s \u200b ( C \u200b e \u039b 1 \u200b ( t \u2212 s ) \u200b U 1 \u200b e \u039b 2 \u200b ( s \u2212 r ) \u200b U 2 \u200b x r ) absent subscript 0 \ud835\udc5f \ud835\udc60 \ud835\udc61 differential-d \ud835\udc5f differential-d \ud835\udc60 \ud835\udc36 superscript \ud835\udc52 subscript \u039b 1 \ud835\udc61 \ud835\udc60 subscript \ud835\udc48 1 superscript \ud835\udc52 subscript \u039b 2 \ud835\udc60 \ud835\udc5f subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\int_{0<r<s<t}drds\\left(Ce^{\\Lambda_{1}(t-s)}U_{1}e^{\\Lambda_{2}(s-r)}U_{2}x_{r}\\right) (30) = \u222b 0 < r < t \u222b r < s < t \ud835\udc51 r \u200b \ud835\udc51 s \u200b ( C \u200b e \u039b 1 \u200b ( t \u2212 s ) \u200b U 1 \u200b e \u039b 2 \u200b ( s \u2212 r ) \u200b U 2 \u200b x r ) absent subscript 0 \ud835\udc5f \ud835\udc61 subscript \ud835\udc5f \ud835\udc60 \ud835\udc61 differential-d \ud835\udc5f differential-d \ud835\udc60 \ud835\udc36 superscript \ud835\udc52 subscript \u039b 1 \ud835\udc61 \ud835\udc60 subscript \ud835\udc48 1 superscript \ud835\udc52 subscript \u039b 2 \ud835\udc60 \ud835\udc5f subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\int_{0<r<t}\\int_{r<s<t}drds\\left(Ce^{\\Lambda_{1}(t-s)}U_{1}e^{\\Lambda_{2}(s-r)}U_{2}x_{r}\\right) (31) = \u222b 0 t \ud835\udc51 r \u200b \u222b r t \ud835\udc51 s \u200b ( C \u200b e \u039b 1 \u200b ( t \u2212 s ) \u200b U 1 \u200b e \u039b 2 \u200b ( s \u2212 r ) \u200b U 2 \u200b x r ) absent superscript subscript 0 \ud835\udc61 differential-d \ud835\udc5f superscript subscript \ud835\udc5f \ud835\udc61 differential-d \ud835\udc60 \ud835\udc36 superscript \ud835\udc52 subscript \u039b 1 \ud835\udc61 \ud835\udc60 subscript \ud835\udc48 1 superscript \ud835\udc52 subscript \u039b 2 \ud835\udc60 \ud835\udc5f subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\int_{0}^{t}dr\\int_{r}^{t}ds\\left(Ce^{\\Lambda_{1}(t-s)}U_{1}e^{\\Lambda_{2}(s-r)}U_{2}x_{r}\\right) (32) = \u2211 i = 1 d h \u200b 1 \u2211 j = 1 d h \u200b 2 \u222b 0 t \ud835\udc51 r \u200b \u222b r t \ud835\udc51 s \u200b ( C \u200b e \u03bb 1 \u200b i \u200b ( t \u2212 s ) \u200b U 1 , i \u200b j \u200b e \u03bb 2 \u200b j \u200b ( s \u2212 r ) \u200b U 2 \u200b x r ) absent superscript subscript \ud835\udc56 1 subscript \ud835\udc51 \u210e 1 superscript subscript \ud835\udc57 1 subscript \ud835\udc51 \u210e 2 superscript subscript 0 \ud835\udc61 differential-d \ud835\udc5f superscript subscript \ud835\udc5f \ud835\udc61 differential-d \ud835\udc60 \ud835\udc36 superscript \ud835\udc52 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc61 \ud835\udc60 subscript \ud835\udc48 1 \ud835\udc56 \ud835\udc57 superscript \ud835\udc52 subscript \ud835\udf06 2 \ud835\udc57 \ud835\udc60 \ud835\udc5f subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\sum_{i=1}^{d_{h1}}\\sum_{j=1}^{d_{h2}}\\int_{0}^{t}dr\\int_{r}^{t}ds\\left(Ce^{\\lambda_{1i}(t-s)}U_{1,ij}e^{\\lambda_{2j}(s-r)}U_{2}x_{r}\\right) (33) = \u2211 i = 1 d h \u200b 1 \u2211 j = 1 d h \u200b 2 \u222b 0 t \ud835\udc51 r \u200b \u222b r t \ud835\udc51 s \u200b ( e \u03bb 1 \u200b i \u200b ( t \u2212 s ) \u200b U 1 , i \u200b j \u200b e \u03bb 2 \u200b j \u200b ( s \u2212 r ) \u200b C \u200b U 2 \u200b x r ) absent superscript subscript \ud835\udc56 1 subscript \ud835\udc51 \u210e 1 superscript subscript \ud835\udc57 1 subscript \ud835\udc51 \u210e 2 superscript subscript 0 \ud835\udc61 differential-d \ud835\udc5f superscript subscript \ud835\udc5f \ud835\udc61 differential-d \ud835\udc60 superscript \ud835\udc52 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc61 \ud835\udc60 subscript \ud835\udc48 1 \ud835\udc56 \ud835\udc57 superscript \ud835\udc52 subscript \ud835\udf06 2 \ud835\udc57 \ud835\udc60 \ud835\udc5f \ud835\udc36 subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\sum_{i=1}^{d_{h1}}\\sum_{j=1}^{d_{h2}}\\int_{0}^{t}dr\\int_{r}^{t}ds\\left(e^{\\lambda_{1i}(t-s)}U_{1,ij}e^{\\lambda_{2j}(s-r)}CU_{2}x_{r}\\right) (34) = \u2211 i \u200b j \u222b 0 t \ud835\udc51 r \u200b \u222b r t \ud835\udc51 s \u200b ( e \u03bb 1 \u200b i \u200b ( t \u2212 s ) + \u03bb 2 \u200b j \u200b ( s \u2212 r ) \u200b U 1 , i \u200b j \u200b C \u200b U 2 \u200b x r ) absent subscript \ud835\udc56 \ud835\udc57 superscript subscript 0 \ud835\udc61 differential-d \ud835\udc5f superscript subscript \ud835\udc5f \ud835\udc61 differential-d \ud835\udc60 superscript \ud835\udc52 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc61 \ud835\udc60 subscript \ud835\udf06 2 \ud835\udc57 \ud835\udc60 \ud835\udc5f subscript \ud835\udc48 1 \ud835\udc56 \ud835\udc57 \ud835\udc36 subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\sum_{ij}\\int_{0}^{t}dr\\int_{r}^{t}ds\\left(e^{\\lambda_{1i}(t-s)+\\lambda_{2j}(s-r)}U_{1,ij}CU_{2}x_{r}\\right) (35) = \u2211 i \u200b j \u222b 0 t \ud835\udc51 r \u200b ( \u222b r t \ud835\udc51 s \u200b e ( \u03bb 2 \u200b j \u2212 \u03bb 1 \u200b i ) \u200b s ) \u200b ( e \u03bb 1 \u200b i \u200b t \u2212 \u03bb 2 \u200b j \u200b r \u200b U 1 , i \u200b j \u200b C \u200b U 2 \u200b x r ) absent subscript \ud835\udc56 \ud835\udc57 superscript subscript 0 \ud835\udc61 differential-d \ud835\udc5f superscript subscript \ud835\udc5f \ud835\udc61 differential-d \ud835\udc60 superscript \ud835\udc52 subscript \ud835\udf06 2 \ud835\udc57 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc60 superscript \ud835\udc52 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc61 subscript \ud835\udf06 2 \ud835\udc57 \ud835\udc5f subscript \ud835\udc48 1 \ud835\udc56 \ud835\udc57 \ud835\udc36 subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\sum_{ij}\\int_{0}^{t}dr\\left(\\int_{r}^{t}ds\\ e^{(\\lambda_{2j}-\\lambda_{1i})s}\\right)\\left(e^{\\lambda_{1i}t-\\lambda_{2j}r}U_{1,ij}CU_{2}x_{r}\\right) (36) = \u2211 i \u200b j \u222b 0 t \ud835\udc51 r \u200b ( 1 \u03bb 2 \u200b j \u2212 \u03bb 1 \u200b i \u200b ( e ( \u03bb 2 \u200b j \u2212 \u03bb 1 \u200b i ) \u200b t \u2212 e ( \u03bb 2 \u200b j \u2212 \u03bb 1 \u200b i ) \u200b r ) ) \u200b ( e \u03bb 1 \u200b i \u200b t \u2212 \u03bb 2 \u200b j \u200b r \u200b U 1 , i \u200b j \u200b C \u200b U 2 \u200b x r ) absent subscript \ud835\udc56 \ud835\udc57 superscript subscript 0 \ud835\udc61 differential-d \ud835\udc5f 1 subscript \ud835\udf06 2 \ud835\udc57 subscript \ud835\udf06 1 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udf06 2 \ud835\udc57 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udf06 2 \ud835\udc57 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc5f superscript \ud835\udc52 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc61 subscript \ud835\udf06 2 \ud835\udc57 \ud835\udc5f subscript \ud835\udc48 1 \ud835\udc56 \ud835\udc57 \ud835\udc36 subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\sum_{ij}\\int_{0}^{t}dr\\left(\\frac{1}{\\lambda_{2j}-\\lambda_{1i}}(e^{(\\lambda_{2j}-\\lambda_{1i})t}-e^{(\\lambda_{2j}-\\lambda_{1i})r})\\right)\\left(e^{\\lambda_{1i}t-\\lambda_{2j}r}U_{1,ij}CU_{2}x_{r}\\right) (37) = \u2211 i \u200b j \u222b 0 t \ud835\udc51 r \u200b ( 1 \u03bb 2 \u200b j \u2212 \u03bb 1 \u200b i \u200b ( e \u03bb 2 \u200b j \u200b ( t \u2212 r ) \u2212 e \u03bb 1 \u200b i \u200b ( t \u2212 r ) ) \u200b U 1 , i \u200b j \u200b C \u200b U 2 \u200b x r ) absent subscript \ud835\udc56 \ud835\udc57 superscript subscript 0 \ud835\udc61 differential-d \ud835\udc5f 1 subscript \ud835\udf06 2 \ud835\udc57 subscript \ud835\udf06 1 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udf06 2 \ud835\udc57 \ud835\udc61 \ud835\udc5f superscript \ud835\udc52 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc61 \ud835\udc5f subscript \ud835\udc48 1 \ud835\udc56 \ud835\udc57 \ud835\udc36 subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\sum_{ij}\\int_{0}^{t}dr\\left(\\frac{1}{\\lambda_{2j}-\\lambda_{1i}}(e^{\\lambda_{2j}(t-r)}-e^{\\lambda_{1i}(t-r)})U_{1,ij}CU_{2}x_{r}\\right) (38) = \u222b 0 t \ud835\udc51 r \u200b \u2211 i \u200b j ( 1 \u03bb 2 \u200b j \u2212 \u03bb 1 \u200b i \u200b ( e \u03bb 2 \u200b j \u200b ( t \u2212 r ) \u2212 e \u03bb 1 \u200b i \u200b ( t \u2212 r ) ) \u200b U 1 , i \u200b j \u200b C \u200b U 2 ) \u200b x r absent superscript subscript 0 \ud835\udc61 differential-d \ud835\udc5f subscript \ud835\udc56 \ud835\udc57 1 subscript \ud835\udf06 2 \ud835\udc57 subscript \ud835\udf06 1 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udf06 2 \ud835\udc57 \ud835\udc61 \ud835\udc5f superscript \ud835\udc52 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc61 \ud835\udc5f subscript \ud835\udc48 1 \ud835\udc56 \ud835\udc57 \ud835\udc36 subscript \ud835\udc48 2 subscript \ud835\udc65 \ud835\udc5f \\displaystyle=\\int_{0}^{t}dr\\sum_{ij}\\left(\\frac{1}{\\lambda_{2j}-\\lambda_{1i}}(e^{\\lambda_{2j}(t-r)}-e^{\\lambda_{1i}(t-r)})U_{1,ij}CU_{2}\\right)x_{r} (39)\n\nThe memory function of two-layer linear RNN is\n\n\u03c1 ^ t = \u2211 i \u200b j ( 1 \u03bb 2 \u200b j \u2212 \u03bb 1 \u200b i \u200b ( e \u03bb 2 \u200b j \u200b ( t ) \u2212 e \u03bb 1 \u200b i \u200b ( t ) ) \u200b U 1 , i \u200b j \u200b C \u200b U 2 ) . subscript ^ \ud835\udf0c \ud835\udc61 subscript \ud835\udc56 \ud835\udc57 1 subscript \ud835\udf06 2 \ud835\udc57 subscript \ud835\udf06 1 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udf06 2 \ud835\udc57 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udf06 1 \ud835\udc56 \ud835\udc61 subscript \ud835\udc48 1 \ud835\udc56 \ud835\udc57 \ud835\udc36 subscript \ud835\udc48 2 \\hat{\\rho}_{t}=\\sum_{ij}\\left(\\frac{1}{\\lambda_{2j}-\\lambda_{1i}}(e^{\\lambda_{2j}(t)}-e^{\\lambda_{1i}(t)})U_{1,ij}CU_{2}\\right). (40)\n\nThe main advantage of two-layer linear RNN, compared with single-layer linear RNN, is that it can learn unbounded combination of exponential decay function as long as and are close enough. Notice that for simplicity we do not include the bias term in the evaluation of memory function. The multi-layer linear Recurrent Neural Network (RNN), including a bias term, can be perceived as a linear combination of various depth-specific linear RNNs, devoid of a bias term. Appendix B Proof\n\nB.1 Universal approximation\n\nHere we include the universal approximation from Barron [27], it is frequently used in the later constructive proof for state-space models. Theorem B.1 (Cybenko [28], Barron [27]). For any continous function and compact set . Assume the activation function is bounded and sigmoidal:\n\nlim z \u2192 \u221e \u03c3 \u200b ( z ) = 1 , lim z \u2192 \u2212 \u221e \u03c3 \u200b ( z ) = \u2212 1 . formulae-sequence subscript \u2192 \ud835\udc67 \ud835\udf0e \ud835\udc67 1 subscript \u2192 \ud835\udc67 \ud835\udf0e \ud835\udc67 1 \\lim_{z\\to\\infty}\\sigma(z)=1,\\lim_{z\\to-\\infty}\\sigma(z)=-1. (41)\n\nFor any , there exists such that\n\nsup x \u2208 K \u2016 f \u200b ( x ) \u2212 g \u200b ( x ) \u2016 \u2264 \u03f5 , subscript supremum \ud835\udc65 \ud835\udc3e norm \ud835\udc53 \ud835\udc65 \ud835\udc54 \ud835\udc65 italic-\u03f5 \\sup_{x\\in K}\\|f(x)-g(x)\\|\\leq\\epsilon, (42)\n\nwhere\n\ng \u200b ( x ) = C \u200b \u03c3 \u200b ( A \u200b x + b ) . \ud835\udc54 \ud835\udc65 \ud835\udc36 \ud835\udf0e \ud835\udc34 \ud835\udc65 \ud835\udc4f g(x)=C\\sigma(Ax+b). (43)\n\nB.2 Proof for Proposition 3.1\n\nProof. Fix the bounded continuous function . For any , according to the universal approximation theorem in Theorem B.1, there exists such that\n\n| f \u200b ( x ) \u2212 U 2 \u200b \u03c3 \u200b ( U 1 \u200b x + b 1 ) | \u2264 \u03f5 . \ud835\udc53 \ud835\udc65 subscript \ud835\udc48 2 \ud835\udf0e subscript \ud835\udc48 1 \ud835\udc65 subscript \ud835\udc4f 1 italic-\u03f5 |f(x)-U_{2}\\sigma(U_{1}x+b_{1})|\\leq\\epsilon. (44)\n\nTake , the two-layer state-space model can approximate element-wise continuous functions. (Here the element-wise function mean the sequences that for each , only depends on .) \u220e\n\nB.3 Proof for Proposition 3.2\n\nProof. The original statement is given in the discrete index. We prove the result for the continuous index and the discrete case can be derived by discretization. Without loss of generality, assume the input output sequence are all one-dimensional.",
    "ssmlayernonlin-23": "Otherwise we shall stacking the for different input-output channels. Since the input and output are one-dimensional, we know are vectors and is a square matrix. To approximate the target convolution:\n\ny t = \u222b 0 t \u03c1 t \u2212 s \u200b x s \u200b \ud835\udc51 s subscript \ud835\udc66 \ud835\udc61 superscript subscript 0 \ud835\udc61 subscript \ud835\udf0c \ud835\udc61 \ud835\udc60 subscript \ud835\udc65 \ud835\udc60 differential-d \ud835\udc60 y_{t}=\\int_{0}^{t}\\rho_{t-s}x_{s}ds (45)\n\nby state-space model\n\ny ^ t = \u222b 0 t c \u22a4 \u200b e W \u200b ( t \u2212 s ) \u200b u \u200b x s \u200b \ud835\udc51 s . subscript ^ \ud835\udc66 \ud835\udc61 superscript subscript 0 \ud835\udc61 superscript \ud835\udc50 top superscript \ud835\udc52 \ud835\udc4a \ud835\udc61 \ud835\udc60 \ud835\udc62 subscript \ud835\udc65 \ud835\udc60 differential-d \ud835\udc60 \\hat{y}_{t}=\\int_{0}^{t}c^{\\top}e^{W(t-s)}ux_{s}ds. (46)\n\nSince the prediction error can be decomposed into following form\n\ny t \u2212 y ^ t = \u222b 0 t ( \u03c1 t \u2212 s \u2212 c \u22a4 \u200b e W \u200b ( t \u2212 s ) \u200b u ) \u200b x s \u200b \ud835\udc51 s . subscript \ud835\udc66 \ud835\udc61 subscript ^ \ud835\udc66 \ud835\udc61 superscript subscript 0 \ud835\udc61 subscript \ud835\udf0c \ud835\udc61 \ud835\udc60 superscript \ud835\udc50 top superscript \ud835\udc52 \ud835\udc4a \ud835\udc61 \ud835\udc60 \ud835\udc62 subscript \ud835\udc65 \ud835\udc60 differential-d \ud835\udc60 y_{t}-\\hat{y}_{t}=\\int_{0}^{t}\\left(\\rho_{t-s}-c^{\\top}e^{W(t-s)}u\\right)x_{s}ds. (47)\n\nApproximating the temporal convolution with state-space model over all bounded inputs can be reduced to function approximating problem:\n\nmax s \u2208 [ 0 , T ] \u2061 | \u03c1 s \u2212 \u2211 i = 1 m c i \u200b e \u2212 \u03bb i \u200b s | < \u03f5 , \u03bb i \u2265 0 . formulae-sequence subscript \ud835\udc60 0 \ud835\udc47 subscript \ud835\udf0c \ud835\udc60 superscript subscript \ud835\udc56 1 \ud835\udc5a subscript \ud835\udc50 \ud835\udc56 superscript \ud835\udc52 subscript \ud835\udf06 \ud835\udc56 \ud835\udc60 italic-\u03f5 subscript \ud835\udf06 \ud835\udc56 0 \\max_{s\\in[0,T]}\\left|\\rho_{s}-\\sum_{i=1}^{m}c_{i}e^{-\\lambda_{i}s}\\right|<\\epsilon,\\quad\\lambda_{i}\\geq 0. (48)\n\nIn other words, approximating a convolution layer with state-space model is equivalent to approximating a general integrable function by function with exponential form . By change of variable, we have\n\nmax \u03c4 \u2208 [ e \u2212 T , 1 ] \u2061 | f \u200b ( \u03c4 ) \u2212 \u2211 i = 1 m c i \u200b \u03c4 \u03bb i | < \u03f5 . subscript \ud835\udf0f superscript \ud835\udc52 \ud835\udc47 1 \ud835\udc53 \ud835\udf0f superscript subscript \ud835\udc56 1 \ud835\udc5a subscript \ud835\udc50 \ud835\udc56 superscript \ud835\udf0f subscript \ud835\udf06 \ud835\udc56 italic-\u03f5 \\max_{\\tau\\in[e^{-T},1]}|f(\\tau)-\\sum_{i=1}^{m}c_{i}\\tau^{\\lambda_{i}}|<\\epsilon. (49)\n\nHere . Since polynomials are universal approximators on compact intervals, we know there exists such that the aforementioned inequality is satisfied. (For example, if is smooth, we can take the Taylor expansion of function .) \u220e\n\nB.4 Proof for Proposition 3.6\n\nProof. Our proof is based on Equation 18, for any sequence relationship , we need to approximate and , and subsequently approximate the representation prescribed by the Kolmogorov-Arnold theorem. Fix tolerance . As is shown in LABEL:{fig:Multi-layer_state_space_models_are_universal}, the first element-wise function can be approximated by two-layer state-space model. This is a result from the direct application of Proposition 3.1. In math terms, there exists such that two-layer state-space model approximate function . \u2016 \u03d5 \u200b ( x ) \u2212 U 2 \u200b \u03c3 \u200b ( U 1 \u200b x + b 1 ) \u2016 \u2264 \u03f5 . norm italic-\u03d5 \ud835\udc65 subscript \ud835\udc48 2 \ud835\udf0e subscript \ud835\udc48 1 \ud835\udc65 subscript \ud835\udc4f 1 italic-\u03f5 \\|\\phi(x)-U_{2}\\sigma(U_{1}x+b_{1})\\|\\leq\\epsilon. (50)\n\nWe shall denote by . Next, according to Proposition 3.2, we know the (temporal) convolution can be approximated via single-layer state-space model. There exists weights such that\n\n\u2016 \u2211 p = 1 T \u03c1 T \u2212 p \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) \u2212 \u2211 p = 1 T C 3 \u200b W 3 T \u2212 p \u200b U 3 \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) \u2016 \u2264 \u03f5 . norm superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udf0c \ud835\udc47 \ud835\udc5d ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udc36 3 superscript subscript \ud835\udc4a 3 \ud835\udc47 \ud835\udc5d subscript \ud835\udc48 3 ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e italic-\u03f5 \\left\\|\\sum_{p=1}^{T}\\rho_{T-p}\\hat{\\phi}(x_{p}+qa)-\\sum_{p=1}^{T}C_{3}W_{3}^{T-p}U_{3}\\hat{\\phi}(x_{p}+qa)\\right\\|\\leq\\epsilon. (51)\n\nThe last layer is again an element-wise function , which can be approximate by two-layer state-space model with weights . (LABEL:{fig:Multi-layer_state_space_models_are_universal})\n\n\u2016 \u03a6 \u200b ( \u2211 p = 1 T C 3 \u200b W 3 T \u2212 p \u200b U 3 \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) ) \u2212 U 5 \u200b ( U 4 \u200b ( \u2211 p = 1 T C 3 \u200b W 3 T \u2212 p \u200b U 3 \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) ) + b 4 ) \u2016 \u2264 \u03f5 . norm \u03a6 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udc36 3 superscript subscript \ud835\udc4a 3 \ud835\udc47 \ud835\udc5d subscript \ud835\udc48 3 ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e subscript \ud835\udc48 5 subscript \ud835\udc48 4 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udc36 3 superscript subscript \ud835\udc4a 3 \ud835\udc47 \ud835\udc5d subscript \ud835\udc48 3 ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e subscript \ud835\udc4f 4 italic-\u03f5 \\left\\|\\Phi(\\sum_{p=1}^{T}C_{3}W_{3}^{T-p}U_{3}\\hat{\\phi}(x_{p}+qa))-U_{5}(U_{4}(\\sum_{p=1}^{T}C_{3}W_{3}^{T-p}U_{3}\\hat{\\phi}(x_{p}+qa))+b_{4})\\right\\|\\leq\\epsilon. (52)\n\nBased on the above result, without loss of generality we assume the outer function is Lipschitz continuous with coefficient , then the final approximation error is bounded by . \u2016 \u03a6 \u200b ( \u2211 p = 1 T \u03c1 T \u2212 p \u200b \u03d5 \u200b ( x i + q \u200b a ) ) \u2212 U 5 \u200b ( U 4 \u200b ( \u2211 p = 1 T C 3 \u200b W 3 T \u2212 p \u200b U 3 \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) ) + b 4 ) \u2016 norm \u03a6 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udf0c \ud835\udc47 \ud835\udc5d italic-\u03d5 subscript \ud835\udc65 \ud835\udc56 \ud835\udc5e \ud835\udc4e subscript \ud835\udc48 5 subscript \ud835\udc48 4 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udc36 3 superscript subscript \ud835\udc4a 3 \ud835\udc47 \ud835\udc5d subscript \ud835\udc48 3 ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e subscript \ud835\udc4f 4 \\displaystyle\\left\\|\\Phi(\\sum_{p=1}^{T}\\rho_{T-p}\\phi(x_{i}+qa))-U_{5}(U_{4}(\\sum_{p=1}^{T}C_{3}W_{3}^{T-p}U_{3}\\hat{\\phi}(x_{p}+qa))+b_{4})\\right\\| (53) \u2264 \\displaystyle\\leq \u2016 \u03a6 \u200b ( \u2211 p = 1 T \u03c1 T \u2212 p \u200b \u03d5 \u200b ( x i + q \u200b a ) ) \u2212 \u03a6 \u200b ( \u2211 p = 1 T \u03c1 T \u2212 p \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) ) \u2016 norm \u03a6 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udf0c \ud835\udc47 \ud835\udc5d italic-\u03d5 subscript \ud835\udc65 \ud835\udc56 \ud835\udc5e \ud835\udc4e \u03a6 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udf0c \ud835\udc47 \ud835\udc5d ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e \\displaystyle\\left\\|\\Phi(\\sum_{p=1}^{T}\\rho_{T-p}\\phi(x_{i}+qa))-\\Phi(\\sum_{p=1}^{T}\\rho_{T-p}\\hat{\\phi}(x_{p}+qa))\\right\\| (54) + \\displaystyle+ \u2016 \u03a6 \u200b ( \u2211 p = 1 T \u03c1 T \u2212 p \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) ) \u2212 \u03a6 \u200b ( \u2211 p = 1 T C 3 \u200b W 3 T \u2212 p \u200b U 3 \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) ) \u2016 norm \u03a6 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udf0c \ud835\udc47 \ud835\udc5d ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e \u03a6 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udc36 3 superscript subscript \ud835\udc4a 3 \ud835\udc47 \ud835\udc5d subscript \ud835\udc48 3 ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e \\displaystyle\\left\\|\\Phi(\\sum_{p=1}^{T}\\rho_{T-p}\\hat{\\phi}(x_{p}+qa))-\\Phi(\\sum_{p=1}^{T}C_{3}W_{3}^{T-p}U_{3}\\hat{\\phi}(x_{p}+qa))\\right\\| (55) + \\displaystyle+ \u2016 \u03a6 \u200b ( \u2211 p = 1 T C 3 \u200b W 3 T \u2212 p \u200b U 3 \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) ) \u2212 U 5 \u200b ( U 4 \u200b ( \u2211 p = 1 T C 3 \u200b W 3 T \u2212 p \u200b U 3 \u200b \u03d5 ^ \u200b ( x p + q \u200b a ) ) + b 4 ) \u2016 norm \u03a6 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udc36 3 superscript subscript \ud835\udc4a 3 \ud835\udc47 \ud835\udc5d subscript \ud835\udc48 3 ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e subscript \ud835\udc48 5 subscript \ud835\udc48 4 superscript subscript \ud835\udc5d 1 \ud835\udc47 subscript \ud835\udc36 3 superscript subscript \ud835\udc4a 3 \ud835\udc47 \ud835\udc5d subscript \ud835\udc48 3 ^ italic-\u03d5 subscript \ud835\udc65 \ud835\udc5d \ud835\udc5e \ud835\udc4e subscript \ud835\udc4f 4 \\displaystyle\\left\\|\\Phi(\\sum_{p=1}^{T}C_{3}W_{3}^{T-p}U_{3}\\hat{\\phi}(x_{p}+qa))-U_{5}(U_{4}(\\sum_{p=1}^{T}C_{3}W_{3}^{T-p}U_{3}\\hat{\\phi}(x_{p}+qa))+b_{4})\\right\\| (56) \u2264 \\displaystyle\\leq L \u200b \u2016 \u03c1 \u2016 L 1 \u200b \u03f5 + L \u200b \u03f5 + \u03f5 . \ud835\udc3f subscript norm \ud835\udf0c superscript \ud835\udc3f 1 italic-\u03f5 \ud835\udc3f italic-\u03f5 italic-\u03f5 \\displaystyle L\\|\\rho\\|_{L^{1}}\\epsilon+L\\epsilon+\\epsilon. (57)\n\nTo summarize, we achieve the approximation of general nonlinear sequence-to-sequence relationship with representation (with Lipschitz continuous ) via five-layer state-space model. Since the Lipscthiz continuous function is dense in the set of continuous function, therefore the universal approximation can be generalized to the representation with not necessarily Lipschitz continuous. \u220e\n\nB.5 Proof for Proposition 3.9\n\nProof. For simplicity, we will only present the approximation of an -th order component of the Volterra Series. The general approximation of nonlinear sequence-to-sequence relationship can be achieved by approximating different order component separately and taking the linear combination. Consider the target functional\n\ny n \u200b ( t ) = \u222b 0 t \u22ef \u200b \u222b 0 t h n \u200b ( \u03c4 1 , \u2026 , \u03c4 n ) \u200b \u220f j = 1 n x \u200b ( t \u2212 \u03c4 j ) \u200b d \u200b \u03c4 j . subscript \ud835\udc66 \ud835\udc5b \ud835\udc61 superscript subscript 0 \ud835\udc61 \u22ef superscript subscript 0 \ud835\udc61 subscript \u210e \ud835\udc5b subscript \ud835\udf0f 1 \u2026 subscript \ud835\udf0f \ud835\udc5b superscript subscript product \ud835\udc57 1 \ud835\udc5b \ud835\udc65 \ud835\udc61 subscript \ud835\udf0f \ud835\udc57 \ud835\udc51 subscript \ud835\udf0f \ud835\udc57 y_{n}(t)=\\int_{0}^{t}\\cdots\\int_{0}^{t}h_{n}(\\tau_{1},\\dots,\\tau_{n})\\prod_{j=1}^{n}x(t-\\tau_{j})d\\tau_{j}. (58)\n\nThe state-space model\u2019s -th order term can be represented by\n\ny ^ n \u200b ( t ) = \u2211 i = 1 m \u222b 0 t \u22ef \u200b \u222b 0 t ( \u220f j = 1 n h ^ j ( m ) \u200b ( \u03c4 j ) ) \u200b \u220f j = 1 n x \u200b ( t \u2212 \u03c4 j ) \u200b d \u200b \u03c4 j . subscript ^ \ud835\udc66 \ud835\udc5b \ud835\udc61 superscript subscript \ud835\udc56 1 \ud835\udc5a superscript subscript 0 \ud835\udc61 \u22ef superscript subscript 0 \ud835\udc61 superscript subscript product \ud835\udc57 1 \ud835\udc5b superscript subscript ^ \u210e \ud835\udc57 \ud835\udc5a subscript \ud835\udf0f \ud835\udc57 superscript subscript product \ud835\udc57 1 \ud835\udc5b \ud835\udc65 \ud835\udc61 subscript \ud835\udf0f \ud835\udc57 \ud835\udc51 subscript \ud835\udf0f \ud835\udc57 \\hat{y}_{n}(t)=\\sum_{i=1}^{m}\\int_{0}^{t}\\cdots\\int_{0}^{t}\\left(\\prod_{j=1}^{n}\\hat{h}_{j}^{(m)}(\\tau_{j})\\right)\\prod_{j=1}^{n}x(t-\\tau_{j})d\\tau_{j}. (59)\n\nIt can be seen the kernel function of state-space model is while the original -th order kernel is multi-variable function . For any tolerance , there exists a sufficiently large hidden dimension such that the multi-variable function is approximated by the single-variable function\u2019s product:\n\n| \u2211 i = 1 m \u220f j = 1 n h ^ j ( m ) \u200b ( \u03c4 j ) \u2212 h n \u200b ( \u03c4 1 , \u2026 , \u03c4 n ) | \u2264 \u03f5 . superscript subscript \ud835\udc56 1 \ud835\udc5a superscript subscript product \ud835\udc57 1 \ud835\udc5b superscript subscript ^ \u210e \ud835\udc57 \ud835\udc5a subscript \ud835\udf0f \ud835\udc57 subscript \u210e \ud835\udc5b subscript \ud835\udf0f 1 \u2026 subscript \ud835\udf0f \ud835\udc5b italic-\u03f5 \\left|\\sum_{i=1}^{m}\\prod_{j=1}^{n}\\hat{h}_{j}^{(m)}(\\tau_{j})-h_{n}(\\tau_{1},\\dots,\\tau_{n})\\right|\\leq\\epsilon. (60)\n\nFor example, we may consider to be polynomial of and take the Taylor expansion of kernel . \u220e\n\nRemark B.2. While the aforementioned proof for approximation is provided through a polynomial method, it\u2019s commonly accepted that polynomials may not be the most efficient means to parameterize kernel functions. Consequently, a variety of techniques have been empirically investigated to represent convolutional layers [22]. B.6 Proof for Proposition 3.11\n\nProof. As converges to exponentially and is integrable, we know the limit of exists\n\nlim t \u2192 \u221e y t = lim t \u2192 \u221e \u222b 0 \u221e \u03c1 s \u200b x t \u2212 s \u200b \ud835\udc51 s = \u222b 0 \u221e \u03c1 s \u200b lim t \u2192 \u221e x t \u2212 s \u200b d \u200b s = \u222b 0 \u221e \u03c1 s \u200b x \u2217 \u200b \ud835\udc51 s . subscript \u2192 \ud835\udc61 subscript \ud835\udc66 \ud835\udc61 subscript \u2192 \ud835\udc61 superscript subscript 0 subscript \ud835\udf0c \ud835\udc60 subscript \ud835\udc65 \ud835\udc61 \ud835\udc60 differential-d \ud835\udc60 superscript subscript 0 subscript \ud835\udf0c \ud835\udc60 subscript \u2192 \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \ud835\udc60 \ud835\udc51 \ud835\udc60 superscript subscript 0 subscript \ud835\udf0c \ud835\udc60 superscript \ud835\udc65 differential-d \ud835\udc60 \\displaystyle\\lim_{t\\to\\infty}y_{t}=\\lim_{t\\to\\infty}\\int_{0}^{\\infty}\\rho_{s}x_{t-s}ds=\\int_{0}^{\\infty}\\rho_{s}\\lim_{t\\to\\infty}x_{t-s}ds=\\int_{0}^{\\infty}\\rho_{s}x^{*}ds. (61)\n\nHere is the memory function for (continuous) state-space model. By the dominated convergence theorem, as is integrable and is a bounded sequence,\n\nlim t \u2192 \u221e e c 0 \u200b t \u200b \u2016 y t \u2212 y \u2217 \u2016 subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 norm subscript \ud835\udc66 \ud835\udc61 superscript \ud835\udc66 \\displaystyle\\lim_{t\\to\\infty}e^{c_{0}t}\\|y_{t}-y^{*}\\| = lim t \u2192 \u221e e c 0 \u200b t \u200b \u2016 \u222b 0 \u221e \u03c1 s \u200b ( x t \u2212 s \u2212 x \u2217 ) \u200b \ud835\udc51 s \u2016 absent subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 norm superscript subscript 0 subscript \ud835\udf0c \ud835\udc60 subscript \ud835\udc65 \ud835\udc61 \ud835\udc60 superscript \ud835\udc65 differential-d \ud835\udc60 \\displaystyle=\\lim_{t\\to\\infty}e^{c_{0}t}\\left\\|\\int_{0}^{\\infty}\\rho_{s}(x_{t-s}-x^{*})ds\\right\\| (62) \u2264 \u2016 \u222b 0 \u221e | \u200b \u03c1 s \u200b | lim t \u2192 \u221e e c 0 \u200b t | \u200b x t \u2212 s \u2212 x \u2217 \u200b | d \u200b s \u2016 absent delimited-\u2016| superscript subscript 0 subscript \ud835\udf0c \ud835\udc60 subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \ud835\udc60 superscript \ud835\udc65 delimited-|\u2016 \ud835\udc51 \ud835\udc60 \\displaystyle\\leq\\left\\|\\int_{0}^{\\infty}|\\rho_{s}|\\lim_{t\\to\\infty}e^{c_{0}t}|x_{t-s}-x^{*}|ds\\right\\| (63) = \u2016 \u222b 0 \u221e | \u03c1 s | \u22c5 0 \u200b \ud835\udc51 s \u2016 = 0 . absent norm superscript subscript 0 \u22c5 subscript \ud835\udf0c \ud835\udc60 0 differential-d \ud835\udc60 0 \\displaystyle=\\left\\|\\int_{0}^{\\infty}|\\rho_{s}|\\cdot 0ds\\right\\|=0. (64)\n\nB.7 Proof for Proposition 3.12\n\nProof. Since , by continuity of the activation function\n\nlim t \u2192 \u221e y t = lim t \u2192 \u221e \u03c3 \u200b ( x t ) = \u03c3 \u200b ( lim t \u2192 \u221e x t ) = \u03c3 \u200b ( x \u2217 ) . subscript \u2192 \ud835\udc61 subscript \ud835\udc66 \ud835\udc61 subscript \u2192 \ud835\udc61 \ud835\udf0e subscript \ud835\udc65 \ud835\udc61 \ud835\udf0e subscript \u2192 \ud835\udc61 subscript \ud835\udc65 \ud835\udc61 \ud835\udf0e superscript \ud835\udc65 \\lim_{t\\to\\infty}y_{t}=\\lim_{t\\to\\infty}\\sigma(x_{t})=\\sigma(\\lim_{t\\to\\infty}x_{t})=\\sigma(x^{*}). (65)\n\nHereafter we define . Since is Lipschitz continuous, let be its Lipschitz constant\n\nlim t \u2192 \u221e e c 0 \u200b t \u200b \u2016 y t \u2212 y \u2217 \u2016 subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 norm subscript \ud835\udc66 \ud835\udc61 superscript \ud835\udc66 \\displaystyle\\lim_{t\\to\\infty}e^{c_{0}t}\\|y_{t}-y^{*}\\| = lim t \u2192 \u221e e c 0 \u200b t \u200b \u2016 \u03c3 \u200b ( x t ) \u2212 \u03c3 \u200b ( x \u2217 ) \u2016 absent subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 norm \ud835\udf0e subscript \ud835\udc65 \ud835\udc61 \ud835\udf0e superscript \ud835\udc65 \\displaystyle=\\lim_{t\\to\\infty}e^{c_{0}t}\\|\\sigma(x_{t})-\\sigma(x^{*})\\| (66) \u2264 lim t \u2192 \u221e e c 0 \u200b t \u200b L \u200b \u2016 x t \u2212 x \u2217 \u2016 absent subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 \ud835\udc3f norm subscript \ud835\udc65 \ud835\udc61 superscript \ud835\udc65 \\displaystyle\\leq\\lim_{t\\to\\infty}e^{c_{0}t}L\\|x_{t}-x^{*}\\| (67) = L \u200b lim t \u2192 \u221e e c 0 \u200b t \u200b \u2016 x t \u2212 x \u2217 \u2016 absent \ud835\udc3f subscript \u2192 \ud835\udc61 superscript \ud835\udc52 subscript \ud835\udc50 0 \ud835\udc61 norm subscript \ud835\udc65 \ud835\udc61 superscript \ud835\udc65 \\displaystyle=L\\lim_{t\\to\\infty}e^{c_{0}t}\\|x_{t}-x^{*}\\| (68) = 0 . absent 0 \\displaystyle=0. (69)\n\nThe Lipschitz continuity is a weak assumption that most of the activations such as ReLU, GeLU, tanh, hardtanh satisfy. Appendix C Limitations\n\nTheoretical results on approximation do not have quantitative results. Therefore it is not easy to directly compare the efficiency of different types of models over a specific task. The good news is that the general memory function servers as a guideline in constructing models. Take the Hyena [12] architecture as an example, as the implicit representation of convolution kernel is utilized in the network, Hyena is not a recurrent model. If we replace the convolution layer by state-space model, we can \u201ctranslate\u201d a non-recurrent neural network into a recurrent neural network without sacrificing the approximation capacity. The main advantage of recurrent networks, compared with implicit convolution form, lies in the lower inference memory cost. This study primarily explores the qualitative attributes of the state space model within the context of approximation theory. To better delineate it from other architectures like linear transformers, a more in-depth investigation into the rate of approximation is necessary. Appendix D Comparisons between RNNs, SSMs and S4\n\nIn Table 2, we compare different recurrent models in terms of the recurrence, universality, temporal parallel and the memory decay speed. Our core findings are established for SSM; however, we argue that they are applicable to S4 as well. The primary differences between the vanilla state-space models (SSMs) and S4 involve model parameterisation, weight initialisation, discretisation, normalisation, dropout, and residual connections. The model architectures of SSMs and S4 are almost identical, as both alternately stack linear RNNs and nonlinear activation layers. Therefore, in terms of universal approximation, the approximation capacities of both SSMs and S4 are equivalent. Appendix E Further discussion on the two proofs for universality of SSMs\n\nWe provided comparison between the two proof methods: An analogy between Kolmogorov-theorem and classical fully-connected neural networks can be drawn because of the finite number of layers in both. It is shown in Eldan and Shamir [29] that a simple function expressible by a small 3-layer feed-forward neural networks cannot be approximated to a certain accuracy unless the network\u2019s width is exponential in the dimension. In contrast, Volterra-Series shares similarities with deep learning, and as a result, the advantages of deep learning over classical fully-connected neural networks carry over to this approach. The authors are inclined to consider Volterra-Series based construction as relatively superior, and increasing the depth to be a more efficient way to scale up the model. \u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Feb 28 04:35:39 2024 by LaTeXML"
}