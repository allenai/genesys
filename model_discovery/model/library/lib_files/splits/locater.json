{
    "locater-0": "# Self-attention Networks Localize When QK-eigenspectrum Concentrates \n\nHan Bao ${ }^{1, \\dagger}$, Ryuichiro Hataya ${ }^{2}$, and Ryo Karakida ${ }^{3}$<br>${ }^{1}$ Kyoto University<br>${ }^{2}$ RIKEN AIP<br>${ }^{3}$ AIST<br>${ }^{\\dagger}$ bao@i.kyoto-u.ac.jp\n\nFebruary 6, 2024\n\n\n#### Abstract\n\nThe self-attention mechanism prevails in modern machine learning.",
    "locater-1": "It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability. ## 1 Introduction\n\nTransformers have been widely adopted in language modeling [28], vision tasks [7, 27], and speech recognition [13]. A crucial building block in transformers is the attention mechanism, dating back to Graves [8], which was initially designed to capture long-range signals in sequential inputs by mixing individual tokens but has also been leveraged to capture general structures of input data. After the fully-attentionbased language model has appeared [5], the research community gets interested in the functionality and benefits of the attention. To mention a few, transformers implicitly prefer hierarchical interpretations of input sequences [10]; store relational knowledge in MLP layers as an associative memory [15]; its computational graphs tend to be tree-structured [18]; suddenly capture tree structures of inputs after long training epochs [17]. Theoretically, training dynamics analysis explains how to learn spatially correlated patches by vision transformers (ViT) [9], select dominant tokens [25], store information as an associative memory [3], and select max-margin tokens [24], whereas Xie et al. [29] explains the in-context learning as a process of concept learning in Bayesian inference. Among many aspects of attention, we specifically focus on localization-for a query token, selfattention can select a few relevant tokens only (which we call localized attention) or select many tokens\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-02.jpg?height=569&width=780&top_left_y=275&top_left_x=638)\n\nFigure 1: Comparison of softmax $\\mathbf{S}$ and the piecewise approximation $\\widetilde{\\mathbf{S}}$ for two-dimensional inputs. uniformly. As attention can be regarded as a token mixer, it plays a pivotal role in studying how it selects tokens to reveal the characteristics of the token embeddings. To this end, we have the following research questions: (Q1) When is self-attention localized or uniform?(Q2) How does localization affect model performances?",
    "locater-2": "Along this line, previous studies mainly investigated from the model expressivity and training stability perspectives. On the one hand, Dong et al. [6] and Noci et al. [19] initiated the discussion of attention localization and theoretically showed that a network with self-attention layers without skip connections exponentially loses the rank of hidden layers; the fact indicates that the model expressivity shall be immediately lost with more self-attention layers stacked. On the other hand, Zhai et al. [31] empirically found that attention entropy-averaged Shannon entropy of an attention probability matrix-correlates with training stability. Specifically, a training loss curve tends to fall into a plateau when attention entropy is low. Since higher entropy indicates near-uniform attention weights, their finding apparently suggests that localized attention may lead the learning dynamics to a plateau. Up until now, these two failure modes have been discussed independently with slightly different notions of attention localization, and hence, our understanding of the blessing and curse of attention localization remains elusive. To better comprehend, we characterize self-attention patterns by attention parameter matrices to reconcile the two collapse modes. We formulate the concept of localization by signal propagation probability (Section 3), which describes how likely the signal of a specific input token propagates to the gradient of a training objective. If the signal propagation probability is high for a few numbers of tokens only, attention is regarded to be localized. We show that the localization mode can be characterized by the eigenspectrum of attention weight matrices (Section 4). Specifically, attention is localized in the above sense when the eigenspectrum of the query-key parameter matrix has a non-zero mean and a small variance. Furthermore, the small eigenspectrum variance is relevant to both the rank collapse and entropy collapse (Section 5), and thus, we give a unified perspective of the two notions of attention collapse. For this reason, we argue that attention collapse and its performance can be viewed more transparently based on the eigenspectrum variance. Lastly, we verified the correlation of the eigenspectrum and the model performance in the experiments with the WikiText dataset [16] by introducing a regularization scheme called LocAteR.",
    "locater-3": "## 2 Setup\n\nNotation. We write vectors with all zeros and ones by 0 and $\\mathbf{1}$, respectively, whereas the $i$-th one-hot vector is written as $\\mathbf{e}^{i}$. A vector is written in bold-face like $\\mathbf{a}$, and its $i$-th scalar element is written by non-bold $a_{i}$. A matrix is written by capital bold-face like $\\mathbf{A}$, and $\\mathbf{A}_{i}$ denotes its $i$-th column vector\nunless otherwise noted. The identity matrix is denoted by I. The Hadamard product of $\\mathbf{A}$ and $\\mathbf{A}$ is written by $\\mathbf{A}^{\\odot 2}:=\\mathbf{A} \\odot \\mathbf{A}$. We write the error function as erf and use $\\operatorname{erf}(-z)=-\\operatorname{erf}(z)$ without explicitly mentioning it. Infinitesimal asymptotic orders $o(\\cdot)$ are with respect to the sequence length $T$ unless otherwise noted. Transformer. Let $\\mathbf{X}:=\\left[\\begin{array}{llll}\\mathbf{x}_{1} & \\mathbf{x}_{2} \\ldots & \\ldots & \\mathbf{x}_{T}\\end{array}\\right] \\in \\mathbb{R}^{d \\times T}$ be an input with $T$ tokens, defined later shortly. We suppose that all input sequences have the same length $T$, and $T$ is occasionally taken sufficiently large. The $\\ell$-th (single-head) self-attention layer is defined as\n\n$$\n\\begin{aligned}\n& \\mathbf{A}^{\\ell}:=\\mathbf{S}\\left(\\frac{\\left(\\mathbf{X}^{\\ell-1}\\right)^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{X}^{\\ell-1}}{\\lambda}\\right) \\\\\n& \\mathbf{U}^{\\ell}:=\\mathbf{W}_{\\mathrm{V}} \\mathbf{X}^{\\ell-1} \\mathbf{A}^{\\ell}\n\\end{aligned}\n$$\n\nwhere $\\mathbf{W}_{\\mathrm{V}} \\in \\mathbb{R}^{d \\times d}$ is the value parameters, $\\mathbf{W}_{\\mathrm{QK}}\\left(:=\\mathbf{W}_{\\mathrm{Q}}^{\\top} \\mathbf{W}_{\\mathrm{K}}\\right) \\in \\mathbb{R}^{d \\times d}$ is the query-key parameters (with joint parametrization), $\\lambda>0$ is temperature, commonly $\\lambda=\\sqrt{d}$, and $\\mathbf{S}: \\mathbb{R}^{T} \\rightarrow \\mathbb{R}^{T}$ is the softmax applied for each row. In this way, each input token in $\\mathbf{X}^{\\ell-1}$ (embedded by $\\mathbf{W}_{\\mathrm{V}}$ ) is mixed by $\\mathbf{A}^{\\ell}$. Then, the transformer block (without layer normalization [1]) is defined as\n\n$$\n\\begin{aligned}\n& \\mathbf{Z}^{\\ell}:=\\mathbf{U}^{\\ell}+\\mathbf{X}^{\\ell-1} \\\\\n& \\mathbf{H}^{\\ell}:=\\mathbf{W}_{\\mathrm{F}_{2}} \\sigma\\left(\\mathbf{W}_{\\mathrm{F}_{1}} \\mathbf{Z}^{\\ell}\\right) \\\\\n& \\mathbf{X}^{\\ell}:=\\mathbf{H}^{\\ell}+\\mathbf{Z}^{\\ell}\n\\end{aligned}\n$$\n\nwhere $\\mathbf{H}^{\\ell}$ is a feed-forward net with parameters $\\mathbf{W}_{\\mathrm{F}_{1}}, \\mathbf{W}_{\\mathrm{F}_{2}} \\in \\mathbb{R}^{d \\times d}$ and an (element-wise) activation $\\sigma: \\mathbb{R} \\rightarrow \\mathbb{R}$. We omit the token embedding layer and set $\\mathbf{X}^{0}:=\\mathbf{X}$. There are two common variants of layer normalization positions, Post-LN [28] and Pre-LN [30], which are applied token-wise after the residual connections ( $\\mathbf{Z}^{\\ell}$ and $\\mathbf{X}^{\\ell+1}$ ) and before the inputs ( $\\mathbf{X}^{\\ell}$ and $\\mathbf{Z}^{\\ell}$ ), respectively. Then, the transformer block $\\mathbf{X}^{\\ell}$ is stacked $L$ times and $\\mathbf{F}(\\mathbf{X}):=\\mathbf{X}^{L} \\in \\mathbb{R}^{d \\times T}$ is the output. Learning task. We focus on causal language modeling, where a model predicts the next token given contextual tokens. Formally, given $T$ contextual tokens $\\mathbf{X} \\in \\mathbb{R}^{d \\times T}$, the prediction target is the $(T+1)$-th token $\\mathbf{y}:=\\mathbf{x}_{T+1} \\in \\mathbb{R}^{d}$. With the squared loss, the objective is written as follows:\n\n$$\nJ(\\boldsymbol{\\Theta}):=\\frac{1}{2} \\mathbb{E}\\left\\|\\mathbf{y}-\\mathbf{F}(\\mathbf{X})_{T}\\right\\|^{2}\n$$\n\nwhere $\\boldsymbol{\\Theta}:=\\left(\\mathbf{W}_{\\mathrm{V}}, \\mathbf{W}_{\\mathrm{QK}}, \\mathbf{W}_{\\mathrm{F}_{1}}, \\mathbf{W}_{\\mathrm{F}_{2}}\\right)$ denotes the model parameter set, and the expectation is taken over input sequences $(\\mathbf{X}, \\mathbf{y})$. Here, our decoding procedure in consideration is to simply choose the embedded last token $\\mathbf{F}(\\mathbf{X})_{T} \\in \\mathbb{R}^{T}$. The parameters $\\boldsymbol{\\Theta}$ are learned by minimizing $J$. Note that our analysis considers optimizing the query-key parameters jointly. Although such joint parameterization is less common in practice, it is convenient for the theoretical derivation of the gradients and has been used in several previous studies [9,25]. Interested readers may refer to a recent work revealing that the joint and separate QK-parametrization lead to different implicit regularizations [23]. Picewise linear approximation of softmax. In this article, we choose to approximate the softmax function $\\mathbf{S}$ by linearization. For a $T$-dimensional input $\\omega \\in \\mathbb{R}^{T}$, the softmax function is defined as\n\n$$\nS(\\boldsymbol{\\omega})_{i}:=\\frac{\\exp \\left(\\omega_{i}\\right)}{\\sum_{j \\in[T]} \\exp \\left(\\omega_{j}\\right)} \\text { for all } i \\in[T]\n$$\n\nFor linearization, the Taylor expansion of $S(\\boldsymbol{\\omega})_{i}$ around the origin $\\left\\langle\\gamma^{i}, \\omega\\right\\rangle+\\gamma_{0}^{i}$ is used, where\n\n$$\n\\gamma^{i}:=\\nabla_{i} \\mathbf{S}(\\mathbf{0})=\\frac{1}{T} \\mathbf{e}^{i}-\\frac{1}{T^{2}} \\mathbf{1}, \\quad \\gamma_{0}^{i}:=S(\\mathbf{0})_{i}=\\frac{1}{T}\n$$\n\nThen, we approximate $\\mathbf{S}$ by the piecewise linear function such that $S(\\boldsymbol{\\omega})_{i} \\approx \\max \\left\\{0, \\min \\left\\{1,\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\right.\\right.$ $\\left.\\left.\\gamma_{0}^{i}\\right\\}\\right\\}=\\left\\langle\\widetilde{\\gamma}^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\widetilde{\\gamma}_{0}^{i}$, where\n\n$$\n\\left(\\widetilde{\\boldsymbol{\\gamma}}^{i}, \\widetilde{\\gamma}_{0}^{i}\\right)= \\begin{cases}(\\mathbf{0}, 0) & \\text { if }\\left\\langle\\boldsymbol{\\gamma}^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i}<0 \\\\ \\left(\\boldsymbol{\\gamma}^{i}, \\gamma_{0}^{i}\\right) & \\text { if }\\left\\langle\\boldsymbol{\\gamma}^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i} \\in[0,1] \\\\ (\\mathbf{0}, 1) & \\text { if }\\left\\langle\\boldsymbol{\\gamma}^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i}>1\\end{cases}\n$$\n\nIn the vector form, the piecewise approximation $\\mathbf{S}(\\boldsymbol{\\omega}) \\approx \\widetilde{\\mathbf{S}}(\\boldsymbol{\\omega})$ is given by\n\n$$\n\\widetilde{\\mathbf{S}}(\\boldsymbol{\\omega})=\\boldsymbol{\\Gamma}^{\\top} \\boldsymbol{\\omega}+\\widetilde{\\gamma}_{0}, \\text { where }\\left\\{\\begin{array}{l}\n\\boldsymbol{\\Gamma}:=\\left[\\widetilde{\\gamma}^{1} \\widetilde{\\gamma}^{2} \\ldots \\widetilde{\\boldsymbol{\\gamma}}^{T}\\right] \\\\\n\\widetilde{\\gamma}_{0}=\\left[\\widetilde{\\gamma}_{0}^{1}, \\widetilde{\\gamma}_{0}^{2}, \\ldots, \\widetilde{\\gamma}_{0}^{T}\\right]^{\\top}\n\\end{array}\\right.",
    "locater-4": "$$\n\nFor notational simplicity, the column vectors of $\\Gamma$ are exceptionally denoted by $\\widetilde{\\gamma}^{i}$ with superscripts, for which the $\\alpha$-th element is written by $\\widetilde{\\gamma}_{\\alpha}^{i}$.",
    "locater-5": "The difference between $\\mathbf{S}$ and $\\widetilde{\\mathbf{S}}$ is illustrated in Fig. 1. Note that a popular alternative to softmax, sparsemax [14], is also a piecewise linear function, although the functional form is slightly different from $\\widetilde{\\mathbf{S}}$. Remark 1. Each $\\left(\\widetilde{\\gamma}^{i}, \\widetilde{\\gamma}_{0}^{i}\\right)$ depends on the softmax input $\\omega$, unlike the Taylor coefficient $\\left(\\gamma^{i}, \\gamma_{0}^{i}\\right)$ being independent from $\\omega$. This point matters particularly when we take expectations of terms involving $\\left(\\widetilde{\\gamma}^{i}, \\widetilde{\\gamma}_{0}^{i}\\right)$. Remark 2. When $T$ is sufficiently large, the coefficient vector $\\gamma^{i}=T^{-1} \\mathbf{e}^{i}+o(1)$. In this regime, $\\gamma_{\\alpha}^{i}=$ $o(1)$ for any $\\alpha \\in[T] \\backslash\\{i\\}$, so $\\gamma^{i}$ behaves like a selector of the $i$-th input. Hence, $\\gamma_{i}^{i}=\\gamma_{0}^{i}=T^{-1}+o(1)$. ## 3 Signal propagation probability\n\nWe analyze how much each token contributes to the learning dynamics. To this end, we formalize how much the signal of a specific input token $\\mathbf{x}_{i}$ propagates to the gradient $\\nabla J$. Remark that this notion is slightly different from the contribution of an input token $\\mathbf{x}_{i}$ to the model output $\\mathbf{F}(\\mathbf{X})_{j}$ analyzed by Kobayashi et al.",
    "locater-6": "[12] recently. Uniform vs. localized softmax. The piecewise linear approximation implies that the $i$-th input signal is propagated to the subsequent blocks when $\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i} \\in[0,1]$; otherwise, $\\widetilde{S}(\\boldsymbol{\\omega})_{i}=\\left\\langle\\widetilde{\\gamma}^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\widetilde{\\gamma}_{0}^{i}=\\widetilde{\\gamma}_{0}^{i}$, which hinders the input token $\\mathbf{x}_{i}$ from contributing to the self-attention layer (2). Thus, we will focus on the following quantity. Definition 1 (Signal propagation probability). Suppose that $\\mathbf{W}_{\\mathrm{QK}}$ is independent from $\\mathbf{X}$. For $i \\in[T]$, the signal propagation probability of the $i$-th token is defined as follows:\n\n$$\n\\rho_{i}:=\\mathbb{P}\\left\\{\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i} \\in[0,1]\\right\\}\n$$\n\nwhere $\\boldsymbol{\\omega}:=\\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} / \\lambda$ and the randomness originates solely from the input tokens $\\mathbf{X}$. When only a few $\\rho_{i}$ are significantly larger than zero, we can interpret it as localized softmax; in this case, the self-attention (2) is dominated by a small number of tokens. By contrast, most of the tokens contribute to self-attention almost equally if $\\rho_{i}$ takes a similar value across different $i$; this situation is interpreted as uniform softmax. Through the lens of gradient. The signal propagation probability naturally arises in the gradient. Since the learning dynamics of causal language modeling is governed by the gradient flow of $J$, we can benefit from deriving the gradient of $J$ to see how attention affects the learning dynamics. To keep the derivation concise, we consider a 1-layer transformer (where we drop the superscripts $\\ell$ ) without layer normalization\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-05.jpg?height=753&width=1381&top_left_y=278&top_left_x=337)\n\nFigure 2: The theoretical plots of the signal propagation probability $\\rho(\\theta)$ with different $\\xi=\\operatorname{tr}(\\mathbf{W}) / \\sqrt{\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)}$ and $\\eta=$ $\\sqrt{\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)} / \\lambda^{2}$. The vertical axes indicate relative token position $\\theta=i / T$ ( $i$ : token index, $T$ : number of tokens). Smaller $\\theta$ close to zero and larger $\\theta$ close to one correspond to early-site and late-site tokens, respectively. and simplify the feed-forward net $\\mathbf{H}$ by supposing the identity activation. With the approximated softmax $\\widetilde{\\mathbf{S}}$, the transformer can be written as follows:\n\n$$\n\\begin{aligned}\n\\mathbf{F}(\\mathbf{X})_{T} & =\\mathbf{W}_{\\mathrm{F}}\\left\\{\\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\widetilde{\\mathbf{S}}(\\boldsymbol{\\omega})+\\mathbf{x}_{T}\\right\\} \\\\\n& =\\mathbf{W}_{\\mathrm{F}}\\left\\{\\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\boldsymbol{\\Gamma}^{\\top} \\boldsymbol{\\omega}+\\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\widetilde{\\boldsymbol{\\gamma}}_{0}+\\mathbf{x}_{T}\\right\\}\n\\end{aligned}\n$$\n\nwhere $\\mathbf{W}_{\\mathrm{F}}:=\\mathbf{W}_{\\mathrm{F}_{2}} \\mathbf{W}_{\\mathrm{F}_{1}}+\\mathbf{I}$ and $\\boldsymbol{\\omega}:=\\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} / \\lambda$. For this architecture, the QK-gradient is computed:\n\n$$\n\\begin{aligned}\n\\nabla_{\\mathbf{w}_{\\mathrm{QK}}} J= & \\lambda^{-2} \\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{P} \\boldsymbol{\\Gamma}^{\\top} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& +\\lambda^{-1} \\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\widetilde{\\mathbf{\\gamma}}_{0} \\mathbf{x}_{T}^{\\top}\\right]+\\lambda^{-1} \\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{q} \\mathbf{x}_{T}^{\\top}\\right]\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n\\mathbf{P} & :=\\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top} \\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\\\\n\\mathbf{q} & :=\\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{x}_{T}-\\mathbf{y}\\right)\n\\end{aligned}\n$$\n\nWhen $T$ is sufficiently large, we can drop asymptotically negligible terms with respect to $T$ as detailed in Appendix B, and the QK-gradient (3) is simplified as follows:\n\n$$\n\\frac{1}{\\lambda^{2}} \\sum_{i, j, \\alpha, \\beta \\in[T]} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{\\beta}^{j}\\left(\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}} \\mathbf{x}_{j}\\right)\\left(\\mathbf{x}_{\\beta}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right) \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right]\n$$\n\nwhere $\\check{\\mathbf{P}}:=\\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top} \\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}}$. Now, in the gradient term (4), the summands with $\\widetilde{\\gamma}_{i}^{i}$ are asymptotically dominant over those with $\\widetilde{\\gamma}_{\\alpha}^{i}$ (with $\\alpha \\neq i$ ) because $\\widetilde{\\gamma}_{i}^{i}=T^{-1}$ and $\\widetilde{\\gamma}_{\\alpha}^{i}=o\\left(T^{-1}\\right)$. Additionally, the $i$-th signal propagates when $\\widetilde{\\gamma}_{i}^{i}>0$, which holds iff $\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i} \\in[0,1]$ by definition. Therefore, we are motivated to check the condition $\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i} \\in[0,1]$ to see whether $\\mathbf{x}_{i}$ contributes to the gradient (3). The signal propagation probability $\\rho_{i}$ characterizes its strength. Summary. In this section, we introduced the signal propagation probability $\\rho_{i}$, which characterizes how likely a given token $\\mathbf{x}_{i}$ contributes to the learning dynamics. Specifically, $\\widetilde{\\gamma}^{i} \\neq \\mathbf{0}$ holds more likely with larger $\\rho_{i}$, where $\\mathbf{x}_{i}$ contributes to the QK -gradient (3). Subsequently, we will analyze the quantity\n$\\rho_{i}$ to see the behavior of the probability vector $\\boldsymbol{\\rho} \\in[0,1]^{T}$. When does the mass of $\\boldsymbol{\\rho}$ concentrate to only a few tokens or scatter across most of the tokens? ## 4 When does attention localize? We derive the signal propagation probability $\\rho_{i}$ based on the following synthetic data model for the sake of clarity. Assumption 1 (Random walk). The tokens $\\left(\\mathrm{x}_{t}\\right)_{t \\geq 1}$ are generated by the following Gaussian random walk:\n\n$$\n\\mathbf{x}_{1} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}), \\quad \\mathbf{x}_{t+1} \\sim \\mathcal{N}\\left(\\mathbf{x}_{t}, \\mathbf{\\Sigma}\\right)\n$$\n\nTo derive $\\rho_{i}$, we resort to the Gaussian approximation of $\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i}$. Define\n\n$$\n\\mu^{i}:=\\mathbb{E}\\left[\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i}\\right] \\text { and } v^{i}:=\\mathbb{V}\\left[\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i}\\right]\n$$\n\nWe approximately suppose that $\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i} \\sim \\mathcal{N}\\left(\\mu^{i}, v^{i}\\right)$. Then, the signal propagation probability is approximated:\n\n$$\n\\rho_{i} \\approx \\frac{1}{2}\\left\\{\\operatorname{erf}\\left(\\frac{1-\\mu^{i}}{\\sqrt{2 v^{i}}}\\right)+\\operatorname{erf}\\left(\\frac{\\mu^{i}}{\\sqrt{2 v^{i}}}\\right)\\right\\}\n$$\n\nTo leverage this formula, we derive $\\mu^{i}$ and $v^{i}$. Lemma 1. Suppose that $\\mathbf{W}_{\\mathrm{QK}}$ is symmetric and independent from $\\mathbf{X}$, and let $\\mathbf{W}:=\\mathbf{W}_{\\mathrm{QK}} \\boldsymbol{\\Sigma}$. Under Assumption 1, for $i \\in[T]$, the mean $\\mu^{i}$ and variance $v^{i}$ of $\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i}$ with the input $\\boldsymbol{\\omega}:=\\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} / \\lambda$ are given as follows:\n\n$$\n\\begin{aligned}\n\\mu^{i} & =\\left(\\frac{i}{T}-\\frac{1}{2}\\right) \\frac{\\operatorname{tr}(\\mathbf{W})}{\\lambda}+o(1) \\\\\nv^{i} & =\\left(\\frac{2 i^{2}}{T^{2}}+\\frac{7}{12}\\right) \\frac{\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)}{\\lambda^{2}}+o(1)\n\\end{aligned}\n$$\n\nThe proof is given in Appendix C. The symmetry of $\\mathbf{W}_{\\mathrm{QK}}$ is assumed for convenience. In the case of asymmetric $\\mathbf{W}_{\\mathrm{QK}}$, we can redefine the signal propagation probability with the symmetrized matrix $\\left(\\mathbf{W}_{\\mathrm{QK}}+\\mathbf{W}_{\\mathrm{QK}}^{\\top}\\right) / 2$\n\nRecall that $\\operatorname{tr}(\\mathbf{W})=\\sum_{i \\in[d]} w_{i}$ if we write the eigenvalues of $\\mathbf{W}$ by $\\left(w_{1}, w_{2}, \\ldots, w_{d}\\right)$, and that $\\operatorname{tr}(\\mathbf{W})^{2} \\leq d \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ holds due to Jensen's inequality. This implies that\n\n$$\n-\\sqrt{d \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)} \\leq \\operatorname{tr}(\\mathbf{W}) \\leq \\sqrt{d \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)}\n$$\n\nMoreover, $\\mu^{i}$ and $v^{i}$ are determined by the relative token location $i / T$. By continuously extending $i / T$ to $\\theta \\in[0,1]$, the signal propagation probability $\\rho_{i}$ can be extended to $\\rho:[0,1] \\rightarrow[0,1]$, defined over relative token locations:\n\n$$\n\\rho(\\theta):=\\Phi\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi ; \\theta\\right)-\\Phi\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi-\\frac{1}{\\eta} ; \\theta\\right)\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& \\xi:=\\frac{\\operatorname{tr}(\\mathbf{W})}{\\sqrt{\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)}}, \\quad \\eta:=\\frac{\\sqrt{\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)}}{\\lambda} \\\\\n& \\Phi(z ; \\theta):=\\frac{1}{2} \\operatorname{erf}\\left(\\frac{z}{\\sqrt{2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)}}\\right)\n\\end{aligned}\n$$\n\nand the parameter ranges are $\\xi \\in[-\\sqrt{d}, \\sqrt{d}]$ (due to the bound (5)) and $\\eta \\in(0, \\infty)$. Here, $\\xi$ and $\\eta$ can be regarded independent (when $\\mathbf{W}$ is independent from $\\mathbf{X}$ ) because the eigenspectrum scale $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ can be modulated within the bound (5) once the eigenspectrum of $\\mathbf{W}$ is given. ![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-07.jpg?height=318&width=750&top_left_y=275&top_left_x=652)\n\nFigure 3: The theoretical plots of $\\rho(\\theta)$. For each $\\xi=128,512$, the product value $\\xi \\eta=1.28,5.12$, respectively. The latter is sufficiently larger than the localization threshold $r=2$ and localized. ![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-07.jpg?height=318&width=652&top_left_y=686&top_left_x=702)\n\nFigure 4: Entropy lower bound (9) by Zhai et al. [31]. Figure 2 numerically illustrates $\\rho(\\theta)$ with different $\\xi$ and $\\eta$. From these figures, we obtain a couple of observations. - Localization. $\\rho(\\theta)$ concentrates on fewer tokens as $\\eta$ increases (see $|\\xi| \\geq 5$ ). By contrast, $\\rho(\\theta)$ behaves relatively uniformly regardless of $\\eta$ for small $|\\xi| \\leq 1$. - Late-/middle-/early-site focus. Focus on small $\\eta$ such as $\\eta=0.001$. As $\\xi$ increases to a large positive, $\\rho(\\theta)$ puts positive weights for only late-site tokens, i.e., $\\theta>0.5$. By contrast, as $\\xi$ decreases to a negative, $\\rho(\\theta)$ focuses on early-site tokens, i.e., $\\theta<0.5$. When $\\eta$ increases (see $\\eta \\geq 0.5$ ), $\\rho(\\theta)$ localizes around $\\theta=0.5$ with sufficiently large $\\xi$ (say, $|\\xi| \\geq 5$ ), which indicates middle-site focus. - Vanishing signal. As $\\eta$ increases, $\\rho(\\theta)$ degenerates to zero for any $\\theta \\in[0,1]$ regardless of $\\xi$. How $\\rho$ behaves at the limit. Subsequently, we claim the above observations formally, which is proved in Appendix C. Lemma 2. $\\rho(\\theta)$ satisfies the following properties. 1. (Late-/middle-site) As $(\\xi, \\eta) \\rightarrow(\\infty, 0)$ with $\\xi \\eta \\rightarrow r$,\n\n$$\n\\rho(\\theta) \\rightarrow\\left\\{\\begin{array}{ll}\n\\mathbb{1}_{\\left\\{\\theta \\geq \\frac{1}{2}\\right\\}} & \\text { if } 0 \\leq r \\leq 2 \\\\\n\\mathbb{1}_{\\left\\{\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}+\\frac{1}{r}\\right\\}} & \\text { if } r>2\n\\end{array} .\\right. $$\n\n2. (Early-/middle-site) As $(\\xi, \\eta) \\rightarrow(-\\infty, 0)$ with $\\xi \\eta \\rightarrow r$,\n\n$$\n\\rho(\\theta) \\rightarrow\\left\\{\\begin{array}{ll}\n\\mathbb{1}_{\\left\\{\\theta \\leq \\frac{1}{2}\\right\\}} & \\text { if }-2 \\leq r<0 \\\\\n\\mathbb{1}_{\\left\\{\\frac{1}{2}+\\frac{1}{r} \\leq \\theta \\leq \\frac{1}{2}\\right\\}} & \\text { if } r<-2\n\\end{array} .\\right. $$\n\n3. (Uniformity) Fix $\\eta$ as a finite value. As $|\\xi| \\rightarrow 0,\\left|\\rho^{\\prime}(\\theta)\\right| \\rightarrow 0$ for any $\\theta \\in[0,1]$. 4. (Vanishing signal) Fix $\\xi$ as a finite value. As $\\eta \\rightarrow \\infty, \\rho(\\theta) \\rightarrow 0$ for any $\\theta \\in[0,1]$. From late-/middle-/early-site focus in Lemma 2, we see interestingly that $\\rho(\\theta)$ localizes when $\\xi \\eta=$ $\\operatorname{tr}(\\mathbf{W}) / \\lambda$ asymptotically deviates from zero significantly so that $|r| \\gg 2$. At this limit, $\\rho(\\theta)$ concentrates on $\\theta=0.5$, inducing the middle-site focus. Conversely, attention becomes relatively uniform when $\\xi \\eta=\\operatorname{tr}(\\mathbf{W}) / \\lambda$ is kept close to zero. In Fig. 3, we numerically illustrate this regime: $\\rho$ localizes at the middle site when $\\eta=0.02$ (i.e., $\\xi \\eta=5.12$ ). Let us investigate the limiting condition of $(\\xi, \\eta)$ for localization: When is $(\\xi, \\eta)$ close to the limit\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-08.jpg?height=681&width=1596&top_left_y=285&top_left_x=237)\n\nFigure 5: Simulated signal propagation probability. In the top and bottom rows, the results for the isotropic and anisotropic covariances (the details in the text) are shown, respectively. (Left) Signal propagation probability $\\rho_{i}$ computed over repeatedly sampled 300 random walks (Assumption 1) with 40 tokens. For each line, $\\mathbf{W}_{\\mathrm{QK}}(d=128)$ is sampled 10 times with the corresponding mean and scale of the eigenvalue distribution, and the averaged $\\rho_{i}$ is denoted by the bold line. (Right) The attention entropy $[31]$ is computed for $\\mathbf{W}_{\\mathrm{QK}}$ with different eigenvalue mean-scale pairs. $(\\infty, 0)$ while $\\xi \\eta \\rightarrow r \\gg 2$ ? Here, we focus on the eigenspectrum of $\\mathbf{W}$ by regarding its eigenvalues $\\left(w_{i}\\right)_{i \\in[d]}$ as being sampled from a distribution with the mean $\\operatorname{tr}(\\mathbf{W})=\\sum_{i \\in[d]} w_{i}$ and scale $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)=\\sum_{i \\in[d]} w_{i}^{2}$. First, $\\eta \\rightarrow 0$ indicates that the scale $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ should be close to zero. Next, $\\xi \\eta(=\\operatorname{tr}(\\mathbf{W}) / \\lambda) \\rightarrow r \\gg 2$ means that $\\operatorname{tr}(\\mathbf{W}) \\gg 2 \\lambda$ at the limit, i.e., $\\operatorname{tr}(\\mathbf{W})$ should be significantly away from zero. By combining them, we tell that $\\rho$ localizes when the eigenspectrum concentrates around a non-zero mean. This happens more likely when the embedding dimension $d$ is excessively large to make the eigenvalue sum $\\operatorname{tr}(\\mathbf{W})$ bounded away from zero while keeping the scale $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ close to zero (i.e., keeping every eigenvalue close to zero). Thus, a larger embedding dimension $d$ is beneficial to drive attention to localization. Next, from the claim of uniformity in Lemma 2, we tell that $\\rho$ fluctuates less and less with $\\xi$ closer to zero. Hence, $\\rho(\\theta)$ takes a similar value across different token positions $\\theta$ in this limit. When $\\operatorname{tr}(\\mathbf{W}) \\rightarrow 0$, $\\rho$ attains this limit.",
    "locater-7": "Summary. Wrapping up this section, we obtain answers to (Q1) posed in Section 1 under the random walk model. ## A1: When does attention localize? - $\\rho$ localizes when $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ is close to zero while $|\\operatorname{tr}(\\mathbf{W})|$ is significantly bounded away from zero, i.e., $\\mathbf{W}$-eigenspectrum concentrates to a non-zero mean. - $\\rho$ is uniform when $\\operatorname{tr}(\\mathbf{W})$ is close to zero while $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ remains finite, i.e., $\\mathbf{W}$-eigenspectrum has the zero mean and a finite variance. - $\\rho$ degenerates to zero uniformly when $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ is sufficiently large, i.e., $\\mathbf{W}$-eigenspectrum has an infinitely large variance. ## 5 Are different collapse regimes reconcilable? We discuss the results of our analysis in Section 4 and the previous arguments related to attention uniformity. Connection to rank collapse. Dong et al. [6] showed that self-attention blocks $\\mathbf{U}^{\\ell}$ (see Eq. (2)) converges to a rank-1 matrix $\\mathbf{z 1}^{\\top}$ (for some $\\mathbf{z}$ ) with $L \\rightarrow \\infty$ without skip connections or feed-forward blocks, which is called rank collapse. ${ }^{1}$ They argued the importance of avoiding rank collapse for better expressivity because each token embedding in a rank- 1 self-attention block degenerates to the same. Hence, the rank collapse is related to the failure mode attributed to the uniformity after mixing key tokens by attention, which is slightly different from what we are concerned about-how each token contributes during mixing by attention (through the gradient, as discussed in Section 3). Dong et al. [6, Theorem 2.2] proved that the convergence rate to a rank-1 matrix slows down when the matrix $\\ell_{1}$-norm $\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{1}$ is large. Because we can draw the following connection between $\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{1}$ and $|\\operatorname{tr}(\\mathbf{W})|$ :\n\n$$\n\\frac{|\\operatorname{tr}(\\mathbf{W})|}{\\sqrt{d}\\|\\boldsymbol{\\Sigma}\\|_{2}} \\leq\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{2} \\leq\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{\\mathrm{F}} \\leq\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{1}\n$$\n\nwhere the first inequality is due to the bound (5) and the Cauchy-Schwarz inequality, it is advisable to increase $|\\operatorname{tr}(\\mathbf{W})|$ under fixed $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ to mitigate the rank collapse. This is equivalent to reducing the eigenspectrum variance\n\n$$\nd^{2} \\mathbb{V}\\left[w_{i}\\right]=d^{2}\\left(\\mathbb{E}\\left[w_{i}^{2}\\right]-\\mathbb{E}\\left[w_{i}\\right]^{2}\\right)=d \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)-|\\operatorname{tr}(\\mathbf{W})|^{2}\n$$\n\nHence, minimizing the $\\mathbf{W}$-eigenspectrum variance leads to better expressivity.",
    "locater-8": "Connection to entropy collapse. Zhai et al. [31] introduced a concept called entropy collapse, in which the average Shannon entropy of the columns of the attention matrix $\\mathbf{A}^{\\ell}$ (see Eq. (1)) shrinks. Intuitively speaking, low attention entropy induces localized attention. This notion of localization is akin to ours because the attention entropy measures the uniformity the attention is applied to input tokens during mixing. They empirically observed that the training loss falls into a plateau with low attention entropy, which causes training instability of transformers, and hence advocate for keeping attention less peaked during training. In Zhai et al. [31, Theorem 3.1], the attention entropy is asymptotically lower-bounded for large $T$ by\n\n$$\n\\ln (1+T \\exp (-\\nu))+\\frac{\\nu \\exp (-\\nu / 2)}{T^{-1}+\\exp (-\\nu)}\n$$\n\nwhere $\\nu:=\\left\\|\\mathbf{X X}^{\\top}\\right\\|_{2}\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{2}$. This lower bound is unimodal in $\\nu$ and vanishes at $\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{2} \\rightarrow \\infty$ (see Fig. 4), so the attention entropy tends to be higher when $\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{2}$ is small. If $|\\operatorname{tr}(\\mathbf{W})|$ is not too small, $\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{2}$ is lower-bounded (see Eq. (7)) and the attention entropy may be kept close to the peak of the lower bound (9). To mitigate the entropy collapse, it is natural to decrease $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ under fixed $\\operatorname{tr}(\\mathbf{W})$ (which is equivalent to minimizing the eigenspectrum variance by Eq. (8)) because of the bound\n\n$$\n\\left\\|\\boldsymbol{\\Sigma}^{-1}\\right\\|_{\\mathrm{F}} \\sqrt{\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)} \\geq\\left|\\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right)\\right| \\geq\\left\\|\\mathbf{W}_{\\mathrm{QK}}\\right\\|_{2}\n$$\n\nwhere the first inequality is due to the Cauchy-Schwarz inequality. Hence, minimizing the $\\mathbf{W}$-eigenspectrum variance helps the model to avoid the entropy collapse. [^0]![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-10.jpg?height=326&width=1610&top_left_y=277&top_left_x=223)\n\nFigure 6: Experimental results of language modeling (WikiText-2) with $d=128$ and 1-layer transformers, fixed $\\kappa_{1}=100$, and varying regularization intensity $\\kappa_{2}$. With stronger $\\kappa_{2}$, the eigenspectrum scale shrinks (B), the attention entropy increases (C), and the perplexity improves (D). The rightmost figure magnifies the x-range, where the perplexity attains the minimum. Rank collapse vs. entropy collapse. At first sight, the two notions of collapse seem to contradict each other because avoiding the rank collapse leads to diverse token embeddings, whereas avoiding the entropy collapse leads to a uniform token mixer. Indeed, the matrix $\\ell_{1}$-norm (that decreases under the rank collapse) and the spectral norm (that increases under the entropy collapse) are equivalent norms, and the two modes appear to be incompatible. However, as we discussed above, this trade-off is reconcilable from the viewpoint of the $\\mathbf{W}$-eigenspectrum. Setting the eigenspectrum mean to be bounded away from zero, we can avoid the rank collapse owing to the bound (7). Under a fixed eigenspectrum mean, minimizing the eigenspectrum scale (equivalently, minimizing its variance (8)) leads to high attention entropy due to the bound (10) and the unimodal shape of the entropy lower bound (9). This variance minimization is nothing else but the condition of attention localization. Eventually, $\\rho(\\theta)$ localizes and attends to specific sites of tokens, as we showed in Lemma 2. Hence, the signal propagation probability offers us a better view of localization. Let us summarize our second take-home. ## A2: How does attention localization impact? - Better expressivity: If $|\\operatorname{tr}(\\mathbf{W})|$ is maximized for a fixed $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$, the convergence to the rank collapse becomes slow. - High attention entropy: If $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$ is minimized for a fixed $|\\operatorname{tr}(\\mathbf{W})|$ bounded away from zero, the attention entropy is increased. Both of the above are attributed to minimizing the $\\mathbf{W}$-eigenspectrum variance (8). Numerical simulation. To see the relationship between the eigenspectrum, $\\rho$, and attention entropy, we simulated the signal propagation probability $\\rho_{i}$ using synthesized random walks following Assumption 1 with the isotropic $\\boldsymbol{\\Sigma}=\\mathbf{I}$ and anisotropic $\\boldsymbol{\\Sigma}$. To obtain an anisotropic $\\boldsymbol{\\Sigma}$, we first sampled $\\mathbf{R} \\in \\mathbb{R}^{d \\times d}$ from element-wise Unif $(-2.5,2.5)$ and computed $\\boldsymbol{\\Sigma}=\\mathbf{R}^{\\top} \\mathbf{R} / d$. We sampled 300 sequences with 40 tokens, and obtained $\\mathbf{W}_{\\mathrm{QK}}$ by generating 128 eigenvalues $\\left(w_{i}\\right)_{i \\in[d]}$ from $\\mathcal{N}\\left(\\right.$ mean, scale $\\left.{ }^{2}\\right)$ and composed with a sampled orthogonal basis matrix $\\mathbf{B}$, by the eigendecomposition formula $\\mathbf{W}_{\\mathrm{QK}}=\\mathbf{B} \\operatorname{diag}\\left(\\left(w_{i}\\right)_{i}\\right) \\mathbf{B}^{\\top}$. The signal propagation probability was averaged over 300 sequences. Figure 5 shows the averaged $\\rho_{i}$ with different mean-scale pairs and the corresponding attention entropy in the right-most figure. As seen, $\\rho_{i}$ localizes with smaller scales and larger means, which is consistent with the conclusion in Section 4. This trend supports the validity of $\\mathbf{W}_{\\mathrm{QK}}$-eigenspectrum as a proxy to $\\mathbf{W}$-eigenspectrum. Moreover, we observe that $\\mathbf{W}_{\\mathrm{QK}}$-eigenspectrum with a fixed mean and scale leads to higher attention entropy. ![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-11.jpg?height=544&width=812&top_left_y=282&top_left_x=628)\n\nFigure 7: The signal propagation probabilities are shown at each iteration over 50000 iterations. (Top) LocAteR with $\\kappa_{1}=100$ and $\\kappa_{2}=1$. A couple of light and dark horizontal stripes correspond to the attention localization. (Bottom) No LocAteR. Overall, the signal propagation probability is uniform at each time. ## 6 Intervening attention localization\n\nTo empirically see the impact of localization on the model performance, we propose a method to control the degree of attention localization. We focus on the eigenspectrum of $\\mathbf{W}_{\\mathrm{QK}}$ instead of $\\mathbf{W}=\\mathbf{W}_{\\mathrm{QK}} \\boldsymbol{\\Sigma}$ because $\\boldsymbol{\\Sigma}$ does not change during training, and the numerical simulation showed that $\\mathbf{W}_{\\mathrm{QK}}$ behaves as a reasonable proxy to $\\mathbf{W}$ (see Section 5). We minimize the loss function $J$ while minimizing the eigenspectrum scale and maintaining the mean to a fixed level:\n\n$$\n\\min _{\\boldsymbol{\\Theta}}\\left\\{J(\\boldsymbol{\\Theta})+\\kappa_{1} \\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}^{\\top} \\mathbf{W}_{\\mathrm{QK}}\\right)+\\kappa_{2}\\left(\\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right)-1\\right)^{2}\\right\\}\n$$\n\nwhere $\\kappa_{1}, \\kappa_{2}>0$ are the regularization strengths. Here, we allow $\\mathbf{W}_{\\mathrm{QK}}$ to be asymmetric, and the eigenspectrum scale is represented by $\\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}^{\\top} \\mathbf{W}_{\\mathrm{QK}}\\right)$. The regularization terms can be optimized fairly easily thanks to the following derivative formulae (for $\\mathbf{W}_{\\mathrm{QK}}=\\mathbf{W}_{\\mathrm{Q}}^{\\top} \\mathbf{W}_{\\mathrm{K}}$ ):\n\n$$\n\\begin{aligned}\n& \\nabla_{\\mathbf{W}_{\\mathrm{Q}}} \\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}^{\\top} \\mathbf{W}_{\\mathrm{QK}}\\right)=2 \\mathbf{W}_{\\mathrm{K}} \\mathbf{W}_{\\mathrm{K}}^{\\top} \\mathbf{W}_{\\mathrm{Q}} \\\\\n& \\nabla_{\\mathbf{W}_{\\mathrm{K}}} \\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}^{\\top} \\mathbf{W}_{\\mathrm{QK}}\\right)=2 \\mathbf{W}_{\\mathrm{Q}} \\mathbf{W}_{\\mathrm{Q}}^{\\top} \\mathbf{W}_{\\mathrm{K}} \\\\\n& \\nabla_{\\mathbf{W}_{\\mathrm{Q}}}\\left[\\left(\\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right)-1\\right)^{2}\\right]=\\left[\\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right)-1\\right] \\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right) \\mathbf{W}_{\\mathrm{K}} \\\\\n& \\nabla_{\\mathbf{W}_{\\mathrm{K}}}\\left[\\left(\\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right)-1\\right)^{2}\\right]=\\left[\\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right)-1\\right] \\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right) \\mathbf{W}_{\\mathrm{Q}}\n\\end{aligned}\n$$\n\nSince this whole objective drives the eigenspectrum scale to a small value, the signal vanishing can be avoided automatically. We call this regularization scheme LocAteR (LOCalized ATtEntion Regularization). ## 7 Experiments\n\nWe aim to observe the correlation between the eigenspectrum and localization. To this end, we train transformers with LocAteR and varying $\\kappa_{1}, \\kappa_{2}$, and see how the model performances and attention foci change over time. Setup. We used fairseq v0.12.2 [20], which is a toolkit oriented for sequence modeling, to implement and train transformers. The basic training scheme was inherited from fairseq-cli/train.py. The model is a 1-layer transformer with a single-head self-attention and Post-LN (default), and the input embedding dimension, attention embedding dimension, and feed-forward net embedding dimension are\nset to 128 altogether (namely, $d=128$ ). ${ }^{2}$ Input data were transformed into 64 tokens (namely, $T=64$ ) with batch size 64 . The optimizer is Adam [11] with default parameters and no clip norm, and the weight decay with 0.01 is used. The learning rate is fixed to $2.5 \\times 10^{-5}$ without any scheduling. The FP16 quantizer was applied to reduce memory usage. All the other configs remain to be the same as the default in fairseq-cli/train.py. Under this config, we updated the model with 50000 iters. Language modeling. We conducted the language modeling task. The dataset we used is WikiText2 [16], which is a collection of high-quality Wikipedia articles. We conduced the experiments with fixed scale regularization strength $\\kappa_{1}=100$ and varying mean regularization strength $\\kappa_{2}$ from $0,10^{-3}, 10^{-2}, \\ldots, 10^{0}$. The results are shown in Fig. 6, in which stronger regularizers tend to make the eigenspectrum scale smaller. This, in turn, maintains the attention entropy higher during the updates entirely, and eventually, the model achieves better perplexity. While the better model performance with higher attention entropy has already been observed by Zhai et al. [31], we also showed that a smaller eigenspectrum scale contributes to higher attention entropy. This empirically corroborates that attention localization leads to better model performance, probably because the attention mechanism appropriately selects relevant tokens during training. Figure 7 shows the signal propagation probability at each training iteration. We compute the signal propagation probability of token $i$ by counting the frequency of $\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i} \\in[0,1]$ in a given batch. LocAteR entails salient horizontal stripes, each corresponding to attended tokens. Yet, the stripes do not appear in \"bulk\" as we analyzed in Section 4 because our synthetic data model in Assumption 1 does not perfectly align with real datasets. Nevertheless, our experiments evidently contrast the localized and uniformed attention depending on the eigenspectrum scale because no salient stripes are observed without LOCAteR. In Figs. 6 and 7, we observe different learning phases for the first $10^{4}$ and the rest iters. Indeed, Tian et al. [26] observed similar phenomena and explained that it is due to the different convergence speeds between attention weights corresponding to informative and non-informative tokens. The relationship between the $\\mathbf{W}_{\\mathrm{QK}}$-eigenspectrum and this dynamics is beyond our scope and left for future work. ## 8 Conclusion and limitation\n\nWe revealed that attention localizes when the eigenspectrum of $\\mathbf{W}$ concentrates to a non-zero mean, or equivalently, with larger eigenspectrum mean $\\operatorname{tr}(\\mathbf{W})$ and smaller scale $\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)$. Based on it, LocAteR was proposed to shrink the scale $\\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}^{2}\\right)$ while maintaining the mean $\\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right)$. Interestingly, maximizing the scale is related to mitigating both rank collapse and entropy collapse, and hence, the two apparently contradictory failure modes can be reconciled. The experiments on a real-world dataset corroborate it, though the random walk model is not perfectly satisfied. We recognize three limitations of this work. First, we rely on the strong random walk model. Although the Gaussianity may be reasonable because of usual initialization schemes of transformer embedding layers, it is interesting to consider an alternative model to capture token correlations better. Second, the formal analysis is mainly restricted to 1-layer transformers. Recent studies often consider gradient explosion in the large-depth limit from the viewpoint of layer normalization [30, 22] and initialization [2, 21]. It must be fruitful to integrate these perspectives to our gradient analysis through Eq. (3). Lastly, why attention localization leads to better model performance still remains elusive. Whereas localization is related to avoiding rank collapse (and hence higher model expressivity), we need additional effort to fully understand the mechanism. [^1]\n## Acknowledgments\n\nA part of the experiments of this research was conducted using Wisteria/Aquarius in the Information Technology Center, the University of Tokyo. ## References\n\n[1] Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] Bachlechner, T., Majumder, B. P., Mao, H., Cottrell, G., and McAuley, J. ReZero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pp. 1352-1361. PMLR, 2021. [3] Bietti, A., Cabannes, V., Bouchacourt, D., Jegou, H., and Bottou, L. Birth of a transformer: A memory viewpoint. Advances in Neural Information Processing Systems, 36, 2023. [4] Brookes, M. The matrix reference manual. http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/ intro.html, 1998.",
    "locater-9": "[Online; accessed 01-September-2023]. [5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877-1901, 2020. [6] Dong, Y., Cordonnier, J.-B., and Loukas, A. Attention is not all you need: Pure attention loses rank doubly exponentially with depth.",
    "locater-10": "In Proceedings of the 38th International Conference on Machine Learning, pp. 2793-2803. PMLR, 2021. [7] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the 9th International Conference on Learning Representations, 2021. [8] Graves, A. Generating sequences with recurrent neural networks.",
    "locater-11": "arXiv preprint arXiv:1308.0850, 2013. [9] Jelassi, S., Sander, M., and Li, Y. Vision transformers provably learn spatial structure. Advances in Neural Information Processing Systems, 35:37822-37836, 2022. [10] Kharitonov, E. and Chaabouni, R. What they do when in doubt: a study of inductive biases in seq2seq learners. In Proceedings of the 9th International Conference on Learning Representations, 2021 . [11] Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization.",
    "locater-12": "In Proceedings of the 3rd International Conference on Learning Representations, 2015. [12] Kobayashi, G., Kuribayashi, T., Yokoi, S., and Inui, K. Analyzing feed-forward blocks in transformers through the lens of attention map.",
    "locater-13": "arXiv preprint arXiv:2302.00456, 2023. [13] Likhomanenko, T., Xu, Q., Kahn, J., Synnaeve, G., and Collobert, R. slimIPL: Language-model-free iterative pseudo-labeling. In Proc. Interspeech 2021, pp. 741-745, 2021. [14] Martins, A. and Astudillo, R. From softmax to sparsemax: A sparse model of attention and multilabel classification. In Proceedings of the 34th International Conference on Machine Learning, pp. 1614-1623. PMLR, 2016. [15] Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in GPT. Advances in Neural Information Processing Systems, 35:17359-17372, 2022. [16] Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models.",
    "locater-14": "arXiv preprint arXiv:1609.07843, 2016. [17] Murty, S., Sharma, P., Andreas, J., and Manning, C. Grokking of hierarchical structure in vanilla transformers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 439-448. Association for Computational Linguistics, 2023. [18] Murty, S., Sharma, P., Andreas, J., and Manning, C. D. Characterizing intrinsic compositionality in transformers with tree projections. In Proceedings of the 11th International Conference on Learning Representations, 2023.",
    "locater-15": "[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.",
    "locater-16": "40770-40803. PMLR, 2023. ## A Helper lemmas\n\nLemma 3. Let $\\mathbf{W} \\in \\mathbb{R}^{d \\times d}$ be a symmetric matrix. Fix $\\mathbf{a}, \\boldsymbol{\\mu} \\in \\mathbb{R}^{d}$ and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ be a covariance matrix. For $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{m}, \\boldsymbol{\\Sigma})$, the following moment formulae hold:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbf{x}^{\\top} \\mathbf{W} \\mathbf{x}\\right] & =\\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma})+\\mathbf{m}^{\\top} \\mathbf{W} \\mathbf{m} \\\\\n\\mathbb{E}\\left[\\mathbf{x} \\mathbf{x}^{\\top}\\right] & =\\boldsymbol{\\Sigma}+\\mathbf{m} \\mathbf{m}^{\\top} \\\\\n\\mathbb{E}\\left[\\mathbf{a}^{\\top} \\mathbf{W} \\mathbf{x} \\mathbf{x}^{\\top} \\mathbf{W} \\mathbf{x}\\right] & =2 \\mathbf{a}^{\\top} \\mathbf{W} \\boldsymbol{\\Sigma} \\mathbf{W} \\mathbf{m}+\\mathbf{a}^{\\top} \\mathbf{W} \\mathbf{m}\\left\\{\\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma})+\\mathbf{m}^{\\top} \\mathbf{W} \\mathbf{m}\\right\\} \\\\\n\\mathbb{E}\\left[\\mathbf{x}^{\\top} \\mathbf{W} \\mathbf{x} \\mathbf{x}^{\\top} \\mathbf{W} \\mathbf{x}\\right] & =2 \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma} \\mathbf{W} \\boldsymbol{\\Sigma})+\\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma})^{2}+4 \\mathbf{m}^{\\top} \\mathbf{W} \\boldsymbol{\\Sigma} \\mathbf{W} \\mathbf{m}+2 \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma}) \\mathbf{m}^{\\top} \\mathbf{W} \\mathbf{m}+\\mathbf{m}^{\\top} \\mathbf{W m m}^{\\top} \\mathbf{W} \\mathbf{m}\n\\end{aligned}\n$$\n\nThe formulae in Lemma 3 are standard and cropped from Brookes [4].",
    "locater-17": "Lemma 4. Let $\\mathbf{W} \\in \\mathbb{R}^{d \\times d}$ be a symmetric matrix. For $i \\leq j$, suppose that $\\mathbf{x}_{i}, \\mathbf{x}_{j}$ follow Assumption 1 . Then, the following formulae hold:\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W} \\mathbf{x}_{i}\\right] & =(i-1) \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma}) \\\\\n\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W} \\mathbf{x}_{i} \\mathbf{x}_{i}^{\\top} \\mathbf{W} \\mathbf{x}_{i}\\right] & =\\left(i^{2}-2 i+2\\right)\\left\\{2 \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma} \\mathbf{W} \\boldsymbol{\\Sigma})+\\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma})^{2}\\right\\} \\\\\n\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W} \\mathbf{x}_{j} \\mathbf{x}_{j}^{\\top} \\mathbf{W} \\mathbf{x}_{i}\\right] & =\\left(i^{2}+i j-3 i-j+4\\right) \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma} \\mathbf{W} \\boldsymbol{\\Sigma})+\\left(i^{2}-2 i+2\\right) \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma})^{2} \\\\\n\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W} \\mathbf{x}_{j} \\mathbf{x}_{j}^{\\top} \\mathbf{W} \\mathbf{x}_{j}\\right] & =(i j-i-j+2)\\left\\{2 \\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma} \\mathbf{W} \\boldsymbol{\\Sigma})+\\operatorname{tr}(\\mathbf{W} \\boldsymbol{\\Sigma})^{2}\\right\\}\n\\end{aligned}\n$$\n\nThe formulae in Lemma 4 can be shown by recursively applying Lemma 3 .",
    "locater-18": "We omit the proofs since they are elementary. ## B Omitted derivations\n\n## B. 1 QK-gradient\n\nHere, we complement the derivations of the QK-gradient terms shown in Section 3. To get Eq. (3), we compute $\\nabla_{\\mathrm{w}_{\\mathrm{QK}}} J$ :\n\n$$\n\\begin{aligned}\n\\nabla_{\\mathbf{W}_{\\mathrm{QK}}} J & =\\frac{1}{2} \\mathbb{E}\\left[\\nabla_{\\mathbf{W}_{\\mathrm{QK}}}\\left\\|\\mathbf{y}-\\mathbf{F}(\\mathbf{X})_{T}\\right\\|^{2}\\right] \\\\\n& =\\frac{1}{2} \\mathbb{E}\\left[\\nabla_{\\mathbf{W}_{\\mathrm{QK}}}\\left\\{\\mathbf{F}(\\mathbf{X})_{T}^{\\top} \\mathbf{F}(\\mathbf{X})_{T}-2 \\mathbf{y}^{\\top} \\mathbf{F}(\\mathbf{X})_{T}\\right\\}\\right] \\\\\n& =\\frac{1}{2} \\mathbb{E}\\left[\\boldsymbol{\\omega}^{\\top} \\boldsymbol{\\Gamma} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top} \\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\boldsymbol{\\Gamma}^{\\top} \\boldsymbol{\\omega}+2\\left(\\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\widetilde{\\boldsymbol{\\gamma}}_{0}+\\mathbf{x}_{T}\\right)^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top} \\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\boldsymbol{\\Gamma}^{\\top} \\boldsymbol{\\omega}-2 \\mathbf{y}^{\\top} \\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\boldsymbol{\\Gamma}^{\\top} \\boldsymbol{\\omega}\\right] \\\\\n& =\\frac{1}{2 \\lambda^{2}} \\mathbb{E}\\left[\\mathbf{x}_{T}^{\\top} \\mathbf{W}_{\\mathrm{QK}}^{\\top} \\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{P} \\boldsymbol{\\Gamma}^{\\top} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]+\\frac{1}{\\lambda} \\mathbb{E}\\left[\\left(\\widetilde{\\boldsymbol{\\gamma}}_{0}\\right)^{\\top} \\mathbf{P} \\boldsymbol{\\Gamma}^{\\top} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]+\\frac{1}{\\lambda} \\mathbb{E}\\left[\\mathbf{q}^{\\top} \\boldsymbol{\\Gamma}^{\\top} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right] \\\\\n& =\\frac{1}{\\lambda^{2}} \\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{P} \\boldsymbol{\\Gamma}^{\\top} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top}\\right]+\\frac{1}{\\lambda} \\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\widetilde{\\boldsymbol{\\gamma}}_{0} \\mathbf{x}_{T}^{\\top}\\right]+\\frac{1}{\\lambda} \\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{x}_{T}^{\\top}\\right] . \\end{aligned}\n$$\n\nBy expanding the first term of $\\nabla_{\\mathrm{W}_{\\mathrm{QK}}} J$, we get the following:\n\n$$\n\\frac{1}{\\lambda^{2}} \\sum_{i, j, \\alpha, \\beta \\in[T]} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{\\beta}^{j}\\left(\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}} \\mathbf{x}_{j}\\right)\\left(\\mathbf{x}_{\\beta}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right) \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right]\n$$\n\nwhere $\\check{\\mathbf{P}}:=\\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top} \\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}}$. Similarly, by expanding the second and third terms of $\\nabla_{\\mathbf{W}_{\\mathrm{QK}}} J$, we get the following terms, respectively:\n\n$$\n\\begin{aligned}\n& \\frac{1}{\\lambda} \\sum_{i, \\alpha, \\beta \\in[T]} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{0}^{\\beta}\\left(\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}} \\mathbf{x}_{\\beta}\\right) \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& \\frac{1}{\\lambda} \\sum_{i, \\alpha \\in[T]} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i}\\left\\{\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{x}_{T}-\\mathbf{y}\\right)\\right\\} \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right]\n\\end{aligned}\n$$\n\nTo get Eq. (19), we expand the first term of $\\nabla_{\\mathbf{w}_{\\mathrm{QK}}} J$ :\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{P} \\boldsymbol{\\Gamma}^{\\top} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top}\\right] & =\\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top} \\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\boldsymbol{\\Gamma}^{\\top} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\mathbb{E}\\left[\\left\\{\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X}\\right)(\\mathbf{X} \\boldsymbol{\\Gamma})^{\\top}\\right\\}^{\\top}\\left\\{\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X}\\right)(\\mathbf{X} \\boldsymbol{\\Gamma})^{\\top}\\right\\} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\mathbb{E}\\left[\\left\\{\\sum_{i \\in[T]}\\left(\\mathbf{X} \\widetilde{\\gamma}^{i}\\right)\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{x}_{i}\\right)^{\\top}\\right\\}\\left\\{\\sum_{j \\in[T]}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{x}_{j}\\right)\\left(\\mathbf{X}_{\\mathbf{\\gamma}^{j}}\\right)^{\\top}\\right\\} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\mathbb{E}\\left[\\sum_{i, j \\in[T]}\\left(\\mathbf{X} \\widetilde{\\gamma}^{i}\\right)\\left\\{\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{x}_{i}\\right)^{\\top}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{x}_{j}\\right)\\right\\}\\left(\\mathbf{X} \\widetilde{\\gamma}^{j}\\right)^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\mathbb{E}\\left[\\sum_{\\alpha, \\beta \\in[T]}\\left\\{\\sum_{i, j \\in[T]} \\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}}^{\\top} \\mathbf{x}_{j}\\right\\} \\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{\\beta}^{j} \\mathbf{x}_{\\alpha} \\mathbf{x}_{\\beta}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\sum_{i, j, \\alpha, \\beta \\in[T]} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{\\beta}^{j}\\left(\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}}^{\\prime} \\mathbf{x}_{j}\\right)\\left(\\mathbf{x}_{\\beta}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right) \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right]\n\\end{aligned}\n$$\n\nTo get Eq. (20), we expand the second term of $\\nabla_{\\mathbf{W}_{\\mathrm{QK}}} J$ :\n\n$$\n\\begin{aligned}\n& \\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{P} \\widetilde{\\gamma}_{0} \\mathbf{x}_{T}^{\\top}\\right]=\\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top} \\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X} \\widetilde{\\boldsymbol{\\gamma}}_{0} \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\mathbb{E}\\left[\\left\\{\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X}\\right)(\\mathbf{X} \\boldsymbol{\\Gamma})^{\\top}\\right\\}^{\\top}\\left\\{\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X}\\right)\\left(\\widetilde{\\gamma}_{0} \\mathbf{x}_{T}^{\\top}\\right)\\right\\}\\right] \\\\\n& =\\mathbb{E}\\left[\\left\\{\\sum_{i \\in[T]}\\left(\\mathbf{X} \\widetilde{\\gamma}^{i}\\right)\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{x}_{i}\\right)^{\\top}\\right\\}\\left\\{\\sum_{\\beta \\in[T]}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{x}_{\\beta}\\right)\\left(\\widetilde{\\gamma}_{0}^{\\beta} \\mathbf{x}_{T}^{\\top}\\right)\\right\\}\\right] \\\\\n& =\\mathbb{E}\\left[\\sum_{i, \\beta \\in[T]}\\left(\\mathbf{X} \\widetilde{\\gamma}^{i}\\right)\\left\\{\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{x}_{i}\\right)^{\\top}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{x}_{\\beta}\\right)\\right\\}\\left(\\widetilde{\\gamma}_{0}^{\\beta} \\mathbf{x}_{T}^{\\top}\\right)\\right] \\\\\n& =\\mathbb{E}\\left[\\sum_{\\alpha \\in[T]}\\left\\{\\sum_{i, \\beta \\in[T]} \\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}} \\mathbf{x}_{\\beta}\\right\\} \\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{0}^{\\beta} \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\sum_{i, \\alpha, \\beta \\in[T]} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{0}^{\\beta}\\left(\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}} \\mathbf{x}_{\\beta}\\right) \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right]\n\\end{aligned}\n$$\n\nTo get Eq. (21), we expand the third term of $\\nabla_{\\mathbf{W}_{\\mathrm{QK}}} J$ :\n\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{q} \\mathbf{x}_{T}^{\\top}\\right] & =\\mathbb{E}\\left[\\mathbf{X} \\boldsymbol{\\Gamma} \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{x}_{T}-\\mathbf{y}\\right) \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\mathbb{E}\\left[\\left\\{\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{X}\\right)(\\mathbf{X} \\boldsymbol{\\Gamma})^{\\top}\\right\\}^{\\top}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{x}_{T}-\\mathbf{y}\\right) \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\mathbb{E}\\left[\\left\\{\\sum_{i \\in[T]}\\left(\\mathbf{X}_{\\gamma}^{i}\\right)\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{W}_{\\mathrm{V}} \\mathbf{x}_{i}\\right)^{\\top}\\right\\}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{x}_{T}-\\mathbf{y}\\right) \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\mathbb{E}\\left[\\left\\{\\sum_{i, \\alpha \\in[T]} \\widetilde{\\gamma}_{\\alpha}^{i} \\mathbf{x}_{\\alpha} \\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top}\\right\\}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{x}_{T}-\\mathbf{y}\\right) \\mathbf{x}_{T}^{\\top}\\right] \\\\\n& =\\sum_{i, \\alpha \\in[T]} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i}\\left\\{\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{x}_{T}-\\mathbf{y}\\right)\\right\\} \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right]\n\\end{aligned}\n$$\n\n## B. 2 Order evaluation of QK-gradient\n\nThe orders of the QK-gradient terms (19), (20), and (21) are evaluated.",
    "locater-19": "In this subsection, we assume that the covariance matrix in Assumption 1 is $\\boldsymbol{\\Sigma}=\\mathbf{I}$ for simplicity. The following evaluation still applies with minor modifications for a general $\\boldsymbol{\\Sigma}$. For Eq. (19), the Cauchy-Schwarz inequality implies that\n\nEq. (19) $=\\left|\\sum_{i, j, \\alpha, \\beta} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{\\beta}^{j}\\left(\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}} \\mathbf{x}_{j}\\right)\\left(\\mathbf{x}_{\\beta}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right) \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right]\\right|^{2}$\n\n$$\n\\leq \\underbrace{\\left\\{\\sum_{i, j, \\alpha, \\beta} \\mathbb{E}\\left[\\left(\\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{\\beta}^{j}\\right)^{2}\\right]\\right\\}}_{(\\mathrm{A})} \\underbrace{\\left\\{\\sum_{i, j, \\alpha, \\beta} \\mathbb{E}\\left[\\left(\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}}_{\\mathbf{x}_{j}}\\right)^{2}\\right]\\right\\}}_{\\text {(B) }} \\underbrace{\\left\\{\\sum_{i, j, \\alpha, \\beta} \\mathbb{E}\\left[\\left(\\mathbf{x}_{\\beta}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right)^{2}\\right]\\right\\}}_{(\\mathrm{C})} \\underbrace{\\left\\{\\sum_{i, j, \\alpha, \\beta} \\mathbb{E}\\left[\\left(\\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right)^{\\odot 2}\\right]\\right\\}}_{\\text {(D) }}\n$$\n\nFor (A), we have $\\widetilde{\\gamma}_{\\alpha}^{i}, \\widetilde{\\gamma}_{\\beta}^{j} \\leq T^{-1}$ by definition, and hence (A) $=O(1)$. For (B), by using Eq. (17),\n(B) $=T^{2} \\sum_{i, j} \\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}} \\mathbf{x}_{j} \\mathbf{x}_{j}^{\\top} \\check{\\mathbf{P}} \\mathbf{x}_{i}\\right]=T^{2} \\sum_{i, j}\\left\\{\\left(i^{2}+i j-3 i-j+4\\right) \\operatorname{tr}\\left(\\check{\\mathbf{P}}^{2}\\right)+\\left(i^{2}-2 i+2\\right) \\operatorname{tr}(\\check{\\mathbf{P}})^{2}\\right\\}=O\\left(T^{6}\\right)$. For (C), by following the same computation as Eq. (22),\n\n$$\n\\begin{aligned}\n(\\mathrm{C}) & =T^{3} \\sum_{\\beta} \\mathbb{E}\\left[\\mathbf{x}_{\\beta}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{\\beta}\\right] \\\\\n& =T^{3} \\sum_{\\beta}\\left\\{\\left(\\beta^{2}+(T-3) \\beta-(T-4)\\right) \\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}^{2}\\right)+\\left(\\beta^{2}-2 \\beta+2\\right) \\operatorname{tr}\\left(\\mathbf{W}_{\\mathrm{QK}}\\right)^{2}\\right\\}=O\\left(T^{6}\\right)\n\\end{aligned}\n$$\n\nFor (D), its $(i, j)$-element can be evaluated as follows (no matter whether $i=j$ or not):\n$(\\mathrm{D})_{i j}=T^{3} \\sum_{\\alpha} \\mathbb{E}\\left[x_{\\alpha, i}^{2} x_{T, j}^{2}\\right]=T^{3} \\sum_{\\alpha} \\mathbb{E}\\left[x_{\\alpha, i}^{2}\\left\\{(T-\\alpha)+x_{\\alpha, j}^{2}\\right\\}\\right]=O\\left(T^{5}\\right) \\sum_{\\alpha} \\mathbb{E}\\left[x_{\\alpha, i}^{2}\\right]+T^{3} \\sum_{\\alpha} \\mathbb{E}\\left[x_{\\alpha, i}^{2} x_{\\alpha, j}^{2}\\right]=O\\left(T^{6}\\right)$\nBy plugging them back, we now confirmed that $\\mid$ Eq. (19) $\\mid=O\\left(T^{8}\\right)$. The orders of Eqs. (20) and (21) can be evaluated similarly and the detailed evaluations are omitted. $$\n\\begin{aligned}\n\\mid \\text { Eq. (20)| }\\left.\\right|^{2} & =\\left|\\sum_{i, \\alpha, \\beta} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{0}^{\\beta}\\left(\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}} \\mathbf{x}_{\\beta}\\right) \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right]\\right|^{2} \\\\\n& \\leq\\left\\{\\sum_{i, \\alpha, \\beta} \\mathbb{E}\\left[\\left(\\widetilde{\\gamma}_{\\alpha}^{i} \\widetilde{\\gamma}_{0}^{\\beta}\\right)^{2}\\right]\\right\\}\\left\\{\\sum _ { i , \\alpha , \\beta } \\mathbb { E } \\left[\\left(\\mathbf{x}_{i}^{\\top} \\check{\\mathbf{P}}_{\\left.\\left.\\mathbf{x}_{\\beta}\\right)^{2}\\right]}\\right\\}\\left\\{\\sum_{i, \\alpha, \\beta} \\mathbb{E}\\left[\\left(\\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right)^{\\odot 2}\\right]\\right\\}\\right.\\right. \\\\\n& =O(T) \\cdot O\\left(T^{4}\\right) \\cdot O\\left(T^{5}\\right) \\\\\n& =O\\left(T^{10}\\right) \\\\\n\\Longrightarrow \\mid \\text { Eq. (20)|} & =O\\left(T^{5}\\right) \\cdot \\\\\n\\mid \\text { Eq. (21)| }\\left.\\right|^{2} & =\\left|\\sum_{i, \\alpha} \\mathbb{E}\\left[\\widetilde{\\gamma}_{\\alpha}^{i}\\left\\{\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{x}_{T}-\\mathbf{y}\\right)\\right\\} \\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right]\\right|^{2} \\\\\n& \\leq\\left\\{\\sum_{i, \\alpha} \\mathbb{E}\\left[\\left(\\widetilde{\\gamma}_{\\alpha}^{i}\\right)^{2}\\right]\\right\\}\\left\\{\\sum_{i, \\alpha} \\mathbb{E}\\left[\\left(\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{V}}^{\\top} \\mathbf{W}_{\\mathrm{F}}^{\\top}\\left(\\mathbf{W}_{\\mathrm{F}} \\mathbf{x}_{T}-\\mathbf{y}\\right)\\right)^{2}\\right]\\right\\}\\left\\{\\sum_{i, \\alpha} \\mathbb{E}\\left[\\left(\\mathbf{x}_{\\alpha} \\mathbf{x}_{T}^{\\top}\\right)^{\\odot 2}\\right]\\right\\} \\\\\n& =O(1) \\cdot O\\left(T^{4}\\right) \\cdot O\\left(T^{4}\\right) \\\\\n& =O\\left(T^{8}\\right) \\\\\n\\Longrightarrow \\mid \\text { Eq. (21)|} & =O\\left(T^{4}\\right) . \\end{aligned}\n$$\n\nHence, we have $\\mid$ Eq. (19) $\\mid=O\\left(T^{8}\\right)$, $\\mid$ Eq. (20) $\\mid=O\\left(T^{5}\\right)$, and $\\mid$ Eq. (21) $\\mid=O\\left(T^{4}\\right)$, which implies that the QK-gradient (3) is asymptotically dominated by Eq.",
    "locater-20": "(19). ## C Proofs\n\nLemma 1. Suppose that $\\mathbf{W}_{\\mathrm{QK}}$ is symmetric and independent from $\\mathbf{X}$, and let $\\mathbf{W}:=\\mathbf{W}_{\\mathrm{QK}} \\boldsymbol{\\Sigma}$. Under Assumption 1, for $i \\in[T]$, the mean $\\mu^{i}$ and variance $v^{i}$ of $\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle+\\gamma_{0}^{i}$ with the input $\\boldsymbol{\\omega}:=\\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} / \\lambda$ are given as follows:\n\n$$\n\\begin{aligned}\n\\mu^{i} & =\\left(\\frac{i}{T}-\\frac{1}{2}\\right) \\frac{\\operatorname{tr}(\\mathbf{W})}{\\lambda}+o(1) \\\\\nv^{i} & =\\left(\\frac{2 i^{2}}{T^{2}}+\\frac{7}{12}\\right) \\frac{\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)}{\\lambda^{2}}+o(1)\n\\end{aligned}\n$$\n\nProof. To derive the mean, we use Eq. (15). $$\n\\begin{aligned}\n\\mu^{i} & =\\frac{1}{\\lambda T} \\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]-\\frac{1}{\\lambda T^{2}} \\sum_{j \\in[T]} \\mathbb{E}\\left[\\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]+o(1) \\\\\n& =\\frac{i-1}{\\lambda T} \\operatorname{tr}(\\mathbf{W})-\\frac{\\sum_{j \\in[T]}(j-1)}{\\lambda T^{2}} \\operatorname{tr}(\\mathbf{W})+o(1) \\\\\n& =\\left(i-\\frac{T+1}{2}\\right) \\frac{\\operatorname{tr}(\\mathbf{W})}{\\lambda T}+o(1)\n\\end{aligned}\n$$\n\nNote that $\\gamma_{0}^{i}=o(1)$. To derive the variance, we first compute $\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]$ (for $i \\leq j \\leq T$ ). $$\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right] & =\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}}\\left(\\mathbf{x}_{T} \\mathbf{x}_{T}^{\\top}\\right) \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{j}\\right] \\\\\n& =\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}}\\left\\{(T-j) \\boldsymbol{\\Sigma}+\\mathbf{x}_{j} \\mathbf{x}_{j}^{\\top}\\right\\} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{j}\\right] \\\\\n& =(T-j) \\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\boldsymbol{\\Sigma} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{j}\\right]+\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{j} \\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{j}\\right] \\\\\n& =(T-j)(i-1) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+(i j-i-j+2)\\left\\{2 \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+\\operatorname{tr}(\\mathbf{W})^{2}\\right\\} \\\\\n& =\\left(i j+(T-2) i-j-(T-4) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+(i j-i-j+2) \\operatorname{tr}(\\mathbf{W})^{2}\\right.",
    "locater-21": "\\end{aligned}\n$$\n\nwhere Eq. (11) is used recursively at the second identity and Eqs. (15) and (18) are used at the fourth identity. Then, the expectation of the squared term is expanded:\n\n$$\n\\begin{aligned}\n\\mathbb{E} & {\\left[\\left\\langle\\gamma^{i}, \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right\\rangle^{2}\\right] } \\\\\n= & \\mathbb{E}\\left[\\left(\\frac{1}{T} \\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}-\\frac{1}{T^{2}} \\sum_{j \\in[T]} \\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right)^{2}\\right] \\\\\n= & \\mathbb{E}\\left[\\frac{1}{T^{2}} \\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}-\\frac{2}{T^{3}} \\sum_{j \\in[T]} \\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}+\\frac{1}{T^{4}} \\sum_{j, j^{\\prime} \\in[T]} \\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T^{\\prime}} \\mathbf{x}_{j^{\\prime}}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right] \\\\\n= & \\frac{1}{T^{2}} \\underbrace{\\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]}_{(\\mathrm{A})} \\\\\n& -\\frac{1}{T^{3}} \\underbrace{2 \\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]}_{(\\mathrm{B} 1)}-\\frac{1}{T^{3}} \\underbrace{\\sum_{j>i} \\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]}_{(\\mathrm{Cl})}-\\frac{1}{T^{3}} \\underbrace{\\sum_{j<i} \\mathbb{E}\\left[\\mathbf{x}_{i}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]}_{(\\mathrm{B} 2)} \\\\\n& +\\frac{1}{T^{4}} \\underbrace{\\sum_{j \\in[T]} \\mathbb{E}\\left[\\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T} \\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]}_{(\\mathrm{B} 3)}+\\underbrace{\\frac{1}{\\sum_{j<j^{\\prime}}} \\mathbb{E}\\left[\\mathbf{x}_{j}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T^{\\prime}} \\mathbf{x}_{j^{\\prime}}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right]}_{T^{4}} . \\end{aligned}\n$$\n\nEach term is computed by using Eq.",
    "locater-22": "(22) multiple times. $$\n\\begin{aligned}\n(\\mathrm{A}) & =\\left(i^{2}+(T-3) i-(T-4)\\right) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+\\left(i^{2}-2 i+2\\right) \\operatorname{tr}(\\mathbf{W})^{2} \\\\\n& =\\left(i^{2}+T i\\right) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+i^{2} \\operatorname{tr}(\\mathbf{W})^{2}+o\\left(T^{2}\\right), \\\\\n(\\mathrm{B} 1) & =o\\left(T^{3}\\right), \\\\\n(\\mathrm{B} 2) & =2 \\sum_{j>i}\\left\\{(i j+(T-2) i-j-(T-4)) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+(i j-i-j+2) \\operatorname{tr}(\\mathbf{W})^{2}\\right\\} \\\\\n& =\\left(T^{2} i-2 T i^{2}-i^{3}\\right) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+\\left(T^{2} i-i^{3}\\right) \\operatorname{tr}(\\mathbf{W})^{2}+o\\left(T^{3}\\right), \\\\\n\\text { (B3) } & =2 \\sum_{j<i}\\left\\{(i j+(T-2) j-i-(T-4)) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+(i j-i-j+2) \\operatorname{tr}(\\mathbf{W})^{2}\\right\\} \\\\\n& =\\left(T i^{2}+i^{3}\\right) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+i^{3} \\operatorname{tr}(\\mathbf{W})^{2}+o\\left(T^{3}\\right), \\\\\n(\\mathrm{C} 1) & =\\sum_{j \\in[T]}\\left\\{\\left(j^{2}+(T-3) j-(T-4)\\right) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+\\left(j^{2}-2 j+2\\right) \\operatorname{tr}(\\mathbf{W})^{2}\\right\\} \\\\\n& =o\\left(T^{4}\\right), \\\\\n(\\mathrm{C} 2) & =2 \\sum_{j<j^{\\prime}}\\left\\{\\left(j j^{\\prime}+(T-2) j-j^{\\prime}-(T-4)\\right) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+\\left(j j^{\\prime}-j-j^{\\prime}+2\\right) \\operatorname{tr}(\\mathbf{W})^{2}\\right\\} \\\\\n& =2 \\sum_{j<j^{\\prime}} j j^{\\prime}\\left\\{\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+\\operatorname{tr}(\\mathbf{W})^{2}\\right\\}+2 \\sum_{j<j^{\\prime}} T j \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+o\\left(T^{4}\\right) \\\\\n& =\\sum_{j \\in[T]} j(T-j)(T+j+1)\\left\\{\\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+\\operatorname{tr}(\\mathbf{W})^{2}\\right\\}+2 T \\sum_{j \\in[T]}(T-j) j \\operatorname{tr}\\left(\\mathbf{W}{ }^{2}\\right)+o\\left(T^{4}\\right) \\\\\n& =\\sum_{j \\in[T]}\\left(T^{2} j-j^{3}\\right)\\left\\{\\operatorname{tr}\\left(\\mathbf{W}{ }^{2}\\right)+\\operatorname{tr}(\\mathbf{W})^{2}\\right\\}+\\frac{T^{4}}{3} \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+o\\left(T^{4}\\right) \\\\\n& =\\frac{7 T^{4}}{12} \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+\\frac{T^{4}}{4} \\operatorname{tr}(\\mathbf{W})^{2}+o\\left(T^{4}\\right) . \\end{aligned}\n$$\n\nBy plugging them back,\n\n$$\n\\mathbb{E}\\left[\\left\\langle\\gamma^{i}, \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right\\rangle^{2}\\right]=\\left(\\frac{7}{12}+\\frac{2 i^{2}}{T^{2}}\\right) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+\\left(\\frac{1}{4}-\\frac{i}{T}+\\frac{i^{2}}{T^{2}}\\right) \\operatorname{tr}(\\mathbf{W})^{2}+o(1)\n$$\n\nHence, the variance is derived:\n\n$$\n\\begin{aligned}\nv^{i} & =\\mathbb{V}\\left[\\left\\langle\\gamma^{i}, \\boldsymbol{\\omega}\\right\\rangle\\right] \\\\\n& =\\frac{1}{\\lambda^{2}} \\mathbb{E}\\left[\\left\\langle\\gamma^{i}, \\mathbf{X}^{\\top} \\mathbf{W}_{\\mathrm{QK}} \\mathbf{x}_{T}\\right\\rangle^{2}\\right]-\\left(\\mu^{i}\\right)^{2} \\\\\n& =\\frac{1}{\\lambda^{2}}\\left(\\frac{7}{12}+\\frac{2 i^{2}}{T^{2}}\\right) \\operatorname{tr}\\left(\\mathbf{W}^{2}\\right)+o(1)\n\\end{aligned}\n$$\n\nLemma 2.",
    "locater-23": "$\\rho(\\theta)$ satisfies the following properties. 1. (Late-/middle-site) $\\operatorname{As}(\\xi, \\eta) \\rightarrow(\\infty, 0)$ with $\\xi \\eta \\rightarrow r$,\n\n$$\n\\rho(\\theta) \\rightarrow \\begin{cases}\\mathbb{1}_{\\left\\{\\theta \\geq \\frac{1}{2}\\right\\}} & \\text { if } 0 \\leq r \\leq 2 \\\\ \\mathbb{1}_{\\left\\{\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}+\\frac{1}{r}\\right\\}} & \\text { if } r>2\\end{cases}\n$$\n\n2. (Early-/middle-site) As $(\\xi, \\eta) \\rightarrow(-\\infty, 0)$ with $\\xi \\eta \\rightarrow r$,\n\n$$\n\\rho(\\theta) \\rightarrow \\begin{cases}\\mathbb{1}_{\\left\\{\\theta \\leq \\frac{1}{2}\\right\\}} & \\text { if }-2 \\leq r<0 \\\\ \\mathbb{1}_{\\left\\{\\frac{1}{2}+\\frac{1}{r} \\leq \\theta \\leq \\frac{1}{2}\\right\\}} & \\text { if } r<-2\\end{cases}\n$$\n\n3. (Uniformity) Fix $\\eta$ as a finite value. As $|\\xi| \\rightarrow 0,\\left|\\rho^{\\prime}(\\theta)\\right| \\rightarrow 0$ for any $\\theta \\in[0,1]$. 4. (Vanishing signal) Fix $\\xi$ as a finite value. As $\\eta \\rightarrow \\infty, \\rho(\\theta) \\rightarrow 0$ for any $\\theta \\in[0,1]$. Proof. To see 1: We first see that as $\\xi \\rightarrow \\infty$,\n\n$$\n\\Phi\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi ; \\theta\\right) \\rightarrow \\begin{cases}\\frac{1}{2} & \\text { if } \\theta>\\frac{1}{2} \\\\ 0 & \\text { if } \\theta=\\frac{1}{2} \\\\ -\\frac{1}{2} & \\text { if } \\theta<\\frac{1}{2}\\end{cases}\n$$\n\nIn addition, as $\\xi \\rightarrow \\infty$ and $\\eta \\rightarrow 0$ with $\\xi \\eta \\rightarrow r \\in[0,2]$,\n\n$$\n\\Phi\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi-\\frac{1}{\\eta} ; \\theta\\right) \\rightarrow \\frac{1}{2} \\operatorname{erf}\\left(\\frac{\\left(\\theta-\\frac{1}{2}\\right) r-1}{\\eta \\sqrt{2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)}}\\right) \\rightarrow-\\frac{1}{2}\n$$\n\nBy combining them, $\\rho(\\theta) \\rightarrow \\mathbb{1}_{\\left\\{\\theta \\geq \\frac{1}{2}\\right\\}}$ at the limit. If $r>2$,\n\n$$\n\\Phi\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi-\\frac{1}{\\eta} ; \\theta\\right) \\rightarrow \\frac{1}{2} \\operatorname{erf}\\left(\\frac{\\left(\\theta-\\frac{1}{2}\\right) r-1}{\\eta \\sqrt{2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)}}\\right) \\rightarrow \\begin{cases}-\\frac{1}{2} & \\text { if } \\theta<\\frac{1}{2}+\\frac{1}{r} \\\\ 0 & \\text { if } \\theta=\\frac{1}{2}+\\frac{1}{r} \\\\ \\frac{1}{2} & \\text { if } \\theta>\\frac{1}{2}+\\frac{1}{r}\\end{cases}\n$$\n\nand $\\rho(\\theta) \\rightarrow \\mathbb{1}_{\\left\\{\\frac{1}{2} \\leq \\theta \\leq \\frac{1}{2}+\\frac{1}{r}\\right\\}}$ at the limit. We can see 2 in the same way as 1. To see 3: First, compute $\\rho^{\\prime}(\\theta)$ by using $\\frac{d}{d z} \\operatorname{erf}(z)=\\frac{2}{\\sqrt{\\pi}} \\exp \\left(-z^{2}\\right)$ :\n\n$$\n\\begin{aligned}\n\\rho^{\\prime}(\\theta) & =\\frac{1}{\\sqrt{\\pi}} \\exp \\left(-\\frac{\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi\\right)^{2}}{2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)}\\right) \\frac{d}{d \\theta}\\left\\{\\frac{\\left(\\theta-\\frac{1}{2}\\right) \\xi}{\\sqrt{2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)}}\\right\\}-\\frac{1}{\\sqrt{\\pi}} \\exp \\left(-\\frac{\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi-\\frac{1}{\\eta}\\right)^{2}}{2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)}\\right) \\frac{d}{d \\theta}\\left\\{\\frac{\\left(\\theta-\\frac{1}{2}\\right) \\xi-\\frac{1}{\\eta}}{\\sqrt{2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)}}\\right\\} \\\\\n& =\\left[\\frac{1}{\\sqrt{\\pi}} \\exp \\left(-\\frac{\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi\\right)^{2}}{2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)}\\right) \\frac{4 \\theta^{2}-\\theta+\\frac{5}{3}}{\\left(2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)\\right)^{3 / 2}}-\\frac{1}{\\sqrt{\\pi}} \\exp \\left(-\\frac{\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi-\\frac{1}{\\eta}\\right)^{2}}{2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)}\\right) \\frac{4 \\theta^{2}-\\theta+\\frac{5}{3}-\\frac{1}{\\eta}}{\\left(2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)\\right)^{3 / 2}}\\right]\n\\end{aligned}\n$$\n\nBy noting that $0<\\exp \\left(-z^{2}\\right) \\leq 1$,\n\n$$\n\\begin{aligned}\n\\left|\\rho^{\\prime}(\\theta)\\right| & \\leq \\frac{|\\xi|}{\\sqrt{\\pi}}\\left|\\frac{4 \\theta^{2}-\\theta+\\frac{5}{3}}{\\left(2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)\\right)^{3 / 2}}-\\frac{4 \\theta^{2}-\\theta+\\frac{5}{3}-\\frac{1}{\\eta}}{\\left(2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)\\right)^{3 / 2}}\\right| \\\\\n& =\\frac{|\\xi|}{\\sqrt{\\pi}} \\frac{1}{\\left(2\\left(2 \\theta^{2}+\\frac{7}{12}\\right)\\right)^{3 / 2} \\eta} \\\\\n& \\rightarrow 0 \\text { as }|\\xi| \\rightarrow 0 . \\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-23.jpg?height=879&width=1149&top_left_y=281&top_left_x=452)\n\nFigure 8: Experimental results of language modeling (WikiText-2) with $d=32$ with 1-layers transformers, fixed $\\kappa_{1}=100$, and varying regularization intensity $\\kappa_{2}$. With stronger $\\kappa_{2}$, the eigenspectrum scale shrinks (B), the attention entropy increases (C), and the perplexity improves (D). To see 4: For finite $\\xi$,\n\n$$\n\\lim _{\\eta \\rightarrow \\infty} \\Phi\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi-\\frac{1}{\\eta}\\right)=\\Phi\\left(\\left(\\theta-\\frac{1}{2}\\right) \\xi\\right)\n$$\n\nwhich indicates that $\\rho(\\theta) \\rightarrow 0$ at the limit $\\eta \\rightarrow \\infty$. ## D Additional experiments\n\nHere, we show additional results of the language modeling task with 1-/3-/6-layer transformers with different embedding dimensions $d=32,128$. For $d=128$, the configurations remain the same except for the number of decoder layers as in Section 7. For $d=32$, we used the learning rate 0.0001 (instead of 0.000025 used for $d=128$ ), and the other configurations remain the same. The results are shown in Fig. 8 ( $d=32$, 1-layers), Fig. $9(d=32$, 3-layers), Fig. $10(d=32,6$-layers $)$, Fig. $11(d=128$, 3-layers , and Fig. $12(d=128,6$-layers). The overall trends are quite similar to the case of 1-layer transformers with $d=128$ as seen in Fig. 6: As $\\kappa_{2}$ increases, the eigenspectrum scale decreases, the attention entropy increases, and eventually, the perplexity improves (namely, decreases). ![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-24.jpg?height=872&width=1150&top_left_y=382&top_left_x=451)\n\nFigure 9: Experimental results of language modeling (WikiText-2) with $d=32$ with 3-layers transformers, fixed $\\kappa_{1}=100$, and varying regularization intensity $\\kappa_{2}$. With stronger $\\kappa_{2}$, the eigenspectrum scale shrinks (B), the attention entropy increases (C), and the perplexity improves (D). ![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-24.jpg?height=891&width=1158&top_left_y=1549&top_left_x=447)\n\nFigure 10: Experimental results of language modeling (WikiText-2) with $d=32$ with 6 -layers transformers, fixed $\\kappa_{1}=100$, and varying regularization intensity $\\kappa_{2}$. With stronger $\\kappa_{2}$, the eigenspectrum scale shrinks (B), the attention entropy increases (C), and the perplexity improves (D). ![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-25.jpg?height=876&width=1153&top_left_y=382&top_left_x=448)\n\nFigure 11: Experimental results of language modeling (WikiText-2) with $d=128$ with 3-layers transformers, fixed $\\kappa_{1}=100$, and varying regularization intensity $\\kappa_{2}$. With stronger $\\kappa_{2}$, the eigenspectrum scale shrinks $(\\mathbf{B})$, the attention entropy increases (C), and the perplexity improves (D). ![](https://cdn.mathpix.com/cropped/2024_09_12_82543d419cbc987219f8g-25.jpg?height=875&width=1154&top_left_y=1562&top_left_x=451)\n\nFigure 12: Experimental results of language modeling (WikiText-2) with $d=128$ with 6 -layers transformers, fixed $\\kappa_{1}=100$, and varying regularization intensity $\\kappa_{2}$. With stronger $\\kappa_{2}$, the eigenspectrum scale shrinks $(\\mathbf{B})$, the attention entropy increases (C), and the perplexity improves (D).",
    "locater-24": "[^0]:    ${ }^{1}$ Note that our matrix notation is different from the one used in Dong et al. [6] so that we chose to let each column of $\\mathbf{X}$ store a token, whereas they let each row of $\\mathbf{X}$ store a token. [^1]:    ${ }^{2}$ The experimental results with deeper transformers are shown in Appendix D.",
    "locater-25": "The overall trends remain alike.",
    "locater-26": ""
}