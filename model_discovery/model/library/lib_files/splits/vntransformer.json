{
    "vntransformer-0": "# VN-Transformer: Rotation-Equivariant Attention for Vector Neurons \n\nSerge Assaad *<br>serge.assaad@duke.edu<br>Duke University<br>Carlton Downey<br>cmdowney@waymo.com<br>Waymo LLC<br>Rami Al-Rfou<br>rmyeid@waymo.com<br>Waymo LLC<br>Nigamaa Nayakanti nigamaa@waymo.com<br>Waymo LLC<br>Ben Sapp Waymo LLC bensapp@waymo.com\n\nReviewed on OpenReview: https://openreview. net/forum? id=EiX2L4sDPG\n\n\n#### Abstract\n\nRotation equivariance is a desirable property in many practical applications such as motion forecasting and 3 D perception, where it can offer benefits like sample efficiency, better generalization, and robustness to input perturbations. Vector Neurons (VN) is a recently developed framework offering a simple yet effective approach for deriving rotation-equivariant analogs of standard machine learning operations by extending one-dimensional scalar neurons to three-dimensional \"vector neurons.\" We introduce a novel \"VN-Transformer\" architecture to address several shortcomings of the current VN models. Our contributions are: $(i)$ we derive a rotation-equivariant attention mechanism which eliminates the need for the heavy feature preprocessing required by the original Vector Neurons models; (ii) we extend the VN framework to support non-spatial attributes, expanding the applicability of these models to real-world datasets; (iii) we derive a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution, greatly speeding up inference and training; (iv) we show that small tradeoffs in equivariance ( $\\epsilon$-approximate equivariance) can be used to obtain large improvements in numerical stability and training robustness on accelerated hardware, and we bound the propagation of equivariance violations in our models. Finally, we apply our VN-Transformer to 3D shape classification and motion forecasting with compelling results. [^0]\n## 1 Introduction\n\nA chair - seen from the front, the back, the top, or the side - is still a chair. When driving a car, our driving behavior is independent of our direction of travel. These simple examples demonstrate how humans excel at using rotation invariance and equivariance to understand the world in context (see figure on the right). Unfortunately, typical machine learning models struggle to preserve equivariance/invariance when appropriate - it is indeed challenging to equip neural networks with the right inductive biases to represent 3 D objects in an equivariant manner. Modeling spatial data is a core component in many domains such as CAD, AR/VR, and medical imaging applications. In assistive robotics and autonomous vehicle applications, 3D object detection, tracking, and motion forecasting form the basis for how a robot interacts with\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-02.jpg?height=541&width=514&top_left_y=426&top_left_x=1334)\nhumans in the real world. Preserving rotation invariance/equivariance can improve training time, reduce model size, and provide crucial guarantees about model performance in the presence of noise. Spatial data is often represented as a point-cloud data structure. These point-clouds require both permutation invariance and rotation equivariance to be modeled sufficiently well. Approaches addressing permutation invariance include Zaheer et al. (2018); Lee et al. (2019); Qi et al. (2017). Recently, approaches jointly addressing rotation invariance and equivariance are gaining momentum. These can roughly be categorized into modeling invariance or equivariance by: (i) data augmentation, (ii) canonical pose estimation, and (iii) model construction. Approaches $(i)$ and $(i i)$ do not guarantee exact equivariance since they rely on model parameters to learn the right inductive biases Qi et al., 2017, Esteves et al., 2018.",
    "vntransformer-1": "Jaderberg et al. 2015 Chang et al. 2015). Further, data augmentation makes training more costly and errors in pose estimation propagate to downstream tasks, degrading model performance. Moreover, pose estimation requires labeling objects with their observed poses. In contrast, proposals in (iii) do provide equivariance guarantees, including the Tensor Field Networks of Thomas et al. (2018) and the SE(3)-Transformer of Fuchs et al. (2020) (see Section 2). However, their formulations require complex mathematical machinery or are limited to specific network architectures. Most recently, Deng et al. (2021) proposed a simple and generalizable framework, dubbed Vector Neurons (VNs), that can be used to replace traditional building blocks of neural networks with rotation-equivariant analogs. The basis of the framework is lifting scalar neurons to 3-dimensional vectors, which admit simple mappings of $\\mathrm{SO}(3)$ actions to latent spaces.",
    "vntransformer-2": "While Deng et al. (2021) developed a framework and basic layers, many issues required for practical deployment on real-world applications remain unaddressed. A summary of our contributions and their motivations are as follows:\n\nVN-Transformer. The use of Transformers in deep learning has exploded in popularity in recent years as the de facto standard mechanism for learned soft attention over input and latent representations. They have enjoyed many successes in image and natural language understanding (Khan et al., 2022, Vaswani et al. 2017), and they have become an essential modeling component in most domains. Our primary contribution of this paper is to develop a VN formulation of soft attention by generalizing scalar inner-product based attention to matrix inner-products (i.e., the Frobenius inner product). Thanks to Transformers' ability to model functions on sets (since they are permutation-equivariant), they are a natural fit to model functions on point-clouds. Our VN-Transformer possesses all the appealing properties that have made the original Transformer so successful, as well as rotation equivariance as an added benefit. Direct point-set input. The original VN paper relied on edge convolution as a pre-processing step to capture local point-cloud structure. Such feature engineering is not data-driven and requires human involvement in designing and tuning. Moreover, the sparsity of these computations makes them slow to run\non accelerated hardware. Our proposed rotationequivariant attention mechanism learns higher-level features directly from single points for arbitrary point-clouds (see Section 4). Handling points augmented with non-spatial attributes. Real-world point-cloud datasets have complicated features sets - the $[x, y, z]$ spatial dimensions are typically augmented with crucial non-spatial attributes $[[x, y, z] ;[a]]$ where $a$ can be high-dimensional. For example, Lidar point-clouds have intensity \\& elongation values associated with each point, multi-sensor point-clouds have modality types, and point-clouds with semantic type have semantic attributes. The VN framework restricted the scope of their work to spatial point-cloud data, limiting the applicability of their models for real-world point-clouds with attributes. We investigate two mechanisms to integrate attributes into equivariant models while preserving rotation equivariance (see Section 5 ). Equivariant multi-scale feature reduction. Practical data structures such as Lidar point-clouds are extremely large, consisting of hundreds of objects each with potentially millions of points. To handle such computationally challenging situations, we design a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution. This mechanism learns how to pool the point set in a context-sensitive manner leading to a significant reduction in training and inference latency (see Section 6). $\\epsilon$-approximate equivariance. When attempting to scale up VN models for distributed accelerated hardware we observed significant numerical stability issues. We determined that these stemmed from a fundamental limitation of the original VN framework, where bias values could not be included in linear layers while preserving equivariance. We introduce the notion of $\\epsilon$-approximate equivariance, and use it to show that small tradeoffs in equivariance can be controlled to obtain large improvements in numerical stability via the addition of small biases, improving robustness of training on accelerated hardware.",
    "vntransformer-3": "Additionally, we theoretically bound the propagation of rotation equivariance violations in VN networks (see Section 7 ). Empirical analysis. Finally, we evaluate our VN-Transformer on $(i)$ the ModelNet40 shape classification task, (ii) a modified ModelNet 40 which includes per-point non-spatial attributes, and (iii) a modified version of the Waymo Open Motion Dataset trajectory forecasting task (see Section 8). ## 2 Related work\n\nThe machine learning community has long been interested in building models that achieve equivariance to certain transformations, e.g., permutations, translations, and rotations.",
    "vntransformer-4": "For a thorough review, see Bronstein et al. (2021). Learned approximate transformation invariance. A very common approach is to learn robustness to input transforms via data augmentation (Zhou \\& Tuzel, 2018, Qi et al., 2018; Krizhevsky et al. 2012 Lang et al., 2019; Yang et al., 2018) or by explicitly predicting transforms to canonicalize pose (Jaderberg et al., 2015 Hinton et al., 2018 Esteves et al. 2018; Chang et al., 2015). Rotation-equivariant CNNs. Recently, there has been specific interest in designing rotation-equivariant image models for 2D perception tasks (Cohen \\& Welling, 2016.",
    "vntransformer-5": "Worrall et al., 2017, Marcos et al., 2017, Chidester et al. 2018). Worrall \\& Brostow (2018) extended this work to 3D perception, and Veeling et al. (2018) demonstrated the promise of rotation-equivariant models for medical images. Equivariant point-cloud models. Thomas et al. (2018) proposed Tensor Field Networks (TFNs), which use tensor representations of point-cloud data, Clebsch-Gordan coefficients, and spherical harmonic filters to build rotation-equivariant models. Fuchs et al. (2020) propose an \"SE(3)-Transformer\" by adding an attention mechanism for TFNs. One of the key ideas behind this body of work is to create highly restricted weight matrices that commute with rotation operations by construction (i.e., $W R=R W$ ). In contrast, we propose a simpler alternative: a \"VN-Transformer\" which guarantees equivariance for arbitrary weight matrices, removing the need for the complex mathematical machinery of the $\\mathrm{SE}(3)$-Transformer.",
    "vntransformer-6": "For a detailed comparison with Fuchs et al. (2020), see Appendix A. Controllable approximate equivariance. Finzi et al. (2021) proposed equivariant priors on model weight matrices to achieve approximate equivariance, and Wang et al. (2022) proposed a relaxed steerable 2D convolution along with soft equivariance regularization. In this work, we introduce the related notion of \" $\\epsilon$-approximate equivariance,\" achieved by adding biases with small and controllable norms. We theoretically bound the equivariance violation introduced by this bias, and we also bound how such violations propagate through deep VN networks. Non-spatial attributes. TFNs and the SE(3)-Transformer account for non-spatial attributes associated with each point (e.g., color, intensity), which they refer to as \"type-0\" features. In this work, we investigate two mechanisms (early \\& late fusion) to incorporate non-spatial data into the VN framework. Attention-based architectures. Since the introduction of Transformers by Vaswani et al. (2017), selfattention and cross-attention mechanisms have provided powerful and versatile components which are propelling the field of natural language processing forward (Devlin et al., 2019, Liu et al., 2019, Lan et al., 2020, Yang et al., 2020). Lately, so-called \"Vision Transformers\" (Dosovitskiy et al., 2021| Khan et al., 2022) have had a similar impact on the field of computer vision, providing a compelling alternative to convolutional networks. ## 3 Background\n\n### 3.1 Notation \\& preliminaries\n\nDataset. Suppose we have a dataset $\\mathcal{D} \\triangleq\\left\\{X_{p}, Y_{p}\\right\\}_{p=1}^{P}$, where $p \\in\\{1, \\ldots, P\\}$ is an index into a pointcloud/label pair $\\left\\{X_{p}, Y_{p}\\right\\}$ - we omit the subscript $p$ whenever it is unambiguous to do so. $X \\in \\mathcal{X} \\subset \\mathbb{R}^{N \\times 3}$ is a single 3 D point-cloud with $N$ points. In a classification problem, $Y \\in \\mathcal{Y} \\subset\\{1, \\ldots, \\kappa\\}$, where $\\kappa$ is the number of classes. In a regression problem, we might have $Y \\in \\mathcal{Y} \\subset \\mathbb{R}^{N_{\\text {out }} \\times S_{\\text {out }}}$ where $N_{\\text {out }}$ is the number of output points, and $S_{\\text {out }}$ is the dimension of each output point (with $N_{\\text {out }}=S_{\\text {out }}=1$ corresponding to univariate regression).",
    "vntransformer-7": "Index notation. We use \"numpy-like\" indexing of tensors. Assuming we have a tensor $Z \\in \\mathbb{R}^{A \\times B \\times C}$, we present some examples of this indexing scheme: $Z^{(a)} \\in \\mathbb{R}^{B \\times C}, \\quad Z^{(:,, c)} \\in \\mathbb{R}^{A \\times B}, \\quad Z^{\\left(a_{\\mathrm{lo}}: a_{\\mathrm{hi}}\\right)} \\in$ $\\mathbb{R}^{\\left(a_{\\mathrm{hi}}-a_{\\mathrm{lo}}+1\\right) \\times B \\times C}$. Rotations \\& weights. Suppose we have a tensor $V \\in \\mathbb{R}^{N \\times C \\times 3}$ and a rotation matrix $R \\in \\mathrm{SO}(3)$, where $\\mathrm{SO}(3)$ is the three-dimensional rotation group. We denote the \"rotation\" of the tensor by $V R \\in \\mathbb{R}^{N \\times C \\times 3}$, defined as: $(V R)^{(n)} \\triangleq V^{(n)} R, \\quad \\forall n \\in\\{1, \\ldots, N\\}$ - in other words, the rotated tensor $V R$ is simply the concatenation of the $N$ individually rotated matrices $V^{(n)} R \\in \\mathbb{R}^{C \\times 3}$. Additionally, if we have a matrix of weights $W \\in \\mathbb{R}^{C^{\\prime} \\times C}$, we define the product $W V \\in \\mathbb{R}^{N \\times C^{\\prime} \\times 3}$ by $(W V)^{(n)} \\triangleq W V^{(n)}$. ## Invariance and equivariance. Definition 1 (Rotation Invariance). $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ is rotation-invariant if $\\forall R \\in \\mathrm{SO}(3), X \\in \\mathcal{X}, \\quad f(X R)=$ $f(X)$. Definition 2 (Rotation Equivariance). $f: \\mathcal{X} \\rightarrow \\mathcal{Y}$ is rotation-equivariant if $\\forall R \\in \\operatorname{SO}(3), X \\in \\mathcal{X}, \\quad f(X R)=$ $f(X) R$.",
    "vntransformer-8": "For simplicity, we defined invariance/equivariance as above instead of the more general $f\\left(X \\rho_{X}(g)\\right)=$ $f(X) \\rho_{Y}(g)$, which requires background on group theory and representation theory. Proofs. We defer proofs to Appendix C\n\n### 3.2 The Vector Neuron (VN) framework\n\nIn the Vector Neuron framework (Deng et al., 2021), the authors represent a single point (e.g., in a hidden layer of a neural network) as a matrix $V^{(n)} \\in \\mathbb{R}^{C \\times 3}$ (see inset figure), where $V \\in \\mathbb{R}^{N \\times C \\times 3}$ can be thought of as a tensor representation of the entire point-cloud. This representation allows for the design of $\\mathrm{SO}(3)$-equivariant analogs of standard neural network operations. ![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-05.jpg?height=202&width=183&top_left_y=452&top_left_x=1269)\n\nClassical neurons\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-05.jpg?height=198&width=313&top_left_y=454&top_left_x=1535)\n\nVector Neurons\nVN-Linear layer. As an illustrative example, the VN-Linear layer is a function VN-Linear $(\\cdot ; W): \\mathbb{R}^{C \\times 3} \\rightarrow$ $\\mathbb{R}^{C^{\\prime} \\times 3}$, defined by $\\operatorname{VN}-\\operatorname{Linear}\\left(V^{(n)} ; W\\right) \\triangleq W V^{(n)}$, where $W \\in \\mathbb{R}^{C^{\\prime} \\times C}$ is a matrix of learnable weights. This operation is rotation-equivariant: VN-Linear $\\left(V^{(n)} R ; W\\right)=W V^{(n)} R=\\left(W V^{(n)}\\right) R=\\operatorname{VN}-L i n e a r\\left(V^{(n)} ; W\\right) R$. Deng et al. (2021) also develop VN analogs of common deep network layers ReLU, MLP, BatchNorm, and Pool. For further definitions and proofs of equivariance, see Appendix C For further details, we point the reader to Deng et al. (2021). ## 4 The VN-Transformer\n\nIn this section, we extend the ideas presented in Deng et al. (2021) to design a \"VN-Transformer\" that enjoys the rotation equivariance property. ### 4.1 Rotation-invariant inner product\n\nThe notion of an inner product between tokens is central to the attention operation from the original Transformer (Vaswani et al. 2017). Consider the Frobenius inner product between two VN representations, defined below. Definition 3 (Frobenius inner product). The Frobenius inner product between two matrices $V^{(n)}, V^{\\left(n^{\\prime}\\right)} \\in$ $\\mathbb{R}^{C \\times 3}$ is defined by $\\left\\langle V^{(n)}, V^{\\left(n^{\\prime}\\right)}\\right\\rangle_{F} \\triangleq \\sum_{c=1}^{C} \\sum_{s=1}^{3} V^{(n, c, s)} V^{\\left(n^{\\prime}, c, s\\right)}=\\sum_{c=1}^{C} V^{(n, c)} V^{\\left(n^{\\prime}, c\\right) \\top}$. This choice of inner product is convenient because of its rotation invariance property, stated below. Proposition 1. The Frobenius inner product between Vector Neuron representations $V^{(n)}, V^{\\left(n^{\\prime}\\right)} \\in \\mathbb{R}^{C \\times 3}$ is rotation-invariant, i.e. $\\left\\langle V^{(n)} R, V^{\\left(n^{\\prime}\\right)} R\\right\\rangle_{F}=\\left\\langle V^{(n)}, V^{\\left(n^{\\prime}\\right)}\\right\\rangle_{F}, \\quad \\forall R \\in \\mathrm{SO}(3)$. Proof. $$\n\\left\\langle V^{(n)} R, V^{\\left(n^{\\prime}\\right)} R\\right\\rangle_{F}=\\sum_{c=1}^{C}\\left(V^{(n, c)} R\\right)\\left(V^{\\left(n^{\\prime}, c\\right)} R\\right)^{\\top}=\\sum_{c=1}^{C} V^{(n, c)} R R^{\\top} V^{\\left(n^{\\prime}, c\\right) \\top}=\\sum_{c=1}^{C} V^{(n, c)} V^{\\left(n^{\\prime}, c\\right) \\top}=\\left\\langle V^{(n)}, V^{\\left(n^{\\prime}\\right)}\\right\\rangle_{F}\n$$\n\nThis rotation-invariant inner product between VN representations allows us to construct a rotation-equivariant attention operation, detailed in the next section. ### 4.2 Rotation-equivariant attention\n\nConsider two tensors $Q \\in \\mathbb{R}^{M \\times C \\times 3}$ and $K \\in \\mathbb{R}^{N \\times C \\times 3}$, which can be thought of as sets of $M$ (resp. $N$ ) tokens, each a $C \\times 3$ matrix. Using the Frobenius inner product, we can define an attention matrix $A(Q, K) \\in \\mathbb{R}^{M \\times N}$ between the two sets as follows:\n\n$$\nA(Q, K)^{(m)} \\triangleq \\operatorname{softmax}\\left(\\frac{1}{\\sqrt{3 C}}\\left[\\left\\langle Q^{(m)}, K^{(n)}\\right\\rangle_{F}\\right]_{n=1}^{N}\\right)\n$$\n\nThis attention operation is rotation-equivariant w.r.t. simultaneous rotation of all inputs: Following Vaswani et al.",
    "vntransformer-9": "(2017), we divide the inner products by $\\sqrt{3 C}$ since $Q^{(m)}, K^{(n)} \\in \\mathbb{R}^{C \\times 3}$. From Proposition 1, $A(Q R, K R)=A(Q, K) \\quad \\forall R \\in \\mathrm{SO}(3)$. Finally, we define the operation VN-Attn : $\\mathbb{R}^{M \\times C \\times 3} \\times \\mathbb{R}^{N \\times C \\times 3} \\times$ $\\mathbb{R}^{N \\times C^{\\prime} \\times 3} \\rightarrow \\mathbb{R}^{M \\times C^{\\prime} \\times 3}$ as:\n\n$$\n\\mathrm{VN}-\\operatorname{Attn}(Q, K, Z)^{(m)} \\triangleq \\sum_{n=1}^{N} A(Q, K)^{(m, n)} Z^{(n)}\n$$\n\nProposition 2. VN- $\\operatorname{Attn}(Q R, K R, Z R)=\\mathrm{VN}-\\operatorname{Attn}(Q, K, Z) R$. Proof. $$\n\\begin{aligned}\n& \\mathrm{VN}-\\operatorname{Attn}(Q R, K R, Z R)^{(m)}=\\sum_{n=1}^{N} A(Q R, K R)^{(m, n)} Z^{(n)} R \\\\\n& \\stackrel{(*)}{=}\\left[\\sum_{n=1}^{N} A(Q, K)^{(m, n)} Z^{(n)}\\right] R=\\mathrm{VN}-\\operatorname{Attn}(Q, K, Z)^{(m)} R\n\\end{aligned}\n$$\n\nwhere $(*)$ holds since $A(Q R, K R)=A(Q, K)$ (which follows straightforwardly from Proposition 1 and equation 2 . This is extendable to multi-head attention with $H$ heads, VN-MultiHeadAttn : $\\mathbb{R}^{M \\times C \\times 3} \\times \\mathbb{R}^{N \\times C \\times 3} \\times \\mathbb{R}^{N \\times C^{\\prime} \\times 3} \\rightarrow \\mathbb{R}^{M \\times C^{\\prime} \\times 3}$ :\n![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-06.jpg?height=625&width=362&top_left_y=275&top_left_x=1421)\n\nFigure 2: VN-Transformer encoder block architecture. VN-MultiheadAttn and VNLayerNorm are defined in equation 6 and equation 7 respectively. VN-MLP is a composition of VN-Linear, VN-BatchNorm, and VN-ReLU layers from Deng et al. (2021). $$\n\\operatorname{VN}-\\operatorname{MultiHeadAttn}(Q, K, Z) \\triangleq W^{O}\\left[\\mathrm{VN}-\\operatorname{Attn}\\left(W_{h}^{Q} Q, W_{h}^{K} K, W_{h}^{Z} Z\\right)\\right]_{h=1}^{H}\n$$\n\nwhere $W_{h}^{Q}, W_{h}^{K} \\in \\mathbb{R}^{P \\times C}, W_{h}^{Z} \\in \\mathbb{R}^{P \\times C^{\\prime}}$ are feature, key, and value projection matrices of the $h$-th head (respectively), and $W^{O} \\in \\mathbb{R}^{C^{\\prime} \\times H P}$ is an output projection matrix Vaswani et al. 2017, 1 VN-MultiHeadAttn is also rotation-equivariant, and is the key building block of our rotation-equivariant VN -Transformer.",
    "vntransformer-10": "We note that a similar idea was proposed in Fuchs et al. (2020) - namely, they use inner products between equivariant representations (obtained from the TFN framework of Thomas et al. (2018) to create a rotationinvariant attention matrix and a rotation-equivariant attention mechanism. Our attention mechanism can be thought of as the same treatment applied to the VN framework. For a more detailed comparison between this work and the proposal of Fuchs et al. (2020), see Appendix A\n\n### 4.3 Rotation-equivariant layer normalization\n\nDeng et al. (2021) allude to a rotation-equivariant version of the well-known layer normalization operation (Ba et al., 2016), but do not explicitly provide it - we do so here for completeness (see Figure 3):\n\n$$\n\\begin{aligned}\n& \\operatorname{VN}-\\text { LayerNorm }\\left(V^{(n)}\\right) \\triangleq \\\\\n& {\\left[\\frac{V^{(n, c)}}{\\left\\|V^{(n, c)}\\right\\|_{2}}\\right]_{c=1}^{C} \\odot \\operatorname{LayerNorm}\\left(\\left[\\left\\|V^{(n, c)}\\right\\|_{2}\\right]_{c=1}^{C}\\right) \\mathbb{1}_{1 \\times 3}}\n\\end{aligned}\n$$\n\nwhere $\\odot$ is an elementwise product, LayerNorm: $\\mathbb{R}^{C} \\rightarrow \\mathbb{R}^{C}$ is the layer normalization operation of Ba et al. (2016), and $\\mathbb{1}_{1 \\times 3}$ is a row-vector of ones. ![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-06.jpg?height=313&width=618&top_left_y=1898&top_left_x=1271)\n\nFigure 3: VN-LayerNorm: rotationequivariant layer normalization. \"Layer Norm\" is the standard layer normalization operation of Ba et al.",
    "vntransformer-11": "(2016). and $\\oslash$ are row-wise multiplication and division. [^1]\n### 4.4 Encoder architecture\n\nFigure 2 details the architecture of our proposed rotation-equivariant VN-Transformer encoder. The encoder is structurally identical to the original Transformer encoder of Vaswani et al. (2017), with each operation replaced by its rotation-equivariant VN analog. ## 5 Non-spatial attributes\n\n\"Real-world\" point-clouds are typically augmented with crucial meta-data such as intensity \\& elongation for Lidar point-clouds and \\& sensor type for multi-sensor point-clouds. Handling such point-clouds while still satisfying equivariance/invariance w.r.t. spatial inputs would be useful for many applications. We investigate two strategies (late fusion and early fusion) to handle non-spatial attributes while maintaining rotation equivariance/invariance w.r.t. spatial dimensions:\nLate fusion. In this approach, we propose to incorporate non-spatial attributes into the model at a later stage, where we have already processed the spatial inputs in a rotationequivariant fashion - our \"late fusion\" models for classification and trajectory prediction are shown in Figure 4\nEarly fusion. Early fusion is a simple yet powerful way to process non-spatial attributes (Jaegle et al. 2021). In this approach, we do not treat non-spatial attributes differently (see Figure 1) - we simply concatenate spatial \\& non-spatial inputs before feeding them into the VN-Transformer. The VN representations obtained are $C \\times\\left(3+d_{A}\\right)$ matrices (instead of $C \\times 3$ ). ![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-07.jpg?height=645&width=674&top_left_y=669&top_left_x=1174)\nFigure 4: VN-Transformer (\"late fusion\") models. Legend: ( $\\square$ ) SO(3)-equivariant features; ( $\\square$ ) SO(3)-invariant features; ( $\\quad$ ) Non-spatial features. ## 6 Rotation-equivariant multi-scale feature aggregation\n\nJaegle et al. (2021) recently proposed an attention-based architecture, PerceiverIO, which reduces the computational complexity of the vanilla Transformer by reducing the number of tokens (and their dimension) in the intermediate representations of the network. They achieve this reduction by learning a set $Z \\in \\mathbb{R}^{M \\times C^{\\prime}}$ of \"latent features,\" which they use to perform QKV attention with the original input tokens $X \\in \\mathbb{R}^{N \\times C}$ (with $M<<N$ and $\\left.C^{\\prime}<<C\\right)$. Finally, they perform self-attention operations on the resulting $M \\times C^{\\prime}$ array, leading to a $\\mathcal{O}\\left(M^{2} C^{\\prime}\\right)$ runtime instead of\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-07.jpg?height=245&width=589&top_left_y=1753&top_left_x=1256)\n\nFigure 5: Rotation-equivariant latent features for Vector Neurons. $\\mathcal{O}\\left(N^{2} C\\right)$ for each encoder self-attention operation, greatly improving time complexity during training and inference - a boon for time-critical applications such as real-time motion forecasting. However, such learnable latent features would violate equivariance in our case, since these learnable features would have no information about the original input's orientation. To remedy this, we instead propose to a learn a transformation from the inputs to the latent features (where the number of latent features is much smaller than the number of original inputs). Specifically, we propose to use a mean projection function VN-MeanProject $(V)^{(m)} \\triangleq$ $W^{(m)}\\left[\\frac{1}{N} \\sum_{n=1}^{N} V^{(n)}\\right]$, where $W \\in \\mathbb{R}^{M \\times C^{\\prime} \\times C}$ is a learnable tensor. VN-MeanProject is both rotationequivariant and permutation-invariant. We then perform VN-MultiHeadAttention between the resulting latent features and the original inputs $V$ to get a smaller set of VN representations. The architecture diagram for our proposed rotation-equivariant \"latent feature\" mechanism is shown in Figure 5\n\n## 7 -approximate equivariance\n\nWe noticed that distributed training on accelerated hardware is numerically unstable for points with small norms. This is unique to VN models - the VN-Linear layer does not include a bias vector, which leads to frequent underflow issues on distributed accelerated hardware. We found that introducing small and controllable additive biases fixes these issues - we modify the VN-Linear layer by adding a bias with controllable norm:\n\n$$\n\\operatorname{VN-LinearWithBias}\\left(V^{(n)} ; W, U, \\epsilon\\right) \\triangleq W V^{(n)}+\\epsilon U, \\quad U^{(c)} \\triangleq B^{(c)} /\\left\\|B^{(c)}\\right\\|_{2}\n$$\n\nwhere $\\epsilon \\geq 0$ is a hyperparameter controlling the bias norm, and $B \\in \\mathbb{R}^{C^{\\prime} \\times 3}$ is a learnable matrix. This leads to significant improvements in training stability and model quality. In principle, VN-LinearWithBias is not equivariant, but its violation of equivariance can be bounded. Work on equivariance by construction typically treats rotation equivariance as a binary idea - a model is either equivariant, or it is not. This can be relaxed by asking: how large is the violation of equivariance? We quantify this with the equivariance violation metric, defined by:\n\n$$\n\\Delta(f, X, R) \\triangleq\\|f(X R)-f(X) R\\|_{F}\n$$\n\nIf $\\Delta(f, X, R) \\leq \\epsilon$, we say $f$ is $\\epsilon$-approximately equivariant. We bound the equivariance violation of VN-LinearWithBias $(\\cdot ; W, U, \\epsilon): \\mathbb{R}^{C \\times 3} \\rightarrow \\mathbb{R}^{C^{\\prime} \\times 3}$ below:\nProposition 3. VN-LinearWithBias is $\\left(2 \\epsilon \\sqrt{C^{\\prime}}\\right.$ )-approximately equivariant (tight when $R=-I$ ). A natural next question is: how do such equivariance violations propagate through a deep model? Proposition 4. Suppose we have $K$ functions $f_{k}: \\mathcal{X}_{k} \\rightarrow \\mathcal{X}_{k+1}$ (with $\\mathcal{X}_{k} \\subset \\mathbb{R}^{C_{k} \\times 3}, \\mathcal{X}_{k+1} \\subset \\mathbb{R}^{C_{k+1} \\times 3}$, $k \\in\\{1, \\ldots, K\\})$ s.t. 1. $f_{k}$ is $\\epsilon_{k}$-approximately equivariant for all $k \\in\\{1, \\ldots, K\\}$\n2. $f_{k}$ is $L_{k}$-Lipschitz (w.r.t. $\\|\\cdot\\|_{F}$ ) for all $k \\in\\{2, \\ldots, K\\}$. Then, the composition $f_{K} \\circ \\cdots \\circ f_{1}$ is $\\epsilon_{1 \\ldots K}$-approximately equivariant, where\n\n$$\n\\epsilon_{1 \\ldots K} \\triangleq L_{K}\\left(\\cdots\\left(L_{3}\\left(L_{2} \\epsilon_{1}+\\epsilon_{2}\\right)+\\epsilon_{3}\\right)+\\cdots\\right)+\\epsilon_{K}\n$$\n\nIntuitively, each layer $f_{k}$ \"stretches\" the equivariance violation error of the previous layers by its Lipschitz constant $L_{k}$, and adds its own violation $\\epsilon_{k}$ to the total error. ## 8 Experiments\n\n### 8.1 Rotation-invariant classification\n\nFigure 1a shows our proposed VN-Transformer architecture for classification. It consists of rotation-equivariant operations (VN-MLP and VN-Transformer Encoder blocks), followed by an invariant operation (VN-Invariant), and finally standard Flatten/Pool/MLP operations to get class predictions. The resulting logits/class predictions are rotation-invariant. Table 1: ModelNet40 test accuracy. Top block shows $\\mathrm{SO}(3)$-invariant baselines taken from Deng et al. (2021), included here for convenience. | Model | Acc. | \\# Params |\n| :---: | :---: | :---: |\n| TFN Thomas et al. 2018 | $88.5 \\%$ | - |\n| RI-Conv (Zhang et al. 2019 | $86.5 \\%$ | - |\n| GC-Conv Zhang et al. 2020. | $89.0 \\%$ | - |\n| VN-PointNet (Deng et al. 2021 | $77.2 \\%$ | 2.20 M |\n| VN-DGCNN (Deng et al., 2021 | $90.0 \\%$ | 2.00 M |\n| VN-Transformer (ours) | $90.8 \\%$ | 0.04 M |\n\nWe evaluate our VN-Transformer classifier on the commonly used ModelNet40 dataset (Wu et al., 2015), a 40-class point-cloud classification problem. In Table 1 we compare our model with recent rotation-invariant models. The VN-Transformer outperforms the baseline VN models with orders of magnitude fewer parameters. Furthermore, we dispense with the computationally expensive edge-convolution used as a preprocessing step\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-09.jpg?height=497&width=749&top_left_y=283&top_left_x=273)\n\nFigure 6: Example point-clouds from the ModelNet40 Polka-dot dataset. Cyan points correspond to $a_{i}=0$, and pink points correspond to $a_{i}=1$. Note that the \"airplane\" class has a narrower polka-dot radius than the \"table\" class, since we make the polka-dot radius dependent on the object class. ![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-09.jpg?height=495&width=741&top_left_y=281&top_left_x=1104)\n\nFigure 7: VN-Transformer ModelNet40 accuracy and relative training speed vs. number of nearest neighbors used in edge-convolution preprocessing. Speed is computed relative to zero neighbors (i.e., no edgeconvolution). Edge-convolution slows training speed by $\\sim 5 \\mathrm{x}$ and has little effect on model performance. in the models of Deng et al. (2021) and find that the VN-Transformer's performance is relatively unaffected (see Figure 7). ![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-09.jpg?height=494&width=744&top_left_y=1260&top_left_x=254)\n\nFigure 8: Test set accuracy on ModelNet40 vs. number of latent features (described in Figure 5). Latent features provide a $\\sim 2 \\mathrm{x}$ speedup with minimal acc.",
    "vntransformer-12": "degradation (vs. row 3 of Table 2 ). Table 2: Test set accuracy on ModelNet40 \"PolkaDot\" dataset. Top block shows ModelNet40 results (i.e., only spatial inputs). Bottom block shows ModelNet40 Polka-dot results. $\\epsilon$ is the bias norm in VNLinearWithBias of eq. equation 8\n\n| Model | $\\epsilon$ | Fusion | Features | Acc. |\n| :--- | :---: | :--- | :--- | ---: |\n| VN-PointNet | 0 | - | $[x, y, z]$ | $77.2 \\%$ |\n| VN-Transformer | 0 | - | $[x, y, z]$ | $88.5 \\%$ |\n| VN-Transformer | $10^{-6}$ | - | $[x, y, z]$ | $90.8 \\%$ |\n| VN-PointNet | 0 | Early | $[x, y, z, a]$ | $82.0 \\%$ |\n| VN-Transformer | 0 | Early | $[x, y, z, a]$ | $91.1 \\%$ |\n| VN-Transformer | $10^{-6}$ | Early | $[x, y, z, a]$ | $\\mathbf{9 5 . 4 \\%}$ |\n| VN-Transformer | $10^{-6}$ | Late | $[x, y, z, a]$ | $91.0 \\%$ |\n\n### 8.2 Classification with non-spatial attributes\n\nTo evaluate our model's ability to handle non-spatial attributes, we design a modified version of the ModelNet40 dataset, called ModelNet40 \"Polka-dot,\" which we construct (from ModelNet40) as follows: to each point $\\left[x_{i}, y_{i}, z_{i}\\right]$, we append $a_{i} \\in\\{0,1\\}$. Within a radius $r$ of a randomly chosen center, we randomly select 30 points and set $a_{i}=1$. We make the \"polka-dot\" radius $r$ depend on the label $y \\in\\{1, \\ldots, 40\\}$ via $r(y) \\triangleq r_{\\mathrm{lo}}+\\frac{1}{39}(y-1)\\left(r_{\\mathrm{hi}}-r_{\\mathrm{lo}}\\right)$, where $r_{\\mathrm{lo}}=0.3, r_{\\mathrm{hi}}=1$. Figure 6 shows example point-clouds from the ModelNet40 Polka-dot dataset. By generating ModelNet40 Polka-dot in this way, we directly embed class information into the non-spatial attributes. In order to perform well on this task, models need to effectively fuse spatial and non-spatial information (there is no useful information in the non-spatial attributes alone since all point-clouds have $\\sum_{i=1}^{N} a_{i}=30$ ). The results on ModelNet40 Polka-dot are shown in Table 2 Both VN-PointNet and VN-Transformer benefit significantly from the binary polka-dots, suggesting they are able to effectively combine spatial and non-spatial information. ### 8.3 Latent features\n\nFigure 8 shows our results on ModelNet40 when we reduce the number of tokens from 1024 (the number of points in the original point-cloud) to 32 using the latent feature mechanism presented in Figure 5 . Using latent features provides a $\\sim 2 \\mathrm{x}$ latency improvement (in training steps $/ \\mathrm{sec}$ ) with minimal ( $\\sim 1.7 \\%$ ) accuracy degradation (compared with row 3 of Table 2). This suggests a real benefit of the latent feature mechanism in time-sensitive applications such as autonomous driving. ### 8.4 Rotation-equivariant motion forecasting\n\nFigure 1bshows our proposed rotation-equivariant architecture for motion forecasting. In motion forecasting the goal is to predict the $[x, y, z]$ locations of an agent for a sequence of future timesteps, given as input the past locations of the agent. We evaluate the model on a simplified version of the Waymo Open Motion Dataset (WOMD; Ettinger et al., 2021):\n\n- We select 4904 trajectories (3915 for training, 979 for testing). - Each trajectory consists of $91[x, y, z]$ points for a single vehicle sampled at 5 Hz . - We use the first 11 points (the past) as input and we predict the remaining 80 points (the future). We evaluate the quality of our trajectory forecasting models using the Average Distance Error (ADE): $\\operatorname{ADE}\\left(Y_{i}, \\hat{Y}_{i}\\right) \\triangleq \\frac{1}{T} \\sum_{t=1}^{T}\\left\\|Y_{i}^{(t)}-\\hat{Y}_{i}^{(t)}\\right\\|_{2}$, where $T$ is the number of time-steps in the output trajectory and $Y_{i}, \\hat{Y}_{i} \\in \\mathbb{R}^{T \\times 3}$ are the ground-truth trajectory and the predicted trajectory, respectively. Results are shown in Table 3. Adding training-time random rotations about the $z$-axis yields improves the performance of the vanilla Transformer, and the VN-Transformer outperforms the vanilla Transformer (without the need for train-time rotation augmentations, thanks to equivariance). Figure 9 shows example predictions on WOMD. Equivariance violations of the vanilla Transformer models (columns (a) and (b)) are clearly demonstrated here, in contrast with the equivariant VN-Transformer (column (c)). ## 9 Conclusion\n\nIn this paper, we introduced the VN-Transformer, a rotation-equivariant Transformer model based on the Vector Neurons framework. VN-Transformer is a significant step towards building powerful, modular, and easy-to-use models that have appealing equivariance properties for point-cloud data. Limitations of our work include: (i) similar to previous work (Deng et al., 2021; Qi et al., 2017) we assume input data has been mean-centered. This is sensitive to outliers, and prevents us from making single-pass predictions for multi-object problems (we have to independently mean-center each agent first). Similarly, we have not addressed other types of invariance/equivariance (e.g., scale invariance) in this work; (ii) Proposition 4 shows an error bound on the total equivariance violation of the network with $L_{k}$-Lipschitz layers. We know the Lipschitz constants of VN-Linear and VN-LinearWithBias (see Appendix C), but we have not yet determined them for other layers (e.g., VN-ReLU, VN-MultiHeadAttn). We will address these gaps in future\nwork, and we will also leverage VN-Transformers to obtain state-of-the-art performance on a number of key benchmarks such as the full Waymo Open Motion Dataset and ScanObjectNN.",
    "vntransformer-13": "## References\n\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. (Cited on Page 6)\nMichael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veli\u010dkovi\u0107. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. ArXiv, abs/2104.13478, 2021. (Cited on Page 3)\n\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository.",
    "vntransformer-14": "arXiv preprint arXiv:1512.03012, 2015. (Cited on Page 2, 3)\nBenjamin Chidester, Minh N. Do, and Jian Ma. Rotation equivariance and invariance in convolutional neural networks, 2018. (Cited on Page 3)\nTaco Cohen and Max Welling. Group equivariant convolutional networks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 2990-2999, New York, New York, USA, 20-22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/cohenc16.html. (Cited on Page 3)\n\nCongyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas Guibas. Vector neurons: A general framework for so(3)-equivariant networks, 2021.",
    "vntransformer-15": "(Cited on Page 2, $5,6,8,9,10,14,15$ 20.21 . Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. (Cited on Page 4)\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale, 2021. (Cited on Page 4)\n\nCarlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, and Kostas Daniilidis. Polar transformer networks. In International Conference on Learning Representations, 2018. (Cited on Page 23 )\n\nScott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Aur\u00e9lien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset.",
    "vntransformer-16": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9710-9719, October 2021. (Cited on Page 10, 15)\n\nMarc Anton Finzi, Gregory Benton, and Andrew Gordon Wilson. Residual pathway priors for soft equivariance constraints.",
    "vntransformer-17": "In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=k505ekjMzww. (Cited on Page 4)\n\nFabian B. Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-translation equivariant attention networks, 2020. (Cited on Page 2, 4, 6,14 )\n\nGeoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In International conference on learning representations, 2018. (Cited on Page 3)\n\nMax Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in neural information processing systems, 28:2017-2025, 2015. (Cited on Page 2, 33)\n\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver io: A general architecture for structured inputs \\& outputs, 2021. (Cited on Page 7)\n\nSalman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM Computing Surveys, Jan 2022. ISSN 1557-7341. doi: 10.1145/3505244. URL http://dx.doi.org/10.1145/3505244. (Cited on Page 2, 4)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097-1105, 2012. (Cited on Page 3)\n\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations, 2020. (Cited on Page 4)\n\nAlex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds.",
    "vntransformer-18": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12697-12705, 2019. (Cited on Page 3)\n\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks, 2019. (Cited on Page 2)\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. (Cited on Page 4)\n\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. (Cited on Page 15 )\nDiego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector field networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017. (Cited on Page 3. Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation, 2017. (Cited on Page 2, 10 )\n\nCharles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum pointnets for 3d object detection from rgb-d data.",
    "vntransformer-19": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 918-927, 2018. (Cited on Page 3)\n\nNathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds, 2018.",
    "vntransformer-20": "(Cited on Page 2, 4, 8 . Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. (Cited on Page 2, 4, 5, $6,7,15$\n\nBastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology, 2018. (Cited on Page 3)\n\nRui Wang, Robin Walters, and Rose Yu. Approximately equivariant networks for imperfectly symmetric dynamics, 2022. URL https://arxiv.org/abs/2201.11969. (Cited on Page 4)\n\nDaniel Worrall and Gabriel Brostow. Cubenet: Equivariance to 3d rotation and translation.",
    "vntransformer-21": "In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. (Cited on Page 3)\n\nDaniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. (Cited on Page 3)\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes, 2015. (Cited on Page 8, 15)\nBin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-time 3d object detection from point clouds.",
    "vntransformer-22": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 7652-7660, 2018. (Cited on Page 3)\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding, 2020. (Cited on Page 4)\n\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets, 2018. (Cited on Page 2)\n\nZhiyuan Zhang, Binh-Son Hua, David W. Rosen, and Sai-Kit Yeung. Rotation invariant convolutions for 3d point clouds deep learning, 2019. URL https://arxiv.org/abs/1908.06297. (Cited on Page 8)\n\nZhiyuan Zhang, Binh-Son Hua, Wei Chen, Yibin Tian, and Sai-Kit Yeung. Global context aware convolutions for 3d point cloud understanding. In 2020 International Conference on 3D Vision (3DV), pp. 210-219, 2020. doi: 10.1109/3DV50981.2020.00031. (Cited on Page 8)\n\nYin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection.",
    "vntransformer-23": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4490-4499, 2018. (Cited on Page 3)\n\n|  | VN-Transformer (ours) | SE(3)-Transformer (with $\\ell=1$ ) <br> (Fuchs et al., 2020) |\n| :---: | :---: | :---: |\n| Weight shape | $W^{Q} \\in \\mathbb{R}^{C^{\\prime} \\times C}\\left(C, C^{\\prime}=\\#\\right.$ of channels $)$ | $W_{Q}^{11} \\in \\mathbb{R}^{3 \\times 3}\\left(\\in \\mathbb{R}^{\\left(2 \\ell^{\\prime}+1\\right) \\times(2 \\ell+1)}\\right.$ in <br> the general case) |\n| Action on input | Left-multiply: <br> $W^{Q} V^{(n)}\\left(V^{(n)} \\in \\mathbb{R}^{C \\times 3}\\right)$ | Right-multiply: <br> $f_{i}^{1} W_{Q}^{11}\\left(f_{i}^{1} \\in \\mathbb{R}^{1 \\times 3}\\right)$ |\n| Requirement for equiv- <br> ariance | None, $W^{Q} \\in \\mathbb{R}^{C^{\\prime} \\times C}$ is arbitrary | $W_{Q}^{11}=$ <br> $\\sum_{J=0}^{2} \\sum_{m=-J}^{J} \\varphi_{J}^{11}\\left(\\left\\\\|x_{i}\\right\\\\|\\right) Y_{J m}\\left(x_{i} /\\left\\\\|x_{i}\\right\\\\|\\right) Q_{J m}^{11}$, <br> $\\left(\\varphi=\\right.$ radial neural net., $Y_{J m}=$ <br> spherical harmonics, $Q_{J m}^{11}=$ <br> Clebsch-Gordan coefficients) |\n\nTable 4: Attention query computation in VN-Transformer vs. SE(3)-Transformer. $W^{Q} \\in \\mathbb{R}^{C^{\\prime} \\times C}$ is the VN-Transformer weight matrix used to compute the query ( $C$ and $C^{\\prime}$ are the number of input and query channels, respectively). ## A Detailed comparison with SE(3)-Transformer (Fuchs et al., 2020)\n\n## A. 1 Attention computation\n\nThere is a rich literature on equivariant models using steerable kernels, and the $\\mathrm{SE}(3)$-Transformer is the closest development in this field to our work. Here, we make a detailed comparison between our work and the SE(3)-Transformer (and related steerable kernel-based models). For simplicity, we will compare the VN-Transformer with only spatial features vs. the $\\mathrm{SE}(3)$-Transformer with only type-1 features (i.e., spatial features).",
    "vntransformer-24": "The key difference is in the way the weight matrices are defined and how they interact with the input. Specifically, Fuchs et al. use $3 \\times 3$ weight matrices $W_{Q}^{11}, W_{K}^{11}, W_{V}^{11} \\in \\mathbb{R}^{3 \\times 3}$ that act on the spatial/representation dimension of the input points (e.g., via $f_{i}^{1} W_{Q}^{11}$ where $\\left.f_{i}^{1} \\in \\mathbb{R}^{1 \\times 3}\\right) \\underbrace{2}$ As a result, in order to guarantee equivariance they need to design these matrices $W_{Q}^{11}, W_{K}^{11}, W_{V}^{11}$ such that they each commute with a rotation operation (in general $\\left(f_{i}^{1} R\\right) W_{Q}^{11} \\neq\\left(f_{i}^{1} W_{Q}^{11}\\right) R$ - this depends on the choice of $\\left.W_{Q}^{11}\\right)$, hence the need for the machinery of Clebsch-Gordan coefficients, spherical harmonics, and radial neural nets to construct the weights. In contrast, in our proposed attention mechanism, the matrices $W^{Q}, W^{K}, W^{Z} \\in \\mathbb{R}^{C^{\\prime} \\times C}$ act on the channel dimension of the input (e.g., via $W^{Q} V^{(n)}$, where $V^{(n)} \\in \\mathbb{R}^{C \\times 3}$ ) and not the spatial dimension. As a result, the operations $W^{Q} V^{(n)}, W^{K} V^{(n)}, W^{Z} V^{(n)}$ are equivariant no matter the choice of $W^{Q}, W^{K}, W^{Z}$, since $W\\left(V^{(n)} R\\right)=\\left(W V^{(n)}\\right) R$. This results in a significantly simpler construction of rotation-equivariant attention that is $(i)$ accessible to a wider audience (i.e., it does not require an understanding of group theory, representation theory, spherical harmonics, Clebsch-Gordan coefficients, etc.) and (ii) much easier to implement. For a side-by-side comparison of both attention query computations, see Table 4 above.",
    "vntransformer-25": "## A. 2 VN-Linear vs. SE(3)-Transformer \"self-interaction\"\n\nThere is a relationship between the VN-Linear operation of Deng et al. (2021), and the \"linear self-interaction\" layers of Fuchs et al.",
    "vntransformer-26": "(2020). Comparing equation (12) of Fuchs et al. (2020), repeated here for convenience:\n\n$$\n\\mathbf{f}_{\\text {out }, i, c^{\\prime}}^{\\ell}=\\sum_{c} w_{c^{\\prime} c}^{\\ell \\ell} \\mathbf{f}_{\\mathrm{in}, i, c}^{\\ell}\n$$\n\n[^2]with the VN-Linear operation of Deng et al. (2021):\n$$\nV_{\\text {out }}^{(n)}=W V^{(n)}, \\quad W \\in \\mathbb{R}^{C^{\\prime} \\times C}, V^{(n)} \\in \\mathbb{R}^{C \\times 3}\n$$\nwe see that these operations are identical. However, our proposed VN-MultiHeadAttention is different and significantly simpler than the attention mechanism of the $\\mathrm{SE}(3)$-Transformer (see Section A.1), as it relies only on $(i)$ the rotation-invariant Frobenius inner product and (ii) straightforward multiplication by an arbitary weight matrix to compute the keys, queries, and values (equivalent to VN-Linear/linear self-interaction). In that sense, it is closer in spirit to the original Transformer of Vaswani et al. (replacing vector inner product with Frobenius inner product). Further, as explained previously, it does not require the special construction of weight matrices using Clebsch-Gordan coefficients, spherical harmonics, and radial neural networks as in the $\\mathrm{SE}(3)$-Transformer. ## B Experimental details\n\n## B. 1 Datasets\n\nModelNet40 The ModelNet40 dataset (Wu et al. 2015) is publicly available at https://modelnet.cs.princeton.edu, with the following comment under \"Copyright\":\n\"All CAD models are downloaded from the Internet and the original authors hold the copyright of the CAD models. The label of the data was obtained by us via Amazon Mechanical Turk service and it is provided freely. This dataset is provided for the convenience of academic research only.\"\n\nWaymo Open Motion Dataset The Waymo Open Motion Dataset (Ettinger et al., 2021) is publicly available at https://waymo.com/open/data/motion/ under a non-commercial use license agreement. Full license details can be found here: https://waymo.com/open/terms/. ## B. 2 Hyperparameter tuning\n\nTable 5 shows the hyperparameters we swept over for all our experiments on ModelNet40, ModelNet40 Polka-dot, and the Waymo Open Motion Dataset. | Hyperparameter | Value/Range |\n| :--- | :--- |\n| Feature dimension of VN-Transformer | $\\{32,64,128,256,512,1024\\}$ |\n| Number of attention heads | $\\{4,8,16,32,64,128\\}$ |\n| Hidden layer dimension in encoder's VN-MLP | $\\{32,64,128,256,512\\}$ |\n| Learning rate | $10^{-3}$ |\n| Learning rate schedule | Linear decay |\n| Optimizer | AdamW (Loshchilov \\& Hutter, 2019 |\n| Epochs | 4000 |\n| $\\epsilon$ of VN-LinearWithBias | $\\left\\{0,10^{-6}\\right\\}$ |\n\nTable 5: Model hyperparameter ranges for ModelNet40, ModelNet40 Polka-dot, and Waymo Open Motion Dataset.",
    "vntransformer-27": "## B. 3 Compute infrastructure\n\nWe trained our models on TPU-v3 devices. which are accessible through Google Cloud. Our longest training jobs ran for less than 3 hours on 32 TPU cores. ## C Proofs\n\nIn this section, for convenience we will treat all 3 D vectors as row-vectors: e.g., $x \\in \\mathbb{R}^{1 \\times 3}$. We also note that, while all our proofs of invariance/equivariance use matrices $V^{(n)} \\in \\mathbb{R}^{C \\times 3}$, they can all be trivially generalized to $V^{(n)} \\in \\mathbb{R}^{C \\times S}$. ## C. 1 Partial invariance \\& equivariance\n\nAssuming the input point-cloud consists of spatial inputs $X \\in \\mathcal{X} \\subset \\mathbb{R}^{N \\times 3}$ and associated non-spatial attributes $A \\in \\mathcal{A} \\subset \\mathbb{R}^{N \\times d_{A}}$ ( $d_{A}$ is the number of non-spatial attributes associated with each point), we would like our model $f: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{Y}$ to satisfy the following property:\nDefinition 4 (Partial rotation invariance). A model $f: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{Y}$ satisfies partial rotation invariance if $\\forall X \\in \\mathcal{X}, A \\in \\mathcal{A}, R \\in \\mathrm{SO}(3), \\quad f(X R, A)=f(X, A)$. Definition 5 (Partial rotation equivariance). A model $f: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{Y}$ (where $\\mathcal{Y} \\subset \\mathbb{R}^{N_{\\text {out }} \\times\\left(3+d_{A}\\right)}$ ) satisfies partial rotation equivariance if $\\forall X \\in \\mathcal{X}, A \\in \\mathcal{A}, R \\in \\mathrm{SO}(3), \\quad f(X R, A)^{(:,: 3)}=f(X, A)^{(:,: 3)} R$. We show here that the models in Figure 1 satisfy partial rotation invariance and equivariance (respectively). Consider two rotation matrices $R_{d_{1} \\times d_{1}} \\in \\mathrm{SO}\\left(d_{1}\\right)$ and $R_{d_{2} \\times d_{2}} \\in \\mathrm{SO}\\left(d_{2}\\right)$. Lemma 1. The matrix $R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)} \\triangleq\\left[\\begin{array}{ll}R_{d_{1} \\times d_{1}} & \\mathbf{0}_{d_{1} \\times d_{2}} \\\\ \\mathbf{0}_{d_{2} \\times d_{1}} & R_{d_{2} \\times d_{2}}\\end{array}\\right]$ is a valid rotation matrix in $\\mathrm{SO}\\left(d_{1}+d_{2}\\right)$. Proof. We begin by showing that $R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}^{\\top}=R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}^{-1}$. We first compute $R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}^{\\top}$ :\n\n$$\nR_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}^{\\boldsymbol{\\top}}=\\left[\\begin{array}{ll}\nR_{d_{1} \\times d_{1}} & \\mathbf{0}_{d_{1} \\times d_{2}} \\\\\n\\mathbf{0}_{d_{2} \\times d_{1}} & R_{d_{2} \\times d_{2}}\n\\end{array}\\right]^{\\top}=\\left[\\begin{array}{ll}\nR_{d_{1} \\times d_{1}}^{\\top} & \\mathbf{0}_{d_{2} \\times d_{1}}^{\\top} \\\\\n\\mathbf{0}_{d_{1} \\times d_{2}}^{\\top} & R_{d_{2} \\times d_{2}}^{\\top}\n\\end{array}\\right] \\stackrel{(*)}{=}\\left[\\begin{array}{ll}\nR_{d_{1} \\times d_{1}}^{-1} & \\mathbf{0}_{d_{1} \\times d_{2}} \\\\\n\\mathbf{0}_{d_{2} \\times d_{1}} & R_{d_{2} \\times d_{2}}^{-1}\n\\end{array}\\right]\n$$\n\nwhere ( $*$ ) holds since $R_{d_{1} \\times d_{1}} \\in \\mathrm{SO}\\left(d_{1}\\right)$ and $R_{d_{2} \\times d_{2}} \\in \\mathrm{SO}\\left(d_{2}\\right)$ by assumption. Now, we compute $R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}^{-1}$ :\n\n$$\n\\begin{aligned}\n& R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}^{-1}=\\left[\\begin{array}{ll}\nR_{d_{1} \\times d_{1}} & \\mathbf{0}_{d_{1} \\times d_{2}} \\\\\n\\mathbf{0}_{d_{2} \\times d_{1}} & R_{d_{2} \\times d_{2}}\n\\end{array}\\right]^{-1} \\\\\n& =\\left[\\begin{array}{cc}\n{\\left[R_{d_{1} \\times d_{1}}-0_{d_{1} \\times d_{2}} R_{d_{2} \\times d_{2}}^{-1} 0_{d_{2} \\times d_{1}}\\right]^{-1}} & \\mathbf{0}_{d_{1} \\times d_{2}} \\\\\n\\mathbf{0}_{d_{2} \\times d_{1}} & {\\left[R_{d_{2} \\times d_{2}}-0_{d_{2} \\times d_{1}} R_{d_{1} \\times d_{1}}^{-1} 0_{d_{1} \\times d_{2}}\\right]^{-1}}\n\\end{array}\\right] \\\\\n& =\\left[\\begin{array}{ll}\nR_{d_{1} \\times d_{1}}^{-1} & \\mathbf{0}_{d_{1} \\times d_{2}} \\\\\n\\mathbf{0}_{d_{2} \\times d_{1}} & R_{d_{2} \\times d_{2}}^{-1}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nHence, we have that $R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}^{-1}=R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}^{\\top}$. Finally, we show that $\\operatorname{det}\\left(R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}\\right)=1$ :\n\n$$\n\\operatorname{det}\\left(R_{\\left(d_{1}+d_{2}\\right) \\times\\left(d_{1}+d_{2}\\right)}\\right)=\\operatorname{det}\\left(\\left[\\begin{array}{ll}\nR_{d_{1} \\times d_{1}} & \\mathbf{0}_{d_{1} \\times d_{2}} \\\\\n\\mathbf{0}_{d_{2} \\times d_{1}} & R_{d_{2} \\times d_{2}}\n\\end{array}\\right]\\right)=\\operatorname{det}\\left(R_{d_{1} \\times d_{1}}\\right) \\operatorname{det}\\left(R_{d_{2} \\times d_{2}}\\right) \\stackrel{(*)}{=} 1 \\cdot 1=1\n$$\n\nwhere $(*)$ holds since $R_{d_{1} \\times d_{1}} \\in \\mathrm{SO}\\left(d_{1}\\right)$ and $R_{d_{2} \\times d_{2}} \\in \\mathrm{SO}\\left(d_{2}\\right)$ by assumption. Proposition 5. The VN-Transformer model $f: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{Y}\\left(\\right.$ where $\\left.\\mathcal{Y} \\subset \\mathbb{R}^{\\kappa}\\right)$ shown in Figure 1a satisfies partial rotation invariance. Proof. - For convenience, we reparametrize the model $f$ as $f_{\\text {concat }}: \\mathbb{R}^{N \\times\\left(3+d_{A}\\right)} \\rightarrow \\mathbb{R}^{\\kappa}$ (with $\\kappa$ the number of object classes) where $f_{\\text {concat }}([X, A])=f(X, A)$. It then suffices to show that $f_{\\text {concat }}([X R, A])=$ $f_{\\text {concat }}([X, A]) R$. - First, note that $f_{\\text {concat }}$ is $\\mathrm{SO}\\left(3+d_{A}\\right)$-invariant, since it is composed of $\\mathrm{SO}\\left(3+d_{A}\\right)$-equivariant operations followed by a $\\mathrm{SO}\\left(3+d_{A}\\right)$-invariant operation. - Consider the matrix $R_{\\left(3+d_{A}\\right) \\times\\left(3+d_{A}\\right)} \\triangleq\\left[\\begin{array}{cc}R & \\mathbf{0}_{3 \\times d_{A}} \\\\ \\mathbf{0}_{d_{A} \\times 3} & I_{d_{A} \\times d_{A}}\\end{array}\\right]$, where $R \\in S O(3)$ is an arbitrary 3dimensional rotation. From Lemma 1 , $R_{\\left(3+d_{A}\\right) \\times\\left(3+d_{A}\\right)} \\in S O\\left(3+d_{A}\\right)$. $$\n\\begin{aligned}\n& f_{\\text {concat }}\\left([X, A] R_{\\left(3+d_{A}\\right) \\times\\left(3+d_{A}\\right)}\\right) \\stackrel{(*)}{=} f_{\\text {concat }}([X, A]) \\\\\n\\Rightarrow & f_{\\text {concat }}\\left(\\left[X R+A \\mathbf{0}_{d_{A} \\times 3}, X \\mathbf{0}_{3 \\times d_{A}}+A I_{d_{A} \\times d_{A}}\\right]\\right)=f_{\\text {concat }}([X, A]) \\\\\n\\Rightarrow & f_{\\text {concat }}([X R, A])=f_{\\text {concat }}([X, A])\n\\end{aligned}\n$$\n\nwhere $(*)$ holds from $\\mathrm{SO}\\left(3+d_{A}\\right)$-invariance of $f_{\\text {concat }}$. Proposition 6. The VN-Transformer model $f: \\mathcal{X} \\times \\mathcal{A} \\rightarrow \\mathcal{Y}\\left(\\right.$ where $\\left.\\mathcal{Y} \\subset \\mathbb{R}^{N_{\\text {out }} \\times\\left(3+d_{A}\\right)}\\right)$ shown in Figure $1 b$ satisfies partial rotation equivariance. Proof. - For convenience, we reparametrize the model $f$ as $f_{\\text {concat }}: \\mathbb{R}^{N \\times\\left(3+d_{A}\\right)} \\rightarrow \\mathbb{R}^{N \\text { out } \\times\\left(3+d_{A}\\right)}$ where $f_{\\text {concat }}([X, A])=f(X, A)$. It then suffices to show that $f_{\\text {concat }}([X R, A])^{(:,: 3)}=f_{\\text {concat }}([X, A])^{(,:, 3)} R$. - First, note that $f_{\\text {concat }}$ is $\\mathrm{SO}\\left(3+d_{A}\\right)$-equivariant, since it is composed of $\\mathrm{SO}\\left(3+d_{A}\\right)$-equivariant operations. - Consider the matrix $R_{\\left(3+d_{A}\\right) \\times\\left(3+d_{A}\\right)} \\triangleq\\left[\\begin{array}{cc}R & \\mathbf{0}_{3 \\times d_{A}} \\\\ \\mathbf{0}_{d_{A} \\times 3} & I_{d_{A} \\times d_{A}}\\end{array}\\right]$, where $R \\in S O(3)$ is an arbitrary 3dimensional rotation. From Lemma 1 , $R_{\\left(3+d_{A}\\right) \\times\\left(3+d_{A}\\right)} \\in S O\\left(3+d_{A}\\right)$. $$\n\\begin{aligned}\n& f_{\\text {concat }}\\left([X, A] R_{\\left(3+d_{A}\\right) \\times\\left(3+d_{A}\\right)}\\right) \\stackrel{(*)}{=} f_{\\text {concat }}([X, A]) R_{\\left(3+d_{A}\\right) \\times\\left(3+d_{A}\\right)} \\\\\n\\Rightarrow & f_{\\text {concat }}\\left(\\left[X R+A \\mathbf{0}_{d_{A} \\times 3}, X \\mathbf{0}_{3 \\times d_{A}}+A I_{d_{A} \\times d_{A}}\\right]\\right) \\\\\n& =\\left[f_{\\text {concat }}([X, A])^{(:,: 3)} R+f_{\\text {concat }}([X, A])^{(:, 4)} \\mathbf{0}_{d_{A} \\times 3}, f_{\\text {concat }}([X, A])^{(:,: 3)} \\mathbf{0}_{3 \\times d_{A}}+f_{\\text {concat }}([X, A])^{(:, 4:)} \\mathbf{I}_{d_{A} \\times d_{A}}\\right] \\\\\n\\Rightarrow & f_{\\text {concat }}([X R, A])=\\left[f_{\\text {concat }}([X, A])^{(:,: 3)} R, f_{\\text {concat }}([X, A])^{(:, 4:)}\\right] \\\\\n\\Rightarrow & f_{\\text {concat }}([X R, A])^{(:,: 3)}=f_{\\text {concat }}([X, A])^{(:,: 3)} R\n\\end{aligned}\n$$\n\nwhere $(*)$ holds from $\\mathrm{SO}\\left(3+d_{A}\\right)$-equivariance of $f_{\\text {concat }}$.",
    "vntransformer-28": "## C. $2 \\epsilon$-approximate equivariance\n\nProposition 3 (Restated). VN-LinearWithBias $(\\cdot ; W, U, \\epsilon)$ is $\\left(2 \\epsilon \\sqrt{C^{\\prime}}\\right)$-approximately equivariant. This bound is tight when $R=-I_{3 \\times 3}$. Proof. Set $f \\triangleq$ VN-LinearWithBias $(\\cdot ; W, U, \\epsilon)$ :\n\n$$\n\\begin{aligned}\nf(X R)-f(X) R & =(W X R+\\epsilon U)-(W X+\\epsilon U) R \\\\\n& =\\epsilon U-\\epsilon U R \\\\\n\\Rightarrow \\Delta(f, X, R)^{2} & =\\|f(X R)-f(X) R\\|_{F}^{2} \\\\\n& =\\sum_{c=1}^{C^{\\prime}}\\left\\|\\epsilon\\left(U^{(c)}-U^{(c)} R\\right)\\right\\|_{2}^{2} \\\\\n& =\\epsilon^{2} \\sum_{c=1}^{C^{\\prime}}\\left\\|U^{(c)}-U^{(c)} R\\right\\|_{2}^{2} \\\\\n& =\\epsilon^{2} \\sum_{c=1}^{C^{\\prime}}\\left\\|U^{(c)}\\right\\|_{2}^{2}+\\left\\|U^{(c)} R\\right\\|_{2}^{2}-2 U^{(c)} R^{\\boldsymbol{\\top}} U^{(c) \\top} \\\\\n& \\leq \\epsilon^{2} \\sum_{c=1}^{C^{\\prime}}\\left\\|U^{(c)}\\right\\|_{2}^{2}+\\left\\|U^{(c)} R\\right\\|_{2}^{2}+2 U^{(c)} U^{(c) \\top} \\\\\n& =\\epsilon^{2} \\sum_{c=1}^{C^{\\prime}} 4\\left\\|U^{(c)}\\right\\|_{2}^{2} \\\\\n& =4 \\epsilon^{2} C^{\\prime} \\\\\n\\Rightarrow \\Delta(f, X, R) & \\leq 2 \\epsilon \\sqrt{C^{\\prime}}\n\\end{aligned}\n$$\n\n## Lemma 2. Suppose\n\n1. $f: \\mathcal{X}_{1} \\rightarrow \\mathcal{X}_{2}$ (with $\\mathcal{X}_{1} \\subset \\mathbb{R}^{C_{1} \\times 3}, \\mathcal{X}_{2} \\subset \\mathbb{R}^{C_{2} \\times 3}$ ) is $\\epsilon_{f}$-approximately equivariant. 2. $g: \\mathcal{X}_{2} \\rightarrow \\mathcal{X}_{3}$ (with $\\mathcal{X}_{3} \\subset \\mathbb{R}^{C_{3} \\times 3}$ ) is $\\epsilon_{g}$-approximately equivariant and $L_{g}$-Lipschitz (w.r.t. the Frobenius norm). Then the composition $g \\circ f: \\mathcal{X}_{1} \\rightarrow \\mathcal{X}_{3}$ is $\\left(L_{g} \\epsilon_{f}+\\epsilon_{g}\\right)$-approximately equivariant. Proof. $$\n\\begin{aligned}\n\\Delta(g \\circ f, X, R) & =\\|g(f(X R))-g(f(X)) R\\|_{F} \\\\\n& =\\|g(f(X R))-g(f(X) R)+g(f(X) R)-g(f(X)) R\\|_{F} \\\\\n& \\leq\\|g(f(X R))-g(f(X) R)\\|_{F}+\\|g(f(X) R)-g(f(X)) R\\|_{F} \\\\\n& \\stackrel{(*)}{\\leq}\\|g(f(X R))-g(f(X) R)\\|_{F}+\\epsilon_{g} \\\\\n& \\stackrel{(* *)}{\\leq} L_{g}\\|f(X R)-f(X) R\\|_{F}+\\epsilon_{g} \\\\\n& =L_{g} \\Delta(f, X, R)+\\epsilon_{g} \\\\\n& \\stackrel{(* * *)}{\\leq} L_{g} \\epsilon_{f}+\\epsilon_{g}\n\\end{aligned}\n$$\n\nwhere ( $(*)$ holds from $\\epsilon_{g}$-approximate equivariance of $g,(* *)$ holds because $g$ is $L_{g}$-Lipschitz, and $(* * *)$ holds from $\\epsilon_{f}$-approximate equivariance of $f$. Proposition 4 (Restated). Suppose we have $K$ functions $f_{k}: \\mathcal{X}_{k} \\rightarrow \\mathcal{X}_{k+1}$ (with $\\mathcal{X}_{k} \\subset \\mathbb{R}^{C_{k} \\times 3}, \\mathcal{X}_{k+1} \\subset$ $\\mathbb{R}^{C_{k+1} \\times 3}$ ) for $k \\in\\{1, \\ldots, K\\}$, satisfying the following:\n\n1. $f_{k}$ is $\\epsilon_{k}$-approximately equivariant for all $k \\in\\{1, \\ldots, K\\}$. 2. $f_{k}$ is $L_{k}$-Lipschitz (w.r.t. $\\|\\cdot\\|_{F}$ ) for all $k \\in\\{2, \\ldots, K\\}$. ![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-19.jpg?height=313&width=1161&top_left_y=272&top_left_x=487)\n\nFigure 10: Violations of equivariance in a neural network with $L_{k}$-Lipschitz and $\\epsilon_{k}$-approximately equivariant layers. Then, the composition $f_{K} \\circ \\cdots \\circ f_{1}$ is $\\epsilon_{1 \\ldots K}$-approximately equivariant, where:\n\n$$\n\\epsilon_{1 \\ldots K} \\triangleq L_{K}\\left(\\cdots\\left(L_{3}\\left(L_{2} \\epsilon_{1}+\\epsilon_{2}\\right)+\\epsilon_{3}\\right)+\\cdots\\right)+\\epsilon_{K}\n$$\n\nProof. $$\n\\begin{aligned}\n\\Delta\\left(f_{K} \\circ \\cdots \\circ f_{1}, X, R\\right)= & \\Delta\\left(f_{K} \\circ\\left(f_{K-1} \\circ \\cdots \\circ f_{1}\\right), X, R\\right) \\\\\n& \\stackrel{(K)}{\\leq} L_{K} \\Delta\\left(f_{K-1} \\circ \\cdots \\circ f_{1}, X, R\\right)+\\epsilon_{K} \\\\\n& (K-1) \\\\\n& \\leq L_{K}\\left(L_{K-1}\\left(\\Delta\\left(f_{K-2} \\circ \\cdots \\circ f_{1}, X, R\\right)+\\epsilon_{K-1}\\right)+\\epsilon_{K}\\right. \\\\\n& \\vdots \\\\\n& \\stackrel{(1)}{\\leq} L_{K}\\left(\\cdots\\left(L_{3}\\left(L_{2} \\Delta\\left(f_{1}, X, R\\right)+\\epsilon_{2}\\right)+\\epsilon_{3}\\right)+\\cdots\\right)+\\epsilon_{K} \\\\\n& \\leq L_{K}\\left(\\cdots\\left(L_{3}\\left(L_{2} \\epsilon_{1}+\\epsilon_{2}\\right)+\\epsilon_{3}\\right)+\\cdots\\right)+\\epsilon_{K}\n\\end{aligned}\n$$\n\nwhere $(K)-(1)$ hold from applying inequality equation 40 from Lemma $2 K$ times (setting $g \\triangleq f_{k}$ and $f \\triangleq f_{k-1} \\circ \\ldots f_{1}$ at each step $)$. Figure 10 illustrates the propagation of equivariance violations through a composition of 3 functions. Proposition 7. The VN-Linear $(\\cdot ; W): \\mathbb{R}^{C \\times S} \\rightarrow \\mathbb{R}^{C^{\\prime} \\times S}$ layer is $\\sigma(W)$-Lipschitz w.r.t. the Frobenius norm, where $\\sigma(W)$ is the spectral norm of $W$. The same holds for VN-LinearWithBias $(\\cdot ; W, U, \\epsilon)$. Proof. Consider $X_{1}, X_{2} \\in \\mathbb{R}^{C \\times S}$. We can write:\n\n$$\n\\begin{aligned}\n\\left\\|W X_{1}-W X_{2}\\right\\|_{F}^{2} & =\\sum_{s=1}^{S}\\left\\|W X_{1}^{(:, s)}-W X_{2}^{(:, s)}\\right\\|_{2}^{2} \\\\\n& =\\sum_{s=1}^{S}\\left\\|W\\left(X_{1}^{(:, s)}-X_{2}^{(:, s)}\\right)\\right\\|_{2}^{2} \\\\\n& \\leq \\sum_{s=1}^{S} \\sigma^{2}(W)\\left\\|X_{1}^{(:, s)}-X_{2}^{(:, s)}\\right\\|_{2}^{2} \\\\\n& =\\sigma(W)^{2} \\sum_{s=1}^{S}\\left\\|X_{1}^{(:, s)}-X_{2}^{(:, s)}\\right\\|_{2}^{2} \\\\\n& =\\sigma(W)^{2}\\left\\|X_{1}-X_{2}\\right\\|_{F}^{2} \\\\\n\\Rightarrow\\left\\|W X_{1}-W X_{2}\\right\\|_{F} & \\leq \\sigma(W)\\left\\|X_{1}-X_{2}\\right\\|_{F}\n\\end{aligned}\n$$\n\nTo see this for VN-LinearWithBias, note that $\\left\\|\\left(W X_{1}+\\epsilon U\\right)-\\left(W X_{2}+\\epsilon U\\right)\\right\\|_{F}^{2}=\\left\\|W X_{1}-W X_{2}\\right\\|_{F}^{2}-$ we can then show the same result using the above proof. ## C. 3 Equivariance of VN-LayerNorm\n\nWe define the VN analog of the layer normalization operation as follows:\n\n$$\n\\operatorname{VN-LayerNorm}\\left(V^{(n)}\\right) \\triangleq\\left[\\frac{V^{(n, c)}}{\\left\\|V^{(n, c)}\\right\\|_{2}}\\right]_{c=1}^{C} \\odot \\operatorname{LayerNorm}\\left(\\left[\\left\\|V^{(n, c)}\\right\\|_{2}\\right]_{c=1}^{C}\\right) \\mathbb{1}_{1 \\times 3}\n$$\n\nProposition 8. VN-LayerNorm : $\\mathbb{R}^{C \\times 3} \\rightarrow \\mathbb{R}^{C \\times 3}$ is rotation-equivariant. Proof. $$\n\\begin{aligned}\n\\operatorname{VN}-\\operatorname{LayerNorm}\\left(V^{(n)} R\\right)^{(c)} & =\\frac{V^{(n, c)} R}{\\left\\|V^{(n, c)} R\\right\\|_{2}} \\operatorname{LayerNorm}\\left(\\left[\\left\\|V^{(n, c)} R\\right\\|_{2}\\right]_{c^{\\prime}=1}^{C}\\right)^{(c)} \\\\\n& \\stackrel{(*)}{=} \\frac{V^{(n, c)} R}{\\left\\|V^{(n, c)}\\right\\|_{2}} \\operatorname{LayerNorm}\\left(\\left[\\left\\|V^{(n, c)}\\right\\|_{2}\\right]_{c^{\\prime}=1}^{C}\\right)^{(c)} \\\\\n& =\\left[\\frac{V^{(n, c)}}{\\left\\|V^{(n, c)}\\right\\|_{2}} \\operatorname{LayerNorm}\\left(\\left[\\left\\|V^{(n, c)}\\right\\|_{2}\\right]_{c^{\\prime}=1}^{C}\\right)^{(c)}\\right] R \\\\\n& =\\text { VN-LayerNorm }\\left(V^{(n)}\\right)^{(c)} R \\\\\n& =\\left[\\operatorname{VN}-\\operatorname{LayerNorm}\\left(V^{(n)}\\right) R\\right]^{(c)} \\\\\n\\Rightarrow \\operatorname{VN}-\\operatorname{LayerNorm}\\left(V^{(n)} R\\right) & =\\text { VN-LayerNorm }\\left(V^{(n)}\\right) R\n\\end{aligned}\n$$\n\nwhere $(*)$ holds from invariance of vector norms to rotations. ## C. 4 Definitions of VN layers from Deng et al. (2021)\n\nVN-ReLU layer The VN-ReLU layer is constructed as follows: from a given representation $V^{(n)} \\in \\mathbb{R}^{C \\times 3}$, we compute a feature set $q \\in \\mathbb{R}^{C \\times 3}$ :\n\n$$\nq \\triangleq W V^{(n)}, \\quad W \\in \\mathbb{R}^{C \\times C}\n$$\n\nThen, we compute a set of $C$ \"learnable directions\" $k \\in \\mathbb{R}^{C \\times 3}$ :\n\n$$\nk \\triangleq U V^{(n)}, \\quad U \\in \\mathbb{R}^{C \\times C}\n$$\n\nNote that $W, U$ are learnable square matrices. Finally, we compute the output of the VN-ReLU operation $\\operatorname{VN-ReLU}(\\cdot ; W, U): \\mathbb{R}^{C \\times 3} \\rightarrow \\mathbb{R}^{C \\times 3}$ as follows:\n\n$$\n\\mathrm{VN}-\\operatorname{ReLU}\\left(V^{(n)}\\right)^{(c)} \\triangleq\\left\\{\\begin{array}{l}\nq^{(c)} \\quad \\text { if }\\left\\langle q^{(c)}, k^{(c)}\\right\\rangle \\geq 0 \\\\\nq^{(c)}-\\left\\langle q^{(c)}, \\frac{k^{(c)}}{\\left\\|k^{(c)}\\right\\|}\\right\\rangle \\frac{k^{(c)}}{\\left\\|k^{(c)}\\right\\|} \\text { o.w. }\n\\end{array}\\right. $$\n\nOtherwise stated: if the inner product between the feature $q^{(c)}$ and the learnable direction $k^{(c)}$ is positive, return $q^{(c)}$, else return the projection of $q^{(c)}$ onto the plane defined by the direction $k^{(c)}$. It can be readily shown that VN-ReLU is rotation-equivariant (for a proof, see Appendix C.5). VN-Invariant layer VN-Invariant $(\\cdot ; W): \\mathbb{R}^{C \\times 3} \\rightarrow \\mathbb{R}^{C \\times 3}$ is defined as:\n\n$$\n\\operatorname{VN-Invariant}\\left(V^{(n)} ; W\\right) \\triangleq V^{(n)} \\operatorname{VN}-\\operatorname{MLP}\\left(V^{(n)} ; W\\right)^{\\top}\n$$\n\nwhere $\\operatorname{VN}-\\operatorname{MLP}(\\cdot ; W): \\mathbb{R}^{C \\times 3} \\rightarrow \\mathbb{R}^{3 \\times 3}$ is a composition of VN-Linear and VN-ReLU layers, and $W$ is the set of all learnable parameters in VN-MLP. It can be easily shown that VN-Invariant is rotation-invariant (see Appendix C.5 for a proof). VN-Batch Norm, VN-Pool For rotation-equivariant analogs of the standard batch norm and pooling operations, we point the reader to Deng et al. (2021). ## C. 5 Invariance \\& equivariance of VN layers of Deng et al. (2021)\n\nProposition 9. Deng et al., 2021) VN-Linear $\\cdot ; W): \\mathbb{R}^{C \\times 3} \\rightarrow \\mathbb{R}^{C^{\\prime} \\times 3}$ is rotation-equivariant. Proof. $$\n\\operatorname{VN-Linear}\\left(V^{(n)} R ; W\\right) \\triangleq W V^{(n)} R=\\left(W V^{(n)}\\right) R=\\operatorname{VN-Linear}\\left(V^{(n)} ; W\\right) R\n$$\n\nProposition 10. Deng et al., 2021) VN-ReLU : $\\mathbb{R}^{C \\times 3} \\rightarrow \\mathbb{R}^{C \\times 3}$ is rotation-equivariant. Proof. $$\n\\begin{aligned}\n& \\mathrm{VN}-\\operatorname{ReLU}\\left(V^{(n)} R\\right)^{(c)} \\stackrel{(*)}{=}\\left\\{\\begin{array}{l}\nq^{(c)} R \\quad \\text { if }\\left\\langle q^{(c)} R, k^{(c)} R\\right\\rangle \\geq 0 \\\\\nq^{(c)} R-\\left\\langle q^{(c)} R, \\frac{k^{(c)} R}{\\left\\|k^{(c)} R\\right\\|_{2}}\\right\\rangle \\frac{k^{(c)} R}{\\left\\|k^{(c)} R\\right\\|_{2}} \\quad \\text { o.w. }\n\\end{array}\\right. \\\\\n& \\stackrel{(* *)}{=} \\begin{cases}q^{(c)} R & \\text { if }\\left\\langle q^{(c)}, k^{(c)}\\right\\rangle \\geq 0 \\\\\nq^{(c)} R-\\left\\langle q^{(c)}, \\frac{k^{(c)}}{\\left\\|k^{(c)}\\right\\|_{2}}\\right\\rangle \\frac{k^{(c)} R}{\\left\\|k^{(c)}\\right\\|_{2}} \\quad \\text { o.w. }\\end{cases}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& =\\mathrm{VN}-\\operatorname{ReLU}\\left(V^{(n)}\\right)^{(c)} R \\\\\n& =\\left[\\mathrm{VN}-\\operatorname{ReLU}\\left(V^{(n)}\\right) R\\right]^{(c)} \\\\\n& \\Rightarrow \\operatorname{VN}-\\operatorname{ReLU}\\left(V^{(n)} R\\right)=\\operatorname{VN}-\\operatorname{ReLU}\\left(V^{(n)}\\right) R \\text {, }\n\\end{aligned}\n$$\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0432fab30c4e3356510bg-21.jpg?height=126&width=673&top_left_y=1059&top_left_x=864)\nwhere $(*)$ holds because $q$ and $k$ are rotation-equivariant w.r.t. $V^{(n)}$ and $(* *)$ holds because vector inner products are rotation-invariant. Proposition 11. Deng et al., 2021) VN-Invariant : $\\mathbb{R}^{C \\times 3} \\rightarrow \\mathbb{R}^{C \\times 3}$ is rotation-invariant. Proof. $$\n\\begin{aligned}\n\\operatorname{VN}-\\operatorname{Invariant}\\left(V^{(n)} R ; W\\right) & =\\left(V^{(n)} R\\right) \\operatorname{VN}-\\operatorname{MLP}\\left(V^{(n)} R ; W\\right)^{\\top} \\\\\n& \\stackrel{(*)}{=} V^{(n)} R\\left[\\operatorname{VN}-\\operatorname{MLP}\\left(V^{(n)} ; W\\right) R\\right]^{\\top} \\\\\n& =V^{(n)} R R^{\\top} \\operatorname{VN}-\\operatorname{MLP}\\left(V^{(n)} ; W\\right)^{\\top} \\\\\n& =V^{(n)} \\operatorname{VN}-\\operatorname{MLP}\\left(V^{(n)} ; W\\right)^{\\top} \\\\\n& =\\mathrm{VN}-\\operatorname{Invariant}\\left(V^{(n)} ; W\\right)\n\\end{aligned}\n$$\n\nwhere $(*)$ holds by equivariance of VN-MLP. [^0]:    *Work done during an internship at Waymo LLC. [^1]:    ${ }^{1}$ In practice, we set $H$ and $P$ such that $H P=C^{\\prime}$. [^2]:    ${ }^{2} W_{Q}^{\\ell \\ell^{\\prime}}, W_{K}^{\\ell \\ell^{\\prime}}, W_{V}^{\\ell \\ell^{\\prime}} \\in \\mathbb{R}^{(2 \\ell+1) \\times\\left(2 \\ell^{\\prime}+1\\right)}$ in the general case, where $\\ell, \\ell^{\\prime} \\in\\{0,1,2\\}$ are feature types\n\n"
}