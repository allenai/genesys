{
    "vntransformer-0": "\\newfloatcommand\n\ncapbtabboxtable[][\\FBwidth]\n\nVN-Transformer: Rotation-Equivariant Attention for Vector Neurons\n\nSerge Assaad serge.assaad@duke.edu Duke University Carlton Downey cmdowney@waymo.com Waymo LLC Rami Al-Rfou rmyeid@waymo.com Waymo LLC Nigamaa Nayakanti nigamaa@waymo.com Waymo LLC Ben Sapp bensapp@waymo.com Waymo LLC Work done during an internship at Waymo LLC.",
    "vntransformer-1": "Abstract\n\nRotation equivariance is a desirable property in many practical applications such as motion forecasting and 3D perception, where it can offer benefits like sample efficiency, better generalization, and robustness to input perturbations. Vector Neurons (VN) is a recently developed framework offering a simple yet effective approach for deriving rotation-equivariant analogs of standard machine learning operations by extending one-dimensional scalar neurons to three-dimensional \u201cvector neurons.\u201d We introduce a novel \u201cVN-Transformer\u201d architecture to address several shortcomings of the current VN models. Our contributions are: we derive a rotation-equivariant attention mechanism which eliminates the need for the heavy feature preprocessing required by the original Vector Neurons models; we extend the VN framework to support non-spatial attributes, expanding the applicability of these models to real-world datasets; we derive a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution, greatly speeding up inference and training; we show that small tradeoffs in equivariance (-approximate equivariance) can be used to obtain large improvements in numerical stability and training robustness on accelerated hardware, and we bound the propagation of equivariance violations in our models. Finally, we apply our VN-Transformer to 3D shape classification and motion forecasting with compelling results. 1 Introduction\n\nA chair \u2013 seen from the front, the back, the top, or the side \u2013 is still a chair. When driving a car, our driving behavior is independent of our direction of travel. These simple examples demonstrate how humans excel at using rotation invariance and equivariance to understand the world in context (see figure on the right). Unfortunately, typical machine learning models struggle to preserve equivariance/invariance when appropriate \u2013 it is indeed challenging to equip neural networks with the right inductive biases to represent 3D objects in an equivariant manner. Modeling spatial data is a core component in many domains such as CAD, AR/VR, and medical imaging applications. In assistive robotics and autonomous vehicle applications, 3D object detection, tracking, and motion forecasting form the basis for how a robot interacts with\n\nhumans in the real world. Preserving rotation invariance/equivariance can improve training time, reduce model size, and provide crucial guarantees about model performance in the presence of noise. Spatial data is often represented as a point-cloud data structure. These point-clouds require both permutation invariance and rotation equivariance to be modeled sufficiently well. Approaches addressing permutation invariance include Zaheer et al. (2018); Lee et al. (2019); Qi et al. (2017). Recently, approaches jointly addressing rotation invariance and equivariance are gaining momentum. These can roughly be categorized into modeling invariance or equivariance by: (i) data augmentation, (ii) canonical pose estimation, and (iii) model construction. Approaches and do not guarantee exact equivariance since they rely on model parameters to learn the right inductive biases (Qi et al., 2017; Esteves et al., 2018; Jaderberg et al., 2015; Chang et al., 2015). Further, data augmentation makes training more costly and errors in pose estimation propagate to downstream tasks, degrading model performance. Moreover, pose estimation requires labeling objects with their observed poses. In contrast, proposals in do provide equivariance guarantees, including the Tensor Field Networks of Thomas et al. (2018) and the SE(3)-Transformer of Fuchs et al. (2020) (see Section 2). However, their formulations require complex mathematical machinery or are limited to specific network architectures. Most recently, Deng et al. (2021) proposed a simple and generalizable framework, dubbed Vector Neurons (VNs), that can be used to replace traditional building blocks of neural networks with rotation-equivariant analogs. The basis of the framework is lifting scalar neurons to 3-dimensional vectors, which admit simple mappings of actions to latent spaces. While Deng et al. (2021) developed a framework and basic layers, many issues required for practical deployment on real-world applications remain unaddressed. A summary of our contributions and their motivations are as follows:\n\nVN-Transformer. The use of Transformers in deep learning has exploded in popularity in recent years as the de facto standard mechanism for learned soft attention over input and latent representations. They have enjoyed many successes in image and natural language understanding (Khan et al., 2022; Vaswani et al., 2017), and they have become an essential modeling component in most domains. Our primary contribution of this paper is to develop a VN formulation of soft attention by generalizing scalar inner-product based attention to matrix inner-products (i.e., the Frobenius inner product). Thanks to Transformers\u2019 ability to model functions on sets (since they are permutation-equivariant), they are a natural fit to model functions on point-clouds. Our VN-Transformer possesses all the appealing properties that have made the original Transformer so successful, as well as rotation equivariance as an added benefit. Direct point-set input. The original VN paper relied on edge convolution as a pre-processing step to capture local point-cloud structure. Such feature engineering is not data-driven and requires human involvement in designing and tuning. Moreover, the sparsity of these computations makes them slow to run\n\non accelerated hardware. Our proposed rotation-equivariant attention mechanism learns higher-level features directly from single points for arbitrary point-clouds (see Section 4). Handling points augmented with non-spatial attributes. Real-world point-cloud datasets have complicated features sets \u2013 the spatial dimensions are typically augmented with crucial non-spatial attributes where can be high-dimensional. For example, Lidar point-clouds have intensity & elongation values associated with each point, multi-sensor point-clouds have modality types, and point-clouds with semantic type have semantic attributes. The VN framework restricted the scope of their work to spatial point-cloud data, limiting the applicability of their models for real-world point-clouds with attributes. We investigate two mechanisms to integrate attributes into equivariant models while preserving rotation equivariance (see Section 5). Equivariant multi-scale feature reduction. Practical data structures such as Lidar point-clouds are extremely large, consisting of hundreds of objects each with potentially millions of points. To handle such computationally\n\n\\ffigbox\n\n[]{subfloatrow} \\ffigbox[0.46] Rotation-invariant classification model. \\ffigbox[0.46]Rotation-equivariant trajectory forecasting model. VN-Transformer (\u201cearly fusion\u201d) models. Legend: ( ) SO(3)-equivariant features; ( ) SO(3)-invariant features; ( ) Non-spatial features. challenging situations, we design a rotation-equivariant mechanism for multi-scale reduction of point-cloud resolution. This mechanism learns how to pool the point set in a context-sensitive manner leading to a significant reduction in training and inference latency (see Section 6). approximate equivariance. When attempting to scale up VN models for distributed accelerated hardware we observed significant numerical stability issues. We determined that these stemmed from a fundamental limitation of the original VN framework, where bias values could not be included in linear layers while preserving equivariance. We introduce the notion of -approximate equivariance, and use it to show that small tradeoffs in equivariance can be controlled to obtain large improvements in numerical stability via the addition of small biases, improving robustness of training on accelerated hardware.",
    "vntransformer-2": "Additionally, we theoretically bound the propagation of rotation equivariance violations in VN networks (see Section 7). Empirical analysis. Finally, we evaluate our VN-Transformer on the ModelNet40 shape classification task, a modified ModelNet40 which includes per-point non-spatial attributes, and a modified version of the Waymo Open Motion Dataset trajectory forecasting task (see Section 8). 2 Related work\n\nThe machine learning community has long been interested in building models that achieve equivariance to certain transformations, e.g., permutations, translations, and rotations. For a thorough review, see Bronstein et al. (2021). Learned approximate transformation invariance. A very common approach is to learn robustness to input transforms via data augmentation (Zhou & Tuzel, 2018; Qi et al., 2018; Krizhevsky et al., 2012; Lang et al., 2019; Yang et al., 2018) or by explicitly predicting transforms to canonicalize pose (Jaderberg et al., 2015; Hinton et al., 2018; Esteves et al., 2018; Chang et al., 2015). Rotation-equivariant CNNs. Recently, there has been specific interest in designing rotation-equivariant image models for 2D perception tasks (Cohen & Welling, 2016; Worrall et al., 2017; Marcos et al., 2017; Chidester et al., 2018). Worrall & Brostow (2018) extended this work to 3D perception, and Veeling et al. (2018) demonstrated the promise of rotation-equivariant models for medical images. Equivariant point-cloud models. Thomas et al. (2018) proposed Tensor Field Networks (TFNs), which use tensor representations of point-cloud data, Clebsch-Gordan coefficients, and spherical harmonic filters to build rotation-equivariant models. Fuchs et al. (2020) propose an \u201cSE(3)-Transformer\u201d by adding an attention mechanism for TFNs. One of the key ideas behind this body of work is to create highly restricted weight matrices that commute with rotation operations by construction (i.e., ). In contrast, we propose a simpler alternative: a \u201cVN-Transformer\u201d which guarantees equivariance for arbitrary weight matrices, removing the need for the complex mathematical machinery of the SE(3)-Transformer. For a detailed comparison with Fuchs et al. (2020), see Appendix A. Controllable approximate equivariance. Finzi et al. (2021) proposed equivariant priors on model weight matrices to achieve approximate equivariance, and Wang et al. (2022) proposed a relaxed steerable 2D convolution along with soft equivariance regularization. In this work, we introduce the related notion of \u201c-approximate equivariance,\u201d achieved by adding biases with small and controllable norms. We theoretically bound the equivariance violation introduced by this bias, and we also bound how such violations propagate through deep VN networks. Non-spatial attributes. TFNs and the SE(3)-Transformer account for non-spatial attributes associated with each point (e.g., color, intensity), which they refer to as \u201ctype-\u201d features. In this work, we investigate two mechanisms (early & late fusion) to incorporate non-spatial data into the VN framework. Attention-based architectures. Since the introduction of Transformers by Vaswani et al. (2017), self-attention and cross-attention mechanisms have provided powerful and versatile components which are propelling the field of natural language processing forward (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Yang et al., 2020). Lately, so-called \u201cVision Transformers\u201d (Dosovitskiy et al., 2021; Khan et al., 2022) have had a similar impact on the field of computer vision, providing a compelling alternative to convolutional networks.",
    "vntransformer-3": "3 Background\n\n3.1 Notation & preliminaries\n\nDataset. Suppose we have a dataset , where is an index into a point-cloud/label pair \u2013 we omit the subscript whenever it is unambiguous to do so. is a single 3D point-cloud with points. In a classification problem, , where is the number of classes. In a regression problem, we might have where is the number of output points, and is the dimension of each output point (with corresponding to univariate regression). Index notation. We use \u201cnumpy-like\u201d indexing of tensors. Assuming we have a tensor , we present some examples of this indexing scheme:\n\nRotations & weights. Suppose we have a tensor and a rotation matrix , where is the three-dimensional rotation group. We denote the \u201crotation\u201d of the tensor by , defined as: \u2013 in other words, the rotated tensor is simply the concatenation of the individually rotated matrices . Additionally, if we have a matrix of weights , we define the product by . Invariance and equivariance. Definition 1 (Rotation Invariance). is rotation-invariant if . Definition 2 (Rotation Equivariance). is rotation-equivariant if . For simplicity, we defined invariance/equivariance as above instead of the more general , which requires background on group theory and representation theory.",
    "vntransformer-4": "Proofs. We defer proofs to Appendix C. 3.2 The Vector Neuron (VN) framework\n\nIn the Vector Neuron framework (Deng et al., 2021), the authors represent a single point (e.g., in a hidden layer of a neural network) as a matrix (see inset figure), where can be thought of as a tensor representation of the entire point-cloud. This representation allows for the design of SO(3)-equivariant analogs of standard neural network operations. VN-Linear layer. As an illustrative example, the VN-Linear layer is a function , defined by , where is a matrix of learnable weights. This operation is rotation-equivariant: . Deng et al. (2021) also develop VN analogs of common deep network layers ReLU, MLP, BatchNorm, and Pool. For further definitions and proofs of equivariance, see Appendix C.",
    "vntransformer-5": "For further details, we point the reader to Deng et al. (2021). 4 The VN-Transformer\n\nIn this section, we extend the ideas presented in Deng et al. (2021) to design a \u201cVN-Transformer\u201d that enjoys the rotation equivariance property. 4.1 Rotation-invariant inner product\n\nThe notion of an inner product between tokens is central to the attention operation from the original Transformer (Vaswani et al., 2017). Consider the Frobenius inner product between two VN representations, defined below. Definition 3 (Frobenius inner product). The Frobenius inner product between two matrices is defined by\n\nThis choice of inner product is convenient because of its rotation invariance property, stated below. Proposition 1. The Frobenius inner product between Vector Neuron representations is rotation-invariant, i.e. Proof. \u27e8 V ( n ) \u200b R , V ( n \u2032 ) \u200b R \u27e9 F = \u2211 c = 1 C ( V ( n , c ) \u200b R ) \u200b ( V ( n \u2032 , c ) \u200b R ) \u22ba = \u2211 c = 1 C V ( n , c ) \u200b R \u200b R \u22ba \u200b V ( n \u2032 , c ) \u22ba = \u2211 c = 1 C V ( n , c ) \u200b V ( n \u2032 , c ) \u22ba = \u27e8 V ( n ) , V ( n \u2032 ) \u27e9 F subscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 superscript \ud835\udc49 superscript \ud835\udc5b \u2032 \ud835\udc45 \ud835\udc39 superscript subscript \ud835\udc50 1 \ud835\udc36 superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 \ud835\udc45 superscript superscript \ud835\udc49 superscript \ud835\udc5b \u2032 \ud835\udc50 \ud835\udc45 \u22ba superscript subscript \ud835\udc50 1 \ud835\udc36 superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 \ud835\udc45 superscript \ud835\udc45 \u22ba superscript \ud835\udc49 limit-from superscript \ud835\udc5b \u2032 \ud835\udc50 \u22ba superscript subscript \ud835\udc50 1 \ud835\udc36 superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 superscript \ud835\udc49 limit-from superscript \ud835\udc5b \u2032 \ud835\udc50 \u22ba subscript superscript \ud835\udc49 \ud835\udc5b superscript \ud835\udc49 superscript \ud835\udc5b \u2032 \ud835\udc39 \\displaystyle\\langle V^{(n)}R,V^{(n^{\\prime})}R\\rangle_{F}=\\sum_{c=1}^{C}(V^{(n,c)}R)(V^{(n^{\\prime},c)}R)^{\\intercal}=\\sum_{c=1}^{C}V^{(n,c)}RR^{\\intercal}V^{(n^{\\prime},c)\\intercal}=\\sum_{c=1}^{C}V^{(n,c)}V^{(n^{\\prime},c)\\intercal}=\\langle V^{(n)},V^{(n^{\\prime})}\\rangle_{F} (1)\n\nThis rotation-invariant inner product between VN representations allows us to construct a rotation-equivariant attention operation, detailed in the next section. 4.2 Rotation-equivariant attention\n\nConsider two tensors and , which can be thought of as sets of (resp. ) tokens, each a matrix. Using the Frobenius inner product, we can define an attention matrix between the two sets as follows:\n\nA \u200b ( Q , K ) ( m ) \u225c softmax \u200b ( 1 3 \u200b C \u200b [ \u27e8 Q ( m ) , K ( n ) \u27e9 F ] n = 1 N ) , \u225c \ud835\udc34 superscript \ud835\udc44 \ud835\udc3e \ud835\udc5a softmax 1 3 \ud835\udc36 superscript subscript delimited-[] subscript superscript \ud835\udc44 \ud835\udc5a superscript \ud835\udc3e \ud835\udc5b \ud835\udc39 \ud835\udc5b 1 \ud835\udc41 \\displaystyle A(Q,K)^{(m)}\\triangleq\\text{softmax}\\Bigl{(}\\frac{1}{\\sqrt{3C}}\\left[\\langle Q^{(m)},K^{(n)}\\rangle_{F}\\right]_{n=1}^{N}\\Bigr{)}, (2)\n\nThis attention operation is rotation-equivariant w.r.t.",
    "vntransformer-6": "simultaneous rotation of all inputs: Following Vaswani et al.",
    "vntransformer-7": "(2017), we divide the inner products by since . From Proposition 1, . Finally, we define the operation as:\n\nVN-Attn \u200b ( Q , K , Z ) ( m ) \u225c \u2211 n = 1 N A \u200b ( Q , K ) ( m , n ) \u200b Z ( n ) . \u225c VN-Attn superscript \ud835\udc44 \ud835\udc3e \ud835\udc4d \ud835\udc5a superscript subscript \ud835\udc5b 1 \ud835\udc41 \ud835\udc34 superscript \ud835\udc44 \ud835\udc3e \ud835\udc5a \ud835\udc5b superscript \ud835\udc4d \ud835\udc5b \\displaystyle\\text{VN-Attn}(Q,K,Z)^{(m)}\\triangleq\\sum_{n=1}^{N}A(Q,K)^{(m,n)}Z^{(n)}. (3)\n\nProposition 2. Proof. VN-Attn \u200b ( Q \u200b R , K \u200b R , Z \u200b R ) ( m ) = \u2211 n = 1 N A \u200b ( Q \u200b R , K \u200b R ) ( m , n ) \u200b Z ( n ) \u200b R VN-Attn superscript \ud835\udc44 \ud835\udc45 \ud835\udc3e \ud835\udc45 \ud835\udc4d \ud835\udc45 \ud835\udc5a superscript subscript \ud835\udc5b 1 \ud835\udc41 \ud835\udc34 superscript \ud835\udc44 \ud835\udc45 \ud835\udc3e \ud835\udc45 \ud835\udc5a \ud835\udc5b superscript \ud835\udc4d \ud835\udc5b \ud835\udc45 \\displaystyle\\text{VN-Attn}(QR,KR,ZR)^{(m)}=\\sum_{n=1}^{N}A(QR,KR)^{(m,n)}Z^{(n)}R (4) = ( \u2217 ) \u200b [ \u2211 n = 1 N A \u200b ( Q , K ) ( m , n ) \u200b Z ( n ) ] \u200b R = VN-Attn \u200b ( Q , K , Z ) ( m ) \u200b R , delimited-[] superscript subscript \ud835\udc5b 1 \ud835\udc41 \ud835\udc34 superscript \ud835\udc44 \ud835\udc3e \ud835\udc5a \ud835\udc5b superscript \ud835\udc4d \ud835\udc5b \ud835\udc45 VN-Attn superscript \ud835\udc44 \ud835\udc3e \ud835\udc4d \ud835\udc5a \ud835\udc45 \\displaystyle\\overset{(*)}{=}\\left[\\sum_{n=1}^{N}A(Q,K)^{(m,n)}Z^{(n)}\\right]R=\\text{VN-Attn}(Q,K,Z)^{(m)}R, (5)\n\nwhere holds since (which follows straightforwardly from Proposition 1 and equation 2). \u220e\n\nThis is extendable to multi-head attention with heads, :\n\nVN- MultiHeadAttn \u200b ( Q , K , Z ) \u225c W O \u200b [ VN-Attn \u200b ( W h Q \u200b Q , W h K \u200b K , W h Z \u200b Z ) ] h = 1 H , \u225c MultiHeadAttn \ud835\udc44 \ud835\udc3e \ud835\udc4d superscript \ud835\udc4a \ud835\udc42 superscript subscript delimited-[] VN-Attn superscript subscript \ud835\udc4a \u210e \ud835\udc44 \ud835\udc44 superscript subscript \ud835\udc4a \u210e \ud835\udc3e \ud835\udc3e superscript subscript \ud835\udc4a \u210e \ud835\udc4d \ud835\udc4d \u210e 1 \ud835\udc3b \\displaystyle\\text{MultiHeadAttn}(Q,K,Z)\\triangleq W^{O}\\left[\\text{VN-Attn}(W_{h}^{Q}Q,W_{h}^{K}K,W_{h}^{Z}Z)\\right]_{h=1}^{H}, (6)\n\nwhere , are feature, key, and value projection matrices of the -th head (respectively), and is an output projection matrix (Vaswani et al., 2017).111In practice, we set and such that . VN-MultiHeadAttn is also rotation-equivariant, and is the key building block of our rotation-equivariant VN-Transformer. We note that a similar idea was proposed in Fuchs et al. (2020) \u2013 namely, they use inner products between equivariant representations (obtained from the TFN framework of Thomas et al. (2018)) to create a rotation-invariant attention matrix and a rotation-equivariant attention mechanism. Our attention mechanism can be thought of as the same treatment applied to the VN framework.",
    "vntransformer-8": "For a more detailed comparison between this work and the proposal of Fuchs et al. (2020), see Appendix A. 4.3 Rotation-equivariant layer normalization\n\nDeng et al. (2021) allude to a rotation-equivariant version of the well-known layer normalization operation (Ba et al., 2016), but do not explicitly provide it \u2013 we do so here for completeness (see Figure 2):\n\nVN-LayerNorm \u200b ( V ( n ) ) \u225c \u225c VN-LayerNorm superscript \ud835\udc49 \ud835\udc5b absent \\displaystyle\\text{VN-LayerNorm}(V^{(n)})\\triangleq (7) [ V ( n , c ) \u2016 V ( n , c ) \u2016 2 ] c = 1 C \u2299 LayerNorm \u200b ( [ \u2016 V ( n , c ) \u2016 2 ] c = 1 C ) \u200b \ud835\udfd9 1 \u00d7 3 , direct-product superscript subscript delimited-[] superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 2 \ud835\udc50 1 \ud835\udc36 LayerNorm superscript subscript delimited-[] subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 2 \ud835\udc50 1 \ud835\udc36 subscript 1 1 3 \\displaystyle\\left[\\frac{V^{(n,c)}}{||V^{(n,c)}||_{2}}\\right]_{c=1}^{C}\\odot\\text{LayerNorm}\\left(\\left[||V^{(n,c)}||_{2}\\right]_{c=1}^{C}\\right)\\mathds{1}_{1\\times 3},\n\nwhere is an elementwise product, LayerNorm: is the layer normalization operation of Ba et al. (2016), and is a row-vector of ones. 4.4 Encoder architecture\n\nFigure 1 details the architecture of our proposed rotation-equivariant VN-Transformer encoder. The encoder is structurally identical to the original Transformer encoder of Vaswani et al. (2017), with each operation replaced by its rotation-equivariant VN analog. 5 Non-spatial attributes\n\n\u201cReal-world\u201d point-clouds are typically augmented with crucial meta-data such as intensity & elongation for Lidar point-clouds and & sensor type for multi-sensor point-clouds. Handling such point-clouds while still satisfying equivariance/invariance w.r.t. spatial inputs would be useful for many applications. We investigate two strategies (late fusion and early fusion) to handle non-spatial attributes while maintaining rotation equivariance/invariance w.r.t. spatial dimensions:\n\nLate fusion. In this approach, we propose to incorporate non-spatial attributes into the model at a later stage, where we have already processed the spatial inputs in a rotation-equivariant fashion \u2013 our \u201clate fusion\u201d models for classification and trajectory prediction are shown in Figure 5. Early fusion. Early fusion is a simple yet powerful way to process non-spatial attributes (Jaegle et al., 2021). In this approach, we do not treat non-spatial attributes differently (see Figure 1) \u2013 we simply concatenate spatial & non-spatial inputs before feeding them into the VN-Transformer. The VN representations obtained are matrices (instead of ). \\ffigbox\n\n[]{subfloatrow} \\ffigbox[0.45] Rotation-invariant classification model. \\ffigbox[0.5]Rotation-equivariant trajectory forecasting model. VN-Transformer (\u201clate fusion\u201d) models. Legend: ( ) SO(3)-equivariant features; ( ) SO(3)-invariant features; ( ) Non-spatial features. 6 Rotation-equivariant multi-scale feature aggregation\n\nJaegle et al. (2021) recently proposed an attention-based architecture, PerceiverIO, which reduces the computational complexity of the vanilla Transformer by reducing the number of tokens (and their dimension) in the intermediate representations of the network. They achieve this reduction by learning a set of \u201clatent features,\u201d which they use to perform QKV attention with the original input tokens (with and ). Finally, they perform self-attention operations on the resulting array, leading to a runtime instead of\n\nfor each encoder self-attention operation, greatly improving time complexity during training and inference \u2013 a boon for time-critical applications such as real-time motion forecasting. However, such learnable latent features would violate equivariance in our case, since these learnable features would have no information about the original input\u2019s orientation. To remedy this, we instead propose to a learn a transformation from the inputs to the latent features (where the number of latent features is much smaller than the number of original inputs). Specifically, we propose to use a mean projection function , where is a learnable tensor. VN-MeanProject is both rotation-equivariant and permutation-invariant. We then perform VN-MultiHeadAttention between the resulting latent features and the original inputs to get a smaller set of VN representations. The architecture diagram for our proposed rotation-equivariant \u201clatent feature\u201d mechanism is shown in Figure 3. 7 -approximate equivariance\n\nWe noticed that distributed training on accelerated hardware is numerically unstable for points with small norms. This is unique to VN models \u2013 the VN-Linear layer does not include a bias vector, which leads to frequent underflow issues on distributed accelerated hardware. We found that introducing small and controllable additive biases fixes these issues \u2013 we modify the VN-Linear layer by adding a bias with controllable norm:\n\nVN-LinearWithBias \u200b ( V ( n ) ; W , U , \u03f5 ) VN-LinearWithBias superscript \ud835\udc49 \ud835\udc5b \ud835\udc4a \ud835\udc48 italic-\u03f5 \\displaystyle\\textup{VN-LinearWithBias}(V^{(n)};W,U,\\epsilon) \u225c W \u200b V ( n ) + \u03f5 \u200b U , U ( c ) \u225c B ( c ) / \u2016 B ( c ) \u2016 2 , formulae-sequence \u225c absent \ud835\udc4a superscript \ud835\udc49 \ud835\udc5b italic-\u03f5 \ud835\udc48 \u225c superscript \ud835\udc48 \ud835\udc50 superscript \ud835\udc35 \ud835\udc50 subscript norm superscript \ud835\udc35 \ud835\udc50 2 \\displaystyle\\triangleq WV^{(n)}+\\epsilon U,~{}~{}~{}~{}U^{(c)}\\triangleq B^{(c)}/||B^{(c)}||_{2}, (8)\n\nwhere is a hyperparameter controlling the bias norm, and is a learnable matrix. This leads to significant improvements in training stability and model quality. In principle, VN-LinearWithBias is not equivariant, but its violation of equivariance can be bounded. Work on equivariance by construction typically treats rotation equivariance as a binary idea \u2013 a model is either equivariant, or it is not. This can be relaxed by asking: how large is the violation of equivariance? We quantify this with the equivariance violation metric, defined by:\n\n\u0394 \u200b ( f , X , R ) \u225c \u2016 f \u200b ( X \u200b R ) \u2212 f \u200b ( X ) \u200b R \u2016 F . \u225c \u0394 \ud835\udc53 \ud835\udc4b \ud835\udc45 subscript norm \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc39 \\displaystyle\\Delta(f,X,R)\\triangleq||f(XR)-f(X)R||_{F}. (9)\n\nIf , we say is -approximately equivariant. We bound the equivariance violation of below:\n\nProposition 3. VN-LinearWithBias is -approximately equivariant (tight when . A natural next question is: how do such equivariance violations propagate through a deep model? Proposition 4. Suppose we have functions (with , ) s.t. 1. is -approximately equivariant for all\n\n2. is -Lipschitz (w.r.t. ) for all . Then, the composition is -approximately equivariant, where\n\n\u03f5 1 \u200b \u2026 \u200b K \u225c L K \u200b ( \u22ef \u200b ( L 3 \u200b ( L 2 \u200b \u03f5 1 + \u03f5 2 ) + \u03f5 3 ) + \u22ef ) + \u03f5 K . \u225c subscript italic-\u03f5 1 \u2026 \ud835\udc3e subscript \ud835\udc3f \ud835\udc3e \u22ef subscript \ud835\udc3f 3 subscript \ud835\udc3f 2 subscript italic-\u03f5 1 subscript italic-\u03f5 2 subscript italic-\u03f5 3 \u22ef subscript italic-\u03f5 \ud835\udc3e \\displaystyle\\epsilon_{1\\ldots K}\\triangleq L_{K}(\\cdots(L_{3}(L_{2}\\epsilon_{1}+\\epsilon_{2})+\\epsilon_{3})+\\cdots)+\\epsilon_{K}. (10)\n\nIntuitively, each layer \u201cstretches\u201d the equivariance violation error of the previous layers by its Lipschitz constant , and adds its own violation to the total error. 8 Experiments\n\n8.1 Rotation-invariant classification\n\nFigure 1 shows our proposed VN-Transformer architecture for classification. It consists of rotation-equivariant operations (VN-MLP and VN-Transformer Encoder blocks), followed by an invariant operation (VN-Invariant), and finally standard Flatten/Pool/MLP operations to get class predictions. The resulting logits/class predictions are rotation-invariant. Model Acc. # Params TFN (Thomas et\u00a0al., 2018 ) 88.5% \u2013 RI-Conv (Zhang et\u00a0al., 2019 ) 86.5% \u2013 GC-Conv (Zhang et\u00a0al., 2020 ) 89.0% \u2013 VN-PointNet (Deng et\u00a0al., 2021 ) 77.2% 2.20M VN-DGCNN (Deng et\u00a0al., 2021 ) 90.0% 2.00M VN-Transformer (ours) 90.8% 0.04M\n\nWe evaluate our VN-Transformer classifier on the commonly used ModelNet40 dataset (Wu et al., 2015), a 40-class point-cloud classification problem. In Table 1, we compare our model with recent rotation-invariant models. The VN-Transformer outperforms the baseline VN models with orders of magnitude fewer parameters. Furthermore, we dispense with the computationally expensive edge-convolution used as a preprocessing step in the models of Deng et al. (2021) and find that the VN-Transformer\u2019s performance is relatively unaffected (see Figure 5). Model \u03f5 italic-\u03f5 {{{\\epsilon}}} Fusion Features Acc. VN-PointNet 0 \u2013 [ x , y , z ] \ud835\udc65 \ud835\udc66 \ud835\udc67 [x,y,z] 77.2% VN-Transformer 0 0 \u2013 [ x , y , z ] \ud835\udc65 \ud835\udc66 \ud835\udc67 [x,y,z] 88.5% VN-Transformer 10 \u2212 6 superscript 10 6 10^{-6} \u2013 [ x , y , z ] \ud835\udc65 \ud835\udc66 \ud835\udc67 [x,y,z] 90.8% VN-PointNet 0 Early [ x , y , z , a ] \ud835\udc65 \ud835\udc66 \ud835\udc67 \ud835\udc4e [x,y,z,a] 82.0% VN-Transformer 0 Early [ x , y , z , a ] \ud835\udc65 \ud835\udc66 \ud835\udc67 \ud835\udc4e [x,y,z,a] 91.1% VN-Transformer 10 \u2212 6 superscript 10 6 10^{-6} Early [ x , y , z , a ] \ud835\udc65 \ud835\udc66 \ud835\udc67 \ud835\udc4e [x,y,z,a] 95.4 % VN-Transformer 10 \u2212 6 superscript 10 6 10^{-6} Late [ x , y , z , a ] \ud835\udc65 \ud835\udc66 \ud835\udc67 \ud835\udc4e [x,y,z,a] 91.0%\n\n8.2 Classification with non-spatial attributes\n\nTo evaluate our model\u2019s ability to handle non-spatial attributes, we design a modified version of the ModelNet40 dataset, called ModelNet40 \u201cPolka-dot,\u201d which we construct (from ModelNet40) as follows: to each point , we append . Within a radius of a randomly chosen center, we randomly select 30 points and set . We make the \u201cpolka-dot\u201d radius depend on the label via where . Figure 5 shows example point-clouds from the ModelNet40 Polka-dot dataset. By generating ModelNet40 Polka-dot in this way, we directly embed class information into the non-spatial attributes. In order to perform well on this task, models need to effectively fuse spatial and non-spatial information (there is no useful information in the non-spatial attributes alone since all point-clouds have ). The results on ModelNet40 Polka-dot are shown in Table 2. Both VN-PointNet and VN-Transformer benefit significantly from the binary polka-dots, suggesting they are able to effectively combine spatial and non-spatial information. 8.3 Latent features\n\nFigure 6 shows our results on ModelNet40 when we reduce the number of tokens from 1024 (the number of points in the original point-cloud) to 32 using the latent feature mechanism presented in Figure 3. Using latent features provides a 2x latency improvement (in training steps/sec) with minimal (1.7%) accuracy degradation (compared with row 3 of Table 2). This suggests a real benefit of the latent feature mechanism in time-sensitive applications such as autonomous driving. 8.4 Rotation-equivariant motion forecasting\n\nFigure 1 shows our proposed rotation-equivariant architecture for motion forecasting. In motion forecasting the goal is to predict the locations of an agent for a sequence of future timesteps, given as input the past locations of the agent. We evaluate the model on a simplified version of the Waymo Open Motion Dataset (WOMD; Ettinger et al., 2021):\n\n\u2022\n\nWe select 4904 trajectories (3915 for training, 979 for testing). \u2022\n\nEach trajectory consists of 91 points for a single vehicle sampled at 5 Hz. \u2022\n\nWe use the first 11 points (the past) as input and we predict the remaining 80 points (the future). Model \u03f5 italic-\u03f5 \\epsilon Features ADE ( \u2193 \u2193 \\downarrow ) Transformer \u2013 [ x , y , z ] \ud835\udc65 \ud835\udc66 \ud835\udc67 [x,y,z] 5.01 Transformer + \u21bb \u21bb \\circlearrowright z \ud835\udc67 \\,z \u2013 [ x , y , z ] \ud835\udc65 \ud835\udc66 \ud835\udc67 [x,y,z] 4.51 VN-Transformer 0 [ x , y , z ] \ud835\udc65 \ud835\udc66 \ud835\udc67 [x,y,z] 4.91 VN-Transformer 10 \u2212 6 superscript 10 6 10^{-6} [ x , y , z ] \ud835\udc65 \ud835\udc66 \ud835\udc67 [x,y,z] 3.95 VN-Transformer 0 [ x , y , z , a ] \ud835\udc65 \ud835\udc66 \ud835\udc67 \ud835\udc4e [x,y,z,a] 5.01 VN-Transformer 10 \u2212 6 superscript 10 6 10^{-6} [ x , y , z , a ] \ud835\udc65 \ud835\udc66 \ud835\udc67 \ud835\udc4e [x,y,z,a] 3.67\n\nWe evaluate the quality of our trajectory forecasting models using the Average Distance Error (ADE): where is the number of time-steps in the output trajectory and are the ground-truth trajectory and the predicted trajectory, respectively. Results are shown in Table 3. Adding training-time random rotations about the -axis yields improves the performance of the vanilla Transformer, and the VN-Transformer outperforms the vanilla Transformer (without the need for train-time rotation augmentations, thanks to equivariance). Figure 7 shows example predictions on WOMD. Equivariance violations of the vanilla Transformer models (columns (a) and (b)) are clearly demonstrated here, in contrast with the equivariant VN-Transformer (column (c)). 9 Conclusion\n\nIn this paper, we introduced the VN-Transformer, a rotation-equivariant Transformer model based on the Vector Neurons framework. VN-Transformer is a significant step towards building powerful, modular, and easy-to-use models that have appealing equivariance properties for point-cloud data. Limitations of our work include: similar to previous work (Deng et al., 2021; Qi et al., 2017) we assume input data has been mean-centered. This is sensitive to outliers, and prevents us from making single-pass predictions for multi-object problems (we have to independently mean-center each agent first). Similarly, we have not addressed other types of invariance/equivariance (e.g., scale invariance) in this work; Proposition 4 shows an error bound on the total equivariance violation of the network with -Lipschitz layers. We know the Lipschitz constants of VN-Linear and VN-LinearWithBias (see Appendix C), but we have not yet determined them for other layers (e.g., VN-ReLU, VN-MultiHeadAttn). We will address these gaps in future work, and we will also leverage VN-Transformers to obtain state-of-the-art performance on a number of key benchmarks such as the full Waymo Open Motion Dataset and ScanObjectNN.",
    "vntransformer-9": "References\n\nBa et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. Bronstein et al. (2021) Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veli\u010dkovi\u0107. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. ArXiv, abs/2104.13478, 2021. Chang et al. (2015) Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository.",
    "vntransformer-10": "arXiv preprint arXiv:1512.03012, 2015. Chidester et al. (2018) Benjamin Chidester, Minh N. Do, and Jian Ma. Rotation equivariance and invariance in convolutional neural networks, 2018. Cohen & Welling (2016) Taco Cohen and Max Welling. Group equivariant convolutional networks. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 2990\u20132999, New York, New York, USA, 20\u201322 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/cohenc16.html. Deng et al. (2021) Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas Guibas. Vector neurons: A general framework for so(3)-equivariant networks, 2021.",
    "vntransformer-11": "Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. Esteves et al. (2018) Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, and Kostas Daniilidis. Polar transformer networks. In International Conference on Learning Representations, 2018. Ettinger et al. (2021) Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Aur\u00e9lien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9710\u20139719, October 2021. Finzi et al. (2021) Marc Anton Finzi, Gregory Benton, and Andrew Gordon Wilson. Residual pathway priors for soft equivariance constraints.",
    "vntransformer-12": "In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=k505ekjMzww. Fuchs et al. (2020) Fabian B. Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-translation equivariant attention networks, 2020. Hinton et al. (2018) Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix capsules with em routing. In International conference on learning representations, 2018. Jaderberg et al. (2015) Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in neural information processing systems, 28:2017\u20132025, 2015. Jaegle et al. (2021) Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\u00e9naff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver io: A general architecture for structured inputs & outputs, 2021.",
    "vntransformer-13": "Khan et al. (2022) Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM Computing Surveys, Jan 2022. ISSN 1557-7341. doi: 10.1145/3505244. URL http://dx.doi.org/10.1145/3505244. Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097\u20131105, 2012. Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations, 2020.",
    "vntransformer-14": "Lang et al. (2019) Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds.",
    "vntransformer-15": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12697\u201312705, 2019. Lee et al. (2019) Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks, 2019.",
    "vntransformer-16": "Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. Marcos et al. (2017) Diego Marcos, Michele Volpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector field networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017. Qi et al. (2017) Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation, 2017. Qi et al. (2018) Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum pointnets for 3d object detection from rgb-d data.",
    "vntransformer-17": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 918\u2013927, 2018. Thomas et al. (2018) Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation- and translation-equivariant neural networks for 3d point clouds, 2018.",
    "vntransformer-18": "Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. Veeling et al. (2018) Bastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology, 2018. Wang et al. (2022) Rui Wang, Robin Walters, and Rose Yu. Approximately equivariant networks for imperfectly symmetric dynamics, 2022. URL https://arxiv.org/abs/2201.11969. Worrall & Brostow (2018) Daniel Worrall and Gabriel Brostow. Cubenet: Equivariance to 3d rotation and translation.",
    "vntransformer-19": "In Proceedings of the European Conference on Computer Vision (ECCV), September 2018. Worrall et al. (2017) Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, and Gabriel J. Brostow. Harmonic networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. Wu et al. (2015) Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes, 2015. Yang et al. (2018) Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-time 3d object detection from point clouds.",
    "vntransformer-20": "In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 7652\u20137660, 2018. Yang et al. (2020) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding, 2020. Zaheer et al. (2018) Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and Alexander Smola. Deep sets, 2018. Zhang et al. (2019) Zhiyuan Zhang, Binh-Son Hua, David W. Rosen, and Sai-Kit Yeung. Rotation invariant convolutions for 3d point clouds deep learning, 2019. URL https://arxiv.org/abs/1908.06297. Zhang et al. (2020) Zhiyuan Zhang, Binh-Son Hua, Wei Chen, Yibin Tian, and Sai-Kit Yeung. Global context aware convolutions for 3d point cloud understanding. In 2020 International Conference on 3D Vision (3DV), pp. 210\u2013219, 2020. doi: 10.1109/3DV50981.2020.00031. Zhou & Tuzel (2018) Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection.",
    "vntransformer-21": "In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4490\u20134499, 2018. Appendix A Detailed comparison with SE(3)-Transformer (Fuchs et al., 2020)\n\nA.1 Attention computation\n\nThere is a rich literature on equivariant models using steerable kernels, and the SE(3)-Transformer is the closest development in this field to our work. Here, we make a detailed comparison between our work and the SE(3)-Transformer (and related steerable kernel-based models). For simplicity, we will compare the VN-Transformer with only spatial features vs. the SE(3)-Transformer with only type-1 features (i.e., spatial features). The key difference is in the way the weight matrices are defined and how they interact with the input. Specifically, Fuchs et al. use weight matrices that act on the spatial/representation dimension of the input points (e.g., via where ).222 in the general case, where are feature types As a result, in order to guarantee equivariance they need to design these matrices such that they each commute with a rotation operation (in general \u2013 this depends on the choice of ), hence the need for the machinery of Clebsch-Gordan coefficients, spherical harmonics, and radial neural nets to construct the weights. In contrast, in our proposed attention mechanism, the matrices act on the channel dimension of the input (e.g., via , where ) and not the spatial dimension. As a result, the operations are equivariant no matter the choice of , since . This results in a significantly simpler construction of rotation-equivariant attention that is accessible to a wider audience (i.e., it does not require an understanding of group theory, representation theory, spherical harmonics, Clebsch-Gordan coefficients, etc.) and much easier to implement. For a side-by-side comparison of both attention query computations, see Table 4 above. A.2 VN-Linear vs. SE(3)-Transformer \u201cself-interaction\u201d\n\nThere is a relationship between the VN-Linear operation of Deng et al. (2021), and the \u201clinear self-interaction\u201d layers of Fuchs et al. (2020). Comparing equation of Fuchs et al. (2020), repeated here for convenience:\n\n\ud835\udc1f out , i , c \u2032 \u2113 = \u2211 c w c \u2032 \u200b c \u2113 \u200b \u2113 \u200b \ud835\udc1f in , i , c \u2113 , superscript subscript \ud835\udc1f out \ud835\udc56 superscript \ud835\udc50 \u2032 \u2113 subscript \ud835\udc50 superscript subscript \ud835\udc64 superscript \ud835\udc50 \u2032 \ud835\udc50 \u2113 \u2113 superscript subscript \ud835\udc1f in \ud835\udc56 \ud835\udc50 \u2113 \\displaystyle\\mathbf{f}_{\\text{out},i,c^{\\prime}}^{\\ell}=\\sum_{c}w_{c^{\\prime}c}^{\\ell\\ell}\\mathbf{f}_{\\text{in},i,c}^{\\ell}, (11)\n\nwith the VN-Linear operation of Deng et al. (2021):\n\nV out ( n ) = W \u200b V ( n ) , W \u2208 \u211d C \u2032 \u00d7 C , V ( n ) \u2208 \u211d C \u00d7 3 , formulae-sequence subscript superscript \ud835\udc49 \ud835\udc5b out \ud835\udc4a superscript \ud835\udc49 \ud835\udc5b formulae-sequence \ud835\udc4a superscript \u211d superscript \ud835\udc36 \u2032 \ud835\udc36 superscript \ud835\udc49 \ud835\udc5b superscript \u211d \ud835\udc36 3 \\displaystyle V^{(n)}_{\\text{out}}=WV^{(n)},~{}~{}W\\in\\mathbb{R}^{C^{\\prime}\\times C},V^{(n)}\\in\\mathbb{R}^{C\\times 3}, (12)\n\nwe see that these operations are identical. However, our proposed VN-MultiHeadAttention is different and significantly simpler than the attention mechanism of the SE(3)-Transformer (see Section A.1), as it relies only on the rotation-invariant Frobenius inner product and straightforward multiplication by an arbitary weight matrix to compute the keys, queries, and values (equivalent to VN-Linear/linear self-interaction). In that sense, it is closer in spirit to the original Transformer of Vaswani et al. (replacing vector inner product with Frobenius inner product). Further, as explained previously, it does not require the special construction of weight matrices using Clebsch-Gordan coefficients, spherical harmonics, and radial neural networks as in the SE(3)-Transformer. Appendix B Experimental details\n\nB.1 Datasets\n\nModelNet40\n\nThe ModelNet40 dataset (Wu et al., 2015) is publicly available at https://modelnet.cs.princeton.edu, with the following comment under \u201cCopyright\u201d:\n\n\u201cAll CAD models are downloaded from the Internet and the original authors hold the copyright of the CAD models. The label of the data was obtained by us via Amazon Mechanical Turk service and it is provided freely. This dataset is provided for the convenience of academic research only.\u201d\n\nWaymo Open Motion Dataset\n\nThe Waymo Open Motion Dataset (Ettinger et al., 2021) is publicly available at https://waymo.com/open/data/motion/ under a non-commercial use license agreement. Full license details can be found here: https://waymo.com/open/terms/. B.2 Hyperparameter tuning\n\nTable 5 shows the hyperparameters we swept over for all our experiments on ModelNet40, ModelNet40 Polka-dot, and the Waymo Open Motion Dataset. B.3 Compute infrastructure\n\nWe trained our models on TPU-v3 devices. which are accessible through Google Cloud. Our longest training jobs ran for less than 3 hours on 32 TPU cores. Appendix C Proofs\n\nIn this section, for convenience we will treat all 3D vectors as row-vectors: e.g., . We also note that, while all our proofs of invariance/equivariance use matrices , they can all be trivially generalized to . C.1 Partial invariance & equivariance\n\nAssuming the input point-cloud consists of spatial inputs and associated non-spatial attributes ( is the number of non-spatial attributes associated with each point), we would like our model to satisfy the following property:\n\nDefinition 4 (Partial rotation invariance). A model satisfies partial rotation invariance if\n\nDefinition 5 (Partial rotation equivariance). A model (where ) satisfies partial rotation equivariance if\n\nWe show here that the models in Figure 1 satisfy partial rotation invariance and equivariance (respectively). Consider two rotation matrices and . Lemma 1. The matrix is a valid rotation matrix in .",
    "vntransformer-22": "Proof. We begin by showing that . We first compute :\n\nR ( d 1 + d 2 ) \u00d7 ( d 1 + d 2 ) \u22ba = [ R d 1 \u00d7 d 1 \ud835\udfce d 1 \u00d7 d 2 \ud835\udfce d 2 \u00d7 d 1 R d 2 \u00d7 d 2 ] \u22ba = [ R d 1 \u00d7 d 1 \u22ba \ud835\udfce d 2 \u00d7 d 1 \u22ba \ud835\udfce d 1 \u00d7 d 2 \u22ba R d 2 \u00d7 d 2 \u22ba ] \u200b = ( \u2217 ) \u200b [ R d 1 \u00d7 d 1 \u2212 1 \ud835\udfce d 1 \u00d7 d 2 \ud835\udfce d 2 \u00d7 d 1 R d 2 \u00d7 d 2 \u2212 1 ] , superscript subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 2 subscript \ud835\udc51 1 subscript \ud835\udc51 2 \u22ba superscript matrix subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 1 subscript 0 subscript \ud835\udc51 1 subscript \ud835\udc51 2 subscript 0 subscript \ud835\udc51 2 subscript \ud835\udc51 1 subscript \ud835\udc45 subscript \ud835\udc51 2 subscript \ud835\udc51 2 \u22ba matrix superscript subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 1 \u22ba superscript subscript 0 subscript \ud835\udc51 2 subscript \ud835\udc51 1 \u22ba superscript subscript 0 subscript \ud835\udc51 1 subscript \ud835\udc51 2 \u22ba superscript subscript \ud835\udc45 subscript \ud835\udc51 2 subscript \ud835\udc51 2 \u22ba matrix superscript subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 1 1 subscript 0 subscript \ud835\udc51 1 subscript \ud835\udc51 2 subscript 0 subscript \ud835\udc51 2 subscript \ud835\udc51 1 superscript subscript \ud835\udc45 subscript \ud835\udc51 2 subscript \ud835\udc51 2 1 \\displaystyle R_{(d_{1}+d_{2})\\times(d_{1}+d_{2})}^{\\intercal}=\\begin{bmatrix}R_{d_{1}\\times d_{1}}&\\mathbf{0}_{d_{1}\\times d_{2}}\\\\\n\\mathbf{0}_{d_{2}\\times d_{1}}&R_{d_{2}\\times d_{2}}\\end{bmatrix}^{\\intercal}=\\begin{bmatrix}R_{d_{1}\\times d_{1}}^{\\intercal}&\\mathbf{0}_{d_{2}\\times d_{1}}^{\\intercal}\\\\\n\\mathbf{0}_{d_{1}\\times d_{2}}^{\\intercal}&R_{d_{2}\\times d_{2}}^{\\intercal}\\end{bmatrix}\\overset{(*)}{=}\\begin{bmatrix}R_{d_{1}\\times d_{1}}^{-1}&\\mathbf{0}_{d_{1}\\times d_{2}}\\\\\n\\mathbf{0}_{d_{2}\\times d_{1}}&R_{d_{2}\\times d_{2}}^{-1}\\end{bmatrix}, (13)\n\nwhere holds since and by assumption. Now, we compute :\n\nR ( d 1 + d 2 ) \u00d7 ( d 1 + d 2 ) \u2212 1 superscript subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 2 subscript \ud835\udc51 1 subscript \ud835\udc51 2 1 \\displaystyle R_{(d_{1}+d_{2})\\times(d_{1}+d_{2})}^{-1} = [ R d 1 \u00d7 d 1 \ud835\udfce d 1 \u00d7 d 2 \ud835\udfce d 2 \u00d7 d 1 R d 2 \u00d7 d 2 ] \u2212 1 absent superscript matrix subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 1 subscript 0 subscript \ud835\udc51 1 subscript \ud835\udc51 2 subscript 0 subscript \ud835\udc51 2 subscript \ud835\udc51 1 subscript \ud835\udc45 subscript \ud835\udc51 2 subscript \ud835\udc51 2 1 \\displaystyle=\\begin{bmatrix}R_{d_{1}\\times d_{1}}&\\mathbf{0}_{d_{1}\\times d_{2}}\\\\\n\\mathbf{0}_{d_{2}\\times d_{1}}&R_{d_{2}\\times d_{2}}\\end{bmatrix}^{-1} (14) = [ [ R d 1 \u00d7 d 1 \u2212 0 d 1 \u00d7 d 2 \u200b R d 2 \u00d7 d 2 \u2212 1 \u200b 0 d 2 \u00d7 d 1 ] \u2212 1 \ud835\udfce d 1 \u00d7 d 2 \ud835\udfce d 2 \u00d7 d 1 [ R d 2 \u00d7 d 2 \u2212 0 d 2 \u00d7 d 1 \u200b R d 1 \u00d7 d 1 \u2212 1 \u200b 0 d 1 \u00d7 d 2 ] \u2212 1 ] absent matrix superscript delimited-[] subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 1 subscript 0 subscript \ud835\udc51 1 subscript \ud835\udc51 2 superscript subscript \ud835\udc45 subscript \ud835\udc51 2 subscript \ud835\udc51 2 1 subscript 0 subscript \ud835\udc51 2 subscript \ud835\udc51 1 1 subscript 0 subscript \ud835\udc51 1 subscript \ud835\udc51 2 subscript 0 subscript \ud835\udc51 2 subscript \ud835\udc51 1 superscript delimited-[] subscript \ud835\udc45 subscript \ud835\udc51 2 subscript \ud835\udc51 2 subscript 0 subscript \ud835\udc51 2 subscript \ud835\udc51 1 superscript subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 1 1 subscript 0 subscript \ud835\udc51 1 subscript \ud835\udc51 2 1 \\displaystyle=\\begin{bmatrix}\\left[R_{d_{1}\\times d_{1}}-0_{d_{1}\\times d_{2}}R_{d_{2}\\times d_{2}}^{-1}0_{d_{2}\\times d_{1}}\\right]^{-1}&\\mathbf{0}_{d_{1}\\times d_{2}}\\\\\n\\mathbf{0}_{d_{2}\\times d_{1}}&\\left[R_{d_{2}\\times d_{2}}-0_{d_{2}\\times d_{1}}R_{d_{1}\\times d_{1}}^{-1}0_{d_{1}\\times d_{2}}\\right]^{-1}\\end{bmatrix} (15) = [ R d 1 \u00d7 d 1 \u2212 1 \ud835\udfce d 1 \u00d7 d 2 \ud835\udfce d 2 \u00d7 d 1 R d 2 \u00d7 d 2 \u2212 1 ] . absent matrix superscript subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 1 1 subscript 0 subscript \ud835\udc51 1 subscript \ud835\udc51 2 subscript 0 subscript \ud835\udc51 2 subscript \ud835\udc51 1 superscript subscript \ud835\udc45 subscript \ud835\udc51 2 subscript \ud835\udc51 2 1 \\displaystyle=\\begin{bmatrix}R_{d_{1}\\times d_{1}}^{-1}&\\mathbf{0}_{d_{1}\\times d_{2}}\\\\\n\\mathbf{0}_{d_{2}\\times d_{1}}&R_{d_{2}\\times d_{2}}^{-1}\\end{bmatrix}. (16)\n\nHence, we have that . Finally, we show that :\n\ndet ( R ( d 1 + d 2 ) \u00d7 ( d 1 + d 2 ) ) = det ( [ R d 1 \u00d7 d 1 \ud835\udfce d 1 \u00d7 d 2 \ud835\udfce d 2 \u00d7 d 1 R d 2 \u00d7 d 2 ] ) = det ( R d 1 \u00d7 d 1 ) \u200b det ( R d 2 \u00d7 d 2 ) \u200b = ( \u2217 ) \u200b 1 \u22c5 1 = 1 , subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 2 subscript \ud835\udc51 1 subscript \ud835\udc51 2 matrix subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 1 subscript 0 subscript \ud835\udc51 1 subscript \ud835\udc51 2 subscript 0 subscript \ud835\udc51 2 subscript \ud835\udc51 1 subscript \ud835\udc45 subscript \ud835\udc51 2 subscript \ud835\udc51 2 subscript \ud835\udc45 subscript \ud835\udc51 1 subscript \ud835\udc51 1 \u22c5 subscript \ud835\udc45 subscript \ud835\udc51 2 subscript \ud835\udc51 2 1 1 1 \\displaystyle\\det(R_{(d_{1}+d_{2})\\times(d_{1}+d_{2})})=\\det\\left(\\begin{bmatrix}R_{d_{1}\\times d_{1}}&\\mathbf{0}_{d_{1}\\times d_{2}}\\\\\n\\mathbf{0}_{d_{2}\\times d_{1}}&R_{d_{2}\\times d_{2}}\\end{bmatrix}\\right)=\\det(R_{d_{1}\\times d_{1}})\\det(R_{d_{2}\\times d_{2}})\\overset{(*)}{=}1\\cdot 1=1, (17)\n\nwhere holds since and by assumption. \u220e\n\nProposition 5. The VN-Transformer model (where ) shown in Figure 1 satisfies partial rotation invariance. Proof. \u2022\n\nFor convenience, we reparametrize the model as (with the number of object classes) where . It then suffices to show that . \u2022\n\nFirst, note that is -invariant, since it is composed of -equivariant operations followed by a -invariant operation. \u2022\n\nConsider the matrix , where is an arbitrary 3-dimensional rotation. From Lemma 1, . f concat \u200b ( [ X , A ] \u200b R ( 3 + d A ) \u00d7 ( 3 + d A ) ) \u200b = ( \u2217 ) \u200b f concat \u200b ( [ X , A ] ) subscript \ud835\udc53 concat \ud835\udc4b \ud835\udc34 subscript \ud835\udc45 3 subscript \ud835\udc51 \ud835\udc34 3 subscript \ud835\udc51 \ud835\udc34 subscript \ud835\udc53 concat \ud835\udc4b \ud835\udc34 \\displaystyle f_{\\text{concat}}([X,A]R_{(3+d_{A})\\times(3+d_{A})})\\overset{(*)}{=}f_{\\text{concat}}([X,A]) (18) \u21d2 \u21d2 \\displaystyle\\Rightarrow f concat \u200b ( [ X \u200b R + A \u200b \ud835\udfce d A \u00d7 3 , X \u200b \ud835\udfce 3 \u00d7 d A + A \u200b I d A \u00d7 d A ] ) = f concat \u200b ( [ X , A ] ) subscript \ud835\udc53 concat \ud835\udc4b \ud835\udc45 \ud835\udc34 subscript 0 subscript \ud835\udc51 \ud835\udc34 3 \ud835\udc4b subscript 0 3 subscript \ud835\udc51 \ud835\udc34 \ud835\udc34 subscript \ud835\udc3c subscript \ud835\udc51 \ud835\udc34 subscript \ud835\udc51 \ud835\udc34 subscript \ud835\udc53 concat \ud835\udc4b \ud835\udc34 \\displaystyle f_{\\text{concat}}([XR+A\\mathbf{0}_{d_{A}\\times 3},X\\mathbf{0}_{3\\times d_{A}}+AI_{d_{A}\\times d_{A}}])=f_{\\text{concat}}([X,A]) (19) \u21d2 \u21d2 \\displaystyle\\Rightarrow f concat \u200b ( [ X \u200b R , A ] ) = f concat \u200b ( [ X , A ] ) , subscript \ud835\udc53 concat \ud835\udc4b \ud835\udc45 \ud835\udc34 subscript \ud835\udc53 concat \ud835\udc4b \ud835\udc34 \\displaystyle f_{\\text{concat}}([XR,A])=f_{\\text{concat}}([X,A]), (20)\n\nwhere holds from -invariance of . Proposition 6. The VN-Transformer model (where ) shown in Figure 1 satisfies partial rotation equivariance. Proof. \u2022\n\nFor convenience, we reparametrize the model as where . It then suffices to show that . \u2022\n\nFirst, note that is -equivariant, since it is composed of -equivariant operations. \u2022\n\nConsider the matrix , where is an arbitrary 3-dimensional rotation. From Lemma 1, . f concat \u200b ( [ X , A ] \u200b R ( 3 + d A ) \u00d7 ( 3 + d A ) ) \u200b = ( \u2217 ) \u200b f concat \u200b ( [ X , A ] ) \u200b R ( 3 + d A ) \u00d7 ( 3 + d A ) subscript \ud835\udc53 concat \ud835\udc4b \ud835\udc34 subscript \ud835\udc45 3 subscript \ud835\udc51 \ud835\udc34 3 subscript \ud835\udc51 \ud835\udc34 subscript \ud835\udc53 concat \ud835\udc4b \ud835\udc34 subscript \ud835\udc45 3 subscript \ud835\udc51 \ud835\udc34 3 subscript \ud835\udc51 \ud835\udc34 \\displaystyle f_{\\text{concat}}([X,A]R_{(3+d_{A})\\times(3+d_{A})})\\overset{(*)}{=}f_{\\text{concat}}([X,A])R_{(3+d_{A})\\times(3+d_{A})} (21) \u21d2 \u21d2 \\displaystyle\\Rightarrow f concat \u200b ( [ X \u200b R + A \u200b \ud835\udfce d A \u00d7 3 , X \u200b \ud835\udfce 3 \u00d7 d A + A \u200b I d A \u00d7 d A ] ) subscript \ud835\udc53 concat \ud835\udc4b \ud835\udc45 \ud835\udc34 subscript 0 subscript \ud835\udc51 \ud835\udc34 3 \ud835\udc4b subscript 0 3 subscript \ud835\udc51 \ud835\udc34 \ud835\udc34 subscript \ud835\udc3c subscript \ud835\udc51 \ud835\udc34 subscript \ud835\udc51 \ud835\udc34 \\displaystyle f_{\\text{concat}}([XR+A\\mathbf{0}_{d_{A}\\times 3},X\\mathbf{0}_{3\\times d_{A}}+AI_{d_{A}\\times d_{A}}]) = [ f concat \u200b ( [ X , A ] ) ( : , : 3 ) \u200b R + f concat \u200b ( [ X , A ] ) ( : , 4 : ) \u200b \ud835\udfce d A \u00d7 3 , f concat \u200b ( [ X , A ] ) ( : , : 3 ) \u200b \ud835\udfce 3 \u00d7 d A + f concat \u200b ( [ X , A ] ) ( : , 4 : ) \u200b \ud835\udc08 d A \u00d7 d A ] \\displaystyle=\\Big{[}f_{\\text{concat}}([X,A])^{(:,:3)}R+f_{\\text{concat}}([X,A])^{(:,4:)}\\mathbf{0}_{d_{A}\\times 3},f_{\\text{concat}}([X,A])^{(:,:3)}\\mathbf{0}_{3\\times d_{A}}+f_{\\text{concat}}([X,A])^{(:,4:)}\\mathbf{I}_{d_{A}\\times d_{A}}\\Big{]} (22) \u21d2 \u21d2 \\displaystyle\\Rightarrow f concat \u200b ( [ X \u200b R , A ] ) = [ f concat \u200b ( [ X , A ] ) ( : , : 3 ) \u200b R , f concat \u200b ( [ X , A ] ) ( : , 4 : ) ] \\displaystyle f_{\\text{concat}}([XR,A])=\\left[f_{\\text{concat}}([X,A])^{(:,:3)}R,f_{\\text{concat}}([X,A])^{(:,4:)}\\right] (23) \u21d2 \u21d2 \\displaystyle\\Rightarrow f concat \u200b ( [ X \u200b R , A ] ) ( : , : 3 ) = f concat \u200b ( [ X , A ] ) ( : , : 3 ) \u200b R , \\displaystyle f_{\\text{concat}}([XR,A])^{(:,:3)}=f_{\\text{concat}}([X,A])^{(:,:3)}R, (24)\n\nwhere holds from -equivariance of . C.2 -approximate equivariance\n\nProposition 3 (Restated). is -approximately equivariant. This bound is tight when . Proof. Set :\n\nf \u200b ( X \u200b R ) \u2212 f \u200b ( X ) \u200b R \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc53 \ud835\udc4b \ud835\udc45 \\displaystyle f(XR)-f(X)R = ( W \u200b X \u200b R + \u03f5 \u200b U ) \u2212 ( W \u200b X + \u03f5 \u200b U ) \u200b R absent \ud835\udc4a \ud835\udc4b \ud835\udc45 italic-\u03f5 \ud835\udc48 \ud835\udc4a \ud835\udc4b italic-\u03f5 \ud835\udc48 \ud835\udc45 \\displaystyle=(WXR+\\epsilon U)-(WX+\\epsilon U)R (25) = \u03f5 \u200b U \u2212 \u03f5 \u200b U \u200b R absent italic-\u03f5 \ud835\udc48 italic-\u03f5 \ud835\udc48 \ud835\udc45 \\displaystyle=\\epsilon U-\\epsilon UR (26) \u21d2 \u0394 \u200b ( f , X , R ) 2 \u21d2 absent \u0394 superscript \ud835\udc53 \ud835\udc4b \ud835\udc45 2 \\displaystyle\\Rightarrow\\Delta(f,X,R)^{2} = \u2016 f \u200b ( X \u200b R ) \u2212 f \u200b ( X ) \u200b R \u2016 F 2 absent superscript subscript norm \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc39 2 \\displaystyle=||f(XR)-f(X)R||_{F}^{2} (27) = \u2211 c = 1 C \u2032 \u2016 \u03f5 \u200b ( U ( c ) \u2212 U ( c ) \u200b R ) \u2016 2 2 absent superscript subscript \ud835\udc50 1 superscript \ud835\udc36 \u2032 subscript superscript norm italic-\u03f5 superscript \ud835\udc48 \ud835\udc50 superscript \ud835\udc48 \ud835\udc50 \ud835\udc45 2 2 \\displaystyle=\\sum_{c=1}^{C^{\\prime}}||\\epsilon(U^{(c)}-U^{(c)}R)||^{2}_{2} (28) = \u03f5 2 \u200b \u2211 c = 1 C \u2032 \u2016 U ( c ) \u2212 U ( c ) \u200b R \u2016 2 2 absent superscript italic-\u03f5 2 superscript subscript \ud835\udc50 1 superscript \ud835\udc36 \u2032 subscript superscript norm superscript \ud835\udc48 \ud835\udc50 superscript \ud835\udc48 \ud835\udc50 \ud835\udc45 2 2 \\displaystyle=\\epsilon^{2}\\sum_{c=1}^{C^{\\prime}}||U^{(c)}-U^{(c)}R||^{2}_{2} (29) = \u03f5 2 \u200b \u2211 c = 1 C \u2032 \u2016 U ( c ) \u2016 2 2 + \u2016 U ( c ) \u200b R \u2016 2 2 \u2212 2 \u200b U ( c ) \u200b R \u22ba \u200b U ( c ) \u22ba absent superscript italic-\u03f5 2 superscript subscript \ud835\udc50 1 superscript \ud835\udc36 \u2032 superscript subscript norm superscript \ud835\udc48 \ud835\udc50 2 2 superscript subscript norm superscript \ud835\udc48 \ud835\udc50 \ud835\udc45 2 2 2 superscript \ud835\udc48 \ud835\udc50 superscript \ud835\udc45 \u22ba superscript \ud835\udc48 limit-from \ud835\udc50 \u22ba \\displaystyle=\\epsilon^{2}\\sum_{c=1}^{C^{\\prime}}||U^{(c)}||_{2}^{2}+||U^{(c)}R||_{2}^{2}-2U^{(c)}R^{\\intercal}U^{(c)\\intercal} (30) \u2264 \u03f5 2 \u200b \u2211 c = 1 C \u2032 \u2016 U ( c ) \u2016 2 2 + \u2016 U ( c ) \u200b R \u2016 2 2 + 2 \u200b U ( c ) \u200b U ( c ) \u22ba absent superscript italic-\u03f5 2 superscript subscript \ud835\udc50 1 superscript \ud835\udc36 \u2032 superscript subscript norm superscript \ud835\udc48 \ud835\udc50 2 2 superscript subscript norm superscript \ud835\udc48 \ud835\udc50 \ud835\udc45 2 2 2 superscript \ud835\udc48 \ud835\udc50 superscript \ud835\udc48 limit-from \ud835\udc50 \u22ba \\displaystyle\\leq\\epsilon^{2}\\sum_{c=1}^{C^{\\prime}}||U^{(c)}||_{2}^{2}+||U^{(c)}R||_{2}^{2}+2U^{(c)}U^{(c)\\intercal} (31) = \u03f5 2 \u200b \u2211 c = 1 C \u2032 4 \u200b \u2016 U ( c ) \u2016 2 2 absent superscript italic-\u03f5 2 superscript subscript \ud835\udc50 1 superscript \ud835\udc36 \u2032 4 superscript subscript norm superscript \ud835\udc48 \ud835\udc50 2 2 \\displaystyle=\\epsilon^{2}\\sum_{c=1}^{C^{\\prime}}4||U^{(c)}||_{2}^{2} (32) = 4 \u200b \u03f5 2 \u200b C \u2032 absent 4 superscript italic-\u03f5 2 superscript \ud835\udc36 \u2032 \\displaystyle=4\\epsilon^{2}{C^{\\prime}} (33) \u21d2 \u0394 \u200b ( f , X , R ) \u21d2 absent \u0394 \ud835\udc53 \ud835\udc4b \ud835\udc45 \\displaystyle\\Rightarrow\\Delta(f,X,R) \u2264 2 \u200b \u03f5 \u200b C \u2032 .",
    "vntransformer-23": "absent 2 italic-\u03f5 superscript \ud835\udc36 \u2032 \\displaystyle\\leq 2\\epsilon\\sqrt{C^{\\prime}}.",
    "vntransformer-24": "(34)\n\nLemma 2. Suppose\n\n1. (with ) is -approximately equivariant. 2. (with ) is -approximately equivariant and -Lipschitz (w.r.t. the Frobenius norm). Then the composition is -approximately equivariant. Proof. \u0394 \u200b ( g \u2218 f , X , R ) \u0394 \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \\displaystyle\\Delta(g\\circ f,X,R) = \u2016 g \u200b ( f \u200b ( X \u200b R ) ) \u2212 g \u200b ( f \u200b ( X ) ) \u200b R \u2016 F absent subscript norm \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc39 \\displaystyle=||g(f(XR))-g(f(X))R||_{F} (35) = \u2016 g \u200b ( f \u200b ( X \u200b R ) ) \u2212 g \u200b ( f \u200b ( X ) \u200b R ) + g \u200b ( f \u200b ( X ) \u200b R ) \u2212 g \u200b ( f \u200b ( X ) ) \u200b R \u2016 F absent subscript norm \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc39 \\displaystyle=||g(f(XR))-g(f(X)R)+g(f(X)R)-g(f(X))R||_{F} (36) \u2264 \u2016 g \u200b ( f \u200b ( X \u200b R ) ) \u2212 g \u200b ( f \u200b ( X ) \u200b R ) \u2016 F + \u2016 g \u200b ( f \u200b ( X ) \u200b R ) \u2212 g \u200b ( f \u200b ( X ) ) \u200b R \u2016 F absent subscript norm \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc39 subscript norm \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc39 \\displaystyle\\leq||g(f(XR))-g(f(X)R)||_{F}+||g(f(X)R)-g(f(X))R||_{F} (37) \u2264 ( \u2217 ) \u200b \u2016 g \u200b ( f \u200b ( X \u200b R ) ) \u2212 g \u200b ( f \u200b ( X ) \u200b R ) \u2016 F + \u03f5 g subscript norm \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc54 \ud835\udc53 \ud835\udc4b \ud835\udc45 \ud835\udc39 subscript italic-\u03f5 \ud835\udc54 \\displaystyle\\overset{(*)}{\\leq}||g(f(XR))-g(f(X)R)||_{F}+\\epsilon_{g} (38) \u2264 ( \u2217 \u2217 ) \u200b L g \u200b \u2016 f \u200b ( X \u200b R ) \u2212 f \u200b ( X ) \u200b R \u2016 F + \u03f5 g \\displaystyle\\overset{(**)}{\\leq}L_{g}||f(XR)-f(X)R||_{F}+\\epsilon_{g} (39) = L g \u200b \u0394 \u200b ( f , X , R ) + \u03f5 g absent subscript \ud835\udc3f \ud835\udc54 \u0394 \ud835\udc53 \ud835\udc4b \ud835\udc45 subscript italic-\u03f5 \ud835\udc54 \\displaystyle=L_{g}\\Delta(f,X,R)+\\epsilon_{g} (40) \u2264 ( \u2217 \u2217 \u2217 ) \u200b L g \u200b \u03f5 f + \u03f5 g , \\displaystyle\\overset{(***)}{\\leq}L_{g}\\epsilon_{f}+\\epsilon_{g}, (41)\n\nwhere holds from -approximate equivariance of , holds because is -Lipschitz, and holds from -approximate equivariance of . \u220e\n\nProposition 4 (Restated). Suppose we have functions (with ) for , satisfying the following:\n\n1. is -approximately equivariant for all . 2. is -Lipschitz (w.r.t. ) for all . Then, the composition is -approximately equivariant, where:\n\n\u03f5 1 \u200b \u2026 \u200b K \u225c L K \u200b ( \u22ef \u200b ( L 3 \u200b ( L 2 \u200b \u03f5 1 + \u03f5 2 ) + \u03f5 3 ) + \u22ef ) + \u03f5 K \u225c subscript italic-\u03f5 1 \u2026 \ud835\udc3e subscript \ud835\udc3f \ud835\udc3e \u22ef subscript \ud835\udc3f 3 subscript \ud835\udc3f 2 subscript italic-\u03f5 1 subscript italic-\u03f5 2 subscript italic-\u03f5 3 \u22ef subscript italic-\u03f5 \ud835\udc3e \\displaystyle\\epsilon_{1\\ldots K}\\triangleq L_{K}(\\cdots(L_{3}(L_{2}\\epsilon_{1}+\\epsilon_{2})+\\epsilon_{3})+\\cdots)+\\epsilon_{K} (42)\n\nProof. \u0394 \u200b ( f K \u2218 \u22ef \u2218 f 1 , X , R ) \u0394 subscript \ud835\udc53 \ud835\udc3e \u22ef subscript \ud835\udc53 1 \ud835\udc4b \ud835\udc45 \\displaystyle\\Delta(f_{K}\\circ\\dots\\circ f_{1},X,R) = \u0394 \u200b ( f K \u2218 ( f K \u2212 1 \u2218 \u22ef \u2218 f 1 ) , X , R ) absent \u0394 subscript \ud835\udc53 \ud835\udc3e subscript \ud835\udc53 \ud835\udc3e 1 \u22ef subscript \ud835\udc53 1 \ud835\udc4b \ud835\udc45 \\displaystyle=\\Delta(f_{K}\\circ(f_{K-1}\\circ\\dots\\circ f_{1}),X,R) (43) \u2264 ( K ) \u200b L K \u200b \u0394 \u200b ( f K \u2212 1 \u2218 \u22ef \u2218 f 1 , X , R ) + \u03f5 K \ud835\udc3e subscript \ud835\udc3f \ud835\udc3e \u0394 subscript \ud835\udc53 \ud835\udc3e 1 \u22ef subscript \ud835\udc53 1 \ud835\udc4b \ud835\udc45 subscript italic-\u03f5 \ud835\udc3e \\displaystyle~{}~{}\\overset{(K)}{\\leq}L_{K}\\Delta(f_{K-1}\\circ\\dots\\circ f_{1},X,R)+\\epsilon_{K} (44) \u2264 ( K \u2212 1 ) L K ( L K \u2212 1 ( \u0394 ( f K \u2212 2 \u2218 \u22ef \u2218 f 1 , X , R ) + \u03f5 K \u2212 1 ) + \u03f5 K \\displaystyle\\overset{(K-1)}{\\leq}L_{K}(L_{K-1}(\\Delta(f_{K-2}\\circ\\dots\\circ f_{1},X,R)+\\epsilon_{K-1})+\\epsilon_{K} (45) \u22ee \u22ee \\displaystyle\\hskip 14.0pt\\vdots (46) \u2264 ( 1 ) \u200b L K \u200b ( \u22ef \u200b ( L 3 \u200b ( L 2 \u200b \u0394 \u200b ( f 1 , X , R ) + \u03f5 2 ) + \u03f5 3 ) + \u22ef ) + \u03f5 K 1 subscript \ud835\udc3f \ud835\udc3e \u22ef subscript \ud835\udc3f 3 subscript \ud835\udc3f 2 \u0394 subscript \ud835\udc53 1 \ud835\udc4b \ud835\udc45 subscript italic-\u03f5 2 subscript italic-\u03f5 3 \u22ef subscript italic-\u03f5 \ud835\udc3e \\displaystyle~{}~{}~{}\\overset{(1)}{\\leq}L_{K}(\\cdots(L_{3}(L_{2}\\Delta(f_{1},X,R)+\\epsilon_{2})+\\epsilon_{3})+\\cdots)+\\epsilon_{K} (47) \u2264 L K \u200b ( \u22ef \u200b ( L 3 \u200b ( L 2 \u200b \u03f5 1 + \u03f5 2 ) + \u03f5 3 ) + \u22ef ) + \u03f5 K absent subscript \ud835\udc3f \ud835\udc3e \u22ef subscript \ud835\udc3f 3 subscript \ud835\udc3f 2 subscript italic-\u03f5 1 subscript italic-\u03f5 2 subscript italic-\u03f5 3 \u22ef subscript italic-\u03f5 \ud835\udc3e \\displaystyle~{}~{}~{}\\leq L_{K}(\\cdots(L_{3}(L_{2}\\epsilon_{1}+\\epsilon_{2})+\\epsilon_{3})+\\cdots)+\\epsilon_{K} (48)\n\nwhere hold from applying inequality equation 40 from Lemma 2 times (setting and at each step). \u220e\n\nFigure 8 illustrates the propagation of equivariance violations through a composition of 3 functions. Proposition 7. The layer is -Lipschitz w.r.t. the Frobenius norm, where is the spectral norm of . The same holds for . Proof. Consider . We can write:\n\n\u2016 W \u200b X 1 \u2212 W \u200b X 2 \u2016 F 2 superscript subscript norm \ud835\udc4a subscript \ud835\udc4b 1 \ud835\udc4a subscript \ud835\udc4b 2 \ud835\udc39 2 \\displaystyle||WX_{1}-WX_{2}||_{F}^{2} = \u2211 s = 1 S \u2016 W \u200b X 1 ( : , s ) \u2212 W \u200b X 2 ( : , s ) \u2016 2 2 absent superscript subscript \ud835\udc60 1 \ud835\udc46 superscript subscript norm \ud835\udc4a superscript subscript \ud835\udc4b 1 : \ud835\udc60 \ud835\udc4a superscript subscript \ud835\udc4b 2 : \ud835\udc60 2 2 \\displaystyle=\\sum_{s=1}^{S}||WX_{1}^{(:,s)}-WX_{2}^{(:,s)}||_{2}^{2} (49) = \u2211 s = 1 S \u2016 W \u200b ( X 1 ( : , s ) \u2212 X 2 ( : , s ) ) \u2016 2 2 absent superscript subscript \ud835\udc60 1 \ud835\udc46 superscript subscript norm \ud835\udc4a superscript subscript \ud835\udc4b 1 : \ud835\udc60 superscript subscript \ud835\udc4b 2 : \ud835\udc60 2 2 \\displaystyle=\\sum_{s=1}^{S}||W(X_{1}^{(:,s)}-X_{2}^{(:,s)})||_{2}^{2} (50) \u2264 \u2211 s = 1 S \u03c3 2 \u200b ( W ) \u200b \u2016 X 1 ( : , s ) \u2212 X 2 ( : , s ) \u2016 2 2 absent superscript subscript \ud835\udc60 1 \ud835\udc46 superscript \ud835\udf0e 2 \ud835\udc4a superscript subscript norm superscript subscript \ud835\udc4b 1 : \ud835\udc60 superscript subscript \ud835\udc4b 2 : \ud835\udc60 2 2 \\displaystyle\\leq\\sum_{s=1}^{S}\\sigma^{2}(W)||X_{1}^{(:,s)}-X_{2}^{(:,s)}||_{2}^{2} (51) = \u03c3 \u200b ( W ) 2 \u200b \u2211 s = 1 S \u2016 X 1 ( : , s ) \u2212 X 2 ( : , s ) \u2016 2 2 absent \ud835\udf0e superscript \ud835\udc4a 2 superscript subscript \ud835\udc60 1 \ud835\udc46 superscript subscript norm superscript subscript \ud835\udc4b 1 : \ud835\udc60 superscript subscript \ud835\udc4b 2 : \ud835\udc60 2 2 \\displaystyle=\\sigma(W)^{2}\\sum_{s=1}^{S}||X_{1}^{(:,s)}-X_{2}^{(:,s)}||_{2}^{2} (52) = \u03c3 \u200b ( W ) 2 \u200b \u2016 X 1 \u2212 X 2 \u2016 F 2 absent \ud835\udf0e superscript \ud835\udc4a 2 superscript subscript norm subscript \ud835\udc4b 1 subscript \ud835\udc4b 2 \ud835\udc39 2 \\displaystyle=\\sigma(W)^{2}||X_{1}-X_{2}||_{F}^{2} (53) \u21d2 \u2016 W \u200b X 1 \u2212 W \u200b X 2 \u2016 F \u21d2 absent subscript norm \ud835\udc4a subscript \ud835\udc4b 1 \ud835\udc4a subscript \ud835\udc4b 2 \ud835\udc39 \\displaystyle\\Rightarrow||WX_{1}-WX_{2}||_{F} \u2264 \u03c3 \u200b ( W ) \u200b \u2016 X 1 \u2212 X 2 \u2016 F absent \ud835\udf0e \ud835\udc4a subscript norm subscript \ud835\udc4b 1 subscript \ud835\udc4b 2 \ud835\udc39 \\displaystyle\\leq\\sigma(W){||X_{1}-X_{2}||_{F}} (54)\n\nTo see this for VN-LinearWithBias, note that \u2013 we can then show the same result using the above proof. \u220e\n\nC.3 Equivariance of VN-LayerNorm\n\nWe define the VN analog of the layer normalization operation as follows:\n\nVN-LayerNorm \u200b ( V ( n ) ) \u225c [ V ( n , c ) \u2016 V ( n , c ) \u2016 2 ] c = 1 C \u2299 LayerNorm \u200b ( [ \u2016 V ( n , c ) \u2016 2 ] c = 1 C ) \u200b \ud835\udfd9 1 \u00d7 3 \u225c VN-LayerNorm superscript \ud835\udc49 \ud835\udc5b direct-product superscript subscript delimited-[] superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 2 \ud835\udc50 1 \ud835\udc36 LayerNorm superscript subscript delimited-[] subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 2 \ud835\udc50 1 \ud835\udc36 subscript 1 1 3 \\displaystyle\\text{VN-LayerNorm}(V^{(n)})\\triangleq\\left[\\frac{V^{(n,c)}}{||V^{(n,c)}||_{2}}\\right]_{c=1}^{C}\\odot\\text{LayerNorm}\\left(\\left[||V^{(n,c)}||_{2}\\right]_{c=1}^{C}\\right)\\mathds{1}_{1\\times 3} (55)\n\nProposition 8. is rotation-equivariant. Proof. VN-LayerNorm \u200b ( V ( n ) \u200b R ) ( c ) VN-LayerNorm superscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \ud835\udc50 \\displaystyle\\textup{VN-LayerNorm}(V^{(n)}R)^{(c)} = V ( n , c ) \u200b R \u2016 V ( n , c ) \u200b R \u2016 2 \u200b LayerNorm \u200b ( [ \u2016 V ( n , c ) \u200b R \u2016 2 ] c \u2032 = 1 C ) ( c ) absent superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 \ud835\udc45 subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 \ud835\udc45 2 LayerNorm superscript superscript subscript delimited-[] subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 \ud835\udc45 2 superscript \ud835\udc50 \u2032 1 \ud835\udc36 \ud835\udc50 \\displaystyle=\\frac{V^{(n,c)}R}{||V^{(n,c)}R||_{2}}\\text{LayerNorm}\\left(\\left[||V^{(n,c)}R||_{2}\\right]_{c^{\\prime}=1}^{C}\\right)^{(c)} (56) = ( \u2217 ) \u200b V ( n , c ) \u200b R \u2016 V ( n , c ) \u2016 2 \u200b LayerNorm \u200b ( [ \u2016 V ( n , c ) \u2016 2 ] c \u2032 = 1 C ) ( c ) superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 \ud835\udc45 subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 2 LayerNorm superscript superscript subscript delimited-[] subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 2 superscript \ud835\udc50 \u2032 1 \ud835\udc36 \ud835\udc50 \\displaystyle\\overset{(*)}{=}\\frac{V^{(n,c)}R}{||V^{(n,c)}||_{2}}\\text{LayerNorm}\\left(\\left[||V^{(n,c)}||_{2}\\right]_{c^{\\prime}=1}^{C}\\right)^{(c)} (57) = [ V ( n , c ) \u2016 V ( n , c ) \u2016 2 \u200b LayerNorm \u200b ( [ \u2016 V ( n , c ) \u2016 2 ] c \u2032 = 1 C ) ( c ) ] \u200b R absent delimited-[] superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 2 LayerNorm superscript superscript subscript delimited-[] subscript norm superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 2 superscript \ud835\udc50 \u2032 1 \ud835\udc36 \ud835\udc50 \ud835\udc45 \\displaystyle=\\left[\\frac{V^{(n,c)}}{||V^{(n,c)}||_{2}}\\text{LayerNorm}\\left(\\left[||V^{(n,c)}||_{2}\\right]_{c^{\\prime}=1}^{C}\\right)^{(c)}\\right]R (58) = VN-LayerNorm \u200b ( V ( n ) ) ( c ) \u200b R absent VN-LayerNorm superscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 \ud835\udc45 \\displaystyle=\\textup{VN-LayerNorm}(V^{(n)})^{(c)}R (59) = [ VN-LayerNorm \u200b ( V ( n ) ) \u200b R ] ( c ) absent superscript delimited-[] VN-LayerNorm superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \ud835\udc50 \\displaystyle=[\\textup{VN-LayerNorm}(V^{(n)})R]^{(c)} (60) \u21d2 VN-LayerNorm \u200b ( V ( n ) \u200b R ) \u21d2 absent VN-LayerNorm superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \\displaystyle\\Rightarrow\\textup{VN-LayerNorm}(V^{(n)}R) = VN-LayerNorm \u200b ( V ( n ) ) \u200b R , absent VN-LayerNorm superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \\displaystyle=\\textup{VN-LayerNorm}(V^{(n)})R, (61)\n\nwhere holds from invariance of vector norms to rotations. \u220e\n\nC.4 Definitions of VN layers from Deng et al. (2021)\n\nVN-ReLU layer\n\nThe VN-ReLU layer is constructed as follows: from a given representation , we compute a feature set :\n\nq \u225c W \u200b V ( n ) , W \u2208 \u211d C \u00d7 C . formulae-sequence \u225c \ud835\udc5e \ud835\udc4a superscript \ud835\udc49 \ud835\udc5b \ud835\udc4a superscript \u211d \ud835\udc36 \ud835\udc36 \\displaystyle q\\triangleq WV^{(n)},~{}~{}~{}W\\in\\mathbb{R}^{C\\times C}. (62)\n\nThen, we compute a set of \u201clearnable directions\u201d :\n\nk \u225c U \u200b V ( n ) , U \u2208 \u211d C \u00d7 C . formulae-sequence \u225c \ud835\udc58 \ud835\udc48 superscript \ud835\udc49 \ud835\udc5b \ud835\udc48 superscript \u211d \ud835\udc36 \ud835\udc36 \\displaystyle k\\triangleq UV^{(n)},~{}~{}~{}U\\in\\mathbb{R}^{C\\times C}. (63)\n\nNote that are learnable square matrices. Finally, we compute the output of the VN-ReLU operation as follows:\n\nVN-ReLU \u200b ( V ( n ) ) ( c ) \u225c { q ( c ) \u200b if \u200b \u27e8 q ( c ) , k ( c ) \u27e9 \u2265 0 q ( c ) \u2212 \u27e8 q ( c ) , k ( c ) \u2016 k ( c ) \u2016 \u27e9 \u200b k ( c ) \u2016 k ( c ) \u2016 \u200b o.w. \u225c VN-ReLU superscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 cases superscript \ud835\udc5e \ud835\udc50 if superscript \ud835\udc5e \ud835\udc50 superscript \ud835\udc58 \ud835\udc50 0 otherwise superscript \ud835\udc5e \ud835\udc50 superscript \ud835\udc5e \ud835\udc50 superscript \ud835\udc58 \ud835\udc50 norm superscript \ud835\udc58 \ud835\udc50 superscript \ud835\udc58 \ud835\udc50 norm superscript \ud835\udc58 \ud835\udc50 o.w. otherwise \\displaystyle\\text{VN-ReLU}(V^{(n)})^{(c)}\\triangleq\\begin{cases}q^{(c)}~{}~{}\\text{if}~{}\\langle q^{(c)},k^{(c)}\\rangle\\geq 0\\\\\nq^{(c)}-\\langle q^{(c)},\\frac{k^{(c)}}{||k^{(c)}||}\\rangle\\frac{k^{(c)}}{||k^{(c)}||}~{}~{}\\text{o.w.}\\end{cases} (64)\n\nOtherwise stated: if the inner product between the feature and the learnable direction is positive, return , else return the projection of onto the plane defined by the direction . It can be readily shown that VN-ReLU is rotation-equivariant (for a proof, see Appendix C.5). VN-Invariant layer\n\nis defined as:\n\nVN-Invariant \u200b ( V ( n ) ; W ) \u225c V ( n ) \u200b VN-MLP \u200b ( V ( n ) ; W ) \u22ba , \u225c VN-Invariant superscript \ud835\udc49 \ud835\udc5b \ud835\udc4a superscript \ud835\udc49 \ud835\udc5b VN-MLP superscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc4a \u22ba \\displaystyle\\text{VN-Invariant}(V^{(n)};W)\\triangleq V^{(n)}\\text{VN-MLP}(V^{(n)};W)^{\\intercal}, (65)\n\nwhere is a composition of VN-Linear and VN-ReLU layers, and is the set of all learnable parameters in VN-MLP. It can be easily shown that VN-Invariant is rotation-invariant (see Appendix C.5 for a proof). VN-Batch Norm, VN-Pool\n\nFor rotation-equivariant analogs of the standard batch norm and pooling operations, we point the reader to Deng et al. (2021). C.5 Invariance & equivariance of VN layers of Deng et al. (2021)\n\nProposition 9. (Deng et al., 2021) is rotation-equivariant. Proof. VN-Linear \u200b ( V ( n ) \u200b R ; W ) \u225c W \u200b V ( n ) \u200b R = ( W \u200b V ( n ) ) \u200b R = VN-Linear \u200b ( V ( n ) ; W ) \u200b R \u225c VN-Linear superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \ud835\udc4a \ud835\udc4a superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \ud835\udc4a superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 VN-Linear superscript \ud835\udc49 \ud835\udc5b \ud835\udc4a \ud835\udc45 \\displaystyle\\textup{VN-Linear}(V^{(n)}R;W)\\triangleq WV^{(n)}R=(WV^{(n)})R=\\textup{VN-Linear}(V^{(n)};W)R (66)\n\nProposition 10. (Deng et al., 2021) is rotation-equivariant. Proof. VN-ReLU \u200b ( V ( n ) \u200b R ) ( c ) VN-ReLU superscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \ud835\udc50 \\displaystyle\\text{VN-ReLU}(V^{(n)}R)^{(c)} = ( \u2217 ) \u200b { q ( c ) \u200b R \u200b if \u200b \u27e8 q ( c ) \u200b R , k ( c ) \u200b R \u27e9 \u2265 0 q ( c ) \u200b R \u2212 \u27e8 q ( c ) \u200b R , k ( c ) \u200b R \u2016 k ( c ) \u200b R \u2016 2 \u27e9 \u200b k ( c ) \u200b R \u2016 k ( c ) \u200b R \u2016 2 \u200b o.w. cases superscript \ud835\udc5e \ud835\udc50 \ud835\udc45 if superscript \ud835\udc5e \ud835\udc50 \ud835\udc45 superscript \ud835\udc58 \ud835\udc50 \ud835\udc45 0 otherwise superscript \ud835\udc5e \ud835\udc50 \ud835\udc45 superscript \ud835\udc5e \ud835\udc50 \ud835\udc45 superscript \ud835\udc58 \ud835\udc50 \ud835\udc45 subscript norm superscript \ud835\udc58 \ud835\udc50 \ud835\udc45 2 superscript \ud835\udc58 \ud835\udc50 \ud835\udc45 subscript norm superscript \ud835\udc58 \ud835\udc50 \ud835\udc45 2 o.w. otherwise \\displaystyle\\overset{(*)}{=}\\begin{cases}q^{(c)}R~{}~{}~{}\\text{if}~{}\\langle q^{(c)}R,k^{(c)}R\\rangle\\geq 0\\\\\nq^{(c)}R-\\langle q^{(c)}R,\\frac{k^{(c)}R}{||k^{(c)}R||_{2}}\\rangle\\frac{k^{(c)}R}{||k^{(c)}R||_{2}}~{}~{}~{}\\text{o.w.}\\end{cases} (67) = ( \u2217 \u2217 ) \u200b { q ( c ) \u200b R \u200b if \u200b \u27e8 q ( c ) , k ( c ) \u27e9 \u2265 0 q ( c ) \u200b R \u2212 \u27e8 q ( c ) , k ( c ) \u2016 k ( c ) \u2016 2 \u27e9 \u200b k ( c ) \u200b R \u2016 k ( c ) \u2016 2 \u200b o.w. \\displaystyle\\overset{(**)}{=}\\begin{cases}q^{(c)}R~{}~{}~{}\\text{if}~{}\\langle q^{(c)},k^{(c)}\\rangle\\geq 0\\\\\nq^{(c)}R-\\langle q^{(c)},\\frac{k^{(c)}}{||k^{(c)}||_{2}}\\rangle\\frac{k^{(c)}R}{||k^{(c)}||_{2}}~{}~{}~{}\\text{o.w.}\\end{cases} (68) = [ { q ( c ) \u200b if \u200b \u27e8 q ( c ) , k ( c ) \u27e9 \u2265 0 q ( c ) \u2212 \u27e8 q ( c ) , k ( c ) \u2016 k ( c ) \u2016 \u27e9 \u200b k ( c ) \u2016 k ( c ) \u2016 2 \u200b o.w. ] \u200b R absent delimited-[] cases superscript \ud835\udc5e \ud835\udc50 if superscript \ud835\udc5e \ud835\udc50 superscript \ud835\udc58 \ud835\udc50 0 otherwise superscript \ud835\udc5e \ud835\udc50 superscript \ud835\udc5e \ud835\udc50 superscript \ud835\udc58 \ud835\udc50 norm superscript \ud835\udc58 \ud835\udc50 superscript \ud835\udc58 \ud835\udc50 subscript norm superscript \ud835\udc58 \ud835\udc50 2 o.w. otherwise \ud835\udc45 \\displaystyle=\\left[\\begin{cases}q^{(c)}~{}~{}~{}\\text{if}~{}\\langle q^{(c)},k^{(c)}\\rangle\\geq 0\\\\\nq^{(c)}-\\langle q^{(c)},\\frac{k^{(c)}}{||k^{(c)}||}\\rangle\\frac{k^{(c)}}{||k^{(c)}||_{2}}~{}~{}~{}\\text{o.w.}\\end{cases}\\right]R (69) = VN-ReLU \u200b ( V ( n ) ) ( c ) \u200b R absent VN-ReLU superscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc50 \ud835\udc45 \\displaystyle=\\text{VN-ReLU}(V^{(n)})^{(c)}R (70) = [ VN-ReLU \u200b ( V ( n ) ) \u200b R ] ( c ) absent superscript delimited-[] VN-ReLU superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \ud835\udc50 \\displaystyle=[\\text{VN-ReLU}(V^{(n)})R]^{(c)} (71) \u21d2 VN-ReLU \u200b ( V ( n ) \u200b R ) \u21d2 absent VN-ReLU superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \\displaystyle\\Rightarrow\\text{VN-ReLU}(V^{(n)}R) = VN-ReLU \u200b ( V ( n ) ) \u200b R , absent VN-ReLU superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \\displaystyle=\\text{VN-ReLU}(V^{(n)})R, (72)\n\nwhere holds because and are rotation-equivariant w.r.t.",
    "vntransformer-25": "and holds because vector inner products are rotation-invariant. \u220e\n\nProposition 11. (Deng et al., 2021) is rotation-invariant. Proof. VN-Invariant \u200b ( V ( n ) \u200b R ; W ) VN-Invariant superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \ud835\udc4a \\displaystyle\\text{VN-Invariant}(V^{(n)}R;W) = ( V ( n ) \u200b R ) \u200b VN-MLP \u200b ( V ( n ) \u200b R ; W ) \u22ba absent superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 VN-MLP superscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 \ud835\udc4a \u22ba \\displaystyle=(V^{(n)}R)\\text{VN-MLP}(V^{(n)}R;W)^{\\intercal} (73) = ( \u2217 ) \u200b V ( n ) \u200b R \u200b [ VN-MLP \u200b ( V ( n ) ; W ) \u200b R ] \u22ba superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 superscript delimited-[] VN-MLP superscript \ud835\udc49 \ud835\udc5b \ud835\udc4a \ud835\udc45 \u22ba \\displaystyle\\overset{(*)}{=}V^{(n)}R\\left[\\text{VN-MLP}(V^{(n)};W)R\\right]^{\\intercal} (74) = V ( n ) \u200b R \u200b R \u22ba \u200b VN-MLP \u200b ( V ( n ) ; W ) \u22ba absent superscript \ud835\udc49 \ud835\udc5b \ud835\udc45 superscript \ud835\udc45 \u22ba VN-MLP superscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc4a \u22ba \\displaystyle=V^{(n)}RR^{\\intercal}\\text{VN-MLP}(V^{(n)};W)^{\\intercal} (75) = V ( n ) \u200b VN-MLP \u200b ( V ( n ) ; W ) \u22ba absent superscript \ud835\udc49 \ud835\udc5b VN-MLP superscript superscript \ud835\udc49 \ud835\udc5b \ud835\udc4a \u22ba \\displaystyle=V^{(n)}\\text{VN-MLP}(V^{(n)};W)^{\\intercal} (76) = VN-Invariant \u200b ( V ( n ) ; W ) , absent VN-Invariant superscript \ud835\udc49 \ud835\udc5b \ud835\udc4a \\displaystyle=\\text{VN-Invariant}(V^{(n)};W), (77)\n\nwhere holds by equivariance of VN-MLP.",
    "vntransformer-26": "\u220e\n\n\u25c4 Feeling lucky?",
    "vntransformer-27": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Mon Mar 11 19:23:46 2024 by LaTeXML"
}