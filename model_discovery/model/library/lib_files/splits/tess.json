{
    "tess-0": "TESS: Text-to-Text Self-Conditioned Simplex Diffusion\n\nRabeeh Karimi Mahabadi1,4 Jaesung Tae2 Hamish Ivison3 James Henderson4 Iz Beltagy3 Matthew E.",
    "tess-1": "Peters3\u2217 Arman Cohan2,3 1EPFL 2Yale University 3Allen Institute for AI 4Idiap Research Institute rabeeh.karimimahabadi@epfl.ch, {jake.tae,arman.cohan}@yale.edu {hamishi,beltagy,matthewp}@allenai.org, james.henderson@idiap.ch Equal advising. Abstract\n\nDiffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various domains with continuous-valued inputs. Despite the promises of fully non-autoregressive text generation, applying diffusion models to natural language remains challenging due to its discrete nature. In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the typical learned embedding space. Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models and is competitive with pretrained autoregressive sequence-to-sequence models. 1 Introduction\n\nDiffusion models (sohl2015deep; ho2020denoising; song2021scorebased) have achieved state-of-the-art performance in various continuous-valued domains, such as image (nichol2021improved), audio (kong2020diffwave; Shen2023NaturalSpeech2L), video (ho2022video), and text-conditional image generation (saharia2022photorealistic; ramesh2022hierarchical). However, due to the inherently discrete nature of text, applying diffusion models to natural language remains a challenging task. Inspired by the success of diffusion for continuous domains, recent works have adapted diffusion to discrete spaces, such as text austin2021structured; hoogeboom2021argmax; savinov2021step; reid2022diffuser. One line of work proposes diffusing the model latent space by adding Gaussian noise to input word embeddings li2022diffusion. Another approach, SSD-LM (han2022ssd), adds noise to the vocabulary probability simplex. Direct diffusion on the probability simplex is desirable richemond2022categorical as it eliminates the need for an extra step to map diffused embeddings to actual discrete inputs or auxiliary methods such as binary encoding chen2022analog. Despite its strong performance, however, SSD-LM has several shortcomings. Notably, it is semi-autoregressive and restricted to generating text in blocks of 25 tokens, which hinders the potential benefits of full diffusion, e.g., the ability to perform arbitrary infilling, flexible generation, and a global view of the sequence. It also does not include self-conditioning chen2022analog, which has been shown to improve model performance strudel2022self. Moreover, SSD-LM lacks extensive comparisons against other embedding-based diffusion baselines for natural language tasks. In this work, we present TESS, a diffusion-based model for natural language tasks, which overcomes several limitations of prior works: restrictions on scale hoogeboom2021argmax; austin2021structured, dependence on pretrained embeddings strudel2022self, semi-autoregressive nature han2022ssd, and short generation length gong2022diffuseq. TESS closely follows han2022ssd by performing diffusion on the vocabulary logit space rather than the typical embedding space. Unlike SSD-LM, however, TESS is fully non-autoregressive and performs diffusion on the entire sequence. It also incorporates a novel form of self-conditioning, which empirically demonstrates competitive edge over the original self-conditioning method chen2022analog. We evaluate TESS on a suite of natural language generation (NLG) tasks including summarization, text simplification, paraphrase generation, and question generation. Our empirical results surpass the current state-of-the-art non-autoregressive and diffusion-based approaches and are on par with a strong pretrained encoder-decoder language model lewis2020bart. In particular, our proposed simplex-based self-conditioning method substantially improves generation quality. We also evaluate TESS on natural language understanding (NLU) tasks from the GLUE benchmark wang2018glue and show that it performs comparably to strong masked language model baselines.",
    "tess-2": "Our contributions can be summarized as follows. 1. We demonstrate the effectiveness of a fully non-autoregressive scheme for text diffusion models, which outperforms existing parallel and autoregressive methods. 2. We propose a new self-conditioning method that exploits the simplex semantics of the diffusion space. 3. We evaluate our approach over a suite of diverse NLG and NLU tasks, highlighting the effectiveness of our text-to-text simplex diffusion paradigm. We plan to release our trained models and code to promote open research in the field of diffusion-based text generation. 2 Background\n\nWe revisit continuous diffusion models (sohl2015deep), following the formulation of Denoising Diffusion Models (ho2020denoising; song2020denoising). Training\n\nGiven a sample from a data distribution , a forward diffusion process is a Markov chain that generates a sequence of latent variables by gradually adding Gaussian noise at each time step with variance :\n\nq \u200b ( \ud835\udc31 t | \ud835\udc31 t \u2212 1 ) = \ud835\udca9 \u200b ( \ud835\udc31 t ; 1 \u2212 \u03b2 t \u200b \ud835\udc31 t \u2212 1 , \u03b2 t \u200b \ud835\udc08 ) . \ud835\udc5e conditional subscript \ud835\udc31 \ud835\udc61 subscript \ud835\udc31 \ud835\udc61 1 \ud835\udca9 subscript \ud835\udc31 \ud835\udc61 1 subscript \ud835\udefd \ud835\udc61 subscript \ud835\udc31 \ud835\udc61 1 subscript \ud835\udefd \ud835\udc61 \ud835\udc08 \\displaystyle q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1},\\beta_{t}\\mathbf{I}). (1)\n\nLet , , and . Then sampling at an arbitrary time step has the closed-form solution\n\n\ud835\udc31 t = \u03b1 \u00af t \u200b \ud835\udc31 0 + 1 \u2212 \u03b1 \u00af t \u200b \u03f5 t . subscript \ud835\udc31 \ud835\udc61 subscript \u00af \ud835\udefc \ud835\udc61 subscript \ud835\udc31 0 1 subscript \u00af \ud835\udefc \ud835\udc61 subscript bold-italic-\u03f5 \ud835\udc61 \\displaystyle\\mathbf{x}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\bm{\\epsilon}_{t}. (2)\n\nGiven a well-behaved noise schedule , follows a stationary prior distribution . Therefore, if we can approximate the reverse process via a model with parameters , then we can sample random noise from a standard Gaussian and gradually denoise it to sample from . The reverse process is thus parametrized as\n\np \u03b8 \u200b ( \ud835\udc31 t \u2212 1 | \ud835\udc31 t ) = \ud835\udca9 \u200b ( \ud835\udf41 \ud835\udf3d \u200b ( \ud835\udc31 t , t ) , \ud835\udeba \ud835\udf3d \u200b ( \ud835\udc31 t , t ) ) . subscript \ud835\udc5d \ud835\udf03 conditional subscript \ud835\udc31 \ud835\udc61 1 subscript \ud835\udc31 \ud835\udc61 \ud835\udca9 subscript \ud835\udf41 \ud835\udf3d subscript \ud835\udc31 \ud835\udc61 \ud835\udc61 subscript \ud835\udeba \ud835\udf3d subscript \ud835\udc31 \ud835\udc61 \ud835\udc61 \\displaystyle p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})=\\mathcal{N}(\\bm{\\mu_{\\theta}}(\\mathbf{x}_{t},t),\\bm{\\Sigma_{\\theta}}(\\mathbf{x}_{t},t)). (3)\n\nThe model is trained by minimizing the mean squared error between the ground-truth data and its estimate :111Alternatively, we can train the model to predict the added noise; see ho2020denoising. See also song2021scorebased for a score-matching interpretation. \u2112 = \ud835\udd3c t , q \u200b ( \ud835\udc31 0 ) , q \u200b ( \ud835\udc31 t | \ud835\udc31 0 ) \u200b \u2016 \ud835\udc31 0 \u2212 \ud835\udc31 ^ \ud835\udf3d \u200b ( \ud835\udc31 t , t ) \u2016 2 . \u2112 subscript \ud835\udd3c \ud835\udc61 \ud835\udc5e subscript \ud835\udc31 0 \ud835\udc5e conditional subscript \ud835\udc31 \ud835\udc61 subscript \ud835\udc31 0 superscript norm subscript \ud835\udc31 0 subscript ^ \ud835\udc31 \ud835\udf3d subscript \ud835\udc31 \ud835\udc61 \ud835\udc61 2 \\displaystyle\\mathcal{L}=\\mathbb{E}_{t,q(\\mathbf{x}_{0}),q(\\mathbf{x}_{t}|\\mathbf{x}_{0})}\\|\\mathbf{x}_{0}-\\hat{\\mathbf{x}}_{\\bm{\\theta}}(\\mathbf{x}_{t},t)\\|^{2}. (4)\n\nNoise Schedule\n\nThe forward diffusion process is defined by a noise schedule. In this work, we follow the cosine schedule (nichol2021improved) for :\n\n\u03b1 \u00af t = f \u200b ( t ) f \u200b ( 0 ) , f ( t ) = cos ( t / T + s 1 + s .",
    "tess-3": "\u03c0 2 ) 2 . \\displaystyle\\bar{\\alpha}_{t}=\\frac{f(t)}{f(0)},\\quad f(t)=\\cos\\left(\\frac{t/T+s}{1+s}.\\frac{\\pi}{2}\\right)^{2}. (5)\n\nInference\n\nIn song2020denoising, model predictions are iteratively denoised for starting from pure noise, following\n\n\ud835\udc31 t \u2212 1 = \u03b1 t \u2212 1 \u200b \ud835\udc31 ^ \ud835\udf3d + 1 \u2212 \u03b1 t \u2212 1 \u22c5 \ud835\udc31 t \u2212 \u03b1 t \u200b \ud835\udc31 ^ \ud835\udf3d 1 \u2212 \u03b1 t . subscript \ud835\udc31 \ud835\udc61 1 subscript \ud835\udefc \ud835\udc61 1 subscript ^ \ud835\udc31 \ud835\udf3d \u22c5 1 subscript \ud835\udefc \ud835\udc61 1 subscript \ud835\udc31 \ud835\udc61 subscript \ud835\udefc \ud835\udc61 subscript ^ \ud835\udc31 \ud835\udf3d 1 subscript \ud835\udefc \ud835\udc61 \\mathbf{x}_{t-1}=\\sqrt{\\alpha_{t-1}}\\hat{\\mathbf{x}}_{\\bm{\\theta}}+\\sqrt{1-\\alpha_{t-1}}\\cdot\\frac{\\mathbf{x}_{t}-\\sqrt{\\alpha_{t}}\\hat{\\mathbf{x}}_{\\bm{\\theta}}}{\\sqrt{1-\\alpha_{t}}}. We derive a simplified form of this inference procedure in Appendix LABEL:app:inference-step, which allows us to apply diffusion to text without employing auxiliary methods that map categorical data to continuous space, and vice versa richemond2022categorical. 3 Method\n\nIn this section, we present TESS, a simplex diffusion-based text-to-text model. Building upon SSD-LM han2022ssd, we propose a fully non-autoregressive model with self-conditioning. Continuous Data Representation\n\nLet denote the vocabulary space. Following han2022ssd, we map the ID of each token to be generated to a -logit simplex to produce , whose -th component satisfies\n\ns ( i ) w = { k , if i = w , \u2212 k , otherwise , subscript superscript \ud835\udc60 \ud835\udc64 \ud835\udc56 cases \ud835\udc58 if \ud835\udc56 \ud835\udc64 \ud835\udc58 otherwise s^{w}_{(i)}=\\begin{cases}k,&\\text{if}\\quad i=w,\\\\\n-k,&\\text{otherwise},\\end{cases} (6)\n\nwith a hyperparameter . We then produce a probability simplex over via . Finally, we compute the weighted sum of word embeddings to obtain a continuous embedding vector, , where is the word embedding matrix, denotes the size of the hidden dimension, and . Time Step Embeddings\n\nAfter computing the continuous word embeddings, we add the time step embeddings to inform the model of the current time step. Our time step embedding is a linear layer, and we feed scaled time steps to this layer. The output is a time step embedding in that is added to to produce the final latent input vector. Fully Non-Autoregressive Modeling\n\nUnlike SSD-LM, which feeds small blocks of text to semi-autoregressively generate sequences of text, we feed the entire latent vector into an encoder transformer model. This is a key difference between our approach and SSD-LM, as it allows for a fully non-autoregressive model capable of generating sequences of any length. In practice, our evaluation tasks often require output sequences of 100 tokens or more, and by moving to a fully non-autoregressive paradigm, we are able to generate entire output sequences in parallel without resorting to semi-autoregressive generation. Forward Diffusion\n\nLet be a sentence of tokens such that , and be the -logit simplex representation of . We add noise to the -logit simplex representation during training according to\n\n\ud835\udc12 t = \u03b1 \u00af t \u200b \ud835\udc12 0 + 1 \u2212 \u03b1 \u00af t \u200b \u03f5 t , subscript \ud835\udc12 \ud835\udc61 subscript \u00af \ud835\udefc \ud835\udc61 subscript \ud835\udc12 0 1 subscript \u00af \ud835\udefc \ud835\udc61 subscript bold-italic-\u03f5 \ud835\udc61 \\displaystyle\\mathbf{S}_{t}=\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{S}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}}\\bm{\\epsilon}_{t}, (7)\n\nwhere subscript denotes the time step and . Training\n\nTypical diffusion models are trained with mean squared error loss as in Equation (4) to predict the ground-truth data. This objective is known to be unstable for text diffusion models (dieleman2022continuous). strudel2022self froze word embeddings and used specific scaling to deal with training instability. In this work, following han2022ssd, we instead compute the usual cross-entropy loss between the ground-truth tokens and the model prediction given a noisy logit simplex at time step .",
    "tess-4": "\u2112 \u2112 \\displaystyle\\mathcal{L} = \ud835\udd3c t , q \u200b ( \ud835\udc12 0 ) , q \u200b ( \ud835\udc12 t | \ud835\udc12 0 ) \u200b [ \u2212 \u2211 i = 1 L log \u2061 p \ud835\udf3d \u200b ( w i | \ud835\udc12 t , t ) ] . absent subscript \ud835\udd3c \ud835\udc61 \ud835\udc5e subscript \ud835\udc12 0 \ud835\udc5e conditional subscript \ud835\udc12 \ud835\udc61 subscript \ud835\udc12 0 delimited-[] superscript subscript \ud835\udc56 1 \ud835\udc3f subscript \ud835\udc5d \ud835\udf3d conditional subscript \ud835\udc64 \ud835\udc56 subscript \ud835\udc12 \ud835\udc61 \ud835\udc61 \\displaystyle=\\mathbb{E}_{t,q(\\mathbf{S}_{0}),q(\\mathbf{S}_{t}|\\mathbf{S}_{0})}\\left[-\\sum_{i=1}^{L}\\log p_{\\bm{\\theta}}(w_{i}|\\mathbf{S}_{t},t)\\right]. (8)\n\nSampling\n\nDuring inference, we sample from the prior and run the reverse process for on the noisy -logit simplex. The reverse process can be approximated via\n\n\ud835\udc12 t \u2212 1 = \u03b1 \u00af t \u2212 1 \u200b \ud835\udc12 ^ \ud835\udf3d \u200b ( \ud835\udc12 t , t ) + 1 \u2212 \u03b1 \u00af t \u2212 1 \u200b \u03f5 t . subscript \ud835\udc12 \ud835\udc61 1 subscript \u00af \ud835\udefc \ud835\udc61 1 subscript ^ \ud835\udc12 \ud835\udf3d subscript \ud835\udc12 \ud835\udc61 \ud835\udc61 1 subscript \u00af \ud835\udefc \ud835\udc61 1 subscript bold-italic-\u03f5 \ud835\udc61 \\displaystyle\\mathbf{S}_{t-1}=\\sqrt{\\bar{\\alpha}_{t-1}}\\hat{\\mathbf{S}}_{\\bm{\\theta}}(\\mathbf{S}_{t},t)+\\sqrt{1-\\bar{\\alpha}_{t-1}}\\bm{\\epsilon}_{t}. (9)\n\nSee Appendix LABEL:app:inference-step for details. This resembles the forward process in Equation (7), which allows for an intuitive interpretation: to reverse one step from , we take the model prediction as the hypothetical ground-truth, then corrupt it by time steps. To construct the model prediction, we project the logits predicted by the underlying encoder model via argmax as a pseudo-inverse of Equation (6) to match the initial -logit representation:\n\ns ^ ( i ) w = { k , if \u200b i = argmax \u200b ( \ud835\udc2c w ) , \u2212 k , otherwise . subscript superscript ^ \ud835\udc60 \ud835\udc64 \ud835\udc56 cases \ud835\udc58 if \ud835\udc56 argmax superscript \ud835\udc2c \ud835\udc64 \ud835\udc58 otherwise \\hat{s}^{w}_{(i)}=\\begin{cases}k,&\\text{if\\quad}i=\\text{argmax}(\\mathbf{s}^{w}),\\\\\n-k,&\\text{otherwise}.\\end{cases} (10)\n\nSelf-Conditioning\n\nIn typical diffusion models, the model predicts the original data conditioned on its corrupted version, i.e., , where denotes the estimate of at time step . In this setting, the model\u2019s estimates at previous time steps are discarded. However, in self-conditioning (chen2022analog), the model conditions its prediction on both and its previously generated output, i.e., . To adapt the model for self-conditioning, we stochastically zero out the self-condition such that\n\n\ud835\udc31 ^ 0 t = { \ud835\udc31 ^ \ud835\udf3d \u200b ( \ud835\udc31 t , \ud835\udc31 ^ 0 t + 1 , t ) , with probability \u03c1 \ud835\udc31 ^ \ud835\udf3d \u200b ( \ud835\udc31 t , 0 , t ) , otherwise , superscript subscript ^ \ud835\udc31 0 \ud835\udc61 cases subscript ^ \ud835\udc31 \ud835\udf3d subscript \ud835\udc31 \ud835\udc61 superscript subscript ^ \ud835\udc31 0 \ud835\udc61 1 \ud835\udc61 with probability \u03c1 subscript ^ \ud835\udc31 \ud835\udf3d subscript \ud835\udc31 \ud835\udc61 0 \ud835\udc61 otherwise \\hat{\\mathbf{x}}_{0}^{t}=\\begin{cases}\\hat{\\mathbf{x}}_{\\bm{\\theta}}(\\mathbf{x}_{t},\\hat{\\mathbf{x}}_{0}^{t+1},t),&\\text{with probability $\\rho$}\\\\\n\\hat{\\mathbf{x}}_{\\bm{\\theta}}(\\mathbf{x}_{t},0,t),&\\text{otherwise},\\end{cases} (11)\n\nwhere the self-conditioning previous prediction is computed as , with gradients detached.",
    "tess-5": "We always use self-conditioning during sampling. In practice, self-conditioning is implemented by concatenating and along the feature dimension and projecting them back to the original dimension with a linear layer chen2022analog. In contrast, we propose a new self-conditioning method that exploits the simplex nature of our diffusion space. Let be a noised -logit simplex for an arbitrary token .222We write as for brevity. Instead of concatenating the previous prediction with and re-projecting, we first compute the average of simplex probabilities as\n\n\ud835\udc29 avg w = 1 2 \u200b ( softmax \u200b ( \ud835\udc2c t ) + softmax \u200b ( \ud835\udc2c ^ 0 t + 1 ) ) . subscript superscript \ud835\udc29 \ud835\udc64 avg 1 2 softmax subscript \ud835\udc2c \ud835\udc61 softmax subscript superscript ^ \ud835\udc2c \ud835\udc61 1 0 \\displaystyle\\mathbf{p}^{w}_{\\text{avg}}=\\frac{1}{2}\\left(\\text{softmax}(\\mathbf{s}_{t})+\\text{softmax}(\\hat{\\mathbf{s}}^{t+1}_{0})\\right).",
    "tess-6": "(12)\n\nNote that is a well-defined categorical distribution over with non-negative components that sum to 1. We then compute a continuous embedding vector, . This is more efficient than the original self-conditioning method, which projects down the concatenated vectors. In Section \u00a7LABEL:sec:ablations, we also demonstrate the empirical effectiveness of this method over the original. Variable Sequence Length\n\nOne notable challenge in non-autoregressive generation is the assumption of fixed sequence lengths during inference. To overcome this issue, we follow prior work in embedding-space diffusion, which uses padding tokens li2022diffusion. Specifically, during training, we always pad the variable-length output sequence to a fixed length using padding tokens. These padding tokens are included when computing the cross-entropy loss so that TESS learns to generate them. During inference, we specify the maximum sequence length and run sampling as usual. Due to the variable number of padding tokens used during training, TESS flexibly generates a variable-length output sequence by appropriately adapting the number of padding tokens. We show in Section \u00a7LABEL:subsec:analysis-length that TESS is capable of producing variable-length outputs to match the underlying distribution of sequence lengths in the ground-truth data. 4 Experiments\n\nThis section details the experimental evaluation of our proposed method. Specifically, we compare TESS with the current state-of-the-art models for non-autoregressive text-to-text generation and pretrained encoder-decoder models of comparable size on several NLG and NLU datasets. Results show that TESS outperforms prior work in non-autoregressive generation and is comparable with autoregressive models of similar size. 4.1 Tasks and Datasets\n\nParaphrase Generation\n\nThe goal of paraphrase generation is to rephrase a sentence by generating an output that maintains the semantics of the original sentence, but with different words or sentence structures. We use the Quota Question Pairs (QQP) dataset,333https://www.kaggle.com/c/quora-question-pairs which is composed of 147K positive pairs. The dataset includes labels for whether a pair of questions has the same meaning. The positive pairs are used for paraphrase evaluation. Text Simplification\n\nThe objective of this task is to simplify complex sentences while retaining their original meaning. We use the NEWSELA-AUTO dataset (jiang2020neural), which is composed of 666K complex-simplified sentences. Question Generation\n\nThe objective of this task is to generate a question given an input context. We use the QUASAR-T dataset (dhingra2017quasar) processed by yuan2022seqdiffuseq, resulting in 119K document-question pairs. Summarization\n\nThis task entails generating a shorter version of a document while preserving important information. We evaluate our method on the CNN-DailyMail dataset (hermann2015teaching), which comprises 300K articles and summaries. Classification\n\nWe consider a set of classification tasks in the widely used GLUE benchmark (wang2018glue) covering a variety of tasks, including paraphrase detection (MRPC, QQP), sentiment classification (SST-2), natural language inference (MNLI,444We report the accuracy on the matched validation set. RTE, QNLI), and linguistic acceptability (CoLA).555Following devlin-etal-2019-bert; raffel2019exploring, as a common practice and due to the adversarial nature of WNLI with respect to the training set, we do not experiment with WNLI. 4.2 Baselines\n\nWe compare TESS to several autoregressive baselines as well as state-of-the-art text diffusion models. For autoregressive methods, we specifically consider the following fine-tuned models: GPT-2 radford2019language, as a standard autoregressive LM; BART lewis2020bart, a strong sequence-to-sequence pretrained model; and GPVAE-T5 du2022diverse, a latent-structured variable model and an extension to T5 raffel2019exploring capable of high-quality generations. For text diffusion models, we compare TESS to the following sequence-to-sequence diffusion models: Diffuser reid2022diffuser, DiffuSeq gong2022diffuseq, SeqDiffuSeq yuan2022seqdiffuseq, and SUNDAE (savinov2021step). We also compare TESS with LevT (gu2019levenshtein), a widely used iterative non-autoregressive model. Finally, given its strong relationship to our work, we also consider SSD-LM (han2022ssd) initialized from the same pretrained RoBERTa model as TESS and trained from the official SSD-LM codebase.666https://github.com/xhan77/ssd-lm\n\n4.3 Evaluation\n\nFor summarization, we use the standard ROUGE metric rouge, reporting ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (R-L) variants as done in prior text summarization work lewis2020bart. We quantify both generation quality and diversity. For quality, we report BLEU bleu, ROUGE-L rouge and BERTScore zhang2019bertscore following gong2022diffuseq and yuan2022seqdiffuseq. For diversity, we report distant unigrams (D-1) and diverse 4-grams (D-4) Deshpande2018FastDA. For text simplification, we use the standard SARI xu2016optimizing, and following gong2022diffuseq and yuan2022seqdiffuseq, we also include BLEU, BERTScore, and ROUGE-L metrics. 4.4 Implementation\n\nWe start from the RoBERTa pretrained checkpoint (roberta) and finetune the model on downstream tasks using our proposed self-conditioned simplex diffusion method. We initialize from a pretrained model as we find it results in faster convergence and improved performance in our pilot studies. Given the effectiveness of the diffusion-based training scheme, we expect it to scale to general pretraining and leave this area for future work. We use Hugging Face Transformers (wolf-etal2020transformers) for pretrained checkpoints, model implementation, as well as training and evaluation scripts; Datasets (lhoest-etal-2021-datasets) for accessing datasets and running evaluations; and Diffusers777https://huggingface.co/docs/diffusers as the base library for our diffusion model pipeline. Our experiments are performed on 8 NVIDIA A6000/A100 GPUs. The number of diffusion sampling steps at inference time is set to for generation and for classification tasks. During training, . We set the simplex scale to . Additional details are listed in Appendix LABEL:app:experimental_detials. 5 Results\n\n5.1 Paraphrase Generation\n\nAs seen in Table LABEL:tbl:paraphrase, TESS significantly outperforms GPT-2 and other non-autoregressive and diffusion baselines in quality metrics (BLEU, BERT, and ROUGE) while achieving parity in diversity metrics (D-1/D-4). Moreover, TESS obtains competitive overall performance with BART. Note that BART uses a denoising pretraining objective, which is substantially conducive to sequence-to-sequence tasks (lewis2020bart); we do not perform any additional pretraining beyond RoBERTa\u2019s checkpoint, which was only pretrained on the general masked language modeling objective. We suspect that TESS could significantly benefit from additional diffusion pretraining, as discussed in Section \u00a7LABEL:sec:future.",
    "tess-7": "\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Thu Feb 29 08:34:07 2024 by LaTeXML"
}