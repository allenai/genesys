{
    "dynsmoothssms-0": "# State Space Models are Comparable to Transformers in Estimating Functions with Dynamic Smoothness \n\nNaoki Nishikawa<br>The University of Tokyo<br>Tokyo, Japan<br>nishikawa-naoki259@g.ecc.u-tokyo.ac.jp\n\nTaiji Suzuki<br>The University of Tokyo, RIKEN AIP<br>Tokyo, Japan<br>taiji@mist.i.u-tokyo.ac.jp\n\n\n#### Abstract\n\nDeep neural networks based on state space models (SSMs) are attracting much attention in sequence modeling since their computational cost is significantly smaller than that of Transformers.",
    "dynsmoothssms-1": "While the capabilities of SSMs have been primarily investigated through experimental comparisons, theoretical understanding of SSMs is still limited. In particular, there is a lack of statistical and quantitative evaluation of whether SSM can replace Transformers. In this paper, we theoretically explore in which tasks SSMs can be alternatives of Transformers from the perspective of estimating sequence-to-sequence functions. We consider the setting where the target function has direction-dependent smoothness and prove that SSMs can estimate such functions with the same convergence rate as Transformers. Additionally, we prove that SSMs can estimate the target function, even if the smoothness changes depending on the input sequence, as well as Transformers. Our results show the possibility that SSMs can replace Transformers when estimating the functions in certain classes that appear in practice. ## 1 Introduction\n\nFoundation models based on Transformers have achieved remarkable success in various sequence modeling tasks such as natural language processing (Vaswani et al., 2017), computer vision (Dosovitskiy et al., 2020), and speech recognition (Radford et al., 2023). The superior performance of Transformers is attributed to the self-attention mechanism, which enables the model to aggregate the information from the input sequence. In contrast to its success, self-attention mechanism has a potential problem that it requires a large amount of computation and memory. To deal with this issue, many studies have been attempted to develop efficient models that can replace Transformers. Among them, Structured State Space Models (SSMs) have garnered considerable interest recently. One advantage of SSMs is that the output can be computed with a significantly small time using convolution via FFT algorithm or recursive computation. Based on the original SSMs, many improvements have been proposed, such as HiPPO-based intialization (Gu et al. 2021) and architectures using gated convolutions (Fu et al. 2022, Poli et al., 2023). Networks based on SSMs have accomplished high performance in various applications such as gene analysis (Nguyen et al., 2024), audio generation (Goel et al., 2022) and speech recognition (Saon et al. 2023). On the other hand, some of the recent studies pointed out the limitations of SSMs, especially for their abilities to solve tasks. For example, Merrill et al. (2024) show that SSMs cannot solve sequential problems from the view of computational complexity theory. Additionally, Gu and Dao (2023) pointed out that SSMs are less effective for the tasks to handle discrete and information-dense data such as language processing. Therefore, it is still unclear in what situation we can replace Transformers with SSMs. ![](https://cdn.mathpix.com/cropped/2024_09_12_5e098f2beaae0c90c478g-02.jpg?height=258&width=1373&top_left_y=245&top_left_x=368)\n\nFigure 1.1: Conceptual illustrations of the abilities of SSMs. Left: This represents the classification task in which the existence of the token \" $A$ \" or \" $B$ \" is important. We prove that SSMs can extract such tokens, even if the position of the important token is different for each input. Right: This illustrate the task to predict the next token in the sequence. In this task, models have to output the token associated with the last token. We also prove that SSMs can solve such tasks, i.e., SSMs can extract appropriate tokens even if the position of essential tokens are different for each token in the same sequence. Recently, some studies have theoretically investigated the abilities of SSMs. For instance, Wang and Xue (2024) show that SSMs are universal approximators for continuous sequence-to-sequence functions. Additionally, Massaroli et al. (2024) constructed the parameters of SSMs to solve the task called associated recall. Moreover, Cirone et al. (2024) studied the abilities of SSMs using rough path theory. However, they mainly focus on the expressive power of SSMs and do not provide statistical understanding. Furthermore, quantitative evaluations to compare SSMs and Transformers is limited. Leaving aside SSMs, many studies have investigated the abilities of deep neural networks to estimate functions. Some of them analyze the estimation abilities of fully connected neural networks (FNNs) with the assumption that the target function is in certain function classes (Schmidt-Hieber, 2020, Suzuki, 2018) or have a specific smoothness structure (Suzuki and Nitanda, 2021). Moreover, Nakada and Imaizumi (2020) and Chen et al. (2022) consider the setting that the data distribution has a low-dimensional structure. Additionally, Okumoto and Suzuki (2021) studied convolutional neural networks (CNNs) and showed that CNNs can estimate the functions that have smoothness structures with the minimax optimal rate even if the input is infinite-dimensional. As for the Transformers, Takakura and Suzuki (2023) showed that Transformers can estimate the functions with infinitedimensional input as well as CNNs. Additionally, they showed that Transformers can estimate the functions whose smoothness structure changes depending on the input. Our contributions. In this paper, we explore the abilities of SSMs with gated convolution to replace Transformers from the perspective of statistical learning theory. More specifically, we investigate the estimation ability of SSMs for the function classes called $\\gamma$-smooth and piecewise $\\gamma$-smooth. For the function in these classes, Takakura and Suzuki (2023) showed that Transformers can estimate them effectively. We prove that SSMs can also estimate those functions with the same convergence rate as Transformers, and show that SSMs can replace Transformers when estimating those functions. The essential point of the two function classes above is that they have smoothness structures. As for $\\gamma$-smooth functions, the smoothness of the function is the same for all input sequences, i.e., the important features to extract are fixed. On the other hand, piecewise $\\gamma$-smooth functions have different smoothness depending on the input. This function class characterizes the ability of Transformers and SSMs to extract important features dynamically. In addition to the settings considered in Takakura and Suzuki (2023), we also consider the functions whose smoothness structure also changes depending on the position output token. This setting is inspired by the ability of Transformers to solve the task called associative recall (Ba et al. 2016). We show that SSMs can also replace Transformers in this setting. See Figure 1.1 for the conceptual illustrations of those function classes. The contributions of this paper are summarized as follows:\n\n1. We theoretically investigate the estimation ability of SSMs with gated convolutions for $\\gamma$ smooth functions and piecewise $\\gamma$-smooth functions. We show that SSMs can achieve the same estimation error as Transformers, which implies that SSMs may be alternative to Transformers in terms of estimating functions in those classes. 2. Inspired by recent research on the abilities of Transformers, we define function classes that are extensions of the piecewise $\\gamma$-smooth function class. Then, we prove that SSMs can also attain the similar estimation error for those classes. Other related works. The function classes with piecewise smoothness are also considered in Petersen and Voigtlaender (2018) and Imaizumi and Fukumizu (2019). They do not consider anisotropic smoothness or the sequence-to-sequence functions, while we consider such situations. One of the other directions to investigate the abilities of SSMs is to utilize the control theory, like Alonso et al. (2024). Instead of focusing on the statistical aspect, they mainly provide a comprehensive understanding of existing SSMs. Gu and DaO (2023) proposed an SSM-based architecture called Mamba, whose filter is controlled by the input. While convolution with FFT algorithms cannot be used for Mamba, they proposed a hardware-aware efficient implementation. In this paper, we do not focus on the setting that filters are controlled by the input, and we consider SSMs with gated convolution with data-independent filters.",
    "dynsmoothssms-2": "Notations. For $l \\in \\mathbb{N}$, let $[l]$ be the set $\\{1, \\ldots, l\\}$ and for $l, r \\in \\mathbb{Z}(l \\leq r)$, let $[l: r]$ be the set $\\{l, \\ldots, r\\}$. For a set $S \\subseteq \\mathbb{R}$ and $d \\in \\mathbb{N}$, let $S^{d \\times \\infty}:=\\left\\{\\left[\\ldots, s_{-2}, s_{-1}, s_{0}\\right] \\mid s_{i} \\in S^{d}\\right\\}$. For $F: \\Omega \\rightarrow \\mathbb{R}^{l}$, let $\\|F\\|_{\\infty}:=\\sup _{X \\in \\Omega}\\|F(X)\\|_{\\infty}$. For the probability measure $P_{X}$ on $\\Omega$ and $p>0$, the norm $\\|\\cdot\\|_{p, P_{X}}$ is defined by $\\|f\\|_{p, P_{X}}=\\left(\\int_{\\Omega}\\|f(X)\\|_{p}^{p} \\mathrm{~d} P_{X}\\right)^{1 / p}$. For a matrix $A$, let $\\|A\\|_{0}=$ $\\left|\\left\\{(i, j) \\mid A_{i j} \\neq 0\\right\\}\\right|$. For $F: \\mathbb{R}^{d \\times \\infty} \\rightarrow \\mathbb{R}^{1 \\times \\infty}$, we denote $F=\\left(F_{j}\\right)_{j=-\\infty}^{\\infty}$ with $F_{j}:[0,1]^{d \\times \\infty} \\rightarrow \\mathbb{R}$. ## 2 Problem Settings\n\nIn this paper, we consider a non-parametric regression problem where the input is infinite-dimensional. More concretely, we suppose that the input $X:={ }_{x} x_{i=-\\infty}^{0} \\in \\mathbb{R}^{d \\times \\infty}$ is a sequence of $d$-dimensional tokens ${ }^{1}$. Let $P_{X}$ be a probability measure on $\\left([0,1]^{d \\times \\infty}, \\mathcal{B}\\left([0,1]^{d \\times \\infty}\\right)\\right)$, and denote $\\Omega:=\\operatorname{supp}\\left(P_{X}\\right)$. We assume that $P_{X}$ is shift-invariant, i.e., for any $i \\in \\mathbb{Z}$ and $B \\in \\mathcal{B}\\left([0,1]^{d \\times \\infty}\\right)$, it holds that $P_{X}(B)=P_{X}\\left(\\left\\{\\Sigma_{i}(X) \\mid X \\in B\\right\\}\\right)$ for any $i \\in \\mathbb{N}$, where $\\Sigma_{j}$ is the shift operator $\\Sigma_{j}: \\mathbb{R}^{d \\times \\infty} \\rightarrow$ $\\mathbb{R}^{d \\times \\infty}$ defined by $\\left(\\Sigma_{j}(X)\\right)_{i}=x_{i-j}$ for $X=\\left[x_{j}\\right]_{j=-\\infty}^{0} \\in \\mathbb{R}^{d \\times \\infty}$. We consider the setting where the output is also infinite-dimensional. As same as the usual nonparametric regression setting, suppose that we observe $n$ i.i.d. inputs $X^{(i)} \\sim P_{X}(i=1, \\ldots, n)$ and the corresponding outputs $Y^{(i)} \\in \\mathbb{R}^{1 \\times \\infty}$ generated by $Y^{(i)}=F^{\\circ}\\left(X^{(i)}\\right)+\\xi^{(i)}$, where $\\xi^{(i)} \\in \\mathbb{R}^{d \\times \\infty}$ is the i.i.d. noise generated from $\\mathcal{N}\\left(0, \\sigma^{2}\\right)(\\sigma>0)$. We further assume that $\\left\\{\\xi^{(i)}\\right\\}_{i=1}^{n}$ is independent of the inputs $\\left\\{X^{(i)}\\right\\}_{i=1}^{n}$. Given the pairs of inputs and outputs $\\left\\{\\left(X^{(i)}, Y^{(i)}\\right)\\right\\}_{i=1}^{n}$, we obtain the estimator $\\hat{F}$ of the target function $F$ through empirical risk minimization:\n\n$$\n\\hat{F}=\\arg \\min _{F \\in \\mathcal{S}} \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=l}^{r}\\left(Y^{(i)}-F^{\\circ}\\left(X^{(i)}\\right)\\right)^{2}\n$$\n\nwhere $\\mathcal{S}$ is supposed to be the class of networks that we define in the next section. To measure the statistical performance of the estimator $\\hat{F}$, we utilize mean squared error (MSE) defined by\n\n$$\nR_{l, r}\\left(\\hat{F}, F^{\\circ}\\right)=\\frac{1}{l-r+1} \\sum_{j=l}^{r} \\mathbb{E}\\left[\\left\\|\\hat{F}_{j}(X)-F_{j}^{\\circ}(X)\\right\\|_{2, P_{X}}^{2}\\right]\n$$\n\nwhere the expectation is taken for $\\left\\{\\xi^{(i)}\\right\\}_{i=1}^{n}$.",
    "dynsmoothssms-3": "Note that we consider the average error over the finite segment $[l: r]$ to avoid the technical difficulty of infinite-dimensionality. However, the estimation error analysis in Section 5 is independent of the choice of $l$ and $r$. Similar to the setting of Takakura and Suzuki (2023), we assume that target function is shift equivariant. That is, we impose that the target function $F: \\mathbb{R}^{d \\times \\infty} \\rightarrow \\mathbb{R}^{d \\times \\infty}$ satisfies $F\\left(\\Sigma_{j}(X)\\right)=$ $\\Sigma_{j}(F(X))$ for any $j \\in \\mathbb{N}$. This is a natural assumption in various applications such as language processing, audio processing, and time-series analysis. Additionally, we assume that the target function $F$ is included in a certain function classes defined in the remainings of this section. ## $2.1 \\gamma$-smooth function class\n\nHere, we introduce the $\\gamma$-smooth function class, which was first proposed by Okumoto and Suzuki (2021). First of all, for $r \\in \\mathbb{Z}_{0}^{d \\times \\infty}$, we define $\\psi_{r_{i j}}:[0,1] \\rightarrow \\mathbb{R}$ by\n\n$$\n\\psi_{r_{i j}}(x):= \\begin{cases}\\sqrt{2} \\cos \\left(2 \\pi\\left|r_{i j}\\right| x\\right) & \\left(r_{i j}<0\\right) \\\\ 1 & \\left(r_{i j}=0\\right) \\\\ \\sqrt{2} \\sin \\left(2 \\pi\\left|r_{i j}\\right| x\\right) & \\left(r_{i j}>0\\right)\\end{cases}\n$$\n\n[^0]and $\\psi_{r}:[0,1]^{d \\times \\infty} \\rightarrow \\mathbb{R}$ by $\\psi_{r}(X)=\\prod_{i=1} \\prod_{j=1} \\psi_{r_{i j}}\\left(X_{i j}\\right)$. Then, $\\left\\{\\psi_{r}\\right\\}_{r \\in \\mathbb{Z}_{0}^{d \\times \\infty}}$ forms a complete orthonormal system of $L^{2}\\left([0,1]^{d \\times \\infty}\\right)$, Therefore, any $f \\in L^{2}\\left([0,1]^{d \\times \\infty}\\right)$ can be expanded as $f=\\sum_{r \\in \\mathbb{Z}_{0}^{d \\times \\infty}}\\left\\langle f, \\psi_{r}\\right\\rangle \\psi_{r}$. For $s \\in \\mathbb{N}_{0}^{d \\times \\infty}$, we define\n$$\n\\delta_{s}(f):=\\sum_{r \\in \\mathbb{Z}_{0}^{d \\times \\infty},\\left\\lfloor 2^{s_{i j}-1}\\right\\rfloor \\leq r_{i j}<2^{s_{i j}}}\\left\\langle f, \\psi_{r}\\right\\rangle \\psi_{r}\n$$\n\nThen, we define the $\\gamma$-smooth function class as follows. Definition 2.1 ( $\\gamma$-smooth function class). For a given $\\gamma: \\mathbb{N}_{0}^{d \\times \\infty} \\rightarrow \\mathbb{R}$ which is monotonically non-decreasing with respect to each coordinate and $p \\geq 2, \\theta \\geq 1$, we define the $\\gamma$-smooth function space as follows:\n\n$$\n\\mathcal{F}_{p, \\theta}^{\\gamma}\\left([0,1]^{d \\times \\infty}\\right):=\\left\\{f \\in L^{2}\\left([0,1]^{d \\times \\infty}\\right) \\mid\\|f\\|_{\\mathcal{F}_{p, \\theta}^{\\gamma}}<\\infty\\right\\}\n$$\n\nwhere the norm $\\|f\\|_{\\mathcal{F}_{p, \\theta}^{\\gamma}}$ is defined as $\\|f\\|_{\\mathcal{F}_{p, \\theta}^{\\gamma}}:=\\left(\\sum_{s \\in \\mathbb{N}_{0}^{d \\times \\infty}} 2^{\\theta \\gamma(s)}\\left\\|\\delta_{s}(f)\\right\\|_{p, P_{X}}^{\\theta}\\right)^{1 / \\theta}$. We also define the finite dimensional version of $\\gamma$-smooth function space $\\mathcal{F}_{p, \\theta}^{\\gamma}\\left([0,1]^{d \\times l}\\right)$ for $l \\in \\mathbb{N}$ in the same way. Note that $\\delta_{s}(f)$ can be seen as the frequency component of $f$ with frequency $\\left|r_{i j}\\right| \\sim 2^{s_{i j}}$ toward each coordinate. Therefore, we can interpret that $\\gamma$ controls the amplitude of each frequency component through weighting the term $\\left\\|\\delta_{s}(f)\\right\\|_{p, P_{X}}$ in the norm. In other words, if $\\gamma(s)$ is larger, the norm of frequency component $\\delta_{s}(f)$ is smaller. As a special case of $\\gamma$, we consider the following two types of smoothness:\nDefinition 2.2 (Mixed and anisotropic smoothness). For $a \\in \\mathbb{R}_{>0}^{d \\times \\infty}$, mixed smoothness and anisotropic smoothness is defined as follows:\n\n$$\n\\begin{array}{ll}\n\\text { (Mixed smoothness) } & \\gamma(s)=\\langle a, s\\rangle \\\\\n\\text { (Anisotropic smoothness) } & \\gamma(s)=\\max \\left\\{a_{i j} s_{i j} \\mid i \\in[d], j \\in \\mathbb{Z}\\right\\}\n\\end{array}\n$$\n\nFor each $i \\in[d], j \\in \\mathbb{Z}$, the parameter $a_{i j}$ can be viewed as the smoothness for the coordinate $X_{i j}$. When $a_{i j}$ is large, $\\gamma(s)$ with $s_{i j} \\neq 0$ increases and the frequency component $\\delta_{s}(f)$ with $s_{i j} \\neq 0$ becomes smaller. In contrast, small $a_{i j}$ implies that the function is not smooth towards the coordinate $(i, j)$, which implies that $X_{i j}$ is an important feature. The function class $\\mathcal{F}_{p, \\theta}^{\\gamma}\\left([0,1]^{d \\times \\infty}\\right)$ can be seen as extension of some famous function spaces to the infinite-dimensional setting. Indeed, if $P_{X}$ is uniform distribution on $[0,1]^{1 \\times l}$ and $p<\\infty$, then $\\mathcal{F}_{p, \\theta}^{\\gamma}\\left([0,1]^{1 \\times l}\\right)$ with mixed smoothness is equivalent to the mixed-Besov space. Moreover, if $P_{X}$ is uniform distribution, then the anisotropic Sobolev space included in the unit ball of $\\mathcal{F}_{2,2}^{\\gamma}$ with anisotropic smoothness. At the close of this subsection, we introduce some notation related to the smoothness parameter $a$.",
    "dynsmoothssms-4": "Let $\\bar{a}$ be the sequence obtained by sorting $a$ in ascending order. That is, $\\bar{a}=\\left[a_{i_{1}, j_{1}}, \\ldots, a_{i_{k}, j_{k}}, \\ldots\\right]$ satisfies $a_{i_{k}, j_{k}} \\leq a_{i_{k+1}, j_{k+1}}$ for any $k \\in \\mathbb{N}$. Then, we define weak $l^{\\alpha}$-norm for $\\alpha>0$ as $\\|a\\|_{w l^{\\alpha}}:=$ $\\sup _{j} j^{\\alpha} \\bar{a}_{j}^{-1}$. Additionally, we define $a^{\\dagger}=\\bar{a}_{1}$ for the mixed smoothness and $a^{\\dagger}=\\left(\\sum_{i=1}^{\\infty} \\bar{a}_{i}^{-1}\\right)^{-1}$ for the anisotropic smoothness. ### 2.2 Piecewise $\\gamma$-smooth function class\n\nIn this subsection, we describe the piecewise $\\gamma$-smooth function class. This was firstly proposed by Takakura and Suzuki (2023) to clarify the advantage of Transformers compared to CNNs. More specifically, they pointed out that Transformers have ability to determine which features to extract depending on the input, while the estimation error analysis for $\\gamma$-smooth function class does not reflect such ability. Since piecewise $\\gamma$-smooth function has different smoothness depending on the input unlike $\\gamma$-smooth function, it is necessary to choose appropriate features to extract when estimating a function in this class. The rigourous definition of piecewise $\\gamma$-smooth function class is given as follows. Definition 2.3 (Piecewise $\\gamma$-smooth function class). For an index set $\\Lambda$, let $\\left\\{\\Omega_{\\lambda}\\right\\}_{\\lambda \\in \\Lambda}$ be a disjoint partition of $\\Omega$. That is, $\\left\\{\\Omega_{\\lambda}\\right\\}_{\\lambda \\in \\Lambda}$ satisfies $\\Omega=\\bigcup_{\\lambda \\in \\Lambda} \\Omega_{\\lambda}, \\Omega_{\\lambda} \\cap \\Omega_{\\lambda^{\\prime}}=\\emptyset\\left(\\lambda \\neq \\lambda^{\\prime}\\right)$. For $V \\in \\mathbb{N}$ and a set of bijections $\\left\\{\\pi_{\\lambda}\\right\\}_{\\lambda \\in \\Lambda}$ between $[V+1]$ and $[-V: 0]$, define $\\Pi_{\\lambda}: \\mathbb{R}^{d \\times[-V: 0]} \\rightarrow \\mathbb{R}^{d \\times(V+1)}$ and $\\Pi: \\Omega \\rightarrow \\mathbb{R}^{d \\times(V+1)}$ by\n\n$$\n\\begin{aligned}\n\\Pi_{\\lambda}\\left(\\left[x_{-V}, \\ldots, x_{0}\\right]\\right) & :=\\left[x_{\\pi_{\\lambda}(1)}, \\ldots, x_{\\pi_{\\lambda}(V+1)}\\right] \\\\\n\\Pi(X) & :=\\Pi_{\\lambda}(X[-V: 0]) \\quad \\text { if } X \\in \\Omega_{\\lambda}\n\\end{aligned}\n$$\n\nThen, for $p \\geq 2, \\theta \\geq 1$ and $\\gamma: \\mathbb{N}_{0}^{d \\times \\infty} \\rightarrow \\mathbb{R}$, the function class with piecewise $\\gamma$-smoothness is defined as follows:\n\n$$\n\\mathcal{P}_{p, \\theta}^{\\gamma}(\\Omega):=\\left\\{g=f \\circ \\Pi \\mid f \\in \\mathcal{F}_{p, \\theta}^{\\gamma}\\left([0,1]^{d \\times(V+1)}\\right),\\|g\\|_{\\mathcal{P}_{p, \\theta}^{\\gamma}}<\\infty\\right\\}\n$$\n\nwhere the norm $\\|g\\|_{\\mathcal{P}_{p, \\theta}^{\\gamma}}$ is defined by $\\|g\\|_{\\mathcal{P}_{p, \\theta}^{\\gamma}}:=\\left(\\sum_{s \\in \\mathbb{N}_{0}^{d \\times[-V: 0]}} 2^{\\theta \\gamma(s)}\\left\\|\\delta_{s}(f) \\circ \\Pi\\right\\|_{p, P_{X}}^{\\theta}\\right)^{1 / \\theta}$. For $g \\in \\mathcal{P}_{p, \\theta}^{\\gamma}(\\Omega)$ and $X \\in \\Omega_{\\lambda_{1}}, Y \\in \\Omega_{\\lambda_{2}}$ with $\\lambda_{1} \\neq \\lambda_{2}$, the smoothness parameter of $g$ at $X$ and $Y$ are different. This means that the coordinates of important features can change depending on the input. Takakura and Suzuki (2023) gives the convergence rate of the estimation error, and show that it almost coincides the rate for $\\gamma$-smooth functions. This reveals the ability of Transformers to extract important features depending on the input. To express how the disjoint partitions $\\left\\{\\Omega_{\\lambda}\\right\\}_{\\lambda \\in \\Lambda}$ are determined, we introduce the importance function defined as follows. Definition 2.4 (importance function). A function $\\mu: \\Omega \\rightarrow \\mathbb{R}^{\\infty}$ is called an importance function for $\\left\\{\\Omega_{\\lambda}\\right\\}_{\\lambda \\in \\Lambda}$ if $\\mu$ satisfies $\\Omega_{\\lambda}=\\left\\{X \\in \\Omega \\mid \\mu(X)_{\\pi_{\\lambda}(1)}>\\cdots>\\mu(X)_{\\pi_{\\lambda}(V+1)}\\right\\}$. In simple terms, the paritions $\\left\\{\\Omega_{\\lambda}\\right\\}_{\\lambda \\in \\Lambda}$ are determined to sort the inputs in descending order of the importance function. As same as Takakura and Suzuki (2023), we assume that an importance function $\\mu$ is well-separated, i.e., for some constant $c, \\beta>0, \\mu$ satisfies $\\mu(X)_{\\pi_{\\lambda}(i)} \\geq \\mu(X)_{\\pi_{\\lambda}(i+1)}+i^{-\\beta}$ for any $X \\in \\Omega_{\\lambda}$. This implies the probability that $X$ satisfies $\\mu(X)_{i} \\simeq \\mu(X)_{j}(i \\neq j)$ is zero. Similar assumption can be found in the literature of statistics such as Hall and Horowitz (2007). ### 2.3 Function class for the importance function\n\nAs for the importance function $\\mu$, Takakura and Suzuki (2023) assume that it belongs to $\\gamma$-smooth function class. Consequently, the value of importance of a token is unique in the input sequence, and does not change depending on the position of the output token. In order to consider the setting where the importance of tokens can change depending on the output token, in addition to the case where $\\mu$ is (i) $\\gamma$-smooth, we consider the following case:\n(ii) similarity of features: Let $\\zeta_{1}, \\ldots, \\zeta_{d^{\\prime}} \\in \\mathcal{F}_{p, \\theta}^{\\gamma}$, and $v_{j}:=\\left[\\zeta_{i}\\left(\\Sigma_{j} X\\right)\\right]_{i=1}^{d^{\\prime}} \\in \\mathbb{R}^{d^{\\prime}}(j \\in \\mathbb{N})$. Suppose that $\\left\\|v_{j}\\right\\|_{2} \\leq 1$ for any $j \\in \\mathbb{N}$. Then, the importance function $\\mu$ is can be represented by $\\mu(X)=\\left[-\\left\\|v_{0}-v_{j}\\right\\|_{2}\\right]_{j=-\\infty}^{0}$ or $\\mu(X)=\\left[v_{0}^{\\top} v_{j}\\right]_{j=-\\infty}^{0}$. In this setting, $v_{j}$ intuitively represents the features of the token at the position $j$, and we assume that the importance of a token is controlled by the similarity between the target token and the current token. The setting (ii) makes it possible for us to consider additional synthetic task. Specifically, the functions in setting (ii) includes the task called associative recall Ba et al. (2016). In this task, the query token has appeared in the past, and the model is required to output the same token that followed it in the previous occurrence, e.g., if the input is \"a 2 c 4 b 3 d 1 e 7 c\", the model should output \" 4 \". Additionally, (ii) includes in-context learning with $k$-nearest neighbors algorithm.",
    "dynsmoothssms-5": "Note that the setting (i) includes some important settings such as induction head and selective copying Gu and Dao (2023). Indeed, in those tasks, the absolute position of the important token is fixed. ## 3 The Definition of State Space Models with Gated Convolution\n\n(Discretized) state space models with input $\\left[u_{t}\\right]_{t=-L}^{0}$, the latent vectors $\\left[x_{t}\\right]_{t=-L}^{0}$ and the output $\\left[y_{t}\\right]_{t=-L}^{0}\\left(u_{t} \\in \\mathbb{R}, x_{t} \\in \\mathbb{R}, y_{t} \\in \\mathbb{R}\\right)$ is represented as follows:\n\n$$\nx_{t+1}=\\mathrm{A} x_{t}+\\mathrm{B} u_{t}, \\quad y_{t}=\\mathrm{C} x_{t}+\\mathrm{D} u_{t} \\quad(t=-L, \\ldots,-1)\n$$\n\nwhere $\\mathrm{A}, \\mathrm{B}, \\mathrm{C}, \\mathrm{D} \\in \\mathbb{R}$ are the (learnable) parameters.",
    "dynsmoothssms-6": "Then, the output $y_{t}$ can be written explicitly as $y_{t}=\\sum_{n=0}^{L+t}\\left(\\mathrm{CA}^{t-n} \\mathrm{~B}+\\mathrm{D} \\delta_{t-n}\\right) u_{n}$. By setting $h_{t}:=\\mathrm{CA}^{t} \\mathrm{~B}+\\mathrm{D} \\delta_{t}$ and $h=\\left[h_{t}\\right]_{t=0}^{L}$, we can rewrite the output as $y_{t}=(h * u)_{t}:=\\sum_{n=0}^{L+t} h_{t-n} u_{n}$. If the filter $\\left[h_{t}\\right]_{t=0}^{L}$ is precomputed, the output can be computed with $O(L \\log L)$ time complexity using FFT algorithm, which is much faster than the computation cost of Transformers, $O\\left(L^{2}\\right)$. In this paper, we consider the state space models with gated convolution like H3 (Fu et al. 2022) and Hyena (Poli et al. 2023). State space model with gated convolution is an architecture that consists of the three components: (i) FNN layers, (ii) gated convolution layers, and (iii) an embedding layer. (i) FNN layer An FNN with depth $L$ and width $W$ is defined as\n\n$$\nf(x):=\\left(A_{L} \\eta(\\cdot)+b_{L}\\right) \\circ \\cdots \\circ\\left(A_{1} x+b_{1}\\right)\n$$\n\nwhere $\\eta=\\operatorname{ReLU}$, and $A_{i} \\in \\mathbb{R}^{d_{i+1} \\times d_{i}}, b_{i} \\in \\mathbb{R}^{d_{i+1}}$ with $\\max _{i} d_{i} \\leq W$. Then, we define the class of FNN with depth $L$, width $W$, norm bound $B$ and sparsity $S$ by\n\n$$\n\\Psi(L, W, S, B):=\\left\\{f \\mid \\max _{i}\\left\\{\\left\\|A_{i}\\right\\|_{\\infty},\\left\\|b_{i}\\right\\|_{\\infty}\\right\\} \\leq B, \\sum_{i=1}^{L}\\left\\|A_{i}\\right\\|_{0}+\\left\\|b_{i}\\right\\|_{0} \\leq S\\right\\}\n$$\n\n(ii) Gated convolution layer Next, we define gated convolution, which is an idea originally proposed by Dauphin et al. (2017). Let $W_{V}, W_{Q} \\in \\mathbb{R}^{D \\times D}$ be learnable weights, and $D$ be the embedding dimension. Then, the gated convolution layer $g: \\mathbb{R}^{|X|} \\rightarrow \\mathbb{R}^{|X|}$ with window size $U$ is defined as\n\n$$\ng(X):=\\left(W^{Q} X\\right) \\odot\\left(H *\\left(W^{V} X\\right)\\right), \\quad H_{k, j}:=c_{1, k} \\cos \\left(\\frac{2 \\pi j \\cdot a_{1, k}}{U}\\right)+c_{2, k} \\sin \\left(\\frac{2 \\pi j \\cdot a_{2, k}}{U}\\right)\n$$\n\nwhere $H \\in \\mathbb{R}^{D \\times U}$ is a filter controlled by learnable parameters $c_{1}, c_{2}, a_{1}, a_{2} \\in \\mathbb{R}^{D}$. Note that $\\odot: \\mathbb{R}^{D} \\times \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{D}$ is the element-wise product, and $X * Y:=\\left[X_{i} * Y_{i}\\right]_{i=1}^{d}$ for $X=\\left[X_{i}\\right]_{i=1}^{d} \\in$ $\\mathbb{R}^{d \\times \\infty}, Y=\\left[Y_{i}\\right]_{i=1}^{d} \\in \\mathbb{R}^{d \\times \\infty}$. Then, we define the class of gated convolution layer with window size $U$, embedding dimension $D$ and norm constraint $B$ by\n\n$$\n\\mathcal{C}(U, D, B):=\\left\\{g \\mid \\max \\left\\{\\left\\|W^{V}\\right\\|_{\\infty},\\left\\|W^{Q}\\right\\|_{\\infty},\\left\\|a_{1}\\right\\|_{\\infty},\\left\\|a_{2}\\right\\|_{\\infty},\\left\\|c_{1}\\right\\|_{\\infty},\\left\\|c_{2}\\right\\|_{\\infty}\\right\\} \\leq B\\right\\}\n$$\n\nThis setting is basically inspired Hyena, in which neural networks with $\\sin$ activation is used as a convolution filter. However, this can be easily extended to the SSMs with gated convolution with ordenary filter $h_{t}=\\mathrm{CA}^{t} \\mathrm{~B}+\\mathrm{D} \\delta_{t}$. Indeed, by constructing the parameters in the filter appropriately, we can obtain the same architecture as the gated convolution layer. See Appendix $\\mid$ Afor the details. Note that we assume the finite window size $U$, as well as Takakura and Suzuki (2023). (iii) Embedding layer Finally, we define the embedding layer. For embedding dimension $D$, an embedding layer is defined as\n\n$$\n\\operatorname{Emb}(X)=\\left[E_{1} X_{i}+E_{2}\\right]_{i=-\\infty}^{\\infty}\n$$\n\nwhere $E_{1} \\in \\mathbb{R}^{D \\times d}$ and $E_{2} \\in \\mathbb{R}^{D}$ are learnable parameters. Then, the output of the whole network for input $X$ is computed by $f_{M} \\circ g_{M} \\circ \\cdots \\circ f_{1} \\circ g_{1} \\circ \\operatorname{Emb}(X)$. Due to the technical convenience to analyze the estimation error, we consider the setting where the output of the network is bounded. For this purpose, we assume that the output above is fed into the\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5e098f2beaae0c90c478g-06.jpg?height=136&width=1389&top_left_y=2141&top_left_x=365) by the FNN with depth 1 and width 2 , such assumption does not far from the practical setting. Summarizing above, we define the class of data-controlled SSMs by\n$\\mathcal{S}(M, U, D, L, W, S, B):=\\left\\{\\begin{array}{l|l}\\operatorname{clip}_{R} \\circ f_{M} \\circ g_{M} \\circ \\cdots \\circ f_{1} \\circ g_{1} \\circ \\operatorname{Emb} & \\begin{array}{l}f_{i} \\in \\Psi(L, W, S, B), \\\\ g_{i} \\in \\mathcal{C}(U, D, B), \\\\ \\left\\|E_{1}\\right\\|_{\\infty} \\leq B,\\left\\|E_{2}\\right\\|_{\\infty} \\leq B .\\end{array}\\end{array}\\right\\}$. ## 4 Approximation Ability of SSMs with Gated Convolution\n\n### 4.1 Approximation of $\\gamma$-smooth functions\n\nNow, we establish the approximation ability of $\\gamma$-smooth functions via SSMs.",
    "dynsmoothssms-7": "First, we state the detailed assumptions on the target function. Assumption 4.1. The true function $F^{\\circ}$ is shift-equivariant and satisfies $F_{0}^{\\circ} \\in \\mathcal{F}_{p, \\theta}^{\\gamma}$, where $\\gamma$ is mixed or anisotropic smoothness. Suppose that it holds $\\|F\\|_{\\mathcal{F}_{p, \\theta}^{\\gamma}} \\leq 1$ and $\\left\\|F_{0}^{\\circ}\\right\\|_{\\infty} \\leq R$, where $R>0$ is a constant. Additionally, we assume the smoothness parameter a satisfies $\\|a\\|_{w l^{\\alpha}} \\leq 1$ for some $0<\\alpha<\\infty$ and $a_{i j}=\\Omega(\\log (|j|+1))$. Moreover, if $\\gamma$ is mixed smoothness, we assume $\\bar{a}_{1}<\\bar{a}_{2}$. Remark. The assumption $\\|a\\|_{w l^{\\alpha}} \\leq 1$ implies that the $j$-th smallest element of smoothness parameter $a$ increases polynomially with respect to $j$, which indicates the sparsity of the important features. Moreover, the assuption $a_{i j}=\\Omega(\\log (|j|+1))$ means that the token distant from the current token is less important. Thus, those two assumptions are natural in various applications. Then, we have the following theorem which shows that the SSM can approximate $\\gamma$-smooth functions. Theorem 4.2. Suppose that target function $F^{\\circ}$ satisfies Assumption 4.1 Then, for any $T>0$, there exists an $S S M F \\in \\mathcal{S}(M, U, D, L, W, S, B)$ with\n\n$$\n\\begin{aligned}\n& M=1, \\quad \\log U \\sim T, \\quad D \\sim T^{1 / \\alpha}, \\quad L \\sim T, \\quad W \\sim T^{1 / \\alpha} \\\\\n& W^{\\prime} \\sim T^{1 / \\alpha} 2^{T / a^{\\dagger}}, \\quad S \\sim T^{2 / \\alpha} \\max \\left\\{T^{2 / \\alpha}, T^{2}\\right\\} 2^{T / a^{\\dagger}}, \\quad \\log B \\sim T^{1 / \\alpha}\n\\end{aligned}\n$$\n\nsuch that $\\left\\|F-F^{\\circ}\\right\\|_{2, P_{X}} \\lesssim 2^{-T}$. The proof can be found in Appendix D. We can see that, the number of parameters to achieve certain error does not suffer from the infinite dimensionality. This is due to the fact the SSMs use same convolution filter for each tokens, as well as self-attention of Transformers. ### 4.2 Approximation of piecewise $\\gamma$-smooth functions\n\nNext, we show that the SSMs can approximate the piecewise $\\gamma$-smooth functions. To this end, we first state the detailed assumptions on the target function. Assumption 4.3. The true function $F^{\\circ}$ is shift-equivariant and satisfies $F_{0}^{\\circ} \\in \\mathcal{P}_{p, \\theta}^{\\gamma}$, where $\\gamma$ is mixed or anisotropic smoothness, and suppose that $\\left\\|F_{0}^{\\circ}\\right\\|_{\\mathcal{P}_{p, \\theta}^{\\gamma}} \\leq 1$ and $\\left\\|F_{0}^{\\circ}\\right\\|_{\\infty} \\leq R$, where $R>0$ is a constant. Additionally, we assume $a_{i j}=\\Omega\\left(j^{\\alpha}\\right)$ and $\\|a\\|_{w l^{\\alpha}} \\leq 1$ for some $0<\\alpha<\\infty$. Moreover, there exists a importance function $\\mu$ of $\\mathcal{P}_{p, \\theta}^{\\gamma}$ that is represented by (i) $\\gamma$-smooth condition or (ii) similarity. For case (i), we further assume that $\\mu$ satisfies Assumption 4.1. Remark. The assumptions on the norm of the smoothness parameter $a$ and the function $F_{0}^{\\circ}$ are same as Assumption 4.1. The interpretion for the conditions on the importance function is described in Subsection 2.3. The condition $a_{i j}=\\Omega\\left(j^{\\alpha}\\right)$ indicates that the function is smoother with respect to the token with small importance the tokens are sorted in order of decreasing importance by the map $\\Pi$. Note that, unlike the case of Assumption 4.1. the condition $a_{i j}=\\Omega\\left(j^{\\alpha}\\right)$ is imposed on the smoothness of permuted tokens, and $j$ can be different from the original position. Then, we have the following theorem on the approximation ability of SSMs for piecewise $\\gamma$-smooth functions. Theorem 4.4. Let $F^{\\circ}$ be a function satisfying Assumption 4.3. Then, for any $T>0$, there exists a $S S M \\hat{F} \\in \\mathcal{S}(M, U, D, L, W, S, B)$ with\n\n$$\n\\begin{aligned}\n& M \\lesssim T^{1 / \\alpha}, \\quad U=V, \\quad D \\lesssim T^{c_{\\alpha, \\beta}} \\log ^{2} V, \\quad L \\lesssim T^{c_{\\alpha, \\beta}} \\log ^{3} V \\\\\n& W \\lesssim 2^{T / a^{\\dagger}} T^{c_{\\alpha, \\beta}} \\log ^{2} V, \\quad S \\lesssim 2^{T / a^{\\dagger}} T^{c_{\\alpha, \\beta}} \\log ^{3} V, \\quad \\log B \\lesssim T^{c_{\\alpha, \\beta}} \\log ^{3} V\n\\end{aligned}\n$$\n\nsuch that $\\left\\|F^{\\circ}-\\hat{F}\\right\\|_{2} \\lesssim 2^{-T}$. Here, $c_{\\alpha, \\beta}$ is a constant depending on $\\alpha$ and $\\beta$ such that $c_{\\alpha, \\beta} \\leq$ $4+2 / \\alpha+3 \\beta / \\alpha$\n\nThe proof can be found in Appendix E This result reveals that the number of parameters to attain the error $2^{-T}$ is same as the case of $\\gamma$-smoothness, which implies SSMs have the ability to extract tokens depending on the input sequence and the token at the output position. ## 5 Estimation Ability of SSMs with Gated Convolution\n\nUsing the approximation theory in the previous section, we now establish the theory estimation ability of SSMs. To this end, we first introduce the following theorem, which evaluates the estimation ability of ERM estimators in $\\mathcal{S}(M, U, D, L, W, S, B)$. Theorem 5.1. Let $\\hat{F} \\in \\mathcal{S}(M, U, D, L, W, S, B)$ be an ERM estimator which minimizes the emprical cost. Then, for any $\\delta \\in(0,1)$, it holds that\n\n$$\nR_{l, r}\\left(\\hat{F}, F^{\\circ}\\right) \\lesssim \\inf _{F \\in \\mathcal{S}} \\frac{1}{r-l+1} \\sum_{i=l}^{r}\\left\\|F_{i}-F_{i}^{\\circ}\\right\\|_{2, P_{X}}^{2}+\\frac{1}{n} \\cdot M^{2} L(S+D) \\log \\left(\\frac{D U L W B}{\\delta}\\right)+\\delta\n$$\n\nThis theorem can be proved by using Theorem 5.2 of Takakura and Suzuki 2023) and the bound of covering number of the space $\\mathcal{S}$. The proof can be found in Appendix $F$\nCombining the appoximation theory (Theorem 4.2) and the theorem on the estimation ability (Theorem 5.1, we obtain the following theorem, which evaluates the estimation ability of SSMs for $\\gamma$-smooth functions. Theorem 5.2. Suppose that the target function $F^{\\circ}$ satisfies Assumption 4.1. Let $\\hat{F}$ be an ERM estimator in $\\mathcal{S}(M, U, D, L, W, S, B)$, with $M, U, D, L, W, S, B$ defined as 4.1 for $T=\\frac{a^{\\dagger}}{2 a^{\\dagger}+1}$. Then, for any $l, r \\in \\mathbb{Z}$, it holds\n\n$$\nR_{l, r}\\left(\\hat{F}, F^{\\circ}\\right) \\lesssim n^{-\\frac{2 a^{\\dagger}}{2 a^{\\dagger}+1}}(\\log n)^{2+2 / \\alpha} \\max \\left\\{(\\log n)^{3 / \\alpha},(\\log n)^{4}\\right\\}\n$$\n\nWe prove this theorem in Appendix Grom this theorem, we can see that SSMs can avoids the bad effect of infinite dimensionality, and achieves the convergence rate with respect to the sample size that is independent to the dimension of input and output. Additionally, this convergence rate is identical to that of CNNs (Okumoto and Suzuki, 2021) and Transformers (Takakura and Suzuki, 2023) up to poly-log factor. These facts imply that SSMs have the ability avoid the curse of dimensionality using the smoothness structure of the target function as CNNs and Transformers do. Moreover, for the case of anisotropic smoothness with finite dimensional input and output, the convergence rate matches the minimax optimal rate given by Suzuki and Nitanda (2021) up to poly-log factor. This indicates that SSMs is a optimal estimator for the anisotropic smoothness function class. Next, we evaluate the estimation ability of SSMs for piecewise $\\gamma$-smooth functions. Theorem 5.3. Suppose that the target function $F^{\\circ}$ satisfies Assumption 4.3. Let $\\hat{F}$ be an ERM estimator in $\\mathcal{S}(M, U, D, L, W, S, B)$, with $M, U, D, L, W, S, B$ defined as 4.2 for $T=\\frac{a^{\\dagger}}{2 a^{\\dagger}+1}$. Then, for any $l, r \\in \\mathbb{Z}$, it holds\n\n$$\nR_{l, r}\\left(\\hat{F}, F^{\\circ}\\right) \\lesssim n^{-\\frac{2 a \\dagger}{2 a a^{\\dagger}+1}}(\\log n)^{c_{\\alpha, \\beta}^{\\prime}} \\log ^{10} V\n$$\n\nwhere $c_{\\alpha, \\beta}^{\\prime}$ is a constant depending on $\\alpha, \\beta$ such that $c_{\\alpha, \\beta}^{\\prime} \\leq 16+10 / \\alpha+12 \\beta / \\alpha$. The proof can be found in Appendix H As well as the case of $\\gamma$-smooth functions, this convergence rate with respect to $n$ matches that of Transformers. This indicates that SSMs have ability to select important tokens depending on the inputs, similary to Transformers. In addition, Theorem 5.3 includes the setting of extention of importance functions described in Subsection 2.3. This fact reveals that, even in situations where the positions of important tokens differ for each output token, it is possible for SSMs to identify them. Moreover, since the estimation error bound depends on $V$ with only poly-log factor, if $V=\\operatorname{poly}(n)$, the estimation error rate does not change up to poly-log factor. This aspect also matches the result of Transformers. Overall, we can conclude that, SSMs can be alternative of Transformers, (i) when the target function has a smoothness structure, and (ii) in the case where the positions of important tokens differ depending on the input. ## 6 Numerical Experiments\n\nTo demonstrate that our theory is compatible with the real-world tasks, we conducted two numerical experiments. We utilize the pretrained model of Hyena provided by Nguyen et al. (2024), which\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5e098f2beaae0c90c478g-09.jpg?height=481&width=1397&top_left_y=242&top_left_x=364)\n\nFigure 6.1: The experimental results. Left: We choose one target sequence from the test data, and repeatedly mask the tokens with less importance. Then, the probability of correct classification decreases slowly, which means the essential tokens for the classification are limited. On the other hand, if we mask tokens at the same position for other sequences, the accuracy decreases faster. This indicates that the model can select the important tokens depending on the input. Right: We fix one target token for each sequence, and repeatedly mask the input tokens with less importance. Then, the probability of correct prediction decreases slower for the chosen token than the other tokens. This reveals that the important tokens to pay attention is different depending on the output token. is trained via the next token prediction task with the nucleic acid base sequences. We fine-tune the model using Genomic Benchmark dataset (Gre\u0161ov\u00e1 et al., 2023). Additional description on the experimental setting can be found in Appendix I. First, we verify that SSMs can select important tokens depending on the input. We fine-tune the model with the binary classification task. Then, we choose one sequence in the test data and plot the transition of the probability of correct classification when we repeatedly mask the input tokens that do not affect the classification result. Additionally, we observe how the correctness for other sequences changes when we mask the same tokens as the chosen sequence. The result is shown on the left of Figure 6.1. We can see that SSMs can classify the sequences even if we mask most of the tokens. This indicates that the number of important tokens to classify the sequence is small. Moreover, the accuracies for other sequences decrease when there are a few tokens left, which means that the important tokens of other sequences differ from the chosen sequence. Second, we demonstrate the ability of SSMs to extract important tokens depending on the output token. We consider the task to predict the masked token at the last of sequence. Then, we choose one token for each sequence and plot the transition of the probability of correct prediction when we repeatedly mask the tokens whose impact on the prediction is small. Additionally, we plot the transition of the correctness for other tokens when we mask the the tokens at the same positions. We show the result in the right part of Figure 6.1. We can notice that the accuracy for the chosen token decreases slower than the other tokens. This reveals that the important tokens to pay attention to are different depending on the position of the output token, and SSMs can adaptively extract the important tokens depending on the output token. ## 7 Conclusion\n\nIn this paper, we theoretically explore the possibility of SSMs as an alternative to Transformers. To this end, we established the theory on the approximation and estimation ability of SSMs for a certain class. In addition to the function spaces considered in the previous work, we extend the piecewise $\\gamma$-smooth function class to include the case where the importance of a token can change depending on the output token. Consequently, we prove that SSMs have the same estimation ability as Transformers for those function classes. Limitations and future works We investigate the ability of ERM estimators, and did not discuss whether the SSMs can be optimized efficiently. Analyzing how the optimization algorithm works for SSMs is an possible direction for future work. Additionally, we did not investigate the other types of models that is said to be an alternative to Transformers, such as SSMs with data-dependent filters (like Mamba (Gu and Dao, 2023)) and linear attention (Katharopoulos et al., 2020). Future research could focus on the comparison between those models and SSMs with gated convolution.",
    "dynsmoothssms-8": "## References\n\nC. A. Alonso, J. Sieber, and M.",
    "dynsmoothssms-9": "N. Zeilinger. State space models as foundation models: A control theoretic overview.",
    "dynsmoothssms-10": "arXiv preprint arXiv:2403.16899, 2024. J. Ba, G. E. Hinton, V. Mnih, J. Z. Leibo, and C. Ionescu. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016. M. Chen, H. Jiang, W. Liao, and T. Zhao. Nonparametric regression on low-dimensional manifolds using deep relu networks: Function approximation and statistical recovery. Information and Inference: A Journal of the IMA, 11(4):1203-1253, 2022.",
    "dynsmoothssms-11": "N. M. Cirone, A. Orvieto, B. Walker, C. Salvi, and T. Lyons. Theoretical foundations of deep selective state-space models. arXiv preprint arXiv:2402.19047, 2024.",
    "dynsmoothssms-12": "Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.",
    "dynsmoothssms-13": "D. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2022.",
    "dynsmoothssms-14": "K. Goel, A. Gu, C. Donahue, and C. R\u00e9. It's raw! audio generation with state-space models. In International Conference on Machine Learning, pages 7616-7633. PMLR, 2022. K. Gre\u0161ov\u00e1, V. Martinek, D. \u010cech\u00e1k, P. \u0160ime\u010dek, and P. Alexiou. Genomic benchmarks: a collection of datasets for genomic sequence classification. BMC Genomic Data, 24(1):25, 2023. A. Gu and T. Dao. Mamba: Linear-time sequence modeling with selective state spaces.",
    "dynsmoothssms-15": "arXiv preprint arXiv:2312.00752, 2023. A. Gu, K. Goel, and C. R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. P. Hall and J. L. Horowitz. Methodology and convergence rates for functional linear regression.",
    "dynsmoothssms-16": "2007. M. Imaizumi and K. Fukumizu. Deep neural networks learn non-smooth functions effectively. In The 22nd international conference on artificial intelligence and statistics, pages 869-878. PMLR, 2019. A. Katharopoulos, A. Vyas, N.",
    "dynsmoothssms-17": "Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156-5165. PMLR, 2020. S. Massaroli, M. Poli, D. Fu, H. Kumbong, R. Parnichkun, D. Romero, A. Timalsina, Q. McIntyre, B. Chen, A. Rudra, et al. Laughing hyena distillery: Extracting compact recurrences from convolutions.",
    "dynsmoothssms-18": "Advances in Neural Information Processing Systems, 36, 2024.",
    "dynsmoothssms-19": "W. Merrill, J. Petty, and A. Sabharwal. The illusion of state in state-space models. arXiv preprint arXiv:2404.08819, 2024. R. Nakada and M. Imaizumi. Adaptive approximation and generalization of deep neural network with intrinsic dimensionality. Journal of Machine Learning Research, 21(174):1-38, 2020. E. Nguyen, M. Poli, M. Faizi, A. Thomas, M. Wornow, C. Birch-Sykes, S. Massaroli, A. Patel, C. Rabideau, Y. Bengio, et al. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution.",
    "dynsmoothssms-20": "Advances in neural information processing systems, 36, 2024.",
    "dynsmoothssms-21": "K. Oko, S. Akiyama, and T. Suzuki. Diffusion models are minimax optimal distribution estimators. In International Conference on Machine Learning, pages 26517-26582. PMLR, 2023. S. Okumoto and T. Suzuki. Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness. In International Conference on Learning Representations, 2021.",
    "dynsmoothssms-22": "D. Perekrestenko, P. Grohs, D. Elbr\u00e4chter, and H. B\u00f6lcskei. The universal approximation power of finite-width deep relu networks. arXiv preprint arXiv:1806.01528, 2018. P. Petersen and F. Voigtlaender. Optimal approximation of piecewise smooth functions using deep relu neural networks. Neural Networks, 108:296-330, 2018. M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023.",
    "dynsmoothssms-23": "A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 28492-28518. PMLR, 2023. G. Saon, A. Gupta, and X. Cui. Diagonal state space augmented transformers for speech recognition. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023 . J. Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation function.",
    "dynsmoothssms-24": "2020.",
    "dynsmoothssms-25": "T. Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality.",
    "dynsmoothssms-26": "In International Conference on Learning Representations, 2018.",
    "dynsmoothssms-27": "T. Suzuki and A. Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic besov space. Advances in Neural Information Processing Systems, 34:3609-3621, 2021. S. Takakura and T. Suzuki. Approximation and estimation ability of transformers for sequence-to-sequence functions with infinite dimensional input. In Proceedings of the 40th International Conference on Machine Learning, pages 33416-33447. PMLR, 2023. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. S. Wang and B. Xue. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Advances in Neural Information Processing Systems, 36, 2024. ## \u2014\u2014 Appendix\n\n## A Extension to ordinary SSM filter\n\nIn this section, we describe how to extend our setting to the ordinary SSM filter. More specifically, our setting with embedding dimension $D$ can be extended to the ordinary SSM filter with embedding dimension $4 D$. For simplicity, we consider the case $D=1$. We constuct the parameters $A, B, C, D \\in \\mathbb{R}^{2 \\times 2}$ to make the filter $h_{t}:=\\mathrm{CA}^{t} \\mathrm{~B}+\\mathrm{D} \\delta_{t-n}$ same as the filter defined in Section3 Let us set $\\mathrm{D}=0$, and\n\n$$\n\\mathrm{A}=\\left[\\begin{array}{cccc}\n\\cos \\left(\\frac{2 \\pi a_{1,1}}{U}\\right) & -\\sin \\left(\\frac{2 \\pi a_{1,1}}{U}\\right) & 0 & 0 \\\\\n\\sin \\left(\\frac{2 \\pi a_{1,1}}{U}\\right) & \\cos \\left(\\frac{2 \\pi a_{1,1}}{U}\\right) & 0 & 0 \\\\\n0 & 0 & \\cos \\left(\\frac{2 \\pi a_{1,2}}{U}\\right) & -\\sin \\left(\\frac{2 \\pi a_{1,2}}{U}\\right) \\\\\n0 & 0 & \\sin \\left(\\frac{2 \\pi a_{1,2}}{U}\\right) & \\cos \\left(\\frac{2 \\pi a_{1,2}}{U}\\right)\n\\end{array}\\right]\n$$\n\nThen, we have\n\n$$\nA^{t}=\\left[\\begin{array}{cccc}\n\\cos \\left(\\frac{2 \\pi a_{1,1} t}{U}\\right) & -\\sin \\left(\\frac{2 \\pi a_{1,1} t}{U}\\right) & 0 & 0 \\\\\n\\sin \\left(\\frac{2 \\pi a_{1,1} t}{U}\\right) & \\cos \\left(\\frac{2 \\pi a_{1,1} t}{U}\\right) & 0 & 0 \\\\\n0 & 0 & \\cos \\left(\\frac{2 \\pi a_{1,2} t}{U}\\right) & -\\sin \\left(\\frac{2 \\pi a_{1,2} t}{U}\\right) \\\\\n0 & 0 & \\sin \\left(\\frac{2 \\pi a_{1,2} t}{U}\\right) & \\cos \\left(\\frac{2 \\pi a_{1,2} t}{U}\\right)\n\\end{array}\\right]\n$$\n\nTherefore, if we set\n\n$$\n\\mathrm{B}=\\left[\\begin{array}{llll}\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{array}\\right], \\quad \\mathrm{C}=\\left[\\begin{array}{cccc}\nc_{1,1} & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\nc_{1,2} & 0 & 0 & 0\n\\end{array}\\right]\n$$\n\nthen we have\n\n$$\nh_{t}=\\left[\\begin{array}{cccc}\nc_{1,1} \\cos \\left(\\frac{2 \\pi a_{1,1} t}{U}\\right) & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\nc_{1,2} \\sin \\left(\\frac{2 \\pi a_{1,2} t}{U}\\right) & 0 & 0 & 0\n\\end{array}\\right]\n$$\n\nThen, if we appropriately set $W^{V}$ and $W^{Q}$, this filter can realize the same output with our setting. While we do not show the estimation ability for the filter above, we can easily extend our proof to derive the almost same estimation error bound for it. ## B Key Insight on SSMs with Gated Convolution\n\nBefore starting the proof, we show the key insight on SSMs with gated convolution. Suppose $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{2 n}, x \\mapsto\\left[\\phi_{A}^{(1)}(x), \\ldots, \\phi_{A}^{(n)}(x), \\phi_{B}^{(1)}(x), \\ldots, \\phi_{B}^{(n)}(x)\\right]$ is a fully connected neural network. Additionally, let $g \\in \\mathcal{C}(U, D, B)$ with $D=2 n$. Recall that $g$ is defined as\n\n$$\ng(X)=\\left(W^{Q} X\\right) \\odot\\left(H *\\left(W^{V} X\\right)\\right) \\quad\\left(X \\in \\mathbb{R}^{2 n \\times \\infty}\\right)\n$$\n\nwhere $H \\in \\mathbb{R}^{2 n \\times 2 n}$ is a filter defined by\n\n$$\nH_{k, j}:=c_{1, k} \\cos \\left(\\frac{2 \\pi j \\cdot a_{1, k}}{U}\\right)+c_{2, k} \\sin \\left(\\frac{2 \\pi j \\cdot a_{2, k}}{U}\\right)\n$$\n\nLet us set the parameters $W^{Q}, W^{V} \\in \\mathbb{R}^{2 n \\times 2 n}, c_{1}, c_{2} \\in \\mathbb{R}^{2 n}$ in $g$ as\n\n$$\n\\begin{aligned}\nW^{Q} & =\\left[\\begin{array}{cc}\nO & I \\\\\nO & O\n\\end{array}\\right], \\quad W^{V}=\\left[\\begin{array}{cc}\nI & O \\\\\nO & O\n\\end{array}\\right] \\\\\nc_{1} & =\\left[\\begin{array}{lll}\n\\alpha_{1} \\cdot \\mathbb{I}_{S}(1) & \\cdots & \\alpha_{n} \\cdot \\mathbb{I}_{S}(n)\n\\end{array}\\right]^{\\top} \\\\\nc_{2} & =\\left[\\begin{array}{lll}\n\\alpha_{1} \\cdot \\mathbb{I}_{\\bar{S}}(1) & \\cdots & \\alpha_{n} \\cdot \\mathbb{I}_{\\bar{S}}(n)\n\\end{array}\\right]^{\\top}\n\\end{aligned}\n$$\n\nwhere $S \\subseteq[n]$ and $\\alpha_{1}, \\ldots, \\alpha_{n} \\in \\mathbb{R}$. Then, for any input $X:=\\left[x_{t}\\right]_{t=-\\infty}^{0} \\in \\mathbb{R}^{2 n \\times \\infty}$, we have\n\n$$\n(g \\circ f(X))_{0}=\\left[\\begin{array}{c}\n\\sum_{t=-U}^{0} \\alpha_{1} \\cdot \\psi^{(1)}(t / U) \\phi_{A}^{(1)}\\left(x_{t}\\right) \\phi_{B}^{(1)}\\left(x_{0}\\right) \\\\\n\\vdots \\\\\n\\sum_{t=-U}^{0} \\alpha_{n} \\cdot \\psi^{(n)}(t / U) \\phi_{A}^{(n)}\\left(x_{t}\\right) \\phi_{B}^{(n)}\\left(x_{0}\\right) \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]\n$$\n\nwhere\n\n$$\n\\psi^{(k)}(s):= \\begin{cases}\\cos \\left(2 \\pi a_{1, k} \\cdot s\\right) & (k \\in S) \\\\ \\sin \\left(2 \\pi a_{2, k} \\cdot s\\right) & (k \\notin S)\\end{cases}\n$$\n\nTherefore, we can see that, for any function $K: \\mathbb{R}^{2 n+1} \\rightarrow \\mathbb{R}$ that can be approximated by the form of\n\n$$\nK(t, x, y)=\\sum_{k=1}^{n} \\alpha_{k} \\cdot \\psi^{(k)}(t / U) \\phi_{A}^{(k)}(x) \\phi_{B}^{(k)}(y)\n$$\n\none FNN and one SSM with gated convolution can approximate its summation over\n\n$$\n(t, x, y)=\\left(0, x_{0}, x_{0}\\right),\\left(-1, x_{-1}, x_{0}\\right), \\ldots,\\left(-U, x_{-U}, x_{0}\\right)\n$$\n\ni.e.,\n\n$$\n\\sum_{t=-U}^{0} K\\left(t, x_{t}, x_{0}\\right)\n$$\n\nIn the following proof, we use this fact for sevaral times as described below. Extracting a token at a specific position We can extract one token from $x_{0}, x_{-1}, \\ldots, x_{-U}$ by setting $K$ to satisfy\n\n$$\nK(t, x, y) \\approx \\begin{cases}x & \\left(t=t_{0}\\right) \\\\ 0 & \\left(t \\neq t_{0}\\right)\\end{cases}\n$$\n\nfor some $t_{0} \\in[-U: 0]$. Extracting a feature of the token with high similarity Let $\\rho: \\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ be a kernel function that measures the similarity between two vectors. Suppose that $\\rho$ can be approximated by a finite sum as follows:\n\n$$\n\\rho(x, y) \\approx \\sum_{k=1}^{n} \\alpha_{i} \\cdot \\eta_{k}(x) \\eta_{k}(y)\n$$\n\nLet us set $K$ to satisfy\n\n$$\nK(t, x, y) \\approx \\sum_{k=1}^{n} x_{m} \\cdot \\eta_{k}(x) \\cdot \\eta_{k}(y)\n$$\n\nfor some $m \\in[n]$. Then we can extract the $m$-th coordinate of the token that has the highest similarity with $x_{0}$ among $x_{-U}, \\ldots, x_{0}$, if the similarities between other tokens and $x$ are significantly lower. Note that $x \\mapsto x_{m} \\cdot \\eta_{k}(x)$ can be approximated by a FNN. ## C Auxiliary Lemmas\n\nIn the following discusstion, to simplify the notation, we define the function class $\\Psi^{\\prime}(D, B)$ by\n\n$$\n\\Psi^{\\prime}(D, B):=\\left\\{t \\mapsto\\left[c_{1, k} \\cos \\left(2 \\pi a_{1, k} t\\right)+c_{2, k} \\sin \\left(2 \\pi a_{2, k} t\\right)\\right]_{k=1}^{D} \\mid\\|c\\|_{\\infty} \\leq B,\\|a\\|_{\\infty} \\leq B\\right. $$\n\nFirst, we prove the following lemma, which states the properties of the Softmax and multi-variate Swish function. Lemma C. 1 (Properties of Softmax and Multi-variate Swish function). Fix $\\theta \\in \\mathbb{R}^{d}$. Assume that there exists an index $i^{*} \\in[d]$ and $\\delta>0$ such that $\\theta_{i^{*}}>\\theta_{i}+\\delta$ for all $i \\neq i^{*}$. Then, the following two statements hold:\n\n1. (Lemma C.1 of Takakura and Suzuki (2023)) It holds\n\n$$\n\\sum_{i=1}^{d}\\left|\\operatorname{Softmax}(\\theta)_{i}-\\delta_{i, i^{*}}\\right| \\leq 2 d \\exp (-\\delta)\n$$\n\n2. For any $x \\in[0,1]^{d}$, it holds\n\n$$\n\\left|\\sum_{i=1}^{d} \\operatorname{Softmax}(\\theta)_{i} \\cdot x_{i}-\\max \\left\\{x_{1}, \\ldots, x_{d}\\right\\}\\right| \\leq 2 d^{2} \\exp (-\\delta)\n$$\n\nProof. We prove the second one. Using the first argument, we have\n\n$$\n\\begin{aligned}\n& \\left|\\sum_{i=1}^{d} \\operatorname{Softmax}(\\theta)_{i} \\cdot x_{i}-\\max \\left\\{x_{1}, \\ldots, x_{d}\\right\\}\\right| \\\\\n& \\quad \\leq\\left|\\sum_{i \\neq i^{*}} \\operatorname{Softmax}(\\theta)_{i} \\cdot x_{i}+\\left(\\operatorname{Softmax}(\\theta)_{i^{*}} \\cdot x_{i^{*}}-\\max \\left\\{x_{1}, \\ldots, x_{d}\\right\\}\\right)\\right| \\\\\n& \\quad=\\left|\\sum_{i \\neq i^{*}} \\operatorname{Softmax}(\\theta)_{i} \\cdot x_{i}+\\left(\\operatorname{Softmax}(\\theta)_{i^{*}} \\cdot x_{i^{*}}-\\delta_{i, i^{*}} x_{i^{*}}\\right)\\right| \\\\\n& \\quad \\leq \\sum_{i \\neq i^{*}} \\operatorname{Softmax}(\\theta)_{i} \\cdot x_{i}+\\left|\\operatorname{Softmax}(\\theta)_{i^{*}}-\\delta_{i, i^{*}}\\right| \\cdot x_{i^{*}} \\\\\n& \\quad \\leq \\sum_{i=1}^{d}\\left|\\operatorname{Softmax}(\\theta)_{i}-\\delta_{i, i^{*}}\\right| \\cdot x_{i} \\\\\n& \\quad \\leq 2 d^{2} \\exp (-\\delta)\n\\end{aligned}\n$$\n\nwhich completes the proof. The following lemma shows the approximation ability of FNN for some elementary functions.",
    "dynsmoothssms-28": "Lemma C. 2 (Lemma F.6, Lemma F.7, Lemma F. 12 of Oko et al. (2023), Corollary 4.2 of Perekrestenko et al. (2018)). The following statements hold:\n\n1. Let $d \\geq 2, C \\geq 1, \\epsilon_{\\text {error }} \\in(0,1]$. For any $\\epsilon>0$, there exists a neural network $f_{\\text {mult }} \\in$ $\\Psi(L, W, S, B)$ with\n$L \\lesssim\\left(\\log \\epsilon^{-1}+d \\log C\\right) \\cdot \\log d, \\quad W \\lesssim d, \\quad S \\lesssim d \\log \\epsilon^{-1}+d \\log C, \\quad \\log B \\lesssim d \\log C$, such that, for any $x \\in[0, C]^{d}$ and $x \\in \\mathbb{R}^{d}$ with $\\left\\|x-x^{\\prime}\\right\\|_{\\infty} \\leq \\epsilon_{\\text {error }}$, it holds\n\n$$\n\\left|f_{\\text {mult }}\\left(x^{\\prime}\\right)-\\prod_{i=1}^{d} x_{i}\\right| \\leq \\epsilon+d \\cdot C^{d} \\cdot \\epsilon_{\\text {error }}\n$$\n\n2. For any $\\epsilon \\in(0,1)$, there exists $f_{\\mathrm{rec}} \\in \\Psi(L, W, S, B)$ with\n\n$$\nL \\lesssim \\log ^{2} \\epsilon^{-1}, \\quad W \\lesssim \\log ^{3} \\epsilon^{-1}, \\quad S \\lesssim \\log ^{4} \\epsilon^{-1}, \\quad \\log B \\lesssim \\log \\epsilon^{-1}\n$$\n\nsuch that, for any $x \\in\\left[\\epsilon, \\epsilon^{-1}\\right]$ and $x^{\\prime} \\in \\mathbb{R}$, it holds\n\n$$\n\\left|f_{\\mathrm{rec}}\\left(x^{\\prime}\\right)-\\frac{1}{x}\\right| \\leq \\epsilon+\\frac{\\left|x^{\\prime}-x\\right|}{\\epsilon^{2}}\n$$\n\n3. For any $\\epsilon>0$, there exists $f_{\\exp } \\in \\Psi(L, W, S, B)$ with\n\n$$\nL \\lesssim \\log ^{2} \\epsilon^{-1}, \\quad W \\lesssim \\log \\epsilon^{-1}, \\quad S \\lesssim \\log ^{2} \\epsilon^{-1}, \\quad \\log B \\lesssim \\log ^{2} \\epsilon^{-1}\n$$\n\nsuch that, for any $x, x^{\\prime} \\geq 0$, it holds\n\n$$\n\\left|f_{\\exp }\\left(x^{\\prime}\\right)-\\exp (x)\\right| \\leq \\epsilon+\\left|x^{\\prime}-x\\right|\n$$\n\n4. For any $\\epsilon>0, a>0, b \\in \\mathbb{R}, C \\geq 1$, there exists $f_{\\cos } \\in \\Psi(L, W, S, B)$ with\n\n$$\n\\begin{aligned}\n& L \\lesssim \\log ^{2} \\epsilon^{-1}+\\log (a D+b), \\quad W \\lesssim 1 \\\\\n& S \\lesssim \\log ^{2} \\epsilon^{-1}+\\log (a D+b), \\quad \\log B \\lesssim \\max \\{1, \\log |b / a|\\}\n\\end{aligned}\n$$\n\nsuch that, for any $x \\in[-D, D]$, it holds\n\n$$\n\\left|f_{\\cos }(x)-\\cos (a x+b)\\right| \\leq \\epsilon\n$$\n\nThe following is a famous fact that there exists a neural network that realize the clipping function. Lemma C.3. Let $a, b \\in \\mathbb{R}$. There exists a neural neural network $f_{\\text {clip }} \\in \\Psi(L, W, S, B)$ with\n\n$$\nL \\lesssim 1, \\quad W \\lesssim 1, \\quad S \\lesssim 1, \\quad B \\lesssim|a|+|b|\n$$\n\nsuch that, for any $x \\in \\mathbb{R}$, it holds\n\n$$\nf_{\\text {clip }}(x)= \\begin{cases}a & \\text { if } x \\leq a \\\\ x & \\text { if } a \\leq x \\leq b \\\\ b & \\text { if } b \\leq x\\end{cases}\n$$\n\nLastly, we state the following lemma, which shows that the dirac delta function can be approximated by a neural network. Lemma C.4. There exists $N \\in \\mathbb{N}$ and FNNs $\\phi_{n}, \\phi_{n}^{\\prime}, \\phi_{n}^{\\prime \\prime} \\in \\Psi_{1,1}(L, W, S, B), \\psi_{n}, \\psi_{n}^{\\prime} \\in$ $\\Psi^{\\prime}(1, B)(n=1, \\ldots, N)$ with\n\n$$\n\\begin{aligned}\n& N \\lesssim \\log \\epsilon^{-1} \\\\\n& L \\lesssim \\log ^{2} \\epsilon^{-1} \\log ^{2} \\kappa, \\quad W \\lesssim \\log ^{2} \\epsilon^{-1}, \\quad S \\lesssim \\log ^{4} \\epsilon^{-1} \\log ^{2} \\kappa, \\quad \\log B \\lesssim \\log \\epsilon^{-1} \\log \\kappa \\\\\n& L^{\\prime}=1, \\quad W^{\\prime} \\lesssim \\log ^{2} \\epsilon^{-1}, \\quad S^{\\prime} \\lesssim \\log ^{2} \\epsilon^{-1}\n\\end{aligned}\n$$\n\nsuch that,\n\n- for any $t, x \\in \\mathbb{R}$, it holds\n\n$$\n\\left|\\exp \\left(-\\kappa \\cdot \\sin ^{2}\\left(\\frac{\\pi}{2}(t-x)\\right)\\right)-\\sum_{n=1}^{N} \\psi_{n}(t) \\phi_{n}(x)\\right| \\lesssim \\epsilon\n$$\n\n- for any $x, y \\in \\mathbb{R}$, it holds\n\n$$\n\\left|\\exp \\left(-\\kappa \\cdot \\sin ^{2}\\left(\\frac{\\pi}{2}(x-y)\\right)\\right)-\\sum_{n=1}^{N} \\phi_{n}^{\\prime}(x) \\phi_{n}^{\\prime \\prime}(y)\\right| \\lesssim \\epsilon\n$$\n\n- for any $t \\in[-1,1]$, it holds\n\n$$\n\\left|\\exp \\left(-\\kappa \\cdot \\sin ^{2}\\left(\\frac{\\pi t}{2}\\right)\\right)-\\sum_{n=1}^{N} \\psi_{n}^{\\prime}(t)\\right| \\lesssim \\epsilon\n$$\n\nProof. The first part of the proof is inspired by Lemma F.",
    "dynsmoothssms-29": "12 of Oko et al. (2023). Let us set $A=\\log 3 \\epsilon^{-1}$. The Taylor expansion of exp shows that, for any $x \\in[0, A]$, it holds\n\n$$\n\\left|\\exp (-x)-\\sum_{n=0}^{N-1} \\frac{(-1)^{n}}{n!} x^{n}\\right| \\leq \\frac{A^{N}}{N!}\n$$\n\nAdditionally, we can evaluate the right-hand side as $A^{k} / k!\\leq(\\mathrm{e} A / k)^{k}$. Therefore, if we set $N=\\max \\left\\{2 \\mathrm{e} A,\\left\\lceil\\log _{2} 3 \\epsilon^{-1}\\right\\rceil\\right\\}$, the error can be bounded by $\\epsilon / 3$. Moreover, for $x>A$, we have\n\n$$\n\\begin{aligned}\n\\left|\\exp (-x)-\\sum_{n=0}^{N-1} \\frac{(-1)^{n}}{n!} x^{n}\\right| & \\leq|\\exp (-x)-\\exp (-A)|+\\left|\\exp (-A)-\\sum_{n=0}^{N-1} \\frac{(-1)^{n}}{n!} x^{n}\\right| \\\\\n& \\leq \\frac{\\epsilon}{3}+\\frac{2 \\epsilon}{3}=\\epsilon\n\\end{aligned}\n$$\n\nNext, let us approximate $\\sum_{n=0}^{N-1} \\frac{(-\\kappa)^{n}}{n!} \\sin ^{2 n}\\left(\\frac{\\pi}{2}(t-x)\\right)$. We use the fact that\n\n$$\n\\begin{aligned}\n\\sin ^{2 n}(x) & =\\left(\\frac{\\mathrm{e}^{i x}-\\mathrm{e}^{-i x}}{2}\\right)^{2 n}=\\frac{1}{2^{2 n}} \\sum_{k=0}^{2 n}\\binom{2 n}{k}(-1)^{k} \\mathrm{e}^{i(2 k-2 n) x} \\\\\n& =\\frac{(-1)^{n}}{2^{2 n}}\\binom{2 n}{n}+\\sum_{k \\geq n+1} \\frac{(-1)^{k}}{2^{2 n-1}}\\binom{2 n}{k} \\cos ((2 k-2 n) x)\n\\end{aligned}\n$$\n\nwhere $c_{n}=1$ if $n$ is even and $c_{n}=0$ if $n$ is odd. Thus, we have\n\n$$\n\\begin{aligned}\n\\sum_{n=0}^{N-1} & \\frac{(-\\kappa)^{n}}{n!} \\sin ^{2 n}\\left(\\frac{\\pi}{2}(t-x)\\right) \\\\\n= & \\sum_{n=0}^{N-1} \\frac{\\kappa^{n}}{n!2^{2 n}}\\binom{2 n}{n}+\\sum_{n=0}^{N-1} \\sum_{k \\geq n+1} \\frac{(-\\kappa)^{n}}{n!} \\frac{1}{2^{2 n-1}}\\binom{2 n}{k} \\cos (\\pi(k-n)(t-x)) \\\\\n= & \\sum_{n=0}^{N-1} \\frac{\\kappa^{n}}{n!2^{2 n}}\\binom{2 n}{n}+\\sum_{n=0}^{N-1} \\sum_{k \\geq n+1} \\frac{(-\\kappa)^{n}}{n!} \\frac{1}{2^{2 n-1}}\\binom{2 n}{k}(\\cos (\\pi(k-n) t) \\cos (\\pi(k-n) x) \\\\\n& +\\sin (\\pi(k-n) t) \\sin (\\pi(k-n) x))\n\\end{aligned}\n$$\n\nwhich is decomposed into the sum of products of functions of $t$ and $x$. Since\n\n$$\n\\left|\\frac{(-\\kappa)^{n}}{n!} \\frac{1}{2^{2 n-1}}\\binom{2 n}{k}\\right| \\leq \\frac{\\kappa^{n}}{n!2^{n}} \\frac{(2 n)!}{k!(2 n-k)!} \\leq \\frac{\\kappa^{n}}{n!2^{n}} \\frac{2^{n}(n!)^{2}}{(\\max (k, 2 n-k))!}=\\frac{\\kappa^{n}}{n!2^{n}} \\frac{2^{n}(n!)^{2}}{n!} \\leq \\kappa^{N}\n$$\n\nwe can see that, there exists $C_{0}, C_{n, k}(n \\in[N], k \\in[0: n / 2])$ with\n\n$$\nC_{0} \\leq \\kappa^{N}, \\quad C_{n, k} \\leq \\kappa^{N}\n$$\n\nsuch that\n\n$$\n\\begin{array}{r}\n\\sum_{n=0}^{N-1} \\frac{(-\\kappa)^{n}}{n!} \\sin ^{2 n}\\left(\\frac{\\pi}{2}(t-x)\\right)=C_{0}+\\sum_{n=0}^{N-1} \\sum_{k \\geq(n+1) / 2} C_{n, k}(\\cos (\\pi(k-n) t) \\cos (\\pi(k-n) x) \\\\\n+\\sin (\\pi(k-n) t) \\sin (\\pi(k-n) x))\n\\end{array}\n$$\n\nThe second equation to be proved is already obtained setting $x=0$. Finally, we approximate each term using neural networks. Lemma C.2 implies that, for any $n, k$ and $\\epsilon>0$, there exists a neural network $\\phi_{1, n, k}, \\phi_{2, n, k} \\in \\Psi_{1,1}(L, W, S, B)$ with\n\n$$\nL \\lesssim N^{2} \\log ^{2} \\kappa+\\log ^{2} \\epsilon^{-1}, \\quad W \\lesssim 1, \\quad S \\lesssim N^{2} \\log ^{2} \\kappa+\\log ^{2} \\epsilon^{-1}, \\quad \\log B \\lesssim 1\n$$\n\nsuch that\n\n$$\n\\left|\\cos (\\pi(k-n) x)-\\phi_{1, n, k}(x)\\right| \\leq \\epsilon /\\left(N^{2} \\kappa^{N}\\right), \\quad\\left|\\sin (\\pi(k-n) x)-\\phi_{2, n, k}(x)\\right| \\leq \\epsilon /\\left(N^{2} \\kappa^{N}\\right)\n$$\n\nThen, if we approximate $\\exp (-\\kappa \\cdot \\cos (2 \\pi(t-x)))$ by\n\n$$\nC_{0}+\\sum_{n=0}^{N-1} \\sum_{k \\geq(n+1) / 2} C_{n, k}\\left(\\cos (\\pi(k-n) t) \\phi_{1, n, k}(x)+\\sin (\\pi(k-n) t) \\phi_{2, n, k}(x)\\right)\n$$\n\nthe error can be bounded by\n\n$$\n\\epsilon+\\sum_{n=0}^{N-1} \\sum_{k \\geq(n+1) / 2} C_{n, k} \\cdot \\frac{2 \\epsilon}{N^{2} \\kappa^{N}} \\leq \\epsilon+N^{2} \\kappa^{N} \\cdot \\frac{2 \\epsilon}{N^{2} \\kappa^{N}} \\leq 3 \\epsilon\n$$\n\nwhich completes the proof. ## D Proof of Theorem 4.2\n\nGiven a smoothness function $\\gamma: \\mathbb{N}_{0}^{d \\times \\infty} \\rightarrow \\mathbb{R}$, we define\n\n$$\n\\begin{aligned}\nI(T, \\gamma) & :=\\left\\{(i, j) \\mid \\exists s \\in \\mathbb{N}_{0}^{d \\times \\infty} \\text { such that } s_{i j} \\neq 0, \\gamma(s)<T\\right\\} \\\\\nd_{\\max } & :=|I(T, \\gamma)|\n\\end{aligned}\n$$\n\nThe feature extraction map $\\Gamma: \\mathbb{R}^{d \\times \\infty} \\rightarrow \\mathbb{R}^{d_{\\max }}$ is defined as\n\n$$\n\\Gamma(X)=\\left[X_{i_{1}, j_{1}}, \\ldots, X_{i_{d_{\\max }}, j_{d_{\\max }}}\\right]\n$$\n\nThe following lemma shows that, if FNN receives finite number of \"important\" features, it can approximate $\\gamma$-smooth functions and piecewise $\\gamma$-smooth functions.",
    "dynsmoothssms-30": "This is mainly due to the condition $\\|a\\|_{w l^{\\alpha}} \\leq 1$, which induces sparsity of important features.",
    "dynsmoothssms-31": "Lemma D. 1 (Theorem D. 3 in Takakura and Suzuki (2023)). Suppose that the target functions $f \\in \\mathcal{F}_{p, \\theta}^{\\gamma}$ and $g \\in \\mathcal{P}_{p, \\theta}^{\\gamma}$ satisfy $\\|f\\|_{\\infty} \\leq R$ and $\\|g\\|_{\\infty} \\leq R$, where $R>0$ and $\\gamma$ is the mixed or anisotropic smoothness and the smoothness parameter a satisfies $\\|a\\|_{w^{\\alpha}} \\leq 1$. For any $T>0$, there exist FNNs $\\hat{f}_{T}, \\hat{g}_{T} \\in \\Psi(L, W, S, B)$ such that\n\n$$\n\\begin{aligned}\n\\left\\|\\hat{f}_{T} \\circ \\Gamma-f\\right\\|_{2, P_{X}} & \\lesssim 2^{-T} \\\\\n\\left\\|\\hat{g}_{T} \\circ \\Gamma \\circ \\Pi-g\\right\\|_{2, P_{X}} & \\lesssim 2^{-T}\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& L \\sim \\max \\left\\{T^{2 / \\alpha}, T^{2}\\right\\}, W \\sim T^{1 / \\alpha} 2^{T / a^{\\dagger}} \\\\\n& S \\sim T^{2 / \\alpha} \\max \\left\\{T^{2 / \\alpha}, T^{2}\\right\\} 2^{T / a^{\\dagger}}, \\log B \\sim T^{1 / \\alpha}\n\\end{aligned}\n$$\n\nFrom this lemma, we can see that, if the the convolution layer can approximate $\\Gamma$, the SSM can give important features to the FNN, and the FNN can approximate the target function. Now, we prove Theorem 4.2\nProof of Theorem 4.2 Firstly, we construct the embedding layer Emb: $\\mathbb{R}^{d \\times \\infty} \\rightarrow \\mathbb{R}^{D \\times \\infty}$. Set the embedding dimension $D$ as $\\max \\left\\{d, d_{\\max }\\right\\}+1$. We set $E_{1} \\in \\mathbb{R}^{D \\times d}$ to satisfy\n\n$$\nE_{1} x=[x_{1}, \\ldots, x_{d}, 0, \\underbrace{0, \\ldots, 0}_{D-d-1 \\text { elements }}]^{\\top}\n$$\n\nfor $x=\\left[x_{1}, \\ldots, x_{d}\\right] \\in \\mathbb{R}^{d}$. Additionally, we set $E_{2} \\in \\mathbb{R}^{D}$ to satisfy\n\n$$\nE_{2}=[\\underbrace{0, \\ldots, 0}_{d \\text { elements }}, 1, \\underbrace{0, \\ldots, 0}_{D-d-1 \\text { elements }}]^{\\top} . $$\n\nNote that $\\left\\|E_{1}\\right\\|_{\\infty}=\\left\\|E_{2}\\right\\|_{\\infty}=1$. Then, the constructed embedding layer Emb is represented as follows:\n\n$$\n\\operatorname{Emb}(X)=\\left[\\begin{array}{ccc}\n\\cdots & x_{t} & \\cdots \\\\\n\\cdots & 1 & \\cdots \\\\\n\\cdots & 0 & \\cdots \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\cdots & 0 & \\cdots\n\\end{array}\\right] \\in \\mathbb{R}^{D \\times \\infty}\n$$\n\nSecondly, we construct the gated convolution layer. The role of this layer is to approximate the feature extractor $\\Gamma$. The weight matrix $W^{V} \\in \\mathbb{R}^{D \\times|X|}$ is set to extract the important \"dimensions\" $\\left(i_{1}, \\ldots, i_{d_{\\max }}\\right)$. More precisely, we set $W_{V}$ to satisfy\n\n$$\nW^{V} y=[y_{i_{1}}, \\ldots, y_{i_{d_{\\max }}}, \\underbrace{0, \\ldots, 0}_{D-d_{\\max } \\text { elements }}] \\in \\mathbb{R}^{D}\n$$\n\nfor $y=\\left[y_{1}, \\ldots, y_{D}\\right] \\in \\mathbb{R}^{D}$. Then, the resulted projection is represented as follows:\n\n$$\nW^{V}(\\operatorname{Emb}(X))=\\left[\\begin{array}{ccc}\n\\cdots & X_{t, i_{1}} & \\cdots \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\cdots & X_{t, i_{d_{\\max }}} & \\cdots \\\\\n\\cdots & 0 & \\cdots \\\\\n\\vdots & \\vdots & \\cdots \\\\\n\\cdots & 0 & \\cdots\n\\end{array}\\right] \\in \\mathbb{R}^{D \\times \\infty}\n$$\n\nNext, we construct the convolution filter. From the assumption $a_{i j}=\\Omega(\\log (|j|+1))$, we can choose the window size $U \\in \\mathbb{N}$ such that\n\n$$\n\\log U \\sim T \\quad \\text { and } \\quad a_{i j} \\leq T \\Longrightarrow j \\leq U\n$$\n\nLemma C. 4 shows that, for each $j_{m}\\left(m=1, \\ldots, d_{\\max }\\right)$, for any $\\epsilon>0, \\kappa>0$, there exists $k_{m} \\in \\Psi^{\\prime}\\left(W^{\\prime}, B\\right)$ with\n\n$$\nW^{\\prime} \\lesssim \\log ^{2} \\epsilon^{-1}, \\quad B \\lesssim \\log \\epsilon^{-1} \\log \\kappa\n$$\n\nsuch that\n\n$$\n\\max _{j=0, \\ldots, U}\\left|k_{m}\\left(\\frac{j}{U}\\right)-\\exp \\left(-\\kappa \\cdot \\sin ^{2}\\left(\\frac{\\pi}{2}\\left(\\frac{j}{U}-\\frac{j_{m}}{U}\\right)\\right)\\right)\\right| \\lesssim \\epsilon\n$$\n\nNow, if $\\left|j-j_{m}\\right| \\geq 1$, it holds\n\n$$\n\\exp \\left(-\\kappa \\cdot \\sin ^{2}\\left(\\frac{\\pi}{2}\\left(\\frac{j}{U}-\\frac{j_{m}}{U}\\right)\\right)\\right) \\leq \\exp \\left(-\\kappa \\cdot\\left(\\frac{2}{\\pi} \\cdot \\frac{\\pi}{2} \\cdot \\frac{1}{U}\\right)^{2}\\right)=\\exp \\left(-\\frac{\\kappa}{U^{2}}\\right)\n$$\n\nand, if $j=j_{m}$, it holds\n\n$$\n\\exp \\left(-\\kappa \\cdot \\sin ^{2}\\left(\\frac{\\pi}{2}\\left(\\frac{j}{U}-\\frac{j_{m}}{U}\\right)\\right)\\right)=1\n$$\n\nTherefore, if we set $\\kappa=U^{2} \\log \\epsilon^{-1}$, we have\n\n$$\n\\max _{j=0, \\ldots, U}\\left|k_{m}\\left(\\frac{j}{U}\\right)-\\delta_{j_{m}}(j)\\right| \\lesssim 2 \\epsilon\n$$\n\nwhere $\\delta_{j^{\\prime}}$ is the function defined by\n\n$$\n\\delta_{j^{\\prime}}(j)= \\begin{cases}1 & \\text { if } j=j^{\\prime} \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nThis inequality show that the filter $k$ can approximately extract the important tokens. Finally, we set the weight matrix $W^{Q}$ by\n\n$$\nW_{i, j}^{Q}= \\begin{cases}1 & \\text { if } j=d+1 \\\\ 0 & \\text { otherwise }\\end{cases}\n$$\n\nwhich results in $W^{Q}(\\operatorname{Emb}(X))=[1, \\ldots, 1]^{\\top}$ and\n\n$$\n\\begin{aligned}\ng_{1} \\circ \\operatorname{Emb}(X) & =W^{Q}(\\operatorname{Emb}(X)) \\odot\\left(\\beta^{(1)} * W^{(0)}(\\operatorname{Emb}(X))\\right) \\\\\n& =\\beta^{(1)} * W^{(0)}(\\operatorname{Emb}(X)) \\\\\n& =\\left[z_{t}\\right]_{t=-\\infty}^{\\infty} \\in \\mathbb{R}^{D \\times \\infty} \\\\\nz_{t} & =\\sum_{s=0}^{U-1} k(s) \\sum_{i=1}^{D} W_{i, t-s}^{(0)} X_{i, t-s} \\\\\n& =\\left[\\begin{array}{c}\n\\sum_{s=0}^{U-1}(k(s))_{1} X_{i_{1}, t-s} \\\\\n\\vdots \\\\\n\\sum_{s=0}^{U-1}(k(s))_{d_{\\max }} X_{i_{d_{\\max }}, t-s} \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{array}\\right]\n\\end{aligned}\n$$\n\nThirdly, we construct the FNN layer. From Lemma D.1, there exists an FNN $\\hat{f} \\in \\Psi(L, W, S, B)$ such that\n\n$$\n\\left\\|\\hat{f} \\circ \\Gamma-F_{0}^{\\circ}\\right\\|_{2, P_{X}} \\lesssim 2^{-T}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& L \\sim \\max \\left\\{T^{2 / \\alpha}, T^{2}\\right\\}, W \\sim T^{1 / \\alpha} 2^{T / a^{\\dagger}} \\\\\n& S \\sim T^{2 / \\alpha} \\max \\left\\{T^{2 / \\alpha}, T^{2}\\right\\} 2^{T / a^{\\dagger}}, \\log B \\sim T^{1 / \\alpha}\n\\end{aligned}\n$$\n\nLet $C: \\mathbb{R}^{D} \\rightarrow \\mathbb{R}^{d}$ be a linear map such that\n\n$$\nC y=\\left[y_{1}, \\ldots, y_{d_{\\max }}\\right]^{\\top}\n$$\n\nfor $y=\\left[y_{1}, \\ldots, y_{D}\\right]^{\\top} \\in \\mathbb{R}^{D}$, and we set $f_{1}:=\\hat{f} \\circ C$. Note that $f_{1} \\in \\Psi(L, W, S, B)$ for $L, W, S, B$ defined in D.2. The constructed data-controlled $\\operatorname{SSM} \\hat{F}_{t}(t \\in \\mathbb{Z})$ is represented as follows:\n\n$$\n\\hat{F}_{t}(X)=f_{1}\\left(z_{t}\\right)=\\hat{f} \\circ C\\left(z_{t}\\right)=\\hat{f} \\circ \\hat{\\Gamma} \\circ \\Sigma_{t}(X)\n$$\n\nwhere\n\n$$\n\\hat{\\Gamma}(X)=\\left[\\sum_{s=0}^{U-1}(k(s))_{m} X_{i_{m},-s}\\right]_{m=1}^{d_{\\max }} \\in \\mathbb{R}^{d_{\\max }}\n$$\n\nNow, we evaluate the error between the target function $F_{t}^{\\circ}$ and the constructed model $\\hat{F}_{t}$ for $t \\in \\mathbb{Z}$. Due to the shift-equivariance of $F^{\\circ}$, we have $F_{t}^{\\circ}=F_{0}^{\\circ} \\circ \\Sigma_{t}$. Additionally, we can easily check that $\\hat{F}_{t}$ is also shift-equivariant, i.e., $\\hat{F}_{t}(X)=\\hat{F}_{0}\\left(X \\circ \\Sigma_{t}\\right)$. Moreover, since $P_{X}$ is also shift-equivariant, we have $\\|u\\|_{2, P_{X}}=\\left\\|u \\circ \\Sigma_{t}\\right\\|_{2, P_{X}}$ for any $t \\in \\mathbb{Z}$ and $u: \\mathbb{R}^{d \\times \\infty} \\rightarrow \\mathbb{R}^{d}$ such that $\\|u\\|_{2, P_{X}}<\\infty$. Therefore, it holds\n\n$$\n\\left\\|\\hat{F}_{t}-F_{t}^{\\circ}\\right\\|_{2, P_{X}}=\\left\\|\\hat{F}_{0} \\circ \\Sigma_{t}-F_{0}^{\\circ} \\circ \\Sigma_{t}\\right\\|_{2, P_{X}}=\\left\\|\\hat{F}_{0}-F_{0}^{\\circ}\\right\\|_{2, P_{X}}\n$$\n\nfor any $t \\in \\mathbb{Z}$. Therefore, it is sufficient to evaluate $\\left\\|\\hat{F}_{0}-F_{0}^{\\circ}\\right\\|_{2, P_{X}}$. We evaluate the error by separating into two terms:\n\n$$\n\\left\\|\\hat{F}_{0}-F_{0}^{\\circ}\\right\\|_{2, P_{X}} \\leq\\left\\|\\hat{F}_{0}-\\hat{f} \\circ \\Gamma\\right\\|_{2, P_{X}}+\\left\\|\\hat{f} \\circ \\Gamma-F_{0}^{\\circ}\\right\\|_{2, P_{X}}\n$$\n\nThe second term can be bounded by (D.1), so we evaluate the first term. Since $\\hat{f} \\in \\Psi(L, W, S, B)$ is $(B W)^{L}$-lipschitz continuous, for any $X \\in[0,1]^{d \\times \\infty}$, we have\n\n$$\n\\left|\\hat{F}_{0}(X)-\\hat{f} \\circ \\Gamma(X)\\right|=|\\hat{f}(\\hat{\\Gamma}(X))-\\hat{f}(\\Gamma(X))| \\leq(B W)^{L}\\|\\hat{\\Gamma}(X)-\\Gamma(X)\\|_{\\infty}\n$$\n\nSince $X \\in[0,1]^{d \\times \\infty}$, it holds\n\n$$\n\\begin{aligned}\n\\|\\hat{\\Gamma}(X)-\\Gamma(X)\\|_{\\infty} & =\\max _{m=1, \\ldots, d_{\\max }}\\left|\\sum_{s=0}^{U-1}(k(s))_{m} X_{i_{m},-s}-\\delta_{j_{m}}(s) X_{i_{m},-s}\\right| \\\\\n& \\leq \\max _{m=1, \\ldots, d_{\\max }} \\sum_{s=0}^{U-1}\\left|(k(s))_{m}-\\delta_{j_{m}}(s)\\right| \\\\\n& \\leq U \\epsilon\n\\end{aligned}\n$$\n\nBy setting $\\epsilon=2^{-T} / U$, we have\n\n$$\n\\left|\\hat{F}_{0}(X)-\\hat{f} \\circ \\Gamma(X)\\right| \\leq\\|\\hat{\\Gamma}(X)-\\Gamma(X)\\|_{\\infty} \\leq 2^{-T}\n$$\n\nfor any $X \\in[0,1]^{d \\times \\infty}$. Therefore, it holds\n\n$$\n\\begin{aligned}\n\\left\\|\\hat{F}_{0}-F_{0}^{\\circ}\\right\\|_{2, P_{X}} & \\leq\\left\\|\\hat{F}_{0}-\\hat{f} \\circ \\Gamma\\right\\|_{2, P_{X}}+\\left\\|\\hat{f} \\circ \\Gamma-F_{0}^{\\circ}\\right\\|_{2, P_{X}} \\\\\n& \\leq \\sup _{X \\in[0,1]^{d \\times \\infty}}\\left|\\hat{F}_{0}(X)-\\hat{f} \\circ \\Gamma(X)\\right|+\\left\\|\\hat{f} \\circ \\Gamma-F_{0}^{\\circ}\\right\\|_{2, P_{X}} \\\\\n& \\lesssim 2^{-T}\n\\end{aligned}\n$$\n\nFinally, we evaluate the parameters $L, W, S, B$ which controls the class of $k \\in \\Psi^{\\prime}\\left(W^{\\prime}, B\\right)$. Since $\\|a\\|_{w l^{\\alpha}}=\\sup _{j} j^{\\alpha} \\bar{a}_{j}^{-1} \\leq 1$, it holds\n\n$$\nd_{\\max }:=\\left|\\left\\{(i, j) \\mid \\exists s \\in \\mathbb{N}_{0}^{d \\times \\infty}, s_{i j} \\neq 0, \\gamma(s)<T\\right\\}\\right| \\leq T^{1 / \\alpha}\n$$\n\nTherefore, we have\n\n$$\n\\begin{aligned}\nW^{\\prime} & =d_{\\max } \\cdot \\log ^{2} \\epsilon^{-1} \\lesssim T^{2+1 / \\alpha} \\\\\n\\log B & \\sim \\log \\epsilon^{-1} \\log \\left(U^{2} \\log \\epsilon^{-1}\\right) \\lesssim T^{2}\n\\end{aligned}\n$$\n\nThis completes the proof. ## E Proof of Theorem 4.4\n\n## E. 1 Proof for the case of (i) $\\gamma$-smooth importance function\n\nProof of Theorem 4.4 for the case of (i) $\\gamma$-smooth importance function. For $T>0$, we define\n\n$$\n\\begin{aligned}\nI_{j}(T, \\gamma) & :=\\{i \\mid(i, j) \\in I(T, \\gamma)\\}=\\left\\{i_{1}^{(j)}, \\ldots, i_{\\left|I_{j}\\right|}^{(j)}\\right\\} \\\\\nr_{\\max }(T, \\gamma) & :=\\max \\left\\{j \\in[J] \\mid I_{j}(T, \\gamma) \\neq \\emptyset\\right\\}\n\\end{aligned}\n$$\n\nNote that $r_{\\max }(T, \\gamma) \\sim T^{1 / \\alpha}$ since $a_{i j}=\\Omega\\left(j^{\\alpha}\\right)$. Theorem 4.2 implies that there exist an embedding layer Emb, an FNN $f_{1} \\in \\Psi(L, W, S, B)$ and a gated convolution layer $g_{1} \\in \\mathcal{C}\\left(U, D, L^{\\prime}, W^{\\prime}, S, B\\right)$ with\n\n$$\n\\begin{aligned}\n& M=1, \\log U \\sim T, D \\sim T^{1 / \\alpha} \\\\\n& L \\sim T, W_{1} \\sim T^{1 / \\alpha} \\\\\n& L^{\\prime} \\sim \\max \\left\\{T^{2 / \\alpha}, T^{2}\\right\\}, W^{\\prime} \\sim T^{1 / \\alpha} 2^{T / a^{\\dagger}} \\\\\n& S \\sim T^{2 / \\alpha} \\max \\left\\{T^{2 / \\alpha}, T^{2}\\right\\} 2^{T / a^{\\dagger}}, \\log B \\sim T^{1 / \\alpha}\n\\end{aligned}\n$$\n\nsuch that\n\n$$\nf_{1} \\circ g_{1} \\circ \\operatorname{Emb}(X)_{i}=[x_{i}^{\\top}, \\widehat{\\mu}_{i}(X), \\underbrace{0, \\ldots, 0}_{d_{\\max } \\text { elements }}, \\underbrace{-1, \\ldots,-1}_{r_{\\max } \\text { elements }}]^{\\top},\n$$\n\nfor all $i \\in \\mathbb{Z}$, where $\\widehat{\\mu}_{i}(X)$ satisfies\n\n$$\n\\left|\\widehat{\\mu}_{i}(X)_{-t}-\\left(\\mu_{i}(X)-1\\right)\\right| \\lesssim 2^{-T}\n$$\n\nIntuitively, the $i$-th elements for $i=3, \\ldots, 2+d_{\\max }$ are used to store the feature $X_{t-i, j}$ for $j \\in[d]$, and the $i$-th elements for $i=3+d_{\\max }, \\ldots, 2+d_{\\max }+r_{\\max }$ are buffers to store which elements are already selected. Note that, for any $i \\leq r_{\\max }$, it holds\n\n$$\n\\widehat{\\mu}(X)_{\\pi_{\\lambda}(i)}-\\widehat{\\mu}(X)_{\\pi_{\\lambda}(i+1)} \\gtrsim\\left(\\mu(X)_{\\pi_{\\lambda}(i)}-2^{-T}\\right)-\\left(\\mu(X)_{\\pi_{\\lambda}(i+1)}+2^{-T}\\right) \\gtrsim T^{-\\beta / \\alpha}\n$$\n\nand $\\widehat{\\mu}(X)_{t} \\in[-1,0]$ for all $t \\in[0: V]$. In the following, we set $U=V$. Let us set $\\chi_{T} \\sim \\frac{T \\log 2+2 \\log U}{T^{-\\beta / \\alpha}}$. Using Lemma C.2. we see that, there exists a neural network $\\phi_{\\exp } \\in \\Psi(L, W, S, B)$ with\n$L \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} U, \\quad W \\lesssim T^{1+\\beta / \\alpha} \\log U, \\quad S \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} U, \\quad \\log B \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} U$, such that, for any $x \\leq 0$, it holds\n\n$$\n\\left|\\phi_{\\exp }\\left(\\chi_{T} x\\right)-\\exp \\left(\\chi_{T} x\\right)\\right| \\leq 2^{-2 T^{1+\\beta / \\alpha}} / U^{3}\n$$\n\nMoreover, using Lemma C. 2 again, we see that there exists a neural network $\\phi_{\\times}$with\n\n$$\nL \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} U, \\quad W \\lesssim 1, \\quad S \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} U, \\quad \\log B \\lesssim T^{1+\\beta / \\alpha} \\log U\n$$\n\nsuch that, for any $0 \\leq x \\lesssim U^{2} \\exp \\left(T^{1+\\beta / \\alpha}\\right), 0 \\leq y \\lesssim 1$, it holds\n\n$$\n\\left|\\phi_{\\times}(x, y)-x y\\right| \\leq 2^{-2 T^{1+\\beta / \\alpha}} / U^{3}\n$$\n\nThen, for any $x \\leq 0$ and $y \\in[0,1]$, it holds\n\n$$\n\\begin{aligned}\n\\left|\\phi_{\\times}\\left(\\phi_{\\exp }\\left(\\chi_{T} x\\right), y\\right)-\\exp \\left(\\chi_{T} x\\right) y\\right| & \\leq\\left|\\phi_{\\times}\\left(\\phi_{\\exp }\\left(\\chi_{T} x\\right), y\\right)-\\phi_{\\exp }\\left(\\chi_{T} x\\right) y\\right|+\\left|\\phi_{\\exp }\\left(\\chi_{T} x\\right) y-\\exp \\left(\\chi_{T} x\\right) y\\right| \\\\\n& \\leq 2^{-2 T^{1+\\beta / \\alpha}} / U^{3}+2^{-2 T^{1+\\beta / \\alpha}} / U^{3} \\\\\n& \\lesssim 2^{-2 T^{1+\\beta / \\alpha}} / U^{3}\n\\end{aligned}\n$$\n\nThen, let us define $f_{1}^{\\prime}$ be an FNN layer such that it holds\n\n$$\nf_{1}^{\\prime} \\circ f_{1} \\circ g_{1}(X) \\circ \\operatorname{Emb}(X)_{i}=[\\phi_{\\times}\\left(\\phi_{\\exp }\\left(\\widehat{\\mu}_{i}(X)\\right), x_{i}\\right), \\underbrace{0, \\ldots, 0}_{d_{\\max } \\text { elements }}, \\underbrace{-1, \\ldots,-1}_{r_{\\max } \\text { elements }}]^{\\top}\n$$\n\nAdditionally, we define\n\n$$\nZ_{m}:=\\left(f_{m}^{\\prime} \\circ g_{m} \\circ f_{m}\\right) \\circ \\cdots \\circ\\left(f_{1}^{\\prime} \\circ g_{1} \\circ f_{1}\\right) \\circ \\operatorname{Emb}(X)\n$$\n\nfor $m \\in\\left[1: r_{\\max }\\right]$. We construct remaining layers $f_{2}, g_{2}, f_{2}^{\\prime}, \\ldots, f_{r_{\\max }+1}, g_{r_{\\max }+1}, f_{r_{\\max }+1}^{\\prime}$ to make them satisfying\n\n$$\n\\begin{array}{r}\nZ_{m}=[\\phi_{\\times}\\left(\\phi_{\\exp }\\left(\\widehat{\\mu}_{i}(X)\\right), x_{i}\\right), \\widehat{X}_{i_{1}^{(1)}, j_{1}}, \\ldots, \\widehat{X}_{i_{\\left|I_{1}\\right|}^{(1)}, j_{1}}, \\ldots, \\widehat{X}_{i_{1}^{(m)}, j_{m}}, \\ldots, \\widehat{X}_{i_{\\left|I_{m}\\right|}^{(m)}, j_{m}}, \\underbrace{0, \\ldots, 0}_{d_{\\max }-\\sum_{j=1}^{m}\\left|I_{j}\\right| \\text { elements }} \\\\\n\\widehat{j}_{1} / U, \\ldots, \\widehat{j}_{m} / U, \\underbrace{-1, \\ldots,-1}_{r_{\\max }-m \\text { elements }}]^{\\top}\n\\end{array}\n$$\n\nwhere $\\widehat{X}_{i_{k}^{\\left(j_{m}\\right)}, j_{m}}, \\widehat{j}_{m}$ are the approximation of $\\widehat{X}_{i_{k}^{\\left(j_{m}\\right)}, j_{m}}, \\widehat{j}_{m}\\left(m=1, \\ldots, M ; k=1, \\ldots,\\left|I_{j_{m}}\\right|\\right)$ respectively such that\n\n$$\n\\left|\\widehat{X}_{i_{k}^{\\left(j_{m}\\right)}, j_{m}}-X_{i_{k}^{\\left(j_{m}\\right)}, j_{m}}\\right| \\lesssim 2^{-T}, \\quad\\left|\\widehat{j}_{m} / U-j_{m} / U\\right| \\lesssim 2^{-3 T^{1+\\beta / \\alpha}} / V^{5}\n$$\n\nThen, we see that\n$Z_{M}=\\left[x_{i}^{\\top}, \\widehat{\\mu}_{i}(X), \\widehat{X}_{i_{1}^{(1)}, j_{1}}, \\ldots, \\widehat{X}_{i_{\\left|I_{1}\\right|}^{(1)}, j_{1}}, \\ldots, \\widehat{X}_{i_{1}^{\\left(r_{\\max }\\right)}, j_{r_{\\max }}}, \\ldots, \\widehat{X}_{i_{\\left|I_{r_{\\max } \\mid}^{\\left(r_{\\max } \\mid\\right.}\\right|}, j_{r_{\\max }}}, \\widehat{j}_{1} / U, \\ldots, \\widehat{j}_{M} / U\\right]^{\\top}$. Hence, Lemma D.1 shows that there exists a FNN $f_{M}^{\\prime} \\in \\Psi(L, W, S, B)$ with\n\n$$\n\\begin{aligned}\n& L \\lesssim \\max \\left\\{T^{2 / \\alpha}, T^{2}\\right\\}, \\quad W \\lesssim T^{1 / \\alpha} 2^{T / \\alpha^{\\dagger}} \\\\\n& S \\lesssim T^{2 / \\alpha} \\max \\left\\{T^{2 / \\alpha, T^{2}}\\right\\} 2^{T / \\alpha^{\\dagger}}, \\quad \\log B \\lesssim T^{1 / \\alpha}\n\\end{aligned}\n$$\n\nsuch that\n\n$$\n\\left\\|f_{M}^{\\prime}\\left(Z_{M}\\right)-f\\right\\|_{2} \\lesssim 2^{-T}\n$$\n\nThe same discussion as Theorem 4.2 gives the desired result. In the following, we construct an FNN $f_{m}$ and a gated convolution layer $g_{m}$ for $m \\in\\left[1: r_{\\text {max }}\\right]$. The proof mainly divided into two parts: (i) obtaining $\\widehat{X}_{i_{k}^{(m)}, j_{m}}$, i.e., the approximation of important features $X_{i_{k}^{(m)}, j_{m}}\\left(k=1, \\ldots,\\left|I_{m}\\right|\\right)$ and (ii) getting $\\widehat{j}_{m}$, i.e., recording which token $j_{m}$ was selected. Picking up the important features $X_{i_{k}^{(m)}, j_{m}}\\left(k=1, \\ldots,\\left|I_{m}\\right|\\right) \\quad$ Due to Lemma C.1 and the fact that $j_{m} \\in[0: V]$ is an index such that $\\mu_{t-j}\\left(\\widehat{\\mu}_{t-j}\\right)$ is the largest in $\\mu_{t-j}\\left(\\widehat{\\mu}_{t-j}\\right)\\left(j \\neq j_{1}, \\ldots, j_{m-1}\\right)$, for any $t \\in[0: U]$ with $t \\neq t_{0}$, it holds\n\n$$\n\\left|\\frac{\\sum_{j=0}^{V} X_{i, j} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}_{t-j}\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right)}{\\sum_{j=0}^{V} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}_{t-j}\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right)}-X_{i, j_{m}}\\right| \\leq 2 U^{2} \\exp \\left(-\\chi_{T} \\cdot T^{-\\beta / \\alpha}\\right) \\lesssim 2^{-T}\n$$\n\nwhere $S=\\left\\{j_{1}, \\ldots, j_{m-1}\\right\\}$. Now, let us approximate\n\n$$\n\\frac{\\sum_{j=0}^{V} X_{i, j} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}_{t_{i}}\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right)}{\\sum_{j=0}^{V} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}_{t-i}\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right)}=\\frac{\\frac{1}{V} \\sum_{j=0}^{V} X_{i, j} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}_{t_{i}}\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right)}{\\frac{1}{V} \\sum_{j=0}^{V} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}_{t-i}\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right)}\n$$\n\nusing neural networks. Using LemmaC.2 we see that, for any $\\epsilon_{1}>0$, there exists a neural network $\\phi_{\\text {rec }} \\in \\Psi(L, W, S, B)$ with\n\n$$\nL \\lesssim \\log ^{2} \\epsilon_{1}^{-1}, \\quad W \\lesssim \\log ^{3} \\epsilon_{1}^{-1}, \\quad S \\lesssim \\log ^{4} \\epsilon_{1}^{-1}, \\quad \\log B \\lesssim \\log \\epsilon_{1}^{-1}\n$$\n\nsuch that, for any $x \\in\\left[\\epsilon_{1}, \\epsilon_{1}^{-1}\\right]$, it holds\n\n$$\n\\left|\\phi_{\\mathrm{rec}}(x)-\\frac{1}{x}\\right| \\leq \\epsilon_{1}\n$$\n\nand $\\phi_{\\text {rec }}(x) \\in(0,1]$. Moreover, using Lemma C. 2 again, we see that, for any $\\epsilon_{3}>0$, there exists a neural network $\\phi_{\\text {mult }} \\in \\Psi(L, W, S, B)$ with\n\n$$\nL \\lesssim \\log \\epsilon_{2}^{-1}, \\quad W \\lesssim 1, \\quad S \\lesssim \\log \\epsilon_{2}^{-1}, \\quad \\log B \\lesssim 1\n$$\n\nsuch that, for any $x \\in[0,1]^{2}$, it holds\n\n$$\n\\left|\\phi_{\\text {mult }}(x)-x_{1} x_{2}\\right| \\leq \\epsilon_{2}\n$$\n\nSetting $\\epsilon_{1}=\\min \\left\\{\\exp \\left(-\\chi_{T}\\right), 2^{-T}\\right\\}, \\epsilon_{2}=2^{-T}$ and\n\n$$\n\\phi_{*}:\\left[\\exp \\left(-\\chi_{T}\\right), 1\\right] \\times[0,1] \\rightarrow \\mathbb{R},(x, y) \\mapsto \\phi_{\\mathrm{mult}}\\left(\\phi_{\\mathrm{rec}}(x), y\\right)\n$$\n\nwe see that $\\phi_{*} \\in \\Psi(L, W, S, B)$ with $L \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} U, \\quad W \\lesssim T^{3(1+\\beta / \\alpha)} \\log ^{3} U, \\quad S \\lesssim T^{4(1+\\beta / \\alpha)} \\log ^{4} U, \\quad \\log B \\lesssim T^{1+\\beta / \\alpha} \\log U$, and, for any $x \\in\\left[\\exp \\left(-\\chi_{T}\\right), \\exp \\left(\\chi_{T}\\right)\\right], y \\in[0, U]$, it holds\n\n$$\n\\begin{aligned}\n\\left|\\phi_{*}(x, y)-\\frac{y}{x}\\right| & =\\left|\\phi_{\\mathrm{mult}}\\left(\\phi_{\\mathrm{rec}}(x), y\\right)-\\frac{y}{x}\\right| \\\\\n& \\leq\\left|\\phi_{\\mathrm{mult}}\\left(\\phi_{\\mathrm{rec}}(x), y\\right)-y \\phi_{\\mathrm{rec}}(x)\\right|+\\left|y \\phi_{\\mathrm{rec}}(x)-\\frac{y}{x}\\right| \\\\\n& \\leq \\epsilon_{2}+\\epsilon_{1} \\lesssim 2^{-T}\n\\end{aligned}\n$$\n\nNow, if we set $w(x, y):=y / x$ for $x, y>0$, we have\n\n$$\n\\|\\nabla w(x, y)\\|_{2}=\\sqrt{\\left(\\frac{1}{x}\\right)^{2}+\\left(-\\frac{y}{x^{2}}\\right)^{2}}=\\frac{1}{x} \\sqrt{1+\\left(\\frac{y}{x}\\right)^{2}}\n$$\n\nTherefore, if $x \\geq 1$ and $0 \\leq y \\leq x$, it holds\n\n$$\n\\|\\nabla w(x, y)\\|_{2} \\leq \\frac{1}{x} \\sqrt{1+\\left(\\frac{x}{x}\\right)^{2}}=\\sqrt{2} \\exp \\left(\\chi_{T}\\right) \\lesssim U^{2} 2^{T^{1+\\beta / \\alpha}}\n$$\n\nwhich means, for any $x, x^{\\prime}, y, y^{\\prime}>0$ with $x, x^{\\prime} \\in\\left[\\exp \\left(-\\chi_{T}\\right), 1\\right]$ and $0 \\leq y \\leq x, 0 \\leq y^{\\prime} \\leq x^{\\prime}$, it holds\n\n$$\n\\left|\\frac{y}{x}-\\frac{y^{\\prime}}{x^{\\prime}}\\right| \\lesssim U^{2} 2^{T^{1+\\beta / \\alpha}}\\left(\\left|x-x^{\\prime}\\right|+\\left|y-y^{\\prime}\\right|\\right)\n$$\n\nwhich means\n\n$$\n\\begin{aligned}\n\\left|\\phi_{*}\\left(x^{\\prime}, y^{\\prime}\\right)-\\frac{y}{x}\\right| & \\leq\\left|\\phi_{1}\\left(x^{\\prime}, y^{\\prime}\\right)-\\frac{y^{\\prime}}{x^{\\prime}}\\right|+\\left|\\frac{y^{\\prime}}{x^{\\prime}}-\\frac{y}{x}\\right| \\\\\n& \\lesssim 2^{-T}+U^{2} 2^{T^{1+\\beta / \\alpha}}\\left(\\left|x-x^{\\prime}\\right|+\\left|y-y^{\\prime}\\right|\\right)\n\\end{aligned}\n$$\n\nNext, Lemma C. 4 implies that there exists a neural networks $\\phi_{n}^{\\prime} \\in \\Psi^{\\prime}(1, B)$ and $\\phi_{n} \\in$ $\\Psi(L, W, S, B)(n=1, \\ldots, N)$ with\n\n$$\n\\begin{aligned}\nN & \\lesssim T^{1+\\beta / \\alpha} \\log T \\log V \\\\\nL & \\lesssim T^{2(1+\\beta / \\alpha)} \\log T \\log ^{2} V, \\quad W \\lesssim 1 \\\\\nS & \\lesssim T^{2(1+\\beta / \\alpha)} \\log T \\log ^{2} V, \\quad \\log B \\lesssim 1\n\\end{aligned}\n$$\n\nsuch that, for any $t, x, \\widehat{x} \\in[0,1]$, it holds\n\n$$\n\\left|\\sum_{n=0}^{N} \\phi_{n}^{\\prime}(t) \\phi_{n}(\\widehat{x})-\\exp \\left(-\\frac{V^{2}\\left(\\frac{1}{\\alpha} \\log T+2 T^{1+\\beta / \\alpha}+2 \\log V\\right) \\cdot \\sin ^{2}\\left(\\frac{\\pi}{2}(t-x)\\right)}{2}\\right)\\right|\n$$\n\nSince\n\n$$\n\\begin{aligned}\n& \\exp \\left(-\\frac{V^{2}\\left(\\frac{1}{\\alpha} \\log T+2 T^{1+\\beta / \\alpha}+2 \\log V\\right) \\sin ^{2}\\left(\\frac{\\pi}{2}(t-x)\\right)}{2}\\right) \\\\\n& \\begin{cases}\\leq T^{-1 / \\alpha} 2^{-2 T^{1+\\beta / \\alpha}} / V^{2} & (|t-x| \\geq 1 / V) \\\\\n=1 & (t=x)\\end{cases}\n\\end{aligned}\n$$\n\nwe have\n$\\left|\\exp \\left(-\\frac{V^{2}\\left(\\frac{1}{\\alpha} \\log T+2 T^{1+\\beta / \\alpha}+2 \\log V\\right) \\sin ^{2}\\left(\\frac{\\pi}{2}(t-x)\\right)}{2}\\right)-\\mathbb{I}_{\\{x\\}}(t)\\right| \\lesssim 2 T^{-1 / \\alpha} 2^{-2 T^{1+\\beta / \\alpha}} / V^{2}$. Therefore, we have\n\n$$\n\\left|\\sum_{n=1}^{N} \\phi_{n}^{\\prime}(t) \\phi_{n}(\\widehat{x})-\\mathbb{I}_{\\{x\\}}(t)\\right| \\lesssim T^{-1 / \\alpha} 2^{-(1+\\beta / \\alpha) T} / V^{2}+T^{1+\\beta / \\alpha} V^{3}|x-\\hat{x}|\n$$\n\nSumming up over $x=j_{1} / U, \\ldots, j_{m-1} / U$, we have\n\n$$\n\\begin{aligned}\n\\mid \\sum_{m^{\\prime}=1}^{m-1} & \\sum_{i=1}^{I} \\phi_{0}^{\\left(j_{m^{\\prime}}, i\\right)}(t) \\phi_{1}^{\\left(j_{m^{\\prime}}, i\\right)}\\left(\\hat{j}_{m^{\\prime}} / U\\right)-\\mathbb{I}_{S}(t) \\mid \\\\\n& \\lesssim r_{\\max }\\left(T^{-1 / \\alpha} 2^{-2 T^{1+\\beta / \\alpha}} / V^{2}+T^{1+\\beta / \\alpha} V^{3}\\left|\\hat{j}_{m^{\\prime}} / U-j_{m^{\\prime}} / U\\right|\\right) \\\\\n& \\lesssim 2^{-2 T^{1+\\beta / \\alpha}} / V^{2}\n\\end{aligned}\n$$\n\nCombining the results above, we have\n\n$$\n\\begin{aligned}\n& \\left\\lvert\\, \\frac{1}{V} \\sum_{j=0}^{V} \\phi_{\\times}\\left(\\phi_{\\exp }\\left(X_{i, j}, \\chi_{T} \\widehat{\\cdot} \\mu_{t-j}\\right)\\right) \\cdot\\left(1-\\sum_{m^{\\prime}=1}^{m-1} \\sum_{i=1}^{I} \\phi_{0}^{\\left(j_{m^{\\prime}}, i\\right)}(t) \\phi_{1}^{\\left(j_{m^{\\prime}}, i\\right)}\\left(\\hat{j}_{m^{\\prime}} / U\\right)\\right)\\right. \\\\\n& -\\frac{1}{V} \\sum_{j=0}^{V} X_{i, j} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}_{t-j}\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right) \\\\\n& \\lesssim \\frac{1}{V} \\sum_{j=0}^{V}\\left(\\left|\\phi_{\\times}\\left(\\phi_{\\exp }\\left(X_{i, j}, \\chi_{T} \\cdot \\widehat{\\mu}_{t-j}\\right)\\right) \\cdot\\left(\\sum_{m^{\\prime}=1}^{m-1} \\sum_{i=1}^{I} \\phi_{0}^{\\left(j_{m^{\\prime}}, i\\right)}(t) \\phi_{1}^{\\left(j_{m^{\\prime}}, i\\right)}\\left(\\hat{j}_{m^{\\prime}} / U\\right)-\\mathbb{I}_{S}(j)\\right)\\right|\\right. \\\\\n& \\left.+\\left|\\left(\\phi_{\\times}\\left(\\phi_{\\exp }\\left(X_{i, j}, \\chi_{T} \\widehat{\\cdot} \\mu_{t-j}\\right)\\right)-X_{i, j} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}_{t-j}\\right)\\right) \\mathbb{I}_{S}(j)\\right|\\right) \\\\\n& \\lesssim 2^{-2 T^{1+\\beta / \\alpha}} / V^{2}\n\\end{aligned}\n$$\n\nSimilarly, we have\n\n$$\n\\begin{aligned}\n& \\left\\lvert\\, \\frac{1}{V} \\sum_{j=0}^{V} \\phi_{\\exp }\\left(\\chi_{T} \\widehat{\\cdot} \\mu_{t-j}\\right) \\cdot\\left(1-\\sum_{m^{\\prime}=1}^{m-1} \\sum_{i=1}^{I} \\phi_{0}^{\\left(j_{m^{\\prime}}, i\\right)}(t) \\phi_{1}^{\\left(j_{m^{\\prime}}, i\\right)}\\left(\\hat{j}_{m^{\\prime}} / U\\right)\\right)\\right. \\\\\n& \\left.-\\frac{1}{V} \\sum_{j=0}^{V} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}_{t-j}\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right) \\right\\rvert\\, \\\\\n& \\lesssim 2^{-2 T^{1+\\beta / \\alpha}} / V^{2}\n\\end{aligned}\n$$\n\nUsing the facts that\n\n$$\n\\begin{gathered}\n\\exp \\left(-\\chi_{T}\\right) \\leq \\frac{1}{V} \\sum_{t=0}^{V} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}[t]\\right) \\leq 1 \\\\\n\\frac{1}{V} \\sum_{t=0}^{V} u[t] \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}[t]\\right) \\leq \\frac{1}{V} \\sum_{t=0}^{V} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}[t]\\right)\n\\end{gathered}\n$$\n\nwe have\n\n$$\n\\begin{aligned}\n& \\left\\lvert\\, \\phi_{1}\\left(\\frac{1}{V} \\sum_{j=0}^{V} \\phi_{\\times}\\left(X_{i, j}, \\phi_{\\exp }\\left(\\widehat{\\chi}_{T} \\cdot \\mu_{t-j}\\right)\\right) \\cdot\\left(1-\\sum_{m^{\\prime}=1}^{m-1} \\sum_{i=1}^{I} \\phi_{0}^{\\left(j_{m^{\\prime}}, i\\right)}(t) \\phi_{1}^{\\left(j_{m^{\\prime}}, i\\right)}\\left(\\hat{j}_{m^{\\prime}} / U\\right)\\right),\\right.\\right. \\\\\n& \\left.\\frac{1}{V} \\sum_{j=0}^{V} \\phi_{\\exp }\\left(\\widehat{\\chi}_{T} \\cdot \\mu_{t-j}\\right) \\cdot\\left(1-\\sum_{m^{\\prime}=1}^{m-1} \\sum_{i=1}^{I} \\phi_{0}^{\\left(j_{m^{\\prime}}, i\\right)}(t) \\phi_{1}^{\\left(j_{m^{\\prime}}, i\\right)}\\left(\\hat{j}_{m^{\\prime}} / U\\right)\\right)\\right) \\\\\n& \\left.-\\frac{\\sum_{t=0}^{U} u[t] \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}[t]\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right)}{\\sum_{t=0}^{U} \\exp \\left(\\chi_{T} \\cdot \\widehat{\\mu}[t]\\right) \\cdot\\left(1-\\mathbb{I}_{S}(j)\\right)} \\right\\rvert\\, \\\\\n& \\lesssim 2^{-T}+V^{2} 2^{T^{1+\\beta / \\alpha}} \\cdot 2^{-2 T^{1+\\beta / \\alpha}} / V^{2} \\lesssim 2^{-T} . \\end{aligned}\n$$\n\nOverall, we can see that, there exist neural networks $\\phi_{O} \\in \\Psi^{\\prime}(L, W, S, B)$ and $\\phi_{A}, \\phi_{B}, \\phi_{C} \\in$ $\\Psi(L, W, S, B)$ with\n\n$$\n\\begin{aligned}\n& L \\lesssim T^{3+1 / \\alpha+3 \\beta / \\alpha} \\log T \\log ^{3} V, \\quad W \\lesssim T^{2+1 / \\alpha+2 \\beta / \\alpha} \\log T \\log ^{2} V \\\\\n& S \\lesssim T^{3+1 / \\alpha+3 \\beta / \\alpha} \\log T \\log ^{3} V, \\quad \\log B \\lesssim T^{3+1 / \\alpha+3 \\beta / \\alpha} \\log T \\log ^{3} V\n\\end{aligned}\n$$\n\nsuch that\n\n$$\n\\max _{i \\in\\left\\{i_{1}^{(m)}, \\ldots, i_{\\left|I_{m}\\right|}^{(m)}\\right\\}}|\\underbrace{\\phi_{C}\\left(\\sum_{j=0}^{V} \\phi_{O}(j / V) \\phi_{A}\\left(Z_{m-1}\\right) \\phi_{B}\\left(Z_{m-1}[-j]\\right)\\right)}_{=: \\widehat{X}_{i, j_{m}}}-X_{j_{m}, i}| \\lesssim 2^{-T}\n$$\n\nRecording which token was picked up Similar discussion as above shows that there exist neural networks $\\phi_{O}^{\\prime} \\in \\Psi^{\\prime}(L, W, S, B)$ and $\\phi_{A}^{\\prime}, \\phi_{B}^{\\prime}, \\phi_{C}^{\\prime} \\in \\Psi(L, W, S, B)$ with\n\n$$\n\\begin{array}{ll}\nL \\lesssim T^{3+1 / \\alpha+3 \\beta / \\alpha} \\log T \\log ^{3} V, & W \\lesssim T^{2+1 / \\alpha+2 \\beta / \\alpha} \\log T \\log ^{2} V \\\\\nS \\lesssim T^{3+1 / \\alpha+3 \\beta / \\alpha} \\log T \\log ^{3} V, & \\log B \\lesssim T^{3+1 / \\alpha+3 \\beta / \\alpha} \\log T \\log ^{3} V\n\\end{array}\n$$\n\nsuch that\n\n$$\n\\left|\\phi_{C}^{\\prime}\\left(\\sum_{j=0}^{V} \\phi_{O}^{\\prime}(j / V) \\phi_{A}^{\\prime}\\left(Z_{m-1}\\right) \\phi_{B}^{\\prime}\\left(Z_{m-1}[-j]\\right)\\right)-\\sin \\left(\\frac{\\pi}{4} \\frac{j_{m}}{V}\\right)\\right| \\lesssim 2^{-T}\n$$\n\nLemma C. 2 shows that there exists a neural network $\\phi_{\\arcsin } \\in \\Psi(L, W, S, B)$ with\n\n$$\nL \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} V, \\quad W \\lesssim 1, \\quad S \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} V, \\quad \\log B \\lesssim T^{1+\\beta / \\alpha} \\log V\n$$\n\nfor any $x \\in[0, \\pi / 4]$, it holds\n\n$$\n\\left|\\phi_{\\arcsin }(x)-\\arcsin (x)\\right| \\lesssim 2^{-3 T^{1+\\beta / \\alpha}} / V^{5}\n$$\n\nUsing this network, we can obtain $\\widehat{j}_{m} / V$ such that $\\left|\\widehat{j}_{m} / V-j_{m} / V\\right| \\lesssim 2^{-3 T^{1+\\beta / \\alpha}} / V^{5}$. Finishing the proof We can easily see that, constructing the weight matrix in the gated convolution layers appropriately, we can obtain $Z_{m}$ from $Z_{m-1}$ using the neural networks constructed above.",
    "dynsmoothssms-32": "This completes the proof. ## E. 2 Proof for the case of (ii) importance functions with similarity\n\nIn this subsection, we consider the case of similarity-based importance function. First, we show the approximation error when the importance is given by the distance. Thanks to the separated condition of the importance function, we can see that, for any $j \\neq j^{\\prime}$, it holds\n\n$$\n\\left\\|v_{0}-v_{j}\\right\\|^{2}-\\left\\|v_{0}-v_{j^{\\prime}}\\right\\|^{2}=\\left(\\left\\|v_{0}-v_{j}\\right\\|-\\left\\|v_{0}-v_{j^{\\prime}}\\right\\|\\right)\\left(\\left\\|v_{0}+v_{j}\\right\\|+\\left\\|v_{0}+v_{j^{\\prime}}\\right\\|\\right) \\gtrsim T^{-2 \\beta / \\alpha}\n$$\n\nNow, since it is hold that\n\n$$\nu^{2}-\\frac{u^{4}}{3} \\leq \\sin ^{2}(u)=\\frac{1-\\cos 2 u}{2} \\leq u^{2}\n$$\n\nfor $u \\in[0, \\pi / 2]$, for $A>0$, it holds\n\n$$\n\\begin{aligned}\n&\\left|\\left(A \\sin \\left(\\frac{\\pi}{2 A}\\left(v_{0 i}-v_{j i}\\right)\\right)\\right)^{2}-\\left(\\frac{\\pi}{2}\\left(v_{0 i}-v_{j i}\\right)\\right)^{2}\\right| \\\\\n&=\\left|A^{2} \\sin ^{2}\\left(\\frac{\\pi}{2 A}\\left(v_{0 i}-v_{j i}\\right)\\right)-A^{2}\\left(\\frac{\\pi}{2 A}\\left(v_{0 i}-v_{j i}\\right)\\right)^{2}\\right| \\\\\n& \\lesssim \\frac{1}{A^{2}}\n\\end{aligned}\n$$\n\nTherefore, if we set $A \\sim \\sqrt{d^{\\prime}} T^{\\beta / \\alpha}$, it holds\n\n$$\n\\sum_{i=1}^{d^{\\prime}}\\left(A \\sin \\left(\\frac{\\pi}{2 A}\\left(v_{0 i}-v_{j i}\\right)\\right)\\right)^{2}-\\sum_{i=1}^{d^{\\prime}}\\left(A \\sin \\left(\\frac{\\pi}{2 A}\\left(v_{0 i}-v_{j i^{\\prime}}\\right)\\right)^{2} \\gtrsim T^{-2 \\beta / \\alpha}\\right. $$\n\nThen, let us set $\\kappa \\sim \\frac{T+\\log V}{T^{-2 \\beta / \\alpha}}$. Therefore, if we can approximate\n\n$$\n\\exp \\left(-\\kappa \\sum_{i=1}^{d^{\\prime}}\\left(A \\sin \\left(\\frac{\\pi}{2 A}\\left(v_{0 i}-v_{j i}\\right)\\right)\\right)^{2}\\right)=\\prod_{i=1}^{d^{\\prime}} \\exp \\left(-\\kappa A^{2} \\sin ^{2}\\left(\\frac{\\pi}{2}\\left(v_{0 i}-v_{j i}\\right)\\right)\\right)\n$$\n\nwith the error less then $2^{-2 T^{1+\\beta / \\alpha}} / V^{3}$ efficiently, then the same discussion as the case of (i) gives the desired result, due to Lemma C. 1\n\nUsing Lemma C.4. we can see that there exists sneural network $\\phi_{n}^{(i)}, \\psi_{n}^{(i)} \\in \\Psi(L, W, S, B)(n=$ $1, \\ldots, N)$ with\n\n$$\n\\begin{array}{ll}\nN \\lesssim T^{1+\\beta / \\alpha} \\log V & \\\\\nL \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} T \\log ^{3} V, & W \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} V \\\\\nS \\lesssim T^{4(1+\\beta / \\alpha)} \\log ^{2} T \\log ^{5} V, & \\log B \\lesssim T^{1+\\beta / \\alpha} \\log T \\log ^{2} V\n\\end{array}\n$$\n\nsuch that, for any $x, y \\in[0,1]$, it holds\n\n$$\n\\left|\\sum_{n=1}^{N} \\phi_{n}^{(i)}(x) \\psi_{n}^{(i)}(y)-\\exp \\left(-\\kappa \\sin ^{2}\\left(\\frac{\\pi}{2}(x-y)\\right)\\right)\\right| \\lesssim \\frac{2^{-2 T^{1+\\beta / \\alpha}}}{d^{\\prime 2} V^{3}}\n$$\n\nSince we can see that $\\exp \\left(-\\kappa \\sin ^{2}\\left(\\pi / 2\\left(v_{0 i}-v j i\\right)\\right)\\right) \\in(0,1]$, it holds\n\n$$\n\\left|\\prod_{i=1}^{d^{\\prime}}\\left(\\sum_{n=1}^{N} \\phi_{n}^{(i)}\\left(v_{0 i}\\right) \\psi_{n}^{(i)}\\left(v_{i j}\\right)\\right)-\\prod_{i=1}^{d^{\\prime}} \\exp \\left(-\\kappa A^{2} \\sin ^{2}\\left(\\frac{\\pi}{2}\\left(v_{0 i}-v_{j i}\\right)\\right)\\right)\\right| \\lesssim \\frac{2^{-2 T^{1+\\beta / \\alpha}}}{d^{\\prime} V^{3}}\n$$\n\nFinally, Lemma C. 2 shows that there exists a neural network $\\phi_{\\times}$with\n\n$$\nL \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} U, \\quad W \\lesssim 1, \\quad S \\lesssim T^{2(1+\\beta / \\alpha)} \\log ^{2} U, \\quad \\log B \\lesssim T^{1+\\beta / \\alpha} \\log U\n$$\n\nsuch that\n\n$$\n\\left|\\phi_{\\times}\\left(\\left[\\sum_{n=1}^{N} \\phi_{n}^{(i)}\\left(v_{0, i}\\right) \\psi_{n}^{(i)}\\left(v_{j, i}\\right)\\right]_{i=1}^{d^{\\prime}}\\right)-\\prod_{i=1}^{d^{\\prime}}\\left(\\sum_{n=1}^{N} \\phi_{n}^{(i)}\\left(v_{0, i}\\right) \\psi_{n}^{(i)}\\left(v_{j, i}\\right)\\right)\\right| \\lesssim 2^{-2 T^{1+\\beta / \\alpha}} / V^{3}\n$$\n\nwhich completes the proof. As for the setting of inner product, we have\n\n$$\n\\exp \\left(v_{0}^{\\top} v_{j}\\right)=\\exp \\left(\\frac{1}{2}\\left\\|v_{0}\\right\\|^{2}\\right) \\exp \\left(\\frac{1}{2}\\left\\|v_{j}\\right\\|^{2}\\right) \\exp \\left(-\\frac{1}{2}\\left\\|v_{0}-v_{j}\\right\\|^{2}\\right)\n$$\n\nSince $\\exp \\left(\\frac{1}{2}\\left\\|v_{0}\\right\\|^{2}\\right)$ and $\\exp \\left(\\frac{1}{2}\\left\\|v_{j}\\right\\|^{2}\\right)$ can be approximated by neural networks in each token, we immediately obtain the desired result. ## F Proof of Theorem 5.1\n\nTo prove the theorem, we use the following proposition. Proposition F. 1 (Theorem 5.2 in Takakura and Suzuki (2023)). For a given class $\\mathcal{F}$ of functions from $[0,1]^{d \\times \\infty}$ to $\\mathbb{R}^{\\infty}$, let $\\hat{F} \\in \\mathcal{F}$ be an ERM estimator which minimizes the empirical cost. Suppose that there exists a constant $R>0$ such that $\\left\\|F^{\\circ}\\right\\|_{\\infty} \\leq R,\\|F\\|_{\\infty} \\leq R$ for any $F \\in \\mathcal{F}$, and $\\mathcal{N}\\left(\\mathcal{F}, \\delta,\\|\\cdot\\|_{\\infty}\\right) \\geq 3$. Then, for any $0<\\delta<1$, it holds that\n\n$$\nR_{l, r}\\left(\\hat{F}, F^{\\circ}\\right) \\lesssim \\inf _{F \\in \\mathcal{F}} \\frac{1}{r-l+1} \\sum_{i=l}^{r}\\left\\|F_{i}-F_{i}^{\\circ}\\right\\|_{2, P_{X}}^{2}+\\left(R^{2}+\\sigma^{2}\\right) \\frac{\\log \\mathcal{N}\\left(\\mathcal{F}, \\delta,\\|\\cdot\\|_{\\infty}\\right)}{n}+(R+\\sigma) \\delta\n$$\n\nwhere $\\mathcal{N}(\\mathcal{F}, \\delta,\\|\\cdot\\|)$ is the $\\delta$-covering number of the space $\\mathcal{F}$ associated with the norm $\\|\\cdot\\|$, defined by\n\n$$\n\\mathcal{N}(\\mathcal{F}, \\delta,\\|\\cdot\\|):=\\inf \\left\\{m \\in \\mathbb{N} \\mid \\exists F_{1}, \\ldots, F_{m} \\in \\mathcal{F}, \\forall F \\in \\mathcal{F}, \\exists i \\in[m] \\text { s.t. }\\left\\|F-F_{i}\\right\\| \\leq \\delta\\right\\}\n$$\n\nThanks to this proposition, the problem to obtain the upper bound of the excess risk of the estimator $\\hat{F}$ is reduced to the problem to evaluate the covering number of the function class $\\mathcal{S}$.",
    "dynsmoothssms-33": "The covering number of the function class $\\mathcal{S}$ can be evaluated as follows. Theorem F. 2 (Covering number of SSMs with gated convolution). The covering number of the function class $\\mathcal{S}(M, U, D, L, W, S, B)$ can be bounded as\n\n$$\n\\log \\mathcal{N}\\left(\\mathcal{S}(M, U, D, L, W, S, B), \\delta,\\|\\cdot\\|_{\\infty}\\right) \\lesssim M^{2} L\\left(S+D^{2}\\right) \\log \\left(\\frac{D U L W B}{\\delta}\\right)\n$$\n\nThis theorem implies that the upper bound of the covering number of the function class $\\mathcal{S}$ polynomially increases with respect to the embedding dimensions $D$, the number of layers $M, L$ and the sparsity $S$ of the parameters. This result is similar to the result by Takakura and Suzuki (2023) on the covering number of Transformers. A large difference of the covering number between the SSMs and Transformers is the dependence on the window size $U$; the covering number of the SSMs depends on $U$ logarithmically, while that of the Transformers does not depend on $U$. This is because SSMs sum up the tokens in the convolution without normalization. Whereas it is prefered that the covering number does not depend on $U$, the logarithmic dependence on $U$ is not a serious problem for the estimation ability, as we will see later. In the following, we prove Theorem F. 2 . First of all, we introduce the lemma below, which is useful to evaluate the covering number. Lemma F.3. Let $\\left\\{f_{\\theta}\\right\\}_{\\theta \\in \\Theta}$ be a parametrized function class from $[0,1]^{d \\times \\infty}$ to $\\mathbb{R}^{\\infty}$. Suppose that the parameter space $\\Theta$ satisfies $\\Theta \\subseteq[-B, B]^{D}$ for some $B>0, D>0$. Additionally, suppose that\n\n$$\n|\\{\\theta \\mid \\theta \\neq 0, \\theta \\in \\Theta\\}| \\leq S\n$$\n\nMoreover, assume that there exists a constant $r>0$ such that\n\n$$\n\\left\\|f_{\\theta}-f_{\\tilde{\\theta}}\\right\\|_{\\infty} \\leq r\\|\\theta-\\tilde{\\theta}\\|_{\\infty} \\quad \\text { for any } \\theta, \\tilde{\\theta} \\in \\Theta\n$$\n\nThen, it holds\n\n$$\n\\log \\mathcal{N}\\left(\\mathcal{F}, \\delta,\\|\\cdot\\|_{\\infty}\\right) \\leq S \\log \\left(\\frac{r B D}{\\delta}\\right)\n$$\n\nThe following lemma is drawn from Takakura and Suzuki (2023), which evaluates the norm of the output of FNN, the lipschitz constant with respect to the input, and the lipschitz constant with respect to the parameters.",
    "dynsmoothssms-34": "Lemma F.",
    "dynsmoothssms-35": "4 (Lemma E. 3 in Suzuki (2018)). Suppose that two FNNs $f, \\widetilde{f}$ with L layers and $W$ hidden units is given by\n\n$$\n\\begin{aligned}\n& f(x):=\\left(A_{L} \\sigma(\\cdot)+b_{L}\\right) \\circ \\cdots \\circ\\left(A_{1} \\sigma(x)+b_{1}\\right) \\\\\n& \\widetilde{f}(x):=\\left(\\widetilde{A}_{L} \\sigma(\\cdot)+\\widetilde{b}_{L}\\right) \\circ \\cdots \\circ\\left(\\widetilde{A}_{1} \\sigma(x)+\\widetilde{b}_{1}\\right)\n\\end{aligned}\n$$\n\nwhere $\\sigma$ is the ReLU activation function. Assume that for any $l=1, \\ldots, L$, it holds\n\n$$\n\\left\\|A_{l}\\right\\|_{\\infty} \\leq B, \\quad\\left\\|\\widetilde{A}_{l}\\right\\|_{\\infty} \\leq B, \\quad\\left\\|b_{l}\\right\\|_{\\infty} \\leq B, \\quad\\left\\|\\widetilde{b}_{l}\\right\\|_{\\infty} \\leq B\n$$\n\nAdditionally, let $r \\geq 1$ be a constant. 1. For any $x \\in \\mathbb{R}^{D \\times \\infty}$ with $\\|x\\|_{\\infty} \\leq r$, it holds\n\n$$\n\\|f(x)\\|_{\\infty} \\leq(2 B W)^{L} r\n$$\n\n2. For any $X, X^{\\prime} \\in \\mathbb{R}^{D \\times \\infty}$, it holds\n\n$$\n\\left\\|f(x)-f\\left(x^{\\prime}\\right)\\right\\|_{\\infty} \\leq(B W)^{L}\\left\\|X-X^{\\prime}\\right\\|_{\\infty}\n$$\n\n3. Assume that, for any $l=1, \\ldots, L$, it holds\n\n$$\n\\left\\|A_{l}-\\widetilde{A}_{l}\\right\\|_{\\infty} \\leq \\delta, \\quad\\left\\|b_{l}-\\widetilde{b}_{l}\\right\\|_{\\infty} \\leq \\delta\n$$\n\nThen, for any $x \\in \\mathbb{R}^{D}$ with $\\|x\\|_{\\infty} \\leq r$, it holds\n\n$$\n\\|f(x)-\\widetilde{f}(x)\\|_{\\infty} \\leq 2(2 B W)^{L} r \\cdot \\delta\n$$\n\nWe also evaluate them for the gated convolution layers. Lemma F.5. Suppose that two gated convolution layers $g, \\tilde{g}$ with window size $U$ and embedding dimention $D$ is given by\n\n$$\n\\begin{aligned}\n& g(X):=\\left(W_{Q} X\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right) \\\\\n& \\widetilde{g}(X):=\\left(\\widetilde{W}_{Q} X\\right) \\odot\\left(\\widetilde{\\beta}(X) *\\left(\\widetilde{W}_{V} X\\right)\\right)\n\\end{aligned}\n$$\n\nLet $r \\geq 1$ be a constant. Assume that it holds\n\n$$\n\\left\\|W_{Q}\\right\\|_{\\infty} \\leq B, \\quad\\left\\|\\widetilde{W}_{Q}\\right\\|_{\\infty} \\leq B, \\quad\\left\\|W_{V}\\right\\|_{\\infty} \\leq B, \\quad\\left\\|\\widetilde{W}_{V}\\right\\|_{\\infty} \\leq B\n$$\n\nand, for any $h=0, \\ldots, H$ and $X \\in \\mathbb{R}^{d \\times \\infty}$ with $\\|X\\|_{\\infty} \\leq r$, it holds\n\n$$\n\\|\\beta(X)\\|_{1} \\leq c, \\quad\\|\\widetilde{\\beta}(X)\\|_{1} \\leq c\n$$\n\nfor some $B \\geq 1, c \\geq 1$. Then, the following statements hold. 1. For any $X \\in \\mathbb{R}^{D \\times \\infty}$ with $\\|X\\|_{\\infty} \\leq r$, it holds\n\n$$\n\\|g(X)\\|_{\\infty} \\leq(B D r c)^{2}\n$$\n\n2. Suppose that $X, X^{\\prime} \\in \\mathbb{R}^{D \\times \\infty}$ satisfies $\\|X\\|_{\\infty} \\leq r,\\left\\|X^{\\prime}\\right\\|_{\\infty} \\leq r$ and\n\n$$\n\\left\\|\\beta(X)-\\beta\\left(X^{\\prime}\\right)\\right\\|_{1} \\leq \\kappa\\left\\|X-X^{\\prime}\\right\\|_{\\infty}\n$$\n\nfor some $\\kappa \\geq 2^{3}$ Then, it holds\n\n$$\n\\left\\|g(X)-g\\left(X^{\\prime}\\right)\\right\\|_{\\infty} \\leq\\left(2 B^{2} r c+B r \\cdot \\kappa\\right)\\left\\|X-X^{\\prime}\\right\\|_{\\infty}\n$$\n\n3. Assume that, for any $h=0, \\ldots, H$, it holds\n\n$$\n\\left\\|W_{Q}-\\widetilde{W}_{Q}\\right\\|_{\\infty} \\leq \\delta, \\quad\\left\\|W_{V}-\\widetilde{W}_{V}\\right\\|_{\\infty} \\leq \\delta, \\quad\\|\\beta(X)-\\widetilde{\\beta}(X)\\|_{1} \\leq \\iota \\delta\n$$\n\nfor $\\iota>0$. Then, it holds\n\n$$\n\\|g(X)-\\widetilde{g}(X)\\|_{\\infty} \\leq\\left(2 B r^{2} c+(B r)^{2} \\cdot \\iota\\right) \\cdot \\delta\n$$\n\nProof. We use frequently the following three inequalities:\n\n$$\n\\begin{aligned}\n\\|W X\\|_{\\infty} & \\leq\\|W\\|_{1}\\|X\\|_{\\infty} \\leq D \\cdot\\|W\\|_{\\infty}\\|X\\|_{\\infty} \\\\\n\\|X \\odot Y\\|_{\\infty} & \\leq\\|X\\|_{\\infty}\\|Y\\|_{\\infty} \\\\\n\\|\\beta * X\\|_{\\infty} & \\leq\\|\\beta\\|_{1}\\|X\\|_{\\infty}\n\\end{aligned}\n$$\n\nwhere $W \\in \\mathbb{R}^{D \\times D}, X \\in \\mathbb{R}^{D \\times \\infty}, Y \\in \\mathbb{R}^{D \\times \\infty}, \\beta \\in \\mathbb{R}^{D \\times U}$. Proof of 1 We have\n\n$$\n\\begin{aligned}\n\\|g(X)\\|_{\\infty} & =\\left\\|\\left(W_{Q} X\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n& \\leq\\left\\|W_{Q} X\\right\\|_{\\infty} \\cdot\\left\\|\\beta(X) *\\left(W_{V} X\\right)\\right\\|_{\\infty} \\\\\n& \\leq\\left\\|W_{Q} X\\right\\|_{\\infty} \\cdot\\left\\|W_{V} X\\right\\|_{\\infty} \\cdot\\|\\beta(X)\\|_{1} \\\\\n& \\leq(B D r)^{2} \\cdot c \\leq(B D r c)^{2}\n\\end{aligned}\n$$\n\n[^1]Proof of 2 We have\n\n$$\n\\begin{aligned}\n\\left\\|g(X)-g\\left(X^{\\prime}\\right)\\right\\|_{\\infty}= & \\left\\|\\left(W_{Q} X\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)-\\left(W_{Q} X^{\\prime}\\right) \\odot\\left(\\beta\\left(X^{\\prime}\\right) *\\left(W_{V} X^{\\prime}\\right)\\right)\\right\\|_{\\infty} \\\\\n\\leq & \\left\\|\\left(W_{Q} X\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)-\\left(W_{Q} X^{\\prime}\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n& +\\left\\|\\left(W_{Q} X^{\\prime}\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)-\\left(W_{Q} X^{\\prime}\\right) \\odot\\left(\\beta\\left(X^{\\prime}\\right) *\\left(W_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n& +\\left\\|\\left(W_{Q} X^{\\prime}\\right) \\odot\\left(\\beta\\left(X^{\\prime}\\right) *\\left(W_{V} X\\right)\\right)-\\left(W_{Q} X^{\\prime}\\right) \\odot\\left(\\beta\\left(X^{\\prime}\\right) *\\left(W_{V} X^{\\prime}\\right)\\right)\\right\\|_{\\infty} \\\\\n\\leq & \\left\\|\\left(W_{Q}\\left(X-X^{\\prime}\\right)\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n& +\\left\\|\\left(W_{Q} X^{\\prime}\\right) \\odot\\left(\\left(\\beta(X)-\\beta\\left(X^{\\prime}\\right)\\right) *\\left(W_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n& +\\left\\|\\left(W_{Q} X^{\\prime}\\right) \\odot\\left(\\beta\\left(X^{\\prime}\\right) *\\left(W_{V}\\left(X-X^{\\prime}\\right)\\right)\\right)\\right\\|_{\\infty} \\\\\n\\leq & \\left\\|W_{Q}\\left(X-X^{\\prime}\\right)\\right\\|_{\\infty} \\cdot\\|\\beta(X)\\|_{1} \\cdot\\left\\|W_{V} X\\right\\|_{\\infty} \\\\\n& +\\left\\|W_{Q} X^{\\prime}\\right\\|_{\\infty} \\cdot\\left\\|\\beta(X)-\\beta\\left(X^{\\prime}\\right)\\right\\|_{1} \\cdot\\left\\|W_{V} X\\right\\|_{\\infty} \\\\\n& +\\left\\|W_{Q} X^{\\prime}\\right\\|_{\\infty} \\cdot\\left\\|\\beta\\left(X^{\\prime}\\right)\\right\\|_{1} \\cdot\\left\\|W_{V}\\left(X-X^{\\prime}\\right)\\right\\|_{\\infty} \\\\\n\\leq & B\\left\\|X-X^{\\prime}\\right\\|_{\\infty} \\cdot c \\cdot B r+B r \\cdot \\kappa\\left\\|X-X^{\\prime}\\right\\|_{\\infty} \\cdot B r+B r \\cdot c \\cdot B\\left\\|X-X^{\\prime}\\right\\|_{\\infty} \\\\\n= & \\left(2 B^{2} r c+B r \\cdot \\kappa\\right)\\left\\|X-X^{\\prime}\\right\\|_{\\infty}\n\\end{aligned}\n$$\n\nProof of 3 We have\n\n$$\n\\begin{aligned}\n\\|g(X)-\\widetilde{g}(X)\\|_{\\infty}= & \\left\\|\\left(W_{Q} X\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)-\\left(\\widetilde{W}_{Q} X\\right) \\odot\\left(\\widetilde{\\beta}(X) *\\left(\\widetilde{W}_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n\\leq & \\left\\|\\left(W_{Q} X\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)-\\left(\\widetilde{W}_{Q} X\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n& +\\left\\|\\left(\\widetilde{W}_{Q} X\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)-\\left(\\widetilde{W}_{Q} X\\right) \\odot\\left(\\widetilde{\\beta}(X) *\\left(W_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n& +\\left\\|\\left(\\widetilde{W}_{Q} X\\right) \\odot\\left(\\widetilde{\\beta}(X) *\\left(W_{V} X\\right)\\right)-\\left(\\widetilde{W}_{Q} X\\right) \\odot\\left(\\widetilde{\\beta}(X) *\\left(\\widetilde{W}_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n\\leq & \\left\\|\\left(\\left(W_{Q}-\\widetilde{W}_{Q}\\right) X\\right) \\odot\\left(\\beta(X) *\\left(W_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n& +\\left\\|\\left(\\widetilde{W}_{Q} X\\right) \\odot\\left((\\beta(X)-\\widetilde{\\beta}(X)) *\\left(W_{V} X\\right)\\right)\\right\\|_{\\infty} \\\\\n& +\\left\\|\\left(\\widetilde{W}_{Q} X\\right) \\odot\\left(\\widetilde{\\beta}(X) *\\left(\\left(W_{V}-\\widetilde{W}_{V}\\right) X\\right)\\right)\\right\\|_{\\infty} \\\\\n\\leq & \\left\\|\\left(W_{Q}-\\widetilde{W}_{Q}\\right) X\\right\\|_{\\infty} \\cdot\\|\\beta(X)\\|_{1} \\cdot\\left\\|W_{V} X\\right\\|_{\\infty} \\\\\n& +\\left\\|\\widetilde{W}_{Q} X\\right\\|_{\\infty} \\cdot\\|\\beta(X)-\\widetilde{\\beta}(X)\\|_{1} \\cdot\\left\\|W_{V} X\\right\\|_{\\infty} \\\\\n& +\\left\\|\\widetilde{W}_{Q} X\\right\\|_{\\infty} \\cdot\\|\\widetilde{\\beta}(X)\\|_{1} \\cdot\\left\\|\\left(W_{V}-\\widetilde{W}_{V}\\right) X\\right\\|_{\\infty} \\\\\n\\leq & \\delta r \\cdot c \\cdot B r+B r \\cdot \\iota \\delta \\cdot B r+B r \\cdot c \\cdot \\delta r \\\\\n= & \\left(2 B r^{2} c+(B r)^{2} \\cdot \\iota\\right) \\delta\n\\end{aligned}\n$$\n\nSubsequently, we evaluate the lipschitz constant of the composition of the layers with respect to the input and the parameters. Lemma F.6. Let $\\left(f_{1}, \\widetilde{f}_{1}\\right), \\ldots,\\left(f_{M}, \\widetilde{f}_{M}\\right)$ be pairs of two FNNs which satisfy the same condition of the pair $(f, \\tilde{f})$ in Lemma F. 4 Additionally, let $\\left(g_{1}, \\widetilde{g}_{1}\\right), \\ldots,\\left(g_{M}, \\widetilde{g}_{M}\\right)$ be gated convolution layers which satisfy the same condition of the pair $(g, \\widetilde{g})$ in Lemma F.5. Suppose $R>0$ be a constant, and $F, \\widetilde{F}:[0,1]^{d \\times \\infty} \\rightarrow \\mathbb{R}^{\\infty}$ are two functions defined by\n\n$$\n\\begin{aligned}\n& F:=\\operatorname{clip}_{R} \\circ f_{M} \\circ g_{M} \\circ \\cdots \\circ \\operatorname{clip}_{R} \\circ f_{1} \\circ g_{1} \\\\\n& \\widetilde{F}:=\\operatorname{clip}_{R} \\circ \\widetilde{f}_{M} \\circ \\widetilde{g}_{M} \\circ \\cdots \\circ \\operatorname{clip}_{R} \\circ \\widetilde{f}_{1} \\circ \\widetilde{g}_{1}\n\\end{aligned}\n$$\n\nMoreover, assume that $B \\geq 1, c \\geq 1, r \\geq 1$. Then, it holds\n\n$$\n\\|F(X)-\\widetilde{F}(X)\\|_{\\infty} \\leq 2^{M+1}(2 B W)^{M L}(B D R c)^{2 M}(1+\\kappa)^{M}(1+\\iota) \\cdot \\delta\n$$\n\nProof. For $m=1, \\ldots, M$, we define\n$F_{m}:=\\operatorname{clip}_{R} \\circ f_{m} \\circ g_{m} \\circ \\cdots \\circ \\operatorname{clip}_{R} \\circ f_{1} \\circ g_{1}, \\quad \\widetilde{F}_{m}:=\\operatorname{clip}_{R} \\circ \\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ \\cdots \\circ \\operatorname{clip}_{R} \\circ \\widetilde{f}_{1} \\circ \\widetilde{g}_{1}$, and $F_{0}:=\\operatorname{id}, \\widetilde{F}_{0}:=$ id. Then, it holds\n\n$$\nF_{m}=\\operatorname{clip}_{R} \\circ f_{m} \\circ g_{m} \\circ F_{m-1}, \\quad \\widetilde{F}_{m}=\\operatorname{clip}_{R} \\circ \\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ \\widetilde{F}_{m-1}\n$$\n\nfor $m=1, \\ldots, M$. Note that $\\left\\|F_{m}\\right\\|_{\\infty} \\leq R$ and $\\left\\|\\widetilde{F}_{m}\\right\\|_{\\infty} \\leq R$ for any $m=1, \\ldots, M$ due to the clipping. For any $X \\in \\mathbb{R}^{d \\times \\infty}$ with $\\|X\\|_{\\infty} \\leq r$ and $m=1, \\ldots, M$, we have\n\n$$\n\\begin{aligned}\n&\\left\\|F_{m}(X)-\\widetilde{F}_{m}(X)\\right\\|_{\\infty}=\\left\\|\\operatorname{clip}_{R} \\circ f_{m} \\circ g_{m} \\circ F_{m-1}(X)-\\operatorname{clip}_{R} \\circ \\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ \\widetilde{F}_{m-1}(X)\\right\\|_{\\infty} \\\\\n&=\\left\\|f_{m} \\circ g_{m} \\circ F_{m-1}(X)-\\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ \\widetilde{F}_{m-1}(X)\\right\\|_{\\infty} \\\\\n& \\quad\\left(\\because \\operatorname{clip}_{R} \\text { is 1-lipschitz continuous. }\\right) \\\\\n& \\leq\\left\\|f_{m} \\circ g_{m} \\circ F_{m-1}(X)-\\widetilde{f}_{m} \\circ g_{m} \\circ F_{m-1}(X)\\right\\|_{\\infty} \\\\\n&+\\left\\|\\widetilde{f}_{m} \\circ g_{m} \\circ F_{m-1}(X)-\\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ F_{m-1}(X)\\right\\|_{\\infty} \\\\\n&+\\left\\|\\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ F_{m-1}(X)-\\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ \\widetilde{F}_{m-1}(X)\\right\\|_{\\infty}\n\\end{aligned}\n$$\n\nFor the first term, since $\\left\\|g_{m} \\circ F_{m-1}(X)\\right\\| \\leq(B D R c)^{2}$ due to the first argument of Lemma F. 5 . using the third argument of Lemma F.4 we have\n\n$$\n\\left\\|f_{m} \\circ g_{m} \\circ F_{m-1}(X)-\\widetilde{f}_{m} \\circ g_{m} \\circ F_{m-1}(X)\\right\\|_{\\infty} \\leq 2(2 B W)^{L}(B D R c)^{2} \\cdot \\delta\n$$\n\nFor the second term, the second argument of Lemma F.4 and the third argument of Lemma F.5 yield\n\n$$\n\\begin{aligned}\n\\left\\|\\widetilde{f}_{m} \\circ g_{m} \\circ F_{m-1}(X)-\\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ F_{m-1}(X)\\right\\|_{\\infty} & \\leq(B W)^{L}\\left\\|g_{m} \\circ F_{m-1}(X)-\\widetilde{g}_{m} \\circ F_{m-1}(X)\\right\\|_{\\infty} \\\\\n& \\leq(B W)^{L} \\cdot\\left(2 B R^{2} c+(B R)^{2} \\cdot \\iota\\right) \\cdot \\delta\n\\end{aligned}\n$$\n\nFor the thrid term, the third argument of Lemma F.4 and the third argument of Lemma F. 5 imply\n\n$$\n\\begin{aligned}\n\\| \\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ F_{m-1}(X) & -\\widetilde{f}_{m} \\circ \\widetilde{g}_{m} \\circ \\widetilde{F}_{m-1}(X) \\|_{\\infty} \\\\\n& \\leq(B W)^{L}\\left\\|\\widetilde{g}_{m} \\circ F_{m-1}(X)-\\widetilde{g}_{m} \\circ \\widetilde{F}_{m-1}(X)\\right\\|_{\\infty} \\\\\n& \\leq(B W)^{L} \\cdot\\left(2 B^{2} R c+B R \\cdot \\kappa\\right) \\cdot\\left\\|F_{m-1}(X)-\\widetilde{F}_{m-1}(X)\\right\\|_{\\infty}\n\\end{aligned}\n$$\n\nLet $\\lambda_{1}, \\lambda_{2}$ be the constants defined by\n\n$$\n\\begin{aligned}\n& \\lambda_{1}:=\\left(2(2 B W)^{L}(B D R c)^{2}+(B W)^{L} \\cdot\\left(2 B R^{2} c+(B R)^{2} \\cdot \\iota\\right)\\right) \\cdot \\delta \\\\\n& \\lambda_{2}:=(B W)^{L} \\cdot\\left(2 B^{2} R c+B R \\cdot \\kappa\\right)\n\\end{aligned}\n$$\n\nThen, we have\n\n$$\n\\left\\|F_{m}(X)-\\widetilde{F}_{m}(X)\\right\\|_{\\infty} \\leq \\lambda_{1}+\\lambda_{2} \\cdot\\left\\|F_{m-1}(X)-\\widetilde{F}_{m-1}(X)\\right\\|_{\\infty}\n$$\n\nThis implies\n\n$$\n\\left\\|F_{m}(X)-\\widetilde{F}_{m}(X)\\right\\|_{\\infty}+\\frac{\\lambda_{1}}{\\lambda_{2}-1} \\leq \\lambda_{2} \\cdot\\left(\\left\\|F_{m-1}(X)-\\widetilde{F}_{m-1}(X)\\right\\|_{\\infty}+\\frac{\\lambda_{1}}{\\lambda_{2}-1}\\right)\n$$\n\nThus, by induction, we have\n\n$$\n\\left\\|F_{m}(X)-\\widetilde{F}_{m}(X)\\right\\|_{\\infty}+\\frac{\\lambda_{1}}{\\lambda_{2}-1} \\leq \\lambda_{2}^{m} \\cdot\\left(\\left\\|F_{0}(X)-\\widetilde{F}_{0}(X)\\right\\|_{\\infty}+\\frac{\\lambda_{1}}{\\lambda_{2}-1}\\right)=\\frac{\\lambda_{2}^{m} \\cdot \\lambda_{1}}{\\lambda_{2}-1}\n$$\n\nSince $\\lambda_{2}>1$, it holds\n\n$$\n\\left\\|F_{m}(X)-\\widetilde{F}_{m}(X)\\right\\|_{\\infty} \\leq \\lambda_{1} \\cdot \\frac{\\lambda_{2}^{m}-1}{\\lambda_{2}-1}=\\lambda_{1} \\cdot\\left(1+\\lambda_{2}+\\cdot+\\lambda_{2}^{m-1}\\right) \\leq m \\lambda_{1} \\lambda_{2}^{m-1}\n$$\n\nNow, using\n\n$$\n\\lambda_{1} \\leq 3(2 B W)^{L}(B D R c)^{2}(1+\\iota) \\cdot \\delta, \\quad \\lambda_{2} \\leq 2(2 B W)^{L}(B D R c)^{2}(1+\\kappa)\n$$\n\nwe have\n\n$$\n\\|F(X)-\\widetilde{F}(X)\\|_{\\infty} \\leq M \\lambda_{1} \\lambda_{2}^{M-1} \\leq 2^{M+1}(2 B W)^{M L}(B D R c)^{2 M}(1+\\kappa)^{M}(1+\\iota) \\cdot \\delta\n$$\n\nwhich completes the proof.",
    "dynsmoothssms-36": "Finally, we prove Theorem F. 2\n\n## Proof of Theorem F. 2 . $$\n\\begin{aligned}\n\\kappa & =0 \\\\\n\\iota & \\leq 2 U \\cdot\\left(2 B W^{\\prime}\\right)^{L^{\\prime}} \\\\\nc & \\leq U\\left(2 B W^{\\prime}\\right)^{L^{\\prime}}\n\\end{aligned}\n$$\n\nTherefore, we have\n\n$$\n\\begin{aligned}\n\\|F(X)-\\widetilde{F}(X)\\|_{\\infty} & \\leq 2^{M+1}(2 B W)^{M L}\\left(B D R U\\left(2 B W^{\\prime}\\right)^{L^{\\prime}}\\right)^{2 M} \\cdot\\left(2 \\cdot 2 U\\left(2 B W^{\\prime}\\right)^{L^{\\prime}}\\right) \\cdot \\delta \\\\\n& =2^{M+3}(2 B W)^{M L}\\left(2 B W^{\\prime}\\right)^{(2 M+1) L^{\\prime}}(B D R U)^{2 M+1} \\cdot \\delta\n\\end{aligned}\n$$\n\nThe number of parameters in a FNN is $2 W^{2} L$. Additionally, the number of parameters in a gated convolution layer is $2 D^{2}$. Moreover, the number of nonzero parameters in whole network is bounded by $M\\left(S+2 D^{2}\\right)$. Therefore, the covering number can be evaluated as\n\n$$\n\\begin{aligned}\n& \\log \\mathcal{N}\\left(\\mathcal{S}(M, U, D, L, W, S, B), \\delta,\\|\\cdot\\|_{\\infty}\\right) \\\\\n& \\quad \\leq M\\left(S+2 D^{2}\\right) \\\\\n& \\quad+\\log \\left(\\frac{M\\left(2 W^{2} L+D^{2}\\right) \\cdot B \\cdot 2^{M+3}(2 B W)^{M L}\\left(2 B W^{\\prime}\\right)^{(2 M+1) L^{\\prime}}(B D R U)^{2 M+1}}{\\delta}\\right) \\\\\n& \\quad \\lesssim M^{2} L\\left(S+D^{2}\\right) \\log \\left(\\frac{D U L W B}{\\delta}\\right)\n\\end{aligned}\n$$\n\nwhich completes the proof. ## G Proof of Theorem 5.2\n\nProof of Theorem 5.2. Theorem 4.2 implies that, for any $T>0$, there exists an SSM $F \\in$ $\\mathcal{S}(M, U, D, L, W, S, B)$ with\n\n$$\n\\begin{aligned}\n& M=1, \\quad \\log U \\sim T, \\quad D \\sim T^{1 / \\alpha}, \\quad L \\sim T, \\quad W \\sim T^{1 / \\alpha} 2^{T / a^{\\dagger}} \\\\\n& S \\sim T^{2 / \\alpha} \\max \\left\\{T^{2 / \\alpha}, T^{2}\\right\\} 2^{T / a^{\\dagger}}, \\quad \\log B \\sim T^{1 / \\alpha}\n\\end{aligned}\n$$\n\nsuch that $\\left\\|F-F^{\\circ}\\right\\|_{2, P_{X}} \\lesssim 2^{-T}$. Therefore, it holds\n\n$$\n\\frac{1}{r-l+1} \\sum_{t=l}^{r}\\left\\|F_{i}-F_{i}^{\\circ}\\right\\|_{2, P_{X}}^{2} \\leq 2^{-2 T}\n$$\n\nNote that, thanks to the clipping, it holds $\\|F\\|_{\\infty} \\leq R$, and thus this inequality gives the upper bound for the first term of the right-hand side of Proposition F. 1 . Next, Theorem F. 2 shows that it holds\n\n$$\n\\log \\mathcal{N}\\left(\\mathcal{S}, \\delta,\\|\\cdot\\|_{\\infty}\\right) \\lesssim 2^{T / a^{\\dagger}} T^{1+2 / \\alpha} \\max \\left\\{T^{3 / \\alpha}, T^{4}\\right\\} \\log \\frac{T}{\\delta}\n$$\n\nCombining the above two inequalities and Proposition F.1, it holds\n\n$$\nR_{l, r}\\left(\\hat{F}, F^{\\circ}\\right) \\lesssim 2^{-2 T}+\\frac{2^{T / a^{\\dagger}} T^{1+2 / \\alpha} \\max \\left\\{T^{3 / \\alpha}, T^{4}\\right\\} \\log \\frac{T}{\\delta}}{n}+\\delta\n$$\n\nBy setting $T=\\frac{a^{\\dagger}}{2 a^{\\dagger}+1} \\log n$ and $\\delta=1 / n$, we have\n\n$$\nR_{l, r}\\left(\\hat{F}, F^{\\circ}\\right) \\lesssim n^{-\\frac{2 a^{\\dagger}}{2 a^{\\dagger}+1}}(\\log n)^{2+2 / \\alpha} \\max \\left\\{(\\log n)^{3 / \\alpha},(\\log n)^{4}\\right\\}\n$$\n\n## H Proof of Theorem 5.3\n\nProof of Theorem 5.3. Theorem 4.4 implies that, for any $T>0$, there exists an SSM $F \\in$ $\\mathcal{S}(M, U, D, L, W, S, B)$ with\n\n$$\n\\begin{aligned}\n& M=T^{1 / \\alpha}, \\quad U=V, \\quad D \\sim T^{c_{\\alpha, \\beta}} \\log ^{2} V \\\\\n& L \\sim T^{c_{\\alpha, \\beta}} \\log ^{3} V, \\quad W \\sim 2^{T / a^{\\dagger}} T^{c_{\\alpha, \\beta}} \\log ^{2} V \\\\\n& S \\sim 2^{T / a^{\\dagger}} T^{c_{\\alpha, \\beta}} \\log ^{3} V, \\quad \\log B \\sim T^{c_{\\alpha, \\beta}} \\log ^{3} V\n\\end{aligned}\n$$\n\nsuch that $\\left\\|F-F^{\\circ}\\right\\|_{2, P_{X}} \\lesssim 2^{-T}$. Therefore, it holds\n\n$$\n\\frac{1}{r-l+1} \\sum_{t=l}^{r}\\left\\|F_{i}-F_{i}^{\\circ}\\right\\|_{2, P_{X}}^{2} \\leq 2^{-2 T}\n$$\n\nNext, Theorem F. 2 shows that it holds\n\n$$\n\\log N\\left(\\mathcal{S}, \\delta,\\|\\cdot\\|_{\\infty}\\right) \\lesssim 2^{T / a^{\\dagger}} T^{2 / \\alpha+4 c_{\\alpha, \\beta}}(\\log V)^{10} \\log \\frac{1}{\\delta}\n$$\n\nAs same as the proof of Theorem5.2, we can show that\n\n$$\nR_{l, r}\\left(\\hat{F}, F^{\\circ}\\right) \\lesssim 2^{-2 T}+\\frac{2^{T / a^{\\dagger}} T^{2 / \\alpha+4 c_{\\alpha, \\beta}}(\\log V)^{10} \\log \\frac{1}{\\delta}}{n}+\\delta\n$$\n\nBy setting $T=\\frac{a^{\\dagger}}{2 a^{\\dagger}+1} \\log n$ and $\\delta=1 / n$, we have\n\n$$\nR_{l, r}\\left(\\hat{F}, F^{\\circ}\\right) \\lesssim n^{-\\frac{2 a^{\\dagger}}{2 a^{\\dagger}+1}}(\\log n)^{1+2 / \\alpha+4 c_{\\alpha, \\beta}} \\log ^{10} V\n$$\n\n## I Additional details on the experiments\n\nAll the code was implemented in Python 3.10.14 with Pytorch 1.13.1 and CUDA ver 11.7. The experiments were conducted on Ubuntu 20.04.5 with A100 PCIe 40GB. Genomic Benchmark dataset (Gre\u0161ov\u00e1 et al., 2023) is given with the Apache License Version 2.0 and can be accessed from https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks. The pretrained model of Hyena is given with the Apache License Version 2.0 and can be accessed from https://github.com/HazyResearch/safari?tab=readme-ov-file. For the training and evaluation of models, we utilized the code provided at https://colab research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL. Experiment 1. We used the dataset human_enhancers_cohn of Genomic Benchmark dataset. As for the pretrained model of Hyena, we used hyenadna-tiny- 1 k -seqlen. The model was fine-tuned for 100 epochs. Then, we sampled 20 different test sequences whose correct probability is larger or equal to 0.95 . For each sequence, we repeatedly mask the tokens that maximize the correct probability. The error bar is calculated by the standard deviation of these 20 samples. The source code for the experiment is downstream_finetune.py and downstream_mask.py, which can be found in the supplemental material. Finetuning needs around one hour, and masking needs around 90 minutes. Experiment 2. We used the dataset demo_human_or_worm, and we fine-tuned the model using the data labeled \"human\". As for the pretrained model of Hyena, we used hyenadna-small-32k-seqlen. The model was fine-tuned for 10 epochs. Then, we sampled 20 different test sequences that have more than 20 tokens with the correct probability $>0.35$. The error bar is calculated by the standard deviation of these 20 samples. The source code for the experiment is nextword_finetune.py and nextword_mask.py, which can be found in the supplemental material. Finetuning needs around 6 hours, and masking needs around 20 minutes. [^0]:    ${ }^{1}$ This setting is a little different from Takakura and Suzuki (2023), which considers bidirectional sequences. We assume that the input is unidirectional due to the natural setting of SSMs, but our theory can be extended to the bidirectional setting by changing the convolution filter in Section 3\n\n[^1]:    ${ }^{2}$ This architecture can be easily extended to the multi-order version since it corresponds to $g_{H} \\circ g_{H-1} \\circ \\cdots \\circ g_{1}$ with $W_{V}=I$ for $g_{2}, \\ldots, g_{H}$. ${ }^{3}$ If the filter is not data-controlled, then $\\kappa=0$.",
    "dynsmoothssms-37": ""
}