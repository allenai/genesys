{
    "ntk-0": "r/LocalLLaMA\nSearch in $r / L \\ldots$\nCreate\n(1\nr/LocalLLaMA 1 yr. ago\nbloc97\n\n# NTK-Aware Scaled RoPE allows LLaMA models to have extended ( $8 \\mathrm{k}+$ ) context size without any fine-tuning and minimal perplexity degradation. ## News\n\nI've seen the posts about SuperHOT and just recently, the paper from Meta which uses RoPE interpolation, and I've noticed an immediate improvement that can be brought to this method. Basically if you apply Neural Tangent Kernel (NTK) theory to this problem, it becomes clear that simply interpolating the RoPE's fourier space \"linearly\" is very sub-optimal, as it prevents the network to distinguish the order and positions of tokens that are very close by. Borrowing from NTK literature, scaling down the fourier features too much will eventually even prevent succesful finetunes (this is corroborated by the recent paper by Meta that suggests an upper bound of $\\sim 600 x$ )\n\nInstead of the simple linear interpolation scheme, I've tried to design a nonlinear interpolation scheme using tools from NTK literature. Basically this interpolation scheme changes the base of the RoPE instead of the scale, which intuitively changes the \"spinning\" speed which each of the RoPE's dimension vectors compared to the next. Because it does not scale the fourier features directly, all the positions are perfectly distinguishable from eachother, even when taken to the extreme (eg. streched 1 million times, which is effectively a context size of 2 Billion)\n\nTo my surprise, this method works extremely well, so much so that you don't even need to fine tune the LLaMA 7B model for 4096 context size! The perplexity degradation is minimal. I'm sure with fine tuning this would become even better. Enough explanations, here's some empirical results. All the perplexity measurements are done on LLaMA 7b with the tau/scrolls $\\cdot$ Datasets at Hugging Face dataset (I only used a subset of gov_report). Here's a graph showing the average perplexity of LLaMA 7 b on a set of 40 very long prompts (12k+ context size). Compared to changing the scale (from SuperHOT, which was set to 4), we change a factor alpha, which when equal to 8 provides the same context size increase but with much less perplexity degradation. All without any finetuning! ![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-02.jpg?height=1029&width=1774&top_left_y=179&top_left_x=147)\n\nGraph showing the average perplexity of LLaMA 7 b on set of 40 very long prompt ( $12 \\mathrm{k}+$ context size) with previous and new interpolation scheme\n\nHere's more results, showing more scale and alpha factors. ![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-02.jpg?height=1100&width=1780&top_left_y=1483&top_left_x=144)\n\nHere's more results, showing more scale and alpha factors. Zoomed-in version of the second graph, showing the details. ![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-03.jpg?height=1074&width=1775&top_left_y=134&top_left_x=152)\n\nZoomed-in version of the second graph, showing the details. Code can be found on a colab notebook with a test example. NTKAwareScaledRotaryEmbedding.ipynb Colaboratory_(google.com). Again, the changes to the RoPE code is only 3 lines. Also just for fun, I've pasted the entirety of the Meta paper [2306.15595] Extending Context Window of Large Language Models via Positional Interpolation (arxiv.org), with the abstract and conclusions removed, and asked OASST-sft-6-Ilama-30b some questions. I think it fared pretty well given it hasn't been fine tuned at all for $6000+$ context size. I used an alpha of 8 for this prompt. The entire paper is about 6200 tokens. These prompts can be found in the colab notebook linked above. The following questions are simply appended at the end of the prompt which contains the entire research paper. ## QUESTION\n\nPlease give me a brief summary of this research paper in a few bullet points. ## ANSWER\n\n* Large Language models (LLMs) often have a pre-defined context window size, but sometimes they need to operate on longer contexts, e.g., in conversation systems or summarization tasks. * Extending the context window of an existing pre-trained LLM is challenging because direct extrapolation of positional encodings can lead to poor performance. * Researchers propose \"Position Interpolation\" to extend the context window of LLMs by downscaling the positional encodings. * They experimentally validate the effectiveness of Position Interpolation on LLaMA models, reducing the perplexity gap between the original and extended contexts. * They compare Position Interpolation with direct extrapolation and find that Position Interpolation works better. * They apply Position Interpolation to LLaMa models and fine-tune them on the Pile dataset, finding that the models can effectively utilize longer contexts. * They also evaluate the models' abilities to attend to longer contexts and perform well on benchmarks within the original context size. * They apply Position Interpolation to long document summarization and obtain competitive results. * They provide insights into the interpolation/extrapolation tradeoff and discuss potential improvements. * They plan to explore more advanced methods like data augmentation and multi-task learning to improve the interpolation. ## QUESTION\n\nPlease write me the abstract for this paper. ## ANSWER\n\nThis paper presents a technique called \"Position Interpolation\" for extending the context window size of pre-trained Large Language Models (LLMs) without modifying their architecture. Previous methods like ALiBi and LeX were unable to efficiently extend the context window size of LLMs, leading to poor performance. The authors propose Position Interpolation, which involves downscaling the positional encoding of the LLM's attention mechanism to match the desired longer context window size. They prove mathematically that interpolated attention scores are more stable than extrapolated ones, allowing the model to adapt to longer contexts with just a few hundred fine-tuning steps. Experiments showed that Position Interpolation enables LLMs to perform well on longer contexts, reducing perplexity by up to 0.8 points on average, and generating strong summarizations. Extending the context window size to 32768 led to slight degradation on the original benchmarks, but the models adapted to longer texts. The authors conclude that interpolating positional encodings allows the model to attend to relevant evidence in the longer contexts, and the stability of the interpolated attention score helps the model avoid catastrophic behaviors. ## QUESTION\n\nHow many steps was the model fine tuned for the final results? Give a short answer. ## ANSWER\n\nThe authors of the paper \"Extending Context Windows of Large Language Models via Position Interpolation\" fine-tuned the model for 1000 batches for the experiments presented in the paper. ## ANSWER\n\nThe interpolation bound is at least $2-29.7^{*} 10^{\\wedge}(-6)$ times smaller than the extrapolation bound.",
    "ntk-1": "Sorry for the large dump of information on reddit, I'm too lazy to write a blog post for this. I might give a more detailed explanation on how I derived the formula used to calculate the base if enough people are interested. I did not test fine-tuning performance as I do not have the resources or the time to fine tune an LLM, I just derived this formula during lunch and experimented with it. However, I think that this method will do even better with fine tuning. Also thanks to the people behind the SuperHOT blogpost, it was their hard work that inspired me and allowed me to make this contribution for everyone! Finally, I really hope this post will inspire others to start experimenting on ways to improve LLMs.",
    "ntk-2": "There's so much to learn and so much left to discover! What a time to be alive! Students, whether you're creating show-stopping images, making memorable reports, or designing iconic posters, Adobe Creative Cloud helps you express all of your ideas. Save over 65\\% on Adobe Creative Cloud. Students save over 60\\% on Adobe Creative Cloud. Learn more\nLearn More\nadobe.com\n\nAdd a comment\n\nSort by: Best $\\vee$\nSearch Comments\nkaiokendev \u30fb 1y ago\nI am curious - You emailed this approach to me? I did not get a chance to test it with finetune yet, but based on your result I am impressed. Solid work\n$84 \\square$ Reply\nbloc97 OP \u30fb1y ago \u30fbEdited 1y ago\nsurprised that they did not even mention NTK in that paper as it's generally a \"proven\" theory and used extensively in the field of ML + computer vision/graphics and inverse rendering. For example the paper about Fourier feature networks. Where the positional embeddeding used for 2D/3D rendering is very similar to RoPE. $44 \\quad \\square$ Reply\n\n## kaiokendev \u30fb 1y ago\n\nI think it is commonly the case that the application of some easy things will go unnoticed or forgotten because everyone's mind is stuck in \"complex-mode\". In this case, RoPE is developed in 2021, so now we can look back and say how no one thought to apply this in the last 2 years, but in reality there are a lot of factors that result in various group being hyperfocused on specific approaches, and that's not even mentioning the fact only a small percentage of those end up getting reproduced and incorporated. The results end up looking more like the Tesla tunnels than the Autobahn. Sometimes all it takes is one person looking at it from the simple angle to get the breakthrough results like what you did here. At least thats my thought on it\nReply\n\nBangkokPadang \u30fb1y ago\nSometimes it just takes a fresh set of eyes. I love in u/kaiokendev 's explanation of how he came up with the approach used in the superHOT models, it was because he noticed that OpenAl's context seemed to be growing by 4 x , so he tried scaling the frequency window down by a factor of .25 and that did it, lol. $24 \\quad$ Reply\nbloc97 OP \u30fb 1y ago\nI think that openai must have used some trick to increase context size, because training gpt4 to have 32 k is unsustainable. If in a few weeks they extend their context by $4 x$ again then we will be sure that they did not know about SuperHOT and or NTK-aware RoPE scaling. $19 \\square$ Reply\nDannyDaemonic \u30fb1y ago\nCan this be used with superHOT to get 32 k ? 1 Reply\nBangkokPadang \u30fb 1y ago\nhttps://kaiokendev.github.io\nThe dev that came up with this method has some pretty detailed writings about it, and the possible ramifications of it. Looks like he's currently exploring 16k context here:\nhttps://huggingface.co/kaiokendev/superhot-13b-16k-no-rlhf-test\nSo there's really no solid \"yes or no\" answer yet. My guess would be Probably at some point. $2 \\square$ Reply\nJain92ru $\\cdot 1$ y ago\nCongratulations, now this post is cited in a new Meta paper:\nhttps://ai.meta.com/research/publications/code-Ilama-open-foundation-models-for-code\n$8 \\square$ Reply\n\nErikBjare \u30fb1y ago\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-07.jpg?height=295&width=623&top_left_y=1343&top_left_x=405)\n\nIncredible. Well done u/bloc97\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-07.jpg?height=140&width=141&top_left_y=1783&top_left_x=352)\n$\\square$ Reply\n\nTheWheelsOnTheBusG \u30fb1y ago\nActually legendary. $3 \\square$ Reply\nunital \u30fb1y ago \u30fbEdited 1y ago\nCan I ask if you could give a high level explanation of how NTK is being used in CV? My\n(rudimentary) understanding of the NTK is that training an infinitely wide NN is essentially the same as doing kernel regression - and the kernel is precisely the NTK.",
    "ntk-3": "How does this translate to applications in CV? Thanks! $2 \\square$ Reply\nbloc97 OP \u30fb1y ago \u30fbEdited 1y ago\n\nMost of the time in computer graphics, especially in inverse rendering, when you are trying to represent a 3D scene, we like to use some kind of continuous field (whether its a light field, signed distance field, radiance field, etc.), because in order to do inverse rendering (eg. obtain a 3D scene/object given a few 2D images), you need the 3D representation to be differentiable (so that you can find the gradient of the 3D scene w.r.t. the 2D images)\n\nWell what is differentiable and very capable at representing complex functions? Neural networks! However for decades researchers couldn't find a reason why NNs failed to learn low-dimensional data (eg. a light field is a 5D function, which is really \"low-dimensional\", compared to MNIST images which are 784-dimensional)\n\nIn NeRF they solved this issue and proved it mathematically using NTK, and showed that NNs, when used as a regression algorithm, tend to prefer very smooth solutions on lowdimensional data unless you force it to learn high frequencies using special positional encoding techniques. (They call the specific technique they used \"Fourier Features\" Fourier Feature Networks (bmild.github.io). )\n\nSince then all inverse rendering techniques are somewhat indirectly based off the insights found in the NeRF paper. $13 \\square$ Reply\n\nStill-Base-7503 \u30fb1y ago\nYou sir are a hero.",
    "ntk-4": "thanks fo your work. Now my 13b bimbo can remember things or two haha. $19 \\square$ Reply\n01\nIntimidatingOstrich6 \u30fb1y ago\nlolling at people busting their asses doing all this math and science and here we all are just like \"thanks brah, now my Al torture victim will actually remember that they couldn't run away even if they managed to abrade through their chain, since I cut one of their feet off two months ago!\"\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-08.jpg?height=229&width=592&top_left_y=1847&top_left_x=275)\n\nI've made a very cool python script to cut puppy legs based on heuristics and nodal transfers. $5 \\square$ Reply\n\nBangkokPadang \u30fb1y ago\n. $2 \\square$ Reply\n\nLumiphoton \u30fb1y ago \u30fbEdited 1y ago\n\nHopefully you get around to setting up a GitHub page for this work so that it will be seen by the right sets of eyes, I can see it getting quite a lot of attention on there. 14 Reply\nbloc97 OP \u30fb 1y ago\nThanks! However I'm not sure about writing anything more at this point. I made this post out of my spare time just so it doesn't get forgotten or make us unnecessarily waste time on rediscovering it in the future. $11 \\square$ Reply\n\nReturningTarzan \u30fb1y ago\nExLlama Developer\n(this is corroborated by the recent paper by Meta that suggests an upper bound of $\\sim 600 \\mathrm{x}$ )\n\nI mean... by then you're at 1.2 million tokens and you'll have hit many walls already: memory requirements, inference speed, numerical precision/stability... I think it's safe to say that aiming for context above, say, 100k without rethinking the basic idea of self-attention is a dead end. From the charts provided, alpha $=16$ appears to be a negative result (perplexity increases with more context added, starting from it seems 500 tokens?), alpha $=8$ survives the catastrophic collapse that alpha $=4$ suffers at about 5 k tokens, but it doesn't seem to consider more than 5 k tokens after that point. It'll be very interesting to see how it improves with finetuning. 27 Reply\nkaiokendev \u30fb 1y ago \u30fbEdited 1y ago\nYes, I agree. Still it is nice to see alpha $=2$ perform so well with no fine-tuning. Like you say I already observed base llama with no-finetuning can still perform well with linear scaling factor of 2 with no fine-tuning (at least, up to 4 K ), and we can see it in his chart as well (dotted yellow line), I think the scale of 4 on the base model is a bad example -- it is known there is massive ppl increase when scale $<0.5$ for the untrained model but performs well for 0.5 for some reason.",
    "ntk-5": "The alpha $=4$ also looks promising. But, it does not hurt to have a potentially better method, but we will see with fine-tuning. Everything else I will echo with you: the main problem is that damn cache :)\n$11 \\square$ Reply\nbloc97 OP \u30fb 1y ago\nI'm not sure if it's quite right to say it doesn't consider more tokens after 5k, as the network still stays coherent when generating beyond 5 k tokens, you can think of it having decreasing perplexity until about 6 k tokens, then it has 2 k more tokens that it can generate when reading the previous 6 k . $4 \\square$ Reply\nReturningTarzan \u30fb1y ago \u30fbEdited 1y ago\nExLlama Developer\n\nSkip to main content\naverage over lots of examples to rule out that that it's not just the sample text getting more surprising. $4 \\square$ Reply\nbloc97 OP \u30fb1y ago \u30fbEdited 1y ago\nPerplexity increasing does not necessarily mean that the network is not able to retrieve from and attend to previous tokens. Perplexity is a good indicator for open-ended generation, but usually is not that good at determining whether the network \"remembers\" past tokens. An obvious edge case could be you hiding a password inside of a document that is just full of $A$ repeating, eg:\n\"AAAA[...x2000]AAAAAThe password is: 86763AAAAAA[...x2000]AAA\"\nIf then you asked the question: \"What is the password?\"\n\nWhether the network can respond successfully or not will mostly be independent of perplexity. A network that only assumes the whole text is A will only have a very tiny increase in perplexity because it was surprised to see numbers instead of $A$. Meanwhile a network with high perplexity could still be surprised for the whole prompt because it finds it very suprising that someone would be spamming A on the keyboard, but still correctly be able to retrieve and give the correct password, no matter where that password was located (beginning, middle or end). I hope that makes sense, I find this quite hard to explain... However I do admit that my evaluation is lacking substance, as I should have provided PPL scores for the \"sliding window\" approach, so that we can see for sure that PPL (whether increasing or not) with the new method is actually lower than just truncating previous tokens. I might get to that when I have time, but my preliminary tests show that even at alpha $=16$ with very high perplexity, the network is able to attend to all the context size, however due to high PPL it hallucinates and loses coherence if you let generate for more than a few words... ## 5 Reply\n\nReturningTarzan \u30fb1y ago\nExLlama Developer\nPerplexity doesn't tell the whole story, no. But in a sequence that \"makes sense\", having more context to work with should, on average, make each subsequent token less surprising. Talking here about a natural language, of course, not a sequence that tries to break the patterns that the model is trained on, although even in a sequence of all A's the model should still be less and less surprised with each successive A. I think the best way to really measure would be to fix the sequence being evaluated while providing varying amounts of past context relevant to it. l.e. start at some position n in a long sequence and run inference on tokens [n-a*x:n+b], for some constants a and b and increasing values of $x$, while measuring the average perplexity only on $[n: n+b]$. As $x$ increases, the ground truth at [n:n+b] should become less and less surprising, until the model reaches some limit to the amount of context it can process, one way or another. Skip to main content\n\nI'm out of my Element but wouldn't there still be value for prose generation? Maybe I'm misunderstanding what you're responding to. ![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-11.jpg?height=183&width=533&top_left_y=414&top_left_x=336)\n\nIf the model isn't actually attending to tokens 1-1000 in a 6000 token sequence, you'll get the same result by not including tokens 1-1000 in the first place. It'll just be faster. But really you want the model to care about the whole context. And whether it does or not is reflected in perplexity, which is a measure of how \"surprised\" the model is by the ground truth. Having more context should make it easier for the model to predict what's coming, which is the flip side of its ability to generate coherent text. If perplexity bottoms out at some point it means you're no longer gaining any predictive power from adding more context, so you may as well not. ![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-11.jpg?height=184&width=512&top_left_y=1087&top_left_x=401)\n\nThanks for the reply\n$1 \\square$ Reply\n\n## ironborn123 \u30fb1y ago\n\nInterestingly, Proust's Remembrance of Things Past is generally regarded as the longest book at 1.3 mil words. so if tokens were words, then meta's 600x context would almost be able to consume it whole. Although currently tokens are on average half a word, so the book would be around 2.6 mil tokens. A cool benchmark for an Ilm to achieve one day. $1 \\square$ Reply\nLetterRip \u30fb 1y ago\ntokens are generally $3 / 4$ of a word. $1 \\square$ Reply\nmrjackspade \u30fb 1y ago\nHow long until we can summarize \"The Loud House: Revamped\"? $$\n1 \\square \\text { Reply }\n$$\n\nkryptkpr \u30fb1y ago\nLlama 3\nFantastic results! It continuously blows my mind how the most interesting work on ML is happening on a tiny subreddit. 18 Reply\n$8 \\square$ Reply\nardentis_ignis \u30fb 1y ago\nThis is genuinely mind-blowing! I'm incredibly eager to experience firsthand the future you're both ingeniously designing! u/kaiokendev and $u /$ bloc97, you are awe-inspiring. Your dedication, effort, and particularly your ingenuity stand as a beacon for all to admire. The fact that you've achieved such extraordinary results without the support of any companies or universities is a testament to your skills and determination. You're setting a formidable precedent, showcasing how passion, hard work, and innovative thinking can result in unparalleled achievements. I'm thoroughly impressed by your accomplishments and wait with bated breath to see what you'll pioneer next! Keep up the phenomenal work - you are truly making a significant impact. 12\nReply\n\nSingle_Ring4886 \u30fb1y ago\nYou are great! 65 Reply\n\nObiWanCanShowIMe \u30fb1y ago\nThis is exactly why AI will help humaniy is crazy ways. OP found something right out in the open and applied it. Imagine an Al with access finding everything that is right out in the open that no one is currently looking at or piecing together. The implications for general research are crazy. 11 Reply\nsolidsnakeblue \u30fb 1y ago\n\nThis is exactly why Al will help humaniy is crazy ways. OP found something right out in the open and applied it. Imagine an Al with access finding everything that is right out in the open that no one is currently looking at or piecing together. The implications for general research are crazy. This application of AI is what gets me the most excited. ![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-12.jpg?height=173&width=587&top_left_y=2389&top_left_x=169)\n\nDo we have an AI that can reason yet? I know we can test for reason, but those standardized tests are designed for humans without eidetic memory. $2 \\square$ Reply\n\nSkip to main content\n\nYeah of course, this method only changes the initial RoPE cache values in huggingface, it does not touch anything related to the model's inference code. $3 \\square$ Reply\nklop2031 \u30fb 1y ago\nThis community is amazing! 4 Reply\npseudonerv \u30fb 1y ago\ncan you eli18 how do you go from NTK theory to\nbase $=$ base ${ }^{*}$ a $^{* *}(\\operatorname{dim} /(\\operatorname{dim}-2))$\n??? $4 \\quad$ Reply\nbloc97 OP \u30fb1y ago \u30fb Edited 1y ago\n60. So for each second, the minute hand turns $1 / 60$ th of a minute, and for each minute, the hour hand turns $1 / 60$ th. Now if you slowed down time by a factor of $4 x$, that is a linear RoPE scaling used in SuperHOT. Unfortunately now it is really really hard to distinguish each second, because now the seconds hand barely moves each second. So if someone gave you two different times, which is only different by a single second, you won't be able to distinguish them from afar (let's say the NNs have myopia because that's basically what NTK predicts)\n\nNow NTK-Aware RoPE scaling does not slow down the seconds. One second is still one second, but it slows down minutes by a factor of let's say 1.5 , and the hours by a factor of 2 . This way you can fit 90 minutes in a hour, and fit 24 hours in half a day. So now you basically have a clock that can measure 129.6 k seconds instead of 43.2 k seconds. Because you don't need a precise measurement of the hour hand when looking at the time, scaling the hours more compared to seconds is crucial. You don't want to lose the precision of the seconds hand, but you can afford to lose precision on the minutes hand and even more on the hours hand. Then, it's just a matter of deriving the base change formula in order to obtain such a scaling. (where less precise dimensions are scaled more and more)\n\nI hope that makes sense! Edit: Fixed small calculation error, the seconds shouldn't change... $20 \\square$ Reply\npseudonerv \u30fb 1y ago\nThanks for the detailed eli14. So using my 14 yo math skill, the factor,\n$a^{* *}(\\operatorname{dim} /(\\operatorname{dim}-2))$\nis just a constant\n$8^{* *}(128 / 126)$\n\nThe angle theta now have an additional factor of $8^{\\star *}(-2(\\mathrm{i}-1) / 126)$.",
    "ntk-6": "For $\\mathrm{i}=1$, the factor is 1 . for $\\mathrm{i}=\\mathrm{d} / 2=64$, the factor is $1 / 8$. Perfect! 35 Reply\ndisperaller \u30fb3mo ago\nHi sir, the math is quite straight forward. However, according to this post, somehow i got confused again (gradientai/Llama-3-8B-Instruct-262k $\\cdot$ Hugging Face). Gradient, they use NTKaware to expand Ilama3 8 b from 8 k to 65 k , then from 65 k to 262 k , the rope theta they used to expand from $8 k$ to $65 k$ is 15.3 million, don't see how this 15.3 million calculates since $65 / 8$ should be about 8, and times the original llama3 rope theta ( 0.5 million), should be 4 m instead of 15.3 m .",
    "ntk-7": "I was hoping someone could help explain the math behind this, thanks in advance. Skip to main content\n\nIcaruswept $\\cdot 1 y$ ago\nThis is very cool. Please do a short paper or post somewhere so that it can be referred to in literature in the field. 4 Reply\n\nBackgroundFeeling707 \u30fb1y ago\nHow could I test this in Ilama.cpp? 3 Reply\n\nIgoory \u30fb1y ago \u30fbEdited 1y ago\nYou would have to compile it, after applying a patch to ggml.c\nFrom what I could tell it's just a matter of changing the lines:\nconst float theta_scale $=\\operatorname{powf}\\left(10000.0,-2.0 f / n \\_d i m s\\right) ;$\nto\nconst float theta_scale $=\\operatorname{powf(10000.0*}$ powf(8.0, n_dims / (n_dims - 2.0)), -2.0f/n_dims);\n$6 \\quad \\square$ Reply\n\nBackgroundFeeling707 \u30fb1y ago\ngreat, thanks! $1 \\square$ Reply\n(i)\nambient_temp_xeno \u30fb 1y ago\nLlama 65B\nJust to check, is this on the regular source code or a PR we already had? 1 Reply\n\nIgoory \u30fb 1y ago\nOn the regular source code, but you will also need the scratch buffer patch from the PR\n3 Reply\nambient_temp_xeno \u30fb1y ago\nLlama 65B\nExcellent, thanks! 15 Reply\nambient_temp_xeno $\\cdot 1$ y ago $\\cdot$ Edited 1y ago\nLlama 65B\nIt works. Skip to main content\n\nHow does VRAM behave? 2\nReply\nbloc97 OP \u30fb1y ago\nExtremely bad, as expected VRAM usage grows quadratically, I had to stop at 12 k tokens using the original huggingface transformers implementation. Maybe better attention methods would reduce vram requirements, but it is not the focus of this work... 5 Reply\ntronathan $\\cdot 1 y$ ago \u30fbEdited 1y ago\nForgive my ignorance; Do you happen to know, or have a general sense, of how exllama's vram usage compares to HF transformers in this regard? afaik exllama / exllama_hf uses a much more vram-efficient algorithm. You may want to look into this (using text-generation-webui, you can load it with `--loader exllama`). I am curious to hear some concrete numbers on how VRAM scales with context length on various models (7/13/33) using exllama.",
    "ntk-8": "I'm sure this information will surface soon!",
    "ntk-9": "Really amazing work! Thank you! The ELI14 explanation was most excellent. ![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-16.jpg?height=164&width=432&top_left_y=1420&top_left_x=275)\nin my experience it seems to scale linearly but the compute time does increase quadratically. 3 Reply\n\nAlternative_World936 \u30fb 1y ago \u30fbEdited 1y ago\nThat's Cool!",
    "ntk-10": "From my understanding, we scale the base up to achieve non-linear interpolation. However, according to the code, you put the scaling like:\nwhy do we need this \\alpha^ ${ }^{\\text {dim }} /$ dim-2\\}, can we simply scale the base up with alpha ? $2 \\square$ Reply\nmaratonininkas \u30fb1y ago\nThis will definitely work with just alpha\n$2 \\square$ Reply\n<img class=\"imgSvg\" id = \"m0zx6ptbwkvh7ffsho\" src=\"data:image/svg+xml;base64,<svg id="smiles-m0zx6ptbwkvh7ffsho" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 226 123.83938652028935" style="width: 225.75002448647723px; height: 123.83938652028935px; overflow: visible;"><defs><linearGradient id="line-m0zx6ptbwkvh7ffsho-1" gradientUnits="userSpaceOnUse" x1="152.25002448648996" y1="48.279842631120445" x2="183.75002448647723" y2="48.27987090573481"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-3" gradientUnits="userSpaceOnUse" x1="136.49999999996197" y1="75.55962871301207" x2="152.25002448648996" y2="48.279842631120445"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-5" gradientUnits="userSpaceOnUse" x1="104.99999999997463" y1="75.55960043839771" x2="136.49999999996197" y2="75.55962871301207"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-7" gradientUnits="userSpaceOnUse" x1="85.91461646735897" y1="97.27640350452516" x2="98.51463605658134" y2="75.45257463901183"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-9" gradientUnits="userSpaceOnUse" x1="89.24997551344667" y1="102.83938652028935" x2="104.99999999997463" y2="75.55960043839771"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-11" gradientUnits="userSpaceOnUse" x1="89.25002448651524" y1="48.279786081891686" x2="104.99999999997463" y2="75.55960043839771"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-13" gradientUnits="userSpaceOnUse" x1="57.74997551345936" y1="102.83935824567502" x2="89.24997551344667" y2="102.83938652028935"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-15" gradientUnits="userSpaceOnUse" x1="60.9000193970961" y1="53.949760634736506" x2="86.10001939708594" y2="53.94978325442797"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-17" gradientUnits="userSpaceOnUse" x1="57.75002448652795" y1="48.27975780727736" x2="89.25002448651524" y2="48.279786081891686"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-19" gradientUnits="userSpaceOnUse" x1="89.25002448651524" y1="48.279786081891686" x2="105.00004897304316" y2="21"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-21" gradientUnits="userSpaceOnUse" x1="48.485364135517" y1="75.45252973239693" x2="61.0853445462845" y2="97.27638121760174"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-23" gradientUnits="userSpaceOnUse" x1="42" y1="75.55954388916902" x2="57.74997551345936" y2="102.83935824567502"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-25" gradientUnits="userSpaceOnUse" x1="42" y1="75.55954388916902" x2="57.75002448652795" y2="48.27975780727736"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient><linearGradient id="line-m0zx6ptbwkvh7ffsho-27" gradientUnits="userSpaceOnUse" x1="105.00004897304316" y1="21" x2="136.50004897303052" y2="21.00002827461428"><stop stop-color="currentColor" offset="20%"></stop><stop stop-color="currentColor" offset="100%"></stop></linearGradient></defs><mask id="text-mask-m0zx6ptbwkvh7ffsho"><rect x="0" y="0" width="100%" height="100%" fill="white"></rect><circle cx="136.49999999996197" cy="75.55962871301207" r="7.875" fill="black"></circle></mask><style>
                .element-m0zx6ptbwkvh7ffsho {
                    font: 14px Helvetica, Arial, sans-serif;
                    alignment-baseline: 'middle';
                }
                .sub-m0zx6ptbwkvh7ffsho {
                    font: 8.4px Helvetica, Arial, sans-serif;
                }
            </style><g mask="url(#text-mask-m0zx6ptbwkvh7ffsho)"><line x1="152.25002448648996" y1="48.279842631120445" x2="183.75002448647723" y2="48.27987090573481" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-1')"></line><line x1="136.49999999996197" y1="75.55962871301207" x2="152.25002448648996" y2="48.279842631120445" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-3')"></line><line x1="104.99999999997463" y1="75.55960043839771" x2="136.49999999996197" y2="75.55962871301207" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-5')"></line><line x1="85.91461646735897" y1="97.27640350452516" x2="98.51463605658134" y2="75.45257463901183" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-7')"></line><line x1="89.24997551344667" y1="102.83938652028935" x2="104.99999999997463" y2="75.55960043839771" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-9')"></line><line x1="89.25002448651524" y1="48.279786081891686" x2="104.99999999997463" y2="75.55960043839771" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-11')"></line><line x1="57.74997551345936" y1="102.83935824567502" x2="89.24997551344667" y2="102.83938652028935" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-13')"></line><line x1="60.9000193970961" y1="53.949760634736506" x2="86.10001939708594" y2="53.94978325442797" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-15')"></line><line x1="57.75002448652795" y1="48.27975780727736" x2="89.25002448651524" y2="48.279786081891686" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-17')"></line><line x1="89.25002448651524" y1="48.279786081891686" x2="105.00004897304316" y2="21" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-19')"></line><line x1="48.485364135517" y1="75.45252973239693" x2="61.0853445462845" y2="97.27638121760174" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-21')"></line><line x1="42" y1="75.55954388916902" x2="57.74997551345936" y2="102.83935824567502" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-23')"></line><line x1="42" y1="75.55954388916902" x2="57.75002448652795" y2="48.27975780727736" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-25')"></line><line x1="105.00004897304316" y1="21" x2="136.50004897303052" y2="21.00002827461428" style="stroke-linecap:round;stroke-dasharray:none;stroke-width:1.26" stroke="url('#line-m0zx6ptbwkvh7ffsho-27')"></line></g><g><text x="183.75002448647723" y="48.27987090573481" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="152.25002448648996" y="48.279842631120445" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="136.49999999996197" y="67.68462871301207" class="element-m0zx6ptbwkvh7ffsho" fill="currentColor" style="
                text-anchor: start;
                writing-mode: vertical-rl;
                text-orientation: upright;
                letter-spacing: -1px;
                direction: ltr;
            "><tspan>O</tspan></text><text x="136.49999999996197" y="75.55962871301207" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="104.99999999997463" y="75.55960043839771" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="89.24997551344667" y="102.83938652028935" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="57.74997551345936" y="102.83935824567502" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="42" y="75.55954388916902" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="57.75002448652795" y="48.27975780727736" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="89.25002448651524" y="48.279786081891686" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="105.00004897304316" y="21" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text><text x="136.50004897303052" y="21.00002827461428" class="debug" fill="#ff0000" style="
                font: 5px Droid Sans, sans-serif;
            "></text></g></svg>\"/>\n\nSexPanther_Bot \u30fb1y ago\n$60 \\%$ of the time, it works every time\n\nI am confused, what does $60 \\%$ of the time mean?",
    "ntk-11": "$$\n2 \\square \\text { Reply }\n$$\n\nFeeling-Currency-360 \u30fb1y ago\nwell so coincidental that the base did indeed now grow bigger with the base model's now being up to 4 k . 1 Reply\n\nJonathanFly \u30fb1y ago\nDoes anyone have or know of an example implementation in plain pytorch, not huggingface transformers. Like something you could plug into https://github.com/karpathy/nanoGPT ?",
    "ntk-12": "This is probably trivial for anyone who knows what they are doing but I could use an example... Reply\nfemtoGPT? 1 Reply\nkache_y \u30fb1y ago\nCongratulations on the code llama cite\n$2 \\square$ Reply\nshamblack19 \u30fb3mo ago\nHey Im trying to understand the math here and have some questions:\n\n```\nbase = base * a ** (dim / (dim-2)) #Base change formula\n```\n\n1: Seems like the same base will be used at all positions along the embedding dimension, right? I thought the goal was to increase the base at higher embedding positions, where is that happening here? 2: I don't understand what (dim/(dim-2)) is doing. Why (dim-2)?? Let me know if my understanding is wrong, still trying to wrap my head around the intuition\n\n$$\n1 \\square \\text { Reply }\n$$\n\npower of $-2 i / d$ will change based on $i$, so if $i$ is small, then this thing will become 1 , has no effect on the low dimension values, for large $i$, this thing will also grow, causing the the high dimension values to show interpolation effects.",
    "ntk-13": "2. me neither :<\n$2 \\square$ Reply\nshamblack19 \u30fb3mo ago\nYou're awesome!!!! I actually had to go onto Desmos and plot out the math but I fully understand it now!! Really appreciate you haha\n\nGlad you're also confused about that exponent. It always evaluates to a constant that's close to 1, I'm thinking it's redundant. $1 \\square$ Reply\n\nDependent-Pomelo-853 \u30fb1y ago\nHas anyone been able to run the Colab? For me it stays stuck saying I need to install accelerate, when I literally have it imported. $1 \\square$ Reply\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dbe32765baccd1d0b68bg-18.jpg?height=117&width=115&top_left_y=1355&top_left_x=165)\ngijs4g \u30fb1y ago\n!pip install accelerate $-U$\n\nAfter installing, then restart runtime\n1 Reply\n\nPhotographMaster8285 \u30fb1y ago\nGreat work! Have you ever tested long-distance information, such as \"Please tell me the author's email\"? I tried it on a 7B Llama, and while it did generate coherent sentences, it didn't retain long-distance information like the author's email address. $1 \\square$ Reply\nzju1997_2022 \u30fb 1y ago\ndamn hot! $1 \\leftrightarrows$ Reply\n\nSirius-ctrl \u30fb1y ago\nHi, thanks for the great work. Do you mind sharing the code that you use to plot those figures? I use the same technique on original Llama ckpt but it seems like the perplexity is super high. I think their might be some bugs for my code but I cannot fine them. 1 Reply\n\nSkip to main content\n\nCould anyone please provide an integration of this approach for vllm? I unfrotunately have no idea how to apply this method in vllm :(\n$1 \\square$ Reply\n\nYesodYIN \u30fb 1y ago\nyour work is pretty good. i think it can be called as RoPE-twister to make it clear to understand\n1 Reply\n\nAdditional_Box_3063 \u30fb1y ago\nI have been facing with oom errors whenever i try to increase the context window of an IIm. The maximum i could output size of 4 k . Anything above is giving me the oom error. I have been trying NTKbased rope for extending the window size to 16k. 4 GPUs make up 128Gb of RAM in the DGX. Is this not enough ?,or am I doing anything wrong? I even set the devices to be auto which distributes the weights across all the devices. does anyone know on how to deal with this error ?? 1 Reply\n\nAccurate-Door3692 \u30fb1y ago\n128Gb VRAM is pretty enough to load model even without LoRA, but you'll be unable to use big context, coz memory consumption will raise immediately during inference and it doesn't matter how much GPUs you have, you'll see that memory all of them would be consumed. Experimenting with Llama2 7B model I noticed that it doesn't matter deploying it on one or two GPUs, during inference behaviour will be the same.",
    "ntk-14": "Can't say what is wrong here... 2 Reply\n\nAdditional_Box_3063 \u30fb1y ago\nSo increasing the context window is a myth ?? Why isn't anyone talking about it?? I have seen fine tuned models beings posted with 16k, 32 k in huggingface. Not sure how to work around with models. Any idea on how to deal with this context? The author in this paper fed the entire research paper to the prompt and it generated the output\n$1 \\square$ Reply\n\nAccurate-Door3692 \u30fb1y ago\nIt's not a myth, but to handle a big context you have to have something like A100 80GB\nI was able to repeat author's experiment on a single RTX 3090 and on two of them. So results I described above. 2\n(\\) Reply\n\nSkip to main content\n$1 \\square$ Reply\n\n## More posts you may like\n\nr/MachineLearning\n[R] Fine-Tuning Language Models with Just Forward Passes\n196 upvotes $\\cdot$ 34 comments\nr/\nr/AskFeminists\nWhy is it \"empowering\" for women to wear revealing clothes? 82 upvotes $\\cdot 66$ comments\nr/leetcode\nday\\#1 neetcode\n128 upvotes $\\cdot$ 16 comments\nr/LocalLLaMA\nExllamaV2, Now With Tensor Parallelism! 102 upvotes $\\cdot$ 42 comments\nr/LocalLLaMA\nSomeone claims to have finetuned Pythia for 8 k context length. 19 upvotes $\\cdot 8$ comments\nr/LocalLLaMA\n\"Today, the diff weights for LLaMA 7B were published which enable it to support context sizes of up to $32 \\mathrm{k}^{\\prime \\prime}$\n131 upvotes $\\cdot$ 24 comments\nr/PersonalFinanceCanada\nWhen did you get involved in your parents' finances? 88 upvotes $\\cdot 69$ comments\nr/LocalLLaMA\nMiniCPM3-4B Released! 139 upvotes $\\cdot$ 28 comments\n\nSkip to main content\n475 upvotes $\\cdot 216$ comments\nr/LocalLLaMA\nPingPong: Role-playing benchmark for LLMs\n137 upvotes $\\cdot$ 37 comments\nr/LocalLLaMA\nNew pruning method, Wanda, can prune LLMs to $50 \\%$ sparsity with no retraining or weight update needed and minimal degradation\n161 upvotes $\\cdot$ 28 comments\nr/MachineLearning\n[D] Does GPT-4-32k eliminates/reduces the use of chunk strategies? 31 upvotes $\\cdot$ 15 comments\nr/LocalLLaMA\nLooking for Ideas: Former AMD Engineer \\& Startup Founder Ready to Build an Open-Source Project\u2014 What Problems Need Solving? 218 upvotes $\\cdot$ 103 comments\nr/LocalLLaMA\nExclude Top Choices (XTC): A sampler that boosts creativity, breaks writing clich\u00e9s, and inhibits nonverbatim repetition, from the creator of DRY\n218 upvotes $\\cdot$ 106 comments\nr/LocalLLaMA\nQwen2 VL 7B: Far More Impressive Than I Thought! 109 upvotes $\\cdot$ 29 comments\nr/LocalLLaMA\nreMind: An Open-Source Digital Memory Assistant\n116 upvotes $\\cdot$ 21 comments\nr/MachineLearning\nLIMA, a 65B-Param LLaMa fine-tuned with standard supervised loss on only 1,000 carefully curated prompts \\& responses, without any RLHF, demonstrates remarkably strong... 309 upvotes $\\cdot$ 29 comments\nr/LocalLLaMA\nClaude-Dev Now With Local LLM support! (Ollama, OpenAI Compatible Servers)\n$[R]$ Tiny Language Models (below 10m parameters or only one transformer block) can generate paragraphs of coherent text and reason...provided training is limited to stories that only contain words... 575 upvotes $\\cdot$ 123 comments\nr/LocalLLaMA\nFlux. 1 converted into GGUF - what interesting opportunity it offers in Ilm space? 134 upvotes $\\cdot$ 60 comments\nr/LocalLLaMA\nReflection 70B lessons learned\n169 upvotes $\\cdot$ 59 comments\nr/ r/ChatGPTCoding\nMy experience with PrivateGPT (Iv\u00e1n Mart\u00ednez's project)\n30 upvotes $\\cdot$ 28 comments\nr/StableDiffusion\nSD's noise schedule is flawed! This new paper investigates it. 386 upvotes $\\cdot$ 122 comments\n$r /$ PygmalionAI\nThe Varying Levels of Getting Started with \"Uncensored\" LLM-Powered Chatbots\n51 upvotes $\\cdot 29$ comments\nr/LocalLLaMA\nCommand R+ Still the GOAT and why we need unaligned models\n268 upvotes $\\cdot$ 68 comments\n\n"
}