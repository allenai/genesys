{
    "kangpt-0": "# KAN 2.0: <br> Kolmogorov-Arnold Networks Meet Science \n\nZiming Liu ${ }^{1,4 *}$ Pingchuan Ma ${ }^{1,3} \\quad$ Yixuan Wang $^{2} \\quad$ Wojciech Matusik $^{1,3} \\quad$ Max Tegmark $^{1,4}$<br>${ }^{1}$ Massachusetts Institute of Technology<br>${ }^{2}$ California Institute of Technology<br>${ }^{3}$ Computer Science and Artificial Intelligence Laboratory (CSAIL), MIT<br>${ }^{4}$ The NSF Institute for Artificial Intelligence and Fundamental Interactions\n\n\n#### Abstract\n\nA major challenge of $\\mathrm{AI}+$ Science lies in their inherent incompatibility: today's AI is primarily based on connectionism, while science depends on symbolism.",
    "kangpt-1": "To bridge the two worlds, we propose a framework to seamlessly synergize Kolmogorov-Arnold Networks (KANs) and science. The framework highlights KANs' usage for three aspects of scientific discovery: identifying relevant features, revealing modular structures, and discovering symbolic formulas. The synergy is bidirectional: science to KAN (incorporating scientific knowledge into KANs), and KAN to science (extracting scientific insights from KANs). We highlight major new functionalities in pykan: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN compiler that compiles symbolic formulas into KANs. (3) tree converter: convert KANs (or any neural networks) to tree graphs. Based on these tools, we demonstrate KANs' capability to discover various types of physical laws, including conserved quantities, Lagrangians, symmetries, and constitutive laws. ## Science\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-01.jpg?height=516&width=1354&top_left_y=1715&top_left_x=380)\n\nFigure 1: Synergizing science and the Kolmogorov-Arnold Network (KAN). [^0]\n## 1 Introduction\n\nIn recent years, $\\mathrm{AI}+$ Science has emerged as a promising new field, leading to significant scientific advancements including protein folding prediction [37], automated theorem proving [95, 83], weather forecast [41], among others. A common thread among these tasks is that they can all be well formulated into problems with clear objectives, optimizable by black-box AI systems. While this paradigm works exceptionally well for application-driven science, a different kind of science exists: curiosity-driven science. In curiosity-driven research, the procedure is more exploratory, often lacking clear goals beyond \"gaining more understanding\". To clarify, curiosity-driven science is far from useless; quite the opposite. The scientific knowledge and understanding gained through curiosity often lay a solid foundation for tomorrow's technology and foster a wide range of applications. Although both application-driven and curiosity-driven science are invaluable and irreplaceable, they ask different questions. When astronomers observe the motion of celestial bodies, application-driven researchers focus on predicting their future states, while curiosity-driven researchers explore the physics behind the motion. Another example is AlphaFold, which, despite its tremendous success in predicting protein structures, remains in the realm of application-driven science because it does not provide new knowledge at a more fundamental level (e.g., atomic forces). Hypothetically, AlphaFold must have uncovered important unknown physics to achieve its highly accurate predictions. However, this information remains hidden from us, leaving AlphaFold largely a black box. Therefore, we advocate for new AI paradigms to support curiosity-driven science. This new paradigm of AI + Science demands a higher degree of interpretability and interactivity in AI tools so that they can be seamlessly integrated into scientific research. Recently, a new type of neural network called Kolmogorov-Arnold Network (KAN) [57], has shown promise for science-related tasks. Unlike multi-layer perceptrons (MLPs), which have fixed activation functions on nodes, KANs feature learnable activation functions on edges. Because KANs can decompose high-dimensional functions into one-dimensional functions, interpretability can be gained by symbolically regressing these 1D functions. However, their definition of interpretability is somewhat narrow, equating it almost exclusively with the ability to extract symbolic formulas. This limited definition restricts their scope, as symbolic formulas are not always necessary or feasible in science. For example, while symbolic equations are powerful and prevalent and physics, systems in chemistry and biology the systems are often too complex to be represented by such equations. In these fields, modular structures and key features may be sufficient to characterize interesting aspects of these systems. Another overlooked aspect is the reverse task of embedding knowledge into KANs: How can we incorporate prior knowledge into KANs, in the spirit of physics-informed learning? We enhance and extend KANs to make them easily used for curiosity-driven science. The goal of this paper can be summarized as follows:\n\nGoal: Synergize Kolmogorov-Arnold Networks $\\Leftrightarrow$ Science. $\\Leftarrow$ : Build in scientific knowledge to KANs (Section 3). $\\Rightarrow$ : Extract out scientific knowledge from KANs (Section 4). To be more concrete, scientific explanations may have different levels, ranging from the coarsest/easiest/correlational to the finest/hardest/causal:\n\n- Important features: For example, \" $y$ is fully determined by $x_{1}$ and $x_{2}$, while other factors do no matter.\" In other words, there exists a function $f$ such that $y=f\\left(x_{1}, x_{2}\\right)$. - Modular structures: For instance, \" $x_{1}$ and $x_{2}$ contributes to $y$ independently in an additive way.\" This means there exists functions $g$ and $h$ such that $y=g\\left(x_{1}\\right)+h\\left(x_{2}\\right)$. - Symbolic formulas: For example, \" $y$ depends on $x_{1}$ as a sine function and on $x_{2}$ as an exponential function\". In other words, $y=\\sin \\left(x_{1}\\right)+\\exp \\left(x_{2}\\right)$. The paper reports on how to incorporate and extract these properties from KANs. The structure of the paper is as follows (illustrated in Figure 1): In Section 2, we augment the original KAN with multiplication nodes, introducing a new model called MultKAN. In Section 3, we explore ways to embed scientific inductive biases into KANs, focusing on important features (Section 3.1), modular\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-03.jpg?height=1015&width=1362&top_left_y=243&top_left_x=379)\n\nFigure 2: Top: comparing KAN and MultKAN diagrams. MultKAN has extra multiplication layers M. Bottom: After training on $f(x, y)=x y$, KAN learns an algorithm requiring two addition nodes, while MultKAN requires only one multiplication node. structures (Section 3.2), and symbolic formulas (Section 3.3). In Section 4, we propose methods to extract scientific knowledge from KANs, again covering important features (Section 4.1), modular structures (Section 4.2), and symbolic formulas (Section 4.3). In Section 5, we apply KANs to various scientific discovery tasks using the tools developed in the previous sections. These tasks include discovering conserved quantities, symmetries, Lagrangians, and constitutive laws. Codes are available at https://github.com/KindXiaoming/pykan and can also be installed via pip install pykan. Although the title of the paper is \"KAN 2.0\", the release version of pykan is 0.2.x. ## 2 MultKAN: Augmenting KANs with multiplications\n\nThe Kolmogorov-Arnold representation theorem (KART) states that any continuous highdimensional function can be decomposed into a finite composition of univariate continuous functions and additions:\n\n$$\nf(\\mathbf{x})=f\\left(x_{1}, \\cdots, x_{n}\\right)=\\sum_{q=1}^{2 n+1} \\Phi_{q}\\left(\\sum_{p=1}^{n} \\phi_{q, p}\\left(x_{p}\\right)\\right)\n$$\n\nThis implies that addition is the only true multivariate operation, while other multivariate operations (including multiplication) can be expressed as additions combined with univariate functions. For example, to multiply two positive numbers $x$ and $y$, we can express this as $x y=\\exp (\\log x+\\log y)^{2}$ whose right-hand side only consists of addition and univariate functions ( $\\log$ and exp). [^1]However, given the prevalence of multiplications in both science and everyday life, it is desirable to explicitly include multiplications in KANs, which could potentially enhance both interpretability and capacity. Kolmogorov-Arnold Network (KAN) While the KART Eq. (1) corresponds to a two-layer network, Liu et al. [57] managed to extend it to arbitrary depths by recognizing that seemingly different outer functions $\\Phi_{q}$ and inner functions $\\phi_{q, p}$ can be unified through their proposed KAN layers. A depth- $L$ KAN can be constructed simply by stacking $L$ KAN layers. The shape of a depth- $L$ KAN is represented by an integer array $\\left[n_{0}, n_{1}, \\cdots, n_{L}\\right]$ where $n_{l}$ denotes the number of neurons in the $l^{\\text {th }}$ neuron layers. The $l^{\\text {th }}$ KAN layer, with $n_{l}$ input dimensions and $n_{l+1}$ output dimensions, transforms an input vector $\\mathbf{x}_{l} \\in \\mathbb{R}^{n_{l}}$ to $\\mathbf{x}_{l+1} \\in \\mathbb{R}^{n_{l+1}}$\n\n$$\n\\mathbf{x}_{l+1}=\\underbrace{\\left(\\begin{array}{cccc}\n\\phi_{l, 1,1}(\\cdot) & \\phi_{l, 2,1}(\\cdot) & \\cdots & \\phi_{l, n_{l}, 1}(\\cdot) \\\\\n\\phi_{l, 1,2}(\\cdot) & \\phi_{l, 2,2}(\\cdot) & \\cdots & \\phi_{l, n_{l}, 2}(\\cdot) \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\phi_{l, 1, n_{l+1}}(\\cdot) & \\phi_{l, 2, n_{l+1}}(\\cdot) & \\cdots & \\phi_{l, n_{l}, n_{l+1}}(\\cdot)\n\\end{array}\\right)}_{\\boldsymbol{\\Phi}_{l}} \\mathbf{x}_{l}\n$$\n\nand the whole network is a composition of $L$ KAN layers, i.e.,\n\n$$\n\\operatorname{KAN}(\\mathbf{x})=\\left(\\boldsymbol{\\Phi}_{L-1} \\circ \\cdots \\circ \\boldsymbol{\\Phi}_{1} \\circ \\boldsymbol{\\Phi}_{0}\\right) \\mathbf{x}\n$$\n\nIn diagrams, KANs can be intuitively visualized as a network consisting of nodes (summation) and edges (learnable activations), as shown in Figure 2 top left. When trained on the dataset generated from $f(x, y)=x y$, the KAN (Figure 2 bottom left) uses two addition nodes, making it unclear what the network is doing. However, after some consideration, we realize it leverages the equality $x y=\\left((x+y)^{2}-(x-y)^{2}\\right) / 4$ but this is far from obvious. Multiplicative Kolmogorov-Arnold Networks (MultKAN) To explicitly introduce multiplication operations, we propose the MultKAN, which can reveal multiplicative structures in data more clearly. A MultKAN (shown in Figure 2 top right) is similar to a KAN, with both having standard KAN layers. We refer to the input nodes of a KAN layer as nodes, and the output nodes of a KAN layer subnodes. The difference between KAN and MultKAN lies in the transformations from the current layer's subnodes to the next layer's nodes. In KANs, nodes are directly copied from the previous layer's subnodes. In MultKANs, some nodes (addition nodes) are copied from corresponding subnodes, while other nodes (multiplication nodes) perform multiplication on $k$ subnodes from the previous layer. For simplicity, we set $k=2$ below ${ }^{3}$\nBased on the MultKAN diagram (Figure 2 top right), it can be intuitively understood that a MultKAN is a normal KAN with optional multiplications inserted in. To be mathematically precise, we define the following notations: The number of addition (multiplication) operations in layer $l$ are denoted as $n_{l}^{a}\\left(n_{l}^{m}\\right)$, respectively. These are collected into arrays: addition width $\\mathbf{n}^{a} \\equiv\\left[n_{0}^{a}, n_{1}^{a}, \\cdots, n_{L}^{a}\\right]$ and multiplication width $\\mathbf{n}^{m} \\equiv\\left[n_{0}^{m}, n_{1}^{m}, \\cdots, n_{L}^{m}\\right]$. When $n_{0}^{m}=n_{1}^{m}=$ $\\cdots=n_{L}^{m}=0$, the MultKAN reduces to a KAN. For example, Figure 2 (top right) shows a MultKAN with $\\mathbf{n}^{a}=[2,2,1]$ and $\\mathbf{n}^{m}=[0,2,0]$. A MultKAN layer consists of a standard KANLayer $\\boldsymbol{\\Phi}_{l}$ and a multiplication layer $\\mathbf{M}_{l} . \\boldsymbol{\\Phi}_{l}$ takes in an input vector $\\mathbf{x}_{l} \\in \\mathbb{R}^{n_{l}^{a}+n_{l}^{m}}$ and outputs $\\mathbf{z}_{l}=\\boldsymbol{\\Phi}_{l}(\\mathbf{x}) \\in \\mathbb{R}^{n_{l+1}^{a}+2 n_{l+1}^{m}}$. The multiplication layer consists of two parts: the multiplication part performs multiplications on subnode pairs, while the other part performs identity transformation. Written in Python, $\\mathbf{M}_{l}$ transforms $\\mathbf{z}_{l}$ as follows:\n\n$$\n\\mathbf{M}_{l}\\left(\\mathbf{z}_{l}\\right)=\\operatorname{concatenate}\\left(\\mathbf{z}_{l}\\left[: n_{l+1}^{a}\\right], \\mathbf{z}_{l}\\left[n_{l+1}^{a}:: 2\\right] \\odot \\mathbf{z}_{l}\\left[n_{l+1}^{a}+1:: 2\\right]\\right) \\in \\mathbb{R}^{n_{l+1}^{a}+n_{l+1}^{m}}\n$$\n\nwhere $\\odot$ is element-wise multiplication. The MultKANLayer can be succinctly represented as $\\boldsymbol{\\Psi}_{l} \\equiv$ $\\mathbf{M}_{l} \\circ \\boldsymbol{\\Phi}_{l}$. The whole MultKAN is thus:\n\n$$\n\\operatorname{MultKAN}(\\mathbf{x})=\\left(\\boldsymbol{\\Psi}_{L} \\circ \\boldsymbol{\\Psi}_{L-1} \\circ \\cdots \\circ \\boldsymbol{\\Psi}_{1} \\circ \\boldsymbol{\\Psi}_{0}\\right) \\mathbf{x}\n$$\n\nSince there are no trainable parameters in multiplication layers, all sparse regularization techniques (e.g., $\\ell_{1}$ and entropy regularization) for KANs [57] can be directly applied to MultKANs. For\n\n[^2]![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-05.jpg?height=455&width=1328&top_left_y=272&top_left_x=416)\n\nFigure 3: Adding auxiliary variables to inputs enhances interpretability. For the relativistic mass equation, $m=m_{0} / \\sqrt{1-v^{2} / c^{2}}$, (a) a two-layer KAN is needed if only ( $m_{0}, v, c$ ) are used as inputs. (b) If we add $\\beta \\equiv v / c$ and $\\gamma \\equiv 1 / \\sqrt{1-\\beta^{2}}$ as auxiliary variables to KANs, a one-layer KAN suffices (seed 0 ). (c) seed 1 finds a different solution, which is sub-optimal and can be avoided through hypothesis testing (Section 4.3). the multiplication task $f(x, y)=x y$, the MultKAN indeed learns to use one multiplication node, making it perform simple multiplication, as all the learned activation functions are linear (Figure 2 bottom right). Although KANs have previously been seen as a special case of MultKANs, we extend the definition and treat \"KAN\" and \"MultKAN\" as synonyms. By default, when we refer to KANs, multiplication is allowed. If we specifically refer to a KAN without multiplication, we will explicitly state so. ## 3 Science to KANs\n\nIn science, domain knowledge is crucial, allowing us to work effectively even with small or zero data. Therefore, it is beneficial to adopt a physics-informed approach for KANs: we should incorporate available inductive biases into KANs while preserving their flexibility to discover new physics from data. We explore three types of inductive biases that can be integrated into KANs. From the coarsest/easiest/correlational to the finest/hardest/causal, they are important features (Section 3.1), modular structures (Section 3.2) and symbolic formulas (Section 3.3). ### 3.1 Adding important features to KANs\n\nIn a regression problem, the goal is to find a function $f$ such that $y=f\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)$. Suppose we want to introduce an auxiliary input variable $a=a\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$, transforming the function to $y=f\\left(x_{1}, \\cdots, x_{n}, a\\right)$. Although the auxiliary variable $a$ does not add new information, it can increase the expressive power of the neural network. This is because the network does not need to expend resources to calculate the auxiliary variable. Additionally, the computations may become simpler, leading to improved interpretability. Users can add auxiliary features to inputs using the augment_input method:\n\n```\nmodel.augment_input(original_variables, auxiliary_variables, dataset)\n```\n\nAs an example, consider the formula for relativistic mass $m\\left(m_{0}, v, c\\right)=m_{0} / \\sqrt{1-(v / c)^{2}}$ where $m_{0}$ is the rest mass, $v$ is the velocity of the point mass, and $c$ is the speed of light. Since physicists often work with dimensionless numbers $\\beta \\equiv v / c$ and $\\gamma \\equiv 1 / \\sqrt{1-\\beta^{2}} \\equiv 1 / \\sqrt{1-(v / c)^{2}}$, they might introduce $\\beta$ and $\\gamma$ alongside $v$ and $c$ as inputs. Figure 3, shows KANs with and without these auxiliary variables: (a) illustrates the KAN compiled from the symbolic formula (see Section 3.3 for the KAN compiler), which requires 5 edges; (b)(c) shows KANs with auxiliary variables, requiring only 2 or 3 edges and achieving loses of $10^{-6}$ and $10^{-4}$, respectively. Note that (b) and (c) differ only in random seeds. Seed 1 represents a sub-optimal solution because it also identifies $\\beta=v / c$ as a key feature. This is not surprising, as in the classical limit $v \\ll c, \\gamma \\equiv 1 / \\sqrt{1-(v / c)^{2}} \\approx$ $1+(v / c)^{2} / 2=1+\\beta^{2} / 2$. The variation due to different seeds can be seen either as a feature or a bug: As a feature, this diversity can help find sub-optimal solutions which may nevertheless offer\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-06.jpg?height=814&width=1375&top_left_y=246&top_left_x=364)\n\nFigure 4: Building modular structures to KANs: (a) multiplicative separability;(b) symmetries. interesting insights; as a bug, it can be eliminated using the hypothesis testing method proposed in Section 4.3. ### 3.2 Building modular structures to KANs\n\nModularity is prevalent in nature: for example, the human cerebral cortex is divided into several functionally distinct modules, each of these modules responsible for specific tasks such as perception or decision making. This modularity simplifies the understanding of neural networks, as it allows us to interpret clusters of neurons collectively rather than analyzing each neuron individually. Structural modularity is characterized by clusters of connections where intra-cluster connections are much stronger than inter-cluster ones. To enforce modularity, we introduce the module method, which preserves intra-cluster connections while removing inter-cluster connections. The modules are specified by users. The syntax is\nmodel.module(start_layer_id, '[nodes_id]->[subnodes_id] -> [nodes_id] . . ')\n\nFor example, if a user wants to assign specific nodes/subnodes to a module - say, the $0^{\\text {th }}$ node in layer 1 , the $1^{\\text {st }}$ and $3^{\\text {rd }}$ subnode in layer 1 , the $1^{\\text {st }}$ and $3^{\\text {rd }}$ node in layer $2-$ they might use module ( 1, ' $\\left.[0]->[1,3]->[1,3]^{\\prime}\\right)$. To be concrete, there are two types of modularity: separability and symmetry. Separability We say a function is considered separable if it can be expressed as a sum or product of functions of non-overlapping variable groups. For example, a fourvariable function $f\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)$ is maximally multiplicatively separable if it has the form $f_{1}\\left(x_{1}\\right) f_{2}\\left(x_{2}\\right) f_{3}\\left(x_{3}\\right) f_{4}\\left(x_{4}\\right)$, creating four distinct groups (1), (2), (3), (4). Users can create these modules by calling the module method four times: module( 0 ,'[i]->[i]'), $i=0,1,2,3$, shown in Figure 4 (a). The final call may be skipped since the first three are sufficient to define the groups. Weaker forms of multiplicative separability might be $f_{1}\\left(x_{1}, x_{2}\\right) f_{2}\\left(x_{3}, x_{4}\\right)$ (calling module $\\left(0,6^{\\prime}[0,1]->[0,1]^{\\prime}\\right)$ ) or $f_{1}\\left(x_{1}\\right) f_{2}\\left(x_{2}, x_{3}, x_{4}\\right)$ (calling module ( $\\left.0,{ }^{\\prime}[0]->[0] '\\right)$ ). Generalized Symmetry We say a function is symmetric in variables $\\left(x_{1}, x_{2}\\right)$ if $f\\left(x_{1}, x_{2}, x_{3}, \\cdots\\right)=$ $g\\left(h\\left(x_{1}, x_{2}\\right), x_{3}, \\cdots\\right)$. This property is termed symmetry because the value of $f$ remains unchanged as long as $h\\left(x_{1}, x_{2}\\right)$ is constant, even if $x_{1}$ and $x_{2}$ vary. For example, a function $f$ is rotational invariant in 2D if $f\\left(x_{1}, x_{2}\\right)=g(r)$, where $r \\equiv \\sqrt{x_{1}^{2}+x_{2}^{2}}$. When symmetry involves only a subset of variables, it can be considered hierarchical since $x_{1}$ and $x_{2}$ interact first through $h$ (2-Layer KAN), and then $h$ interacts with other variables via $g$ (2-Layer KAN). Suppose a four-variable function has a hierarchical form $f\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)=h\\left(f\\left(x_{1}, x_{2}\\right), g\\left(x_{3}, x_{4}\\right)\\right)$,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-07.jpg?height=1746&width=1360&top_left_y=249&top_left_x=386)\n\nFigure 5: KAN compiler (kanpiler) converts symbolic expressions to KANs. (a) how kanpiler works: the symbolic formula is first parsed to an expression tree, which is then converted to a KAN. (b) Applying KANs to 10 equations (selected from the Feynman dataset). (c) Expand a compiled KAN to increase its expressive power. as illustrated in Figure 4 (b). We can use the module method to create this structure by calling module $\\left(0,{ }^{\\prime}[0,1]->[0,1]->[0,1]->[0]^{\\prime}\\right)$, ensuring that the variable groups $\\left(x_{1}, x_{2}\\right)$ and $\\left(x_{3}, x_{4}\\right)$ do not interact in the first two layers. ### 3.3 Compiling symbolic formulas to KANs\n\nScientists often find satisfaction in representing complex phenomena through symbolic equations. However, while these equations are concise, they may lack the expressive power needed to capture\nall nuances due to their specific functional forms. In contrast, neural networks are highly expressive but may inefficiently spend training time and data to learn domain knowledge already known to scientists. To leverage the strengths of both approaches, we propose a two-step procedure: (1) compile symbolic equations into KANs and (2) fine-tune these KANs using data. The first step aims to embed known domain knowledge into KANs, while the second step focuses on learning new \"physics\" from data. kanpiler (KAN compiler) The goal of the kanpiler is to convert a symbolic formula to a KAN. The process, illustrated in Figure 5 (a), involves three main steps: (1) The symbolic formula is parsed into a tree structure, where nodes represent expressions, and edges denote operations/functions. (2) This tree is then modified to align with the structure of a KAN graph. Modifications include moving all leaf nodes to the input layer via dummy edges, and adding dummy subnodes/nodes to match KAN architecture. These dummy edges/nodes/subnodes only perform identity transformation.",
    "kangpt-2": "(3) The variables are combined in the first layer, effectively converting the tree into a graph. For visual clarity, 1D curves are placed on edges to represent functions. We have benchmarked the kanpiler on the Feynman dataset and it successfully handles all 120 equations. Examples are shown in Figure 5 (b). The kanpiler takes input variables (as sympy symbols) and output expression (as a sympy expression), and returns a KAN model\n\n```\nmodel = kanpiler(input_variables, output_expression)\n```\n\nNote that the returned KAN model is in the symbolic mode, i.e., the symbolic functions are exactly encoded. If we instead use cubic splines to approximate these symbolic functions, we get MSE losses $\\ell \\propto N^{-8}$ [57], where $N$ is the number of grid intervals (proportional to the number of model parameters). Width/depth expansion for increased expressive power The KAN network generated by the kanpiler is compact, without no redundant edges, which might limit its expressive power and hinder further fine-tuning. To address this, we propose expand_width and expand_depth methods to expand the network to become wider and deeper, as shown in Figure 5(c). The expansion methods initially add zero activation functions, which suffer from zero gradients during training. Therefore, the perturb method should be used to perturb these zero functions into non-zero values, making them trainable with non-zero gradients. ## 4 KANs to Science\n\nToday's black box deep neural networks are powerful, but interpreting these models remains challenging. Scientists seek not only high-performing models but also the ability to extract meaningful knowledge from the models. In this section, we focus on enhancing the interpretability of KANs scientific purposes. We will explore three levels of knowledge extraction from KANs, from the most basic to the most complex: important features (Section 4.1), modular structures (Section 4.2), and symbolic formulas (Section 4.3). ### 4.1 Identifying important features from KANs\n\nIdentifying important variables is crucial for many tasks. Given a regression model $f$ where $y \\approx$ $f\\left(x_{1}, x_{2}, \\ldots, x_{n}\\right)$, we aim to assign scores to the input variables to gauge their importance. Liu et al. [57], used the function L1 norm to indicate the importance of edges, but this metric could be problematic as it only considers local information. To address this, we introduce a more effective attribution score which better reflects the importance of variables than the L1 norm. For simplicity, let us assume there are multiplication nodes, so we do not need to differentiate between nodes and subnodes ${ }^{4}$ Suppose we have an $L$-layer KAN with width $\\left[n_{0}, n_{1}, \\cdots, n_{L}\\right]$. We define $E_{l, i, j}$ as the standard deviation of the activations on the $(l, i, j)$ edge, and $N_{l, i}$ as the standard deviation of the activations on the $(l, i)$ node. We then define the node (attribution) score $A_{l, i}$ and the edge (attribution) score $B_{l, i, j}$. In [57], we simply defined $B_{l, i, j}=E_{l, i, j}$ and $A_{l, i}=N_{l, i}$. However, this definition fails to account for the later parts of the network; even if a node or an edge has a large norm itself, it may not contribute to the output if the rest of the network is effectively a zero function. Therefore, we now compute node and edge scores\n\n[^3](a) comparing L1 function norm and attribution score\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-09.jpg?height=686&width=760&top_left_y=302&top_left_x=379)\n(b) prune inputs based on attribution scores\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-09.jpg?height=686&width=573&top_left_y=302&top_left_x=1166)\n\nFigure 6: Identifying important features in KANs. (a) comparing the attribution score to the L1 norm used in Liu et al. [57. On two synthetic tasks, the attribution score brings more insights than the L1 norm. (b) Attribution scores can be computed for inputs and used for input pruning. iteratively from the output layer to the input layer. We set all output dimensions to have unit scores, i.e., $A_{L, i}=1, i=0,1, \\cdots, n_{L}-15$, and compute scores as follows:\n\n$$\nB_{l-1, i, j}=A_{l, j} \\frac{E_{l, j}}{N_{l+1, j}}, \\quad A_{l-1, i}=\\sum_{j=0}^{n_{l}} B_{l-1, i, j}, \\quad l=L, L-1, \\cdots, 1\n$$\n\nComparing $E_{l, i, j}$ and $B_{l, i, j}$ We find that $B_{l, i, j}$ provides a more accurate reflection of edge importance. In Figure 6, we compare KANs trained on two equations $y=\\exp \\left(\\sin \\left(\\pi x_{1}\\right)+x_{2}^{2}\\right)$ and $y=\\left(x_{1}^{2}+x_{2}^{2}\\right)^{2}+\\left(x_{3}^{2}+x_{4}^{2}\\right)^{2}$ and visualize KANs with importance scores being $E$ (L1 norm) or $B$ (attribution score). For the first equation, attributions scores reveal a cleaner graph than L1 norms, as many active edges in the first layer do not contribute to the final output due to inactive subsequent edges. The attribution score accounts for this, resulting in a more meaningful graph. For the second equation $y=\\left(x_{1}^{2}+x_{2}^{2}\\right)^{2}+\\left(x_{3}^{2}+x_{4}^{2}\\right)^{2}$, we can tell from the symbolic equation that all four variables are equally important. The attribution scores correctly reflect the equal importance of all four variables, whereas the L1 norm incorrectly suggests that $x_{3}$ and $x_{4}$ are more important than $x_{1}$ and $x_{2}$. Pruning inputs based on attribution scores In real datasets, input dimensionality can be large, but only a few variables may be relevant. To address this, we propose pruning away irrelevant features based on attribution scores so that we can focus on the most relevant ones. Users can apply the prune_input to retain only the most relevant variables. For instance, if there are 100 input features ordered by decreasing relevance in the function $y=\\sum_{i=0}^{99} x_{i}^{2} / 2^{i}, x_{i} \\in[-1,1]$, and after training, only the first five features show significantly higher attribution scores, the prune_input method will retain only these five features. The pruned network becomes compact and interpretable, whereas the original KAN with 100 inputs is too dense for straightforward interpretation. ### 4.2 Identifying modular structures from KANs\n\nAlthough the attribution score provides valuable insights into which edges or nodes are important, it does not reveal modular structures, i.e., how the important edges and nodes are connected. In this part, we aim to uncover modular structures from trained KANs and MLPs by examining two types of modularity: anatomical modularity and functional modularity. ![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-10.jpg?height=716&width=1340&top_left_y=260&top_left_x=379)\n\nFigure 7: Inducing anatomical modularity in neural networks through neuron swapping. The approach involves assigning spatial coordinates to neurons and permuting them to minimize the overall connection cost. For two tasks (left: multitask parity, right: hierarchical majority voting), neuron swapping works for KANs (top) in both cases and works for MLPs (bottom) for multitask parity. ### 4.2.1 Anatomical modularity\n\nAnatomical modularity refers to the tendency for neurons placed close to each other spatially to have stronger connections than those further apart. Although artificial neural networks lack physical spatial coordinates, introducing the concept of physical space has been shown to enhance interpretability [51, 52]. We adopt the neuron swapping method from [51, 52], which shortens connections while preserving the network's functionality. We call the method auto_swap. The anatomical modular structure revealed through neuron swapping facilitates easy identification of modules, even visually, for two tasks shown Figure 77. (1) multitask sparse parity; and (2) hierarchical majority voting. For multitask sparse parity, we have 10 input bits $x_{i} \\in\\{0,1\\}, i=1,2, \\cdots, 10$, and output $y_{j}=x_{2 j-1} \\oplus x_{2 j}, j=1, \\cdots, 5$, where $\\oplus$ denotes modulo 2 addition. The task exhibits modularity because each output depends only on a subset of inputs. auto_swap successfully identifies modules for both KANs and MLPs, with the KAN discovering simpler modules. For hierarchical majority voting, with 9 input bits $x_{i} \\in\\{0,1\\}, i=1, \\cdots, 9$, and the output $y=\\operatorname{maj}\\left(\\operatorname{maj}\\left(x_{1}, x_{2}, x_{3}\\right), \\operatorname{maj}\\left(x_{4}, x_{5}, x_{6}\\right), \\operatorname{maj}\\left(x_{7}, x_{8}, x_{9}\\right)\\right)$, where maj stands for majority voting (output 1 if two or three inputs are 1 , otherwise 0 ). The KAN reveals the modular structure even before auto_swap, and the diagram becomes more organized after auto_swap. The MLP shows some modular structure from the pattern of the first layer weights, indicating interactions among variables, but the global modular structure remains unclear regardless of auto_swap. ### 4.2.2 Functional modularity\n\nFunctional modularity pertains to the overall function represented by the neural network. Given an Oracle network where internal details such as weights and hidden layer activations are inaccessible (too complicated to analyze), we can still gather information about functional modularity through forward and backward passes at the inputs and outputs. We define three types of functional modularity (see Figure 8(a)), based largely on [84]. Separability: A function $f$ is additively separable if\n\n$$\nf\\left(x_{1}, x_{2}, \\cdots x_{n}\\right)=g\\left(x_{1}, \\ldots, x_{k}\\right)+h\\left(x_{k+1}, \\ldots, x_{n}\\right)\n$$\n\nNote that $\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}=0$ when $1 \\leq i \\leq k, k+1 \\leq j \\leq n$. To detect the separability, we can compute the Hessian matrix $\\mathbf{H} \\equiv \\nabla^{T} \\nabla f\\left(\\mathbf{H}_{i j}=\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}\\right)$ and check for block structure. If $\\mathbf{H}_{i j}=0$ for\n\n[^4]![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-11.jpg?height=1535&width=1331&top_left_y=509&top_left_x=387)\n\nFigure 8: Detecting functional modularity in KANs. (a) We study three types of functional modularity: separability (additive or multiplicative), general separability, and symmetry. (b) Applying these tests recursively converts a function into a tree. Here the function can be symbolic functions (top), KANs (middle) or MLPs (bottom). Both KANs and MLPs produce correct tree graphs at the end of training but show different training dynamics. all $1 \\leq i \\leq k$ and $k+1 \\leq j \\leq n$, then we know $f$ is additively separable. For multiplicative separability, we can convert it to additive separability by taking the logarithm:\n\n$$\n\\begin{aligned}\n& f\\left(x_{1}, x_{2}, \\cdots x_{n}\\right)=g\\left(x_{1}, \\ldots, x_{k}\\right) \\times h\\left(x_{k+1}, \\ldots, x_{n}\\right) \\\\\n& \\log \\left|f\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)\\right|=\\log \\left|g\\left(x_{1}, \\ldots, x_{k}\\right)\\right|+\\log \\left|h\\left(x_{k+1}, \\ldots, x_{n}\\right)\\right|\n\\end{aligned}\n$$\n\nTo detect multiplicative separability, we define $\\mathbf{H}_{i j} \\equiv \\frac{\\partial^{2} \\log |f|}{\\partial x_{i} \\partial x_{j}}$, and check for block structure. Users can call test_separability to test general separability. Generalized separability: A function $f$ has generalized separability if\n\n$$\nf\\left(x_{1}, x_{2}, \\cdots x_{n}\\right)=F\\left(g\\left(x_{1}, \\ldots, x_{k}\\right)+h\\left(x_{k+1}, \\ldots, x_{n}\\right)\\right)\n$$\n\nTo detect generalized separability, we compute\n\n$$\n\\begin{aligned}\n& \\frac{\\partial f}{\\partial x_{i}}=\\frac{\\partial F}{\\partial g} \\frac{\\partial g}{\\partial x_{i}}(1 \\leq i \\leq k), \\frac{\\partial f}{\\partial x_{j}}=\\frac{\\partial F}{\\partial h} \\frac{\\partial h}{\\partial x_{i}}(k+1 \\leq j \\leq n) \\\\\n& \\frac{\\partial f / \\partial x_{i}}{\\partial f / \\partial x_{j}}=\\frac{\\partial F / \\partial g}{\\partial F / \\partial h} \\frac{\\partial g / \\partial x_{i}}{\\partial h / \\partial x_{j}}=\\frac{\\partial g / \\partial x_{i}}{\\partial h / \\partial x_{j}}=g_{x_{i}}\\left(x_{1}, x_{2}, \\cdots x_{k}\\right) \\times \\frac{1}{h_{x_{j}}\\left(x_{k+1}, \\cdots, x_{n}\\right)}\n\\end{aligned}\n$$\n\nwhere we have used $\\frac{\\partial F}{\\partial g}=\\frac{\\partial F}{\\partial h}$. Note that $\\frac{\\partial f / \\partial x_{i}}{\\partial f / \\partial x_{j}}$ is multiplicatively separable, it can be detected by the separability test proposed above. Users can call test_general_separability to check for additive or multiplicative separability. Generalized Symmetry: A function has generalized symmetry (in the first $k$ variables) if\n\n$$\nf\\left(x_{1}, x_{2}, \\cdots, x_{n}\\right)=g\\left(h\\left(x_{1}, \\cdots, x_{k}\\right), x_{k+1}, \\cdots, x_{n}\\right)\n$$\n\nWe denote $\\mathbf{y}=\\left(x_{1}, \\cdots, x_{k}\\right)$ and $\\mathbf{z}=\\left(x_{k+1}, \\cdots, x_{n}\\right)$. This property is called generalized symmetry because $f$ retains the same value as long as $h$ is held constant, regardless of individual values of $x_{1}, \\cdots, x_{k}$. We compute the gradient of $f$ with respect to $\\mathbf{y}: \\nabla_{\\mathbf{y}} f=\\frac{\\partial g}{\\partial h} \\nabla_{\\mathbf{y}} h$. Since $\\frac{\\partial g}{\\partial h}$ is a scalar function, it does not change the direction of $\\nabla_{\\mathbf{y}} h$. Thus, the direction of $\\widehat{\\nabla_{\\mathbf{y}} f} \\equiv \\frac{\\nabla_{\\mathbf{y}} f}{\\left|\\nabla_{\\mathbf{y}} f\\right|}$ is independent of $\\mathbf{z}$, i.e.,\n\n$$\n\\nabla_{\\mathbf{z}}\\left(\\widehat{\\nabla_{\\mathbf{y}} f}\\right)=0\n$$\n\nwhich is the condition for symmetry. Users can call the test_symmetry method to check for symmetries. Tree converter The three types of functional modularity form a hierarchy: symmetry is the most general, general separability is intermediate, and separability is the most specific. Mathematically,\n\n$$\n\\text { Separability } \\subset \\text { Generalized Separability } \\subset \\text { Generalized Symmetry }\n$$\n\nTo obtain the maximal hierarchy of modular structures, we apply generalized symmetry detection recursively, forming groups as small as $k=2$ variables and extending to all $k=n$ variables. For example, let us consider an 8 -variable function\n\n$$\nf\\left(x_{1}, \\cdots, x_{8}\\right)=\\left(\\left(x_{1}^{2}+x_{2}^{2}\\right)^{2}+\\left(x_{3}^{2}+x_{4}^{2}\\right)^{2}\\right)^{2}+\\left(\\left(x_{5}^{2}+x_{6}^{2}\\right)^{2}+\\left(x_{7}^{2}+x_{8}^{2}\\right)^{2}\\right)^{2}\n$$\n\nwhich has four $k=2$ generalized symmetries, involving groups $\\left(x_{1}, x_{2}\\right),\\left(x_{3}, x_{4}\\right),\\left(x_{5}, x_{6}\\right)$, $\\left(x_{7}, x_{8}\\right)$; two $k=2$ generalized symmetries, involving groups $\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)$ and $\\left(x_{5}, x_{6}, x_{7}, x_{8}\\right)$. As such, each $k=4$ group contains two $k=2$ groups, demonstrating a hierarchy. For each generalized symmetry, we can also test if the generalized symmetry is further generalized separable or separable. Users can use the method plot_tree to obtain the tree graph for a function (the function could be any Python expressions, neural networks, etc.). For a neural network model, users can simply call model.tree(). The tree plot can have the style 'tree' (by default) or 'box'. Examples Figure 8 (b) provides two examples. When the exact symbolic functions are input to plot_tree, the ground truth tree graphs are obtained. We are particularly interested in whether the tree converter works for neural networks. For these simple cases, both KANs and MLPs can find the correct graph if sufficiently trained. Figure 8 (b) (bottom) shows the evolution of the tree graphs during KAN and MLP training. It is particularly interesting to see how neural networks gradually learn the correct modular structure. In the first case $f\\left(x_{1}, x_{2}, x_{3}, x_{4}\\right)=\\left(x_{1}^{2}+x_{2}^{2}\\right)^{2}+\\left(x_{3}^{2}+x_{4}^{2}\\right)^{2}$, both KAN and MLP gradually pick up more inductive biases (their intermediate states are different)\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-13.jpg?height=1197&width=1370&top_left_y=258&top_left_x=386)\n\nFigure 9: Three tricks to facilitate symbolic regression. Trick A (top row): detecting and leveraging modular structures. Trick B (middle row): sparse connection initialization. Trick C (bottom row): Hypothesis testing. until they reach the correct structure. In the second case, $f\\left(x_{1}, x_{2}, x_{3}\\right)=\\sin \\left(x_{1}\\right) / \\sqrt{x_{2}^{2}+x_{3}^{2}}$, both the models initially detect multiplicative separability for all three variables, showing even higher symmetry than the correct structure. After training progresses, both models \"realize\" that: in order to better fit data (loss becomes lower), such high symmetry structure can no longer be met and should be relaxed to a less stringent structure. An additional observation is that KAN has an intermediate structure not found in the MLP. There are two caveats we would like to mention: (1) results can be seed and/or threshold-dependent. (2) all tests rely on second-order derivatives, which may not be robust due to the model being trained only on zero-order information. Adversarial constructions such as $f_{\\epsilon}(x)=f(x)+\\epsilon \\sin \\left(\\frac{x}{\\epsilon}\\right)$ could lead to issues, because although $\\left|f_{\\epsilon}(x)-f(x)\\right| \\rightarrow 0$ as $\\epsilon \\rightarrow 0$, $\\left|f_{\\epsilon}^{\\prime \\prime}(x)-f^{\\prime \\prime}(x)\\right| \\rightarrow \\infty$ as $\\epsilon \\rightarrow 0$. Although such extreme cases are unlikely in practice, smoothness is necessary to ensure the success of our methods. ### 4.3 Identifying symbolic formulas from KANs\n\nSymbolic formulas are the most informative, as they clearly reveal both important features and modular structures once they are known. In Liu et al. [57], the authors showed a bunch of examples from which they can extract symbolic formulas, with some prior knowledge when needed. With the new tools proposed above (feature importance, modular structures, and symbolic formulas), users can leverage these new tools to easily interact and collaborate with KANs, making symbolic regression easier. We present three tricks below, illustrated in Figure 9. Trick A: discover and leverage modular structures We can first train a general network and probe its modularity. Once the modular structure is identified, we initialize a new model with this modular structure as inductive biases. For instance, consider the function $f(q, v, B, m)=q v B / m$. We first initialize a large KAN (presumably expressive enough) to fit the dataset to a reasonable accuracy. After training, the tree graph is extracted (ref Sec 4.2) from the trained KAN, which shows multiplicative separability. Then we can build the modular structure into a second KAN (ref Sec 3.2 ), train it, and then symbolify all 1D functions to derive the formula. Trick B: Sparse initialization Symbolic formulas typically correspond to KANs with sparse connections (see Figure 5(b)), so initializing KANs sparsely aligns them better with the inductive biases of symbolic formulas. Otherwise, densely initialized KANs require careful regularization to promote sparsity. Sparse initialization can be achieved by passing the argument \"sparse_init=True\" to the KAN initializer. For example, for the function $f(q, E, v, B, \\theta)=q(E+v B \\sin \\theta)$, a sparsely initialized KAN closely resembles the final trained KAN, requiring only minor adjustments in training. In contrast, a dense initialization would involve extensive training to remove unnecessary edges. Trick C: Hypothesis Testing When faced with multiple reasonable hypotheses, we can try all of them (branching into \"parallel universes\") to test which hypothesis is the most accurate and/or simplest. To facilitate hypothesis testing, we build a checkpoint system that automatically saves model versions whenever changes (e.g., training, pruning) are made. For example, consider the function $f\\left(m_{0}, v, c\\right)=m_{0} / \\sqrt{1-(v / c)^{2}}$. We start from a randomly initialized KAN, which has version 0.0 . After training, it evolves to version 0.1 , where it activates on both $\\beta=v / c$ and $\\gamma=1 / \\sqrt{1-(v / c)^{2}}$. Hypothesize that only $\\beta$ or $\\gamma$ might be needed. We first set the edge on $\\gamma$ to zero, and train the model, obtaining a $6.5 \\times 10^{-4}$ test RMSE (version 0.2 ). To test the alternative hypothesis, we want to revert back to the branching point (version 0.1 ) - we call model.rewind (' 0.1 ') which rewinds the model back to version 0.1 . To indicate that rewind is called, version 0.1 is renamed to version 1.1 . Now we set the edge on $\\beta$ to be zero, train the model, obtaining a $2.0 \\times 10^{-6}$ test RMSE (the version becomes 1.2 ). Comparing versions 0.2 and 1.2 indicates that the second hypothesis is better due to the lower loss given the same complexity (both hypotheses have two non-zero edges). ## 5 Applications\n\nThe previous sections primarily focused on regression problems for pedagogical purposes. In this section, we apply KANs to discover physical concepts, such as conserved quantities, Lagrangians, hidden symmetries, and constitutive laws. These examples illustrate how the tools proposed in this paper can be effectively integrated into real-life scientific research to tackle these complex tasks. ### 5.1 Discovering conserved quantities\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-14.jpg?height=435&width=1343&top_left_y=1638&top_left_x=388)\n\nFigure 10: Using KANs to discover conserved quantities for the 2D harmonic oscillator. Conserved quantities are physical quantities that remain constant over time. For example, a freefalling ball converts its gravitational potential energy into kinetic energy, while the total energy (the sum of both forms of energy) remains constant (assuming negligible air resistance). Conserved quantities are crucial because they often correspond to symmetries in physical systems and can simplify calculations by reducing the dimensionality of the system. Traditionally, deriving conserved quantities with paper and pencil can be time-consuming and demands extensive domain knowledge. Recently, machine learning techniques have been explored to discover conserved quantities [55, 53, 54, 58, 32, 89]. We follow the approach Liu et al. [53], which derived a differential equation that conserved quantities must satisfy, thus transforming the problem of finding conserved quantities into differential equation solving. They used multi-layer perceptrons (MLPs) to parameterize conserved quantities. We basically follow their procedure but replace MLPs with KANs. To be specific, they consider a dynamical system with the state variable $\\mathbf{z} \\in \\mathbb{R}^{d}$ governed by the equation $\\frac{d \\mathbf{z}}{d t}=\\mathbf{f}(\\mathbf{z})$. The necessary and sufficient condition for a function $H(\\mathbf{z})$ to be a conserved quantity is that $\\mathbf{f}(\\mathbf{z}) \\cdot \\nabla H(\\mathbf{z})=0$ for all $\\mathbf{z}$. For example, in a 1D harmonic oscillator, the phase space is characterized by position and momentum, $\\mathbf{z}=(x, p)$, and the evolution equation is $d(x, p) / d t=(p,-x)$. The energy $H=\\frac{1}{2}\\left(x^{2}+p^{2}\\right)$ is a conserved quantity because $\\mathbf{f}(\\mathbf{z}) \\cdot \\nabla H(\\mathbf{z})=(p,-x) \\cdot(x, p)=0$. We parameterize $H$ using a KAN, and train it with the loss function $\\ell=\\sum_{i=1}^{N}\\left|\\mathbf{f}\\left(\\mathbf{z}^{(i)}\\right) \\cdot \\widehat{\\nabla} H\\left(\\mathbf{z}^{(i)}\\right)\\right|^{2}$ where $\\widehat{\\nabla}$ is the normalized gradient, and $\\mathbf{z}^{(i)}$ are the $i^{\\text {th }}$ data point uniformly drawn from the hypercube $[-1,1]^{d}$. We choose the 2 D harmonic oscillator to test KANs, characterized by $\\left(x, y, p_{x}, p_{y}\\right)$. It has three conserved quantities: (1) energy along $x$ direction: $H_{1}=\\frac{1}{2}\\left(x^{2}+p_{x}^{2}\\right)$; (2) energy along $y$ direction: $H_{2}=\\frac{1}{2}\\left(y^{2}+p_{y}^{2}\\right)$; (3) angular momentum $H_{3}=x p_{y}-y p_{x}$. We train $[4,[0,2], 1]$ KANs with three different random seeds, as shown in Figure 10, which correspond to $H_{1}, H_{2}$ and $H_{3}$ respectively. ### 5.2 Discovering Lagrangians\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-15.jpg?height=803&width=1356&top_left_y=1057&top_left_x=382)\n\nFigure 11: Use KANs to learn Lagrangians for the single pendulum (top) and a relativistic mass in a uniform field (bottom). In physics, Lagrangian mechanics is a formulation of classical mechanics based on the principle of stationary action. It describes a mechanical system using phase space and a smooth function $\\mathcal{L}$ known as the Lagrangian. For many systems, $\\mathcal{L}=T-V$, where $T$ and $V$ represent the kinetic and potential energy of the system, respectively. The phase space is typically described by $(\\mathbf{q}, \\dot{\\mathbf{q}})$, where $\\mathbf{q}$ and $\\dot{\\mathbf{q}}$ denotes coordinates and velocities, respectively. The equation of motion can be derived from the Lagrangian via the Euler-Lagrange equation: $\\frac{d}{d t}\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\dot{\\mathbf{q}}}\\right)=\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{q}}$, or equivalently\n\n$$\n\\ddot{\\mathbf{q}}=\\left(\\nabla_{\\dot{\\mathbf{q}}} \\nabla_{\\dot{\\mathbf{q}}}^{T} \\mathcal{L}\\right)^{-1}\\left[\\nabla_{\\mathbf{q}} \\mathcal{L}-\\left(\\nabla_{\\mathbf{q}} \\nabla_{\\dot{\\mathbf{q}}}^{T} \\dot{\\mathbf{q}}\\right)\\right]\n$$\n\nGiven the fundamental role of the Lagrangian, an interesting question is whether we can infer the Lagrangian from data. Following [19], we train a Lagrangian neural network to predict $\\ddot{\\mathbf{q}}$ from ( $\\mathbf{q}, \\dot{\\mathbf{q}})$. An LNN uses an MLP to parameterize $\\mathcal{L}(\\mathbf{q}, \\dot{\\mathbf{q}})$, and computes the Eq. 18 to predict instant accelerations $\\ddot{\\mathbf{q}}$. However, LNNs face two main challenges: (1) The training of LNNs can be unstable\ndue to the second-order derivatives and matrix inversion in Eq. (18). (2) LNNs lack interpretability because MLPs themselves are not easily interpretable. We address these issues using KANs. To tackle the first challenge, we note that the matrix inversion of the Hessian $\\left(\\nabla_{\\dot{\\mathbf{q}}} \\nabla_{\\dot{\\mathbf{q}}}^{T} \\mathcal{L}\\right)^{-1}$ becomes problematic when the Hessian has eigenvalues close to zero. To mitigate this, we initialize $\\left(\\nabla_{\\dot{\\mathbf{q}}} \\nabla_{\\dot{\\mathbf{q}}}^{T} \\mathcal{L}\\right)$ as a positive definite matrix (or a positive number in 1D). Since $\\left(\\nabla_{\\dot{\\mathbf{q}}} \\nabla_{\\dot{\\mathbf{q}}}^{T} \\mathcal{L}\\right)$ is the mass $m$ in classical mechanics and kinetic energy is usually $T=\\frac{1}{2} m \\dot{\\mathbf{q}}^{2}$, encoding this prior knowledge into KANs is more straightforward than into MLPs (using the kanpiler introduced in Section 3.3). The kanpiler can convert the symbolic formula $T$ into a KAN (as shown in Figure 11). We use this converted KAN for initialization and continue training, resulting in much greater stability compared to random initialization. After training, symbolic regression can be applied to each edge to extract out symbolic formulas, addressing the second challenge. We show two 1D examples in Figure 11, a single pendulum and a relativistic mass in a uniform field. The compiled KANs are displayed on the left, with edges on $\\dot{q}$ displaying quadratic functions and edges on $q$ as zero functions. Single pendulum The $\\dot{q}$ part remains a quadratic function $T(\\dot{q})=\\frac{1}{2} \\dot{q}^{2}$ while the $q$ part learns to be a cosine function, as $V(q)=1-\\cos (q)$. In Figure 11 top, the results from suggest_symbolic display the top five functions that best match the splines, considering both fitness and simplicity. As expected, the cosine and the quadratic function appear at the top of the lists. Relativistic mass in a uniform field After training, the kinetic energy part deviates from $T=$ $\\frac{1}{2} \\dot{q}^{2}$ because, for a relativistic particle, $T_{r}=\\left(1-\\dot{q}^{2}\\right)^{-1 / 2}-1$. In Figure 11 (bottom), symbolic regression successfully finds $V(q)=q$, but fails to identify $T_{r}$ due to its compositional nature, as our symbolic regression only searches for simple functions. By assuming the first function composition is quadratic, we create another $[1,1,1] \\mathrm{KAN}$ to fit $T_{r}$ and set the first function to be the quadratic function using fix_symbolic, and train only the second learnable function. After training, we see that the ground truth $x^{-1 / 2}$ appears among the top five candidates. However, $x^{1 / 2}$ fits the spline slightly better, as indicated by a higher R-squared value. This suggests that symbolic regression is sensitive to noise (due to imperfect learning) and prior knowledge is crucial for correct judgment. For instance, knowing that kinetic energy should diverge as velocity approaches the speed of light helps confirm $x^{-1 / 2}$ as the correct term, since $x^{1 / 2}$ does not exhibit the expected divergence. ### 5.3 Discovering hidden symmetry\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-16.jpg?height=310&width=1360&top_left_y=1556&top_left_x=380)\n\nFigure 12: Rediscovering the hidden symmetry of the Schwarzschild black hole with MLPs and KANs. (a) $\\Delta t(r)$ learned by the MLP is a globally smooth solution; (b) $\\Delta t(r)$ learned by the KAN is a domain-wall solution; (c) The KAN shows a loss spike at the domain wall; (d) A KAN can be used to fine-tune the MLP solution close to machine precision. Philip Anderson famously argued that \"it is only slightly overstating that case to say that physics is the study of symmetry\", emphasizing how the discovery of symmetries has been invaluable for both deepening our understanding and solving problems more efficiently. However, symmetries are sometimes not manifest but hidden, only revealed by applying some coordinate transformation. For example, after Schwarzschild discovered his eponymous black hole metric, it took 17 years for Painlev\u00e9, Gullstrand and Lema\u00eetre to uncover its hidden translational symmetry. They demonstrated that the spatial sections could be made translationally invariant with a clever coordinate transformation, thereby deepening our understanding of black holes [65]. Liu \\& Tegmark [56] showed that the Gullstrand-Painlev\u00e9 transformation can be discovered by training an MLP in minutes. However, they did not get extremely high precision (i.e., machine precision) for the solution. We attempt to revisit this problem using KANs. Suppose there is a Schwarzschild black hole in spacetime $(t, x, y, z)$ with mass $2 M=1$, centered at $x=y=z=0$ with a radius $r_{s}=2 M=1$. The Schwarzschild metric describes how space and time distorts around it:\n\n$$\n\\mathbf{g}_{\\mu \\nu}=\\left(\\begin{array}{cccc}\n1-\\frac{2 M}{r} & 0 & 0 & 0 \\\\\n0 & -1-\\frac{2 M x^{2}}{(r-2 M) r^{2}} & -\\frac{2 M x y}{(r-2 M) r^{2}} & -\\frac{2 M x z}{(r-2 M) r^{2}} \\\\\n0 & -\\frac{2 M x y}{(r-2 M) r^{2}} & -1-\\frac{2 M y^{2}}{(r-2 M) r^{2}} & -\\frac{2 M y z}{(r-2 M) r^{2}} \\\\\n0 & -\\frac{2 M x z}{(r-2 M) r^{2}} & -\\frac{2 M y z}{(r-2 M) r^{2}} & -1-\\frac{2 M z^{2}}{(r-2 M) r^{2}} . \\end{array}\\right)\n$$\n\nApplying the Gullstrand-Painlev\u00e9 transformation $t^{\\prime}=t+2 M\\left(2 u+\\ln \\left(\\frac{u-1}{u+1}\\right)\\right), u \\equiv \\sqrt{\\frac{r}{2 M}}, x^{\\prime}=x$, $y^{\\prime}=y, z^{\\prime}=z$, the metric in the new coordinates becomes:\n\n$$\n\\mathbf{g}_{\\mu \\nu}^{\\prime}=\\left(\\begin{array}{cccc}\n1-\\frac{2 M}{r} & -\\sqrt{\\frac{2 M}{r}} \\frac{x}{r} & -\\sqrt{\\frac{2 M}{r}} \\frac{y}{r} & -\\sqrt{\\frac{2 M}{r}} \\frac{z}{r} \\\\\n-\\sqrt{\\frac{2 M}{r}} \\frac{x}{r} & -1 & 0 & 0 \\\\\n-\\sqrt{\\frac{2 M}{r}} \\frac{y}{r} & 0 & -1 & 0 \\\\\n-\\sqrt{\\frac{2 M}{r}} \\frac{z}{r} & 0 & 0 & -1\n\\end{array}\\right)\n$$\n\nwhich exhibits translation invariance in the spatial section (the lower right $3 \\times 3$ block is the Euclidean metric). Liu \\& Tegmark [56] used an MLP to learn the mapping from $(t, x, y, z)$ to $\\left(t^{\\prime}, x^{\\prime}, y^{\\prime}, z^{\\prime}\\right)$. Defining the Jacobian matrix $\\mathbf{J} \\equiv \\frac{\\partial\\left(t^{\\prime}, x^{\\prime}, y^{\\prime}, z^{\\prime}\\right)}{\\partial(t, x, y, z)}, \\mathbf{g}$ is tranformed to $\\mathbf{g}^{\\prime}=\\mathbf{J}^{-T} \\mathbf{g} \\mathbf{J}^{-1}$. We take the bottom right $3 \\times 3$ block of $\\mathbf{g}^{\\prime}$ and take its difference to the Euclidean metric to obtain the MSE loss. The loss is minimized by doing gradient descents on the MLP. To make things simple, they assume knowing $x^{\\prime}=x, y^{\\prime}=y, z^{\\prime}=z$, and only use an MLP ( 1 input and 1 output) to predict the temporal difference $\\Delta t(r)=t^{\\prime}-t=2 M\\left(2 u+\\ln \\left(\\frac{u-1}{u+1}\\right)\\right), u \\equiv \\sqrt{\\frac{r}{2 M}}$ from the radius $r$. MLP and KAN find different solutions We trained both an MLP and a KAN to minimize this loss function, with results shown in Figure 12 Since the task has 1 input dimension and 1 output dimension, the KAN effectively reduces to a spline. We originally expected KANs to outperform MLPs, because splines are known to be superior in low-dimensional settings [63]. However, while MLP can achieve $10^{-8}$ loss, the KAN gets stuck at $10^{-3}$ loss despite grid refinements. It turned out that KAN and MLP learned two different solutions: while the MLP found a globally smooth solution (Figure 12(a)), the KAN learned a domain-wall solution (Figure 12(b)). The domain wall solution has a singular point that separates the whole curve into two segments. The left segment learns $\\Delta t(r)$ correctly, while the right segment learns $-\\Delta t(r)$, which is also a valid solution but differs from the left segment by a minus sign. There is a loss spike appearing at the singular point (Figure 12 (c)). One might consider this as a feature of KANs because domain wall solutions are prevalent in nature. However, if one considers this a flaw, KANs can still obtain globally smooth solutions by adding regularizations (to reduce spline oscillations) or experimenting with different random seeds (roughly 1 out of 3 random seeds finds a global smooth solution). KANs can achieve extreme precision Although the MLP finds the globally smooth solution and achieves $10^{-8}$ loss, the loss is still far from machine precision. We found that neither longer training nor increasing the MLP's size significantly reduced the loss. Therefore, we turned to KANs, which, as splines in 1D, can achieve arbitrary accuracy by refining the grid (given infinite data). We first used the MLP as a teacher, generating supervised pairs $(x, y)$ to train the KAN to fit the supervised data. This way, the KAN is initialized to a globally smooth solution. We then iteratively refined the KAN by increasing the number of grid intervals to 1000. In the end, the fine-tuned KANs achieve a loss of $10^{-15}$, close to machine precision (Figure 12 (d)). ### 5.4 Learning constitutive laws\n\nA constitutive law defines the behavior and properties of a material by modeling how it responds to external forces or deformations. One of the simplest forms of constitutive law is Hooke's Law [34], which relates the strain and stress of elastic materials linearly. Constitutive laws encompass a wide range of materials, including elastic materials [80, 68], plastic materials [64], and fluids [8]. Traditionally, these laws were derived from first principles based on theoretical and experimental studies [79, 81, 6, 29]. Recent advancements, however, have introduced data-driven approaches that leverage machine learning to discover and refine these laws from dedicated datasets [73, 91, 59, 60]. ![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-18.jpg?height=1646&width=1354&top_left_y=481&top_left_x=380)\n\nFigure 13: Discovering constitutive laws (relations between the pressure tensor $P$ and the strain tensor $F$ ) with KANs by interacting with them. Top: predicting the diagonal element $P_{11}$; bottom: predicting the off-diagonal element $P_{12}$. We follow the standard notations and experimental setups in the elasticity part of NCLaw [59] and define the constitutive law as a parameterized function $\\mathcal{E}_{\\theta}(\\mathbf{F}) \\rightarrow \\mathbf{P}$, where $\\mathbf{F}$ denotes the deformation tensor, $\\mathbf{P}$ the first Piola-Kirchhoff stress tensor, and $\\theta$ the parameters in the constitutive law. Many isotropic materials have linear constitutive laws when deformation is small:\n\n$$\n\\mathbf{P}_{l}=\\mu\\left(\\mathbf{F}+\\mathbf{F}^{T}-2 \\mathbf{I}\\right)+\\lambda(\\operatorname{Tr}(\\mathbf{F})-3) \\mathbf{I}\n$$\n\nHowever, when deformation gets larger, nonlinear effects start to kick in. For example, a NeoHookean material has the following constitutive law:\n\n$$\n\\mathbf{P}=\\mu\\left(\\mathbf{F F}^{T}-\\mathbf{I}\\right)+\\lambda \\log (\\operatorname{det}(\\mathbf{F})) \\mathbf{I}\n$$\n\nwhere $\\mu$ and $\\lambda$ are the so-called Lam\u00e9 parameters determined by the so-called Young's modulus $Y$ and Poisson ratio $\\nu$ as $\\mu=\\frac{Y}{2(1+\\nu)}, \\lambda=\\frac{Y \\nu}{(1+\\nu)(1-2 \\nu)}$. For simplicity, we choose $Y=1$ and $\\nu=0.2$, hence $\\mu=\\frac{5}{12} \\approx 0.42$ and $\\lambda=\\frac{5}{18} \\approx 0.28$. Assuming we are working with Neo-Hookean materials, and our goal is to use KANs to predict the $\\mathbf{P}$ tensor from the $\\mathbf{F}$ tensor. Suppose we do not know they are neo-Hookean materials, but we have the prior knowledge that the linear constitutive law is approximately valid for small deformation. Due to symmetries, it suffices to demonstrate that we can accurately predict $P_{11}$ and $P_{12}$ from the 9 matrix elements of $\\mathbf{F}$. We want to compile linear constitutive laws into KANs, which are $P_{11}=2 \\mu\\left(F_{11}-1\\right)+\\lambda\\left(F_{11}+F_{22}+F_{33}-3\\right)$, and $P_{12}=\\mu\\left(F_{12}+F_{21}\\right)$. We want to extract Neo-Hookean laws from trained KANs, which are $P_{11}=\\mu\\left(F_{11}^{2}+F_{12}^{2}+F_{13}^{2}-1\\right)+\\lambda \\log (\\operatorname{det}(\\mathbf{F}))$, and $P_{12}=\\mu\\left(F_{11} F_{21}+F_{12} F_{22}+F_{13} F_{23}\\right)$. We generate a synthetic dataset by sampling $F_{i j}$ independently from $U\\left[\\delta_{i j}-w, \\delta_{i j}+w\\right](w=0.2)$ and using the Neo-Hookean constitutive law to compute $\\mathbf{P}$. Our interaction with KANs is illustrated in Figure 13. In both cases, we successfully figured out the true symbolic formulas in the end, with the aid of some inductive biases. However, the key takeaway is not that we can rediscover the exact symbolic formulas - given that prior knowledge skews the process - but rather in real-world scenarios, where the answers are unknown and users can make guesses based on prior knowledge, the pykan package makes it easy to test or incorporate prior knowledge. Predicting $P_{11}$ In step 1, we compile the linear constitutive law $P_{11}=2 \\mu\\left(F_{11}-1\\right)+\\lambda\\left(F_{11}+\\right.$ $F_{22}+F_{33}-3$ ) to a KAN using kanpiler, resulting in a $10^{-2}$ loss. In step 2, we perturb the KAN so that it becomes trainable (indicated by the color change from red to purple; red denotes a purely symbolic part, while purple indicates that both symbolic and spline parts are active). In step 3, we train the perturbed model until convergence, giving a $6 \\times 10^{-3}$ loss. In step 4 , assuming that the determinant is a key auxiliary variable, we use expand_width (for the KAN) and augment_input (for the dataset) to include the determinant $|F|$. In step 5, we train the KAN until convergence, giving a $2 \\times 10^{-4}$ loss. In step 6 , we symbolify the KAN to obtain a symbolic formula $P_{11}=$ $0.42\\left(F_{11}^{2}+F_{12}^{2}+F_{13}^{2}-1\\right)+0.28 \\log (|F|)$, which achieves a $3 \\times 10^{-11}$ loss. Predicting $P_{12}$ We experimented with and without encoding the linear constitutive law as prior knowledge. With prior knowledge: in step 1, we compile the linear constitutive law to a KAN, resulting in a loss of $10^{-2}$. We then perform a series of operations, including expand (step 2), perturb (step 3 ), train (step 4), prune (step 5) and finally symbolic (step 6). The influence of prior knowledge is evident, as the final KAN only identifies minor correction terms to the linear constitutive law. The final KAN is symbolified as $P_{12}=0.42\\left(F_{12}+F_{21}\\right)+0.44 F_{13} F_{23}-0.03 F_{21}^{2}+0.02 F_{12}^{2}$ which yields a $7 \\times 10^{-3}$ loss, only slightly better than the linear constitutive law. Without prior knowledge: in step 1, we randomly initialize the KAN model. In step 2, we train the KAN with regularization. In step 3, we prune the KAN to be a more compact model. In step 4, we symbolify the KAN, yielding $P_{12}=0.42\\left(F_{11} F_{21}+F_{12} F_{22}+F_{13} F_{23}\\right)$, which closely matches the exact formula, achieving a $6 \\times 10^{-9}$ loss. Comparing the two scenarios - one with and one without prior knowledge - reveals a surprising outcome: in this example, prior knowledge appears harmful, possibly because the linear constitutive law is probably near a (bad) local minimum which is hard for the model to escape. However, we should probably not randomly extrapolate this conclusion to more complicated tasks and larger networks. For more complicated tasks, finding a local minimum via gradient descent might be challenging enough, making an approximate initial solution desirable. Additionally, larger networks might be sufficiently over-parameterized to eliminate bad local minima, ensuring that all local minima are global and interconnected. ![](https://cdn.mathpix.com/cropped/2024_09_12_2c24fd935e9b6ec70a25g-20.jpg?height=487&width=1363&top_left_y=254&top_left_x=380)\n\nFigure 14: KAN interpolates between software 1.0 and 2.0. (a) KANs strike a balance between interpretability (software 1.0) and learnability (software 2.0). (b) KANs' Pareto frontier on the interpretability-scale plane. The amount of interpretation we can get from KANs depends on problem scales and interpretability methods. ## 6 Related works\n\nKolmogorov-Arnold Networks (KANs), inspired by the Kolmogorov-Arnold representation theorem (KART), were recently proposed by Liu et al. [57]. Although the connection between KART and networks has long been deemed irrelevant [30], Liu et al. generalized the original two-layer network to arbitrary depths and demonstrated their promise for science-oriented tasks given their accuracy and interpretability. Subsequent research has explored the application of KANs across various domains, including graphs [12, 22, 38, 99], partial differential equations [87, 78] and operator learning [1] 78, 67], tabular data [70], time series [85, 28, 93, 27], human activity recognition [49, 50], neuroscience [96, 33], quantum science [40, 46, 4], computer vision [17, 7, 44, 16, 76, 10], kernel learning [101], nuclear physics [48], electrical engineering [69], biology [71]. Liu et al. used B-splines to parameterize 1D functions, and other research have explored various activation functions, including wavelet [11, 76], radial basis function [47], Fourier series [92]), finite basis [35, 82], Jacobi basis functions [2], polynomial basis functions [75], rational functions [3]. Other techniques for KANs have also been proposed including regularization [5], Kansformer (combining transformer and KAN) [15], adaptive grid update [72], federated learning [98], Convolutional KANs [10]. There have been ongoing debates regarding whether KANs really outperform other neural networks (especially MLPs) on various domains [7, 16, 42, 77, 97], which suggests that while KANs show promise for machine learning tasks, further development is needed to surpass state-of-the-art models. Machine Learning for Physical Laws A major goal for KANs is to aid in the discovery of new physical laws from data. Previous research has shown that machine learning can be used to learn various types of physical laws, including equations of motion [90, 13, 43, 20], conservation laws [55, 53, 54, 58, 32, 89], symmetries [39, 56, 94], phase transitions [88, 14], Lagrangian and Hamiltonian [19, 31], and symbolic regression [18, 61, 23, 74], etc. However, making neural networks interpretable often requires domain-specific knowledge, limiting their generality. We hope that KANs will evolve into universal foundation models for physical discoveries. Mechanistic Interpretability seeks to understand how neural networks operate in a fundamental level [21, 62, 86, 25, 66, 100, 51, 24, 45, 26]. Some research in this area focuses on designing models that are inherently interpretable [24] or proposing training methods that explicitly promote interpretability [51]. KANs fall into this category since the Kolmogorov-Arnold theorem decomposes a high-dimensional function into a collection of 1D functions, which are significantly easier to interpret than high-dimensional functions. ## 7 Discussion\n\nKAN interpolates between software 1.0 and 2.0 The key difference between Kolmogorov-Arnold Networks (KANs) and other neural networks (software 2.0, a term coined by Andrej Karpathy) lies in their greater interpretability, which allows for manipulation by users, similar to traditional software (software 1.0). However, KANs are not entirely traditional software, as they (1) learnability (good), enabling them to learn new things from data, and (2) reduced interpretability (bad) as they\nbecome less interpretable and controllable as the network scales increase. In Figure 14 (a) visualizes the position of software 1.0 , software 2.0 , and KANs on the interpretability-learnability plane, illustrating how KANs can balance the trade-offs between these two paradigms. The goal of this paper is to propose various tools that make KANs more like software 1.0, while leveraging the learnability of software 2.0. Efficiency improvement The original pykan package [57] was poor in efficiency. We have incorporated a few techniques to improve its efficiency. 1. Efficient splines evaluations. Inspired by Efficient KAN [9], we have optimized spline evaluations by avoiding unnecessary input expansions. For a KAN with $L$ layers, $N$ neurons per layer, and grid size $G$, memory usage has been reduced from $O\\left(L N^{2} G\\right)$ to $O(L N G)$.",
    "kangpt-3": "2. Enabling the symbolic branch only when needed. A KAN layer contains both a spline branch and a symbolic branch. The symbolic branch is much more time-consuming than the spline branch since it cannot be parallelized (disastrous double loops are needed). However, in many applications, the symbolic branch is unnecessary, so we can skip it when possible, significantly reducing runtime, especially when the network is large. 3. Saving intermediate activations only when needed. To plot KAN diagrams, intermediate activations must be saved. Initially, activations were saved by default, leading to slower runtime and excessive memory usage. We now save intermediate activations only when needed (e.g., for plotting or applying regularizations in training). Users can enable these efficiency improvements with a single line: model.",
    "kangpt-4": "speed(). 4. GPU acceleration. Initially, all models were run on CPUs due to the small-scale nature of the problems. We have now made the model GPU-compatible ${ }^{6}$. For example, training a [4,100,100,100,1] with Adam for 100 steps used to take an entire day on a CPU (before implementing $1,2,3$ ), but now takes 20 seconds on a CPU and less than one second on a GPU. However, KANs still lag behind MLPs in efficiency, especially at large scales. The community has been working towards benchmarking and improving KAN's efficiency and the efficiency gap has been significantly reduced [36]. Since the objective of this paper is to make KANs more like software 1.0, when facing trade-offs between 1.0 (being interactive and versatile) and 2.0 (being efficient and specific), we prioritize interactivity and versatility over efficiency. For example, we store cached data within models (which consumes additional memory), so users can simply call model. plot () to generate a KAN diagram without manually doing a forward pass to collect data. Interpretability Although the learnable univariate functions in KANs are more interpretable than weight matrices in MLPs, scalability remains a challenge. As KAN models scale up, even if all spline functions are interpretable individually, it becomes increasingly difficult to manage the combined output of these 1D functions. Consequently, a KAN may only remain interpretable when the network scale is relatively small (Figure 14 (b), thick red line). It is important to note that interpretability depends on both intrinsic factors (related to the model itself) and extrinsic factors (related to interpretability methods). Advanced interpretability methods should be able to handle interpretability at various levels. For example, by interpreting KANs with symbolic regression, modularity discovery and feature attribution (Figure 14 (b), thin red lines), the Pareto Frontier of interpretability versus scale extends beyond what a KAN alone can achieve. A promising direction for future research is to develop more advanced interpretability methods that can further push the current Pareto Frontiers. Future work This paper introduces a framework that integrates KANs with scientific knowledge, focusing primarily on small-scale, physics-related examples. Moving forward, two promising directions include applying this framework to larger-scale problems and extending it to other scientific disciplines beyond physics. ## Acknowledgement\n\nWe would like to thank Yizhou Liu, Di Luo, Akash Kundu and many GitHub users for fruitful discussion and constructive suggestions. We extend special thanks to GitHub user Blealtan for\n\n[^5]making public their awesome work on making KANs efficient. Z.L. and M.T. are supported by IAIFI through NSF grant PHY-2019786. ## References\n\n[1] D. W. Abueidda, P. Pantidis, and M.",
    "kangpt-5": "E. Mobasher. Deepokan: Deep operator network based on kolmogorov arnold networks for mechanics problems.",
    "kangpt-6": "arXiv preprint arXiv:2405.19143, 2024. [2] A. A. Aghaei. fkan: Fractional kolmogorov-arnold networks with trainable jacobi basis functions.",
    "kangpt-7": "arXiv preprint arXiv:2406.07456, 2024. [3] A. A. Aghaei. rkan: Rational kolmogorov-arnold networks.",
    "kangpt-8": "arXiv preprint arXiv:2406.14495, 2024. [4] T. Ahmed and M. H. R. Sifat. Graphkan: Graph kolmogorov arnold network for small molecule-protein interaction predictions. In ICML'24 Workshop ML for Life and Material Science: From Theory to Industry Applications, 2024.",
    "kangpt-9": "[5] M. G. Altarabichi. Dropkan: Regularizing kans by masking post-activations.",
    "kangpt-10": "arXiv preprint arXiv:2407.13044, 2024. [6] E. M. Arruda and M. C. Boyce. A three-dimensional constitutive model for the large stretch behavior of rubber elastic materials. Journal of the Mechanics and Physics of Solids, 41(2):389-412, 1993. [7] B. Azam and N. Akhtar. Suitability of kans for computer vision: A preliminary investigation. arXiv preprint arXiv:2406.09087, 2024.",
    "kangpt-11": "[8] G. K. Batchelor. An introduction to fluid dynamics. Cambridge university press, 2000. [9] Blealtan. Blealtan/efficient-kan: An efficient pure-pytorch implementation of kolmogorovarnold network (kan).",
    "kangpt-12": "[10] A. D. Bodner, A. S. Tepsich, J. N. Spolski, and S. Pourteau. Convolutional kolmogorovarnold networks. arXiv preprint arXiv:2406.13155, 2024. [11] Z. Bozorgasl and H. Chen. Wav-kan: Wavelet kolmogorov-arnold networks. arXiv preprint arXiv:2405.12832, 2024. [12] R. Bresson, G. Nikolentzos, G. Panagopoulos, M. Chatzianastasis, J. Pang, and M. Vazirgiannis. Kagnns: Kolmogorov-arnold networks meet graph learning. arXiv preprint arXiv:2406.18380, 2024.",
    "kangpt-13": "[13] S. L. Brunton, J. L. Proctor, and J.",
    "kangpt-14": "N. Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15):3932-3937, 2016. [14] J. Carrasquilla and R. G. Melko. Machine learning phases of matter. Nature Physics, 13(5):431-434, 2017. [15] Y. Chen, Z. Zhu, S. Zhu, L. Qiu, B. Zou, F. Jia, Y. Zhu, C. Zhang, Z.",
    "kangpt-15": "Fang, F. Qin, et al. Sckansformer: Fine-grained classification of bone marrow cells via kansformer backbone and hierarchical attention mechanisms.",
    "kangpt-16": "arXiv preprint arXiv:2406.09931, 2024.",
    "kangpt-17": "[16] M. Cheon. Demonstrating the efficacy of kolmogorov-arnold networks in vision tasks. arXiv preprint arXiv:2406.14916, 2024.",
    "kangpt-18": "[17] M. Cheon. Kolmogorov-arnold network for satellite image classification in remote sensing.",
    "kangpt-19": "arXiv preprint arXiv:2406.00600, 2024.",
    "kangpt-20": "[18] M. Cranmer. Interpretable machine learning for science with pysr and symbolicregression. jl. arXiv preprint arXiv:2305.01582, 2023. [19] M. Cranmer, S. Greydanus, S. Hoyer, P. Battaglia, D. Spergel, and S. Ho. Lagrangian neural networks. arXiv preprint arXiv:2003.04630, 2020. [20] M. Cranmer, A. Sanchez Gonzalez, P. Battaglia, R. Xu, K. Cranmer, D. Spergel, and S. Ho. Discovering symbolic models from deep learning with inductive biases. Advances in neural information processing systems, 33:17429-17442, 2020. [21] H. Cunningham, A. Ewart, L. Riggs, R. Huben, and L. Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. [22] G. De Carlo, A. Mastropietro, and A. Anagnostopoulos. Kolmogorov-arnold graph neural networks. arXiv preprint arXiv:2406.18354, 2024. [23] O. Dugan, R. Dangovski, A. Costa, S. Kim, P. Goyal, J. Jacobson, and M. Solja\u010di\u0107. Occamnet: A fast neural model for symbolic regression at scale. arXiv preprint arXiv:2007.10784, 2020. [24] N. Elhage, T. Hume, C. Olsson, N. Nanda, T. Henighan, S. Johnston, S. ElShowk, N. Joseph, N. DasSarma, B. Mann, D. Hernandez, A. Askell, K. Ndousse, A. Jones, D. Drain, A. Chen, Y. Bai, D. Ganguli, L. Lovitt, Z. Hatfield-Dodds, J. Kernion, T. Conerly, S. Kravec, S. Fort, S. Kadavath, J. Jacobson, E. Tran-Johnson, J. Kaplan, J. Clark, T. Brown, S. McCandlish, D. Amodei, and C. Olah. Softmax linear units. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/solu/index.html. [25] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby, D. Drain, C. Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022. [26] J. Engels, I. Liao, E. J. Michaud, W. Gurnee, and M. Tegmark. Not all language model features are linear. arXiv preprint arXiv:2405.14860, 2024. [27] R. Genet and H. Inzirillo. A temporal kolmogorov-arnold transformer for time series forecasting. arXiv preprint arXiv:2406.02486, 2024. [28] R. Genet and H. Inzirillo. Tkan: Temporal kolmogorov-arnold networks. arXiv preprint arXiv:2405.07344, 2024.",
    "kangpt-21": "[29] A. N. Gent. A new constitutive relation for rubber. Rubber chemistry and technology, $69(1): 59-61,1996$.",
    "kangpt-22": "[30] F. Girosi and T. Poggio. Representation properties of networks: Kolmogorov's theorem is irrelevant. Neural Computation, 1(4):465-469, 1989. [31] S. Greydanus, M. Dzamba, and J. Yosinski. Hamiltonian neural networks. Advances in neural information processing systems, 32, 2019. [32] S. Ha and H. Jeong. Discovering conservation laws from trajectories via machine learning. arXiv preprint arXiv:2102.04008, 2021.",
    "kangpt-23": "[33] L. F. Herbozo Contreras, J. Cui, L. Yu, Z. Huang, A. Nikpour, and O. Kavehei. Kan-eeg: Towards replacing backbone-mlp for an effective seizure detection system.",
    "kangpt-24": "medRxiv, pages 2024-06, 2024. [34] R. Hooke. Lectures de potentia restitutiva, or of spring explaining the power of springing bodies.",
    "kangpt-25": "Number 6. John Martyn, 2016. [35] A. A. Howard, B. Jacob, S. H. Murphy, A. Heinlein, and P. Stinis. Finite basis kolmogorovarnold networks: domain decomposition for data-driven and physics-informed problems. arXiv preprint arXiv:2406.19662, 2024. [36] Jerry-Master. Jerry-master/kan-benchmarking. [37] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. \u017d\u00eddek, A. Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583-589, 2021. [38] M. Kiamari, M. Kiamari, and B. Krishnamachari. Gkan: Graph kolmogorov-arnold networks.",
    "kangpt-26": "arXiv preprint arXiv:2406.06470, 2024. [39] S. Krippendorf and M. Syvaeri. Detecting symmetries with neural networks. Machine Learning: Science and Technology, 2(1):015010, 2020. [40] A. Kundu, A. Sarkar, and A. Sadhu. Kanqas: Kolmogorov arnold network for quantum architecture search. arXiv preprint arXiv:2406.17630, 2024. [41] R. Lam, A. Sanchez-Gonzalez, M. Willson, P. Wirnsberger, M. Fortunato, F. Alet, S. Ravuri, T. Ewalds, Z. Eaton-Rosen, W. Hu, et al. Learning skillful medium-range global weather forecasting. Science, 382(6677):1416-1421, 2023.",
    "kangpt-27": "[42] T. X. H. Le, T. D. Tran, H. L. Pham, V. T. D. Le, T. H. Vu, V. T. Nguyen, Y. Nakashima, et al. Exploring the limitations of kolmogorov-arnold networks in classification: Insights to software training and hardware implementation. arXiv preprint arXiv:2407.17790, 2024. [43] P. Lemos, N. Jeffrey, M. Cranmer, S. Ho, and P. Battaglia. Rediscovering orbital mechanics with machine learning. Machine Learning: Science and Technology, 4(4):045002, 2023. [44] C. Li, X. Liu, W. Li, C. Wang, H. Liu, and Y. Yuan. U-kan makes strong backbone for medical image segmentation and generation.",
    "kangpt-28": "arXiv preprint arXiv:2406.02918, 2024. [45] K. Li, A. K. Hopkins, D. Bau, F. Vi\u00e9gas, H. Pfister, and M. Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. In The Eleventh International Conference on Learning Representations, 2023. [46] X. Li, Z. Feng, Y. Chen, W. Dai, Z. He, Y. Zhou, and S. Jiao. Coeff-kans: A paradigm to address the electrolyte field with kans.",
    "kangpt-29": "arXiv preprint arXiv:2407.20265, 2024. [47] Z. Li. Kolmogorov-arnold networks are radial basis function networks.",
    "kangpt-30": "arXiv preprint arXiv:2405.06721, 2024. [48] H. Liu, J. Lei, and Z. Ren. From complexity to clarity: Kolmogorov-arnold networks in nuclear binding energy prediction, 2024.",
    "kangpt-31": "[49] M. Liu, S. Bian, B. Zhou, and P. Lukowicz. ikan: Global incremental learning with kan for human activity recognition across heterogeneous datasets.",
    "kangpt-32": "arXiv preprint arXiv:2406.01646, 2024. [50] M. Liu, D. Gei\u00dfler, D. Nshimyimana, S. Bian, B. Zhou, and P. Lukowicz. Initial investigation of kolmogorov-arnold networks (kans) as feature extractors for imu based human activity recognition.",
    "kangpt-33": "arXiv preprint arXiv:2406.11914, 2024. [51] Z. Liu, E. Gan, and M. Tegmark. Seeing is believing: Brain-inspired modular training for mechanistic interpretability.",
    "kangpt-34": "Entropy, 26(1):41, 2023. [52] Z. Liu, M. Khona, I. R. Fiete, and M. Tegmark. Growing brains: Co-emergence of anatomical and functional modularity in recurrent neural networks. arXiv preprint arXiv:2310.07711, 2023. [53] Z. Liu, V. Madhavan, and M. Tegmark. Machine learning conservation laws from differential equations. Physical Review E, 106(4):045307, 2022. [54] Z. Liu, P. O. Sturm, S. Bharadwaj, S.",
    "kangpt-35": "J. Silva, and M. Tegmark. Interpretable conservation laws as sparse invariants.",
    "kangpt-36": "Phys. Rev. E, 109:L023301, Feb 2024. [55] Z. Liu and M. Tegmark. Machine learning conservation laws from trajectories.",
    "kangpt-37": "Phys. Rev. Lett., 126:180604, May 2021. [56] Z. Liu and M. Tegmark. Machine learning hidden symmetries. Physical Review Letters, 128(18):180201, 2022. [57] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Solja\u010di\u0107, T.",
    "kangpt-38": "Y. Hou, and M. Tegmark. Kan: Kolmogorov-arnold networks. arXiv preprint arXiv:2404.19756, 2024. [58] P. Y. Lu, R. Dangovski, and M. Solja\u010di\u0107. Discovering conservation laws using optimal transport and manifold learning. Nature Communications, 14(1):4744, 2023. [59] P. Ma, P. Y. Chen, B. Deng, J. B. Tenenbaum, T. Du, C. Gan, and W. Matusik. Learning neural constitutive laws from motion observations for generalizable pde dynamics. In International Conference on Machine Learning, pages 23279-23300. PMLR, 2023. [60] P. Ma, T.-H. Wang, M. Guo, Z. Sun, J. B. Tenenbaum, D. Rus, C. Gan, and W. Matusik. Llm and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery. In Forty-first International Conference on Machine Learning, 2024. [61] G. Martius and C. H. Lampert. Extrapolation and learning equations. arXiv preprint arXiv:1610.02995, 2016. [62] K. Meng, D. Bau, A. J. Andonian, and Y. Belinkov. Locating and editing factual associations in GPT.",
    "kangpt-39": "In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing Systems, 2022.",
    "kangpt-40": "[63] E. J. Michaud, Z. Liu, and M. Tegmark. Precision machine learning. Entropy, 25(1):175, 2023.",
    "kangpt-41": "[64] R.",
    "kangpt-42": "v. Mises. Mechanik der festen k\u00f6rper im plastisch-deformablen zustand. Nachrichten von der Gesellschaft der Wissenschaften zu G\u00f6ttingen, Mathematisch-Physikalische Klasse, 1913:582-592, 1913. [65] C. Misner, K. Thorne, and J. Wheeler. Gravitation. Princeton University Press, 2017. [66] N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations, 2023. [67] G. Nehma and M. Tiwari. Leveraging kans for enhanced deep koopman operator discovery. arXiv preprint arXiv:2406.02875, 2024. [68] R. W. Ogden. Non-linear elastic deformations. Courier Corporation, 1997. [69] Y. Peng, M. He, F. Hu, Z. Mao, X. Huang, and J. Ding. Predictive modeling of flexible ehd pumps using kolmogorov-arnold networks. arXiv preprint arXiv:2405.07488, 2024. [70] E. Poeta, F. Giobergia, E. Pastor, T. Cerquitelli, and E. Baralis. A benchmarking study of kolmogorov-arnold networks on tabular data. arXiv preprint arXiv:2406.14529, 2024. [71] P. Pratyush, C. Carrier, S. Pokharel, H. D. Ismail, M. Chaudhari, and D.",
    "kangpt-43": "B. KC. Calmphoskan: Prediction of general phosphorylation sites in proteins via fusion of codon aware embeddings with amino acid aware embeddings and wavelet-based kolmogorov arnold network.",
    "kangpt-44": "bioRxiv, pages 2024-07, 2024. [72] S. Rigas, M. Papachristou, T. Papadopoulos, F. Anagnostopoulos, and G. Alexandridis. Adaptive training of grid-dependent physics-informed kolmogorov-arnold networks. arXiv preprint arXiv:2407.17611, 2024. [73] A. Sanchez-Gonzalez, J. Godwin, T. Pfaff, R. Ying, J. Leskovec, and P. Battaglia. Learning to simulate complex physics with graph networks. In International conference on machine learning, pages 8459-8468. PMLR, 2020. [74] M. Schmidt and H. Lipson. Distilling free-form natural laws from experimental data. science, 324(5923):81-85, 2009.",
    "kangpt-45": "[75] S. T. Seydi. Exploring the potential of polynomial basis functions in kolmogorov-arnold networks: A comparative study of different groups of polynomials. arXiv preprint arXiv:2406.02583, 2024.",
    "kangpt-46": "[76] S. T. Seydi. Unveiling the power of wavelets: A wavelet-based kolmogorov-arnold network for hyperspectral image classification. arXiv preprint arXiv:2406.07869, 2024. [77] H. Shen, C. Zeng, J. Wang, and Q. Wang. Reduced effectiveness of kolmogorov-arnold networks on functions with noise.",
    "kangpt-47": "arXiv preprint arXiv:2407.14882, 2024. [78] K. Shukla, J. D. Toscano, Z. Wang, Z. Zou, and G.",
    "kangpt-48": "E. Karniadakis. A comprehensive and fair comparison between mlp and kan representations for differential equations and operator networks.",
    "kangpt-49": "arXiv preprint arXiv:2406.02917, 2024. [79] E. Sifakis and J. Barbic. Fem simulation of 3d deformable solids: a practitioner's guide to theory, discretization and model reduction. In Acm siggraph 2012 courses, pages 1-50.",
    "kangpt-50": "2012. [80] W. S. Slaughter. The linearized theory of elasticity. Springer Science \\& Business Media, 2012. [81] B. Smith, F. D. Goes, and T. Kim. Stable neo-hookean flesh simulation. ACM Transactions on Graphics (TOG), 37(2):1-15, 2018.",
    "kangpt-51": "[82] H.-T. Ta. Bsrbf-kan: A combination of b-splines and radial basic functions in kolmogorovarnold networks.",
    "kangpt-52": "arXiv preprint arXiv:2406.11173, 2024.",
    "kangpt-53": "[83] T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476-482, 2024. [84] S.-M. Udrescu, A. Tan, J. Feng, O. Neto, T. Wu, and M. Tegmark. Ai feynman 2.0: Paretooptimal symbolic regression exploiting graph modularity. Advances in Neural Information Processing Systems, 33:4860-4871, 2020.",
    "kangpt-54": "[85] C. J. Vaca-Rubio, L. Blanco, R.",
    "kangpt-55": "Pereira, and M. Caus. Kolmogorov-arnold networks (kans) for time series analysis. arXiv preprint arXiv:2405.08790, 2024.",
    "kangpt-56": "[86] K. R. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, 2023.",
    "kangpt-57": "[87] Y. Wang, J. Sun, J. Bai, C. Anitescu, M. S. Eshaghi, X. Zhuang, T.",
    "kangpt-58": "Rabczuk, and Y. Liu. Kolmogorov arnold informed neural network: A physics-informed deep learning framework for solving pdes based on kolmogorov arnold networks.",
    "kangpt-59": "arXiv preprint arXiv:2406.11045, 2024.",
    "kangpt-60": "[88] S. J. Wetzel. Unsupervised learning of phase transitions: From principal component analysis to variational autoencoders. Physical Review E, 96(2):022140, 2017. [89] S. J. Wetzel, R. G. Melko, J. Scott, M. Panju, and V. Ganesh. Discovering symmetry invariants and conserved quantities by interpreting siamese neural networks.",
    "kangpt-61": "Phys. Rev. Res., 2:033499, Sep 2020. [90] T. Wu and M. Tegmark. Toward an artificial intelligence physicist for unsupervised learning. Physical Review E, 100(3):033311, 2019. [91] H. Xu, F. Sin, Y. Zhu, and J. Barbi\u010d. Nonlinear material design using principal stretches. ACM Transactions on Graphics (TOG), 34(4):1-11, 2015. [92] J. Xu, Z. Chen, J. Li, S. Yang, W. Wang, X. Hu, and E.",
    "kangpt-62": "C.-H. Ngai. Fourierkan-gcf: Fourier kolmogorov-arnold network-an effective and efficient feature transformation for graph collaborative filtering.",
    "kangpt-63": "arXiv preprint arXiv:2406.01034, 2024. [93] K. Xu, L. Chen, and S. Wang. Kolmogorov-arnold networks for time series: Bridging predictive power and interpretability. arXiv preprint arXiv:2406.02496, 2024. [94] J. Yang, R. Walters, N. Dehmamy, and R. Yu. Generative adversarial symmetry discovery. In International Conference on Machine Learning, pages 39488-39508. PMLR, 2023. [95] K. Yang, A. Swope, A. Gu, R. Chalamala, P. Song, S. Yu, S. Godil, R. J. Prenger, and A. Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems, 36, 2024. [96] S. Yang, L. Qin, and X. Yu. Endowing interpretability for neural cognitive diagnosis by efficient kolmogorov-arnold networks. arXiv preprint arXiv:2405.14399, 2024. [97] R. Yu, W. Yu, and X. Wang. Kan or mlp: A fairer comparison. arXiv preprint arXiv:2407.16674, 2024. [98] E. Zeydan, C. J. Vaca-Rubio, L. Blanco, R. Pereira, M. Caus, and A. Aydeger. F-kans: Federated kolmogorov-arnold networks. arXiv preprint arXiv:2407.20100, 2024. [99] F. Zhang and X. Zhang. Graphkan: Enhancing feature extraction with graph kolmogorov arnold networks.",
    "kangpt-64": "arXiv preprint arXiv:2406.13597, 2024. [100] Z. Zhong, Z. Liu, M. Tegmark, and J. Andreas. The clock and the pizza: Two stories in mechanistic explanation of neural networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [101] S. Zinage, S. Mondal, and S. Sarkar. Dkl-kan: Scalable deep kernel learning using kolmogorov-arnold networks. arXiv preprint arXiv:2407.21176, 2024. [^0]:    *zmliu@ mit.edu\n\n[^1]:    ${ }^{2}$ If $x$ and $y$ can be negative, one may choose a large $c>0$ and express $x y=\\exp (\\log (x+c)+\\log (y+$ c) $)-c(x+y)-c^{2}$. Other constructions include quadratic functions, such as $x y=\\left((x+y)^{2}-(x-y)^{2}\\right) / 4$ or $x y=\\left((x+y)^{2}-x^{2}-y^{2}\\right) / 2$. [^2]:    ${ }^{3}$ We set $k=2$ for simplicity, but the pykan package allows $k$ to be any integer $k \\geq 2$. Users can even set different $k$ values for different multiplication nodes. However, if different $k \\mathrm{~s}$ values are used within the same layer, it can be challenging to parallelize these multiplications. [^3]:    ${ }^{4}$ For subnodes belonging to multiplication node, the subnodes inherit their scores from the multiplication node. [^4]:    ${ }^{5}$ Other choices can be made based on the perceived importance of each output dimension, though this is less critical when outputs are typically one-dimensional. [^5]:    ${ }^{6}$ Models can be trained on GPUs, but not all functionalities already supported GPU. "
}