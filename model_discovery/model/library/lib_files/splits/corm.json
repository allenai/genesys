{
    "corm-0": "# CORM: Cache Optimization with Recent Message for Large Language Model Inference \n\nJincheng Dai<br>ZheJiang University<br>jincheng_dai@zju.edu.cn\n\nZhuowei Huang<br>ZheJiang University<br>zw_huang@zju.edu.cn\n\nHaiyun Jiang*<br>Tencent AI Lab<br>haiyunjiang@tencent.com\n\nChen Chen<br>Tencent AI Lab<br>chenzchen@tencent.com\n\nDeng Cai<br>Tencent AI Lab<br>jcykcai@tencent.com\n\nWei Bi<br>Tencent AI Lab<br>victoriabi@tencent.com\n\nShuming Shi<br>Tencent AI Lab<br>shumingshi@tencent.com\n\n\n#### Abstract\n\nLarge Language Models (LLMs), despite their remarkable performance across a wide range of tasks, necessitate substantial GPU memory and consume significant computational resources.",
    "corm-1": "Beyond the memory taken up by model weights, the memory used by the KV cache rises linearly with sequence length, becoming a primary bottleneck for inference. In this paper, we introduce an innovative method for optimizing the KV cache, which considerably minimizes its memory footprint. Upon thorough investigation, we discover that in most Transformer models, (i) there is a striking similarity between adjacent tokens' query vectors, and (ii) the attention calculation of the current query can rely exclusively on the attention information of a small fraction of preceding queries. Based on these observations, we present CORM, a KV cache eviction policy that dynamically retains essential key-value pairs for inference without the need for model fine-tuning. Our validation shows that CORM reduces the inference memory usage of KV cache by up to $70 \\%$ with negligible performance degradation across six tasks in LongBench. Furthermore, we demonstrate that CORM is compatible with GQA for further compression rate. ## 1 Introduction\n\nLarge language models (LLMs) have exhibited remarkable proficiency across a wide range of natural language processing tasks, including question answering, summarization, and multi-turn dialogues [1-3]. However, the considerable deployment cost associated with these models, due to their massive size and quadratic cost of the attention layer, have spurred numerous studies on model compression and memory-efficient attention techniques [4-7]. A critical yet often overlooked aspect is the size of the key-value (KV) cache, which stores prior tokens' key and value states to avoid redundant computation. The KV cache scales linearly with sequence length during generation, resulting in substantial memory overhead. For example, a 7 billion-parameter model with a batch size of 128 and a sequence length of 4096 necessitates a 256 GB KV cache. This far exceeds the memory consumed by the model itself, which amounts to a mere 14 GB . One intuitive solution is to eliminate less informative KV cache to decrease memory usage. The primary challenge, however, is striking a\n\n[^0]delicate balance between discarding as much KV cache as possible while preserving optimal model performance. Despite multi-query attention [8] and grouped-query attention [9] can reduce the size of KV cache by reducing key-value heads, these methods need retraining to restore performance of original model. Recent studies [10-14] have explored implementing KV cache using specific eviction policy, that determines which key-value states should be evicted from KV cache. These methods are geared towards condensing the KV cache to a predefined budget size, thereby reducing memory and computational overhead. However, they preserve same quantity of key-value pairs across all attention heads and layers, neglecting that the number of keys playing an important role may vary across different attention heads and layers, as highlighted in [15]. ![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-02.jpg?height=401&width=1399&top_left_y=687&top_left_x=361)\n\nFigure 1: Attention sparsity of LLaMA2-7B. (a) Layer-wise attention sparsity. (b) Head-wise attention sparsity of layer 0 and layer 1. Intuitively, in scenarios where critical information within KV cache exceeds the predefined budget size, it is reasonable to anticipate a degradation in model performance due to the inevitable eviction of crucial information. Our preliminary investigations corroborate this intuition, revealing distinct degrees of sparsity across different attention layers and heads as depicted in Figure 1 First, we observe that bottom layers of the model are relatively dense ${ }^{2}$, while the remaining attention layers exhibit significant sparsity. Second, even within the same layer, different heads can exhibit obvious differences in sparsity degrees. These properties suggest that we need to allocate budget for different layers and heads individually, rather than applying the same budget size across the entire model. In addition, we prove that completely similar queries have similar concerns for keys, and observe that recent query vectors are quite similar on Transformer models. Consequently, it is feasible for the current query to leverage recent query attention messages during generation process. Based on the above insights, we propose Cache Optimization with Recent Message (CORM), a framework that exploits recent query attention messages for KV cache optimization and token generation of LLMs. Specifically,\n\n- In Section 3, we explore the similarity between query vectors of all tokens within same sequence, revealing that recent query vectors are highly similar, which implies that $(i)$ keys that are important for recent queries might be also important for the current query; and (ii) removing key-value pairs that appear to be less informative for recent queries can greatly preserve the performance of the model. - In Section 4, we present a simple method which dynamically evicts minor key-value pairs determined by recent query attention messages, and design the generation process algorithm of LLMs with a budget-unrestricted KV cache. We conduct comprehensive experiments on LLaMA2-7B-Chat and Vicuna-7b-v1.5-16k, considering their popularity and extensive usage, to evaluate CORM across 6 tasks from LongBench [16] containing question answering, summarization, code completion, etc. Experiments indicate that even without explicitly setting a budget size, CORM is still able to achieve a high compression rate. Our method achieves better performance compared to StreamingLLM, Scissorhands and $\\mathrm{H}_{2} \\mathrm{O}$ with over $70 \\% \\mathrm{KV}$ cache reduction rate and can even come close to fully restoring the performance of the model. Furthermore, we demonstrate that CORM can be effectively integrated with grouped-query attention (GQA) to achieve addictional compression without noticeable performance degradation. [^1]\n## 2 Related Work\n\nAttention Let $x \\in \\mathbb{R}^{n \\times d}$ denote the input embeddings from a sequence of $n$ feature vectors of dimension $d$. The multi-head self-attention [17], as a core module of Transformer model, facilitates contextual information interaction within each head in the following manner:\n\n$$\n\\operatorname{Attention}(x)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{h}}}\\right) V\n$$\n\n$\\mathrm{Q}, \\mathrm{K}, \\mathrm{V}$ represent the query, key, and value matrices, which are obtained by linearly mapping $x$ using weight matrices $W_{q}, W_{k}$, and $W_{v} \\in \\mathbb{R}^{d \\times d_{h}}$, respectively. $d_{h}$ is the dimension of each individual head. KV Cache Compression A common paradigm for inference of Transformer models is to retain the key-value pairs of previous tokens for subsequent reuse, to avoid inefficiency of recomputation. Thus, the consumption of KV cache becomes linearly correlated with sequence length, potentially resulting in excessive memory and latency issues when dealing with long input or output. Many efforts have been made to improve efficiency for LLMs. Multi-query attention (MQA) [8] and grouped-query attention (GQA) [9] are proposed to reduce key-value heads to decrease memory usage and the number of memory swaps. Adaptively Sparse Attention [18] learns to drop uninformative tokens from the context at any point across the generation process. Dynamic Memory Compression (DMC) [19] learns to apply different compression rates in different heads and layers with continual training. However, these methods require additional training to keep model performance due to the incapacity of direct conversion. Another set of efforts is dedicated to addressing the balance between model efficiency and inference cost, achieved without extra training and architectural changes, by evicting the KV cache using different algorithms. StreamingLLM [10] and LM-Infinite [20] keep initial token and recent tokens throughout decoding process to align with the training window. InfLLM [21] stores evicted tokens into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Scissorhands [11] maintains pivotal tokens and recent tokens based on the persistence of importance hypothesis. $\\mathrm{H}_{2} \\mathrm{O}$ [12] utilizes accumulated attention score to maintain heavy hitters and recent tokens for inference. TOVA [13] removes tokens with the lowest current attention score from the fixed cache at each decoding step. RoCo [14] retains tokens in the fixed cache based on high mean cumulative attention scores and top r standard deviations. SnapKV [22] automatically compresses KV cache by selecting clustered important KV positions for each attention head. Aforementioned methods consistently operate on a fixed cache, ignoring that the number of tokens playing an important role may vary across different attention heads and layers. Though FastGen [23] applies four compression policies to construct KV cache in an adaptive manner, human-imposed strategies and model behaviors may exhibit certain biases, ultimately impacting model performance. ## 3 Observations\n\nWe first demonstrate the existence of attention sparsity in LLMs in Section 3.1, then discuss the phenomenon that similar queries have similar attention concerns for keys in Section 3.2 In Section 3.3. we show an intriguing observation that current query is most similar to recent queries. ### 3.1 Attention sparsity in LLMs\n\nWe first explore the sparsity in attention layers of LLMs, which provides an effective guarantee for us to reduce KV cache size. Specifically, we use proportion of important keys to represent attention sparsity. Let $q_{t} \\in \\mathbb{R}^{1 \\times d}$ denote the query state vector at step $t, k_{i} \\in \\mathbb{R}^{1 \\times d}$ denote the key state vector at step $i(1 \\leq i \\leq t)$, where $d$ is hidden dimension (for the sake of simplicity, we only consider a single head here). The normalized attention score of $q_{t}$ for $k_{i}$ is computed as:\n\n$$\n\\alpha_{t, i}=\\frac{\\exp \\left(q_{t} k_{i}^{T} / \\sqrt{d}\\right)}{\\sum_{j=1}^{t} \\exp \\left(q_{t} k_{j}^{T} / \\sqrt{d}\\right)}\n$$\n\nDefinition 3.1 (Important Key) We define a key $k_{i}$ is considered important at step $t$, if and only if $\\alpha_{t, i} \\geq \\frac{1}{t}$, otherwise it is considered minor. We conduct zero-shot inference with LLaMA2-7B model on the test set of PG-19 [24]. We plot the layer-wise and head-wise sparsity within attention blocks, the figures are presented in Figure 1 It reveals that attention score matrices of the model is really sparse, specifically the bottom layers are relatively dense, while other layers are highly sparse with over $90 \\%$ sparsity. This makes it possible to do attention computation on only small part of KV cache during generation. We also give attention sparsity plots of Falcon-7B and Qwen1.5-7B in Appendix A. 1\n\n### 3.2 Similar queries have similar concerns for keys\n\nThe previous section reveals the existence of attention sparsity in LLMs, which provides an opportunity to reduce KV cache size while maintaining performance. In this section we provide both theoretical analysis and empirical evidence that similar queries have similar concerns for keys for eviction policy design. Consider the $i$-th and $j$-th query state vectors $q_{i}$ and $q_{j}$ in a sequence of token length $T(i<j \\leq T)$. Their cosine similarity can be computed as:\n\n$$\n\\operatorname{cosine} \\_ \\text {similarity }\\left(q_{i}, q_{j}\\right)=\\frac{q_{i} q_{j}^{T}}{\\left\\|q_{i}\\right\\| \\cdot\\left\\|q_{j}\\right\\|}\n$$\n\nConsider all key states $k_{1}, k_{2}, \\ldots, k_{i-1}$ before $i$-th key. Assume that cosine_similarity $\\left(q_{i}, q_{j}\\right)=1$, then $q_{i}=m \\cdot q_{j}$ where $m \\in \\mathbb{R}^{+}$. The attention weight ${ }^{3}$ of $q_{i}$ to the previous $i-1$ keys can be represented as:\n\n$$\n\\text { attention_weight }=\\frac{1}{\\sqrt{d}} \\cdot\\left(q_{i} k_{1}^{T}, q_{i} k_{2}^{T}, \\ldots, q_{i} k_{i-1}^{T}\\right)=\\frac{m}{\\sqrt{d}} \\cdot\\left(q_{j} k_{1}^{T}, q_{j} k_{2}^{T}, \\ldots, q_{j} k_{i-1}^{T}\\right)\n$$\n\nNote that $m$ is a positive number that does not affect the relative order of the attention weights. For example, if $q_{i} k_{1}^{T}>q_{i} k_{2}^{T}$ for $q_{i}$, there must be $q_{j} k_{1}^{T}>q_{j} k_{2}^{T}$ for $q_{j}$. This means if a key is important to $q_{i}$, it is also important to $q_{j}$, though the degree of importance may vary due to the softmax function. Although it's nearly impossible that cosine_similarity $\\left(q_{i}, q_{j}\\right)=1$ in real situation, we can make the hypothesis that two similar queries may have similar concerns for keys. To validate this hypothesis, we provide two attention maps of a sentence randomly drawn from PG-19 using LLaMA2-7B, as shown in Figure 2 Important keys are marked with bright green, more plots are available in Appendix A. 2 We observe that the hypothesis is true, similar queries exhibit similar concerns for important keys. At the same time, important keys only account for a small proportion especially in deeper attention layers, which is consistent with the finding that deeper layers are sparser in previous section. ### 3.3 Similarity exploration of query vectors\n\nWe have validated two similar queries have similar concerns for keys in Section 3.2, we further investigate whether at each step we can find a previous query state that is similar enough to current query state in same layer and same head. To check this, we visualize cosine similarity of query vectors within same sequence as shown in Figure 3. more plots are available in Appendix A.3. We observe an intriguing phenomenon that many images show clear oblique color segmentation, with the top oblique block closest to dark red which means current query is most similar to recent queries. Through above observations, we see an opportunity to design a KV cache eviction policy based on query similarity that preserves LLMs generation performance. ## 4 Cache Optimization with Recent Message\n\nIn this section, we present CORM, a method reduces KV cache memory based on recent query attention messages without any fine-tuning process. In Section 4.1, we derive that current query can\n\n[^2]![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-05.jpg?height=739&width=1395&top_left_y=243&top_left_x=365)\n\nFigure 2: Similar queries have similar concerns for keys. We plot the attention maps from two different layers in a sentence. We discretize the attention score and those important keys are shown in bright green. Each attention map has two red borders, the bottom border shows important keys that current query actually focuses on, while another border shows important keys that the most similar query focuses on. ![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-05.jpg?height=300&width=1394&top_left_y=1265&top_left_x=365)\n\nFigure 3: Visualization of query vectors' cosine similarity over randomly sampled sentence with a length of 1024 on LLaMA2-7B. The $i$-th row of the map represents cosine similarity of the $i$-th query to all previous queries. The redder the color, the higher the similarity between two queries. The plot reveals that in most cases current query is most similar to recent queries. directly use recent query attention messages during generation. In Section 4.2, we present CORM eviction policy and describe how it works during generation. ### 4.1 Inference based on recent query attention messages\n\nConsider observations in Section 3, intuitively, we can directly store all queries and their attention messages for future reference. At each generation step, use current query to find the most similar one from previous queries, and use its saved attention information to calculate solely on important keys. However, this approach incurs a significant additional cost. First, storing all queries results in a substantial increase in memory overhead. Second, the requirement of performing similarity calculations at each step adds to the computational overhead. Since in most cases current query is most similar to recent queries as described in Section 3.3 we can just use recent query attention messages. And from Figure 2 we can also observe that only a small proportion of keys are considered important by recent queries. Therefore even if we save all the keys that are considered important in previous steps, we can save a lot of memory. # 4.2 Eviction algorithm with recent message \n\nWe have shown recent query attention information is enough for cache optimization in previous section. In the following, we formally define this algorithm and introduce how to integrate it into LLM generation directly. Definition 4.1 (Long-term Minor Key) A key $k_{i}$ is considered as long-term minor key only if it is considered minor in all recent $r$ steps (from $t-r+1$ to $t$ ). #### Abstract\n\nApproach CORM will have a recent window of size $w$ to record the information of recent $w$ queries (for each query, we will record the keys it considers important), and will always keep recent $r$ keys unremoved to prevent them from being discarded prematurely due to insufficient observations. During generation, $k_{i}, v_{i}$ will be discarded once $k_{i}$ is regarded as a long-term minor key. For better explanation we present PyTorch code ${ }^{4}$ of main algorithm in Algorithm 1. Intuitively, when $w$ is larger, more keys and values will be saved, the compression rate will be smaller and performance will be better; Conversely, when $w$ is smaller, fewer keys and values will be saved, the compression rate will be larger and performance will be worse. So there's a tradeoff between performance and compress rate. Memory Overhead Analysis In order to reduce memory overhead of KV cache, an extra memory overhead is introduced by recent information cache. We need to store recent query messages which increase memory overhead. However, these overheads are far less than compressed KV cache, one can use a small portion of memory to avoid maintaining full KV cache memory without obvious performance degradation. ## 5 Empirical Evaluation\n\nIn this section, we present the results that demonstrate CORM can reduce up to $70 \\%$ of the memory footprint of KV Cache without accuracy degradation on LLaMA2-7B-Chat and Vicuna-7b-v1.5-16k. Dataset To broadly validate feasibility of our method on real-world use cases, we choose LongBench [16] as our evaluation benchmark, which contains a wide range of long-text tasks such as question answering [25-30], summarization [31-34], few-shot learning [35-38], synthetic task and code completion [39,40]. Here we do not consider short text tasks, because even full cache doesn't have any memory bottlenecks. Models Since sequence length is the main factor in the continuous growth of KV Cache, we employ LLaMA2-7B-Chat [2] for 4 K test and Vicuna-7b-v1.5-16k for 12 k test considering their wide usage. Baselines Since CORM reduces KV cache without need for training, we consider several similar approaches as our baselines: StreamLLM [10], Scissorhands [11] and $\\mathrm{H}_{2} \\mathrm{O}$ [12]. In addition, the full KV cache is also considered as strong baseline to measure performance loss of other methods. Settings All baselines can be regarded as fixed budget size KV cache compression, however CORM is a dynamic compression method. Since we find that CORM has similar compression rates for various task texts with the same sequence length. For fair comparison, we plot the relationship between model compression rate and sequence length using texts randomly sampled from PG19 [24] as shown in Figure 4 . For LLaMA2-7B-Chat we use NVIDIA V100 32GB GPU and for Vicuna-7b-v1.5-16k we use NVIDIA A100 80GB GPU. Main Results We evaluate LLaMA2-7B-Chat for 4K length text and Vicuna-7b-v1.5-16k for 12k length text. Results are summarized in Table 1 \\& 2 for LLaMA2-7B-Chat and Table 788 for Vicuna-7b-v1.5-16k. The following observations can be drawn: (1) CORM consistently outperforms previous methods at the similar compression rate across a wide range of tasks. (2) Meanwhile, with over $70 \\% \\mathrm{KV}$ cache reduction, CORM achieves comparable performance as the model with full\n\n[^3]```\nAlgorithm 1 Single-head KV cache eviction with CORM (unbatched)\ndef corm_eviction(keys, values, message, attn_score, w, r, t):\n\"\"\"\nArgs:\n    keys: previous key states, a tensor with shape [l, d]\n    values: previous value states, a tensor with shape [l, d]\n    message: attention message, a tensor with shape of [m, l-1]\n    attn_score: current step's attention score, a tensor with shape of [1, l]\n    w: window size, a scalar\n    r: recent size, a scalar\n    t: current step, a scalar\nReturns:\n    updated_keys: updated keys\n    updated_values: updated values\n    updated_message: updated message\n\"\"\"\nm = message.shape[0]\n# update attention message\nmessage = torch.cat([message, torch.zeros(m, 1)], dim=1) # pad to [m, l]\ncur_message = attn_score >= 1/t\nmessage = torch.cat([message, cur_message], dim=1)[-w:, :]\nif message.shape[0] < w:\n    return keys, values, message\nelse:\n    # determine the key-value pairs that necessitate discarding\n    decision = message.any(dim=0)\n    decision[-r:] = True # always keep recent r tokens unremoved\n    indices = torch.nonzero(decision).squeeze()\n    keys = keys[indices, :]\n    values = values[indices, :]\n    return keys, values, message\n```\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-07.jpg?height=462&width=1176&top_left_y=1561&top_left_x=472)\n\nFigure 4: Relationship between compression rate and sequence length averaged by 10 texts randomly sampled from PG19. Plots show that compression rate with CORM \"256+256\" ( $w=256, \\mathrm{r}=256$ ) closely matches a budget of 1024 for LLaMA2-7B-Chat, and a budget of 2048 for Vicuna-7b-v1.516k. KV cache and even surpass it on some tasks, we speculate it's because there's some noise in full KV cache that affects model output and our method can eliminate this noise to a certain extent by discarding some key-value pairs. We also evaluate the perplexity of different methods on first 5 texts drawn from PG-19 test set, the result is shown in Table 3, it also shows CORM is better than other methods. Table 1: Results (\\%) on single-doc QA, multi-doc QA and summarization tasks. \"Full\" refers to LLaMA2-7B-Chat utilizing full KV Cache, \"StreamLLM\" is configured with 4+1020, \"Scissorhands\" is configured with $768+256$ where window size $=256, \" \\mathrm{H}_{2} \\mathrm{O}$ \" is configured with $768+256$, \"CORM\" is configured with $256+256$ for fair comparison. For the sake of brevity we use ID to denote dataset here, mapping from ID to dataset can be found in Appendix B. | Method | Single-Doc QA |  |  |  |  | Multi-Doc QA |  |  |  |  | Summarization |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $1-1$ | $1-2$ | $1-3$ | $1-4$ |  | $2-1$ | $2-2$ | $2-3$ | $2-4$ |  | $3-1$ | $3-2$ | $3-3$ | $3-4$ |\n| Full | 19.0 | 22.1 | 36.7 | 11.8 |  | 27.8 | 31.5 | 8.3 | 6.8 |  | 26.8 | 20.7 | 26.2 | 0.2 |\n| StreamLLM | 13.2 | 15.4 | 27.2 | 6.5 |  | 24.2 | 25.4 | 5.3 | 4.4 |  | 21.6 | 19.8 | 24.4 | 0.1 |\n| Scissorhands | 16.6 | 18.7 | 32.4 | 9.9 |  | 26.3 | 32.1 | 8.9 | 5.7 |  | 22.1 | 20.7 | 25.4 | 0.2 |\n| $\\mathrm{H}_{2} \\mathrm{O}$ | 17.9 | 19.5 | 34.9 | 11.5 |  | 27.5 | 29.7 | 7.5 | 7.1 |  | 24.5 | 21.0 | 25.8 | 0.2 |\n| CORM | 18.9 | 22.2 | 38.6 | 12.0 |  | 27.6 | 31.6 | 8.4 | 7.1 |  | 26.4 | 21.0 | 25.8 | 0.2 |\n\nTable 2: Results (\\%) on few-shot learning, synthetic, and code tasks. \"Overall\" is computed by the macro-average over major task categories. This is computed on English (EN) tasks, Chinese (ZH) tasks, and all (All) tasks, code tasks are included in both languages. | Method | Few-shot Learning |  |  |  | Synthetic |  |  | Code |  | Overall |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 4-1 | 4-2 | 4-3 | 4-4 | 5-1 | 5-2 | 5-3 | 6-1 | 6-2 | EN | ZN | All |\n| Full | 64.0 | 83.3 | 41.4 | 17.3 | 2.9 | 7.8 | 10.0 | 58.3 | 52.2 | 32.8 | 16.9 | 28.9 |\n| StreamLLM | 61.0 | 82.9 | 39.1 | 14.5 | 1.8 | 4.7 | 6.5 | 57.6 | 50.0 | 29.5 | 14.3 | 25.7 |\n| Scissorhands | 52.5 | 83.6 | 40.7 | 17.0 | 3.1 | 6.5 | 7.7 | 56.8 | 52.1 | 31.0 | 15.8 | 27.2 |\n| $\\mathrm{H}_{2} \\mathrm{O}$ | 63.0 | 81.5 | 39.9 | 17.0 | 2.8 | 7.0 | 7.3 | 57.8 | 52.3 | 31.8 | 16.4 | 28.0 |\n| CORM | 64.0 | 83.5 | 41.3 | 17.3 | 2.9 | 9.0 | 9.1 | 58.3 | 52.0 | 32.9 | 16.8 | 28.9 |\n\n### 5.1 Impact of window size\n\nIntuitively, when window size is larger, more key-value pairs will be cached which means more information will be saved, the performance will be better. So we explore the impact of window size on model performance by evaluating on LLaMA2-7B-Chat under different window sizes. We present the results in Appendix C.2, it shows that there is a positive correlation between window size and model performance. However, even if we set the window size to 64, it won't bring much performance degradation, which means we can keep model performance with even higher compression rate. Table 3: Perplexity comparison of different models and different methods. | Model | Full | StreamLLM | Scissorhands | $\\mathrm{H}_{2} \\mathrm{O}$ | CORM |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| LLaMA2-7B | 9.79 | 10.15 | 10.05 | 10.02 | 9.89 |\n| Vicuna-7b-v1.5-16k | 12.49 | 13.15 | 13.80 | 154.02 | 12.88 |\n\nTable 4: Different Positional Encoding $128+1282048$. | Model | Full | CORM | Reduction |\n| :--- | :---: | :---: | :---: |\n| LLaMA1-7B | 10.58 | 10.69 | $74.4 \\%$ |\n| Bloom-7B | 17.02 | 17.05 | $4.4 \\%$ |\n| OPT-6.7B | 15.83 | 16.39 | $73.4 \\%$ |\n\n### 5.2 Budget unnecessity\n\nWe primarily focus on the effectiveness of not setting a budget versus setting a fixed budget. Note that since we use same window size and recent size as Scissorhands in the experiment, it can be regarded as a natural ablation experiment. And Table $1 \\& 2$ have shown that, at the similar compression rate, CORM is much better than Scissorhands on most tasks, and performance of other tasks is close. This\nverifies that different transformer layers and heads should be treated differently rather than setting a same fixed budget size or a single compression rate. We attribute this to the varying sparsity across different layers and heads of Transformer models. ### 5.3 Integrate into grouped-query attention (GQA)\n\nGQA is widely adopted in LLMs to reduce memory overhead, we also extend CORM to GQA, for different groups, we discard keys that are considered minor by all recent queries within the group. The results of LLaMA3-8B-Instruct in Table 5 \\& 6 validate the effectiveness of our extension, even in GQA setting, CORM can still achieve $60 \\%$ KV cache reduction and keep performance. Table 5: Results (\\%) on single-doc QA, multi-doc QA and summarization tasks. \"Full\" refers to LLaMA3-8B-Instruct utilizing full KV Cache, \"CORM\" is configured with 256+256. | Method | Single-Doc QA |  |  |  | Multi-Doc QA |  |  |  | Summarization |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $1-1$ | $1-2$ | 1-3 | $1-4$ | 2-1 | 2-2 | 2-3 | 2-4 | 3-1 | 3-2 | 3-3 | 3-4 |\n| Full | 21.0 | 44.1 | 44.9 | 51.8 | 47.2 | 35.8 | 22.9 | 13.0 | 29.1 | 21.6 | 24.5 | 0.2 |\n| CORM | 21.0 | 43.7 | 44.8 | 51.3 | 45.7 | 36.3 | 23.1 | 13.7 | 28.6 | 21.7 | 24.4 | 0.2 |\n\nTable 6: Results (\\%) on few-shot learning, synthetic, and code tasks. | Method | Few-shot Learning |  |  |  | Synthetic |  |  | Code |  | Overall |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 4-1 | 4-2 | 4-3 | 4-4 | 5-1 | 5-2 | 5-3 | 6-1 | 6-2 | EN | ZN | All |\n| Full | 74.5 | 90.5 | 42.3 | 23.8 | 6.0 | 64.5 | 85.5 | 56.9 | 51.6 | 42.6 | 38.1 | 42.2 |\n| CORM | 74.5 | 90.6 | 41.9 | 23.8 | 6.0 | 64.0 | 82.5 | 57.9 | 52.3 | 42.6 | 37.8 | 42.0 |\n\n### 5.4 Generalization\n\nCORM works on the observation that similar queries have similar concerns for keys. We further explore whether all Transformer models have this phenomenon. We conduct experiments on Falcon7B [41], Mistral-7B [42], LLaMA3-8B, Qwen1.5-7B [43], OPT-6.7B [44] and Bloom-7B [45]. We plot query similarity within same sequence as shown in Figure 5, more plots can be found in Appendix C. 3 The results show that each model has such phenomenon except OPT-6.7B and Bloom-7B. We find that OPT-6.7B use absolute position embedding and Bloom-7B use ALiBi [46] position embedding, while others use rotary position embedding (RoPE) [47]. We hypothesis that rotary position embedding can allow the model to make adjacent queries more similar during training process. We conduct further experiments to compare the perplexity of LLaMA1-7B, OPT-6.7B, and Bloom-7B on the first five texts of PG19 as shown in Table 4 it reveals that there are two situations when CORM is applied to non-RoPE models: (i) CORM cannot effectively reduce the space occupied by KV cache, for example the compression rate of Bloom-7B is only $4 \\%$ after applying CORM. (ii) CORM can effectively reduce the space occupied by KV cache, such as OPT-6.7B, but the performance loss will be higher compared to RoPE models. ![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-09.jpg?height=340&width=1401&top_left_y=2049&top_left_x=362)\nFigure 5: Visualization of query vectors' cosine similarity over randomly sampled sentence with a length of 1024 across Falcon-7B, Qwen1.5-7B, LLaMA3-8B, OPT-6.7B. ## 6 Conclusion\n\nIn this paper, we address a critical memory bottleneck in deployment of LLM, specifically KV cache. Drawing on the observation that similar queries exhibit similar concerns for keys, and recent queries tend to be sufficiently similar, we introduce CORM, an innovative eviction policy for KV cache that does not rely on a predefined budget. It significantly diminishes the memory footprint by utilizing attention messages of recent queries. Through comprehensive evaluations, we demonstrate that CORM can reduce the inference memory usage of KV cache by up to $70 \\%$, while maintaining robust performance across a variety of tasks. ## References\n\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [2] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [3] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, HengTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. [4] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022. [5] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023. [6] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. [7] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [8] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. [9] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\u00f3n, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints.",
    "corm-2": "arXiv preprint arXiv:2305.13245, 2023. [10] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks.",
    "corm-3": "arXiv preprint arXiv:2309.17453, 2023. [11] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024. [12] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R\u00e9, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models.",
    "corm-4": "Advances in Neural Information Processing Systems, 36, 2024. [13] Matanel Oren, Michael Hassid, Yossi Adi, and Roy Schwartz. Transformers are multi-state rnns.",
    "corm-5": "arXiv preprint arXiv:2401.06104, 2024. [14] Siyu Ren and Kenny Q Zhu. On the efficacy of eviction policy for key-value constrained generative language model inference.",
    "corm-6": "arXiv preprint arXiv:2402.06262, 2024. [15] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019. [16] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [18] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. Advances in Neural Information Processing Systems, 36, 2024. [19] Piotr Nawrot, Adrian \u0141a\u0144cucki, Marcin Chochowski, David Tarjan, and Edoardo M Ponti. Dynamic memory compression: Retrofitting llms for accelerated inference. arXiv preprint arXiv:2403.09636, 2024. [20] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023 . [21] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory.",
    "corm-7": "arXiv preprint arXiv:2402.04617, 2024. [22] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024. [23] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms.",
    "corm-8": "arXiv preprint arXiv:2310.01801, 2023. [24] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling.",
    "corm-9": "arXiv preprint arXiv:1911.05507, 2019. [25] Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328, 2018. [26] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011, 2021. [27] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [28] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. [29] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition.",
    "corm-10": "Transactions of the Association for Computational Linguistics, 10:539-554, 2022. [30] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, et al. Dureader: a chinese machine reading comprehension dataset from real-world applications.",
    "corm-11": "arXiv preprint arXiv:1711.05073, 2017. [31] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021. [32] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new benchmark for query-based multi-domain meeting summarization. arXiv preprint arXiv:2104.05938, 2021. [33] Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. arXiv preprint arXiv:1906.01749, 2019. [34] Han Wu, Mingjie Zhan, Haochen Tan, Zhaohui Hou, Ding Liang, and Linqi Song. Vcsum: A versatile chinese meeting summarization dataset.",
    "corm-12": "arXiv preprint arXiv:2305.05280, 2023. [35] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. [36] Task definition for large scale text categorization at nlpcc 2014. http://tcci.ccf.org.cn/ conference/2014/dldoc/evatask6.pdf, 2014. [37] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-annotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237, 2019. [38] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.",
    "corm-13": "arXiv preprint arXiv:1705.03551, 2017. [39] Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. Longcoder: A long-range pre-trained language model for code completion. In International Conference on Machine Learning, pages 12098-12107. PMLR, 2023. [40] Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems.",
    "corm-14": "arXiv preprint arXiv:2306.03091, 2023. [41] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/2306.01116\n[42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [44] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. [45] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. [46] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation.",
    "corm-15": "arXiv preprint arXiv:2108.12409, 2021. [47] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.",
    "corm-16": "Neurocomputing, 568:127063, 2024. ## A More Plots\n\n## A. 1 Attention sparsity in LLMs\n\nWe provide attention sparsity visualizations of Falcon-7B and Qwen1.5-7B, as depicted in Figure 6 and Figure 7, respectively. They also demonstrate that the attention score matrices exhibit sparsity, with varying degrees of sparsity observed across different attention layers and heads. ![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-14.jpg?height=412&width=1401&top_left_y=538&top_left_x=361)\n\nFigure 6: Attention sparsity of Falcon-7B. (a) Layer-wise attention sparsity. (b) Head-wise attention sparsity of layer 0 and layer 1. ![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-14.jpg?height=455&width=1400&top_left_y=1083&top_left_x=362)\n\nFigure 7: Attention sparsity of Qwen1.5-7B. (a) Layer-wise attention sparsity. (b) Head-wise attention sparsity of layer 0 and layer 1. ## A. 2 Similar queries have similar concerns for keys\n\nWe provide the attention maps similar to Figure 2 but from different heads on the same text in Figure 8. Figure 9 . Plots from a different layer on the same text are shown in Figure 10\n\n## A. 3 Similarity between query vectors\n\nWe provide the query vectors' cosine similarity visualizations similar to Figure 3 but from different layers and heads on the same text in Figure 11 . ## B Task Mapping\n\nAn overview of the dataset statistics and mapping from ID to dataset in LongBench is shown in Figure 12\n\n## C Additional Experiment\n\n## C. 1 Vicuna results\n\nWe evaluate Vicuna-7b-v1.5-16k for 12k length text, results are summarized in Table 7\\& 8. \"Full\" refers to Vicuna-7b-v1.5-16k utilizing full KV Cache, \"StreamLLM\" is configured with 4+2044,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-15.jpg?height=737&width=1395&top_left_y=246&top_left_x=365)\n\nFigure 8: Attention Map at Layer 0, Head 10, 20\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-15.jpg?height=746&width=1389&top_left_y=1080&top_left_x=368)\n\nFigure 9: Attention Map at Layer 31, Head 10, 20\n\"Scissorhands\" is configured with $1792+256$ where window size $=256, \" \\mathrm{H}_{2} \\mathrm{O}$ \" is configured with $1792+256$, \"CORM\" is configured with $256+256$ for fair comparison. Table 7: Results (\\%) on single-doc QA, multi-doc QA and summarization tasks. | Method | Single-Doc QA |  |  |  |  | Multi-Doc QA |  |  |  |  | Summarization |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $1-1$ | $1-2$ | $1-3$ | $1-4$ |  | $2-1$ | $2-2$ | $2-3$ | $2-4$ |  | $3-1$ | $3-2$ | $3-3$ | $3-4$ |\n| Full | 16.6 | 26.5 | 38.4 | 41.8 |  | 23.3 | 21.4 | 8.6 | 23.3 |  | 29.0 | 22.7 | 27.5 | 14.5 |\n| StreamLLM | 12.1 | 20.6 | 20.0 | 29.2 |  | 19.1 | 18.4 | 4.4 | 16.1 |  | 13.2 | 18.0 | 25.9 | 13.1 |\n| Scissorhands | 14.1 | 22.0 | 28.8 | 25.8 |  | 19.4 | 21.7 | 5.5 | 13.3 |  | 20.7 | 19.5 | 26.8 | 8.0 |\n| $\\mathrm{H}_{2} \\mathrm{O}$ | 10.9 | 23.2 | 29.7 | 31.6 |  | 16.8 | 16.8 | 6.4 | 19.3 |  | 25.6 | 21.2 | 27.2 | 13.7 |\n| CORM | 16.5 | 25.5 | 36.4 | 41.3 |  | 25.2 | 22.0 | 7.7 | 22.7 |  | 27.4 | 23.1 | 26.9 | 13.2 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-16.jpg?height=1105&width=1394&top_left_y=246&top_left_x=365)\n\nFigure 10: Attention Map at Layer 5, 15, 25, Head 0\nTable 8: Results (\\%) on few-shot learning, synthetic, and code tasks. | Method | Few-shot Learning |  |  |  | Synthetic |  |  | Code |  | Overall |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 4-1 | 4-2 | 4-3 | 4-4 | 5-1 | 5-2 | 5-3 | 6-1 | 6-2 | EN | ZN | All |\n| Full | 69.5 | 86.5 | 41.0 | 27.3 | 4.0 | 5.0 | 4.5 | 49.2 | 43.2 | 31.3 | 26.3 | 30.0 |\n| StreamLLM | 62.0 | 84.5 | 29.6 | 19.8 | 3.0 | 5.5 | 6.0 | 46.5 | 39.3 | 26.1 | 21.2 | 24.9 |\n| Scissorhands | 54.0 | 71.8 | 38.6 | 19.5 | 5.0 | 2.5 | 2.2 | 48.1 | 32.6 | 26.4 | 18.2 | 24.3 |\n| $\\mathrm{H}_{2} \\mathrm{O}$ | 67.0 | 79.9 | 28.6 | 17.2 | 4.7 | 4.0 | 2.6 | 50.0 | 36.4 | 27.6 | 21.3 | 26.0 |\n| CORM | 69.0 | 83.7 | 41.3 | 25.3 | 4.5 | 5.5 | 5.0 | 48.9 | 43.4 | 31.0 | 25.6 | 29.7 |\n\n## C. 2 CORM with different window sizes\n\nWe evaluate CORM on LLaMA2-7B-Chat with different window sizes, results are summarized in Table 9 \\& 10\n\nTable 9: Results (\\%) on single-doc QA, multi-doc QA and summarization tasks. | Method | Single-Doc QA |  |  |  |  | Multi-Doc QA |  |  |  |  | Summarization |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $1-1$ | $1-2$ | $1-3$ | $1-4$ |  | $2-1$ | $2-2$ | $2-3$ | $2-4$ |  | $3-1$ | $3-2$ | $3-3$ | $3-4$ |\n| Full | 19.0 | 22.1 | 36.7 | 11.8 |  | 27.8 | 31.5 | 8.3 | 6.8 |  | 26.8 | 20.7 | 26.2 | 0.2 |\n| CORM 256+256 | 18.9 | 22.2 | 38.6 | 12.0 |  | 27.6 | 31.6 | 8.4 | 7.1 |  | 26.4 | 21.0 | 25.8 | 0.2 |\n| CORM 128+128 | 18.8 | 21.5 | 38.4 | 11.8 |  | 27.0 | 31.9 | 8.3 | 6.7 |  | 24.7 | 20.9 | 25.5 | 0.2 |\n| CORM 64+64 | 19.3 | 20.9 | 37.8 | 10.8 |  | 27.3 | 31.6 | 8.6 | 6.6 |  | 22.5 | 21.2 | 24.3 | 0.2 |\n| CORM 32+32 | 19.1 | 20.1 | 37.3 | 10.8 |  | 27.7 | 31.7 | 8.5 | 5.6 |  | 21.0 | 21.2 | 22.2 | 0.2 |\n| CORM 16+16 | 18.8 | 18.7 | 35.4 | 9.6 |  | 28.1 | 31.0 | 8.1 | 5.6 |  | 20.2 | 20.8 | 20.1 | 0.1 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-17.jpg?height=1469&width=1393&top_left_y=243&top_left_x=364)\n\nFigure 11: Visualization of query vectors' cosine similarity across different layers and heads\nTable 10: Results (\\%) on few-shot learning, synthetic, and code tasks. | Method | Few-shot Learning |  |  |  | Synthetic |  |  | Code |  | Overall |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 4-1 | 4-2 | 4-3 | 4-4 | 5-1 | 5-2 | 5-3 | 6-1 | 6-2 | EN | ZN | All |\n| Full | 64.0 | 83.3 | 41.4 | 17.3 | 2.9 | 7.8 | 10.0 | 58.3 | 52.2 | 32.8 | 16.9 | 28.9 |\n| CORM 256+256 | 64.0 | 83.5 | 41.3 | 17.3 | 2.9 | 9.0 | 9.1 | 58.3 | 52.0 | 32.9 | 16.8 | 28.9 |\n| CORM 128+128 | 64.0 | 83.0 | 41.2 | 17.3 | 3.4 | 8.5 | 8.5 | 57.2 | 52.7 | 32.7 | 16.6 | 28.7 |\n| CORM 64+64 | 64.0 | 83.0 | 40.7 | 17.8 | 3.0 | 8.0 | 8.0 | 58.4 | 52.1 | 32.4 | 16.4 | 28.5 |\n| CORM 32+32 | 64.0 | 83.4 | 40.5 | 16.8 | 3.6 | 7.0 | 7.7 | 57.6 | 51.3 | 32.0 | 15.9 | 28.0 |\n| CORM 16+16 | 63.5 | 83.4 | 40.1 | 18.0 | 2.9 | 5.5 | 6.6 | 54.7 | 50.6 | 31.0 | 15.4 | 27.2 |\n\n## C. 3 Query similarity visualization of different models\n\nWe plot query similarity within same sequence across Falcon-7B, Mistral-7B, LLaMA3-8B, Qwen1.57B, OPT-6.7B and Bloom-7B, as shown in Figure 13 Models using rotary position embedding all show current query is most similar to recent queries. | Dataset | ID | Source | Avg len | Metric | Language | \\#data |\n| :--- | :--- | :--- | ---: | :---: | :---: | :---: |\n| Single-Document QA |  |  |  |  |  |  |\n| NarrativeQA | $1-1$ | Literature, Film | 18,409 | F1 | English | 200 |\n| Qasper | $1-2$ | Science | 3,619 | F1 | English | 200 |\n| MultiFieldQA-en | $1-3$ | Multi-field | 4,559 | F1 | English | 150 |\n| MultiFieldQA-zh | $1-4$ | Multi-field | 6,701 | F1 | Chinese | 200 |\n| Multi-Document QA |  |  |  |  |  |  |\n| HotpotQA | $2-1$ | Wikipedia | 9,151 | F1 | English | 200 |\n| 2WikiMultihopQA | $2-2$ | Wikipedia | 4,887 | F1 | English | 200 |\n| MuSiQue | $2-3$ | Wikipedia | 11,214 | F1 | English | 200 |\n| DuReader | $2-4$ | Baidu Search | 15,768 | Rouge-L | Chinese | 200 |\n| Summarization |  |  |  |  |  |  |\n| GovReport | $3-1$ | Government report | 8,734 | Rouge-L | English | 200 |\n| QMSum | $3-2$ | Meeting | 10,614 | Rouge-L | English | 200 |\n| MultiNews | $3-3$ | News | 2,113 | Rouge-L | English | 200 |\n| VCSUM | $3-4$ | Meeting | 15,380 | Rouge-L | Chinese | 200 |\n| Few-shot Learning |  |  |  |  |  |  |\n| TREC | $4-1$ | Web question | 5,177 | Accuracy (CLS) | English | 200 |\n| TriviaQA | $4-2$ | Wikipedia, Web | 8,209 | F1 | English | 200 |\n| SAMSum | $4-3$ | Dialogue | 6,258 | Rouge-L | English | 200 |\n| LSHT | $4-4$ | News | 22,337 | Accuracy (CLS) | Chinese | 200 |\n| Synthetic Task |  |  |  |  |  |  |\n| PassageCount | $5-1$ | Wikipedia | 11,141 | Accuracy (EM) | English | 200 |\n| PassageRetrieval-en | $5-2$ | Wikipedia | 9,289 | Accuracy (EM) | English | 200 |\n| PassageRetrieval-zh | $5-3$ | C4 Dataset | 6,745 | Accuracy (EM) | Chinese | 200 |\n| Code Completion |  |  |  |  |  |  |\n| LCC | Github | 1,235 | Edit Sim | Python/C\\#/Java | 500 |  |\n| RepoBench-P | $6-2$ | Github repository | 4,206 | Edit Sim | Python/Java | 500 |\n\nFigure 12: An overview of the dataset statistics in LongBench\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_8517fcb31e1c95d553dfg-18.jpg?height=876&width=1384&top_left_y=1426&top_left_x=362)\n\nFigure 13: Visualization of query vectors' cosine similarity over randomly sampled sentence with a length of 1024 across Falcon-7B, Mistral-7B, LLaMA3-8B, Qwen1.5-7B, OPT-6.7B, Bloom-7B\n\n\n[^0]:    *Corresponding Author\n\n[^1]:    ${ }^{2}$ Let $t$ denote current decoding step, we count the proportion of keys which attention score larger than average score $\\frac{1}{t}$ and denote it as $r$.",
    "corm-17": "A higher value of $r$ indicates greater sparsity within the layer and head. [^2]:    ${ }^{3}$ Attention weight is unnormalized attention score. [^3]:    ${ }^{4}$ For the sake of brevity, the code snippet only demonstrates single-head eviction operation, while in the actual implementation, it will be performed on each head at every layer. "
}