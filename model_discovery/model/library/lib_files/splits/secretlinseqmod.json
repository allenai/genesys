{
    "secretlinseqmod-0": "Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective\n\nZhen Qin Xuyang Shen Dong Li Weigao Sun Stan Birchfield Richard Hartley Yiran Zhong\n\nAbstract\n\nWe present the Linear Complexity Sequence Model (LCSM), a comprehensive solution that unites various sequence modeling techniques with linear complexity, including linear attention, state space model, long convolution, and linear RNN, within a single framework.",
    "secretlinseqmod-1": "The goal is to enhance comprehension of these models by analyzing the impact of each component from a cohesive and streamlined viewpoint. Specifically, we segment the modeling processes of these models into three distinct stages: Expand, Oscillation, and Shrink (EOS), with each model having its own specific settings. The Expand stage involves projecting the input signal onto a high-dimensional memory state. This is followed by recursive operations performed on the memory state in the Oscillation stage. Finally, the memory state is projected back to a low-dimensional space in the Shrink stage. We perform comprehensive experiments to analyze the impact of different stage settings on language modeling and retrieval tasks. Our results show that data-driven methods are crucial for the effectiveness of the three stages in language modeling, whereas hand-crafted methods yield better performance in retrieval tasks. Linear attention, sequence parallelism, unlimited sequence length, large language model\n\n1 Introduction\n\nTransformers (Vaswani et al., 2017) have revolutionized deep learning in recent years. Nevertheless, the escalating demand for extra-long sequence modeling and the abundance of training data have necessitated the development of sequence modeling techniques beyond the original formulation. In particular, recent years have witnessed the increased development of linear complexity sequence modeling techniques. Various advanced efficient approaches have emerged lately, exhibiting notable performance improvements and narrowing the performance gap with transformer-based methods (Qin et al., 2023b; Gu & Dao, 2023; Yang et al., 2023). These methods can be categorized into four primary groups. 1) Linear attention (Katharopoulos et al., 2020b; Choromanski et al., 2021) involves a series of techniques aimed at computing attention matrices using the \"right-product kernel trick.\" This approach computes the key-value production first, thereby avoiding quadratic query-key production. Recent studies (Qin et al., 2023b; Sun et al., 2023) have shown that Linear attention can achieve performance levels comparable to softmax attention while significantly improving speed. 2) State Space Model (Gu et al., 2022a; Smith et al., 2022) (SSM) encompasses methods that utilize State Space Equations for sequence modeling. Through the use of special initialization, diagonalization assumptions, and mixed techniques, SSM methods can achieve performance levels comparable to softmax attention methods in language modeling (Gu & Dao, 2023). 3) Long convolution (Qin et al., 2023a; Poli et al., 2023) models adopt the concept of CNN but utilize a kernel with a size equal to the input sequence length. Fast Fourier Transforms (FFT) are employed to reduce computational complexities to log-linear (Fu et al., 2023a, b). However, the language modeling performance of these methods still falls short of that of transformers. 4) Linear RNNs (Qin et al., 2023d; Orvieto et al., 2023) enable parallel training of the RNN, and demonstrate competitive performance against similarly scaled Transformers. Recent research suggests that there are inherent connections between these methodologies. For instance, Long convolution models can be transformed into SSM models during inference using a closed-form solution (Qin & Zhong, 2023). Similarly, linear attention and SSM models can conduct inference in an RNN-style fashion (Katharopoulos et al., 2020b). By delving into the intrinsic connections, in this paper, we present a unified framework called Linear Complexity Sequence Model (LCSM) that effectively encompasses all established methods for linear complexity sequence modeling. This framework facilitates a deeper understanding of the operational principles underlying these methods. The LCSM model consists of three stages: Expand, Oscillation, and Shrink (EOS) as well as a Memory state. During the Expand stage, input signals are transformed into a high-dimensional memory state using a predefined projection function such as the outer product. The memory state is then combined with previous memory states through recursive operations in the Oscillation stage. Finally, the combined memory state is reduced to a low-dimensional output space in the Shrink stage. Different methodologies often exhibit variations in three key areas: 1) parameterization strategies used to calculate the Expand and Shrink parameters, 2) data dependency strategies for EOS, and 3) construction methods for the Oscillation state. Specifically, we examine two parameterization strategies, eight EOS data dependency strategies and twelve approaches for constructing the Oscillation state in order to assess the impact of each option. Ablation studies are conducted to analyze the impact of activation functions and hyperparameters, providing complementary insights. Our main focus is on evaluating the language modeling capabilities and information retrieval abilities of these variants. Our empirical findings indicates that data dependency plays a crucial role in the construction of oscillation states and the parameterization of Expand and Shrink stages in language modeling tasks, while its significance is comparatively lower in retrieval tasks. Our use of carefully hand-crafted EOS parameterization has resulted in improved overall performance in retrieval tasks. 2 Method\n\nThis section begins with a general definition of sequence modeling and then delves into the specifics of our Linear Complexity Sequence Model (LCSM). We will also provide examples to illustrate that all current linear complexity sequence modeling methods can be considered as a special case of LCSM. 2.1 General Definition of Sequence Modeling\n\nLet us consider a sequence mapping :\n\n[ \ud835\udc32 1 \u22a4 \u22ee \ud835\udc32 n \u22a4 ] = \ud835\udc18 = f \u200b ( \ud835\udc17 ) = f \u200b ( [ \ud835\udc31 1 \u22a4 \u22ee \ud835\udc31 n \u22a4 ] ) , delimited-[] superscript subscript \ud835\udc32 1 top \u22ee superscript subscript \ud835\udc32 \ud835\udc5b top \ud835\udc18 \ud835\udc53 \ud835\udc17 \ud835\udc53 delimited-[] superscript subscript \ud835\udc31 1 top \u22ee superscript subscript \ud835\udc31 \ud835\udc5b top \\small\\begin{gathered}{\\left[\\begin{array}[]{c}\\mathbf{y}_{1}^{\\top}\\\\\n\\vdots\\\\\n\\mathbf{y}_{n}^{\\top}\\end{array}\\right]=\\mathbf{Y}=f(\\mathbf{X})=f\\left(\\left[\\begin{array}[]{c}\\mathbf{x}_{1}^{\\top}\\\\\n\\vdots\\\\\n\\mathbf{x}_{n}^{\\top}\\end{array}\\right]\\right)},\\end{gathered} (1)\n\nor , where , is the temporal length of the sequence, and is the feature dimensionality.",
    "secretlinseqmod-2": "Let be the transpose of the th row of . In particular, we consider the causal mapping :\n\n\ud835\udc32 n = f n \u200b ( \ud835\udc31 1 , \u2026 , \ud835\udc31 n ) \u225c f \u200b ( \ud835\udc31 1 , \u2026 , \ud835\udc31 n ) n . subscript \ud835\udc32 \ud835\udc5b subscript \ud835\udc53 \ud835\udc5b subscript \ud835\udc31 1 \u2026 subscript \ud835\udc31 \ud835\udc5b \u225c \ud835\udc53 subscript subscript \ud835\udc31 1 \u2026 subscript \ud835\udc31 \ud835\udc5b \ud835\udc5b \\mathbf{y}_{n}=f_{n}(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})\\triangleq f(\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n})_{n}. Language modeling is a typical example of causal mapping. The subsequent sections of the paper will focus on causal mapping for the purpose of simplifying the explanation, but the extension to non-causal mapping is straightforward. 2.2 Linear Complexity Sequence Modeling\n\nOur proposed Linear Complexity Sequence Model (LCSM) performs the mapping sequentially as follows:\n\n\ud835\udc26 t subscript \ud835\udc26 \ud835\udc61 \\displaystyle\\mathbf{m}_{t} = g \u03c8 \u200b ( \ud835\udc28 t , \ud835\udc26 t \u2212 1 ) + \ud835\udc1e t \u200b \ud835\udc22 t \u22a4 absent subscript \ud835\udc54 \ud835\udf13 subscript \ud835\udc28 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 subscript \ud835\udc1e \ud835\udc61 superscript subscript \ud835\udc22 \ud835\udc61 top \\displaystyle=g_{\\psi}(\\mathbf{o}_{t},\\mathbf{m}_{t-1})+\\mathbf{e}_{t}\\mathbf{i}_{t}^{\\top} (2) \ud835\udc32 t subscript \ud835\udc32 \ud835\udc61 \\displaystyle\\mathbf{y}_{t} = \ud835\udc26 t \u22a4 \u200b \ud835\udc2c t , absent superscript subscript \ud835\udc26 \ud835\udc61 top subscript \ud835\udc2c \ud835\udc61 \\displaystyle=\\mathbf{m}_{t}^{\\top}\\mathbf{s}_{t}, (3)\n\nwhere is the expand state, is the oscillation state, is the shrink state, is the input state, is the expand dimension, and is defined below. In other words, there are three stages to the information flow of the sequence modeling process. These stages, known as Expand, Oscillation, and Shrink (EOS), are repeated for each time step :\n\n1. Expand Stage: The input state is expanded, via the expand state :\n\n\ud835\udc26 \u00af t = \ud835\udc1e t \ud835\udc22 t \u22a4 \u2208 \u211d k \u00d7 d . \\bar{\\mathbf{m}}_{t}=\\mathbf{e}_{t}\\mathbf{i}_{t}^{\\top}\\quad\\in\\mathbb{R}^{k\\times d}. (4)\n\n2. Oscillation Stage: The memory state is updated:\n\n\ud835\udc26 t = g \u03c8 ( \ud835\udc28 t , \ud835\udc26 t \u2212 1 ) + \ud835\udc26 \u00af t \u2208 \u211d k \u00d7 d , \\mathbf{m}_{t}=g_{\\psi}(\\mathbf{o}_{t},\\mathbf{m}_{t-1})+\\bar{\\mathbf{m}}_{t}\\quad\\in\\mathbb{R}^{k\\times d},\n\nwhere for initialization we set , which is a matrix of zeros. 3. Shrink Stage: The output state is obtained by projecting the memory state back to low-dimensional space via the shrink state :\n\n\ud835\udc32 t = \ud835\udc26 t \u22a4 \ud835\udc2c t \u2208 \u211d d . \\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top}\\mathbf{s}_{t}\\quad\\in\\mathbb{R}^{d}. (5)\n\nWe consider two possible binary operators . If , the projection function is element-wise multiplication, whereas if , it is matrix multiplication:\n\ng \u2299 \u200b ( \ud835\udc28 t , \ud835\udc26 t \u2212 1 ) subscript \ud835\udc54 direct-product subscript \ud835\udc28 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 \\displaystyle g_{\\odot}(\\mathbf{o}_{t},\\mathbf{m}_{t-1}) = \ud835\udc28 t \u2299 \ud835\udc26 t \u2212 1 absent direct-product subscript \ud835\udc28 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 \\displaystyle=\\mathbf{o}_{t}\\odot\\mathbf{m}_{t-1} (6) g \u00d7 \u200b ( \ud835\udc28 t , \ud835\udc26 t \u2212 1 ) subscript \ud835\udc54 subscript \ud835\udc28 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 \\displaystyle g_{\\times}(\\mathbf{o}_{t},\\mathbf{m}_{t-1}) = \ud835\udc28 t \u200b \ud835\udc26 t \u2212 1 . absent subscript \ud835\udc28 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 \\displaystyle=\\mathbf{o}_{t}\\mathbf{m}_{t-1}. (7)\n\nFrom matrix dimensions, it is easy to see that\n\np = { d if \u200b \u03c8 = \u2299 k if \u200b \u03c8 = \u00d7 . \ud835\udc5d cases \ud835\udc51 if \ud835\udf13 direct-product \ud835\udc58 if \ud835\udf13 p=\\begin{cases}d&\\text{if }\\psi=\\odot\\\\\nk&\\text{if }\\psi=\\times.\\end{cases} (8)\n\nThe discussion is simplified by noting that, when is diagonalizable, which is a common assumption in practice (Gupta et al., 2022), can be reduced to . This is because , where , leads to:\n\ng \u00d7 \u200b ( \ud835\udc28 t , \ud835\udc26 t \u2212 1 ) = \ud835\udc28 t \u200b \ud835\udc26 t \u2212 1 = diag \u200b ( \ud835\udc28 \u00af t ) \u200b \ud835\udc26 t \u2212 1 subscript \ud835\udc54 subscript \ud835\udc28 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 subscript \ud835\udc28 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 diag subscript \u00af \ud835\udc28 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 \\displaystyle g_{\\times}(\\mathbf{o}_{t},\\mathbf{m}_{t-1})=\\mathbf{o}_{t}\\mathbf{m}_{t-1}=\\text{diag}(\\mathbf{\\bar{o}}_{t})\\mathbf{m}_{t-1} (9) = ( \ud835\udc28 \u00af t \u200b \ud835\udfcf { k } \u22a4 ) \u2299 \ud835\udc26 t \u2212 1 = g \u2299 \u200b ( \ud835\udc28 \u00af t \u200b \ud835\udfcf { k } \u22a4 , \ud835\udc26 t \u2212 1 ) absent direct-product subscript \u00af \ud835\udc28 \ud835\udc61 superscript subscript 1 \ud835\udc58 top subscript \ud835\udc26 \ud835\udc61 1 subscript \ud835\udc54 direct-product subscript \u00af \ud835\udc28 \ud835\udc61 superscript subscript 1 \ud835\udc58 top subscript \ud835\udc26 \ud835\udc61 1 \\displaystyle=\\left(\\mathbf{\\bar{o}}_{t}\\mathbf{1}_{\\{k\\}}^{\\top}\\right)\\odot\\mathbf{m}_{t-1}=g_{\\odot}(\\mathbf{\\bar{o}}_{t}\\mathbf{1}_{\\{k\\}}^{\\top},\\mathbf{m}_{t-1})\n\nTherefore, to simplify the discussion, we only consider the case where in the main text, which also includes the case when and is diagonalizable. We discuss a few examples where is not diagonalizable in the Appendix 5.2. 2.3 Examples\n\nTo effectively demonstrate our unifying LCSM framework, below we describe how various well-established linear complexity sequence modeling methods can be integrated into the framework. The correspondence is listed in Table 1. There are two different families of methods. In data-dependent methods, the expand, oscillation and shrink states are either fully or partially computed based on the input . Notationally, this is represented using the subscript , as in , , . Alternatively, in data-independent methods, these states are computed regardless of the input . Notationally, we drop the subscript in this case, leading to , , . S4 (Gu et al., 2022a). By setting , this method performs channel-wise mapping. The full mapping requires a bank of these single-input single-output (SISO) SSMs. In other words, the computation is repeated times, as in, , where . (Note that . And are obtained through SSM parameterization.)\n\nS5 (Smith et al., 2022). The recurrence equation of S5 is the same as S4, with the only difference being that the bank of SISO SSMs is replaced by a single multi-input multi-output (MIMO) SSM. The direct definition of the mapping is and . DSS (Gupta et al., 2022). The recurrence equation of DSS is same as S5, except that the computation is greatly simplified by diagonalizing . TNN (Qin et al., 2023a). Toeplitz neural networks (TNNs) exhibit competitive performance on long-range tasks with log-linear space-time complexity. As shown in (Qin & Zhong, 2023), a TNN can be losslessly converted to an SSM, where , get from through linear projection, and it can be expressed as a recursive formula:\n\n\ud835\udc26 t = \ud835\udc00\ud835\udc26 t \u2212 1 + \ud835\udc01\ud835\udc2e t \u22a4 , \ud835\udc32 t = \ud835\udc26 t \u22a4 \u200b \ud835\udc02 . formulae-sequence subscript \ud835\udc26 \ud835\udc61 subscript \ud835\udc00\ud835\udc26 \ud835\udc61 1 superscript subscript \ud835\udc01\ud835\udc2e \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript \ud835\udc26 \ud835\udc61 top \ud835\udc02 \\mathbf{m}_{t}=\\mathbf{A}\\mathbf{m}_{t-1}+\\mathbf{B}\\mathbf{u}_{t}^{\\top},\\\\\n\\quad\\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top}\\mathbf{C}. Linear Attention (Katharopoulos et al., 2020b). In Linear Attention, we obtain query , key , value from the input through linear projection, and recursively calculate as follows:\n\n[ \ud835\udc24\ud835\udc2f ] t = [ \ud835\udc24\ud835\udc2f ] t \u2212 1 + \ud835\udc24 t \u200b \ud835\udc2f t \u22a4 , \ud835\udc32 t = [ \ud835\udc24\ud835\udc2f ] t \u22a4 \u200b \ud835\udc2a t . formulae-sequence subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 1 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 top subscript \ud835\udc2a \ud835\udc61 [\\mathbf{kv}]_{t}=[\\mathbf{kv}]_{t-1}+\\mathbf{k}_{t}\\mathbf{v}_{t}^{\\top},\\\\\n\\quad\\mathbf{y}_{t}=[\\mathbf{kv}]_{t}^{\\top}\\mathbf{q}_{t}. TNL/RetNet (Qin et al., 2023b; Sun et al., 2023). TNL/RetNet is a form of Linear Attention with exponential decay, and the method for getting is the same as those in Linear Attention, and is a predefined parameter that cannot be learned. Its recursive calculation is:\n\n[ \ud835\udc24\ud835\udc2f ] t = \u03bb \u200b [ \ud835\udc24\ud835\udc2f ] t \u2212 1 + \ud835\udc24 t \u200b \ud835\udc2f t \u22a4 , \ud835\udc32 t = [ \ud835\udc24\ud835\udc2f ] t \u22a4 \u200b \ud835\udc2a t . formulae-sequence subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 \ud835\udf06 subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 1 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 top subscript \ud835\udc2a \ud835\udc61 [\\mathbf{kv}]_{t}=\\lambda[\\mathbf{kv}]_{t-1}+\\mathbf{k}_{t}\\mathbf{v}_{t}^{\\top},\\\\\n\\quad\\mathbf{y}_{t}=[\\mathbf{kv}]_{t}^{\\top}\\mathbf{q}_{t}. Mamba (Gu & Dao, 2023). Mamba can be seen as a data-dependent S4 with elementwise multiplication. It uses the similar method to get , the are computed through and . Its recurrence equation is defined as:\n\n\ud835\udc26 t = \ud835\udc00 t \u2299 \ud835\udc26 t \u2212 1 + \ud835\udc01 t \u200b \ud835\udc2e t \u22a4 , \ud835\udc32 t = \ud835\udc26 t \u22a4 \u200b \ud835\udc02 t . formulae-sequence subscript \ud835\udc26 \ud835\udc61 direct-product subscript \ud835\udc00 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 subscript \ud835\udc01 \ud835\udc61 superscript subscript \ud835\udc2e \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript \ud835\udc26 \ud835\udc61 top subscript \ud835\udc02 \ud835\udc61 \\mathbf{m}_{t}=\\mathbf{A}_{t}\\odot\\mathbf{m}_{t-1}+\\mathbf{B}_{t}\\mathbf{u}_{t}^{\\top},\\\\\n\\quad\\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top}\\mathbf{C}_{t}. RWKV4 (Peng et al., 2023). In RWKV4, we get through linear projection from input and as a learnable weight. Ignoring the denominator of RWKV4, the recurrence equation can be simplified as:\n\n\ud835\udc26 t = exp \u2061 ( \u2212 w ) \u200b \ud835\udc26 t \u2212 1 + exp \u2061 ( \ud835\udc24 t ) \u200b \ud835\udc2f t \u22a4 , \ud835\udc32 t = \ud835\udc26 t \u22a4 \u200b \ud835\udc2b t . formulae-sequence subscript \ud835\udc26 \ud835\udc61 \ud835\udc64 subscript \ud835\udc26 \ud835\udc61 1 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript \ud835\udc26 \ud835\udc61 top subscript \ud835\udc2b \ud835\udc61 \\mathbf{m}_{t}=\\exp(-w)\\mathbf{m}_{t-1}+\\exp(\\mathbf{k}_{t})\\mathbf{v}_{t}^{\\top},\\\\\n\\quad\\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top}\\mathbf{r}_{t}. Similar to S4, RWKV4 uses channel-wise mapping of . Cosformer (Qin et al., 2022). In Cosformer, we obtain query , key , value from the input and a prefined (not learnable). Then recursively calculate as follows:\n\n[ \ud835\udc24\ud835\udc2f ] t = exp \u2061 ( i \u200b \u03b8 ) \u200b [ \ud835\udc24\ud835\udc2f ] t \u2212 1 + \ud835\udc24 t \u200b \ud835\udc2f t \u22a4 , \ud835\udc32 t = Rel \u200b { [ \ud835\udc24\ud835\udc2f ] t } \u22a4 \u200b \ud835\udc2a t . formulae-sequence subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 \ud835\udc56 \ud835\udf03 subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 1 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 Rel superscript subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 top subscript \ud835\udc2a \ud835\udc61 [\\mathbf{kv}]_{t}=\\exp(i\\theta)[\\mathbf{kv}]_{t-1}+\\mathbf{k}_{t}\\mathbf{v}_{t}^{\\top},\\\\\n\\quad\\mathbf{y}_{t}=\\mathrm{Rel}\\{[\\mathbf{kv}]_{t}\\}^{\\top}\\mathbf{q}_{t}. We provide the proof in the Appendix 5.3. LRPE (Qin et al., 2023c). In LRPE, we obtain query , key , value from the input , as a learnable weight and recursively calculate as follows:\n\nt = \u039b \u200b [ \ud835\udc24\ud835\udc2f ] t \u2212 1 + \ud835\udc24 t \u200b \ud835\udc2f t \u22a4 , absent \u039b subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 1 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 top \\displaystyle=\\Lambda[\\mathbf{kv}]_{t-1}+\\mathbf{k}_{t}\\mathbf{v}_{t}^{\\top}, (10) \u039b \u039b \\displaystyle\\Lambda = diag \u200b { exp \u2061 ( i \u200b \u03b8 1 ) , \u2026 , exp \u2061 ( i \u200b \u03b8 k ) } , absent diag \ud835\udc56 subscript \ud835\udf03 1 \u2026 \ud835\udc56 subscript \ud835\udf03 \ud835\udc58 \\displaystyle=\\mathrm{diag}\\{\\exp(i\\theta_{1}),\\ldots,\\exp(i\\theta_{k})\\}, \ud835\udc32 t subscript \ud835\udc32 \ud835\udc61 \\displaystyle\\mathbf{y}_{t} = Rel \u200b { [ \ud835\udc24\ud835\udc2f ] t } \u22a4 \u200b \ud835\udc2a t .",
    "secretlinseqmod-3": "absent Rel superscript subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 top subscript \ud835\udc2a \ud835\udc61 \\displaystyle=\\mathrm{Rel}\\{[\\mathbf{kv}]_{t}\\}^{\\top}\\mathbf{q}_{t}. We provide the proof in the Appendix 5.3. GLA/GateLoop (Yang et al., 2023; Katsch, 2024). In GLA/GateLoop, we obtain query , key , value , decay from the input and recursively calculate as follows:\n\n[ \ud835\udc24\ud835\udc2f ] t = Diag \u200b { \ud835\udc20 t } \u200b [ \ud835\udc24\ud835\udc2f ] t \u2212 1 + \ud835\udc24 t \u200b \ud835\udc2f t \u22a4 , \ud835\udc32 t = [ \ud835\udc24\ud835\udc2f ] t \u22a4 \u200b \ud835\udc2a t . formulae-sequence subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 Diag subscript \ud835\udc20 \ud835\udc61 subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 1 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 top subscript \ud835\udc2a \ud835\udc61 [\\mathbf{kv}]_{t}=\\mathrm{Diag}\\{\\mathbf{g}_{t}\\}[\\mathbf{kv}]_{t-1}+\\mathbf{k}_{t}\\mathbf{v}_{t}^{\\top},\\quad\\mathbf{y}_{t}=[\\mathbf{kv}]_{t}^{\\top}\\mathbf{q}_{t}. DUR/GFW (Mao, 2022; Schlag & Schmidhuber, 2018) In DUR/GFW, we obtain query , key , value , decay from the input , and recursively calculate as follows:\n\n[ \ud835\udc24\ud835\udc2f ] t = ( \ud835\udc20 t \u200b \ud835\udc20 \u00af t \u22a4 ) \u2299 [ \ud835\udc24\ud835\udc2f ] t \u2212 1 + \ud835\udc24 t \u200b \ud835\udc2f t \u22a4 , \ud835\udc32 t = [ \ud835\udc24\ud835\udc2f ] t \u22a4 \u200b \ud835\udc2a t . formulae-sequence subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 direct-product subscript \ud835\udc20 \ud835\udc61 superscript subscript \u00af \ud835\udc20 \ud835\udc61 top subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 1 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 top subscript \ud835\udc2a \ud835\udc61 [\\mathbf{kv}]_{t}=(\\mathbf{g}_{t}\\bar{\\mathbf{g}}_{t}^{\\top})\\odot[\\mathbf{kv}]_{t-1}+\\mathbf{k}_{t}\\mathbf{v}_{t}^{\\top},\\quad\\mathbf{y}_{t}=[\\mathbf{kv}]_{t}^{\\top}\\mathbf{q}_{t}. HGRN/LRN (Qin et al., 2023d; Martin & Cundy, 2018). In HGRN/LRN, we obtain output gate , forget gate , input state from the input , and recursively calculate as follows:\n\n\ud835\udc21 t = \ud835\udc1f t \u2299 \ud835\udc21 t \u2212 1 + ( 1 \u2212 \ud835\udc1f t ) \u200b \ud835\udc22 t \u22a4 , \ud835\udc32 t = \ud835\udc21 t \u22a4 \u200b \ud835\udc28 t . formulae-sequence subscript \ud835\udc21 \ud835\udc61 direct-product subscript \ud835\udc1f \ud835\udc61 subscript \ud835\udc21 \ud835\udc61 1 1 subscript \ud835\udc1f \ud835\udc61 superscript subscript \ud835\udc22 \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript \ud835\udc21 \ud835\udc61 top subscript \ud835\udc28 \ud835\udc61 \\mathbf{h}_{t}=\\mathbf{f}_{t}\\odot\\mathbf{h}_{t-1}+(1-\\mathbf{f}_{t})\\mathbf{i}_{t}^{\\top},\\quad\\mathbf{y}_{t}=\\mathbf{h}_{t}^{\\top}\\mathbf{o}_{t}. Similar to S4, HGRN/LRN uses channel-wise mapping of . 2.4 State Calculation\n\nIn the preceding section, it was demonstrated that various sequence modeling methods employ different approaches to calculate the EOS states. Here, we summarize possible calculation strategies, including methods that have been previously adopted and those that have not been utilized before. Input state. For input state , We obtain it through a linear projection from input . Parameterization. Two parameterization strategies are commonly utilized. The first involves employing a state space model to derive the parameters (Gu et al., 2022b), while the second entails learning the parameters from data through linear projection (Katharopoulos et al., 2020a). Data Dependency. As indicated in Table 1, it is shown that the EOS states of certain methods are dependent on the input state. Specifically, methods with a subscript in their EOS states exhibit this dependency, while those without a subscript do not. For data-dependent cases, we assume that each element of the oscillation state belongs to and is calculated using (Yang et al., 2023), where the hyper-parameter controls the oscillation rate. For data-independent cases, we use the method of initialization with Alibi (Press et al., 2022). Construction Methods of Oscillation State. The Oscillation state exhibits the most significant variations among the three states. Here, we select 12 distinct scenarios within the Oscillation state, which include the utilization of complex numbers, non-learnable data-independent situations, and the all-ones scenario. We list the following possibilities through the form of Einstein Summation in Table 3. Activation Function Test. In a manner akin to the kernel functions utilized for processing queries and keys in Linear Transformers (Katharopoulos et al., 2020b), activation functions are also employed to manipulate the Expand and Shrink states. To assess the efficacy of various activation functions, we conducted tests on several widely-used options.",
    "secretlinseqmod-4": "We list them in Table 2. Tau Test. controls the oscillation rate. We also ablate the efficacy of in controlling the oscillation rate. To avoid the oscillation state becoming too small, we used the form following (Yang et al., 2023). In this test, we varied the value of , with the default value being following (Yang et al., 2023). 2.5 Model Code\n\nFor the ease of representing the model variants, we use model codes to classify different variants. Given the following variables: parameterization type, expand type, oscillation type, shrink type, activation function type, we use the format \u2018e-o-s-a\u2018 to denote the model code. We use a separate code 0 to denote SSM parameterization methods like Mamba, and S4, whereas other codes represent linear parameterization. In this context, and take values of 0 or 1, representing the expand type and shrink type, where 0 indicates data independence and 1 indicates data dependence. The variable ranges from 0 to 11 and denotes the method of constructing the oscillation state, with the complete table detailed in Table 3. Similarly, the variable ranges from 0 to 7 and indicates the type of activation function, with the complete table shown in Table 2. For instance, a setting of \u201c0-0-0-0\u201d signifies the use of linear parameterization, with all states being data independent and no activation function being utilized. 3 Experiments\n\nWe perform extensive experiments to evaluate the model variants in language modeling performance and long-context recall capabilities on WikiText-103 (Merity et al., 2017) dataset and the multi-query associative recall (MQAR) task (Arora et al., 2023). The experiment setups are delineated below. For the convergence experiments on WikiText-103, we adhered to the configuration outlined in prior studies (Qin et al., 2023a). Specifically, we employed a batch size of 128, and a sequence length of 512, and ran the experiments for 50,000 iterations. The learning rate was set to 5e-4, with complete hyperparameters listed in Appendix 6. For the MQAR task, we standardized the expand ratio for all models to 128. We conducted a grid search for the learning rate, spanning values from {1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2}, and reported the best results for each model. Additionally, we tested various sequence lengths 32, 64, 128, 256, and feature dimensions 64, 128, 256, 512 to determine the optimal configuration. 3.1 Experiments on Wikitext-103\n\nParameterization Test. We carried out a comparison between SSM parameterization and naive parameterization, with the findings presented in Table 5. The analysis revealed that SSM parameterization showed inferior performance compared to naive parameterization on the Wikitext dataset. Data Dependence Test. We proceeded to assess the extent of dependency on data-dependent states, as shown in Table 4. Analysis of the Wikitext data indicates that when all EOS states () are non-data-dependent, the performance is notably subpar, even worse than the scenario where the state is not learnable. Introducing data dependence on just one of these elements, particularly on the state (), demonstrates the most significant effectiveness. When data dependence is introduced on two elements, the combination of and () proves to be the most effective, yielding results comparable to scenarios where , , and are all data-dependent. O Types Test. In Table 4, we evaluated various strategies for constructing the state. A method we labeled as , yielded the most favorable outcomes. This approach outperformed the configuration. On the other hand, methods that relied on a naive state (such as linear attention (Katharopoulos et al., 2020b)) demonstrated inferior performance. Activation Functions Test. Then we examined the influence of activation functions on the outcomes, results are presented in Table 6. It\u2019s clear that employing activation functions yields considerable benefits, with and demonstrating the most favorable performance. Tau Values Test. Finally we explored the determination of tau values for the configuration, results are also listed in Table 6. On the WikiTest-103 task, selecting tau=8 resulted in the most favorable outcomes. 3.2 Experiments on MQAR\n\nParameterization Test. In Fig.3, a comparative analysis was carried out between SSM parameterization and naive parameterization, illustrating that naive parameterization achieves inferior performance in the MQAR task. Data Dependence Test. We proceeded to evaluate the reliance on data-dependent states, as shown in Fig. 4. In the MQAR task, the majority of configurations resulted in subpar performance, with the exception of the and configurations, which showed satisfactory results for sequence lengths up to 128. Nonetheless, when the sequence length was increased to 256, solely the configuration managed to converge successfully.",
    "secretlinseqmod-5": "O Types Test. In Fig. 5, we conducted a comparison of various methods for constructing state. Within the context of the MQAR task, the performance of the complex variant stood out as particularly favorable, surpassing other variants significantly. Activation Functions Test. In Figure 6, we explored the impact of activation functions. Employing activation functions resulted in a significant improvement in performance, particularly with longer sequence lengths. Additionally, the activation function surpassed the performance of other activation functions. Tau Values Test. We delved into establishing the tau values for the configuration. As shown in Fig. 7, selecting a larger , (thus a large state) led to enhanced outcomes for MQAR task, which aligns with the capability for long document retrieval. 4 Conclusion\n\nThis paper introduced the Linear Complexity Sequence Model (LCSM), a unified framework that synergizes various linear complexity sequence modeling techniques. Our exploration of the LCSM\u2019s three integral stages\u2014Expand, Oscillation, and Shrink (EOS)\u2014and the role of the Memory state has unveiled the critical impact of data dependency and parameterization on model performance. The LCSM framework not only enhances our understanding of the operational dynamics of linear complexity models but also opens avenues for future research to further refine and adapt these models for improved efficiency and effectiveness across a broader spectrum of sequence modeling challenges.",
    "secretlinseqmod-6": "References\n\nArora et al. (2023) Arora, S., Eyuboglu, S., Timalsina, A., Johnson, I., Poli, M., Zou, J., Rudra, A., and R\u00e9, C. Zoology: Measuring and improving recall in efficient language models. CoRR, abs/2312.04927, 2023. Choromanski et al. (2021) Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D.",
    "secretlinseqmod-7": "B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=Ua6zuk0WRH. Fu et al. (2023a) Fu, D. Y., Epstein, E. L., Nguyen, E., Thomas, A. W., Zhang, M., Dao, T., Rudra, A., and R\u00e9, C. Simple hardware-efficient long convolutions for sequence modeling. CoRR, abs/2302.06646, 2023a. doi: 10.48550/arXiv.2302.06646. URL https://doi.org/10.48550/arXiv.2302.06646. Fu et al. (2023b) Fu, D. Y., Kumbong, H., Nguyen, E., and R\u00e9, C. FlashFFTConv: Efficient convolutions for long sequences with tensor cores.",
    "secretlinseqmod-8": "2023b. Gu & Dao (2023) Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2023. Gu et al. (2022a) Gu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022a. Gu et al. (2022b) Gu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022b. URL https://openreview.net/forum?id=uYLFoz1vlAC. Gupta et al. (2022) Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces.",
    "secretlinseqmod-9": "In NeurIPS, 2022. Katharopoulos et al. (2020a) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "secretlinseqmod-10": "In International Conference on Machine Learning, pp. 5156\u20135165. PMLR, 2020a. Katharopoulos et al. (2020b) Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "secretlinseqmod-11": "In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5156\u20135165. PMLR, 2020b. URL http://proceedings.mlr.press/v119/katharopoulos20a.html. Katsch (2024) Katsch, T. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2024. URL https://openreview.net/forum?id=02Ug9N8DCI. Mao (2022) Mao, H. H. Fine-tuning pre-trained transformers into decaying fast weights, 2022. Martin & Cundy (2018) Martin, E. and Cundy, C. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HyUNwulC-. Merity et al. (2017) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models.",
    "secretlinseqmod-12": "5th International Conference on Learning Representations, ICLR, Toulon, France, 2017. Orvieto et al. (2023) Orvieto, A., Smith, S. L., Gu, A., Fernando, A., G\u00fcl\u00e7ehre, \u00c7., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. doi: 10.48550/arXiv.2303.06349. URL https://doi.org/10.48550/arXiv.2303.06349. Peng et al. (2023) Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., He, X., Hou, H., Kazienko, P., Kocon, J., Kong, J., Koptyra, B., Lau, H., Mantri, K. S. I., Mom, F., Saito, A., Tang, X., Wang, B., Wind, J. S., Wozniak, S., Zhang, R., Zhang, Z., Zhao, Q., Zhou, P., Zhu, J., and Zhu, R.-J. Rwkv: Reinventing rnns for the transformer era, 2023. Poli et al. (2023) Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and R\u00e9, C. Hyena hierarchy: Towards larger convolutional language models. CoRR, abs/2302.10866, 2023. doi: 10.48550/arXiv.2302.10866. URL https://doi.org/10.48550/arXiv.2302.10866. Press et al. (2022) Press, O., Smith, N., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation.",
    "secretlinseqmod-13": "In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Qin & Zhong (2023) Qin, Z. and Zhong, Y. Accelerating toeplitz neural network with constant-time inference complexity. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, December 2023. Qin et al. (2022) Qin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and Zhong, Y. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=Bl8CQrx2Up4. Qin et al. (2023a) Qin, Z., Han, X., Sun, W., He, B., Li, D., Li, D., Dai, Y., Kong, L., and Zhong, Y. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=IxmWsm4xrua. Qin et al. (2023b) Qin, Z., Li, D., Sun, W., Sun, W., Shen, X., Han, X., Wei, Y., Lv, B., Yuan, F., Luo, X., et al. Scaling transnormer to 175 billion parameters.",
    "secretlinseqmod-14": "arXiv preprint arXiv:2307.14995, 2023b. Qin et al. (2023c) Qin, Z., Sun, W., Lu, K., Deng, H., Li, D., Han, X., Dai, Y., Kong, L., and Zhong, Y. Linearized relative positional encoding.",
    "secretlinseqmod-15": "Transactions on Machine Learning Research, 2023c. Qin et al. (2023d) Qin, Z., Yang, S., and Zhong, Y. Hierarchically gated recurrent neural network for sequence modeling. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023d. Schlag & Schmidhuber (2018) Schlag, I. and Schmidhuber, J. GATED FAST WEIGHTS FOR ASSOCIATIVE RETRIEVAL, 2018.",
    "secretlinseqmod-16": "URL https://openreview.net/forum?id=HJ8W1Q-0Z. Schlag et al. (2021) Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers, 2021.",
    "secretlinseqmod-17": "Smith et al. (2022) Smith, J. T.",
    "secretlinseqmod-18": "H., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling. CoRR, abs/2208.04933, 2022. doi: 10.48550/arXiv.2208.04933. URL https://doi.org/10.48550/arXiv.2208.04933. Sun et al. (2023) Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yang et al. (2023) Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. CoRR, abs/2312.06635, 2023.",
    "secretlinseqmod-19": "doi: 10.48550/ARXIV.2312.06635. URL https://doi.org/10.48550/arXiv.2312.06635. 5 Appendix\n\n5.1 Backward Pass\n\nIn the main text, we only defined the forward pass of LCSM. Here, we define the backward pass of LCSM. For convenience, we will refer to the case where () as Type1, and the case where () as Type2. Type1. The forward pass of Type 1 is:\n\n\ud835\udc26 t = \ud835\udc1f t \u2299 \ud835\udc26 t \u2212 1 + \ud835\udc1e t \u200b \ud835\udc22 t \u22a4 , \ud835\udc32 t = \ud835\udc26 t \u22a4 \u200b \ud835\udc2c t . formulae-sequence subscript \ud835\udc26 \ud835\udc61 direct-product subscript \ud835\udc1f \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 subscript \ud835\udc1e \ud835\udc61 superscript subscript \ud835\udc22 \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript \ud835\udc26 \ud835\udc61 top subscript \ud835\udc2c \ud835\udc61 \\mathbf{m}_{t}=\\mathbf{f}_{t}\\odot\\mathbf{m}_{t-1}+\\mathbf{e}_{t}\\mathbf{i}_{t}^{\\top},\\\\\n\\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top}\\mathbf{s}_{t}. So the backward pass can be computed as follows:\n\n\ud835\udc1d\ud835\udc2c t subscript \ud835\udc1d\ud835\udc2c \ud835\udc61 \\displaystyle\\mathbf{ds}_{t} = \ud835\udc26 t \u200b \ud835\udc1d\ud835\udc32 \ud835\udc2d \u2208 \u211d k , absent subscript \ud835\udc26 \ud835\udc61 subscript \ud835\udc1d\ud835\udc32 \ud835\udc2d superscript \u211d \ud835\udc58 \\displaystyle=\\mathbf{m}_{t}\\mathbf{dy_{t}}\\in\\mathbb{R}^{k}, (11) \ud835\udc1d\ud835\udc26 t \u2212 1 subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 1 \\displaystyle\\mathbf{dm}_{t-1} = \ud835\udc1f t \u2299 \ud835\udc1d\ud835\udc26 t + \ud835\udc2c t \u2212 1 \u200b \ud835\udc1d\ud835\udc32 t \u2212 1 \u22a4 \u2208 \u211d k \u00d7 d , absent direct-product subscript \ud835\udc1f \ud835\udc61 subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 subscript \ud835\udc2c \ud835\udc61 1 superscript subscript \ud835\udc1d\ud835\udc32 \ud835\udc61 1 top superscript \u211d \ud835\udc58 \ud835\udc51 \\displaystyle=\\mathbf{f}_{t}\\odot\\mathbf{dm}_{t}+\\mathbf{s}_{t-1}\\mathbf{dy}_{t-1}^{\\top}\\in\\mathbb{R}^{k\\times d}, \ud835\udc1d\ud835\udc1f t subscript \ud835\udc1d\ud835\udc1f \ud835\udc61 \\displaystyle\\mathbf{df}_{t} = \ud835\udc1d\ud835\udc26 t \u2299 \ud835\udc26 t \u2208 \u211d k \u00d7 d , absent direct-product subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 superscript \u211d \ud835\udc58 \ud835\udc51 \\displaystyle=\\mathbf{dm}_{t}\\odot\\mathbf{m}_{t}\\in\\mathbb{R}^{k\\times d}, \ud835\udc1d\ud835\udc1e t subscript \ud835\udc1d\ud835\udc1e \ud835\udc61 \\displaystyle\\mathbf{de}_{t} = \ud835\udc1d\ud835\udc26 t \u200b \ud835\udc22 t \u2208 \u211d k , absent subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 subscript \ud835\udc22 \ud835\udc61 superscript \u211d \ud835\udc58 \\displaystyle=\\mathbf{dm}_{t}\\mathbf{i}_{t}\\in\\mathbb{R}^{k}, \ud835\udc1d\ud835\udc22 t subscript \ud835\udc1d\ud835\udc22 \ud835\udc61 \\displaystyle\\mathbf{di}_{t} = \ud835\udc1d\ud835\udc26 t \u22a4 \u200b \ud835\udc1e t \u2208 \u211d d . absent superscript subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 top subscript \ud835\udc1e \ud835\udc61 superscript \u211d \ud835\udc51 \\displaystyle=\\mathbf{dm}_{t}^{\\top}\\mathbf{e}_{t}\\in\\mathbb{R}^{d}. Type2. The forward pass of Type2 is:\n\n\ud835\udc26 t = \ud835\udc1f t \u200b \ud835\udc26 t \u2212 1 + \ud835\udc1e t \u200b \ud835\udc22 t \u22a4 , \ud835\udc32 t = \ud835\udc26 t \u22a4 \u200b \ud835\udc2c t . formulae-sequence subscript \ud835\udc26 \ud835\udc61 subscript \ud835\udc1f \ud835\udc61 subscript \ud835\udc26 \ud835\udc61 1 subscript \ud835\udc1e \ud835\udc61 superscript subscript \ud835\udc22 \ud835\udc61 top subscript \ud835\udc32 \ud835\udc61 superscript subscript \ud835\udc26 \ud835\udc61 top subscript \ud835\udc2c \ud835\udc61 \\mathbf{m}_{t}=\\mathbf{f}_{t}\\mathbf{m}_{t-1}+\\mathbf{e}_{t}\\mathbf{i}_{t}^{\\top},\\\\\n\\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top}\\mathbf{s}_{t}. So the backward pass can be computed as follows:\n\n\ud835\udc1d\ud835\udc2c t subscript \ud835\udc1d\ud835\udc2c \ud835\udc61 \\displaystyle\\mathbf{ds}_{t} = \ud835\udc26 t \u200b \ud835\udc1d\ud835\udc32 \ud835\udc2d \u2208 \u211d k , absent subscript \ud835\udc26 \ud835\udc61 subscript \ud835\udc1d\ud835\udc32 \ud835\udc2d superscript \u211d \ud835\udc58 \\displaystyle=\\mathbf{m}_{t}\\mathbf{dy_{t}}\\in\\mathbb{R}^{k}, (12) \ud835\udc1d\ud835\udc26 t \u2212 1 subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 1 \\displaystyle\\mathbf{dm}_{t-1} = \ud835\udc1f t \u200b \ud835\udc1d\ud835\udc26 t + \ud835\udc2c t \u2212 1 \u200b \ud835\udc1d\ud835\udc32 t \u2212 1 \u22a4 \u2208 \u211d k \u00d7 d , absent subscript \ud835\udc1f \ud835\udc61 subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 subscript \ud835\udc2c \ud835\udc61 1 superscript subscript \ud835\udc1d\ud835\udc32 \ud835\udc61 1 top superscript \u211d \ud835\udc58 \ud835\udc51 \\displaystyle=\\mathbf{f}_{t}\\mathbf{dm}_{t}+\\mathbf{s}_{t-1}\\mathbf{dy}_{t-1}^{\\top}\\in\\mathbb{R}^{k\\times d}, \ud835\udc1d\ud835\udc1f t subscript \ud835\udc1d\ud835\udc1f \ud835\udc61 \\displaystyle\\mathbf{df}_{t} = \ud835\udc1d\ud835\udc26 t \u200b \ud835\udc26 t \u22a4 \u2208 \u211d k \u00d7 k , absent subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 superscript subscript \ud835\udc26 \ud835\udc61 top superscript \u211d \ud835\udc58 \ud835\udc58 \\displaystyle=\\mathbf{dm}_{t}\\mathbf{m}_{t}^{\\top}\\in\\mathbb{R}^{k\\times k}, \ud835\udc1d\ud835\udc1e t subscript \ud835\udc1d\ud835\udc1e \ud835\udc61 \\displaystyle\\mathbf{de}_{t} = \ud835\udc1d\ud835\udc26 t \u200b \ud835\udc22 t \u2208 \u211d k , absent subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 subscript \ud835\udc22 \ud835\udc61 superscript \u211d \ud835\udc58 \\displaystyle=\\mathbf{dm}_{t}\\mathbf{i}_{t}\\in\\mathbb{R}^{k}, \ud835\udc1d\ud835\udc22 t subscript \ud835\udc1d\ud835\udc22 \ud835\udc61 \\displaystyle\\mathbf{di}_{t} = \ud835\udc1d\ud835\udc26 t \u22a4 \u200b \ud835\udc1e t \u2208 \u211d d . absent superscript subscript \ud835\udc1d\ud835\udc26 \ud835\udc61 top subscript \ud835\udc1e \ud835\udc61 superscript \u211d \ud835\udc51 \\displaystyle=\\mathbf{dm}_{t}^{\\top}\\mathbf{e}_{t}\\in\\mathbb{R}^{d}. 5.2 More discussion about\n\nIn the main text, we primarily discuss the case of and assume that is diagonalizable when . In this section, we will discuss the more general case of . that is, the non-diagonalizable case, which we will list in Table 7. FWP (Schlag et al., 2021)\n\n\ud835\udc16 t subscript \ud835\udc16 \ud835\udc61 \\displaystyle\\mathbf{W}_{t} = \ud835\udc16 t \u2212 1 + \u03b2 t \u200b \ud835\udc24 t \u200b ( \ud835\udc2f t \u2212 \ud835\udc2f \u00af t ) \u22a4 absent subscript \ud835\udc16 \ud835\udc61 1 subscript \ud835\udefd \ud835\udc61 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 subscript \u00af \ud835\udc2f \ud835\udc61 top \\displaystyle=\\mathbf{W}_{t-1}+\\beta_{t}\\mathbf{k}_{t}(\\mathbf{v}_{t}-\\bar{\\mathbf{v}}_{t})^{\\top} (13) = \ud835\udc16 t \u2212 1 + \u03b2 t \u200b \ud835\udc24 t \u200b ( \ud835\udc2f t \u2212 \ud835\udc16 t \u2212 1 \u22a4 \u200b \ud835\udc24 t ) \u22a4 absent subscript \ud835\udc16 \ud835\udc61 1 subscript \ud835\udefd \ud835\udc61 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 superscript subscript \ud835\udc16 \ud835\udc61 1 top subscript \ud835\udc24 \ud835\udc61 top \\displaystyle=\\mathbf{W}_{t-1}+\\beta_{t}\\mathbf{k}_{t}(\\mathbf{v}_{t}-\\mathbf{W}_{t-1}^{\\top}\\mathbf{k}_{t})^{\\top} = ( \ud835\udc08 k \u2212 \u03b2 t \u200b \ud835\udc24 t \u200b \ud835\udc24 t \u22a4 ) \u200b \ud835\udc16 t \u2212 1 + \u03b2 t \u200b \ud835\udc24 t \u200b \ud835\udc2f t \u22a4 absent subscript \ud835\udc08 \ud835\udc58 subscript \ud835\udefd \ud835\udc61 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc24 \ud835\udc61 top subscript \ud835\udc16 \ud835\udc61 1 subscript \ud835\udefd \ud835\udc61 subscript \ud835\udc24 \ud835\udc61 superscript subscript \ud835\udc2f \ud835\udc61 top \\displaystyle=(\\mathbf{I}_{k}-\\beta_{t}\\mathbf{k}_{t}\\mathbf{k}_{t}^{\\top})\\mathbf{W}_{t-1}+\\beta_{t}\\mathbf{k}_{t}\\mathbf{v}_{t}^{\\top} \ud835\udc32 t subscript \ud835\udc32 \ud835\udc61 \\displaystyle\\mathbf{y}_{t} = \ud835\udc16 t \u22a4 \u200b \ud835\udc2a t . absent superscript subscript \ud835\udc16 \ud835\udc61 top subscript \ud835\udc2a \ud835\udc61 \\displaystyle=\\mathbf{W}_{t}^{\\top}\\mathbf{q}_{t}. Based on the aforementioned forms, we propose several new forms for future research in Table 7,\n\n5.3 Prove the equivalent expression of Cosformer and LRPE. Cosformer. [ \ud835\udc24\ud835\udc2f ] t subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 \\displaystyle{[\\mathbf{kv}]_{t}} = \u2211 s = 1 t \u039b t \u2212 s \u200b \ud835\udc24 s \u200b \ud835\udc2f s \u22a4 absent superscript subscript \ud835\udc60 1 \ud835\udc61 superscript \u039b \ud835\udc61 \ud835\udc60 subscript \ud835\udc24 \ud835\udc60 superscript subscript \ud835\udc2f \ud835\udc60 top \\displaystyle=\\sum_{s=1}^{t}\\Lambda^{t-s}\\mathbf{k}_{s}\\mathbf{v}_{s}^{\\top} (14) \ud835\udc32 t subscript \ud835\udc32 \ud835\udc61 \\displaystyle\\mathbf{y}_{t} = Rel \u200b { [ \ud835\udc24\ud835\udc2f t ] } \u22a4 \u200b \ud835\udc2a t absent Rel superscript delimited-[] subscript \ud835\udc24\ud835\udc2f \ud835\udc61 top subscript \ud835\udc2a \ud835\udc61 \\displaystyle=\\mathrm{Rel}\\{[\\mathbf{kv}_{t}]\\}^{\\top}\\mathbf{q}_{t} = Rel \u200b { \u2211 s = 1 t \u039b t \u2212 s \u200b \ud835\udc2f s \u200b \ud835\udc24 s \u22a4 \u200b \ud835\udc2a t } absent Rel superscript subscript \ud835\udc60 1 \ud835\udc61 superscript \u039b \ud835\udc61 \ud835\udc60 subscript \ud835\udc2f \ud835\udc60 superscript subscript \ud835\udc24 \ud835\udc60 top subscript \ud835\udc2a \ud835\udc61 \\displaystyle=\\mathrm{Rel}\\left\\{\\sum_{s=1}^{t}\\Lambda^{t-s}\\mathbf{v}_{s}\\mathbf{k}_{s}^{\\top}\\mathbf{q}_{t}\\right\\} = \u2211 s = 1 t \ud835\udc2f s \u200b \ud835\udc24 s \u22a4 \u200b \u039b \u00af t \u2212 s \u200b \ud835\udc2a t absent superscript subscript \ud835\udc60 1 \ud835\udc61 subscript \ud835\udc2f \ud835\udc60 superscript subscript \ud835\udc24 \ud835\udc60 top subscript \u00af \u039b \ud835\udc61 \ud835\udc60 subscript \ud835\udc2a \ud835\udc61 \\displaystyle=\\sum_{s=1}^{t}\\mathbf{v}_{s}\\mathbf{k}_{s}^{\\top}\\bar{\\Lambda}_{t-s}\\mathbf{q}_{t} \u039b \u00af t \u2212 s subscript \u00af \u039b \ud835\udc61 \ud835\udc60 \\displaystyle\\bar{\\Lambda}_{t-s} = diag \u200b { cos \u2061 ( ( t \u2212 s ) \u200b \u03b8 1 ) , \u2026 , cos \u2061 ( ( t \u2212 s ) \u200b \u03b8 k ) } . absent diag \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 1 \u2026 \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 \ud835\udc58 \\displaystyle=\\mathrm{diag}\\{\\cos((t-s)\\theta_{1}),\\ldots,\\cos((t-s)\\theta_{k})\\}. LRPE. [ \ud835\udc24\ud835\udc2f ] t subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 \\displaystyle{[\\mathbf{kv}]_{t}} = \u2211 s = 1 t exp \u2061 ( i \u200b ( t \u2212 s ) \u200b \u03b8 ) \u200b \ud835\udc24 s \u200b \ud835\udc2f s \u22a4 absent superscript subscript \ud835\udc60 1 \ud835\udc61 \ud835\udc56 \ud835\udc61 \ud835\udc60 \ud835\udf03 subscript \ud835\udc24 \ud835\udc60 superscript subscript \ud835\udc2f \ud835\udc60 top \\displaystyle=\\sum_{s=1}^{t}\\exp(i(t-s)\\theta)\\mathbf{k}_{s}\\mathbf{v}_{s}^{\\top} (15) \ud835\udc32 t subscript \ud835\udc32 \ud835\udc61 \\displaystyle\\mathbf{y}_{t} = Rel \u200b { [ \ud835\udc24\ud835\udc2f ] t } \u22a4 \u200b \ud835\udc2a t absent Rel superscript subscript delimited-[] \ud835\udc24\ud835\udc2f \ud835\udc61 top subscript \ud835\udc2a \ud835\udc61 \\displaystyle=\\mathrm{Rel}\\{[\\mathbf{kv}]_{t}\\}^{\\top}\\mathbf{q}_{t} = Rel \u200b { \u2211 s = 1 t exp \u2061 ( i \u200b ( t \u2212 s ) \u200b \u03b8 ) \u200b \ud835\udc2f s \u200b \ud835\udc24 s \u22a4 \u200b \ud835\udc2a t } absent Rel superscript subscript \ud835\udc60 1 \ud835\udc61 \ud835\udc56 \ud835\udc61 \ud835\udc60 \ud835\udf03 subscript \ud835\udc2f \ud835\udc60 superscript subscript \ud835\udc24 \ud835\udc60 top subscript \ud835\udc2a \ud835\udc61 \\displaystyle=\\mathrm{Rel}\\left\\{\\sum_{s=1}^{t}\\exp(i(t-s)\\theta)\\mathbf{v}_{s}\\mathbf{k}_{s}^{\\top}\\mathbf{q}_{t}\\right\\} = \u2211 s = 1 t cos \u2061 ( ( t \u2212 s ) \u200b \u03b8 ) \u200b \ud835\udc2f s \u200b \ud835\udc24 s \u22a4 \u200b \ud835\udc2a t . absent superscript subscript \ud835\udc60 1 \ud835\udc61 \ud835\udc61 \ud835\udc60 \ud835\udf03 subscript \ud835\udc2f \ud835\udc60 superscript subscript \ud835\udc24 \ud835\udc60 top subscript \ud835\udc2a \ud835\udc61 \\displaystyle=\\sum_{s=1}^{t}\\cos((t-s)\\theta)\\mathbf{v}_{s}\\mathbf{k}_{s}^{\\top}\\mathbf{q}_{t}. 6 Configurations\n\n\u25c4 Feeling lucky? Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Jun 5 16:22:07 2024 by LaTeXML"
}