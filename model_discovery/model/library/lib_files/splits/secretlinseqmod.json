{
    "secretlinseqmod-0": "# Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective \n\nZhen Qin ${ }^{1}$ Xuyang Shen ${ }^{2}$ Dong Li ${ }^{2}$ Weigao Sun ${ }^{2}$ Stan Birchfield ${ }^{3}$ Richard Hartley ${ }^{4}$ Yiran Zhong ${ }^{2}$\n\n\n#### Abstract\n\nWe present the Linear Complexity Sequence Model (LCSM), a comprehensive solution that unites various sequence modeling techniques with linear complexity, including linear attention, state space model, long convolution, and linear RNN, within a single framework.",
    "secretlinseqmod-1": "The goal is to enhance comprehension of these models by analyzing the impact of each component from a cohesive and streamlined viewpoint. Specifically, we segment the modeling processes of these models into three distinct stages: Expand, Oscillation, and Shrink (EOS), with each model having its own specific settings. The Expand stage involves projecting the input signal onto a high-dimensional memory state. This is followed by recursive operations performed on the memory state in the Oscillation stage. Finally, the memory state is projected back to a low-dimensional space in the Shrink stage. We perform comprehensive experiments to analyze the impact of different stage settings on language modeling and retrieval tasks. Our results show that data-driven methods are crucial for the effectiveness of the three stages in language modeling, whereas hand-crafted methods yield better performance in retrieval tasks. ## 1. Introduction\n\nTransformers (Vaswani et al., 2017) have revolutionized deep learning in recent years. Nevertheless, the escalating demand for extra-long sequence modeling and the abundance of training data have necessitated the development of sequence modeling techniques beyond the original formulation. In particular, recent years have witnessed the increased development of linear complexity sequence modeling techniques. Various advanced efficient approaches have emerged lately, exhibiting notable performance improve-\n\n[^0]Preliminary work., Copyright 2024 by the author(s). ments and narrowing the performance gap with transformerbased methods (Qin et al., 2023b; Gu \\& Dao, 2023; Yang et al., 2023). These methods can be categorized into four primary groups. 1) Linear attention (Katharopoulos et al., 2020b; Choromanski et al., 2021) involves a series of techniques aimed at computing attention matrices using the \"right-product kernel trick.\" This approach computes the key-value production first, thereby avoiding quadratic query-key production. Recent studies (Qin et al., 2023b; Sun et al., 2023) have shown that Linear attention can achieve performance levels comparable to softmax attention while significantly improving speed. 2) State Space Model (Gu et al., 2022a; Smith et al., 2022) (SSM) encompasses methods that utilize State Space Equations for sequence modeling. Through the use of special initialization, diagonalization assumptions, and mixed techniques, SSM methods can achieve performance levels comparable to softmax attention methods in language modeling (Gu \\& Dao, 2023). 3) Long convolution (Qin et al., 2023a; Poli et al., 2023) models adopt the concept of CNN but utilize a kernel with a size equal to the input sequence length. Fast Fourier Transforms (FFT) are employed to reduce computational complexities to log-linear (Fu et al., 2023a;b). However, the language modeling performance of these methods still falls short of that of transformers. 4) Linear RNNs (Qin et al., 2023d; Orvieto et al., 2023) enable parallel training of the RNN, and demonstrate competitive performance against similarly scaled Transformers. Recent research suggests that there are inherent connections between these methodologies. For instance, Long convolution models can be transformed into SSM models during inference using a closed-form solution (Qin \\& Zhong, 2023). Similarly, linear attention and SSM models can conduct inference in an RNN-style fashion (Katharopoulos et al., 2020b). By delving into the intrinsic connections, in this paper, we present a unified framework called Linear Complexity Sequence Model (LCSM) that effectively encompasses all established methods for linear complexity sequence modeling. This framework facilitates a deeper understanding of the operational principles underlying these methods. The LCSM model consists of three stages: Expand, Os-\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_2eaf6b6d4a3474b0fdebg-02.jpg?height=292&width=1692&top_left_y=226&top_left_x=185)\n\nFigure 1. Primary Groups of Linear Complexity Sequence Model. cillation, and Shrink (EOS) as well as a Memory state. During the Expand stage, input signals are transformed into a high-dimensional memory state using a predefined projection function such as the outer product. The memory state is then combined with previous memory states through recursive operations in the Oscillation stage. Finally, the combined memory state is reduced to a low-dimensional output space in the Shrink stage. Different methodologies often exhibit variations in three key areas: 1) parameterization strategies used to calculate the Expand and Shrink parameters, 2) data dependency strategies for EOS, and 3) construction methods for the Oscillation state. Specifically, we examine two parameterization strategies, eight EOS data dependency strategies and twelve approaches for constructing the Oscillation state in order to assess the impact of each option. Ablation studies are conducted to analyze the impact of activation functions and hyperparameters, providing complementary insights. Our main focus is on evaluating the language modeling capabilities and information retrieval abilities of these variants. Our empirical findings indicates that data dependency plays a crucial role in the construction of oscillation states and the parameterization of Expand and Shrink stages in language modeling tasks, while its significance is comparatively lower in retrieval tasks. Our use of carefully hand-crafted EOS parameterization has resulted in improved overall performance in retrieval tasks. ## 2. Method\n\nThis section begins with a general definition of sequence modeling and then delves into the specifics of our Linear Complexity Sequence Model (LCSM). We will also provide examples to illustrate that all current linear complexity sequence modeling methods can be considered as a special case of LCSM. ### 2.1. General Definition of Sequence Modeling\n\nLet us consider a sequence mapping $f: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$ :\n\n$$\n\\left[\\begin{array}{c}\n\\mathbf{y}_{1}^{\\top} \\\\\n\\vdots \\\\\n\\mathbf{y}_{n}^{\\top}\n\\end{array}\\right]=\\mathbf{Y}=f(\\mathbf{X})=f\\left(\\left[\\begin{array}{c}\n\\mathbf{x}_{1}^{\\top} \\\\\n\\vdots \\\\\n\\mathbf{x}_{n}^{\\top}\n\\end{array}\\right]\\right)\n$$\n\nor $\\mathbf{Y}=f(\\mathbf{X})$, where $\\mathbf{X}, \\mathbf{Y} \\in \\mathbb{R}^{n \\times d}, n$ is the temporal length of the sequence, and $d$ is the feature dimensionality. Let $\\mathbf{y}_{m}=f\\left(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right)_{m} \\in \\mathbb{R}^{d}$ be the transpose of the $m$ th row of $\\mathbf{Y}$. In particular, we consider the causal mapping $f_{n}: \\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}:$\n\n$$\n\\mathbf{y}_{n}=f_{n}\\left(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right) \\triangleq f\\left(\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{n}\\right)_{n}\n$$\n\nLanguage modeling is a typical example of causal mapping. The subsequent sections of the paper will focus on causal mapping for the purpose of simplifying the explanation, but the extension to non-causal mapping is straightforward. ### 2.2. Linear Complexity Sequence Modeling\n\nOur proposed Linear Complexity Sequence Model (LCSM) performs the mapping sequentially as follows:\n\n$$\n\\begin{aligned}\n\\mathbf{m}_{t} & =g_{\\psi}\\left(\\mathbf{o}_{t}, \\mathbf{m}_{t-1}\\right)+\\mathbf{e}_{t} \\mathbf{i}_{t}^{\\top} \\\\\n\\mathbf{y}_{t} & =\\mathbf{m}_{t}^{\\top} \\mathbf{s}_{t}\n\\end{aligned}\n$$\n\nwhere $\\mathbf{e}_{t} \\in \\mathbb{R}^{k}$ is the expand state, $\\mathbf{o}_{t} \\in \\mathbb{R}^{k \\times p}$ is the oscillation state, $\\mathbf{s}_{t} \\in \\mathbb{R}^{k}$ is the shrink state, $\\mathbf{i}_{t} \\in \\mathbb{R}^{d}$ is the input state, $k$ is the expand dimension, and $p$ is defined below. In other words, there are three stages to the information flow of the sequence modeling process. These stages, known as Expand, Oscillation, and Shrink (EOS), are repeated for each time step $t$ :\n\n1. Expand Stage: The input state $\\mathbf{i}_{t}$ is expanded, via the expand state $\\mathbf{e}_{t}$ :\n\n$$\n\\overline{\\mathbf{m}}_{t}=\\mathbf{e}_{t} \\mathbf{i}_{t}^{\\top} \\quad \\in \\mathbb{R}^{k \\times d}\n$$\n\n2. Oscillation Stage: The memory state $\\mathbf{m}_{t}$ is updated:\n\n$$\n\\mathbf{m}_{t}=g_{\\psi}\\left(\\mathbf{o}_{t}, \\mathbf{m}_{t-1}\\right)+\\overline{\\mathbf{m}}_{t} \\quad \\in \\mathbb{R}^{k \\times d}\n$$\n\nwhere for initialization we set $\\mathbf{m}_{0}=\\operatorname{zeros}(k, d)$, which is a $k \\times d$ matrix of zeros. 3. Shrink Stage: The output state $\\mathbf{y}_{t}$ is obtained by projecting the memory state $\\mathbf{m}_{t}$ back to low-dimensional space via the shrink state $\\mathbf{s}_{t}$ :\n\n$$\n\\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top} \\mathbf{s}_{t} \\quad \\in \\mathbb{R}^{d}\n$$\n\nWe consider two possible binary operators $\\psi$. If $\\psi=\\odot$, the projection function is element-wise multiplication, whereas\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_2eaf6b6d4a3474b0fdebg-03.jpg?height=532&width=1389&top_left_y=238&top_left_x=346)\n\nFigure 2. Diagram of LCSM. LCSM works in a recurrent form and consists of three states: Expand, Oscillation, and Shrink. Input signals are transformed into a high-dimensional state in the Expand stage and then recursively merged in the Oscillation stage. The combined memory state is condensed into a low-dimensional output space in the Shrink stage. if $\\psi=\\times$, it is matrix multiplication:\n\n$$\n\\begin{aligned}\ng_{\\odot}\\left(\\mathbf{o}_{t}, \\mathbf{m}_{t-1}\\right) & =\\mathbf{o}_{t} \\odot \\mathbf{m}_{t-1} \\\\\ng_{\\times}\\left(\\mathbf{o}_{t}, \\mathbf{m}_{t-1}\\right) & =\\mathbf{o}_{t} \\mathbf{m}_{t-1}\n\\end{aligned}\n$$\n\nFrom matrix dimensions, it is easy to see that\n\n$$\np= \\begin{cases}d & \\text { if } \\psi=\\odot \\\\ k & \\text { if } \\psi=\\times\\end{cases}\n$$\n\nThe discussion is simplified by noting that, when $\\mathbf{o}_{t}$ is diagonalizable, which is a common assumption in practice (Gupta et al., 2022), $\\psi=\\times$ can be reduced to $\\psi=\\odot$. This is because $\\mathbf{o}_{t}=\\operatorname{diag}\\left\\{\\overline{\\mathbf{o}}_{\\mathbf{t}}\\right\\}$, where $\\overline{\\mathbf{o}}_{\\mathbf{t}} \\in \\mathbb{R}^{k}$, leads to:\n\n$$\n\\begin{aligned}\n& g_{\\times}\\left(\\mathbf{o}_{t}, \\mathbf{m}_{t-1}\\right)=\\mathbf{o}_{t} \\mathbf{m}_{t-1}=\\operatorname{diag}\\left(\\overline{\\mathbf{o}}_{t}\\right) \\mathbf{m}_{t-1} \\\\\n& =\\left(\\overline{\\mathbf{o}}_{t} \\mathbf{1}_{\\{k\\}}^{\\top}\\right) \\odot \\mathbf{m}_{t-1}=g_{\\odot}\\left(\\overline{\\mathbf{o}}_{t} \\mathbf{1}_{\\{k\\}}^{\\top}, \\mathbf{m}_{t-1}\\right)\n\\end{aligned}\n$$\n\nTherefore, to simplify the discussion, we only consider the case where $\\psi=\\odot$ in the main text, which also includes the case when $\\psi=\\times$ and $\\mathbf{o}_{t}$ is diagonalizable. We discuss a few examples where $\\mathbf{o}_{t}$ is not diagonalizable in the Appendix 5.2. ### 2.3. Examples\n\nTo effectively demonstrate our unifying LCSM framework, below we describe how various well-established linear complexity sequence modeling methods can be integrated into the framework. The correspondence is listed in Table 1. There are two different families of methods. In datadependent methods, the expand, oscillation and shrink states are either fully or partially computed based on the input $\\mathbf{x}_{t}$. Notationally, this is represented using the subscript $t$, as in $\\mathbf{e}_{t}, \\mathbf{o}_{t}, \\mathbf{s}_{t}$. Alternatively, in data-independent methods, these states are computed regardless of the input $\\mathbf{x}_{t}$. Notationally, we drop the subscript in this case, leading to $\\mathbf{e}, \\mathbf{o}, \\mathbf{s}$. - S4 (Gu et al., 2022a). By setting $d=1$, this method performs channel-wise mapping. The full mapping $f$ : $\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$ requires a bank of these single-input singleoutput (SISO) SSMs. In other words, the computation is repeated $d$ times, as in, $f_{i}$, where $i=1, \\ldots, d$. (Note that $\\mathbf{u}_{t}, \\mathbf{y}_{t} \\in \\mathbb{R}^{k}$. And $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$ are obtained through SSM parameterization.)\n- $\\mathbf{S 5}$ (Smith et al., 2022). The recurrence equation of S5 is the same as $S 4$, with the only difference being that the bank of SISO SSMs is replaced by a single multi-input multioutput (MIMO) SSM. The direct definition of the mapping is $\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$ and $\\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{k \\times d}$. - DSS (Gupta et al., 2022). The recurrence equation of DSS is same as S5, except that the computation is greatly simplified by diagonalizing $\\mathbf{A}=\\operatorname{Diag}\\{\\mathbf{a}\\} \\in \\mathbf{R}^{k \\times k}$. - TNN (Qin et al., 2023a). Toeplitz neural networks (TNNs) exhibit competitive performance on long-range tasks with log-linear space-time complexity. As shown in (Qin \\& Zhong, 2023), a TNN can be losslessly converted to an SSM, where $\\mathbf{C}=\\mathbf{J}^{(k d)} \\in \\mathbb{R}^{k \\times d}, \\mathbf{B} \\in \\mathbb{R}^{k \\times d}, \\mathbf{A}=$ $\\operatorname{Diag}\\left\\{\\lambda_{1}, \\ldots, \\lambda_{k}\\right\\} \\in \\mathbb{R}^{k \\times k}$, get $\\mathbf{u}_{t}$ from $\\mathbf{x}_{t}$ through linear projection, and it can be expressed as a recursive formula:\n\n$$\n\\mathbf{m}_{t}=\\mathbf{A} \\mathbf{m}_{t-1}+\\mathbf{B} \\mathbf{u}_{t}^{\\top}, \\quad \\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top} \\mathbf{C}\n$$\n\n- Linear Attention (Katharopoulos et al., 2020b). In Linear Attention, we obtain query $\\mathbf{q}_{t} \\in \\mathbb{R}^{k}$, key $\\mathbf{k}_{t} \\in \\mathbb{R}^{k}$, value $\\mathbf{v}_{t} \\in \\mathbb{R}^{d}$ from the input $\\mathbf{x}_{t} \\in \\mathbb{R}^{d}$ through linear projection, and recursively calculate as follows:\n\n$$\n[\\mathbf{k v}]_{t}=[\\mathbf{k v}]_{t-1}+\\mathbf{k}_{t} \\mathbf{v}_{t}^{\\top}, \\quad \\mathbf{y}_{t}=[\\mathbf{k v}]_{t}^{\\top} \\mathbf{q}_{t}\n$$\n\n- TNL/RetNet (Qin et al., 2023b; Sun et al., 2023). TNL/RetNet is a form of Linear Attention with exponential decay, and the method for getting $\\mathbf{q}_{t}, \\mathbf{k}_{t}, \\mathbf{v}_{t}$ is the same as those in Linear Attention, and $\\lambda$ is a predefined parameter\n\nTable 1. Checklist for typical linear complexity sequence modeling within the defined LCSM Framework. For each methodology, the following states and functions are outlined: Input State, Expand State, Oscillation State, Shrink State, Memory State, and the operation. If the state is directly linked to the input sequence, the subscript ${ }_{i}$ is emphasized. We use $\\mathbf{1}^{(k)} \\in \\mathbb{R}^{k}$, where $\\mathbf{1}_{j}^{(k)}=1$ for $j=1, \\ldots, k$,\n\n| and $\\mathbf{J}^{(k d)}=\\mathbf{1}^{(k)} \\mathbf{1}^{(d)^{\\top}} \\in \\mathbb{R}^{k \\times d}$. | Input | Expand | Oscillation | Shrink | Memory $k \\times d$ | $\\psi$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Methods | $\\mathbf{u}_{t}$ | $\\mathbf{B}$ | $\\mathbf{A}$ | $\\mathbf{C}$ | $k \\times 1$ | $\\times$ |\n| S4 | $\\mathbf{u}_{t}$ | $\\mathbf{B}$ | $\\mathbf{A}$ | $\\mathbf{C}$ | $k \\times d$ | $\\times$ |\n| S5 | $\\mathbf{u}_{t}$ | $\\mathbf{B}$ | $\\mathbf{a} \\mathbf{1}_{k}^{\\top}$ | $\\mathbf{C}$ | $k \\times d$ | $\\times$ |\n| DSS | $\\mathbf{x}_{t}$ | $\\mathbf{B}$ | $\\mathbf{A}$ | $\\mathbf{C}$ | $k \\times d$ | $\\times$ |\n| TNN | $\\mathbf{v}_{t}$ | $\\mathbf{k}_{t}$ | $\\mathbf{J}^{(k d)}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\odot$ |\n| Linear Attention | $\\mathbf{v}_{t}$ | $\\mathbf{k}_{t}$ | $\\lambda \\mathbf{J}^{(k)}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\odot$ |\n| TNL/RetNet | $\\mathbf{u}_{t}$ | $\\mathbf{B}_{t}$ | $\\mathbf{A}_{t}$ | $\\mathbf{C}_{t}$ | $k \\times d$ | $\\odot$ |\n| Mamba | $\\mathbf{v}_{t}$ | $\\exp \\left(\\mathbf{k}_{t}\\right)$ | $\\exp (-w)$ | $\\mathbf{R}_{t}$ | $1 \\times 1$ | $\\odot$ |\n| RWKV4 | $\\mathbf{v}_{t}$ | $\\mathbf{k}_{t}$ | $\\exp (i \\theta) \\mathbf{J}^{(k d)}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\odot$ |\n| Cosformer | $\\mathbf{v}_{t}$ | $\\mathbf{k}_{t}$ | $\\exp (i \\Theta) \\mathbf{1}^{(d)}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\odot$ |\n| LRPE | $\\mathbf{v}_{t}$ | $\\mathbf{k}_{t}$ | $\\mathbf{g}_{t} \\mathbf{1}_{d}^{\\top}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\odot$ |\n| GLA/GateLoop | $\\mathbf{v}_{t}$ | $\\mathbf{k}_{t}$ | $\\mathbf{g}_{t} \\mathbf{g}_{t}^{\\top}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\odot$ |\n| DUR/GFW | $\\mathbf{i}_{t}$ | $1-\\lambda_{t}$ | $\\lambda_{t}$ | $\\mathbf{o}_{t}$ | $1 \\times 1$ | $\\odot$ |\n| HGRN/LRN |  |  |  |  | $\\odot$ |  |\n\nthat cannot be learned. Its recursive calculation is:\n\n$$\n[\\mathbf{k v}]_{t}=\\lambda[\\mathbf{k v}]_{t-1}+\\mathbf{k}_{t} \\mathbf{v}_{t}^{\\top}, \\quad \\mathbf{y}_{t}=[\\mathbf{k v}]_{t}^{\\top} \\mathbf{q}_{t}\n$$\n\n- Mamba (Gu \\& Dao, 2023). Mamba can be seen as a data-dependent S 4 with elementwise multiplication. It uses the similar method to get $\\mathbf{u}_{t}, \\mathbf{A}, \\mathbf{B}, \\mathbf{C}$, the $\\mathbf{A}_{\\mathbf{t}}, \\mathbf{B}_{t}, \\mathbf{C}_{t}$ are computed through $\\mathbf{x}_{t}$ and $\\mathbf{A}, \\mathbf{B}, \\mathbf{C}$. Its recurrence equation is defined as:\n\n$$\n\\mathbf{m}_{t}=\\mathbf{A}_{t} \\odot \\mathbf{m}_{t-1}+\\mathbf{B}_{t} \\mathbf{u}_{t}^{\\top}, \\quad \\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top} \\mathbf{C}_{t}\n$$\n\n- RWKV4 (Peng et al., 2023). In RWKV4, we get $\\mathbf{r}_{t}, \\mathbf{k}_{t}, \\mathbf{v}_{t}$ through linear projection from input $\\mathbf{x}_{t}$ and $\\mathbf{w}$ as a learnable weight. Ignoring the denominator of RWKV4, the recurrence equation can be simplified as:\n\n$$\n\\mathbf{m}_{t}=\\exp (-w) \\mathbf{m}_{t-1}+\\exp \\left(\\mathbf{k}_{t}\\right) \\mathbf{v}_{t}^{\\top}, \\quad \\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top} \\mathbf{r}_{t}\n$$\n\nSimilar to S4, RWKV4 uses channel-wise mapping $f_{i}, i=$ $1, \\ldots, d$ of $\\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{n \\times 1}$. - Cosformer (Qin et al., 2022). In Cosformer, we obtain query $\\mathbf{q}_{t} \\in \\mathbb{R}^{k}$, key $\\mathbf{k}_{t} \\in \\mathbb{R}^{k}$, value $\\mathbf{v}_{t} \\in \\mathbb{R}^{d}$ from the input $\\mathbf{x}_{t} \\in \\mathbb{R}^{d}$ and a prefined $\\theta$ (not learnable). Then recursively calculate as follows:\n$[\\mathbf{k v}]_{t}=\\exp (i \\theta)[\\mathbf{k v}]_{t-1}+\\mathbf{k}_{t} \\mathbf{v}_{t}^{\\top}, \\quad \\mathbf{y}_{t}=\\operatorname{Rel}\\left\\{[\\mathbf{k v}]_{t}\\right\\}^{\\top} \\mathbf{q}_{t}$. We provide the proof in the Appendix 5.3. - LRPE (Qin et al., 2023c). In LRPE, we obtain query $\\mathbf{q}_{t} \\in \\mathbb{R}^{k}$, key $\\mathbf{k}_{t} \\in \\mathbb{R}^{k}$, value $\\mathbf{v}_{t} \\in \\mathbb{R}^{d}$ from the input $\\mathbf{x}_{t} \\in \\mathbb{R}^{d}, \\theta$ as a learnable weight and recursively calculate as follows:\n\n$$\n\\begin{aligned}\n{[\\mathbf{k v}]_{t} } & =\\Lambda[\\mathbf{k v}]_{t-1}+\\mathbf{k}_{t} \\mathbf{v}_{t}^{\\top} \\\\\n\\Lambda & =\\operatorname{diag}\\left\\{\\exp \\left(i \\theta_{1}\\right), \\ldots, \\exp \\left(i \\theta_{k}\\right)\\right\\} \\\\\n\\mathbf{y}_{t} & =\\operatorname{Rel}\\left\\{[\\mathbf{k v}]_{t}\\right\\}^{\\top} \\mathbf{q}_{t}\n\\end{aligned}\n$$\n\nWe provide the proof in the Appendix 5.3. - GLA/GateLoop (Yang et al., 2023; Katsch, 2024). In GLA/GateLoop, we obtain query $\\mathbf{q}_{t} \\in \\mathbb{R}^{k}$, key $\\mathbf{k}_{t} \\in \\mathbb{R}^{k}$, value $\\mathbf{v}_{t} \\in \\mathbb{R}^{d}$, decay $\\mathbf{g}_{t} \\in \\mathbf{R}^{k}$ from the input $\\mathbf{x}_{t} \\in \\mathbb{R}^{d}$ and recursively calculate as follows:\n\n$$\n[\\mathbf{k v}]_{t}=\\operatorname{Diag}\\left\\{\\mathbf{g}_{t}\\right\\}[\\mathbf{k v}]_{t-1}+\\mathbf{k}_{t} \\mathbf{v}_{t}^{\\top}, \\quad \\mathbf{y}_{t}=[\\mathbf{k v}]_{t}^{\\top} \\mathbf{q}_{t}\n$$\n\n- DUR/GFW (Mao, 2022; Schlag \\& Schmidhuber, 2018) In DUR/GFW, we obtain query $\\mathbf{q}_{t} \\in \\mathbb{R}^{k}$, key $\\mathbf{k}_{t} \\in \\mathbb{R}^{k}$, value $\\mathbf{v}_{t} \\in \\mathbb{R}^{d}$, decay $\\mathbf{g}_{t} \\in \\mathbf{R}^{k}$, $\\overline{\\mathbf{g}}_{t} \\in \\mathbf{R}^{d}$ from the input $\\mathbf{x}_{t} \\in \\mathbb{R}^{d}$, and recursively calculate as follows:\n\n$$\n[\\mathbf{k v}]_{t}=\\left(\\mathbf{g}_{t} \\overline{\\mathbf{g}}_{t}^{\\top}\\right) \\odot[\\mathbf{k v}]_{t-1}+\\mathbf{k}_{t} \\mathbf{v}_{t}^{\\top}, \\quad \\mathbf{y}_{t}=[\\mathbf{k v}]_{t}^{\\top} \\mathbf{q}_{t}\n$$\n\n- HGRN/LRN (Qin et al., 2023d; Martin \\& Cundy, 2018). In HGRN/LRN, we obtain output gate $\\mathbf{o}_{t} \\in \\mathbb{R}^{1}$, forget gate $\\mathbf{f}_{t} \\in \\mathbb{R}^{1}$, input state $\\mathbf{i}_{t} \\in \\mathbb{R}^{1}$ from the input $\\mathbf{x}_{t} \\in \\mathbb{R}^{1}$, and recursively calculate as follows:\n\n$$\n\\mathbf{h}_{t}=\\mathbf{f}_{t} \\odot \\mathbf{h}_{t-1}+\\left(1-\\mathbf{f}_{t}\\right) \\mathbf{i}_{t}^{\\top}, \\quad \\mathbf{y}_{t}=\\mathbf{h}_{t}^{\\top} \\mathbf{o}_{t}\n$$\n\nSimilar to S4, HGRN/LRN uses channel-wise mapping $f_{i}, i=1, \\ldots, d$ of $\\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{n \\times 1}$.",
    "secretlinseqmod-2": "### 2.4. State Calculation\n\nIn the preceding section, it was demonstrated that various sequence modeling methods employ different approaches to calculate the EOS states. Here, we summarize possible calculation strategies, including methods that have been previously adopted and those that have not been utilized before. Input state. For input state $\\mathbf{i}_{t}$, We obtain it through a linear projection from input $\\mathbf{x}_{t}$. Parameterization. Two parameterization strategies are commonly utilized. The first involves employing a state space model to derive the parameters (Gu et al., 2022b),\n\nTable 2. Code Mapping of Activation Function. We perform experiments among the no activation function and seven distinct activation functions. | act_code | $\\mathbf{0}$ | $\\mathbf{1}$ | $\\mathbf{2}$ | $\\mathbf{3}$ | $\\mathbf{4}$ | $\\mathbf{5}$ | $\\mathbf{6}$ | $\\mathbf{7}$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| meaning | $x$ | $\\operatorname{relu}(x)$ | $\\operatorname{sigmoid}(x)$ | $1+\\operatorname{elu}(x)$ | $\\operatorname{silu}(x)$ | $\\operatorname{elu}(x)$ | $\\operatorname{relu}^{2}(x)$ | $x^{2}$ |\n\nTable 3. Code Mapping of Oscillation State. We use Blue to denote data dependent and Black to denote data independent and use Einstein Summation notation. For example, $k, d \\rightarrow k d$ means using a data independent $k$ dim vector and a data dependent $d \\operatorname{dim}$ vector to construct a matrix of size $k \\times d$ using outproduct. | o_code | $\\mathbf{0}$ | $\\mathbf{1}$ | $\\mathbf{2}$ | $\\mathbf{3}$ | $\\mathbf{4}$ | $\\mathbf{5}$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| meaning | $k d$ | $k, d \\rightarrow k d$ | $d^{\\text {broadcast }} \\rightarrow d$ | $k^{\\text {broadcast }} k d$ | $k^{\\text {broadcast }} k d$ | $d^{\\text {broadcast }} k d$ |\n| o_code | $\\mathbf{6}$ | $\\mathbf{7}$ | $\\mathbf{8}$ | $\\mathbf{9}$ | $\\mathbf{1 0}$ | $\\mathbf{1 1}$ |\n| meaning | $k, k d \\rightarrow k d$ | $d, k d \\rightarrow k d$ | $k, d \\rightarrow k d$ | $k, d \\rightarrow k d$ | $\\mathbf{1}_{k \\times d}$ | $\\exp (i \\boldsymbol{\\Theta}) \\mathbf{1}_{d}$ |\n\nwhile the second entails learning the parameters from data through linear projection (Katharopoulos et al., 2020a). Data Dependency. As indicated in Table 1, it is shown that the EOS states of certain methods are dependent on the input state. Specifically, methods with a subscript $t$ in their EOS states exhibit this dependency, while those without a subscript $t$ do not. For data-dependent cases, we assume that each element of the oscillation state belongs to $[0,1]$ and is calculated using sigmoid $(\\mathrm{x})^{1 / \\tau}$ (Yang et al., 2023), where the hyper-parameter $\\tau$ controls the oscillation rate. For dataindependent cases, we use the method of initialization with Alibi (Press et al., 2022). Construction Methods of Oscillation State. The Oscillation state exhibits the most significant variations among the three states. Here, we select 12 distinct scenarios within the Oscillation state, which include the utilization of complex numbers, non-learnable data-independent situations, and the all-ones scenario. We list the following possibilities through the form of Einstein Summation in Table 3. Activation Function Test. In a manner akin to the kernel functions utilized for processing queries and keys in Linear Transformers (Katharopoulos et al., 2020b), activation functions are also employed to manipulate the Expand and Shrink states. To assess the efficacy of various activation functions, we conducted tests on several widely-used options. We list them in Table 2. Tau Test. $\\tau$ controls the oscillation rate. We also ablate the efficacy of $\\tau$ in controlling the oscillation rate. To avoid the oscillation state becoming too small, we used the form $\\operatorname{Sigmoid}(x)^{1 / \\tau}$ following (Yang et al., 2023). In this test, we varied the value of $\\tau$, with the default value being $\\tau=16$ following (Yang et al., 2023). ### 2.5. Model Code\n\nFor the ease of representing the model variants, we use model codes to classify different variants. Given the fol-\nTable 4. Data Dependence Test (left) and $O$ Types Test (right). Perplexity values of validation and test split are reported. | Code | Valid | Test | Param. |\n| :---: | :---: | :---: | :---: |\n| $0-0-0-0$ | 29.62 | 30.5 | 43.45 |\n| $0-0-0-0$ | 28.26 | 29.1 | 43.45 |\n| $0-0-1-0$ | 28.23 | 29.07 | 43.45 |\n| $0-1-0-0$ | 25.89 | 26.61 | 44.04 |\n| $0-1-1-0$ | 25.36 | 26.21 | 44.04 |\n| $1-0-0-0$ | 27.94 | 28.77 | 43.45 |\n| $1-0-1-0$ | 26.12 | 26.65 | 43.45 |\n| $1-1-0-0$ | 24.24 | 24.91 | 44.04 |\n| $1-1-1-0$ | 24.26 | 24.75 | 44.04 |\n| $1-2-1-0$ | 29.27 | 29.77 | 43.45 |\n| $1-3-1-0$ | 28.22 | 28.86 | 43.45 |\n| $1-4-1-0$ | 27.55 | 28.21 | 44.50 |\n| $1-5-1-0$ | 29.01 | 29.82 | 44.51 |\n| $1-6-1-0$ | 25.02 | 25.48 | 44.11 |\n| $1-7-1-0$ | 24.82 | 25.17 | 44.04 |\n| $1-8-1-0$ | 24.71 | 25.08 | 43.45 |\n| $1-9-1-0$ | 24.23 | 24.54 | 43.46 |\n| $1-10-1-0$ | 29.91 | 30.31 | 43.45 |\n| $1-11-1-0$ | 28.6 | 29.36 | 44.11 |\n\nTable 5. Parameterization Test. Perplexity values of validation and test split are reported. | Code | Valid | Test | Param. |\n| :--- | :---: | :---: | :---: |\n| 0 | 27.41 | 27.99 | 43.78 |\n| $1-1-1-0$ | 24.26 | 24.75 | 44.04 |\n\nlowing variables: parameterization type, expand type, oscillation type, shrink type, activation function type, we use the format 'e-o-s-a' to denote the model code. We use a separate code 0 to denote SSM parameterization methods like Mamba, and S4, whereas other codes represent linear parameterization. In this context, $e$ and $s$ take values of 0 or 1 , representing the expand type and shrink type, where 0 indicates data independence and 1 indicates data dependence. The variable $o$ ranges from 0 to 11 and denotes the method of constructing the oscillation state, with the complete table detailed in Table 3. Similarly, the variable $a$ ranges from 0 to 7 and\n\nTable 6. Activation Function Test(left) and Tau Value Test (right). Perplexity values of validation and test split are reported. ![](https://cdn.mathpix.com/cropped/2024_09_17_2eaf6b6d4a3474b0fdebg-06.jpg?height=734&width=1693&top_left_y=289&top_left_x=180)\n\nFigure 3. Parameterization Test. The accuracy for various sequence lengths and model dimensions are reported. indicates the type of activation function, with the complete table shown in Table 2.",
    "secretlinseqmod-3": "For instance, a setting of \" $0-0-0-0$ \" signifies the use of linear parameterization, with all states being data independent and no activation function being utilized. ## 3. Experiments\n\nWe perform extensive experiments to evaluate the model variants in language modeling performance and longcontext recall capabilities on WikiText-103 (Merity et al., 2017) dataset and the multi-query associative recall (MQAR) task (Arora et al., 2023). The experiment setups are delineated below. For the convergence experiments on WikiText-103, we adhered to the configuration outlined in prior studies (Qin et al., 2023a). Specifically, we employed a batch size of 128, and a sequence length of 512 , and ran the experiments for 50,000 iterations. The learning rate was set to $5 \\mathrm{e}-4$, with complete hyperparameters listed in Appendix 6. For the MQAR task, we standardized the expand ratio $k$ for all models to 128 . We conducted a grid search for the learning rate, spanning values from $\\{1 \\mathrm{e}-5,5 \\mathrm{e}-5,1 \\mathrm{e}-4$, $5 \\mathrm{e}-$ $4,1 e-3,5 e-3,1 e-2\\}$, and reported the best results for each model. Additionally, we tested various sequence lengths 32, $64,128,256$, and feature dimensions $64,128,256,512$ to determine the optimal configuration. ### 3.1. Experiments on Wikitext-103\n\nParameterization Test. We carried out a comparison between SSM parameterization and naive parameterization, with the findings presented in Table 5. The analysis revealed that SSM parameterization showed inferior performance compared to naive parameterization on the Wikitext dataset. Data Dependence Test. We proceeded to assess the extent of dependency on data-dependent states, as shown in Table 4. Analysis of the Wikitext data indicates that when all EOS states $(e, o, s)$ are non-data-dependent, the performance is notably subpar, even worse than the scenario where the $o$ state is not learnable. Introducing data dependence on just one of these elements, particularly on the $o$ state $(0-$ $1-0-0)$, demonstrates the most significant effectiveness. When data dependence is introduced on two elements, the combination of $o$ and $s(1-1-0-0)$ proves to be the most effective, yielding results comparable to scenarios where $e$, $o$, and $s$ are all data-dependent. O Types Test. In Table 4, we evaluated various strategies for constructing the $o$ state. A method we labeled as $1-9-$ $1-0$, yielded the most favorable outcomes. This approach outperformed the $1-1-1-0$ configuration. On the other hand, methods that relied on a naive o state (such as linear attention (Katharopoulos et al., 2020b)) demonstrated inferior performance. Activation Functions Test. Then we examined the influence of activation functions on the outcomes, results are presented in Table 6. It's clear that employing activation functions yields considerable benefits, with silu and relu ${ }^{2}$ demonstrating the most favorable performance.",
    "secretlinseqmod-4": "![](https://cdn.mathpix.com/cropped/2024_09_17_2eaf6b6d4a3474b0fdebg-07.jpg?height=1383&width=1671&top_left_y=248&top_left_x=205)\n\nFigure 5. O Types Test. The accuracy for various sequence lengths and model dimensions are reported. Tau Values Test. Finally we explored the determination of tau values for the $1-1-1-0$ configuration, results are also listed in Table 6. On the WikiTest-103 task, selecting tau $=8$ resulted in the most favorable outcomes. ### 3.2. Experiments on MQAR\n\nParameterization Test. In Fig.3, a comparative analysis was carried out between SSM parameterization and naive parameterization, illustrating that naive parameterization achieves inferior performance in the MQAR task.",
    "secretlinseqmod-5": "Data Dependence Test.",
    "secretlinseqmod-6": "We proceeded to evaluate the reliance on data-dependent states, as shown in Fig. 4. In the MQAR task, the majority of configurations resulted in subpar performance, with the exception of the $1-0-1$ and $1-1-1$ configurations, which showed satisfactory results for sequence lengths up to 128 . Nonetheless, when the sequence length was increased to 256 , solely the $1-0-1-0$ configuration managed to converge successfully.",
    "secretlinseqmod-7": "O Types Test. In Fig. 5, we conducted a comparison of various methods for constructing $o$ state. Within the context of the MQAR task, the performance of the complex variant stood out as particularly favorable, surpassing other variants significantly. Activation Functions Test. In Figure 6, we explored the impact of activation functions. Employing activation functions resulted in a significant improvement in performance, particularly with longer sequence lengths. Additionally, the $\\operatorname{sigmoid}(1-1-1-2)$ activation function surpassed the performance of other activation functions. Tau Values Test.",
    "secretlinseqmod-8": "We delved into establishing the tau values for the 1-1-1-0 configuration. As shown in Fig. 7, selecting a larger $\\tau$, (thus a large $o$ state) led to enhanced outcomes for MQAR task, which aligns with the capability\n\n## LCSM\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_2eaf6b6d4a3474b0fdebg-08.jpg?height=1387&width=1671&top_left_y=205&top_left_x=202)\n\nFigure 7.",
    "secretlinseqmod-9": "Tau Values Test. The accuracy for various sequence lengths and model dimensions are reported. for long document retrieval. ## 4. Conclusion\n\nThis paper introduced the Linear Complexity Sequence Model (LCSM), a unified framework that synergizes various linear complexity sequence modeling techniques. Our exploration of the LCSM's three integral stages-Expand, Oscillation, and Shrink (EOS)\u2014and the role of the Memory state has unveiled the critical impact of data dependency and parameterization on model performance. The LCSM framework not only enhances our understanding of the operational dynamics of linear complexity models but also opens avenues for future research to further refine and adapt these models for improved efficiency and effectiveness across a broader spectrum of sequence modeling challenges. ## References\n\nArora, S., Eyuboglu, S., Timalsina, A., Johnson, I., Poli, M., Zou, J., Rudra, A., and R\u00e9, C. Zoology: Measuring and improving recall in efficient language models. CoRR, abs/2312.04927, 2023. Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., Belanger, D.",
    "secretlinseqmod-10": "B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum? id=Ua6zuk0WRH. Fu, D. Y., Epstein, E. L., Nguyen, E., Thomas, A. W., Zhang, M., Dao, T., Rudra, A., and R\u00e9, C. Simple hardware-efficient long convolutions for sequence modeling. CoRR, abs/2302.06646, 2023a. doi: 10.48550/arXiv. 2302.06646. URL https://doi.org/10. 48550 / arXiv. 2302.06646. Fu, D. Y., Kumbong, H., Nguyen, E., and R\u00e9, C. FlashFFTConv: Efficient convolutions for long sequences with tensor cores.",
    "secretlinseqmod-11": "2023b. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2023. Gu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022a. Gu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022b. URL https: / / openreview. net / forum?id=uYLFoz1vlAC. Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces. In NeurIPS, 2022. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "secretlinseqmod-12": "In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020a. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.",
    "secretlinseqmod-13": "In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 1318 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 5156-5165. PMLR, 2020b. URL http://proceedings.mlr.press/ v119/katharopoulos20a.html. Katsch, T. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2024.",
    "secretlinseqmod-14": "URL https: / /openreview.net/forum?id=02Ug9N8DCI.",
    "secretlinseqmod-15": "Mao, H. H. Fine-tuning pre-trained transformers into decaying fast weights, 2022.",
    "secretlinseqmod-16": "Martin, E. and Cundy, C. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018. URL https: / /openreview.net/forum?id=HyUNwulC-. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. 5th International Conference on Learning Representations, ICLR, Toulon, France, 2017. Orvieto, A., Smith, S. L., Gu, A., Fernando, A., G\u00fcl\u00e7ehre, \u00c7., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. doi: 10.48550 /arXiv.2303.06349. URL https: //doi.org/10.48550/arXiv. 2303.06349. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., He, X., Hou, H., Kazienko, P., Kocon, J., Kong, J., Koptyra, B., Lau, H., Mantri, K. S. I., Mom, F., Saito, A., Tang, X., Wang, B., Wind, J. S., Wozniak, S., Zhang, R., Zhang, Z., Zhao, Q., Zhou, P., Zhu, J., and Zhu, R.-J. Rwkv: Reinventing rnns for the transformer era, 2023. Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and R\u00e9, C. Hyena hierarchy: Towards larger convolutional language models. CoRR, abs/2302.10866, 2023. doi: 10.48550/arXiv. 2302.10866. URL https://doi.org/10. 48550 / arXiv.2302.10866. Press, O., Smith, N., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=R8sQPpGCv0. Qin, Z. and Zhong, Y. Accelerating toeplitz neural network with constant-time inference complexity. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, December 2023. Qin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and Zhong, Y. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations, 2022. URL https: / openreview. net/forum?id=Bl8CQrx2Up4. Qin, Z., Han, X., Sun, W., He, B., Li, D., Li, D., Dai, Y., Kong, L., and Zhong, Y. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023a. URL https : / openreview.net/forum?id=IxmWsm4xrua. Qin, Z., Li, D., Sun, W., Sun, W., Shen, X., Han, X., Wei, Y., Lv, B., Yuan, F., Luo, X., et al. Scaling transnormer to 175 billion parameters.",
    "secretlinseqmod-17": "arXiv preprint arXiv:2307.14995, 2023b. Qin, Z., Sun, W., Lu, K., Deng, H., Li, D., Han, X., Dai, Y., Kong, L., and Zhong, Y. Linearized relative positional encoding. Transactions on Machine Learning Research, 2023c. Qin, Z., Yang, S., and Zhong, Y. Hierarchically gated recurrent neural network for sequence modeling. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023d. Schlag, I. and Schmidhuber, J. GATED FAST WEIGHTS FOR ASSOCIATIVE RETRIEVAL, 2018. URL https : / /openreview.net/forum?id=HJ8W1Q-0Z. Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers, 2021.",
    "secretlinseqmod-18": "Smith, J. T. H., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling. CoRR, abs/2208.04933, 2022. doi: 10.48550/arXiv. 2208.04933. URL https://doi.org/10.48550/ arXiv. 2208.04933. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: A successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. CoRR, abs/2312.06635, 2023. doi: 10.48550/ARXIV. 2312.06635. URL https://doi.org/10.48550/ arXiv.2312.06635. ## 5. Appendix\n\n### 5.1. Backward Pass\n\nIn the main text, we only defined the forward pass of LCSM. Here, we define the backward pass of LCSM. For convenience, we will refer to the case where $(\\psi=\\odot)$ as Type1, and the case where $(\\psi=\\times)$ as Type2. Type1. The forward pass of Type 1 is:\n\n$$\n\\mathbf{m}_{t}=\\mathbf{f}_{t} \\odot \\mathbf{m}_{t-1}+\\mathbf{e}_{t} \\mathbf{i}_{t}^{\\top}, \\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top} \\mathbf{s}_{t}\n$$\n\nSo the backward pass can be computed as follows:\n\n$$\n\\begin{aligned}\n\\mathbf{d s}_{t} & =\\mathbf{m}_{t} \\mathbf{d} \\mathbf{y}_{\\mathbf{t}} \\in \\mathbb{R}^{k} \\\\\n\\mathbf{d m}_{t-1} & =\\mathbf{f}_{t} \\odot \\mathbf{d m}_{t}+\\mathbf{s}_{t-1} \\mathbf{d} \\mathbf{y}_{t-1}^{\\top} \\in \\mathbb{R}^{k \\times d} \\\\\n\\mathbf{d f}_{t} & =\\mathbf{d m}_{t} \\odot \\mathbf{m}_{t} \\in \\mathbb{R}^{k \\times d} \\\\\n\\mathbf{d e}_{t} & =\\mathbf{d m}_{t} \\mathbf{i}_{t} \\in \\mathbb{R}^{k} \\\\\n\\mathbf{d i}_{t} & =\\mathbf{d m}_{t}^{\\top} \\mathbf{e}_{t} \\in \\mathbb{R}^{d}\n\\end{aligned}\n$$\n\nType2. The forward pass of Type 2 is:\n\n$$\n\\mathbf{m}_{t}=\\mathbf{f}_{t} \\mathbf{m}_{t-1}+\\mathbf{e}_{t} \\mathbf{i}_{t}^{\\top}, \\mathbf{y}_{t}=\\mathbf{m}_{t}^{\\top} \\mathbf{s}_{t}\n$$\n\nSo the backward pass can be computed as follows:\n\n$$\n\\begin{aligned}\n\\mathbf{d s}_{t} & =\\mathbf{m}_{t} \\mathbf{d y}_{\\mathbf{t}} \\in \\mathbb{R}^{k} \\\\\n\\mathbf{d m}_{t-1} & =\\mathbf{f}_{t} \\mathbf{d m}_{t}+\\mathbf{s}_{t-1} \\mathbf{d y}_{t-1}^{\\top} \\in \\mathbb{R}^{k \\times d} \\\\\n\\mathbf{d f}_{t} & =\\mathbf{d m}_{t} \\mathbf{m}_{t}^{\\top} \\in \\mathbb{R}^{k \\times k} \\\\\n\\mathbf{d e}_{t} & =\\mathbf{d m}_{t} \\mathbf{i}_{t} \\in \\mathbb{R}^{k} \\\\\n\\mathbf{d i}_{t} & =\\mathbf{d m}_{t}^{\\top} \\mathbf{e}_{t} \\in \\mathbb{R}^{d}\n\\end{aligned}\n$$\n\n### 5.2. More discussion about $g=\\times$\n\nIn the main text, we primarily discuss the case of $g=\\odot$ and assume that $\\mathbf{o}_{t}$ is diagonalizable when $g=\\times$. In this section, we will discuss the more general case of $g=\\times$. that is, the non-diagonalizable case, which we will list in Table 7. FWP (Schlag et al., 2021)\n\n$$\n\\begin{aligned}\n\\mathbf{W}_{t} & =\\mathbf{W}_{t-1}+\\beta_{t} \\mathbf{k}_{t}\\left(\\mathbf{v}_{t}-\\overline{\\mathbf{v}}_{t}\\right)^{\\top} \\\\\n& =\\mathbf{W}_{t-1}+\\beta_{t} \\mathbf{k}_{t}\\left(\\mathbf{v}_{t}-\\mathbf{W}_{t-1}^{\\top} \\mathbf{k}_{t}\\right)^{\\top} \\\\\n& =\\left(\\mathbf{I}_{k}-\\beta_{t} \\mathbf{k}_{t} \\mathbf{k}_{t}^{\\top}\\right) \\mathbf{W}_{t-1}+\\beta_{t} \\mathbf{k}_{t} \\mathbf{v}_{t}^{\\top} \\\\\n\\mathbf{y}_{t} & =\\mathbf{W}_{t}^{\\top} \\mathbf{q}_{t}\n\\end{aligned}\n$$\n\nBased on the aforementioned forms, we propose several new forms for future research in Table 7,\n\n### 5.3. Prove the equivalent expression of Cosformer and LRPE. ## Cosformer. $$\n\\begin{aligned}\n{[\\mathbf{k v}]_{t} } & =\\sum_{s=1}^{t} \\Lambda^{t-s} \\mathbf{k}_{s} \\mathbf{v}_{s}^{\\top} \\\\\n\\mathbf{y}_{t} & =\\operatorname{Rel}\\left\\{\\left[\\mathbf{k v}_{t}\\right]\\right\\}^{\\top} \\mathbf{q}_{t} \\\\\n& =\\operatorname{Rel}\\left\\{\\sum_{s=1}^{t} \\Lambda^{t-s} \\mathbf{v}_{s} \\mathbf{k}_{s}^{\\top} \\mathbf{q}_{t}\\right\\} \\\\\n& =\\sum_{s=1}^{t} \\mathbf{v}_{s} \\mathbf{k}_{s}^{\\top} \\bar{\\Lambda}_{t-s} \\mathbf{q}_{t} \\\\\n\\bar{\\Lambda}_{t-s} & =\\operatorname{diag}\\left\\{\\cos \\left((t-s) \\theta_{1}\\right), \\ldots, \\cos \\left((t-s) \\theta_{k}\\right)\\right\\}\n\\end{aligned}\n$$\n\n## LRPE. $$\n\\begin{aligned}\n{[\\mathbf{k v}]_{t} } & =\\sum_{s=1}^{t} \\exp (i(t-s) \\theta) \\mathbf{k}_{s} \\mathbf{v}_{s}^{\\top} \\\\\n\\mathbf{y}_{t} & =\\operatorname{Rel}\\left\\{[\\mathbf{k v}]_{t}\\right\\}^{\\top} \\mathbf{q}_{t} \\\\\n& =\\operatorname{Rel}\\left\\{\\sum_{s=1}^{t} \\exp (i(t-s) \\theta) \\mathbf{v}_{s} \\mathbf{k}_{s}^{\\top} \\mathbf{q}_{t}\\right\\} \\\\\n& =\\sum_{s=1}^{t} \\cos ((t-s) \\theta) \\mathbf{v}_{s} \\mathbf{k}_{s}^{\\top} \\mathbf{q}_{t}\n\\end{aligned}\n$$\n\n## 6. Configurations\n\n## LCSM\n\nTable 7. The checklist when $\\mathbf{o}_{t}$ is not diagonalizable, we also give M1, M2, M3 for future research. | Method | Input | Expand | Oscillation | Shrink | Memory Size $k \\times d$ | $g$ |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| FWP | $\\mathbf{v}_{t}$ | $\\beta_{t} \\mathbf{k}_{t}$ | $\\mathbf{I}_{k}-\\beta_{t} \\mathbf{k}_{t} \\mathbf{k}_{t}^{\\top}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\times$ |\n| M1 | $\\mathbf{v}_{t}$ | $\\mathbf{k}_{t}$ | $\\mathbf{I}_{k}-\\mathbf{k}_{t} \\mathbf{k}_{t}^{T}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\times$ |\n| M2 | $\\mathbf{v}_{t}$ | $\\mathbf{k}_{t}$ | $\\mathbf{I}_{k}-\\mathbf{k}_{t} \\mathbf{k}_{t}^{\\top}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\times$ |\n| M3 | $\\mathbf{v}_{t}$ | $\\mathbf{k}_{t}$ | $\\mathbf{I}_{k}-\\operatorname{Diag}\\left\\{\\mathbf{k}_{t}\\right\\}$ | $\\mathbf{q}_{t}$ | $k \\times d$ | $\\times$ |\n\nTable 8. Detailed training configurations used in for Language Modeling task. \"Total batch size\" means batch_per_gpu $\\times$ update_freq $\\times$ num_gpus. |  | LM |\n| :--- | :--- |\n| Data | WikiText-103 |\n| Tokenizer method | BPE |\n| Src Vocab size | 50265 |\n| Sequence length | 512 |\n| Total batch size | 128 |\n| Number of updates/epochs | 50 k updates |\n| Warmup steps/epochs | 4 k steps |\n| Peak learning rate | $5 \\mathrm{e}-4$ |\n| Learning rate scheduler | Inverse sqrt |\n| Optimizer | Adam |\n| Adam $\\epsilon$ | $1 \\mathrm{e}-8$ |\n| Adam $\\left(\\beta_{1}, \\beta_{2}\\right)$ | $(0.9,0.98)$ |\n| Weight decay | 0.1 |\n| Gradient clipping | - |\n\nTable 9. Additional Activation Functions Test. Perplexity values of validation and test split are reported. | Code | PPL (Valid) | PPL (Test) | Paramerters |\n| :--- | :---: | :---: | :---: |\n| $1-10-1-0$ | 29.91 | 30.31 | 43.45 |\n| $1-10-1-1$ | 27.02 | 27.44 | 43.45 |\n| $1-10-1-2$ | 26.77 | 27.37 | 43.45 |\n| $1-10-1-3$ | 26.82 | 27.6 | 43.45 |\n| $1-10-1-4$ | 27.28 | 27.58 | 43.45 |\n| $1-10-1-5$ | 28.64 | 28.98 | 43.45 |\n| $1-10-1-6$ | 26.68 | 27.18 | 43.45 |\n| $1-10-1-7$ | 28.08 | 28.63 | 43.45 |\n\n\n[^0]:    ${ }^{1}$ Taptap ${ }^{2}$ OpenNLPLab, Shanghai AI Lab ${ }^{3}$ NVIDIA ${ }^{4}$ The Australian National University.",
    "secretlinseqmod-19": "Correspondence to: Yiran Zhong [zhongyiran@gmail.com](mailto:zhongyiran@gmail.com).",
    "secretlinseqmod-20": ""
}