{
    "mqa-0": "# Fast Transformer Decoding: One Write-Head is All You Need \n\nNoam Shazeer<br>Google<br>noam@google.com\n\nNovember 7, 2019\n\n\n#### Abstract\n\nMulti-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences.",
    "mqa-1": "While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large \"keys\" and \"values\" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention \"heads\", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline. ## 1 Introduction\n\nThe Transformer neural sequence model [Vaswani et al., 2017] has emerged as a popular alternative to recurrent sequence models. Transformer relies on attention layers to communicate information between and across sequences. One major challenge with Transformer is the speed of incremental inference. As we will discuss, the speed of incremental Transformer inference on modern computing hardware is limited by the memory bandwidth necessary to reload the large \"keys\" and \"values\" tensors which encode the state of the attention layers. In the following sections, we will review the multi-head-attention layers used by Transformer, provide a performance analysis, and propose an architectural variation (multi-query attention) which greatly improves inference speed with only minor quality degradation. ## 2 Background: Neural Attention\n\nNeural Attention, introduced by [Bahdanau et al., 2014], is a powerful tool for manipulating variable-length representations. A neural attention function takes a single query-vector $q$ and a set of $m$ different (key-vector, value-vector) pairs (represented by the matrices $K$ and $V$ ), and produces an output vector $y$. The output $y$ is computed as a weighted sum of the different value vectors, where the weights are derived by comparing the query to the keys. ### 2.1 Dot-Product Attention\n\nThe following code describes a common formulation, where the weights are computed as the softmax of the dot-products of the query with the different keys. ```\ndef DotProductAttention(q, K, V):\n    \" \" \" Dot-Product Attention on one query. Args:\n        q: a vector with shape [k]\n        K: a matrix with shape [m, k]\n        V: a matrix with shape [m, v ]\n    Returns:\n        \\\"n: a vector with shape [v]\n    logits = tf.einsum(\"k,mk->m\", q, K)\n    weights = tf.softmax(logits)\n    return tf.einsum(\"m,mv->v\", weights, V)\n```\n\nOur code samples use einsum notation, as defined in TensorFlow and numpy, for generalized contractions between tensors of arbitrary dimension.",
    "mqa-2": "In this notation, an equation names the dimensions of the input and output Tensors. The computation is numerically equivalent to broadcasting each input to have the union of all dimensions, multiplying component-wise, and summing across all dimensions not in the desired output shape. ### 2.2 Multi-head Attention\n\nThe \"Transformer\" seuqence-to-sequence model [Vaswani et al., 2017] uses $h$ different attention layers (heads) in parallel, which the authors refer to as \"Multi-head attention\".",
    "mqa-3": "The query vectors for the $h$ different layers are derived from $h$ different learned linear projections $P_{q}$ of an input vector $x$. Similarly, the keys and values are derived from $h$ different learned linear projections $P_{k}, P_{v}$ of a collection $M$ of $m$ different input vectors. The outputs of the $h$ layers are themselves passed through different learned linear projections $P_{o}$, then summed. For simplicity, we give the input and output vectors identical dimensionality $d$. The The computation can be expressed as follows:\n\n```\ndef MultiheadAttention (\n    x, M, P_q, P_k, P_v, P_o):\n    \"\" \" Multi-head Attention on one query. Args :\n        \\(x:\\) a vector with shape \\([d]\\)\n        M: a matrix with shape \\([\\mathrm{m}, \\mathrm{d}]\\)\n        P_q: a tensor with shape [h, d, k]\n        P_k: a tensor with shape \\([h, d, k]\\)\n        P_v: a tensor with shape [h, d, v]\n        P_o: a tensor with shape [h, d, v]\n    Returns:\n    \".\" y : a vector with shape [d]\n    \\(\\mathrm{q}=\\mathrm{tf} . \\operatorname{einsum}\\left({ }^{\\mathrm{d}} \\mathrm{d}, \\mathrm{hdk} \\rightarrow\\right.\\) hk\" \\(\\left., \\mathrm{x}, \\mathrm{P} \\_\\mathrm{q}\\right)\\)\n    \\(K=\\) tf.einsum(\"md,hdk->hmk\"\uff0cM\uff0cP_k)\n    \\(\\mathrm{V}=\\mathrm{tf} . \\operatorname{einsum}(\" \\mathrm{md}, \\mathrm{hdv} \\rightarrow\\) hmv\"\uff0cM\uff0cP_v)\n    logits = tf.einsum(\"hk,hmk->hm\", q, K)\n    weights = tf.softmax(logits)\n    \\(o=\\) tf.einsum(\"hm,hmv->hv\", weights, V)\n    \\(y=\\) tf.einsum(\"hv, hdv \\(\\rightarrow\\) d\" \uff0co, P_o)\n    return y\n```\n\nNote: [Vaswani et al., 2017] include a constant scaling factor on the logits. We omit this in our code, as it can be folded into the linear projections $P_{q}$ or $P_{k}$. ### 2.3 Multi-head Attention (Batched)\n\nIn practice, it is far more efficient to batch together multiple queries. The code below adds two types of batching. First, we generate queries from $n$ different positions in a sequence. These queries all interact with the same keys and values. In addition, we process a batch of $b$ different non-interacting sequences at once. Following [Vaswani et al., 2017], in an autoregressive model, we can prevent backward-information-flow by adding a \"mask\" to the logits containing the value $-\\infty$ in the illegal positions. ```\ndef MultiheadAttentionBatched(\n    X, M, mask, P_q, P_k, P_v, P_o):\n    \" \" \" Multi-head Attention. Args:\n        \\(X\\) : a tensor with shape \\([b, n, d]\\)\n        M : a tensor with shape \\([b, m, d]\\)\n        mask: a tensor with shape [b, h, n, m]\n        P_q: a tensor with shape [h, d, k]\n        P_k: a tensor with shape \\([h, d, k]\\)\n        P_v: a tensor with shape \\([h, d, \\mathrm{v}]\\)\n        P_o: a tensor with shape \\([h, d, \\mathrm{v}]\\)\n    Returns:\n        Y: a tensor with shape \\([b, n, d]\\)\n    \" \" \"\n\\(\\mathrm{Q}=\\) tf.einsum(\"bnd, hdk \\(->\\) bhnk\" , X, P_q)\n\\(\\mathrm{K}=\\) tf.einsum(\"bmd, hdk \\(->\\) bhmk\"\uff0cM\uff0cP_k)\n\\(\\mathrm{V}=\\) tf.einsum(\"bmd,hdv \\(\\rightarrow\\) bhmv\"\uff0cM\uff0cP_v\uff09\nlogits = tf.einsum(\"bhnk, bhmk \\(->\\) bhnm\", Q, K)\nweights \\(=\\) tf.softmax\uff08logits + mask \\()\\)\n\\(\\mathrm{O}=\\mathrm{tf} . \\mathrm{einsum}(\"\\) bhnm, bhmv \\(\\rightarrow\\) bhnv\", weights, V)\n\\(\\mathrm{Y}=\\mathrm{tf} . \\operatorname{einsum}(\" \\mathrm{bhnv}, \\mathrm{hdv}->\\) bnd \"\uff0cO\uff0cP_o)\nreturn Y\n```\n\n\n### 2.3.1 Performance Analysis of Batched Multi-head Attention\n\nTo simplify the performance analysis, we will make several simplifying assumptions:\n\n- $m=n$\n- $k=v=\\frac{d}{h}$, as suggested by [Vaswani et al., 2017]\n- $n \\leq d$\n\nThe total number of arithmetic operations is $\\Theta\\left(b n d^{2}\\right)$. (Since the complexity of each of the tf.einsum operations above is $O\\left(b n d^{2}\\right)$ given the simplifying assumptions. The total size of memory to be accessed is equal to the sum of the sizes of all the tensors involved: $O\\left(b n d+b h n^{2}+d^{2}\\right)$. The first term is due to $X, M, Q, K, V, O$ and $Y$, the second term due to the logits and weights, and the third term due to the projection tensors $P_{q}, P_{k}, P_{v}$ and $P_{o}$. Dividing the two, we find that the ratio of memory access to arithmetic operations is $O\\left(\\frac{1}{k}+\\frac{1}{b n}\\right)$. This low ratio is necessary for good performance on modern GPU/TPU hardware, where the computational capacity can be two orders of magnitude higher than the memory bandwidth. ### 2.4 Multihead Attention (Incremental)\n\nIn some settings, data dependencies make it is impossible to process queries from multiple positions in parallel. An example is a self-attention layer in an autoregressive language model such as Transformer [Vaswani et al., 2017]. The queries produced at each position attend to key-value pairs produced at all positions up to and including that position. During training, the ground-truth target sequence is known, and we can use an\nefficient parallel implementation similar to that in section 2.3. However, when generating from the trained model, the output of the self-attention layer at a particular position affects the token that is generated at the next position, which in turn affects the input to that layer at the next position. This prevents parallel computation. Code for incrementally computing this self-attention layer is shown below. ```\ndef MultiheadSelfAttentionIncremental (\n    x, prev_K, prev__V, P_q, P_k, P_v, P_o):\n    \" \" \" Multi-head Self-Attention (one step). Args:\n        \\(x:\\) a tensor with shape \\([b, d]\\)\n        prev_K: tensor with shape [b, h, m, k]\n        prev_V: tensor with shape [b, h, m, v]\n        P_q: a tensor with shape [h, d, k]\n        P_k: a tensor with shape \\([h, d, k]\\)\n        P_v: a tensor with shape [h, d, v]\n        P_o: a tensor with shape [h, d, v]\nReturns:\n    y: a tensor with shape \\([b, d]\\)\n    new_K: tensor with shape [b, h, m+1, k]\n    new_V: tensor with shape \\([b, h, m+1, \\mathrm{v}]\\)\n    \" \"\n    \\(q=\\) tf.einsum(\"bd, hdk \\(\\rightarrow\\) bhk\" , x, P_q)\n    new_K = tf.concat\n        [prev_K, tf.expand_dims(tf.einsum(\"bd,hdk->bhk\", M, P_k), axis = 2)],\n        axis \\(=2\\) )\n    new__V \\(=\\) tf.concat\n        [prev__V, tf.expand_dims(tf.einsum(\"bd,hdv \\(\\rightarrow\\) bhv\"\uff0cM, P_v), axis=2)],\n        axis \\(=2\\) )\n    logits = tf.einsum(\"bhk, bhmk \\(\\rightarrow\\) bhm\", q, new_K)\n    weights = tf.softmax(logits)\n    \\(o=\\) tf.einsum(\"bhm, bhmv->bhv\", weights, new_V)\n    \\(\\mathrm{y}=\\mathrm{tf} . \\operatorname{einsum}(\\) \"bhv, hdv \\(\\rightarrow\\) bd \" , O, P_o)\n    return y, new_K, new_V\n```\n\n\n### 2.4.1 Performance Analysis\n\nWe make the same simplifying assumptions as in section 2.3.1. Across $n$ calls, the total number of arithmetic operations is again $\\Theta\\left(b n d^{2}\\right)$. Across $n$ calls, the total amount of memory access is $\\Theta\\left(b n^{2} d+n d^{2}\\right)$, the first term due to $K$ and $V$ and the second term due to $P_{q}, P_{k}, P_{v}$ and $P_{o}$. Dividing the memory by the computations, we find that the ratio of memory access to arithmetic operations is $\\Theta\\left(\\frac{n}{d}+\\frac{1}{b}\\right)$. When $n \\approx d$ or $b \\approx 1$, the ratio is close to 1 , causing memory bandwidth to be a major performance bottleneck on modern computing hardware. In order to make incremental generation efficient, we must reduce both of these terms to be $\\ll 1$. The $\\frac{1}{b}$ term is the easier one - we can just use a larger batch size, memory size permitting. Reducing the $\\frac{n}{d}$ term is harder. This term is related to the expense of reloading at each step the $K$ and $V$ tensors representing the memory which have size $b h m k=b n^{2}$. One solution is to limit the sequence length $n$. Another is to reduce the number of positions being attended-to, either by attending to a local neighborhood, or by otherwise compressing the number of memory positions, as in [Liu et al., 2018], [Zhang et al., 2018], [Povey et al., 2018]. In this paper we present an orthogonal approach to reducing the size of the $K$ and $V$ tensors - namely removing their \"heads\" dimension, while maintaining the \"heads\" dimension in the queries. ## 3 Multi-Query Attention\n\nWe introduce multi-query Attention as a variation of multi-head attention as described in [Vaswani et al., 2017]. Multi-head attention consists of multiple attention layers (heads) in parallel with different linear transformations on the queries, keys, values and outputs. Multi-query attention is identical except that the different heads share a single set of keys and values. The code for (incremental) multi-query (self) attention is identical to the code listed above for multi-head attention, except that we remove the letter \"h\" from the tf.einsum equations where it represents the \"heads\" dimension of $K, V, P_{k}$, or $P_{v}$. ```\ndef MultiqueryAttentionBatched(\n    \\(X, M\\), mask, P_q, P_k, P_v, P_o):\n    \" \" \" Multi-Query Attention. Args:\n        X: a tensor with shape \\([b, n, d]\\)\n        M: a tensor with shape \\([b, m, d]\\)\n        mask: a tensor with shape [b, h, n, m]\n        P_q: a tensor with shape [h, d, k]\n        P_k: a tensor with shape [d, k]\n        P_v: a tensor with shape d, v]\n        P_o: a tensor with shape \\([h, d, v]\\)\n    Returns:\n        Y: a tensor with shape \\([b, n, d]\\)\n    \" \" \"\n    \\(\\mathrm{Q}=\\mathrm{tf} . \\operatorname{einsum}(\\) \" bnd, hdk \\(\\rightarrow\\) bhnk \"\uff0cX, P_q)\n    \\(K=\\) tf.einsum(\"bmd, dk \\(\\rightarrow\\) bmk\"\uff0cM\uff0cP_k)\n    \\(\\mathrm{V}=\\mathrm{tf}\\). einsum(\"bmd, dv \\(\\rightarrow\\) bmv\"\uff0cM\uff0cP_v\uff09\n    logits = tf.einsum(\"bhnk, bmk->bhnm\"\uff0cQ\uff0cK\uff09\n    weights = tf.softmax (logits + mask \\()\\)\n    \\(O=\\) tf.einsum(\"bhnm,bmv->bhnv\", weights, V)\n    \\(\\mathrm{Y}=\\mathrm{tf} . \\mathrm{einsum}\\left(\\right.\\) \"bhnv, hdv \\(->\\) bnd\" \\(\\left., \\mathrm{O}, \\mathrm{P} \\_\\mathrm{o}\\right)\\)\n    return Y\n```\n\n```\ndef MultiquerySelfAttentionIncremental(\n    x, prev_K, prev__V, P_q, P_k, P_v, P__o):\n    \" \" \" Multi-query Self-Attention (one step). Args:\n        x: a tensor with shape [b, d]\n        prev__K: tensor with shape [b, m, k]\n        prev__V: tensor with shape [b, m, v]\n        P_q: a tensor with shape [h, d, k]\n        P_k: a tensor with shape [d, k]\n        P_v: a tensor with shape [d, v]\n        P_o: a tensor with shape [h, d, v]\n    Returns:\n        y: a tensor with shape [b, d]\n        new_K: tensor with shape [b, m+1, k]\n        new__V: tensor with shape [b, m+1, v]\n    \" \" \"\n    q = tf.einsum(\" bd, hdk->bhk\", x, P_q)\n    K}=\\textrm{tf.concat\n        [prev_K, tf.expand__dims(tf.einsum(\"bd,dk->bk\" , M, P__k), axis=2)],\n        axis=2)\n    V}=\\textrm{tf.concat(\n        [prev_V, tf.expand__dims(tf.einsum(\"bd,dv->bv\",M, P_v), axis=2)],\n        axis=2)\n    logits = tf.einsum(\"bhk,bmk->bhm\", q, K)\n    weights = tf.softmax(logits)\n    o = tf.einsum(\"bhm,bmv->bhv\", weights, V)\n    y = tf.einsum(\"bhv,hdv ->bd\" , O, P_o)\n    return y, K, V\n```\n\n\n### 3.1 Performance Analysis for Incremental Multi-Query Attention\n\nWe make the same simplifying assumptions as in section 2.3.1.",
    "mqa-4": "Across $n$ calls, the total number of arithmetic operations is again $\\Theta\\left(b n d^{2}\\right)$. Across $n$ calls, the total amount of memory access is $\\Theta\\left(b n d+b n^{2} k+n d^{2}\\right)$, the first term due to $x, q, o$ and $y$, the second term due to $K$ and $V$ and the third term due to $P_{q}, P_{k}, P_{v}, P_{o}$. Dividing the memory by the computations, we find that the ratio of memory access to arithmetic operations is $\\Theta\\left(\\frac{1}{d}+\\frac{n}{d h}+\\frac{1}{b}\\right)$. We have reduced the offensive $\\frac{n}{d}$ by a factor of $h$. Theoretically, given large batch size $b$, this should dramatically improve performance of incremental generation. In our experimental section, we will show that the performance gains are real and that model quality remains high. ## 4 Experiments and Results\n\n### 4.1 Experimental Setup\n\nFollowing [Vaswani et al., 2017], we evaluate on the WMT 2014 English-German translation task. As a baseline, we use an encoder-decoder Transformer model with 6 layers, using $d_{\\text {model }}=1024 d_{f f}=4096$, $h=8, d_{k}=d_{v}=128$, learned positional embeddings, and weight-sharing between the token-embedding and output layers. The baseline model and all variations have 211 million parameters. All models were trained for 100,000 steps ( 20 epochs). Each training batch consisted of 128 examples, each of which consisted of a 256 -token input sequence and a 256 -token target sequence (multiple training sentences were concatenated together to reach this length). Models were trained on a 32-core TPUv3 cluster, with each model taking about 2 hours to train. We used an implementation from the tensor2tensor and mesh-tensorflow libraries. The configurations used can be found at [to be added before publication], including details about learning rates, dropout, label smoothing, etc. In our \"multi-query\" model, we replace all of the attention layers in the model to multi-query attention. This includes the encoder-self-attention, decoder-self-attention and encoder-decoder-attention layers. We widen the feed-forward hidden layers from 4096 to 5440 to make the total parameter-count equal to that of the baseline. To demonstrate that local-attention and multi-query attention are orthogonal, we also trained \"local\" versions of the baseline and multi-query models, where the decoder-self-attention layers (but not the other attention layers) restrict attention to the current position and the previous 31 positions. A simpler alternative way to reduce the sizes of $K$ and $V$ is to reduce the number of heads $h$ and/or to reduce the dimensionalities $k$ and $v$ of the keys and values. We trained several such models for comparison, again widening the feed-forward hidden layers to make the total parameter-count equal to that of the baseline. We preformed a similar set of experiments using \"transformer-decoder\" language models on the BillionWord Language Modeling Benchmark [Chelba et al., 2013]. For the baseline, we use a model with 6 layers, $d_{\\text {model }}=1024 d_{f f}=8192, h=8, d_{k}=d_{v}=128$. The total parameter count is 192 million for the baseline and for all variations. We trained for 136 K steps ( 10 epochs) at a batch size of 64 K tokens. Again, we used a 32-core TPUv3 cluster for approximately 3 hours to train each model. ### 4.2 Model Quality\n\nTable 1 shows results for the machine-translation experiments. We decoded the dev set using greedy maximum-likelihood decoding and computed BLEU score with sacrebleu \"sacrebleu -t wmt13 -l en-de -tok intl\". We also list per-subword-token perplexity on the dev set. According to both of these metrics, the multi-query attention model seems to be slightly worse than the baseline, but much closer than any of the alternatives involving decreasing $h, d_{k}$ and $d_{v}$. We validated the results by decoding the test set using both greedy decoding and beam search (beam $4, \\alpha=0.6$ ), and evaluated with sacrebleu \"sacrebleu -t wmt14 -l en-de -tok intl\". Again, the multiquery model performed similarly to the baseline, and actually had the highest BLEU score (28.5) with beam- 4 decoding. Table 3 shows results for the billion-word language modeling benchmark. Models were evaluated by perword (not per-subword-token) perplexity on the dev set. The results paint a similar picture to the translation results. The multi-query attention model was slightly worse than the baseline, but significantly better than any of the alternatives involving decreasing $h, d_{k}$ and $d_{v}$. ### 4.3 Speed\n\nTable 2 shows training and inference times for the various models. Both training and inference speeds were evaluated on one TPUv2 (8 cores). A training step (consisting of 32,768 input tokens and 32,768 target tokens, as described above) took 433 ms for the base model and 425 ms for the multi-query model. Dividing by 32,768 , we find that the training time is $13.2 \\mu \\mathrm{s}$ per (input-token + target-token), as listed in Table 2. We ran incremental greedy inference on a batch of 1024 sequences ( 128 per core) using a source-sequence length of 128 tokens and a target sequence length of $128 .{ }^{1}$ For the baseline model, the encoder part of the model took 222 ms and each incremental step of the decoder took 47 ms . Dividing by the respective numbers of tokens, we find that the amortized inference time is $1.7 \\mu \\mathrm{s}$ per token for the encoder and a much larger $46 \\mu s$ per token for the decoder, as listed in Table 2. For the multi-query model, the encoder took 195 ms and the decoder took 3.9 ms per step, for amortized per-token costs of $1.5 \\mu \\mathrm{s}$ and $3.8 \\mu \\mathrm{s}$ respectively. Table 2 shows these values as well as similar results for beam-search. [^0]Table 1: WMT14 EN-DE Results. | Attention <br> Type | $h$ | $d_{k}, d_{v}$ | $d_{f f}$ | $\\ln (\\mathrm{PPL})$ <br> $(\\mathrm{dev})$ | BLEU <br> $(\\mathrm{dev})$ | BLEU (test) <br> beam 1 / 4 |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| multi-head | 8 | 128 | 4096 | $\\mathbf{1 . 4 2 4}$ | $\\mathbf{2 6 . 7}$ | $27.7 / 28.4$ |\n| multi-query | 8 | 128 | 5440 | 1.439 | 26.5 | $27.5 / \\mathbf{2 8 . 5}$ |\n| multi-head local | 8 | 128 | 4096 | 1.427 | 26.6 | $27.5 / 28.3$ |\n| multi-query local | 8 | 128 | 5440 | 1.437 | 26.5 | $27.6 / 28.2$ |\n| multi-head | 1 | 128 | 6784 | 1.518 | 25.8 |  |\n| multi-head | 2 | 64 | 6784 | 1.480 | 26.2 | $26.8 / 27.9$ |\n| multi-head | 4 | 32 | 6784 | 1.488 | 26.1 |  |\n| multi-head | 8 | 16 | 6784 | 1.513 | 25.8 |  |\n\nTable 2: Amortized training and inference costs for WMT14 EN-DE Translation Task with sequence length 128. Values listed are in TPUv2-microseconds per output token. | Attention <br> Type | Training | Inference <br> enc. + dec. | Beam-4 Search <br> enc. + dec. |\n| ---: | :---: | :---: | :---: |\n| multi-head | 13.2 | $1.7+46$ | $2.0+203$ |\n| multi-query | $\\mathbf{1 3 . 0}$ | $1.5+3.8$ | $1.6+32$ |\n| multi-head local | 13.2 | $1.7+23$ | $1.9+47$ |\n| multi-query local | $\\mathbf{1 3 .",
    "mqa-5": "0}$ | $\\mathbf{1 .",
    "mqa-6": "5}+\\mathbf{3 . 3}$ | $\\mathbf{1 . 6}+\\mathbf{1 6}$ |\n\nTable 3: Billion-Word LM Benchmark Results. | Attention | $h$ | $d_{k}, d_{v}$ | $d_{f f}$ | dev-PPL |\n| :---: | :---: | :---: | :---: | :---: |\n| multi-head | 8 | 128 | 8192 | $\\mathbf{2 9 . 9}$ |\n| multi-query | 8 | 128 | 9088 | 30.2 |\n| multi-head | 1 | 128 | 9984 | 31.2 |\n| multi-head | 2 | 64 | 9984 | 31.1 |\n| multi-head | 4 | 32 | 9984 | 31.0 |\n| multi-head | 8 | 16 | 9984 | 30.9 |\n\n## 5 Conclusion\n\nWe have proposed multi-query attention - an alternative to multi-head attention with much lower memorybandwidth requirements in the incremental setting.",
    "mqa-7": "We believe that this enables wider adoption of attentionbased sequence models in inference-performance-critical applications. ## References\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate, 2014. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. One billion word benchmark for measuring progress in statistical language modeling. CoRR, abs/1312.3005, 2013. URL http://arxiv.org/abs/1312.3005. Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In Proceedings of the International Conference on Learning Representations, 2018. Daniel Povey, Hossein Hadian, Pegah Ghahremani, Ke Li, and Sanjeev Khudanpur. A time-restricted selfattention layer for ASR.",
    "mqa-8": "In Proceddings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. Biao Zhang, Deyi Xiong, and Jinsong Su. Accelerating neural transformer via an average attention network, 2018 . [^0]:    ${ }^{1}$ Due to system limitations requiring fixed shapes, we used padding and masking in our decoder-self-attention implementation. The memory tensors were thus padded to the maximum length (128), or to the window-size (32) in the case of local attention. Each decoding step thus took the same amount of time. An alternative implementation of incrementally growing the tensors could save time near the beginning of the sequence. "
}