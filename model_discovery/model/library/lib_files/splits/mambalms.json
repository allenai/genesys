{
    "mambalms-0": "# An Empirical Study of Mamba-based Language Models \n\nRoger Waleffe ${ }^{1,2 *}$ Wonmin Byeon ${ }^{1}$ Duncan Riach ${ }^{1}$ Brandon Norick ${ }^{1 \\dagger}$<br>Vijay Korthikanti ${ }^{1}$ Tri Dao ${ }^{3,4}$ Albert Gu ${ }^{5,6}$ Ali Hatamizadeh ${ }^{1}$ Sudhakar Singh ${ }^{1}$<br>Deepak Narayanan ${ }^{1}$ Garvit Kulshreshtha ${ }^{1}$ Vartika Singh ${ }^{1}$ Jared Casper ${ }^{1}$<br>Jan Kautz ${ }^{1}$ Mohammad Shoeybi ${ }^{1}$ Bryan Catanzaro ${ }^{1}$<br>${ }^{1}$ NVIDIA $\\quad{ }^{2}$ University of Wisconsin-Madison ${ }^{3}$ Princeton University<br>${ }^{4}$ Together AI $\\quad{ }^{5}$ Carnegie Mellon University ${ }^{6}$ Cartesia AI\n\n\n#### Abstract\n\nSelective state-space models (SSMs) like Mamba (Gu and Dao 2023) overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache.",
    "mambalms-1": "Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same training data), however, studies so far have only presented small scale experiments (training with $<3 \\mathrm{~B}$ parameters and $<1 \\mathrm{~T}$ tokens) comparing SSMs to equivalent Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to an 8B-parameter hybrid architecture consisting of $43 \\%$ Mamba-2, $7 \\%$ self-attention, and $50 \\%$ MLP layers (Mamba-2-Hybrid). Using a diverse set of natural language tasks, we answer the important question of whether Mamba models can match their Transformer counterparts at larger training budgets. Our results show that while pure SSM-based models match or exceed Transformers on many tasks, both Mamba and Mamba-2 models lag behind Transformer models on tasks which require strong copying or in-context learning abilities (e.g., five-shot MMLU, Phonebook Lookup) or long-context reasoning. In contrast, we find that the 8B-parameter Mamba2-Hybrid exceeds the 8B-parameter Transformer on all 12 standard tasks we evaluated $(+2.65$ points on average) and is predicted to be up to $8 \\times$ faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support $16 \\mathrm{~K}, 32 \\mathrm{~K}$, and 128 K sequence lengths. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our SSM-based models as part of NVIDIA's Megatron-LM project (https://github.com/NVIDIA/Megatron-LM) ${ }^{1}$. ## 1 Introduction\n\nTransformer-based large language models (LLMs) (Vaswani et al. 2017) have become the dominant neural network architecture for natural language processing and have achieved impressive results across a wide array of tasks (Achiam et al. 2023; Touvron et al. 2023). Much of the success of these models can be attributed to their self-attention layers (Bahdanau, Cho, and Bengio 2014), which enable all-to-all information routing between tokens in a sequence, and their ability to improve with scaling model and dataset sizes. However, self-attention layers suffer from some drawbacks that make training and deploying these models on long sequences challenging. At training time, the computation required for self-attention layers scales quadratically with the sequence length. At inference time, generating one token requires a memory capacity that scales linearly with the number of preceding tokens, necessitating a large key-value cache to store the required state. Many recent works have attempted to address the\n\n[^0]efficiency issues with self-attention layers (Tay et al. 2022); these works however have yet to match self-attention's language modeling capabilities. Structured state space models (Gu, Goel, and Re 2021), in particular Mamba (Gu and Dao 2023) and more recently Mamba-2 (Dao and Gu 2024), have been proposed as a promising alternative to self-attention layers and Transformers. These models use constant computation and memory to generate a single token at inference time (after initializing the SSM states based on the context) and can be computed efficiently using hardware-aware algorithms during training. They have been shown to match or exceed the downstream accuracy of Transformers on standard language modeling tasks for models up to 2.8 B parameters (Dao and Gu 2024; Gu and Dao 2023). Follow up work has sought to further probe the in-context learning abilities of these models at small scale (Park et al. 2024), and some recent work has investigated combining Mamba layers with attention layers to form hybrid models (Glorioso et al. 2024; Lieber et al. 2024). These works scale Mamba-based hybrid models beyond 7B parameters and show that doing so can result in high quality models. However, in these studies the larger models were not compared with equivalent Transformers in a controlled setting (i.e., same training data, parameter count). Such controlled comparisons have been limited to small-scale experiments and larger-scale studies of Mamba-2 models are still lacking. In this technical report, we present a direct comparison between Mamba-based and Transformer-based LLMs trained on large datasets. In particular, our primary goal is to provide a rigorous apples-to-apples comparison between Mamba, Mamba-2, Mamba-2-Hybrid (containing Mamba-2, attention, and MLP layers), and Transformers for 8B-parameter models trained on up to 3.5 T tokens, with the same hyperparameters. Using a diverse set of natural language tasks, we answer the important question of whether Mamba models can match their Transformer counterparts at larger training budgets. We evaluate these models on 35 popular downstream language modeling tasks and use the exact same evaluation setup for Mamba-based and Transformer models. To ensure our evaluations are standard and reproducible, we provide details about the specific open-source benchmark suites and versions used in our experiments in Section 2. Overall, our experiments eliminate the common difficulty of comparing LLMs, where it is often the case that both the model architecture but also the training data, tokenizer, and evaluation pipeline have changed. Our experiments show that while Mamba and Mamba-2 models are good at modeling language (e.g., they match or exceed Transformers on many downstream tasks), they lag behind Transformer models when it comes to in-context learning and recalling information from the context.",
    "mambalms-2": "This confirms recent findings at smaller scales (Park et al. 2024). In particular, we highlight the difficulty pure SSM models face with the standard five-shot MMLU (Hendrycks, Burns, et al. 2020) and two-shot Phonebook tasks. For the former, after training for 1.1 T tokens, both Mamba and Mamba-2 models produce nearly 15 points lower accuracy when compared to a Transformer model on this task. While the MMLU accuracy gap is partially addressed by training with more tokens (e.g., 3.5T tokens), SSM models still lag behind Transformer models for this common benchmark. We find that Phonebook and standard long-context benchmark tasks remain challenging for SSM models regardless of the number of training tokens. Based on the above findings, we study in detail the potential for hybrid SSM-Transformer models to overcome the challenges faced by pure SSM architectures while retaining (some of) their inferencetime benefits. Similar to Lieber et al. 2024, we focus on LLMs consisting of a mixture of Mamba-2, self-attention, and MLP layers. Our ablation experiments aiming to identify the best hybrid model architecture lead us to design an 8B-parameter Mamba-2-Hybrid with 24 Mamba-2 layers, 4 self-attention layers, and 28 MLP layers. The self-attention and MLP layers are evenly distributed throughout the model. Extensive evaluations of this architecture show that it matches or exceeds Transformers on common natural language evaluations. When training for 3.5 T tokens, a Mamba-2-Hybrid model exceeds a corresponding Transformer on all 12 short-context benchmarks we evaluated. On MMLU, the hybrid model reaches a five-shot accuracy 3.5 points higher than the Transformer. We also study long-context extensions of Mamba-2-Hybrid and the corresponding Transformer to support 16 K and 32 K context lengths. On 23 long-context evaluations, the 16 K and 32 K models closely match or exceed the Transformer baselines on average. Our results show that the hybrid models are particularly good at retrieving, tracking, and aggregating information over long contexts. We highlight three multi-document question answering tasks, however, which challenged the long-context hybrid models. We discuss potential reasons for these results and highlight areas of future work related to\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b92355d6bd48fca7f050g-03.jpg?height=744&width=1491&top_left_y=190&top_left_x=315)\n\nFigure 1: Mamba and Mamba-2 blocks with tensor model parallel size two. Mamba requires two all-reduces per layer while Mamba-2 requires only one. More details can be found in Dao and Gu 2024. extending hybrid SSM-Transformer models to long sequence lengths. Finally we highlight that, due to our use of global attention without any explicit position encoding in these models, long-context Mamba-2-Hybrid models can generalize beyond their trained sequence length. This is in contrast with recent hybrid models that use windowed attention and exhibit accuracy degradation on contexts larger than the window size but less than the pretraining sequence length (De et al. 2024). We find that a Mamba-2-Hybrid extended to support 128 K contexts can perform the Phonebook lookup task perfectly even when the phone book contains more than 150 K tokens. We present our findings above to highlight the promise for larger-scale SSM-based models to provide faster, more efficient language model inference without compromising training efficiency or model accuracy compared to Transformers. We hope that by releasing these results, the community is further excited by the potential of Mamba-based LLMs. To help enable further adoption, we release the code used to train our Mamba, Mamba-2, and Mamba-2-Hybrid hybrid models as part of NVIDIA's Megatron-LM library (https:// github.com/NVIDIA/Megatron-LM). We also release the model weights for our Mamba-2 8B and Mamba-2-Hybrid 8B on Hugging Face. ## 2 Preliminaries\n\nIn this section, we discuss briefly our implementation of SSM layers in Megatron-LM and discuss the training data and evaluations used throughout this report. ### 2.1 Model Implementation\n\nTo support efficient large-scale training, we implement Mamba and Mamba-2 layers with support for tensor (Shoeybi et al. 2019), sequence (Korthikanti et al. 2022), and pipeline parallelism (Narayanan et al. 2021) (only for Mamba-2). As described in Dao and Gu 2024, tensor-parallel support for Mamba layers requires two all-reduces per block compared to just one all-reduce for Transformer layers (Figure 1), leading to increased communication overheads for training larger-scale Mamba models. Mamba-2 tensor parallel support, on the other hand, requires only one all-reduce per layer, but requires the use of GroupNorm rather than LayerNorm for the internal block normalization (see Figure 1). We found that using GroupNorm lead to no difference in validation loss when compared to using full LayerNorm as long as the group size (the model hidden dimension divided by the number of groups) is sufficiently large to allow for accurate calculations of the per-group normalization statistics (in our\nexperience this meant a group size greater than 256). To implement SSM-Transformer hybrid models, we combine our Mamba or Mamba-2 layers with the existing self-attention and MLP layers supported in Megatron-LM. These layers support all the previously mentioned parallelization strategies enabling us to immediately train hybrid models with tensor, sequence, and pipeline parallelism. ### 2.2 Training Data\n\nWe train the models discussed in this report on 1.1 T and 3.5 T token datasets. Both datasets are predecessors of the dataset used to train Nemotron-4 and are comprised of $70 \\%$ English, $15 \\%$ non-English, and $15 \\%$ code. For additional details, refer to the discussion included in the Nemotron- 4 technical report (Parmar et al. 2024). We use a vocabulary of 256 K tokens trained with SentencePiece (Kudo and Richardson 2018). ### 2.3 Evaluation Tasks and Setup\n\nWe now discuss the evaluations used throughout the paper. Wherever possible, we use open-source LLM benchmark suites to ensure our evaluations are standard and reproducible. We report results using a large number of common tasks:\n\n- Standard Short-Context Tasks: We use the open-source LM Evaluation Harness library (commit 94cc1850) (L. Gao et al. 2023) to evaluate the following 12 tasks (metric used for evaluation reported in parentheses): WinoGrande (accuracy) (Sakaguchi et al. 2021), PIQA (accuracy) (Bisk et al. 2020), HellaSwag (normalized accuracy) (Zellers et al. 2019), ARCEasy and ARC-Challenge (accuracy and normalized accuracy) (Clark et al. 2018), MMLU (accuracy) (Hendrycks, Burns, et al. 2020), OpenBookQA (normalized accuracy) (Mihaylov et al. 2018), TruthFulQA (accuracy) (Lin, Hilton, and Evans 2021), PubMedQA (accuracy) (Jin et al. 2019), and RACE (accuracy) (Lai et al. 2017). Each of the proceeding tasks are evaluated by measuring the probability returned by the model for each possible answer choice. We also use the generation-based tasks Natural Questions (NQ) (exact match) (Lee, Chang, and Toutanova 2019) and SquadV2 (F1) (Rajpurkar, Jia, and Liang 2018). - Natural Long-Context Tasks: To evaluate long-context models, as above, we use three tasks from LM Evaluation Harness: NarrativeQA (F1) (Ko\u010disk\u1ef3 et al.",
    "mambalms-3": "2018), Qasper (F1) (Dasigi et al. 2021), and QuALITY (normalized accuracy) (Shaham et al. 2022). The first two tasks are generation-based, while the latter uses continuation probabilities returned by the model for each answer. Each of these three tasks requires the model to answer a given question based on a long input document. We also use six tasks from the LongBench (Bai et al. 2023) long-context evaluation benchmark (commit 48798083): MultiFieldQA-English (F1), HotpotQA (F1) (Yang et al. 2018), 2WikiMQA (F1) (Ho et al. 2020), Musique (F1) (Trivedi et al. 2022), TREC (accuracy) (Li and Roth 2002), and TriviaQA (F1) (Joshi et al. 2017). Each of these six tasks requires the model to generate the answer. MultiFieldQA tests a model's ability to perform single-document question answering while HotpotQA, 2WikiMQA, and Musique measure multi-document question answer capabilities. TREC and TriviaQA are used to measure a model's ability to perform in-context learning over long inputs. - Synthetic Long-Context Tasks: Finally, we also evaluate our models using synthetic tasks that aim to measure a model's ability to retrieve, track, and aggregate information across long input texts. For these evaluations, we use the Phonebook task introduced in Jelassi et al. 2024 (illustrated in Figure 2b) and 13 open-source, generation-based tasks in the RULER benchmark, described explicitly in Appendix B of Hsieh et al. 2024. The RULER tasks consist of eight Needle In A Haystack (NIAH) variants, one multi-hop tracing task called Variable Tracking (VT), two long-context aggregation tasks (Common Words Extraction (CWE) and Keywords Extraction (KWE)), one single-document question answer task (SquadQA), and one multi-document question answer task (HotpotQA). For all tasks, we report the accuracy on 400 synthetic samples generated by RULER. Table 1: 8B-parameter Mamba, Mamba-2, and Transformer architectures used in the experiments. | Model | Params (B) | \\# Layers | Model Dim | Attn. Heads | State Dim. | \\# Groups | Pos. Emb. | Seq. Len |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer | 8.53 | 32 | 4096 | 32 | - | - | Rope | 4096 |\n| Mamba | 8.15 | 56 | 4096 | - | 128 | - | None | 4096 |\n| Mamba-2 | 8.24 | 56 | 4096 | - | 128 | 8 | None | 4096 |\n\n## 3 Mamba and Mamba-2 Compared to Transformers\n\nIn this section we discuss our observations and experimental results training 8 billion (8B) parameter Mamba and Mamba-2 models and compare with 8B-parameter Transformer models. We find that Mamba and Mamba-2 can match or exceed Transformers on standard zero-shot tasks (Section 3.3.1) but lag behind on MMLU and copying tasks, which we discuss in details in Sections 3.3.2 and 3.3.3. ### 3.1 Model Architectures\n\nWe train Mamba, Mamba-2, and Transformer models with the architectures summarized in Table 1. We discuss the architectures in more detail next. Additional details can be found in the released model checkpoints and open-sourced code in Megatron-LM. Transformer. Our 8B Transformer model follows the style of GPT3 (Brown et al. 2020) and consists of 32 layers (each Multi-Head Attention + MLP) with a hidden dimension of 4096. We use 32 attention heads, 128 KV -channels, a $4 \\times$ expansion for the MLPs, SwiGLU activation (Shazeer 2020), LayerNorm (Ba, Kiros, and Hinton 2016), and RoPE (J. Su et al. 2024) for position embeddings. We do not use bias weights for linear layers or Dropout. Additionally, we use seperate parameters for model embeddings and output layer weights (which we refer to as untied embeddings). Mamba. We train an 8B-parameter Mamba model with hidden dimension 4096 and 56 layers (typically 2 Mamba layers have around the same parameters as one block of attention + MLP). The state dimension for each Mamba layer is set to 128 and we use GELU (Hendrycks and Gimpel 2016) activation. Following (Gu and Dao 2023), we do not use any explicit position encoding and for normalization we use RMSNorm (B. Zhang and Sennrich 2019). As for the Transformer, we do not use bias weights for linear layers or Dropout and we use untied embeddings. Mamba-2. For Mamba-2 models, we use the same architecture as above for Mamba except replace each layer with the updated Mamba-2 block (Dao and Gu 2024). We set the internal Mamba-2 state dimension to 128 and use eight groups. We retain the default values from Dao and Gu 2024 and use a head dimension of 64 , expansion factor of two, and window size of four for convolution. ### 3.2 Training Hyperparameters\n\nWe train the above models on 1.1T and 3.5T token datasets (see details in Section 2) using the following hyperparameters: On the smaller dataset, we use a batch size of 256, peak learning rate of $1 \\mathrm{e}-4$ and minimum learning rate of $1 \\mathrm{e}-5$. On the larger dataset we increase the batch size to 1024 and use higher learning rates: a peak of $3 \\mathrm{e}-4$ and minimum of $3 \\mathrm{e}-5$. On both datasets we use learning rate warm up over 122 K samples, a cosine learning rate schedule, weight decay of $0.1,0.9$ and 0.95 for Adam $\\beta_{1}$ and $\\beta_{2}$ parameters respectively, and train using BF16. We performed some studies at smaller scale and found that Mamba network hyperparameters are similar to that of Transformers and as a result we use the same hyperparameters across models to make a rigorous direct comparison. ### 3.3 Empirical Evaluation of Mamba and Mamba-2\n\n### 3.3.1 Downstream Language Modeling Tasks\n\nIn Table 2 and 3 we report the results of training our 8B-parameter Mamba, Mamba-2, and Transformer models on 1.1 T and 3.5 T tokens respectively, using six standard tasks for measuring natural language understanding. On the 3.5 T dataset, we train only a pure Mamba- 2 model (and not a Mamba model)\n\nTable 2: Evaluation results for 8B-parameter models trained on 1.1T tokens. Pure SSM models (Mamba and Mamba-2) match or exceed Transformers on many natural language tasks, but fall short on others (e.g., MMLU) (see Section 3.3). | Model | WinoGrande | PIQA | HellaSwag | ARC-E | ARC-C | MMLU |  | Avg. w/o MMLU | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  |  |  |  |  | 0 -Shot | 5-Shot |  |  |\n| Transformer | 69.22 | 78.29 | 75.6 | 73.15 | 43.09 | 38.32 | 46.28 | 67.87 | 60.56 |\n| Mamba | 68.27 | 78.89 | 75.63 | 75.42 | 42.15 | 28.63 | 28.00 | 68.07 | 56.71 |\n| Mamba-2 | 70.8 | 78.35 | 75.54 | 75.13 | 43.00 | 28.94 | 29.19 | 68.56 | 57.28 |\n\nfor efficiency reasons - the pure Mamba model on the 1.1 T dataset was almost $3 \\times$ slower than the pure Mamba-2 model due to the large state dimension. Our results confirm those of prior works (Dao and Gu 2024); both Mamba and Mamba-2 models can match or exceed Transformer models on common tasks. On both datasets, pure SSM models achieve higher accuracy than Transformers when averaged over the WinoGrande, PIQA, HellaSwag, ARC-Easy, and ARC-Challenge evaluation tasks. The results on the 1.1T dataset also highlight that pure Mamba-2 models are equal or better than Mamba models on average. The most interesting observation from these results, however, is that the accuracy on MMLU is significantly worse for pure SSM models compared to the Transformer when training for the shorter token horizon (1.1T tokens). For example Mamba-2 five-shot accuracy is 17 points lower than that of the Transformer in this setting. Table 3 shows that training for more tokens helps the Mamba-2 model improve on MMLU (visualized in Figure 6), closing the gap to the Transformer to just 1.37 points. We discuss the MMLU result in more detail in the following section. Table 3: Evaluation results for 8B-parameter models trained on 3.5 T tokens. We train only pure Mamba-2 and Transformer models in this setting due to efficiency issues with training larger-scale Mamba models. Training for 3.5 T tokens instead of 1.1 T (Table 2) allows Mamba-2 to approach Transformer-level accuracy on MMLU and produces a pure SSM model that exceeds the Transfomer in terms of average task accuracy. | Model | WinoGrande | PIQA | HellaSwag | ARC-E | ARC-C | MMLU | 0-Shot | 5-Shot | Avg. $\\mathrm{n} / \\mathrm{MLU}$ | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  |  |  |  |  | 45.69 |  | 68.14 |  |  |\n| Mamba-2 | $\\mathbf{7 1 .",
    "mambalms-4": "5 9}$ | $\\mathbf{7 9 . 8 2}$ | $\\mathbf{7 7 . 6 9}$ | $\\mathbf{7 5 . 9 3}$ | $\\mathbf{4 8 . 1 2}$ | $\\mathbf{4 7 .",
    "mambalms-5": "2 5}$ | 48.7 | $\\mathbf{7 0 . 6 3}$ | $\\mathbf{6 4 . 1 6}$ |  |\n\n### 3.3.2 A Closer Look at MMLU\n\nWe investigate the gap in MMLU accuracy between pure SSM models and Transformers by evaluating our 1.1T Mamba, Mamba-2, and Transformer models (where the gap is largest) on different instances of this task. Generally, MMLU accuracy is calculated by prompting the model with a question that includes four answer choices labeled with the letters A, B, C, and D. The model is then shown each of the four letters A, B, C, and D and the letter most likely to follow the prompt (measured by probabilities output from the model) is taken as the model's answer (Figure 2a). MMLU accuracy, however, can also be measured by calculating the probability of the full answer choice following the prompt (which we call a choice-text-in-targets variant) or using a cloze format. In the latter case, the model is prompted with only the question (no answer choices are provided) and the text of each answer is used to calculate probabilities. We show the results of evaluating our 8B-parameter pure SSM and Transformer models trained on 1.1T tokens on the three formulations of MMLU described above in Table 4. While the pure SSM models struggle with the standard and choice-text-in-targets formulations, they actually exceed the accuracy of the Transformer in the cloze setting. This experiment, together with the MMLU results for Mamba-2 trained on 3.5 T tokens (Table 3, Figure 6), highlight that the pure SSM models contain the same knowledge as the Transformer, but that they require substantially more training to understand the format of the multiple choice questions in the first two settings. We hypothesize\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b92355d6bd48fca7f050g-07.jpg?height=593&width=759&top_left_y=202&top_left_x=288)\n(a) Three different formats for MMLU multiple choice questions. In all cases the model is first prompted with a question. In the standard and 'choice text in targets' variants (but not 'cloze' variant), the prompt includes four multiple choice answers following the question. The correct answer is then calculated by measuring which of the four answers, represented in the target format, is predicted to have the highest probability of following the given prompt. ![](https://cdn.mathpix.com/cropped/2024_09_12_b92355d6bd48fca7f050g-07.jpg?height=605&width=749&top_left_y=196&top_left_x=1076)\n(b) Two different versions of the synthetic Phonebook task. In the standard configuration (left) the model is first prompted with a phone book. It is then shown two in-context example questions before the final question which asks it to recall the phone number for a specific person. In the 'reversed' setting (right) the model is told which phone number to look for before being shown the phone book and then later asked to recall this number. Figure 2: Illustrations of the MMLU and Phonebook tasks used in the experiments. Table 4: MMLU accuracy for pure SSM and Transformer models according to the three task variants described in Figure 2a. Results are for 8B-parameter models trained on 1.1T tokens. While pure SSM models struggle with the standard and choice-text-in-targets formulations on this token horizon, they match or exceed the Transformer model in the cloze setting. Together with the results for Mamba-2 trained on 3.5 T tokens (Table 3, Figure 6), this suggests that SSM models contain the same knowledge as Transformers, but require more training to understand the MMLU task formatting. | Model | MMLU-Standard |  | MMLU-W/Choice |  | MMLU-Cloze |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | 0-Shot | 5-Shot | 0-Shot | 5-Shot | 0-Shot | 5-Shot |\n| Transformer | $\\mathbf{3 8 .",
    "mambalms-6": "3 2}$ | $\\mathbf{4 6 . 2 8}$ | $\\mathbf{3 3 . 5 4}$ | $\\mathbf{4 6 . 6 4}$ | 37.26 | 39.24 |\n| Mamba | 28.63 | 28.00 | 27.42 | 29.17 | $\\mathbf{3 8 . 2 6}$ | $\\mathbf{3 9 . 2 8}$ |\n| Mamba-2 | 28.94 | 29.19 | 28.54 | 30.68 | 37.68 | 38.17 |\n\nthat the reason for this confusion, especially in the standard MMLU setting, is that pure SSM models are unable to directly route the knowledge of each answer into a single answer token. In contrast, the self-attention layers in the Transformer are particularly good at that task, especially when they are shown several in-context examples that teach them to do such routing (e.g., 5 -Shot MMLU in the standard formulation). Finally, we note that while the Mamba-2 hybrid model trained for 3.5T tokens closes the MMLU gap to the Transformer, it sees an accuracy improvement on standard MMLU of only 1.45 points when moving from 0 - to 5 - shot, compared with 4.38 for the Transformer, providing additional evidence that Transformers may have superior in-context learning capabilities. ### 3.3.3 Copying Tasks\n\nBeyond downstream language modeling tasks, we also evaluate pure SSM-based models and compare to Transformers on the synthetic Phonebook task (Jelassi et al. 2024) that aims to measure a model's ability to perform in-context learning (through few shot examples) and copying from earlier in the context. We illustrate an example Phonebook prompt in Figure 2b. The model is first prompted with a list of (name, phone number) pairs, and then asked 'What is the phone number for \\{name\\}?' with two\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b92355d6bd48fca7f050g-08.jpg?height=404&width=1541&top_left_y=186&top_left_x=292)\n\nFigure 3: Evaluation results for pure SSM and Transformer models (trained for 1.1T tokens) on the Phonebook task illustrated in Figure 2b. (a) On the standard Phonebook task, Transformers are capable of in-context learning and answering questions that require copying from the input, but SSM models struggle with this task. (b) In the standard Phonebook setting (i.e., (a)), SSM models exhibit fuzzy memory - while they are unable to correctly predict the phone number, they predict phone numbers that share multiple digits (in the right locations) with the correct answer (see Section 3.3.3). (c) On the Reversed Phonebook formulation, even when notified at the beginning of the context which phone number they will be asked to recall, SSM models still lag behind Transformer models. example question answer pairs before the actual question used for testing. For each trial, we randomly generate names and phone numbers to create the phone book and randomly select which names are used for the two examples and the final query. Accuracy on this task is then measured by whether the model generates the correct phone number or not. We vary the length of the phone book (the number of (name, phone number) pairs) and plot the accuracy for each phone book length averaged over 20 different random initializations in Figure 3a. The 8B Transformer model can respond correctly with near $100 \\%$ accuracy for phone book lengths up to its pretraining context length (4096). In contrast, both Mamba and Mamba- 2 models begin to respond incorrectly for input sequence lengths beyond approximately 500 tokens. In contrast to MMLU, this behavior persists for Mamba-2 even when training for 3.5 T tokens (Figure 7a). A closer look at the SSM model predictions shows that while they cannot perfectly recall the correct phone number, these models have compressed information about each phone book entry into their running states-we show in Figure 3b the average number of correct tokens predicted by Mamba and Mamba-2 on Phonebook by comparing the predicted answer to the true answer. Figure 3b shows that pure SSM-based models have fuzzy memory. That is, while they cannot predict the phone number exactly, they do generally respond with phone numbers that are similar to the correct answer. Finally, we evaluate whether changing the Phonebook prompt allows for SSM models to achieve better results. In particular, we prompt the model with the name of the person whose phone number it will be asked to recall before showing it the phone book (the Reversed formulation in Figure 2b). Figure 3c shows the results of the 8B Mamba, Mamab-2, and Transformer models in this modified Phonebook setting. Interestingly, while the SSM models achieve better accuracy as a function of phone book length using this prompt, the accuracy still degrades for phone books with lengths shorter than 4096 (the sequence length used for pretraining). Even with the modified Phonebook prompt, it remains challenging for the SSM to decide which information to store exactly and which information to forget on this task. We hypothesize that finetuning Mamba and Mamba-2 on the Phonebook task would lead to improved accuracy. ### 3.3.4 Takeaway\n\nOur experiments training 8B-parameter Mamba and Mamba-2 models showed that while these models achieve comparable or better accuracy than Transformers on many standard natural language modeling tasks, they achieve lower accuracy on others. In particular, we identified MMLU (with smaller token horizons) and Phonebook as challenging tasks for pure SSM-based models and hypothesize that this is because these tasks require in-context learning, information routing between tokens, and copying from the context. ## 4 Hybrid Mamba-Transformer Models\n\nMotivated by the difficulties pure SSM models face with retrieving information from the context and in-context learning, we now study the hypothesis that adding a few Transformer layers (made of self-attention and MLP layers) back into the architecture enables the model to overcome these issues. In this section we consider hybrid models containing a combination of Mamba/Mamba-2, self-attention, and MLP layers. ### 4.1 Designing a Hybrid SSM-Transformer Model\n\nWe begin by discussing the ablation studies that led us to design our final hybrid model architecture. For the experiments reported in this section, all model variants have the same number of parameters per layer. This ensures that model quality changes are not due to an increase or decrease in the overall number of parameters, and also that we can control the ratio of parameters by controlling the ratio of layers. To do so, we adjust both the number of attention heads (while keeping the head size constant) and the MLP expansion factor such that self-attention and MLP layers have approximately the same number of parameters as Mamba layers. Number of Attention and MLP Layers. We first study how the number of self-attention and MLP layers in a hybrid model impacts model quality. For these experiments, we train 130 M parameter hybrid models with 24 layers, and vary the percentage of the those layers that are attention and that are MLP. As we increase the percentage of these layer types, we evenly distribute them throughout the model, as described in Appendix A. We report the validation loss as a function of the attention layer ratio in Figure 4. From these experiments, we discover that validation loss is minimized when roughly $8 \\%$ of the layers are self-attention layers. Experiments with 840 M parameter models confirm that these findings scale across model sizes. These results are also consistent with those reported by Dao and Gu 2024. After\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b92355d6bd48fca7f050g-09.jpg?height=351&width=611&top_left_y=952&top_left_x=1212)\n\nFigure 4: Validation loss versus percentage of attention layers for $130 \\mathrm{M}-$ parameter hybrid Mamba-Transformer models ( 24 total layers). fixing the percentage of attention layers to 8, we vary the percentage of MLP layers between 5 and 50. We conclude that 30\\%-50\\% of the layers can be MLPs without increasing model loss. In general, we prefer larger MLP layer ratios from an efficiency perspective-with $50 \\%$ of the layers set as MLPs, training is $20 \\%$ faster than when MLP layers make up only $5 \\%$ of the model. Position Embeddings. We next evaluate whether or not to add Rotary Position Embeddings (RoPE) (J. Su et al. 2024) to every self-attention layer in a hybrid model. For these experiments, we train an 840M-parameter Mamba-Hybrid model on the 1.1 T token dataset with and without RoPE, each with an attention layer ratio of $8 \\%$. We use a 4096 sequence length. We then extend these base models to a context length of 16384 through continued pretraining on the longer sequence length for an additional 16B tokens. We experiment with and without adjusting the RoPE base frequency during continued pretraining (continued pretraining with an increase base frequency was introduced by Xiong et al. (2023)). Results are reported in Table 5. The base 840M model trained with RoPE provides a similar accuracy to the model without RoPE, but achieves a lower average accuracy after long context extension (regardless of whether the RoPE base frequency is modified or not). Based on these experiments, as in recent work (Lieber et al. 2024), we opt to ignore RoPE position embeddings for larger-scale hybrid models. Additional Ablation Experiments. We also evaluated how the ordering of Mamba/Mamba-2, self-attention, and MLP layers affects the resulting model's natural language abilities (measured with validation loss). When testing, following Park et al. (2024), we made certain that a Mamba layer appears at the beginning of the architecture - this ensures that the hybrid model can operate without position embeddings, as the first Mamba layer naturally learns to encode the positional information. Our experiments found no significantly better configuration than to evenly distribute self-attention and MLP layers throughout the model, as described in Appendix A. We did not find it necessary to\n\nTable 5: Comparison of 840M parameter hybrid Mamba-Transformer models with and without RoPE position embeddings when using an attention layer ratio of $8 \\%$ and training on 1.1 T tokens. We train a base model with a sequence length of 4096 . We extend this base model to a sequence length of 16384 with continued pretraining for an additional 16B tokens. We find that hybrid models do not need position embeddings, and may perform better without them on long contexts.",
    "mambalms-7": "| Seq. | RoPE | RoPE Freq. Base | WinoGrande | PIQA | HellaSwag | ARC-E | ARC-C | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| 4 K | no | - | 56.2 | 70.84 | 52.13 | 55.01 | 26.79 | 52.19 |\n|  | yes | 10 K | 55.8 | 71.71 | 52.3 | 55.43 | 26.71 | 52.39 |\n| 16 K | no | - | 56.83 | 72.31 | 53.86 | 56.52 | 27.65 | $\\mathbf{5 3 . 4 3}$ |\n|  | yes | 10 K | 54.62 | 71.49 | 53.17 | 56.23 | 27.56 | 52.61 |\n|  | yes | 500 K | 54.7 | 71.38 | 50.42 | 54.29 | 26.79 | 51.52 |\n\nTable 6: Summary of the 8B-parameter Mamba-2-Hybrid architecture used throughout the experiments. We design the layer pattern to spread attention and MLP layers evenly throughout the model (Appendix A). | Model | Params (B) | \\# Layers | Model Dim | Attn. Heads | State Dim. | \\# Groups | Pos. Emb. | Seq. Len |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Mamba-2-Hybrid | 8.66 | 56 | 4096 | 32 | 128 | 8 | None | 4096 |\n|  | Hybrid Layer Pattern $\\left(\\mathrm{M}=\\right.$ Mamba-2, ${ }^{*}=$ Self-Attention, $\\left.+=\\mathrm{MLP}\\right)$ |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |\n\nconstruct hybrid model architectures using a repeated block pattern. We also found that hybrid models can use self-attention layers with Group-Query Attention (GQA) (Ainslie et al. 2023) rather than Multi-Head Attention (MHA) with little degradation in model quality (validation perplexity increases $\\approx 0.04 \\%$ ). Given the decrease in the amount of computation and memory required for inference with GQA compared to MHA, we thus opt to use GQA when training larger-scale hybrid models. ### 4.2 Mamba-2-Hybrid 8B\n\nModel Architecture and Hyperparameters. Based on the study described in Section 4.1, we train an 8B-parameter hybrid SSM-Transformer model with the architecture summarized in Table 6. Out of 56 total layers, the hybrid model has $4(7.1 \\%)$ self-attention layers, $24(42.9 \\%)$ Mamba-2 layers, and $28(50 \\%)$ MLP layers. Rather than using a single repeated hybrid block structure, to construct our model we allocate the layers such that 1) a Mamba-2 layer comes first and 2) the attention and MLP layers are evenly distributed throughout the model, as described in Appendix A. We use Mamba-2 for the SSM-layers rather than Mamba, as the SSM scan used by Mamba-2 is up to $8 \\times$ faster than that of Mamba (Dao and Gu 2024). Moreover, our experiments in Section 3.3, showed that 8B-parameter Mamba-2 models match or exceed 8B-parameter Mamba models on common downstream natural language tasks. For the Mamba-2 layers, we use the same parameters as for our pure Mamba- 2 model (Section 3.1). That is, we use an internal state dimension of 128, eight groups, a head dimension of 64 , expansion factor two, and window size of four for convolution. For the attention layers, we use Group Query Attention with eight groups, 32 attention heads, and 128 KV-Channels. For MLP layers, we use a $4 \\times$ expansion ratio. Throughout the model, we use a hidden dimension of 4096 and GELU activation. We opt to use no explicit position embeddings. For each layer, we include a residual skip connection and RMSNorm before the Mamba-2, self-attention, or MLP block. As for the pure SSM and Transformer models, we do not use Dropout, biases for linear layers, and we use separate parameters for model embeddings and output layer weights (i.e., untied embeddings). We train our Mamba-2-Hybrid 8 B on the 1.1 T token and 3.5 T token datasets using the hyperparameters described in Section 3.1 (i.e., the exact same ones as for the Transformer models and pure SSM models). Training Efficiency. We highlight that our Mamba-2-Hybrid model implemented in Megatron-LM can be trained efficiently on thousands of GPUs. To do so, we compare our measured Model Flop Utilization (MFU) with that of Transformers. As in prior work (Korthikanti et al. 2022), we define the MFU as follows: First we define the model FLOPs per second to be the number of FLOPs required to\n\nTable 7: Detailed evaluation results on 12 common natural language tasks comparing an 8B-parameter hybrid model (Mamba-2-Hybrid) with the pure Mamba-2 SSM and Transformer models from Section 3.3 when training for 3.5T tokens. The Mamba-2-Hybrid model achieves the highest overall accuracy and is 2.65 points better than the Transformer on average. | Model | WG | PIQA | HellaSwag | ARC-E | ARC-C | MMLU | OpenBook | TruthFul | PubMed | RACE | NQ | SquadV2 | Avg |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  |  |  |  |  | 45.69 |  |  |  |  |  | 15.15 |  | 53.17 |\n| Mamba-2 | $\\mathbf{7 1 . 5 9}$ | $\\mathbf{7 9 . 8 2}$ | $\\mathbf{7 7 . 6 9}$ | 75.93 | $\\mathbf{4 8 . 1 2}$ | 47.25 | 48.7 | $\\mathbf{4 4 . 2}$ | 35.66 | $\\mathbf{7 5 . 2}$ | 37.7 | 17.17 | 51.9 | 54.69 |\n| Mamba-2-Hybrid | 71.27 | 79.65 | $\\mathbf{7 7 .",
    "mambalms-8": "6 8}$ | $\\mathbf{7 7 . 2 3}$ | 47.7 | $\\mathbf{5 1 . 4 6}$ | $\\mathbf{5 3 . 6 0}$ | 42.80 | $\\mathbf{3 8 . 7 2}$ | 69.80 | $\\mathbf{3 9 . 7 1}$ | $\\mathbf{1 7 .",
    "mambalms-9": "3 4}$ | $\\mathbf{5 8 . 6 7}$ | $\\mathbf{5 5 . 8 2}$ |\n\nperform a model forward and backward pass divided by the iteration time. We can then define the MFU to be the model FLOPs per second divided by the peak theoretical FLOPs per second of the GPUs used for training. When training on NVIDIA H100 GPUs (NVIDIA 2023), with a tensor-parallel size of four and data-parallel size of 256 (1024 total GPUs) (micro batch size 4, global batch size 1024), our Mamba-2-Hybrid achieves an MFU of $29.9 \\%$. This can be compared to the $30.7 \\% \\mathrm{MFU}$ of a corresponding 8B parameter Transformer implemented in Megatron-LM and trained with the same parallelization configuration. Inference Speed. We also highlight that the hybrid model benefits from the inference-time speedups expected of a pure SSM model compared to a pure Transformer model. In Figure 5, we plot the predicted time to generate one token for the 8B Transformer model over the time for the 8B Mamba-2-Hybrid model using a batch size of 32 . For short input context lengths, both models can generate the next token in roughly equivalent time. For long context lengths, however, the hybrid model benefits from its many SSM layers and generates the content nearly $8 \\times$ faster than the Transformer. We expect additional inference-time benefits for the hybrid model due to a reduced key-value cache size that should enable Mamba-2-Hybrid to use larger batch sizes than possible with the Transformer model. ![](https://cdn.mathpix.com/cropped/2024_09_12_b92355d6bd48fca7f050g-11.jpg?height=405&width=614&top_left_y=958&top_left_x=1211)\n\nFigure 5: Predicted speedup to generate one token for an 8B-parameter Mamba-2Hybrid model compared to a Transformer. ### 4.3 Empirical Evaluation of Mamba-2-Hybrid\n\n### 4.3.1 Downstream Language Modeling Tasks\n\nWe evaluate the Mamba-2-Hybrid 8B model trained on the 3.5T token dataset using downstream natural language tasks in Table 7. We include comparisons with the pure SSM and Transformer models discussed in Section 3.3. Remarkably, Mamba-2-Hybrid achieves higher accuracy than the corresponding Transformer on all 12 common natural language tasks we evaluated (Table 7). The average improvement on these tasks compared to the Transformer model is 2.65 points. Figure 6 shows the behavior of Mamba-2-Hybrid, Mamba-2, and the corresponding Transformer for MMLU accuracy as training progresses. While Mamba-2-Hybrid always leads pure Mamba-2, early in training the hybrid is inferior in this regard to the Transformer. However, once the hybrid matches the Transformer's accuracy at 1.5 T tokens it quickly gains and maintains a strong advantage. We believe this motivates further study into the data efficiency and saturation behavior of Mamba-based models. ![](https://cdn.mathpix.com/cropped/2024_09_12_b92355d6bd48fca7f050g-11.jpg?height=408&width=614&top_left_y=1731&top_left_x=1211)\n\nFigure 6: Five-shot MMLU accuracy (standard formulation) for 8B-parameter models trained on 3.5 T tokens as a function of training completion. Table 7 and Figure 6 show that when training on sufficiently long token horizons (in our case 3.5T tokens), a hybrid model containing Mamba-2, self-attention, and MLP layers can exceed the accuracy\n\nTable 8: Standard task evaluation results of 16 K and 32 K sequence length variants of the 8 B -parameter Mamba-2-Hybrid and Transformer model trained on 3.5T tokens. These tasks do not require longcontext abilities but they show that the accuracy on these tasks does not degrade for the long-context models when compared to the base 4 K models. | Model | WG | PIQA | HellaSwag | ARC-E | ARC-C | MMLU | OpenBook | TruthFul | PubMed | RACE | NQ | SquadV2 | Avg |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  |  |  |  |  |  | 45.69 |  |  |  |  |  |  |  | 53.17 |\n| Mamba-2-Hybrid-4K | $\\mathbf{7 1 .",
    "mambalms-10": "2 7}$ | $\\mathbf{7 9 . 6 5}$ | $\\mathbf{7 7 . 6 8}$ | $\\mathbf{7 7 . 2 3}$ | $\\mathbf{4 7 . 7}$ | $\\mathbf{5 1 . 4 6}$ | $\\mathbf{5 3 . 6 0}$ | $\\mathbf{4 2 . 8 0}$ | $\\mathbf{3 8 . 7 2}$ | $\\mathbf{6 9 . 8 0}$ | $\\mathbf{3 9 . 7 1}$ | $\\mathbf{1 7 .",
    "mambalms-11": "3 4}$ | $\\mathbf{5 8 . 6 7}$ | $\\mathbf{5 5 . 8 2}$ |\n| Transformer-16K | 70.4 | 78.67 | 76.4 | 74.45 | 44.37 | 47.48 | 51.22 | 42.2 | 36.33 | 69.8 | 38.95 | 14.29 | 55.38 | 53.84 |\n| Mamba-2-Hybrid-16k | $\\mathbf{7 1 .",
    "mambalms-12": "6 7}$ | $\\mathbf{7 9 . 9 2}$ | $\\mathbf{7 8 . 2 4}$ | $\\mathbf{7 7 . 9 5}$ | $\\mathbf{4 8 . 1 2}$ | $\\mathbf{5 2 . 0 1}$ | $\\mathbf{5 4 . 9 2}$ | $\\mathbf{4 4 . 6 0}$ | $\\mathbf{3 7 . 2 3}$ | $\\mathbf{7 0 . 0 0}$ | $\\mathbf{3 9 . 3 3}$ | $\\mathbf{1 8 .",
    "mambalms-13": "5 0}$ | $\\mathbf{5 8 . 9 9}$ | $\\mathbf{5 6 . 2 7}$ |\n| Transformer-32K | 69.22 | 78.51 | 76.01 | 73.74 | 43.09 | 47.80 | 50.42 | 41.60 | 36.28 | 69.40 | 38.66 | 15.79 | 54.93 | 53.50 |\n| Mamba-2-Hybrid-32K | $\\mathbf{7 1 .",
    "mambalms-14": "4 3}$ | $\\mathbf{7 9 . 5 4}$ | $\\mathbf{7 8 . 0 8}$ | $\\mathbf{7 8 . 0 7}$ | $\\mathbf{4 7 . 7 0}$ | $\\mathbf{5 2 . 4 1}$ | $\\mathbf{5 5 . 0 9}$ | $\\mathbf{4 5 . 4 0}$ | $\\mathbf{3 7 . 8 6}$ | $\\mathbf{7 1 . 0 0}$ | $\\mathbf{4 0 . 1 0}$ | $\\mathbf{1 8 .",
    "mambalms-15": "6 4}$ | $\\mathbf{5 7 . 9 3}$ | $\\mathbf{5 6 . 4}$ |\n\nof a pure Mamba-2 and a pure Transformer model when averaged over a wide variety of downstream natural language tasks. These results provide exciting evidence for the capability of hybrid models to provide faster LLM inference and greater model quality when compared to Transformers. ### 4.3.2 Long-Context Evaluation\n\nIn this section, we evaluate the long-context ability of hybrid SSM-Transformer models by training two Mamba-2-Hybrid 8B extensions - a 16386 (16K) and 32768 (32K) variant-and compare to corresponding extensions of the 8 B Transformer. We extend the base models (pretrained using sequence lengths of 4096) to 16 K and 32 K versions through continued pretraining on the respective larger context lengths. We use full global attention in the four self-attention layers. In this initial study, we use the same underlying data as in our 3.5 T dataset. That is, we do not explicitly select a data subset consisting of long documents, but rather use packed sequences to generate 16 K and 32 K inputs for the model. All long context models are trained for an additional 50B tokens with a learning rate that increases linearly over the first 1.7B tokens and then decays according to cosine annealing thereafter. We use a max learning rate of $3 \\mathrm{e}-5$ and minimum learning rate of $3 \\mathrm{e}-6$. For the Transformer extensions, we automatically adapt the RoPE base frequency to the longer context lengths using the dynamic NTK scaling described in bloc97 (2023). Results on Standard Short-Context Tasks. We first evaluate the 16K and 32K Mamba-2-Hybrid and Transformer models on the 12 standard natural language tasks used above. While these tasks do not require long-context abilities, we aim to check whether model accuracy degrades on common tasks as a result of extending our models to long-context variants. Results are reported in Table 8. On average, we find no accuracy decrease on these tasks for the long-context variants. In fact, the 16 K and 32 K models slightly improve compared to the base models which is due to the 16 K and 32 K models seeing $1.4 \\%$ more data. As for the original 4 K evaluations, the 16 K and 32 K Mamba- 2 -Hybrid is more than 2 points better than the corresponding Transformer models on average. Results on Natural Long-Context Tasks. We now focus on evaluating the 16 K and 32 K model extensions on tasks which require natural language reasoning across long contexts. Results when evaluating these models on nine common long-context tasks are shown in Table 9. In this setting, the base (4K) Mamba-2-Hybrid and Transformer models achieve similar accuracy on most tasks and are more than 6 points better than the pure Mamba- 2 model. For both architectures, the 16 K and 32 K variants improve over the base models by an average of roughly 4 points. This is particularly due to a large accuracy increase on tasks with many long inputs (e.g., NarrativeQA). Comparing the 16 K and 32 K Mamba-2-Hybrid to the corresponding 16 K and 32 K Transformer, we observe that the Transformer models separate from the hybrid models on some tasks, particularly Multi-Document Question Answering tasks (e.g., HotpotQA). This leads the 16 K and 32 K Transformer models to reach approximately one point higher average accuracy than the 16 K and 32 K Mamba- 2 -Hybrid models respectively. We hypothesize that the hybrid model reaches lower accuracy than the Transformer on these tasks because the SSM layer states are sometimes confused by documents irrelevant to the question (which is unknown until the end of the sequence)-The Muti-Document Question Answering tasks in Table 9 are taken from the LongBench evaluation suite which generates long-context inputs by concatenating the\n\nTable 9: Natural long-context evaluation results for 8B-parameter base and long-context extensions of Mamba-2-Hybrid and Transformer models trained on 3.5T tokens. While the 4 K models are comparable on these tasks, the 16 K and 32 K Transformer is roughly 1 point better on average when compared to the 16 K and 32 K Mamba-2-Hybrid.",
    "mambalms-16": "These improvements come largely on Multi-Document Question Answering tasks from LongBench (Bai et al. 2023). | Model <br> (avg, max) Ctx. Len: | Single Doc. QA |  |  |  | Multi-Doc. QA |  |  | Few-Shot Learning |  | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | NarrativeQA | Qasper | MultiFieldQA | QuALITY | HotpotQA | 2WikiMQA | Musique | TREC | TriviaQA |  |\n|  | (86K, 506 K ) | (4.9K, 22 K$)$ | ( $7.2 \\mathrm{~K}, 17 \\mathrm{~K}$ ) | (6.5K, 9.8 K$)$ | (13.3K, 19K) | (7.5K, 17 K$)$ | $\\overline{(16.3 \\mathrm{~K}, 18 \\mathrm{~K})}$ | $\\overline{(7.1 \\mathrm{~K}, 11.9 \\mathrm{~K})}$ | $\\overline{(12.3 \\mathrm{~K}, 25 \\mathrm{~K})}$ |  |\n| Mamba-2 | 22.53 | 25.74 | 29.26 | 33.84 | 29.99 | 24.6 | 11.1 | 54 | 77.77 | 34.31 |\n| Transformer-4K | 23.44 | 28.51 | 38.39 | 36.48 | 36.28 | 33.48 | 17.68 | 68 | 83.65 | 40.66 |\n| Mamba-2-Hybrid-4k | 24.53 | 28.75 | 39.01 | 35.6 | 36.24 | 32.91 | 15.24 | 68.5 | 86.7 | 40.83 |\n| Transformer-16K | 27.51 | 29.71 | 41.13 | 39.02 | 48.61 | 34.87 | 21.42 | 78.5 | 86.39 | 45.24 |\n| Mamba-2-Hybrid-16k | 29.76 | 30.93 | 40.9 | 38.93 | 42.17 | 31.34 | 23.32 | 74 | 90.05 | 44.6 |\n| Transformer-32K | 30.06 | 29.09 | 40.61 | 39.17 | 48.2 | 35.52 | 24.55 | 76.5 | 86.43 | 45.57 |\n| Mamba-2-Hybrid-32K | 31.56 | 30.55 | 40.69 | 38.88 | 41.9 | 29.06 | 21.33 | 74.5 | 89 | 44.16 |\n\nfew documents from each task needed to answer the question (e.g., HotpotQA questions contain two paragraphs and then a question which requires knowledge from both) with many random paragraphs sampled from Wikipedia. This confusion could be due to our continued pretraining recipe which simply packs unrelated sequences together to make 16 K and 32 K inputs, potentially leading the SSM layers to believe separate documents are related when they are not. While this recipe is widely used for Transformers, it may not directly apply to hybrid models. Based on our experience evaluating these tasks, we also note that hybrid models may be more sensitive to prompt formatting than Transformer models. As evidence to support this hypothesis, we found that minor prompt modifications could change the results for both models, but more so for the hybrid model. For example, on Musique, prompt modifications led the accuracy for the Mamba-2-Hybrid-4K model to fall in the range $[10.63,16.16]$. In contrast, the accuracy for the Transformer was relatively steady, remaining in the range $[15.25,17.68]$. We highlight, however, that the prompt format for the majority of the tasks in Table 9 (e.g., the Multi-Document QA tasks, see Section 2) are taken from the LongBench evaluation suite (Bai et al. 2023) and have been optimized for Transformer models. As a result of these observations, we believe interesting areas of future work involve further study on the prompt robustness of hybrid models and comparing aligned and instruction-tuned hybrid models to their Transformer counterparts. Results on Synthetic Long-Context Tasks. Beyond the natural long-context tasks discussed above, we also evaluate the 16 K and 32 K hybrid and Transformer extensions on the synthetic tasks in the RULER (Hsieh et al. 2024) benchmark suite. These tasks expand upon the basic Needle In A Haystack (NIAH) problem where the model is asked to recall information (the needle) from long inputs of otherwise irrelevant text. RULER also includes tasks which require tracing and aggregating information across the context. For these evaluations, the task context lengths are set to 4 K for the base models, 16 K for the 16 K extensions, and 32 K for the 32 K models. Results on the 13 RULER tasks are shown in Table 10. Overall, the Mamba-2-Hybrid models show significantly improved NIAH abilities compared to the Transformer models and pure Mamba-2 model. For example, The 16K hybrid model achieves 13 points higher average accuracy on these tasks compared to the 16 K Transformer. The long-context Transformer models are particularly challenged by the Variable Tracking (VT) task. This task includes a one shot demonstration of the task in the context and a closer inspection of the model predictions shows that the Transformer tends to directly copy the answer for the in-context example instead of predicting the output of the actual question. This behavior is consistent with prior observations for LWM-7B and Yi-34B models on these tasks (Hsieh et al. 2024). Interestingly, while the hybrid model is generally better on most tasks, the Transformer consistently reaches higher accuracy on Keywords Extraction (KWE). We also observe that the hybrid model reaches higher accuracy than the Transformer on the HotpotQA task, which contrasts the behavior in Table 9 when running HotpotQA using the LongBench evaluation suite. As described above, while the latter benchmark constructs long context HotpotQA questions by adding random Wikipedia passages to the relevant information, RULER extends the context length of HotpotQA by adding paragraphs randomly sampled from HotpotQA itself. This slight difference\n\nTable 10: Evaluation results of 8B-parameter Mamba-2-Hybrid and Transformer models trained for 3.5T tokens, plus their long-context extensions, on the synthetic RULER long-context benchmark suite (Hsieh et al. 2024). Mamba-2-Hybrid models achieve higher accuracy than Transformers on these tasks, highlighting their improved ability to recall, trace, and aggregate information across long inputs. | Model | Synthetic Tasks |  |  |  |  | Needle-In-A-Haystack Tasks (NIAH) |  |  |  |  |  |  |  | Avg |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | HotpotQA | SquadQA | CWE | VT | KWE | NIAH-1 | NIAH-2 | NIAH-3 | MK-NIAH-1 | MK-NIAH-2 | MK-NIAH-3 | MV-NIAH | MQ-NIAH |  |\n| Mamba-2 | 31.75 | 35.5 | 32.87 | 76.45 | 76.58 | 100 | 98 | 83 | 40.25 | 13.75 | 5.5 | 35 | 49.12 | 52.14 |\n| Transformer-4K | 42.25 | 56.5 | 36.42 | 79.2 | 76.33 | 100 | 100 | 75.25 | 99.5 | 94 | 58 | 96.56 | 95.06 | 77.62 |\n| Mamba-2-Hybrid-4k | 48.75 | 56.5 | 32.2 | 90.55 | 65.58 | 100 | 100 | 95.75 | 89.5 | 95.5 | 96 | 97.94 | 97.62 | 81.99 |\n| Transformer-16K | 37.25 | 45 | 3 | 7.25 | 58.33 | 100 | 99.5 | 75.75 | 93.75 | 56.5 | 57.5 | 88.5 | 87.94 | 62.33 |\n| Mamba-2-Hybrid-16k | 44 | 48.75 | 12.88 | 83.2 | 46.83 | 100 | 100 | 81.5 | 92 | 92.25 | 83 | 89.81 | 90.19 | 74.19 |\n| Transformer-32K | 35 | 41.5 | 4.42 | 0.35 | 53.5 | 100 | 99 | 76.5 | 70.75 | 57.75 | 41.25 | 69.69 | 84.69 | 56.49 |\n| Mamba-2-Hybrid-32K | 38.5 | 41.75 | 8.4 | 79.9 | 36.5 | 100 | 100 | 96.75 | 84 | 76.5 | 81.5 | 84.31 | 80.94 | 69.93 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b92355d6bd48fca7f050g-14.jpg?height=403&width=1541&top_left_y=704&top_left_x=291)\n\nFigure 7: Phonebook evaluation results for 8B-parameter models trained on 3.5T tokens and their long-context extensions. We use the standard Phonebook formulation (Figure 2b). Mamba-2-Hybrid models can generalize beyond their pretraining sequence length and perform the Phonebook task on contexts longer than pure SSM or Transformer models. (a) Base 4K Mamba-2, Mamba-2-Hybrid, and Transformer model Phonebook evaluations. (b) 16K long-context extensions of the Mamba-2-Hybrid and Transformer model evaluated on Phonebook. (c) Phonebook evaluations for the Mamba-2-Hybrid and Transformer models extended to support 32 K sequence lengths. in the distribution used for context creation seems to confuse the hybrid model in one case (i.e., LongBench HotpotQA (Table 9), but not in the other (i.e., RULER HotpotQA (Table 10) and provides an interesting area of future study. Results on Copying Tasks: Phonebook. Finally, we evaluate the long-context hybrid and Transformer models on the synthetic Phonebook task (Section 3.3.3, Figure 2b). We use the standard formulation which tests a model's ability to perform in-context learning and copying from the context. Results for the base 4 K models trained on 3.5 T tokens are shown in Figure 7a. In this Figure, we also include the results for the pure Mamba-2 model trained on 3.5T tokens. As highlighted for pure Mamba models trained on 1.1 T tokens (Section 3.3.3), the Mamba-2 model is unable to accurately predict the required phone numbers for sequences $>1000$ tokens. In contrast, the Transformer and Mamba-2-Hybrid can do the Phonebook task with near perfect accuracy up to the pretraining context length (4K). In fact, the hybrid model can generalize slightly beyond this sequence length, achieving 100 percent accuracy on Phonebook up to 5.5 K tokens. Similar results hold for the long-context models (Figure 7b and Figure 7c). Both the 16 K and 32 K Mamba-2-Hybrid extensions can perform the Phonebook task perfectly beyond their trained context length. The long-context Transformer models, however, start to make mistakes as the phone book length approaches their trained context lengths. In Griffin (De et al. 2024), the authors make similar observations, finding that their Transformer baseline slowly degrades as it approaches the training context length and that hybrid architectures show near-perfect accuracy up to their attention window size. As with the RULER evaluations above, these experiments highlight again the strong ability for hybrid models to perform in-context learning and to retrieve information from a long context. A 128K Mamba-2-Hybrid Model. While we focused in this section on evaluating 16K and 32K Mamba-2-Hybrid long-context extensions and comparing them to corresponding Transformer models,\nwe now show that the hybrid architecture can extend to context lengths well beyond 32 K . We extend the base 4 K Mamba-2-Hybrid model to a sequence length of 128 K through continued pretraining as described above, using full global attention for the four self-attention layers. This training required only tensor and pipeline parallelism in Megatron-LM to prevent out-of-memory issues. We report results for this model on the Phonebook task in Figure 8. As for the 4K, 16K, and 32K Mamba-2-Hybrid models, the 128 K model is able to do this task perfectly up to and beyond the sequence length it was trained on. This experiment highlights the promising potential for extending hybrid models to long context lengths. Takeaway. We have presented a detailed evaluation of long-context 8B-parameter Mamba-2-Hybrid models and compared them with their Transformer counterparts. Overall, the hybrid models match or exceed the long-context capabilities of the Transformers in most tasks. This is particularly true for tasks like Phonebook and the Needle In A Haystack (NIAH) present in the synthetic RULER benchmark. We have identified, however, a few tasks where the hybrid models failed to reach Transformer-level accuracy (e.g., Multi-Document Question Answering in the LongBench evaluation suite). We encourage further research into these settings and into long-context versions of hybrid SSM-Transformer architectures. ## 5 Related Work\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_b92355d6bd48fca7f050g-15.jpg?height=394&width=614&top_left_y=562&top_left_x=1211)\n\nFigure 8: Phonebook accuracy (standard setting) for an 8 B Mamba-2-Hybrid trained on 3.5 T tokens and extended to 128 K sequence length through continued pretraining for 50B tokens. Recent work has also introduced Mamba-Attention hybrid models to improve accuracy and efficiency compared to pure Mamba and Transformers. Park et al. (2024) show the limitations of Mamba on in-context learning (ICL) tasks and propose a hybrid model to improve the ICL accuracy. Their experiments, however, are isolated to ICL tasks, and the model size is small (up to 77 M parameters). Jamba (Lieber et al. 2024) and Zamba (Glorioso et al. 2024) train Mamba-Attention hybrid models at 7B scale. Both show that their hybrid models significantly improve inference speed and GPU memory compared to other models including Llama (Touvron et al. 2023) and Mistral-7B (Jiang et al. 2023). Jamba improves the model accuracy and efficiency by adding Mixture-of-Experts (MoE), which increases the total model capacity (52B total parameters) but not its active parameters. They compare their hybrid architecture with pure Mamba and Transformer models on four standard and three long-context tasks, but only using 1.3B parameter models trained for 250 B tokens, or 7 B parameter models trained for 50B tokens. Zamba introduces a shared attention module and uses an annealing phase during training with high-quality datasets, which boosts the quality of their hybrid model. We focus on combining Mamba, attention, and MLP layers into hybrid models for direct comparison with Transformer baselines at larger scales ( $>$ 7B parameters and $>1 \\mathrm{~T}$ tokens). Other recent work introduces hybrid models that mix either linear RNNs or convolutions with attention. De et al. (2024) introduce a hybrid model that blends gated linear recurrences with local (sliding window) attention and show that the hybrid model can improve next token prediction latency with increasing context length. They train 1B parameter models on 8 K sequence lengths for long-context modeling. Arora et al. (2023) report that a simple convolution-attention hybrid model outperforms pure attention in multi-query associative recall problems while reducing total FLOPs. Several additional works add SSM layers to the Transformer architecture to increase accuracy: Saon, Gupta, and Cui (2023) use SSM layers together with Transformers to improve speech recognition quality. Pilault et al. (2024) combine an SSM and a block-wise Transformer at every layer. They show improved perplexity and generalization capabilities for longer sequences (up to 65 K ). Their model is scaled up to 1.3B parameters. We note that all the hybrid models mentioned above have manually designed architectures and place an MLP layer (if they use MLP) after each attention layer, similar to Transformers. Our study, however, finds that a specific hybrid architecture design or pattern is not required. Instead, the relative proportions of each type of hybrid component appears to be the key factor that determines the quality of the model. ## 6 Conclusion\n\nTo address the question of whether SSM models can match the accuracy of Transformers at larger training budgets, in this report we presented a direct experimental comparison between 8B-parameter Mamba, Mamba-2, Mamba-2-Hybrid, and Transformer models trained on up to 3.5T tokens. Our experiments showed that pure SSM models match or exceed the capabilities of their Transformer counterparts on most downstream tasks but are challenged by tasks that require context-based information retrieval (e.g., copying) and in-context learning. We also showed that hybrid SSM-Transformer models (Mamba2-Hybrid) reach higher accuracy than Transformers on all common benchmarks we evaluated. Further, these hybrid models continue to show strong capabilities compared to Transformers when extended to 16 K and 32 K contexts. Based on these results, we are encouraged by the potential for SSM-based models to deliver inference-time speedups without accuracy degradation compared to Transformer models. We look forward to future work focusing on how hybrid models can make use of the large ecosystem of frameworks, methods, and libraries currently tailored to the large-scale training and inference of Transformers. ## References\n\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. \"GPT-4 Technical Report\". In: arXiv preprint arXiv:2303.08774 (2023). [2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr\u00f3n, and Sumit Sanghai. \"GQA: Training Generalized Multi-Query Transformer Models from Multi-head Checkpoints\".",
    "mambalms-17": "In: arXiv preprint arXiv:2305.13245 (2023). [3] Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. \"Zoology: Measuring and Improving Recall in Efficient Language Models\".",
    "mambalms-18": "In: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".",
    "mambalms-19": "In: arXiv preprint arXiv:2308.14508 (2023). [7] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. \"PIQA: Reasoning about Physical Commonsense in Natural Language\". In: Proceedings of the AAAI Conference on Artificial Intelligence.",
    "mambalms-20": "Vol. 34. 05. 2020, pp. 7432-7439. [8] bloc97. \"NTK-aware Scaled RoPE allows LLaMA models to have Extended ( $8 \\mathrm{k}+$ ) Context Size Without any Fine-tuning and Minimal Perplexity Degradation\". In: (2023). URL: https: //www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_\\ scaled_rope_allows_ llama_models_to_have. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. \"Language Models are Few-shot Learners\". In: Advances in Neural Information Processing Systems 33 (2020), pp. 18771901. [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. \"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\". In: arXiv preprint arXiv:1803.05457 (2018). [11] Tri Dao and Albert Gu. \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\".",
    "mambalms-21": "In: International Conference on Machine Learning (ICML). 2024. [12] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. \"A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers\". In: Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021, pp. 4599-4610. [13] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. \"Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models\". In: arXiv preprint arXiv:2402.19427 (2024). [14] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A Framework for Few-shot Language Model Evaluation.",
    "mambalms-22": "Version v0.4.0. Dec. 2023. URL: https://zenodo.org/ records/10256836. [15] Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. \"Zamba: A Compact 7B SSM Hybrid Model\". In: arXiv preprint arXiv:2405.16712 (2024). [16] Albert Gu and Tri Dao. \"Mamba: Linear-time Sequence Modeling with Selective State Spaces\". In: arXiv preprint arXiv:2312.00752 (2023). [17] Albert Gu, Karan Goel, and Christopher Re. \"Efficiently Modeling Long Sequences with Structured State Spaces\". In: International Conference on Learning Representations. 2021. [18] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. \"Measuring Massive Multitask Language Understanding\".",
    "mambalms-23": "In: International Conference on Learning Representations. 2020. [19] Dan Hendrycks and Kevin Gimpel. \"Gaussian Error Linear Units (GELUs)\".",
    "mambalms-24": "In: arXiv preprint arXiv:1606.08415 (2016). [20] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. \"Constructing A Multihop QA Dataset for Comprehensive Evaluation of Reasoning Steps\". In: Proceedings of the 28th International Conference on Computational Linguistics.",
    "mambalms-25": "2020, pp. 6609-6625. [21] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. \"RULER: What's the Real Context Size of Your Long-Context Language Models?\" In: arXiv preprint arXiv:2404.06654 (2024). [22] Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. \"Repeat After Me: Transformers are Better than State Space Models at Copying\". In: arXiv preprint arXiv:2402.01032 (2024). [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. \"Mistral 7B\".",
    "mambalms-26": "In: arXiv preprint arXiv:2310.06825 (2023). [24] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. \"PubMedQA: A Dataset for Biomedical Research Question Answering\". In: arXiv preprint arXiv:1909.06146 (2019). [25] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. \"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\". In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2017, pp. 1601-1611. [26] Tom\u00e1\u0161 Ko\u010disk\u1ef3, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. \"The NarrativeQA Reading Comprehension Challenge\". In: Transactions of the Association for Computational Linguistics 6 (2018), pp. 317-328. [27] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. \"Reducing Activation Recomputation in Large Transformer Models\". In: arXiv preprint arXiv:2205.05198 (2022). [28] Taku Kudo and John Richardson. \"Sentencepiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing\".",
    "mambalms-27": "In: arXiv preprint arXiv:1808.06226 (2018). [29] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. \"RACE: Large-scale ReAding Comprehension Dataset From Examinations\".",
    "mambalms-28": "In: arXiv preprint arXiv:1704.04683 (2017). [30] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. \"Latent Retrieval for Weakly Supervised Open Domain Question Answering\". In: arXiv preprint arXiv:1906.00300 (2019). [31] Xin Li and Dan Roth. \"Learning Question Classifiers\". In: COLING 2002: The 19th International Conference on Computational Linguistics. 2002. [32] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. \"Jamba: A Hybrid Transformermamba Language Model\".",
    "mambalms-29": "In: arXiv preprint arXiv:2403.19887 (2024). [33] Stephanie Lin, Jacob Hilton, and Owain Evans. \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\".",
    "mambalms-30": "In: arXiv preprint arXiv:2109.07958 (2021). [34] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. \"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering\".",
    "mambalms-31": "In: arXiv preprint arXiv:1809.02789 (2018). [35] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. \"Efficient Large-scale Language Model Training on GPU Clusters using Megatron-LM\". In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021. [36] NVIDIA. NVIDIA H100 Tensor Core GPU. https://www.nvidia.com/en-us/data-center/ h100/. 2023. [37] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. \"Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks\". In: arXiv preprint arXiv:2402.04248 (2024). [38] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, et al. \"Nemotron-4 15B Technical Report\". In: arXiv preprint arXiv:2402.16819 (2024). [39] Jonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and Ross Goroshin. \"Block-state Transformers\". In: Advances in Neural Information Processing Systems 36 (2024). [40] Pranav Rajpurkar, Robin Jia, and Percy Liang. \"Know what you don't Know: Unanswerable Questions for SQuAD\". In: arXiv preprint arXiv:1806.03822 (2018). [41] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. \"WinoGrande: An Adversarial Winograd Schema Challenge at Scale\". In: Communications of the ACM 64.9 (2021), pp. 99-106. [42] George Saon, Ankit Gupta, and Xiaodong Cui. \"Diagonal State Space Augmented Transformers for Speech Recognition\". In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2023, pp. 1-5. [43] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. \"Scrolls: Standardized Comparison over Long Language Sequences\". In: arXiv preprint arXiv:2201.03533 (2022). [44] Noam Shazeer. \"GLU Variants Improve Transformer\". In: arXiv preprint arXiv:2002.05202 (2020). [45] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. \"Megatron-LM: Training Multi-billion Parameter Language Models using Model Parallelism\". In: arXiv preprint arXiv:1909.08053 (2019). [46] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. \"Roformer: Enhanced Transformer with Rotary Position Embedding\".",
    "mambalms-32": "In: Neurocomputing 568 (2024), p. 127063. [47] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. \"Efficient Transformers: A Survey\". In: ACM Computing Surveys 55.6 (2022), pp. 1-28. [48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. \"Llama 2: Open Foundation and Fine-tuned Chat Models\".",
    "mambalms-33": "In: arXiv preprint arXiv:2307.09288 (2023). [49] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. \"MuSiQue: Multihop Questions via Single-hop Question Composition\".",
    "mambalms-34": "In: Transactions of the Association for Computational Linguistics 10 (2022), pp. 539-554. [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. \"Attention is All You Need\".",
    "mambalms-35": "In: Advances in Neural Information Processing Systems 30 (2017). [51] Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. \"Effective Long-context Scaling of Foundation Models\". In: arXiv preprint arXiv:2309.16039 (2023). [52] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. \"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\".",
    "mambalms-36": "In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018, pp. 2369-2380. [53] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. \"HellaSwag: Can a Machine Really Finish your Sentence?\" In: arXiv preprint arXiv:1905.07830 (2019). [54] Biao Zhang and Rico Sennrich. \"Root Mean Square Layer Normalization\". In: Advances in Neural Information Processing Systems 32 (2019). ## A Hybrid Layer Allocation Algorithm\n\nAlthough we are able to specify, and experiment with, an arbitrary sequence of Mamba, self-attention, and MLP layers in our hybrid models, by default we use the allocation algorithm described in Algorithm 1. This algorithm first attempts to place any self-attention layers such that the intervening runs of contiguous Mamba layers are as equal in length as possible, while also beginning and ending the layer sequence with a run of Mamba layers. Then, any MLP layers are evenly distributed throughout the sequence while not replacing any self-attention layers. The MLP layers are biased away from the start of the sequence so that the layer sequence begins with a Mamba layer (if there are any Mamba layers) and ends with an MLP layer (if there are any MLP layers). ```\nAlgorithm 1 Hybrid Layer Allocation\n    Input: total_layers_count, target_attention_ratio, target_mlp_ratio\n    Output: layer_type_list\n    attention_layers_count \\(\\leftarrow\\) round(total_layers_count * target_attention_ratio)\n    mamba_layers_count \\(\\leftarrow\\) total_layers_count - attention_layers_count\n    mamba_sections_count \\(\\leftarrow\\) attention_layers_count +1\n    mamba_section_length \\(\\leftarrow\\) mamba_layers_count / mamba_sections_count\n    layer_type_list \\(\\leftarrow\\) array of Symbols.MAMBA of size total_layers_count\n    \\(\\mathrm{x} \\leftarrow\\) mamba_section_length\n    for 1 in 0 to total_layers_count - 1 do\n        if \\(x<0.5\\) then\n            layer_type_list \\([1] \\leftarrow\\) Symbols.ATTENTION\n            \\(\\mathrm{x} \\leftarrow \\overline{\\mathrm{x}}+\\) mamba_section_length\n        else\n            \\(\\mathrm{x} \\leftarrow \\mathrm{x}-1\\)\n    mlp_layers_count \\(\\leftarrow\\) round(total_layers_count * target_mlp_ratio)\n    if mlp_layers_count \\(>0\\) then\n        mamba_layers_count \\(\\leftarrow\\) mamba_layers_count - mlp_layers_count\n        mamba_to_mlp_ratio \\(\\leftarrow\\) mamba_layers_count / mlp_layers_count\n        \\(\\mathrm{x} \\leftarrow\\) mamba_to_mlp_ratio\n        for 1 in 0 to total_layers_count - 1 do\n            if layer_type__list[l] == Symbols.MAMBA then\n                if \\(\\mathrm{x}<0.5\\) then\n                    layer_type_list[1] \\(\\leftarrow\\) Symbols.MLP\n                    \\(\\mathrm{x} \\leftarrow \\mathrm{x}+\\) mamba_to_mlp_ratio\n            else\n                        \\(\\mathrm{x} \\leftarrow \\mathrm{x}-1\\)\n```\n\nTable 11 provides examples of some layer patterns generated by Algorithm 1. | TLC | ATT | MLP | Pattern |\n| :---: | :---: | :---: | :--- |\n| 24 | 0.00 | 0.00 | MMMMMMMMMMMMMMMMMMMMMM |\n| 24 | 0.08 | 0.00 | MMMMMMM $*$ MMMMMMMM $*$ MMMMMM |\n| 24 | 0.17 | 0.00 | MMMM $*$ MMMM $*$ MMMM $*$ MMMM $*$ MMMM |\n| 24 | 0.08 | 0.30 | $\\mathrm{MM}+\\mathrm{MM}+\\mathrm{M} * \\mathrm{M}+\\mathrm{MMM}+\\mathrm{MM} *+\\mathrm{MM}+\\mathrm{MM}+$ |\n| 24 | 0.08 | 0.50 | $\\mathrm{M}+\\mathrm{M}+\\mathrm{M}++* \\mathrm{M}+\\mathrm{M}+\\mathrm{M}+\\mathrm{M}+* \\mathrm{M}++\\mathrm{M}+\\mathrm{M}+$ |\n| 24 | 0.50 | 0.50 | $+*+*+*+*+*+* *+*+*+*+*+*+$ |\n| 48 | 0.08 | 0.50 | $\\mathrm{M}+\\mathrm{M}+\\mathrm{M}++\\mathrm{M}+* \\mathrm{M}+\\mathrm{M}+\\mathrm{M}+\\mathrm{M}++* \\mathrm{M}+\\mathrm{M}+\\mathrm{M}+\\mathrm{M}+* \\mathrm{M}++\\mathrm{M}+\\mathrm{M}+\\mathrm{M}+* \\mathrm{M}+\\mathrm{M}++\\mathrm{M}+\\mathrm{M}+$ |\n| 56 | 0.08 | 0.50 | $\\mathrm{M}+\\mathrm{M}+\\mathrm{M}++\\mathrm{M}+\\mathrm{M} *+\\mathrm{M}+\\mathrm{M}+\\mathrm{M}+\\mathrm{M}++\\mathrm{M} *+\\mathrm{M}+\\mathrm{M}+\\mathrm{M}+\\mathrm{M}+\\mathrm{M} *++\\mathrm{M}+\\mathrm{M}+\\mathrm{M}+\\mathrm{M}+\\mathrm{M} *+\\mathrm{M}++\\mathrm{M}+\\mathrm{M}+\\mathrm{M}+$ |\n\nTable 11: Some examples of hybrid layer patterns generated by Algorithm 1. TLC=total_layer_count, ATT $=$ target_attention_ratio, $\\mathrm{MLP}=$ target_mlp_ratio. In the pattern, $\\mathrm{M}=\\mathrm{Mamba},{ }^{*}=$ Self-Attention, and $+=$ MLP. [^0]:    *Work done as an intern at NVIDIA\n    ${ }^{\\dagger}$ Correspondence to bnorick@nvidia.com\n    ${ }^{1}$ A fixed snapshot of the code used in this technical report is available at\n    https://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba. "
}