{
    "scalelawrope-0": "Scaling Laws of RoPE-based Extrapolation\n\nXiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, Dahua Lin School of Computer Science, Fudan University Shanghai AI Lab\n\nAbstract\n\nThe extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding Su et al.",
    "scalelawrope-1": "(2021) is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose Scaling Laws of RoPE-based Extrapolation, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by critical dimension for extrapolation. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B (Touvron et al., 2023b). 1 Introduction\n\nLarge Language Models (LLMs) have become the dominant architecture in a variety of natural language processing tasks(OpenAI, 2023; Touvron et al., 2023a; b), while Transformers (Vaswani et al., 2017) based on Rotary Position Embedding (RoPE) (Su et al., 2021) have become the dominant backbone in wide range of LLM design (Chowdhery et al., 2022; Nijkamp et al., 2022; Touvron et al., 2023a; b). While RoPE can theoretically represent sequences through trigonometric functions, as detailed in Appendix A, its performance drops when the input sequence or context length surpasses the training length(Press et al., 2021; Chen et al., 2023), seen in Figure 1. This extrapolation problem (Press et al., 2021) limits tasks like long text modeling and summarization(An et al., 2023). Concerning the extrapolation issue with RoPE, different works have provided various interpretations and corresponding solving attempts. These works could divided into two schools of thought. One limits the scope of self-attention (Ratner et al., 2022; Han et al., 2023) given the fact that self-attention computations in RoPE fail to keep stable beyond training context and exhibit attention score explosion as well as monotonous entropy increase (Chen et al., 2023; Han et al., 2023). The other aims to capture longer contexts by using smaller rotation angles and longer fine-tuning context (Chen et al., 2023; Peng et al., 2023). Currently, popular methods, such as Dynamic NTK (LocalLLaMA, 2023a) and Code LLaMA (Rozi\u00e8re et al., 2023), mainly come from the second approach. Both approaches adapt RoPE to longer contexts with a larger rotary base. Specifically, Dynamic NTK (LocalLLaMA, 2023a) adjusts the base with a coefficient increasing with the length of inference, allowing RoPE-based LLMs to adapt simultaneously to longer context, while Code LLaMA (Rozi\u00e8re et al., 2023) directly sets the base at 1000000 and gets further trained on sequences with 16K tokens, yielding a context beyond 100K. While recent studies have shown promising results, they have primarily focused on specific base values and tuning context lengths. This leaves a notable gap in understanding how base value, tuning length, and extrapolation performance relate. For instance, while larger bases improve extrapolation in models like LLaMA2 (Touvron et al., 2023a), surprisingly, we also find that fine-tuning with smaller bases with the original training length is also conducive to the extrapolation capability of LLaMA2, which is also demonstrated in Figure 1. Furthermore, when trained in a longer context, RoPE with a smaller base can match or even surpass those with a larger one. At the same time, fine-tuning with a base of 1000000 on the original training length also achieves extrapolation up to 100K. These findings pose several questions. Q1: Is 10000 the worst base value for extrapolation in the fine-tuning phase? Q2: Is there a mathematical relationship between rotary base, training context length, and extrapolation limit? Q3: If so, can we achieve unbound extrapolation accordingly? In this paper, we conduct further experiments on increasing and decreasing the rotary base in Section 2 and subsequently discover that adjusting the rotary base in both directions can contribute to the extrapolation of RoPE-based LLMs. Building upon these observations, we provide a comprehensive explanation for the seemingly counter-intuitive phenomenon from a periodic perspective. Meanwhile, we establish a unified theoretical framework for RoPE-based extrapolation known as the Scaling Laws of RoPE-based Extrapolation111It is important to note that the scaling laws proposed in this work are irrelevant with the well-known scaling laws (Kaplan et al., 2020). In this paper, scale refers to the adjustment of RoPE\u2019s rotary angles (LocalLLaMA, 2023a), rather than the change of the model size. Unlike the scaling laws (Kaplan et al., 2020) that are empirically derived, our scaling laws define mathematical relations between context window size and rotary base, supported by experiments.. We pinpoint specific changes during base reduction that lead to a significant boost in extrapolation in Section 3.1 and identify the upper bound of extrapolation for larger bases in Section 3.3. This clarifies how Code LLaMA (Rozi\u00e8re et al., 2023) manages a 100K extrapolation with only 16K training context. Furthermore, we validate our theories in Section 3.4 and Appendix C, shedding light on both the core principles of Dynamic NTK (LocalLLaMA, 2023a) and pinpointing instability sources self-attention computations in RoPE-based extrapolation (Han et al., 2023). Finally, we present the contributions and guiding significance of this work for other methods that achieve extrapolation during the inference phase. In summary, our contribution can be summarized as follows:\n\n\u2022\n\nWe first highlight a surprisingly strange phenomenon 10000 is the worst base value for RoPE-based extrapolation in the fine-tuning phase. Remarkably, fine-tuning with either a larger or smaller base within the training context length greatly enhances extrapolation, which provides a new vision to the extrapolation research of RoPE (Su et al., 2021). \u2022\n\nThen we introduce a unified theoretical framework for RoPE-based extrapolation from a periodic perspective, known as the Scaling Laws of RoPE-based Extrapolation, which not only clarifies the aforementioned observations and addresses unanswered questions in existing research (Rozi\u00e8re et al., 2023), but also discover the Critical Dimension for RoPE-based Extrapolation, revealing the underlying reasons for the extrapolation issue of RoPE. \u2022\n\nFinally, for extrapolation within a defined context, we present the suggested fine-tuning base value determined by the context limit and extend the context of LLaMA2 7B and 13B (Touvron et al., 2023b) to surpass 100K tokens by tuning RoPE with base 1000000 and 4K tuning length. For unpredictable extrapolation, we propose a RoPE with a smaller base, such as 500, and achieve an almost 1M token context with a mere 16K tuning length. 2 Observation\n\n2.1 Larger Bases Promise Better Extrapolation\n\nWe first conduct the extrapolation experiments with larger bases, based on the experimental setup in Appendix B.1. It is evident that tuning with larger bases could significantly improve the extrapolation performance of RoPE as shown in Figure 2. Besides, there are several noteworthy points. First, Larger bases allow LLaMA2 (Touvron et al., 2023b) to extrapolate beyond its training context length, aligning with findings from Rozi\u00e8re et al. (2023). Secondly, the extrapolation with larger bases has a clear limit where language modeling perplexity stays consistent. Beyond this limit, the extrapolation performance declines significantly. Furthermore, as the base value rises, LLaMA2 can extrapolate to a longer context. Finally, compared to Dynamic NTK (LocalLLaMA, 2023a), RoPE tuned with larger bases degrades much quicker beyond its extrapolation upper bound. Therefore, for fine-tuning with larger bases, the performance beyond the upper bound could be consistently overtaken by Dynamic NTK. Nevertheless, within the upper bound, this approach still outperforms Dynamic NTK by a considerable margin, leading to a context beyond 100K with only a 4K tuning length, when the base is set over 600000. 2.2 Smaller Bases Also Promise Better Extrapolation\n\nWe then conduct the extrapolation experiments with smaller bases, using the same setup as for larger bases. Interestingly, even though this goes against common research findings (LocalLLaMA, 2023a; Rozi\u00e8re et al., 2023), fine-tuning RoPE with smaller bases on the original context length still boosts extrapolation, as shown in Figure 3. It also extends the context window beyond the training length. Yet, there are distinct differences when comparing RoPE with smaller bases to larger ones. Firstly, RoPE with smaller bases does not have a distinct upper bound of extrapolation. While perplexity worsens as context length grows, this decline is gentler with smaller bases. Secondly, the enhancement in extrapolation of RoPE with smaller bases is not uniform. Between a base of 10000 and 8000, the extrapolation performance exhibits a tiny improvement. Then between a base of 8000 and 2608, improvement is moderate. After that, from 2608 to 1304 and further to 652, the improvement becomes more pronounced. Finally, when the rotary base is 500, the extrapolation curve becomes sufficiently smooth thus resulting in strong extrapolation over 48K context length and superior performance over Dynamic NTK (LocalLLaMA, 2023a). Combining these two results, we observe a surprisingly strange phenomenon in RoPE-based extrapolation as depicted in Figure 4. Specifically, base 10000 yields the worst extrapolation performance when fine-tuned, thus answering Q1 in the Introduction. As the base either decreases or increases, performance notably improves. Interestingly, the improvements differ between the two directions. For larger bases, although the performance steadily improves, there exists a clear extrapolation upper bound. In contrast, for smaller bases, while the improvement is not uniform, the resulting extrapolation curve does not have an obvious breaking point. In the heatmap in Figure 4, there is a clear and continuous boundary for larger bases as well as a distinct transition phase for smaller bases. 3 Explanation\n\n3.1 Scaling Law for Smaller Bases\n\nTo understand how RoPE with smaller bases achieves impressive extrapolation within a constrained tuning context, it is crucial to explore the impact of reducing the base. As outlined in Appendix A, a smaller base amplifies the rotary angle . This shortens , the periods of or that RoPE uses to represent relative positions. Figure 5 shows the cosine waves for different dimensions to represent position information. In Figure 5(a), it is evident that for smaller bases like 500, any period of is confined to 4096, i.e., the training length of LLaMA2 (Touvron et al., 2023b). In contrast, larger bases like 10000, extend the periods for several dimensions beyond the training length, as detailed in Section 3.2. Hence, smaller bases lead to a broader range of or inputs during pre-training or fine-tuning, which ensures every dimension of and gets a well-trained representation. Additionally, as the base decreases, three pivotal points emerge, , , and . Only when the or inputs in every dimension span from 0 to during training does the RoPE-based LLM recognize the negative values of and the non-monotonicity nature of . Similarly, the LLM becomes aware of the non-monotonicity pf and negative values once inputs hit . The RoPE-based LLM fully grasps the entire range of and only when the inputs surpass , potentially embracing the periodicity of position embedding in every dimension.",
    "scalelawrope-2": "Then we propose Theorem 1.",
    "scalelawrope-3": "as follows. Theorem 1. (Scaling Law of Smaller Bases)\n\nFor RoPE-based LLMs pre-trained with context length , if we adjust the base to and conduct fine-tuning still with context length , the extrapolation performance of RoPE-based LLMs will get improved. If is reduced to as calculated below, the or inputs in every dimension will span from 0 to respectively, resulting in a more significant improvement. \u03b2 1 = 2 \u200b T train \u03c0 \u200b , \u200b \u03b2 2 = T train \u03c0 \u200b , \u200b \u03b2 3 = T train 2 \u200b \u03c0 subscript \ud835\udefd 1 2 subscript \ud835\udc47 train \ud835\udf0b , subscript \ud835\udefd 2 subscript \ud835\udc47 train \ud835\udf0b , subscript \ud835\udefd 3 subscript \ud835\udc47 train 2 \ud835\udf0b \\beta_{1}=\\frac{2T_{\\text{train}}}{\\pi}\\text{, \\quad}\\beta_{2}=\\frac{T_{\\text{train}}}{\\pi}\\text{, \\quad}\\beta_{3}=\\frac{T_{\\text{train}}}{2\\pi} (1)\n\nParticularly, for LLaMA2(Touvron et al., 2023b), where the context length is 4096, we have , , . It is worth noting that these three bases align with the pivotal points where improvement speeds up, as previously discussed during base reduction. Theorem 1 tells the improving paradigm of base reduction. Since it does not set an explicit extrapolation upper bound, RoPE with much smaller bases can potentially realize extrapolation to infinite context. 3.2 Critical Dimension for Extrapolation\n\nIn comparison to much smaller bases like 500, where each period of fits within the training context, the default base in RoPE (Su et al., 2021), which is 10000, causes periods of certain dimensions to extend beyond the training context, as visualized in Figure 5(b). Therefore, for RoPE-based LLMs, there exists a specific feature dimension, . For dimensions before , the periods of corresponding remain shorter than , while for those after , the periods stretch beyond . In other words, essentially, is the number of dimensions where and can cycle through their values within one period during pre-training or fine-tuning. Consequently, for dimensions beyond , when RoPE-based LLMs extrapolate beyond , the absolute position information of newly added tokens and the relative positional information in relation to previous tokens become out-of-distribution (OOD). This misalignment means the attention scores related to these dimensions, as illustrated in Equation 2, deviate from their expected distribution, causing a noticeable out-of-distribution in overall attention scores, thus leading to the extrapolation issue. We refer to this key dimension as the Critical Dimension for RoPE-based extrapolation, which is formally defined and calculated as shown in Lemma 1. \ud835\udc68 t , s = Re \u200b [ \u2211 n = 0 d / 2 \u2212 1 q ~ t ( n ) \u200b k ~ s ( n ) \u200b e i \u200b ( t \u2212 s ) \u200b \u03b8 n \u2217 \u23df full attention scores in RoPE ] = Re \u200b [ \u2211 n = 0 d extra / 2 \u2212 1 q ~ t ( n ) \u200b k ~ s ( n ) \u200b e i \u200b ( t \u2212 s ) \u200b \u03b8 n \u2217 \u23df reliable part for extrapolation + \u2211 n = d extra / 2 d / 2 \u2212 1 q ~ t ( n ) \u200b k ~ s ( n ) \u200b e i \u200b ( t \u2212 s ) \u200b \u03b8 n \u2217 \u23df OOD part for extrapolation ] \u200b . subscript \ud835\udc68 \ud835\udc61 \ud835\udc60 absent Re delimited-[] subscript \u23df superscript subscript \ud835\udc5b 0 \ud835\udc51 2 1 superscript subscript ~ \ud835\udc5e \ud835\udc61 \ud835\udc5b superscript subscript ~ \ud835\udc58 \ud835\udc60 \ud835\udc5b superscript superscript \ud835\udc52 \ud835\udc56 \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 \ud835\udc5b full attention scores in RoPE missing-subexpression absent Re delimited-[] subscript \u23df superscript subscript \ud835\udc5b 0 subscript \ud835\udc51 extra 2 1 superscript subscript ~ \ud835\udc5e \ud835\udc61 \ud835\udc5b superscript subscript ~ \ud835\udc58 \ud835\udc60 \ud835\udc5b superscript superscript \ud835\udc52 \ud835\udc56 \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 \ud835\udc5b reliable part for extrapolation subscript \u23df superscript subscript \ud835\udc5b subscript \ud835\udc51 extra 2 \ud835\udc51 2 1 superscript subscript ~ \ud835\udc5e \ud835\udc61 \ud835\udc5b superscript subscript ~ \ud835\udc58 \ud835\udc60 \ud835\udc5b superscript superscript \ud835\udc52 \ud835\udc56 \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 \ud835\udc5b OOD part for extrapolation . \\begin{aligned} \\bm{A}_{t,s}&=\\mathrm{Re}\\left[\\underbrace{\\color[rgb]{.75,0,.25}\\definecolor[named]{pgfstrokecolor}{rgb}{.75,0,.25}\\sum_{n=0}^{d/2-1}\\tilde{q}_{t}^{(n)}\\tilde{k}_{s}^{(n)}{}^{*}e^{i(t-s)\\theta_{n}}}_{\\text{full attention scores in RoPE}}\\right]\\\\\n&=\\mathrm{Re}\\left[{\\underbrace{\\color[rgb]{0,0,1}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\\sum_{n=0}^{d_{\\text{extra}}/2-1}\\tilde{q}_{t}^{(n)}\\tilde{k}_{s}^{(n)}{}^{*}e^{i(t-s)\\theta_{n}}}_{\\text{reliable part for extrapolation}}+\\underbrace{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\sum_{n=d_{\\text{extra}}/2}^{d/2-1}\\tilde{q}_{t}^{(n)}\\tilde{k}_{s}^{(n)}{}^{*}e^{i(t-s)\\theta_{n}}}_{\\text{OOD part for extrapolation}}}\\right]\\end{aligned}\\text{.} (2)\n\nLemma 1. (Definition of Critical Dimension)\n\nFor RoPE-based LLMs pre-trained with context length , assuming that the size of self-attention head is , there are at most the preceding dimensions that perceive complete periodic information thus receiving sufficient training for extrapolation, which is formally described as follows:\n\nT n = 2 \u200b \u03c0 \u03b8 n = 2 \u200b \u03c0 \u22c5 10000 2 \u200b n d \u2264 T train \u200b , subscript \ud835\udc47 \ud835\udc5b 2 \ud835\udf0b subscript \ud835\udf03 \ud835\udc5b \u22c5 2 \ud835\udf0b superscript 10000 2 \ud835\udc5b \ud835\udc51 subscript \ud835\udc47 train , \\displaystyle T_{n}=\\frac{2\\pi}{\\theta_{n}}=2\\pi\\cdot{10000}^{\\frac{2n}{d}}\\leq T_{\\text{train}}\\text{,} for \u200b n = 0 , \u22ef , d extra / 2 \u2212 1 \u200b , for \ud835\udc5b 0 \u22ef subscript \ud835\udc51 extra 2 1 , \\displaystyle\\text{\\quad for\\ }n=0,\\cdots,d_{\\text{extra}}/2-1\\text{,} (3) T n = 2 \u200b \u03c0 \u03b8 n = 2 \u200b \u03c0 \u22c5 10000 2 \u200b n d > T train \u200b , subscript \ud835\udc47 \ud835\udc5b 2 \ud835\udf0b subscript \ud835\udf03 \ud835\udc5b \u22c5 2 \ud835\udf0b superscript 10000 2 \ud835\udc5b \ud835\udc51 subscript \ud835\udc47 train , \\displaystyle T_{n}=\\frac{2\\pi}{\\theta_{n}}=2\\pi\\cdot{10000}^{\\frac{2n}{d}}>T_{\\text{train}}\\text{,} for \u200b n = d extra / 2 , \u22ef , d / 2 \u2212 1 \u200b . for \ud835\udc5b subscript \ud835\udc51 extra 2 \u22ef \ud835\udc51 2 1 . \\displaystyle\\text{\\quad for\\ }n=d_{\\text{extra}}/2,\\cdots,d/2-1\\text{.}\n\nThen we define as the critical dimension for RoPE-based extrapolation and calculate it given\n\nd extra = 2 \u200b \u2308 d 2 \u200b log 10000 \u2061 T train 2 \u200b \u03c0 \u2309 \u200b . subscript \ud835\udc51 extra 2 \ud835\udc51 2 subscript 10000 subscript \ud835\udc47 train 2 \ud835\udf0b . d_{\\text{extra}}=2\\left\\lceil{\\dfrac{d}{2}}\\log_{10000}{\\dfrac{T_{\\text{train}}}{2\\pi}}\\right\\rceil\\text{.} (4)\n\nFor LLaMA2(Touvron et al., 2023b), the critical dimension is 92. This implies that only the first 92 dimensions of the vectors of LLaMA2 have seen the complete positional information during the pre-training phase and are adequately trained. In other words, the last 36 dimensions lack sufficient training, contributing to the extrapolation challenges seen in RoPE-based LLMs (Chen et al., 2023; Han et al., 2023). The critical dimension plays a key role in enhancing extrapolation. A further discussion of the critical dimension is presented in Section 3.4. Here, we examine the attention score changes in the initial 92 versus the final 36 dimensions in relation to relative positions when the base is reduced. As is shown in Figure 6(a), the attention scores of RoPE with smaller bases have effectively captured the oscillations from sin and cos within training, mitigating OOD concerns during extrapolation. Moreover, as the base becomes smaller, the perception becomes more comprehensive, resulting in improved extrapolation performance. 3.3 Scaling Law for Larger Bases\n\nBased on the concept of the critical dimension, we can clarify the extrapolation results when fine-tuning RoPE with larger bases at the original context length. For LLaMA2 (Touvron et al., 2023b), since the periods of the first 92 dimensions fit within the training length, these feature dimensions start with a strong foundation for fine-tuning, adjusting to the new periodic shifts of positional embedding for extended contexts. Therefore, when RoPE is fine-tuned with a larger base value like 1000000, even though the tuning length is shorter than the extended periods corresponding to larger bases, these dimensions can still represent positional information correctly as is shown in Figure 5(c). However, for the last 36 dimensions, the absence of a full understanding of periodicity leads to over-fitting. Furthermore, when the base is expanded, extending the period, these dimensions still fail to capture the entire positional information within the context length. So these dimensions are reliable only when the value of is previously observed. Therefore, we can use the updated period of the critical dimension as an upper bound for extrapolation in RoPE-based LLM. As a result, we obtain Theorem 2., the scaling law for RoPE-based extrapolation with larger bases, which tells the relation between base value and extrapolation upper bound and thus answers Q2 in the Introduction. Theorem 2. (Scaling Law of Larger Bases)\n\nFor RoPE-based LLMs pre-trained with context length , if we adjust the base to and conduct fine-tuning still with context length , the extrapolation performance of RoPE-based LLMs will get improved. The extrapolation upper bound of RoPE-based LLM with larger bases, , is calculated as follows:\n\nT extra = 2 \u200b \u03c0 \u22c5 \u03b2 d extra \u22c5 1 d = 2 \u200b \u03c0 \u22c5 \u03b2 \u2308 d 2 \u200b log 10000 \u2061 T train 2 \u200b \u03c0 \u2309 \u22c5 2 d \u200b . subscript \ud835\udc47 extra \u22c5 2 \ud835\udf0b superscript \ud835\udefd \u22c5 subscript \ud835\udc51 extra 1 \ud835\udc51 \u22c5 2 \ud835\udf0b superscript \ud835\udefd \u22c5 \ud835\udc51 2 subscript 10000 subscript \ud835\udc47 train 2 \ud835\udf0b 2 \ud835\udc51 . T_{\\text{extra}}=2\\pi\\cdot\\beta^{d_{\\text{extra}}\\cdot\\frac{1}{d}}=2\\pi\\cdot\\beta^{\\left\\lceil{\\frac{d}{2}}\\log_{10000}{\\frac{T_{\\text{train}}}{2\\pi}}\\right\\rceil\\cdot{\\frac{2}{d}}}\\text{.} (5)\n\nInversely, if there is an expected extrapolation upper bound , then the smallest capable base is calculated as follows. \u03b2 0 = 10000 log T train 2 \u200b \u03c0 \u2061 T ~ extra 2 \u200b \u03c0 \u200b . subscript \ud835\udefd 0 superscript 10000 subscript subscript \ud835\udc47 train 2 \ud835\udf0b subscript ~ \ud835\udc47 extra 2 \ud835\udf0b . \\beta_{0}={10000}^{\\log_{\\frac{T_{\\text{train}}}{2\\pi}}{\\frac{\\tilde{T}_{\\text{extra}}}{2\\pi}}}\\text{.} (6)\n\nFor extrapolation within a limited context, we can derive the suggested based on the expected context length. in Equation 6 is referred to as critical base for extrapolation and discussed in detail in Appendix C. To support the claims of Theorem 2, as illustrated in Figure 6(b), we examined the attention scores of the first 92 and final 36 dimensions under various larger bases. Notably, while the attention scores of the first 92 dimensions remain relatively stable regardless of relative positions, the last 36 dimensions show significant variations. Reviewing the perplexity increase curves, it is evident that once the context length surpasses the extrapolation upper bound, the last 36 dimensions encounter unfamiliar positional information, leading to OOD attention scores and a sharp rise in perplexity. To further validate Theorem 2, we compare the max supported context lengths for different bases with the extrapolation upper bound derived from Theorem 2 in Figure 7. Impressively, there is a remarkable alignment between empirical results and theoretical predictions. 3.4 Further Validation for Extrapolation\n\nTo further illustrate the causal relationship between critical dimension and extrapolation, we undertake the following three experiments. First, we set a max index of 4096 for the position embedding of the last 36 dimensions of in LLaMA2 7B (Touvron et al., 2023b). We find that apart from an outlier in one attention head, the attention score variations are considerably reduced, leading to better extrapolation. Second, we visualize the attention score of the first 92 and the last 36 dimensions during extrapolation with Dynamic NTK (LocalLLaMA, 2023a) in Figure 8(a). Consistent with our theory, the attention scores of the last 36 dimensions display enhanced consistency compared with straightforward extrapolation. Lastly, we remove the final 36 dimensions of in LLaMA2 7B (Touvron et al., 2023b) and fine-tuned using the setup in Appendix B.1. Remarkably, post-trimming fine-tuning substantially surpassed direct fine-tuning, allowing extrapolation beyond 20K tokens, as illustrated in Figure 8(b). This offers compelling evidence for the correlation between the critical dimension, attention score shifts, and extrapolation upper bound. It confirms that interpreting and enhancing RoPE-based LLM extrapolation from a periodic viewpoint is both valid and effective. In summary, we establish a comprehensive framework from a periodic perspective and identify the fundamental factors, the critical dimension for RoPE-based extrapolation. This not only unveils the root causes of attention score explosion highlighted in Chen et al. (2023) and Han et al. (2023), but also intuitively demonstrates how adjusting the base during fine-tuning can elevate the extrapolation capabilities of RoPE-based LLMs. Furthermore, we answer Q3 in the Introduction and introduce the Extended Scaling Law of RoPE-based Extrapolation in Appendix C, the combination of Theorem 1., Lemma 1. and Theorem 2., for tuning in a longer context. Additionally, we also present the instructive value of our theory for other works focused on achieving longer context in the testing phase in Appendix D. Finally, as depicted in Figure 1, tuning RoPE with a base of 500 or 1000000 can both outperform Linear PI (Chen et al., 2023) and NTK method (LocalLLaMA, 2023b; a). Besides, we compare the performance of LLaMA2 7B Touvron et al. (2023b) with base 500 and 1000000 in 1M context length in Table 1 and find that for extrapolating to unpredictable length, RoPE with base 500, has remarkable advantages, especially when combined with log-scaled attention (Su, 2023b). 4 Related Work\n\nRecently, there have been a great deal of efforts devoted to expanding the context length of RoPE-based LLMs to 16K, 32K, or even 100K (Chen et al., 2023; LocalLLaMA, 2023b; Pal et al., 2023). Considering that LLMs, such as LLaMA2 (Touvron et al., 2023b), have already acquired sufficient knowledge in the pre-training stage and demonstrated excellence in short-context tasks, the emerging extrapolation improvements have primarily focused on the fine-tuning and the testing phase. Extrapolating RoPE in Fine-tuning\n\nIn the fine-tuning phase, current strategies primarily concentrate on using smaller rotary angles to depict long context features. For example, Chen et al. (2023) extends the context window to 16K by linearly reducing the rotary angles with to align the input position index within the original context size. Rozi\u00e8re et al. (2023) the achieve 100K context window for code by increasing the rotary base to 1000000 and further training with 16K context length. Besides, Pal et al. (2023) also extends the context window to 32K by a new truncation strategy for refining the down-sampling method, which even sets some rotary angles to zero. Additionally, Peng et al. (2023) proposes a scaling function, fine-tunes LLaMA2 with 64K context, and finally achieves the 128K context length. Effective as they are, these methods all need additional tuning in longer contexts or face a disastrous collapse after the extrapolation bound. Extrapolating RoPE in Inference\n\nIn the inference phase, context window division or restriction is a well-discussed strategy for any LLM without further training. Effective as it is, it prohibits tokens of their deserved global attention (Han et al., 2023) or destroys the order between token chunks (Ratner et al., 2022; Su, 2023a), which is unsupported for long document summarization (Su, 2023a). Besides, it is worth noting the remarkable effect of the Neural Tangents Kernel (NTK) method (LocalLLaMA, 2023b), especially the dynamic version (LocalLLaMA, 2023a) as follows:\n\nfor \u200b \ud835\udc68 t , s \u200b , \u200b \u03b8 n = ( 10000 \u22c5 \u03b1 t ) \u2212 2 \u200b n / d \u200b , where \u200b \u03b1 t = max \u2061 ( 1 , 2 \u2308 log 2 \u2061 t T train \u2309 + 1 \u2212 1 ) \u200b . for subscript \ud835\udc68 \ud835\udc61 \ud835\udc60 , subscript \ud835\udf03 \ud835\udc5b superscript \u22c5 10000 subscript \ud835\udefc \ud835\udc61 2 \ud835\udc5b \ud835\udc51 , where subscript \ud835\udefc \ud835\udc61 1 superscript 2 subscript 2 \ud835\udc61 subscript \ud835\udc47 train 1 1 . \\text{for }\\bm{A}_{t,s}\\text{, }\\theta_{n}={\\left(10000\\cdot\\alpha_{t}\\right)}^{-2n/d}\\text{, where }\\alpha_{t}=\\max\\left(1,2^{\\left\\lceil\\log_{2}{\\frac{t}{T_{\\text{train}}}}\\right\\rceil+1}-1\\right)\\text{.} (7)\n\nDynamic NTK 222In this work, we follow the form of scaling function in QWenLM (Alibaba, 2023) simply decreases the rotary angle exponentially with scaling coefficient as a function w.r.t. the inference length, which is discussed in detail in Appendix D. Dynamic NTK enables LLaMA2(Touvron et al., 2023b) to extrapolate without any further tuning or window assisting and keeps good performance within 16K context. However, Dynamic NTK still presents an obvious degradation in performance around 32K context size as shown in Figure 1. 5 Conclusion\n\nIn summary, we initially highlight an intriguing observation: fine-tuning RoPE (Su et al., 2021) with either a larger or smaller base using the original pre-training context can boost the length extrapolation of RoPE-based LLMs. We then elucidate this observation through a unified theoretical lens rooted in a periodic view, Scaling Laws of RoPE-based Extrapolation. This framework clarifies the score of the RoPE-based extrapolation challenge and offers insights into how base modifications in fine-tuning or inference can enhance RoPE-based extrapolation. Finally, we provide the strategy for both extrapolating LLaMA2 (Touvron et al., 2023b) to a limited context and unpredictable inputs. References\n\nAlibaba (2023) Alibaba. Qwen technical report. Technical report, 2023. URL https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf.",
    "scalelawrope-4": "An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models.",
    "scalelawrope-5": "arXiv preprint arXiv:2307.11088, 2023. Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation.",
    "scalelawrope-6": "arXiv preprint arXiv:2306.15595, 2023. Chiang & Cholak (2022) David Chiang and Peter Cholak. Overcoming a theoretical limitation of self-attention. arXiv preprint arXiv:2202.12172, 2022. Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.",
    "scalelawrope-7": "arXiv preprint arXiv:2204.02311, 2022. Dao (2023) Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning.",
    "scalelawrope-8": "arXiv preprint arXiv:2307.08691, 2023. Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.",
    "scalelawrope-9": "arXiv preprint arXiv:2101.00027, 2020. Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models.",
    "scalelawrope-10": "arXiv preprint arXiv:2308.16137, 2023. Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.",
    "scalelawrope-11": "arXiv preprint arXiv:1705.03551, 2017. Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019. LocalLLaMA (2023a) LocalLLaMA. Dynamically scaled rope further increases performance of long context llama with zero fine-tuning, July 2023a. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. LocalLLaMA (2023b) LocalLLaMA. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., June 2023b. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/. Loshchilov & Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis.",
    "scalelawrope-12": "arXiv preprint arXiv:2203.13474, 2022. OpenAI (2023) OpenAI. Gpt-4 technical report. Technical report, 2023. Pal et al. (2023) Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. Giraffe: Adventures in expanding context lengths in llms.",
    "scalelawrope-13": "arXiv preprint arXiv:2308.10882, 2023. Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models.",
    "scalelawrope-14": "arXiv preprint arXiv:2309.00071, 2023. Press et al. (2021) Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation.",
    "scalelawrope-15": "arXiv preprint arXiv:2108.12409, 2021. Presser (2020) Shawn Presser. Books3, 2020. URL https://twitter.com/theshawwn/status/1320282149329784833. Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1\u201316. IEEE, 2020. Ratner et al. (2022) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve in-context learning of large language models.",
    "scalelawrope-16": "arXiv preprint arXiv:2212.10947, 2022. Rozi\u00e8re et al. (2023) Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code.",
    "scalelawrope-17": "arXiv preprint arXiv:2308.12950, 2023. Su (2023a) Jianlin Su. Nbce: Naive bayes-based context extension, May 2023a. Su (2023b) Jianlin Su. Improving transformer: Length extrapolation ability and position robustness. https://spaces.ac.cn/archives/9444, 2023b.",
    "scalelawrope-18": "Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.",
    "scalelawrope-19": "arXiv preprint arXiv:2104.09864, 2021. Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022. Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.",
    "scalelawrope-20": "arXiv preprint arXiv:2307.09288, 2023b. Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang et al. (2019) Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019. Appendix A Preliminary\n\nA.1 RoPE from Sequence Domain\n\nTransformer models require the integration of explicit positional information through positional embeddings to effectively discern the order of input sequences (Vaswani et al., 2017). In this work, we direct our attention to the specific instance of positional encoding known as Rotary Position Embedding (RoPE) (Su et al., 2021), as prominently featured in the architecture of the LLaMA model (Touvron et al., 2023a; b). Given a query vector at position and a key vector at position , RoPE first splits into pairs on the direction of feature dimensions, with every two dimensions forming a complex number, or a vector in the complex plane as follows:\n\n\ud835\udc92 ~ t = [ q ~ t ( 0 ) , \u22ef , q ~ t ( d / 2 \u2212 1 ) ] q ~ t ( n ) = q t ( 2 \u200b n ) + i \u200b q t ( 2 \u200b n + 1 ) \ud835\udc8c ~ s = [ k ~ s ( 0 ) , \u22ef , k ~ s ( d / 2 \u2212 1 ) ] k ~ s ( n ) = k s ( 2 \u200b n ) + i \u200b k s ( 2 \u200b n + 1 ) \u200b . formulae-sequence formulae-sequence subscript ~ \ud835\udc92 \ud835\udc61 superscript subscript ~ \ud835\udc5e \ud835\udc61 0 \u22ef superscript subscript ~ \ud835\udc5e \ud835\udc61 \ud835\udc51 2 1 superscript subscript ~ \ud835\udc5e \ud835\udc61 \ud835\udc5b superscript subscript \ud835\udc5e \ud835\udc61 2 \ud835\udc5b \ud835\udc56 superscript subscript \ud835\udc5e \ud835\udc61 2 \ud835\udc5b 1 subscript ~ \ud835\udc8c \ud835\udc60 superscript subscript ~ \ud835\udc58 \ud835\udc60 0 \u22ef superscript subscript ~ \ud835\udc58 \ud835\udc60 \ud835\udc51 2 1 superscript subscript ~ \ud835\udc58 \ud835\udc60 \ud835\udc5b superscript subscript \ud835\udc58 \ud835\udc60 2 \ud835\udc5b \ud835\udc56 superscript subscript \ud835\udc58 \ud835\udc60 2 \ud835\udc5b 1 . \\begin{gathered}\\tilde{\\bm{q}}_{t}=\\left[\\tilde{q}_{t}^{(0)},\\cdots,\\tilde{q}_{t}^{(d/2-1)}\\right]\\quad\\tilde{q}_{t}^{(n)}=q_{t}^{(2n)}+iq_{t}^{(2n+1)}\\\\\n\\tilde{\\bm{k}}_{s}=\\left[\\tilde{k}_{s}^{(0)},\\cdots,\\tilde{k}_{s}^{(d/2-1)}\\right]\\quad\\tilde{k}_{s}^{(n)}=k_{s}^{(2n)}+ik_{s}^{(2n+1)}\\end{gathered}\\text{.} (8)\n\nAfter that, RoPE injects the position information by an element-wise multiplication between the pre-processed and a list of -parameterized rotary vectors in the complex plane. When attention is calculated, relative position information is acquired through and .",
    "scalelawrope-21": "\ud835\udc68 t , s = Re \u200b [ ( \ud835\udc92 ~ t \u2299 e i \u200b t \u200b \ud835\udf3d ) \u22c5 ( \ud835\udc8c ~ s \u2299 e i \u200b s \u200b \ud835\udf3d ) T ] = Re \u200b [ \u2211 n = 0 d / 2 \u2212 1 q ~ t ( n ) \u200b e i \u200b t \u200b \u03b8 n \u200b ( k ~ s ( n ) \u200b e i \u200b s \u200b \u03b8 n ) \u2217 ] = Re \u200b [ \u2211 n = 0 d / 2 \u2212 1 q ~ t ( n ) \u200b k ~ s ( n ) \u200b e i \u200b ( t \u2212 s ) \u200b \u03b8 n \u2217 ] = \u2211 n = 0 d / 2 \u2212 1 ( q t ( 2 \u200b n ) \u200b k s ( 2 \u200b n ) + q t ( 2 \u200b n + 1 ) \u200b k s ( 2 \u200b n + 1 ) ) \u200b cos \u2061 ( t \u2212 s ) \u200b \u03b8 n + ( q t ( 2 \u200b n ) \u200b k s ( 2 \u200b n + 1 ) \u2212 q t ( 2 \u200b n + 1 ) \u200b k s ( 2 \u200b n ) ) \u200b sin \u2061 ( t \u2212 s ) \u200b \u03b8 n \u200b . subscript \ud835\udc68 \ud835\udc61 \ud835\udc60 absent Re delimited-[] \u22c5 direct-product subscript ~ \ud835\udc92 \ud835\udc61 superscript \ud835\udc52 \ud835\udc56 \ud835\udc61 \ud835\udf3d superscript direct-product subscript ~ \ud835\udc8c \ud835\udc60 superscript \ud835\udc52 \ud835\udc56 \ud835\udc60 \ud835\udf3d \ud835\udc47 missing-subexpression absent Re delimited-[] superscript subscript \ud835\udc5b 0 \ud835\udc51 2 1 superscript subscript ~ \ud835\udc5e \ud835\udc61 \ud835\udc5b superscript \ud835\udc52 \ud835\udc56 \ud835\udc61 subscript \ud835\udf03 \ud835\udc5b superscript superscript subscript ~ \ud835\udc58 \ud835\udc60 \ud835\udc5b superscript \ud835\udc52 \ud835\udc56 \ud835\udc60 subscript \ud835\udf03 \ud835\udc5b Re delimited-[] superscript subscript \ud835\udc5b 0 \ud835\udc51 2 1 superscript subscript ~ \ud835\udc5e \ud835\udc61 \ud835\udc5b superscript subscript ~ \ud835\udc58 \ud835\udc60 \ud835\udc5b superscript superscript \ud835\udc52 \ud835\udc56 \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 \ud835\udc5b missing-subexpression absent superscript subscript \ud835\udc5b 0 \ud835\udc51 2 1 missing-subexpression limit-from superscript subscript \ud835\udc5e \ud835\udc61 2 \ud835\udc5b superscript subscript \ud835\udc58 \ud835\udc60 2 \ud835\udc5b superscript subscript \ud835\udc5e \ud835\udc61 2 \ud835\udc5b 1 superscript subscript \ud835\udc58 \ud835\udc60 2 \ud835\udc5b 1 \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 \ud835\udc5b missing-subexpression superscript subscript \ud835\udc5e \ud835\udc61 2 \ud835\udc5b superscript subscript \ud835\udc58 \ud835\udc60 2 \ud835\udc5b 1 superscript subscript \ud835\udc5e \ud835\udc61 2 \ud835\udc5b 1 superscript subscript \ud835\udc58 \ud835\udc60 2 \ud835\udc5b \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 \ud835\udc5b . \\begin{aligned} \\bm{A}_{t,s}&=\\mathrm{Re}\\left[\\left(\\tilde{\\bm{q}}_{t}\\odot{e^{it\\bm{\\theta}}}\\right)\\cdot\\left(\\tilde{\\bm{k}}_{s}\\odot{e^{is\\bm{\\theta}}}\\right)^{T}\\right]\\\\\n&=\\mathrm{Re}\\left[\\sum_{n=0}^{d/2-1}{\\tilde{q}_{t}^{(n)}e^{it\\theta_{n}}\\left(\\tilde{k}_{s}^{(n)}e^{is\\theta_{n}}\\right)^{*}}\\right]=\\mathrm{Re}\\left[{\\sum_{n=0}^{d/2-1}\\tilde{q}_{t}^{(n)}\\tilde{k}_{s}^{(n)}{}^{*}e^{i(t-s)\\theta_{n}}}\\right]\\\\\n&=\\sum_{n=0}^{d/2-1}{\\begin{aligned} &\\left(q_{t}^{(2n)}k_{s}^{(2n)}+q_{t}^{(2n+1)}k_{s}^{(2n+1)}\\right)\\cos{(t-s)\\theta_{n}}+\\\\\n&\\left(q_{t}^{(2n)}k_{s}^{(2n+1)}-q_{t}^{(2n+1)}k_{s}^{(2n)}\\right)\\sin{(t-s)\\theta_{n}}\\end{aligned}}\\end{aligned}\\text{\\ .} (9)\n\nWhile RoPE can theoretically convey the relative information at any context length, RoPE still fails to extrapolate practically. It is worth noting that rotary angles in Equation 9 play an important role. In the vanilla design of RoPE, is defined as Equation 10. Different angles correspond to different features and that is the starting point of most RoPE-based extrapolation methods (LocalLLaMA, 2023b; Rozi\u00e8re et al., 2023; Pal et al., 2023) shown in Table 2. \ud835\udf3d = [ \u03b8 0 , \u22ef , \u03b8 d / 2 \u2212 1 ] \u03b8 n = 10000 \u2212 2 \u200b n / d \u200b . formulae-sequence \ud835\udf3d matrix subscript \ud835\udf03 0 \u22ef subscript \ud835\udf03 \ud835\udc51 2 1 subscript \ud835\udf03 \ud835\udc5b superscript 10000 2 \ud835\udc5b \ud835\udc51 . \\begin{aligned} \\bm{\\theta}=\\begin{bmatrix}\\theta_{0},\\cdots,\\theta_{d/2-1}\\end{bmatrix}\\quad\\theta_{n}=10000^{-2n/d}\\end{aligned}\\text{.} (10)\n\nA.2 RoPE from Frequency Domain\n\nFrom a frequency domain perspective, the rotation operation of RoPE can be viewed as the Inverse Fourier Transform from the frequency domain to the time domain. \ud835\udc68 t , s = Re \u200b [ \u2211 n = 0 d / 2 \u2212 1 q ~ t ( n ) \u200b k ~ s ( n ) \u200b e i \u200b ( t \u2212 s ) \u200b \u03b8 n \u2217 ] = Re [ \u2131 \ud835\udf3d \u2212 1 [ \ud835\udc92 ~ t ( n ) \ud835\udc8c ~ s ( n ) ] \u2217 ] \u200b . \\begin{aligned} \\bm{A}_{t,s}&=\\mathrm{Re}\\left[{\\sum_{n=0}^{d/2-1}\\tilde{q}_{t}^{(n)}\\tilde{k}_{s}^{(n)}{}^{*}e^{i(t-s)\\theta_{n}}}\\right]\\\\\n&=\\mathrm{Re}\\left[\\mathcal{F}^{-1}_{\\bm{\\theta}}\\left[\\tilde{\\bm{q}}^{(n)}_{t}\\tilde{\\bm{k}}^{(n)}_{s}{}^{*}\\right]\\right]\\end{aligned}\\text{.} (11)\n\nAt this juncture, the learning objective for RoPE-based LLM is essentially to understand features in terms of frequency. Depending on the value of , the higher dimensions correspond to the longer period as shown in Equation 12 as well as the lower-frequency features reflecting the longer contextual semantic relationship (Chen et al., 2023; Rozi\u00e8re et al., 2023). T n = 2 \u200b \u03c0 \u03b8 n = 2 \u200b \u03c0 \u22c5 10000 2 \u200b n / d \u200b ,\u2003for \u200b n = 0 \u200b \u22ef \u200b d 2 \u2212 1 \u200b . subscript \ud835\udc47 \ud835\udc5b 2 \ud835\udf0b subscript \ud835\udf03 \ud835\udc5b \u22c5 2 \ud835\udf0b superscript 10000 2 \ud835\udc5b \ud835\udc51 ,\u2003for \ud835\udc5b 0 \u22ef \ud835\udc51 2 1 . T_{n}=\\frac{2\\pi}{\\theta_{n}}=2\\pi\\cdot{10000}^{2n/d}\\text{,\\quad for }n=0\\cdots\\frac{d}{2}-1\\text{.} (12)\n\nHowever, as illustrated in Figure 5, the trigonometric functions of lower-frequency features do not complete a full period within the training context. As a result, RoPE-based LLMs might not fully recognize the periodic nature of and waves, leading to inadequate training. Consequently, these lower-frequency features are more susceptible to under-fitting or over-fitting. Hence, the number of well-trained dimensions is essential and this is the critical dimension raised in Section 3.2. Appendix B Appendix\n\nB.1 Experiment Setup\n\nWe conduct experiments on the pre-trained 7B and 13B LLaMA2 models (Touvron et al., 2023b). For fine-tuning 7B and 13B models, we use 32 A100 GPUs and adopt ZeRO3 strategies (Rajbhandari et al., 2020). We use AdamW (Loshchilov & Hutter, 2017) with and . We set the learning rate to with no warmup. We set the max gradient norm to 2.5 for 7B and 1 for 13B respectively. We set the weight decay to zero. For fine-tuning RoPE with different bases, we set the global batch size to 128, tuning the context length to 4K, the same as the training length, and the evaluating context length to 100K. We fine-tune the models for 1K steps using the next token prediction objective with training data from the Pile (Gao et al., 2020) and compare the tuning performance on the validation set of Books3 subset (Presser, 2020) from the Pile. We fine-tune LLaMA2 with CoLLiE333https://github.com/OpenLMLab/collie, a collaborative toolbox for tuning large language models in an efficient way, and conduct evaluation discussed in Appendix B.2 with OpenCompass444https://opencompass.org.cn/. Both training and testing are accelerated by FlashAttention-2 (Dao, 2023). We compare results with mainstream extrapolating strategies, such as Linear Position Interpolation (Chen et al., 2023) and NTK method (LocalLLaMA, 2023b; a). For fine-tuning with Linear PI, we set the global batch size to 64 and tuning length to 8K, which follows Chen et al. (2023). For fine-tuning with 16K context length, wet set the global batch size 32. When we fine-tune LLaMA2 7B with the last 36 dimensions being cut off, discussed in Section 3.4, we set the softmax scale , the square root of updated dimension size of , and keep other setups the same. Except for the position embedding, we do not modify the architecture of LLaMA2 (Touvron et al., 2023b). B.2 Short Context Validation\n\nNext, we validate whether Scaling RoPE, namely the fine-tuning RoPE with different bases on original context length, has a side effect on LLM. We use short context tasks, such as NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), MMLU (Hendrycks et al., 2020) and SuperGLUE (Wang et al., 2019), to assess whether the model loses the knowledge acquired in pre-training after the fine-tuning process. The results obtained are shown in the Table 3. We find that the fine-tuning results, though slightly lower compared to the original LLaMA2 7B and 13B (Touvron et al., 2023b) and directly fine-tuned version, are still acceptable. In fact, in some tasks, the performance of Scaling-RoPE even appears to be better than before. This indicates that our approach not only enhances the extrapolation performance but also does not significantly harm the inherent knowledge of LLM. Appendix C Extension\n\nConcerning that the current extrapolation methods in the fine-tuning phase dominantly apply longer tuning contexts, we propose an extended version of the scaling law of RoPE-based extrapolation. Theorem 3. (Extended Scaling Law of RoPE-based Extrapolation)\n\nFor RoPE-based LLMs pre-trained with context length and critical dimension , if we adjust the base to and then conduct fine-tuning with context length , the extrapolation performance of RoPE-based LLMs will get improved. Importantly, there exists a critical base decided by and . \u03b2 0 = 10000 log T train 2 \u200b \u03c0 \u2061 T tune 2 \u200b \u03c0 \u200b . subscript \ud835\udefd 0 superscript 10000 subscript subscript \ud835\udc47 train 2 \ud835\udf0b subscript \ud835\udc47 tune 2 \ud835\udf0b . \\beta_{0}={10000}^{\\log_{\\frac{T_{\\text{train}}}{2\\pi}}{\\frac{T_{\\text{tune}}}{2\\pi}}}\\text{.} (13)\n\nIf , the extrapolation upper bound is decided by and as follows:\n\nT extra = 2 \u200b \u03c0 \u22c5 \u03b2 d extra \u22c5 1 d = 2 \u200b \u03c0 \u22c5 \u03b2 \u2308 d 2 \u200b log 10000 \u2061 T train 2 \u200b \u03c0 \u2309 \u22c5 2 d \u200b . subscript \ud835\udc47 extra \u22c5 2 \ud835\udf0b superscript \ud835\udefd \u22c5 subscript \ud835\udc51 extra 1 \ud835\udc51 \u22c5 2 \ud835\udf0b superscript \ud835\udefd \u22c5 \ud835\udc51 2 subscript 10000 subscript \ud835\udc47 train 2 \ud835\udf0b 2 \ud835\udc51 . T_{\\text{extra}}=2\\pi\\cdot\\beta^{d_{\\text{extra}}\\cdot\\frac{1}{d}}=2\\pi\\cdot\\beta^{\\left\\lceil{\\frac{d}{2}}\\log_{10000}{\\frac{T_{\\text{train}}}{2\\pi}}\\right\\rceil\\cdot{\\frac{2}{d}}}\\text{.} (14)\n\nOtherwise, the extrapolation upper bound is and the critical dimension is updated satisfying\n\nd extra \u2032 = 2 \u200b \u2308 d 2 \u200b log \u03b2 \u2061 T tune 2 \u200b \u03c0 \u2309 \u2265 2 \u200b \u2308 d 2 \u200b log 10000 \u2061 T train 2 \u200b \u03c0 \u2309 = d extra \u200b . subscript superscript \ud835\udc51 \u2032 extra 2 \ud835\udc51 2 subscript \ud835\udefd subscript \ud835\udc47 tune 2 \ud835\udf0b 2 \ud835\udc51 2 subscript 10000 subscript \ud835\udc47 train 2 \ud835\udf0b subscript \ud835\udc51 extra . d^{\\prime}_{\\text{extra}}=2\\left\\lceil{\\frac{d}{2}}\\log_{\\beta}{\\frac{T_{\\text{tune}}}{2\\pi}}\\right\\rceil\\geq 2\\left\\lceil{\\frac{d}{2}}\\log_{10000}{\\frac{T_{\\text{train}}}{2\\pi}}\\right\\rceil=d_{\\text{extra}}\\text{.} (15)\n\nHowever, the extrapolation beyond will acquire further enhancement if gets smaller. Particularly, when is smaller than as follows, the enhancement will be more significant. \u03b2 1 = 2 \u200b T tune \u03c0 \u200b , \u200b \u03b2 2 = T tune \u03c0 \u200b , \u200b \u03b2 3 = T tune 2 \u200b \u03c0 \u200b . subscript \ud835\udefd 1 2 subscript \ud835\udc47 tune \ud835\udf0b , subscript \ud835\udefd 2 subscript \ud835\udc47 tune \ud835\udf0b , subscript \ud835\udefd 3 subscript \ud835\udc47 tune 2 \ud835\udf0b . \\beta_{1}=\\frac{2T_{\\text{tune}}}{\\pi}\\text{, \\quad}\\beta_{2}=\\frac{T_{\\text{tune}}}{\\pi}\\text{, \\quad}\\beta_{3}=\\frac{T_{\\text{tune}}}{2\\pi}\\text{.} (16)\n\nTheorem 3.",
    "scalelawrope-22": "serves as both a combination and generalization of Theorem 1., Lemma 1. and Theorem 2. Here, the critical base is the worst base for extrapolation as well as the smallest base forcing RoPE to extrapolate depending on the feature dimensions within the critical dimension. Specifically, when , the critical base, , relates to the intriguing observation outlined in Section 2. Equation 14 corresponds to tuning RoPE with larger bases discussed in Section 3.3, Equation 15 corresponds to the definition of critical dimension in Section 3.2 and Equation 16 corresponds to tuning RoPE with smaller bases discussed in Section 3.1. If , RoPE-based LLMs can accommodate a broader context window. As illustrated in Figure 9, when fine-tuning LLaMA2 7B and 13B (Touvron et al., 2023b) within a 16K context, the max context length exceeds or equals 16K, surpassing the original LLaMA2 7B and 13B respectively, regardless of the base value. From a periodic perspective, since both base and training length have changed, it prompts us to consider whether additional positional information has been integrated during the fine-tuning phase for feature dimensions beyond the critical dimension, namely the 92nd dimension for LLaMA2 (Touvron et al., 2023b), as indicated in Equation 5. According to the definition of the critical dimension, based on the current base , we can calculate how many dimensions the RoPE-based LLM has where and complete a period within the tuning length. If base , then , the number of dimensions that cover a period during fine-tuning have already been able to traverse a complete period during pre-training, given that\n\nd ~ extra = 2 \u200b \u2308 d 2 \u200b log \u03b2 \u2061 T tune 2 \u200b \u03c0 \u2309 \u2264 2 \u200b \u2308 d 2 \u200b log \u03b2 0 \u2061 T tune 2 \u200b \u03c0 \u2309 = 2 \u200b \u2308 d 2 \u200b log 10000 log T train 2 \u200b \u03c0 \u2061 T tune 2 \u200b \u03c0 \u2061 T tune 2 \u200b \u03c0 \u2309 = 2 \u200b \u2308 d 2 \u200b 1 log T train 2 \u200b \u03c0 \u2061 T tune 2 \u200b \u03c0 \u200b log 10000 \u2061 T tune 2 \u200b \u03c0 \u2309 = 2 \u200b \u2308 d 2 \u200b log T tune 2 \u200b \u03c0 \u2061 T train 2 \u200b \u03c0 log T tune 2 \u200b \u03c0 \u2061 10000 \u2309 = 2 \u200b \u2308 d 2 \u200b log 10000 \u2061 T train 2 \u200b \u03c0 \u2309 = d extra \u200b . subscript ~ \ud835\udc51 extra 2 \ud835\udc51 2 subscript \ud835\udefd subscript \ud835\udc47 tune 2 \ud835\udf0b absent 2 \ud835\udc51 2 subscript subscript \ud835\udefd 0 subscript \ud835\udc47 tune 2 \ud835\udf0b 2 \ud835\udc51 2 subscript superscript 10000 subscript subscript \ud835\udc47 train 2 \ud835\udf0b subscript \ud835\udc47 tune 2 \ud835\udf0b subscript \ud835\udc47 tune 2 \ud835\udf0b missing-subexpression absent 2 \ud835\udc51 2 1 subscript subscript \ud835\udc47 train 2 \ud835\udf0b subscript \ud835\udc47 tune 2 \ud835\udf0b subscript 10000 subscript \ud835\udc47 tune 2 \ud835\udf0b 2 \ud835\udc51 2 subscript subscript \ud835\udc47 tune 2 \ud835\udf0b subscript \ud835\udc47 train 2 \ud835\udf0b subscript subscript \ud835\udc47 tune 2 \ud835\udf0b 10000 missing-subexpression absent 2 \ud835\udc51 2 subscript 10000 subscript \ud835\udc47 train 2 \ud835\udf0b subscript \ud835\udc51 extra . \\begin{aligned} \\tilde{d}_{\\text{extra}}=2\\left\\lceil{\\frac{d}{2}}\\log_{\\beta}{\\frac{T_{\\text{tune}}}{2\\pi}}\\right\\rceil&\\leq 2\\left\\lceil{\\frac{d}{2}}\\log_{\\beta_{0}}{\\frac{T_{\\text{tune}}}{2\\pi}}\\right\\rceil=2\\left\\lceil{\\frac{d}{2}}\\log_{{10000}^{\\log_{\\frac{T_{\\text{train}}}{2\\pi}}{\\frac{T_{\\text{tune}}}{2\\pi}}}}{\\frac{T_{\\text{tune}}}{2\\pi}}\\right\\rceil\\\\\n&=2\\left\\lceil{\\frac{d}{2}}\\frac{1}{{\\log_{\\frac{T_{\\text{train}}}{2\\pi}}{\\frac{T_{\\text{tune}}}{2\\pi}}}}\\log_{10000}{\\frac{T_{\\text{tune}}}{2\\pi}}\\right\\rceil=2\\left\\lceil{\\frac{d}{2}}\\frac{{\\log_{\\frac{T_{\\text{tune}}}{2\\pi}}{\\frac{T_{\\text{train}}}{2\\pi}}}}{\\log_{\\frac{T_{\\text{tune}}}{2\\pi}}{10000}}\\right\\rceil\\\\\n&=2\\left\\lceil{\\frac{d}{2}}\\log_{10000}{\\frac{T_{\\text{train}}}{2\\pi}}\\right\\rceil=d_{\\text{extra}}\\end{aligned}\\text{.}\n\nTherefore, the critical dimension remains unchanged. Referring to Theorem 2 in Section 3.3, we can calculate the extrapolation upper bound based on the updated base and the original critical dimension as Equation 14, exactly the same as Equation 5. For LLaMA2 (Touvron et al., 2023b) fine-tuned with a 16K context, as illustrated in Figure 9, the critical base is 71738, given Equation 13. For bases greater than 71738, such as 80000, 120000, and 1000000, their extrapolation upper bounds surpass 16K and the larger base corresponds to a longer context, corroborating our theoretical framework. If base , then during the fine-tuning phase, the number of dimensions able to complete a period surpasses the original critical dimension, so the critical dimension is updated as Equation 15. Besides, since this dimension depends on the fine-tuning length, the extrapolation upper bound is still constrained within the fine-tuning length . However, if is so small that the input of every can span values from 0 to , , or within the fine-tuning length , as indicated by Equation 16, similar to Theorem 1 in Section 3.1, then the extrapolation performance will get further improved, marked by a more stable perplexity growth curve. For LLaMA2 (Touvron et al., 2023b) fine-tuned with a 16K context, as shown in Figure 9, for bases smaller than the critical base 71738, such as 60000, 20000, 10000, and 500, the performance curves become progressively more stable. Among them, although performs poorly in fine-tuning at the original context length, the performance gets significantly improved this time because the inputs of or have traversed to within the 16K context length. When , LLaMA2 achieved a similar terrific performance as , namely the design of Code LLaMA (Rozi\u00e8re et al., 2023), a context length with at least 100K tokens, breaking the curse of entropy explosion mentioned in Han et al. (2023). Since there exists an upper bound for extrapolation based on , RoPE tuned with base 500 on 16K context length has the potential to extrapolate to an infinite context, thus answering Q3 in the Introduction. Similarly, we also use scatter plots to visualize the fluctuation of attention scores for different bases after fine-tuning with a 16K context as illustrated in Figure 10. For base 500, given its exposure to enough fluctuations during the training phase, the perplexity curve remains notably stable. For base 10000, it is clear that the fluctuation of attention scores in the last 36 dimensions is somewhat limited, leading to a noticeable improvement in extrapolation performance given Equation 16. For base 40000, the position information acquired in the fine-tuning phase shrinks further as the base increases. For base 120000, the critical dimension goes back to 92 dimensions, and the extrapolation performance is governed by the first 92 dimensions. Remarkably, the extrapolation upper bound given Equation 14 matches the maximum supported context length. For base 1000000, the period of the first 92 dimensions is further extended, corresponding to a context length expanding beyond 100K. Eventually, based on the above interpretation, we validate the correctness of Theorem 3. and provide a unified framework to explain the extrapolation of RoPE-based LLM with arbitrary base and fine-tuning context length. Appendix D Discussion\n\nBesides, we discuss the instructive value of our theory for other extrapolation strategies focused on achieving longer context during the testing phase. These methods are still necessary given two facts. On one hand, the performance of RoPE with a smaller base is still left behind compared with RoPE with much larger bases, such as 1000000, as shown in Figure 1. On the other hand, for RoPE with a base that is not large enough, it still can not extrapolate to a context of 100K or longer as shown in Figure 2. In order to further enhance RoPE\u2019s adaptability to a longer context, whatever the base value is, we discuss the effect of two complementary methods in the inference phase, log-scaled attention (Su, 2023b) and dynamic-scaled RoPE (LocalLLaMA, 2023a) on RoPE with different bases. Log-scaled Attention\n\nas shown in Equation 17, is a classic technique originally raised in Chiang & Cholak (2022) and currently applied in RoPE-based extrapolation (Su, 2023b; Alibaba, 2023). It involves multiplying the original attention matrix by the logarithm of the current inference length . Traditionally, the base of the logarithm is training length . However, given Equation 14 in Theorem 3., the attention score within the max supported context length is reliable. So we take as the logarithm base and set the lower limit for the logarithmic correction value as 1, meaning that no additional log scale is required within the extrapolation upper bound. \ud835\udc68 t , s = Re \u200b [ \u2211 n = 0 d / 2 \u2212 1 q ~ t ( n ) \u200b k ~ s ( n ) \u200b e i \u200b ( t \u2212 s ) \u200b \u03b8 n \u2217 ] \u22c5 p t p t = max \u2061 ( 1 , log T extra \u2061 t ) \u200b . subscript \ud835\udc68 \ud835\udc61 \ud835\udc60 \u22c5 Re delimited-[] superscript subscript \ud835\udc5b 0 \ud835\udc51 2 1 superscript subscript ~ \ud835\udc5e \ud835\udc61 \ud835\udc5b superscript subscript ~ \ud835\udc58 \ud835\udc60 \ud835\udc5b superscript superscript \ud835\udc52 \ud835\udc56 \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 \ud835\udc5b subscript \ud835\udc5d \ud835\udc61 subscript \ud835\udc5d \ud835\udc61 1 subscript subscript \ud835\udc47 extra \ud835\udc61 . \\begin{gathered}\\bm{A}_{t,s}=\\mathrm{Re}\\left[{\\sum_{n=0}^{d/2-1}\\tilde{q}_{t}^{(n)}\\tilde{k}_{s}^{(n)}{}^{*}e^{i(t-s)\\theta_{n}}}\\right]\\cdot{p_{t}}\\\\\np_{t}=\\max\\left(1,\\log_{T_{\\text{extra}}}{t}\\right)\\end{gathered}\\text{.} (17)\n\nBesides Log-scaled attention, window method, such as sliding window and its variant, is also a widely accepted strategy for extrapolation, used in inference or evaluation (Press et al., 2021; Sun et al., 2022). Compared with the above strict window-based method, we follow the xPos method proposed in Sun et al. (2022), shown in Equation 18, originally used in the pre-training phase. In this work, we regard this method as a soft sliding window used in the inference phase as use it as a further complement to the log-scaled method. Still, we do little modification besides using the as the denominator instead of the original denominator . \ud835\udc68 t , s = Re \u200b [ \u2211 n = 0 d / 2 \u2212 1 q ~ t ( n ) \u200b k ~ s ( n ) \u200b \u03b6 n t \u2212 s T extra \u2217 \u200b e i \u200b ( t \u2212 s ) \u200b \u03b8 n ] \u03b6 n = \u03b3 + 2 \u200b n / d \u03b3 + 1 , n = 0 \u200b \u22ef \u200b d 2 \u2212 1 , \u03b3 = 0.4 \u200b . formulae-sequence subscript \ud835\udc68 \ud835\udc61 \ud835\udc60 Re delimited-[] superscript subscript \ud835\udc5b 0 \ud835\udc51 2 1 superscript subscript ~ \ud835\udc5e \ud835\udc61 \ud835\udc5b superscript subscript ~ \ud835\udc58 \ud835\udc60 \ud835\udc5b superscript superscript subscript \ud835\udf01 \ud835\udc5b \ud835\udc61 \ud835\udc60 subscript \ud835\udc47 extra superscript \ud835\udc52 \ud835\udc56 \ud835\udc61 \ud835\udc60 subscript \ud835\udf03 \ud835\udc5b subscript \ud835\udf01 \ud835\udc5b \ud835\udefe 2 \ud835\udc5b \ud835\udc51 \ud835\udefe 1 formulae-sequence \ud835\udc5b 0 \u22ef \ud835\udc51 2 1 \ud835\udefe 0.4 . \\begin{gathered}\\bm{A}_{t,s}=\\mathrm{Re}\\left[{\\sum_{n=0}^{d/2-1}\\tilde{q}_{t}^{(n)}\\tilde{k}_{s}^{(n)}{}^{*}\\zeta_{n}^{\\frac{t-s}{T_{\\text{extra}}}}e^{i(t-s)\\theta_{n}}}\\right]\\\\\n\\zeta_{n}=\\frac{\\gamma+2n/d}{\\gamma+1},\\ n=0\\cdots\\frac{d}{2}-1,\\ \\gamma=0.4\\end{gathered}\\text{.} (18)\n\nDynamic-scaled RoPE\n\nnamely Dynamic NTK (LocalLLaMA, 2023a) is a widely used extrapolation. Here, we only do two little modifications. One is to change the base 10000 in vanilla RoPE (Su et al., 2021) with the base scaled in the fine-tuning phase, . The other is still to replace the in Equation 7 with we derive given Equation 14 in Theorem 3. for \u200b \ud835\udc68 t , s \u200b , \u200b \u03b8 n = ( \u03b2 \u22c5 \u03b1 t ) \u2212 2 \u200b n / d \u200b , where \u200b \u03b1 t = max \u2061 ( 1 , 2 \u2308 log 2 \u2061 t T extra \u2309 + 1 \u2212 1 ) \u200b . for subscript \ud835\udc68 \ud835\udc61 \ud835\udc60 , subscript \ud835\udf03 \ud835\udc5b superscript \u22c5 \ud835\udefd subscript \ud835\udefc \ud835\udc61 2 \ud835\udc5b \ud835\udc51 , where subscript \ud835\udefc \ud835\udc61 1 superscript 2 subscript 2 \ud835\udc61 subscript \ud835\udc47 extra 1 1 . \\text{for }\\bm{A}_{t,s}\\text{, }\\theta_{n}={\\left(\\beta\\cdot\\alpha_{t}\\right)}^{-2n/d}\\text{, where }\\alpha_{t}=\\max\\left(1,2^{\\left\\lceil\\log_{2}{\\frac{t}{T_{\\text{extra}}}}\\right\\rceil+1}-1\\right)\\text{.} (19)\n\nWe experiment with these two methods on LLaMA2 7B (Touvron et al., 2023b) and get the results as shown in Figure 11. Figure 11(a) shows the results on LLaMA2 based on RoPE with base 10000. It is clear that both log-scaled attention almost does nothing for pre-trained and fine-tuned LLaMA2, but show great improvement for fine-tuning LLaMA2 with cutting the last 36 dimensions in off. This phenomenon further proves that the fluctuation coming from the dimensions of beyond the critical dimension is the root cause of the extrapolation problem of RoPE-based LLM. Figure 11(b) shows the results on LLaMA2 based on RoPE with bases smaller than 10000. There is a clear trend that with the reduction of the base value, the improvement obtained from the log-scaled attention is more dominant while the effect of Dynamic NTK shrinks gradually. For RoPE with base 500, the perplexity curve of log-scaled RoPE is flat enough, indicating the extrapolation capability to support 100K context length. On the contrary, Dynamic NTK shows a clear side effect. Hence, the position information learned in the training phase is reliable enough for LLM to extrapolate further, corresponding to Theorem 1. in Section 3.1. Figure 11(c) shows the results on LLaMA2 based on RoPE with bases larger than 10000, such as 40000, 160000, 400000, and 600000. We do not test the performance of two methods on RoPE with base 1000000, since it already achieved the context length of 100K. Here, we enable RoPE with bases larger than 10000 and smaller than 1000000 to extrapolate beyond the context length of 100K in the inference phase. For RoPE with larger bases, the improvement of extrapolation performance obtained from Dynamic NTK is more remarkable. The working principle of Dynamic NTK has been visualized in Figure 8 and discussed in Section 3.4. Besides, replacing the with becomes significantly important for RoPE with larger bases. For example, if Dynamic NTK is carried out based on , the improvement will be limited and even destroyed when the base is large enough like 400000 and 600000. This phenomenon proves the guidance value of this work for other extrapolation works. In conclusion, for a base smaller than defined in Equation 1, every dimension has learned complete positional information. Then the log-scaled method is sufficient to enhance extrapolation. For a base larger than , namely 10000 for tuning on the original context, Dynamic NTK in the correct way is a good helper for extrapolation to a much longer context. Appendix E Text Continuation\n\nFinally, we execute a text continuation experiment. For LLaMA2 7B(Touvron et al., 2023b) fine-tuned with various base values and fine-tuning lengths, we provide a context of 32K tokens from Books3(Presser, 2020) and prompt the model to continue writing. The resulting texts, after cleaning the special characters, are shown in Figure 12. Remarkably, whether the base is set at 500 or 1000000, the generated text remains coherent, grammatically accurate, and logically consistent. For base 500, extended tuning length or incorporating log-scaled attention Su (2023b) in the testing phase yields improved continuation results. Given the strict extrapolation upper bound for base 1000000, there is a compelling case that a model fine-tuned with base 500 possesses an infinite extrapolation potential. Limitation\n\nIn this work, our primary objective is to elucidate the mechanisms by which RoPE, using either smaller or larger bases, enhances extrapolation performance. Furthermore, there remains a need to rigorously assess our method on extended context data. Additionally, it needs additional efforts to integrate whether there exist upper and lower bounds for adjusting the base. Finally, since the base value and pre-training context length is the key to extrapolation, there might be an optimal pair of base and training length for RoPE-based LLM pre-training to be discovered. \u25c4 Feeling lucky?",
    "scalelawrope-23": "Conversion report Report an issue View original on arXiv\u25ba\n\nCopyright Privacy Policy\n\nGenerated on Wed Feb 28 02:31:34 2024 by LaTeXML"
}